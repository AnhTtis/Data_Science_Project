# Data_Science_Project

*A Multi-Milestone Data Science Project*

---

## ğŸ§­ Overview

- This project is part of the **Introduction to Data Science** course offered by the **Department of Computer Science, University of Science (VNU-HCMC)**.

- The first milestone focuses on **data scraping engineering** â€” transforming theoretical knowledge of data crawling into practical implementation by harvesting academic papers from **arXiv**, an open-access scientific repository.

---

## ğŸ‘¨â€ğŸ’» Team Members

| Name                   | Student ID | 
| ---------------------- | ---------- | 
| **Nguyá»…n Há»¯u Anh TrÃ­** | 23127130   | 
| **Nguyá»…n Huy QuÃ¢n**    | 23127107   | 
| **Cao Táº¥n HoÃ ng Huy**  | 23127051   | 

---

## ğŸ¯ Milestone 1: Data Scraping and Repository Engineering

Milestone 1 enables students to:

* Implement **multi-threaded web scraping** techniques to retrieve structured data at scale and  understand the **engineering workflow** of building a data collection pipeline from open APIs.
* Practice handling **large datasets** and **metadata organization**.
* Integrate and synchronize multiple data sources: **arXiv API**, **OAI-PMH**, **Semantic Scholar API**, and **Kaggle dataset**.

---

## âš™ï¸ Tools and Technologies

| Tool / Library                   | Purpose                                                            |
| -------------------------------- | ------------------------------------------------------------------ |
| **arXiv API**                    | Retrieve paper metadata, version history, and source URLs          |
| **OAI-PMH (Sickle library)**     | Harvest bulk metadata for large-scale retrieval                    |
| **Semantic Scholar Graph API**   | Extract paper references, citation graphs, and related identifiers |
| **Kaggle Cornell ArXiv Dataset** | Pre-aggregated metadata for local data exploration                 |
| **Python threading & requests**  | Concurrent download and network request handling                   |
| **pandas / json / os**           | Data transformation, storage, and management                       |

---

## ğŸ§µ Multi-Threaded Data Pipeline

To ensure efficiency and scalability, the pipeline employs **four concurrent threads**, each managing a critical phase of the data scraping workflow:

1. **Thread 1 â€“ Entry Discovery**

   * Uses the **arXiv API** and **OAI-PMH** to collect all assigned **arXiv IDs**.
   * Initializes folder structures for each paper in the format `yyyymm-id/`.

2. **Thread 2 â€“ Metadata Retrieval**

   * Queries all available versions of each paper.
   * Extracts title, authors, submission/revision dates, and venue.
   * Stores results as structured **`metadata.json`** files.

3. **Thread 3 â€“ Full-Text Download**

   * Retrieves `.tar.gz` LaTeX sources (via `arxiv.py` or `arxiv-downloader`) and corresponding PDFs.
   * Handles file integrity and version tracking within the `tex/` subdirectory.
   * Removes figure files to reduce data size, as per course requirements.

4. **Thread 4 â€“ Reference Extraction**

   * Fetches **BibTeX** entries and **external identifiers** (DOI, ArXiv ID) via **Semantic Scholar API**.
   * Saves structured references to `references.json` and citation data to `references.bib`.

This modularized and concurrent design supports fault isolation, improved runtime performance, and higher throughput for large datasets.

---

## ğŸ“‚ Project Structure

The folder hierarchy follows the required format from the course guideline:

```
Data_Scince_Project/
â”‚
â”œâ”€â”€ README.md                                   # Main project documentation
â”‚
â””â”€â”€ Milestone1/
    â”‚
    â”œâ”€â”€ data/                                   # Main data repository
    â”‚   â”œâ”€â”€ 232303-07857/                         # Folder named after an arXiv ID (yyyymm-id)
    â”‚   â”‚   â”œâ”€â”€ 2303.07857v1/                             # Subfolder for version 1 of the paper
    â”‚   â”‚   â”‚   â”œâ”€â”€ paper_2310-12345v1.tex
    â”‚   â”‚   â”‚   â””â”€â”€ references.bib
    â”‚   â”‚   â”œâ”€â”€ v2/                             # Version 2, same structure
    â”‚   â”‚   â”‚   â”œâ”€â”€ paper_2310-12345v2.tex
    â”‚   â”‚   â”‚   â”œâ”€â”€ references.json
    â”‚   â”‚   â”‚   â””â”€â”€ references.bib
    â”‚   â”‚   â”œâ”€â”€ metadata.json
    â”‚   â”‚   â”‚â”€â”€ references.json
    â”‚   â”œâ”€â”€ 232303-07858/
    â”‚   â”‚   â”œâ”€â”€ v1/
    â”‚   â”‚   â”‚   â”œâ”€â”€ paper_2310-12678v1.tex
    â”‚   â”‚   â”‚   â””â”€â”€ references.bib
    â”‚   â”‚   â”‚â”€â”€ metadata.json
    â”‚   â”‚   â”‚â”€â”€ references.json
    â”‚   â”‚
    â”‚   â””â”€â”€ ...                                 # Additional arXiv paper folders
    â”‚
    â”œâ”€â”€ scripts/                                # Source code and automation modules
    â”‚   â”œâ”€â”€ main.py                             # Entry point to run the complete pipeline
    â”‚   â”œâ”€â”€ arxiv_handler.py                    # Handles API queries, ID retrieval, folder creation
    â”‚   â”œâ”€â”€ downloader.py                       # Downloads .tar.gz sources and PDFs
    â”‚   â”œâ”€â”€ metadata_collector.py               # Retrieves metadata for all versions
    â”‚   â”œâ”€â”€ reference_extractor.py              # Collects references from Semantic Scholar
    â”‚
    â”œâ”€â”€ Milestone1_Report.pdf                   # Single official report file (methodology & performance)
    â”‚
    â”œâ”€â”€ Milestone1_Demo_Video.mp4               # Single 2-minute demo video
    â”‚
    â””â”€â”€requirements.txt                        # Python dependencies

```

---

## ğŸ§° Environment Setup

**Requirements:**

* Python â‰¥ 3.9
* Internet connection (for API requests)
* Recommended libraries:

```bash
pip install -r requirements.txt
```

**Run Command Example:**

```bash
python scripts/main.py
```
---

## ğŸ“Š Evaluation Metrics

Each scraper execution is evaluated using the following criteria (as required by the course specification):

| Category              | Metric                                       |
| --------------------- | -------------------------------------------- |
| **Data Completeness** | Number of papers scraped successfully        |
| **Metadata Coverage** | Ratio of successfully retrieved JSON entries |
| **Performance**       | Average runtime and memory footprint         |
| **Data Efficiency**   | Storage reduction after removing figures     |
| **Quality**           | Correctness of references and file structure |

---

## ğŸ§¾ Deliverables

* **Source Code:** All `.py` files organized under `scripts/` and `src/`.
* **Dataset:** Compressed `.zip` following the naming rule `<StudentID>.zip`.
* **Report:** Technical report detailing implementation and performance analysis.
* **Demo Video:** 2-minute demonstration of system execution and explanation.

---

## ğŸ“š References

1. Waleed Ammar et al., *The Semantic Scholar Open Research Corpus*, [arXiv:1805.02234](https://arxiv.org/abs/1805.02234)
2. [arXiv API Basics](https://info.arxiv.org/help/api/basics.html)
3. [OAI-PMH Protocol](https://info.arxiv.org/help/oa/index.html)
4. [Semantic Scholar Graph API](https://api.semanticscholar.org/api-docs/graph)
5. [Cornell University ArXiv Dataset on Kaggle](https://www.kaggle.com/datasets/Cornell-University/arxiv)
6. [arxiv-downloader GitHub Repository](https://github.com/braun-steven/arxiv-downloader)
7. [arxiv.py Wrapper Library](https://github.com/lukasschwab/arxiv.py)

---

**Â© 2025 University of Science (VNU-HCMC)**
Developed for **Introduction to Data Science â€“ Milestone 1** under the guidance of **Huá»³nh LÃ¢m Háº£i ÄÄƒng**.
