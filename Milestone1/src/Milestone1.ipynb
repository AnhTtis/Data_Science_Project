{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltYoAOFRZE_7"
      },
      "source": [
        "\n",
        "## Required Python Libraries\n",
        "\n",
        "Your workflow depends on several libraries for interacting with the arXiv API, handling files, managing concurrency, monitoring system resources, and processing text.\n",
        "\n",
        "Below is a quick overview of what each library does:\n",
        "\n",
        "* **arxiv** ‚Äì Fetching paper metadata and downloading PDFs from arXiv.\n",
        "* **requests** ‚Äì Sending HTTP requests (used for downloading files or checking links).\n",
        "* **psutil** ‚Äì Monitoring memory, CPU usage, and process information.\n",
        "* **threading** ‚Äì Running tasks concurrently to speed up download/extraction.\n",
        "* **queue** ‚Äì Thread-safe task queues for worker threads.\n",
        "* **tarfile** ‚Äì Extracting `.tar.gz` or `.tgz` source files from arXiv.\n",
        "* **re** ‚Äì Handling regular expressions for parsing text.\n",
        "* **json** ‚Äì Reading and writing JSON configurations or metadata.\n",
        "* **time** ‚Äì Sleeping, benchmarking, and timing processes.\n",
        "* **os** ‚Äì Path handling, checking directories, cleaning files.\n",
        "* **random** ‚Äì Random selection or sampling of papers.\n",
        "* **string** ‚Äì Sanitizing filenames and keys.\n",
        "* **logging** ‚Äì Suppressing or handling logs cleanly (`arxiv` library can be noisy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYwDXFE2ZFrW",
        "outputId": "9ce77129-4535-4071-8bf4-12e2ca22c20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv requests psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a-Yg8486Y_d4"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.getLogger(\"arxiv\").setLevel(logging.ERROR)\n",
        "import arxiv\n",
        "import re\n",
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "import string\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import threading\n",
        "import queue\n",
        "import psutil\n",
        "import random\n",
        "import shutil\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGfK3YkzbPOz"
      },
      "source": [
        "# **I. Overview: arXiv ID discovery and range utilities**\n",
        "\n",
        "This module provides a set of functions for generating, validating, and enumerating arXiv IDs within a given date range. arXiv uses the format `YYMM.NNNNN`, where `NNNNN` is an incrementing paper number starting from `00001` each month. Since arXiv does not provide an API to directly list all valid IDs, the code implements efficient search techniques to infer the boundaries of valid IDs.\n",
        "\n",
        "---\n",
        "\n",
        "## **I.1. ID Generation**\n",
        "\n",
        "### `get_ID(month, year, number)`\n",
        "\n",
        "Generates an arXiv ID in the form `YYMM.NNNNN`.\n",
        "\n",
        "* `year % 100` extracts the last two digits of the year\n",
        "* `month:02d` ensures the month is zero-padded\n",
        "* `number:05d` produces a five-digit paper index\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "get_ID(4, 2023, 12)  # \"2304.00012\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **I.2. Checking whether an arXiv ID exists or not**\n",
        "\n",
        "### `id_exists(paper_id)`\n",
        "\n",
        "Determines whether a specific arXiv ID corresponds to a real paper by performing a lookup using the arXiv API.\n",
        "\n",
        "Behavior:\n",
        "\n",
        "* Returns `True` if metadata for the ID is found\n",
        "* Returns `False` if:\n",
        "\n",
        "  * The ID does not exist\n",
        "  * The API returns no results\n",
        "  * Any network or parsing error occurs\n",
        "\n",
        "This function is the foundation for locating valid ID boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "## **I.3. Locating the first valid ID of a month**\n",
        "\n",
        "### `find_first_id(year, month)`\n",
        "\n",
        "Finds the first valid ID for a given month using a two-stage search strategy:\n",
        "\n",
        "1. **Exponential search**\n",
        "   Doubles the index (`high = 1, 2, 4, 8, ...`) until it finds a valid ID.\n",
        "\n",
        "2. **Binary search**\n",
        "   Narrows the range between `low` and `high` to identify the smallest existing index.\n",
        "\n",
        "\n",
        "Returns:\n",
        "\n",
        "* The earliest valid index\n",
        "* `None` if the month contains no papers\n",
        "\n",
        "---\n",
        "\n",
        "## **I.4. Locating the last valid ID of a month**\n",
        "\n",
        "### `find_last_id(year, month)`\n",
        "\n",
        "Finds the last valid ID for a month using an approach similar to `find_first_id`:\n",
        "\n",
        "1. **Exponential search**\n",
        "   Increases the index until encountering a missing (nonexistent) ID.\n",
        "\n",
        "2. **Binary search**\n",
        "   Determines the highest index that still corresponds to a real paper.\n",
        "\n",
        "Returns:\n",
        "\n",
        "* The last valid index\n",
        "* `99999` if the search reaches the maximum allowed numeric range\n",
        "\n",
        "---\n",
        "\n",
        "## **I.5. Listing IDs within a single month**\n",
        "\n",
        "### `get_IDs_month(month, year, start_number, end_number)`\n",
        "\n",
        "Generates all IDs between `start_number` and `end_number` for the specified month.\n",
        "\n",
        "Output format:\n",
        "\n",
        "```\n",
        "[\"YYMM.00001\", \"YYMM.00002\", ...]\n",
        "```\n",
        "\n",
        "Assumes the caller has already determined valid start and end indices.\n",
        "\n",
        "---\n",
        "\n",
        "## **I.6. Generating IDs across multiple months**\n",
        "\n",
        "### `get_IDs_All(start_month, start_year, start_ID, end_month, end_year, end_ID)`\n",
        "\n",
        "Constructs a complete list of all arXiv IDs between two points in time, even when the range spans multiple months or years.\n",
        "\n",
        "Method:\n",
        "\n",
        "1. For each month in the range:\n",
        "\n",
        "   * Determine the ending index:\n",
        "\n",
        "     * If the month is the last one: use `end_ID`\n",
        "     * Otherwise: detect automatically using `find_last_id`\n",
        "   * Collect all IDs for that month using `get_IDs_month`\n",
        "2. Move to the next month, adjusting the year when necessary.\n",
        "3. Reset the starting index using `find_first_id` for each new month.\n",
        "\n",
        "This allows flexible queries, for example from March 2022 (starting at ID 2000) to July 2023 (ending at ID 0500).\n",
        "\n",
        "---\n",
        "\n",
        "## **I.7. Formatting arXiv IDs for filenames or keys**\n",
        "\n",
        "### `format_arxiv_id_for_key(arxiv_id)`\n",
        "\n",
        "Transforms an arXiv ID from:\n",
        "\n",
        "```\n",
        "YYMM.NNNNN\n",
        "```\n",
        "\n",
        "to:\n",
        "\n",
        "```\n",
        "yyyymm-NNNNN\n",
        "```\n",
        "\n",
        "This format is often more convenient for filenames, folders, or BibTeX keys.\n",
        "\n",
        "Examples:\n",
        "\n",
        "```\n",
        "2304.07856 ‚Üí 202304-07856\n",
        "1912.00123 ‚Üí 201912-00123\n",
        "```\n",
        "\n",
        "If the input does not match the expected format, the function returns it unchanged.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z0XD_OH8ZAKh"
      },
      "outputs": [],
      "source": [
        "def get_ID(month, year, number):\n",
        "    \"\"\"Return arXiv ID in YYMM.NNNNN format.\"\"\"\n",
        "    return f\"{year % 100:02d}{month:02d}.{number:05d}\"\n",
        "\n",
        "def id_exists(paper_id):\n",
        "    \"\"\"Check if a specific arXiv ID exists.\"\"\"\n",
        "    search = arxiv.Search(id_list=[paper_id])\n",
        "    client = arxiv.Client(page_size=1, delay_seconds=0.2)\n",
        "    try:\n",
        "        next(client.results(search))\n",
        "        return True\n",
        "    except StopIteration:\n",
        "        return False\n",
        "    except Exception:\n",
        "        # Network or parsing error ‚Äî assume not found for safety\n",
        "        return False\n",
        "\n",
        "def find_first_id(year, month):\n",
        "    \"\"\"Find the first valid arXiv ID of a given month using exponential + binary search.\"\"\"\n",
        "    low, high = 1, 1\n",
        "    # Exponential search upward until we find a valid ID\n",
        "    while not id_exists(get_ID(month, year, high)):\n",
        "        high *= 2\n",
        "        if high > 99999:\n",
        "            return None  # No valid papers this month\n",
        "\n",
        "    # Now binary search between low and high to find the *first* valid ID\n",
        "    while low + 1 < high:\n",
        "        mid = (low + high) // 2\n",
        "        if id_exists(get_ID(month, year, mid)):\n",
        "            high = mid\n",
        "        else:\n",
        "            low = mid\n",
        "    return high\n",
        "\n",
        "def find_last_id(year, month):\n",
        "    \"\"\"Find the last valid arXiv ID of a given month.\"\"\"\n",
        "    low, high = 1, 1\n",
        "    # Exponential search upward until we find a missing ID\n",
        "    while id_exists(get_ID(month, year, high)):\n",
        "        high *= 2\n",
        "        if high > 99999:\n",
        "            return 99999\n",
        "    low = high // 2\n",
        "    # Binary search for the last existing ID\n",
        "    while low + 1 < high:\n",
        "        mid = (low + high) // 2\n",
        "        if id_exists(get_ID(month, year, mid)):\n",
        "            low = mid\n",
        "        else:\n",
        "            high = mid\n",
        "    return low\n",
        "\n",
        "def get_IDs_month(month, year, start_number, end_number):\n",
        "    \"\"\"Get all valid arXiv IDs in a given month.\"\"\"\n",
        "    return [get_ID(month, year, i) for i in range(start_number, end_number + 1)]\n",
        "\n",
        "def get_IDs_All(start_month, start_year, start_ID, end_month, end_year, end_ID):\n",
        "    \"\"\"Get all valid arXiv IDs in the given range.\"\"\"\n",
        "    ids = []\n",
        "    y, m = start_year, start_month\n",
        "    n_start = start_ID\n",
        "    n_end = None\n",
        "    while True:\n",
        "        if y == end_year and m == end_month:\n",
        "            n_end = end_ID\n",
        "        else:\n",
        "            n_end = find_last_id(y, m)\n",
        "            if n_end is None:\n",
        "                n_end = 0  # No papers this month\n",
        "\n",
        "        if n_start <= n_end:\n",
        "            ids.extend(get_IDs_month(m, y, n_start, n_end))\n",
        "\n",
        "        if y == end_year and m == end_month:\n",
        "            break\n",
        "        m += 1\n",
        "        if m > 12:\n",
        "            m, y = 1, y + 1\n",
        "        n_start = find_first_id(y, m)  # reset numbering\n",
        "\n",
        "    return ids\n",
        "\n",
        "def format_arxiv_id_for_key(arxiv_id):\n",
        "    \"\"\"\n",
        "    Convert arXiv ID from YYMM.NNNNN format to yyyymm-id format.\n",
        "    Examples:\n",
        "        2304.07856 -> 202304-07856\n",
        "        1912.00123 -> 201912-00123\n",
        "    \"\"\"\n",
        "    match = re.match(r'^(\\d{2})(\\d{2})\\.(\\d{5})$', arxiv_id)\n",
        "    if match:\n",
        "        yy, mm, id_num = match.groups()\n",
        "        # Convert YY to YYYY (assuming 20YY for papers after 2000)\n",
        "        yyyy = f\"20{yy}\"\n",
        "        return f\"{yyyy}{mm}-{id_num}\"\n",
        "    return arxiv_id  # Return as-is if format doesn't match\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrkZBK9sbwdE"
      },
      "source": [
        "# **II. Metadata extraction and storage**\n",
        "\n",
        "This section describes the functions responsible for converting an `arxiv.Result` object into structured metadata and saving it in JSON format. These utilities help standardize how each downloaded paper is documented, including its versions, authors, categories, and other descriptive attributes.\n",
        "\n",
        "---\n",
        "## **II.1. Converting an arXiv Result into metadata**\n",
        "\n",
        "### `create_metadata(paper)`\n",
        "\n",
        "Transforms an `arxiv.Result` object into a dictionary containing relevant metadata fields.\n",
        "\n",
        "Key steps:\n",
        "\n",
        "1. **Extract base ID and version**\n",
        "   `paper.get_short_id()` returns values like `2305.00633v4`.\n",
        "\n",
        "   * The function separates the base ID (`2305.00633`)\n",
        "   * And the version number (`4`)\n",
        "\n",
        "2. **Generate PDF URLs**\n",
        "\n",
        "   * If the paper has multiple versions, PDF URLs for all versions are created\n",
        "   * Otherwise, only the current version URL is included\n",
        "\n",
        "3. **Construct the metadata dictionary**, which includes:\n",
        "\n",
        "   * `arxiv_id`: base ID without version suffix\n",
        "   * `paper_title`: title string\n",
        "   * `authors`: list of author names\n",
        "   * `submission_date`: original publication date\n",
        "   * `revised_dates`: list with updated dates (empty if only one version)\n",
        "   * `latest_version`: integer version number\n",
        "   * `categories`: classification tags\n",
        "   * `abstract`: cleaned summary\n",
        "   * `pdf_urls`: one or more PDF download URLs\n",
        "\n",
        "4. **Optional fields**\n",
        "\n",
        "   * `publication_venue` is filled if `paper.comment` is present\n",
        "   * `doi` is included if available\n",
        "\n",
        "The function returns the full metadata dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "## **II.2. Saving metadata**\n",
        "\n",
        "### `save_metadata(paper, folder)`\n",
        "\n",
        "Stores the metadata for a single paper into a `metadata.json` file located in the specified folder.\n",
        "\n",
        "Steps performed:\n",
        "\n",
        "1. Calls `create_metadata(paper)` to generate the metadata dictionary.\n",
        "2. Ensures the target folder exists (creates it if necessary).\n",
        "3. Writes the metadata to `metadata.json` using UTF-8 encoding and readable indentation.\n",
        "4. Prints a message indicating the save location.\n",
        "5. Returns the metadata dictionary for further use.\n",
        "\n",
        "This function ensures that each paper‚Äôs descriptive information is saved consistently alongside its downloaded files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4BMOwWqrZJKx"
      },
      "outputs": [],
      "source": [
        "def create_metadata(paper):\n",
        "    \"\"\"\n",
        "    Convert an arxiv.Result object into a metadata dictionary.\n",
        "    \"\"\"\n",
        "    arxiv_id = paper.get_short_id()         # e.g. '2305.00633v4'\n",
        "    base_id = arxiv_id.split('v')[0]        # e.g. '2305.00633'\n",
        "    version = int(arxiv_id.split('v')[1])   # e.g. 4\n",
        "\n",
        "    # Generate all version URLs if version > 1\n",
        "    if version > 1:\n",
        "        pdf_urls = [f\"http://arxiv.org/pdf/{base_id}v{i}\" for i in range(1, version + 1)]\n",
        "    else:\n",
        "        pdf_urls = [f\"http://arxiv.org/pdf/{arxiv_id}\"]\n",
        "\n",
        "    metadata = {\n",
        "        \"arxiv_id\": base_id,\n",
        "        \"paper_title\": paper.title.strip(),\n",
        "        \"authors\": [author.name for author in paper.authors],\n",
        "        \"submission_date\": paper.published.strftime(\"%Y-%m-%d\"),\n",
        "        \"revised_dates\": [\n",
        "            paper.updated.strftime(\"%Y-%m-%d\")\n",
        "        ] if paper.updated != paper.published else [],\n",
        "        \"latest_version\": version,\n",
        "        \"categories\": paper.categories,\n",
        "        \"abstract\": paper.summary.strip(),\n",
        "        \"pdf_urls\": pdf_urls\n",
        "    }\n",
        "\n",
        "    # Optional metadata fields\n",
        "    if paper.comment:\n",
        "        metadata[\"publication_venue\"] = paper.comment.strip()\n",
        "    else:\n",
        "        metadata[\"publication_venue\"] = None\n",
        "\n",
        "    if paper.doi:\n",
        "        metadata[\"doi\"] = paper.doi\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def save_metadata(paper, folder):\n",
        "    \"\"\"\n",
        "    Save metadata.json for a single paper into the given folder.\n",
        "    \"\"\"\n",
        "    metadata = create_metadata(paper)\n",
        "\n",
        "    folder_path = os.path.abspath(folder)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "    save_path = os.path.join(folder_path, \"metadata.json\")\n",
        "\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"üíæ Saved metadata to {save_path}\")\n",
        "    return metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xeT_r1ocBG3"
      },
      "source": [
        "# **III. Utility functions for handling arXiv source downloads**\n",
        "\n",
        "This module contains helper functions for preparing valid filenames, safely extracting `.tar.gz` source archives, downloading files from arXiv, cleaning extracted folders, and orchestrating the full download process for all versions of a paper. These functions work together to ensure that downloaded source code is stored in a consistent, safe, and organized folder structure.\n",
        "\n",
        "---\n",
        "\n",
        "## **III.1. Formatting and Sanitization**\n",
        "\n",
        "### `format_yymm_id(base_id: str) -> str`\n",
        "\n",
        "Converts an arXiv base ID from:\n",
        "\n",
        "```\n",
        "2303.07856\n",
        "```\n",
        "\n",
        "to:\n",
        "\n",
        "```\n",
        "2303-07856\n",
        "```\n",
        "\n",
        "\n",
        "### `sanitize_filename(name: str) -> str`\n",
        "\n",
        "Cleans a path or filename by replacing any unsafe characters with underscores.\n",
        "Allowed characters include alphanumeric characters, underscores, hyphens, dots, and slashes.\n",
        "\n",
        "The purpose is to prevent:\n",
        "\n",
        "* Invalid filesystem paths\n",
        "* Accidental directory traversal\n",
        "* Issues caused by special characters in filenames\n",
        "\n",
        "---\n",
        "\n",
        "## **III.2. Safe extraction of tar.gz files**\n",
        "\n",
        "### `safe_extract_tar(tar_path: str, extract_to: str) -> None`\n",
        "\n",
        "Extracts a `.tar.gz` archive while applying safety checks:\n",
        "\n",
        "* Rejects symbolic links and hard links\n",
        "* Rejects absolute paths\n",
        "* Rejects any entry containing `..`\n",
        "* Sanitizes filenames before extraction\n",
        "* Attempts to extract using `tar.extract(..., filter=\"data\")`\n",
        "* Creates necessary directories before writing files\n",
        "* Skips any broken or unreadable entries rather than stopping the entire process\n",
        "\n",
        "If an unrecoverable error occurs, it prints a message but does not raise an exception.\n",
        "\n",
        "This ensures source archives extracted from arXiv cannot overwrite unintended paths or create unsafe filesystem structures.\n",
        "\n",
        "---\n",
        "\n",
        "## **III.3. Downloading remote files**\n",
        "\n",
        "### `download_url(url: str, out_path: str) -> bool`\n",
        "\n",
        "Downloads a file from a given URL and streams it to disk in chunks.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "* Uses a custom User-Agent\n",
        "* Streams data to avoid high memory usage\n",
        "* Ensures the output directory exists\n",
        "* Returns `True` if the HTTP response is OK (`200`)\n",
        "* Otherwise prints an error and returns `False`\n",
        "\n",
        "This is a minimal downloader without retry or backoff logic.\n",
        "\n",
        "---\n",
        "\n",
        "## **III.4. Cleaning extracted source files**\n",
        "\n",
        "### `cleanup_non_tex_bib_files(folder: str) -> None`\n",
        "\n",
        "Walks through the extracted directory and removes every file except:\n",
        "\n",
        "* `.tex` files\n",
        "* `.bib` files\n",
        "\n",
        "This keeps only the essential LaTeX source code and bibliography files, ignoring figures or auxiliary files.\n",
        "\n",
        "---\n",
        "\n",
        "## **III.5. Full download procedure for all versions**\n",
        "\n",
        "### `download(list_download: list, base_dir: str) -> None`\n",
        "\n",
        "Coordinates the process of downloading all available versions of an arXiv paper, extracting source files, cleaning them, and saving metadata.\n",
        "\n",
        "Main steps:\n",
        "\n",
        "1. **Validate input**\n",
        "\n",
        "   * Ensures the list of results is not empty\n",
        "   * Extracts the base ID using regex (four digits, dot, five digits)\n",
        "\n",
        "2. **Prepare folder structure**\n",
        "\n",
        "   * Creates a directory named after the paper (using `YYMM-NNNNN`)\n",
        "   * Creates a `tex` subfolder\n",
        "   * Creates per-version subfolders inside `tex`\n",
        "\n",
        "3. **Download each version**\n",
        "   For each paper version:\n",
        "\n",
        "   * Build the `/src/{id}` URL\n",
        "   * Download the corresponding `.tar.gz` archive\n",
        "   * Validate that it is a real tar archive\n",
        "   * Extract it safely using `safe_extract_tar`\n",
        "   * Clean non-LaTeX files\n",
        "   * Remove the `.tar.gz` after extraction\n",
        "\n",
        "4. **Save metadata**\n",
        "\n",
        "   * After all versions have been processed, calls `save_metadata`\n",
        "   * Stores a `metadata.json` file in the paper‚Äôs root folder\n",
        "\n",
        "Errors encountered during any stage are reported but do not stop the overall process unless critical information is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u7E4VJMxZMrz"
      },
      "outputs": [],
      "source": [
        "ARXIV_HOST = \"https://arxiv.org\"\n",
        "\n",
        "def format_yymm_id(base_id: str) -> str:\n",
        "    \"\"\"'2303.07856' -> '2303-07856'\"\"\"\n",
        "    return base_id.replace('.', '-')\n",
        "\n",
        "def sanitize_filename(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace unsafe characters and limit path depth to avoid errors.\n",
        "    Keeps only alphanumeric, underscores, hyphens, dots, and slashes.\n",
        "    \"\"\"\n",
        "    safe_chars = f\"-_.{string.ascii_letters}{string.digits}/\"\n",
        "    return ''.join(c if c in safe_chars else '_' for c in name)\n",
        "\n",
        "def safe_extract_tar(tar_path: str, extract_to: str) -> None:\n",
        "    \"\"\"Safely extract a tar.gz file using 'filter=data', skipping broken entries.\"\"\"\n",
        "    try:\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            for member in tar.getmembers():\n",
        "                try:\n",
        "                    # Skip symbolic links and absolute paths (security)\n",
        "                    if member.islnk() or member.issym() or member.name.startswith(\"/\") or \"..\" in member.name:\n",
        "                        continue\n",
        "\n",
        "                    member.name = sanitize_filename(member.name)\n",
        "                    target_path = os.path.join(extract_to, member.name)\n",
        "                    target_dir = os.path.dirname(target_path)\n",
        "                    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "                    # Extract safely\n",
        "                    tar.extract(member, path=extract_to, filter=\"data\")\n",
        "                except (FileNotFoundError, OSError, tarfile.ExtractError) as inner_e:\n",
        "                    print(f\"‚ö†Ô∏è Skipped bad entry in {os.path.basename(tar_path)}: {member.name} ({inner_e})\")\n",
        "                    continue\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] Extraction failed for {tar_path}: {e}\")\n",
        "\n",
        "\n",
        "def download_url(url: str, out_path: str) -> bool:\n",
        "    \"\"\"Basic downloader (no retry, no backoff).\"\"\"\n",
        "    headers = {\"User-Agent\": \"arxiv-downloader/1.0 (+https://github.com/AnhTtis/Data_Science_Project)\"}\n",
        "    try:\n",
        "        with requests.get(url, headers=headers, stream=True, timeout=30) as r:\n",
        "            if r.status_code == 200:\n",
        "                os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "                with open(out_path, \"wb\") as f:\n",
        "                    for chunk in r.iter_content(8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                return True\n",
        "            print(f\"HTTP {r.status_code} for {url}\")\n",
        "            return False\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Download failed for {url}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def cleanup_non_tex_bib_files(folder: str) -> None:\n",
        "    \"\"\"Remove all non-.tex and non-.bib files.\"\"\"\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if not (file.endswith(\".tex\") or file.endswith(\".bib\")):\n",
        "                try:\n",
        "                    os.remove(os.path.join(root, file))\n",
        "                except OSError as e:\n",
        "                    print(f\"Warning: could not remove {file}: {e}\")\n",
        "\n",
        "\n",
        "def download(list_download: list, base_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Downloads all versions of an arXiv paper using /src/{id} URL.\n",
        "    Extracts .tex/.bib files and saves metadata.\n",
        "    \"\"\"\n",
        "    if not list_download:\n",
        "        print(\"‚ö†Ô∏è list_download is empty ‚Äî skipping.\")\n",
        "        return\n",
        "\n",
        "    match = re.match(r\"^(\\d{4}\\.\\d{5})\", list_download[0].get_short_id())\n",
        "    if not match:\n",
        "        print(f\"Invalid arXiv ID format: {list_download[0].get_short_id()}\")\n",
        "        return\n",
        "\n",
        "    arxiv_id = match.group(1)\n",
        "    folder_arxiv = os.path.join(base_dir, format_yymm_id(arxiv_id))\n",
        "    print(f\"Processing {arxiv_id} ‚Üí {folder_arxiv}\")\n",
        "\n",
        "    os.makedirs(folder_arxiv, exist_ok=True)\n",
        "    tex_root = os.path.join(folder_arxiv, \"tex\")\n",
        "    os.makedirs(tex_root, exist_ok=True)\n",
        "\n",
        "    for result in list_download:\n",
        "        full_id = result.get_short_id()  # e.g. '2305.00633v4'\n",
        "        folder_version = os.path.join(tex_root, full_id)  # put all versions under .../<paper>/tex/<version>\n",
        "        os.makedirs(folder_version, exist_ok=True)\n",
        "\n",
        "        src_url = f\"{ARXIV_HOST}/src/{full_id}\"\n",
        "        tar_path = os.path.join(folder_version, f\"{full_id}.tar.gz\")\n",
        "        print(f\"Attempting source: {src_url}\")\n",
        "\n",
        "        if not download_url(src_url, tar_path):\n",
        "            print(f\"Source unavailable for {full_id}\")\n",
        "            continue\n",
        "\n",
        "        # Validate and extract\n",
        "        if not tarfile.is_tarfile(tar_path):\n",
        "            print(f\"Invalid tar archive for {full_id}. Removing file.\")\n",
        "            try:\n",
        "                os.remove(tar_path)\n",
        "            except OSError as e:\n",
        "                print(f\"Could not remove invalid file {tar_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            safe_extract_tar(tar_path, folder_version)\n",
        "            cleanup_non_tex_bib_files(folder_version)\n",
        "            print(f\"‚úÖ Extracted to {folder_version}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Extraction failed for {full_id}: {e}\")\n",
        "        finally:\n",
        "            try:\n",
        "                os.remove(tar_path)\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "    # Save metadata after all versions\n",
        "    try:\n",
        "        save_metadata(result, folder_arxiv)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Metadata save failed for {arxiv_id}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAtYCRuZcKsp"
      },
      "source": [
        "# **IV. Semantic Scholar reference extraction module**\n",
        "\n",
        "This module handles fetching citation references from the Semantic Scholar Graph API, converting them into structured metadata, and saving them in JSON format. It includes a rate limiter to ensure API compliance, helper functions for converting IDs, and utilities for preparing a standardized references dataset for each arXiv paper.\n",
        "\n",
        "---\n",
        "\n",
        "## **IV.1. Rate limiting for API requests**\n",
        "\n",
        "### `wait_for_rate_limit()`\n",
        "\n",
        "Ensures that at least one second passes between requests.\n",
        "A global timestamp `_last_request_time` is shared across threads, protected by a lock:\n",
        "\n",
        "* Prevents exceeding Semantic Scholar‚Äôs free-tier request limits\n",
        "* Enforces a fixed delay before each new API call\n",
        "\n",
        "This is critical for multi-threaded reference extraction.\n",
        "\n",
        "---\n",
        "\n",
        "## **IV.2. Fetching references from Semantic Scholar**\n",
        "\n",
        "### `get_paper_references(arxiv_id, delay=1)`\n",
        "\n",
        "Retrieves reference metadata for a given arXiv paper from the Semantic Scholar API.\n",
        "\n",
        "Workflow:\n",
        "\n",
        "1. **Normalize the ID** by removing version suffix (`v1`, `v2`, etc.).\n",
        "2. Build the API endpoint:\n",
        "   `https://api.semanticscholar.org/graph/v1/paper/arXiv:{id}/references`\n",
        "3. Use `fields=` to request:\n",
        "\n",
        "   * title\n",
        "   * authors\n",
        "   * year\n",
        "   * venue\n",
        "   * externalIds (ArXiv, DOI)\n",
        "   * publicationDate\n",
        "4. Apply strict rate limiting before each request.\n",
        "5. Handle error conditions:\n",
        "\n",
        "   * `429`: wait and retry\n",
        "   * `404`: return an empty list\n",
        "   * other errors: retry with delay\n",
        "6. Return a list of reference entries, or an empty list on failure.\n",
        "\n",
        "---\n",
        "\n",
        "## **IV.3. Converting Semantic Scholar data to standard format**\n",
        "\n",
        "### `convert_to_references_dict(references)`\n",
        "\n",
        "Transforms API results into a dictionary indexed by normalized IDs.\n",
        "\n",
        "Key behaviors:\n",
        "\n",
        "* Extract `citedPaper` from each reference.\n",
        "* Skip invalid or missing entries.\n",
        "* Focus primarily on references **with an arXiv ID**.\n",
        "* Use `format_yymm_id()` to convert IDs such as `2304.07856` ‚Üí `2304-07856`.\n",
        "\n",
        "Metadata fields created per reference:\n",
        "\n",
        "* `title`\n",
        "* `authors`\n",
        "* `submission_date` (from publication date or constructed from year)\n",
        "* `revised_dates` (empty, since Semantic Scholar does not track versions)\n",
        "* optional:\n",
        "\n",
        "  * `doi`\n",
        "  * `arxiv_id`\n",
        "  * `venue`\n",
        "  * `year`\n",
        "\n",
        "This produces a clean dictionary suitable for storage as JSON.\n",
        "\n",
        "---\n",
        "\n",
        "## **IV.4. Saving references**\n",
        "\n",
        "### `save_references(arxiv_id, paper_folder, verbose=True)`\n",
        "\n",
        "Retrieves references for a specific arXiv ID and writes them to:\n",
        "\n",
        "```\n",
        "<folder>/references.json\n",
        "```\n",
        "\n",
        "Process:\n",
        "\n",
        "1. Create the target folder if missing.\n",
        "2. Fetch references via `get_paper_references`.\n",
        "3. Convert results to normalized metadata via `convert_to_references_dict`.\n",
        "4. Save the dictionary as JSON using UTF-8 encoding and pretty indentation.\n",
        "\n",
        "If no references are found, the file is still created but contains an empty dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "## **IV.5. Extracting references**\n",
        "\n",
        "### `extract_references_for_paper(paper_id, base_data_dir=\"../data\")`\n",
        "\n",
        "Handles reference extraction for all versions of a paper.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Normalize the paper folder name using `format_yymm_id()`.\n",
        "2. Build the paper‚Äôs directory path under `base_data_dir`.\n",
        "3. Call `save_references()` to fetch and store reference information.\n",
        "\n",
        "This function is designed to integrate with your main download pipeline so that downloading a paper and extracting its references can be performed in a consistent workflow.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qMcMMALRZU9m"
      },
      "outputs": [],
      "source": [
        "# Semantic Scholar API Key\n",
        "API_KEY = \"bsPmoNlOlZ3wqxmgirXF45aTWNbqAelr3ldxGRbu\"\n",
        "\n",
        "# Rate limiter: 1 request per second across all threads\n",
        "_rate_limit_lock = threading.Lock()\n",
        "_last_request_time = 0\n",
        "\n",
        "def wait_for_rate_limit():\n",
        "    \"\"\"Ensure at least 1 second has passed since last API call\"\"\"\n",
        "    global _last_request_time\n",
        "    with _rate_limit_lock:\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - _last_request_time\n",
        "        if time_since_last < 1.0:\n",
        "            sleep_time = 1.0 - time_since_last\n",
        "            time.sleep(sleep_time)\n",
        "        _last_request_time = time.time()\n",
        "\n",
        "def get_paper_references(arxiv_id, delay=1):\n",
        "    \"\"\"\n",
        "    Fetch references for a paper from Semantic Scholar API.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID (format: YYMM.NNNNN or YYMM.NNNNNvN)\n",
        "        retry: number of retry attempts\n",
        "        delay: delay between retries in seconds\n",
        "\n",
        "    Returns:\n",
        "        list: List of references with detailed information\n",
        "    \"\"\"\n",
        "    # Clean arxiv_id (remove version suffix if present)\n",
        "    clean_id = re.sub(r'v\\d+$', '', arxiv_id)\n",
        "    url = f\"https://api.semanticscholar.org/graph/v1/paper/arXiv:{clean_id}/references\"\n",
        "    params = {\n",
        "        \"fields\": \"title,authors,year,venue,externalIds,publicationDate\"\n",
        "    }\n",
        "    headers = {\n",
        "        \"x-api-key\": API_KEY,\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Wait to respect rate limit before making request\n",
        "            wait_for_rate_limit()\n",
        "\n",
        "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                # API tr·∫£ v·ªÅ {\"data\": [list of references]}\n",
        "                return data.get(\"data\", [])\n",
        "            elif response.status_code == 429:\n",
        "                print(f\"  Rate limit hit. Waiting {delay}s before retry...\")\n",
        "                time.sleep(delay)\n",
        "            elif response.status_code == 404:\n",
        "                print(f\"  Paper {arxiv_id} not found in Semantic Scholar\")\n",
        "                return []\n",
        "            else:\n",
        "                print(f\"  API returned status {response.status_code}, retrying in {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  Request error: {e}, retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "\n",
        "def convert_to_references_dict(references):\n",
        "    \"\"\"\n",
        "    Convert Semantic Scholar references to the required format:\n",
        "    Dictionary with arXiv IDs as keys (in \"yyyymm-id\" format) for papers with arXiv IDs.\n",
        "    For papers without arXiv IDs, use DOI or generate a unique key.\n",
        "\n",
        "    Args:\n",
        "        references: List of references from Semantic Scholar API\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with paper IDs as keys and metadata as values\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    non_arxiv_counter = 1\n",
        "\n",
        "    for ref in references:\n",
        "        # Extract citedPaper from the reference object\n",
        "        cited_paper = ref.get(\"citedPaper\", {})\n",
        "\n",
        "        # Skip if citedPaper is None or empty\n",
        "        if not cited_paper:\n",
        "            continue\n",
        "\n",
        "        # Extract external IDs (may be None)\n",
        "        external_ids = cited_paper.get(\"externalIds\", {})\n",
        "        if external_ids is None:\n",
        "            external_ids = {}\n",
        "\n",
        "        arxiv_id = external_ids.get(\"ArXiv\", \"\")\n",
        "        doi = external_ids.get(\"DOI\", \"\")\n",
        "        # Only keep references that have arXiv_id\n",
        "        if not arxiv_id:\n",
        "            continue\n",
        "\n",
        "        # Determine the key for this reference\n",
        "        if arxiv_id:\n",
        "            # Use arXiv ID in yyyymm-id format\n",
        "            key = format_yymm_id(arxiv_id)\n",
        "        elif doi:\n",
        "            # Use DOI as key (sanitize it)\n",
        "            key = f\"doi:{doi.replace('/', '_')}\"\n",
        "        else:\n",
        "            # Generate a unique key for papers without arXiv ID or DOI\n",
        "            title = cited_paper.get(\"title\", \"\")\n",
        "            if title:\n",
        "                # Use first word of title + counter\n",
        "                first_word = re.sub(r'[^\\w]', '', title.split()[0] if title.split() else \"unknown\")\n",
        "                key = f\"ref_{first_word[:20]}_{non_arxiv_counter}\"\n",
        "            else:\n",
        "                key = f\"ref_unknown_{non_arxiv_counter}\"\n",
        "            non_arxiv_counter += 1\n",
        "\n",
        "        # Extract authors\n",
        "        authors_list = cited_paper.get(\"authors\", [])\n",
        "        authors = [author.get(\"name\", \"\") for author in authors_list if author.get(\"name\")]\n",
        "\n",
        "        # Extract dates (use publicationDate if available)\n",
        "        publication_date = cited_paper.get(\"publicationDate\", \"\")\n",
        "        year = cited_paper.get(\"year\")\n",
        "\n",
        "        # If no publication date but have year, create an ISO-like format\n",
        "        if not publication_date and year:\n",
        "            publication_date = f\"{year}-01-01\"  # Use Jan 1st as placeholder\n",
        "\n",
        "        # Build metadata dictionary with required fields\n",
        "        metadata = {\n",
        "            \"title\": cited_paper.get(\"title\", \"\"),\n",
        "            \"authors\": authors,\n",
        "            \"submission_date\": publication_date if publication_date else \"\",\n",
        "            \"revised_dates\": []  # Semantic Scholar doesn't provide revision history\n",
        "        }\n",
        "\n",
        "        # Add optional fields for reference\n",
        "        if doi:\n",
        "            metadata[\"doi\"] = doi\n",
        "        if arxiv_id:\n",
        "            metadata[\"arxiv_id\"] = arxiv_id\n",
        "        if cited_paper.get(\"venue\"):\n",
        "            metadata[\"venue\"] = cited_paper.get(\"venue\")\n",
        "        if year:\n",
        "            metadata[\"year\"] = year\n",
        "\n",
        "        result[key] = metadata\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def save_references(arxiv_id, paper_folder, verbose=True):\n",
        "    \"\"\"\n",
        "    Fetch and save references for a paper version to both JSON and BibTeX formats.\n",
        "\n",
        "    Args:\n",
        "        arxiv_id: arXiv ID (e.g., \"2304.07856v1\")\n",
        "        version_folder: Path to version folder (e.g., \"data/2304.07856/v1/\")\n",
        "        verbose: Whether to print progress messages\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    # Check if the folder exists, if not, create it\n",
        "    if not os.path.exists(paper_folder):\n",
        "        os.makedirs(paper_folder, exist_ok=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Fetching references for {arxiv_id}...\")\n",
        "\n",
        "    references = get_paper_references(arxiv_id)\n",
        "\n",
        "    if not references:\n",
        "        if verbose:\n",
        "            print(f\"  No references found for {arxiv_id}\")\n",
        "        json_path = os.path.join(paper_folder, \"references.json\")\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump({}, f, indent=2, ensure_ascii=False)\n",
        "        return False\n",
        "\n",
        "    json_path = os.path.join(paper_folder, \"references.json\")\n",
        "    references_dict = convert_to_references_dict(references)\n",
        "    try:\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(references_dict, f, indent=2, ensure_ascii=False)\n",
        "        if verbose:\n",
        "            print(f\"  Saved {len(references_dict)} references to references.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error saving JSON: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def extract_references_for_paper(paper_id, base_data_dir=\"../data\"):\n",
        "    \"\"\"\n",
        "    Extract references for all versions of a paper.\n",
        "\n",
        "    Args:\n",
        "        paper_id: arXiv paper ID without version (e.g., \"2304.07856\")\n",
        "        base_data_dir: Base directory containing data folders\n",
        "\n",
        "    Returns:\n",
        "        dict: Statistics about the extraction\n",
        "    \"\"\"\n",
        "    paper_id_key = format_yymm_id(paper_id)\n",
        "    paper_folder = os.path.join(base_data_dir, paper_id_key)\n",
        "\n",
        "    save_references(paper_id, os.path.join(paper_folder))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkfZXnYVUAK8"
      },
      "source": [
        "# **V.Performance monitoring utilities**\n",
        "\n",
        "This module provides tools for monitoring RAM usage, disk consumption, and runtime performance during long-running data-processing pipelines. It includes helpers for measuring memory usage, estimating disk footprint, and a `Benchmark` class that aggregates performance metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## **V.1. RAM & Disk helpers**\n",
        "\n",
        "These functions allow the pipeline to track memory and storage usage accurately.\n",
        "\n",
        "\n",
        "### **`ram_process_mb()`**\n",
        "\n",
        "Returns the memory usage **of the current Python process** in megabytes.\n",
        "\n",
        "* Uses `psutil.Process(os.getpid())` to access the running process.\n",
        "* Retrieves `rss` (Resident Set Size):\n",
        "  the actual memory held in RAM.\n",
        "* Converts bytes ‚Üí megabytes (`1024**2`).\n",
        "\n",
        "\n",
        "### **`ram_system_mb()`**\n",
        "\n",
        "Returns the **total RAM used by the operating system**, not just Python.\n",
        "\n",
        "* Reads `/proc/meminfo`\n",
        "* Extracts:\n",
        "\n",
        "  * `MemTotal`\n",
        "  * `MemFree`\n",
        "  * `Buffers`\n",
        "  * `Cached`\n",
        "* Computes:\n",
        "  **used = MemTotal ‚àí (Free + Buffers + Cached)**\n",
        "  Convert to MB.\n",
        "\n",
        "### **`print_ram_report(title=\"\")`**\n",
        "\n",
        "Prints a formatted summary of RAM usage:\n",
        "\n",
        "```\n",
        "===================== [title] =====================\n",
        "üîπ RAM Python-process : XX.XX MB\n",
        "üîπ RAM System-global  : YY.YY MB\n",
        "===================================================\n",
        "```\n",
        "\n",
        "\n",
        "### **`folder_size_mb(folder)`**\n",
        "\n",
        "Recursively calculates the total size of a directory in megabytes.\n",
        "\n",
        "* Walks the directory using `os.walk`.\n",
        "* For each file:\n",
        "\n",
        "  * Retrieves file size via `os.path.getsize()`\n",
        "  * Accumulates total size\n",
        "* Converts bytes ‚Üí MB.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **V.2. Benchmarking Class**\n",
        "\n",
        "The `Benchmark` class collects performance metrics over the lifetime of the pipeline. It centralizes timing, memory, and disk usage measurements.\n",
        "\n",
        "\n",
        "### **Initialization**\n",
        "\n",
        "```python\n",
        "self.start_time = time.time()\n",
        "self.id_fetch_time = 0\n",
        "self.download_times = {}\n",
        "self.reference_times = {}\n",
        "self.ram_samples = []\n",
        "self.peak_disk_mb = 0\n",
        "```\n",
        "\n",
        "### **Tracks**\n",
        "\n",
        "* **Pipeline start time**\n",
        "* **Time spent fetching arXiv IDs**\n",
        "* **Per-paper download durations**\n",
        "* **Per-paper reference extraction durations**\n",
        "* **RAM samples over time**\n",
        "* **Peak disk usage** (computed from output folder)\n",
        "\n",
        "\n",
        "### **`update_disk(base_dir)`**\n",
        "\n",
        "Updates the peak disk usage metric:\n",
        "\n",
        "* Computes current folder size with `folder_size_mb()`\n",
        "* Updates `peak_disk_mb` to the highest observed value\n",
        "\n",
        "### **`report(base_dir, total_papers)`**\n",
        "\n",
        "Prints a complete performance summary.\n",
        "\n",
        "### **Metrics computed**\n",
        "\n",
        "* **Total pipeline runtime**\n",
        "* **Time spent discovering IDs**\n",
        "* **Average download time per paper**\n",
        "* **Average reference extraction time per paper**\n",
        "* **Maximum process RAM usage**\n",
        "* **Average process RAM usage**\n",
        "* **Peak disk usage during processing**\n",
        "* **Final disk size of output directory**\n",
        "* **Total number of processed papers**\n",
        "\n",
        "### **Sample output**\n",
        "\n",
        "```\n",
        "===================== PERFORMANCE REPORT =====================\n",
        "Total runtime pipeline         : 182.43 sec\n",
        "Time for entry discovery       : 12.48 sec\n",
        "Average download time/paper    : 0.73 sec\n",
        "Average reference time/paper   : 0.21 sec\n",
        "Python process max RAM         : 745.22 MB\n",
        "Python process avg RAM         : 512.88 MB\n",
        "Peak disk usage during runtime : 912.41 MB\n",
        "Final disk usage               : 890.55 MB\n",
        "Total papers processed         : 200\n",
        "==============================================================\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AZqTJR8VUAK9"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# RAM & Disk helpers\n",
        "# ---------------------------\n",
        "def ram_process_mb():\n",
        "    return psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
        "\n",
        "def ram_system_mb():\n",
        "    try:\n",
        "        meminfo = {}\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                key, val = line.split(\":\")\n",
        "                meminfo[key.strip()] = int(val.strip().split()[0])\n",
        "        mem_total = meminfo[\"MemTotal\"]\n",
        "        mem_free = meminfo[\"MemFree\"] + meminfo.get(\"Buffers\", 0) + meminfo.get(\"Cached\", 0)\n",
        "        return (mem_total - mem_free) / 1024\n",
        "    except Exception:\n",
        "        vm = psutil.virtual_memory()\n",
        "        return (vm.total - vm.available) / 1024**2\n",
        "\n",
        "def print_ram_report(title=\"\"):\n",
        "    print(f\"\"\"\n",
        "===================== {title} =====================\n",
        "üîπ RAM Python-process : {ram_process_mb():.2f} MB\n",
        "üîπ RAM System-global  : {ram_system_mb():.2f} MB\n",
        "===================================================\n",
        "\"\"\")\n",
        "\n",
        "def folder_size_mb(folder):\n",
        "    total = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(fp):\n",
        "                total += os.path.getsize(fp)\n",
        "    return total / 1024**2\n",
        "\n",
        "# ---------------------------\n",
        "# Benchmarking class\n",
        "# ---------------------------\n",
        "class Benchmark:\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "        self.id_fetch_time = 0\n",
        "        self.download_times = {}\n",
        "        self.reference_times = {}\n",
        "        self.ram_samples = []\n",
        "        self.peak_disk_mb = 0\n",
        "\n",
        "    def sample_ram(self):\n",
        "        self.ram_samples.append(ram_process_mb())\n",
        "\n",
        "    def update_disk(self, base_dir):\n",
        "        self.peak_disk_mb = max(self.peak_disk_mb, folder_size_mb(base_dir))\n",
        "\n",
        "    def report(self, base_dir, total_papers):\n",
        "        total_time = time.time() - self.start_time\n",
        "        avg_download = sum(self.download_times.values()) / max(1,len(self.download_times))\n",
        "        avg_reference = sum(self.reference_times.values()) / max(1,len(self.reference_times))\n",
        "        max_ram = max(self.ram_samples) if self.ram_samples else 0\n",
        "        avg_ram = sum(self.ram_samples)/len(self.ram_samples) if self.ram_samples else 0\n",
        "        disk_mb = folder_size_mb(base_dir)\n",
        "\n",
        "        print(\"\\n===================== PERFORMANCE REPORT =====================\")\n",
        "        print(f\"Total runtime pipeline         : {total_time:.2f} sec\")\n",
        "        print(f\"Time for entry discovery       : {self.id_fetch_time:.2f} sec\")\n",
        "        print(f\"Average download time/paper    : {avg_download:.2f} sec\")\n",
        "        print(f\"Average reference time/paper   : {avg_reference:.2f} sec\")\n",
        "        print(f\"Python process max RAM         : {max_ram:.2f} MB\")\n",
        "        print(f\"Python process avg RAM         : {avg_ram:.2f} MB\")\n",
        "        print(f\"Peak disk usage during runtime : {self.peak_disk_mb:.2f} MB\")\n",
        "        print(f\"Final disk usage               : {disk_mb:.2f} MB\")\n",
        "        print(f\"Total papers processed         : {total_papers}\")\n",
        "        print(\"==============================================================\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-dHn0bXUAK9"
      },
      "source": [
        "# **VI.Pipeline overview and execution flow**\n",
        "\n",
        "This module coordinates a **multi-threaded arXiv processing pipeline**, responsible for:\n",
        "\n",
        "* Fetching arXiv IDs\n",
        "* Downloading papers (including all versions)\n",
        "* Extracting references\n",
        "* Measuring performance (RAM, disk, timing)\n",
        "* Detecting and recovering missing papers\n",
        "---\n",
        "\n",
        "## **VI.1. Global Settings**\n",
        "\n",
        "```python\n",
        "DOWNLOAD_THREAD_COUNT = 3\n",
        "REFERENCE_THREAD_COUNT = 2\n",
        "\n",
        "benchmark = Benchmark()\n",
        "```\n",
        "\n",
        "* The pipeline uses *separate thread pools* for downloading PDFs and extracting references.\n",
        "* A single global `Benchmark()` instance gathers performance metrics across the full run.\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.2. Detecting existing papers**\n",
        "\n",
        "### `collect_existing_ids(base_dir, target_months, yymm_ranges)`\n",
        "\n",
        "Scans the data directory and collects existing folders matching the pattern:\n",
        "\n",
        "```\n",
        "YYYY-TTTTT ‚Üí e.g., 2401-01532\n",
        "```\n",
        "\n",
        "It only records IDs that fall strictly within your requested ranges (`yymm_ranges`), making the scan fast and consistent.\n",
        "\n",
        "**Functional steps:**\n",
        "\n",
        "* Initialize an empty set per target month.\n",
        "* Scan folder names using a regex matcher.\n",
        "* Validate that the folder‚Äôs tail number falls inside its allowed range.\n",
        "* Return a dictionary:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"2401\": {1532, 1533, ...},\n",
        "  \"2312\": {8911, 8912, ...}\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.3. Fetching ID ranges**\n",
        "\n",
        "### `fetch_ids_worker(...)`\n",
        "\n",
        "This function fetches all arXiv IDs within a date+tail range, then optionally slices the list depending on whether the user wants:\n",
        "\n",
        "* **All papers** from the range, or\n",
        "* A subset (`start_index`, `num_papers`)\n",
        "\n",
        "It also constructs the internal representation:\n",
        "\n",
        "```\n",
        "yymm_ranges = {\n",
        "    \"2401\": (01500, 01700),\n",
        "    \"2312\": (08900, 09020)\n",
        "}\n",
        "```\n",
        "\n",
        "This becomes the backbone for:\n",
        "\n",
        "* Detecting missing downloads\n",
        "* Restricting folder scanning\n",
        "* Recovering re-downloadable entries\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.4. Downloading papers**\n",
        "\n",
        "### `download_worker(id_queue, download_queue, base_data_dir, delay=1.0)`\n",
        "\n",
        "This worker thread:\n",
        "\n",
        "1. Consumes IDs from `id_queue`\n",
        "2. Purges partially downloaded folders (`remove_folder_if_has_data`)\n",
        "3. Downloads **all versions** of each paper using retry logic\n",
        "4. Stores results in `base_data_dir`\n",
        "5. Sends IDs to `download_queue` so the reference extractor can process them\n",
        "\n",
        "### Retry logic includes:\n",
        "\n",
        "* Exponential backoff for 429/503 rate limits\n",
        "* Graceful handling of missing versions\n",
        "* Full traceback logging for unexpected errors\n",
        "\n",
        "### Benchmarking:\n",
        "\n",
        "Each download updates:\n",
        "\n",
        "* `benchmark.download_times[arxiv_id]`\n",
        "* RAM sampling\n",
        "* Disk peak tracking\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.5. Extracting references**\n",
        "\n",
        "### `reference_worker(download_queue, base_data_dir, delay=0.5)`\n",
        "\n",
        "This worker:\n",
        "\n",
        "* Consumes IDs from `download_queue`\n",
        "* Calls `extract_references_for_paper()`\n",
        "* Tracks per-paper extraction time\n",
        "* Updates RAM and disk usage\n",
        "\n",
        "Using a dedicated queue allows reference extraction to start **immediately** after the first paper finishes downloading‚Äîno need to wait for the full batch.\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.6. Recovering missing papers**\n",
        "\n",
        "### `recover_missing_papers(base_dir, yymm_ranges, selected_ids)`\n",
        "\n",
        "This is a robust mechanism to **ensure the entire intended dataset exists**, even if:\n",
        "\n",
        "* A previous run crashed\n",
        "* Network or rate-limit failures occurred\n",
        "* Disk was cleaned manually\n",
        "* Files were partially written\n",
        "\n",
        "### Recovery steps:\n",
        "\n",
        "1. Compare all expected arXiv IDs (`selected_ids`) to the folders physically present.\n",
        "2. Detect missing items **only within the user-selected set**.\n",
        "3. Spawn download and reference threads to reprocess the missing entries.\n",
        "4. Produce a full memory/disk/timing report.\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.7. Thread coordination**\n",
        "\n",
        "The system uses two queues:\n",
        "\n",
        "### 1. `id_queue`\n",
        "\n",
        "* Filled with all IDs to download\n",
        "* Ends with N `None` sentinels ‚Üí one for each downloader thread\n",
        "\n",
        "### 2. `download_queue`\n",
        "\n",
        "* Receives finished downloads\n",
        "* Ends with M `None` sentinels ‚Üí one for each reference extractor\n",
        "\n",
        "Finally, the system waits using:\n",
        "\n",
        "```\n",
        "id_queue.join()\n",
        "download_queue.join()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "```\n",
        "\n",
        "This guarantees a clean shutdown of all workers.\n",
        "\n",
        "---\n",
        "\n",
        "## **VI.8. Benchmark integration**\n",
        "\n",
        "The pipeline gathers:\n",
        "\n",
        "* Total pipeline runtime\n",
        "* Time to fetch IDs\n",
        "* Average download time per paper\n",
        "* Average reference extraction time\n",
        "* Max and average RAM usage\n",
        "* Peak and final disk size\n",
        "* Total processed paper count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8jQiw-dhsHLC"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# MAIN\n",
        "# ---------------------------\n",
        "DOWNLOAD_THREAD_COUNT = 3\n",
        "REFERENCE_THREAD_COUNT = 2\n",
        "\n",
        "#---------------------------\n",
        "# Global benchmark instance\n",
        "#---------------------------\n",
        "\n",
        "benchmark = Benchmark()\n",
        "# ---------------------------\n",
        "# Missing papers helpers\n",
        "# ---------------------------\n",
        "PATTERN = re.compile(r\"^(\\d{4})-(\\d{5})$\")\n",
        "\n",
        "def collect_existing_ids(base_dir: str, target_months: list, yymm_ranges: dict):\n",
        "    \"\"\"\n",
        "    Collect existing folder tails only for target_months and only within\n",
        "    the start/end ranges defined in yymm_ranges.\n",
        "\n",
        "    Returns dict: { yymm: set(tail_ints) } limited to those ranges.\n",
        "    \"\"\"\n",
        "    existing = {ym: set() for ym in target_months}\n",
        "    if not os.path.isdir(base_dir):\n",
        "        return existing\n",
        "\n",
        "    for entry in os.scandir(base_dir):\n",
        "        if not entry.is_dir():\n",
        "            continue\n",
        "        m = PATTERN.match(entry.name)\n",
        "        if not m:\n",
        "            continue\n",
        "        yymm, tail = m.group(1), int(m.group(2))\n",
        "        if yymm not in target_months:\n",
        "            continue\n",
        "        # Only consider folders whose tail falls inside the requested yymm_ranges.\n",
        "        # yymm_ranges is expected to contain [start_tail, end_tail] for each yymm.\n",
        "        if yymm not in yymm_ranges:\n",
        "            # If no explicit range provided for this month, ignore it.\n",
        "            continue\n",
        "        start_tail, end_tail = yymm_ranges[yymm]\n",
        "        if start_tail is None or end_tail is None:\n",
        "            continue\n",
        "        if start_tail <= tail <= end_tail:\n",
        "            existing[yymm].add(tail)\n",
        "\n",
        "    return existing\n",
        "\n",
        "\n",
        "# def find_missing_ids(yymm, existing_set, start_tail, end_tail):\n",
        "#     return sorted(set(range(start_tail, end_tail + 1)) - existing_set)\n",
        "\n",
        "def format_arxiv_ids(yymm, tails):\n",
        "    return [f\"{yymm}.{t:05d}\" for t in tails]\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Fetch IDs Worker\n",
        "# ---------------------------\n",
        "def fetch_ids_worker(start_month, start_year, start_ID,\n",
        "                     end_month, end_year, end_ID,\n",
        "                     start_index, num_papers,\n",
        "                     download_all):\n",
        "    t0 = time.time()\n",
        "    print_ram_report(\"Before fetch_ids_worker\")\n",
        "    ids = get_IDs_All(start_month, start_year, start_ID,\n",
        "                      end_month, end_year, end_ID)\n",
        "\n",
        "    if download_all:\n",
        "        selected_ids = ids\n",
        "    else:\n",
        "        selected_ids = ids[start_index:start_index + num_papers]\n",
        "\n",
        "    yymm_ranges = {}\n",
        "    for arxiv_id in selected_ids:\n",
        "        yymm, tail = arxiv_id.split(\".\")\n",
        "        tail = int(tail)\n",
        "        if yymm not in yymm_ranges:\n",
        "            yymm_ranges[yymm] = [tail, tail]\n",
        "        else:\n",
        "            yymm_ranges[yymm][0] = min(yymm_ranges[yymm][0], tail)\n",
        "            yymm_ranges[yymm][1] = max(yymm_ranges[yymm][1], tail)\n",
        "\n",
        "    benchmark.id_fetch_time = time.time() - t0\n",
        "    print_ram_report(\"After fetch_ids_worker\")\n",
        "    print(f\" Time for fetch_ids_worker: {benchmark.id_fetch_time:.2f} sec\\n\")\n",
        "    return selected_ids, yymm_ranges\n",
        "\n",
        "# ---------------------------\n",
        "# Download helpers\n",
        "# ---------------------------\n",
        "def download_with_retries(client, arxiv_id, max_retries=5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            search = arxiv.Search(id_list=[arxiv_id])\n",
        "            return next(client.results(search))\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e) or \"503\" in str(e):\n",
        "                wait = min(60*(2**attempt), 600) + random.random()*5\n",
        "                print(f\"[Download] Rate-limited: {e}, retry in {wait:.1f}s\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(1+random.random())\n",
        "                    continue\n",
        "                raise\n",
        "    raise RuntimeError(f\"[Download] Failed after retries: {arxiv_id}\")\n",
        "\n",
        "def remove_folder_if_has_data(base_dir, arxiv_id):\n",
        "    yymm, tail = arxiv_id.split(\".\")\n",
        "    folder_name = f\"{yymm}-{int(tail):05d}\"\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "    if os.path.isdir(folder_path) and any(os.scandir(folder_path)):\n",
        "        shutil.rmtree(folder_path)\n",
        "        print(f\"Removed existing folder: {folder_name}\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# ---------------------------\n",
        "# Download & Reference workers\n",
        "# ---------------------------\n",
        "def download_worker(id_queue, download_queue, base_data_dir, delay=1.0):\n",
        "    client = arxiv.Client()\n",
        "    processed = 0\n",
        "\n",
        "    while True:\n",
        "        arxiv_id = id_queue.get()\n",
        "        if arxiv_id is None:\n",
        "            id_queue.task_done()\n",
        "            print(f\"[Download] Thread exit. Total downloaded: {processed}\")\n",
        "            break\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            remove_folder_if_has_data(base_data_dir, arxiv_id)\n",
        "            print(f\"[Download] Start {arxiv_id}\")\n",
        "            result_latest = download_with_retries(client, arxiv_id)\n",
        "            short = result_latest.get_short_id()\n",
        "            latest_version = int(short.split('v')[-1]) if 'v' in short else 1\n",
        "            results_all = [result_latest]\n",
        "            for v in range(1, latest_version):\n",
        "                try:\n",
        "                    res = download_with_retries(client, f\"{arxiv_id}v{v}\")\n",
        "                    results_all.append(res)\n",
        "                except Exception as e:\n",
        "                    print(f\"[Download] Warning: couldn't fetch version {v} for {arxiv_id}: {e}\")\n",
        "            download(results_all, base_data_dir)\n",
        "            processed += 1\n",
        "            print(f\"[Download] Done {arxiv_id} (Total {processed})\")\n",
        "            download_queue.put(arxiv_id)\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"[Download] Error {arxiv_id}: {e}\")\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            benchmark.download_times[arxiv_id] = time.time() - t0\n",
        "            benchmark.sample_ram()\n",
        "            benchmark.update_disk(base_data_dir)\n",
        "            id_queue.task_done()\n",
        "\n",
        "def reference_worker(download_queue, base_data_dir, delay=0.5):\n",
        "    processed = 0\n",
        "    while True:\n",
        "        arxiv_id = download_queue.get()\n",
        "        if arxiv_id is None:\n",
        "            download_queue.task_done()\n",
        "            print(f\"[Reference] Thread exit. Total extracted: {processed}\")\n",
        "            break\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            print(f\"[Reference] Start {arxiv_id}\")\n",
        "            extract_references_for_paper(arxiv_id, base_data_dir)\n",
        "            processed += 1\n",
        "            print(f\"[Reference] Done {arxiv_id} (Total {processed})\")\n",
        "            time.sleep(delay)\n",
        "        except Exception as e:\n",
        "            print(f\"[Reference] Error {arxiv_id}: {e}\")\n",
        "            traceback.print_exc()\n",
        "        finally:\n",
        "            benchmark.reference_times[arxiv_id] = time.time() - t0\n",
        "            benchmark.sample_ram()\n",
        "            benchmark.update_disk(base_data_dir)\n",
        "            download_queue.task_done()\n",
        "\n",
        "# ---------------------------\n",
        "# Recover missing papers\n",
        "# ---------------------------\n",
        "def recover_missing_papers(base_dir, yymm_ranges, selected_ids):\n",
        "    \"\"\"\n",
        "    Recover missing papers, strictly within selected_ids and the yymm_ranges.\n",
        "    This function:\n",
        "      - builds the exact expected IDs from yymm_ranges,\n",
        "      - intersects with selected_ids so only requested slice is recovered,\n",
        "      - runs the download+reference workers with correct sentinel handling.\n",
        "    \"\"\"\n",
        "    print(\"\\n‚è≥ Starting: recover_missing_papers\")\n",
        "    t0 = time.time()\n",
        "    print_ram_report(\"Before recover_missing_papers\")\n",
        "\n",
        "    # Fast lookup of the user's requested set\n",
        "    selected_set = set(selected_ids)\n",
        "\n",
        "    # Collect existing IDs strictly inside the numeric ranges (using fixed collect_existing_ids)\n",
        "    target_months = list(yymm_ranges.keys())\n",
        "    existing = collect_existing_ids(base_dir, target_months, yymm_ranges)\n",
        "\n",
        "    # Build the *expected* IDs strictly from yymm_ranges, then keep only those in selected_set\n",
        "    missing_ids = []\n",
        "    for yymm, (start_tail, end_tail) in yymm_ranges.items():\n",
        "        if start_tail is None or end_tail is None:\n",
        "            continue\n",
        "        for tail in range(start_tail, end_tail + 1):\n",
        "            aid = f\"{yymm}.{tail:05d}\"\n",
        "            # only consider if user selected it\n",
        "            if aid not in selected_set:\n",
        "                continue\n",
        "            # only consider missing if not present in existing[yymm]\n",
        "            if tail not in existing.get(yymm, set()):\n",
        "                missing_ids.append(aid)\n",
        "\n",
        "    if not missing_ids:\n",
        "        print(f\"\\n‚úÖ No missing papers left for the selected range\")\n",
        "        print_ram_report(\"After recover_missing_papers\")\n",
        "        print(f\"‚è±Ô∏è Time for recover_missing_papers: {time.time() - t0:.2f} sec\\n\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚ö†Ô∏è Missing papers detected ({len(missing_ids)}): {missing_ids[:20]}{'...' if len(missing_ids) > 20 else ''}\")\n",
        "\n",
        "    # Setup queues and worker threads\n",
        "    id_queue = queue.Queue(maxsize=len(missing_ids) + DOWNLOAD_THREAD_COUNT + 2)\n",
        "    download_queue = queue.Queue(maxsize=len(missing_ids) + REFERENCE_THREAD_COUNT + 2)\n",
        "\n",
        "    # Enqueue missing IDs\n",
        "    for aid in missing_ids:\n",
        "        id_queue.put(aid)\n",
        "    # Put sentinel None for each download thread so they exit when done\n",
        "    for _ in range(DOWNLOAD_THREAD_COUNT):\n",
        "        id_queue.put(None)\n",
        "\n",
        "    # Start download workers\n",
        "    download_threads = []\n",
        "    for _ in range(DOWNLOAD_THREAD_COUNT):\n",
        "        t = threading.Thread(target=download_worker, args=(id_queue, download_queue, base_dir))\n",
        "        t.start()\n",
        "        download_threads.append(t)\n",
        "\n",
        "    # Start reference workers (they will wait on download_queue)\n",
        "    reference_threads = []\n",
        "    for _ in range(REFERENCE_THREAD_COUNT):\n",
        "        t = threading.Thread(target=reference_worker, args=(download_queue, base_dir))\n",
        "        t.start()\n",
        "        reference_threads.append(t)\n",
        "\n",
        "    # Wait until all downloads enqueued tasks are done\n",
        "    id_queue.join()\n",
        "    # signal reference workers to exit (one None per reference thread)\n",
        "    for _ in range(REFERENCE_THREAD_COUNT):\n",
        "        download_queue.put(None)\n",
        "    # wait for reference processing to finish\n",
        "    download_queue.join()\n",
        "\n",
        "    # join all threads cleanly\n",
        "    for t in download_threads:\n",
        "        t.join()\n",
        "    for t in reference_threads:\n",
        "        t.join()\n",
        "\n",
        "    print_ram_report(\"After recover_missing_papers\")\n",
        "    print(f\"‚è±Ô∏è Time for recover_missing_papers: {time.time() - t0:.2f} sec\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnXI_mjpUAK-"
      },
      "source": [
        "# **VII. Google Drive integration & pipeline execution**\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.1. Mounting Google Drive**\n",
        "\n",
        "To ensure all downloaded PDFs, extracted references, and metadata are stored persistently, the pipeline will attempt to mount Google Drive automatically.\n",
        "\n",
        "```python\n",
        "print(\"Attempting to mount Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted!\\n\")\n",
        "except Exception:\n",
        "    print(\" Not running on Google Colab ‚Üí skipping Google Drive mount.\\n\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.2. Setting up the Data Directory**\n",
        "\n",
        "All downloaded papers and extracted references are stored in a configurable base folder:\n",
        "\n",
        "```python\n",
        "base_data_dir = \"/content/drive/MyDrive/arxiv_data\"\n",
        "os.makedirs(base_data_dir, exist_ok=True)\n",
        "```\n",
        "\n",
        "* This folder lives inside your mounted Google Drive ‚Üí **persistent between notebook sessions**.\n",
        "* The pipeline automatically creates it if it does not already exist.\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.3. Configuring the pipeline run**\n",
        "\n",
        "You choose the input arXiv ranges and how many papers to download:\n",
        "\n",
        "```python\n",
        "start_month, start_year = 3, 2023\n",
        "end_month, end_year = 4, 2023\n",
        "start_ID, end_ID = 7856, 4606\n",
        "start_index, num_papers = 0, 10\n",
        "download_all = False\n",
        "```\n",
        "\n",
        "### Meaning of parameters:\n",
        "\n",
        "* **(start_month, start_year)** ‚Üí beginning of the date range\n",
        "* **(end_month, end_year)** ‚Üí end of the date range\n",
        "* **start_ID, end_ID** ‚Üí tail bounds inside each month\n",
        "* **start_index, num_papers** ‚Üí take a slice from the full ID list\n",
        "* **download_all**\n",
        "\n",
        "  * `False` ‚Üí take only a subset\n",
        "  * `True` ‚Üí download the entire range\n",
        "\n",
        "This gives full flexibility for partial downloads, sampling, or full-range crawls.\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.4. Fetching IDs and preparing ranges**\n",
        "\n",
        "The pipeline starts by discovering all arXiv IDs that match your parameters:\n",
        "\n",
        "```python\n",
        "selected_ids, yymm_ranges = fetch_ids_worker(...)\n",
        "```\n",
        "\n",
        "### Outputs:\n",
        "\n",
        "* **selected_ids** ‚Üí a flat list of full arXiv IDs\n",
        "* **yymm_ranges** ‚Üí a dictionary describing:\n",
        "\n",
        "  ```\n",
        "  { \"YYYYMM\": (start_tail, end_tail), ... }\n",
        "  ```\n",
        "\n",
        "These ranges are later used for:\n",
        "\n",
        "* folder existence checks\n",
        "* missing-paper recovery\n",
        "* consistency validation\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.5. Launching download & reference threads**\n",
        "\n",
        "The worker queues coordinate the pipeline:\n",
        "\n",
        "```python\n",
        "id_queue = queue.Queue(maxsize=len(selected_ids)+DOWNLOAD_THREAD_COUNT+2)\n",
        "download_queue = queue.Queue(maxsize=len(selected_ids)+REFERENCE_THREAD_COUNT+2)\n",
        "\n",
        "for aid in selected_ids:\n",
        "    id_queue.put(aid)\n",
        "\n",
        "# One sentinel per downloader\n",
        "for _ in range(DOWNLOAD_THREAD_COUNT):\n",
        "    id_queue.put(None)\n",
        "```\n",
        "\n",
        "### Threads started:\n",
        "\n",
        "```python\n",
        "download_threads = [...]\n",
        "reference_threads = [...]\n",
        "```\n",
        "\n",
        "* **Download workers** fetch PDFs (all versions), clean old folders, and push completed IDs into `download_queue`.\n",
        "* **Reference workers** extract citation metadata from each just-downloaded folder.\n",
        "\n",
        "### Thread termination:\n",
        "\n",
        "```python\n",
        "id_queue.join()\n",
        "for _ in range(REFERENCE_THREAD_COUNT):\n",
        "    download_queue.put(None)\n",
        "download_queue.join()\n",
        "for t in combined_threads:\n",
        "    t.join()\n",
        "```\n",
        "\n",
        "This ensures a clean shutdown with no deadlocks.\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.6. Automatic recovery of missing papers**\n",
        "\n",
        "After the main pipeline finishes, the system ensures full integrity:\n",
        "\n",
        "```python\n",
        "recover_missing_papers(base_data_dir, yymm_ranges, selected_ids)\n",
        "```\n",
        "\n",
        "This step will:\n",
        "\n",
        "* Re-scan the storage directory\n",
        "* Detect missing folders (corrupted or incomplete downloads)\n",
        "* Re-launch a mini-pipeline to re-fetch them\n",
        "* Print diagnostic information\n",
        "\n",
        "This guarantees **dataset completeness** even if the run was interrupted earlier.\n",
        "\n",
        "---\n",
        "\n",
        "## **VII.7. Performance report**\n",
        "\n",
        "At the end of the execution, a full performance summary is printed:\n",
        "\n",
        "```python\n",
        "benchmark.report(base_data_dir, total_papers=len(selected_ids))\n",
        "```\n",
        "\n",
        "The report includes:\n",
        "\n",
        "* Total runtime\n",
        "* Time spent fetching IDs\n",
        "* Average download time\n",
        "* Average reference extraction time\n",
        "* Max & average RAM usage\n",
        "* Peak disk growth\n",
        "* Final disk footprint\n",
        "* Total processed papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FolF5h3OD4MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629b89fb-b660-4d62-d934-7e3b0d87365d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to mount Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted!\n",
            "\n",
            "Starting pipeline...\n",
            "\n",
            "\n",
            "===================== Pipeline Start =====================\n",
            "üîπ RAM Python-process : 108.75 MB\n",
            "üîπ RAM System-global  : 1264.15 MB\n",
            "===================================================\n",
            "\n",
            "\n",
            "===================== Before fetch_ids_worker =====================\n",
            "üîπ RAM Python-process : 108.75 MB\n",
            "üîπ RAM System-global  : 1264.15 MB\n",
            "===================================================\n",
            "\n",
            "\n",
            "===================== After fetch_ids_worker =====================\n",
            "üîπ RAM Python-process : 109.84 MB\n",
            "üîπ RAM System-global  : 1314.21 MB\n",
            "===================================================\n",
            "\n",
            " Time for fetch_ids_worker: 1.94 sec\n",
            "\n",
            "‚Üí Implemented ranges (yymm -> start_tail,end_tail): {'2303': [7856, 7865]}\n",
            "‚Üí Selected IDs (first 20): ['2303.07856', '2303.07857', '2303.07858', '2303.07859', '2303.07860', '2303.07861', '2303.07862', '2303.07863', '2303.07864', '2303.07865']\n",
            "[Download] Start 2303.07856\n",
            "[Download] Start 2303.07857\n",
            "[Download] Start 2303.07858\n",
            "Processing 2303.07858 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07858\n",
            "Processing 2303.07857 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07857\n",
            "Attempting source: https://arxiv.org/src/2303.07858v1\n",
            "Attempting source: https://arxiv.org/src/2303.07857v1\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07857/tex/2303.07857v1\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07857/metadata.json\n",
            "[Download] Done 2303.07857 (Total 1)\n",
            "[Reference] Start 2303.07857\n",
            "Fetching references for 2303.07857...\n",
            "  Saved 23 references to references.json\n",
            "[Reference] Done 2303.07857 (Total 1)\n",
            "[Download] Start 2303.07859\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07858/tex/2303.07858v1\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07858/metadata.json\n",
            "[Download] Done 2303.07858 (Total 1)\n",
            "[Reference] Start 2303.07858\n",
            "Fetching references for 2303.07858...\n",
            "  Rate limit hit. Waiting 1s before retry...\n",
            "Processing 2303.07856 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07856\n",
            "Attempting source: https://arxiv.org/src/2303.07856v2\n",
            "[Download] Start 2303.07860\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07856/tex/2303.07856v2\n",
            "Attempting source: https://arxiv.org/src/2303.07856v1\n",
            "  Saved 4 references to references.json\n",
            "[Reference] Done 2303.07858 (Total 1)\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07856/tex/2303.07856v1\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07856/metadata.json\n",
            "[Download] Done 2303.07856 (Total 1)\n",
            "[Reference] Start 2303.07856\n",
            "Fetching references for 2303.07856...\n",
            "  Paper 2303.07856 not found in Semantic Scholar\n",
            "  No references found for 2303.07856\n",
            "[Reference] Done 2303.07856 (Total 2)\n",
            "[Download] Start 2303.07861\n",
            "Processing 2303.07861 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07861\n",
            "Attempting source: https://arxiv.org/src/2303.07861v1\n",
            "Processing 2303.07860 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07860\n",
            "Attempting source: https://arxiv.org/src/2303.07860v2\n",
            "Invalid tar archive for 2303.07860v2. Removing file.\n",
            "Attempting source: https://arxiv.org/src/2303.07860v1\n",
            "Invalid tar archive for 2303.07860v1. Removing file.\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07860/metadata.json\n",
            "[Download] Done 2303.07860 (Total 2)\n",
            "[Reference] Start 2303.07860\n",
            "Fetching references for 2303.07860...\n",
            "  Saved 5 references to references.json\n",
            "[Reference] Done 2303.07860 (Total 2)\n",
            "[Download] Start 2303.07862\n",
            "Processing 2303.07859 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07859\n",
            "Attempting source: https://arxiv.org/src/2303.07859v3\n",
            "Processing 2303.07862 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07862\n",
            "Attempting source: https://arxiv.org/src/2303.07862v1\n",
            "Invalid tar archive for 2303.07862v1. Removing file.\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07862/metadata.json\n",
            "[Download] Done 2303.07862 (Total 3)\n",
            "[Reference] Start 2303.07862\n",
            "Fetching references for 2303.07862...\n",
            "  Saved 2 references to references.json\n",
            "[Reference] Done 2303.07862 (Total 3)\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07859/tex/2303.07859v3\n",
            "Attempting source: https://arxiv.org/src/2303.07859v1\n",
            "[Download] Start 2303.07863\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07859/tex/2303.07859v1\n",
            "Attempting source: https://arxiv.org/src/2303.07859v2\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07861/tex/2303.07861v1\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07861/metadata.json\n",
            "[Download] Done 2303.07861 (Total 2)\n",
            "[Reference] Start 2303.07861\n",
            "Fetching references for 2303.07861...\n",
            "  Saved 6 references to references.json\n",
            "[Reference] Done 2303.07861 (Total 3)\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07859/tex/2303.07859v2\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07859/metadata.json\n",
            "[Download] Done 2303.07859 (Total 2)\n",
            "[Reference] Start 2303.07859\n",
            "Fetching references for 2303.07859...\n",
            "[Download] Start 2303.07864\n",
            "Processing 2303.07864 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07864\n",
            "Attempting source: https://arxiv.org/src/2303.07864v1\n",
            "  Saved 29 references to references.json\n",
            "[Reference] Done 2303.07859 (Total 4)\n",
            "[Download] Start 2303.07865\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07864/tex/2303.07864v1\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07864/metadata.json\n",
            "[Download] Done 2303.07864 (Total 3)\n",
            "[Reference] Start 2303.07864\n",
            "Fetching references for 2303.07864...\n",
            "  Saved 27 references to references.json\n",
            "[Reference] Done 2303.07864 (Total 4)\n",
            "[Download] Thread exit. Total downloaded: 3\n",
            "Processing 2303.07863 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07863\n",
            "Attempting source: https://arxiv.org/src/2303.07863v2\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07863/tex/2303.07863v2\n",
            "Attempting source: https://arxiv.org/src/2303.07863v1\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07863/tex/2303.07863v1\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07863/metadata.json\n",
            "[Download] Done 2303.07863 (Total 4)\n",
            "[Reference] Start 2303.07863\n",
            "Fetching references for 2303.07863...\n",
            "  Saved 67 references to references.json\n",
            "[Reference] Done 2303.07863 (Total 5)\n",
            "[Download] Thread exit. Total downloaded: 4\n",
            "Processing 2303.07865 ‚Üí /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865\n",
            "Attempting source: https://arxiv.org/src/2303.07865v6\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/tex/2303.07865v6\n",
            "Attempting source: https://arxiv.org/src/2303.07865v1\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/tex/2303.07865v1\n",
            "Attempting source: https://arxiv.org/src/2303.07865v2\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/tex/2303.07865v2\n",
            "Attempting source: https://arxiv.org/src/2303.07865v3\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/tex/2303.07865v3\n",
            "Attempting source: https://arxiv.org/src/2303.07865v4\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/tex/2303.07865v4\n",
            "Attempting source: https://arxiv.org/src/2303.07865v5\n",
            "‚úÖ Extracted to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/tex/2303.07865v5\n",
            "üíæ Saved metadata to /content/drive/MyDrive/arxiv_data_paper_test2/2303-07865/metadata.json\n",
            "[Download] Done 2303.07865 (Total 3)\n",
            "[Reference] Start 2303.07865\n",
            "Fetching references for 2303.07865...\n",
            "  Saved 16 references to references.json\n",
            "[Reference] Done 2303.07865 (Total 5)\n",
            "[Download] Thread exit. Total downloaded: 3\n",
            "[Reference] Thread exit. Total extracted: 5\n",
            "[Reference] Thread exit. Total extracted: 5\n",
            "\n",
            "‚è≥ Starting: recover_missing_papers\n",
            "\n",
            "===================== Before recover_missing_papers =====================\n",
            "üîπ RAM Python-process : 113.61 MB\n",
            "üîπ RAM System-global  : 1387.18 MB\n",
            "===================================================\n",
            "\n",
            "\n",
            "‚úÖ No missing papers left for the selected range\n",
            "\n",
            "===================== After recover_missing_papers =====================\n",
            "üîπ RAM Python-process : 113.61 MB\n",
            "üîπ RAM System-global  : 1387.18 MB\n",
            "===================================================\n",
            "\n",
            "‚è±Ô∏è Time for recover_missing_papers: 0.00 sec\n",
            "\n",
            "\n",
            "===================== PERFORMANCE REPORT =====================\n",
            "Total runtime pipeline         : 42.45 sec\n",
            "Time for entry discovery       : 1.94 sec\n",
            "Average download time/paper    : 6.83 sec\n",
            "Average reference time/paper   : 1.03 sec\n",
            "Python process max RAM         : 113.66 MB\n",
            "Python process avg RAM         : 112.68 MB\n",
            "Peak disk usage during runtime : 24.60 MB\n",
            "Final disk usage               : 3.30 MB\n",
            "Total papers processed         : 10\n",
            "==============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Google Drive mount\n",
        "# ---------------------------\n",
        "print(\"Attempting to mount Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted!\\n\")\n",
        "except Exception:\n",
        "    print(\"Not running on Google Colab ‚Üí skipping Google Drive mount.\\n\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# MAIN\n",
        "# ---------------------------\n",
        "base_data_dir = \"/content/drive/MyDrive/arxiv_data_paper_test2\"\n",
        "os.makedirs(base_data_dir, exist_ok=True)\n",
        "\n",
        "start_month, start_year = 3, 2023\n",
        "end_month, end_year = 4, 2023\n",
        "start_ID, end_ID = 7856, 4606\n",
        "start_index, num_papers = 0, 10\n",
        "download_all = False\n",
        "\n",
        "print(\"Starting pipeline...\\n\")\n",
        "print_ram_report(\"Pipeline Start\")\n",
        "selected_ids, yymm_ranges = fetch_ids_worker(\n",
        "        start_month, start_year, start_ID,\n",
        "        end_month, end_year, end_ID,\n",
        "        start_index, num_papers,\n",
        "        download_all\n",
        "    )\n",
        "\n",
        "print(\"‚Üí Implemented ranges (yymm -> start_tail,end_tail):\", yymm_ranges)\n",
        "print(f\"‚Üí Selected IDs (first 20): {selected_ids[:20]}{'...' if len(selected_ids)>20 else ''}\")\n",
        "\n",
        "# Queues for threading\n",
        "id_queue = queue.Queue(maxsize=len(selected_ids)+DOWNLOAD_THREAD_COUNT+2)\n",
        "download_queue = queue.Queue(maxsize=len(selected_ids)+REFERENCE_THREAD_COUNT+2)\n",
        "for aid in selected_ids: id_queue.put(aid)\n",
        "for _ in range(DOWNLOAD_THREAD_COUNT): id_queue.put(None)\n",
        "\n",
        "download_threads = [threading.Thread(target=download_worker, args=(id_queue, download_queue, base_data_dir)) for _ in range(DOWNLOAD_THREAD_COUNT)]\n",
        "for t in download_threads: t.start()\n",
        "reference_threads = [threading.Thread(target=reference_worker, args=(download_queue, base_data_dir)) for _ in range(REFERENCE_THREAD_COUNT)]\n",
        "for t in reference_threads: t.start()\n",
        "\n",
        "id_queue.join()\n",
        "for _ in range(REFERENCE_THREAD_COUNT): download_queue.put(None)\n",
        "download_queue.join()\n",
        "for t in download_threads + reference_threads: t.join()\n",
        "\n",
        "recover_missing_papers(base_data_dir, yymm_ranges, selected_ids)\n",
        "\n",
        "# Final performance report\n",
        "benchmark.report(base_data_dir, total_papers=len(selected_ids))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}