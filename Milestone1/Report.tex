\documentclass[12pt]{article}

% --- BEGIN CONTENT FROM hcmus-report-template.sty ---
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{xcolor}  
\usepackage{framed}
\usepackage{multirow}

\usepackage{array}
\usepackage{longtable}
\usepackage[colorlinks=true,linkcolor=blue, citecolor=red]{hyperref}
\usepackage{url}
\usepackage[top=.75in, left=.75in, right=.75in, bottom=1in]{geometry}
\usepackage[utf8]{vietnam}

% For algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{hyperref}  % for clickable URLs
\usepackage{bibentry}  % for including full BibTeX entries

\nobibliography*  % initialize bibentry


% ============ CODE ============
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Styling for the code.
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}


% ============ HEADER AND FOOTER ============
% Header length
\setlength{\headheight}{29.43912pt}

% Footer page number would be on the lower-right corner
\pagestyle{fancy}
\fancyfoot{}
\fancyfoot[R]{\textbf{Page \thepage}}

% Vietnamese lstlisting
\newcommand{\vietnameselst}{
	\lstset{literate=%
	% Vần a
		{á}{{\'a}}1
		{à}{{\`a}}1
		{ạ}{{\d a}}1
		{ả}{{\h a}}1
		{ã}{{\~ a}}1
		%
		{Á}{{\'A}}1
		{À}{{\`A}}1
		{Ạ}{{\d A}}1
		{Ả}{{\h A}}1
		{Ã}{{\~ A}}1
	%
	% Vần ă
		{ă}{{\u a}}1
		{ắ}{{\'\abreve }}1
		{ằ}{{\`\abreve }}1
		{ặ}{{\d \abreve }}1
		{ẳ}{{\h \abreve }}1
		{ẵ}{{\~\abreve }}1
		%
		{Ă}{{\u A}}1
		{Ắ}{{\'\ABREVE }}1
		{Ằ}{{\`\ABREVE }}1
		{Ặ}{{\d \ABREVE }}1
		{Ẳ}{{\h \ABREVE }}1
		{Ẵ}{{\~\ABREVE }}1
	%
	% Vần â
		{â}{{\^ a}}1
		{ấ}{{\'\acircumflex }}1
		{ầ}{{\`\acircumflex }}1
		{ậ}{{\d \acircumflex }}1
		{ẩ}{{\h \acircumflex }}1
		{ẫ}{{\~\acircumflex }}1
		%
		{Â}{{\^ A}}1
		{Ấ}{{\'\ACIRCUMFLEX }}1
		{Ầ}{{\`\ACIRCUMFLEX }}1
		{Ậ}{{\d \ACIRCUMFLEX }}1
		{Ẩ}{{\h \ACIRCUMFLEX }}1
		{Ẫ}{{\~\ACIRCUMFLEX }}1
	%
	% Vần đ
		{đ}{{\dj }}1
		{Đ}{{\DJ }}1
	%
	% Vần e
		{é}{{\'e}}1
		{è}{{\`e}}1
		{ẹ}{{\d e}}1
		{ẻ}{{\h e}}1
		{ẽ}{{\~ e}}1
		%
		{É}{{\'E}}1
		{È}{{\`E}}1
		{Ẹ}{{\d E}}1
		{Ẻ}{{\h E}}1
		{Ẽ}{{\~ E}}1
	%
	% Vần ê
		{ê}{{\^e}}1
		{ế}{{\'\ecircumflex }}1
		{ề}{{\`\ecircumflex }}1
		{ệ}{{\d \ecircumflex }}1
		{ể}{{\h \ecircumflex }}1
		{ễ}{{\~\ecircumflex }}1
		%
		{Ê}{{\^E}}1
		{Ế}{{\'\ECIRCUMFLEX }}1
		{Ề}{{\`\ECIRCUMFLEX }}1
		{Ệ}{{\d \ECIRCUMFLEX }}1
		{Ể}{{\h \ECIRCUMFLEX }}1
		{Ễ}{{\~\ECIRCUMFLEX }}1
	%
	% Vần i
		{í}{{\'i}}1
		{ì}{{\`\i }}1
		{ị}{{\d i}}1
		{ỉ}{{\h i}}1
		{ĩ}{{\~\i }}1
		%
		{Í}{{\'I}}1
		{Ì}{{\`I}}1
		{Ị}{{\d I}}1
		{Ỉ}{{\h I}}1
		{Ĩ}{{\~I}}1
	%
	% Vần o
		{ó}{{\'o}}1
		{ò}{{\`o}}1
		{ọ}{{\d o}}1
		{ỏ}{{\h o}}1
		{õ}{{\~o}}1
		%
		{Ó}{{\'O}}1
		{Ò}{{\`O}}1
		{Ọ}{{\d O}}1
		{Ỏ}{{\h O}}1
		{Õ}{{\~O}}1
	%
	% Vần ô
		{ô}{{\^o}}1
		{ố}{{\'\ocircumflex }}1
		{ồ}{{\`\ocircumflex }}1
		{ộ}{{\d \ocircumflex }}1
		{ổ}{{\h \ocircumflex }}1
		{ỗ}{{\~\ocircumflex }}1
		%
		{Ô}{{\^O}}1
		{Ố}{{\'\OCIRCUMFLEX }}1
		{Ồ}{{\`\OCIRCUMFLEX }}1
		{Ộ}{{\d \OCIRCUMFLEX }}1
		{Ổ}{{\h \OCIRCUMFLEX }}1
		{Ỗ}{{\~\OCIRCUMFLEX }}1
	%
	% Vần ơ
		{ơ}{{\ohorn }}1
		{ớ}{{\'\ohorn }}1
		{ờ}{{\`\ohorn }}1
		{ợ}{{\d \ohorn }}1
		{ở}{{\h \ohorn }}1
		{ỡ}{{\~\ohorn }}1
		%
		{Ơ}{{\OHORN }}1
		{Ớ}{{\'\OHORN }}1
		{Ờ}{{\`\OHORN }}1
		{Ợ}{{\d \OHORN }}1
		{Ở}{{\h \OHORN }}1
		{Ỡ}{{\~\OHORN }}1
	%
	% Vần u
		{ú}{{\'u}}1
		{ù}{{\`u}}1
		{ụ}{{\d u}}1
		{ủ}{{\h u}}1
		{ũ}{{\~u}}1
		%
		{Ú}{{\'U}}1
		{Ù}{{\`U}}1
		{Ụ}{{\d U}}1
		{Ủ}{{\h U}}1
		{Ũ}{{\~U}}1
	%
	% Vần ư
		{ư}{{\uhorn }}1
		{ứ}{{\'\uhorn }}1
		{ừ}{{\`\uhorn }}1
		{ự}{{\d \uhorn }}1
		{ử}{{\h \uhorn }}1
		{ữ}{{\~\uhorn }}1
		%
		{Ư}{{\UHORN }}1
		{Ứ}{{\'\UHORN }}1
		{Ừ}{{\`\UHORN }}1
		{Ự}{{\d \UHORN }}1
		{Ử}{{\h \UHORN }}1
		{Ữ}{{\~\UHORN }}1
	%
	% Vần y
		{ý}{{\'y}}1
		{ỳ}{{\`y}}1
		{ỵ}{{\d y}}1
		{ỷ}{{\h y}}1
		{ỹ}{{\~y}}1
		%
		{Ý}{{\'Y}}1
		{Ỳ}{{\`Y}}1
		{Ỵ}{{\d Y}}1
		{Ỷ}{{\h Y}}1
		{Ỹ}{{\~Y}}1
	}
}
% --- END CONTENT FROM hcmus-report-template.sty ---


\usepackage{enumitem}

% Disable indentation on new paragraphs
\setlength{\parindent}{0pt}

% Line spacing 1.5
\renewcommand{\baselinestretch}{1.5}

% Optional: graphic path
% \graphicspath{PATH_TO_GRAPHIC_FOLDER}

% To use Times font family, uncomment this row
% \usepackage{mathptmx}

% To use roman section / subsection, uncomment these rows
% \renewcommand{\thesection}{\Roman{section}}
% \renewcommand{\thesubsection}{\thesection.\Roman{subsection}}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\question}[1]{\par\colorbox{light-gray}
{\parbox{\dimexpr\textwidth-2\fboxsep\relax}{#1}}}

\newcommand{\nle}{\text{\space}\overline{\le}\text{\space}}
\newcommand{\nge}{\text{\space}\overline{\ge}\text{\space}}

% Define course name, report name and report title.
\newcommand{\reporttitle}{Conclusion}

% Header
\lhead{Faculty of Information Technology}
\newcommand{\leftfooter}{\texttt{CSC14119 – Introduction to Data Science}}
\lfoot{\leftfooter}

% ============ DOCUMENT ============
\begin{document}

\pagenumbering{roman}
% --- BEGIN CONTENT FROM title.tex ---
\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\centering
\textsc{\Large University of Science - VNUHCM}\\[0.5cm]
\textsc{\large Faculty of Information Technology}\\[0.5cm]
\includegraphics[scale=.30]{img/hcmus-logo.png}\\[0.5cm] 
\Large{\bfseries{COURSE MILESTONE 1}}\\[0.5cm]
\textbf{\large CSC14119 – Introduction to Data Science}\\[2.0cm]

\noindent
\begin{minipage}[t]{0.4\textwidth}
\begin{flushleft} \large
\emph{Student in charge:}\\
Cao Tấn Hoàng Huy - 23127051\\
Nguyễn Hữu Anh Trí - 23127130\\
Nguyễn Huy Quân - 23127107\\
\end{flushleft}
\end{minipage}
\noindent
\begin{minipage}[t]{0.4\textwidth}
\begin{flushright} \large
\emph{Instructor:}\\
Huỳnh Lâm Hải Đăng\\
\end{flushright}
\end{minipage}
\end{titlepage}
% --- END CONTENT FROM title.tex ---


\tableofcontents
\pagebreak
% \listoftables
% \pagebreak
% \listoffigures
% \pagebreak

\pagenumbering{arabic}
\setcounter{page}{1}
    \section{Introduction}
    This report covers the design and performance of a data scraper built for the "Introduction to Data Science" course milestone. The main goal was to practice web scraping by building a functional tool that collects scientific paper data. \\

    The project focuses on \textbf{arXiv}, a massive public repository for research papers. Unlike typical scraping projects that only collect basic data (metadata), our system was uniquely designed to get the \textbf{full-text \LaTeX{} source files} for all paper versions. This provides a much richer dataset than using PDFs, which is highly valuable for advanced research tasks like text analysis and studying citation networks.\\

    The scraper operates in multiple stages: (1) finding all the assigned papers (Entry Discovery), (2) downloading and cleaning the raw \LaTeX{} source code (Full-Text Source Download and processing), and (3) gathering citation details using the Semantic Scholar API (Reference Extraction). \\

    The rest of this report will detail the tools used, present key scraping statistics, and provide a full analysis of the scraper's speed and resource.

    \section{Implementation Details}
        This section details the technical implementation of the scraper, outlining the tools used and the sequential workflow for processing each arXiv paper.
        \subsection{Tools and Libraries}
        The scraper is built in Python and relies on several key external libraries and APIs:
        
        \begin{itemize}
            \item \textbf{Libraries:}
            \begin{itemize}
                \item \texttt{arxiv}: The primary library used to query the arXiv API for paper discovery, metadata retrieval, and downloading source files.
                \item \texttt{requests}: Used to perform HTTP GET requests to the Semantic Scholar API for reference extraction and for downloading files or checking links.
                \item \texttt{psutil}: Monitors system resources such as memory and CPU usage, enabling performance tracking and optimization.
                \item \texttt{threading}: Enables concurrent execution of tasks, allowing multiple papers to be processed in parallel for improved efficiency.
                \item \texttt{queue}: Provides thread-safe task queues to coordinate work between producer and consumer threads.
                \item \texttt{tarfile}: Extracts \texttt{.tar.gz} or \texttt{.tgz} source archives downloaded from arXiv, preserving the original directory structure.
                \item \texttt{re} (Regular Expressions): Used to parse TeX files, remove figure-related commands, and sanitize or extract information from text.
                \item \texttt{json}: Handles reading and writing of JSON files for metadata and reference storage.
                \item \texttt{os}: Manages file paths, directory creation, and file system operations.
                \item \texttt{time}: Used for benchmarking, sleeping between requests, and measuring process durations.
                \item \texttt{random}: Supports random selection or sampling of papers, and introduces jitter in retry logic to avoid API rate limits.
                \item \texttt{string}: Assists in sanitizing filenames and dictionary keys to ensure compatibility with file systems.
                \item \texttt{logging}: Used to suppress or manage log output, especially from verbose libraries such as \texttt{arxiv}.
            \end{itemize}
            
            \item \textbf{APIs:}
            \begin{itemize}
                \item \textbf{arXiv API} \cite{arxiv_api_basics_2025, arxiv_api_users_manual_2025} : The backend for the \texttt{arxiv} library, providing programmatic access to paper metadata and source files.
                \item \textbf{Semantic Scholar API} \cite{semanticscholar_snippet_text_2025}: Used to fetch citation data and identify arXiv IDs within a paper's reference list.
            \end{itemize}
        \end{itemize}
        
        \subsection{Overall Pipeline}
        The scraping process is arranged by \texttt{main.py}, which executes a sequential pipeline for each assigned arXiv ID.
        The workflow is modularized into distinct steps, handled by the different Python scripts:
        
        \begin{enumerate}
            \item \textbf{ID Discovery and Validation} \\
            The pipeline begins by determining the exact set of arXiv paper IDs to process. Since the last valid ID for each month is not known in advance, a binary search strategy is used to efficiently find the first and last available IDs within the assigned range. This ensures that all relevant papers are included and none are missed or duplicated. The resulting list of IDs is then formatted and prepared for the next stages.
        
            \item \textbf{Metadata and Bib\TeX\ Collection} \\
            For each paper ID, the system queries the arXiv API to gather all available metadata, including the paper’s title, list of authors, submission date, revision history, subject categories, and a cleaned abstract. If available, publication venue and Bib\TeX\ entries are also collected. All this information is organized and saved as \texttt{metadata.json} (and \texttt{references.bib} if present) in the paper’s designated subfolder.
        
            \item \textbf{Full-Text Source Download and Extraction} \\
            The system downloads the full source files for every version of each paper (e.g., \texttt{v1}, \texttt{v2}, etc.), ensuring that all revisions are included. Each version’s \texttt{.tar.gz} archive is downloaded and safely extracted, with special care taken to avoid problematic files or paths. Only the essential \texttt{.tex} and \texttt{.bib} files are retained, while all other files are removed to comply with requirements and save storage space.
        
            \item \textbf{Citation Network Extraction} \\
            To build a comprehensive citation network, the system queries the Semantic Scholar API for each paper’s arXiv ID. Only references that can be matched to another arXiv paper are included. For each matched reference, key metadata such as title, authors, submission date, and Semantic Scholar ID are collected and saved in a dedicated \texttt{references.json} file.
        
            \item \textbf{Missing Papers Recovery} \\
            Throughout the process, the system continuously checks for missing or incomplete papers by comparing the expected list of IDs with the actual folders and files present. Any papers found to be missing or incomplete are automatically re-queued for download and processing, ensuring that the final dataset is as complete as possible.
        
            \item \textbf{Benchmarking} \\
            The system monitors its own performance, recording statistics such as total runtime, time spent on each step, memory usage, and disk space consumed. This benchmarking information is used to optimize the workflow and provides a detailed report on the efficiency and reliability of the scraping process.
        \end{enumerate}
        
        This six-step process is repeated for every paper in the assigned range, resulting in a complete, structured dataset that meets all assignment requirements.
        % --- END CONTENT FROM overall_flow.tex ---
        
    \section{Pipeline Detail}
        % --- BEGIN CONTENT FROM entry.tex ---
        \subsection{Step 1 - Entry Discovery}        
        This initial step is responsible for generating the complete and accurate list of all arXiv paper IDs that the rest of the pipeline will scrape.
        
        \subsubsection{The Core Problem}
        
        The main challenge is that, although arXiv IDs (e.g., \texttt{YYMM.NNNNN}) are assigned sequentially within each month, arXiv does not provide a direct way to discover the ID of the last paper submitted in any given month. While the numeric part of the ID increases monotonically, there may be occasional gaps due to withdrawn submissions or administrative removals. Furthermore, since the arXiv API does not support listing all valid IDs for a month, it is not feasible to simply loop from \texttt{00001} to \texttt{99999} without making a large number of unnecessary or failed API requests. Therefore, the pipeline must efficiently and accurately determine the \textit{actual} first and last valid ID for each month in the target range, ensuring that only real papers are included in the subsequent processing steps.
        
        \subsubsection{The Solution}
        The solution is a sophisticated search algorithm that efficiently finds the valid ID boundaries for each month. Instead of checking one by one, it uses a two-stage strategy:
        \begin{enumerate}
            \item \textbf{Exponential Search:} To quickly find an ID in the correct range, the search index is doubled (e.g., \texttt{1}, \texttt{2}, \texttt{4}, \texttt{8}, \dots) until a valid (or invalid) paper is found.
            \item \textbf{Binary Search:} Once a rough range is established, a binary search is used to pinpoint the \textit{exact} first or last valid ID in that range.
        \end{enumerate}
        \subsubsection{Breakdown of the Process}
        \begin{itemize}
            \item \textbf{\texttt{id\_exists(paper\_id)}:} This is the core "oracle" function. It queries the arXiv API with a single ID. It returns \texttt{True} if the paper exists and \texttt{False} if it doesn't (or if an error occurs).
            \item \textbf{\texttt{find\_first\_id(year, month)}:} This function finds the \textit{first} valid ID. It uses an exponential search to find *any* valid ID, and then a binary search to find the lowest valid index.
            \item \textbf{\texttt{find\_last\_id(year, month)}:} This function finds the \textit{last} valid ID. It uses an exponential search to find a *non-existent* ID, and then a binary search to find the highest valid index just before it.
            \item \textbf{\texttt{get\_IDs\_All(...)}:} This is the main orchestrator. It loops month by month from the start date to the end date. For the first and last months, it uses the user-provided start/end IDs. For all months in between, it uses \texttt{find\_first\_id} and \texttt{find\_last\_id} to get the complete list of IDs for that month.
        \end{itemize}
        
        \subsubsection{Final Output}        
        The result of this step is a single, complete list of all valid arXiv paper IDs within the specified date and ID range, ready to be passed to the next stage of the pipeline.
        % --- END CONTENT FROM entry.tex ---
        
        % --- BEGIN CONTENT FROM download.tex ---
        \subsection{Step 2 - Metadata Collection}     
        This step is responsible for fetching all descriptive information for a paper and saving it to a structured \texttt{metadata.json} file.
        
        \subsubsection{The Core Problem}        
        The \texttt{arxiv.Result} object returned by the API is a Python object and not a persistent data format. This data must be parsed, cleaned, and structured according to the assignment's requirements, including extracting all authors, handling version history, and capturing submission/revision dates.
        
        \subsubsection{The Solution}        
        The notebook implements a two-function system. One function, \texttt{create\_metadata}, is responsible for parsing the API object into a clean dictionary. The second function, \texttt{save\_metadata}, handles the file I/O and saves the dictionary to disk.
        
        \subsubsection{Breakdown of the Process}        
        \begin{enumerate}
            \item \textbf{Data Parsing (\texttt{create\_metadata(paper)}):} This function acts as the core parser. It takes the \texttt{arxiv.Result} object and extracts all required fields:
            \begin{itemize}
                \item It splits the ID (e.g., \texttt{2305.00633v4}) into its base ID (\texttt{2305.00633}) and latest version (\texttt{4}).
                \item It formats authors into a simple list of names and correctly formats the submission and revised dates.
                \item It generates a list of PDF URLs for \textit{all} versions of the paper, from \texttt{v1} to the latest.
                \item It extracts the title, abstract, categories, and optional fields like DOI and publication venue (from the comments).
                \item It returns a single, clean Python dictionary.
            \end{itemize}
            \item \textbf{Saving (\texttt{save\_metadata(paper, folder)}):} This function orchestrates the saving process:
            \begin{itemize}
                \item It first calls \texttt{create\_metadata} to get the structured dictionary.
                \item It ensures the target save folder (e.g., \texttt{.../2305-00633/}) exists.
                \item It writes the dictionary to \texttt{metadata.json} using UTF-8 encoding and pretty-printing (\texttt{indent=4}) for human readability.
            \end{itemize}
        \end{enumerate}
        \subsubsection{Final Output}        
        The result of this step is a single, clean \texttt{metadata.json} file placed in the main directory for each paper, containing all of its metadata and version history.
        \pagebreak
        \subsection{Step 3 - Full-Text Source Download \& Processing}        
        This step fetches the actual \LaTeX source code for every version of a paper, extracts the archive, and cleans it to meet the project's requirements.
        \subsubsection{The Core Problem}        
        The assignment requires the full \LaTeX source, not just the metadata. This involves two main challenges:
        \begin{enumerate}
            \item \textbf{Completeness:} The source code for \textit{all} versions of the paper must be downloaded, not just the latest.
            \item \textbf{Cleaning:} The raw \texttt{.tar.gz} source archives contain many non-text files, such as large images (\texttt{.png}, \texttt{.jpg}, \texttt{.pdf}). These must be removed to isolate the text source and reduce data size.
        \end{enumerate}
        \subsubsection{The Solution}        
        The pipeline implements a robust "Download-Extract-Clean" workflow. This process is repeated for every version of every paper. It uses helper functions to ensure files are extracted safely and that all non-essential files are subsequently deleted.
        \subsubsection{Breakdown of the Process}        
        \begin{enumerate}
            \item \textbf{Orchestration (\texttt{download(...)}):} This main function iterates through the list of \texttt{arxiv.Result} objects for a paper (one for each version).
            \item \textbf{URL Generation \& Download:} For each version (e.g., \texttt{2304.07856v2}), it:
            \begin{itemize}
                \item Creates a version-specific subfolder (e.g., \texttt{.../tex/2304.07856v2/}).
                \item Constructs the direct download URL (e.g., \texttt{https://arxiv.org/src/2304.07856v2}).
                \item Downloads the \texttt{.tar.gz} archive into that folder using the \texttt{download\_url} helper.
            \end{itemize}
            \item \textbf{Validation \& Safe Extraction:}
            \begin{itemize}
                \item The script first validates the downloaded file is a genuine \texttt{.tar.gz} archive.
                \item It then uses \texttt{safe\_extract\_tar} to extract the contents. This function is critical for security and stability, as it sanitizes filenames and explicitly skips dangerous files like symbolic links or files with path traversal (\texttt{../}).
            \end{itemize}
            \item \textbf{Content Cleanup (\texttt{cleanup\_non\_tex\_bib\_files}):} After successful extraction, this function is called. It scans the new folder and \textbf{deletes every file} that does not have a \texttt{.tex} or \texttt{.bib} extension, effectively removing all images and other non-text content.
            \item \textbf{Finalization:} The original \texttt{.tar.gz} file is deleted to conserve disk space.
        \end{enumerate}
        
        \subsubsection{Final Output}        
        The result is a clean directory structure (e.g., \texttt{.../2304-07856/tex/}) containing a subfolder for each paper version. Each subfolder contains \textit{only} the \texttt{.tex} and \texttt{.bib} source files for that version.
        % --- END CONTENT FROM download.tex ---
        
        % --- BEGIN CONTENT FROM reference_crawl.tex ---
       \subsection{Step 4 - Citation Network Extraction}        
        This final step builds the \texttt{references.json} file by identifying all of a paper's citations that are \textit{also} available on arXiv.
        
        \subsubsection{The Core Problem}        
        The arXiv API does not provide citation data. An external source is required. The challenge is to query this source (Semantic Scholar), parse its complex response, and then filter a large bibliography to find \textit{only} the references that are explicitly linked to an arXiv ID, ignoring all other journal or conference citations.
        
        \subsubsection{The Solution}        
        The pipeline uses the Semantic Scholar Graph API, which indexes papers and their citations, including \texttt{externalIds} like arXiv IDs. The solution involves querying this API, then iterating over the results to filter and restructure the data.
        
        \subsubsection{Breakdown of the Process}
        \begin{enumerate}
            \item \textbf{API Query (\texttt{get\_paper\_references(arxiv\_id)}):} This function fetches the raw data.
            \begin{itemize}
                \item It constructs the Semantic Scholar API URL (e.g., \texttt{.../paper/arXiv:2304.07856}).
                \item It specifically requests the \texttt{references} field, along with sub-fields like \texttt{references.externalIds}, \texttt{references.title}, \texttt{references.authors}, and \texttt{references.year}.
                \item It includes robust error handling, especially for API rate limits (HTTP 429) and papers not found (HTTP 404).
            \end{itemize}
            \item \textbf{Filtering \& Structuring (\texttt{convert\_to\_references\_dict(...)}):} This is the core filtering logic.
            \begin{itemize}
                \item It iterates through the full list of references returned by the API.
                \item For each reference, it inspects the \texttt{externalIds} dictionary.
                \item \textbf{If, and only if, a key named \texttt{'ArXiv'} exists} in \texttt{externalIds}, the reference is kept. All other references are discarded.
                \item For the kept references, it extracts the title, author names, and publication date/year.
                \item It formats this data into a new dictionary, where the \textbf{key is the cited paper's arXiv ID} (e.g., \texttt{2210.07675}).
            \end{itemize}
            \item \textbf{Saving (\texttt{save\_references(...)}):} This function orchestrates the process. It calls the fetch and convert functions, then saves the final, structured dictionary to \texttt{references.json}. If no arXiv-to-arXiv citations are found, it correctly saves an empty dictionary (\texttt{\{\}}).
        \end{enumerate}
        
        \subsubsection{Final Output}        
        The result of this step is a \texttt{references.json} file in the paper's main directory. This file contains a dictionary mapping the arXiv IDs of cited papers to their metadata, providing a clean list of the paper's arXiv-to-arXiv citations.
        % --- END CONTENT FROM reference_crawl.tex ---
        \subsection{Step 5 - Missing Paper Recovery}        
        This is a crucial data integrity step executed after the main multi-threaded download process is complete.
        
        \subsubsection{The Core Problem}        
        Due to the concurrent nature of the pipeline and reliance on external APIs, transient errors such as network timeouts, API rate limits (HTTP 429), or other temporary disruptions can cause some papers to be skipped during the initial run. Without a verification step, the final dataset would be incomplete.
        
        \subsubsection{The Solution}        
        The solution is a verification and recovery process. This process compares the initial list of \textit{expected} paper IDs (from Step 1) against the list of paper IDs \textit{actually} present in the data directory, then re-queues only the missing ones for processing.
        
        \subsubsection{Breakdown of the Process}
        \begin{enumerate}
            \item \textbf{Collect Existing IDs (\texttt{collect\_existing\_ids}):}
            This function scans the base data directory (e.g., \texttt{../data/}). It reads the names of all subfolders (e.g., \texttt{2304-07856}, \texttt{2304-07857}) and uses a regular expression to parse them. It returns a dictionary mapping each month (e.g., \texttt{2304}) to a set of ID numbers (e.g., \texttt{\{7856, 7857, ...\}}) that have been successfully downloaded.
            
            \item \textbf{Find Missing IDs (\texttt{find\_missing\_ids}):}
            This function takes a target month (e.g., \texttt{2304}), the set of existing IDs for that month, and the expected start and end ID numbers for that month. It compares the set of expected IDs against the set of existing IDs and returns a sorted list of the missing ID numbers.
            
            \item \textbf{Recover Missing Papers (\texttt{recover\_missing\_papers}):}
            This is the main orchestration function for this step. It iterates through each month in the assignment's range. For each month, it calls \texttt{find\_missing\_ids} to get a list of what's missing. It then puts these missing paper IDs back into the download and reference queues to be processed by the worker threads, just like the initial batch.
        \end{enumerate}
        
        \subsubsection{Final Output}        
        The output of this step is a complete dataset. It ensures that every paper within the target range has been successfully downloaded and processed, even if it failed on the first attempt.
        
        \subsection{Step 6 -- Performance Monitoring (Benchmarking)}

        This final step is dedicated to monitoring and reporting the performance of the entire scraping pipeline, ensuring transparency, reproducibility, and providing actionable insights for future optimization.
        
        \subsubsection{The Core Problem}
        
        Large-scale data collection tasks can be resource-intensive and time-consuming. Without systematic monitoring, it is difficult to assess the efficiency of the pipeline, identify bottlenecks, or estimate the computational resources required for future runs. Accurate benchmarking is essential for both reproducibility and scalability.
        
        \subsubsection{The Solution}
        
        The solution is to integrate a comprehensive benchmarking system that tracks key performance metrics throughout the pipeline’s execution. This system records timing, memory usage, and disk usage at multiple points, and summarizes the results at the end of the process, providing a clear overview of resource consumption and efficiency.
        
        \subsubsection{Breakdown of the Process}
        \begin{enumerate}
            \item \textbf{Initialization:} \\
            At the very start of the main script, a \texttt{Benchmark} object is created. Its initialization method records the current time as the \texttt{start\_time} and prepares containers for tracking RAM, disk usage, and per-step timings.
            
            \item \textbf{RAM Monitoring:} \\
            The pipeline periodically samples the memory usage of the Python process using \texttt{psutil}. Each sample is appended to a list, allowing calculation of both the maximum and average RAM usage over the entire run. Additionally, a system-wide RAM usage function is available for more comprehensive diagnostics, using either \texttt{/proc/meminfo} (on Linux) or \texttt{psutil.virtual\_memory()} as a fallback.
            
            \item \textbf{Disk Usage Tracking:} \\
            The pipeline tracks the peak disk usage by recursively summing the sizes of all files in the output directory at various points during execution. This allows the benchmark to report both the maximum disk usage observed during runtime and the final disk usage at completion.
            
            \item \textbf{Step Timing:} \\
            The benchmark records the time spent in key phases: entry discovery (\texttt{id\_fetch\_time}), downloading papers (\texttt{download\_times}), and extracting references (\texttt{reference\_times}). For downloads and reference extraction, per-paper timings are stored, enabling calculation of average times per paper for each step.
            
            \item \textbf{Reporting:} \\
            After all pipeline steps, including any recovery operations, are complete, the \texttt{report} method is called. This method:
            \begin{itemize}
                \item Calculates the total elapsed time (\texttt{total\_time}) since initialization.
                \item Reports the time spent on entry discovery, average download time per paper, and average reference extraction time per paper.
                \item Reports the maximum and average RAM usage of the Python process.
                \item Reports the peak disk usage observed during runtime and the final disk usage at completion.
                \item Reports the total number of papers processed.
            \end{itemize}
        \end{enumerate}
        
        \subsubsection{Final Output}
        
        The output of this step is a detailed performance report, including total runtime, per-step timings, peak and average memory usage, peak and final disk usage, and the total number of papers processed. This information is invaluable for evaluating the efficiency of the pipeline, planning future data collection efforts, and ensuring the reproducibility and scalability of results.
   
        % --- BEGIN CONTENT FROM statist.tex ---
       
        % --- END CONTENT FROM statist.tex ---
        \section{Performance Report (10 Papers)}
        
        \begin{itemize}
            \item \textbf{Test batch size:} 10 papers\\
            The pipeline was evaluated on a batch of 10 arXiv papers to benchmark performance and resource usage.
            
            \item \textbf{Total runtime:} 42.45 seconds\\
            The complete scraping process, from entry discovery to final data output, was completed in just over 42 seconds.
            
            \item \textbf{Time for entry discovery:} 1.94 seconds\\
            The initial identification of valid arXiv IDs was performed rapidly, demonstrating the efficiency of the ID discovery logic.
            
            \item \textbf{Average download time per paper:} 6.83 seconds\\
            Each paper, including all available versions, was downloaded in under 7 seconds on average, reflecting effective parallelization and network utilization.
            
            \item \textbf{Average reference extraction time per paper:} 1.03 seconds\\
            Reference metadata extraction, including API calls and parsing, averaged just over 1 second per paper.
            
            \item \textbf{Python process max RAM usage:} 113.66 MB\\
            The peak memory usage of the Python process remained low, indicating efficient memory management throughout the run.
            
            \item \textbf{Python process average RAM usage:} 112.68 MB\\
            The average memory footprint was stable, with minimal fluctuation during processing.
            
            \item \textbf{Peak disk usage during runtime:} 24.60 MB\\
            The maximum disk space required during the scraping process, including temporary and intermediate files, was under 25 MB.
            
            \item \textbf{Final disk usage:} 3.30 MB\\
            After completion and cleanup, the total disk space occupied by the final dataset was only 3.3 MB, reflecting efficient storage and removal of temporary files.
            
            \item \textbf{Total papers processed:} 10\\
            All 10 papers in the test batch were successfully processed, with complete metadata and references extracted.
        \end{itemize}
        \begin{figure} [H]
            \centering
            \includegraphics[width=0.9\linewidth]{img/image.png}
            \caption{Performance with 10 papers}
            \label{fig_label_with_H}
        \end{figure}

    \section{Task distribution}
    % Please add the following required packages to your document preamble:
% \usepackage{multirow}
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|l|l|c|}
    \hline
    \textbf{MSSV} &
      \multicolumn{1}{c|}{\textbf{Name}} &
      \multicolumn{1}{c|}{\textbf{Tasks}} &
      \textbf{Progress} \\ \hline
    \multirow{3}{*}{23127130} &
      \multicolumn{1}{c|}{\multirow{3}{*}{Nguyễn Hữu Anh Trí}} &
      \begin{tabular}[c]{@{}l@{}}Handling the format and collecting \\ ArXiv papers' IDs in given range\end{tabular} &
      \textbf{100\%} \\ \cline{3-4} 
     &
      \multicolumn{1}{c|}{} &
      \begin{tabular}[c]{@{}l@{}}Downloading and extracting Source \\ of the papers\end{tabular} &
      \textbf{100\%} \\ \cline{3-4} 
     &
      \multicolumn{1}{c|}{} &
      \begin{tabular}[c]{@{}l@{}}Writing READ.ME for project overview \\ and instructions on running the program\end{tabular} &
      \textbf{100\%} \\ \hline
    \multirow{3}{*}{23127051} &
      \multirow{3}{*}{Cao Tấn Hoàng Huy} &
      \begin{tabular}[c]{@{}l@{}}Handing the metadata's structure and \\ collecting metadata for papers\end{tabular} &
      \textbf{100\%} \\ \cline{3-4} 
     &
       &
      \begin{tabular}[c]{@{}l@{}}Building a function that recovers the \\ missing papers after finishing process.\end{tabular} &
      \textbf{100\%} \\ \cline{3-4} 
     &
       &
      \begin{tabular}[c]{@{}l@{}}Writing Report for explaining pipeline \\ and evaluating the result\end{tabular} &
      \textbf{100\%} \\ \hline
    \multirow{3}{*}{23127107} &
      \multirow{3}{*}{Nguyễn Huy Quân} &
      \begin{tabular}[c]{@{}l@{}}Handling the Semantic Scholar requests \\ to download references\end{tabular} &
      \textbf{100\%} \\ \cline{3-4} 
     &
       &
      \begin{tabular}[c]{@{}l@{}}Building a class for calculating the RAM \\ and time of the process\end{tabular} &
      \textbf{100\%} \\ \cline{3-4} 
     &
       &
      \begin{tabular}[c]{@{}l@{}}Making a demo Video that explains the \\ pipeline and  tests the source code\end{tabular} &
      \textbf{100\%} \\ \hline
    \end{tabular}
    \label{fig_label_with_H}
    \caption{Table of work distribution.}
    \end{table}
            
    \section{Pipeline Resources and Demonstration}

    \subsection{Source Code Repository}
    The complete source code for the arXiv scraping pipeline, including all modules, utilities, and documentation, is publicly available on GitHub:
    \begin{itemize}
        \item \textbf{GitHub Repository:} \url{https://github.com/AnhTtis/Data_Science_Project}
    \end{itemize}
    The repository contains detailed instructions for installation, configuration, and usage, as well as example scripts and test cases.
    
    \subsection{Demonstration Video}
    A step-by-step demonstration of the pipeline in action, including setup, execution, and key features, is available on YouTube:
    \begin{itemize}
        \item \textbf{YouTube Video:} \url{https://youtu.be/MWt4aQyfHkc}
    \end{itemize}
    The video walkthrough highlights the main components of the system, showcases performance monitoring, and provides practical tips for running large-scale data collection tasks.
    
    \subsection{Summary}
    These resources provide both the technical foundation and a visual overview of the pipeline, enabling reproducibility and facilitating further development or adaptation by the research community.
        % --- END CONTENT FROM evaluation.tex ---

    \section{Reference}
    \renewcommand{\refname}{}
    \begin{filecontents}{\jobname.bib}
    @misc{arxiv_api_basics_2025,
      title        = {{arXiv API Basics}},
      author       = {{arXiv}},
      howpublished = {\url{https://info.arxiv.org/help/api/basics.html}},
      year         = {2025},
      urldate      = {2025-11-17},
      note         = {Accessed: 2025-11-17}
    }
    @misc{arxiv_api_users_manual_2025,
      title        = {{arXiv API User’s Manual}},
      author       = {{arXiv}},
      howpublished = {\url{https://info.arxiv.org/help/api/user-manual.html}},
      year         = {2025},
      urldate      = {2025-11-17},
      note         = {Accessed: 2025-11-17}
    }
    
    @misc{semanticscholar_snippet_text_2025,
      title        = {Semantic Scholar API — Snippet Text},
      author       = {Semantic Scholar},
      howpublished = {\url{https://api.semanticscholar.org/api-docs/graph#tag/Snippet-Text}},
      year         = {2025},
      urldate      = {2025-11-17},
      note         = {Accessed: 2025-11-17}
    }
    \end{filecontents}
    \bibliographystyle{plain}  % style for BibTeX entries
    \bibliography{\jobname}  
\end{document}