\section{Conclusions}
\label{sec:conclusions}
In this paper, a two-stage teacher-student framework has been proposed, where a student neural transducer ASR model distils the knowledge either from one or multiple complementary SSL pre-trained speech foundation models. In the first stage, the student ASR encoder is trained to approximate the embeddings generated by one or multiple teacher encoders without using the ground truth labels. In the second stage, the entire student model is fine-tuned with paired audio-text data, where the paired texts can be generated either by human annotation or by existing teacher ASR models. On LibriSpeech 100h, the averaged WER of a non-streaming student model trained with a single teacher is 11\% relative lower than that trained from scratch and using the multi-teacher setup further increases the relative WER reduction to 14\%. The proposed KD framework is also effective for streaming neural transducers using an additional time-delay factor, which is to resolve the emission mismatch between the non-streaming teacher and the streaming student. The proposed KD framework is also complementary to existing KD methods, leading to further performance improvement in combination. Further WER reductions can be achieved when scaling up the amount of unlabelled data used in the first stage. The best-performing student is obtained under a multi-teacher setup with extra unlabelled data, resulting relative WERR of 55\%.