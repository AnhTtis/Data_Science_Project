\begin{abstract}
Although large foundation models pre-trained by self-supervised learning have achieved state-of-the-art performance in many tasks including automatic speech recognition (ASR), knowledge distillation (KD) is often required in practice to transfer the knowledge learned by large teacher models into much smaller student models with affordable computation and memory costs. This paper proposes a novel two-stage KD framework to distil the knowledge from multiple speech foundation models as teachers into a single student neural transducer model for ASR. In the first stage, the student model encoder is pre-trained using the embeddings extracted from multiple teacher models. In the second stage, the student encoder is fine-tuned with the audio-text pairs based on the ASR task. Experiments on the LibriSpeech 100-hour subset show that the proposed KD framework improves the performance of both streaming and non-streaming student models when using only one teacher. The performance of the student model can be further enhanced when multiple teachers are used jointly, achieving word error rate reductions (WERRs) of 17.5\% and 10.6\%. Our proposed framework can be combined with other existing KD methods to achieve further improvements. Further WERRs were obtained by incorporating extra unlabelled data during encoder pre-training, leading to a total relative WERR of 55.0\% on the non-streaming student model.
%Self-supervised pre-trained models are achieving state-of-the-art performance in automatic speech recognition (ASR). Despite of their good performance, their large sizes make them inapplicable in real-life scenarios. To benefit from the self-supervised pre-training while keeping the model small, various knowledge distillation (KD) methods have been proposed to improve a small ASR model's performance by transferring knowledge from self-supervised pre-trained models. However, most of these methods focuses on only one teacher model instead of multiple teacher models, which could lead to bigger performance improvement due to the increase of information source. In this work, a simple 2-stage KD framework is proposed to jointly distil knowledge from multiple self-supervised models to a single student neural transducer model. In the first stage, the student model's encoder is pre-trained using only the acoustic embeddings extracted from different teacher models. The well-trained encoder is then fine-tuned with audio-text pairs in the second stage to perform ASR tasks.  Experiments on LibriSpeech 100 hour shows that the proposed KD framework improves the performance of both streaming and non-streaming student model when using only a single teacher. The performance of the student model is further improved by under a multi-teacher setup, achieving word-error-rate reductions (WERR) of xx\% and xx\%. We also show that it can be combined with existing KD methods to achieve even larger performance gain. By incorporating extra unlabelled data during encoder pre-training, further WERR can be observed, leading to a WERR of xx\% on non-streaming student model.
\end{abstract}

\begin{IEEEkeywords}
Foundation model, teacher-student training, multi-teacher knowledge distillation, neural transducer, ASR
\end{IEEEkeywords}