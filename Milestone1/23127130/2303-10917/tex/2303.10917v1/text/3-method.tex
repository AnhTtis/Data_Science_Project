\section{Multi-Teacher Knowledge Distillation for Neural Transducers}
\label{sec:multi-teacher KD}

\subsection{Motivation}

The quality of the teacher model is a critical factor in KD training, since the resulting student model tends to have better performance with labels generated by better teacher models. Using an ensemble of multiple models is a common strategy to improve the teacher quality \cite{dietterich2000ensemble, sahraeian2018cross} and various attempts have been made to distil the knowledge from an ensemble of teacher models \cite{sau2016deep, zhao2020highlight}. 

SSL pre-training utilises the richness of unlabelled data and helps foundation models learn better feature representations. Good performance can be achieved when fine-tuning with even a small amount of labelled data. The data representations learned through different pre-training tasks could be very different, and maybe even complementary to each other. Therefore, it is desirable to use multiple SSL pre-trained foundation models to leverage their complementarity. In practice, it is reasonable to have multiple foundation models with similar performance, which can have different model structures or be trained on different datasets.
In this work, we propose a two-stage multi-teacher KD framework, where a student transducer model learns from multiple teacher foundation models at the same time. In the first stage, the encoder of the student transducer model is learned to regress the teacher models' embeddings. Extra unlabelled speech can be incorporated at this stage as no ground truth transcriptions are required. In the second stage, a neural transducer is initialised from the encoder in stage one, which will be then fine-tuned with audio-text pairs using the RNNT loss. Existing KD methods can be combined during the fine-tuning stage for performance improvement. 
%Furthermore, since stage one only requires unlabelled speech, the proposed KD framework can utilise the richness of unlabelled speech for further performance improvement.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figs/pipeline.pdf}
    \caption{The proposed 2-stage KD framework. The left part describes the first stage: encoder pre-training. The teacher model receives raw waveform as input and generates embeddings. The right part is the second stage of training, where the student model is initialised from the well-trained encoder in the first stage and fine-tuned with audio-text pair.}
    \label{fig:framework}
\end{figure*}
\vspace{-0.2em}
\subsection{Encoder pre-training}
\label{sec:encoder pretraining}

%The encoder of a neural transducer is commonly believed to be the most import component as it covers the majority of the total number of parameters. 
The encoder of a neural transducer often has the majority of the total number of parameters. Therefore, we propose to carry out KD on encoder embeddings. The proposed KD training framework consists of two stages. In the first stage, only the encoder of the student model is considered while the decoder and joint network remain frozen. Assuming the number of teacher models to be $N$ and the encoder output dimension of each teacher to be $D^{\mathcal{T}_n}$. Given the same input acoustic features $\bm{X}$, the encoder of $n$-th teacher model generates $\mathcal{E}^{\mathcal{T}_n}=\bm{e}^{\mathcal{T}_n}_1,...,\bm{e}^{\mathcal{T}_n}_T$ as output. The encoder of the student model also generates embeddings $\mathcal{E}^{\mathcal{S}}=\bm{e}^{\mathcal{S}}_1,...,\bm{e}^{\mathcal{S}}_T$ of dimension $D^{\mathcal{S}}$. During training, the student attempts to regress the teacher embeddings $\mathcal{E}_n, n=1,...,N$ by sampling one teacher embedding for each training utterance in the mini-batch. Assuming the $n$-th teacher model is selected, the loss function for a training sample is as follows:
\begin{align}
    \mathcal{L}_{\text{KD}} = \frac{1}{T} \sum_{t=1}^T \textsc{Dist}(\textsc{LossNet}(\bm{e}^{\mathcal{S}_t}), \bm{e}^{\mathcal{T}_n}_t),
    \label{eqn:KD loss}
\end{align}
where $\textsc{LossNet}(\cdot)$ is a linear transformation that maps the student vector from $D^{\mathcal{S}}$ to $D^\mathcal{T}_n$ and $\textsc{Dist}(\cdot)$ is any function that measures the distance between two vectors of the same dimension, such as the commonly used $L_1$ and $L_2$ distances. Since KD is carried out at the encoder level, the computational and storage costs for encoder embedding level pre-training are independent of the output vocabulary size $V$, which alleviates the computation issue in distribution-based KD approaches for neural transducers\cite{panchapagesan2021efficient, yang2022knowledge}.

% Different sampling mechanisms for choosing the target teacher model can be applied. The simplest sampling method would be assigning the same probability, i.e $p_{n}=1/N$, to all $N$ teachers. However, this sampling method treats all teacher models equally, ignoring the performance difference between different teachers. Therefore, an improved sampling method can be applied:
% \begin{align}
%     % p_{n} = \frac{N-1}{N} \frac{\sum_{k=1,k\neq n }^{N} \text{WER}_{n}}{\sum_{k=1}^{N} \text{WER}_{n}},
%     p_{n} = \frac{1}{N-1} \frac{\sum_{k=1,k\neq n }^{N} \text{WER}_{n}}{\sum_{k=1}^{N} \text{WER}_{n}},
% \end{align}
% where $\text{WER}_n$ is the $n$-th teacher model's WER on a pre-defined test set. Intuitively, teacher models with lower WERs will be more likely to be chosen than those with higher WERs.

% The selection of the teacher model can also be determined by the similarity between the teacher and student embeddings. If a student model's embedding is ``closer'' to one of the embeddings from the teacher pool, the student model would be able to adapt to that specific teacher model more easily. The probability of selecting the $i$-th teacher model can be expressed as:
% \begin{align}
%     p_i =  \frac{\textsc{Sim}(\mathcal{E}^{\mathcal{T}_i}, \mathcal{E}^{\mathcal{S}})}{\sum_{n=1}^{N} \textsc{Sim}(\mathcal{E}^{\mathcal{T}_n}, \mathcal{E}^{\mathcal{S}})}
%     \label{eqn:sim prob}
% \end{align}
% where $\textsc{Sim}$ is a normalised function that returns a higher value if two vectors are similar to each other. According to \eqndot{sim prob}, the teacher model whose embedding is more similar to the student embedding is more likely to be chosen. % as teacher targets.

Streaming models tend to emit symbols later due to the lack of future context. Therefore, directly applying \eqndot{KD loss} to streaming student models can be problematic. Inspired by \cite{yang2022knowledge}, a time-delay factor $\tau$ is adopted during the first stage of training for streaming student models. The $t$-th frame of $n$-th teacher model's embedding will be aligned to the $\left(t+\tau \right)$-th frame of the student model. As a consequence, the last $\tau$ frames in the teacher model are discarded as no corresponding frames exist in the student model. 
This leads to a modified version of the distillation loss:
\begin{align}
    \mathcal{L}_{\text{KD}} = \frac{1}{T} \sum_{t=1}^{T-\tau} \textsc{Dist}(\textsc{LossNet}\left(\bm{e}^{\mathcal{S}}_t\right), \bm{e}^{\mathcal{T}_n}_{t+\tau}).
    \label{eqn:KD loss streaming}
\end{align}
Note that due to the introduction of the time-delay factor, the last $\tau$ frames of the student embedding have to be discarded during KD loss computation as no teacher embeddings exist for them. Intuitively, $\tau$ controls the trade-off between emission latency and model performance: a larger $\tau$ allows the student model to have more future contexts but increases emission delay and vice versa. However, a recent method applying this idea claims that the latency of the student model also decreases while the recognition accuracy also improves \cite{guo2022predicting}. With a carefully tuned $\tau$, this could be because the fixed time delay not only provides good supervision but also enforces the student model to emit earlier.

\subsection{Supervised fine-tuning}

In the second stage, the encoder pre-trained with embeddings from different teacher models in stage 1 is adopted as the initialisation of a student transducer model's encoder. The decoder and the joint network are randomly initialised. If the encoder is sufficiently trained, it should be able to accurately reproduce the output of the teacher model given the same input features. Therefore, this encoder can be treated as a weaker approximation to the self-supervised pre-trained model. The encoder-initialised student model is then fine-tuned with labelled speech to perform ASR tasks. To avoid catastrophic forgetting, a tri-stage learning rate schedule is used so that the model does not quickly depart from the initialisation. During the first warm-up stage, the learning rate increases linearly from a very small value to a warm-up learning rate. The learning rate keeps the same fixed value during the second stage and then decreases linearly to the final learning rate. The RNNT loss \cite{graves2012RNNT} is used to train the whole student model. Note that there is a fundamental difference between the proposed KD framework with existing embedding-level KD methods. Our framework splits the embedding learning and ASR training into two distinct processes whilst other methods perform KD in a multi-task fashion by treating KD as an auxiliary task during ASR training. An illustration of the proposed 2-stage KD framework is shown in \figdot{framework}

\section{Combination with other KD methods}

The proposed KD method performs distillation only using the output of the encoder, which does not involve the predictor and the joint network in the student neural transducer. Therefore, it is likely to be compatible with distribution-based KD methods as they focus on a different part of a transducer model. Here, we use the 1-best alignment KD \cite{yang2022knowledge} in addition to the proposed encoder embedding level KD. Under the single-teacher scenario, the 1-best loss can be added during supervised fine-tuning as an auxiliary loss to the normal RNNT loss. However, modifications need to be made when integrating the 1-best alignment-based KD under a multi-teacher KD setup. In this paper, we extend the 1-best alignment KD to n-best alignment KD by using the 1-best alignment from each individual teacher as the teacher label. By doing so, the loss function of n-best alignment KD becomes a weighted sum of $N$ 1-best KD loss defined in \eqndot{1best alignment}:
% Assuming the number of teacher model to be $N$, each teacher model generates its 1-best alignmentresulting in $N$ 1-best alignments. The student model can be trained in a multitask fashion by using the sum of individual one-best loss for different alignments:
\begin{align}
    \mathcal{L}_{\text{nBest}} =  \sum_{n=1}^{N} \omega_n \mathcal{L}^{n}_{\text{1Best}}
\end{align}
where $\omega_n$ is the weight of the alignment decoded from the $n$-th teacher model and it holds $\sum_{n=1}^{N} \omega_n = 1$.


This auxiliary loss is interpolated with the original RNNT loss during the second stage of training, resulting in the final loss function:
\begin{align}
    \mathcal{L}_{\text{final}} = \mathcal{L}_{\text{RNNT}} + \lambda \mathcal{L}_{\text{nBest}},
\end{align}
where $\lambda$ is a tunable coefficient controlling the contribution of the auxiliary loss.