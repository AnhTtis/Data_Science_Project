\section{Ablations}
\label{sec:ablations}

\subsection{Teacher Model Selection Strategy}

In this section, three teacher model selection strategies are compared. The first strategy is uniform sampling, i.e selecting each teacher with probability $p_n=\frac{1}{N}$. This strategy is also adopted in this work. However, this sampling method treats all teacher models equally, ignoring the performance difference between different teachers. To consider this, another sampling strategy can be applied:
\begin{align}
    % p_{n} = \frac{N-1}{N} \frac{\sum_{k=1,k\neq n }^{N} \text{WER}_{n}}{\sum_{k=1}^{N} \text{WER}_{n}},
    p_{n} = \frac{1}{N-1} \frac{\sum_{k=1,k\neq n }^{N} \text{WER}_{n}}{\sum_{k=1}^{N} \text{WER}_{n}},
\end{align}
where $\text{WER}_n$ is the $n$-th teacher model's WER on a pre-defined test set. Intuitively, teacher models with lower WERs will be more likely to be chosen than those with higher WERs. The third strategy assigns sampling probabilities by comparing the similarity between the student embedding and teacher embedding:
\begin{align}
    p_i =  \frac{\textsc{Sim}(\mathcal{E}^{\mathcal{T}_i}, \mathcal{E}^{\mathcal{S}})}{\sum_{n=1}^{N} \textsc{Sim}(\mathcal{E}^{\mathcal{T}_n}, \mathcal{E}^{\mathcal{S}})}
    \label{eqn:sim prob}
\end{align}
where $\textsc{Sim}$ is a normalised function that returns a higher value if two vectors are similar to each other. Intuitively, the teacher model whose embedding is more similar to the student embedding is more likely to be chosen.

\begin{table}[!ht]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        \multirow{2}{*}{Teacher} & \multirow{2}{*}{\shortstack[c]{Sampling\\ method}}  & \multicolumn{2}{c}{dev}   & \multicolumn{2}{c}{test}  \\
        \cmidrule(lr){3-4}  \cmidrule(lr){5-6} &
        & \multicolumn{1}{c}{clean} & \multicolumn{1}{c}{other} & \multicolumn{1}{c}{clean} & \multicolumn{1}{c}{other} \\ 
        \midrule
        
        \multirow{3}{*}{W2v2+Hu} & uniform & 6.4 & 19.6 & 6.6 & 20.0 \\
        & WER & 6.4 & 19.6 & 6.6 & 20.0 \\
        & similarity & 6.6 & 20.0 & 6.8 & 20.4 \\
        \midrule
        \multirow{3}{*}{WL+Hu} & uniform  & 6.2 & 19.2 & 6.6 & 19.5 \\
        & WER & 6.2 & 19.3 & 6.6 & 19.5 \\
        & similarity & 6.4 & 19.9 & 6.8 & 20.2 \\
        
        \bottomrule
    \end{tabular}
    \caption{\%WERs for non-streaming student transducer models trained with different teacher model selection strategies under a multi-teacher setup.}
    \label{tab:teacher model selection 100h}
\end{table}


The results of three teacher model selection strategies described above are shown in \tbl{teacher model selection 100h}. The cosine similarity averaged over a number of acoustic frames was used in the similarity-based strategy. Two multi-teacher setups (W2v2+Hu, WL+Hu) were compared. In WER-based probability assignment strategies, the mean WER of dev-clean and dev-other was used. Experiments were carried out on the ``train-clean-100'' set. Sampling teacher models uniformly and assigning probabilities to teacher models based on their WERs yield almost the same performance. This could be explained by the small WER differences between different teacher models.  Choosing teacher labels by comparing the similarity between teacher embeddings and student embeddings yields the weakest performance. After a few epochs during training, it is observed that the selection of teacher labels converges slowly to one specific teacher. Then, the system degrades from multi-teacher to single-teacher, thus reducing the diversity of teacher labels and undermining KD training. 

In the future, we would like to further explore the WER-related sampling strategy. After vast amount of pre-training, the large foundation models all achieve very good performance after sufficient fine-tuning. The small WER differences between them make the WER-related sampling strategy very similar to uniform sampling. To simulate scenarios where teacher models have very different qualities, teacher models can be constructed by deliberately reducing the fine-tuning steps. This should help us investigate if a more sophisticated sampling strategy is better than uniform sampling.

% Different sampling mechanisms for choosing the target teacher model can be applied. The simplest sampling method would be assigning the same probability, i.e $p_{n}=1/N$, to all $N$ teachers. However, this sampling method treats all teacher models equally, ignoring the performance difference between different teachers. Therefore, an improved sampling method can be applied:
% \begin{align}
%     % p_{n} = \frac{N-1}{N} \frac{\sum_{k=1,k\neq n }^{N} \text{WER}_{n}}{\sum_{k=1}^{N} \text{WER}_{n}},
%     p_{n} = \frac{1}{N-1} \frac{\sum_{k=1,k\neq n }^{N} \text{WER}_{n}}{\sum_{k=1}^{N} \text{WER}_{n}},
% \end{align}
% where $\text{WER}_n$ is the $n$-th teacher model's WER on a pre-defined test set. Intuitively, teacher models with lower WERs will be more likely to be chosen than those with higher WERs.

% The selection of the teacher model can also be determined by the similarity between the teacher and student embeddings. If a student model's embedding is ``closer'' to one of the embeddings from the teacher pool, the student model would be able to adapt to that specific teacher model more easily. The probability of selecting the $i$-th teacher model can be expressed as:
% \begin{align}
%     p_i =  \frac{\textsc{Sim}(\mathcal{E}^{\mathcal{T}_i}, \mathcal{E}^{\mathcal{S}})}{\sum_{n=1}^{N} \textsc{Sim}(\mathcal{E}^{\mathcal{T}_n}, \mathcal{E}^{\mathcal{S}})}
%     \label{eqn:sim prob}
% \end{align}
% where $\textsc{Sim}$ is a normalised function that returns a higher value if two vectors are similar to each other. According to \eqndot{sim prob}, the teacher model whose embedding is more similar to the student embedding is more likely to be chosen. % as teacher targets.