\section{Introduction}

\IEEEPARstart{A}{utomatic} speech recognition (ASR) is the task of mapping an input speech signal to its corresponding text sequence. %Traditional ASR systems usually consists of a hidden Markov model (HMM) based acoustic model, a language model and a pronunciation model, whereas an end-to-end (E2E) system fuses all components together and enables joint optimization\cite{graves2012RNNT, chan2015listen}.
%A traditional ASR system consists of multiple components, whereas an end-to-end (E2E) system implements all the components using a single model and achieves joint training \cite{graves2012RNNT, chan2015listen}.  
%With the increasing computing power and modern neural network architecture, 
%Therefore, E2E models have become the most popular modelling choice for ASR. 
An end-to-end (E2E) trainable ASR model implements this using a single trainable neural network such as neural transducers~\cite{graves2012RNNT,Graves2013} and attention-based encoder-decoder models~\cite{Bahdanau2015EndtoendAL,Lu2015,chan2015listen,Kim2017}.  
As a result, E2E trainable models have become the most prevalent ASR approach in both industry and academia. 

In real-world applications, the size of an E2E trainable ASR system is usually limited by computation and memory budgets. Knowledge distillation (KD)~\cite{hinton2015knowledgedistillation}, also known as teacher-student training, is a commonly used approach for model compression, 
from the original large model ("teacher") to  the compressed small model ("student").
%where the terms \textit{teacher} and \textit{student} refer to the original large model and compressed small model respectively. 
Instead of training the student purely using ground truth labels, KD also enables the use of the outputs of the teacher model, namely teacher labels. 
Different KD loss functions and training targets for KD to improve the performance of the student ASR model have been previously explored. For example, Takashima et al~\cite{takashima2018investigation} uses the final output distributions as the teacher labels and the Kullback-Leibler (KL) divergence as the KD loss together with the connectionist temporal classification (CTC) loss for ASR. 
In \cite{swaminathan2021codert} hidden layer representations or embeddings were extracted as the teacher labels for training the student ASR model. Unlike the parameter quantisation-based methods~\cite{hwang2014fixed, courbariaux2015binaryconnect} that usually require the teacher and student to share the same model structure, KD does not impose this constraint, making it a more flexible approach. Meanwhile, as ground-truth labels are not required, KD can be used as a semi-supervised training method to leverage a large amount of unlabelled data to improve the student~\cite{menghani2019learning,Li2017LargeScaleDA}. 
Furthermore, the student model can also be jointly trained using multiple teachers. Since the ensemble of multiple models can usually outperform a single model~\cite{zhou2021ensemble}, it can serve as a more reliable teacher during KD training. 
The weighted sum of the distributions output from multiple teachers is used as distillation targets in \cite{ensembledistribution}, leading to a larger performance gain compared to using only a single teacher. Wong et al~\cite{JeremyDecisionTree} ensembled the teachers with different output nodes obtained by decision tree clustering, and sequence-level KL divergence was also developed as an improved KD loss~\cite{Wong2016SequenceST}.  
In \cite{fukuda2017efficient}, one teacher from a teacher pool was selected in every mini-batch to generate the teacher labels.

Recently, pre-trained foundation models~\cite{bommasani2021foundation} obtained by self-supervised learning (SSL)~\cite{SSLloss} have achieved state-of-the-art performance on many speech processing tasks~\cite{baevski2020wav2vec,hsu2021hubert, chen2022wavlm,chen2020big,wang2021unispeech,kreyssig2022biased}. After being pre-trained on a large amount of unlabelled speech data, foundation models can be fine-tuned with the labelled data for specific downstream tasks, such as ASR~\cite{baevski2020wav2vec, hsu2021hubert}, speaker diarisation~\cite{chen2022wavlm,zheng2022tandem}, and emotion recognition~\cite{pepino2021emotion,Shen2020}  \textit{etc}. However, foundation models tend to be very large (e.g. from hundreds of millions~\cite{baevski2020wav2vec, hsu2021hubert} to even billions~\cite{hsu2021hubert} of model parameters) in order to fully leverage the richness and diversity of the unlabelled data during pre-training. This prevents their uses in resource-constrained scenarios, such as on-device streaming ASR, due to the large latency, computational footprint and inference cost. Therefore, it is of great interest to compress these large foundation models with as little performance degradation as possible. To this end, Peng et al~\cite{peng2021shrinking} projected the embeddings generated by a wav2vec 2.0 model~\cite{baevski2020wav2vec} to distributions and performed KD based on the KL divergence. Chang et al~\cite{chang2022distilhubert} proposed a multi-task KD framework by predicting embeddings extracted from different hidden layers of a HuBERT~\cite{hsu2021hubert} layer with multiple heads.

Due to use of various loss functions, model structures, and training data, embeddings generated by different foundation models can be very different and complementary. Therefore, further potential performance improvement could be achieved if a student model can learn from multiple foundation models. 
In this paper, we propose an efficient 2-stage distillation method to train student neural transducers for E2E ASR using the knowledge distilled from multiple foundation models as teachers. KD is carried out first at the embedding level and then at the hypothesis level. At the embedding level, the student jointly learns the embeddings from multiple teacher models using a regression loss. At the hypothesis level, the top two ASR hypotheses obtained by beam search (termed as \textit{1-best} and \textit{2-best}) can be used as the supervision in the transducer loss. 
The main contributions of this paper are summarised as follows:
\begin{enumerate}
    \item Proposes a two-stage KD framework which is effective both for streaming and non-streaming transducer models;
    \item Demonstrates that using multiple teachers in the proposed KD framework leads to lower student word error rates (WERs) compared to a single teacher;
    \item Shows that the proposed KD framework is complementary with existing KD methods for neural transducers, and lower student WERs can be achieved when incorporating more unlabelled data.
\end{enumerate}

In the rest of this paper, \sectdot{related} briefly reviews the background and related work.  %about neural transducer, SSL foundations models, and the existing KD methods for ASR. 
In \sectdot{multi-teacher KD}, details of the proposed two-stage KD framework are presented. % and how multi-teacher contribute to the KD training is described. 
The experimental setups and results are given in \twosectdot{exp_setup}{exp_results}. Finally, conclusions are drawn in \sectdot{conclusions}.
