\section{Related work}
\label{sec:related}

\subsection{Neural transducer models}
The neural transducer \cite{graves2012RNNT, Graves2013} is a prevalent E2E trainable ASR approach. It consists of an encoder, a predictor and a joint network (see \figdot{rnnt}). Given a pair of input feature sequence $\bm{X}=\bm{x}_1,\ldots,\bm{x}_M$ of length $M$ and its transcription $\bm{y}=y_1,\ldots,y_U$ of length $U$, the neural transducer is trained to maximise the conditional probability $P(\bm{y}|\bm{X})$. The encoder receives $\bm{X}$ and generates an acoustic embedding sequence $\bm{F}=\bm{f}_1,\ldots,\bm{f}_T$ of length $T$. In practice, $T$ is often set to $M$, or a value smaller than $M$ by a constant factor (\textit{e.g.} when $\bm{X}$ is a {raw waveform} instead of acoustic feature sequence). The predictor produces a text embedding $\bm{g}_u$ for each text token $y_u$, and $\bm{G}=\bm{g}_1\ldots,\bm{g}_U$. The joint network takes each pair of $\bm{f}_t$ and $\bm{g}_u$ as input and generates a $V$-dimensional (-d) distribution, where $V$ is the size of the output layer (i.e the token vocabulary) including an extra \textit{blank symbol} which is removed from the final output sequence. 

A (transducer) lattice refers to the collection of all valid alignments between $\bm{F}$ and $\bm{y}$ (and thus $\bm{X}$ and $\bm{y}$) traversing from $(t=0,u=0)$ to $(t=T,u=U)$. The training objective $P(\bm{y}|\bm{X})$ is then the sum of the probability of all valid alignments: 
%Neural transducer\cite{graves2012RNNT} is a powerful E2E ASR approach. It consists of an encoder, a predictor and a joint network (see \figdot{rnnt}). Given a pair of acoustic feature $\bm{X} = \bm{x}_1,...\bm{x}_T$ of length $T$ and its transcription $\bm{y}$ of length $U$, the neural transducer is trained to maximise the the conditional probability $P(\bm{y}|\bm{X})$. The encoder receives input acoustic features $\bm{X}$ and generate acoustic embeddings $\bm{F} = \bm{f}_1,...,\bm{f}_T$. The predictor processes the corresponding transcription sequence $\bm{y} = y_1, y_2,..,y_U$ and produces text embeddings $\bm{G} = \bm{g}_1,...\bm{g}_U$. The joint network takes $\bm{F}$ and $\bm{G}$ as input and generates a 3-dimensional distribution lattice $\bm{Z}$ of shape $(T,U,V)$, where $V$ is the size of output vocabulary. Each node $\bm{Z}(t,u,k)$ in the distribution lattice represents the probability of generating $k$-th token in the vocabulary at time $t$ after generating the partial text sequence $y_{1:u}$. Each path in the lattice traversing from $(t=0,u=0)$ to $(t=T,u=U)$ is a valid alignment between $\bm{X}$ and $\bm{y}$. The training objective $p(\bm{y}|\bm{X})$ is then the sum of the probability of all valid alignments: 
\begin{align}
    P(\bm{y}|\bm{X}) = \sum\nolimits_{\alpha \in \mathcal{A}}P(\alpha|\bm{X})
\end{align}
where $\mathcal{A}$ is the set containing all valid alignments between $\bm{X}$ and $\bm{y}$. Enumerating all valid alignments is computationally prohibitive and the forward-backward procedure can be used~\cite{graves2012RNNT, Graves2013} to efficiently compute the summation (the RNNT loss). During decoding, Viterbi beam search can be applied to generate output tokens in a time-synchronous manner. Compared to other E2E ASR approaches, including connectionist temporal classification (CTC)~\cite{graves2006CTC} attention-based encoder-decoder (AED) models~\cite{Bahdanau2015EndtoendAL,chan2015listen,watanabeHybrid}, the neural transducer generally achieves lower WERs than CTC and more naturally handles streaming input speech than AED.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figs/rnnt.pdf}
    \caption{A neural transducer model.}
    \label{fig:rnnt}
\end{figure}
In addition, we use the term \textit{distribution lattice} to refer to the $(T,U,V)$-d tensor composed by $\bm{F}$, $\bm{G}$ and all output distributions, which is denoted as $\bm{Z}$. Each node in the distribution lattice, $\bm{Z}(t,u,k)$, represents the probability of generating $k$-th token in the vocabulary at time $t$ after generating the partial text sequence $y_{1},\ldots,y_u$. 

\presec
\presec
\subsection{Knowledge distillation for neural transducers}
\postsec

KD~\cite{hinton2015knowledgedistillation} is a widely used technique that compresses a large teacher model to a small student model by training the small model to match the output of the large model. Depending on the output of the teacher model, different loss functions can be used. For example, the KL-divergence is commonly used as the distillation loss to match probability distributions whereas $L_1$ or $L_2$ loss is more appropriate for matching hidden representations. Since the output of a neural transducer for an input acoustic sequence is a distribution lattice, applying KD to neural transducers is inherently expensive. A straightforward way of applying KD to neural transducers is to use the KL-divergence between the teacher distribution lattice $\bm{Z}_{\mathcal{T}}(t,u,k)$ and the student distribution lattice $\bm{Z}_{\mathcal{S}}(t,u,k)$ as the distillation loss:
\begin{align}
    \mathcal{L}_{\text{KD}} = -\sum_{t=1}^{T}\sum_{u=1}^{U}\sum_{k=1}^{V} \bm{Z}_{\mathcal{T}}(t,u,k) \log \bm{Z}_{\mathcal{S}}(t,u,k).
    \label{eqn:fulllatticeKD}
\end{align}
%where $\bm{Z}_{\mathcal{T}}(t,u,k)$ and $\bm{Z}_{\mathcal{S}}(t,u,k)$ are the posterior distribution lattice of teacher model and student model respectively.

In practice, however, directly applying the KL-divergence loss defined in \eqndot{fulllatticeKD} is inefficient, due to the high computational and storage costs for the distribution lattice. To reduce cost, \cite{panchapagesan2021efficient} kept only three elements in each node of the original distribution lattice: the probability of the correct symbol $y_u$, the probability of the blank symbol and the sum of the probabilities of all other symbols, and used this collapsed distribution lattice for KD:
\begin{align}
    \mathcal{L}_{\text{KD}}\approx -\sum_{t=1}^{T}\sum_{u=1}^{U}\sum_{k=1}^{3} \bm{Z}'_{\mathcal{T}}(t,u,k) \log \bm{Z}'_{\mathcal{S}}(t,u,k)
    \label{eqn:collapsedKD}
\end{align}
where $\bm{Z}'_{\mathcal{T}}(t,u,k)$ and $\bm{Z}'_{\mathcal{S}}(t,u,k)$ are the collapsed versions of the teacher and student posterior lattice.
As a result, the computational complexity is reduced from $\mathcal{O}(TUV)$ to $\mathcal{O}(TU)$. However, a large amount of information is discarded 
%as the correlations between different output symbols are discarded, 
and this is sub-optimal for KD training. To keep the full distribution while being efficient, \cite{yang2022knowledge} approximated the distribution lattice with its one-best alignment, which was obtained while generating the transducer lattice and saved prior to training. KD then uses only on the one-best alignment instead of the whole lattice, leading to another approximation of the KD loss:
\begin{align}
    \mathcal{L}_{\text{KD}}\approx -\sum_{(t,u)\in \textsc{1Best}}\sum_{k=1}^{V} \mathbf{Z}_{\mathcal{T}}(t,u,k) \log \mathbf{Z}_{\mathcal{S}}(t,u,k).
    \label{eqn:1best alignment}
\end{align}
As the length of an alignment is $T+U$, this approximation reduces the computation from $\mathcal{O}(TUV)$ to $\mathcal{O}((T+U)V)$. When $V$ is large, a potential drawback of this method is that it would still use a large amount of memory. 
%, hindering its application under such situations.

As a workaround, \cite{guo2022predicting} used the encoder features from an intermediate layer of a pre-trained teacher model for KD. Since the encoder output dimension of a teacher model is fixed, the computational cost is no longer related to $V$. To reduce the amount of data received from the teacher in on-the-fly KD, a multi-codebook vector quantisation method was used to compress 32-bit ``float'' features to 8-bit integers. During KD, the student model tries to predict the codebook indexes of teacher embeddings. This comes at a cost of slight performance degradation when compared to $L_1$ and $L_2$ loss \cite{guo2022predicting}.

Natural support for streaming is one reason why neural transducers are gaining more popularity. Streaming models have less future context and tend to emit non-blank symbols later than their non-streaming counterparts. Therefore, performing knowledge distillation on streaming transducers is more challenging when its teacher model is non-streaming. A 3-step distillation process is proposed in \cite{kurata20_interspeech} to delay the emission of the non-streaming teacher model using a streaming model before performing KD. Yang et al~\cite{yang2022knowledge} modified the 1-best alignment KD loss to improve the performance of the streaming student model. Directly applying \eqndot{1best alignment} to a streaming student could be problematic if the pre-trained teacher model is non-streaming as this would force the student to ``guess'' the future. Therefore, a time-shift variable $\tau$ was introduced to allow the student streaming model to emit symbols later than the teacher model. This leads to a modified version of \eqndot{1best alignment}:
\begin{align}
     \mathcal{L}_{\text{KD}}\approx -\sum_{(t,u)\in \textsc{1Best}}\sum_{k=1}^{K} \mathbf{Z}_{\mathcal{T}}(t,u,k) \log \mathbf{Z}_{\mathcal{S}}(t+\tau,u,k).
    \label{eqn:1best alignment streaming}
\end{align}

\subsection{Self-supervised learning for speech foundation models}
A foundation model refers to a large model pre-trained on a vast quantity of unlabelled data at scale based on SSL \cite{bommasani2021foundation}.   
It has become a new paradigm that an increasing number of research works are conducted by fine-tuning pre-trained foundation models using labelled data. 
%Self-supervised pre-training with unlabelled data and finetuning with labelled data has become a paradigm to learn general data representations. 
Recently, many speech foundation models were developed and achieved state-of-the-art performance for various speech processing tasks \cite{baevski2020wav2vec, hsu2021hubert, chen2022wavlm}. Wav2vec 2.0 \cite{baevski2020wav2vec} used product quantisation to quantise acoustic features encoded by a stack of convolutional layers. The Gumbel softmax \cite{jang2016gumbelsoftmax} is applied to make the codebook selection process differentiable. SSL pre-training is carried out using a contrastive loss to encourage the Transformer encoder to discriminate each ground truth quantised vector from a set of distractors. HuBERT \cite{hsu2021hubert} applies k-means clustering to the speech representations and generates pseudo-class labels using the clustering results. During pre-training, the model is trained to predict the ground truth class labels of both masked and unmasked timestamps. To refine the clustering results, the features from an intermediate Transformer block are extracted for the second round of k-means clustering. WavLM \cite{chen2022wavlm} adopted the same idea as HuBERT for generating pseudo labels during pre-training, but used a more diverse pre-training dataset to increase the model's generalisation capability. It also performs speech denoising modelling during pre-training by adding noises and overlapping speech to the input. This not only enhances the robustness of the learned features, but also makes it more suitable for non-ASR tasks.

\subsection{Multi-teacher knowledge distillation}

In teacher-student training, the quality of the teacher model is crucial to the performance of the student model. Ensemble \cite{dietterich2000ensemble, rokach2010ensemble, lakshminarayanan2017simple} is a common approach for constructing stronger models by combining the output of different models. Lindqvist et al~\cite{ensembledistribution} used an ensemble as the teacher model to perform KD training, where a student model learns from the ensemble distribution obtained by averaging the distribution. However, the individual teacher information cannot be recovered from the weighted mean distribution, limiting the knowledge to be transferred from each information source. To address this information loss, \cite{fukuda2017efficient} proposed to randomly sample a teacher from a teacher pool for each training batch to so that the student model is exposed to individual teachers. In \cite{liu2020adaptive}, embeddings extracted from multiple teacher models are used as teacher targets and $L_2$ loss between the teacher embeddings and student embeddings was added as an auxiliary loss for training. Regarding ASR, \cite{JeremyDecisionTree} proposed an ensemble of hybrid teacher acoustic models constructed based on different tied-state triphones or tri-characters derived using the decision tree clustering approach.  