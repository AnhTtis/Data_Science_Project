\section{Experimental Setup}
\label{sec:exp_setup}

\subsection{Model}

The Base version of Wav2vec 2.0 \cite{baevski2020wav2vec}, HuBERT \cite{hsu2021hubert} and WavLM \cite{chen2022wavlm} were selected as teacher models. For convenience, they will be referred to as ``W2v2'', ``HU'' and ``WL'' in the following experiments. Note that there are two Base versions of WavLM, with different amounts of unlabelled data used during pre-training. Here, the one pre-trained on MIX-94k hour is chosen as it is expected to have better performance. All three pre-trained models take the raw waveform as input and generate 50 encoder frames per second, whereas the most commonly used filter bank features for E2E trainable ASR usually operates at 25Hz. An extra subsampling module is appended to the pre-trained model, which concatenates two subsequent frames and applies a linear transform followed by a non-linear activation which eliminates the frame mismatch problem and the memory and computation cost is also reduced. The student model architecture is the small version of the Conformer transducer model (Cfm-S) \cite{gulati2020conformer} with 16 encoder layers of a hidden dimension of 144. To ensure the teacher model and student model have the same encoder feature dimension, an extra linear projection layer is added to the end of the encoder stack of the student model. All teacher models have a single-layer 640-d long short-term memory (LSTM) predictor network, and all student models have a single-layer 320-d LSTM predictor network. The streaming student model has zero future context. A triangular attention mask and causal convolution~\cite{oord2016wavenet} are used to ensure the model only attends to previous frames. The details of the teacher and student model and WERs of three teacher models are shown in \twotbl{model details}{teacher WER}. WavLM transducer achieves the lowest WERs among the three teachers because of the larger pretraining set. Note that the teacher models are about 10 times the size of the student model. All models are trained using the ESPnet~\cite{watanabe2018espnet} ASR toolkit.

\begin{table}[ht]
    \pretbl
    \begin{adjustbox}{width=\linewidth}
    \centering
    \begin{tabular}{c c c c c}
        \toprule
        %Model & W2v2 & HuBERT & WavLM  \\
         & \multicolumn{3}{c}{Teacher Transducer} & \multicolumn{1}{c}{Student} \\
        \midrule
        Encoder & Wav2vec 2.0 \footnote{https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec\_small.pt} & HuBERT\footnote{https://dl.fbaipublicfiles.com/hubert/hubert\_base\_ls960.pt} & WavLM\footnote{https://github.com/microsoft/unilm/tree/master/wavlm} & Cfm-S \\
        Encoder size & 768-d & 768-d & 768-d & 144-d \\
        Pretraining data & LS-960 & LS-960 & MIX-94k & None \\
        Num. params & 99.2M & 99.5M & 99.2M & 10.3M \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Details of the teacher models. ``LS-960'' and ``MIX-94k'' denote the full LibriSpeech dataset and 94k mix dataset.}
    \label{tab:model details}
    \posttbl
    \posttbl
\end{table}

\begin{table}[!h]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{Teacher Transducer}    & \multicolumn{2}{c}{dev}   & \multicolumn{2}{c}{test}  \\
        \cmidrule(lr){2-3}  \cmidrule(lr){4-5}
        & \multicolumn{1}{c}{clean} & \multicolumn{1}{c}{other} & \multicolumn{1}{c}{clean} & \multicolumn{1}{c}{other} \\ 
        \midrule
        Wav2vec 2.0 (W2v2) & 5.1 & 12.2 & 5.2 & 11.8\\
        Hubert (Hu)  & 5.2 & 11.0 & 5.3 & 11.2 \\
        WavLM  (WL) & 3.9 & 8.4 & 3.9 & 8.3 \\
        \bottomrule
    \end{tabular}
    \caption{WERs of teacher models fine-tuned on LibriSpeech ``train-clean-100''. }
    \label{tab:teacher WER}
\end{table}


\subsection{Dataset}
The two popular datasets LibriSpeech~\cite{panayotov2015librispeech} and Libri-Light~\cite{kahn2020libri} were used for the experiments. The full LibriSpeech training set contains 960 hours of audiobook recordings with corresponding transcriptions and the LibriLight dataset contains 60+k hours of unlabelled speech. To verify the effectiveness of the proposed multi-teacher KD framework, experiments are first carried out on the ``train-clean-100" subset. All teacher models are also fine-tuned on the ``train-clean-100" subset using the raw waveform. During KD training, extra unlabelled speech is used to augment the training set for potential performance improvement. To investigate the effectiveness of this, the teacher model's embeddings of the remaining 860h of LibriSpeech and 1000 hours of speech randomly sampled from Libri-Light were extracted as teacher labels, resulting in a total pre-training dataset of around 2k hours. Pseudo transcriptions were also used to improve student models' performance. The pseudo transcriptions of 860 hours of remaining speech from LibriSpeech were obtained by performing beam search decoding of size 8 on individual teacher model. The transcriptions were not improved by language model fusion. 

During training, SpecAugment~\cite{park2019specaugment} was applied for data augmentation. Speed perturbation was not used in order to reduce the number of teacher labels that need to be stored. The output vocabulary has 256 sub-word units generated by SentencePiece~\cite{kudo2018sentencepiece}. During inference, the model with the lowest WER on the dev-other development set was chosen and the WERs on two test sets (test-clean and test-other) are reported.

\subsection{KD Training}
In the encoder pre-training stage, the raw waveform is fed to the fine-tuned teacher models to generate encoder embeddings. The masking in the teacher models was disabled and the embeddings were stored on disk prior to training. Various teacher model selection strategies were explored including uniform sampling and WER-related or similarity-related sampling. It is found that uniform sampling yields the best performance after fine-tuning and this sampling strategy is adopted in the rest of this paper. During training, the student model was given the 82-d filter bank features concatenated with 1-d pitch features corresponding to the waveform that is used to generate embeddings. SpecAugment is applied during student encoder pre-training to improve the robustness of features learned by the student encoder and Noam \cite{vaswani2017transformer} optimiser with warmup was used to update student encoder parameters. To evaluate the student model's encoder, the teacher embeddings of the ``dev-other" development set were also collected. The model with the lowest $L_1$ error on the ``dev-other" was selected as the initialisation for the second stage of training. When multiple teachers are used during KD, the averaged $L_1$ error over all teachers embeddings on ``dev-other'' was used as the model selection criterion.

During the second stage of KD training, the model was fine-tuned with audio-text pairs. The texts are either ground truth transcriptions (for ``train-clean-100'') or pseudo transcriptions decoded on unlabelled speech (the remaining 860-hour LibriSpeech). A tri-stage fine-tuning learning rate schedule was adopted, where the warm-up stage, constant stage and decay stage takes 10\%, 40\% and 50\% of the total training steps. The learning rate schedule for 100-hour and 960-hour experiments are shown in \tbl{lr schedule}.

\begin{table}[!h]
    \centering
    \begin{tabular}{c c c c c}
    \toprule
        Fine-tuning data    & Initial & Warmup & Decay \\
    \midrule
       100 hour  & 1e-6 & 1e-4 & 5e-6 \\
       960 hour  & 1e-6 & 5e-4 & 1e-5 \\ 
    \bottomrule
    \end{tabular}
    \caption{The learning rate schedule of fine-tuning in 100-hour and 960-hour experiments.}
    \label{tab:lr schedule}
\end{table}