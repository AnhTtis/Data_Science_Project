\documentclass{article}
\usepackage{arxiv}
\usepackage{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[pdfusetitle]{hyperref}
\usepackage{xurl}            % simple URL typesetting

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{booktabs}    

\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{caption}
\usepackage{subcaption}

\usepackage{authblk}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
% \usepackage[textsize=small]{todonotes}
%NOTE: disable todos by uncommenting below 
\usepackage[disable,textsize=small]{todonotes}

\usepackage{pifont}
\usepackage{float}
\usepackage[safe]{tipa}
\usepackage{tikz}

%NOTE: natbib setup
\usepackage[square, numbers, semicolon, sort&compress]{natbib}
\bibliographystyle{abbrvnat} % abbrv --> [1], [2], ecc.
\setcitestyle{authoryear,open={(},close={)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%NOTE: User-defined commands:
\usepackage{macros}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Approaching an unknown communication system by latent space exploration and causal inference}

\author[1,3,+]{Ga\v{s}per Begu\v{s}}
\author[1,3,+]{Andrej Leban}
\author[2,3,4]{Shane Gero}

\affil[1]{University of California, Berkeley, Berkeley, CA, United States}
\affil[2]{Department of Biology, Carleton University, Ottawa, Canada}
\affil[3]{Project CETI, New York, NY, United States}
\affil[4]{The Dominica Sperm Whale Project, Roseau, Dominica}
\affil[+]{these authors contributed equally to this work}
\affil[ ]{\texttt {\{andrej\_leban, begus\}@berkeley.edu}}


\begin{document}

\maketitle

\begin{abstract}
This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values outside the training range with methods inspired by causal inference into an approach we call \textit{causal disentanglement with extreme values} (CDEV) and show that this approach yields insights for model interpretability. Using this technique, we can infer what properties of unknown data the model encodes as meaningful. We apply the methodology to test what is meaningful in the communication system of sperm whales (\textit{Physeter macrocephalus}), one of the most intriguing and understudied animal communication systems. We train a network that has been shown to learn meaningful representations of speech and test whether we can leverage such unsupervised learning to decipher the properties of another vocal communication system for which we have no ground truth. The proposed technique suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. Some of these findings are consistent with existing hypotheses, while others are proposed for the first time. We also argue that our models uncover rules that govern the structure of communication units in the sperm whale communication system and apply them while generating innovative data not shown during training. This paper suggests that an interpretation of the outputs of deep neural networks with causal methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. Finally, the proposed approach combining latent space manipulation and causal inference can be extended to other architectures and arbitrary datasets. 
\end{abstract}




\flushbottom

\thispagestyle{empty}

\section{Introduction}
\label{sec:introduction}


How do we approach a communication system for which we not only don't understand what is meaningful but also do not know how to test for what is meaningful? One such case is the communication system of sperm whales (\textit{Physeter macrocephalus}). Sperm whales communicate with sets of click vocalizations called \textit{codas} \citep{Watkins77}. Their vocalizations are likely meaningful because they are produced in duet-like exchanges or group choruses \citep{Schulz08}, predominantly during socialization and before dives but never while alone or when foraging at depth \citep{watwood06,whitehead03, Whitehead1991}, and appear to be socially learned \citep{Rendell2012, CulturalLives}. While evidence supports the use of codas as identity signals \citep{geroIndividual, Rendell2003, Hersh2022}, very little is currently known about what individual utterances mean or even what kind of properties of the communication system could have the potential to carry meaning. 

In this paper, we propose an approach for discovering meaningful properties in data by using an expressive generative model as the learning mechanism. The network is trained with two objectives: (i) imitation of data and  (ii) encoding of information (Figure \ref{fig:whale}). We then combine latent space exploration with methods borrowed from causal inference into a methodology we call \textit{causal disentanglement with extreme values} (CDEV). This approach yields insights into what  properties the network has learned encode uniquely relevant information for the synthetic vocalizations. 

If sperm whales encode information into their vocalizations and if our model can learn to imitate those well, it is likely that the encoding in our models can reveal what might be meaningful in the sperm whale communication system. We argue that our technique reveals both properties that were posited as meaningful by human researchers as well as novel properties that have so far not yet been hypothesized as such. 

One of the advantages of the proposed approach is that the fiwGAN architecture \citep{begusCiw} used as the learning mechanism is innovative in a highly informative way. For example, when trained on human speech, the fiwGAN network generates novel words that were never part of training data \citep{begusCiw,begus2020identity}. Similarly, when trained on sperm whale codas, the network not only replicates codas from the training set but also generates codas not shown during training. As such, our network discovers rules that govern the structure of codas and applies the rules learned from observed training data to novel, innovative outputs. Such inventiveness makes fiwGAN especially well-suited to the use as the learning mechanism of properties in data about which little is understood, which is the cornerstone of the approach presented in this work. In some sense, we use the network (in conjunction with CDEV) as an infinitely-flexible and information-preserving tool for decomposing the data into meaningful, observable properties without the need for, e.g., dimensionality reduction. 

Causal inference is a discipline that aims to develop the assumptions, experiment designs, and statistical methodology needed to determine the \textit{causal effect} of a particular variable that can be manipulated --- the \textit{treatment} ---  on some outcome(s) of interest \citep{hill2015}. While this usually involves evaluating counterfactual scenarios, for example via the \textit{potential outcomes} framework, this is not \textit{the} fundamental purpose of the discipline, allowing us to use the methodology in studies such as the one presented in this paper.


\subsection{Sperm whale communication}
\label{sperm-whale-communication}

In social contexts, sperm whales communicate in short (less than two seconds), socially learned, stereotyped patterned series of \emph{clicks} called \emph{codas} that marine biologists have grouped into several coda \emph{types} based on the variation in the number, rhythm, and tempo of the clicks within codas (summarized in \citep{whitehead03, CulturalLives}.  Fig.~\ref{fig:coda} is an illustration of such codas. Even when living in the same waters, whales only associate with other whales which use a similar dialect of coda types, creating a higher level in their social structure delineated based on culture --- the vocal \textit{clan} \citep{Rendell2003, Gero2016clans, Hersh2022}.  While both are broadband in the spectrum, coda clicks --- used in communication --- can be  acoustically distinguished from echolocation clicks, which are used for navigation and foraging \citep{whitehead03,Madsen2002}.
   
The dataset of recordings for this study originates from The Dominica Sperm Whale Project (see \citealt{Gero2014}) and was collected off the coast of the island of Dominica between 2014 and 2018 from over 4000 hours in the company of sperm whales. The vast majority of the recordings are made of one sperm whale vocal clan. In the dialect of this Eastern Caribbean Clan, there are about 22 different coda types that have been defined \citep{Gero2016clans}. The actual distribution and production rates of coda types are highly asymmetrical in which the two most common codas comprise 65\% of all coda vocalizations recorded between 2005 and 2010 \citep{geroIndividual}.

Codas were collected from animal-borne sound and movement tags between 2014 and 2018 (\textit{Dtag} generation 3; \citealt{Dtag2003}). \textit{Dtags} record two-channel audio at 120 kHz with a 16-bit resolution, providing a flat (±2 dB) frequency response between 0.4 and 45 kHz.  As a result, we were able to record clean signals for both the coda and echolocation clicks produced by sperm whales; and to obtain the temporal patterning and spectral properties of coda clicks used in this analysis.

Due to the above-mentioned rarity of some types in the dataset, we limit the training set used in this paper to the five most common coda types (1+1+3, 5R1, 4R2, 5R2, and 5R3, Fig. \ref{fig:whale}). This is further exacerbated by the fact we are dealing with a communication system, meaning that a significant portion of the data could not be used due to too strong of a presence of whale dialogue in which codas are often overlapped by one or more animals \citep{Schulz08}. The residual effects of this are dealt with by our algorithms, as illustrated in Section \ref{sec:data-generation}. Restricting the number of coda types also helps us  test whether the deep neural network can predict unobserved coda types. All in all, 2209 distinct training samples were used. GANs, contrary to other architectures, do not require extensive datasets and have been shown to learn informative properties of language with similar or substantially smaller datasets \cite{begusLocal,begusCiw}.

In terms of data preprocessing, a constant DC microphone bias was removed from the extracted coda recordings, which were then augmented by random zero-padding in the front to address the fact that all extracted codas would otherwise have a click at the very beginning.

\begin{figure}[t]
  \centering
  \includegraphics[width=.7\textwidth]{figs/CausalCetiFig1.pdf}
  % \includegraphics[width=0.45\textwidth]{figs/codaTypes.png}
  \caption{Overview of the fiwGAN architecture \citep{begusCiw} and data used in training. The figure illustrates three networks: the Generator with 5 upconvolutional layers, the Q-network with 5 convolutional layers, and the Discriminator with 5 convolutional layers.}
  \label{fig:whale}
\end{figure}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/codaexample1+1+3.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/codaexample9R.pdf}
  \end{subfigure}
  \caption{Examples of real codas. Left: type \textit{1+1+3} with 5 clicks, right: type \textit{9R} with 9 clicks. The 9R type was never part of the training data (Section \ref{sperm-whale-communication}), yet our network learns to generate codas with a high number of  clicks that resemble this type and its regularity despite never having access to codas with more than 5 clicks (Sec. \ref{sec:regularity}).}
 \label{fig:coda}
\end{figure}


\subsection{Why use deep learning?}
\label{why-use-deep-learning}

While biologists have classified the vocalizations into \emph{codas}, virtually nothing is known of their informational content \citep{andreas22} One outstanding question about codas is whether the acoustic properties have any informational content. In this work, we aim to use the fact that the network used learns to encode informationally meaningful properties in an \textit{unsupervised} way while being able to generate convincing synthetic whale utterances to discover what might be crucial to the communication system.

Convolutional neural networks (CNNs) are relatively unbiased compared to human researchers and can learn to encode properties that human analysis might not observe. Moreover, learning is performed on raw audio without information-losing transformations. To our knowledge, this is the first attempt to model sperm whale communication with deep learning on raw acoustic data.

Deep neural networks have recently been used to tackle some of the hard problems across sciences. Probing learned representations in deep neural networks has yielded insights in fields as diverse as drug discovery \citep{stokes20}, protein design \citep{jumper21}, or geometry \citep{davies21}, by shrinking the hypothesis space for each of the problems. To our knowledge, deep neural networks have not yet been used in attempts to decipher an unknown communication system. 

The advantage of the proposed approach is that the discovery of meaningful properties uses as few assumptions as possible, treating the network as a black-box learning unit. The proposed approach, where individual units in the inputs of deep neural networks are manipulated to extreme values, and their effects are estimated via causal inference methods, could be applied to other architectures and data.


\subsection{The architecture used}
\label{sec:architecture}



Generative adversarial networks (GANs; \citealt{goodfellow14GAN}), in the most basic form, consist of two
separate networks trained in an adversarial fashion. The \emph{generator} outputs synthetic data with the goal of
tricking the \emph{discriminator} that the data is real. The latter's goal is the inverse: to distinguish the
real data from the synthetic as well as possible.

The training objective is, therefore, a \emph{minimax game:}

\begin{equation*}
    \min_{G} \max_D \EE_{X \sim P_{data}}[D(x)] + \EE_{Z \sim P_{synth}} [1 - D(G(z))]
    \label{eq:gan}
\end{equation*}

The input to the generator during training is a randomly sampled continuous vector of a given length, usually chosen to be 100. Since there is no correlation between the input selected and the loss imposed on the generator, the latter learns to associate the inputs with the outputs in such a convoluted way that the inputs can be treated as \emph{incompressible noise}. We use this assumption to justify the use of some of the causal inference estimators used in this work. Since the elements of the input are continuous  numbers, it can be assumed that the provided length of the vector gives the model enough ``informational space''.

The network architecture used in this work is fiwGAN \citep{begusCiw}, an InfoGAN \citep{chen16InfoGAN} adaptation of the WaveGAN \citep{donahue19} model (which itself is based on DCGAN; \citealt{radford15}). Unlike InfoGAN, the fiwGAN features a separate Q-network and a binary code instead of a one-hot vector which enables featural learning. In short, it partitions the input into an \emph{incompressible noise} $z$ and an additional \emph{featural encoding} $c$.  The latter is learned in an unsupervised way by an additional network \emph{Q} during training,  with the objective of ensuring consistency of output across similar values of this vector, in contrast to $z$. It achieves this consistency by additionally penalizing the generator for inconsistent output for a given value  of $c$ by backpropagating a loss based on mutual information between the generated output and the encoding $c$ (Figure \ref{fig:whale}).

In our case, the generator needs to learn to generate audio that resembles sperm whale codas without accessing the actual sperm whale vocalizations directly. Learning thus occurs primarily by imitation in a fully unsupervised manner via the adversarial loss. Additionally, the network needs to encode information into its synthetic outputs so that another network --- Q ---  is able to decode this in a game that mimics communicative intent (Figure \ref{fig:whale}).

\citet{begus19} proposes a technique to uncover individual latent variables that have linguistic meaning by setting latent variables to extreme values outside of the latent space and interpolating from extreme values of those variables. It is shown that networks trained on speech data learn to associate lexical items with code values and sublexical structure with individual bits in $c$ \citep{begusCiw,begusZhouInterspeech}. 


\subsection{Prior work}

Combining latent space exploration with causal inference is a novel approach to the interpretability of unsupervised deep neural networks. Commonly used methods for interpretability,  such as integrated gradients \citep{integratedgrads}, require access to the network itself, which is not the case here.

Conversely, the majority of prior work using deep learning in causal inference uses it in the opposite direction (c.f. \citet{deepcausal}): to infer the outcome curve with missing observations rather than applying causal inference methods themselves on the outputs of deep networks.

 \citet{chockler21} use causal methods for interpreting CNNs, but their objective is to find the subset of the input image that most influences the decision of a supervised classifier when the input is occluded rather than to uncover what an unsupervised generative model is learning in terms of derived quantities. \citet{kocaoglu2018causalgan}  present an approach to training a generative model that preserves an a-priori determined causal relationship. \citet{bose2022controllable, bose2022cage}  use causal inference with binary treatments to quantify implicit causal relationships \textit{between} latent space variables \textit{within} models (i.e., a white-box method) such as GANs via the use of proxy classifiers, in order to aid the controllable generation of outputs. In contrast, \textit{CDEV} is a black-box method that uses continuous-treatment causal inference methodology to uncover \textit{observable} attributes that are encoded as significant by an unsupervised learner. As such, it makes no claims about potential causality within the latent space and does not need access to the latter, simplifying the justification for the use of the methods presented (Sec. \ref{sec:continuous-treatment-causal-inference}).  Additionally, to our best knowledge, none of the related work makes use of setting the inputs to extreme values in order to disentangle the encodings.
 % \citet{bose2022controllable, bose2022cage}  do not observe the effects of latent space when individual variables are set to values outside the training range, which is a crucial step in our approach as individual variables are highly entangled in the training range.
 
In this work, we, therefore, analyze the generator \textit{from the outside}: after having it learn to replicate the whale utterances and encode what it considers meaningful in an unsupervised way, we aim to uncover the latter by merely manipulating the input to the model and applying statistical methods on the generated audio: in this way, the methodology is agnostic to the type of generative process used. We present the setup of the experiment in Section \ref{sec:data-generation} and argue for the use of causal inference methods in Section \ref{sec:continuous-treatment-causal-inference}.  The three particular causal methods used are presented in Section \ref{sec:nclicks}, which also illustrates the first encoding uncovered: the range of the number of clicks. The two succeeding sections present the results for two additional (groups of) attributes: coda regularity in Section \ref{sec:regularity}, and the acoustic characteristics of the generated codas in Section \ref{sec:auditory}.







\section{The CDEV technique and data generation}
\label{sec:data-generation}

\begin{figure}[h!t]
  \centering
  \includegraphics[width=.5\textwidth]{figs/CausalCetiFig2.pdf}
  \caption{Overview of the \textit{causal disentanglement with extreme values} process: individual bits in the code $c$ are set to extreme values while $z$ is kept within the training range.}
  \label{fig:causal}
\end{figure}

In simple GANs, the generation is done by uncoupling the generator after training and again feeding it vectors $z$ sampled at random; in essence, being told to generate a random outcome from its learned distribution. In our architecture, the incompressible noise - $z$ - entries are again sampled \textit{i.i.d} $\sim$ Unif(0, 1), while the featural encoding $c$ is set manually to a desired value - \emph{dosage} in causal inference terminology.

Unlike the incompressible noise \(z\), the featural encoding \(c\) has been trained to correspond to consistent outputs; however, it is picked up in an unsupervised fashion. Therefore, this work aims to uncover whether it (and the degree it does) corresponds to observable properties.
\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/paste-ADE898ED.png}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/paste-DDBD33C8.png}
  \end{subfigure}
  \caption{Examples of generated codas.}
  \label{fig:genCodas}
\end{figure}

Since the consistency of output is only enforced in a loose way, this often only becomes readily apparent when setting the numerical values outside the bounds seen in training, where the primary associated effect begins to dominate \citep{begus19,begus2020identity,begusLocal}.  In this work, we extend this approach into a methodology we call \textit{causal disentanglement with extreme values} (CDEV)  (Figure \ref{fig:causal}). The space available for the featural encodings is limited; hence finding the real-world attributes that map almost one-to-one with the feature space encodings suggest that the generator considers them very important to generating convincing outputs. The number of values reserved for the encoding in the network used is five, which equals the number of distinct coda types shown to the network during training.



In order to perform statistically meaningful experiments, we need to be able to measure our observables algorithmically. Using the \textit{causal disentanglement with extreme values} approach, the output can become relatively noisy as the encoding values $c$ move significantly out of the training range, which was within $[-1, 1]$ for all the five bits. In addition, we observe that for the network used here, the output crystallizes from noise at about the lower edge of the training range (\textbf{$-1$}), with all lower values (for any of the bits) inducing the generator to output noise, as illustrated in Figure \ref{fig:noise}.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/noise.pdf}
  \caption{Example of noisy output for encoding values up to the training range limit of $-1$, in this case, $[-1, 0, 0, 0, 0]$.}
  \label{fig:noise}
\end{figure}

For the first two observables considered - the number and regularity of clicks (Sections \ref{sec:nclicks} and \ref{sec:regularity}) ---  we, therefore, use an algorithmic \emph{click detector} to find local maxima in the generated audio signal that correspond to clicks. It uses minimal thresholds for both the amplitude and the temporal separation to choose between (potentially) multiple sets of detected "clicks". The latter threshold is based on the minimum  peak separation possible by the whale physiology and was set to 40 ms.  The amplitude threshold is necessary to deal with the residual presence in the dataset of interwoven codas coming from other whales; it was set to $0.4$ in terms of the relative amplitude to the peak click and obtained from an analysis of the amplitudes of vocalizations coming from the primary whale and secondary whales in the data.

Even so, as we move towards more extreme encoding values (in the positive direction), the output becomes progressively noisier, as shown for a relatively pathological example in Figure \ref{fig:clickDetector}.  Therefore, the detector also uses signal filtering together with a sub-algorithm that strives to “maximize entropy”  by preferring well-spaced peaks over counting a single jagged peak multiple times: i.e., if multiple valid "solutions" given the minimal threshold constraints are still found, it will prefer the more spaced-out solution, which corresponds to our intuition. The final numerical range of the encodings tested: $[-1, 12.5]$ was thus also selected so that the outputs are still reasonably meaningful and the behavior of all the algorithms used for detecting the observables is predictable.



\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.55\textwidth]{figs/clickCounterNew.pdf}
  \caption{Example of the output of the click detector on an instance of relatively noisy generated data, which can happen when the encodings are set to extreme values. The minor peaks are artifacts picked up in training due to the data not being free of whale dialogue; the yellow horizontal line is the inferred volume level for the vocalization to be coming from the primary whale.}
  \label{fig:clickDetector}
\end{figure}

\section{The experiment as a continuous-treatment causal inference problem}
\label{sec:continuous-treatment-causal-inference}

The methodology used in this work is inspired by causal inference, more specifically, \textit{continuous-treatment} causal inference. In short, the latter deals with an experimental setup where the usual treatment assignment variable \(Z\) becomes a continuous variable \(T\), often called a \emph{dose} due to it being common in pharmacological studies. In the case of observational studies, the \textit{propensity score} $e(x)$ thus becomes a function of two variables $r(t, x)$ \citep{imbens_propensity}:

\[
e(x) \rightarrow r(t,x) = \P(T=t | X=x)
\]

Given the assumptions common to the discrete case, the analysis proceeds in much the same fashion, with the treatment dose being discretized at several levels $t$ and the most common quantity of interest again being the difference in expectations of the outcome $Y$:

\[
\EE[Y(t) | r(t, X) = r) - \EE[Y(s) | r(t, X) = r),
\]

relative to some \emph{baseline dose} \(s\). The choice of the latter is natural (\(s=0\)) in pharmacology, for example, where no drug being administered carries a special meaning. In our context, the choice of such a baseline is not clear,  as will be addressed in this work. The ultimate goal is to obtain the full \textit{outcome curve} for all \(t\) with regard to the \(s\) chosen.

In terms of causal inference terminology, we thus have the following variables: the \emph{incompressible noise} $z$ part of the output is treated as the 95-dimensional covariate vector $X$, while the \emph{featural encoding} is considered to be the treatment $t$ and is a five-dimensional vector. The observables considered, such as the number of clicks (output by using the click detector on the generated audio data) for a given $X$ and $t$, are thus the outcome variable $Y_i$. From here on, we will prefer the latter causal inference notation in equations over the usual one used when discussing GANs, i.e., $z$ and $c$.

Each audio output was generated by sampling $X \iid_{1...95} Unif[-1,1]$. Such vectors can be treated as unique within the generated sample of audio outputs used for the estimation of a particular quantity and hence correspond to a separate \textit{unit} indexed by $i$. These were then fed to the generator at each level of the treatment $t \in [-1, 12.5]$ set at each of the five featural bits, meaning the covariates were kept the same across  treatment levels. The generated audio was then run through the appropriate  detection algorithm, such as the click detector, and the outcome $Y_i(t)$ was observed. The total number of units was kept at $N = 2500$.

In other words, we perform a completely randomized experiment with the added bonus that the outcome is observed at each treatment dose for each unit:

$$\P(T_i=t | X_i=x_i) = 1, ~ \forall i$$

This means that there is no \emph{fundamental problem of causal inference} \citep{holland86} at play here, simplifying the estimation. This relies on the assumption that we \emph{can ``re-set'' the unit for each dose}, corresponding to the assumption that $X$ and $t$ have enough informational capacity to encode the vast majority of the latent space salient to our outcome - in other words, given $X$ and $t$, the observable of interest is \textit{consistent}.

Our primary motivation for approaching the problem from the point of view of causal inference is due to being primarily interested in estimating the \emph{amount of an observed effect} in a very complex system without making additional assumptions or needing access to the internals of the generative process, for which we can borrow several interesting methods of estimation from causal inference. Therefore, their (potential) (dis)agreement is also of interest. Likewise, the main goal of this section is to argue for the applicability of such methods.

 The assumption of consistency of the observables has been borne out in similar studies: in a work that generated canary bird vocalizations with GANs \citep{canary21}, the authors found that the total variation of the dataset was sufficiently captured with a latent space length of only 3. Since the training dataset in our case was restricted to the few most popular coda types, we have no reason to severely doubt the consistency assumption, given the total available latent space length of 100. This is also apparent by visually and aurally examining the generated data for fixed \((X, t)\) - the differences, if any, are imperceptible. To summarize - given the whole input \((X, t)\), the observable becomes quite deterministic, while, on the other hand, only a fraction of \(X\) is actually correlated with the outcome of interest, and that in a highly convoluted way. 

It is thus how we square the seemingly opposing concepts of \(X\) being \emph{incompressible noise} only serving as an index to the \textit{units} and the fact that conditioning on the whole of \(X\) gives us \emph{ignorability.} In other words, the working assumption is that the relation of \(X\) to the outcome is so complex that sampling it uniformly at random does not produce a noticeable additional effect besides the treatment \(t\). This can be likened to the concept of \textit{deterministic chaos}, where a non-linear (in $X$) mapping  produces essentially random output, allowing us to perform \textit{completely randomized experiments}. On the other hand, the observables derived from an output generated given a specific \((X,t)\) is consistent, giving us \textit{ignorability} when conditioning on \(X\).

The main part that remains potentially problematic for using causal inference approaches here is that there is less of a guarantee  of a \emph{non-interaction} between the treatment and the covariate $X$. Even though the training enforces $t$ to correspond to consistent output while $X$ can vary without constraints, the process by no means enforces total separation; moreover, what is optimized is actually the mutual information's lower bound \citep{chen16InfoGAN}. This corresponds to potential \emph{treatment effect heterogeneity
induced by the covariates}. In cases when the true outcome function is linear, this is not an issue if the covariates $X$ are centered  $\bar{X}=0$, which is the case here. However, in our case, the function is non-linear (and highly complex);  therefore, a necessary condition for the simpler estimators examined here would be that any possible interaction of the covariates $X$  with the treatment $t$ is an approximately odd function since $X$ are sampled uniformly on $[-1,1]$.




\section{An introduction to the methodology: the number of clicks}
\label{sec:nclicks}

The first observable we're interested in is the \textit{number of clicks} in the generated audio. As this quantity is the easiest to visualize, this section will also introduce the methodological approaches used.

\subsection{Continuous average treatment effect}
\label{continuous-average-treatment-effect}

The most basic estimator is the usual average treatment effect (ATE), applied separately at each treatment dose value $t$. Formally, we're interested in the effect relative to some \emph{baseline dose} $t'$:

\begin{equation}
    \EE[Y(t)] - \EE[Y(t')] = \EE[\EE[Y|X, T=t]] - \EE[\EE[Y|X, T=t']]
    \label{eq:ATEpop}
\end{equation}

Since the units $i$ are defined by their randomly drawn $X$, and we observe every $Y_i(t)$, this simply corresponds to the difference in sample averages:

\begin{equation}
    \hat{\tau} (t) = \frac{1}{N} \sum_{i=1}^{N} Y_i(t) - \frac{1}{N} \sum_{i=1}^{N} Y_i(t'),
    \label{eq:ATEsample}
\end{equation}

This is estimable since both ignorability and overlap hold due to this being a completely randomized experiment, as discussed in Section \ref{sec:continuous-treatment-causal-inference}.

At first, the ``baseline dose'' will be chosen as $t' = [0, 0, 0, 0, 0]$. The treatments correspond to setting the values of single bits in the encoding while keeping the others at zero. However, as mentioned in Section\ref{sec:continuous-treatment-causal-inference}, this baseline \emph{has no special meaning} since every encoding is meaningful; this will  be addressed shortly in Section \ref{sec:better_baseline}



\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/nClicksMeanNew.pdf}
    \caption{The mean number of clicks per dose $t$ level.}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/nClicksATE0New.pdf}
    \caption{The ATE per bit with the baseline at $t=0$.}
  \end{subfigure}
  \caption{The mean number of clicks and the ATE with regard to the baseline $t'=[0, 0, 0, 0, 0]$.}
  \label{fig:ATE0}
\end{figure}


The mean number of clicks per treatment level is presented for all bits in Figure \ref{fig:ATE0}. The results indicate that bit 1 is the most influential on the number of clicks in the outcome. Using the outcome at $0$ as the baseline, the average treatment effect is presented on the right in Figure \ref{fig:ATE0}. In addition to the effect of bit 1, we also note a persistence of an effect in bit 3, which stabilizes at a different "stationary" value. In the following sections, we will examine whether this persists when using different estimators.

We get a fuller picture by examining the standard deviation in the number of clicks, as shown in Figure \ref{fig:nclickStd}. We again observe the same phenomenon of \textit{disentanglement} of bits other than $1$, while the latter takes on the encoding as before. Its real effect is thus the encoding of the \textit{range of clicks} output by the generator.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/nClicksStdNew.pdf}
  \caption{Standard deviation in the number of clicks with increasing value of a particular bit.}
  \label{fig:nclickStd}
\end{figure}


\subsubsection{Choosing a better baseline}
\label{sec:better_baseline}

We observe the issue already mentioned in \citet{begusCiw} of high entanglement of the learned encodings within the range seen in training ($[-1, 1]$), prompting us to disentangle them by setting their value above that range. In tandem with this, the other bits stabilize at roughly a constant effect, which lends further credence to interpreting this as a \textit{process of disentanglement}: with higher values, the primary encoded effect starts to dominate, while the other bits lose their previous, entangled effect on the number of clicks, which is not their primary associated encoding.

This leads us to consider setting different, more natural values for the baseline. The limits of the training range: -1, where the output coalesces out of pure noise (cf. Figure \ref{fig:noise}) and +1, where the process of disentanglement begins, serve as logical choices and are shown in Figure \ref{fig:ATEbaseline}.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/nClicksATE-1.png}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/nClicksATE1.png}
  \end{subfigure}
  \caption{The ATE for the number of clicks with more natural choices of the baseline dose; left: at $t'=-1$, right: at $t'=1$.}
  \label{fig:ATEbaseline}
\end{figure}


\subsection{Incremental causal effect}
\label{incremental-causal-effect}

The question of selecting the appropriate baseline dose is elegantly avoided by using the \emph{incremental causal
effect} proposed by \citet{yu_incremental} --- the effect due to an infinitesimal shift in dosage:

\begin{equation}
    \tau_{ICE} = \EE[Y(T + \delta)] - \EE[Y(T)]
    \label{eq:ICE1}
\end{equation}

Under \emph{local ignorability} and \emph{local overlap,} we have:

\begin{equation}
    \EE[Y'(t)|T=t, X=x] = \partial_t\EE[Y|T=t, X=x],
    \label{eq:ICE2}    
\end{equation}

meaning that the right-hand side - the true incremental causal effect - is identifiable by the expectation of the derivative - the left-hand side.

In our case, both assumptions hold trivially: (i) local ignorability holds because strong ignorability holds, and (ii) local overlap holds since the probability of treatment being assigned is a constant ---  1. Conceptually, we are again interested in this estimator due to its being agnostic with regard to the generative model, with the added benefit of it being invariant to any choice of a baseline.

In interpreting the results, we are interested in whether the incremental effect for a single bit is prominent and consistent across the whole range under consideration while the others are not; in such a case, we can justifiably argue that that bit encodes the number of clicks in a vocalization.

A potential formal drawback of using this estimator in this setup is the discontinuity of the outcome: the number of clicks is a discrete variable, while the authors assume $Y(t)$ to be continuous, meaning that we are formally estimating a discontinuous derivative. However, the finite differences used are naturally always defined. Furthermore, this does not apply to the other observables evaluated in this work.

Its estimator --- $\hat{\tau}_{ICE}$  --- is the usual sample mean of the numeric differences in the outcomes for single units.
It is presented for two bits in Figure \ref{fig:ICE}. We again observe the phenomenon of disentanglement outside the training range, where the effect of bit 1 remains consistently positive. In contrast, that of bit 4, for example, returns to an oscillation around zero.

\begin{figure}[ht!]
  \centering
    \includegraphics[width=0.5\textwidth]{figs/nClicksICE.png}
  \caption{The incremental causal effect on the number of clicks across the range for two bits; the training range
  is delineated by dashed lines.}
  \label{fig:ICE}
\end{figure}

Given the expectation of the derivative curve, we can define the finite sample \emph{expected effect of an  infinitesimal increase:}

\begin{equation}
    \hat{\theta}_{fs} := \frac{1}{N_t} \sum_{i_t=1}^{N_t} \EE[Y'(t_{i_t})|T=t_{i_t}, X=x_i]
    = \frac{1}{N_t}  \frac{1} {N} \sum_{i_t=1}^{N_t} \sum_{i=1}^N \frac{\Delta Y_i(t_{i_t})} {\Delta t_{i_t}},
    \label{eq:ICEEffect}
\end{equation}

where $N_t$ is the number of examined treatment levels with the corresponding $t_{i_t}$, and $N$ is the number of
units with the corresponding $x_i$, each receiving each treatment level.

In short, this is the \emph{expected overall effect} if all the current treatments were infinitesimally shifted (in the positive direction) for the given sample. While formally different from $\EE[Y'(t)]$, this meaning is well-defined and very helpful to the problem at hand.

Moreover, it is a single metric and can be interpreted as an analog to the \emph{average treatment effect} for binary treatments without the need for a baseline dose as a reference. Its values are presented in Table \ref{tab:Expectedinfinitesimal},  corroborating the results obtained with the simple average treatment effect.


{
\small
\begin{table}[ht!]
\caption{\label{tab:Expectedinfinitesimal}The expected effect of an infinitesimal increase in the treatment on the number of clicks.}
\centering
\begin{tabular}[t]{r|r|r|r|r|r}
bit & 0 & 1 & 2 & 3 & 4\\
\hline
$\hat{\theta}_{fs}$ & -0.028  &  0.096 & -0.039 & -0.007 & -0.046\\
$\hat{\theta}_{fs} \vert t \geq 1$ & -0.082  &  0.079 &-0.087 & -0.038 & -0.083
\end{tabular}
\end{table}
}

We again observe the \emph{disentangling} phenomenon where bit 1 stands out in its effect while the others are grouped together in a smaller, opposite effect. We thus have another indication that bit 1 primarily encodes the number of clicks. As before, bit 3 retains a measure of influence on the outcome when using this estimator, as well, as evidenced by its relative distance from bits 0, 2, and 4.

\subsection{Regressing the outcomes directly}
\label{sec:regressingDirectly}

An additional approach is to estimate the outcome with machine learning methods, as done for observational studies in \citet{atheyML}. There, the first step is the estimation of the propensity score, which we can skip here since the latter is always 1. What remains is the regression of the outcome on the treatment and covariate for each unit.

This means we regress the \emph{individual} observed outcomes $Y_i(t)$ (in this section, the number of clicks) against the full input vector for each unit at each level $t$, i.e., for bit 1: $x_i = [0, t, 0, 0, 0 \vert \, z_i]$.
The \emph{outcome curve} in this setting corresponds to the \emph{mean} of the individual inferred outcomes at each treatment level.

For this task, we've chosen boosted tree regression as implemented in the \texttt{lightgbm} package \citep{lightgbm}. This method combines extraordinary flexibility with a greater degree of interpretability as compared to, e.g., using an additional neural net to regress the outcome curve. \texttt{lightgbm} trees differ from other  gradient-boosted trees in that the trees are grown by leaves, as opposed to depth-first. Hence the main parameter  corresponding to model complexity is the (maximum allowed) number of leaves in the base learner.

We intentionally use no sparsity-inducing regularization to prevent overfitting: we wish to check if the hypothesized encodings are consistently picked up by a completely unrelated non-parametric method that is (i) able to approximate any function arbitrarily closely with sufficient model complexity  
(ii) when unregularized, is prone to overfitting. Hence we are looking for \textit{consistency of explanation} across varying model complexity, \textit{despite all odds}.

For determining feature importance, we use values obtained from SHAP \citep{lundbergSHAP}, which uses a game-theoretic concept called the \textit{Shapley value} to distribute attribution to the outcome to the input features. Note that while one could, in theory, use SHAP on the generative model itself, it is much more computationally efficient when used with trees \citep{lundbergSHAPtree}. As mentioned, we additionally wish to test our findings via an unrelated method as opposed to disassembling the generative network.

Most of the hyperparameters except the main one - the number of leaves - were chosen by an early stopping of the training as determined by a separate validation set; the MSE on this set for trees with different numbers of leaves is presented on the right in Figure \ref{fig:treeRegression} and serves as the final determinant of what we consider the optimal model.

We would like to re-emphasize that the goal here is the inference of the effect of particular parts of the input by way of letting an unrelated, expressive model "test out the competing hypotheses" itself. 
Specifically, we would like to see such models of adequate but varying complexity \textit{consistently} assign the credit for the outcome to the encodings suggested by the other methods,  despite not being prohibited from picking up spurious relationships. This differs somewhat from other uses of such methods in causal inference, where correctly accounting for unobserved outcomes takes precedence \citep{athey2016, kunzel2019}.

The figure on the left in Figure \ref{fig:treeRegression} shows the inferred \textit{outcome curve} as predicted by the model with a maximum of 13 leaves per tree (the best model in terms of the MSE) using the whole of the inputs (i.e., including the validation set, which had only been used as a stopping criterion). We can reasonably say that the model is able to approximate the real outcomes sufficiently well.

The SHAP plot in Figure \ref{fig:treeSHAPbit1} corroborates the results obtained with the other estimators, picking up bit 1 as the most salient feature in its effect on the number of clicks for each unit, with the same relationship.  This confirms that the uncovered relationship between the encoding bit and the outcome is not spurious --- i.e., the outcome being just as related to the \textit{incompressible noise} part of the input. Lower values are "bunched" together in a slightly negative effect due to residual entanglement at the lower end of the treatment values. Additionally, the consistency of explanation can be somewhat observed here by the homogeneity of the coloring --- i.e., the  lack of dots representing high values in the region of low impact. 

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/gbmClickFitNew.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/gbmClickMSEbit1New.pdf}
  \end{subfigure}
  \caption{Right: MSE by model complexity as determined by the maximum number of leaves allowed. Left: the outcome curve derived from the data (the 
  mean number of clicks, cf. Fig. \ref{fig:ATE0}) and the one inferred by the best fitting model in terms of the MSE.}
  \label{fig:treeRegression}
\end{figure}


\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.65\textwidth]{figs/shapbeesB1L11New.pdf}
  \caption{SHAP values for the best fitting model for bit 1.}
  \label{fig:treeSHAPbit1}
\end{figure}

The consistency of explanation can be better observed in Figure \ref{fig:heatSHAPbit1},  which presents a \textit{heatmap} plot as produced by the \texttt{shap} package, with the units being ordered on the $x$-axis in terms of increasing treatment value. The $y$-axis displays the features ordered in terms of their overall importance as measured by SHAP. The plot above the central heatmap plot is the output of the model centered around the explanation's mean value, and the bars on the right display the feature's cumulative contribution.

The features for individual units are colored in terms of their contribution to the outcome. For bits encoding the observable, we expect to see \textit{local  consistency} in the assigned effects since the units are ordered by increasing values of $t$. Conversely,  we should observe much less consistently assigned effects for the other bits once the process of disentanglement has reached stationary values. A point we'd like to raise here is that the \textit{CDEV} process does not necessarily start at exactly the same values of treatment for different quantities, so we might not observe full disentanglement for all observables presented.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/shapheatB1L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/shapheatB3L11NewCR.pdf}
  \end{subfigure}
  \caption{SHAP \textit{heatmap} for best-fitting models for bit 1 (left) and bit 3 (right). The instances are ordered in terms of increasing $t$ from left to right.}
  \label{fig:heatSHAPbit1}
\end{figure}

In Figure \ref{fig:heatSHAPbit1}, we thus observe a greater degree of local consistency in the high-end of the treatment value of the bit encoding the property --- bit 1 --- as opposed to bit 3, which has reached the stationary state of disentanglement.

\section{Click spacing and regularity}
\label{sec:regularity}

In this and the following section, we will apply this methodology to uncover additional properties of the communication system that the network encodes as meaningful. In behavior, the encoding works similarly to the one presented for the number of clicks: one bit is assigned an encoding and has an observable effect on the quantity, while the rest move in relative unison to a \emph{stationary value}.

The observables we are interested in in this section are the \textit{click spacing}, as measured by the mean \textit{inter-click interval} (ICI) of a coda, and the \textit{coda regularity}, which we measure by the standard deviation of the inter-click intervals within a coda. Since  these quantities are not completely independent of the overall coda length, we stratify the results by the number of clicks observed.

\subsection{Average treatment effect}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ATE-1ICIbit1New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ATE-1ICIbit2New.pdf}
  \end{subfigure}
    \caption{\textbf{Mean ICI ATE}, stratified by the number of clicks. Left: bit 1, right: bit 2.
    We observe a consistent, monotonic effect for bit 1 regardless of the number of clicks observed, while for, e.g., bit 2, the behavior is inconsistent. The missing high number of clicks data in the latter is simply due to the value of bit 1 being too low.}
  \label{fig:meanICIATE}
\end{figure}

Figure \ref{fig:meanICIATE} shows the ATE with regard to the baseline at $-1$ and $1$ for the mean ICI for bits 1 and 2. We again observe a consistent effect in the value of bit 1, while bit 2 (and others not shown in the figure) do not display any consistency across varying numbers of clicks. Since the former also encodes the range of the number of clicks, as shown in Section \ref{sec:nclicks}, the data for codas with high numbers of clicks is almost impossible to get while keeping the value of bit 1 at 0.

Note that after stratifying by the number of clicks, the overall coda length should not act as a constraint on the mean of the intervals - the network architecture supported outputs up to 2s in coda duration, which was not reached by any of the codas generated; or, indeed, any in the dataset (see Figure S1 in \citep{Gero2016clans}.

We present the  ATE for the inter-click distance regularity (i.e., coda regularity) in Figure \ref{fig:stdICIATE} for both baselines.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ATE-1ICIbit1STDNew.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ATE-1ICIbit2STDNew.pdf}
  \end{subfigure}
    \caption{\textbf{Coda regularity ATE}, stratified by the number of clicks. Left: bit 1, right: bit 2.
    In terms of coda regularity, as measured by the standard deviation of the inter-click intervals in a coda, we again observe a consistent, monotonic relationship with the encoding of bit 1 across codas with varying numbers of clicks, while other bits carry a much more indeterminate behavior.}
  \label{fig:stdICIATE}
\end{figure}


We observe that bit 1 additionally encodes an increasing \textit{coda regularity} (i.e., decreasing variance in the spacings between clicks) across all coda types (proxied here by the number of clicks). Since it also encodes the number of clicks, this implies that the generator has learned to connect these two properties: the codas with a higher number of clicks are more regular, with the clicks being more closely spaced together. This is especially poignant since the same property holds for actual whale codas. Gero et al. \citep{Gero2016clans} results suggest that codas reach a limit in duration around 2s long and that as coda length in clicks increases, mean ICI decreases to fit clicks within this duration limit, which appears to be the result of avoiding overlapping with the next coda within an exchange between whales. Furthermore, codas with more clicks are often more regular in their ICIs regardless of their duration. The generator has inferred this connection \textit{without the codas with a high (>5) number of clicks even being present in the dataset} due to data quality limitations (cf. Section \ref{sec:introduction}) and encoded it in the limited space (5 bits) it has reserved for encodings.  

In this, we again observe the remarkable propensity of generative adversarial networks to discover hidden structures of the data and innovate in semantically meaningful ways, as shown, for instance, in \citet{begusCiw}.


\subsection{Expected effect of an infinitesimal increase}

Table \ref{tab:ExpectedinfinitesimalmeanICI} shows the expected effect of an infinitesimal increase (Eq. \ref{eq:ICEEffect}) in bit values on the mean ICI, stratified by the overall number of clicks in the coda.



{
\small
\begin{table}[ht!]
    \centering
    \begin{tabular}{l|rrrrrrrrrr}
    \toprule
    \# clicks &     2  &     3  &     4  &     5  &     6  &     7  &     8  &     9  &     10 &     11 \\
    bit &        &        &        &        &        &        &        &        &        &        \\
    \midrule
    0   & -0.013 & -0.002 &  0.007 &  0.007 &  0.005 &  0.004 &  0.004 &  0.009 &  0.008 &    N/A \\
    1   & -0.018 & -0.011 & -0.011 & -0.009 & -0.007 & -0.006 & -0.003 & -0.003 & -0.003 & -0.007 \\
    2   & -0.014 & -0.011 & -0.005 &  0.002 &  0.006 &  0.004 & -0.005 & -0.010 & -0.027 & -0.008 \\
    3   & -0.017 & -0.012 & -0.006 & -0.004 &  0.000 &  0.001 &  0.003 & -0.002 & -0.008 &    N/A \\
    4   & -0.022 & -0.017 & -0.011 & -0.002 &  0.004 &  0.005 & -0.001 & -0.006 & -0.001 &  0.000 \\
    \bottomrule
    \end{tabular}
    \caption{\vspace{\baselineskip}\label{tab:ExpectedinfinitesimalmeanICI} The expected effect of an infinitesimal increase in the treatment on the mean inter-click distance.}
\end{table}
}

Since comparing the effects across differing numbers of clicks can be somewhat harder to summarize, we can get an overall picture of the effect of a particular bit by simply looking at whether the effect is positive or negative for generated codas with a particular number of clicks. This is presented as the \emph{sign score} in Table \ref{tab:meanICIScore}.

{
\small
\begin{table}[ht!]
    \centering
    \begin{tabular}{l|rrrrr}
    \toprule
    bit & 0 & 1 & 2 & 3 & 4 \\
    \midrule
    \emph{sign score} & 4  & -10 &  -4  &  -4 &  -4\\
    \bottomrule
    \end{tabular}
    \caption{\vspace{\baselineskip}\label{tab:meanICIScore} Overall \emph{sign score} of an infinitesimal increase in the treatment on the mean inter-click distance.}
\end{table}
}

Similarly, we show the results for coda regularity in Table \ref{tab:ICEstdICI} and the corresponding \emph{sign score} in Table \ref{tab:ICEstdICIScore}.

{
\small
\begin{table}[ht!]
    \begin{tabular}{l|rrrrrrrrrr}
    \toprule
    \# clicks &     2  &     3  &     4  &     5  &     6  &     7  &     8  &     9  &     10 &     11 \\
    bit &        &        &        &        &        &        &        &        &        &        \\
    \midrule
    0   & -0.004 &  0.005 &  0.023 &  0.023 &  0.019 &  0.013 &  0.016 &  0.037 &  0.019 &    N/A \\
    1   & -0.013 & -0.009 & -0.007 & -0.006 & -0.005 & -0.003 & -0.001 & -0.005 & -0.007 & -0.005 \\
    2   & -0.019 & -0.014 & -0.003 &  0.012 &  0.022 &  0.021 & -0.004 &      0 & -0.072 & -0.074 \\
    3   & -0.014 & -0.009 & -0.006 & -0.003 & -0.003 & -0.001 &  0.001 & -0.013 & -0.004 &    N/A \\
    4   & -0.017 & -0.012 & -0.005 &  0.012 &  0.016 &  0.016 &  -0.01 & -0.006 & -0.014 &  0.015 \\
    \bottomrule
    \end{tabular}
    \caption{\vspace{\baselineskip}\label{tab:ICEstdICI} The expected effect on an infinitesimal increase in the treatment on the standard deviation of the ICIs.}
\end{table}
}
{
\small
\begin{table}[ht!]
    \centering
    \begin{tabular}{l|rrrrr}
    \toprule
    bit & 0 & 1 & 2 & 3 & 4 \\
    \midrule
    \emph{sign score} & 6  & -10 &  -2  &  -8 &  -2\\
    \bottomrule
    \end{tabular}
    \caption{\vspace{\baselineskip}\label{tab:ICEstdICIScore} Overall \emph{sign score} of an infinitesimal increase in the treatment on the inter-click interval standard deviation.}
\end{table}
}


This estimator confirms that bit 1 is consistent in its encoding of both click spacing and regularity, regardless of the overall number of clicks in the generated codas. While bit 3 again remains slightly entangled for the latter quantity, its corresponding values of the expected effects are smaller than those of bit 1.



\subsection{Direct regression}

We again apply the methodology discussed in Section \ref{sec:regressingDirectly}. The observations and corresponding inputs are again stratified by the number of clicks and regressed separately with boosted tree ensembles of varying complexity.  We are investigating whether such models corroborate the encodings discovered by the preceding estimators. Due to this separate regression, it is somewhat more challenging to visually present consistency across multiple numbers of clicks, as we could in the prior section. For the two bits - 1 and 2 -  compared in Figure \ref{fig:meanICIATE}, we show the corresponding out-of-sample MSEs for the coda regularity regression as a function of model complexity in Figure \ref{fig:ICIstdmses}. The figure demonstrates that more complex models are increasingly finding spurious relationships for bit 2, while this is not the case for bit 1, which the preceding estimators suggest encodes the coda regularity. This is another way of saying that the relationship is consistent in bit 1, while not in bit 2.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/ICIlgbmMSE1New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/ICIlgbmMSE2New.pdf}
  \end{subfigure}
  \caption{Validation set MSEs for left: bit 1, right: bit 2 for the coda regularity regression (cf. Fig. \ref{fig:stdICIATE}).}
  \label{fig:ICIstdmses}
\end{figure}


Figure \ref{fig:ICImeanshap}  displays the associated SHAP values for the best-fitting models, restricted to codas with five clicks due to the above-mentioned presentation issue. We can again make use of the \textit{heatmap} plots to evaluate the degree the \textit{disentanglement} process has taken place within the range examined. Figure \ref{fig:ICImeanshap} presents the results for the same two bits as before for the mean ICI, while Figure \ref{fig:ICIregshap} does the same for coda regularity.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesB1C5L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesB2C5L8New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatB1C5L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatB2C5L8NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Coda mean ICI}: SHAP values for codas with \textbf{5 clicks} for the best-fitting models for \textit{left}: bit 1 and \textit{right:} bit 2. Bottom: heatmap plots show the effects of individual units ordered in terms of increasing $t$ from left to right.
  Note that the number of outcomes with 5 clicks is different across the two experiments.}
  \label{fig:ICImeanshap}
\end{figure}


For the mean ICIs, shown in Figure \ref{fig:ICImeanshap}, we observe the effect suggested by the preceding estimators. We additionally observe greater \textit{consistency} with regard to the expected value in local neighborhoods for bit 1 but not for bit 2, as evidenced by interchanging positive and negative contributions for units with the same treatment value.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesstdB1C5L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesstdB2C5L8New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatstdB1C5L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatstdB2C5L8NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Coda regularity}: SHAP values for codas with five clicks for the best-fitting models for \textit{left}: bit 1 and \textit{right:} bit 2. Bottom: heatmap plots show the effects of individual units ordered in terms of increasing $t$ from left to right.
   Note that the number of outcomes with 5 clicks is different across the two experiments.}
  \label{fig:ICIregshap}
\end{figure}

Similarly, for coda regularity shown in Figure \ref{fig:ICIregshap}, we again confirm the suggested effect and observe a greater \textit{consistency} of the effect with regard to the expected value in local neighborhoods for bit 1.

Overall, the disentanglement process (as seen in the \textit{heatmap} plots) seems to have progressed further for the mean ICIs than for the coda regularity. The \textit{local inconsistency} of the assigned SHAP values in the bits \textit{not} primarily encoding the quantity is a sign of the bit being almost completely disentangled; we can still, however, pronounce the bit as \textit{not} encoding the quantity in question if it (i) displays the observed behavior, i.e., moving in unison with other bits, and (ii) is inconsistent across codas with a varying number of clicks, as shown in the two preceding sections, as well as with varying model complexity, as illustrated in Figure \ref{fig:ICIstdmses}.

\newpage

\section{Acoustic properties}
\label{sec:auditory}

We now apply the methodology to acoustic quantities, as captured by the spectra at either the coda or click level. So far, little is known (or has been hypothesized) about the informational content of the acoustic properties of whale communication. 


Before calculating the spectra, we apply a slight high-pass filter to the generated data, similar to what \citet{bronsteinWhale} applied to the real data.

\subsection{Average treatment effect}

\subsubsection{Mean spectral frequency}
\label{sec:audiomeanfreq}

The first quantity we consider is the mean spectral frequency with regard to the treatment value, either at the coda or click level. In the case of the former, this is the mean frequency of the coda-level periodogram, computed with an added Hamming window.  At the click level, we first isolated the clicks with our click detector algorithm, then applied the same on the extracted audio slices. The quantity considered in this case is the average spectral mean across all the per-click means in a particular generated coda. The results and corresponding ATEs are displayed in Figure \ref{fig:spectralmeanATE}.



\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectral_meanNew.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectral_mean_clickNew.pdf}
  \end{subfigure}
    \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectralMeanClickATE-1New.pdf}
  \end{subfigure}
    \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectralMeanClickATE+1New.pdf}
  \end{subfigure}
    \caption{Top: left --- coda spectral mean, right --- average click spectral mean. Bottom: The corresponding ATE at the click level (i.e., top-right) for the baseline at $-1$ (left) and $+1$ (right). The encoding seems to be picked up by bit 0.}
  \label{fig:spectralmeanATE}
\end{figure}


We again observe the familiar pattern corresponding to \textit{CDEV} --- one bit has a positive effect on the observable quantity while all the rest have a common, less pronounced negative effect. As before, we thus hypothesize that \textit{bit 0 encodes the spectral mean}. The difference in the coda- and average click-level means is negligible, as is to be expected, leaving us to concentrate on the average click-level quantities for estimation. As before, we will give further credence to this hypothesis by applying the other estimation approaches.

The potential for the spectral means of codas to be meaningful has not been hypothesized in previous research, but the hypothesis is not completely ungrounded. In a recent paper, \cite{madsen23} suggest that odontocetes can vocalize in different registers and that their articulators are sufficiently flexible for such differences to be possible. 

\subsubsection{Acoustic regularity}

We are also interested in \textit{acoustic regularity}, which we measure by the standard deviation of the click
spectral means \textit{within} each generated coda. The per-click spectra were estimated as discussed in Section \ref{sec:audiomeanfreq}. The results are presented in Figure \ref{fig:spectralstdATE}.



\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.55\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectralClickStdNew.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectralClickStdATE-1New.pdf}
  \end{subfigure}
    \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectralClickStdATE+1New.pdf}
  \end{subfigure}
    \caption{Top: Average click spectral mean (per coda) standard deviation. Bottom: The corresponding ATE for  the baseline at $-1$ (left) and $+1$ (right). The encoding seems to be picked up by bit 3.}
  \label{fig:spectralstdATE}
\end{figure}


The results suggest an additional meaningful encoding: bit 3 appears to encode the within-coda \textit{acoustic regularity} of the output. It can, additionally,  be suspected that bits 1, 2, and 4 have not yet reached stationary values of their disentanglement. Unfortunately, we are limited in setting the upper limit of our treatment since, much like on the negative side (cf. Fig. \ref{fig:noise}), the output becomes too noisy above a certain level for meaningful estimation.



\subsection{Wasserstein mean spectral distances}

Since the spectra are distributions themselves, we can measure the  effect of applying treatments in 
specific bits of the encoding on the \textit{overall acoustic output}, as captured by the average coda-level spectrum (across $N$ units with random $X$). The Wasserstein distances from the baseline at $-1$ (where the output
coalesces from noise) and at  $+1$ (the upper limit of the training range) of the average coda-level spectrum for the output generated by setting the corresponding bit to the treatment value $t$ are presented in Figure \ref{fig:audio_click_wasserstein}.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/mean_spectrum_wasserstein-1.png}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/mean_spectrum_wasserstein+1.png}
  \end{subfigure}
    \caption{Average spectrum Wasserstein distance to the spectrum at the value $(-1)$ (left) and $+1$ (right) of the corresponding bit.}
 \label{fig:audio_click_wasserstein}
\end{figure}


 The result shows a relative spectral stabilization slightly above the limit of the training range for bits 2 and 4.  Very notably,  bits 0, 1, and 3, for which we have uncovered observable effects - spectral mean, click number (range) and regularity,  and spectral regularity, respectively,  continue to evolve their average spectra with further treatment. The fact that the start of the disentanglement process is somewhat shifted for acoustic properties was evident in the ATE estimation in the preceding section, as well.  This is also the reason that the average spectra for increasing bit 0 are generally more similar to the average spectrum at $t=+1$ (right figure)  (cf. Fig. \ref{fig:spectralmeanATE}).
 

\subsection{Expected effect of an infinitesimal increase}



Table \ref{tab:infinitesimalCausalMeanFreq} presents the results for the expected effect of an infinitesimal increase on the coda and click mean frequency, both for the overall range of treatments and outside the training range, where the encoding becomes disentangled. The results corroborate the results obtained via the ATE estimator since the effect of bit 0 dominates the others.

{
\small
\begin{table}[ht!]

\caption{\label{tab:infinitesimalCausalMeanFreq} The expected effect of an infinitesimal increase in the treatment on the average coda/click mean frequency}
\centering
\begin{tabular}[t]{l|r r r r}
bit & coda $\hat{\theta}_{fs} (\bar{f})$ & click $\hat{\theta}_{fs} (\bar{f})$ & coda $\hat{\theta}_{fs} (\bar{f}) \vert t \geq 1$ & click $\hat{\theta}_{fs} (\bar{f}) \vert t\geq 1$\\
\hline
0 & 63.07 & 62.15 & 127.37 & 128.58\\
\hline
1 & -14.73 & -30.07 & 25.52 & 10.99\\
\hline
2 & -29.93 & -44.55 & 19.31 & 4.56\\
\hline
3 & -32.85 & -45.48 & 2.75 & -8.71\\
\hline
4 & -41.93 & -53.95 & -1.24 & -0.35\\
\hline
\end{tabular}
\end{table}

}

Similarly, Table \ref{tab:infinitesimalCausalFrequencyMeanStd} shows bit 3 to be the outlier, with bits 0 an 1 (for which we have uncovered associated encodings, hence having an "unintended" effect on the spectral mean standard deviation) also being relative outliers to the baseline of \textit{disentanglement} evident in bits 2 and 4.
{
\small
\begin{table}[ht!]

\caption{\label{tab:infinitesimalCausalFrequencyMeanStd}The expected effect of an infinitesimal increase in the treatment on the average click mean frequency
standard deviation within a coda.}
\centering
\begin{tabular}[t]{l|r r}
bit & $\hat{\theta}_{fs} (\bar{\sigma}_f)$ & $\hat{\theta}_{fs} (\bar{\sigma}_f) \vert t \geq 1$ \\
\hline
0 & -21.17 & -10.42\\
\hline
1 & -21.99 & -10.42 \\
\hline
2 & -35.85 & -30.74\\
\hline
3 & 7.37 & 21.09 \\
\hline
4 & -34.01 & -28.64\\
\hline
\end{tabular}
\end{table}

}

The clicks matched to an observable effect also mostly show a more pronounced infinitesimal causal effect on the \emph{Wasserstein} distance between the average spectra relative to the point where the respective bit value is set to -1 and +1, as shown in Table \ref{tab:infinitesimalCausalWasserstein}. The result for bit 0 is an outlier due to the fact that for the click mean frequency, the disentanglement only picks up with relatively higher values of $t$ (cf. Fig. \ref{fig:spectralmeanATE}).


{
\small
\begin{table}[ht!]
\caption{\label{tab:infinitesimalCausalWasserstein} The expected effect of an infinitesimal increase in the treatment on the Wasserstein distance between average spectra relative to $t=-1$ and $t=+1$}
\centering
\begin{tabular}[t]{l|r r}
bit & $\hat{\theta}_{fs}(W_1(P_t, P_{-1}))$ & $\hat{\theta}_{fs} (W_1(P_t, P_1))$ \\
\hline
0 & 77.57 & 22.00 \\
\hline
1 & 94.37 & 94.37\\
\hline
2 & 74.14 & 74.14 \\
\hline
3 & 78.70 & 78.70\\
\hline
4 & 60.32 & 60.32\\
\hline
\end{tabular}
\end{table}
}


\subsection{Direct regression}

Similarly, we can regress all the observed outcomes $Y_i$ against all the inputs $X_i$ using gradient-boosted trees. In Figure \ref{fig:audiomeanshap}, we again observe the expected patterns when comparing the SHAP values obtained from the best-performing models for the bit that picks up the encoding of spectral mean --- bit 0 --- and bit 4, which does not. As usual, the inferred effects move in opposite directions with regard to the expected (in terms of SHAP) outcome, with the effects becoming random in bit 4 once the process of \textit{disentanglement} is complete.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesB0L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesB4L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatB0L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatB4L11NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Spectral mean}: comparison in SHAP values between \textit{left}: bit 0, and \textit{right} bit 4. Bottom: \textit{heatmap} shows individual units in terms of their increasing value of $t$ from left to right.}
  \label{fig:audiomeanshap}
\end{figure}


Conversely, the results for the spectral regularity shown in Figure \ref{fig:audiostdshap} do not display the common randomness of assigned effect for values with high treatment in bit 4 --- the bit that does not pick up the encoding, meaning that the process of disentanglement has not reached the stationary value yet, as we suspected when discussing the ATE results (Sec. \ref{sec:audiomeanfreq}). Nonetheless, this does not affect the conclusion that bit 3 seems to encode the spectral regularity of a coda, as evidenced by the agreement of all three estimation approaches.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesstdB3L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesstdB4L23New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatstdB3L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatstdB4L23NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Spectral regularity}: comparison in SHAP values between \textit{left}: bit 3, and \textit{right} bit 4. Bottom: \textit{heatmap} shows individual units in terms of their increasing value of $t$ from left to right.}
  \label{fig:audiostdshap}
\end{figure}

 


\section{Conclusion}
\label{sec:conclusion}


In this paper, we have presented a model-agnostic approach to uncover the observable properties a deep generative model encodes as meaningful as a way of gleaning information from data that is alien to us in the true sense of the word: the vocalizations of sperm whales.
In this, we leverage the power of  information-theoretic GANs to encode semantically meaningful properties in a completely unsupervised fashion. Since the model is constrained in the number of such encodings it can learn, we can argue that it must consider these critical to its ability to generate data.

To uncover these properties, we consider the trained model as an experiment and use a technique 
we call \textit{causal disentanglement with extreme values} to facilitate the discovery of the encodings. We present three independent methods inspired by causal inference that enable us to consistently pair up particular bits of the encoding with a physically observable property of the communication system. The agreement between the methods gives further credence to the results.

With this setup, we confirm that the number of clicks, which is what the existing coda classification developed by marine biologists is primarily based upon, indeed seems to be a fundamental property of the communication system. This can be seen as a good grounding point with regard to the credibility of the approach.

Since generative adversarial networks are well known for their innovative generation of examples not seen in the data, we discover that the network correctly associates synthetic codas with a high number of clicks with their increasing regularity by encoding both properties simultaneously, despite scarcely having had access to such codas in the training data. Thus, it correctly infers a property of unseen real-life codas, illustrating its ability to learn the hidden structure of the data.

Using the proposed technique, we also uncover that two acoustic properties  might be meaningful in the sperm whale communication system:  (i) the coda spectral mean and (ii) the regularity in the spectra across the clicks within a coda (coda spectral regularity). These two properties have not been considered significant by previous research. This could serve as an indicator that the  sperm whale communication system is not merely a Morse-like code where only the number of clicks and the intervals between them are meaningful, but that whales actively control the acoustic properties of their vocalizations and encode meaning in those acoustic properties.

Finally, the methodology presented could be applied to any problem for which one would like to leverage the immense expressiveness of models such as GANs as a way of  consistently discovering what properties of the data are semantically meaningful. 

% ---------------------------------------------------------------------

\section*{Acknowledgments}
This study was funded by Project CETI via grants from Dalio Philanthropies and Ocean X; Sea Grape Foundation; Rosamund Zander/Hansjorg Wyss, Chris Anderson/Jacqueline Novogratz through The Audacious Project: a collaborative funding initiative housed at TED.

Data was collected through fieldwork for The Dominica Sperm Whale Project undertaken through permits from Fisheries Division of the Government of Dominica. Research was funded through a FNU fellowship for the Danish Council for Independent Research supplemented by a Sapere Aude Research Talent Award, a Carlsberg Foundation expedition grant, a grant from Focused on Nature, two Explorer Grants from the National Geographic Society, and supplementary grants from the Arizona Center for Nature Conservation, and Quarters For Conservation all to S.G. Further funding was provided by Discovery and Equipment grants from the Natural Sciences and Engineering Research Council of Canada (NSERC) to Hal Whitehead of Dalhousie University, and a FNU large frame grant and a Villum Foundation Grant to Peter Madsen of Aarhus University; and small grants from the Dansk Akustisks Selskab, Oticon Foundation, and the Dansk Tennis Fond to Pernille T\o{}nnesen of Aarhus University. Through 2014-2019, we were grateful for collaborative access to DTAGs and use of custom analytical tag code from Mark Johnson, Peter Tyack, and Peter Madsen.

A.L. would like to thank Prof. Peng Ding (UC Berkeley) for suggesting the incremental causal effect estimator.

% ---------------------------------------------------------------------


\bibliography{paper.bib}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}


\end{document}
