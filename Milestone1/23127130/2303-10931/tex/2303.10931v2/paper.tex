\documentclass{article}
\usepackage{template, times}


%NOTE: User-defined commands, merge if needed:
% \input{macros.tex}
\input{math_commands.tex}



% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023

\PassOptionsToPackage{semicolon, sort&compress}{natbib}

% ready for submission
% \usepackage{neurips_2023}
%NOTE: to check if author formatting etc OK, disable before submission
% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:


% TODO: At submission time, please omit the final and preprint options. This will anonymize your

% submission and add line numbers to aid review. Please do not refer to these line numbers in your
% paper as they will be removed during generation of camera-ready copies.
% \usepackage[]{neurips_2023}

% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2023}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%% EXTRA PACKAGES
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{booktabs}    

\usepackage{caption}
\usepackage{subcaption}

% \usepackage{layouts}

%% ORCID integration
\usepackage{academicons}
% \definecolor{orcidlogocol}{HTML}{A6CE39}
% \newcommand{\orcid}[1]{\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\aiOrcid}}}
\usepackage{orcidlink}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%NOTE: for todos not to complain, comment out for final
% \setlength {\marginparwidth }{1cm}
% \usepackage[textsize=small]{todonotes}
%NOTE: disable todos by uncommenting below 
\usepackage[disable,textsize=small]{todonotes}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
% \newcommand*\cetithanks[2][\value{footnote}]{\footnotemark[#2]}
\bibliographystyle{abbrvnat}
% \setcitestyle{authoryear,open={(},close={)}}
\setcitestyle{numbers}
% open={(},close={)}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Approaching an unknown communication system by latent space exploration and causal inference}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Ga\v{s}per Begu\v{s}\thanks{Equal contribution, alphabetical ordering} $~$ \orcidlink{0000-0002-6459-0551},\\
  University of California, Berkeley,\\
  Berkeley, CA, United States, and\\
  Project CETI,\\
  New York, NY, United States,\\
  \texttt{begus@berkeley.edu}
\And
  Andrej Leban\samethanks $~$ \orcidlink{0000-0003-0617-6843},\\
  Department of Statistics,\\
  University of Michigan,\\
  Ann Arbor, MI, United States, and\\
  Project CETI,\\
  New York, NY, United States,\\
  \texttt{leban@umich.edu} \\
\And
  Shane Gero \orcidlink{0000-0001-6854-044X},\\
  Department of Biology,\\
  Carleton University,\\
  Ottawa, Canada, and\\
  Project CETI,\\
  New York, NY, United States\\
}


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle


\begin{abstract}
This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values with methods inspired by causal inference into an approach we call \textit{causal disentanglement with extreme values} (CDEV) and show that this method yields insights for model interpretability. With this, we can test for what properties of unknown data the model encodes as meaningful, using it to glean insight into the communication system of sperm whales (\textit{Physeter macrocephalus}), one of the most intriguing and understudied animal communication systems. The network architecture used has been shown to learn meaningful representations of speech; here, it is used as a learning mechanism to decipher the properties of another vocal communication system in which case we have no ground truth. The proposed methodology suggests that sperm whales encode information using the number of clicks in a sequence, the regularity of their timing, and audio properties such as the spectral mean and the acoustic regularity of the sequences. Some of these findings are consistent with existing hypotheses, while others are proposed for the first time. We also argue that our models uncover rules that govern the structure of units in the communication system and apply them while generating innovative data not shown during training. This paper suggests that an interpretation of the outputs of deep neural networks with causal inference methodology can be a viable strategy for approaching data about which little is known and presents another case of how deep learning can limit the hypothesis space. Finally, the proposed approach can be extended to other architectures and datasets. 
\end{abstract}


% \printinunitsof{in}\prntlen{\textwidth}

\section{Introduction}
\label{sec:introduction}

How do we approach a communication system for which we not only do not understand what is meaningful but are also unsure about how to go about testing for meaning? One such case is the communication system of sperm whales (\textit{Physeter macrocephalus}), whose vocalizations very likely carry meaning because they are produced in duet-like exchanges or group choruses \citep{Schulz08},  but never while alone  \citep{Whitehead1991}, and appear to be socially learned \citep{Rendell2012}. While evidence supports the use of codas as identity signals \citep{geroIndividual}, there is little that is currently known about what individual utterances mean, or even concrete evidence about what kind of properties of the communication system could serve to carry meaning. 

% there is currently little evidence  about what individual utterances mean or even what kind of properties of the communication system could serve to carry meaning. 
% very little is currently known about what individual utterances mean or even what kind of properties of the communication system could serve to carry meaning. 

In this paper, we propose an approach that aims to provide insight into the latter by using an expressive generative model as the learning mechanism. The network is trained with two objectives: (i) imitation of data and  (ii) encoding of information (Fig. \ref{fig:whale}).  We then combine latent space exploration with methods borrowed from causal inference into a methodology we call \textit{causal disentanglement with extreme values} (CDEV, Fig. \ref{fig:causal}). This approach helps us test for what observable properties the network has learned encode uniquely relevant information for the synthetic vocalizations. If sperm whales encode information into their vocalizations and our model can learn to imitate those well, the encoding in our models can likely reveal what might be meaningful in the sperm whale communication system. We argue that our technique reveals both properties that were posited as meaningful by human researchers as well as novel properties that have, so far, not yet been hypothesized as such. The former can thus serve to bolster the credibility of the latter.


The advantage of the proposed approach is that the discovery of such properties uses as few assumptions as possible, treating the network as a black-box learning unit. The fiwGAN architecture \citep{begusCiw} used has been repeatedly shown to learn a variety of meaningful properties about human language from audio recordings of human speech. Networks trained on raw speech data are shown to learn to associate lexical items with code values and sublexical structure with individual bits \citep{begusCiw,begusZhouInterspeech}, uncovering known linguistic rules \cite{begus2020identity,begusLocal,begus19}.  In other words, the method is verified on the human communication system--- language ---where the ground truth is available. We observe the same ability to infer the hidden structure of the data in our results (Sec. \ref{sec:regularity}). Moreover, learning is performed on raw audio without information-losing (or biasing) transformations.

To our knowledge, this is the first attempt to model sperm whale communication with deep learning on raw acoustic data. In this context, we use the network (in conjunction with CDEV) as an extraordinarily flexible and information-preserving tool for decomposing the data into meaningful, observable properties. Furthermore, the proposed approach, where individual units in the inputs of deep neural networks are manipulated to extreme values, and their effects on observable properties of the outputs are estimated via causal inference methods, could also be applied to other architectures and data with few, if any, modifications.


\subsection{Sperm whale communication}
\label{sec:sperm-whale-communication}

Sperm whales communicate in short (less than two seconds), socially learned series of \emph{clicks} called \emph{codas} that marine biologists have grouped into several coda \emph{types} based on the variation in the number, rhythm, and tempo of the clicks (\citep{whitehead03, CulturalLives} (Fig. \ref{fig:coda}). In the dataset used, there are about 22 different coda types \citep{Gero2016clans}. The actual distribution of coda types is highly asymmetrical - the two most common codas comprise 65\% of all coda vocalizations recorded \citep{geroIndividual}.
   
Due to this rarity of some types, we further limit the training set used in this paper to the five most common coda types (1+1+3, 5R1, 4R2, 5R2, and 5R3). Since we are dealing with a communication system, a significant portion of the data could not be used due to too strong of a presence of whale dialogue \citep{Schulz08}. The residual effects of this are dealt with by our detection algorithms (Sec. \ref{sec:data-generation}). Restricting the number of coda types also allows us to test whether the network can predict unobserved coda types. All in all, 2209 distinct training samples were used. GANs, contrary to other architectures, do not necessarily require extensive datasets and have been shown to learn informative properties of language with similar or substantially smaller datasets \cite{begusLocal,begusCiw}.




\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.375\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/coda_type_1+1+3.pdf}
  \end{subfigure}
  \begin{subfigure}{.375\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/coda_type_9R.pdf}
  \end{subfigure}
  \caption{Examples of real codas. \textbf{Left}: type \textit{1+1+3} with 5 clicks, \textbf{right}: type \textit{9R} with 9 clicks. The 9R type was never part of the training data, yet our network learns to generate codas that resemble this type (Sec. \ref{sec:regularity}).}
 \label{fig:coda}
\end{figure}





\subsection{Related work}


\todo{more citations as examples?}

Combining latent space exploration with the use of causal inference estimators on generated audio trained on data for which we have no ground truth is a novel approach, borrowing both from machine learning interpretability methods, as well as disentangled representation learning (DRL) \citep{bengio_representation_2014}.

In terms of the former, partial dependence plots \citep{friedman_gradboost} sample a section of the data randomly while examining the predicted outcome of a supervised model, which bears some similarities to our application of the ATE estimator (Sec. \ref{sec:nclicks}). Local surrogate models, such as LIME and SHAP \citep{lime2016, lundbergSHAP}, instead focus on explaining the local effect of perturbations via a surrogate model. While both methods bear some similarities to our application of the ICE estimator (Sec. \ref{sec:nclicks}), they are based on individual predictions. 
% \todo{rephrase, shorten}
% % White-box deep learning interpretability methods, such as integrated gradients \citep{integratedgrads}, conversely require access to the internals of the network; our methodology allows us to infer the component parts of the data by using solely the outputs of a generative model, making generalization of the methodology easier.


In terms of the taxonomy of disentangled representation learning presented in \citep{wang_disentangled_2023}, our method is related to dimension-wise unsupervised DRL due to the use of an InfoGAN \citep{chen16InfoGAN} variant as the learning mechanism. We eschew any assumptions about the structure of the generating factors \citep{kim_disentangling_2019}, as well as modifying the loss function of the architecture itself; both of these are easier to justify in applications where the ground truth exists, e.g., image generation \citep{higgins_beta-vae_2016, liu_learning_2022}. Additionally, the application to audio data is a relatively unexplored case; these two factors prompt us to base our approach on an architecture \citep{begusCiw} that has been shown to learn meaningful representations on the closest dataset where the ground truth is available - human speech, and "perform the disentanglement \textit{post-hoc}" (i.e., from generated data) with the use of causal inference methods.
\todo{why post-hoc}
\todo{scholkopf, no need for SCM}
Fully causal DRL methods \citep{suter_robustly_2019} often use a structural causal model as a prior \citep{yang_causalvae_2021}, which is, again, hard to justify for an unknown communication system. As an example, \citep{bose2022controllable, bose2022cage}  use causal inference to quantify implicit causal relationships \textit{between} latent space variables, while our use is based on effect estimation in service of uncovering encoded properties. As an example of the latter, \citep{chockler21} use causal methods for interpreting CNNs in a supervised classification setting when the input is occluded.




\todo{do - metrics for unsupervised?}
% \todo{remove if already contrasting above}
% In contrast to all the methods mentioned, the context and the scope of our method are different - we are manipulating the learned encodings (i.e., the latent space) of an \textit{unsupervised} model and are interested in their \textit{global overall} effect on observable quantities in order to decipher their encoded meaning, with the approach permitting the use of a wide range of estimators.  The method thus requires fewer assumptions than most, is model agnostic, and does not need access to the model's internals. 

% \todo{end rewrite}





\section{The architecture used and the CDEV methodology}
\label{sec:data-generation}

The network architecture used in this work is fiwGAN \citep{begusCiw}, an InfoGAN \citep{chen16InfoGAN} adaptation of the WaveGAN \citep{donahue19} model. In short, the input is partitioned into an \emph{incompressible noise} $X$ and an additional \emph{featural encoding} $t$, which is sampled as Bernoulli variables during training \citep{begusCiw}. The goal of the additional Q-network during training is to correctly determine the value of the encoding $t$ while only having access to the generated data; a loss based on the mutual information between the two is backpropagated to the generator otherwise. This ensures consistency of output across similar values of the encoding vector (Fig. \ref{fig:whale}) and encourages disentanglement in a game that mimics communicative intent - learning by imitation in a fully unsupervised manner.



\begin{figure}[t]
  \centering
  \begin{subfigure}{.425\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/SchematicFiwCropped.pdf}
    \caption{Overview of the fiwGAN architecture.}
    \label{fig:whale}
  \end{subfigure}
  \begin{subfigure}{.275\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/SchematicCDEV.pdf}
    \caption{Overview of the \textit{causal disentanglement with extreme values} (CDEV) technique.}
    \label{fig:causal}
  \end{subfigure}
  \caption{Model and approach overview.}
\end{figure}

\citet{begus19} proposes a technique to uncover individual latent variables that have linguistic meaning by setting them to extreme values outside of those seen in training and interpolating from there.
\todo{point out that outputs don't need to be entirely "realistic" here}
In this work, we conversely treat the generator as an experiment in the vein of causal inference and test for \textit{observable} properties of the data that have (or can be) hypothesized as meaningful. When generating output samples, the incompressible noise - $X$ - entries are sampled randomly, while the featural encoding $t$ is set manually to a desired value. Since the consistency of output with regard to the encoding is only enforced in a loose way, this relationship often only becomes readily apparent when setting the numerical values outside the bounds seen in training, where the primary associated effect begins to dominate \citep{begus19,begus2020identity,begusLocal}. We then apply statistical estimators on the candidate property samples derived from the raw generated outputs to determine whether there exists a statistically consistent relationship between the encoding and the outcomes. This procedure gives rise to a methodology we call \textit{causal disentanglement with extreme values} (CDEV) (Fig. \ref{fig:causal}). 

The space available for the featural encodings is limited; hence, finding the real-world attributes that map almost one-to-one with the encodings suggests that the generator considers them very important to generating convincing outputs, in which it is being checked by the discriminator with access to real data. We limit our featural encoding space to five bits (= $2^5 = 32$ classes) for the five coda types present in the data to allow the model to capture compositionality. However, we demonstrate in Appendix \ref{sub:fiw6} that the method is robust to the number of bits chosen, as well as model training specifics. Similarly, on language data, the architecture uncovers meaningful properties even when there is a mismatch between the number of true meaningful classes and the size of the binary code \cite{begusCiw}. Therefore, any intuition about the desired size of the encoding acts only as a rough prior. Additionally, the uniqueness of the encoding matches is verified by an unrelated method, presented in Appendix \ref{sec:regressingDirectly}.



For the first two observables considered - the number and regularity of clicks (Sections \ref{sec:nclicks} and \ref{sec:regularity}) ---  we use an algorithmic \emph{click detector}. As we move towards more extreme encoding values, the output becomes progressively noisier, as shown for a relatively pathological example in Figure \ref{fig:clickDetector}.  Therefore, the detector uses minimal thresholds, signal filtering, and a sub-algorithm that, if multiple valid "solutions" given the minimal threshold constraints are still found, selects the more evenly spaced-out solution, which corresponds to our intuition. In concert with this, we chose the upper limit of the range tested ($t=12.5$) so that the quantities measured by algorithms remained consistent despite increasingly noisy output (details in Appendix \ref{sec:algoMeasurement}). The latter might, in part, be due to the network training not having converged completely;  since the goal is to discover what observable properties are encoded and not high-fidelity whale communication generation, this is not a significant impediment. Likewise, the actual numeric magnitudes of the estimated effects are less important than discovering the existence of such consistent relationships. The fact that the disentanglement might not be possible for some observables where the detection is too noise-sensitive, together with the need for sufficient sample sizes, which necessitates algorithmic measurement being possible and (potentially) incurs a considerable computational cost, thus act as the main limitations on the proposed method besides the need to specify candidate properties to test for.


\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.35\textwidth]{figs/clickDetectorCropped.pdf}
  \caption{Example of the application of the click detector on noisy generated data. The minor peaks are artifacts picked up in training due to the data not being free of whale dialogue, the red vertical lines are the detected clicks, and the yellow horizontal line is the inferred volume level for the vocalization to be coming from the primary whale.}
  \label{fig:clickDetector}
\end{figure}

\subsection{The experiment from the continuous-treatment causal inference point of view}
\label{sec:continuous-treatment-causal-inference}

The methodology used in this work is inspired by \textit{continuous-treatment} causal inference from the Neyman-Rubin framework point of view \citep{rubin1974estimating}. In short, it deals with an experimental setup where the usual treatment assignment variable \(Z\) becomes a continuous variable \(T\), often called a \emph{dose} due to it being common in the pharmacological context. Given the assumptions common to the discrete case, the analysis proceeds in much the same fashion, with the treatment dose being discretized at several levels $t$. In the case of observational studies, the \textit{propensity score} $e(x)$ thus becomes a function of two variables $r(t, x)$  \citep{imbens_propensity}: 
\begin{equation*}
    e(x) \rightarrow r(t,x) = \P(T=t | X=x);
\end{equation*}
in any setup, the most common quantity of interest is again the difference in the conditional expectations of the outcome $Y$:
\begin{equation*}
    \EE_{Y(t)}[Y(t) | r(t, X) = r] - \EE_{Y(s)}[Y(s) | r(t, X) = r],
    \label{eq:cont_causal}
\end{equation*}
relative to some \emph{baseline dose} \(s\). The choice of the latter is natural (\(s=0\)) in pharmacology, for example, where no drug being administered carries a special meaning. In our context, the choice of such a baseline is not clear, as will be addressed in this work. The ultimate goal is to obtain the full \textit{outcome curve} for all \(t\) with regard to the \(s\) chosen.

In terms of causal inference terminology, we have the following variables: the \emph{incompressible noise} part of the output is treated as the 95-dimensional covariate vector $X$, while the \emph{featural encoding} is considered to be the treatment $t$ and is a five-dimensional vector. The observables considered, such as the number of clicks for a given $X$ and $t$, are thus the outcome variable $Y_i$.  Each output was generated by sampling $X \iid_{1...95} ~\sim~ Unif[-1,1]$. Such vectors can be treated as unique within the generated sample used for the estimation of a particular quantity and hence can be thought of as denoting a separate \textit{unit} indexed by $i$ (cf. Appendix \ref{sec:causal}). These were fed to the generator at each level of the treatment $t \in [-1, 12.5]$ set at each of the five featural bits, meaning the covariates $X$ were kept the same across treatment levels. The generated audio was then run through the appropriate detection algorithm, such as the click detector, giving us the outcome $Y_i(t)$. The total number of units was chosen as $N = 2500$, for which we determined the estimates to have completely stabilized (cf. Appendix \ref{sub:nclicks_acoustic}). This means we perform a completely randomized experiment with the added bonus that the outcome is observed at each treatment dose for each unit,
%
% $$\P(T_i=t | X_i=x_i) = 1, ~ \forall i,$$
%
meaning that there is no \emph{fundamental problem of causal inference} \citep{holland86} at play here, simplifying the estimation. 


\section{An introduction to the methodology: the number of clicks}
\label{sec:nclicks}

The first observable we're interested in is the \textit{number of clicks} in the generated audio samples. As this quantity is the easiest to visualize, this section will also introduce the estimators used. The first estimator used is the \textit{average treatment effect} (ATE), applied separately at each treatment dose value $t$. Formally, we're interested in the effect relative to some \emph{baseline dose} $t'$:

\begin{equation}
    \EE[Y(t)] - \EE[Y(t')] = \EE_X[\EE_{Y}[Y|X, T=t]] - \EE_X[\EE_Y[Y|X, T=t']]
    \label{eq:ATEpop}
\end{equation}

Since the units $i$ are defined by their randomly drawn $X$, and we observe every $Y_i(t)$, this simply corresponds to the difference in sample averages:
 
\begin{equation}
    \hat{\tau} (t) := \frac{1}{N} \sum_{i=1}^{N} Y_i(t) - \frac{1}{N} \sum_{i=1}^{N} Y_i(t'),
    \label{eq:ATEsample}
\end{equation}

The treatments correspond to setting the values of single bits in the encoding while keeping the others at zero. This leaves us to consider what would be the most natural value for the baseline; the limits of the training range: -1, where the output coalesces out of pure noise (cf. Appendix \ref{sec:algoMeasurement}) and +1, where the process of disentanglement begins, serve as logical choices; the results are shown in Figure \ref{fig:ATEbaseline}.

% \todo{see if you can stretch fig. \ref{fig:ATEbaseline}}
\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.35\textwidth}%{.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/nclickATE-1.pdf}
  \end{subfigure}
  \begin{subfigure}{.35\textwidth}%{.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/nclickATE+1.pdf}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}%{.375\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/nclickstdATE-1.pdf}
  \end{subfigure}
  \caption{\textbf{Top:} The ATE for the number of clicks with the baseline chosen as \textbf{left}: at $t'=-1$, \textbf{right}: at $t'=1$.
  \textbf{Bottom:} The ATE for the standard deviation in the number of clicks for the baseline at $t'=-1$.}
  \label{fig:ATEbaseline}
\end{figure}

We observe a high degree of entanglement \citep{begusCiw} of the learned encodings within the range seen in training ($[-1, 1]$). However, when setting the treatment value above that range, all the bits but \textit{bit 1} stabilize at roughly a constant effect, which lends credence to interpreting this as a \textit{process of disentanglement}: with higher values, the primary encoded effect in each bit starts to dominate, leading the other bits to lose their secondary effects on the number of clicks. Thus, we propose that \textit{bit 1} primarily encodes the number of clicks. % 
In addition to the effect of bit 1, we also note a persistence of an effect in bit 3, which stabilizes at a different stationary value.
% We will also examine whether this effect persists when using a different estimator.
Examining the standard deviation in the number of clicks, as shown on the bottom of  Figure \ref{fig:ATEbaseline}, we again observe the same phenomenon of \textit{disentanglement}, leading us to conclude that \textit{bit 1} encodes the \textit{range of clicks} output by the generator.

The question of selecting the appropriate baseline dose is elegantly avoided by using the \emph{incremental causal
effect} (ICE) proposed by \citep{yu_incremental} --- the effect due to an infinitesimal shift in dosage:

\begin{equation}
    \tau_{ICE} := \EE[Y(T + \delta)] - \EE[Y(T)].
    \label{eq:ICE1}
\end{equation}
Since we are dealing with a completely randomized experiment (Sec. \ref{sec:continuous-treatment-causal-inference}, Appendix \ref{sec:causal}), the following holds:

\begin{equation}
    \partial_t\EE[Y|T=t, X=x] = \EE[Y'(t)|T=t, X=x],
    \label{eq:ICE2}    
\end{equation}
meaning that the left-hand side - the true incremental causal effect - is identifiable by the expectation of the derivative - the right-hand side. Its estimator --- $\hat{\tau}_{ICE}$ --- is the usual sample mean of the numeric finite differences in the outcomes for single units. Given the expectation of the derivative curve (Appendix \ref{sec:additionalICE}), we can also define the \emph{expected effect of an infinitesimal increase in the treatment:}

\begin{equation}
    \hat{\theta}_{fs} := \frac{1}{N_t} \sum_{i_t=1}^{N_t} \EE_{Y'(t_{i_t})}[Y'(t_{i_t})|T=t_{i_t}, X=x_i]
    = \frac{1}{N_t}  \sum_{i_t=1}^{N_t} \frac{1} {N}  \sum_{i=1}^N \frac{\Delta Y_i(t_{i_t}, x_i)} {\Delta t_{i_t}},
    \label{eq:ICEEffect}
\end{equation}

where $N_t$ is the number of examined treatment levels with the corresponding $t_{i_t}$, and $N$ is the number of
units with the corresponding $x_i$, each receiving each treatment level. In short, this is the \emph{expected overall effect} if all the current treatments were infinitesimally shifted (in the positive direction) for the given sample. As a single metric, it can be interpreted as an analog to the \emph{average treatment effect} for binary treatments without the need for a baseline dose as a reference. Its values are presented in Table \ref{tab:Expectedinfinitesimal},  corroborating the results obtained with the average treatment effect estimator.
{
\small
\begin{table}[ht!]
\caption{\label{tab:Expectedinfinitesimal}The expected effect of an infinitesimal increase in the treatment on the number of clicks.}
\centering
\begin{tabular}[t]{r|r|r|r|r|r}
bit & 0 & 1 & 2 & 3 & 4\\
\hline
$\hat{\theta}_{fs}$ & -0.028  &  0.096 & -0.039 & -0.007 & -0.046\\
$\hat{\theta}_{fs} \vert t \geq 1$ & -0.082  &  0.079 &-0.087 & -0.038 & -0.083
\end{tabular}
\end{table}
}

% \todo{ disentanglement with SHAP if space}



\section{Click spacing and regularity}
\label{sec:regularity}

We can also apply this methodology to test for additional properties of the communication system that the network encodes as meaningful.  The observables we are interested in in this section are the \textit{click spacing}, as measured by the mean \textit{inter-click interval} (ICI) of a coda, and the \textit{coda regularity}, which we measure by the standard deviation of the inter-click intervals within a coda. Since these quantities are not completely independent of the overall coda length, we stratify the results by the number of clicks observed.

The top row of Figure \ref{fig:ICIATE} shows the ATE with regard to the baseline at $-1$ for the mean ICI for bits 1 and 2. We observe a consistent, monotonic effect in the value of bit 1, while bit 2 (and others not shown in the figure) do not display any such consistency across varying numbers of clicks.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figs/ATE-1meanICIbit1.pdf}
    \end{subfigure}
    \begin{subfigure}{.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figs/ATE-1meanICIbit2.pdf}
    \end{subfigure}
    \begin{subfigure}{.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figs/ATE-1stdICIbit1.pdf}
    \end{subfigure}
    \begin{subfigure}{.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figs/ATE-1stdICIbit2.pdf}
    \end{subfigure}
    \caption{\textbf{Left:} Bit 1, \textbf{right:} bit 2. \textbf{Top:} \textbf{Mean ICI ATE}, stratified by the number of clicks.
        \textbf{Bottom}: \textbf{Coda regularity ATE}, stratified by the number of clicks. The training range is denoted by dashed lines, with the baseline set at $t'=-1$ for all.
    }
  \label{fig:ICIATE}
\end{figure}

We present the ATE for coda regularity in the bottom row of Figure \ref{fig:ICIATE} for the baseline at $t=-1$, again for bits 1 and 2. We observe that bit 1 additionally encodes an increasing \textit{coda regularity} (i.e., decreasing variance in the spacings between clicks) across all coda types (proxied here by the number of clicks). Since it also encodes the number of clicks, this implies that the generator has learned to connect these properties: the codas with a higher number of clicks are more regular, with the clicks being more closely spaced together. This is especially poignant since the same property holds for actual whale codas. \citet{Gero2016clans} suggest that as coda length in clicks increases, mean ICI decreases to fit clicks within this duration limit, which appears to be the result of avoiding overlapping with the next coda within an exchange between whales. Furthermore, codas with more clicks are often more regular in their ICIs, regardless of their duration. The generator has thus inferred this connection \textit{without the codas with a high (>5) number of clicks even being present in the dataset} due to data quality limitations (cf. Sec. \ref{sec:sperm-whale-communication}) and encoded it in the limited space (five bits) it has reserved for encodings.  In this, we again observe the remarkable propensity of generative adversarial networks to discover hidden structures of the data and innovate in semantically meaningful ways. 

As a summary of the overall incremental effects, we can apply and sum the $sign$ function on the expected effect of an infinitesimal increase in the treatment across codas with varying numbers of clicks, which we call the \textit{sign score} and present in Tables \ref{tab:meanICIScore} and \ref{tab:ICEstdICIScore}. This metric confirms that bit 1 is consistent in its encoding of both click spacing and regularity, regardless of the overall number of clicks in the generated codas. While bit 3 again remains slightly entangled for the latter quantity, its actual values of the expected effects were smaller than those of bit 1 (cf. Appendix \ref{sec:appRegularity}).

{
\small
\begin{table}[ht!]
    \parbox{.45\linewidth}{
    \centering
    \begin{tabular}{l|rrrrr}
    \toprule
    bit & 0 & 1 & 2 & 3 & 4 \\
    \midrule
    \emph{sign score} & 4  & -10 &  -4  &  -4 &  -4\\
    \bottomrule
    \end{tabular}
    \caption{
    % \vspace{\baselineskip}
    \label{tab:meanICIScore} Overall \emph{sign score} of an infinitesimal increase in the treatment on the mean inter-click distance.}
% \end{table}
}
\hfill
\parbox{.45\linewidth}{
% \begin{table}[ht!]
    \centering
    \begin{tabular}{l|rrrrr}
    \toprule
    bit & 0 & 1 & 2 & 3 & 4 \\
    \midrule
    \emph{sign score} & 6  & -10 &  -2  &  -8 &  -2\\
    \bottomrule
    \end{tabular}
    \caption{
    % \vspace{\baselineskip}
    \label{tab:ICEstdICIScore} Overall \emph{sign score} of an infinitesimal increase in the treatment on the inter-click interval standard deviation.}
}
\end{table}
}


\section{Acoustic properties}
\label{sec:auditory}

We now apply the methodology to acoustic quantities, as captured by the spectra at the click or coda level. So far, little is known (or has been hypothesized) about the informational content of the acoustic properties of whale communication.  \todo{Shane, could you add some citations?}
The first quantity we consider is the \textit{mean spectral frequency}, where we first isolate the clicks with the click detector, estimate the mean frequency of each periodogram, and compute the average across all the per-click means in a particular generated coda. The corresponding ATE is displayed on the top left in Figure \ref{fig:spectralmeanATE}.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.35\textwidth}%{.325\textwidth}
    \centering
    % \includegraphics[width=\textwidth]{figs/spectralMeanClickATE+1New.pdf}
    \includegraphics[width=\textwidth]{figs/spectralMeanClickATE+1Neurips.pdf}
  \end{subfigure}
  \begin{subfigure}{.35\textwidth}%{.325\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/spectralClickStdATE+1Neurips.pdf}
  \end{subfigure}
  \begin{subfigure}{.46\textwidth}%{.325\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/spectralWasserstein+1Neurips.pdf}
  \end{subfigure}
  \caption{\textbf{Top}, \textbf{left}: the ATE for the click-level \textit{spectral mean} with the baseline at $t=+1$, \textbf{right}: the ATE for the click-level \textit{spectral mean standard deviation} with the same baseline.
  \textbf{Bottom:} The Wasserstein mean coda-level spectral distances to the same baseline.
    }
  \label{fig:spectralmeanATE}
\end{figure}


We again observe the familiar pattern corresponding to \textit{CDEV} --- \textit{bit 0} has a positive effect on the observable quantity while all the rest converge to a smaller, stationary value. Following the outlined process, we thus hypothesize that this bit --- \textit{bit 0 ---  encodes the spectral mean}.

We are also interested in \textit{acoustic regularity}, which we measure by the standard deviation of the click
spectral means \textit{within} each generated coda. The results are presented on the top right in Figure \ref{fig:spectralmeanATE}. The results suggest \textit{an additional meaningful encoding:} \textit{bit 3} appears to encode the within-coda \textit{acoustic regularity} of the output. In this case, it can be suspected that bits 1, 2, and 4 have not yet fully reached stationary values of their disentanglement. 

\todo{vowels here}
The potential for the spectral means of codas to be meaningful had not been hypothesized in previous research, but the hypothesis is not completely ungrounded. In a recent paper, \cite{madsen23} suggest that odontocetes can vocalize in different registers and that their articulators are sufficiently flexible for such differences to be possible. Subsequent to the results presented here, the provided clues --- \textit{coda-level spectral mean and regularity} --- were followed up on in \citet{begus_sprouse_leban_silva_gero_2023}, which identified two discrete patterns in sperm whale communication. The first pattern corresponds to a spectral property analogous to those found in human \textit{vowels}: the codas contain examples that exclusively exhibit either a single or two clearly separated local spectral maxima at the level of clicks, a pattern corresponding to \textit{formants} found in human \textit{a}- and \textit{i}-vowels, respectively. This pattern corresponds to the variation in \textit{spectral means} encoded by our model, corresponding to the encoding of the same coda type in terms of the number and regularity of clicks, but with shifted spectral means (cf. Appendix \ref{sec:audioATE}, Figure \ref{fig:spectralATE} for the results when testing on the coda-level).

The second identified pattern is a trajectory of the formant within a coda which corresponds to the pattern observed in human \textit{dipthongs}: the formants are found to be either level, rising, or falling between clicks within a coda, independently of the coda type \textit{and} vowel. This corresponds to the separate encoding of the coda-level \textit{spectral regularity} by our model, i.e., the codas with rising or falling patterns exhibit a much smaller spectral regularity. The fact that these two patterns were shown to occur in sperm whale dialogues independently of the traditionally recognized coda types (characterized by the number of clicks and their timing and encoded in \textit{bit 1}) and each other (encoded in \textit{bits 0} and \textit{3}, respectively) also gives additional experimental validation of the \textit{disentanglement} achieved with CDEV.


Table \ref{tab:infinitesimalCausalMeanFreq} presents the results for the expected effect of an infinitesimal increase on the click mean frequency, both for the overall range of treatments and the range above the training range. The results corroborate the results obtained via the ATE estimator with the effect of bit 0 dominating the others. Similarly, the results for coda acoustic regularity in Table \ref{tab:infinitesimalCausalFrequencyMeanStd} show bit 3 to be the outlier, with bits 0 an 1 (for which we have uncovered associated observable effects, hence likely having an "unintended" effect on the spectral mean standard deviation) also being relative outliers to the baseline of \textit{disentanglement} evident in bits 2 and 4.
{
\small
\begin{table}[ht!]
    \parbox{.45\linewidth}{
    \centering
    \begin{tabular}[t]{l|r r r r}
    % \hline
    bit & $\hat{\theta}_{fs} (\bar{f})$ & $\hat{\theta}_{fs} (\bar{f}) \vert t\geq 1$\\
    \hline
    0 & 62.15 & 128.58\\
    \hline
    1 & -30.07 & 10.99\\
    \hline
    2 & -44.55 & 4.56\\
    \hline
    3 & -45.48 & -8.71\\
    \hline
    4 & -53.95 & -15.18\\
    \hline
    \end{tabular}
    \caption{\label{tab:infinitesimalCausalMeanFreq} The expected effect of an infinitesimal increase in the treatment on the average click mean frequency.}
  }
\hfill
\parbox{.45\linewidth}{
    \centering
    \begin{tabular}[t]{l|r r}
    % \hline
    bit & $\hat{\theta}_{fs} (\bar{\sigma}_f)$ & $\hat{\theta}_{fs} (\bar{\sigma}_f) \vert t \geq 1$ \\
    \hline
    0 & -21.17 & -10.42\\
    \hline
    1 & -21.99 & -10.42 \\
    \hline
    2 & -35.85 & -30.74\\
    \hline
    3 & 7.37 & 21.09 \\
    \hline
    4 & -34.01 & -21.83\\
    \hline
    \end{tabular}
    \caption{\label{tab:infinitesimalCausalFrequencyMeanStd}The expected effect of an infinitesimal increase in the treatment on the click mean frequency standard deviation within a coda.}
}
\end{table}
}

% \subsection{Wasserstein mean spectral distances}
% \label{sec:wasserstein}

Since the spectra are distributions themselves, we can measure the effect of specific bits on the \textit{overall acoustic output}, as captured by the average coda-level spectrum (across $N$ units with random $X$ for each fixed $t$). The Wasserstein distances from the baseline at $+1$ of the average coda-level spectra are presented on the bottom in Figure \ref{fig:spectralmeanATE}.
%
% \todo{move figure here?}
% \todo{empty space in figure, make use of - maybe the wasserstein table?}
% \begin{figure}
%     \centering
%     \includegraphics[width=.45\textwidth]{figs/mean_spectrum_wasserstein+1.png}
%     \caption{Average spectrum Wasserstein distance to the average spectrum at the value $+1$ (right) of the corresponding bit.}
%     \label{fig:audio_click_wasserstein}
% \end{figure}
 The result shows a relative spectral stabilization close to the limit of the training range for bits 2 and 4. Notably,  bits 0, 1, and 3, for which we have uncovered observable effects - spectral mean, click number (range) and regularity,  and spectral regularity, respectively,  continue to evolve their average spectra with further treatment. 
 % This confirms that those bits also significantly affect the raw output, bolstering our claim that the generator chose to encode properties crucial to the generation of believable synthetic data. 
 % The fact that the start of the disentanglement process is somewhat delayed for acoustic properties was evident in the ATE estimation in the preceding section, as well.  This is also the reason that the average spectra for increasing bit 0 are generally more similar to the average spectrum at $t=+1$ (right figure)  (cf. Fig. \ref{fig:spectralATE}).
 

\section{Conclusion}
\label{sec:conclusion}

% \todo{merge paragraphs if space needed}
In this paper, we have presented a model-agnostic approach to test for candidate observable properties a deep generative model encodes as meaningful as a way of gleaning information from data that is alien to us in the true sense of the word: the communication of sperm whales. In this, we leverage the power of information-theoretic GANs to encode semantically meaningful properties in a completely unsupervised fashion. Since the model is constrained in the number of such encodings it can learn, we can argue that it must consider these critical to its ability to generate data.

To uncover these properties, we consider the trained model as an experiment and propose a method 
we call \textit{causal disentanglement with extreme values} to discover the encodings. We present causal inference-inspired estimation methods that enable us to consistently pair up particular bits of the encoding with a physically observable property of the communication system. The agreement between the methods gives further credence to the results.

With this setup, we confirm that the number of clicks, which is what the existing classification developed by marine biologists is primarily based upon, indeed seems to be a fundamental property of the communication system. This can be seen as a good grounding point with regard to the credibility of the approach. While generating innovative examples of codas not seen in the data, we discover that the network correctly associates synthetic codas with a high number of clicks with their increasing regularity by encoding both properties simultaneously. Thus, it correctly infers a property of unseen real-life codas, illustrating its ability to learn the hidden structure of the data.  Using the proposed technique, we also uncover that two acoustic properties might be meaningful in the system: (i) the coda spectral mean and (ii) the regularity in the spectra across the clicks within a coda (coda spectral regularity). These two properties have not been considered significant by previous research. This could serve as an indicator that the communication system is not merely a Morse-like code where only the number of clicks and the intervals between them are meaningful, but that whales actively control the acoustic properties of their vocalizations and encode meaning in those properties.
\todo{vowels update}
Following on these clues, \citet{begus_sprouse_leban_silva_gero_2023} subsequently identified concrete patterns in sperm whale communication that correspond one--to--one to the encoded acoustic properties presented here.

Finally, the methodology presented could be applied to any problem for which one would like to leverage the immense expressiveness of deep generative models to consistently test for what properties of the data are semantically meaningful. 


\newpage

\bibliography{bibliography.bib}

\newpage
\section*{Acknowledgments}
This study was funded by Project CETI via grants from Dalio Philanthropies and Ocean X; Sea Grape Foundation; Rosamund Zander/Hansjorg Wyss, Chris Anderson/Jacqueline Novogratz through The Audacious Project: a collaborative funding initiative housed at TED.

Data was collected through fieldwork for The Dominica Sperm Whale Project undertaken through permits from Fisheries Division of the Government of Dominica. Research was funded through a FNU fellowship for the Danish Council for Independent Research supplemented by a Sapere Aude Research Talent Award, a Carlsberg Foundation expedition grant, a grant from Focused on Nature, two Explorer Grants from the National Geographic Society, and supplementary grants from the Arizona Center for Nature Conservation, and Quarters For Conservation all to S.G. Further funding was provided by Discovery and Equipment grants from the Natural Sciences and Engineering Research Council of Canada (NSERC) to Hal Whitehead of Dalhousie University, and a FNU large frame grant and a Villum Foundation Grant to Peter Madsen of Aarhus University; and small grants from the Dansk Akustisks Selskab, Oticon Foundation, and the Dansk Tennis Fond to Pernille T\o{}nnesen of Aarhus University. Through 2014-2019, we were grateful for collaborative access to DTAGs and use of custom analytical tag code from Mark Johnson, Peter Tyack, and Peter Madsen.

A.L. would like to thank Prof. Peng Ding (UC Berkeley) for suggesting the incremental causal effect estimator.

\newpage

\appendix
\section{Appendix}

\subsection{Code repository, reproducibility, and robustness}

The accompanying code repository can be found \href{https://github.com/andleb/Approaching-an-unknown-communication-system}{on GitHub} \footnote{\href{https://github.com/andleb/Approaching-an-unknown-communication-system}{\scriptsize {\url{https://github.com/andleb/Approaching-an-unknown-communication-system}}}}. It contains the source code for the model used as the base learning mechanism, the experiment data generating code, and the analysis code needed to replicate the results presented in the paper. 

Since we cannot provide the raw audio source data (cf. Appendix \ref{sec:data}), we provide two trained models in the same repository. The two models included therein are the main model with an encoding space size of five bits, as well as an independently trained model with an encoding space size of six bits that is used to demonstrate the reproducibility of the results in \ref{sub:fiw6}.

We also provide intermediate generated data needed to reproduce the final results presented in the paper. The \texttt{README.md} document located at the root of the repository provides instructions for reproducing the results as well as further technical details, such as the compute resources used to train the model and produce the results.

% \newpage 
% \subsection{Replication and robustness to initialization}

\subsubsection{Robustness to encoding size and model initialization: results obtained from a separate model with a six-bit encoding}
\label{sub:fiw6}

We can verify the robustness to the choice of the number of encodings desired (i.e., size of $t$), as well as to re-initialization by performing the same analysis on an independently trained model where we selected the size of the reserved encoding space $t$ to be \textbf{six} bits. The results for the number of clicks are presented in Figure \ref{fig:nClicks_fiw6} and again show the same pattern of disentanglement with a single bit - bit 0 in this case - picking up the encoding.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.5\textwidth]{figs/nclickATE0Fiw6-1.pdf}
      \caption{The ATE for the number of clicks with regard to the baseline at $-1$ obtained from a separate model that used an encoding size of \textbf{six} bits.}
  \label{fig:nClicks_fiw6}
\end{figure}

Please note that this bit is a different one from the one encoded by the model used for the results presented in Section \ref{sec:nclicks}, demonstrating that due to the nature of the unsupervised learning of the encodings, the ordering of the bits does not imply an ordering of the importance of properties.
Interestingly, we again observe a degree of entanglement of the encoding with an additional bit, in this case, bit 4.
% Additionally, please note that the results presented here were created using a smaller grid of sample points and treatment values since the goal is merely to verify the robustness of the method.  

\subsubsection{An estimate for the accuracy of the estimation: the number of clicks ATE obtained from the acoustic data} 
\label{sub:nclicks_acoustic}

Since the raw data for the acoustic results was generated in an independent run, i.e., with the set of covariates $X$ sampled independently (with a different random seed) of those for the results for the number of clicks and their regularity presented in the paper, we can use it as a check on those results. While a complete error estimation would be computationally expensive, we present the ATEs for the number of clicks from both sets of synthetic data in Figure \ref{fig:nClicks_check}, with their almost exact match validating the significance of the results. 



\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \includegraphics[width=\textwidth]{figs/nClicksATEacoustic.pdf}
  \end{subfigure}
    \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/nclickATE-1Supp.pdf}
  \end{subfigure}
      \caption{Comparison between the ATE for the number of clicks with regard to the baseline at $-1$  from the data generated for the \textbf{left:} acoustic results, \textbf{right:} the click results presented in the main paper.}
  \label{fig:nClicks_check}
\end{figure}


\subsection{Details regarding the architecture}


The loss opitimized by the fiwGAN architecture \citep{begusCiw} is the following:
\begin{equation}
    \min_{G, Q} \max_D V_{I W G A N}(D, G, Q)=V_{W G A N}(D, G)-\lambda L_I(G, Q),
    \label{eq:fiwGAN_loss}
\end{equation}


Where $V_{WGAN}$ refers to the loss of the Wavegan architecture \citep{donahue19}. $L_I$ is the variational lower bound of the mutual information between the encoding and the generated data $I(t; G(X , t))$, and $\lambda$ is a hyperparameter. 
Please see \citet{chen16InfoGAN} for more details.


\subsection{The data}
\label{sec:data}

The dataset of recordings for this study originates from The Dominica Sperm Whale Project (see \citealt{Gero2014}) and was collected off the coast of the island of Dominica between 2014 and 2018. The vast majority of the recordings are made of one sperm whale vocal clan. In the dialect of this Eastern Caribbean Clan, there are about 22 different coda types that have been defined \citep{Gero2016clans}. 

Codas were collected from animal-borne sound and movement tags between 2014 and 2018 (\textit{Dtag} generation 3; \citealt{Dtag2003}). \textit{Dtags} record two-channel audio at 120 kHz with a 16-bit resolution, providing a flat (2 dB) frequency response between 0.4 and 45 kHz. As a result, both coda and echolocation clicks produced by sperm whales were able to be recorded cleanly, allowing us to obtain the temporal patterning and spectral properties of coda clicks used in this analysis.

In order to generate the dataset used for training the neural network, the codas were extracted from the raw audio with the help of annotations provided by marine biologists and subsequently resampled to 32 kHz mono \textit{wav} format and slightly augmented as described in the \texttt{README.md} document in the supplement. Finally, 2209 samples belonging to the five most common coda types (1+1+3, 5R1, 4R2, 5R2, and 5R3) were selected on the basis of the recordings being relatively free of whale dialogue.



\subsection{Details regarding the algorithmic measurement of the quantities of interests}
\label{sec:algoMeasurement}


While the upper bound of the range of treatments was selected so that the quantities measured by the detection algorithms remained consistent (as discussed in the main work), the lower bound was chosen as the lower bound of the training range, as all lower values (for any of the bits) induced the generator to output noise (as illustrated in Fig. \ref{fig:noise}) too often to make estimation possible.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.45\textwidth]{figs/noise.pdf}
  \caption{Example of noisy output for encoding values up to the training range limit of $-1$, in this case, $[-1, 0, 0, 0, 0]$.}
  \label{fig:noise}
\end{figure}

\subsubsection{The click detector}
\label{sec:clickDetector}


To measure the number of clicks and the intervals between them, we use an algorithmic \emph{click detector}. We can make use of prior knowledge to set minimum thresholds for both the amplitude and the temporal separation in order to choose between (potentially) multiple sets of detected "clicks". The latter threshold is based on the minimum  peak separation possible by the whale physiology and was set to 40 ms.  The amplitude threshold is necessary to deal with the residual presence in the dataset of interwoven codas coming from other whales; it was set to $0.4$ in terms of the relative amplitude to the peak click and obtained from an analysis of the amplitudes of vocalizations coming from the primary whale and secondary whales in the data.

Even so, the output can become progressively noisier as we move towards more extreme encoding values (in the positive direction).  Therefore, the detector also uses a band-pass Butterworth filter permitting frequencies between 2 and 16 kHz (based on known spectral ranges of whale clicks), as well as a sub-algorithm that strives to "maximize entropy"  by preferring well-spaced peaks over counting a single jagged peak multiple times: i.e., if multiple valid "solutions" given the minimal threshold constraints are still found, it will prefer the more evenly spaced-out solution, which corresponds to our intuition (cf. Fig. \ref{fig:clickDetector}).



\subsubsection{Measurement of the acoustic properties}

The acoustic properties were derived from the raw generated audio by again applying the filter and then calculating the periodogram (computed with an added Hamming window) in the case of coda-level properties (not presented in the main paper, presented in Appendix \ref{sec:audioATE} as a supplementary result), and the spectrogram in the case of click-level properties. The latter was subsequently cut up into clicks using the click detector described above, and the quantities of interest, such as the per-click spectral mean and the per-click spectral standard deviation, obtained as the weighted statistics, with the spectrogram values serving as weights - e.g., the weighted mean of the spectrogram slice frequencies, weighted by the values of the same slice. The results presented as the click-level spectral mean and regularity were then obtained as the average of the corresponding quantities across the clicks within the coda considered.




\subsection{Additional ATE estimator results --- acoustic properties}
\label{sec:audioATE}

In addition to the click-level quantities presented in the paper, we can also examine the same quantities at the \emph{coda level}. The results for the mean spectral frequency ATE are presented in Figure \ref{fig:spectralATE} and are consistent with each other, with the coda-level estimation naturally involving more noise.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.49\textwidth}
  \includegraphics[width=\textwidth]{figs/spectralMeanCodaATE+1New.pdf}
  \end{subfigure}
  \begin{subfigure}{.49\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/spectralMeanClickATE+1New.pdf}
  \end{subfigure}
      \caption{The (average in the case of click-level) spectral mean ATE for the baseline at $+1$ at the \textbf{left:} \emph{coda-level}, \textbf{right:} \emph{click-level}.}
  \label{fig:spectralATE}
\end{figure}

\newpage

\subsection{Additional ICE estimator results and discussion}
\label{sec:additionalICE}

Formally, \citet{yu_incremental} show that, in order for the true incremental causal effect (ICE) to be identifiable by the expectation of the derivative, we need \emph{local ignorability} and \emph{local overlap.} Based on the discussion in Appendix \ref{sec:causal}, both assumptions hold trivially in our case: (i) local ignorability holds because strong ignorability holds, and (ii) local overlap holds since the probability of treatment being assigned is a constant: -1.

A potential formal drawback of using this metric in this setup is the discontinuity of the outcome for the number of clicks, which is a discrete variable. However, since the estimator used --- $\hat{\tau}_{ICE}$  --- is the sample mean of the finite differences, which are, naturally, always defined, we do not consider this to be a major impediment for this use case. Furthermore, this does not apply to the other observables evaluated in this work.

As an addendum to the aggregate \emph{expected effect of an infinitesimal increase} results presented in the main text, the estimator $\hat{\tau}_{ICE}$ (i.e., the estimator for the quantity in Eq. \ref{eq:ICE1}) for the number of clicks is presented for two bits in Figure \ref{fig:ICE}. We again observe the phenomenon of disentanglement outside the training range, where the effect of bit 1 remains consistently positive. In contrast, that of bit 4, for example, returns to an oscillation around zero. 



\begin{figure}[ht!]
  \centering
    \includegraphics[width=0.5\textwidth]{figs/nClicksICE.png}
  \caption{The incremental causal effect $\hat{\tau}_{ICE}$ estimate on the number of clicks for bits 1 and 4; the training range is delineated by dashed lines.}
  \label{fig:ICE}
\end{figure}




\subsubsection{Click spacing and regularity}
\label{sec:appRegularity}

Table \ref{tab:ExpectedinfinitesimalmeanICI} shows the expected effect of an infinitesimal increase in bit values on the mean ICI, stratified by the overall number of clicks in the coda over the range $t \in [-1, 12.5]$. These results are summarized in the \textit{sign score} presented in Table \ref{tab:meanICIScore} in the main text.


{
\small
\begin{table}[ht!]
    \centering
    \begin{tabular}{l|rrrrrrrrrr}
    \toprule
    \# clicks &     2  &     3  &     4  &     5  &     6  &     7  &     8  &     9  &     10 &     11 \\
    bit &        &        &        &        &        &        &        &        &        &        \\
    \midrule
    0   & -0.013 & -0.002 &  0.007 &  0.007 &  0.005 &  0.004 &  0.004 &  0.009 &  0.008 &    N/A \\
    1   & -0.018 & -0.011 & -0.011 & -0.009 & -0.007 & -0.006 & -0.003 & -0.003 & -0.003 & -0.007 \\
    2   & -0.014 & -0.011 & -0.005 &  0.002 &  0.006 &  0.004 & -0.005 & -0.010 & -0.027 & -0.008 \\
    3   & -0.017 & -0.012 & -0.006 & -0.004 &  0.000 &  0.001 &  0.003 & -0.002 & -0.008 &    N/A \\
    4   & -0.022 & -0.017 & -0.011 & -0.002 &  0.004 &  0.005 & -0.001 & -0.006 & -0.001 &  0.000 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tab:ExpectedinfinitesimalmeanICI} The expected effect of an infinitesimal increase in the treatment on the mean inter-click distance.}
\end{table}
}


Similarly, we show the results for coda regularity in Table~\ref{tab:ICEstdICI}, summarized in Table~\ref{tab:ICEstdICIScore} in the main text.

{
\small
\begin{table}[ht!]
    \begin{tabular}{l|rrrrrrrrrr}
    \toprule
    \# clicks &     2  &     3  &     4  &     5  &     6  &     7  &     8  &     9  &     10 &     11 \\
    bit &        &        &        &        &        &        &        &        &        &        \\
    \midrule
    0   & -0.004 &  0.005 &  0.023 &  0.023 &  0.019 &  0.013 &  0.016 &  0.037 &  0.019 &    N/A \\
    1   & -0.013 & -0.009 & -0.007 & -0.006 & -0.005 & -0.003 & -0.001 & -0.005 & -0.007 & -0.005 \\
    2   & -0.019 & -0.014 & -0.003 &  0.012 &  0.022 &  0.021 & -0.004 &      0 & -0.072 & -0.074 \\
    3   & -0.014 & -0.009 & -0.006 & -0.003 & -0.003 & -0.001 &  0.001 & -0.013 & -0.004 &    N/A \\
    4   & -0.017 & -0.012 & -0.005 &  0.012 &  0.016 &  0.016 &  -0.01 & -0.006 & -0.014 &  0.015 \\
    \bottomrule
    \end{tabular}
    \caption{\label{tab:ICEstdICI} The expected effect of an infinitesimal increase in the treatment on the standard deviation of the ICIs.}
\end{table}
}

Since we have shown \emph{bit 1} to primarily encode the number of clicks, holding it at constant $t_1 = 0$ while varying the other bits results in the codas with a very large number of clicks --- 11 --- not being generated in the case of bits 0 and 3.


\subsubsection{Wasserstein distance ICE}
\label{sec:wassersteinICE}


We can also evaluate the expected effect of an infinitesimal increase in treatment on the average spectral Wasserstein distance (cf. Sec. \ref{sec:auditory}), presented in Table \ref{tab:infinitesimalCausalWasserstein}.
The clicks matched to an observable effect --- bits 0, 1, and 3 -- also mostly show a more pronounced expected effect on the \emph{Wasserstein} distance between the average spectra relative to the point where the respective bit value is set to $t=1$. The result for bit 0 is an outlier due to the fact that for the click mean frequency, the disentanglement only picks up with relatively higher values of $t$ (around $t=5$, cf. top left of Fig. \ref{fig:spectralmeanATE} in the main paper), which means that the spectrum at that point is again relatively similar to the initial point and only diverges at values above that, leading to an overall effect in the distance comparable to bits 2 and 4, which disentangle to a point with about the same average spectral distance effect.



{
\small
\begin{table}[ht!]
    \centering
    \begin{tabular}[t]{l|r r r r r}
    % \hline
    bit &  0 & 1 & 2 & 3 & 4\\
    \hline
    $\hat{\theta}_{fs} (W_1(P_t, P_1))$ & 22.00 & 56.72 & 23.30 &  51.92 & 27.66
    \end{tabular}
    \caption{\label{tab:infinitesimalCausalWasserstein} The expected effect of an infinitesimal increase in the treatment on the Wasserstein distance between average spectra relative to $t=1$.}
\end{table}
}





\subsection{An alternative approach: regressing the outcomes directly}
\label{sec:regressingDirectly}

While our usage of the ATE and ICE estimators bears some relation to explainability methods such as partial dependence plots \citep{friedman_gradboost} and local methods such as LIME \citep{lime2016}, respectively, we can also somewhat mirror the latter in another way by attempting to estimate the effects on the outcomes with a surrogate model. In practice, this means we regress the \emph{individual} observed outcomes $Y_i(t)$ against the full input vector for each unit at each level $t$, i.e., for bit 1: $x_i = [0, t, 0, 0, 0 \vert \, X_i]$. The \emph{outcome curve} in this setting corresponds to the \emph{mean} of the individual inferred outcomes at each treatment level.

For this task, we've chosen boosted tree regression as implemented in the \texttt{lightgbm} package \citep{lightgbm}. For our specific purpose, we intentionally use \textit{no} sparsity-inducing regularization to prevent overfitting: we wish to check if the hypothesized encodings are consistently picked up by a completely unrelated non-parametric method that is (i) able to approximate any function arbitrarily closely with sufficient model complexity  (ii) when unregularized, is prone to overfitting. Hence, we are looking for \textit{consistency of explanation} across varying model complexity, \textit{despite all odds}. In other words, we are testing whether the encodings uncovered by the other two estimators could be spurious.

To explain the results and quantify the consistency of the explanations, we use feature importance values obtained from SHAP \citep{lundbergSHAP}, which uses a game-theoretic concept called the \textit{Shapley value} to distribute attribution to the outcome to the input features. Note that while one could, in theory, use SHAP on the generative model itself, it is much more computationally efficient when used with trees \citep{lundbergSHAPtree}. As mentioned, we also wish to test our findings explicitly via an unrelated method as opposed to disassembling the generative network.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.4\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/gbmClickFitNew.pdf}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/gbmClickMSEbit1New.pdf}
  \end{subfigure}
  \caption{Regression results for the number of clicks outcome: \textbf{Left:} the outcome curve derived from the data (the mean number of clicks) and the one inferred by the best fitting model in terms of the out-of-sample MSE.
  \textbf{Right:} MSE by model complexity as determined by the maximum number of leaves allowed. 
  }
  \label{fig:treeRegression}
\end{figure}



Most of the hyperparameters except the main one - the number of leaves - were chosen by an early stopping of the training as determined by a separate validation set, composed of randomly sampled (in terms of $X$) 10\% of the data; the MSE on this set for trees with different numbers of leaves is presented on the right in Figure \ref{fig:treeRegression} (for the number of clicks outcome) and serves as the final determinant of what we consider the optimal model. The (maximum allowed) number of leaves in the base learner thus serves as our measure of model complexity.

We would like to re-emphasize that the goal here is the inference of the effect of particular parts of the input by way of letting an unrelated, expressive model "test out the competing hypotheses" itself. Specifically, we would like to see such models of adequate but varying complexity \textit{consistently} assign the credit for the outcome to the encodings suggested by the other methods despite not being prohibited from picking up spurious relationships. This differs from other uses of such methods in causal inference, where correctly accounting for unobserved outcomes takes precedence \citep{athey2016, kunzel2019}.

The figure on the left in Figure \ref{fig:treeRegression} shows the inferred \textit{outcome curve} from the model with a maximum of 13 leaves per tree (the best model in terms of the MSE) using the whole of the inputs (including the validation set, which had only been used as a stopping criterion), illustrating that the model is complex enough to be able to fit the observed curve. The training data is included since we want to confirm that the model is able to approximate the generated outcomes sufficiently well, i.e., this is an inference problem.


\subsubsection{The number of clicks}
\label{sec:treeNClicks}

The SHAP plot in Figure \ref{fig:treeSHAPbit1} corroborates the results obtained with the other estimators, picking up bit 1 as the most salient feature in its effect on the number of clicks for each unit, with the same relationship.  This confirms that the uncovered relationship between the encoding bit and the outcome is not spurious --- i.e., the outcome being just as related to the \textit{incompressible noise} $X$ part of the input. Lower values are "bunched" together in a slightly negative effect due to residual entanglement at the lower end of the treatment values. Additionally, the consistency of explanation can be somewhat observed here by the homogeneity of the coloring --- i.e., the lack of dots representing high values in the region of negative impact. 



\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.65\textwidth]{figs/shapbeesB1L11New.pdf}
  \caption{SHAP values for the best fitting model for bit 1.}
  \label{fig:treeSHAPbit1}
\end{figure}

This consistency of explanation can be better observed in Figure \ref{fig:heatSHAPbit1}, which presents a \textit{heatmap} plot as produced by the \texttt{shap} package, with the units being ordered on the $x$-axis in terms of increasing treatment value $t$. The $y$-axis displays the features ordered in terms of their overall importance as measured by SHAP. The plot above the central heatmap plot is the output of the model centered around the explanation's mean value, and the bars on the right display the feature's cumulative contribution.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/shapheatB1L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/shapheatB3L11NewCR.pdf}
  \end{subfigure}
  \caption{SHAP \textit{heatmap} plot for best-fitting models for bit 1 (\textbf{left}) and bit 3 (\textbf{right}). The instances are ordered in terms of increasing $t$ from left to right.}
  \label{fig:heatSHAPbit1}
\end{figure}

The features for individual units are colored in terms of their contribution to the outcome. For bits encoding the observable, we expect to see \textit{local  consistency} in the assigned effects since the units are ordered by increasing values of $t$. Conversely,  we should observe much less consistently assigned effects for the other bits once the process of disentanglement has reached stationary values. A point we'd like to raise here is that the \textit{CDEV} process does not necessarily start at exactly the same values of treatment for different quantities, so we might not observe full disentanglement for all observables presented. As expected, in Figure \ref{fig:heatSHAPbit1}, we thus observe a greater degree of local consistency in the high-end of the treatment value (as evidenced by the direction and magnitude of the attributed effect for the individual units) for the bit encoding the property --- bit 1 --- as opposed to bit 3, which has reached the stationary state of disentanglement.


% \newpage


\subsubsection{Click spacing and regularity}
\label{sec:treeRegularity}


We again apply the same methodology, with the difference being that the observations and corresponding inputs are again stratified by the number of clicks (as in the case of other estimators) and thus regressed separately with boosted tree ensembles of varying complexity. Due to these stratified regressions, it is somewhat more challenging to present consistency across multiple numbers of clicks visually. For the two bits --- 1 and 2 ---  compared in the main paper, we show the corresponding out-of-sample MSEs for the coda regularity regression as a function of model complexity in Figure \ref{fig:ICIstdmses}. The figure demonstrates that more complex models are increasingly finding spurious relationships for bit 2. At the same time, this is not the case for bit 1, for which the preceding estimators suggest encodes the coda regularity. This is another way of saying that the inferred relationship is \emph{consistent} in bit 1 while not in bit 2.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ICIlgbmMSE1New.pdf}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ICIlgbmMSE2New.pdf}
  \end{subfigure}
  \caption{Validation set MSEs for left: bit 1, right: bit 2 for the coda regularity regression stratified by the number of clicks observed.}
  \label{fig:ICIstdmses}
\end{figure}



As in the preceding section, we present the associated SHAP values for the best-fitting models for bits 1 and 2, restricted to codas with \emph{five clicks} due to the above-mentioned presentation issue. We can again make use of the \textit{heatmap} plots to evaluate the degree to which the \textit{disentanglement} process has taken place within the range examined. Figure \ref{fig:ICImeanshap} presents the results for the same two bits as before for the mean ICI, while Figure \ref{fig:ICIregshap} does the same for coda regularity.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesB1C5L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesB2C5L8New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatB1C5L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatB2C5L8NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Coda mean ICI}: \textbf{left:} bit 1, \textbf{right:} bit 2. \textbf{Top:} SHAP values for codas with \textbf{five clicks} for the best-fitting models. \textbf{Bottom}: heatmap plots show the effects of individual units ordered in terms of increasing $t$ from left to right.
  Note that the number of outcomes with five clicks differs between the two subsets.}
  \label{fig:ICImeanshap}
\end{figure}

% \newpage

For the mean ICIs shown in Figure \ref{fig:ICImeanshap}, we observe the effect suggested by the preceding estimators. We additionally observe greater \textit{consistency} with regard to the expected value in local neighborhoods for bit 1 but not for bit 2, as evidenced by interchanging positive and negative contributions for units with the same treatment value.

\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesstdB1C5L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapbeesstdB2C5L8New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatstdB1C5L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/ici/shapheatstdB2C5L8NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Coda regularity}: \textbf{left:} bit 1, \textbf{right:} bit 2. \textbf{Top:} SHAP values for codas with five clicks for the best-fitting models. \textbf{Bottom:} heatmap plots show the effects of individual units ordered in terms of increasing $t$ from left to right. Note that the number of outcomes with five clicks differs between the two subsets.}
  \label{fig:ICIregshap}
\end{figure}

Similarly, for coda regularity shown in Figure \ref{fig:ICIregshap}, we again confirm the suggested effect and observe a greater \textit{consistency} of the effect with regard to the expected value in local neighborhoods for bit 1. Overall, the disentanglement process (as seen in the \textit{heatmap} plots) seems to have progressed further for the mean ICIs than for the coda regularity. The \textit{local inconsistency} of the assigned SHAP values in the bits \textit{not} primarily encoding the quantity is a sign of the bit being almost completely disentangled; we can still, however, pronounce the bit as \textit{not} encoding the quantity in question if it (i) displays the observed disentanglement behavior, i.e., moving in unison with other bits, and (ii) is inconsistent across codas with a varying number of clicks, as demonstrated with the other estimators, as well as with varying model complexity, as illustrated in Figure \ref{fig:ICIstdmses}.




% \newpage



\subsubsection{Acoustic properties}
\label{sec:treeAcoustic}


For the click-level spectral mean, we present the results in Figure \ref{fig:audiomeanshap}. We again observe the expected patterns when comparing the SHAP values obtained from the best-performing models for the bit that picks up the encoding of spectral mean --- bit 0 --- and bit 4, which does not. As usual, the inferred effects move in opposite directions with regard to the expected (in terms of SHAP) outcome, with the effects becoming inconsistent in bit 4 once the process of \textit{disentanglement} is complete.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesB0L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesB4L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatB0L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatB4L11NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Spectral mean}: \textbf{left:} bit 1, \textbf{right:} bit 2. \textbf{Top:} comparison in SHAP values. \textbf{Bottom:} \textit{heatmap} shows individual units in terms of their increasing value of $t$ from left to right.}
  \label{fig:audiomeanshap}
\end{figure}


Conversely, the results for the spectral regularity shown in Figure \ref{fig:audiostdshap} do not display the common inconsistency of assigned effect for values with high treatment in bit 4 --- the bit that does not pick up the encoding, meaning that the process of disentanglement has not reached the stationary value yet, illustrating what was suspected when discussing the ATE results in the main paper. Nonetheless, this does not affect the conclusion that bit 3 seems to encode the spectral regularity of a coda since the bit still displays the expected disentanglement behavior indicated by the other estimation approaches.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesstdB3L11New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapbeesstdB4L23New.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatstdB3L11NewCR.pdf}
  \end{subfigure}
  \begin{subfigure}{.495\textwidth}
  \centering
    \includegraphics[width=\textwidth]{figs/audio/shapheatstdB4L23NewCR.pdf}
  \end{subfigure}
  \caption{\textbf{Spectral regularity}: \textbf{left:} bit 1, \textbf{right:} bit 2. \textbf{Top:} comparison in SHAP values. \textbf{Bottom:} \textit{heatmap} shows individual units in terms of their increasing value of $t$ from left to right.}
  \label{fig:audiostdshap}
\end{figure}


\newpage

\subsection{A further discussion of the approach from the view of causal inference}
\label{sec:causal}


The assumption that we encounter no \emph{fundamental problem of causal inference} \citep{holland86} itself relies on the assumption that we can "re-set" the unit \textit{characterized by its }$X_i$ for each dose $t$. This corresponds to the assumption that $X$ and $t$ have enough informational capacity to encode our observables of interest - in other words, given a specific $X$ and $t$, the observable of interest is \textit{consistent}.

This assumption of consistency of the observables has been borne out in studies with similar datasets: in a work that generated canary bird vocalizations with GANs \citep{canary21}, the authors found that the total variation of the dataset was sufficiently captured with a latent space length of only 3. Since the training dataset in our case was restricted to the few most popular coda types, we have no reason to severely doubt the consistency assumption, given the total available latent space length of 100. This is also apparent by visually and aurally examining the generated data for fixed \((X, t)\) - the differences, if any, are imperceptible. To summarize - given the whole input \((X, t)\), the observable becomes quite deterministic. On the other hand, only a fraction of \(X\) is actually correlated with the outcome of interest (e.g., the number of clicks), and that in a highly convoluted way. 

It is thus how we square the seemingly opposing concepts of \(X\) being \emph{incompressible noise} only serving as an index to the completely randomized \textit{units} and the fact that conditioning on the whole of \(X\) gives us \emph{ignorability}. In other words, the assumption is that the relation of \(X\) to the outcome is so complex that sampling it uniformly at random does not produce a noticeable additional effect besides the treatment \(t\). This can be likened to the concept of \textit{deterministic chaos}, where a non-linear (in $X$) mapping  produces essentially random output, allowing us to perform \textit{completely randomized experiments}. On the other hand, the observables derived from an output generated given a specific \((X,t)\) are consistent, giving us \textit{ignorability} when conditioning on \(X\).

A part that remains potentially an issue here for a fully rigorous causal inference argument is that there is less of a guarantee  of a \emph{non-interaction} between the treatment and the covariate $X$. Even though the training enforces (via the Q-network and the associated mutual-information loss) $t$ to correspond to consistent output while $X$ can vary without constraints, the process by no means enforces total separation; moreover, what is optimized is actually the mutual information's lower bound \citep{chen16InfoGAN}. This corresponds to potential \emph{treatment effect heterogeneity induced by the covariates}. In cases when the true outcome function is linear, this is not an issue if the covariates $X$ are centered $\bar{X} = 0$, which is the case here. In our case, however, the function is non-linear (and highly complex);  therefore, the estimators examined would require that any possible interaction of the covariates $X$ with the treatment $t$ is an approximately odd function (since $X$ are sampled uniformly on $[-1,1]$) for a more rigorous causal argument.

In contrast, our primary motivation for using causal inference approaches is due to being primarily interested in estimating the \emph{presence of an observed effect} in a very complex system without making additional assumptions or needing access to the internals of the generative process, for which we believe the setup presented is sufficient. Additionally, the (potential) (dis)agreement between the estimators presented can also act as another check on the methodology since they are not direct derivatives of one another.



\end{document}