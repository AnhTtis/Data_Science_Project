\documentclass[10pt]{extarticle}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage[sort&compress]{natbib}
\usepackage{hyperref}
\usepackage[inline]{enumitem}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[inline]{enumitem}

\usepackage{cases}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{define}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newenvironment{enumproof}
  { \begin{enumerate}[label={\textbf{\arabic*)}}, wide, labelwidth=!, labelindent=0pt] }
  { \end{enumerate} }

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\newcommand{\eqdef}{\doteq}

\newcommand{\x}{\vect{x}}
\newcommand{\h}{\vect{h}}
\newcommand{\y}{\vect{y}}
\newcommand{\Vx}{\mathbb{V}_{\x}}
\newcommand{\Vh}{\mathbb{V}_{\h}}
\newcommand{\Vy}{\mathbb{V}_{\y}}

\newcommand{\s}{\vect{s}}
\newcommand{\z}{\vect{z}}
\newcommand{\uu}{\vect{u}}

\newcommand{\mF}{\vect{F}}
\newcommand{\mP}{\vect{P}}
\newcommand{\mA}{\vect{A}}
\newcommand{\veta}{\vect{\eta}}
\newcommand{\vnu}{\vect{\nu}}
\newcommand{\vetaA}{\vect{\alpha}}

\title{\bfseries Graph Kalman Filters}
\date{}

\usepackage{authblk}
\author[12]{Cesare Alippi\footnote{Equal contribution.}}

\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]}
\author[1]{Daniele Zambon\protect\CoAuthorMark\thanks{Corresponding author: \texttt{daniele.zambon@usi.ch}}}

\affil[1]{\small The Swiss AI Lab IDSIA \& Universit\`a della Svizzera italiana, Switzerland.}
\affil[2]{\small Politecnico di Milano, Italy.}


\begin{document}

\maketitle


\begin{abstract}
The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
\end{abstract}



\section{Introduction}

The Kalman Filter (KF) \citep{kalman1960new} is a state-space representation architecture for modeling dynamical systems. Since its introduction more than 60 years ago, the KF has been standing out for its performance in tracking and controlling applications as well as for its relative simplicity. 
The KF operates on linear dynamical systems of the form
\begin{equation}\label{eq:state-space-linear-time-inv} 
\begin{cases}
\h_t = \mF\, \h_{t-1} + \vect G\, \x_{t} + \veta_{t}, 
    \\
\y_t = \vect H\, \h_{t} + \vect \vnu_{t},
\end{cases}
\end{equation}
by estimating the hidden system state vector $\h_t\in \R^{d_h}$ at time $t$ given input vector $\x_t\in\R^{d_x}$ and output vector $\y_t\in\R^{d_y}$; $\{\veta_t\}$ and $\{\vnu_t\}$ are white noise stochastic processes. 
At each time step $t$, the system state and its uncertainty update by iteratively leveraging on previous estimates and incorporating newly observed system outputs and inputs while accounting for uncertainties. Notably, KF estimators are proven to be optimal, being unbiased and of minimum variance.
Generalizations and variants of the KF cover continuous-time reformulations of \eqref{eq:state-space-linear-time-inv}, time-variant state-space setups, and nonlinear systems. The nonlinear system formulation of \eqref{eq:state-space-linear-time-inv} can be cast in the form 
\begin{equation}\label{eq:state-space-nonlinear}
\begin{cases}
\h_t = f_\textsc{st}(\h_{t-1},\x_t) + \veta_t,
\\
\y_t = f_\textsc{ro}(\h_t) + \vnu_t;
\end{cases}
\end{equation}
we refer the reader to \citet{julier2004unscented,simon2006optimal}, and references therein, for a comprehensive review. 

In this paper, for the first time in the literature, we provide a graph-based version of the KF where inputs,  outputs, and states are attributed graphs whose topology is allowed to change over time. Its neural deep nonlinear implementation derives from a graph state space (GSS) \citep{zambon2023graph} modeling a discrete-time, time-invariant, stochastic data-generating process $\mathcal P$
\begin{equation}\label{eq:state-space}
\begin{cases}
\h_t = f_\textsc{st}(\h_{t-1},\x_t, \veta_t),
\\
\y_t = f_\textsc{ro}(\h_t, \vnu_t),
\end{cases}
\end{equation}
where inputs $\x_t$, states $\h_t$, and outputs $\y_t$ are attributed graphs belonging to graph spaces $\mathcal X$, $\mathcal H$, and  $\mathcal Y$, respectively. 
Stochastic processes $\{\veta_t\}$, $\{\vnu_t\}$ are white noise impacting the node signals and/or the topology of the graphs.

The GSS formulation \eqref{eq:state-space} poses three main challenges addressed in this paper.
Firstly, functions $f_\textsc{st}$ and $f_\textsc{ro}$ are assumed to be unknown and, differently from \eqref{eq:state-space-nonlinear}, nonlinear also with respect to the noise components.
Secondly, unknown states are attributed graphs of unknown and time-varying topology, hence it is requested us to estimate both node features and graph topology. Note that the topology is a discrete entity;  as such, it is not amenable to standard gradient-based optimization.
Lastly, the data dimensionality -- say the nodes and edges of the involved graphs -- is not fixed and can be very large, yielding ill-posed estimation, in general. 
We address the above challenges, by devising a spatio-temporal graph neural network (STGNN) that approximates the state-transition and the readout functions. 

After reviewing the standard KF and one of its generalizations to nonlinear vector systems in Section~\ref{sec:KF}, we provide the considered GSS system model, formulate the learning problem, and present a parametric family of GSS models to learn the system dynamics in Section~\ref{sec:GSS}. In Section~\ref{sec:GKF}, finally, we derive the proposed graph KF architecture.



\section{Kalman Filters}\label{sec:KF}

Consider the discrete time-invariant system model \eqref{eq:state-space-nonlinear} with a random initial state $\h_0$ drawn from a known finite-variance distribution, \emph{state transition} $f_\textsc{sc}$ and \emph{readout} $f_\textsc{ro}$ are differentiable with respect to the states and affected by white-noise stochastic processes with covariance matrices $\cov[\veta_t]=\vect Q_t$ and $\cov[\vnu_t]=\vect R_t$, for all $t$. %=1,2,3,\dots$.
Let $\h_0,\veta_t$ and $\vnu_t$ be mutually independent, for all $t$.
 
Assume to have observed $\x_i,\y_i$ for all $i\le t-1$ and generated an estimate $\h_{t-1}^+$ of $\E[\h_{t-1}]$ with $P_{t-1}^+\eqdef \cov[\h_{t-1}-\h_{t-1}^+]$.
A single iteration of the KF algorithm aimed at modeling \eqref{eq:state-space-nonlinear} is two-step: 
\begin{enumerate*}[label={(\roman*)}]
    \item 
    Once the new input $\x_t$ is available an \emph{a priori} estimate $\h_t^-$ of $\E[\h_t]$ is produced, along with the covariance matrix $\mP_{t}^-\eqdef\cov[\h_t-\h_t^-]$, followed by a prediction $\y_t^-=f_\textsc{sc}(\h_t^-,\x_t)$ of the system output $\y_t$; estimates denoted with superscript ``$-$'' are named ``a priori'' as they are obtained before observing the system output $\y_t$. 
    \item 
    Once $\y_t$ is observed, an \emph{a posteriori} estimate $\h_t^+$ refines $\h_t^-$ and the covariance matrix $\mP_{t}^+\eqdef \cov[\h_t-\h_t^+]$ is derived from $\mP_{t}^-$.
\end{enumerate*}



\subsection{KF for Linear Systems}
\label{sec:KF-lin}

This section derives the KF procedure for discrete-time, time-variant, linear systems
\begin{equation}\label{eq:state-space-linear-time-var} 
\begin{cases}
\h_t = \mF_{t} \h_{t-1} + \vect G_{t} \x_{t} + \veta_{t}, 
    \\
\y_t = \vect H_{t} \h_{t} + \vnu_{t},
\end{cases}
\end{equation}
a generalization of the time-invariant system \eqref{eq:state-space-linear-time-inv}
where matrices $\mF_t$, $\vect G_t$ and $\vect H_t$ depends on $t$.
Although we aim at developing a KF for time-invariant GSS models like \eqref{eq:state-space}, in order to deal with nonlinear state-transition and readout functions,
it is suitable to rely on
the following time-variant derivation.


\paragraph{A priori estimate.} Note that
\begin{align}
    \E[\h_t]
        &=\E[\mF_{t}\h_{t-1} + \vect G_{t}\x_t + \veta_t]
      \\&=\mF_{t}\E[\h_{t-1}] + \vect G_{t}\x_t + 0,
\end{align}
so, if $\E[\h_{t-1}^+]=\E[\h_{t-1}]$, then 
\begin{equation}\label{eq:kf:ht-}
    \h_t^- \eqdef \mF_{t} \h_{t-1}^+ + \vect G_{t}\x_t
\end{equation}
is an unbiased estimator of $\h_t$; assume $\h_0^+=\E[\h_0]$. The covariance matrix of the a priori estimation error
\begin{align}
    \mP_t^- &= \cov[\h_t - \h_t^-] = \E[(\h_t - \h_t^-)(\h_t - \h_t^-)^\top]
\end{align}
as $\E[\h_{t}^-]=\E[\h_{t}]$, and is expressed as function of the $\mP_{t-1}^+$, from the previous time step:
\begin{align}
\mP_t^-
&=\cov[\mF_{t}(\h_{t-1}-\h_{t-1}^+)+\vect G_t(\x_t-\x_t) + \veta_t]
\\
        &=\cov[\mF_{t}(\h_{t-1}-\h_{t-1}^+)] + \cov[\veta_t],
\\\label{eq:kf:Pt-}
        &=\mF_{t}\mP_{t-1}^+\mF_{t}^\top + \vect R_t,
\end{align}
given the independence between $\veta_t$ and both $\h_{t-1}$ and $\h_{t-1}^+$.


\paragraph{A posteriori estimate.}
The a posteriori estimate has the form
\begin{align}\label{eq:kf:ht+}
    \h_t^+ &\eqdef \h_t^- + \vect K_t (\y_t - \y_t^-)
\end{align}
where matrix $\vect K_t$ is known as \emph{gain} while residual $\y_t - \y_t^-$ is called \emph{innovation}. As $\h_t^-$ is unbiased, then $\E[\y_t-\y_t^-]=0$, and we see that $\h_t^+$ is unbiased as well, regardless of the choice of the gain. Therefore, we can select $\vect K_t$ to minimize the total variance
\begin{equation}
    \textrm{Tr}(\mP_t^+)=\E[(\h_t-\h_t^+)^\top(\h_t-\h_t^+)],
\end{equation}
i.e., the trace of the matrix $\mP_t^+$.
By exploiting the independence between $\h_t^-$ and $\vnu_t$, we get
\begin{align}
    \h_t^+ &= \h_t^- + \vect K_t (\vect H_t(\h_t -\h_t^-)+\vnu_t)
\\
    \h_t - \h_t^+ 
    &= (\h_t -\h_t^-) - \vect K_t \vect H_t(\h_t -\h_t^-) - \vect K_t\vnu_t
\\
    &= (\mathbb I - \vect K_t \vect H_t)( \h_t - \h_t^-) - \vect K_t \vnu_t
\\\label{eq:kf:Pt+}
    \mP_t^+ &= (\mathbb I - \vect K_t \vect H_t)\mP_t^-(\mathbb I - \vect K_t \vect H_t)^\top + \vect K_t \vect R_t\vect K_t^\top.
\end{align}
Finally, the gain minimizing $\textrm{Tr}(\mP_t^+)$ is
\begin{equation}
    \label{eq:kf:Kt}
    \vect K_t = \mP_t^-\vect H_t^\top (\vect H_t\mP_t^-\vect H_t^\top + \vect R_t)^{-1};
\end{equation}
for further developments see, e.g., \citet{simon2006optimal}. 

\paragraph{KF steps.} Given the above, the KF iteration for the generic time step $t$ is
\begin{align}
    \h_t^- &\stackrel{\eqref{eq:kf:ht-}}= \mF_{t} \h_{t-1}^+ + \vect G_{t}\x_t
    \\
    \mP_t^- &\stackrel{\eqref{eq:kf:Pt-}}= \mF_{t}\mP_{t-1}^+\mF_{t}^\top + \vect R_t
    \\
    \vect K_t &\stackrel{\eqref{eq:kf:Kt}}= \mP_t^-\vect H_t^\top (\vect H_t\mP_t^-\vect H_t^\top + \vect R_t)^{-1}
    \\
    \y_t^- &= \vect H_t \h_t^-
    \\
    \h_t^+ &\stackrel{\eqref{eq:kf:ht+}}= \h_t^- + \vect K_t (\y_t - \y_t^-)
    \\
    \mP_t^+ &\stackrel{\eqref{eq:kf:Pt+}}= (\mathbb I - \vect K_t \vect H_t)\mP_t^-(\mathbb I - \vect K_t \vect H_t)^\top + \vect K_t \vect R_t\vect K_t^\top
\end{align}

Finally, we mention that the literature shows different equivalent rewritings to meet specific implementational requirements \citep{simon2006optimal}. Such derivations are out of this paper's scope. 


\subsection{Extended KF for Nonlinear Systems}\label{sec:EKF}

The Extended KF (EKF) adapts the KF of Section~\ref{sec:KF-lin} to the nonlinear case of \eqref{eq:state-space-nonlinear}. EKF operates by linearizing the state-transition and readout functions around the last available state estimate.
EKF requires the first-order Taylor approximation of function $f_\textsc{st}(\,\cdot\,, \x_{t})$ around $\h_{t-1}^+$
\begin{align}\label{eq:ekf:Ft}
    f_\textsc{st}(\h, \x_{t})
&\approx f_\textsc{st}\left( \h_{t-1}^+, \x_{t}\right) 
    + \big(\underbrace{\nabla_{\h} f_\textsc{st}(\h_{t-1}^+, \x_{t})}_{\mF_t}\big) (\h - \h_{t-1}^+) 
\\\label{eq:ekf:xt-tilde}
&= 
    \mF_{t}\h  + f_\textsc{st}\left( \h_{t-1}^+, \x_{t}\right) - \mF_{t}\h_{t-1}^+.
\end{align}
Similarly, we expand with Taylor function $f_\textsc{ro}$ around estimate $\h_{t}^-$
\begin{align}\label{eq:ekf:Ht}
    f_\textsc{ro}(\h)
&\approx f_\textsc{ro}\left( \h_{t}^-\right) 
    + \big(\underbrace{\nabla_{\h} f_\textsc{ro}(\h_{t}^-)}_{\vect H_t}\big) (\h - \h_{t}^-) 
\\\label{eq:ekf:zt}
&= 
    \vect H_{t}\h  + f_\textsc{ro}\left( \h_{t}^-\right) - \vect H_{t}\h_{t}^-.
\end{align}
Derivations lead to the following computation at every time instant $t$:
\begin{align}
    \mF_t &\stackrel{\eqref{eq:ekf:Ft}}=\nabla_{\h} f_\textsc{st}(\h_{t-1}^+, \x_{t})
    \\
    \h_t^- &\stackrel{\eqref{eq:ekf:xt-tilde}}=
    f_\textsc{st}\left( \h_{t-1}^+, \x_{t}\right)
    \\
    \mP_t^- &\stackrel{\eqref{eq:kf:Pt-}}= \mF_{t}\mP_{t-1}^+\mF_{t}^\top + \vect R_t
    \\
    \vect H_t &\stackrel{\eqref{eq:ekf:Ft}}= \nabla_{\h} f_\textsc{ro}(\h_{t}^-)
    \\
    \vect K_t &\stackrel{\eqref{eq:kf:Kt}}= \mP_t^-\vect H_t^\top (\vect H_t\mP_t^-\vect H_t^\top + \vect R_t)^{-1}
    \\
    \y_t^- &\stackrel{\eqref{eq:ekf:zt}}= 
    f_\textsc{ro}\left( \h_{t}^-\right)
    \\
    \h_t^+ &\stackrel{\eqref{eq:kf:ht+}}= \h_t^- + \vect K_t (\y_t - \y_t^-)
    \\
    \mP_t^+ &\stackrel{\eqref{eq:kf:Pt+}}= (\mathbb I - \vect K_t \vect H_t)\mP_t^-(\mathbb I - \vect K_t \vect H_t)^\top + \vect K_t \vect R_t\vect K_t^\top.
\end{align}

As anticipated at the beginning of Section~\ref{sec:KF}, by linearizing the time-invariant system \eqref{eq:state-space-nonlinear} we obtain a linear time-variant system like \eqref{eq:state-space-linear-time-var}, where $\mF_{t}$ and $\vect H_t$ depend on the given input and current state estimates.
The EKF is applicable to more general system models, where the interaction with the noise processes is nonlinear. We expand this discussion in Section~\ref{sec:GKF}, when deriving the KF for graphs.



\section{Graph State-Space Models}
\label{sec:GSS}

\begin{figure}%[b]
\centering
\includegraphics[width=.7\textwidth]{img/graph-signal.drawio.pdf}
\caption{An example of a spatio-temporal data over a set $\Vx$ of 5 nodes. Graph $\x_t$ is given at each step $t$. $\x_t$ is defined over a node set $V(\x_t)\subseteq\Vx$ and has node signals $s_v(\x_t)\in\mathbb R^{d_x}$ associated with each given node.}
\label{fig:dynamic-graph}
\end{figure}

 
The input graph $\x_t\in\mathcal X$ at time $t$ is defined over node set $V(\x_t)$, e.g., associated with the sensors of a sensor network, and edge set represented as adjacency matrix $\mA(\x_t)$ that encodes the relations existing among the nodes, such as physical proximity, signal correlations, or causal dependencies. 
The node sets and the particular topologies observed at different time steps $t,t'$ are generally different but, typically, $V(\x_t)\cap V(\x_{t'})\ne \emptyset$, implying the existence of a partial correspondence among groups of nodes. We denote with $\Vx = \bigcup_{t} V(\x_t)$ the union set of all nodes, whose cardinality is assumed to be finite. 
Input graphs are attributed with node features attached to them, like sensor readings, collected in graph signal $s(\x_t)\in\R^{|\Vx|\times d_x}$. 
A visual representation of the resulting graph-based spatio-temporal data sequence $\x_1,\x_2,\dots,\x_t,\dots$ is provided in Figure~\ref{fig:dynamic-graph}.


Given sequence $\{\x_t\}_t$ of graphs defined over node set $\Vx$, we aim at predicting output graphs $\y_t\in\mathcal Y$ at each time step $t$. 
We model the data-generating process $\mathcal P$ as formulated in \eqref{eq:state-space}, with system model
\begin{equation}\label{eq:state-space-system-model}
\begin{cases}
\h_t = f_\textsc{st}(\h_{t-1},\x_t, \veta_t),
\\
\y_t = f_\textsc{ro}(\h_t, \vnu_t),
\end{cases}
\end{equation}
involving representations of the system states as graphs $\h_t \in \mathcal H$; 
with a consistent notation, $V(\h_t)\in\Vh, \mA(\h_t)$, and $s(\h_t)$ ($V(\y_t)\in\Vy, \mA(\y_t)$, and $s(\y_t)$) denote the node set, the edge set and the signal of state graph $\h_t$ (output graph $\y_t$).
Stochastic processes $\{\veta_t\}$ and $\{\vnu_t\}$ are white noises impacting the edges and the node signals of the states.
Functions $f_\textsc{st}$ and $f_\textsc{ro}$, as well as the noise distributions, are assumed unknown with finite second moments.
Finally, exogenous variables, like those referring to extra sensor information or functional relations, can be included as well in the framework and encoded in $\x_t$ to avoid overwhelming notation.
As better discussed by \citet{zambon2023graph}, the GSS \eqref{eq:state-space-system-model} does not require any identification between the nodes in sets $\Vx$, $\Vh$, and $\Vy$, although this might be the case in some scenarios.

We introduce a GSS family of stochastic predictive models \citep{zambon2023graph}
\begin{equation}\label{eq:state-space-approx-generic}
\begin{cases}
\h_t = f_{\theta}(\h_{t-1}, f_\vartheta(\x_t), \veta_{t})
\\
 \y_t = f_\psi(s(\h_t), \vnu_t)
\end{cases}
\end{equation}
fitted on a realization of process $\mathcal P$ to predict the output graph given the input one.
Initial condition $\h_0$ is drawn from a given prior distribution $P_{\h_0}$. 
Function $f_\vartheta$ is the input encoder, mapping $\x_t$ to the nodes of graph space $\mathcal H$, function $f_\theta$ dictates the state transition and incorporates a graph learning module, whereas $f_\psi$ is the readout. 
Graph functions $f_\vartheta$, $f_\theta$, and $f_\psi$ are parametrized in real vectors $\vartheta,\theta,$ and $\psi$, respectively, and learned directly from data; we assume them differentiable with respect to the associated parameter vectors.
A schematic view of \eqref{eq:state-space-approx-generic} is given in Figure~\ref{fig:gss-approx}.

Importantly, and differently from what is typically done with state-space models, state representations are first predicted and then refined once the system output is observed, in line with traditional vectorial KF. While training the model parameters is carried out with standard deep learning techniques, the state estimate refinement follows the KF proposed here and is derived in the following section.


\begin{figure}
    \vspace{0.5cm}
    \centering
    \includegraphics[width=\textwidth]{img/state-graph-space-scheme.drawio.pdf}
    \caption{Block diagram of the GSS model components in \eqref{eq:state-space-approx-generic}.}
    \label{fig:gss-approx}
\end{figure}


\section{Graph Kalman Filter}\label{sec:GKF}


The proposed Graph KF (GKF) follows the linearization of the EKF, taking care of differentiating  with respect to the noise components, too. 
Accordingly, we assume that both $f_\theta$ and $f_\psi$ are differentiable with respect to the state and the noise terms, as better formalized in following Section~\ref{sec:GKF:linearize}.
To facilitate readability, we assume the following.
\begin{assumption}\label{a:ht-nodes} 
The node sets of input, state, and output graph coincide: $\Vy=\Vh=\Vx$.
\end{assumption}
\begin{assumption}\label{a:fixed-topology} 
The adjacency matrix $\mA(\h_t)$ is written as 
\begin{equation}\label{eq:noise-Aht}
    \mA(\h_t)=\mA_\theta + \vetaA_t,
\end{equation}
where $\vetaA_t$ is a stochastic $|\Vh|\times |\Vh|$ noise matrix accounting for the randomness associated with the state transition (i.e., $\veta_t=\vetaA_t$). 
\end{assumption}
\begin{assumption}\label{a:yt-topology} 
The topology of the output graph is either that of the state ($\mA(\y_t)=\mA(\h_t)$) or that of the input ($\mA(\y_t)=\mA(\x_t)$).
\end{assumption}

We stress that the above assumptions are either made to make the derivation more amenable or ease the readability of the outcomes by providing a simplified notation. 
For instance, Assumption~\ref{a:ht-nodes} ensures an immediate correspondence between nodes of the inputs, states, and outputs (without the need for graph pooling and upscaling operators as in \citep{zambon2023graph}), whereas predicting the output adjacency matrix $\mA(\y_t)$ -- thus relaxing Assumption~\ref{a:yt-topology} -- would only request additional components of $f_\psi$ to be linearized.
It follows that weaker assumptions can be considered at the cost of more complex mathematics that, however, will not change the spirit of what is derived.



\subsection{State-Transition and Readout Functions Linearization}\label{sec:GKF:linearize}


Note that, by Assumption \ref{a:fixed-topology}, $\mA_\theta$ is encoded by parameter vector $\theta$ and can be incorporated in $f_\theta$ so that GSS \eqref{eq:state-space-approx-generic} can be simplified to
\begin{equation}\label{eq:state-space-approx-assumptions}
\begin{cases}
\h_t = f_{\theta}(\s_{t-1}, f_\vartheta(\x_t), \vetaA_{t})
\\
 \y_t = f_\psi(\s_t, \vnu_t)
\end{cases}
\end{equation}
where $\s_t$ denotes the signal $s(\h_t)$; we note that, although the noise term $\vetaA_t$ is added to $\mA_\theta$ to form $\mA(\h_t)$ in \eqref{eq:noise-Aht}, $\vetaA_t$ impacts on the signal $s(\h_t)$, too, if $f_\theta$ is, e.g., an STGNN.
Moreover, with \eqref{eq:state-space-approx-assumptions}, we requests that $f_\theta$ is differentiable with respect to $\s_{t-1}$ and $\vetaA_t$, representable as a vector in $\R^{|\Vh|}$ and a matrix in $\R^{|\Vh|\times|\Vh|}$, respectively; similarly, $f_\psi$ has to be differentiable with respect to $\s_t$ and $\vnu_t$.


Following the EKF procedure, we approximate the nonlinear system \eqref{eq:state-space-approx-assumptions} with a first-order Taylor expansion approximating $f_\theta(\s_{t-1}^+f_\vartheta(\x_t),\vetaA_t)$ and $f_\psi(\s_t^-,\vnu_t)$.

\begin{align}
f_\theta(\s, \uu_t,\vetaA)
&\approx f_\theta\left( \s_{t-1}^+, \uu_t, \vect 0 \right) 
    + \left(\nabla_{\s} f_\theta({\s_{t-1}^+, \uu_t, \vect 0})\right) (\s - \s_{t-1}^+) 
    + \left(\nabla_{\vetaA} f_\theta({\s_{t-1}^+, \uu_t, \vect 0})\right)\bullet \vetaA
\\&= f_\theta\left( \s_{t-1}^+, \uu_t, \vect 0 \right) 
    +  \mF_t (\s - \s_{t-1}^+) 
    + \vect L_t\bullet  \vetaA
\\&= \mF_t \s 
    + {f_\theta\left( \s_{t-1}^+, \uu_t, \vect 0 \right) -  \mF_t \s_{t-1}^+}
    + {\vect L_t \bullet \vetaA}
\end{align}
where  $\uu_t=f_\varphi(\x_t)$ and $\vect B\bullet \vect C\in \R^{|\Vh|}$ denotes the product $$[\vect B\bullet \vect C]_v=\sum_{i,j=1}^{|\Vh|} \vect B_{v,i,j}\vect C_{i,j}$$
for all $\vect B\in\R^{|\Vh|\times|\Vh|\times|\Vh|}$ and $\vect C\in\R^{|\Vh|\times|\Vh|}$. Similarly, we linearize the readout function:
\begin{align}
f_\psi(\s, \vnu) &\approx f_\psi(\s_t^-,\vect 0) 
    + (\underbrace{\nabla_{\s} f_\psi({\s_t^-,\vect 0})}_{\vect H_t}) (\s-\s_t^-)
    + (\underbrace{\nabla_{\vnu} f_\psi({\s_t^-,\vect 0})}_{\vect M_t}) {\vnu}
    \\ &= 
    \vect H_t \s
    + {f_\psi(\s_t^-,\vect 0) - \vect H_t \s_t^-}
    + {\vect M_t\vnu_t}.
\end{align}
We stress out that, even though the graph topology might not be visible in the above notation, graph-based processing is still carried out within the linear operators defined by matrices $\mF_t$ and $\vect L_t$. 





\subsection{Graph KF Iterations}
\label{sec:GKF-inference}

Assume to have learned parameter vectors $\vartheta,\theta,$ and $\psi$ of our GSS model \eqref{eq:state-space-approx-assumptions}, then the following iterations define the proposed Graph KF.
Start by initializing 
\begin{align}
    \s_0^+ &= \E_{\s\sim P_{\s_0}}[\s] \in \R^{|\Vh|}
\\
    \vect P_0^+ &= \cov_{\s\sim P_{\s_0}}[\s] \in \R^{|\Vh|^2},
\end{align}
from a prior distribution $P_{\s_0}$, or by learning $\s_0^+$ and $\vect P_0^+$ directly from data along with parameter vectors $\vartheta,\theta$, and $\psi$. 
Then, for $t=1,2,3,\dots$ 
\begin{enumerate}[label=(\alph*)]
    \item Compute the Jacobians associated with the state transition: 
    \begin{align}\label{eq:gkf:Ft}
    \mF_{t} &= \nabla_{\s}f_\theta(\s_{t-1}^+,f_\vartheta(\x_t),\vect 0)    \in \R^{|\Vh|\times|\Vh|}  
    \\\label{eq:gkf:Lt}
    \vect L_{t} &= \nabla_{\vetaA}f_\theta(\s_{t-1}^+,f_\vartheta(\x_{t}),\vect 0)   \in \R^{|\Vh|\times|\Vh|^2} 
    \end{align}
    (possibly, by utilizing automatic differentiation tools).
    
    \item Update the a priori state estimate: 
    \begin{align}
    \label{eq:gkf:st-}
        \s_t^- &= f_\theta(\s_{t-1}^+,f_\vartheta(\x_{t}), \vect 0)
        \\\label{eq:gkf:Pt-}
        \mP_t^- &= 
            \mF_{t}\mP_{t-1}^+\mF_{t}^\top + \vect L_{t}\vect Q_{t} \vect L_{t}^\top 
    \end{align}

    \item
    Make the prediction
    \begin{equation}\label{eq:gkf:yt-}
        \y_t^- = f_\psi(\s_t^-,\vect 0)
    \end{equation}

    \item Compute Jacobians of the readout:
    \begin{align}\label{eq:gkf:Ht}
    \vect H_{t} &= \nabla_{\s}f_\psi(\s_{t}^-,\vect 0))    \in \R^{|\Vh|\times|\Vh|}  
    \\\label{eq:gkf:Mt}
    \vect M_{t} &= \nabla_{\vnu}f_\psi(\s_{t}^-,\vect 0))    \in \R^{|\Vh|\times|\Vh|}  
    \end{align}

    \item Compute the gain matrix and update the a posteriori state estimate:
    \begin{align}\label{eq:gkf:Kt}
        \vect K_t &= 
        \vect P_t^-\vect H_t^\top (\vect H_t \vect P_t^-\vect H_t^\top + \vect M_t\vect R_t\vect M_t^\top)^{-1}
        \\\label{eq:gkf:st+}
        \s_t^+ &= \s_t^- + \vect K_t (\y_t - \y_t^-)
        \\\label{eq:gkf:Pt+}
        \mP_t^+ &= (\mathbb I - \vect K_t \vect H_t)\mP_t^-(\mathbb I - \vect K_t \vect H_t)^\top + \vect K_t \vect M_t \vect R_t\vect M_t^\top\vect K_t^\top.
    \end{align}
    
\end{enumerate}


\subsection{Model Training with Kalman Filtering}
\label{sec:GKF-training}

We train model parameters $\theta,\vartheta,$ and $\psi$ by gradient-based optimization minimizing the squared error 
\begin{equation}
    \ell_t(\theta,\vartheta,\psi) \eqdef \lVert \y_t - \y_t^-\rVert_2^2 + \lVert \y_t - \y_t^+\rVert_2^2
\end{equation}
of both the a priori prediction $\y_t^-$ in \eqref{eq:gkf:yt-} and the a posteriori prediction
\begin{equation}\label{eq:gkf:yt+}
    \y_t^+ = f_\psi(\s_t^+, \vect 0).
\end{equation}

Note that the computations in \eqref{eq:gkf:Ft}, \eqref{eq:gkf:Lt}, \eqref{eq:gkf:st-} do not involve the perturbed adjacency $\mA(\h_t)$ and noise term $\vetaA_t$ (see Assumption~\ref{a:fixed-topology}), only $\mA_\theta$; similarly, \eqref{eq:gkf:yt-}, \eqref{eq:gkf:Ht}, and \eqref{eq:gkf:Mt} do not account for the noise term $\vnu_t$. As the noise distributions have to be learned along with the rest of the model parameters, during the training phase the above iterations are modified. To compute $\mF_t$, $\vect L_t$, and $\s_t^-$, instead of $\vect 0$, we pass a sample of $\vetaA_t$ drawn from the model learned so far. Likewise, we feed a sample of $\vnu_t$ to compute $\vect M_t$, $\vect H_t$, $\y_t^-$,  and $\y_t^+$. We optimize the probability distributions of $\vetaA_t$ and $\vnu_t$ exploiting straight-through gradient estimators.


\section{Conclusions}
This paper constitutes the first attempt to derive the Kalman and extended Kalman filters where inputs, states, and outputs are attributed graphs of variable topology. The contribution is theoretical and, as such, an experimental section is deferred.
Hypotheses are made to facilitate the reading and derivation thanks to a lighter notational setup; their relaxation is subject of future research that, however, will not change the spirit of what here proposed.


\subsection*{Acknowledgements}
This work was supported by the Swiss National Science Foundation project FNS 204061: \emph{High-Order Relations and Dynamics in Graph Neural Networks}.

\bibliographystyle{plainnat}
\bibliography{biblio}


\end{document}
