% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{booktabs}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{MS-MT: Multi-Scale Mean Teacher with Contrastive Unpaired Translation for Cross-Modality Vestibular Schwannoma and Cochlea Segmentation}


\titlerunning{Multi-scale Mean Teacher}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Ziyuan Zhao\orcidID{0000-0002-4403-825X} \and Huai Zhe Yeo\orcidID{0000-0002-2689-5661} \and Kaixin Xu\orcidID{0000-0002-7222-2628}}

\author{Ziyuan Zhao\inst{1, 2, 3}
\and Kaixin Xu\inst{1}
\and Huai Zhe Yeo\inst{1,4,}\thanks{This work was
done when Huai Zhe was an intern at I2R, A*STAR.}
\and Xulei Yang\inst{1, 2} 
\and Cuntai Guan\inst{3}}


%
\authorrunning{Zhao et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Institute for Infocomm Research (I$^2$R), A*STAR, Singapore \and
Artificial Intelligence, Analytics And Informatics (AI$^3$), A*STAR, Singapore \and
Nanyang Technological University, Singapore \and
National University of Singapore, Singapore 
}

% \institute{Institute for Infocomm Research (I$^2$R), A*STAR, Singapore\\
% \email{Zhao\_Ziyuan@i2r.a-star.edu.sg}\\
% }

%
\maketitle              % typeset the header of the contribution
%


\begin{abstract}
Domain shift has been a long-standing issue for medical image segmentation.
Recently, unsupervised domain adaptation (UDA) methods have achieved promising cross-modality segmentation performance by distilling knowledge from a label-rich source domain to a target domain without labels. 
In this work, we propose a multi-scale self-ensembling based UDA framework for automatic segmentation of two key brain structures~\emph{i.e.,} Vestibular Schwannoma (VS) and Cochlea on high-resolution T2 images.
First, a segmentation-enhanced contrastive unpaired image translation module is designed for image-level domain adaptation from source T1 to target T2.
Next, multi-scale deep supervision and consistency regularization are introduced to a mean teacher network for self-ensemble learning to further close the domain gap.
Furthermore, self-training and intensity augmentation techniques are utilized to mitigate label scarcity and boost cross-modality segmentation performance.
Our method demonstrates promising segmentation performance with a mean Dice score of $83.8\%$ and $81.4\%$ and an average asymmetric surface distance (ASSD) of $0.55$ mm and $0.26$ mm for the VS and Cochlea, respectively in the validation phase of the crossMoDA $2022$ challenge.




% In this work, an unsupervised domain adaptation (UDA) framework is proposed to perform autonomous segmentation of the Vestibular Schwannoma (VS) and Cochlea on high-resolution T2 (HRT2) Magnetic Resonance Images (MRI) using labeled contrast-enhanced T1 (CET1) MRI. Firstly, a shape-preserving contrastive learning approach is used to conduct domain translation from the source CET1 MRI domain and the target HRT2 MRI domain. Next, a multi-scale mean teacher segmentation network is trained using the generated HRT2 MRI and the corresponding ground truth labels. Finally, techniques such as semi-supervised learning using pseudo-labels and image augmentation using medical domain knowledge /are used to mitigate the limited annotations available for training. The method demonstrated a mean dice score of  0.8298 ± 0.1025 and 0.8124 ± 0.0331 and an average asymmetric surface distance (ASSD) of 0.5773 ± 0.3865 and 0.2699 ± 0.1589 for the VS and Cochlea respectively, outperforming several other approaches on the CrossMODA 2022 validation set. 



\keywords{Medical image segmentation \and Unsupervised domain adaptation \and Vestibular Schwannoma}


% Unsupervised domain adaptation, medical image segmentation, cross-modality learning, semisupervised learning, adversarial learning

\end{abstract}
%
%
%
\section{Introduction}
Medical image segmentation plays a vital role in the field of medical image analysis, delivering valuable information for diagnostic analysis and treatment planning~\cite{hesamian2019deep}. 
Accurate segmentation and measurement of Vestibular Schwannoma (VS) and Cochlea from MRI can assist in VS treatment planning, improving clinical workflow~\cite{shapey2019artificial}.
To this end, researchers have turned to deep learning as a solution to perform autonomous segmentation of VS and Cochlea. 
Recent findings also suggest that high-resolution T2 (hrT2) MRI could be a safer and more cost-efficient alternative to contrast-enhanced T1 (ceT1) MRI. 
However, the large domain shift between MRI images with different contrasts coupled with the costly and laborious process of re-annotating medical image scans on another modality, makes it difficult for deep learning to generalize well across both domains.
Therefore, we are encouraged to perform unsupervised domain adaptation~(UDA) and conduct VS and Cochlea segmentation in the hrT2 domain by leveraging labeled ceT1 scans and unlabeled hrT2 scans. 
In this work, we propose an effective and intuitive UDA method based on image translation and self-ensembling learning. 
Firstly, we translate MRI scans from the ceT1 domain to the hrT2 domain using a modified CUT model~\cite{park2020cut} for image-level adaptation. 
Then, a nnU-Net~\cite{nnunet} is trained on synthetic hrT2 images to generate pseudo annotations for unlabeled hrT2 images. 
Finally, we build a multi-scale mean-teacher (MS-MT) network~\cite{meanteacher,li2021hierarchical} to transfer multi-level knowledge from the teacher model to the student model for improving the cross-modality segmentation performance. 
The experimental results show that the proposed UDA network can greatly reduce the domain gap, achieving promising segmentation performance on hrT2 scans.
 
 
\section{Related Work}

To bridge the domain gap across different modalities, many UDA methods have been developed in the medical imaging field to align the distributions between modalities from different perspectives, including image-level alignment~\cite{zhu2017unpaired,huo2018adversarial,zhang2018task}, feature-level alignment~\cite{tzeng2014deep,long2015learning,dou2018pnp,wang2022unsupervised} and their combinations~\cite{hoffman2018cycada,chen2020unsupervised}. Image alignment is a practical UDA approach that translates source images to appear as if they were sampled from the target domain or vice versa. CycleGAN~\cite{zhu2017unpaired} is mainly used to achieve unpaired image-to-image translation for UDA in medical image segmentation~\cite{huo2018adversarial,zhang2018task}. For instance, Huo~\emph{et al.}~\cite{huo2018adversarial} leveraged CycleGAN to perform MRI to CT synthesis for enabling CT splenomegaly segmentation without using target labels. Zhang~\emph{et al.}~\cite{zhang2018task} proposed a task-driven generative adversarial network by introducing segmentation consistency into the DRR to X-ray translation process to achieve X-ray segmentation with the pre-trained DRR segmentation models. Recently, contrastive learning has demonstrated a powerful capacity for unsupervised visual representation learning in various computer vision tasks~\cite{jaiswal2021survey}. Park~\emph{et al.}~\cite{park2020cut} first proposed CUT to use contrastive learning for image synthesis, in which, the mutual information between original and generated patches was maximized to keep the content unchanged after translation. Several studies have applied CUT in cross-domain medical image analysis~\cite{choi2022using,liu2022unsupervised}. Therefore, in this study, we use CUT for image-level adaptation. Another line of research in UDA is to align the feature distributions across domains from different aspects, including explicit discrepancy minimization~\cite{tzeng2014deep,long2015learning} and implicit adversarial learning~\cite{ganin2016domain,dou2018pnp}. For example, Dou~\emph{et al.}~\cite{dou2018pnp} proposed to fine-tune specific feature layers via adversarial learning for cross-modality cardiac segmentation. To achieve image and feature adaptation, CyCADA~\cite{hoffman2018cycada} was proposed to bridge the appearance gap using CycleGAN and align the feature spaces with adversarial learning separately. Chen~\emph{et al.}~\cite{chen2020unsupervised} further introduced additional discriminators in CycleGAN to achieve concurrent feature and image adaptation. These UDA methods have been used in cross-modality VS and Cochlea Segmentation for closing the domain gap~\cite{CrossModaChallenge}. More recently, the usage of weak labels, such as scribbles on the target domain, for weakly-supervised domain adaptation~\cite{dorent2020scribble} has also shown much promise, receiving attention from the community.

On the other hand, since the labels on the target domain are not accessible, not-so-supervised learning methods~\cite{tajbakhsh2020embracing}, such as semi-supervised learning (SSL)~\cite{bai2017semi,meanteacher} can be leveraged to relax the dependence on target labels for improving the adaptation performance~\cite{zou2018unsupervised,zhao2021mt,zhao2022le}. Zou~\emph{et al.}~\cite{zou2018unsupervised} built a self-training pseudo-labeling framework for UDA, in which, pseudo-target labels are generated to retrain the model iteratively for improving the UDA performance. Zhao~\emph{et al}~\cite{zhao2021mt,zhao2022le} investigated the source label scarcity problem in UDA, and leveraged self-ensembling models~\cite{meanteacher} to address annotation scarcity on both domains for label-efficient UDA. These works suggest that SSL techniques can also be leveraged in UDA to improve adaptation performance. In this regard, pseudo-labeling and self-ensembling learning methodologies were explored in our UDA framework.

% iteratively generate pseudo labels and use them to retrain the
% network. 
%  in which pseudo
% labels predicted by a pre-trained model are used to update
% the model for improving SSL performance iteratively.


% everal
% attempts have been made to combine the pseudo-labeling
% scheme with different proposed methods to further improve the
% performance [12]–[14].
%  CUTPark et al. (2020) first used contrastive
% learning for image synthesis

% In an attempt to bridge this gap, the CrossModa challenge~\cite{CrossModaData1,CrossModaData2,CrossModaData3} tasks participants to perform cross-modality medical image segmentation on HRT2 MRI using only annotated CET1 MRI scans with unsupervised domain adaptation (UDA) techniques. As such, this paper introduces a UDA framework to perform image translation from the CET1 source domain to the HRT2 target domain followed by a semi-supervised learning approach to train a segmentation model on the generated HRT2 scans. Firstly, the contrastive unpaired translation (CUT)~\cite{park2020cut,CycleGAN2017,isola2017image} framework was modified by adding a shape-preserving semantic loss to perform contrastive and adversarial learning for HRT2 image generation from CET1 images. Next, building upon the nnU-Net ~\cite{nnunet}, a multi-scale mean teacher approach was used to train a segmentation model using a supervised learning approach with the generated HRT2 images and the ground truth labels. Finally, by inferring pseudo-labels on real HRT2 scans and with the help of image augmentations using medical domain knowledge, the generalizability of the modified nnU-Net could be improved significantly.




% The rapid proliferation and advancement of deep learning have come to define the state-of-the-art across the field of medical image analysis, greatly reducing the need for manual labor in several tasks, such as medical image segmentation. While these models perform well in a fully-supervised setting on a single domain, they often see a big drop in performance and fail to generalize well when subjected to other domains. This is a pervasive issue in medical image analysis due to the multi-modal nature of image capturing techniques such as magnetic resonance images (MRI). 
% An example of this would be in the diagnosis and surveillance of vestibular schwannoma (VS). Recent findings suggest that high-resolution T2 (HRT2) MRI could not only be a safer but more cost-efficient alternative to contrast-enhanced T1 (CET1) MRI. However, CET1 has been the modality of choice in conducting MRI segmentation, meaning that there are limited annotated HRT2 data to perform fully supervised learning for VS segmentation.


\section{Methods}


Given an unpaired dataset of two modalities,~\emph{i.e.,} annotated ceT1 MRI images $\mathcal{D}_{s}=\left\{\left(\mathbf{x}_{i}^{s}, y_{i}^{s}\right)\right\}_{i=1}^{N}$ and non-annotated hrT2 MRI scans  $\mathcal{D}_{t}=\left\{\left(\mathbf{x}_{i}^{t}\right)\right\}_{i=1}^{M}$, sharing the same classes (VS and Cochlea), we aim to exploit $\mathcal{D}_{s}$ and $\mathcal{D}_{t}$ for unsupervised domain adaptation to enhance the cross-modality segmentation performance of the VS and Cochlea on hrT2 MRI images. The overview of our UDA framework is shown in Fig.~\ref{fig:pipeline}.
% Given an unpaired dataset of two MRI contrasts~\cite{CrossModaChallenge,CrossModaData1,CrossModaData2,CrossModaData3}, annotated ceT1 MRI and non-annotated hrT2 MRI, the task was to perform autonomous segmentation of the VS and Cochlea on hrT2 scans. This section will describe the proposed framework in detail to perform this task.

% we focus on the problem of unsupervised domain adaptation for
% medical image semantic segmentation

% Please refer the papers to carefully polish and revise the methodology part

% https://crossmoda-challenge.ml/media/papers/mip.pdf
% https://crossmoda-challenge.ml/media/papers/jwc-rad.pdf
% https://crossmoda-challenge.ml/media/papers/samoyed.pdf
% https://crossmoda-challenge.ml/media/papers/premilab.pdf

\begin{figure}[t]
    \centering
    \includegraphics[width = \columnwidth]{image/Picture4.png}
    \caption{The overview of our proposed method. First, synthetic hrT2 images are generated with the proposed segmentation-enhanced translation network. Then, we employ a nnU-Net for training on synthetic hrT2 images and generate pseudo labels for unlabeled hrT2 images. Finally, a multi-scale mean teacher network is employed to further close the domain gap.}
    \label{fig:pipeline}
\end{figure}

% mage-level domain alignment is a simple but effective method to tackle UDA
% problem by reducing the distribution mismatch at the image-level, i.e., pseudo
% image synthesis.

% 1


\subsection{Segmentation-enhanced translation}

To close the domain gap across the modalities, we first conduct image-level domain adaptation to generate synthetic target samples. 
In this regard, the model trained on the synthetic target images will be used for VS and Cochlea segmentation on real hrT2 scans.
For unpaired image-to-image translation, we adopt the Contrastive Unpaired Translation (CUT)~\cite{park2020cut} as our backbone since it is faster and less memory-intensive than CycleGAN~\cite{zhu2017unpaired}.
Moreover, we enhance the 2D CUT with an additional segmentation decoder for maintaining the structural information of VS and Cochlea (see Fig.~\ref{fig:pipeline}).
Specifically, a ResNet-based generator is used to translate images from the source domain to the target domain, while a PatchGAN discriminator is employed to distinguish between the real and generated images~\cite{park2020cut}. We follow the SIFA architecture~\cite{chen2020unsupervised} and connect two layers of the encoder, specifically at the last layer and the layer before the last downsampling, with the segmenter decoder to generate multi-level segmentation predictions. The segmentation loss can help the encoder focus more on areas related to the segmentation task and preserve the structure of VS and Cochlea in the translated images. In Fig.~\ref{fig:generation_comparsion}, we can observe that the modified CUT can better preserve the shape of the VS and Cochlea in comparison with the original CUT framework.




% Considering the task of performing segmentation on the target hrT2 domain, we first propose an image translation module using a modified CUT framework~\cite{park2020cut} to convert the annotated ceT1 scans into hrT2 to train the segmentation model. The proposed image translation frame follows the 2D CUT structure, using a Resnet-based generator to convert images from the source to target and a PatchGAN discriminator~\cite{CycleGAN2017,isola2017image} to distinguish between the real and generated scan. Using this structure as a backbone, we attached a segmentation decoder to produce an additional segmentation mask to predict the VS and Cochlea from the input image, as seen on the left side of Fig. 1. Following the SIFA framework~\cite{chen2019synergistic}, the segmentation decoder takes the features from the last 2 layers of the encoder and produces the respective segmentation mask which are then compared with the original ground truth annotations. It can be observed that the additional segmentors help preserve the shape of the objects of interest during image translation as seen on Fig. 2.





%In conducting image translation, the proposed image translation framework closely follows the 2D CUT~\cite{park2020cut} structure with the addition of a decoder and a segmentation loss. The additional segmentation decoder shares the encoder with the image generator to predict the VS and Cochlea mask given an input image. CUT was chosen over CycleGAN due to the baseline experimentation results, in which CUT was able to perform significantly better than CycleGAN using the default settings. However, the original CUT network was unable to perfectly preserve the shape of the VS and Cochlea in some cases, leading to a deterioration in the performance of the model. As such, the proposed segmentation decoder plays an important role in preserving the semantics of the source images during image translation. The segmentation decoder is trained during the translation from real CET1 to pseudo HRT2 images and pseudo HRT2 images to identity HRT2 images, comparing the dice and cross-entropy loss between the ground truth annotation and the predicted segmentation masks from the pseudo HRT2 and identity HRT2 images. From Fig. (INSERT FIGURE to compare between CUT generated, Cyclegan generated, original CET1, and modified CUT generated), it can be observed that the modified CUT framework is beneficial not only in preserving the shapes of the objects of interest (VS and Cochlea) but also in bridging the gaps between the two domains.


\begin{figure}[t]
    \centering
    \includegraphics[width =0.9\columnwidth]{image/cut.png}
    \caption{Visual comparison of the synthetic hrT2 images by different methods.}
    \label{fig:generation_comparsion}
\end{figure}



%In conducting image translation, the proposed image translation framework closely follows the 2D CUT~\cite{park2020cut} structure with the addition of a decoder and a segmentation loss. The additional segmentation decoder shares the encoder with the image generator to predict the VS and Cochlea mask given an input image. CUT was chosen over CycleGAN due to the baseline experimentation results, in which CUT was able to perform significantly better than CycleGAN using the default settings. However, the original CUT network was unable to perfectly preserve the shape of the VS and Cochlea in some cases, leading to a deterioration in the performance of the model. As such, the proposed segmentation decoder plays an important role in preserving the semantics of the source images during image translation. The segmentation decoder is trained during the translation from real CET1 to pseudo HRT2 images and pseudo HRT2 images to identity HRT2 images, comparing the dice and cross-entropy loss between the ground truth annotation and the predicted segmentation masks from the pseudo HRT2 and identity HRT2 images. From Fig. (INSERT FIGURE to compare between CUT generated, Cyclegan generated, original CET1, and modified CUT generated), it can be observed that the modified CUT framework is beneficial not only in preserving the shapes of the objects of interest (VS and Cochlea) but also in bridging the gaps between the two domains.
% 1. Image Generation
% - We use CUT, but we also insert the segmenter head for CUT to maintain the structural information, similar to top winners like last year.
% - We should mention how CUT do, why is better than CycleGAN, and then why we add segmentation head here.

\subsection{Intensity augmentation and pseudo-labeling}

% choi2021using

Considering that tumors exhibit T2 heterogeneous signal intensity~\cite{VS_hypointense} and Cochleas show T2 hyperintense signal intensity~\cite{Cochlea_hyperintense}, we perform intensity augmentation~(IA) and generate augmented data for diversifying the training distributions, thereby improving the model generalizability. 
Using the generated hrT2 images and the ground truth annotations, the signal intensities of both the VS and Cochlea were muted and intensified by $50\%$ respectively, doubling the training data.
On the other hand, to boost the segmentation performance on real hrT2 images, we adopt a Pseudo-Labeling (PL) strategy to leverage unlabeled hrT2 images by generating pseudo hrT2 annotations. 
We employ a 3D full resolution nnU-Net~\cite{nnunet} for training on synthetic hrT2 images and augmented images, which is then used on unlabeled hrT2 images to generate pseudo-labels.

% On MR T2 imaging, vestibular schwannomas are generally hyperintense but
% some tumors can show heterogeneous signal intensity [4]. To introduce heterogeneity of tumor signals to mimic such clinical characteristics, we generate additional training data by reducing the signal intensity of the labeled vestibular
% schwannomas by 50% (hereafter referred to as DA).

% To improve the generalizability of the network, we introduced two types of data augmentation into the segmentation network training process after image translation. The two types of data augmentations were muting the intensity of the VS and intensifying the intensity of the Cochlea. Based on medical literature, while the VS normally appears to be hyper-intense in T2 MRI, some may appear to be rather hypo-intense~\cite{VS_hypointense}. On the other hand, Cochleas tend to appear more hyper-intense in T2 MRI scans~\cite{Cochlea_hyperintense}. As such, using the generated hrT2 images and the ground truth, the intensities of the VS and Cochlea were muted and intensified by 50 \% respectively. Based on the validation data, the inclusion of data augmentation improved the mean Dice by approximately 2 \%.


% On MR T2 imaging, vestibular schwannomas are generally hyperintense but
% some tumors can show heterogeneous signal intensity [4]. To introduce heterogeneity of tumor signals to mimic such clinical characteristics, we generate additional training data by reducing the signal intensity of the labeled vestibular
% schwannomas by 50\% (hereafter referred to as DA). Thus, with DA, 210 cases
% were used as training data instead of 105 cases. We evaluate segmentation results
% of models trained on the original training data and the data with DA.



\subsection{Multi-scale self-ensembling learning}
To further utilize all available data, we propose to take advantage of the self-ensembling network,  mean teacher (MT)~\cite{meanteacher}, in which, a teacher model is constructed with the same architecture as the student model, and updated with an exponential moving average (EMA) of the student parameters during training. 
In our training process, the outputs of the student and teacher models with different perturbations are optimized to be consistent by minimizing the difference using the mean square error (MSE) loss.
Inspired by the great success of multi-scale learning in medical image analysis~\cite{dou20173d,li2021hierarchical,zhao2022mmgl}, we follow~\cite{li2021hierarchical} to construct a multi-scale mean teacher (MS-MT) network to leverage multi-scale predictions for deep supervision and consistency regularization. 
We implement 3D full resolution nnU-Net~\cite{nnunet} as the backbone for both teacher and student networks, in which, auxiliary layers are connected to each block of the last five blocks to obtain multi-scale predictions (see Fig.~\ref{fig:pipeline}).


% With the generated hrT2 images and the corresponding ground truth annotations, the default 3D Full Resolution nnU-Net with the multi-scale mean teacher (MS-MT) training scheme~\cite{meanteacher,li2021hierarchical} was trained to generate pseudo-labels on the unannotated, original hrT2 images. 
% Using a combination of both the generated hrT2 images paired with the ground truth and the original hrT2 images paired with the predicted pseudo-labels, the proposed segmentation model was retrained to produce the final predictions. 
% The MS-MT network was chosen to tackle the potential problem of annotation scarcity in clinical settings and has been found to be able to achieve state-of-the-art performance even when subjected to scarce annotations. 
% The proposed MS-MT consists of a student model which is trained using a deep supervision scheme and a teacher model as seen on the right side of Fig. 2. The teacher model transfers information to the student model in each upsampling layer using the multi-scale consistency loss. 
% Using the validation data, the MS-MT model outperformed the nnU-Net on the validation set in terms of both the dice score and ASSD.

%With the generated HRT2 images and the corresponding ground truth annotations, a segmentation network can be trained in a supervised manner on the target HRT2 domain. As a baseline, the nnU-Net was the segmentation network of choice due to its success across several medical segmentation tasks and the ability to automate the hyper-parameter selection process. While the nnU-Net has been found to perform well in a supervised setting, it is important to also consider that there could be a lack of HRT2 annotation within a clinical setting which could, in turn, be detrimental to supervised learning. As such, using the nnU-Net as a backbone, a multi-scale mean teacher (MS-MT) network was proposed as a modification in an attempt to tackle the limitation of annotation scarcity. The MS-MT network has been found to be able to achieve state-of-the-art performance comparable to that of a network trained on a supervised setting~\cite{li2021hierarchical}. Using the 3D nnU-Net as the backbone model, the model was trained on 5 validation folds and the resulting predictions were ensembled to generate a final prediction.
% 2. Our multi-scale mean teacher network, ref to https://arxiv.org/pdf/2105.10369.pdf
% - we select nnUNet as our segmentation backbone
% - leverage self-ensembling model (mean teacher) for consistency
% - deep supervision for better results,
% - multi-level multi-scale consistency
% \subsection{Data Augmentation}

% 3. Our training strategy,
% - We first train a normal nnUNet to generate p labels.
% - We leverage mean teahcer training here.

% 4. implementation details 






\section{Experiments and Results}

\subsection{Dataset and pre-processing}
The training dataset released by the MICCAI challenge crossMoDA 2022~\cite{CrossModaChallenge,CrossModaData1,CrossModaData2,CrossModaData3}, includes $210$ labeled ceT1 images and $210$ unlabeled hrT2 images. 
Due to the varying voxel spacing in the training data, we resampled the images into a common spacing of $0.6 \times 0.6 \times 1.0$ mm and normalized the intensity to [0, 1] using the Min-Max scaling. 
To remove the noise, the images were cropped into $256\times256$ pixels in the xy-plane using the $75$ percentile binary threshold~\cite{choi2022using}, resulting in $256 \times 256 
\times$ N image volumes for 3D nnU-Net training. The processed 3D volumes were sliced along the z-axis to produce N number of 2D image training samples for the 2D CUT. 
The Dice Score (DSC [\%])~\cite{sudre2017generalised} and the Average Symmetric Surface Distance(ASSD [voxel])~\cite{lu2017automatic} were used to assess the model performance on VS and Cochlea segmentation.




% The dataset was released by the MICCAI challenge CrossMoDA 2021.
% The official crossMoDA 2022 training set~\cite{CrossModaChallenge,CrossModaData1,CrossModaData2,CrossModaData3} includes ceT1 scans with segmentation labels and hrT2 scans without segmentation labels from a total of 210 patients each.
% The validation set includes hrT2 scans obtained from 32 patients and no additional data were provided or used as described in the challenge rules.

% As the training data includes scans with varying voxel spacing, we resampled all cases and labels to a common voxel shaping of 0.6 x 0.6 x 1.0 mm using the linear and nearest neighbor interpolation. The images were then cropped to 256 x 256 pixels in the xy-plane using the 75 percentile binary threshold, resulting in 256 x 256 x N image volumes. Finally, the intensities were rescaled to uint8 integers from 0-255 using a min-max scaling and the volume data were sliced along the z-axis to produce N jpeg images of size 256 x 256 for 2D image translation. 

% ref https://crossmoda-challenge.ml/media/papers/jwc-rad.pdf

\subsection{Implementation details}

We used single NVIDIA A40 GPU with 48GB of memory for model training.
We followed~\cite{park2020cut} to optimize the proposed segmentation-enhanced CUT network, in which the weights for adversarial loss, contrastive loss, and segmentation loss were set the same. 
Following~\cite{chen2020unsupervised}, the loss weights for additional segmentation in the modified CUT were set to $1$ and $0.1$ for the last layer and the second last downsampling layers respectively.
For pseudo-labeling, we closely followed the nnU-Net optimized settings~\cite{nnunet} and trained for $200$ epochs with the generated hrT2 images and augmented hrT2 images using cross-validation.
The segmentation results from an ensemble of five-fold cross-validations on unlabeled hrT2 images were then used as pseudo labels for the following processes.
For the multi-scale mean teacher, the EMA update $\alpha$ was set to $0.9$, and the loss weights for consistency regularization were set to  $\{0.05, 0.05, 0.05, 0.4, 0.5\}$, assigned to each feature map according to its size in ascending order, and we followed the deep supervision scheme in nnU-Net~\cite{nnunet}.
The loss weights were ramped up across $160$ epochs using the sigmoid function.
We used a combination of Dice and cross-entropy losses as our objective function and trained the MS-MT model with an initial learning rate of $0.01$ using stochastic gradient descent for $300$ epochs. 
Following~\cite{meanteacher}, the results from the five-fold teacher models were ensembled, and then post-processed by computing the largest connected component (LCC) to remove unconnected labels (The first LCC was preserved for the VS while the first and second LCCs were preserved for the Cochlea) for final submission.


\begin{table}[t]
\centering
\setlength\tabcolsep{7pt}
\caption{Quantitative results on the validation dataset. The metrics are presented in the format of mean±std.}
\label{tab:results}
\scalebox{0.9}{
\begin{tabular}{|c|c|c|c|c|}
\hline
VS Dice         & VS ASSD         & Cochlea Dice    & Cochlea ASSD    & Mean Dice      \\ \hline
0.8380±0.097 & 0.5555±0.3647 & 0.8139±0.0312 & 0.2652±0.1553 & 0.8260±0.0508 \\ \hline
\end{tabular}
}
\end{table}

\subsection{Experimental results}

The results of our method on the validation dataset are presented in Table.~\ref{tab:results}. 
With a mean Dice score of 0.826, our method secures a top-10 finish in the CrossMoDA 2022 competition.
Fig.~\ref{fig:sample} highlights some qualitative results produced by our method on the validation set. Finally, our method ranked 5th in the test phase of the crossMoDA 2022 challenge with a mean Dice score of 85.65\%, and ranked in 3rd place for the Vestibular Schwannoma segmentation with a mean
Dice score of 86.7\%.



\begin{figure}[h]
    \centering
    \includegraphics[width = 0.9\columnwidth]{image/vismask.png}
    \caption{Qualitative results produced by our method. The VS and Cochlea are indicated in \textcolor{green}{green} and \textcolor{yellow}{yellow} color, respectively.}
    \label{fig:sample}
\end{figure}

\subsection{Ablation study}
Table~\ref{tab:ablation} shows the results of methods of different components,~\emph{e.g.,} intensity augmentation(IA), pseudo-labeling (PL), and mean teacher(MT) on the validation dataset. We start by training a 3D nnU-Net on generated images by CUT,~\emph{i.e.,} nnU-Net+CUT, which achieves a mean dice of 78.95\%. By adding augmented images into the training process, we can observe an improvement in the segmentation performance of both Cochlea and VS. Then, we introduce pseudo-labeling into the training process, which leads to 1.06\% increments in mean Dice, and a continued increase was observed with the inclusion of self-ensembling learning. We further build a multi-scale mean teacher, which achieved better performance. Finally, compared to CUT, the model trained on generated images with the proposed segmentation-enhanced CUT (SE-CUT) can achieve better performance. 



\begin{table}[ht]
\centering
\setlength\tabcolsep{7pt}
\caption{Quantitative results (Dice Score [\%]) by different methods on the validation dataset.}
\label{tab:ablation}
\scalebox{0.9}{
\begin{tabular}{lccc} 
\toprule
Method                                           & Cochlea         & VS              & Mean             \\ 
\hline
nnU-Net+CUT                                 & 79.67±3.72 & 78.22±21.04 & 78.95±11.13  \\
nnU-Net+CUT+IA                            & 80.48±4.14 & 78.31±21.00 & 79.39±11.28  \\
nnU-Net+CUT+IA+PL                       & 81.06±3.42 & 79.85±12.88 & 80.45±6.60  \\
nnU-Net+CUT+IA+PL+MT                  & 81.06±3.28 & 82.67±11.89 & 81.86±6.18  \\
nnU-Net+CUT+IA+PL+MS-MT               & 81.24±3.31 & 82.98±10.25 & 82.11±5.39  \\
nnU-Net+SE-CUT+IA+PL+MS-MT & 81.39±3.12 & 83.80±9.72 & 82.60±5.08  \\
\bottomrule
\end{tabular}
}
\end{table}





\section{Discussion}
In this work, we propose to explore image-level domain adaptation and semi-supervised learning to address the unsupervised domain adaptation problem in cross-modality vestibular schwannoma (VS) and cochlea segmentation. Additionally, intensity augmentation is performed to generate additional training data with different VS and cochlea intensities for diversifying the data distributions. Our final submission achieved a mean Dice score of 82.6\% and 85.65\% in the validation phase and the testing phase, respectively. In particular, our method achieved 5th place for overall segmentation performance and ranked 3rd for Vestibular Schwannoma segmentation. From our experiments, we observe that image adaptation contributes the most to the adaptation performance. We can regard the 3D nnU-Net trained on generated target images with CUT (\emph{i.e.,} nnU-Net+CUT) as a strong baseline, achieving dice scores of approximately 80\%, which signifies a competitive result within the leaderboards. However, synthetic images may include noises and artifacts, which would influence the follow-up segmentation performance. Many works~\cite{shin2022cosmos,park2020cut} have been proposed to address the limitations of GANs. In our work, we design a segmentation-enhanced translation network for improved adaptation performance. For data augmentation, we adjust the intensities of VS and cochlea based on clinical characteristics and medical knowledge and achieved better performance. Besides, spatial augmentation methods~\cite{zhao2022meta} can be further explored in this UDA task. For SSL, we adopted pseudo-labeling and self-ensembling learning methodologies to improve the adaptation performance. According to our experimental results, both SSL methods are beneficial to improved segmentation performance. However, one limitation of pseudo-labeling is that low-quality pseudo-labels would adversely influence the training process. Since different perturbations are introduced to the self-ensembling models, the influence of noise from synthetic images and pseudo-labels could be weakened to some extent. We further advance multi-level consistency and deep supervision into self-ensembling learning, achieving better UDA performance. 








\section{Conclusion}
In this work, we proposed a three-stage UDA method based on image translation, pseudo-labeling and multi-scale self-ensembling learning to close the gap between two domains and conduct effective image segmentation on the target domain. Specifically, we worked with annotated ceT1 and unannotated hrT2 data to perform segmentation on the target hrT2 domain. In addition, intensity augmentation was implemented to improve the generalizability of the model. With a mean Dice of 0.8216, we achieved a top 10 finish in the CrossMoDA 2022 validation phase and eventually, placed 5th overall in the final competition ranking, demonstrating its effectiveness in bridging the gap between the ceT1 and hrT2 MRI modalities. 




%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{refs}

\end{document}
