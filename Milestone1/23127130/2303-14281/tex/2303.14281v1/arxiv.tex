\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{1}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

% Extra by us

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{comment}
\usepackage{amsthm}

\usepackage{graphicx}
\usepackage{bm}
\usepackage{enumerate,enumitem}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

%\usepackage{setspace}
\usepackage{amsmath}
\usepackage[ruled]{algorithm}
%\usepackage{etoolbox}
%\AtBeginEnvironment{algorithm}{\setstretch{1.25}}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{rotating}

\usepackage{authblk}
\usepackage{appendix}
\setlength{\bibsep}{3pt}

\def\argmax{\text{argmax}}
\def\argmin{\text{argmin}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newtheorem{assumption}{Assumption}
\newcommand{\QZL}[1]{{\color{blue}{(QZL: #1})}}


\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
%\theoremstyle{remark}{}
%\newtheorem*{example}{Example}
%\newtheorem*{fact}{Fact}
\newtheorem{remark}{Remark}
\newtheorem{cond}{Condition}



\newcommand{\tp}{\intercal}
\newcommand{\brm}[1]{\bm{\mathrm{#1}}}
\newcommand{\bigO}{\ensuremath{\mathop{}\mathopen{}\mathcal{O}\mathopen{}}}
\newcommand{\smallO}{ \scalebox{0.7}{$\mathcal{O}$}}
\newcommand{\bigOp}{\bigO_\mathrm{p}}
\newcommand{\smallOp}{\smallO_\mathrm{p}}
\newcommand{\ind}{\mbox{$\perp\!\!\!\perp$}}
\newcommand{\Ascr}{{\mathcal{A}}}
\newcommand{\Hscr}{{\mathcal{H}}}
\newcommand{\Gscr}{{\mathcal{G}}}
\newcommand{\Kscr}{{\mathcal{K}}}
\newcommand{\Vscr}{{\mathcal{V}}}
\newcommand{\Fscr}{{\mathcal{F}}}
\newcommand{\Wscr}{{\mathcal{W}}}
\newcommand{\kernel}{{\kappa}}
\newcommand{\tard}{d_{}^\pi}
\newcommand{\blam}{\bm \lambda}
\def\cvgn{\mathop{\longrightarrow}_{n\to+\infty}}
\def\ds1{{\mathrm{1 \hspace{-2.6pt} I}}}
\def\dsB{\mathbb {B}}
\def\dsC{\mathbb {C}}
\def\dsE{\mathbb {E}}
\def\dsN{\mathbb {N}}
\def\dsP{\mathbb {P}}
\def\dsQ{\mathbb {Q}}
\def\dsR{\mathbb {R}}
\def\dsS{\mathbb {S}}
\def\dsV{\mathbb {V}}
\def\dsZ{\mathbb {Z}}
\def\calA{{\cal A}}
\def\calB{{\cal B}}
\def\calBX{{{\cal B}_\calX}}
\def\calBY{{{\cal B}_\calY}}
\def\calBR{{\cal B}}
\def\calBZ{{{\cal B}_\calZ}}
\def\calC{{\cal C}}
\def\tcalC{{\tilde{\cal C}}}
\def\calCm{{{\cal C}_{max}}}
\def\calD{{\cal D}}
\def\calE{{\cal E}}
\def\calF{{\cal F}}
\def\calFXY{{\cal F}}%(\dsR^d,\calY)}
\def\calG{{\cal G}}
\def\calH{{\cal H}}
\def\calI{{\cal I}}
\def\calK{{\cal K}}
\def\calL{{\cal L}}
\def\calM{{\cal M}}
\def\calN{{\cal N}}
\def\calP{{\cal P}}
\def\calQ{{\cal Q}}
\def\calR{{\cal R}}
\def\tcalR{{\tilde{\cal R}}}
\def\calS{{\cal S}}
\def\calT{{\cal T}}
\def\calU{{\cal U}}
\def\calV{{\cal V}}
\def\calW{{\cal W}}
\def\calX{{\cal X}}
\def\calY{{\cal Y}}
\def\calZ{{\cal Z}}

\newcommand{\zp}[1]{z_p(#1)}
\newcommand{\psip}[1]{\psi_p(#1)}
\def\Dpi{\mathcal{D}^\pi}
\def\Dpin{ \hat{\mathcal{D}}^\pi_n}
\def\barotimes{\,\bar{\otimes}\,}
\def\Var{\text{Var}}
\def\EE{\mathbb{E}}
\def\w{\boldsymbol{w}}
\def\e{\boldsymbol{e}}
\def\f{\boldsymbol{f}}
\def\vpsi{\boldsymbol{\psi}}
\def\XX{\textbf{X}}
\def\ZZ{\textbf{Z}}
\def\rwrd{\textbf{R}}
\def\vphi{\boldsymbol{\phi}}
\def\R{\mathbb{R}}
\def\Regret{\operatorname{Regret}}
\def\Rn{\mathbf{R}_n}
\def\Rem{\operatorname{Rem}}
\def\Yn{\mathbf{Y}_n}
\def\I{\,\mathcal{I}}
\def\ones{\mathbf{1}}
\def\zeros{\mathbf{0}}
\def\B{\mathcal{B}}
\def\argmin{\operatorname{argmin}}
\def\argminb{\operatorname*{argmin}}
\def\argmax{\operatorname{argmax}}
\def\N{\mathcal{N}}
\def\argmaxb{\operatorname*{argmax}}
\def\wcvg{\Rightarrow}
\renewcommand{\liminf}{\varliminf}
%\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\limsup}{\varlimsup}
\def\op{\operatorname{op}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\jiao{\cap}
\def\bing{\cup}
\def\C{\mathcal{C}}
\newcommand{\sam}[1]{\textcolor{orange}{[SAM:\ #1]}}
%\newcommand{\samfixed}[1]{\textcolor{blue}{[SAM:\ #1]}}
\newcommand{\samfixed}[1]{}
\newcommand{\liminfn}{\liminf_{n \goes \infty}}
\newcommand{\limsupn}{\limsup_{n \goes \infty}}
\newcommand{\limn}{\lim_{n \goes \infty}}
\def\linfty{l^{\infty}}
\def\Q{\mathcal{Q}}
\def\given{\, | \,}
\def\Given{\, \Big| \,}
%\let\oldtau\tau
%\renewcommand{\tau}\uptau
\def\GG{\mathbb{G}}
\def\Gn{\mathbb{G}_n}
\def\V{\mathcal{V}}
\def\transpose{\top}
% \def\M{\mathcal{M}}
\def\H{\mathcal{H}}
\def\Morth{\mathcal{M}^{\perp}}
\def\opt{\text{opt}}
\def\Span{\text{Span}}
\def\Vn{\hat{\mathcal{V}}_n}
\def\Mn{\hat{M}_N}
\newcommand{\inblue}[1]{{\color{blue}{#1}}}

%\usepackage{xr}
%\externaldocument{seek_supp}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Sequential Knockoffs for Variable Selection in Reinforcement Learning}
  % \author{Tao Ma$^1$
  % %\thanks{
  %   %The authors gratefully acknowledge 
  %   %\textit{please remember to list all relevant funding sources in the unblinded version}
  %   %}
  %   \hspace{.2cm}\\
  %   London School of Economics and Political Science\\
  %   and \\
  %   Author 2 \\
  %   Department of ZZZ, University of WWW}
\author[1]{Tao Ma}
\author[2]{Hengrui Cai}
\author[3]{Zhengling Qi}
\author[1]{Chengchun Shi}
\author[4]{Eric B. Laber}
\affil[1]{London School of Economics and Political Science}
\affil[2]{University of California, Irvine}
\affil[3]{George Washington University}
\affil[4]{Duke University}
\date{}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Sequential Knockoffs for Variable Selection in Reinforcement Learning}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}

In real-world applications of reinforcement learning,  it is often challenging to obtain a state representation that is parsimonious and satisfies the Markov property without prior knowledge. 
Consequently, it is common practice to construct a state which is larger than necessary, e.g., by concatenating measurements over contiguous time points.  However, needlessly increasing the dimension of the state can slow learning and obfuscate the learned policy. We introduce the notion of a minimal sufficient state in a Markov decision process (MDP) as the smallest subvector of the original state under which the process remains an MDP and shares the same optimal policy as the original process. We propose a novel \underline{se}qu\underline{e}ntial \underline{k}nockoffs (SEEK) algorithm that estimates the minimal sufficient state in a system with high-dimensional complex nonlinear dynamics. In large samples, the proposed method controls the false discovery rate, and selects all sufficient variables with probability approaching one.  As the method is agnostic to the reinforcement learning algorithm being applied, it benefits downstream tasks such as policy optimization.  Empirical experiments verify theoretical results and show the proposed approach outperforms several competing methods in terms of variable selection accuracy and regret. 
\end{abstract}

\noindent%
{\it Keywords:}  Reinforcement learning, Variable selection, Sequential knockoffs, False discovery rate control, %Selection consistency, 
Power analysis
\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!
\section{Introduction}
\label{sec: 1}

Interest in reinforcement learning \citep[RL,][]{sutton2018reinforcement} has
increased dramatically in recent years due in part to a number of 
high-profile successes in games \citep[][]{mnih2013playing,mnih2015human}, 
autonomous driving \citep[][]{sallab2017deep}, and precision medicine 
\citep[][]{tsiatis2019dynamic}.  However, despite theoretical and computational
advances, real-world application of RL remains difficult.  A primary 
challenge is dealing with high-dimensional state representations.  
Such representations occur naturally in systems
with high-dimensional measurements, like images or 
audio, but can also occur when the system state is constructed
by concatenating a series of measurements over a contiguous
block of time.  A high-dimensional state---when a more
parsimonious one would suffice---dilutes the efficiency of learning
algorithms and makes the estimated optimal policy harder to interpret.  Thus, methods for removing uninformative
or redundant variables from the state are of tremendous practical 
value.  

%\vspace{-1cm}

\begin{comment}
	
	Reinforcement learning  \citep[RL,][]{sutton2018reinforcement} %models and solves 
	studies the sequential decision problem in which an agent is trained to learn an optimal policy %for maximizing some utility function
	to maximize the return. %Recently we have seen tremendous successes of implementing RL algorithms in many 
	Its success has been demonstrated in a number of applications including games \citep[][]{mnih2013playing}, autonomous driving \citep[][]{sallab2017deep}, precision medicine \citep[][]{murphy2003optimal,gottesman2019guidelines}, among many others. This paper considers policy learning with a pre-collected historical dataset \citep[see][for an overview of offline RL]{levine2020offline}. With the fast development of new technology and big data explosion, it becomes easy to collect a large number of state variables at each time point. 
	More importantly, to guarantee that the observed data satisfy the Markov property, a common practice is to construct the state by concatenating the data over multiple decision points. This inevitably leads to a high-dimensional state space that is difficult to model effectively for policy optimization. As such, it is urgent to perform variable selection for optimal decision making in the offline RL.
	
\end{comment}

%make an effective use of information that is relevant to optimal decision making. This makes feature selection as an emerging need for implementing RL.
%Unlike online applications (e.g., video games) where it is cheap to simulate 


%An important issue even before adopting any RL algorithms for training agents is the high dimensional state space, which is commonly treated as fixed. Then there lie two potential issues. To begin with, the efficiency in algorithms is restricted. For example, when helping doctors learn the relations between treatments and potential reactions in patients, the larger number of characteristics to measure for a patient, the longer time needed for an algorithm to reach a conclusion. Thus, squeezing the state space can significantly reduce the computation time, even with the same training algorithm. On the other side, reducing the total number of states sheds light on interpretations of the resulting learned policy. With a lot of noise covariates in the state space, a well performing algorithm still fails to tell why it works. In another instance, a recommendation algorithm succeeds in providing interesting movies to customers depending on their former watching history respectively. Then an advertiser wonders which characteristic makes a movie attractive, but such problem can only be answered when the number of variables are not too large, otherwise it is hard to choose some informative characteristics among a lot of noise features.

%To address above issues, this paper proposes the variable selection methods for state covariates in sequential decision problems, called \underline{se}qu\underline{e}ntial \underline{k}nockoffs in sample-\underline{e}fficient \underline{R}L (SEEK), which builds a solid foundation for all potential evaluation algorithms.

%% I tried to take what's below and shorten it 
We develop a general variable selection algorithm for offline RL, which aims to learn an optimal policy using only logged data, i.e., without any additional online interaction.  Our contributions can be summarized as follows: (i)  we formally define a {\em minimal sufficient state} for an MDP and argue that it is an appropriate 
target by which to design and evaluate variable selection methods in RL; (ii) we show that na\"ive variable selection methods based on the state 
or reward alone need not recover the minimal sufficient state; (iii)  we propose a novel sequential knockoffs (SEEK) algorithm that applies with general black-box
learning methods, and, under a $\beta$-mixing condition, consistently recovers the minimal sufficient state, and controls the 
false discovery rate (FDR, the ratio of the number 
of selected irrelevant variables to the number of selected variables); and 
(iv) we develop a novel algorithm to estimate the $\beta$-mixing coefficients of an MDP. The algorithm in (iv) is important in its own right as it applies to a number of applications beyond RL \citep{mcdonald2015estimating}. %\textcolor{red}{theoretical contribution?}

\vspace{-0.5cm}

\begin{comment}
	We propose an original sequential knockoffs algorithm for variable selection in  offline RL.
	%This paper is concerned with feature selection in offline RL. 
	Our contributions are summarized as follows. Scientifically, variable selection plays a crucial role in large-scale RL problems with high-dimensional state information in enhancing the efficiency of policy optimization algorithms and reducing the corresponding computational costs. However, it has been less studied in the existing literature.  Our proposal effectively reduces the number of state variables from a large to a moderate scale, yielding a less noisy policy with a larger value. In addition, it offers a more interpretable policy that relies on a small set of significant variables only, compared with directly implementing state-of-the-art RL algorithms.
	
	Methodologically, we formally define a \textit{minimal sufficient state} for the optimal decision making and our novel sequential knockoffs algorithm for variable selection enjoys great flexibility in seeking it. Many existing variable selection methods are ineffective in complex nonlinear dynamic systems. Our proposal involves two key components: an iterative model-based procedure to select sufficient state variables in a sequential manner, and a novel knockoffs algorithm that can capture complex nonlinear associations among high-dimensional variables and has valid finite-sample guarantees with time-dependent observations. 
	
	Theoretically, we systematically analyze several variable selection procedures, including an iterative method, a one-step method and a reward-only method. In particular, we prove that only the iterative method is able to consistently identify the minimal sufficient state. %a subset of the entire state vector that is sufficient for learning the optimal policy (see Section \ref{sec: 2} for a detailed definition). 
	To the contrary, the one-step method will fail to eliminate time-dependent variables that are irrelevant for optimal decision making, leading to a high false positive rate whereas the reward-only method will miss some important variables in the sufficient state, resulting in a high false negative rate and not being able to fulfill the Markov assumption. In addition, we also establish the theoretical properties of the proposed knockoffs method. Our theory is able to accommodate any complex machine learning models, and allows data observations to be time-dependent, which is typically the case in RL.
\end{comment}



\subsection{Related Work}
\addtolength{\textheight}{.3in}
Our work uses the idea of knockoff variables, also known as pseudo-variables, for variable selection.  Intuitively, knockoff methods
augment the observed data with noise variables (knockoffs) and use the rate at which these noise variables are selected by a variable
selection algorithm to characterize the algorithmâ€™s operating characteristics \citep[][]{wu2007controlling, barber2015controlling}.   A key
strength of knockoff methods is their generality. Knockoffs can be used to estimate (and subsequently tune) the FDR of 
general black-box variable selection algorithms \citep[][]{candes2018panning,  lu2018deeppink, romano2020deep, zhu2021deeplink}. Being
algorithm agnostic is especially appealing in a vibrant field like RL, where algorithms are emerging and evolving rapidly. Unfortunately, it
is not possible to directly export existing knockoff methods to RL because  
knockoffs have been developed almost exclusively in the regression setting under the assumption of independent observations; an assumption which is 
clearly violated in RL settings.  One exception is a recent proposal for knockoffs in time series data \citep[][]{chi2021high}.  
However, as will show in Section \ref{sec: minimal}, %(see the one-step approach), 
applying this one-step approach to the states in an MDP need not recover the minimal sufficient state. 

%Our proposal is closely related to the 
%There is a huge literature on variable selection in the supervised learning. Most existing works developed penalized regression methods for high-dimensional parametric models \citep[see][for an overview]{fan2010selective}.
%Our proposal is closely related to an emerging line of research in developing the knockoff filter, a general framework for controlling the false discovery rate (FDR, the ratio of the number of selected irrelevant variables to the number of selected variables) while performing feature selection \citep[see e.g.,][]{barber2015controlling, candes2018panning, huang2020relaxing, romano2020deep, ren2021derandomizing, zhu2021deeplink, liu2022model}. FDR is a widely used measure for evaluating the performance of feature selection. However, all the aforementioned works require the data observations to be independent. Their methods are not directly applicable to the RL setting with time-dependent observations. Very recently, we notice a parallel work discussing knockoffs on time series data \citep[][]{chi2021high}. With some data dependent constant $h$, the final proposed algorithm focuses on $2/(h+1)$ of all the data, with the rest playing minor roles, which obviously reduces the efficiency in data usage and robustness of conclusions. Especially to theoretically achieve better control on FDR, $h$ should be large enough with respect to the sample size, which makes such inefficiency quite significant.

%We review the former literature in both variable selection and RL. To begin with, variable selection is crucial in analysing models, with advantages like reducing the cost of computation and better interpretation. Recently the method of knockoffs selection keeps getting noticed with theoretical guarantee and performance in real data \citep[][]{barber2015controlling, candes2018panning}. The main idea is to construct the so-called knockoff variables corresponding to primary covariates, with which feature statistics for each primary dimension could be gained. Such feature statistics serve to separate the useful variables with null ones and achieve good selection. The False Discovery Rate (FDR) is used to measure the proportion of nulls in the selection, which was shown to be controlled with the procedure of knockoffs. On top of that, deep neural network helps better sample the approximation to knockoff variables \citep[][]{romano2020deep}, provide more accurate assessment for importance of variables \citep[][]{lu2018deeppink}, or even incorporate the whole procedure \citep[][]{zhu2021deeplink}. To further derandomize the knockoffs for robustness, multiple procedures could be carried out and aggregated in terms of a selection probability \citep[][]{ren2021derandomizing}. In addition, concerning the mixed data types, the calculation of feature statistics is to be discussed according to the type of data \citep[][]{kormaksson2021sequential}. We emphasize that all current generally proposed knockoffs-based methods assume the data to be independently generated, which is essential in the proof of FDR control.

In the statistics literature, RL is closely related to a growing line of works on learning optimal 
dynamic treatment regimes in precision medicine \citep[see e.g.,][for an overview]{kosorok2019precision}. A number of variable selection approaches have been proposed to estimate a 
parsimonious treatment regime 
\citep{gunter2011variable,qian2011performance,song2015sparse,fan2016sequential,shi2018high,zhang2018variable,qi2020multi,bian2021variable}. However, these methods are designed for a short
time horizon (e.g., 2-5 times points), and are not applicable to long or infinite horizons which
are common in RL.  
 
Relative to its practical importance, variable selection for RL is under-explored. In the computer science literature,  
\citet{kroon2009automatic} and \citet{guo2017sample} studied variable selection in factored Markov decision processes (MDPs). 
\citet{nguyen2013online} applied penalized logistic regression to learn transition functions in structured MDPs. 
In MDPs without any specific structure, a common approach is penalized estimation.  
This includes temporal-difference (TD) learning with LASSO \citep{tibshirani1996regression} or the Dantzig selector \citep{candes2007dantzig}; see e.g., \citet{kolter2009regularization,ghavamzadeh2011finite,geist2012dantzig}. In a similar spirit, 
\citet{hao2021sparse} recently proposed combining LASSO with fitted Q-iteration 
\citep[FQI]{ernst2005tree}.
However, these methods are designed for linear function approximation whereas our goal is to develop variable selection methods 
that are not reliant on a specific algorithm or functional form.  
\vspace{-0.1cm}

Our proposal is also related to a line of research on abstract representation learning in the RL literature, where the goal is to %build some abstraction of the state space, which is just a function from the observed ground state to some corresponding abstract state, to reduce the size of the state space and group states according to their behaviours in reward, transition, value function, optimality and so on \citep[][]{chapman1991input, mccallum1996reinforcement,boutilier2000stochastic,jong2005state,li2006towards}, where different abstraction criteria lead to different topologies defined on the state space. 
construct a state abstraction, defined as a mapping from the original state space to a much smaller abstraction space that preserves many properties of the original MDP \citep[see e.g.][for an overview]{jiang2018notes}. 
In particular, %Among those available, %approaches, 
bisimulation %has the strongest assumption on such abstraction and the resulting 
corresponds to a class of abstraction methods where the system preserves the Markov property and the optimal policy based on the resulting abstract MDP can achieve the same value as that of the original MDP \citep{dean1997model, li2006towards}. However, identifying the ``minimal'' bisimulation is NP-hard \citep{givan2003equivalence}. Existing solutions either focus on building an approximate bisimulation \citep{ferns2011bisimulation, ferns2014bisimulation, zhang2020learning}, 
or learning a Markov state abstraction while ignoring the reward signal \citep{allen2021learning}. These methods share similar ideas with existing dimension reduction and representation learning methods developed in the RL literature \citep[see e.g.,][]{wang2017sufficient, uehara2021representation}. 
%\citep{dean1997model, li2006towards} 
%Due to high cost and strong assumptions in finding such abstraction, the bisimulation metric is introduced to approximately build a bisimulation \citep{ferns2011bisimulation, ferns2014bisimulation, zhang2020learning}. Relaxing the requirement from bisimulation, there are work to learn state abstraction only requiring that the resulting abstract MDP is Markov \citep{allen2021learning}, though a block structure is assumed \citep{du2019provably}. In addition, for some abstraction approaches with promising properties, factored MDP is focused on, while the general case is yet to be discussed \citep{diuk2009adaptive, van2014efficient}. Notice that instead of restricting the abstraction function on state space, finding simultaneous representation of the state-action pair would lead to the representation learning on low rank MDP, though such assumptions are still strong\citep{agarwal2020flambe, uehara2021representation}.


%Finally, we note that dimension reduction and representation learning are related in that they share our goal of identifying a simplified representation of the process \citep[e.g., see][]{tangkaratt2016model,wang2017sufficient, modi2021model,uehara2021representation}.
%However, 
In contrast with the aforementioned state abstraction and dimension reduction methods, we focus on selecting a subvector of the original state rather than engineering new features.  This approach is advantageous in settings such as mobile-health
where an interpretable and parsimonious learned policy is important for generating new clinical insights.  However,
our methods can also be used with basis expansions constructed from the original state such as splines or tile
codings. Another advantage of our approach is its computational efficiency, we show that SEEK is guaranteed
to terminate after no more than $p$ iterations where $p$ is the dimension of the state.  



%As such, it has better interpretations, yielding interpretable policies that rely on a small set of significant variables %only. In addition, it is much more computationally efficient than identifying the minimal bisimulation. Indeed, our %proposal is guaranteed to be terminated in $p$ iterations with $p$ being the state dimension. 
%while most above abstract representation learning methods treat all states equivalently, only building partitions in the state space and do not decrease the number of variables. Compared to abstract representation learning, variable selection has better interpretation of the results, which is necessary in applications. In addition, the method introduced in this paper only has mild assumptions on the unknown MDP, which guarantees applicability.
\vspace{-0.5cm}
\subsection{Organization of the paper}
	
In Section \ref{sec: 2}, we review the Model-X knockoffs method for variable selection in regression, and describe the challenges brought by sequential decision-making. %dependence settings. 
In Section \ref{sec: minimal}, we formulate the variable selection problem in RL and introduce the proposed minimal sufficient state. %, where the minimal sufficient state is defined as the selection target, with properties in optimality, Markov property and uniqueness. A toy example is also provided, with comparison among different high level approaches, to show the necessity of sequential selection approach. Following that, 
In Section \ref{sec: seek}, we present the sequential knockoffs (SEEK)  procedure for identifying the minimal sufficient state. %, and also design a way to tune the number of splits before implementing SEEK in practice.  
%and design a practical algorithm to adaptively tune the number of splits. 
In Section \ref{sec: theory}, %theoretical properties of the methods are given, which lead to theorems on controlling 
we show our method controls both modified FDR and FDR, %as well as theorems on the lower bound for selection power. 
with power approaching one asymptotically. 
%We finally perform simulation and real data analysis 
Finally, we investigate the empirical performance of our method via extensive simulations and a real data analysis in Sections \ref{sec: exp} and \ref{sec: real}. Some implementation details, technical proofs, and additional theoretical and numerical results are available in the Supplementary Material.
%Further details in numerical results, implementation, theories and proof are in the supplementary material.

\vspace{-0.5cm}

%Finally,  dimension reduction in RL \citep[see e.g.,][]{tangkaratt2016model,wang2017sufficient} or perform representation learning in low-rank MDPs %\citep[see e.g.,][]{modi2021model,uehara2021representation}. We remark that dimension reduction (or representation learning) differs from variable selection. These two problems are closely related, and both can reduce the dimensionality of state variables. Nonetheless, dimension reduction transforms the high-dimensional features into a lower dimension (e.g., using a linear combination of features) whereas variable selection aims to identify a subset of significant features in the original feature space. 
%is closely related to feature selection. However, it differs from feature selection in that the former transforms the features into a lower dimension (e.g., using a linear combination of features) whereas the latter selects a subset of the whole features without changing them. 

%At the same time, there are different methods in selecting significant state variables in the setting of RL. Assuming in addition the process to be a factored MDP, feature selection in terms of the dependence on parent sets is performed \citep[][]{kroon2009automatic, guo2017sample}. On the other hand, efficient construction of policies are explored for blocked MDP \citep[][]{du2019provably}, linear MDP \citep[][]{yang2020reinforcement} and low-rank MDP \citep[][]{uehara2021representation}. For the general setting of MDP, it is still of high interest for the selection problem. There is one work that combines fitted Q-evaluation with LASSO, either to incorporate the selection simultaneously into the evaluation step, or gain an estimate on significant features and proceed to perform evaluation \citep[][]{hao2021sparse}.
\begin{comment}
	To the best of our knowledge, this paper is the first to apply knockoffs based ideas to variable selection problem in sequential decision setting. Very recently, we notice a parallel work discussing knockoffs on time series data \citep[][]{chi2021high}. Through their procedure, to reduce the effects of dependence among data, they pick one datum out of a consecutive sequence of $h+1$ data points with some data dependent constant $h$, then the final proposed algorithm focuses on $2/(h+1)$ of all the data, with the rest playing minor roles, which obviously reduces the efficiency in data usage and robustness of conclusions. Especially to theoretically achieve better control on FDR, $h$ should be large enough with respect to the sample size, which makes such inefficiency quite significant. On the other hand, our approach takes all data points equally into account, which is one of the several advantages.
\end{comment}

\begin{comment}
	\subsection{Contributions}
	
	\begin{itemize}
		\item This work extends the variables selection theories, especially the knockoffs, by dropping the assumption of independence among data. As required in knockoffs, data points are assumed to be independently and identically distributed (i.i.d.). Here we break such requirement and allow data to be correlated. This relaxation can add flexibility for implementation in more scenarios.In addition, to deal with dependent data, it is common in former work to drop a significant portion of a dataset to reduce correlations between data points. While in our method all data are fully and equally made use of, which guarantees the accuracy and robustness of the conclusion.
		
		\item Instead of being restricted by linear assumptions, this method consider both linear and nonlinear models, especially with implementation focusing on nonlinear cases.
		
		\item We perform variable selection in an iterative manner. Different from former work, the selections consists of several iterations, first with regressions towards rewards, then towards formerly selected dimensions. In such way, we not only select covariates directly associative with rewards, but also dig out all variables indirectly contributing to rewards.
		
		%    \item The selection is applied conditional on actions. Instead of treating all states and actions as variables as a whole, we sort data points into subsets according to the values of actions, and perform selection in each subset. Later a statistics can be computed combining selections from each subset to reach a robust conclusion.
		
		\item We provide and prove the control of FDR and mFDR for the proposed methods. From the view of RL, this work gives a novel way to reduce the dimension of state space. From the view of variable selection, we largely relax the limitation on assumptions. The proof consists of general ideas applicable to other selections procedures besides knockoffs, making above relaxation possible for further research in larger ranges of problem settings.
	\end{itemize}
	\subsection{Organization of the paper}
	
	The structure of this paper is as follows. In Section \ref{sec: 2} we formulate the variable selection problem in Reinforcement learning, and describe the new obstacles brought by sequential settings. Following that, we provide the methods in splitting data, as well as the procedure for sequential knockoffs in Section \ref{sec: 3}. A toy example is also provided to show the idea. Then in Section \ref{sec: 4}, theoretical properties of the methods are given, which lead to two main theorems on controlling both modified FDR and FDR. We later perform simulation as well as real data analysis in Section \ref{sec: 5} and conclude our discussion by Section \ref{sec: 6}, followed by potential work. Further details in theories and implementation are attached in Appendices.
\end{comment}
\vspace{-0.2cm}
\section{Preliminaries: Variable Selection with Knockoffs and %\textcolor{red}{
Challenges of Adapting to RL}%?}}
\label{sec: 2}
%\vspace{-0.3cm}
We briefly review the Model-X knockoffs algorithm in the
regression setting \citep{candes2018panning}. Suppose that the observed data are
$\mathcal{D} = \left\lbrace (\mathbf{X}_i, \mathbf{Y}_i)\right\rbrace_{i=1}^n$ which comprise $n$ i.i.d. 
copies of $(\mathbf{X}, \mathbf{Y})$, where $\mathbf{X} = \left(X_1,\ldots, X_p\right)^\top \in\mathbb{R}^p$ 
is a vector of inputs
and $\mathbf{Y} \in \mathbb{R}^d$ is a (possibly multivariate) response.  
We say $X_j$ is a null variable if it is conditionally independent
of $\mathbf{Y}$ given the other variables; a variable that is not null is said to be
significant.  Let $\mathcal{H}_0 \subseteq \left\lbrace 1,\ldots, p\right\rbrace$ 
be the indices of the null variables and $\mathcal{I} = \mathcal{H}_0^c$ the indices of important
variables.   Given an estimator $\widehat{\mathcal{I}}$  of $\mathcal{I}$ constructed from $\mathcal{D}$ and some pre-specified level $q\in (0,1)$,  
the false discovery rate
(FDR) and modified FDR (mFDR) associated with $\widehat{\mathcal{I}}$ are given by
$\mathbb{E} \lbrace \# (\widehat{\mathcal{I}} \bigcap \mathcal{H}_0 )/ \max(1, \#\widehat{\mathcal{I}})
\rbrace 
$ and $\mathbb{E} \lbrace \# (\widehat{\mathcal{I}} \bigcap \mathcal{H}_0 )/ (1/q+\#\widehat{\mathcal{I}})
\rbrace 
$, respectively.  The goal of knockoffs is to construct an estimator of $\mathcal{I}$ which ensures the 
(m)FDR is below $q$.  


The key innovation of noise-variable addition methods is the construction of knockoff variables 
$\widetilde{\mathbf{X}} = \left(\widetilde{X}_1,\ldots,\widetilde{X}_p\right)^\top\in\mathbb{R}^p$ which mimic the covariates
$\mathbf{X}$ except that they are conditionally independent of the response 
and thus known to be null variables.  
Knockoffs are constructed so that they satisfy: (1) $\widetilde{\mathbf{X}}\indep \mathbf{Y}|\mathbf{X}$ (i.e., conditional independence of the response); (2) $( \mathbf{X}, \widetilde{\mathbf{X}} )_{\text{swap}(B)} \overset{d}{=} ( \mathbf{X}, \widetilde{\mathbf{X}})$ for any subset $B\subset \{1,2,\ldots,p\}$ (i.e., exchangeability  in distribution), %such that
%\begin{enumerate}
%    \item $\widetilde{\mathbf{X}}\indep Y|\mathbf{X}$ (conditional independence of the response)
%    \item $( \mathbf{X}, \widetilde{\mathbf{X}} )_{\text{swap}(B)} \overset{d}{=} ( \mathbf{X}, \widetilde{\mathbf{X}})$ for any subset $B\subset \{1,2,...,p\}$ (exchangeability  in distribution)
%\end{enumerate}
where $( \mathbf{X}, \widetilde{\mathbf{X}} )_{\text{swap}(B)}$ is the vector formed by switching the $j$-th entries of $\mathbf{X}$ and $\widetilde{\mathbf{X}}$ for each $j\in B$. 
Once $\widetilde{\mathbf{X}}$ is obtained, let $\widetilde{\mathcal{D}} = \lbrace
(\mathbf{X}_i, \widetilde{\mathbf{X}}_i, \mathbf{Y}_i)
\rbrace_{i=1}^n$ denote an augmented version of the observed data in which each 
$\mathbf{X}_i$ is augmented with a knockoff variable $\widetilde{\mathbf{X}}_i$.%\footnote{
	%Note that unless the distribution of $\mathbf{X}$ is known, one must estimate it in order to 
	%construct the knockoffs (See Appendix \ref{apdx: b} for additional discussion); thus, it may be more notationally consistent to write
	%$\widehat{\widetilde{\mathbf{X}}}_i$ when discussing randomly generated knockoffs, however, we suppress this dependence 
	%and simply write $\widetilde{\mathbf{X}}_i$}. 

From $\widetilde{\mathcal{D}}$ we compute feature importance statistics $Z_j$ and $\widetilde{Z}_j$ for each variable $X_j$ and its knockoff $\widetilde{X}_j$ (see Section \ref{sec: 5} for specific
examples).   Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$ be an anti-symmetric function,
i.e., $f(u,v) = -f(v, u)$ for all $u,v \in \mathbb{R}^2$, and define
$W_j = f(Z_j, \widetilde{Z}_j)$ so that higher values of $W_j$ are associated with
stronger evidence that $X_j$ is significant.  
Given a target FDR level $q$, the $j$th variable is selected if $W_j$ is no less than some threshold $\tau_q$ (or $\tau_{q+}$ in knockoffs+ method).  For instance, the set
of selected variables is $\widehat{\mathcal{I}} = 
\lbrace
j \,:\, W_j \ge \tau_q
\rbrace$ with 
\begin{align*}
	\tau_q = \min \left\lbrace
	\tau >0 \,:\,  \# \lbrace
	j\,:\, W_j \leq -\tau 
	\rbrace
	/ \# \lbrace
	j\,:\, W_j \geq \tau 
	\rbrace \le q 
	\right \rbrace,
\end{align*}
in the knockoffs method. See %Algorithm \ref{alg: 1} and 
the related part in Supplementary Material \ref{apdx: b}
for more details. As long as the $W_j$'s satisfy the flip-sign property (see Section \ref{sec: 5}), 
the knockoffs procedure controls the FDR (and mFDR). %See %the discussion of such property and 
%the corresponding theoretical results in Section \ref{sec: 5}.





Model-X knockoffs possess a number of desirable statistical properties.  
First, it provides non-asymptotic control of the FDR given the covariate
distribution.  
Second, it does not
depend on any specific model structure between the outcome and covariates; thus, the knockoffs method applies in nonlinear and high-dimensional settings.  Finally,
the true positive rate is near optimal in a wide class of problems 
\citep[][]{fan2019rank, weinstein2020power, wang2020power, ke2020power}.  However, the derivation of these theoretical
properties relies critically on the assumption that the observed data are independent---an assumption that is clearly violated in RL problems.  Another challenge  is that the definition of a significant variable in an MDP is less obvious than it is
in regression.  Attempting to mimic the regression setting, say by defining significant state variables
as those associated with the reward will fail to capture variables which impact
cumulative reward indirectly through state transitions.  Conversely, if one 
defines the `outcome' as the reward concatenated with the next state, then the knockoffs procedure 
will incorrectly 
select those which are associated with the next state but have no effect on the (present or 
future) reward.  We propose to define variable significance in RL in terms
of the minimal sufficient state which we introduce in the next section.  





%Model-X knockoffs enjoys several nice statistical properties. First, it achieves exact FDR control in finite samples. In contrast, most variable selection algorithms are %\textit{asymptotically} selection consistent \citep[see e.g.,][]{zhao2006model,meinshausen2010stability,fan2011nonconcave}. Second and more importantly, %it can accommodate any model 
%such a finite sample properties does not require any model assumption between the response and the covariates, allowing for arbitrary nonlinear relationship and dimensionality of covariates. %Finally, existing works including \cite{weinstein2020power} and \cite{wang2020power} have shown that the power (defined similar as the true positive rate) of Model-X knockoffs is near-optimal %in many applications. These properties allow us to handle complicated high-dimensional systems in offline RL domains with limited number of observations, as we elaborate in detail later. 
%This is ultimately different from most variable selection algorithms (e.g., LASSO, the Dantzig selector) with asymptotic guarantees of consistent model selection only. Second,  

%there is no any model assumption on the relationship between $Y$ and $\mathbf{X}$, where Model-X knockoffs method is able to select significant variables with FDR control.  Second, no restriction is imposed on the dimension of covariates i.e., $p$, compared to sample size $n$. \QZL{Add one sentence why the first two are important for RL}. %In addition, such procedure can deal with random design in data, which accommodates with sequential decision scenario. Finally, Model-X knockoffs control the FDR exactly in the finite-sample setting. %Motivated by these, we consider generalize this variable selection idea to the setting of MDPs.

%An essential requirement for establishing the theoretical guarantees of Model-X knockoffs is the \textit{independent} assumption among all the data observations. As commented before, this assumption is typically violated in RL. As such, directly applying the Model-X knockoff method would fail to control the FDR. In addition, in supervised learning, the definition of significant variables is clear. However, the definition becomes less clear in RL. This is because a significant state variable can affect the cumulative reward either through its dependence upon the immediate reward, or that upon the future \textit{significant} states. Ignoring either dependence will lead to insufficient selection of state variables, whereas considering features involved in either the reward function or the state transition function (as a function of the entire future state vector) will overselect variables. See Section \ref{sec:select} for more details. Therefore, addressing these challenges requires the development of new methods and theory for feature selection in offline RL.
%affecting either the reward or the entire future state (including both significant and insignificant variables) would result in false positives. 

%One building block for the Model-X knockoff method is the independent assumption among all observations e.g., it requires that $(\mathbf{x,y})\in\mathbb{R}^{n\times (p+1)}$ are i.i.d. random samples. However, as introduced in Section \ref{sec: 2.1}, the transition tuple $(\mathbf{S}_t, A_t, R_t, S_{t+1})$ is generated sequentially, which is a unique characteristic of MDP and should never be relaxed. Breaking such assumption makes the standard Model-X knockoff method fail and new algorithm needs to be designed with FDR control. What's more, the definition of significant dimensions does not apply to RL settings. Originally in a selection problem, we want to extract a small portion of variables from $\mathbf{X}$ so that it contains the same contribution to $Y$ as $\mathbf{X}$. But the contribution in RL is two fold: the dependence of reward $R_t$ on $(\mathbf{S}_t, A_t)$ through function $r$, and the dependence of transition $P(\mathbf{S}_{t+1}|(\cdot,\cdot))$ on $(\mathbf{S}_t, A_t)$. Ignoring either one will lead to insufficient selection, while simply combing $r$ and $P$ as a global response causes over-estimation of $\hat{G}$ (explained further in an example in \ref{sec: 3.4}). This paper solves both issues through the design of SEEK.

%\subsection{Markov Decision Processes}
%\label{sec: 2.1}

%A sequential decision problem is 

%At each decision point $t$, an agent is at state $\mathbf{S}_t \in \mathcal{S}$. She chooses an action $A_t$ with a probability \QZL{Tao, we study the offline setting. It is strange to mention some target policy here} $\pi(A_t|\mathbf{S}_t)$ according to some stationary policy $\pi$, then receives a reward $R_t = R(\mathbf{S}_t, A_t, \mathbf{S}_{t+1})$, and proceeds to the next state $\mathbf{S}_{t+1}$ under the transition kernel $P(\mathbf{S}_{t+1}|\mathbf{S}_t, A_t)$. We impose the following assumptions on our observed data:

\begin{comment}
	\begin{assumption}
		[Markov assumption]
		\label{asp: 1}
		For any $t \geq 1$, $\mathbf{S}_{t+1}\indep (\mathbf{S}_j,A_j)_{0\leq j \leq t-1}|(\mathbf{S}_t, A_t)$
	\end{assumption}
	
	\begin{assumption}
		[Conditional mean independence assumption]
		\label{asp: 2}
		$R_t = R(\mathbf{S}_t,A_t,\mathbf{S}_{t+1})$ 
		for some known function $R$, \QZL{Tao: please try not to define sth in assumption}which then gives
		$\mathbb{E}(R_t|(\mathbf{S}_t,A_t)) = r(\mathbf{S}_t,A_t)$ for some unknown deterministic function $r$.
	\end{assumption}
	
	\begin{assumption}
		[Stationarity and exponentially $\beta$-mixing]
		\label{asp: 3}
		Data are generated by some fixed stationary policy. %$b$ i.e., $\prob(A_t=a|\overline{S}_t)=b(a|S_t)$ for any $t \geq 0$.
		In addition, the stochastic process $\{(\mathbf{S}_t, A_t)\}_{t\ge 0}$ is stationary and exponentially $\beta$-mixing (see the definition in \ref{apdx: 2}).
	\end{assumption}
	
	
	The first two assumptions are standard in the literature of RL. The last assumption is commonly imposed in the literature to handle the offline data \citep[see e.g.][]{kallus2022efficiently}. 
	%For consistency, we adopt the following for notations: upper case for random variables/vectors, lower case for realizations of random vectors; bold face for vectors. 
	Under the above assumptions, $\{(\mathbf{S}_t, A_t)\}_{t\ge 0}$ forms a time-homogeneous Markov chain. %We remark that assumptions can be further relaxed. For instance, we can let the behavior policy be history-dependent and our method developed below can be naturally extended.
	One essential goal of RL is to learn a policy that maximizes the value
	$
	\mathcal{V}(\pi)=\sum_{\mathbf{s}\in \mathcal{S}} V^{\pi}(\mathbf{s}) a(\mathbf{s}),
	$
	where $a$ denotes some reference distribution on $\mathcal{S}$, and the expected cumulative reward given an initial state $\mathbf{s}$ is represented by the value function
	$ V^{\pi}(\mathbf{s})=\sum_{t=0}^{+\infty} \gamma^\top \mathbb{E}^{\pi} (R_{t}|\mathbf{S}_{0}=\mathbf{s})$. 
	Here $\mathbb{E}^{\pi}$ denotes the expectation assuming the actions are selected according to $\pi$. 
	
	
	One of the biggest challengings in (batch) RL is the high-dimensional state space, which makes both policy evaluation and optimization extremely difficulty. Thus, it is important to select useful variables and exclude useless ones in order to enhance the efficiency of the policy optimization. Generally in a problem of variable selection, we are given a vector of covariates $\mathbf{X}=(X_1,...X_p)\in\mathbb{R}^p$ and response variable $Y$, with the goal to select a subset of the covariates to be the significant ones contributing to $Y$. 
	A variable $X_i$ is said to be null if it is independent of the response given all the other dimensions, i.e. 
	$X_i \indep Y | X_{-i}$, where $X_{-i}$ denotes the vector by dropping $X_i$. The performance of the variable selection procedure is usually measured by false discovery rate (FDR) and its variants. Denote the index set of dimensions in $\mathbf{X}$ for the null variables as $H_0$. Then for any selecting subset of variables $G\subset\{1,...,p\}$, we can define its FDR \citep{benjamini1995controlling} and the modified FDR (mFDR) respectively as
	\begin{align*}
		\text{FDR}(G)
		&=\mathbb{E}\left[\frac{\displaystyle|G\cap \mathcal{H}_0|}{\displaystyle|G|}\right]  && \mbox{and} 
		&&&
		\text{mFDR}(G) = \mathbb{E}\left[\frac{\displaystyle|G\cap \mathcal{H}_0|}{\displaystyle|G|+1/q}\right],
	\end{align*}
	i.e. the proportion of selection that corresponds to nulls, where $|\cdot|$ on a set means its cardinality. \QZL{Tao: Discribe the goal of variable selection: control FDR, etc.}
	
	
	\QZL{Tao: you need to describe what the meaning of significant variables in RL is; such as contribute to reward and dynamic?}In the problem of RL, we want to select useful dimensions out of all state variables $S_1,...,S_p$ and treat the action dimension to be always significant. And our work is to propose a knockoffs-based sequential procedure (SEEK) to make a selection $\hat{G}$ with theoretical control on both mFDR and FDR.
	
\end{comment}

\vspace{-0.5cm}
\section{Minimal Sufficient State and Model-based Selections}\label{sec: minimal}
%\vspace{-0.3cm}
\begin{comment} # removing as we said all this above
In this section, we formally define the minimal sufficient state. We show
that na\"ive extensions of variable selection methods designed
for supervised learning (knockoffs or
otherwise) fail to recover the minimal sufficient state.  Insights gleaned 
from the shortcomings of these extensions are used to construct
a method  that is provably to find the minimal sufficient state.
\end{comment}

We model the observed data as a time-homogeneous MDP, denoted by  $\mathcal{M} = (\mathcal{S}, \mathcal{A}, r, \mathcal{P}, \gamma )$, with a state space $\mathcal{S}=\prod_{j=1}^p \mathcal{S}_j\subseteq \mathbb{R}^p$, a discrete action space $\mathcal{A}$, a mean reward function $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, a transition kernel $\mathcal{P}:\mathcal{S}  \times \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^+$, and a discount factor $\gamma\in[0,1)$ \citep[see][for additional background on MDPs]{puterman2014markov}. Let $(\mathbf{S}_t, A_t,R_t)$ denote the state-action-reward triplet at time $t$. The dynamic system is Markovian such that both 
the reward and next state, 
$R_t$ and $\mathbf{S}_{t+1}$, are conditionally independent of the past data history given the current state and action, $\mathbf{S}_t$ and $A_t$. The transition kernel defines the conditional distribution of the next state given the current state-action pair. The reward function, $r$, characterizes the conditional mean of $R_t$ given $\mathbf{S}_t$ and $A_t$. We further 
assume the reward is bounded, i.e., 
there exists constant $R_{\max}$ such that 
$|R_t|\le R_{\max}$ almost surely for any $t$. 
We use $(\mathbf{S}_t, A_t, R_t, \mathbf{S}_{t+1})$ to refer 
to a generic state-action-reward-next state tuple. For any  $\mathbf{\nu} \in \mathbb{R}^p$ and index set $G\subseteq \{1, 2, \ldots, p\}$, let 
$\mathbf{\nu}_{G} = \{\nu_{g} \, \mid \, g \in G \}$; thus, 
$\mathbf{S}_{t,G}$ is the subvector of state $\mathbf{S}_t$ indexed
by components in $G$. Let $G^c = \lbrace 1,\ldots, p\rbrace
\setminus G$ be the complement of $G$. 
\begin{definition}[Sufficient State]\label{def: sufficient state}
	We say $\mathbf{S}_{G}$ is a sufficient state in an MDP if 
	$R_{t} \indep \mathbf{S}_{t, G^c} \given (\mathbf{S}_{t, G}, A_t)$ and $\mathbf{S}_{t+1,G} \indep \mathbf{S}_{t, G^c} \given (\mathbf{S}_{t, G}, A_t)$, for all $t \ge 0$. 
	%\item[(c)] There exists a policy $\widetilde{\pi}: \mathcal{S}_G \rightarrow \mathcal{A}$ that maximizes $\mathcal{V}(\pi)$.
	%\end{enumerate}
\end{definition}
This definition of sufficiency is weaker than the condition that $R_t$ and $\mathbf{S}_{t+1,G}$ are \textit{jointly} conditionally independent of $\mathbf{S}_{t, G^c}$.
Furthermore,  a sufficient state always exists as $\mathbf{S}_t$ itself is sufficient. The following results justify the use of the 
term {\em sufficient} in Definition \ref{def: sufficient state}.
\begin{proposition}\label{prop1}
	If $\mathbf{S}_{G}$ is a sufficient state, then there exists an optimal policy that depends only on  $\mathbf{S}_G$.
	%there must exit a policy $\pi_G: \mathcal{S}_G \rightarrow \mathcal{A}$ that achieves the same value function as the optimal policy that is a function of the entire state vector.
	%maximizes $\mathcal{V}(\pi)$.
\end{proposition}


\begin{proposition}\label{prop2}
	If $\mathbf{S}_{G}$ is a sufficient state, then the process $\{\left(
 \mathbf{S}_{t, G}, A_t, R_{t}\right)\}_{t\geq0}$ forms a time-homogeneous MDP. 
\end{proposition}
%\textcolor{red}{In the proof, we need to show the time-homogeneity as well.}

Proposition \ref{prop1} allows us to focus on policies that are functions of sufficient states only. Most existing RL algorithms require the data-generating process to follow an MDP model, Proposition \ref{prop2} implies that once the sufficient state is identified, these algorithms can be directly applied 
to the reduced process $\{\left(\mathbf{S}_{t, G}, A_t, R_{t}\right)\}_{t\geq0}$. A low-dimensional sufficient state can enhance the performance of policy optimization and reduce computational costs. Therefore we aim to 
identify \textit{minimal sufficient state}, which is defined
as follows.

\begin{definition}[Minimal Sufficient State]
	We say $\mathbf{S}_G$ is a minimal sufficient state if it is the sufficient state with the minimal dimension among all sufficient states.
	%$\bar{G} = \cap_{G \in \mathbb{G}} G$, where $\mathbb{G}$ denotes the set of all sufficient states.
\end{definition}

Below we show that the minimal sufficient state is well-defined and unique under mild conditions.

\begin{proposition}\label{prop3}
	The minimal sufficient state always exists. Furthermore, if the %condition in Proposition \ref{prop1} holds. 
	transition kernel is strictly positive, then the minimal sufficient state is unique. 
\end{proposition}

%The minimal sufficient state always exists. However, it is not necessarily unique. To elaborate, consider the example with two perfectly correlated states variables such that $\mathbf{S}_{t,1}=-\mathbf{S}_{t,2}$ for any $t\ge 0$. Suppose both states contribute to the immediate reward. Then both $\mathbf{S}_1$ and $\mathbf{S}_2$ are the minimal sufficient state. 
%might not be unique. 
\begin{comment}
	The following proposition guarantees that the minimal sufficient state always exists.
	\begin{proposition}
		If there exists a nonempty set $G$ such that for any $t \geq 0$, $R_{t} \indep S_{t, G^c} \, \mid \, \mathbf{S}_{t, G}$, then the minimal sufficient state $\bar{G}$ always exists.
	\end{proposition}
	Now suppose we are given batch data $\mathcal{D} $ consisting of $N$ independent and identically distributed finite-horizon trajectories with length $T$, denoted by
	$
	\mathcal{D} = \left\{ \left\{\left(S^i_t, A_t^i, R_t^i, S^i_{t+1}\right)\right\}_{0 \leq t < T} \right\}_{1 \leq i \leq N}.
	$
\end{comment}
Our goal is to identify the minimal sufficient state using  batch data. 
%As discussed also in \citet{candes2018panning}, in selection problems, further analysis will be based on such uniqueness, guaranteed with mild conditions.
%$\mathcal{D}$ 
%with theoretical warranty.
To provide insights into this problem and to motivate our
proposed approach, we first discuss two extensions 
of variable selection methods for supervised learning
to MDPs which fail to recover the minimal sufficient state. 

\textbf{Reward-only approach}: One approach to extending
variable selection methods for regression to the MDP setting is
to treat the state and action as covariates and the
reward as the outcome, and then apply an existing regression
method as if the data were comprised of i.i.d. input (state, action) output (reward) pairs.  This
approach will fail to identify components of the state
which do not affect immediate reward but rather 
affect long-term rewards through their impact on
the next state. Thus, this approach can have a high
false negative rate,
need not be consistent for a sufficient state, and a
process indexed by a state constructed using this approach does not need to satisfy the Markov property.  
\begin{comment}
	This na\"ive method directly searches the significant state variables that affect the immediate reward \textit{only}, which can be regarded as a myopic approach. This method will lead to a high false negative rate due to the missing of important variables in the transition dynamics. Moreover, %as shown by the following proposition, 
	as we will show later, 
	one cannot guarantee the selected variables by such reward-only method satisfy the Markovian assumption, which makes most existing RL algorithms fail.
\end{comment}
\begin{comment}
	\begin{proposition}
		There exists an MDP such that the state selected by any reward-only method is \textit{not sufficient}.
	\end{proposition}
\end{comment}

\textbf{One-step approach}: To capture components of the 
state associated with both short- and long-term impacts
on the reward, one can treat the reward and the next state 
jointly as a multivariate outcome and the current state-action pair
as the input. With this setup, one can apply variable
selection techniques for multiple-outcome regression.  
This approach was considered by \cite{tangkaratt2016model}
in the context of dimension reduction.  While
this approach can consistently remove pure noise variables,
it will fail to remove time-dependent variables that do not affect
cumulative reward.  
Consequently, as shown below, %this approach 
it will 
identify a sufficient
state but need not identify a minimally sufficient one. 
\begin{comment}
	In order to avoid myopia and maintain the MDP structure among selected state variables, we can treat the immediate reward and next state as a multiple-response variable and implement a variable selection method \textit{once} for finding the significant state variables. A similar algorithm was developed by \cite{tangkaratt2016model} for dimension reduction. As we will show later, 
	while  %it can be shown in the following proposition that 
	the selected state is \textit{sufficient}, it may fail to eliminate time-dependent state variables that are irrelevant for the optimal decision making, and thus lead to a high false positive rate.
\end{comment}
\begin{comment}
	\begin{proposition}
		Assume that the variable selection in a one-step approach is oracle. Then the selected state is \textit{sufficient}.
	\end{proposition}
\end{comment}

\textbf{Proposed iterative approach}: The one-step approach fails to identify the minimal sufficient state because the response includes the entire next state vector (including those which are unrelated to the
cumulative reward). Conceptually, one could avoid this
by taking every possible subset $G$ of $\left \lbrace 1,\ldots, p
\right\rbrace$, and testing the sufficiency condition in Definition \ref{def: sufficient state}.
% $R_t \indep \mathbf{S}_{t, G^c} |(\mathbf{S}_{t,G}, A_t)$ and
% $\mathbf{S}_{t+1,G} \indep \mathbf{S}_{t,G^c} | 
% (\mathbf{S}_{t,G}, A_t)$ for
% all $t \ge 0$.  
However, such an approach is not scalable as
it requires examining $2^p$ subsets and, for each,
conducting 
multivariate conditional independence tests. %{\color{orange} 
As a result, we propose a sequential knockoffs procedure that is (asymptotically) equivalent to such a procedure while requiring at most $p$ iterations to converge.  At a high level, the algorithm proceeds as follows. %(a formal description is given in Section \ref{sec: 4}).  
At the first iteration, a variable selection algorithm is applied to identify, $\widehat G_1$, the components of state significant for predicting the immediate reward. The second iteration uses  
the selection algorithm to identify, $\widehat G_2$, the components of state significant for predicting 
$\mathbf{S}_{t+1,\widehat G_1}$. %for all $t \ge 0$ \textcolor{red}{how to make sure for all $t \ge 0$. This looks strange to me.}. 
Then recursively for $j\geq 2$, we identify the set of variables $\widehat G_j$ which predict
$\mathbf{S}_{t+1, \widehat G_{j-1}}$. The algorithm terminates when $\widehat G_j = \widehat G_{j+1}$. As the sets $\widehat G_{j}$ are monotonically non-decreasing, the algorithm terminates in no more than $p$ steps. %\textcolor{red}{the index $j$ is not defined}
%}


\begin{comment}
	A more effective approach is to iteratively select state variables that affect the reward or those \textit{significant} next state variables. Specifically, at the initial iteration, where no state variables are selected, we simply apply the reward-only approach for variable selection. In later steps, we treat %the reward and 
	the selected state variables in the next time point as a whole response to perform variable selection. Essentially, at the $m$th iteration, this approach identifies variables that are significant to the reward at the $m$th step further. 
	%The basic idea is that at $m$th iteration, one implements a variable selection method for finding state variables that are significant to the reward at the $m$-th step further. Intuitively, this iterative approach can find the sufficient state and more importantly, 
	It avoids selecting useless time-dependent state variables compared with the one-step approach, and more importantly, correctly identifies the minimal sufficient state. We consider the following toy example to further elaborate. 
\end{comment}


%To further illustrate the advantage of developing an iterative selection approach, we design the following toy example. 
\textbf{A numerical example}: We conduct a simulation example to illustrate the advantage of the iterative selection method. In this example, the dimension of the state is $20$ and the index set of the minimal sufficient state is $G_{\text{M}}= \{1,2\}$.
%\textcolor{red}{why is there a subscript $M$?} 
The generative model is such that $S_{t, 1}$ directly influences $R_t$ whereas $S_{t, 2}$  affects $S_{t+1,1}$ and hence has an indirect effect on the reward.
%only involves in the transition dynamics of $S_{t, 1}$, therefore only influencing the future rewards. 
The remaining state variables can be divided into two groups. The first
group, $G_{\text{AR}}=\{3,\ldots,11\}$ indexes state variables whose transitions are independent and follow a first-order autoregressive (AR(1)) model, i.e.,
\begin{align*}
	\mathbb{P}(S_{t+1,j}|\mathbf{S}_t,A_t) = \mathbb{P}(S_{t+1,j}|S_{t,j}), \qquad j= 3,\ldots,11.
\end{align*}
The second group $G_{\text{WN}}=\{12,\ldots,20\}$ indexes state variables which are i.i.d. white noise across all decision points. 
% {\color{orange}
	% Details needed: did we use the model-X knockoffs for all cases?  Assuming that
	%  we did, what was the nominal FDR rate thas was used?  How many MC
	%  iterations were used?  If example really shows an asymptotic/theoretical 
	%  result then we shouldn't say it's numeric.  
	% }
We conduct a numerical study using the above three approaches, and summarize the results in Figure \ref{fig:toy}  and Table \ref{tab:toy} under a fixed target FDR of 0.3, aggregated over 20 runs. The implementation details of these methods is  presented in Section \ref{sec: seek}. It can be seen from Table \ref{tab:toy} that
%As we will see later, 
the reward-only method fails to select $S_{t, 2}$, as expected. Furthermore, the one-step method selects all variables in $G_{\text{AR}}$ because they are time-dependent and contribute to the state transition function. In contrast, the iterative procedure (based on our proposed algorithm) avoids selecting redundant variables in $G_{\text{AR}} \cup G_{\text{WN}}$ and is thus more appealing. It can be seen from Figure \ref{fig:toy} that the proposed algorithm controls the FDR and has a much larger area under the receiver operating characteristic curve among the three methods.
%To further demonstrate the superior performance of the iterative approach, following three results in comparison to the reward-only and one-step methods: 1. the actual FDR versus the target FDR by fixing the threshold for variable selection as 0.7 in the left panel of Figure \ref{fig:toy}; 2. the receiver operating characteristic curve under a fixed target FDR as 0.3 in the right panel of Figure \ref{fig:toy}; and 3. the mode of selected variables in Table \ref{tab:toy}. % under the threshold as 0.7 and the target FDR as 0.1. 




%\begin{figure}[!t]

\begin{figure}
%\begin{minipage}{0.67\linewidth}
	\centering
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/Toy_FDR.pdf} 
	\end{subfigure}%
	\begin{subfigure}{0.45\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/Toy_TPR.pdf} 
	\end{subfigure}%
	\caption{Receiver operating characteristic (ROC) curves for different methods in the toy example.}% All the results are aggregated over 20 runs.}
\label{fig:toy}
%\end{minipage}
\end{figure}

% \vspace{-0.3cm}
%\end{figure}


\begin{table}
\linespread{1.25}\selectfont
\caption{Most frequently selected states by different methods in the toy example.}\label{tab:toy}
\centering
\small
\begin{tabular}{ll}
	\toprule  
	Method     & Selected  States   \\
	\midrule
	SEEK (Iterative) & [1, 2]      \\
	Reward-only Method     & [1]      \\
	One-step Method     & [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \\
	\bottomrule
\end{tabular}
\end{table}

%\QZL{will add a description}.

The following proposition summarizes the properties of these three approaches in the population level, i.e., assuming we have infinitely many observations.

\begin{proposition}\label{prop4}
Assume that one applies a 
selection-consistent algorithm for the above three approaches. Then, (i) there exists an MDP such that the state selected by the reward-only method is not sufficient; and (ii) the state selected by the one-step method is sufficient but not minimally sufficient.  In addition, the iterative approach
is able to correctly recover the minimal sufficient state for any MDP provided the
transition kernel is strictly positive.  %\textcolor{red}{what is the meaning of consistent? This proposition looks heuristic to me.}
%\begin{enumerate}
%    \item[(i)] There exists an MDP such that the state selected by any reward-only method is not sufficient.
%    \item[(ii)] The state selected by the one-step method is sufficient. However, there exists an MDP such that some of the selected state variables are redundant. 
%    \item[(iii)] 
%\end{enumerate} 
\end{proposition}


The preceding result does not depend on the selection algorithm being applied, only that
it is selection consistent. Thus, this result is of independent interest as it provides
a template for constructing variable selection methods for MDPs.  In the
remainder of this paper, we focus on developing the 
\underline{se}qu\underline{e}ntial \underline{k}nockoffs (SEEK) algorithm
which extends Model-X knockoffs to the MDP setting combining the 
iterative procedure described above.  


%The above discussions demonstrate the superiority of the iterative approach among all three methods. It remains to determine the variable selection method at each iteration. As commented in Section \ref{sec: 2}, Model-X knockoffs is particularly appealing in offline RL mainly due to its capability of capturing complex nonlinear relationships %between dependent and independent variables 
%with exact FDR control and good power properties. Motivated by these, we propose a \underline{se}qu\underline{e}ntial \underline{k}nockoffs (SEEK) algorithm that innovatively couple knockoffs with the iterative procedure to identify the minimal sufficient state. 
%We emphasize that na\"ively combining these two ideas can not be valid due to the temporal dependence in the batch data. %$\calD_N$.



\begin{remark}
Our proposed variable selection is general in that it benefits a wide variety of downstream analyses. %\textcolor{red}{Don't use E.G. as the start of a sentence.}
For instance, one can evaluate any number of candidate policies, conduct exploratory analyses, etc. This is especially
appealing with batch data collected in biomedical applications such as mobile-health (mHealth), in which there are numerous scientific questions of interest 
\cite[][]{tewari2017ads,luckett2019estimating,liao2020batch}.  
We note that if one were interested in, say, identifying variables present in the optimal policy, one could apply our method as an initial screening step 
% (the minimal sufficient state might include some additional variables not directly relevant to policy optimization, in order to satisfy the Markov assumption) 
and then apply an algorithm like penalized Q-learning to the reduced data.  
\end{remark}


\begin{comment}
To conclude this section, we remark that our definition of the minimal sufficient state is model-based as it characterizes on the conditional independence between the reward-next state and the current state-action pair. Alternatively, one may use a model-free definition and consider state variables that appear in the optimal Q-function or the optimal policy. However, these significant states alone might not meet the Markov property and existing RL algorithms cannot be directly applied to the data subset with reduced dimensionality for policy optimization. As shown in Proposition \ref{prop1}, the proposed minimal sufficient state contains those significant states involved in $Q^{\ast}$. Meanwhile, it might include some additional states that contribute to the transition function of those significant states, but are not directly relevant to optimal decision making. Nonetheless, in some applications, we are interested in these additional states as well {\color{blue}(Eric, could you please add a remark here?)}
\end{comment}
\vspace{-0.5cm}
\section{SEEK: Sequential Knockoffs for Variable Selection}
\label{sec: seek}
\vspace{-0.3cm}
\subsection{The Main Idea}
%\vspace{-0.3cm}
In this section, we introduce our SEEK algorithm, which consists of three key steps. An outline is displayed in Algorithm \ref{alg: 1}. Additional implementation details can be found in Section  \ref{apdx: b} of the Supplementary Material. Suppose that we are given batch data $\mathcal{D}$ consisting of $N$ i.i.d. finite-horizon trajectories, each of length $T$, which can be summarized as $NT$ transition tuples. Note that observations within each trajectory are temporally dependent.
%The complete algorithm and technical derivations can be found in Appendix XXX.    

\textbf{Step 1: Data splitting}. %\\
We first split all $NT$ transition tuples in the batch data into $K$ non-overlapping sub-datasets, leading to a partition $\calD=\{\mathcal{D}_k, k\in[K]\}$ such that for each tuple, $
\left(\mathbf{S}_{t}, A_{t}, R_{t}, \mathbf{S}_{t+1}\right) \in \mathcal{D}_k$ if $t \, \textrm{mod} \, K = {k-1}$.
After the split, every two tuples within the same subset $\mathcal{D}_k$ either come from the same trajectory with a time gap of at least $K$, or belong to two different trajectories. When the system is $\beta$-mixing \citep{bradley2005basic}, by an innovative use of Berbee's coupling lemma \citep{berbee1987convergence} and a careful choice of $K$, we can guarantee that all transition tuples within each subset $\mathcal{D}_k$ are ``\textit{approximately independent}.'' %with a high probability. 
See Supplementary Material Section \ref{apdx: proof} for a rigorous statement of this result. This 
justifies our iterative use of Model-X knockoffs on each $\mathcal{D}_k$  for finding the minimal sufficient state in the following steps. Finally, using  majority vote, we aggregate the selection results from all the data subsets. %of using all the batch data and reach a more robust result.

In Section \ref{sec: bestK}, we develop a practical algorithm to adaptively select the number of splits $K$ under the $\beta$-mixing condition. When the system is non-ergodic, these $\mathcal{D}_k$'s can be constructed based on transition tuples from different trajectories to satisfy the independence assumption. 
%\textbf{Step 2: Action-based subgrouping}\\
%For each subset $\mathcal{D}_k$, we further divide $\mathcal{D}_k$ into $\mathcal{F}_k=\{\mathcal{D}_k^{(j)},j\in \mathcal{A}\}$ such that all transition tuples in $\mathcal{D}_k^{(j)}$ have the same action value. By further splitting each data subset by their actions, we have great flexibility to choose different black-box machine learning algorithms for constructing statistics for each state variable.

%The measure of importance in each variable (dimension) is gained primarily from model fitting with some appropriate machine learning black box. Especially, as defined in below, this is a partial selection scenario, where we should always keep action variable without even bothering to test its importance. But that does not mean some regression is only from states to rewards. In fact, due to the potential interaction between states and actions, both should be included in the model. Then the idea way is to separate the data according to different values of actions, and go on to perform model fitting.


\textbf{Step 2: Iterative selection}. 
On each data subset $\mathcal{D}_k$, we implement our iterative selection approach using Model-X knockoffs. We first apply the aggregated knockoffs procedure (across actions, see Algorithm \ref{alg: 2}) to select the significant state variables for predicting the \textit{immediate reward} only. Denote this  initial selection by $\widehat{G}_{k,1}$. Subsequently, we treat all selected state variables in $\widehat{G}_{k,1}$ in the next decision point as a multivariate response and use aggregated knockoffs again to select state variables that are significant to this response. %those in $\widehat{G}_{k,1}$. 
The final index set is denoted by $\widehat{G}_{k,2}$. We repeat this procedure to obtain $\widehat{G}_{k,3}, \widehat{G}_{k,4}, \ldots,$ where at each iteration we combine all previously selected state variables in the next-time-point to construct the multiple-response variable, until no more significant variables are found. The resulting index set of selected variables is denoted by $\widehat{G}_k$. 


 In our implementation, we adopt a second-order machine to construct knockoff variables. However, other construction approaches are possible, see approaches in Section \ref{apdx: b} of the Supplementary Material. In addition, for linear systems, we set each feature importance statistics $Z_j$ (or $\widetilde{Z}_j$) to be the %difference in the 
$\ell_{\infty}$-norm of the estimated regression coefficients in multivariate response regression. %and its knockoff. 
In nonlinear systems, we set $Z_j$ (or $\widetilde{Z}_j$) by the variable importance with general machine learning methods. See details in Section \ref{apdx: b}.



%Our idea of developing this iterative selection procedure is also inspired by the celebrated FQI. In FQI, at $m$-th iteration, given the previous $Q^\ast$ estimate $Q_{m-1}$, one updates the estimation of $Q^\ast$ by performing supervised learning from $R + \gamma\max_{a' \in \mathcal{A}}Q_{m-1}(\mathbf{S}', a')$ on $(\mathbf{S}, A)$ and it will eventually converge to $Q^\ast$. In particular, when $m =1$, it is a direct regression from $R$ on $(\mathbf{S}, A)$. Our iterative procedure is analogous to FQI. As seen in the previous paragraph, at the beginning step, we select state variables that are significant to the immediate reward only. Once $\widehat{G}_{k,1}$ is obtained, we then implement our variable selection procedure to select state variables that are only significant to those targeted variables in $\widehat{G}_{k,1}$ because the first step of our iterative procedure implies that the first update of the optimal $Q$-function estimation only depends on $\widehat{G}_{k,1}$, based on which it is sufficient to focus on obtaining $\widehat{G}_{k,2}$ that are significant to $S'_{\widehat{G}_{k,1}}$ in the second step of our procedure, mimicking the second step of FQI. Through the comparision with FQI, we give an intuition why our method can guarantee to find the minimal sufficient state for the optimal decision making.

\textbf{Step 3: Majority vote.}  Compute the proportion of subsets in which variable $j$ was selected, $\widehat{p}_j = K^{-1}\sum_{k=1}^K \mathbb{I}\left(
j \in \widehat{G}_k
\right)$, and define the final selected set 
$\widehat{G} = \left\lbrace j\,:\, \widehat{p}_j \ge \alpha \right\rbrace$,
where $\alpha \in (0, 1)$ is a pre-specified threshold (e.g., 0.5).  

The number of splits, $K$, is a potentially important tuning parameter.  We discuss an adaptive (data-driven) procedure for selecting $K$ in the next section. 


%Specifically, a state variable is finally selected if it has been selected by more than $\alpha$ proportion of all the $\widehat{G}_k$, where $\alpha \in (0, 1)$ is some pre-specified threshold (e.g., 0.5). We denote the output of our final selected index set as $\widehat{G}$.

\begin{algorithm}[t]
%\setstretch{1.35}
\linespread{1.35}\selectfont
\caption{SEEK Algorithm}\label{alg: 1}
\hspace*{\algorithmicindent} \textbf{Input:} Batch data $\mathcal{D}$, number of data subsets $K$, a target FDR level $q \in (0, 0.5)$, and a threshold $\alpha\in(0,1)$ for the majority vote.
\begin{algorithmic}[1]
	\State Split $\mathcal{D}$ evenly into $\{\mathcal{D}_k, k\in[K]\}$ , with $NT/K$ transition tuples in each $\mathcal{D}_k$. Initialize all $\widehat{G}_k$'s as empty sets.
	
	\For {$k=1,2,\ldots,K$}
	%	        \State Define an empty index set $\widehat{G}_k$.
	%            \State Set the candidate index set $B$ to $\{1,...,p\}$
	\State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
		Apply Algorithm \ref{alg: 2} to all $(\mathbf{S}, A, R)$ tuples in $\mathcal{D}_k$ with %the candidate index set $B=\{1,...,p\}$ and 
  the target FDR level $q$. Denote the selected index set by $\widehat{G}_{k, 1}$ and let $\widehat{G}_k = \widehat{G}_{k, 1}$. %\strut}
	\strut}
%    	    \State Let $\widehat{G}_k = \widehat{G}_{k, 1}$
\For{$j = 2, 3, ...$}
%\State For each tuple $(\mathbf{S}, A, R)$, let $\mathbf{Y}= \mathbf{S}_{\widehat{G}_{k}}'$
\State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
	Apply Algorithm \ref{alg: 2} to all $(\mathbf{S}, A, \mathbf{Y})$ tuples in $\mathcal{D}_k$ with $\mathbf{Y}= \mathbf{S}_{\widehat{G}_{k}}'$ %, the candidate index\\  set $B=\widehat{G}_{k}^c$, 
    and the target FDR level $q$. %%for selecting significant states and 
%   	    , \\to obtain $\widehat{G}_{+}$.
%\State 
Denote the selected index set by $\widehat{G}_{k, j}$. \strut}
 \State \textbf{Break} if $\widehat{G}_{k, j} \subset \widehat{G}_{k}$.
\State Let  $\widehat{G}_k = \widehat{G}_k \cup \widehat{G}_{k, j}$. 
\EndFor
\EndFor

\State \textbf{Output}: $
\widehat{G} := \{j \in \{1,\ldots,p\}: \sum_{1\leq k\leq K}\mathbb{I}(j \in \widehat{G}_k) \geq \alpha K\}.
$
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[t!]
\linespread{1.25}\selectfont
\caption{Aggregated Knockoffs} %in Iterative Selection}
\label{alg: 2}
\hspace*{\algorithmicindent} \textbf{Input:} Batch data $\mathcal{D}_k$ consisting of  random tuples $\{(\mathbf{S}_i, A_i, \mathbf{Y}_i): 1\le i\le NT/K \}$, %a candidate index set $ B \subset \{1,...,p\}$, 
and a target FDR level $q \in (0, 0.5)$.
\begin{algorithmic}[1]
\State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
Split $\mathcal{D}_k$ into $\{\mathcal{D}_k^{(a)},a\in [m]\}$ where $\mathcal{D}_k^{(a)} = \{(\mathbf{S}_i, \mathbf{Y}_i): A_i=a\}$. %based on their action values $\mathbf{A}^{(a)} \in \mathcal{A}$.
\strut}
\State For each $a$, %use an appropriate method on $\mathcal{D}_k^{(a)}$ to 
construct knockoff variables $\{\widetilde{\mathbf{S}}_i:A_i=a\}$.

\State Apply any machine learning algorithms (e.g., LASSO or random forest) to $\widetilde{\mathcal{D}}_k^{(a)}=\{(\mathbf{S}_i, \widetilde{\mathbf{S}}_i, \mathbf{Y}_i): A_i=a\}$ to construct feature importance statistics $Z_j^{(a)}$ and $\widetilde{Z}_j^{(a)}$ for $S_j$ and its knockoff variable, respectively, for each $j\in [p]$.


\State For each $j \in B$, set $Z_j=\max_{a} Z_j^{(a)}$, $\widetilde{Z}_j=\max_{a} \widetilde{Z}_j^{(a)}$ and $W_j=f(Z_j, \widetilde{Z}_j)$.
%aggregate all $W_j^{a}$ across all action values and compute feature statistics $W_j$ that satisfies the flip-coin property.

\State To implement the (standard) knockoffs method, set the threshold $\tau$ to  $$\min\left\{t>0: \frac{\displaystyle\#\{i\in [p]: W_i \leq -t\}}{\displaystyle\#\{i\in [p]: W_i \geq t\}} \leq q\right\},$$ \textbf{Output} $\widehat{G}_k = \{i\in [p]: W_i \geq \tau\}$.
\State To implement the knockoffs+ method, set the threshold $\tau_+$ to $$\min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in [p]: W_i \leq -t\}}{\displaystyle \#\{i\in [p]: W_i \geq t\}} \leq q\right\},$$ 
\textbf{Output} $\widehat{G}_k = \{i\in [p]: W_i \geq \tau_+\}$.
\end{algorithmic} 
\end{algorithm}

% \begin{algorithm}[t!]
% \linespread{1.25}\selectfont
% \caption{Aggregated Knockoffs} %in Iterative Selection}
% \label{alg: 2}
% \hspace*{\algorithmicindent} \textbf{Input:} Batch data $\mathcal{D}_k$ consisting of  random tuples $\{(\mathbf{S}_i, A_i, \mathbf{Y}_i): 1\le i\le NT/K \}$, a candidate index set $ B \subset \{1,...,p\}$, and a target FDR level $q \in (0, 0.5)$
% \begin{algorithmic}[1]
% \State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
% Split $\mathcal{D}_k$ into $\{\mathcal{D}_k^{(a)},a\in [m]\}$ where $\mathcal{D}_k^{(a)} = \{(\mathbf{S}_i, A_i, \mathbf{Y}_i): A_i=a\}$. %based on their action values $\mathbf{A}^{(a)} \in \mathcal{A}$.
% \strut}
% \State For each $a$, use an appropriate method on $\mathcal{D}_k^{(a)}$ to construct knockoff variables $\{\widetilde{\mathbf{S}}_i:A_i=a\}$.

% \State Apply any machine learning algorithms (e.g., LASSO and random forest) to $\{(\mathbf{S}_i, \widetilde{\mathbf{S}}_i, A_i, \mathbf{Y}_i): A_i=a\}$ to construct feature importance statistics $Z_j^{(a)}$ and $\widetilde{Z}_j^{(a)}$ for $S_j$ and its knockoff variable, respectively, for each $j\in B$.


% \State For each $j \in B$, set $Z_j=\max_{a} Z_j^{(a)}$, $\widetilde{Z}_j=\max_{a} \widetilde{Z}_j^{(a)}$ and $W_j=f(Z_j, \widetilde{Z}_j)$.
% %aggregate all $W_j^{a}$ across all action values and compute feature statistics $W_j$ that satisfies the flip-coin property.

% \State To implement the (standard) knockoffs method, set the threshold $\tau$ to  $$\min\left\{t>0: \frac{\displaystyle\#\{i\in B: W_i \leq -t\}}{\displaystyle\#\{i\in B: W_i \geq t\}} \leq q\right\},$$ \textbf{Output} $\widehat{G}_k = \{i\in B: W_i \geq \tau\}$
% \State To implement the knockoffs+ method, set the threshold $\tau_+$ to $$\min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in B: W_i \leq -t\}}{\displaystyle \#\{i\in B: W_i \geq t\}} \leq q\right\},$$ 
% \textbf{Output} $\widehat{G}_k = \{i\in B: W_i \geq \tau_+\}$
% \end{algorithmic} 
% \end{algorithm}

%\vspace{-0.3cm}
\subsection{Determining the number of splits}
\label{sec: bestK}
%\vspace{-0.3cm}
%\textbf{Motivation.}
%This whole subsection is about choosing a good $K$ before implementing SEEK proposed above, and only for practical purposes. 
The performance of the proposed algorithm relies on the number of sub-datasets $K$ in the data splitting step. In this section, we present a practical algorithm to adaptively select $K$. 
%As in Theorem \ref{thm1}, the FDR would be controlled by $q+O\{K^{-1}(NT)^{-c}\}$, where the second term is a negligible extra pay compared to target FDR $q$, and vanished as $NT\rightarrow\infty$. According to 
A closer look at the proof for Theorem \ref{thm1} reveals that for a given $K$, %without assuming the $\beta$-mixing decay is exponential, such FDR control takes the form  
the FDR is bounded above by
$q + (NT/K)\beta(K),$ where $\beta(K)$ denotes the $\beta$-mixing coefficient, measuring the temporal dependence of the MDP  %between two %subsequences of the 
%processes with 
at lag $K$. See Definition \ref{def: beta_mix} for details. Thus, the choice of $K$ represents the following trade-off. On the one hand, when the system is $\beta$-mixing, we have $\lim_{K\to\infty} \beta(K)=0$. In theory, $K$ should diverge to infinity to avoid inflation of type-I error. On the other hand, $K$ should not be too large in order to guarantee that each data subset $\mathcal{D}_k$ has sufficiently many observations. To balance this trade-off, we propose to set $K$ to $K^*=\argmin \{k\ge 1: k^{-1} (NT)\beta(k)\le  \delta\}$ and estimate it via
%Then on the practical  side, when we fix a target FDR level $q$ with a given offline dataset, we want to choose an appropriate value for $K$, the number of splits, so that such extra pay is small enough.
% To be exact, let's take a look at the complete form of the upper bound for FDR control, which is, according to the proof for Theorem \ref{thm1},
% \begin{align*}
%     q + \frac{NT}{K}\beta(K)
% \end{align*}
% where $\beta(K)$ is the $\beta$-mixing coefficient, dependent on $K$, measuring the dependence between two subsequences of the process with such lag, due to Definition \ref{def: beta_mix}. On the other hand, for the power analysis, we also see from corresponding proof details that they are all negatively associated with $K$. Then to control the such term, given $NT$ fixed due to fixed data and function $\beta$ fixed given the same process, we could only modify $K$, the number of splits. 
\begin{comment}
\textbf{Balance in the value of $K$.}
For a $\beta$-mixing process, the usual trend is that as $K$ increases, $\beta(K)$ decreases. Given fixed $NT$, $[NT/K]\beta(K)$ is decreasing in $K$. However, we cannot make $K$ arbitrarily large. The concern lies in the model-based selection  for each data subset $\mathcal{D}_k$. As introduced in Conditions \ref{ass: ols}, \ref{ass: lasso} and \ref{ass: sep}, no matter which method, we need an acceptable estimation performance, so that variable importance can help distinguish between true and null variables. Then the sample size for each $\mathcal{D}_k$, $(NT)/K$, should be large enough to guarantee such performance. That concern requires $K$ not to be too large. In a word, we need a moderate value for $K$ according to given data, so as to balance the need for a large sample size of each $\mathcal{D}_k$ and the goal to limit the extra pay $(NT/K)\beta(K)$.
\end{comment}
%\textbf{Criterion of choice.}
%Combining all the above needs, we want $K=K^*$ such that
\begin{align}\label{k_critn}
    \widehat{K} = \argmin \left\{k\ge 1: \frac{NT}{k}\widehat{\beta}(k) \leq \delta \right\},
\end{align}
where $\widehat{\beta}(\bullet)$ denotes the estimated mixing coefficient and $\delta$ denotes a pre-specified upper bound for the inflation of the type-I error (e.g., 0.01 or 0.05). %error bound for extra pay. 
%In such a way the FDR control would be $q+\delta$. As a summary of the procedure, we estimate $\beta(K)$ as a function of different values of $K$, then solve $K^*$ from equation \ref{k_critn}. Finally, we use $K^*$ as the number of splits and implement the whole procedure of SEEK.

It remains to estimate the $\beta$-mixing coefficient. Existing solutions in the time series literature are based on histograms
 \citep{mcdonald2015estimating} and thus suffer from the curse of dimensionality and perform poorly in problems with moderate dimension. Furthermore, it is
 not trivial to extend these methods to produce an accurate estimator 
 of $K^*$ without imposing additional structure on the problem.    
  To see this, suppose that $\beta(k)$ decays exponentially fast with respect to the lag $k$. Then $\beta(K^*)$ is typically of the order 
%in order to ensure the inflated error is upper bounded by $\delta$, $\beta(\widehat{K})$ shall be of the order 
$\delta (NT)^{-1}$ up to some logarithmic factor. Nonetheless, without additional assumptions, its estimation error is at least of the order $(N T)^{-1/2}\gg \delta (NT)^{-1}$. 
%it is impossible to consistently estimate $\beta(K)$ for $K$ close to or larger than $T$ without additional assumptions. 
To address both challenges, we develop a novel three-step algorithm detailed below. %Our algorithm consists of two steps, including the use of a generic density estimation method  
%We focus on the setting where $\beta(K)$ decays exponentially fast with respect to $K$, i.e., $\beta(K)=a_0\exp(-b_0K)$ for some $a_0,b_0>0$. Meanwhile, our algorithm can be adapted to handle settings where $\beta(K)$ decays at a polynomial order with respect to $K$. In the following, we break our algorithm into 

%\textbf{Detailed estimation procedure of $K^*$ under the exponential case.} Now we take the case when the process is exponentially $\beta$-mixing as an example to show the detailed procedure. Then, as we normally assumed according to Condition \ref{ass: mixing}, the $\beta$-mixing takes some exponential form, i.e. for some constant $\rho$,
%$\beta(K) = O\{\rho^K\}$.


\textbf{Step 1. } %Primary estimation of mixing coefficients.} 
Construct initial estimators of $\beta(1)$, $\cdots$, $\beta(K_0)$ for a specified integer value of $K_0$ using a generic density estimator. Assuming both the behavior policy $\pi_b$ and the MDP are stationary, it follows that 
\begin{eqnarray}\label{eqn:betak}
\begin{split}
    \beta(k)
    &=\frac{1}{2}\sum_{a,a'} \int_{s,s'} |f(s) \pi_b (a|s)f(s')\pi_b(a'|s')-f_k(s,s')\pi_b'(a|s,s')\pi_b(a'|s')|\\
    &=\frac{1}{2}\sum_{a} \int_{s,s'} |f(s) \pi_b (a|s)f(s')-f_k(s,s')\pi_b' (a|s,s')|,
\end{split} 
\end{eqnarray}
where $f$ denotes the marginal state density function, $f_k$ denotes the joint distribution function of $(\mathbf{S}_t, \mathbf{S}_{t+k})$, and $\pi_b'$ is the conditional probability of the action given both the current state and the state after lag $k$. Thus, we first estimate $f$, $\pi_b$, $\pi_b'$, and $f_k$, and then plug in these estimators to estimate $\beta(k)$. In practice, we use a 
Gaussian mixture model to approximate $f$ based on tuples $(\mathbf{S}_t)$, 
and similarly to approximate $f_k$ based on tuples $(\mathbf{S}_t,\mathbf{S}_{t+k})$. The behavior policy $\pi_b$ and the conditional probability $\pi_b'$ can be estimated via supervised learning (e.g., multinomial logistic regression) using tuples $(\mathbf{S}_t,A_t)$ and tuples $(\mathbf{S}_t,A_t, \mathbf{S}_{t+k})$. %through logistic regression if for a binary action space and random forest or deep neural network (DNN) if for a general discrete space. 
In addition, we apply importance sampling to numerically calculate the integral in \eqref{eqn:betak}; details are in Section \ref{apdx: k_select}. Let $\widetilde{\beta}(1)$, $\cdots$, $\widetilde{\beta}(K_0)$ denote resulting estimators. 

%To implement the criterion of choice defined above, we first compute estimates of $\beta$-mixing coefficients with respect to different $K$ up to some limit, say $\tilde{\beta}_1, \tilde{\beta}_2,...,\tilde{\beta}_m$, where our method is inspired by \citet{mcdonald2015estimating}. According to their work, for a Markov process $(\mathbf{S}_t, A_t)$, its $\beta$-mixing coefficient with lag $K$ can be consistently estimated by $\tilde{\beta}_K = \frac{1}{2}\int |\hat{f}_K^{(2)} - [\hat{f}^{(1)}]^2|$, where $\hat{f}^{(1)}$ is the estimator of the density function of $(\mathbf{S}_t, A_t)$, while $\hat{f}_K^{(2)}$ is the density estimator of the pair of observations with lag $K$, i.e. $(\mathbf{S}_t, A_t, \mathbf{S}_{t+K}, A_{t+K})$. Although they focus on using histograms for density estimations, other methods may have better efficiency and could also be considered, say Gaussian mixture models. In addition, we also use importance sampling, so that the variances of mixing coefficient estimates are well-bounded. See  Appendix \ref{apdx: k_select} for details.



\textbf{Step 2. } %Improved estimation of mixing coefficients.} 
The second step is to impose parametric structure on the mixing coefficients to refine the initial estimators and to estimate $\beta(k)$ for $k\ge K_0$. We assume $\beta(k)=a_0 \exp(-b_0 k)$ for some $a_0,b_0>0$. However, our proposal also applies
to settings where $\beta(k)$ decays polynomially fast in $k$. To estimate these model parameters, we assume the first-step estimators satisfy
\begin{eqnarray}\label{eqn:initialestmodel}
    \widetilde{\beta}(k)=\eta_0 + a_0\exp(-b_0k) + \epsilon_k,
\end{eqnarray}
for some $\eta_0>0$ and mean-zero random errors $\{\epsilon_k\}_{k=1}^{K_0}$. Here, $\eta_0$ and $\epsilon_k$ measure the bias and variance of the initial estimator. We use least squares to construct estimators $\widehat{\eta}_0$, $\widehat{a}_0$, $\widehat{b}_0$ and subsequently the final estimator $\widehat{\beta}(k)=\widehat{a}_0\exp(\widehat{b}_0 k)$. It is worth mentioning that we remove the bias term when constructing the final estimator to ensure that $\widehat{\beta}(k)$ indeed decays exponentially fast with respect to $k$. Nonetheless, in our numerical experiments, we find that the inclusion of $\eta_0$ in \eqref{eqn:initialestmodel} is essential to ensure the consistency of $\widehat{\beta}(k)$ due to the finite sample bias of $\widetilde{\beta}(k)$. %We also remark that we assume the bias term is constant as a function of $k$. More generally, one may impose another parametric model to characterize the bias. 

\textbf{Step 3}. We set $\widehat{K}$ to the smallest integer $k$ such that $\widehat{\beta}(k)$ is no larger than $\delta(NT)^{-1} k$. We find that the choice of $\widehat{K}$ is not overly sensitive to the value of $K_0$ used in the first step. 

\begin{comment}
Remark here that similar to what is used in the FDR proof, it's a common way to use $a\exp(-bK)$ as the equivalent form of $O\{\rho^K\}$.
However, the trivial estimates $\{\tilde{\beta}_k\}_1^K$ obtained from the first step still may not be close enough to true values $\beta_1, \beta_2,...,\beta_m$, due to either random error ($\epsilon_K$) or some consistent bias ($\eta_0$), with the form
\begin{align}\label{beta_model}
    \tilde{\beta}_K = \eta_0 + a_0\exp(-b_0K) + \epsilon_K
\end{align}
Then we should not directly use $\tilde{\beta}_K$'s estimated from the first step into equation \ref{k_critn}. Instead, we still need to estimate $\eta_0, a_0, b_0$.
As a result, we treat those $\tilde{\beta}_K$'s as input data, and fit the corresponding regression model with respect to the least squared estimation to find the estimates for $\eta_0, a_0, b_0$, leading to a smooth estimate of the coefficient function, i.e. $\hat{\beta}(K) = a_0 \exp(-b_0 K)$, the function of which would then be used in equation \ref{k_critn}.

%\textbf{Step 3. Finalize the number of splits.} 
%Plugging the function $\hat{\beta}(K)$, into equation \ref{k_critn}, we would find $K^*$. Then we go back to implement the whole SEEK algorithm. For convenience, this procedure is referred to as the best $K$ selection in this paper.
\end{comment}

%\section{Experiments}


%\subsection{Sequential Knockoffs in the General Case}
\vspace{-0.5cm}
\section{Theoretical Results}
\label{sec: theory}
We first introduce two key assumptions on the system dynamics and $W$-statistics (defined in Step 4 of Algorithm \ref{alg: 2})
%\textcolor{red}{defined before or not?} 
that are needed to establish our theoretical results. We next study the FDR and the power of SEEK. 
\vspace{-0.3cm}
\subsection{Technical Conditions}
%\vspace{-0.3cm}
%\subsection{Assumptions}
%\label{sec: 4.1}
%On the theoretical side, we show the control of FDR as well as the power of SEEK. We first introduce two key assumptions on the system dynamics and $W$-statistics that are needed to establish our theoretical results on FDR control. Then in the next part, we will provide mild conditions, with which the power can be proved to be asymptotically 1. The set of proof techniques we introduce forms a standard machine in proving FDR and power of any specific sequential selection approach, which can accommodate the theoretical need for the sequential version of other methods beyond knockoffs.
%In this subsection, we list all the related assumptions for establishing our theoretical results in Section \ref{sec: 4.2}.
\begin{comment}
\begin{assumption}
[Markov assumption]
\label{asp: 1}
For any $t \geq 1$, $(\mathbf{S}_{t+1}, R_t) \indep (\mathbf{S}_j,A_j)_{0\leq j \leq t-1} \, | \, (\mathbf{S}_t, A_t)$.
\end{assumption}
\end{comment}

%\subsection{FDR Control}

\begin{cond}[Stationarity and exponential $\beta$-mixing]\label{ass: mixing}
\label{asp: 3}
%The batch data are generated by some fixed stationary policy. %$b$ i.e., $\prob(A_t=a|\overline{S}_t)=b(a|S_t)$ for any $t \geq 0$.
%In addition, 
The process $\{(\mathbf{S}_t, A_t, R_t)\}_{t\ge 0}$ is stationary and exponentially $\beta$-mixing, i.e., its $\beta$-mixing coefficient at time lag $k$ is of the order $\rho^k$ for some $0<\rho<1$. %The $\beta$-mixing coefficient at time lag $t$ satisfies that $\beta_k \leq \beta_0\exp(-\beta_1 k)$ for $\beta_0\geq 0$ and $\beta_1 >0$.
\end{cond}

When the behavior policy is stationary, the exponential $\beta$-mixing condition in Condition \ref{ass: mixing} %characterizes the dependency among observations over time, where the dependency between $\{(\mathbf{S}_t, A_t, R_t)\}_{t \leq j}$ and $\{(\mathbf{S}_t, A_t, R_t)\}_{t \geq j+k}$ decays to $0$ at an exponential rate with respect to the time lag $k$. Suppose the behavior policy is Markov. Such a condition 
is equivalent to requiring the Markov chain $\{(\mathbf{S}_t, A_t, R_t)\}_{t\ge 0}$  be geometrically ergodic \citep{bradley2005basic}. Similar conditions are commonly imposed in the literature \citep[see e.g.,][]{luckett2019estimating,chen2022well}.
%for the exact definition of an exponentially $\beta$-mixing sequence.  

%Note that in every step of Algorithm \ref{alg: 2} when implementing Model-X knockoffs, we need to construct statistics $W_j$ for each state variable. In order to have a valid FDR control on our proposed algorithm, we specify one essential property that these statistics $W_j$ should satisfy in the following.

We next introduce the flip-sign property which allows the algorithm to achieve FDR control. Specifically, we say the $W$-statistics $\mathbf{W} = \left(W_1,\ldots ,W_p\right)^\top$ satisfies the flip-sign property on the augmented data matrix $\widetilde{\mathcal{D}}_k=[\mathbf{s}_k,\tilde{\mathbf{s}}_k,\mathbf{a}_k,\mathbf{y}_k]$ if for any $i\in[p]$ and $B\subset[p]$,
\begin{align*}
W_i ([\mathbf{s}_k,\tilde{\mathbf{s}}_k]_{\text{swap}(B)},\mathbf{a}_k,\mathbf{y}_k) = W_i ([\mathbf{s}_k,\tilde{\mathbf{s}}_k],\mathbf{a}_k,\mathbf{y}_k) \cdot
\begin{cases}
-1, & \text{if}\ i\in B \\
+1, & \text{otherwise}
\end{cases}
\end{align*}
where $\mathbf{s}_k,\tilde{\mathbf{s}}_k\in\mathbb{R}^{(NT/K)\times p}$ denote the matrices of the states and their knockoffs, $\mathbf{a}_k\in\mathbb{R}^{NT/K}$ denote the action vector, $\mathbf{y}_k\in\mathbb{R}^{(NT/K)\times d_0}$ denotes the response matrix with $d_0$ being the number of variables in the response, and $[\mathbf{s}_k,\tilde{\mathbf{s}}_k]_{\text{swap}(B)}$ is obtained by swapping all $j$-th columns in $\mathbf{s}_k,\tilde{\mathbf{s}}_k$ for $j\in B$. 
%for all significant state variables and $\mathbb{P}(\epsilon_j =1)=1/2$ %\textrm{Binomial}(\{1, -1\}, 1/2)$ independently for all null state variables. 

\begin{cond}[Flip-sign property]\label{ass: flipcoin}
All $W$-statistics used in Algorithm \ref{alg: 2} satisfy the flip-sign property on all $\widetilde{\mathcal{D}}_k,k\in[K]$. 
\end{cond}

%\QZL{Some remarks}

The flip-sign property is commonly imposed in the literature on knockoffs \citep[see e.g.,][]{barber2015controlling,candes2018panning,huang2020relaxing}. It is automatically satisfied in our implementation (see Section \ref{apdx: preliminary} of the Supplementary Material for details). 
%\textcolor{red}{If it is automatically satisfied, do we still need this condition in our theorem?}
%was proven to hold for knockoffs satisfying the two assumptions in Section \ref{sec: 2} \citep{candes2018panning}.
%\textbf{Remark 1}. Proposition 1 here serves as the theoretical criterion, and as long as specific choices by readers in sampling knockoffs and calculating feature statistics could satisfy this, the theorem below would hold.  It should be emphasized that we do not require Proposition 1 to hold in data-dependent case, instead, as far as it holds on i.i.d. data, the Algorithm \ref{alg: 1} would be powerful enough to control FDR/mFDR on dependent data from MDPs.


%\textbf{Remark 2}. Also notice that it's not hard to satisfy, and in the Appendix we provide several feasible choices. In addition, a detailed cookbook example on Gaussian random design is given, in which specific choices are fixed, a group of theoretical properties depending on those choices are proved step by step, and Proposition 1 is shown to be satisfied. Readers are encouraged to follow such standard machine to justify their own choices.
\subsection{FDR Control}
\label{sec: 5}
%\subsection{Main Results}
%Let $\mathcal{H}_0$ be the index set of the minimal sufficient state. In the following, we first show that each selected $\widehat{G}_k$ from the data subset $\calD_k$ has a valid FDR control for $k = 1, 2, ..., K$ with $K = ?$. Note that all the transition tuples in $\calD_k$ are not independent and thus theoretical results from \cite{candes2018panning} cannot be applied here.
We next show that the knockoff algorithm in Algorithm \ref{alg: 2} achieves a valid FDR control. Notice that all the transition tuples in $\calD_k$ are not independent and thus theoretical results from \cite{candes2018panning} cannot be directly applied here.
\begin{theorem}\label{thm1}
Suppose Conditions \ref{ass: mixing} and \ref{ass: flipcoin} hold. %and all knockoff statistics used in Algorithm \ref{alg: 1} satisfy the flip-coin property, 
Set the number of sample splits $K=k_0 \log(NT)$ for some $k_0>-\log^{-1} \rho$ where $\rho$ is defined in Condition \ref{ass: mixing}.
Then %for each $1 \leq k \leq K$, 
$\widehat{G}_k$ obtained by Algorithm \ref{alg: 2} with standard knockoffs controls the modified FDR (mFDR), i.e., 
\begin{align*}
%\textbf{E}\left[\frac{\displaystyle|\widehat{G}_k\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_k| + 1/q}\right] \leq q 
\textrm{mFDR}\leq q + O\left\lbrace K^{-1}(NT)^{-c}\right\rbrace,
\end{align*}
where the constant $c=-k_0\log(\rho)-1>0$. 
In addition, $\widehat{G}_k$ obtained by implementing the knockoffs+ in Algorithm \ref{alg: 2} controls the usual FDR, i.e., 
\begin{align*}
%\textbf{E}\left[\frac{\displaystyle|\widehat{G}_k\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_k| \vee 1}\right] \leq q 
\textrm{FDR}\leq q+ O\left\lbrace K^{-1}(NT)^{-c}\right\rbrace.
\end{align*}
%for every $1 \leq k \leq K$.
\end{theorem}
%Here $c>1$ is determined by $c=k_0\log(1/\rho)-1$, where $k_0, \rho$ are the constants in the definition of exponential $\beta$-mixing in \ref{apdx: 2}. And
%when either $N$ or $T$ increases to infinity, exact control by $q$ is attained.
%We present the detailed definitions of mFDR and FDR in the Appendix. 
The upper error bounds in Theorem \ref{thm1} can be decomposed into the sum of the target FDR level and a negligible term. The presence of the second term is due to the temporal dependence among transition tuples, which diverges to zero as the number of observations in $\mathcal{D}_k$ grows to infinity. %It decreases as the number of sample splits $K$ increases.
Let $p_0$ denote the dimension of the minimum sufficient state.  %is selection consistent.  
%\textbf{Proof sketch}. The difficulty lies in the dependency among data points. To tackle that, we first construct a new sequence of independent random variables such that they correspondingly have the same marginal distributions as row vectors in data $\mathcal{D}$ with high probability. Then if the equivalence in distribution holds, the exact control on FDR is expected. If not, additional cost is necessary as the extra and unavoidable pay due to dependency. The sum of the above leads to the conclusion. Formal statements are provided in \ref{apdx: 2}.


\begin{theorem}\label{thm2}
Suppose conditions in Theorem \ref{thm1} hold and $p p_0 (NT)^{-c}\to 0$. Suppose $q$ decays to zero as $NT\to\infty$ and satisfies $Kp p_0 q\to 0$. Then as $NT\to \infty$, SEEK in Algorithm \ref{alg: 1} with knockoffs+ will not select any null variable with probability approaching $1$.
%\textcolor{red}{Can we use math instead of plain language?} 
\end{theorem}

The preceding result shows that when the target FDR level decays to zero, the sequential algorithm in Algorithm \ref{alg: 1} will not select any null variable asymptotically.

%with probability at least $1-$
%In addition to assume all conditions in Theorem \ref{thm1}, we specify that the targeted FDR control $q=q(NT)<1$ is a function of $NT$, such that $\lim_{NT\rightarrow\infty} Kp(1+p_1)q=0$, where $p_1$ is the number of useful variables in $[p]$, i.e. $p_1=|[p]\backslash\mathcal{H}_0|$, and recall $K=k_0\log(NT)$. Then the output $\widehat{G}$ by Algorithm \ref{alg: 2} using the knockoffs+ satisfies that
\begin{comment}
\begin{align*}
\textbf{E}\left[\frac{\displaystyle|\widehat{G}\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}| \vee 1}\right]
\leq Kp (1+p_1)(q + O((NT)^{-c})).
\end{align*}
and when $N$ or $T$ increases to infinity, FDR will vanish.
\end{comment}
%\vspace{-0.5cm}
\subsection{Power Analysis}\label{sec:poweranalysis}
%\vspace{-0.3cm}
In this section, we show that the probability that $\widehat{G}$ contains the indices of the minimal sufficient state (e.g., the power) approaches one. %\textcolor{red}{Do we need to formally define power if this is a different notion from standard hypothesis testing?} 
The analysis is dependent upon the machine learning algorithm used to construct the feature importance statistics (see Algorithm \ref{alg: 2} for details). 
%In this subsection we derive the power of SEEK. Recall that we have a large range of flexibility in choosing the modeling method so as to gain some variable importance for each state variable. 
%Here, we provide the power analysis when using ordinary least square (OLS) and LASSO respectively, after which we also suggest a consistent approach to calculating variable importance with any general machine learning method, like random forest or deep neural network (DNN), and show the power accordingly. 
We first provide the power analysis based on the LASSO. We then provide sufficient conditions to ensure that the power approaches one when a generic machine learning algorithm is used. 
Throughout the analysis, we assume $p_0$ is fixed while allowing $p$ to grow exponentially fast with respect to the sample size in Section \ref{sec:LASSO}. 
%The whole section is to show that the proposed SEEK not only controls FDR, but with mild conditions, also achieves high power, asymptotically to 1.

%It should also be emphasized that for OLS, the sample size diverges, while state variables and true ones have fixed numbers $p$ and $p_0$. On the other hand, for LASSO or general ML method cases, both $p,p_0$ diverge as $NT\rightarrow \infty$. In addition, when not specified, SEEK means for both SEEK with knockoffs and SEEK with knockoffs+.
%\vspace{-0.6cm}

% \subsubsection{Ordinary Least Squares}
% \vspace{-0.3cm}
% Given the underlying true model is linear, we denote the true coefficient vector by $\beta\in\mathbb{R}^{2p}$, where the entries for the first $p$ are just the underlying values for the $p$ state variables, while $\beta_j=0$ for all $p+1\leq j \leq 2p$, since they correspond to the knockoff variables manually constructed. We also use $\hat{\beta}$ to denote the one calculated by OLS from data. The same notations are also used in LASSO. For notation convenience, we sometimes use $n=NT/K$ to denote the sample size of each data subset $\mathcal{D}_k, k\in[K]$.
% \begin{cond} [OLS estimation bound]\label{ass: ols} 
% By OLS estimation, exist constants $c_1,c_2, C_1>0$, s.t. for $n$ large, with high probability of at least $1-c_1n^{-c_2}$, 
% \begin{align*}
%     C_1\log(n)n^{-1/2} &\geq ||\hat{\beta}^{(a)} - \beta^{(a)}||_1 ~ \text{for}~ \forall a\in\mathcal{A}.
% \end{align*}

% \end{cond}

% \begin{remark}
%     Above Condition \ref{ass: ols} is usually observed in practice, which could be shown from the commonly used mild conditions, detailed in Appendix section \ref{remark_thy}.
% \end{remark}

% \begin{cond} [Enough signals] \label{ass: signal_ols}
%    Exists constant $C_2>0$, s.t. $|\{j\in[p]: \beta_j^{(a)} > 0 ~ \text{for some } a\in\mathcal{A} \}| \geq 1/q+C_2$.
% \end{cond}

% \begin{remark}
%     The above conditions are actually imposed in each data subset $k\in[K]$, and only for those iteration $l\in[p]$, when before the $l$-th selection, there are still true variables, with respect to the current response variables (defined in Algorithm \ref{alg: 2}), left out. Without loss of generality, we omit the superscripts $(k,l)$ for all related variables and constants.  The same idea will be adopted in condition statements in LASSO and general ML cases. It should be emphasized that all final theoretical results are for the whole procedure of SEEK, aggregating all iterations and data subsets. See Appendix \ref{remark_thy} for further explanation.
% \end{remark}

% % \noindent
% % Due to the performance in bounding the OLS estimation errors, true state variables tend to have large $|W_j|$'s, which leads to the following theorem. 
% \begin{theorem} \label{thm: power_ols}
%     With Conditions \ref{ass: ols}, \ref{ass: signal_ols},  the power of SEEK using ordinary least squares is lower bounded with
%     \begin{align*}
%         \textbf{Power} := \mathbf{E}\left(\frac{|\hat{G}\cap G|}{|G|}\right) \geq 1 - O\{n^{-c_2}  + K^{-1}(NT)^{-c}\}
%     \end{align*}
%     which guarantees that the asymptotic power is  1.
% \end{theorem}

% % \begin{remark}
% %     In this result we allow significant flexibility in the choice of target FDR $q$. If $q$ is globally fixed throughout the implementation of SEEK, independent of the increase of $n$, then the convergence requirement, $n^{-c_2}q\rightarrow 0$, is automatically satisfied. What is more interesting is that $q$ could even slowly increase, as long as its speed is dominated by $n^{c_2}$ and $K(NT)^c$, but then it provides high flexibility in procedure design.
% % \end{remark}


% \vspace{-0.6cm}

\subsubsection{LASSO}\label{sec:LASSO}
%\vspace{-0.3cm}
To analyze LASSO, we consider a linear system where the reward and next state satisfy the following model:
\begin{eqnarray*}
    [\mathbf{S}_{t+1}^\top,~R_t]
    =\sum_{a\in\mathcal{A}} \mathbb{I}(A_t=a) \mathbf{S}_t^\top \bm{B}^{(a)} 
 + \bm{\varepsilon}_t,
\end{eqnarray*}
for some $p \times (p+1)$ matrices $\{\bm{B}^{(a)} \}_{a\in\mathcal{A}}$, where the zero-mean error vector $\bm{\varepsilon}_t$ is independent of the state-action pair. 

The indices of the minimal sufficient state $G_{\textrm{M}}$ can be identified as follows. %Let $S_0=\{i\in [p]: B_{a,p+1,i}\neq 0~\textrm{for~some}~a\}$ denote the support of the reward and $S_j=\{i\in [p]: B_{a,j,i}\neq 0~\textrm{for~some}~a\}$ denote the support of the $j$th state. We begin with $G_1=S_0$. Next, for $\ell \ge 2$, we iteratively define $G_{\ell}$ as $(\cup_{j \in G_{\ell-1}} S_j) \cup G_{\ell-1}$ and set $G_{\textrm{M}}=G_{\ell}$ whenever $G_{\ell}=G_{\ell-1}$. 
%the index set 
We begin with $G_1=\{j\in [p]: B_{j,p+1}^{(a)}\neq 0~\textrm{for~some}~a\}$. Next, for any $\ell\ge 2$, we iteratively define 
$G_{\ell}=\{j\in [p]: B_{j,i}^{(a)}\neq 0~\textrm{for~some}~a~\textrm{and}~i\in G_{\ell-1}\}\cup G_{\ell-1}$ and set $G_{\textrm{M}}=G_{\ell}$ whenever $G_{\ell}=G_{\ell-1}$. These coefficients matrices $\{\bm{B}^{(a)} \}_{a\in\mathcal{A}}$ define a directed graph. In particular, consider an augmented $(p+1)\times (p+1)$ coefficient matrix $\bm{B}$ such that $B_{j,i}=\max_a |B_{j,i}^{(a)}|$ for any $1\le j\le p,1\le i\le p+1$ and $B_{p+1,i}=0$ for any $1\le i\le p+1$, and a directed graph $\mathcal{G}(\bm{B})$ whose weight adjacency matrix is given by $\bm{B}$. By definition, there exists a directed edge from the $j$th node to the $i$th node if and only if $\max_a |B_{j,i}^{(a)}|\neq 0$. It is straightforward to show that $j\in G_{\textrm{M}}$ if and only if there exists a directed path from the $j$th node (i.e., the $j$th state variable) to the $(p+1)$th node (i.e., the reward). 
% \textcolor{red}{The direction is strange. It looks like reward towards state variables but indeed should be state variables towards rewards.}
%. It follows from Proposition \ref{prop4} that $G_{\textrm{M}}=\cup_{j\ge 1} G_j$. 


To estimate $\bm{B}^{(a)}$, for each $k$, we apply node-wise LASSO to the augmented data subset $\widetilde{\mathcal{D}}_k^{(a)}$ (see Step 3 of Algorithm \ref{alg: 2}) with both the observed states as well as their knockoff variables that serve as the ``predictors'', and the rewards or the next states that serve as the ``response'' (i.e., LASSO for each response variable). In particular, we use $\widehat{\bm{B}}^{(a,k)}_{p+1}\in \mathbb{R}^{2p}$ to denote the regression coefficient vector when the reward serves as the response and $\widehat{\bm{B}}^{(a,k)}_{i}\in \mathbb{R}^{2p}$ to denote the vector when the $i$th next state variable serves as the response for $1\le i\le p$. Concatenating these vectors together yields the estimated coefficient matrix $\widehat{\bm{B}}^{(a,k)}\in \mathbb{R}^{2p\times (p+1)}$.
Let $\bm{B}^{(a,*)}=[(\bm{B}^{(a)})^\top, \bm{O}_{p\times (p+1)}^\top]^\top$ where $\bm{O}_{p\times (p+1)}$ denotes a zero matrix of dimension $p\times (p+1)$. 
%Correspondingly, we have $B_{j,i}=0,p+1\leq j \leq 2p, i\in [p+1]$.
%$\{(\mathbf{S}_t,\widetilde{\mathbf{S}}_t):A_i=a\}$ as the predictors and 
%$\{\mathbf{S}_t,~ \widetilde{\mathbf{S}}_t, \}$
To simplify notation, we use $n=NT/K$ to denote the sample size of each data subset $\mathcal{D}_k$. %With a slight abuse of notation, we also use $\bm{B}^{(a)}$ to denote the  
We impose the following conditions. 

\begin{comment}
Given the underlying true models for both reward and transitions are linear, we can denote the corresponding true coefficient matrix by $\beta\in\mathbb{R}^{2p\times (p+1)}$, and
\begin{align*}
    [\mathbf{S}_{t+1}^\top,~ R_t] = [\mathbf{S}_t^\top,~ \widetilde{\mathbf{S}}_t^\top] \beta,
\end{align*}
where the entries for the first $p$ rows in $\beta$ are just the underlying values for the $p$ state variables, while  $\beta_{ji}=0$ for all $p+1\leq j \leq 2p, i\in[p+1]$, as they correspond to the knockoff variables manually constructed. We use $\hat{\beta}$ to denote the estimates calculated by LASSO from data. The same notations are adopted in OLS. For notation convenience, we sometimes use $n=NT/K$ to denote the sample size of each data subset $\mathcal{D}_k, k\in[K]$. We also use $G(R)=\{j\in[p]: \beta_{j,p+1} \neq 0\}$ and $G(S_i)=\{j\in[p]: \beta_{ji} \neq 0\}, i\in [p]$ to denote the index set for true variables with respect to reward and transition in $S_i$ respectively.




Due to the implementation of knockoffs, the regression is on $2p$-vector $[\mathbf{S}_t,~ \widetilde{\mathbf{S}}_t]$, then the corresponding coefficients matrices $B^{(a)}\in\mathbb{R}^{2p\times (p+1)}$ are also with $2p$ rows (with abuse of notation), where the first $p$ rows are just the same as in the above linear model, while  $B_{ji}=0$ for 
$ p+1\leq j \leq 2p, i\in[p+1]$. And $\widehat{B}^{(a)}\in\mathbb{R}^{2p\times (p+1)}$ is the estimates by LASSO.
\end{comment}
\begin{cond} [Minimal signal strength] \label{ass: recovery_lasso}
%For each $j\in G_{\textrm{M}}$, %and $i\in G_{\textrm{M}}\cup \{p+1\}$, suppose %we have%as $n\to \infty$, %$\max_{i\in G_{\textrm{M}\cup \{p+1\}} }\max_{a\in \mathcal{A}} |B_{j,i}^{(a)}|$
%\begin{eqnarray*}
%    \frac{\max_{i\in G_{\textrm{M}}\cup \{p+1\}}\max_{a\in \mathcal{A}} |B_{j,i}^{(a)}|}{n^{-1/2}\sqrt{\log p}}\to \infty,\,\,\,\,\textrm{as}~n\to \infty.
%\end{eqnarray*} 
There exists a sequence $\kappa_n\to \infty$ as $n\to \infty$ such that the subgraph of $\mathcal{G}(\bm{B})$, obtained by removing edges with weights smaller than $\kappa_n n^{-1/2} \sqrt{\log p}$, still identifies $G_{\textrm{M}}$, i.e., there exists a directed path in this subgraph from each variable in the minimal sufficient state to the reward. 
\begin{comment}
Exists some $\kappa_{n}\rightarrow\infty$ with $n\rightarrow\infty$ for all $i\in G_\textrm{M}\cup \{p+1\}$, s.t. 
\begin{align*}
    \kappa_{n} (\log p)^{1/2} n^{-1/2} \leq\min_{j\in G(S_i)}\max_{a\in\mathcal{A}}|B_{ji}^{(a)}|.
\end{align*}
\end{comment}
\end{cond}

\begin{comment}
\begin{cond} [Strong signals] \label{ass: strong_lasso}
Exists constant  $C_4\in((qp_0)^{-1},1)$ s.t. for all $i\in G_\textrm{M}\cup \{p+1\}$, 
\begin{align*}
    |\{j\in[p]: \max_{a\in\mathcal{A}}|B_{ji}^{(a)}| \gg p_0^{1/2} (\log p)^{1/2} n^{-1/2} \}| \geq C_4p_0.
\end{align*}
\end{cond}

\begin{remark}
    Condition \ref{ass: strong_lasso} plays an important role in the theory, both for intuition and for proof techniques. It not only guarantees a significant selection size, but also controls the total number of iterations. See Appendix \ref{remark_thy} for further discussion.
\end{remark}

\begin{remark}
If $\kappa_{n} \gg p_0^{1/2}$ and $|G|> C_4 
p_0$ for large $n$, Condition \ref{ass: recovery_lasso} implies Condition \ref{ass: strong_lasso}.
\end{remark}
\end{comment}

\begin{cond} [LASSO estimation bound] \label{ass: lasso}
%With probability at least $1-c_3p^{-c_4}$,
There exists some positive constants $c_1, C_1, c_{\lambda}^{(a)}$ such that with probability at least $1-O(p^{-c_1})$, for any $i\in [p+1], a\in\mathcal{A}$ and $k\in [K]$,
\begin{align*}
    % \|\widehat{\mathbf{B}}_i^{(a,k)} - \mathbf{B}_i^{(a,*)}\|_1
    % \leq C_1 p_0\lambda^{(a)}\,\,\hbox{and}\,\,
    \|\widehat{\mathbf{B}}_i^{(a,k)} - \mathbf{B}_i^{(a,*)}\|_2
    \leq C_1 p_0^{1/2}\lambda^{(a)},
\end{align*}
%for all $i\in [p+1]$ and  $a\in\mathcal{A}$, with  $\mathbf{B}_i^{(a)}$ $(\widehat{\mathbf{B}}_i^{(a)})$ the $i$-th column of $\mathbf{B}^{(a)}$ $(\widehat{\mathbf{B}}^{(a)})$, 
where $\lambda^{(a)} = c_{\lambda}^{(a)} \{(\log p)/n\}^{1/2}$ denotes the LASSO regularization parameter. %$\lambda := \max_{a\in\mathcal{A}}\lambda^{(a)} = c_{\lambda} \{(\log p)/n\}^{1/2}$, for constants $c_3, c_4, C_5, C_6, c_{\lambda}^{(a)}>0$.
\end{cond}

\begin{figure}[!t]
\centering
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=0.35\linewidth]{figs/graph_eg1.png}
\caption{2 state variables}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[width=0.35\linewidth]{figs/graph_eg2.png}
\caption{3 state variables}
	\end{subfigure}
\caption{Two examples of dependence graph among reward and true state variables.}
\label{fig: graph_eg}
%\vspace{-0.3cm}
\end{figure}

We make a few remarks. First, Condition \ref{ass: recovery_lasso} is mild. It is much weaker than requiring all the nonzero elements in $\bm{B}_{G_{\textrm{M}}}$, the sub-matrix of $\bm{B}$ formed by its rows in $G_{\textrm{M}}$, to be much larger than $n^{-1/2} \sqrt{\log p}$ (the threshold for strong signals). To elaborate, consider a simple example where the first two variables are the minimal sufficient state, both $S_{t,1}$ and $S_{t,2}$ directly influence $R_t$ whereas $S_{t,2}$ affects $S_{t,1}$ as well. In that case, Condition 3 requires (i) $\max_a |B^{(a)}_{1,p+1}|\gg \sqrt{n^{-1}\log p}$ and (ii) either $\max_a |B^{(a)}_{2,p+1}|\gg \sqrt{n^{-1}\log p}$ or $\max_a |B^{(a)}_{2,1}|\gg \sqrt{n^{-1}\log p}$. Notice that (ii) is much weaker than requiring both $\max_a |B^{(a)}_{2,p+1}|\gg \sqrt{n^{-1}\log p}$ and $\max_a |B^{(a)}_{2,1}|\gg \sqrt{n^{-1}\log p}$. This occurs essentially because there exist two directed paths from the second state variable to the reward, given by $S_{t,2}\to R_t$ and $S_{t,2}\to S_{t,1}\to R_t$, and we only require one of them to appear in the subgraph after removing edges with weak signals. Thus, we refer to this phenomenon as ``the blessings of multiple paths,'' which allows us to impose a weaker minimal signal strength condition. We call edges remaining in the subgraph strong edges and removed ones weak edges. Two examples are given in Figure \ref{fig: graph_eg}, where solid lines denote strong edges, while dashed lines denote weak edges. Further interpretations are given in Lemma \ref{lemma: true_ctn}  of the Supplemental Material and the remarks following it.
%We also remark that in the variable selection literature, the minimal signal strength condition is typically imposed to guarantee the sure screening property, i.e., all significant variables will be selected with probability approaching $1$ \citep[see e.g.,][]{shi2021statistical}.
%the minimal signal strength condition essentially requires 
Second, Condition \ref{ass: lasso} is mild as well. It is automatically satisfied when the matrix $\mathbf{E} [\mathbf{S}_t \mathbf{S}_t^\top \mathbb{I}(A_t=a)]$ possesses the restricted eigenvalue condition and the error $\mathbf{\varepsilon}_t$ has sub-exponential tails \citep[see e.g.,][]{bickel2009simultaneous}. It guarantees all strong signals will be detected via LASSO with probability approaching one. 
%Together with Condition \ref{ass: recovery_lasso}, it guarantees that all significant variables will be 

\begin{comment}
\begin{cond} \label{ass: slow_div}
$(\phi+1)|\mathcal{A}| C_5 c_\lambda  p_0^2 < \min(1, C_3) \kappa_{n}$ for $n$ large and any $i\in G\cup \{p+1\}$.
\end{cond}


\begin{remark}
    Note that given Condition \ref{ass: slow_div}, we immediately know $\kappa_n \gg p_0^{1/2}$, but Condition \ref{ass: strong_lasso} is still necessary, since we also need that total number of true variables is at least $C_4p_0$.
\end{remark}

\begin{remark}
    The above conditions are actually imposed in each data subset $k\in[K]$, and only for those iteration $l\in[p]$, when before the $l$-th selection, there are still true variables, with respect to the current response variables (defined in Algorithm \ref{alg: 2}), left out. Without loss of generality, we omit the superscripts $(k,l)$ for all related variables and constants.  The same idea will be adopted in condition statements in OLS and general ML cases. It should be emphasized that all final theoretical results are for the whole procedure of SEEK, aggregating all iterations and data subsets. See Appendix \ref{remark_thy} for further explanation.
\end{remark}
\end{comment}

% \begin{cond}
%     Target FDR $q$ decays to zero as $NT\rightarrow 0$, with $p^{1-c_2/2} q \rightarrow 0$ and $ K^{-1}(NT)^{-c}pq \rightarrow 0$.
% \end{cond}


% \begin{remark}
% Actually all above are written by omitting some superscripts, and to be exact, we require those to be true for each $i$-th iteration in each data subset $\mathcal{D}_k$. In that sense, solely for the conditions, we actually have constants indicated by superscripts $k,l$, including $\kappa_n^{(k,l)}, \beta_j^{(a),(k,l)}, c^{(k,l)}, c_1^{(k,l)}, c_2^{(k,l)}, c_0^{(k,l)}, C_0^{(k,l)}, \lambda^{(k,l)}, c_\lambda^{(k,l)}, C^{(k,l)}$. Then for results,  we choose $c_1 := \max_{k,l}c_1^{(k,l)}, c_2 := \min_{k,l}c_2^{(k,l)}, c_0 := \max_{k,l}c_0^{(k,l)}, C_0 := \max_{k,l}C_0^{(k,l)}, c_\lambda := \max_{k,l}c_\lambda^{(k,l)}, C := \min_{k,l}C^{(k,l)},  c := \min_{k,l}c^{(k,l)}$, with which Lemma 2, Lemma 3 and final results hold when removing the $(k,l)$ on those terms whose max/min's have been taken.
% \end{remark}

\begin{theorem}\label{thm: power_lasso}
    Assume Conditions  \ref{ass: recovery_lasso} and \ref{ass: lasso} hold. The power of SEEK using LASSO is lower bounded by
    \begin{align*}
        \textbf{Power} := \mathbf{E}\left(\frac{|\widehat{G}\cap G_\textrm{M}|}{|G_\textrm{M}|}\right) 
        \geq 1 - O\{p^{-c_1}\},
    \end{align*}
    which approaches one as $NT\rightarrow \infty$.
    %\textcolor{red}{Does the RHS depend on all relevant parameters?}
\end{theorem}



%\vspace{-0.6cm}
\subsubsection{Generic Machine Learning Methods}
%\vspace{-0.3cm}
%\textbf{Definition of variable importance}
In this section, we focus on the setting where a generic machine learning method is applied to $\widetilde{\mathcal{D}}_k^{(a)}$ to calculate a variable importance (VI) score for each state as well as its knockoff variable. %(denoted by $\{\textrm{VI}_{j}^{(a,k)}:j\in [2p]\}$). 
When LASSO is employed, these VI's correspond to the estimated regression coefficients. When the random forest algorithm is applied, it will simultaneously produce an importance measure for each variable. Alternatively, deep neural networks are applicable to produce these measures as well \citep{lu2018deeppink}. To handle multiple outcomes, we propose to calculate the VI for each individual response variable and obtain the maximum value over all responses. Let $\textrm{VI}^{(a,k)}_{j,i}$ denote the $j$th variable's VI with the $i$th state being the outcome. 
Then we can define the $W$-statistic 
%\begin{align*}
    $W_j^{(k)} = \max_{a\in\mathcal{A}}\max_{i}|\textrm{VI}_{j,i}^{(a,k)}| - \max_{a\in\mathcal{A}}\max_{i}|\textrm{VI}_{j+p,i}^{(a,k)}|$. 
%Similarly 
%Consider the case where 

%For any general machine learning (ML) method (which could be non-parametric, like random forest or DNN), one way to calculate the variable importance is as follows. We first apply such a method to data, with the results of which we compute the residual sum of squares (RSS) between the estimation by the method and original responses. Then to evaluate the importance of any one variable, we kick it out from the learning and redo the ML process, the new RSS of which would be higher. And the difference between the two RSS (with some normalization) would be its variable importance, denoted by $VI$. Such a definition is applicable to most ML methods. For instance, when it comes to random forest, our definition just coincides with the common approach in defining variable importance score. In our scenario, we need to perform any specific method given each fixed action $a\in\mathcal{A}$, for each variable $j\in[2p]$, to have variable importance $VI_j^{(a)}$.
%\noindent
%Given that specification

To characterize the power property, similar to Section \ref{sec:LASSO}, we construct a $(p+1)\times (p+1)$ matrix $\mathbf{V}^{(k)}$ whose $(j,i)$th entry equals $\max_a |\textrm{VI}_{j,i}^{(a,k)}|$ for $j\le p$ and $0$ for $j=p+1$. This generates a directed graph $\mathcal{G}(\mathbf{V}^{(k)})$. 
%For each $k\in[K]$, by replacing the entries in $\mathbf{B}$ with VI's, we can have a new matrix $\mathbf{V}^{(k)}$, where $V_{j,i}^{(k)} = \max_a |VI_{j,i}^{(a,k)}|$ for $j\in[p], i\in[p+1]$ and $V_{p+1,i}^{(k)} = 0, i \in [p+1]$. Then define  directed graphs $\mathcal{G}(\mathbf{V}^{(k)})$, with similar interpretations as in LASSO case. Notice that the graph in LASSO case is by the coefficients in the underlying true model, while here each graph $\mathcal{G}(\mathbf{V}^{(k)})$ is defined by VI computed on a data sample $\mathcal{D}_k$.  
%\end{align*}
%and all following procedures in SEEK are exactly the same.

%\textbf{General conditions and results}
%The most possible condition we could require is some mild separation properties in variable importance, which could possibly be achieved.

\begin{cond} [Separation] \label{ass: sep}
There exists a function $\gamma(n,p)$ and constant $c_2>0$, %s.t. for $n$ large, 
such that with probability of at least $1-O(p^{-c_2})$, and any $k\in[K]$: 
\begin{enumerate}
    \item $\max_{a\in\mathcal{A}}|\textrm{VI}_{j,i}^{(a,k)}| 
    < \gamma(n,p)/2$
    for any $j\in[2p]\backslash G_\textrm{M}$ and any $i\in [p+1]$; 
 %   \begin{align*}
 %   2\max_{a\in\mathcal{A}}|\textrm{VI}_{j,i}^{(a,k)}| 
 %   &< \gamma(n,p) 
 %   \end{align*}
    \item
    the subgraph of $\mathcal{G}(\mathbf{V}^{(k)})$, by removing edges with weights smaller than $\gamma(n,p)$, identifies $G_{\textrm{M}}$.
\end{enumerate}
\end{cond}

%\textcolor{red}{can we provide one or two examples about $\gamma(n, p)$}


% For each $k\in[K]$, by replacing the entries in $\mathbf{B}$ with VI's, we can have a new matrix $\mathbf{V}^{(k)}$, where $V_{j,i}^{(k)} = \max_a |VI_{j,i}^{(a,k)}|$ for $j\in[p], i\in[p+1]$ and $V_{p+1,i}^{(k)} = 0, i \in [p+1]$. Then define  directed graphs $\mathcal{G}(\mathbf{V}^{(k)})$, with similar interpretations as in LASSO case.
\begin{comment}
\begin{cond} 
\label{ass: recovery_ml}
With probability of at least $1-p^{-c_3}$, for any $k\in[K]$, the subgraph of $\mathcal{G}(\mathbf{V}^{(k)})$, by removing edges with weights smaller than $\gamma(n,p)$ in Condition \ref{ass: sep}, is able to identify $G_{\textrm{M}}$. 
\end{cond}
\end{comment}
\begin{remark}
    The separation assumption is %quite 
    mild in %several aspects. First, 
    the sense that we do not require $\textrm{VI}_{j,i}^{(a,k)}$'s to converge to some population-level variable importance measures as the sample size increases. The first part of Condition \ref{ass: sep} essentially upper bounds the importance of the insignificant variables whereas the second part corresponds to the signal strength condition. Similar to Condition \ref{ass: recovery_lasso}, it only requires the existence of one path from the reward to each minimal sufficient state after removing edges with weak signals. 
    %when an explicit true model exists, in the sense that each variable has a true (limiting) coefficient/importance score, then by consistency result, Condition \ref{ass: sep} can be shown. When either consistency is hard to achieve, or no such limiting importance score exists, the power is still achievable, as long as the VI's computed on data samples follow the separation defined in Condition \ref{ass: sep} with high probability. Finally, in different samples, the subgraph defined could even be different, i.e. one variable can have high VI directly towards reward in one dataset while moderate in the second set, as long as it has a path of strong edges (instead of a single strong edge) towards reward. Such flexibility in turn supports the generality of SEEK.
\end{remark}

\begin{comment}
\begin{cond} [Enough signals] \label{ass: signals_ml}
   Exists constant $C_7>0$, s.t. $|\{j\in[p]: \beta_j^{(a)} > 0 ~ \text{for some } a\in\mathcal{A} \}| \geq 1/q+C_7$.
\end{cond}
\end{comment}

\begin{theorem}\label{thm: power_ml}
Suppose Condition \ref{ass: sep} holds. Then the power of SEEK using general ML methods is bounded below by
\begin{align*}
\textbf{Power} 
% := \mathbf{E}\left(\frac{|\hat{G}\cap G_\textrm{M}|}{|G_\textrm{M}|}\right) 
\geq 1 - O\lbrace p^{-c_2} \rbrace,
\end{align*}
%\textcolor{red}{power has been defined before} 
which asymptotically increases to one as $NT\rightarrow \infty$.
\end{theorem}

%To our knowledge, this is the first theorem that establishes the power property of knockoff-type methods based on a generic machine learning algorithm. 
 %on selecting important variables in offline reinforcement learning via extensive simulations and comparisons. 
% The computing infrastructure used is a virtual machine in the AWS Platform with 72 processor cores and
% 144GB memory.
%, and the average running time of our method is less than 1 minute.
%\vspace{-1cm}

%%%Hengrui read until here.

\section{Simulation Experiments}
\label{sec: exp}
%\vspace{-0.3cm}
%We investigate the finite sample performance of SEEK via both simulation experiments in this section and real data analysis in the next one.
%\vspace{-0.6cm}
\subsection{Experiment Design, Benchmarks, and Evaluation Metrics}
%\vspace{-0.3cm}
In this section, we investigate the finite sample performance of SEEK via extensive simulation studies. Specifically, we consider a total of four different environments, including an autoregressive model (denoted AR) where all states follow an AR(1) model detailed in Section \ref{apdx: a} of the Supplemental Material, the one specified in Section \ref{sec: minimal} which includes both autoregressive and i.i.d. states (denoted Mixed) and two environments from OpenAI Gym \citep{brockman2016openai}: `CartPole-v0â€™ (denoted CP) 
and `LunarLander-v2â€™ (denoted LL). 

In the first two environments, the action is binary. The dimensions of the minimal sufficient state $p_0$ and the null state $p-p_0$ are given by 2 and 18, respectively. The horizon is fixed at 150 whereas the number of trajectories $N$ is chosen from $\{10,20,40\}$ for the AR environment and $\{50,100,200\}$ for the Mixed environment. In the last two environments, $p_0$ equals 4 and 8, respectively. In addition, we manually include null variables in the state with $p$ taking values in $\{50, 100, 150, 200\}$ which leads to a challenging high-dimensional state system, and consider both AR(1) and i.i.d. white noises for the null variables. $N$ is chosen from $\{100, 200\}$, where each trajectory contains approximately 130 decision points in `CartPole-v0â€™ and 340 decision points in `LunarLander-v2â€™.
%\textbf{Environment settings} We consider the performance of SEEK in different environments. First, consider an autoregressive (AR) model, where the minimal sufficient state has dimension $p_0 = 2$, and $p-p_0$ null state variables are included with $p=20$, the environment of which is referred to as AR environment. Here the initial state follows standard normal distribution, the action space contains 2 different actions, and reward depends on the first 2 state variables. The transition is such that given each action, the next state follows an AR(1) model of the current state with a standard normal random error. See Appendix \ref{apdx: a} for further details.
%Finally, 

Each batch dataset is generated using $\epsilon$-greedy algorithm with $\epsilon=0.3$ as the behavior policy. The optimal policies in the first two environments have closed-form expressions. In the last two environments, 
CP and LL, 
the optimal policy is trained via a Deep Q-Network \citep{mnih2015human} agent for CP or by a Duelling Double Deep Q-Network \citep{wang2016dueling} agent for 
LL.
%Notice that the corresponding optimal policy can be deduced directly since we know the environment.
%The second environment is just the same one as described in Section \ref{sec: minimal}, which we refer to as Mixed environment.

To implement SEEK, we fix the majority vote threshold $\alpha=0.5$ and the target FDR at level $q=0.1$. In the first two environments, the exponential $\beta$-mixing condition (see Condition \ref{ass: mixing}) is satisfied and we apply our proposal in Section \ref{sec: bestK} to adaptively determine the number of splits, with $\delta$ fixed to $0.01$. In addition, because the system is linear, we apply LASSO in Step 3 of Algorithm \ref{alg: 2} to implement SEEK. However, the last two environments are neither ergodic nor linear. As such, we fix $K$ across different simulation replications, conduct a sensitivity analysis in below to analyse the effect of $K$, and apply random forests (RFs) to implement Algorithm \ref{alg: 2}. Three benchmark methods are included, including two one-step approaches (see Section \ref{sec: minimal}) that conduct variable selection via LASSO and RF, denoted by VS-LASSO and VS-RF, respectively, as well as the sparse feature selection (SFS) method by \citet{hao2021sparse} based on LASSO. For the first two linear environments, we only report the results based on SEEK, VS-LASSO, and SFS whereas, for the last two nonlinear environments, we report results based on SEEK, VS-RF, and SFS. 

The aforementioned methods are evaluated by the following criteria: FDR/mFDR, false positive rate (FPR, the percentage of falsely selected variables), true positive rate (TPR, the percentage of correctly selected variables), and the value of the estimated optimal policy based on the selected state variables using the same batch data. To compute the value, we first apply FQI to the reduced MDP with the selected state to estimate the optimal policy and then apply the Monte Carlo method for policy value evaluation. When implementing FQI, we employ the multilayer perceptron regressor implementation of \citet{pedregosa2011scikit} to perform supervised learning. Information about hyper-parameters is given in Tables \ref{tab:hyper_lasso} and \ref{tab:hyper_rf} of the Supplemental Materials.

\begin{table}
\linespread{1.25}\selectfont
\caption{Performances of SEEK and benchmark methods in the AR environment, aggregated over 100 simulation runs.}
\label{tab: ar_compare}
\centering
\footnotesize
\begin{tabular}{llllllllll}
\toprule
& \multicolumn{3}{c}{SEEK} &  & SFS &  & \multicolumn{3}{c}{VS-LASSO} \\ \hline
\multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
\multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{$K$} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & 29.28 \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 0.60 & 0.60 & 0.60 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.07 & 0.03 & \multicolumn{1}{l|}{0.06} & 0.90 & 0.90 & 0.90 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 1.00 & 1.00 & 1.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & 1.00 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.32 & 0.48 & \multicolumn{1}{l|}{0.87} & 0.35 & 0.44 & 0.51 \\ \hline
\multicolumn{1}{l|}{Value} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.02 & 2.06 & \multicolumn{1}{l|}{2.07} & 1.49 & 1.77 & 1.90 \\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\linespread{1.25}\selectfont
\caption{Performances of SEEK and benchmark methods in the Mixed environment, aggregated over
100 simulation runs.}
\label{tab: mixed_compare}
\centering
\footnotesize
\begin{tabular}{llllllllll}
\toprule
 & \multicolumn{3}{c}{SEEK} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS-LASSO} \\ \hline
\multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
\multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{$K$} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & 4.56 \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.14 & 0.09 & \multicolumn{1}{l|}{0.04} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.02 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.51 & 0.53 & \multicolumn{1}{l|}{0.54} & 0.50 & 0.50 & 0.50 \\ \hline
\multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{comment}
% The third one is called OhioT1DM environment \citep{shi2020does}, which is an environment to generate synthetic data based on real data from the OhioT1DM dataset, primarily documented in \citet{marling2020ohiot1dm}. Such environment is to generate simulated patients similar to actual ones with type 1 diabetes from original data, where the 
%  three true state variables are the average glucose level, meal taken (represented by the carbohydrate estimate) and exercise (represented by patients' subjective judgment of intensity), with extra $p-p_0$ manually added null variables, where $p=20$. All such measurements are for respective interval of one hour. More details about the OhioT1DM dataset are provided in the real data part in below.

 The performance of different methods are evaluated by FDR/mFDR, true positive rate (TPR), and the value of the trained policy based on selected variables. All results are averaged over 50 runs. For AR environment, horizon of one trajectory is 150 and total numbers of trajectories  ranging from 10, 20 to 40. For Mixed, there are 50, 100 or 200 trajectories, with horizon 150. Due to limited space a part of results are in Appendix \ref{apdx: a}. In addition, we consider extra scenarios there, even when exponential decay in temporal dependence is violated, which further shows the advantage of SEEK.  %Finally, OhioT1DM has the same range of trajectories as AR, but with horizon of 2000.

% \textbf{Experiment design.} We consider two environments from OpenAI Gym \citep{brockman2016openai}, `CartPole-v0' and `LunarLander-v2', where the dimension of the minimal sufficient state $p_0$ equals $4$ and $8$, respectively. For each environment, we manually include $p-p_0$ null variables in the state with $p$ taking values in $\{50,100,150,200\}$, leading to a challenging high-dimensional state system. We consider 
% both AR(1) and i.i.d. white noise for the null variables. The number of trajectories $N$ in the offline dataset is chosen from $\{100,200\}$, where each trajectory contains approximately 130 decision points in `CartPole-v0' and 340 decision points in `LunarLander-v2'. %All these RL agents follow 
% The behavior policy that generates the batch data is set to an $\epsilon$-greedy algorithm %as the behavior policy, 
% with $\epsilon=0.3$, where the optimal policy is fixed and trained via a Deep Q-Network \citep{mnih2015human} agent for `CartPole-v0' or by a Duelling Double Deep Q-Network \citep{wang2016dueling} agent for `LunarLander-v2'.


\textbf{Methods and benchmark specification}
In each environment, given the offline data generated, we apply SEEK respectively. 
%And for each, to calculate the variable importance, we try both LASSO and random forest, which leads to four methods, including SEEK-LASSO, SEEK-LASSO+, SEEK-RF, SEEK-RF+. 
We fix the majority vote threshold $\alpha$ to be $0.5$, and the constraint on extra pay for best $K$ selection, $\delta$, is set to be $0.01$.  Two benchmarks are included, including a one-step approach that performs variable selection using LASSO (denoted by VS-LASSO),
%or random forest (VS-RF), 
and the sparse feature selection (SFS) method by \cite{hao2021sparse}. Finally, after gaining the selected state variables by SEEK, VS and SFS respectively, 
%using offline policy evaluation, 
we train the policies and evaluate the value of each resulting policy. 
\end{comment}




% \textbf{Methods and benchmark specification.} We apply the proposed SEEK method in Algorithm \ref{alg: 1} to the simulated offline datasets. We apply two machine learning algorithms to compute the $W$-statistics, corresponding to LASSO and random forest (see their hyper-parameters information in Tables \ref{tab:hyper_lasso} and \ref{tab:hyper_rf} of Appendix \ref{apdx: a}), respectively, to compute feature statistics, 
% %with knockoffs and knockoff+, respectively, for variable selection criterion. 
% and implements both the knockoff and knockoff+ procedures in Algorithm \ref{alg: 2} to determine the threshold. This yields four methods, denoted by SEEK-LASSO, SEEK-RF, SEEK-LASSO+, and SEEK-RF+. %We split samples by $K = \log(NT)$. 
% In the implementation, we fix $K=\log(NT)$, $q=0.1$ and $\alpha=0.5$.  %The target FDR is set to be  $q=0.1$ and the threshold $\alpha$ is fixed to $0.5$. 
% Two benchmarks are included for comparison, including a one-step approach that performs variable selection using LASSO or random forest (denoted by VS-LASSO or VS-RF), and %.The first one is the 
% %We consider the simple variable selection (VS) method with the response variable to be the augmented vector $[R, \mathbf{S}']$  based on either LASSO and random forest, namely, VS-LASSO and VS-RF. Here, for VS-RF, we fine-tune the best threshold for variable selection based on the feature important so that it can control the false discovery rate under most cases. We also implement 
% the sparse feature selection (SFS) method proposed by \cite{hao2021sparse} based on LASSO. %(denoted by SFS-LASSO).
%\vspace{-0.8cm}


\begin{comment}
\begin{table}[t]
\linespread{1.25}\selectfont
\caption{Performances of SEEK with different $K$ in the Mixed environment, aggregated over 100 simulation runs.}
\label{tab: mix_compare}
\centering
\footnotesize
\begin{tabular}{lllllllllllll}
\toprule
 & \multicolumn{3}{c}{Selected $K$} & \multicolumn{3}{c}{$K$=5} & \multicolumn{3}{c}{$K$=10} & \multicolumn{3}{c}{$K$=20} \\ \hline
\multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
\multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{$K$} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.01 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.95 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.98} & 0.50 & 0.50 & 0.50 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.21 & 0.39 & \multicolumn{1}{l|}{0.54} & 0.27 & 0.37 & \multicolumn{1}{l|}{0.50} & 0.53 & 0.36 & \multicolumn{1}{l|}{0.85} & 0.39 & 0.98 & 1.17 \\ \hline
\multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ 
\bottomrule
\end{tabular}
\end{table}
\end{comment}


\begin{table}
\linespread{1.25}\selectfont
\caption{Performances of SEEK for different $K$ in the AR environment, aggregated over
100 simulation runs.}
\label{tab: ar_bestK}
\centering
\footnotesize
\begin{tabular}{llllllllllllllll}
\toprule
 & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} \\ \hline
\multicolumn{1}{l|}{N} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
\multicolumn{1}{l|}{T} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{K} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.01 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.04 & \multicolumn{1}{l|}{0.03} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.01 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & 1.00 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.02 & 0.06 & \multicolumn{1}{l|}{0.04} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & 0.16 \\ \hline
\multicolumn{1}{l|}{Value} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 1.87 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.01 & 2.04 & 2.08 \\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}[t]
\linespread{1.25}\selectfont
\caption{Performances of SEEK with different $K$ in the Mixed environment, aggregated over 100 simulation runs.}
\label{tab: mix_compare}
\centering
\footnotesize
\begin{tabular}{llllllllllllllll}
\toprule
 & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} \\ \hline
\multicolumn{1}{l|}{N} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
\multicolumn{1}{l|}{T} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{K} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.01 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.95 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.98} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.50 & 0.50 & 0.50 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.21 & 0.39 & \multicolumn{1}{l|}{0.54} & 0.27 & 0.37 & \multicolumn{1}{l|}{0.50} & 0.53 & 0.36 & \multicolumn{1}{l|}{0.85} & 0.39 & 0.98 & \multicolumn{1}{l|}{1.17} & 0.68 & 0.52 & 2.21 \\ \hline
\multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Results}%Performance of SEEK and Comparison with Baseline Methods}
%\vspace{-0.3cm}
% \begin{table}
% \caption{Performances of SEEK and benchmark methods in AR environment}
% \label{tab: ar_compare}
% \centering
% \footnotesize
% \begin{tabular}{llllllllll}
% \toprule
% & \multicolumn{3}{c}{SEEK} &  & SFS &  & \multicolumn{3}{c}{VS+LASSO} \\ \hline
% \multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & 29.28 \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 0.60 & 0.60 & 0.60 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.07 & 0.03 & \multicolumn{1}{l|}{0.06} & 0.90 & 0.90 & 0.90 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 1.00 & 1.00 & 1.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & 1.00 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.32 & 0.48 & \multicolumn{1}{l|}{0.87} & 0.35 & 0.44 & 0.51 \\ \hline
% \multicolumn{1}{l|}{Value} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.02 & 2.06 & \multicolumn{1}{l|}{2.07} & 1.49 & 1.77 & 1.90 \\ 
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[!htp]
% \caption{Performances of SEEK and benchmark methods in Mixed environment}
% \label{tab: mixed_compare}
% \centering
% \footnotesize
% \begin{tabular}{llllllllll}
% \toprule
%  & \multicolumn{3}{c}{SEEK} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS+LASSO} \\ \hline
% \multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & 4.56 \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.14 & 0.09 & \multicolumn{1}{l|}{0.04} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.02 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.51 & 0.53 & \multicolumn{1}{l|}{0.54} & 0.50 & 0.50 & 0.50 \\ \hline
% \multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ 
% \bottomrule
% \end{tabular}
% \end{table}

%FDRs and TPRs are averaged over 100 runs. %and values are calculated by Monte Carlo with the optimal policy estimated by fitted-Q iteration (20 iterations) over 10 runs for `CartPole-v0' only. 
% Other results are presented in Figure \ref{fig:carpole_rf} of the Appendix \ref{apdx: a}. %to save space. 
%for two environments aggregated over 100 runs, as well as the value of the trained optimal policy using fitted-Q iteration (20 iterations) based on the selected variables for `CartPole-v0' aggregated over 10 runs. The results for the approximately linear environment `CartPole-v0'under SEEK-LASSO, SEEK-LASSO+, VS-LASSO, and SFS-LASSO are provided in Figure \ref{fig:carpole} for the approximately linear environment `CartPole-v0', 
%We evaluate the performance of different methods by the false discovery rate (FDR) and the true positive rate (TPR) for two environments aggregated over 100 runs, as well as the value of the trained optimal policy using fitted-Q iteration (20 iterations) based on the selected variables for `CartPole-v0' aggregated over 10 runs. The results for the approximately linear environment `CartPole-v0'under SEEK-LASSO, SEEK-LASSO+, VS-LASSO, and SFS-LASSO are provided in Figure \ref{fig:carpole} for the approximately linear environment `CartPole-v0', , under each method and setting in Figures \ref{fig:carpole} and \ref{fig:lunarlander}. 

Results are reported in Tables \ref{tab: ar_compare} \& \ref{tab: mixed_compare} as well as Figures \ref{fig:carpole_rf} \& \ref{fig:lunarlander} respectively. 
%To save space, Table \ref{tab: mixed_compare} and Figure \ref{fig:lunarlander} are relegated to Section  \ref{apdx: a} of the Supplemental Material. 
We summarize our findings as follows. First, the proposed SEEK algorithm %(with either LASSO or random forest) 
outperforms other baseline methods, with FDRs close to zero and TPRs close to one in almost all cases. In addition, it can be seen from Tables \ref{tab: ar_compare} \& \ref{tab: mixed_compare} and Figure \ref{fig:carpole_rf} that the resulting policies based on the selected state variables achieve the highest values. This demonstrates the importance of variable selection to RL. Second, the one-step variable selection method fails with high FDR when the null variables follow an AR model and does not select enough variables in the Mixed environment. Third, SFS has low power in the Mixed environment and fails to control FDR in the LL environment. As noted previously, this is because SFS  requires linear function approximation and cannot handle complex nonlinear systems. These results align with our theoretical findings, demonstrating the advantage of the proposed SEEK algorithm. 

% In addition, only focusing on the proposed algorithm, in each environment, we can see the advantage of selecting the best $K$ before implementing SEEK. In all cases, we can observe that with the best $K$ selected, corresponding FDR and TPR are very close to 0 and 1 respectively. Especially, take AR environment for an example, it could be expected that with $K$ increasing but not too large, performances on both FDR and TPR should also improve. But those trends could be reversed when $K$ is too large, due to the significant decrease in the sample size of each split data subset. What we see form the results is that the rough trend of improvement is obvious when $K$ increases from $5$, $10$, till $20$. But then both FDR and TPR get worse when $K=40$. And due to our design of best $K$, the one selected is around $20$, still much lower than $40$, which perfectly align with the trends observed. With that said, selecting the best $K$ further guarantee the performance in practice.



% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

% \begin{table}[!htp]
% \caption{Performances of SEEK and benchmark methods in AR environment}
% \label{tab: ar_compare}
% \centering
% \footnotesize
% \begin{tabular}{llllllllll}
% \toprule
% & \multicolumn{3}{c}{SEEK} &  & SFS &  & \multicolumn{3}{c}{VS+LASSO} \\ \hline
% \multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & 29.28 \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 0.60 & 0.60 & 0.60 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.07 & 0.03 & \multicolumn{1}{l|}{0.06} & 0.90 & 0.90 & 0.90 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 1.00 & 1.00 & 1.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & 1.00 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.32 & 0.48 & \multicolumn{1}{l|}{0.87} & 0.35 & 0.44 & 0.51 \\ \hline
% \multicolumn{1}{l|}{Value} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.02 & 2.06 & \multicolumn{1}{l|}{2.07} & 1.49 & 1.77 & 1.90 \\ 
% \bottomrule
% \end{tabular}
% \end{table}

\begin{figure}[t]
	\centering
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/fdr_rf_n100_car_0107.png}
		\caption{$N=100$, i.i.d.}
	\end{subfigure}%
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/fdr_rf_n200_car_0107.png}
		\caption{$N=200$, i.i.d.}
	\end{subfigure}%
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n100_car_0107.png}
		\caption{$N=100$, AR noise.}
	\end{subfigure}%
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n200_car_0107.png}
		\caption{$N=200$, AR noise.}
	\end{subfigure}% \\
	\\
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/tpr_rf_n100_car_0107.png}
		\caption{$N=100$, i.i.d.}
	\end{subfigure}%
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/tpr_rf_n200_car_0107.png}
		\caption{$N=200$, i.i.d.}
	\end{subfigure}%
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n100_car_0107.png}
		\caption{$N=100$, AR noise.}
	\end{subfigure}%
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n200_car_0107.png}
		\caption{$N=200$, AR noise.}
	\end{subfigure}% 
	\\
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/value_rf_n100_car_0107.png}
		\caption{$N=100$, i.i.d.}
	\end{subfigure}%
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/value_rf_n200_car_0107.png}
		\caption{$N=200$, i.i.d.}
	\end{subfigure}%
	\begin{subfigure}{0.24\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/value_rf_ar_n100_car_0107.png}
		\caption{$N=100$, AR noise.}
	\end{subfigure}%
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/value_rf_ar_n200_car_0107.png}
		\caption{$N=200$, AR noise.}
	\end{subfigure}% 
	\caption{Comparison results of the false discovery rate (FDR), the true positive rate (TPR), and the value based on SEEK-RF, %SEEK-RF+, 
	VS-RF, and SFS under `CartPole-v0'.}
	\label{fig:carpole_rf}
	% \vspace{-0.3cm}
\end{figure}


\begin{figure}[t]
\centering
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_rf_n100_lunar_0107.png}
\caption{$N=100$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_rf_n200_lunar_0107.png}
\caption{$N=200$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n100_lunar_0107.png}
\caption{$N=100$, AR noise.}
\end{subfigure}%
\begin{subfigure}{0.32\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n200_lunar_0107.png}
\caption{$N=200$, AR noise.}
\end{subfigure}% \\
\\
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_rf_n100_lunar_0107.png}
\caption{$N=100$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_rf_n200_lunar_0107.png}
\caption{$N=200$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n100_lunar_0107.png}
\caption{$N=100$, AR noise.}
\end{subfigure}%
\begin{subfigure}{0.32\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n200_lunar_0107.png}
\caption{$N=200$, AR noise.}
\end{subfigure}%  
\caption{Comparison results of the false discovery rate (FDR) and the true positive rate (TPR) based on SEEK-RF, %SEEK-RF+, 
VS-RF, and SFS under `LunarLander-v2', aggregated over 100 runs.}
\label{fig:lunarlander}
% \vspace{-0.3cm}
\end{figure}


\begin{figure}[!htp]
	\centering
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/SA_FDR_k0_overq.pdf} 
		\caption{$K =\log(NT)$}
	\end{subfigure}%
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/SA_FDR_k1_overq.pdf} 
		\caption{$K = 1.5\log(NT)$}
	\end{subfigure}%
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/SA_FDR_k2_overq.pdf} 
		\caption{$K = 2\log(NT)$}
	\end{subfigure}%
	\caption{Results of the FDR using the proposed SEEK methods under `CartPole-v0' with $N=100$, $p=100$, $\alpha = 0.5$, and AR(1) noises. All the results are aggregated over 20 runs.}
	\label{fig:sa1}
	% \vspace{-0.3cm}
\end{figure}

\begin{figure}[!t]
	\centering
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/SA_TPR_k0_overq.pdf} 
		\caption{$K = \log(NT)$}
	\end{subfigure}%
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/SA_TPR_k1_overq.pdf} 
		\caption{$K = 1.5\log(NT)$}
	\end{subfigure}%
	\begin{subfigure}{0.32\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{figs/SA_TPR_k2_overq.pdf} 
		\caption{$K = 2\log(NT)$}
	\end{subfigure}%
	\caption{Results of the TPR using the proposed SEEK methods under `CartPole-v0' with $N=100$, $p=100$, $\alpha = 0.5$, and AR(1) noises. All the results are aggregated over 20 runs.}
	\label{fig:sa2}
	% \vspace{-0.3cm}
\end{figure}


% \textbf{Sensitivity analyses.} We conduct a sensitivity analysis to test the robustness of four SEEK methods (SEEK-LASSO, SEEK-RF, SEEK-LASSO+, and SEEK-RF+) to different hyper-parameters ($K$ and $q$). For a simple illustration, we focus on `CartPole-v0' with $N=100$, $p=100$, and AR(1) noise. We split the sample by $K = k_0\log(NT)$ for $k_0\in \{1,1.5,2\}$, and choose different $q\in(0,0.3]$ for the FDR control. We always fix $\alpha = 0.5$ during the majority vote. The results of the corresponding FDRs and TPRs are provided in Figures \ref{fig:sa1} and \ref{fig:sa2} of Appendix \ref{apdx: a} respectively. It can be concluded that our method performs uniformly well across different combinations of $K$ and $q$. %In addition, as $k_0$ increases, i.e., the number of the splitting samples increases, we can observe slightly lower FDRs and TPRs, which reflects the bias-and-variance trade-off.


%We report the false discovery rate (FDR), the modified FDR (mFDR), the false positive rate (FPR), and the true positive rate (TPR) for each method under different settings in Tables in Appendix, over 100 replicates. We further evaluate the trained optimal policy using fitted-Q iteration based on the selected variables and summarize the corresponding value. 


%based on $\epsilon-Greedy as the behavior policy for offline data generating follows $\epsilon-Greedy with the $\epsilon=0.3$ probability to select  random action and $1-\epsilon$ to choose the optimal action. The optimal policy is trained by Deep Q-Network \citep{mnih2015human} for `CartPole-v0' and by Duelling Double Deep Q Network \citep{wang2016dueling} for `LunarLander-v2'. 

%\vspace{-0.71cm}
%\subsection{Mixing Coefficients Estimation and the Best K Selection}
%\vspace{-0.3cm}
To investigate the proposed $K$-selection algorithm in Section \ref{sec: bestK}, we conduct another analysis that applies SEEK to the first two environments with $K$ fixed to $5,10,20,40$ and compares the results against those obtained based on SEEK with adaptively selected $K$. See 
%first directly apply SEEK with $K=5,10,20,40$ fixed respectively. Then we, as introduced in section \ref{sec: bestK}, select the best $K$ accordingly, and finally implement SEEK. The results for AR environment are summarized in 
Tables \ref{tab: ar_bestK} \&  \ref{tab: mix_compare}%, with those for other scenarios in Appendix 
for details. It can be seen from Table \ref{tab: mix_compare} that mFDR, FDR, and TPR depend on the specification of $K$. In particular, when $K$ is moderately large (e.g., 10 or 20),  the TPR is reduced to half whereas the mFDR, FDR, and FPR are exactly equal to zero.  On the contrary, the proposed $K$-selection algorithm tends to select a small value of $K$ in the Mixed environment, achieving a better balance between (m)FDR/FPR and TPR. 
 
In addition, we conduct a sensitivity analysis to test the robustness of four SEEK methods (SEEK-LASSO, SEEK-RF, SEEK-LASSO+, and SEEK-RF+, where SEEK-X+ refers to the SEEK-X algorithm with knockoffs+ for variable selection) to different choices of $K$ and $q$, when the $\beta$-mixing assumption does not hold. %For a simple illustration, 
We focus on the `CartPole-v0' environment with $N=100$, $p=100$, and AR(1) noise, set $K = k_0\log(NT)$ with $k_0\in \{1,1.5,2\}$, choose $q\in(0,0.3]$ for the FDR control, %We always fix $\alpha = 0.5$ during the majority vote. The results of 
and plot the corresponding FDRs and TPRs %are provided 
in Figures \ref{fig:sa1} and \ref{fig:sa2} respectively. It can be seen that our method controls the FDR for any $K$ and $q$. In addition, the performance is not overly sensitive to the choice of $K$. 

%We further conduct a sensitivity analysis in Section \ref{sec:addsimuresults} of the supplementary article and find that the performance of 
%It is obvious that as $K$ increases, performances on both FDR/mFDR and TPR tend to improve. However, when the number of splits is too large, making sample size in each data subset not enough, the selection is negatively influenced instead. That is why FDR/mFDR and TPR deviate a little bit from $0$ and $1$ respectively in these cases. Then we need a balance as emphasized in Section \ref{sec: bestK}. Taking AR environment for an example,  the trend of improvement is obvious when $K$ increases from $5$, $10$, till $20$. But then both FDR and TPR get worse when $K=40$. And due to our design of best $K$, the one selected is around $20$, still much lower than $40$, which perfectly aligns with the trend. Observing that, selecting the best $K$ further guarantees the performance in practice.
%\vspace{-1cm}

% \begin{table}[!htp]
% \caption{Performances of SEEK for different K in AR environment}
% \centering
% \footnotesize
% \begin{tabular}{lllllllllllll}
% \toprule
% &  & K=5 &  &  & K=10 &  &  & K=20 &  &  & K=40 &  \\ \hline
% \multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
% \multicolumn{1}{l|}{$T$}  & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% % Selected K & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
% \multicolumn{1}{l|}{mFDR}  & 0.01 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & 0.00 \\ \hline
% FDR & 0.03 & 0.04 & \multicolumn{1}{l|}{0.03} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & 0.00 \\ \hline
% FPR & 0.00 & 0.01 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & 0.00 \\ \hline
% TPR & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & 1.00 \\ \hline
% % Speed(min) & 0.02 & 0.06 & \multicolumn{1}{l|}{0.04} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & 0.16 \\ \hline
% Value & 2.025 & 2.061 & \multicolumn{1}{l|}{2.075} & 2.029 & 2.062 & \multicolumn{1}{l|}{2.077} & 1.871 & 2.065 & \multicolumn{1}{l|}{2.077} & 2.011 & 2.040 & 2.077 \\ \bottomrule
% \end{tabular}
% \end{table}
 



% \begin{table}[!htp]
% \caption{Performances of SEEK for different K in toy environment}
% \centering
% \footnotesize
% \begin{tabular}{lllllllllllll}
% \toprule
%  & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} \\ \hline
% \multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.01 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.95 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.98} & 0.50 & 0.50 & 0.50 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.21 & 0.39 & \multicolumn{1}{l|}{0.54} & 0.27 & 0.37 & \multicolumn{1}{l|}{0.50} & 0.53 & 0.36 & \multicolumn{1}{l|}{0.85} & 0.39 & 0.98 & 1.17 \\ \hline
% \multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ 
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{sidewaystable}
% \caption{Performances of SEEK for different K in AR environment}
% \centering
% \footnotesize
% \begin{tabular}{lllllllllllll}
% \toprule
%  &  & K=5 &  &  & K=10 &  &  & K=20 &  &  & K=40 &  \\ \hline
% \multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% % \multicolumn{1}{l|}{Selected K} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.01 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.03 & 0.04 & \multicolumn{1}{l|}{0.03} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.00 & 0.01 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & 1.00 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.02 & 0.06 & \multicolumn{1}{l|}{0.04} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & 0.16 \\ \hline
% \multicolumn{1}{l|}{Value} & 2.025 & 2.061 & \multicolumn{1}{l|}{2.075} & 2.029 & 2.062 & \multicolumn{1}{l|}{2.077} & 1.871 & 2.065 & \multicolumn{1}{l|}{2.077} & 2.011 & 2.040 & 2.077 \\ \bottomrule
% \end{tabular}
% \end{sidewaystable}

%\vspace{-1cm}
\section{Real Data Analysis}
\label{sec: real}
%\vspace{-0.5cm}
%\subsection{Settings and Methods}
%\vspace{-0.3cm}

\begin{figure}[!t]
\centering
\begin{subfigure}{1\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/real_SEEK.pdf}
\caption{Results under SEEK.}
\end{subfigure}\\%\
\begin{subfigure}{1\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/real_HBT.pdf}
\caption{Results under SFS.}
\end{subfigure}\\%
\begin{subfigure}{1\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/real_VSLASSO.pdf}
\caption{Results under VS-LASSO.}
\end{subfigure}\\%
\begin{subfigure}{0.35\linewidth}
\centering
\includegraphics[trim={0 8cm 0 0},width=1\linewidth]{figs/real_colorbar.pdf}
% \caption{Color bar.}
\end{subfigure}% \\
\caption{Results of real data analysis based on SEEK, VS-LASSO, and SFS.}
\label{fig:real}
%\vspace{-0.3cm}
\end{figure}

% The third one is called OhioT1DM environment \citep{shi2020does}, which is an environment to generate synthetic data based on real data from the OhioT1DM dataset, primarily documented in \citet{marling2020ohiot1dm}. Such environment is to generate simulated patients similar to actual ones with type 1 diabetes from original data, where the 
%  three true state variables are the average glucose level, meal taken (represented by the carbohydrate estimate) and exercise (represented by patients' subjective judgment of intensity), with extra $p-p_0$ manually added null variables, where $p=20$. All such measurements are for respective interval of one hour. More details about the OhioT1DM dataset are provided in the real data part in below.
%We mainly 
In this section, we apply SEEK to the %data of patients with type 1 diabetes, from the 
OhioT1DM dataset \citep{marling2020ohiot1dm} which contains data from 6  patients with type-I diabetes. For each patient, their glucose levels and self-reported meals are continuously measured over 8 weeks. These variables can be used to construct data-driven decision rules to determine whether a patient needs insulin injections to improve their health \citep{luckett2019estimating,shi2020statistical,zhou2022estimating}. 

To analyze this data, we divide the eight weeks into one-hour intervals and compute the average glucose level $G_t$ and the average carbohydrate estimate for the meal $M_t$ over each one-hour interval $(t-1,t]$. Previous studies found these variables do not satisfy the Markov assumption \citep[see e.g.,][]{shi2020does}. This motivates us to construct the state $S_t$ by including both the current observations $G_t$, $M_t$ as well as the past measurements within 5 hours, i.e., $G_{t-i}$ and $M_{t-i}$ for $i=1,2,3,4,5$. We also manually include 10 null variables (denoted by $N_1,\cdots,N_{10}$) in the state, each following an AR(1) model. The action $A_t$ is binary, indicating whether the patient injected the insulin or not in the last hour. 
%corresponds to the amount of insulin doses a patient receives during the time interval $(t-1,t]$. 

We next apply SEEK, VS-LASSO, and SFS to 4 out of 6 data trajectories for variable selection, 
%for variable selection and employ cross-validation to evaluate the value of the subsequent estimated optimal policy based on the selected state variables. Specifically, we use four trajectories for variable selection and policy learning, and use the remaining two trajectories for policy value evaluation. We 
iterate this procedure for all $\binom{6}{4} =15$ combinations, compute the percentage of each state variable being selected and report the results in % as well as the average value of the estimated optimal policy. 
%split the six trajectories into training and testing subsets
%The 12 true state variables are average glucose level ($G(t)$), meal taken ($M(t)$, represented by the carbohydrate estimate), and their former measurements up till 5 hours ($G(t-i), M(t-i), i= 1,...,5$), summarized for each one hour interval, over a total of 8 weeks. Due to \citep{shi2020does}, states in former time intervals have delayed influence on future, that is why we extend the state space to include $5$ lags. We also manually add 10 null variables ($N(i), i=1,...,10$) with autoregressive dependence.
%similar to AR environment.
%simulation, but with $c=0.5$. 
%Then over that extended state vector, we implement SEEK, SFS as well as VS.
%(with all related variants of methods).
% \begin{figure}[!t]
% \centering
% \begin{subfigure}{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/real_SEEK.pdf}
% \caption{Results under SEEK.}
% \end{subfigure}\\%\
% \begin{subfigure}{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/real_HBT.pdf}
% \caption{Results under SFS.}
% \end{subfigure}\\%
% \begin{subfigure}{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/real_VSLASSO.pdf}
% \caption{Results under VS-LASSO.}
% \end{subfigure}\\%
% \begin{subfigure}{0.35\linewidth}
% \centering
% \includegraphics[trim={0 8cm 0 0},width=1\linewidth]{figs/real_colorbar.pdf}
% % \caption{Color bar.}
% \end{subfigure}% \\
% \caption{Results of real data analysis based on SEEK, VS-LASSO, and SFS.}
% \label{fig:real}
% %\vspace{-0.3cm}
% \end{figure}
%\vspace{-0.6cm}
%\subsection{Results}
%\vspace{-0.3cm}
%To have a straightforward view of the results, we provide bar charts of selection rates in Figure \ref{fig:real}. For each method, to gain robust conclusion, we divide the 6 patients into 2 groups for cross validation, with the first consisting of 4 patients, used to select variables, and the second of 2 patients, to train optimal policies with selected ones and find corresponding values of policies. In that way, we perform selections for $\binom{6}{2} =15$ replications. Then we calculate the selection rates, which are shown as the heights of variables in each graph.
Figure \ref{fig:real}. %visualizes the variable selection results. 
It can be seen that (i) VS-LASSO has a high FDP with each of the null variables being selected at least $50\%$ of the time; (ii) SFS fails to identify the glucose levels and carbohydrate estimates as significant variables, suffering from a low TPR; (iii) SEEK rarely selects null variables and has a high success rate in selecting glucose levels and carbohydrate estimates. 
%SEEK hardly selects any null variable
%It can be seen that most measurements of glucose and meal are selected by SEEK with high rates, while all added nulls are almost never selected. 
%(with only the first null selected once out of 15 runs). On the other hand, although SFS does select $G(t-5)$, all other significant state variables are only identified with low rates. The reason is SFS has its theoretical foundation on linear function approximation, with limited ability to deal with complex systems as the one concerned here. In addition, VS selects all variables, failing to control false discoveries, as expected from Section \ref{sec: minimal}. In a word, for real data with complex systems, SEEK has advantages over benchmarks.

\vspace{-0.5cm}
\section{Discussion}

% \subsection{Conclusion}
% In this work we proposed the algorithm SEEK to perform state variable selection with knockoffs based ideas in RL. We first split the sequence of data into subsets with reduced dependence, and further subgroup them conditional on actions, on which sequential knockoffs can be applied. Iterative selection is also necessary so that all state variables either directly or indirectly influencing the rewards will be selected. Theoretical control on FDR/mFDR is provided, together with power analysis, in the sense that simultaneously with sample size increasing, SEEK tends to select all true state variables without including nulls. Implementations in simulation are then provided in different scenarios, showing advantage over benchmark methods. Flexible choices in the design of sequential selection is also provided, to incorporate nonlinear dependence on covariates and better satisfy the requirement on sampling knockoff variables.


%\subsection{Future Work}
There are several potential topics for further research.
In this paper, we assume the environment is time-homogeneous. 
It would be interesting to extend the proposed ideas to handle
non-stationary environments %with potentially time-evolving 
where the set of significant variables evolves with time as well.  
Another extension %of the underlying environment would be to 
is to consider variable selection in
a partially observed MDP (POMDP); in this 
case, %one potential approach 
%is to 
we may want to apply SEEK to the belief state MDP.
%Finally, one might 
Finally, it is worthwhile to consider %the application of 
other 
(non-knockoff) variable
selection methods %to sequential decision making %based on the definition of a 
to identify the minimal sufficient state in sequential decision making.  







\newpage
\bibliographystyle{agsm}
\bibliography{references}


\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}
\counterwithin{definition}{section}
\counterwithin{lemma}{section}
\counterwithin{proposition}{section}
\counterwithin{theorem}{section}

\newpage
%\appendixpage

\addtolength{\textheight}{-.3in}

\section{Additional Numerical Details}\label{apdx: a}
In this section, we provide additional numerical configurations, especially the data generating processes and hyper-parameters information.%Specifically, we present  additional results for environments in Section \ref{sec: exp}, and summarize the hyper-parameters information in the applied machine learning methods to compute the $W$-statistics,  including LASSO in Table \ref{tab:hyper_lasso} and random forest in Table \ref{tab:hyper_rf}, respectively. %Finally, extra environments are considered, including `CartPole-v0' and `LunarLander-v2', to further elaborate the advantage of SEEK.

%\subsection{Data Generating Processes, Hyper-parameters Information}
The AR environment in Section \ref{sec: exp} is generated as follows. Each state is generated according to: $S_{t+1,j}=0.9 S_{t,j}A_t+0.09S_{t,j}(1-A_t)+N(0,1)$ for any $t$ and $j$. The reward is given by $R_t=A_t (S_{t,1}+S_{t,2}) + N(0,1)$ for any $t$. 
%, to be exact, for the whole states,
\begin{comment}
\begin{align*}
    \mathbf{s}_{t+1} = 
    \begin{cases}
        c~\mathbf{s}_{t} + N(0,\mathbf{I}) & \text{if}~ a=1\\
        \frac{c}{10}~\mathbf{s}_{t} + N(0,\mathbf{I}) & \text{otherwise}\\
    \end{cases}
\end{align*}
with constant $c=0.9$. While for 
\end{comment}
In the real data analysis, each of the null variables follows an AR(1) model, given by $S_{t+1,j}=0.5 S_{t,j}+N(0,1)$. 

In our implementation, we apply LASSO and random forest to compute the $W$-statistics and summarize their hyper-parameters information in Tables \ref{tab:hyper_lasso} and \ref{tab:hyper_rf}, respectively.
%apart from the originally recorded variables, we have those null ones, manually added by us, solely following an AR(1) process by
%\begin{align*}
%    \mathbf{x}_{t+1} = c~\mathbf{x}_{t} + N(0,\mathbf{I})
%\end{align*}
%with $c=0.5$ and $\mathbf{x}_t$ stands for the vector of nulls at time $t$.

\begin{comment}
\subsection{Additional Simulation Results}\label{sec:addsimuresults}
Table \ref{tab: mixed_compare} reports the variable selection and policy value results when applying SEEK, SFS, and VS-LASSO to the Mixed environment. Table \ref{tab: ar_bestK} reports the results when applying SEEK with the proposed $K$-selection algorithm or with $K$ fixed to $10,20,40$ to the AR environment. Figure \ref{fig:lunarlander} reports the variable selection results when applying SEEK-RF, VS-RF, and SFS to the `LunarLander-v2' environment. 

\end{comment}


%We include the performances of SEEK with respect to different $K$ in Mixed environment in Table \ref{tab: mix_compare}. It is obvious that no matter which value $K$ takes, neither FDR/mFDR nor TPR improves a lot, with TPR even dropping significantly, indicating that the sample size after the split is already not enough. Then a even smaller value should be chosen, which perfectly aligns with the selected ones in the first 3 columns. Due to limited width, we defer the complete tables containing performances with more different values of $K$ in Tables \ref{tab: ar_long} and \ref{tab: mix_long}.
\begin{comment}
\begin{table}
\linespread{1.25}\selectfont
\caption{Performances of SEEK and benchmark methods in the Mixed environment, aggregated over
100 simulation runs.}
\centering
\footnotesize
\begin{tabular}{llllllllllllllll}
\toprule
 & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} & \multicolumn{3}{c}{Selected K} \\ \hline
\multicolumn{1}{l|}{N} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200\\ \hline
\multicolumn{1}{l|}{T} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150\\ \hline
\multicolumn{1}{l|}{K} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 &  4.56 \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.01 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.04 & 0.01 & 0.01 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.04 & 0.05 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.04 & 0.01 & 0.01 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.50 & 0.95 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.98} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.71 & 0.98 & 1.00 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.27 & 0.37 & \multicolumn{1}{l|}{0.50} & 0.53 & 0.36 & \multicolumn{1}{l|}{0.85} & 0.39 & 0.98 & \multicolumn{1}{l|}{1.17} & 0.68 & 0.52 & \multicolumn{1}{l|}{2.21} & 0.21 & 0.39 & 0.54 \\ \hline
\multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ \bottomrule
\end{tabular}
\end{table}
\end{comment}

% \begin{table}
% \linespread{1.25}\selectfont
% \caption{Performances of SEEK and benchmark methods in the Mixed environment, aggregated over
% 100 simulation runs.}
% \label{tab: mixed_compare}
% \centering
% \footnotesize
% \begin{tabular}{llllllllll}
% \toprule
%  & \multicolumn{3}{c}{SEEK} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS-LASSO} \\ \hline
% \multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & 4.56 \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.14 & 0.09 & \multicolumn{1}{l|}{0.04} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.02 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.51 & 0.53 & \multicolumn{1}{l|}{0.54} & 0.50 & 0.50 & 0.50 \\ \hline
% \multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ 
% \bottomrule
% \end{tabular}
% \end{table}

\begin{comment}
\begin{table}
\linespread{1.25}\selectfont
\caption{Performances of SEEK for different $K$ in the AR environment, aggregated over
100 simulation runs.}
\label{tab: ar_bestK}
\centering
\footnotesize
\begin{tabular}{lllllllllllll}
\toprule
 & \multicolumn{3}{c}{Selected $K$} & \multicolumn{3}{c}{$K$=10} & \multicolumn{3}{c}{$K$=20} & \multicolumn{3}{c}{$K$=40} \\ \hline
\multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
\multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{$K$} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & 1.00 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & 0.16 \\ \hline
\multicolumn{1}{l|}{Value} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 1.87 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.01 & 2.04 & 2.08 \\ 
\bottomrule
\end{tabular}
\end{table}
\end{comment}

% \begin{table}
% \linespread{1.25}\selectfont
% \caption{Performances of SEEK for different $K$ in the AR environment, aggregated over
% 100 simulation runs.}
% \label{tab: ar_bestK}
% \centering
% \footnotesize
% \begin{tabular}{llllllllllllllll}
% \toprule
%  & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} \\ \hline
% \multicolumn{1}{l|}{N} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
% \multicolumn{1}{l|}{T} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{K} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & / \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.01 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.04 & \multicolumn{1}{l|}{0.03} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.01 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & 1.00 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.02 & 0.06 & \multicolumn{1}{l|}{0.04} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & 0.16 \\ \hline
% \multicolumn{1}{l|}{Value} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 1.87 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.01 & 2.04 & 2.08 \\ 
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[h]
	\caption{Hyper-parameters information for LASSO.}\label{tab:hyper_lasso}
	\centering
	\begin{tabular}{ll}
		\toprule  
		Hyper-parameters     & Values   \\
		\midrule 
		Penalty term $\lambda$ selection criterion    &  Bayesian information criterion (BIC)    \\
		Penalty term $\lambda$ selection range     & $[\exp(-20),\exp(-8)]$ \\
		\bottomrule
	\end{tabular}
\end{table}
\begin{table}[!htp]
	\caption{Hyper-parameters information for random forest.}\label{tab:hyper_rf}
	\centering
	\begin{tabular}{ll}
		\toprule  
		Hyper-parameters     & Values   \\
		\midrule
		Number of trees in the forest & $NT/K/30$    \\
		Maximum depth of the tree    & 4       \\
		Number of variables for the best split    & $\sqrt{p}$  \\
		Others   & Default values in python package `sklearn'  \\
		\bottomrule
	\end{tabular}
\end{table}

% \begin{sidewaystable}
% \caption{Performances of all methods in AR environment}
% \label{tab: ar_long}
% \centering
% \scriptsize
% \begin{tabular}{llllllllllllllllllllll}
% \hline
%  & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS+LASSO} \\ \hline
% \multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & 29.28 \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.01 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 0.60 & 0.60 & 0.60 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.03 & 0.04 & \multicolumn{1}{l|}{0.03} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.07 & 0.03 & \multicolumn{1}{l|}{0.06} & 0.90 & 0.90 & 0.90 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.00 & 0.01 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 1.00 & 1.00 & 1.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & 1.00 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.02 & 0.06 & \multicolumn{1}{l|}{0.04} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & \multicolumn{1}{l|}{0.16} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.32 & 0.48 & \multicolumn{1}{l|}{0.87} & 0.35 & 0.44 & 0.51 \\ \hline
% \multicolumn{1}{l|}{Value} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 1.87 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.01 & 2.04 & \multicolumn{1}{l|}{2.08} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.02 & 2.06 & \multicolumn{1}{l|}{2.07} & 1.49 & 1.77 & 1.90 \\ \hline
% \end{tabular}
% \end{sidewaystable}


% \begin{sidewaystable}
% \caption{Performances of all methods in Mixed environment}
% \label{tab: mix_long}
% \centering
% \scriptsize
% \begin{tabular}{llllllllllllllllllllll}
% \hline
%  & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS+LASSO} \\ \hline
% \multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
% \multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
% \multicolumn{1}{l|}{$K$} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & 4.56 \\ \hline
% \multicolumn{1}{l|}{mFDR} & 0.01 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FDR} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.14 & 0.09 & \multicolumn{1}{l|}{0.04} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.02 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
% \multicolumn{1}{l|}{TPR} & 0.50 & 0.95 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.98} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.51 & 0.53 & \multicolumn{1}{l|}{0.54} & 0.50 & 0.50 & 0.50 \\ \hline
% % \multicolumn{1}{l|}{Speed(min)} & 0.27 & 0.37 & \multicolumn{1}{l|}{0.50} & 0.53 & 0.36 & \multicolumn{1}{l|}{0.85} & 0.39 & 0.98 & \multicolumn{1}{l|}{1.17} & 0.68 & 0.52 & \multicolumn{1}{l|}{2.21} & 0.21 & 0.39 & \multicolumn{1}{l|}{0.54} & 1.17 & 2.58 & \multicolumn{1}{l|}{6.70} & 2.15 & 3.96 & 5.07 \\ \hline
% \multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ \hline
% \end{tabular}
% \end{sidewaystable}


%\subsection{Simulation in More Scenarios}

\begin{comment}
\begin{figure}[!t]
\centering
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_ls_n100_car_0107.png}
\caption{$N=100$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_ls_n200_car_0107.png}
\caption{$N=200$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_ls_ar_n100_car_0107.png}
\caption{$N=100$, AR noise.}
\end{subfigure}%
\begin{subfigure}{0.34\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/fdr_ls_ar_n200_car_0107.png}
\caption{$N=200$, AR noise.}
\end{subfigure}% \\
\\
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_ls_n100_car_0107.png}
\caption{$N=100$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_ls_n200_car_0107.png}
\caption{$N=200$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_ls_ar_n100_car_0107.png}
\caption{$N=100$, AR noise.}
\end{subfigure}%
\begin{subfigure}{0.35\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/tpr_ls_ar_n200_car_0107.png}
\caption{$N=200$, AR noise.}
\end{subfigure}% 
\\
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/value_ls_n100_car_0107.png}
\caption{$N=100$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/value_ls_n200_car_0107.png}
\caption{$N=200$, i.i.d.}
\end{subfigure}%
\begin{subfigure}{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/value_ls_ar_n100_car_0107.png}
\caption{$N=100$, AR noise.}
\end{subfigure}%
\begin{subfigure}{0.36\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/value_ls_ar_n200_car_0107.png}
\caption{$N=200$, AR noise.}
\end{subfigure}% 
\caption{Comparison results of the FDR (the smaller the better), the TPR (the larger the better), and the value based on SEEK-LASSO,  %SEEK-LASSO+, 
VS-LASSO, and SFS under `CartPole-v0'.}
\label{fig:carpole}
% \vspace{-0.3cm}
\end{figure}

We further consider the finite sample performance of SEEK  on extra scenarios. Notice that the following environments are mostly governed by some specific and complicated physics laws, under which it should not be expected that the $\beta$-mixing coefficients would follow some exponential-like decay, even in long run. Then there is no need for choosing the best $K$, and we focus on the advantage of SEEK itself. %on selecting important variables in offline reinforcement learning via extensive simulations and comparisons. 
Finally, the computing infrastructure used is a virtual machine in the AWS Platform with 72 processor cores and
144GB memory. %, and the average running time of our method is less than 1 minute.

\textbf{Experiment design.} We choose two complicated environments from OpenAI Gym \citep{brockman2016openai}, `CartPole-v0' and `LunarLander-v2', where the dimension of the minimal sufficient state $p_0$ equals $4$ and $8$, respectively. For each environment, we manually include $p-p_0$ null variables in the state with $p$ taking values in $\{50,100,150,200\}$, leading to a challenging high-dimensional state system. We consider 
both AR(1) and i.i.d. white noise for the null variables. The number of trajectories $N$ in the offline dataset is chosen from $\{100,200\}$, where each trajectory contains approximately 130 decision points in `CartPole-v0' and 340 decision points in `LunarLander-v2'. %All these RL agents follow 
The behavior policy that generates the batch data is set to an $\epsilon$-greedy algorithm %as the behavior policy, 
with $\epsilon=0.3$, where the optimal policy is fixed and trained via a Deep Q-Network \citep{mnih2015human} agent for `CartPole-v0' or by a Duelling Double Deep Q-Network \citep{wang2016dueling} agent for `LunarLander-v2'.

\textbf{Methods and benchmark specification.} Same as in the main experiments, we apply the proposed SEEK method in Algorithm \ref{alg: 1} to the simulated offline datasets (see their hyper-parameters information in Tables \ref{tab:hyper_lasso} and \ref{tab:hyper_rf}), respectively. And we apply SEEK,  using either LASSO or random forest. This yields two methods, denoted by SEEK-LASSO, SEEK-RF. %We split samples by $K = \log(NT)$. 
In the implementation, we fix $K=\log(NT)$, $q=0.1$ and $\alpha=0.5$.  %The target FDR is set to be  $q=0.1$ and the threshold $\alpha$ is fixed to $0.5$. 
The two same benchmarks are included for comparison.
%including a one-step approach that performs variable selection using LASSO or random forest (denoted by VS-LASSO or VS-RF), and %.The first one is the 
%We consider the simple variable selection (VS) method with the response variable to be the augmented vector $[R, \mathbf{S}']$  based on either LASSO and random forest, namely, VS-LASSO and VS-RF. Here, for VS-RF, we fine-tune the best threshold for variable selection based on the feature important so that it can control the false discovery rate under most cases. We also implement 
%the sparse feature selection (SFS) method proposed by \cite{hao2021sparse} based on LASSO. %(denoted by SFS-LASSO).
\end{comment}

\begin{comment}
\textbf{Results and conclusion.} We still focus on FDR and TPR, as well as the value of the estimated optimal policy based on the selected variables. For the `CartPole-v0' environment, we first report results based on
%We summarize results under the approximately linear environment `CartPole-v0' based on 
SEEK-LASSO, VS-LASSO, and SFS in Figure \ref{fig:carpole}, and those based on SEEK-RF, VS-RF and SFS in Figure \ref{fig:carpole_rf}. Then for %, and under a more complex non-linear environment 
the even more complicated `LunarLander-v2' environment, we report results based on SEEK-RF, VS-RF, and SFS in Figure \ref{fig:lunarlander}. Results are averaged over 100 runs.
\end{comment}
%FDRs and TPRs are averaged over 100 runs. %and values are calculated by Monte Carlo with the optimal policy estimated by fitted-Q iteration (20 iterations) over 10 runs for `CartPole-v0' only. 
%to save space. 
%for two environments aggregated over 100 runs, as well as the value of the trained optimal policy using fitted-Q iteration (20 iterations) based on the selected variables for `CartPole-v0' aggregated over 10 runs. The results for the approximately linear environment `CartPole-v0'under SEEK-LASSO, SEEK-LASSO+, VS-LASSO, and SFS-LASSO are provided in Figure \ref{fig:carpole} for the approximately linear environment `CartPole-v0', 
%We evaluate the performance of different methods by the false discovery rate (FDR) and the true positive rate (TPR) for two environments aggregated over 100 runs, as well as the value of the trained optimal policy using fitted-Q iteration (20 iterations) based on the selected variables for `CartPole-v0' aggregated over 10 runs. The results for the approximately linear environment `CartPole-v0'under SEEK-LASSO, SEEK-LASSO+, VS-LASSO, and SFS-LASSO are provided in Figure \ref{fig:carpole} for the approximately linear environment `CartPole-v0', , under each method and setting in Figures \ref{fig:carpole} and \ref{fig:lunarlander}. 

% \begin{figure}[!t]
% \centering
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_ls_n100_car_0107.png}
% \caption{$N=100$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_ls_n200_car_0107.png}
% \caption{$N=200$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_ls_ar_n100_car_0107.png}
% \caption{$N=100$, AR noise.}
% \end{subfigure}%
% \begin{subfigure}{0.36\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_ls_ar_n200_car_0107.png}
% \caption{$N=200$, AR noise.}
% \end{subfigure}% \\
% \\
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_ls_n100_car_0107.png}
% \caption{$N=100$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_ls_n200_car_0107.png}
% \caption{$N=200$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_ls_ar_n100_car_0107.png}
% \caption{$N=100$, AR noise.}
% \end{subfigure}%
% \begin{subfigure}{0.36\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_ls_ar_n200_car_0107.png}
% \caption{$N=200$, AR noise.}
% \end{subfigure}% 
% \\
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/value_ls_n100_car_0107.png}
% \caption{$N=100$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/value_ls_n200_car_0107.png}
% \caption{$N=200$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/value_ls_ar_n100_car_0107.png}
% \caption{$N=100$, AR noise.}
% \end{subfigure}%
% \begin{subfigure}{0.36\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/value_ls_ar_n200_car_0107.png}
% \caption{$N=200$, AR noise.}
% \end{subfigure}% 
% \caption{Comparison results of the FDR (the smaller the better), the TPR (the larger the better), and the value based on SEEK-LASSO,  %SEEK-LASSO+, 
% VS-LASSO, and SFS under `CartPole-v0'.}
% \label{fig:carpole}
% % \vspace{-0.3cm}
% \end{figure}




% \begin{figure}[t]
% \centering
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_rf_n100_lunar_0107.png}
% \caption{$N=100$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_rf_n200_lunar_0107.png}
% \caption{$N=200$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n100_lunar_0107.png}
% \caption{$N=100$, AR noise.}
% \end{subfigure}%
% \begin{subfigure}{0.32\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n200_lunar_0107.png}
% \caption{$N=200$, AR noise.}
% \end{subfigure}% \\
% \\
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_rf_n100_lunar_0107.png}
% \caption{$N=100$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_rf_n200_lunar_0107.png}
% \caption{$N=200$, i.i.d.}
% \end{subfigure}%
% \begin{subfigure}{0.24\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n100_lunar_0107.png}
% \caption{$N=100$, AR noise.}
% \end{subfigure}%
% \begin{subfigure}{0.32\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n200_lunar_0107.png}
% \caption{$N=200$, AR noise.}
% \end{subfigure}%  
% \caption{Comparison results of the false discovery rate (FDR) and the true positive rate (TPR) based on SEEK-RF, %SEEK-RF+, 
% VS-RF, and SFS under `LunarLander-v2', aggregated over 100 runs.}
% \label{fig:lunarlander}
% % \vspace{-0.3cm}
% \end{figure}

%In the following the results are presented. To begin with, the proposed algorithm in our work, compared to other baseline methods, achieve FDRs closer to zero and TPRs closer to one. What's more, as what are shown in Figures \ref{fig:carpole} and \ref{fig:carpole_rf}, the resulting estimated optimal policy based on the selected state variables gains the largest value among all methods. On contrary, the one-step variable selection method includes too many false variables when the null variables have an AR dependence through time. Its FDR is close to one in these settings. Finally, SFS works reasonably well in CartPole environment. However, it is not able to control FDR in the LunarLander environment. As noted previously, this is because SFS requires linear function approximation and cannot handle complex nonlinear systems as concerned here.
%It is clear that the proposed SEEK-RF (red triangle) outperforms all methods in all cases, followed by a slightly conservative SEEK-RF+ (pink asterisk). Specifically,  both SEEK-RF and SEEK-RF+ yields an FDR of nearly zero and an TPR of nearly one regardless of the complexity of the environment, the dimensionality of state variables, and the type of noises. In the simple environment `CartPole-v0', SEEK-LASSO (orange square) and SEEK-LASSO+ (yellow cross) achieve comparably good performance as SEEK-RF and SEEK-RF+, with their values over 160 in most cases. As the number of trajectory increases, the performance of our methods becomes even better and close to the optimal case as expected. In contrast, though VS-LASSO (green circle) and SFS-LASSO (brown dot) have reasonable good performance under `CartPole-v0' with i.i.d. noises, they overestimate the minimal sufficient state in the settings with AR noises or in the more complicated environment `LunarLander-v2'. 
%These results align with our theoretical findings, and demonstrate the value of the proposed SEEK algorithm. 

% Question: should we keep this part?
%\textbf{Sensitivity analyses.} 
\begin{comment}
In addition, we conduct a sensitivity analysis to test the robustness of four SEEK methods (SEEK-LASSO, SEEK-RF, SEEK-LASSO+, and SEEK-RF+, where SEEK-X+ refers to the SEEK-X algorithm with knockoffs+ for variable selection) to different choices of $K$ and $q$. %For a simple illustration, 
We focus on the `CartPole-v0' environment with $N=100$, $p=100$, and AR(1) noise, set $K = k_0\log(NT)$ with $k_0\in \{1,1.5,2\}$, choose $q\in(0,0.3]$ for the FDR control, %We always fix $\alpha = 0.5$ during the majority vote. The results of 
and plot the corresponding FDRs and TPRs %are provided 
in Figures \ref{fig:sa1} and \ref{fig:sa2} respectively. It can be seen that our method controls the FDR for any $K$ and $q$. In addition, the performance is not overly sensitive to the choice of $K$. 
\end{comment}


%is not overly sensitive to the choices of 
%performs uniformly well across different combinations of $K$ and $q$. This is also a good example to show that the performance of SEEK is not necessarily dependent on the assumption of exponential $\beta$-mixing.

 



% \begin{figure}[!htp]
% 	\centering
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/fdr_rf_n100_car_0107.png}
% 		\caption{$N=100$, i.i.d.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/fdr_rf_n200_car_0107.png}
% 		\caption{$N=200$, i.i.d.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n100_car_0107.png}
% 		\caption{$N=100$, AR noise.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.35\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/fdr_rf_ar_n200_car_0107.png}
% 		\caption{$N=200$, AR noise.}
% 	\end{subfigure}% \\
% 	\\
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/tpr_rf_n100_car_0107.png}
% 		\caption{$N=100$, i.i.d.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/tpr_rf_n200_car_0107.png}
% 		\caption{$N=200$, i.i.d.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n100_car_0107.png}
% 		\caption{$N=100$, AR noise.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.35\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/tpr_rf_ar_n200_car_0107.png}
% 		\caption{$N=200$, AR noise.}
% 	\end{subfigure}% 
% 	\\
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/value_rf_n100_car_0107.png}
% 		\caption{$N=100$, i.i.d.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/value_rf_n200_car_0107.png}
% 		\caption{$N=200$, i.i.d.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.24\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/value_rf_ar_n100_car_0107.png}
% 		\caption{$N=100$, AR noise.}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.35\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/value_rf_ar_n200_car_0107.png}
% 		\caption{$N=200$, AR noise.}
% 	\end{subfigure}% 
% 	\caption{Comparison results of the false discovery rate (FDR), the true positive rate (TPR), and the value based on SEEK-RF, %SEEK-RF+, 
% 	VS-RF, and SFS under `CartPole-v0'.}
% 	\label{fig:carpole_rf}
% 	% \vspace{-0.3cm}
% \end{figure}



% \begin{figure}[!htp]
% 	\centering
% 	\begin{subfigure}{0.32\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/SA_FDR_k0_overq.pdf} 
% 		\caption{$K =\log(NT)$}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.32\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/SA_FDR_k1_overq.pdf} 
% 		\caption{$K = 1.5\log(NT)$}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.32\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/SA_FDR_k2_overq.pdf} 
% 		\caption{$K = 2\log(NT)$}
% 	\end{subfigure}%
% 	\caption{Results of the FDR using the proposed SEEK methods under `CartPole-v0' with $N=100$, $p=100$, $\alpha = 0.5$, and AR(1) noises. All the results are aggregated over 20 runs.}
% 	\label{fig:sa1}
% 	% \vspace{-0.3cm}
% \end{figure}

% \begin{figure}[!t]
% 	\centering
% 	\begin{subfigure}{0.32\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/SA_TPR_k0_overq.pdf} 
% 		\caption{$K = \log(NT)$}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.32\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/SA_TPR_k1_overq.pdf} 
% 		\caption{$K = 1.5\log(NT)$}
% 	\end{subfigure}%
% 	\begin{subfigure}{0.32\linewidth}
% 		\centering
% 		\includegraphics[width=1\linewidth]{figs/SA_TPR_k2_overq.pdf} 
% 		\caption{$K = 2\log(NT)$}
% 	\end{subfigure}%
% 	\caption{Results of the TPR using the proposed SEEK methods under `CartPole-v0' with $N=100$, $p=100$, $\alpha = 0.5$, and AR(1) noises. All the results are aggregated over 20 runs.}
% 	\label{fig:sa2}
% 	% \vspace{-0.3cm}
% \end{figure}

\begin{comment}
\begin{sidewaystable}
\caption{Performances of all methods in AR environment}
\label{tab: ar_long}
\centering
\scriptsize
\begin{tabular}{llllllllllllllllllllll}
\hline
 & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS-LASSO} \\ \hline
\multicolumn{1}{l|}{$N$} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & \multicolumn{1}{l|}{40} & 10 & 20 & 40 \\ \hline
\multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{$K$} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & \multicolumn{1}{l|}{29.28} & / & / & \multicolumn{1}{l|}{/} & 19.60 & 24.62 & 29.28 \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.01 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.03 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 0.60 & 0.60 & 0.60 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.03 & 0.04 & \multicolumn{1}{l|}{0.03} & 0.01 & 0.02 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.07 & 0.03 & \multicolumn{1}{l|}{0.06} & 0.90 & 0.90 & 0.90 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.00 & 0.01 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.02 & 0.01 & \multicolumn{1}{l|}{0.02} & 1.00 & 1.00 & 1.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 0.89 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.96 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & \multicolumn{1}{l|}{1.00} & 1.00 & 1.00 & 1.00 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.02 & 0.06 & \multicolumn{1}{l|}{0.04} & 0.04 & 0.06 & \multicolumn{1}{l|}{0.08} & 0.08 & 0.08 & \multicolumn{1}{l|}{0.10} & 0.29 & 0.15 & \multicolumn{1}{l|}{0.16} & 0.11 & 0.14 & \multicolumn{1}{l|}{0.15} & 0.32 & 0.48 & \multicolumn{1}{l|}{0.87} & 0.35 & 0.44 & 0.51 \\ \hline
\multicolumn{1}{l|}{Value} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 2.03 & 2.06 & \multicolumn{1}{l|}{2.08} & 1.87 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.01 & 2.04 & \multicolumn{1}{l|}{2.08} & 1.98 & 2.07 & \multicolumn{1}{l|}{2.08} & 2.02 & 2.06 & \multicolumn{1}{l|}{2.07} & 1.49 & 1.77 & 1.90 \\ \hline
\end{tabular}
\end{sidewaystable}


\begin{sidewaystable}
\caption{Performances of all methods in the Mixed environment}
\label{tab: mix_long}
\centering
\scriptsize
\begin{tabular}{llllllllllllllllllllll}
\hline
 & \multicolumn{3}{c}{K=5} & \multicolumn{3}{c}{K=10} & \multicolumn{3}{c}{K=20} & \multicolumn{3}{c}{K=40} & \multicolumn{3}{c}{Selected K} & \multicolumn{3}{c}{SFS} & \multicolumn{3}{c}{VS-LASSO} \\ \hline
\multicolumn{1}{l|}{$N$} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & \multicolumn{1}{l|}{200} & 50 & 100 & 200 \\ \hline
\multicolumn{1}{l|}{$T$} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & \multicolumn{1}{l|}{150} & 150 & 150 & 150 \\ \hline
\multicolumn{1}{l|}{$K$} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & \multicolumn{1}{l|}{4.56} & / & / & \multicolumn{1}{l|}{/} & 3.22 & 4.16 & 4.56 \\ \hline
\multicolumn{1}{l|}{mFDR} & 0.01 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FDR} & 0.03 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.12 & 0.04 & \multicolumn{1}{l|}{0.05} & 0.14 & 0.09 & \multicolumn{1}{l|}{0.04} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{FPR} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.00 & 0.00 & \multicolumn{1}{l|}{0.00} & 0.04 & 0.01 & \multicolumn{1}{l|}{0.01} & 0.02 & 0.02 & \multicolumn{1}{l|}{0.01} & 0.00 & 0.00 & 0.00 \\ \hline
\multicolumn{1}{l|}{TPR} & 0.50 & 0.95 & \multicolumn{1}{l|}{1.00} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.98} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.50 & 0.50 & \multicolumn{1}{l|}{0.50} & 0.71 & 0.98 & \multicolumn{1}{l|}{1.00} & 0.51 & 0.53 & \multicolumn{1}{l|}{0.54} & 0.50 & 0.50 & 0.50 \\ \hline
% \multicolumn{1}{l|}{Speed(min)} & 0.27 & 0.37 & \multicolumn{1}{l|}{0.50} & 0.53 & 0.36 & \multicolumn{1}{l|}{0.85} & 0.39 & 0.98 & \multicolumn{1}{l|}{1.17} & 0.68 & 0.52 & \multicolumn{1}{l|}{2.21} & 0.21 & 0.39 & \multicolumn{1}{l|}{0.54} & 1.17 & 2.58 & \multicolumn{1}{l|}{6.70} & 2.15 & 3.96 & 5.07 \\ \hline
\multicolumn{1}{l|}{Value} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & \multicolumn{1}{l|}{2.19} & 2.17 & 2.19 & 2.19 \\ \hline
\end{tabular}
\end{sidewaystable}
\end{comment}




\section{Additional Algorithmic Details} %in Algorithms}
\label{apdx: b}


%\subsubsection{The construction of knockoffs and feature statistics}

%We have provided a general description of our method in the above. 
In this section, we give more details about some major steps of the proposed algorithm. 
%and discuss the flexibility of . %in which the flexibility in choosing different machine learning methods are discussed.
%\noindent
%Given one subset of the several datasets split from the original data, named $\mathcal{D}_k$, as defined by Section 3.2, we further subgroup it conditional on the value of actions, into a collection of subgroups $\{\mathcal{D}_k^{(j)}\}$, indexed by action value $j$. For each $\{\mathcal{D}_k^{(j)}\}$, we perform some regressions to compute the importance of each dimension, with which the feature statistics $W_i$, indexed by dimension $i$, combining results across all $j$ would be calculated. Using those feature statistics we can perform the knockoff selection. And we want to emphasize that the regression target is the reward, which means that this one-time procedure selects those covariates most significant in contribution to the reward. The formal procedure is included in Algorithm \ref{alg: 1}.\\

%\noindent
%In addition, we are not satisfied to apply Algorithm \ref{alg: 1} only once. Instead, we first apply it with rewards as the regression target, then treat all the state variables currently selected as new response variable, and perform regression as well as Algorithm \ref{alg: 1} iteratively to select all dimensions affecting the transition of the MDP towards important dimensions. Such iterative idea is defined in Algorithm \ref{alg: 2}.
%\\


% \begin{algorithm}
	% 	\caption{Sequential Knockoffs with Exact Construction}
	%     \hspace*{\algorithmicindent} \textbf{Input:} Data $\mathcal{D} = [\mathbf{s}, \mathbf{\theta}, \mathbf{y}]$, potential selection dimensions $ B_\text{ps} \subset \{1,...,p\}$
	% 	\begin{algorithmic}[1]
		%     	\For {each row of $\mathcal{D}$}
		%     		\For {$i=1,2,\ldots,p$}
		%     		    \State Calculate  $\mathcal{L}(S_i|S_{-i},\mathbf{\theta}, \tilde{S}_{1:i-1})$ and sample $\tilde{S}_i$ from it
		%     		    \State Compute new joint distribution
		%     		    $\mathcal{L}(S_{1:p},\mathbf{\theta}, \tilde{S}_{1:i}) = \mathcal{L}(\tilde{S}_1|S_{-1},\mathbf{\theta}, \tilde{S}_{1:i-1}) \mathcal{L}(S_{1:p},\mathbf{\theta}, \tilde{S}_{1:i-1})$
		%     		\EndFor
		%     	\EndFor
		% 		\State  Obtain the estimates for linear regression with interactions 
		
		% 		\noindent
		% 		$\mathbf{\beta} = \argmin_{ \mathbf{b}\in \mathbb{R}^{(4p+2)\times n}}
		%         ||\mathbf{y} - [\mathbf{s}, ~ \tilde{\mathbf{s}}, ~
		%         \mathbf{\theta}, ~ \mathbf{s}\odot\mathbf{\theta}, ~
		%         \tilde{\mathbf{s}}\odot\mathbf{\theta}, ~
		%         \mathbf{1}]\mathbf{b}||_2^2$
		%         where $n$ is the number of columns in $\mathbf{y}$
		
		%         \State For $i \in B_{\text{ps}}$, compute feature statistics $ W_i := \max\limits_{\mathcal{\theta}}||\mathbf{\beta}_i + \mathbf{\beta}_{i + 1 + 2p}\mathcal{\theta}|| - \max\limits_{\mathcal{\theta}}
		%         ||\mathbf{\beta}_{i + p} + \mathbf{\beta}_{i + 1 + 3p}\mathcal{\theta}||$
		
		%         \State  For knockoffs, $\tau = \min\left\{t>0: \frac{\displaystyle\#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle\#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$,
		
		%         \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau\}$
		%         \State For knockoffs+, $\tau_+ = \min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle \#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$, 
		
		%         \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau_+\}$
		% 	\end{algorithmic} 
	% \end{algorithm}

% \noindent
% We can construct the knockoffs $\tilde{\mathbf{s}}$ row by row, and in each row the value for each dimension $i$ is generated one by one from $1$ to $p$, not simultaneously. Given the joint distribution $\mathcal{L}(S_1,...,S_p,a)$, we use the conditional distribution $\mathcal{L}(S_1|S_{-1},a)$ to sample $\tilde{S}_1$, and perform such sampling row by row to gain the vector $\tilde{\mathbf{s}}_1$. Then we calculate  $\mathcal{L}(S_{1:p},a, \tilde{S}_{1:i-1})$ to proceed to generate $\tilde{\mathbf{s}}_i$ for $i = 2,...,p$, using the conditional distribution $\mathcal{L}(S_i | S_{-i},a, \tilde{S}_{1:i-1})$. Such dimension-wise sampling will construct knockoffs exactly satisfying Property 2.\\

% \noindent
% However, we would not actually apply this idea in real implementations. The computation of conditional probability in each step has quite high cost, which lead to long processing time.

% \noindent 
% After that we can fit the linear model with interactions between state variables $\mathbf{S}$ and $\mathbf{\Theta}$, where $\mathbf{\Theta}$ stands for all covariates other than state dimensions. In the primary setting of RL, $\mathbf{\Theta}$ corresponds to the dimension of actions $a$, although we still use $\mathbf{\Theta}$ for generality of algorithms. Also notice in the calculation of feature statistics $W_i$, we use
% $ ||\mathbf{\beta}_i + \mathbf{\beta}_{i + 1 + 2p}\mathcal{\theta}|| $ 
% to represent the importance of the $i$-th dimension with interactions and take maximum over all values in $\mathbf{\Theta}$ to gain the highest possible importance for this dimension. The same idea applies for the corresponding $i$-th dimension in knockoffs with 
% $ \max\limits_{\mathcal{\theta}}
%         ||\mathbf{\beta}_{i + p} + \mathbf{\beta}_{i + 1 + 3p}\mathcal{\theta}||$.\\

% \noindent
% With the above calculated, we perform the selection, where only dimensions with feature statistics positive and large enough will be selected, as described in steps 9 and 10.



% \subsubsection{Iterative Selection}

% As we emphasized in the introduction, the unique characteristic of RL, in terms of variables selection, is the implicit contribution to reward by transition among states. For example, dimension 1 directly affect the values of the immediate reward, which will likely be selected by Algorithm 1. At the meantime, dimension 2 has no direct effect on the reward, but its value contribute to the transition probability in dimension 1, i.e. $P(S_1 | S_2)$ is not uniform. Then dimension 2 should also be considered significant, although not likely to be selected by Algorithm 1. In order to take into consideration of those dimensions that indirectly contributing to the change in reward through the transition probability, we adopt the iterative selection procedure as described in Algorithm 2. First those dimensions with direct influence on reward are selected, denoted as $\hat{G}$. Then we make $\mathbf{s}^\prime (\hat{G})$ as the response data, where $\mathbf{s}^\prime (\hat{G})$ stand for the submatrix of $\mathbf{s}^\prime$ containing all dimensions in $\hat{G}$, and select further dimensions out of $\{1,...,p\}\backslash\hat{G}$. After that we glue all selected dimensions together to be the new response variable and repeat such selection until no further one can be selected.
%\QZL{We probably need to move the illustrative example after iterative selection in the previous section.}




%We give a quick example to show the performance of SEEK. Here we plug Algorithm \ref{alg: 1}, the one-time knockoffs, into the iterative procedure machine, i.e. Algorithm \ref{alg: 2}, and apply the whole procedure to the simulated data.





%In addition, $I_{\text{AR}}=\{3,...,11\}, I_{\text{WN}}=\{12,...,20\}$ are respectively two index sets of null dimensions. $I_{\text{AR}}$ corresponds to those whose transition follows AR(1), depending only on itself:
%\begin{align*}
%\mathbb{P}(S_{t,i}|\mathbf{S}_t,A_t) = \mathbb{P}(S_{t,i}|S_{t-1,i}), i= 3,...,11
%\end{align*}
%while $I_{\text{WN}}$ are just independently and identically distributed white noises, following some normal distribution with zero mean. Without considering the implicit influence on reward, we will only apply Algorithm \ref{alg: 1} once, which possibly drops dimension 2, with conclusion $\hat{G}=\{1\}$. Then there are two choices to fix it. The first is our iterative selection described in the above, while the second is still to apply Algorithm \ref{alg: 1} once, but with the response variable to be the augmented vector $[a, \mathbf{S}]$. However, the second approach will overestimate the significant dimensions, since those in $I_{\text{AR}}$ will be selected as well, due to their contribution to the transition of $\mathbf{S}$,  with conclusion $
%\hat{G}=I_\text{U}\cup I_{\text{AR}}$. So our approach not only takes into account all direct and indirect significant dimensions, but also avoids overestimate due to those autoregressive noises, which will reach a conclusion closer to $I_\text{U}$. This is verified in our results. As shown in the table,...

\begin{comment}
\subsection{Data Splitting}

The observed data consist of $N$ independent trajectories, %corresponding to $N$ independently distributed copies, 
each with horizon  $T$. Remark that even if the lengths are not consistent across trajectories, we either pick the minimum length for $T$ and keep the first $T$ steps for all trajectories, or just glue all together to have $N=1$. In either case the following results hold. And we combine all trajectories into one dataset, so that the index of time step ranges from $0$ till $NT-1$.
Assume $T/K$ is an integer, then we define the family of sub-datasets  $\mathcal{F}=\{\mathcal{D}_k, k\in[K]\}$ as the partition of raw dataset.
By such splitting, we partially reduce the dependence among tuples inside each subset. As we discussed in Section \ref{sec: seek}, after our innovative data splitting, and with the assumption of exponential $\beta$-mixing, transition tuples in each $\mathcal{D}_k$ can be roughly regarded as i.i.d samples, on which we can perform selections. For the purpose of illustration, in the whole Section. \ref{apdx: b}, we consider one generic subset $\mathcal{D}_k\in\mathbb{R}^{n\times(2p+2)}$ from $\mathcal{F}$,
where $n=NT/K$. And by the definition in Section \ref{sec: seek}, we know $\mathcal{D}_k = \{(\mathbf{S}_{k-1+(j-1)K},A_{k-1+(j-1)K},R_{k-1+(j-1)K},\mathbf{S}_{k+(j-1)K})\}_{j=1}^{NT/K}$.
\end{comment}




% \begin{algorithm}
	% 	\caption{Knockoffs for dependent data}
	% 	\label{alg: 3}
	%     \hspace*{\algorithmicindent} \textbf{Input:} Data $\mathcal{D} = [\mathbf{s}, \mathbf{a}, \mathbf{y}],$ potential selection dimensions $ B_\text{ps} \subset \{1,...,p\}$, target control $q$
	% 	\begin{algorithmic}[1]
		% 		\State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
			% 		Subgroup $\mathcal{D}$ into $\{\mathcal{D}^{(j)} = \left[\mathbf{s}^{(j)}, \mathbf{a}^{(j)}, \mathbf{y}^{(j)} \right], 1 \leq j \leq m \}$ s.t. every row of $\mathbf{a}^{(j)}$ takes the j-th distinct action value in $\mathcal{A}$
			% 		\strut}
		% 		\State Sample knockoffs $\tilde{\mathbf{s}}^{(j)}$ row by row according to some exact/approximate methods
		
		% 		\State  Obtain the summarized regression estimates $\mathbf{\beta} = [\mathbf{\beta}^{(1)T}, ..., \mathbf{\beta}^{(m)T}]^\top$ from fitting an appropriate model
		% 		$Y^{(j)}=f(\mathbf{S}^{(j)},\tilde{\mathbf{S}}^{(j)},  \beta^{(j)})$ for each $j$,
		% 		$\mathbf{\beta}^{(j)}\in \mathbb{R}^{(2p+1)\times k}$,
		%         where $k$ is the number of columns in $\mathbf{y}$
		
		%         \State For $i \in B_{\text{ps}}$, compute feature statistics $ W_i := \max\limits_j\{||\mathbf{\beta}^{(j)}_i||\} - \max\limits_j\{||\mathbf{\beta}^{(j)}_{i+p}||\}$
		
		%         \noindent
		%         where $\mathbf{\beta}^{(j)}_i$ is the i-th row in $\mathbf{\beta}^{(j)}$, $||\cdot||$ is some appropriate norm
		%         \State  For knockoffs, $\tau = \min\left\{t>0: \frac{\displaystyle\#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle\#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$,
		
		%         \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau\}$
		%         \State For knockoffs+, $\tau_+ = \min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle \#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$, 
		
		%         \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau_+\}$
		% 	\end{algorithmic} 
	% \end{algorithm}




\subsection{Action-based Sub-grouping}

% \begin{algorithm}[t!]
% %\linespread{1.25}\selectfont
% \caption{Aggregated Knockoffs} %in Iterative Selection}
% \label{alg: 2}
% \hspace*{\algorithmicindent} \textbf{Input:} Batch data $\mathcal{D}_k$ consisting of  random tuples $\{(\mathbf{S}_i, A_i, \mathbf{Y}_i): 1\le i\le NT/K \}$, a candidate index set $ B \subset \{1,...,p\}$, and a target FDR level $q \in (0, 0.5)$
% \begin{algorithmic}[1]
% \State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
% Split $\mathcal{D}_k$ into $\{\mathcal{D}_k^{(a)},a\in [m]\}$ where $\mathcal{D}_k^{(a)} = \{(\mathbf{S}_i, A_i, \mathbf{Y}_i): A_i=a\}$. %based on their action values $\mathbf{A}^{(a)} \in \mathcal{A}$.
% \strut}
% \State For each $a$, use an appropriate method on $\mathcal{D}_k^{(a)}$ to construct knockoff variables $\{\widetilde{\mathbf{S}}_i:A_i=a\}$.

% \State Apply any machine learning algorithms (e.g., LASSO and random forest) to $\{(\mathbf{S}_i, \widetilde{\mathbf{S}}_i, A_i, \mathbf{Y}_i): A_i=a\}$ to construct feature importance statistics $Z_j^{(a)}$ and $\widetilde{Z}_j^{(a)}$ for $S_j$ and its knockoff variable, respectively, for each $j\in B$.


% \State For each $j \in B$, set $Z_j=\max_{a} Z_j^{(a)}$, $\widetilde{Z}_j=\max_{a} \widetilde{Z}_j^{(a)}$ and $W_j=f(Z_j, \widetilde{Z}_j)$.
% %aggregate all $W_j^{a}$ across all action values and compute feature statistics $W_j$ that satisfies the flip-coin property.

% \State To implement the (standard) knockoffs method, set the threshold $\tau$ to  $$\min\left\{t>0: \frac{\displaystyle\#\{i\in B: W_i \leq -t\}}{\displaystyle\#\{i\in B: W_i \geq t\}} \leq q\right\},$$ \textbf{Output} $\widehat{G}_k = \{i\in B: W_i \geq \tau\}$
% \State To implement the knockoffs+ method, set the threshold $\tau_+$ to $$\min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in B: W_i \leq -t\}}{\displaystyle \#\{i\in B: W_i \geq t\}} \leq q\right\},$$ 
% \textbf{Output} $\widehat{G}_k = \{i\in B: W_i \geq \tau_+\}$
% \end{algorithmic} 
% \end{algorithm}

We first provide the intuition behind the sub-grouping. %splitting conditional on actions. %Let us emphasize that the goal for RL is to optimize the cumulative reward through generating high rewards through actions step by step. Thus the dimension of actions should always be treated as significant.
Different from the classical supervised learning setting, % we always view actions as significant variables. This raises the following challenge: 
%Then in the MDP setting lies a main difficulty: 
although both the state and action contribute to the reward function and system transition, we only care about the selection of state variables. %Generally using 
Let $I_s, I_a$ %to 
denote the index sets of states and actions respectively. %we know the total number of potential variables to be selected is strictly smaller than the total number of variables contributing to rewards (while in former work of knockoffs \citep{candes2018panning}, these two are the same). So the 
Our %ultimate 
goal is to select a subset of $I_s$ (denoted by $G$), treating the variables in $I_a$ as significant variables. %and the resulting useful dimensions are $G\cup I_a$. 
We call this a \textbf{partial selection problem}.


Under %such a 
partial selection, %problem, 
the knockoffs $\tilde{\mathbf{S}}$ in our setting should satisfy the two properties mentioned in Section \ref{sec: 2} %, but with involvement of actions.
conditional on actions. %Then to exclude actions in the selection procedure, while keeping its contribution to rewards, there are two approaches in constructing knockoffs. 
We next discuss two approaches to meet these properties. To motivate the first approach, %is to include actions on both sides of property 2, which is now
we notice that according to the exchangeability property,
\begin{eqnarray*}
	[ \mathbf{S}, \widetilde{\mathbf{S}}, \mathbf{a}]_{\text{swap}(B)}
	\overset{d}{=}
	[ \mathbf{S}, \widetilde{\mathbf{S}}, \mathbf{a}]
\end{eqnarray*}
for any index subset $B\in\{1,...,p\}$. This leads to the exact construction method %in constructing the knockoffs 
\citep[see ][Algorithm 1]{candes2018panning}, which   
%The method could be applied by 
iteratively samples the knockoff variable $\widetilde{S}_j$ for $j=1$ till $p$,
row by row. %to $\tilde{S}_p$ 
%. 
%but the cost of complexity is quite high, suggesting that it's not practical in large scale data implementation, and will not be the implementation focus of this paper.
However, such a method is computationally infeasible in moderate dimensional settings. 


The second method is to construct knockoffs that approximately satisfy the exchangeability property conditional on each distinct value of actions, as we adopt %. %and later design a feature statistics to learn from all such knockoffs and achieve a robust selection. This is incorporated mainly 
in Algorithm \ref{alg: 2}. %Given $\mathcal{A} = \{ 1,2,...,m\}$, 
 %Here in the Algorithm \ref{alg: 2} all data sets consist of %only have 3 arguments $\mathbf{S},A, \mathbf{Y}$ %, where no reward $\mathbf{R}$ or next states $\mathbf{S}^\prime$ appear. Actually they are simultaneously incorporated in the notation $\mathbf{Y}$. Recall from the outline that in the first round of selection, we use reward as response, then $\mathbf{Y}\leftarrow \mathbf{R}$, and there's no need to include $\mathbf{S}^\prime$ in the input. Then starting from the second round of selection, only state variables already selected (with index set $\widehat{G}_{k,1}$) are treated as responses, then $\mathbf{Y}\leftarrow\mathbf{S}^\prime_{\widehat{G}_{k,1}}$, and there's no need to include $\mathbf{R}$. As a result, 
%where 
We detail this method in the next subsection. %to represent either one in different iterations to make the presentation of Algorithm \ref{alg: 2} universal. Here for the first argument of input we normalize each column of states in the following way (corresponding to the column of values for each state variable): first center the entries in each column to have sample mean zero by deducting the column-wise mean; then individually divide each column by its 2-norm.

\subsection{Knockoffs Construction}
\label{apdx: b.3}
We begin with some notations. 
For each $a \in \mathcal{A}=\{ 1,2,...,m\}$, %we have the data 
let $\mathcal{D}_k^{(a)}$ denote the data subset $\left\{(\mathbf{S}_i, A_i, \mathbf{Y}_i)\in\mathcal{D}_k:A_i=a\right\}$, with size $n_{k,a}$. Notice that $\sum_a n_{k,a} = NT/K$.
Recall that $\mathbf{Y}_i$ represents either an immediate reward, or some significant states. 

For %convenience
ease of representation below, 
we write $\mathcal{D}_k$ in the matrix form, e.g., %it is
\begin{align*}
	\mathcal{D}_k = [\mathbf{s}_k,\mathbf{a}_k,\mathbf{y}_k,\mathbf{s}_k^\prime]
\end{align*}
with $\mathbf{s}_k,\mathbf{s}_k^\prime\in\mathbb{R}^{(NT/K)\times p},\mathbf{a}_k,\mathbf{r}_k\in\mathbb{R}^{NT/K}$ and each row of $\mathcal{D}_k$ corresponds to one transition tuple. Similarly, for each $a\in\mathcal{A}$, we have
\begin{align*}
	\mathcal{D}_k^{(a)} = [\mathbf{s}_k^{(a)},\mathbf{y}_k^{(a)},\mathbf{s}_k^{\prime(a)}]
\end{align*}
with $\mathbf{s}_k^{(a)},\mathbf{s}_k^{\prime(a)}\in\mathbb{R}^{n_{k,a}\times p},%\mathbf{a}_k^{(a)},
\mathbf{y}_k^{(a)}\in\mathbb{R}^{n_{k,a}}$. %We also remark that for each iteration of selection, the response data $\mathbf{y}_k\in\mathbb{R}^{(NT/K)\times d_0}$ can be either $\mathbf{a}_k$ or a submatrix of $\mathbf{s}_k^\prime$, where $d_0$ is the number of variables in the response.
\begin{remark}
We briefly discuss these notations. Upper and lower case letters refer to random variables and data realizations respectively, whereas %lower cases are for samples, and the 
boldface letters denote vectors or matrices. %No matter for variables or data, we always attach 
The subscript $k$ and/or superscript $(a)$ is included to highlight the data subset $\mathcal{D}_k^{(a)}$ obtained by data splitting and/or action sub-grouping. For example, $\mathbf{s}_k^{(a)}$ %is the vector of all state variables, with size $p\time 1$, concerned
denotes the states in the data subset $\mathcal{D}_k^{(a)}$. %while $\mathbf{s}_k$ is the sample of states with size $(NT/K)\times p$ in $\mathcal{D}_k$. And 
Finally, $S_{k,j}$ denotes the $j$th element of $\mathbf{S}_k$.
\end{remark}





On each $\mathcal{D}_k^{(a)}$, we aim to construct knockoffs %row by row in the form of  $\tilde{\mathbf{s}}_k^{(a)}\in\mathbb{R}^{n_{k,a}\times p}$
so that %in each row for any subset $B\subset \{1,...,p\}$,
\begin{align*}
	[ \mathbf{S}_k^{(a)} ~ \widetilde{\mathbf{S}}_k^{(a)} ]_{\text{swap}(B)} \overset{d}{=} [ \mathbf{S}_k^{(a)} ~ \widetilde{\mathbf{S}}_k^{(a)} ].
\end{align*}
%Such exchangeability is not a strong requirement to satisfy, and we provide several ways to sample $\tilde{\mathbf{s}}_k^{(a)}$ in below. Such range provides readers large with room in sampling, to accommodate their different needs in accuracy and efficiency.
We discuss two concrete proposals below to construct knockoffs. Meanwhile, other proposals are equally applicable as long as they meet the exchangeability assumption. 


\begin{comment}
\begin{remark}
	We briefly introduce the feasible choices to apply to the meta-algorithm of \ref{alg: 1}, and much more could be fitted in it without being mentioned here, relating to this subsection on knockoffs construction, as well as the following two subsections on machine learning model fitting and design of feature statistics. As far as readers can justify their new choices through the standard machine of proof provided in Section \ref{apdx: c.2}.
\end{remark}
\end{comment}



\subsubsection{Second-order Machine (Gaussian Sampling)}%\\
%If we believe the random design 
Suppose $\mathbf{S}_k^{(a)}$ follows a multivariate normal distribution with mean zero (we normalized the state in practice to meet this assumption), covariance matrix $\Sigma_k^{(a)}$. %given each
%conditional on that the action equals $a\in\mathcal{A}$. 
In order to satisfy the exchangeability assumption, %in $(\mathbf{S}_k^{(a)},\widetilde{\mathbf{S}}_k^{(a)})$, we only need to 
it suffices to construct $\widetilde{\mathbf{S}}_k^{(a)}$
%in the form of $n_{k,a}\times p$ matrix by the conditional distribution in r.v.'s:
according to the following, 
\begin{align*}
	\tilde{\mathbf{S}}_k^{(a)}|\mathbf{S}_k^{(a)} \overset{d}{=}
	\mathcal{N}(\mathbf{\mu}_k^{(a)}, V_k^{(a)}),
\end{align*}
where $(\mathbf{\mu}_k^{(a)}, V_k^{(a)})$ is given by %the conditional distribution in multivariate normal:
\begin{align*}
	\mathbf{\mu}_k^{(a)} &= \mathbf{S}_k^{(a)} - \mathbf{S}_k^{(a)} (\Sigma_k^{(a)})^{-1} \text{diag}\{\mathbf{d}_k^{(a)}\},\\
	V_k^{(a)} &= 2\text{diag}\{\mathbf{d}_k^{(a)}\} - \text{diag}\{\mathbf{d}_k^{(a)}\} (\Sigma_k^{(a)})^{-1} \text{diag}\{\mathbf{d}_k^{(a)}\}.
\end{align*}
%where we have asssumed $\mathbf{S}_k^{(a)} \sim \mathcal{N}(\mathbf{0}, \Sigma_k^{(a)})$. We perform sampling row by row, and the generated samples is the $n_{k,a}\times p$ matrix $\tilde{\mathbf{s}}_k^{(a)}$.

%$$
%\tilde{\mathbf{s}}^{(j)} = \mathbf{s}^{(j)}(\mathbf{I} - \Sigma^{-1}\text{diag}\{\mathbf{d}\}) + \tilde{U}C
%$$
%where
%\begin{enumerate}
%  \item $\Sigma = \mathbf{s}^{(j)^\top}\mathbf{s}^{(j)}$
%  \item $\text{diag}\{\mathbf{d}\} \geq \mathbf{0},$ and %$ 2\Sigma \geq \text{diag}\{\mathbf{d}\} , \mathbf{d} \in \mathbb{R}^p $
%  \item $\tilde{U} \in \mathbb{R}^{n_j\times p}$ be orthonormal matrix with $\tilde{U}^\top \mathbf{s}^{(j)} = \mathbf{0}$
%  \item $C^\topC = 2\text{diag}\{\mathbf{d}\} - \text{diag}\{\mathbf{d}\}\Sigma^{-1}\text{diag}\{\mathbf{d}\} \geq \mathbf{0}$ and $C$ is obtained by Cholesky decomposition
%\end{enumerate}


\begin{remark}
	As proven in Appendix \ref{apdx: preliminary}, such  a construction leads to the following joint distribution:
	\begin{align*}
		(\mathbf{S}_k^{(a)}, \widetilde{\mathbf{S}}_k^{(a)}) \sim
		\mathcal{N}(\mathbf{0}, \mathbf{U}_k^{(a)}),
		\mathbf{U}_k^{(a)}=
		\begin{bmatrix}
			\Sigma_k^{(a)} & \Sigma_k^{(a)} - \text{diag}\{\mathbf{d}_k^{(a)}\}\\
			\Sigma_k^{(a)} - \text{diag}\{\mathbf{d}_k^{(a)}\}& \Sigma_k^{(a)}
		\end{bmatrix}
		\geq \mathbf{0}.
	\end{align*}
	%which is proved in .  
 The diagonal elements $\mathbf{d}_k^{(a)}\in\mathbb{R}^p$ shall be chosen such that $\mathbf{U}_k^{(a)}$ is positive semidefinite %and all its 
 with all entries being nonnegative. On the other hand, each entry in $\mathbf{d}_k^{(a)}$ should be as large as possible to reduce the correlation between $S_k^{(a)}$ and $\widetilde{S}_k^{(a)}$. One could adopt the approximate semidefinite program %is adopted with details introduced 
	developed in \citet[][]{candes2018panning} to determine $\mathbf{d}_k^{(a)}$.
\end{remark}
\begin{remark}
%In addition, since we have normalized the data column-wise, we know the above Gaussian idea 
The above construction %is essentially
%is trying 
%to 
allows the first two moments of $\widetilde{S}_k^{(a)}$ to match those of $S_k^{(a)}$.
%matches the first two moments between $S_k^{(a)}$ and $\widetilde{S}_k^{(a)}$.  %even 
Thus, we refer to this method as a second-order (sampling) machine.
In practice, it works well even without the %assumption of multivariate 
normality assumption.  
%distribution in state variables, and with sample size large enough, such approximation can be close to achieving such goal. As a result, 
\end{remark}

\subsubsection{Deep Sampling}

%In some cases 
Alternatively, we %want to sample
can construct the knockoff variables by matching higher order moments %. Toward that end, we train a 
%based on 
using deep neural networks \citep[see e.g.,][]{romano2020deep}. The basic idea is to construct an objective function $J(S_k^{(a)},\widetilde{S}_k^{(a)})$ which measures the %extent to which $\widetilde{S}_k^{(a)}$ are good approximate knockoffs 
discrepancy between $S_k^{(a)}$ and $\widetilde{S}_k^{(a)}$ in distribution and %means, second moments and higher order moments. Then 
apply a deep generative model to %is then applied to construct $\widetilde{S}_k^{(a)}$ that %through 
minimizes this objective function.

\begin{comment}
\textbf{3. Other choices}\\
There are potentially other choices. For example, if we look closely at the expected equivalence
\begin{eqnarray*}
	[ \mathbf{S}, \tilde{\mathbf{S}}, A]_{\text{swap}(B)}
	\overset{d}{=}
	[ \mathbf{S}, \tilde{\mathbf{S}}, A]
\end{eqnarray*}
for each row in $\mathcal{D}_k$, we can iteratively sample $S_{k,j}$ one by one for $j\in[p]$, using the Algorithm 1 in \citet{candes2018panning}. But such implementation is quite slow in practice, though it guarantees the exchangeability in distribution without assuming any specific distribution for the random design in state variables.
\end{comment}
% \begin{algorithm}
	% 	\caption{Sequential Knockoffs with Exact Construction}
	%     \hspace*{\algorithmicindent} \textbf{Input:} Data $\mathcal{D} = [\mathbf{s}, \mathbf{\theta}, \mathbf{y}]$, potential selection dimensions $ B_\text{ps} \subset \{1,...,p\}$
	% 	\begin{algorithmic}[1]
		%     	\For {each row of $\mathcal{D}$}
		%     		\For {$i=1,2,\ldots,p$}
		%     		    \State Calculate  $\mathcal{L}(S_i|S_{-i},\mathbf{\theta}, \tilde{S}_{1:i-1})$ and sample $\tilde{S}_i$ from it
		%     		    \State Compute new joint distribution
		%     		    $\mathcal{L}(S_{1:p},\mathbf{\theta}, \tilde{S}_{1:i}) = \mathcal{L}(\tilde{S}_i|S_{-1},\mathbf{\theta}, \tilde{S}_{1:i-1}) \mathcal{L}(S_{1:p},\mathbf{\theta}, \tilde{S}_{1:i-1})$
		%     		\EndFor
		%     	\EndFor
		% 		\State  Obtain the estimates for linear regression with interactions 
		
		% 		\noindent
		% 		$\mathbf{\beta} = \argmin_{ \mathbf{b}\in \mathbb{R}^{(4p+2)\times n}}
		%         ||\mathbf{y} - [\mathbf{s}, ~ \tilde{\mathbf{s}}, ~
		%         \mathbf{\theta}, ~ \mathbf{s}\odot\mathbf{\theta}, ~
		%         \tilde{\mathbf{s}}\odot\mathbf{\theta}, ~
		%         \mathbf{1}]\mathbf{b}||_2^2$
		%         where $n$ is the number of columns in $\mathbf{y}$
		
		%         \State For $i \in B_{\text{ps}}$, compute feature statistics $ W_i := \max\limits_{\mathcal{\theta}}||\mathbf{\beta}_i + \mathbf{\beta}_{i + 1 + 2p}\mathcal{\theta}|| - \max\limits_{\mathcal{\theta}}
		%         ||\mathbf{\beta}_{i + p} + \mathbf{\beta}_{i + 1 + 3p}\mathcal{\theta}||$
		
		% %        \State  For knockoffs, $\tau = \min\left\{t>0: \frac{\displaystyle\#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle\#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$,
		
		% %        \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau\}$
		% %        \State For knockoffs+, $\tau_+ = \min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle \#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$, 
		
		% %        \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau_+\}$
		% 	\end{algorithmic} 
	% \end{algorithm}





%by the conditional distribution in r.v.'s:

%$$
%\tilde{\mathbf{S}}^{(j)}|\mathbf{S}^{(j)} \overset{d}{=}
%\mathcal{N}(\mathbf{\mu}^{(j)}, V^{(j)})
%$$
%with $(\mathbf{\mu}^{(j)}, V^{(j)})$ given by the conditional distribution in multivariate normal:

%\begin{align*}
%    \mathbf{\mu}^{(j)} &= \mathbf{S}^{(j)} - \mathbf{S}^{(j)} (\Sigma^{(j)})^{-1} \text{diag}\{\mathbf{d}^{(j)}\}\\
%    V^{(j)} &= 2\text{diag}\{\mathbf{d}^{(j)}\} - \text{diag}\{\mathbf{d}^{(j)}\} (\Sigma^{(j)})^{-1} \text{diag}\{\mathbf{d}^{(j)}\}
%\end{align*}

%\noindent
%where we have asssumed %$\mathbf{S}^{(j)} \sim \mathcal{N}(\mathbf{0}, \Sigma^{(j)})$. We perform sampling row by row, and the generated samples is the $n_j\times p$ matrix $\tilde{\mathbf{s}}^{(j)}$.\\


%\noindent
%\textbf{Remark}. Such construction then leads to the following joint distribution:
%$$
%(\mathbf{S}^{(j)}, \tilde{\mathbf{S}}^{(j)}) \sim
%\mathcal{N}(\mathbf{0}, G^{(j)})
%$$
%$$ G^{(j)}=
%\begin{bmatrix}
%\Sigma^{(j)} & \Sigma^{(j)} - \text{diag}\{\mathbf{d}^{(j)}\}\\
%\Sigma^{(j)} - \text{diag}\{\mathbf{d}^{(j)}\}& \Sigma^{(j)}
%\end{bmatrix}
%\geq \mathbf{0}
%$$
%which are proved in the Appendix.  In addition, $\mathbf{d}^{(j)}\in\mathbb{R}^p$ is chosen such that $G^{(j)}$ is positive semidefinite and $d_j\geq 0, j\in[p]$. Each of its entry should be as large as possible to reduce the similarity between $S_j$ and $\tilde{S}_j$. As a result, the approximate semidefinite program is adopted with details introduced in \citet[][]{candes2018panning}.\\




\subsection{Feature Importance Statistics}%Consolidated Model Fitting}
Conditional on $A=a$, each state variable $S_{k,j}^{(a)}$ (and its knockoff $\widetilde{S}_{k,j}^{(a)}$) in the augmented data subset $\widetilde{\mathcal{D}}_k^{(a)}$ (by augmenting $\mathcal{D}_k^{(a)}$ with $\tilde{s}_{k}^{(a)}$) has its own feature importance statistics, denoted by $Z_{k,j}^{(a)}$ (respectively, $\widetilde{Z}_{k,j}^{(a)}$). To aggregate them over different actions, we define 
\begin{align*}
	Z_{k,j} = \max\limits_a Z_{k,j}^{(a)},\qquad
	\widetilde{Z}_{k,j} = \max\limits_a \widetilde{Z}_{k,j}^{(a)}.
\end{align*}
Alternative aggregation methods can be considered as well. After that, we use an anti-symmetric function $f$ to calculate the $W$-statistics. 
\begin{comment}
for all $j$ by
\begin{align*}
    W_j = f(Z_{k,j},\widetilde{Z}_{k,j} )
\end{align*}
\end{comment}
In our implementation, we set %with $W$-statistic to 
$f(u,v)$ to $u-v$, and show that the resulting $W$-statistic satisfies the flip-sign property in 
%as long as they 
%Actually, more designs of feature importance statistics are also possible, as long as $W_{k,j}=f(Z_{k,j},Z_{k,j}^{(a)})$ satisfy the asymmetry as formally defined and proved in 
Lemma \ref{lemma: c.3} of Appendix \ref{apdx: preliminary}.
We next discuss the construction of $Z_{k,j}^{(a)}$ (and $\widetilde{Z}_{k,j}^{(a)}$) based on three %statistical and 
machine learning methods. 

\begin{comment}
We then perform some machine learning black box methods for model fitting respectively on $\mathcal{D}_k^{(a)}$ for each $a$, to obtain a collection of coefficient vectors $\beta_k^{(a)}\in\mathbb{R}^{(2p+1)
	\times d_0}$, to form
\begin{align*}
	\mathbf{\beta}_k := [\mathbf{\beta}_k^{(1)\top}, ~ \mathbf{\beta}_k^{(2)\top}, ..., \mathbf{\beta}_k^{(m)\top}]^\top
\end{align*}
Here $2p+1$ corresponds to the total number of covariates in $(\mathbf{S}_{k}^{(a)}, \tilde{\mathbf{S}}_{k}^{(a)}) = (\{\mathbf{S}_{k,j}^{(a)}\}_{j=1}^p, \{\widetilde{\mathbf{S}}_{k,j}^{(a)}\}_{j=1}^p)$ and the intercept, while $d_0$ corresponds to the number of dimensions in the response $\mathbf{Y}_k$. By such fitting, those coefficients can primarily reveal the importance of each variable, on top of which we can calculate action-wise feature importance measures as $Z_{k,j}^{(a)}$ and $\widetilde{Z}_{k,j}^{(a)}$, with the calculation method depending on the specific model used.

It should be emphasized that the choices of models are quite flexible, as introduced below. That is why nonlinearity in unknown reward prediction function $r(\mathbf{S}_t, A_t)$ as well as that in transition probability $P(\mathbf{S}_{t+1}|\mathbf{S}_t, A_t)$ are incorporated. Different models could also be tried in order to learn specific environment.

No matter which model is used, as long as action-wise feature importance $Z_{k,j}^{(a)}$'s and $\widetilde{Z}_{k,j}^{(a)}$'s are calculated for all $a\in\mathcal{A}$, we summarize the importance measure across $a$ by scalars $Z_{k,j}$ and $\widetilde{Z}_{k,j}$ for all $j$, the method of which is introduced in the next subsection.

The following are some models that could be considered:
\end{comment}
%\noindent
\subsubsection{Penalized Linear Regression}%\\% with penalties}\\
The first approach is to perform penalized linear regressions via LASSO \citep[][]{tibshirani1996regression}, SCAD \citep[][]{fan2001variable}, or MCP \citep[][]{zhang2010nearly} to the data subset $\mathcal{D}_k^{(a)}$ to obtain the feature importance statistics. %$\beta_{k,a}$'s.
% \begin{align*}
	% \mathbf{\beta}_k = \argminb_{\substack{\mathbf{b}=[\mathbf{b}_1^\top, ..., \mathbf{b}_m^\top]^\top\\ \mathbf{b}_a\in \mathbb{R}^{2p+1}}}
	% \sum\limits_{1\leq a \leq m}
	% ||\mathbf{y}_k^{(a)} - [\mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ~ \mathbf{1}]\mathbf{b}_a||_2^2 + \text{penalty}
	% \end{align*}
\begin{comment}
Especially we could impose different penalties with methods like LASSO \citep[][]{tibshirani1996regression}, SCAD \citep[][]{fan2001variable}, MCP \citep[][]{zhang2010nearly}, etc. 
\end{comment}
%After the fitting, 
For each $a$, denote the estimated coefficients by %vector contains
\begin{align*}
	\widehat{\mathbf{B}}^{(a,k)} = [\widehat{\mathbf{B}}^{(a,k)}_1,...,\widehat{\mathbf{B}}^{(a,k)}_{p+1}],
\end{align*}
where $\widehat{\mathbf{B}}^{(a,k)}_i\in\mathbb{R}^{2p}$ is the coefficient vector for $i$-th response variable, and the $j$-th row in $\widehat{\mathbf{B}}^{(a,k)}$ corresponds to all the coefficients with the $j$-th state variable, i.e. $S_{k,j}^{(a)}$ if $j\leq p$, $\widetilde{S}_{k,j-p}^{(a)}$ if $p+1\leq j \leq 2p$.
Then the feature importance $Z_{k,j}^{(a)}$ and $\widetilde{Z}_{k,j}^{(a)}$ can be measured by 
\begin{align*}
	Z_{k,j}^{(a)} &= \max_{i\in I}|\widehat{B}^{(a,k)}_{j,i}|, \qquad 1\leq j\leq p,\\
	\widetilde{Z}_{k,j}^{(a)} &= \max_{i\in I}|\widehat{B}^{(a,k)}_{j+p,i}|, \qquad 1\leq j\leq p.
\end{align*}
where $I$ denoted the index set for all the current response variables.
\subsubsection{Random Forest}%\\
For each $a\in\mathcal{A}$, we set $Z_{k,j}^{(a)}$ and $\widetilde{Z}_{k,j}^{(a)}$ to the corresponding variable importance score of the predictors $\mathbf{S}_{k,j}^{(a)}$ and $\widetilde{\mathbf{S}}_{k,j}^{(a)}$ output by the random forest algorithm. %performed on data $\mathcal{D}_k^{(a)}$.\\
%\noindent
\subsubsection{Deep Neural Network}%\\
Alternatively, we can fit a deep neural network to the data, and combine the coefficients in each layer to calculate the variable importance scores $Z_{k,j}^{(a)}$ and $\widetilde{Z}_{k,j}^{(a)}$ \citep[see][]{lu2018deeppink}.



% \noindent
% \textbf{Step 4: Feature Statistics}\\

% \noindent
% %\textbf{Method 1}
% For $1\leq i \leq p$,
% $$
% W_i := \max\limits_j\{|\mathbf{\beta}^{(j)}_i|\} - \max\limits_j\{|\mathbf{\beta}^{(j)}_{i+p}|\}
% $$
% where $\mathbf{\beta}^{(j)}_i$ is the i-th entry of the coefficient vector $\mathbf{\beta}^{(j)}$.\\



% \noindent
% \textbf{Step 5: Control under target FDR on $\mathcal{D}$}\\

% \noindent
% 1. Knockoffs
% $$\tau = \min\left\{t>0: \frac{\displaystyle\#\{i: W_i \leq -t\}}{\displaystyle\#\{i: W_i \geq t\}} \leq q\right\}$$
% Selection is $\hat{G} = \{i: W_i \geq \tau\}$\\


% \noindent
% 2.Knockoffs+
% $$\tau_+ = \min\left\{t>0: \frac{\displaystyle 1 + \#\{i: W_i \leq -t\}}{\displaystyle \#\{i: W_i \geq t\}} \leq q\right\}$$
% Selection is $\hat{G} = \{i: W_i \geq \tau_+\}$\\

%\subsection{Design of Feature Importance Statistics}


\begin{comment}
Such flexibility allowed in our algorithm is quite important, as it leaves enough space to design scenario-based statistics to represent the importance for each state variable, which helps interpret the selection results.




\subsection{Knockoff Selection}

Here we provide further details on how a selection is performed. Adopting the background and notations in Section \ref{sec: 2}, the false discovery proportion (FDP) for a selection $\widehat{\mathcal{I}}$ made by a given method is
\begin{align*}
    \text{FDP}(\widehat{\mathcal{I}}) := \frac{\#(\widehat{\mathcal{I}}\cap\mathcal{H}_0)}{\#\widehat{\mathcal{I}}}
\end{align*}
And then we know for such method, $\text{FDR}=\mathbf{E}\lbrace \text{FDP}(\widehat{\mathcal{I}})\rbrace$.

%We also remark that in real implementations, 
In practice, it can happen that
there is no such $\tau>0$ %such 
that
$
\# \lbrace
j\,:\, W_j \leq -\tau \rbrace/ \# \lbrace j\,:\, W_j \geq \tau 
\rbrace \le q 
$
holds. In that case, we set $\tau_q$ or $\tau_{q+}$ to $\infty$. %which makes
Then we output $\widehat{\mathcal{I}}=\emptyset$. %And then, by convention, $\text{FDP}(\emptyset)=0$ (since no null is selected).
\end{comment}
%\subsection{Majority Vote}
%We recommend to fix the threshold $\alpha=0.5$. 
%For each subset $\mathcal{D}_k$, we can perform above iterative procedure to arrive at a selection $\hat{G}_k$. Then we can conclude with the final selection $\hat{G}$ by majority vote with threshold $\alpha \in (0, 1)$. The reason for adopting such approach is two-fold. First, since we split the raw data set into $K$ subsets, especially all subsets have the same size, we need to design a way, not only summarizing the selection results from all $\mathcal{D}_k$'s, but also make their contribution to $G$ even across $k$. Second, due to randomness in the distributions of transitions tuples as well as in random errors, a correction is necessary. Then the majority vote keeps those variables selected across $k$ with high frequencies, which satisfies the two needs. In this work $\alpha = 0.5$ is suggested.

% where
% \begin{align*}
	% \hat{G} := \{i \in \{1,...,p\}: \frac{\Sigma_{1\leq k\leq\log(nT)\}}\mathbf{1}(i \in \hat{G}_k) }{\log(nT)} \geq \alpha\}
	% \end{align*}

% \textbf{Iterative machine}\\
% Another highlight of our method is the iterative selection. It is possible that some state dimensions, the collection of which is denoted by $G_1$, contribute to the reward directly, while some other significant ones influence the transition probability for the change of values in $G_1$, which makes their contribution to rewards significant but indirect. If we denote such collection as $G_2$, we should know such indirect contribution is also important, since in the MDP setting, the dependence of $G_1$ on $G_2$ corresponds to the partial transition of the environment. So we should actually select all dimensions in either $G_1$ or $G_2$. Then we go on with such idea to find all those dimensions significant on the transition to $G_1\cup G_2$ and see the chain of contributions towards reward as
% \begin{align*}
	% \text{reward} \longleftarrow G_1 \longleftarrow G_1 \cup G_2 \longleftarrow
	% \longleftarrow G_1 \cup G_2 \cup G_3 \longleftarrow
	% \cdots
	% \end{align*}
% To dig out all direct and indirect variables that are significant, we first apply sequential knockoffs, with response variable being the reward, to gain the first selection set $\hat{G}$. Then we treat the current $\hat{G}$ as vector valued response variable, and apply sequential knockoffs only out of the potential dimensions $\{1,...,p\}\backslash G_1$ to have additional selection $\Delta G$, and update the current selection as $\hat{G} = \hat{G}\cup \Delta G$. Finally repeat such iterative selection until no further dimension can be selected.




% \subsection{Second Order Machine}


% \subsubsection{From exactness to approximation}

% Although in Section 3.3 we have described ideas to exactly construct knockoffs by iterative conditional sampling, in practice we can still go further to speed up the procedure to reach the selection results. Since our main requirement is to ensure the equivalence in distributions for the augmentation of variables and their knockoffs before and after swapping, we can have an approximate equivalence by equaling the first two moments. Note also that under normal assumptions, such approximation is actually exact.

% % \subsubsection{Normal assumptions}


% % An important assumption is that the random design $\mathbf{S}$ follows normal distribution with zero mean. In addition, we can also assume for r.v.'s $\mathbf{r} \in \mathbb{R}^n, \mathbf{s} \in \mathbb{R}^{n\times p}, \mathbf{a} \in \mathbb{R}$ with coefficients $\mathbf{\beta}^{(a)} \in \mathbb{R}^p$ dependent on actions, we have $\mathbf{r} = \mathbf{s} \mathbf{\beta}^{(a)} + \mathbf{\epsilon}$ with $ \mathbf{\epsilon} \sim \mathbb{N}(0, \sigma^2 \mathbf{I})$, for each action $a$ fixed.

% \subsubsection{Knockoffs construction for normal design}


% \begin{algorithm}
	% 	\caption{Second-order sequential knockoffs}\label{alg: 4}
	%     \hspace*{\algorithmicindent} \textbf{Input:} Data $\mathcal{D} = [\mathbf{s}, \mathbf{\theta}, \mathbf{y}],$ potential selection dimensions $ B_\text{ps} \subset \{1,...,p\}$
	% 	\begin{algorithmic}[1]
		% 		\State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
			% 		Split $\mathcal{D}$ into $\{\mathcal{D}^{(j)} = \left[\mathbf{s}^{(j)}, \mathbf{\theta}^{(j)}, \mathbf{y}^{(j)} \right], 1 \leq j \leq m \}$ s.t. every row of $\mathbf{\theta}^{(j)}$ takes the j-th distinct value
			% 		\strut}
		% 		\For {$j=1,2,\ldots,m$}
		% 		    \State  \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{Sample $\tilde{\mathbf{S}}^{(j)}$ row by row from $\mathcal{N}(\mathbf{\mu}^{(j)}, V^{(j)})$ with 
			
			% 		    $\mathbf{\mu}^{(j)} = \mathbf{\mathbf{S}}^{(j)} - \mathbf{\mathbf{S}}^{(j)} (\Sigma^{(j)})^{-1} \text{diag}\{\mathbf{d}^{(j)}\}$,
			
			% 		    $V^{(j)} = 2\text{diag}\{\mathbf{d}^{(j)}\} - \text{diag}\{\mathbf{d}^{(j)}\} (\Sigma^{(j)})^{-1} \text{diag}\{\mathbf{d}^{(j)}\}$
			% 		    \strut}
		% 		\EndFor
		% 		\State  Obtain the regression estimates
		
		% 		\noindent
		% 		$\mathbf{\beta} = \argmin_{\substack{\mathbf{b}=[\mathbf{b}_1^\top, ..., \mathbf{b}_m^\top]^\top\\ \mathbf{b}^{(j)}\in \mathbb{R}^{(2p+1)\times k}}}
		%         \sum\limits_{1\leq j \leq m}
		%         ||\mathbf{y}^{(j)} - [\mathbf{s}^{(j)} ~ \tilde{\mathbf{s}}^{(j)} ~ \mathbf{1}]\mathbf{b}^{(j)}||_2^2
		%         + \text{penalty}
		%         $
		
		%         \noindent
		%         where $k$ is the number of columns in $\mathbf{y}$
		
		%         \State For $i \in B_{\text{ps}}$, compute feature statistics $ W_i := \max\limits_j\{||\mathbf{\beta}^{(j)}_i||\} - \max\limits_j\{||\mathbf{\beta}^{(j)}_{i+p}||\}$
		
		%         \noindent
		%         where $\mathbf{\beta}^{(j)}_i$ is the i-th row in $\mathbf{\beta}^{(j)}$
		%         \State  For knockoffs, $\tau = \min\left\{t>0: \frac{\displaystyle\#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle\#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$,
		
		%         \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau\}$
		%         \State For knockoffs+, $\tau_+ = \min\left\{t>0: \frac{\displaystyle 1 + \#\{i\in B_\text{ps}: W_i \leq -t\}}{\displaystyle \#\{i\in B_\text{ps}: W_i \geq t\}} \leq q\right\}$, 
		
		%         \noindent selection is $\hat{G} = \{i\in B_\text{ps}: W_i \geq \tau_+\}$
		% 	\end{algorithmic} 
	% \end{algorithm}


% % \begin{algorithm}
	% % 	\caption{Iterative Second-order MX Knockoffs in States}
	% %     \hspace*{\algorithmicindent} \textbf{Input:} Data $\mathcal{D} = [\mathbf{s}, \mathbf{a}, \mathbf{r}, \mathbf{s}^\prime]$
	% % 	\begin{algorithmic}[1]
		% % 	    \State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
			% % 	    Perform Algorithm 2 on data $\mathcal{D}_0 = [\mathbf{s}, \mathbf{a}, \mathbf{r}]$ and potential selection
			% % 	    dimensions $ B_0 = \{1,...,p\}$,
			% %         to get initial selection $\hat{G}$
			% %         \strut}
		% % 	    \State Define the selection-to-add $d\hat{G}$ and let $d\hat{G} = \hat{G}$
		% % 		\While {$d\hat{G} \neq \emptyset$}
		% % 		    \State \parbox[t]{\dimexpr\textwidth-\leftmargin-\labelsep-\labelwidth}{
			% % 		    Perform Algorithm 2 on data $\mathcal{D}_1 = [\mathbf{s}, \mathbf{a}, \mathbf{s}^\prime]$ and potential selection
			% % 		    dimensions $ B = \{1,...,p\}\setminus \hat{G}$, 
			% %             to get new selection-to-add $d\hat{G}$
			% %             \strut}
		% %             \State $\hat{G} \leftarrow \hat{G} \cup d\hat{G}$
		% % 	    \EndWhile
		% % 	\end{algorithmic} 
	% % \end{algorithm}


% Given the flexibility in implementing Algorithm \ref{alg: 1}, we provide formal description on each step below for clarification.


% \textbf{Step 1: Conditional Subgrouping}\\
% Given $\mathcal{A} = \{ 1,2,...,m\}$, for $j \in \mathcal{A}$, let
% \begin{align*} 
	% \mathcal{D}^{(j)} &:=  \{\text{row}\in \mathcal{D}: \text{action} = j\}
	% =  \left[\mathbf{s}^{(j)}, \mathbf{a}^{(j)}, \mathbf{r}^{(j)}, \mathbf{s}^{\prime(j)}\right]
	% \end{align*}
% with $\mathbf{s}^{(j)}\in\mathbb{R}^{n_j\times p}, \mathbf{a}^{(j)}\in\mathbb{R}^{n_j}, \mathbf{r}^{(j)}\in\mathbb{R}^{n_j}, \mathbf{s}^{\prime(j)}\in\mathbb{R}^{n_j\times p}, n_j\gg p, \sum_j n_j = n$. Here we normalize columns of $\mathbf{s}^{(j)}$ in the following way: first center the entries in each column to have sample mean zero by deducting the column-wise mean; then individually divide each column by its 2-norm.

% \textbf{Step 2: Knockoffs Construction}\\


% % which then satisfies
% % \begin{enumerate}
	% %     \item $\tilde{\mathbf{S}}^{(j)}\indep\mathbf{R}^{(j)}|~\mathbf{S}^{(j)}$
	% %     \item $[ \mathbf{S}^{(j)} ~ \tilde{\mathbf{S}}^{(j)} ]_{\text{swap}(B)}^\top [ \mathbf{S}^{(j)} ~ \tilde{\mathbf{S}}^{(j)} ]_{\text{swap}(B)} \overset{d}{=} [ \mathbf{S}^{(j)} ~ \tilde{\mathbf{S}}^{(j)} ]^\top [ \mathbf{S}^{(j)} ~ \tilde{\mathbf{S}}^{(j)} ]$\\ for any subset $B\subset [1,2,...,p]$
	% % \end{enumerate}


% \textbf{Step 3: Consolidated Regressions}\\
% Nonlinearity is also incorporated with the help of neural network.

% \textbf{Step 4: Feature Statistics}\\
% %\textbf{Method 1}
% For $1\leq i \leq p$,
% \begin{align*}
	% W_i := \max\limits_j\{|\mathbf{\beta}^{(j)}_i|\} - \max\limits_j\{|\mathbf{\beta}^{(j)}_{i+p}|\}
	% \end{align*}
% where $\mathbf{\beta}^{(j)}_i$ is the i-th entry of the coefficient vector $\mathbf{\beta}^{(j)}$.

% % \noindent
% % \textbf{Method 2}
% % For $1\leq i \leq p$,
% % $$
% % W_i := \max\limits_j W^{(j)}_i = \max\limits_j\{|\mathbf{\beta}^{(j)}_i| - |\mathbf{\beta}^{(j)}_{i+p}|\}
% % $$
% % where $W^{(j)}_i := |\mathbf{\beta}^{(j)}_i| - |\mathbf{\beta}^{(j)}_{i+p}|$. In below I will show why this method cannot work. So actually we should stick with Method 1.\\

% \textbf{Step 5: Control under target FDR on $\mathcal{D}$}\\
% 1. Knockoffs
% \begin{align*}
	% \tau = \min\left\{t>0: \frac{\displaystyle\#\{i: W_i \leq -t\}}{\displaystyle\#\{i: W_i \geq t\}} \leq q\right\}
	% \end{align*}
% Selection is $\hat{G} = \{i: W_i \geq \tau\}$
% %\textbf{Theorem 1}. $\textbf{E}\left[\frac{\displaystyle|\hat{G}\cap \mathcal{H}_0|}{\displaystyle|\hat{G}| + 1/q}\right] \leq q$\\


% 2. Knockoffs+
% \begin{align*}
	% \tau_+ = \min\left\{t>0: \frac{\displaystyle 1 + \#\{i: W_i \leq -t\}}{\displaystyle \#\{i: W_i \geq t\}} \leq q\right\}
	% \end{align*}
% Selection is $\hat{G} = \{i: W_i \geq \tau_+\}$
% %\textbf{Theorem 2}. $\textbf{E}\left[\frac{\displaystyle|\hat{G}\cap \mathcal{H}_0|}{\displaystyle|\hat{G}| \vee 1}\right] \leq q$


\subsection{Additional Details in the  K-Selection Algorithm}
\label{apdx: k_select}
%To have more efficient and accurate estimation on the $\beta$-mixing coefficient, we also incorporate importance sampling. After gaining the estimated densities $\hat{f}^{(1)}, \hat{f}_K^{(2)}$, the straightforward way to calculate $\tilde{\beta}_K$ is by Monte Carlo (MC) algorithm with uniform sampling, i.e.,
We employ importance sampling to evaluate the integral in \eqref{eqn:initialestmodel} when estimating the $\beta$-mixing coefficients. To simplify the calculation, we assume $\pi_b$ and $\pi_b'$ are approximately the same so that $\beta(k)$ can be approximated by $\int_{s,s'} |f(s) f(s')-f_k(s,s')|/2$. A naive method is to uniformly sample $(S_i,S_i')$ over the product space $\mathcal{S}\times \mathcal{S}$ and estimate $\beta(k)$ by
    \begin{align}\label{eqn:naiveest}
        %\tilde{\beta}_K = \frac{1}{2}\frac{\sum_{i=1}^n \hat{f}_K^{(2)}(\mathbf{x}_i) - [\hat{f}^{(1)}(\mathbf{x}_i)]^2}{n}
        \frac{|\mathcal{S}|^2}{2M} \sum_{i=1}^M |\widehat{f}(S_i) \widehat{f}(S_i')-\widehat{f}_k(S_i,S_i')|,
    \end{align}
where $\widehat{f}$ and $\widehat{f}_k$ denote the corresponding estimators for $f$ and $f_k$, respectively. However, this method requires the state space to be bounded. In addition, the resulting estimator may suffer from a large variance when the area of the state space is large. 
    
%where $\mathbf{x}_i$ corresponds to every data point and $n$ is still the sample size. However, 
In our implementation, we propose to sample $(S_i,S_i')$ according to a mixture distribution $1/2[\widehat{f}(S_i) \widehat{f}(S_i')+\widehat{f}_k(S_i,S_i')]$. This yields the following estimator %apply importance sampling by defining $q(\mathbf{x}) = (\hat{f}_K^{(2)}(\mathbf{x}) + \hat{f}^{(1)}(\mathbf{x}))/2$ as the proposal distribution, the one according to which we sample data points. The the resulting estimation, after canceling common constant factors, is by
\begin{align*}
    \frac{1}{M}\sum_{i=1}^M\frac{|\widehat{f}(S_i) \widehat{f}(S_i')-\widehat{f}_k(S_i,S_i')|}{|\widehat{f}(S_i) \widehat{f}(S_i')+\widehat{f}_k(S_i,S_i')|}.
\end{align*}
Different from \eqref{eqn:naiveest}, the above importance sampling ratio is strictly smaller than $1$, reducing the variance of the resulting estimator. When $\widehat{f}$ and $\widehat{f}_k$ are consistent, it can be shown that its variance is upper bounded by $\beta(k)/M$ asymptotically. 
%which not only has the same consistent estimation as the trivial MC, but also has variance upper bounded by $O(n^{-1})$.
 
\begin{comment}
\section{Additional Theoretical Results}\label{remark_thy}
\subsection{Ordinary Least Squares}
\vspace{-0.3cm}
Given the underlying true model is linear, we denote the true coefficient vector by $\beta\in\mathbb{R}^{2p}$, where the entries for the first $p$ are just the underlying values for the $p$ state variables, while $\beta_j=0$ for all $p+1\leq j \leq 2p$, since they correspond to the knockoff variables manually constructed. We also use $\hat{\beta}$ to denote the one calculated by OLS from data. The same notations are also used in LASSO. For notation convenience, we sometimes use $n=NT/K$ to denote the sample size of each data subset $\mathcal{D}_k, k\in[K]$.
\begin{cond} [OLS estimation bound]\label{ass: ols} 
By OLS estimation, exist constants $c_1,c_2, C_1>0$, s.t. for $n$ large, with high probability of at least $1-c_1n^{-c_2}$, 
\begin{align*}
    C_1\log(n)n^{-1/2} &\geq ||\hat{\beta}_i^{(a)} - \beta_i^{(a)}||_1 ~ 
\end{align*}
for any $a\in\mathcal{A}$ and $i\in [p]$.
\end{cond}

\begin{remark}
    Above Condition \ref{ass: ols} is usually observed in practice, which could be shown from the commonly used mild conditions, detailed in below.
\end{remark}


% \begin{cond} [Enough signals] \label{ass: signal_ols}
%    Exists constant $C_2>0$, s.t. $|\{j\in[p]: \beta_j^{(a)} > 0 ~ \text{for some } a\in\mathcal{A} \}| \geq 1/q+C_2$.
% \end{cond}


% \begin{remark}
%     The above conditions are actually imposed in each data subset $k\in[K]$, and only for those iteration $l\in[p]$, when before the $l$-th selection, there are still true variables, with respect to the current response variables (defined in Algorithm \ref{alg: 2}), left out. Without loss of generality, we omit the superscripts $(k,l)$ for all related variables and constants.  The same idea will be adopted in condition statements in LASSO and general ML cases. It should be emphasized that all final theoretical results are for the whole procedure of SEEK, aggregating all iterations and data subsets. See Appendix \ref{remark_thy} for further explanation.
% \end{remark}

% \noindent
% Due to the performance in bounding the OLS estimation errors, true state variables tend to have large $|W_j|$'s, which leads to the following theorem. 
\begin{theorem} \label{thm: power_ols}
    With Condition \ref{ass: ols}, the power of SEEK using ordinary least squares is lower bounded with
    \begin{align*}
        \textbf{Power} := \mathbf{E}\left(\frac{|\hat{G}\cap G|}{|G|}\right) \geq 1 - O\{n^{-c_2}  + K^{-1}(NT)^{-c}\}
    \end{align*}
    which guarantees that the asymptotic power is  1.
\end{theorem}

% \begin{remark}
%     In this result we allow significant flexibility in the choice of target FDR $q$. If $q$ is globally fixed throughout the implementation of SEEK, independent of the increase of $n$, then the convergence requirement, $n^{-c_2}q\rightarrow 0$, is automatically satisfied. What is more interesting is that $q$ could even slowly increase, as long as its speed is dominated by $n^{c_2}$ and $K(NT)^c$, but then it provides high flexibility in procedure design.
% \end{remark}


\vspace{-0.6cm}

\subsection{OLS}
\subsubsection{Common assumptions to achieve good estimation bound}
For each action $a\in\mathcal{A}$, true model is linear with sub-Gaussian errors: $y_i=\mathbf{x}_i\beta^{(a)} + \epsilon_i$. Full rank: $n>2p$ (here sample size $n$ is for the data subset after split by K) and the input data matrix $[\mathbf{X}\tilde{\mathbf{X}}]\in \mathbb{R}^{n\times 2p}$ has full rank $2p$. Independence: $\epsilon\indep \mathbf{x}$.  i.i.d. error: $\mathbb{E}(\mathbf{\epsilon}^\top \mathbf{\epsilon}|\mathbf{X})=\sigma^2 \mathbf{I}$. With such Condition \ref{ass: ols} could be achieved as a well known result.

\subsubsection{Omitting superscripts}
% Even if the constants in conditions are different for different iteration $i$ or subset $k$, say by Condition \ref{ass: ols}, the consistency is with probability $1-c_1^{(k,l)}n^{-c_2^{(k,l)}}$, we can take $c_1 := \max_{k,l}c_1^{(k,l)}$ and $c_2 = \min_{k,l}c_2^{(k,l)}$. With the same idea, for the Condition \ref{ass: signal_ols}, we take $C_2 := \min_{k,l}C_2^{(k,l)}$. Then for any results below, for each $(k,l)$ pair, the results also hold when removing superscripts $(k,l)$. 
Actually all conditions, for OLS, LASSO and general ML cases, are written by omitting some superscripts, and to be exact, we require those to be true for each $i$-th iteration in each data subset $\mathcal{D}_k$. In that sense, solely for the conditions, we actually have constants (like $c_1,c_2$, etc) and parameters (like $p,p_0,n$, etc), indicated by superscripts $k,l$. For those diverging constants, we just omit superscripts for notation convenience. In addition, taking some corresponding max/min in those constants would make sure such ignorance does not influence results (though we do not even bother to do this, as explained in the following paragraph). 

Another reason to omit corresponding superscripts lies in how we conduct the proof. By checking all results in independent data, for either OLS, LASSO or general ML methods, it is obvious that when proving the power for temporally dependent data case, all we need from independent case is the large probability with which the selection power is 1. So all above constants find no place in further steps of proof. So such ignorance makes no difference.
This notation simplification also applies to all related proof.

In addition, we only require all conditions to hold for all those $i$-th iterations, when before such iteration, there exist true variables, with respect to the current response variables ($\bigcup_{h=1}^{i-1}\widehat{G}_{k,h}$), not selected . In notations this means after $i-1$ iterations of selections, $\widetilde{G}_{k,i} \neq \emptyset$, where $\widetilde{G}_{k,i} := \{j\in [p]: \max_{a\in\mathcal{A}} \beta_j^{(k,l),(a)} > 0\}\backslash \bigcup_{h=1}^{i-1}\widehat{G}_{k,h}$. (Remark that $\widetilde{G}_{k,i}$ could be different from $G_i$, as explained in Section \ref{sec: temp_ols}) This  restriction on iterations already guarantees that whenever there is some true signals, all conditions would hold, leading to the high power as shown in the theory part. In addition, we do not want to impose such assumption on all iterations, so that when there are only nulls (still w.r.t $\bigcup_{h=1}^{i-1}\widehat{G}_{k,h}$) left, the power is not always high, to avoid selecting too many noises.

\subsubsection{Enough strong signals}
On one hand, Condition \ref{ass: strong_lasso} leads to the fact that the selection size is large enough for each iteration, which is formally stated by Lemma \ref{lemma: sel_size} and proved. Only with this intermediate result, we could know that the thresholding $T$ will not be too large, compared to the $W_j$'s for trues, so that more true variables could be included. On the other hand, since each selection size is lower bounded, the total number of selections for each data subset $\mathcal{D}_k$ is upper bounded by some term related to target FDR $q$. Then by  choosing the value of $q$ depending on $n$ as $n$ increases, the power of SEEK can be finally guaranteed.

\subsubsection{Rate in q}\label{q_rate}
According to the conditions in Theorem \ref{thm: power_ols}, $q$ is allowed to be increasing with respect to $n$ very slowly. In addition, even we want $q$ to be decreasing in $n$, it cannot decay to zero. The reason is due to Condition \ref{ass: signal_ols}, which tells us
\begin{align*}
    p_0 \geq \frac{1}{q} + C_2
    \Rightarrow
    q \geq \frac{1}{p_0 - C_2}
\end{align*}
since in the case of OLS, $p_0>C_2>0$ is fixed, we know even $q$ decreases, it at most converges to $1/(p_0-C_2)$. 



\subsection{LASSO}
% \subsubsection{Omitting superscripts}
% Actually all conditions are written by omitting some superscripts, and to be exact, we require those to be true for each $i$-th iteration in each data subset $\mathcal{D}_k$. In that sense, solely for the conditions, we actually have constants indicated by superscripts $k,l$, although as in OLS case, taking some corresponding max/min would make sure such ignorance does not influence results.

% %including $\kappa_n^{(k,l)}, \beta_j^{(a),(k,l)}, c^{(k,l)}, c_1^{(k,l)}, c_2^{(k,l)}, c_0^{(k,l)}, C_0^{(k,l)}, \lambda^{(k,l)}, c_\lambda^{(k,l)}, C^{(k,l)}$. Then for results,  we choose $c_1 := \max_{k,l}c_1^{(k,l)}, c_2 := \min_{k,l}c_2^{(k,l)}, c_0 := \max_{k,l}c_0^{(k,l)}, C_0 := \max_{k,l}C_0^{(k,l)}, c_\lambda := \max_{k,l}c_\lambda^{(k,l)}, C := \min_{k,l}C^{(k,l)},  c := \min_{k,l}c^{(k,l)}$, with which propositions and final results hold when removing the $(k,l)$ on those terms whose max/min's have been taken.

% Another reason to omit corresponding superscripts lies in how we conduct the proof. By checking all results in independent data, for either OLS, LASSO or general ML methods, it is obvious that when proving the power for temporally dependent data case, all we need from independent case is the large probability with which the selection power is 1. So all above constants find no place in further steps of proof.

\subsubsection{Interpretation of power bounds}
For both LASSO and general ML cases, the lower bound of the power is close to 1, with a negligible loss increasing in $q$. Due to the selection method, whenever a thresholding value $T$ satisfies the control of estimated FDP by the target FDR $q$, as detailed in Algorithm \ref{alg: 2}, then it automatically satisfies such control by a larger value in target FDR, making the power in one selection non-decreasing in $q$. And for the whole method of SEEK, as an obvious remark to the proof below, we know the power from a larger $q$ would share the same lower bound derived by s smaller $q$, which is the intuition gained from such algorithm. Then at first glance it seems that the power analysis is against the intuition just mentioned. However, there is actually no contradiction between the theories and intuition. And the short answer is that the theoretical results are for lower bound of power, but not for the power itself, and especially the actual power does not need to be around such bound. In addition, lower bounds are not unique.

Since the idea is exactly the same, we just take the general ML case for an instance. Then to be exact, assume we have already a set of parameters in the settings, like $p,p_0,n,q$, all constants related and so on, fixed and satisfying Condition \ref{ass: sep} and \ref{ass: signals_ml}. Then the result in Theorem \ref{thm: power_ml} holds, the lower bound in which is denoted by $lb_q$. When we only change $q$ to a larger value $q^\prime > q$ while keeping all the others unchanged, it is obvious that all assumptions are still satisfied and now, re-running the whole proof,  we can have another power lower bound result in $q^\prime$, denoted by $lb_{q^\prime}$ which is lower than that for $q$, i.e. $lb_q$. By the above intuition, the actual power by $q^\prime$ shares the same lower bound with that by $q$. So for the $q^\prime$ compared to $q$, we can see it has 2 valid lower bounds, which leads to no contradiction.

If we go further from the observation in the last paragraph, the reason we use a small and even vanishing $q$ is that we need the lower bound to be high enough. If we choose a large $q$, the lower bound could even be close to zero, providing limited information on the asymptotics in power itself. While with vanishing $q$, the lower bound is proved to be close to 1, pushing the power itself to 1 as well. Finally note that for every such pair $q,q^\prime$ with $q<q^\prime$, since both lower bounds, $lb_q, lb_{q^\prime}$, are lower bounds for the power in $q^\prime$, we only need to focus on the highest lower bound, which in turn supports our use of small and vanishing $q$ in the presentation and proof of the power lower bound.
\end{comment}


\section{Proofs}
\label{apdx: proof}

\subsection{Proofs of Propositions}

\subsubsection{Proof of Proposition \ref{prop1}}
Under the MDP assumption, %For the MDP concerned here, 
%we know 
the optimal policy is %always 
greedy with respect to the optimal $Q$-function, denoted by $Q^\ast$ \citep[][Section 6]{puterman2014markov}. %Especially 
By the Bellman optimality equation, we have
\begin{align*}
	Q^\ast(\mathbf{S},A)
	&= \mathbf{E}\left[R + \gamma\max\limits_{a\in\mathcal{A}}Q^\ast(\mathbf{S}^\prime,a)|\mathbf{S},A\right]\\
	&= \mathbf{E}\left[R |\mathbf{S},A\right] +
	\mathbf{E}\left[ \gamma\max\limits_{a\in\mathcal{A}}Q^\ast(\mathbf{S}^\prime,a)|\mathbf{S},A\right],
\end{align*}
almost surely. 
Next, according to the definition of sufficient state, we notice that the two conditional expectations in the second line depend on the state only through $\mathbf{S}_G$. As such, $Q^*$ as well as the optimal policy depend on the state only through $\mathbf{S}_G$.
%$Q^\ast$, in terms of the form in the sum of two expectations, only depend on state variables and actions through $\mathbf{S}_G,A$, which completes the proof.

\subsubsection{Proof of Proposition \ref{prop2}}
Since $\mathcal{M}$ is an MDP, it must satisfy the following Markov properties\\
1. (Markov property for the next state) 
For any $t\geq 0$,
\begin{align*}
	\mathbb{P}(\mathbf{S}_{t+1}|\{\mathbf{S}_{j},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t},A_t)
	= \mathbb{P}(\mathbf{S}_{t+1,}|\mathbf{S}_{t},A_t)
\end{align*}
2. (Markov property for the reward) %(Conditional mean independence property) 
For any $t\geq 0$,
\begin{align*}
	\mathbb{P}(R_{t}|\{\mathbf{S}_{j},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t},A_t)
	= \mathbb{P}(R_{t}|\mathbf{S}_{t},A_t)
\end{align*}
To prove Proposition \ref{prop2}, we aim to show the reduced data generating process based on the sufficient state satisfies such two Markov properties as well. To save space, we only show the Markov property holds for the next state, 
%For the process restricted to sufficient state, first we want to show the Markov property, 
i.e., for any $t\geq 0$,
\begin{align*}
	\mathbb{P}(\mathbf{S}_{t+1,G}|\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t,G},A_t)
	= \mathbb{P}(\mathbf{S}_{t+1,G}|\mathbf{S}_{t,G},A_t).
\end{align*}
This is equivalent to require
\begin{align*}
	\mathbf{S}_{t+1,G}\indep 
	\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t}
	|\mathbf{S}_{t,G},A_t.
\end{align*}
To prove the conditional independence, we first notice that, it follows from the Markov property of the full model $\mathcal{M}$ that
\begin{align*}
	\mathbf{S}_{t+1,G}\indep 
	\{\mathbf{S}_{j,G},\mathbf{S}_{j,G^c},A_j,R_j\}_{0 \leq j < t}
	|\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t.
\end{align*}
Then by the decomposition rule of conditional independence\footnote{\url{https://en.wikipedia.org/wiki/Conditional_independence}},
%\textcolor{red}{haha, delete?},
\begin{align*}
	\mathbf{S}_{t+1,G}\indep 
	\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t}
	|\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t
\end{align*}
On the other hand, by the definition of sufficient state, we have that
\begin{align*}
	\mathbf{S}_{t+1,G}\indep 
	\mathbf{S}_{t,G^c}|\mathbf{S}_{t,G},A_t.
\end{align*}
Finally, combining the above two observations with the contraction rule of conditional independence, we have that
\begin{align*}
	\mathbf{S}_{t+1,G}\indep 
	\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t,G^c}
	|\mathbf{S}_{t,G},A_t
\end{align*}
Again applying the decomposition rule yields the desired %we can prove the 
Markov property. Especially, due to the time-homogeneity of $\mathcal{M}$, the reduced process is also time-homogeneous.

\begin{comment}
Then we go on to show the conditional mean independence property, i.e., for any $t\geq 0$,
\begin{align*}
	\mathbf{E}(\mathbf{R}_{t}|\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t,G},A_t)
	= \mathbf{E}(\mathbf{R}_{t}|\mathbf{S}_{t},A_t)
\end{align*}
First note that
\begin{align*}
	\mathbf{E}(\mathbf{R}_{t}|\{\mathbf{S}_{j,G},\mathbf{S}_{j,G^c},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t)
	= \mathbf{E}(\mathbf{R}_{t}|\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t)
\end{align*}
which leads to
\begin{align*}
	\mathbf{E}(\mathbf{R}_{t}|\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t)
	= \mathbf{E}(\mathbf{R}_{t}|\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t)
\end{align*}
In addition, by definition of sufficient state,
\begin{align*}
	R_t\indep S_{t,G^c}|\mathbf{S}_{t,G},A_t
\end{align*}
Then we can perform further simplification
\begin{align*}
	\mathbf{E}(\mathbf{R}_{t}|\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t)
	&= \int \mathbf{r}_t f_{\mathbf{R}_{t}|\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t}(\mathbf{r}|\mathbf{x},\mathbf{y},a) d\mathbb{P}\\
	&= \int \mathbf{r}_t f_{\mathbf{R}_{t}|\mathbf{S}_{t,G},A_t}(\mathbf{r}|\mathbf{x},a) d\mathbb{P}\\
	&= \mathbf{E}(\mathbf{R}_{t}|\mathbf{S}_{t,G},A_t)
\end{align*}
Combining the above facts we have
\begin{align*}
	\mathbf{E}(\mathbf{R}_{t}|\{\mathbf{S}_{j,G},A_j,R_j\}_{0 \leq j < t},\mathbf{S}_{t,G},\mathbf{S}_{t,G^c},A_t)
	= \mathbf{E}(\mathbf{R}_{t}|\mathbf{S}_{t,G},A_t)
\end{align*}
the decomposition of which leads to the second property we want. Then we can conclude $\mathcal{M}$ restricted to $G$ is also an MDP.

In above we repeated use the following fact:\\
\begin{lemma} [Decomposition of independence in conditional expectation] For random variables $W,X,Y,Z$, if $\mathbf{E}(W|X,Y,Z)=\mathbf{E}(W|Z)$, then $\mathbf{E}(W|X,Z)=\mathbf{E}(W|Z)$.
\end{lemma}

\textbf{Proof}. If we use $f$ to denote corresponding probability density, then by the assumption,
\begin{align*}
	\int f_{W|X,Y,Z}(w|x,y,z)w d\mathbb{P}_w
	= \int f_{W|Z}(w|z)w d\mathbb{P}_w
\end{align*}
Then observe
\begin{align*}
	\mathbf{E}(W|X,Z)
	&= \int f_{W|X,Z}(w|x,z)w d\mathbb{P}_w\\
	&= \int\int f_{W|X,Y,Z}(w|x,y,z)w d\mathbb{P}_y d\mathbb{P}_w\\
	&= \int\int f_{W|X,Y,Z}(w|x,y,z)w d\mathbb{P}_w d\mathbb{P}_y\\
	&= \int\int f_{W|Z}(w|z)w d\mathbb{P}_w d\mathbb{P}_y\\
	&= \int f_{W|Z}(w|z)w d\mathbb{P}_w\\
	&= \mathbf{E}(W|Z)
\end{align*}
by Fubini's theorem.
\end{comment}

\subsubsection{Proof of Proposition \ref{prop3} and \ref{prop4}} 
\textbf{Part 1 of the proof of Proposition \ref{prop3} (existence)}. Denote the set of all index sets for sufficient state as $\mathcal{F}_\text{sf}$. First we know from the definition, $[p]\in\mathcal{F}_\text{sf}$, meaning that $\mathcal{F}_\text{sf}$ is nonempty. In addition, for any $G_1,G_2\in\mathcal{F}_\text{sf}$, we can define the order $\leq_\text{sf}$ by $G_1\leq_\text{sf}G_2$ iff $|G_1|\leq|G_2|$. Then we can see $\leq_\text{sf}$ is a total order on $\mathcal{F}_\text{sf}$ with cardinality of elements lower bounded by $0$. Then the minimal sufficient state defined by
\begin{align*}
	G^\ast = \argmin\limits_{G\in\mathcal{F}_\text{sf}}|G|
\end{align*}
always exists.

If we further assume the strict positivity of the transition kernel,
% i.e., for any $s_i,s_i^\prime\in\mathcal{S}_i,i\in[p]$ and any $a\in\mathcal{A}$,
% \begin{align*}
	%     \mathbb{P}(s_1^\prime,...,s_p^\prime|s_1,...,s_p,a) >0
	% \end{align*}
%we claim that 
$G^\ast$ is unique, as shown below. %We will combine this proof with the proof for Proposition 4 below.

\textbf{Part 2 of the proof of Proposition \ref{prop3} (uniqueness) and proof of Proposition \ref{prop4}}. We first discuss the 3 methods one by one to prove Proposition \ref{prop4}.

\textbf{Reward-only approach}: %A counter-example is enough, where we just use 
The toy example described in Section \ref{sec: minimal} demonstrates the insufficiency of the state selected by the reward-only approach. First, we show $\mathbf{S}_{G_M}=\mathbf{S}_{\{1,2\}}$ is a sufficient state. Due to the fact only $S_{t,1}$ contributes to reward, we have for any $t\geq 0$ that
\begin{align*}
	R_t\indep \mathbf{S}_{t, \{1\}^c}|(S_{t,\lbrace 1\rbrace},A_t)
\end{align*}
%And by the property of 
According to the weak union rule of conditional independence, we have that 
\begin{align*}
	R_t\indep \mathbf{S}_{t, G_M^c}|(\mathbf{S}_{t,G_M},A_t)
\end{align*}
In addition, since the state transition %towards the change 
in the first two dimensions only depend on themselves, we have that
\begin{align*}
	\mathbf{S}_{t+1,G_M}\indep \mathbf{S}_{t, G_M^c}|(\mathbf{S}_{t,G_M},A_t).
\end{align*}
%which shows 
This implies that $\mathbf{S}_{G_M}$ is a sufficient state. Furthermore, %from above 
we notice for any index set $G$, if $1\notin G$, then Markov property for the reward will be violated. If $2\notin G$, %conditional mean independence property would not hold. Then 
the Markov property for the next state will be violated. Then for any $G\in\mathcal{F}_\text{sf}$, we must have $1,2\in G$. This leads to our conclusion that $\mathbf{S}_{G_M}$ is the unique minimal sufficient state in this scenario.

%Back to 
As for the reward-only approach, it only selects the first state variable and is thus insufficient. 
%any such method will only select $\{1\}$, which is not even sufficient.

\textbf{One-step approach}: Any such method would %make a selection 
select a subset $G$ such that
\begin{align*}
	(R_t,\mathbf{S}_{t+1,G})\indep \mathbf{S}_{t, G^c}|(\mathbf{S}_{t,G},A_t).
\end{align*}
It follows from the decomposition rule of conditional independence that
\begin{align*}
	R_t
	&\indep \mathbf{S}_{t, G^c}|(\mathbf{S}_{t,G},A_t)\\
	\mathbf{S}_{t+1,G}
	&\indep \mathbf{S}_{t, G^c}|(\mathbf{S}_{t,G},A_t).
\end{align*}
%which shows 
As a result, %that 
$\mathbf{S}_{G}$ is a sufficient state.

However, the selected subset is not guaranteed to be minimally sufficient. Again, consider the toy example in Section \ref{sec: minimal}. %we used. 
Any one-step method would select $G_M\cup G_{\text{AR}}$, which includes some redundant variables $G_{\text{AR}}$.

\textbf{Iterative approach}: For a given MDP $\mathcal{M}$, %any appropriate 
the iterative method proceeds as follows. First, all state variables directly contributing to reward are selected, whose index set is %of which 
denoted by $G_R$ (the uniqueness of $G_R$ will be proven later), %with the fact 
such that for any $t$,
\begin{align}\label{eqn:sigreward}
	R_t\indep \mathbf{S}_{t, G_R^c}|(\mathbf{S}_{t,G_R},A_t).
\end{align}
Next, in the second step we select those states (among $\{G_R^c\}$) %out of $[p]\backslash G_R$, 
that contribute to the state transition of of $\mathbf{S}_{G_R}$, denoted as $G_1$, %in the sense that
such that
\begin{align*}
	\mathbf{S}_{t+1,G_R}
	\indep \mathbf{S}_{t, (G_R\cup G_1)^c}|(\mathbf{S}_{t,G_R},\mathbf{S}_{t,G_1},A_t).
\end{align*}
Similarly, in the third step we select those among $\{G_R^c\cap G_1^c\}$ %all out of $[p]\backslash (G_R\cup G_1)$, 
that contribute to the state transition function of $(\mathbf{S}_{G_R},\mathbf{S}_{G_1})$, denoted as $G_2$, in the sense that
\begin{align*}
	(\mathbf{S}_{t+1,G_R},\mathbf{S}_{t+1,G_1})
	\indep \mathbf{S}_{t, (G_R\cup G_1\cup G_2)^c}|(\mathbf{S}_{t,G_R},\mathbf{S}_{t,G_1},\mathbf{S}_{t,G_2},A_t)
\end{align*}
We %continue such iterations 
repeat the iterations until no more state can be selected. %the total time of which 
The number of iterations is always finite, and is upper bounded by $p$. %For convenience, we denote the total times of selection, excluding the first one with respect to reward, as $m$ (dependent on $\mathcal{M}$), and for sure $m\leq p$. Then our conclusion from any iterative method is always  
Let $m+1$ denote the total number of iterations. The selected subset is given by
$G_\text{iter} = (\cup_{1\leq j \leq m}G_j) \cup G_R$. %And we emphasize such selection satisfies that 
According to the selection procedure, we have for $1\leq j \leq m-1$ that
\begin{align*}
	(\mathbf{S}_{t+1,G_R},\mathbf{S}_{t+1,G_1},...,\mathbf{S}_{t+1,G_j})
	\indep \mathbf{S}_{t, (G_R\cup G_1\cup...\cup G_{j+1})^c}|(\mathbf{S}_{t,G_R},\mathbf{S}_{t,G_1},...,\mathbf{S}_{t,G_{j+1}},A_t)
\end{align*}
Since the procedure stops after $m+1$ iterations, %means 
it implies that the following termination condition is met
\begin{align*}
	\mathbf{S}_{t+1,G_\text{iter}}
	\indep \mathbf{S}_{t, G_\text{iter}^c}|(\mathbf{S}_{t,G_\text{iter}},A_t).
\end{align*}
%What's more, 
In addition, by \eqref{eqn:sigreward} and the weak union rule, %property,
\begin{align*}
	R_t
	\indep \mathbf{S}_{t, G_\text{iter}^c}|(\mathbf{S}_{t,G_\text{iter}},A_t).
\end{align*}
%which shows that 
Therefore, $\mathbf{S}_{G_\text{iter}}$ is a sufficient state. We emphasize that the sufficiency relies on the consistency of the selection algorithm. This condition is imposed in the statement of Proposition \ref{prop4}.

%It remains to show that 
We next prove that when the transition probability is strictly positive, %under the positivity assumption, 
each subset $G_R$, $G_1,\cdots,G_m$ is uniquely defined. This implies that the iterative method will output a unique subset $\mathbf{S}_{G_\text{iter}}$. %and will not get stuck in 
%Finally let us recall the additional assumption that transition probability is strictly positive. 
For convenience, we focus on the case when $\mathcal{S}$ is finite. %and remark that the following 
Meanwhile, the proof %is exactly the same for continuous case, just 
is applicable to the continuous state space setting as well, by replacing %transition probability by 
the transition probability mass function with the probability density function. %and mass probability by probability measure. For example, whenever we say there exists some point in $\mathcal{S}$ in the finite case, it corresponds to say there exists a subset of $\mathcal{S}$ with positive measure. Here we should emphasize that in each iteration, without such 
We emphasize that without the positivity assumption, the selected subset may not be unique or minimally sufficient. 
%due to the dependence among variables. 

We begin by considering the first iteration where reward is the target and prove the uniqueness of $G_R$ by contraction. By definition, $G_R$ does \textbf{not} contain the index of a state $S_j$ if and only if %such that
\begin{eqnarray}\label{eqn:sigrewardj}
    R_t\indep S_{t,j}|(\mathbf{S}_{t, \{j\}^c},A_t).
\end{eqnarray}
%Before we discuss different cases, we should be clear that, when we say $\mathbf{S}_{G_R}$ is the significant variables selected in the first iteration, and the selection algorithm is consistent, we actually mean the following (when such consistency is realized):
\begin{comment}
1. $
r(\mathbf{S}_t,A_t) = 
r_1 (\mathbf{S}_{t,G_R},A_t)
$
over $\Pi_{j=1}^p \mathcal{S}_j\times\mathcal{A}$ for some deterministic function $r_1$;\\
2. For each nonempty subset $G_0\subset G_R$, there exists $a\in\mathcal{A}$, a vector $\mathbf{x}\in\Pi_{j\in G_R\backslash G_0} \mathcal{S}_j$ and vectors $\mathbf{y}\neq \mathbf{z}$ with $\mathbf{y}, \mathbf{z}\in \Pi_{j\in  G_0}\mathcal{S}_j$, such that \begin{align*}
	r_1 (\mathbf{S}_{t,G_R\backslash G_0}=\mathbf{x},S_{t, G_0}=\mathbf{y},A_t=a)
	\neq  r_1 (\mathbf{S}_{t,G_R\backslash G_0}=\mathbf{x},S_{t, G_0}=\mathbf{z},A_t=a)
\end{align*}
where the second one is to guarantee the significance of each variables without redundancy, while the first one is for sufficiency without missing. In below we refer to such two properties as the first and second requirement for repeated use.
\end{comment}
%We prove out statement by contradiction. Then 
%we assume 
%In other words, %suppose there is another 
For any index set $\Lambda\subset[p]$ that is different from $G_R$, such that $\mathbf{S}_\Lambda$ satisfies the conditional independence assumption in \eqref{eqn:sigreward}, e.g., 
\begin{eqnarray}\label{eqn:sigreward0}
R_t\indep \mathbf{S}_{t, \Lambda^c}|(\mathbf{S}_{t,\Lambda},A_t),
\end{eqnarray}
we aim to show $G_R\subseteq \Lambda$. Suppose $G_R - \Lambda \neq \emptyset$. Let $\mathcal{A}_t(s)$ denote the support of $A_t$ conditional on $\mathbf{S}_t=\mathbf{s}$. Since the transition function is strictly positive, for any $s\in \mathcal{S}, t\ge 1$ and any $a\in \mathcal{A}_t(s)$, the probability $\mathbb{P}(\mathbf{S}_t=\mathbf{s}, A_t=a)$ is strictly positive as well. From \eqref{eqn:sigreward} and weak union rule of conditional independence we have
\begin{eqnarray}\label{eqn:sigreward_weak}
    R_t\indep \mathbf{S}_{t, G_R^c\cap \Lambda}| (\mathbf{S}_{t, G_R\cap \Lambda}, \mathbf{S}_{t, G_R\cap \Lambda^c},\mathbf{S}_{t, G_R^c\cap \Lambda^c},A_t)
\end{eqnarray}
It follows from \eqref{eqn:sigreward0}, \eqref{eqn:sigreward_weak} and the intersection rule of conditional independence that 
\begin{eqnarray}\label{eqn:sigreward2}
    R_t\indep \mathbf{S}_{t, \Lambda^c \cup G_R^c}|(\mathbf{S}_{t,\Lambda\cap G_R},A_t),
\end{eqnarray}
By the assumption $G_R-\Lambda \neq \emptyset$, there exists some $j_0\notin \Lambda \cap G_R$ such that $j_0 \in G_R$, and then $j_0\in\Lambda^c \cup G_R^c$.
This together with \eqref{eqn:sigreward2} and the weak union rule yields that
\begin{eqnarray*}
    R_t\indep S_{t,j_0}|(\mathbf{S}_{t,\{j_0\}^c},A_t),
\end{eqnarray*}
which contradicts \eqref{eqn:sigrewardj}. As such, we must have $G_R\subseteq \Lambda$. Consequently, $G^R$ is the intersection of all the sets $\Lambda$ satisfying \eqref{eqn:sigreward0} and is hence uniquely defined. Similarly, we can show that each subset $G_j$ is unique as well. The uniqueness of $G_{\text{iter}}$ is thus proven.

%When the probability mass function of 

%is also the significant vector for reward, i.e., $\mathbf{S}_\Lambda$ satisfies the above two requirements, with another deterministic function $r_2$.
\begin{comment}
we discuss two separate cases:\\
Case 1: If $|G_R|=|\Lambda|$, then by inequality of sets we know $G_R^c\cap\Lambda\neq\emptyset$ and 
$G_R\cap\Lambda^c\neq\emptyset$.\\
If their cardinalities are different, and without loss of generality, say $|G_R|<|\Lambda|$, then obviously $G_R^c\cap\Lambda\neq\emptyset$. In addition, if $G_R\cap\Lambda^c = \emptyset$, then we immediately have $G_R\subset\Lambda$. But then $\Lambda\backslash G_R\neq\emptyset$, which then guarantees that there exists $a\in\mathcal{A}$, a vector $\mathbf{x}\in\Pi_{j\in  G_R}\mathcal{S}_j$ and vectors $\mathbf{y}\neq \mathbf{z}$ with $\mathbf{y}, \mathbf{z}\in \Pi_{j\in \Lambda\backslash G_R} \mathcal{S}_j$, such that \begin{align*}
	r_2 (S_{t, G_R}=\mathbf{x},\mathbf{S}_{t,\Lambda\backslash G_R}=\mathbf{y},A_t=a)
	\neq  r_2 (S_{t, G_R}=\mathbf{x},\mathbf{S}_{t,\Lambda\backslash G_R}=\mathbf{z},A_t=a)
\end{align*}
But we already know $r(\mathbf{S}_t,A_t)$ is already a constant given the vector $S_{t, G_R}=\mathbf{x}$ fixed, then it leads to a contradiction. As a result, in the second case when cardinalities are different, we still have $G_R\cap\Lambda^c\neq\emptyset$. Combining above two cases we conclude it always holds that
\begin{align*}
	G_R^c\cap\Lambda\neq\emptyset ~\text{and}~ 
	G_R\cap\Lambda^c\neq\emptyset
\end{align*}
This is important, in the sense that we can now fix a vector $x_0\in\Pi_{j\in  G_R^c\cap\Lambda}\mathcal{S}_j$, and by above definitions we also know there exists $a\in\mathcal{A}$, a vector $\mathbf{x}_1\in\Pi_{j\in  G_R\cap\Lambda}\mathcal{S}_j$ and vectors $\mathbf{y}\neq \mathbf{z}$ with $\mathbf{y}, \mathbf{z}\in \Pi_{j\in G_R\cap\Lambda^c} \mathcal{S}_j$, such that \begin{align*}
	r_1 (\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{y},A_t=a)
	\neq  r_1 (\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z},A_t=a)
\end{align*}
Especially by the consistence in reward across different representations by different significant variables, we can always choose $y$ such that
\begin{align*}
	r_2 (\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,A_t=a)
	= r_1 (\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{y},A_t=a)
\end{align*}
But then we must have the inconsistence 
\begin{align*}
	r_2 (\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,A_t=a)
	\neq r_1 (\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z},A_t=a)
\end{align*}
This tells us given the choices of $\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1, \mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z},A_t=a$ could not happen simultaneously, and especially for any $\mathbf{S}_{t+1}$,
\begin{align*}
	\mathbb{P}( \mathbf{S}_{t+1},\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z},A_t=a) = 0
\end{align*}
However, for any given initial state $\mathbf{S}_{0}$, since the transition kernel is strictly positive, we know
\begin{align*}
	\mathbb{P}(\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z}| \mathbf{S}_0,\pi) >0
\end{align*}
for any policy $\pi$, and for any $\mathbf{S}_{t+1}$,
\begin{align*}
	\mathbb{P}(\mathbf{S}_{t+1}|\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z},A_t=a) > 0
\end{align*}
which then leads to
\begin{align*}
	\mathbb{P}( \mathbf{S}_{t+1},\mathbf{S}_{t,G_R^c\cap\Lambda}=\mathbf{x}_0,\mathbf{S}_{t, G_R\cap\Lambda}=\mathbf{x}_1,\mathbf{S}_{t,G_R\cap\Lambda^c}=\mathbf{z},A_t=a) > 0
\end{align*}
which is a contradiction. So we can conclude that $G_R=\Lambda$, i.e. such selection towards reward is unique. With the same idea, we can actually prove, under the iterative approach and each iteration outputting a selection satisfying above two requirements, all $G_j$ and the resulting final selection $G_\text{iter}$ are unique. 
\end{comment}

Finally, we show that $\mathbf{S}_{G_\text{iter}}$ is minimally sufficient. 
For any sufficient state $\mathbf{S}_G$, we first claim that $G$ contains $G_R$. Otherwise, there exists some $j_0\notin G\cap G_R$ that satisfies $j_0\in G_R$ and \eqref{eqn:sigrewardj}. However, this is impossible based on the above arguments. Similarly, we can iteratively show that $G$ must contain $G_j$ for $j=1,\cdots,m$. The proof is hence completed.
%using similar arguments in proving the uniqueness of $G_R$, we can show there 

\begin{comment}
we notice the reward part of the definition of a sufficient state always leads to the first requirement above, and it's easy to show that any index set satisfying only the first requirement will always contain $G_R$. That is because whenever we find such set $G$ that is sufficient, it automatically satisfies the first requirement. If it does not satisfy the second requirement, we just perform backward elimination until we find $G^\prime\subset G$ satisfy both requirements. But by uniqueness we know $G_R=G^\prime$. So it always holds that $G_R\subset G$. And to satisfy the transition part in the definition of sufficient state, we necessarily require
\begin{align*}
	\mathbf{S}_{t+1,G_R}\indep \mathbf{S}_{t,G^c}|(\mathbf{S}_{t,G},A_t)
\end{align*}
i.e., we at least need to include variables that are sufficient to determine the transition on $\mathbf{S}_{G_R}$, which leads again to the first requirement with target as the probability density of $\mathbf{S}_{G_R}$, instead of reward. Then we know $G_1\subset G$. Continue such arguments we know $G_\text{iter}\subset G$ for any $G\in\mathcal{F}_\text{sf}$. Then we have a total order on $\mathcal{F}_\text{sf}$ with inclusion, and conclude that minimal sufficient state $G^\ast$ is unique and $G_\text{iter} = G^\ast$.
\end{comment}
% \subsection{Implementation Details in Algorithm 1}


% \noindent
% \textbf{Step 0: Joint sample distribution}\\

% \noindent
% By counting frequencies, calculate the joint distribution $\mathcal{L}(S_1,...,S_p,a)$, where $(S, a)^\top = (S_1,...,S_p,a)^\top$ is the random variable.\\

% \noindent
% \textbf{Step 1: Knockoffs sampling and new joint distribution in 1st iteration}\\

% \noindent
% Then we can know $\mathcal{L}(S_1|S_{-1},a) = \frac{\displaystyle\mathcal{L}(S_1,...,S_p,a)}{\displaystyle\sum_u \mathcal{L}(S_1 = u, S_2, ..., S_p,a)}$\\
% and sample $\tilde{S}_1$ from $\mathcal{L}(S_1|S_{-1},a)$ row by row, using the values of $(S_2, ..., S_p,a)$ in each row (which makes $\tilde{S}_1$ independent of $S_1$).\\
% After that, by independence, we also know the new joint distribution
% \begin{align*}
	%     \mathcal{L}(S_{1:p},a, \tilde{S}_1)
	%     &= \mathcal{L}(\tilde{S}_1|S_{1:p},a) \mathcal{L}(S_{1:p},a)\\
	%     &= \mathcal{L}(\tilde{S}_1|S_{-1},a) \mathcal{L}(S_{1:p},a)
	% \end{align*}

% \noindent
% \textbf{Step j: Knockoffs sampling and new joint distribution in j-th iteration}\\

% \noindent
% We already know the joint distribution $\mathcal{L}(S_{1:p},a, \tilde{S}_{1:j-1})$, with which we calculate the conditional distribution
% $$
% \mathcal{L}(S_j|S_{-j},a, \tilde{S}_{1:j-1}) = \frac{\displaystyle \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j-1})}{\displaystyle\sum_u \mathcal{L}(S_1,...,S_{j-1},S_j = u,S_{j+1},...,S_p,a, \tilde{S}_{1:j-1})}
% $$

% to sample $\tilde{S}_j$. And then also calculate the new joint distribution
% \begin{align*}
	%     \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j})
	%     &= \mathcal{L}(\tilde{S}_j|S_{1:p},a, \tilde{S}_{1:j-1}) \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j-1})\\
	%     &= \mathcal{L}(\tilde{S}_1|S_{-1},a, \tilde{S}_{1:j-1}) \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j-1})
	% \end{align*}

% \noindent
% \textbf{Step p+1: Stopping}\\

% \noindent
% Stop the procedure when sampling has been done in all dimensions for the state, i.e. do not bother to generate samples for the action variable.

% \noindent
% Then follow Steps 5-8 in Algorithm 2.

\subsection{Preliminary Results}
\label{apdx: preliminary}

In this section, we provide a series of lemmas to show designs in our proposed procedure (e.g., Algorithm \ref{alg: 2}) would lead to the flip-sign property of $W$-statistics if computed from independent data. Such property on independent data is useful to show Theorems \ref{thm1} and \ref{thm2} when observations in our data are dependent. As a result, for any specific designs in SEEK, as long as they can be shown to satisfy the Assumption \ref{ass: flipcoin}, Theorems \ref{thm1} and \ref{thm2} would be guaranteed. For the ease of presentation, we focus on one $\mathcal{D}_k$ and simply assume that observations in $\mathcal{D}_k$ are independent. We will show how to remove this independent assumption and prove Theorems \ref{thm1} and \ref{thm2} in later sections.



%we focus on one $\mathcal{D}_k$ and simply assume that observations in $\mathcal{D}_k$ are independent. We will show how to remove this independent assumption and Theorem \ref{thm1} in Section \ref{apdx: 2}. Although assuming independence, this does not affect our final proof in FDR (mFDR) control. The results here solely serve to show designs proposed in this work would lead to the flip-sign property of $W$-statistics computed from independent data, introduced in Assumption \ref{ass: flipcoin}. Such property on independent data is sufficient to make sure Theorem \ref{thm1} and \ref{thm2} hold o dependent data consequently. As a result, for any specific designs in SEEK, as long as they can be shown to satisfy the Assumption \ref{ass: flipcoin}, all control results would be guaranteed. So flip-coin property on independent is the criterion for a feasible design, and the following sequence of lemmas serve as an example on how to prove such property.

% \subsubsection{Results and Proof for Algorithm 1}

% \textbf{Definition} 
% $[ \mathbf{S} ~ \tilde{\mathbf{S}}, a ]_{\text{swap}(B)}$
% is obtained by swapping the the pair of r.v.'s $(\mathbf{S}_i, \tilde{\mathbf{S}}_i)$ for all $i\in B$.\\

% \noindent
% \textbf{Lemma 1}. (Exchangeability in state variables)
% For any subset $B\in \{1,...,p\}$,
% $$
% (S, \tilde{S}, a)_{\text{swap}(B)}\overset{d}{=} [S, \tilde{S}, a]
% $$

% \noindent
% \textbf{Proof of Lemma 1}. We show the proof for discrete $\mathcal{S}$.\\
% I) After Step 1, $\tilde{S}_1\indep S_1|(S_{-1}, a)$ and $\tilde{S}_1|(S_{-1}, a) \overset{d}{=} S_1|(S_{-1}, a)$. Then we know $(S_1, \tilde{S}_1)$ are exchangeable in the joint distribution $(S_{1:p}, a, \tilde{S}_1)$.\\

% \noindent
% II) We assume after $(j-1)$ steps ($j\geq 2$), every pair of r.v.'s $(S_k, \tilde{S}_k)$ are exchangeable in the joint distribution $(S_{1:p}, a, \tilde{S}_{1:j-1})$ for $k=1,...,j-1$, i.e. $(S_{1:p}, a, \tilde{S}_{1:j-1})$ is symmetric in $(S_k, \tilde{S}_k)$. And now we perform Step $j$ to sample the next knockoff r.v. $\tilde{S}_j$. By construction,
% $$
% \mathcal{L}(\tilde{S_j} = v|S_{1:p},a, \tilde{S}_{1:j-1}) = \frac{\displaystyle \mathcal{L}(S_1,...,S_{j-1},S_j = v,S_{j+1},...,S_p,a, \tilde{S}_{1:j-1})}{\displaystyle\sum_u \mathcal{L}(S_1,...,S_{j-1},S_j = u,S_{j+1},...,S_p,a, \tilde{S}_{1:j-1})}
% $$
% with which we also have the new joint distribution
% \begin{align*}
	% \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j}) 
	% &= \mathbb{P}(\tilde{S}_j|S_{1:p},a, \tilde{S}_{1:j-1}) \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j-1})\\
	% &=
	% \frac{\displaystyle
		% \mathcal{L}(S_1,...,S_{j-1},\tilde{S}_j,S_{j+1},...,S_p,a, \tilde{S}_{1:j-1}) ~ \mathcal{L}(S_{1:p},a, \tilde{S}_{1:j-1})}
	% {\displaystyle\sum_u \mathcal{L}(S_1,...,S_{j-1},u,S_{j+1},...,S_p,a, \tilde{S}_{1:j-1})}
	% \end{align*}

% which is obviously symmetric in $(S_j, \tilde{S}_j)$. Together with the assumption of symmetry for the first $(j-1)$ dimensions we know the new joint distribution is symmetric in $(S_k, \tilde{S}_k), 1\leq k \leq j$.\\

% \noindent
% Combining the above and by Mathematical Induction we proved Lemma 1.\\

% \noindent
% \textbf{Lemma 2}. (Exchangeability in joint distributions) For any subset $B\subset \mathcal{H}_0$,
% $$
% ([ \mathbf{s}, \tilde{\mathbf{s}}, \mathbf{a}]_{\text{swap}(B)}, \mathbf{r})
% \overset{d}{=}
% ([ \mathbf{s}, \tilde{\mathbf{s}}, \mathbf{a}], \mathbf{r})
% $$

% \noindent
% \textbf{Proof of Lemma 2}. By independence between rows, we only need to prove the equivalence in each row (equivalence in r.v.'s), i.e.
% $$
% ([ \mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B)}, r)
% \overset{d}{=}
% ([ \mathbf{S}, \tilde{\mathbf{S}}, a], r)
% $$
% By Lemma 1, our goal is then simplified again to show
% $$
% (r| [\mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B)})
% \overset{d}{=}
% (r|[ \mathbf{S}, \tilde{\mathbf{S}}, a])
% $$

% \noindent
% Consider the conditional probability
% \begin{align*}
	% \mathbb{P}_{r| [\mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B)}}(r| [s, \tilde{s}, a])
	% &= 
	% \mathbb{P}_{r| [\mathbf{S}, \tilde{\mathbf{S}}, a]}(r| [s, \tilde{s}, a]_{\text{swap}(B)})\\
	% &=
	% \mathbb{P}_{r| [\mathbf{S}, a]}(r| [s^{\text{swap}}_1, s^{\text{swap}}_2,...,s^{\text{swap}}_p, a])
	% \end{align*}
% where by definition of $\text{swap}(B)$, $s^{\text{swap}}_i = \mathbf{1}_{i\in B}~\tilde{s}_i + \mathbf{1}_{i\not\in B}~s_i$, and the equality is due to the independence between $r$ and $\tilde{s}$ given $s,a$. WLOG we just assume the first index in $B$ is $1$. Then we also know dimension $1$ is null, leading to
% \begin{align*}
	% \mathbb{P}_{r| [\mathbf{S}, a]}(r| [s^{\text{swap}}_1, s^{\text{swap}}_2,...,s^{\text{swap}}_p, a])
	% &=
	% \mathbb{P}_{r| [\mathbf{S}, a]}(r| [\tilde{s}_1, s^{\text{swap}}_2,...,s^{\text{swap}}_p, a])\\
	% &=
	% \mathbb{P}_{r| [S_2,...,S_p, a]}(r| [ s^{\text{swap}}_2,...,s^{\text{swap}}_p, a])\\
	% &=
	% \mathbb{P}_{r| [\mathbf{S}, a]}(r| [s_1, s^{\text{swap}}_2,...,s^{\text{swap}}_p, a])\\
	% &=
	% \mathbb{P}_{r| [\mathbf{S}, \tilde{\mathbf{S}}, a]}(r| [s,\tilde{s}, a]_{\text{swap}(B\setminus\{1\})})
	% \end{align*}
% which shows that
% $$
% (r| [\mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B\setminus\{1\})})
% \overset{d}{=}
% (r|[ \mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B)})
% $$
% Following the above idea, we can continue to show
% $$
% (r| [\mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B\setminus\{1,2\})})
% \overset{d}{=}
% (r|[ \mathbf{S}, \tilde{\mathbf{S}}, a]_{\text{swap}(B\setminus\{1\})})
% $$
% and so on, until we exclude all elements in $B$, which then concludes our proof.\\


% \noindent
% \textbf{Lemma 3}. (Antisymmetry property of feature statistics) For any $i\in\{1,...,p\}$ and any subset $B\subset \{1,...,p\}$,
% $$
% W_i([ \mathbf{s} ~ \tilde{\mathbf{s}} ]_{\text{swap}(B)}, \mathbf{r})
% =
% W_i([ \mathbf{s} ~ \tilde{\mathbf{s}} ], \mathbf{r}) \cdot
% \begin{cases}
	%     -1, & \text{if}\ i\in B \\
	%     +1, & \text{otherwise}
	% \end{cases}
% $$

% \noindent
% \textbf{Proof of Lemma 3}. Notice that swapping any pairs of columns have no influence on the set of coefficients computed by linear regression, with only the ordering changed.\\
% If $i\notin B$, then the equivalence is trivial.\\
% If $i\in B$, then we have
% $$
% W_i([ \mathbf{s} ~ \tilde{\mathbf{s}} ]_{\text{swap}(B)}, \mathbf{r}) =  \max\limits_\theta\{||\mathbf{\beta}_{i+p}+\mathbf{\beta}_{i+1+3p}\theta||\} - \max\limits_\theta\{||\mathbf{\beta}_{i}+\mathbf{\beta}_{i+1+2p}\theta||\} = - W_i([ \mathbf{s} ~ \tilde{\mathbf{s}} ], \mathbf{r})
% $$

% \noindent
% \textbf{Lemma 4}  (Flip-coin properties in null states)
% For a sign vector $\mathbf{\epsilon}\in \{\pm 1\}^p$ independent of $\mathbf{W} = [W_1,...,W_p]^\top$, where $\epsilon_j = 1$ for all non-null state variables and $\epsilon_j \sim B(\{1, -1\}, 1/2)$ are i.i.d. for all null state variables. Then
% $$
% (W_1,...,W_p)\overset{d}{=} (W_1 \cdot \epsilon_1,...,W_p\cdot \epsilon_p)
% $$
% (Proof is the same as that for Lemma 8 below)

%\subsubsection{Results and Proof for Algorithm 1}
%\label{apdx: 1.1}



%\textbf{Lemma 1}. (Invariance under swaps) For the grouped matrix
%$$
%[\mathbf{s}^{(1)} ~ \tilde{\mathbf{s}}^{(1)}, \mathbf{s}^{(2)} ~ \tilde{\mathbf{s}}^{(2)},...,\mathbf{s}^{(m)} ~ \tilde{\mathbf{s}}^{(m)}]
%$$
%its distribution is invariant under swaps w.r.t. any subset $B\subset \{1,2,...,p\}$, i.e.
%$$
%[\mathbf{s}^{(1)} ~ \tilde{\mathbf{s}}^{(1)}, \mathbf{s}^{(2)} ~ \tilde{\mathbf{s}}^{(2)},...,\mathbf{s}^{(m)} ~ \tilde{\mathbf{s}}^{(m)}]_{\text{swap}(B)}
%\overset{d}{=}
%[\mathbf{s}^{(1)} ~ \tilde{\mathbf{s}}^{(1)}, \mathbf{s}^{(2)} ~ \tilde{\mathbf{s}}^{(2)},...,\mathbf{s}^{(m)} ~ \tilde{\mathbf{s}}^{(m)}]
%$$
%where

\begin{definition}[Swapped data]
	\label{def: c.1}
	For any $1 \leq a \leq m$, the dataset $[\mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ]_{\text{swap}(B)}
	$ 
	is obtained by swapping the $j$-th columns of $\mathbf{s}_k^{(a)}$ and $\tilde{\mathbf{s}}_k^{(a)}$ for all $j\in B$.
\end{definition}

\begin{lemma} [Exchangeability in state variables]
	\label{lemma: c.1}
	Assume the random design $\mathbf{S}_k^{(a)}$ follows normal distribution with zero mean, given each action $a\in\mathcal{A}$. For any $a\in\{1,...,m\}$, any subset $B\subset \{1,...,p\}$, and $\tilde{\mathbf{S}}_k^{(a)}$ constructed by the second order machine defined in Appendix \ref{apdx: b.3}, we have
	\begin{align*}
		[ \mathbf{S}_k^{(a)} ~ \tilde{\mathbf{S}}_k^{(a)} ]_{\text{swap}(B)} \overset{d}{=} [ \mathbf{S}_k^{(a)} ~ \tilde{\mathbf{S}}_k^{(a)} ]
	\end{align*}
\end{lemma}
The proof of Lemma \ref{lemma: c.1} is based on the use of the conditional normal distribution formula. We omit the details for brevity. In the following, we show the exchangeability holds jointly on states and responses when swapping null variables.


\begin{lemma} [Exchangeability on null variables]
	\label{lemma: c.2}
	For any $a\in\{1,...,m\}$, and any subset $B\subset \mathcal{H}_0$,
	\begin{align*}
		([ \mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ]_{\text{swap}(B)}, \mathbf{a}_k^{(a)}, \mathbf{y}_k^{(a)})
		\overset{d}{=}
		([ \mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ],\mathbf{a}_k^{(a)},     \mathbf{y}_k^{(a)}),
	\end{align*}
	where $\mathbf{y}_k^{(a)}$ is some generic responses.
\end{lemma}
\textbf{Proof of Lemma \ref{lemma: c.2}}. By our definition of action-based subgrouping in Algorithm \ref{alg: 2}, for each $a$, $\mathbf{a}_k^{(a)}=[a,...,a]\in\mathbb{R}^{n_{k,a}}$ is a constant vector. Then to show the lemma we only need to show
\begin{align*}
		([ \mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ]_{\text{swap}(B)},  \mathbf{y}_k^{(a)})
		\overset{d}{=}
		([ \mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ],    \mathbf{y}_k^{(a)}),
	\end{align*}
And by the exchangeability proved in Lemma \ref{lemma: c.1} as well as the proof of Lemma 3.2 in \cite{candes2018panning}, we can conclude our proof.  While here we allow $\mathbf{y}_k^{(a)}$ to be multi-dimensional, the two proofs are almost the same, through showing equivalence by swapping state variables in $B$ one by one. We omit further details of the proof.
\begin{remark}
We also emphasize that as long as Lemma \ref{lemma: c.1} is proved, Lemma \ref{lemma: c.2} is automatically satisfied, without dependence on specific method in sampling knockoffs.
\end{remark}

In the following lemma, we show the feature importance statistics we compute from $\calD_k$ satisfies the flip-sign property introduced in Assumption \ref{ass: flipcoin}.


\begin{lemma} [Flip-sign property of feature importance statistics]
	\label {lemma: c.3}
	For any $i\in\{1,...,p\}$ and any subset $B\subset \{1,...,p\}$,
	\begin{align*}
		W_i([ \mathbf{s}_k ~ \tilde{\mathbf{s}}_k ]_{\text{swap}(B)},
		\mathbf{a}_k, \mathbf{y}_k)
		=
		W_i([ \mathbf{s}_k ~ \tilde{\mathbf{s}}_k ],
		\mathbf{a}_k, \mathbf{y}_k) \cdot
		\begin{cases}
			-1, & \text{if}\ i\in B \\
			+1, & \text{otherwise}
		\end{cases}
	\end{align*}
\end{lemma}
\textbf{Proof of Lemma \ref{lemma: c.3}}. Recall that swapping any pairs of columns when fitting either penalized linear regression or random forest only changes the order of output in terms of coefficients or variable importance respectively. Then if $i\notin B$, then the equation clearly holds.
If $i\in B$, taking penalized linear regression as an example, we have
\begin{align*}
	W_i([ \mathbf{s}_k ~ \tilde{\mathbf{s}}_k ]_{\text{swap}(B)},\mathbf{a}_k, \mathbf{y}_k) =  \max\limits_a\{||\mathbf{\beta}_k^{(a)}(i+p)||_\infty\} - \max\limits_a\{||\mathbf{\beta}_k^{(a)}(i)||_\infty\} = - W_i([ \mathbf{s}_k ~ \tilde{\mathbf{s}}_k ],\mathbf{a}_k, \mathbf{y}_k)
\end{align*}
With knockoff variables constructed and the designed $W$-statistics that satisfies the above results, the control of FDR(mFDR) is then achieved:
\begin{theorem} \label{thm: indep fdr}
Assume the flip-coin property is satisfied (i.e., Lemma \ref{lemma: c.3} proved), on data $\mathcal{D}_k$, the selection $\widehat{G}_k$ obtained from applying knockoffs method in Algorithm \ref{alg: 2} controls mFDR, e.g.
\begin{align*}
   \text{mFDR}(\widehat{G}_k)\leq q
\end{align*}
And $\widehat{G}_k$ from applying knockoffs+ method in Algorithm \ref{alg: 2} controls FDR, e.g.
\begin{align*}
    \text{FDR}(\widehat{G}_k) \leq q
\end{align*}
\end{theorem}



\textbf{Proof of Theorem \ref{thm: indep fdr}}.
For statistics $W$ calculated, we denote $W_{\text{swap}(B)}$ to be the $W$-statistics computed after the swap w.r.t. $B\subset \{1,...,p\}$. Now consider a sign vector $\mathbf{\epsilon}\in \{\pm 1\}^p$ independent of $\mathbf{W} = [W_1,...,W_p]^\top$, where $\epsilon_i = 1$ for all non-null state variables and $\mathbb{P}(\epsilon_i=1) =  1/2$ are independent for all null state variables. Then for such $\mathbf{\epsilon}$, denote $B:=\{i: \epsilon_i = -1\}$, which is a subset of $\mathcal{H}_0$ by the assumption (and recall that $\mathcal{H}_0$ is the collection of all null variables). By Lemma \ref{lemma: c.3} we know
\begin{align*}
	(W_1 \cdot \epsilon_1,...,W_p\cdot \epsilon_p) = W_{\text{swap}(B)}
\end{align*}
For convenience, we also use $F$ to denote the the map from a data set to its $W$-statistics, i.e., on $[\mathbf{s}_k,\tilde{\mathbf{s}}_k,\mathbf{a}_k,\mathbf{y}_k]$,
\begin{align*}
    \mathbf{W} = F(\mathbf{s}_k,\tilde{\mathbf{s}}_k,\mathbf{a}_k,\mathbf{y}_k)
\end{align*}
Referring to the subgrouping procedure in Algorithm \ref{alg: 2} we know
\begin{align*}
	\mathbf{W}_{\text{swap}(B)}
	&= F([\mathbf{s}_k ~ \tilde{\mathbf{s}}_k]_{\text{swap}(B)},\mathbf{a}_k,\mathbf{y}_k)\\
	&= F(\lbrace ([ \mathbf{s}_k^{(1)} ~ \tilde{\mathbf{s}}_k^{(1)} ]_{\text{swap}(B)},\mathbf{a}_k^{(1)}, \mathbf{y}_k^{(1)}),
	...,
	([ \mathbf{s}_k^{(m)} ~ \tilde{\mathbf{s}}_k^{(m)} ]_{\text{swap}(B)},\mathbf{a}_k^{(m)}, \mathbf{y}_k^{(m)})\rbrace)\\
	&\overset{d}{=} F(( [ \mathbf{s}_k^{(1)} ~ \tilde{\mathbf{s}}_k^{(1)} ],\mathbf{a}_k^{(m)}, \mathbf{y}_k^{(1)}),
	...,
	([ \mathbf{s}_k^{(m)} ~ \tilde{\mathbf{s}}_k^{(m)} ],\mathbf{a}_k^{(m)}, \mathbf{y}_k^{(m)}))\\
	&= \mathbf{W}
\end{align*}
where the third equality (in distribution) is due to Lemma \ref{lemma: c.2} and the independence across subgroups $\lbrace ([ \mathbf{s}_k^{(a)} ~ \tilde{\mathbf{s}}_k^{(a)} ]_{\text{swap}(B)},\mathbf{a}_k^{(a)}, \mathbf{y}_k^{(a)})\rbrace_{a=1}^m$. Combing the above we then observe 
\begin{align*}
	(W_1,...,W_p)\overset{d}{=} (W_1 \cdot \epsilon_1,...,W_p\cdot \epsilon_p)
\end{align*}
And such observation can be called the symmetry of $\mathbf{W}$ in null variables. Given such symmetry, the rest of the proof will be the same as that for Theorem 3.4 of \citep[][]{candes2018panning}, which then leads to the conclusion.


%\textbf{Proof of Lemma 1}. Given any subset $B\subset \{1,2,...,p\}$, for each $j\in\{1,...,m\}$, by Lemma 2 in Barber and Candes (2015),
%$$
%[ \mathbf{s}^{(j)} ~ \tilde{\mathbf{s}}^{(j)} ]_{\text{swap}(B)}^\top [ \mathbf{s}^{(j)} ~ \tilde{\mathbf{s}}^{(j)} ]_{\text{swap}(B)} = [ \mathbf{s}^{(j)} ~ \tilde{\mathbf{s}}^{(j)} ]^\top [ \mathbf{s}^{(j)} ~ \tilde{\mathbf{s}}^{(j)} ]
%$$
% \noindent
% \textbf{Method 2 (will show why it cannot work)}

% \noindent
% \textbf{Lemma} 5 (flip-coin properties in null state variables)
% For a sign vector $\mathbf{\epsilon}\in \{\pm 1\}^p$ independent of $\mathbf{W} = [W_1,...,W_p]^\top$, where $\epsilon_j = 1$ and $\epsilon_j \sim B(\{1, -1\}, 1/2)$ are i.i.d. for all null state variables. Then
% $$
% (W_1,...,W_p)\overset{d}{=} (W_1 \cdot \epsilon_1,...,W_p\cdot \epsilon_p)
% $$

% \noindent
% Proof of Lemma 2.\\
% By definition, $W_i := \max\limits_j W^{(j)}_i$. For any $\mathbf{\epsilon}$ arbitrarily fixed, where by Lemma 1 in Barber and Candes (2015), we have the i.i.d. signs for the nulls in each subgroup $j$, i.e.

% $$
% (W_1^{(j)},...,W_p^{(j)})\overset{d}{=} (W_1^{(j)} \cdot \epsilon_1,...,W_p^{(j)}\cdot \epsilon_p)
% $$

% By independence between rows in $\mathcal{D}$, we know feature statistics are also independent across different $j$, which leads to

% $$
% (W_1^{(1)},...,W_p^{(1)},W_1^{(2)},...,W_p^{(2)},...,W_1^{(m)},...,W_p^{(m)}) \overset{d}{=}
% $$
% $$
% (W_1^{(1)} \cdot \epsilon_1,...,W_p^{(1)}\cdot \epsilon_p, W_1^{(2)} \cdot \epsilon_1,...,W_p^{(2)}\cdot \epsilon_p,...,W_1^{(m)} \cdot \epsilon_1,...,W_p^{(m)}\cdot \epsilon_p)
% $$
% Then take maximum over each dimension across all subgroups,
% $$
% (\max\limits_j W_1^{(j)},...,\max\limits_jW_p^{(j)}) \overset{d}{=}
% (\max\limits_j [W_1^{(j)} \cdot \epsilon_1],...,\max\limits_j [W_p^{(j)}\cdot \epsilon_p])
% $$
% Notice for each $i \leq p$:

% \begin{enumerate}
	%     \item if $\epsilon_i = 1$, then $\max\limits_j [W_i^{(j)}\cdot \epsilon_p] = \max\limits_j W_i^{(j)}$
	%     \item if $\epsilon_i = -1$, then  then $\max\limits_j [W_i^{(j)}\cdot \epsilon_p] = \max\limits_j [-W_i^{(j)}] =  -\min\limits_j W_i^{(j)}$. Again by independence and flip-coin property in nulls for each subgroup $j$, fixing all other covariates $\max\limits_j [W_k^{(j)}\cdot \epsilon_p], k \neq i$, we know the equivalence in the joint distribution
	%     $$
	%     (\max\limits_j [W_1^{(j)} \cdot \epsilon_1],...,\max\limits_j [W_{i-1}^{(j)} \cdot \epsilon_{i-1}], - \min\limits_j W_i^{(j)},\max\limits_j [W_{i+1}^{(j)} \cdot \epsilon_{i+1}], ...\max\limits_j [W_p^{(j)}\cdot \epsilon_p])
	%     $$
	%     $$
	%     \overset{d}{=}
	%     (\max\limits_j [W_1^{(j)} \cdot \epsilon_1],...,\max\limits_j [W_{i-1}^{(j)} \cdot \epsilon_{i-1}], - \max\limits_j W_i^{(j)},\max\limits_j [W_{i+1}^{(j)} \cdot \epsilon_{i+1}], ...\max\limits_j [W_p^{(j)}\cdot \epsilon_p])
	%     $$
	%     which does not hold.
	%     Actually we can see such definition of $W$ does not satisfy the flip-sign property under swap. So it's not well defined for our selection aim.
	% \end{enumerate}

% \subsection{Proofs of Propositions}
% \textbf{Proof of Proposition \ref{prop1}}: 
% %We provide a sketch of the proof only. 
% Under the MDP assumption, the optimal policy is greedy with respect to the optimal $Q$-function  $Q^\ast$ \citep[Section 6]{puterman2014markov}. Then by the Bellman optimality equation, we have that almost surely, %for every $(s, a) \in \calS \times \calA$,
% \begin{align*}
	%     Q^\ast(\mathbf{S}, A) =  \EE\left[R + \gamma\max_{a \in \calA}Q^\ast(\mathbf{S}', a) \given \mathbf{S}, A \right].
	% \end{align*}
% It follows from the conditional independence assumption that the right-hand-side depends on the state only through the sufficient state. Consequently, $Q^*(\mathbf{S}, A)$ is a function of $\mathbf{S}_G$ and $A$ only. The proof is hence completed.




\subsection{Proof of Theorem \ref{thm1}}
\label{apdx: 2}
By Theorem \ref{thm: indep fdr} proved above, we can show that our algorithm can exactly control FDR(mFDR) as $q$ when using $\calD_k$, if observations in $\calD_k$ are independent, due to the the flip-sign properties of our feature importance statistics shown in Lemma \ref{lemma: c.3}. Here we do not impose independence assumption anymore but Assumption \ref{ass: mixing} instead. First, we provide some facts that are related to our proof.


\begin{definition} [Total variation] For a given measure space $(\Omega, \Sigma, \mu)$, where $\Omega$ is the underlying space, $\Sigma$ is an $\sigma$-algebra over $\Omega$, and $\mu$ is a corresponding signed measure, the total variation of $\mu$ is
	\begin{align*}
		||\mu||_{TV} := \sup\limits_{A\in\Sigma}\mu(A) - \mu(A^c)
	\end{align*}
\end{definition}
\begin{definition} [$\beta$-coefficient] Two random vectors $X, Y$ are defined on a space $\Omega$, the $\beta$-coefficient measuring dependency between $X$ and $Y$ is defined as
	\begin{align*}
		\beta(X, Y) := \frac{1}{2} ||P_{X,Y} - P_X P_Y||_{TV}
	\end{align*}
	where $P_X,P_Y,$ and $P_{X,Y}$ are probability measures induced respectively by $X,Y$ and jointly $(X,Y)$. 
\end{definition}
The following important property can be obtained from \citep{berbee1987convergence}.

\begin{proposition}[Bound by measurability] 
	For random vectors $X,Y,Y^\prime,$ where $Y^\prime$ is $Y$-measurable, we must have
	\begin{align*}
		\beta(X,Y^\prime) \leq \beta(X,Y).
	\end{align*}
\end{proposition}

Now we review an important result first introduced in Lemma 2.1 of \citet[][]{berbee1987convergence}, which is the foundation of our proof.
\begin{lemma} 
	\label{lemma: c.5}
	On a given space $\Omega$ with probability measure $\mathbb{P}$ and real random variables $X_1,...,X_n$, define a sequence of coefficients
	\begin{align*}
		\beta_i := \beta(X_i, (X_{i+1},...,X_n))
	\end{align*}
	for $i=1,...,n-1$. Then we can always construct a new sequence of random variables on $\Omega$, denoted as $Z_1,...,Z_n$ such that
	\begin{enumerate}
		\item $Z_1,...,Z_n$ are independent
		\item $Z_i \overset{d}{=}  X_i$
		\item $\mathbb{P}(Z_i \neq X_i \text{ for some } i\in[n]) \leq \beta_1+\cdots+\beta_{n-1}$
	\end{enumerate}
\end{lemma}
The proof is provided in \citet[][]{berbee1987convergence}. In particular, the construction of $Z_1,...,Z_n$ can be found in \citet[][]{schwarz1980finitely}.The following definition is due to \citet[][]{bradley2005basic}:
\begin{definition} [$\beta$-mixing]
	\label{def: beta_mix}
	For a sequence of random variables $\{X_t\}$, define its $\beta$-mixing coefficient as
	\begin{align*}
		\beta(m) := \sup\limits_{i\in\mathcal{Z}} 
		\beta(\mathcal{F}_{-\infty}^{i}, \mathcal{F}_{i+m}^{\infty})
	\end{align*}
	where $\mathcal{F}_{J}^{L}$ is the $\sigma$-field generated by $\{X_J,...,X_L\}$. And a stationary Markov chain $\{X_t\}_{t\geq 0}$ is said to be exponentially $\beta$-mixing if its $\beta$-mixing coefficients $\beta(m)$ satisfy
	\begin{align*}
		\beta(m) = O(\rho^{m})
	\end{align*}
	for some constant $0<\rho<1$.
\end{definition}

Next, we prove Theorem \ref{thm1}. It is sufficient to focus on a generic set $\mathcal{D}_k\in\mathbb{R}^{(NT/K)\times(2p+2)}$ with
\begin{align*}
	\mathcal{D}_k = [\mathbf{s}_k, \mathbf{a}_k, \mathbf{r}_k, \mathbf{s}_k^\prime] = 
	\begin{bmatrix}
		s_{k-1} & a_{k-1} & r_{k-1} & s_{k-1}^\prime\\
		s_{k-1+K} & a_{k-1+K} & r_{k-1+K} & s_{k-1+K}^\prime\\
		\vdots & \vdots & \vdots & \vdots\\
		s_{k-1+jK} & a_{k-1+jK} & r_{k-1+jK} & s_{k-1+jK}^\prime\\
		\vdots & \vdots & \vdots & \vdots\\
		s_{k-1+(NT-K)} & a_{k-1+(NT-K)} & r_{k-1+(NT-K)} & s_{k-1+(NT-K)}^\prime
	\end{bmatrix}.
\end{align*}
Then by \citet[][]{schwarz1980finitely} and Lemma \ref{lemma: c.5} above, we can construct $[\mathbf{z}, \mathbf{b}]\in \mathbb{R}^{\frac{NT}{K}\times {(p+1)}}$, a collection of $\frac{NT}{K}$ i.i.d. samples $[\mathbf{z}_1, b_1],...,[\mathbf{z}_\frac{NT}{K}, b_\frac{NT}{K}] $, s.t.  
\begin{eqnarray*} \label{eq: 1}
	\mathbb{P} ([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some } i \in \{1,...,\frac{NT}{K}\}) \leq \beta_1 + ... + \beta_{\frac{NT}{K}-1}
\end{eqnarray*}
where 
\begin{eqnarray*}
	\label{eqn: 2}
	\beta_i := \beta([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}], ([\mathbf{s}_{k-1+iK},a_{k-1+iK}],...,[\mathbf{s}_{k-1+(NT-K)},a_{k-1+(NT-K)}]))
\end{eqnarray*}
for $i=1,...,\frac{NT}{K} - 1$. And notice the index gap above is at least $K$.


Before we show the rest of the proof, let us first obtain some intuition behind it. The main reason for such an extra construction is to transform our original problem back to the scenario with independence. Since 
$\mathbb{P} ([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some } i \in \{1,...,\frac{NT}{K}\})$ stands for the probability that the original data $[\mathbf{s},a]$ is different from the construction $[\mathbf{z},\mathbf{b}]$, we know with probability
$ 1- \mathbb{P} ([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some } i \in \{1,...,\frac{NT}{K}\})$, our construction perfectly replicates the original data, i.e.
$ [\mathbf{s}_k,\mathbf{a}_k] = [\mathbf{z},\mathbf{b}]$. Then we discuss by cases: when such perfect replication exists, we can directly apply Algorithm \ref{alg: 2} as if the data are independent, with FDR/mFDR controlled; when there exists discrepancies between the two, we would need to relax the the upper bound for FDR/mFDR, which is the cost we have to pay due to the relaxation of assumptions from independence to dependence among data points. The combination of the two would provide the final upper bound on FDR/mFDR control.





By Assumption \ref{ass: mixing}, the stochastic process induced by $\calD_k$ is stationary and exponentially $\beta$-mixing, where the $\beta$-mixing coefficient $\beta(m)$  is of $O(\rho^{m})$. Recall that $\rho < 1$ stated in Assumption \ref{ass: mixing}. Then by the construction of $\calD_k$ with $K = k_0\log(NT)$, and definition of $\beta_i$ in $\calD_k$ (with index gap at least $K$ between points),
\begin{align*}
	\beta_i \leq O(\rho^{K}) = O(\rho^{k_0\log(NT)}).
\end{align*}
We remark that if any two consecutive points in $\calD_k$ are from two different trajectories, then they are independent, and the above inequality still holds, which justifies our flexibility of splitting our batch data in terms of $N$ and length $T$. Then we obtain that
\begin{align*}
	& \mathbb{P} ([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] 
	\neq [\mathbf{z}_i,b_i] \text{ for some } i \in \{1,...,\frac{NT}{k_0\log(NT)}\})\\
	\leq& \frac{NT}{k_0\log(NT)}O(\rho^{k_0\log(NT)})\\
	=& O(\frac{NT}{k_0\log(NT)}\rho^{k_0\log(NT)})
\end{align*}
Then by some algebra, we can further show that
\begin{align*}
	\frac{NT}{k_0\log(NT)}\rho^{k_0\log(NT)} 
	&= \frac{NT}{k_0\log(NT)}\exp\{k_0\log(NT)\log(\rho)\}\\
	&= \frac{NT}{k_0\log(NT)} (\exp\{\log(NT)\})^{k_0\log(\rho)}\\
	&= \frac{NT}{k_0\log(NT)} (NT)^{-k_0\log(\frac{1}{\rho})}\\
	&= \frac{NT}{k_0\log(NT)} \left(\frac{1}{NT}\right)^{k_0\log(\frac{1}{\rho})}\\
	&= \frac{1}{k_0\log(NT)} \left(\frac{1}{NT}\right)^{k_0\log(\frac{1}{\rho}) - 1}
\end{align*}
Let $c = k_0\log(\frac{1}{\rho}) - 1$ and we have
\begin{align*}
	\mathbb{P} ([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some } i \in \{1,...,\frac{NT}{k_0\log(NT)}\}) \leq O(K^{-1}(NT)^{-c}).
\end{align*}
Now, we can show the resulting selection $\widehat{G}_k$ control the FDR/mFDR asymptotically. Specifically,
\begin{align*}
	\textbf{E}\left[\frac{\displaystyle|\widehat{G}_k\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_k| \vee 1}\right]
	=& \textbf{E}\left[\frac{\displaystyle|\widehat{G}_k\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_k| \vee 1} \mathbf{1}_{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\}} \right]\\
	&+ \textbf{E}\left[\frac{\displaystyle|\widehat{G}_k\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_k| \vee 1} \mathbf{1}_{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some i}\}} \right]\\
	\leq & q + \mathbb{P} ([\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some } i \in \{1,...,\frac{NT}{k_0\log(NT)}\})\\
	\leq & q + O(K^{-1}(NT)^{-c}),
	%&\leq q \textbf{E}\left[ \mathbf{1}_{\{\mathbf{s}_i = \hat{\mathbf{s}}_i \text{ for all i}\}} \right]
	%+ \textbf{E}\left[ \mathbf{1}_{\{\mathbf{s}_i \neq \hat{\mathbf{s}}_i \text{ for some i}\}} \right]\\
	%&= q \mathbb{P}(\mathbf{s}_i = \hat{\mathbf{s}}_i \text{ for all i})
	%+ \mathbb{P}(\mathbf{s}_i \neq \hat{\mathbf{s}}_i \text{ for some i})\\
	%&\leq q + \mathbb{P}(\mathbf{s}_i \neq \hat{\mathbf{s}}_i \text{ for some i})\\
	%&\leq q 
	%+ O\left(\frac{nT}{\log(nT)} \left(\frac{1}{nT}\right)^{k\log(\frac{1}{\rho})}\right)
\end{align*}
where the first inequality is given by the exact FDR/mFDR control on the independent data discussed in Theorem \ref{thm: indep fdr} and $\frac{\displaystyle|\widehat{G}_k\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_k| \vee 1} \leq 1$. 



\subsection{Proof of Theorem \ref{thm2}} Recall that our algorithm will run at most $p$ iterations for $1 \leq k \leq K$. Then denote the false discovery proportion (FDP) in $i$-th iteration using $\mathcal{D}_k$ of Algorithm \ref{alg: 1} by $\eta_{k,i}$, i.e. 
\begin{align*}
	\eta_{k,i}:= \frac{\displaystyle|\widehat{G}_{k,i}\cap \mathcal{H}_0|}{\displaystyle|\widehat{G}_{k,i}| \vee 1}
	&= \left(1 + \frac{\displaystyle  |\widehat{G}_{k,i}\backslash\mathcal{H}_0|}{\displaystyle|\widehat{G}_{k,i}\cap \mathcal{H}_0|}\right)^{-1}
\end{align*}
which implies that $\eta_{k,i}$ strictly decreases when $|\widehat{G}_{k,i}\backslash\mathcal{H}_0|/|\widehat{G}_{k,i}\cap \mathcal{H}_0|$ increases (and recall FDR is the expectation of FDP), and here we have assumed $|\widehat{G}_{k,i}\cap \mathcal{H}_0|>0$. And when $|\widehat{G}_{k,i}\cap \mathcal{H}_0|=0$, FDP vanishes. Since $p_0$ is the dimension of the minimal sufficient state, now we know at least one null is selected if and only if
\begin{align*}
	\eta_{k,i}\geq (1+p_0)^{-1}
\end{align*}
Combining two cases, we demonstrate that if and only if $|\hat{G}_{k,i}\cap \mathcal{H}_0|=0$, $\eta_{k,i} < (1+p_0)^{-1}$, which implies that
\begin{align*}
	\mathbb{P}(\eta_{k,i} =0) 
	= \mathbb{P}(\eta_{k,i} 
	< (1+p_0)^{-1})
\end{align*}
as well as
\begin{align*}
	\mathbb{P}(\eta_{k,i} > 0) = \mathbb{P}(\eta_{k,i} 
	\geq (1+p_0)^{-1})
\end{align*}
Now by Markov's inequality and Theorem \ref{thm1},
\begin{align*}
	\mathbb{P}(\eta_{k,i} \geq(1+p_0)^{-1}) 
	\leq \frac{\mathbf{E}(\eta_{k,i})}{(1+p_0)^{-1}} 
	\leq (1+p_0)(q + O(K^{-1}(NT)^{-c})) 
\end{align*}
By Bonferroni inequalities,
\begin{align*}
	\mathbb{P}\left(\bigcup\limits_{i=1}^p \{\eta_{k,i} \geq(1+p_0)^{-1}\}\right) \leq \sum\limits_{i=1}^p \mathbb{P}(\eta_{k,i} \geq(1+p_0)^{-1})
\end{align*}
Here we emphasize that upper limit is $p$ since at most $p$ iterations can be performed for each $\mathcal{D}_k$, corresponding to the case only one variable is selected in each iteration. In addition, when no selection is made at the $i$-th iteration, or when selection has stopped even before the $i$-th iteration, $\eta_{k,i}:=0$, which justifies that above notations are well-defined. So we continue to plug in the upper bounds to see
\begin{align*}
	\mathbb{P}\left(\bigcup\limits_{i=1}^p \{\eta_{k,i} \geq(1+p_0)^{-1}\}\right)
	&\leq \sum\limits_{i=1}^p 
	(1+p_0)(q + O(K^{-1}(NT)^{-c}))\\
	&= p (1+p_0)(q + O(K^{-1}(NT)^{-c})),
\end{align*}
for $1 \leq k \leq K$. Summarizing together, we can show that
\begin{align*}
	\mathbb{P}\left(\bigcup\limits_{k=1}^K\bigcup\limits_{i=1}^p \{\eta_{k,i} \geq (1+p_0)^{-1}\}\right)
	&\leq \sum\limits_{k=1}^K \mathbb{P}\left(\bigcup\limits_{i=1}^p \{\eta_{k,i} \geq(1+p_0)^{-1}\}\right)\\
	&\leq \sum\limits_{k=1}^K p (1+p_0)(q + O(K^{-1}(NT)^{-c}))\\
	&= Kp (1+p_0)(q + O(K^{-1}(NT)^{-c}))
\end{align*}
% Recall that by definition $K=k_0\log(NT)$, with the property of $O(\cdot)$ we know
% $$\mathbb{P}\left(\bigcup\limits_{k=1}^K\bigcup\limits_{i=1}^p \{\eta_{k,i} \geq \epsilon\}\right) \leq k_0\log(NT) p \frac{O((NT)^{-c})}{\epsilon}
% = \log(NT)O((NT)^{-c})$$
% Since $c>1$, we must have
% $$\lim\limits_{NT\rightarrow\infty}\log(NT)O((NT)^{-c}) = 0$$
% which leads to the following convergence in probability:
% $$\lim\limits_{NT\rightarrow\infty} \mathbb{P}\left(\bigcup\limits_{k=1}^K\bigcup\limits_{i=1}^p \{\eta_{k,i} \geq \epsilon\}\right) = 0$$
which further implies that
\begin{align*}
	\mathbb{P}\left(\bigcap\limits_{k=1}^K\bigcap\limits_{i=1}^p \{\eta_{k,i} = 0\}\right) 
	&= \mathbb{P}\left(\bigcap\limits_{k=1}^K\bigcap\limits_{i=1}^p \{\eta_{k,i} < (1+p_0)^{-1}\}\right) \\
	&\geq 1-Kp (1+p_0)(q + O(K^{-1}(NT)^{-c}))
\end{align*}
Remark that when $\eta_{k,i} =0$ for all $k,i$, we must know $\widehat{G}_{k,i}\cap\mathcal{H}_0=\emptyset$, then the final output from Algorithm \ref{alg: 1}, $\widehat{G}$, will attain $\text{FDP}(\hat{G})=0$.
But we should also notice that even if $\eta_{k,i} > 0$ for some $k,i$, it is still possible for $\text{FDP}(\widehat{G})$ to vanish, due to the correction by majority vote. As a result,
\begin{align*}
	\bigcap\limits_{k=1}^K\bigcap\limits_{i=1}^p \{\eta_{k,i} =0\} \subsetneq \{\text{FDP}(\widehat{G})=0\}.
\end{align*}
We then can obtain that
\begin{align*}
	\mathbb{P}(\text{FDP}(\widehat{G})=0) 
	\geq 1-Kp (1+p_0)(q  + O(K^{-1}(NT)^{-c})).
\end{align*}
Furthermore, we have
\begin{align*}
	\text{FDR}(\widehat{G}):=\mathbf{E}(\text{FDP}(\widehat{G}))
	&=\mathbf{E}(\text{FDP}(\widehat{G})\mathbf{1}_{\text{FDP}(\widehat{G})=0}) + \mathbf{E}(\text{FDP}(\widehat{G})\mathbf{1}_{\text{FDP}(\widehat{G})>0})\\
	&= \mathbf{E}(\text{FDP}(\widehat{G})\mathbf{1}_{\text{FDP}(\widehat{G})>0})\\
	&\leq \mathbf{E}(\mathbf{1}_{\text{FDP}(\widehat{G})>0})\\
	&\leq Kp (1+p_0)(q  + O(K^{-1}(NT)^{-c})).
\end{align*}
Finally, we can show that
\begin{align*}
	\limsup\limits_{NT\rightarrow\infty} \text{FDR}(\widehat{G}) 
	&\leq\lim\limits_{NT\rightarrow\infty}Kp (1+p_0)(q  + O(K^{-1}(NT)^{-c}))\\
	& = \lim\limits_{NT\rightarrow\infty}
	Kp (1+p_0)q 
	+ \lim\limits_{NT\rightarrow\infty}
	p (1+p_0) O((NT)^{-c})\\
	& = 0
\end{align*}
which concludes that 
\begin{align*}
	\lim\limits_{NT\rightarrow\infty} \text{FDR}(\widehat{G}) = 0
\end{align*}

\begin{comment}
\subsection{Proof of Theorem \ref{thm: power_ols}}


All the following proofs are for SEEK+, i.e. SEEK with knockoffs+. By the definition in Algorithm \ref{alg: 2}, we know immediately that for each iteration in each data subset, the selection set by Algorithm \ref{alg: 2} with knockoffs+ is always a subset of that with knockoffs, which guarantees that the corresponding power in such iteration with knockoffs is lower bounded by that with knockoffs+. Then by checking the following proof, we can see when extending such results to the power of the whole procedure, i.e. SEEK/SEEK+, power of SEEK also shares the lower bound by that of SEEK+. As a result, we only need to prove the power lower bound for SEEK+.


\noindent
\textbf{Proof sketch.} We organize the proof step by step, starting from the simplest case and generalize the results in later steps. We begin by assuming the transition tuples $(\mathbf{s}_t, a_t,r_t, \mathbf{s}_{t+1})$ are i.i.d., on which the selection approach we proposed in Algorithm \ref{alg: 2} has power asymptotically to 1. Then we add back the temporal dependence among transitions tuples, and show the power for the selection in the first several iterations inside one data subset $\mathcal{D}_k$, accordingly. Given such result, we continue to derive the power of selecting all true variables by our sequential selection, which combines all iterations in $\mathcal{D}_k$. Finally, the power is adjusted due to our majority vote across all subsets $\{\mathcal{D}_k\}_{k=1}^K$, which is exactly the power of SEEK. Such stream of ideas is also in the proof of LASSO and general machine learning case in below.

\subsubsection{Independent Data}

\begin{remark}
    Here by independent data we mean all transition tuples $(\mathbf{s}_t, a_t,r_t, \mathbf{s}_{t+1})$ are i.i.d samples from the same stationary distribution of the MDP, given the unknown environment and a specific policy. Such assumption just serves as the starting point to analyze the power, and later discussion and proof will be relaxed back to the actual case with temporal dependence among data.
\end{remark}

\begin{proposition}[Power for independent data]\label{prop: indep_ols}
    With Condition \ref{ass: ols}, if further assume data are independent, then for each iteration in each data subset,  when $n$ is large, with probability higher than $1-c_1n^{-c_2}$, selection power is 1. 
\end{proposition} 
\noindent
\textbf{Proof of Proposition \ref{prop: indep_ols}.} 
%Then for all true signals, $\beta_j^{(a)} \gg cn^{-1/2} ~ \text{for some } a\in\mathcal{A}$ and constant $c>0$. With the same constant $C,c>0$, we immediately have $|\{j\in[p]: \beta_j^{(a)} \gg cn^{-1/2} ~ \text{for some } a\in\mathcal{A} \}| \geq 2/q+C$.
Recall $W_j,j\in[p]$ the knockoffs statistics based on the differences among OLS estimators $\widehat{B}$, and the set of true variables $G$ with cardinality $p_0$ (fixed p, with divergence of n). Also recall that, by assumption, for continuous response, with probability asymptotically to 1, there are no ties in the magnitude of
$W_j$'s and no ties in the components of the OLS estimates across all actions
with asymptotic probability one.
\noindent
By OLS estimation bound in Condition \ref{ass: ols}, exists $c_1,c_2, C_1>0$, s.t. with high probability of at least $1-c_1n^{-c_2}$,  for any  $a\in\mathcal{A}$ and $i\in [p+1]$,
\begin{align*}
    C_1\log(n)n^{-1/2} &\geq ||\widehat{B}_i^{(a)} - B_i^{(a)}||_1
\end{align*}
which then leads to, for any $j\in[p], a\in\mathcal{A}$ and $i\in [p+1]$,
\begin{align*}
    C_1\log(n)n^{-1/2} &\geq |\widehat{B}_{j,i}^{(a)} - B_{j,i}^{(a)}| ,\\
    C_1\log(n)n^{-1/2} &\geq |\widehat{B}_{j+p,i}^{(a)}|
\end{align*}
for $n$ large. Now we denote the index set of all current response variables by $I$. Then it is immediate to see that, for any $j\in[p]$,
% \begin{align*}
%     \max_{i\in [p]}|\hat{\beta}_{j+p,i}^{(a)}| &\leq C_1\log(n)n^{-1/2}\\
%     \max_{i\in [p]}|\hat{\beta}_{ji}^{(a)} - \beta_{ji}^{(a)}| &\leq C_1\log(n)n^{-1/2}
% \end{align*}
% which  further implies
\begin{align*}
    \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}| &\leq C_1\log(n)n^{-1/2}\\
    \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)} - B_{ji}^{(a)}| &\leq C_1\log(n)n^{-1/2}
\end{align*}
Then for $j\in[p]$, we can have
\begin{align*}
    W_j 
    \geq - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|
    \geq - C_1\log(n)n^{-1/2}
\end{align*}
Then for each $j\in [p]$, combining above bounds,
\begin{align*}
    W_j  
    &= \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    &= \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)} + B_{ji}^{(a)} - B_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    % &\geq |\hat{\beta}_j^{(a)} + \beta_j^{(a)} - \beta_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}| \quad \text{for any } a\\
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}\{|B_{ji}^{(a)}| - |\widehat{B}_{ji}^{(a)} - B_{ji}^{(a)}|\} - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)} - B_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - 2C_1\log(n)n^{-1/2}
\end{align*}
Especially, still by the above relaxation, for all for true signals $j\in \cup_{i\in I\cap G_\textrm{M}} G(S_i)$,
\begin{align*}
    W_j  
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - 2C_1\log(n)n^{-1/2}
    \gg C_1\log(n)n^{-1/2}
\end{align*}
given large enough $n$. The last inequality is because, for every $j\in \cup_{i\in I\cap G_\textrm{M}} G(S_i)$, there must exist some $a\in\mathcal{A}$ and $i\in I\cap G_\textrm{M}$ s.t. $|B_{ji}^{(a)}|>0$, which, especially, is a positive constant independent from $n$, so does the corresponding maximum $\max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}|$.
Then we have observed that true signals obtain $W_j \gg C_1\log(n)n^{-1/2}$, while others only have $W_j \geq -C_1\log(n)n^{-1/2}$. 
\noindent
So we know as long as $T\geq\min \{\max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}|: j\in[p], B_{ji}^{(a)} >0 ~ \text{for some } a\in\mathcal{A}, i\in I\cap G_\textrm{M} \} - 2C_1\log(n)n^{-1/2}$, we always have $|\{j:W_j\leq -T\}|=0$, making the ratio to control, $|\{j:W_j\leq -T\}|/|\{j:W_j\geq T\}|$, vanish, whatever the denominator is (recall that for FDP, $0/0$ is defined to be zero).


% Especially such ratio takes its minimum value of at most $1/|\{j\in[p], \beta_j^{(a)} >0 ~ \text{for some } a\in\mathcal{A} \}|$ (may still have some nulls with large $W$), when $T=\min \{\max_{a\in\mathcal{A}}\beta_j^{(a)}: j\in[p], \beta_j^{(a)} >0 ~ \text{for some } a\in\mathcal{A} \} - 2C_1\log(n)n^{-1/2}$. By Condition \ref{ass: signal_ols} we know 
% \begin{align*}
% \frac{1}{|\{j\in[p]: \beta_j^{(a)} >0 ~ \text{for some } a\in\mathcal{A} \}|}
% \leq \frac{1}{\frac{1}{q}+C_2}    
% = \frac{q}{1+C_2q}  < q
% \end{align*}


Such observations implies that we must have $T \leq \min \{\max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}|: j\in[p], B_{ji}^{(a)} >0 ~ \text{for some } a\in\mathcal{A}, i\in I\cap G_\textrm{M} \} - 2C_1\log(n)n^{-1/2}$, and then,
\begin{align*}
    \{j\in[p]:W_j\geq T\}  \supseteq \{j\in[p]: B_{ji}^{(a)} >0 ~ \text{for some } a\in\mathcal{A}, i\in I\cap G_\textrm{M} \}
\end{align*}
meaning that, with probability at least $1-c_1n^{-c_2}$,  we always have $\cup_{i\in I\cap G} G(S_i) \subseteq \widehat{G}$. Especially the overall power takes the form
\begin{align*}
    &\mathbb{E}\left[\frac{\displaystyle|
    (\cup_{i\in I\cap G_\textrm{M}} G(S_i))\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap G} G(S_i)|}\right]\\
    =~& \mathbb{E}\left[\frac{\displaystyle|
    (\cup_{i\in I\cap G_\textrm{M}} G(S_i))\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap G} G(S_i)|} \mathbf{1}\{||\widehat{B}_i^{(a)} - B_i^{(a)}||_1 \leq C_1\log(n)n^{-1/2} \text{ for all } a\in\mathcal{A}, i\in I \} \right]\\
    &+ \mathbb{E}\left[\frac{\displaystyle|
    (\cup_{i\in I\cap G_\textrm{M}} G(S_i))\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap G} G(S_i)|} \mathbf{1}\{||\widehat{B}_i^{(a)} - B_i^{(a)}||_1 > C_1\log(n)n^{-1/2} \text{ for some } a\in\mathcal{A},i\in I\}  \right]\\
    \geq~& \mathbb{E}\left[\frac{\displaystyle|
    (\cup_{i\in I\cap G_\textrm{M}} G(S_i))\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap G} G(S_i)|} \mathbf{1}\{||\widehat{B}_i^{(a)} - B_i^{(a)}||_1 \leq C_1\log(n)n^{-1/2} \text{ for all } a\in\mathcal{A}, i\in I\} \right]\\
    =~& \mathbb{P}\left( |\widehat{B}_i^{(a)} - B_i^{(a)}|_1 \leq C_1\log(n)n^{-1/2} \text{ for all } a\in\mathcal{A},i\in I  \right)\\
    \geq~& 1-c_1n^{-c_2}
\end{align*}

\subsubsection{Power in Temporally Dependent Data} \label{sec: temp_ols}
idea: inclusion layer by layer without any miss

\subsubsection{Power in One Sequence}\label{pf_seq_ols}

\subsubsection{Power with Majority Vote}\label{pf_vote}
\begin{align*}
\mathbb{P}(G \subseteq \hat{G}) \geq
\mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\subseteq \hat{G}_k\} \geq \alpha) 
&= \mathbb{P}(\frac{1}{K}\sum_{k=1}^K 1 - \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq \alpha)\\
&= \mathbb{P}(1 - \frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq \alpha)\\
&= \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \leq 1 - \alpha)\\
&= 1 - \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \alpha)
\end{align*}
Since $\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq 0$, by Markov's inequality,
\begin{align*}
    ~&\mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \alpha)\\ 
    \leq~& \frac{\mathbb{E}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\})}{1 - \alpha}\\
    =~& \frac{\frac{1}{K}\sum_{k=1}^K \mathbb{P} (G\nsubseteq \hat{G}_k)}{1-\alpha}\\
    \leq~& \frac{O\{n^{-c_2} + K^{-1}(NT)^{-c}\}}{1-\alpha}\\
    =~& O\{n^{-c_2} + K^{-1}(NT)^{-c}\} \rightarrow 0
    % =~& O\{K^{c_2}(NT)^{-c_2}q^3 + 3K^{c_2}(NT)^{-c_2}q^2 + 2K^{c_2}(NT)^{-c_2}q + K^{-1}(NT)^{-c}q\}
\end{align*}
% In addition, given the comparison
% \begin{align*}
%     \frac{n^{-c_2}q^3}{n^{-c_2}q^2} = q<1 \quad \text{and} \quad \frac{n^{-c_2}q^2}{n^{-c_2}q} = q<1
% \end{align*}
% by the definition of $q$ as the target FDR constraint, we simplify the above probability upper bound by
% \begin{align*}
%     \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \gamma)
%     \leq O\{n^{-c_2}q + K^{-1}(NT)^{-c}q\}
%     \longrightarrow 0
% \end{align*}
%which,  as $NT\rightarrow\infty$, converges to $[p - (1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon}]/(1-\gamma)$ that is arbitrarily close to $0$. (even if in later cases, i.e. LASSO or random forest, where we let $p$ and $p_0$ diverge, we can let $\epsilon$ decay as $n$ increases s.t. $p - (1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon}\rightarrow 0$) 
With that we have shown the Algorithm 1 (SEEK) has asymptotic power  1.
\begin{remark}
    The fractional scaling $1/(1-\alpha)$ also shows the balance in choosing $\alpha$. On one hand, we need $\alpha$ large so as to only select variables significant enough. On the other hand, such scaling is strictly increasing in $\alpha$, then to have larger power, $\alpha$ should not be too large. That is why numbers in the middle part of $[0,1]$ tend to be good choices, which is also shown in experiments.
\end{remark}
\end{comment}


\begin{comment}
\subsubsection{Power in Temporally Dependent Data} \label{sec: temp_ols}
Starting from here we revise a piece of notations. Now we still use $G$ to denote the index set of all true state variables for the given MDP, but for each iteration $i\geq 1$, $G_i$ stands for the corresponding index set of true variables that only influence reward after $i-1$ lags. For example, $G_1$ is for all that directly contribute to reward, $G_2$ is for all that never directly contribute to reward but directly contribute to some variables in $G_1$, and so on for other $i\geq 2$. Then we have a partition $G=\cup_{i\geq 1}G_i$ with $G_i\cap G_j = \emptyset$ for $i\neq j$.

Now we take back the temporal dependence between transition tuples and show the result in one iteration ($l$-th) inside one subset $\mathcal{D}_k$, where the selection is $\widehat{G}_{k,l}$ and the underlying truth (excluding all formerly selected ones) is $\widetilde{G}_{k,l}$. Remark that $\widetilde{G}_{k,l}$ could be different from $G_l$ for $l\geq 2$. This is because, if in $(l-1)$-th iteration, some $j\in G_l$ is already selected due to randomness, i.e. $j\in\widehat{G}_{k,l-1}$, then it will be excluded from the list of potential selection in Algorithm \ref{alg: 2} for the $l$-th iteration, meaning that it would never happen that $j\in \widetilde{G}_{k,l}$. So the interpretation is that $\widetilde{G}_{k,l}$ represents the extra true state variables with respect to all selected ones till $(l-1)$-th iteration, in subset $\mathcal{D}_k$. Then we immediately know that not only $\widetilde{G}_{k,l}$ may contain more indices from $G$ but not in $G_l$, it can also contain null variables (in the case when some nulls follow autoregressive models and part of them is already selected in former iterations). But none of the 2 discrepancies mentioned would hurt our final result, since, as shown later in Lemma \ref{lemma: inc}, with high probability, all elements of $G$ will be selected somehow. As the following proof techniques are repeated used in this section, we summarize them in an intermediate result:
\begin{lemma} \label{lemma: temp_dep}
    If, under the additional assumption that transition tuples are independent, we can know that with probability at least $1-\epsilon(n,p)$, Algorithm \ref{alg: 2} would select all true variables (with respect to the current response variables) accordingly, then for actual temporally dependent data $\mathcal{D}_k$ and up till the $l$-th selection, 
    \begin{align*}
        \mathbb{P}\left(\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right) 
        &\geq (1-\epsilon(n,p))^{l} [1 - O(K^{-1}(NT)^{-c})] \quad \text{and} \\
        \mathbb{P}\left(\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right)
        &\geq (1-\epsilon(n,p))^{l} [1 - O(K^{-1}(NT)^{-c})]
    \end{align*}
\end{lemma}
\noindent
\textbf{Proof of Lemma \ref{lemma: temp_dep}}.
By the above analysis, and due to Lemma \ref{lemma: inc} we know
\begin{align*}
    ~& \mathbb{P}\left(\left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right\}
    \right)\\
    \geq~& \mathbb{P}\left(\left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right\}
    \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \}
    \right)\\
    \geq~& \mathbb{P}\left(\left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}
    \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \}
    \right)
\end{align*}
In addition,
\begin{align*}
    ~& \mathbb{P}\left(\left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}\right)\\
    \geq~& \mathbb{P}\left( \left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \} \right)\\
    =~& \mathbb{P}\left( \left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
    ~&\cdot \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
    =~& \Pi_{h=1}^{l}\mathbb{P}\left(  \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
    ~&\cdot \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
    \geq~& (1 - \epsilon(n,p))^l [1 - O\{K^{-1}(NT)^{-c}\}]
\end{align*}

% \begin{align*}
%     ~&\mathbb{P}\left(\left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right\}\right)\\
%     \geq~& \mathbb{P}\left(\left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right\} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~  \forall i\}\cap \left\{\bigcup_{h=1}^{l-1} G_h\subseteq \bigcup_{h=1}^{l-1}\hat{G}_{k,h}\right\}\right)\\
%     =~& \mathbb{P}\left(\left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right\} | \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i\}\cap \left\{\bigcup_{h=1}^{l-1} G_h\subseteq \bigcup_{h=1}^{l-1}\hat{G}_{k,h}\right\}\right)\\
%     ~&\cdot \mathbb{P}\left( \left\{\bigcup_{h=1}^{l-1} G_h\subseteq \bigcup_{h=1}^{l-1}\hat{G}_{k,h}\right\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     ~&\cdot \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)
%     %&\geq (1-\epsilon) [1 - O(K^{-1}(NT)^{-c})]
% \end{align*}


% Then to evaluate the second factor term in the above, we claim that for any $l\in[p]$, we always have
% \begin{align*}
%     \mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)
%     \geq (1-\epsilon(n,p))^l
% \end{align*}
% This is because first, if $l=1$, it is just the result shown in the independent data case. Then, if we already know such claim holds for $1,2,...,l$, then for $l+1$, by definition,
% \begin{align*}
%     &\mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l+1]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     =~& \mathbb{P}\left( \{G_{l+1}\subseteq \hat{G}_{k,l+1}\} | \{G_h\subseteq \hat{G}_{k,h}, h\in[l]\} \cap [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     \cdot~& \mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     \geq~& (1-\epsilon(n,p))(1-\epsilon(n,p))^l\\
%     =~& (1-\epsilon(n,p))^{l+1}
% \end{align*}
% Then by Mathematical Induction such claim is proved. With that we now know
% \begin{align*}
%     \mathbb{P}\left(G_l\subseteq \hat{G}_{k,l}\right)
%     &~\geq \mathbb{P}\left(\{G_l\subseteq \hat{G}_{k,l}\} | \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i\}\cap \{G_h\subseteq \hat{G}_{k,h}, h\in[l-1]\}\right)\\
%     &~\cdot \mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l-1]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     &~\cdot \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     &~\geq (1-\epsilon(n,p)) (1-\epsilon(n,p))^{l-1} [1 - O(K^{-1}(NT)^{-c})]\\
%     &~= (1-\epsilon(n,p))^{l} [1 - O(K^{-1}(NT)^{-c})]
% \end{align*}

% Especially we also know
% \begin{align*}
% 	\mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|}\right]
% 	=& \mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|} \mathbf{1}{\{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\}} \right]\\
% 	&+ \mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|} \mathbf{1}{\{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some i}\}} \right]\\
% 	\geq& \mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|} \mathbf{1}{\{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\}} \right]\\
% 	\geq& (1 - \epsilon) \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\} \right)\\
% 	\geq& (1 - \epsilon) [1 - O(K^{-1}(NT)^{-c})]
% \end{align*}

where the last equality is because the good performance of variable importance estimation in different iterations are independent, since each of them is just a simple OLS/LASSO or general ML implementation (and when all of $[p]$ are selected before $l$-th iteration, the power of later "iterations" are defined as 1). In the OLS case, from above we know $\epsilon(n,p) = c_1n^{-c_2}$, so such probability of perfect selection till $l$-th iteration, i.e. $\mathbb{P}\left(\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}\right)$, is $(1-c_1n^{-c_2})^{l} [1 - O(K^{-1}(NT)^{-c})]$, which converges to 1 as $NT\rightarrow\infty$ (recall that in each data subset $\mathcal{D}_k, n=NT/K\rightarrow \infty$). 

\end{comment}

\begin{comment}
\subsubsection{Power in One Sequence}\label{pf_seq_ols}
Recall from above analysis, for each iteration $i$, with probability at least $(1-c_1n^{-c_2})^{i} [1 - O(K^{-1}(NT)^{-c})]$, $\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l}\hat{G}_{k,h}$, and simultaneously $\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}$ for all $h\in [l]$, which shows that, when some true $j\in G$ is still left out before $h$-th iteration, $|\hat{G}_{k,h}|\geq |\widetilde{G}_{k,h}|\geq 1/q+C_2$ with such high probability,  due to Condition \ref{ass: signal_ols}. Then the total number of iterations needed for including the whole $G$ in such subset $\mathcal{D}_k$  is upper bounded by 
\begin{align*}
    \frac{p}{\min_i |\hat{G}_{k,i}|} \leq \frac{p}{\frac{1}{q}+C_2} = \frac{pq}{1+C_2q} < pq
\end{align*}
which shows that, with high probability, there are at most $\lceil pq \rceil$ selections on $\mathcal{D}_k$ in a sequence, where by convention, for some $i$-th iteration, the selection power is defined to be $1$, if there are no true variables left to be selected. In addition, even if there exists some variable with index $j\in G_i$ that is not selected exactly in $i$-th iteration, there is still possibility that it is included in other iterations due to randomness, then we claim the following relations between random events,
\begin{lemma}[Selection inclusion]\label{lemma: inc}
Assume data are independent. For $1 \leq l \leq \lceil pq \rceil$,
\begin{align*}
    \left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}
    \subseteq
    \left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l} \hat{G}_{k,h}\right\} 
    \quad \text{and}
\end{align*}
\begin{align*}
    \left\{\bigcap_{h=1}^{\lceil pq \rceil} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}
    \subseteq
    \{G\subseteq \hat{G}_{k}\}
\end{align*}
\end{lemma}

\begin{remark}
    The assumption of independence here does not hurt, as the concerned probabilities with temporal dependence, as shown rigorously later, are lower bounded by the one with independence, and recall that we want the power to be large.
\end{remark}
\noindent
\textbf{Proof of Lemma \ref{lemma: inc}.}
This can be derived by observing $\widetilde{G}_{k,h}$ one by one (here we assume the left hand side already holds respectively and prove that the right hand side must happen correspondingly):

\textbf{Step 1.} We use Steps 1 and 2 for Part 1 (the first result). First, for $l=1$, when we know $\widetilde{G}_{k,1}\subseteq \hat{G}_{k,1}$, since the current response variable is only reward, we immediately have $G_1 \subseteq \widetilde{G}_{k,1} \subseteq \hat{G}_{k,1}$. Then  such claim in Part 1 for $l=1$ is proved.

% If actually there is some $j\in G$ left out, then we know Condition \ref{ass: signal_ols} is again satisfied. And especially for all $j\in G_2\backslash \hat{G}_{k,1}$, we know it must be that $j\in \widetilde{G}_{k,2}$. Then result from Proposition \ref{prop: indep_ols} holds again, telling us, with high probability asymptotic to 1, $\widetilde{G}_{k,2} \in \widehat{G}_{k,2}$, which gives $G_2\backslash \hat{G}_{k,1} \subseteq \widehat{G}_{k,2} \neq \emptyset$ and $\bigcup_{h=1}^2 G_h  \subseteq \bigcup_{h=1}^2 \widehat{G}_{k,h}$. On the other hand, even if $G_2\subseteq \hat{G}_{k,1}$, then $G_3\subseteq \widetilde{G}_{k,2}$, and still $\widehat{G}_{k,h} \neq \emptyset$, so the selection will not terminate (the same idea for $G_i\subseteq \widetilde{G}_{k,2}\neq \emptyset$ with $i\geq 3$ if $\bigcup_{h=1}^{i-1} G_h  \subseteq \hat{G}_{k,1}$). To conclude, part 1 when $l=1$ is proved, and as long as some true variable is left, the second selection would happen.

\textbf{Step 2.} Assume for some $1\leq l \leq \lceil pq \rceil - 1$, we already know $ \left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}
\subseteq
\left\{\bigcup_{h=1}^{l} G_h\subseteq \bigcup_{h=1}^{l} \hat{G}_{k,h}\right\} $. Now in Step 2 we are given the information $\bigcap_{h=1}^{l+1} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}$ holds (which immediately guarantees that $\bigcup_{h=1}^l G_h  \subseteq \bigcup_{h=1}^l \widehat{G}_{k,h}$). If no element of $G$ is left out after $l$-th iteration, we can have $ \left\{\bigcap_{h=1}^{l+1} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}
\subseteq
\left\{\bigcup_{h=1}^{l+1} G_h\subseteq \bigcup_{h=1}^{l+1} \hat{G}_{k,h}\right\} $, which proves the case for $l+1$. If some is still not selected, there must exist a minimum index $l_1$, s.t. $l_1\geq l+1$, $G_{l_1}\backslash \{\bigcup_{h=1}^l \widehat{G}_{k,h}\} \neq \emptyset$, and $(G\backslash \{\bigcup_{h=1}^l \widehat{G}_{k,h}\}) \subseteq \bigcup_{h \geq l_1} G_h$. If $l_1 = l+1$, then again $(G_{l_1}\backslash \{\bigcup_{h=1}^l \widehat{G}_{k,h}\} )\subseteq \widetilde{G}_{k,l+1}\subseteq \widehat{G}_{k,l+1}$, which leads to $\bigcup_{h=1}^{l+1} G_h  \subseteq \bigcup_{h=1}^{l+1} \widehat{G}_{k,h}$. Such result would automatically be satisfied, due to the definition of $l_1$, if $l_1 > l+1$.  Then by Mathematical Induction the first part is proved for all possible $l$.

% Similar idea as in the last part of Step 1, we can know the selection will continue as long as some true $j\in G$ is still outside the current union of selections.

\textbf{Step 3.} Then for the second part, i.e. final inclusion. By above 2 steps, we already show $ \left\{\bigcap_{h=1}^{\lceil pq \rceil} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}
\subseteq
\left\{\bigcup_{h=1}^{\lceil pq \rceil} G_h\subseteq \bigcup_{h=1}^{\lceil pq \rceil} \hat{G}_{k,h}\right\} $. Now given the information of the left hand side, the right hand side is guaranteed, i.e., $\bigcup_{h=1}^{\lceil pq \rceil} G_h  \subseteq \bigcup_{h=1}^{\lceil pq \rceil} \widehat{G}_{k,h}$. Then assume there is still $j\in G \backslash \bigcup_{h=1}^{\lceil pq \rceil} \widehat{G}_{k,h}$. However, this means in every step $h$ with $h= 1,...,\lceil pq \rceil$ (remark $\lceil pq \rceil\geq2$), before making the corresponding selection $\widehat{G}_{k,h}$, some true $j\in G$ is still left out, then $\widehat{G}_{k,h} \neq \emptyset$ (see the following remark for further explanation) and due to Condition \ref{ass: signal_ols}, the selection size in the coming selection is large in the sense that $|\widehat{G}_{k,h}|\geq 1/q +C_2$, due to Proposition \ref{prop: indep_ols}. But then the current total selection has size $|\bigcup_{h=1}^{\lceil pq \rceil} \widehat{G}_{k,h}| \geq q $. So
there is no way such true $j$ is not in $\bigcup_{h=1}^{\lceil pq \rceil} \widehat{G}_{k,h}$. By contradiction we show part 2 and finish the proof of Lemma \ref{lemma: inc}.
\begin{remark}
    As discussed in former steps, we must have $G_1\subseteq \widetilde{G}_{k,1}\subseteq \widehat{G}_{k,1}$. After first selection, if some $j\in G_2$ is not selected yet, then it must be true variable w.r.t $\widehat{G}_{k,1}$. Then  $\widetilde{G}_{k,2}\neq \emptyset$ and $|\widetilde{G}_{k,2}|\geq 1/q+C_2$. If all $G_2$ selected in $\widehat{G}_1$, but with some of $G_3$ left out, by the same idea we immediately have $\widetilde{G}_{k,2}\neq \emptyset$. Then by such idea we know, as long as some $j\in G$ is not selected in $\widehat{G}_1$, we have $\widetilde{G}_{k,2}\neq \emptyset$. Applying the same procedure to all further iterations, we conclude that as long as some $j\in G$ not selected during the first until $l$-th iteration, $\widetilde{G}_{k,l+1}\neq \emptyset$. Then by analysis in Step 3, we know after even $\lceil pq \rceil$ iterations, some $j\in G$ is still outside the selection, then together with the above results we know $\widetilde{G}_{k,h}\neq \emptyset$ for all $h=1,...,\lceil pq \rceil$.
\end{remark}

\noindent
Then by Lemma \ref{lemma: temp_dep} and \ref{lemma: inc} we know
\begin{align*}
    \mathbb{P}\left(G\subseteq \hat{G}_{k} \right)
    &\geq \mathbb{P}\left(G\subseteq \hat{G}_{k} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \} \right)\\
    &\geq \mathbb{P}\left( \left\{\bigcap_{h=1}^{\lceil pq \rceil} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \} \right)\\
    &\geq (1- c_1 n^{-c_2})^{\lceil pq \rceil} [1 - O(K^{-1}(NT)^{-c})]
\end{align*}
First notice
\begin{align*}
    (1-c_1n^{-c_2})^{\lceil pq \rceil} 
    &= \sum_{h=0}^{\lceil pq \rceil} \binom{\lceil pq \rceil}{h} (-1)^h c_1^h n^{-hc_2}
    % &= S_1 + S_2
\end{align*}
% where
% \begin{align*}
%     S_1
%     &= \sum_{h=0}^{\lfloor i/2 \rfloor} \binom{i}{h} (-1)^h c_1^h p^{-hc_2}\\
%     % &= 1 - O(i c_1p^{-c_2}) + O(i^2 c_1^2p^{-2c_2}) + \cdots + (-1)^hO(i^h c_1^h p^{-hc_2}) + \cdots\\
%     S_1
%     &= \sum_{\lfloor i/2 \rfloor + 1}^i \binom{i}{h} (-1)^h c_1^h p^{-hc_2}
% \end{align*}
Now for any $h=0,..., \lceil pq \rceil - 1$, we know
\begin{align*}
    \left|\frac{\binom{\lceil pq \rceil}{h+1}(-1)^{h+1} c_1^{h+1} n^{-(h+1)c_2}}{\binom{\lceil pq \rceil}{h}(-1)^h c_1^h n^{-hc_2}}\right| 
    =  c_1n^{-c_2}\frac{\lceil pq \rceil!/[(h+1)!(\lceil pq \rceil-h-1)!]}{\lceil pq \rceil!/[h!(\lceil pq \rceil-h)!]}
    = c_1n^{-c_2}\frac{\lceil pq \rceil-h}{h+1}
\end{align*}
And specifically,
\begin{align*}
    \frac{\lceil pq \rceil -h}{h+1} = \frac{-h-1}{h+1} + \frac{\lceil pq \rceil+1}{h+1} = -1 + \frac{\lceil pq \rceil+1}{h+1} \in [\frac{1}{\lceil pq \rceil}, \lceil pq \rceil]
\end{align*}
Then we know
\begin{align*}
    c_1n^{-c_2}\frac{\lceil pq \rceil-h}{h+1} \in
    [c_1n^{-c_2}\frac{1}{\lceil pq \rceil}, c_1n^{-c_2}\left\lceil pq\right\rceil]
\end{align*}
By assumption, we immediately know $c_1n^{-c_2}\lceil pq \rceil\rightarrow 0$, then with sample size large, $c_1n^{-c_2}\lceil pq \rceil < 1$, which further tells us
\begin{align*}
    \left|\frac{\binom{\lceil pq \rceil}{h+1}(-1)^{h+1} c_1^{h+1} n^{-(h+1)c_2}}{\binom{\lceil pq \rceil}{h}(-1)^h c_1^h n^{-hc_2}}\right|  < 1
    \quad \text{for} ~ h=0,..., \lceil pq \rceil - 1
\end{align*}
% Notice $O(i c_1p^{-c_2}) \leq O(pq c_1p^{-c_2}) = O(c_1p^{1-c_2}q) \rightarrow 0$ and
% \begin{align*}
%     \frac{O(i^h c_1^h p^{-hc_2})}{i c_1p^{-c_2}} = O(i^{h-1} c_1^{h-1} p^{-(h-1)c_2}) \rightarrow 0
% \end{align*}
with which we must have
\begin{align*}
    (1-c_1n^{-c_2})^{\lceil pq \rceil} 
    &> 1 - \lceil pq \rceil( \lceil pq \rceil c_1n^{-c_2}) = 1 - \lceil pq \rceil^2 c_1n^{-c_2}
\end{align*}
As a result,
\begin{align*}
    \mathbb{P}\left(G\subseteq \hat{G}_{k}\right)
    &\geq (1- c_1 n^{-c_2})^{\lceil pq \rceil} [1 - O(K^{-1}(NT)^{-c})]\\
    &\geq (1 - \lceil pq \rceil^2 c_1n^{-c_2})[1 - O(K^{-1}(NT)^{-c})]\\
    &\geq 1 - \lceil pq \rceil^2 c_1n^{-c_2} - O(K^{-1}(NT)^{-c})\\
    &= 1 - O\{n^{-c_2} + K^{-1}(NT)^{-c}\}
\end{align*}


% with which we obtain the upper bound on probability of insufficient selection
% \begin{align*}
%     \mathbb{P}\left(G\nsubseteq \hat{G}_{k}\right)
%     = 1 - \mathbb{P}\left(G\subseteq \hat{G}_{k}\right)
%     &\leq 1 - \mathbb{P}\left(\bigcap\limits_{i=1}^{\lceil pq \rceil} G_i\subseteq \hat{G}_{k,i}\right)\\
%     &=\mathbb{P}\left(\bigcup\limits_{i=1}^{\lceil pq \rceil} G_i\nsubseteq \hat{G}_{k,i}\right)\\
%     &\leq \sum_{i=1}^{\lceil pq \rceil} \mathbb{P}\left(G_i\nsubseteq \hat{G}_{k,i}\right)\\
%     &\leq \sum_{i=1}^{\lceil pq \rceil} (1 - (1-c_1n^{-c_2})^{i} [1 - O(K^{-1}(NT)^{-c})] )\\
%     &= \left\lceil pq \right\rceil - [1 - O(K^{-1}(NT)^{-c})] \sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i}
%     % &= \frac{pq}{2} - [1 - O(K^{-1}(NT)^{-c})] (1-c_1p^{-c_2}) \frac{1- (1-c_1p^{-c_2})^{pq/2}}{c_1p^{-c_2}}\\
%     % &< \sum_{i=1}^{pq/2} (1 - (1-c_1p^{-c_2})^{pq/2} [1 - O(K^{-1}(NT)^{-c})] )\\
%     % &= \frac{pq}{2} - \frac{pq}{2} (1-c_1p^{-c_2})^{pq/2} [1 - O(K^{-1}(NT)^{-c})]\\
%     % &< \frac{pq}{2} - \frac{pq}{2} (1-c_1p^{-c_2})^{pq/2} \\
%     % &\leq \sum_{i=1}^{pq/2} (1 - (1-c_1p^{-c_2})^{pq/2} [1 - O(K^{-1}(NT)^{-c})] )\\
%     % & = \frac{pq}{2} (1 - [1 - O(K^{-1}(NT)^{-c})] (1-c_1p^{-c_2})^{pq/2})
%     % &\longrightarrow 0
% \end{align*}
% \noindent
% First notice
% \begin{align*}
%     (1-c_1n^{-c_2})^{i} 
%     &= \sum_{h=0}^i \binom{i}{h} (-1)^h c_1^h n^{-hc_2}
%     % &= S_1 + S_2
% \end{align*}
% % where
% % \begin{align*}
% %     S_1
% %     &= \sum_{h=0}^{\lfloor i/2 \rfloor} \binom{i}{h} (-1)^h c_1^h p^{-hc_2}\\
% %     % &= 1 - O(i c_1p^{-c_2}) + O(i^2 c_1^2p^{-2c_2}) + \cdots + (-1)^hO(i^h c_1^h p^{-hc_2}) + \cdots\\
% %     S_1
% %     &= \sum_{\lfloor i/2 \rfloor + 1}^i \binom{i}{h} (-1)^h c_1^h p^{-hc_2}
% % \end{align*}
% Now for any $h=0,...,i - 1$, we know
% \begin{align*}
%     \left|\frac{\binom{i}{h+1}(-1)^{h+1} c_1^{h+1} n^{-(h+1)c_2}}{\binom{i}{h}(-1)^h c_1^h n^{-hc_2}}\right| 
%     =  c_1n^{-c_2}\frac{i!/[(h+1)!(i-h-1)!]}{i!/[h!(i-h)!]}
%     = c_1n^{-c_2}\frac{i-h}{h+1}
% \end{align*}
% And specifically,
% \begin{align*}
%     \frac{i-h}{h+1} = \frac{-h-1}{h+1} + \frac{i+1}{h+1} = -1 + \frac{i+1}{h+1} \in [\frac{1}{i}, i]
% \end{align*}
% Since $i\in[1,\lceil pq \rceil]$, we know
% \begin{align*}
%     c_1n^{-c_2}\frac{i-h}{h+1} \in
%     [c_1n^{-c_2}\frac{1}{i}, c_1n^{-c_2} i] \subseteq [c_1n^{-c_2}\frac{1}{\lceil pq \rceil}, c_1n^{-c_2}\left\lceil pq\right\rceil]
% \end{align*}
% By assumption, we immediately know $c_1n^{-c_2}\lceil pq \rceil\rightarrow 0$, then with sample size large, $c_1n^{-c_2}\lceil pq \rceil < 1$, which further tells us
% \begin{align*}
%     \left|\frac{\binom{i}{h+1}(-1)^{h+1} c_1^{h+1} n^{-(h+1)c_2}}{\binom{i}{h}(-1)^h c_1^h n^{-hc_2}}\right| < 1
%     \quad \text{for} ~ h=0,...,i - 1 ~\text{and}~ i=1,2,...,\lceil pq \rceil
% \end{align*}
% % Notice $O(i c_1p^{-c_2}) \leq O(pq c_1p^{-c_2}) = O(c_1p^{1-c_2}q) \rightarrow 0$ and
% % \begin{align*}
% %     \frac{O(i^h c_1^h p^{-hc_2})}{i c_1p^{-c_2}} = O(i^{h-1} c_1^{h-1} p^{-(h-1)c_2}) \rightarrow 0
% % \end{align*}
% with which we must have
% \begin{align*}
%     (1-c_1p^{-c_2})^{i} 
%     &> 1 - i(i c_1n^{-c_2}) = 1 - i^2 c_1n^{-c_2}
% \end{align*}
% % An example,
% % \begin{align*}
% %     (1-c_1p^{-c_2})^{pq/2} 
% %     &= \sum_{h=0}^{pq/2}  \binom{pq/2}{h} (-1)^h c_1^h p^{-hc_2}\\
% %     &= 1 - \frac{pq}{2} c_1p^{-c_2} + \binom{pq/2}{2} c_1^2p^{-2c_2} + \cdots\\
% %     &= 1 - O(c_1p^{1-c_2}q) + O(c_1^2p^{2-2c_2}q^2) + \cdots + O(c_1^hp^{h-hc_2}q^h) + \cdots + O(c_1^{pq-h}p^{pq-h-hc_2}q^{pq-h}) + \cdots\\
% %     &= 1 - O(p^{1-c_2}q)
% % \end{align*}
% Then we sum then up to have
% \begin{align*}
%     \sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i}
%     &> \sum_{i=1}^{\lceil pq \rceil} 1 - i^2 c_1n^{-c_2}\\
%     &= \left\lceil pq\right\rceil - c_1 n^{-c_2} \sum_{i=1}^{\lceil pq \rceil} i^2\\
%     &= \left\lceil pq \right\rceil - c_1 n^{-c_2} \frac{\left\lceil pq \right\rceil(\left\lceil pq \right\rceil+1)(2\lceil pq \rceil+1)}{6}\\
%     &= \left\lceil pq \right\rceil - O\{ n^{-c_2} (q^3 + q^2 + q)\}\\
%     \Rightarrow
%     \left\lceil pq \right\rceil - \sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i}
%     &< O\{ n^{-c_2} (q^3 + q^2 + q)\}\\
%     &= O\{ n^{-c_2}q^3 + n^{-c_2}q^2 + n^{-c_2}q\}
% \end{align*}
% where we have assumed $n^{-c_2} q \longrightarrow 0$. On the other hand,
% \begin{align*}
%     O(K^{-1}(NT)^{-c})\sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i}
%     &< O(K^{-1}(NT)^{-c}) \sum_{i=1}^{\lceil pq \rceil} 1\\
%     &= O(K^{-1}(NT)^{-c})\left\lceil pq \right\rceil\\
%     &= O(K^{-1}(NT)^{-c}q)
%     \longrightarrow 0
% \end{align*}
% And finally,
% \begin{align*}
%     \mathbb{P}\left(G\nsubseteq \hat{G}_{k}\right)
%     \leq&~\left\lceil pq \right\rceil - [1 - O(K^{-1}(NT)^{-c})] \sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i}\\
%     =&~ \left\lceil pq \right\rceil -  \sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i} + O(K^{-1}(NT)^{-c})\sum_{i=1}^{\lceil pq \rceil}(1-c_1n^{-c_2})^{i}\\
%     <&~ O\{ n^{-c_2}q^3 + n^{-c_2}q^2 + n^{-c_2}q\} + O\{K^{-1}(NT)^{-c}q\}\\
%     =&~ O\{n^{-c_2}q^3 + n^{-c_2}q^2 + n^{-c_2}q + K^{-1}(NT)^{-c}q\}\\
%     =&~ O\{n^{-c_2}q + K^{-1}(NT)^{-c}q\}\\
%     =&~ O\{n^{-c_2} + K^{-1}(NT)^{-c}\}
% \end{align*}
% where the last two lines are due to the requirement $0<q<1$.



\end{comment}






\subsection{Proof of Theorem \ref{thm: power_lasso}}

\textbf{Proof sketch.}
We organize the proof step by step, starting from the simplest case and generalize the results in later steps. We begin by focusing on the data of the transition tuples $(\mathbf{s}_t, a_t,r_t, \mathbf{s}_{t+1})$ in a generic iteration of a generic data subset $\mathcal{D}_k$, on which the selection approach we proposed in Algorithm \ref{alg: 2} has power asymptotically to 1.  Given such result, we continue to derive the power of selecting all true variables by our sequential selection, which combines all iterations in $\mathcal{D}_k$. Finally, the power is adjusted due to our majority vote across all subsets $\{\mathcal{D}_k\}_{k=1}^K$, which is exactly the power of SEEK. Such stream of ideas is also in the proof of  general machine learning case in below. Finally, we also provide a necessary and  sufficient condition for a true state variables, as further interpretations of the novel assumption in Conditions \ref{ass: recovery_lasso} and \ref{ass: sep}.

\subsubsection{One Iteration}

\begin{proposition}[Power in one iteration]\label{prop: indep_lasso}
If Conditions \ref{ass: recovery_lasso} and \ref{ass: lasso} hold,  then for each iteration in each data subset,  with probability at least $1-O(p^{-c_1})$, all strong true signals with respect to the current response are selected.
% $
% \cup_{i\in I\cap G_\textrm{M}} G(S_i) \subseteq \widehat{G}.
%     % \frac{\displaystyle|
%     % (\cup_{i\in I\cap G_\textrm{M}} G(S_i))\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap G} G(S_i)|}
%     % \geq 1 -  (\phi+1)|\mathcal{A}| C_5 c_\lambda \kappa_n^{-1}
% $
\end{proposition}
\begin{comment}
\begin{remark}
    The golden ratio refers to the positive solution to $\phi^2 - \phi - 1 = 0$, i.e. $\phi = (1+\sqrt{5})/2$.
\end{remark}
\end{comment}

\begin{remark}
    Here we have abuse of notations. In the above proposition, $\widehat{G}$ refers to the  selected ones, in one generic iteration in one generic data subset, not the same as the union of selections over all iterations, defined in Algorithm \ref{alg: 1}. And we omit all superscripts in $l,k$ for convenience. And the definition of strong true signals are those state variables in $G(S_i,*)$ as defined below.
\end{remark}

\noindent
\textbf{Proof of Proposition \ref{prop: indep_lasso}.} 
%Then for all true signals, $\beta_j^{(a)} \gg cn^{-1/2} ~ \text{for some } a\in\mathcal{A}$ and constant $c>0$. With the same constant $C,c>0$, we immediately have $|\{j\in[p]: \beta_j^{(a)} \gg cn^{-1/2} ~ \text{for some } a\in\mathcal{A} \}| \geq 2/q+C$.
Recall $W_j,j\in[p]$ the knockoffs statistics based on the differences among LASSO estimators $\widehat{\mathbf{B}}$, and the set of true variables $I\cap G_\textrm{M}$ with cardinality $p_0$ (can be either fixed or diverging, with divergence of n). 
% Also recall that, by assumption, for continuous response, with probability asymptotically to 1, there are no ties in the magnitude of
% $W_j$'s and no ties in the components of the LASSO estimates across all actions
% with asymptotic probability one.
\noindent
By LASSO estimation bound in Condition \ref{ass: lasso}, exists $c_1, C_1>0$, s.t. with high probability of at least $1-O(p^{-c_1})$,  for any  $a\in\mathcal{A}$ and $i\in [p+1]$,
\begin{align*}
    C_1p_0^{-1/2}\lambda^{(a)} &\geq ||\widehat{\mathbf{B}}_i^{(a)} - \mathbf{B}_i^{(a,*)}||_2
\end{align*}
which then leads to, for any $j\in[p], a\in\mathcal{A}$ and $i\in [p+1]$,
\begin{align*}
    C_1p_0^{-1/2}\lambda^{(a)} &\geq |\widehat{B}_{j,i}^{(a)} - B_{j,i}^{(a)}| ,\\
    C_1p_0^{-1/2}\lambda^{(a)} &\geq |\widehat{B}_{j+p,i}^{(a)}|
\end{align*}
for $n$ large. Now we still use $I$ to represent  the index set of all current response variables. Then for any $j\in[p]$,
% \begin{align*}
%     \max_{i\in [p]}|\hat{\beta}_{j+p,i}^{(a)}| &\leq C_1\log(n)n^{-1/2}\\
%     \max_{i\in [p]}|\hat{\beta}_{ji}^{(a)} - \beta_{ji}^{(a)}| &\leq C_1\log(n)n^{-1/2}
% \end{align*}
% which  further implies
\begin{align*}
    \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}| &\leq C_1p_0^{-1/2}\lambda\\
    \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)} - B_{ji}^{(a)}| &\leq C_1p_0^{-1/2}\lambda
\end{align*}
In addition, for $j\in[p]$, we now have
\begin{align*}
    W_j 
    \geq - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|
    \geq - C_1p_0^{-1/2}\lambda
    = - C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}
\end{align*}
Then for each $j\in [p]$, combining the above bounds,
\begin{align*}
    W_j  
    &= \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    &= \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)} + B_{ji}^{(a)} - B_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    % &\geq |\hat{\beta}_j^{(a)} + \beta_j^{(a)} - \beta_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}| \quad \text{for any } a\\
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}\{|B_{ji}^{(a)}| - |\widehat{B}_{ji}^{(a)} - B_{ji}^{(a)}|\} - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)} - B_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - 2C_1p_0^{-1/2}\lambda\\
    &= \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}
\end{align*}
Meanwhile, by Condition \ref{ass: recovery_lasso}, we further define the index set of all strong true signals with respect to each true variable
$i\in G_\textrm{M}\cup\{p+1\}$ by
\begin{align*}
    G(S_i,*):= 
    \{ j\in G(S_i):
    \max_{a\in \mathcal{A}} |B_{j,i}^{(a)}| \geq
    \kappa_n n^{-1/2}\sqrt{\log p} \quad\text{for}~ n ~\text{large}\}.
\end{align*} 
which, by definition, is a subset of $G(S_i)$ (and allowed to be a proper subset, even an empty set for some $i$). Especially, when $i=p+1$, $S_i$ stands for reward $R$ (and in below we would show that $G(R,*)\neq\emptyset$).

Still by the above relaxation, for all strong true signals $j\in \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)$ with respect to the current response,
\begin{align*}
    W_j  
    &\geq \max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}| - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}\\
    &= \max_{i\in I}\max_{a\in\mathcal{A}}|B_{ji}^{(a)}| - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}\\
    &\geq \max_{i\in I\cap (G_\textrm{M}\cup\{p+1\})}\max_{a\in\mathcal{A}}|B_{ji}^{(a)}| - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}\\
    &\geq \kappa_n n^{-1/2}\sqrt{\log p}  - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}\\
    &\gg C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}
\end{align*}
given large enough $n$, by the definition of $G(S_i,*)$. 
Then we have observed that true signals obtain $W_j \gg C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}$, while others only have $W_j \geq -C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}$. 
So we know as long as $T\geq\min \{\max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}|: j\in \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*) \} - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p}$, we always have $|\{j:W_j\leq -T\}|=0$, making the ratio to control, $|\{j:W_j\leq -T\}|/|\{j:W_j\geq T\}|$, vanish, whatever the denominator is (recall that for FDP, $0/0$ is defined to be zero).

\begin{comment}
Especially such ratio takes its minimum value of at most $1/|\{j\in[p], \beta_j^{(a)} >0 ~ \text{for some } a\in\mathcal{A} \}|$ (may still have some nulls with large $W$), when $T=\min \{\max_{a\in\mathcal{A}}\beta_j^{(a)}: j\in[p], \beta_j^{(a)} >0 ~ \text{for some } a\in\mathcal{A} \} - 2C_1\log(n)n^{-1/2}$. By Condition \ref{ass: signal_ols} we know 
\begin{align*}
\frac{1}{|\{j\in[p]: \beta_j^{(a)} >0 ~ \text{for some } a\in\mathcal{A} \}|}
\leq \frac{1}{\frac{1}{q}+C_2}    
= \frac{q}{1+C_2q}  < q
\end{align*}
\end{comment}

Such observations implies that we must have 
\begin{align*}
T \leq \min \{\max_{a\in\mathcal{A}}\max_{i\in I}|B_{ji}^{(a)}|: j\in \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*) \} - 2C_1c_\lambda p_0^{-1/2}n^{-1/2}\sqrt{\log p} \end{align*} 
and then it is obvious that,
\begin{align*}
    \{j\in[p]:W_j\geq T\}  \supseteq \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*) \}.
\end{align*}
Especially the overall power takes the form
\begin{align*}
    &\mathbb{E}\left[\frac{\displaystyle|
    \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)|}\right]\\
    =~& \mathbb{E}\left[\frac{\displaystyle|
    \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)|} \mathbf{1}\{||\widehat{\mathbf{B}}_i^{(a)} - \mathbf{B}_i^{(a)}||_1 \leq C_1p_0^{-1/2}\lambda^{(a)} \text{ for all } a\in\mathcal{A}, i\in I \} \right]\\
    &+ \mathbb{E}\left[\frac{\displaystyle|
    \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)|} \mathbf{1}\{||\widehat{\mathbf{B}}_i^{(a)} - \mathbf{B}_i^{(a)}||_1 > C_1p_0^{-1/2}\lambda^{(a)} \text{ for some } a\in\mathcal{A},i\in I\}  \right]\\
    \geq~& \mathbb{E}\left[\frac{\displaystyle|
    \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*)|} \mathbf{1}\{||\widehat{\mathbf{B}}_i^{(a)} - \mathbf{B}_i^{(a)}||_1 \leq C_1p_0^{-1/2}\lambda^{(a)} \text{ for all } a\in\mathcal{A}, i\in I\} \right]\\
    =~& \mathbb{P}\left( ||\widehat{\mathbf{B}}_i^{(a)} - \mathbf{B}_i^{(a)}||_1 \leq C_1p_0^{-1/2}\lambda^{(a)} \text{ for all } a\in\mathcal{A},i\in I  \right)\\
    \geq~& 1-O(p^{-c_1})
\end{align*}

\subsubsection{Power in a Sequence}
%idea: 1) By graph definition, must exist strong for reward; 2) for each true, the path is less than $p_0$, then in such number of iterations, inclusion with high probability is guaranteed.
To continue, we  now consider the power in the whole sequence of selections, for a generic data subset $\mathcal{D}_k, k\in[K]$. From the last part we know, given the index set of the current response variables $I$, in a generic iteration, with probability as least $1-O(p^{-c_1})$,
\begin{align*}
    \{j\in[p]:W_j\geq T\}  \supseteq \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G(S_i,*) \}.
\end{align*}
Then we take three steps to investigate the whole sequence of iterations:

\textbf{Step 1.} By Condition \ref{ass: recovery_lasso}, we immediately know,  for each $j\in G_\textrm{M}$, there is at least one directed path, with only strong edges, from the itself to the reward. On the other way round, it means that there exists some state variable $j\in G_1$ (recall $G_1\subseteq G_\textrm{M}$ corresponds to all the true variables with respect to the reward) which meets the minimal signal strength threshold in Condition \ref{ass: recovery_lasso}, i.e. $\max_{a}|B_{j,i}^{(a)}| \geq \kappa_n n^{-1/2} \sqrt{\log p}$. By such we know $G(R,*)\neq \emptyset$. So the first iteration would lead to a valid selection with some state variables due to the result in the last subsection, i.e. $\widehat{G}_1 \neq \emptyset$.

\textbf{Step 2.} Then for each arbitrarily fixed true state variable with index $j_*\in G_\textrm{M}$, by Condition \ref{ass: recovery_lasso}, there exists a path $S_{j_*}\rightarrow S_{j_{h-1}} \rightarrow S_{j_2} \rightarrow S_{j_1} \rightarrow R$, so that $S_{j_{l+1}}\in G(S_{j_{l}},*)$ for $1\leq l \leq h-2$, $S_{j_{1}}\in G(R,*)$ and $S_{j_{*}}\in G(S_{j_{h-1}},*)$. Then due to the last subsection, with high probability of at least $1-O(p^{-c_1})$, $S_{j_1}$ would be selected, on top of which, $S_{j_2}$ will be selected. Such guarantee goes on until $S_{j_{*}}$ is included in the selection. In addition, due to Condition \ref{ass: lasso}, all those inclusions of variables would happen simultaneously with probability lower bounded by  $1-O(p^{-c_1})$. This shows that  any true variable can be selected with enough number of iterations., and actually the number is at most $h$, i.e. the number of edges in the specific path.

In the above analysis, we implicitly assume $h\geq 3$. However, when $h=1$, the path is just $S_{j_{*}}\rightarrow R$, and the result is already proved due to Step 1. When $h=2$, the path is $S_{j_{*}}\rightarrow S_{j_{1}}\rightarrow R$, the same result can also be proved using the derivation here.

\textbf{Step 3.} Since $|G_\textrm{M}| = p_0 < \infty$ is a fixed constant, we know for any arbitrary true state variable, the path defined in Step 2 has the number of edges $h\leq p_0$. This is because, whenever $h > p_0$, there must be repeated appearance of some state variable in the path specified, and then by reducing the redundancy (the loop of dependence) we can always find a simplified path with $h\leq p_0$. Then still by the uniform bound in Condition \ref{ass: lasso}, we know all true variables will be selected simultaneously within $p_0$ iterations in a sequence, with probability of at least $1-O(p^{-c_1})$. The last step emphasizes that with the divergence of both $n,p$, the iterations we need to have high power is always bounded by $p_0$. Finally, probability to include all true variables is just by
\begin{align*}
    \mathbb{P}(G_\textrm{M} \subseteq \widehat{G}_k)
    %&\geq [1-O(p^{-c_1})][1 - O(K^{-1}(NT)^{-c})]\\
    &= 1- O(p^{-c_1})
\end{align*}

\subsubsection{Power with Majority Vote}\label{pf_vote}
Finally we combine the results from different $\mathcal{D}_k$ with the majority vote, and observe
\begin{align*}
\mathbb{P}(G_\textrm{M} \subseteq \hat{G}) \geq
\mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G_\textrm{M}\subseteq \hat{G}_k\} \geq \alpha) 
&= \mathbb{P}(\frac{1}{K}\sum_{k=1}^K 1 - \mathbf{1}\{G_\textrm{M}\nsubseteq \hat{G}_k\} \geq \alpha)\\
&= \mathbb{P}(1 - \frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G_\textrm{M}\nsubseteq \hat{G}_k\} \geq \alpha)\\
&= \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G_\textrm{M}\nsubseteq \hat{G}_k\} \leq 1 - \alpha)\\
&= 1 - \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G_\textrm{M}\nsubseteq \hat{G}_k\} > 1 - \alpha)
\end{align*}
Since $\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq 0$, by Markov's inequality,
\begin{align*}
    \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G_\textrm{M}\nsubseteq \hat{G}_k\} > 1 - \alpha) 
    \leq~& \frac{\mathbb{E}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G_\textrm{M}\nsubseteq \hat{G}_k\})}{1 - \alpha}\\
    =~& \frac{\frac{1}{K}\sum_{k=1}^K \mathbb{P} (G_\textrm{M}\nsubseteq \hat{G}_k)}{1-\alpha}\\
    \leq~& \frac{O\{p^{-c_1}\}}{1-\alpha}\\
    =~& O\{p^{-c_1}\} \rightarrow 0
    % =~& O\{K^{c_2}(NT)^{-c_2}q^3 + 3K^{c_2}(NT)^{-c_2}q^2 + 2K^{c_2}(NT)^{-c_2}q + K^{-1}(NT)^{-c}q\}
\end{align*}
% In addition, given the comparison
% \begin{align*}
%     \frac{n^{-c_2}q^3}{n^{-c_2}q^2} = q<1 \quad \text{and} \quad \frac{n^{-c_2}q^2}{n^{-c_2}q} = q<1
% \end{align*}
% by the definition of $q$ as the target FDR constraint, we simplify the above probability upper bound by
% \begin{align*}
%     \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \gamma)
%     \leq O\{n^{-c_2}q + K^{-1}(NT)^{-c}q\}
%     \longrightarrow 0
% \end{align*}
%which,  as $NT\rightarrow\infty$, converges to $[p - (1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon}]/(1-\gamma)$ that is arbitrarily close to $0$. (even if in later cases, i.e. LASSO or random forest, where we let $p$ and $p_0$ diverge, we can let $\epsilon$ decay as $n$ increases s.t. $p - (1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon}\rightarrow 0$) 
So we finally conclude
\begin{align*}
    \mathbb{P}(G_\textrm{M} \subseteq \hat{G}) \geq
    1 - O\{p^{-c_1}\}
\end{align*}
In addition, the power is by
\begin{align*}
    \mathbf{E}\left(\frac{|\hat{G}\cap G_\textrm{M}|}{p_0}\right)
    &\geq \mathbf{E}\left(\frac{|\hat{G}\cap G_\textrm{M}|}{p_0} \mathbf{1}\{\hat{G}\supseteq G_\textrm{M} \}\right)\\
    &\geq \mathbb{P}(\hat{G}\supseteq G_\textrm{M})\\
    &= 1 - O\{p^{-c_1}\} \longrightarrow 1
\end{align*}
With that we have shown the Algorithm \ref{alg: 1} (SEEK) has asymptotic power  1.
\begin{remark}
    The fractional scaling $1/(1-\alpha)$ also shows the balance in choosing $\alpha$. On one hand, we need $\alpha$ large so as to only select variables significant enough. On the other hand, such scaling is strictly increasing in $\alpha$, then to have larger power, $\alpha$ should not be too large. That is why numbers in the middle part of $[0,1]$ tend to be good choices, which is also shown in experiments.
\end{remark}
Condition \ref{ass: recovery_lasso} is a novel definition proposed in this work by the language of graphs. Here we further derive a necessary and sufficient condition for true variables, both to provide more interpretations of such condition, and to show it is quite mild. For every remaining edge in the subgraph defined in Condition \ref{ass: recovery_lasso}, we call it a strong edge from some $S_j$ to $S_i$ ($S_j$ is a strong true signal with respect to $S_i$).
\begin{lemma}[True signal criterion]
\label{lemma: true_ctn}
    Assume Condition \ref{ass: recovery_lasso} holds. Then a state variable $S_j$ is in the minimal sufficient state, i.e.  $j\in G_\textrm{M}$, if and only if :
    
    1) there exists another different true variable $S_i$ with a strong edge from $S_j$ to $S_i$; or
    
    2) there is a strong edge from $S_j$ to the reward.
\end{lemma}
\noindent
\textbf{Proof of Lemma \ref{lemma: true_ctn}}

\textbf{a) ``If'' part} We first assume the assumptions after the ``if and only if'' are true. Now we consider an arbitrary state variable with index $j$. If there is already a strong edge from itself to the reward, then the path $S_j \rightarrow R$ would remain in the subgraph defined in Condition \ref{ass: recovery_lasso}, indicating that $j\in G_\textrm{M}$. If such edge from itself to the reward does not exist, then there must be another different true variable $S_{i_1}$, s.t. $S_j\rightarrow S_{i_1}$ is a strong edge. Due to Condition \ref{ass: recovery_lasso}, there is a path that remains in such subgraph, from $S_{i_1}$ to the reward. Then such path attached with the edge $S_j\rightarrow S_{i_1}$ will also be a strong path (a path with only strong edges) that remains in the subgraph, implying that $j\in G_\textrm{M}$.

\textbf{b) ``Only if'' part} Whenever we have true state variable $S_j$, then there must be a strong path from  $S_j$ to the reward. After removing the self-looping dependence edges, if there is only one edge in such path, case 2) is satisfied. If not, case 1) holds. 

\begin{remark}
    This lemma provides a straightforward and efficient way to describe how a true state variable contributes to other variables or the reward. Since such a true one only needs to strongly contribute to either reward or another true variable, Condition \ref{ass: recovery_lasso} is quite mild.

    In addition, the number of strong edges/paths is small compared to all existing  edges/paths among true variables, not to say the number of all potential connections among variables. Take (b) in Figure \ref{fig: graph_eg} as an example. There are only 3 strong edges, i.e. $S_1\rightarrow R$, $S_2\rightarrow R$ and $S_3\rightarrow S_2$. But there are actually 5 edges (either strong or weak). What's more, since potentially any variable may contribute to itself, another one or the reward in the next time point (like $S_1\rightarrow R$, $S_1\rightarrow S_1$, $S_2\rightarrow S_1$, $S_1\rightarrow S_2$), the total number of all potential dependence relations are $3 + 3^2 = 12$. Then we can see that, out of $12$ potential relations, we only require $3$ of them to be strong dependence. Finally, it is obvious to see that there are $3$ different paths from $S_3$ to the reward, and only one of them is a strong one. All the above observations show the large range of flexibility allowed by Condition \ref{ass: recovery_lasso}.
\end{remark}


\begin{comment}
\textbf{Proof of Proposition \ref{prop: indep_lasso}.} 
By Condition \ref{ass: lasso}, with probability at least $1-c_3p^{-c_4}$, for any $a\in\mathcal{A}$ and $i\in [p+1]$,
\begin{align*}
    ||\widehat{B}_i^{(a)} - B_i^{(a)}||_1
    &\leq C_5 p_0\lambda^{(a)}\\
    ||\widehat{B}_i^{(a)} - B_i^{(a)}||_2
    &\leq C_6 p_0^{1/2}\lambda^{(a)}
\end{align*}
with  $\max_{a\in\mathcal{A}}\lambda^{(a)} = c_{\lambda} \{(\log p)/n\}^{1/2}$ for constants $C_5, C_6, c_{\lambda}>0$. For continuous response, with probability asymptotically to $1$, we know there are no ties in the magnitude of all nonzero $W_j$'s as well as no ties in the magnitude of all nonzero $\hat{\beta}_j$'s. All other notations follow exactly the same as in OLS case. Note that by the use of LASSO, when having significant penalty terms, without loss of generality, part of estimate coefficients would be zero, making some of $W_j$ vanish, which is also implicitly assumed in \citet{fan2019rank}. Order $W_j$'s as
\begin{align*}
    |W_{(1)}|\geq|W_{(2)}|\geq...\geq|W_{(p)}|
\end{align*}

For given target FDR $q\in(0,1)$, denote by $j^*$ the order index for thresholding, i.e. $|W_{(j^*)}|= T$. Then when $n$ is large enough, we know $|W_{(j^*+1)}|<T$ (existence of $W_{(j^*+1)}$ is guaranteed by the existence of zero). Recall the definition of $T$:
\begin{align*}
    T=\min \left\lbrace t\in\{W_j,j\in[p]\}: \frac{ |\{j:W_j\leq -t\}|}{|\{j:W_j\geq t\}|}\leq q\right\rbrace
\end{align*}
which specifically means
\begin{align*}
    \frac{|\{j:W_j\leq -T\}|}{|\{j:W_j\geq T\}|}\leq q
\end{align*}
If $W_{(j^*+1)}>0$, then by no tie assumption, $|\{j:W_j\leq -T\}| = |\{j:W_j\leq -W_{(j^*+1)}\}|$, and $|\{j:W_j\geq W_{(j^*+1)}\}| = |\{j:W_j\geq T\}| + 1$, so we observes that
\begin{align*}
    \min \left\lbrace t\in\{W_j,j\in[p]\}: \frac{|\{j:W_j\leq -t\}|}{|\{j:W_j\geq t\}|}\leq q\right\rbrace \leq W_{(j^*+1)} < T
\end{align*}
which leads to a contradiction. Then we know it must be the case
\begin{align*}
    -T < W_{(j^*+1)} \leq 0
\end{align*}
\end{comment}

\begin{comment}
\noindent
\textbf{Scenario 1.} 
$-T < W_{j^*+1} < 0$.

\noindent
By no ties assumption, we then must have
\begin{align*}
    |\{j:W_j\leq  -|W_{(j^*+1)}|\}| &= |\{j:W_j\leq  -T\}| + 1 \quad\text{and}\\
    |\{j:W_j\geq  |W_{(j^*+1)}|\}| &= |\{j:W_j\geq  T\}|
\end{align*}
On the other hand, by the minimum part in the definition of $T$, the ratio below must be large in the sense that
\begin{align*}
    \frac{|\{j:W_j\leq  -|W_{(j^*+1)}|\}|}{|\{j:W_j\geq  |W_{(j^*+1)}|\}|} &> q\\
    \Rightarrow
    \frac{|\{j:W_j\leq  -T\}| + 1}{|\{j:W_j\geq  T\}|} &>q\\
    \Rightarrow
    |\{j:W_j\leq  -T\}| &> q|\{j:W_j\geq  T\}| - 1
\end{align*}
where in the last step we use the result from Lemma \ref{lemma: sel_size} below that $|\{j:W_j\geq  T\}|\geq C_4p_0$ for some constant $C_4\in((qp_0)^{-1}, 1)$, with high probability (asymptotically to 1, such result simultaneously holds with Condition \ref{ass: lasso}), which guarantees that $|\{j:W_j\geq  T\}| > 0$ and $ |\{j:W_j\leq  -T\}| > q|\{j:W_j\geq  T\}|-1  > C_4qp_0 -1 > 0$.


Meanwhile, for any index $j$ with $W_j\leq  -T$, we have
\begin{align*}
    \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| -  \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}| \leq -T\\
    \Rightarrow
    \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| -  \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}| \\
    = \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| + \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|
    -  \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}|\\
    + \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| -  \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    \leq -T + C_5p_0\lambda
    \\
    \Rightarrow
    \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}| \geq T + \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| - C_5p_0\lambda
    \geq T - C_5p_0\lambda
\end{align*}
Together with the coefficients estimation bound we have
\begin{align*}
    |\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} 
    &\geq \sum_{a\in\mathcal{A}}||\widehat{B}_i^{(a)} -  B_i^{(a)}||_1 
\end{align*}
for all $i\in I\cap G_\textrm{M}$. Then
\begin{align*}
|\mathcal{A}|C_5p_0^2\max_{a\in\mathcal{A}}\lambda^{(a)} 
&\geq |I\cap G_\textrm{M}||\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} 
\geq \sum_{i\in I\cap G_\textrm{M}} \sum_{a\in\mathcal{A}}||\widehat{B}_i^{(a)} -  B_i^{(a)}||_1 
\geq \sum_{i\in I\cap G_\textrm{M}} \sum_{a\in\mathcal{A}}\sum_{j\in[p]} |\widehat{B}_{j+p,i}^{(a)}| \\
&\geq \sum_{j\in[p]}\max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}  |\widehat{B}_{j+p,i}^{(a)}|
\geq \sum_{j: W_j\leq -T}\max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}  |\widehat{B}_{j+p,i}^{(a)}|\\
&\geq |\{j: W_j\leq -T\}| (T- C_5p_0\lambda)\\
\Rightarrow
|\mathcal{A}|C_5p_0^2\max_{a\in\mathcal{A}}\lambda^{(a)} 
&\geq (q|\{j: W_j\geq T\}|-1) (T- C_5p_0\lambda)\\
\Rightarrow
T
&\leq \frac{|\mathcal{A}|C_5p_0^2\max_{a\in\mathcal{A}}\lambda^{(a)} }{q|\{j: W_j\geq T\}|-1} + C_5p_0\lambda\\
\Rightarrow
    T
    &\leq \frac{|\mathcal{A}|C_5p_0^2\max_{a\in\mathcal{A}}\lambda^{(a)}}
    {C_4p_0q - 1} + C_5p_0\lambda
    % \leq \frac{|\mathcal{A}|C_5p_0^2\max_{a\in\mathcal{A}}\lambda^{(a)}}
    % {C_3}
    \leq \frac{\lambda\kappa_n}{c_\lambda \phi} + C_5p_0\lambda
\end{align*}

\begin{align*}
    \frac{\phi|\mathcal{A}|C_5c_\lambda p_0^2}{C_4p_0q - 1}  \leq \kappa_n
\end{align*}


% \begin{align*}
%     |\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} 
%     &\geq \sum_{a\in\mathcal{A}}||\hat{\beta}_i^{(a)} -  \beta_i^{(a)}||_1 \geq \sum_{a\in\mathcal{A}}\sum_{j\in[p]} |\hat{\beta}_{j+p,i}^{(a)}| 
%     \geq \sum_{j\in[p]}\max_{a\in\mathcal{A}} |\hat{\beta}_{j+p}^{(a)}|\\
%     &\geq \sum_{j: W_j\leq -T}\max_{a\in\mathcal{A}} |\hat{\beta}_{j+p}^{(a)}|
%     \geq |\{j: W_j\leq -T\}| T\\
%     \Rightarrow
%     |\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} 
%     &\geq (q|\{j: W_j\geq T\}|-2) T\\
%     \Rightarrow
%     T
%     &\leq \frac{|\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} }{q|\{j: W_j\geq T\}|-2}\\
%     \Rightarrow
%     T
%     &\leq \frac{|\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)}}
%     {C_4p_0q - 2}
%     \leq \frac{|\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)}}
%     {C_3}
%     \leq \frac{\lambda\kappa_n}{c_\lambda \phi}
% \end{align*}


where $|\mathcal{A}|,C_4, C_5, c_\lambda, \phi$ are constants, while $\kappa_n$ diverges and $\lambda$ depends on $n,p$ and due to the fact from Condition \ref{ass: slow_div}.
% Notice that although we allow $p_0$ to also diverge,  (here we fix C to global constant?  i.e. independent of $p_0$ [intuition: more and more strong signal])
% % Another way is to require $Cp_0q - 2 \geq c_{min}$ for some positive lower bound, i.e. $c \geq (2 + c_{min})/(p_0q)$. [Intuition: enough strong signals, but total number may be constantly lower bounded] e.g. $Cp_0q - 2 \geq \phi/(\phi+1)$
% the fractional part $p_0/(Cp_0q - 2)$ has derivative
% \begin{align*}
%     \frac{\partial}{\partial p_0} \frac{p_0}{Cp_0q - 2} 
%     = \frac{Cp_0q - 2 - p_0Cq}{(Cp_0q - 2)^2} = \frac{- 2}{(Cp_0q - 2)^2} <0
% \end{align*}
% meaning that is strictly decreasing, then bounded by some constant.
For any index $j$ with $W_j < T$, we have
\begin{align*}
    \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| -  \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}| < T\\
    \Rightarrow
    \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| -  \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}| \\
    = \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| + \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|
    -  \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}|\\
    + \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{ji}^{(a)}| -  \max_{a\in\mathcal{A}}\max_{i\in I}|\widehat{B}_{j+p,i}^{(a)}|\\
    \leq T + C_5p_0\lambda
    \\
    \Rightarrow
    \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}| \geq -T + \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| - C_5p_0\lambda
\end{align*}
Still by such estimation bound,
\begin{align*}
    |I\cap G_\textrm{M}||\mathcal{A}|C_5p_0\lambda
    &\geq \sum_{i\in I\cap G_\textrm{M}}\sum_{a\in\mathcal{A}}||\widehat{B}_i^{(a)} -  B_i^{(a)}||_1
    = \sum_{i\in I\cap G_\textrm{M}}\sum_{a\in\mathcal{A}}\sum_{j\in[p]} \{|\widehat{B}_{j,i}^{(a)} - B_{j,i}^{(a)}| + |\widehat{B}_{j+p,i}^{(a)}|\}\\
    &\geq \sum_{i\in I\cap G_\textrm{M}}\sum_{j\in[p]} 
    \{|\widehat{B}_{j,i}^{(a_{ji})} - B_{j,i}^{(a_{ji})}| + \max_{a\in\mathcal{A}}|\widehat{B}_{j+p,i}^{(a)}|\}
    \quad a_{ji} \text{ is s.t. } |B_{ji}^{(a_{ji})}| = \max_{a\in\mathcal{A}}|B_{ji}^{(a)}|\\
    &\geq  \sum_{i\in I\cap G_\textrm{M}}\sum_{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T} \{|\widehat{B}_{ji}^{(a_{ji})} - B_{ji}^{(a_{ji})}| + \max_{a\in\mathcal{A}}|\widehat{B}_{j+p,i}^{(a)}|\}\\
    &\geq  \sum_{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T} \{|\widehat{B}_{ji}^{(a_{ji})} - B_{ji}^{(a_{ji})}| + \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{j+p,i}^{(a)}|\} \quad \text{for any}~ i\in I\cap G_\textrm{M}\\
    &\geq  \sum_{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T} \{|\widehat{B}_{ji}^{(a_{ji})} - B_{ji}^{(a_{ji})}| + \max_{a\in\mathcal{A}}\max_{i\in I\cap G_\textrm{M}}|\widehat{B}_{ji}^{(a)}| - T - C_5p_0\lambda\}\\
    &\geq  \sum_{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T} \{|\widehat{B}_{ji}^{(a_{ji})} - B_{ji}^{(a_{ji})}| + |\widehat{B}_{ji}^{(a_{ji})}| - T - C_5p_0\lambda\}\\
    &\geq  \sum_{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T} \{|B_{ji}^{(a_{ji})}| - T - C_5p_0\lambda\}\\
    &=  \sum_{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T} \{\max_{a\in\mathcal{A}}|B_{ji}^{(a)}| - T - C_5p_0\lambda\}\\
    &\geq |\{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T\}| (\max_{i\in I\cap G_\textrm{M}}\min_{j\in G(S_i)}\max_{a\in\mathcal{A}}|B_{ji}^{(a)}| - T - C_5p_0\lambda)
\end{align*}
which then leads to
\begin{align*}
    |\{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T\}|
    &\leq \frac{  |I\cap G_\textrm{M}||\mathcal{A}|C_5p_0\lambda}{\max_{i\in I\cap G_\textrm{M}}\min_{j\in G(S_i)}\max_{a\in\mathcal{A}}|B_{ji}^{(a)}| - T - C_5p_0\lambda}\\
    \Rightarrow
    \frac{|\cup_{i\in I\cap G_\textrm{M} }G(S_i)\cap\hat{G}|}{|\cup_{i\in I\cap G_\textrm{M} }G(S_i)|} 
    &= 1- \frac{|\{j\in \cup_{i\in I\cap G_\textrm{M} }G(S_i): W_j< T\}|}{|\cup_{i\in I\cap G_\textrm{M} }G(S_i)|}\\
    &\geq 1- \frac{  |\mathcal{A}|C_5p_0^2\lambda}{|\cup_{i\in I\cap G_\textrm{M} }G(S_i)|(\max_{i\in I\cap G_\textrm{M}}\min_{j\in G(S_i)}\max_{a\in\mathcal{A}}|B_{ji}^{(a)}| - T - C_5p_0\lambda)}\\
    &\geq 1- \frac{  |\mathcal{A}|C_5p_0^2\lambda}{C_4p_0(\kappa_n\{\frac{\log p}{n}\}^{1/2} - \frac{\lambda\kappa_n}{c_\lambda \phi}   - C_5p_0\lambda - C_5p_0\lambda)}\\
    &= 1- \frac{  |\mathcal{A}|C_5p_0 c_\lambda \{\frac{\log p}{n}\}^{1/2}}{C_4(\kappa_n\{\frac{\log p}{n}\}^{1/2} - \frac{\kappa_n}{\phi}\{\frac{\log p}{n}\}^{1/2} - 2C_5p_0c_\lambda\{\frac{\log p}{n}\}^{1/2})}\\
    &= 1- \frac{  |\mathcal{A}|C_5p_0 c_\lambda }{C_4(\frac{\phi-1}{\phi}\kappa_n - 2C_5p_0c_\lambda)}
    % &= 1 - \frac{ |\mathcal{A}|C_5 c_\lambda }{\frac{\phi-1}{\phi}\kappa_n}\\
    % &= 1 - (\phi+1) |\mathcal{A}|C_5 c_\lambda \kappa_n^{-1}
\end{align*}
where in the middle lines the denominator is guaranteed to be strictly positive due to Conditions \ref{ass: recovery_lasso} and $\phi>1$.
\begin{align*}
    C_4(\frac{\phi-1}{\phi}\kappa_n - 2C_5p_0c_\lambda) > |\mathcal{A}|C_5p_0^2 c_\lambda 
\end{align*}
\noindent
\textbf{Scenario 2.} 
$W_{j^*+1} = 0$ and $\{j: W_j \leq -T\}> |\mathcal{A}|\phi C_5 c_\lambda p_0 \kappa_n^{-1}$

Then we immediately know $\hat{G} = \{j: W_j >0\}$ and by no ties property, $\{j: W_j \leq -T\} = \{j: W_j < 0\}$. And whenever $W_j\leq -T \Rightarrow \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}|\geq T$. Now given $\{j: W_j < 0\} > |\mathcal{A}|\phi c_0 c_\lambda p_0 \kappa_n^{-1}$,  still by coefficients estimation error bound, with probability at least $1- c_3 p^{-c_4}$,
\begin{align*}
    |\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} 
    &\geq \sum_{a\in\mathcal{A}}||\hat{\beta}^{(a)} -  \beta^{(a)}||_1 \geq \sum_{a\in\mathcal{A}}\sum_{j\in[p]} |\hat{\beta}_{j+p}^{(a)}| 
    \geq \sum_{j\in[p]}\max_{a\in\mathcal{A}} |\hat{\beta}_{j+p}^{(a)}|\\
    &\geq \sum_{j: W_j\leq -T}\max_{a\in\mathcal{A}} |\hat{\beta}_{j+p}^{(a)}|
    \geq |\{j: W_j\leq -T\}| T\\
    \Rightarrow
    |\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} 
    &\geq |\mathcal{A}|\phi C_5 c_\lambda p_0 \kappa_n^{-1} T\\
    \Rightarrow
    T
    &\leq \frac{|\mathcal{A}|C_5p_0\max_{a\in\mathcal{A}}\lambda^{(a)} }{|\mathcal{A}|\phi C_5 c_\lambda p_0 \kappa_n^{-1}}\\
    &= \frac{\lambda \kappa_n}
    {\phi c_\lambda}
\end{align*}



Then similar as procedure in Scenario 1 we have
\begin{align*}
     |\{j\in G: W_j< T\}|
    &\leq \frac{  |\mathcal{A}|C_5p_0\lambda}{\min_{j\in G}\max_{a\in\mathcal{A}}|\beta_j^{(a)}| - T}
\end{align*}
then
\begin{align*}
    \frac{|\hat{G} \cap G|}{p_0} 
    &= 1- \frac{|\{j\in G: W_j< T\}|}{p_0}\\
    &\geq 1- \frac{  |\mathcal{A}|C_5p_0\lambda}{p_0(\min_{j\in G}\max_{a\in\mathcal{A}}|\beta_j^{(a)}| - T)}\\
    &\geq 1- \frac{  |\mathcal{A}|C_5p_0\lambda}{p_0(\kappa_n\{\frac{\log p}{n}\}^{1/2} - \frac{\lambda\kappa_n}{c_\lambda \phi})}\\
    % &= 1- \frac{  |\mathcal{A}|c_0p_0 c_\lambda \{\frac{\log p}{n}\}^{1/2}}{p_0(\kappa_n\{\frac{\log p}{n}\}^{1/2} - \frac{\kappa_n}{\phi}\{\frac{\log p}{n}\}^{1/2})}\\
    % &= 1 - \frac{ |\mathcal{A}|c_0 c_\lambda }{\frac{\phi-1}{\phi}\kappa_n}\\
    &= 1 - (\phi+1) |\mathcal{A}|C_5 c_\lambda \kappa_n^{-1}
\end{align*}

\noindent
\textbf{Scenario 3.} 
$W_{j^*+1} = 0$ and $\{j: W_j \leq -T\} \leq |\mathcal{A}|\phi C_5 c_\lambda p_0 \kappa_n^{-1}$

Still we know $\hat{G} = \{j: W_j >0\}$ and  $\{j: W_j \leq -T\} = \{j: W_j < 0\}$. 
Notice
\begin{align*}
    \left|\hat{G}\cap G\right| 
    &= \left|\{j:W_j> 0\}\cap G\right|\\
    &= \left|(\{j:W_j \neq 0\}\backslash\{j:W_j < 0\})\cap G\right|\\
    &= \left|\{j:W_j \neq 0\}\cap G\right| - \left|\{j:W_j < 0\}\cap G\right|
\end{align*}
By estimation bound in LASSO, with probability $1-c_3p^{-c_4}$, for constant $C_5>0$,
\begin{align*}
    |\mathcal{A}|C_5p_0\lambda
    &\geq \sum_{a\in\mathcal{A}} ||\hat{\beta}^{(a)} - \beta^{(a)}||_1
    \geq \sum_{j\in [p]}\sum_{a\in\mathcal{A}} |\hat{\beta}_j^{(a)} - \beta_j^{(a)}|\\
    &\geq \sum_{j\in G:~ \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| = 0}~
    \sum_{a\in\mathcal{A}} |\hat{\beta}_j^{(a)} - \beta_j^{(a)}|\\
    &= \sum_{j\in G:~ \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| = 0}~
    \sum_{a\in\mathcal{A}} | \beta_j^{(a)}|\\
    &\geq \sum_{j\in G:~ \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| = 0}
    \max_{a\in\mathcal{A}} | \beta_j^{(a)}|\\
    &\geq \left|\{j\in[p]: \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| = 0\}\cap G\right|\min_{j\in G}\max_{a\in\mathcal{A}} | \beta_j^{(a)}|\\
\end{align*}
leading to
\begin{align*}
    \left|\{j\in[p]: \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| = 0\}\cap G\right| 
    &\leq \frac{ |\mathcal{A}|C_5p_0\lambda }{\min_{j\in G}\max_{a\in\mathcal{A}} | \beta_j^{(a)}|}\\
    \Rightarrow
    \left|\{j\in[p]: \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| \neq 0\}\cap G\right|
    &\geq p_0 -  \frac{ |\mathcal{A}|C_5p_0\lambda }{\min_{j\in G}\max_{a\in\mathcal{A}} | \beta_j^{(a)}|}
\end{align*}
On the other hand, for any index $j\in[p]$, if $W_j=0$, by definition, $\max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}| = 0$. Due to non-existence of nonzero ties in $|\hat{\beta}_j^{(a)}|$ across all $j\in[2p]$ and all $a\in\mathcal{A}$, we must have $\max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| = \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}| = 0$, and then immediately $\hat{\beta}_j^{(a)} = \hat{\beta}_{j+p}^{(a)} = 0$ for all $a\in\mathcal{A}$. By such observation we can know $\{j\in[p]:W_j \neq 0\} \supset \{j\in[p]:\hat{\beta}_j^{(a)} \neq 0 ~ \text{for some}~ a\} = \{j\in[p]: \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| \neq 0\}$,
which then leads to
\begin{align*}
    \left|\{j\in[p]:W_j \neq 0\}\cap G\right| 
    &\geq \left|\{j\in[p]: \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| \neq 0\}\cap G\right| \geq p_0 -  \frac{ |\mathcal{A}|C_5p_0\lambda }{\min_{j\in G}\max_{a\in\mathcal{A}} | \beta_j^{(a)}|}\\
    \Rightarrow
    \left|\hat{G}\cap G\right|
    &\geq p_0 -  \frac{ |\mathcal{A}|C_5p_0\lambda }{\min_{j\in G}\max_{a\in\mathcal{A}} | \beta_j^{(a)}|} - |\mathcal{A}|\phi C_5 c_\lambda p_0 \kappa_n^{-1}\\
    &\geq p_0 -  \frac{ |\mathcal{A}|C_5p_0\lambda }{ \kappa_n\{(\log p)/n\}^{1/2}} - |\mathcal{A}|\phi C_5 c_\lambda p_0 \kappa_n^{-1}\\
    &= p_0 - (\phi+1)|\mathcal{A}| C_5 c_\lambda p_0 \kappa_n^{-1}\\
    \Rightarrow
    \frac{\left|\hat{G}\cap G\right|}{p_0}
    &\geq 1 -  (\phi+1)|\mathcal{A}| C_5 c_\lambda \kappa_n^{-1}
\end{align*}

\begin{lemma} [Enough selection size]\label{lemma: sel_size}
Under Conditions  \ref{ass: strong_lasso}, \ref{ass: lasso}, with the same constants $C_3>0$ and $C_4\in((2+C_3)(p_0q)^{-1}, 1)$, it holds that $|\{j\in[p]: W_j \geq T \}| \geq C_4p_0$, with probability at least $1-c_3p^{-c_4}$ (actually simultaneously with Condition \ref{ass: lasso}), for positive constants $c_3, c_4$, the same from Condition \ref{ass: lasso}.
\end{lemma}
\noindent
\textbf{Proof of Lemma \ref{lemma: sel_size}.} 
By estimation bound in 2-norm, with high probability of at least $1-c_3p^{-c_4}$,
\begin{align*}
    C_6p_0^{1/2}\lambda &\geq ||\hat{\beta}^{(a)} - \beta^{(a)}||_2 ~ \text{for}~ \forall a\in\mathcal{A}
\end{align*}
which then leads to
\begin{align*}
    c^\prime[p_0\log(p)/n]^{1/2}
    &\geq |\hat{\beta}_j^{(a)} - \beta_j^{(a)}| ~ \text{for}~ 
    \forall j\in[p], \forall a\in\mathcal{A},\\
    c^\prime[p_0\log(p)/n]^{1/2}
    &\geq |\hat{\beta}_{j+p}^{(a)}| ~ \text{for}~ 
    \forall j\in[p], \forall a\in\mathcal{A}
\end{align*}
for some new constant $c^\prime$. Then we immediately know, for any $j\in[p]$,
\begin{align*}
    \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}| &\leq  c^\prime[p_0\log(p)/n]^{1/2}\\
    \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)} - \beta_j^{(a)}| &\leq  c^\prime[p_0\log(p)/n]^{1/2}
\end{align*}
Then for $j\in[p]$, we can have
\begin{align*}
    W_j \geq - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}| \geq -  c^\prime[p_0\log(p)/n]^{1/2}
\end{align*}
Meanwhile for each $j\in [p]$, combining above bounds,
\begin{align*}
    W_j  
    &= \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}|\\
    &= \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)} + \beta_j^{(a)} - \beta_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}|\\
    % &\geq |\hat{\beta}_j^{(a)} + \beta_j^{(a)} - \beta_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}| \quad \text{for any } a\\
    &\geq \max_{a\in\mathcal{A}}\{|\beta_j^{(a)}| - |\hat{\beta}_j^{(a)} - \beta_j^{(a)}|\} - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}|\\
    &\geq \max_{a\in\mathcal{A}}|\beta_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)} - \beta_j^{(a)}| - \max_{a\in\mathcal{A}}|\hat{\beta}_{j+p}^{(a)}|\\
    &\geq \max_{a\in\mathcal{A}}|\beta_j^{(a)}| - 
    2 c^\prime[p_0\log(p)/n]^{1/2}
\end{align*}
Especially for strong signals, still by above relaxation, due to Condition \ref{ass: strong_lasso},
\begin{align*}
    W_j  
    &\geq \max_{a\in\mathcal{A}}|\beta_j^{(a)}| - 2 c^\prime[p_0\log(p)/n]^{1/2}
    \gg  c^\prime[p_0\log(p)/n]^{1/2}
\end{align*}
\noindent
We know as long as $T\geq\min \{\max_{a\in\mathcal{A}}|\beta_j^{(a)}|: j\in[p], \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \} - 2c^\prime[p_0\log(p)/n]^{1/2}$, for $n$ large, we always have $|\{j:W_j\leq -T\}|=0$, making the ratio to control, $(1+|\{j:W_j\leq -T\}|)/|\{j:W_j\geq T\}|$, increasing in $T$. Especially such ratio takes its minimum value of at most $1/|\{j\in[p]: \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \}|$ (may still have some nulls with large $W$, which makes such ratio even smaller), when\\ $T=\min \{\max_{a\in\mathcal{A}}|\beta_j^{(a)}|: j\in[p], \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \} - 2c^\prime[p_0\log(p)/n]^{1/2}$. By strong signal assumption, i.e. Condition \ref{ass: strong_lasso}, we know 
\begin{align*}
\frac{1}{|\{j\in[p]: \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \}|}
\leq \frac{1}{C_4p_0}    
< \frac{q}{2}<q
\end{align*}
then we must have $T \leq \min \{\max_{a\in\mathcal{A}}|\beta_j^{(a)}|: j\in[p], \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \} - 2c^\prime[p_0\log(p)/n]^{1/2}$, which leads to
\begin{align*}
    \{j:W_j\geq T\}  
    &\supseteq \{j\in[p]: \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \}\\
    \Rightarrow
    |\{j:W_j\geq T\}|  
    &\geq |\{j\in[p]: \max_{a\in\mathcal{A}}|\beta_j^{(a)}| \gg  [p_0\log(p)/n]^{1/2} \}|\\
    &\geq C_4p_0
\end{align*}
\end{comment}

\begin{comment}
\subsubsection{Power in Temporally Dependent Data}
Recall the result in independent data:
\begin{align*}
     \left|\hat{G}\cap G\right|
    \geq [1 -  (\phi+1)|\mathcal{A}| C_5 c_\lambda \kappa_n^{-1}] p_0
\end{align*}
with probability at least $1 - c_3p^{-c_4}$. Then to make sure $G\subseteq \hat{G}$, we actually need
\begin{align*}
    [1 -  (\phi+1)|\mathcal{A}| C_5 c_\lambda \kappa_n^{-1}] p_0 > p_0 - 1\\
    \Rightarrow
    (\phi+1)|\mathcal{A}| C_5 c_\lambda \kappa_n^{-1} p_0 < 1\\
     \Rightarrow
    (\phi+1)|\mathcal{A}| C_5 c_\lambda  p_0 < \kappa_n
\end{align*}
which is satisfied due to Condition \ref{ass: slow_div}, so with probability at least $1 - c_3p^{-c_4}$, $G\subseteq \hat{G}$. 
Then consider the result from the first till $l$-th iteration, inside one subset $\mathcal{D}_k$, where the selection is $\widehat{G}_{k,h}$ and the underlying truth is $\widetilde{G}_{k,h},h\in[l]$.
% \begin{align*}
%     &~\mathbb{P}\left(G_l\subseteq \hat{G}_{k,l} \right)\\
%     \geq& ~ \mathbb{P}\left(\{G_l\subseteq \hat{G}_{k,l}\} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~  \forall i\}\cap \{G_h\subseteq \hat{G}_{k,h}, h\in[l-1]\}\right)\\
%     \geq& ~ \mathbb{P}\left(\{G_l\subseteq \hat{G}_{k,l}\} | \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i\}\cap \{G_h\subseteq \hat{G}_{k,h}, h\in[l-1]\}\right)\\
%     ~\cdot& ~ \mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l-1]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     ~\cdot& ~ \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     %&\geq (1-\epsilon) [1 - O(K^{-1}(NT)^{-c})]\\
% \end{align*}
% Then to evaluate the second factor term in the above, we claim that for $l\in[p]$, we always have
% \begin{align*}
%     \mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)
%     \geq (1-c_1p^{-c_2})^l
% \end{align*}
% This is because first, if $l=1$, it is just the result shown in the independent data case. Then, if we already know such claim holds for $1,2,...,l$, then for $l+1$, by definition,
% \begin{align*}
%     &\mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l+1]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     =~& \mathbb{P}\left( \{G_{l+1}\subseteq \hat{G}_{k,l+1}\} | \{G_h\subseteq \hat{G}_{k,h}, h\in[l]\} \cap [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     \cdot~& \mathbb{P}\left( \{G_h\subseteq \hat{G}_{k,h}, h\in[l]\} | [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \right)\\
%     \geq~& (1-c_1p^{-c_2})(1-c_1p^{-c_2})^l\\
%     =~& (1-c_1p^{-c_2})^{l+1}
% \end{align*}
By Mathematical Induction with the same idea as in OLS case, from the result from Lemma \ref{lemma: temp_dep}, we can have
\begin{align*}
    ~& \mathbb{P}\left(\left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\}\right)\\
    \geq~& \mathbb{P}\left( \left\{\bigcap_{h=1}^{l} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \} \right)\\
    \geq~& (1 - c_3p^{-c_4})^l [1 - O\{K^{-1}(NT)^{-c}\}]
\end{align*}

% Especially we also know
% \begin{align*}
% 	\mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|}\right]
% 	=& \mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|} \mathbf{1}{\{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\}} \right]\\
% 	&+ \mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|} \mathbf{1}{\{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] \neq [\mathbf{z}_i,b_i] \text{ for some i}\}} \right]\\
% 	\geq& \mathbb{E}\left[\frac{\displaystyle|\widehat{G}_{k,l}\cap \mathcal{H}_{0,l}|}{\displaystyle|\mathcal{H}_{0,l}|} \mathbf{1}{\{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\}} \right]\\
% 	\geq& (1 - \epsilon) \mathbb{P}\left( [\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i] \text{ for all i}\} \right)\\
% 	\geq& (1 - \epsilon) [1 - O(K^{-1}(NT)^{-c})]
% \end{align*}

%Such probability converges to $(1-c_1p^{-c_2})^{l}$ as $NT\rightarrow\infty$, the value of which  can be chosen arbitrarily large towards $1$. Note that it takes its minimum of $(1-c_1p^{-c_2})^{p}$ at the last iteration. For $(1-c_1p^{-c_2})^{p}\geq \alpha_0$, we only need $c_1p^{-c_2}\leq 1- \alpha_0 ^{1/p}$.

\end{comment}

\begin{comment}
\subsubsection{Power in One Sequence}
% At most $p/\min_i|G_i|<pq/2$ selections on $\mathcal{D}_k$ in a sequence, where by convention, for some $i$-th iteration, the selection power is defined to be $1$, i.e. $G_i\subseteq \hat{G}_{k,i}$, if there are no true variables left to be selected. In addition, even if there exists some variable with index $j\in G_i$ that is not selected exactly in $i$-th iteration, there is still possibility that it is included in other iterations due to randomness, then we can have
Similar to OLS case, Appendix section \ref{pf_seq_ols}, recall from above analysis, for each iteration $i$ (when true variables exist), with probability at least $1 - c_3p^{-c_4}$, $|\hat{G}_{k,i}|\geq (2+C_3)/q$,  due to Lemma \ref{lemma: sel_size}. Then the total number of iterations needed for such subset $\mathcal{D}_k$ is upper bounded by 
\begin{align*}
    \frac{p}{\min_i |\hat{G}_{k,i}|} \leq \frac{p}{(2+C_3)/q} = \frac{pq}{2+C_3} < \frac{pq}{2}
\end{align*}
which shows that, possibly, there are at most $\lceil pq/2 \rceil$ selections on $\mathcal{D}_k$ in a sequence, where by convention, for some $i$-th iteration, the selection power is defined to be $1$, i.e. $G_i\subseteq \hat{G}_{k,i}$, if there are no true variables left to be selected. In addition, even if there exists some variable with index $j\in G_i$ that is not selected exactly in $i$-th iteration, there is still possibility that it is included in other iterations due to randomness, then to be exact,
% \begin{align*}
%     \mathbb{P}\left(G\subseteq \hat{G}_{k}\right)
%     % &\geq \mathbb{P}\left(G\subseteq \hat{G}_{k} ~\text{and}~ \# \text{iterations} \leq \lceil pq/2 \rceil \right)\\
%     % % &= \mathbb{P}\left(G\subseteq \hat{G}_{k}|\# \text{iterations} \leq \lceil pq \rceil\right) \mathbb{P}\left(\# \text{iterations} \leq \lceil pq \rceil \right)\\
%     % &\geq \mathbb{P}\left(\bigcap\limits_{i=1}^{\lceil pq \rceil} G_i\subseteq \hat{G}_{k,i}  ~\text{and}~ \# \text{iterations} \leq \lceil pq/2 \rceil\right)\\
%     % &= \mathbb{P}\left(\bigcap\limits_{i=1}^{\lceil pq/2 \rceil} G_i\subseteq \hat{G}_{k,i}\right)
%     &\geq \mathbb{P}\left(\bigcap\limits_{i=1}^{\lceil pq/2 \rceil} \{G_i\subseteq \hat{G}_{k,i}\}\right)
% \end{align*}
% \begin{align*}
%     \mathbb{P}\left(G\subseteq \hat{G}_{k}\right)
%     &\geq \mathbb{P}\left(\bigcap
%     \limits_{i=1}^{pq/2}\left\{ G_i\subseteq \hat{G}_{k,i}\right\}\right)
% \end{align*}
according to Lemma \ref{lemma: temp_dep} and \ref{lemma: inc}, till adopting the same approach as in Appendix section \ref{pf_seq_ols}, we know
\begin{align*}
    \mathbb{P}\left(G\subseteq \hat{G}_{k} \right)
    &\geq \mathbb{P}\left(G\subseteq \hat{G}_{k} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \} \right)\\
    &\geq \mathbb{P}\left( \left\{\bigcap_{h=1}^{\lceil pq/2 \rceil} \left\{\widetilde{G}_{k,h}\subseteq \hat{G}_{k,h}\right\}\right\} \cap \{[\mathbf{s}_{k-1+(i-1)K},a_{k-1+(i-1)K}] = [\mathbf{z}_i,b_i],~ \forall i \} \right)\\
    &\geq (1- c_3 p^{-c_4})^{\lceil pq/2 \rceil} [1 - O(K^{-1}(NT)^{-c})]
\end{align*}
Again we observe that
\begin{align*}
    (1-c_3 p^{-c_4})^{\lceil pq/2 \rceil} 
    &= \sum_{h=0}^{\lceil pq/2 \rceil} \binom{\lceil pq/2 \rceil}{h} (-1)^h c_3^h p^{-hc_4}
    % &= S_1 + S_2
\end{align*}
% where
% \begin{align*}
%     S_1
%     &= \sum_{h=0}^{\lfloor i/2 \rfloor} \binom{i}{h} (-1)^h c_1^h p^{-hc_2}\\
%     % &= 1 - O(i c_1p^{-c_2}) + O(i^2 c_1^2p^{-2c_2}) + \cdots + (-1)^hO(i^h c_1^h p^{-hc_2}) + \cdots\\
%     S_1
%     &= \sum_{\lfloor i/2 \rfloor + 1}^i \binom{i}{h} (-1)^h c_1^h p^{-hc_2}
% \end{align*}
Also, for any $h=0,..., \lceil pq/2 \rceil - 1$,
\begin{align*}
    \left|\frac{\binom{\lceil pq/2 \rceil}{h+1}(-1)^{h+1} c_3^{h+1} p^{-(h+1)c_4}}{\binom{\lceil pq/2 \rceil}{h}(-1)^h c_3^h p^{-hc_4}}\right| 
    =  c_3 p^{-c_4}\frac{\lceil pq/2 \rceil!/[(h+1)!(\lceil pq/2 \rceil-h-1)!]}{\lceil pq/2 \rceil!/[h!(\lceil pq/2 \rceil-h)!]}
    = c_3 p^{-c_4}\frac{\lceil pq/2 \rceil-h}{h+1}
\end{align*}
where
\begin{align*}
    \frac{\lceil pq/2 \rceil -h}{h+1}  \in [\frac{1}{\lceil pq/2 \rceil}, \lceil pq/2 \rceil]
\end{align*}
and
\begin{align*}
    c_3p^{-c_4}\frac{\lceil pq/2 \rceil-h}{h+1} \in
    [c_3p^{-c_4}\frac{1}{\lceil pq/2 \rceil}, c_3p^{-c_4}\left\lceil pq/2 \right\rceil]
\end{align*}
By assumption in LASSO case, we still have $c_3 p^{-c_4}\lceil pq/2 \rceil\rightarrow 0$, and $c_3 p^{-c_4}\lceil pq/2 \rceil < 1$ for $n$  (and then $p$ as a result) large, which guarantees
\begin{align*}
    \left|\frac{\binom{\lceil pq/2 \rceil}{h+1}(-1)^{h+1} c_3^{h+1} p^{-(h+1)c_4}}{\binom{\lceil pq/2 \rceil}{h}(-1)^h c_3^h p^{-hc_4}}\right|  < 1
    \quad \text{for} ~ h=0,..., \lceil pq/2 \rceil - 1
\end{align*}
% Notice $O(i c_1p^{-c_2}) \leq O(pq c_1p^{-c_2}) = O(c_1p^{1-c_2}q) \rightarrow 0$ and
% \begin{align*}
%     \frac{O(i^h c_1^h p^{-hc_2})}{i c_1p^{-c_2}} = O(i^{h-1} c_1^{h-1} p^{-(h-1)c_2}) \rightarrow 0
% \end{align*}
then we can have s similar bound as in OLS,
\begin{align*}
    (1-c_3 p^{-c_4})^{\lceil pq/2 \rceil} 
    &> 1 - \lceil pq/2 \rceil( \lceil pq/2 \rceil c_3p^{-c_4}) = 1 - \lceil pq/2 \rceil^2 c_3p^{-c_4}
\end{align*}
What is more, we should note that by Condition \ref{ass: strong_lasso}, $C_4\in((2+C_3)(qp_0)^{-1},1)$, which implicitly requires that $(2+C_3)(qp_0)^{-1} < 1$, so $p_0q$ is lower bounded by $2+C_3$. By definition, $p_0\leq p$, leading to $pq > 2+C_3$. 
With that said,
\begin{align*}
    \mathbb{P}\left(G\subseteq \hat{G}_{k}\right)
    &\geq (1- c_3 p^{-c_4})^{\lceil pq/2 \rceil} [1 - O(K^{-1}(NT)^{-c})]\\
    &\geq (1 - \lceil pq/2 \rceil^2 c_3p^{-c_4})[1 - O(K^{-1}(NT)^{-c})]\\
    &\geq 1 - p^2q^2 c_3p^{-c_4} - O(K^{-1}(NT)^{-c})\\
    &= 1 - O\{p^{2-c_4}q^2 + K^{-1}(NT)^{-c}\}
\end{align*}

% Still adopting the same approach as in Appendix section \ref{pf_seq_ols} we can derive
% \begin{align*}
%     \mathbb{P}\left(G\nsubseteq \hat{G}_{k}\right)
%     % = 1 - \mathbb{P}\left(G\subseteq \hat{G}_{k}\right)
%     % &\leq 1 - \mathbb{P}\left(\bigcap\limits_{i=1}^{pq/2}\left\{ G_i\subseteq \hat{G}_{k,i}\right\}\right)\\
%     % &=\mathbb{P}\left(\bigcup\limits_{i=1}^{pq/2} \left\{G_i\nsubseteq \hat{G}_{k,i}\right\}\right)\\
%     % &\leq \sum_{i=1}^{pq/2} \mathbb{P}\left(G_i\nsubseteq \hat{G}_{k,i}\right)\\
%     % &\leq \sum_{i=1}^{pq/2} (1 - (1-c_1p^{-c_2})^{i} [1 - O(K^{-1}(NT)^{-c})] )\\
%     % &= \frac{pq}{2} - [1 - O(K^{-1}(NT)^{-c})] \sum_{i=1}^{pq/2} (1-c_1p^{-c_2})^{i}
%     &\leq \left\lceil\frac{pq}{2}\right\rceil - [1 - O(K^{-1}(NT)^{-c})] \sum_{i=1}^{\lceil pq/2 \rceil} (1-c_1p^{-c_2})^{i}
%     % &= \frac{pq}{2} - [1 - O(K^{-1}(NT)^{-c})] (1-c_1p^{-c_2}) \frac{1- (1-c_1p^{-c_2})^{p}}{c_1p^{-c_2}}
% \end{align*}
% Again we observe that
% \begin{align*}
%     (1-c_1p^{-c_2})^{i} 
%     &= \sum_{h=0}^i \binom{i}{h} (-1)^h c_1^h p^{-hc_2}
%     % &= S_1 + S_2
% \end{align*}
% % where
% % \begin{align*}
% %     S_1
% %     &= \sum_{h=0}^{\lfloor i/2 \rfloor} \binom{i}{h} (-1)^h c_1^h p^{-hc_2}\\
% %     % &= 1 - O(i c_1p^{-c_2}) + O(i^2 c_1^2p^{-2c_2}) + \cdots + (-1)^hO(i^h c_1^h p^{-hc_2}) + \cdots\\
% %     S_1
% %     &= \sum_{\lfloor i/2 \rfloor + 1}^i \binom{i}{h} (-1)^h c_1^h p^{-hc_2}
% % \end{align*}
% Now for $h=0,...,i - 1$, we know
% \begin{align*}
%     \left|\frac{\binom{i}{h+1}(-1)^{h+1} c_1^{h+1} p^{-(h+1)c_2}}{\binom{i}{h}(-1)^h c_1^h p^{-hc_2}}\right| 
%     =  c_1p^{-c_2}\frac{i!/[(h+1)!(i-h-1)!]}{i!/[h!(i-h)!]}
%     = c_1p^{-c_2}\frac{i-h}{h+1}
% \end{align*}
% where
% \begin{align*}
%     %\frac{i-h}{h+1} = \frac{-h-1}{h+1} + \frac{i+1}{h+1} = -1 + \frac{i+1}{h+1} \in [\frac{1}{i}, i]
%     \frac{i-h}{h+1}  \in [\frac{1}{i}, i]
% \end{align*}
% and
% \begin{align*}
%     c_1p^{-c_2}\frac{i-h}{h+1} \in
%     [c_1p^{-c_2}\frac{1}{i}, c_1p^{-c_2} i] \subseteq [c_1p^{-c_2}\left\lceil\frac{pq}{2}\right\rceil^{-1}, c_1p^{-c_2}\left\lceil\frac{pq}{2}\right\rceil]
% \end{align*}
% By assumption, we immediately know $c_1p^{-c_2}\lceil pq/2 \rceil\rightarrow 0$, then with sample size large, $c_1p^{-c_2}\lceil pq/2 \rceil<1$, which further tells us
% \begin{align*}
%     \left|\frac{\binom{i}{h+1}(-1)^{h+1} c_1^{h+1} p^{-(h+1)c_2}}{\binom{i}{h}(-1)^h c_1^h p^{-hc_2}}\right| < 1
%     \quad \text{for} ~ h=0,...,i - 1, ~ \text{and} ~ i \leq \lceil pq/2 \rceil
% \end{align*}
% % Notice $O(i c_1p^{-c_2}) \leq O(pq c_1p^{-c_2}) = O(c_1p^{1-c_2}q) \rightarrow 0$ and
% % \begin{align*}
% %     \frac{O(i^h c_1^h p^{-hc_2})}{i c_1p^{-c_2}} = O(i^{h-1} c_1^{h-1} p^{-(h-1)c_2}) \rightarrow 0
% % \end{align*}
% with which we must have
% \begin{align*}
%     (1-c_1p^{-c_2})^{i} 
%     &> 1 - i(i c_1p^{-c_2}) = 1 - i^2 c_1p^{-c_2}
% \end{align*}
% % An example,
% % \begin{align*}
% %     (1-c_1p^{-c_2})^{pq/2} 
% %     &= \sum_{h=0}^{pq/2}  \binom{pq/2}{h} (-1)^h c_1^h p^{-hc_2}\\
% %     &= 1 - \frac{pq}{2} c_1p^{-c_2} + \binom{pq/2}{2} c_1^2p^{-2c_2} + \cdots\\
% %     &= 1 - O(c_1p^{1-c_2}q) + O(c_1^2p^{2-2c_2}q^2) + \cdots + O(c_1^hp^{h-hc_2}q^h) + \cdots + O(c_1^{pq-h}p^{pq-h-hc_2}q^{pq-h}) + \cdots\\
% %     &= 1 - O(p^{1-c_2}q)
% % \end{align*}
% Then we sum then up to have
% \begin{align*}
%     \sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i}
%     &> \sum_{i=1}^{\lceil pq/2 \rceil} 1 - i^2 c_1p^{-c_2}\\
%     &= \left\lceil\frac{pq}{2}\right\rceil - c_1 p^{-c_2} \sum_{i=1}^{\lceil pq/2 \rceil} i^2\\
%     &= \left\lceil\frac{pq}{2}\right\rceil - c_1 p^{-c_2} \frac{\left\lceil\frac{pq}{2}\right\rceil(\left\lceil\frac{pq}{2}\right\rceil+1)(\left\lceil pq \right\rceil+1)}{6}\\
%     &= \left\lceil\frac{pq}{2}\right\rceil - O\{ p^{-c_2} (p^3q^3 + p^2q^2 + pq)\}\\
%     \Rightarrow
%     \left\lceil\frac{pq}{2}\right\rceil - \sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i}
%     &< O\{ p^{-c_2} (p^3q^3 + p^2q^2 + pq)\}\\
%     &= O\{p^{3-c_2}q^3 + p^{2-c_2} q^2 + p^{1-c_2} q\}
% \end{align*}
% where we have assumed $p^{1-1/3c_2} q \longrightarrow 0$. Finally we should note that by Condition \ref{ass: strong_lasso}, $C\in((2+C^\prime)(qp_0)^{-1},1)$, which implicitly requires that $(2+C^\prime)(qp_0)^{-1} < 1$, so $p_0q$ is lower bounded by $2+C^\prime$. By definition, $p_0\leq p$, leading to $pq > 2+C^\prime$. On the other hand, we immediately see that 
% \begin{align*}
%     \frac{p^{3-c_2}q^3}{p^{2-c_2} q^2} &= pq > 2+C^\prime \quad \text{and}\\
%     \frac{p^{2-c_2}q^2}{p^{1-c_2} q} &= pq > 2+C^\prime
% \end{align*}
% which further simplifies the above probability upper bound by
% \begin{align*}
%     \left\lceil\frac{pq}{2}\right\rceil - \sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i}
%     &< O\{p^{3-c_2}q^3\}
% \end{align*}
% \noindent
% On the other hand,
% \begin{align*}
%     O(K^{-1}(NT)^{-c})\sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i}
%     &< O(K^{-1}(NT)^{-c}) \sum_{i=1}^{\lceil pq/2 \rceil} 1 \\
%     &< O(K^{-1}(NT)^{-c})\left\lceil\frac{pq}{2}\right\rceil\\
%     &= O(K^{-1}(NT)^{-c}pq)
%     \longrightarrow 0
% \end{align*}
% And finally,
% \begin{align*}
%     \mathbb{P}\left(G\nsubseteq \hat{G}_{k}\right)
%     \leq&~\left\lceil\frac{pq}{2}\right\rceil - [1 - O(K^{-1}(NT)^{-c})] \sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i}\\
%     =&~ \left\lceil\frac{pq}{2}\right\rceil -  \sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i} + O(K^{-1}(NT)^{-c})\sum_{i=1}^{\lceil pq/2 \rceil}(1-c_1p^{-c_2})^{i}\\
%     <&~ O\{p^{3-c_2}q^3\} + O\{K^{-1}(NT)^{-c}pq\}\\
%     =&~ O\{p^{3-c_2}q^3 + K^{-1}(NT)^{-c}pq\}\longrightarrow 0
% \end{align*}

% \begin{align*}
%     p^{-c_2} + 2p^{-2c_2} + 
%     3p^{-3c_2} + \cdots
%     &= 
%     p^{-c_2} + p^{-2c_2} + 
%     p^{-3c_2} + \cdots +
%     p^{-2c_2} + 
%     p^{-3c_2} + \cdots +
%     p^{-3c_2} + 
%     p^{-4c_2} + \cdots\\
%     &= p^{-c_2} \frac{1}{1-p^{-c_2}} + 
%     p^{-2c_2} \frac{1}{1-p^{-c_2}}  + 
%     p^{-3c_2} \frac{1}{1-p^{-c_2}}  + \cdots\\
%     &= p^{-c_2} \frac{1}{(1-p^{-c_2})^2} \longrightarrow 0
% \end{align*}

\end{comment}

\begin{comment}
\subsubsection{Power with Majority Vote}
By similar idea as in OLS case, Appendix section \ref{pf_vote}, we finally have
\begin{align*}
\mathbb{P}(G \subseteq \hat{G}) 
% \geq \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\subseteq \hat{G}_k\} \geq \gamma) 
% &= \mathbb{P}(\frac{1}{K}\sum_{k=1}^K 1 - \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq \gamma)\\
% &= \mathbb{P}(1 - \frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq \gamma)\\
% &= \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \leq 1 - \gamma)\\
% &= 1 - \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \gamma)
&\geq 1 - \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \alpha)
\end{align*}
with 
% \begin{align*}
%     \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \gamma) 
%     % &\leq \frac{\mathbb{E}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\})}{1 - \gamma}\\
%     % &= \frac{\frac{1}{K}\sum_{k=1}^K \mathbb{P} (G\nsubseteq \hat{G}_k)}{1-\gamma}\\
%     &< \frac{O\{p^{3-c_2}q^3+ K^{-1}(NT)^{-c}pq\}}{1-\gamma}
% \end{align*}
\begin{align*}
    \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \alpha)
    &< \frac{O\{p^{2-c_4}q^2 + K^{-1}(NT)^{-c}\}}{1-\alpha}\\
    &= O\{p^{2-c_4}q^2 + K^{-1}(NT)^{-c}\}
    \longrightarrow 0
\end{align*}
% Since $\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} \geq 0$, by Markov's inequality,
% \begin{align*}
%     \mathbb{P}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\} > 1 - \gamma) 
%     &\leq \frac{\mathbb{E}(\frac{1}{K}\sum_{k=1}^K \mathbf{1}\{G\nsubseteq \hat{G}_k\})}{1 - \gamma}\\
%     &= \frac{\frac{1}{K}\sum_{k=1}^K \mathbb{P} (G\nsubseteq \hat{G}_k)}{1-\gamma}\\
%     &< \frac{O\{p^{3-c_2}q^3 + 3p^{2-c_2} q^2 + 2p^{1-c_2} q + K^{-1}(NT)^{-c}pq\}}{1-\gamma}\\
%     &\longrightarrow 0
% \end{align*}
%which,  as $NT\rightarrow\infty$, converges to $[p - (1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon}]/(1-\gamma)$ that is arbitrarily close to $0$. (even if in later cases, i.e. LASSO or random forest, where we let $p$ and $p_0$ diverge, we can let $\epsilon$ decay as $n$ increases s.t. $p - (1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon}\rightarrow 0$) 
With that we have shown the Algorithm \ref{alg: 1} (SEEK) has asymptotic power  1.
\begin{remark}
    The fractional scaling $1/(1-\alpha)$ also shows the balance in choosing $\alpha$. On one hand, we need $\alpha$ large so as to only select variables significant enough. On the other hand, such scaling is strictly increasing in $\alpha$, then to have larger power, $\alpha$ should not be too large. That is why numbers in the middle part of $[0,1]$ tend to be good choices, which is also shown in experiments.
\end{remark}

\end{comment}

\subsection{Proof of Theorem \ref{thm: power_ml}}

\subsubsection{One Iteration}

% Then we immediately know, for any $j\in[p]$,
% \begin{align*}
%     \max_{a\in\mathcal{A}}|VI_{j+p}^{(a)}| &< \delta(n)\\
%     %\max_{a\in\mathcal{A}}|\hat{\beta}_j^{(a)} - \beta_j^{(a)}| &\leq \delta(n)
% \end{align*}

\begin{proposition}[Power in one iteration]\label{prop: indep_ml}
    If Condition \ref{ass: sep} holds, then for each iteration in each data subset, when $n$ is large, with probability at least $1-O(p^{-c_2})$, all strong true signals with respect to the current response are included in the selection. 
\end{proposition} 
\begin{remark}
    In the whole proof, we fix an arbitrary data subset $\mathcal{D}_k$. And we omit all superscripts in iteration number $l$ for convenience.
    Similar to the LASSO case, and due to the settings by Conditions \ref{ass: sep}, for a generic data subset $\mathcal{D}_k$, with respect to any response variable $S_i, i\in G_\textrm{M}\cup\{p+1\}$, we can further define its strong true signals by the set
    \begin{align*}
        G^{(k)}(S_i,*) = \{j\in G(S_i): \max_{a\in\mathcal{A}} |VI_{j,i}^{(a,k)}| \geq \gamma(n,p) \quad \text{for} ~ n  ~ \text{large}\}.
    \end{align*}
    and note that such set for the same $i$ can be different with different $k\in[K]$.
\end{remark}
\noindent
\textbf{Proof of Proposition \ref{prop: indep_ml}}.
We still use $I$ to denote the index set of all response variables in the current iteration.  From Condition \ref{ass: sep}, we have, with probability at least $1-O(p^{-c_2})$, for all $j\in[2p]\backslash G_\textrm{M}$ and $i\in[p+1]$,
\begin{align*}
    2\max_{a\in\mathcal{A}} |VI_{j,i}^{(a,k)}| < \gamma(n,p).
\end{align*}
Then it is straightforward to see that
\begin{align*}
    \max_{a\in\mathcal{A}}\max_{i\in I}|VI_{j+p,i}^{(a,k)}| 
    < \frac{1}{2}\gamma(n,p)
\end{align*}
for any $j\in[p]$.
Then by definition, for any $j\in[p]$, we can have
\begin{align*}
    W_j^{(k)} \geq - \max_{a\in\mathcal{A}}\max_{i\in I}|VI_{j+p,i}^{(a,k)}| 
    > -\frac{1}{2}\gamma(n,p)
\end{align*}
On the other hand, for each state variable $j\in [p]$, by similar idea as in the LASSO case, 
\begin{align*}
    W_j^{(k)} 
    &= \max_{a\in\mathcal{A}}\max_{i\in I}|VI_{j,i}^{(a,k)}| - \max_{a\in\mathcal{A}}\max_{i\in I}|VI_{j+p,i}^{(a,k)}|\\
    &> \max_{a\in\mathcal{A}}\max_{i\in I}|VI_{j,i}^{(a,k)}| - \frac{1}{2}\gamma(n,p)
\end{align*}
given large enough $n$. Especially for any strong true signal $j\in \cup_{i\in I\cap(G_\textrm{M}\cup\{p+1\})}G^{(k)}(S_i,*)$,
\begin{align*}
    W_j^{(k)}
    &> \max_{a\in\mathcal{A}}\max_{i\in I}|VI_{j,i}^{(a,k)}| - \frac{1}{2}\gamma(n,p)\\
    &\geq 
    \max_{i\in I\cap (G_\textrm{M}\cup\{p+1\})}\max_{a\in\mathcal{A}}|VI_{j,i}^{(a,k)}| - \frac{1}{2}\gamma(n,p)\\
    &\geq \gamma(n,p)- \frac{1}{2}\gamma(n,p)\\
    &=  \frac{1}{2}\gamma(n,p)
\end{align*}
due to Condition \ref{ass: sep}. Especially, the probability of Condition \ref{ass: sep}  being satisfied is lower bounded by
$1 - O(p^{-c_2})$.
Now we have observed some separation in $W_j^{(k)}$'s between the strong true state variables and the nulls, then with a similar argument as in the LASSO case, we can have 
\begin{align*}
    \{j\in[p]:W_j^{(k)}\geq T\}  \supseteq \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G^{(k)}(S_i,*) \}.
\end{align*}
with probability at least $1-O(p^{-c_2})$.
\begin{comment}
We have observed that true signals obtain $W_j > \max_{j\in [2p]\backslash G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}|$, while all null variables only have $W_j \geq -\max_{j\in [2p]\backslash G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}|$. 
\noindent
So we know as long as $T \geq \min_{j\in G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}| - \max_{j\in [2p]\backslash G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}|$, we always have $|\{j:W_j\leq -T\}|=0$, making the ratio to control, $(1+|\{j:W_j\leq -T\}|)/|\{j:W_j\geq T\}|$, increasing in $T$. Especially such ratio takes its minimum value of at most $1/|G|$ (may still have some nulls with large $W$, with low probability), when $T = \min_{j\in G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}| - \max_{j\in [2p]\backslash G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}|$. By true signal assumption we know 
\begin{align*}
\frac{1}{|G|}
\leq \frac{1}{\frac{1}{q}+C_7}    
= \frac{q}{1+C_7q}  < q
\end{align*}
which means we must have $T \leq \min_{j\in G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}| - \max_{j\in [2p]\backslash G}\max_{a\in\mathcal{A}}|VI_{j}^{(a)}|$, and,
\begin{align*}
    \{j:W_j\geq T\}  \supseteq G
\end{align*}
meaning that the power is 1 with probability at least $1-c_5p^{-c_6}$. 
\end{comment}
And the power is lower bounded with
\begin{align*}
    \mathbb{E}\left[\frac{\displaystyle|
    \cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G^{(k)}(S_i,*)\cap\widehat{G}|}{\displaystyle|\cup_{i\in I\cap (G_\textrm{M}\cup\{p+1\})} G^{(k)}(S_i,*)|}\right]
    \geq 1-O(p^{-c_2})
\end{align*}
\subsubsection{Selections in Sequences with Majority Vote}
Then the extension to sequential selection and majority vote are similar to those in LASSO case. Finally we can have the power
\begin{align*}
    \mathbf{E}\left(\frac{|\widehat{G}\cap G_\textrm{M}|}{p_0}\right)
    &\geq \mathbf{E}\left(\frac{|\widehat{G}\cap G_\textrm{M}|}{p_0} \mathbf{1}\{\widehat{G}\supseteq G_\textrm{M} \}\right)\\
    &\geq 1 - O\{ p^{-c_2}\}
    \longrightarrow 1
\end{align*}













% \begin{remark}
% To be exact, since $(1-\epsilon) \frac{1- (1-\epsilon)^{p}}{\epsilon} = \sum_{i\in[p]} (1-\epsilon)^{i} < p (1-\epsilon)^{p}$, 
% \end{remark}



% \subsection{Equivalence Results in data splitting}

% In Algorithm \ref{alg: 2}, to make sure action variable is always selected without being concerned in the FDR/mFDR control procedure, we split the data conditional on actions. It is straight forward to imagine that in further iterative selections described in Algorithm \ref{alg: 2}, data should be further partitioned conditional on $\{a\} \cup G_1$, where $G_1$ is the selection $\hat{G}$ just after finishing Step 1 of Algorithm \ref{alg: 2}. However, as seen in later steps there, we still continue regressions and computation of statistics on $\mathcal{D}^{(j)}$, those partitioned only conditional on actions. The reasons consist of two parts, and for convenience, we denote the collection of subsets split conditional on $\{a\} $ and on $\{a\} \cup G_1$ as $\mathcal{F}(a)$ and $\mathcal{F}(a, G_1)$ respectively, and let $\mathcal{I} = \{1,...,p\}$. Further denote variables corresponding to an index set $G$ as $S_G$.


% First, if we perform knockoff selection on $\mathcal{F}(a, G_1)$ with potential dimensions $\mathcal{I}\backslash G_1$, due to the property of conditional distribution from multi-normal, we know
% $
% S_{\mathcal{I}\backslash G_1}|
% S_{ G_1}
% $
% is also normal. Then following above standard machine in proofs, we can achieve FDR/mFDR control in one iteration of selection, i.e. from  $G_1$ to $G_2$, where $G_2 = G_1 \cup \Delta G$ with $\Delta G$ the new selection after one iteration in Step 4 of Algorithm \ref{alg: 2}. We remark here that this shows that FDR/mFDR is control in each iteration towards former selected dimensions.


% However, when in all iterations we still focus on $\mathcal{F}(a)$, we should emphasize that regression and calculation of dimensional importance is always over all dimensions $\mathcal{I}$, while feature statistics are only calculated for potential dimensions $\mathcal{I}\backslash G_1$. Then notice that by construction of $\tilde{\mathbf{s}}$, we proved that for any $j\in\{1,...,m\}$ and any subset $B\subset \{1,...,p\}$,
% \begin{align*}
	% [ S^{(j)} ~ \tilde{S}^{(j)} ]_{\text{swap}(B)} \overset{d}{=} [ S^{(j)} ~ \tilde{S}^{(j)} ]
	% \end{align*}
% Since $\mathcal{I}\backslash G_1 \subset \mathcal{I}$, we know above Lemma \ref{lemma: 1} in \ref{apdx: c.2} still holds when making selection from $G_1$ to $G_2$ according Algorithm \ref{alg: 2}, since all we need for proof is to restrict $B\subset \mathcal{I}\backslash G_1$. Then in this approach control is also achieved.


% Combining above results on two approaches, we see that they achieve exactly the same FDR/mFDR control in each iteration. Then when adopting only the second approach, i.e. the one we chose in Algorithm \ref{alg: 2}, we do not need to split data sets smaller and smaller with number of iterations increasing (otherwise sample size for each distinct regression can be too small starting from certain iteration). In addition, there is no need to update the knockoffs in each iteration. In conclusion, our choice in Algorithm \ref{alg: 2} fully inherits the results of FDR/mFDR control if performing regressions conditional on all selected dimensions, and reduce the workload of computation.

%\bibliographystyle{agsm}
%\bibliography{references.bib}























\end{document}
