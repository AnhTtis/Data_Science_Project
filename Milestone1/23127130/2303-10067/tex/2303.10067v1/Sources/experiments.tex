\section{Experiments}
\label{experiments}

This section presents the experimental results of the proposed approach to the DBLP dataset. 


\subsection{Dataset}
%Although there are several proposed datasets for Author Name Disambiguation, most of them are not practical and do not reflect the real scenario. Section~\ref{subsec:exdata} lists the most used datasets and why they cannot be used to evaluate our approach.


The following datasets are widely used to evaluate author name disambiguation approaches but the results on these datasets cannot reflect the results on real scenario streaming data.


\begin{itemize}[leftmargin=*]
    \item \textbf{ORCID~\footnote{\url{https://figshare.com/articles/ORCID_Public_Data_File_2017/5479792}}:} it is the largest accurate dataset as the publication is assigned to the author only after authorship claim or another rigorous authorship confirmation. However, this accuracy comes at the cost of the number of assignments. Our investigation shows that most of the registered authors are not assigned to any publication and an important number of authors are not even registered. This is because most of the authors are not keen to claim their publications due to several reasons.  
    
    \item \textbf{KDD Cup 2013~\footnote{\url{https://www.kaggle.com/c/kdd-cup-2013-author-paper-identification-challenge}}:} it is a large dataset that consists of 2.5M papers authored by 250K authors. All author metadata are available including affiliation. 
    
    \item \textbf{Manually labelled (e.g. PENN~\footnote{\url{http://clgiles.ist.psu.edu/data/nameset_author-disamb.tar.zip}}, QIAN~\footnote{\url{https://github.com/yaya213/DBLP-Name-Disambiguation-Dataset}}, AMINER~\footnote{\url{http://arnetminer.org/lab-datasets/disambiguation/rich-author-disambiguation-data.zip}}, KISTI~\footnote{\url{http://www.lbd.dcc.ufmg.br/lbd/collections/disambiguation/DBLP.tar.gz/at_download/file}}):} These datasets are supposed to be very accurate since they are manually labelled. However, this process is expensive and time-consuming and, therefore, it can cover only a small portion of authors who share the same names. 
    
\end{itemize}


In this work, we collected our dataset from the DBLP bibliographic repository\footnote{\url{https://dblp.uni-trier.de/xml/} (July 2020)}. The DBLP version of July 2020 contains 5.4 million bibliographic records such as conference papers, articles, thesis, etc., from various fields of research. 
 As stated by the maintainers of DBLP~\footnote{\url{https://dblp.org/faq/How+accurate+is+the+data+in+dblp.html}}, the accuracy of the data is not guaranteed. However, a lot of effort is put into manually disambiguating homonym cases when reported by other users. Consequently, we are aware of possible homonym cases that are not resolved yet. 
 From the repository, we collected only records of publications published in journals and proceedings. Each record in this collection represents metadata information of a publication with one or more authors, title, journal, year of publication and a few other attributes. The availability of these attributes differs from one reference to another. Also, the authors in DBLP who share the same name have a suffix number to differentiate them. For instance, the authors with the same name ‘Bing Li’ are given suffixes such as ‘Bing Li 0001’, and ‘Bing Li 0002’. The statistical details of the used DBLP collection are shown in Table~\ref{tab:dblp_stat}.



% \begin{tabular}{l@{\hskip 1cm}l}
%     \# of records & $5258623$\\
%     \# of unique authors & $2665634$\\
%     \# of unique author names & $2613577$\\
%     \# of unique atomic name variates & $1555517$
% \end{tabular}
\begin{table}[]
    \caption{Statistical details of the used DBLP collection. }
    \label{tab:dblp_stat}
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \# of records     &  $5258623$\\
    \hline
    \# of unique authors & $2665634$\\
    \hline
    \# of unique author names & $2613577$\\
     \hline
    \# of unique atomic name variates & $1555517$\\
    \hline
    \end{tabular}
    \vspace{-0.3cm}
\end{table}


Figure~\ref{fig:authrec} indicates that the majority of target authors in the sub-collections (each sub-collection includes all records of authors with the same name) have distinct full names. However, a considerable number of them share full names, leading to a significant challenge, particularly when multiple authors (e.g. over 80 in 4 out of 5 sub-collections) share the same full name but have an unequal number of publications. In such cases, it becomes more challenging to differentiate these authors from the dominant author with the same name.



\begin{figure}[ht]
\centering
  \includegraphics[width=0.8\textwidth]{Figures/author_name.pdf}
  \caption{The \emph{log} frequency of authors sharing the same full name for the top five sub-collections.}
  \label{fig:authrec}
\end{figure}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.8\textwidth]{Figures/records_name.pdf}
  \caption{The \emph{log} frequency of records with the same full name of the target author for the top five sub-collections.}
  \label{fig:authname}
\end{figure}


Figure~\ref{fig:authname} illustrates the log frequency of bibliographic records with the same full name in the top five sub-collections used in this paper. As illustrated, in all sub-collections, the target authors of around half of the records authored a few records (less than 5) and have unique names. Although it is simple to distinguish these authors when their full names occur, it is extremely challenging to recognize them among more than $2000$ authors sharing the same atomic name variate due to the unbalance of records with the other authors. 



Figure~\ref{exp:frequency} shows the frequency of authors sharing the same names and the same atomic name variates. As can be seen, the problem is more critical when the authors are cited with their atomic name variate as there are five atomic name variates shared by around $11.5k$ authors. This makes the problem of disambiguation critical because not only targets authors who might share the same atomic name variate but also their co-authors. For instance, we observed publications authored by the pair of co-authors having the atomic name variates: \emph{Y. Wang} and \emph{Y. Zhang}. However, they refer to different \emph{Y. Wang} and \emph{Y. Zhang} pairs of real-world authors.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.2]{Figures/frequency.pdf}
    \caption{Frequency of authors sharing the same atomic name variate (Blue) / the same full name (Red).}
    \label{exp:frequency}
\end{figure}
\vspace{-0.5cm}




Since our approach gathers authors with the same name variates, $261464$ models are required to disambiguate all author names in our collection. Therefore, we present in this paper the experimental results on 5 models corresponding to the highest number of authors sharing the same name variates. Table~\ref{tab:stat} presents statistical details of the five sub-collections which demonstrates the challenges inherent in author name disambiguation in real-world scenarios.  \textbf{\# R2A} for instance, in some publications, two co-authors have the same exact names. This makes the disambiguation more difficult as these authors share not only their names but also co-authors and papers. 



\begin{table}[]
    \caption{Statistical details of the top 5 sub-collections of authors sharing the same atomic name variates, where \textbf{\# ANV} is the corresponding atomic name variate, \textbf{\# UTA} is the number of unique target authors, \textbf{\# RCD} is the number of bibliographic records, \textbf{\# UCA} is the number of unique co-author full names, \textbf{\# UAN} is the number of unique target author full names, \textbf{\# R2A} is the number of records with two co-authors of the same record having the same names or the same atomic name variates and \textbf{\# R3A} is the number of records with three co-authors of the same record having the same names or the same atomic name variates. For \textbf{\# R2A} and \textbf{\# R3A}, it is not necessary that the authors have the same name / atomic name variate as the target author but most probably.}
    \label{tab:stat}
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}
    \hline
         &  `Y Wang'   & `Y Zhang'   & `Y Chen'   & `Y Li'   & `Y Liu'\\
    \hline
    \textbf{\# UTA}     &  2601   & 2285   & 2260   & 2166   & 2142\\
    \hline
    \textbf{\# RCD}     &  37409  & 33639  & 26155  & 29154  & 27691\\
    \hline
    \textbf{\# UCA}     &  43199  & 39389  & 33461  & 35765  & 33754\\
    \hline
    \textbf{\# UAN}     &  2005   & 1667   & 2034   & 1734   & 1606\\
    \hline
    \textbf{\# R2A}     &  582    & 598    & 316    & 372    & 338\\
    \hline
    \textbf{\# R3A}     &  13     & 12     & 4      & 4      & 3\\
    \hline
    \end{tabular}
    \vspace{-0.3cm}
\end{table}




To ensure a credible evaluation and result reproducibility in real scenarios, we split the records in each sub-collection into a training set ($\thicksim 70\%$), validation set ($\thicksim 15\%$) and testing set ($\thicksim 15\%$) in terms of records/target author. Specifically, for each target author, we randomly split the corresponding records. If the target author did not author enough publications for the split, we prioritize the training set, then validation and finally the test set. Consequently, the number of samples is not necessarily split according to $70:15:15$ as the number of co-authors differs among publications. Moreover, it is highly likely that the records of a unique target author are completely different among the three sets. Consequently, it is difficult for the model to recognize the appropriate author only from his/her co-authors and research area. However, we believe that this is more realistic and a perfect simulation of the real scenario. 

To account for possible name variates, each input sample of full names is duplicated, where the duplicate down sample full names of all co-authors to atomic name variates. Note that this is applied to training, validation and test sets. The goal is to let the model capture all name variates for each author and his/her co-authors. In none of the sets, the variates are mixed in a single sample as we assume that this case is very less likely to occur in the real world. The experiments were conducted on a machine with the following specifications:
\begin{itemize}
    \item Processor: AMD Ryzen Threadripper 1950X 16-Core
    \item RAM: 12 GB 
    \item Graphics card: NVIDIA Titan V GV100
\end{itemize}
The algorithm was implemented in Python 3.7 using the TensorFlow library.





\subsection{Results}

The existing AND approaches use different datasets to design and evaluate their models. This lead to different assumptions and challenge disparity. Unfortunately, the codes to reproduce the results of these approaches are not available or easily accessed~\cite{hussain2017survey}. Therefore, it is not possible to fairly compare \emph{WhoIs} against baseline approaches. For future work, our code and the used datasets are publicly available~\footnote{\url{https://doi.org/10.5281/zenodo.7744775}}. %Moreover, we provide an online demo of \emph{WhoIs}~\footnote{anonymous link} to be permanently available after training all models.


Table~\ref{tab:result0} presents the result of \emph{WhoIs} on the sub-collections presented in Table~\ref{tab:stat}. The label \emph{All} in the table denotes that all samples were predicted twice, one with full names of the target author and its co-authors and another time with only their atomic name variates, whereas the label \emph{ANV} denotes that only samples with atomic names are predicted. The obtained results show that an important number of publications are not properly assigned to their appropriate authors. This is due to the properties of the sub-collections which were discussed above and statistically presented in Table~\ref{tab:stat}. For example, 1) two authors with the same common name authoring a single publication. 2) more than one author with the same common atomic name variate authoring a single publication, 3) number of authors with the same full name, 4) the uncertainty of the accuracy of the dataset, etc. 


\begin{table}[]
    \caption{Detailed results of \emph{WhoIs} on the sub-collections corresponding to the top five of authors sharing the same atomic name variates in the DBLP repository. The results are presented in terms of Micro average precision (\textbf{MiAP}), Macro average precision (\textbf{MaAP}), Micro average recall (\textbf{MiAR}), Macro average recall (\textbf{MaAR}), Micro average F1-score (\textbf{MiAF1}) and Macro average F1-score (\textbf{MaAF1}). \textbf{ANV} denotes that only atomic name variates were used for all target authors and all their co-authors.}
    \label{tab:result0}
    \centering
    \begin{tabular}{|l|c|c|c|c|c|}

    \hline
            &  `Y Wang'   & `Y Zhang'   & `Y Chen'   & `Y Li'   & `Y Liu'\\

    \hline
    \textbf{MaAP}(ANV)&  $0.226$  & $0.212$  & $0.255$  & $0.193$  & $0.218$\\
    \textbf{MaAP}(All)&  $0.387$  & $0.351$  & $0.404$  & $0.342$  & $0.347$\\
    \hline
    \textbf{MaAR}(ANV)&  $0.299$    & $0.276$   & $0.301$   & $0.229$   & $0.267$\\
    \textbf{MaAR}(All)&  $0.433$    & $0.383$   & $0.409$   & $0.339$   & $0.361$\\
    \hline
    \textbf{MaAF1}(ANV)&  $0.239$    & $0.220$    & $0.258$     & $0.195$     & $0.223$\\
    \textbf{MaAF1}(All)&  $0.385$    & $0.342$    & $0.383$     & $0.321$     & $0.332$\\
    \hline
    \textbf{MiAF1}(ANV)&   $0.274$   & $0.278$    & $0.366$    & $0.260$    & $0.322$\\
    \textbf{MiAF1}(All)&  $0.501$    & $0.482$    & $0.561$    & $0.492$    & $0.504$\\
    \hline
    \end{tabular}
    \vspace{-0.2 cm}
\end{table}


Although the comparison is difficult and cannot be completely fair, we compare \emph{WhoIs} to other state-of-the-art approaches, whose results are reported in~\cite{zhang2017name}. These results are obtained on a collection from CiteSeerX~\footnote{\url{http://clgiles.ist.psu.edu/data/}} that contains records of authors with the name / atomic name variate `\emph{Y Chen}'. This collection consists of $848$ complete documents authored by $71$ distinct authors. We picked this name for comparison because of two reasons; 1) the number of authors sharing this name is among the top five as shown in Table~\ref{tab:stat} and 2) All methods cited in~\cite{zhang2017name} could not achieve a good result. We applied \emph{WhoIs} on this collection by randomly splitting the records into $70\%$ for training, $15\%$ for validation and $15\%$ for testing. The results are shown in Table~\ref{tab:result1}. Note that in our collection, we consider way more records and distinct authors (see Table~\ref{tab:stat}) and we use only reference attributes (i.e. co-authors, title and source). 

As the results presented in Table~\ref{tab:result1} show, \emph{WhoIs} outperforms other methods in resolving the disambiguation of the author name `\emph{Y Chen}' on the CiteSeerX dataset, which is a relatively small dataset and does not really reflect the performance of all presented approaches in real scenarios. The disparity between the results shown in Table~\ref{tab:result0} and Table~\ref{tab:result1} demonstrates that the existing benchmark datasets are manually prepared for the sake of accuracy. However, this leads to covering a very small portion of records whose authors share similar names. This disparity confirms that author name disambiguation is still an open problem in digital libraries and far from being solved. 


\begin{table}[]
    \caption{Comparison between \emph{WhoIs} and other baseline methods on CiteSeerX dataset in terms of Macro F1 score as reported in~\cite{zhang2017name}. \textbf{ANV} denotes that only atomic name variates were used for all target authors and all their co-authors.}
    \label{tab:result1}
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
         &  Macro ALL/ANV & Micro ALL/ANV \\
         \hline
        \emph{WhoIs} & $\mathbf{0.713}$ / $\mathbf{0.702}$ & $0.873$ / $0.861$ \\
        \hline
        NDAG~\cite{zhang2017name} & $0.367$ & N/A \\
        \hline
        GF~\cite{kuang2012symmetric} & $0.439$ & N/A\\
        \hline
        DeepWalk~\cite{perozzi2014deepwalk} & $0.118$ & N/A \\

        \hline
        LINE~\cite{tang2015line} & $0.193$ & N/A \\
        \hline
        Node2Vec~\cite{grover2016node2vec} & $0.058$ & N/A \\
        \hline
        PTE~\cite{tang2015pte} & $0.199$ & N/A \\
        \hline
        GL4~\cite{hermansson2013entity} & $0.385$ & N/A \\
        
        \hline
        Rand~\cite{zhang2017name} & $0.069$ & N/A \\
        
        \hline
        AuthorList~\cite{zhang2017name} & $0.325$ & N/A \\
        
        \hline
        AuthorList-NNMF~\cite{zhang2017name} & $0.355$ & N/A \\
        

    
        \hline
    \end{tabular}
\end{table}





The obtained results of \emph{WhoIs} illustrate the importance of relying on the research area of target authors and their co-authors to disambiguate their names. However, they trigger the need to encourage all authors to use different author identifiers such as ORCID~\cite{baglioni2021we} in their publications as the automatic approaches are not able to provide a perfect result mainly due to the complexity of the problem. 



\subsection{Limitations and obstacles of \emph{WhoIs}:}
\label{subsec:limit}

\emph{WhoIs} demonstrated a satisfactory result and outperformed state-of-the-art approaches on a challenging dataset. However, the approach faces several obstacles that will be addressed in our future works. In the following, we list the limitations of the proposed approach:
% and real-world scenario, which makes it ready to be integrated into bibliographic systems. 
 





\begin{itemize}[leftmargin=*]
    \item New authors cannot be properly handled by our approach, where a confidence threshold is set to decide whether the input corresponds to a new author or an existing one. To our knowledge, none of the existing supervised approaches is capable to handle this situation. 
    
    \item Commonly, authors found new collaborations which lead to new co-authorship. Our approach cannot benefit from the occurrence of new co-combinations of co-authors as they were never seen during training. \\
    \textbf{Planned solution:} We will train an independent model to embed the author's discipline using his/her known publications.  With this, we assume that authors working in the same area of research will be put close to each other even if they did not publish a paper together, the model would be able to capture the potential co-authorship between a pair of authors in terms of their area of research.
    
    \item Authors continuously extend their research expertise by co-authoring new publications in relatively different disciplines. This means that the titles and journals are not discriminative anymore. Consequently, it is hard for our approach to disambiguate authors holding common names. \\
    \textbf{Planned solution:} we plan to determine the author’s areas of research by mining domain-specific keywords from the entire paper instead of its title assuming that the author uses similar keywords/writing styles even in different research areas with gradual changes which can be captured by the model.

    
    \item There are a lot of models that have to be trained to disambiguate all authors in the DBLP repository. 
    
    \item Commonly, the number of samples is very small compared to the number of classes (i.e. authors sharing the same atomic name variate) which leads to overfitting the model. \\
    \textbf{Planned solution:} we plan to follow a reverse strategy of disambiguation. Instead of employing the co-authors of the target author, we will employ their co-authors aiming to find the target author among them. We aim also to learn co-author representation by employing their co-authors to help resolve the disambiguation of the target author's name.
    
    \item As mentioned earlier and stated by the maintainers of the platform~\footnote{\url{https://dblp.org/faq/How+accurate+is+the+data+in+dblp.html}}, the accuracy of the DBLP repository is not guaranteed. 
    
\end{itemize}




