\section{Method - Bayesian Inference}
We now formulate the problem setting and describe our approach. We aim to generate improved outputs $x_1$ (e.g., high-quality summaries), according to human preferences, given language feedback $f$ on an initial model-generated output $x_0$, and a context $c$ (e.g., a source document). We tackle this problem by updating an LM $\pi_{\theta}$ based on evidence provided by language feedback.

%In this section we define our problem formally and introduce our algorithm in its most general form. Given a context $c$ we seek to generate an output $x_1$ that is helpful and harmless according to human preference judgments. We assume access to language feedback $f$ on an initial model-generated output $x_0$ given the context $c$. In the following section, we frame this problem as Bayesian inference, i.e. updating an LM $\pi_{\theta}$ based on evidence provided by language feedback. Then we introduce ILF, our algorithm for learning from language feedback. 

\paragraph{Language Feedback as Variational Inference}
Our goal is to produce a high-quality output $x_1$ for a context $c \sim p(c)$ (e.g., a summary of a document). We use an LM $\pi_{\theta}$ to generate an output $x_1$, by conditioning on the context $c$, i.e., $x_1 \sim p_{\theta}(x_1|c)$. We then introduce the predicate $\mathcal{I}$, a random variable such that $\mathcal{I}=1$ if the output is high quality according to human preferences.
We denote this data-generating process, shown in Fig.~\ref{fig:data_generating_process}, as:
\begin{align}
    p_\theta(c, x_1, \mathcal{I}) = p(c) \pi_\theta(x_1|c) p(\mathcal{I}|c,x_1).
\end{align}
%The goal of maximizing summary helpfulness across contexts $c$ can then be framed as maximising the expected probability of helpfulness:
%\begin{align}
%p_\theta(\mathcal{I}=1) &= \mathbb{E}_{c \sim p(c)} p(\mathcal{I}=1|c) \\
%&= \mathbb{E}_{c \sim p(c)} \mathbb{E}_{x_1\sim \pi_\theta(x_1|c)} p(\mathcal{I}=1|c,x_1)
%\end{align}
%If we treat $p(\mathcal{I}=1|c, x_1)$ as a reward, this objective is equivalent to the RL objective of the expected reward. 
%However, instead of using RL, 
We frame our goal as maximizing the marginal log probability of quality across contexts: $\mathbb{E}_{c \sim p(c)} \log p(\mathcal{I}=1|c).$ %
% of generating high-quality summaries as 
% expected log probability of summary quality, i.e., $\mathbb{E}_{c \sim p(c)}  \mathbb{E}_{x_1\sim \pi_\theta(x_1|c)} \log p(\mathcal{I}=1|c,x_1)$.  
For a particular context $c$, we approximate $\log p(\mathcal{I}=1|c)$ by introducing an importance sampling proposal distribution $q(x_1|c)$ and using the Evidence Lower Bound (ELBo):
%We will use the fact that $p(\mathcal{I}=1|c)$ has the following lower bound: 
\begin{align}
\log p(\mathcal{I}=1|c) &= \log \sum_{x_1} p_\theta(x_1, \mathcal{I}=1|c) \label{eq:two} \\
% &= \log \sum_{x_1} q(x_1|c) \frac{p_\theta(x_1, \mathcal{I}=1|c)}{q(x_1|c)} \label{eq:three}\\
&\geq \sum_{x_1} q(x_1|c) \log \frac{p_\theta(x_1, \mathcal{I}=1|c)}{q(x_1|c)} \label{eq:four}
% &= \sum_{x_1} q(x_1|c)  \Big[ \log p_\theta(x_1, \mathcal{I}=1|c) - \log q(x_1|c) \Big] \label{eq:five} \\
%&:= F(\theta, q) \label{eq:six}
\end{align}
%In Eq.~\ref{eq:two}, we use the sum rule, in Eq.~\ref{eq:three}, we introduce an importance sampling proposal distribution $q(x_1|c)$, in Eq.~\ref{eq:four} we use the Jensen inequality and in Eq.~\ref{eq:six} we define $F$, the lower bound to maximize. 

We maximize the lower bound in Eq. \ref{eq:two}, henceforth called $F(\theta, q)$, using an Expectation-Maximization (EM) procedure: alternating between maximizing $F$ w.r.t. the proposal distribution $q$ (E-step) and w.r.t. $\pi_{\theta}$ (M-step)  We call this algorithm Imitation learning from Language Feedback.

 %
% There exists a closed form for such an optimal $q$ (see Appendix~\ref{appendix:optimal_q} for the full derivation):
% \begin{align*}
% \operatorname*{argmax}_q F(\theta, q)
% &= p_\theta(x_1|\mathcal{I}=1,c) \vcentcolon= q^*(x_1|c).
% \end{align*}
% \begin{figure}[t]
%     \centering
%     \begin{subfigure}
%         \input{images/dgp_q}
%         \caption{A graphical model of the data generating process underlying the proposal distribution $q^*$}
%         \label{fig:graphical_model}
%     \end{subfigure}
    % \begin{subfigure}
    %     \input{images/dgp_p}
    %     \caption{A graphical model of the data generating process underlying the proposal distribution $q^*$}
    %     \label{fig:graphical_model}
    % \end{subfigure}
% \end{figure}
% \paragraph{Approximating the Proposal Distribution}
% Even with a closed-form expression for $q^*$, we still do not have an easy way to sample from it. To sample from the posterior $p_{\theta} (x_1|\mathcal{I}=1,c)$, a distribution over high-quality texts, 
\paragraph{E-step}

Maximizing $F(\theta,q)$ w.r.t $q$ corresponds to refining the proposal distribution $q$ to assign higher likelihood to high-quality texts.
%We perform this refinement 
This is achieved by embedding $x_1$ into a data-generating process involving humans, by introducing the initial output $x_0$, and human feedback $f$ (via sum rule):
% and cast $\mathcal{I}$ in terms of human feedback:
\begin{align}
q(x_1|c) &= \sum_{x_0, f} p_{\theta}(x_0, f, x_1|\mathcal{I}=1,c) \label{eq:13}\\
&\propto\sum_{x_0, f}p_{\theta}(x_0,f,x_1|c)p_{\theta}(\mathcal{I}=1|c,x_0, f, x_1) \label{eq:14} \\
&= \sum_{x_0, f} p_{\theta}(x_0|c)p(f|c,x_0)p_{\theta}(x_1|c,x_0,f) \nonumber\\ & \quad\quad\quad p_{\theta}(\mathcal{I}=1|c,x_0, f, x_1).  \label{eq:15}  
\end{align}
%In Eq.~\ref{eq:13}, we introduce latent variables $x_0$, the initial summary, and $f$, the human feedback, using the sum rule. In Eq~\ref{eq:14}, we use Bayes' Rule, and in Eq.~\ref{eq:15}, we factorize the joint distribution $q(x_0,f,x_1)$ using the product rule
Eq. \ref{eq:15} gives rise to the following sampling procedure (see also Fig.~\ref{fig:data_generating_process}, right): First, an LM is conditioned on the context $c$ and generates an initial output $x_0$. Second, a human provides language feedback $f$ on the $(c, x_0)$ pair. Third, the LM generates a refined text $x_1$ conditioned on $(c, x_0, f)$. Finally, a binary variable $\mathcal{I}$ indicates whether $x_1$ is a high-quality text, given an initial output $x_0$, feedback $f$, and a context $c$. We model $p_{\theta}(\mathcal{I}=1|c, x_0, f, x_1)$ as a Boltzmann distribution:
\begin{align}
    p_{\theta}(\mathcal{I}=1|c, x_0, f, x_1) \propto \exp(R(c,x_0,f,x_1)/\beta),\label{eq:boltzmann}
\end{align}
which uses a reward function $R$ defined in terms of four variables: $c, x_0,f,x_1$; $\beta$ is a temperature hyperparameter. This Boltzmann distribution makes quality easy to evaluate since it expresses it as a reward function $R$ of a previous output and human language feedback. 

We now argue why the E-step results in a proposal distribution that is better than the original distribution $p_\theta(x_1|c)$, i.e., why samples from $q(x_1|c)$ tend to be of higher quality than samples from $p_\theta(x_1|c)$. First, we know that $x_0$ is already a reasonably good output (since $\pi_{\theta_\text{old}} \approx \pi_\theta$). We can assume that the feedback $f$ is informative and high-quality. Therefore $x_1 \sim p_\theta(x_1|c,x_0,f)$ is going to be of higher quality than $x_0 \sim p_\theta(x_0|c)$ because it leverages useful information from the feedback.  Furthermore, let us choose $R$ to assign higher values to refined texts $x_1$ that improve upon $x_0$ w.r.t to $f$ and $c$. Consequently, Eq.~\ref{eq:boltzmann} assigns a higher likelihood to high-quality outputs $x_1$, allowing us to put additional weight on high-quality outputs and improving the proposal distribution $q$ further.


% will result in proposal $q(x_1|c)$ being improved over $\pi_\theta(x_1|c)$. 


%\begin{enumerate}
   % \item A LM is conditioned on the document $c$ and generates initial summary $x_0$,
  %  \item A human provides language feedback $f$ on the $(c, x_0)$ pair,
   % \item A LM generates a refined summary $x_1$ conditioned on $(c, x_0, f)$,
   % \item Finally, a binary variable $\mathcal{I}$ indicates whether $x_1$ improves upon $x_0$ with respect to feedback $f$ and source document $c$.
%\end{enumerate}

% This data-generating process gives rise to the following joint distribution:
% \begin{align}
%     q(x_0, f, x_1, \mathcal{I}|c) = &q(x_0|c) q(f|c, x_0)  \\ &q(x_1|f, x_0, c) q(\mathcal{I}|c, x_0, f, x_1) \nonumber
% \end{align}

\paragraph{M-step}

Maximizing $F(\theta,q)$ w.r.t. the policy $\pi_{\theta}$ is equivalent to supervised learning (minimizing cross-entropy loss) on a distribution defined by $q$. To see that, we drop all the terms from Eq.~\ref{eq:four} that do not depend on $\theta$:
\begin{align}
\operatorname*{argmax}_\theta F(\theta, q) 
&= \operatorname*{argmax}_\theta \mathbb{E}_{x_1 \sim q(x_1|c)}  \log p_\theta(x_1,\mathcal{I}=1|c) \nonumber \\
 &= \operatorname*{argmin}_\theta \mathbb{E}_{x_1 \sim q(x_1|c)} -\log \pi_\theta(x_1|c ).\label{eq:supervised_finetuning} \hspace{-10px}
\end{align}
 %In Eq.~\ref{eq:eight} we decompose $\log p_{\theta}(x_1,I=1|c)$, drop $\log p(\mathcal{I}=1|c, x_1)$ as it does not depend on $\theta$ and switch from maximization to minimization.

\paragraph{ILF: Imitation Learning from Language Feedback}
\label{sec:ilf_reward}

In ILF, we alternate between the E-step and M-step, using the pseudocode in Algorithm~\ref{alg:expert_iteration}. In the M-step step, we use the model from the previous iteration $\pi_{\theta_\text{old}}$ as both $p_{\theta}(x_0|c)$ and $p_{\theta}(x_1|c,x_0,f)$. In practice, we implement $R$ by conditioning an instruction-finetuned LM on a binary question such as \textit{Does this new text incorporate the feedback provided? Answer Yes or No.} where the label $y$ is either $y_{\text{good}}$ (`` Yes") or $y_{\text{bad}}$ (`` No"). We use the probability of the positive answer $y_{\text{good}}$ given the prompt as a reward, i.e. $p_\text{norm}(y_{\text{good}}|\text{prompt}) = \frac{p(y_{\text{good}}|\text{prompt})}{p(y_{\text{good}}|\text{prompt}) + p(y_{\text{bad}}|\text{prompt})}$. With these assumptions, $q$ takes the form:
\begin{align*}
   q(x_1|c) \propto &\mathbb{E}_{x_0 \sim \pi_{\theta_\text{old}}(x_0|c)} \mathbb{E}_{f\sim p(f|c,x_0)} \\ &\pi_{\theta_\text{old}}(x_1|c,x_0,f) 
   \exp(R(c,x_0,f,x_1)/\beta).\nonumber
\end{align*}
We take advantage of this proposal distribution and perform the M-step, i.e., $\text{argmax}_{\theta} F(\theta, q)$ on optimized data. 
Finally, we approximate sampling from $q(x_1|c)$ by best-of-$N$ sampling. To obtain a sample $x_1 \sim q$, we %
% first sample $x_0 \sim \pi_{\theta_\text{old}}(x_0|c)$ and $f \sim p(f|c,x_0)$, then
sample $N$ refinements $\{x_1^1, \dots, x_1^N \} \sim \pi_{\theta_\text{old}}(x_1|c,x_0,f)$, and compute
\begin{align*}
x_1 = \text{argmax}_{x_1^i} \exp R(c,x_0, f, x_1^i).
\end{align*}

In summary, we show that ILF can be understood as Bayesian inference. This process involves updating an LM based on the evidence provided by language feedback. This lens highlights the correspondence between ILF and RL with Human Feedback ~\citep[][\textit{inter alia}]{ziegler2019fine, stiennon2020learning}, which was previously demonstrated to be equivalent to Bayesian inference~\citep{korbak2022rl}. 


% from a distribution without the exponential:
% \begin{align*}
% \hat{q}^*(x_1|c) = \mathbb{E}_{x_0 \sim \pi_{\theta_\text{old}}(x_0|c)} \mathbb{E}_{f\sim p(f|c,x_0)} \pi_{\theta_\text{old}}(x_1|c,x_0,f)
% \end{align*}
% using $\exp R(c,x_0,f,x_1)/\beta$ as scores for reranking. 

% In other words, 



% The complete pseudocode is shown in Algorithm~\ref{alg:expert_iteration}. 
%\footnote{To see why approximating sampling from a Boltzmann distribution with best-of-$N$ is justified, note that best-of-all (argmax) corresponds to sampling from $q^*(x_1)$ with temperature to $\beta\to 0$ while best-of-1 (unbiased sampling) corresponds to sampling from  $q^*(x_1)$  with temperature $\beta \to \infty$.} 
%In other words, to obtain a sample $x_1 \sim q^*$, we first sample $x_0 \sim \pi_{\theta_\text{old}}(x_0|c)$ and $f \sim p(f|c,x_0)$, then sample $N$ refinements $\{x_1^1, \dots, x_1^N \} \sim \pi_{\theta_\text{old}}(x_1|c,x_0,f)$, and compute
%\begin{align*}
%x_1 = \text{argmax}_{x_1^i} \exp \frac{1}%{\beta}R(c,x_0, f, x_1^i).
%\end{align*}







%Because we do an EM-style optimization, in the data optimization step we drop the dependence on $\pi_\theta$ and use $\pi_\theta$ from a previous iteration, $\pi_{\theta_\text{old}}$, for both $q(x_0)$ and $q(x_1|x_0,f)$. Additionally, we redefine $q(\mathcal{I}=1|c, x_0, f, x_1)$ as proportional to $\exp(R(c,x_0,f,x_1)/\beta$, a Boltzmann distribution involving a reward function $R$ defined in terms of four variables: $c, x_0,f,x_1$. This is much more tractable in practice because we can now express the notion of quality in terms of a previous summary and human feedback on it: a good summary $x_1$ will improve given feedback $f$ upon $x_0$. Note that that since $\pi_{\theta_\text{old}} \approx \pi_\theta$, we can assume $x_0$ is already pretty good. We also assume that feedback $f$ is informative. Therefore, we can safely assume that improving upon $x_0$ with respect to $f$ corresponds to genuine progress. 
%In practice, we implement $R$ by conditioning an instruction-finetuned LM on a question that elicits a binary answer, such as \textit{Does this new summary incorporate the feedback provided? Answer Yes or No}. The binary label $y$ is either $y_{\text{good}}$ (``Yes") or $y_{\text{bad}}$ (``No"). We then evaluate the normalized probability of $y_{\text{good}
%}$ given the prompt, i.e., $p_\text{norm}(y_{\text{good}}|\text{prompt}) = \frac{p(y_{\text{good}}|\text{prompt})}{p(y_{\text{good}}|\text{prompt}) + p(y_{\text{bad}}|\text{prompt})}$, and use this normalized probability as a reward. 


%(such as \textit{Yes} or \textit{No}) and use the log-likelihood of the corresponding tokens as a reward. In the case of summarization, the question could, for example, be \textit{Does the following refinement include the feedback on the initial summary?}. We call this reward model \textit{InstructRM}.

% In practice, we implement $R$ by



