\section{Conclusion}
In this work, we propose Imitation learning from Language Feedback (ILF), an iterative algorithm for training LMs to behave in line with human preferences, by learning from language feedback. We validate our approach on a carefully-controlled word-removal task, showing that only large LMs (175B parameters) accurately incorporate feedback. Using this insight, we then test our algorithm on the real-world task of text summarization. Combining ILF and learning from binary feedback brought a GPT-3 model to roughly human-level summarization ability. ILF on its own outperformed finetuning on human summaries, despite human summaries being of higher quality, suggesting that the model is better at approximating the distribution of refinements. %Our analysis further demonstrated that refinements generated with LMs typically incorporate feedback, especially when using an LM to select refinements.
% Initial results suggest that multiple iterations of ILF can incorporate human preferences even better.
%Language feedback is a natural form of communicating with models, making ILF a promising algorithm to align LM outputs with human preferences.
Our work opens up many avenues for future work, from improving algorithms for learning from language to tackling settings where it is hard to learn from sparse or binary feedback.
