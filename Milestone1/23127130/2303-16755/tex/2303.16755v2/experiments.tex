

% \section{Experiments}
\section{Can Language Models Use Feedback?}
\label{sec:targeted_word_removal}
For our algorithm to work, LMs must be able to accurately incorporate feedback to generate refinements.
Thus, we first validate the refinement step of our algorithm on a carefully-controlled synthetic task of removing specific offensive words from a given sentence.
We examine how effectively various models incorporate feedback to determine what model to use for refining outputs.

\paragraph{Experimental Setup}
We instruct an LM to refine an automatically-generated sentence with $\leq 10$ offensive words by removing $\leq 3$ specific words (see Appendix \ref{sec:word_removal_example} for a detailed explanation and examples). In this experiment, we generate one output per sample with greedy decoding, i.e., we do not sample with best-of-$N$ . We evaluate how often the generated refinement exactly matches the target sentence, which we automatically generate. For our LMs, we use differently-sized GPT-3 models~\citep{brown2020language} and text-davinci-001, their instruction-finetuned (Feedback Made Easy or FeedME) counterparts~\citep{ouyang2022training,openai_feedme}.\footnote{Via the \href{https://beta.openai.com/}{OpenAI API}.} We report all hyperparameters used in Appendix \ref{app:hp_tuning}.
We report the mean and standard error for all results in our work.


\paragraph{Results}
Table~\ref{tab:targeted_word_removal} shows the results. We observe that only the largest GPT-3 and FeedME models (175B parameters) incorporate feedback in a non-negligible amount of time. Using this insight, we only use the 175B parameter models in the rest of our experiments. Specifically, we use FeedME, because it is an instruction-finetuned model.

\input{synthetic_experiment_table}

\section{Summarization from Language Feedback}
Having established that large LMs can leverage language feedback, we now evaluate our algorithm on the real-world task of text summarization. In \S\ref{sec:data}, we introduce a novel summarization dataset that we use to evaluate our algorithm, in
\S\ref{sec:ranking_refinements}, we explore different methods for ranking refinements and
in \S\ref{sec:comparing_algorithms}, we use the best ranking method to learn from language feedback.

\input{data}


\begin{table}[t!]
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c} \toprule
 & Scoring Function &  \makecell{Win Rate in \% vs. \\ Random Selection} \\
\toprule
\makecell{Task Specific \\ Heuristic} & Max Length & $65.0 \pm  2.7$\\ 
\midrule
\midrule
\multirow{7}{*}{Zero-Shot} & 
Embedding Similarity   &  
$48.3 \pm 3.0$\\
& InstructRM Prompt 1 & $55.0 \pm 3.0$ \\
& InstructRM Prompt 2 & $58.0 \pm 2.9$ \\
& InstructRM Prompt 3 & $56.5 \pm 2.9$ \\
& InstructRM Prompt 4 & $55.8 \pm 2.8$ \\
& InstructRM Prompt 5 & $50.0 \pm 3.0$ \\
& \textbf{InstructRM Ensemble} & \textbf{56.0} $\pm$ \textbf{3.0}\\
\bottomrule
\end{tabular}}
\caption{We compare various ranking methods for selecting refinements using a human evaluation. InstructRM Ensemble performs best and is used throughout our paper.}
\label{tab:scoring_function_results} 
%\vspace{-12px}
\end{table}


\subsection{Comparing Refinement Ranking Methods}
\label{sec:ranking_refinements}
\paragraph{Generating Refinements}
We condition FeedME on the initial summaries of our train dataset (generated with FeedME) and the human-written feedback and generate 5 refinements $x_1^1, ...,x_1^5$ using the instructions in App.~\ref{app:summarization_prompts}.

%We use initial summaries (generated with FeedME) and the human-written feedback of our train dataset with 5000 samples We condition FeedME-175 on the initial summaries and the feedback and generate 5 refinements $x_1^1, ...,x_1^5$ using the instructions in App. \ref{app:summarization_prompts}.


\paragraph{Scoring Refinements with InstructRM} \label{sec:scoring_functions} We chose a refinement with a scoring function $R$ that scores refinements for how effectively they incorporate feedback. 
%For $R$ we use the instruction finetuned LM to evaluate its own output and call this method \textit{Instruct Reward Model (InstructRM)}. 
For $R$ we use the instruction-finetuned LM FeedME and ask it whether a refinement is better than the initial summary  (see \S\ref{sec:methods} for more details). We then evaluate the probability that the refinement incorporates language feedback on the initial summary and is accordingly a high-quality summary, i.e., $p(y_{\text{good}}|\text{prompt})$.
%e.g., $p(\text{improved}) = \frac{{p(\textit{" Yes"} | \text{prompt})}}{p(\textit{" Yes"}|\text{prompt}) + (p(\textit{" No"}|\text{prompt})}$.
LMs are sensitive to the exact prompt used~\citep{perez2021true,lu2021fantastically}, so we write 5 different prompts (see App.~\ref{app:instruct_rm_prompts})
and select the refinement with the highest average $p(y_{\text{good}}|\text{prompt})$ and call this method InstructRM Ensemble.


\paragraph{Scoring Refinements with Embedding Similarity}
Previous work \citep{scheurer2022training} use a contrastive pre-trained text-embedding function \citep{neelakantan2022text} to embed the feedback $f$ and refinements $x_1^1, ...,x_1^5$ and select the refinement with the highest cosine similarity to the feedback. They use this scoring function because feedback would often describe what the ideal text should look like. This method is less general because it assumes that good refinements are semantically similar to the feedback, which is not necessarily the case for all tasks or forms of feedback.

%Previous work \citep{scheurer2022training} use a contrastive pre-trained text-embedding function \citep{neelakantan2022text} to embed the feedback $f$ and refinements $x_1^1, ...,x_1^5$. They then choose the refinement with the highest cosine similarity with the feedback. They chose this scoring function because feedback would often describe what the ideal or improved text would look like. This method is less general because it assumes that good refinements are semantically similar to the feedback, which is not necessarily the case for all tasks or forms of feedback.


%In reality, however, this scoring function has clear limitations that become apparent when going beyond simple feedback that mentions missing information. It is unclear how an embedding similarity could handle feedback that talks about removing information or making stylistic comments etc. We conducted an analysis where we compared our InstructRM scoring function, with the embedding similarity scoring function. 

\paragraph{Results}
\label{sec:scoring_function_results}
We now evaluate the above ranking methods on the development dataset by calculating the fraction of times the refinement selected by a method is better than a randomly-selected refinement (``win rate''), according to a ranking given by human evaluators (see App.~\ref{app:ranking} for more details). The results, shown in Table~\ref{tab:scoring_function_results}, show that the embedding similarity selection does not outperform random selection, while most (4/5) InstructRM prompts do. While the embedding similarity worked well in previous work~\citep{scheurer2022training}, it does not perform well on our dataset. We believe this is because the feedback we collect, written by many annotators, is much more diverse, while in \citet{scheurer2022training}, the authors wrote the feedback themselves. InstructRM Ensemble has a win rate of $56.0 \pm 3.0 \%$ against random selection, demonstrating that an LM can evaluate its own output to some extent. Based on these results, we recommend using the InstructRM Ensemble approach, as it performs well and is less sensitive to the particular prompt.
%As a result, the method does not rely on a development dataset to select a specific prompt.% determine if a prompt is outperforming random selection.
Throughout our paper, we use InstructRM Ensemble as our scoring function to select refinements and refer to our method of generating and selecting refinements as \textit{Refinement with Feedback + Best of N}.
% We evaluate the ranking quality by comparing against a human-evaluated ranking of the selected refinement, with the rank of a randomly selected refinement.

\begin{figure}[t!]
   \begin{center} 
    \small{
    \scalebox{0.4}{\cstar{150}{199}{157}} ILF + OPT-RM (best-of-64)\quad \\
    \cblock{191}{134}{173} OPT-RM best-of-64 FeedME \quad \cblock{160}{125}{108} FeedME \quad\\
    \cblock{206}{119}{107} ILF: Finetuned on Refinements \quad
   \cblock{243}{192}{125} Finetuned on Initial Summaries \quad
   \cblock{125}{194}{209} Finetuned on Human Summaries 
   }
    \end{center}
    \includegraphics[scale=0.48]{images/finetuned_methods_comparison.pdf}
\caption{How often human evaluators prefer summaries from ILF, OPT-RM best-of-64 FeedME, ILF + OPT-RM (best-of-64), finetuning baselines and FeedME to human summaries. ILF + OPT-RM (best-of-64) generates summaries of a similar quality to human summaries.}
    \label{fig:finetuned_methods_comparison}
\end{figure}

\subsection{Comparing Feedback Learning Algorithms}
\label{sec:comparing_algorithms}
In this section, we compare various algorithms for learning from language feedback, binary feedback, and normal supervised finetuning. We present an overview of each method and then provide the results of our evaluations. 

\subsubsection{Methods}
\label{sec:finetuning_refinements}
\paragraph{Finetuning on Refinements (ILF)}
%Our Expert Iteration algorithm is designed to incorporate human preferences into the training of language models. This is achieved through an iterative process that leverages feedback in the form of language.
For this evaluation, we use a single iteration of ILF to learn from language feedback.
We finetune GPT3-175B (davinci)~\citep{brown2020language}\footnote{FeedME cannot be finetuned via OpenAI's API.} to 
maximize the log-likelihood of the refinement given the input prompt (consisting of the Reddit title, and post), i.e., $\log p(x_1 | \text{prompt})$, using the refinements generated with Refinement with Feedback + Best of N. %maximize the log-probability of the refinement given the initial summary $\log p(x_1 | x_0)$ using the \textsc{Refinement with Feedback + Best of N} dataset.
For all our finetuning methods we add $\lambda \log p(\text{prompt})$ to the loss \citep{radford2018improving,openai_documentation}, which maximizes the log-probability of the prompt. The prompt-loss weight $\lambda \in [0, 1]$ is chosen on our development dataset (see paragraph \textit{Finetuning on Human Summaries}). The selected hyperparameters are detailed in App.~\ref{app:hp_tuning} and the finetuning prompts in App.~\ref{app:finetuning_prompts}.

%The full prompt templates that we used for finetuning can be viewed in Appendix~\ref{app:finetuning_prompts}.
%We do extensive hyperparameter tuning, (see paragraph \textit{Finetuning on Human Summaries}) and  provide additional details in Appendix \ref{app:hp_tuning} for the selected hyperparameters. 
\paragraph{Finetuning on Human Summaries}
\label{sec:finetuning_human_summaries_hp}
Here we finetune GPT3-175B on the dataset of human-written summaries $x_{\text{human}}$, with the objective of maximizing the log-probability of human summaries given the input prompt (consisting of the Reddit title and post) with the additional loss term, i.e. $\log p(x_{\text{human}} | \text{prompt}) + \lambda \log p(\text{prompt})$.
To ensure the best performance of our finetuned models, we conduct 
 thorough hyperparameter tuning on the human-written summary datasets of various sizes (100, 1K, 5K). The hyperparameters optimized include the number of training epochs, the prompt loss weight $\lambda$, and the learning rate multiplier, as detailed in the OpenAI documentation \citep{openai_documentation}. We use the perplexity of the predicted summaries on the development dataset to select the most effective hyperparameters. The selected hyperparameters are applied to all datasets, i.e., finetuning on refinements, initial summaries, and human-written summaries, with the same sample size.  More details on hyperparameter tuning can be found in Appendix~\ref{app:hp_tuning}.


\paragraph{Finetuning on Initial Summaries}
We finetune GPT3-175B on the dataset of initial summaries (generated by FeedME). The objective is to maximize the log probability of the initial summary given the prompt (consisting of the Reddit title and post) with the additional loss term i.e. $\log p(x_0 | \text{prompt}) + \lambda \log p(\text{prompt})$. Details on hyperparameter tuning can be found in the paragraph \textit{Finetuning on Human Summaries} and Appendix~\ref{app:hp_tuning}.



 %To select the most effective model, we evaluate all finetuned models on the development dataset and use the perplexity of the predicted summaries as our selection criteria. Given that the human summary dataset, the initial summary dataset, and the refinement dataset only differ in the actual summaries, we apply the same selected hyperparameters to all datasets with the same sample size. We refer to Appendix~\ref{app:hp_tuning} for more details on hyperparameter tuning.


%We do extensive hyperparameter tuning on the human summaries, for each of the datasets splits with 100, 1K and 5K samples. We optimize the number of training epochs, the prompt loss weight $\lambda$, and the learning rate multiplier\footnote{See OpenAI's documentation \citep{openai_documentation} for more details on the hyperparameters}. We evaluate all finetuned models on the development dataset and use the perplexity of the predicted summaries as selection criteria. Since the human summary dataset, the initial summary dataset and the refinement dataset only differ in the actual summaries, we use the same selected hyperparameters for all datasets with the same amount of samples. 


% The prompt is however much longer than the completion and accordingly has a much higher influence on the loss. We thus introduce a hyperparameter called the prompt loss weight $\lambda \in [0...1]$, which controls the weight of the loss for the prompt tokens. 

\paragraph{Learning from Binary Feedback: Best-of-$N$}
\label{sec:binary_feedback}
We compare ILF against binary feedback as a baseline, the standard approach for learning from feedback. One way of learning from binary feedback is to train a reward model and use it to do best-of-$N$ sampling. We use best-of-N because it is often competitive with RL from human feedback~\citep{nakano2021webgpt}, a highly effective but more sophisticated approach~\cite{stiennon2020learning,ouyang2022training}.
To train the RM, we finetune OPT-13B (OPT-RM) \citep{zhang2022opt} to classify whether a summary $x_0$ is high quality or not.
To do so, we use the instruction \textit{Is the above an excellent summary of the given text? An excellent summary is coherent, accurate, concise, and detailed. Answer with Yes or No.}, where the label $y$ is either $y_{\text{good}}$ (`` Yes") or $y_{\text{bad}}$ (`` No"). Given human labels on which of two summaries is preferred, we label the preferred summary with $y_{\text{good}}$ and the other summary with $y_{\text{bad}}$.
We then finetune the LM to maximize $\log p(y | x_0) + \lambda \log p(x_0)$, where $\lambda \in [0,1]$, chosen using the development dataset, and $y \in \{y_{\text{good}}, y_{\text{bad}}\}$.
Using the finetuned LM, we evaluate a given summary by computing $p(y_{\text{good}}|x_0)$ and select the summary with the higher probability. We find that this approach leads to more accurate RMs than other RM training methods, such as the commonly used method from \citet{stiennon2020learning}; see Appendix~\ref{app:reward_model} for comparisons and Appendix~\ref{app:rm_prompts} for the used prompts. We perform Bayesian hyperparameter optimization for OPT-RM and sweep over the learning rate, batch size, and prompt-loss weight $\lambda$, using classification accuracy on the development dataset as the selection criteria (see Appendix~\ref{app:hp_tuning} for more details).

%use the above approach throughout our paper since we found that it leads to more accurate RMs than other RM training methods, such as the commonly used method from \citet{stiennon2020learning}; see Appendix~\ref{app:reward_model} for comparisons and Appendix~\ref{app:rm_prompts} for the prompt templates used. We perform a Bayesian hyperparameter optimization for OPT-RM and sweep over the learning rate, the batch size, and the prompt-loss weight. We use classification accuracy on the development dataset as a selection criterion (see Appendix~\ref{app:hp_tuning} for more details).

%$p(\text{summary is high-quality}) = \frac{p(y_{\text{good}}|x_0)}{p(y_{\text{good}}|x_0) + p(y_{\text{bad}}|x_0)}$
%We then pick the one with the higher probability for \textit{is high-quality summary}. 

\paragraph{ILF + Learning from Binary Feedback}
\label{sec:ILF_OPT}
As a final step, we combine ILF and learning from binary feedback, by first finetuning GPT3-175B on the refinements as described in the paragraph finetuning on refinements (ILF). We then train the reward model, OPT-RM, and use it to perform best-of-$N$ sampling, as outlined in the paragraph on learning from binary feedback. At test time, we generate 64 summaries with our finetuned model and rank them based on their 
probability of being a high-quality summary, $p_\text{norm}(y_{\text{good}}|x_0)$, using OPT-RM. The summary with the highest normalized probability is then selected.
%We begin by finetuning GPT-3 \textit{davinci} on refinements as described in the previous paragraph \textit{Finetuning on Refinements (ILF)}. We then follow the process outlined in the paragraph on learning from binary feedback to train the reward model, OPT-RM, and use it to perform best-of-$N$ sampling. At test time, we generate $N$ summaries using our finetuned model and use OPT-RM to rank them based on their normalized probability of being a high-quality summary, $p_\text{norm}(y_{\text{good}}|x_0)$. The summary with the highest normalized probability is then selected.

%Finally, we introduce the combination of ILF and Learning from Binary Feedback. First finetung \textit{davinci} on refinements as described in Finetuning on Refinements (ILF). Then train OPT-RM as described in Learning from Binary FeedbacK: Best-of-$N$. At test time we sample $N$ summaries from our finetuned model and use OPT-RM to rank them. We then select the summary with the highest normalized probability $p_\text{norm}(y_{\text{good}}|x_0)$.


\subsubsection{Evaluation}
\label{sec:evaluation_results}
We evaluate the effectiveness of our learning algorithm, by comparing it to human written reference summaries, several finetuning baselines, and OPT-RM on the task of text summarization using 100, 1K, and 5K train samples. Using a test dataset of 698 samples, we generate a summary for each method and evaluate them with human evaluators who rank them based on quality, using a standard ranking scheme that allows for ties between summaries (see App.~\ref{app:hp_tuning} for more details). Based on the rankings, we calculate the fraction of times each method's sampled summary outperforms the human-written reference summary, referred to as the ``win rate''. We sample summaries up to 48 tokens in length (as in \citet{stiennon2020learning}) using nucleus sampling \citep{holtzman2019curious} with $p=0.95$ and temperature $t=1.0$ (see App.~\ref{app:hp_tuning} for further details on hyperparameters and postprocessing). We use best-of-64 sampling with summaries sampled from FeedME for learning from binary feedback.

%For learning from binary feedback we perform best-of-64 sampling using summaries sampled from FeedME-175. We sample summaries up to 48 tokens in length (as in \citet{stiennon2020learning}) using nucleus sampling \citep{holtzman2019curious} with $p=0.95$ and temperature $t=1.0$ (further details on hyperparameters and summary postprocessing can be found in Appendix~\ref{app:hp_tuning}). Human annotators rank the generated summaries of all methods based on their quality, using a standard ranking scheme that allows for ties between summaries of equal quality (see Appendix~\ref{app:ranking} for more details on the ranking scheme). Based on the rankings of all methods, we calculate the fraction of times each method's sampled summaries outperform the human-written reference summary, referred to as the ``win rate''.


%To continue the iterative process, we use $\pi_{\theta_1}$ to generate summaries for a new set of Reddit posts and gather human feedback on the generated summaries. In order to incorporate this feedback into the training process, we use an instruction-finetuned model \textit{text-davinci-001}, to generate additional refinements\footnote{Ideally one would use $\pi_{\theta_1}$ to generate refinements, however, in our case, $\pi_{\theta_1}$ is based on \textit{davinci} and is thus not an instruction-finetuned model.}. We then use our InstructRM algorithm to select the refinement that incorporates the most feedback and add it to our fine-tuning dataset $\mathcal{D}_k$. 

%After repeating this process on all new samples, we finetune $\pi_{\theta_1}$ on $\mathcal{D}k$. This results in an updated model, $\pi_{\theta_2}$, which can be used in the next iteration of the ELF algorithm. This process can be repeated until the desired level of performance is achieved. For more details on hyperparameter tuning, see Appendix \ref{app:hp_tuning}.

%Overall, ELF allows us to incorporate more information about human preferences into the training of language models, resulting in improved performance on tasks such as text summarization. It is a scalable approach that can be applied to large-scale language models, making it a promising tool for incorporating human feedback into natural language processing systems.


%In one iteration of our Expert Iteration algorithm, we first generate an improved data distribution leveraging feedback, . We then finetune GPT-3 \textit{davinci}~\citep{brown2020language}\footnote{\textit{text-davinci-001} cannot be finetuning via OpenAI's API.} on the refinements (we refer to Appendix \ref{app:hp_tuning} for details on how we tune hyperparameters), yielding a model which we denote as $\pi_{\theta_1}$. The above steps can iteratively be applied, resulting in our full Expert Iteration algorithm. We can now use $\pi_{\theta_1}$ to sample summaries for a new set of Reddit posts. Then, we gather human feedback on the generated summaries and use \textit{texti-davinci-001} to generate refinements. Ideally one would use $\pi_{\theta_1}$ to generate refinements, however, in our case, $\pi_{\theta_1}$ is based on \textit{davinci} and is thus not an instruction-finetuned model. Thus we use \textit{text-davinci-001}, an instruction-finetuned model, to generate refinements (as we do already in the first iteration of our algorithm). We use InstructRM to select the refinement incorporating most of the feedback and add it to our fine-tuning dataset $\mathcal{D}_k$. Eventually, after repeating this process on all new samples, we finetuned $\pi_{\theta'}$ on $\mathcal{D}_k$, which yields the updated model $\pi_{\theta_2}$. One can now use $\pi_{\theta_2}$ and  iteratively repeated the described steps.

%\begin{figure}[t]
%\begin{center}
    
% \small{
%    \cblock{206}{119}{107} Expert Iteration (continuous finetuning)\quad
%   \cblock{150}{199}{157} Expert Iteration (retrain on all samples, refinements produced with Expert %Iteration (continuous fine-tuning)
 %  }
 %  \end{center}
  %  \centering \includegraphics[width=.48\textwidth,keepaspectratio]{images/expert_iteration_threeway_comparison.pdf}
  %  \caption{ }
  %  \label{fig:iterative_refinement}
%\end{figure}



\subsubsection{Results}
\label{sec:main_results}
Our results, shown in Fig.~\ref{fig:finetuned_methods_comparison}, demonstrate that finetuning on refinements (ILF) outperforms all other finetuning methods\footnote{Finetuning on 100 refinements is tied with finetuning on 100 initial summaries.}), including sampling from FeedME, with a win rate against human summaries of $31.3 \pm 1.7 \%$ (for finetuning on 5K samples), while the other methods achieve win rates of $27.3 \pm 1.7 \%$ (finetuning on initial summaries), $28.9 \pm 1.7 \%$ (finetuning on human summaries), and $22.5 \pm 1.6 \%$ (FeedME). It is surprising that ILF outperforms finetuning on human summarise across all sample sizes, despite human-written summaries generally being of higher quality (see Fig.~\ref{fig:win_rates_and_incorporating_feedback}, top). Further evaluation (see App. Fig.~\ref{fig:finetuned_methods_loss_scaling}) shows that the model finetuned on 1K refinements (ILF) exhibits significantly lower loss when evaluated on the validation dataset of refinements compared to the model finetuned on human summaries when evaluated on the validation dataset of human summaries, suggesting that the model is more adept at approximating the distribution of refinements. Additionally, when evaluating GPT3-175B on the summaries of 1K samples from various train datasets, we observe significantly lower loss on the refinement dataset than on the dataset of human summaries (see Table.~\ref{tab:Kl_distance_of_finetuned_models}). Overall, these results demonstrate the effectiveness of our proposed ILF approach in accurately incorporating feedback and improving model performance, even outperforming finetuning on human summaries.

%when using 5K train samples, finetuning on refinements (ILF) outperforms all other finetuning methods, as well as sampling from FeedME-175, with a win rate against human summaries of $31.3 \pm 1.8 \%$. The other finetuning methods achieve win rates of $27.3 \pm 1.7 \%$ (finetuning on initial summaries), $28.9 \pm 1.7 \%$ (finetuning on human summaries), and sampling from FeedME-175 obtains a win rate of $22.5 \pm 1.6 \%$. These results are surprising, as they show that ILF outperforms finetuning on human summaries across all sample sizes, despite human-written summaries generally being of higher quality. This is demonstrated in Fig.~\ref{fig:win_rates_and_incorporating_feedback} (left), where human summaries achieve a win rate of $83.2 \pm 1.7 \%$ against initial summaries, while refinements with feedback + best-of-$N$ obtain a win rate of $69.1 \pm 1.9 \%$.
%Furthermore, when evaluating \textit{davinci} on the summaries of 1K samples from the various train datasets (i.e. initial summaries, refinements, and human summaries), we observe significantly lower loss on the refinement dataset (see Tablele~\ref{tab:Kl_distance_of_finetuned_models}, first column). Additionally, the model finetuned on refinements exhibits significantly lower loss when evaluated on the validation dataset of refinements compared to the model finetuned on human summaries when evaluated on the validation dataset of human summaries, suggesting that the model is more adept at approximating the distribution of refinements.


%Fig.~\ref{fig:finetuned_methods_comparison} shows our results. When using 5K train samples, finetuning on refinements (ILF) outperforms all other finetuning methods and sampling from FeedME-175, with a win rate against \textsc{Human summaries}, of  $31.3\% \pm 1.8$. Other finetuning methods only obtain win rates of $27.3\% \pm 1.7 $ (finetuning on \textsc{Initial Summaries}), $28.9\% \pm 1.7 $ (finetuning on human summaries), and sampling from FeedME-175 obtains a win rate of $22.5  \% \pm 1.6 $. The results surprisingly show that ILF outperforms finetuning on human summaries across all dataset sizes. This is unexpected, since human-written summaries are of much higher quality than refinements, as shown in Fig.~\ref{fig:win_rates_and_incorporating_feedback} (left) where human summaries achieve a win rate of $83.2 \pm 1.7 \%$ against initial summaries, and refinements with feedback + best-of-$N$ obtain a win rate of $69.1 \pm 1.9 \%$.
%Additionally, when evaluating \textit{davinci} on 1K samples of the various finetuning distributions, i.e. initial summaries, refinements and human summaries, we observe significantly lower loss on the refinement dataset (see Tablele~\ref{tab:Kl_distance_of_finetuned_models} first column). Similarly, we show that the model finetuned on refinements obtains significantly lower loss when evaluating it on the validation dataset of refinements than the model finetuned on human summaries achieves on the validation dataset of human summaries. This suggests that the model is able to more closely approximate the distribution of refinements than the distribution of human summaries.


\citep{scheurer2022training} found that ILF with 100 feedback samples outperformed FeedME, while here we find it underperforms FeedME with 100 feedback samples.
Prior work uses author-written feedback that often conveys what the refinement should include, while our work includes more varied, crowdsourced feedback.
As a result, we observe that embedding similarity does not properly rank refinements on our human feedback dataset (Table~\ref{tab:scoring_function_results}), and we believe the difference in feedback may be a significant source of differences in results in this section as well; see Appendix~\ref{app:comparison_scheurer_22} for more discussion. 

\begin{figure}[t!]
\begin{minipage}[t]{.45\textwidth}
    \centering
\includegraphics[scale=0.48]{images/validation_500_refinement_method_comparison.pdf}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\textwidth}
     \centering
\includegraphics[scale=0.48]{images/validation_500_refinement_methods_is_feedback_incorporated.pdf}
\end{minipage}
\caption{\textbf{Top}: Human evaluators prefer summaries from
all refinement methods to the initial summaries (FeedME). Refine with Feedback + best-of-5 is rated highest. \textbf{Bottom}: Refine with Feedback + best-of-5 generally does incorporate the most important feedback point.}
\label{fig:win_rates_and_incorporating_feedback}
\vspace{-10px}
\end{figure}


Our results demonstrate that using OPT-RM for best-of-64 sampling on FeedME summaries outperforms all finetuning methods and sampling approaches across all sample sizes. The improved performance of OPT-RM best-of-64 FeedME comes at the cost of added inference time for best-of-$N$ sampling. Combining ILF and learning from binary feedback (ILF + OPT-RM (best-of-64)) achieves human-level summarization performance with a win rate of $50.8 \pm 1.9 \%$ using 5K samples for training. This suggests that both methods independently learn valuable information about human preferences that can be cumulative when used together. It should be noted that the result for ILF + OPT-RM (best-of-64) is obtained through a separate human evaluation with different comparison summaries (see App. Fig.~\ref{fig:additional_results}), and was added to Fig.~\ref{fig:finetuned_methods_comparison} for reference. 
%The results of the evaluation shown in Fig.~\ref{fig:additional_results}, demonstrate that OPT-RM best-of-64 FeedME achieves a win rate of $45.1 \pm 1.9 \%$, while here (see Fig.~\ref{fig:finetuned_methods_comparison}) it achieves a comparable win rate of $43.7 \pm 1.9 \%$.
In App.~\ref{app:ilf_multiple_iteratins}, we present some initial, promising results for multiple iterations of ILF. These results suggest that the method is effective, but further experimentation is necessary to understand it better.
%initial experiments with multiple iterations of ILF (see ), we observe promising results, but further experimentation is necessary to better understand 


%We also demonstrate that the combination of ILF and learning from binary feedback (OPT-RM best-of-64 finetuned on refinements) achieves the best performance when using 5K samples for training, achieving human-level summarization performance with a win rate of $50.8 \pm 1.9 \%$. This result suggests that both methods, i.e., learning from language feedback and learning from binary feedback, independently learn certain human preferences that can be cumulative when used together. It is worth noting that, while OPT-RM best-of-64 FeedME outperforms ILF, this advantage comes at the cost of added inference time for best-of-$N$ sampling. To make a completely fair comparison, one would need to finetune \textit{davinci} on binary comparisons, which has not been done to our knowledge. It is also worth mentioning that the result for OPT-RM best-of-64 finetuned on refinements was obtained through a separate human evaluation with different comparison summaries (see Fig.~\ref{fig:additional_results}). In Fig.~\ref{fig:additional_results}, we show that OPT-RM best-of-64 FeedME achieves a win rate of $45.1 \pm 1.9 \%$, while it achieving a comparable win rate of $43.7 \pm 1.9 \%$ in Fig.~\ref{fig:finetuned_methods_comparison}.

%In Appendix~\ref{app:ilf_multiple_iteratins}, we present some initial, promising results for multiple iterations of ILF. While these results suggest that the method is effective, more thorough experimentation is necessary to fully confirm its efficacy and to gain a deeper understanding of how and why it works.

%Our results demonstrate that using OPT-RM for best-of-64 sampling on FeedME-175 summaries outperforms all other finetuning methods across all dataset sizes. We finally show that combining both ILF and learning from binary feedback (OPT-RM best-of-64 Finetuned on Refinements) performs best when using 5000 samples for training. We note that the result for OPT-RM best-of-64 Finetuned on Refinements was conducted in a different human evaluation with other comparison summaries (see Fig.~\ref{fig:additional_results}), and we added the result here\footnote{The win rates for OPT-RM best-of-64 are comparable for both evaluations, in Fig.~\ref{fig:additional_results} we show that OPT-RM best-of-64 FeedME achieves a win rate of $45.1 \pm 1.9 \%$, while it achieves a win rate of $43.7 \pm 1.9 \%$ here in Fig.~\ref{fig:finetuned_methods_comparison}}. This result suggests that both methods, i.e., learning from language feedback and learning from binary feedback, orthogonally learn certain human preferences that can add up. We further note that while OPT-RM best-of-64 FeedME outperforms ILF, this comes at an added inference cost of doing best-of-$N$ sampling. For a completely fair comparison, one would have to finetuned \textit{davinci} on binary comparisons, which as far as we know has not previously been done. 


%Our results demonstrate that using OPT Binary RM outperforms all other finetuning methods, including fine-tuning on refinements. This unexpected finding suggests that preference learning may be more sample efficient than learning from language feedback, even in low data regimes. This is particularly noteworthy considering that performing a binary comparison is significantly faster than providing language feedback or writing an ideal summary, as indicated in Table.~\ref{tab:annotation_times}.

% Prior work had the paper authors write the feedback manually, while we collect more diverse feedback from crowdworkers. This is supported by our results (see yadaya), which show that the Embedding Similarity, the socring function used in prior work, does not perform well on our dataset. which we believe is a cause for the differences in our results;

% This is surprising because we observe that refinements are of good quality but human summaries are even better. We hypothesize that the model is better able to approximate the reinfments than the human summaries! 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Additionally, we find that in the low data regime, finetuning on the initial summaries generated by \textit{text-davinci-001} outperforms finetuning on human summaries. This may be because the initial summaries are closer to the distribution of \textit{davinci} (the model we finetune) and therefore easier to learn from. However, in the large data regime, finetuning on human refinements outperforms finetuning on initial summaries. Interestingly, even in this case, finetuning on refinements outperforms finetuning on human summaries. We will further investigate this phenomenon in section \ref{}.




%Our results demonstrate that using OPT Binary RM outperforms all other finetuning methods, including fine-tuning on refinements. This unexpected finding suggests that preference learning may be more sample efficient than learning from language feedback, even in low data regimes. This is particularly noteworthy considering that performing a binary comparison is significantly faster than providing language feedback or writing an ideal summary, as indicated in Table.~\ref{tab:annotation_times}.


%We also explore combining finetuning on refinements with the use of the RM. Specifically, we sample 64 summaries from the model finetuned on 5K refinements and used the RM to select the best sample. This results in the highest overall performance, with summaries on par with human summaries and a win rate of $50.8 \pm 1.9 \% $. It is worth noting that this method was evaluated in a separate human evaluation (though on the same test dataset) and is included here for reference purposes. The full results can be found in Fig.~\ref{fig:additional_results}.

%The effectiveness of our learning algorithm, in incorporating human feedback into the training of language models, in combination with using an RM on top,  gives us confidence in its potential for future applications. As demonstrated in Table.~\ref{tab:annotation_times}, providing feedback (either binary feedback or language feedback) is faster than writing ground truth samples. This not only makes our algorithm a more practical choice for improving language model performance, but it also allows for the possibility of using feedback as a means of addressing tasks that may be too difficult to directly solve. The ability to learn from feedback, rather than relying solely on ground truth samples, expands the range of problems that can be addressed using our approach.

%We evaluate the performance of our learning algorithm finetuned on 100, 1K, and 5K samples against the OPT Binary RM and various baselines, to investigate the sample efficiency of the respective methods. Fig.~\ref{fig:finetuned_methods_comparison} reports the win rate of our learning algorithm  over \textsc{Human summaries}, which is $31.3 \pm 1.8 \%$. In contrast, all finetuning baselines underperform our learning algorithm, with win rates of $27.3 \pm 1.7 \%$ (finetuning on \textsc{Initial Summaries}), $28.9 \pm 1.7 \%$ (finetuning n human summaries), and $22.5 \pm 1.6  \%$ (\textit{text-davinci-001}). It is surprising that finetuning on refinements is better than finetuning on ideal, \textsc{Human Summaries}. Especially in the low data regime, finetuning on \textsc{Initial Summaries} outperforms finetuning on \textsc{Human Summaries}. We hypothesize that this could be because the initial summaries, wich are sampled with \textit{text-davinci-001}, are closer to model distribution of \textit{davinci}, than human written summaries. This could make initial summaries easier to learn on, especially in the low-data regime. In the large data regime finetunign on human refinements outperforms finetuning on initial summaries. It is still surprising though, that finetuning on refinements outperforms finetuning on human summaries. We analyze this in more depth in section \ref{}. This makes us hopeful that this method can be used in the future, especially given that providing feedback is faster than writing ground truth samples (see Table~\ref{tab:annotation_times}. Also given that we might not be able to actually solve really hard tasks, but provide feedback.



%Our results further show that using OPT RM Binary outperforms all other finetuning methods. This surprisingly indicates that preference learning is more sample efficient than learning from language feedback, even in the very low data regimes. This is especially remarkable considering that doing a binary comparison is much faster than writing feedback or an ideal summary (see Table~\ref{tab:annotation_times}).

%Lastly, we combine both finetuning on refinements with our RM. Concretely, we sample $64$ summaries from our model that is finetuned on 5K refinements and use the RM to pick the best sample. This performs best overall and generates summaries on par with human summaries with a win rate of $50.8 \pm 1.9 \%$ \footnote{Note that this method was evaluated in a different human evaluation (though on the same test dataset) and we add this datapoint only for reference. For the full results we refer to Fig~\ref{fig:additional_results}}. 


\subsection{Does Language Feedback Improve Refinements?}
The improvements from ILF suggest that the refinements used for finetuning are high-quality, so here we investigate whether language feedback is responsible for the high quality.
% Our algorithm relies on having high-quality refinements to finetune on, so here, we investigate which aspects of our algorithm are responsible for improving the refinement quality.
%We now aim to examine the importance of various aspects of our algorithm for generating high-quality refinements (before finetuning).
To do so, we have human evaluators rank Refinement with Feedback + Best of N summaries against summaries from several other methods, similar to \S\ref{sec:ranking_refinements}.
We use the human ranking to compute a win rate between each method and the initial summary.
We compare against Refinement with Feedback, which \textit{randomly} chooses a refinement $ \in {x_1^1, \dots, x_1^5}$.
This ablation helps to evaluate the importance of choosing a refinement with our scoring function $R$, i.e., InstructRM Ensemble.
We also evaluate Refinement without Feedback, which instructs the LM to refine the initial summary but without feedback.
This ablation helps to evaluate the importance of using language feedback.
Lastly, we evaluate Human Summaries and Initial Summaries i.e., the initial summary $x_0$ generated by FeedME. We evaluate all methods on the validation dataset.

\paragraph{Results.}
Fig.~\ref{fig:win_rates_and_incorporating_feedback} (top) shows the win rates of summaries from various methods against initial summaries.
Surprisingly, instructing a model to improve its output without feedback already leads to a significant improvement (win rate of $59.4 \pm 2.1 \%$ over the initial summaries).
Refinements with Feedback achieve an improved win rate of $63.9 \pm 2.0 \%$, showing that language feedback is useful for improving refinement quality.
Refinement with Feedback + Best of N achieves an even better win rate of $69.1 \pm 1.9 \%$, highlighting that Best-of-N with the InstructRM Ensemble further improves the refinements.
Overall, language feedback is important for high-quality refinements, especially when using Best-of-N sampling.

\subsection{Do Refinements Incorporate the Feedback?}
To determine whether refinements are of higher quality due to incorporating feedback rather than improving the summary in other ways, we conduct a study on the validation dataset in which crowd workers evaluate how often the most important point of the feedback is incorporated in the refinements produced by various methods. As shown in Fig.~\ref{fig:win_rates_and_incorporating_feedback}, bottom, our method Refinement with Feedback + Best of N incorporates the most important point in the feedback most frequently ($57.4 \pm 2.2 \%$ often). Refinement with Feedback incorporates feedback $49.6 \pm 2.2\%$ of the time, showing that Best-of-N sampling improves how often the feedback is incorporated. For reference, Refinement without Feedback fixes the most important point in the feedback $30.8 \pm 2.1\%$ of the time, despite the model not receiving the language feedback. Human Summaries address the most important point in the feedback $74.0 \pm 1.9 \%$ of the time when writing the summary from scratch despite not receiving the feedback explicitly. Our results suggest that refinements are high-quality in part because they incorporate the most important point in the feedback.

%Here, we examine whether refinements are of higher quality because they incorporate the feedback, rather than improving the summary in other ways.
%To do so, we have human evaluators evaluate how often the most important point in the feedback is incorporated in the refinement, for various refinement methods.
%Fig.~\ref{fig:win_rates_and_incorporating_feedback} (right) shows that our method \textsc{Refinement with Feedback + Best of N} incorporates the most important feedback point most of the time ($57.4 \pm 2.2 \%$ often). \textsc{Refinement with Feedback} incorporates feedback $49.6 \pm 2.2\%$ of the time, showing that Best-of-N sampling improves how often the feedback is incorporated.
%For reference, \textsc{Refinement without feedback} fixes the most important feedback point $30.8 \pm 2.1\%$ of the time, despite the model not receiving the language feedback.
%The \textsc{Human Summaries} have addressed the most important feedback point $74.0 \pm 1.9 \%$ of the time when writing the summary from scratch and despite not receiving the feedback explicitly.
% and human summaries act as counterfactuals, i.e., they never observed feedback. They measure, however, how often a direct refinement without feedback would, by chance, incorporate feedback, and how much an ideal, human-written summary would address feedback on a fictitious summary. The former incorporates feedback $30.8 \pm \%$ of the time, and the latter $74.0 \pm 1.9 \%$ of the time.


\subsection{Which Finetuning Dataset Changes Models Most?}
\label{sec:results_distribution}
Here, we aim to understand how the summaries used for finetuning influence how much the model changes after finetuning.
\citet{gao2022scaling} find that models optimized with binary human feedback are more likely to learn undesirable behaviors when their output distribution deviates more from the initial, pretrained LM.
It is unclear whether these findings apply to models trained with language feedback, but
we take a preliminary step in this direction for understanding language feedback-trained models. % understanding language feedback-trained models.
In particular, we measure the (reverse) KL divergence \citep[following][]{gao2022scaling} between an ILF-finetuned model and the pretrained LM before ILF-training, $D_{\text{KL}}(\text{finetuned}|\text{GPT3-175B})$, by unconditionally sampling from the finetuned model and evaluating the log-likelihood of the generated text with \text{GPT3-175B}.
We also report the forward KL divergence, $D_{\text{KL}}(\text{GPT3-175B} | \text{finetuned})$. 
For reference, we evaluate both of the above for models finetuned on the initial summaries and on human summaries.

\paragraph{Results.}
Finetuning on refinements (ILF) shows the largest KL divergence (in both directions), followed by finetuning on human summaries, and then followed by finetuning on initial summaries; see App. Table~\ref{tab:Kl_distance_of_finetuned_models} for the exact numbers.
We find it surprising that finetuning on refinements results in higher KL divergences than finetuning on human summaries; we expected the refinements to be closer to the model's initial output distribution, relative to human summaries, therefore causing the finetuned model to undergo less change. The larger KL divergence with ILF may be partly responsible for the larger gains in human evaluations observed in Fig.~\ref{fig:finetuned_methods_comparison}.


%First, we calculate the negative log-likelihood of the \textit{davinci} model on the train dataset with 1K samples of the initial summaries, the refinements, and the human summaries. We find that the human summaries have the highest negative log-likelihood, followed by refinements and then initial summaries. This suggests that the human summaries are the farthest in terms of KL distance from the distribution learned by \textit{davinci}. This could potentially explain why it is more challenging to learn the human summary distribution, especially in the low data regime.

%We further calculate the forward and backward KL distances between the various finetuned models and \textit{davinci}. The forward KL distance is calculated by sampling $64$ tokens from \textit{davinci} and evaluating the generated text with the finetuned models, while the backward KL distance is calculated by sampling $64$ tokens from a finetuned model and evaluating \textit{davinci} on this text. The results, shown in Tablele~\ref{tab:Kl_distance_of_finetuned_models}, reveal that, in both the forward and backward KL divergences, the model finetuned on refinements is the furthest from \textit{davinci}, followed by finetuning on initial summaries and then human summaries. This suggests that the model finetuned on refinements is the most optimized . Lastly, we provide a PoS analysis of the 1K datasets of those summaries in Appendix \ref{app:pos}.