\subsection{Summarization with Language Feedback Dataset}
\label{sec:data}
We evaluate the effectiveness of ILF on the task of text summarization using the TL;DR dataset~\citep{volske-etal-2017-tl}, which consists of Reddit titles, posts, and their corresponding summaries. \citet{stiennon2020learning} adapt this dataset and show that it is a more realistic task for evaluating summarization models compared to the commonly used CNN/DM dataset~\citep{hermann2015teaching}. To ensure the quality of our dataset, we follow the same preprocessing steps as outlined in~\citet{stiennon2020learning} and extract a train dataset with 5000 samples, a development dataset with 200 samples, a validation dataset with 500 samples, and a test dataset with 698 samples\footnote{The train and development datasets are taken from \citet{stiennon2020learning}'s train dataset, and the validation and test set are taken from their test dataset.}. We then hire experienced annotators through Surge AI\footnote{\url{https://surgehq.ai}} to create our language feedback dataset, which we open source along with our code\footnote{Data: \href{https://huggingface.co/datasets/JeremyAlain/SLF5K}{HuggingFace};
Code: \href{https://github.com/JeremyAlain/imitation_learning_from_language_feedback}{Github}}. %make available as supplementary material. 
For each sample, we first generate three summaries for each Reddit post using the instruction-finetuned model text-davinci-001 (FeedME)~\citep{ouyang2022training, openai_feedme}. Two of these summaries are used for a binary comparison, in which annotators indicate their preference. The third summary serves as the initial output for which we solicit language feedback. This feedback should address the single most important shortcoming of the summary and can be related to coverage (how well the summary covers the important information in the post), accuracy (the factual accuracy of the summary), coherence (the coherence of the summary on its own), or other. We do not impose any restrictions on how the feedback should be written. In addition to providing feedback, annotators are also asked to write an ideal summary that is maximally 48 tokens long. The same crowd worker annotates all three tasks for a given sample. Overall the dataset collection and human evaluations cost 40K\$. On selected samples of the binary comparison task, we achieve an author-annotator agreement of $81.0\%$ and annotator-annotator agreement of $70.0\%$. The human summaries we collect are of excellent quality, as demonstrated in a human evaluation, where we compare our human-written summaries to the ones automatically extracted from Reddit \citep{volske-etal-2017-tl} (also used as baselines in \citet{stiennon2020learning,scheurer2022training}). We find that our human-written summaries are preferred $72.0 \pm 3.2 \%$ of the time, making them a much stronger baseline.




%To collect human feedback for our evaluation, we first generate three summaries for each Reddit post using the model \textit{text-davinci-001} (\textit{FeedME})~\citep{brown2020language, ouyang2022training, openai_feedme}. Two of these summaries are used for a binary comparison, in which annotators indicate their preference. The third summary serves as the initial output for which we solicit language feedback from annotators. This feedback should address the single most important shortcoming of the summary and can be related to coverage (how well the summary covers the important information in the post), accuracy (the factual accuracy of the summary), coherence (the coherence of the summary on its own), or other. We do not impose any restrictions on how the feedback should be written. In addition to providing feedback, annotators are also asked to write an ideal summary that is maximally 48 tokens long, the same length that we restrict the model-generated summaries to. While \citet{stiennon2020learning} do provide comparisons in their dataset, we want to enforce that the same annotators label the comparison, the feedback, and the human summary. This further allows us to time each annotation task and measure its difficulty qualitatively.

%To ensure the accuracy and reliability of our human annotations, we hire experienced annotators through Surge AI\footnote{\url{https://surgehq.ai}}. In a selection process, we select candidates based on the author-annotator agreement on binary summary comparisons and by evaluating handwritten feedback and ideal summaries. Instructions provided to the annotators can be found in Appendix~\ref{app:annotator_instructions}. On selected samples of the training dataset, we achieve $81 \%$ author-annotator agreement on binary comparisons. The human summaries we collect are of excellent quality, as demonstrated in a human evaluation on the  development dataset. We compare our human-written summaries to those automatically extracted from Reddit \citep{volske-etal-2017-tl}, which are also used as a baseline in \citet{stiennon2020learning, scheurer2022training}. We find that ours are preferred $72 \pm 3.2 \%$ of the time, making the human summaries we compare to a much stronger baseline. Lastly, we measure the annotation time of the various tasks, i.e. (i.e., binary comparison, feedback writing, and ideal summary writing) on our development dataset.  Accounting for outliers, we find that it takes annotators $61.6\pm 5.3$ seconds for the binary comparison task, $182.5\pm 6.3$ seconds for the feedbacks annotation task, and $195.5\pm 6.1$ seconds for the summary-writing task (we report the means and standard error throughout our paper). This makes the annotation of binary feedback vastly faster than writing feedback while still making feedback preferable to writing an ideal summary.

%We now evaluate our algorithm on the real-world task of text summarization.
%We follow prior work on learning from human preferences~\citep{stiennon2020learning} and learn to summarize Reddit posts from the TL;DR dataset ~\citet{volske-etal-2017-tl}. As argued by~\citep{stiennon2020learning}, we select the TL;DR dataset over the frequently used CNN/DM dataset~\citep{hermann2015teaching}, because very strong performance can be attained on the latter with simple extractive baselines. \citet{stiennon2020learning} further shows with a human evaluation that extractive baselines perform poorly on the TL;DR dataset, which makes it overall a realistic task. \citet{stiennon2020learning} further filters the TL;DR dataset to ensure quality. We extract $3$ datasets from their train split, a \textit{train dataset} with $5000$ samples, a \textit{development dataset} with 200 samples, and a \textit{validation dataset} with 500 samples, and also extract a \textit{test dataset} with 700 samples from their test dataset. These extracted datasets contain the \textit{title}, \textit{post}, and \textit{category}, for each Reddit post. We then conduct a large human data annotation for our datasets. First, we generate three summaries with \textit{text-davinci-001} \citep{brown2020language} given the Reddit title and post. Two of these summaries are used for a binary comparison, where annotators indicate which one they prefer, which will be used to train our reward models. The third summary is used as an initial summary on which we ask humans to write feedback. The feedback should be short and simple and address the single most important shortcoming of the summary. We don't impose any others restrictions on how the feedback should be written. Feedback will generally address one of the following axes, \textit{coverage}, \textit{accuracy}, or \textit{coherence}. Coverage addresses the question of how well the summary covers the important information in the post. Accuracy asks whether the factual information in the summary accurately matches the post. Coherence addresses how coherent the summary is on its own. We ask annotators to label their feedback as belonging to one of those categories, and otherwise label it as \textit{other}. Lastly, we ask annotators to write an ideal summary that is maximally $48$ tokens long (which is the same length that we restrict the model-generated summaries to).  While \citet{stiennon2020learning} do provide comparisons in their dataset, we want to enforce that the same annotators, label the comparison, the feedback, and the human summary. This further allows us to time each annotation task, and get a qualitative measurement of its difficulty.



