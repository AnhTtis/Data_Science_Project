\section{Methods}
\label{sec:methods}
We now formulate the problem setting and describe our approach. We aim to generate improved outputs $x_1$ (e.g., high-quality summaries), according to human preferences, given language feedback $f$ on an initial model-generated output $x_0$, and a context $c$ (e.g., a source document). We tackle this problem by updating an LM $\pi_{\theta}$ based on evidence provided by language feedback. 

Our goal is to sample a diverse set of high-quality outputs $x_1$ given a context $c$ (e.g., a summary of a document), where $c$ is drawn from the context distribution $p(c)$. We do so by fitting an autoregressive LM $\pi_{\theta}$ to approximate the ground-truth distribution $p_c^*(x_1)$ which is proportional to the quality of $x_1$, measured by the reward function $R$. Fitting $\pi_{\theta}$ can be written down as minimizing the expected KL-divergence from the true distribution $p_c^*(x_1)$ to $\pi_{\theta}$ over the context distribution $p(c)$:
% We use an LM $\pi_{\theta}$ to generate an output $x_1$, by conditioning on the context $c$, i.e., $x_1 \sim p_{\theta}(x_1|c)$. We can thus formalize our objective as minimizing the KL-divergence between the target distribution $p_c^*(x1)$ under the expectation of a context distribution $p(c)$: 

\begin{align}
\label{eq:objective}
\min_{\theta} \mathbb{E}_{c\sim p(c)}&\mathrm{KL}(p^*_c, \pi_\theta), \\
\text{where } p_c^*(x_1) &\propto \exp(\beta R(x_1|c)). \nonumber 
\end{align}


Minimizing the objective in Eq.~\ref{eq:objective} equivalent to minimizing the cross-entropy loss (i.e., supervised learning):
\begin{align*}
\mathcal{L}(\theta) &=-\mathbb{E}_{c\sim p(c)}\mathcal{L}_\theta(c), \\
\text{where } \mathcal{L}_\theta(c) &= \sum_{x_1} p^*_c(x_1) \log \pi_\theta (x_1|c). \nonumber
\end{align*}

It is intractable to compute this loss exactly for a number of reasons, including the exponential size of the space of $x_1$ as well as the intractability of computing the normalization constant of $p_c^*(x_1)$. To avoid the first issue, we use Monte Carlo approximation sampling using a small set of samples drawn from $p_c^*$. Directly sampling from $p_c^*$ is however still intractable. We thus resort to using importance sampling with a proposal distribution $q_c(x_1)$ that is simpler to sample:

\begin{align}
\mathcal{L}_\theta(c) =\sum_{x_1} q_c(x_1) \frac{p^*_c(x_1)}{q_c(x_1)} \log \pi_\theta(x_1|c)  \label{eq:importance_sampling}
\end{align}

 To minimize the variance, we must design $q_c$ to be as close as possible to $p_c^*$. We achieve this goal by defining $q_c$ to incorporate human feedback that directly reflects the unknown reward function $R$, in the process of sampling. We do so by first drawing an initial output $x_0$ from a suboptimal LM $\pi_{\theta}$ given the context $c$. Second, we ask humans to rate $x_0$ and provide language feedback $f$ on the $(c,x_0),$ pair. Third, a refinement LM $\pi_{\psi}$ generates a refined output $x_1$ conditioned on $(c,x_0,f)$. The proposal distribution, corresponding to this sampling procedure, can be written down as:

\begin{align}q_c(x_1) = \sum_{f, x_0}\pi_\psi(x_1|x_0,f)p(f|x_0)\pi_\theta(x_0|c). \nonumber
\end{align}

% Since we use a data-generating process that involves humans, we argue that $q_c$ moves us closer to $p^*_c$ than $\pi_{\theta}$. We further assume that $x_0$ is already a good output and that conditioning on feedback further improves it. We further use best-of-N sampling to strengthen this argument. 

Let $x_1^i, \dots, x_1^N$ be $N$ summaries sampled from $q_c(x_1)$. Then, we can approximate the objective in Eq.~\ref{eq:importance_sampling} as: 

\begin{align}
\mathcal{L}_\theta(c)&\approx\sum_{i=1}^N \underbrace{\frac{p^*_c(x_1^i)}{q_c(x_1^i)}}_{=\omega^i} 
\log \pi_\theta(x_1^i|c),
\label{eq:importance_weight}
%\nonumber
% \\
% &=\sum_{i=1}^N \omega^i \log \pi_\theta(x_1^i|c), \\
% &\propto\sum_{i=1}^N \frac{\omega^i}{\sum_{j=1}^N \omega^j} \log \pi_\theta(x_1^i|c) \nonumber
\end{align}

where $\omega^i$ is the importance weight of the $i$-th sample from $q_c$. 
% and in Eq.~\ref{eq:importance_weight} we normalize $\omega^i$'s to sum to 1. 
The importance weight $\omega^i$ is not computable as it is because we do not have access to $q_c$ other than being able to draw samples from it.
We avoid this issue by assuming that $q_c(x_1^i)$ is constant, implying that our samples are all equally good due to the high quality of human feedback. 
We then replace $R(x_1^i|c)$ in the definition of $p_c^*$ by $R(x_1^i|x_0, f, c)$, as the quality is not dependent on the intermediate summary and feedback but can be more easily assessed with these quantities. This allows us to compute the unnormalized $p_c^*$, after which we use self-normalization to finally compute the above loss.
%Second, we replace $R(c,x_1^i)$ with a reward function $R’(c, x^i_0, f^i, x^i_1)$ defined in terms of the original output $x_0^i$ and feedback $f^i$ corresponding to $x^i$ since it is much easier to assess the quality of a summary if you have an original summary with feedback. 

We implement $R$ by conditioning an instruction-finetuned LM on a binary question such as \textit{Does this new text $\left[x_1\right]$ incorporate the feedback $\left[f\right]$ provided on the initial text  $\left[x_0\right]$? Answer Yes or No.}, where the label $y$ is either $y_{\text{good}}$ (`` Yes") or $y_{\text{bad}}$ (`` No"). We use the probability of the positive answer $y_{\text{good}}$ as $R$, i.e. $R(x_1|x_0,f,c) = \frac{p(y_{\text{good}}|\text{prompt})}{p(y_{\text{good}}|\text{prompt}) + p(y_{\text{bad}}|\text{prompt})}$.

Finally, we use an extremely low temperature when computing $p_c^*$, i.e., $\beta \to \infty$. Due to self-normalization, this is equivalent to using only the best summary $x_1^*$ per context $c$ sampled from $q_c$ for computing the loss, resulting in the following, final objective:

%Third, we apply best-of-$N$ sampling against $R'$, i.e. we discard all samples except $x^*_1 = \text{argmax}_{i \in (1,K)} R’(c, x_0^i, f^i, x^i_1)$, which corresponds to setting $\beta\to \infty$, or in other words setting $x_1^*$'s normalized importance weight to 1, and all other outputs normalized importance weights to 0. After these approximations, our objective takes the form 
\begin{align}
\mathcal{L}(\theta) \approx\mathbb{E}_{c\sim p(c)} \log \pi_\theta(x_1^*|c) \label{eq:final_objective}
\end{align}

Our objective of approximating the ground truth distribution $p_c^*(x_1)$, which is proportional to the reward $R$ has clear connections to maximizing reward in RL. However, in RL, the goal is to find the best policy that maximizes the reward, whereas our algorithm results in a distribution of high-quality outputs $x_1$ given a document $c$, which allows us to draw a diverse set of outputs achieving a high reward. The broad diversity of high-quality outputs endows downstream users and systems with more control over which aspects they prefer and want to avoid.
In App.~\ref{app:bayesian_inference_derivation}, we further provide an alternative derivation of ILF that follows variational inference and shows that ILF can also be understood as Bayesian Inference. This process involves updating an LM based on the evidence provided by language feedback. This different lense highlights the correspondence between ILF and RL with Human Feedback ~\citep[][\textit{inter alia}]{ziegler2019fine, stiennon2020learning}, which was previously demonstrated to be equivalent to Bayesian inference~\citep{korbak2022rl}.