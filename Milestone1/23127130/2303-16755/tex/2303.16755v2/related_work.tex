\section{Related Work}
Our work builds upon our previous report \citep{scheurer2022training}, which showed that large LMs can refine outputs with language feedback. There, we introduce the same three-step algorithm that ILF builds upon, with the key difference that here we use an LM, i.e., InstructRM Ensemble, to evaluate whether a refinement incorporates feedback, whereas in \citet{scheurer2022training} we use a contrastive pre-trained text-embedding function \citep{neelakantan2022text}. InstructRM Ensemble is more general than this Embedding Similarity since it does not assume semantic similarity of the refinements to the feedback. Another difference is that ILF is an iterative, refine-and-finetune algorithm, which can be understood as Bayesian Inference corresponding to RL with Human Feedback. In addition, here we conduct different and more extensive experiments than in \citet{scheurer2022training} and  use human annotators. In particular, we show that ILF outperforms finetuning on human summaries and that combining ILF with learning from binary feedback achieves roughly human-level summarization performance. For a more detailed comparison to \citet{scheurer2022training} we refer to App.~\ref{app:comparison_scheurer_22}.

Subsequent work to ours suggests several ways to improve upon our approach.
\citet{saunders2022self} show that LMs themselves write high-quality feedback on LM outputs.
\citet{bai2022constitutional} then train a dialog assistant using ILF to learn from LM-written language feedback, eliminating the cost and effort of collecting human feedback.
\citet{liu2022improving,schick2022peer} train LMs to refine outputs based on feedback (without finetuning on the refinements), an approach that improves results when incorporated into ILF, as shown in subsequent work to ours \citep{shi2022life}.
% \citet{shi2022life} build upon ILF by finetuning a model to generate refinements given an initial output and human feedback, by using gold refinements.
% This paper introduces an algorithm that allows LMs to learn from language feedback. Subsequent work from \citet{bai2022constitutional} builds on our , but they use the resulting model in an RL training scheme,  and \citet{saunders2022self}, who learn to generate refinements from critiques by finetuning on human-written (gold) refinements, but use model-generated critiques at test time.
% In contrast, we finetune directly on refinements without access to gold refinements or initial outputs.
% Using LM-written feedback may obtain the benefits of learning from language feedback without the cost and effort of collecting human feedback.

%where we show that large LMs can incorporate language feedback for the first time, and introduce an algorithm to learn from language feedback. Follow-up work~\citep {bai2022constitutional} is similar to our current paper since they refine model outputs with feedback and then finetune on the human-written (gold) refinements. In contrast to us, they then further use this model in an RL training scheme. Furthermore, the feedback used to generate refinements is model generated, whereas ours is written by humans. 
%Similarly, \citet{saunders2022self} learn to generate refinements from critiques by finetuning on human written refinements. We, on the other hand, do not have access to gold refinements but generate refinements with our algorithm. Furthermore, even at test time \citet{saunders2022self} refines an initial output with model-generated critiques\footnote{They also refine initial outputs without feedback.}. This is different from finetuning on refinements directly in order to generate high-quality outputs without access to an initial output. They also finetune a single model to generate critiques and refinements, which makes it harder to reason about how human preferences are incorporated into the model. In a similar vein, \citet{shi2022life, liu2022improving,schick2022peer} finetune a model to generate refinements given an initial output and human feedback, requiring access to gold refinements.

Other work aims to use language in other ways than we do.
Some work investigates using explanations for \textit{gold labeled outputs} to \textit{classification tasks}, while our work addresses the more general text generation setting which classification tasks can be formulated as~\cite{Radford2019LanguageMA,raffel2020exploring,brown2020language}.
Explanations describe why a labeled output is correct, while feedback describes how to improve a candidate's output.
Prior work explores ways of using explanations to train text classification models, with mixed results~\citep[][\textit{inter alia}]{camburu2018snli,stacey2021natural,pruthi2021evaluating,wiegreffe-etal-2021-measuring,hase2021can, lampinen2022can}.
A few prior works also learn from language feedback for the purpose of ranking candidate outputs rather than generating outputs~\citep[]{weston2016dialog, li2016dialogue, hancock2019learning,li2022using,xu2022learning}.
\citet{matiana2021cut} learn text embeddings of language feedback, where improvements could benefit the refinement-scoring step of our algorithm. Language has also been used for various purposes in RL settings as well, as discussed in App.~\ref{app:releated_works_rl}.
% Other work learns to rank~\citep[]{weston2016dialog, li2016dialogue, hancock2019learning}, or generate \citep{li2022using} textual feedback, which serves as supervision signal to improve overall performance on question answering tasks. Futhermore, \citep[]{weston2016dialog, li2016dialogue, hancock2019learning} use ranking algorithms to select answers, and while \citep{li2022using} does learn to generate feedback, their final model is also used to rank answers. We on the other hand do not predict feedback, but directly leverage it to improve open ended text genertation. Lastly, \citet{matiana2021cut} learn text embeddings of language feedback, where improvements could benefit the refinement-scoring step of our algorithm.

Several other works draw connections between Bayesian Inference and learning algorithms for LMs.
\citet{korbak2022rl} show that KL-regularised RL is equivalent to variational inference: approximating a Bayesian posterior which specifies how to update a prior LM to conform with evidence provided by a reward function. \citet{dohan2022language} further argues that the process of generating output through multiple rounds of interaction between prompted LMs and other agents (e.g. humans providing language feedback) can be seen as executing probabilistic programs.