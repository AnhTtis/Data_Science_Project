\section{Reward Model}
\label{app:reward_model}
Here we describe the various RMs that we evaluate in more detail. We evaluate the final RM that we use, which produces a language output (e.g., `` Yes" or `` No") and a standard reward model that produces a scalar output.

\paragraph{Standard RM.}
Akin to \cite{stiennon2020learning}, we remove the last embedding layer of a language model and train it to output a scalar value. This scalar value predicts which summary, $x \in {\{x_0^0, x_0^1\}}$, is better as judged by a human, given a context $c$. We use the OPT 13B LM, introduced in \citep{zhang2022opt}, as the base model for our RM and finetune it on the human preference comparisons that we collected. It is worth noting that it is not possible to add linear layers on top of GPT-3 models provided via the API, which is why we use the OPT model.

%If the summary preferred by the human is $x_0^i$, the standard RM loss can be defined as:
%$$\text{loss}_{\text{base}}(r_{\theta})= - \mathbb{E}_{(c, x_0^0, x_0^1, i) \sim D} [\log (\sigma (r_{\theta}(c, x_0^i) - r_{\theta}(c, x_0^{1-i})))] $$

\paragraph{Reward Model with Language Output.}
In addition to the classic RM~\citep{stiennon2020learning},
we train an RM to output language tokens instead of a scalar value. To do so, we finetune an LM to classify whether a summary $x_0$ is high quality or not, by training it to predict a label $y \in \{ y_{good}, y_{bad} \}$.
We then finetune the LM to maximize $\lambda \log p(x_0) + \log p(y | x_0)$, where $\lambda \in [0,1]$, chosen using the development dataset. The complete loss can also be written as: 
$$\mathcal{L}(p_{\theta}, x, y) = - \lambda \cdot \sum_{t=1}^{|x|} \log p_{\theta} (x_t|x_{<t}) - \sum_{t=1}^{|y|}\log p_{\theta} (y_t|x, y_{<t}).$$
where the subscript $t$ indicates the token index.
We evaluate the finetuned LM on a given summary $x_0$ by computing $p(y_{good}|x_0)$. The best RM overall uses the following instruction \textit{Is the
above an excellent summary of the given text? An excellent summary is coherent, accurate, concise, and detailed. Answer with Yes or No.}, which we refer to as the OPT-RM (when finetuning OPT-13B) and GPT-3 Binary (when finetuning GPT-3-175B). We also explore finetuning on another prompt, where we provide both summaries $A$ and $B$ to the LM and instruct it to indicate which summary is preferred, i.e. \textit{Question: Which summary is the better one? An excellent summary is coherent, accurate, concise, and detailed. Answer with A or B.} We then finetune the LM on the label of the preferred summary (according to binary human feedback), i.e. on $y \in \{ y_{A}, y_{B} \}$. We evaluate the finetuned LM on a given summary $x_0$ by computing $p(y_{A}|x_0)$. We refer to this RM as \textit{Comparison} RM.  We explore two RMs, namely, OPT-13B \citet{zhang2022opt}, and GPT-3-175B and refer to Appendix~\ref{app:hp_tuning} for the hyperparameters we use and to Appendix~\ref{app:rm_prompts} for the prompt templates).



%We further introduce a new method to train reward models. Instead of adding a linear layer on top of LMs and predicting a scalar value, we condition the model to output normal tokens and optimize the LM with the standard LM loss, i.e. cross-entropy.

%Concretely we propose two methods of eliciting a summary preference from the LM. The first uses a \textit{Comparison} prompt, where we provide both summaries A, i.e. $x_0^0$, and B, i.e. $x_0^1$, to the LM and instruct it to indicate which summary is preferred. We then read of the log-likelihood of the tokens \textit{A} and \textit{B} and calculate the normalized log-likelihood of \textit{A}, i.e. $\frac{\log(p(\textit{A}))}{\log(p(\textit{A})) + \log(p(\textit{B}))}$. 

%The other method uses a \textit{Classification} prompt. Given two summaries $x_0^0$ and $x_0^1$ we know which one is preferred by humans. We can use this information and create a binary classification task. The preferred label is assigned to the positive class, while the other summary is assigned to the negative class. One can then independently classify both summaries, given the context. At test time one can independently look at the probability of the summary belonging to the positive class and pick the one with a higher probability. Note that this approach makes the assumption that the worse out of two summaries is actually a \textit{bad} summary, which is not necessarily true. See Appendix \ref{app:rm_prompts}) for the full prompts that we use.

%Both methods, \textit{Comparison} and \textit{Classificatin} are defined by a prompt and completion, and we can use the standard LM loss of optimizing to predict the next token. The loss can be decomposed into two parts, the prompt and the completion. This can be problematic if the prompt is much longer than the completion, as the loss will be dominated by prompt. For the task of summarization this problem applies, since we mostly care about being able to summarize the text and not about being able to predict the text. One can thus introduce a hyperparameter, the prompt loss weight $\lambda$, which weights the loss of the prompt tokens. It allows one to control how much the model tries to learn to generate the prompt, as compared to the completion (which always has a weight of 1.0). Formally we can write, 

%$$\text{Loss}_{\text{LM}}(r_{\theta})= - \mathbb{E}_{(c, x_0^0, x_0^1, i)} [\lambda \cdot CE(\text{prompt}, CE(\text{completion})] $$

%where $CE$ is the cross-entropy loss. The advantage of using an LM loss is that we keep the semantic meaning of the tokens, that were learned during training.


\paragraph{Results.}
We evaluate all RMs on our validation dataset, and calculate the accuracy of predicting the preferred summary out of two, based on human preferences. Table~\ref{tab:reward_model_accuracy_development} shows the complete results, and here we report on some of the RMs trained on 5K samples. The OPT model with the standard RM loss achieves an accuracy of $71.8 \pm 2.0 \%$ on the validation dataset. The results further show that both of our methods for training OPT with the LM loss outperform the standard RM loss, with OPT comparison achieving an accuracy of $72.6 \pm 1.9 \%$, and OPT-RM an accuracy of $73.4 \pm 1.9 \%$. We obtain similar results with finetuning GPT-3-175B, achieving an accuracy of $71.2 \pm 2.0 \%$ with the GPT3 Comparison, and an accuracy of $74.2 \pm 2.0 \%$ with GPT-3 Binary, which outperforms the OPT-RM. %It is worth noting that we have more fine-grained control over various hyperparameters when training OPT compared to using OpenAI's API, making it difficult to directly compare the performance of both models and draw concrete conclusions.

Based on these results, we further evaluate the OPT Binary and GPT-3-175B Binary models on the development dataset that we use to evaluate the scoring functions in \S\ref{sec:scoring_functions}. We calculate the fraction of times the refinement selected by an RM is better than a randomly-selected refinement (``win rate"), according to a ranking given by human evaluators (see App.~\ref{app:ranking} for more details). The results can be found in Table~\ref{tab:scoring_function_results_with_opt}. OPT-RM achieves a win rate of $63.3 \pm 2.7 \%$, and the GPT-3-175B Binary model achieved a win rate of $61.8 \pm 2.9 \%$. In this evaluation, OPT-RM outperforms GPT-3 Binary. When considering the results from both the validation and development datasets, both OPT-RM and GPT-3-Binary seem to perform similarly. Given that we have more control over the training process of OPT, the possibility of releasing the model, and the cost involved in training using OpenAI's API, we select OPT-RM model as our reward model for comparison with ILF.
In Figure~\ref{fig:opt_reward_model_scaling}, we show the validation accuracy of OPT-RM trained on 100, 1K, and 5K samples on a log-log plot. The figure shows scaling when increasing the dataset size.



%We evaluate all RMs on our validation dataset (500 samples) and calculate the accuracy of predicting the preferred out of two summaries, given human preferences. We report results on some of the RMs trained on $5K$ samples (the complete results are shown in  Table~\ref{tab:reward_model_accuracy_development}). OPT with the standard RM loss achieves an accuracy of $71.8 \% \pm 2.0$ on the validation dataset. The results show that both our methods of training OPT with the LM loss outperform the standard RM loss. The Comparison method achieves $72.6 \% \pm 1.9$ accuracy, and surprisingly the Binary method outperforms all others with an accuracy of $73.4 \pm 1.9$. We achieve similar results with our GPT-3 RM, getting an accuracy of $71.2 \% \pm 2.0$ with the Comparison method, and an accuracy of $74 \% \pm 2.0$, which outperforms the OPT Binary RM \footnote{We want to note that the model size of \textit{davinci} is not publicly known. Furthermore, when training OPT we have much more fine-grained control over various hyperparameters, compared to OpenAI's API. It is thus difficult to directly compare the performance of both models, and have concrete takeaways}. These results show that the Binary method performs best across models. 
%We thus further evaluate OPT Binary and GPT-3 Binary on the development dataset we used to evaluate the scoring functions in \S\ref{sec:scoring_functions}). The task is to select the best out of $5$ refinements. We then calculate the win rate against selecting a refinement randomly. The results can be found in Table~\ref{tab:scoring_function_results}. OPT Binary achieves a win rate of $63.3 \% \pm 2.7$ and GPT-3 Binary a win rate of $61.8  \% \pm 2.9$. In this evaluation, OPT outperforms GPT-3. When looking at the results of the validation and development dataset, both OPT and GPT-3 seem to perform similarly. Given that we have much more control over the training process of OPT, the possibility of releasing the model, and the cost involved in training OpenAI's API, we use OPT Binary as our reward model that we will compare our algorithm to. In Appendix~\ref{app:reward_model} Fig~\ref{fig:opt_reward_model_scaling} we further show how the performance of OPT Binary scales with finetuning on $100$, $1K$, and $5K$ samples.

 We further evaluate results for finetuning OPT-RM on the dataset of \citet{stiennon2020learning}, and also evaluating their model with 1.3B parameters on our dataset. We observe that the binary preference distribution of the training dataset has a significant impact on the performance of the reward model. For example, OPT-RM trained on 5K samples of our own train dataset (i.e., our final reward model) achieves an accuracy of $61.9 \pm 0.2 \%$ on the test set from \citet{stiennon2020learning} (not shown in Table~\ref{tab:reward_model_accuracy_development}). When this same model is trained on 90K samples from the train dataset of \citet{stiennon2020learning}, it achieves an accuracy of $69.3 \pm 0.2 \%$ on their test set (also not shown in Table~\ref{tab:reward_model_accuracy_development}). In contrast, this same model trained on 90K samples from their train dataset achieves an accuracy of only $68.6 \pm 2.0 \%$ on our validation dataset, which is significantly lower than the accuracy of $73.4 \pm 1.9 \%$ achieved by the model trained on 5K samples of our own train dataset. Similar patterns can be observed when comparing the OPT Binary model with 1.3B parameters trained on 5K samples of our own train dataset to the released 1.3B reward model trained by \citet{stiennon2020learning} on approx. 64K samples of their own train dataset. The former model achieves an accuracy of $69.6 \pm 2.0 \%$ on our validation dataset, while the latter only achieves an accuracy of $63.8 \pm 2.1 \%$ (note, though, that the RMs are trained with different loss functions).
These results highlight two important considerations: (1) preference distributions can vary significantly and have a strong effect on what a reward model learns, and (2) the sample efficiency of a reward model depends heavily on the train and test distributions. If the test distribution differs from the train distribution, reward models may be very sample inefficient and fail to accurately learn the true distribution, even when given significantly more samples.

\begin{table*}[t!]
\centering
\begin{tabular}{c c c} \toprule
 & Scoring Function &  \makecell{Win Rate vs Random Selection (in \%)} \\
\toprule
Task Specific Heuristic & Max Length & $65.0 \pm  2.7$\\
\midrule
\midrule
\multirow{2}{*}{Zero-Shot} & 
Embedding Similarity   &  
$48.3 \pm 3.0$\\
& \textbf{InstructRM Ensemble} & \textbf{56.0} $\pm$ \textbf{3.0}\\
\midrule
\multirow{2}{*}{Finetuning on 5K samples} & \textbf{OPT Binary} &  \textbf{63.3} $\pm$ \textbf{2.7}\\
& GPT-3 Binary & $61.8 \pm 2.9$\\
\bottomrule
\end{tabular}%}
\caption{In a human evaluation, we compare reward models and ranking methods on the development dataset (in the same way as in Fig~\ref{tab:scoring_function_results}. Both RMs are trained on 5K samples and outperform the zero-shot methods.}
\label{tab:scoring_function_results_with_opt} 
\end{table*}


\begin{table*}[t!]\resizebox{\textwidth}{!}
{
\begin{tabular}{c c c c c c c} \toprule
& Models  & \makecell{\# Params} & \makecell{Train Data Size} & \makecell{Development Accuracy (in \%)} & \makecell{Validation Accuracy (in \%)} \\
\midrule
\multirow{7}{*}{LM Loss / Our dataset}
& OPT Comparison & 13B& 5K & $66.5 \pm 3.3$&$72.6 \pm 1.9$ &\\
& OPT RM  & 1.3B & 5K & $70.0 \pm 3.2 $&$69.6 $$\pm 2.0$ &\\
& OPT RM  & 13B & 100 & $54.5 \pm 3.5$&$53.4 \pm 2.2$&\\
& OPT RM  & 13B & 1K & $68.5 \pm 3.2$& $67.2 \pm 2.1$&\\
& \textbf{OPT RM} & \textbf{13B} &\textbf{5K} & \textbf{69.5} $\pm$ 
\textbf{3.2}& \textbf{73.4} $\pm$ \textbf{1.9} &\\
& GPT-3 Comparison  & - & 5K&68.0  & $71.2 \pm 2.0$ &\\
& \textbf{GPT-3 Binary}  & - & \textbf{5K} & - & \textbf{74.2} $\pm$ \textbf{2.0} &\\
\midrule
RM Loss / Our dataset & 
OPT  & 13B & 5K &$68.5 \pm 3.2$ &$71.8 \pm 2.0$ &\\
RM Loss / \citet{stiennon2020learning} train dataset  & \citet{stiennon2020learning} RM & 1.3B & 64K & $58.0 \pm 3.4$ & $63.8 \pm 2.1$ &\\
\midrule
LM Loss / \citet{stiennon2020learning} train dataset & OPT Binary & 13B & 90K & $69.0 \pm 3.2 $&$68.6 \pm 2.0$&\\

\hline
 \bottomrule
\end{tabular}}
\caption{In a human evaluation, we evaluate various RMs on the development dataset and validation dataset. We also report the results of training on the train dataset of \citet{stiennon2020learning} and evaluating on our development and validation datasets. We calculate the accuracy of predicting which of two summaries is preferred by a human.}
\label{tab:reward_model_accuracy_development}
\end{table*}
