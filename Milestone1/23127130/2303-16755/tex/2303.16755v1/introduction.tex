\begin{figure}[ht]
    \centering \includegraphics[width=\columnwidth]{images/expert_iteration_language_feedback_diagram.pdf}
    \caption{To learn from language feedback on a language model (LM) output, we have an LM generate multiple refinements of the original output based on the feedback. We use an LM to pick the best refinement and finetune the original LM to maximize the likelihood of the chosen refinement.}
    \label{fig:illustration}
\end{figure}

\section{Introduction}
Language Models (LMs) achieve strong performance across diverse NLP tasks, from summarization to question answering and dialog \citep[][\textit{inter alia}]{Radford2018ImprovingLU,Radford2019LanguageMA,brown2020language, rae2021scaling}. One of their key limitations, however, is that they generate text that violates human preferences, such as misinformation \cite{lin2021truthfulqa}, offensive language \cite{gehman2020realtoxicityprompts}, and factually incorrect summaries \cite{stiennon2020learning}. To alleviate such issues, existing methods train LMs to generate text that scores highly according to human preferences or a predictive model thereof~\cite{ziegler2019fine,stiennon2020learning, nakano2021webgpt,ouyang2022training}. These approaches learn from human feedback regarding which of two outputs is better. However, each comparison only conveys limited information about human preferences.


 
%\begin{figure}[t!]
 %  \vspace{-12px}
 %   \begin{subfigure}{0.2\textwidth}
    % \centering
 %          \scalebox{0.85}{\input{images/dgp_p}}
 %        \label{fig:dgp_p}
%\end{subfigure}\hspace{-0.01\textwidth}%
%\begin{subfigure}{0.2\textwidth}
% \centering
   %     \scalebox{0.85}{\input{images/dgp_q}}
   %              \vspace{-12px}
 %        \label{fig:dgp_q}  
%\end{subfigure}

 %\caption{
 % A graphical model describing the target distribution that our algorithm embeds in a data-generating process.
 %\textbf{Left:} The graphical model of the target distribution $p_{\theta}$ that our algorithm approximates. $c$ is a document, $x_1$ is a high-quality LM output, and $\mathcal{I}$ indicates whether an output is high-quality according to human preferences. \textbf{Right:} The graphical model for the proposal distribution $q^*$ we use for importance sampling. $x_0$ is an initial LM output and $f$ is human language feedback on $x_0$.} %\textbf{Right}: Imitation Learning from Langauge Feedback Pseudocode}
%\label{fig:data_generating_process}
%\end{figure}
%\begin{algorithm}[t!]
%   \caption{Imitation Learning from Language Feedback}
%   \label{alg:expert_iteration}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} number of iterations $K$, a sequence of sets of source documents $\mathcal{C}=[\mathcal{C}_1, ..., \mathcal{C}_K]$, language model $\pi_{\theta}$, language reward model $R$
 %   \FOR{$k$ {\bfseries in} $1...K$}
 %       \STATE Initialize finetuning dataset $\mathcal{D}_k = \{ \}$
  %     \FOR{document $c$ {\bfseries in} $\mathcal{C}_k$}
   %        \STATE $x_0 \sim \pi_{\theta}(x_0|c)$
    %       \STATE Human provides feedback $f$ on $(c,x_0)$
     %      \STATE $\{x_1^1, \dots, x_1^N \} \sim \pi_{\theta}(x_1|c,x_0,f)$
      %     \STATE $x_1 = \text{argmax}_{x_1^n} 
 %          R(c,x_0, f, x_1^n)$
%            \STATE Add $(c,x_1)$ to $\mathcal{D}_k$
 %       \ENDFOR
  %      \STATE Update $\pi_{\theta}$ by supervised finetuning on $\mathcal{D}_k$ (as in Eq.~\ref{eq:supervised_finetuning})
   %\ENDFOR
%\end{algorithmic}
%\end{algorithm}
%Current methods alleviate such issues by training LMs to generate text that scores highly according to human preferences or a predictive model thereof~\cite{ziegler2019fine,stiennon2020learning, nakano2021webgpt,ouyang2022training}. In this line of work, human evaluators indicate their preferences by comparing text outputs. However, each comparison provides limited signal about such preferences.

We propose an alternative approach that learns from language feedback, an information-rich and natural form of human feedback.
We introduce Imitation learning from Language Feedback (ILF), a 3-step algorithm for learning from language feedback (Fig.~\ref{fig:illustration}). First, we generate multiple refinements of an LM-generated output, given the input, initial LM-generated output, and human-written feedback on the output. Second, we use an instruction-finetuned LM to choose the refinement that best incorporates the feedback. Third, we finetune the LM that generated the initial output on the chosen refinement given the input. In this way, we finetune an LM using language feedback; with the resulting model, we may then collect more feedback on its outputs and learn with the above refine-and-finetune approach. The algorithm's pseudocode (Algorithm~\ref{alg:expert_iteration}) and the corresponding graphical model are shown in Fig~\ref{fig:data_generating_process}.
ILF departs from prior work, which uses reinforcement learning (RL)~\citep[][\textit{inter alia}]{ziegler2019fine, stiennon2020learning} or auxiliary losses~\cite{stacey2021natural} and cannot be straightforwardly generalized to using free-form language feedback.

\begin{figure}
\centering
\begin{minipage}{0.48\textwidth}
   % \vspace{-12px}
    \begin{subfigure}{0.2\textwidth}
           \scalebox{0.9}{\input{images/dgp_p}}
         %\label{fig:dgp_p}
    \end{subfigure}
\hspace{0.12\textwidth}%
    \begin{subfigure}{0.2\textwidth}
        \scalebox{0.9}{\input{images/dgp_q}}
        % \label{fig:dgp_q}  
\end{subfigure}
\end{minipage}
\setcounter{figure}{1}    
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
 \caption{Imitation Learning from Language Feedback}
   \label{alg:expert_iteration}
\begin{algorithmic}
\STATE {\bfseries Input:} number of iterations $K$, a sequence of sets of source documents $\mathcal{C}=[\mathcal{C}_1, ..., \mathcal{C}_K]$, language model $\pi_{\theta}$, refinement language model $\pi_{\psi}$, reward model $R$
    \FOR{$k$ {\bfseries in} $1...K$}
        \STATE Initialize finetuning dataset $\mathcal{D}_k = \{ \}$
       \FOR{document $c$ {\bfseries in} $\mathcal{C}_k$}
           \STATE $x_0 \sim \pi_{\theta}(x_0|c)$
           \STATE Human provides feedback $f$ on $(c,x_0)$
           \STATE $\{x_1^1, \dots, x_1^N \} \sim \pi_{\psi}(x_1|c,x_0,f)$
           \STATE $x_1 = \text{argmax}_{x_1^n} 
          R(x_1^i|x_0, f, c)$
            \STATE Add $(c,x_1)$ to $\mathcal{D}_k$
        \ENDFOR
        \STATE Update $\pi_{\theta}$ by supervised finetuning on $\mathcal{D}_k$ (as in Eq.~\ref{eq:final_objective})
   \ENDFOR
\end{algorithmic}
\end{algorithm}
\end{minipage}
\caption{
 \textbf{Top Left:} The graphical model of the target distribution $p_{\theta}$ that our algorithm approximates. $c$ is a context and $x_1$ is a high-quality LM output. \textbf{Top Right:} Graphical model of the proposal distribution $q$ for importance sampling. $x_0$ is an initial LM output and $f$ is language feedback on $x_0$. \textbf{Bottom:} Pseudocode for our learning algorithm.} 
 \label{fig:data_generating_process}
 \vspace{-10px}
 \end{figure}

We analyze our approach both theoretically and empirically.
We show that ILF can be viewed as Bayesian Inference, similar to RL with Human Feedback with KL penalties~\citep{korbak2022rl}.
We then validate our algorithm on a carefully-controlled synthetic task of removing offensive words from a sentence with GPT-3-based models~\citep{brown2020language,ouyang2022training}.
We find that only the largest GPT-3-based models (175B parameters) accurately refine outputs.
Using this insight, we use the largest GPT-3 models to test our algorithm on text summarization, following~\citet{stiennon2020learning}.
% A model trained with our algorithm generates summaries that human evaluators prefer to human reference summaries $\sim$51\% of the time.
Our work extends our earlier unpublished results~\citep{scheurer2022training}, showing that ILF improves LM-generated summaries monotonically with the amount of feedback provided, testing up to 5k samples.
In all data regimes, ILF leads to comparable or better results to finetuning on \textit{human-written} summaries, suggesting our approach is a strong alternative to supervised learning on human demonstrations.
We also introduce an approach for learning from both language and comparison feedback by choosing the best-of-N samples from an ILF-trained model using a model trained with comparison feedback.
The hybrid approach outperforms learning from each form of feedback alone, leading to summaries that human evaluators prefer over high-quality human reference summaries $\sim 50.8$\% of the time.
% We find that combining ILF with a reward model trained on binary comparisons produces the best results and achieves roughly human-level summarization performance.
Our analysis shows that LM-generated refinements typically incorporate the feedback, especially when we use an LM to choose the refinement that best incorporates the feedback.
In our concurrent paper~\citep{chen2023feedback}, we show that ILF also achieves strong performance on code generation.
Our results suggest that language feedback is a promising avenue for learning human preferences.






% Our algorithm builds our earlier, unpublished work~\citep{scheurer2022training}, and we extend our algorithm to code generation in concurrent work~\citep{chen2023feedback}.

% We make the following key contributions:
% \begin{enumerate}
% \itemsep0em 
%     \item We introduce a new algorithm called Imitation learning from Language Feedback (ILF), and demonstrate that ILF can be viewed as a form of Bayesian Inference.
%     \item We evaluate its effectiveness on synthetic data and the SLF-5K dataset, a new feedback learning dataset.  
%     \item We show that large LMs can accurately incorporate feedback using ILF and that finetuning with ILF scales well with the size of the dataset, even outperforming finetuning on human summaries.
%     \item We find that combining ILF with a reward model trained on binary comparisons produces the best results and achieves roughly human-level summarization performance.
% \end{enumerate}



%We propose to use language feedback, which contains more information per evaluation and introduce a novel algorithm called imitation learning from language feedback (ILF), shown in Fig.~\ref{fig:illustration}.
 %First, we condition an LM on an input, model-generated output, and human-written feedback to sample many possible refinements of the output. Second, we choose the refinement incorporating most of the feedback by leveraging an instruction-finetuned LM and using it as a zero-shot language reward model, which we call \textit{InstructRM}. Third, we finetune an LM on the chosen refinements. We then iteratively apply these three steps to the finetuned LM. In a formal derivation, we show that ILF can be viewed as Bayesian inference. Our algorithm departs from prior work, which uses reinforcement learning methods~\citep[][\textit{inter alia}]{ziegler2019fine, stiennon2020learning} or auxiliary losses~\cite{stacey2021natural} and which cannot be straightforwardly generalized to using language feedback.




%We validate the ability of LMs to refine outputs given feedback on a carefully-controlled synthetic task of removing offensive words from a sentence with GPT-3-based models~\citep{brown2020language,ouyang2022training}.
%We find that only the largest GPT-3-based models accurately refine outputs.
%Using the above insight, we use the largest GPT-3 models to test our algorithm on a novel text summarization from language feedback dataset with $5K$ samples, called \textit{SLF-5K}, which is derived from~\citet{stiennon2020learning}. A model trained with one iteration ILF generates summaries that human evaluators prefer $31.3 \pm 1.8\%$ of the time, to human-written summaries. Our algorithm even outperforms summaries generated by a model finetuned on human written summaries at a rate of $53.8 \pm 1.9\% $. However, we find that a reward model trained on binary comparisons surprisingly outperforms one iteration of ILF, and is more sample efficient for all data sizes. We combine both our finetuned model and the reward model by sampling $64$ summaries and using the reward model to select the best summary. This method achieves the best performance overall, generating summaries that human evaluators prefer to human reference summaries $50.8 \pm 1.9\%$ of the time.
%Our analysis shows that LM-generated refinements typically incorporate the feedback, especially when choosing the refinement with \textit{InstructRM}. In preliminary results, we further show that multiple rounds of ILF can yield additional performance gains, making the full expert iteration from the language feedback algorithm a promising avenue for learning from human preferences.
