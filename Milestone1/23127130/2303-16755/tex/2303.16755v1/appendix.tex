\section{Additional derivations}
\label{app:additional_derivation}


\subsection{Imitation Learning from Language Feedback as Bayesian Inference}
\label{app:bayesian_inference_derivation}
\input{method_bayesian_inference}

%\subsection{Optimal proposal distribution}\label{appendix:optimal_q}

%\begin{align}
%\text{argmax}_q F(\theta, q)
%&= \text{argmin}_q \sum_{x_1|c} q(x_1|c) \log \frac{q(x_1|c)}{p_\theta(x_1,\mathcal{I}=1|c)} \label{eq:nine}\\
%&= \text{argmin}_q \sum_{x_1|c} q(x_1|c) \log \frac{q(x_1|c)}{p_\theta(x_1|c,\mathcal{I}=1)} \label{eq:ten}\\
%&= \text{argmin}_q D_\text{KL} \Big( q(x|c), p_\theta(x_1|c,\mathcal{I}=1) \Big) \label{eq:eleven}\\
%&= p_\theta(x_1|c, \mathcal{I}=1) .\label{eq:twelve}
%\end{align}

%In Eq.~\ref{eq:nine}, we add a minus in front of Eq.~\ref{eq:four}, which flips the fraction in the logarithm and switches from maximization to minimization. In Eq.~\ref{eq:ten}, we multiply by the constant $\log p(\mathcal{I}=1|c)$, which converts the joint distribution in the denominator to a conditional distribution. In Eq.~\ref{eq:eleven}, we use the definition of the Kullback-Leibler (KL) divergence, and in Eq.~\ref{eq:twelve} use the fact that $D_{KL}(a,b)$ is minimized at $a=b$. Therefore, we arrive at a closed-form solution for the optimal proposal distribution, which we call $q*$.

% \begin{figure}[h!]
%     \begin{subfigure}{0.5\textwidth}
%     \centering
%            \input{images/dgp_p}
%          \label{fig:dgp_p}
% \end{subfigure}%
% \begin{subfigure}{0.5\textwidth}
% \centering
%         \input{images/dgp_q}
%          \label{fig:dgp_q}  
% \end{subfigure}
%  \caption{\textbf{Top:} Target distribution $p_{\theta}$. \textbf{Bottom:} Proposal distribution $q^*$.}
% \label{fig:data_generating_process}
% \end{figure}

\section{Additional Related Work on Language in RL Settings}
\label{app:releated_works_rl}
Language has been widely used in RL for various purposes~\citep[see][for an overview]{luketina2019survey}, such as specifying tasks~\citep[``instruction following''][\textit{inter alia}], driving exploration~\citep{tam2022semantic}, inferring reward functions~\citep[][\textit{inter alia}]{lin2022inferring, sumers2021learning, fidler2017teaching}, and training a model via strong supervision~\cite{andreas2017modular,kaplan2017beating}, reward shaping~\cite{goyal2019using}, or by providing descriptions of trajectories \citep{nguyen2021interactive}. In contrast, we use language to correct faulty behavior. Other work uses language feedback at test time to correct mistakes in a model's behavior, e.g., image segmentation~\citep{rupprecht2018guide} or code generation~\cite{elgohary-etal-2020-speak,austin2021program}.
In contrast, we use feedback to \textit{train} models, and our approach does not require human intervention at test time.

\section{Dataset Collection and Analysis}
\label{app:dataset}


\paragraph{Annotation process}
To ensure the high quality of our human annotations, we employ experienced annotators sourced through the data-labeling company Surge AI. During an onboarding and evaluation process, we calculate author-annotator agreement on the binary comparison task and manually review the quality of the written feedback and ideal summaries to ensure their high quality. 
Then we select 31 qualified annotators for all annotation tasks, though they can choose which tasks to participate in and for how long. To further ensure the quality of our annotations, we provide detailed instructions, which we provide to the annotators, and update throughout the process to ensure continuous improvement (these instructions can be found in Appendix~\ref{app:annotator_instructions}). To measure the agreement rate between the annotators and the authors, we select a sample of 10 Reddit posts from the training dataset as a gold standard and have 17 annotators label them. When comparing the binary comparison annotations with our own ones, this results in an author-annotator agreement rate of $81.0 \%$. We also calculate the average agreement rate between all the possible annotator combinations, yielding an annotator-annotator agreement of $70\%$. By utilizing these thorough processes and evaluations, we can ensure the accuracy and reliability of our human annotations.

%We hire annotators through the data-labeling startup Surge AI\footnote{\url{https://surgehq.ai}}. We first onboard the annotators and evaluate their annotation quality, after which we pick 31 annotators (a few annotators were later removed, due to poor annotation quality). For all annotation tasks, only those 31 annotators were allowed to work on them, however, they were free to choose which tasks they wanted to participate in and for how long. We evaluate the annotation quality by measuring the agreement rate with the authors on the binary comparison task, and by manually reviewing the quality of the written feedback and human-written, ideal summaries. We use detailed procedures and instructions to ensure high agreement between the annotators and us, on the given task (See Appendix~\ref{app:annotator_instructions} for the instructions we provided to the annotators). We provide feedback to the annotators and update instructions throughout the process to ensure continuous improvement in the annotation quality. For our largest annotation, the training dataset, we selected $10$ samples as the gold standard, on which we indicated our binary preferences. We then had $17$ annotators label those samples, which results in an author-annotator agreement of 81$\%$.

\paragraph{Dataset Analysis}
\label{sec:dataset_analysis}
The feedback we collect typically addresses the most critical shortcomings of the summaries. In $92.0 \%$ of our train samples, the annotators' feedback was complete and addressed all important shortcomings of the summary, as reported by the annotators. Across our train dataset, we observe that the majority of the feedback pertains to coverage ($77.0 \%$), with smaller percentages relating to accuracy ($16.0 \%$), coherence ($5.0 \%$), and other categories ($2.0 \%$). We also analyze the length of the various summaries and feedback, measured in the average number of tokens. Our human-written summaries have an average length of $41.0 \pm 0.1$ tokens, the extracted human summaries from Reddit had an average length of $32.5 \pm 0.1$ tokens, the initial summaries generated by FeedME have an average length of $29.3 \pm 0.1$ tokens, and the feedback written by annotators on these initial summaries has an average length of $20.4 \pm 0.2$ tokens.

In addition to these analyses, we also measure the time it takes annotators to complete various tasks (i.e., binary comparison, feedback writing, and ideal summary writing) on our development dataset. We ignore outliers and consider only samples with annotation times of at least 20 seconds and at most 420 seconds (7 minutes). Annotators take $61.5 \pm 5.3$ seconds on average on the binary comparison task, $182.5\pm 6.3$ seconds on the feedback task, and $195.5\pm 6.1$ seconds on the ideal summary task.
 We plot the annotation times on the development dataset for the tasks of annotating binary comparisons, writing feedback, and writing ideal summaries as histograms in Fig.~\ref{fig:annotation_times}. The annotators are much faster at annotating binary comparisons than feedback or ideal summaries. Writing feedback takes less time than writing ideal summaries, which is expected, as critiquing a task is usually easier than solving it. These comprehensive evaluations demonstrate the high quality and thoroughness of our dataset and annotation processes.


 \begin{figure}[bh!]
\centering
%\begin{subfigure}{.3\textwidth}
    \includegraphics[valign=t, scale=0.4]{images/human_annotation_time_Binary_Comparison.pdf}
  %  \label{fig:annotation_time_comparison}
%\end{subfigure}
\hfill
%\begin{subfigure}{.3\textwidth}
    \includegraphics[valign=t,scale=0.4]{images/human_annotation_time_Feedback.pdf}
   % \label{fig:annotatino_time_feedback}
%\end{subfigure}
\hfill
%\begin{subfigure}{.3\textwidth}
    \includegraphics[valign=t, scale=0.4]{images/human_annotation_time_Human_Summary.pdf}
  %  \label{fig:annotation_time_ideal}
%\end{subfigure}
\caption{Histogram Plot of annotation times (in seconds) of the binary comparison task, the feedback annotation task and the human summary writing task. The evaluation is conducted on the development dataset. We observe that annotators are much quicker at the binary comparison task, which is expected. The results also show that writing feedback takes less time than writing an ideal summary.}
\label{fig:annotation_times}
\end{figure}

%The dataset we gather is of very high quality. In a human evaluation using our development dataset, we compare our human written summaries, with the human summaries automatically extracted from Reddit \citep{volske-etal-2017-tl}. The human summaries in our dataset are preferred to the ones from \citep{volske-etal-2017-tl} with a win-rate of $72\% \pm 3.2$\footnote{We indicate mean and standard error throughout our paper.}. Previous work \citep{stiennon2020learning,scheurer2022training} used the automatically extracted summaries from \citep{volske-etal-2017-tl} to benchmark their methods, while we compare our methods to the significantly stronger human summaries we collect. 

%The feedback we gather usually addresses the most important shortcomings of the summaries. On our train dataset, we asked annotators after writing their feedback, whether there is additional feedback about an important shortcoming of the summary that they would want to additionally mention. In $92\%$
%of our train samples, the feedback written by the annotators was complete, and there is no further feedback that would address additional important shortcomings. Across our train dataset, we observe that the majority of the feedback belongs to the category coverage, with $77\%$. Annotators classified their feedback as accuracy-related in $16\%$ of the times, coherence-related in $5\%$ of the times and other in $2\%$ of the times. We also evaluate the length of the various summaries and the feedback, measured in the average number of tokens. Our human written summaries have an average token length of $41.0 \pm 0.1$, the extracted human summaries from Reddit have an average token length of $32.5 \pm 0.1$, the initial summaries generated by \textit{text-davinci-001} on which we then write feedback have an average length of $29.3 \pm 0.1$, and the feedbacks the annotators write on this initial summary has an average token length of $20.4 \pm 0.1$. On our development dataset, we measure the time the annotators take to do different tasks, i.e. the time it takes to do the binary comparison, write feedback, and write an ideal summary. We ignore outliers and only look at samples that had annotation times of at least $20$ seconds and at most $420$ seconds (i.e. 7 minutes). We report the median annotation times with standard errors in Table~\ref{tab:annotation_times}. We further provide histogram plots of the annotation time for the different data types in Appendix \ref{app:annotation_times}. 




\section{Targeted Word Removal Details}
\label{sec:word_removal_example}
Below is an example of how we instruct or ``prompt'' an LM to remove specific, offensive words from a sentence.
\begin{quote}
    \textit{``In this text, many toxic and offensive words are used: You are such a jerk, and a nice person, and an idiot. The ideal text should remove the word jerk, but otherwise be unchanged: You are''}
\end{quote}
Here, the target completion is \textit{`` such a nice person and an idiot.''}
More formally, we sample offensive sentences by using $k$ offensive words from a fixed set of 25 offensive words drawn uniformly at random (without replacement). Each offensive sentence also includes the words "nice person" in addition to all the offensive words. For each $k \in \{1, \dots, 10\}$, we sample 50 offensive sentences. The task is then to remove $l \in [1, 2, 3]$ offensive words from a given sentence with $k \geq l$. Since we include the words "nice person" in the offensive sentence, we can remove $l=k$ offensive words and still have a target sentence that intuitively makes sense.

\section{Details about Ranking Procedure}
\label{app:ranking}
We use a standard ranking scheme where each of $K$ summaries is given a rank between 1 and $K$ (inclusive). Sometimes refinements are exact copies of the initial summaries or are very similar in terms of quality, which is why we allow for summaries to be tied. When calclating the win rate we assign $0.5$ wins for tied samples. We assign the rank $r'$ to all summaries ranked in a tie, where $r'=\frac{r + (r+n-1)}{2}$, $r$ is the rank of the tied elements, and $n$ is the number of ties at the rank. For example, we map a ranking of $(1,2,2,4,5) \rightarrow (1,2.5,2.5,4,5)$ and a ranking of $(1,2,3,3,3) \rightarrow (1,2,4,4,4)$.



\input{reward_model}

\section{Hyper Parameters}
\label{app:hp_tuning}

\subsection{Generating Refinements}
\label{app:postprocessing}
For the targeted word removal experiments (\S\ref{sec:targeted_word_removal}), we use greedy decoding until 200 tokens or \textit{/\ n} is generated. For all summarization experiments we sample up to 48 tokens~\citep[as in][]{stiennon2020learning} with nucleus sampling~\cite{holtzman2019curious} with $p=0.95$ and temperature $t=1.0$. We strip non-alphanumeric characters (e.g., newlines) from the beginning of sampled summaries. We further remove empty white spaces in the generated summaries and remove all text that comes after a new line token \textit{/\ n}. Due to the maximum token length, sampled summaries sometimes end with incomplete sentences. Thus, we remove ending sentences that do not end in ``.'', ``!'', or ``?''. The described temperature and post-processing are applied to all summary generations, i.e., for generating initial summaries, refinements, and test summaries. 

\subsection{Finetuning on Summaries}
We conduct independent hyperparameter optimization sweeps with three dataset sizes of human summaries of 100, 1K and 5K samples, and then use the same hyperparameters for finetuning on refinements (ILF) and finetuning on initial summaries. We choose to run the hyperparameter sweep on Human summaries since this will not give an unfair advantage to our algorithm that finetunes on refinements. For the sweep, we utilize the train dataset of human summaries (consisting of 100, 1K, and 5K samples) and evaluate on the development dataset. Unfortunately, the OpenAI API only provides validation loss and token accuracy for batches of the development dataset, making it impossible to evaluate the model on the full development dataset during training. As a result, we utilize the model API to evaluate on the full development dataset after finetuning and calculate the perplexity of the generated summaries as a performance measure.

To determine the optimal hyperparameters, we perform a sweep over a range of values for the following parameters: \textit{epochs} $\{1,2,3,4\}$, \textit{prompt loss weight} $\{0,0.01,0.05,0.1\}$, and \textit{learning rates} $\{0.02,0.05,0.1,0.2\}$. We first sweep over epochs and select the best value, then perform a sweep using that value for the prompt loss weight, and so on. Our empirical observations indicate that the number of epochs has the greatest impact on perplexity, with training for more than one epoch resulting in overfitting. The selected hyperparameters can be found in Table~\ref{tab:finetuning_hp}.

During the finetuning phase for the \textsc{Refinements} and \textsc{Initial Summaries} datasets with 1K samples each, we made an error in our hyperparameter selection. Instead of using a prompt loss weight of $0.05$, we mistakenly used a value of 0, when finetuning on human summaries. While this error may have slightly impacted our results, the difference in perplexity between the two settings is minimal, with a value of $6.68$ for a prompt loss weight of $0.05$ and $6.71$ for a prompt loss weight of 0. Despite this mistake, our method still outperforms finetuning on human summaries for 1K samples, as well as finetuning on initial summaries using suboptimal hyperparameters.


\begin{table}[t!]
\centering
{
\begin{tabular}{c c c c c} \toprule
Samples  & \makecell{Epochs} & \makecell{Prompt Loss Weight} & \makecell{Learning Rate} \\
\hline
100 & $1$ & $0$ &  $0.05$ \\
1K & $1$ & $0.05*$ &  $0.02$ \\
5K & $1$ & $0.1$ &  $0.2$ \\

 \bottomrule
\end{tabular}}
\caption{We report the chosen hyperparameters of finetuning on 100, 1K, and 5K samples of \textsc{Human Summaries}.\newline
*This hyperparameter is optimal but used only for finetuning on \textsc{Human Summaries}. For finetuning on \textsc{Refinements} and \textsc{Initial Summaries} we inadvertently use the prompt loss weight 0.}
\label{tab:finetuning_hp}
\end{table}


\subsection{Multiple Iterations of ILF}
To evaluate multiple iterations of ILF, i.e., multiple iterations of refining-and-finetuning, we finetune GPT-3-175B on a refinement dataset with 200 and 300 samples. Thus we conduct a hyperparameter optimization on a train dataset of 200 and 300 refinements and evaluate on a development dataset of 200 refinements (instead of human summaries). To determine the optimal hyperparameters, we perform a sweep over a range of values for the following parameters: \textit{epochs} $\{1,2,3,4\}$, \textit{prompt loss weight} $\{0,0.01,0.05,0.1\}$, and \textit{learning rates} $\{0.02,0.05,0.1,0.2\}$. We first sweep over epochs and select the best value, then perform a sweep using that value for the prompt loss weight, and so on. For finetuning on 200 refinements we select the following hyperparameters: $\text{epochs}=1$, $\text{prompt loss weight}=0.05$, $\text{learning rate multiplier}= 0.1$. For finetuning on 300 refinements we select $\text{epochs}=1$, $\text{prompt loss weight}=0$, and $\text{learning rate multiplier}=0.2$.




\subsection{Finetuning Reward Models}

\paragraph{OPT Reward Model.}
% jon can you please fill this out.
For finetuning the OPT Reward Model, we perform bayesian hyperparameter optimization for each of the three different types of reward models:  \textit{Standard}, \textit{Comparison} and \textit{Classification} (see section \ref{app:reward_model}). We sweep over the learning rate in the range of $[1\text{e}^{-5}, 1\text{e}^{-6}]$ and the batch size $\{32, 64\}$ for all the models. For the reward models using the language loss, we also optimize the prompt-loss weight $\{0.0, 0.01, 0.05, 0.1, 0.5, 1.0\}$. We run 10 iterations per model and evaluate all the sweeps with the 200 development examples. We use a linear learning rate scheduler and a weight decay of $0.1$ for all the runs. The optimal batch size is 32 for all the models. The best prompt loss weight is $0.01$ for both the \textit{Comparison} and \textit{Classification} RMs. As for the learning rate,  we use $9.3\text{e}^{-6}$ for the \textit{Standard} RM, $5.8\text{e}^{-6}$ for the \textit{Classification} RM and $1\text{e}^{-6}$ for the \textit{Comparison} RM. In the final finetuning, we select the best RM in the validation split over 10 epochs. 

\paragraph{GPT-3 Reward Model.}
In order to finetune GPT-3-175B as an RM, we utilize the OpenAI API. We finetune two types of RMs: the \textit{Comparison} RM, which learns to predict which of two summaries is superior, and the \textit{Classification} RM, which predicts whether a given summary is of high quality or not. For cost considerations, we conduct hyperparameter tuning on a training dataset of 1K samples (instead of 5K) and evaluate on a development dataset of 200 samples. We use a dataset with 1K samples for cost reasons. We then apply the same hyperparameters when finetuning on 5K samples while implementing early stopping in terms of epochs. Due to the binary nature of the human preference annotations in the classification reward model, the effective train dataset size for this model is doubled to 2K samples.

In order to determine the optimal hyperparameters, we perform a sweep over a range of values for the number of epochs $\{1,2,3,4\}$ and the prompt loss weights $\{0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5\}$. The OpenAI API provides classification accuracy (for both the comparison and classification tasks) for the full development dataset after each epoch, allowing us to select the appropriate number of epochs and prompt loss weight. When finetuning on 5K samples, we utilize early stopping to prevent overfitting, using 1 epoch and a prompt loss weight of 0 for the comparison model and 4 epochs and a prompt loss weight of $0.001$ for the classification model. We use default values for all other hyperparameters, which may vary depending on the dataset size.




\begin{figure}[t!]
\begin{center}
 \small{
   \cblock{191}{134}{173} OPT-RM, LM Loss/Binary\quad
   }
\end{center}
    \centering \includegraphics[width=.48\textwidth,keepaspectratio]{images/opt_13B_classification_rm_scaling_plot.pdf}
    \caption{Here we plot the validation accuracy of OPT-RM trained on 100, 1K, and 5K samples on a log-log plot. The figure shows scaling when increasing the dataset size.}
    \label{fig:opt_reward_model_scaling}
\end{figure}
\begin{figure}[t!]
\begin{center}
     \small{
   \cblock{125}{194}{209} Finetuned on Human Summaries\quad
   \cblock{206}{119}{107} Finetuned on Refinements\quad
   \cblock{243}{192}{125} Finetuned on Initial Summaries\quad
   }
   \end{center}

    \centering \includegraphics[scale=0.6]{images/finetuned_models_loss_scaling_validation_500.pdf}
    \caption{Evaluation of models finetuned on 5K initial summaries, refinements, and human summaries on 500 samples from the corresponding validation datasets. For example, the model finetuned on human summaries is evaluated on 500 human summaries from the validation dataset. The model finetuned on refinements has a significantly lower negative log-likelihood than the model finetuned on human summaries.}
    \label{fig:finetuned_methods_loss_scaling}
\end{figure}




\section{Additional Results}
\subsection{Analyis of Finetuned Models}
In Table~\ref{tab:Kl_distance_of_finetuned_models}, we evaluate GPT-3-175B on various finetuning datasets used for finetuning: the refinements, the initial summaries, and the human summaries. We evaluate the log-likelihood of GPT-3-175B on the summaries of 1K samples from the various train datasets (i.e. initial summaries, refinements, and human summaries). Concretely, we pass the whole prompt to GPT-3-175B, including the Reddit post, but only evaluate the log-likelihood of the completion, i.e. the generated summary. We also measure the (reverse) KL divergence \citep[following][]{gao2022scaling}  between an ILF-finetuned model and the pretrained LM before ILF-training, $D_{\text{KL}}(\text{finetuned}|\text{GPT-3-175B})$. We sample unconditionally (i.e. using a beginning of sentence token) from the finetuned models and evaluate the log-likelihood of the generated text with GPT-3-175B. We also report the forward KL divergence, $D_{\text{KL}}(\text{GPT-3-175B}| \text{finetuned})$. We discuss the results in \S \ref{sec:results_distribution}.



\begin{table*}[t!]\resizebox{\textwidth}{!}{
\begin{tabular}{c c c c} \toprule
Model &  \makecell{Neg. Log Likelihood of GPT-3-175B \\ on 1K train samples of respective distribution} &\makecell{$D_{KL}(\text{GPT-3-175B} | \text{finetuned})$ (in nats)} & \makecell{$D_{KL}(\text{finetuned}|\text{GPT-3-175B} )$ (in nats)}\\
\hline
Finetuned on Initial Summaries  &  $1.19 \pm 0.01 $&$0.43 \pm 0.11$ & $0.83 \pm 0.08$  \\
Finetuned on Refinements &$1.37 \pm 0.01 $&$0.60 \pm 0.10 $& $1.10 \pm 0.06$ \\
Finetuned on Human Summaries & $1.61 \pm 0.01$ &$0.12 \pm 0.09$ &$0.55 \pm 0.01$ \\
OPT-RM best-of-64 FeedME & - &- & $3.17$ \\
\hline
\bottomrule
\end{tabular}}
\caption{First we evaluate the log-likelihood of GPT-3-175B on the 1K samples of the various data distributions that we finetune on. Then we empirically calculate the KL-divergence by sampling 2000 texts of length 64 tokens from GPT-3-175B and evaluating the log-likelihood of the finetuned models on the samples (for the reverse KL we sample from the finetuned models and evaluate GPT-3-175B on the samples). We report the mean and standard error across 2 runs. For Best of 64 on a specific reward model, we use the analytical formula $KL(N, RM)= \log N - \frac{N-1}{N}$ (see also \cite{goodhart_gao}).}
\label{tab:Kl_distance_of_finetuned_models} 
\end{table*}

\subsection{Results: ILF + OPT-RM}
In this section, we present the full results of our best-performing method ILF + OPT-RM and other additional methods (see \S \ref{sec:ILF_OPT} for a description of ILF + OPT-RM and \S \ref{sec:main_results} for a discussion of the results). We conduct the same evaluation as described in \S \ref{sec:evaluation_results}, i.e. in a human evaluation, annotators rank various test summaries based on quality. We then calculate the win rate against human written summaries, which we use as an evaluation metric. Importantly, all methods evaluated here are trained on datasets with 5K samples. Note that the methods compared here are not exactly the same as the methods compared in Fig.~\ref{fig:finetuned_methods_comparison}. Concretely, the test summaries generated by the methods finetuning on refinements (ILF), finetuning on human summaries, and OPT-RM best-of-64 FeedME are the same as in Fig.~\ref{fig:finetuned_methods_comparison}, for the test summaries generated by corresponding methods trained on 5K samples. Here, however, we don't evaluate FeedME and finetuning on initial summaries. However, we evaluate ILF + OPT-RM (best-of-64), our best-performing model, which we also added to Fig.~\ref{fig:finetuned_methods_comparison} for reference. We also evaluate a new method called \textit{Finetuned on Feedback + Refinements}, which we describe below.

For finetuning on feedback + refinements, we us a title, post, and summary as input and the model is trained to predict the corresponding feedback and refinement. Our motivation for this approach is that generating feedback first may improve the quality of the resulting refinements, similar to the findings of previous work on self-prompting methods \citet{saunders2022self,bai2022constitutional} and the Chain of Thought (CoT) prompting technique \citet{wei2022chain}. CoT has been shown to improve the performance of models across various tasks \citet{wei2022chain} when allowing the model to reason before answering a question. 
For finetuning on feedback and refinements, we utilize the initial summaries that were used to gather human feedback, as well as the refinements generated by our method. We use the loss  $\log p(x_1, f | \text{prompt}) + \lambda \log p(\text{prompt})$, i.e. we learn to predict the refinement and the feedback. We employ the same hyperparameters as in the finetuning on refinements algorithm (including the prompt loss weight). During testing, we require initial summaries, from which we generate feedback and refinements. As initial summaries, we use the test samples generated by FeedME (as evaluated in Figure~\ref{fig:finetuned_methods_comparison}). To ensure compatibility with the 48-token length restriction of the test summaries, we append the special end token \textit{/\ n \#\#\#} to the end of the feedback and refinements during training. At test time, we set the maximum number of tokens to generate 300, and terminate generation when the stop-word \textit{/\ n \#\#\#} appears. We then apply the same postprocessing procedure outlined in Appendix~\ref{app:postprocessing} to shorten the refinements to 48 tokens. We refer to Appendix~\ref{app:finetuning_prompts} for the exact prompt templates we used.



We present all the results in Fig.~\ref{fig:additional_results}. We find that finetuning on a set of 5K refinements achieves a win rate of $36.0 \pm 1.8 \%$, while ILF + OPT-RM (best-of-64) has a win rate of $50.8 \pm 1.9 \%$, achieving human-level summarization performance (see \S \ref{sec:main_results} for a more detailed discussion). OPT-rM best-of-64 FeedMe achieves a win rate of $45.1 \pm 1.9 \%$, finetuning on a set of 5K human-generated summaries achieves a win rate of $35.4 \pm 1.8 \%$, and finetuning on a combination of 5K feedback and refinements has a win rate of $26.1 \pm 1.7 \%$. It is worth noting that the performance of finetuning on feedback and refinements is lower than that of finetuning on refinements alone. We attribute this to the increased difficulty of generating both feedback and refinements and believe that this discrepancy may be due to limitations in our models, dataset size, or hyperparameters. Previous work has demonstrated the feasibility of training models to generate feedback \citet{saunders2022self,bai2022constitutional}, so we believe that further optimization and experimentation may improve the performance of this method. We further want to note that the results for finetuning on 5K refinements, 5K human summaries, and best-of-64 FeedME deviate from the results in Fig~\ref{fig:finetuned_methods_comparison}. This is because we compare different methods with each other, and human annotations generally contain some amount of noise (given that different people annotate the same samples).


\begin{figure}[hb!]
    \centering    \centering \includegraphics[scale=0.5]{images/additional_results_test_set.pdf}
    \caption{How often human evaluators prefer summaries from ILF: Finetuned on Refinements, OPT-RM best-of-64 FeedME, ILF + OPT-RM (best-of-64), finetuning on human summaries, and finetuning on feedback + refinements (all methods finetuned on 5K samples). ILF + OPT-RM (best-of-64) generates summaries of a similar quality to human summaries. Finetuning on feedback + refinements performs worse than finetuning on refinements (ILF).}
    \label{fig:additional_results}
\end{figure}


\subsection{Multiple Iterations of ILF}
\label{app:ilf_multiple_iteratins}
Our experiments suggest that ILF is an effective method for leveraging language feedback in the training of LMs. Here we explore ILF in its most general form by doing multiple iterations of refining-and-finetuning.

\paragraph{Dataset Improvement.}
In this experiment, we evaluate the effectiveness of iterative refinement of the dataset distribution using ILF. To this end, we first finetune GPT-3-175B on 100 refinements from iteration 1 of ILF (i.e. doing one iteration of refining initial summaries, as we did in the main results of our paper, see \S \ref{sec:evaluation_results}) and refer to this finetuned model as $M_1^{100}$. The notation we use here is that the subscript indicates the iteration of ILF that the refinements were generated in, and the superscript indicates the number of overall samples the model is finetuned on. We also refer to the dataset of 100 refinements from iteration 1 as $\mathcal{D}_1^{100}$. As a baseline, we finetune $M_1^{100}$ on an additional 100 refinements from ILF iteration 1, resulting in $M_1^{200}$, i.e., a model trained on 200 refinements from ILF iteration 1. We then compare this baseline to two iterations of ILF. Specifically, we use $M_1^{100}$ to generate summaries for an additional 100 samples (the same Reddit posts as for the baseline) and collect human feedback on those summaries. We then use this feedback to generate 5 refinements using the FeedME\footnote{Ideally, one would use the same model $M_1^{100}$ to generate the refinements. However, in our case, this is not possible since we finetuned GPT-3-175B, which is not an instruction-finetuned model.} and then select the best refinement using our InstructRM method. We refer to these 100 selected refinements from the second iteration of ILF as $\mathcal{D}_2^{100}$. Finally, we finetune $M_1^{100}$ on $\mathcal{D}_2^{100}$ to obtain the model $M_{1,2}^{200}$, which has been trained on a total of 200 refinements generated in both the first and second iterations of ILF. All finetuning was performed using the same hyperparameters as described in Appendix~\ref{app:hp_tuning} for finetuning on 100 refinements. We refer to Table~\ref{tab:ilf_iterative} for an overview of all models and train datasets.

\begin{table}
\centering
\begin{tabular}{ |p{2.3cm}|p{2.3cm}||p{2.3cm}|p{2.3cm}|p{2.3cm}|| p{2.5cm}|}
 \hline
  \multirow{2}{*}{Initial Model} & \multirow{2}{*}{Finetuned Model} & \multicolumn{3}{c||}{Finetuning dataset} & \multirow{2}{*}{Produces Dataset}\\
  \cline{3-5}
  & &  ILF iteration 1 &  ILF iteration 2 &  ILF iteration 3 & \\
 \hline
 \hline
 %GPT-3-175B &  &      & & & $\mathcal{D}^{100}_1$, $\mathcal{D}^{200}_1$, $\mathcal{D}^{300}_1$ \\
%\hline
 GPT-3-175B & $M^{100}_1$   &  $\mathcal{D}^{100}_1$    & & & $\mathcal{D}^{100}_2$  \\
 \hline
  $M^{100}_1$ & $M^{200}_1$   &  $\mathcal{D}^{100*}_1$,      & & &  \\
 \hline
  GPT-3-175B & $M^{200}_{scratch,1}$  &  $\mathcal{D}^{200}_1$ & & &  \\
 \hline
  GPT-3-175B & $M^{300}_{scratch,1}$  &  $\mathcal{D}^{300}_1$ & & &  \\
 \hline
  $M^{100}_1$ & $M^{200}_{1,2}$   &  $\mathcal{D}^{100}_1$    & $\mathcal{D}^{100}_2$ &  & $\mathcal{D}^{100}_3$  \\
 \hline
  $M^{200}_{1,2}$  & $M^{300}_{1,2,3}$   &  $\mathcal{D}^{100}_1$    & $\mathcal{D}^{100}_2$ & $\mathcal{D}^{100}_3$ & \\
 \hline
    GPT-3-175B & $M^{200}_{scratch,1,2}$   &  \multicolumn{2}{c|}{$\mathcal{D}^{100}_1$ +  $\mathcal{D}^{100}_2$} & & \\
 \hline
      GPT-3-175B & $M^{300}_{scratch,1,2,3}$   &  \multicolumn{3}{c||}{$\mathcal{D}^{100}_1$ +  $\mathcal{D}^{100}_2$ + $\mathcal{D}^{100}_3$} &  \\
 \hline
\end{tabular}
\caption{Datasets (refinements) over which the models $M$ are trained, and which they generate. The superscript indicates the number of samples, whereas the subscript indicates the ILF step. In this figure we do not show FeedME which is used to generate the refinements given feedback. \newline
* these samples are new samples from the interval [100,200] of  $\mathcal{D}^{200}_1$}.
\label{tab:ilf_iterative}
\end{table}


In this human evaluation, we compare the performance of the summaries generated by the baseline model ($M_1^{200}$) with those generated by two iterations of ILF ($M_{1,2}^{200}$) on our test set. Human evaluators are asked to indicate their preferred summary for each comparison, and the win rate of $M_{1,2}^{200}$ against $M_1^{200}$ is calculated and plotted in Fig.~\ref{fig:exert_iteration} (left)\footnote{Note, we set the win rate manually to  $50 \%$ at 100 samples, since the baseline is equivalent to one iteration of ILF.}. Our results show that two iterations of ILF outperform one iteration with a win rate of $53.2 \pm 1.9 \%$ indicating that applying multiple rounds of ILF can improve the data distribution. However, we also want to investigate whether multiple rounds of ILF lead to better models than directly finetuning on the same number of refinements from the first round from scratch. In other words, while our current baseline consists of further finetuning $M_1^{100}$ on an additional 100 samples, it is also possible to directly finetune GPT-3-175B on 200 refinements from the first iteration of ILF from scratch, i.e. $M_{scratch,1}^{200}$. We aim to determine the relative effectiveness of these two approaches in improving model performance on the text summarization task.


\paragraph{Model Improvement.}
In this experiment, we aim to compare the performance of multiple rounds of ILF to directly finetuning on a comparable number of refinements from the first iteration of ILF. As a baseline, we finetune GPT-3-175B on 200 and 300 refinements from the first iteration of ILF and conduct hyperparameter tuning as described in the Appendix~\ref{app:hp_tuning}. We then compare these baselines to two and three rounds of ILF. For the two-round ILF model, we use the previously described $M_{1,2}^{200}$. To obtain the three-round ILF model, we use $M_{1,2}^{200}$ to generate summaries for an additional 100 samples (on the same Reddit posts as for the baseline), gather human feedback, generate 5 refinements with GPT-3-175B using the feedback, and select the best refinement using InstructRM, resulting in $\mathcal{D}_3^{100}$. We then finetune $M_{1,2}^{200}$ on $\mathcal{D}_3^{100}$ to obtain the model $M_{1,2,3}^{300}$. It is important to note that while our baselines finetune GPT-3-175B from scratch on 200 and 300 refinements, the models $M_{1,2}^{200}$ and $M_{1,2,3}^{300}$ are obtained by continuously finetuning a model iteratively on additional refinements. This difference in approach may introduce a discrepancy in the results, as we use different hyperparameters, and the dataset size may affect the learning dynamics. To control for this potential difference, we also finetune GPT-3-175B from scratch on the refinements generated through various iterations of ILF. Specifically, as an alternative to $M_{1,2}^{200}$, we finetune GPT-3-175B from scratch on a concatenation of 100 refinements from the first round of ILF (i.e., $\mathcal{D}_1^{100}$) and 100 refinements from the second round of ILF (i.e., $\mathcal{D}_2^{100}$), and refer to the resulting model as $M_{scratch 1,2}^{200}$. Similarly, as an alternative to $M_{1,2,3}^{300}$, we finetune GPT-3-175B from scratch on a concatenation of 100 refinements from the first round of ILF ($\mathcal{D}_1^{100}$), 100 refinements from the second round of ILF $\mathcal{D}_2^{100}$, and refinements from the third round of ILF (i.e. $\mathcal{D}_3^{100}$), and refer to the resulting model as $M_{scratch 1,2,3}^{300}$. It is worth noting that the refinements from the second and third rounds of ILF (i.e. $\mathcal{D}_2^{100}$ and $\mathcal{D}_3^{100}$) are based on summaries generated using models that were continuously finetuned (i.e. $M_1^{100}$ and $M_{1,2}^{200}$). As such, the models $M_{scratch 1,2}^{200}$ and $M_{scratch 1,2,3}^{300}$ are not a direct application of ILF, but rather an approximation of the distribution induced by ILF. We refer to Table ~\ref{tab:ilf_iterative} for an overview of all models and train datasets.


%We now investigate how multiple rounds of ILF compare against directly finetuning on the same number of refinements from the first iteration of ILF. Concretely, as a baseline, we finetune on $200$ and $300$ refinements from the first iteration of ILF (we do hyperparameter tuning on both datasets as described in Appendix~\ref{app:hp_tuning}). We compare this baseline to two and three rounds of ILF. We use the previously described model $M_{1,2}^{200}$ which represents two rounds of ILF. We then again use this model in an additional third round of ILF, i.e. we generate summaries on an additional 100 samples, gather human feedback, generate $5$ refinements, and select the best refinement with InstructRM, yielding $\text{refinement}_3^{100}$. We then finetune $M_{1,2}^{200}$ on $\text{refinement}_3^{100}$ yielding $M_{1,2,3}^{300}$, which is the result of three rounds of ILF. Note that while our baselines trained GPT-3-175B from scratch on $200$ and $300$ refinements, here we continuously finetuned the same model on 100 additional refinements. This could lead to a discrepancy in the results, since we use different hyperparameters, and the learning dynamics can vary for different dataset sizes. Thus we introduce an additional model which should control for this difference in hyperparameters. Instead of further finetuning the same model, we also finetune GPT-3-175B from scratch on the refinements generated through various iterations of ILF. As an alternative to  $M_{1,2}^{200}$, we finetuned GPT-3-175B from scratch on a concatenation of 100 refinements from the first round of ILF, and 100 refinements from the second round of ILF, i.e.  $\text{refinement}_2^{100}$, and call the resulting model $M_{scratch 1,2}^{200}$. Similarly, as an alternative to $M_{1,2,3}^{300}$, we finetune GPT-3-175B from scratch on a concatenation of 100 refinements from the first round of ILF, $\text{refinement}_2^{100}$, and refinements from the third round of ILF, i.e. $\text{refinement}_3^{100}$, and call the resulting model $M_{scratch 1,2,3}^{300}$. It is important to note that the refinements from round two and three of ILF, i.e. $\text{refinement}_2^{100}$ and $\text{refinement}_3^{100}$, we're based on summaries that are generated with models $M_1^{100}$ and $M_{1,2}^{200}$, i.e. models that were continuously finetuned. The alternative models $M_{scratch 1,2}^{200}$ and $M_{scratch 1,2,3}^{300}$ are thus not a direct application of ILF, but an approximation of the distribution induced by ILF.

Using a human evaluation, we compare the performance of the three methods on the test dataset: the baseline, ILF with continuous finetuning, and ILF approximated by finetuning from scratch. The results are shown in Fig.~\ref{fig:exert_iteration} (right). With this more realistic baseline, we find that directly applying ILF does not improve upon the baselines, with win rates of $49.4 \pm 1.9 \%$ and $50.9 \pm 1.9 \%$ for 200 and 300 samples, respectively. However, approximating ILF by finetuning from scratch on the distributions induced by ILF significantly improves upon the baseline for 300 samples, with a win rate of $55.6 \pm 1.9 \%$. The method is slightly worse than the baseline for 200 samples, with a win rate of $48.9 \pm 1.9 \%$. We currently hypothesize that continuous finetuning may lead to catastrophic forgetting, while finetuning from scratch may not have this problem. This could explain why $M_{scratch 1,2,3}^{300}$ performs significantly better than $M_{1,2,3}^{300}$ for 300 samples. Specifically, $M_{1,2}^{200}$ may actually generate an improved distribution in the third iteration of ILF. However, when further finetuning $M_{1,2}^{200}$ on this improved distribution $\mathcal{D}_2^{100}$, the model may forget what it learned previously. On the other hand, the model $M_{scratch 1,2,3}^{300}$ that learns from scratch on the concatenation of all datasets produced by ILF may actually benefit from the improved dataset distribution because it does not unlearn anything. It is, however, unclear why $M_{scratch 1,2}^{200}$ does not benefit from the improved data distribution $\mathcal{D}_2^{100}$. It is also possible that the hyperparameters play a significant role in the final performance of the various models and that the dataset size has a strong influence on model performance (e.g., finetuning on more samples may be more stable than finetuning on fewer samples). In future work, we plan to conduct more elaborate experiments to answer these questions and better understand the effects of the dataset size and number of iterations on ILF. Specifically, we aim to run multiple iterations of ILF and use $M_{scratch 1,2}^{200}$ as the model to generate summaries in the third round of ILF (instead of $M_{1,2}^{200}$). This would be a direct implementation of ILF, rather than an approximation of it, as we would be finetuning the same model with which we are also generating an improved distribution. We also hope to investigate the effect of the dataset size and number of iterations on ILF. Overall, our results suggest that ILF has the potential to improve the performance of natural language processing systems by continuously incorporating human feedback into the training of language models, but further research is needed to fully understand the best ways to leverage this approach.


%In a human evaluation, we evaluate all three methods on the test dataset, i.e. the baseline, ILF (continuous fine-tuning) and approximating ILF by fine-tuning from scratch. We show the results in Fig.~\ref{alg:expert_iteration} (right). Against this more realistic baseline, directly applying ILF does not improve upon the baselines with win rates of $49.4 \% \pm 1.9$ and $50.8 \% \pm 1.9$ for $200$ and $300$ samples. However, approximating ILF by finetuning on the distributions induced by ILF from scratch does significantly improve upon the baselines for $300$ samples with a win rate of $55.6 \% \pm 1.9$ (and is slightly worse than the baseline for $200$ samples with a win rate of $48.9 \% \pm 1.9$). We currently hypothesize that continuous finetuning might lead to catastrophic forgetting while finetuning from scratch might not have this problem. This could explain why $M_{scratch 1,2,3}^{300}$ performs significantly better than $M_{1,2,3}^{300}$ for $300$ samples. Concretely $M_{1,2}^{200}$ might actually generate an improved distribution in iteration three of ILF. However, when further finetuning $M_{1,2}^{200}$ on this improved distribution  $\text{refinement}_2^{100}$ the model might forget what it learned previously. The model $M_{scratch 1,2,3}^{300}$ that learns from scratch on the concatenation of all datasets produced by ILF, might actually benefit from the improved dataset distribution because it does unlearn anything. It is however also possible that the hyperparameters are playing a large role in the final performance of the various models. Lastly, another possibility is that the dataset size has a strong influence on the model performance, i.e. finetuning on more samples is more stable than finetuning on fewer samples. Overall it is hard to have a concrete takeaway and we would like to answer the above questions in future work with more elaborate experiments. Concretely, we would also like to run multiple iterations of ILF and use $M_{scratch 1,2}^{200}$ as model to generate summaries in the third round of ILF (instead of $M_{1,2}^{200}$). This would be a direct implementation of ILF, and not an approximation of it, i.e. we want to finetune the same model with which we are also generating an improved distribution. Furthermore we would like to investigate the effect of the dataset size and number of iterations on  ILF.



\begin{figure*}[t!]
\begin{minipage}[t]{.45\textwidth}
 \begin{center} 
    \small{
    \cblock{150}{199}{157} ILF ($M_{1,2}^{200}$)\quad \\}
    \end{center}
    \centering
\includegraphics[scale=0.5]{images/expert_iteration_comparison_100.pdf}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\textwidth}
\begin{center} 
    \small{
    \cblock{31}{119}{180} ILF (continuous finetuning) - 
  ($M_{1,2}^{200}$/$M_{1,2,3}^{300}$)\quad \\
     \cblock{255}{127}{14} Approximating ILF  - ($M_{scratch 1,2}^{200}$ /$M_{scratch 1,2,3}^{300}$)\quad \\
    }
    \end{center}
     \centering
\includegraphics[scale=0.5]{images/expert_iteration_threeway_comparison.pdf}
\end{minipage}
\caption{\textbf{Left}: Win rate of 2 iterations of ILF against finetuning on the same number of refinements from the first iteration of ILF. \textbf{Right}: Win rate of 3 iterations of ILF, and approximating 3 iterations of ILF by finetuning from scratch, against finetuning on the same number of refinements from the first iteration of ILF.}
\label{fig:exert_iteration}
\end{figure*}


\subsection{Part-of-Speech Distribution for Finetuning Datasets}
\label{app:pos}
We evaluate the negative log-likelihood of GPT-3-175B on the three finetuning datasets, i.e. on initial summaries, refinements, and human summaries. We use the training dataset with 1K samples and calculate the negative log-likelihood over different Part-of-Speech tags. We use Stanza \cite{qi2020stanza} as the PoS tagger for this experiment and then we separate the words into three groups: function words, content words, and others. The function words are words that have little lexical meaning: articles, pronouns, adpositions, conjunctions, auxiliary verbs, particles and interjections. On the other hand, content words are words that contain semantic information: nouns, adjectives, adverbs and lexical verbs. We keep numbers and symbols under the group \textit{others}. With this analysis, we want to spot different patterns between model-generated (initial summaries and refinements) and human-written summaries. Note that a high negative log-likelihood implies a high loss. We present the results in Fig~\ref{fig:pos_distribution}. Since the average loss is higher for human summaries, we normalize all the loss values by transforming them to have mean 0 and standard deviation 1. Overall, the word distribution is very similar for all three finetuning datasets. In terms of normalized mean loss, it is interesting how the content words have a bigger influence on the refinements dataset. We believe that this is related to our results in section \ref{sec:main_results}, where we obtain the best results when finetuning on refinements. 

\subsection{Comparison to Results of \citet{scheurer2022training} }
\label{app:comparison_scheurer_22}

Here we relate our results to previous work by \citet{scheurer2022training}. In Fig. 2 of \citet{scheurer2022training}, they compare their method of finetuning on refinements against various baselines, such as finetuning on initial summaries, sampling from FeedME (called InstructGPT), and sampling from GPT-3-175B. They calculate the win rate of all methods against human written summaries \citep{volske-etal-2017-tl} that are automatically extracted from Reddit. As shown in \S\ref{sec:data} and App.\ref{sec:dataset_analysis}, our human summaries are preferred $72.3 \pm 3.2 \%$ to the human summaries of \citet{volske-etal-2017-tl}. This implies that the win rates in \citet{scheurer2022training} are much higher than in our case since we use a much stronger baseline. 

We now present three differences between the results found in \citet{scheurer2022training} and the results found in our paper. Then we will provide various potential reasons that could explain the differences. First, when comparing the results (in relative terms) in \citet{scheurer2022training} Fig. 2 to our results in Fig.~\ref{fig:finetuned_methods_comparison} where we finetune on 100 samples, we see differences in performance. \citet{scheurer2022training} reports that finetuning on refinements outperforms finetuning on initial summaries. And both methods outperform sampling from FeedME (i.e., InstructGPT). In our experiments finetuning on 100 refinements achieves a win rate of $19.6 \pm 1.5 \%$ against human summaries, finetuning on initial summaries a win rate of $19.6 \pm 1.5 \%$, and FeedME a win rate of $20.8 \pm 1.5 \%$. Thus both finetuned methods perform equally and are worse than sampling from FeedME. 

Second, we compare the results of refining a summary with feedback. Note that \citet{scheurer2022training} uses an embedding-based scoring function to select refinements, whereas we use InstructRM. In \citet{scheurer2022training} Fig. 3 (left) \textsc{Refine with Feedback + Best of N} achieves a win rate of $67.0 \pm 3.1 \%$ against initial summaries (sampled from FeedME), \textsc{Refine with Feedback} achieves a win rate of $60.5 \pm 3.0 \%$, \textsc{Refine without Feedback} achieves $50.3 \pm 2.6 \%$ and Human Summaries have a win rate of $60.8 \pm 3.4$. In our Fig.~\ref{fig:win_rates_and_incorporating_feedback} (left) Refine with Feedback + Best-of-5 achieves a win rate of $69.1 \pm 1.9 \%$, Refine with Feedback achieves a win rate of $63.9 \pm 2.0 \%$, Refinement without Feedback achieves a win rate of $59.4 \pm 2.0 \%$ and Human Summaries a win rate of $83.2 \pm 1.7 \%$. The difference in the human summaries is expected, given that we use better human summaries. The Refinement without Feedback method achieves higher results in our work than in \citet{scheurer2022training}. 

Third, it is also noteworthy that using the embedding similarity as a scoring function worked well in \citet{scheurer2022training}, while it does not work in our setting (see Table \ref{tab:scoring_function_results} and \S\ref{sec:scoring_function_results} for a discussion of the results). We believe this is because the feedback we collect is written by many annotators and is thus much more diverse, while in \citet{scheurer2022training}, the authors themselves
wrote the feedback.


Here we now list various differences in the setup of \citet{scheurer2022training} and our paper, which could all account for the different results.
\begin{enumerate}
    \item \citet{scheurer2022training} use an embedding similarity as a scoring function, while we use InstructRM Ensemble. Looking at Tab.~\ref{tab:scoring_function_results} and the corresponding discussion in  \S\ref{sec:scoring_function_results}, already shows that the methods are very different. 
    \item The human-written summaries are of much higher quality in our paper than in \citet{scheurer2022training} (see \S\ref{sec:data} and App.~\ref{sec:dataset_analysis})

    \item In \citet{scheurer2022training}, the annotation instructions specifically state that the feedback should mention how to improve a summary. In our work, we collect much more unrestricted and diverse feedback. This difference is also apparent in the fact that the embedding similarity does not work well as a scoring function in our setting. 
    \item In \citet{scheurer2022training}, the authors themselves annotated the data, i.e., they wrote the feedback and evaluated the final summaries. In our case, we use independent evaluators who are trained on this task. Using 31 annotators overall also gives us a more diverse and less biased estimate of our methods. Also, doing human evaluations is inherently noisy and will never lead to the exact same results. 
    \item The evaluation in \citet{scheurer2022training} was done on a different dataset than in this work. Specifically, they used only 100 samples to evaluate their method, while we use a test set of 698 samples.
    \item The hyperparameters in \citet{scheurer2022training} used for sampling and finetuning are different from the hyperparameters used in our work.
    \item Overall, we use different prompts than \citet{scheurer2022training} (see App.~\ref{app:finetuning_prompts} and App.~\ref{app:summarization_prompts})
\end{enumerate}

\begin{figure*}[t!]
\begin{minipage}[t]{.45\textwidth}
    \centering
\includegraphics[scale=0.4]{images/Word_distribution.pdf}
\end{minipage}
\hfill
\begin{minipage}[t]{.45\textwidth}
     \centering
\includegraphics[scale=0.4]{images/Normalized_mean_loss.pdf}
\end{minipage}
\caption{Distribution of tokens of various finetuning datasets with 1K samples in terms of content and function words. We only evaluate the various completions, i.e., summaries, since the prompts are the same for all distributions.}
\label{fig:pos_distribution}
\end{figure*}

\section{Annotator Instructions}
\label{app:annotator_instructions}

Overall we completed many annotations to create datasets and evaluate our algorithm. The instructions were task-specific and also continuously updated. In the following, we provide the instructions we used to create our train dataset and the instructions we provided for evaluating the summary quality (of 6 summaries). We will not share more instructions for brevity but can provide them upon request.

\subsection{Train Dataset Annotation Instructions}
\label{app:train_data_annotation_instructions}

\textbf{Task Overview}

You are given a Reddit Post, which you first need to read carefully. You then need to complete 5 subtasks which consist of comparing two summaries, writing feedback on a summary, classifying the type of feedback, indicating whether there is additional Feedback, and writing an ideal summary. When doing these tasks, please adhere to the guidelines below.


\textbf{What makes for a good summary?} Roughly speaking, a good summary is a short piece of text that has the essence of the original text. A good summary tries to accomplish the same purpose and conveys the same information as the original text. We would like you to consider these different dimensions of summaries: 


\textbf{Essence}: Is the summary a good representation of the post? How well does the summary cover the important information in the post?


\textbf{Clarity}: Is the summary reader-friendly? Does it express ideas clearly? 

\textbf{Accuracy}: Does the summary contain the same information as the post? 

\textbf{Purpose}: Does the summary serve the same purpose as the original post? 

\textbf{Concise}: Is the summary short and to the point?

\textbf{Style}: Is the summary written in the same style as the original post? 


Generally speaking, we give higher weight to the dimensions at the top of the list. The evaluation can be complicated though, since none of the above dimensions are simple yes/no matters, and there arent hard and fast rules for trading off different dimensions. Use your best judgment and common sense to make these trade-offs. In case the subreddit, title, and Reddit post leave open some ambiguity about what happened, it is important to accurately reflect that in your annotations and not just interpret the text in a certain way. Always look at all the subreddit, title, and Reddit Post and use all information given to make your judgments (sometimes the title may contain crucial information that does not appear in the post but should nevertheless be used). 



First, read the Subreddit category, title, and post carefully. A Subreddit is a forum dedicated to a specific topic on the website Reddit. Take your time with this step and re-read the parts that you might not have understood at first. Below is a detailed description of the task you will need to complete for each Reddit post.



Below is a detailed description of each task you will need to complete for each Reddit post: 

\begin{enumerate}
    \item \textbf{Comparison Task}: Given a pair of summaries, indicate which is better.
    
    \textit{Details}: Use the above description of what makes a good summary. It is alright to choose either summary if both summaries are identical copies of each other or if there is no distinguishing feature that makes one summary superior to the other. However, if there is a small detail that makes one summary better than the other, that is enough reason to select that summary.


    \item \textbf{Feedback Task}: Write short and simple feedback on the given summary about the single, most important shortcoming of the summary. The feedback should NOT mention what category (Accuracy, Coverage, Coherence, other) the feedback belongs to, nor should it assume knowledge about the definitions of Coverage, Accuracy, or Coherence (see below). Otherwise, the feedback should be as short and simple as possible while still addressing the most important shortcoming of the summary.

    \textit{Details}: You can write the feedback in one or several sentences, but it should only address the single, most important shortcoming of the summary and be as short as possible. There are no other restrictions as to how you write the feedback and what exactly it addresses. If there are no shortcomings in the summary, the feedback can also mention a positive thing about the summary.
    Use the description of what makes a good summary to trade off the various dimensions that make for a good summary. Often the feedback will (but does not have to) address one of the following axes.  
    \begin{itemize}
        \item \textbf{Coverage}: For this axis, answer the question, how well does the summary cover the important information in the post? A summary has good coverage if it mentions the main information from the post thats important to understand the situation described in the post. A summary has poor coverage if someone reading only the summary would miss several important pieces of information about the situation in the post. A summary with good coverage should also match the purpose of the original post (e.g., to ask for advice).
        \item \textbf{Accuracy}: For this axis, answer the question, does the factual information in the summary accurately match the post? A summary is accurate if it doesnt say things that arent in the article, doesnt mix up people, and is generally not misleading.  If the summary says anything at all that is not mentioned in the post or contradicts something in the post, it is NOT accurate.
        \item \textbf{Coherence}: For this axis, answer the question, how coherent is the summary on its own? A summary is coherent if, when read by itself, its easy to understand and free of English errors. A summary is not coherent if its difficult to understand what the summary is trying to say. Generally, its more important that the summary is understandable than being free of grammar errors.
    \end{itemize}
    
    Additional Rules: 
    The feedback should NOT mention what category (Accuracy, Coverage, Coherence, other) the feedback belongs to, nor should it assume knowledge about the definitions of Coverage, Accuracy, Coherence, or other (as defined above).
    Example: One should NOT write "This is missing in the area of coverage", or "This summary lacks in the category of accuracy, because ...". The feedback should be understandable to a person who has never read the definition of "Coverage", "Accuracy", and "Coherence". You are, however, ALLOWED to use those words if they make sense on their own, e.g., you CAN say, "This summary does not cover the important parts of the text because", or "This summary is inaccurate as it states ...", or "This is not a coherent summary because ...".

    \item \textbf{Feedback Type Task}: If your feedback falls into the categories Accuracy-related, Coherence-related, or Coverage-related, mark it as such by checking the corresponding checkbox for the (single) category it is related to. If your feedback is not related to any of these three categories, then check the "Other" checkbox.

    \item \textbf{More Feedback Task}: Answer with Yes if there is additional Feedback about an important shortcoming of the summary that you would want to mention and No otherwise.

    \item \textbf{Ideal Summary Task}: Ideal Summary Task: Write a short summary for the Reddit post that is ideal in your view.

    \textit{Details}: The ideal summary should be ideal in terms of all the criteria mentioned above, i.e., essence, clarity, accuracy, coverage, purpose, conciseness, coherence, and style. In other words, you should not be able to find an obvious critique of the ideal summary that you write. It is okay to reuse parts of previous summaries but only if those parts should be a part of an ideal summary. The ideal summary should maximally be 48 tokens long (otherwise, you can't submit your annotation). Tokens are generated by taking your ideal summary and splitting up certain words into individual pieces (this is necessary to train our AI). The interface will show you how many tokens your ideal summary has already taken up.
\end{enumerate}


\subsection{Summary Quality Evaluation Instructions}
\label{app:summary_quality_instructions}

\textbf{Task Overview}

You will be given a Subreddit category, a title, and a Reddit Post, which you first need to read carefully. Your task is then to compare 6 summaries and rank them according to quality.


\textbf{What makes for a good summary?} Roughly speaking, a good summary is a short piece of text that has the essence of the original text. A good summary tries to accomplish the same purpose and conveys the same information as the original text. We would like you to consider these different dimensions of summaries: 


\textbf{Essence}: Is the summary a good representation of the post? How well does the summary cover the important information in the post?

\textbf{Clarity}: Is the summary reader-friendly? Does it express ideas clearly? 

\textbf{Accuracy}: Does the summary contain the same information as the post? 

\textbf{Purpose}: Does the summary serve the same purpose as the original post? 

\textbf{Concise}: Is the summary short and to the point?

\textbf{Style}: Is the summary written in the same style as the original post? 


Generally speaking, we give higher weight to the dimensions at the top of the list. The evaluation can be complicated though, since none of the above dimensions are simple yes/no matters, and there arent hard and fast rules for trading off different dimensions. Use your best judgment and common sense to make these trade-offs. In case the subreddit, title, and Reddit post leave open some ambiguity about what happened, it is important to accurately reflect that in your annotations and not just interpret the text in a certain way. Always look at all the subreddit, title, and Reddit Post and use all information given to make your judgments (sometimes the title may contain crucial information that does not appear in the post but should nevertheless be used). 


First, read the Subreddit category, title, and post carefully. A Subreddit is a forum dedicated to a specific topic on the website Reddit. Take your time with this step and re-read the parts that you might not have understood at first. Below is a detailed description of the task you will need to complete for each Reddit post.



\textbf{Comparison Task}: Given 6 summaries, indicate which is better by ranking them according to quality. Rank 1 is considered the highest rank, and Rank 6 is considered the lowest rank. The summary with the best quality should be ranked highest, i.e., as Rank 1, and the summary with the worst quality should be ranked lowest, i.e. Rank 6. Use the above description of what makes a good summary. Ties between summaries are allowed, but only if summaries are exact copies of each other or if there is no distinguishing feature that makes one summary superior to the other. However, if there is a small detail that makes one summary better than the other, that is enough reason to rank that summary as better than the other summary. We use Standard Competition ranking (i.e., example rankings of 122456). In standard competition ranking, items that compare equally receive the same ranking number, and then a gap is left in the ranking numbers. The number of ranking numbers that are left out in this gap is one less than the number of items that are compared equally. Equivalently, each items ranking number is 1 plus the number of items ranked above it.


\section{Prompts}
\subsection{Summarization Prompts}
\label{app:summarization_prompts}
\input{appendix_summary_promts}

\subsection{InstructRM Prompts}
\label{app:instruct_rm_prompts}
\input{appendix_instruct_rm_prompts}

\subsection{Finetuning Prompts}
\label{app:finetuning_prompts}
\input{appendix_finetuning_prompts}

\subsection{Reward Model Prompts}
\label{app:rm_prompts}
\input{appendix_reward_model_prompts}


% keep this out for blind review
