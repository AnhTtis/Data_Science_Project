\section{Evaluation}\label{sec:eval}
% why not ART-OLC? ART is sensitive to skewness and value distribution of keys. Added text on this to related work
This section compares \sys with eRPC-Masstree \cite{erpc,masstree}, a state-of-the-art ordered key-value store that supports variable-size keys and values (we discuss why we chose this baseline in \Cref{sec:related}). It also evaluates the impact of several optimizations on performance.

 
% We firstly evaluate the system performance of \sys and eRPC-Masstree on the same server platform. 
% To further understand the differences, we also evaluate the system latency, performance sensitivity on key size and range scan size. 
% Lastly, the impact of our design optimizations are evaluated. 
% These include using log block in leaf nodes, supporting linearizability with MVCC and caching internal nodes in accelerator memory. 


\begin{figure*}[ht!]
    \centering
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ycsb_perf_uniform.pdf}
        % \caption{}
        \label{fig:ycsb_perf_uniform}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ycsb_perf_watt_uniform.pdf}
        % \caption{}
        \label{fig:ycsb_perf_watt_uniform}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ycsb_perf_zipf.pdf}
        % \caption{}
        \label{fig:ycsb_perf_zipf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ycsb_perf_watt_zipf.pdf}
        % \caption{}
        \label{fig:ycsb_perf_watt_zipf}
    \end{minipage}
    %%
    \vskip -16pt 
    \caption{Comparison of throughput and efficiency for YCSB workloads using uniform and Zipfian/latest distributions.}
    \label{fig:ycsb_perf_comparison}
\end{figure*}


\subsection{Experimental setup}
% DELL Z9100-ON switch
% CPU: Intel Xeon E5-2660 v3 @ 2.89 GHz (default 2.60GHZ)
% DRAM: 64GB, 4 channels DDR4-2133
% NIC: Mellanox ConnectX-3
% FPGA: Intel Arria 10 1150 (also used in Microsoft Azure SmartNIC and Brainwave)

Experiments ran one server on one machine and clients on another.
They ran on a single socket of a dual-socket machine because this provided the best cost-performance for both systems. Each
socket had a 10-core Intel Xeon E5-2660 v3 @ 2.89 GHz and four channels to 64~GB of DDR4-2133 DRAM.
The FPGA accelerator card had an Intel Arria 10 1150 FPGA with two channels to 4~GB on-board DDR4-2133 DRAM,
two PCIe Gen3 x8 channels, and 50-Gbps Ethernet. Each machine had 
a separate 40-Gbps Ethernet ConnectX-3 NIC. We used the socket that is attached to both the FPGA and the NIC.
We ran 10 software threads, each pinned to a different core on the socket.
\sys ran on Windows Server 2016 Datacenter using the FPGA, and eRPC-Masstree ran on Ubuntu 20.04 with DPDK 19.11.5 using
the ConnectX-3 NIC.
Machines were connected to a DELL Z9100-ON switch. The different bandwidths did not affect the comparison because 
eRPC-Masstree was never bottlenecked on the network. 


% \begin{figure*}[t]
%     \centering
%     \begin{minipage}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/tree_size.pdf}
%         % \vskip -12pt 
%         \caption{Performance impact of varying tree size on 1-item {\sc scan}. }
%         \label{fig:tree_size}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/log_block_size.pdf}
%         % \vskip -12pt 
%         \caption{Performance impact of varying leaf log block size on \sys. }
%         \label{fig:log_block_size}
%     \end{minipage} 
%     \hfill
%     \begin{minipage}[b]{0.3\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/mvcc_impact.pdf}
%         % \vskip -12pt 
%         \caption{Performance impact of using MVCC support on \sys. }
%         \label{fig:mvcc_impact}
%     \end{minipage}
% \end{figure*}


%\subsection{\sys accelerator configuration}

\if 0
%TODO figure out how to add this back
The processing latency of the block data in KSUs and RSUs is bounded. 
Given a B-Tree node with $K$-byte keys in $P$-byte KV pairs, we will typically have at most $512/P$ keys in the block data. 
% (\textit{e.g.} KV pair in an iterior node shortcut block with 16-byte keys has $2+16+2=24$ bytes). 
Each key takes $K/W + 1$ cycles to be streamed for comparison, where $W$ represents the comparison byte width. 
There are additional $Q$ cycles for initiating the processing and generating the result.
$Q$ is fixed and below 10 cycles in our implementation. % typically between 2 and 10
The average processing latency of KSU/RSU on shortcut block and the sorted layout is 
\begin{equation*}
    L = \frac{512}{2 \times P} \times (\frac{K}{W} + 1) + Q
\end{equation*}
, which is equivalent to hitting a key in the middle of the block data.
Given a B-Tree with $N_{inter}$ internal levels and a target request throughput $T_{req}$, we will have the number of KSUs:
$
% \begin{equation*}
    N_{KSU} = 2 \times N_{inter} \times T_{req} \times L_{inter}
% \end{equation*}
$
, where $L_{inter}$ is average KSU processing latency.
Similarly, we also need to scale the number MSI adapters for KSUs: 
$
    N_{Inter-MSI} = 2 \times N_{inter} \times T_{req} \times L_{mem}
$
, where $L_{mem}$ is the average latency of transferring one requested block data via MSI. 

The number of each RSU variants, $N_{RSU}$, is equal to $T_{req} \times L_{scan}$, where $L_{scan}$ is the latency of scanning one block data. 
When processing the unsorted layout, Log-RSU will take extra $512/P$ cycles to go over all KV pairs at the beginning.
In particular, the RSU designed for large block scanning consumes more cycles and logic resources to merge all key-value pairs detected in the corresponding leaf node. 
The number of MSI adapters for RSUs is $3 \times T_{req} \times L_{mem}$. 
The number of buffer slots mentioned in \Cref{sec:rsu}, $N_{slot}$, is equal to $T_{req} \times L_{range}$, where $L_{range}$ represents the latency of finishing one range scan on leaf nodes. 
In our implementation, $N_{slot}$ can be less than 64, and the total on-chip memory used for scan buffers is below 256 KB. 
When configuring the number of KSUs and RSUs, we also apply around $1.4\times$ over provisioning to incorporate the burstiness in hardware processing patterns. 
\fi

\begin{table}[t]
    \centering
    \resizebox{0.9\columnwidth}{!}{
        \begin{tabular}{ l | r r r } 
            \hline
                & Logic & Register & Block RAM \\ 
            \hline\hline
            Shell               & 14.0\%    & 8.9\%     & 14.6\%    \\ 
            Networking          & 6.9\%     & 2.4\%     & 21.0\%    \\
            B-Tree accelerator  & 33.0\%    & 11.5\%    & 35.6\%    \\
            Memory subsystem    & 7.3\%     & 2.9\%     & 7.7\%     \\
            \hline
            Total               & 61.2\%    & 25.7\%    & 78.8\%    \\
            \hline
        \end{tabular}
    }
    \caption{FPGA resource usage of \sys.}
    \label{tab:hw_res_usage}
\end{table}

\begin{table}[t]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ l | c c c c c c} 
            \hline
                & A & B & C & D & E & F \\ 
            \hline\hline
            \thead{Read \\Operation}      & {\sc lookup}    & {\sc lookup}    & { \sc lookup}    & {\sc lookup}    & \thead{{\sc scan} \\ (1 to 100)}   & {\sc lookup   }  \\
            \thead{Write \\Operation}     & {\sc update}    & {\sc update}    & -                & {\sc insert}    & {\sc insert}   & \thead{{\sc rd-mod-} \\{\sc wr}}  \\
            \thead{Rd-Wr \\Ratio (\%)}    & 50:50     & 95:5      & 100:0     & 95:5      & 95:5     & \thead{50:50 \\ (66.6\% { \sc lookup})}      \\
            \hline
        \end{tabular}
    }
    \caption{YCSB workloads.}
    \label{tab:ycsb_workloads}
\end{table}

We configured \sys with MVCC by default even though eRPC-Masstree does not provide linearizability for scans.
The modular design of the \sys accelerator enables the user to choose configurations with different numbers of 
KSUs, RSUs, and MSIs. This is critical to achieving good power efficiency because it allows the user to trade off performance for power and hardware resource efficiency.
We used a simple analytic performance model and tuning experiments to select the minimum number of these units to achieve a target {\sc scan} operation throughput of approximately 10 Mops/s. We used 14 KSUs, 4 shortcut-RSUs, 5 log-RSUs, 5 sorted-RSUs, and 4 MSI adapters in all the experiments described in this paper. The breakdown of FPGA resource usage is shown in \Cref{tab:hw_res_usage}.
The FPGA design tool \cite{quartuspower} reports a TDP of 34.9 W. 


We ran each experiment three times and present the average of the results. The range of the results was below 4\% of the average for all experiments.

% removed because it is quite a bit more than half for BRAM. 
%The hardware design only utilizes a bit over half of the entire FPGA. 
%The average logic cost of single KSU/RSU is just $0.7\sim1.2\%$. 


\subsection{Workloads}

We used two sets of workloads in the evaluation {\em YCSB}~\cite{ycsb,ycsbworkloads} and a {\em cloud-storage} workload representative of the distributed file system application~\cite{azurestorage} discussed throughout the paper. We ran all six YCSB workloads (see~\Cref{tab:ycsb_workloads}) with both uniform and skewed access patterns. The read-modify-write operation in YCSB-F is a combination of a {\sc lookup} followed by an {\sc update}. 
The cloud-storage workload is similar to YCSB-E but uses shorter scans and we varied the percentage of scans from 50\% to 100\% to characterize the range of read-write ratios for which \sys is beneficial.  We also varied the number of key-value pairs returned by scans and the size of keys and values. 
In all workloads, insert keys were generated randomly with uniform distribution as in \cite{learnedcache}. We used both uniform and Zipfian~\cite{ycsb} ($\theta=0.99$) distributions for lookup keys and the start keys of scans. 

All experiments used the uniform distribution, and 16-byte keys and values unless specified otherwise. The store had 128 million key-value pairs in the initial state for all experiments, which are stored in a 4-level tree in \sys.
We observed that eRPC-Masstree consumes $3\times$ more memory than the total size of the key-value pairs to store the whole B-Tree, whereas \sys only consumes about $1.44\times$. 

\begin{figure*}[ht!]
    \centering
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/perf_uniform.pdf}
        % \caption{}
        \label{fig:perf_uniform}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/perf_watt_uniform.pdf}
        % \caption{}
        \label{fig:perf_watt_uniform}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/perf_zipf.pdf}
        % \caption{}
        \label{fig:perf_zipf}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/perf_watt_zipf.pdf}
        % \caption{}
        \label{fig:perf_watt_zipf}
    \end{minipage}
    %%
    \vskip -16pt 
    \caption{Comparison of throughput and efficiency for cloud-storage workloads with 50\% to 100\% reads using uniform and Zipfian distributions. The read operation is {\sc scan} with 3 to 4 items in the range. }
    % \vskip -8pt 
    \label{fig:perf_comparison}
\end{figure*}


\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.245\textwidth}
    \centering
        \includegraphics[width=\textwidth]{figures/latency_throughput.pdf}
        \vskip -8pt 
        \caption{Latency-throughput. \\ \quad } % with a uniform distribution
        \label{fig:throughput_latency}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
      \centering
        \includegraphics[width=\textwidth]{figures/scan_size.pdf}
        \vskip -8pt 
        \caption{Varying {\sc scan} size. \\ \quad }
        \label{fig:scan_size}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
          \centering
        \includegraphics[width=\textwidth]{figures/key_size.pdf}
        \vskip -8pt 
        \caption{Varying key sizes. \\ \quad}
        \label{fig:key_size}
    \end{minipage} 
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mvcc_impact.pdf}
        
        \vskip -8pt 
      \caption{Impact of MVCC.\\ \quad }
      \label{fig:mvcc_impact}
      \end{minipage}
    \vskip -8pt 
\end{figure*}


\subsection{Comparison with eRPC-Masstree} 

Cost-performance is the key metric to optimize in large scale data centers. We use TDP as a proxy for total cost of ownership (TCO) as in~\cite{jouppi2021ten}. 
%It correlates better with TCO than measured power because it accounts for the cost of provisioning power and cooling when servers run at full power~\cite{jouppi2021ten}. 
We use a single-socket server for \sys because adding another socket increases TDP without increasing the throughput of the hardware accelerator. We ran experiments with two sockets for eRPC-Masstree but they resulted in worse cost-performance, e.g.,  
it achieved $1.34\times$ better cost-performance with one socket than with two for read-only 3-item scan workloads. Therefore, we also present single socket results for eRPC-Masstree. 
We compute TDP by adding numbers for each component from published documentation. The server TDP is 127~W for eRPC-Masstree and 157.9~W for \sys. The TDP for 
\sys is larger because
 it uses a 40-W FPGA accelerator board instead of a 10-W ConnectX-3 NIC \cite{connectx3}.
 
 Both the throughput and the TDP should scale by a factor of two when using an additional CPU socket connected to an additional NIC (for eRPC-Masstree) or SmartNIC (for \sys). Therefore, the performance per watt of TDP for such a system would be similar to the results we present with a single socket.
 

\if 0
We also ran experiments with a software-only version of \sys running on the CPU. It performed significantly worse than eRPC-Masstree because our design is optimized for hybrid FPGA-CPU execution across PCIe not for exploiting the CPU memory hierarchy. Therefore, we do not show results for the software-only version.
\fi

{\bf YCSB throughput and efficiency:}
\Cref{fig:ycsb_perf_comparison} compares both average throughput (Mreqs/s) and throughput per watt of TDP (Kreqs/s/W) of eRPC-Masstree and \sys.
Since \sys is not optimized for write-heavy workloads, it is less efficient running YCSB-A and F.
% as in YCSB-A~\cite{ycsb}, it achieves similar throughout against eRPC-Masstress by 8\% less with uniform and 4\% less with Zipfian. The corresponding throughput per watt is 26\% and 22\% less respectively. \sys starts to show $1.2\times$  better performance with similar power efficiency as in YCSB-F, where $2/3$ operations are lookups. 
For YCSB-B, C and D, \sys improves throughput per watt by $1.5\times$ with both uniform and Zipfian/latest distributions. It also improves the throughput for these workloads by $1.9\times$ with uniform and $1.8\times$ with Zipfian/latest distributions. 
Since \sys is optimized for scans, it does particularly well in YCSB-E improving throughout by up to $2.9\times$ and efficiency by up $2.3\times$. \sys is bottlenecked on the network for large scans while eRPC-Masstree is always bottlenecked on the CPU.

%As the response message size can be over 3 KB, \sys performance has been throttled when handling large network transfers. Meanwhile, eRPC-Masstress %performance is bottelnecked by CPU.

\begin{figure*}[th!]
    \centering
    % \begin{subfigure}[b]{0.25\textwidth}
    \subfloat[Performance and cache hit rate.]{
        \centering
        \includegraphics[width=0.25\textwidth]{figures/cache_size_throughput_hit_rate.pdf}
        % \vskip -8pt 
        % \caption{Performance and cache hit rate. }
        \label{fig:cache_size_tput_hit}
    }
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.37\textwidth}
    \subfloat[FPGA memory bandwidth breakdown.]{
        \centering
        \includegraphics[width=0.36\textwidth]{figures/cache_size_bandwidth.pdf}
        % \vskip -8pt 
        % \caption{FPGA memory bandwidth breakdown. }
        \label{fig:cache_size_bandwidth}
    }
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.37\textwidth}
    \subfloat[FPGA memory IOPS breakdown.]{
        \centering
        \includegraphics[width=0.36\textwidth]{figures/cache_size_iops.pdf}
        % \vskip -8pt 
        % \caption{FPGA memory IOPS breakdown.}
        \label{fig:cache_size_iops}
    }
    % \end{subfigure}
    %%
    \vskip -6pt 
    \caption{Performance impact of caching and load balancing on \sys (FPGA only) 1-item {\sc scan}. }
    \vskip -6pt 
    \label{fig:cache_size}
\end{figure*}

\begin{figure}
    \centering
          \includegraphics[width=0.7\columnwidth]{figures/log_block_size.pdf}
          \vskip -8pt 
          \caption{Performance impact of log block size. }
          \vskip -6pt 
          \label{fig:log_block_size}
\end{figure}



{\bf Cloud storage throughput and efficiency:} \label{sec:cloud_storage_compare}
% \Cref{fig:perf_comparison} compares both average throughput (Mreqs/s) and throughput per watt of TDP (Kreqs/s/W) of eRPC-Masstree and \sys. 
\Cref{fig:perf_comparison} compares performance and efficiency of eRPC-Masstree and \sys running the cloud-storage workload.
In this experiment, the boundary keys in \sys~{\sc scan(K$_l$, K$_u$)} requests were chosen to return exactly three items when executed on the inital key-value store. However, they can return between three and four items because of newly-inserted items. Since eRPC-Masstree provides a different {\sc scan(K,N)} operation that returns the {\sc N} items following {\sc K}, we selected a value of $N$ between three and four to ensure {\sc scan} operations return the same average number of items in both systems. For high read percentages, \sys also uses CPU cores to execute {\sc scan} operations. We configured eRPC-Masstree to enable any thread to execute {\sc scan}s.

The results show that \sys improves both performance and cost-performance significantly for scan-mostly workloads. Since \sys accelerates only read operations, the improvement grows with the read percentage. In the worst case of 100\% writes, \sys achieves only 58\% of the throughput of eRPC-Masstree (47\% of the cost-performance) because there is no hardware acceleration and the B-Tree is optimized for hybrid CPU-FPGA execution. However, even for workloads with 50\% writes, \sys achieves a similar throughput per watt as eRPC-Masstree and better throughput ($1.2\times$ better with uniform distribution). For workloads with at least 80\% reads, \sys improves throughput per watt by $1.9\times$ with uniform and $1.6\times$ with Zipfian distributions. It also improves the throughput for these workloads by $2.3\times$ with uniform and $2.0\times$ with Zipfian distributions.

% We also ran YCSB D (95\% gets and 5\% inserts) on both systems. \sys improves throughput per watt  by 1.5$\times$  with uniform and 1.4$\times$ with Zipfian. It also has 1.9$\times$ better throughput with uniform and 1.7$\times$ better with Zipfian.

As in YCSB workloads, the gains from acceleration are lower with the Zipfian distribution. Whereas eRPC-Masstree can leverage better locality with CPU caching, the current implementation of \sys does not cache leaf nodes, which prevents it from caching leaves containing popular items. We plan to explore leaf caching in the future.

With modern cloud storage server designs that leverage NVMe SSDs~\cite{ocp2021nvme, ocpSamsungSSD} and fast networks to provide tens of millions of IOPS per server, indexing metadata requires powerful CPUs that account for a large fraction of the overall TCO, e.g., CPUs, DRAM, and the NIC account for half the TDP in Open Compute Project's Poseidon~\cite{ocp2022poseidon} storage server. Therefore, \sys can significantly increase overall performance per TCO for these storage servers, e.g., we estimate improvements around 20\% for server designs similar to Poseidon. This is very significant for large-scale data center deployments.

We expect \sys's cost-performance gains to increase with future hardware because newer FPGAs have more on-chip memory to cache B-Tree nodes and use PCIe Gen5 that has $4\times$ the bandwidth of PCIe Gen3.  Despite the bandwidth improvement, we expect PCIe to remain the bottleneck because we can use configurations with more KSUs, RSUs, and MSIs to increase parallelism. Therefore, the techniques proposed in this paper will continue to be important.


{\bf End-to-end latency:}
\Cref{fig:throughput_latency} shows throughput-latency curves for median latency measured at the client in a read-only workload where each {\sc scan} returns exactly three items. \sys can provide better throughput than eRPC-Masstree's throughput at lower latency. However, eRPC-Masstree has lower latency at low load. This is mostly because \sys memory accesses over PCIe and to on-board DRAM have higher latency than CPU memory accesses.
%  but also because the network path in the ConnectX-3 NIC is faster than the one implemented in the FPGA.
%TODO: explain why? some of the network path in soft logic and also PCI dma

{\bf Scan size:}
\Cref{fig:scan_size} shows the throughput of both systems for a read-only workload when varying the number of items returned by {\sc scan}s.
The gains of acceleration increase with scan size, for example, \sys has $4.0\times$ better throughput and $3.2\times$ better throughput per watt with 24-item scans.
Since eRPC-Masstree must follow pointers to each item in the scan range, these random memory accesses become a bottleneck.
\sys can amortize PCIe accesses to a leaf node over many items because it inlines variable-sized items in leaves. 
% We limit {\sc scan} length to 24 because the current implementations of both \sys and eRPC-Masstree limit responses to 1KB.
With longer scans \sys becomes network bound while eRPC-Masstree remains CPU bound (as observed in YCSB-E).

%We evaluate {\sc scan} length up to 24, with  (nearly 1-KB response message ) because \sys consumes close to 40-Gbps network bandwidth. 
%When scanning further more items, \sys performance will be network bound.  

% Since \sys implements {\sc get} using {\sc scan}, the result for 1-item {\sc scan} corresponds to YCSB C.
% We ran YCSB C on eRPC-Masstree. It achieves 2.6\% higher throughput with {\sc get} than with {\sc scan}. \sys
% achieves 1.8$\times$ better throughput and 1.5$\times$ better throughput per watt on YCSB C.


{\bf Key size:}
\Cref{fig:key_size} shows the throughput of both systems running 1-item {\sc scan} on the initial store when the size of keys and values increases (equal key and value sizes). We use 1-item {\sc scan} to better isolate the impact of increasing key sizes on tree traversal.
The performance of both systems drops as key size increases. eRPC-Masstree has deeper trees to traverse with larger keys.
The depth of the \sys does not change but the accelerator must fetch larger sorted block segments.
We also compared both systems on a store with a mix of key and value sizes chosen uniformly from multiples of 8B less than or equal to 32B.
These demonstrate that \sys performs well with variable-sized KV pairs. 
% Their performance is similar to accessing the store with 24B-sized KV pairs. 
% and the time to compare keys in KSUs and RSUs also increases.
%Ideally, we can further increase the number of processing units to avoid this bottleneck. 

\if 0
{\bf Tree size:}
% \Cref{fig:tree_size} shows 
The throughput of both systems running 1-item {\sc scan} on a store with 16-byte keys and values when the number of key-value pairs increases. 
The performance of eRPC-Masstree drops by up to 7.8\%, while \sys sustains the same throughput with the same B-Tree depth. 
We also observe that eRPC-Masstree consumes $3\times$ more memory than the total size of the key-value pairs, whereas \sys only consumes about $1.44\times$. 
\fi

\subsection{Impact of optimizations}

We ran experiments to investigate the performance impact of using MVCC, log blocks, the cache, and the load balancer.

{\bf Cost of MVCC:}
All experiments ran with MVCC to provide linearizable scans. Since eRPC-Masstree does not provide linearizable scans, we also ran experiments to evaluate the impact of MVCC on throughput.
\Cref{fig:mvcc_impact} shows that turning off MVCC improves \sys performance on cloud-storage workloads by up to 14\% when the workload is bottlenecked by {\sc insert}s. The overhead on read-heavy workloads is negligible. 

%As described in \Cref{sec:rel_ring}, the release ring improves write performance by batching multiple updates to the global read version in the accelerator instead of updating it on every write. 
%The results show that 
% The results show that the release ring significantly improves throughput. It reduces MVCC overhead to 7\% for workloads with at least 90\% reads. We intend to improve performance further by allowing threads to run other operations instead of blocking to release changes, while delaying responses to clients until changes are released to preserve consistency.

{\bf Log block:}
\Cref{fig:log_block_size} shows throughput of 1-item scans in a read-only workload and of inserts in a write-only workload with varying log block size.
Using log blocks improves insert performance because it avoids adding the new item to the sorted block, adjusting shortcuts, and updating the accelerator page table on every operation. However, it reduces {\sc scan} performance because it increases the amount of data accessed over PCIe. Using a 512-byte block achieves most of the benefit while decreasing {\sc scan} throughput by only 8\%. 

{\bf Accelerator cache and load balancer:}
\Cref{fig:cache_size} shows the impact of caching on 1-item {\sc scan} throughput  in a read-only workload using only the FPGA (no CPU).
\Cref{fig:cache_size_tput_hit} shows request throughput and cache hit rate for interior node accesses.
Caching the root node in on-chip memory (RT) improves performance by 30\% over the no-cache case.
Adding an on-board DRAM cache with 256~MB (which can hold all interior nodes) improves performance by 
$2.5\times$. The load balancer directs some cache hits to host memory over PCIe
to maximize off-chip bandwidth utilization. Removing the load balancer (NoLB) decreases throughput by 13\%.

Figures \ref{fig:cache_size_bandwidth} and \ref{fig:cache_size_iops} break down bandwidth and IOPS for PCIe and on-board DRAM traffic.
For small cache sizes, the bottleneck is PCIe bandwidth and there are writes to on-board DRAM due to cache replacement. With a 256~MB cache
(100\% hit rate) and no load balancing (NoLB), on-board DRAM bandwidth is the bottleneck while 4 GB/s of PCIe bandwidth is left unused.
The load balancer shifts traffic to PCIe to maximize off-chip bandwidth utilization increasing throughput from 8.0 to 9.1 Mreqs/s.
Page table and cache metadata reads consume a small fraction of available on-board DRAM bandwidth but a significant fraction of IOPS. For small cache sizes, there are no metadata reads from on-board DRAM because all metadata fits in the on-chip cache.


\if 0
\subsection{Cloud-storage server throughput/TCO}

We analyze the improvement provided by \sys on the overall cost-performance of a recent Open Compute Project (OCP) 1U dual-socket server reference design with 32 NVMe SSDs targeting cloud storage~\cite{ocp2022poseidon, ocp2021nvme, ocpSamsungSSD}. As in other cloud storage systems \cite{chang2006bigtable,azurestorage}, we assume that a sharded tree-based index running on each storage server is used to locate the data in a distributed storage system consisting of many of these servers. Clients send {\sc scan} requests to map ranges of logical file offsets to the servers storing the data, which may not be the server with the relevant shard of the index, and then fetch the data with NVMe over fabric from these servers. 

We consider two implementations for the index eRPC-Masstree and \sys. 
Since the chosen SSD\cite{ocpSamsungSSD} provides 1M random read IOPS, we can achieve up to 32M IOPS per server. We extrapolate from the results in \Cref{sec:cloud_storage_compare} to dimension the eRPC-Masstree and \sys implementations to achieve 32M {\sc scan} operations per second.
For eRPC-Masstree, we use a Xeon Platinum 8358 CPU with 32 cores and a TDP of 250W. For \sys, we can use a less powerful Xeon Gold 5318N CPU with 24 cores and a 135W TDP. Both implementations have two CPUs, two 100-Gbps NICs, 16 channels of DRAM, and 32$\times$ 14-W SSDs \cite{ocp2022poseidon, ocpSamsungSSD}. Additionally, the \sys implementation has two Intel Arria 10 1150 FPGAs with a 35W TDP. The eRPC-Masstree implementation has an overall TDP of 1024W compared to 876W for \sys. Both implementations are bottlenecked on the number of SSD IOPS regardless of the read-write ratio. The \sys implementation provides 17\% better throughput per watt of TDP (a proxy for TCO). 


We expect \sys's cost-performance gains to increase with future hardware because newer FPGAs have more on-chip memory to cache B-Tree nodes and use PCIe Gen5 that has $4\times$ the bandwidth of PCIe Gen3.  Despite the bandwidth improvement, we expect PCIe to remain the bottleneck because we can use configurations with more KSUs, RSUs, and MSIs to increase parallelism. Therefore, the techniques proposed in this paper will continue to be important.

\fi


\if 0
As specified in the reference design, the CPU is a 3rd-gen Intel Xeon scalable processor with 185~W TDP constraints. 
If the system is based on Masstree, we will select Xeon Platinum 8352M Processor \cite{xeon8352M} with 28 cores at 2.4GHz.
For \sys, we can use a less powerful CPU, Xeon Gold 5318N Processor, with 24 cores at 2.6GHz and 135W TDP.
Both setups have two CPUs, two 100-Gbps NICs, 16 channels of DRAM, and 32x 14-W SSDs \cite{ocp2022poseidon, ocpSamsungSSD}.
\sys-based system will have two extra 35W FPGA accelerators as used in the evaluation (achieving 8.5~M reqs/sec small {\sc scan}). 
Over 80\% of system power is consumed by CPUs and SSDs.
Since the clock is already more than 10\% slower, we assume the per-core software performance is at least same as those measured with our older CPUs (0.5~M reqs/sec/core for Masstree and 0.33~M reqs/sec/core for \sys on small {\sc scan}). 
Therefore, if the storage server is equipped for peaking SSD read performance, Masstree-based system can only reach 28 M reqs/sec with 894~W, and \sys-based system can fully sustain 32M IOPS of SSD random reads with 876~W.
This means \sys accelerator on the old-generation FPGA can already provide 14\% and 17\% improvement on system throughput and efficiency respectively. 
If using new FPGA with PCIe Gen4, \sys accelerator can achieve more throughput, which will further improve the efficiency with lower-TDP CPUs.
When the storage system is bottlenecked by SSD writes, \sys-based system is also better on the efficiency as it already consumes less system power. 
\fi



