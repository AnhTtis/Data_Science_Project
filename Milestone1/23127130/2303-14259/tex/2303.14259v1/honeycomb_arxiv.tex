%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}

% Reference
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}

% Other packages
\usepackage{color}
\usepackage{verbatim}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{url}
\usepackage{xspace}

\usepackage{makecell}

\usepackage{caption}
% \usepackage{subcaption}

\usepackage{subfig}

\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% Abbreviations
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\etal}[1]{{#1} {\em et al.}}

% \newcommand{\sys}{H-BTree\xspace}
\newcommand{\sys}{Honeycomb\xspace}

% authors same contribution footnote


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{\sys: ordered key-value store acceleration on an FPGA-based SmartNIC}

%for single author (just remove % characters)
\author{
{\rm Junyi Liu}\\
Microsoft
\and
{\rm Aleksandar Dragojevi\'{c}\footnotemark[1]}\\
Citadel Securities
\and
{\rm Shane Flemming\footnotemark[1]}\\
AMD
\and
{\rm Antonios Katsarakis\footnotemark[1]}\\
Huawei Research
\and
{\rm Dario Korolija\footnotemark[1]}\\
ETH Zurich
\and
{\rm Igor Zablotchi\footnotemark[1]}\\
Mysten Labs
\and
{\rm Ho-cheung Ng\footnotemark[1]}\\
Imperial College London
\and
{\rm Anuj Kalia}\\
Microsoft
\and
{\rm Miguel Castro}\\
Microsoft
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\date{}
\maketitle

\footnotetext[1]{This work was done when affiliated with Microsoft}

\footnotetext[2]{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\input{abstract.tex}


\section{Introduction} \label{sec:intro}
% Opening

%TODO: change story from pcie bottleneck to off-chip bandwidth bottleneck?
%TODO: log block without reserved space

In-memory key-value stores are an important building block in modern distributed applications~\cite{redis,memcached,tao,azurestorage,a1}. They store data in servers that provide a simple interface ({\sc put(K,V)}, {\sc V=get(K)}, {\sc update(K,V)} and {\sc delete(K)}) to clients across the network. Ordered key-value stores expand the set of supported applications
%~\cite{redis,tao,azurestorage,a1} 
by providing an efficient {\sc scan} operation to retrieve the key-value pairs whose keys are within a specified range.
For example, distributed file systems can use {\sc scan} to map ranges of logical file offsets to the nodes storing the data~\cite{azurestorage}. It is also used to query graph stores~\cite{tao,a1} and the popular Redis~\cite{redis} offers sorted sets.
 We describe \sys, a system that provides hardware acceleration for an in-memory ordered key-value store.


There is a large body of research on improving the performance of ordered key-value stores, e.g.,~\cite{masstree,erpc,cell,farmsosp,drtm,ziegler,learnedcache}. Recent research has shown how to leverage modern networks to achieve high throughput and low latency with an RPC-based system~\cite{erpc}. Other research has explored using one-sided RDMA reads to bypass the server CPU for {\sc get} and {\sc scan} operations~\cite{farmsosp,drtm,cell,ziegler,learnedcache}. Since RDMA NICs only provide simple one-sided reads of contiguous memory regions, these systems require at least two RDMA reads per operation when supporting variable-sized keys or values, and they require client-side caching to avoid additional RDMAs when traversing the tree data structures stored by the servers. 


\sys accelerates an ordered key-value store using an FPGA-based SmartNIC~\cite{Putnam2014,Caulfield2016} attached to a host CPU. 
These SmartNICs are widely deployed in data centers~\cite{Putnam2014,Caulfield2016,firestone2017vfp,firestone2018azure,fowers2018a}. They enable effective CPU offload by avoiding the functionality limitations of RDMA and the performance problems of SmartNICs based on general-purpose, low-power cores~\cite{prism}. Previous work used a similar SmartNIC to accelerate an unordered key-value store~\cite{kv-direct}. \sys solves a harder problem because ordered key-value stores use more complex data structures whose operations perform O(log(n)) memory accesses instead of the O(1) accesses performed by hash table operations in unordered key-value stores.


\sys implements the ordered key-value store using a B-Tree~\cite{btree} that supports variable-sized keys and values that are stored inline in B-Tree nodes to improve the performance of scans. It uses
multiversion-concurrency control (MVCC~\cite{bernstein1987}) to provide
linearizability~\cite{linearizability} for scans. This is important to provide strong consistency, e.g., in the distributed file system example above, clients may fail to read previously written data when scans are not linearizable.

 

\sys stores the B-Tree in host memory. It accelerates {\sc get} and {\sc scan} operations by executing them on the FPGA, but {\sc put}, {\sc update} and {\sc delete} operations are executed by the host CPU. Since most workloads are read-dominated~\cite{memcachedanalysis,tao,bwtree,ycsb,facebookrocksdb},
there is less benefit in accelerating write operations and the cost of accelerating them in hardware is high because they may require complex splitting and merging of B-Tree nodes. This hybrid approach enables much larger stores than would be possible using the limited DRAM attached to the FPGA and it reduces the complexity of the FPGA implementation. However, it brings the challenge of data access and synchronization across the slow PCIe bus. We overcome this challenge with careful co-design of the software running on the CPU and the FPGA hardware image.

\sys uses several techniques to reduce data accesses over PCIe. It caches B-Tree nodes on both FPGA SRAM and on-board DRAM. It also uses large B-Tree nodes with {\em shortcuts}, a list of sorted keys and offsets into the node that divide it into segments of similar size. This allows requests to fetch only the relevant segments of each node they access, which reduces the number of bytes accessed to search for a key and the total amount of memory required to cache interior nodes compared to simply using smaller nodes.

To use available off-chip FPGA bandwidth efficiently, \sys exploits request-level parallelism and avoids head of line blocking with out-of-order request execution. To use all available off-chip bandwidth, \sys also implements a dynamic load balancer that directs some requests that hit on the cache to host memory if there is no bandwidth available to on-board DRAM and there is unused bandwidth over PCIe.


\sys reduces the cost of synchronization across PCIe by making {\sc get} and {\sc scan} operations wait free~\cite{waitfree}.
It also divides each leaf B-Tree node into a {\em sorted block} with sorted key-value pairs and a small {\em log block} that logs recent {\sc put} and {\sc delete} operations. The log block is merged into the sorted block when its size excedes a threshold. This ensures reads traverse mostly sorted items while avoiding 
sorting the node  and synchronizing the CPU and the FPGA on each write. 

\if 0
\sys also introduces the {\em release ring}, a batching technique to reduce communication across PCIe when making new node versions visible to readers with MVCC.
\fi

\if 0
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/server_node_diagram.pdf}
    \caption{Block diagram of \sys B-Tree server node.}
    \label{fig:server_node}
\end{figure}
\fi

A comparison with a state-of-the-art ordered key-value store, which uses eRPC~\cite{erpc} and Masstree~\cite{masstree}, shows that \sys improves both peak performance and cost-performance, which is more important in large-scale data centers~\cite{jouppi2021ten}, significantly for scan-dominated workloads.
For example, in uniform workloads with inserts and short scans, \sys improves the throughput by $1.2\times$ for workloads with 50\% reads and by more than $2.3\times$ for workloads with at least 80\% reads. Most importantly, \sys improves throughput per watt of Termal Design Power (TDP), which is a proxy for total cost of ownership~\cite{jouppi2021ten}, by more than $1.9\times$ for workloads with at least 80\% reads.
We expect these gains to increase with future hardware because newer FPGAs have more on-chip memory to cache B-Tree nodes and they are connected to the host using PCIe Gen5 that has four times higher bandwidth than the PCIe Gen3 used in our system. 


\if 0
\begin{figure}[t]
    \centering
    \includegraphics[trim={0 8.7cm 0 10cm}, clip, width=\columnwidth]{figures/perf_watt_comparison.pdf}
    \caption{\sys B-Tree perf/W comparison of three server platforms.}
    \label{fig:perf_watt_comparison}
\end{figure}
\fi

We describe the architecture of \sys in \Cref{sec:hw_impl}, the algorithms and data structures used to implement the B-Tree in \Cref{sec:btree_design}, and how these are implemented in hardware in Sections \ref{sec:hw_B-Tree} and \ref{sec:hw_mem_subsys}. \Cref{sec:eval} presents the evaluation.


\if 0
With the slowdown of efficiency improvement on server CPUs, data center workloads are increasingly seeking hardware offloads to achieve better performance per watt (perf/W). 
Many recent hardware accelerators are designed for compute-heavy data processing such as machine learning, video encoding, compression, encryption, \textit{etc}. 
Meanwhile, there are continuous effots focused on achieving performant distributed data store systems that can match the performance demands from data processing. 
These software-based solutions mostly benefit from the improving system architectures with more processor cores, larger cache, more high-bandwidth memory channels and RDMA support in NICs. 
The recent hardware acceleration of data stores is mostly aimed at simple abstractions like hash table for key-value stores. 
The limited support of high-level abstractions restricts the choice of hardware acceleration to be considered in distributed data store systems.

% Propose our hybrid solution
In this paper, we demonstrate that high-performance complex data stores like B-Tree can be efficiently implemented with the co-design of software and hardware. 
B-Tree is a commonly used data structure for ordered indexing in many key-value stores, relational databases and file systems.
The design of our proposed data store indexing, \sys B-Tree, is targeted at the workloads that is not write-dominant. 
The read operations of B-Tree are relatively less complex than the write operations.
B-Tree inserts and deletes may require splits and merges of B-Tree nodes, and they are also non-trivial to be implemented with strong consistency and concurrency in software. 
These make B-Tree lookups and range scans better suited to be accelerated in custom hardware. 
When inserts and deletes of B-Tree are infrequent from the workloads, it is not worth to offload these complex operations in hardware.
Even if the read-write ratio is balanced, hardware acceleration will free up a considerable amount of processor cores to the write operations. 
Therefore, \sys B-Tree is a co-designed hybrid solution on a heterogeneous system.
CPU runs B-Tree inserts and deletes, and a hardware accelerator is developed to run lookups and range scans. 
Compared to full software solutions, \sys B-Tree achieves much better perf/W to save total cost of ownership (TCO) while retaining general software programmability. 
\fi


\if 0
% Justify our hybrid solution
\sys B-Tree is prototyped on a server system with x86 CPU, host memory DRAMs, storage SSDs, and a PCIe-attached accelerator card with direct network access. 
As illustrated in \Cref{fig:server_node}, the accelerator handles networking and B-Tree read operations with a small amount of on-board DRAM, which is similar to a SmartNIC. 
The current prototype uses FPGA as the custom hardware platform. 
The implemented accelerator is capable of replacing at least 14 x86 server cores when processing \sys B-Tree read operations over the network (more details in \textcolor{red}{Section Experiments}). 
According to the analysis of FPGA implementation, it only consumes around 35W when running the proposed accelerator. 
\Cref{fig:perf_watt_comparison} provides the esitmated perf/W comparison of three different plarforms running \sys B-Tree. 
The perf/W numbers are calculated with the measured performance of reads and writes, and the power numbers are derived with the specifications of CPU, DRAM and NIC used in the experiments. 
In this comparison, we also plot the projected perf/W of a reasonably assumed system-on-chip (SoC) platform, which represents our ideal target platform. 
This SoC contains same amount of processor cores with $20\%$ less power consumption, and the accelerator is hardened as an integrated SoC component with 4W estimated power consumption.
Although FPGA is less power efficient than ASIC, using single-socket CPU and FPGA accelerator can already provide up to $42\%$ more perf/W than standard dual-socket CPU.
In particular, the gain on performance efficiency starts to grow from the workloads with $30\%$ reads. 
On the SoC platform, perf/W is further improved across all read-write ratios and can even be doubled on read-dominant workloads. 

The high performance efficiency of \sys B-Tree is not realized by just leveraging hardware offloads out of the box. 
It is the outcome of carefully co-designing both software and hardware components. 
The cost of synchronization between software (writes) and hardware (reads) is minimized, which guarantees that the hardware will not be blocked by the slower software. 
Otherwise, frequent synchronization over PCIe will become the bottleneck of the entire processing system. 
The data struture design of \sys B-Tree also has balanced optimizations on both software and hardware. 
When appling hardware-friendly data layout or traversing method, we also make sure the software performance is not suffered from extra complexity. 
Traversing B-Tree requires accessing the memory multiple times and also randomly. 
The design of \sys B-Tree and its hardware memory subsytem aim to use memory bandwidth and IOs efficiently. 
The amount of memory acccesses per traversing is reduced, and we avoid reading a large chunk of data that only contains a small piece of useful information. 
The hardware implementation also adopts a highly scalable architecture, so that the hardware resources and power can scale with the required processing performance. 
\fi


\section{\sys architecture}\label{sec:hw_impl}

\Cref{fig:hardware_sys} shows the architecture of \sys: an FPGA-based SmartNIC that connects a host server to the datacenter network.
The \sys accelerator is built on the Catapult FPGA infrastructure \cite{Putnam2014,Caulfield2016}, which provides a shell with IO support. The core FPGA image has three subsystems: the B-Tree accelerator, networking, and memory. 

The B-Tree is stored in host DRAM because it scales to much larger capacities than on-board DRAM in FPGAs. The B-Tree accelerator implements {\sc get} and {\sc scan} operations and {\sc put} and {\sc delete} operations are executed by the CPU. 

% network
The networking subsystem implements LTL~\cite{Caulfield2016}, a UDP-based transport protocol similar to RoCE v2~\cite{zhu2015congestion} on top of 50~Gbps Ethernet, and an RDMA engine to enable efficient host-to-FPGA communication. The RDMA engine supports custom commands for {\sc get} and {\sc scan} in addition to
general commands like {\sc read}, {\sc write}, {\sc send}, and {\sc receive}. These enable kernel bypass at the clients and at the servers for {\sc put}, {\sc update} and {\sc delete}. They also enable the B-Tree accelerator to completely bypass the CPU for {\sc get} and {\sc scan} requests by receiving these commands directly from the network subsystem, processing them, and sending back replies. 

% external memory
The memory subsystem interfaces with host memory over PCIe and with on-board DRAM. It maintains a cache and a page table. The cache stores interior B-Tree nodes in on-board DRAM and the root of the B-Tree in on-chip SRAM. The page table simplifies atomic updates to the B-Tree (\Cref{sec:insert}) by adding indirection~\cite{bwtree}. It is stored in on-board DRAM and maps logical identifiers (LIDs) for B-Tree nodes to their physical addresses in host memory. 

The FPGA is connected over two PCIe Gen 3 $\times8$ to the host. We measured a peak throughput of 13 GB/s and a latency of more than 1~$\mu$s (depending on load). For comparison, the bandwidth between host CPU and DRAM is up to 64 GB/s with an order of magnitude lower latency, and CPU cores have large caches that improve performance further. Therefore, \sys must address the challenge of data access and synchronization across the PCIe bus to achieve performance and cost-performance gains relative to CPU-only systems. The next sections describe how we overcame this challenge with careful hardware-software co-design.


%TODO: add page table in host to figure and say host implements put and delete


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/hardware_system_overview}
    \caption{Hardware system architecture.}
    \label{fig:hardware_sys}
\end{figure}



\section{B-Tree}\label{sec:btree_design}
\sys implements a B+ tree~\cite{btree}, i.e., key-value pairs (also called items) are stored only in leaf nodes while interior nodes store keys and pointers to child nodes. 
%Each leaf stores pointers to its siblings to improve the performance of scans. 
\sys guarantees linearizability~\cite{linearizability} for all operations including scans.
% This section describes the design of the \sys B-Tree. 

 \if 0
Its implementation employs fine-grained synchronization with no locking by read operations. 
The B-Tree guarantees linearizability \footnote{Each operation appears to take place atomically at a single, indivisible point in time, even if operations execute concurrently. } for all operations.
It starts from B-Tree node layout optimized for both software and hardware implementation. 
Then we discuss how the synchronization is optimized to be light-weight and the implementation details of B-Tree operations.
\fi

\subsection{Node layout} \label{sec:node_layout}
% LID and page table
B-Tree nodes are fixed-size, which is configured to 8~KB in this paper.
They are allocated in pinned host memory to allow 
the FPGA to use physical memory addresses directly.
B-Tree nodes do not store addresses of child or sibling nodes directly. 
Instead, they store 6-byte logical identifiers (LIDs)~\cite{bwtree}.  
The mapping from a LID to the virtual and physical addresses of a node is maintained in a page table. 
% With 8-byte table entries stored on 4-GB on-board DRAM, we support trees up to 4 TB, which is large enough for in-memory stores on modern servers. 

\if 0
, allowing us to store nodes on larger storge devices such as NVMe drives in the future. 
It also helps with synchronizing accesses to the tree, as write operations perform copy-on-write of nodes in some cases (more details in \textcolor{red}{\Cref{sec:insert}}). 
\fi



% Node header and item
The node layout is shown in \Cref{fig:node_layout}. 
The node header is stored in the first 48 bytes. 
It contains fields specifying the node type (interior or leaf), the number of bytes used, a lock word, and a version number to synchronize accesses to the node. The header of an interior node stores the LID of the leftmost child. 
The header of a leaf node stores the LIDs of the siblings to improve scan performance. Leaf nodes store keys and values in variable-size blobs. 
Each blob has a 2-byte header that specifies its size. 
Keys and values are stored inline in B-Tree nodes to improve performance.
In the current configuration, the maximum key length is 460~bytes and values larger than 469~bytes are stored outside the node.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\columnwidth]{figures/node_layout.pdf}
    \caption{\sys node layout.}
    \label{fig:node_layout}
\end{figure}

% Intro large and small block
Leaf nodes are split in two blocks: sorted and log. 
The pointer to the boundary between them is stored in the node’s header. 
%The pointer is an offset relative to the start of the large block. 
The sorted block stores items in ascending lexicographic order of keys and the log block stores a log of recent changes. 
When the log block grows larger than a threshold (currently set to 512 bytes), it gets merged with the sorted block. 
Therefore, read operations benefit from accessing mostly sorted data and write operations avoid the overhead of sorting the node on every change. It also reduces synchronization across PCIe because it requires one update to the FPGA page table per-merge instead of one on every write operation. 

Since interior nodes change infrequently, the cost of storing a log block and accessing it on reads outweighs the benefits. Therefore, interior nodes do not have log blocks. 



% Small block
Entries in the log block are either newly inserted or updated items or delete markers. 
Each entry stores a 2-byte pointer to an item in the sorted block. 
For a newly inserted item, it points to the first item with key greater than the item’s key. For other entries, it points to the deleted or updated item.
In addition, each entry records the 1-byte index in the sorted order of log items at the time of insertion. 
These fields are used to sort all node items, when searching the leaf, without comparing keys and with O(1) cost per log block item (\Cref{sec:small_block_sort}).

\if 0
These indexes are “replayed” as shifting and storing elements in a small indirection array, while searching the small block to rebuild the current sorted order (\textcolor{red}{more details in \Cref{sec:small_block}}). 
This sorting is resource-efficient in hardware and does not introduce significant latency. 
\fi

% Shortcut block

To optimize the search for a key in a node, we split the sorted block into segments of roughly equal size. 
We store the keys at the boundaries of segments and the pointers to the boundaries in the shortcut block, which is stored immediately after the node header. We currently reserve 48~bytes for the header and 464~bytes for the shortcut block. The shortcut keys are selected during the merge of the sorted and log blocks. They are stored only in the shortcut block, i.e., the start of the segment for boundary key K contains the value associated with K rather than another copy of K followed by the value.

The search for a key starts by traversing the shortcut block to identify the segment that contains the key. 
The search examines only that segment and the log block, which reduces the amount of data read both from host memory and the cache.
Since \sys's bottleneck is FPGA off-chip bandwidth, this optimization significantly improves performance. 
For small keys with the current configuration, a search reads at most 1.5~KB of data from an 8~KB node, which is a $5\times$ performance improvement relative to reading the whole node.

Analysis shows that using large nodes with shortcuts reduces the number of bytes accessed to search for a key and the total amount of memory required to cache interior nodes compared to simply using smaller nodes. For the example of a five-level tree with 8~KB nodes, 16-byte keys and values at 55\% occupancy, searching the tree with this optimization accesses fewer than 75\% of the bytes accessed searching a simple tree with 512-byte nodes with a similar number of items. It also requires approximately four times less space to cache all the interior nodes.
The intuition is that the FPGA can use the information in the header to fetch a sorted block segment or a log block with a single DMA that reads only the bytes in use, whereas it would need to read the whole node or issue more dependent DMAs for the simple tree. Also using smaller nodes increases overhead because it requires more storage for pointers to children in interior nodes. Using log blocks reduces the impact of sorting the large node on writes. 


\subsection{Synchronization}\label{sec:sync}

{\sc get} and {\sc scan} operations executed by the accelerator are wait-free~\cite{waitfree} --
they never block or retry due to {\sc put} or {\sc delete} operations executed by the CPU.
This reduces the overhead of synchronization over PCIe and provides predictable latency.

% MVCC with two global versions
We use multi-version concurrency control (MVCC)~\cite{bernstein1987}
to ensure that read operations can always access a consistent snapshot of data. 
\sys maintains two shared 64-bit version numbers: the {\em global write version} and the {\em global read version}.
A write operation performs an atomic fetch and add on the global write version to obtain its write version, which is used to version any items it creates.
Writers {\em release changes} to readers in version order: a writer sets the global read version to its write version when it becomes the writer with the smallest write version, and updates the copy of the global read version maintained by the accelerator over PCIe. We delay responses to write requests until this update completes.
Writers synchronize with each other using node locks, which are ignored by readers. 
Read operations read the global read version maintained by the accelerator to get their read version.
They ignore items with versions greater than this value. 



\if 0
By using two global version numbers, read operations always access items for which the commit is complete, without having to wait for concurrent operations. 
This means that reads are wait-free, and in our system, the hardware readers will not need to wait for software writers. 
We opt to use two global versions at the cost of ordering the release of write operations. 
This overhead is reduced by using a release ring data structure that allows efficiently determining the release order \textcolor{red}{(more details in \Cref{sec:rel_ring})}. 
\fi

% Node and item versioning
All items in the sorted block have the same version that is stored in the node header and is called the \textit{node version}. 
The node version is the write version of the write operation that created the version of the node. Nodes also store a pointer to the state of the node as of the previous version. A reader that observes a node version greater than its read version follows the chain of old version pointers until it reaches a node with a version less than or equal to its read version. The version of an item in the log block is stored in the item header.
To reduce the size of these headers, we store a 5-byte version delta from the node version instead the entire 8-byte version. 
If a write would cause the delta version to wrap around, it merges the sorted and log blocks. 
Readers ignore all items in the log block with versions greater than their read version. 

% Node garbage collection, garbage collection of old versions
To ensure operations observe a consistent snapshot, \sys does not reclaim old versions of nodes while at least one inflight operation can access a snapshot to which the old version belongs. To garbage collect old versions that are not accessible anymore, \sys uses an epoch-based memory manager~\cite{mckenney2007rcu}. Each CPU thread executes operations one at a time, assigns a sequence number to each inflight operation, and exposes its current operation sequence number to the memory manager. Similarly, the B-Tree accelerator assigns a sequence number to each operation and exposes the sequence numbers of the newest and the oldest inflight operation to the memory manager over PCIe. When a CPU thread makes a change that removes old versions of nodes from the tree, it puts the old versions into a garbage collection list tagged with a vector timestamp. This vector timestamp has entries for the current operation sequence numbers of all CPU threads and the newest operation sequence number on the accelerator. 

Periodically, a thread scans the garbage collection list and reclaims all node versions that are not reachable by CPU threads or the accelerator, i.e., node versions with a vector timestamp where the entry for each CPU thread is smaller than the current operation sequence number for the thread, and the entry for the accelerator is smaller than the current oldest inflight operation on the accelerator. If a writer fails to allocate memory for a new node and the garbage collection list is not empty, the operation is aborted and retried.
 
The page table is stored both in host DRAM and in FPGA on-board DRAM. 
% (but the copy in the FPGA only stores physical addresses).
When a new node mapping is created or when a node’s mapping is changed, the CPU updates the table in host memory and issues a PCIe command to the FPGA to update the copy of the table in FPGA on-board memory. 

\sys can be configured with MVCC off by using zero as the version number for all nodes and log items, and by not setting old version pointers. This improves write performance a little by avoiding updates to the global read version on the FPGA.
The atomic tree updates using  the page table still ensure linearizability for {\sc get}, {\sc put}, and {\sc delete}, but not for {\sc scan} because sibling pointers are not updated atomically. 

\if 0
 Many ordered key-value stores offer only this weaker guarantee (e.g.,~\cite{masstree}), which is sufficient for many applications. 
\fi

\subsection{Scan and get} \label{sec:lookup} \label{sec:scan}

\sys implements {\sc scan(K$_l$, K$_u$)} that finds the largest key {\sc K$_s$} less than or equal to {\sc K$_l$}, and returns a sorted list with all key value pairs with keys greater than or equal to {\sc K$_s$} and less than or equal to {\sc K$_u$}. We implement this version of {\sc scan} to support mapping from a range of logical file offsets to the nodes storing the data in distributed file systems~\cite{azurestorage}. In this application, the key-value pairs represent variable-sized chunks of file data where the key is the logical offset of the first byte in the chunk and the value points to the servers storing the data. Since the start of a logical file offset range in a client request can be in the middle of a chunk, the  {\sc scan} semantics we implement are necessary for correctness.

{\sc scan(K$_l$, K$_u$)} traverses the tree from the root to a leaf. It fetches the node header and shortcut block of each node it visits (currently stored in the first 512 bytes). It follows old version pointers if needed to find a node with version less than or equal to the operation's read version. Then it searches for the largest shortcut key less than or equal to {\sc $K_l$}, and fetches the associated sorted block segment. When {\sc scan} visits an interior node, it searches the segment for the item with the largest key less than or equal to {\sc $K_l$} (which could be the shortcut key), and obtains the LID of the next node to visit from the item.

When {\sc scan} reaches the leaf, it  fetches the log block in parallel with searching the shortcuts. Then it searches both the sorted and log blocks for the largest key {\sc K$_s$} less than or equal to {\sc K$_l$}. It scans forward from {\sc K$_s$} until it reaches a key greater than {\sc K$_u$} or the end of the tree. It ignores any items with versions greater than the read version. If it finds one or more items with the same key, it returns the value associated with the item with the latest version unless that item is a delete. Otherwise, it ignores the item. {\sc scan} uses sibling pointers to move to the next leaf if needed.

The accelerator optimizes scan performance by determining a sorted order for the items in the log block of a leaf, and uses the back pointers from log block items to sorted block items to determine a sorted order across all the items in the leaf (see~\Cref{sec:small_block_sort}). This enables efficient scanning by overlapping data access with key comparisons, generating results that are already sorted, and avoiding key comparisons with all items in the log block.

\sys implements {\sc get(K)} as {\sc scan(K,K)} and post-processes the result to return the value associated with {\sc K}, or return not found if  {\sc K} is not in the result.

\if 0
XXX don't think we need this either.
To keep the items sorted, the scan of a leaf merges the items in the range from the three node blocks.
 
While scanning, the ordering between the large and the small block is preserved by the back pointer in the small-block item. 
If the next small-block item points to the next large-block item, the small-block item is returned and the scan moves to the next small-block item; otherwise, the large-block item is returned, and the scan moves on to the next large-block item. 
To handle shortcut keys correctly, the scan keeps track of the end of the current large-block segment. 
At the beginning of each segment, the scan retrieves the key from the shortcut block and the value from the large block. 
% In the middle of a segment, both the key and the value are in place. 
\fi
\if 0
XXXX from the old lookup; I don't think we need this.
Items in the small block with a version newer than the lookup’s read version are ignored. 
The lookup follows the pointer stored in the item with the larger key that found in the small and the large block. 
Making this decision does not require comparing the keys – if the back pointer of the small block item points just after the large block item, it follows the pointer in the small block; otherwise, it follows the pointer in the large block. 
If the target key is smaller than both the first key in the large block and in the small block, the lookup follows the leftmost pointer. 
XXXX
\fi

\subsection{Insert} \label{sec:insert}

A {\sc put(K,V)} operation inserts a new item if there is no item with key {\sc K} in the tree. It traverses the tree as described for {\sc scan(K,K)} without acquiring any locks, but it reads the latest version of each node to implement linearizability. The operation locks the leaf node before modifying it to ensure the state matches the one observed during the traversal.

The node header has a 32-bit lock word with a lock bit and a 31-bit sequence number, which is incremented on writes to the node.
{\sc put} tries to acquire the leaf's lock using an atomic compare-and-swap instruction that checks if the sequence number matches the one observed during the traversal. If the compare-and-swap fails, the {\sc put} operation restarts. 

{\bf Fast-path insert:}
In the common case, the operation simply adds a new item to the log block: it copies the item to the log block; acquires the write version with a fetch-and-add on the global write version; sets the version of the item in the log block; updates the node size; increments the node sequence number; and unlocks the node. The node size and the lock word are packed into a 64-bit word. So that the last three steps can be performed with a single instruction. Concurrent operations either observe the node without the item or with the item correctly written, and readers ignore the item until the change is released (\Cref{sec:sync}).

 
%They also ignore the item before its write version is released, as the read version is older than the item. 
% Note that FPGA does not cache leaf nodes. If it did, the writer would have to invalidate the cache by issuing commands over PCIe, which would introduce additional overheads to the common case insert.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/large_small_merge.pdf}
    \caption{Merging the sorted and log blocks.}
    \label{fig:large_small_merge}
\end{figure}


{\bf Sorted and log block merge:}
If the size of the log block exceeds a threshold (currently 512 bytes),
the operation merges the sorted and log blocks as shown in \Cref{fig:large_small_merge}. 
It allocates a new memory buffer for the node and sorts all the items into a single sorted block in the new buffer.
While sorting, it selects the shortcut keys.
For each item, it decides whether to put the key in the shortcut block based on the number of bytes copied so far, the remaining bytes to copy, and the average size of keys and values. It attempts 
to maximize the number of shortcut keys while ensuring that segments have similar sizes, a minimum size (currently 256 bytes), and that 
shortcut keys fit in the shortcut block.

The operation sets the node version to the write version, and the old version pointer to the address of the old buffer (\Cref{fig:large_small_merge}b). Then it updates the LID mapping with the address of the new buffer in the page tables of both CPU and FPGA (\Cref{fig:large_small_merge}c). The parent node does not need to be changed because the LID of the child remains the same. The operation also puts the old buffer in the garbage collection list and releases the changes to make them visible to readers.
This ensures concurrent operations observe the change atomically.

 
\if
, unlocks it, and sets the “deleted” flag in its header to ensure that concurrent writers do not update the old buffer. 
Ongoing operations can still read from the old buffer, but if an operation needs to update a deleted node, it retries. 
On retry, it will use a new read version and the new node mapping.
\fi

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/node_split.pdf}
    \caption{Splitting a node during insert. 
    %$N_{swap}$ is a newly allocated node buffer. $N_{L}$ and $N_{R}$ are new nodes with a new LID and a memory buffer.
    }
    \label{fig:node_split}
\end{figure}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/btree_hw_design}
%     \caption{B-Tree accelerator architecture.}
%     \label{fig:btree_hw_design}
% \end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{figures/ksu_design}
%     \caption{Key seach unit}
%     \label{fig:ksu_design}
% \end{figure}

\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.79\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/btree_hw_design}
        \caption{B-Tree accelerator architecture.}
        \label{fig:btree_hw_design}
    \end{minipage}
    % \hfill
    % \hspace{3pt}
    \begin{minipage}[b]{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ksu_design}
        \caption{Key seach unit}
        \label{fig:ksu_design}
    \end{minipage}
    \vspace{-6pt}
\end{figure*}


{\bf Node splitting:}
If there is not enough space in the leaf for the merged block, the operation splits the leaf in two. This requires inserting a new item in the parent node as shown in \Cref{fig:node_split}. If the parent is full, the operation also splits the parent. Splitting can propagate to the root. The last interior node that is updated but not split is called \emph{the root of the split}. 
%The operation creates a new subtree with all the splits and replaces the old subtree atomically.

The operation acquires  locks on all interior nodes to split and on the root of the split. If locking fails due to a sequence number mismatch, it 
releases all acquired locks and restarts. The operation also allocates two new nodes (new LID and memory buffer) for each node that it splits and a memory buffer (same LID) for the root of the split ($N_L$, $N_R$, and $N_{swap}$ in \Cref{fig:node_split}b). Then it processes nodes from the leaf up splitting the items in each node evenly into the two newly allocated nodes. When it gets to the root of the split, the operation copies the items into the new buffer and adds the LIDs of the new child nodes and the key at the boundary between them. 
%If memory allocation fails because system is out of memory, the writer gives up and returns the appropriate status code to the caller.

To swap in the new subtree atomically, the operation updates the LID mapping for the root of the split in both CPU and FPGA page tables to point to the new buffer (\Cref{fig:node_split}c). It also locks the sibling leaves and updates their sibling pointers to point to the new leaves. This is sufficient to ensure linearizability for all operations but scans, because the updates to the sibling leaves and the subtree swap are not atomic. 

Linearizable scans are important for applications like the distributed file system~\cite{azurestorage}. Without them, a client $C$ could write data $D$ to a file at a logical offset range $R$, and a later read from a range containing $R$ could fail to return $D$. This can happen if the B-Tree node with the mapping for the servers that store $D$ is split and the sibling pointers are not updated before the scan to find the servers storing the data being read.

To provide linearizable scans, the operation sets the versions in all newly allocated buffers to its write version, and sets the old version pointer in the new root of the split to the address of the old buffer (\Cref{fig:node_split}b). This ensures that operations with older read versions traverse the old subtree. It also sets the old version pointers of the new leaves to the address of the old leaf. This ensures that scans with older read versions, which may reach the new leaves using sibling pointers, traverse the old leaf. The operation also puts the LIDs and memory buffers of all nodes that got split, and the old memory buffer of the root of the split in the garbage collection list. The operation then unlocks all nodes and releases the changes to readers (\Cref{sec:sync}). This ensures readers observe all changes made during the split, or none of them. 
 
If the tree root must be split, the operation grows the tree by allocating a LID and a memory buffer for the new root, and updating the root and tree height in the accelerator.
%, which is equivalent to swapping in a new subtree. 


\subsection{Delete and update}

\if 0
The implementations of {\sc delete} and insert are similar. {\sc delete} appends a delete entry to the log block, which points to the item being deleted in the sorted block to indicate that readers should ignore the item. The space for deleted items is reclaimed when the sorted and log blocks are merged. Nodes whose occupancy drops below a threshold are merged and nodes that become empty are deleted. We use similar techniques to ensure atomicity as for splits but omit the details.

A {\sc put(K,V)} operation updates the item with key {\sc K} if it is already in the tree. We implement update as a combination of delete and insert. It appends a delete entry for the old item and an insert entry for the new item to the log block atomically. 
\fi

The implementations of {\sc update} and {\sc delete} are similar to {\sc insert}.
{\sc update} appends the updated key-value pair to the log block, which points to the previous version of the item in the sorted or log block. For {\sc delete}, a delete marker is appended in the same way to indicate that readers should ignore the deleted item. The space for stale and deleted items is reclaimed when merging the sorted and log blocks. Nodes whose occupancy drops below a threshold are merged and nodes that become empty are deleted. We use similar techniques to ensure atomicity as for splits but omit the details.



\if 0
\subsection{Release ring}\label{sec:rel_ring}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/release_ring.pdf}
    \caption{Release ring with eight slots.}
    \label{fig:rel_ring}
\end{figure}

To ensure linearizability with MVCC, writers must make their changes visible to readers in version order. In a simple implementation, a write operation sets the global read version to its write version when it becomes the operation with the smallest write version, and updates the copy of the global read version in the accelerator over PCIe. We introduce a lock-free {\em release ring} to batch many updates on a single PCIe command to improve performance while preserving consistency.

The release ring is a circular buffer with a power-of-two number of slots $R$ (currently configured to 128). The least significant bits of an operation's write version are used to map it to a slot. Each slot contains a 64-bit word with a 1-bit state and the most significant bits of the write version (the remaining bits are determined by the slot index). A slot can be in one of two states: the initial state \emph{in-progress} and \emph{completed}. Slots are padded to the cache line size to avoid false sharing. \Cref{fig:rel_ring} shows how the release ring is used for batching.

To release its changes, an operation with write version $V_{w}$ ensures that its slot is not in use by the previous owner (i.e., the operation with version $V_{w}-R$) by spinning on the global read version until it is greater than or equal to $V_{w}-R$. Then it stores its version in the slot and sets the state to \emph{completed} to announce that it is ready to release its changes. Next, the operation spins on the global read version again to determine if it became the \emph{releaser} or if its changes have already been released. If the global read version becomes equal to $V_{w}-1$, the operation becomes the releaser. If it becomes greater than or equal to $V_{w}$, the operation has already been released. 

When an operation is the releaser, it scans the ring forward starting from its slot until it finds a slot marked \emph{in-progress}. It adds all \emph{completed} operations it scanned to the batch and resets their state to \emph{in-progress}. It releases the batch by 
updating the copy of the global read version in the accelerator to the write version of the last operation in the batch and then 
updating the global read version in the CPU.

\if 0
Our release ring supports early return for operations that are guaranteed to be released by a preceding operation. 
To check if it can safely return, the operation checks the state of the preceding slot, just before spinning on the global version. 
If the preceding operation has not started to release the batch, which is the case if the preceding slot is in \emph{completed} state or its version is less than $V_{w}$ - 1, the operation can return without spinning on the global read version. 
It is guaranteed that the releaser will check its slot and release it eventually.
Early return can improve concurrency in the system as it reduces waiting, but it can weaken consistency guarantees. 
For instance, if a user thread inserts a key into the tree and the insert returns early before the operation has been released, a subsequent lookup for the key might not find it in the tree, resulting in non-intuitive behaviour. 
Therefore, early return will be used to improve writer performance when such weaker consistency is acceptable. 
\fi
\fi


\section{B-Tree accelerator implementation}\label{sec:hw_B-Tree}

% accelerator overview
The B-Tree accelerator has three components (\Cref{fig:btree_hw_design}): request management, interior-node search engine, and leaf-node scan engine. Request management parses requests and assigns them sequence numbers and read versions. The interior-node search engine is responsible for traversing the tree from the root to a leaf.  The leaf-node scan engine traverses the leaf or, for {\sc scan} operations, leaves using sibling pointers.


\subsection{Request management}
% out-of-order request execution
The time to complete requests is variable, e.g., due to different size keys or cache misses. Therefore, the B-Tree accelerator
exploits request-level parallelism and avoids head of line blocking by supporting out-of-order request execution. This 
maximizes the utilization of compute resources, and on-board DRAM and PCIe bandwidth.

% Key buffers
The request pre-process module extracts the lower (LB) and upper bound (UB) keys from the request and stores them in centralized key buffers. These buffers support multiple read ports to enable the interior-node search engine and the leaf-node scan engine to read multiple keys in parallel. When a request completes, its key buffers are freed. 


The request pre-process module also initializes {\em request metadata} that includes: identifiers of the buffers holding the keys, the LID of the node being traversed, the node's level, the block type being traversed (shortcut, sorted, or log), and the start offset of the block/segment within the node. The request metadata flows over a wide data path through the accelerator.

Request management is also responsible for storing the accelerator's copy of the 64-bit global read version, which is updated 
by the CPU over PCIe (\Cref{sec:sync}). It reads the global read version to assign a read version to each incoming request. Requests are also assigned a 64-bit sequence number by incrementing a counter $S_{new}$ maintained by the epoch manager.
These are part of the request metadata.

The epoch manager also monitors request completion to keep track of $S_{old}$, the sequence number of the oldest inflight request. It exposes $S_{old}$ and $S_{new}$ over PCIe to the memory manager running on the CPU to enable reclaiming memory when it is no longer accessible by inflight requests (\Cref{sec:sync}). Accesses to $S_{old}$ and $S_{new}$ over PCIe are infrequent.



% Epoch control
%If $(S_{new} - S_{old})$ is greater than the epoch length, no new requests will be accepted by the accelerator. 

% % sync with software
% When the software makes any changes on the tree, it can record $S_{new}$ from the hardware to be $S_{change}$. 
% Once the software sees $S_{old} \geq S_{change}$, this means that all the old requests potentially accessing the staled tree nodes have been finished. 
% In other words, B-Tree accelerator can only reach the tree nodes with the new changes. 
% From this point of time, the staled tree nodes will be freed safely to recycle their memory space.

\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/small_block_insertion.pdf}
        \caption{Example of order hints in the log block.}
        \label{fig:small_block_insertion}
    \end{minipage}
    % \hfill
    % \hspace{3pt}
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/small_block_sort.pdf}
        \caption{Sorting the log indirection array with order hints.}
        \label{fig:small_block_sort}
    \end{minipage}
    \vspace{-6pt}
\end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/small_block_insertion.pdf}
%     \caption{Example of order hints in the log block.}
%     \label{fig:small_block_insertion}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/small_block_sort.pdf}
%     \caption{Sorting the log indirection array with order hints.}
%     \label{fig:small_block_sort}
% \end{figure}

\subsection{Interior-node search engine}

% overview
The interior-node search engine is responsible for traversing interior nodes from the root as described in \Cref{sec:lookup}. It uses a ring architecture to search node blocks iteratively while overlapping memory reads with compute (\Cref{fig:btree_hw_design}). The architecture exploits request-level parallelism by using multiple memory subsystem interface (MSI) adapters  and key search units (KSU). Additionally, each MSI adapter supports out-of-order data transfer to fully utilize off-chip bandwidth.

For each interior node visited by a {\sc get} or {\sc scan} request, the memory access generator first generates a memory access request to one of the MSI adapters to fetch the node's header and shortcut block. The adapter issues the request to the memory subsystem, waits for the response, and sends the response to one of the KSU. 
The KSU first checks if the version in the node header is greater than the request's read version. If so, it sends the updated request metadata back to the memory access generator to visit the old version of the node. If not, the KSU searches the shortcuts for the sorted block segment to fetch. 
Then sends the updated metadata back to the  memory access generator that generates a memory access request to one of the MSI adapters to fetch the sorted block segment. When the adapter gets the segment, it is sent to one of the KSU. The KSU searches the segment for the LID of the next node to visit. If the next node is a leaf, the request is passed on to the leaf-node scan engine. Otherwise, the request is passed to the memory access generator to continue with the child.


% search request generator and scan request generator are confusing names because they do not geneate k-v requests. They generate %requests to fetch blocks.


% KSU
The architecture of a KSU is shown in \Cref{fig:ksu_design}. It uses the request input metadata to fetch the request's lower bound key.
Then it processes the block data (header and shortcut, or sorted block segment) to find the largest key that is smaller than or equal to the request key. Since keys are variable size, KSU have to process keys sequentially. They process keys in a streaming fashion overlapping data transfer with key comparisons. The key comparison pipeline implements comparison logic equivalent to the C++ \code{memcmp()} function. It compares key fragments stored in registers with configurable width (currently 16~bytes). Key alignment uses barrel shifters to stream keys into the comparison pipeline. The result generator outputs updated request metadata.


%When a key greater than the request key is found, the result generator takes the value from the previous KV pair to determine the next %traverse access. 
%In case that no greater key is found, the result is generated with the last KV pair from the block data. 

\if 0
% TODO: Move this to an evaluation section on how we configure the accelerator. It should highlight modularity and the flexbility to
% trade off power area for throughput. Plus explain that how we configure MSIs based on the PCI bottleneck, how we configure KSUs 
% and RSUs based on the number of MSIs, and how we balance MSIs across interior-node search engine and leaf-node scan engine based on the tree % height.
% sizing the number of KSUs
The processing latency of the block data is bounded in KSU. 
Given a B-Tree node with $K$-byte keys in $P$-byte KV pairs, we will typically have at most $512/P$ keys in the block data. 
Each key takes $K/W + 1$ cycles to be streamed for comparison, where $W$ represents the comparison byte width. 
There are additional $Q$ cycles for initiating the processing and generating the result.
$Q$ is fixed and below 10 cycles in our implementation. % typically between 2 and 10
The average processing latency of KSU on the sorted layout is 
\begin{equation*}
    L_{search} = \frac{512}{2 \times P} \times (\frac{K}{W} + 1) + Q
\end{equation*}
, which is equivalent to hitting a key in the middle of the block data.
When processing the unsorted layout, KSU will take extra $512/P$ cycles to go over all KV pairs at the beginning. 
Given a B-Tree with $N_{inter}$ interior levels and a target request throughput $T_{req}$, we will have:
\begin{equation*}
    N_{KSU} = N_{inter} \times T_{req} \times L_{search}
\end{equation*}
, to calculate the number of needed KSUs, $N_{KSU}$, for each type of the block data.
$N_{inter} \times T_{req}$ is also equivalent to the target throughput of reading the corresponding block data from the memory subsystem. 
In general, the total number of KSUs is the sum of $N_{KSU}$ calculated for three block types.
\fi

\subsection{Leaf-node scan engine} \label{sec:rsu}
% overview
The leaf-node scan engine is responsible for traversing leaf nodes as described in \Cref{sec:lookup}. As shown in \Cref{fig:btree_hw_design}, it also uses a ring architecture to scan leaf node blocks iteratively, and exploits request-level parallelism by using multiple MSI adapters and range scan units (RSU). 

Unlike the key search array, the range scan array includes several buffers: {\em result buffer,  SC buffer, S buffer}, and {\em L buffer}. 
The result buffer is used to accumulate partial results because the leaf-node scan engine produces a sorted list of key-value pairs spread across different blocks in one or more leaves. The SC buffer, S buffer, and L buffer are used to buffer data from shortcut, sorted, and log blocks in a leaf, respectively. This enables iteration over keys in the three blocks in a sorted order to generate sorted results. The range scan array also includes different RSU variants to scan each type of block and finite state machines (FSM) that coordinate scans across the three RSU variants. The buffers are separate from RSUs to enable overlapping of compute with data buffering. The buffers are divided into request slots that also include the FSM state.
The memory access generator maintains a list of available slots. It assigns slots to requests or pushes back if no slot available. 



\if 0
%TODO: Move this to the configuration part in evaluation
 
The number of buffer slots, $N_{slot}$, is equal to $T_{req} \times L_{range}$, where $L_{range}$ represents the latency of finishing one range scan on leaf nodes. 
In our implementation, $N_{slot}$ can be less than 64, and the total on-chip memory used for scan buffers is below 256 KB. 

The number of each RSU variants, $N_{RSU}$, is equal to $T_{req} \times L_{scan}$, where $L_{scan}$ is the latency of scanning one block data. 
In particular, the RSU designed for large block scanning consumes more cycles and logic resources to merge all key-value pairs detected in the corresponding leaf node. 
\fi

% RSU
The architecture of the RSU is similar to the KSU (\Cref{fig:ksu_design}) but it has two parallel key comparison pipelines to compare keys with both the lower and upper bound keys in the {\sc scan} operation. Additionally, the RSU variant responsible for log block processing must first sort the log block to enable iterating over the key-value pairs in order.

\label{sec:small_block_sort}

We leverage hardware-software co-design to optimize log block sorting.
Sorting does not compare keys and costs only $O(1)$ cycles per item. 
It uses 1-byte order hints stored in log block items by inserts. 
\Cref{fig:small_block_insertion} illustrates an example sequence of insertions into the log block. 
For clarity, the number in the item represents its key. 
The number above the item represents its order hint. 
The log block is initially empty. Item 90 is inserted first and is assigned order hint 0. 
Next, item 60 is inserted. 
Since it is the smallest item in the log block, it is assigned order hint 0. 
The order hints in existing items are not changed. 
Then, item 30 is inserted with order hint 0 since it is the smallest in the log block again. 
The final item is 45 and its order hint is 1. 

% The indexes are “replayed” while searching the small block to rebuild the current sorted order. 
% The established order is stored in an small indirection array with low memory overhead in both software and hardware.


Sorting creates an indirection array with offsets of items in the log block in ascending key order.
\Cref{fig:small_block_sort} illustrates sorting of the log block in \Cref{fig:small_block_insertion}. It shows keys instead of item offsets for clarity.
Items are processed in the order in which they are stored. When the item with order hint $i$ is processed, its offset in the log block is inserted into position $i$ in the indirection array and all the elements at positions $j \geq i$ are shifted to the right.
Since the indirection array is stored in a large shift register, insertion costs one cycle and can be overlapped with fetching the next item.
Any items with version greater than the operation's read version are filtered out.
After sorting, the RSU uses the indirection array to stream keys to the key comparison pipelines in order.



\if 0
If the upper-bound key is not found, range scan array will start a new scan on the sibling leaf node, and the current partial results will be forwarded to the result buffer. 
\fi

\section{Memory subsystem}\label{sec:hw_mem_subsys}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/memory_subsystem_design}
    \caption{Memory Subsystem}
    \vspace{-8pt}
    \label{fig:mem_subsys_design}
\end{figure}

% the benefits of our cache
As shown in \Cref{fig:mem_subsys_design}, the memory subsystem includes the cache and the page table.
It caches B-Tree nodes to improve performance by reducing off-chip memory accesses, reducing memory accesses over PCIe, and maximizing the use of available off-chip bandwidth. The root node, which is accessed by all requests, is cached in on-chip memory to save off-chip bandwidth. Other interior nodes are cached in on-board DRAM to reduce PCIe accesses, which both improves request throughput and latency. The load balancer directs some memory accesses over PCIe even when they hit in the cache to use all available off-chip memory bandwidth. 

The memory subsystem supports multiple input/output interfaces for parallel access from the interior-node search and leaf-node scan engines. It also
maintains a page table mapping node LIDs to node physical addresses. This is exposed over PCIe for write operations to replace subtrees atomically by changing mappings (\Cref{sec:insert}). When the slow path for a write operation changes a page table mapping, the cache entry for the node with that LID is invalidated. Leaf nodes are not cached to avoid the need for cache invalidations over PCIe for every write operation to the B-Tree. 


% metedata table
The metadata table keeps track of cached B-Tree nodes in on-board DRAM. It implements a four-way set associative cache indexed by LIDs. Each entry in a set  records the node LID, the physical address, and a 32-bit occupancy map where each bit represents 256~bytes of node data (with 8~KB nodes). The metadata table itself is stored in on-board DRAM. \sys uses a small on-chip metadata cache, which has capacity for 1K entries in the current implementation, to minimize the traffic to on-board DRAM.


% write back
On cache misses, the cache module fetches an integral number of 256-byte chunks over PCIe and attempts to write them back to the cache. The write back module implements a locking scheme to avoid conflicts when the cache is invalidated on page table updates or during B-Tree root updates. The write back operation allocates space for a new node in the cache only if the missing access was to the header and shortcut block of the node. The write back module fills other chunks in the allocated cache space when it reads sorted block segments for the node.  When interior nodes cannot fit in the cache, we evict a random node from the same set.
We leave more complex cache replacement policies for future investigation. 

% NAT
The metadata table determines if incoming memory read requests are hits or misses, and sends them to the node address table (NAT). The NAT is responsible for ensuring each request has a consistent view of each node it visits. For example, without the NAT, a request could read the shortcut block from the cache and an inconsistent sorted block segment from host memory after the page table entry for the node is updated.
The NAT maps the request sequence number to the physical address of the node version first accessed by the request. The NAT entry can be written by both the metadata table and the page table depending on whether the first access by the request to a node is a hit or a miss. When an access to a sorted block hits in the cache, the physical address in the cache entry must match the  physical address recorded in the NAT for the request. In case of a mismatch or cache miss, the sorted block segment is always loaded from host memory using the physical address from the NAT for the request. 

% load balance

With large caches, the accelerator can become bottlenecked on accesses to on-board DRAM while underutilizing PCIe bandwidth.
To use all available off-chip memory bandwidth, the load balancer directs some memory accesses that hit in the cache to host memory over PCIe. 
It constantly monitors the number of inflight operations and the total number of bytes being read by those operations on both DRAM and PCIe interfaces.
Then balances load across the two interfaces.

\if 0
However, if a high traffic load is diverted to PCIe, its queuing latency can be much higher than that of DRAM. 
The cache hits will firstly consume DRAM throughput, and they will be diverted only when PCIe interface is underutilized. 
\fi


\input{experiments.tex}

\section{Related work}\label{sec:related}

%ordered key value stores
% masstree, ART, bwtree, cuco-trie, openbwtree
We build on a large body of work on indices for in-memory ordered key-value stores (e.g.,~\cite{art,bwtree,masstree,openbwtree,cuckootrie}). A comparison study~\cite{openbwtree} showed that the Adaptive Radix Tree (ART)~\cite{art} performed best followed by Masstree~\cite{masstree} except on scan-dominated workloads where a B+ tree~\cite{btree} variant was better. We chose Masstree~\cite{masstree} as a baseline because it has better scan performance than ART and it supports variable-size keys unlike the B+ tree variant in~\cite{openbwtree}. Cuckoo Trie~\cite{cuckootrie} exploits memory level parallelism to improve multi-core performance but it has worse scan performance than ART. None of these indices supports linearizable scans. Given our focus on scan-dominated workloads, \sys implements a B+ tree variant with support for variable-size keys and MVCC. 
Many of the optimizations in recent work exploit the memory hierarchy of modern server-class CPUs and are not applicable to \sys where the index is accessed over PCIe.

%erpc
%one-sided RDMA based improvements, 
Ordered key-value stores shard an index across servers and provide access to remote clients across the network. Recent research has shown how to leverage kernel-bypass using DPDK~\cite{dpdk} to implement eRPC~\cite{erpc}, a fast RPC mechanism. eRPC and Masstree have been used to implement a high throughput, low-latency, key-value store~\cite{erpc} that we use as our baseline.
Other research has explored using one-sided RDMA reads to bypass the server CPU for {\sc get} and {\sc scan} operations~\cite{farmsosp,drtm,cell,ziegler,learnedcache}. Since RDMA NICs only provide simple reads of contiguous memory, these systems require at least two RDMA reads per operation to support variable-sized keys or values, and they use client-side caching to avoid additional RDMA reads when traversing the index. XStore~\cite{learnedcache} uses a learned cache, 
a compact client-side cache design inspired by the work on learned indices~\cite{learnedindex,ding2020alex}, but it does not support variable-sized keys or linearizable {\sc scan}s.

CliqueMap~\cite{singhvi21cliquemap} implements a hybrid in-memory
key-value caching system with both RPC and remote memory access (RMA). Like \sys, it accelerates {\sc get} with RMA and executes {\sc set} via RPC to save CPU cost and improve performance. However, this implementation is based on a hash table and does not provide support for efficient {\sc scan}s.

There are proposals to extend RDMA, e.g.,~\cite{aguilera2019designing,prism}, but these have yet to be implemented in NIC hardware.

SmartNICs avoid the functionality limitations of RDMA by providing programmable processing engines in the NIC. There are two types: SoC-based with general purpose cores and accelerators for common functionality like compression and encryption (see~\cite{ipipe} for a survey); and FPGA-based~\cite{Putnam2014,Caulfield2016}. Since the later offer the promise of better performance and energy efficiency at the cost of being harder to program, \sys leverages an FPGA-based SmartNIC but implements complex split and merge of B-Tree nodes on the host CPU.

SmartNICs have been used as offloads for accelerating networking, e.g.,~\cite{clicknp,firestone2017vfp,firestone2018azure,floem,hxdp,tonic2020,eran2019nica}, AI inference~\cite{fowers2018a}, distributed file systems~\cite{linefs}, transactions~\cite{schuh2021xenic}, unordered key-value stores~\cite{chalamalasetti2013fpga,kv-direct,ren2019low,eran2019nica}, and ordered key-value stores~\cite{ipipe,yang2021heterokv,heinrich2015hybrid}. 

KV-direct~\cite{kv-direct}, which is the best unordered key-value store offload, stores the index in host memory and
implements hash table reads and writes, which are much simpler than B-Tree writes, in the FPGA.  It achieves better performance than \sys because this reduces synchronization across PCIe and hash table operations only require O(1) memory accesses, but it does not provide {\sc scan}s. The best ordered key-value store offload, HeteroKV~\cite{yang2021heterokv}, implements a B-Tree where the leaves are hash tables, which results in poor scan performance. It cannot support large stores because 
it stores the B-Tree in on-chip FPGA memory, and it supports only fixed-size keys.
\sys provides support for large stores, variable-sized keys and values, fast scans, and strong consistency.


% TODO: say something about iPIPE


%E3 [42] and 𝜆-NIC [25] offload microservices to SoC-based



%btrees on fpga
%yang2021heterokv btree of hashtables. btree on fpga and L2/core sized hash tables as leaves on CPU. Stores the btree nodes in BRAM on-chip memory. So it cannot support very large trees or large keys. A small tree with 4 byte keys and pointers uses 48% of BRAM resources.
% Also only supports fixed-size keys. Do not discuss consistency guarantees and synchronization. When configired to achieve a throughput of 9M scans per %second approximatelly the same as ours, the latency is around 100us. So about 5x worse than ours. Can we say this. If they are returning 100 elements, 
% they are returning more data than us. Rely on batching that works best on small trees. Also they assume that scan do not span leaves. This is not going to %be true in practice, e.g., because the start key is close the boundary at the end of the leaf.

%heinrich2015hybrid and heinrich2018search Only searches the upper levels of the btree in the FPGA and hands the rest of the search to the CPU. Uses the 
% same technique to accelerate the search part of inserts/deletes. They transfer the whole top level of the tree to the FPGA. They only support fixed-size keys
% and they do parallel search across all the keys in a node. Write performance is a disaster and it is assumed that there are almost not updates to the tree. 
% Plus do not look at the cost of transferring the partial results from the FPGA to the CPU over PCIe which dominates latency.

\input{conclusion.tex}

% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% To ...
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \bibliographystyle{ACM-Reference-Format}
\bibliographystyle{plain}
\bibliography{honeycomb_arxiv}

% %%
% %% If your work has an appendix, this is the place to put it.
% \appendix
% \section{XXX}

\end{document}
\endinput
%%
%% End of file `honeycomb_sosp.tex'.
