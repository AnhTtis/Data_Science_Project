\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[table,xcdraw]{xcolor}
\input{math_commands.tex}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{rotating}

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
  
  \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\definecolor{citecolor}{HTML}{0071bc}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,  citecolor=citecolor,bookmarks=false]{hyperref}

\usepackage{xcolor}

\iccvfinalcopy
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\input{micro}

\begin{document}
%%%%%%%%% TITLE
\title{Sensitivity-Aware Visual Parameter-Efficient Tuning}

\author{%
  Haoyu He$^{1}$ ~~ Jianfei Cai$^{1}$ ~~ Jing Zhang$^{2}$ \\  ~~ Dacheng Tao$^{2,3}$ ~~ Bohan Zhuang$^{1}\thanks{Corresponding author. E-mail: $\tt  bohan.zhuang@gmail.com$}$ \\ [0.25cm]
$^1$ Monash University \quad $^2$ The University of Sydney \quad $^3$ JD Explore Academy \\[0.1cm]
% 
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel \textbf{S}ensitivity-aware visual \textbf{P}arameter-efficient \textbf{T}uning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of sensitive parameters exceeds a pre-defined threshold by utilizing any of the existing structured tuning methods, e.g., LoRA~\cite{hu2022lora} or Adapter~\cite{houlsby2019parameter}, 
to replace directly tuning the selected sensitive parameters (unstructured tuning) under the budget. Extensive experiments on a wide range of downstream recognition tasks show that our SPT is complementary to the existing VPET methods and largely boosts their performance, e.g., SPT improves Adapter with supervised pre-trained ViT-B/16 backbone by 4.2\% and 1.4\% mean Top-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks, respectively. Source code is at \url{https://github.com/ziplab/SPT}.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{1-introduction}
\input{2-related}
\input{3-method}
\input{4-experiments}


%-------------------------------------------------------------------------
\vspace{-0.3em}
\section{Conclusion}
\vspace{-0.2em}
In this paper, we have explored identifying and allocating trainable parameters to task-specific important positions for visual parameter-efficient tuning. Specifically, we have proposed a novel criterion to quickly measure the sensitivity of the pre-trained parameters for each specific task before fine-tuning. Based on the parameter sensitivity, we have proposed a trainable parameter allocation strategy that adaptively combines both unstructured and structured tuning under a desired trainable parameter budget, enabling high representational capability and flexibility. Finally, we have conducted extensive experiments on a total of 24 downstream recognition tasks with both plain and hierarchical vision Transformer backbones under different pre-training strategies to demonstrate the versatility and effectiveness of our proposed SPT. Notably, we have shown that our approach is complementary to the existing VPET methods and improves their performance significantly. In the future, we will explore adapting large vision models to more downstream tasks with SPT, 
\eg, dense prediction and vision-and-language tasks, and improve the training efficiency of SPT for on-device training~\cite{cai2020tinytl,lin2022device}. 


\bibliographystyle{abbrv}
{\small
\bibliography{reference}
}

\clearpage
\appendix
\input{5-appendix}

\end{document}
