\vspace{-0.3em}
\section{Related Work}
\label{subsec:related}
\vspace{-0.3em}

\noindent{\textbf{Parameter-efficient tuning.}} 
Full fine-tuning is the most predominant approach when adapting a large-scale pre-trained model to downstream tasks, where the model is initialized from the pre-trained weights with all parameters trainable. Yet, when a model becomes larger, parameter-efficient tuning (PET)~\cite{lester2021power,li-liang-2021-prefix} is highly desirable, which tunes only a tiny portion of parameters to alleviate the storage burden. The general PET approaches can be categorized into addition-based PET methods and reparameterization-based PET methods. 

\emph{Addition-based PET} attaches additional trainable parameters to the backbone and only tunes these parameters. Apart from Prompt tuning~\cite{jia2022vpt} and Adapter~\cite{houlsby2019parameter}, recent addition-based methods study connecting or combining existing VPET methods. For instance, He \etal~\cite{he2022towards} connect Prompt tuning and Adapter and provide a unified view that all VPET approaches share the same design to adjust the hidden representations. Zhang \etal~\cite{zhang2022neural} search for the optimal configurations to combine multiple VPET approaches following once-for-all scheme~\cite{cai2019once,wu2021autoformer}. Since the additional parameters require extra computations compared to full fine-tuning, a few recent works~\cite{sung2022lst,tu2022visual} design specific architectures to avoid storing the intermediate activations, thereby alleviating the fine-tuning memory cost. However, it is noteworthy that enhancing training efficiency is not the primary objective of our work.

\emph{Reparameterization-based PET} aims to avoid extra computational costs by tuning parameters that are inherently in or can be reparameterized into the backbone during inference. Prior works select the parameters that are inherently in the backbone, including the bias terms~\cite{zaken2022bitfit}, the last several layers~\cite{yosinski2014transferable,caelles2017one}, and weight connections~\cite{guo2021parameter,zhao2020masking}. To reparameterize new parameters into the backbone~\cite{hao2023consolidator,lian2022scaling}, representative work LoRA~\cite{hu2022lora} optimizes two low-rank matrices which can be further merged into the weight matrices. In contrast to the aforementioned works, we argue the importance of tuning parameters at task-specific important positions and quickly identify them with our proposed parameter sensitivity criterion before tuning, which is complementary to and provides valuable guidance for the existing VPET methods. Moreover, our SPT can also be inference-efficient when implementing structured tuning with any reparameterization-based structured tuning method. Recently, SSF~\cite{lian2022scaling} is proposed to introduce trainable scaling and shifting parameters that can be absorbed into the previous linear layers. However, it cannot scale to higher trainable parameter budgets and requires a complex and time-consuming hyper-parameter search for learning rate, weight decay, and drop-path rate on each individual dataset, thus is not directly comparable to our method.

\noindent\textbf{Task-specific transfer learning.} The effectiveness of transferring pre-trained models to downstream tasks strongly depends on the relationship between the source and target tasks~\cite{rosenstein2005transfer,wang2019characterizing,kumar2022finetuning,plested2022deep}. This has motivated the community to explore the optimal pre-training data~\cite{cui2018large,yoon2020data}, model~\cite{tran2019transferability,nguyen2020leep}, and weights~\cite{guo2019spottune,xu2021raise} for the target task. To seek suitable \emph{task-specific pre-training data}, Cui~\etal~\cite{cui2018large} select the source domain data from the top-k most similar classes measured by Earth Mover's Distance; Yoon~\etal~\cite{yoon2020data} weight each class in the source domain with reinforcement learning; and Puigcerver~\etal~\cite{puigcerver2020scalable} first train a diverse set of experts and then select the most relevant expert for each target task. Another line of work selects a suitable \emph{pre-trained model for the target task} ahead of fine-tuning by measuring the transferability of pre-trained models to the target domain with interclass covariance between the source data and target classes~\cite{bao2019information} or conditional cross-entropy~\cite{tran2019transferability} between the source and target labels. Considering the transferability of the feature representations at distinct layers for the same pre-trained model is different~\cite{yosinski2014transferable,neyshabur2020being}, recent works~\cite{guo2020adafilter,sun2020adashare} endeavour \emph{transfer task-specific weights} by freezing some pre-trained weights and fine-tuning the rest.
For example, the task-specific fine-tuned weights are selected by learning a policy network with Gumbel-Softmax~\cite{guo2019spottune}, optimizing a sparse mask with $L_0$ norm~\cite{guo2021parameter}, and learning binary gates for each parameter~\cite{zhao2020masking}. Our SPT also adaptively selects task-specific parameters. In contrast to the previous work, we 1) derive task-specific important positions prior to fine-tuning with only a single forward and backward pass, which is computationally efficient; 2) mask the gradients for insensitive parameters in unstructured tuning with fixed binary masks, thereby having more affordable fine-tuning memory than optimizing learnable binary masks in~\cite{guo2021parameter,zhao2020masking}. Moreover, we are pioneering work to adaptively allocate task-specific trainable parameters with both fine-grained unstructured and coarse-grained structured tuning granularities to achieve both high flexibility and representational capability.