

\begin{table*}[t]
\centering
\resizebox{0.8\linewidth}{!}{% <------ Don't forget this %
\begin{tabular}{
l  !{\color{tabvline}\vrule}
r  !{\color{tabvline}\vrule}
rr   !{\color{tabvline}\vrule}
rrrrr
}
\toprule
\textbf{\vit{}-B/16} & \textbf{Total}
&\multicolumn{2}{c!{\color{tabvline}\vrule}}{\bf{FGVC}}
&\multicolumn{5}{c}{\bf{\vtab{}}}
\\
\bf{(85.8M)} & \textbf{params}
&\bf{\scriptsize{Tuned / Total}}
&\bf{\scriptsize{Mean Acc.}}
&\bf{\scriptsize{Tuned / Total}}
&\bf{\scriptsize{Natural}} &\bf{\scriptsize{Specialized}} &\bf{\scriptsize{Structured}}
&\bf{\scriptsize{Mean Acc.}}
\\
\midrule
\band \fullft{} & 24.02$\times$ & 100\%
&88.5 & 100\% &75.9 & 83.4 & 47.6& 69.0
\\\midrule
 \multicolumn{9}{c}{\bf{Addition-based methods}}
\\\midrule
\mlp-3 & 1.35$\times$ &1.50\%
&79.8 & 1.42\% &67.8 &72.8 &30.6 & 57.1
\\
\shallowprompt{} & 1.04$\times$ & 0.31\%
&84.6 & 0.13\% &76.8 &79.7 & 47.0 & 67.8
\\\deepprompt{} & 1.18$\times$ &  0.98\%
& 89.1 & 1.14\%
&78.5 &82.4 &55.0 & 72.0
\\
\adapter{}-8 & 1.06$\times$ & 0.39\%
& 85.5 & 0.23\% &79.0 & 84.1 & 58.5 & 73.9
\\
\adapter{}-32 & 1.19$\times$ & 0.95\%
&85.6 & 0.71\% &79.6 &84.0 &58.3 & 74.0
\\
\adaptformer{} & 1.09$\times$ & 0.44\% &
85.1 & 0.36\%
& 80.6 & \underline{85.4} & 58.5 & 74.8 \\
\noah{} & - & - &
- & 0.52\%
& 80.2 & 84.9 & \underline{61.3} & 75.5 \\
\SPTa{} (Ours) & 1.08$\times$ & 0.41\%
& \underline{89.5} & 0.30\% & \underline{81.3} & 85.3 & 60.8 & \underline{75.8}\\
\SPTa{} (Ours) & 1.10$\times$ & 0.47\%
& \textbf{89.8} & 0.44\% & \textbf{82.0} & \textbf{85.8} & \textbf{61.4} & \textbf{76.4}
\\
\midrule
 \multicolumn{9}{c}{\bf{Reparameterization-based methods}}
\\\midrule
\linear{} & 1.02$\times$ & 0.12\%
&79.3 & 0.04\% &68.9 &77.2 &26.8 & 57.6
\\
\partialft{}-1 & 3.00$\times$ &8.38\%
&82.6 & 8.30\% &69.4 &78.5 &34.2 & 60.7
\\
\bias{} & 1.05$\times$ & 0.13\%
&88.4 & 0.13\% &73.3 &78.3 &44.1 & 65.2
\\
\lora{}-8 & 1.07$\times$ & 0.55\%
& 86.0 & 0.23\% &79.5 & 84.6 & 60.5 & 74.9
\\
\lora{}-16 & 1.18$\times$ & 0.90\%
& 84.8 & 0.69\% &79.8 &84.9 & 60.2 & 75.0
\\
\SPTl{} (Ours) & 1.08$\times$ & 0.41\%
& \underline{89.3} & 0.31\% & \underline{81.5} & \underline{85.6} & \underline{60.7} & \underline{75.9} \\
\SPTl{} (Ours) & 1.15$\times$ & 0.60\%
& \textbf{90.1} & 0.63\% & \textbf{81.9} & \textbf{85.9} & \textbf{61.3} & \textbf{76.4} \\
\bottomrule
\end{tabular}% <------ Don't forget this %
}
\vspace{-0.5em}
\caption{Comparisons on FGVC and \vtab{}~\cite{zhai2019vtab} benchmarks using supervised pre-trained ViT-B/16 backbone pre-trained on ImageNet-21k. ``Total params'' denotes the ratio of the total number of parameters needed for all downstream tasks relative to the one for the pre-trained backbone, and ``Tuned/Total'' denotes the fraction of trainable parameters. Top-1 accuracy (\%) is reported. The best result is in \textbf{bold}, and the second-best result is \underline{underlined}.}
\label{tab:main_sup}
\vspace{-0.5em}
\end{table*}

\vspace{-0.2em}
\section{Experiments}
\vspace{-0.2em}
\subsection{Experimental Setup}
\vspace{-0.2em}

\noindent\textbf{Datasets and metrics.} We evaluate our SPT on total $24$ downstream tasks in two groups following~\cite{jia2022vpt}. 1) FGVC is a benchmark for fine-grained visual classification, including \cub{}~\cite{wah2011caltech}, \nabirds{}~\cite{van2015building}, \flowers{}~\cite{nilsback2008automated}, \cars{}~\cite{gebru2017cars}, and \dogs{}~\cite{Khosla_FGVC2011dogs} datasets. Each FGVC dataset contains between 55 to 200 classes and a few thousand images for train, validation, and test. We follow the validation splits in~\cite{jia2022vpt} if the validation set is unavailable. 2) \vtab{}~\cite{zhai2019vtab} is a large-scale transfer learning benchmark consisting of a collection of 19 visual classification tasks. \vtab{} can further be divided into three groups, including Natural tasks with natural images, Specialized tasks with images captured by specialized equipment, e.g., medical images, and Structured tasks with images mostly generated from synthetic environments. Each of the \vtab{} dataset has only 800 training and 200 validation samples, while the test set sizes vary. We use top-1 accuracy (\%) averaged within each group as our main metric following~\cite{jia2022vpt}.

\noindent\textbf{Pre-trained backbones.} We conduct experiments on the plain vision Transformer backbone ViT-B/16~\cite{vit} that is pre-trained on \imagenet{}~\cite{krizhevsky2012imagenet} with different pre-training strategies following~\cite{jia2022vpt}, including supervised pre-training and self-supervised pre-training with \mae{}~\cite{he2022masked} and \moco{}~\cite{chen2021empirical} following~\cite{jia2022vpt}. We also conduct experiments on the representative hierarchical vision Transformer backbone Swin-B~\cite{swin} under supervised pre-training.

\noindent\textbf{Contenders.} We categorize the baseline methods into addition-based and reparameterization-based VPET methods as introduced in Section~\ref{subsec:related}. Unless specified, all baseline methods keep the backbone frozen. Addition-based methods require extra computations during inference, including \mlp{}-$k$, \shallowprompt{}~\cite{jia2022vpt}, \deepprompt{}~\cite{jia2022vpt}, \adapter{}-$k$~\cite{houlsby2019parameter}, \adaptformer{}~\cite{chen2022adaptformer}, and \noah{}~\cite{zhang2022neural}. Reparameterization-based methods have no additional computational overhead during inference, including \linear{}, \partialft{}-$k$, \bias{}~\cite{zaken2022bitfit}, and \lora{}-$k$~\cite{hu2022lora}. Here $k$ represents the number of bottleneck dimension in \adapter{}-$k$ and \lora{}-$k$. We also 
compare with full fine-tuning which is denoted by \fullft{}. We introduce the details of these methods in appendix.

We also introduce two variants of our SPT: addition-based \SPTa{} and reparameterization-based \SPTl{}. \SPTa{} directly adjusts the hidden representations that are computed by sensitive weight matrices following~\cite{houlsby2019parameter}, while \SPTl{} approximates updating the sensitive weight matrices following~\cite{hu2022lora}. For the two variants, we follow the exact weight initializations that are described in~\cite{hu2022lora} and follow~\cite{zhang2022neural} to set the bottleneck dimension as 8.

\noindent\textbf{Implementation details.}
Following~\cite{zhang2022neural}, we use the AdamW optimizer~\cite{loshchilov2018fixing} with cosine learning rate decay and set the batch size, learning rate, and weight decay as 64, $1\times10^{-3}$, and $1\times10^{-4}$, respectively. We also follow~\cite{zhang2022neural} for the standard data augmentation pipeline. We set the number of training samples $C$ used to calculate our parameter sensitivities in Algorithm~\ref{alg:tps} to be 400 for \vtab{} and 800 for FGVC benchmarks.


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{% <------ Don't forget this %
\begin{tabular}{
l  !{\color{tabvline}\vrule}
r  !{\color{tabvline}\vrule}
rrrrr   !{\color{tabvline}\vrule}
rrrrr}
\toprule
\textbf{\vit{}-B/16}
&\bf{\scriptsize{Total}}
&\multicolumn{5}{c!{\color{tabvline}\vrule}}{  \bf{\vtab{}} \quad \mae{}}
&\multicolumn{5}{c}{\bf{\vtab{}} \quad  \moco{}}
\\
\bf{(85.8M)}
&\bf{\scriptsize{Params}}
&\bf{\scriptsize{Tuned / Total}}
&\bf{\scriptsize{Natural}} &\bf{\scriptsize{Specialized}} &\bf{\scriptsize{Structured}}
&\bf{\scriptsize{Mean Acc.}}
&\bf{\scriptsize{Tuned / Total}}
&\bf{\scriptsize{Natural}} &\bf{\scriptsize{Specialized}} &\bf{\scriptsize{Structured}}
&\bf{\scriptsize{Mean Acc.}}
\\
\midrule
\band \fullft{} & 38.02$\times$ & 100\%
&59.3 &79.7 &53.8 &64.3 & 100\% &72.0 &84.7 &42.0 & 69.6
\\\midrule
 \multicolumn{12}{c}{\bf{Addition-based methods}}
\\\midrule
\adapter{}-8 &1.08$\times$ & 0.23\%
& 57.2 & 78.4 & 54.7 & 63.4 & 0.23\% & 27.6 & 70.9 & 48.4 & 49.0 
\\
\adapter{}-32 &1.28$\times$ & 0.95\%
& 55.3 & 78.8 & 53.3 & 62.5 & 0.99\% & 74.2 & 82.7 & 47.7 & 68.2
\\
\shallowprompt{} & 1.02$\times$ & 0.12\%
& 40.0 & 69.7 & 27.5 & 45.7 & 0.12\%  & 67.3 & 82.3 & 37.6 & 62.4
\\\deepprompt{} & 1.05$\times$ & 0.23\%  & 36.0 & 60.6 & 26.6
& 41.1 & 0.07\%
&70.3 & 83.0 & 42.4 & 65.2\\
\SPTa{} (Ours) & 1.07$\times$ & 0.26\% & \underline{64.8} & \underline{82.4} & \underline{60.4} & \underline{69.2} & 0.08\% & \underline{76.1} & \underline{84.9} & \underline{60.1} & \underline{73.7}
\\
\SPTa{} (Ours) & 1.13$\times$ & 0.41\% & \textbf{65.6} & \textbf{82.7} & \textbf{60.7} & \textbf{69.7} & 0.30\% & \textbf{76.6} & \textbf{85.0} & \textbf{61.7} & \textbf{74.4}
\\\midrule
 \multicolumn{12}{c}{\bf{Reparameterization-based methods}}
\\\midrule
\linear{} & 1.02$\times$ & 0.04\% & 18.9 & 52.7 & 23.7 & 32.1 & 0.04\% & 67.5 &81.1 & 30.3 & 59.6
\\
\partialft{}-1 & 4.16$\times$ &8.30\%
&58.4 & 78.3 & 47.6 & 61.5 & 8.30\% &72.3 & 84.6 &47.9 & 68.3
\\
\bias{} & 1.06$\times$ & 0.13\%
&54.6 & 75.7 & 47.7 & 59.3 & 0.13\% & 72.9 &81.1 &53.4 & 69.2
\\
\lora{}-8 &1.08$\times$ & 0.23\%
& 57.5 & 77.7 & 57.7 & 64.3 & 0.23\% & 21.2 & 66.7 & 45.1 & 44.3
\\
\lora{}-16 &1.28$\times$ & 0.69\%
& 57.3 & 77.1 & 59.9 & 64.8 & 0.69\% & 16.0 & 64.0 & 48.7 & 42.9
\\
\SPTl{} (Ours) & 1.11$\times$& 0.29\% & \underline{63.8} & \underline{81.6} & \underline{60.0} & \underline{68.5} & 0.30\% & \textbf{76.5} & \underline{85.4} & \underline{63.0} & \underline{75.0} \\
\SPTl{} (Ours) & 1.23$\times$& 0.69\% & \textbf{65.4} & \textbf{82.4} & \textbf{61.5} & \textbf{69.8} & 0.50\% & \textbf{76.5} & \textbf{86.0} & \textbf{63.6} & \textbf{75.3} \\
\bottomrule
\end{tabular}% <------ Don't forget this %
}
\vspace{-0.5em}
\caption{Comparisons on \vtab{}~\cite{zhai2019vtab} benchmark using self-supervised ViT-B/16 backbone pre-trained by \mae~\cite{he2022masked} and \moco~\cite{chen2021empirical}. ``Total params'' denotes the ratio of the total number of parameters needed for all downstream tasks relative to the one for the pre-trained backbone, and ``Tuned/Total'' denotes the fraction of trainable parameters. Top-1 accuracy (\%) is reported. The best result is in \textbf{bold}, and the second-best result is \underline{underlined}.}
\label{tab:main_ssup}
\vspace{-1.5em}
\end{table*}

\begin{table}[t!]
\centering
\resizebox{\linewidth}{!}{%
    \begin{tabular}{l!{\color{tabvline}\vrule}ccccc}
    \toprule
    \bf{Method} & \begin{tabular}[c]{@{}c@{}} \bf{Tuned / } \\  \bf{Total} \end{tabular}  &\bf{Natural} &\bf{Specialized} &\bf{Structured}
    & \begin{tabular}[c]{@{}c@{}} \bf{Mean / } \\  \bf{Acc.} \end{tabular} \\
    \midrule
    \band \fullft{} & 100\% & 79.1 & 86.2 & 59.7 & 75.0
    \\\midrule
     \multicolumn{6}{c}{\bf{Addition-based methods}}
    \\\midrule
    \mlp{}-3 & 1.60\% & 73.6 & 75.2 & 35.7 & 61.5 \\
    \shallowprompt{} & 0.04\% & 79.9 & 82.5 & 37.8 & 66.7  \\
    \deepprompt{} & 0.23\% & 76.8 & 84.5 & 53.4 & 71.6 \\
    \adapter{}-8 & 1.18\% & 81.7 & \textbf{87.3} & 61.2 & 76.7 \\
    \SPTa{} (ours) & 0.33\% & \textbf{83.0} & \textbf{87.3} & \textbf{62.1} & \textbf{77.5}
    \\\midrule
     \multicolumn{6}{c}{\bf{Reparameterization-based methods}}
    \\\midrule
    \linear{} & 0.04\% & 73.5 & 80.8 & 33.5 & 62.6 \\
    \partialft{}-1 & 2.15\% & 73.1 & 81.7 & 35.0 & 63.3 \\
    \lora{}-8 & 1.18\% & 81.7 & 87.2 & 60.1 & 76.3 \\
    \SPTl{} (ours) & 0.49\% & \textbf{83.1} & \textbf{87.4} & \textbf{60.4} &  \textbf{77.2} \\
    \bottomrule
    \end{tabular}%
    }
        \vspace{-0.5em}
    \caption{Comparisons on \vtab{}~\cite{zhai2019vtab} benchmark with supervised pre-trained Swin-B~\cite{swin}. ``Tuned/Total'' denotes the fraction of trainable parameters. Top-1 accuracy (\%) is reported.}
        \vspace{-1.5em}
    \label{tab:main_swin}
\end{table}

\vspace{-0.3em}
\subsection{Main Results}
\vspace{-0.2em}
\label{subsec:main_results}

We evaluate the effectiveness of our method by comparing it with the baseline methods under vision Transformer backbones with various pre-training strategies.

First, \emph{our proposed \SPTa{} and \SPTl{} achieve the best performance under different trainable parameter budgets} with supervised pre-trained ViT-B/16 backbone, as shown in Table~\ref{tab:main_sup} and Figure~\ref{fig:abl}~(a). For instance, \SPTa{} outperforms the SOTA method \noah{} by a clear margin of 0.9\% mean top-1 accuracy over the 19 \vtab{} datasets with fewer trainable parameters. We speculate that our SPT variants allocate trainable parameters at task-specific positions compared to the heuristically selected positions in the baseline methods, which contributes to our superior performance. We also observe that our \SPTa{} and \SPTl{} achieve large performance gains over \adapter{} and \lora{} variants, respectively. For example, \SPTa{} and \SPTl{} with 0.41\% trainable parameters respectively improve \adapter{}-8 and \lora{}-8 significantly by 4.0\% and 3.3\% mean accuracy on the FGVC benchmark. This suggests that identifying task-specific important positions and combining both unstructured and structured tuning granularities with SPT are complementary to the existing VPET methods and boost their performance.

Second, \emph{\SPT{} variants outperform baseline methods and full fine-tuning by significant margins with the self-supervised pre-trained ViT-B/16 backbones.} As shown in Table~\ref{tab:main_ssup}, existing VPET approaches exhibit inferior results than full fine-tuning with the self-supervised pre-trained backbones \mae{} and \moco{}.
It is worth noting that
previous VPET methods yield inconsistent results with the backbones of different pre-training strategies. In contrast,
SPT variants consistently outperform full fine-tuning. In particular, \SPTa{} achieves remarkable 5.8\% and 5.5\% mean top-1 accuracy gains over the best-performing baseline method on \vtab{} benchmark with only 0.26\% and 0.08\% trainable parameters for \mae{} and \moco{} pre-trained backbones, respectively. Moreover, our observation in appendix suggests that self-supervised pre-trained ViT backbones have more diverse sensitivity distributions and a higher variance in sensitivity across different tasks than the supervised pre-trained one. This leads to the conjecture that baseline methods that assign trainable parameters to the same positions for all tasks may fail to mitigate the distinct domain gaps in individual downstream datasets, whereas our SPT allocates trainable parameters to task-specific positions accurately. From Table~\ref{tab:main_swin}, we observe that our \SPTl{} and \SPTa{} also achieve SOTA performance with Swin-B backbone on all dataset groups, which further demonstrates the versatility and effectiveness of our SPT.

\vspace{-0.2em}
\subsection{Ablation Study}
\label{subsec:abl}
\vspace{-0.2em}
\noindent\textbf{Effect of the sensitivity criterion.} We investigate the effectiveness of our sensitivity criterion on~\vtab{} by employing structured tuning methods from~\cite{jia2022vpt,houlsby2019parameter,hu2022lora} to the task-specific sensitive weight matrices. Note that we do not conduct unstructured tuning to ensure fair comparisons. The results are presented in Figure~\ref{fig:abl}~(b). Our criterion brings consistent 1.1\%, 1.6\%, and 0.8\% performance gains for \deepprompt{}, \adapter{}-32, and \lora{}-16, respectively, which demonstrates the effectiveness and versatility of our sensitivity criterion to identify accurate task-specific important positions.

\noindent\textbf{Effect of structured and unstructured tuning.} We investigate the effectiveness of unstructured and structured tuning individually on~\vtab{}. The results are presented in Table~\ref{tab:structured}. We start by applying \adapter{}-8 to the sensitive weight matrices identified by our sensitivity criterion (\SPTa{} w/o unstructured). We observe that our sensitivity criterion boosts the performance of all the dataset groups by clear margins, which again demonstrates the importance of our sensitivity criterion. Next, we observe that allocating the trainable parameters to the unstructured sensitive weight connections also brings accuracy improvement to the Natural and Specialized datasets from \adapter{}-8. However, we find that structured tuning is especially important for achieving good performance on Structured datasets. To further investigate this phenomenon, we observe that Structured datasets have larger domain gaps from the pre-training source domain~\cite{krizhevsky2012imagenet} compared to Natural and Specialized datasets as visualized in Figure~\ref{fig:abl}~(c). We hence conjecture that structured tuning has a higher representational capability than unstructured tuning which facilitates mitigating the large domain gaps during fine-tuning (see the appendix for visual examples). Finally, we observe that incorporating both structured and unstructured tuning at task-specific important positions achieves the highest performance on all dataset groups.


\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{pics/abl.pdf}
\end{center}
\vspace{-2em}
\caption{ (a) Accuracy vs. parameter efficiency with supervised pre-trained ViT-B/16 backbone on \vtab{}~\cite{zhai2019vtab}. SPT variants perform favorably against the other VPET approaches and are more scalable. (b) Applying other VPET structured tuning methods~\cite{jia2022vpt,houlsby2019parameter,hu2022lora} to the task-specific sensitive weight matrices (denoted by TSM) identified by our criterion with supervised pre-trained ViT-B/16 backbone on \vtab{}. Our criterion brings consistent performance gain. (c) Domain vs. performance gaps for different dataset groups in VTAB-1k~\cite{zhai2019vtab}. The blue bars show the domain gaps between the source domain (\imagenet{}~\cite{krizhevsky2012imagenet}) and target domains, which are measured by Maximum Mean Discrepancy (MMD) distance~\cite{tzeng2014deep}. The red line represents the performance gaps between \SPTa{} w/o unstructured and w/o structured, using supervised pre-trained ViT-B/16 backbone. The dataset groups are Natural, Specialized, and Structured. Structured tuning is important for achieving good performance on Structured datasets with larger domain gaps.}
\vspace{-1.5em}
\label{fig:abl}
\end{figure*}

\begin{figure}[!th]
\begin{center}
    \includegraphics[width=\linewidth]{pics/sensitivity_block_operation.pdf}
\end{center}\vspace{-2em}
\caption{Parameter sensitivity patterns under 0.4M trainable parameter budget for supervised pre-trained ViT-B/16 backbone on three sample tasks from VTAB-1k~\cite{zhai2019large}. Left: proportions of the sensitivity parameters for each block vary across different tasks. Right: the most insensitive matrices are the query $\mW_{q}$ and key $\mW_{k}$ weight matrices.}
\label{fig:sensitivity}
\vspace{-2em}
\end{figure}

\begin{table}[t!]
\centering
\resizebox{\linewidth}{!}{%
    \begin{tabular}{l!{\color{tabvline}\vrule}ccccc}
    \toprule
    \bf{Method} & \begin{tabular}[c]{@{}c@{}} \bf{Tuned / } \\  \bf{Total} \end{tabular}  &\bf{Natural} &\bf{Specialized} &\bf{Structured}
    & \begin{tabular}[c]{@{}c@{}} \bf{Mean / } \\  \bf{Acc.} \end{tabular} \\
    \midrule
    \adapter{}-8 & 0.23\% & 79.0 & 84.1 & 58.5 & 73.9 \\
    \SPTa{} w/o unstructured & 0.29\% & 81.2 & 85.1 & 60.3 & 75.5 \\
    \SPTa{} w/o structured & 0.34\% & 81.2 & 85.0 & 59.6 & 75.3 \\
    \SPTa{} & 0.30\% & \textbf{81.3} & \textbf{85.3} & \textbf{60.8} & \textbf{75.8} \\
    \bottomrule
    \end{tabular}%
    }
        \vspace{-0.5em}
    \caption{Ablation study on structured and unstructured tuning only with supervised pre-trained ViT-B/16 backbone. Top-1 accuracy (\%) is reported. We set different parameter constraints to align the fractions of trainable parameters for these cases.}
\vspace{-0.5em}
    \label{tab:structured}
\end{table}

\begin{table}[t!]
\centering
    \resizebox{0.6\linewidth}{!}{%
    \begin{tabular}{l!{\color{tabvline}\vrule}cccc}
    \toprule
     $C$ & 240 & 400 & 560 & 800 \\ \midrule
     \bf{\scriptsize{Mean Acc.}} & 76.3 & \textbf{76.4} & \textbf{76.4}& \textbf{76.4}\\
    \bottomrule
    \end{tabular}%
    }
        \vspace{-0.5em}
\caption{Effect of the number of training samples used to get the sensitivity for \SPTl{} with supervised pre-trained ViT-B/16 backbone on \vtab{}~\cite{zhai2019vtab}. Top-1 accuracy (\%) is reported.
}
\vspace{-2em}
\label{tab:num_samples}
\end{table}

\noindent\textbf{Effect of number of training samples $C$ to get parameter sensitivity.} We investigate the effect of the number of training images $C$ for calculating our parameter sensitivity (Algorithm~\ref{alg:tps}). We randomly sample training samples and report the mean results over three runs in Table~\ref{tab:num_samples}. We find that our SPT is robust to the number of training samples $C$ and randomly sampling 400 out of a total of 800 training samples is sufficient to obtain accurate task-specific important positions, \eg, calculating the sensitivity for ViT-B/16 backbone takes only 5.5 seconds with a single GPU on any of the \vtab{} datasets and this computation is required only once.

\vspace{-0.3em}
\subsection{Observations on Sensitivity Patterns}\label{subsec:observation}
\vspace{-0.2em}

Our sensitivity criterion identifies task-specific important positions, which can reveal the contributions of the pre-trained weights to different downstream tasks during transfer learning. We visualize the proportions of the sensitive parameters for the supervised pre-trained ViT-B/16 backbone under 0.4M trainable parameter budget in Figure~\ref{fig:sensitivity}. First, we investigate the most sensitive blocks, whose numbers of sensitive parameters are summed and normalized over the 12 ViT-B/16 blocks. We observe that the patterns of the sensitive parameter proportions vary markedly across different tasks, which echoes the observations made in~\cite{guo2019spottune}. This suggests that we should not introduce trainable parameters to the same positions for each individual task but allocate trainable parameters at task-specific ones as we proposed. Next, we investigate the most insensitive weight matrices within a block. A ViT block consists of a query $\mW_{q}$, a key $\mW_{k}$, a value $\mW_{v}$, and an output $\mW_{o}$ weight matrices in the multi-head self-attention layer and two weight matrices $\mW_{fc1}$ and $\mW_{fc2}$ in the feed-forward network as elaborated in~\cite{vaswani2017attention,vit}. We observe that the query $\mW_{q}$ and key $\mW_{k}$ weight matrices have the lowest proportions of sensitive parameters for all three sample tasks. Since $\mW_{q}$ and $\mW_{k}$ are responsible for learning the attention scores which indicate the pairwise similarity among the patches, we speculate that although domain changes, the patch relationships learned during pre-training can be efficiently reused when transferred to downstream classification tasks.


