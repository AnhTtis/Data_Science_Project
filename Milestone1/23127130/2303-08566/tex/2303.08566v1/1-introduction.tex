\vspace{-1em}
\section{Introduction}
% \vspace{-0.4em}

\begin{figure}[t]
\begin{center}    \includegraphics[width=0.95\linewidth]{pics/intro.pdf}
\end{center}\vspace{-2em}
\caption{(a) Existing VPET methods, such as Adapter~\cite{houlsby2019parameter} introduce trainable parameters to the same positions for all downstream tasks. However, these methods design task-agnostic positions to employ trainable parameters relying on heuristics and neglect consideration of the distinct domain gaps and characteristics for the downstream tasks. (b) Our Sensitivity-aware visual Parameter-efficient Tuning (SPT) introduces trainable parameters to the task-specific important positions and allocates them with both unstructured and structured tuning granularities, simultaneously. For structured tuning, SPT can exploit any existing structured tuning methods, such as LoRA~\cite{hu2022lora} or Adapter~\cite{houlsby2019parameter}. Red lines and blocks represent trainable parameters and modules, while blue lines represent frozen parameters.}
\label{fig:intro}
\vspace{-1.5em}
\end{figure}

The pre-training and fine-tuning paradigm has underpinned the most recent breakthroughs in vision, yielding stunning empirical performance on a series of tasks such as segmentation~\cite{chen2017deeplab,ronneberger2015u} and detection~\cite{he2017mask,carion2020end}. Transformer~\cite{vaswani2017attention} has been widely adopted as the standard architecture for pre-trained vision models, with representatives including CLIP~\cite{radford2021learning}, MAE~\cite{he2022masked}, BEiT~\cite{bao2021beit}, \etc.
To effectively adapt the pre-trained representations to the downstream tasks, the de-facto choice is full fine-tuning, which initializes the model with the pre-trained weights and tunes all the parameters. 
However, vanilla full fine-tuning needs to store a separate instance of parameters for each task and each deployment scenario. It can be extremely storage-intensive as the storage cost grows linearly with the number of possible cases, considering there are vast varieties of downstream tasks and dynamic deployment environments, especially when deploying the large vision models~\cite{vit,swin,xu2021vitae} to mobile systems. For example, even storing a single large pre-trained ViT-H~\cite{he2022masked} model on a local disk requires at least 2.3GB, while the Top-10 U.S. apps required only collectively 2.2GB in May 2021.\footnote{https://sensortower.com/blog/ios-app-size-growth-2021}


Notably, an emerging solution is to replace vanilla fine-tuning with Visual Parameter-Efficient Tuning (VPET)~\cite{jia2022vpt,chen2022adaptformer,zhang2022neural,jie2022convolutional}, which only tunes a small number of trainable parameters while freezing the vast majority ones that are shared by multiple tasks.
As VPET approaches exhibit less than 1\% of trainable parameters, the storage burden is largely alleviated. Another attractive property of VPET is that tuning fewer parameters eases the optimization difficulty and mitigates the overfitting issue when adapting large pre-trained models on the target dataset, thereby achieving comparable or even better performance than vanilla fine-tuning~\cite{jia2022vpt}. Although promising, the existing VPET approaches introduce trainable parameters to the same positions for all downstream tasks, relying on human heuristics and neglecting the task-specific domain gaps and characteristics, which limits their performance. For instance, in a task-agnostic manner, Prompt Tuning-deep~\cite{jia2022vpt} and Adapter~\cite{houlsby2019parameter} respectively add trainable parameters to multi-head self-attention (MSA) and feed-forward network (FFN) layers for all distinct tasks as depicted in Figure~\ref{fig:intro} (a).

To address this fundamental challenge,
we explore \textit{where to introduce} and \textit{how to allocate} trainable parameters 
under a desired parameter budget by presenting a novel \textbf{S}ensitivity-aware visual \textbf{P}arameter-efficient \textbf{T}uning (SPT) scheme that identifies the \textit{task-specific important positions} to adaptively allocate trainable parameters. Since the pre-trained weights at distinct positions 
have varying contributions for 
different downstream tasks~\cite{yosinski2014transferable,kumar2022finetuning,neyshabur2020being}, we first propose a new criterion to quickly identify the task-specific sensitive parameters that require tuning in a data-dependent way. Inspired by model pruning metrics~\cite{srivastava2015training,molchanov2019importance,bender2018understanding,brock2017smash}, we propose to measure the parameter sensitivity with the loss reduction when being tuned, which can be approximated by a first-order Taylor expansion derived within a single forward and backward pass ahead of fine-tuning in one-shot. Our sensitivity criterion is simple and effective, which can identify the task-specific important positions to introduce trainable parameters for any backbone quickly. For instance, calculating the sensitivity for ViT-B/16 backbone takes only 5.5 seconds with a single GPU on any of the \vtab{} datasets.

With our criterion, we empirically observe that the proportions of the sensitivity parameters for each block indeed vary markedly across different tasks in Section~\ref{subsec:observation}. To allocate the trainable parameters under a desired trainable parameter budge, an intuitive solution is to directly tune the most sensitive weight connections, which we name unstructured tuning. Despite its simplicity and flexibility, unstructured tuning only tunes a few parameters which still lacks representational capability and is challenging to bridge the domain gap. To this end, we propose to further incorporate structured tuning to replace unstructured tuning at the sensitive weight matrices whose numbers of sensitive parameters exceed a pre-defined threshold to improve the representational capability under a similar parameter budget. Structured tuning can be implemented by any parameter-efficient structured tuning methods~\cite{hu2022lora,chen2022adaptformer,jie2022convolutional,jia2022vpt} that directly adjust the hidden representations, \eg, inserting an adapter module sequentially after the sensitive weight matrices. Therefore, our SPT adaptively combines both unstructured and structured tuning granularity and allocates trainable parameters with high flexibility and representational capability for each distinct downstream task.

This paper has the following key contributions. 1) We make the pioneering exploration to identify the task-specific important positions under the VPET setting, which is fast, effective, versatile to be applied to various backbones with different pre-training strategies, and orthogonal to the existing VPET methods.
2) Based on the sensitivity criterion, we propose a trainable parameter allocation strategy that adaptively combines both unstructured and structured tuning under a desired parameter budget to achieve high flexibility, large capacity, and favorable trade-off between parameter efficiency and accuracy.
3) Extensive experiments on a total of 24 downstream recognition tasks with both plain and hierarchical vision Transformer backbones under supervised and self-supervised pre-trainings show that our SPT is complementary to the existing VPET methods and boosts their performance by large margins. For instance, SPT improves Adapter~\cite{houlsby2019parameter} by 4.2\% mean Top-1 accuracy, outperforming the SOTA VPET methods on the FGVC benchmark.