\onecolumn
\section*{Appendix}

We organize our supplementary material as follows. 
\begin{itemize}
    \item In Section~\ref{subsec:supp_contenders}, we introduce more details about the contenders.
    \item In Section~\ref{subsec:supp_pattern1}, we show more sensitivity patterns for ViT-B/16 with various pre-training strategies.
    \item In Section~\ref{subsec:supp_visual}, we show some dataset samples from \imagenet~\cite{krizhevsky2012imagenet} and \vtab{}~\cite{zhai2019vtab}.
    \item In Tables~\ref{tab:full_fgvc} and~\ref{tab:full_vtab}, we show per-task results for our SPT variants on FGVC and \vtab{} benchmarks, respectively.
    
\end{itemize}

\section{More Details of Contenders} 
\label{subsec:supp_contenders}

\begin{itemize}[leftmargin=2em]{

\item \fullft{}: fully tunes all the backbone and classification head parameters.
\vspace{-0.75em}
\item\linear{}: freezes all the backbone parameters and only tunes a linear classification head.
\vspace{-0.75em}
\item\bias{}~\cite{zaken2022bitfit}: freezes all the backbone parameters except for the bias terms and also tunes the linear classification head.
\vspace{-0.75em}
\item\partialft{}-$k$: freezes all the backbone parameters except for the last $k$ layers and also tunes the linear classification head as described in~\cite{jia2022vpt}.
\vspace{-0.75em}
\item \mlp{}-$k$: freezes all the backbone parameters and tunes the classification head which is implemented by a trainable $k$-layer multi-layer perceptron as described in~\cite{jia2022vpt}.
\vspace{-0.75em}
\item \shallowprompt{}~\cite{jia2022vpt}: freezes all the backbone parameters while introducing additional trainable prompts to the input space of the pretrained ViT.
\vspace{-0.75em}
\item \deepprompt{}~\cite{jia2022vpt}: freezes all the backbone parameters while appending additional trainable prompts to the sequence in the multi-head self-attention layer of each ViT block.
\vspace{-0.75em}
\item\adapter{}-$k$~\cite{houlsby2019parameter}: freezes all the backbone parameters while adding a down projection, a ReLU~\cite{hendrycks2016gaussian} non-linearity, and an up projection layer sequentially in the feed-forward network (FFN) of each visual Transformer block. 
We follow the training details of~\cite{zhang2022neural} to achieve better performance.
\vspace{-0.75em}
\item \lora{}-$k$~\cite{hu2022lora}: freezes all the backbone parameters while adding a concurrent branch including two low-rank matrices to the weight matrices in the multi-head self-attention layers to approximate efficiently updating them. 
The low-rank matrices can be merged into the backbone weights after fine-tuning. We follow the training details of~\cite{zhang2022neural} to achieve better performance.
\vspace{-0.75em}
\item \adaptformer{}~\cite{chen2022adaptformer}: freezes all the backbone parameters while adding a concurrent branch including a down projection, a ReLU~\cite{agarap2018deep} non-linearity, an up projection layer, and a pre-defined scaling factor to the FFN layer of each ViT block.
\vspace{-0.75em}
\item \noah{}~\cite{zhang2022neural}: searches for an optimal configuration with a once-for-all~\cite{cai2019once} network that includes trainable prompts, adapter modules, and LoRA modules, which requires a longer training schedule than the other VPET methods.
}
\end{itemize}

\section{More Parameter Sensitivity Patterns}
\label{subsec:supp_pattern1}
\rev{We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE~\cite{he2022masked} and MoCo V3~\cite{chen2021empirical}) and datasets sampled from FGVC benchmark~\cite{jia2022vpt}. We visualize the proportions of the sensitive parameters under 0.4M trainable parameter budget. Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViT-B/16 are shown in Figures~\ref{fig:sensitive_sup},~\ref{fig:sensitive_mae},~\ref{fig:sensitive_moco}. Visualizations of sampled FGVC datasets with supervised pre-trained ViT-B/16 are shown in Figure~\ref{fig:sens_fgvc}. We find our observations in the main paper are general: the proportions of the sensitive parameter exhibit: 1) dataset-specific varying patterns in terms of network depth; and 2) dataset-agnostic similar patterns in terms of operations. We empirically find} that the self-supervised pre-trained backbones have higher sensitivity variances than the supervised pre-trained one across the 19 downstream tasks. In particular, the variance of ViT-B/16 pre-trained with MAE~\cite{he2022masked} is twice as large as that of the supervised pre-trained ViT-B/16. We speculate that our SPT variants can better handle the large variances for self-supervised pre-trained backbones (Table 2 of the main paper) by identifying task-specific positions to introduce the trainable parameters.

\begin{figure}[htb]
\begin{center}
    \includegraphics[width=\linewidth]{sensitive_sup.pdf}
\end{center}
\caption{The distribution of sensitive parameters by blocks under 0.4M trainable parameter budget with supervised pre-trained ViT-B/16 backbone. We sample six tasks from VTAB-1k~\cite{zhai2019vtab}.
}
\label{fig:sensitive_sup}
\end{figure}

\begin{figure}[htb]
\begin{center}
    \includegraphics[width=\linewidth]{sensitive_mae.pdf}
\end{center}
\caption{The distribution of sensitive parameters by blocks under 0.4M trainable parameter budget with \mae{}~\cite{he2022masked} pre-trained ViT-B/16 backbone. We sample six tasks from VTAB-1k~\cite{zhai2019vtab}.}
\label{fig:sensitive_mae}
\end{figure}

\begin{figure}[tb]
\begin{center}
    \includegraphics[width=\linewidth]{sensitive_moco.pdf}
\end{center}
\caption{The distribution of sensitive parameters by blocks under 0.4M trainable parameter budget for \moco{}~\cite{chen2021empirical} pre-trained ViT-B/16 backbone. We sample six tasks from VTAB-1k~\cite{zhai2019vtab}.}
\label{fig:sensitive_moco}
\end{figure}

\begin{figure}[tb]
\begin{center}
    \includegraphics[width=0.8\linewidth]{rebuttal_fgvc_sensitivity.pdf}
\end{center}
\caption{Sensitivity patterns under 0.4M trainable parameters for Oxford Flowers~\cite{nilsback2008automated}, Stanford Cars~\cite{gebru2017cars}, and Stanford Dogs~\cite{Khosla_FGVC2011dogs}. We show the proportions of the sensitive
parameters for the query $\mW_{q}$, key $\mW_{k}$, value $\mW_{v}$, and $\mW_{o}$ weight matrices in the multi-head self-attention layer and two weight matrices $\mW_{fc1}$ and $\mW_{fc2}$ in the feed-forward network. 
}
\label{fig:sens_fgvc}
\end{figure}

\begin{figure}[htb]
\begin{center}
    \includegraphics[width=0.6\linewidth]{variance.pdf}
\end{center}
\caption{Comparisons of sensitivity variances across backbones with different pre-training strategies on \vtab{}.}
\label{fig:variance}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.6\linewidth]{natural_structured.pdf}
\end{center}
\caption{Dataset samples from \imagenet~\cite{krizhevsky2012imagenet} and \vtab{}~\cite{zhai2019vtab}. Samples from Natural tasks of \vtab{} ((a), (b), and (c)) are relatively more similar to the source \imagenet{} samples compared to the ones from Structured tasks of \vtab{} ((d), (e), and (f)).}
\label{fig:domain}
\end{figure}

\section{Dataset Samples for the Source and Target Domains}
\label{subsec:supp_visual}
We visualize some sampled images from the source domain (\imagenet~\cite{krizhevsky2012imagenet}) and the target domains (\vtab{}~\cite{zhai2019vtab}) in Figure~\ref{fig:domain}. We observe that the images from the Natural tasks of \vtab{} are relatively more similar to the source domain compared to those from the Structured tasks of \vtab{}, which aligns with our observation that Structured tasks have large domain gaps. As structured tuning
improves the performance of Structured datasets (Section 4.3 of the main paper), we speculate that
structured tuning facilitates mitigating such large domain gaps.

\begin{table}[t]

\scriptsize
\resizebox{\textwidth}{!}{%
    \begin{tabular}{lc| cccccc}
    \toprule
      &  Tuned / Total &\bf{\cub{}} 
  &\bf{\nabirds{}}
  &\bf{\flowers{}} &\bf{\dogs{}} &\bf{\cars{}}
  &\bf{Mean Acc.} \\
    \midrule
    \band \fullft{} & 100\% &87.3 &82.7 &98.8 &89.4 &84.5 &88.5\\
    \midrule
        \multicolumn{8}{c}{\bf{Addition-based methods}}\\
    \midrule
    \mlp{}-3 & 1.50\% &85.1 &77.3 &97.9 &84.9 &53.8 &79.8
    \\
    \shallowprompt{} & 0.31\% & 86.7 &78.8 &98.4 &\underline{90.7} &68.7 &84.6\\
    \deepprompt{} & 0.98\% & \underline{88.5} &\underline{84.2} &\underline{99.0} &90.2 &83.6 &89.1\\
    \adapter{}-8 & 0.39\% & 87.3 &\textbf{84.3} &98.4 &88.8 &68.4 &85.5\\
    \adapter{}-32 & 0.95\% & 87.2 &\textbf{84.3} &98.5 &89.6 &68.4 &85.6\\
    \adaptformer{} & 0.44\% & 84.7 &75.2 &97.9 &84.7 &83.1 &85.1\\
    \SPTa{} & 0.41\% & \textbf{89.1} &83.3 &\textbf{99.2} &90.5 &\underline{85.6} &\underline{89.5}\\
    \SPTa{} & 0.47\% & \textbf{89.1} &83.3 &\textbf{99.2} &\textbf{91.1} &\textbf{86.2} &\textbf{89.8}\\
    \midrule
     \multicolumn{8}{c}{\bf{Reparameterization-based methods}}\\
    \midrule
    \linear{} & 0.12\% & 85.3 &75.9 &97.9 &86.2 &51.3 &79.3\\
    \partialft{}-1 & 8.38\% &85.6 &77.8 &98.2 &85.5 &66.2 &82.6\\
    \bias{} & 0.13\% &\underline{88.4} &\textbf{84.2} &98.8 &\underline{91.2} &79.4 &88.4\\
    \lora{}-8 & 0.55\% &84.9 &79.0 &98.1 &88.1 &79.8 &86.0 \\
    \lora{}-16 & 0.90\% &85.6 &79.8 &98.9 &87.6 &72.0 &84.8 \\
    \SPTl{} & 0.41\% &\textbf{88.6} &82.8 &\underline{99.4} &\textbf{91.4} &\underline{84.5} &\underline{89.3} \\
    \SPTl{} & 0.60\% &\textbf{88.6} &\underline{83.4} &\textbf{99.5} &\textbf{91.4} &\textbf{87.3} &\textbf{90.1} \\
\bottomrule
    \end{tabular}}
    \caption{
    Per-task results on the FGVC benchmark from Table~1 of the main paper. ``Tuned / Total'' denotes the fraction of the trainable parameters. Top-1 accuracy (\%) is reported. The best result is in \textbf{bold}, and the second-best result is \underline{underlined}.
}\label{tab:full_fgvc}
\end{table}


\begin{sidewaystable}[t]
\scriptsize
\resizebox{\textwidth}{!}{%
    \begin{tabular}{lc | cccccccc | ccccc | ccccccccc}
    \toprule
    & & \multicolumn{8}{c|}{\textbf{Natural}} & \multicolumn{5}{c|}{\textbf{Specialized}} & \multicolumn{9}{c}{\textbf{Structured}} \\
    & \rotatebox{90}{Tuned / Total} & \rotatebox{90}{\bf{Cifar100}} & \rotatebox{90}{\bf{Caltech101}} & \rotatebox{90}{\bf{DTD}} & \rotatebox{90}{\bf{Flower102}} & \rotatebox{90}{\bf{Pets}} & \rotatebox{90}{\bf{SVHN}}  & \rotatebox{90}{\bf{Sun397}} & \rotatebox{90}{\bf{Mean Acc.}} & \rotatebox{90}{\bf{Camelyon}}  & \rotatebox{90}{\bf{EuroSAT}}   & \rotatebox{90}{\bf{Resisc45}}  & \rotatebox{90}{\bf{Retinopathy}} & \rotatebox{90}{\bf{Mean Acc.}} & \rotatebox{90}{\bf{Clevr-Count}} & \rotatebox{90}{\bf{Clevr-Dist}}  & \rotatebox{90}{\bf{DMLab}} & \rotatebox{90}{\bf{KITTI-Dist}}  & \rotatebox{90}{\bf{dSpr-Loc}} & \rotatebox{90}{\bf{dSpr-Ori}}   & \rotatebox{90}{\bf{sNORB-Azim}}  & \rotatebox{90}{\bf{sNORB-Ele}} & \rotatebox{90}{\bf{Mean Acc.}}   \\
    \midrule
\band \fullft{} & 100\% &68.9 &87.7 &64.3 &97.2 &86.9 &87.4 &38.8 &75.9 &79.7 &95.7 &84.2 &73.9 &83.4 &56.3 &58.6 &41.7 &65.5 &57.5 &46.7 &25.7 &29.1 &47.6

    \\\midrule
     \multicolumn{22}{c}{\bf{Addition-based methods}}
    \\\midrule
    \mlp{}-3 & 1.50\% &63.8 &84.7 &62.3 &97.4 &84.7 &32.5 &49.2 &67.8 &77.0 &88.0 &70.2 &56.1 &72.8 &47.8 &32.8 &32.3 &58.1 &12.9 &21.2 &15.2 &24.8 &30.6\\
    \shallowprompt{} & 0.31\%  & 77.7 &86.9 &62.6 &97.5 &87.3 &74.5 &51.2 &76.8 &78.2 &92.0 &75.6 &72.9 &79.7 &50.5 &58.6 &40.5 &67.1 &68.7 &36.1 &20.2 &34.1 &47.0\\
    \deepprompt{} & 0.98\% &78.8 &90.8 &65.8 &98.0 &88.3 &78.1 &49.6 &78.5 &81.8 &96.1 &83.4 &68.4 &82.4 &68.5 &60.0 &46.5 &72.8 &73.6 &47.9 &32.9 &37.8 &55.0\\
    \adapter{}-8  & 0.39\% & 69.2 & 90.1 & 68.0 & 98.8 & 89.9 & 82.8 & 54.3 & 79.0 & 84.0 & 94.9 & 81.9 & 75.5 & 84.1 & 80.9 & 65.3 & 48.6 & 78.3 & 74.8 & 48.5 & 29.9 & 41.6 & 58.5\\
    \adapter{}-32 & 0.71\% & 68.7 & 92.2 & 69.8 &98.9 & 90.3& 84.2& 53.0& 79.6& 83.2& 95.4& 83.2& 74.3 & 84.0 & 81.9 & 63.9& 48.7 & 80.6& 76.2& 47.6& 30.8& 36.4 & 58.3 \\
    \noah{} & 0.50\% & 69.6 & 92.7 & 70.2 & 99.1 & 90.4 & 86.1 & 53.7 & 80.2 & 84.4 & 95.4 & 83.9 & 75.8 & 84.9 & 82.8 & 68.9 & 49.9 & 81.7 & 81.8 & 48.3 & 32.8 & 44.2 & 61.3\\
    \SPTa{} & 0.30\% & 72.9 & 93.2 & 72.5 & 99.3 & 91.4 & 84.6 & 55.2 & 81.3 & 85.3 & 96.0 & 84.3 & 75.5 & 85.3 & 82.2 & 68.0 & 49.3 & 80.0 & 82.4 & 51.9 & 31.7 & 41.2 & 60.8\\
    \SPTa{} & 0.44\% & 72.9 & 93.2 & 72.5 & 99.3 & 91.4 & 88.8 & 55.8 & 82.0 & 86.2 & 96.1 & 85.5 & 75.5 & 85.8 & 83.0 & 68.0 & 51.9 & 81.2 & 82.4 & 51.9 & 31.7 & 41.2 & 61.4\\
    \midrule
     \multicolumn{22}{c}{\bf{Reparameterization-based methods}}
    \\\midrule
    \linear{} & 0.12\% & 63.4 &85.0 &63.2 &97.0 &86.3 &36.6 &51.0 &68.9 &78.5 &87.5 &68.6 &74.0 &77.2 &34.3 &30.6 &33.2 &55.4 &12.5 &20.0 &9.6 &19.2 &26.8\\
    \partialft{}-1 & 8.38\% &66.8 &85.9 &62.5 &97.3 &85.5 &37.6 &50.6 &69.4 &78.6 &89.8 &72.5 &73.3 &78.5 &41.5 &34.3 &33.9 &61.0 &31.3 &32.8 &16.3 &22.4 &34.2\\
    \bias{} & 0.13\% &72.8 &87.0 &59.2 &97.5 &85.3 &59.9 &51.4 &73.3 &78.7 &91.6 &72.9 &69.8 &78.3 &61.5 &55.6 &32.4 &55.9 &66.6 &40.0 &15.7 &25.1 &44.1\\
\lora{}-8 & 0.55\% & 67.1 & 91.4 & 69.4 & 98.8 & 90.4 & 85.3 & 54.0 &79.5 & 84.9 & 95.3 & 84.4 & 73.6 & 84.6 & 82.9 & 69.2 & 49.8 & 78.5 & 75.7 & 47.1 & 31.0 & 44.0 & 60.5 \\
\lora{}-16 & 0.90\% & 68.1 & 91.4 & 69.8 & 99.0 & 90.5 & 86.4 & 53.1 &79.8 & 85.1 & 95.8 & 84.7 & 74.2 & 84.9 & 83.0 & 66.9 & 50.4 & 81.4 & 80.2 & 46.6 & 32.2 & 41.1 & 60.2 \\
\SPTl{} & 0.31\% & 72.3 & 93.0 & 72.5 & 99.3 & 91.5 & 86.2  & 55.5 & 81.5 & 85.0 & 96.2 & 85.1 & 75.9 & 85.6 & 83.7 & 66.4 & 52.5 & 80.2 & 80.1 & 51.1 &  30.1 & 41.3 & 60.7 \\
\SPTl{} & 0.63\% & 73.5 & 93.3 & 72.5 & 99.3 & 91.5 & 87.9 & 55.5 & 81.9 & 85.7 & 96.2 & 85.9 & 75.9 & 85.9 & 84.4 & 67.6 & 52.5 & 82.0 & 81.0 & 51.1 &  30.2 & 41.3 & 61.3 \\
\bottomrule
    \end{tabular}}
    \caption{
    Per-task results on the \vtab{} benchmark from Table~1 of the main paper. ``Tuned / Total'' denotes the fraction of the trainable parameters. Top-1 accuracy (\%) is reported.
}\label{tab:full_vtab}
\end{sidewaystable}
