\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[table,xcdraw]{xcolor}
\input{math_commands.tex}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{rotating}
%\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
  
  \makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{citecolor}{HTML}{0071bc}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,  citecolor=citecolor,bookmarks=false]{hyperref}

\usepackage{xcolor}
\newcommand{\bohan}[1]{\textcolor{black}{#1}}


\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2240} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\input{micro}
\def\revise{\textcolor{black}}

\definecolor{mypink}{RGB}{255,105,180}
\def\rev{\textcolor{black}}

\begin{document}
%%%%%%%%% TITLE
\title{Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning}

\author{%
  Haoyu He$^{1}$ ~~ Jianfei Cai$^{1}$ ~~ Jing Zhang$^{2}$  ~~ Dacheng Tao$^{2}$ ~~ Bohan Zhuang$^{1}\thanks{Corresponding author. E-mail: $\tt  bohan.zhuang@gmail.com$}$ \\ [0.25cm]
$^1$ ZIP Lab, Monash University \quad $^2$ The University of Sydney \\[0.1cm]
% 
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Visual Parameter-Efficient Fine-Tuning (PEFT) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing PEFT methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel \textbf{S}ensitivity-aware visual \textbf{P}arameter-efficient fine-\textbf{T}uning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of sensitive parameters exceeds a pre-defined threshold by utilizing existing structured tuning methods, e.g., LoRA~\cite{hu2022lora} or Adapter~\cite{houlsby2019parameter}, 
to replace directly tuning the selected sensitive parameters (unstructured tuning) under the budget. Extensive experiments on a wide range of downstream recognition tasks show that our SPT is complementary to the existing PEFT methods and largely boosts their performance, e.g., SPT improves Adapter with supervised pre-trained ViT-B/16 backbone by 4.2\% and 1.4\% mean Top-1 accuracy, reaching SOTA performance on FGVC and VTAB-1k benchmarks, respectively. Source code is at \url{https://github.com/ziplab/SPT}.
\end{abstract}





%%%%%%%%% BODY TEXT
\input{1-introduction}
\input{2-related}
\input{3-method}
\input{4-experiments}

\section{Conclusion}
\vspace{-0.3em}
In this paper, we have explored identifying and allocating trainable parameters to task-specific important positions for visual parameter-efficient tuning. Specifically, we have proposed a novel criterion to quickly measure the sensitivity of the pre-trained parameters for each specific task before fine-tuning. Based on the parameter sensitivity, we have proposed a trainable parameter allocation strategy that adaptively combines both unstructured and structured tuning under a desired trainable parameter budget, enabling high representational capability and flexibility. Finally, we have conducted extensive experiments on a total of 24 downstream recognition tasks with both plain and hierarchical vision Transformer backbones under different pre-training strategies to demonstrate the versatility and effectiveness of our proposed SPT. Notably, we have shown that our approach is complementary to the existing PEFT methods and improves their performance significantly. In the future, we will explore adapting large vision models to more downstream tasks with SPT, \eg, dense prediction and vision-and-language tasks, and improve the training efficiency of SPT for on-device training~\cite{cai2020tinytl,lin2022device}. 

\noindent\textbf{Acknowledgement.} We thank Jing liu and Ziyi Liu for their helpful discussions. This research is partially supported by Monash FIT Start-up Grant. Dr. Jing Zhang is supported by the Australian Research Council project FL-170100117.


\clearpage

\bibliographystyle{abbrv}
{\small
 \bibliography{egbib}
}


\input{5-appendix}


\end{document}
