 \section{LIMITATIONS}

The focus of this work is on improving the approximation of the cross-entropy loss and the quality of the samples from the softmax distribution. For some tasks, it may be the case that different objectives, including multi-task, pretraining, contrastive, and others may lead to models that generalize better. After applying \nystrom, the approximate re-encoding step, and the sampling approximations, we lose theoretical properties. Future work could investigate both why these approximations work well and how to bound their approximation quality.

\section{ETHICAL CONSIDERATIONS}

The proposed approach is subject to the same biases and ethical considerations as the dual encoders used as the base model. The reduction in error cannot be assumed to reduce or exacerbate the potentially negative aspects of such an encoder model. The biases and considerations of any such retrieval/classification task need to be carefully considered as with any model. The implications of negative sampling via uniform vs. hard methods will likely impact the decision boundary of the model (as observed in our empirical experiments). Understanding how the training method relates to biases in the data, labels, targets, would be an important consideration.