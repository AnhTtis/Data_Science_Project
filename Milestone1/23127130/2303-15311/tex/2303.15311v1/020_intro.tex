\vspace{-3mm}
\section{INTRODUCTION}

Dual encoder models map input queries and output targets to a common vector space in which inner products of query and target vectors yield an accurate  similarity function.  
They are a highly effective, widely deployed solution for classification and retrieval tasks 
such as passage retrieval \citep{karpukhin2020dense}, question answering \citep{qu2021rocketqa}, recommendation systems \citep{wu2020joint}, entity linking \citep{gillick2019learning}, and fine-grained  classification \citep{xiong2022extreme}.
These tasks are characterized as having a large number of targets, often on the order of millions to billions.
Dual encoders achieve scalability to this large number of targets in two ways: weight sharing among targets through a parametric encoder and an efficient inner product-based scoring function.
The encoder models are often parameterized as deep neural networks, e.g., transformers \citep{vaswani2017attention,devlin2019bert}, and trained with the cross-entropy loss between the model's softmax distribution and query's true labeled target(s).

Training dual encoder models \emph{efficiently} and \emph{effectively} poses two key challenges:

\noindent\textbf{Computationally intensive loss function.} Computing the gradient of the softmax partition function becomes computationally costly when the number of possible targets is large \citep[inter alia]{bengio2008adaptive,daume2017logarithmic,lindgren2021efficient}, necessitating approximation.  
The common approach approximates the large sum in the partition function gradient by sampling relatively few of its largest terms originating from ``hard'' negative targets, which are often found using an efficient nearest-neighbor index \citep{guu2020retrieval,agarwal2022entity}.
However, the approximation introduces bias in gradient estimation affecting learning and resulting accuracy~\citep{rawat2019sampled, ajalloeian2020convergence}. 


\noindent\textbf{Moving embeddings.}
The embedded representations of both queries and targets continuously change during training as the underlying encoder parameters are updated. 
Since re-embedding all targets after each step of training is computationally infeasible,
prior work uses `stale' representations for negative mining, i.e., the vector representation from the encoder parameters from $t$ steps ago.  
Re-encoding and re-indexing even at moderately-sized intervals remains an expensive operation \citep{izacard2022few}. 

In this paper, we present a new dual-encoder training algorithm, which we call \ours,
that addresses both of the above challenges, 
and we provide both theoretical and empirical analysis. To elaborate:

\noindent\textbf{Efficient gradient bias reduction}. 
The expensive term in the exact gradient computation is the evaluation of an expectation with respect to the entire softmax distribution.
To approximate this expectation, we design a Metropolis-Hastings sampler that uses a tree-structured hierarchical clustering of the targets to provide an efficient proposal distribution in \S\ref{sec:sample_cov_sm}. We then relate bias reduction to the granularity of the clustering.
We also present an efficient method for dynamically updating this tree structure in response to encoding parameter gradients. 

\noindent \textbf{Efficient re-embedding}. 
Instead of frequently re-running the updated (typically fairly large and expensive) target encoder model to produce up-to-date embeddings of targets, we propose to approximate (with effective end-task performance) the effect of a gradient update on the target embeddings with an efficient Nystr\"{o}m low-rank approximation (generally using several orders of magnitude less time and memory)  in \S\ref{sec:lowDimApprox}. 

\noindent \textbf{Theoretical analysis}. 
We study the running time of our algorithm under mild assumptions such as Lipschitz encoder functions and expansion rate, showing per-step cost is a function of the number of clusters in our approximation,  considerably smaller than the number of targets itself (Prop.~\ref{proposition:normalizedError}, Prop.~\ref{thm:rejection}). 
We also present a bound on the bias of our softmax estimator, ensuring convergence with stochastic gradient descent (Prop.~\ref{proposition:gradientBias}).

\noindent \textbf{Empirical performance}. 
In \S\ref{sec:exp}, we evaluate our algorithm on two passage retrieval task datasets, {\it Natural Questions (NQ)} \citep{kwiatkowski2019natural} and {\it MSMARCO} \citep{bajaj2016ms}.
On NQ, which has over twenty million targets, we find that our approach cuts error by half in relation to a practically-infeasible oracle brute-force negative mining. Previous state-of-the-art methods can incrementally increase accuracy by increasing memory usage at training time; yet we find that our method still surpasses previous state-of-the-art when using 150x less accelerator memory.