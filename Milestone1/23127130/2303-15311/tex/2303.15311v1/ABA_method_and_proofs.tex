\section{EFFICIENT AND ACCURATE SAMPLES FROM THE SOFTMAX DISTRIBUTION}

\subsection{Rejection Sampling}
\label{sec:rejection_sampling}

Rejection sampling for the softmax distribution follows closely the proof in \cite{zaheer2017canopy} for mixture models. 

We sample from: 
\begin{align*}
    \lbl \sim \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})}{\sum_{\lbl'}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})})}
\end{align*}
and accept with probability
\begin{align*}
  \expNegRejError \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})}.
\end{align*}

If we have
\begin{align}
     \expNegRejError \leq \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})} \leq \expRejError.
 \end{align}
Then, if we want determine the probability of sampling a particular target, $\lbl$, denoted $\textsf{Pr}(\lbl)$. Producing $\lbl$ can be done by \textcolor{teal}{ sampling and accepting $\lbl$ or} and \textcolor{purple}{sampling and rejecting another target $\lbl'$ and then accepting $\lbl$ in one of the subsequent rounds of sampling}.
%
\newcommand{\acceptY}{{\color{teal} \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} \expNegRejError \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})} }}
\newcommand{\rejectOther}{\color{purple} \textsf{Pr}(\lbl) \sum_{y' \in \labelset} \left ( 1 - \expNegRejError \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl')})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})})} \right ) 
\frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})})}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})}
}
%
\begin{equation}
\resizebox{.99\linewidth}{!}{%
$\begin{aligned}
    \textsf{Pr}(\lbl) &= \acceptY + \rejectOther \\
    &= \expNegRejError \frac{ \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc({\lbl})})}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} + \frac{\textsf{Pr}(\lbl)}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} \sum_{y' \in \labelset} \left ( 1 - \expNegRejError \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl')})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})})} \right ) \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})}) \\
    &= \expNegRejError \frac{ \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc({\lbl})})}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} + \frac{\textsf{Pr}(\lbl)}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} \sum_{y' \in \labelset} \left ( \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})}) - \expNegRejError {\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl')})} \right )  \\
    &= \expNegRejError \frac{ \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc({\lbl})})}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} + \frac{\textsf{Pr}(\lbl)}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} \left ( \sum_{y' \in \labelset}  \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})}) - \expNegRejError \sum_{y' \in \labelset}  {\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl')})} \right )  \\
    &= \expNegRejError \frac{ \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc({\lbl})})}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} + \textsf{Pr}(\lbl) - \frac{\textsf{Pr}(\lbl)}{\sum_{\lbl''}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl''})})} \expNegRejError Z  \\
    \textsf{Pr}(\lbl) &= \frac{1}{Z} \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc({\lbl})})
\end{aligned}$}
\end{equation} 
And so the rejection sampling strategy will sample from the true softmax distribution.

\subsection{Metropolis-Hastings Approximate Softmax}
\label{sec:mh_appendix}

A common approach for providing samples from a distribution that is difficult to sample from is the Metropolis-Hastings algorithm. 
Recall that Metropolis-Hastings produces a sample from $P(\lbl|\datapoint)$ by iteratively sampling a state change from a proposal distribution, denoted $\approxsoftmax$, and determines whether or not to `accept' the state change based on an \emph{acceptance ratio}.
In particular, we will use an \emph{independent} Metropolis-Hastings method. That is the proposal distribution $\approxsoftmax$ is independent of the current state of the Markov chain. For the $t$-state in the chain, we have a proposal distribution of the form $\lbl^{(t)} \sim \approxsoftmax(\lbl|\datapoint,\lbl^{(t-1)}) = \approxsoftmax(\lbl|\datapoint)$. 
The acceptance ratio is defined as:
\begin{equation}
    \acceptanceratio(\lbl^{(t-1)}, \lbl^{(t)}) = {\min} \left (1, \frac{P(\lbl^{(t)}|\datapoint)}{P(\lbl^{(t-1)}|\datapoint)}
    \frac{\approxsoftmax(\lbl^{(t-1)}|\datapoint)}{\approxsoftmax(\lbl^{(t)}|\datapoint)} \right )
\end{equation} 

\subsection{Proofs for \S\ref{sec:search}: Hierarchical Clustering Structures}

\unnormalizedError*

\begin{proof}

\begin{align}
   \frac{\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition)}})}{\exp(\beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})} 
    &\leq \exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition)}) - \lblenc(\lbl)}) \\ 
     &\leq \exp(\beta \norm{\dataenc(\datapoint)}_2 \norm{\lblenc(\lbl^{(\partition)}) - \lblenc(\lbl)}_2) &\text{\mycomment{Cauchy-Schwartz}}\\  \\ 
     &\leq \exp(\beta \norm{\lblenc(\lbl^{(\partition)}) - \lblenc(\lbl)}_2) &\text{\mycomment{Assumption~\ref{assumption:unitNorm}}}\\ 
     &\leq \exp(\beta \ctbase^\ell) &\text{\mycomment{Covering Property}}. 
\end{align}

\end{proof}

\normalizedError*
\begin{proof}

Let $\ell$ be the level of the clustering $\partition$ used.

\begin{align}
    \max_{\lbl \in \labelset} \frac{P(y|x)}{Q(y|x;\partition_{(\ell)})} &= \max_{\lbl \in \labelset}  \frac{\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl})}{\exp(\beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})})}} \cdot \frac{\hat{Z}}{Z} \\
    & \leq  \max_{\lbl \in \labelset} \frac{\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl})}{\exp(\beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})})}} \cdot \max_{\lbl' \in \labelset}  \frac{\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl'^{(\partition_{(\ell)})})})}{\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl'})} 
\end{align}
The above inequality follows by property of mediant. Now define:
\begin{align}
    R_1 & \triangleq \max_{\lbl \in \labelset} 
    \frac{
        \exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl)})
    }
    {
        \exp(\beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})})})
    }  \\
    R_2 & \triangleq  \max_{\lbl' \in \labelset}  \frac{
        \exp(\beta\ip{\dataenc(\datapoint)}{\lblenc({\lbl'}^{(\partition_{(\ell)})})})
    }{
        \exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl')}))
    }  .
\end{align}

Observe for $R_1$:
\begin{align}
    \max_{\lbl \in \labelset} 
    \frac{
        \exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl)})
    }
    {
        \exp(\beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})})})
    }   &=  
    \max_{\lbl \in \labelset} {
        \exp(\beta \ip{
            \dataenc(\datapoint)}
            {\lblenc(\lbl)} - \beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})})})
    } \\
    &= \max_{\lbl \in \labelset} {\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl) -   \lblenc(\lbl^{(\partition_{(\ell)})})})} \\
    &\leq \max_{\lbl \in \labelset} \exp(\beta \norm{\dataenc(\datapoint)}_2 \norm{\lblenc(\lbl) -   \lblenc(\lbl^{(\partition_{(\ell)})})}_2) \\
    &\leq \max_{\lbl \in \labelset} \exp(\beta \norm{\lblenc(\lbl) -   \lblenc(\lbl^{(\partition_{(\ell)})})}_2)
    \leq \exp(\beta \ctbase^{\ell})
\end{align}
Similarly for $R_2$:
\begin{align}
    \max_{\lbl \in \labelset}  \frac{\exp(\beta \ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})}})}{\exp(\beta\ip{\dataenc(\datapoint)}{\lblenc(\lbl)}} 
    \leq \max_{\lbl \in \labelset} \exp(\beta \norm{\lblenc(\lbl^{(\partition_{(\ell)})}) -   \lblenc(\lbl)}_2)
    \leq \exp(\beta \ctbase^{\ell})
\end{align}

Combining above two:
\begin{align}
    \max_{\lbl \in \labelset} \frac{P(y|x)}{Q(y|x;\partition)} \leq R_1 \times R_2 \leq  \exp(\beta \ctbase^{\ell}) \exp(\beta \ctbase^{\ell}) = \exp(2\beta \ctbase^{\ell})
\end{align}

Thus, to maintain maximum ratio $\gamma$, we need to select $\ell$ such that:
\begin{align}
    \exp(2\beta \ctbase^{\ell}) & \leq \gamma \\
    \ctbase^\ell &\leq \frac{1}{2\beta} \log \gamma 
\end{align}
which is as stated in the proposition.
\end{proof}

\changeFlat*
\begin{proof}
Let,
\begin{align}
    \ctbase^\ell &\leq \frac{1}{2\beta} \log \gamma \\
    \ctbase^{\ell-1} &\leq \frac{1}{2\beta} \log {\gamma'},
\end{align}
then,
\begin{align}
    \frac{1}{b}\ctbase^\ell &\leq \frac{1}{b} \frac{1}{2\beta} \log \gamma \\
    \ctbase^{\ell-1} &\leq \frac{1}{2\beta} \log {\gamma}^{\frac{1}{b}},
\end{align}
and so $\gamma'\leq \gamma^{\frac{1}{b}}$.
\end{proof}

\rejectionSamplingThm*
\begin{proof}
We want to show two properties of the rejection sampling algorithm, its running time and its correctness (e.g., that it samples from the true posterior).

The proof follows very closely to the analogous algorithm for mixture models by \cite{zaheer2017canopy}.

First, we want to show that the running time is $\bigo(|\partition_{(\ell)}| + \textsf{BF} e^{\ctbase^\ell})$ where $\textsf{BF}$ is the branching factor for cover/SG trees ($\bigo(\alpha^4)$ and $\bigo(\alpha^3)$ respectively). We begin by considering the expected number of rejections from the first level $\ell$. Based on our definition of $Q$, the number of samples is upper bounded by $e^{\beta \ctbase^{\ell+2}}$, therefore the number of rejections is $e^{\beta \ctbase^{\ell+2}}-1$. If we consider the next level down, there would be $e^{\beta \ctbase^{\ell+1}}-1$ rejections, the level after that $e^{\beta \ctbase^{\ell}}-1$, $e^{\beta \ctbase^{\ell-1}}-1$, etc. We can use the branching factor $\textsf{BF}$ to give a bound on how expensive the sampling step is at every level. We are interested therefore in:
\begin{align}
   \textsf{BF} \sum_{k=1}^\infty \left (e^{\beta \ctbase^{\ell-k}}-1 \right ).
\end{align}
We have that $e^x -1 \leq xe^a$ for $x\in[0,a]$ and $\sum_{k=1}^\infty \ctbase^{-k} = 1$ and so: 
\begin{align}
   \textsf{BF} \sum_{k=1}^\infty \left (e^{\beta \ctbase^{\ell-k}}-1 \right ) \leq \textsf{BF} \cdot e^{\beta \ctbase^{\ell}} \sum_{k=1}^\infty \ctbase^{-k} = \textsf{BF} \cdot e^{\beta\ctbase^{\ell}}
\end{align}
We then need to consider the cost of the initial sampling step, which is not $\textsf{BF}$, but rather depends on the number of clusters in the partition $|\partition_{(\ell)}|$, leading to $\bigo(|\partition_{(\ell)}| + \textsf{BF}\cdot e^{\beta \ctbase^\ell})$. It is important to note here that while the running time of the algorithm depends only on the branching factor of the trees, the depth of the SG tree differs from that of the cover tree. The depth of the SG tree is $\bigo(\log(\frac{d_\textrm{max}}{d_\textrm{min}}))$ where $d_\textrm{max}$ is the largest pairwise distance and $d_\textrm{min}$ the minimum pairwise distance (this ratio also know as the aspect ratio), whereas the depth of the cover tree is $\bigo(\alpha^2\log |\labelset|)$. 

Next, let's consider the correctness of the method. We want to determine the probability of sampling a particular target $\lbl$, denoted $\textsf{Pr}(y)$. Recall that to sample $y$, we need to follow the path in the tree from level $\ell$ to the level in which $y$ first appears as a cluster representative and accept that cluster. We will refer to this level in which $y$ first appears as $k$. There is a path of selected clusters/nodes $\cluster_\ell,\cluster_{\ell-1},\dots,\cluster_k,$, where we indicate the level of the selected node in the subscript. Let $\mathcal{A}(\lbl)$ be the probability of accepting $\lbl$. To reduce the complexity of notation, define: 
\begin{align}
    \textsf{w}_{\cluster} = \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster)})
\end{align}
We can write $\mathcal{A}(\lbl)$ as:
\begin{align}
    \mathcal{A}(\lbl) &= \frac{1}{{Z}_\ell} e^{\beta \ctbase^\ell} \blacktriangle_{\cluster_\ell} \textsf{w}_{\cluster_\ell} &\text{\mycomment{Sample at top level}} \\
    & \times \left [ \prod_{j=k+1}^\ell \left (1-\frac{\frac{1}{|\labelset|} \textsf{w}_{\cluster_j}}{e^{\beta\ctbase^j} \blacktriangle_{\cluster_j} \textsf{w}_{\cluster_j}} \right ) \frac{e^{\beta\ctbase^{j-1}}\blacktriangle_{\cluster_{j-1}} \textsf{w}_{\cluster_{j-1}}}{e^{\beta\ctbase^j} \blacktriangle_{\cluster_{j}} \textsf{w}_{\cluster_j} - \frac{1}{|\labelset|} \textsf{w}_{\cluster_j} } \right ] &\text{\mycomment{Rejecting and selecting path to $\cluster_k$}} \\
    &\times \frac{\frac{1}{|\labelset|}\textsf{w}_{\cluster_k}}{e^{\beta\ctbase^k \blacktriangle_{\cluster_{k}} \textsf{w}_{\cluster_k}}} &\text{\mycomment{Accepting the given cluster $\cluster_k$}} \\
    &= \frac{1}{{Z}_\ell} e^{\beta \ctbase^\ell} \blacktriangle_{\cluster_\ell} \textsf{w}_{\cluster_\ell} \left [\prod_{j=k+1}^\ell \frac{e^{\beta\ctbase^{j-1}} \blacktriangle_{\cluster_{j-1}} \textsf{w}_{\cluster_{j-1}}}{e^{\beta\ctbase^{j}} \blacktriangle_{\cluster_{j}} \textsf{w}_{\cluster_{j}}} \right ] \frac{\frac{1}{|\labelset|} \textsf{w}_{\cluster_{k}}}{e^{\beta\ctbase^{k}} \blacktriangle_{\cluster_{k}} \textsf{w}_{\cluster_{k}}} \\
    &= \frac{1}{{Z}_\ell} e^{\beta \ctbase^\ell} \blacktriangle_{\cluster_\ell} \textsf{w}_{\cluster_\ell} \left [ \frac{e^{\beta\ctbase^{k}} \blacktriangle_{\cluster_{k}} \textsf{w}_{\cluster_{k}}}{e^{\beta\ctbase^{\ell}} \blacktriangle_{\cluster_{\ell}} \textsf{w}_{\cluster_{\ell}}} \right ] \frac{\frac{1}{|\labelset|} \textsf{w}_{\cluster_{k}}}{e^{\beta\ctbase^{k}} \blacktriangle_{\cluster_{k}} \textsf{w}_{\cluster_{k}}} \\
    &= \frac{1}{{Z}_\ell} \frac{1}{|\labelset|} \textsf{w}_{\cluster_{k}}
\end{align}
Now, we have to consider the probability $\mathcal{R}$ of restarting the sampler, e.g., 
\begin{align}
    \mathcal{R} = 1-\sum_{\lbl'}\mathcal{A}(\lbl') = 1 - \sum_{\lbl'} \frac{1}{{Z}_\ell} \frac{1}{|\labelset|} \textsf{w}_{\cluster_{k}} = 1 - \frac{1}{|\labelset|}\frac{Z}{Z_\ell}
\end{align}
Finally, consider $\textsf{Pr}(\lbl)$:
\begin{align}
    \textsf{Pr}(\lbl) &= \mathcal{A}(\lbl) +  \textsf{Pr}(\lbl) \cdot \mathcal{R} \\
    &= \frac{1}{{Z}_\ell} \frac{1}{|\labelset|} \textsf{w}_{\lbl} + \textsf{Pr}(\lbl)  - \textsf{Pr}(\lbl)  \frac{1}{|\labelset|}\frac{Z}{Z_\ell} \\
    \textsf{Pr}(\lbl) &= \frac{1}{Z}  \textsf{w}_{\lbl}
\end{align}
Therefore, the algorithm samples from the true softmax.
\end{proof}


\searchCorrectness*
\begin{proof}
Recall that Proposition~\ref{proposition:normalizedError} ensures that the initial partition achieves $\max_{\lbl \in \labelset} \frac{P(\lbl|\datapoint)}{Q(\lbl|\datapoint;\partition_{(\ell)})} = \gamma$. For any cluster in the partition discovered by the algorithm, $\cluster \in \partition$, let $k < \ell$ be level of the cluster. We therefore have: 
\begin{align}
    \max_{\lbl \in \cluster} \frac{P(\lbl|\datapoint)}{Q(\lbl|\datapoint;\partition)} \leq \exp(\beta \ctbase^k) \leq \exp(\beta \ctbase^\ell).  
\end{align}

The covering property and triangle inequality ensures that if $\norm{\dataenc(\datapoint) - \lblenc(\cluster)}_2 > \ctbase^k + \ctbase^{m}$, then every target in $\cluster$ at least $\ctbase^m$ from ${\dataenc(\datapoint)}$.

\end{proof}


\subsection{Proofs for \S\ref{sec:covertree_estimaor}: Gradient-Bias of Our Estimator}

\gradientBias*
\begin{proof}

Observe that:
\begin{align}
    \nabla_\Theta {\loss}(\datapoint_i,\lbl_i) &= -\invtemp \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)} + \invtemp \E_{P}[\nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)}] \\ 
    \E[\nabla_\Theta {\hat{\loss}}(\datapoint_i,\lbl_i)] &= -\invtemp \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)} + \invtemp \E_{Q_\textsf{MH}}[\nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)}] \\
    \E[\nabla_\Theta {\hat{\loss}}(\datapoint_i,\lbl_i)] - \nabla_\Theta {{\loss}}(\datapoint_i,\lbl_i) &= \beta \E_{Q_\textsf{MH}}[\nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)}] - \beta \E_{P}[\nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)}] \\
    \E[\nabla_\Theta {\hat{\loss}}(\datapoint_i,\lbl_i)] - \nabla_\Theta {{\loss}}(\datapoint_i,\lbl_i) &=\invtemp \sum_{y} \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)} (P(y|x) - Q_\textsf{MH}(y|x)) 
\end{align}

Now using the bound on the total variation:
\begin{align}
    \norm{\E[\nabla_\Theta {\hat{\loss}}(\datapoint_i,\lbl_i)] - \nabla_\Theta {{\loss}}(\datapoint_i,\lbl_i)} &= \bignorm{\invtemp \sum_{y} \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)} (P(y|x) - Q_\textsf{MH}(y|x))} \\ 
    &= \invtemp \sum_{y}\norm{ \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)}} \cdot |(P(y|x) - Q_\textsf{MH}(y|x)| \\ 
    &\leq \invtemp \sum_{y} M |(P(y|x) - Q_\textsf{MH}(y|x)| \\ 
    &= \invtemp M \|P - Q_\textsf{MH}\|_1 \\ 
    &= 2\invtemp M \|P - Q_\textsf{MH}\|_\text{TV} \\
    &\leq 2 \invtemp M \epsilon 
\end{align}
\end{proof}

\subsection{Proofs for \S\ref{sec:dynamic}: Dynamic Maintenance of the Tree Structure}

\textbf{Proposition} \ref{proposition:pairwiseAmountOfChange} \emph{Under Assumptions~\ref{assumption:lipchitz},\ref{assumption:boundedGradient},\ref{assumption:unitNorm},
let $\phi_t$ and $\phi_{t+w}$ refer to the model parameters and model parameters after $w$ more steps of gradient descent with learning rate $\eta$.}
\begin{equation}
   \left | \norm{\lblencPTstep{t} - f_{\phi_{t}}(\lbl')}_2 - \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(\lbl')}_2 \right | \leq 4w\eta \beta LM.
    \label{eq:changedAmount}
\end{equation}


\begin{proof}
We analyze this with similar techniques and assumptions as \cite{lindgren2021efficient}.
First consider a bound on the gradient norm. Let $k$ by the number of negative samples where the negative samples are given by $N_s = \{y_{s_j} : j \in [k]\}$
\begin{align}
   \norm{\nabla_\Theta {\hat{\loss}}(\datapoint_i,\lbl_i)} &= \bignorm{-\invtemp \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)} + \invtemp \frac{1}{k} \sum_{j=1}^{k} \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_{y_{s_j}})}} \\ 
   &\leq \invtemp \norm{\nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)}} + \invtemp \frac{1}{k} \bignorm{ \sum_{j=1}^{k} \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_{y_{s_j}})}} \\
   &\leq \invtemp \norm{\nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)}} + \invtemp \frac{1}{k}  \sum_{j=1}^{k}\bignorm{ \nabla_\Theta \ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_{y_{s_j}})}} \\
   &\leq 2\invtemp  M
\end{align}

Now, consider the difference of the dual encoder parameters at timestep, $t$, $\allparameters_{t}$ and at timestep, $t+w$, $\allparameters_{t+w}$.
\begin{align}
    \allparameters_{t} - \allparameters_{t+w} &= \sum_{i=t}^{t+w} \eta \nabla_\allparameters \hat{\loss}(x_i,y_i) \\
    \norm{\allparameters_{t} - \allparameters_{t+w}} &= \bignorm{\sum_{i=t}^{t+w} \eta \nabla_\allparameters \hat{\loss}(x_i,y_i)} \\
     &\leq \sum_{i=t}^{t+w} \eta \bignorm{\nabla_\allparameters \hat{\loss}(x_i,y_i)} \\
    &= w \eta 2 \invtemp M
\end{align}

Applying the triangle inequality
\begin{align}
    &\left | \norm{\lblencPTstep{t} - f_{\phi_{t}}(y')} - \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(y')} \right | \\
    &\leq \left | \norm{\lblencPTstep{t} - f_{\phi_{t+w}}(y)} +  \norm{\lblencPTstep{t+w} - f_{\phi_{t}}(y')} - \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(y')} \right | \\
    &\leq \left | \norm{\lblencPTstep{t} - \lblencPTstep{t+w}} +  \norm{f_{\phi_{t+w}}(y') - f_{\phi_{t}}(y')} +  \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(y')} - \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(y')} \right | \\
    &= \left | \norm{\lblencPTstep{t} - \lblencPTstep{t+w}} +  \norm{f_{\phi_{t}}(y') - f_{\phi_{t+w}}(y')} \right |  
\end{align}
Recall that the dual encoders satifies the Lipschitz assumption (Assumption~\ref{assumption:lipchitz}):
\begin{align}
      \norm{\lblencPTstep{t} - \lblencPTstep{t+w}}  &\leq  L\norm{\Theta-\Theta'} \leq w \eta 2 \invtemp L M \ \ \ \forall y
\end{align}

And so:
\begin{align}
& \left | \norm{\lblencPTstep{t} - f_{\phi_{t}}(y')} + \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(y')} \right | \leq 4w\eta \invtemp LM. 
\end{align}
\end{proof}



