\section{BACKGROUND}
\vspace{-1mm}
Given a data point $\datapoint \in \dataset$ (e.g., a query),
we are tasked with predicting a target $\lbl \in \labelset$ (e.g., a passage answering the question). In our experiments, the number of targets is large, such as tens of millions. 
We assume that the targets $\lbl$ are featurized. We use \emph{encoder} models to represent
both the points and targets. These encoders, which map a point or target's features to a fixed dimensional embedding, are often large parametric pre-trained models (such as transformers \citep{vaswani2017attention,devlin2019bert}).
We denote the encoder model for points as $\dataenc(\datapoint) \in \RR^{\dimensionality}$ and for targets as  $\lblenc(\lbl) \in \RR^{\dimensionality}$. The softmax distribution is:
\begin{equation}
\resizebox{.88\linewidth}{!}{%
$\begin{aligned}
P(\lbl|\datapoint) =\frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})}{Z \triangleq \sum_{\lbl' \in \labelset} \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl')})},
\end{aligned}$
}\label{eq:softmax}
\end{equation}

where $\invtemp$ denotes the inverse temperature hyperparameter.

We train the parameters of the encoder models $\allparameters = \{\dataparameters,\lblparameters\}$, given labeled training data pairs $(\datapoint_1,\lbl_1),\dots,
(\datapoint_\numtraining,\lbl_\numtraining)$. Our training objective is the cross-entropy loss, which for a given training pair is defined as:
\begin{equation}
    \loss(\datapoint_i,\lbl_i) = - \invtemp\ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)} + \log Z. 
\end{equation}
As mentioned by \cite{rawat2019sampled}, most methods use first order optimization, so we consider the gradient wrt $\allparameters$:
\begin{equation}
\begin{aligned}
\nabla_\allparameters \loss(\datapoint_i,\lbl_i) &= - \nabla_\allparameters \invtemp\ip{\dataenc(\datapoint_i)}{\lblenc(\lbl_i)} + \nabla_\allparameters \log Z  \\
\nabla_\allparameters \log Z &= \E_{y \sim P(y|x_i)} \nabla_\allparameters \invtemp\ip{\dataenc(\datapoint_i)}{\lblenc(\lbl)}. \nonumber
\end{aligned}
\end{equation}
Training with cross-entropy loss is computationally challenging because computing the partition function, $Z$, or its gradient requires $|\labelset|$ inner-products. Furthermore, it requires $|\labelset|$ encoding calls to the target encoding model. This latter challenge is unique to training dual encoders and is not a challenge when targets have their own free parameters (e.g., \citet{daume2017logarithmic,sun2019contextual}).

Many works replace the expensive full expectation computation with a Monte Carlo estimate from a small constant number of samples obtained in different ways~\citep{henderson2017efficient,reddi2019stochastic, rawat2019sampled,karpukhin2020dense,lindgren2021efficient}. 
In our work, we design a novel proposal distribution from which it is efficient to sample, and which has bounded gradient bias,
to ensure faster convergence of SGD \citep{ajalloeian2020convergence}. We refer to our approximate loss as $\hat{\loss}$.

Our work will use the same, mild assumptions of many previous works \citep{rawat2019sampled,lindgren2021efficient}.

\begin{restatable}{assum}{lipschitzAssumptions}
\label{assumption:lipchitz} \emph{\textbf{(Lipschitz Encoders)}}
 The dual encoders are $L$-Lipschitz in parameters $\Theta$, that is $\norm{\dataenc(\datapoint) - \dataencprime(\datapoint)} \leq L \norm{\Theta-\Theta'}$ and $\norm{\lblenc(\lbl) - \lblencprime(\lbl)} \leq L\norm{\Theta-\Theta'}$ for all $\datapoint \in \dataset$, $\lbl \in \labelset$.
\end{restatable}
\begin{restatable}{assum}{boundedGradient}
\label{assumption:boundedGradient}
\emph{\textbf{(Bounded Gradients)}} We assume that the logits have bounded gradients, $\norm{\nabla\ip{\dataenc(\datapoint)}{\lblenc(\lbl)}} < M$.
\end{restatable}

\begin{restatable}{assum}{unitNorm}
\label{assumption:unitNorm}
\emph{\textbf{(Unit Norm)}} Dual-encoders produce unit normed vector embeddings\footnote{We note that it is common practice to unit norm the representations from dual encoders, e.g., \citep{gillick2019learning,rawat2020doubly,lindgren2021efficient}}, $\forall \lbl \in \labelset,\ \norm{\lblenc{(\lbl)}} = 1$.
\end{restatable}

