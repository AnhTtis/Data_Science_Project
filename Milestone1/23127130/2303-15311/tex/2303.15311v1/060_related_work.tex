\section{RELATED WORK}

\textbf{Per-Target Free Parameters}. 
A large body of work trains classifiers with a massive number of targets. In much of this work, the targets are directly parameterized with a $d$ dimensional vector, rather an embedding produced by an encoder \citep{bengio2008adaptive,choromanska2015logarithmic,jernite2017simultaneous,daume2017logarithmic,sun2019contextual,yu2020pecos}. Most closely related to our work are methods that adaptively re-arrange tree-structured classifiers \cite[inter alia]{sun2019contextual,kobus21aOnline}.
\cite{blanc2018adaptive,rawat2019sampled} use kernel-based approximations of the softmax, which gives strong theoretical guarantees. Future work could consider how to extend these methods to the dual-encoder setting.

\textbf{Partition Functions \& Probabilistic Models.} 
MCMC methods are widely used for posterior inference \citep[inter alia]{
neal1993probabilistic,neal2000markov,chang2013parallel,zaheer2016exponential,zaheer2017canopy}. These methods are concerned with finding a high quality estimate of the distribution as the end task. In our setting, we need an estimate for every training point in each step of training. Most similar (and the inspiration for our approach) is the Canopy-sampler \citep{zaheer2017canopy}, which uses a cover tree to derive an efficient rejection sampler/Metropolis-Hastings algorithm for exponential family mixture models. Unlike that work, we consider approximation of the softmax, moving embedded representations during training, and the relationship between the approximation quality and the gradient bias of our estimator. \cite{vembu2009probabilistic} use MCMC-based approximations of distributions with large output spaces, but does not use the clustering-based approximations presented here nor does it consider the dual-encoder setting.

\textbf{Reparameterization, discrete distributions}. OS* sampling, the Gumbel-Max trick, and Perturb-and-MAP, are alternative methods for sampling from distributions such as the softmax  \citep{dymetman2012algorithm,tucker2017rebar,maddison2016concrete,paulus2020gradient,jang2016categorical, huijben2022review}. Future work could consider combining such methods and our approach. 

\textbf{Nearest Neighbor Search, Clustering, Dynamic Structures}. Cover Trees \citep{beygelzimer2006cover} and SG Trees \citep{zaheer2019sg} were originally used as nearest neighbor indexes. There are many tree and DAG-based index structures that support addition and deletion of items such as Navigating Nets \citep{krauthgamer2004navigating},  HNSW \citep{malkov2014approximate}, and NN-Descent \citep{dong2011efficient}. Also, closely related is work on maintaining dynamic hash-based indexes \citep{jain2008online,zhang2020continuously}. Apart from nearest neighbor index structures, scalable methods for hierarchical clustering organize large datasets into tree structures \citep{bateni2017affinity,moseley2019framework,dhulipala2022hierarchical}. Other work builds these clusterings in an incremental or dynamic setting \citep{liberty2016algorithm,choromanska2012online,kobren2017hierarchical,menon2019online}. Maintaining structures in a dynamic setting is studied in graph algorithms (e.g., minimum spanning tree).   Efficient and parallelizable dynamic graph algorithms exist \cite[inter alia]{Sleator1981ADS,holm2001poly,tseng2019batch,dhulipala2020parallel}.

\noindent \textbf{Cover Trees.} New algorithms and extensions of cover trees have been developed \cite[inter alia]{curtin2013tree,curtin2015plug,izbicki2015faster,zaheer2019sg,elkin2021new,gu2022parallel}. Cover trees are also widely used in other settings such as $k$-means clustering \citep{curtin2017dual} and Gaussian processes \citep{terenin2022numerically}. 

\noindent \textbf{Task Specific Related Work.} Learned models for passage retrieval and entity linking are extensively studied \cite[inter alia]{wu2019scalable,karpukhin2020dense,bhowmik2021fast,qu2021rocketqa,thakur2021beir,ren2021rocketqav2,ni2021large,fitzgerald-etal-2021-moleman,gao2021condenser,dai2022promptagator,izacard2022few}. Most similar to our work is ANCE \citep{xiong2020approximate}, which uses a nearest neighbor index for a contrastive objective. Other work includes alternative architectures to dual encoders \citep{khattab2020colbert,qian2022multi,santhanam2021colbertv2} and learning efficient hashing-based representations \citep{yamada2021efficient}.  