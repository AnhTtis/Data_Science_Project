
\section{EFFICIENT RE-ENCODING OF TARGETS}
\label{appendix:LowDim}

In this section, we describe in more detail
the re-encoding part of \ours. 
Let's recap the structure of the re-encoding model.
Recall that we keep a cached, low-dimensional version of each encoded target. We denote this cache as  $Y_t'\subset \mathbb{R}^{d'}$ where $d'$ is the number of landmarks used in \nystrom\ / the reduced dimensionality and $t$ represents the number of gradient steps. Note that while we denote separate caches for each time step, we need not physically store multiple such caches. The cache from the previous time step can be overwritten during the update. 

We refer to $Y_t$ as the set of target embeddings in their full dimensional space. This set $Y_t$ is never fully instantiated. 
We have a set of $d'$ \emph{landmark} points, which are kept fixed during training and which were sampled uniformly at random. We will store these landmarks in their full dimensional form. We denote the set of landmarks as $\mathscr{S}$. We also have a set of training points used to fit the regression model, $\mathscr{R}$, which maps target embeddings in $Y_t$ to form $Y_{t+w}$. Each time we fit $\mathscr{R}$, we sample $s'$ targets uniformly at random and build a training dataset for the regressor using the updated dual encoder, $f_{\phi_{t+w}}$, specifically, $(\vec{y_1},\lblencPTstepNoLbl{t+w}(y_1)),\dots(\vec{y_{s'}},\lblencPTstepNoLbl{t+w}(y_{s'}))$. We only need to store the regression training dataset points and the landmarks in their full dimension form. For all other targets, we can compute their approximate full dimensional representation using $\mathscr{R}$ and then immediately project into lower dimensional space $d'$ using the landmarks/\nystrom. We define the regression model $\mathscr{R}$ to be a low-rank, \nystrom, regression model with the same landmarks $\mathscr{S}$. Therefore, the input to the regression model can be the low-dimensional representations. 

In summary, the approximate re-encoding procedure would,  re-encode $s'$ points using the new encoder model, $\lblencPTstepNoLbl{t+w}$, build a training dataset for the regression model $\mathscr{R}$, fit the regression model $\mathscr{R}$, use $\mathscr{R}$ to update the landmark point representation $\mathscr{S}$, use  $\mathscr{R}$ and $\mathscr{S}$ to build the low-dimensional embeddings for all targets in $Y_t'$ as well as the corresponding $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}$ projection matrix. This is summarized in Algorithm~\ref{alg:reEncode} (with initialization in Algorithm~\ref{alg:initialEncoding}), which is called as a subroutine of the overall training algorithm (Algorithm~\ref{alg:dynnibalComplete}).

It is important to note that we do not use these approximate
re-encoded targets at evaluation time. We only use these 
dimensionality reduced targets at training time. 

Lastly, we note that while we sample the landmark points from only the targets empirically, one could (in a more principled way) sample from all datapoints and targets.

\subsection{Building SG Trees with \nystrom Representations}
\label{sec:nystromSG}

When we use our \nystrom-based lower dimensional representations, the unit norm assumptions about the targets no longer apply. At query time, for a given point $x$, we can think about it as corresponding to some row $i$ in the $\mathbf{KS}$ matrix. Similarly each target $y$ to some column $j$ in $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}\mathbf{S}^T\mathbf{K}$. This corresponds to using the $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}\mathbf{S}^T\mathbf{K}$ representations as the target cluster representatives in the tree structure. However, when building (or re-building) the tree structure, we need to be able to measure the similarity between two targets, i.e., using both $\mathbf{KS}$ and $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}\mathbf{S}^T\mathbf{K}$. We only need to use the $\mathbf{KS}$ representations during construction / re-building time however and if memory is a concern, we could only store a single representation and use the $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}$ when measuring (dis)similarity. 

Finally, we need to be able to use the SG Tree despite using inner product similarities (rather than a proper distance measure). Rather than the approach presented by \cite{zaheer2019sg}, we find that converting to distance by $\exp(-\mathbf{KS}(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}\mathbf{S}^T\mathbf{K})$ is more effective. We find that setting the base of the SG tree to be 1.05 seems to work well with this conversion of similarities to distances. Note that these modifications mean that even running the exact nearest neighbor search algorithm may result in approximations. However, we find empirically that this structure is sufficient for training dual encoders.

\begin{algorithm}
\caption{Approximate Re-Encode}
\begin{algorithmic}[1]
\STATE Let $\mathscr{S}$ refer to the set of landmark points 
\STATE Sample $s'$ targets uniformly at random from the collection of targets to form $S'_t$.
\STATE Let $Y'_{S'_t}$ be the low dimensional representations of the training points. 
\STATE Build regression training dataset  $(\vec{y_1},\lblencPTstepNoLbl{t+w}(y_1)),\dots(\vec{y_{s'}},\lblencPTstepNoLbl{t+w}(y_{s'}))$ using the updated encoder model. 
\STATE Train $\mathscr{R}$ on $(\vec{y_1},\lblencPTstepNoLbl{t+w}(y_1)),\dots(\vec{y_{s'}},\lblencPTstepNoLbl{t+w}(y_{s'}))$ \mycomment{$\mathscr{R}$ is a low-rank regression model using the same landmarks $\mathscr{S}$.}
\STATE Update the embedding of landmark representatives $\mathscr{S}$ using $\mathscr{R}$. Update \nystrom projection matrix $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}$
\STATE Produce new low-dimensional embeddings for all targets $Y'_{t+w}$ using $\mathscr{R}$.
\end{algorithmic}
\label{alg:reEncode}
\end{algorithm} 


\begin{algorithm}
\caption{Initial Target Encoding}
\begin{algorithmic}[1]
\STATE Sample landmarks $\mathscr{S}$ uniformly at random
\STATE Encode landmarks, fit \nystrom.
\STATE Encode all of the targets with the encoder model $\lblencPTstepNoLbl{}$ and use \nystrom to produce low dimensional representations $Y'_{0}$
\end{algorithmic}
\label{alg:initialEncoding}
\end{algorithm} 