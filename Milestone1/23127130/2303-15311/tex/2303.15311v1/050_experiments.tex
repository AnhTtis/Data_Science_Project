
\begin{table*}
\vspace{-2mm}
    \centering
    \begin{tabular}{@{}l c c c c c c@{}}
    \toprule
         Performance  & Mem. \% & R@1 & R@5 & R@10 & R@20 & R@100 \\
         \midrule
         In-batch Negatives& 0.0\% &  0.356 &  0.613 &  0.695 &  0.757 &  0.843  \\
         Uniform Negatives& 0.0\% & 0.386 &  0.644 &  0.723 &  0.775 &  0.848  \\
         \rowcolor{red!10} \ours (This Paper) &  0.3\% &  0.485 &  0.695 &  0.754 &  0.801 &  0.862  \\
         Stochastic Negative Mining & 1.0\% & 0.444 &  0.672 &  0.739 &  0.785 &  0.855 \\
         \midrule
         Stochastic Negative Mining &  3\% & 0.461 &  0.689 &  0.750 &  0.794 &  0.860 \\
         Negative Cache & 6.3\% & - & - & - & 0.784 & 0.856 \\
         Stochastic Negative Mining & 10\% & 0.468 &  0.690 &  0.758 &  0.798 &  0.862 \\
         \midrule
         Exhaustive Brute Force & 100\% &  0.500 &  0.700 &  0.765 &  0.804 &  0.866 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Natural Questions}. We report the performance on the test set using the answer string retrieval recall \citep{karpukhin2020dense}. Methods are listed in ascending order of required accelerator memory (as \% of Exhaustive Brute Force memory). Scaling to many targets requires an approach that does not need accelerator memory, since storing such targets on the accelerator will become a limiting bottleneck in terms of memory.  \ours uses lower dimensional representations in CPU memory, an efficient tree update, and approximate re-encoding during training. \ours performs significantly better than the practical approaches with $\leq1.0\%$ memory. Error with respect to the Exhaustive Brute Force method is cut by half in terms of R@1 with respect to all methods other than the impractical 10\% memory Stochastic Negative Mining. In more detail, \ours (0.485 R@1) cuts error to 1.5 points compared to Exhaustive Brute Force (0.500), compared to Stochastic Negative Mining 3\% (0.461) cuts error to 3.9 points. Compared to  Stochastic Negative Mining 10\% (0.468) which cuts error to 3.2, \ours cuts error by over 2x more. In terms of R@5, \ours also observes a strong 2 point gain over the low-memory variant of Stochastic Negative Mining (1.0\% Mem.)}
    \label{tab:nq_table}
    \vspace{-4mm}
\end{table*}

\begin{table}[!htb]
    \vspace{2mm}
    \centering
    \begin{tabular}{@{}l@{} c c c c @{}}
    \toprule
         MRR  & Mem. \% & @1 & @10 & @100 \\
         \midrule
         In-batch Negatives & 0 & 0.140 & 0.242 & 0.254 \\
         Uniform Negatives & 0 &  0.196  & 0.305 & 0.316 \\
         Negative Cache & 0.06\% & - & 0.310 & - \\
         Negative Cache & 0.24\% & - & 0.315 & - \\
         \rowcolor{red!10} \ours   &   0.76\%  &  0.223 &  0.334  & 0.345  \\
         Negative Cache & 0.96\%   & - & 0.323 & - \\
         Stochastic Neg. & 1.0\% &  0.200  & 0.309 & 0.320  \\
         \midrule
         Stochastic Neg. & 3.0\% &  0.216  & 0.331 & 0.342  \\
         Negative Cache & 3.8\%  & - & 0.322  & - \\
         Negative Cache & 15.15\% & -  & 0.331 & - \\
         \midrule
         Exhaustive & 100\% &  0.228 & 0.345 & 0.356  \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{MSMarco Results}. \ours again outperforms competing methods that use similar low percentage memory requirements. In particular, \ours outperforms 1\% memory Stochastic Negative Mining in terms of MRR@1 by more than 2 points. Furthermore, we see that even when Negative Cache uses 15.15\% memory, a 150x increase compared to our approach, \ours still produces a higher MRR@10 result.}
    \label{tab:msmarco_table}
\end{table}

\vspace{-4mm}
\section{EXPERIMENTS}
\label{sec:exp}
\vspace{-2mm}

We compare our proposed approach, \ours, to multiple state-of-the-art methods for training dual encoders. We evaluate on two retrieval datasets and also the entity linking dataset, {Zeshel} \citep{logeswaran2019zero}, in  \S\ref{app:zeshel}.

\noindent \textbf{Natural Questions (NQ)} \citep{kwiatkowski2019natural} is a dataset for passage retrieval. The data points are natural language questions. The targets are passages from Wikipedia. There are over 21 million targets and about 60K training examples. We report results using the string-match-based recall evaluation that is used by previous work \citep{karpukhin2020dense,lindgren2021efficient}. 

\noindent \textbf{MSMARCO} \citep{bajaj2016ms} contains 8.8 million targets and 500K training examples. Data points are natural language questions and targets are passages extracted from web documents. The task is to provide the correct passage for a given question. Following previous work, we report  the mean reciprocal rank. 


We compare the following methods with dual encoders as transformer \citep{vaswani2017attention} initialized from pretrained RoBERTa base \citep{liu2019roberta}. See details in \S\ref{app:empirical_details}.

\noindent\textbf{In-batch Negatives}. We approximate the softmax distribution by only using the positive target labels from within the batch as in previous work (e.g., \cite{henderson2017efficient})

\noindent\textbf{Uniform Negatives}. Sample targets uniformly at random from the collection of targets and use this to approximate the softmax distribution (e.g.,  \cite{karpukhin2020dense}). 

\noindent\textbf{Stochastic Negative Mining} \citep{reddi2019stochastic}. A large number of targets is sampled uniformly at random. These targets are stored with their stale representations on the accelerator device. From this large number of targets, we approximate the softmax distribution with $k$ hard negatives. We periodically, e.g. every 100 or 500 steps refresh and change the negatives stored on the accelerator device. The performance of this method depends on how many targets are stored on the accelerator device. In most settings, memory becomes a bottleneck before the computational burden of computing pairwise similarities on the accelerator. We report performance in terms of this memory bottleneck, relative to the overall dataset size. 

\noindent\textbf{Negative Cache} \citep{lindgren2021efficient}. A recent approach that keeps track of a collection of targets in a cache on accelerator similar to Stochastic Negative Mining. However, rather than randomly refreshing, negatives are added and removed the targets in a streaming manner, FIFO or LRU. We report results directly from the published paper. 

\noindent\textbf{Oracle Exhaustive Brute-Force}. We exhaustively compute all logits and exhaustively find the top-$k$ closest targets for training. Including this method provides insights into upper-bound empirical results, but it is impractical 
in practice since it is 
extremely expensive in computation and accelerator memory, and thus also financial cost. 
While we obtain results from this ``oracle'' method on the datasets above, running on meaningfully larger data would not have been possible even if computation/budget were no object.



\noindent\textbf{\ours}. 
This paper's proposed cluster and approximate re-encoding approach. We note that the only portion of our training method sitting on accelerator memory is the low-rank regression-based approximate re-encoding model (\S\ref{sec:approxReencode}), the landmark points, and regression training data. Our use of lower dimensional (64 or 128) \nystrom embeddings along with scalable and relatively lightweight index structures stored in cheap CPU memory would allow our approach to scale to billions of targets, and further scale to many billions by leveraging the tree structure for additional targeted truncations and cluster-based approximations.  

Empirically, we find that the Metropolis-Hasting-based sampling outperforms rejection sampling. We find using lower temperature and approximating the sampling procedure to ensure that ``harder'' negatives are selected is beneficial to end task performance. Rather than considering the entire partition of targets, $\partition$, returned by Algorithm~\ref{alg:covertreesearchforpartition}, we consider the top-$k$ closest clusters. We set $m=-11$ (MSMARCO) and $m=-14$ (NQ) as the deepest level of clusters and restrict the size of frontier in Algorithm~\ref{alg:covertreesearchforpartition} to be 100. We run chains of length 2. Finally, as another empirical approximation, we select the top scoring targets from all the chains. We discuss these choices further in \S\ref{app:empirical_details}. 



% \vspace{-9mm}
\subsection{Empirical Results}
\vspace{-4mm}
We report the performance of each method in terms of each dataset's given evaluation metric. Alongside the performance, we report the accelerator (GPU/TPU) memory requirement as a percentage with respect to the Oracle Exhaustive Brute-Force approach (storing all targets as full dimensional embeddings). 
As noted when describing Stochastic Negative Mining, we find that the main bottleneck for our competitors is the memory required to store target embeddings on the accelerators (along with the transformer encoders), not the computation of logits.
Thus of crucial importance are \ours's advantages over baselines with respect to reduced need for accelerator memory.

Table~\ref{tab:nq_table} shows the results on the test set for NQ. We observe that \ours outperforms In-batch Negatives, Uniform Negatives, and Stochastic Negative Mining using 1.0\% memory in all recall metrics.  In terms of recall at 1, \ours cuts the performance gap with respect to exhaustive brute force by more than half compared to all methods except 10\% memory Stochastic Negative Mining. Even using 10\% memory with Stochastic Negative Mining compared to 0.3\% for \ours leaves large performance gaps in terms of recall at 1. Stochastic Negative Mining achieves 0.468 versus \ours's 0.485.

Table~\ref{tab:msmarco_table} reports performance for all methods on MSMARCO in terms of mean reciprocal rank. Following past work \citep{lindgren2021efficient}, we evaluate on the development set. We find that performance on MSMARCO follows a similar trend as NQ in that \ours outperforms all of the other low-memory approaches. In fact, even using 150x more memory Negative Cache still does not perform as well as \ours. \ours sees a multiple point improvement in MRR@1 compared to competing methods. 

We consider the time needed to maintain the SG tree structure during training. We find on MSMARCO that, given a collection of updated target embeddings, our dynamic method (\S\ref{sec:dynamic}) is about 4x faster than the traditional approaches that rebuild the entire structure. Furthermore, we find that our regression-based approximate re-encoding is about 8x faster than running the encoder model on MSMARCO, and requires much less accelerator expense. In terms of training steps-per-second efficiency between index maintenance, our approach does involve more computation: it is about four times slower than Stochastic Negative Mining's simple logit-based selection on a small subset of targets; however, some of that is balanced by faster index updates. We expect that additional engineering design could significantly speed up our method; in any case, running Stochastic Negative Mining for more training steps did not increase its accuracy.