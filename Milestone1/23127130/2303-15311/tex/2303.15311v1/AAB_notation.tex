

\begin{table}[]
    \centering
    \begin{tabular}{@{}ll@{}}
    \toprule
    \bf Symbol & \bf Definition \\
    \midrule
        $\datapoint$ & A data point / the features of a data point, e.g. input to the data dual encoder. \\
        $\dataset$ & The set of data points \\
        $\lbl$ & A target / the features of a target, e.g. input to the target dual encoder. \\
        $\labelset$ & Set of all targets \\
        $\loss$ & Loss function, in particular cross entropy \\
        $\hat{\loss}$ & Our approximate, sampling based loss function \\
        L & Lipschitz value, Assumption~\ref{assumption:lipchitz} \\
        M & Bound on gradient of logits, Assumption~\ref{assumption:boundedGradient}  \\
        $\dataenc$ & The data point dual encoder \\
        $\lblenc$ & The target dual encoder \\
         $\Theta$ & The parameters of both the data and target dual encoders. \\
         \midrule
        $Z$ & The partition function for the softmax.    Eq~\ref{eq:softmax}. \\
        $\hat{Z}$ & The approximated partition function using the clustering-based approach. Eq~\ref{eq:approxclustersoftmax}. \\
        $Z_k$ & The normalizer constant when descending to level $k$ in the rejection sampling approach. \\
        $P$, $P(y|x)$ & True, exact softmax distribution \\
        $\approxsoftmax$ & The proposal distribution for Metropolis-Hastings. Eq~\ref{eq:approxclustersoftmax}. \\
        $\approxsoftmax_\textsf{MH}$ & The distribution over labels given by Metropolis Hastings sampling procedure. See Appendix~\ref{sec:mh_appendix}.\\
        $\expNegRejError$ & Used for unnormalized probability error term in rejection sampling \\
        $d$ & Dimensionality of dual encoder output. \\
         \midrule
        $\partition$ & A clustering of the targets. \\
        $\lbl^{(\partition)}$ & The cluster assignment of target $\lbl$. Overloaded to also be the cluster's representative (or its features).  \\
        $\cluster$ & A cluster of targets, e.g. $\cluster \subseteq \labelset$. \\
        $\lblenc(\lbl^{(\partition)})$,$\lblenc(\cluster)$ & The encoded representation of the cluster's representative. \\
        $\hc$ & A hierarchical clustering / cover or SG Tree of the targets \\
        $\ctbase$ & The base parameter of the cover / SG tree \\
        $\children{(\cluster)}$ & The children of a node in the hierarchical clustering/cover tree. \\
        $\partition_{(\ell)}$ & The partition associated with level $\ell$ of the cover tree. \\
        $Y_{(\ell)}$ & The set of cluster representatives for the partition associated with level $\ell$ of the cover tree. \\
        $\alpha$ & The expansion constant used in our theoretical analysis~\ref{def:expansionConstant}\\
        $\textsc{maxd}(\cluster)$ & The maximum distance between the cluster representative of $\cluster$ and one of its descendants in an SG Tree. \\ 
        $\textsc{mind}(\cluster)$ & The minimum distance between one of the pairs of children of $\cluster$ in an SG Tree. \\ 
        \midrule
        $\gamma$ & The upper bound on the ratio of the true softmax distribution, $\max_y \frac{P(y|x)}{Q(y|x;\partition)} \leq \gamma$ \\
        $w$ & Number of steps of gradient descent. \\
        $s$ & The Metropolis Hastings chain length \\
        $\eta$ & Gradient descent learning rate \\
        $d_{max}$ & Maximum pairwise distance among all pairs of points \\
        $d_{min}$ & Minimum pairwise distance among all pairs of points \\
        \midrule
        $\mathbf{S}$ & Binary matrix representing landmarks for \nystrom \\
        $\mathscr{S}$ & Set of landmarks for \nystrom \\
        $\mathbf{K}$ & Pairwise similarity matrix for \nystrom \\
        $\mathscr{R}$ & Low-rank (\nystrom) regression model to approximately re-encode the targets \\
        $Y'_{S'_t}$ & The low dimensional representations of the training points for $\mathscr{R}$. Not to be confused \\
        &  with $Y_{(\ell)}$, the representatives in a level of the cover tree.  \\
         \bottomrule
    \end{tabular}
    \caption{\textbf{Summary of Notation Used}}
    \label{tab:notation_table}
\end{table}