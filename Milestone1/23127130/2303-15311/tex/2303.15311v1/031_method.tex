\section{EFFICIENT AND ACCURATE SAMPLES FROM THE SOFTMAX DISTRIBUTION}
 \label{sec:sample_cov_sm}
 
In this section, we present our approach for maintaining a dynamic tree-structured clustering that supports efficient and provably accurate sampling from the softmax distribution. In the next section (\S \ref{sec:lowDimApprox}), we will show a novel use of low-rank regression-based approximations to obviate the need for using the encoder to produce updated embeddings and describe the complete training algorithm.  
Proofs for all statements are relegated to the Supplement.
Given the properties of the proposed method, we refer to our approach as \ours, in reference to \textbf{Dy}namic \textbf{N}earest \textbf{N}eighbor \textbf{I}ndex for \textbf{B}ias-reduced \textbf{A}pproximate \textbf{L}oss
(Figure~\ref{fig:main}).

 
 \newcommand{\rejError}{\epsilon_\textrm{r}}
 \newcommand{\expRejError}{e^{\rejError}}
 \newcommand{\expNegRejError}{e^{-\rejError}}
 \subsection{Accurate Samples from Softmax Distribution}
 \label{sec:cluster_based_proposal}
 We would like to accurately 
 sample from the softmax distribution, without having to compute the computationally intensive
 partition function, $Z$. We consider
 familiar methods: rejection sampling and Metropolis-Hastings. 
 
 To apply rejection sampling, we approximate the unnormalized softmax probability for each target $y$ with an approximation $\hat{y}$ such that:
 \begin{align}
     \expNegRejError \leq \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})} \leq \expRejError.
 \end{align}
Then, if we sample from: 
\begin{align}
    \lbl \sim \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})}{\sum_{\lbl'}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl'})})}
\end{align}
and accept with probability
\begin{align}
  \expNegRejError \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\hat{\lbl})})},
\end{align}
we will sample from the softmax, akin to past work on rejection sampling for mixture models \citep{zaheer2017canopy}.

Similarly, we can use Metropolis-Hastings to produce a sample from the true softmax distribution $P(\lbl|\datapoint)$ by iteratively sampling (and accepting/rejecting) a state change from a proposal distribution, denoted $\approxsoftmax$.
The approximation error in terms of the total variation of the distribution given by Metropolis-Hastings, $\approxsoftmax_\textsf{MH}$, compared to the true softmax distribution $P$ using a $s$-length chain can be bounded by \citep{mengersen1996rates,cai2000exact,bachem2016fast}:
\begin{equation}
\resizebox{.88\linewidth}{!}{%
$\begin{aligned}
    ||P - \approxsoftmax_\textsf{MH}||_\textsf{TV} \leq \exp \left (-\frac{s-1}{\gamma} \right ) \quad  \gamma = \max_{\lbl \in \labelset} \frac{P(\lbl|\datapoint)}{\approxsoftmax(\lbl|\datapoint)}.
\end{aligned}$}
\end{equation} 
This means that $s>1+\gamma \log \frac{1}{\epsilon}$ gives $||P - \approxsoftmax_\textsf{MH}||_\textsf{TV} \leq \epsilon$.
 
 We achieve high quality samples if the proposal distribution $\approxsoftmax$ is `close' to the true softmax in terms of the ratio $P/\approxsoftmax$. From high quality samples, we will see that bias is minimized to aid convergence of SGD. 


 \vspace{-2mm}
 \subsection{Clustering-based Approximations}
 \label{sec:covertree}
 \vspace{-2mm}

The main idea of our approach is to build a clustering of the targets that quantizes the true softmax distribution $P$ by assigning each target to a cluster such that targets in the same cluster have the same probability. Ideally, these clustering-based approximations would be efficient to construct/maintain, and would minimize approximation error.

As an introduction, consider a flat-clustering based approach. We denote the clustering of targets as $\partition$. We denote the cluster assignment of the target $\lbl$ as $\lbl^{(\partition)}$. Each cluster in the clustering $\cluster \in \partition,\ \cluster \subseteq \labelset$ is associated with a representative. We overload notation and use $\cluster$ and $\lbl^{(\partition)}$ to refer to both (1) a set of targets (when used in context of clustering) as well as (2) the features of the cluster's representative (when used as input to a dual encoder model). 
\begin{figure}
     \vspace{-3mm}
     \centering
     \includegraphics[width=0.35\textwidth]{zzz_main_figure.pdf}
     \vspace{-2mm}
     \caption{\textbf{Proposed Approach: DyNNIBAL}. Our  approach searches the tree structured index for a clustering of the labels. This clustering is then used to provide an approximate softmax distribution used as a proposal distribution for Metropolis-Hastings. The tree structure supports efficient updates as parameters of the dual encoders change.}
     \label{fig:main}
     \vspace{-3mm}
 \end{figure} 
 
We define the distribution $\approxsoftmax$, by replacing each target with a representative for its cluster. 
\begin{equation}
\resizebox{.88\linewidth}{!}{%
$\begin{aligned}
\approxsoftmax(\lbl|\datapoint;\partition) =\frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition)})})}{\hat{Z}\triangleq\sum_{\cluster \in \partition} | \cluster| \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster)})},
\end{aligned}$
}\label{eq:approxclustersoftmax}
\end{equation}

The complexity of sampling from this distribution is a function of the number of clusters, or rather, we only need to measure similarity between the datapoint $\datapoint$ and each of the cluster representatives, e.g., $\bigo(|\partition|)$ inner products to compute the unnormalized probabilities and partition function. 
Next, we investigate how to discover a clustering of targets, which provides efficient sampling while bounding the error of Metropolis-Hastings and increasing the probability of acceptance for rejection sampling.

\vspace{-2mm}
\newcommand{\ctbase}{b}

\subsection{Hierarchical Clustering Structures}
\vspace{-2mm}
\label{sec:search}

Our method for discovering such a clustering is to use hierarchical clustering, in particular SG Trees \citep{zaheer2019sg}, an efficient variant of cover trees \citep{beygelzimer2006cover}. These efficient hierarchical approaches will allow us to bound the approximation quality and discover an adaptive clustering of targets for each point.

\begin{definition}\textbf{\emph{(Hierarchical Clustering)}}
A hierarchical clustering $\hc$ is a set of nested clusterings. For  $\cluster_\mathsf{par}$, `child' clusters are $\children(\cluster_\mathsf{par})$ s.t. $\forall \cluster_\mathsf{kid} \in \children(\cluster_\mathsf{par}), \cluster_\mathsf{kid} \subsetneq \cluster_\mathsf{par}$ and $\nexists \cluster' \in \hc$ s.t. $\cluster_\mathsf{kid} \subset \cluster' \subset \cluster_\mathsf{par}$. 
\end{definition}
\vspace{-1mm}
%
Cover trees, originally proposed as a nearest neighbor index, are highly scalable to hundreds of millions of targets \citep{zaheer2017canopy} and enjoy theoretical guarantees. 

\begin{definition}\emph{(\textbf{Cover Tree} \citep{beygelzimer2006cover})} A cover tree with base $\ctbase$ is a \emph{level-wise} structure. Levels are clusterings, $\partition_{(\ell)}$. The set of cluster representatives in a level is $Y_{(\ell)}$.
A cover tree maintains the invariants: 
\begin{enumerate}[topsep=0pt,itemsep=0ex,partopsep=1ex,parsep=1ex,leftmargin=5mm]
    \item \textbf{Nesting}. The cluster representatives at a parent level are a subset of the child level, i.e., $Y_{(\ell)} \subseteq Y_{(\ell-1)}$.
    \item \textbf{Covering}. For all $y \in Y_{(\ell-1)}$, there exists a parent node $y_\mathsf{par} \in Y_{(\ell)}$ such that $\norm{\lblenc(y) - \lblenc(y_\mathsf{par})} \leq \ctbase^\ell $
    \item \textbf{Separation}. All distinct nodes in a given $y,y' \in Y_{(\ell)}$, satisfy $\norm{\lblenc(y) - \lblenc(y')} \geq \ctbase^\ell $.
\end{enumerate}
\end{definition}
%
Closely related are SG Trees  ~\citep{zaheer2019sg}:

\begin{definition}\emph{(\textbf{Stable Greedy (SG) Tree} \citep{zaheer2019sg})} An SG tree is a cover tree with separation defined to only apply to siblings rather than nodes in the same level: 
\begin{enumerate}
[topsep=0pt,itemsep=0ex,partopsep=1ex,parsep=1ex,leftmargin=5mm]
  \setcounter{enumi}{2}
    \item \textbf{Separation}. All distinct siblings, $y,y' \in \children{(y_\mathsf{par})}$, satisfy $\norm{\lblenc(y) - \lblenc(y')} \geq \ctbase^\ell $.
\end{enumerate}
\end{definition}
%
Because SG trees are considerably more efficient to construct \citep{zaheer2019sg}, they are the focus of our work. However, where possible, we will also describe how cover trees could be used in our methods.

The cover tree and SG tree data structures are not new to this paper. Their use to provide an approximation to the softmax is novel and is the contribution of this section. 

 We make standard assumptions about the representations \citep[inter alia]{beygelzimer2006cover,zaheer2019sg}. 

\begin{restatable}{definition}{expansionConstant}
The expansion constant $\alpha$ for the encoded  targets $Y = \{\lblenc{(\lbl)}: \lbl \in \labelset\}$ is the smallest $\alpha \geq 2$ such that $|B(p, 2r)| \leq \alpha |B(p,r)|$ for all $p \in Y$ and $r \geq 0$ where $B(p,r)$ denotes a ball of radius $r$ around $p$.
\label{def:expansionConstant}
\end{restatable}

First, notice how a particular level of the tree structure can serve as a clustering used in the approximate distribution. Each cluster in the given level has bounded radius over its descendants (cluster members).
By bounding the approximation error of the unnormalized and normalized probabilities for a selected clustering, we can determine the quality of samples using Metropolis-Hastings as well as  the acceptance probability for rejection sampling.


\begin{restatable}{proposition}{unnormalizedError}
\label{proposition:unnormalizedError}
    Under  Assumption~\ref{assumption:unitNorm}, given the clustering at level $\ell$, $\partition_{(\ell)}$, approximating $\lbl$ with the cluster representative, $\lbl^{(\partition_{(\ell)})}$, satisfies the following with $\rejError = \beta \cdot \ctbase^\ell$:
    \vspace{-1mm}
     \begin{align}
     \expNegRejError \leq \frac{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition_{(\ell)})})})}{\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl)})} \leq \expRejError.
 \end{align}
\end{restatable}

Similarly, to achieve a given bound on the total variation for Metropolis-Hastings, we would like to control $\gamma$, the ratio of the true softmax to our proposal distribution, $\max_{\lbl \in \labelset} \frac{P(y|x)}{Q(y|x;\partition_{(\ell)})} = \gamma$, in terms of the level $\ell$ selected.

\begin{restatable}{proposition}{normalizedError}
\label{proposition:normalizedError}
Given Assumption~\ref{assumption:unitNorm}, to achieve a maximum ratio of true softmax to proposal distribution equal to $\gamma$ i.e., $\max_{\lbl \in \labelset} \frac{P(y|x)}{Q(y|x;\partition_{(\ell)})} = \gamma$, we need the clustering at level $\ell$, where:
$
    \ell \triangleq \max \{\ell \in \mathbb{Z} \ : \ \ctbase^{\ell}  \leq \frac{1}{2\beta} \log \gamma  \}.  
$
\end{restatable}

\begin{restatable}{remark}{changeFlat}
Observe the relationship between $\gamma$ and $\ell$. Descending one more level of the tree to level $\ell-1$, reduces the ratio from $\gamma$ (selecting level $\ell$) to $\gamma^{\frac{1}{b}}$.
\end{restatable}

\vspace{-1mm}
The aforementioned results describe how to achieve a given approximation error by selecting a level of the tree structure. 
Now, let's consider methods which adaptively
use the hierarchical structure to produce 
a sample for a given datapoint. These approaches 
will start with a coarse clustering (e.g., some level $\ell$ satisfying a minimal requirement on the aforementioned sources of error).

First, let's consider a theoretically motivated rejection sampling approach based on the rejection sampling methods for mixture models proposed by \cite{zaheer2017canopy}. The approach works by
iteratively selecting a finer-grained set of cluster representatives to sample from while still being a valid rejection sampler for the softmax distribution. The sampling is modified such that if a given cluster is accepted, we return its representative. We start with the clusters at level $\ell$. We perform one step of rejection sampling. If we accept, we return the cluster representative itself. Otherwise, we descend to that 
cluster's children in level $\ell-1$ and repeat the following procedure until the leaves of the tree. We sample among the children of the node and one specially defined \emph{restart} option, $\uptodownarrow$. If the \emph{restart} option is sampled, we begin the algorithm again at level $\ell$. If we sample a child other the nested self-child and we accept the child's cluster, we return its representative with a given probability. If we sample the nested-child or if we do not accept the sampled child, we descend the tree and consider the children of the sampled node. To be a valid rejection sampler, we maintain running normalizers as shown in Algorithm~\ref{alg:rejection_sample_alg}.

\begin{restatable}{proposition}{rejectionSamplingThm}
Algorithm~\ref{alg:rejection_sample_alg} produces samples from the softmax $P(\lbl|\datapoint)$ in time $\bigo(|\partition_{\ell}| + \alpha^4e^{\beta \ctbase^{\ell+2}})$ for cover trees and $\bigo(|\partition_{\ell}| + \alpha^3 e^{\beta \ctbase^{\ell+2}})$ for SG trees. 
\label{thm:rejection}
\end{restatable}

\vspace{-2mm}
\begin{wrapfigure}{r}{0.51\linewidth}
\vspace{-7mm}
\hspace*{-7mm}
    \includegraphics[width=0.3\textwidth]{zzz_cost.pdf}
    \vspace{-9mm}
\end{wrapfigure}
In Proposition~\ref{thm:rejection}, the first term of the cost corresponds to initial computation and second term to quality of the proposal.
If we pick $\ell$ to be too close to the root, then initial computation is cheaper as cluster size $|\partition_{\ell}|$ will be small, but quality of the proposal will be worse leading to many rejections. As we descend down the down the tree for picking $\ell$ the quality of proposal will keep improving but so will initial computation cost as $|\partition_{\ell}|$ grows. There is a good trade-off point in between as illustrated in the figure.

\setlength{\textfloatsep}{5pt}
\newcommand{\numdesc}{\blacktriangle}
\begin{algorithm}[t]
\caption{\textsc{RejectionSampling}}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} $\hc$: tree, $\datapoint$: point, $\ell$: initial level.} 
\STATE{\textbf{Output:} A sample from $P(\lbl|\datapoint)$}
\STATE Define $\numdesc_\cluster \gets \frac{|\cluster|}{ |\labelset|}$ for all tree nodes $\cluster \in \hc$.
\STATE $Z_\ell \gets e^{\ctbase^\ell} \sum_{\cluster \in \partition_i} \numdesc_\cluster \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster)})$
\STATE Define $\delta_{\ell,\cluster} \gets {Z_\ell}^{-1} e^{\ctbase^\ell}  \numdesc_\cluster \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster)})  $
\STATE Sample $\cluster_\ell$ from $\partition_\ell$ proportional to  $\delta_{\ell,\cluster_\ell}$
\STATE Accept \& \textbf{return} the representative of $\cluster_\ell$ with prob:\vspace{-2mm} \[\pi_\ell \gets {Z_\ell}^{-1} {\delta_{i,\cluster_i}}^{-1} {|\labelset|}^{-1} \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster_i)})\] \vspace{-6mm}
\FOR{$k$ \textbf{from} $\ell-1$ \textbf{down to} $-\infty$}
\STATE $Z_k \gets \delta_{k+1}(1-\pi_{k+1})$
\STATE $\delta_{k,\cluster_k} \gets Z_k^{-1} e^{\ctbase^k} \numdesc_{\cluster_k} \exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster_k)})$
\STATE $\delta_{k,\uptodownarrow} \gets 1-\sum_{\cluster \in \children{(\cluster_{k+1}})} \delta_{k,\cluster}$ 
\STATE Sample $\cluster_k$ from $\children{(\cluster_{k+1})}  \bigcup \{\uptodownarrow\}$ prop. to $\delta_{k,\cluster_k}$.
\IF{$\cluster_k =\ \uptodownarrow$}
\STATE \textsc{RejectionSampling}($\hc,\datapoint,\ell$)
\ELSIF{the representative of $\cluster_k$ \& $\cluster_{k+1}$ differ}
\STATE Accept \& \textbf{return} the rep. of $\cluster_k$ with prob $\pi_{k}$:
\vspace{-2mm}
\[
\pi_{k} \gets Z_k^{-1}{\delta_{k,\cluster_k}}^{-1}{|\labelset|}^{-1}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\cluster_k)})
\]
\vspace{-4mm}
\ENDIF
\ENDFOR
\end{algorithmic}
\label{alg:rejection_sample_alg}
\end{algorithm} 

Next, we consider a more practical and simple adaptive extension for Metropolis-Hastings. Intuitively, we want to reduce approximation error for high probability targets. Our approach splits a cluster if the radius is at least $\ctbase^m$ and if it \emph{may} contain a target $\lbl$, such that  $\norm{{\dataenc(\datapoint)}-{\lblenc(\lbl)}} < \ctbase^m$. This follows the intuition that we would like to emphasize the closest targets to a point.
In other words, we descend the tree from level $\ell$ to level $m$ splitting clusters which might contain a target that is within $\ctbase^{m}$ of $\dataenc{(\datapoint)}$ (Algorithm~\ref{alg:covertreesearchforpartition}). 


\begin{restatable}{proposition}{searchCorrectness}
\label{prop:searchCorrectness} 
Let $\partition$ be the output of Algorithm~\ref{alg:covertreesearchforpartition}, then $\max_{\lbl \in \labelset} \frac{P(\lbl|\datapoint)}{Q(\lbl|\datapoint,\partition)} \leq \gamma$ under Assumption~\ref{assumption:unitNorm}.
\end{restatable}
% \vspace{-4mm}
\begin{restatable}{remark}{searchTime}
While in the worst case, we select all clusters in level $m$, e.g., $\bigo(|\partition_{(m)}|)$ clusters, in practice we expect this to be much less and can limit to a specified number by either limiting the size of the frontier or output partition.
\end{restatable}
\vspace{-2mm}


To put our results in perspective, 
consider a simple alternative, uniform negative sampling. 
Let $Q_\textsf{unif}$ be a uniform proposal distribution, for which $\max_{y \in \labelset} \frac{P(y|x)}{Q_\textsf{unif}(y|x)} \leq |\labelset|$. 

\begin{remark}\textbf{\emph{(Uniform Negatives vs DyNNIBAL)}}
Consider the case where we have fixed compute budget for the chain length. 
For a uniform distribution, the total variation is bounded by $\exp\left(-\bigo\left(\frac{s}{|\labelset|}\right)\right)$.
Since our each sample is slightly more expensive, we can only afford to have a chain of length $\frac{s}{|\partition_{(\ell)}|}$ for any selected clustering $\partition_{(\ell)}$.
But even this reduced length chain will yield a much better total variation bound by Proposition~\ref{proposition:unnormalizedError}.
In particular, if we pick a level just $\log\log|\labelset|$ below the root, which is not very deep, then we obtain the total variation bound as $\exp\left(-\bigo\left(\frac{s}{\log|\labelset|}\right)\right)$.
Notice that this is marked improvement because of the logarithmic term in the denominator. 
\end{remark}

\begin{algorithm}[t]
\caption{\sc FindClustering}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} $\hc$: tree, $x$: point, $\gamma, m$: allowed  error} 
\STATE{\textbf{Output:} A clustering $\partition$ }
\STATE $\ell \gets \max \{\ell \in \mathbb{Z} \ : \ \ctbase^{\ell}  \leq \frac{1}{2\beta} \log \gamma   \}$
\STATE $\mathscr{F}_{\ell} \gets \partition_{\ell}$ \mycomment{Initialize frontier to be the $\ell^{th}$ level of $\hc$.}
\STATE $\partition \gets \{\}$  \mycomment{The output clustering}
\FOR{$k$ \textbf{from} $\ell$ \textbf{down to} $m$}
\STATE $\mathscr{F} \gets \{\children{(F)}: F \in \mathscr{F}_k\}$
\STATE $\mathscr{F}_{k-1} \gets \{\}$
\FOR{$F$ in $\mathscr{F}$}
\IF{$\norm{\dataenc(\datapoint) - \lblenc(F)} > \ctbase^k + \ctbase^{m}$}
\STATE $\partition \gets \partition \cup \{F\}$
\ELSE 
\STATE $\mathscr{F}_{k-1} \gets \{F\}$
\ENDIF
\ENDFOR
\ENDFOR
\STATE \textbf{return} $\partition \cup \mathscr{F}_{m}$
\end{algorithmic}
\label{alg:covertreesearchforpartition}
\end{algorithm} 

\subsection{Gradient Bias of Our Estimator}
\label{sec:covertree_estimaor}
\vspace{-2mm}
To ensure convergence in gradient descent, we need our estimator to have bounded gradient bias \citep{ajalloeian2020convergence}. We are interested in the bias of the gradient estimate: $\norm{\E [\nabla_\Theta \hat{\loss}] - \nabla_\Theta \loss}$, where the expectation is over the Metropolis-Hasting samples. 

\begin{restatable}{proposition}{gradientBias}
\label{proposition:gradientBias} Let $P$ be the true softmax and $Q_\mathsf{MH}$ be the  Metropolis-Hastings approximation to the softmax. Under Assumption~\ref{assumption:boundedGradient}, we have 
$\norm{\E [\nabla_\Theta \hat{\loss}] -  \nabla_\Theta \loss} \leq 2\epsilon \beta M$, where $\norm{P-Q_\mathsf{MH}}_\mathsf{TV} \leq \epsilon$.
\end{restatable}
\vspace{-2mm}
\subsection{Dynamic Maintenance of the Tree Structure}
\label{sec:dynamic}
\vspace{-2mm}

During training, the parameters of the dual encoder models, $\dataenc$, $\lblenc$ are updated. As a result the tree structure properties may no longer be upheld. In this section, we analyze how representations could change under standard assumptions about the data. We then describe an algorithm for maintaining an SG tree and a simple approximation in practice. 

Finding the part of the SG tree that no longer maintains its invariants after a parameter update depends on how the distance between a pair of targets changes after $w$ steps of gradient descent. Let the learning rate be $\eta$. The dual encoder parameters are updated at step $t$ as $\allparameters_t \gets \allparameters_{t-1}  - \eta \nabla_\allparameters \hat{\loss}(\datapoint_t,\lbl_t)$. We can bound the pairwise change:

\begin{restatable}{proposition}{pairwiseAmountOfChange}
\label{proposition:pairwiseAmountOfChange} Under Assumptions~\ref{assumption:lipchitz},\ref{assumption:boundedGradient},\ref{assumption:unitNorm},
let $\phi_t$ and $\phi_{t+w}$ refer to encoder parameters after $w$ more steps of gradient descent with learning rate $\eta$.
\vspace{-1mm}
\begin{equation}
\resizebox{.9\linewidth}{!}{%
$\begin{aligned}
   \left | \norm{\lblencPTstep{t} - f_{\phi_{t}}(\lbl')}_2 - \norm{\lblencPTstep{t+w} - f_{\phi_{t+w}}(\lbl')}_2 \right | \leq 4w\eta \beta LM.
    \label{eq:changedAmount}
\end{aligned}$}
\end{equation}
\end{restatable}
\vspace{-1mm}
We can detect whether, for a given pair of tree nodes with representatives $y$ and $y'$, if the covering property with respect to level $\ell$ will be maintained after the gradient update:
\begin{align}
\norm{\lblencPTstep{t} - f_{\phi_{t}}(\lbl')}_2 +  4w\eta \beta LM \leq \ctbase^{\ell},
\end{align}
and similarly for separation:
\begin{align}
\norm{\lblencPTstep{t} - f_{\phi_{t}}(\lbl')}_2 -  4w\eta \beta LM \geq \ctbase^{\ell-1}.
\end{align}

This leads to a simple algorithm for detecting which parts of the tree structure need to be re-arranged. We can maintain for each node, the distance to its farthest descendant, denoted $\textsc{maxd}(\cluster)$, to detect if covering is maintained. Similarly, we can maintain for each node, the distance between its closest pair of children nodes (SG tree), denoted $\textsc{mind}(\cluster)$, to detect if separation is maintained. Notice however that even if an ancestor node maintains the properties, a descendant may still violate them.  

An algorithm for updating the SG tree is: delete and rebuild the smallest subtrees such that levels above the subtree root maintain covering and separation (checking $\textsc{maxd}$ and $\textsc{mind}$)). We notice that for SG Trees since the separation property is only maintained between siblings and not all nodes in a given level, when we rebuild the structure we can re-attach the rebuilt subtree with the same root. This is described in Algorithm~\ref{alg:update_sg}. 
We observe that the selected bound $4w\eta \beta LM$ is equivalent to picking a level at which we rebuild, which can be easier in practice.
Rebuilding subtrees can be empirically very efficient. The rebuilding can be done independently in parallel. Each subtree contains relatively far fewer targets than the tree as a whole.


\begin{algorithm}[t]
\caption{\sc UpdateSGTree}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} $\cluster_\ell$: a subtree root at level $\ell$, $\cluster_r$: an ancestor node covering  its descendants or $\emptyset$ to indicate we are rebuilding at the top level, $4w\eta \beta LM$: bound on change}
\IF {$\ctbase^\ell \leq 4w\eta \beta LM$}
\STATE Rebuild the subtree $\cluster_\ell$ under $\cluster_r$.
\ELSE
\STATE $\textsc{maxd}(\cluster_\ell)$ is the max dist btw $\cluster_\ell$ to a descendant. \\
\STATE $\textsc{mind}(\cluster_\ell)$ is the min dist btw children of $\cluster_\ell$. \\
\STATE{$\textsf{cov} \gets \textsc{maxd}(\cluster_\ell) +  4w\eta \beta LM \leq \ctbase^\ell$}
\STATE{$\textsf{sep} \gets \textsc{mind}(\cluster_\ell) -  4w\eta \beta LM \geq \ctbase^{\ell-1}$}
\IF{$\textsf{cov}$ and $\textsf{sep}$}
\FOR{$\cluster_{\ell-1} \in \children{(\cluster_{\ell})}$}
\STATE \textsc{UpdateSGTree}($\cluster_{\ell-1}, \cluster_{\ell}, 4w\eta \beta LM$)
\ENDFOR
\ELSE 
\STATE Rebuild the subtree $\cluster_\ell$ under $\cluster_r$.
\ENDIF
\ENDIF
\end{algorithmic}
\label{alg:update_sg}
\end{algorithm}

\vspace{-3mm}
\section{EFFICIENT RE-ENCODING}
\label{sec:lowDimApprox}
\vspace{-2mm}

We address further computational bottlenecks. First, the running time (and space) is a function of the dimensionality of dual encoder embeddings. This dimensionality can typically be quite large ($d=768$  \citep{devlin2019bert}). 
Second, we previously implicitly described the representations of the targets stored in the tree, as $\lblenc{(\cluster)}$ for some cluster representative $\cluster$. We do not re-apply the encoder every time we compare a given data point's embedding to a cluster representative. Instead, we use a cached version of the target embedding. However, re-running 
the dual encoder to re-encode and update this cache (say every $w$ steps of gradient descent) would be very time-consuming and require use of hardware accelerators (GPU/TPU).

We present approaches for addressing each of these computational burdens. The \nystrom method is used to reduce dimensionality. Then we use these low-dimensional representations in a low-rank regression model that approximates the re-running of the encoder model to produce the newest encoded representations (no accelerator required).  

\vspace{-3mm}
\subsection{Reducing Dimensionality}
\vspace{-2mm}

The \nystrom method factorizes a pairwise similarity matrix by representing each row and column (e.g., targets or datapoints) as a $\numlandmarks$ dimensional vector \cite[inter alia]{williams2000using,kumar2012sampling,gittens2013revisiting}. Each dimension of the $d'$ dimensional vector can be thought of as the (scaled) pairwise similarity between the representations of the row/column and a \emph{landmark} representative associated with the particular dimension. Let $\mathbf{K} \in \mathbb{R}^{n \times n}$ be the pairwise similarity matrix, $\mathbf{S} \in \{0,1\}^{n \times \numlandmarks}, \forall j \in [\numlandmarks] \sum_{i\in n} \mathbf{S}_{ij} = 1$ is an indicator matrix corresponding to the sampled landmarks, $n = |\labelset| + |\dataset|$. The \nystrom approximation is then,
    $\mathbf{K}\mathbf{S} \left (\mathbf{S}^T\mathbf{K}\mathbf{S}\right)^{-1}\mathbf{S}^T\mathbf{K}$.
It is important to note that we do not actually \emph{explicitly} instantiate $\mathbf{K}$. We sample our landmarks, compute the pairwise similarity between points/targets and the landmarks to compute $\mathbf{K}\mathbf{S}$ and the landmarks themselves $(\mathbf{S}^T\mathbf{K}\mathbf{S})^{-1}$.
The low-dimensional representation of a target is the corresponding row in  $\mathbf{K}\mathbf{S} \left (\mathbf{S}^T\mathbf{K}\mathbf{S}\right)^{-1}$ or $\mathbf{S}^T\mathbf{K}$. We can approximate all of the pairwise distance computations needed with an inner product of two $\numlandmarks$ dimensional vectors, where $\numlandmarks \ll d$.

\vspace{-3mm}
\subsection{Regression-based Approximation of Re-Encoding}
\label{sec:approxReencode}
\vspace{-2mm}
After $w$ steps of gradient descent,  our model parameters $\allparameters_{t}$ have been updated to be $\allparameters_{t+w}$. We would like to approximate the re-encoding of targets $\lblencPTstep{t+w}$ without having to run the encoder model. Let $Y_t$ be the set of cached target embeddings after $t$ steps, where $\vec{y}$ represents the cached vector for the target $y$. We want to build $Y_{t+w}$ and we will do so by training a regression model, $\mathscr{R}$ to map each $\vec{y} \in Y_t$ to $\lblencPTstepNoLbl{t+w}(y)$. We build $s'$ training data pairs of the form, $(\vec{y_1},\lblencPTstepNoLbl{t+w}(y_1)),\dots(\vec{y_{s'}},\lblencPTstepNoLbl{t+w}(y_{s'}))$. We then fit $\mathscr{R}$ using kernel ridge regression for which we use the above \nystrom approximation to the kernel matrix. 

To update the cached target representations, we apply the regression model $\mathscr{R}$ to every cached target in $Y_{t}$, creating the new cache of target representations $Y_{t+w}$. This provides an extreme speedup in re-encoding time. Instead of $\bigo(|\labelset|)$ calls to the encoder, we have $\bigo(s')$ encoder calls where $s'$ is the number of training examples for $\mathscr{R}$ and $s' \ll |\labelset|$ along with one application of $\mathscr{R}$ to each target, effectively a multiply of a $d'\times d'$ matrix.

Further details can be found in Appendix~\ref{appendix:LowDim}. The methodological techniques of \nystrom and low-rank regression are not new, of course; our contribution is the use of these techniques to facilitate efficient re-encoding, which is a costly and time consuming step in dual encoder training.

\vspace{-3mm}
\subsection{Complete Training Algorithm}
\vspace{-2mm}

In Algorithm~\ref{alg:dynnibalComplete}, we summarize \ours. Initially, we are given a set of targets $\labelset$. We select (randomly) landmarks to apply \nystrom. We encode and reduce the dimensionality of the targets. We construct an SG tree $\hc$ over these targets. 
For a given training example $\datapoint_t$ with label $\lbl_t$, we use Algorithm~\ref{alg:covertreesearchforpartition} (with inputs of $\hc$, the datapoint $\datapoint_t$ and a given $\gamma$ allowable error) to find a clustering of the labels $\partition$. From this clustering, we can define the proposal distribution $\approxsoftmax(y|\datapoint_t,\partition)$ (Equation~\ref{eq:approxclustersoftmax}). Then, we use this proposal distribution to run a given number of Metropolis-Hastings steps to provide samples from the softmax distribution. The resulting samples are used to compute a loss for the given example, $\hat{\loss}(\datapoint_t,\lbl_t)$, and the result in a gradient step for the dual-encoder parameters. After every $w$ steps, we update the tree using Algorithm~\ref{alg:update_sg}. We describe the algorithm as SGD for simplicity; a minibatch version is used in practice.

\begin{algorithm}[h]
\caption{\ours Training Algorithm}
\begin{algorithmic}[1]
\STATE{\textbf{Input:} $\labelset$: Targets, $\dataset_\mathsf{train} = \{(\datapoint_1,\lbl_1),\dots,\}$: Training, $\gamma,m$: allowed  error, $\dataenc,\lblenc$: encoders, $k$: num samples, $\eta$: learning rate, $U$: update upper-bound} 
\STATE \textsc{InitializeTargetEncoding()} \mycomment{Algorithm~\ref{alg:initialEncoding}}
\STATE Construct $\hc$ \mycomment{Algorithm from \cite{zaheer2019sg}}
\FOR{$t$ from 0 to \textrm{NumSteps}}
\STATE Sample $(x_t,y_t)$ from $\dataset_\mathsf{train}$
\STATE $\partition \gets$ \textsc{FindClustering}$(\hc, x_t, \gamma, m)$ \mycomment{Alg.~\ref{alg:covertreesearchforpartition}}
\STATE $\approxsoftmax(\lbl|\datapoint;\partition) = \frac{1}{\hat{Z}}\exp(\invtemp\ip{\dataenc(\datapoint)}{\lblenc(\lbl^{(\partition)})})$ \mycomment{Eq.~\ref{eq:approxclustersoftmax}}
\STATE Sample $N_s = \{y_{s_i} : i \in [k]\}$ with  $y_{s_i}\sim Q_\mathsf{MH}$.

\STATE \resizebox{.99\linewidth}{!}{
$\hat{Z} \gets \exp( \invtemp\ip{\dataenc(\datapoint_t)}{\lblenc(\lbl_t)}) + \sum_{i \in [k]} \exp( \invtemp\ip{\dataenc(\datapoint_t)}{\lblenc(\lbl_{s_i})} )$
}
\STATE $\hat{\loss} \gets -\invtemp\ip{\dataenc(\datapoint_t)}{\lblenc(\lbl_t)} + \log(\hat{Z}) $.
\STATE $\Theta \gets \Theta - \eta \nabla_\Theta \hat{\loss}$
\IF{$t \ \textrm{mod} \ w = 0$}
\STATE \textsc{ApproximateReEncode()} \mycomment{Algorithm~\ref{alg:reEncode}}
\STATE $\hc \gets \textsc{UpdateSGTree}(\hc, \emptyset, U)$ \mycomment{Algorithm~\ref{alg:update_sg}}
\ENDIF
\ENDFOR
\STATE \textbf{return} $\dataenc,\lblenc$
\end{algorithmic}
\label{alg:dynnibalComplete}
\end{algorithm} 

