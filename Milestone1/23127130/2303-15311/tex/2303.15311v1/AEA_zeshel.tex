
\section{ZESHEL EXPERIMENTAL RESULTS}
\label{app:zeshel}

\noindent \textbf{Zeshel} \citep{logeswaran2019zero} is a dataset for classifying (linking) ambiguous mentions of entities to their unambiguous entity pages in Fandom Wikias. The original dataset separates individual Fandom Wikia into separate domains, which severely limits the number of targets. To make the task more challenging and better suited for evaluation of models approximating the softmax loss, we combine all Wikias together, resulting in a collection of 492K targets. We evaluate in terms of recall of the correct entity.

Tables \ref{tab:zeshel_dev_table} and \ref{tab:zeshel_test_table} report the dev and test scores for the Zeshel dataset. Recall that the number of targets in this dataset is significantly less than the other two datasets ($\sim$500K compared to 21M (Natural Questions), 8.8M (MSMARCO)). We find that the performance of \ours improves upon baselines of Stochastic Negative Sampling and Uniform Negatives. 

Because of the reduced size of the dataset, we use only 256 training points for the regression model. We also use a relatively small number of examples for stochastic negative mining (32768 in total), though this accounts for a larger memory percentage overall because of the reduced number of targets. 


\begin{table}[]
    \centering
    \begin{tabular}{@{}l c c c c @{}}
    \toprule
        Recall & Mem. \%  & R@1 & R@10 & R@100 \\
         \midrule
         In-batch Negatives & 0\% &  0.388 & 0.726 & 0.861 \\
         Uniform Negatives& 0\% &  0.393 & 0.713 & 0.851 \\
         Stochastic Neg & 6.59\% & 0.413 & 0.737 & 0.865 \\
         \ours & 0.064\% &  0.421  & 0.742 & 0.870  \\
         \midrule
         Exhaustive & 100\% &  0.444 & 0.756  & 0.880 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Zeshel (Dev) Results}. We measure the recall performance on the entity linking dataset. We note that this dataset is considerably smaller than the other datasets in terms of number of targets. Still, we find that \ours can achieve better performance than baseline methods while approaching the performance of the brute force oracle. \ours achieves better performance that a Stochastic Negative Sampling baseline which uses more memory.}
    \label{tab:zeshel_dev_table}
\end{table}
\begin{table}[]
    \centering
    \begin{tabular}{@{}l c c c c @{}}
    \toprule
        Recall & Mem. \%  & R@1 & R@10 & R@100 \\
         \midrule
         In-batch Negatives & 0\% & 0.344 &0.627   &  0.778  \\
         Uniform Negatives& 0\% & 0.323  & 0.593  & 0.747  \\
         Stochastic Neg & 6.59\% & 0.330 & 0.609 & 0.756  \\
         \ours & 0.064\% &  0.348  & 0.623 & 0.766  \\
         \midrule
         Exhaustive & 100\% &  0.348 & 0.617  & 0.767 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Zeshel (Test) Results}. We measure the recall performance on the entity linking dataset. While the dataset has considerably fewer targets, \ours achieves nearly the same result as the exhaustive brute force method and better results than Stochastic Negative Mining and Uniform Negatives. In this setting, In-Batch Negatives performs very well, perhaps because of the relatively small number of targets.}
    \label{tab:zeshel_test_table}
\end{table}
