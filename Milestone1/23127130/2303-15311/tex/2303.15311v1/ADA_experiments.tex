\section{ADDITIONAL EMPIRICAL DETAILS}
\label{app:empirical_details}

The dual encoder models used in all experiments are transformers, initialized with the Roberta base model \citep{liu2019roberta}. We use the hyperparameters presented in Table~\ref{tab:hyperparameters}. We note that the changes unique for Zeshel were done to account for the longer data point input length.

\begin{table}[]
    \centering
    \begin{tabular}{l|l}
    \toprule
    \bf Name & \bf Value  \\
    \midrule
    Train Batch Size & 128 total\\
    Uniform Negatives & 64 per training example (32 for Zeshel)  \\
    Sampled Negatives & 64 per training example \\
    Initial Warmup Learning Rate & 1e-5 \\
    End Warmup Learning Rate & 1e-7 \\
    Warmup Steps & 10,000 \\
    Warmup Decay Type & Polynomial \\
    Optimizer & Adam \\
    Optimizer Beta1 & 0.9 \\
    Optimizer Beta2 & 0.999 \\
    Optimizer Epsilon & 1e-8 \\
    Score Scaling & 20.0 \\
    Max Data Point Feature Length & 128 (256 for Zeshel) \\
    Max Target Feature Length & 256 \\
    Encoder Embedding Dimension & 768 \\
    \bottomrule
    \end{tabular}
    \caption{Hyperparameters used for training dual encoder models.}
    \label{tab:hyperparameters}
\end{table}

All models unit-norm the output embedding that is produced by the dual encoder. All models perform score-scaling as in \cite{lindgren2021efficient} with a scaling factor of 20.0. All models share sampled negative examples across all positives in the same batch. In-batch negatives are used by all models. Stochastic Negative mining and \ours use uniform negatives in addition to the sampled hard negatives.

We first train models using uniform negatives and then apply \ours. Note that this mimics the hypothesis that the softmax distribution would be closer to uniform in the beginning of training. We perform 4000 steps of uniform negatives for NQ and 8000 MSMARCO. 

We use 128 landmarks in \nystrom/regression model for all datasets. For Natural Questions and MSMARCO we use 8192 training examples for the regression model. For Zeshel we use 256 training examples for the regression model. For NQ, we perform a full refresh of all embeddings (re-initializing \nystrom/regression model) once during training at step 7500.

We find that changing the number of landmarks used does not dramatically change the performance of the method. For instance, on Natural Questions (NQ), we find that in terms of R@1, 256 landmarks achieves 0.488
while with 128 landmarks gets 0.481 and 64 landmarks gets 0.479 and R@100 with 256 landmarks is 0.862 and 128
landmarks gets 0.859 and 64 landmarks gets 0.859.

\subsection{SG Tree Details}

We use the \nystrom-modified SG Tree that is described in \S\ref{sec:nystromSG}. We notice that the tree construction and sampling procedure can be done much more efficiently on smaller trees. And so, rather than constructing one tree over all the targets, we construct a forest of 100 trees of roughly equal size. We construct and build samples from this forest independently and aggregate samples (by taking the max unnormalized probability).

