\FloatBarrier
\section{Training procedure of {{\method}}}
\label{appendix:alg}
\input{alg}
\vspace{-0.05in}

\FloatBarrier
\section{Experimental details}
\label{appendix:setup}

\subsection{Training details}
\label{appendix:setup:detail}

Unless otherwise noted, we train each model for 200K updates for CIFAR-10 models, and 1M updates for ImageNet models. For training {{\method}} models, we use $\alpha=10.0, \beta=0.0001$ unless otherwise noted. We use different training configurations depending on the encoder architecture, \ie whether is it ResNet or ViT: (a) For ResNet-based models, we train the encoder part ($f$) via stochastic gradient descent (SGD) with batch size of 64 using Nesterov momentum of weight 0.9 without dampening. We set a weight decay of $10^{-4}$, and use the cosine learning rate scheduling \cite{loshchilov2016sgdr} from the initial learning rate of 0.1. For the remainder parts of our {{\method}} architecture, \eg the decoder $g$ and discriminator MLPs, on the other hand, we follow the training practices of GAN instead: specifically, we use Adam \cite{kingma15adam} with $(\alpha, \beta_1, \beta_2)=(0.0002, 0.5, 0.999)$, following the hyperparameter practices explored by \citet{kurach2019large}. (b) For ViT-based models, on the other hand, we train both (transformer-based) encoder and decoder models via AdamW \cite{loshchilov2018decoupled} with a weight decay of $10^{-4}$, using batch size 128 and $(\alpha, \beta_1, \beta_2)=(0.0002, 0.9, 0.999)$ with the cosine learning rate scheduling \cite{loshchilov2016sgdr}. We use 2K and 100K steps of a linear warm-up phase in learning rate for CIFAR and ImageNet models, respectively. Overall, we observe that a stable training of ViT (even for CIFAR-10) requires much stronger regularization compared to ResNets, otherwise they often significantly suffer from overfitting. In this respect, we apply the regularization practices those are now widely used for ViTs on ImageNet, namely mixup \cite{zhang2018mixup}, CutMix \cite{yun2019cutmix}, and RandAugment \cite{cubuk2020randaug}, following those established in \citet{beyer2022better}.  


\subsection{Datasets}

\textbf{CIFAR-10/100} datasets \citep{dataset/cifar} consist of 60,000 images of size 32$\times$32 pixels, 50,000 for training and 10,000 for testing. Each of the images is labeled to one of 10 and 100 classes, and the number of data per class is set evenly, \ie 6,000 and 600 images per each class, respectively. By default, we use the random translation up to 4 pixels as a data pre-processing. 
We normalize the images in pixel-wise by the mean and the standard deviation calculated from the training set. The full dataset can be downloaded at \url{https://www.cs.toronto.edu/~kriz/cifar.html}.

{\textbf{CIFAR-10/100-C, and ImageNet-C} datasets \cite{hendrycks2018benchmarking} are collections of 75 replicas of the CIFAR-10/100 test datasets (of size 10,000) and ImageNet validation dataset (of size 50,000), respectively, which consists of 15 different types of common corruptions each of which contains 5 levels of corruption severities. Specifically, the datasets includes the following corruption types: (a) \emph{noise}: Gaussian, shot, and impulse noise; (b) \emph{blur}: defocus, glass, motion, zoom; (c) \emph{weather}: snow, frost, fog, bright; and (d) \emph{digital}: contrast, elastic, pixel, JPEG compression. In our experiments, we evaluate test errors on CIFAR-10/100-C for models trained on the ``clean'' CIFAR-10/100 datasets, where the error values are averaged across different corruption types per severity level. For ImageNet-C, on the other hand, we compute and compare the mean Corruption Error (mCE) proposed by \citet{hendrycks2018benchmarking}. Specifically, mCE is the average of Corruption Error (CE) over corruption types, where CE is defined by the error rates normalized by those from AlexNet \cite{kriz2012alexnet} to adjust varying difficulties across corruption types. Formally, for a classifier $f$, CE for a specific corruption type $c$ is defined by:
\begin{equation}
    \mathrm{CE}_c^f := \left(\sum_{s=1}^5 \mathtt{error}_{c, s}^f \right) \Bigg/ \left( \sum_{s=1}^5 \mathtt{error}_{c, s}^{\mathrm{AlexNet}} \right),
\end{equation}
where $s$ denotes the severity level ($1 \le s \le 5$). The full datasets, as well as the information on the pre-computed AlexNet error rates on ImageNet-C (to compute mCE), can be downloaded at \url{https://github.com/hendrycks/robustness}.}

{\textbf{CIFAR-10.1/10.2} datasets \cite{recht2018cifar10.1,lu2020harder} are reproductions of the CIFAR-10 test set that are separately collected from Tiny Images dataset \cite{torralba2008tiny}. Both datasets consist 2,000 samples for testing, and designed to minimize distribution shift relative to the original CIFAR-10 dataset in their data creation pipelines. The datasets can be downloaded at \url{https://github.com/modestyachts/CIFAR-10.1} (for CIFAR-10.1; we use the ``\texttt{v6}'' version) and \url{https://github.com/modestyachts/cifar-10.2} (for CIFAR-10.2).}

{\textbf{CINIC-10} dataset \cite{darlow2018cinic} is an extension of the CIFAR-10 dataset generated via addition of down-sampled ImageNet images. The dataset consists of 270,000 images in total of size 32$\times$32 pixels, those are equally distributed for train, validation and test splits, \ie the test dataset (that we use for our evaluation) consists of 90,000 samples. The full datasets can be downloaded at \url{https://github.com/BayesWatch/cinic-10}.}

{\textbf{ImageNet} dataset \cite{dataset/ilsvrc}, also known as ILSVRC 2012 classification dataset, consists of 1.2 million high-resolution training images and 50,000 validation images, which are labeled with 1,000 classes. As a data pre-processing step, we perform a 256$\times$256 resized random cropping and horizontal flipping for training images. For testing images, on the other hand, we apply a 256$\times$256 center cropping for testing images after re-scaling the images to have 256 in their shorter edges. Similar to CIFAR-10, all the images are normalized by the pre-computed mean and standard deviation. A link for downloading the full dataset can be found in \url{http://image-net.org/download}.}

{\textbf{ImageNet-R} dataset \cite{Hendrycks_2021_ICCV} consists of 30,000 images of various artistic renditions for 200 (out of 1,000) ImageNet classes: \eg art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, video game renditions, and so on. To perform an evaluation of ImageNet classifiers on this dataset, we apply masking on classifier logits for the 800 classes those are not in ImageNet-R. The full dataset can be downloaded at \url{https://github.com/hendrycks/imagenet-r}.}

{\textbf{ImageNet-Sketch} dataset \cite{wang2019learning} consists of 50,000 sketch-like images, 50 images for each of the 1,000 ImageNet classes. The dataset is constructed with Google image search, using queries of the form ``{\tt sketch of [CLS]}'' within the ``black and white'' color scheme, where {\tt [CLS]} is the placeholder for class names. The full dataset as well as the scripts to collect the dataset can be accessed at \url{https://github.com/HaohanWang/ImageNet-Sketch}.}

\textbf{CelebFaces Attributes (CelebA)} dataset \cite{liu2015faceattributes} consists of 202,599 face images, where each is labeled with 40 attribute annotations. We follow the standard train/validation/test splits of the dataset as provided by \citet{liu2015faceattributes}, and use the train split for training and computing FID scores following the protocol of other baselines \cite{parmar2021dual,aneja2021contrastive}. We also follow the pre-processing procedure of \cite{liu2015faceattributes} to fit in the images into the size of 64$\times$64: namely, we first perform a center crop into size 140$\times$140 to the images, followed by a resizing operation into 64$\times$64. The full dataset can be downloaded at \url{https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html}.

\subsection{Computing infrastructure} 

Unless otherwise noted, we use a single NVIDIA Geforce RTX-2080Ti GPU to execute each of the experiments. For experiments based on StyleGAN2 architecture (reported in Table~\ref{table:csd_fid}), we use two NVIDIA Geforce RTX-2080Ti GPUs per run. For the ImageNet experiments (reported in Table~\ref{tab:imagenet} in the main text), we use 8 NVIDIA Geforce RTX-3090 GPUs per run. Overall, our experiments are based on well-established encoder architectures such as ResNet and ViT, and the model size of AENIB is generally followed by these backbone models.

\clearpage
\FloatBarrier
\section{Architectural details}
\label{appendix:architecture}

\begin{figure}[ht]
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{assets/figures/concept_nibae_v1.png}
    \caption{ConvNet-based AENIB}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.65\linewidth]{assets/figures/concept_nibae_v1_vit.png}
    \caption{Transformer-based AENIB}
\end{subfigure}
% \vspace{-0.1in}
\caption{An overview of the proposed framework, \emph{nuisance-extended information bottleneck} (NIB), instantiated by an autoencoder-based design with (a) ConvNet-based and 
 (b) Transformer-based architectures.}\label{figure:overall_vit}
% \vspace{-0.15in}
\end{figure}

Recall that our proposed {{\method}} architecture consists of (a) an encoder $f$, (b) a decoder $g$, and (c) MLP-based discriminators $d_\rvy$, $d_\rvz$, and an MLP for feature statistic projection $\Pi_f$. We set 128 as the nuisance dimension $\rvz_n$, and use hidden layer of size 1,024 for MLP-based discriminators, \eg $d_\rvy$, $d_\rvz$, and MLPs for projection $\Pi_f$.

\subsection{ConvNet-based architecture}
\label{ap:arch_convnet}

We mainly consider ResNet-18 \citep{he2016deep} as a ConvNet-based encoder. For this encoder, we consider the generator architecture of FastGAN \citep{liu2021towards} as the decoder, but with a modification on normalization layers: specifically, we replace the standard batch normalization \cite{ioffe2015batch} layers in the architecture with adaptive instance normalization (AdaIN) \cite{karras2019style} so that the affine parameters can be modulated by $\rvz$ and $\rvz_n$ as well as the decoder input: we observe a consistent gain in FID from this modification.

\vspace{0.05in}
\noindent\textbf{Adversarial similarity based guidance. }
We present an additional \emph{adversarial objective} leveraging the efficiency of \emph{feature statistics} based encoder (see Section~\ref{ss:arch}) for ConvNet-based AENIB models to boost their generative modeling. Namely, we use the feature statistics based encoder to also provide the decoder $g$ an extra guidance in minimizing the (pixel-level) reconstruction loss \eqref{eq:recon}. We propose to additionally place a discriminator network, say $d_{\rvx}: \mathbb{R}^{|\Pi_E|}\rightarrow {\mathbb{R}}^{e}$, that computes similarity between $\Pi_f(\rvx)$ and $\Pi_f(g(\rvz, \rvz_n))$ and to perform an adversarial training: 
\begin{equation}\label{eq:adv_sim}
    L_{\tt sim} := \max_{d_{\rvx}}\ \log(1 - \sigma(\tfrac{1}{\tau} \cdot \mathrm{sim}(d_{\rvx}(\Pi_f(\rvx)), d_{\rvx}(\Pi_f(g(\rvz, \rvz_n)))))),
\end{equation}
where $\sigma(\cdot)$ is the sigmoid function and $\mathrm{sim}(\rvx, \rvy):=\tfrac{\rvx \cdot \rvy}{\|\rvx\|\|\rvy\|}$ denotes the cosine similarity. Here, $\tau$ is a temperature hyperparameter, and we use $\tau=0.2$ throughout our experiments. We apply this additional objective for ConvNet-based {\method} models in our experiments by default, which turns out to be helpful particularly for the generation quality of the learned autoencoders.


\subsection{Transformer-based architecture}

We consider ViT-S and ViT-B \citep{dosovitskiy2021an,touvron2021training} in our experiments. When Transformer-based encoder is used, we use the same Transformer architecture as the decoder model where it is preceded by linear layers that maps both $\rvz$ and $\rvz_n$ into the space of patch embedding. We assume the patch size of ViT to be 4 for CIFAR-10 and 16 for ImageNet, \ie we denote it as ViT-S/4 and ViT-S/16, respectively, so that the outputs from the models have similar numbers of patch embeddings ($8\times8$ and $16\times16$, respectively) to those of ResNet-18. To model $\rvz$ and $\rvz_n$ in the ViT architecture, we simply split the output patch embedding into two separate embeddings (of reduced embedding dimensions): one of these embeddings is average-pooled to define $\rvz$, and the remaining one is vector-quantized \cite{yu2022vectorquantized} to define a nuisance representation $\rvz_n$, as described in the next paragraph in more details. Figure~\ref{figure:overall_vit} illustrates an overview of our proposed {\method} for ViT-based architectures.
 
\vspace{0.05in}
\noindent\textbf{VQ-based nuisance modeling for ViT-{\method}. } Remark that our current ViT-based design allocates \emph{different} numbers of feature dimension for $\rvz$ and $\rvz_n$: specifically, we only apply global average pooling for $\rvz$ (not for $\rvz_n$), so that its dimensionality becomes independent to the input resolution, while the nuisance $\rvz_n$ would still get an increasing dimensionality for higher-resolution inputs. In practice, this difference in feature dimension may cause some training difficulties in {\method} training: (a) it makes harder to balance between the objectives given that {\method} is essentially a ``competition'' between two information channels, \ie $\rvz$ and $\rvz_n$; also, (b) it becomes increasingly difficult to force $\rvz_n$ to follow the independent Gaussian marginal for a tractable sampling as $\rvz_n$ gets higher dimensions, unlike our ConvNet-based design. To alleviate these issues, we propose to apply the \emph{vector quantization} (VQ) \cite{oord2017neural,yu2022vectorquantized} to the nuisance embedding: namely, we train the output of the nuisnace head, say $\hat{\rvz}_n$, to have one of (discrete) vectors in a learned dictionary $\mathbf{e}$. With this, now $\rvz_n$ becomes independent to the dimensionality of per-patch embeddings, which allows a more scalable balancing with $\rvz$, as well as offering a tractable marginal distribution to sample: given that $\rvz_n$ now gets a sequence of discrete distribution, one can apply a post-hoc generative modeling (\eg with an autoregressive prior \cite{oord2017neural}, or with a diffusion model \cite{gu2022vector}) to allow an efficient sampling. Specifically, we add the following objective upon our proposed {\method} objective \eqref{eq:final} to enable VQ-based nuisance modeling:
\begin{equation}
    L_{\tt VQ}(\rvx; \mathbf{e}) := \| \mathtt{sg}[\hat{\mathbf{z}}_n(\mathbf{x})] - e \|_2^2 + \beta\| \hat{\mathbf{z}}_n(\mathbf{x}) - \mathtt{sg}[e] \|_2^2,
\end{equation}
where $e := \min_i \|\hat{\mathbf{z}}_n(\mathbf{x}) - e_i \|_2^2$, and $\mathtt{sg}(\cdot)$ denotes the stop-gradient operator defined by $\mathtt{sg}(x) \equiv x$ and $\frac{d}{dx}\mathtt{sg}(x) \equiv 0$. Here, the commitment hyperparameter $\beta$ is set to 0.25 following \cite{oord2017neural,yu2022vectorquantized}. We allocate 32 per-patch dimensions for $\rvz_n$, with an embedding dictionary $\mathbf{e}$ of size 256. We adopt the embedding normalization \cite{yu2022vectorquantized} as we found it consistently improves the stability of VQ-based training.

\vspace{0.05in}
\noindent\textbf{SSIM-based $D_2^2$ reconstruction loss. } Recall that our proposed {\method} is based on minimizing reconstruction loss \eqref{eq:recon} to implement $I(\rvx; \rvz, \rvz_n)$ in NIB \eqref{eq:nib}. Although we introduce the \emph{normalized mean-squared error} (NMSE) as a default design choice, the choice may not be limited to that: here, we demonstrate a SSIM-based \cite{wang2004image} reconstruction loss as an alternative, and show its effectiveness on improving corruption robustness. Specifically, for a given pair of images\footnote{In practice, SSIM is often computed in per-patch basis for a sliding window of a certain kernel size, \eg 8. The values are then averaged to define the metric. In our experiments, we also follow this implementation.} $(x, y)$, the \emph{structural similarity index measure} (SSIM) defines a similarity metric between $x$ and $y$ considering differences in luminance (represented by $S_1$) and structures (represented by $S_2$): 
\begin{equation}\label{eq:ssim}
    \mathrm{SSIM}(x, y) = \frac{2\mu_x \mu_y + c_1}{\mu_x^2 + \mu_y^2 + c_1} \cdot \frac{2\sigma_{xy} + c_2}{\sigma_x^2 + \sigma_y^2 + c_2} =: S_1 \cdot S_2,
\end{equation}
where $c_1:=0.01^2$ and $c_2:=0.03^2$ are small constants for numerical stability, as well as to simulate the saturation effects of visual system under low luminance (and contrast) \cite{brunet2011mathematical}. Given that SSIM itself is not a distance metric (\eg it often allows the value to be negative), however, we instead consider the following modification of SSIM, the \emph{squared-$D_2$} ($D_2^2$), as our reconstruction loss, where $D_2$ is originally defined by \citet{brunet2011mathematical} that is shown to be a distance: 
\begin{equation}\label{eq:d2_loss}
    D_2^2(x, y):= \sqrt{(1 - S_1) + (1 - S_2)}^2 = {2 - S_1 - S_2}.
\end{equation}
In Table~\ref{tab:d2_loss}, we compare the effect of having different reconstruction losses in AENIB between the default choice of NMSE and $D_2^2$: the results on CIFAR-10/100-C with ViT-S/4 show that $D_2^2$-based reconstruction loss can reliable improve corruption robustness of the AENIB models over NMSE. This confirms that the choice of reconstruction loss impacts the final robustness of AENIB, and also suggests that a more perceptually-aligned similarity metric could possibly make the model less biased toward spurious features that are not necessary to build a robust representation.

In this respect, we adopt the $D_2^2$-based loss in AENIB for ViT-based models in our experiments: somewhat interestingly, we found the objective becomes much harder to be minimized for ConvNet-based models, where we keep the default choice of NMSE. This is possibly because that there can be a discrepancy between what ConvNets typically extract and those from a $D_2^2$-based reconstruction.

\begin{table*}[h]
\centering
\begin{adjustbox}{width=0.95\linewidth}
\input{assets/tables/nmse}
\end{adjustbox}
\caption{Comparison of average per-corruption error rates (\%; $\rda$) on CIFAR-10/100-C \cite{hendrycks2018benchmarking}. We use ViT-S/4 for this experiment. All the models reported here are trained via AENIB but with different reconstruction losses.}
\label{tab:d2_loss}
\end{table*}


\section{Ablation study}
\label{appendix:ablation}

\begin{figure*}[ht]
    \centering
    \hfill
    \begin{minipage}[b]{.45\textwidth}
      \centering
      \includegraphics[width=0.9\textwidth]{assets/figures/resample_v3.png}
      \captionof{figure}{Reconstructions under random nuisance $\rvz_n$ (upper rows) and random semantic $\rvz$ (lower row). The leftmost per row shows the original reconstruction.}
      \label{fig:resample}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\linewidth}
    \begin{adjustbox}{width=\linewidth}
    \input{assets/tables/ablation}
    \end{adjustbox}
    \captionof{table}{Comparison of the test error rate (Err.; \%, $\rda$), corruption error (C-Err.; \%, $\rda$) and FID on CIFAR-10 across ablations. We use ResNet-18 architecture for this experiment.}
    \label{tab:ablation}
    \end{minipage}
    \hfill
\end{figure*}


We further perform an ablation study on CIFAR-10 for a detailed analysis of the proposed {{\method}}. We use ResNet-18 based AENIB throughout this section.

\vspace{0.05in}
\noindent\textbf{Effect of $\beta$. } As also introduced in the original IB objective, $\beta \ge 0$ plays the key role in {{\method}} training as it controls the information balance between the semantic $\rvz$ and the nuisance $\rvz_n$. Here, Figure~\ref{fig:resample} examine how using different value of $\beta$ affect the actual representations, by comparing the reconstructed samples for a fixed input while randomizing the nuisance $\rvz_n$. Indeed, we observe a clear trend from this comparison demonstrating the effect of $\beta$: having larger $\beta$ makes the model to push more ``semantic'' information into $\rvz_n$ regarding it as the nuisance. Without information bottleneck, \ie in case when $\beta=0.0$, we qualitatively observe that the network rather encodes most information in $\rvz$, due to the minimax loss applied to the nuisance $\rvz_n$. {Quantitatively, this behavior is further evidenced in Table~\ref{tab:ablation} as an increase in the corruption errors when using larger $\beta$.}

\vspace{0.05in}
\noindent\textbf{Reconstruction loss. } The reconstruction loss $L_{\tt recon}$ is one of essential part to make {{\method}} work as a ``nuisance modeling'': in Table~\ref{tab:ablation}, we provide an ablation when this loss is omitted, showing a significant degradation in the final accuracy, {and more crucially in the corruption error. This confirms the necessity of reconstruction loss to obtain a robust representation in {{\method}}.} Nevertheless, due to the adversarial similarity loss $L_{\tt sim}$ that can also work (while not perfectly) as a reconstruction loss, one can still observe that the FID of the model can be moderately preserved.  

\vspace{0.05in}
\noindent\textbf{Nuisance loss. } From the ablation of $L_{\tt nuis}$ given in Table~\ref{tab:ablation}, we observe not only a considerable degradation in clean accuracy but also in its corruption robustness. This shows that strictly forcing the nuisance-ness to $\rvz_n$ (against $\rvy$) indeed helps $\rvz$ to learn a more robust representation, possibly from encouraging $\rvz$ to extract more diverse class-related information in a faithful manner by keeping the remainder information in $\rvz_n$ sufficient to infer $\rvx$.

\vspace{0.05in}
\noindent\textbf{Independence loss. } 
The independence loss $L_{\tt ind}$ in our current design, which essentially performs a GAN training toward $p (\rvz, \rvz_n) \sim \mathcal{N}(0, \mathbf{I})$, not only forces $\rvz \perp \rvz_n$ but also leads $\rvz$ and $\rvz_n$ to have a tractable marginal distribution: so that one could efficiently perform a sampling from the learned decoder. In a practical aspect, therefore, omitting $L_{\tt ind}$ in {\method} can directly harm its generation quality as given in Table~\ref{tab:ablation}.
Nevertheless, it is still remarkable that the ablation could rather improve the corruption error: this suggests that our current design of forcing the full Gaussian may be restrictive. An alternative design for the future work could assume a weaker condition for $\rvz$ and $\rvz_n$, instead with a more sophisticated sampling to obtain a valid generative model from {\method}. 

\vspace{0.05in}
\noindent\textbf{Adversarial similarity. } As detailed in Appendix~\ref{ap:arch_convnet}, we introduce an additional adversarial reconstruction loss $L_{\tt sim}$ \eqref{eq:adv_sim} for cases when the backbone is convolutional. When the $L_{\tt sim}$ is omitted in the ConvNet-based AENIB training, we observe a significant degradation in both FID and accuracy, in a similar manner to $L_{\tt recon}$ but with a more impact on FID. This is reasonable given that $L_{\tt sim}$ and $L_{\tt recon}$ have the same goal of reconstructing input but in different metrics. Overall, it confirms the effectiveness of the proposed adversarial similarity based guidance to improve decoder performance.


\section{Proof of Lemma~1}
\label{appendix:proof}

\vspace{0.05in}
\noindent\textbf{Lemma~\ref{thm:noisy}.}
\textit{Let $\rvx \in \mathcal{X}$, and $\rvy \in \mathcal{Y}$ be random variables, $\hat{\rvx}$ be a noisy observation of $\rvx$ with $I(\rvx;\rvy) = I(\hat{\rvx};\rvy)$. Given that a representation $[\hat{\rvz}, \hat{\rvz}_n] := f(\hat{\rvx})$ of $\hat{\rvx}$ satisfies (a) $H(\hat{\rvx}|\hat{\rvz}, \hat{\rvz}_n)=0$, (b) $I(\hat{\rvz}_n; \rvy) = 0$, and (c) $\hat{\rvz} \perp \hat{\rvz}_n$, it holds $I(\hat{\rvz};\rvy) = I(\rvx;\rvy)$.}
\begin{proof}[Proof]
The condition $H(\hat{\rvx}|\hat{\rvz}, \hat{\rvz}_n)=0$ implies that there exists a deterministic $g$ such that $g(\hat{\rvz}, \hat{\rvz}_n) = \hat{\rvx}$ almost surely. Denoting $f(\cdot)$ by $f_0(\cdot, \rveps)$ with a certain deterministic mapping $f_0$ and an \textit{i.i.d.} random variable $\rveps \sim p(\rveps)$ via reparametrization trick, remark that a family of determinsitic functions $f_0^{\epsilon}(\cdot):= f_0(\cdot, \epsilon)$ becomes invertible almost surely for $\epsilon \sim p(\rveps)$ and $\hat{\rvx} \sim p(\hat{\rvx})$, \ie as $g(f_0^{\epsilon}(\hat{\rvx}))=\hat{\rvx}$. Then, the statement follows from the information preservation of invertible mapping and the chain rule of mutual information (and that of conditional mutual information), as well as by applying (b) and (c):
\begin{align}
    I(\rvx;\rvy) = I(\hat{\rvx};\rvy) &= I(\rvy; \hat{\rvz}, \hat{\rvz}_n) = I(\rvy; \hat{\rvz}_n) + I(\rvy; \hat{\rvz} | \hat{\rvz}_n) \\
    &= I(\rvy; \hat{\rvz}) + H(\hat{\rvz}_n| \rvy) + H(\hat{\rvz}_n| \hat{\rvz}) - H(\hat{\rvz}_n| \rvy, \hat{\rvz}) - H(\hat{\rvz}_n) \\
    &= I(\rvy; \hat{\rvz}) = I(\hat{\rvz}; \rvy).
\end{align}
\end{proof}


\section{Additional background}
\label{appendix:related}

\subsection{Detailed survey on related work}

\vspace{0.05in}
\noindent\textbf{Out-of-distribution robustness. } 
Since the seminal works \cite{szegedy2014intriguing,Nguyen_2015_CVPR,amodei2016concrete} revealing the fragility of neural networks for out-of-distribution inputs, there have been significant attempts on identifying and improving various notions of robustness: \eg detecting novel inputs \cite{hendrycks2017a,lee2018maha,hendrycks2018deep,hendrycks2019using,lee2018training,tack2020csi,xiao2020lregret}, robustness against corruptions \cite{hendrycks2018benchmarking,geirhos2018imagenettrained,hendrycks2020augmix,wang2021tent,diffenderfer2021winning}, and adversarial noise \cite{goodfellow2014explaining,madry2018towards,pmlr-v80-athalye18a,zhang2019theoretically,cohen2019certified,carlini2019evaluating}, to name a few. Due to its fundamental challenges in making neural network to extrapolate, however, most of the advances in the robustness literature has been made under assuming priors closely related to the individual problems: \eg \emph{Outlier Exposure} \cite{hendrycks2018deep} and \emph{AugMix} \cite{hendrycks2020augmix} assume an external dataset or a pipeline of data augmentations to improve the performances in novelty detection and corruption robustness, respectively; \emph{Tent} \cite{wang2021tent} leverages extra information available from a batch of samples in test-time to adapt a given neural network; \citet{tramer2019adversarial,kang2019transfer} observe that neural networks robust to a certain type of adversarial attack (\eg an $\ell_\infty$-constrained adversary) do not necessarily robust to other types of adversary (\eg an $\ell_1$ adversary), \ie adversarial robustness hardly generalizes from the adversary assumed \emph{a priori} for training. In this work, we aim to improve multiple notions of robustness without assuming such priors, through a new training scheme that extends the standard information bottleneck principle under noisy observations. 

\vspace{0.05in}
\noindent\textbf{Hybrid generative-discriminative modeling. }
Our proposed method can be also viewed as a new approach of improving the robustness of discriminative models by incorporating a generative model, in the context that has been explored in recent works \cite{lee2018maha,schott2018towards,Grathwohl2020Your,Yang_2021_ICCV}: for example, \citet{lee2018maha, pmlr-v97-lee19f} have shown that assuming a simple Gaussian mixture model on the deep discriminative representations can improve novelty detection and robustness to noisy labels, respectively; \citet{schott2018towards} develop an empirical defense against adversarial examples via generative classifiers; A line of research on \emph{Joint Energy-based Models} (JEM) \cite{Grathwohl2020Your,Yang_2021_ICCV} assumes the entire discriminative model as a joint generative model by interpreting the logits of $p(\rvy|\rvx)$ as unnormalized log-densities of $p(\rvx|\rvy)$, and shows that modeling $p(\rvx|\rvy)$ as well as $p(\rvy|\rvx)$ can improve out-of-distribution generalization of the classifier. 
Nevertheless, it is still an unexplored and open question that how to ``better'' incorporate generative representation into  discriminative models: in case of novelty detection, for example, several recent works \cite{nalisnick2018do, ren2019lratio, Serra2020Input, xiao2020lregret} observe that existing likelihood-based generative models are not accurate enough to detect out-of-distribution datasets, suggesting that relying solely on (likelihood-based) deep generative representation may not enough for robust classification \cite{Fetaya2020Understanding}. In case of JEM, on the other hand, it has been shown that directly assuming a joint generative-discriminative representation often makes a significant training instability. In this work, we propose to introduce an autoencoder-based model to avoid the training instability, and consider a design that the \emph{nuisance} can succinctly supplement the given discriminative representation to be generative.

\vspace{0.05in}
\noindent\textbf{Invertible representations {and nuisance modeling}. } The idea of incorporating nuisances can be also considered in the context of \emph{invertible} modeling, or as known as \emph{flow-based models} \cite{dinh2016density,kingma2018glow,jacobsen2018irevnet,behrmann2019invertible,chen2019residual,grathwohl2019ffjord},\footnote{A more complete survey on flow-based models can be found in \cite{kobyzev2020normalizing}.} which maps a given input $\rvx$ into a representation $\rvz$ of the same dimension so that one can construct an inverse of $\rvz$ to $\rvx$: here, the nuisance can be naturally defined as the remainder information of $\rvz$ for a given subspace of interest, \eg to model $\rvy$. For example, \citet{jacobsen2018excessive} adopt a fully-invertible variant of i-RevNet \cite{jacobsen2018irevnet} to analyze \emph{excessive invariance} in neural networks, \ie the existence of pairs of completely different samples with the same representation in a neural network, and proposes to maximize the cross-entropy for the nuisances in a similar manner to our proposed minimax-based nuisance loss (\eqref{eq:nuis} in the main text); \citet{lynton2020training}, on the other hand, leverages invertible neural network to model a Gaussian mixture based generative classifier in the representation space, so that nuisance information can be preserved until its representation. 
Compared to such approaches relying on invertible neural networks, our autoencoder-based nuisance modeling does not guarantee the ``full'' invertibility for arbitrary inputs: instead, it only focuses on inverting the data manifold given, and this enables (a) a much flexible encoder design in practice, \ie other than flow-based designs, and (b) a more scalable generative modeling of nuisance representation $\rvz_n$ while forcing its \emph{independence} to the semantic space $\rvz$. {This is due to that it works on a compact space rather than those proportional to the input dimension, which is an important benefit of our modeling in terms of the scalability of nuisance-aware training, \eg beyond an MNIST-scale as done by \citet{jacobsen2018excessive}.}
{More closer related works \cite{jaiswal2018unsupervised, jaiswal2019discovery, pan2021disentangled} in this respect instead introduce a separate encoder for nuisance factors, where the nuisanceness is induced by the independence to $\rvz$: \eg DisenIB \cite{pan2021disentangled} applies FactorVAE \cite{kim2018disentangling} between semantic and nuisance embeddings to force their independence.\footnote{{We provide a more direct empirical comparison with DisenIB \cite{pan2021disentangled} to {\method} in Section~\ref{exp:disenib}.}} Yet, similarly to the invertible approach, the literature has been questioned on that the idea can be scaled-up beyond, \eg MNIST, and our work does explore and establish a practical design that is applicable for recent architectures and datasets addressing modern security metrics, \eg corruption robustness. On the technical side, we find that, \eg the ``nuisanceness to $\rvy$'' is more important for $\rvz_n$ than the ``independence with $\rvz$'' (as usually done in the previous works \cite{jaiswal2018unsupervised, jaiswal2019discovery, pan2021disentangled}) to induce a robust representation, as verified in our ablation study in Appendix~\ref{appendix:ablation}, which can be a useful practice for the future research concerning robust representation learning.} 

\vspace{0.05in}
\noindent\textbf{Autoencoder-based generative models. } 
There have been steady advances in generative modeling based on autoencoder architectures, especially since the development in \emph{variational autoencoders} (VAEs) \cite{kingma2014autoencoding}: due to its ability of estimating data likelihoods, and its flexibility to implement various statistical assumptions \cite{louizos2015variational,kingma2016improved,kim2018disentangling}. With the advances in its training objectives \cite{pascal2008denoising,makhzani2015adversarial,higgins2016beta} as well as the architectural improvements \cite{vahdat2020nvae,child2021very}, VAE-based models are currently considered as one of state-of-the-art approaches in likelihood based generative modeling: \eg a state-of-the-art \emph{diffusion models}~\citep{ho2020ddpm, song2021denoising} is built upon the denoising autoencoders under Gaussian perturbations, and recently-proposed \emph{hierarchical VAEs} \cite{vahdat2020nvae,child2021very} have shown that VAEs can benefit from scaling up its architectures into deeper encoder networks. In perspectives of viewing our method as a \emph{generative modeling}, {{\method}} is based on \emph{adversarial autoencoders} \cite{makhzani2015adversarial} that replaces the KL-divergence based regularization in standard VAEs with a GAN-based adversarial loss, with a novel encoder architecture that is based on the \emph{internal feature statistics} of discriminative models: so that the model can better encode lower-level features without changing the backbone architecture. We observe that this design enables autoencoder-based modeling even from a large, pre-trained discriminative models, and this ``projection'' of internal features can significantly benefit the generation quality, as well as for generative adversarial networks (GANs) as observed in Table~\ref{table:csd_fid}. 


\subsection{Technical background}
\label{appendix:tech}

\vspace{0.05in}
\noindent\textbf{Variational information bottleneck. }
Although the information bottleneck (IB) principle given in \eqref{eq:ib} \cite{tishby99information} suggests a useful definition on what we mean by a ``good'' representation, computing mutual information of two random variables is generally hard and this makes the IB objective infeasible in practice. To overcome this, \emph{variational information bottleneck} (VIB) \cite{alemi2016deep,chalk2016relevant} applies variational inference to obtain a lower bound on the IB objective \eqref{eq:ib}. Specifically, it approximates: (a) $p(\rvy|\rvz)$ by a (parametrized) ``decoder'' neural network $q(\rvy|\rvz)$, and (b) $p(\rvz)$ by an ``easier'' distribution $r(\rvz)$, \eg isotropic Gaussian $\mathcal{N}(\rvz|0, \mathbf{I})$. Having such (variational) approximations in computing \eqref{eq:ib} as well as the Markov chain property $\rvy - \rvx - \rvz$ of neural networks, one yields the following lower bound on the IB objective \eqref{eq:ib}:
\begin{align}\label{eq:vib}
    I(\rvz;\rvy) - \beta I(\rvz, \rvx) &\ge \mathbb{E}_{\rvx, \rvy}\left[\int dz \left(p(z|\rvx) \log q(\rvy|z) - \beta p(z|\rvx) \log \frac{p(z|\rvx)}{r(z)} \right) \right].
\end{align}
This bound can now be approximated with the empirical distribution {$p(\rvx, \rvy)\approx \tfrac{1}{n}\sum_i \delta_{x_i}(\rvx)\delta_{y_i}(\rvy)$} from data.
By further assuming a Gaussian encoder $p(\rvz|\rvx) := \mathcal{N}(\rvz|f^{\mu}(\rvx), f^{\sigma}(\rvx))$ as defined in \eqref{eq:gaussian_decoder} and applying the reprarametrization trick \cite{kingma2014autoencoding}, we get the following VIB objective:
\begin{equation}
    L_{\tt VIB}^{\beta} := \frac{1}{n} \sum_{i=1}^{n}\ \mathbb{E}_{\rveps}[-\log q(y_i | f(x_i, \rveps))] + \beta\  \mathrm{KL}~(p(\rvz|x_i) \| r(\rvz)).
\end{equation}

\vspace{0.05in}
\noindent\textbf{Generative adversarial networks. }
\emph{Generative adversarial network} (GAN) \cite{goodfellow2014gan} considers the problem of learning a generative model $p_g$ from given data $\{x_i\}_{i=1}^n$, where $x_i\sim p_d(\rvx)$ and $\rvx \in \mathcal{X}$. Specifically, GAN consists of two neural networks: (a) a \emph{generator} network $G: \mathcal{Z} \rightarrow \mathcal{X}$ that maps a latent variable $z \sim p(\rvz)$ into $\mathcal{X}$, where $p(\rvz)$ is a specific prior distribution, and (b) a \emph{discriminator} network $D: \mathcal{X} \rightarrow [0, 1]$ that discriminates samples from $p_d$ and those from the implicit distribution $p_g$ derived from $G(\rvz)$. The primitive form of training $G$ and $D$ is the following:
\begin{equation}\label{eq:minimax}
    \min_{G}\max_{D} V(G, D) \coloneqq \mathbb{E}_{\rvx}[\log (D(\rvx))] 
        + \mathbb{E}_{\rvz}[\log (1 - D(G(\rvz)))].
\end{equation}
For a fixed $G$, the inner maximization objective \eqref{eq:minimax} with respect to $D$ leads to the following \emph{optimal discriminator} $D^*_G$, and consequently the outer minimization objective with respect to $G$ becomes to minimize the \emph{Jensen-Shannon divergence} between $p_d$ and $p_g$, namely $D^*_{G} := \frac{p_d}{p_d+p_g}$.


\FloatBarrier
\section{Results on MNIST-C}
\label{appendix:mnistc}

\begin{table*}[ht]
\centering
\small
\begin{adjustbox}{width=\linewidth}
\setlength{\tabcolsep}{3pt}
\input{assets/tables/mnistc_v2}
\end{adjustbox}
\caption{{Comparison of (a) clean error (\%; $\rda$), (b) AUROC ($\gua$) on detecting Gaussian noise (higher is better), and (c) corruption errors (\%; $\rda$) per corruption type on MNIST-C \cite{mu2019mnist}. Each classifier is trained on MNIST with random translation as augmentation. We highlight our results as blue whenever the value improves the baselines more than 3\% in absolute values.}}
\label{tab:mnistc}
\end{table*}

We also evaluate our proposed {{\method}} training on MNIST-C \cite{mu2019mnist}, a collection of corrupted versions of the MNIST \cite{dataset/mnist} test dataset of 15 corruption types constructed in a similar manner to CIFAR-10/100-C \cite{hendrycks2018benchmarking}, to get a clearer view on the effectiveness of our method on a simpler setup. For this experiments, we use a simple 4-layer convolutional network (with batch normalization \cite{ioffe2015batch}) as the encoder architecture, and trained every model on the (clean) MNIST training dataset for 100K updates following other training details of the CIFAR experiments (see Appendix~\ref{appendix:setup:detail}): again, we notice that the training does not assume specific prior on the corruptions. 
We compare {{\method}} with the direct ablations of cross-entropy and VIB based models, as well as some variants of VIB, namely Nonlinear-VIB \cite{kolchinsky2019nonlinear}, Squared-VIB/NLIB \cite{thobaben2020convex}, and DisenIB \cite{pan2021disentangled}. 

Table~\ref{tab:mnistc} summarizes the results: overall, we observe that the effectiveness of {{\method}} training still applies to MNIST-C, \eg our {{\method}} training improves the average corruption error from the baseline cross-entropy based training from $34.5\% \rightarrow 29.8\%$, which could not be obtained by simply sweeping on the baseline VIB training. 
Given that MNIST-C allows a visually clearer distinction between contents and corruptions compared to CIFAR-10/100-C, one can better interpret the behavior of given models on each corruption types: here, we observe that our training can dramatically improve robustness for certain types of corruptions where the baselines shows poor performances, \eg Impulse, Glass, and Motion, while still some types of corruptions are still remaining challenging even with {{\method}}, \eg especially for low-frequency biased corruptions such as Brightness and Stripe.
{Compared to DisenIB, on the other hand, we observe that the effectiveness from DisenIB, e.g., its gain in AUROC (as conducted by \citet{pan2021disentangled}), could not be further generalized on MNIST-C, where {{\method}} still improves upon it as well as achieving the perfect score at the same OOD task.}


\FloatBarrier
\section{Additional results on corruption robustness}
\label{appendix:additional_exp}

\begin{table*}[h]
\centering
\small
\begin{adjustbox}{width=\linewidth}
\input{assets/tables/corruption_resnet.tex}
\end{adjustbox}
\caption{Comparison of average corruption error rates (\%; $\rda$) per severity level on \mbox{CIFAR-10/100-C} \cite{hendrycks2018benchmarking} with ResNet-18 architecture. Bold and underline denote the best and runner-up, respectively.  All the models are trained only using random translation as data augmentation.}
\label{tab:corr_resnet}
\end{table*}

Table~\ref{tab:corr_resnet} summarizes our experiments on corruption robustness with ResNet-18 architectures. Again, our AENIB consistently improves the robustness at common corruptions benefiting from nuisance modeling compared to VIB objectives. For example, we obtain an absolute $6.5\%$ of reduction in error rates on CIFAR-10-C of the highest severity, \ie by $43.5\% \rightarrow 37.0\%$. Here, we highlight that we do not utilize any data augmentations but random translation for training ResNet-18 models; the gain from our method indeed comes from the better robustness itself, unlike common practices in improving the corruption robustness by carefully designing augmentations. Interestingly, we observe that the impact of {{\method}} in the clean error can be different depending on the encoder architecture: compared to ViT results as given Table~\ref{tab:corruption_vit}, {{\method}} with ResNet-18 makes a slight decrease in clean accuracy, while the robust accuracy is still improved. In this case, it is more clear to see that AENIB could improve the \emph{effective robustness} of the learned representation. This is possibly due to that the representation induced via {{\method}} can be extracted better with non-local (attention-based) operations such as ViT. 


\FloatBarrier
\section{{Experiments on image generation}}
\label{appendix:generation}


\begin{table*}[h]
    \centering
    \hfill
    \begin{minipage}[b]{0.5\linewidth}
        \centering
        \begin{adjustbox}{width=\linewidth}
        \input{assets/tables/csd_gan}
        \end{adjustbox}
        \captionof{table}{{Test FID and IS of GANs on CIFAR-10. Bold and underline indicate the best and runner-up, respectively. We note that the value of ADA$^*$ \cite{karras2020training} is taken after $2\times$ longer training steps.}}
        \label{table:csd_fid}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.38\linewidth}
    \begin{adjustbox}{width=\linewidth}
    \input{assets/tables/vae_table}
    \end{adjustbox}
    \captionof{table}{Test FID and IS of VAE models on unconditional generation of CIFAR-10 and CelebA. {Bold and underline denote the best and runner-up, respectively}.} 
    \label{tab:fid}
    \end{minipage}
    \hfill
    \vspace{-0.1in}
\end{table*}


\subsection{{Feature statistics discriminator for GANs}}
\label{appendix:exp_fsd}

Designing a stable discriminator has been crucial for GANs: a usual practice in the literature is to have a separate, carefully-designed network with a comparable generator, but with a significant overhead. We observe that the \emph{internal feature statistics} of a convolutional encoder $f$ can be a surprisingly effective representation to define a simple yet efficient discriminator. Concretely, for a given encoder $f$ and an input $\rvx$, we consider $L$ intermediate feature maps of $\rvx$, namely $\rvx^{(1)}, \cdots, \rvx^{(L)}$ from $f(\rvx)$, 
{the \emph{features statistics discriminator} (FSD) we consider here is then simply a 3-layer MLP applied on $\Pi_{f}(\rvx)$ \eqref{eq:projection}. In the following, we empirically confirm that this simplest design of discriminator can dramatically accelerate GAN training, particularly when applied upon pre-trained discriminative encoders: similarly to \citet{sauer2021projected} but with a simpler architecture.}

We evaluate the effect of the proposed feature statistics discriminator to the generation quality of GANs: here, we consider ImageNet-pretrained ResNet-18 (R-18) and ResNet-50 (R-50) \cite{he2016deep}, and define GAN discriminators via FSD upon the pre-trained models. We adopt StyleGAN2 \cite{karras2020analyzing} and FastGAN \cite{liu2021towards} for the generator architectures. For the StyleGAN2-based models, we follow the training details of DiffAug \cite{zhao2020diffaugment} and ADA \cite{karras2020training} in their CIFAR experiments: specifically, we use Adam with $(\alpha, \beta_1, \beta_2)=(0.002, 0.0, 0.99)$ for optimization with batch size of 64. We use non-saturating loss for training, and use $R_1$ regularization \cite{mescheder2018training} with $\gamma=0.01$. We do not use, however, the path length regularization and the lazy regularization \cite{karras2020analyzing} in training. We take exponential moving average on the generator weights with half-life of 500K samples. We stop training after 800K generator updates, which is about the half of those conducted for the ADA baseline \cite{karras2020training}. For the FastGAN baseline, on the other hand, we run the official implementation of FastGAN\footnote{\url{https://github.com/odegeasslbc/FastGAN-pytorch}} \cite{liu2021towards} on CIFAR-10 for the length of 6.4M samples with batch size 16. For the ``Projected GAN'' baseline, we adapt the official implementation\footnote{\url{https://github.com/autonomousvision/projected_gan}} \cite{sauer2021projected} onto the ImageNet pre-trained ResNet-18, and trained for 6.4M samples with batch size 64. Our results (``FSD'') follows the same training details, but with a difference in its discriminator. 

Table~\ref{table:csd_fid} summarizes the results. Overall, we observe that FSD can aid GAN training of given generator network surprisingly effectively: by leveraging pre-trained representations, FSD could achieve FID competitive with a state-of-the-art level approach of ADA \cite{karras2020training} even with using much weaker data augmentation. Compared to Projected GAN \cite{sauer2021projected} that also leverages pre-trained models to stabilize GANs, our approach offers a more simpler approach to leverage the given representations, \ie by just aggregating the features statistics, yet achieving a better FID. 

\subsection{Feature statistics encoder for autoencoders}
\label{exp:generation}

We also evaluate our proposed architecture and method as a \emph{generative modeling}, especially focusing on the effectiveness of the \emph{feature statistics encoder} (Section~\ref{ss:arch}) and the \emph{adversarial similarity} based training for ConvNet-based AENIB autoencoders on CIFAR-10 \cite{dataset/cifar} and CelebA \cite{liu2015faceattributes} datasets. To this end, we consider an ``unsupervised'' version of {{\method}} which omits the VIB loss ($L_{\tt VIB}^\beta$; \eqref{eq:vib}) and the nuisance loss ($L_{\tt nuis}$; \eqref{eq:nuis}) in training, so that the model can assume the setup of unconditional generation.

{Table~\ref{tab:fid} summarizes the quantitative generation results of our AENIB models optimized with different objectives.\footnote{Following other baselines, we compute FIDs from 50,000 generated samples against the training dataset.}} Firstly, it confirms the effectiveness of adversarial similarity based training: when it is solely applied upon $L_{\tt recon}$ (``$L_{\tt recon}$ only''; equivalent to \cite{makhzani2015adversarial}) it makes a significant improvements in both FID and IS. 
To further investigate the effectiveness of our proposed feature statistics encoder, we also test a scenario that the encoder is \emph{fixed} by \mbox{ResNet-18} pre-trained on ImageNet, akin to the setup of Table~\ref{table:csd_fid}: we observe that our encoder design can surprisingly benefit from using better representation, \eg ``+ Projection (R-18)'' in Table~\ref{tab:fid} further improves FID on CIFAR-10 from $46.8 \rightarrow 12.6$, better than the best results among considered VAE-based models, by only training an MLP upon the feature statistics of the (fixed) model. It is notable that the gain only appears when we apply the adversarial similarity based training: \ie even with the pre-trained model, it only achieves 67.5 in FID on CIFAR-10 without the training. This observation suggests an interesting direction to scale-up autoencoder-based models with large pre-trained representations, in a similar vein as \cite{sauer2021projected} as presented in the context of GANs. 

\clearpage
\FloatBarrier
\subsection{{Qualitative results}}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.85\linewidth]{assets/figures/recon_celeba.png}
 \vspace{-0.05in}
\caption{Qualitative comparison of reconstructions of fixed samples of unconditional {{\method}} model (and its ablations) on CelebA.}
\label{fig:uncond_recon_celeba}
\vspace{0.15in}

\centering
\hspace*{\fill}
\begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{assets/figures/sample_celeba_f0.png}
    \caption{$L_{\tt recon}$ only (FID: 50.1)}
\end{subfigure}
\hspace*{\fill}
\begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{assets/figures/sample_celeba_f1.png}
    \caption{+ Adv. similarity (FID: 25.1)}
\end{subfigure}
\hspace*{\fill}
\begin{subfigure}{0.24\linewidth}
    \includegraphics[width=\linewidth]{assets/figures/sample_celeba_pre.png}
    \caption{+ Projected (FID: 6.91)}
\end{subfigure}
\hspace*{\fill}
\vspace{-0.05in}
\caption{Qualitative comparison on uncurated random samples from unconditional {{\method}} model (and its ablations) on CelebA.}
\label{figure:uncond_sampling_celeba}
\vspace{0.15in}

\centering
\hspace*{\fill}
\begin{subfigure}{0.38\linewidth}
    \includegraphics[width=\linewidth]{assets/figures/recon_cifar10.png}
    \caption{Reconstruction}
    \label{fig:original_cifar}
\end{subfigure}
\hspace*{\fill}
\begin{subfigure}{0.38\linewidth}
    \includegraphics[width=\linewidth]{assets/figures/sample_cifar10.png}
    \caption{Sampling}
    \label{fig:recon_cifar}
\end{subfigure}
\hspace*{\fill}
\vspace{-0.05in}
\caption{Qualitative comparison on uncurated random samples from unconditional {{\method}} model (and its ablations) on CelebA.}
\label{figure:uncond_sampling_cifar}
\end{figure*}

\clearpage
\FloatBarrier
\section{Application to model debugging}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{assets/figures/error_analysis.png}
  \caption{Qualitative comparison between (a) original input (the leftmost column), (b) its reconstruction (the second column), and (c) its further reconstructions with random nuisance $\rvz_n$ (the remaining columns), examined for test samples misclassified by a CIFAR-10 {{\method}} model.}
  \label{fig:model_debugging}
\end{figure*}

To further understand how the proposed {{\method}} model internally works with its representation $\rvz$ and $\rvz_n$, we examine an {{\method}} model trained on CIFAR-10 to analyze how the model reconstruct given inputs when the model incorrectly classifies them. Specifically, Figure~\ref{fig:model_debugging} illustrates a subset of CIFAR-10 test samples misclassified by an {{\method}} model by comparing the original input with its reconstructed samples from the model. Overall, we observe that such a qualitative comparison can provide a useful signal to interpret model errors: it effectively visualizes which visual cues of a given input negatively affected the decision making process of the given model, also visualizing the closest (misclassified) realizations that the model decodes for a given representation, \ie what the model actually perceived. For example, for the test input given at the first row of Figure~\ref{fig:model_debugging}, one can observe that the model essentially ``ignored'' the tiny part that represent the true semantic, \ie the ``deer'', and reconstructed the remaining part as a ``ship''. 
