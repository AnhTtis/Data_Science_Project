% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.

\RequirePackage{algorithm}
\RequirePackage{algorithmic}

\usepackage{bm}
\usepackage{bbm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{placeins}

\usepackage{xcolor}         % colors
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{setspace}

\renewcommand{\ie}{\textit{i}.\textit{e}., }
\renewcommand{\eg}{\textit{e}.\textit{g}., }
\newcommand{\viz}{\textit{viz}., }
\newcommand{\dev}[1]{\tiny{$\pm$#1}}
\newcommand{\ghdev}[1]{\phantom{\dev{#1}}}
\newcommand{\diff}[1]{\tiny{\textcolor{Green7}{$(#1)$}}}

\newcommand{\note}[1]{\textcolor{red}{#1}}
% \renewcommand{\note}[1]{{#1}}

\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}
\def\rveps{{\boldsymbol{\epsilon}}}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{color}
\definecolor{Blue9}{rgb}{0.098,0.3,0.9}
\definecolor{Red7}{rgb}{0.941, 0.243, 0.243}
\definecolor{Green7}{RGB}{55, 178, 77}
\definecolor{BrickRed}{rgb}{0.6,0,0}
\definecolor{RoyalBlue}{rgb}{0,0,0.8}
\definecolor{Tdgreen}{rgb}{0,0.4,0.7}

\definecolor{pinegreen}{rgb}{0.0, 0.47, 0.44}
\definecolor{cornellred}{rgb}{0.7, 0.11, 0.11}
\definecolor{cadmiumgreen}{rgb}{0.0, 0.42, 0.24}
\definecolor{spirodiscoball}{rgb}{0.06, 0.75, 0.99}
\definecolor{Gray}{gray}{0.9}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\ck}{\color{Green7}{\cmark}}
\newcommand{\xk}{\color{Red7}{\xmark}}
\newcommand{\gua}{\color{Green7}{\uparrow}}
\newcommand{\rda}{\color{Red7}{\downarrow}}

\newcommand{\dif}[1]{\tiny{\textcolor{Green7}{$\boldsymbol{(#1)}$}}}
\newcommand{\gdif}[1]{\phantom{\tiny{$\boldsymbol{(#1)}$}}}

\newcommand{\method}{AENIB}

\newcommand{\citet}{\cite}
\newcommand{\citep}{\cite}

\newcommand{\todo}[1]{\textcolor{blue}{\textit{TODO: #1}}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\hypersetup{citecolor=Tdgreen}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{12271} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Enhancing Multiple Reliability Measures via \\ Nuisance-extended Information Bottleneck}

\author{Jongheon Jeong$^\dagger$ \quad  Sihyun Yu$^\dagger$ \quad  Hankook Lee$^\ddag$\thanks{Work done at KAIST.} \quad Jinwoo Shin$^\dagger$ \\
$^\dagger$Korea Advanced Institute of Science and Technology (KAIST) \quad $^\ddag$LG AI Research\\
{\tt\small \{jongheonj,sihyun.yu,jinwoos\}@kaist.ac.kr \quad hankook.lee@lgresearch.ai}
% First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
In practical scenarios where training data is limited, many predictive signals in the data can be rather from some biases in data acquisition (\ie less generalizable), so that one cannot prevent a model from co-adapting on such (so-called) ``shortcut'' signals: this makes the model fragile in various distribution shifts. To bypass such failure modes, we consider an adversarial threat model under a mutual information constraint to cover a wider class of perturbations in training. This motivates us to extend the standard \emph{information bottleneck} to additionally model the \emph{nuisance information}. We propose an autoencoder-based training to implement the objective, as well as practical encoder designs to facilitate the proposed hybrid discriminative-generative training concerning both convolutional- and Transformer-based architectures. Our experimental results show that the proposed scheme improves robustness of learned representations (remarkably without using any domain-specific knowledge), with respect to multiple challenging reliability measures. For example, our model could advance the state-of-the-art on a recent challenging OBJECTS benchmark in novelty detection by $78.4\% \rightarrow 87.2\%$ in AUROC, while simultaneously enjoying improved corruption, background and (certified) adversarial robustness. Code is available at \url{https://github.com/jh-jeong/nuisance_ib}.
\end{abstract}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{assets/figures/teaser_b3.png}
  \caption{A high-level illustration of our method, \emph{nuisance-extended information bottleneck} (NIB). In this paper, we focus on scenarios when the input $\rvx$ can be corrupted $\rvx \rightarrow \hat{\rvx}$ in test-time while preserving its semantics. Unlike the standard cross-entropy training (CE), NIB aims to encode \emph{every} target-correlated signal in $\rvx$, some of which can be more reliable under distribution shifts.}
  \label{fig:teaser}
\vspace{-0.1in}
\end{figure}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{s:intro}

Despite the recent breakthroughs in computer vision in aid of deep learning, \eg in image/video recognition~\cite{brock2021high,dai2021coatnet,tong2022videomae}, synthesis \cite{karras2020analyzing, rombach2022high,yu2022generating,ho2022imagen}, and 3D scene rendering~\cite{mildenhall2021nerf,tancik2022block, mildenhall2022dark}, deploying deep learning models to the real-world still places a burden on contents providers as it affects the \emph{reliability} of their services. In many cases, \emph{deep neural networks} make substantially fragile predictions for \emph{out-of-distribution} inputs, \ie samples that are not likely from the training distribution, even when the inputs are semantically close enough to in-distribution samples for humans \cite{szegedy2014intriguing,hendrycks2018benchmarking}. Such a vulnerability can be a significant threat in risk-sensitive systems, such as autonomous driving, medical imaging, and health-care applications, to name a few \cite{amodei2016concrete}. Overall, the phenomena highlight that deep neural networks tend to extract ``shortcut'' signals \cite{geirhos2020shortcut} from given limited (or potentially biased) training data in practice. 

To address such concerns, multiple literatures have been independently developed based on different aspects of model reliability. Namely, their methods use different \emph{threat models} and benchmarks, depending on how a shift in input distribution happens in test-time, and how to evaluate model performance against the shift. For example, in the context of \emph{adversarial robustness} \cite{madry2018towards,carlini2019evaluating,cohen2019certified,zhang2019theoretically}, a typical threat model is to consider the \emph{worst-case} noise inside a fixed $\ell_p$-ball around given test samples. Another example of \emph{corruption robustness} \cite{hendrycks2018benchmarking,hendrycks2020augmix,Hendrycks_2021_ICCV,wang2021tent} instead assumes pre-defined types of common corruptions (\eg Gaussian noise, fog, \emph{etc.}) that applies to the test samples. Lastly, \emph{novelty detection} \cite{hendrycks2017a,liang2018enhancing,lee2018maha,liu2020energy} usually tests whether a model can detect a specific benchmark dataset as out-of-distribution from the (in-distribution) test samples.

Due to discrepancy between each of ``ideal'' objectives and its practical threat models, however, the literatures have commonly found that optimizing under a certain threat model often hardly generalizes to other threat ones: \eg 
(a) several works \cite{xie2020adversarial,chun2020empirical,kireev2022effectiveness} have observed that standard adversarial training \cite{madry2018towards} often harms other reliability measures such as corruption robustness or uncertainty estimation, as well as its classification performance; 
(b) Hendrycks et al.~\citet{Hendrycks_2021_ICCV} criticize that none of the previous claims on corruption robustness could consistently generalize on a more comprehensive benchmark.
This also happens even for threat models targeting the same objective: \eg
(c) Yang et al.~\citet{yang2022fsood} show that state-of-the-arts in novelty detection are often too sensitive, so that they tend to also detect ``near-in-distribution'' samples as out-of-distribution and perform poorly on a benchmark regarding this. 
Overall, these observations suggest that one should avoid optimizing reliability measures assuming a specific threat model or benchmark, and motivate to find a new threat model that is generally applicable for diverse scenarios of reliability concerns.

\vspace{0.05in}
\noindent\textbf{Contribution. } In this paper, we propose \emph{nuisance-extended information bottleneck} (NIB), a new training objective targeting model reliability without assuming a prior on domain-specific tasks. Our method is motivated by rethinking the \emph{information bottleneck} (IB) principle \cite{tishby99information,naf} under presence of distribution shifts. Specifically, we argue that a ``robust'' representation $\rvz \coloneqq f(\rvx)$ should always encode \emph{every} signal in the input $\rvx$ that is correlated with the target $\rvy$, rather than extracting only a few shortcuts (\eg Figure~\ref{fig:teaser}). This motivates us to consider an \emph{adversarial} form of threat model on distribution shifts in $\rvx$, under a constraint on the mutual information $I(\rvx, \rvy)$. To implement this idea, we propose a practical design by incorporating a \emph{nuisance representation} $\rvz_n$ alongside $\rvz$ of the standard IB so that $(\rvz, \rvz_n)$ can reconstruct $\rvx$. This results in a novel synthesis of \emph{adversarial autoencoder} \cite{makhzani2015adversarial} and \emph{variational IB} \citep{alemi2016deep} into a single framework. For the architectural side, we propose (a) to utilize the \emph{internal feature statistics} for convolutional network based encoders, and (b) to incorporate \emph{vector-quantized} patch representations for Transformer-based \cite{dosovitskiy2021an} encoders to model $\rvz_n$, mainly to efficiently encode the nuisance $\rvz_n$ (as well as $\rvz$) in a scalable manner.


We perform an extensive evaluation on the representations learned by our scheme, showing comprehensive improvements in modern reliability metrics: including (a) novelty detection, (b) corruption (or natural) robustness, (c) background robustness and (d) certified adversarial robustness. The results are particularly remarkable as the gains are not from assuming a prior on each of specific threat models. For example, we obtain a significant reduction in \mbox{CIFAR-10-C} error rates of the highest severity, \ie by $26.5\% \rightarrow 19.5\%$, without extra {domain-specific} prior as assumed in recent methods \cite{hendrycks2020augmix,hendrycks2022pixmix}. Here, we also show that the effectiveness of our method is scalable to larger-scale (ImageNet) datasets. % in terms of robustness.   
For novelty detection, we could advance AUROCs in recent OBJECTS \cite{yang2022fsood} benchmarks by a large margin of $78.4\% \rightarrow 87.2\%$ in average upon the state-of-the-art, showing that
our representations can provide a more semantic information to better discriminate out-of-distribution samples. Finally, we also demonstrate how the representations can further offer enhanced robustness against adversarial examples, by applying randomized smoothing \cite{cohen2019certified} on them.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{assets/figures/concept_aenib.png}
  \caption{An overview of the proposed framework, the \emph{autoencoder-based nuisance-extended information bottleneck} (AENIB). 
  It illustrates the general pipeline, and Appendix~\ref{appendix:architecture} provides specific instantiations for convolutional and Transformer-based architectures. Overall, we incorporate \textcolor{Green7}{adversarial autoencoder} into \textcolor{blue}{variational information bottleneck} by introducing a \textcolor{red}{\emph{nuisance} $\rvz_n$ (to $\rvy$)} in representation learning.}
  \label{fig:overall}
\vspace{-0.1in}
\end{figure*}


\section{Background}

\vspace{0.05in}
\noindent\textbf{Notation. } Given two random variables $\rvx \in \mathcal{X}$, the input, and $\rvy \in \mathcal{Y}$, the target, 
our goal is is to find a mapping (or an \emph{encoder}) $f: \mathcal{X} \rightarrow \mathcal{Z}$ from data $\mathcal{D}=\{(x_i, y_i)\}^n_{i=1}$\footnote{Although we focus on \emph{supervised learning}, the framework itself in general does not rule out more general scenarios, \eg when the target $\rvy$ can be \emph{self-supervised} from $\rvx$ \cite{oord2018representation,chen2020simclr}.} so that $\rvz:=f(\rvx)$, the representation, can predict $\rvy$ with a simper (\eg linear) mapping \cite{bell1995information,kohonen1990self}. We assume that $f$ is parametrized by a neural network, and is \emph{stochastic} to adopt an information theoretic view \cite{naf}, \ie the encoder output is a random variable defined as $p_f(\rvz|\rvx)$ rather than a constant. Such a modeling can be done through the \emph{reparametrization trick} \cite{kingma2014autoencoding} with an independent random variable $\rveps$ and (deterministic) $f$ by assuming $\rvz := f(\rvx, \rveps)$. For example, one of standard designs parametrizes $f$ by:
\begin{equation}\label{eq:gaussian_decoder}
    f(\rvx, \rveps):= f^{\mu}(\rvx) + \rveps \cdot  f^{\sigma}(\rvx),
\end{equation}
{where $f^{\mu}\in\mathbb{R}^{|\mathcal{Z}|}$ and $f^{\sigma}\in\mathbb{R}_{+}^{|\mathcal{Z}|}$ are deterministic mappings modeling $\mu$ and $\sigma$ in $\mathcal{N}(\rvx; \mu, \sigma^2I)$, respectively, 
so that they can still be learned through a gradient-based optimization.}

The data $\mathcal{D}$ is usually assumed to consist of \textit{i.i.d.}\ samples from a certain \emph{data generating distribution} $(x_i, y_i) \sim p_d(\rvx, \rvy)$. One expects that $f$ learned from $\mathcal{D}$ could generalize to predict $p_d(\rvy|\rvx)$ for unseen samples from $p_d(\rvx, \rvy)$. The formulation, however, does not specify how $f$ should {behave} for inputs that are not likely from $p_d$, say $\hat{x}$. This becomes problematic for those who expect that the decision making of $f$ should be close to that of human being, at least when $\hat{x}$ differs from $p_d$ only up to what humans regard as \emph{nuisance}. This is where the current neural networks commonly fail under the standard training practices. 

\vspace{0.05in}
\noindent\textbf{{Information bottleneck. }} Intuitively, a ``good'' representation $\rvz$ should keep information of $\rvx$ that is correlated with $\rvy$, while preventing $\rvz$ from being too complex. The \emph{information bottleneck} \cite{tishby99information,naf} (IB) is a principled approach to obtain such a succinct representation $\mathbf{x} \rightarrow \mathbf{z}$ for a given downstream task $\mathbf{x} \rightarrow \mathbf{y}$: namely, it finds $\mathbf{z}$ that (a) maximizes the (task-relevant) mutual information $I(\mathbf{z}; \mathbf{y})$, while (b) minimizing $I(\mathbf{x}; \mathbf{z})$ to constrain the capacity of $\mathbf{z}$ for better generalization. 
In other words, it sets the \emph{mutual information} $I(\rvx;\rvz)$ as the complexity measure of~$\rvz$. Specifically, it aims to maximize the following objective: 
\begin{equation}\label{eq:ib}
    \max_f R_{\tt IB}(f),\quad\mbox{for}\quad R_{\tt IB}(f) := I(\rvz;\rvy) - \beta I(\rvx;\rvz), 
\end{equation}
where $\beta \ge 0$ controls the capacity constraint which ensures $I(\rvx;\rvz) \le I_\beta$ for some $I_\beta$. 

\section{Nuisance-extended information bottleneck}

The standard information bottleneck (IB) objective \eqref{eq:ib} obtains a representation $\rvz:=f(\rvx)$ on premise that the future inputs will be also from $p_d(\rvx, \rvy)$. In this paper, we aim to extend IB under assumption that the input $\rvx$ can be corrupted through an \emph{unknown} noisy channel in the future, say $\rvx \rightarrow \hat{\rvx}$, while $\hat{\rvx}$ still preserves the \emph{semantics} of $\rvx$ with respect to $\rvy$: in other words, we assume $I(\rvx; \rvy) = I(\hat{\rvx}; \rvy) > 0$. Intuitively, one can imagine a scenario that $\rvx$ contains multiple signals that each is already highly correlated with $\rvy$, \ie filtering out the remainder from $\rvx$ does not affect its mutual information with $\rvy$. It may or may not be surprising that such signals are quite prevalent in deep neural networks, \eg \citet{ilyas2019adversarial} empirically observe that {adversarial perturbations} \cite{szegedy2014intriguing,goodfellow2014explaining} are {sufficient} for a model to perform accurate classification.

In the context of IB framework, where the goal is to obtain a succinct encoder $f$, it is now reasonable to presume that the noisy channel $\hat{\rvx}$ acts like an \emph{adversary}, \ie it minimizes:
\begin{equation}\label{eq:x_hat}
    \min_{\hat{\rvx} }I(\hat{\rvz}:=f(\hat{\rvx}); \rvy) \text{\ \ subject to\ \ } I(\rvx; \rvy) = I(\hat{\rvx}; \rvy),
\end{equation}
given that one has no information on how the channel would behave \emph{a priori}. This optimization thus would require $f$ to extract \emph{every} signal in $\rvx$ whenever it is highly correlated with $\rvy$, to avoid the case when $\hat{\rvx}$ filters out all the signal except one that $f$ has missed. We notice that, nevertheless, directly optimizing \eqref{eq:x_hat} with respect to $\hat{\rvx}$ is computationally infeasible in practice, considering that (a) it is in many cases an unconstrained optimization in a high-dimensional $\mathcal{X}$, (b) with a constraint on (hard-to-compute) mutual information. 

In this paper, to make sure that $f$ still exhibits the adversarial behavior without \eqref{eq:x_hat}, we propose to let $f$ to model the \emph{nuisance representation} $\rvz_n$ as well as $\rvz$. Specifically, $\rvz_n$ aims to model the ``remainder'' information from $\rvz$ needed to reconstruct $\rvx$, \ie it maximizes $I(\rvx;\rvz, \rvz_n)$. At the same time, $\rvz_n$ compresses out any information that is correlated with $\rvy$, \ie it also minimizes $I(\rvz_n; \rvy)$. Therefore, every information that is correlated with $\rvy$ should be encoded into $\rvz$ in a complementary manner. Here, we remark that now the role of the capacity constraint in \eqref{eq:ib} becomes more important: not only for regularizing $\rvz$ to be simpler, it also penalizes $\rvz_n$ from pushing out unnecessary information to predict $\rvy$ into $\rvz$, making the objective competitive again between $\rvz$ and $\rvz_n$ as like in \eqref{eq:x_hat}. Combined with the original IB \eqref{eq:ib}, we {define \emph{nuisance-extended IB} (NIB) as the following}: 
\begin{equation}\label{eq:nib}
    \max_f R_{\tt NIB}(f) := {R_{\tt IB}(f)} - I(\rvz_n;\rvy) + \alpha I(\rvx; \rvz, \rvz_n), 
\end{equation}
where $\alpha \ge 0$. 
{The proposed {NIB} objective can be viewed as a regularized form of IB by introducing {a nuisance} $\rvz_n$. {Specifically, this optimization additionally forces $I(\mathbf{x}; \mathbf{z}, \mathbf{z}_n)$ and $I(\mathbf{z}_n;\mathbf{y})$ in \eqref{eq:nib} to be maximized and minimized, \ie $H({\mathbf{x}}|{\mathbf{z}},{\mathbf{z}}_n)=0$ and $I({\mathbf{z}}_n; \mathbf{y}) = 0$, respectively.} The following highlights that having these conditions, also with the independence $\rvz\perp\rvz_n$, leads $f$ that can recover the original $I(\rvx;\rvy)$ from $I(\hat{\rvz}; \rvy)$ (see Appendix~\ref{appendix:proof} for the derivation):
\begin{lemma}\label{thm:noisy}
Let $\rvx \in \mathcal{X}$, and $\rvy \in \mathcal{Y}$ be random variables, $\hat{\rvx}$ be a noisy observation of $\rvx$ with $I(\rvx;\rvy) = I(\hat{\rvx};\rvy)$. Given that $[\hat{\rvz}, \hat{\rvz}_n] := f(\hat{\rvx})$ of $\hat{\rvx}$ satisfies (a) $H(\hat{\rvx}|\hat{\rvz}, \hat{\rvz}_n)=0$, (b) $I(\hat{\rvz}_n; \rvy) = 0$, and (c) $\hat{\rvz} \perp \hat{\rvz}_n$, it holds $I(\hat{\rvz};\rvy) = I(\rvx;\rvy)$. 
\end{lemma}
}

In the following sections, we provide a practical design of the proposed NIB based on an autoencoder-based architecture. {Section~\ref{ss:training} and \ref{ss:arch} detail out its losses and architectures, respectively, and Section~\ref{ss:overall} summarizes the overall training.} Figure~\ref{fig:overall} illustrates an overview of our framework.

\subsection{{{\method}: A practical autoencoder-based design}}
\label{ss:training}

{Based on the NIB objective defined in \eqref{eq:nib} {and Lemma~\ref{thm:noisy}},} we design a practical training objective to implement the proposed framework. {Here, we present a simple instantiation of NIB with an autoencoder-based architecture upon \emph{variational information bottleneck} (VIB) \cite{alemi2016deep}, calling it {\emph{autoencoder-based NIB}} ({\method}).}

Overall, Lemma~\ref{thm:noisy} states that a robust encoder $f$ demands for a ``good'' nuisance model that achieves generalization on $\hat{\rvz}$ in three aspects: (a) a \emph{good reconstruction}, (b) \emph{nuisance-ness}, and (c) the \emph{independence between $\rvz$ and $\rvz_n$}. 
{To model these behaviors, we consider a decoder $g: \mathcal{Z} \rightarrow \mathcal{X}$ as well as the encoder $f: \mathcal{X} \rightarrow  \mathcal{Z}$, and adopt the following practical training objectives which incorporates an autoencoder-based loss and two adversarial losses \cite{goodfellow2014gan}:}
\begin{enumerate}[label=(\alph*),leftmargin=8mm]
    \item {We first pose a reconstruction loss to maximize $\log p(\rvx|\rvz, \rvz_n)$; standard designs assume the decoder output to follow $\mathcal{N}(\rvx, \sigma^2 \mathbf{I})$, which is equivalent to the \emph{mean-squared error} (MSE). Here, we use the \emph{normalized MSE} to efficiently balance with other losses}:\footnote{{We also explore a SSIM-based \cite{wang2004image} reconstruction loss as given in Appendix~\ref{appendix:architecture}, which we found beneficial for robustness particularly with Transformer-based models.}}
    \begin{equation}\label{eq:recon}
    \noindent    L_{\tt recon} := \tfrac{1}{\|\rvx\|_2^2}\|\rvx - {g}(\rvz, \rvz_n) \|_2^2 
    \end{equation}
    \item To force the nuisance-ness of $\rvz_n$ with respect to $\rvy$, we approximate $p(\rvy|\rvz_n)$ with a multi-layer perceptron (MLP), say $q_{n}$, and perform an adversarial training: 
    \begin{multline}\label{eq:nuis}
        L_{\tt nuis} := \mathbb{E}_{\rvx}[\mathbb{CE}(q^*_n(\rvz_n), \tfrac{\bm{1}}{|\mathcal{Y}|})], \\ \text{where } q^*_{n} := \min_{q_{n}} \mathbb{E}_{\rvx, \rvy}[\mathbb{CE}(q_n(\rvz_n), \rvy)],
    \end{multline}
    and $\mathbb{CE}$ denotes the cross entropy loss. Here, it optimizes $\mathbb{CE}$ towards the uniform distribution in $\mathcal{Y}$.\footnote{Alternatively, one can directly maximize $\mathbb{CE}(q^*_n(\rvz_n), \rvy)$; we use the current design to avoid potential instability of the maximization-based loss.}
    \item To induce the independence between $\rvz$ and $\rvz_n$, we assume that the joint prior of $\rvz$ and $\rvz_n$ is the isotropic Gaussian, \ie $p(\rvz, \rvz_n) \sim \mathcal{N}(0, \mathbf{I})$, and performs a GAN training with a 2-layer MLP discriminator $q_{\rvz}$:
    \begin{multline}\label{eq:loss_ind}
        L_{\tt ind} := \max_{q_{\rvz}} \mathbb{E}_{\rvx}[\log (q_{\rvz}({f}(\rvx)))] \\
        + \mathbb{E}_{\rvz, \rvz_n \sim \mathcal{N}(0, \mathbf{I})}[\log (1 - q_{\rvz}(\rvz, \rvz_n))].
    \end{multline}
\end{enumerate}
Lastly, to approximate the original IB objective $R_{\tt IB}(f)$ in NIB \eqref{eq:nib}, we instead maximize the \emph{variational information bottleneck} (VIB) \cite{alemi2016deep} {objective} $L_{\tt VIB}^{\beta}$, that can provide a lower bound on $R_{\tt IB}$.\footnote{{A more detailed description on the VIB framework (as well as on GAN) can be found in Appendix~\ref{appendix:tech}.}} Specifically, it makes {variational} approximations of: (a) $p(\rvy|\rvz)$ by a (parametrized) decoder neural network $q(\rvy|\rvz)$, and (b) $p(\rvz)$ by an ``easier'' distribution $r(\rvz)$, \eg isotropic Gaussian $\mathcal{N}(\rvz|0, \mathbf{I})$. Assuming a Gaussian decoder \eqref{eq:gaussian_decoder} for $f(\rvx, \rveps)$, we have:
\begin{multline}\label{eq:loss_vib}
    L_{\tt VIB}^{\beta} := \frac{1}{n} \sum_{i=1}^{n}\ \mathbb{E}_{\rveps}[-\log q(y_i | f(x_i, \rveps))] \\ + \beta\  \mathrm{KL}~(p(\rvz|x_i) \| r(\rvz)).
\end{multline}


\begin{table*}[t]
\centering
\small
    \begin{adjustbox}{width=0.9\linewidth}
    \input{assets/tables/corruption_vit.tex}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{Comparison of average corruption error rates (\%; $\rda$) per severity level on \mbox{CIFAR-10/100-C} \cite{hendrycks2018benchmarking}. Bold and underline denote the best and runner-up, respectively.
    $^\dagger$PixMix \cite{hendrycks2022pixmix} utilizes an external dataset consisting of pattern- and fractal-like images.}\label{tab:corruption}
    \vspace{-0.1in}
\end{table*}

\begin{table}[t]
\centering
\small
    \begin{adjustbox}{width=0.85\linewidth}
    \input{assets/tables/generalization}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{{Comparison of test error rates (\%;~$\rda$) on CIFAR-10 and its variants: CIFAR-10-C/10.1/10.2, and CINIC. Bold and underline indicate the best and runner-up results, respectively.}}\label{tab:corruption_vit}
    \vspace{-0.1in}
\end{table}


\subsection{{Architectures for nuisance modeling}}
\label{ss:arch}

In principle, our framework is generally compatible with any encoder architectures: \eg say an encoder $f: \mathcal{X} \rightarrow  \mathcal{Z}$ and decoder $g: \mathcal{Z} \rightarrow \mathcal{X}$, respectively. In order to apply VIB, we assume that the encoder has two output heads of dimension $2K$, where $K$ denotes the dimension of $\rvz$. Here, each output head models a Gaussian random variable by reparametrization, \ie by modeling $(\mu, \sigma)$ as the encoder output for both $\rvz \in \mathbb{R}^K$ and $\rvz_n\in \mathbb{R}^{K_n}$. 

Although it is possible that $f$ models $\rvz$ and $\rvz_n$ by simply taking deep feed-forward representations following conventions, we observe that modeling nuisances $\rvz_n$ (which is essentially ``generative'') in standard (discriminative) architectures incur a training instability thus in performance: the nuisance information often requires to model finer details in a given inputs, which may be available rather in early layers of $f$, but not in the later layers for classification. 

In this paper, we propose simple architectural treatments to improve the stability of nuisance modeling concerning both convolutional networks and Vision Transformers~\cite{dosovitskiy2021an} (ViTs). 
This section focuses on introducing the design for convolutional networks, and we refer the readers for the ViT-based design to Appendix~\ref{appendix:architecture}: which is even simpler thanks to their patch-level representations available.

Given a convolutional encoder $f$,  we encode $\rvz_n$ (as well as $\rvz$) from the collection of \emph{internal features statistics}, rather than directly using the output of $f$.
Specifically, we extract $L$ intermediate feature maps of a given input $\rvx$, namely $\rvx^{(1)}, \cdots, \rvx^{(L)}$ from $f(\rvx)$, and define the \emph{projection} of $\rvx$ by:
\begin{equation}\label{eq:projection}
    \Pi_{f}(\rvx) := \begin{bmatrix}
        \mathbf{m}^{(1)} & \mathbf{m}^{(2)} & \cdots & \mathbf{m}^{(L)} \\
        \mathbf{s}^{(1)} & \mathbf{s}^{(2)} & \cdots & \mathbf{s}^{(L)}
    \end{bmatrix},
\end{equation}
where $\mathbf{m}^{(l)}$ and $\mathbf{s}^{(l)}$ are the first and second moment of feature maps in $\rvx^{(l)}$, assuming that $\rvx^{(l)} \in \mathbb{R}^{HWC}$: 
$\mathbf{m}^{(l)}_c \coloneqq \tfrac{1}{HW}\sum_{hw} \rvx^{(l)}_{hwc}, \text{ and } \mathbf{s}^{(l)}_c \coloneqq \tfrac{1}{HW}\sum_{hw} (\rvx^{(l)}_{hwc} - \mathbf{m}^{(l)}_c)^2$.

{In Appendix~\ref{appendix:generation}, we demonstrate that this simple projection can sufficiently encode a \emph{generative} representation of $\rvx$: \viz we show that one can successfully and efficiently train GANs with a discriminator defined upon $\Pi_f$.}
Motivated by this observation, we adopt $\Pi_f$ in modeling the encoder representations $\rvz$ and $\rvz_n$. We encode $\rvz$ and $\rvz_n$ by simply applying MLPs to $\Pi_f(\rvx)$ \eqref{eq:projection}. Despite its simplicity, we observe this treatment enables a stable training of {\method}.


\subsection{{Overall training objective}}
\label{ss:overall}

Combining the proposed objectives as well as the VIB loss, $L_{\tt VIB}^{\beta}$ \eqref{eq:loss_vib} leads us to the final objective. Although combining multiple losses in practice may introduce additional hyperparameters, we found most of the proposed losses can be added without scaling except for the reconstruction loss $L_{\tt recon}$ and the $\beta$ in the original VIB loss. Hence, we get: 
\begin{equation}\label{eq:final}
    L_{\tt {\method}} := L_{\tt VIB}^{\beta} + \alpha \cdot L_{\tt recon} + L_{\tt nuis} + L_{\tt ind}.
\end{equation}
Algorithm~\ref{alg:training} in Appendix~\ref{appendix:alg} summarizes the procedure.


\section{Experiments}
\label{s:experiments}


We verify the effectiveness of our proposed {{\method} training} for various aspects of model reliability: specifically, we cover (a) corruption and natural robustness (Section~\ref{exp:corruption}), (b) novelty detection (Section~\ref{exp:ood}), {and} (c) certified adversarial robustness (Section~\ref{exp:adv}) tasks which all have been challenging without task-specific priors \cite{hendrycks2020augmix,hendrycks2018deep,madry2018towards}. We provide an ablation study in Appendix~\ref{appendix:ablation} for a component-wise analysis. We also present an evaluation on our proposed components in the context of generative modeling in Appendix~\ref{appendix:generation}. The full details on the experiments, \eg datasets, training details, and hyperparameters, can be found in Appendix~\ref{appendix:setup}.

\subsection{{Robustness against natural corruptions}}\label{exp:corruption}

We first evaluate corruption robustness of our method, \ie its generalization ability under natural corruptions (\eg fog, brightness, \textit{etc}.) and distribution shifts those are still semantic to humans. To this end, {we consider a wide range of benchmarks those are derived from CIFAR-10 and ImageNet for the purpose of measuring generalization. Namely, for CIFAR-10 models we test on} (a) CIFAR-10/100-C~\citep{hendrycks2018benchmarking}, a corrupted version of CIFAR-10/100 simulating 15 common corruptions in 5 severity levels, as well as (b) CIFAR-10.1 \cite{recht2018cifar10.1}, CIFAR-10.2 \cite{lu2020harder}, and CINIC-10 \cite{darlow2018cinic}, \ie three re-generations of the CIFAR-10 test set. For ImageNet models, on the other hand, we test (a) ImageNet-C \citep{hendrycks2018benchmarking}, a corrupted version of ImageNet validation set, (b) \mbox{ImageNet-R}~\cite{Hendrycks_2021_ICCV}, a collection of rendition images for 200 ImageNet classes, and ImageNet-Sketch \cite{wang2019learning}, as well as (c) the Background Challenge \cite{xiao2021noise} benchmark to evaluate model bias against background changes. This section mainly reports the results from ViT \cite{dosovitskiy2021an,touvron2021training} based architectures, but we also report the results with ResNet-18 \cite{he2016deep} in Appendix~\ref{appendix:additional_exp}.

{Table~\ref{tab:corruption} and \ref{tab:corruption_vit} summarize the results on CIFAR-based models.}
{In Table~\ref{tab:corruption}, we observe that {{\method}} significantly and consistently improves corruption errors upon VIB, and these gains are strong even compared with state-of-the-art methods: \eg 
{{\method}} can solely outperform a strong baseline of AugMix \cite{hendrycks2020augmix}. Although a more recent method of PixMix \cite{hendrycks2022pixmix} could achieve a lower corruption error by utilizing extra (pattern-like) data, we remark that (a) {{\method}} also benefit from PixMix (\ie the extra data) as given in ``{{\method}} + PixMix'', and (b) the results on Table~\ref{tab:corruption_vit} show that the generalization capability of {{\method}} is better than PixMix on CIFAR-10.1, 10.2 and CINIC-10, \ie in beyond common corruptions, by less relying on domain-specific data. 

{Next, Table~\ref{tab:imagenet} highlights that the effectiveness of {\method} can generalize to a more larger-scale, higher-resolution dataset of ImageNet: we still observe that {\method} can consistently improve robust accuracy for diverse corruption types, again without leveraging any further data augmentation during training.}
Figure~\ref{figure:in-vs-out-error} compares the linear trends made by Cross-entropy and {{\method}} across different data augmentations and hyperparameters, confirming that {{\method}} exhibits a better operating points even in terms of \emph{effective robustness} \cite{taori2020measuring}, given the recent observations on the correlation between in- \emph{vs.}~out-of-distribution performances across different models \cite{taori2020measuring,Hendrycks_2021_ICCV,miller2021accuracy}.} 

Lastly, Table~\ref{tab:background} further evaluates the ImageNet classifiers on \emph{Background Challenge} \cite{xiao2021noise}, a benchmark established to test the model robustness against background shifts: specifically, it constructs variants of ImageNet-9 (that combines 370 subclasses of ImageNet; \textsc{Original}) with different combinations of backgrounds. Our AENIB-based models still consistently improve upon the cross-entropy baseline on the benchmark, showing that AENIB indeed tends to learn less-biased features against background changes.  

\begin{table}[t]
\centering
\small
    \begin{adjustbox}{width=\linewidth}
    \input{assets/tables/imagenet_1col}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{Comparison of error rates (\%; $\rda$) or mean corruption error (mCE, \%; $\rda$) on ImageNet (IN) and its variants, namely IN-C \cite{hendrycks2018benchmarking}, IN-R \cite{Hendrycks_2021_ICCV}, and IN-Sketch \cite{wang2019learning}. Bold indicates the best results.}\label{tab:imagenet}
    \vspace{-0.1in}
\end{table}

\begin{table}[t]
\centering
\small
    \begin{adjustbox}{width=\linewidth}
    \input{assets/tables/backgrounds.tex}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{Evaluation of AENIB on Backgrounds Challenge \cite{xiao2021noise} compared to the cross-entropy baseline. All the models are trained on ImageNet, and warped to perform classification on ImageNet-9.}
    \label{tab:background}
    \vspace{-0.1in}
\end{table}

\begin{table}[t]
    \centering
    \Large
    \begin{adjustbox}{width=\linewidth}
    \input{assets/tables/ood_v2}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{{Comparison of AUROC (\%; $\gua$) for OOD detection from CIFAR-10 with five OOD datasets: SVHN, LSUN, ImageNet, CIFAR-100, and CelebA. Bolds indicate the best results.}}\label{tab:ood}
    \vspace{-0.1in}
\end{table}


\begin{table*}[t]
\centering
\small
    \begin{adjustbox}{width=0.93\textwidth}
    \input{assets/tables/newood_far}
    \end{adjustbox}
    \vspace{-0.05in}
    \caption{{Comparison of OOD detection performances on the OBJECTS benchmark \cite{yang2022fsood}, which considers CIFAR-10-C and ImageNet-10 as in-distribution as well as the training in-distribution of CIFAR-10. Bold and underline denote the best and runner-up results, respectively.}}\label{tab:newood_far}
    \vspace{-0.1in}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{figure}[t]
\begin{minipage}{.47\linewidth}
    \centering
    \includegraphics[width=0.92\linewidth]{assets/figures/vit_c10_aenib_v4.pdf}
    \vspace{-0.1in}
    \caption{{Comparison of trends in clean vs.~corruption errors against Gaussian on ViT-S/4.
    }}\label{figure:in-vs-out-error}
\end{minipage}
\hfill
\begin{minipage}{.47\linewidth}
    \centering
    \includegraphics[width=0.92\linewidth]{assets/figures/rs_vit_v5.pdf}
    \vspace{-0.1in}
    \caption{Comparison of certified adversarial robust accuracy at various radii on CIFAR-10.}\label{figure:adv_robustness}
\end{minipage}
\vspace{-0.15in}
\end{figure}

\subsection{Novelty detection}
\label{exp:ood}

{Next, we show that our {{\method}} model can be also a good detector for \emph{out-of-distribution samples} (OODs), \ie to solve the \emph{novelty detection} task.} In general, the task is defined by a binary classification problem that aims to discriminate novel samples from in-distribution samples. A typical practice here is to define a \emph{score function} for each input, \eg the maximum confidence score \cite{hendrycks2017a}, to threshold out samples as out-of-distribution when the score is low. To define a score function for {{\method}} models, we first observe that the \emph{log-likelihood} of $\rvz_n$, which is only available for {{\method}} (and not for standard models), can be a strong score to detect novelties those are semantically far from in-distribution. Specifically, we use $\log \mathcal{N}(\rvz_n; 0, I) = -\tfrac{1}{2} \|\rvz_n \|^2$, as we assume that $\rvz$ follows isotropic Gaussian $\mathcal{N}(0, I)$. For detecting so-called ``harder'' novelties, we propose to use the log-likelihood score of $\rvy$ under a \emph{symmetric {Dirichlet} distribution} of parameter $\alpha > 0$, namely $\mathrm{Dir}_{\alpha}(\rvy) \in \Delta^{|\mathcal{Y}|-1}$, rather than simply using $\max_y p(y|x)$: \ie $\log \mathrm{Dir}_{\alpha}(\rvy) = (\alpha - 1) \sum_i \log y_i$. Note that the distribution gets closer to the {symmetric (discrete)} one-hot distribution as $\alpha \rightarrow 0$, {which makes sense for most classification tasks}, and here we simply use $\alpha=0.05$ throughout experiments.\footnote{{In practice, we observe that other choices in a moderate range of $\alpha$ near 0 do not much affect performance.}} 

We consider two evaluation benchmarks: (a) the \emph{``standard''} benchmark, that has been actively adopted in the literature \cite{hendrycks2017a,liang2018enhancing,lee2018maha}, assumes the CIFAR-10 test set as in-distribution and measures the detection performance of other independent datasets; (b) a recent \emph{OBJECTS} benchmark  \cite{yang2022fsood}, on the other hand, extends the CIFAR-10 benchmark to also consider ``near'' in-distribution in OOD evaluation. Specifically, OBJECTS assumes CIFAR-10-C \cite{hendrycks2018benchmarking} and ImageNet-10 as in-distribution in test-time as well as CIFAR-10, making the detection much more challenging. In this experiment, we compare ResNet-18 \cite{he2016deep} models trained on CIFAR-10 following the setup of \cite{yang2022fsood}.

The results are reported in Table~\ref{tab:ood} and \ref{tab:newood_far} for the standard and OBJECTS benchmarks, respectively. Overall, we confirm that the score function combining the information of $\rvz_n$ and $\rvy$ of {{\method}} significantly improves novelty detection in a complementary manner over strong baselines, showing the effectiveness of modeling nuisance. For example, in Table~\ref{tab:ood}, the combined score achieves near-perfect AUROCs for detecting SVHN, LSUN and ImageNet datasets. Regarding Table~\ref{tab:newood_far}, on the other hand,  {{\method}} improves the previous best AUROC (of Mahalanobis \cite{lee2018maha}) on OBJECTS \emph{vs.}~MNIST from $77.04 \rightarrow 92.43$. The improved results on OBJECTS imply that both of the representation and score obtained from {{\method}} help to better discriminate in- \emph{vs.}~out-of-distribution in more \emph{semantic} senses. 


\subsection{Certified adversarial robustness}
\label{exp:adv}

We also evaluate adversarial robustness \cite{szegedy2014intriguing,goodfellow2014explaining,madry2018towards} adopting the \emph{randomized smoothing} framework \cite{lecuyer2019certified,cohen2019certified} that can measure a \emph{certified} robustness for a given representation. Specifically, any classifier can be robustified by averaging its predictions under Gaussian noise, where the robustness at input $x$ depends on how consistent the classifier is on classifying $\mathcal{N}(x, \sigma^2 \mathbf{I})$ \cite{jeong2020consistency}. {Under this evaluation protocol, we suggest that adversarially-robust representations can be a natural byproduct from {\method} when combined with the randomized smoothing technique, without using any thorough adversarial training methods \cite{madry2018towards} that often require  significant training cost with no certification on the robustness.}

We follow the standard certification protocol \cite{cohen2019certified} to compare the \emph{certified test accuracy at radius $r$}, which is defined by the fraction of the test samples that a smoothed classifier classifies correctly with its certified radius larger than $r$. {We consider ViT-S models on CIFAR-10, and assume $\sigma=0.1$ for this experiment. The results summarized in Figure~\ref{figure:adv_robustness} show that our proposed {{\method}} achieves significantly better certified robustness compared to the baselines at all radii tested: \eg it improves certified robust accuracy of VIB by $39.6\% \rightarrow 56.8\%$ at $\varepsilon=0.1$. Again, the robustness obtained from {\method} is not from specific knowledge on the threat model, which implies that {\method} could offer \emph{free} adversarial robustness when combined with randomized smoothing.} This confirms that the robustness of {{\method}} is not only significant but also consistent \emph{per input}, especially considering its high certified robustness at higher $r$'s.

\begin{figure}[t]
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{assets/figures/swap_zn_disenib.png}
    \caption{DisenIB \cite{pan2021disentangled}}\label{figure:swap_zn_disenib}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.85\linewidth]{assets/figures/swap_zn_aenib.png}
    \caption{AENIB (Ours)}\label{figure:swap_zn_aenib}
\end{subfigure}
\vspace{-0.05in}
\caption{Comparison of CIFAR-10 reconstructions when the nuisance $\rvz_n$ is swapped with those of another (random) sample.}\label{figure:swap_zn}
\vspace{-0.1in}
\end{figure}


\subsection{{Comparison with DisenIB \cite{pan2021disentangled}}}
\label{exp:disenib}

In this section, we provide a comparison of {\method} with a related work of DisenIB \cite{pan2021disentangled}, as well as other variants of VIB, namely Nonlinear-VIB \cite{kolchinsky2019nonlinear} and Squared-VIB \cite{thobaben2020convex}. Here, DisenIB is a variant of IB which also considers a nuisance modeling (based on FactorVAE \cite{kim2018disentangling}), in a purpose of supervised disentangling. Specifically, DisenIB considers two independent encoders $\rvz := f(\rvx)$ and $\rvz_n := g(\rvx)$, and aims to optimize the following objective:
\begin{equation}\label{eq:disenib}
    \max_{f,g} I(\rvz;\rvy) + I(\rvx; \rvz_n, \rvy) - I(\rvz_n; \rvz). 
\end{equation}
Compared to our proposed NIB \eqref{eq:nib}, the most important difference between the two objectives is in their ``reconstruction'' terms: \ie $I(\rvx; \rvz_n, \rvy)$ of \eqref{eq:disenib} \textit{vs.}~$I(\rvx; \rvz, \rvz_n)$ of ours \eqref{eq:nib}. Due to this difference, the DisenIB objective \eqref{eq:disenib} cannot rule out the cases when $\rvz$ only encode few of ``shortcut'' signals in $\rvx$ (correlated to $\rvy$) even at optimum, in contrast to our key motivation of NIB \eqref{eq:nib} that aims to let $\rvz$ to encode \emph{every} $\rvy$-correlated signal in $\rvx$ as much as possible.

{We conduct experimental comparisons based on (a) our CIFAR-10 setups (Table~\ref{tab:corruption_vit}), and (b) directly upon the official implementation\footnote{\url{https://github.com/PanZiqiAI/disentangled-information-bottleneck}} of DisenIB on MNIST \cite{dataset/mnist} (Table~\ref{tab:mnistc} of Appendix~\ref{appendix:mnistc}). Here, we adopt and extend the benchmark to also cover \mbox{\emph{MNIST-C}} \cite{mu2019mnist}, a corrupted version of the MNIST. For the latter comparison, we use a simple 4-layer convolutional network as the encoder architecture. We train every MNIST model here for 100K updates and follow the other training details from the CIFAR experiments (see Appendix~\ref{appendix:setup:detail}).}

Overall, we observe that the effectiveness of {\method} still applies to these benchmarks. This is in contrast to DisenIB, given that the effectiveness from DisenIB, \eg its gain in AUROC on detecting Gaussian noise as an OOD (as conducted by \citet{pan2021disentangled}), could not be further generalized on CIFAR-10-C or MNIST-C, where {{\method}} still improves on as well as achieving the perfect score at the same OOD task. In Figure~\ref{figure:swap_zn_disenib}, we further observe qualitatively that DisenIB often leaves highly semantic information in the nuisance $\rvz_n$: its reconstruction can be completely changed by swapping $\rvz_n$ with those of another sample. This is essentially what AENIB addresses, as compared in Figure~\ref{figure:swap_zn_aenib}. 



\section{Related work}

\vspace{0.05in}
\noindent\textbf{Out-of-distribution robustness. } 
Since the seminal works \cite{szegedy2014intriguing,Nguyen_2015_CVPR,amodei2016concrete} revealing the fragility of neural networks on out-of-distribution inputs, there have been significant attempts on identifying and improving various notions of robustness: \eg detecting novel inputs \cite{hendrycks2017a,lee2018maha,lee2018training,tack2020csi}, robustness against corruptions \cite{hendrycks2018benchmarking,geirhos2018imagenettrained,hendrycks2020augmix,xiao2021noise}, and adversarial noise \cite{madry2018towards,pmlr-v80-athalye18a,cohen2019certified,carlini2019evaluating}, to name a few. Due to its fundamental challenges in making neural network to extrapolate, however, most of the advances in the robustness literature has been made under assuming priors closely related to the individual problems: \eg an external data or data augmentations \cite{hendrycks2018deep,hendrycks2020augmix}, extra information from test-time samples \cite{wang2021tent}, or specific knowledge in threat models \cite{tramer2019adversarial,kang2019transfer}. In this work, we aim to improve multiple notions of robustness without assuming such priors, through a new training scheme that extends the standard information bottleneck principle under noisy observations in test-time. 

\vspace{0.05in}
\noindent\textbf{Hybrid generative-discriminative modeling. }
Our proposed method can be also viewed as a new approach of improving the robustness of discriminative models by incorporating a generative model, in the context that has been explored in recent works \cite{lee2018maha,schott2018towards,Grathwohl2020Your,Yang_2021_ICCV}. For example, \citet{lee2018maha, pmlr-v97-lee19f} have incorporated a simple (but of low expressivity for generation) Gaussian mixture model into discriminative classifiers; a line of research on \emph{Joint Energy-based Models} (JEM) \cite{Grathwohl2020Your,Yang_2021_ICCV} assumes an energy-based model but with a notable training instability for the purpose. In this work, we propose an autoencoder-based model to avoid such training instability, and consider a design that the \emph{nuisance} can succinctly supplement the given discriminative representation to be generative. We demonstrate that our approach can take the best of two worlds; it enables (a) stable training, while (b) attaining the high expressive generative performances.

\vspace{0.05in}
\noindent\textbf{Nuisance modeling. } The idea of incorporating nuisances can be also considered in the context of \emph{invertible} modeling, or as known as \emph{flow-based models} \cite{dinh2016density,kingma2018glow,behrmann2019invertible,grathwohl2019ffjord}, where the nuisance can be defined by splitting the (full-information) encoding $\rvz$ for a given subspace of interest as explored by \citet{jacobsen2018excessive,lynton2020training}. Unlike such approaches, our autoencoder-based nuisance modeling does not focus on the ``full'' invertibility for arbitrary inputs, but rather on inverting the data manifold given, which enabled (a) a much flexible encoder design in practice, and (b) a more scalable generative modeling of nuisance $\rvz_n$, \eg beyond an MNIST-scale as done by \citet{jacobsen2018excessive}. Other related works \cite{jaiswal2018unsupervised, jaiswal2019discovery, pan2021disentangled} do introduce an encoder for nuisance factors, but the notion of nuisance-ness has been focused in terms of the independence to $\rvz$ (for the purpose of feature disentangling), rather than to $\rvy$ as we focus in this work (for the purpose of robustness): \eg DisenIB \cite{pan2021disentangled} applies FactorVAE \cite{kim2018disentangling} between semantic and nuisance embeddings to force their independence. 
Yet, the literature has been also questioned on whether the idea can be scaled-up beyond, \eg MNIST, and our work does explore and establish a practical design with recent architectures and datasets addressing diverse modern security metrics. 

{We provide more extensive and detailed discussions on related works in Appendix~\ref{appendix:related}.}

\section{Conclusion}
\label{s:conclusion}

We suggest that having a good \emph{nuisance model} can be a tangible approach to induce a reliable representation. We develop a practical method of learning deep nuisance representation from data, and show its effectiveness to improve diverse reliability measures under a challenging setup of assuming no prior \cite{taori2020measuring}. We believe our work can be a useful step towards better understanding of out-of-distribution generalization in deep learning. Although the current scope is on a particular design of autoencoder based models, our framework of \emph{nuisance-extended IB} is not limited to it and future works could consider more diverse implementations. {Ultimately, we aim to approximate a challenging form of adversarial training with a mutual information constraint, which we believe will be a promising direction to explore.}

\section*{Acknowledgments}
This work was partly supported by Center for Applied Research in Artificial Intelligence (CARAI) grant funded by Defense Acquisition Program Administration (DAPA) and Agency for Defense Development (ADD) (UD190031RD), and by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2021-0-02068, Artificial Intelligence Innovation Hub; No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)). We thank Subin Kim for the proofreading of our manuscript. 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\clearpage
\appendix
\onecolumn

\begin{center}{\bf {\LARGE Supplementary Material}}
\end{center}
\begin{center}{\bf {\Large Enhancing Multiple Reliability Measures via \\ Nuisance-extended Information Bottleneck}}
\end{center}

\input{appendix}

\end{document}
