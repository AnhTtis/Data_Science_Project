Our method is implemented based on PyTorch~\cite{paszke2019pytorch}. All models are trained on $4$ NVIDIA Quadro RTX 8000 GPUs. The code will be released upon acceptance.

\begin{table*}[t]
\footnotesize
    \centering
    \rowcolors{3}{white}{gray!15}
    \setlength\tabcolsep{8pt}
    \begin{tabular}{c|c|c|c|c|c}
        % \hline
        \toprule\rowcolor{LightBlue} 
         & \multicolumn{4}{c|}{\textbf{PE} / \textbf{NE} / \textbf{PNE} / \textbf{Belief}} & \textbf{hours}\\ \rowcolor{LightBlue} 
        \multirow{-2}{*}{\textbf{$m$}} & \textbf{Error}$\downarrow$ & \textbf{AUROC}$\uparrow$ & \textbf{AUPR}$\uparrow$ & \textbf{FPR at 95\% TPR}$\downarrow$ & \textbf{per epoch}\\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(l){5-5}\cmidrule(l){6-6}
        $0$ & 31.25 / 35.89 / 25.17 / 40.82 & 75.23 / 70.01 / 75.04 / 69.45 & 86.10 / 83.65 / 87.33 / 90.87 & 7.32 / 9.21 / 8.98 / 8.81 & 4\\
        $1$ & 11.21 / 41.20 / 11.16 / 33.33 & 86.42 / 61.26 / \textbf{85.72} / 73.34 & 94.23 / \textbf{96.47} / \textbf{99.43} / \textbf{97.07} & \textbf{4.32} / 5.01 / 5.46 / \textbf{9.09} & 5\\
        % \hline
        $2$ & 11.22 / \textbf{40.14} / 12.18 / 27.91 & \textbf{86.92} / \textbf{63.54} / 85.25 / 83.41 & 96.18 / 89.90 / 93.46 / 90.90 & 4.98 / 5.09 / 4.51 / 9.15 & 9\\
        % \hline
        $3$ & 11.01 / 45.05 / 11.17 / 27.86 & 86.81 / 58.23 / 85.25 / \textbf{85.18} & 98.48 / 95.65 / 99.41 / 90.44 & 4.58 / \textbf{5.00} / 3.98 / \textbf{9.09} & 13\\
        % \hline
        $4$ & 11.28 / 47.12 / 11.17 / 27.52 & 86.56 / 59.01 / 85.13 / 84.72 & 99.43 / 88.98 / 99.40 / 90.42 & 4.64 / 5.02 / 3.21 / \textbf{9.09} & 18\\
        % \hline
        $5$ & \textbf{10.22} / 44.32 / \textbf{11.15} / \textbf{27.49} & 86.86 / 58.87 / 85.30 / 84.66 & \textbf{99.52} / 96.35 / 99.41 / 90.41 & \textbf{4.32} / \textbf{5.00} / \textbf{3.09} / \textbf{9.09} & 23\\
        % \hline
        \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Exploration of number of average primal-dual updating step $m$ on AVA~\cite{gu-2018-cvpr-ava}. }
    \vspace{-2mm}
    \label{tab:ablation-k_iter}
\end{table*}

\begin{table*}[t]
\footnotesize
    \centering  
  \rowcolors{3}{white}{gray!15}
    \setlength\tabcolsep{3pt}
    \begin{tabular}{l|c|c|c|c|c}
        \toprule \rowcolor{LightBlue} 
         & \multicolumn{4}{c|}{\textbf{PE} / \textbf{NE} / \textbf{PNE} / \textbf{Belief}} & \textbf{Closed Set}\\ \rowcolor{LightBlue} 
        \multirow{-2}{*}{\textbf{Method}} & \textbf{Error}$\downarrow$ & \textbf{AUROC}$\uparrow$ & \textbf{AUPR}$\uparrow$ & \textbf{FPR at 95\% TPR}$\downarrow$ & \textbf{mAP}$\uparrow$\\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(l){5-5} \cmidrule(l){6-6}        
         \rowcolor{LightGreen} \sysname{}, R-50 & 11.22 / 40.14 / 12.18 / 27.91 & 86.92 / 63.54 / 85.25 / 83.41 & 96.18 / 89.90 / 93.46 / 90.90 & 4.98 / 5.09 / 4.51 / 9.15 & 27.80\\
        \midrule
        w/o \sysnameacor{} & 50.12 / 50.04 / 12.45 / 39.84 & 51.23 / 51.12 / 84.88 / 82.41  & 86.11 / 86.36 / 88.13 / 92.35 & 15.23 / 15.56 / 34.44 / 9.57 & 25.12\\
        % \hline
        w/o \sysnamemenn{} & 35.12 / 35.31 / 35.12 / 34.78 & 57.10 / 57.59 / 57.88 / 57.01 & 85.71 / 85.65 / 85.66 / 85.66 & 12.03 / 13.00 / 14.98 / 15.32 & 28.81\\
        % \hline
        w/o \sysnameedc{} & 13.16 / 46.32 / 12.16 / 38.23 & 86.00 / 50.13 / 85.12 / 83.05 & 95.12 / 90.67 / 92.45 / 89.19 & 6.31 / 5.05 / 5.01 / 9.59 & 27.16\\
        \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Exploration of different component in \sysname{} with $m=2$ on AVA~\cite{gu-2018-cvpr-ava}. Our complete system is highlighted in \textcolor{Green}{green}.}
    \vspace{-5mm}
    \label{tab:ablation-components}
\end{table*}

\textbf{Datasets.}
Two video datasets covering different cases are used in our experiment.
AVA~\cite{gu-2018-cvpr-ava} is a video dataset for spatio-temporal localizing atomic visual actions. It contains $430$ videos, each with $15$ minutes annotated in $1$-second intervals. Box annotations and their corresponding action labels are provided on key frames. We use version 2.2 of the AVA dataset by default. 
Charades~\cite{sigurdsson2016hollywood_Charades} contains $9,848$ videos that average $30$ seconds in length. This dataset includes $157$ multi-label, daily indoor activities. 

\textbf{Implementation details.} 
Similar to~\cite{pan2020actorcontextactor}, we employ COCO~\cite{lin-2014-eccv} pre-trained Faster R-CNN~\cite{ren-2015-faster} with a ResNeXt-101-FPN~\cite{lin-2017-CVPR} backbone to extract actor and object proposals on key frames. 
To extract context features, Kinetics~\cite{kay-2017-kinetics} pre-trained SlowFast networks~\cite{DBLP:conf/iccv/Feichtenhofer0M19} are used as the backbone of our method. For AVA~\cite{gu-2018-cvpr-ava}, the inputs are $64$-frame clips, where we sample $T=8$ frames with a temporal stride $\tau=8$ for the slow pathway, and $\zeta T$ frames, where $\zeta=4$, for the fast pathway. 
For Charades~\cite{sigurdsson2016hollywood_Charades}, the temporal sampling for the slow pathway is changed to $8\times4$, and the fast pathway takes as an input $32$ continuous frames. 
We train all models end-to-end using synchronous SGD with a batch size of $32$. Linear warm-up~\cite{goyal-2017-arXiv} is performed during the first several epochs. We used both ground-truth boxes and predicted human boxes from~\cite{wu-2019-CVPR} for training. We scale the shorter side of input frames to $256$ pixels for testing. We use detected human boxes with scores greater than $0.85$ for final action detection. 

% \vspace{-1.5mm}
\subsection{Open- and Closed-Set Settings}
Since the above datasets are used for traditional action recognition, we re-split them to adapt to our problem in this work.
% open-set
For open-set settings, videos are evenly divided into three disjoint sets $\mathcal{Z}_1, \mathcal{Z}_2$, and $\mathcal{Z}_3$. We only include actions falling in $\mathcal{Z}_1\cup\mathcal{Z}_2$ for training and actions in $\mathcal{Z}_2\cup\mathcal{Z}_3$ for testing.
Thus $\mathcal{Z}_2$ and $\mathcal{Z}_3$ are a set of known actions and novel actions in inference, respectively.
In practice, each subset in AVA~\cite{gu-2018-cvpr-ava} contains $20$ actions, while actions in Charades~\cite{sigurdsson2016hollywood_Charades} are evenly divided into three subsets ($52/52/53$). 
An actor is considered as novelty (unknown) if it does not contain any action in the training data. 
To detect actors with novel actions, in the testing stage, we ensure that each actor contains ground-truth actions in either $\mathcal{Z}_2$ or $\mathcal{Z}_3$ exclusively \cite{DBLP:conf/nips/WangLBL21}. 
We hence assign each actor in testing videos with a binary novelty label $\{0,1\}$, where $0$ indicates an actor with all known actions in $\mathcal{Z}_2$ and correspondingly, $1$ indicates an actor with all unknown actions in $\mathcal{Z}_3$. 

% closed set 
Closed set action recognition refers to classifying actions into pre-defined categories. Following~\cite{pan2020actorcontextactor}, the closed set studies on AVA~\cite{gu-2018-cvpr-ava} use $235$ videos to train and test on $131$ videos with known actions. 
For Charades~\cite{sigurdsson2016hollywood_Charades}, following~\cite{DBLP:conf/cvpr/ZhangLM21}, we use the officially provided train-test split ($7,985/1,863$) to evaluate the network where all actions are known. 
As this work focuses on open set action recognition, closed set accuracy is for reference only.

\textbf{Evaluation metrics.}
%\label{sec:eval metrics}
Similar to~\cite{liang2017enhancing,liu2020energy}, we adopt the following four metrics to evaluate the performance on novel action detection, \ie, estimate if the action of an actor is novel or not. 
1) \textbf{Detection Error}~\cite{liang2017enhancing} measures the misclassification probability when True Positive Rate (TPR) is $95\%$. The definition of an error $P_e$ is given by $P_e=0.5\cdot(1-\text{TPR})+0.5\cdot\text{FPR}$, where FPR stands for False Positive Rate. 
2) \textbf{AUROC}~\cite{davis2006relationship} is the Area Under the Receiver Operating Characteristic curve, which depicts the relation between TPR and FPR. A perfect detector corresponds to an AUROC score of $1$. 
3) \textbf{AUPR}~\cite{Manning-1999-AUPR} is the Area under the Precision-Recall curve. The PR curve is a graph showing the precision and recall against each other. 
4) \textbf{FPR at 95\% TPR}~\cite{liang2017enhancing} can be interpreted as the probability that a novel example is misclassified as known when TPR is $95\%$.
Additionally, we report the Mean Average Precision (\textbf{mAP}) for $K$-class classification in closed set. 

\subsection{Ablation Study}
To further explore our method, we conduct a detailed ablation study on AVA~\cite{gu-2018-cvpr-ava}. In the following tables, evaluation metrics with ``$\uparrow$" indicate the larger the better, and ``$\downarrow$" indicate the smaller the better.

\textbf{Effectiveness of optimization algorithm.}
We investigate the effectiveness of applying an averaging scheme to the primal sequence in \Cref{alg:algor}. 
According to \Cref{tab:ablation-k_iter}, the larger the primal-dual updating step $m$, the better performance and the lower efficiency. 
If $m=0$, it indicates that we do not use the proposed optimization method.
The dual parameter $\lambda$ in \Cref{eq:primal-solution} is then viewed as a Lagrangian multiplier and set empirically. The results demonstrate the effectiveness of our algorithm in reducing static bias.
Considering the trade-off between performance and efficiency, we use $m=2$ in the following experiments.
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/visualization.pdf}
    \vspace{-7mm}
    \caption{Visual comparison with our method and state-of-the-art on AVA~\cite{gu-2018-cvpr-ava} and Charades~\cite{sigurdsson2016hollywood_Charades}. \textcolor{Cyan}{Cyan} and (\textcolor{Goldenrod!80!black}{yellow}) boxes denote the predictions of actors with known and novel actions, respectively. \textcolor{Green}{\cmark ~marks} and \textcolor{Red}{\xmark~marks} indicate correct and false predictions, respectively.}
    \label{fig:visualization}
    \vspace{-2mm}
\end{figure*}

\textbf{Contribution of important components.}
In Table \ref{tab:ablation-components}, we discuss the contribution of each component of \sysname{} on both open- and closed-set settings as follows.
\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=0ex,partopsep=0ex,parsep=0ex]
    \item \textbf{\sysnameacor{}.} Without \sysnameacor{} representation, we only rely on the video backbone (\ie, SlowFast~\cite{DBLP:conf/iccv/Feichtenhofer0M19}) and a single layer action classifier to predict actions in the video. The results show a significant performance drop in both closed set mAP ($25.12\%$ vs. $27.80\%$) and open set metrics, demonstrating the importance of \sysnameacor{} representation.
    \item \textbf{\sysnamemenn{}.} If we use Beta loss in \Cref{eq:beta-loss} in the network, the open set recognition performance is improved with a slight sacrifice of the closed set mAP ($28.81\%$ vs. $27.80\%$). This is because each class follows a binomial opinion whose conjugate prior is a Beta distribution to benefit open set recognition, rather than a Dirichlet distribution aiming to optimize multi-class classification in a closed-set setting. Although using the Beta loss does not outperform using cross-entropy in a closed-set setting, its performance is still competitive. 
    \item \textbf{\sysnameedc{}.} If we remove the evidence debiasing constraint in network training, our method will suffer from static bias, as explained in \Cref{sec:debias}. The results show that using our \sysnameedc{} module in training brings considerable improvement on all four novelty scores, showing the effectiveness to model static bias. Empirically, we set $\gamma=0.001$ in \Cref{eq:opt-problem}.
\end{itemize}
\textbf{Comparison with novelty estimation mechanisms.} 
As presented in \Cref{tab:ablation-k_iter,tab:ablation-components}, we compare four novelty estimation mechanisms in \Cref{sec:novelty_estimation}. It can be concluded that uncertainty-based scores perform better than belief based one. 
According to \Cref{eq:element-wise-sl}, a belief based score is calculated based on negative evidence. 
We speculate that it is more intuitive and accurate to estimate if an actor performs the same action rather than different actions. 
In summary, positive evidence is more reliable than negative evidence. In the following, we only report Positive Evidence (PE) scores to detect novel actions for clarity.
\begin{table}[t]
\scriptsize
\rowcolors{3}{white}{gray!15}
    \centering
    \setlength\tabcolsep{1.2pt}
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule  
        \rowcolor{LightBlue} \textbf{Methods} & \textbf{Pre-train} & \textbf{Error}$\downarrow$ & \textbf{AUROC}$\uparrow$ & \textbf{AUPR}$\uparrow$ & 
        \textbf{FPR at}$\downarrow$ & \textbf{Closed Set}\\
        \rowcolor{LightBlue} &&&&&\textbf{95\% TPR}& \textbf{mAP}$\uparrow$\\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(l){6-6} \cmidrule(l){7-7}
        % \hline
        Slowfast, R-101~\cite{DBLP:conf/iccv/Feichtenhofer0M19} & K600 & 60.12 & 50.15 & 70.15 & 20.17 & 29.00\\
        % \hline        
        ACAR, R-50~\cite{pan2020actorcontextactor} & K400 & 35.16 & 52.89 & 79.15 & 14.16 & 28.84\\
        % \hline
        ACAR, R-101~\cite{pan2020actorcontextactor} & K700 & 32.26 & 55.18 & 82.15 & 10.16 & \textbf{33.30}\\
        % \hline
        AFAC, R-101~\cite{zhang2021multi}& K600 & 53.14 & 79.69 & 90.79 & 7.15 & 30.20\\ 
        % \hline
        AIA, R-101~\cite{tang2020asynchronous} & K700 & 35.14 & 54.17 & 78.49 & 10.15 & 32.30\\
        % \hline
        DEAR, R-50~\cite{bao-2021-ICCV}& K400 & 23.22 & 82.12 & 83.15 & 8.45 & 18.51\\
        \rowcolor{LightGreen}
        \midrule
        \sysname{} (Ours), R-50 & K400 & 11.22 & 86.92 & 96.18 & 4.98 & 27.80\\ \rowcolor{LightGreen}
        \sysname{} (Ours), R-101 & K700 & \textbf{10.12} & \textbf{88.75} & \textbf{98.18} & \textbf{4.17} & 29.87\\
        \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparison with state-of-the-art on AVA~\cite{gu-2018-cvpr-ava}. Ours is highlighted in \textcolor{Green}{green}. Best value is in \textbf{bold}.}
    \vspace{-3mm}
    \label{tab:baseline_AVA}
\end{table}

\subsection{Results Analysis}
For a fair comparison with previous single-actor/single-action methods, we enhance them to work within the new multi-actor/multi-action paradigm.
Specifically, we calculate positive evidence by the function $\text{ReLU}(h(\mathbf{x};\boldsymbol{\theta}))+1$ (same as described in \Cref{sec:edl}), based on the last layer outputs $h(\mathbf{x};\boldsymbol{\theta})$. 
We apply single-actor based DEAR~\cite{bao-2021-ICCV} to a bounding volume around each detected actor for a multi-actor setting.

According to \Cref{tab:baseline_AVA}, our method is compared with existing methods with both ResNet-50 and ResNet-101 backbones on AVA~\cite{gu-2018-cvpr-ava}.
The baseline SlowFast~\cite{DBLP:conf/iccv/Feichtenhofer0M19} achieves a reasonable closed set mAP score but fails in open set recognition.
Other state-of-the-art methods including ACAR~\cite{pan2020actorcontextactor}, AFAC~\cite{zhang2021multi}, and AIA~\cite{tang2020asynchronous} obtain better accuracy in the closed set but are still unsatisfying to detect novel actions.
Compared with DEAR~\cite{bao-2021-ICCV} using ENNs, our method achieves much better performance in both closed set and open set metrics. This is because our network can handle the situation that each actor may contain more than one action. 
In addition, simple bounding volume strategy is not effective to extract global information in the multi-actor setting.
It indicates the effectiveness of the proposed \sysnamemenn{} for multi-label open set action recognition. 

Furthermore, we observe a similar trend on Charades~\cite{sigurdsson2016hollywood_Charades}.
From \Cref{tab:baseline_Charades}, our \sysname{} achieves comparable closed set mAP and much better open set performance compared with the state-of-the-art. 
It is worth mentioning that AFAC~\cite{zhang2021multi} and CSN~\cite{tran2019video} achieve the best closed set accuracy by using powerful CSN-152 backbone \cite{DBLP:conf/cvpr/ZhangLM21} and IG-65M pre-trained dataset. However, our method using ResNet backbone still obtains a considerable gain in terms of AUROC and FPR at 95\% TPR. 
It is a good fit for the situation that requires lower probability of misclassifying a novel instance as known.

As shown in \Cref{fig:visualization}, we provide several visual results on two datasets, where our method is compared with DEAR~\cite{bao-2021-ICCV}, AFAC~\cite{zhang2021multi} and SlowFast~\cite{DBLP:conf/iccv/Feichtenhofer0M19}. 
The examples show that other methods output several false predictions of known and novel actions in a multi-actor/multi-action setting.
In contrast, our method can handle novelty detection in different situations more accurately.
\begin{table}[t]
\scriptsize
\rowcolors{3}{white}{gray!15}
    \centering
    \setlength\tabcolsep{1.2pt}
    %\resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule  
        \rowcolor{LightBlue} \textbf{Methods} & \textbf{Pre-train} & \textbf{Error}$\downarrow$ & \textbf{AUROC}$\uparrow$ & \textbf{AUPR}$\uparrow$ & 
        \textbf{FPR at}$\downarrow$ & \textbf{Closed Set}\\
        \rowcolor{LightBlue} &&&&&\textbf{95\% TPR}& \textbf{mAP}$\uparrow$\\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(l){6-6} \cmidrule(l){7-7}
        Slowfast, R-101~\cite{DBLP:conf/iccv/Feichtenhofer0M19} & K600 & 25.15 & 79.12 & 79.15 & 50.15 & 45.20\\
        % \hline
        X3D-XL~\cite{feichtenhofer2020x3d} & K600 & 8.45 & 82.15 & 86.49 & 39.98 & 47.20\\ 
        % \hline
        AFAC, R-101~\cite{zhang2021multi} & K600 & 7.00 & 82.20 & 90.15 & 35.19 & 48.10 \\
        % \hline
        AFAC, CSN-152~\cite{zhang2021multi} & IG-65M & 6.18 & 80.12 & 90.79 & 30.15 & \textbf{50.30}\\
        % \hline
        CSN, CSN-152~\cite{tran2019video} & IG-65M & 6.89 & 80.15 & \textbf{92.61} & 35.16  & 46.40\\
        % \hline
        DEAR, R-50~\cite{bao-2021-ICCV}& K400 & 12.15 & 86.15 & 92.35 & 29.96 & 38.12\\
        \rowcolor{LightGreen}
        \midrule
        \sysname{} (Ours), R-50 & K400 & \textbf{6.15} & 85.49 & 90.78 & 25.98 & 45.33\\ \rowcolor{LightGreen}
        \sysname{} (Ours), R-101 & K700 & 6.23 & \textbf{88.49} & 91.15 & \textbf{25.15} & 47.21\\
        \bottomrule
    \end{tabular}
    %}
    \vspace{-3mm}
    \caption{Comparison with state-of-the-art on Charades~\cite{sigurdsson2016hollywood_Charades}. Ours is highlighted in \textcolor{Green}{green}. Best value is in \textbf{bold}.}
    \vspace{-3mm}
    \label{tab:baseline_Charades}
\end{table}