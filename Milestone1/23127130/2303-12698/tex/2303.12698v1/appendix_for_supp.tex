\section{Notations}
\label{sec:notations}
Vectors are denoted by lower case bold face letters, \textit{e.g.,} positive and negative evidence $\boldsymbol{\alpha,\beta}\in\mathbb{R}^k$. Vectors with subscripts of indices, such as $\alpha_i, \beta_i$ indicate the $i$-th entry in $\boldsymbol{\alpha,\beta}$. The Euclidean $\ell_2$-norm is denoted as $\lVert\cdot\rVert$. 
% Given a differentiable function $F(\boldsymbol{\theta},\lambda):\Theta\times\mathbb{R}_+\rightarrow\mathbb{R}$, the gradient at $\boldsymbol{\theta}$ and $\lambda$ is denoted as $\nabla_{\boldsymbol{\theta}}F(\boldsymbol{\theta},\lambda)$ and $\nabla_\lambda F(\boldsymbol{\theta},\lambda)$, respectively. 
Scalars are denoted by lowercase italic letters, \textit{e.g.,} $\eta>0$. Matrices are denoted by capital italic letters, \textit{e.g.,} $X\in\mathbb{R}^{C\times H\times W}$. Proposition
$[u]_+$ denotes the  projection of $[u]$ on the nonnegative orthant.
\begin{table}[h]
% \vspace{-5mm}
   \small
    \centering
    \rowcolors{2}{white}{gray!15}
    \begin{tabular}{p{0.18\linewidth} p{0.75\linewidth}}
        \toprule\rowcolor{LightBlue} 
        \textbf{Notations} & \textbf{Descriptions} \\
        \midrule
        $N$ & Number of detected persons\\
        $M$ & Number of detected objects\\
        $\boldsymbol{\theta}$ & Parameters of the backbone to calculate \sysnameacor\\
        $\mathbf{x}$ & Input video\\
        $X$ & Context feature\\
        $C, H, W$ & Channel, height and width\\
        $A, O$ & Actor (person) and object feature\\
        $F$ & Actor-context-object relation feature\\
        $H$ & Actor relational feature\\
        $\omega$ & Subjective opinion\\
        $b,d$ & Belief and disbelief masses\\
        $u$ & Uncertainty mass\\
        $W$ & Non-informative prior weight\\
        $a$ & Base rate\\
        $K$ & Total number of classes\\
        $\alpha, \beta$ & Positive and negative evidence\\
        $p$ & Class probability\\
        $\gamma$ & The independence criterion relaxation of \sysnameedc{}\\
        $m$  & primal-dual updating step\\
        \bottomrule
    \end{tabular}
    \caption{Important notations and corresponding descriptions.}
    \label{tab:notations}
\vspace{-5mm}
\end{table}
   
\section{Proof Sketch of Proposition 1}
\label{sec:proof of prop1}
 
%\noindent \textbf{Proposition 1.} 
\begin{proposition}
\textit{(Convergence of Averaged Primal Sequence) Under Assumption 1, when the convex set $\Theta$ is compact, let the approximate primal sequence $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}^{\infty}_{m=1}$ be the running averages of the primal iterates given in Equation (6). 
\label{prop1}
Then $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}^{\infty}_{m=1}$ can converge to its limit $\Tilde{\boldsymbol{\theta}}^*$.}
\end{proposition}

To prove the convergence in \cref{prop1}, we first prove the below \cref{lemma:cauchy} that
\begin{lemma}
\label{lemma:cauchy}
The approximate primal sequence $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}^{\infty}_{m=1}$ given in Equation (6) is a Cauchy sequence. That is $\forall \epsilon>0$, there is a $Q\in\mathbb{N}$ such that $||\Tilde{\boldsymbol{\theta}}^{(m')}-\Tilde{\boldsymbol{\theta}}^{(m)}||\leq\epsilon$, $\forall m',m\geq Q$.
\end{lemma}

\begin{proof}
Given Equation (6), we derive
\begin{equation}
\small
\begin{aligned}
    \Tilde{\boldsymbol{\theta}}^{(m+1)} 
    &= \frac{1}{m+1} \sum_{i=1}^{m} \boldsymbol{\theta}^{(i)} \\
    &= \frac{1}{m+1} \big(\boldsymbol{\theta}^{(m)}+\sum_{i=1}^{m-1} \boldsymbol{\theta}^{(i)} \big) \\
    &= \frac{1}{m+1} \boldsymbol{\theta}^{(m)} + \frac{m}{m+1}\cdot\frac{1}{m}\sum_{i=1}^{m-1} \boldsymbol{\theta}^{(i)} \\
    &= \frac{1}{m+1} \boldsymbol{\theta}^{(m)} + \frac{m}{m+1}\Tilde{\boldsymbol{\theta}}^{(m)} \\
    &= \frac{1}{m+1} \boldsymbol{\theta}^{(m)} + \Tilde{\boldsymbol{\theta}}^{(m)} -\frac{1}{m+1} \Tilde{\boldsymbol{\theta}}^{(m)}.
\end{aligned}
\end{equation}
Rearrange the above equation, we have
\begin{equation}
\small
\begin{aligned}
    \Tilde{\boldsymbol{\theta}}^{(m+1)}-\Tilde{\boldsymbol{\theta}}^{(m)} &= \frac{1}{m+1}(\boldsymbol{\theta}^{(m)}-\Tilde{\boldsymbol{\theta}}^{(m)}).
\end{aligned}
\end{equation}
Under Assumption 1, $\Theta$ is a compact convex set and $\Tilde{\boldsymbol{\theta}}^{(m)},\boldsymbol{\theta}^{(m)} \in\Theta$. Let $m'>m$ and $||\Tilde{\boldsymbol{\theta}}^{(m)}||,||\boldsymbol{\theta}^{(m)}||\leq G$, we have
\begin{equation}
\footnotesize
\begin{aligned}
    ||\Tilde{\boldsymbol{\theta}}^{(m')}-\Tilde{\boldsymbol{\theta}}^{(m)}|| 
    &= ||\Tilde{\boldsymbol{\theta}}^{(m')}-\Tilde{\boldsymbol{\theta}}^{(m'-1)}+\cdots+\Tilde{\boldsymbol{\theta}}^{(m+1)}-\Tilde{\boldsymbol{\theta}}^{(m)}|| \\
    &=||\frac{\boldsymbol{\theta}^{(m'-1)}-\Tilde{\boldsymbol{\theta}}^{(m'-1)}}{m'}+\cdots+\frac{\boldsymbol{\theta}^{(m)}-\Tilde{\boldsymbol{\theta}}^{(m)}}{m+1}|| \\
    &\leq \frac{||\boldsymbol{\theta}^{(m'-1)}||+||\Tilde{\boldsymbol{\theta}}^{(m'-1)}||}{m'}+\cdots+\frac{||\boldsymbol{\theta}^{(m)}||+||\Tilde{\boldsymbol{\theta}}^{(m)}||}{m+1} \\
    &\leq \frac{2G(m'-m)}{m+1}.
\end{aligned}
\end{equation}
Therefore, for any arbitrary $\epsilon>0$, let $\frac{2G(m'-m)}{m+1}<\epsilon$, and we have $||\Tilde{\boldsymbol{\theta}}^{(m')}-\Tilde{\boldsymbol{\theta}}^{(m)}||\leq \epsilon$. Therefore we conclude that $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}^{\infty}_{m=1}$ is a Cauchy sequence.
\end{proof}

Next we prove the proposed \cref{prop1}.

\begin{proof}
As stated in \cref{lemma:cauchy} that $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}$ in Equation (6) is a Cauchy sequence, it hence is bounded and there exists a subsequence $b_n$ converging to its limit $L$. For any $\epsilon>0$, there exists $n,q\geq Q$ satisfying $||\Tilde{\boldsymbol{\theta}}^{(n)} - \Tilde{\boldsymbol{\theta}}^{(q)}|| < \frac{\epsilon}{2}$. Thus, there is a $b_m=\Tilde{\boldsymbol{\theta}}^{(q_m)}$, such that $q_m\geq Q$ and $||b_{q_m}-L||<\frac{\epsilon}{2}$.
\begin{equation}
\small
\begin{aligned}
    ||\Tilde{\boldsymbol{\theta}}^{(n)} - L||
    &= ||\Tilde{\boldsymbol{\theta}}^{(n)} - b_m + b_m - L|| \\
    &\leq ||\Tilde{\boldsymbol{\theta}}^{(n)} - b_m|| + ||b_m - L||  \\
    &< ||\Tilde{\boldsymbol{\theta}}^{(n)} - \Tilde{\boldsymbol{\theta}}^{(q)}|| + \frac{\epsilon}{2} \\
    &< \epsilon.
\end{aligned}
\end{equation}
Since $\epsilon$ is arbitrarily small, we prove that the sequence $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}^{\infty}_{m=1}$ converges to its limit $L = \Tilde{\boldsymbol{\theta}}^*$ asymptotically.
\end{proof}  


\begin{table*}[t]
% \vspace{-3mm}
\footnotesize
    \centering
    \rowcolors{3}{white}{gray!15}
    \setlength\tabcolsep{5pt}
    \begin{tabular}{c|l|c|c|c|c}
        % \toprule
        % $\mathbf{m}$ & \multirow{2}{*}{\textbf{Method}} & \textbf{Error} $\downarrow$ & \textbf{AUROC} $\uparrow$ & \textbf{AUPR} $\uparrow$ & \textbf{FPR at 95\% TPR} $\downarrow$ \\
        % \textbf{iterations} & & \multicolumn{4}{c}{\textbf{PE} / \textbf{NE} / \textbf{PNE} / \textbf{Belief}} \\
        
        \toprule\rowcolor{LightBlue} 
        \textbf{m} & & \multicolumn{4}{c}{\textbf{PE} / \textbf{NE} / \textbf{PNE} / \textbf{Belief}}\\ \rowcolor{LightBlue} 
        \textbf{iterations} & \multirow{-2}{*}{\textbf{Method}} & \textbf{Error} $\downarrow$ & \textbf{AUROC} $\uparrow$ & \textbf{AUPR} $\uparrow$ & \textbf{FPR at 95\% TPR} $\downarrow$ \\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(l){5-5} \cmidrule(l){6-6}

        \rowcolor{LightGreen} &\sysname{} & 11.22 / 40.14 / 12.18 / 27.91 & \textbf{86.92} / \textbf{63.54} / 85.25 / 83.41 & 96.18 / 89.90 / 93.46 / 90.90 & 4.98 / 5.09 / 4.51 / 9.15\\
        \cmidrule{2-6}
        &w/o \sysnameacor{} & 50.12 / 50.04 / 12.45 / 39.84 & 51.23 / 51.12 / 84.88 / 82.41  & 86.11 / 86.36 / 88.13 / \textbf{92.35} & 15.23 / 15.56 / 34.44 / 9.57\\
        % \hline
        &w/o \sysnamemenn{} & 35.12 / \textbf{35.31} / 35.12 / 34.78 & 57.10 / 57.59 / 57.88 / 57.01 & 85.71 / 85.65 / 85.66 / 85.66 & 12.03 / 13.00 / 14.98 / 15.32\\
        % \hline
        \multirow{-5}{*}{\rotatebox[origin=c]{90}{$\mathbf{m=2}$}}  &w/o \sysnameedc{} & 13.16 / 46.32 / 12.16 / 38.23 & 86.00 / 50.13 / 85.12 / 83.05 & 95.12 / 90.67 / 92.45 / 89.19 & 6.31 / 5.05 / 5.01 / 9.59\\
        \bottomrule
        
        % \midrule
        \rowcolor{LightGreen} &  \sysname{} & 11.01 / 45.05 / 11.17 / 27.86 & 86.81 / 58.23 / 85.25 / 85.18 & 98.48 / 95.65 / \textbf{99.41} / 90.44 & 4.58 / \textbf{5.00} / 3.98 / 9.09 \\
        \cmidrule{2-6}
        & w/o \sysnameacor{} & 48.34 / 50.32 / 17.77 / 47.79 & 52.78 / 50.15 / 83.34 / 79.26 & 86.39 / 83.39 / 93.93 / 88.93 & 13.35 / 15.01 / 25.93 / 10.00\\
        % \hline
        & w/o \sysnamemenn{} & 32.64 / 45.12 / 50.14 / 21.71 & 61.41 / 55.23 / 80.12 / 83.23 & 83.67 / 83.39 / 84.03 / 88.66 & 12.21 / 15.23 / 12.24 / 22.22 \\
        % \hline
        \multirow{-5}{*}{\rotatebox[origin=c]{90}{$\mathbf{m=3}$}} & w/o \sysnameedc{} & \textbf{10.12} / 48.12 / 11.86 / 38.43 & 80.12 / 52.01 / 84.38 / 73.94 & 93.34 / 93.41 / 98.92 / 89.33 & 5.15 / 6.03 / 7.41 / 10.70 \\
        % \hline
    
        %& w/o \cref{alg:algor} & 30.12 / 43.65 / 21.21 / 40.17 & 60.26 / 54.32 / 75.14 / 63.12 & 86.36 / 72.32 / 79.94 / 85.39 & 6.00 / 7.25 / 7.21 / 3.89 \\
        \bottomrule
        
        \rowcolor{LightGreen} & \sysname{} & 11.28 / 47.12 / 11.17 / 27.52 & 86.56 / 59.01 / 85.13 / 84.72 & 99.43 / 88.98 / 99.40 / 90.42 & 4.64 / 5.02 / 3.21 / 9.09 \\
        \cmidrule{2-6}
        & w/o \sysnameacor{} & 50.21 / 51.71 / 17.64 / 49.35 & 52.35 / 50.01 / 84.17 / 80.33 & 83.36 / 83.49 / 94.97 / 88.21 & 11.02 / 12.65 / 33.33 / 10.23\\
        % \hline
        & w/o \sysnamemenn{} & 42.04 / 50.15 / 48.14 / 29.21 & 83.01 / 51.36 / \textbf{89.33} / 59.63 & 87.21 / 93.39 / 84.29 / 86.48 & 10.01 / 12.12 / 14.00 / 9.70\\
        % \hline
        \multirow{-5}{*}{\rotatebox[origin=c]{90}{$\mathbf{m=4}$}} & w/o \sysnameedc{} & 13.35 / 48.01 / \textbf{10.86} / 34.28 & 82.51 / 50.21 / 85.51 / 78.26 & 93.34 / 93.93 / 98.94 / 89.17 & 5.69 / 6.16 / 3.73 / 9.62\\
        % \hline
        
        %& w/o \cref{alg:algor} & 30.15 / 34.98 / 21.07 / 37.25 & 69.23 / 59.98 / 85.68 / 62.40 & 86.32 / 76.38 / 89.97 / 86.65 & 6.98 / 9.46 / 7.85 / 5.26\\
        \bottomrule
        
        \rowcolor{LightGreen} & \sysname{} & 10.22 / 44.32 / 11.15 / 27.49 & 86.86 / 58.87 / 85.30 / 84.66 & \textbf{99.52} / \textbf{96.35} / \textbf{99.41} / 90.41 & \textbf{4.32} / \textbf{5.00} / \textbf{3.09} / 9.09 \\
        \cmidrule{2-6}
        & w/o \sysnameacor{} & 49.61 / 50.00 / 16.61 / 48.82 & 51.01 / 51.97 / 84.78 / 81.29 & 82.16 / 83.33 / 98.05 / 88.09 & 10.23 / 12.68 / 37.04 / 9.16\\
        % \hline
        & w/o \sysnamemenn{} & 41.34 / 49.67 / 49.21 / \textbf{10.75} & 84.83 / 52.41 / 82.91 / \textbf{90.44} & 86.13 / 93.40 / 87.27 / 90.29 & 9.97 / 11.98 / 12.68 / 13.01\\
        % \hline
        \multirow{-5}{*}{\rotatebox[origin=c]{90}{$\mathbf{m=5}$}}& w/o \sysnameedc{} & 11.09 / 48.01 / \textbf{10.86} / 32.14 & 79.34 / 50.98 / 85.12 / 75.23 & 93.33 / 94.02 / 98.94 / 91.14 & 5.39 / 5.79 / 3.70 / \textbf{7.11}\\
        % \hline
         
        %& w/o \cref{alg:algor} & 31.05 / 38.95 / 21.15 / 38.79 & 70.01 / 62.35 / 85.05 / 62.99 & 86.33 / 86.31 / 89.42 / 86.24 & 7.31 / 8.13 / 6.86 / 4.58\\
        \bottomrule
    \end{tabular}
    % \vspace{-3mm}
    \caption{Exploration of different component in \sysname{} with $m=2,3,4,5$.}
    \label{tab:ablation-compinent_m345}
\end{table*}

  
%\section{Proof Sketch of \cref{prop:prop2}}
\section{Proof Sketch of Proposition 2}
\label{sec:proof of prop2}

%\noindent \textbf{Proposition 2.} 
\begin{proposition}
\textit{(Bounds for $\mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)})$ and the violation of $g(\Tilde{\boldsymbol{\theta}}^{(m)})$~\cite{averagedDS-SJO-2009})
\label{prop2}
Let the dual sequence $\{\lambda^{(m)}\}^{\infty}_{m=1}$ be generated through Equation (8) and $\{\Tilde{\boldsymbol{\theta}}^{(m)}\}^{\infty}_{m=1}$ be the averages in Equation (6). Under Assumption 1, we have
\begin{enumerate}
    \item An upper bound on the amount of constraint violation of $\Tilde{\boldsymbol{\theta}}^{(m)}$ that $\big\lVert\big[g(\Tilde{\boldsymbol{\theta}}^{(m)})\big]_+\big\rVert\leq \frac{\lambda^{(m)}}{m\eta_2}$.
    \item An upper bound on $\mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)})$ that $\mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)}) \leq f^*+\frac{(\lambda^{(0)})^2}{2m\eta_2}+\frac{\eta_2 L^2}{2}$, where $\big\lVert g(\Tilde{\boldsymbol{\theta}}^{(m)})\big\rVert<L$ and $L>0$.
    \item A lower bound $\mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)}) \geq f^*-\lambda^*\cdot\big\lVert\big[g(\Tilde{\boldsymbol{\theta}}^{(m)})\big]_+\big\rVert$.
\end{enumerate}
where $[u]_+$ denotes the projection of $[u]$ on the nonnegative orthant. $f^*$ is the optimal solution of Equation (5) and $\lambda^\ast$ denotes the optimal value of the dual variable.}  
\end{proposition}

\begin{proof}
1. According to Equation (8), we have 
\begin{align*}
    \lambda^{(m)} 
    % &= \Big[\lambda^{(m-1)}+\eta\cdot\nabla_{\lambda}F(\Tilde{\boldsymbol{\theta}}^{(m)},\lambda^{(m-1)})\Big]_+\\
    % &= \Big[\lambda^{(m-1)}+\eta\cdot\Big(g(\Tilde{\boldsymbol{\theta}}^{(m)})-\gamma-\frac{\delta}{2}\Big)\Big]_+\\
    &\geq \lambda^{(m-1)}+\eta_2\cdot\Big(g(\Tilde{\boldsymbol{\theta}}^{(m)})-\gamma-\delta\lambda^{(m-1)}\Big).
\end{align*}
Under Assumption 1, $g(\boldsymbol{\theta})$ is convex, we have
\begin{equation}
\small
\begin{aligned}
    g(\Tilde{\boldsymbol{\theta}}^{(m)})
    &\leq \frac{1}{m}\sum_{i=1}^{m-1} g(\boldsymbol{\theta}^{(i)})\\
    &= \frac{1}{m\eta_2} \sum_{i=1}^{m-1} \eta_2 g(\boldsymbol{\theta}^{(i)})\\
    &\leq \frac{1}{m\eta_2} (\lambda^{(m)}-\lambda^{(0)})\\
    &\leq \frac{\lambda^{(m)}}{m\eta_2}, \quad\forall m\geq 1.
\end{aligned}
\end{equation}
Since $\lambda^{(m)}\geq 0$, we derive $||[g(\Tilde{\boldsymbol{\theta}}^{(m)})]_+||\leq \frac{\lambda^{(m)}}{m\eta_2}$.

2. Under Assumption 1 and Equations (7) and (8), we have $q^*=f^*$. Together with the condition that $f(\boldsymbol{\theta})$ is convex and $\boldsymbol{\theta}\in\Theta$, we have
\begin{equation}
\footnotesize    
\begin{aligned}
    \mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)})
    &\leq \frac{1}{m}\sum_{i=0}^{m-1} \mathcal{L}(\boldsymbol{\theta}^{(i)}) \\
    &=\frac{1}{m}\sum_{i=0}^{m-1} \Big(\mathcal{L}(\boldsymbol{\theta}^{(i)})+\lambda^{(i)} g(\Tilde{\boldsymbol{\theta}}^{(i+1)})-\lambda^{(i)}g(\Tilde{\boldsymbol{\theta}}^{(i+1)}) \Big) \\
    &=\frac{1}{m}\sum_{i=0}^{m-1} \Big(\mathcal{L}(\boldsymbol{\theta}^{(i)})+\lambda^{(i)} g(\Tilde{\boldsymbol{\theta}}^{(i+1)})\Big)- \frac{1}{m}\sum_{i=0}^{(m-1)}\lambda^{(i)} g(\Tilde{\boldsymbol{\theta}}^{(i+1)}) \\
    % &= \frac{1}{m}\sum_{i=0}^{(m-1)} q(\lambda^i)- \frac{1}{m}\sum_{i=0}^{(m-1)}\lambda^i\cdot g(\boldsymbol{\theta}^{(i+1)})\\
    &\leq q^* -\frac{1}{m}\sum_{i=0}^{m-1}\lambda^{(i)}g(\Tilde{\boldsymbol{\theta}}^{(i+1)}).
\end{aligned}
\end{equation}
From Equation (8), we have
\begin{equation}
\small    
\begin{aligned}
    (\lambda^{(i+1)})^2
    % &= \Big(\Big[\lambda^{(i)}+\eta\nabla_{\lambda}F(\Tilde{\boldsymbol{\theta}}^{(i+1)},\lambda^{(i)})\Big]_+\Big)^2\\
    &= \Big(\Big[\lambda^{(i)}+\eta_2\Big(g(\Tilde{\boldsymbol{\theta}}^{(i+1)})-\gamma-\delta\lambda^{(i)}\Big)\Big]_+\Big)^2\\
    &\leq \Big(\lambda^{(i)}+\eta_2 g(\Tilde{\boldsymbol{\theta}}^{(i+1)})\Big)^2\\
    &\leq (\lambda^{(i)})^2+2\eta_2\lambda^{(i)} g(\Tilde{\boldsymbol{\theta}}^{(i+1)})+\Big(\eta_2||g(\Tilde{\boldsymbol{\theta}}^{(i+1)})||\Big)^2\\
\end{aligned}
\end{equation}
Rearrange the above equation, we have
\begin{equation}
\small    
\begin{aligned}
    -\lambda^{(i)} g(\Tilde{\boldsymbol{\theta}}^{(i+1)}) & \leq \frac{(\lambda^{(i)})^2-(\lambda^{(i+1)})^2+\Big(\eta_2||g(\Tilde{\boldsymbol{\theta}}^{(i+1)})||\Big)^2}{2\eta_2}.
\end{aligned}
\end{equation}
Taking $-\lambda^{(i)} g(\Tilde{\boldsymbol{\theta}}^{(i+1)})$ back to $\mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)})$, we have
\begin{equation}
\scriptsize    
\begin{aligned}
    \mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)})
    &\leq q^*+\frac{1}{m}\sum_{i=0}^{m-1} \frac{(\lambda^{(i)})^2-(\lambda^{(i+1)})^2+\Big(\eta_2||g(\Tilde{\boldsymbol{\theta}}^{(i+1)})||\Big)^2}{2\eta_2}\\
    &= q^* + \frac{1}{m}\sum_{i=0}^{m-1} \frac{(\lambda^{(i)})^2-(\lambda^{(i+1)})^2}{2\eta_2} + \frac{1}{m}\sum_{i=0}^{m-1}\frac{\Big(\eta_2||g(\Tilde{\boldsymbol{\theta}}^{(i+1)})||\Big)^2}{2\eta_2}\\
    &= q^* + \frac{(\lambda^{(0)})^2-(\lambda^{(m)})^2}{2m\eta_2} + \frac{\eta_2}{2m}\sum_{i=0}^{m-1}||g(\Tilde{\boldsymbol{\theta}}^{(i+1)})||^2\\
    &\leq f^* + \frac{(\lambda^{(0)})^2}{2m\eta_2} + \frac{\eta_2 L^2}{2}.
\end{aligned}
\end{equation}

3. By definition, $\forall \boldsymbol{\theta}\in\Theta$, we have
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}(\boldsymbol{\theta})+\lambda^*\cdot g(\boldsymbol{\theta})
    \geq \mathcal{L}(\boldsymbol{\theta}^*)+\lambda^*\cdot g(\boldsymbol{\theta}^*)
    = q(\lambda^*).
\end{aligned}
\end{equation}
Since $\Tilde{\boldsymbol{\theta}}\in\Theta$, $\forall m\geq 1$, we have
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)})
    &= \mathcal{L}(\Tilde{\boldsymbol{\theta}}^{(m)}) + \lambda^*\cdot g(\Tilde{\boldsymbol{\theta}}^{(m)}) - \lambda^*\cdot g(\Tilde{\boldsymbol{\theta}}^{(m)})\\
    &\geq q(\lambda^*) - \lambda^*\cdot g(\Tilde{\boldsymbol{\theta}}^{(m)})\\
    &\geq q(\lambda^*) - \lambda^*\cdot \big[g(\Tilde{\boldsymbol{\theta}}^{(m)})\big]_+\\
    &\geq f^*-\lambda^*\cdot \big\lVert \big[g(\Tilde{\boldsymbol{\theta}}^{(m)})\big]_+\big\rVert.
\end{aligned}
\end{equation}
\end{proof}

\begin{table*}[t]
\footnotesize
\rowcolors{3}{white}{gray!15}
    \centering
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule  \rowcolor{LightBlue} 
        &  & \multicolumn{4}{c}{\textbf{PE} / \textbf{NE} / \textbf{PNE} / \textbf{Belief}} & \textbf{Closed Set}\\
         \rowcolor{LightBlue} \multirow{-2}{*}{\textbf{Backbone}} & \multirow{-2}{*}{\textbf{Pre-train}}  & \textbf{Error} $\downarrow$ & \textbf{AUROC} $\uparrow$ & \textbf{AUPR} $\uparrow$ & \textbf{FPR at 95\% TPR} $\downarrow$ & \textbf{mAP} $\uparrow$ \\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(l){6-6} \cmidrule(l){7-7}
        ACAR, R-50~\cite{pan2020actorcontextactor} & K400 & 35.16 / 35.56 / 23.12 / 30.15 & 52.89 / 58.48 / 60.16 / 57.17 & 79.15 / 80.17 / 83.48 / 81.48 & 14.16 / 15.49 / 20.19 / 11.15 & 28.84\\
        % \hline
        ACAR, R-101~\cite{pan2020actorcontextactor} & K700 & 32.26 / \textbf{34.12} / 20.54 / 30.16 & 55.18 / 58.98 / 63.18 / 60.18 & 82.15 / 81.48 / 85.48 / 90.01 & 10.16 / 15.01 / 19.48 / 10.00 & \textbf{33.30}\\
        % \hline
        AIA, R-101~\cite{tang2020asynchronous} & K700 & 35.14 / 34.56 / 25.01 / 32.14 & 54.17 / 59.48 / 59.78 / 59.69 & 78.49 / 80.17 / 86.15 / \textbf{91.48} & 10.15 / 13.98 / 18.48 / 9.68 & 32.30\\
        % \hline
        Slowfast, R-101~\cite{DBLP:conf/iccv/Feichtenhofer0M19} & K600 & 60.12 / 59.12 / 23.15 / 40.15 & 50.15 / 52.23 / 69.14 / 68.15 & 70.15 / 75.15 / 69.68 / 68.21 & 20.17 / 19.56 / 23.12 / 26.15 & 29.00\\
        % \hline
        DEAR, R-50~\cite{bao-2021-ICCV}& K400 & 23.22 / 42.15 / 23.15 / 30.19 & 82.12 / 60.12 / 80.48 / 83.59 & 83.15 / 88.14 / 90.15 / 85.49 & 8.45 / 8.48 / 6.30 / 13.15 & 18.51\\
        % \hline
        AFAC, R-101~\cite{zhang2021multi}& K600 & 53.14 / 49.41 / 40.15 / 59.17 & 79.69 / \textbf{65.89} / 80.48 / 79.79 & 90.79 / 89.18 / 88.18 / 85.15 & 7.15 / 8.79 / \textbf{4.01} / \textbf{8.98} & 30.20\\ \rowcolor{LightGreen}
        \midrule
        \textbf{Ours}, R-50 & K400 & 11.22 / 40.14 / 12.18 / 27.91 & 86.92 / 63.54 / 85.25 / 83.41 & 96.18 / 89.90 / 93.46 / 90.90 & 4.98 / \textbf{5.09} / 4.51 / 9.15 & 27.80\\ \rowcolor{LightGreen}
        \textbf{Ours}, R-101 & K700 & \textbf{10.12} / 40.15 / \textbf{10.56} / \textbf{25.02} & \textbf{88.75} / 65.36 / \textbf{89.48} / \textbf{84.26} & \textbf{98.18} / \textbf{89.95} / \textbf{94.74} / 90.49 & \textbf{4.17} / 5.28 / 4.25 / 10.01 & 29.87\\
        \bottomrule
    \end{tabular}
    \caption{Comparison with state-of-the-art on AVA~\cite{gu-2018-cvpr-ava}. Ours is highlighted in \textcolor{Green}{green}. Best value is in \textbf{bold}.}
    \label{tab:baseline_AVA_full}
\end{table*}


\begin{table*}[t]
\footnotesize
\rowcolors{3}{white}{gray!15}
    \centering
    \setlength\tabcolsep{1pt}
    \begin{tabular}{l|c|c|c|c|c|c}
        \toprule  \rowcolor{LightBlue} 
        &  & \multicolumn{4}{c}{\textbf{PE} / \textbf{NE} / \textbf{PNE} / \textbf{Belief}} & \textbf{Closed Set}\\
         \rowcolor{LightBlue} \multirow{-2}{*}{\textbf{Backbone}} & \multirow{-2}{*}{\textbf{Pre-train}}  & \textbf{Error} $\downarrow$ & \textbf{AUROC} $\uparrow$ & \textbf{AUPR} $\uparrow$ & \textbf{FPR at 95\% TPR} $\downarrow$ & \textbf{mAP} $\uparrow$\\
        \cmidrule(r){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(l){6-6} \cmidrule(l){7-7}
        AFAC, R-101~\cite{zhang2021multi} & K600 & 7.00 / 26.62 / 8.88 / 19.17 & 82.20 / 69.62 / 82.15 / 87.00 & 90.15 / 85.12 / 90.15 / 89.15 & 35.19 / 29.55 / \textbf{23.15} / 31.15 & 48.10 \\
        % \hline
        AFAC, CSN-152~\cite{zhang2021multi} & IG-65M & 6.18 / 19.78 / 8.49 / \textbf{15.59} & 80.12 / 61.02 / \textbf{90.48} / 84.88 & 90.79 / 86.36 / 91.15 / 85.67 & 30.15 / 33.15 / 28.42 / 30.01 & \textbf{50.30}\\
        % \hline
        CSN, CSN-152~\cite{tran2019video} & IG-65M & 6.89 / 25.18 / 9.02 / 18.90 & 80.15 / 70.15 / 90.15 / 87.01 & \textbf{92.61} / 90.17 / 90.99 / 85.15 & 35.16 / 30.15 / 35.98 / 26.69 & 46.40\\
        % \hline
        Slowfast, R-101~\cite{DBLP:conf/iccv/Feichtenhofer0M19} & K600 & 25.15 / 30.15 / 46.12 / 23.00 & 79.12 / 75.12 / 75.36 / 78.89 & 79.15 / 80.15 / 80.46 / 80.08 & 50.15 / 56.15 / 46.12 / 29.16 & 45.20\\
        % \hline
        DEAR, R-50~\cite{bao-2021-ICCV}& K400 & 12.15 / 25.59 / 11.11 / 16.98 & 86.15 / \textbf{79.15} / 82.46 / 82.55 & 92.35 / 90.33 / 89.26 / 90.19 & 29.96 / 26.16 / 23.99 / 28.98 & 38.12\\
        % \hline
        X3D-XL~\cite{feichtenhofer2020x3d} & K600 & 8.45 / 25.85 / 10.15 / 15.51 & 82.15 / 66.64 / 85.00 / 82.15 & 86.49 / 80.16 / 90.15 / 88.15 & 39.98 / 36.14 / 26.15 / 30.55 & 47.20\\ \rowcolor{LightGreen}
        \midrule
        \textbf{Ours}, R-50 & K400 & \textbf{6.15} / 23.15 / 7.89 / 17.11 & 85.49 / 65.17 / 88.79 / \textbf{88.49} & 90.78 / \textbf{92.78} / 94.15 / \textbf{95.03} & 25.98 / 25.29 / 25.49 / \textbf{23.64} & 45.33\\ \rowcolor{LightGreen}
        \textbf{Ours}, R-101 & K700 & 6.23 / \textbf{22.15} / \textbf{7.02} / 20.15 & \textbf{88.49} / 65.01 / 89.41 / 82.16 & 91.15 / 90.78 / \textbf{95.42} / 90.15 & \textbf{25.15} / \textbf{21.16} / 26.46 / 28.98 & 47.21\\
        \bottomrule
    \end{tabular}
    %\vspace{-3mm}
    \caption{Comparison with state-of-the-art on Charades~\cite{sigurdsson2016hollywood_Charades}. Ours is highlighted in \textcolor{Green}{green}. Best value is in \textbf{bold}.}
    \label{tab:baseline_Charades_full}
\end{table*} 

\section{Proof Sketch of Proposition 3}
\label{sec:proof of prop3}
The Beta loss in Equation (2) is equivalent to the loss function used in DEAR when each actor is associated with one action only. The Evidential Neural Network (ENN), which was initially introduced in~\cite{sensoy-2018-nips} and further adopted by DEAR in open-set action recognition, is limited to classifying an actor associated with only one action. The key idea of ENN is to replace the output of a classification network with the parameters $\{\alpha_i\}_{i=1}^K$ of $K$ Dirichlet densities. To detect a novel actor with multiple actions, in this work, we modify ENN by estimating parameter pairs $\{\alpha_i,\beta_i\}_{i=1}^K$ of $K$ Beta distributions. \Cref{prop:prop3} shows that the Beta loss of an actor proposed in Equation (3) is equivalent to the loss function in DEAR when $K=1$.

\begin{proposition}
\label{prop:prop3}
Denote $\mathcal{L}'_j(\boldsymbol{\theta})$ as the loss function introduced in~\cite{sensoy-2018-nips}, \ie,
\begin{equation}
\small
\begin{aligned}
\label{eq:enn-loss}
    \mathcal{L}'_j(\boldsymbol{\theta}) = \sum_{i=1}^{K'} y_{ij} \Big(\psi(\sum_{i=1}^{K'} \alpha_{ij})-\phi(\alpha_{ij})  \Big),
\end{aligned}
\end{equation}
where $K'$ is the total number of classes in a multi-class classification task. Denote $\mathcal{L}_j(\boldsymbol{\theta})$ is the loss function proposed in Equation (3). We have $\mathcal{L}'_j(\boldsymbol{\theta})=\mathcal{L}_j(\boldsymbol{\theta})$ when $K=1$ (\ie, $K'=2$).
\end{proposition}
\begin{proof}
When $K=1$,
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}_j(\boldsymbol{\theta}) = \sum\nolimits_{i=1}^1 \int\Big[\textbf{BCE}(y_{ij},p_{ij})\Big]\textbf{Beta}(p_{ij};\alpha_{ij},\beta_{ij})dp_{ij}.
\end{aligned}
\end{equation}
To simplify, we omit the subscript $i$ and rewrite
\begin{equation}
\small    
\begin{aligned}
    \small
    &\mathcal{L}_j(\boldsymbol{\theta}) 
    =\int\Big[\textbf{BCE}(y_{j},p_{j})\Big]\textbf{Beta}(p_{j};\alpha_{j},\beta_{j})dp_{j}\\
    &=y_{j}\Big(\psi(\alpha_{j}+\beta_{j})-\psi(\alpha_{j})\Big)+(1-y_{j})\Big(\psi(\alpha_{j}+\beta_{j})-\psi(\beta_{j})\Big).
\end{aligned}
\end{equation}
As for $\mathcal{L}'_j(\boldsymbol{\theta})$, $K=1$ indicates binary classification which refers $K'=2$.
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}'_j(\boldsymbol{\theta}) 
    &=\int\Big[\textbf{CE}(y_{j},p_{j})\Big]\textbf{Dir}(p_{j};\alpha_{j},\beta_{j})dp_{j}\nonumber\\
    &=\sum_{i=1}^2 y_{ij}\Big(\psi(\sum_{i=1}^2 \alpha_{ij})-\psi(\alpha_{ij}) \Big).
\end{aligned}
\end{equation}
We complete the proof by setting $\alpha_{1j}=\alpha_j$ and $\alpha_{2j}=\beta_{j}$.
\end{proof}

\section{Detailed Ablation Study}
In Table \ref{tab:ablation-compinent_m345}, we provide a detailed ablation study on the AVA dataset~\cite{gu-2018-cvpr-ava} to explore the contributions of different components in our framework. Along with the primal-dual updating step $m=2,3,4,5$, the performance is slightly improved but nearly saturated when $m=2$. 

\section{Full Experimental Results}
\label{sec:additional results}
As presented in Table \ref{tab:baseline_AVA_full} and Table \ref{tab:baseline_Charades_full}, we report the results of compared methods by using all four novelty score estimation mechanism. It is worth mentioning that other state-of-the-arts perform also well by using the belief based score in terms of some metrics.

Furthermore, we show more visual results of compared methods in Figure \ref{fig:visualization-appendix-AVA} and Figure \ref{fig:visualization-appendix-Charades}. It can be seen that our method performs better than other methods on two datasets for single/multi-actor settings.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/appendix-AVA.pdf}
    % \vspace{-7mm}
    \caption{Visual comparison with our method and state-of-the-art on AVA~\cite{gu-2018-cvpr-ava}. \textcolor{Cyan}{Cyan} and \textcolor{Goldenrod!80!black}{yellow} boxes denote the predictions of actors with known and novel actions, respectively. \textcolor{Green}{\cmark ~marks} and \textcolor{Red}{\xmark~marks} indicate correct and false predictions, respectively.}
    \label{fig:visualization-appendix-AVA}
    % \vspace{-2mm}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/appendix-Charades.pdf}
    % \vspace{-7mm}
    \caption{Visual comparison with our method and state-of-the-art on Charades~\cite{sigurdsson2016hollywood_Charades}. \textcolor{Cyan}{Cyan} and \textcolor{Goldenrod!80!black}{yellow} boxes denote the predictions of actors with known and novel actions, respectively. \textcolor{Green}{\cmark ~marks} and \textcolor{Red}{\xmark~marks} indicate correct and false predictions, respectively.}
    \label{fig:visualization-appendix-Charades}
    % \vspace{-2mm}
\end{figure*}