\newpage

\section*{Revision Report}
\label{sec:Revision}
\noindent \emph{This section for the review process but not for publication.}\\

\noindent Dear TVCG EIC, AE, and reviewers,

This paper was first submitted to EuroVis 2023 (\#1073), and it was not accepted. As summarized by the primary reviewer, ``\emph{the main concern has been that the framework choices have not been justified nor evaluated.}''

While we appreciate that readers would prefer to read a proposed framework that has been comprehensively justified and evaluated, we recognize that it is hugely challenging for research of this nature to meet such an expectation. Some wonderful works in the short history of visualization would not have met such an expectation when they were first published. For example, there was limited justification for the four proposed criteria in Bertin's book \cite{Bertin:2011:book}. As an evaluation, the first paper that evidenced the usability of Munzner's nested model \cite{Munzner:2009:TVCG} was published four years later \cite{AbdulRahman:2013:CGF}.

Perhaps the best justification for Bertin's and Munzner's proposals was the trust that both authors had followed an adequate research process for proposing their frameworks. To help build such a trust, we provide an Appendix ``VII. The Process for Formulating the Framework'' to summarize our research process.

We anticipate the evaluation of such a framework will take decades to reach the level that would be considered as adequate. For example, the Appendix ``VIII. Typedness and Bertin's Criteria'' shows the amount and complexity of further research that would be required to justify and evaluate the proposed five-level assessment for the typedness criterion in Section \ref{sec:Typedness}.

At this stage, it is not possible for anyone to prove that the framework proposed in this paper is an optimal framework. Indeed, no one can offer such a proof for Bertin's criteria or Munzner's nested model, though they were widely accepted by the VIS community. While we truly believe that the proposed framework can and will be improved through continuing research effort, we believe that the best way for improving the framework is for the colleagues in the VIS community to propose improvements or alternative frameworks.

We hope that reviewers will appreciate the need to propose such a framework with a reasonable amount of scrutiny, while leaving the comprehensive justification and evaluation to future research.

\hfill Yours sincerely,

\hfill Hong-Po Hsieh, Amy Zavatsky, and Min Chen

\subsection{Answers to Specific Queries in EuroVis 2023 Reviews}

\noindent \textbf{Abbreviation:} Review (i), Weakness (j) $\longrightarrow$ Riwj\\ 

\noindent \textcolor{orange}{Q1. R1w1,R2w2: channel-based vs. holistic assessment.}\\

\noindent A1. We have the same view as R1 that both types of assessment are needed. However, it is also necessary to balance the workload. Too much variable-based assessment (TypeA) might demand too much effort for scoring individual variable. Too much holistic assessment (TypeD) might demand too much effort for manual aggregation while overlooking some details. We recommend to assess criteria 4-12 with TypeD directly and criteria 1-3 with a two-level approach. Our recommendation is not arbitrarily decided. As discussed in Section \ref{sec:Overview} and illustrated in Fig. \ref{fig:Hierarchy}, With TypeB (or TypeC) assessment, one would need to score many variable-variable relations (or variable-others relations). The effort for scoring is significant higher. Criteria 6-12 are such cases. For criterion 4, we already reasoned our recommendation in the last paragraph of Section \ref{sec:Geometry}. For criterion 5, we added an extra paragraph at the end of Section \ref{sec:Colorimetry}. In the future, the VIS community may improve the recommendation based on the accumulated practical experience.\\ 

\noindent \textcolor{orange}{Q2. R1w2,R2w1,R3w1,R3w2: motivation for individual components.}

\noindent A2. These components are based on the criteria listed explicitly or discussed (though not listed) in \cite{Bertin:2011:book,Maguire:2012:TVCG,Borgo:2013:STAR,Chung:2016:CGF}. 
We considered many different ways of the combining the existing criteria. We have now provided Appendix ``VII. The Process for Formulating the Framework'' to show the reasoning behind the proposed scheme. As discussed in the our cover letter, some major works in VIS were proposed in a similar manner. This MCDA scheme is built on the criteria proposed in \cite{Bertin:2011:book,Maguire:2012:TVCG,Borgo:2013:STAR,Chung:2016:CGF}, and it is a step forward.\\

\noindent \textcolor{orange}{Q3. R1w3: orthogonal criteria.}

\noindent A3. We found that some conceptual overlapping is unavoidable, largely due to the broad interpretation of many words and the interrelated cognitive processes underpinning these concepts. It would be great if future research could propose a set of orthogonal criteria, but it might not be possible or could take decades. One may compare this desire to setting examination papers. It is usually not trivial to determine what is the optimal number of questions in an examination paper and whether the questions are orthogonal enough.
Please see Appendix ``VII. The Process for Formulating the Framework'' for a more detailed answer.\\

\noindent \textcolor{orange}{Q4. R1w4: reducing the number of criteria.}

\noindent A4. The authors welcome any proposal for reducing or increasing the number of criteria. We have not found a better scheme with fewer criteria. When we tried to reduce the number of criteria by combining criteria, we found that this would make each combined criterion difficult to assess.\\

\noindent \textcolor{orange}{Q5. R2w3: weights.}

\noindent A5. The weights are our recommendations and it is the typical approach in MCDA research. The currently recommended weights are relatively simple to follow, i.e., weight 1 for the three TypeA criteria and weight 0.5 for the nine TypeD criteria. The authors have experimented with different sets of weights, and established that minor variations of these weights usually do not affect the overall ranking order of the designs being assessed. 
Future research may propose alternative values for weights. The authors welcome any proposal for using a different set of weights.\\

\noindent \textcolor{orange}{Q6. R2w5: no science (yet) on how complex glyphs are read.}

\noindent A6. Limited scientific understanding should not stop us design glyphs or evaluate glyphs. Doing both is part of scientific development.\\

\noindent \textcolor{orange}{Q7. R2w6: task- and application-dependency.}

\noindent A7. The scheme is formulated as a general purpose scheme for evaluating glyphs. The VIS practitioners can introduce their knowledge about application-specific tasks in the design process. Similarly, Bertin's four criteria is a general purpose scheme. Creating task-dependent schemes for individual applications would not be an answer.\\ 

\noindent \textcolor{orange}{Q8. R2w7: five-level assessment.}

\noindent A8. Comparing with previous schemes such as Bertin's, we define scores in a more detailed and precise manner. We believe that this is a step forward.\\

\noindent \textcolor{orange}{Q9. R3w3: case study.}

\noindent  A9. We have added a second case study in a biomechanical application to demonstrate that the scheme allows a designer to evaluate different options in a design process and helps identify better designs.\\

\noindent \textcolor{orange}{Q10. R3w4: more discussions.}

\noindent A10. We have added two appendices, i.e., ``VII. The Process for Formulating the Framework'' and ``VIII. Typedness and Bertin's Criteria'', for the suggested discussions.\\

\noindent \textcolor{orange}{Q11. R4w1: independent evaluation.}

\noindent A11. Independent evaluation of the scheme may be done by the VIS community through its deployment, similar to the situation when well-known methodologies were proposed with limited evaluation (see also the main revision report on page \pageref{sec:Revision}).

\subsection{EuroVis 2023 Reviews}

\small
\begin{Verbatim}[breaklines]

coordinator review (reviewer 4)
score 3.5/5

  Summary Review Text

    The paper presents a (novel) framework for glyph design and evaluation.
    The paper received scores of 2, 2, 2, and 3.5. However, the reviewer with the
    highest score brings forward some of the same concerns as the other reviewers. All
    the reviewers have been rather articulate about the issues. So, I won't be able to
    do them justice by repeating them here. However, the main concern has been that
    the framework choices have not been justified nor evaluated. Hence, the reader is
    left wondering whether this is a helpful framework. Still, the idea is novel, and
    we hope the authors find the comments helpful in improving their work.

    Thank you for the rebuttal letter. However, it didn't change the opinion of the
    reviewers.

  I certify I have read the Rebuttal (if submitted), or that no rebuttal has been submitted

    I certify

  Paper Type

    Algorithm / Technique

  References

    At large, all important references are included

  Clarity / Presentation Quality

    Fair: the message is not quite clear, there are some structural changes or some
    wordsmithing needed

  Rigor and Soundness

    Fair: A few significant concerns regarding the approach

  Reproducibility

    Many issues discussed, but some important details left out

  Originality, Novelty

    Strong: a strong contribution, clearly fill a gap in the literature

  Significance, Utility, Importance (relevance in general)

    Possibly useful

  Overall Rating

    Borderline on the accept side (3.5/5): While this paper is on the borderline, I
    would vote to accept if forced to choose.

  Expertise

    Passing knowledge (fairly confident that the evaluation is correct)

  Paper Short Description (one sentence)

    The paper conceptualises glyph design and provides a way to rate the overall
    quality of a glyph.

  Paper Strengths

    S1: the major strength of the paper is a comprehensive framework for designing and
    evaluating a glyph.

    S2: (the attempt) Highlighting the framework through several examples.

    S3: a comprehensive literature review

  Paper Major Weaknesses

    W1: There is no validation/evaluation of the framework. I would like to cite two
    examples:
      * The weights seem arbitrary: only 1 and 0.5 are suggested. No discussion is
    spent on what if they were different --> there is no intuition for their current
    weight.
      * the scoring for each of the 12 categories is not questioned
      * Type B/C rankings are never explored; there is no intuition of how they will
    impact glyph design
      * Overall: it is great that there are examples of alternate designs of glyphs
    arising from previous work. However, are they "better," "worse," "good," or "bad"?
    There is no evaluation of these designs. [the examples are great to demonstrate
    how the framework works, though!]

    W2: Some criteria (and/or their scoring) remain unclear. Specifically:
      * Discernability: Why do I "need to differentiate n = k(k - 1)/2 pairs of
    values" instead of just k values?!
      * Intuitiveness: the scoring is unclear; perhaps use a table to clarify
      * Invariance (Geometry): it remains unclear what 'variance at scale X' means.
      * Invariance (Colorimetry): the formula needs to be clarified, there is no
    intuition behind it, and besides stating that it is commonly used, there is no
    reference.
      * Composition: Separability: unclear how to compute s_int
      * Attention: Balance + Searchability: it needs to be clarified how to figure out
    which variable has weak attention (is searchable); even the authors refer to
    future work on this topic.


    W3: there are too many grammar issues in the paper, making the paper hard to read
    at times.

  Paper Minor Issues

    M1: most of the references are inconsistent in terms of capitalization +
    information provided

  Overall Assessment

    While I enjoyed and valued the overall framework, there are too many aspects that
    are not clear: from details of the single criteria to the overall evaluation of
    the framework to the impact of some of the parameters suggested (scores and
    weights). Hence, as a reader, I am not convinced I would want to use this
    framework, nor can I recommend it to anyone. Perhaps it is a great starting point
    for a comprehensive framework; however, the onus is on the authors to convince me
    that it is a robust and reliable framework.

--------------------------------------------

committee member review (reviewer 3)
score 2/5

  I certify I have read the Rebuttal (if submitted), or that no rebuttal has been submitted

    I certify

  Paper Type

    Theory / Model

  References

    Major areas of previous work ignored

  Clarity / Presentation Quality

    Fair: the message is not quite clear, there are some structural changes or some
    wordsmithing needed

  Rigor and Soundness

    Fair: A few significant concerns regarding the approach

  Reproducibility

    Work cannot be replicated because too many critical aspects remain unclear

  Originality, Novelty

    Reasonable: some valuable contribution

  Significance, Utility, Importance (relevance in general)

    Possibly useful

  Overall Rating

    Probably reject (2/5): I would argue for rejecting this paper.

  Expertise

    Knowledgeable (confident but not absolutely certain)

  Paper Short Description (one sentence)

    The paper proposes a Multiple-Criteria Design Analysis (MCDA) rating scheme to
    help the design glyph-based data visualization.

  Paper Strengths

    S1: The paper aims to improve the design of glyphs. Glyphs are often used to
    communicate medical data. Therefore, the societal impact of enhancing
    communication and potentially decision-making in this application domain is
    meaningful.

    S2: The manuscript states that the MCDA process is widely used and well-
    established. This foundational work lends credibility to the approach.

    S3: Although glyphs can theoretically communicate high-dimensional data, they
    primarily appear in pedagogical contexts. Improving the design process could mean
    more widespread use, expanding the toolkit of methods for presenting high-
    dimensional data.

  Paper Major Weaknesses

    W1: The paper proposes using the Multiple-Criteria Design Analysis (MCDA)
    methodology but needs to provide meaningful background information about usage
    scenarios, strengths, and limitations. This lack of background information makes
    it challenging for readers like myself, who are unfamiliar with the methodology,
    to grasp the total contribution and impact of the work without referencing
    external sources. A paper should stand on its own. Section 3 briefly describes
    MCDA as a tree-based scoring system. However, the manuscript does not explain why
    MCDA is superior to other existing glyph evaluation techniques discussed in
    Section 2 (e.g., normative and subjective ratings).


    W2: The paper's primary contribution appears in Section 4, where it proposes an
    MCDA scheme for glyph design. However, the manuscript needs to justify why there
    are 12 criteria and the methods for selecting them. Overall, there needs to be
    more transparency in the methodology for this work.

    W3: Although the paper presents case studies that apply the MCDA scheme to rate
    five different glyphs, it doesn’t address how to utilize the methodology in the
    design process, which is the paper's goal. The case studies suggest that MCDA may
    be more appropriate for evaluation scenarios.

    W4: The discussion section is short (1 paragraph) and does not discuss the paper’s
    contributions. Given W1, W2, and W3, a throughout discussion would have been
    immensely useful for contextualizing the work.

  Paper Minor Issues

    M1: The problem statement could be clearer. The paper states: "This work focuses
    on one particular challenge, that is, there are many desirable properties of glyph
    designs, and likely a good design does not necessarily meet all criteria as one
    might desire, but embodies a relatively optimized set of trade-offs among the
    visual representations of different variables."

  Overall Assessment

    Overall, this project could be of interest to the visualization community—however,
    the paper lask critical information, making it challenging to understand. First,
    the paper does not adequately describe the MCDA scheme. Second, it lacks
    information about the methods. Third, the case studies do not demonstrate the
    usefulness of the proposed solution, nor does it compare it to existing methods
    mentioned in the related work section. Finally, the discussion section does not
    contextualize the contributions. Taken together, this paper is not yet ready for
    publication.

    Post-rebuttal comments:
    Thanks to the authors for submitting the rebuttal. Although it details some
    changes that would improve the paper's clarity, I believe the work needed is
    beyond the scope of the review cycle. Thus, I cannot advocate for this paper's
    acceptance into EuroVis 2023.

--------------------------------------------

reviewer review (reviewer 1)
score 2/5

  I certify I have read the Rebuttal (if submitted), or that no rebuttal has been submitted

    I certify

  Paper Type

    Theory / Model

  References

    At large, all important references are included

  Clarity / Presentation Quality

    Poor: major structural changes or extensive wordsmithing needed

  Rigor and Soundness

    Fair: A few significant concerns regarding the approach

  Reproducibility

    Many issues discussed, but some important details left out

  Originality, Novelty

    Reasonable: some valuable contribution

  Significance, Utility, Importance (relevance in general)

    Case for utility not compelling

  Overall Rating

    Probably reject (2/5): I would argue for rejecting this paper.

  Expertise

    Knowledgeable (confident but not absolutely certain)

  Paper Short Description (one sentence)

    This paper presents a technique for evaluating alternative glyph designs. The
    proposed method is based on multi-criteria decision analysis (MCDA). It covers
    several aspects such as discriminability, intuitiveness, and attention. Each of
    the 12 criteria is applied to individual channels within the glyph and then
    combined hierarchically into one score to capture glyph effectiveness.

  Paper Strengths

    S1: The paper overall is well-motivated; the idea of scoring alternative glyph
    designs based on different criteria is interesting and has merit. A glyph designer
    typically faces an expansive design space with many potential alternatives, so
    techniques such as the one presented here would be a good contribution.

    S2: A strength of the proposed technique is that it is a form of heuristic
    evaluation, which doesn't require a user study. This of course has advantages,
    allowing a designer/practitioner to quickly evaluate a large set of possible
    designs. The MCDA method could in the future be extended to other types of
    visualizations (e.g., dashboards).

  Paper Major Weaknesses

    W1: The MCDA approach treats the glyph as a sum of its parts. The assumption here
    is that if each channel is perceived separably with sufficient
    discriminability/intuitiveness/..., then the glyph design as a whole must be
    effective. Yet, glyphs are also meant to be perceived and compared holistically.
    For example, star glyphs are designed to convey a holistic multi-dimensional shape
    (or signature) from the arrangement of its individual values. It is those shapes
    that are compared on whole. Patterns in how those shapes change (e.g., over
    geography) can be useful feature in a visualization. Yet, the current approach
    doesn't cover this (I think important) use case.

    W2: While the overall approach has merit, the individual components are not always
    well-motivated. It's unclear how/why the authors converged on the 12 components
    discussed in the paper. What was the method used here to develop this final list?
    Why those specific 12 per se?

    W3: At times, there is overlap (at least conceptually) between the different
    criteria. For example, 4.8 and 4.9 deal with different aspects of attention
    (namely, importance vs. balance). The definition of those items, however, seems
    redundant rather than complementary (e.g., attributes being overlooked vs.
    allocating pre-attentive channels to "meaningful" attributes).

    I understand the desire for comprehensiveness, but I suspect that the current list
    is somewhat bloated and could use more refinement.

    W4: The presentation style is a little unengaging. The bulk of the paper reads
    like a laundry list of criteria, which are discussed in the abstract. Only when we
    get to the last page of the paper do we get to see an application to evaluate
    glyphs. The paper read more like a comprehensive technical report and less like a
    research paper that is presenting a new contribution.

    I'd recommend reducing the size of MCDA criteria (perhaps to no more than 7
    dimensions?) and allocating much more space to examples. The latter should also be
    more detailed, with a more concrete explanation of how scores on each dimension
    were calculated (currently there is only a table with little justification for
    scores).

  Paper Minor Issues

    --

  Overall Assessment

    The paper presents an interesting technique for evaluating glyphs. The approach
    could be valuable and potentially even extendible to other visualization types.
    However, while the method on whole is well-motivated, the components of the MCDA
    criteria are not fully justified. The methodology behind the development of those
    components needs better description. The paper could also be rewritten to reduce
    redundancy. More detailed applications and examples of the technique would serve
    to better illustrate the contribution.


    Post rebuttal comments: Thanks to the authors for submitting a rebuttal. In my
    opinion the paper needs a significant re-write / re-framing before it can be
    recommended for acceptance. So I'm hesitant to change my score based on the
    rebuttal alone.

    I will add that, in my opinion, having a (quite) long list of criteria is not the
    best way to showcase this work. While I understand the desire to be comprehensive,
    I urge the authors to refine their initial criteria, remove potentially redundant
    items, or perhaps focus on the most important aspects. I believe this would make
    the paper both more engaging and focus its contribution.

--------------------------------------------

reviewer review (reviewer 2)
score 2/5

  I certify I have read the Rebuttal (if submitted), or that no rebuttal has been submitted

    I certify

  Paper Type

    Theory / Model

  References

    At large, all important references are included

  Clarity / Presentation Quality

    Good: the message is reasonably clear, and there are only minor typos and grammar
    problems

  Rigor and Soundness

    Poor: Fundamentally unsound

  Reproducibility

    Work cannot be replicated because too many critical aspects remain unclear

  Originality, Novelty

    Minor: only incremental improvements over previous work

  Significance, Utility, Importance (relevance in general)

    Possibly useful

  Overall Rating

    Probably reject (2/5): I would argue for rejecting this paper.

  Expertise

    Knowledgeable (confident but not absolutely certain)

  Paper Short Description (one sentence)

    This paper describes a scheme for evaluating and quantifying the quality of
    complex multi-variate glyph designs.

  Paper Strengths

    S1: To my knowledge, there is little or no existing work on how to design complex
    glyphs. There are some annectdotes and examples, but no evaluative frameworks that
    can serve in design iteration.

    S2: Provides a very extensive list of factors to consider when designing glyhphs.

    S3: Points out many holes in the literature... it is remarkable how much of the
    paper either explicitly acknowledges a lack of fundamentals to make a decision or
    quantification, or implicitly suggests it by making an arbitrary choice because
    nothing is known.

  Paper Major Weaknesses

    W1: While the list of factors is extensive, it feels arbitrary. Are these 12
    necessary, sufficient, orthogonal, etc. There is no discussion of where these come
    from or why.

    W2: The design to group things in a two level hierarcy feels arbitrary and
    provides no rationale. Why this way? (other than some decision theory suggests it)

    W3: The weightings and combinations seem arbitrary.

    W4: There is no evidence to support that this is good or useful for design or
    evaluation. The only "evidence" provided are some simple examples that show that
    very bad choices fall under one or more of the problem categories.

    W5: To my knowledge, there is no science (yet) on how complex glyphs are read. The
    realted work doesn't help. It seems to me that it is a complex process that is
    very cognitive (like reading text) rather than pre-attentive (like viewing a
    chart). The impact of this on design is not considered, as it means that the
    comprehensibility is complex.

    W6: The paper really doesn't consider task. Task certainly matters - designs need
    to support tasks. The connection between tasks and design choices and criteria is
    missing.

    W7: Most (if not all) of the scoring for the criteria seem arbitrary. If there is
    a rationale for the scores, the paper doesn't give it.

    W8: Some of the criteria require either a depth of knowledge that I do not believe
    exists (e.g., on cue interaction and distraction), experiments that seem
    impossible to design (e.g., to assess learnability), or simplifications that seem
    too extreme (e.g. difference in RGB values without considering any perceptual
    factors)

    W9: It really isn't clear how common such complex designs are, and how much
    designers are calling for formalism to help design them.

  Paper Minor Issues

    M1: The intro "expects" the paper to be useful - this is a paper, it should
    actually provide value.

    M2: THe intro/related work doesn't get at the downsides of glyphs: they get very
    complicated very quickly, and are really not that popular in practice. It is hard
    to say that better design methods can address these issues (hard to learn, hard to
    read, ...)

    M3: The paper really only defines glyph in Section 3, and even then, I am not sure
    what the scope is. The examples later in the paper help, but I wonder... is this
    all the complex "icons with badges?"

  Overall Assessment

    While I think this paper is trying to do a good thing (make a list of factors to
    consider in glyph design), the work is lacking in the current form. It lacks rigor
    in the choices (everything feels arbitrary), except where over-simplified
    equations and formalism are inserted to places where qualitative judgements seem
    more appropraite (e.g., formulas and numeric scoring for judgments, arbitrary
    combinations and weights, ...)

--------------------------------------------

\end{Verbatim}
\normalsize
