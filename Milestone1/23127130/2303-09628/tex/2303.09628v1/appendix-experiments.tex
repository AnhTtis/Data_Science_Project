\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\theequation}{S\arabic{equation}}
\section*{Appendix} \label{sec:appendix}
\subsection{Environment}
\label{app:env}

\subsubsection{Training environment}

The environment is built on \textit{Pybullet} physics simulation software \cite{coumans2021} and it is shown in Figure \ref{fig:pybullet-env}.
The states $\in \mathbb{R}^{11}$ include the 3D robot's end-effector position, a binary variable representing the gripper state (open/close), the 3D position of the block and the 1D joint position for each of the 3 drawers and the door. The goal state  $\in \mathcal{R}^3$ is the desired block position (e.g. behind the door, inside the mid-drawer or somewhere on the table).
The action space is discrete and consists of 10 object-centric motion primitives, namely reaching every object, manipulating the objects (e.g. sliding or pulling), and opening and closing the gripper. Given the primitives we use are relational to objects they become implicitly parameterized by the corresponding object pose. We also include a \textit{center} action to move the robot back to the center of the desk to have greater reachability. Finally, we also include a \textit{go-to-goal} primitive that moves the end-effector over the goal position. A complete action list is shown below:
\begin{multicols}{2}
\begin{enumerate}
    \item Go to door handle
    \item Go to drawer1 handle
    \item Go to drawer2 handle
    \item Go to drawer3 handle
    \item Go to center
    \item Go to block
    \item Go to goal
    \item Grasp/release (Open/close the gripper)
    \item Pull/push
    \item Slide left/slide right
\end{enumerate}
\end{multicols}

\begin{wrapfigure}{r}{45mm}
\centering
\includegraphics[width=\linewidth]{figures-appendix/fig1_pybullet.png}
\caption{Training desk environment with different objects that can be manipulated. The robot end-effector is represented with a yellow sphere.}
\label{fig:pybullet-env}
\end{wrapfigure}

In the case of multiple primitives for one action (i.e. actions 8, 9 and 10), the executed primitive depends on the current state (e.g. the gripper opens if its current state is closed and viceversa).

If the agent attempts an infeasible action, such as manipulating an object with the wrong primitive (e.g. pulling a sliding door) or moving in a non collision-free path, the environment does not perform a simulation step and the state remains unaltered.  To check for collisions, we use the \code{rayTest} method in \textit{Pybullet}, which performs a single raycast.

We use the sparse reward signal: $R(\cdot|s, g) = \mathbbm{1}_{\{|f(s)-g|_1 < \epsilon\}}$, where $f(s)$ extracts the current block position from the state and $\epsilon=0.1$ is a threshold determining task success.


\subsection{Extended Experimental Results} 
\label{app:experiments}
\subsubsection{HER}


We experiment with combining our method with off-policy goal relabeling \cite{andrychowicz2017hindsight}.
We evaluate different relabeling ratios $k$. As already mentioned in the Results, we observe that the greater the relabeling ratio, the slower the convergence to the optimal policy (see Figure \ref{fig:her-results}). We hypothesize that this is because the environment dynamics are not smooth and the policy fails to generalize to distant goals despite the relabeling, which may hurt performance.


\begin{figure}
\begin{center}%[!htb]
\vspace{2mm}

\minipage{\linewidth}
\small
\centering
\textcolor{elfp}{\rule[2pt]{15pt}{3pt}} \textrm{ELF-P}\quad
\textcolor{her}{\rule[2pt]{15pt}{3pt}} \textrm{ELF-P+HER(K=2)}\quad
\textcolor{ddqn}{\rule[2pt]{15pt}{3pt}} \textrm{ELF-P+HER(K=4)}
\endminipage

% \vspace{-1mm}
\end{center}
\minipage{\linewidth}
\includegraphics[width=0.49\textwidth]{figures-appendix/ours_her_medium.pdf}
\label{fig:her-effect-medium}
% \hspace{0.01\linewidth}
\includegraphics[width=0.49\textwidth]{figures-appendix/ours_her_hard.pdf}
\label{fig:her-effect-hard}
\endminipage
\vspace{-5mm}
\caption{Effect of relabeling ratio on success rate for the Medium (left) and Hard (right) task variants. Results are averaged across 10 independent random seeds, shaded area represents standard deviation. (Play dataset size $10^5$). }
% \vspace{-5mm}
\label{fig:her-results}
\vspace{2mm}
\end{figure}


\subsubsection{Sample complexity}
\label{app:sample-complexity}
Results reported in subsection \textit{Sample Complexity} in Section \ref{sec:Experiments} are obtained by running 5 independent random seeds for each value of $\rho$ in the Medium task.
We report results obtained for the Hard task in Figure \ref{fig:sample-complxity-hard}, which shows an increase of sample complexity as the number of feasible actions increases.
However, in this hard setting, several seeds for several values of $\rho$ don't reach a success rate of 0.95. This is expected given that for example $\rho=0$ recovers DDQN behavior which was shown to fail in learning the Hard task. Consequently, in order not to bias the results, we opt for only reporting the results on $\rho$ values whose seeds are always successful in learning. 


\begin{wrapfigure}{r}{45mm}
\centering
\includegraphics[width=\linewidth]{figures_icra/boxplot_hard_True.pdf}
\caption{Effect of $\rho$ on sample complexity for the Hard task. Means across seeds are connected by lines.}%.Each point represents a seed means are connected by lines.}
\label{fig:sample-complxity-hard}
\end{wrapfigure}

\subsubsection{Robustness to play dataset size}
\label{app:robustness-datasize}
We study the effect of play dataset size on training performance for both \alg and DDQN+Prefill (which affects the amount of data used for training the prior for \alg and the amount of data used to prefill the replay buffer for DDQN+Prefill). In Figure \ref{fig:trainig-curves-datasize-effect} we show the resulting training curves using 3 different dataset sizes for \alg and DDQN+Prefill.


\begin{figure*}[!htb]
\begin{center}%[!htb]

\minipage{\textwidth}
\small
\centering
\textcolor{elfp}{\rule[2pt]{20pt}{3pt}} \textrm{ELF-P} \quad
\
\textcolor{prefill}{\rule[2pt]{20pt}{3pt}} \textrm{DDQN+Prefill} \quad

\endminipage
\vspace{-1mm}
\end{center}
\minipage{\linewidth}
\includegraphics[width=0.24\textwidth]{figures-appendix/medium_datasize_effect.pdf}
\label{fig:medium_datasize_success}
\includegraphics[width=0.24\textwidth]{figures-appendix/medium_inv_action_datasize_effect.pdf}
\label{fig:medium_datasize_inv}
\includegraphics[width=0.24\textwidth]{figures-appendix/hard_datasize_effect.pdf}
\label{fig:hard_datasize_success}
\includegraphics[width=0.24\textwidth]{figures-appendix/hard_inv_action_datasize_effect.pdf}
\label{fig:hard_datasize_inv}
\endminipage
\vspace{-5mm}
\caption{Success rate and number of infeasible actions attempts for the Medium (top) and Hard
(bottom) task variants for 3 different dataset sizes: from darker to lighter $10^3, 10^4 \text{ and } 10^5$ datapoints respectively. When using $10^3$ datapoints DDQN+Prefill fails completely in solving the Hard task.(Results are averaged
across 10 independent random seeds, shaded area represents standard deviation).}
\vspace{1mm}
\label{fig:trainig-curves-datasize-effect}
\end{figure*}


\subsubsection{Soft prior integration}
\label{app:soft-elfp}
We implement  Soft-\alg, an ablation for our method which softens the integration of the prior into learning. This modification could potentially allow Soft-\alg to recover from degenerated priors, i.e., when play data has significant distribution mismatch with the target tasks of interest.
Soft-\alg samples from the prior (instead of sampling from the feasible set $\alpha$) both during initial exploration phase and while performing $\epsilon$-greedy exploration. Additionally, during exploitation, it multiplies the softmax of Q-values by the prior, thus biasing the greedy action selection towards the prior.
Finally, given the soft integration of the prior, it performs Q-learning over the set of all actions, instead of over the reduced set of feasible actions.
In Figure \ref{fig:soft_vs_hard_medium} we compare the performance of \alg with Soft-\alg. We observe that while Soft-\alg is able to learn both medium and hard tasks, it has lower sample efficiency than \alg. One of the main reasons of \alg being faster is because a hard prior integration alleviates the Q-network from learning values for all state-action pairs: it can focus on learning Q-values for feasible state-action pairs only and ignore Q-values for unfeasible actions. This shows one of the main contributions of our algorithm.
Nevertheless, a soft integration could be useful when dealing with degenerated priors and we reserve further exploration on the topic for future work.


\begin{figure}
\begin{center}%[!htb]
\vspace{2mm}

\minipage{\linewidth}
\small
\centering
\textcolor{elfp}{\rule[2pt]{20pt}{3pt}} \textrm{ELF-P} \quad
\textcolor{soft-elfp}{\rule[2pt]{20pt}{3pt}} \textrm{SOFT ELF-P}
\endminipage

\vspace{-1mm}
\end{center}
\minipage{\linewidth}
\includegraphics[width=0.49\textwidth]{figures-appendix/medium_with_soft_datapoints10000.pdf}
\label{fig:medium_soft_success}
% \hspace{0.01\linewidth}
\includegraphics[width=0.49\textwidth]{figures-appendix/medium_inv_action_with_soft_datapoints10000.pdf}
\label{fig:medium_soft_inv}
\endminipage
\vspace{-5mm}
\caption{Comparison between \alg and Soft-\alg. \alg. Success rate and number of infeasible actions attempts for the Hard  task variant. (Play dataset size $10^4$). Results are averaged across 10 independent
random seeds, shaded area represents standard deviation) }
\label{fig:soft_vs_hard_medium}
\vspace{4mm}
\end{figure}


\subsection{Additional experimental details}
\label{app:additional_exp_details}

\subsubsection{Evaluation protocol} 
All reported results are obtained by evaluating the policies over 50 episodes every 2500 environment steps. Results are averaged over 10 random independent seeds unless explicitly stated. Shaded areas represent standard deviation across seeds.

\subsubsection{\textit{Play}-dataset collection}
Given that our method can learn with very little data ($\sim$1h30min of data collection), play data could in practice be collected by a few human operators choosing from a predefined set of primitives.
However for simplicity, we resort to evaluating whether a termination condition for each primitive is met in every configuration. This is a common approach under the options framework \cite{sutton1999between}. 
The tuples $(s,a)$, containing the state of the environment and the feasible action performed in the state respectively, are stored in the \textit{play}-dataset $\mathcal{D}$ for training the prior.

\subsubsection{Hyperparameters and Architectures}
\label{app:baselines}
Across all baselines and experiments, unless explicitly stated, we use the same set of hyperparameters and neural network architectures Values are reported in Table \ref{table:hyperparams-new}. We choose a set of values that was found to perform well in previous works. We report the list of hyperparameters that were tuned for each method in the following subsections. 

\input{parameters-new}
\input{parameters-prior}
\paragraph{DDQN}
      \begin{itemize}
        \item Discount $\gamma$: $0.97$. Tuned over $[0.90, 0.95, 0.97, 0.99]$. 
      \end{itemize}

\paragraph{DDQN+HER}
      \begin{itemize}
        \item Relabeling ratio $k$: $4$. As suggested in the original paper \cite{andrychowicz2017hindsight}.
        \item Discount $\gamma$: $0.97$. Tuned over $[0.90, 0.95, 0.97, 0.99]$. 
      \end{itemize}

\paragraph{DDQN+Prefill}
\begin{itemize}
    \item Discount $\gamma$: $0.95$. Tuned over $[0.90, 0.95, 0.97, 0.99]$. 
     \item Prefill dataset: 
 The dataset used to prefill the replay buffer is the same as the \textit{Play}-dataset used to train the behavioral prior, but extending the tuples $(s,a) \to (s,a,s',g,r)$ to be able to perform TD-loss on them. 
Given that the \textit{Play}-dataset is task-agnostic, we decide to compute the rewards with relabelling, i.e., to relabel the goal $g$ to the achieved goal and to set the reward $r$ to $1$, otherwise set the reward to zero. We experiment with different relabeling frequencies $f \in [0.0, 0.25, 0.5, 0.75, 1.0]$.
We find that $f>0$ leads to overestimation of the Q-values at early stages of training and thus hurt performance. For this reason we use $f=0$, i.e., no relabelling.
 \end{itemize} 
 
\paragraph{DQfD}
\begin{itemize}
    \item Discount $\gamma$: $0.95$. Tuned over $[0.90, 0.95, 0.97, 0.99]$.
    \item Large margin classification loss weight $\lambda_2$: $1e^{-3}$. Tuned over $[1e^{-2}, 1e^{-3}, 1e^{-4}, 1e^{-5}]$.
    \item Expert margin: $0.05$. Tuned over $[0.01, 0.05, 0.1, 0.5, 0.8]$.
    \item L2 regularization loss weight $\lambda_3$: $1e^{-5}$.
    \item Prefill dataset: Same approach as for DDQN+Prefill explained above.
\end{itemize} 

\paragraph{SOFT ELF-P}
\begin{itemize}
    \item Discount $\gamma$: $0.97$. Tuned over $[0.90, 0.95, 0.97, 0.99]$.
\end{itemize} 

\paragraph{SPiRL}
\begin{itemize}
    \item Discount $\gamma$: $0.97$. Tuned over $[0.90, 0.95, 0.97, 0.99]$.
    \item KL weight $\alpha$: 0.01. Tuned over $[0.005, 0.01, 0.05]$.
    \item Actor-network architecture: MLP [128, 256]
\end{itemize} 

    
\paragraph{\alg}
      \begin{itemize}
        \item Discount $\gamma$: $0.97$. Tuned over $[0.90, 0.95, 0.97, 0.99]$. 
        \item Prior training: we use the parameters reported in Table \ref{table:hyperparams-prior}.
      \end{itemize}


\subsection{Hardware experiments}
\label{app:hardware_exp}
The experiments are carried out on a real desktop setting depicted in Figure \ref{fig:method} (upper right).
We use \textit{Boston Dynamics} Spot robot \cite{noauthor_spot_nodate} for all our experiments.
The high-level planner, trained in simulation, is used at inference time to predict the required sequence of motion primitives to achieve a desired goal. The motion primitives are executed using established motion planning methods \cite{zimmermann2022dca} to create optimal control trajectories. 
We then send desired commands to Spot using \textit{Boston Dynamics Spot SDK}, a Python API provided by Boston Dynamics, which we interface with using \textit{pybind11}.
We refer the reader to the Video material for a visualization of the real-word experiments.


\subsection{Optimality}
\label{app:optimality}
Let us consider an MDP $\mathcal{M}$, the reduced MDP $\mathcal{M'}$ as defined in Subsection \ref{ssec:learn_maskedQ1}, the selection operator  ${\alpha: \mathcal{S \to P(A)}}$ and a set of feasible policies under $\alpha$: $\Pi_\alpha=\{ \pi | \pi(s,g) \in \alpha(s) \forall s \in \mathcal{S}, g \in \mathcal{G} \}$.

\begin{theorem}
Given an MDP $\mathcal{M}$, the reduced MDP $\mathcal{M'}$ and a selection operator $\alpha$ such that the optimal policy for $\mathcal{M}$ belongs to $\Pi_\alpha$ (that is $\pi_{\mathcal{M}}^*(s, g) \in \Pi_\alpha$),
the optimal policy in $\mathcal{M'}$ is also optimal for $\mathcal{M}$, i.e. $\pi_{\mathcal{M'}}^*(s, g) = \pi_{\mathcal{M}}^*(s, g)$.
\end{theorem}

\begin{proof}
We can define a goal-conditioned value function $V^\pi(s,g)$, defined as the expected sum of discounted future rewards if the agent starts in $s$ and follows policy $\pi$ thereafter: ${V^\pi(s, g) = \mathbb{E}_{\mu^\pi}\left[\sum_{t-1}^\infty \gamma^{t-1} R(s, g) \right]}$ under the trajectory distribution ${\mu^\pi(\tau|g) = \rho_0(s_0) \prod_{t=0}^\infty P(s_{t+1}|s_t, a_t)}$ with $a_t = \pi(s_t, g) \; \forall t$.
By construction, the transition kernel $P$ and the reward function $R$ are shared across $\mathcal{M}$ and $\mathcal{M'}$, and a feasible policy $\pi$ always selects the same action in both MDPs, thus trajectory distributions and value functions are also identical for feasible policies. More formally, if $\pi \in \Pi_\alpha$, then $\mu^\pi_{\mathcal{M}}=\mu^\pi_{\mathcal{M'}}$ and $V^\pi_{\mathcal{M}}(s, g) = V^\pi_{\mathcal{M'}}(s, g)$.

It is then sufficient to note that 

\begin{equation}
\begin{aligned}
     \pi_{\mathcal{M'}}^*(s, g)&\stackrel{def}{=}\argmax_{\pi \in \Pi_\alpha} V^{\pi}_{\mathcal{M'}}(s, g)=\argmax_{\pi \in \Pi_\alpha} V^{\pi}_{\mathcal{M}}(s,g)=\\
     &=\argmax_{\pi \in \Pi} V^{\pi}_{\mathcal{M}}(s, g)\stackrel{def}{=}\pi_{\mathcal{M}}^*(s, g),
\end{aligned}
\end{equation}

    
where the second equality is due to the previous statement, and the third equality is granted by the assumption that the optimal policy for $\mathcal{M}$ is feasible in $\mathcal{M'}$ (i.e. $\pi_\mathcal{M}^* \in \Pi_\alpha$).
\end{proof}

Hence, our proposed algorithm, which allows us learning directly on the reduced MDP $\mathcal{M'}$, can under mild assumptions retrieve the optimal policy in the original $\mathcal{M}$.

We finally remark that model-free PAC-MDP algorithms have shown to produce upper bounds on sample complexity that are $\tilde O(N)$, where $N \leq |\mathcal{S}||\mathcal{A}|$ \cite{lattimore2012pac}, i.e.,  directly dependent on the number of state-action pairs. 
Hence, learning in the reduced MDP $\mathcal{M'}$ instead of $\mathcal{M}$ could lead to near-linear improvements in sample efficiency as the number of infeasible actions grows. We demonstrate it empirically in the subsection \textit{Sample Complexity} in \ref{sec:Experiments}.
