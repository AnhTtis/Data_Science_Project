\section{Related Work}
\label{sec:rel_work}

\subsection{Planning Algorithms}
Classical motion planning methods to move a robot in a collision-free path are based on sampling \cite{kavraki1996probabilistic, lavalle2001randomized, karaman2011sampling} or constrained optimization \cite{ratliff2009chomp, schulman2014motion} and can efficiently find long-horizon paths. However, they do not allow a robot to alter the world. Task-and-motion planning (TAMP) methods are used when planning for a robot that operates in environments containing a large number of objects, taking actions to navigate the world as well as to change the state of the objects \cite{kaelbling2013integrated, toussaint2015logic, toussaint2017multi,garrett2020integrated, braun2021rhh}.
Despite performing well in long-horizon planning, they are limited by the need of a world model, their task representations, the dimensionality of the search space and the inability to execute high-dimensional, complex tasks robustly.

\subsection{Model-Free Learning}
Model-free learning offers a promising alternative when dealing with unknown environments but relies on a reward function defining the task at hand.
Additionally, model-free methods are sample inefficient \cite{duan_one-shot_2017} in general and have difficulties reasoning over long horizons.
Hierarchical RL (HRL) \cite{barto2003recent, dayan1992feudal, wiering1997hq, sutton1999between,dietterich2000hierarchical} methods have been proposed as a scalable solution that directly leverages temporal abstraction but in practice, they struggle with sample complexity and suffer from brittle generalization \cite{nasiriany2022augmenting}.
Several HRL algorithms rely on goal-conditioned policies \cite{Kaelbling93learningto, andrychowicz2017hindsight, gupta_relay_2019, christen2020hide} for low-level control but they tend to make training unstable (see discussion in \cite{nachum2018data}).
To address this, several works have adopted classical motion planning techniques to replace the high-level policy (e.g. tree search methods \cite{eysenbach2019search,sermanet2021broadly}). In contrast, we sidestep the problem by replacing the low-level policy with a predefined set of task-primitives and train a model-free high-level policy to control their execution.
Closely related to our work, \cite{ICLR16-hausknecht, dalal2021accelerating,nasiriany2022augmenting} use Parameterized Action MDPs \cite{masson2016reinforcement} in which the agent executes a parameterized primitive at each decision-making step. These methods also rely on a specified library of primitives but, unlike ours, they are parameterized. However, this versatility in primitive instantiation comes at a low efficiency cost since the agent needs to explore large amounts of parameters to solve long horizon tasks. Other methods also use motion planners as a low-level controller and learn an RL policy in a high-level action space \cite{relmogen2021, yamada21a, angelov2020}.

\subsection{Guiding Exploration in Large Action Spaces}
As exploration remains a fundamental challenge for RL agents, previous works have attempted to ease the burden of planning in complex spaces by pruning the actions available at each step.
Invalid action masking has been proposed in large strategy games to restrict sampling to a fixed subset of the action space  \cite{vinyals2017new, berner2019dota, ye2020mastering}. However, these methods assume that the set of illegal actions is given a priori. The case in which a random subset of all actions is available at each step was also formally studied by \cite{boutilier2018planning}.
When prior information is not available, several methods naturally propose to learn which actions are suitable. In the action elimination literature, this is achieved with \cite{zahavy2018learn} or without  \cite{even2003action} assuming the availability of additional supervision from the environment, although the latter case remains constrained to tabular settings.
Another relevant framework is that of affordances \cite{gibson1977theory, khetarpal2020what, costales2022possibility}, which measure the possibility for single actions to achieve desired future configurations. Affordances can again be learned from an explicit signal from the environment \cite{khetarpal2020what} or utilize prior information. %(e.g. regarding intents).
Our method is perhaps more closely related to behavioral priors, which can be learned from offline trajectories \cite{pertsch2020spirl, pertsch2021skild, singh2021parrot} or online interaction \cite{tirumala2022behavior} and can be used to direct exploration \cite{singh2021parrot} or regularize learned policies \cite{tirumala2022behavior}. Behavioral priors can be modeled as conditional probability distributions over the action space. Our method also learns a state-conditional model of possible actions from data and does not require explicit supervision (i.e. validity labels for the agent's actions).
However, while behavioral prior literature largely focuses on continuous actions and soft constraints (e.g. KL-regularization), our method learns over a discrete set of actions and relies on a hard integration of the prior into the training process.
