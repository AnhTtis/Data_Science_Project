\section{Problem Statement}
\label{sec:statement}
We model the environment as a goal-conditioned Markov Decision Process represented as an 8-tuple $(\mathcal{S}, \mathcal{G}, \mathcal{A}, P, R, \rho_0, \rho_g, \gamma)$ with possibly continuous states $s \in \mathcal{S}$ and goals $g \in \mathcal{G}$, discrete actions $a \in \mathcal{A}$, transition kernel $P(\cdot|s,a)$, reward function $R(s, g)$, initial state distribution $\rho_0$, goal distribution $\rho_g$ and discount factor $\gamma$. We focus on sparse reward signals: $R(\cdot|s, g) = \mathds{1}_{\{|f(s)-g|_d < \epsilon\}}$, where $|\cdot|_d$ is an arbitrary distance metric, $f: \mathcal{S} \to \mathcal{G}$ is a projection and $\epsilon$ is a threshold.
We additionally denote by $\pi: \mathcal{S \times G \to A}$ a goal-conditioned stationary policy.

Crucially, we assume access to a fixed dataset collected through \textit{play}, following an unknown behavior policy $\pi_\beta$.
Play data \cite{lynch_learning_2019} can be inexpensively collected by a human operator controlling a robot to achieve arbitrary environment configurations while interacting with the objects at hand.
This is useful to extract \textit{affordances} \cite{gibson1977theory}, representing the subset of possible actions which are feasible in a current situation. For example, a drawer can be pushed or pulled, but not moved sideways.
In our framework, \textit{play} happens at a high level, more specifically at the level of motion primitives or skills, where a human can choose from a predefined set of primitives, which are generally available (e.g. precoded behaviors for real-word hardware such as for Spot robot \cite{noauthor_spot_nodate} or as in \cite{saycan2022arxiv}).
Another solution, which could involve recording low-level action sequences, and segmenting them into meaningful skills as in \cite{zhu2022bottom,konidaris2010constructing, niekum2015online}, is left for future works.
The resulting play dataset consists of $N$ high-level state-action pairs $\mathcal{D} = \{(s_i, a_i)\}_{i=1}^N$ where a high level action $a_i$ represents the primitive enacted by the play agent in state $s_i$.




Following the goal-conditioned framework \cite{schaul2015universal}, we intend to find a policy $\pi: \mathcal{S \times G \to A}$ that maximizes the expected discounted return
\begin{equation}
J(\pi) = \mathbb{E}_{g \sim \rho_g, \mu^\pi}\left[\sum_{t=1}^\infty \gamma^{t-1}R(s_t, g)\right],
\end{equation}
under the trajectory distribution \mbox{$\mu^\pi (\tau | g) = \rho_0 (s_0) \prod_{t=0}^{\infty} P(s_{t+1} | s_t,a_t)$ with $a_t=\pi(s_t, g)$}.

We remark that through this paper, the notation of atomic actions $a \in \mathcal{A}$ is used to represent non-parametric motion primitives. We effectively abstract away the complexity of low-level control and let the action space $\mathcal{A}$ be composed of a finite set of high-level behaviors which are, in practice, executed through established motion planning methods. As a consequence, a single timestep in the MDP corresponds to the execution of a single primitive. We finally remark that, while the method described in Section \ref{sec:method} is designed and deployed in this particular setting, it remains generally applicable (e.g. in MDPs with low-level actions).