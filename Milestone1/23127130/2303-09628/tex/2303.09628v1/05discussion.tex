\section{Discussion and Future Work} \label{sec:discussion}
\label{sec:conclusion}

We present \alg, a method that bridges motion planning and deep RL to achieve complex long-horizon manipulation tasks. 
We show that by integrating a discrete behavioral prior learned from easily collectable play data, we can achieve significant gains in sample efficiency compared to other baselines that leverage prior data.
This approach has the added benefit of largely avoiding infeasible actions during training.
By planning in a two-level hierarchy, we show how our method allows reasoning over long-horizons in a mixed decision space in an efficient manner. We finally demonstrate that within this framework, \alg can be easily transferred to physical hardware without further modifications, showing the potential of combining readily available motion planners with sample efficient RL algorithms. 

Despite showing promising results, our method assumes full observability of the state space and perfect execution of the motion primitives. These limitations could be addressed by introducing perception and by querying \alg at higher frequencies at inference time to ensure primitive completion.
 Furthermore, choosing a suitable level of abstractions for skills remains an open question, whose answer could relax the need for providing a predefined set of skills, while still maintaining a low-dimensional parametrization.
Future work may also actively learn the behavioral prior instead of leveraging a static play dataset, reaching a compromise between sample complexity and reliance on collected data.

We expect our work to enable future research directions such as a tighter coupling between the training of the high-level planner and the execution of motion primitives.
Although introducing motion planning in the training loop is time consuming, we believe that the significant gains in sample efficiency demonstrated in this work can help address this challenge.
