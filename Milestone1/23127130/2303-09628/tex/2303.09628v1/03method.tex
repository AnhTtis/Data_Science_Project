\section{Efficient Learning of High Level Plans from Play (\alg)}
\label{sec:method}

We now present our algorithm \alg for solving long-horizon tasks with motion primitives and play-guided RL.
To address exploration, we first learn a discrete behavioral prior that eliminates infeasible actions from the set of primitives and hence prunes the search space (\Cref{ssec:learn_prior}, Figure \ref{fig:spot_priors}).
Next, in \Cref{ssec:learn_maskedQ1} we propose and motivate an integration scheme for the learned prior onto Q-learning. Consequently, our agent can focus on learning Q-values for feasible state-action pairs only, as the prior generally lifts the burden of learning to avoid infeasible actions.


\begin{figure}[b]
\begin{center}
\vspace{-1mm}
\includegraphics[width=\columnwidth]{figures/fig2_v2_small_compressed.pdf}
\end{center}
\vspace{-2mm}
\caption{The trained behavioral prior $\pi^\beta$ learns to estimate the set of feasible primitives in different environment configurations. (Left) When the agent is at the center of the space, the prior favors primitives that involve reaching elements through a free-collision path (go to \textit{door}, \textit{drawer}s or above the \textit{object}), but prevents actions that involve object manipulation. (Right) When being close to the door, the prior learns correct object affordances such as \textit{grasp} or \textit{slide}.}
\vspace{-5mm}
\label{fig:spot_priors}
\end{figure}

\subsection{Learning a Prior from Play} \label{ssec:learn_prior}

Let us start by considering the play dataset \mbox{$\mathcal{D} = \{ (s_1, a_1),  ... (s_N, a_N) \}$} introduced in Section \ref{sec:statement}. While lacking explicit exploitative behaviors, play data inherently favors actions that are \textit{feasible}: while not necessarily desirable for a given goal, those actions are likely to be successfully executed given the current state of the environment.
We aim to extract this information by estimating a (goal-independent) behavioral prior $\pi^\beta(\cdot|s)$, modeled as a conditional categorical distributions over primitives, which associates feasible primitives to higher likelihoods.

We parameterize $\pi^\beta$ through a neural network with learnable parameters $\omega$, which can be trained via standard mini-batch first order techniques to minimize the negative log-likelihood $\mathcal{L}_{NLL}(\omega)$:
\begin{equation}
    \mathcal{L}_{NLL}(\omega) = \mathop{\mathbb{E}}_{\substack{B \sim \mathcal{D}}}\Bigg[\frac{1}{|B|} \sum_{\substack{(s, a) \in B}} -\log \pi^\beta_\omega(a|s) \Bigg],
\label{eq:prior_loss}
\end{equation}
where $B$ represents a batch of state-action pairs sampled uniformly from the dataset $\mathcal{D}$.


\subsection{Selecting Feasible Actions}
Given the learned behavioral prior $\pi_\beta(\cdot|s)$, we propose to turn its soft probability distribution into hard, binary constraints on the action space. We thus define a threshold-based selection operator $\alpha: \mathcal{S} \to \mathcal{P(A)}$:
\begin{equation}
 \alpha(s) = \{ a \in \mathcal{A} \mid \pi_\beta(a|s) > \rho\},
\label{eq:mask}
\end{equation}
where $\mathcal{P(\cdot)}$ represents a powerset.
Ideally, an action $a \not \in \alpha(s)$ would not be chosen by an optimal goal-conditioned policy in state $s$. We refer to $\alpha(s)$ as the set of \textit{feasible} actions for state $s$. See Figure \ref{fig:spot_priors} for a visualization.


\subsection{Learning in a Reduced MDP}\label{ssec:learn_maskedQ1}

The learned selection operator $\alpha(s)$ enables the definition of an auxiliary MDP $\mathcal{M'}$, which we refer to as \textit{reduced} MDP. The definition and solution of the reduced MDP lay at the core of our method.
We model $\mathcal{M'}$ through a generalized definition of MDPs \cite{puterman1994} in which available actions depend on the current state: in the state $s$, the action space is restricted to a subset $\alpha(s) \subseteq \mathcal{A}$.

\begin{definition}[Reduced MDP]
  Given an MDP \mbox{$\mathcal{M}=(\mathcal{S}, \mathcal{G}, \mathcal{A}, P, R, \rho_0, \rho_g, \gamma)$} and a selection operator $\alpha: \mathcal{S} \to \mathcal{P(A)}$ such that for all $s \in \mathcal{S}$, $\alpha(s) \neq \emptyset$, the reduced MDP $\mathcal{M'}$ is defined as the 9-tuple $(\mathcal{S}, \mathcal{G}, \mathcal{A}, P, R, \rho_0, \rho_g, \alpha, \gamma)$.
\end{definition}
\vspace{2mm}
where the assumption on $\alpha(s)$ ensures that there exist a feasible action in each state and Q-values can be well-defined.
Intuitively, $\mathcal{M'}$ encodes the same environment as $\mathcal{M}$ but restricts the set of action-state pairs.

We note that in PAC RL settings, the analysis of sample complexity \cite{kakade2003sample} (i.e. the number of steps for which a learned policy is not $\epsilon$-optimal with high probability) produces upper bounds that are directly dependent on the number of state-action pairs \cite{lattimore2012pac}.
In particular, model-free PAC-MDP algorithms can attain a sample complexity that is $\tilde O(N)$, where $N \leq |\mathcal{S}||\mathcal{A}|$ is the number of state-action pairs, and $\tilde O(\cdot)$ represents $O(\cdot)$ where logarithmic factors are ignored \cite{strehl2006pac}.
Learning in $\mathcal{M'}$ instead of $\mathcal{M}$ is thus desirable and could lead to near-linear improvements in sample efficiency as the number of infeasible actions grows.
Crucially, under mild assumptions, the optimal policy for $\mathcal{M'}$ can not only be retrieved more efficiently but also attains optimality in the original MDP $\mathcal{M}$ (see \OurAppendix \ref{app:optimality}).

We thus propose a practical modified Q-learning iteration on the original MDP $\mathcal{M}$, which is equivalent to performing Q-learning directly in the reduced MDP $\mathcal{M'}$.
Given a transition $(s, a, s', g, r)$:
\begin{equation}
    Q(s, a, g) \gets (1-\delta)Q(s,a,g) + \delta(r + \gamma \max_{a' \in \alpha(s)}Q(s', a', g)),
\end{equation}
where the value of the next-state $s'$ is only computed over feasible actions and $\delta$ is the learning rate.

Under common assumptions (i.e. infinite visitation of each state-action pair and well-behaved learning rate in tabular settings \cite{bertsekas1996neuro}), this algorithm converges to $Q_{\mathcal{M'}}^*$, from which we can easily extract $\pi_{\mathcal{M}}^*(s,g) = \pi_{\mathcal{M'}}^*(s,g) = \argmax_{a \in \alpha(s)} Q_{\mathcal{M'}}(s,a,g)$. For simplicity, we will from now on refer to  $Q_{\mathcal{M'}}$ as $Q$.

In practice, following the goal-conditioned framework \cite{schaul2015universal}, we scale this algorithm by parameterizing $Q_\theta(s,a,g)$ through a neural network.
Inspired by recent success in scaling Q-learning \cite{watkins1992q} to high-dimensional spaces \cite{van2016deep, mnih2015human, fujimoto2018addressing} while reducing overestimation bias, we learn the parameters $\theta$ of the Q-function using Clipped Double Q-learning \cite{fujimoto2018addressing}, which minimizes the temporal difference (TD) loss:
\begin{gather}
    \mathcal L (\theta_j) = \mathbb E_{\substack{(s,a,s',g,r) \sim \mathcal{B} }} \big [(y_j - Q_{\theta_j}(s_t,a_t,g))^2]\label{eq:q-loss}, \text{ with } \\
    \nonumber y_j =  r + \gamma \min_{i=1,2}  Q_{\theta'_i}(s', \argmax_{a_{t+1} \in \alpha(s_{t+1})}  Q_{\theta_j}(s_{t+1}, a_{t+1}, g), g ),
\end{gather}
where $j \in \{1,2\}$, and $\theta_j, \theta_j'$ are the parameters for Q and target Q-networks respectively and where $(s,a,s',g,r)$ tuples are sampled uniformly from an experience replay buffer \cite{lin1992reinforcement} $\mathcal{B}$ exploiting the off-policy nature of Q-learning.
We collect experience following an $\eps$-greedy exploration mechanism on the feasible action set $\alpha(s) \subseteq \mathcal{A}$. We summarize our approach in Algorithm \ref{algorithm}.

\setlength{\textfloatsep}{0mm}
\begin{algorithm}[ht]
 \caption{\alg} \label{alg:algorithm}
\begin{algorithmic}
\small
    \INPUT Trained prior $\pi^\beta_w$ , randomly initialized $Q_\theta$ and Q-target $Q_{\theta'}$ with $\theta = \theta'$, probability threshold $\rho$, learning rate $\eta$, replay buffer $\mathcal{D}=\emptyset$, soft update parameter $\mu$.%, number of training episodes N, episode length T.
    \FOR{episode$=1,\ldots$ N}
        \STATE Sample $s \sim \rho_0$, $g \in \rho_g$.
        \FOR{step$=1,\ldots T$}
        \STATE Compute feasible action set $\alpha(s_t)$ in \eqref{eq:mask}, compute $Q_\theta(s_t, a, g)$ for each $a \in \mathcal{A}$.
        \STATE With probability $\epsilon$ sample $a_t \sim \mathcal{U}\{\alpha(s_t)\}$, else select $a_t = \argmax_{a \in \alpha(s_{t+1})}Q_\theta(s_t, a, g)$.
        \STATE Execute $a_t$ and store transition $(s_t, a_t, r_t, s_{t+1}, g)$ in $\mathcal{D}$.
        \STATE Sample minibatch of transitions $(s_j, a_j, r_j, s_{j+1}, g)$ uniformly from $\mathcal{D}$.
        \STATE Compute TD loss $\mathcal L_{\mathrm{Q}} (\theta)$ in \eqref{eq:q-loss}.
        \STATE Gradient step $\theta \gets \theta - \eta \nabla \mathcal L_{\mathrm{Q}}(\theta)$.
        \STATE Perform soft-update on $\theta' \gets \mu \theta + (1-\mu) \theta' $.
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\label{algorithm}
\end{algorithm}