\section{Introduction}
\label{sec:Intro}

One of the collective visions of robotics is a world where robots help humans with daily household chores, such as setting up a table or emptying the dishwasher.
An immediate challenge is that, to operate in the physical world, a robot must be able to reason in a mixed decision space.
That is, it must combine decisions relating continuous motions with discrete subtasks in order to accomplish complex tasks.
For example, the task of emptying the dishwasher requires the robot to first go to the dishwasher, then open it, grab a single dish, open the cupboard and place the dish inside, and repeat until the dishwasher is empty.
Failing to perform a single subtask or ordering them incorrectly would ultimately lead to a failure.
The ability to reason over long horizons, we argue, is thus a crucial milestone in achieving the above-mentioned vision.

Despite significant improvements in several fields of robotics, ranging from motion planning to robust control \cite{winkler2018optimization, winkler2018gait, dai2014whole, bjelonic2022offline},  long-horizon problems involving several tasks still presents a considerable challenge.
While planning algorithms can efficiently search well-defined state spaces and plan over relatively long horizons, they generally struggle with complex, stochastic dynamics, and high-dimensional systems, in particular featuring narrow passages (i.e. low-measure regions which have to be traversed to reach a goal).
Moreover, they often require accurate models of the environment or handcrafted distance measures \cite{sermanet2021broadly, eysenbach2019search, mainprice2020interior}.

Learning methods have thus emerged as a more scalable approach for handling the high-dimensionality of the real world.
While capable of obtaining hard-to-engineer behaviors \cite{kalashnikov2021mt, akkaya2019solving, andrychowicz2020learning, kalashnikov2018qt}, learning-based methods are similarly
limited to short horizons tasks in general, since longer plans involving multi-level reasoning aggravate the challenges of exploration and temporal credit assignment \cite{nachum2018data}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\columnwidth]{figures/fig1_v3_large_text_compressed.pdf}
\end{center}
\vspace{-4mm}
\caption{Overview of \alg. A discrete behavioral prior learned on play data is used to discard infeasible actions at every state, enabling a high-level RL agent to search over a smaller space of behaviors. Each action is mapped to a motion primitive, which is executed through classical motion planning methods. After training in simulation, the policies can be easily deployed on real hardware.}
\vspace{-5mm}
\label{fig:method}
\end{figure}
In this work, we introduce \alg, a novel approach for robotic learning of long-horizon manipulation tasks, bridging motion planning and deep reinforcement learning (RL).
In a two-level hierarchy, high-level planning is entrusted to a goal-conditioned policy, trained through deep RL and designed to deal with the high-dimensionality and complexity of real-world dynamics. This policy operates at a coarser time scale and controls the execution of primitives, which lie at the lower level in the hierarchy (see Figure \ref{fig:method}). Crucially, we sidestep the instability of jointly learning high and low-level policies and thus rely on a predefined library of object-centric primitives (such as grasping or reaching a target configuration in a collision-free path) obtained through classical motion planning.

While planning over primitives substantially reduces the burden of the higher level, uniform exploration may still be insufficient in long-horizon tasks with sparse rewards, as the space of possible behaviors grows rapidly with the size of the action space and number of steps to the goal \cite{levy2018hierarchical}.
Although humans excel at inferring the set of behaviors enabled by a situation, uninformed RL agents tend to repeatedly attempt futile actions, resulting in poor sample efficiency and potentially unsafe exploration.

We thus additionally propose to prune the high-level action space to a subspace of feasible primitives, inferred dynamically at each step. In practice, this is modelled through a discrete behavioral prior learned from task-agnostic play data.

While existing works on behavioral priors largely rely on soft integration schemes, we instead propose to discretize the prior into a hard, binary constraint. We then define an algorithm that only explores and exploits over the set of feasible actions, effectively performing Q-learning in a reduced Markov Decision Process (MDP).
Theoretically, a near-optimal solution to this MDP can be retrieved more efficiently and under mild assumptions, can be generalized to the original MDP.
Empirically, we observe improved performance in a variety of complex manipulation tasks and highlight the ease of transferring the policy to real-world hardware.

Our approach represents a promising step towards learning to solve long-horizon tasks in a hierarchical manner by combining task-agnostic play data, RL and motion planning algorithms. Our contributions can be summarized as follows: (a) we explore the integration of RL with classic motion planning algorithms and show its feasibility in a simulated manipulation environment; (b) we highlight how the exploration problem can be further addressed by introducing a prior model learned from play data; (c) we evaluate our proposed algorithm both theoretically and empirically, while comparing it against relevant baselines; (d) we show how learned policies can be deployed to physical hardware.
Videos and additional information can be found at \href{https://nuria95.github.io/elf-p/}{\tt nuria95.github.io/elf-p}.
\nocite{coumans2021, kingma2014}
