\section{Results}\label{sec:Experiments}

In this Section, we empirically evaluate the performance of \alg against several baselines. We investigate whether \alg can retrieve optimal policies more efficiently than existing approaches that leverage prior data, and whether the number of infeasible actions that are attempted throughout the training process is reduced.
We evaluate \alg on a variety of manipulation tasks in a simulated environment and then deploy the learned policies on real hardware to evaluate the ease of transfer.
For further details on the environment, additional experiments and extended results, see \OurAppendix \ref{app:env}, \ref{app:experiments}, \ref{app:additional_exp_details} and \ref{app:hardware_exp}.


\begin{figure}[b]
\vspace{2mm}
\begin{center}
\includegraphics[width=\columnwidth]{figures/fig3_small.pdf}
\end{center}
\vspace{-2mm}
\caption{An example of a task.
Starting from an arbitrary configuration, the agent needs to place the block, which is hidden behind the cabinet door, in the mid drawer, which is blocked by the drawer above. The sequence of 19 actions required to achieve the goal is shown below. }
\label{fig:task_example}
\end{figure}

\begin{figure*}[!htb]
\begin{center}
\vspace{0mm}

\minipage{\textwidth}
\small
\centering
\textcolor{elfp}{\rule[2pt]{20pt}{3pt}} \textrm{ELF-P} \quad
\textcolor{ddqn}{\rule[2pt]{20pt}{3pt}} \textrm{DDQN} \quad
\textcolor{her}{\rule[2pt]{20pt}{3pt}} \textrm{DDQN+HER}
\textcolor{prefill}{\rule[2pt]{20pt}{3pt}} \textrm{DDQN+Prefill} \quad
\textcolor{ddqnfd}{\rule[2pt]{20pt}{3pt}} \textrm{DDQNfD} \quad
\textcolor{spirl}{\rule[2pt]{20pt}{3pt}} \textrm{SPIRL}
\endminipage
\vspace{-1mm}
\end{center}
\minipage{\linewidth}
\includegraphics[width=0.24\textwidth]{figures_icra/medium_datapoints10000.pdf}
\label{fig:medium_success}
\includegraphics[width=0.24\textwidth]{figures_icra/medium_inv_action_datapoints10000.pdf}
\label{fig:medium_inv}
\includegraphics[width=0.24\textwidth]{figures_icra/hard_datapoints10000.pdf}
\label{fig:hard_success}
\includegraphics[width=0.24\textwidth]{figures_icra/hard_inv_action_datapoints10000.pdf}
\label{fig:hard_inv}
\endminipage
\vspace{-5mm}
\caption{Success rate and number of infeasible actions attempts for the Medium (left) and Hard (right) task variants. (Results are averaged across 10 independent random seeds, shaded area represents standard deviation).}
\label{fig:simulation-results}
\vspace{-0.6cm}
\end{figure*}


\begin{figure}
\begin{center}%[!htb]
\vspace{2mm}

\minipage{\linewidth}
\small
\centering
\textcolor{elfp}{\rule[2pt]{20pt}{3pt}} \textrm{ELF-P} \quad
\textcolor{soft-elfp}{\rule[2pt]{20pt}{3pt}} \textrm{SOFT ELF-P}
\endminipage
\vspace{-1mm}
\end{center}
\minipage{\linewidth}
\includegraphics[width=0.49\textwidth]{figures_icra/hard_with_soft_datapoints10000.pdf}
\label{fig:hard_soft_success}
\includegraphics[width=0.49\textwidth]{figures_icra/hard_inv_action_with_soft_datapoints10000.pdf}
\label{fig:hard_soft_inv}
\endminipage
\vspace{-5mm}
\caption{Comparison between \alg and Soft-\alg. \alg. Success rate and number of infeasible actions attempts for the Hard  task variant. (Play dataset size $10^4$). Results are averaged across 10 independent
random seeds, shaded area represents standard deviation) }
\label{fig:soft_vs_hard}
\end{figure}
We compare \alg with an unmodified DDQN \cite{van2016deep}, with DDQN with Hindsight Experience Replay (DDQN+HER) \cite{andrychowicz2017hindsight},
with two other state-of-the-art methods  that can leverage prior data, namely DDQNfD \cite{hester2018deep} (with 1-step returns and unprioritized experience replay for a fair comparison), DDQN+Prefill, which initializes its replay buffer with the task-agnostic data $\mathcal{D}$, and with SPiRL \cite{pertsch2020spirl}, which  also uses play data to constrain exploration, but adopts an actor-critic framework with soft prior regularization, which we adapted to operate over discrete action spaces. Extended details are in \OurAppendix \ref{app:baselines}.

\subsection{Simulation Experiments} \label{exp:simulation}
We train and evaluate the algorithm on a variety of simulated long-horizon manipulation tasks in which a robot needs to interact with a realistic desk environment featuring several household items. Possible behaviors include sliding a cabinet door, opening several drawers and moving a wooden block (see Figure \ref{fig:task_example}).
The tasks are defined in the same environment that play data was obtained in. While play data can be collected by humans, for simplicity we use a scripted policy (details are in \OurAppendix \ref{app:additional_exp_details}). % that samples uniformly from the set of feasible actions at each state
The goal of each task is to place the block in an arbitrary desired position, which generally also requires manipulating the rest of the items. The episode ends when exceeding a predefined number of steps or when the goal is achieved. The reward function is sparse and is equal to one if and only if the task is completed in time, otherwise a reward of zero is given.

We evaluate on tasks distributions with two levels of difficulty: \textit{Medium} (M) and \textit{Hard} (H). The average number of actions required to solve the (M) tasks for an expert planner is 14, with the longest task requiring 16 steps. For the (H) tasks, the average is 23 steps, with the longest task requiring 29 steps. See Figure \ref{fig:task_example} for an example.

\vspace{2mm}
\subsubsection{Performance Analysis}

Figure \ref{fig:simulation-results} shows the success rate and the number of infeasible actions attempts averaged over 50 evaluation episodes across the two task distributions.
In the (M) tasks \alg shows the same sample efficiency as the best baseline, while in the (H) tasks it is significantly better than all competitors.
For both (M) and (H) tasks, the number of infeasible actions that are attempted by the agent is significantly lower than other baselines. We note that the execution of infeasible actions is due to inaccuracies in the trained prior.

While vanilla DDQN cannot master the (M) task and fails to solve the (H) task, Prefill+DDQN and DDQNfD manage to solve the (M) task as efficiently as \alg, proving that having access to task-agnostic play data is beneficial for the learning process. However, when the action space grows in size, their performance decreases significantly.
We also notice that although managing to reach over 0.5 success rate for both tasks, SPIRL shows a much lower performance than our method and most competitive baselines, hence integrating a soft prior via KL-regularization might not be beneficial in this setting.
Finally, we observe that adding HER relabeling helps slightly on the (H) tasks but hurts performance on the (M) tasks.
We report that using HER with \alg also hurts performance (see \OurAppendix \ref{app:experiments}). HER relies on a gradual growth of the frontier of achieved goals, which can be used for relabeling. Since the dynamics in our environment are not smooth (i.e. a single action often leads to large changes in the state), we hypothesize that HER cannot interpolate to unseen goals and hence hurts performance.
This behavior was also pointed out by \cite{eysenbach2019search}.

We note that all methods in this section that can leverage prior data have access to a play dataset with $10^4$ datapoints, which simulates an amount of data that could in practice, be collected by a few human operators.

\vspace{2mm}
\begin{wrapfigure}{r}{44mm}
\vspace{-3mm}
\centering
\includegraphics[width=\linewidth]{figures-appendix/boxplot_datasize.pdf}
\vspace{-6mm}
\caption{Effect of play dataset size on sample complexity (DDQN+Prefill fails to solve the task with 1000 datapoints).}
\vspace{-3mm}
\label{fig:boxplot_dataset size}
\end{wrapfigure}
\subsubsection{Robustness to Play Dataset Size}
\label{sec:robustness-datasize}
We study the effect of play dataset size on training performance for both \alg and DDQN+Prefill (which affects the amount of data used for training the prior for \alg and the amount of data used to prefill the replay buffer for DDQN+Prefill). In particular, we measure the number of timesteps required to reach a success rate of 0.95 on (H) tasks when using different dataset sizes.
We report results in Figure \ref{fig:boxplot_dataset size}. We observe that our method retains most of its performance when the prior is trained on minimal quantities of data whereas DDQN+Prefill struggles in low-data regimes. This shows that our method is more suited for the setting of \textit{play}, and is able to learn efficiently with reasonable amounts of data, i.e., data that could in practice be collected by a few human \mbox{operators ($\sim$ 1h30min} of interaction data).
When more data is available, we generally find the performance gap between methods to decrease, while ELF-P consistently outperforms baselines in all data regimes. See \OurAppendix \ref{app:robustness-datasize} for additional results.

\vspace{2mm}
\begin{wrapfigure}{rt}{44mm}
\vspace{-4mm}
\centering
\includegraphics[width=\linewidth]{figures_icra/boxplot_medium_True.pdf}
\vspace{-6mm}
\caption{Effect of $\rho$ on sample complexity. Means across seeds are connected by lines.}
\vspace{-3mm}
\label{fig:sample-complxity}
\end{wrapfigure}
\subsubsection{Sample Complexity}
We further analyze the gains in sample complexity and compare them with the theoretical rates mentioned in Subsection \ref{ssec:learn_maskedQ1}, which hypothesize a linear dependency on the number of feasible action-state pairs in PAC-RL settings.
In particular, we measure the number of timesteps required to reach a success rate of 0.95 on (M) tasks with increasing values for the threshold $\rho$, thus pruning the state-action space more aggressively.
In Figure \ref{fig:sample-complxity} we plot this quantity against the ratio of infeasible state-action pairs for each value of $\rho$, computed according to the visitation distribution of a random policy.
While our algorithm recovers the performance of DDQN for $\rho=0$, we indeed observe a  linear trend in the decrease of sample complexity as the number of infeasible action-state pairs increases. Further details are in \OurAppendix \ref{app:sample-complexity}.
% \vspace{3mm}




\subsubsection{Soft Prior Integration}
We implement  Soft-\alg, an ablation for our method which softens the integration of the prior into learning. This modification could potentially allow Soft-\alg to recover from degenerated priors, i.e., when play data has significant distribution mismatch with the target tasks of interest.
Soft-\alg samples from the prior (instead of sampling from the feasible set $\alpha$) both during initial exploration phase and while performing $\epsilon$-greedy exploration. Additionally, during exploitation, it multiplies the softmax of Q-values by the prior, thus biasing the greedy action selection towards the prior.
Finally, given the soft integration of the prior, it performs Q-learning over the set of all actions, instead of over the reduced set of feasible actions.
In Figure \ref{fig:soft_vs_hard} we compare the performance of \alg with Soft-\alg on the (H) task. Results on the (M) task are in the \OurAppendix \ref{app:soft-elfp}. We observe that while Soft-\alg is able to learn both medium and hard tasks, it has lower sample efficiency than \alg. The hard prior integration of \alg alleviates the Q-network from learning values for all state-action pairs: it can focus on learning Q-values for feasible state-action pairs only and ignore Q-values for unfeasible actions. This shows one of the main contributions of our algorithm.
While binarizing the prior results in greater learning efficiency, a soft integration could be  useful when dealing with degenerated priors and we reserve further exploration on the topic for future work.


\subsection{Real-world Experiments} \label{exp:spot}
In this experiment, we evaluate how \alg performs when transferred to physical hardware.
Figure \ref{fig:method} (upper right) depicts our physical robot workspace and further details can be found in the \OurAppendix \ref{app:hardware_exp}.
We use \textit{Boston Dynamics} Spot robot \cite{noauthor_spot_nodate} for all our experiments because of its stability and robustness when following desired end-effector trajectories.
The high-level planner, trained in simulation, is used at inference time to predict the required sequence of motion primitives to achieve a desired goal. We then instantiate each of them using established motion planning methods \cite{zimmermann2022dca} to create optimal control trajectories. For further implementation details, we refer the reader to \cite{zimmermann2021go}.

We show that \alg can be easily transferred to physical hardware while maintaining its goal-reaching capabilities.
In Figure \ref{fig:spot_priors} we show the agent executing several primitives in order to complete a task. We refer the reader to \OurAppendix \ref{app:hardware_exp} and the video material for extended visualizations.
