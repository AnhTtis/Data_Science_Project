\documentclass[../main.tex]{subfiles}

\begin{document}

\textit{M. J. Hardcastle, J. Forbrich, L. Smith, V. Stolyarov, M. Ashdown, J. Coles}\newline

\noindent The Starmech tackled the Challenge from the point of view of dealing with the Challenge dataset within the constraints of the resources provided to us (a single node with 30 cores and 124~GB RAM, 800~GB root volume and 1\,TB additional data volume). Some computational constraints will be a feature of future working in the field when computing resources are provided as part of shared SKA Regional Centres. 

We considered existing source finding tools: {\sc PyBDSF} \citep{2015ascl.soft02007M}, a continuum source finder, and {\sc SoFiA} and {\sc SoFiA-2}, two generations of a 3D source finder already optimised for H{\sc i} \citep{2021MNRAS.506.3962W}. While {\sc PyBDSF} readily generated a catalogue of the continuum source and could be run on many slices in frequency, slicing and averaging with fixed frequency steps does not give good results since emission lines have a variety of possible widths in frequency space. Instead we focused on the two publicly available 3D source finders. Our tests showed that {\sc SoFiA-2}â€™s memory footprint is much lower than that of {\sc SoFiA} for a given data cube and its speed significantly higher, so it became our algorithm of choice. 

In order to work with the available RAM, we needed to slice the full Challenge datacube either in frequency or spatially. We chose to slice spatially because this allows {\sc SoFiA-2} to operate as expected in frequency space; essentially the approach is to break the sky down into smaller angular regions, run {\sc SoFiA-2} on each one in series, and then join and de-duplicate the resulting catalogue. Whether done in parallel (as in the MPI implementation {\sc SoFiA-X}; \citealt{2021MNRAS.506.3962W}), or in series as we describe here, some approach like this will always be necessary for large enough H{\sc i} series in the SKA era since the full dataset sizes will exceed any feasible RAM in a single node for the foreseeable future. 

Our implementation was a simple {\sc python} wrapper around {\sc SoFiA-2}. The code calculates the number of regions into which the input data cube needs to be divided such that each individual sub-cube can fit into the available RAM. Assuming a tiling of $n \times n$, it then tiles the cube with $n^2$ overlapping rectangular spatial regions. We define a guard region width $g$ in pixels: each region passed to {\sc SoFiA} overlaps the adjacent one, unless on an edge, by $2g$ pixels. Looping over the sub-cubes, {\sc SoFiA-2} is run on each one to produce $n^2$ overlapping catalogues in total. For our final submission we used {\sc SoFiA-2} default parameters with an \texttt{scfind.threshold} of 4.5 sigma, $g=20$ pixels, a spatial offset threshold for de-duplication of 1 pixel, and a frequency threshold of 1 MHz. $g$ was chosen to be larger than the typical size in pixels of any real source. We verified that there were no significant differences, using these parameters, between the reassembled catalogue for a smaller test cube and the catalogue directly generated by running {\sc SoFiA-2} on the same cube, using {\sc TOPCAT} for simple catalogue visualization and cross-matching. Due to time constraints, we did not move on to the next obvious step of optimising the parameters used for {\sc SoFiA-2} based on further runs on the test and development datasets.

We removed source duplication arising from overlapping regions by considering catalogues from adjacent sub-cubes pairwise.  We firstly discarded all catalogue entries with pixel position more than $g$ pixels from the edge of a sub-cube; these should already be present in another catalogue. The remaining overlap region, $2g$ pixels in width, height or both, was cross-matched in position and sources whose position and frequency differ by less than user-defined threshold values were considered duplicates and discarded from one of the two catalogues. Finally the resulting $n^2$ de-duplicated catalogues were merged and catalogue values converted according to units specified by the submission format.

We would like to have explored the utility of dimensional compression of the data as part of the source finding, for example by using moment maps in an attempt to eliminate noise and better pinpoint source detection algorithms. A priori, this would have been of rather technical interest since any resulting bias on source detection would need to be considered. However, in this way, it may have been possible to identify candidate sources to then characterise based on observable parameters such as size and linewidth, in a first step as point sources vs resolved sources, and including flags for potential overlap in projection or velocity.


\end{document}