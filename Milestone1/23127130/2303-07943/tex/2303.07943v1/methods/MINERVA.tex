\documentclass[../main.tex]{subfiles}

\begin{document}

\textit{D. Cornu, B. Semelin, X. Lu, S. Aicardi, P. Salom√©, A. Marchal, J. Freundlich, F. Combes, C. Tasse\newline}

\noindent The MINERVA team developed two pipelines in parallel. The final catalogue merges the results from the two pipelines.

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{images/Minerva_YOLO_CHADHOC_flux_w20_global_relative_diff_coverage.pdf}
	\caption{Team Minerva: Difference in the number of sources found between CHADHOC and YOLO-C catalogues in a flux against line width parameter space. The color encodes the difference in the local number of sources as a proportion of the total merged catalogue size (32652 predicted sources). The contours are the local number of sources averaged between the two catalogues with values: 6, 14, 30, 50, 64, 92, 128, 192. The density heatmap is computed on a 30$\times$30 grid and plotted with interpolation.}
	\label{fig_minerva}
\end{figure}

\subsubsection{YOLO-CIANNA}
\textit{}\newline
The YOLO-CIANNA pipeline implemented a highly customised version of a YOLO \citep[You Only Look Once,][]{redmon_yolo_v1, redmon_yolo_v2, redmon_yolo_v3} network, which is a regression-based CNN dedicated to object detection and classification. We added low-level YOLO capabilities to our own general purpose CNN framework, CIANNA\footnote{\url{https://github.com/Deyht/CIANNA}} (Convolutional Interactive Artificial Neural Networks by/for Astrophysicists).

The definition of the training sample was of major importance to get good results. Most of the sources in the large development dataset are impossible for the network to detect, and tagging them as positive detections would lead to a poorly trained model. For YOLO we used a combination of criteria to define a training set: i) the CHADHOC classical detection algorithm (see Section~\ref{chadhoc}); ii) a volume brightness threshold; iii) a local signal-to-noise ratio estimation. Our refined training set contains around $\sim$1500 `true' objects, with 10\% set aside for validation. All inputs were augmented using position and frequency offsets and flips. YOLO-CIANNA operates on sub-volumes of $48\times48\times192$ (RA, Dec, Frequency) pixels. The network was trained by selecting either a sub-volume that contains at least one true source or a random empty field, in order to learn to exclude all types of noise aggregation and artefacts. 

Each sub-volume was divided into a $6\times6\times12$ grid containing sub-cubes of size $8\times8\times16$ pixels.  A detection prediction was made for each sub-cube, producing the following parameters: $x$, $y$, $z$ the object position; $w$, $h$, $d$ the dimensions of a bounding box containing the object; $O$ an `objectness' score that combines the detection confidence with a box-matching score.  We modified the YOLO loss function to allow us to predict the required H{\sc i} flux, size, line width, position angle and inclination at the same time for each box. The network itself is made of 21 3D convolutional layers which alternate several `large' filters (usually $3\times3\times5$) that extract morphological properties and fewer `smaller' filters (usually $1\times1\times3$) that force a higher degree feature space while preserving a manageable number of weights to optimise. Some of the layers include a higher stride value in order to progressively reduce the dimensions down to the $6\times6\times12$ grid. The last few layers include dropout for regularisation, and error estimation.  

Despite the fact that YOLO networks are known for their fast performance, our customised architecture still requires up to 36 hours of training on a single RTX 3090 GPU using FP16/FP32 Tensor Core mixed precision training. The trained network has an inference speed of 76 sub-volumes per second using a V100 GPU on Jean-Zay/IDRIS, but due to necessary partial overlap and RAM limitations, it still requires up to 20 GPU hours to process the full $\sim$1\,TB data cube.

\subsubsection{CHADHOC}\label{chadhoc}
\textit{}\newline
The Convolutional Hybrid Ad-Hoc pipeline (CHADHOC) has been developed specifically for SDC2. It is composed of three steps: a traditional detection algorithm, a CNN for identifying true sources among the detections, and a set of CNNs for source parameter estimation. 

For detection, we first smooth the signal cube by a 600 kHz width along the frequency dimension and convert to a signal-to-noise ratio on a per channel basis. Pixels below a fixed SNR of $\sim$2.2 are filtered out, and the remaining pixels are aggregated into detected sources using a simple friend-of-friend linking process with a linking length of 2 pixels. The position of each detection is computed by averaging the positions of the aggregated pixels. A catalogue of detections is then produced, ordered according to the summed source SNR values. When applied to the full Challenge dataset, we divide the cube into 25 chunks and produce one catalogue for each chunk.

The selection step is performed with a CNN. A training sample is built by cross-matching with the truth catalogue the $10^5$ brightest detections in the development cube, thus assigning a True/False label to each detection. Unsmoothed signal-to-noise cutouts of $38 \times 38 \times 100$ pixels around the position of each detection are the inputs for the network. The learning set is augmented by flipping in all three dimensions, and one third of the detections is set aside as test set. The comparatively light network is made of 5 3D convolutional layers, containing 8, 16, 32, 32 and 8 filters, and 3 dense layers, containing 96, 32 and 2 neurons. Batch normalisation, dropouts and pooling layers are inserted between almost every convolutional and dense layer. In total the network has of the order of $10^5$ parameters. The training is performed on a single Tesla V100 GPU in at most a few hours, reaching best performances after a few tens of epochs. The model produces a number between 0 (False) and 1 (True) for each detection. The threshold where the source is labelled as True is a parameter that must be tuned to maximise the metric defined by the SDC2. This optimisation is performed independently of the training.

A distinct CNN has been developed to predict each of the source properties and includes a correction to the source position computed during the detection step. The architecture is similar to the one of the selection CNN, with small variations: for example, no dropout is used between convolutional layers for predicting the line flux. Cutouts around the $\sim$1300 brightest sources in the truth catalogue of the development cube are augmented by flipping and used to build the learning and tests sets. The networks are trained for at most a few hundred epochs in a few to 20 minutes each on a Tesla V100 GPU. Training for longer results in overfitting and a drop in accuracy. 
    
Many details impact the final performance of the pipeline. Among them, the centering of the sources in the cutouts. Translational invariance is not trained into the networks. This is dictated by the nature of the detection process and is possibly the main limitation of the pipeline: the selection CNN will never be asked about sources that have not been detected by the traditional algorithm.

\subsubsection{ Merging the catalogues}
If we visualize the catalogues produced by YOLO and CHADHOC in the sources parameter space (Fig.~\ref{fig_minerva}), we find that they occupy slightly different regions. For example, CHADHOC tends to find a (slightly) larger number of typical sources compared to YOLO, but misses more low-brightness sources because of the hard SNR threshold applied during the detection step. Thus, merging the catalogues yields a better catalogue.

Since both pipelines provide a confidence level for each source to be true, we can adjust the thresholds after cross-matching the two catalogues. In case of a cross-match we lower the required confidence level while when no cross-match is found we increase the required threshold. The different thresholds must be tuned to maximise purity and completeness. Finally, the errors on the source properties are at least partially uncorrelated between the two pipelines. Thus averaging the predicted values also improves the resulting catalogue properties.


\end{document}