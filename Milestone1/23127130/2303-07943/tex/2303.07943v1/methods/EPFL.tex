\documentclass[../main.tex]{subfiles}

\begin{document}

\textit{ E. Tolley, D. Korber, A. Peel,  A. Galan, M. Sargent, G. Fourestey, C. Gheller, J.-P. Kneib, F. Courbin, J.-L.Starck}\newline


\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{images/sdc2_workflow_epfl.jpeg}
	\caption{Data processing pipeline used by the EPFL team.}
	\label{fig_epfl}
\end{figure}

% figure to be added
\noindent The EPFL team used a variety of techniques developed specifically for the Challenge and which have been collected into the \texttt{LiSA} library \citep{TOLLEY2022100631} publically available on github\footnote{\url{https://github.com/epfl-radio-astro/LiSA}}. The pipeline (Fig.~\ref{fig_epfl}) first decomposed the Challenge data cube into overlapping domains by dividing along RA and Dec. Each domain was then analysed by a separate node on the computing system. A pre-processing step used 3D wavelet filtering to denoise each domain: decomposition in the 2D spatial dimensions used the Isotropic Undecimated Wavelet Transform \citep{4060954}, while the decimated 9/7 wavelet transform \citep{vonesch2007generalized} was applied to the 1D frequency axis. A joint likelihood model was then calculated from the residual noise and used to identify H{\sc i} source candidates through null hypothesis testing in a sliding window along the frequency axis. Pixels with a likelihood score below a certain threshold (i.e. unlikely to be noise) were grouped into islands. The size and arrangement of these islands were used to reject data artefacts. Ultimately the location of the pixel with the highest significance was kept as an H{\sc i} source candidate location.

A classifier CNN was used to further distinguish true H{\sc i} sources from the set of candidates. The final H{\sc i} source locations were then used to extract data from the original, non-denoised domain to be passed to an Inception CNN which calculated the source parameters. The Inception CNN used multiple modules to examine data features at different scales. Finally, the H{\sc i} source locations and features for each domain were concatenated to create the full catalogue. Both CNNs were trained on the development dataset using extensive data augmentation.



\end{document}