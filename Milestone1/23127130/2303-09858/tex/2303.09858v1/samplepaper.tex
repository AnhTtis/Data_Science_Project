% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{subfigure}
% \usepackage{biblatex}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{adjustbox}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[colorlinks,linkcolor=red]{hyperref}

% \usepackage{booktabs}
% \usepackage{caption}

\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{MedLocker: A Transferable Adversarial Watermarking for Preventing Unauthorized Analysis of Medical Image Dataset}
%Transferable Adversarial Watermarking for Medical Images to Prevent Unauthorized Model Embezzlement
%Transferable Digital Watermarking for Medical Images to Prevent Unauthorized Model Stealing
%Cross-model transferable digital watermarking for medical image copyright protection
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Bangzheng~Pu\inst{1,2} \and
Xingxing~Wei\inst{1,2} \and Shiji~Zhao\inst{2} \and Huazhu~Fu\inst{3}}
\authorrunning{Pu et al.}

\institute{School of Software, Beihang University, Beijing, China.
\email{Pu\_bangzheng@buaa.edu.cn} \\ \and
Institute of Artificial intelligence, Hangzhou Innovation Institute, Beihang University, Beijing, China. \email{\{xxwei, zhaoshiji123\}\{@buaa.edu.cn\}}
\\ \and 
Institute of High Performance Computing (IHPC)
Agency for Science, Technology and Research (A*STAR), Singapore. \email{hzfu@ieee.org}}


% \author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
% %
% \authorrunning{F. Author et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
% %
\maketitle              % typeset the header of the contribution
%
\begin{abstract} 
The collection of medical image datasets is a demanding and laborious process that requires significant resources. Furthermore, these medical datasets may contain personally identifiable information, necessitating measures to ensure that unauthorized access is prevented. 
Failure to do so could violate the intellectual property rights of the dataset owner and potentially compromise the privacy of patients. As a result, safeguarding medical datasets and preventing unauthorized usage by AI diagnostic models is a pressing challenge.
To address this challenge, we propose a novel visible adversarial watermarking method for medical image copyright protection, called MedLocker. Our approach involves continuously optimizing the position and transparency of a watermark logo, which reduces the performance of the target model, leading to incorrect predictions. Importantly, we ensure that our method minimizes the impact on clinical visualization by constraining watermark positions using semantical masks (WSM), which are bounding boxes of lesion regions based on semantic segmentation.
To ensure the transferability of the watermark across different models, we verify the cross-model transferability of the watermark generated on a single model. Additionally, we generate a unique watermark parameter list each time, which can be used as a certification to verify the authorization.
We evaluate the performance of MedLocker on various mainstream backbones and validate the feasibility of adversarial watermarking for copyright protection on two widely-used diabetic retinopathy detection datasets. Our results demonstrate that MedLocker can effectively protect the copyright of medical datasets and prevent unauthorized users from analyzing medical images with AI diagnostic models.
% We evaluated the performance of our MedLocker on various mainstream backbones and validated the feasibility of adversarial watermarking for copyright protection on two widely-used diabetic retinopathy detection datasets. Our results demonstrate that our MedLocker can effectively protect the copyright of medical datasets and prevent unauthorized users from analyzing medical images with AI models.

% The creation of medical imaging datasets requires significant investments of time and resources. However, using these datasets to train medical deep models without proper authorization violates the intellectual property rights of the dataset owner and poses a risk of leaking sensitive patient information. Protecting the copyright of medical datasets and preventing unauthorized users from training diagnostic models is an emergent challenge.
% In this paper, we propose a novel visible adversarial watermarking method for medical image copyright protection, named MedLocker, which can disturb unauthorized training models while minimizing the impact on visual diagnosis. Our approach involves continuously optimizing the position and transparency of a watermark logo, which reduces the performance of the target neural network, leading to incorrect prediction. We verified the cross-model transferability of the watermark generated on a single model given the complexity of the model structures in real-world scenarios. Furthermore, to minimize the impact on medical diagnoses, we constrained watermark positions using semantical masks (WSM), which are bounding boxes of lesion regions based on semantic segmentation.
% Through extensive experiments, we evaluated the performance of adversarial watermarks on various mainstream backbone networks and validated the feasibility of adversarial watermarking for copyright protection on two widely-used diabetic retinopathy detection datasets. Our proposed method can effectively protect the copyright of medical datasets and prevent unauthorized users from training diagnostic models, thus addressing an important challenge in the field.

\keywords{Visible watermarking \and Copyright protection \and Adversarial attack \and Transferability.}
\end{abstract}

\section{Introduction}
% The performance of deep learning medical imaging diagnostic models relies on high-quality annotated datasets, which incurs substantial expenses. 

Production of medical image datasets necessitates the participation of experienced radiologists and costly imaging equipment. Furthermore, it may undergo a substantial duration to amass sufficient cases or procure data simultaneously from multiple healthcare institutions~\cite{Radsch2023}. Unauthorized AI diagnostic models to analyze medical imaging data may lead to the disclosure of patient privacy, infringement of intellectual property and copyright, inaccurate diagnoses, and ethical issues.
In the past, the common method for protecting the ownership of images is adding watermarks, such as visible watermarking \cite{shen1998dct,kankanhalli1999adaptive,hu2005algorithm,liu2010generic}  or invisible digital watermarking \cite{mohanarathinam2020digital,allaf2019review,soualmi2018new,2022An}.  However, these traditional methods are only effective for human declarations of copyright but are unable to prevent images from being analyzed by AI models. 

\begin{figure}[!t]
\includegraphics[width=\textwidth]{motivation1.pdf}
\caption{(a) Our goal is to generate watermarks on medical images to prevent unauthorized analysis of datasets, while minimizing interference with clinical diagnosis. (b) Besides, our method should distinguish and verify the authorized users.} \label{motivation}
\end{figure}

Recently, adversarial examples put a threat to deep neural networks as they can cause models to make incorrect predictions~\cite{Finlayson2019,Zhou2021}. Adversarial watermarks leverage this characteristic to prevent data infringement by DNN models. Tian et al. \cite{tian2022confoundergan} create imperceptible adversarial watermarks to confound label-image mapping, to prevent private images collection from unapproved DNN on the internet. 
Backdoor watermarking \cite{gu2019badnets,turner2019label} is used to protect the copyright of the authorized model by correlating specific categories with a trigger. These adversarial watermarks that associated with specific categories are easier to be detected.
Li et al. \cite{li2022untargeted} propose an untargeted backdoor watermark to improve the stealthiness and verify ownership by hypothesis testing. 
Different from meaningless watermarks from the above research, Jia et al. \cite{jia2020adv} propose a query-based method to generate meaningful adversarial watermarks. 
For medical images, Ma et al. \cite{ma2021understanding} evaluate the adversarial noise against several disease classifiers and Ozbulak et al. \cite{ozbulak2019impact} propose an adaptive mask attack against the segmentation network. 
However, diffusion models \cite{nie2022diffusion} have been prove to be powerful for purifying adversarial noise, but implement less effect on meaningful information of images. Moreover, these watermarks are difficult to prevent the utilization of diagnostic models with unknown information.
% Therefore, meaningful adversarial watermarks have high research value for protecting the copyright of medical datasets.

To address above issues, we propose \textbf{a visible adversarial watermark method called MedLocker for protecting copyrights of medical images}, as shown in Fig.~[\ref{motivation}]. Our method not only misleads the predictions of unauthorized diagnostic models but also displays meaningful logos to claim ownership in the medical image. Adversarial watermarks are generated by querying source models' outputs, and the locations and transparency of the watermark logo are optimized by the evolutionary algorithm. Since the information of medical diagnostic models in practice is unknown, transferred attacks are implemented, and transferability is improved by the ensemble model. 
Furthermore, reliable medical disease diagnosis relies on prior knowledge of anatomy, and to address this, we propose \textbf{a method of semantic masked lesion adversarial watermarking for medical datasets (WSM)}. We obtain the location of lesions in advance using an image segmentation model, and our semantic masks can limit the watermark away from areas with diagnostic meaning, which does not affect the analysis by doctors.
we evaluate the performance of our proposed method using various mainstream backbones on two widely-used diabetic retinopathy detection datasets. Our results demonstrate that MedLocker can effectively prevent unauthorized users from analyzing medical images with AI models. Additionally, we discuss the watermarking certification method, which ensures the ability to distinguish and unlock authorized datasets. \textbf{Our code implementation will be publicly available after acceptance.}

% To, we propose a visible adversarial watermark method for protecting copyrights of medical images, called \textbf{MedLocker}, which can not only mislead the predictions of the diagnostic models but also displays meaningful logos to claim ownership. 
% Our method generates adversarial watermarks by querying source models' outputs, and the locations and transparency of the watermark logo are optimized by the evolutionary algorithm. 
% Due to the unknown structure of medical diagnostic models in practice, we implement transferred attacks and improved transferability by the ensemble model. 
% Additionally, reliable medical disease diagnosis depends on prior knowledge of anatomy, we propose a method of semantic masked lesion adversarial watermarking for medical datasets (WSM). By using an image segmentation model to obtain the location of lesions in advance, our semantic masks can limit the watermark away from areas in the meaning of diagnostics, which does not affect the doctors' analysis. 
% Finally, we discuss the watermarking certification method, which ensures the ability to distinguish and unlock authorized datasets.


% Finally, we discuss the authorization method, the randomness of our algorithm results ensure that the combination of all optimized parameters generated each time is different, thus we can use these results as unique authorization certificates. Our method can generate watermarks on each image in the entire dataset and wrong predictions are random, making it difficult to detect images that truly poison the model.

%通过搜索适合的水印的透明度和位置误导模型的分类结果
% \begin{algorithm}
%     \caption{BHE-AMAW algorithm}
%     \label{alg:1}
%     \begin{algorithmic}
%         \REQUIRE Original dataset $D$, Original image $I_{Ori}$, Logo $W$, Bounding boxes $bbxs$, Initialized parameters(population $N_p$, Gneration $N_g$, alpha range $\alpha$)
%         % \REQUIRE $n \geq 0 \vee x \neq 0$
%         \ENSURE Image with watermark$I_W$, Datasets with watermarks$D_W$
%         \FOR $I_{Ori}$ in $D$
            
%         \STATE $y \Leftarrow 1$
%     \IF{$n < 0$}
%         \STATE $X \Leftarrow 1 / x$
%         \STATE $N \Leftarrow -n$
%     \ELSE
%         \STATE $X \Leftarrow x$
%         \STATE $N \Leftarrow n$
%     \ENDIF
%     \WHILE{$N \neq 0$}
%         \IF{$N$ is even}
%             \STATE $X \Leftarrow X \times X$
%             \STATE $N \Leftarrow N / 2$
%         \ELSE[$N$ is odd]
%             \STATE $y \Leftarrow y \times X$
%             \STATE $N \Leftarrow N - 1$
%         \ENDIF
%     \ENDWHILE
%     \end{algorithmic}
% \end{algorithm}



%Our intention is to generate a watermark of threat models while minimizing the impact on medical diagnosis. MedLocker is applied for copyright protection of medical datasets and prevents analysis by unauthorized models. The datasets can be unlocked by a key, which is a special list of watermarking parameters of each authorized user.

\section{Proposed Method}
% Firstly, we present a brief theoretical explanation of visible watermark blending. Then, we describe the general settings of MedLocker and its application when limited by semantic masks. Then, we propose methods for improving the transferability of our watermarks by using an ensemble model. Lastly, we discuss the process of ownership authentication.
Our goal is to add watermarks to images and prevent the unauthorized analysis of medical datasets while minimizing interference with doctors' judgments (Fig.~[\ref{motivation}]). Our MedLocker has two main settings: watermarks in arbitrary positions (WAP) and watermarks limited by semantic masks (WSM). 
% In this section, we will introduce the theory of visible watermarks, adversarial watermark generation methods in two settings, and transferable attacks across different models.

\subsection{MedLocker Creation in General Settings}

\noindent \textbf{The Visible Watermark:}
We use the alpha blending technique \cite{shen1998dct} to composite watermarks. Given an original image of size $W\times H$ with four channels $RGBA$ and a watermark logo of size $M\times N$ with the same channels, the alpha blended pixel is formulated as follows,
\begin{equation}
    I(i,j)= (1 - \alpha/255) \times I_{ori}(i,j) + \alpha/255 \times I_{logo}(p,q) ,
    \label{eq1}
\end{equation}
where $I_{ori}$ represents the original image, $I_{logo}$ is the watermark logo image, $I$ is the blended image, $\alpha$ represents the pixel value of the alpha channel, $\alpha \in [0,255]$. Assuming the coordinate of the upper-left is the origin point, $i \in [0,W], j \in [0,H]$. let x and y be the coordinates of the upper-left of the watermark logo on the original image, $p \in [x,x+M], q \in [y,x+N], x \in [0,W-x), y \in [0,W-y)$. The size of the watermark logo is recalculated based on the original image size and the scaling factor $sl$, which can be expressed as,
\begin{equation}
    [M',N']= min(W/(sl \times M),H/(sl \times N)) \times [M,N] .
    \label{eq2}
\end{equation}


\begin{figure}[!t]
\includegraphics[width=\textwidth]{framework.pdf}
\caption{The flowchart of our MedLocker. (a) The dataset with MedLocker is generated by optimizing adversarial watermarks on source models, whose transferability is evaluated on target models; (b) The evolutionary strategy is used to optimize the parameters of alpha blending.} \label{framework}
\end{figure}

\vspace{3pt} \noindent \textbf{Optimization Objective:}
As illustrated in Fig.~[\ref{framework}], our approach involves generating adversarial watermarks for each sample in the dataset by querying the source model, followed by optimizing the location $(x,y)$ and transparency $\alpha$ using an evolutionary algorithm. Specifically, we generate dominant individuals $[\alpha,x,y]$ that minimize the ground-truth class probability $f_t(I_n)$ for the $n$-th sample. Mathematically, this can be expressed as follows,
%Seen as Fig.[\ref{framework}], for each sample in datasets, we generated adversarial watermarking by querying the source model and adopted an evolutionary algorithm to optimize the locations $(x,y)$ and transparency $\alpha$. Specifically, by producing dominant individuals $[\alpha,x,y]$, we minimize the ground-truth class probability of the $n$-th sample $f_t(I_n)$. This can be formulated as follows,
\begin{equation}             
      minimize\:f_t(I_n(\alpha,x,y)), \quad s.t. \; \alpha \in [0,255], x \in [0,W-x), y \in [0,W-y) .
        \label{eq3}
\end{equation}

\vspace{3pt} \noindent \textbf{Evolutionary Algorithm:}
Due to the fact that transparency is a continuous variable and position is a discrete variable, we address this issue by using Evolution Strategy, which has strong global search capability and adaptability. Initially, we randomly initialize a set of individuals as parents ($X_{i,g}=[\alpha,x,y]$ represents the $g$-th generation of the $i$-th individual, where $i \in \{1, 2, \dots, N_p\}$ and $g \in \{1, 2, \dots, N_g\}$), and evaluate the fitness of each individual based on the classification confidence $f(I_n)$. We then use Basing Hopping (BH) \cite{jia2020adv} with a few iterations to generate beneficial mutations in the offspring.
\begin{equation} 
    M_{i,g}=BH(X_{i,g},N_{iter},s)
    \label{eq4}
\end{equation}
where $N_{iter}$ is the iteration number and $s$ is the mutation step.

Then a crossover operation is performed between the parents and the mutated offspring with a certain crossover rate $CR$ to enrich the genetic diversity and generate new combinations,
\begin{equation} 
      C_{i,g,j} = \left\{
        \begin{aligned}
        &M_{i,g,j},  \;\; \text{if} \;\; rand(0,1) \leq CR, \\
        &X_{i,g,j},  \;\; \text{Otherwise}.
        \end{aligned}
        \right.
    \label{eq5}
\end{equation}
 where $X_{i,g,j}$ represents $j$-th gene of $X_{i,g}$.
 
Finally, the dominant individuals are selected by comparing $f_{t}(I_n)$,
\begin{equation}      
        S_{i,g}= \left\{
        \begin{aligned}
        &C_{i,g},  \;\; \text{if} \;\; f_{t}[I_n(C_{i,g})] \leq f_{t}[I_n(X_{i,g})], \\
        &X_{i,g},  \;\; \text{Otherwise}.
        \end{aligned}
        \right.
            \label{eq5}
\end{equation}
Thus, by blending the watermark logo with the original image according to the optimal individuals, we can eventually create the MedLocker of our dataset.

% \subsubsection{Watermarks in arbitrary positions (WAP)} 
% Fig.[\ref{framework}]. (a) firstly shows the evolutionary algorithm optimizing the transparency and position, then the 'EyePAC' logo with the fundus images are blended to obtain the adversarial watermark. Follow the formulation\ref{eq1}, the watermark is allowed to be generated at any position in the original images.

\subsection{MedLocker Limited by Semantic Mask (WSM)} 

\begin{figure}[!t]
\includegraphics[width=1\textwidth]{gen_mask.png}
\caption{The process of generating semantic masks begins by creating a pixel-level mask that delineates the lesion regions within the image. Next, we use this mask to derive a set of bounding boxes (bbxs) that serve to constrain the position of the regions of interest.} \label{gen_mask}
\end{figure}

Lesion grading based on pathological features is crucial for accurate diagnosis. To minimize the visual impact of watermarks, we propose a mask-generation method that utilizes semantic information from medical images to avoid occlusion. As depicted in Fig.~[\ref{gen_mask}], our approach comprises the following steps: \\
\textbf{STEP 1:} Obtaining a pixel-level mask $M_n \in {0,1}$ of the fundus lesion via a segmentation network. \\
\textbf{STEP 2:} Utilizing dilated convolution $K(size, n_{iter}) \circledast M_n$ to group adjacent smaller pixels into connected regions $R=\{r_1,r_2,\dots,r_m\}$, where $size$ is the kernel size and $n_{iter}$ is the iteration number.\\ 
\textbf{STEP 3:} Removing small isolated regions with an area below a threshold $\lambda$, denoted as $r_i=0$ if $A_{r_i} \leq \lambda$, where $A_{r_i}$ represents the area of $r_i$. \\
\textbf{STEP 4:} Calculating the bounding boxes $B$ of the remaining regions, represented as a tetrad $(x,y,w,h)$. In the case of bounding boxes with high overlap ($IOU(B_K, B_l) \textgreater \gamma$), they are merged. The merged box in the $j$-th comparison can be expressed as follows,
\begin{equation}             
      B_{merged,j}= ((min(x_k,x_l),min(y_k,y_l),max(w_k,w_l),max(w_k,w_l)).
        \label{eq4}
\end{equation} 
\textbf{STEP 5:} As the watermark logo's top-left vertex position is represented by $(x,y)$, expanding the bounding boxes upwards and leftwards by the width and length of the watermark logo is necessary. 

%Such expansion constraints the watermark area by validating whether $(x,y)$ is within the bounding boxes.

\subsection{Transferable Adversarial Watermark}
The information of medical diagnosis models in practical scenarios is often unknown. Therefore, it is crucial to assess the transferability of our watermarking method on various backbone models, in the hope that the adversarial watermark generated on the source model can also be effective on the target model. To enhance transferability, we employ an ensemble model during watermark generation and launch attacks on it. The confidence score of the $n$-th image in the ensemble model can be formulated as,
\begin{equation}             
      f(I_n)_{ensemble}=\sum_{i=1}^{k} \beta_if_i(I_n) ,
        \label{eq4}
\end{equation}
 where $k$ is the number of models, and $\beta$ is the weight of single model.

% \vspace{3pt} \noindent \textbf{Authorization for Datasets:}
% During the deployment, the datasets can be accessed through a unique key, which is a specialized document of watermarking parameters assigned to each authorized user, $Key=\{I_1:[L_1,x_1,y_1], I_2:[L_2,x_2,y_2],\dots, I_n:[L_n,x_n,y_n]\}$, $L=\{\alpha_1, \alpha_2,\dots, \alpha_m\}$, $m \in [1, M \times N]$. To remove the watermarks, authorized users may employ an inverse alpha blending operation to solve $I_{ori}$ by equation \ref{eq1}.
\vspace{3pt} \noindent \textbf{Authorization for Datasets:}
During the deployment, the datasets can be accessed through a unique key, which is a specialized document of watermarking parameters assigned to each authorized user, $Key=\{I_1:[L_1,x_1,y_1], I_2:[L_2,x_2,y_2],\dots, I_n:[L_n,x_n,y_n]\}$, $L=\{\alpha_1, \alpha_2,\dots, \alpha_m\}$, $m \in [1, M \times N]$. In order to remove the watermarks, authorized users may employ an inverse alpha blending operation, as:
\begin{equation}
    I_{ori}(i,j)= (I(i,j) - \alpha/255 \times I_{logo}(p,q))/(1 - \alpha/255) .
    \label{eq1}
\end{equation}

\section{Experiments}
% \subsection{Experiment Setup}

\noindent \textbf{Datasets:}
We select two diabetic retinopathy (DR) datasets (EyePAC on Kaggle \cite{diabetic-retinopathy-detection} and IDRiD \cite{h25w98-18}) to validate our method. The images in both datasets contain labels for DR grading (no DR, mild, moderate, severe, and proliferative DR ). In addition, IDRiD contains fundus lesion data used to train the semantic segmentation model.
% EyePAC dataset contains 35,126 fundus images, of which 28,952 are the training set and 6,174 are the unlabeled test set. IDRiD dataset contains 516 images, of which 413 are images of DR and 103 images of normal. The images in both datasets contain labels for DR grading (no DR, mild, moderate, severe, and proliferative DR ). In addition, IDRiD contains 81 fundus lesion data used to train the semantic segmentation model.

\vspace{3pt} \noindent \textbf{Models training:}
The source model represents the model used for generating adversarial watermarks. The target model represents the model under transferred attack. We use the EyePAC dataset to train mainstream backbones as our source model for watermarking attacks. Specifically, we split the training and testing sets as a 3:1 ratio. Due to the unbalanced categories, 500 images of each category is evenly sampled as the training set for training the DR classification models of ResNet18 \cite{he2016deep}, VGG16 \cite{simonyan2014very}, Inceptionv3 \cite{szegedy2015going}, MobileNetv3 \cite{howard2017mobilenets}, ResNet50 \cite{he2016deep}, Densenet121 \cite{huang2018densenet}, and ViT \cite{dosovitskiy2021image}. IDRiD segmentation dataset lacks the labels of DR grading but includes pixel-level labels of lesions, so this part of the data is graded as DR, and all classified data is relabeled as two classes (No DR or DR). We train the binary classifier with resnet18 and follow the \cite{wang2022anomaly} settings to train the semantic segmentation model with an UNet++ \cite{zhou2018unet++}.

\vspace{3pt} \noindent \textbf{Watermark generation:}
We randomly sample 150 images from each category on the pre-processed EyePAC dataset, which is cropped and resized. To verify cross-model transferability, we only attack samples that are successfully classified by all networks. In experiments of MedLocker with semantic mask limit, we relabel 80 images from the IDRiD segmentation dataset as DR and sample 80 No DR images from the classification dataset. Our watermark generation experiments follow a unified hyperparameter setting: $sl=4, N_p=50, N_g=3, CR=0.9, s=10, \alpha \in [100,255],N_{iter}=3$. The generation of our semantic masks followed: $K(10,10), a=5 \times 10^{4}, \gamma=0.5$.

\vspace{3pt} \noindent \textbf{Metrics:}
Accuracy is used to measure classifiers' performance and attack success rate (ASR) is used to evaluate adversarial watermarks. ASR is equal to the number of samples successfully attacked, divided by the number of samples correctly predicted.

% \subsection{Result Analysis}

\vspace{3pt} \noindent \textbf{Result of MedLocker in general settings:}
 Table \ref {tab1} shows the ASR of adversarial watermarks by querying source models, the ensemble model including resnet18, vgg16, inceptionv3, and mobilenetv3, whose weighting parameters $\beta_i = 0.25$. Table \ref{tab2} shows the ASR of transferable adversarial watermarks, which come from the samples that successfully attacked source models. We show the results of transferable attacks more intuitively with the average data in Fig.[\ref{ablation&transfer}](b). The average ASR of the source model represents the performance of adversarial watermarks generated by the source model, the ensemble model achieve 80.58\%, and the second is inceptionv3 74.48\%. The average accuracy of the target model shows the robustness of target models against watermarks, among which inceptionv3 has the best accuracy 42.36\%, followed by the ensemble model 33.46\%. The results show that it is beneficial to improve the transferability of watermarks by ensemble models.

\begin{table}[!t]
    \centering
    \caption{Watermarks in EyePAC against source models}
    \label{tab1}
    \vskip -3pt
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Source model & ResNet18 & VGG16 & Inceptionv3 & MobileNetv3 & ResNet50 & DenseNet121 & ViT & Ensemble\\
        \hline
        ASR & 71.69\% & 72.88\% & 50.00\% & 60.89\% & 55.92\% & 66.96\% & 67.26\% & 52.05\%\\
        \hline 
    \end{tabular}
    \end{adjustbox}
\end{table}

\begin{table}[!t]
    \centering
    \caption{Watermarks in EyePAC against target models}
    \vskip -3pt
    \label{tab2}
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|c|cccccccc|}
        \hline
        Source model &&&&Target model&&&&\\
        \hline
         & ResNet18 & VGG16 & Inceptionv3 & MobileNetv3 & ResNet50 & DenseNet121 & ViT & Ensemble\\
        \hline
        ResNet18 & - & 91.10\% & 64.83\% & 69.91\% &	69.49\%	 & 67.01\% & 83.25\% &	70.34\% \\
        VGG16 &	80.83\%	&-	& 54.89\%	&65.41\% &63.16\% &	60.96\%	& 74.56\% &	64.29\% \\
        Inceptionv3	& 73.91\% &	81.64\% &	-	& 70.53\% &	69.08\% & 71.62\% &	79.72\%	 & 74.88\% \\
        MobileNetv3	& 72.69\% &	76.65\% & 55.07\% &	-	&74.01\% &	77.66\%	& 73.4\% & 69.6\% \\
        ResNet50 &	72.25\%	 & 74.01\% & 58.15\% &	74.01\%	& -	& 83.42\% & 78.39\%	& 68.72\% \\
        DenseNet121 & 	62.6\%	& 65.85\% &	55.04\%	& 69.51\% & 73.58\%	& -	& 69.1\% & 52.44\% \\
        ViT & 74.89\% &	79.15\% & 57.87\% &	70.21\% & 74.04\% &	72.34\%	 & -	& 65.53\% \\
        Ensemble &	- & - &	- &	- &	82.11\% & 78.71\% &	80.91\% & - \\
        \hline
    \end{tabular}
    \end{adjustbox}
\end{table}

\begin{table}[!t]
  \centering
  \caption{ASR of Watermarks in IDRiD}
   \begin{adjustbox}{width=0.65\textwidth}
    \begin{tabular}{|c|cc|c|c|}
    \hline
          & \multicolumn{2}{c|}{WAP} & WSM-in bbxs & WSM-out bbxs \\
    
          & DR    & No DR & DR    & DR \\
    \hline
    Basing Hoping & 72.06\% & 73.53\% & 11.69\% & 18.18\% \\
    \hline
    Radom Mutation & 2.59\% & 58.82\% & 12.98\% & 2.59\% \\
    \hline
    \end{tabular}%
    \end{adjustbox}
    \vskip -8pt
  \label{tab3}%
\end{table}%

\begin{figure}[!t]
\centering
\includegraphics[width=0.95\textwidth]{ablation_transfer.pdf}
\vskip -5pt
\caption{(a) Ablation study is performed in resnet18. The horizontal axis is the change in the corresponding parameter, the vertical axis is average ASR of the two datasets, the rest parameters follows the general settings of section 3.1.; (b)Average performance of transferable adversarial watermarks. \textbf{Source model}: the average transferred ASR of watermarks generated by source models; \textbf{Target model}: the average accuracy of target models under transferred attack.} \label{ablation&transfer}
\vskip -5pt
\end{figure}

% \begin{table}[!t]
%     \centering
%     \caption{ASR of Watermarks in IDRiD}
%     \vskip -3pt
%     \label{tab3}
%     \begin{adjustbox}{width=0.6\textwidth}
%     \begin{tabular}{|c|c|c|c|}
%     \hline
%         Method & Class & ~Basing hoping~ & ~Random mutation~ \\ \hline
%         WAP & DR & 72.06\% & 2.59\% \\ 
%         ~ & No DR & 73.53\% & 58.82\% \\ \hline
%         WSM-in bbxs & DR & 11.69\% & 12.98\% \\ 
%         ~ & No DR & - & - \\ \hline
%         WSM-out bbxs & DR & 18.18\% & 2.59\% \\ 
%         ~ & No DR & - & - \\ \hline
%     \end{tabular}
%     \end{adjustbox}
% \end{table}

% Table generated by Excel2LaTeX from sheet 'Sheet1'


% \begin{figure}[!t]
%   \centering
%   \subfigure{
%   % \subcaption{The average ASR of watermarks}
%     \includegraphics[width=0.35\textwidth]{AASR.png}
%     \label{fig:figure1}
%   }
%   \hspace{0.03\textwidth}
%   \subfigure{
%   % \subcaption{The average accuracy of target models}
%     \includegraphics[width=0.35\textwidth]{Aacc.png}
%     \label{fig:figure2}
%   }
%   \caption{Average performance of transferable adversarial watermarks. Left graph: the average transferred ASR of watermarks generated by source models; Right graph: the average accuracy of target models.}
%   \label{average}
% \end{figure}

\vspace{3pt} \noindent \textbf{Result of MedLocker limited by semantic masks:}
%本小节中，我们的实验在IDRiD数据集上进行，我们选取了IDRiD分割数据集中的80张图片作为DR样本，并从分类数据集中采样80张图片作为No DR的样本。如Fig.[\ref{gen_mask}]，我们从分割网络输出的mask上计算得到病灶区域的包围框，我们的水印位置遵从三种设置，WAP：watermarks in arbitary position， WSM-in bbxs：watermarks in bounding boxes，WSM-in bbxs：watermarks out of bounding boxes. 由于包围框只约束ground truth 为DR的图像，所以我们分别展示了每种设置下的DR和No DR的ASR。为了验证我们采用盆地跳跃进行有利突变这一策略的有效性，我们与遵循相同超参数设置下的随机突变算法进行对比。结果显示盆地跳跃算法在相同的攻击设置下提高了ASR，特别是无包围盒约束的情况下。
In this part, our experiments perform on the IDRiD dataset and ResNet18. As shown in Fig.~[\ref{gen_mask}], we calculate the boundaries of the lesion region from the mask output of the segmentation network. Our watermark positions obey three settings, WAP: watermarks in arbitrary positions; WSM-in bbxs: watermarks in bounding boxes; WSM-out bbxs: watermarks out of bounding boxes. Since the bounding box only constrains images with the ground truth of DR, we show the ASR of DR and No DR separately, as shown as Table~\ref{tab3}. The results show that the watermark outside the bounding boxes can also reach 18.18\% ASR. WAP is more threatening to models while WSM-in bbxs provide less visual occlusion of lesion regions. To verify the effectiveness of our strategy which uses basin hopping for directional mutation, we compare the ASR with a random mutation following the same hyperparameter for comparison. The results show that the basing hopping algorithm improves the ASR under the same attack settings, especially without the bbxs constraint.


\vspace{3pt} \noindent \textbf{Ablation study:}
We also perform the ablation study to analyze appropriate hyperparameter settings: where the optimization related includes population number, BH iteration, crossover rate, and mutation step; where the watermark related includes scaling ratio of the watermark and the type of logo. Fig.~[\ref{ablation&transfer}](a) shows the results, where our parameter setting performs a good ASR in a dynamic range.

\section{Conclusion}

In this paper, we propose MedLocker, a novel visible adversarial watermarking method for protecting medical image copyrights. Our approach involves the use of meaningful watermarking logos to prevent unauthorized analysis by diagnostic models, while minimizing visual interference to doctors. 
We conduct experiments to demonstrate the broad applicability of our approach, transferring attacks across different backbones and using an ensemble model as the source model to improve transferability. Furthermore, our semantic mask can restrict the active region of the watermark and significantly reduce model prediction accuracy, even outside the lesion area.
By introducing a visible watermark, we enable copyright protection while ensuring that medical practitioners can still access and interpret images accurately. Our findings emphasize the potential of adversarial watermarking as an effective measure for safeguarding sensitive medical data, and we hope that our work will inspire further research in this area.
% In addition, we investigated the effect of directed mutations on our evolutionary algorithm, results showed that basing hoping with a small number of iterations in the mutation process can gather the optimal solution faster in a limited number of times.
%Since our watermark is hard to avoid lesions in numerous separated small regions of pixel-level masks, we proposed to dilate and remove to get several continued regions, then calculated bounding boxes which can quickly determine the region where the watermark is pasted. 


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{ref}


\end{document}
