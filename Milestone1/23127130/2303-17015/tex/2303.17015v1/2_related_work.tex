\section{Related Work}
\label{sec:related}

\paragraph{Neural Implicit Fields}
Neural implicit fields have shown promising results in representing high-fidelity geometry and appearance in 3D. One seminal early work is DeepSDF~\cite{park2019deepsdf}, which encodes the shapes of a class of objects as signed distance functions using a multi-layer fully-connected neural network. Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf}  encode a coordinate-based radiance field with a \MLP and is capable to producing photo-realistic 2D renderings at novel views through volumetric rendering. To address the bias towards learning low-frequency details in a standard MLP, Fourier features~\cite{tancik2020fourier} and periodic activation functions~\cite{sitzmann2020implicit} have been proposed to improve representation of complex signals.
In this work, we train on the weights of these multi-layer fully-connected neural networks and generate new weights that represent valid signals.


\paragraph{Generative Adversarial Networks (GANs)}
Generative Adversarial Networks (GANs)~\cite{goodfellow2020generative} has been shown to be capable of generating high-resolution 2D images, most notably StyleGAN~\cite{karras2019style, Karras2019stylegan2, Karras2021, Sauer2021ARXIV}. More recently, GANs have been adopted for 3D generation with various underlying representations. For instance, pi-GAN~\cite{chan2021pi} modulates the \MLP network which encodes a radiance field,  EG3D~\cite{chan2022efficient} proposes an efficient tri-plane representation, and GMPI~\cite{zhao2022generative} directly generates multiplane images for efficient training and inference. A similar line of work focuses on generating textures for meshes~\cite{siddiqui2022texturify, gao2022get3d}. A general limitation with GANs is volatile training stability due to the game theoretical nature of generator and discriminator networks competing against each other. This motivates researcher to look into alternative generative models, including Diffusion Process. In this work, we adopt a diffusion model for the MLP weight generation.

\paragraph{2D Diffusion Models}
Diffusion probabilistic models~\cite{ho2020denoising, song2020denoising} have emerged as a powerful alternative to its precursors, such as GANs~\cite{goodfellow2020generative} and energy-based models (EBMs)~\cite{du2019implicit}, for generative modeling. They have been shown to outperform GANs on image synthesis tasks in terms of not only superior quality and fidelity~\cite{dhariwal2021diffusion,rombach2022high}, but also the capability to enable text-conditioning~\cite{nichol2021glide, saharia2022photorealistic}. Specifically, Latent Diffusion~\cite{rombach2021highresolution,https://doi.org/10.48550/arxiv.2204.11824} enables high-resolution image synthesis by applying diffusion in the latent space of pretrained autoencoders. In contrast to Latent Diffusion, our method operates on the \MLP weight space and generates new neural implicit fields.

\paragraph{3D/4D Diffusion Models}
Diffusion models have also been deployed in more challenging tasks beyond image synthesis, including 2D video generation~\cite{ho2022video}, 3D shape and scene generation~\cite{Chan2021, chan2021pi, chou2022diffusionsdf, bautista2022gaudi, poole2022dreamfusion, muller2022diffrf}, and 4D generation with animation of 3D shapes~\cite{singer2023text4d, tevet2022human}. In particular, MDM is a human motion diffusion model where the input is a sequence of human poses \cite{tevet2022human}. They use a transformer architecture  as the denoising network. It takes in a noisy human pose sequence input and denoises it to generate a new motion animation. Unlike their approach, we can directly generate animated meshes instead of just human poses. 

Diffusion modeling in a latent manifold for neural fields has also been concurrently proposed, including DiffusionSDF~\cite{chou2022diffusionsdf}, 3DShape2VecSet~\cite{zhang20233dshape2vecset}, and LION~\cite{zeng2022lion}. Such latent diffusion models require a high-quality latent manifold to be learned, which can be challenging with limited quantities of 3D and 4D data. In contrast, our approach to model the weights of optimized neural fields operates on inherently high quality shape representations that can be extremely well-fit per instance. 

\paragraph{Cross-modality Diffusion Models}
A number of recent publications have been investigating unification of diffusion models across different data dimensions and modalities, including Functa~\cite{dupont2022data}, GEM~\cite{du2021learning}, and GASP~\cite{dupont2021generative}. Most recently, Diffusion Probabilistic Fields~\cite{zhuangdiffusion} proposes a explicit field representation without a latent field parametrization, and formulates the generative model in a single-stage end-to-end training. Our method can be seen as an alternative solution to uniting methods designed across different data dimensions, since the implicit neural fields are essentially dimension-agnostic. 


