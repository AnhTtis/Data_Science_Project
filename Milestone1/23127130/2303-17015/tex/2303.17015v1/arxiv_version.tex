\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{stfloats}
\usepackage{subcaption}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\OURS}{HyperDiffusion\xspace}
\newcommand{\MLP}{MLP\xspace}
\newcommand{\Indicator}{\mathds{1}}


\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{\OURS: Generating Implicit Neural Fields with Weight-Space Diffusion}

\author{Ziya Erko\c{c}\textsuperscript{1}
\and
Fangchang Ma\textsuperscript{2}
\and
Qi Shan\textsuperscript{2}
\and
Matthias Nie{\ss}ner\textsuperscript{1}
\and
Angela Dai\textsuperscript{1}
\and
\textsuperscript{1}Technical University of Munich \, \textsuperscript{2}Apple\\ \\
\url{https://ziyaerkoc.com/hyperdiffusion}
}

\twocolumn[{%
	\renewcommand\twocolumn[1][]{#1}%
	\maketitle
	\begin{center}
		\includegraphics[width=\linewidth]{images/teaser.pdf}
		\captionof{figure}{
		\OURS{} enables a new paradigm in directly generating neural implicit fields by predicting their weight parameters. 
		We leverage implicit neural fields to optimize a set of MLPs that faithfully represent individual dataset instances (``Overfitting," top-left).
        Our network, based on a transformer architecture, then models a diffusion process directly on the optimized MLP weights (``Diffusion," bottom-left). 
        This enables synthesis of new implicit fields (``Synthesis," bottom-right).
		}
		\label{fig:teaser}
	\end{center}    
}]

\maketitle
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\input{0_abstract}

%%%%%%%%% BODY TEXT
\input{1_intro}
\input{2_related_work}
\input{3_overview}
\input{4_overfit}
\input{5_diffusion}
\input{6_experiment_setup}
\input{7_results}
\input{8_conclusion}

\smallskip
\noindent \textbf{Acknowledgements.}
This work was supported by the Bavarian State Ministry of Science and the Arts coordinated by the Bavarian Research Institute for Digital Transformation (bidt), the ERC Starting Grant Scan2CAD (804724), and the German Research Foundation (DFG) Research Unit “Learning and Simulation in Visual Computing.” Apple was not involved in the evaluations and implementation of the code.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\section{Appendix}

\subsection{Additional Qualitative Results}
We provide additional unconditional generation results on 3D and 4D generation in Figure~\ref{fig:supplementary_3d} and Figure~\ref{fig:supplementary_4d}. We can generate diverse sets of shapes in both 3D and 4D settings. Resulting meshes are clean, smooth, and can be readily used in any 3D design software and game engines. Although we can output 16 frames for animation sequences, we only show 3 frames in Figure~\ref{fig:supplementary_4d}. Full animation sequences are available in our website and video.

\subsection{Implementation Details}
We use the diffusion and transformer architecture implementations of \cite{peebles2022learning}, which are modified versions of OpenAI and minGPT implementations, respectively. We have a pre-determined MLP structure which consists of 3 hidden layers, each with 256 neurons. To process the MLPs with a transformer architecture, we first flatten the MLP weights into a 1D vector. Additionally, as a way to establish correspondence between components within the 1D vector and the MLP layers (\eg, first \textit{n} values are weights of the first layer), each layer is considered as two tokens, one for its weights and the other for its biases. Hence, in total we have 8 tokens coming from weights and biases. Thanks to this decomposition, transformer may figure out interaction between weights and biases across different layers during training. We also have one additional token representing the sinusoidal embedding of the timestep value. During synthesis of new samples, we again decompose the generated 1D vector into each layer's weights and biases, and load them into the same MLP structure.

Our transformer has 2880 hidden size (i.e., the size of each token after linear projection), 12 layers, and 16 self-attention heads. We use 500 diffusion timesteps in our implementation and a linear noise scheduler ranging between $1e^{-4}$ and $2e^{-2}$. For the sampling strategy, we used DDIM~\cite{song2020denoising} and do not skip any timesteps during sampling. In addition, our denoising network directly predicts the denoised version, following \cite{peebles2022learning}. We observe that $\approx 15\%$ of airplane, $\approx 16\%$ of chair and $\approx 51\%$ of car shapes in our train split of ShapeNet~\cite{chang2015shapenet} contain major self-intersections in their original shape mesh faces, and so we exclude them from the training set for both our approach as well as for all baselines. 

We also apply de-duplication to generated 3D shape results. Note that this has not been applied to baseline approaches, since their quantitative performance degraded with de-duplication. We achieve de-duplication by sampling twice the necessary amount and removing the ones that are very close to each other.
 
\begin{figure}
 \centering
 \includegraphics[width=0.98\linewidth]{images/supplementary_3d.png}
 \caption{Additional unconditional 3D shape generation results.}
 \label{fig:supplementary_3d}
\end{figure}
\begin{figure}
 \centering
 \includegraphics[width=\linewidth]{images/supplementary_4d.png}
 \caption{Additional unconditional 4D animation sequence generation results. We refer to our website for animated shape results.}
 \label{fig:supplementary_4d}
\end{figure}


\end{document}