\section{Method Overview}
\label{sec:overview}
\OURS is an unconditional generative model for implicit neural fields encoded by MLPs. 
We operate directly on MLP weights, enabling generation of new neural implicit fields characterized by synthesized MLP parameters. 
Our training paradigm encompasses a two-phase approach, as shown in Figure~\ref{fig:overview}. 

In the first MLP overfitting step, detailed in Section~\ref{sec:overfit}, we optimize a collection of MLPs such that each MLP represents a faithful neural occupancy field of a data sample (e.g., a 3D shape) from the training set.
This enables highly-accurate shape fitting due to the representation power of the neural fields.
The optimized MLP weights are flattened into 1D vectors and passed to a downstream diffusion process as ground truth signals. 

In the second step, detailed in Section~\ref{sec:diffusion}, the aforementioned optimized MLP weights are passed into a diffusion network for training. This diffusion network is domain-agnostic without any assumptions or prior knowledge on the dimensionality of the underlying signal, since its input is a set of flattened MLP weight vectors. After training is completed, new MLP weights, which correspond to a valid neural implicit field, can be synthesized through the reverse diffusion process on a randomly sampled noise signal. 

For 3D and 4D generation, the underlying meshes can be further extracted and visualized with Marching cubes~\cite{lorensen1987marching}. 