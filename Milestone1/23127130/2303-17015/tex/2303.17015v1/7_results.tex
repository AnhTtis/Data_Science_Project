
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{images/cars_planes.png}
    \caption{Synthesized 3D shapes with \OURS trained on ShapeNet~\cite{chang2015shapenet}. 
    Each shape is represented with a 3-layer 128-dim MLP. We extract the underlying isosurface from the MLPs with Marching Cubes \cite{lorensen1987marching}.
    }
    \label{fig:generation_3d}
\end{figure}


\subsection{Unconditional 3D Generation}

We demonstrate \OURS{} on unconditional neural field generation of 3D shapes. For our method, we show output meshes extracted with Marching Cubes~\cite{lorensen1987marching}.

\begin{figure}[tp]
    \centering
    \includegraphics[width=1.05\linewidth,trim={0.7cm 0.5cm 0 0cm}]{images/results_3d.pdf}
    \caption{
    Qualitative comparison for 3D shape generation.
    Voxel-based diffusion produces relatively low-resolution outputs, while state-of-the-art PVD~\cite{zhou20213d} and DPC~\cite{luo2021diffusion} synthesize discrete point clouds. 
    In contrast, our neural field synthesis can generate a high-quality, continuous surface representation easily extracted as a mesh.
    }
    \label{fig:3d_comparison}
\end{figure}

\paragraph{Comparison to state of the art}
We compare with state-of-the-art 3D shape generation methods Point-Voxel Diffusion~\cite{zhou20213d} (PVD), Diffusion Point Cloud~\cite{luo2021diffusion} (DPC), as well as our voxel baseline in Table~\ref{tab:quant_3d_uncond_gen}.
\OURS{} achieves improved performance over these baselines in all shape categories and all evaluation metrics except MMD, which has been noted to be less reliable due to lack of sensitivity to low-quality results~\cite{zeng2022lion}.
Notably, we achieve significantly improved FPD, which captures the perceptual quality of our synthesized shapes.
Figure~\ref{fig:3d_comparison} further shows our capability to represent high-resolution detail as captured in our synthesized neural fields, in comparison with baselines.


\paragraph{What is the effect of MLP overfitting weight initialization?}
We analyze the effect of our weight initialization strategy for MLP optimization in Table~\ref{tab:ablation_init}, in comparison with random initialization per-MLP. 
Instead of initializing each MLP completely independently, our consistent weight initialization from a single optimized MLP produces improved results, particularly for the perceptual FPD metric. This could be attributed to the fact that with a consistent initialization the MLP weights stay relatively close to each other.

\paragraph{What is the impact of MLP positional encoding?}
We ablate the effect of positional encoding in our per-instance MLP representations in Table~\ref{tab:ablation_pe}, where No PE denotes the same MLP architecture without any positional encoding.
We see that applying positional encoding to the input coordinates significantly improves generation quality.

\paragraph{Novel shape synthesis.}
We explore the degree of novelty in our generated neural fields in 3D shape generation.
Figure~\ref{fig:novel} our synthesized shapes (red) in comparison to the top-3 nearest neighbours by Chamfer distance from the training set (green). 
We show that our synthesized neural fields represent shapes  not already present in the training set, but rather novel shape data.
This is critical to the core idea of our approach since it supports the claim that running denoising duffusion over neural network parameter weights facilitates generalizablity within the underlying data distribution of 3D meshes.


\begin{table*}[]
\centering
\begin{tabular}{cccccc}
\hline
Category & Method & MMD $\downarrow$ & COV (\%) $\uparrow$ & 1-NNA (\%) $\downarrow$ & FPD $\downarrow$ \\ \hline
\multirow{4}{*}{Airplane} & Voxel Baseline & \ \ 6.0 & 28 & 94.1 & 38.9 \\
 & PVD~\cite{zhou20213d} & \ \ 3.4 & 39 & 76.3 & \ \ 5.8 \\
 & DPC~\cite{luo2021diffusion} & \ \ \textbf{3.1} & 46 & 74.7 & 18.7 \\
 & Ours & \ \ 3.4 & \textbf{49} & \textbf{69.3} & \ \ \textbf{3.5} \\ \hline
\multirow{4}{*}{Car} & Voxel Baseline & \ \ 4.9 & 13 & 98.7 & 10.4 \\
 & PVD~\cite{zhou20213d} & \ \ 3.5 & 27 & 76.8 & \ \ 4.5  \\
 & DPC~\cite{luo2021diffusion} & \ \ \textbf{3.3} & 33 & 82.4 & \ \ 7.6 \\
 & Ours & \ \ 3.4 & \textbf{36} & \textbf{73.1} & \ \ \textbf{2.6} \\ \hline
\multirow{4}{*}{Chair} & Voxel Baseline & 11.8 & 28 & 80.6 & 30.6 \\
 & PVD~\cite{zhou20213d} & \ \ 6.8 & 42 & 58.3 & \ \ 3.5 \\
 & DPC~\cite{luo2021diffusion} & \ \  \textbf{6.3} & 44 & 61.4 & 26.0 \\
& Ours & \ \ 7.1 & \textbf{53} & \textbf{54.1} & \ \ \textbf{1.7} \\ \hline
\end{tabular}
\caption{
Quantitative comparison on unconditional 3D shape generation for the airplane, car and chair categories  from ShapeNet~\cite{chang2015shapenet}. 
Our synthesized neural fields outperform the baseline volumetric method and prior state of the art~\cite{zhou20213d,luo2021diffusion}, particularly on the perceptual FPD metric most representative of visual quality.
}
\label{tab:quant_3d_uncond_gen}
\end{table*}

\begin{table}[]
    \centering
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{cccc}
    \hline
    Methods & MMD $\downarrow$ & COV (\%) $\uparrow$ & 1-NNA (\%) $\downarrow$ \\ \hline
    Voxel Baseline & 21.9 & 35 & 85 \\ %\hline
    Ours & \textbf{15.5}  & \textbf{45} & \textbf{62} \\ \hline
    \end{tabular}
    }
    \caption{
    Quantitative evaluation of 4D unconditional generation of animation sequences from temporally deforming 3D shapes. 
    \OURS{} enables a compact, high-fidelity representation space for synthesis, producing much more detailed, high-quality results than voxel-based diffusion.
    }
    \label{tab:quant_4d_uncond_gen}
\end{table}

\begin{table}[]
    \centering
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{c c c c c}
    \hline
    Init. & MMD $\downarrow$ & COV (\%) $\uparrow$ & 1-NNA (\%) $\downarrow$ & FPD $\downarrow$ \\ \hline
    Random & \textbf{3.34} & 49 & 70.4 & 4.01 \\
    1st MLP & 3.45 & \textbf{49} & \textbf{69.3} & \textbf{3.49} \\ \hline
    \end{tabular}
    }
    \caption{Ablation on weight initialization for 3D shape generation (airplanes). 
    Using a consistent initialization from a single optimized MLP leads to moderately improved results, most noticeably in the perceptual FPD metric.
    }
    \label{tab:ablation_init}
\end{table}

\begin{table}[]
    \centering
\resizebox{0.5\textwidth}{!}{
    \begin{tabular}{ccccccc}
    \hline
     & MMD $\downarrow$ & COV (\%) $\uparrow$ & 1-NNA (\%) $\downarrow$ & FPD $\downarrow$ \\ \hline
    No PE & 3.75 & 44 & 76.5 & 6.40 \\
    With PE & \textbf{ 3.45} & \textbf{49} & \textbf{69.3} & \textbf{3.49} \\ \hline
    \end{tabular}
    }
    \caption{Ablation on positional encoding in the MLP for 3D shape generation (airplanes). Without positional encoding (No PE), performance noticeably degrades.
    }
    \label{tab:ablation_pe}
\end{table}

\begin{figure}
    \newcommand\x{0.49}
    \centering
    \vspace{0.3cm}
    \includegraphics[width=0.86\linewidth]{images/results_4d.pdf}
    \caption{
    Qualitative comparison of 4D animation synthesis. 
    Our neural field synthesis generates not only more detailed animations but also achieves smoother temporal consistency. We also refer to our video for the animated shape results.
    }
    \label{fig:gen_animations}
\end{figure}

\begin{figure}[htb!]
    \centering
    \vspace{0.3cm}
    \includegraphics[width=\linewidth]{images/novels.png}
    \caption{Novel shape generation vs nearest neighbor retrieval. For generated shapes (red) from our method, we look up the top-3 nearest neighbors (green) from the training set based on the Chamfer distance. As shown, our method does not simply memorize train samples and can generalize to novel shapes.}
    \label{fig:novel}
\end{figure}


\subsection{Unconditional 4D Generation}

As \OURS{} can model neural fields that represent arbitrary dimension data, we exploit the compact representation space of neural fields to model 4D sequences of deforming 3D shapes. The fourth dimension here corresponds to time. 
Table~\ref{tab:quant_4d_uncond_gen} and Figure~\ref{fig:gen_animations} show that our approach can generate much higher quality animation sequences with more detailed representations than a voxel-based diffusion, which becomes quickly bound by its quartic growth in dimensionality. In Figure~\ref{fig:gen_animations}, we visualize 4  out of 16 generated frames for both animation sequences. The full animation sequences in our video and website show that we can generate temporally consistent 4D animations corresponding to meaningful actions (\textit{e.g.,} jumping, rotating, and resting). Shape integrity is preserved throughout the generated animation sequence.

\section{Limitations}
While \OURS{} shows a promising approach towards directly generating neural fields, several limitations remain.
For instance, the diffusion process currently operates only on optimized MLP parameters, without knowledge of any surface reconstruction.
Our initial experiments with a naive secondary reconstruction loss did not improve performance, but we believe a more sophisticated formulation to encourage the diffusion process to be aware of the surface being represented would improve its generative modeling capabilities.
Additionally, we operate on datasets of individual MLPs, while neural implicit scene representations for large-scale environments \cite{jiang2020local,peng2020convolutional} typically employ MLPs on a grid for greater spatial capacity.
Extending to modeling multiple MLPs could enable larger-scale scene surface modeling.