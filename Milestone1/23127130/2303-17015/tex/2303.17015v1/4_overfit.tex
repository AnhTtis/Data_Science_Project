\section{Per-Sample MLP Overfitting}
\label{sec:overfit}
In this first step, we optimize MLPs for each data sample from the training set $\left\{S_i, i=1, \dots, N\right\}$ and save their optimized network weights.
Specifically, for a train sample $S_i$, the surface at iso-level $\tau$ is represented as
\begin{align}
\{ &\mathbf{x}\in\mathbb{R}^n,\quad f(\mathbf{x},\theta_i) = \tau \},
\end{align}
where $f$ is an MLP parametrized by $\theta_i \in \mathbb{R}^h$ (\ie, weights and biases in Fig.~\ref{fig:overview}) and $\mathbf{x}$ represents a spatial location. The goal is to minimize the binary cross entropy loss function
\begin{align}
L = \textrm{BCE}(f(\mathbf{x}, \theta_i), o^\textrm{gt}_i(\mathbf{x})),
\end{align}
where $o^\textrm{gt}_i(\mathbf{x})$ is the ground truth occupancy of $\mathbf{x}$ with respect to $S_i$.
Here, we exploit the representation power of neural fields, which can model high-dimensional surfaces with high accuracy.
As these optimized MLPs serve as ground truth for the following diffusion processes, their ability to model high-fidelity shapes is crucial.


Unlike prior methods, we do not require any auto-encoding networks~\cite{bagautdinov2018modeling, bloesch2018codeslam} nor auto-decoding networks (\eg, DeepSDF~\cite{chou2022diffusionsdf}), which typically share the same network parameters across an entire dataset. In contrast, although we use the same MLP architecture for different samples in the training dataset, one set of MLP weights is optimized specifically for each data sample. In other words, there is no parameter sharing, and this per-sample optimization attains the best fidelity possible for the implicit neural field representation.


\paragraph{MLP Architecture and Training}
Our MLP architecture is a standard multi-layer fully-connected network with ReLU activation functions and an input positional encoding~\cite{mildenhall2021nerf}. 
We use 3 hidden layers with 128 neurons each, finally outputting a scalar occupancy value. 
The same MLP architecture is shared across both 3D shape and 4D animation experiments, where each MLP encodes an occupancy field per-train sample. 
This enables dimension-agnostic paradigm for encoding of various data signals, in our case 3D and 4D shapes, where only the positional encoding is adapted for various-dimensional inputs.

To optimize a set of MLP weights and biases for an input 3D shape, we sample points randomly both inside and outside of the 3D surface. 
We normalize all train instances to $[-0.5, 0.5]^3$, and randomly sample $100k$ points within the space. 
To effectively characterize fine-scale surface detail, we further sample $100k$ points near the surface of the mesh. 
Both sets of points are combined and tested for inside/outside occupancy using generalized winding numbers \cite{barill2018fast}; these occupancies are used to supervise the overfitting process. 
We optimize each MLP with a mini-batch size of $2048$ points, trained with a BCE loss for 800 epochs until convergence, which takes $\approx 6$ minutes per shape.

MLP overfitting to 4D shapes is performed analogously to 3D. 
For each temporal frame, we sample $200k$ points and their occupancies, following the 3D shape sampling. 
The sampling process is repeated for each frame of an animation sequence. 
We optimize one set of MLP weights and biases for each animation sequence to represent each 4D shape.


\paragraph{Weight Initialization}
In order to encourage a smooth diffusion process over the set of optimized MLP weights, we guide the MLP optimization process with consistent weight initialization.
That is, we initially optimize one set of MLP weights and biases $\theta_1$ to represent the first train sample $S_1$, and use the optimized weights of $\theta_1$ to initialize optimization of the rest of the MLPs $\{\theta_2,...,\theta_N\}$.