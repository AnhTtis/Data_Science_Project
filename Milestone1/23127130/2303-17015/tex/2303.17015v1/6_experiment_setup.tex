

\section{Results}
\label{sec:results}


\subsection{Datasets}

For 3D shape generation, we use the car, chair, and airplane categories of the ShapeNet~\cite{chang2015shapenet} dataset. The car, chair and airplane categories have 3533, 6778, and 4045 shapes respectively. For the 4D shape generation task, we use 16-frame animal animation sequences from the  DeformingThings4D~\cite{li20214dcomplete} dataset, comprising  1772 sequences. 
For both datasets, we split the data into non-overlapping partitions, including training (80\%), validation (5\%) and testing (15\%) subsets. 



\subsection{Voxel-based Diffusion Baseline}
\label{sec:vox-baseline}
While several existing methods tackle 3D shape generation, unconditional 4D shape generation remains underexplored.
Thus, in addition to existing 3D baselines, we introduce a voxel-based diffusion model as a baseline for both 3D and 4D shape generation.

3D shapes are represented as dense occupancy grids, and a 3D UNet denoising network is applied on the 3D voxel grids.
4D shapes are represented similarly, with each frame of an animation sequence voxelized to a 3D occupancy grid, producing a 4D occupancy grid representing the full sequence.
We use the same 3D UNet as a denoising network to synthesize 4D animation, as a 4D UNet became computationally intractable.

For our experiments, we use a voxel resolution of $24^3$ for 3D shapes and $16\times 24^3$ for 4D shapes (the maximum spatial resolution such that 4D grids could be tractably trained).

\subsection{Evaluation Metrics}

Evaluation of unconditional synthesis of 3D and 4D shapes can be challenging due to lack of direct correspondence to ground truth data.
We thus follow prior works \cite{zeng2022lion, luo2021diffusion, zhou20213d} in evaluating  Minimum Matching Distance (MMD), Coverage (COV), and 1-Nearest-Neighbor Accuracy (1-NNA). For MMD, lower is better; for COV, higher is better; for 1-NNA, 50\% is the optimal.
\begin{align*}
\text{MMD}(S_g, S_r) &= \frac{1}{\vert S_r \vert} \sum_{Y \in S_r} \min_{X \in S_g} D(X, Y), \\
\text{COV}(S_g, S_r) &= \frac{\vert \{ \argmin_{Y \in S_r} D(X, Y) \vert X \in S_g \} \vert}{\vert S_r \vert}, \\
\text{1-NNA}(S_g, S_r) &= \frac{\sum_{X \in S_g} \Indicator[N_X \in S_g] + \sum_{Y \in S_r} \Indicator[N_Y \in S_r] }{\vert S_g \vert + \vert S_r \vert},
\end{align*}
where in the 1-NNA metric $N_X$ is a point cloud that is closest to $X$ in both generated and reference dataset, i.e., 
$$N_X = \argmin_{K \in S_r \cup S_g} D(X, K)$$

We use a Chamfer Distance (CD) distance measure $D(X,Y)$ for computing these metrics in 3D, and report CD values  multiplied by a constant $10^2$. 
To evaluate 4D shapes, we extend to the temporal dimension for $T$ frames:
$$D(X, Y) = \frac{1}{T}\sum_{t=0}^{T - 1} CD(X[t], Y[t]).$$
We note that the MMD metric has been discussed to be unreliable as a measure of generation quality~\cite{zeng2022lion}, and thus also consider perceptual metric for 3D shape generation.
In particular, we follow \cite{zhang20233dshape2vecset} and compute a Frechet Pointnet++ Distance~\cite{qi2017pointnet++} (FPD), analogous to the commonly used Frechet Inception Distance (FID score)~\cite{heusel2017gans} in the image domain. 
FPD instead uses a  3D Pointnet++ network (trained on the ModelNet~\cite{wu20153d}) for feature extraction from generated shapes. For FPD, lower scores are better.

For evaluation, each synthesized and ground truth shape is normalized by its mean and standard deviation on a per-shape basis.
To evaluate point-based measures, we sample 2048 points randomly from all baseline outputs; for our approach and the voxel baseline, points are sampled from the extracted mesh surface, and for point cloud baselines, points are sampled directly from the synthesized outputs. 