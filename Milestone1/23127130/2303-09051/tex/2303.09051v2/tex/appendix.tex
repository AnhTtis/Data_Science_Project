\clearpage
\appendix
\onecolumn

\section{Defenses and Evaluation Configurations}
\label{app:defense_and_evaluation}

\subsection{Attack Configurations}
\label{app:attack_configuration}
We use PGD, BPDA, and AutoAttack for the evaluation on CIFAR-10. PGD uses 200 update iterations, and BPDA and AutoAttack use 100 update iterations. All the attacks use 20 EOT samples. The step size of PGD and BPDA is 0.007. For randomized defenses, such as DiffPure~\citep{Nie2022DiffusionMF}, we use the random version of AutoAttack, and for static defenses, such as SODEF~\citep{kang2021stable} and DISCO~\citep{ho2022disco}, we use the standard version. For diffusion-based purification methods, following the settings in DiffPure, we use a fixed subset of 512 randomly sampled images for all experiments.

\subsection{Diffusion-Based Purification}
\label{app:diffusion_based_purification}
Diffusion-based purification methods follow the algorithm proposed by DiffPure~\citep{Nie2022DiffusionMF}. Diffusion-based purification partialy utilizes the forward and denoising processes. Algorithm~\ref{alg:forward_process} displays the complete forward process of diffusion-based purification. Using the ``notable property" of the forward process~\citep{Ho2020DenoisingDP}, we can sample $\mathbf{x}_{t^*}$ in a single step:
\begin{equation}
    q(\mathbf{x}_{t^*} | \mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_{t^*}; \sqrt{\alpha_{t^*}} \mathbf{x}_{0}, (1 - \alpha_{t^*})\textbf{I}),
\end{equation}
where $\alpha_{t^*} := \prod_{i = 1}^{t^*}(1 - \beta_i)$. 
Algorithm~\ref{alg:denoising_process} displays the complete denoising process of diffusion-based purification. \citet{Song2020DenoisingDI} find that denoising process can be accelerated by a linearly increasing sub-sequence $\{\tau_0, \dotsc, \tau_s\}$ of $[0, \ldots, t^*]$ with $\tau_0 = 0$ and $\tau_s = t^*$ where $s \leq t^*$. Throughout all experiments, we only consider sub-sequences having uniform step size. Given $\sigma_{\tau_i}(\eta) = \eta\sqrt{(1 - \alpha_{\tau_{i-1}})/(1 - \alpha_{\tau_i})}\sqrt{1 - \alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$ for all timesteps, the denoising process is DDPM when $\eta = 1$ and DDIM when $\eta = 0$.

\input{appendix/processes.tex}

\subsection{ADP~\citep{Yoon2021AdversarialPW}}
ADP uses a score-based model trained with denoising score matching:
\begin{equation}
\mathbb{E}_t\mathbb{E}_{q(\mathbf{x}_t|\mathbf{x})p_{\text{data}}(\mathbf{x})} \Bigg[\frac{1}{2}\| \mathbf{s}_\theta(\mathbf{x}_t) - \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t|\mathbf{x}) \|^2 \Bigg],
\end{equation}
where $p_{\text{data}}$ is a data distribution, $t$ is a scale of perturbation, and $\mathbf{s}_\theta$ is a score-based model. To clean an adversarial example, ADP uses a deterministic version of Langevin dynamics:
\begin{equation}
\mathbf{x}_t = \mathbf{x}_{t - 1} + \alpha_{t - 1} \mathbf{s}_\theta(\mathbf{x}_{t - 1}),
\end{equation}
where $\alpha_{t-1}$ is a step size. Before its purification process, ADP adds Gaussian noise to the input, which can improve robustness. In its original implementation, ADP is evaluated on BPDA. However, since ADP uses up to eight steps for purification, calculating the full gradients of the defense is possible. Therefore, we use the full gradients to generate adversarial examples in our evaluation.


\subsection{DiffPure~\citep{Nie2022DiffusionMF}}
Given a forward diffusion process $\mathbf{x}(t)_{t\in[0, 1]}$, DiffPure uses $t^* = 0.1$ and $t^* = 0.075$ on CIFAR-10 against threat models $\ell_\infty (\epsilon = 8/255)$ and $\ell_2 (\epsilon = 0.5)$, respectively (with step size 0.001). Since it is impossible to calculate the gradients using back-propagation, in the original evaluation, DiffPure uses an adjoint method of its underlying numerical SDE (ODE) solver for calculating gradients. In our evaluation, we use gradients of a surrogate process calculated by direct back-propagation. To overcome memory constraints, we increase the step size of the surrogate denoising process in attack to 0.005.


\subsection{GDMP~\citep{Wang2022GuidedDM}}
The basic approach of GDMP is the same as DiffPure~\citep{Nie2022DiffusionMF}, however, it uses two additional techniques: guidance and multiple purification steps. GDMP proposes to use gradients of a distance between an initial input and a target being processed to preserve semantic information:
\begin{equation}
    \mathbf{x}_{t-1} \sim \mathcal{N}(\boldsymbol{\mu}_\theta - s\boldsymbol{\Sigma}_{\theta} \nabla_{\mathbf{x}_{t}} \mathcal{D}(\mathbf{x}_{t}, \mathbf{x}_{\text{adv}, t}), \boldsymbol{\Sigma}_{\theta}),
\end{equation}
where $\boldsymbol{\mu}_\theta$ and $\boldsymbol{\Sigma}_\theta$ are the mean and variance of $\mathbf{x}_{t-1}$ calculated by diffusion models $\boldsymbol{\epsilon}_\theta$, $s$ is a scale of guidance, $\mathbf{x}_{t}$ is a sample that is being purified, and $\mathbf{x}_{\text{adv}, t}$ is a noisy adversarial example. In addition, GDMP finds that iteratively applying the purification process, which we call the purification step, can improve the robustness. GDMP consists of four purification steps, each consisting of 36 forward steps and 36 denoising steps. In our evaluation, we use a surrogate process calculated by direct back-propagation, while GDMP is originally evaluated on BPDA. Since it is impossible to calculate the gradients of the full defense process, we use a surrogate process consisting of four purification steps (each consisting of 36 forward steps and six denoising steps) in the attack.


\subsection{SODEF and DISCO~\citep{kang2021stable, ho2022disco}}
SODEF~\citep{kang2021stable} uses an ODE block that satisfies Lyapunov stability. Lyapunov-stable equilibrium point has a property that its neighborhood gathers to that point by passing through the ODE block. In the original implementation, SODEF uses an adjoint method to calculate its gradients of the ODE block. In our experiment, we use back-propagation instead of the adjoint method.

DISCO~\citep{ho2022disco} employs a local implicit module to restore a clean example. For every pixel, the module estimates the original RGB values of input before sending it to the classifier. While DISCO is evaluated on BPDA in the original evaluation, we use the full gradients of the defense process. To evaluate both SODEF and DISCO, we use the standard version of AutoAttack.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Additional Results on Evaluation for Diffusion-Based Purifications}
\label{app:evaluation}

\subsection{Additional Results on RQ1}
We show the difference between attack success rate when using the adjoint method and back-propagation. \autoref{table:RQ1_extra} shows the robust accuracy of DiffPure~\citep{Nie2022DiffusionMF} and its probability flow ODE~\citep{Song2020ScoreBasedGM} against PGD+EOT $\ell_\infty (\epsilon = 8/255)$ on CIFAR-10. For both DiffPure and its probability flow ODE, the back-propagation can generate adversarial examples more successfully than the adjoint method. For the probability flow ODE, the back-propagation with step size 0.01 has 34.51\% lower robust accuracy than the adjoint method.

\input{appendix/tables/RQ1_additional_result}   % Table 8


\subsection{Additional Results on RQ2}

\autoref{fig:AA_vs_PGD} compares the attack performance of PGD and AutoAttack $\ell_\infty (\epsilon = 8/255)$ on CIFAR-10. This result is conducted on the same settings with \autoref{sec:hyperparameter}. As shown in the \autoref{fig:AA_vs_PGD}, PGD generally has a larger attack success rate than AutoAttack. As the number of forward steps increases, the gap between the two attacks also increases. Therefore, PGD+EOT is a more appropriate attack than AutoAttack for evaluating diffusion-based purification methods.

\input{appendix/figures/AutoAttack_vs_PGD}      % Figure 6


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Additional Results on Analysis of Hyperparameters}
\label{app:hyperparameter}

In this section, we provide further analyses of hyperparameters on CIFAR-10 and ImageNet. In all experiments, the implementation of diffusion-based purification follows noise scheduling, the forward process, and the denoising process of Appendix~\ref{app:diffusion_based_purification}.


\subsection{CIFAR-10}

We provide additional results regarding denoising steps and purification steps on CIFAR-10 through \autoref{fig:sampling_steps_t200}, \autoref{fig:defense_num_iterations_t200}, and \autoref{fig:attack_num_iterations_t200}. We conduct all experiments in a setting identical to \autoref{sec:denoising_steps}, except that the number of forward steps is 200 (i.e., $t^* = 200$). Furthermore, \autoref{fig:iterations_attack} shows the overall influence of the number of purification steps in attack on the attack success rate.

\input{appendix/figures/sampling_steps_t200}    % Figure 7
\input{appendix/figures/num_iterations_t200}    % Figure 8, 9
\input{appendix/figures/iterations_attack}      % Figure 10


\subsection{ImageNet}

We provide additional results regarding forward, denoising, and purification steps on ImageNet through \autoref{fig:imagenet_forward_steps}, \autoref{fig:imagenet_sampling_steps_t200}, and \autoref{table:imagenet_iterations}, respectively. In \autoref{fig:imagenet_sampling_steps_t200} and \autoref{table:imagenet_iterations}, the number of forward steps is set to 200. For ImageNet, we use 20 PGD iterations and 20 EOT samples. And due to memory constraints, the upper bound on the number of function calls is set to ten. Although we employ the experiments only on DDPM, the results are similar to those from CIFAR-10.

\input{appendix/figures/imagenet_forward_steps}     % Figure 11
\input{appendix/figures/imagenet_sampling_steps}    % Figure 12
\input{appendix/tables/imagenet_iterations}         % Table 9


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Surrogate Process for Gradual Noise-Scheduling}
\label{app:gradual_sampling}
The defense process of the gradual noise-scheduling have three levels in the number of forward steps. To validate the robustness of our defense, we evaluate the gradual noise-scheduling with several surrogate processes and report the lowest robust accuracy among them. We denote one purification step as (\# of forward steps, \# of denoising steps). For example, suppose a process uses two purification steps, consisting of 100 forward steps with 20 denoising steps and 120 forward steps with ten denoising steps, respectively. In that case, we denote the defense process as (100, 20), (120, 10). As shown in \autoref{table:attack_processes}, the third surrogate process has the lowest robust accuracy against the gradual noise-scheduling on CIFAR-10. For all experiments of our defense on all datasets, we select the third process as the surrogate process in attacks of the adaptive white-box setting.

\input{appendix/tables/best_attack_processes}   % Table 11


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Memory and Time Requirements for Diffusion-Based Purification}

Evaluating diffusion-based purification requires lots of memory since we use direct back-propagation. \autoref{fig:memory_and_time} briefly shows the memory and time requirements of one PGD iteration for implementing diffusion-based purification methods. We use one A100 GPU. For example, we need almost ten days to evaluate one experiment with 30 function calls for calculating back-propagation with one A100 GPU (against a PGD attack using 200 iterations and 20 EOT samples).

\input{appendix/figures/memory_and_time}

