\section{Related Work}
\label{Related Work}

Adversarial training~\citep{Madry2017TowardsDL,zhang2019theoretically} is one of the most successful adversarial defense methods. These methods train a classifier with adversarial examples in a training phase. \citet{zhang2019theoretically} and \citet{Pang2022RobustnessAA} propose loss functions that can effectively utilize the trade-off between robustness and accuracy. \citet{Huang2021ExploringAI} analyze architectural factors with respect to robustness. \citet{Rebuffi2021DataAC} and \citet{Gowal2021ImprovingRU} improve robustness by utilizing data augmentations.

Adaptive test-time defenses purify adversarial examples using extra neural networks that utilize techniques from other domains. ADP~\citep{Yoon2021AdversarialPW} jointly uses denoising score matching and Langevin dynamics for purification. DiffPure~\citep{Nie2022DiffusionMF} demonstrates from the stochastic differential equations (SDE) perspective that diffusion models can purify adversarial examples. GDMP~\citep{Wang2022GuidedDM} uses the guidance of diffusion models to recover adversarial examples as similar as possible to the original examples. SODEF~\citep{kang2021stable} uses a Lyapunov-stable ODE block so that the input converges to a stable point that can be correctly classified. DISCO~\citep{ho2022disco} is one of the denoising models that predict clean RGB value using local implicit functions.
