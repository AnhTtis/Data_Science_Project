%\input{tables/evaluation_prior_defense.tex}
\section{Evaluation for Diffusion-Based Purification}
\label{sec:evaluation}
In this section, we first review the current practices in evaluating diffusion-based purification methods. We then curate three research questions to address their potential limitations and provide our answers to these questions through empirical evaluations.

\subsection{Current Practices and Research Questions}
Evaluation of the diffusion-based purification against gradient-based white-box attacks is non-trivial due to many function calls on the denoising process. Multiple function calls in the denoising step often require an impractical amount of memory, making it unfeasible to compute the gradient of the full defense process. Because of this problem, most defenses~\citep{Yoon2021AdversarialPW, Wang2022GuidedDM, ho2022disco} consider BPDA the strongest adaptive white-box attack in the current practice since it does not rely on the gradients of defense methods. However, the vulnerability of diffusion-based purification on white-box attacks has yet to be fully identified. The importance of testing in adaptive white-box attacks of purification has been recognized only recently by the work of DiffPure~\citep{Nie2022DiffusionMF}.

DiffPure calculates the full gradients of their defense process using an adjoint method. The adjoint method is employed to avoid the extensive use of memory while obtaining the full gradient. DiffPure is evaluated on AutoAttack, a de facto evaluation method in adversarial training. Although their evaluation framework is more robust than the previous work, the design choices of their evaluation still raise questions since 1) the adjoint method relies on the performance of an underlying {numerical} solver~\citep{Zhuang2020AdaptiveCA}, and 2) there is no comprehensive comparison between different attacks using the full gradient.

Based on our observation, we carefully curate the following three research questions to address the robustness of the current evaluation framework in diffusion-based purification:
\begin{itemize}
    \item \textbf{RQ1.} Is the adjoint method the best way to generate adversarial examples with full gradients? Is there any alternative to the adjoint method?
    \item \textbf{RQ2.} Is AutoAttack still better than the other gradient-based attacks, such as PGD, when the alternative is available?
    \item \textbf{RQ3.} Is BPDA still more effective than the best combination of full-gradient attacks?
\end{itemize}
In the next section, we re-evaluate the existing purification methods to answer these questions.

\subsection{Experimental Results \& Analysis}
\label{sec:results_and_analysis}

We evaluate the performance of three diffusion-based purification methods: ADP\footnote{Although ADP uses a score-based model and Langevin dynamics, since the concept is similar to the diffusion model, we consider ADP diffusion-based purification.}~\citep{Yoon2021AdversarialPW}, DiffPure~\citep{Nie2022DiffusionMF}, GDMP~\citep{Wang2022GuidedDM}. We additionally evaluate two non-diffusion-based adaptive test-time defenses: SODEF~\citep{kang2021stable}, and DISCO~\citep{ho2022disco} to address whether our findings still hold for the non-diffusion-based purification methods.
We evaluate their robustness on CIFAR-10 against three gradient-based attacks, including PGD, BPDA, and AutoAttack, with a maximum attack strength of $\ell_\infty (\epsilon = 8/255)$. 
A comprehensive description of evaluation configurations is provided in \autoref{app:defense_and_evaluation}.

\paragraph{Surrogate process and its gradient.}
The adjoint method can compute the exact gradient in theory, but in practice, the adjoint relies on the performance of the numerical solver, whose performance becomes problematic in some cases as reported by \citet{Zhuang2020AdaptiveCA}.
To answer the RQ1, we compare the adjoint method against the full gradient obtained from back-propagation if possible, and if not due to the memory issue, we use the {approximated} gradient obtained from a \emph{surrogate process}.
The surrogate process utilizes the fact that given the total amount of noise, we can denoise the same amount of noise with different numbers of denoising steps~\cite{Song2020DenoisingDI}. Therefore, instead of using the entire denoising steps, we can mimic the original denoising process with fewer function calls, whose gradients can be obtained by back-propagating the forward and denosing process directly.

The gradients obtained from the surrogate process differ from the exact gradients. However, if the accumulated denoising steps can be approximated with fewer denoising steps, we can use the approximated gradients as a proxy of the exact gradients. The surrogate process can also relax the randomness occurring in multiple denoising steps.

\input{tables/RQ1}
\paragraph{RQ1: Is the adjoint method the best way to generate adversarial examples with full gradients?} 
We compare the adjoint method with the full gradient obtained from direct back-propagation of the defense process with the original or surrogate processes.

\autoref{table:RQ1} shows that the robust accuracy of DiffPure \citep{Nie2022DiffusionMF} with the direct back-propagation is 46.84\% on PGD+EOT attack, which is 27.54\% lower than the reported accuracy with the adjoint method. The results show that direct back-propagation is more effective than the adjoint method. Furthermore, we use a surrogate process for GDMP~\citep{Wang2022GuidedDM}, and the robust accuracy is 24.06\%, which is 51.53\% lower than the reported accuracy against the BPDA attack. It can be concluded that, in cases where the gradients of the defense process are unavailable to calculate, the surrogate process can be an alternative to generate adversarial examples.

SODEF \citep{kang2021stable}, a non-diffusion-based purification, originally uses the adjoint method to generate adversarial examples. We evaluate SODEF with direct back-propagation for the attack and observe 49.28\% robust accuracy against AutoAttack, which is lower than 53.69\% of the underlying model without defense. This result suggests that the use of a numerical solver would not be effective for the adversarial attack.

\input{tables/RQ2}
\paragraph{RQ2: Is AutoAttack still better than the other gradient-based attacks, such as PGD, when the alternative is available?}
AutoAttack has recently been used as a standard method to evaluate defenses due to its robustness against defenses. Although AutoAttack may not be an ideal choice for randomized defenses\footnote{\url{https://github.com/fra31/auto-attack/blob/master/flags_doc.md}}, still many purification methods, such as DiffPure, rely on AutoAttack. However, as shown in \autoref{table:RQ2}, AutoAttack has a lower success rate than PGD+EOT against diffusion-based purification methods. For the $\ell_\infty$ threat model $(\epsilon = 8/255)$, PGD+EOT shows 16.76\% and 26.05\% more attack success rate than AutoAttack against DiffPure and ADP, respectively. We observe a similar result with the $\ell_2$ threat model $(\epsilon = 0.5)$. Therefore, evaluation with PGD+EOT for diffusion-based purification can be useful to evaluate their robustness. Further results of the difference between PGD+EOT against DiffPure with additional settings can be found in \autoref{app:evaluation}.

\input{tables/RQ3}
\paragraph{RQ3: Is BPDA still more effective than the best combination of full-gradient attacks?}
BPDA~\citep{obfuscated-gradients} has been widely used to evaluate defenses that can cause gradient obfuscation. Because multiple function calls can cause gradient obfuscation, ADP, GDMP, and DISCO have been evaluated on BPDA as the strongest adaptive white-box attack. However, our evaluation shows that BPDA has a lower attack success rate than the attacks using direct gradients of the defense process, as shown in \autoref{table:RQ3}. Against PGD+EOT using direct gradients of defense process, ADP and GDMP show robust accuracy of 33.48\% and 24.06\%, respectively, significantly lower than the reported accuracy with BPDA~\citep{Yoon2021AdversarialPW, Wang2022GuidedDM}. DISCO even has 0\% robust accuracy. From the results, we suggest that the direct gradients of the defense process need to be tested to check the robustness.

\paragraph{Recommendation.}
We propose an overall guideline for evaluating diffusion-based purifications as follows. We recommend using PGD+EOT rather than AutoAttack. When calculating gradients, it is best to directly back-propagate the full defense process. If this is unavailable due to memory constraints, using the surrogate process rather than the adjoint method is recommended. %BPDA can be used to check whether gradient obfuscation exists or not.
Note that our recommendation generally follows the suggestions made by \citet{croce2022evaluating} but is more tailored for the diffusion-based purification.
