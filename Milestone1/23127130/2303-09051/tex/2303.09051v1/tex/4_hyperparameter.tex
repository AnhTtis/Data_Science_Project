\section{Analysis of Hyperparameters}
\label{sec:hyperparameter}

The performance of diffusion-based purification is significantly influenced by varying hyperparameter configurations. In this section, we explore the importance of hyperparameters in defense processes.

\subsection{Experimental Settings}
\label{sec:param_settings}

Understanding the importance of hyperparameters can help build a better defense mechanism. We investigate the effect of various hyperparameters of diffusion-based purification methods to determine the most robust configuration for the adaptive attack. 
Specifically, the following three hyperparameters are evaluated 1) the number of forward steps, 2) the number of denoising steps, and 3) the number of purification steps. In addition, we re-evaluate the efficiency of several techniques proposed in previous works under our defense scheme.

We evaluate the purification against PGD+EOT on CIFAR-10. We provide the additional results on CIFAR-10 and ImageNet in \autoref{app:hyperparameter}. Although we do not report ImageNet results in the main text, the overall findings are similar to those from CIFAR-10. Following the settings in DiffPure~\cite{Nie2022DiffusionMF}, we use a fixed subset of 512 randomly sampled images. We use a naturally pretrained WideResNet-28-10~\citep{Zagoruyko2016WideRN} as an underlying classifier provided by Robustbench~\citep{croce2021robustbench}. For diffusion models, we use pretrained DDPM++~\citep{Song2020ScoreBasedGM}. The variances for the diffusion model are set to constants that are linearly increasing from $\beta_1 = 10^{-4}$ to $\beta_T = 0.02$. We use two different denoising models: DDPM and DDIM. 
%The DDPM uses a Markovian diffusion process, whereas the DDIM uses a non-Markovian process. 
For all experiments, we report the mean and standard deviation over five runs to measure the standard and robust accuracy. 
PGD uses 20 update iterations. 20 samples are used to compute EOT. To calculate gradients, we use direct gradients of the entire process. If impossible, we compute the approximated gradients from a surrogate process. In each experiment, we explain the defense process and the surrogate process in more detail.

\input{figures/forward_steps.tex}
\subsection{The Number of Forward Steps}
\label{sec:forward_steps}
We explore the effect of forward noising steps on robustness by varying the number of forward steps from 30 to 300, resulting in the changes of total variance ranged from 0.012 to 0.606. The same number of forward steps are used for both attack and defense, and we set five denoising steps for attack and defense for all experiments.

As shown in \autoref{fig:forward_steps}, the standard accuracy continuously decreases as the number of forward steps increases since more forward steps induce more noise.
The robust accuracy increases first and decreases after 200 forward steps, i.e., $t^* = 200$. When the number of forward steps is small, the DDPM is more robust than the DDIM. However, DDIM shows better accuracy for both standard and robust than DDPM after 200 forward steps.

\input{figures/sampling_steps_t100.tex}

\subsection{The Number of Denoising Steps}
\label{sec:denoising_steps}
Defenders may use fewer denoising steps to accelerate the defense process. From the other perspective, attackers may want to use fewer denoising steps than those used in the defense due to memory constraints. We explore the influence of the number of denoising steps  through the following three experimental settings:
\begin{itemize}
    \item[(a)] The number of denoising steps in attack is set to five, and the number of denoising steps in defense is ranged from one to the maximum number of denoising steps.
    \item[(b)] The number of denoising steps in both the attack and defense are the same, ranging from one to 20.
    \item[(c)] The number of denoising steps in defense is set to the maximum number of denoising steps, and the number of denoising steps in attack is ranged from one to 20.\footnote{The 20 denoising steps is the maximum limit of 40GB of memory.}
\end{itemize}

The results are displayed in \autoref{fig:sampling_steps_t100}. %We check the effect of the number of denoising steps in defense through the results of (a) and (b), respectively. We also check the robustness in an adaptive white-box setting, where an attacker knows the exact number of denoising steps through (c).
From the defense perspective, the results of (a) and (b) demonstrate that more denoising steps can improve robustness. DDPM gains more advantage from having more denoising steps than DDIM. %Note that the benefit comes at a cost. We need more processing time as we increase the denoising steps. %We observe a trade-off between robustness and processing time, as additional denoising steps require more processing time.
(c) shows the effect of the number of denoising steps in the attack. As the number of denoising steps increases, the attack success rate slightly increases. However, we also find that increasing the number of denoising steps in an attack can decrease the attack success rate when the number of forward steps is 200 (i.e., $t^* = 200$). %Additional results on CIFAR-10 and ImageNet are provided in \autoref{app:hyperparameter}.


\input{figures/defense_num_iterations_t100.tex}
\input{figures/attack_num_iterations_t100.tex}
\subsection{The Number of Purification Steps}
\label{sec:purification_steps}
Although a single forward and reverse process can purify the input image, one can apply the purification process multiple times as proposed in \citet{Wang2022GuidedDM}. We denote the number of forward and denoising processes as the number of \textit{purification step}. Similar to the case of the denoising step, computing the gradients of multiple purification steps is impossible due to memory constraints in most cases. 

The number of purification steps can also differ between attack and defense. Through experiments, we measure the changes in robust accuracy with the different number of purification steps in the defense and attack.
For all experiments, we fixed the number of forward steps to 100 ($t^* = 100$), and the number of denoising steps is set to five.

\autoref{fig:defense_num_iterations_t100} shows the standard and robust accuracy with a varying number of purification steps in defense. The robust accuracy increases as the number of purification steps increases while the standard accuracy steadily decreases. 
\autoref{fig:attack_num_iterations_t100} shows the effect of the number of purification steps in the attack. When the number of purification steps in defense is one or two, the same number of purification steps in attack is the most effective. However, as we set the number of purification steps in defense to three and five, two and three purification steps in attack show a better attack success, respectively. % The overall finding on ImageNet is also similar to those from CIFAR-10. %We provide additional results of other settings on CIFAR-10 and ImageNet in \autoref{app:hyperparameter}.


\subsection{Other Techniques}
\label{sec:other_tech}

We evaluate several other techniques proposed in earlier work~\citep{Yoon2021AdversarialPW, Nie2022DiffusionMF, Wang2022GuidedDM} within our new evaluation framework.

\input{tables/guidance_ablation.tex}

\textbf{Guidance.}
GDMP~\citep{Wang2022GuidedDM} proposes to use gradients of a distance between an original input example and a target example to preserve semantic information while denoising. %:
% \begin{equation}
%     \mathbf{x}_{t-1} \sim \mathcal{N}(\mu_\theta - s\Sigma_{\theta} \nabla_{\mathbf{x}_{t}} \mathcal{D}(\mathbf{x}_{t}, \mathbf{x}_{\text{adv}, t}), \Sigma_{\theta})
% \end{equation}
% where $\mu_\theta$ and $\sigma_\theta$ are the mean and variance of $\mathbf{x}_{t-1}$ calculated by diffusion models $\epsilon_\theta$, $s$ is a scale of guidance, $\mathbf{x}_{t}$ is a sample that is being purified, and $\mathbf{x}_{\text{adv}, t}$ is a noisy adversarial examples.
As shown in \autoref{table:guidance_evaluation}, when the gradients of the surrogate process are used in the attack, the guidance of GDMP decreases the robust accuracy. Specifically, the defense with guidance using the SSIM similarity has 10.98\% robust accuracy, which is 32.97\% lower than the defense without guidance.

\input{figures/ensemble.tex}

\textbf{Ensemble of multiple purification runs.}
ADP~\citep{Yoon2021AdversarialPW} uses the ensemble of multiple purification runs as the predicted label to mitigate the randomness in the defense. For diffusion-based purification methods, as shown in \autoref{fig:ensemble}, multiple purification runs especially can help improve standard accuracy while the robust accuracy keeps the same level. In particular, for $t^*=200$ with 40 purification runs, standard accuracy is 8.17\% higher than the case without ensemble.

\input{tables/combination_with_adv_training.tex}

\textbf{Combination with adversarial training.}
As in DiffPure~\citep{Nie2022DiffusionMF}, the diffusion models can be used together with adversarial training. However, as shown in \autoref{table:combi_with_adv}, the adversarial training with purification shows lower robustness than the classifier without purification.




% \subsection{Discussion}

% Our study analyzes how the hyperparameters of diffusion-based purification methods affect robustness. Some parameter values or techniques, such as increasing denoising steps or ensemble of purification runs, can improve robustness. Nonetheless, the optimal hyperparameter values can be changed by various factors such as datasets, the performance of diffusion models, or other hyperparameters. In particular, incorrect choices of hyperparameter values in surrogate process can influence evaluation, leading to an overestimation of the defense performance. Therefore, for future diffusion-based purification methods, we suggest searching for appropriate hyperparameter values to verify or improve their robustness.
