\section{Introduction}
\label{Introduction}

% Adversarial Attack & Adversarial Training & Adaptive test-time defense
Adversarial attacks~\citep{Madry2017TowardsDL, Croce2020ReliableEO} can cause deep neural networks (DNNs) to produce incorrect outputs by adding imperceptible perturbations to inputs. While various adversarial defenses have been proposed, adversarial training \cite{zhang2019theoretically, Gowal2021ImprovingRU} has shown promising results in building robust DNNs. Since adversarial training feeds the model both normal and adversarial examples during training time, one needs to pre-determine which attack method is used to generate the adversarial examples. On the other hand, adaptive test-time defense~\citep{kang2021stable, ho2022disco} has recently gained increasing attention since it adaptively removes the adversarial effect at test time without adversarial training. \textit{Adversarial purification}~\citep{Samangouei2018DefenseGANPC, Nie2022DiffusionMF}, one of the adaptive test-time defenses, uses generative models to restore the clean examples from the adversarial examples.

% Diffusion models Diffusion-based purification
Diffusion-based generative models~\citep{Ho2020DenoisingDP, Song2020ScoreBasedGM} has been suggested as a potential solution for adversarial purification~\citep{Nie2022DiffusionMF, Wang2022GuidedDM}. Diffusion models learn transformations from data distributions to well-known simple distributions such as the Gaussian and vice versa through forward and reverse processes, respectively. When applied to the purification, the forward process gradually adds noise to the input, and the reverse process gradually removes the noises to uncover the original image without imperceptible adversarial noise.
With a theoretical guarantee, the recent success of DiffPure~\citep{Nie2022DiffusionMF} against many adversarial training methods shows the potential of using diffusion processes for improving the robustness against adversarial attacks.

Evaluating the robustness of adaptive test-time defenses is, however, known to be difficult due to their complex defense algorithms and properties. 
\citet{croce2022evaluating} shows that finding worse-case perturbation is important to measure the robustness of the defenses.
Randomness and iterative calls of adaptive test-time defenses, however, make their gradients obfuscated. Therefore, the gradient-based attack methods~\citep{Madry2017TowardsDL, Croce2020ReliableEO} might be inappropriate for measuring the robustness under such obfuscation. 
New algorithms, such as Backward Pass Differentiable Approximation (BPDA)~\citep{obfuscated-gradients}, and additional recommendations~\citep{croce2022evaluating} have been proposed to evaluate their robustness accurately. However, it is unclear whether these algorithms and recommendations can still be used to evaluate diffusion-based purification.

In the first part of this work, we analyze the existing evaluation methods for diffusion-based purification.
We find that the adjoint method, often used to compute a full gradient of the iterative process, relies on the performance of an underlying numerical solver. Tailored to the diffusion models, we propose a surrogate process, an alternative method to approximate the gradient from the iterative procedure and show the strong robustness in recent work can be weaker than claimed with the surrogate process. We then compare two gradient-based attack methods, AutoAttack~\citep{Croce2020ReliableEO} and PGD~\citep{Madry2017TowardsDL}, with the surrogate process and find that PGD is more effective for the diffusion-based purification. To this end, we propose a practical recommendation to evaluate the robustness of diffusion-based purification.

% Current Problem #2. hyperparameters of diffusion-based purifications
In the second part of this work, we analyze the importance of the hyperparameter for successive defenses with purification. 
%Given many hyperparameters, it is hard to know which is important for robustness. 
The diffusion models are trained without adversarial examples. Thus, proper validation of hyperparameters is impossible in general.
%Misunderstanding hyperparameters may even interfere with proper evaluations due to the stochasticity and many iterations of diffusion-based purification. 
Instead, we empirically analyze the influence of different hyperparameter selections from the attacker's and defender's perspectives. 
Based on our analysis, we propose a gradual noise-scheduling for multi-step purification. 
%We empirically verify our strategy against various attacks, including the PGD attack, with an adaptive white-box setting. 
We show that the robustness of our defense strategy is comparable to the state-of-the-art adversarial training methods under our proposed evaluation scheme. 

We summarize our contributions as follows:
\begin{itemize}
    \item We analyze the current evaluation of diffusion-based purification and provide a recommendation for robust evaluation.
    \item We investigate the influence of hyperparameters on the robustness of diffusion-based purification.
    \item We propose a gradual noise-scheduling strategy for diffusion-based purification, showing comparable results against the adversarial training approaches.
\end{itemize}

% (our findings can be converted into research questions in the introduction)