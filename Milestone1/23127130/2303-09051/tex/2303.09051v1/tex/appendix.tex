\clearpage
\appendix
\onecolumn

\section{Defenses and Evaluation Configurations}
\label{app:defense_and_evaluation}
In this section, we provide a detailed explanation of noise scheduling, the forward process, and the denoising process of diffusion-based purification. We also summarize five adaptive test-time defenses and evaluation configurations for each defense.

\subsection{Noise Scheduling and Two Processes of Diffusion-Based Purification}
\label{app:noise_scheduling}
The forward process of diffusion models gradually adds Gaussian noise over T steps to the input sampled from a data distribution $q(\mathbf{x}_0)$:
\begin{equation}
    q(\mathbf{x}_{t} | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_{t}; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\textbf{I}), \quad \text{where} \quad q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1})
\end{equation}
where $\beta_1, \dotsc, \beta_T$ are the variance schedule. We set the total number of timesteps $T$ to 1000 for all experiments, and the variances are set to constants that are linearly increasing from $\beta_1 = 10^{-4}$ to $\beta_T = 0.02$.

Diffusion-based purification uses the forward and denoising process. Algorithm~\ref{alg:forward_process} displays the complete forward process of diffusion-based purification. The forward process adds Gaussian noise to the input $\mathbf{x}_0$ until the timestep becomes $t^*$. Using the notable property of the forward process~\citep{Ho2020DenoisingDP}, we can easily sample $\mathbf{x}_{t^*}$ in a single step:
\begin{equation}
    q(\mathbf{x}_{t^*} | \mathbf{x}_{0}) = \mathcal{N}(\mathbf{x}_{t^*}; \sqrt{\alpha_{t^*}} \mathbf{x}_{0}, (1 - \alpha_{t^*})\textbf{I})
\end{equation}
where $\alpha_{t^*} := \prod_{i = 1}^{t^*}(1 - \beta_i)$. 
Algorithm~\ref{alg:denoising_process} displays the complete denoising process of diffusion-based purification. The denoising process is aimed at purifying the perturbed image $\mathbf{x}_{t^*}$. To accelerate the denoising process, one can use a linearly increasing sub-sequence $\{\tau_0, \dotsc, \tau_s\}$ of $[0, \ldots, t^*]$ with $\tau_0 = 0$ and $\tau_s = t^*$ where $s \leq t^*$. Throughout all experiments, we only consider sub-sequences having uniform step size. Given $\sigma_{\tau_i}(\eta) = \eta\sqrt{(1 - \alpha_{\tau_{i-1}})/(1 - \alpha_{\tau_i})}\sqrt{1 - \alpha_{\tau_i}/\alpha_{\tau_{i-1}}}$ for all timesteps, the denoising process is DDPM when $\eta = 1$ and DDIM when $\eta = 0$.

\input{appendix/processes.tex}

\subsection{ADP~\citep{Yoon2021AdversarialPW}}
ADP uses a score-based model trained with denoising score matching. The neural network learns a score function of Gaussian-perturbed data:
\begin{equation}
\mathbb{E}_{q(\tilde{\mathbf{x}}|\mathbf{x})p_{\text{data}}(\mathbf{x})} \Bigg[\frac{1}{2}\| \mathbf{s}_\theta(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q(\tilde{\mathbf{x}}|\mathbf{x}) \|^2 \Bigg]
\end{equation}
where $p_{\text{data}}$ is a target density, and $\mathbf{s}_\theta$ is a neural network that approximates a real score function. To recover adversarially perturbed examples, ADP uses a deterministic version of Langevin dynamics:
\begin{equation}
\mathbf{x}_t = \mathbf{x}_{t - 1} + \alpha_{t - 1} \mathbf{s}_\theta(\mathbf{x}_{t - 1})
\end{equation}
where $\alpha_{t-1}$ is a step size. In addition, ADP injects Gaussian noise into an image before the purification process to improve robustness.

In the original evaluation, ADP is evaluated on BPDA. However, since ADP uses up to eight steps for purification, calculating the full gradients of the defense is possible. Therefore, we use the full gradients to generate adversarial examples in our evaluation. We use both PGD+EOT and AutoAttack to compare the attack success rate.


\subsection{DiffPure~\citep{Nie2022DiffusionMF}}
DiffPure proves that a forward diffusion process $\mathbf{x}(t)_{t\in[0, 1]}$ can reduce the KL divergence between clean data distribution and adversarial data distribution. Using this property, DiffPure first adds noise to the adversarial examples through the forward process from $t=0$ to $t=t^*$. Since the noisy examples cannot be directly classified, it recovers clean examples via the reverse denoising process of diffusion models. To calculate the gradients of the full defense process, DiffPure uses an adjoint method of its underlying numerical solver. 

In the original implementation against a threat model $\ell_\infty (\epsilon = 8/255)$ on CIFAR-10, DiffPure set the timestep $t^*$ in the forward process to 0.1, and the step size of the denoising process is set to 0.001. In our evaluation, we use gradients of the direct back-propagation with a surrogate process. To circumvent memory constraints, we increase the step size of the surrogate denoising process in attack to 0.005. We use both PGD+EOT and AutoAttack to compare the attack success rate.


\subsection{GDMP~\citep{Wang2022GuidedDM}}
The basic approach of GDMP is the same as DiffPure~\citep{Nie2022DiffusionMF}, but it uses two additional techniques: guidance and multiple purification processes. GDMP proposes to use gradients of a distance between an original input example and a target example to preserve semantic information while denoising:
\begin{equation}
    \mathbf{x}_{t-1} \sim \mathcal{N}(\mu_\theta - s\Sigma_{\theta} \nabla_{\mathbf{x}_{t}} \mathcal{D}(\mathbf{x}_{t}, \mathbf{x}_{\text{adv}, t}), \Sigma_{\theta})
\end{equation}
where $\mu_\theta$ and $\sigma_\theta$ are the mean and variance of $\mathbf{x}_{t-1}$ calculated by diffusion models $\epsilon_\theta$, $s$ is a scale of guidance, $\mathbf{x}_{t}$ is a sample that is being purified, and $\mathbf{x}_{\text{adv}, t}$ is a noisy adversarial example. In addition, GDMP finds that iteratively applying the purification process can improve the robustness. 

GDMP consists of four purification processes, each consisting of 36 forward steps and 36 denoising steps, with the same noise scheduling as Appendix~\ref{app:noise_scheduling}. GDMP uses the guidance for all denoising steps. In our evaluation, we use PGD+EOT with approximated gradients calculated by a surrogate process, while GDMP is originally evaluated on BPDA. Since the gradients of the full defense process can not be calculated due to memory constraints, we use four purification processes in attack, each consisting of 36 forward steps and six denoising steps.


\subsection{SODEF~\citep{kang2021stable}}
SODEF uses an ODE block that satisfies Lyapunov stability. Lyapunov-stable equilibrium point has a property that its neighborhood gathers to that point by passing through the ODE block. SODEF first extracts a feature vector of an adversarial example, and then the feature vector is passed through the Lyapunov-stable ODE block. The feature vector is finally classified with some fully-connected layers. In the original implementation, SODEF uses an adjoint method for the ODE block. In our experiment, we use back-propagation instead of the adjoint method. To evaluate SODEF, we use the standard version of AutoAttack.


\subsection{DISCO~\citep{ho2022disco}}
DISCO restores clean RGB values of adversarial images using two neural networks: an encoder and a local implicit module. The encoder extracts a feature vector of each pixel value, and the local implicit module predicts the clean RGB values of each pixel using the feature vectors within the kernel size $s$. While DISCO is evaluated on BPDA in the original evaluation, we use the full gradients of the defense process with AutoAttack.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Additional Results on Evaluation for Diffusion-Based Purifications}
\label{app:evaluation}

\subsection{Additional Results on RQ1}
We show the difference between attack success rate when using the adjoint method and back-propagation. \autoref{table:RQ1_extra} shows the robust accuracy of DiffPure~\citep{Nie2022DiffusionMF} and its probability flow ODE~\citep{Song2020ScoreBasedGM} against PGD+EOT $\ell_\infty (\epsilon = 8/255)$ on CIFAR-10. The probability flow ODE has the same marginal probability densities as DiffPure. For both DiffPure and its probability flow ODE, the back-propagation can generate adversarial examples more successfully than the adjoint method. For the probability flow ODE, the back-propagation with step size 0.01 has 32.15\% lower robust accuracy than the adjoint method.

\input{appendix/tables/RQ1_additional_result}   % Table 8


\subsection{Additional Results on RQ2}

\autoref{fig:AA_vs_PGD} compares the attack performance of PGD and AutoAttack $\ell_\infty (\epsilon = 8/255)$ on CIFAR-10. This result is conducted on the same settings with \autoref{sec:hyperparameter}. As shown in the \autoref{fig:AA_vs_PGD}, PGD generally has a larger attack success rate than AutoAttack. As the number of forward steps increases, the gap between the two attacks also increases. Therefore, PGD+EOT is a more appropriate attack than AutoAttack for evaluating diffusion-based purification methods.

\input{appendix/figures/AutoAttack_vs_PGD}      % Figure 6


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Additional Results on Analysis of Hyperparameters}
\label{app:hyperparameter}

In this section, we provide further analyses of hyperparameters on CIFAR-10 and ImageNet. In all experiments, the implementation of diffusion-based purification follows noise scheduling, the forward process, and the denoising process of Appendix~\ref{app:noise_scheduling}.


\subsection{CIFAR-10}

We provide additional results regarding denoising steps and purification steps on CIFAR-10 through \autoref{fig:sampling_steps_t200}, \autoref{fig:defense_num_iterations_t200}, and \autoref{fig:attack_num_iterations_t200}. We conduct all experiments in a setting identical to \autoref{sec:denoising_steps}, except that the number of forward steps is 200 (i.e., $t^* = 200$). Furthermore, \autoref{fig:iterations_attack} shows the overall influence of the number of purification steps in attack on the attack success rate.

\input{appendix/figures/sampling_steps_t200}    % Figure 7
\input{appendix/figures/num_iterations_t200}    % Figure 8, 9
\input{appendix/figures/iterations_attack}      % Figure 10


\subsection{ImageNet}

We provide additional results regarding forward, denoising, and purification steps on ImageNet through \autoref{fig:imagenet_forward_steps}, \autoref{fig:imagenet_sampling_steps_t200}, and \autoref{table:imagenet_iterations}, respectively. In \autoref{fig:imagenet_sampling_steps_t200} and \autoref{table:imagenet_iterations}, the number of forward steps is set to 200. Due to memory constraints, the upper bound on the number of function calls is set to ten. Although we employ the experiments only on DDPM, the results are similar to those from CIFAR-10.

\input{appendix/figures/imagenet_forward_steps}     % Figure 11
\input{appendix/figures/imagenet_sampling_steps}    % Figure 12
\input{appendix/tables/imagenet_iterations}         % Table 9


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Different Denoising Model in Attack}
\label{app:different_model}

One may use denoising models in attack, different from the model used in defense. For example, when DDPM is used in defense, DDIM can be used to generate adversarial examples, and vice-versa. We test whether the different denoising models can improve the attack success rate. As shown in \autoref{table:sampler}, the success rates of the different models are slightly lower than those of the original models.

\input{tables/sampler.tex}  % Table 10


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Gradual Noise-Scheduling for Multi-Step Purification}
\label{app:gradual_sampling}

In this section, we explain the evaluation of the gradual noise-scheduling and provide additional experimental results. In all experiments, the implementation of the gradual noise-scheduling follows the noise scheduling, forward process, and denoising process of Appendix~\ref{app:noise_scheduling}.

\subsection{Surrogate Process for Gradual Noise-Scheduling}
The defense process of the gradual noise-scheduling have three levels in the number of forward steps. To validate the robustness of our defense, we evaluate the gradual noise-scheduling with several surrogate processes and report the lowest robust accuracy among them. We denote one purification step as (\# of forward steps, \# of denoising steps). For example, suppose a process uses two purification steps, consisting of 100 forward steps with 20 denoising steps and 120 forward steps with ten denoising steps, respectively. In that case, we denote the defense process as (100, 20), (120, 10). As shown in \autoref{table:attack_processes}, the third surrogate process has the lowest robust accuracy against the gradual noise-scheduling on CIFAR-10. For all experiments of our defense on all datasets, we select the third process as the surrogate process in attacks of the adaptive white-box setting.

\input{appendix/tables/best_attack_processes}   % Table 11


\subsection{Additional Experimental Results}
\autoref{table:best_svhn} shows the robustness of our proposed method on SVHN. Although we use a different evaluation method (i.e., PGD+EOT), the robust accuracy of our defense is comparable with other state-of-the-art adversarial training defenses. We also compare our method against BPDA $\ell_\infty (\epsilon = 8/255)$ on CIFAR-10 with other adversarial purification methods. As shown in \autoref{table:best_BPDA}, our proposed method outperforms all other adversarial purification methods, achieving a robust accuracy of 88.40\%, 7\% greater than the robust accuracy of DiffPure~\citep{Nie2022DiffusionMF}. Furthermore, \autoref{table:best_other_attack} shows our robustness against other attacks, including the Square attack~\citep{Andriushchenko2019SquareAA}, which is a black-box attack. Our defense has a robust accuracy higher than 80\% against all attacks.

\input{appendix/tables/best_svhn}           % Table 12
\input{appendix/tables/best_bpda}           % Table 13
\input{appendix/tables/best_other_attacks}  % Table 14