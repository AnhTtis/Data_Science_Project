\section{Preliminary}
\label{Preliminary}
We provide the background on the adversarial attack, diffusion models, and adversarial purification in this section.

\subsection{Adversarial Attacks}
Adversarial attacks aim to manipulate or trick machine learning models by adding imperceptible perturbations to input data that can cause the model to misclassify or produce incorrect outputs. The adversarial attacks can be categorized into black-box, grey-box, and white-box attacks. The black-box attack assumes that the attacker knows nothing about the internal structure of the classifier and defender. The white-box attack assumes that the attacker can obtain any information about the defender and the target classifier, including the architecture and parameter weights. The grey-box lies between the white- and black-box attacks, where the attacker partially knows the target model. In this work, we only focus on the performance of purification in the white-box attack since the white-box attack is the most difficult to defend from the defender's perspective.

The Projected Gradient Descent (PGD)~\citep{Madry2017TowardsDL} method is a common white-box attack. PGD is a gradient-based attack that iteratively updates an adversarial example using the following rule
\begin{equation}
\mathbf{x}_{i + 1} = \Pi_{\mathcal{X}} \left(\mathbf{x}_i + \alpha_i \text{sign} \nabla_\mathbf{x} \mathcal{L}(f_\phi(\mathbf{x}), y) |_{\mathbf{x}=\mathbf{x}_i}\right),
\end{equation}
where $f_\phi$ represents a classifier, and $\Pi_\mathcal{X}$ indicates a projection operation onto $\mathcal{X}$. PGD can only be applied for the differentiable defense methods. For non-differentiable defense methods, the Backward Pass Differentiable Approximation (BPDA)~\cite{obfuscated-gradients} is widely used, which computes the gradient of the non-differentiable function by using a differentiable approximation. Expectation over Transformation (EOT)~\citep{Athalye2017SynthesizingRA} can be additionally employed for randomized defenses, which optimizes the expectation of the randomness. AutoAttack~\citep{Croce2020ReliableEO} is an ensemble of four different types of attacks. In this work, we measure the robustness of purification methods against these attack methods. 

\subsection{Diffusion Models}
Recently, diffusion-based models~\citep{Ho2020DenoisingDP, Song2020ScoreBasedGM} have gained increasing attention in generative models. Unlike the VAEs and GANs, the diffusion-based models produce samples by gradually removing noise from random noise. The training of diffusion-based models consists of two processes, the forward process, and the reverse denoising process.
The forward process adds Gaussian noise over $T$ steps to the observed input $\mathbf{x}_0$ with a predefined variance scheduler $\beta_t$, whose joint distribution is defined as 
\begin{equation}
    q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod^T_{t=1} q(\mathbf{x}_t \vert \mathbf{x}_{t-1}),
\end{equation}
where $q(\mathbf{x}_t \vert \mathbf{x}_{t-1})$ is a Gaussian transition kernel from $\mathbf{x}_{t-1}$ to $\mathbf{x}_t$ 
\begin{equation}
    q(\mathbf{x}_{t} | \mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_{t}; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t\textbf{I}).
\end{equation}
The reverse process denoises the random noise $\mathbf{x}_{T}$ over $T$ times, whose joint distribution is defined as
\begin{equation}
    p_\theta(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t).
\end{equation}
The transition distribution from $\mathbf{x}_t$ to $\mathbf{x}_{t-1}$ is often modeled by Gaussian distribution
\begin{equation}
    p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma^2_t \textbf{I}),
\end{equation}
where $\sigma_t$ is a variance, and $\boldsymbol{\mu}_\theta$ is a predicted mean of $\mathbf{x}_{t-1}$ derived from a learnable denoising model $\boldsymbol{\epsilon}_\theta$. The denoising model is often trained by predicting a random noise at each time step via following objective
\begin{equation}
L(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \Big[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big], \\
\end{equation}
where $\boldsymbol\epsilon$ is a Gaussian noise, i.e., $\boldsymbol\epsilon \sim \mathcal{N}(0, \textbf{I})$. 
The model $\boldsymbol{\epsilon}_\theta$ takes the noisy input $\mathbf{x}_t$ and the time step $t$ to predict the actual noise $\boldsymbol\epsilon$ at time $t$.
In the Denoising Diffusion Probabilistic Model (DDPM)~\citep{Ho2020DenoisingDP}, the reverse denoising process is performed over $T$ steps through random sampling, resulting in a slower generation of samples compared with GANs and VAEs.

Based on the fact that the multiple denoising steps can be performed at a single step via a non-Markovian process, \citet{Song2020DenoisingDI} proposes a new sampling strategy, which we call Denoising Diffusion Implicit Model (DDIM) sampler, to accelerate the reverse denoising process. In this work, we compare the performances of DDPM and DDIM samplers in the diffusion-based purification approach.

\subsection{Adversarial Purification}
Adversarial purification via generative models is a technique used to improve the robustness of machine learning models against adversarial attacks~\citep{Samangouei2018DefenseGANPC}. The idea behind this technique is to use a generative model to learn the underlying distribution of the clean data and use it to purify the adversarial examples.

Diffusion-based generative models can be used as a purification process if we assume that the imperceptible adversarial signals as noise~\citep{Nie2022DiffusionMF}.
To do so, the purification process adds noise to the adversarial example via the forward process with $t^*$ steps, and it removes noises via the denoising process. The choice of the number of forward steps $t^*$ is essential since too much noise can remove the semantic information of the original example, or too little noise cannot remove adversarial perturbation. 
In theory, as we add more noise to the adversarial example, the distributions over the noisy adversarial example and the true example become close to each other~\citep{Nie2022DiffusionMF}. Therefore, the denoised examples are likely to be similar.
