\section{Analysis of Hyperparameters}
\label{sec:hyperparameter}

The performance of diffusion-based purification is significantly influenced by varying hyperparameter configurations. In this section, we explore the importance of hyperparameters in defense processes.

\subsection{Experimental Settings}
\label{sec:param_settings}

Understanding the importance of hyperparameters can help build a better defense mechanism. We investigate the effect of various hyperparameters of diffusion-based purification methods to determine the most robust configuration for the adaptive attack. 
Specifically, the following three hyperparameters are evaluated 1) the number of forward steps, 2) the number of denoising steps, and 3) the number of purification steps. In addition, we re-evaluate the efficiency of several techniques proposed in previous works under our defense scheme.

We evaluate the purification against PGD+EOT on CIFAR-10. We provide the additional results on CIFAR-10 and ImageNet in \autoref{app:hyperparameter}. Although we do not report ImageNet results in the main text, the overall findings are similar to those from CIFAR-10. We use a naturally pretrained WideResNet-28-10~\citep{Zagoruyko2016WideRN} as an underlying classifier provided by Robustbench~\citep{croce2021robustbench}. For a diffusion model, we use pretrained DDPM++~\citep{Song2020ScoreBasedGM}. The variances for the diffusion model are linearly increasing from $\beta_1 = 10^{-4}$ to $\beta_T = 0.02$ when $T = 1000$ \citep{Ho2020DenoisingDP}. We use two different denoising models: DDPM~\citep{Ho2020DenoisingDP} and DDIM~\citep{Song2020DenoisingDI}. 

For all experiments, we report the mean and standard deviation over five runs to measure the standard and robust accuracy. PGD uses 200 update iterations. 20 samples are used to compute EOT. Following the settings in DiffPure~\citep{Nie2022DiffusionMF}, we use a fixed subset of 512 randomly sampled images. To calculate gradients, we use direct gradients of the entire process. If impossible, we compute the approximated gradients from a surrogate process. In each experiment, we explain the defense process and the surrogate process in more detail.

\input{figures/forward_steps.tex}
\subsection{The Number of Forward Steps}
\label{sec:forward_steps}
We explore the effect of forward noising steps on robustness by varying the number of forward steps from 30 to 300, resulting in the changes of total variance ranged from 0.012 to 0.606. The same number of forward steps are used for both attack and defense, and we set five denoising steps for attack and defense for all experiments.

As shown in \autoref{fig:forward_steps}, the standard accuracy continuously decreases as the number of forward steps increases since more forward steps induce more noise.
The robust accuracy increases first and decreases after 200 forward steps, i.e., $t^* = 200$. When the number of forward steps is small, the DDPM is more robust than the DDIM. However, DDIM shows better accuracy for both standard and robust than DDPM after 200 forward steps.

\input{figures/sampling_steps_t100.tex}

\subsection{The Number of Denoising Steps}
\label{sec:denoising_steps}
Defenders may use fewer denoising steps to accelerate the defense process. From the other perspective, attackers may want to use fewer denoising steps than those used in the defense due to memory constraints. We explore the influence of the number of denoising steps  through the following three experimental settings:
\begin{itemize}
    \item[(a)] The number of denoising steps in attack is set to five, and the number of denoising steps in defense is ranged from one to the maximum number of denoising steps.
    \item[(b)] The number of denoising steps in both the attack and defense are the same, ranging from one to 20.
    \item[(c)] The number of denoising steps in defense is set to the maximum number of denoising steps, and the number of denoising steps in attack is ranged from one to 20.\footnote{The 20 denoising steps is the maximum limit of 40GB of memory.}
\end{itemize}

The results are displayed in \autoref{fig:sampling_steps_t100}.
From the defense perspective, the results of (a) and (b) demonstrate that more denoising steps can improve robustness. DDPM gains more advantage from having more denoising steps than DDIM.
(c) shows the effect of the number of denoising steps in the attack. As the number of denoising steps increases, the attack success rate slightly increases. However, we also find that increasing the number of denoising steps in an attack can decrease the attack success rate when the number of forward steps is 200 (i.e., $t^* = 200$).


\input{figures/defense_num_iterations_t100.tex}
\input{figures/attack_num_iterations_t100.tex}
\subsection{The Number of Purification Steps}
\label{sec:purification_steps}
Although a single forward and reverse process can purify the input image, one can apply the purification process multiple times as proposed in \citet{Wang2022GuidedDM}. We denote the number of forward and denoising processes as the number of \textit{purification step}. Similar to the case of the denoising step, computing the gradients of multiple purification steps is impossible due to memory constraints in most cases. 

The number of purification steps can also differ between attack and defense. Through experiments, we measure the changes in robust accuracy with the different number of purification steps in the defense and attack.
For all experiments, we fixed the number of forward steps to 100 ($t^* = 100$), and the number of denoising steps is set to five.

\autoref{fig:defense_num_iterations_t100} shows the standard and robust accuracy with a varying number of purification steps in defense. The robust accuracy increases as the number of purification steps increases while the standard accuracy steadily decreases. 
\autoref{fig:attack_num_iterations_t100} shows the effect of the number of purification steps in the attack. When the number of purification steps in defense is one or two, the same number of purification steps in attack is the most effective. However, as we set the number of purification steps in defense to three and five, two and three purification steps in attack show a better attack success, respectively.


\subsection{Other Techniques}
\label{sec:other_tech}

We evaluate several other techniques proposed in earlier work~\citep{Yoon2021AdversarialPW, Nie2022DiffusionMF, Wang2022GuidedDM} within our new evaluation framework.

\input{tables/guidance_ablation.tex}
\input{figures/ensemble.tex}

\paragraph{Guidance.}
GDMP~\citep{Wang2022GuidedDM} proposes to use gradients of a distance between an original input example and a target example to preserve semantic information while denoising. They show guidance can improve robustness against preprocessor-blind attacks. However, as shown in \autoref{table:guidance_evaluation}, when the gradients of the surrogate process are used in the attack, the guidance of GDMP decreases the robust accuracy. Specifically, the defense with guidance using the SSIM similarity has 6.88\% robust accuracy, which is 31.56\% lower than the defense without guidance.


\paragraph{Ensemble of multiple purification runs.}
ADP~\citep{Yoon2021AdversarialPW} uses the ensemble of multiple purification runs as the predicted label to mitigate the randomness in the defense. For diffusion-based purification methods, as shown in \autoref{fig:ensemble}, multiple purification runs especially can help improve standard accuracy while the robust accuracy keeps the same level. In particular, for $t^*=200$ with 40 purification runs, standard accuracy is 8.17\% higher than the case without ensemble.

\input{tables/combination_with_adv_training.tex}
\input{tables/sampler.tex}

\paragraph{Combination with adversarial training.}
An adjoint method based DiffPure~\citep{Nie2022DiffusionMF} shows robustness can be improved by using diffusion models together with adversarial training. However, as shown in \autoref{table:combi_with_adv}, the adversarial training with purification shows lower robustness than the classifier without purification.

\input{tables/best_cifar} % Contents of section 5

\paragraph{Transferability of gradients from different samplers in the attack.}
One may employ a sampler of diffusion models to generate adversarial examples different from those used in defense. For example, attacks using gradients from DDPM could be transferred to the defense using DDIM and vice-versa. We test whether the gradients from a different sampler of denoising models can improve the attack success rate. As shown in \autoref{table:sampler}, although the transferred attack is valid, the attack success rates using different samplers are slightly lower than those using the original samplers.