\section{Build A Strong Baseline}
\label{sec:baseline}

Currently, most BSR networks use the following simple sign function for per-tensor activation binarization and per-channel weight
binarization \cite{nagel2021white}:
% to binarize activations in a per-tensor way and use a channel-wise sign function to binarize weights in a per-channel way \cite{nagel2021white}. 
\begin{equation}
% \begin{aligned}
\hat{x}=\operatorname{sign}(x)  \quad
\hat{w_i}=\frac{\|w_i\|_{l1}}{n} \operatorname{sign}(w_i)
\label{eq:poor_binarize} 
% \end{aligned}
\end{equation} 
where $\hat{x}$ and $x$ denote the binary and real-valued activation, respectively,
$\hat{w_i}$ and $w_i$ denote the binary weights and real-valued weights, respectively,
and $n$ denote the number of weights in the $i_{th}$ weight filter. 
However, due to the variation of activation distributions, such simple binarization scheme suffers from convergence issue and low performance
when the BN is removed.
% However, the simple sign function for activation binarization can't adapt to various distributions among pixels, channels, and images as shown in Section \ref{sec:motivation}. 
Therefore, we first build a strong baseline BNN based on the robust binarization method and propose a reliable network structure in this section. 

% \begin{equation}
% \begin{aligned}
% & \hat{w}=\frac{\|w\|_{1}}{n} \operatorname{sign}(w)=\begin{cases}
% \frac{\|w\|_{1}}{n}, &\text { if } w>0 \\
% -\frac{\|w\|_{1}}{n}, &\text { if } w \leq 0
% \end{cases} \\
% & \label{eq:w_binarize} 
% \end{aligned}
% \end{equation}

% \zc{You need to explain the meaning of each element in the formula}

% \begin{equation}
% \begin{aligned}
% & \hat{x}=\operatorname{sign}(x)= \begin{cases}1, & \text { if } x>0 \\
% -1, & \text { if } x \leqslant 0\end{cases} 
% \label{eq:poor_act_binarize}
% \end{aligned}
% \end{equation} 

\subsection{Binarization}
% We adopt a robust binarization method for activations in the BSR network. 
To handle the channel-to-channel variation of the activation mean, we first introduce RSign following ReActNet \cite{liu2020reactnet},
which has a channel-wise learnable threshold.
We also adopt a learnable 
% to provide different learnable thresholds for each channel, which is adaptive to the channel-wise variance. Additionally, we use a learnable 
scaling factor for each activation tensor to further reduce the quantization error between binary and real-valued activations.
Hence, the binarization function used for activations is defined as:
\begin{equation}
\begin{aligned}
& \hat{x}=\alpha\operatorname{sign}(\frac{x-\beta_{i}}{\alpha})
\label{eq:act_binarize}
\end{aligned}
\end{equation} 
where $\beta_{i}$ is the channel-wise learnable thresholds and $\alpha$ is the learnable scaling factor for each activation tensor.
Both of them can be optimized end-to-end with other parameters in the network.
To back-propagate the gradients through the discretized binarization function,
we follow \cite{liu2018bi} to use a piecewise polynomial function as the straight-through estimator (STE),
which can reduce the gradient mismatch effectively. Thus, the gradient w.r.t. $\alpha$ can be calculated as:
\begin{small}
\begin{equation}
\begin{aligned}
&\frac{\partial \hat{x}}{\partial \alpha}=\begin{cases}-1, & \text { if } x \leq \beta-\alpha \\
-2\left(\frac{x-\beta}{\alpha}\right)^2-2 \frac{x-\beta}{\alpha}-1, &\text { if } \beta-\alpha<x \leq \beta \\
2\left(\frac{x-\beta}{\alpha}\right)^2-2 \frac{x-\beta}{\alpha}+1, &\text { if } \beta<x \leq \beta+\alpha \\
1, &\text { if } x>\beta+\alpha 
\end{cases}\\
\label{eq:grad_wrt_alpha} 
\end{aligned}
\end{equation}
\end{small}
while the gradient w.r.t. $\beta_{i}$ can be computed as: 
\begin{small}
\begin{equation}
\begin{aligned}
&\frac{\partial \hat{x}}{\partial \beta_{i}}= \begin{cases}-2-2 \frac{x-\beta_{i}}{\alpha}, & \text { if } \quad \beta_{i}-\alpha<x \leqslant \beta_{i} \\ -2+2 \frac{x-\beta_{i}}{\alpha,}, & \text { if } \beta_{i}<x \leqslant \beta_{i}+\alpha \\ 0, & \text { otherwise }\end{cases}
\label{eq:grad_wrt_beta} 
\end{aligned}
\end{equation}
\end{small}

For weight binarization, we still use the channel-wise sign function in Eq.\ref{eq:poor_binarize},
for which the scaling factors are the average of the $\ell_1$ norm of each filter.
% absolute weight values in each filter. 

\subsection{Baseline Network Structure}
We use the lightweight EDSR with 16 blocks and 64 channels, and EDSR with 32 blocks and 256 channels as our backbones for two variants namely EBSR-light and EBSR. Following existing BSR networks \cite{ma2019efficient, xin2020binarized, jiang2021training, lang2022e2fif}, we only binarize the body module and leave the head and tail modules in full-precision which only contain one convolution layer each.
We also follow Bi-Real Net \cite{liu2018bi} and E2FIF \cite{lang2022e2fif} to use a skip connection bypassing every binary convolution layer in order to keep the full-precision information from being cut off by binary layers. Note that the network structure here doesn't contain BN layers.

Table~\ref{tab:baseline} shows the comparison between our proposed baseline and the prior art E2FIF.
As can be observed, for E2FIF, removing BN leads to a huge performance degradation. 
In contrast, for our strong baseline, its training is stable without BN and it outperforms E2FIF both with and without BN.
% Table  \ref{tab:baseline} shows that training a BSR network without BN leads to huge performance degradation as is mentioned in \cite{jiang2021training}. While our strong baseline without BN outperforms E2FIF \cite{lang2022e2fif} without BN significantly. It also can be seen that our baseline without BN even surpasses E2FIF with BN.

\begin{table}[!tb]
\centering
\caption{Comparison between our strong baseline with E2FIF and their counterparts with BN on Set5 and Urban100 at x4 scale.} 
\label{tab:baseline}
\scalebox{0.9}{
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{Models}                & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{2-5} 
                                       & PSNR         & SSIM       & PSNR          & SSIM         \\ \hline
E2FIF w/ BN     & 31.270                    & 0.879 & 25.070                    & 0.748 \\
\rowcolor[HTML]{FFFFFF} 
E2FIF w/o BN    & 29.274                    & 0.827 & 23.602                    & 0.682 \\
Baseline w/ BN  & 31.296                    & \textbf{0.880} & 25.090                    & \textbf{0.751} \\
\rowcolor[HTML]{FFFFFF} 
Baseline  & \textbf{31.301}                   & \textbf{0.880} & \textbf{25.093}                    & \textbf{0.751} \\ \hline
\end{tabular}
}
\end{table}