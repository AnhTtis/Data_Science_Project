\section{Method}
\label{sec:method}

% \ml{``Inconsistent'' to ``large variation''}

% In this section, we propose our methods based on the observations in Section \ref{sec:motivation}.
In this section, we propose two techniques to further enhance the strong baseline to capture the variation of activation distributions better.
We first introduce spatial re-scaling to adapt the network to pixel-to-pixel variation.
We then propose channel-wise shifting and re-scaling to better capture the channel-to-channel variation.
Meanwhile, as both of the two methods are image-dependent, the image-to-image variation can be captured naturally.
By combining the two methods with our strong baseline, we build our enhanced BNN for SR, named EBSR.

% Because the activation distributions among pixels, channels and images have large variations \red{**are highly inconsistent} in SR networks, we introduce spatial re-scaling to adapt to pixel-wise variations and channel shift and re-scaling to adapt to channel-wise variations. And both of them are image-dependent to adapt to image-wise variations, which means during inference our network re-scales and shifts the distributions of activations flexibly for different input images. Based on these methods, we build an enhanced binary neural network for image super-resolution (EBSR).

% According to [3], the difference of activation magnitudes indicates different scaling factors are needed for each pixel.

\subsection{Spatial Re-scaling}
% It is better to use different scaling factors for different pixels to reduce the quantization error and retain more detailed information for image super-resolution. 

% \ml{In the main method, we do not need to introduce the previous works but can focus on introducing our own method. Channel rescaling in Real-to-binary Net is not relevant in this context.}

% Re-scaling the output of binary convolutions was proposed at the birth of BNN in XNOR-Net \cite{rastegari2016xnor} to reduce quantization error and improve accuracy for image classification tasks.
% It is computed as below:
% \begin{equation}
% \mathcal{A} * \mathcal{W} \approx(\operatorname{sign}(\mathcal{A}) \circledast \operatorname{sign}(\mathcal{W})) \odot \mathcal{K} \alpha
% \label{eq:xnor-net rescale}
% \end{equation}
% where $\circledast$ denotes the binary convolution and $\odot$ denotes the element-wise multiplication.
% $\mathcal{A}$, $\mathcal{W}$, $\alpha$, and $\mathcal{K}$ denote the activation, weight, weight scaling factor, and activation scaling factor, respectively.
%  Later in XNOR-Net++ \cite{bulat2019xnor}, Bulat et al. fuse the activation and weight scaling factors into a single one that is learned end-to-end based on gradients and this improves the classification accuracy on ImageNet dataset.

% % It is computed as Eq.~\ref{eq:xnor-net rescale}, where $\circledast$ denotes 
% %  the binary convolution and $\odot$ denotes the element-wise multiplication. The binary convolution of $\mathcal{A}$ and $\mathcal{W}$ is rescaled by the weight scaling factor $\alpha$ and the activation scaling factor $\mathcal{K}$, both of which are calculated analytically.


% \zc{Similarly, you should explain the meaning of A, W and the operators $\circledast$ in the formula}
% Then in Real-to-binary Net \cite{martinez2020training}, Martinez et al. used a data-driven channel re-scaling module that takes the pre-convolution activations as input to predict the activation scaling factor. Unlike that in XNOR-Net++ \cite{bulat2019xnor}, these scaling factors are not fixed during inference but rather inferred from data. By doing this, they further improved the classification accuracy on ImageNet over XNOR-Net++. 
As is shown in Figure \ref{fig:pixel}, activation distributions have large pixel-to-pixel variation in SR networks
and the difference of activation magnitudes indicates different scaling factors are preferred for different pixels.
Inspired by \cite{martinez2020training}, we propose spatial re-scaling to better adapt the network to the spatial variation
of activation distributions in SR networks.
% fit the various pixel-wise distributions in SR networks.
We take the real-valued activations $A$ before convolution as input and predict pixel-wise scaling factors $S(A)$, which re-scale the binary convolution output. Spatial re-scaling process can be formulated as follows:
\begin{equation}
A * W \approx(\operatorname{sign}(A) \circledast \operatorname{sign}(W)) \odot \alpha \odot S(A)
\label{eq:spatial rescale}
\end{equation}
where $\circledast$ denotes 
the binary convolution and $\odot$ denotes the element-wise multiplication. $A$, $W$, $\alpha$, and $S\left(A\right)$ denote real-valued activations, weights, the scaling factor of weights, and the spatial-wise scaling factor of activations respectively. $S\left(A\right) \in \mathbb{R}^{1\times H\times W}$ can be calculated with a convolution and a sigmoid function.
% as $\sigma\left( CONV\left(A\right)\right)$. 
As shown in Figure \ref{fig:method}(a), real-valued activations first go through a convolution layer,
which has an input channel of $C$ and an output channel of 1, 
and then pass through a sigmoid function to produce the scaling factors $S(A)$ along the spatial dimension.
During inference, the scaling factor will change dynamically according to different input feature maps.
By re-scaling binary convolution output using $S(A)$, we can reduce the quantization error and the original pixel-wise information in FP activation
will be preserved much better.
Spatial re-scaling leads to a large PSNR improvement of 0.24 dB (from 30.30 dB to 31.54 dB) on Set5 and 0.22 dB (from 25.09 dB to 25.31 dB)
on Urban100 compared with our strong baseline. 

\subsection{Channel-wise Shifting and Re-scaling}

\begin{table}[!tb]
\centering
\caption{Comparison between whether to fuse channel-wise shifting and re-scaling or not based on our baseline with spatial re-scaling. }
\label{tab:fusing}

\scalebox{0.65}{
\begin{tabular}{c|cc|cc|cc}
\hline
\multirow{2}{*}{Method}     & \multirow{2}{*}{OPs} & \multirow{2}{*}{Params} & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{4-7} 
                            &                      &                         & PSNR        & SSIM        & PSNR          & SSIM         \\ \hline
Baseline + spatial re-scale & 2.16G                & 0.05M                   & 31.54       & 0.883       & 25.31         & 0.759        \\
+ channel-wise shift and re-scale             & 2.34G                & 0.09M                   & 31.61       & 0.885       & 25.35         & 0.761        \\
+ w/ fusing                   & 2.27G                & 0.08M                   & \textbf{31.64}       & \textbf{0.885}       & \textbf{25.36}         & \textbf{0.761}        \\ \hline
\end{tabular}
}
\end{table}

In SR networks, activation distributions exhibit larger channel-to-channel variation (Figure \ref{fig:chl}).
Both the mean and magnitude of the activation distributions vary significantly across channels.
% Thus we use channel-wise shifting and re-scaling to adapt to various channel-wise distributions. 
\cite{martinez2020training} has proposed the data-driven channel re-scaling, 
but our method differs from them in further introducing data-driven thresholds to handle the channel-wise variation of both mean and magnitude.
Since the blocks to generate the scaling factors and thresholds are very similar, we further propose to fuse them into one module.
% and fusing channel-wise shifting and re-scaling into one module.
We evaluate the effect of fusing the two blocks in Table \ref{tab:fusing}.
With channel-wise shifting and re-scaling fused, our models have fewer operations and parameters overhead and slightly higher performance.

For the specific process, we take the real-valued activations as input and predict different thresholds and scaling factors for each channel. They are also image dependent, e.g., $\beta_{i}$ in Eq.\ref{eq:act_binarize} is no longer fixed during inference but generated according to different input feature maps. Channel-wise shifting and re-scaling can be formulated as follows:
\begin{equation}
A * W \approx(\operatorname{sign}(A-C_s(A)) \circledast \operatorname{sign}(W)) \odot \alpha \odot C_r(A)
\label{eq:channel-wise_shift_and_rescale}
\end{equation}
where $\circledast$ denotes 
the binary convolution and $\odot$ denotes the element-wise multiplication. $C_s(A), C_r(A) \in \mathbb{R}^{C\times1\times1}$ denote the channel-wise threshold and scaling factor, respectively. 
We show the block diagram in Figure \ref{fig:method}(b).
The real-valued input feature map is first squeezed to a ${C\times1\times1}$ vector by a global average pooling (GAP) layer.
The subsequent fully connected layers and ReLU learn the channel-wise information and output a ${2C\times1\times1}$ vector.
Then the ${2C\times1\times1}$ vector is split into two ${C\times1\times1}$ vectors.
We use the first $C$ channels as the channel-wise bias and pass the last $C$ channels through a sigmoid layer 
as the channel-wise scaling factor, which are used to shift the real-valued activations and re-scale the binary convolution output, respectively. 


% \ml{We can mention previously, channel-wise re-scale has been proposed. We propose to fuse them. Add the comparison between fuse v.s. no fuse.}

\begin{figure}[!tbp]%
  \centering
    \includegraphics[width=0.4\textwidth]{fig/methods.png}
  
% \subfloat[channel-wise shifting\&re-scale]{
%     \label{subfig:channel-wise shifting and re-scale}
%     \includegraphics[width=0.2\textwidth]{fig/chl shift and rescale.png}
%   }

  \caption{Block diagram for spatial re-scaling, and channel-wise shifting and re-scaling.} 
  % Input A is the real-valued activation tensor and C, H, and W denote its dimension. GAP stands for global average pooling. The reduction ratio r is set to 16 for a better trade-off between the performance and the number of operations and parameters.}
  \label{fig:method}
\end{figure}


\subsection{Network Structure}

Combining the spatial re-scaling and the channel-wise shifting and re-scaling methods, we construct the enhanced convolution layer (E-Conv).
Then we build our EBSR model based on E-Conv.
In Figure \ref{fig:E-conv}, we compare the binary convolution layer used in the baseline network and our proposed E-Conv.
We use spatial and channel-wise scaling factors to re-scale the binary convolution output,
and use channel-wise shifting to learn appropriate thresholds for each channel before binarization.
The scaling factors and threshold used in E-Conv are learnable and depend on the real-valued input activations.
In this way, our proposed EBSR can adapt to pixel-to-pixel, channel-to-channel, and image-to-image variations
to reduce the large binarization error and preserve more details.
% In this way, our proposed E-Conv reduces the large quantization error caused by binarization and keeps the original information of input feature maps to a large extent.


\begin{figure}[!tb]%
  \centering

    \includegraphics[width=0.5\textwidth]{fig/E-conv.png}

  \caption{Comparison of (a) the binary convolution layer with a skip connection used in our baseline network and (b) the proposed E-Conv.}
  \label{fig:E-conv}
\end{figure}


Figure \ref{fig:network} shows the basic block based on the E-Conv and our EBSR composed of the basic blocks. Following existing works, the convolution layers in the head and tail modules are not binarized. We choose the lightweight EDSR which has 16 basic blocks and 64 channels, and EDSR which has 32 basic blocks and 256 channels as our backbones, which correspond to EBSR-light and EBSR, respectively.

\begin{figure}[!tb]%
  \centering
  {
    \includegraphics[width=0.35\textwidth]{fig/network.png}
  }
  
  \caption{The structure of our proposed EBSR.  Convolution layers in purple are real-valued vanilla 3x3 convolutions.}
  \label{fig:network}
\end{figure}