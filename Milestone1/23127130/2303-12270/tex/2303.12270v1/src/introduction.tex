\section{Introduction}
\label{sec:introduction}

Image super-resolution (SR) is a classic yet challenging problem in computer vision.
It aims to reconstruct high-resolution (HR) images, which have more details and high-frequency information, from low-resolution (LR) images.
Image SR is an ill-posed problem as 
% How to restore them accurately and clearly from corresponding LR images is an ill-posed problem.
there are multiple HR images corresponding to a single SR image \cite{wang2020deep}.
In recent years, deep neural networks (DNNs) have achieved great quality improvement in image SR but also, suffers from intensive memory 
consumption and computational cost \cite{ledig2017photo,lim2017enhanced,zhang2018residual, zhang2019residual}.
The high memory and computation requirements of SR networks hinder their deployment on resource-constrained devices, such as mobile phones and other embedded systems.

% In recent years, deep neural networks (DNNs) achieve great quality improvement in image SR and have attracted a lot of attention \cite{} \ml{Add references to SR}.
% A lot of DNN-based SR networks have been proposed with better accuracy but more parameters and computations, e.g., EDSR \cite{lim2017enhanced} has 43M parameters. The large memory and computation costs of these SR networks  

Network quantization has been naturally used to compress SR models \cite{li2020pams, hong2022daq, zhong2022dynamic} as an effective way to reduce memory and computation costs while maintaining high accuracy. In network quantization, binary neural networks (BNNs), which quantize activations and weights to  $\{-1,1\}$, are of particular interest because of their memory saving by 1-bit parameters and computation saving by replacing convolution with bit-wise operations, including XNOR and bit-count \cite{rastegari2016xnor}. 
Ma et al. \cite{ma2019efficient} are the first to introduce binarization to SR networks. However, they only binarize weights and leave activations at full precision, which impedes the bit-wise operation and leads to a limited speedup. Afterward, many works \cite{xin2020binarized, jiang2021training, lang2022e2fif} have explored BNNs for image SR with both binary activations and weights. 
Though promising results have been achieved, all these BNNs still suffer from a large performance degradation compared to the floating point (FP) counterparts.

\begin{figure}[!tb]
\centering
\includegraphics[width=0.49\textwidth]{fig/intro.png}
\caption{The binary feature maps of the same channels in the same block in our EBSR-light and the prior art E2FIF.} 
% the prior art. The binary feature maps in our model can capture more textures and details for image reconstruction.} 
\label{fig:intro}
\end{figure}

We observe the large performance degradation comes from two reasons. First of all, existing BNNs for SR surprisingly adopt a simple binarization
method, which directly applies the sign function to binarize the activations. In contrast, more advanced binarization methods
have been demonstrated in BNNs for image classification \cite{liu2020reactnet,bai2020binarybert,martinez2020training}.
Hence, how to improve the binarization methods becomes the first question towards high-performance BNNs for SR.

% In this work, we first observe that binary super-resolution networks in previous literature adopt a poor binarization method which is a simple sign function for activations compared with those advanced binarization methods proposed in BNN for image classification. Thus, we build a strong baseline using a better binarization method.

Secondly, the majority of recent full-precision SR networks \cite{lim2017enhanced, zhang2018image, zhang2018residual} remove batch normalization (BN)
for better performance. This is because BN layers normalize the features and destroy the original luminance and contrast information of the input image,
resulting in blurry output images and worse visual quality \cite{lim2017enhanced}. After removing BN, we observe the activation distribution
of SR networks exhibits much larger pixel-to-pixel, channel-to-channel, and image-to-image variation, which we hypothesize is important to
capture the image details for high
performance SR. However, such distribution variation is very unfriendly to BNN. On one hand, it leads to a large binarization error and makes BNN
training harder. For the reason, recent works, e.g., E2FIF \cite{lang2022e2fif}, add the BN back to the BNNs, and thus, suffers from blurry outputs.
On the other hand, the variation of activation distribution is very hard to preserve. For example, each activation tensor in 
the BNN share the same binarization parameters, e.g., scaling factors. This makes it very hard to preserve the magnitude differences across
channels and pixels. Hence, how to better capture the distribution variation is the second important question.

\begin{table}[!tb]
\centering
\caption{Comparison between our works and previous BNN in image classification and super-resolution tasks.}
\label{tab:related}

\scalebox{0.6}{
\begin{tabular}{c|cccccc}
\hline
Method               & Weight     & Act        & Adaptive Quant                                                             & w/ BN & HW Cost        & Task \\ \hline
\cite{bulat2019xnor}           & Per chl    & Per tensor & Spatial                                                       & Yes   & Low            & Cls  \\

\cite{martinez2020training}       & Per chl    & Per tensor & Chl                                                            & Yes   & Low            & Cls  \\
ReActNet\cite{liu2020reactnet}             & Per chl    & Per tensor & No                                                                         & Yes   & Low            & Cls  \\ \hdashline
\cite{ma2019efficient}            & Per chl & FP         & No                                                                         & Yes   & FP Accum.         & SR   \\
BAM \cite{xin2020binarized}                   & Per tensor & Per tensor & No                                                                         & Yes   & Extra FP BN          & SR   \\
BTM \cite{jiang2021training}                  & Per chl    & Per tensor & No                                                                         & No    & Low            & SR   \\
DAQ \cite{hong2022daq}                   & Per chl    & Per chl    & Chl                                                           & No    & FP Mul. and Accum. & SR   \\
E2FIF \cite{lang2022e2fif}                 & Per chl    & Per tensor & No                                                                         & Yes    & Low            & SR   \\ \hdashline
\rowcolor[HTML]{EFEFEF} 
\textbf{EBSR (ours)} & Per chl    & Per tensor & \begin{tabular}[c]{@{}c@{}}Spatial, chl,\\  and img\end{tabular} & No    & Low            & SR   \\ \hline
% \textbf{EBSR (ours)} & Per chl    & Per tensor & Spatial, chl and img & No    & low            & SR   \\ \hline
\end{tabular}
}
\end{table}

To address the aforementioned questions, in this paper, we propose EBSR, an enhanced BNN for image SR.
EBSR features two important changes over the previous BNNs for SR, including leveraging more advanced binarization methods to stabilize training
without BN, and novel methods, including spatial re-scaling as well as channel-wise shifting and re-scaling
to better capture the spatial and channel-wise activation distribution.
The resulting EBSR binary features can preserve much more textures and details for SR compared to prior art methods as shown in Figure~\ref{fig:intro}.
Our contributions can be summarized as below:
\begin{itemize}
    \item We leverage advanced binarization methods to build a strong BNN baseline, which stabilizes the training without BN and outperforms prior art methods.
    \item We observe the spatial and channel-wise variation of activation distribution is important for SR quality and propose novel methods
    to capture the variation, which preserve much more details.
    \item We evaluate our models on benchmark datasets and demonstrate significant performance improvement over the prior art method, i.e, E2FIF.
    Specifically, EBSR-light outperforms E2FIF by 0.31 dB and 0.28 dB on Set5 and Urban100, respectively, at $\times4$ scale 
    at a slight computation increase.
    EBSR-SQ achieves 0.26 dB and 0.27 dB PSNR improvements over E2FIF at the same computation cost.
    % The memory and computation overheads of our EBSR-light and EBSR are slightly heavier than E2FIF, but much fewer than the full-precision counterparts. We also propose EBSR-SQ  to further reduce the memory and computation cost of our models. Our observations and methods are applicable to other CNN-based BSR as well.
\end{itemize}


% Second, we delve into the essence of batch normalization in binary super-resolution networks. On one hand, the majority of recent full-precision SR networks \cite{lim2017enhanced, zhang2018image, zhang2018residual} remove BN for better performance, which is quite different from DCNNs for the classification task. This is because BN layers normalize the features and destroy the original luminance and contrast information of the input image \cite{lim2017enhanced}. \ml{what does this mean?} Also, BN layers are image independent  during inference. These reasons result in blurry output images, lower accuracy, and worse visual perception for SR networks with BN. On the other hand, although removing BN is better for image SR, it is unfriendly to BSR networks because it leads to a large quantization range and various distributions among channels, pixels, and images, which poses real challenges to binarization. How to minimize the quantization error while enjoying the benefits brought by removing BN is important for BSR.
% 
% To solve this problem, we propose two effective methods, i.e., the spatial re-scaling, and the channel-wise shift and re-scaling. We take real-valued input feature maps as input and learn the spatial and channel-wise information, then generate appropriate channel-wise thresholds for activation binarization, and spatial and channel-wise scaling factors to re-scale binary convolution output. During inference, they are not fixed but image dependent. 
% Based on these methods, we construct an enhanced binary neural network for image super-resolution termed EBSR. And there are two variants, i.e., EBSR-light and EBSR which are only different on the model size. With these methods, our model adapts to the highly inconsistent distributions among pixels, channels, and images in SR networks, thus having a lower quantization error and retaining more information from real-valued feature maps as shown in Figure \ref{fig:intro}.
% We evaluate our models on benchmark datasets and they significantly surpass the prior art BSR, i.e, E2FIF. For instance, EBSR-light outperforms E2FIF by 0.31 dB on Set5 at $\times4$ scale. The memory and computation overheads of our EBSR-light and EBSR are slightly heavier than E2FIF, but much fewer than the full-precision counterparts. We also propose EBSR-SQ  to further reduce the memory and computation cost of our models. Our observations and methods are applicable to other CNN-based BSR as well.