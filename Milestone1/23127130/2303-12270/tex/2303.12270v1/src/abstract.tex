%%%%%%%%% ABSTRACT
\begin{abstract}

While the performance of deep convolutional neural networks for image super-resolution (SR) has improved significantly,
the rapid increase of memory and computation requirements hinders their deployment on resource-constrained devices.
Quantized networks, especially binary neural networks (BNN) for SR have been proposed
to significantly improve the model inference efficiency but suffer from large performance degradation.
We observe the activation distribution of SR networks demonstrates very large pixel-to-pixel, channel-to-channel,
and image-to-image variation, which is important for high performance SR but gets lost during binarization.
% which renders existing binarization strategies ineffective and leads to a large binarization error.
To address the problem, we propose two effective methods, including the spatial re-scaling as well as channel-wise
% shifting and re-scaling, which improve binary convolutions with spatial-aware and channel-aware augmentations.
shifting and re-scaling, which augments binary convolutions by retaining more spatial and channel-wise information.
Our proposed models, dubbed EBSR, demonstrate superior performance over prior art methods both quantitatively and qualitatively
across different datasets and different model sizes.
Specifically, for $\times 4$ SR on Set5 and Urban100, EBSR-light improves the PSNR by 0.31 dB and 0.28 dB compared to SRResNet-E2FIF, 
respectively, while EBSR outperforms EDSR-E2FIF by 0.29 dB and 0.32 dB PSNR, respectively.
% However, they haven't delved into the data distribution which is actually different from that in CNN for image classification. Thus we make observations of the activation distributions and find that they are highly inconsistent among pixels, channels, and images in SR networks. To address this problem, we propose the spatial re-scaling, and the channel-wise shift and re-scaling methods and construct an enhanced binary neural network for image super-resolution, which has two variants named EBSR-light and EBSR. Evaluations show that our proposed models significantly surpass state-of-the-art BSR, e.g., 0.31 dB on PSNR on Set5 dataset at $\times 4$ scale, bridging the performance gap between BSR with the full-precision SR networks.



\end{abstract}