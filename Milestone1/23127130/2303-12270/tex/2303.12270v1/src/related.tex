\section{Related Works}
\label{related}

\subsection{Image super-resolution deep neural networks}

DNNs have been widely used in image SR for their satisfying performance. The pioneering work  SRCNN  \cite{dong2015image} first uses a DNN which has only three convolution layers to reconstruct the HR image in an end-to-end way.
VDSR \cite{kim2016accurate} increases the network depth to 20 convolution layers and introduces global residual learning for better performance.
SRResNet \cite{ledig2017photo} introduces residual blocks in SR and achieves better image quality.
SRGAN \cite{ledig2017photo} uses SRResNet as the generator and an additional discriminator to recover more photo-realistic textures.
EDSR \cite{lim2017enhanced} removes BN in the residual block and features with a deeper and wider model with up to 43M model parameters.
Dense connect \cite{zhang2018residual}, channel attention module \cite{zhang2018image}, and non-local attention \cite{zhang2019residual} mechanism are also used in SR networks to improve the image quality.
However, these networks have very large memory and computation overheads that can hardly be deployed on resource-constrained devices.

\subsection{BNN for image super-resolution}
To compress the SR models, BNNs for SR have been studied in recent years. For a BNN, the binarization function can be written as $\hat{x}=\alpha\operatorname{sign}(x-\beta)$, where $\alpha$,  $\beta$, $x$ and $\hat{x}$ denote the scaling factor, bias, the FP and binary variables, respectively.
There are two kinds of binarization strategies \cite{nagel2021white}, including per-tensor and per-channel binarization.
The major difference is per-tensor binarization uses the same $\alpha$ and $\beta$ for the whole tensor while per-channel binarization has
channel-wise $\alpha$s and $\beta$s.
% which quantizes the whole tensor and  per-channel quantization has different scaling factors and biases for activations in each different channel. Both $\alpha$ and $\beta$ can be learnable or calculated analytically.

We compare different BNNs for SR and for image classification tasks in Table \ref{tab:related}.
Ma et al. \cite{ma2019efficient} first introduce binarization to SR networks and reduce the model size by $80\%$ compared with FP SRResNet.
However, they only binarize weights and leave activations at FP, which impedes the bit-wise operation and  requires FP accumulations.
Xin et al. \cite{xin2020binarized} binarizes both weights and activations in SR networks utilizing a bit-accumulation mechanism (BAM) to approximate the FP convolution.
A value accumulation scheme is proposed to binarize each layer based on all previous layers.
% A value accumulation scheme is formulated as $A_n^B \approx \operatorname{Sign}\left(B N\left(\beta_1 A_1+\beta_2 A_2+\ldots+\beta_n A_n\right)\right)$. 
However, their method introduces extra FP BN computation during inference.
Jiang et al. \cite{jiang2021training} find that simply removing BN leads to large performance degradation.
Hence, they explore a binary training mechanism (BTM) to better initialize weights and normalize input LR images.
% including Xavier uniform weight initialization, input normalization, and PReLU activation.
They build a BNN without BN named IBTM.
After that, Hong et al. proposed DAQ\cite{hong2022daq} adaptive to diverse channel-wise distributions.
However, they use per-channel activation quantization which introduces large FP multiplications and accumulations.
% as shown in Figure \ref{subfig:per_chl}.
Recently Lang et al. \cite{lang2022e2fif} propose using end-to-end full-precision information flow in BNN and develops a high
performance BNN named E2FIF.
However, there is a still large gap with the full-precision model. 
None of the binarization methods above have the ability to capture the pixel-to-pixel,
channel-to-channel, and image-to-image variation of the activation distribution.
% In this work, we use spatial re-scaling and channel-wise shift and re-scaling to adapt to the various data distributions in SR networks and further bridge the performance gap between BSR and FP SR networks.

% Re-scaling the output of binary convolutions was proposed at the birth of BNN in XNOR-Net \cite{rastegari2016xnor} to reduce quantization error and improve accuracy for image classification tasks.
% It is computed as below:
% \begin{equation}
% \mathcal{A} * \mathcal{W} \approx(\operatorname{sign}(\mathcal{A}) \circledast \operatorname{sign}(\mathcal{W})) \odot \mathcal{K} \alpha
% \label{eq:xnor-net rescale}
% \end{equation}
% where $\circledast$ denotes the binary convolution and $\odot$ denotes the element-wise multiplication.
% $\mathcal{A}$, $\mathcal{W}$, $\alpha$, and $\mathcal{K}$ denote the activation, weight, weight scaling factor, and activation scaling factor, respectively.
%  Later in XNOR-Net++ \cite{bulat2019xnor}, Bulat et al. fuse the activation and weight scaling factors into a single one that is learned end-to-end based on gradients and this improves the classification accuracy on ImageNet dataset.
% % It is computed as Eq.~\ref{eq:xnor-net rescale}, where $\circledast$ denotes 
% %  the binary convolution and $\odot$ denotes the element-wise multiplication. The binary convolution of $\mathcal{A}$ and $\mathcal{W}$ is rescaled by the weight scaling factor $\alpha$ and the activation scaling factor $\mathcal{K}$, both of which are calculated analytically.
% % \zc{Similarly, you should explain the meaning of A, W and the operators $\circledast$ in the formula}
% Then in Real-to-binary Net \cite{martinez2020training}, Martinez et al. used a data-driven channel re-scaling module that takes the pre-convolution activations as input to predict the activation scaling factor. Unlike that in XNOR-Net++ \cite{bulat2019xnor}, these scaling factors are not fixed during inference but rather inferred from data. By doing this, they further improved the classification accuracy on ImageNet over XNOR-Net++. 