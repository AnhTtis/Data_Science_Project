\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

We train all the models on the training set of DIV2K \cite{timofte2017ntire}. 
Our models operate on RGB channels, i.e., the input and output image are in RGB color space, not YCbCr color space.
For evaluation, we use four standard benchmarks including Set5 \cite{bevilacqua2012low}, Set14 \cite{zeyde2012single}, B100 \cite{martin2001database} and Urban100 \cite{huang2015single}. For evaluation metrics, we use PSNR and SSIM \cite{wang2004image} over the Y channel between the output SR image and the original HR image as most previous works did.

We choose L1 loss between SR image and HR image \cite{lim2017enhanced} as our loss function. 
% Three upscaling factors, i.e., $\times 2$, $\times 3$, and $\times 4$ are evaluated for image super-resolution. 
Input patch size is set to $48 \times 48$. The mini-batch size is set to 16. We use ADAM optimizer with $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$. The learning rate is initialized as $2 \times 10^{-4}$ and halved every 200 epochs. All our models are trained from scratch for 300 epochs.

% \ml{Simplify the caption. Move the results discussion to the main text.}




% \ml{Can we add another ablation study on the impact of quantizing the spatial rescale?}

\begin{table}[!tb]
\caption{Memory and computation overheads of different models. W and A denote the bit-width of weights and activations in quantized spatial re-scaling module.}
\label{tab:cost}
\centering
\scalebox{0.73}{
% \begin{tabular}{c|cc|cc|cc}
% \hline
% \multirow{2}{*}{EBSR-light-SQ} & \multirow{2}{*}{OPs} & \multirow{2}{*}{Params} & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{4-7} 
%                                &                      &                         & PSNR        & SSIM        & PSNR          & SSIM         \\ \hline
% W1A8                           & 1.75G                & 0.06M                   & 31.46       & 0.882       & 25.20         & 0.755        \\
% W2A4                           & 1.75G                & 0.06M                   & 31.38       & 0.880       & 25.17         & 0.753        \\
% W4A4                           & 1.82G                & 0.06M                   & 31.59       & 0.884       & 25.33         & 0.759        \\ \hline
% \end{tabular}
\begin{tabular}{c|cc|cc|cc}
\hline
\multirow{2}{*}{Models}    & \multirow{2}{*}{OPs} & \multirow{2}{*}{Params} & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{4-7} 
                           &                      &                         & PSNR        & SSIM        & PSNR          & SSIM         \\ \hline
SRResNet-fp                & 64.98G               & 1.52M                   & 31.76       & 0.888       & 25.54         & 0.767        \\
SRResNet-E2FIF             & 1.83G                & 0.04M                   & 31.33       & 0.880       & 25.08         & 0.750        \\
\textbf{EBSR-light (ours)} & 2.27G                & 0.08M                   & 31.64       & 0.885       & 25.36         & 0.761        \\ \hdashline
EDSR-fp                    & 1646.68G             & 43.09M                  & 32.46       & 0.897       & 26.64         & 0.803        \\
EDSR-E2FIF                 & 25.32G               & 0.17M                   & 31.91       & 0.890       & 25.74         & 0.774        \\
\textbf{EBSR (ours)}       & 28.58G               & 1.10M                   & 32.20       & 0.892       & 26.06         & 0.783        \\ \hdashline \hdashline
EBSR-SQ (W1A8)             & 1.75G                & 0.06M                   & 31.46       & 0.882       & 25.20         & 0.755        \\
EBSR-SQ (W2A4)             & 1.75G                & 0.06M                   & 31.38       & 0.880       & 25.17         & 0.753        \\
EBSR-SQ (W4A4)             & 1.82G                & 0.06M                   & 31.59       & 0.884       & 25.33         & 0.759        \\ \hline
\end{tabular}
}

\end{table}


\begin{table}[!tbp]
\centering

% \ml{For the ablation study here, add the params and flops comparison as well.}

\caption{Comparison of different methods. - denotes unable to calculate with Eq.\ref{eq:OPs} for quantized network.
% on their operations, parameters, and PSNR and SSIM tested on Set5 and Urban100 at $\times 4$ scale. We start from our strong baseline and investigate the effect of our proposed methods. EBSR-light utilizes both channel-wise shift and re-scale and spatial re-scale. Operations here are calculated on a 128$\times$128 input image.
}
\label{tab:ablation}

\scalebox{0.65}{
% \begin{tabular}{c|cc|cc|cc}
% \hline
% \multirow{2}{*}{Method}             & \multirow{2}{*}{OPs} & \multirow{2}{*}{Params} & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{4-7} 
%                                     &                      &                         & PSNR         & SSIM       & PSNR          & SSIM         \\ \hline
% Baseline                            & 1.56G                & 0.03M                   & 31.30       & 0.880      & 25.09        & 0.751        \\
% Baseline + chl-wise shift\&re-scale & 1.63G                & 0.06M                   & 31.37       & 0.880      & 25.12        & 0.752        \\
% Baseline + spatial re-scale         & 2.16G                & 0.05M                   & 31.54       & 0.883      & 25.31        & 0.759        \\
% EBSR-light                          & 2.27G                & 0.08M                   & 31.64       & 0.885      & 25.36        & 0.761        \\ \hline
% \end{tabular}
\begin{tabular}{c|cc|cc|cc}
\hline
\multirow{2}{*}{Method}             & \multirow{2}{*}{OPs} & \multirow{2}{*}{Params} & \multicolumn{2}{c|}{Set5}                              & \multicolumn{2}{c}{Urban100}                          \\ \cline{4-7} 
                                    &                      &                         & PSNR                      & SSIM                       & PSNR                      & SSIM                      \\ \hline
Baseline                            & 1.56G                & 0.03M                   & 31.30                     & 0.880                      & 25.09                     & 0.751                     \\
Baseline + per chl act quant        & -                    & -                       & 31.38  & 0.880 & 25.12 & 0.752\\
Baseline + chl-wise shift\&re-scale & 1.63G                & 0.06M                   & 31.37                     & 0.880                      & 25.12                     & 0.752                     \\
Baseline + spatial re-scale         & 2.16G                & 0.05M                   & 31.54                     & 0.883                      & 25.31                     & 0.759                     \\
EBSR-light                          & 2.27G                & 0.08M                   & 31.64                     & 0.885                      & 25.36                     & 0.761                     \\ \hline
\end{tabular}
}
\end{table}



% \begin{table}[htbp]
% \centering
% \caption{Memory and computation overheads of different models. We compare the full-precision model, E2FIF, and our proposed model in small and  large size respectively. OPs and params denote total operations and parameters calculated by \ref{eq:OPs}. Operations here are calculated on a 128$\times$128 input image.\ml{Add EBSR-light-W4A4 and EBSR-light-W2A8 here and discuss the results in the main text.}}
% \label{tab:ops_and_memory}

% \scalebox{0.65}{

% \begin{tabular}{c|cccccc}
% \hline
% Models            & FLOPs    & BOPs     & OPs      & fp-param & bi-param & Params \\ \hline
% SRResNet-fp       & 64.98G   & 0        & 64.98G   & 1.52M    & 0        & 1.52M  \\
% SRResNet-E2FIF    & 1.23G    & 38.62G   & 1.83G    & 0.04M    & 0.04M    & 0.04M  \\
% EBSR-light (ours) & 1.67G    & 38.62G   & 2.27G    & 0.08M    & 0.04M    & 0.08M  \\ \hline
% EDSR-fp           & 1646.68G & 0        & 1646.68G & 43.09M   & 0        & 43.09M \\
% EDSR-E2FIF        & 6.00G    & 1236.68G & 25.32G   & 0.15M    & 0.59M    & 0.17M  \\
% EBSR (ours)       & 9.26G    & 1236.68G & 28.58G   & 1.08M    & 0.59M    & 1.10M  \\ \hline
% \end{tabular}

% }
% \end{table}


\subsection{Benchmark Results}

Table \ref{tab:result1} and \ref{tab:result2} present the quantitative results on different datasets. The proposed models outperform all other models. For instance, compared to the prior art SRResNet-E2FIF, our EBSR-light improves the PSNR by 0.38 dB, 0.32 dB, and 0.28 dB on Urban100 for $\times 2$, $\times 3$, and $\times 4$ SR, respectively.
% For EBSR evaluated at $\times2$ scale on Set5 and B100, it is slightly lower than E2FIF which may be caused by over-fitting. 
For the larger model, our EBSR also significantly outperforms EDSR-E2FIF,
e.g. 0.29 dB, 0.27dB, 0.1dB, and 0.32 dB improvements of PSNR on Set5, Se14, B100, and Urban100 respectively at $\times 4$ scale.
Overall our models significantly improve the performance of BNN for SR and further bridge the performance gap between the binary and FP SR networks.

We also provide the qualitative results in Figure \ref{fig:output_img}. As can be seen, the SR image reconstructed by EBSR-light is richer in details and edges. It is closer to the HR image in visual perception than the prior art network E2FIF. We provide more qualitative comparisons in the appendix.

\begin{figure}[!tb]%
  \centering
  {
    \includegraphics[width=0.5\textwidth]{fig/SR_img_new.png}
  }
  
  \caption{Qualitative comparison of our EBSR-light with the prior art network on a $\times 4$ super-resolution.}
  \label{fig:output_img}
\end{figure}


\subsection{Memory and Computation Cost}

We now evaluate the memory and computation cost of our proposed EBSR-light and EBSR.
We compute the total operations and parameters following \cite{zhou2016dorefa} and \cite{liu2018bi} as below: 
\begin{equation}
\begin{small} 
\begin{aligned}
    &OPs = FLOPs + BOPs/64  \\
    &Params = Param_{fp} + Param_{bi}/32
\end{aligned}
\end{small}
\label{eq:OPs}
\end{equation}

% In Table \ref{tab:cost}.
where $BOPs$ and $Param_{bi}$ denote the number of binary operations and parameters. We choose a $128\times128$ input image at $\times 4$ scale for evaluation. Compared with the FP SRResNet and EDSR,
our EBSR-light and EBSR reduce memory usage by $38\times$ and $39\times$, respectively,
and reduce computation by $29\times$ and $58\times$, respectively.
The computation cost of our models is slightly larger than E2FIF in exchange for much higher performance as discussed above. 
% while the extra overheads can be reduced by our EBSR-light with spatial re-scaling quantized (EBSR-SQ). 

We notice that the spatial re-scaling module introduces many FP operations.
This is because the convolution layer in Figure \ref{fig:method}(a) outputs a feature map with $H\times W$ spatial dimension,
which is usually large for SR.
Thus we propose EBSR-SQ to quantize the spatial re-scaling module to low-bits.
We quantize the weights and activations following \cite{esser2019learned} and calculated the equivalent operations and parameters
of EBSR-SQ following \cite{zhou2016dorefa}.
We try three different configurations as shown in Table~\ref{tab:cost},
all of which achieve better performance compared to the SRResNet-E2FIF with fewer operations.
Specifically, EBSR-SQ with 4-bit weight and 4-bit activation improves
the PSNR by 0.26 dB and 0.27dB on Set5 and Urban100, respectively, over SRResNet-E2FIF, with 0.01 G fewer operations.

% With spatial re-scaling quantized, three kinds of our EBSR-SQ all have fewer computation overheads and still a better performance than SRResNet-E2FIF.  EBSR-SQ with 4-bit weight and 4-bit activation has the best performance, improving PSNR by 0.26 dB and 0.27dB on Set5 and Urban100  over SRResNet-E2FIF and has 0.01G  

% Operations here are computed following \cite{zhou2016dorefa}, i.e., a multiplication between m-bit weight and n-bit activation costs $mn$ binary operations. Following \cite{liu2018bi}, total operations and parameters are calculated according to binary and full-precision ones as below:
% We compute the total operations and parameters following \cite{zhou2016dorefa} and \cite{liu2018bi} as below: 
% i.e., a multiplication between m-bit weight and n-bit activation costs $mn$ binary operations. Following \cite{liu2018bi}, total operations and parameters are calculated according to binary and full-precision ones as below:


\subsection{Ablation Study}
\label{subsection: ablation}

We conduct ablation studies based on our strong baseline model to analyze the individual effect of our methods. 
We also compare our methods with the per-channel activation quantization.
As we have discussed in Section~\ref{sec:motivation}, per-channel activation quantization is not hardware friendly.
Hence, we focus on the performance comparison with it.

% Per-channel activation quantization being adaptive to channel-wise variation can improve performance but is hardware unfriendly. In comparison, our EBSR-light has better performance and lower computation and memory overheads.
As shown in Table \ref{tab:ablation}, compared with the baseline model, 
the spatial re-scaling method has 0.24 dB and 0.22 dB improvement on Set5 and Urban100, respectively.
While the improvements of the channel-wise shifting and re-scaling method are 0.07 dB and 0.03 dB on the two datasets.
Our model EBSR-light out-performs the per-channel quantization by 0.26 dB and 0.24 dB on the two datasets while
being much more hardware friendly.
The reason that spatial re-scaling is more effective than channel-wise shifting and re-scaling is probably that in our strong baseline,
we have already used RSign with learnable bias to binarize activations which alleviate the impact of channel-wise variation.
% Thus the spatial re-scaling is more effective to our baseline model which doesn't utilize the spatial information in the original feature map.
% From the table, we can also find that the two proposed methods are somehow orthogonal,
% which is mainly because they are responsible for retaining different spatial and channel-wise information. 

% We also notice that compared with the baseline, the spatial re-scaling method increases more operations than the channel-wise method. This is because the convolution layer in Figure \ref{subfig:Spatial re-scale} outputs a feature map with $H\times W$ spatial dimension, which are usually large for image super-resolution. Thus we quantize the spatial re-scaling module to low-bits in Table \ref{tab:SQ}. We quantize weights using a normal uniform quantizer and activations using LSQ quantizer \cite{esser2019learned}. Operations here are computed according to \cite{zhou2016dorefa}, i.e., a multiplication between m-bit weight and n-bit activation costs $mn$ binary operations. With spatial re-scaling quantized, our models have fewer computation overheads (c.f. the 2nd row in Table \ref{tab:ops_and_memory}) and still a better performance than the prior art. 

\begin{table*}[!tb]
\centering

\caption{Comparison of our proposed EBSR-light with other BNNs for image super-resolution. All the models in this table have a similar lightweight backbone, which has 16 blocks and 64 channels. Note that fp denotes the full-precision model.}
\label{tab:result1}

\scalebox{0.9}{

\begin{tabular}{c|c|cc|cc|cc|cc}
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{Scale} & \multicolumn{2}{c|}{Set5}       & \multicolumn{2}{c|}{Set14}      & \multicolumn{2}{c|}{B100}       & \multicolumn{2}{c}{Urban100}    \\ \cline{3-10} 
                       &                        & PSNR           & SSIM           & PSNR           & SSIM           & PSNR           & SSIM           & PSNR           & SSIM           \\ \hline
SRResNet-fp            & x2                     & 37.76          & 0.958          & 33.27          & 0.914          & 31.95          & 0.895          & 31.28          & 0.919          \\
Bicubic                & x2                     & 33.66          & 0.930          & 30.24          & 0.869          & 29.56          & 0.843          & 26.88          & 0.840          \\
SRResNet-BNN           & x2                     & 35.21          & 0.942          & 31.55          & 0.896          & 30.64          & 0.876          & 28.01          & 0.869          \\
SRResNet-DoReFa        & x2                     & 36.09          & 0.950          & 32.09          & 0.902          & 31.02          & 0.882          & 28.87          & 0.880          \\
SRResNet-BAM           & x2                     & 37.21          & 0.956          & 32.74          & 0.910          & 31.60          & 0.891          & 30.20          & 0.906          \\
SRResNet-E2FIF         & x2                     & 37.50          & 0.958          & 32.96          & 0.911          & 31.79          & 0.894          & 30.73          & 0.913          \\
EBSR-light (ours)       & x2                     & \textbf{37.64} & \textbf{0.958} & \textbf{33.16} & \textbf{0.913} & \textbf{31.88} & \textbf{0.895} & \textbf{31.11} & \textbf{0.917} \\ \hline
SRResNet-fp            & x3                     & 34.07          & 0.922          & 30.04          & 0.835          & 28.91          & 0.798          & 27.50          & 0.837          \\
Bicubic                & x3                     & 30.39          & 0.868          & 27.55          & 0.774          & 27.21          & 0.739          & 24.46          & 0.735          \\
SRResNet-BNN           & x3                     & 31.18          & 0.877          & 28.29          & 0.799          & 27.73          & 0.765          & 25.03          & 0.758          \\
SRResNet-DoReFa        & x3                     & 32.44          & 0.903          & 28.99          & 0.811          & 28.21          & 0.778          & 25.84          & 0.783          \\
SRResNet-BAM           & x3                     & 33.33          & 0.915          & 29.63          & 0.827          & 28.61          & 0.790          & 26.69          & 0.816          \\
SRResNet-E2FIF         & x3                     & 33.65          & 0.920          & 29.67          & 0.830          & 28.72          & 0.795          & 27.01          & 0.825          \\
EBSR-light (ours)       & x3                     & \textbf{33.89} & \textbf{0.921} & \textbf{29.95} & \textbf{0.834} & \textbf{28.82} & \textbf{0.797} & \textbf{27.33} & \textbf{0.833} \\ \hline
SRResNet-fp            & x4                     & 31.76          & 0.888          & 28.25          & 0.773          & 27.38          & 0.727          & 25.54          & 0.767          \\
Bicubic                & x4                     & 28.42          & 0.810          & 26.00          & 0.703          & 25.96          & 0.668          & 23.14          & 0.658          \\
SRResNet-BNN           & x4                     & 29.33          & 0.826          & 26.72          & 0.728          & 26.45          & 0.692          & 23.68          & 0.683          \\
SRResNet-DoReFa        & x4                     & 30.38          & 0.862          & 27.48          & 0.754          & 26.87          & 0.708          & 24.45          & 0.720          \\
SRResNet-BAM           & x4                     & 31.24          & 0.878          & 27.97          & 0.765          & 27.15          & 0.719          & 24.95          & 0.745          \\
SRResNet-E2FIF        & x4                     & 31.33          & 0.880          & 27.93          & 0.766          & 27.20          & 0.723          & 25.08          & 0.750          \\
EBSR-light (ours)       & x4                     & \textbf{31.64} & \textbf{0.885} & \textbf{28.22} & \textbf{0.772} & \textbf{27.30} & \textbf{0.727} & \textbf{25.36} & \textbf{0.761} \\ \hline
\end{tabular}


}
\end{table*}




\begin{table*}[!tbp]
\centering
\caption{Comparison of our proposed EBSR with other BNNs for image super-resolution. All the models in this table have 32 blocks and 256 channels. Note that fp here denotes the full-precision model.}
\label{tab:result2}

\scalebox{0.9}{

\begin{tabular}{c|c|cc|cc|cc|cc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Scale} & \multicolumn{2}{c|}{Set5}       & \multicolumn{2}{c|}{Set14}      & \multicolumn{2}{c|}{B100}       & \multicolumn{2}{c}{Urban100}    \\ \cline{3-10} 
                        &                        & PSNR           & SSIM           & PSNR           & SSIM           & PSNR           & SSIM           & PSNR           & SSIM           \\ \hline
EDSR-fp                 & x2                     & 38.11          & 0.960          & 33.92          & 0.920          & 32.32          & 0.901          & 32.93          & 0.935          \\
Bicubic                 & x2                     & 33.66          & 0.930          & 30.24          & 0.869          & 29.56          & 0.843          & 26.88          & 0.840          \\
EDSR-BNN                & x2                     & 34.47          & 0.938          & 31.06          & 0.891          & 30.27          & 0.872          & 27.72          & 0.864          \\
EDSR-BiReal             & x2                     & 37.13          & 0.956          & 32.73          & 0.909          & 31.54          & 0.891          & 29.94          & 0.903          \\
EDSR-IBTM               & x2                     & 37.80          & 0.960          & 33.38          & 0.916          & 32.04          & 0.898          & 31.49          & 0.922          \\
EDSR-E2FIF              & x2                     & 37.95          & \textbf{0.960} & 33.37          & 0.915          & \textbf{32.13} & \textbf{0.899} & 31.79          & 0.924          \\
EBSR (ours)             & x2                     & \textbf{37.99} & 0.959          & \textbf{33.52} & \textbf{0.916} & 32.10          & 0.898          & \textbf{31.96} & \textbf{0.926} \\ \hline
EDSR-fp                 & x3                     & 34.65          & 0.928          & 32.52          & 0.846          & 29.25          & 0.809          & 28.80          & 0.865          \\
Bicubic                 & x3                     & 30.39          & 0.868          & 27.55          & 0.774          & 27.21          & 0.739          & 24.46          & 0.735          \\
EDSR-BNN                & x3                     & 20.85          & 0.399          & 19.47          & 0.299          & 19.23          & 0.285          & 18.18          & 0.307          \\
EDSR-BiReal             & x3                     & 33.17          & 0.914          & 29.53          & 0.826          & 28.53          & 0.790          & 26.46          & 0.801          \\
EDSR-IBTM               & x3                     & 34.10          & 0.924          & 30.11          & 0.838          & 28.93          & 0.801          & 27.49          & 0.839          \\
EDSR-E2FIF               & x3                     & 34.24          & 0.925          & 30.06          & 0.837          & 29.00          & 0.802          & 27.84          & 0.844          \\
EBSR (ours)             & x3                     & \textbf{34.36} & \textbf{0.925} & \textbf{30.28} & \textbf{0.840} & \textbf{29.04} & \textbf{0.803} & \textbf{28.06} & \textbf{0.849} \\ \hline
EDSR-fp                 & x4                     & 32.46          & 0.897          & 28.80          & 0.787          & 27.71          & 0.742          & 26.64          & 0.803          \\
Bicubic                 & x4                     & 28.42          & 0.810          & 26.00          & 0.703          & 25.96          & 0.668          & 23.14          & 0.658          \\
EDSR-BNN                & x4                     & 17.53          & 0.188          & 17.51          & 0.160          & 17.15          & 0.151          & 16.35          & 0.163          \\
EDSR-BiReal             & x4                     & 30.81          & 0.871          & 27.71          & 0.760          & 27.01          & 0.716          & 24.66          & 0.733          \\
EDSR-IBTM               & x4                     & 31.84          & 0.890          & 28.33          & 0.777          & 27.42          & 0.732          & 25.54          & 0.769          \\
EDSR-E2FIF               & x4                     & 31.91          & 0.890          & 28.29          & 0.755          & 27.44          & 0.731          & 25.74          & 0.774          \\
EBSR (ours)             & x4                     & \textbf{32.20} & \textbf{0.892} & \textbf{28.56} & \textbf{0.780} & \textbf{27.54} & \textbf{0.735} & \textbf{26.06} & \textbf{0.783} \\ \hline
\end{tabular}


}
\end{table*}



% \begin{table}[htbp]
% \scalebox{0.8}{
% \begin{tabular}{c|cc|cc}
% \hline
% \multirow{2}{*}{Models}                & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{2-5} 
%                                        & PSNR         & SSIM       & PSNR          & SSIM         \\ \hline
% Baseline                               & 31.301       & 0.880      & 25.093        & 0.751        \\
% Baseline + chl-wise shift \& re-scale & 31.367       & 0.880      & 25.123        & 0.752        \\
% Baseline + spatial re-scale            & 31.539       & 0.883      & 25.306        & 0.759        \\
% EBSR-light                             & 31.613       & 0.885      & 25.353        & 0.761        \\ \hline
% \end{tabular}
% }
% \end{table}







% and the practical latency on the mobile phone. We calculate the number of binary and full-precision operations and parameters in the models. For practical latency, we implement the models achieved by our networks on Redmi K40S mobile phone equipped with a Qualcomm Snapdragon 870 SoC through larq, an open-source Python library for training neural networks with extremely low-precision weights and activations. When benchmarking the models, we use the number of runs 100, and the number of threads 1 to evaluate its real speed in practice.


% \begin{table}[htbp]
% \centering

% \caption{}
% \label{tab:deployment}

% \scalebox{0.78}{
% \begin{tabular}{c|ccccc}
% \hline
% Models            & FLOPs    & BOPs     & fp-param & bi-param & latency(ms) \\ \hline
% SRResNet-fp       & 64.98G   & 0        & 1.52M    & 0        & 1649        \\
% SRResNet-E2FIF    & 1.23G    & 38.62G   & 0.04M    & 0.04M    & 184         \\
% EBSR-light (ours) & 1.67G    & 38.62G   & 0.08M    & 0.04M    & 348         \\ \hline
% EDSR-fp           & 1646.68G & 0        & 43.09M   & 0        & 44781       \\
% EDSR-E2FIF        & 6.00G    & 1236.68G & 0.15M    & 0.59M    & 3421        \\
% EBSR (ours)       & 9.26G    & 1236.68G & 1.08M    & 0.59M    & 4566        \\ \hline
% \end{tabular}
% }
% \end{table}
