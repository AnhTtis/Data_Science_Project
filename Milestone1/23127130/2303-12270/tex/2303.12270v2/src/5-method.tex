\section{Method}
\label{sec: method}

In this section, we introduce our proposed method SCALES,
which mainly consists of three components, including the layer-wise scaling factor, the spatial re-scaling
module, and the channel-wise re-scaling module capturing the
layer-to-layer, pixel-to-pixel, and channel-to-channel variations efficiently in an input-dependent manner.


\subsection{Layer-wise Scaling Factor}
\label{subsec: layer-wise scaling factor}

Currently, existing BNNs for SR either use complicated binarization functions \cite{woo2018cbam,li2022local,hong2022daq} which have expensive computation costs, 
or use the sign function for activation binarization \cite{lang2022e2fif}, i.e., $\hat{x}=\operatorname{sign}(x)$ which can not capture the variations in SR networks.
To this end,
we propose using the layer-wise scaling factor $\alpha$
to capture the layer-to-layer variation.
Each convolution or linear layer has a scaling factor,
which is learned to have different magnitudes for different layers.
We further introduce the channel-wise threshold $\beta$ that is also learnable inspired by ReActNet \cite{liu2020reactnet}
to capture the channel-wise shifting in Fig.~\ref{subfig:CNN Distribution across channels}.
Thus, our activation binarization function becomes: 
\begin{equation}
\hat{x}=\alpha\operatorname{sign}(\frac{x-\beta}{\alpha}),
\label{eq:act_binarize}
\end{equation} 
where $\beta$ is the channel-wise learnable threshold, and $\alpha$ is the layer-wise scaling factor.
Both can be optimized end-to-end with the gradient-based method with the help of
the straight through estimator (STE)~\cite{liu2018bi}.
The gradient w.r.t. $\alpha$ can be calculated as:
\begin{small}
\begin{equation}
\begin{aligned}
&\frac{\partial \hat{x}}{\partial \alpha}=\begin{cases}-1, & \text { if } x \leq \beta-\alpha \\
-2\left(\frac{x-\beta}{\alpha}\right)^2-2 \frac{x-\beta}{\alpha}-1, &\text { if } \beta-\alpha<x \leq \beta \\
2\left(\frac{x-\beta}{\alpha}\right)^2-2 \frac{x-\beta}{\alpha}+1, &\text { if } \beta<x \leq \beta+\alpha \\
1, &\text { if } x>\beta+\alpha 
\end{cases}\\
\label{eq:grad_wrt_alpha} 
\end{aligned}
\end{equation}
\end{small}
while the gradient w.r.t. $\beta$ can be computed as: 
\begin{small}
\begin{equation}
\begin{aligned}
&\frac{\partial \hat{x}}{\partial \beta}= \begin{cases}-2-2 \frac{x-\beta}{\alpha}, & \text { if } \quad \beta-\alpha<x \leqslant \beta \\ -2+2 \frac{x-\beta}{\alpha,}, & \text { if } \beta<x \leqslant \beta+\alpha \\ 0, & \text { otherwise }\end{cases}
\label{eq:grad_wrt_beta} 
\end{aligned}
\end{equation}
\end{small}

For weights, we binarize them in a per-channel way as usual:
% \begin{equation}
% \hat{w}=\frac{\|w\|_{l1}}{n} \operatorname{sign}(w),
% \label{eq:poor_binarize_w} 
% \end{equation} 
$\hat{w}=\frac{\|w\|_{l1}}{n} \operatorname{sign}(w),$
where $n$ denotes the number of weights.
The scaling factor is the absolute mean value for each output channel.
% With these binarization functions, our model can outperform the prior art E2FIF both with and without BN as shown in Table~\ref{tab:baseline}.
% For E2FIF, removing BN leads to large performance degradation. 
% In contrast, ours has better performance when BN is removed.


% \begin{table}[!tb]
% \centering
% \caption{Comparison between models using our strong binarization functions and E2FIF on Set5 and Urban100 at x4 scale. 
% Note that we use SRResNet \cite{ledig2017photo} as the backbone and the result of E2FIF is different from the original literature because we train all the models in the RGB color space for fair comparison.
% } 
% \label{tab:baseline}
% \scalebox{0.9}{
% \begin{tabular}{c|cc|cc}
% \hline
% \multirow{2}{*}{Models}                & \multicolumn{2}{c|}{Set5} & \multicolumn{2}{c}{Urban100} \\ \cline{2-5} 
%                                        & PSNR         & SSIM       & PSNR          & SSIM         \\ \hline
% E2FIF w/ BN     & 31.270                    & 0.879 & 25.070                    & 0.748 \\
% % \rowcolor[HTML]{FFFFFF} 
% E2FIF w/o BN    & 29.274                    & 0.827 & 23.602                    & 0.682 \\
% SB w/ BN  & 31.296                    & \textbf{0.880} & 25.090                    & \textbf{0.751} \\
% % \rowcolor[HTML]{FFFFFF} 
% SB w/o BN & \textbf{31.301}                   & \textbf{0.880} & \textbf{25.093}                    & \textbf{0.751} \\ \hline
% \end{tabular}
% }
% \end{table}


\subsection{Spatial Re-scaling}

% \begin{figure}[!tb]%
%   \centering
%     \includegraphics[width=0.35\textwidth]{figs/spatial_rescale.pdf}
%   \caption{The proposed spatial re-scaling module (on the right branch).
%   The left branch is the binary convolution.
%   We annotate the dimensions of activations.
%   N is the batch size.
%   C is the channel dimension.
%   H and W are the spatial resolution.} 
%   \label{fig:spatial_rescale}
% \end{figure}

\begin{figure}[!tb]%
  \centering 
  % \hspace*{-0.5cm}
  \subfloat[]{
    \label{subfig: spatial_rescale_CNN}
    \includegraphics[width=0.25\textwidth]{fig/spatial_rescale_CNN_v1.pdf}
  }
  \subfloat[]{
    \label{subfig: spatial_rescale_Transformer}
    \includegraphics[width=0.23\textwidth]{fig/spatial_rescale_Transformer_v1.pdf}
  }
  \caption{
The proposed spatial re-scaling method for (a) CNN-based and (b) Transformer-based SR network.
}
  \label{fig:spatial_rescale}
\end{figure}


To capture the pixel-to-pixel variation in SR networks,
we propose the spatial re-scaling method for both CNN-based networks
in Fig.~\ref{subfig: spatial_rescale_CNN} and Transformer-based networks in Fig.~\ref{subfig: spatial_rescale_Transformer}.

We use the FP activation before binarization as the input
and predict the spatial scaling factors to re-scale the output of binary convolution or binary linear layer.
The spatial scaling factors are predicted
through the right-hand side branch,
i.e., the FP convolution layer with $1\times1$ kernel and sigmoid layer in Fig.~\ref{subfig: spatial_rescale_CNN}
and the FP linear layer and sigmoid layer in Fig.~\ref{subfig: spatial_rescale_Transformer}.
Although they are in FP,
they only introduce little parameters compared to the original parameters
of binary conv or linear layer.
% Sin are $9C^2$,
% when C is 256
% after considering an FP parameter is $32\times$ larger than a binary parameter.
It is also worth noting that during inference the spatial scaling factor
is not fixed but inferred from data.
Thus the spatial re-scaling module can capture spatial information
in an input-dependent manner.
As a result, the pixel-to-pixel and image-to-image variations 
are well captured.
Our spatial re-scaling method is formulated as:
\begin{equation}
A \otimes W \approx(\mathcal{B}_1(A) \otimes \mathcal{B}_2(W)) \odot S(A)
\label{eq:spatial rescale}
\end{equation}
where $A$ and $W$ are the FP activations and weights, $\mathcal{B}_1(\cdot)$ and $\mathcal{B}_2(\cdot)$ denote the binarization function for activations and weights respectively, 
$S\left(A\right)$ is our spatial re-scaling method, 
$\otimes$ denotes 
the binary convolution or multiplication operation, and 
$\odot$ denotes the broadcast element-wise multiplication. 


\subsection{Channel-wise Re-scaling}

\begin{figure}[!tb]
  \centering
    \includegraphics[width=0.25\textwidth]{fig/channel_rescale.pdf}
  \caption{The proposed channel-wise re-scaling method for CNN-based SR network.} 
  \label{fig:chl_rescale}
\end{figure}

To capture the channel-to-channel variation in CNN-based SR networks,
we propose the channel-wise re-scaling method in Fig,~\ref{fig:chl_rescale}.
Note that Transformer-based SR networks do not have channel-to-channel variation due to LN.
We use the FP activations before binarization as input
since it contains rich information.
Then, a global average pooling layer is applied to aggregate the spatial information.
Afterward, we capture the inter-channel information with the Conv1d layer
and derive the channel-wise scaling factor through a sigmoid function.
Our channel-wise re-scaling method can be formulated as:
\begin{equation}
A \otimes W \approx(\mathcal{B}_1(A) \otimes \mathcal{B}_2(W)) \odot C(A)
\label{eq:channel_rescale}
\end{equation}
where $C(A)$ is our channel-wise re-scaling method.
% and the others are defined the same as in Eq.~\ref{eq:spatial rescale}. 
The kernel size of the Conv1d layer is set to 5,
which we find have better performance empirically. 


Previous work \cite{martinez2020training} also introduced a channel re-scaling module for image classification BNN.
However, our method differs from theirs in the way of generating the channel-wise scaling factor.
They adopt a GlobalAvgPool-Linear-ReLU-Linear-Sigmoid structure, which introduces large parameter overhead.
Our method only have $k$ FP parameters, which is the kernel size of the Conv1d layer.
While the Lineaer-ReLU-Linear structure in~\cite{martinez2020training} introduces ${2C^2} / {r}$ FP parameters, where $r$ is the compression ratio,
which are $2C^2 / rk$  times larger than ours.
The ratio will reach 1638 when $r$ is 16, $C$ is 256, and $k$ is 5, which are the typical values.


Combining the aforementioned methods
we achieve our proposed binarization method, SCALES. 
Fig.~\ref{fig:layer_with_SCALES} shows the binary convolution and linear layer equipped with SCALES.
For convolution, we also incorporate a skip connection following~\cite{liu2018bi,lang2022e2fif}.
The binary convolution and linear layers can serve as a drop-in replacement for various SR network architectures,
which enable the binarized SR network to efficiently capture the important
pixel-to-pixel, channel-to-channel, layer-to-layer, and image-to-image variations.

% \begin{figure}[!tb]
%   \centering
%     \includegraphics[width=0.4\textwidth]{figs/SCALES.png}
%   \caption{The binary convolution and linear module integrated with SCALES.
% } 
%   \label{fig:SCALE}
% \end{figure}

\begin{figure}[!tb]%
  \centering 
  % \hspace*{-0.5cm}
  \subfloat[]{
    \label{subfig:conv_module}
    \includegraphics[width=0.26\textwidth]{fig/conv_module.pdf}
  }
  \subfloat[]{
    \label{subfig:linear_module}
    \includegraphics[width=0.2\textwidth]{fig/linear_module.pdf}
  }
  \caption{
The binary (a) convolution and (b) linear layer integrated with SCALES. LSF stands for the layer-wise scaling factor.
}
  \label{fig:layer_with_SCALES}
\end{figure}