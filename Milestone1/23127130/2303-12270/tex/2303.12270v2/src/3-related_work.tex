\section{Background}
\label{background}

% \begin{table}[!tb]
% \centering
% \caption{Comparison between our method and other works. 
% % The definition of per-channel and per-tensor quantization can be referred to \cite{nagel2021white}. ‘Adaptive Quant’ means whether the quantization scheme is input dependent during inference, and if yes, on which dimension Adaptive Quant is adopted.
% % The spatial, channel, and image adaptability indicate whether the binarization methods are able to adjust according to variations in these three dimensions and capture detailed information.
% We compare them in terms of the spatial, channel-wise, layer-wise and image-wise adaptability of the quantization method,
% whether using BN, 
% and the hardware cost.
% }
% \label{tab:related work}

% \scalebox{0.66}{


% \begin{tabular}{c|ccccccc}
% \hline
% Method                          & Spa. Adpt.    & Chl. Adpt.     &Layer Adpt.     & Img. Adpt.        & w/ BN       & HW cost\\      \hline
% \cite{ma2019efficient}          & \ding{55}     & \ding{55}    & \ding{55}     & \ding{55}         & \ding{52}   & FP Accum.            \\
% BAM \cite{xin2020binarized}     & \ding{52}     & \ding{55}    & \ding{55}     & \ding{55}         & \ding{52}   & Extra FP BN            \\
% BTM \cite{jiang2021training}    & \ding{55}     & \ding{55}    & \ding{55}     & \ding{52}         & \ding{55}   & Low                \\
% LMB \cite{li2022local}          & \ding{52}     & \ding{55}    & \ding{55}     & \ding{52}         & \ding{55}   & FP Accum.       \\  
% DAQ \cite{hong2022daq}          & \ding{55}     & \ding{52}    & \ding{55}     & \ding{52}         & \ding{55}   & FP Mul. and Accum.         \\
% E2FIF \cite{lang2022e2fif}      & \ding{55}    & \ding{55}    & \ding{55}      & \ding{55}         & \ding{52}   & Low            \\
% \textbf{SCALES (ours)}          & \ding{52}    & \ding{52}    & \ding{52}       & \ding{52}     & \ding{55}      & Low    \\ \hline
% \end{tabular}

% }
% \end{table}


\subsection{DNNs for Image SR}

DNNs have been widely used in image SR for their satisfying performance. 
SRCNN  \cite{dong2015image} first uses three convolution layers to reconstruct the HR image in an end-to-end way.
VDSR \cite{kim2016accurate} increases the network depth to 20 convolution layers and introduces global residual learning.
SRResNet \cite{ledig2017photo} introduces residual blocks as the basic block.
% SRGAN \cite{ledig2017photo} uses SRResNet as the generator and an additional discriminator to recover more photo-realistic textures.
EDSR \cite{lim2017enhanced} removes batch normalization layers (BN) in the basic block and uses a deeper and wider model, 
which has become the standard architecture for CNN-based SR networks.  
% EDSR features with a deeper and wider model with up to 43M model parameters.
After that, dense connect \cite{zhang2018residual}, channel attention module \cite{zhang2018image}, and non-local attention mechanism \cite{zhang2019residual} are introduced in SR networks for better image quality.
In recent years, Transformer-based SR networks
are proposed with better performance.
IPT~\cite{chen2021pre} leverages the Transformer encoder-decoder
structure and pre-training on large-scale dataset.
SwinIR~\cite{liang2021swinir} is based on Swin Transformer
and performs better than IPT.
HAT~\cite{chen2023activating} combines channel attention,
window-based self-attention, and cross-attention schemes
and reaches better image SR performance. 


The architecture of typical DNNs for SR is shown in Fig.~\ref{fig:network_arch}.
It consists of three modules.
The head module extract shallow features from the input LR image.
The body module utilizes multiple basic blocks to perform deep feature extraction and the deep feature is fused with shallow feature through global residual connection.
CNN-based and Transformer-based networks
use different basic blocks as shown in Fig.~\ref{fig:network_arch}.
The former incorporates convolution layers and ReLU.
The latter incorporates transformer layers including 
layernorm, multi-head self-atteion (MSA), multi-layer perceptron (MLP) and the additional convolution layer.
The tail module reconstruct the high-resolution output by convolution and pixel shuffling.


\subsection{BNNs for SR}
% To compress the SR models, BNNs for SR have been studied in recent years. 
% For a BNN, the binarization function can be written as $\hat{x}=\alpha\operatorname{sign}(x-\beta)$, where $\alpha$,  $\beta$, $x$ and $\hat{x}$ denote the scaling factor, bias, the FP and binary variables, respectively.
% There are two kinds of binarization strategies \cite{nagel2021white}, including per-tensor and per-channel binarization.
% The major difference is per-tensor binarization uses the same $\alpha$ and $\beta$ for the whole tensor while per-channel binarization has
% channel-wise $\alpha$s and $\beta$s.

BNN, which quantizes both activation and weight to $\{-1,1\}$ is first proposed by XNOR-Net \cite{rastegari2016xnor} for the image classification task.
Afterward, a lot of works\cite{lin2017towards,liu2018bi,martinez2020training,liu2020reactnet,tu2022adabin, bai2020binarybert, he2023bivit}
are proposed to reduce the classification accuracy gap between BNNs and their FP counterparts.
However, research on BNNs for SR is relatively scarce.
Table~\ref{tab:related work} lists some representative works.
It is worth noting that they are all designed for CNN-based networks.
\cite{ma2019efficient} first introduces binarization to SR networks and reduces the model size of FP SRResNet.
However, they only binarize weights and leave activations at FP, which impedes the bit-wise operation and requires expensive FP accumulations.
BAM \cite{xin2020binarized} binarizes both weights and activations using a bit-accumulation mechanism to approximate the FP convolution.
They binarize weights and activations in each layer based on the accumulation of previous layers, 
which introduces extra FP accumulation during inference.
BTM \cite{jiang2021training} finds that BN in BNNs introduces a lot of FP calculations.
They design a binary training mechanism to normalize input LR images and build a BNN without BN, named IBTM.
LMB \cite{li2022local} calculates the threshold for each pixel by averaging its neighborhood pixel values, which increases computation significantly for calculating per-pixel threshold.
DAQ \cite{hong2022daq} proposes a per-channel activation quantization method to adapt the diverse channel-wise distributions.
However, it introduces large FP computations 
for calculating the mean and standard deviation of each channel of activations.
E2FIF~\cite{lang2022e2fif} proposes an end-to-end full-precision information flow to improve the performance of BNN.
However, these methods still exhibit a performance gap compared to their FP counterparts because they fail to capture the large activation variations.
Moreover, how to binarize Transformer-based SR networks remains an unresolved issue.
% However, they introduce BN after each convolution of the recent SR networks, in which there is no BN originally, thus harming the performance of BNNs for SR. 

% ABC-Net \cite{lin2017towards} approximates FP weights with the linear combination of multiple binary weight bases and employs multiple binary activations to reduce the quantization error.
% Bi-Real Net \cite{liu2018bi} proposes a double residual topology and a tight approximation to the derivative of the sign function.
% Real-to-Binary Net \cite{martinez2020training} proposes a two-step training strategy and a data-driven channel re-scaling for BNNs.
% ReActNet \cite{liu2020reactnet} further improves the accuracy by generalizing the traditional Sign and PReLU functions to RSign and RPReLU.
% AdaBin \cite{tu2022adabin} uses adaptive binary sets instead of a fixed set, i.e., $\{-1,1\}$ to increase the representation ability of BNNs.

% There are also works on BNNs for SR as listed in Table~\ref{tab:related work}.
% Different from the image classification, SR focuses more on the details of the image for reconstruction.
% % Good quantization methods for SR networks should maintain detailed information during quantization, however, there are few works that achieve this.
% However, existing binarization methods in BNNs for SR lack the ability to capture detailed information and require expensive hardware cost.
% \cite{ma2019efficient} first introduces binarization to SR networks and reduces the model size of FP SRResNet.
% However, they only binarize weights and leave activations at FP, which impedes the bit-wise operation and requires expensive FP accumulations.
% BAM \cite{xin2020binarized} binarizes both weights and activations in SR networks utilizing a bit-accumulation mechanism to approximate the FP convolution.
% They use a value accumulation scheme to binarize weights and activations in each layer based on the previous layers.
% However, their method introduces extra FP BN computation during inference.
% BTM \cite{jiang2021training} finds that BN in BNNs introduces a lot of FP calculations and is unfriendly to low-precision hardware. 
% They explore a binary training mechanism to initialize weights and normalize input LR images, and build a BNN without BN, named IBTM.
% LMB \cite{li2022local} calculates the threshold for each pixel by averaging its neighborhood pixel values, which however increases computation significantly for calculating per-pixel threshold.
% DAQ \cite{hong2022daq} proposes per-channel activation quantization adaptive to diverse channel-wise distributions, which still introduces large FP computations 
% for calculating the mean and standard deviation of each channel of activations.
% Recently  E2FIF \cite{lang2022e2fif} proposes using end-to-end full-precision information flow in BNN and develops a high-performance BNN.
% However, they introduce BN after each convolution of the recent SR networks, in which there is no BN originally, thus harming the performance of BNNs for SR. 
% The performance gap between the current BNNs and their FP counterparts is still large.

\begin{figure}[!tb]
\centering
\includegraphics[width=0.45\textwidth]{fig/SR_network_arch.pdf}
\caption{The typical architecture of CNN-based and
Transformer-based SR networks and the detailed structure of basic blocks.
} 
\label{fig:network_arch}
\end{figure}

\begin{table}[!tb]
\centering
\caption{Comparison between existing BNNs for SR and our work
on the spatial, channel-wise, layer-wise, image-wise adaptability,
and the hardware cost. 
% The definition of per-channel and per-tensor quantization can be referred to \cite{nagel2021white}. ‘Adaptive Quant’ means whether the quantization scheme is input dependent during inference, and if yes, on which dimension Adaptive Quant is adopted.
% The spatial, channel, and image adaptability indicate whether the binarization methods are able to adjust according to variations in these three dimensions and capture detailed information.
% We compare them in terms of the spatial, channel-wise, layer-wise and image-wise adaptability of the quantization method, 
% and the hardware cost.
}
\label{tab:related work}

\scalebox{0.7}{


\begin{tabular}{c|ccccccc}
\hline
Method                          & Spa. Adpt.    & Chl. Adpt.     & Layer Adpt.     & Img. Adpt.             & HW cost \\ \hline
\cite{ma2019efficient}          & \textcolor{darkred}{\ding{55}}     & \textcolor{darkred}{\ding{55}}      & \textcolor{darkred}{\ding{55}}       & \textcolor{darkred}{\ding{55}}            & FP Accum. \\
BAM \cite{xin2020binarized}     & \textcolor{darkgreen}{\ding{52}}     & \textcolor{darkred}{\ding{55}}      & \textcolor{darkred}{\ding{55}}       & \textcolor{darkred}{\ding{55}}            & Extra FP Accum. \\
BTM \cite{jiang2021training}    & \textcolor{darkred}{\ding{55}}     & \textcolor{darkred}{\ding{55}}      & \textcolor{darkred}{\ding{55}}       & \textcolor{darkgreen}{\ding{52}}            & Low \\
LMB \cite{li2022local}          & \textcolor{darkgreen}{\ding{52}}     & \textcolor{darkred}{\ding{55}}      & \textcolor{darkred}{\ding{55}}       & \textcolor{darkgreen}{\ding{52}}            & FP Accum. \\
DAQ \cite{hong2022daq}          & \textcolor{darkred}{\ding{55}}     & \textcolor{darkgreen}{\ding{52}}      & \textcolor{darkred}{\ding{55}}       & \textcolor{darkgreen}{\ding{52}}          & FP Mul. and Accum. \\
E2FIF \cite{lang2022e2fif}      & \textcolor{darkred}{\ding{55}}    & \textcolor{darkred}{\ding{55}}    & \textcolor{darkred}{\ding{55}}      & \textcolor{darkred}{\ding{55}}           & Low \\
\textbf{SCALES (ours)}          & \textcolor{darkgreen}{\ding{52}}    & \textcolor{darkgreen}{\ding{52}}    & \textcolor{darkgreen}{\ding{52}}       & \textcolor{darkgreen}{\ding{52}}         & Low    \\ \hline
\end{tabular}

}
\end{table}


% What's more, there is a still large performance gap between the current BNNs and their FP counterparts.
% None of the binarization methods above have the ability to capture the pixel-to-pixel, channel-to-channel, and image-to-image variation of the activation distribution.
% In this work, we use spatial re-scaling and channel-wise shift and re-scaling to adapt to the various data distributions in SR networks and further bridge the performance gap between BSR and FP SR networks.

% Re-scaling the output of binary convolutions was proposed at the birth of BNN in XNOR-Net \cite{rastegari2016xnor} to reduce quantization error and improve accuracy for image classification tasks.
% It is computed as below:
% \begin{equation}
% \mathcal{A} * \mathcal{W} \approx(\operatorname{sign}(\mathcal{A}) \circledast \operatorname{sign}(\mathcal{W})) \odot \mathcal{K} \alpha
% \label{eq:xnor-net rescale}
% \end{equation}
% where $\circledast$ denotes the binary convolution and $\odot$ denotes the element-wise multiplication.
% $\mathcal{A}$, $\mathcal{W}$, $\alpha$, and $\mathcal{K}$ denote the activation, weight, weight scaling factor, and activation scaling factor, respectively.
%  Later in XNOR-Net++ \cite{bulat2019xnor}, Bulat et al. fuse the activation and weight scaling factors into a single one that is learned end-to-end based on gradients and this improves the classification accuracy on ImageNet dataset.
% % It is computed as Eq.~\ref{eq:xnor-net rescale}, where $\circledast$ denotes 
% %  the binary convolution and $\odot$ denotes the element-wise multiplication. The binary convolution of $\mathcal{A}$ and $\mathcal{W}$ is rescaled by the weight scaling factor $\alpha$ and the activation scaling factor $\mathcal{K}$, both of which are calculated analytically.
% % \zc{Similarly, you should explain the meaning of A, W and the operators $\circledast$ in the formula}
% Then in Real-to-binary Net \cite{martinez2020training}, Martinez et al. used a data-driven channel re-scaling module that takes the pre-convolution activations as input to predict the activation scaling factor. Unlike that in XNOR-Net++ \cite{bulat2019xnor}, these scaling factors are not fixed during inference but rather inferred from data. By doing this, they further improved the classification accuracy on ImageNet over XNOR-Net++. 