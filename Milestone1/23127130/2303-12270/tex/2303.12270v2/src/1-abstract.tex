
\begin{abstract}
Deep neural networks for image super-resolution (SR) have demonstrated superior performance.
However, the large memory and computation consumption hinders their deployment on resource-constrained devices.
Binary neural networks (BNNs), which quantize the floating point weights and activations to 1-bit
can significantly reduce the cost. 
Although BNNs for image classification have made great progress these days, 
existing BNNs for SR still suffer from a large performance gap between the FP SR networks.
To this end, we observe the activation distribution in SR networks and find much larger pixel-to-pixel, channel-to-channel, layer-to-layer, and image-to-image variation in the activation distribution than image classification networks.
However, existing BNNs for SR fail to capture these variations that contain rich information for image reconstruction, leading to inferior performance.
To address this problem, we propose SCALES, a binarization method for SR networks
that consists of the layer-wise scaling factor,
the spatial re-scaling method, and the channel-wise re-scaling method,
capturing the layer-wise, pixel-wise, and channel-wise variations efficiently in an input-dependent manner.
We evaluate our method across different network architectures and datasets.
For CNN-based SR networks, our binarization method SCALES outperforms the prior art method by 0.2dB
with fewer parameters and operations.
With SCALES, we achieve the first accurate binary Transformer-based SR network,
improving PSNR by more than 1dB compared to the baseline method.

\end{abstract}

\begin{IEEEkeywords}
Binary neural network, image super-resolution, layer-wise scaling factor,
spatial re-scaling, channel-wise re-scaling
\end{IEEEkeywords}