\section{Experiments}





\begin{figure}
    \centering
    \includegraphics[width=.95\columnwidth]{img/hist_split2.pdf}
    \caption{Histogram empirical distribution generated and optimized compliances for task-2 (unknown constraints). We see that the generated topologies match well the distribution of compliance for unseen constraint configurations.
    }
    \label{fig:distro-compliance}
\end{figure}

\paragraph{Setup.}
We train all the models for 200k steps on 30000 optimized topologies on a 64x64 domain. 
We set the hyperparameters, conditioning structure, and training routine as proposed in~\citep{maze2022topodiff}.
For all the models we condition on volume fraction and loads.
For TopoDiff we condition on additional stress and energy fields.
For TopoDiff-FF we condition on boundary conditions and kernels.
TopoDiff-GUIDED leverages a compliance regressor and floating material classifier guidance (CLS+REG).
We use a reduced number of sampling steps for all the experiments. See Table~\ref{tab:table-constraints} for an overview of conditioning and guidance variables used for different models and methods considered in this work.

\paragraph{Dataset.}
We use the dataset of optimized topologies gathered using SIMP proposed in~\cite{maze2022topodiff}.
\begin{table}[ht]
    \centering
    \begin{tabular}{c|cccc}
         Cut-off & 10     & 25     & 50     & 100    \\
         \toprule
         $F(x)$  & 0.9871 & 0.9963 & 0.9985 & 0.9990 \\
    \end{tabular}
    \caption{Cumulative Density Function for compliance cut-off on the train set. We can see that more than 99.90\% of the training set has compliance lower than 100.}
    \label{tab:t-F(t)}
\end{table}
For each topology, we have information about the loading condition, boundary condition, volume fraction, and optimal compliance. Additionally, for each configuration, a pre-processing step computes the force and strain energy fields (see Fig.~\ref{fig:kernel-field}). 

\paragraph{Evaluation.}
We employ both engineering and generative metrics to evaluate the effectiveness of our model. Our metrics encompass physical, engineering, and modeling requirements to determine the model's overall performance.
We measure the Volume Fraction Error (VFE), which calculates the deviation between the generated topology volume and the prescribed volume. We also use Floating Material (FM) to assess the manufacturability of the model.
For performance, we use the structural compliance metric~\cite{sigmund2013topology}, which measures the overall displacement under specified constraints. Minimizing compliance is one of the primary objectives of topology optimization. We calculate the average and median compliance (Avg C, Mdn C) for all tasks.
We also use the Compliance Error (CE) to evaluate the engineering performance, which determines the deviation between the compliance calculated by the SIMP optimization method and the compliance generated by the diffusion model. Additionally, we analyze the inference time, which includes preprocessing and sampling time, to determine the model's speed in creating fast concept designs.
To assess the performance of the generative models, we design three test tasks: task-1, task-2, and task-3. We divide the test configurations into various groups based on their constraints and expected optimal performance. Table~\ref{tab:task} presents these tasks.
We notice that all the models have null load disrespect (LD), showing that fulfilling this hard constraint is easy for deep generative models.

\begin{itemize}
    \item Task-1: the constraints in the task-1 test set are identical to those in the training set. We filter out generated configurations with high compliance when measuring performance on this task.
    \item Task-2: the constraints in the task-2 test set differ from those in the training set. We filter out generated configurations with high compliance when measuring performance on this task.
    \item Task-3: the constraints in the task-3 test set are the same as those in task-2. We consider all the generated configurations with low and high compliance when measuring performance on this task.
\end{itemize}

The purpose of these tasks is to evaluate the generalization capability of the machine learning models in- and out-of-distribution. By testing the models on different test sets with varying levels of difficulty, we can assess how well the models can perform on new, unseen data.
More importantly, we want to understand how important the role of the force field and energy strain is with unknown constraints.






\begin{table}[ht]
    \centering
    \begin{tabular}{c | c c c c}
        task & constraints & performance & avg C & mdn C \\
        \midrule
        task-1 & in-distro & in-distro  & 7.73 & 5.54  \\
        task-2 & out-distro & in-distro & 6.84 & 5.29  \\
        task-3 & out-distro & out-distro & 6.85 & 5.31 \\
    \end{tabular}
    \caption{Tasks in order of challenge for the model. 
    We expect the models to perform well on task-1, acceptably on task-2, and fail on task-3. We also report the average and median compliance over the different sets as a measure of optimality for the generated topologies.}
    \label{tab:task}
\end{table}

\subsection{Performance with In-Distribution Constraints}
\label{subsection:in-distro}
\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{c|  c c c c c  c c c c c c c}
        & Task & Steps  & Constraints & Guidance & Avg C $\downarrow$ & Mdn C $\downarrow$ & \%~CE $\downarrow$ & \%~VFE $\downarrow$ & \%~FM $\downarrow$ & Sampling $\downarrow$ & Processing $\downarrow$ &  Inference $\downarrow$ \\
        \midrule
        TopoDiff              & task-1  & 100 & FIELD  & COND   & 8.01 & 5.60  & \underline{5.46} & \underline{1.47}  & \underline{5.79} & 2.23 & 3.31 & 5.54        \\
        TopoDiff-GUIDED       & task-1  & 100 & FIELD  & CLS+REG & 8.15 & 5.63 & 5.93 & 1.49 & 5.82  &  2.46 & 3.31 & 5.87 \\
        TopoDiff-FF           & task-1  & 100 & KERNEL & COND    & 9.76 & 6.31 & 24.90 & 2.05 & 8.15 &  2.23 & 0.12 & \textbf{2.35} \\
        TopoDiff-FF + SIMP (10) & task-1  & 100 & KERNEL & COND & 8.05 & 5.77 & \textbf{4.16} & \textbf{1.16} & \textbf{5.61} & 2.23 & 0.32 & \underline{2.55} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Evaluation of different model variants on in-distribution constraints. We remove samples with compliance >100.
    C: compliance.
    CE: compliance error. VFE: volume fraction error. FM: floating material. We use 100 sampling steps for all models. SIMP (10) means that we run SIMP for 10 iterations. Results averaged over 5 runs.
    For a general overview with confidence intervals see the left side of Fig.~\ref{fig:barplots}.
    }
    \label{tab:constraints-in-distro}
\end{table*}
Table~\ref{tab:constraints-in-distro} presents an evaluation of different model variants on in-distribution constraints for Task 1, with various measures of compliance, compliance error, volume fraction error, and floating material. Lower is better for all the metrics.
The models evaluated include TopoDiff, TopoDiff-GUIDED, TopoDiff-FF, and TopoDiff-FF + SIMP (10), with varying types of constraints and guidance. 
TopoDiff and TopoDiff-GUIDED condition of stress and energy fields, with TopoDiff-GUIDED leveraging surrogate models for guidance (classifier and regressor); TopoDiff-FF conditions on kernel relaxation.
The table shows that TopoDiff and TopoDiff-GUIDED have lower values for most measures of compliance, compliance error, volume fraction error, and floating material compared to TopoDiff-FF. 
The TopoDiff-FF model has higher compliance error as expected but comparable VFE and FM, indicating that a lightweight kernel relaxation is a viable conditioning option.
Additionally, TopoDiff-FF + SIMP (10) has the lowest values for compliance error, volume fraction error, and floating material.
The table also reports the time taken for sampling, pre-processing, and inference for each model. TopoDiff-FF, even reducing the number of sampling steps for TopoDiff to 100, has significantly lower times for pre-processing and overall inference, because of the absence of FEA pre-processing.
Overall, the results suggest that the TopoDiff and TopoDiff-GUIDED models perform better than TopoDiff-FF on in-distribution constraints, with TopoDiff-FF (and TopoDiff-FF+SIMP) being much faster at sampling and a viable data-driven alternative when FEA is impractical or computationally unfeasible.
The results for TopoDiff-FF+SIMP suggest that using a combination of deep generative models and a simplified iterative method (SIMP) can lead to better results in terms of compliance and design requirements, even if the gap with TopoDiff is not so relevant when dealing with the in-distribution scenario.

\subsection{Performance with Out-of-Distribution Constraints}
\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{c|c c c c c c c c c c c c}
        & Task & Steps & Constraints & Guidance & Avg C $\downarrow$ & Mdn C $\downarrow$  & \%~CE $\downarrow$  & \%~VFE $\downarrow$ & \%~FM $\downarrow$ & Sampling $\downarrow$ & Processing $\downarrow$ &  Inference $\downarrow$ \\
        \midrule
        TopoDiff            & task-2 & 100 & FIELD & COND & 7.80 & 5.49 & 12.02 &  1.49 & \underline{6.65} & 2.23 & 3.31  & 5.54      \\
        TopoDiff-GUIDED     & task-2 & 100 & FIELD    & CLS+REG & 7.80 & 5.57 & \underline{10.55}  & \underline{1.47} & 7.39 & 2.46  & 3.31 & 5.87 \\
        TopoDiff-FF         & task-2 & 100 & KERNEL & COND & 11.65 & 6.94 & 58.36 &  1.97 & 7.86 & 2.23  & 0.12 & \textbf{2.35}  \\
        TopoDiff-FF + SIMP (10)    & task-2 & 100 & KERNEL & COND & 7.65 & 5.70 & \textbf{7.84} & \textbf{1.29} & \textbf{6.53} & 2.23  & 0.32 & \underline{2.55} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Evaluation of different model variants on out-of-distribution constraints. We remove samples with compliance >100.
    C: compliance.
    CE: compliance error. VFE: volume fraction error. FM: floating material. We use 100 sampling steps for all models. SIMP (10) means that we run SIMP for 10 iterations. Results averaged over 5 runs.
    For a general overview with confidence intervals see the right side of Fig.~\ref{fig:barplots}.}
    \label{tab:constraints-out-distro}
\end{table*}
Table~\ref{tab:constraints-out-distro} reports results on task-2 with out-of-distribution constraints. The considered models, conditioning, and guidance mechanisms are the same as explained in Subsec.~\ref{subsection:in-distro}.
The results show that in out-of-distribution scenarios, TopoDiff-GUIDED model performs better than the TopoDiff, achieving a lower CE and VFE. TopoDiff-FF performs well in terms of VFE and FM, but poorly in terms of compliance, with high CE. However, the TopoDiff-FF + SIMP (10) model significantly improves the performance on Task-2, achieving the lowest CE, VFE, and FM values among all models.
As for task-1, the inference time for TopoDiff-FF and TopoDiff-FF+ SIMP is much better than for the full model.
Overall, the table shows that the use of guidance and optimization techniques, such as SIMP, can significantly improve the performance of topology optimization models, especially on out-of-distribution constraints. See Fig~\ref{fig:barplots} for a visual overview with confidence intervals over five runs.

\subsection{Performance with Out-of-Distribution Constraints and Outliers}
Table~\ref{tab:constraints-out-distro-out-performance} presents results on task-3 with out-of-distribution and without filtering out outliers, i.e configurations with high-compliance (low performance).
High-compliance configurations (>50/100) are out-of-distribution performance for the training set compliance distribution. In particular in Table~\ref{tab:t-F(t)} where we show that more than 99.8\% of the configurations have compliance lower than 50. This means that when we obtain generated samples with such high compliance, the model has partially failed to fulfill the design constraints.
The considered models, conditioning, and guidance mechanisms are the same as explained in Subsec.~\ref{subsection:in-distro}.
\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{c|c c c c c c c c c c c c c}
        & Task & Steps & Constraints & Guidance & Avg C $\downarrow$ & Mdn C $\downarrow$  & \%~CE $\downarrow$ & \%~VFE $\downarrow$ & \%~FM $\downarrow$ & Sampling $\downarrow$ & Processing $\downarrow$ &  Inference $\downarrow$ \\
        \midrule
        TopoDiff              & task-3 & 100 & FIELD   & COND    & 8.91 & 5.52 & 31.04 & 1.49 & \underline{6.61} & 2.23 & 3.31 & 5.54  \\
        TopoDiff-GUIDED       & task-3 & 100 & FIELD    & CLS+REG & 8.23 & 5.58 & \underline{17.69}  & \underline{1.47} & 7.36 & 2.46 & 3.31 & 5.87 \\
        TopoDiff-FF & task-3 & 100 & KERNEL & COND    & 19.01 & 7.22 & 144.23 &  2.00 & 8.70  & 2.23 & 0.12 & \textbf{2.35}  \\
        TopoDiff-FF + SIMP (10)  & task-3 & 100 & KERNEL & COND & 7.65 & 5.70 & \textbf{7.84} & \textbf{1.29} & \textbf{6.53} & 2.23  & 0.32 & \underline{2.55}  \\
        \bottomrule
    \end{tabular}
    }
    \caption{Evaluation of different model variants on out-of-distribution constraints. We do not filter out samples with high compliance.
    C: compliance.
    CE: compliance error. VFE: volume fraction error. FM: floating material. We use 100 sampling steps for all models. SIMP (10) means that we run SIMP for 10 iterations. Results averaged over 5 runs.}
    \label{tab:constraints-out-distro-out-performance}
\end{table*}
The results in this table are similar to task-2, where TopoDiff-GUIDED performs better than TopoDiff in terms of CE and TopoDiff-FF achieves high compliance. However, using TopoDiff-FF+SIMP we not only improve all the metrics, but we completely get rid of such outlier configurations, being easy for the optimization process to fix the generated high-compliance configurations in a few iterations. Contrarily, for all other methods, there are always outliers in terms of topologies with high compliance that cannot easily be fixed, making a strong case for unifying deep generative models and optimization for topology optimization in challenging scenarios.

\subsection{Compliance Analysis}
Here we discuss and study how compliance influences the generated topology quality.
Compliance magnitude seems to be the most important factor for the generation quality and manufacturability of proposed designs.
In Fig.~\ref{fig:distro-compliance} we show qualitative information, providing empirical compliance distribution for generated vs optimized topologies. In Fig.~\ref{fig:compliance-mean-median-level2} and Fig.~\ref{fig:compliance-threshold}
The growth of compliance error with compliance indicates that the model struggles to generate high-compliance topologies that fulfill the design requirements. This is further supported by the fact that very few configurations have high compliance, as indicated by the dataset. These findings highlight the difficulty of modeling compliant structures with high accuracy and precision and suggest that additional techniques or modifications to the model may be necessary to address this challenge. Moreover, the limited ability of the model to generate high-compliance topologies could have important implications for real-world applications, where compliant structures are often critical components of engineering designs. Therefore, improving the model's ability to generate high-compliance topologies is an important research direction for advancing the state-of-the-art in this field.


\paragraph{High-Compliance Configurations.}
High-compliance configurations are often associated with mechanically unstable structures, meaning that they can collapse or deform easily under load. As a result, practical engineering designs often prioritize achieving low-compliance solutions that are stable, efficient, and lightweight. However, it is still important to study high-compliance configurations as they can provide valuable insights into the behavior and limitations of structural systems, as well as inspire new design concepts and approaches. We show some examples of high-compliance and high-compliance errors in Fig.~\ref{fig:high-compliance}. For low-compliance and low-compliance errors, see Fig.~\ref{fig:low-compliance}.


\begin{figure}[ht]
    \centering
\includegraphics[width=.85\columnwidth]{img/thresh.pdf}
    \caption{Mean and Median Compliance error for different compliance thresholds.
    We see that more than 99.8~\% of the data has compliance lower than 50 and how to mean compliance error increases with compliance, where the median compliance error plateau. }
    \label{fig:compliance-threshold}
\end{figure}



\begin{figure}[ht]
    \centering
\includegraphics[width=.85\columnwidth]{img/mean_median_compliance.pdf}
\quad%
    \caption{Mean and Median Compliance vs \% train set for task-2 (unknown) constraints.
    The generated topologies using TopoDiff perform well for low compliance values.
    Most of the loss in terms of compliance happens for high compliance values.
    }
    \label{fig:compliance-mean-median-level2}
\end{figure}


\subsection{Inference Time}
Where in~\cite{maze2022topodiff} has been shown that TopoDiff-based models largely outperform topologyGAN-based models~\cite{nie2021topologygan} on design performances (CE, VFE, FM) for in- and out-distribution scenarios, inference time for GAN-based model is still much better than for Diffusion-based models. 
\begin{figure}[ht]
\includegraphics[width=.85\columnwidth]{img/topology_sampling.pdf}
    \caption{Sampling time. In this figure, we compare the total inference time (pre-processing and sampling time) for TopoDiff, TopologyGAN, and TopoDiff-FF. We can see that we completely remove the pre-processing time component using the kernel relaxation. And reducing the number of sampling steps reduces, even more, the sampling time, making TopoDiff-FF relatively fast at the price of generating configurations with higher compliance than TopoDiff and TopoDiff-GUIDED.}
    \label{fig:sampling}
\end{figure}
Here we present our approach to improving the efficiency of inference for TopoDiff-FF, measured in terms of both pre-processing and sampling time. Specifically, we focus on two key areas of optimization:
I) Reducing the number of steps required for sampling to improve overall sampling time.
II) Leveraging the kernel relaxation technique to reduce pre-processing time by avoiding the computation of force and energy fields.
To demonstrate the effectiveness of our approach, we compare the pre-processing and sampling times of TopoGAN, TopoDiff, and TopoDiff-FF in Fig.~\ref{fig:sampling}. Our results show that TopoDiff-FF is able to generate high-quality samples that satisfy prescribed constraints and exhibit strong performance, all while requiring significantly less time for both pre-processing and sampling compared to TopoDiff and slightly less time than TopoGAN. This improved efficiency is a key goal of TopoDiff-FF, as it simplifies the conditioning process and enables faster inference.

\subsection{Measuring the Relative Gap in Performance}
To better summarize the results on in-distribution and out-of-distribution constraints, we propose to evaluate the models in terms of the relative design gap between the models, using a global metric that accounts for all the requirements presented in Table~\ref{tab:table-constraints}.
We also consider relative ranking (Avg Rank) between the considered models.
The gap and the rank are global metrics that account for compliance, compliance error, volume fraction error, floating material, processing, and inference time. For the average gap (Avg Gap), we use average compliance and compliance error. For the median gap (Mdn Gap), the corresponding median values.
The design gap is a proxy to quantify the distance from an optimal design, where all the requirements are perfectly satisfied, we have compliance error null, and inference time is negligible.
Table~\ref{tab:table-constraints} presents a comparison of different conditioning configurations for generative designs in terms of their performance on task-1 and task-2. The table also shows whether the configuration includes kernels and field constraints.
Overall, the models perform similarly in terms of the average and median gap on task-1, with TopoDiff-FF+SIMP (10) performing the best overall. On task-2, TopoDiff-FF loses in terms of the average gap because of the large CE on out-of-distribution. Interestingly, when considering average ranking, the gap is reduced because of the fast processing and inference time for TopoDiff-FF.
As noted with the previous experiments, TopoDiff-FF with SIMP outperforms all other models in terms of both design quality and relative performance on both tasks, which suggests that the use of SIMP in conjunction with deep generative models is a promising approach for generating high-quality designs for topology optimization.

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{3pt}
    \resizebox{0.99\columnwidth}{!}{
    \begin{tabular}{c| c c c c c c c c c c c}
                    & Task & Kernels  & Fields & Mdn Gap $\downarrow$ & Avg Gap $\downarrow$ & avg Rank $\downarrow$ \\
        \midrule
        TopoDiff           & task-1   & \xmark & \cmark  & 3.12  & 4.38 & 2.2\\
        TopoDiff-GUIDED    & task-1   & \xmark & \cmark & 3.16  & 4.52 & 3.2\\
        TopoDiff-FF        & task-1   & \cmark & \xmark  & 4.37  & 7.86 & 2.8\\
        TopoDiff-FF + SIMP & task-1   & \cmark & \xmark  & \textbf{2.69}  & \textbf{3.57} & \textbf{1.6} \\
\midrule
        TopoDiff             & task-2      & \xmark & \cmark  &  3.35 & 5.58  & 2.2  \\
        TopoDiff-GUIDED      & task-2      & \xmark & \cmark  &  3.55 & 5.54  & 3.2 \\
        TopoDiff-FF          & task-2      & \cmark & \xmark  &  5.86 & 13.69 & 2.8 \\
        TopoDiff-FF + SIMP   & task-2      & \cmark & \xmark  &  \textbf{3.08} & \textbf{4.27}  & \textbf{1.6} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Global overview of different conditioning configurations showing how far the generative designs are from an ideal optimal design. On in-distribution configurations, our approach is extremely effective. On out-of-distribution configuration there is still a gap in performance. Adding few steps of SIMP (10 steps) as refinement is effective in improving the design quality.}
    \label{tab:design-gap}
\end{table}


\subsection{Merging Deep Generative Models and Optimization}
Table~\ref{tab:model-simp} shows the results of experiments conducted with three algorithms, SIMP, TopoDiff-FF, and TopoDiff-FF + SIMP, and tested on a particular data split. The table presents the values of two metrics, average C and Compliance Error, for each algorithm, with varying numbers of iterations. It is clear that increasing the number of iterations resulted in improved performance for all algorithms. Moreover, the combination of TopoDiff-FF and SIMP performed better than the other algorithms, achieving the lowest values of average C and CE in both experiments. These results suggest that using the combination of TopoDiff-FF and SIMP is a way to impose the performance constraints in the model without the need for surrogate models or guidance. Also, we are sure that some physical properties are respected, reducing the FM, VFE, and CE using a fast and relatively cheap refinement (5/10 iterations).

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{c|c c c c }
    \setlength{\tabcolsep}{3pt}
        & Task & Iter & Avg C $\downarrow$ &  \% CE $\downarrow$ \\
        \midrule
        SIMP                    & task-2  & +5 & 9.87           & 35.13 \\
        TopoDiff-FF     & task-2  & -  & 19.01          & 58.36 \\
        TopoDiff-FF + SIMP      & task-2  & +5 & \textbf{8.67}  & \textbf{20.34} \\
        \midrule
        SIMP                    & task-2  & +10 & 8.57           &  17.61 \\
        TopoDiff-FF     & task-2  & -   & 19.01          & 58.36  \\
        TopoDiff-FF + SIMP      & task-2  & +10 & \textbf{7.65}  & \textbf{7.84} \\
        \midrule
    \end{tabular}
    \caption{Comparison of SIMP, TopoDiff-FF and TopoDiff-FF + SIMP. We see that by merging together the topology generated using a conditional diffusion model and SIMP for refinement we obtain the best results in terms of average compliance and compliance error with just 5 or 10 SIMP iterations.}
    \label{tab:model-simp}
\end{table}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=.95\linewidth]{img/figures_general/bar_split1.pdf}
    \includegraphics[width=.95\linewidth]{img/figures_general/bar_split2.pdf}
    \caption{Overview design requirements for the considered models for task-1 (in-distribution constraints, top) and task-2 (out-of-distribution constraints, bottom).
    From left to right: average compliance in blue. Median compliance in orange. Compliance error in green. Volume fraction error in red. Floating material in purple. Inference time in brown. 
    Lower is better for all the metrics.}
    \label{fig:barplots}
\end{figure*}

\subsection{Manufacturability}
\begin{figure}[ht]
    \centering
\includegraphics[width=.85\columnwidth]{img/level2_manufacturability.pdf}
    \caption{Manufacturability as measured in terms of absence of floating material and load disrespect, and compliance lower than 100 for out-of-distribution constraints.}
    \label{fig:manufacturability}
\end{figure}
We define a topology as manufacturable if it satisfies all constraints, has no floating material, and compliance is below 100. 
In Figure~\ref{fig:manufacturability}, we depict how manufacturability, measured as the percentage of configurations that meet these criteria, varies with performance. We evaluate out-of-distribution constraints and compare models trained on raw loads and boundary conditions with those trained on kernel relaxation. Our results show that manufacturability improves with performance, i.e., lower average compliance, of generated topologies. Notably, we observed a higher fraction of manufacturable topologies when conditioning on the kernel relaxation, supporting our hypothesis that the kernel is an effective approximation for loads and boundaries.


\paragraph{Kernel Ablation.}

The table presents the results of a kernel ablation study for the TopoDiff-FF model. We train the models using short runs. Four different inverse distance kernels with different exponents are evaluated, and their performance is measured in terms of compliance error (CE), volume fraction error (VFE), floating material (FM), load disrespect (LD), and the percentage of compliant samples with a compliance value smaller than 100 ($conf|{c<100}$). Lower values for CE, VFE, FM, and LD indicate better performance, while higher values for $conf|{c<100}$ indicate better compliance with the given constraints. The results show that the $1/r^2$ kernel performs well overall, with the lowest CE and VFE values and the highest $conf|{c<100}$ percentage when learning the power using $\beta$. The $1/r^{\beta}$ kernel also performs well, with the lowest FM value, while the $1/r$ kernel performs the worst in all metrics.

\begin{table}[ht]
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c|c c c c c}
                     & CE $\downarrow$ & VFE $\downarrow$ & FM $\downarrow$ & LD $\downarrow$ & $conf|_{c<t}$ $\uparrow$         \\
    \midrule
       $K(1/r)$         & 189.76 & 15.54 & 8.60 & 1.4 & 48.80 \\
       $K(1/r^2)$       & 47.62 & 2.38 & 9.23  & 0.0 & 94.32  \\
       $K(1/r^4)$       & 55.01 & 2.35 & 8.15 & 0.0  & 95.90   \\
       $K(1/r^{\beta})$ &  36.20 & 2.22 & 8.63 & 0.0 & 95.66  \\
       \bottomrule
    \end{tabular}
    \caption{Kernel Ablation for TopoDiff-FF. All metrics in $\%$. We train models for a shorter time on a subset of the training data.}
    \label{tab:kernel-ablation}
\end{table}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.9\linewidth]{img/figures_general/bad_cases.pdf}
    \caption{Examples of generated topologies with bad performance.}
    \label{fig:high-compliance}
\end{figure*}