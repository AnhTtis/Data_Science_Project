\section{Background}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.9\linewidth]{img/figures_general/topodiff_ff_simp_intro.pdf} 
    \caption{TopoDiff-FF + SIMP. 
    The TopoDiff-FF pipeline is a conditional diffusion model where we condition on a cheap kernel relaxation instead of using expensive FEA to obtain the stress and energy field like in TopoDiff.
    After the generation step, we can improve the generated topology using few steps of SIMP (5/10 iterations) to remove floating material and explicitly optimize performance (minimize structural compliance).}
    \label{fig:topodiff_ff_simp}
\end{figure*}

Here we briefly introduce the Topology Optimization problem~\cite{bendsoe1988generating}, diffusion models~\citep{dickstein2015}, a class of deep generative models, conditioning and guidance mechanisms for diffusion models, and recent deep generative models for topology optimization~\cite{nie2021topologygan, maze2022topodiff}. 

\paragraph{The Topology Optimization Problem.}
Topology optimization (TO) is a powerful computational design approach used to determine the optimal configuration of a given structure, given a set of constraints. The goal of topology optimization is to identify the most efficient use of material while ensuring that the structure satisfies specific performance requirements.
One common approach to topology optimization is the SIMP (Solid Isotropic Material with Penalization) method~\citep{bendsoe1989optimal}. The SIMP method involves modeling the material properties using a density field, where the density represents the proportion of the material present in a given region. The optimization process involves adjusting the density field iteratively, subject to various constraints such as stress and boundary conditions.
\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{4pt}
    \small
    \begin{tabular}{c|c c c c c c c c}
                    & Load & BC & Kernel Load & Kernel BC & Force Field & Energy Field & VF & Performance \\
        \midrule
        SIMP~\cite{bendsoe1988generating} & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \cmark & \cmark \\
        TopologyGAN~\cite{nie2021topologygan}       & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark & \xmark  \\
        TopoDiff~\citep{maze2022topodiff}          & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark & \xmark  \\
        TopoDiff-GUIDED~\citep{maze2022topodiff} & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark & \cmark  \\
        TopoDiff-FF (ours)       & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark  \\
        TopoDiff-FF + SIMP (ours)       & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark  \\
        \bottomrule
    \end{tabular}
    \caption{Conditioning and guiding variables for different optimization methods and model configurations. All models condition directly or indirectly on loads, boundary conditions, and volume fraction. TopologyGAN, TopoDiff, and TopoDiff-GUIDED have additional conditioning on stress and energy fields, where TopoDiff-FF conditions are on the kernels. TopoDiff-GUIDED, TopoDiff-FF + SIMP, and SIMP itself are guided by a measure of performance.
    }
    \label{tab:conditioning-variables}
\end{table*}
The objective of a generic minimum compliance problem for a mechanical system is to
find the material density distribution $\vx \in \mathbb{R}^n$ that minimizes the
structureâ€™s deformation under the prescribed boundary and
loads condition~\citep{sigmund2013topology, liu2014efficient}. Given a set of design variables $\vx = \{\vx_i\}^n_{i=0}$, where $n$ is the domain dimensionality (in our case $n = 64 * 64$), the minimum compliance problems (Fig.~\ref{fig:intro_to}) can be written as: 
\begin{equation}
\begin{aligned}
\min_{\vx} \quad & c(\vx) = F^T U(\vx) \\
\textrm{s.t.} \quad & v(\vx) = v^T \vx < \bar v\\
  & 0 \leq \vx \leq 1    \\
\end{aligned}
\end{equation}
where the goal is to find the design variables that minimize compliance $c(\vx)$ given the constraints. $F$ is the tensor of applied loads and $U(\vx)$ is the node displacement, solution of the equilibrium equation $K(\vx) U(\vx) = F$ where $K(\vx)$ is the stiffness matrix and is a function of the considered material. $v(\vx)$ is the required volume fraction. The problem is a relaxation of the topology optimization task, where the design variables are continuous between 0 and 1. A simple thresholding mechanism is employed to assign the presence or absence of material in each node in the design domain.
One significant advantage of topology optimization is its ability to create optimized structures that meet specific performance requirements. However, a major drawback of topology optimization is that it can be computationally intensive and may require significant computational resources. Additionally, some approaches to topology optimization may be limited in their ability to generate highly complex geometries and get stuck in local minima. 

\paragraph{Diffusion Models.}
Let $\vx$ denote the observed data which is either continuous $\vx \in \bR^D$ or discrete $\vx \in \{0,...,255\}^D$. Let $\vz_1, ..., \vz_T$ denote $T$ latent variables in $\bR^D$.
We now introduce, the \emph{forward or diffusion process} $q$, the \emph{reverse or generative process} $p_{\theta}$, and the objective $L$.
The forward or diffusion process $q$ is defined as~\cite{ho2020}:
\begin{align}
    q(\vz_{1:T} | \vx) &= q(\vz_1 | \vx) \prod_{t=2}^T q(\vz_t | \vz_{t-1}), \\
    q(\vz_t | \vz_{t-1}) &= \cN(\vz_t | \sqrt{1-\beta_t}~\vz_{t-1}, \beta_t I)
    \label{eq:ddpm_inference_main}
\end{align}
The beta schedule $\beta_1, \beta_2, ..., \beta_T$ is chosen such that the final latent image $\vz_T$ is nearly Gaussian noise.
The generative or inverse process $p_{\theta}$ is defined as:
\begin{align}
    p_{\theta}(\vx, \vz_{1:T}) &= p_{\theta}(\vx | \vz_1) p(\vz_T) \prod_{t=2}^T p_{\theta}(\vz_{t-1}|\vz_t), \\
     p_{\theta}(\vz_{t-1}|\vz_t) &= \cN(\vz_{t-1}|\mu_{\theta}(\vz_t, t), \sigma_t^2 I),
\end{align}
where $p(\vz_T)= \cN(\vz_T| 0, I)$, and $\sigma^2_t$ often is fixed (e.g. to $\sigma^2_t = \beta_t$).
The neural network $\mu_{\theta}(\vz_t, t)$ is shared among all time steps and is conditioned on $t$. 
The model is trained with a re-weighted version of the ELBO that relates to denoising score matching \cite{song2019}.
The negative ELBO $L$ can be written as
\begin{align}
    \bE_q\left[- \log \dfrac{p_{\theta}(\vx, \vz_{1:T})}{q(\vz_{1:T} | \vx)} \right]
    = L_0 + \sum_{t=2}^{T} L_{t-1} + L_T,
    \label{eq:loss_ddpm_main}
\end{align}
where $L_0 = \bE_{q(\vz_1|\vx)} \left[- \log p(\vx |\vz_1) \right]$ is the likelihood term (parameterized by a discretized Gaussian distribution)
and, if $\beta_1,...\beta_T$ are fixed, $L_T = \KL[q(\vz_T|\vx), p(\vz_T)]$ is a constant.
The terms $L_{t-1}$ for $t=2,...,T$ can be written as:

\begin{align}
    L_{t-1} & = \bE_{q(\vz_t|\vx)} \Big[\KL[q(\vz_{t-1}|\vz_t,\vx)\ | \ p(\vz_{t-1}|\vz_t)] \Big] \nonumber \\ 
    & = \bE_{q(\vz_t|\vx)} \left[ \frac{1}{2\sigma_t^2} \|\mu_{\theta}(\vz_t, t) - \tilde{\mu}(\vz_t, \vx)\|_2^2 \right] 
    + C_t \ ,
    \label{eq:Lt_kl}
\end{align}
where $C_t = \frac{D}{2} \left[ \frac{\tilde{\beta}_t}{\sigma_t^2} - 1 + \log \frac{\sigma_t^2}{\tilde{\beta}_t}\right]$.
By further applying the reparameterization trick \cite{kingma2013auto}, the terms $L_{1:T-1}$ can be rewritten as a prediction of the noise $\epsilon$ added to $\vx$ in $q(\vz_t|\vx)$.
Parameterizing $\mu_{\theta}$ using the noise prediction $\epsilon_{\theta}$, we can write
\begin{align}
    \label{eq:Lt_simple}
    L_{t-1, \epsilon} (\vx) &= \bE_{q(\epsilon)} \left[ w_t \|\epsilon_{\theta}(\vz_t(\vx, \epsilon)) -\epsilon\|_2^2 \right],
\end{align}
where $w_t = \frac{\beta_t^2}{2\sigma_t^2\alpha_t (1-\bar{\alpha}_t)}$,
which corresponds to the ELBO objective~\citep{jordan1999introduction}.


\paragraph{Conditioning.} 
Similarly, introducing a conditioning variable $\vc$, conditional modeling can be written as
\begin{equation}
    p_{\vtheta}(\vx, \vz_{1:T} | \vc) = p_{\theta}(\vx | \vz_1, \vc)  p_{\vtheta}(\vz_T | \vc) \prod_{t=2}^T p_{\vtheta}(\vz_{t-1} | \vz_t, \vc),
    \label{eq: conditional-diffusion}
\end{equation}
and we can similarly write the per-layer loss with noise prediction as:
\begin{align}
    \label{eq:Lt_simple}
    L^c_{t-1, \epsilon}(\vx) &= \bE_{q(\epsilon)} \left[ w_t \|\epsilon_{\theta}(
    \vz_t(\vx, \epsilon), \vc) -\epsilon\|_2^2 \right].
\end{align}
We will now discuss different ways to condition the model. 

\paragraph{Guidance.}
Guidance is used to improve sample fidelity vs mode coverage in conditional diffusion models post-training~\cite{dhariwal2021diffusion, ho2021classifierfree}.
Classifier guidance~\cite{dhariwal2021diffusion} is based on bayesian inversion and the fact that inverse problems can be tackled relatively easily in diffusion models. In particular, the goal of the guidance is to shift the model mean $\mu_{\theta}$ closer to the target guidance $\vc$.
The general idea in classifier guidance is to leverage Bayes inversion
\begin{equation}
    p(\vz | \vg) \propto p(\vg | \vz) p(\vz),
\end{equation}
and notice that training the conditional model $p(\vz | \vg)$ is equivalent to training an unconditional model $p(\vz)$ and a classifier $p(\vg | \vz)$. Then the training signal will be a composition of unconditional and conditional scores for the model.
Taking the gradients, we can see that classifier guidance is shifting the unconditional score for each class~\cite{dhariwal2021diffusion}.
We can leverage similar ideas for feasible vs unfeasible designs. With similar reasoning, regression guidance~\cite{maze2022topodiff}, aims to guide the sampling process towards configurations with low regression error as the target. In particular, the regression target for generating topologies is low compliance error between predicted (with a regression model) and real (obtained running a FEM solver) compliance. Guidance is useful to reduce high-compliance configurations generated by the model.


\paragraph{TopoDiff.}
Conditional diffusion models have been adapted for constrained engineering problems with performance requirements.
TopoDiff~\citep{maze2022topodiff} proposes to condition on loads, volume fraction, and fields similarly to~\cite{nie2021topologygan} to learn a constrained generative model (Fig.~\ref{fig:topodiff-intro}). 
In particular, the generative model can be written as: 
\begin{equation}
    p_{\theta}(\vx_{t-1} | \vx_{t}, \vc, \vg) = \mathcal{N}(\vx_{t-1} ~\vert~ \mu_{\theta}(\vx_t, \vc) + \vg_{c} + \vg_{fm}, ~\sigma),
\end{equation}
where $\vc$ is a conditioning term and is a function of the loads $l$, volume fraction $v$, and fields $f$, i.e $\vc = h(l, v, f)$. The fields considered are the Von Mises stress $\sigma_{vm} = (\sigma^2_{11} - \sigma_{11}\sigma_{22} + \sigma^2_{22} + 3\sigma^2_{12})^{1/2}$ and the strain energy density field $W=(\sigma_{11}\epsilon_{11} + \sigma_{22}\epsilon_{22} + 2 \sigma_{12}\epsilon_{12})/2$. 
Here $\sigma_{ij}$ and $\epsilon_{ij}$ are the stress and energy components over the domain. 
$\vg$ is a guidance term, containing information to guide the sampling process toward regions with low floating material (using a classifier and $\vg_{fm}$) and regions with low compliance error, where the generated topologies are close to optimized one (using a regression model and $\vg_c$). Where conditioning $\vc$ is always present and applied during training, the guidance mechanism $\vg$ is optional and applied only at inference time.

\paragraph{Limitations.}
TopoDiff is an effective method for generating topologies that satisfy constraints and have low compliance errors. However, the generative model is computationally expensive as hundreds of layers need to be sampled for each sample. Furthermore, the model requires preprocessing of configurations using a FEM solver, which is also computationally expensive and time-consuming. This approach relies heavily on fine-grained knowledge of the domain input, limiting its applicability to more challenging topology problems.
Additionally, the model requires training of two surrogate models (a classification and regression model) which can be helpful for out-of-distribution configurations. However, to train the regression model, additional optimal and suboptimal topologies are needed assuming access to the desired performance metric on the train set. Similarly, the classifier requires additional labeled data to be gathered.