\section{Background}
\textbf{Latent restoration methods} use neural networks to estimate the parameters $\theta,\phi$ of an encoder $E_{\theta}$ and a decoder $D_{\phi}$. The aim is to restore the input from its lower-dimensional latent representation with minimal loss. The standard objective is to minimize the residual, e.g., using mean squared error (MSE) loss: $\min_{\theta,\phi}\sum_{i=1}^N \| x_i - D_\phi(E_\theta(x_i))\|^2$. In the context of variational inference~\cite{kingma2013auto}, the goal is to optimize the parameters $\theta$ of a latent variable model $p_{\theta}$(x) by maximizing the log-likelihood of the observed samples $x$: $\log p_{\theta}(x)$. The term is intractable, but the true posterior $p_{\theta}(z|x)$ can be approximated by $q_{\phi}(z|x)$:
 \begin{equation}
    \label{eq::elbo}
    \log p_\theta(x)  \geq \mathbb{E}_{q(z|x)} [\log p_\theta(x|z)] - KL[q_\phi(z|x) || p(z)] = ELBO(x).
\end{equation}
KL is the Kullback-Leibler divergence;  $q_\phi(z|x)$ and $p_\theta(x|z)$ are usually known as the encoder $E_\phi$ and decoder $D_\theta$; the prior $p(z)$ is usually the normal distribution $\mathcal{N}(\mu_0, \sigma_0)$; and the ELBO denotes the Evidence Lower Bound. In unsupervised anomaly detection, the networks are trained only on normal samples $x \in \mathcal{X} \subset \mathbb{R}^N$. Given an anomalous input $\overline{x} \notin \mathcal{X}$, it is assumed that the reconstruction $x_{ph}=(D_\phi(E_\theta(\overline{x}))) \in \mathcal{X}$ represents its pseudo-healthy version. The aim of the pseudo-healthy reconstructions is to accurately reverse abnormalities present in the input images. This is achieved by preserving the healthy regions while generating healthy-like tissues in anomalous regions. Thus, anomalies can ideally be directly localized by computing the difference between the anomalous input and the pseudo-healthy reconstructions: $s(\overline{x}) = |\overline{x}-x_{ph}|$. 
