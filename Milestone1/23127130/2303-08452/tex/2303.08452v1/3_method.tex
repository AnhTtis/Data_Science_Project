\section{Method}
~\autoref{fig::phanes} shows an overview of our proposed method. We introduce an innovative approach by utilizing masks produced by latent generative networks to condition generative inpainting networks only on healthy tissues. Our framework is modular, which allows for the flexibility of choosing a preferred generative network, such as adversarial, or diffusion-based models for predicting the pseudo-healthy reconstructions. In the following we describe each component in detail.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{fig_arch.png}
    \caption{\textit{PHANES} overview. Our framework offers modularity, enabling the choice of preferred generative networks, such as adversarial or diffusion-based models. First, we use latent generative networks to learn the healthy data distribution and provide approximate pseudo-healthy reconstructions $x_{cph}$. Anomaly maps $m$ obtained from this step are then used to mask out possible anomalous regions in the input. The remaining healthy tissues are used to condition the refined generative networks, which complete the image and replace anomalous regions with pseudo-healthy tissues. This results in accurate PH reconstructions $x_{ph}$, which enables the precise localization of diseases, as shown on the right.}
    \label{fig::phanes}
\end{figure}
\textbf{Latent generative network.} The first step involves generating masks to cover potential anomalous regions in the input image. The goal of this step is to achieve unbiased detection of various pathologies and minimize false positives. It is therefore important to use a method that is restricted to in-distribution samples, particularly healthy samples, while also accurately reconstructing inputs. Here, we have adopted our previous work~\cite{bercea2022ra} that augments a soft introspective variational auto-encoder with a reversed embedding similarity loss with the aim to enforcing more accurate pseudo-healthy reconstructions. The training process encourages the encoder to distinguish between real and generated samples by minimizing the Kullback-Leibler (KL) divergence of the latent distribution of real samples and the prior, and maximizing the KL divergence of generated samples. On the other hand, the decoder is trained to deceive the encoder by reconstructing real data samples using the standard ELBO and minimizing the KL divergence of generated samples compressed by the encoder:  
\begin{flalign}
    \label{eq::ra}
    & \mathcal{L}_{E_{\phi}}(x,z) = ELBO(x) - \frac{1}{\alpha}(exp(\alpha ELBO(D_\theta(z)) + \lambda \mathcal{L}_{Reversed}(x),\\
    & \mathcal{L}_{Reversed}(x) = \sum_{l=0}^L (1 - \mathcal{L}_{Sim}(E_\phi^l(x), E_\phi^l(x_{cph})) + \frac{1}{2} MSE(E_\phi^l(x), E_\phi^l(x_{cph})),\nonumber\\ 
    & \mathcal{L}_{D_{\theta}}(x,z) = ELBO(x) + \gamma ELBO(D_\theta(z)), \nonumber
\end{flalign}
where $E_\phi^l$ is the $l$-th embedding of the $L$ encoder layers, $x_{cph}=D_\theta(E_\phi(x))$, and $\mathcal{L}_{Sim}$ is the cosine similarity.

\textbf{Mask generation.} Simple residual errors have a strong dependence on the underlying intensities~\cite{meissen2022pitfalls}. As it is important to assign higher values to (subtle) pathological structures, we compute anomaly masks as proposed in~\cite{bercea2022ra} by applying adaptive histogram equalization (eq), normalizing with the $95th$ percentile, and augmenting the errors with perceptual differences for robustness:
\begin{equation}
    m(\overline{x}) = norm_{95}(|(eq(x_{cph}) - eq(\overline{x})|) * {\cal S}_{lpips}(eq(x_{cph}), eq(\overline{x})),
\end{equation}
with ${\cal S}_{lpips}$ being the learned perceptual image patch similarity~\cite{zhang2018unreasonable}. Finally, we binarize the masks using the 99th percentile value on the healthy validation set. 

\textbf{Inpainting generative network}. The objective of the refined PH generative network is to complete the masked image by utilizing the remaining healthy tissues to generate a full PH version of the input. Considering computational efficiency, we have employed the recent in-painting AOT-GAN~\cite{zeng2022aggregated}. The method uses a generator ($G$) and discriminator neural network to optimize losses based on residual and perceptual differences, resulting in accurate and visually precise inpainted images. Additionally, the discriminator predicts the input mask from the inpainted image to improve the synthesis of fine textures.
% \begin{flalign}
%     \label{eq::aotgan}
%     & \mathcal{L}_{rec} = \lVert \overline{x} - G(\overline{x}\odot (1-m), m) \rVert_{1}, \\
%     & \mathcal{L}_{per} = \sum_i \frac{\lVert \chi_i(\overline{x}) - \chi_i(x_{ph})  \rVert_1}{N_i},\nonumber\\ 
%     & \mathcal{L}_{sty} = \mathbb{E}_i \Bigl[ \lVert \chi_i(\overline{x})^T\chi_i(\overline{x}) - \chi_i(x_{ph})^T\chi_i(x_{ph}) \Bigr] \rVert_1,\nonumber\\ 
%     & \mathcal{L}_{adv}^{Dis} = \mathbb{E}_{x_{ph}\sim p_{x_{ph}}} \Bigl[ \Bigr] + , \nonumber
% \end{flalign}

\textbf{Anomaly Maps.} The PH reconstruction is computed as follows: $x_{ph} = \overline{x} \odot (1-m) + G(\overline{x}\odot (1-m), m) \odot m$, with $\odot$ being the pixel-wise multiplication. We compute the final anomaly maps based on residual and perceptual differences:
\begin{equation}
    \label{eq::ano}
    s(\overline{x}) = |x_{ph} - \overline{x}| * {\cal S}_{lpips}(x_{ph}, \overline{x})
\end{equation}