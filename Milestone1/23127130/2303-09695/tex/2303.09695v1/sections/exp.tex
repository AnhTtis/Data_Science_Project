\section{Experiments}

\begin{figure}[t]

    \centering
    \includegraphics[scale=0.43]{img/fig6_v2.pdf}
    \caption{\ann{Illustration of fine-grained panel editing by sketch. Given a 3D jacket and different users' sketches, our method can support fine-grained panel shape editing while preserving the intrinsic structure of the 3D garment.}}
  \label{fig:sketch_edit}
\end{figure}

\begin{table*}[t]
\centering
\caption{\textbf{Evaluation of panel-prediction quality} on seen and unseen type garments. M-L2: Mask L2 ; P-L2 : Panel L2; R-L2: Rotation L2 and T-L2: Translation L2 . $\dagger$ represents orderless-LSTM.} %$*$ denotes the models without data-filtering.}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{@{}c|ccccc|ccccc@{}}
\toprule
                                   & \multicolumn{5}{c|}{\textbf{Seen Types}}                                                                                                                                                                                                      & \multicolumn{5}{c}{\textbf{Unseen Types}}                                                                                                                                                                                                     \\ \cmidrule(l){2-11} 
\multirow{-2}{*}{\textbf{Methods}}& \textbf{P-L2}                     & \textbf{\# Panels}                    & \textbf{\# Edges}                     & \textbf{R-L2}                       & \textbf{T-L2}           & \textbf{P-L2}                     & \textbf{\# Panels}                    & \textbf{\# Edges}                     & \textbf{R-L2}                       & \textbf{T-L2}                     \\ \midrule
Baseline-I                 & 3.92                                    &   99.9\%                                    & 100.0 \%                                    & 0.06                             & 0.117                                       & 6.61                                    & 94.6\%                                     &  95.4\%                                     & 0.09                                    & 0.21                                    \\
Baseline-II                                                   & 4.3                                   & 99.4\%                                   &   99.7\%                                        & 0.08                                   & 1.46                                                             & 8.1                                   & 89.3\%                                   & 90.3\%                                   & 121                                   & 1.25                                   \\
%Baseline-III                                                 & 3.91                                   & 99.9 \%                                  & 99.9 \%                                    & 0.06                                   & 0.05                                                            & 6.3                                   & 93.9  \%                                    & 94.2 \%                                    & 0.07                                   & 0.18                                   \\
LSTM                                                       & 2.71                                  & 99.8\%                                & 99.9\%                                & 0.004                                 & 0.32                                                                 & 14.7                                  & 6.5\%                                 & 53.2\%                                & 0.17                                  & 6.75                                  \\
LSTM$^{\dagger}$                                                    & 2.87                                  & 99.4\%                                & 99.9\%                                & \textbf{0.004}                                 & 0.33                                                                   & 12.94                                 & 2.7\%                                 & 59.0\%                                & 0.16                                  & 7.18                                  \\
Neural-Tailor                                                      & \textbf{1.5}                                   & 99.7\%                                & 99.7\%                                & 0.04                                  & 1.46                                                         & 5.2                                   & 83.6\%                                & 87.3\%                                & 0.07                                  & 3.22                                  \\
% Neural-Tailor*                                                    & 1.53                                  & 98.8\%                                & 99.6\%                                & 0.04                                  & 1.45                                                        & 7.96                                  & 73.1\%                                & 80.5\%                                & 0.08                                  & 3.57                                  \\
% Neural-Tailor*                                                    & 1.6/1.95                              & 98.6/97.5\%                           & 99.8/99.2\%                           & 0.07/0.07                             & 2.2/2.5                                                             & 6.2/6.4                               & 81.6/75.2\%                           & 88.5/88.2\%                           & 0.08/0.10                             & 3.9/4.5                               \\ \midrule
\midrule
\textbf{Ours w/ Text}                      & \cellcolor[HTML]{FFFFDB}2.80  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}0.04  & \cellcolor[HTML]{FFFFDB}\textbf{0.04}    & \cellcolor[HTML]{FFFFDB}\textbf{4.20}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.8\%}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  \\
\textbf{Ours w/ Sketch}                      & \cellcolor[HTML]{FFFFDB}2.91  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}0.05  & \cellcolor[HTML]{FFFFDB}\textbf{0.06}    & \cellcolor[HTML]{FFFFDB}\textbf{4.33}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{0.06}  & \cellcolor[HTML]{FFFFDB}\textbf{0.07}  \\
%\textbf{Ours w/ Overlap}                      & \cellcolor[HTML]{FFFFDB}2.80  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}0.04  & \cellcolor[HTML]{FFFFDB}\textbf{0.04}    & \cellcolor[HTML]{FFFFDB}\textbf{4.20}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.8\%}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  \\

 \bottomrule 
\end{tabular}

\label{tab:main_tab}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics [width=0.99\linewidth]{img/fig4_v2.pdf}
    \caption{Comparison of our method with NeuralTailor (\texttt{NT}) \cite{korosteleva2022neuraltailor} on the unseen garments from ‘jacket sleeveless’, ‘skirt waistband’, ‘wb jumpsuit sleeveless’ and ‘dress’ categories of the dataset \cite{korosteleva2021generating}. The numbers show the average Vertex L2 for the shown exemplars. The colored panels indicate predicted panels, and the dash thin lines indicate the ground-truth panels.}
    \label{fig:main_viz}
\end{figure*}
\noindent \textbf{Datasets}
We evaluate the PersonalTailor method on the 3D garments dataset with sewing patterns introduced in \cite{korosteleva2021generating}. It contains 19 garment classes with $22,000$ 3D garment-sewing pattern pairs in total, covering the variations in t-shirts, jackets, pants, skirts, jumpsuits and dresses. The number of samples in train/val/test is 10627/722/729 in the filtered version. 
Following NeuralTailor \cite{korosteleva2022neuraltailor}, the classes of panels are designed based on the panel's role and location around the body across all garment classes. For example, panels located around the back of human body are grouped in the ``back panels'' class. We follow the standard panel labels, data filtering and train/test splits of garment classes. There are seven garment classes unseen to the training and used for evaluation. 



\noindent \textbf{Evaluation metrics}
We use the same evaluation metrics as in \cite{korosteleva2022neuraltailor}. 
We evaluate the accuracy in predicting the number of panels within
every pattern (\ie, \#Panels) and the number of edges within every panel
(\ie, \#Edges). To estimate the quality of panel shape predictions, we use the average distance (L2 norm) between the vertices of predicted and ground
truth panels with curvature coordinates converted to panel space,
acting as panel masks in this comparison (Panel L2). Similarly,
we report L2 normalized differences of predicted panel rotations
(Rot L2) and translations (Transl L2) with the ground truth. The
quality of predicted stitching information is measured by a mean
precision (Precision) and recall (Recall) of predicted stitches.




\noindent \textbf{Implementation details}
We follow the training scheme as \cite{korosteleva2022neuraltailor}.
For language encoding, we use BERT \cite{li2019visualbert} pretrained encoder. 
For sketch encoding, we use SketchRNN \cite{yang2021sketchgnn}. 
We set the maximum number of panels $M=23$.
There are $g=12/8$ garment classes in training/testing set. We set the feature dimension for text and the global embedding $D = 512$. % is set as 512. 
% The number of codes $K$ in codebook is set as 2000. 
% For Stage-1 training, 
Our model is trained for 250 epochs using Adam optimizer with learning rate of $10e-5$ and batch size of 15. 
% For Stage-2 training, our model
The stitching GNN is trained for 50 epochs using SGD optimizer with learning rate of $10e-4$. 
The inference threshold for panel mask head is set as 0.5 and top-$k$ is set as 14. 
% Our model is implemented in Pytorch and trained with batch size of 15 on a single NVIDIA 2080GTX GPU.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.35]{img/final_model.png}
    \caption{\textbf{Illustration of Garment Customization} Based on user input via sketch/text prompt, we illustrate the customization from (a) Pant Straight sides to Skirt 4 Panels (b) Skirt 4 Panels to Pant Straight Sides (c) Dress Sleeveless to Dress Waistband Sleeveless (d) Dress Waistband Sleeveless to Dress Sleeveless respectively. }
    \label{fig:customized}
\end{figure}


\subsection{Personalized pattern fabrication evaluation}
\noindent \textbf{Setting}  To quantitatively showcase the effectiveness of our personalization ability  based on the user input prompts (\ie, text and sketch), we conduct 6 garment class transfer cases under both text and sketch prompt (case 1\&2: Tee $\leftrightarrow$ Jacket, case 3\&4: Jumpsuit$\leftrightarrow$ Dress, case 5\&6: Jacket $\leftrightarrow$ Jacket Sleeveless). We define the \textit{Panel IOU}  metric as the mean of panel-wise IOUs between predicted panels of the source garment class and the average panels of the target garment class. Formally, we use the desired input prompts to transfer the source garment class to the target garment class. Then compare the \textit{Panel IOU} before personalization and after personalization by target class panel attributes in personalized query. 

\noindent \textbf{Baseline} As there is a lack of competing works or open-source baselines, we created our own baselines. More specifically, we created a personalization baseline by removing the prompt embedding and cross-modal embedding module (referred as w/o CB in Tab.~\ref{tab:personalization}) from PersonalTailor. 

\noindent \textbf{Quantitative results} 
The personalization results are illustrated in Tab.~\ref{tab:personalization}. It can be observed that (1) our method can achieve an average panel IOU of $53\%$ over 6 cases by text and $52\%$ by sketch, which outperforms the baseline method (w/o CB) by $13\%$/$16\%$ respectively. This is because the decoder of the baseline is randomly initialized lacking the semantic and structural information of the panel attributes. Thus, it has less personalization ability. (2) our method  has a larger Panel IOU improvement than the baseline before and after personalization under both text ($22\%$ \vs $16\%$) and sketch prompts  ($21\%$ \vs $17\%$) This showcase that our method has better personalization ability.


\noindent \textbf{Qualitative results}
We illustrate the personalized garment transfer process of case 1\&2 by text prompt (target garment’s panel classes) in Fig.~\ref{fig:editing} (a,b), case 3\&4 by sketch prompt (target garment’s average panel silhouettes) in Fig.~\ref{fig:editing} (c,d).  Our method can support panel shape editing with complex topology changes from one garment class to another using personalized prompts, even for some categories that are not shown in the training set, \eg, Jumpsuit and Dress. Beyond topology change, our method also supports adding 
new panels as in Fig.~\ref{fig:teaser} (b), removing panels in Fig.~\ref{fig:teaser} (c), and creating a
new design that is not included in the dataset in Fig.~\ref{fig:teaser} (e).
We also observe that our method can achieve fine-grained panel shape editing by using sketch prompts. As shown in Fig.~\ref{fig:sketch_edit}, given a 3D jacket and different users' sketch prompts, our method can produce panels that reflect the sketches' shape while preserving the intrinsic structure of the 3D shape. We show more illustration of personalized garment transfer in Fig~\ref{fig:customized}. 
%\paragraph{Controllable garment personalization} \annie{figure 4 and 1} We demonstrate our framework is capable of controllable garment personalization. Given a source 3D garment, our PersonalTailor can accurately edit the 2D sewing panel shapes. Additionally, our model can also perform personalization by design choice during inference. From the results in Fig~\ref{fig:editing}, we can observe that PersonalTailor supports controllable editing on the 3D garment shape and topology with
%preserved intrinsic structure. And PerosnalTailor can edit garments with significant shape variations or transfer garments from one category to another by editing on the 2D panels via mask instructions, even for some categories that are not shown in the training set. As an added benefit, our network facilitates both text and sketch based editing to give more expressive power to the users.








\begin{figure*}
    \centering
    \includegraphics[scale=0.43]{img/PTailor_main.png}
    \caption{\textbf{Illustration of PersonalTailors unrefined prediction} As seen from the figure, both text and sketch prompt predicts similar mask prediction with textual prompt marginally better. }
    \label{fig:ptailor_main}
\end{figure*}
\subsection{Standard pattern fabrication evaluation}

\noindent \textbf{Setting} In this setting, we evaluate non-personalized pattern fabrication.  We evaluate on open-set scenario where training and testing garment classes do not overlap \ie $g_{train} \cap g_{test} = \phi$ however the panel classes may overlap $p_{train} \cap p_{test} \neq \phi$. We follow the same setting and dataset splits as proposed in NeuralTailor \cite{korosteleva2022neuraltailor}. More specifically, we evaluate on two settings \ann{(1) train with seen classes and evaluate on unseen data of those seen classes, \ie closed-set setting (2) train with seen classes and evaluate on unseen classes, \ie open-set setting.} 

\noindent \textbf{Baselines} We considered the following baselines for comparison : (a) a competitive garment pattern prediction method Neural Tailor on filtered data (b) a competitive garment pattern prediction method NeuralTailor$^{*}$ on unfiltered data. (c) an LSTM based garment pattern prediction baseline (d) an orderless LSTM based garment pattern prediction baseline (e) a baseline termed as \textit{Baseline-I} we created using GCN encoder and CNN decoder (f) a baseline termed as \textit{Baseline-II} created using PointTransformer encoder and Transformer decoder without language. %(g) a baseline termed as \textit{Baseline-III} created using GCN encoder and Transformer decoder without language. We evaluate all of the above techniques in a similar setup. 

\noindent \textbf{Results} The results are illustrated in Tab.~\ref{tab:main_tab}. 

\noindent \textbf{(I) Closed-Set Settings:} In this settings, our PersonalTailor has near-to 100\% edge and panel accuracy indicating the superior model design. It is interesting to note that PersonalTailor has almost 8x better translation metric indicating the importance of global information. NeuralTailor has the best Panel L2 indicating that learning vertex has better generalization in the closed set than mask prediction.

\noindent \textbf{(II) Open-Set Settings:} In contrast to closed-set, our proposed method is state-of-the-art in all the metrics open surpassing the competitors by a large margin. This indicates the usefulness of \ann{personalized prompts} in open-set generalization of class agnostic masks. %Additionally, we have tested our method on the non-overlapping setting which can be found in the supplementary file.\annie{check}.




\paragraph{Qualitative results} We first present qualitative results for our PersonalTailor in Fig.~\ref{fig:main_viz},\ref{fig:ptailor_main} whilst comparing to our closest competitor NeuralTailor \cite{korosteleva2022neuraltailor} with unseen garments. From the visualization, it can be observed that our PersonalTailor consistently predicts more accurate panel masks than NeuralTailor thanks to our multi-modal embedding-based design where the prompts bring in additional semantic information for the garment shape. Our findings are also reflected in the Panel L2 metric for each garment where our model is superior in open-set scenario. %Additionaly, our model can also handle overlapping garment problem as seen in Fig~\ref{fig:overlap} where our model has significant better mask prediction than NeuralTailor.







\subsection{Stitching prediction} 

We train our stitch prediction network using the edge vectors from ground truth panels and edges outputted by the prediction module under the text prompt scenario. \ann{As shown in Tab.~\ref{tab:stitch}, our stitch prediction network trained on the edges predictions outperforms the second best $3.3/0.5$ and $9.0/0.3$ points on Precision and Recall for Seen Type and Unseen Type respectively.} And the models  trained by edge prediction data (OurPred, NeuralTailorPred) perform better than that of ground truth edge vectors  (OurGT, NeuralTailorGT). The noised prediction data empowers the generalization ability of the networks.


\begin{table}[t]
\caption{\textbf{Evaluation of stitch prediction in different experiments} on both seen and unseen type garments.}
\label{tab:stitch}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Method}}                 & \multicolumn{2}{c|}{\textbf{Seen Type}}     & \multicolumn{2}{c}{\textbf{Unseen Type}}   \\ \cline{2-5}
                & \textbf{Precision}       & \textbf{Recall}          & \textbf{Precision}       & \textbf{Recall}          \\ \hline
NeuralTailorGT   & 96.6\%          & 88.6\%          & 75.3\%          & 60.6\%          \\ \hline
NeuralTailorPred & 96.3\%          & 99.4\%          & 74.7\%          & 83.9\%          \\ \hline
OurGT            & 74.9\%          & 65.0\%          & 76.8\%          & 73.0\%          \\ \hline
OurPred          & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{85.8\%} & \cellcolor[HTML]{F6DDCC}\textbf{84.2\%} \\ \hline
\end{tabular}
}
\label{stitchingres}
\end{table}


\begin{table}[t]
\centering

\caption{Impact of multi-modal embedding module}
\label{tab:mmemb}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Seen}}     & \multicolumn{2}{c}{\textbf{Unseen}}    \\ \cline{2-5} 
                                & \textbf{Panel L2} & \textbf{\# panels} & \textbf{Panel L2} & \textbf{\# panels} \\ \hline
 Ours                            & \cellcolor[HTML]{F6DDCC}\textbf{2.80}                & \cellcolor[HTML]{F6DDCC} \textbf{99.9\%}                 & \cellcolor[HTML]{F6DDCC}\textbf{4.20}               & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%}  \\ \hline
w/o CMLA                         & 5.51               & 99.8\%                & 6.5                & 99.2\%                   \\
w/o MMT                         & 4.42                &  99.9\%                  & 5.32     
& 99.6\%                  \\ 
\hline


% \hline
 % \\ 
\hline
% w/o both                         & 41                & 42                 & 43   
% & 44 \\

\end{tabular}
\end{table}





\subsection{Ablation studies}
We conduct ablation studies to provide insights into effectiveness of each
component under text prompt scenario.

\noindent \textbf{Impact of cross-modal alignment}
We evaluate the importance of cross-modal alignment in the pattern prediction performance in Tab.~\ref{tab:mmemb}. We first removed the cross modal local association block (CMLA) from our model and observed a significant drop of 2.7\%/2.3\% in the Panel L2 metric for Seen and Unseen setting respectively. This signifies the importance of resolving the domain gap between the point cloud and semantic information from the prompt. Once we remove the multi-modal transformer (MMT) we observe a drop of 1.6\%/1.1\% in the Panel L2 metric for Seen and Unseen setting respectively. This indicates that the raw prompt features lack the power of expressivity to predict the panel mask.



\noindent \textbf{Impact of point cloud encoders} We experimentally prove the necessity of our point cloud encoders in Tab.~\ref{tab:ptc}. Our variant without global encoder (GE) performs the worst by a margin of 0.6\%/1.3\% in Panel L2 metric for Seen and Unseen setting respectively. This is expected as global information plays an important role in estimating the cutting pattern and also guides the panel-mask decoder prediction using cross-attention. It additionally also affects the panel mask accuracy shown by a performance drop of 4.7\% in \# panels metric for Unseen setting  without global encoder (GE). The variant without local encoder (LE) however observes a slight drop of 0.5\%/0.7\% indicating fine-grained local structure is also important for the overall performance improvement. 






\begin{table}[t]
\centering
\setlength{\tabcolsep}{3pt}
\caption{Impact of global and local point cloud encoders}
\label{tab:ptc}
\begin{tabular}{cc|cc|cc}
\hline
\multicolumn{2}{c|}{\textbf{Model} }      & \multicolumn{2}{c|}{\textbf{Seen}} & \multicolumn{2}{c}{\textbf{Unseen}} \\
\hline
 \textbf{CE}     & \textbf{GE}                       & \textbf{Panel L2}    & \textbf{\# Panels}      & \textbf{Panel L2}     & \textbf{\# Panels}     \\ \hline
\xmark          & \cmark                         & 3.20          & 99.4\%            & 3.41               & 99.2\%            \\
\cmark         & \xmark         & 3.3          & 99.9\%            & 5.51               & 96.3\%      \\
\hline
 \cmark        & \cmark    & \cellcolor[HTML]{F6DDCC}\textbf{2.80} & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{4.20}  & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\centering
\setlength{\tabcolsep}{3pt}
\caption{Ablation on the design choice of decoder}
\label{tab:dec}
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Model}}       & \multicolumn{2}{c|}{\textbf{Seen}} & \multicolumn{2}{c}{\textbf{Unseen}} \\ \cline{2-5} 
                             & \textbf{Panel L2}    & \textbf{\# Panels}      & \textbf{Panel L2}     & \textbf{\# Panels}     \\ \hline
CNN                          & 5.49          & 93.2\%          & 6.92           & 91.7\%          \\
Trans. w/o PE        & 3.30          & 99.9\%            & 5.61           & 96.1\%    \\
\hline
Ours & \cellcolor[HTML]{F6DDCC}\textbf{2.80} & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{4.20}  & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} \\ \hline
\end{tabular}
\end{table}

\noindent \textbf{Design choice of panel decoder}
To evaluate the expressive power of our panel mask decoder, we compare it against a CNN based baseline (CNN) and a vanilla transformer decoder without positional embedding (Trans. w/o PE). From the results in Tab.~\ref{tab:dec}, it is clear that the CNN based baseline  is not suitable for instruction based mask prediction as it loses out on the panel prediction performance due to lack of self-attention among the panels. This is improved by 2.7\%/2.7\% higher Panel L2 metric with the transformer decoder under Seen and Unseen settings respectively.  Besides, positional encoding is important as it predicts position specific masks which is reflected in the drop of 1.4\%/0.5\% in overall performance without positional encoding under Seen and Unseen settings.

\subsection{In-the-wild Garment Fabrication}
We qualitatively evaluate our PersonalTailor on
the garment captures from DeepFashion3D dataset \cite{zhu2020deep}
in Fig~\ref{fig:wild}. We observed that our model makes relatively correct panel predictions and good guesses about the panel structure than our competitor NeuralTailor \cite{korosteleva2022neuraltailor}. For example, for garments not seen in the dataset, like the t-shirt which has sleeves, was not predicted by \cite{korosteleva2022neuraltailor} but succesfully predicted by our approach. In the case of jeans, our model has better panel uniformity than \cite{korosteleva2022neuraltailor} highlighting the effectivity of our model design. The quality of prediction on the in-the-wild garment scans can be improved further and thus bridging this sim-to-real gap is part of our future work.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.28]{img/3rd_party.png}
    \caption{\textbf{PersonalTailor on In-the-wild Garments} Sewing patterns predicted by NeuralTailor \cite{korosteleva2022neuraltailor} and PersonalTailor on examples from DeepFashion3D dataset.}
    \label{fig:wild}
\end{figure}


\section{Human Evaluation}
To evaluate the performance in a human perceptive level, we conduct thoughtful user studies in this section. Human subjects \ie professional Tailors evaluation is conducted to investigate the usefullness of customized garment fabrication. We have conducted this experiments on twenty human subjects and reported the scores in percentages out of hundred. As observed from Fig~\ref{fig:hstud1}(a), majority of the tailors prefer automated personalization to save time of production and cost of business. It is also interesting to note from Fig~\ref{fig:hstud1}(b) that majority of the tailors prefer sketch as a medium of customer input for the ease of production. However, 5\% of the tailors also raised concerns on the fact that sketch is only for professional designers and not so customer friendly. We also collected tailors feedback on the most requested mode of personalization among (a) adding new garment panels (b) removing panels (c) changing dress topology and (d) creating new designs. From the results in Fig~\ref{fig:hstud2}, it can be observed that majority of customers prefer removing garment panels which is also supported by our proposed solution. 

\begin{figure}
    \centering
    \includegraphics[scale=0.28]{img/human_study_1.png}
    \caption{\textbf{Usefulness of Personalization}}
    \label{fig:hstud1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.23]{img/human_study_2.png}
    \caption{\textbf{Choice of Personalization}}
    \label{fig:hstud2}
\end{figure}
% \subsection{Human evaluation}
