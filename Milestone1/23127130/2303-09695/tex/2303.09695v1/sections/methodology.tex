\section{Method}
\label{sec:method}
\setlength{\belowdisplayskip}{5pt} \setlength{\belowdisplayshortskip}{2pt}

\setlength{\abovedisplayskip}{5pt} \setlength{\abovedisplayshortskip}{5pt}

\noindent \textbf{Problem statement:} 
Given a source 3D garment $G$ and the personalized requests $L$ from a customer, we aim to predict the corresponding 2D pattern that constructs this garment from a preset library of panels, their position in 3D, and their stitching structure.
Our key idea is to define an encoder-decoder network that could encode the multi-modal information from the point clouds and the user's requests and decode it into the 2D panels structure (see Fig.~\ref{fig:overview}).

To apply such learning based approach, we assume a given dataset $D=\{D_{train}, D_{test}\}$ including training and testing garment classes. 
Each subset is in the form of 
$\{P_{i}, L_{i}, Y_{i}\}_{i=1}^{N}$ where
$P_i$ is the garment point cloud,
$L_{i} = \{g_{i},p_{k=1}^{N_{i}}\}$ represents the garment class $g_{i}$ along with the related panel class labels $p_{k}$. 
% It is defined as $D=\{D_{train},D_{test}\}$ where 
% $D_{train/test} = \{P_{i}, L_{i}, Y_{i}\}_{i=1}^{N}$ where $L_{i} = \{g_{i},p_{k=1}^{N_{i}}\}$ represents the garment class $g_{i}$ along with the related panel labels $p_{k}$. 
$Y_{i} = \{m_{j=1}^{N_{i}}, v_{j=1}^{N_{i}}, c_{j=1}^{N_{i}}, r_{j=1}^{N_{i}}, t_{j=1}^{N_{i}}, s_{j=1}^{N_{i}}\}$ where $m_{j}$ is the 2D panel mask of this garment class, 
$v_{j}$  is an ordered list of 2D coordinates for the panel $m_{j}$, $c_{j}$ is the curvature of the edges for $m_{j}$, 
$r_{j}$ and $t_{j}$ are the rotation and  translation (Euler angles) used to define the panel 3D location around a particular body model, $s_{j}$ is the stitch information of $m_{j}$,
and $N_{i}$ is the total number of panels in a given garment $i$ design.  
More details on the ground-truth formulation are provided in supplementary file.
Note that, all the garment classes share the same set of panels from the panels library,
and personalizing the panel configuration allows to form different garments.
%

\begin{figure*}
    \centering
    \includegraphics[scale=0.18]{img/fig3_v3.pdf}
    \caption{(a) {\bf\em Unsupervised cross-modal association} between point cloud local representation and the semantic representation of the user's prompt. 
    % Attribute-Part alignment where part-level point cloud embeddings are matched with multi-modal prompt embeddings, 
    (b) {\bf\em Binary panel mask decoder}: Except the standard inference, personalization can be flexibly supported, \eg, using an instruction mask.}
    \label{fig:decoder}
\end{figure*}


\subsection{Encoder: Multi-modal panel embedding}

\noindent \paragraph{Point-cloud local representation} To facilitate panel extraction from 3D point cloud (without annotation) in an unsupervised manner, we start by 
local patch analysis as in PointBERT \cite{yu2022point} (corresponding to panels or panel parts).
%
Specifically, given a point cloud $P \in \mathbb{R}^{N \times 3}$, we first sample $g$ center points of $p$ via farthest point sampling (FPS). Then use $k$-nearest neighbor ($k$NN) to associate the nearest points in the point cloud to each center point, resulting in $g$ local patches $\{p_{i}\}_{i=1}^{g}$. We normalize the local patches by subtracting their center coordinates, disentangling the structure patterns and spatial coordinates. We employ a mini-PointNet to embed the patches $F_{loc} \in \mathbb{R}^{g \times D}$ where $D$ is the embedding dimension.

\noindent \paragraph{Point-cloud global representation}
% To obtain attribute-level alignment we require a global representation of the garment point-cloud. 
We adopt PointTransformer ($\mathbb{T}$) \cite{zhao2021point} to extract the global representation of point cloud. 
% Formally, the PointTransformer $\mathbb{T}$ (w/o the classification head) has a stack of point-layer blocks, transition-up, transition-down in a U-Net style design.
We first obtain per-point features $F_{p} = \mathbb{T}(P) \in \mathbb{R}^{N \times D}$ with $D$ the feature dimension, then apply positional encoding. 
We finally aggregate $F_{p}$ into a single feature vector $F_{global} = \phi(F_{p})$ by average pooling $\phi(.)$.

\paragraph{Personalization input representation} 
We consider both text prompt and sketch as the possible user request for personalization.
For the text prompt, any prompt encoder (\eg, pre-trained BERT \cite{li2019visualbert}) can be used.
For the sketch, we use the SketchRNN \cite{ha2017neural}.
%
Formally, given a panel set $p$ required by the customer,
we obtain the personalization feature $P_{loc} \in \mathbb{R}^{K \times D}$, where $K$ is the number of panels and $D$ is the feature dimension.

\subsubsection{Cross-modal local association}
Without ground-truth annotation of panels in a point cloud,
it is crucial to perform cross-modal association between local panel features
$F_{loc}$ and personalization feature $P_{loc}$ representation in an {\em unsupervised} manner.
The goal is to achieve multi-modal information fusion at the panel level
with both high-level semantic information from the personalization input (\eg, text prompt or sketch) and low-level fine-grained geometry information from the point cloud.

In essence, associating $F_{loc}$  and $P_{loc}$ is a set-to-set matching problem. 
% We observe that several local patches of point cloud
% may be associated with a single panel. 
Without pairwise labeling, we adopt the Wasserstein distance between the two sets of feature distributions (see Fig. \ref{fig:decoder}(a)). 
We define the cost of matching as the normalized mean-squared error (MSE) between $P_{loc}$ and $F_{loc}$ \cite{yu2022dual}. 
We denote the cost of moving $P_{loc}^{g}$ to $F_{loc}^{k}$ as $\delta_{g,k} = MSE(\hat{P}^{g}_{loc}, \hat{F}^{k}_{loc})$, where $\hat{P}$/$\hat{F}$ denotes an individual feature from $P$/$F$.
To encourage accurate local association, we minimize the following optimal transport cost:
\begin{align}
    OPT(P_{loc}^{g},F_{loc}^{k}) = \sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k}\delta_{g,k} \;\; \\ \text{where} 
    \sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k} = min(\sum_{g=1}^{G}w_{g}^{v},\sum_{k=1}^{K}w_{k}^{v})
\end{align}
where $w_{g}$/$w_{k}$ refers to the moving weight
and $G$/$K$ to the size of $F_{loc}$/$P_{loc}$.
%
To ease optimization, we further derive a proxy normalized loss quantity as:
\begin{equation}
    \mathbb{W}_{D} = \frac{\sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k}\delta_{g,k}}{\sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k}}.
\end{equation}
More details are given in the supplementary file.  

\subsubsection{Multi-modal attentive embedding}
After local alignment between point cloud and personalization input,
we further fuse information across modalities.
This is achieved by leveraging an attention mechanism \cite{vaswani2017attention}.
%
Concretely, each Transformer module consists of a self-attention layer and a feed forward network.
%
We obtain the multi-modal panel embedding $F_{cm} \in \mathbb{R}^{K \times D}$ via:
\begin{equation}
    F_{cm} = \mathcal{T}_{c}(P_{loc}, F_{loc},F_{loc}) \in \mathbb{R}^{K \times D}, 
\end{equation}
where %$\mathcal{T}_{c}$ is the transformer layer with 
we set the query as $P_{loc}$ and key/value both as $F_{loc}$ respectively.
As a result, each element in $F_{cm}$ is linked particularly with a specific panel class.
This facilitates the realization of panel personalization,
as each panel can be manipulated individually.




\begin{table*}[ht]

\centering
\caption{\textbf{Evaluation of personalized pattern fabrication} on Panel IOU for 6 garment transfer cases. x$\shortrightarrow$y indicates before (x) and after (y) personalization. Abbreviations J: Jacket, JP: Jumpsuit, T: Tee, D: Dress, JS: Jacket Sleeveless. }
\setlength\tabcolsep{5pt}
\begin{tabular}{cc|ccccccccc}
\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\bf{Modality}}} & \multirow{2}{*}{\bf{Method}} & \multicolumn{7}{c}{\bf{Panel IOU for personalized edits}}                                                                                                                                                                                                                                                                                                                                                                                                                      \\ \cline{3-9} 
\multicolumn{1}{l|}{}                          &                         & \multicolumn{1}{c|}{\bf{Case 1}}                   & \multicolumn{1}{c|}{\bf{Case 2}}                   & \multicolumn{1}{c|}{\bf{Case 3}}                   & \multicolumn{1}{c|}{\bf{Case 4}}                               & \multicolumn{1}{c|}{\bf{Case 5}}                   & \multicolumn{1}{c|}{\bf{Case 6}}                   &             \\ \hline
\multicolumn{2}{c|}{\bf{Combinations}}                                        & \multicolumn{1}{c|}{\bf{J to T}}                  & \multicolumn{1}{c|}{\bf{T to J}}                  & \multicolumn{1}{c|}{\bf{JP to D}}                  & \multicolumn{1}{c|}{\bf{D to JP}}                   & \multicolumn{1}{c|}{\bf{J to JS}}                 & \multicolumn{1}{c|}{\bf{JS to J}}      & \multicolumn{1}{c}{\bf{Avg}}                    \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{Text}}     & w/o CB                &\multicolumn{1}{c|}{0.32$\shortrightarrow$0.39}                        &\multicolumn{1}{c|}{0.27$\shortrightarrow$0.40}                        &\multicolumn{1}{c|}{0.18$\shortrightarrow$0.32}                                        &\multicolumn{1}{c|}{0.16$\shortrightarrow$0.31}                        &\multicolumn{1}{c|}{0.11$\shortrightarrow$0.32}                        &\multicolumn{1}{c|}{0.16$\shortrightarrow$0.43}    & \multicolumn{1}{c}{0.20$\shortrightarrow$0.36}                 \\ \cline{2-9} 
                \multicolumn{1}{c|}{}          & Ours                    &\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC}0.46$\shortrightarrow$0.52} & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.41$\shortrightarrow$0.53} &\multicolumn{1}{c|}{ \cellcolor[HTML]{F6DDCC} 0.29$\shortrightarrow$0.51} &\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.19$\shortrightarrow$0.48}&\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.25$\shortrightarrow$0.54} &\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.25$\shortrightarrow$0.60}  &\multicolumn{1}{c}  {\cellcolor[HTML]{F6DDCC} 0.31$\shortrightarrow$0.53}     \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{Sketch}}   & w/o CB                & \multicolumn{1}{c|}{0.29$\shortrightarrow$0.33}                        & \multicolumn{1}{c|}{0.24$\shortrightarrow$0.32}                         & \multicolumn{1}{c|}{0.15$\shortrightarrow$0.39}                                      & \multicolumn{1}{c|}{0.12$\shortrightarrow$0.37}                        & \multicolumn{1}{c|}{0.11$\shortrightarrow$0.34}                        & \multicolumn{1}{c|}{0.18$\shortrightarrow$0.40}   &    \multicolumn{1}{c}  {0.19$\shortrightarrow$0.36}                        \\ \cline{2-9} 
\multicolumn{1}{c|}{}                          & Ours                    & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.45$\shortrightarrow$0.52}                        & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.41$\shortrightarrow$0.52}                          & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.28$\shortrightarrow$0.51}                        & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.18$\shortrightarrow$0.46}                                      & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.25$\shortrightarrow$0.55}                        & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.27$\shortrightarrow$0.56}      &    \multicolumn{1}{c}  {\cellcolor[HTML]{F6DDCC} 0.31$\shortrightarrow$0.52}                                       \\ \hline
\end{tabular}

\label{tab:personalization}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.50]{img/fig5_v3.pdf}
    \caption{\ann{Illustration of garment class transfer cases. Given a 3D source garment, we use the desired panel attributes to transfer it to the target garment class. (a) Case 1: Jacket to Tee by text prompt (target garment's panel classes), (b) Case 2: Tee to Jacket by text prompt, (c) Case 3: Jumpsuit to Dress by sketch prompt (target garment's average panel silhouettes, and (d) Case 4: Dress to Jumpsuit by sketch prompt. Highlighted panels denote panels having topology changes from
prediction.}}
    \label{fig:editing}
\end{figure*}

\subsection{Decoder: Panel mask prediction}
Given per-panel multi-modal embedding $F_{cm}$,
we predict the panel masks along with the stitching using a Transformer decoder $\mathcal{C}$ \cite{vaswani2017attention}.
Specifically, to exploit the panel's spatial information, we append positional encoding to $F_{loc} \in \mathbb{R}^{K \times D}$ with $K$ the number of panels. 
We set this embedding as the queries $Q$ of $\mathcal{C}$. 
We then apply self-attention on $F_{loc}$ for local interaction,
followed by cross-attention with the global feature $F_{global}$
to obtain the final panel-specific representation:

\begin{equation}
    F_{comp} = \mathcal{C}(F_{global};Q) \in \mathbb{R}^{K \times D}.
\end{equation}

\noindent \paragraph{Prediction heads} For efficiency, three lightweight heads are built to decode $F_{comp}$. 
The \emph{garment placement head} outputs the stitching information per panel. This is realized by training an MLP to output the rotation $\hat{r} \in \mathbb{R}^{M \times 3}$ and translation $\hat{t} \in \mathbb{R}^{M \times 3}$:
\begin{equation}\footnotesize
    \hat{r} = \sigma(Pool(W_{r}*F_{comp})), \hat{t} = \sigma(Pool(W_{t}*F_{comp}))
\end{equation}
where $W_{t}/W_{r} \in \mathbb{R}^{D \times 3}$ denotes the weights
$pool$ the average pooling operation. 
The \emph{panel confidence head} predicts the confidence of each panel. It is realized by another MLP followed by sigmoid operation:
\begin{equation}
    \hat{p}_{c} = \sigma(W_{p_{c}}*F_{comp}) \in \mathbb{R}^{D \times 1}
\end{equation}
where $W_{p_{c}}$ is the weight. 
The \emph{panel mask head} first unsamples each query in $F_{comp}$ to a fixed dimension binary mask and then applies sigmoid as 
\begin{equation}
   \hat{m}= \sigma(\psi(F_{comp})) \in \mathbb{R}^{H \times W}
\end{equation}
where $\psi(.)$ is a series of up-convolution followed by ReLU except the last layer.


\noindent \textbf{Panel mask smoothing} 
The sewing panels of garments typically present smooth outlines \cite{korosteleva2022neuraltailor}.
To exploit this prior knowledge, we reformulate the predicted panel mask as a closed piece-wise curve with every piece (edge) constrained to be Bezier spline. 
% To understand the predicted panel mask and interpret the panel edge sets, 
Given the predicted mask $\hat{m}_{i}$,
we estimate the 2D starting vertex $\hat{v}_{i}$ and the curvature $\hat{c}_{i}$ of a panel edge as:
\begin{equation}
    \{\hat{v}_{i},\hat{c}_{i}\} := \tau(\mathcal{B}(\hat{m}_{i}))
\end{equation}
where $\mathcal{B}(\dot)$ denotes VGG net with a MLP classifier followed by tanh activation $\tau(\dot)$. 




\subsection{Model training}

% \noindent \textbf{Learning objective} 
% The panel mask head is a binary class prediction problem. 
%
%Following \cite{korosteleva2022neuraltailor} 
We use the dataset $D_{train}$ to train the model. As input we use the input point set, the text of the ground-truth panel classes as text prompts, and the silhouettes of the ground-truth panels for the sketch prompt. As output we use the 2D pannels and sawing patterns.
We adopt MSE loss ($L_{place}$) for 
training the \emph{garment placement head}. 
%
To train the \emph{panel confidence head}, we assign each ground-truth panel position $j$ as $p_{h}(j) = 1$ and otherwise 0. Since panel position prediction is a multi-class multi-label problem, we use binary cross entropy loss ($\mathcal{L}_{conf}$). 
%
We train the \emph{panel mask head} using a binary cross-entropy loss ($\mathcal{L}_{mask}$). 
We assign each ground-truth panel position $j$ with its corresponding mask $m_{j} \in \mathbb{R}^{H \times W}$ to $p_{g}(j)$, otherwise an empty mask.
%
We use $L_1$ regression loss ($\mathcal{L}_{con}$) to estimate the curvature and vertex positions. 
%
We additionaly use the local-association loss ($\mathcal{L}_{asso}$) to penalize the association which is defined by $\mathcal{L}_{asso} = \mathbb{W}_{D}$. The overall objective is the sum of all above loss terms.







\subsection{Model inference and personalization}

Our model can support both standard and personalized pattern fabrication.
% For both cases, given an unseen point cloud and its garment class, we output top-$k$ most confident panel masks.
The difference between the two settings 
lies in how to set the mask instruction $M$.
In the standard setting, 
given an unseen point cloud, we activate all the panels of $M$, that is for the text prompt, we use all $M$ panel classes, and for the sketch prompt, we use the mean sketchRNN \cite{ha2017neural} embedding \ie prototype of each of $M$ panels in all training classes). We use as output the top-$k$ most confident panel masks above a fixed threshold.
For {\em personalization}, we activate only  the panels specified in the customer's input and output their masks as shown in Fig~\ref{fig:decoder}(b).
%\arik{>>> no use of sketches in personalization?}

\subsection{Garment stitching}
With the panel masks predicted,
we can further infer the stitching information for edge sewing across panels. To that end, we design a StitchGraph module leveraging a GNN $\mathcal{G}$ \cite{yang2021sketchgnn}. 
Given a set of panel vertices $\hat{v}_{i}$, panel curvature $\hat{c}_{i}$ and placement information ($\hat{r}_{i}$, $\hat{t}_{i}$) of the panel mask $\hat{m}_{i}$,
we predict the stitching signal $\hat{s}$:
\begin{equation}
    \hat{s} = \mathcal{G}(\hat{v}_{i},\hat{c}_{i},\hat{r}_{i},\hat{t}_{i}) 
\end{equation}
where the value of ``1'' indicates the two edges stitched and ``0'' otherwise. 
We train $\mathcal{G}$ by a binary cross entropy loss $\mathcal{L}_{stitch}$.
This stitching signal is used for garment draping.








