\section{Method}
\label{sec:method}
\setlength{\belowdisplayskip}{5pt} \setlength{\belowdisplayshortskip}{2pt}

\setlength{\abovedisplayskip}{5pt} \setlength{\abovedisplayshortskip}{5pt}

\noindent \textbf{Overview:} 
Given a source 3D garment $G$ and the personalized requests $L$ from a customer, we aim to predict the corresponding 2D patterns (\saura{which can be stitched together to form} this garment) from a specific library of panels, % \saura{from the dataset}, 
their position in 3D, and their stitching structure.
Our key idea is to define an encoder-decoder network that could encode the multi-modal information from the point clouds and the user's instructions and decode it into the 2D panels structure.
An overview is depicted in Fig.~\ref{fig:overview}. 
\saura{We need to address two key challenges. First, as panel-level point-cloud segmentation is expensive to label, we want to learn the multimodal correspondence across the text/sketch and point cloud. Second, the multi-modal latent panel representations need to be disentangled from each other. Such a design allows us to directly manipulate the panel composition during editing, while visually mirroring these transformation results in producing the garments of different designs and topology as shown in Fig~\ref{fig:teaser}.}

% \saura{ There are two key challenges which guide our design. First, as panel-level point cloud segmentations are expensive to obtain, we would like to learn the multimodal correspondence across the text/sketch and point cloud. Second, the multi-modal latent panel representations need to be disentangled from each other. Such a design allows us to directly manipulate the panel composition during editing, while visually mirroring these transformation results in producing garments of different designs and topology as shown in Fig~\ref{fig:teaser}. }

To apply such learning based approach, we assume a given dataset $D=\{D_{train}, D_{test}\}$ including training and testing garment classes. 
Each subset is in the form of 
$\{P_{i}, L_{i}, Y_{i}\}_{i=1}^{N}$ where
$P_i$ is the garment point cloud,
$L_{i} = \{g_{i},p_{k=1}^{N_{i}}\}$ represents the garment class $g_{i}$ along with the related panel class labels $p_{k}$. 
% It is defined as $D=\{D_{train},D_{test}\}$ where 
% $D_{train/test} = \{P_{i}, L_{i}, Y_{i}\}_{i=1}^{N}$ where $L_{i} = \{g_{i},p_{k=1}^{N_{i}}\}$ represents the garment class $g_{i}$ along with the related panel labels $p_{k}$. 
$Y_{i} = \{m_{j=1}^{N_{i}}, v_{j=1}^{N_{i}}, c_{j=1}^{N_{i}}, r_{j=1}^{N_{i}}, t_{j=1}^{N_{i}}, s_{j=1}^{N_{i}}\}$ where $m_{j}$ is the 2D panel mask of this garment class, 
$v_{j}$  is an ordered list of 2D coordinates for the panel $m_{j}$, $c_{j}$ is the curvature of the edges for $m_{j}$, 
$r_{j}$ and $t_{j}$ are the rotation and  translation (Euler angles) used to define the panel 3D location around a particular body model, $s_{j}$ is the stitch information of $m_{j}$,
and $N_{i}$ is the total number of panels in a given garment $i$ design.  
More details on the ground-truth formulation are provided in supplementary file.
Note that, all the garment classes share the same set of panels,
% from the panels library,
and personalizing the panel configuration allows to form different garments.
%


\saura{Given the garment point-cloud  and the text/sketch panel prompts, each panel of the training data can be converted into a discrete feature map. However, directly combining local part level point-cloud features with its corresponding panel embedding is not trivial, since %the point-cloud and the prompt embedding are
they lie in different feature spaces without part correspondence.
%and the correspondence between these two features is not directly accessible. 
To align the point-clouds and the prompt embeddings of the garment, we propose to reduce the distribution gap  between the two modalities using optimal transport based cross-modal association. To further infuse the aligned point-cloud representation into the prompt embedding space, we perform cross-attention between the point-cloud features and the prompt features to obtain multi-modal information infused panel features.  
}

\begin{figure*}
    \centering
    \includegraphics[scale=0.18]{img/fig3_v3.pdf}
    % \vspace{-0.2in}
    \caption{(a) {\bf\em Unsupervised cross-modal association} between point-cloud local representation and semantic representation of user's prompt. 
    % Attribute-Part alignment where part-level point cloud embeddings are matched with multi-modal prompt embeddings, 
    (b) {\bf\em Binary panel mask decoder}: Except the standard inference, personalization can be flexibly supported, \eg, using an instruction mask.}
    \label{fig:decoder}
    % \vspace{-0.2in}
\end{figure*}

 
\subsection{Encoder: Multi-modal panel embedding}
\noindent {\bf Point-cloud local representation} \saura{Since a 3D garment is composed of multiple panels, it is useful to align the local patches of point-cloud (corresponding to panel or panel parts) with the panel prompts.} To facilitate panel extraction from 3D point-cloud (without panel-level annotation) in an unsupervised manner, we start by 
local patch analysis as in PointBERT \cite{yu2022point}.
% (corresponding to panels or panel parts).
%
Specifically, given a point cloud $P \in \mathbb{R}^{N \times 3}$, we first sample $g$ center points of $p$ via farthest point sampling (FPS). Then use $k$-nearest neighbor ($k$NN) to associate the nearest points in the point cloud to each center point, resulting in $g$ local patches $\{p_{i}\}_{i=1}^{g}$. We normalize the local patches by subtracting their center coordinates, disentangling the structure patterns and spatial coordinates. For efficiency, we employ a mini-PointNet to embed the patches $F_{loc} \in \mathbb{R}^{g \times D}$ with $D$ the embedding dimension.

\noindent {\bf Point-cloud global representation}
% To obtain attribute-level alignment we require a global representation of the garment point-cloud. 
% \saura{
% Our model reconstructs latent codes for each panel
% in a sewing pattern given the global latent code. This global bottleneck makes the model prone to relying on the overall shape of the garment and less likely to exploit its per-component structure. 
% Since, transformers are good at global feature extraction we }
\saura{
We leverage the overall shape of garment as contextual information for estimating the sewing patterns.}
To that end
we use a PointTransformer ($\mathbb{T}$) \cite{zhao2021point} to extract the global representation of point cloud, as the self-attention has a global view field.
% Formally, the PointTransformer $\mathbb{T}$ (w/o the classification head) has a stack of point-layer blocks, transition-up, transition-down in a U-Net style design.
We first obtain per-point features $F_{p} = \mathbb{T}(P) \in \mathbb{R}^{N \times D}$ with $D$ the feature dimension, then apply positional encoding. 
We finally aggregate $F_{p}$ into a single feature vector $F_{global} = \phi(F_{p})$ by average pooling $\phi(.)$.

% \vspace{-0.15in}
\paragraph{Personalization input representation} 
We consider (not limited to) both text prompt and sketch as the user instruction for personalization.
For text prompt, any prompt encoder (\eg, pre-trained CLIP\cite{radford2021learning} text encoder) can be used. \saura{We use a static prompt template of $Garment+ \{panel class\}$ where the panel class (denoted by $p_{i}$, $i \in \{1,..,K\}$) can be ``skirt-front'', ``jacket-sleeveless'' etc.}
For sketch, we use the SketchRNN \cite{ha2017neural}.
%
Formally, given a panel set $p$ required by the user, \saura{we obtain 512-D textual features (for textual prompt) for
$K$ panels from CLIP-encoder and the mean latent vector from Sketch-RNN's encoder %(for sketch prompt) 
for the
sketches of all the training garments belonging to $K$ panels. We then use a projection MLP to match the dimensions.}
We finally obtain the personalization feature $P_{loc} \in \mathbb{R}^{K \times D}$, where $K$ is the number of panels and $D$ is the feature dimension.

% \vspace{-0.2in}
\subsubsection{Cross-modal local association}
Without ground-truth annotation of panels in a point cloud,
it is necessary to perform cross-modal association between local panel features
$F_{loc}$ and personalization feature $P_{loc}$ representation in an {\em unsupervised} manner.
The goal is to achieve multi-modal information fusion at the panel level
with both high-level semantic information from the personalization input (\eg, text prompt or sketch) and low-level fine-grained geometry information from the point cloud.

In essence, associating $F_{loc}$  and $P_{loc}$ is a set-to-set matching problem. 
% We observe that several local patches of point cloud
% may be associated with a single panel. 
Without pairwise labeling, we adopt the Wasserstein distance between the two sets of feature distributions (see Fig. \ref{fig:decoder}(a)). 
We define the cost of matching as the normalized mean-squared error (MSE) between $P_{loc}$ and $F_{loc}$ \cite{yu2022dual}. 
We denote the cost of moving $P_{loc}^{g}$ to $F_{loc}^{k}$ as $\delta_{g,k} = MSE(\hat{P}^{g}_{loc}, \hat{F}^{k}_{loc})$, where $\hat{P}$/$\hat{F}$ denotes an individual feature from $P$/$F$.
To encourage accurate local association, we minimize the following optimal transport cost:
\begin{align}
    OPT(P_{loc}^{g},F_{loc}^{k}) = \sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k}\delta_{g,k} \;\; \\ \text{where} 
    \sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k} = min(\sum_{g=1}^{G}w_{g}^{v},\sum_{k=1}^{K}w_{k}^{v})
\end{align}
where $w_{g}$/$w_{k}$ refers to the moving weight
and $G$/$K$ to the size of $F_{loc}$/$P_{loc}$.
%
To ease optimization, we further derive a proxy normalized loss quantity as:
\begin{equation}
    \mathbb{W}_{D} = \frac{\sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k}\delta_{g,k}}{\sum_{g=1}^{G}\sum_{k=1}^{K}f_{g,k}}.
\end{equation}
More details are given in the supplementary file.  
% \vspace{-0.2in}
\subsubsection{Multi-modal attentive embedding}
After local alignment between point cloud and personalization input,
we further fuse the information across modalities. \saura{Motivated by the generality of cross-attention \cite{chen2021crossvit}, we leverage transformers \cite{vaswani2017attention} to fuse multi-modal features via cross-attention}.
% This is achieved by leveraging an attention mechanism \cite{vaswani2017attention}.
%
Concretely, each Transformer module consists of a self-attention layer and a feed forward network.
%
We obtain the multi-modal panel embedding $F_{cm}$ via:
\begin{equation}
    F_{cm} = \mathcal{T}_{c}(P_{loc}, F_{loc},F_{loc}) \in \mathbb{R}^{K \times D}, 
\end{equation}
where %$\mathcal{T}_{c}$ is the transformer layer with 
we set the query as $P_{loc}$ and key/value both as $F_{loc}$ respectively.
As a result, each element in $F_{cm}$ is linked particularly with a specific panel class \saura{$p_{i}$ ($i \in \{1,..,K\}$).}
This \saura{disentanglement of panel specific features} facilitates the realization of panel personalization,
as each panel can be manipulated individually.




\begin{table*}[ht]

\centering
\caption{\textbf{Evaluation of personalized pattern design} on Panel IOU for 6 garment transfer cases. x$\shortrightarrow$y indicates before (x) and after (y) personalization. Abbreviations J: Jacket, JP: Jumpsuit, T: Tee, D: Dress, JS: Jacket Sleeveless. }
\setlength\tabcolsep{5pt}
\begin{tabular}{cc|ccccccccc}
\hline
\multicolumn{1}{l|}{\multirow{2}{*}{\bf{Modality}}} & \multirow{2}{*}{\bf{Method}} & \multicolumn{7}{c}{\bf{Panel IOU for personalized edits}}                                                                                                                                                                                                                                                                                                                                                                                                                      \\ \cline{3-9} 
\multicolumn{1}{l|}{}                          &                         & \multicolumn{1}{c|}{\bf{Case 1}}                   & \multicolumn{1}{c|}{\bf{Case 2}}                   & \multicolumn{1}{c|}{\bf{Case 3}}                   & \multicolumn{1}{c|}{\bf{Case 4}}                               & \multicolumn{1}{c|}{\bf{Case 5}}                   & \multicolumn{1}{c|}{\bf{Case 6}}                   &             \\ \hline
\multicolumn{2}{c|}{\bf{Combinations}}                                        & \multicolumn{1}{c|}{\bf{J to T}}                  & \multicolumn{1}{c|}{\bf{T to J}}                  & \multicolumn{1}{c|}{\bf{JP to D}}                  & \multicolumn{1}{c|}{\bf{D to JP}}                   & \multicolumn{1}{c|}{\bf{J to JS}}                 & \multicolumn{1}{c|}{\bf{JS to J}}      & \multicolumn{1}{c}{\bf{Avg}}                    \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{Text}}     & Baseline                &\multicolumn{1}{c|}{0.32$\shortrightarrow$0.39}                        &\multicolumn{1}{c|}{0.27$\shortrightarrow$0.40}                        &\multicolumn{1}{c|}{0.18$\shortrightarrow$0.32}                                        &\multicolumn{1}{c|}{0.16$\shortrightarrow$0.31}                        &\multicolumn{1}{c|}{0.11$\shortrightarrow$0.32}                        &\multicolumn{1}{c|}{0.16$\shortrightarrow$0.43}    & \multicolumn{1}{c}{0.20$\shortrightarrow$0.36}                 \\ \cline{2-9} 
                \multicolumn{1}{c|}{}          & Ours                    &\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC}0.46$\shortrightarrow$0.52} & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.41$\shortrightarrow$0.53} &\multicolumn{1}{c|}{ \cellcolor[HTML]{F6DDCC} 0.29$\shortrightarrow$0.51} &\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.19$\shortrightarrow$0.48}&\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.25$\shortrightarrow$0.54} &\multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.25$\shortrightarrow$0.60}  &\multicolumn{1}{c}  {\cellcolor[HTML]{F6DDCC} 0.31$\shortrightarrow$0.53}     \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{Sketch}}   & Baseline                & \multicolumn{1}{c|}{0.29$\shortrightarrow$0.33}                        & \multicolumn{1}{c|}{0.24$\shortrightarrow$0.32}                         & \multicolumn{1}{c|}{0.15$\shortrightarrow$0.39}                                      & \multicolumn{1}{c|}{0.12$\shortrightarrow$0.37}                        & \multicolumn{1}{c|}{0.11$\shortrightarrow$0.34}                        & \multicolumn{1}{c|}{0.18$\shortrightarrow$0.40}   &    \multicolumn{1}{c}  {0.19$\shortrightarrow$0.36}                        \\ \cline{2-9} 
\multicolumn{1}{c|}{}                          & Ours                    & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.45$\shortrightarrow$0.52}                        & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.41$\shortrightarrow$0.52}                          & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.28$\shortrightarrow$0.51}                        & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.18$\shortrightarrow$0.46}                                      & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.25$\shortrightarrow$0.55}                        & \multicolumn{1}{c|}{\cellcolor[HTML]{F6DDCC} 0.27$\shortrightarrow$0.56}      &    \multicolumn{1}{c}  {\cellcolor[HTML]{F6DDCC} 0.31$\shortrightarrow$0.52}                                       \\ \hline
\end{tabular}

\label{tab:personalization}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.52]{img/fig5_v3.pdf}
    \caption{\ann{Examples of garment class transfer cases. Given a 3D source garment, we use the desired panel attributes to transfer it to the target garment class. (a) Case 1: Jacket to Tee by text prompt (target garment's panel classes), 
    (b) Case 2: Tee to Jacket by text prompt, 
    (c) Case 3: Jumpsuit to Dress by sketch prompt (target garment's average panel silhouettes), and (d) Case 4: Dress to Jumpsuit by sketch prompt. 
    The topology changes of panel are highlighted.
    % Highlighted panels denote panels having topology changes from
% prediction.
}}
    \label{fig:editing}
    % \vspace{-0.5cm}
\end{figure*}
% \vspace{-0.05in}
\subsection{Decoder: Panel mask prediction}
% \saura{%Our objective is to obtain a 
% The panel-level decomposition of a given 3D garment forms the basis of our panel latent embedding based editing. 
% Previous works \cite{hertz2022spaghetti} have shown the ability by conditioning the shape generation on the parts partitioning, and their manipulation. Similarly, PersonalTailor utilizes this ability to compose different panel combinations to produce customized or standard garment designs.}
\saura{The panel-level decomposition of a given 3D garment forms the basis of our panel embedding based editing. This is similar in spirit with
shape generation by parts partitioning \cite{hertz2022spaghetti}.}
Given per-panel multi-modal embedding $F_{cm}$,
we predict the panel masks along with the stitching using a Transformer decoder $\mathcal{C}$ \cite{vaswani2017attention}.
Specifically, to exploit the panel's spatial information, we append positional encoding to $F_{cm} \in \mathbb{R}^{K \times D}$ with $K$ the number of panels. 
We set this embedding as the queries $Q$ of $\mathcal{C}$. 
We then apply self-attention on $F_{cm}$ for local interaction,
followed by cross-attention with the global feature $F_{global}$
to obtain the final panel-specific representation:
% \vspace{-0.05in}
\begin{equation}
    F_{comp} = \mathcal{C}(F_{global};Q) \in \mathbb{R}^{K \times D}.
\end{equation}

\noindent {\bf Prediction heads} For efficiency, three lightweight heads are built to decode $F_{comp}$. 
The \emph{garment placement head} outputs the stitching information per panel. This is realized by training an MLP to output the rotation $\hat{r} \in \mathbb{R}^{M \times 3}$ and translation $\hat{t} \in \mathbb{R}^{M \times 3}$:
\begin{equation}\footnotesize
    \hat{r} = \sigma(Pool(W_{r}*F_{comp})), \hat{t} = \sigma(Pool(W_{t}*F_{comp}))
\end{equation}
where $W_{t}/W_{r} \in \mathbb{R}^{D \times 3}$ denotes the weights
$pool$ the average pooling operation. 
The \emph{panel confidence head} predicts the confidence of each panel. It is realized by another MLP followed by sigmoid operation:
\begin{equation}
    \hat{p}_{c} = \sigma(W_{p_{c}}*F_{comp}) \in \mathbb{R}^{D \times 1}
\end{equation}
where $W_{p_{c}}$ is the weight. 
The \emph{panel mask head} first unsamples each query in $F_{comp}$ to a fixed dimension binary mask and then applies sigmoid as 
\begin{equation}
   \hat{m}= \sigma(\psi(F_{comp})) \in \mathbb{R}^{H \times W}
\end{equation}
where $\psi(.)$ is a series of up-convolution followed by ReLU except the last layer.
\saura{More specifically, given a 3D garment point-cloud, we predict the 2D binary panel masks $\hat{m}$ per query
position using the mask head. We then obtain most confident ones (predicted by panel
confidence head $\hat{p}_{c}$, thresholded at 0.5).
We select top-$k$ panels among the most confident panel masks ($k=14$ is set
empirically). The placement head $\hat{r} / \hat{t}$ is used for draping the garment.}

\noindent \textbf{Panel mask smoothing} 
The sewing panels of garments typically present smooth outlines \cite{korosteleva2022neuraltailor}.
To exploit this prior, we reformulate the predicted panel mask as a closed piece-wise curve with every piece (edge) constrained to be Bezier spline. 
% To understand the predicted panel mask and interpret the panel edge sets, 
Given the predicted mask $\hat{m}_{i}$,
we estimate the 2D starting vertex $\hat{v}_{i}$ and the curvature $\hat{c}_{i}$ of a panel edge as:
\begin{equation}
    \{\hat{v}_{i},\hat{c}_{i}\} := \tau(\mathcal{B}(\hat{m}_{i}))
\end{equation}
where $\mathcal{B}(\cdot)$ denotes VGG net with an MLP classifier followed by tanh activation $\tau(\cdot)$. 




\subsection{Model training}

% \noindent \textbf{Learning objective} 
% The panel mask head is a binary class prediction problem. 
%
%Following \cite{korosteleva2022neuraltailor} 
We use the dataset $D_{train}$ to train the model. Model input includes the point set, and the user instruction in form of the text of the ground-truth panel classes (\ie, textual prompt) % as text prompts, and
or the silhouettes of the ground-truth panels (\ie, visual prompt).
% for the sketch prompt. 
\saura{Since the user instruction is based on panel positions, we activate the ground-truth panel positions and set null (zero) vectors at other positions. At inference, this enables the removing-panel function.}
Model output includes the 2D panels and sewing patterns.
We adopt the MSE loss ($L_{place}$) for 
training the \emph{garment placement head}. 
%
To train the \emph{panel confidence head}, we assign each ground-truth panel position $j$ as $p_{h}(j) = 1$, otherwise 0. As panel position prediction is a multi-class multi-label problem, we use binary cross entropy loss ($\mathcal{L}_{conf}$). 
%
We train the \emph{panel mask head} using a binary cross-entropy loss ($\mathcal{L}_{mask}$). 
We assign each ground-truth panel position $j$ with its corresponding mask $m_{j} \in \mathbb{R}^{H \times W}$ to $p_{g}(j)$, otherwise an empty mask.
%
We use $L_1$ regression loss ($\mathcal{L}_{con}$) to estimate the curvature and vertex positions. 
%
We additionally use the local-association loss
($\mathcal{L}_{asso} = \mathbb{W}_{D}$)
% ($\mathcal{L}_{asso}$) 
to learn the cross-modal association. 
% which is defined by $\mathcal{L}_{asso} = \mathbb{W}_{D}$. 
The overall objective is the sum of all above loss terms.







% \vspace{-0.05in}
\subsection{Model inference and personalization}

Our model can support both standard and personalized pattern design.
% For both cases, given an unseen point cloud and its garment class, we output top-$k$ most confident panel masks.
The difference between the two settings 
lies in how to set the mask instruction $M$.
In the standard setting, 
given an unseen point cloud, we activate all the panels of $M$.
\saura{Although the training and testing garments share the same panel class set $p$, the panel combination that forms unseen garments is unknown.} Thus, for the textual prompt we use all $M$ panel classes ; For the visual prompt, we use the mean sketchRNN \cite{ha2017neural} embedding (\ie, the prototype of each of $M$ panels in all training classes). We output the top-$k$ most confident panel masks above a fixed threshold.
For {\em personalization}, we activate only the panels specified in the user's instruction mask \saura{(\ie, passing null vectors at other positions)} and output their 2D panels as shown in Fig~\ref{fig:decoder}(b).
%\arik{>>> no use of sketches in personalization?}

% % \vspace{-0.1in}
\subsection{Garment stitching}
With the panel masks predicted,
we can further infer the stitching information for edge sewing across the panels. To that end, we design a StitchGraph module leveraging a GNN $\mathcal{G}$ \cite{yang2021sketchgnn}. 
Given a set of panel vertices $\hat{v}_{i}$, panel curvature $\hat{c}_{i}$ and placement information ($\hat{r}_{i}$, $\hat{t}_{i}$) of the panel mask $\hat{m}_{i}$,
we predict the stitching signal $\hat{s}$:
\begin{equation}
    \hat{s} = \mathcal{G}(\hat{v}_{i},\hat{c}_{i}) 
\end{equation}
where the value of ``1'' indicates the two edges stitched and ``0'' otherwise. 
We train $\mathcal{G}$ by a binary cross entropy loss $\mathcal{L}_{stitch}$.
This stitching signal coupled with panel placement information $\hat{r}_{i},\hat{t}_{i}$ is used for draping the garment on to the human body. \saura{Note, we follow the same stitching evaluation setting (\eg, no optimization of the stitching parameters in 3D space for different body shapes and sizes) as NeuralTailor \cite{korosteleva2022neuraltailor} for facilitating comparison.}
% , thus we do not optimize the stitching parameters in 3D space based on different body shapes and size.}








