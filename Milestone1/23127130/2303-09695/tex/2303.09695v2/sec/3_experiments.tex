\section{Experiments}



\begin{table*}[t]
\small
\centering
\caption{\textbf{Evaluation of panel-prediction quality} on seen and unseen garment classes. M-L2: Mask L2 ; P-L2 : Panel L2; R-L2: Rotation L2; T-L2: Translation L2 . $\dagger$ represents orderless-LSTM.} %$*$ denotes the models without data-filtering.}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{@{}c|ccccc|ccccc@{}}
\toprule
                                   & \multicolumn{5}{c|}{\textbf{Seen classes}}                                                                                                                                                                                                      & \multicolumn{5}{c}{\textbf{Unseen classes}}                                                                                                                                                                                                     \\ \cmidrule(l){2-11} 
\multirow{-2}{*}{\textbf{Methods}}& \textbf{P-L2}                     & \textbf{\# Panels}                    & \textbf{\# Edges}                     & \textbf{R-L2}                       & \textbf{T-L2}           & \textbf{P-L2}                     & \textbf{\# Panels}                    & \textbf{\# Edges}                     & \textbf{R-L2}                       & \textbf{T-L2}                     \\ \midrule
Baseline-I                 & 3.92                                    &  \bf 99.9\%                                    &\bf 100.0 \%                                    & 0.06                             & 0.117                                       & 6.61                                    & 94.6\%                                     &  95.4\%                                     & 0.09                                    & 0.21                                    \\
Baseline-II                                                   & 4.3                                   & 99.4\%                                   &   99.7\%                                        & 0.08                                   & 1.46                                                             & 8.1                                   & 89.3\%                                   & 90.3\%                                   & 121                                   & 1.25                                   \\
%Baseline-III                                                 & 3.91                                   & 99.9 \%                                  & 99.9 \%                                    & 0.06                                   & 0.05                                                            & 6.3                                   & 93.9  \%                                    & 94.2 \%                                    & 0.07                                   & 0.18                                   \\
LSTM                                                       & 2.71                                  & 99.8\%                                & 99.9\%                                & \bf 0.004                                 & 0.32                                                                 & 14.7                                  & 6.5\%                                 & 53.2\%                                & 0.17                                  & 6.75                                  \\
LSTM$^{\dagger}$                                                    & 2.87                                  & 99.4\%                                & 99.9\%                                & \textbf{0.004}                                 & 0.33                                                                   & 12.94                                 & 2.7\%                                 & 59.0\%                                & 0.16                                  & 7.18                                  \\
Neural-Tailor                                                      & \textbf{1.5}                                   & 99.7\%                                & 99.7\%                                & 0.04                                  & 1.46                                                         & 5.2                                   & 83.6\%                                & 87.3\%                                & 0.07                                  & 3.22                                  \\
% Neural-Tailor*                                                    & 1.53                                  & 98.8\%                                & 99.6\%                                & 0.04                                  & 1.45                                                        & 7.96                                  & 73.1\%                                & 80.5\%                                & 0.08                                  & 3.57                                  \\
% Neural-Tailor*                                                    & 1.6/1.95                              & 98.6/97.5\%                           & 99.8/99.2\%                           & 0.07/0.07                             & 2.2/2.5                                                             & 6.2/6.4                               & 81.6/75.2\%                           & 88.5/88.2\%                           & 0.08/0.10                             & 3.9/4.5                               \\ \midrule
\midrule
\textbf{Ours w/ Text}                      & \cellcolor[HTML]{FFFFDB}2.80  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}{99.9\%}  & \cellcolor[HTML]{FFFFDB}0.04  & \cellcolor[HTML]{FFFFDB}\textbf{0.04}    & \cellcolor[HTML]{FFFFDB}\textbf{4.20}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.8\%}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  \\
\textbf{Ours w/ Sketch}                      & \cellcolor[HTML]{FFFFDB}2.91  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}{99.9\%}  & \cellcolor[HTML]{FFFFDB}0.05  & \cellcolor[HTML]{FFFFDB}{0.06}    & \cellcolor[HTML]{FFFFDB}\textbf{4.33}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{0.06}  & \cellcolor[HTML]{FFFFDB}\textbf{0.07}  \\
%\textbf{Ours w/ Overlap}                      & \cellcolor[HTML]{FFFFDB}2.80  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}0.04  & \cellcolor[HTML]{FFFFDB}\textbf{0.04}    & \cellcolor[HTML]{FFFFDB}\textbf{4.20}  & \cellcolor[HTML]{FFFFDB}\textbf{99.9\%}  & \cellcolor[HTML]{FFFFDB}\textbf{99.8\%}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  & \cellcolor[HTML]{FFFFDB}\textbf{0.05}  \\

 \bottomrule 
\end{tabular}

\label{tab:main_tab}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics [width=\linewidth]{img/fig4_v2.pdf}
    \caption{
    Comparing our method with NeuralTailor (\texttt{NT})
    on the unseen garment classes: ‘jacket sleeveless’, ‘skirt waistband’, ‘wb jumpsuit sleeveless’ and ‘dress’.
    {\em Metric}: the average Vertex L2 error.
    {\em Ground-truth}: dash thin lines.
    % Comparison of our method with NeuralTailor (\texttt{NT}) \cite{korosteleva2022neuraltailor} on the unseen garments from ‘jacket sleeveless’, ‘skirt waistband’, ‘wb jumpsuit sleeveless’ and ‘dress’ categories of the dataset \cite{korosteleva2021generating}. The numbers show the average Vertex L2 for the shown exemplars. The colored panels indicate predicted panels, and the dash thin lines indicate the ground-truth panels.
    }
    \label{fig:main_viz}
    % % \vspace{-0.2in}
\end{figure*}
\noindent \textbf{Dataset}
We evaluate the PersonalTailor on the 3D garments dataset with sewing patterns from \cite{korosteleva2021generating}. It contains 19 garment classes with $22,000$ 3D garment-sewing pattern pairs in total, covering the variations in t-shirts, jackets, pants, skirts, jumpsuits and dresses. 
There are 10627/722/729 samples for train/val/test
% The number of samples in train/val/test is 10627/722/729 
in the filtered version. 
Following NeuralTailor \cite{korosteleva2022neuraltailor}, the classes of panels are designed based on the panel's role and location around the body across all garment classes. For example, panels located around the back of human body are grouped in the ``back panels'' class. We follow the standard panel labels, data filtering and train/test splits of garment classes. There are 7 garment classes unseen to training and used for evaluation. 



\noindent \textbf{Evaluation metrics}
We use the same evaluation metrics as in \cite{korosteleva2022neuraltailor}. 
We evaluate the accuracy in predicting the number of panels within
every pattern (\ie, \#Panels) and the number of edges within every panel
(\ie, \#Edges). To estimate the quality of panel shape predictions, we use the average distance (L2 norm) between the vertices of predicted and ground
truth panels with curvature coordinates converted to panel space,
acting as panel masks in this comparison (Panel L2). Similarly,
we report L2 normalized differences of predicted panel rotations
(Rot L2) and translations (Transl L2) with the ground truth. The
quality of predicted stitching information is measured by a mean
precision (Precision) and recall (Recall) of predicted stitches.




\noindent \textbf{Implementation details}
For language encoding, we use CLIP \cite{radford2021learning} pretrained encoder. 
For sketch encoding, we use SketchRNN \cite{yang2021sketchgnn}. 
We follow the training scheme as \cite{korosteleva2022neuraltailor}.
We set the maximum number of panels $M=23$.
There are $g=12/8$ garment classes in training/testing set. We set the feature dimension for text and the global embedding $D = 512$. % is set as 512. 
% The number of codes $K$ in codebook is set as 2000. 
% For Stage-1 training, 
Our model is trained for 250 epochs using Adam optimizer with learning rate of $10e-5$ and batch size of 15. 
% For Stage-2 training, our model
The stitching GNN is trained for 50 epochs using SGD optimizer with learning rate of $10e-4$.
%
% Specifically, %We train our stitch prediction network
% it is trained by the edge vectors from ground truth panels and edges outputted by the prediction module under the text prompt scenario. 
%
Specifically, %We train our stitch prediction network
it is trained by the predicted edges.
%
The inference threshold for panel mask head is set as 0.5 and top-$k$ is set as 14. The code will be made publicly available upon acceptance.
% Our model is implemented in Pytorch and trained with batch size of 15 on a single NVIDIA 2080GTX GPU.

% \begin{figure}[t]
%     \centering
%     \includegraphics[scale=0.35]{img/final_model.png}
%     \caption{\textbf{Examples of garment personalization} 
%     % Based on user input via sketch/text prompt, we illustrate the customization 
%     %
%     % Garment personalization 
%     from (a) Pant Straight sides to Skirt 4 Panels, (b) Skirt 4 Panels to Pant Straight Sides, (c) Dress Sleeveless to Dress Waistband Sleeveless, (d) Dress Waistband Sleeveless to Dress Sleeveless respectively. }
%     \label{fig:customized}
% \end{figure}


\subsection{Personalized pattern design evaluation}
\noindent \textbf{Setting}  To quantitatively evaluate the performance of personalization,
% based on the user input prompts (\ie, text and sketch), 
we conduct 6 garment class transfer cases (case 1\&2: Tee $\leftrightarrow$ Jacket, case 3\&4: Jumpsuit$\leftrightarrow$ Dress, case 5\&6: Jacket $\leftrightarrow$ Jacket Sleeveless)
under both text and sketch prompt. We define the \textit{Panel IOU}  metric as the mean of panel-wise IOUs between predicted panels of the source garment class and the average panels of the target garment class. Formally, we use the desired input prompts to transfer the source garment class to the target garment class. Then we compare the \textit{Panel IOU} before and after personalization against the target class panel attributes. % in personalized query. 

\noindent \textbf{Baseline} Due to lacking of competing works or open-source alternatives, % in the literature, 
% we created our own baselines. More specifically, 
we created a personalization baseline by removing the prompt embedding and cross-modal embedding module (referred as \texttt{baseline}) from our PersonalTailor. 

\noindent \textbf{Quantitative results} 
The personalization results are reported in Tab.~\ref{tab:personalization}. It can be observed that (1) our method can achieve an average panel IOU of $53\%$ over 6 cases by text and $52\%$ by sketch, outperforming the baseline method by $13\%$/$16\%$ respectively. This is because the decoder of the baseline is randomly initialized lacking the semantic and structural information of the panel attributes. Thus, it has less personalization ability. (2) Our method yields a larger gain over the baseline before and after personalization under both text ($22\%$ \vs $16\%$) and sketch prompts  $21\%$ \vs $17\%$). 
This verifies our superior personalization ability.
% of our model design.
% This showcase that our method has better personalization ability.



\noindent \textbf{Qualitative/visual results}
We show the personalized garment transfer process of case 1\&2 by text prompt (target garment’s panel classes) in Fig.~\ref{fig:editing} (a,b), case 3\&4 by sketch prompt (target garment’s average panel silhouettes) in Fig.~\ref{fig:editing} (c,d).  Overall, it is shown that our method can support panel shape editing with complex topology changes from one garment class to another using personalized prompts, even for those unseen during training, \eg, Jumpsuit and Dress. Beyond topology change, it also supports adding 
new panels (Fig.~\ref{fig:teaser} (b)), removing panels (Fig.~\ref{fig:teaser} (c)), and creating a
new design % that is not included in the dataset 
(Fig.~\ref{fig:teaser} (e)).
We also observe that our method can achieve fine-grained panel shape editing by using sketch prompts. As shown in Fig.~\ref{fig:sketch_edit}, given a 3D jacket and different users' sketch prompts, our method can produce the panels aligned with the sketch's shape while preserving the intrinsic structure of the 3D shape. 
% We provide more illustration of personalized garment transfer in Fig.~\ref{fig:customized}. 
%\paragraph{Controllable garment personalization} \annie{figure 4 and 1} We demonstrate our framework is capable of controllable garment personalization. Given a source 3D garment, our PersonalTailor can accurately edit the 2D sewing panel shapes. Additionally, our model can also perform personalization by design choice during inference. From the results in Fig~\ref{fig:editing}, we can observe that PersonalTailor supports controllable editing on the 3D garment shape and topology with
%preserved intrinsic structure. And PerosnalTailor can edit garments with significant shape variations or transfer garments from one category to another by editing on the 2D panels via mask instructions, even for some categories that are not shown in the training set. As an added benefit, our network facilitates both text and sketch based editing to give more expressive power to the users.








% \begin{figure*}
%     \centering
%     \includegraphics[scale=0.43]{img/PTailor_main.png}
%     \caption{\textbf{Examples of PersonalTailor's output (unrefined)}
%     It is shown that our method works similarly well with text and sketch/visual prompts.
%     % As seen from the figure, both text and sketch prompt predicts similar mask prediction with textual prompt marginally better. 
%     }
%     \label{fig:ptailor_main}
% \end{figure*}

\begin{figure}[t]

    \centering
    \includegraphics[scale=0.32]{img/new_fig_6.png}
    % % \vspace{-0.1in}
    \caption{\ann{Illustration of fine-grained panel editing by sketch. Given a 3D garment and different users' sketches, our method can support fine-grained panel shape editing while preserving the intrinsic structure of the 3D garment.}}
  \label{fig:sketch_edit}
  % % \vspace{-0.2in}
\end{figure}
\subsection{Standard pattern design evaluation}

\noindent \textbf{Setting} In this setting, we evaluate the standard (non-personalized) pattern design.  
% We test the open-set scenario where the training and testing garment classes do not overlap, \ie $g_{train} \cap g_{test} = \phi$, whilst the panel classes may overlap $p_{train} \cap p_{test} \neq \phi$. 
We follow the same setting and dataset splits as proposed in NeuralTailor \cite{korosteleva2022neuraltailor}. More specifically, we evaluate on two settings:
\ann{(1) Training with seen classes and evaluating on unseen data of those seen classes, \ie closed-set setting; 
(2) Training with seen classes and evaluating on unseen classes, \ie open-set setting.} 

\noindent \textbf{Competitors} We considered the following competitors for comparison: 
(a) a competitive garment pattern prediction method Neural Tailor on filtered data \cite{korosteleva2021generating}, 
(c) an LSTM \cite{graves2012long} based garment pattern prediction ,
(d) an orderless LSTM \cite{yazici2020orderless} based garment pattern prediction, 
(e) \textit{Baseline-I} we created using GCN encoder and CNN decoder, 
(f) \textit{Baseline-II} we created using PointTransformer encoder \cite{zhao2021point} and Transformer decoder \cite{vaswani2017attention} with random initialized queries. %(g) a baseline termed as \textit{Baseline-III} created using GCN encoder and Transformer decoder without language. We evaluate all of the above techniques in a similar setup. 

\noindent \textbf{Results} The results are reported in Tab.~\ref{tab:main_tab}. 
\textbf{(I) Closed-set settings:} 
% Under this setting, t
The performance of some metrics (\eg, Edges/Panels) has almost saturated.
% by different methods.
In particular, NeuralTailor has the best Panel L2 result, indicating that learning vertex is better in the closed set than mask prediction.
However, our PersonalTailor achieves the best translation prediction, suggesting the importance of global information.
%
% In this settings, our PersonalTailor has near-to 100\% edge and panel accuracy indicating the superior model design. It is interesting to note that PersonalTailor has almost 8x better translation metric indicating the importance of global information. NeuralTailor has the best Panel L2 indicating that learning vertex has better generalization in the closed set than mask prediction.
\textbf{(II) Open-set settings:} %In contrast to the closed-set setting, 
Our method achieves the state-of-the-art in all the metrics, surpassing the competitors by a large margin. This indicates the superiority of \ann{personalized prompts} in open-set generalization.
% of class agnostic masks. %Additionally, we have tested our method on the non-overlapping setting which can be found in the supplementary file.\annie{check}.



% % \vspace{-0.1in}
\paragraph{Qualitative results} 
% We first present qualitative results for our PersonalTailor in Fig.~\ref{fig:main_viz},\ref{fig:ptailor_main} whilst comparing to our closest competitor NeuralTailor \cite{korosteleva2022neuraltailor} with unseen garments. 
We present qualitative results on unseen garments. 
It can be observed from Fig.~\ref{fig:main_viz} that our PersonalTailor predicts more accurate panels over the prior art NeuralTailor \cite{korosteleva2022neuraltailor}, due to our multi-modal embedding-based design enabling the prompt bring in additional semantic information about the garment's shape.  
We also show in Fig. \ref{fig:ptailor_main} that PersonalTailor works well with both text and sketch prompts.
% Our findings are also reflected in the Panel L2 metric for each garment where our model is superior in open-set scenario. %Additionaly, our model can also handle overlapping garment problem as seen in Fig~\ref{fig:overlap} where our model has significant better mask prediction than NeuralTailor.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.35]{img/final_model.png}
    \caption{\textbf{Examples of garment personalization} 
    % Based on user input via sketch/text prompt, we illustrate the customization 
    %
    % Garment personalization 
    from (a) Pant Straight sides to Skirt 4 Panels, (b) Skirt 4 Panels to Pant Straight Sides, (c) Dress Sleeveless to Dress Waistband Sleeveless, (d) Dress Waistband Sleeveless to Dress Sleeveless respectively. }
    \label{fig:customized}
\end{figure}





\subsection{Stitching prediction} 

We evaluate the stitching module design
by comparing with NeuralTailor \cite{korosteleva2022neuraltailor}.
As shown in Tab.~\ref{tab:stitch}, 
our GNN based design is clearly superior particularly
for unseen garment classes.
This validates the efficacy of our exploiting the structural information of panels.
%
We also show that using the edge vectors of ground-truth panels
for training is inferior than using the predicted for both methods,
as the former introduces some inconsistency with model inference.


% our stitch prediction network trained on the edges predictions outperforms the second best $3.3/0.5$ and $9.0/0.3$ points on Precision and Recall for Seen Type and Unseen Type respectively.} And the models  trained by edge prediction data (OurPred, NeuralTailorPred) perform better than that of ground truth edge vectors  (OurGT, NeuralTailorGT). The noised prediction data empowers the generalization ability of the networks.


\begin{table}[h]
\caption{\textbf{Evaluation of stitching prediction} on both seen and unseen garment classes.
$^*$: Trained by the edges of GT panels.
}
\label{tab:stitch}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Method}}                 & \multicolumn{2}{c|}{\textbf{Seen classes}}     & \multicolumn{2}{c}{\textbf{Unseen classes}}   \\ \cline{2-5}
                & \textbf{Precision}       & \textbf{Recall}          & \textbf{Precision}       & \textbf{Recall}          \\ \hline
NeuralTailor$^*$ \cite{korosteleva2022neuraltailor}  & 96.6\%          & 88.6\%          & 75.3\%          & 60.6\%          \\ \hline
NeuralTailor \cite{korosteleva2022neuraltailor} & 96.3\%          & 99.4\%          & 74.7\%          & 83.9\%          \\ \hline
Ours$^*$            & 74.9\%          & 65.0\%          & 76.8\%          & 73.0\%          \\ \hline
Ours          & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{85.8\%} & \cellcolor[HTML]{F6DDCC}\textbf{84.2\%} \\ \hline
\end{tabular}
}
\label{stitchingres}
\end{table}






\begin{figure*}
    \centering
    \includegraphics[scale=0.43]{img/PTailor_main.png}
    \caption{\textbf{Examples of PersonalTailor's output (unrefined)}
    It is shown that our method works similarly well with text and sketch/visual prompts.
    % As seen from the figure, both text and sketch prompt predicts similar mask prediction with textual prompt marginally better. 
    }
    \label{fig:ptailor_main}
\end{figure*}
\subsection{Ablation studies}
We conduct ablation studies to provide insights into each
component with text prompt. \saura{More in-depth ablations are provided in the \texttt{Supplementary}.}

\noindent \textbf{Impact of cross-modal alignment}
We evaluate the importance of cross-modal alignment.
To that end, we consider two down-stripped designs:
{\bf(1)} Removing the cross modal local association block (CMLA);
{\bf(2)} Removing the multi-modal transformer (MMT).
As shown in Tab.~\ref{tab:mmemb},
we find that without CMLA, a significant drop in the Panel L2 metric 
occurs, suggesting the importance of resolving the domain gap between the point cloud and semantic information from the prompt.
It is also shown that MMT is useful in terms of 
fusing information.

\begin{table}[h]
\centering
\small
\caption{Impact of multi-modal embedding.
CMLA: Cross modal local association;
MMT: Multi-modal transformer.
}
\label{tab:mmemb}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{Seen}}     & \multicolumn{2}{c}{\textbf{Unseen}}    \\ \cline{2-5} 
                                & \textbf{Panel L2} & \textbf{\# panels} & \textbf{Panel L2} & \textbf{\# panels} \\ \hline
 Ours                            & \cellcolor[HTML]{F6DDCC}\textbf{2.80}                & \cellcolor[HTML]{F6DDCC} \textbf{99.9\%}                 & \cellcolor[HTML]{F6DDCC}\textbf{4.20}               & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%}  \\ \hline
w/o CMLA                         & 5.51               & 99.8\%                & 6.5                & 99.2\%                   \\
w/o MMT                         & 4.42                &  99.9\%                  & 5.32     
& 99.6\%                  \\ 
\hline


% \hline
 % \\ 
\hline
% w/o both                         & 41                & 42                 & 43   
% & 44 \\

\end{tabular}
% % \vspace{-0.2in}
\end{table}
% as the raw prompt features lack the power of expressively to predict the panel mask.

% in the pattern prediction performance in Tab.~\ref{tab:mmemb}. We first removed the cross modal local association block (CMLA) from our model and observed a significant drop of 2.7\%/2.3\% in the Panel L2 metric for Seen and Unseen setting respectively. This signifies the importance of resolving the domain gap between the point cloud and semantic information from the prompt. Once we remove the multi-modal transformer (MMT) we observe a drop of 1.6\%/1.1\% in the Panel L2 metric for Seen and Unseen setting respectively. This indicates that the raw prompt features lack the power of expressivity to predict the panel mask.



\noindent \textbf{Impact of point-cloud encoders} We evaluate our point-cloud encoder design including 
global encoder (GE) and local encoder (LE).
As shown in Tab.~\ref{tab:ptc}, we see that 
{\bf(1)} GE is useful particularly for unseen garment classes.
This is because global information plays an important role in estimating the cutting pattern and guiding the panel-mask decoder prediction.
% during cross-attention.
{\bf (2)} LE brings in further performance gain
due to extra part-level information introduced.
%
% ({\bf \color{red} W/O LE giving better Panel L2 on Unseen? Double check!})
% experimentally prove the necessity of our point cloud encoders in Tab.~\ref{tab:ptc}. Our variant without global encoder (GE) performs the worst by a margin of 0.6\%/1.3\% in Panel L2 metric for Seen and Unseen setting respectively. This is expected as global information plays an important role in estimating the cutting pattern and also guides the panel-mask decoder prediction using cross-attention. It additionally also affects the panel mask accuracy shown by a performance drop of 4.7\% in \# panels metric for Unseen setting  without global encoder (GE). The variant without local encoder (LE) however observes a slight drop of 0.5\%/0.7\% indicating fine-grained local structure is also important for the overall performance improvement. 






\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{3pt}
\caption{Impact of global and local point cloud encoders.
LE: Local Encoder; GE: Global Encoder.}
\label{tab:ptc}
\begin{tabular}{cc|cc|cc}
\hline
\multicolumn{2}{c|}{\textbf{Model} }      & \multicolumn{2}{c|}{\textbf{Seen}} & \multicolumn{2}{c}{\textbf{Unseen}} \\
\hline
 \textbf{LE}     & \textbf{GE}                       & \textbf{Panel L2}    & \textbf{\# Panels}      & \textbf{Panel L2}     & \textbf{\# Panels}     \\ \hline
\xmark          & \cmark                         & 3.20          & 99.4\%            & 4.90               & 95.2\%            \\
\cmark         & \xmark         & 3.30          & 99.9\%            & 5.51               & 96.3\%      \\
\hline
 \cmark        & \cmark    & \cellcolor[HTML]{F6DDCC}\textbf{2.80} & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{4.20}  & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} \\ \hline
\end{tabular}
\end{table}



\noindent \textbf{Design choice of panel decoder}
We evaluate more choices of panel mask decoder
including (1) a CNN and (2) a Transformer decoder without positional embedding (Trans. w/o PE). 
We observe in Tab.~\ref{tab:dec} that (1) CNN is least performing for instruction based mask prediction as it loses out on the panel prediction performance due to lack of interaction among the panels. 
(2) Positional encoding is important as it predicts position specific masks.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{3pt}
\caption{Ablation on the design choice of decoder.
Trans.: Transformer; PE: Positional Encoding.
}
\label{tab:dec}
\begin{tabular}{c|cc|cc}
\hline
\multirow{2}{*}{\textbf{Model}}       & \multicolumn{2}{c|}{\textbf{Seen}} & \multicolumn{2}{c}{\textbf{Unseen}} \\ \cline{2-5} 
                             & \textbf{Panel L2}    & \textbf{\# Panels}      & \textbf{Panel L2}     & \textbf{\# Panels}     \\ \hline
CNN                          & 5.49          & 93.2\%          & 6.92           & 91.7\%          \\
Trans. w/o PE        & 3.30          & 99.9\%            & 5.61           & 96.1\%    \\
\hline
Ours & \cellcolor[HTML]{F6DDCC}\textbf{2.80} & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} & \cellcolor[HTML]{F6DDCC}\textbf{4.20}  & \cellcolor[HTML]{F6DDCC}\textbf{99.9\%} \\ \hline
\end{tabular}
\end{table}

% To evaluate the expressive power of our panel mask decoder, we compare it against a CNN based baseline (CNN) and a vanilla transformer decoder without positional embedding (Trans. w/o PE). From the results in Tab.~\ref{tab:dec}, it is clear that the CNN based baseline  is not suitable for instruction based mask prediction as it loses out on the panel prediction performance due to lack of self-attention among the panels. This is improved by 2.7\%/2.7\% higher Panel L2 metric with the transformer decoder under Seen and Unseen settings respectively.  Besides, positional encoding is important as it predicts position specific masks which is reflected in the drop of 1.4\%/0.5\% in overall performance without positional encoding under Seen and Unseen settings.

\subsection{In-the-wild garment pattern design}
For more extensive evaluation, we qualitatively test on the garment captures from DeepFashion3D dataset \cite{zhu2020deep}.
We observe in Fig.~\ref{fig:wild} that our model makes better panel predictions than NeuralTailor \cite{korosteleva2022neuraltailor}. For example, NeuralTailor fails to perceive the sleeves
with the T-shirt (unseen to model training), whilst our model succeeds.
In the case of jeans, our model gives better panel uniformity than \cite{korosteleva2022neuraltailor}.
% We observe in Fig.~\ref{fig:wild} that our model makes more correct panel predictions and good guesses about the panel structure than NeuralTailor \cite{korosteleva2022neuraltailor}. For example, for garments not seen in the dataset, like the t-shirt which has sleeves, was not predicted by \cite{korosteleva2022neuraltailor} but succesfully predicted by our approach. In the case of jeans, our model has better panel uniformity than \cite{korosteleva2022neuraltailor} highlighting the effectivity of our model design. The quality of prediction on the in-the-wild garment scans can be improved further and thus bridging this sim-to-real gap is part of our future work.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.28]{img/3rd_party.png}
    \caption{\textbf{In-the-wild garment evaluation} Sewing patterns predicted by NeuralTailor \cite{korosteleva2022neuraltailor} and our PersonalTailor on examples from DeepFashion3D \cite{zhu2020deep}.}
    \label{fig:wild}
\end{figure}



\section{Human Evaluation}

In additional to benchmark based assessment,
we further provide human evaluation with a thoughtful user study.
In particular, we approached 20 professional tailors to request their 
preference on the significance of personalizing garment pattern design.
In particular, we asked them two questions: 
(1) If garment personalization is necessary? 
(2) Which prompt (text or sketch) is preferred?
As shown in Fig.~\ref{fig:hstud1}, 80\% of tailors consider 
automated personalization to be useful, as it saves them time in production and the cost of business.
Besides, 40\% prefer sketch over text for instruction,
whilst 5\% are concerned with sketch to be only for professional designers.
We also collected the tailors feedback on the functions of (a) adding new garment panels, (b) removing garment panels, (c) changing dress topology, and (d) creating new designs. As shown in Fig.~\ref{fig:hstud2}, removing garment panels is most popular, which is supported by our proposed method. 

% To evaluate the performance in a human perceptive level, we conduct thoughtful user studies in this section. Human subjects \ie professional Tailors evaluation is conducted to investigate the usefullness of customized garment pattern design. We have conducted this experiments on twenty human subjects and reported the scores in percentages out of hundred. As observed from Fig~\ref{fig:hstud1}(a), majority of the tailors prefer automated personalization to save time of production and cost of business. It is also interesting to note from Fig~\ref{fig:hstud1}(b) that majority of the tailors prefer sketch as a medium of customer input for the ease of production. However, 5\% of the tailors also raised concerns on the fact that sketch is only for professional designers and not so customer friendly. We also collected tailors feedback on the most requested mode of personalization among (a) adding new garment panels (b) removing panels (c) changing dress topology and (d) creating new designs. From the results in Fig~\ref{fig:hstud2}, it can be observed that majority of customers prefer removing garment panels which is also supported by our proposed solution. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.38]{img/human_eval.png}
    % % \vspace{-0.1in}
    \caption{\textbf{Votes on garment personalization.}}
    \label{fig:hstud1}
    % % \vspace{-0.2in}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.23]{img/human_study_2.png}
    \caption{\textbf{Votes on garment personalization functions.}}
    \label{fig:hstud2}
\end{figure}

\section{Limitations and Future Work}
We presented a personalized 2D pattern design method for 3D garments,
featuring editing capabilities. Our model is 2D panel-aware, requires no panel annotation, and leverages the Transformer architecture to
form globally coherent 2D patterns of varied topology. The network was designed to allow editing at an interactive rate, where, as demonstrated, the user can interact with the model using a simple instruction (refer to Fig 2). However, one limitation is a relatively small set of panels and
garments, lacking multiple test domains for evaluation. Our mask based panel design cannot model complicated 2D patterns like pleats or darts which is another limitation.  As this is an under-studied area, many challenges (3D optimization, fitting to different body structures etc.) still exist for future work.
