\section{Related works}


\noindent \textbf{2D pattern design} is a research problem with both geometry processing and physics-based simulation.
It has been studied in both graphics and vision communities.
Existing methods can be classified into three categories: 
(i) Modifying the predefined pattern templates to fit a particular body shape \cite{bartle2016physics,li2018foldsketch,yang2018physics,wang2018rule,wang2018learning}. 
(ii) Flattening the surface via minimizing the surface cutting and flattening energy \cite{igarashi2005rigid,sharp2018variational,wang2003feature}, fabrication-specific distortion energy \cite{pietroni2022computational}. 
(iii) Inferring the sewing pattern structure directly from 3D shape via deep learning methods \cite{korosteleva2022neuraltailor}. 
%
%
Our work belongs to the third category.
%
Compared to \cite{korosteleva2022neuraltailor},
the key difference is that 
our PersonalTailor uniquely allows panel personalization,
whilst also supporting the standard garment pattern design.
This is realized by
designing a multi-modal panel embedding module
and a panel mask prediction module.




\noindent\textbf{Transformers} was originally introduced as a sequence-to-sequence attention-based language model \cite{vaswani2017attention}.
% for its global context modeling power. 
It has been adopted over various computer vision tasks, \eg, image classification \cite{khan2022transformers, dosovitskiy2020image}, and 3D geometry learning \cite{zhao2021point}. 
There has been a popularity of vision-language models like CLIP \cite{radford2021learning} applying contrastive learning \cite{chen2020simple} across the features of a vision Transformer and a language Transformer. 
% However, very few works have dealt with point-clouds using large language models like CLIP. 
PointCLIP \cite{zhang2022pointclip} uses 2D image models to classify 3D point-clouds, but its performance is inferior when used as a pretrained backbone due to the domain gap between downstream task and pretraining data. 
Popular language models (\eg, CLIP) have been shown to be generally effective such as referring image segmentation \cite{jeyanathan2020immunological}.
% where it can segment out the object in text query. 
Under this observation, here we extend the scope of language models for personalized garment pattern design.
To the best of our knowledge, this is the first work that predicts 2D masks from 3D point-cloud conditioned on language or sketch as prompt. 
% Motivated by this, we use BERT encoder for panel segmentation. To the best of our knowledge, this is the first work that predicts 2-D binary masks from 3-D point-cloud conditioned on language. 

\begin{figure*}[h]
    \centering
    \includegraphics[scale=0.21]{img/fig2_v6.pdf}
    \caption{\textbf{Architecture of our PersonalTailor method}. Taking as input a 3D point cloud garment and the user's instruction (text prompt or sketch), (a) we extract local and global features for the point cloud, and the semantic feature for user input.
    %
    (b) We then perform unsupervised  association 
    between local point cloud features and semantic features
    for cross-modal alignment and multi-modal fusion, leading to a disentangled multi-modal panel embedding space.
    %
    (c)
    We further decode each multi-modal embedding to 
    a binary panel 2D mask, a confidence score and 3D placement.
    Positional encoding is applied for leveraging the spatial information of panels.
    %
    (d)
    We refine and stitch the panel masks to obtain the final draped garment.
    }
    % \vspace{-0.2in}
    \label{fig:overview}
\end{figure*}

\noindent \textbf{Cross-modal association} 
methods could be grouped into two categories: single-stream and dual-stream. Single-stream models (\eg, VisualBERT \cite{li2019visualbert} and UNITER \cite{chen2020uniter})
concatenate the features of multiple modalities and then exploit a single  encoder for representation learning.
Instead, dual-stream models (\eg, CLIP \cite{radford2021learning} and CLIPasso \cite{vinker2022clipasso}) use a separate encoder for each modality before aligning them, with a flexibility of using modality-specific models and tackling more diverse downstream tasks. 
%
Typically, both approaches consider holistic alignment and information fusion.
%
However, our problem needs more fine-grained alignment at part level without the corresponding box annotation (\ie, weakly supervised). 
%
Object detection could be one intuitive solution \cite{li2020oscar}.
%
Nonetheless, there is a lack of strong pretrained garment panel detectors which 
demand a large of labeled training data.
%
To overcome this challenge,
we resort to unsupervised cross-modal association and fusion
in our model design.


