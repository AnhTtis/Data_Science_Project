\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amsbsy,amssymb,amsthm}
\usepackage{multirow,multicol,tabularx,booktabs}
\usepackage{graphicx,placeins,color,url}
\usepackage{bbm,bm}
\usepackage[ruled]{algorithm2e}
\usepackage[shortlabels]{enumitem}
\usepackage{mathrsfs}
\usepackage{textcase}

\usepackage[top=2.5cm,bottom=2.5cm, right=2.5cm,left=2.5cm]{geometry}
\usepackage[hidelinks]{hyperref}
\renewcommand{\baselinestretch}{1.5}


\newtheorem{conj}{Conjecture}[section]
\newtheorem{thm}[conj]{\bf Theorem}
\newtheorem{defi}[conj]{\bf Definition}
\newtheorem{cor}[conj]{\bf Corollary}
\newtheorem{prop}[conj]{\bf Proposition}
\newtheorem{lemma}[conj]{\bf Lemma}
\newtheorem{rem}[conj]{\bf Remark}
\newtheorem{cond}[conj]{\sc Condition}
\newtheorem{example}[conj]{\bf Example}
\newtheorem{assumpt}{\bf Assumption}

%\def\JSH{\color{blue}}

\def\eps{\varepsilon}


\def\bar{\overline}
\def\SN{{\N}(0,1)}
\def\sign{{\; \text{sign}}}

\def\implies{\Longrightarrow}

\def\to{\rightarrow}
\def\To{\longrightarrow}
\def\weakly{\buildrel {W} \over \longrightarrow}
\def\inprobto{{\buildrel \mathbb P \over \longrightarrow \,}}
\def\equalindistr{\buildrel {d} \over =}
\def\indistrto{\buildrel {D} \over \longrightarrow}
\newcommand{\eqinfty}[1]{{\buildrel {#1 \to \infty} \under =}}
\def\simiid{\buildrel {\text{i.i.d.}} \over \sim}

\def\Bias{\operatorname{Bias}}
\def\IB{\operatorname{IBias}^2}
\def\Cov{\operatorname{Cov}}
\def\Var{\operatorname{Var}}
\def\IVar{\operatorname{IVar}}
\def\MSE{\operatorname{MSE}}
\def\MISE{\operatorname{MISE}}
\def\Med{\operatorname{Med}}
\def\supp{\operatorname{supp}}
\def\JS{\operatorname{JS}}
\def\Tr{\operatorname{Tr}}
\def\Card{\operatorname{Card}}

\def\TV{\operatorname{TV}}
\def\KL{\operatorname{KL}}
\def\Jac{\operatorname{Jac}}
\def\Det{\operatorname{Det}}
\def\mc{\operatorname{mc}}

\def\Lap{\operatorname{Lap}}
\def\Pois{\operatorname{Pois}}
\def\Exp{\operatorname{Exp}}
\def\Ber{\operatorname{Ber}}
\def\om{\omega}

\def\Ac{\mbox{$\mathcal A$}}
\def\Bc{{\mathscr B}}
\def\Cc{{\mathscr C}}
\def\Dc{\mbox{$\mathcal D$}}
\def\Ec{{\mathscr E}}
\def\Fc{{\mathscr F}}
\def\Gc{{\mathscr G}}
\def\Hc{\mbox{$\mathcal H$}}
\def\Ic{\mbox{$\mathcal I$}}
\def\Jc{\mbox{$\mathcal J$}}
\def\Kc{\mbox{$\mathcal K$}}
\def\Lc{\mbox{$\mathcal L$}}
\def\Mc{{\mathcal M}}
\def\Nc{\mbox{$\mathcal N$}}
\def\Oc{\mbox{$\mathcal O$}}
\def\Pc{\mbox{$\mathcal P$}}
\def\Qc{\mbox{$\mathcal Q$}}
\def\Rc{\mbox{$\mathcal R$}}
\def\Sc{{\mathcal S}}
\def\Tc{{\mathcal T}}
\def\Uc{\mbox{$\mathcal U$}}
\def\Vc{\mbox{$\mathcal V$}}
\def\Wc{\mbox{$\mathcal W$}}
\def\Xc{{\mathcal X}}
\def\Yc{\mbox{$\mathcal Y$}}
\def\Zc{{\mathcal Z}}

\def\AA{{\mathbb A}}
\def\BB{{\mathbb B}}
\def\CC{{\mathbb C}}
\def\DD{{\mathbb D}}
\def\FF{{\mathbb F}}
\def\HH{{\mathbb H}}
\def\II{{\mathbb I}}
\def\NN{{\mathbb N}}
\def\GG{{\mathbb G}}
\def\QQ{{\mathbb Q}}
\def\Rb{{\mathbb R}}
\def\Vb{{\mathbb V}}
\def\WW{{\mathbb W}}
\def\Zb{\mbox{$\mathbb Z$}}

%\def\Var{{\mathbb{V}\mathrm{ar}}}

\def\a{ {\bf a}}
\def\e{ {\bf e}}
\def\t{ {\bf t}}
\def\u{ {\bf u}}
\def\v{ {\bf v}}
\def\x{ {\bf x}}
\def\y{ {\bf y}}
\def\z{ {\bf z}}

\def\S{ {\bf S}}
\def\U{ {\bf U}}
\def\W{ {\bf W}}
\def\X{ {\bf X}}
\def\Y{ {\bf Y}}
\def\Z{ {\bf Z}}

\def\1{\mathbbm{1}}
\def\floorbeta{{\lfloor \beta \rfloor}}
\def\ceilbeta{{\lceil \beta \rceil}}

\def\Mk{{\mathfrak{M}}}
\def\Sigmam{\mathfrak{S}_m}
\def\Om{\mathcal{O}_m}
\def\proj{{\mathfrak{p}}}
\def\image{{\rm Im}}
\def\ballmA{{{\mathscr B}_m(A)}}

\def\wh{\widehat}
\def\wt{\widetilde}
\def\ol{\overline}


\title{Lower bounds for the trade-off between bias and mean absolute deviation}
\author{Alexis Derumigny\footnote{TU Delft, Mekelweg 5, 2628 CD Delft, The Netherlands. {\small {\em Email:} \texttt{A.F.F.Derumigny@tudelft.nl}}} \, and Johannes Schmidt-Hieber \footnote{University of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands. {\small {\em Email:} \texttt{a.j.schmidt-hieber@utwente.nl}} \newline The research has been supported by the NWO/STAR grant 613.009.034b and the NWO Vidi grant VI.Vidi.192.021.}}
\date{\today}




\begin{document}

\maketitle

\begin{abstract}
    It is a widely observed phenomenon in nonparametric statistics that rate-optimal estimators balance bias and stochastic error. The recent work on overparametrization raises the question whether rate-optimal estimators exist that do not obey this trade-off. In this work we consider pointwise estimation in the Gaussian white noise model with $\beta$-H\"older smooth regression function $f$. It is shown that an estimator with worst-case bias $\lesssim n^{-\beta/(2\beta+1)}=: \psi_n$ must necessarily also have a worst-case mean absolute deviation that is lower bounded by $\gtrsim \psi_n.$  This proves that any estimator achieving the minimax optimal pointwise estimation rate $\psi_n$ must necessarily balance worst-case bias and worst-case mean absolute deviation. To derive the result, we establish an abstract inequality relating the change of expectation for two probability measures to the mean absolute deviation. 
\end{abstract}

\textbf{Keywords:} Bias-variance trade-off, mean absolute deviation, nonparametric estimation, minimax estimation.

\textbf{MSC 2020:} 62C20, 62G05, 62C05.

\section{Summary of previous work}

The recent work \cite{2020arXiv200600278D} derives lower bounds for the bias-variance trade-off covering standard nonparametric and high-dimensional statistical models. For the squared pointwise risk and the mean integrated squared error, it is shown that the bias-variance trade-off is universal in the sense that rate-optimality of an estimator necessarily implies that worst-case squared (integrated) bias and worst-case (integrated) variance are of the same order as the minimax risk. Differently speaking, an estimator with either worst-case squared bias or worst-case variance converging at a faster rate than the minimax estimation rate is never rate-optimal over the whole class. This shows that the bias-variance trade-off is universal and cannot be overcome, in particular by fitting overparametrized models. 

In the sequence model with sparsity, the situation is different and the bias-variance trade-off does not always hold. \cite{2020arXiv200600278D} shows that there are estimation problems driven by the worst-case bias. While the convergence rate of the worst-case variance part cannot be arbitrarily fast, it can be considerably faster than the minimax estimation rate. 

These lower bounds on the bias-variance trade-off rely on a number of abstract inequalities that all relate the variance to the changes that occur if expectations are taken with respect to different probability measures. To outline the idea, we recall one of these change of expectation inequalities that we modify later on. Let $P$ and $Q$ be two probability distributions on the same measurable space. Denote by $E_P$ and $\Var_P$ the expectation and variance with respect to $P$ and let $E_Q$ and $\Var_Q$ be the expectation and variance with respect to $Q.$ The squared Hellinger distance is defined by $H(P,Q)^2 := \tfrac12 \int (\sqrt{p(\om)}-\sqrt{q (\om)})^2 \, d\nu(\om).$

\begin{lemma}[Lemma 2.1 in \cite{2020arXiv200600278D}]\label{lem.general_lb}
For any random variable $X,$
\begin{align}
    \frac{( E_P[X]-E_Q[X])^2}{4-2H^2(P,Q)}
    \Big( \frac{1}{H(P,Q)}-H(P,Q)\Big)^2
    &\leq \Var_P(X)+ \Var_Q(X).
    \label{eq.lem1_2}
\end{align}
\end{lemma}

To derive a lower bound on the bias-variance trade-off from such an inequality, consider a statistical model $(P_\theta:\theta\in \Theta)$ with $P_\theta$ the distribution of the data for parameter $\theta$ and $\Theta \subseteq \mathbb{R}$ the parameter space. Choosing two parameters $\theta,\theta'\in \Theta,$ inequality \eqref{eq.lem1_2} shows that for any estimator $\wh \theta,$ 
\begin{align}
        \frac{( E_\theta[\wh \theta]-E_{\theta'}[\wh \theta])^2}{4-2r^2(\theta,\theta')}
    \Big( \frac{1}{r(\theta,\theta')}-r(\theta,\theta')\Big)^2
    &\leq \Var_\theta\big(\wh \theta\big)+ \Var_{\theta'}\big(\wh \theta\big),
    \label{eq.1}
\end{align}
with $r(\theta,\theta'):=H({P_\theta},P_{\theta'}),$ $E_\theta:=E_{P_\theta}$, and $\Var_\theta:=\Var_{P_\theta}.$ Introducing the bias $\Bias_\theta(\wh \theta)=\theta-E_\theta[\wh \theta],$ one can now rewrite the difference of the expectations as $E_\theta[\wh \theta]-E_{\theta'}[\wh \theta]=\theta-\theta'-\Bias_\theta(\wh \theta)+\Bias_{\theta'}(\wh \theta).$ If we assume that the bias is smaller than some value, say $B$, and take the parameters $\theta,\theta'$ sufficiently far apart, such that $|\theta-\theta'|\geq 4B,$ reverse triangle inequality yields $|E_\theta[\wh \theta]-E_{\theta'}[\wh \theta]|\geq \tfrac 12 |\theta-\theta'|$ and \eqref{eq.1} becomes
\begin{align*}
    \frac{\frac 14 ( \theta-\theta')^2}{4-2r^2(\theta,\theta')}
    \Big( \frac{1}{r(\theta,\theta')}-r(\theta,\theta')\Big)^2
    &\leq 2\sup_{\theta \in \Theta}\Var_\theta\big(\wh \theta\big).
\end{align*}
The left hand side of this inequality does not depend on the estimator $\wh \theta$ anymore. Therefore, this inequality provides us with a lower bound on the worst-case variance of an arbitrary estimator.

While this applies to a one-dimensional parameter, the same procedure can immediately be extended to derive lower bounds on the worst-case variance for pointwise estimation of a function value $f(x_0)$ in a nonparametric statistical model with unknown regression function $f.$ As shown in \cite{2020arXiv200600278D}, one can extend these ideas moreover to derive lower bounds for the integrated variance and for (high-dimensional) parameter vectors. 

Rephrasing the argument leads moreover to lower bounds on the worst-case bias given an upper bound for the worst-case variance. Taking a suitable asymptotics $\theta'\to \theta$ and imposing standard regularity conditions, it can be shown moreover that \eqref{eq.1} converges to the Cram\'er-Rao lower bound (Theorem A.4 in \cite{2020arXiv200600278D}). 


\section{Lower bounds for bias-MAD trade-off}

To measure the stochastic error of an estimator, a competitor of the variance is the mean absolute deviation (MAD). For a random variable $X,$ the MAD is defined as $E\big[ |X-u| \big],$ where the centering point $u$ is either the mean or the median of $X$. If centered at the mean, the MAD is upper bounded by $\sqrt{\Var(X)},$ but compared to the variance, less weight is given to large outcomes of $X.$ For $(P_\theta:\theta\in \Theta),$ the most natural extension seems therefore to study the trade-off between $m(\theta)-\theta$ and $E_\theta[|\wh \theta-m(\theta)|],$ where again $m(\theta)$ is either the mean or the median of the estimator $\wh \theta$ under $P_\theta$. 


The first result provides an abstract inequality that can be used to relate $m(\theta)-\theta$ and $E_\theta[|\wh \theta-m(\theta)|],$ for any centering $m(\theta).$
It can be viewed as an analogue of \eqref{eq.lem1_2}.

\begin{lemma}
\label{lem.abs_deviation}
Let $P,Q$ be two probability distributions on the  same measurable space and write $E_P, E_Q$ for the expectations with respect to $P$ and $Q$. Then for any random variable $X$ and any real numbers $u$, $v$, we have
    \begin{align}
        \frac 15 \big(1 - H^2(P,Q)\big)^2 |u-v|
        \leq E_P \big[\big|X - u \big|\big]
        \vee E_Q\big[\big|X - v \big|\big],
        \label{eq.MAD_lb}
    \end{align}
\end{lemma}

\begin{proof}
Applying the triangle inequality and the Cauchy-Schwarz inequality, we have
\begin{align}
    \begin{split}
    &\big(1- H^2(P,Q) \big) \big| u - v  \big| \\
    &= \int \big| X(\om) - u - X(\om) + v \big|
    \sqrt{p(\om) q(\om)} \, d\nu(\om) \\
    &\leq \int \big| X(\om) - u \big| \sqrt{p(\om) q(\om)} \, d\nu(\om)
    + \int \big| X(\om) - v \big| \sqrt{p(\om) q(\om)} \, d\nu(\om) \\
    &\leq \sqrt{E_P \big[ \big|X - u \big| \big]
    E_Q \big[ \big|X - u \big| \big]}
    + \sqrt{E_P \big[ \big|X - v \big| \big]
    E_Q \big[ \big|X - v \big| \big]}.
    \end{split}
    \label{eq.abs_deviation_basic_ineq}
\end{align}
Bound $E_Q[|X-u|]\leq E_Q[|X-v|]+|u-v|$ and $E_P[|X-v|]\leq E_P[|X-u|]+|u-v|.$ With $a:=E_P[|X-v|] \vee E_Q[|X-u|],$ $b:=|u-v|$ and $d:=1- H^2(P,Q),$ we then have $db\leq 2\sqrt{a^2+ab}$ or equivalently $a^2+ab-d^2b^2/4 \geq 0.$ Since $a\geq 0,$ solving the quadratic equation $a^2+ab-d^2b^2/4=0$ in $a$ gives that $a \geq b (\sqrt{1+d^2}-1)/2.$ Since $0\leq d\leq 1,$ we also have that $\sqrt{1+d^2}-1 \geq 2d^2/5,$ which can be verified by adding one to both sides and squaring. Combining the last two inequalities gives finally the desired result $a\geq bd^2/5.$
\end{proof}



The derived inequality does not directly follow from the triangle inequality $|u-v|\leq |x-u|+|x-v|$ as the expectations on the right-hand side of \eqref{eq.abs_deviation_basic_ineq} are taken with respect to different measures $P$ and $Q.$
Equality up to a constant multiple is attained if $H(P,Q)<1$ and $X=v$ with probability $1$.

As mentioned above, $E_P[|X-E_P[X]|]\leq \sqrt{\Var_P(X)}.$ Moreover, $E_P[|X-E_P[X]|]$ and $\sqrt{\Var P(X)}$ are typically of the same order. It is thus instructive to compare the lower bound for the mean absolute deviation centered at $u=E_P[X],$ $v=E_Q[X]$ with the Hellinger lower bound for the variance \eqref{eq.lem1_2}, that is, $$\frac 1{\sqrt{4-2H^2(P,Q)}} (1-H^2(P,Q)) \frac{|E_P[X]-E_Q[X]|}{H(P,Q)} \leq \sqrt{\Var_P(X)+ \Var_Q(X)}.$$ The variance lower bound also includes a term $H(P,Q)^{-1}$ on the left hand side that improves the inequality if the distributions $P$ and $Q$ are close. The comparison suggests that the factor $H(P,Q)^{-1}$ should also appear in the lower bound for the absolute mean deviation. But if $P$ now tends to $Q$ and $u,v$ are fixed, the lower bound \eqref{eq.MAD_lb} would then tend to infinity. This is impossible and therefore a lower bound of the form $\gtrsim |u-v|/H(P,Q)$ can only hold for special choices of $u$ and $v$ such as $u=E_P[X],$ $v=E_Q[X].$


We now apply the previously derived inequality to pointwise estimation. In the Gaussian white noise model, we observe a random function $Y=(Y_x)_{x\in [0,1]},$ with
\begin{equation}
    dY_x = f(x) \, dx + \frac{1}{\sqrt{n}} \, dW_x,
    \label{eq.mod_GWN}
\end{equation}
where $W$ is an unobserved standard Brownian motion. The aim is to recover the unobserved, real-valued regression function $f\in L^2([0,1])$ from the data $Y$. Below, we study the bias-MAD trade-off for estimation of $f(x_0)$ with fixed $x_0\in [0,1].$

\medskip

Concerning upper bounds for the MAD risk in this setting, optimal convergence rates are obtained in \cite{MR855002} and the first order asymptotics of the mean absolute deviation risk for Lipschitz functions is derived in \cite{MR1292544}.

To obtain lower bounds for the bias-MAD trade-off, denote by $P_f$ the data distribution of the Gaussian white noise model with regression function $f.$ It is known that the Hellinger distance is
\begin{align}
    H^2(P_f,P_g)=1-\exp\Big(-\frac n8 \|f-g\|_2^2\Big),
    \label{eq.Hell_GWN}
\end{align}
whenever $f,g \in L^2([0,1]),$ see \cite{2020arXiv200600278D} for a reference and a derivation. This means that the inequality \eqref{eq.MAD_lb} becomes 
\begin{align}
    \frac 15 \exp\Big(-\frac n4 \|f-g\|_2^2\Big) |u-v|
        \leq E_f \big[\big|X - u \big|\big]
        \vee E_g\big[\big|X - v \big|\big].
        \label{eq.Hell_GWN_rewritten}
\end{align}
As commonly done in nonparametric statistics, we impose an H\"older smoothness condition on the regression function $f.$ 
Let $R > 0$, $\beta > 0$ and denote by $\floorbeta$ the largest integer that is strictly smaller than $\beta$. On a domain $D \subseteq \Rb,$ we define the $\beta$-H\"older norm by $\| f \|_{\Cc^\beta(D)}= \sum_{\ell \leq \floorbeta}  \|f^{(\ell)} \|_{L^\infty(D)}+ \sup_{x, y \in D, x\neq y} |f^{(\floorbeta)}(x) - f^{(\floorbeta)}(y)  | / | x - y  |^{\beta - \floorbeta},$ with $L^\infty(D)$ the supremum norm on $D$ and $f^{(\ell)}$ denoting the $\ell$-th (strong) derivative of $f$ for $\ell \leq \floorbeta$. For $D=[0,1],$ let $\Cc^\beta(R):=\{f:[0,1] \to \Rb :\| f \|_{\Cc^\beta([0,1])} \leq R\}$ be the ball of $\beta$-H\"older smooth functions $f:[0,1] \to \Rb$ with radius $R.$ We also write $\Cc^\beta(\Rb):=\{K:\Rb \to \Rb :\| K \|_{\Cc^\beta(\Rb)} < \infty\}.$


\begin{thm}\label{thm.MAD_lb}
Consider the Gaussian white noise model \eqref{eq.mod_GWN} with parameter space $\Cc^\beta(R)$. Let $C>0$ be a positive constant. If $\wh f(x_0)$ is an estimator for $f(x_0)$ satisfying
\begin{equation*}
    \sup_{f\in \Cc^\beta(R)} \big|\Bias_f(\wh f(x_0))\big|
    < \Big(\frac C n\Big)^{\beta/(2\beta+1)},
\end{equation*}
then, there exist positive constants $c=c(C,R)$ and $N=N(C,R),$ such that
\begin{align*}
        \sup_{f\in \Cc^\beta(R)} E_f \big[\big|\wh f(x_0) -  E_f[\wh f(x_0)] \big|\big]
        &\geq c n^{- \beta / (2 \beta + 1)}, \quad \text{for all} \ n\geq N.
\end{align*}
Explicit expressions for $c$ and $N$ can be derived from the proof. If $\Med_f[\wh f(x_0)] |]$ denotes the median of $\wh f(x_0),$ then the same holds if $\Bias_f(\wh f(x_0))$ and $E_f[|\wh f(x_0) -  E_f[\wh f(x_0)] |]$ are replaced by $\Med_f[\wh f(x_0)]-f(x_0)$ and $E_f[|\wh f(x_0) -  \Med_f[\wh f(x_0)] |],$ respectively.
\end{thm}

The result is considerably weaker than the lower bounds for the bias-variance trade-off for pointwise estimation. This is due to the fact that \eqref{eq.MAD_lb} is less sharp. Nevertheless, the conclusion provides still more information than the minimax lower bound. To see this, observe that by the triangle inequality, $E_f [|\wh f(x_0) -  E_f[\wh f(x_0)]|]\geq E_f[|\wh f(x_0)-f(x_0)|]-|\Bias_f(\wh f(x_0))|.$ Thus, the conclusion of Theorem \ref{thm.MAD_lb} follows from the minimax lower bound $\sup_{f\in \Cc^\beta(R)} E_f[|\wh f(x_0)-f(x_0)] \geq (K/n)^{\beta / (2 \beta + 1)}$ as long as $C<K.$ Arguing via the minimax rate, nothing, however, can be said if $C>K.$ This is still an interesting case, where the bias is of the optimal order with a potentially large constant. Theorem \ref{thm.MAD_lb} shows that even in this case, the worst-case variance cannot converge faster than $n^{-\beta/(2\beta+1)}.$ As we believe that more refined versions of Lemma \ref{lem.abs_deviation} are obtainable, this approach has, moreover, the potential to lead to a complete characterization of the interplay between bias and mean absolute deviation. 





\begin{proof}[Proof of Theorem \ref{thm.MAD_lb}]
For any function $K \in \Cc^\beta(\Rb)$ satisfying $K(0) = 1$ and $\|K\|_2 < +\infty,$ define $V:=R/\|K\|_{\Cc^\beta(\Rb)},$ $r_n:=(2/V)^{1/\beta}(C/n)^{1/(2\beta+1)},$ and
\begin{equation*}
    \Fc:=\Big\{f_\theta(x) =
    \theta
    V r_n^\beta K \Big( \frac{ x - x_0 }{r_n} \Big) : |\theta| \leq  1 \Big\}.
\end{equation*}
By Lemma B.1 in \cite{2020arXiv200600278D}, we have for $0 < h \leq 1,$ $\|h^\beta K ( (\cdot - x_0 )/h )\|_{\Cc^\beta(\Rb)}\leq \|K\|_{\Cc^\beta(\Rb)}.$ Since $r_n\leq 1$ for all sufficiently large $n$, taking $h=r_n,$ we find $\|f_\theta\|_{\Cc^\beta([0,1])}\leq |\theta| V \|K\|_{\Cc^\beta(\Rb)} \leq R$ for all $\theta \in [-1,1].$ Thus, $\Fc \subseteq \Cc^\beta(R)$ whenever $r_n \leq 1.$ We can now apply \eqref{eq.Hell_GWN_rewritten} to the random variable $\wh f(x_0)$ choosing $P=P_{f_{\pm 1}},$ $Q=P_0$ and centering $u=E_{f_{\pm 1}}[\wh f(x_0)],$ $v=E_0[\wh f(x_0)]$,
\begin{align*}
    \frac 15 \exp\Big(- \frac{n}4 & \|f_{\pm 1}\|_2^2\Big)
    \Big| E_{f_{\pm 1}}[\wh f(x_0)] - E_0[\wh f(x_0)]\Big|
    \leq E_{f_{\pm 1}} \big[\big|\wh f(x_0) -  E_{f_{\pm 1}} [\wh f(x_0)] \big|\big]
    \vee E_0\big[\big|\wh f(x_0) - E_0[\wh f(x_0)] \big|\big].
\end{align*}
Using substitution and the definition $r_n=(2/V)^{1/\beta}(C/n)^{1/(2\beta+1)},$ we find $$\|f_{\pm 1}\|_2^2\leq V^2 r_n^{2\beta} \int_{\Rb} K^2\Big(\frac{x-x_0}{r_n}\Big) \, dx=
V^2 r_n^{2\beta+1}\|K\|_2^2= \frac 1n 2^{2+1/\beta} V^{-1/\beta} C\|K\|_2^2$$ and so,
\begin{align*}
    \frac 15 \exp\bigg( -\Big(\frac 2V\Big)^{1/\beta} C\|K\|_2^2\bigg) \big| E_{f_{\pm 1}}[\wh f(x_0)] - E_0[\wh f(x_0)]  \big|
    &\leq 
    \sup_{f \in \Cc^\beta(R)} E_f \big|\wh f(x_0) -  E_f[\wh f(x_0)] \big|.
\end{align*}
Due to $K(0)=1$ and the definition of $r_n,$ we have $f_{\pm 1}(x_0)= \pm V r_n^\beta= \pm 2(C/n)^{\beta/(2\beta+1)}$ and because of the bound on the bias, $E_{f_1}[\wh f(x_0)]\geq (C/n)^{\beta/(2\beta+1)}$ and $E_{f_{-1}}[\wh f(x_0)]\leq -(C/n)^{\beta/(2\beta+1)}.$ Choosing for the lower bound $f_1$ if $E_{f_0}[\wh f(x_0)]$ is negative and $f_{-1}$ if $E_{f_0}[\wh f(x_0)]$ is positive, we find
\begin{align*}
    \frac 15 \exp\bigg( -\Big(\frac 2V\Big)^{1/\beta} C\|K\|_2^2\bigg) \Big(\frac{C}{n}\Big)^{\frac{\beta}{2\beta+1}}
    \leq
    \sup_{f \in \Cc^\beta(R)} E_f \big|\wh f(x_0) -  E_f[\wh f(x_0)] \big|,
\end{align*}
proving the claim. The proof for the median centering follows exactly the same steps. 
\end{proof}

\section{Further extensions of the bias-variance trade-off}

A natural follow-up question is to wonder about other concepts to measure systematic and stochastic error of an estimator. This section is intended as an overview of related concepts.

A large chunk of literature on variations of the bias-variance trade-off is concerned with extensions to classification under $0$\,-\,$1$ loss \cite{kohavi1996bias, breiman1996bias, tibshirani1996bias, james1997generalizations}. These approaches have been compared
in~\cite{rozmusmethods}. \cite{le2005bias} proposes an extension to the multi-class setting.
In a Bayesian framework, \cite{wolpert1997bias} argues that the bias-variance trade-off becomes a bias-covariance-covariance trade-off, where a covariance correction is added. For relational domains, \cite{neville2007bias} propose to separate the bias and the variance due to the learning process from the bias and the variance due to the inference process. Bias-variance decompositions for the Kullback-Leibler divergence and for the log-likelihood are studied in~\cite{Heskes1998likelihood_based}.
Somehow related, \cite{wu2012decomposition} introduces the Kullback-Leibler bias and the Kullback-Leibler variance, and shows, using information theory, that a similar decomposition is valid.
\cite{Domingos2000unified} propose generalized definitions of bias and variance for a general loss, but without showing a bias-variance decomposition. For several exponential families \cite{hansen2000general} shows that there exist a loss $L$ such that a bias-variance decomposition of $L$ is possible.
\cite{James2003variance} studied a bias-variance decomposition for arbitrary loss functions, comparing different ways of defining the bias and the variance in such cases.

\section*{Acknowledgements}
The project has received funding from the Dutch Research Council (NWO) via the Vidi grant VI.Vidi.192.021. 

\bibliographystyle{abbrv}
\bibliography{biblio}{}

\bigskip


\end{document}
