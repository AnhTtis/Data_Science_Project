\section{Related Work}
\label{related}


\paragraph{Fractal inversion.} Finding the parameters of a given fractal image is a long-standing open problem~\cite{vrscay1991iterated,iacus2005approximating,kya2001optimization,guerin2005fractal,graham2019applying}. Many approaches are based on genetic algorithms~\cite{lankhorst1995iterated,gutierrez2000hybrid,nettleton1994evolutionary} or moment matching~\cite{vrscay1989iterated,forte1995solving}. Our approach is novel by tackling the problem via gradient descent and can be easily implemented using deep learning frameworks. We mainly focus on image reconstruction in the image/pixel space, while other ways of evaluation might be considered in the literature. 





\paragraph{Fractal applications.}
Several recent works leveraged fractals to generate geometrically-meaningful synthetic data to facilitate neural network training~\cite{hendrycks2021pixmix,baradad2021learning,Ma_2021_ICCV,Anderson_2022_WACV,kataoka2020pre, gupta2021beyond, nakashima2021can}.
Some other works leveraged the self-similar property for image compression, for example, by partitioned IFS~\cite{fisher1994fractal}, which resembles the whole image with other parts of the same image \cite{pandey2015fractal, al2020review, jacquin1992image, al2016crowding, venkatasekhar2013fast, menassel2018improved,poli2022self}.
Fractals have also been used in other scientific fields such as diagnostic imaging
\cite{karperien2008automated}, 3D modeling
\cite{ullah2021utilizing}, optical systems~\cite{sweet1999topology}, biology~\cite{otaki2021fractal,enright2005mass}, etc. In this paper, we study the inverse problem, which has the potential to benefit these applications.

\paragraph{Fractals and neural networks.}
Fractals also inspire neural network (NN) architecture designs \cite{larsson2016fractalnet, deng2021fractal} and help the understanding of underlying mechanisms of NNs~\cite{camuto2021fractal,dym2020expression}. 
Specifically, some earlier works~\cite{kolen1994recurrent,tino1998recurrent,tino2004markovian} used the IFS formulation to understand the properties of recurrent neural networks (RNNs). These works are starkly different from ours in two aspects. First, we focus on the inverse problem of fractals. Second, we formulate an IFS as a special RNN to facilitate solving the inverse problem via gradient descent. We note that \citet{stark1991iterated} formulated an IFS as sparse feed-forward networks but not for solving the inverse problem. 


\paragraph{Inversion of generative models.} The problem we studied is related to the inversion of generative models such as GANs~\cite{zhu2016generative,xia2021gan}. The goal is to find the latent representation of GANs for a given image that is not necessarily generated by GANs. Our problem has some unique challenges since the fractal generation process is stochastic and not directly end-to-end differentiable.