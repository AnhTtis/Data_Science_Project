
\section{Experiments}
\label{exp} 

We conduct two experiments to quantitatively and qualitatively validate our approach and design choices. These include (1) \textbf{image reconstruction:} learning an IFS fractal to reconstruct each given target image by gradient descent, using the mean squared error on pixels; (2) \textbf{learning fractals with a GAN loss:} given a set of target images, extending our approach to learn a set of fractal parameters that generates images with distributions similar to the targets. 

We note that \emph{we do not intend to compete with other state-of-the-art in these tasks that do not involve fractals}. Our experiments are to demonstrate that our learnable fractals can capture meaningful information in the target images. \emph{Please see the supplementary material for more details and analyses.} 


\subsection{Setups}
\paragraph{Datasets.}
We first reconstruct random fractal images generated following \textbf{FractalDB}~\cite{kataoka2020pre,Anderson_2022_WACV}. 
We then consider images that are not generated by fractals, including \textbf{MNIST} (hand-written digits)~\cite{lecun1998gradient}, \textbf{FMNIST} (fashion clothing)~\cite{Xiao2017FashionMNISTAN}, and \textbf{KMNIST} (hand-written characters)~\cite{clanuwat2018deep}, for both reconstruction and GAN loss experiments. 
We use $32\times32$ binarized (black-and-white) images as target images for reconstruction. 
For the GAN loss experiment, we use the $60K/10K$ training/test images out of $10$ classes in the three non-fractal datasets.


\paragraph{Technical details about fractals.}
Throughout the experiments, we sample $\vv^{(0)}$ and initialize $\sS$ by the random procedure in~\cite{Anderson_2022_WACV}. 
The number of transformations in $\sS$ is fixed to $N=10$, and we set $T=300$ per image. 
These apply to the FractalDB generation as well.  

To learn $\sS$ using our proposed method, we set $\tau=1$ (RBF kernel bandwidth) and apply an Adam optimizer~\cite{kingma2014adam}. We verify our design choices in the ablation studies of~\autoref{ss_inversion} and the supplementary material.



\subsection{Image reconstruction and ablation study}
\label{ss_inversion}

In the image reconstruction task, our goal is to verify our gradient descent-based method can effectively recover target shapes. 
Specifically, given a target image, we learn an $\sS$ using our proposed approach to reconstruct the target shape by minimizing the \textbf{mean squared error (MSE)} between the generated and the target images. 
Each $\sS$ is learned with a batch size $50$ and a learning rate $0.05$ for $1,000$ SGD steps. As we recover any given target by learning a fractal $\sS$, no training and test splits are considered.  
In this experiment, we generate $2K$ target images using FractalDB, and randomly sample $2K$ target images from each MNIST/FMNIST/KMNIST.
For quantitative evaluation, after $\sS$ is learned, we generate $100$ images per $\sS$ (with different sampled sequences $\vz$) and report the minimum MSE to reduce the influence of random sequences. We consider the following variants of our method:
\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
\item Using either the \emph{Expectation} objective or the \emph{Fixed} objective discussed in~\autoref{ss_opt}.
\item In differentiable rendering (see  \autoref{eq:render}), \emph{clamping} each pixel to be within $[0, 1]$ or not. 
\item Applying the \emph{noisy} SGD trick or not (see~\autoref{ss_opt}). 
\item Learning the reparameterized $\sS$ or the original IFS parameters $\sS$ (see~\autoref{ss_opt}).
\end{itemize}
Besides our variants, we also include an \textbf{inverse encoder} baseline similar to the recent work~\cite{graham2019applying} that learns a ResNet18~\cite{he2016deep} to predict the fractal parameters $\sS$ given an image (\ie, as a regression problem). We train such a regressor on $20K$ different FractalDB images (using the corresponding $\sS$ as labels) and use it on the target images (details are in the supplementary material).  


Qualitatively, \autoref{fig:inversion} shows examples of image reconstruction. Our approach works well on fractal shapes. 
Surprisingly, we can also recover non-fractal datasets to an extent.
This reveals that IFS can be more expressive if the parameters are learned in a proper way. 
By contrast, the inverse encoder cannot recover the target shapes and ends up with poor MSE. 

Quantitatively, the reported MSE (averaged over all targets) in~\autoref{tbl:inversion_mse} verifies our design choices of applying the Expectation objective, clamping the rendered images, and applying noisy SGD. 
Notably, without reparameterization, we can hardly learn $\sS$ since the gradients quickly explode. 



\begin{figure}[t]
\centering
\minipage{0.33\columnwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/mnist_gan.png}
\mbox{\small (a) MNIST}
\endminipage
\hfill
\minipage{0.33\columnwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/fmnist_gan.png}
\mbox{\small (b) FMNIST}
\endminipage
\hfill
\minipage{0.33\columnwidth}
\centering
\includegraphics[width=0.9\linewidth]{figures/kmnist_gan.png}
\mbox{\small (b) KMNIST}
\endminipage
\hfill
\vskip -5pt
\caption{\small \textbf{Learning fractals as GANs.} We conduct linear evaluations using features pre-trained (for $0\sim5K$ SGD steps) on the {\color{red}learned}/{\color{gray}random} fractal images. Test accuracy of means (lines) and standard deviations (shades) over $10$ runs are plotted.}  
\label{fig:gan}
\vskip -5pt
\end{figure}


\subsection{Learning fractals as GAN generators}
\label{ss_GAN}

As discussed in~\autoref{ss_extension}, our approach is a flexible deep learning module that can be learned with other losses besides MSE.
To demonstrate such flexibility, we learn fractals like GANs~\cite{goodfellow2014generative} by formulating a set-to-set learning problem --- matching the distributions of the images generated from fractals to the distributions of (real) target images. 
Specifically, we start from $100$ randomly sampled fractals $\sS$ and train them on the target training set of MNIST/FMNIST/KMINIST in an unsupervised way using the GAN loss with a discriminator (a binary LeNet~\cite{lecun1998gradient} classifier). 
Next, we evaluate the learned fractals by using them for model pre-training~\cite{kataoka2020pre}. 

We argue that if the learned fractals $\sS$ can produce image distributions closer to the targets, pre-training on the generated images should provide good features w.r.t. the (real) target images.
We expect pre-training on the images generated from the learned $\sS$ to yield better features for the target datasets, compared to pre-training on random fractals.

Concretely, we pre-train a $100$-way classifier (using the LeNet architecture again) from scratch. We generate $600$ images from each $\sS$ and treat each $\sS$ as a pseudo-class following~\cite{kataoka2020pre} to form a $100$-class classification problem. Then, we conduct a linear evaluation on the pre-trained features using the real training and test sets. 

\autoref{fig:gan} shows the results, in which we compare pre-training on the random fractals (gray lines) and on the learned fractals (red lines), after every $1K$ SGD steps. 
As we pre-train $\sS$ with more steps, the learned fractals can more accurately capture the target image distribution, evidenced by the better pre-trained features that lead to higher test accuracy.  
This study shows the helpfulness of our method for pre-training. 

