\section*{\LARGE Supplementary Material}


We provide details omitted in the main paper. 

\begin{itemize}
    \item \autoref{suppl-sec:train_details}: additional training details (cf. \autoref{exp} of the main paper).
    \item \autoref{suppl-sec:additional_exp}: additional experiments and analyses (cf. \autoref{exp} of the main paper). 
    \item \autoref{suppl-sec:additional_discussions}: additional discussions. 
\end{itemize}




\section{Additional Training Details} \label{suppl-sec:train_details}

\subsection{Training details for image inversion}
\emph{We note that for the image inversion experiment, our goal is to learn $\sS$ for a given target image. The quantitative error is directly measured between the generated images by the learned $\sS$ and the target image.}

We provide training details for the inverse encoder baseline~\cite{graham2019applying} we compared to that predicts fractal parameters $\sS$ (\ie, as a regression problem) given an input image (cf. \autoref{ss_inversion} of the main paper). We first pre-generate $100K$ fractal images from $20K$ different $\sS$; each $\sS$ has 10 transformations. For each image, its corresponding $\sS$ is used as its label (\ie, regression target), and the transformations are ordered by their maximum singular values. Then, we train a ResNet18 to directly output the target $\sS$, which has $10\times(4+2)$ dimensions, using the mean squared error (MSE). The ResNet18 is pre-trained on ImageNet, and we train it on fractal inversion for 100 epochs using the SGD optimizer with learning rate $0.05$, momentum $0.9$, weight decay $1\mathrm{e}{-4}$, and batch size $128$. The learning rate is reduced every epoch following the cosine schedule~\cite{loshchilov2017sgdr}. 


The training details of our learnable fractals are as follows. Given a target image, we randomly initialize an $\sS$ in its reparameterized form (cf. \autoref{eq_repara}). Then, in all experiments, we use the Adam optimizer with a mini-batch of $50$ sequences (\ie, $\vz$) to train $\sS$. For the Fixed objective, we randomly sample the $50$ sequences based on the initial $\sS$ at the beginning and fix them throughout the training. For the Expectation objective, we sample different $50$ sequences at every training step. Our sequence sampling follows~\cite{Anderson_2022_WACV}. We optimize the $\sS$ for $1000$ steps, and the learning rate is initialized with $0.05$ and decayed by $0.5$ every $250$ steps. 




\subsection{Training details for learning fractals as GAN}

\emph{We note that for this experiment, our goal is to learn a set of $\sS$ such that the resulting fractal images can be used to pre-train a LeNet with stronger features. We use supervised pre-training: images generated by each $\sS$ are considered as the same class.}


We start by randomly sampling 100 different fractals $\sS$ (each can be used to generate more fractal images) and gradually adjust them so that the overall distributions of their generated fractal images become closer to the real images in MNIST/FMNIST/KMNIST. 
To achieve this goal, we plug in the GAN loss to train the 100 fractal $\sS$ as a generator. That is, we randomly sample some $\sS$ and use them to generate a mini-batch of images. A discriminator network will compare the distribution of the generated images and the real images and compute the GAN loss for updating the generator. In the GAN training, we adopt batch size $16$ and generator learning rate $1\mathrm{e}{-4}$. We use a LeNet as our discriminator and update it once every $10$ training steps of the generator. The discriminator learning rate is $1\mathrm{e}{-5}$. 

As we gradually train the $100$ fractals $\sS$ to generate images similar to the real data, we expect the generated fractal images to become similar to the real image distributions. We note that this process is unsupervised as GAN training does not require any categorical labels from these datasets. 

We adopt a linear probing way to evaluate the quality of the generated images --- the generated images should be suitable to pre-train features for MNIST/FMNIST/KMNIST. We evaluate the pre-training performance as we keep updating the $\sS$. At every 1K steps of training the 100 fractal $\sS$, we use them to generate $60K$ images for pre-training LeNet features. The features are then evaluated by learning a linear classifier (with frozen features) on the real training sets of MNIST/FMNIST/KMNIST. In the linear evaluation, we train the classifier for 100 epochs using SGD with a learning rate $0.001$, momentum $0.9$, and a batch size $128$. 



\section{Additional Experiments and Analyses} \label{suppl-sec:additional_exp}


\begin{table}[t]
\footnotesize 
\centering
\setlength{\tabcolsep}{3pt}
\vskip -0pt
\caption{Averaged MSE (means$\pm$std over $3$ runs) of our fractal inversion with $N\in{2, 6, 10}$ transformations.}
\begin{tabular}{c|ccc}
\toprule
Dataset & $N=2$ & $N=6$ & $N=10$ \\
\midrule
FractalDB & $0.0954\pm0.0035$ & $0.0687\pm0.0006$ & $0.0627\pm0.0013$ \\ 
MNIST     & $0.0423\pm0.0013$ & $0.0237\pm0.0015$ & $0.0212\pm0.0009$ \\
\bottomrule
\end{tabular}
\label{tbl:diff_num_trans}
\end{table}


\subsection{Different numbers of transformations in $\sS$}

As the number of transformations controls the expressibility of $\sS$, we provide an ablation study to investigate its influence on image inversion. We randomly sample $500$ images for FractalDB/MNIST and adopt our best setup (with clamping, the Expectation objective, and noisy SGD) for training with $N=2, 6, 10$. The averaged MSE is reported in~\autoref{tbl:diff_num_trans}. The results show that the MSE decreases as $N$ increases, reflecting how $N$ affects the expressive power of $\sS$. We conclude that target shapes can be more accurately recovered if more fractal parameters (\ie, $\sS$ with more affine transformations) are used. 


\begin{table}[t]
\footnotesize 
\centering
\setlength{\tabcolsep}{3pt}
\vskip -0pt
\caption{Averaged MSE (means$\pm$std over $3$ runs) of our fractal inversion with Adam or SGD.}
\begin{tabular}{c|cc}
\toprule
Dataset & SGD & Adam \\
\midrule
MNIST     & $0.0656\pm0.0014$ & $0.0212\pm0.0009$ \\
\bottomrule
\end{tabular}
\label{tbl:sgd_vs_adam}
\end{table}

\subsection{Different optimizers (SGD vs. Adam)}

The optimization of fractals is non-trivial and involves several technical details to make it effective and stable. Throughout all our experiments, we adopt the Adam optimizer as we find it particularly effective for learning fractals. We provide the results of our best setup using the SGD optimizer with a learning rate $0.05$ and momentum $0.9$ to justify our choice. We conduct the comparison on randomly sampled $500$ MNIST images. Both the SGD and Adam optimizers are used to train fractals for $1000$ steps with a learning rate decayed by $0.5$ every $250$ steps. As shown in~\autoref{tbl:sgd_vs_adam}, the Adam optimizer obtains lower MSE than SGD with momentum, yielding better reconstruction performance. 


\section{Additional Discussions} \label{suppl-sec:additional_discussions}

\subsection{More discussions on learning fractals by gradient descent}
To validate our approach for learning fractals, we conduct two tasks, including (1) image inversion: learning an IFS fractal to reconstruct each given image using gradient descent, along with mean squared error on pixels; (2) learning fractals with a GAN loss: given a set of images, extending our approach to learn a set of fractal parameters for them. We provide more discussions on them. 

We first study image inversion. Our objective is to verify our gradient-descent-based method can recover target shapes. In this study, we reconstruct any given target image by training a fractal $\sS$, so no training and test splits are considered. Several ablation studies are conducted to qualitatively and quantitatively validate our design choices. We find that using the Expectation objective gives the most significant gain, and the noisy SGD can further improve the inversion quality. The combination of these two, along with clamping pixels to be $[0, 1]$, yields the best setup of our method. 

Next, we adopt our best setup in GAN learning to demonstrate the benefits of making fractals learnable by gradient descent. \emph{We emphasize that our objective is not to obtain state-of-the-art accuracy but to show the potential of our learnable fractals that can capture geometrically-meaningful, discriminative information in the target images.} 
Specifically, we exploit the flexibility of optimizing via gradient descent to plug in another loss function, the GAN loss, to formulate a set-to-set learning. By gradually modifying random fractals~\cite{kataoka2020pre} to generate images similar to the target data, we can pre-train better features that are more suitable for downstream datasets (MNIST/FMNIST/KMNIST). For each dataset, we use its training set (without labels) to adjust $100$ random fractal $\sS$. Then, the features pre-trained using the adjusted fractals are evaluated following the standard linear evaluation. 



\subsection{Potential negative societal impact} 
We provide an optimization framework for learning fractals by gradient descent. In the field of image generation, although we show that fractals can recover various digits and characters, their expressibility is highly constrained by the iterated function systems (IFS), making them less probable to create real-looking fake images like GANs. In deep learning, the main application of fractals is model pre-training. Our method is likely to be applied to improve upon those related studies when limited downstream data are accessible. Therefore, we share the negative societal impact (but less) with the field of image generation and pre-training to have a bias toward the collected real data. Nevertheless, we emphasize that this potential negative impact does not result from our algorithm.

\subsection{Computation resources}

We conduct our experiments on PyTorch and on $4$ NVIDIA RTX A6000 GPUs. It takes roughly $2$ minutes to learn a fractal $\sS$ for $1$ target image on $1$ GPU. Throughout the paper, we train on roughly $103.5K$ images of different datasets and settings, thereby resulting in a total of $3.45K$ GPU hours. 



\clearpage