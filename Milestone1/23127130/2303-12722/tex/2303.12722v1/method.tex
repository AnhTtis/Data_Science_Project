\section{End-to-End Learnable Fractals by Gradient Descent}
\label{difffractal}

In this paper, we study the problem: \emph{given a target image, can we find the parameters $\sS$ such that the generated IFS fractal image looks like it?}
Let us first provide the problem definition and list the challenges, followed by our algorithm and implementation.

\subsection{Problem and challenges}
\paragraph{Problem.} Given a gray-scaled image $\mI\in [0,1]^{H\times W}$, where $H$ is the height and $W$ is the width, we want to find the fractal parameters $\sS$ (cf.~\autoref{eq:fractal_code_simple}) such that the generated IFS fractal image $\mI'\in[0,1]^{H\times W}$ is close to $\mI$. We note that $\mI$ may not be a fractal image generated by an IFS. 

Let us denote the IFS generation process by $\mI'=G(\sS)$, and the image-to-image loss function by $\sL$. This problem can be formulated as an optimization problem:
\begin{align}
\min_{\sS} \sL(G(\sS), \mI). \label{eq_basic}
\end{align}
We mainly consider the pixel-wise square loss $\sL(\mI',\mI)=\|\mI'-\mI\|_2^2$, but other loss functions can be easily applied. \emph{We note that $G$ is stochastic, and we will discuss how to deal with it in \autoref{ss_opt}.}

\paragraph{Goal.} We aim to solve \autoref{eq_basic} via gradient descent (GD) --- as shown in~\autoref{eq:IFS}, each iteration in IFS is differentiable. Specifically, we aim to develop our algorithm as a module such that it can be easily implemented by modern deep learning frameworks like PyTorch and TensorFlow. This can largely promote our algorithm's usage, \eg, to combine with other modules for end-to-end training.


It is worth mentioning that while $\mI'$ depends on $\{p_n\}_{n=1}^N$ and $\{p_n\}_{n=1}^N$ depends on $\{\mA_n\}_{n=1}^N$ (cf. \autoref{eq:fractal_code_simple}), we do not include this path in deriving the gradients w.r.t.~$\{\mA_n\}_{n=1}^N$ for simplicity.

\paragraph{Challenges.} We identify several challenges in achieving our goal. 
\begin{itemize} [leftmargin=*,itemsep=0pt,topsep=0pt]
\item First, it is nontrivial to implement and parallelize the IFS generation process (for multiple fractals) using modern deep learning frameworks.
\item Second, the whole IFS generation process is not immediately differentiable. Specifically, an IFS generates a sequence of points, not directly an image. We need to back-propagate the gradients from an image to the points. 
\item Third, there are several technical details to make the optimization effective and stable. For example, an IFS is stochastic. The IFS generation involves hundreds or thousands of iterations (\ie, $T$); the back-propagation would likely suffer from exploding gradients.
\end{itemize} 

In the rest of this section, we describe our solution to each of these challenges. We provide a summarized pipeline of our approach in~\autoref{fig:render}. 
  

\begin{figure*}[t]
    \centering
    \minipage{0.47\textwidth}
    \centering
    \includegraphics[width=1.\linewidth]{figures/layer.jpg} 
    \mbox{(a) }
    \endminipage
    \hfill
    \minipage{0.5\textwidth}
    \centering
    \includegraphics[width=1.\linewidth]{figures/differentiable_rendering.pdf}
    \vskip -6pt
    \mbox{(b)}
    \endminipage
    \vskip -6pt
    \caption{\small (a) \textbf{Fractal embedding layer} in~\autoref{ss_deep}. We show a case of $T=3$ and batch size $=2$. The purple dashed lines indicate the point transition (like the hidden vector in RNNs). (b) \textbf{Differentiable rendering} in~\autoref{ss_diff}. We show that the gradients w.r.t.~the rendered image can be translated into 2D gradient vectors w.r.t. the IFS points, \ie, $\nabla_{\vv^{(t)}}\sL\in\R^2, \forall t\in\{1,\cdots,T\}$ (zoom-in for better resolutions).}
    \vskip -10pt
    \label{fig:diff}
\end{figure*} 

\subsection{Fractal generation as a deep learning model}
\label{ss_deep}
\paragraph{IFS as an RNN.} We start with how to implement the IFS fractal generation process as a deep learning model. We note that an IFS performs \autoref{eq:IFS} for $T$ iterations. Every iteration involves an affine transformation, which is sampled with replacement from the fractal parameters $\sS$. Let us assume that $\sS$ contains only one pair $(\mA, \vb)$, then an IFS can essentially be seen as a special recurrent neural network (RNN), which has an identity activation function but no input vector:
\begin{align}
\vv^{(t)} = \mA\vv^{(t-1)} + \vb.
\end{align}
Here, $\vv^{(t)}$ is the RNN's hidden vector. That is, an IFS can be implemented like an RNN if it contains only one transformation. In the following, we extend this notion to IFS with multiple transformations. 

\paragraph{IFS rearrangement.} To begin with, let us rearrange the IFS generation process. Instead of sampling an affine transformation from $\sS$ at each of the $T$ IFS iterations, we choose to sample all $T$ of them before an IFS starts. We can do so because the sampling is independent of the intermediate results of an IFS.
Concretely, according to $\{p_n\}_{n=1}^N$, we first sample an index sequence $\vz = [z^{(1)}, \cdots, z^{(t)}]$, in which $z^{(t)}\in\{1, \cdots, N\}$ denotes the index of the sampled affine transformation from $\sS$ at iteration $t$.  
With the pre-sampled $\vz$, we can perform an IFS in a seemingly deterministic way. This is reminiscent of the re-parameterization trick commonly used in generative models~\cite{kingma2013auto}.

\paragraph{Fractal embedding layer.} We now introduce a \emph{fractal embedding} (\FE) layer, which leverages the rearrangement to implement an IFS like an RNN. The \FE layer has two sets of parameters, {\color{red}$\mS_\text{A}\in\R^{4\times N}$} and {\color{red}$\mS_\text{b}\in\R^{2\times N}$}, which record all the parameters in $\sS$ (cf., \autoref{eq:fractal_code_simple}). Specifically, the $n$\ts{th} column of $\mS_\text{A}$ (denoted as $\mS_\text{A}[:, n]$) records the $2\times 2$ parameters of $\mA_n$; \ie, $\mA_n=\texttt{mat}(\mS_\text{A}[:, n])$, where $\texttt{mat}$ is a reshaping operation from a column vector into a matrix. The $n$\ts{th} column of $\mS_\text{b}$ (denoted as $\mS_\text{b}[:,n]$) records $\vb_n$, \ie, $\vb_n = (\mS_\text{b}[:,n])$. Let us define one more notation $\textbf{1}_{z^{(t)}}\in\R^N$, which is an $N$-dimensional one-hot vector whose $z^{(t)}$\ts{th} element is $1$. With these parameters and notation, the \FE layer carries out the IFS computation at iteration $t$ (cf. \autoref{eq:IFS}) as follows
\begin{align}
\FE(z^{(t)}, \vv^{(t-1)}; \{\mS_\text{A}, \mS_\text{b}\})\nonumber
\end{align}
\begin{align}
&= \texttt{mat}(\mS_\text{A}\textbf{1}_{z^{(t)}}) \vv^{(t-1)} + \mS_\text{b}\textbf{1}_{z^{(t)}} \nonumber\\ 
&= \texttt{mat}(\mS_\text{A}[:, z^{(t)}]) \vv^{(t-1)} + \mS_\text{b}[:,
z^{(t)}] \\
&= \mA^{(t)} \vv^{(t-1)} +\vb^{(t)}. \nonumber
\end{align}
That is, the \FE layer takes the index of the  sampled affine transformation of iteration $t$ (\ie, $z^{(t)}$) and the previous point $\vv^{(t-1)}$ as input, and outputs the next point $\vv^{(t)}$. To perform an IFS for $T$ iterations, we only need to call the \FE layer recurrently for $T$ times. This analogy between IFS and RNNs makes it fairly easy to implement the IFS generation process in modern deep learning frameworks. Please see~\autoref{alg:deep_ifs} for a summary.  

\paragraph{Extension to mini-batch versions.} The \FE layer can easily be extended into a mini-batch version to generate multiple fractal images, if they share the same fractal parameters $\sS$. See~\autoref{fig:diff} (a) for an illustration. 
Interestingly, even if the fractal images are based on different fractal parameters, a simple parameter concatenation trick can enable mini-batch computation.
Let us consider a mini-batch of size $2$ with fractal parameters $\sS_1$ and $\sS_2$. We can combine $\sS_1$ and $\sS_2$ into one $\mS_\text{A}\in\R^{4\times (2N)}$ and 
one $\mS_\text{b}\in\R^{2\times (2N)}$, where the number of columns in $\mS_\text{A}$ and $\mS_\text{b}$ are doubled to incorporate the affine transformations in $\sS_1$ and $\sS_2$. When sampling the index sequence for each fractal, we just need to make sure that the indices align with the columns of $\mS_\text{A}$ and $\mS_\text{b}$ to generate the correct fractal images. 


\subsection{Differentiable rendering}
\label{ss_diff}
The \FE layer enables us to implement an IFS via modern deep learning frameworks, upon which we can enjoy automatic back-propagation to calculate the gradients $\nabla_{\sS}\sL$ (cf. \autoref{eq_basic}) if we have obtained the gradients $\nabla_{\vv^{(t)}}\sL, \forall t\in\{1,\cdots,T\}$. However, obtaining the latter is not trivial. An IFS generates a sequence $\{\vv^{(0)},\cdots,\vv^{(T)}\}$ of 2D points, but it is not immediately clear how to render an image $\mI'$ such that subsequently we can back-propagate the gradient $\nabla_{\mI'}\sL(\mI', \mI)$ to obtain $\nabla_{\vv^{(t)}}\sL$. 

Concretely, $\nabla_{\mI'}\sL$ is a 2D tensor of the same size as $\mI'$, where $\nabla_{\mI'[h, w]}\sL\in\R$ indicates if the pixel $[h, w]$ of $\mI'$ should get brighter or darker. Let us follow the definition in~\autoref{background}: IFS draws points on a black canvas. Then if $\nabla_{\mI'[h, w]}\sL\in\R<0$, ideally we want to have some IFS points closer to or located at $[h, w]$, to make $\mI'[h, w]$ brighter. To do so, however, requires a translation of $\nabla_{\mI'[h, w]}\sL<0$ into a ``pulling'' force towards $\mI'[h, w]$ for some of the points in $\{\vv^{(0)},\cdots,\vv^{(T)}\}$ .

To this end, we propose to draw the IFS points $\{\vv^{(0)},\cdots,\vv^{(T)}\}$\footnote{Following~\cite{kataoka2020pre,Anderson_2022_WACV}, we linearly translate and re-scale the points to fit them into the $H\times W$ image plane.} onto $\mI'$ using an RBF kernel --- every point ``diffuses'' its mass to nearby pixels. This is inspired by the differentiable soft quantization module~\cite{qian2020end} proposed for 3D object detection. The rendered image $I'$ is defined as 
\begin{align}
\label{eq:render}
\mI'[h, w] = \sum_{t} \exp(-\left\|
[h, w]^\top
-\vv^{(t)}\right\|_2^2 / \tau), \nonumber \\ 
h \in [H], w \in [W],
\end{align}
where $\tau>0$ governs the kernel's bandwidth. This module not only is differentiable w.r.t.~$\vv^{(t)}$, but also fulfills our requirement. The closer the IFS points to $[h, w]$ are, the larger the intensity of $\mI'[h, w]$ is. Moreover, if $\nabla_{\mI'[h, w]}\sL<0$, then $\frac{\partial \sL}{\partial \mI'[h, w]}\times \frac{\partial \mI'[h, w]}{\partial \vv^{(t)}}$ will be a 2D vector positively proportional to $(\vv^{(t)} - [h, w]^\top)$. When GD is applied, this will pull $\vv^{(t)}$ towards $[h, w]^\top$. See \autoref{fig:diff} (b) for an illustration. 


Since $\mI'[h, w]$ may have a value larger than $1$, we clamp it back to the range $[0, 1]$ by $\min(\mI'[h, w], 1)$.



\subsection{Objective and optimization}
\label{ss_opt}
In \autoref{ss_deep} and \autoref{ss_diff}, we present an end-to-end differentiable framework for learning $\sS$. In this subsection, we discuss the objective function and present important technical insights to stabilize the optimization. 


\paragraph{Objective function.} As pointed out in the subsequent paragraph of \autoref{eq_basic}, since an IFS is stochastic, the objective function in \autoref{eq_basic} is ill-posed. To tackle this, let us rewrite the stochastic $G(\sS)$ by $G(\sS; \vz)$, in which we explicitly represent the stochasticity in $G(\sS)$ by $\vz$: the sampled index sequence according to $\sS$ (see \autoref{ss_deep}). We then present two objective functions:
\begin{align}
\paragraph{Expectation: } \expect{\vz\sim\sS}{\sL(G(\sS, \vz), \mI)},\nonumber \\ \textbf{Fixed: } \expect{\vz\in\sZ}{\sL(G(\sS, \vz), \mI)}, \label{eq_new_obj}
\end{align}
where $\vz\sim\sS$ indicates that $\vz$ is sampled according to $\sS$; $\sZ$ is a set of pre-sampled fixed sequences. For instance, we can sample $\sZ$ according to the initial $\sS$ and fix it throughout the optimization process. The {Expectation} objective encourages every $\vz\sim\sS$ to generate $\mI$. The learned $\sS$ thus would easily generate images $\mI'$ close to $\mI$, but the diversity among $\mI'$ might be limited. The {Fixed} objective is designed to alleviate this limitation by only forcing a (small) set of fixed sequences $\sZ$ to generate $\mI$. However, since $\sZ$ is pre-sampled, their probability of being sampled from the learned $\sS$ might be low. Namely, it might be hard for the learned $\sS$ to generate images like $\mI$. We evaluate both in~\autoref{exp}.




\paragraph{Optimization.} We apply mini-batch stochastic gradient descent (SGD) to optimize the objectives w.r.t.~$\sS$ in~\autoref{eq_new_obj}. 
To avoid confusion, we use ``iteration'' for the IFS generation process, and ``step'' for the SGD optimization process. The term ``stochastic'' refers to sampling a mini-batch of $\vz$ at every optimization step from either $\sS$ or $\sZ$, generating the corresponding fractal images $\mI'$ by IFS, and calculating the average gradients w.r.t.~$\sS$.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/stability3-101.pdf}
\vskip -5pt
\caption{\textbf{The training loss along the steps of SGD w/ and w/o adding random noise to the fractal parameters $\sS$.} We add a Gaussian noise $\sim\mathcal{N}(0, 0.1)$ to each element of the fractal parameters $\sS$ every $5$ SGD steps. This makes the final loss lower and the optimization robust. (The ``shaded'' area is the standard deviation across different random seeds.)}   
\vskip -10pt
\label{fig:stable}
\end{figure}


We note that unlike neural networks that are mostly over-parameterized, $\sS$ has much fewer, usually tens of parameters. This somehow makes the optimization harder. Specifically, we found that the optimization easily gets stuck at poor local minima. (In contrast, over-parameterized models are more resistant to it~\cite{du2018gradient,allen2019learning,arora2019fine}.) To resolve this, whenever we calculate the stochastic gradients, we add a small Gaussian noise to help the current parameters escape local minima, inspired by~\cite{welling2011bayesian}. \autoref{fig:stable} shows that this design stabilizes the optimization process and achieves lower loss. More results are in~\autoref{exp}.

\paragraph{Reparameterized $\sS$.} 
In the extreme case where $\sS$ contains only one affine transformation $(\mA, \vb)$, recurrently applying it multiple times
could easily result in exploding gradients when the maximum singular value of $\mA$ is larger than $1$~\cite{pascanu2013difficulty}. 
This problem happens when $\sS$ contains multiple affine transformations as well, if most of the matrices have such a property. 
To avoid it, we follow~\cite{Anderson_2022_WACV} to decompose ``each'' $\mA$ in $\sS$ into rotation, scaling, and flipping matrices
\begin{align}
    & {\tiny
    \mA=\underbrace{\begin{bmatrix}
    \text{cos}(\theta) & -\text{sin}(\theta) \\
    \text{sin}(\theta) & \text{cos}(\theta) 
    \end{bmatrix}}_{\text{rotation}} 
    \underbrace{\begin{bmatrix}
    \sigma_1 & 0 \\
    0 & \sigma_2
    \end{bmatrix}}_{\text{scaling}} 
    \underbrace{\begin{bmatrix}
    \text{cos}(\phi) & -\text{sin}(\phi) \\
    \text{sin}(\phi) & \text{cos}(\phi) 
    \end{bmatrix}}_{\text{rotation}} 
    \underbrace{\begin{bmatrix}
    d_1 & 0 \\
    0 & d_1
    \end{bmatrix}}_{\text{flipping}},
    } \label{eq_repara}
\end{align}
where $\sigma_1, \sigma_2 \geq 0; d_1, d_2 \in \{-1, 1\}$. That is, $\mA$ is \emph{reparameterized} by $\{\theta, \phi, \sigma_1, \sigma_2, d_1, d_2\}$.
We note that \citet{Anderson_2022_WACV} used this decomposition to sample $\mA$. The purpose was to generate diverse fractal images for pre-training.
Here, we use this decomposition for a drastically different purpose: to stabilize the optimization of $\mA$. We leverage one important property --- $\max(\sigma_1, \sigma_2)$ is exactly the maximum singular value of $\mA$ --- and clamp both $\sigma_1$ and $\sigma_2$ into $[0, 1]$ to avoid exploding gradients.




\begin{figure*}[t!]
\centering
\includegraphics[width=0.75\textwidth]{figures/qualitative_results_v3.pdf}
\vskip -10pt
\caption{\small Image reconstruction examples. Left: FractalDB images. Right: MNIST (row 1-3), KMNIST (row 4-6), and FMNIST images (row 7-9). (A): the target images. (B): an inverse encoder baseline. (C-F):  variants of our approach (corresponding to the ``Index'' column in~\autoref{tbl:inversion_mse}).  The last row shows two examples and the corresponding MSE.}   
\label{fig:inversion}
\end{figure*}    



\begin{table*}[h!]
\footnotesize
\centering
\setlength{\tabcolsep}{1.5pt} 
\vskip -0pt          
\caption{\small Averaged MSE (mean$\pm$std over $3$ runs) of image reconstruction on FractalDB/MNIST/KMNIST/FMNIST dataset.}
\vskip -7pt
\begin{tabular}{l|c|ccc|cccc}
\toprule
 Index & Method & Objective & Clamp & Noisy & FractalDB & MNIST & KMNIST & FMNIST \\
\midrule
(B) & Inverse encoder & - & - & - & $0.3889 \pm 0.0000$ & $0.1724 \pm 0.0000$ & $0.3016 \pm 0.0000$ & $0.3658 \pm 0.0000$ \\
\midrule
(C) &\multirow{4}{*}{Ours} & Fixed & \xmark & \xmark & $0.0790\pm0.0006$ & $0.0475 \pm 0.0012$ & $0.1023 \pm 0.0016$ & $0.0692 \pm 0.0010$ \\
(D) & &Fixed & \cmark & \xmark & $0.0752\pm0.0027$ & $0.0312 \pm 0.0002$ & $0.0979 \pm 0.0008$ & $0.0765 \pm 0.0034$ \\
(E) & &Expectation & \cmark & \xmark & $0.0630\pm0.0008$ & $0.0223 \pm 0.0004$ & $0.0826 \pm 0.0010$ & $0.0547 \pm 0.0005$ \\
(F) & &Expectation & \cmark & \cmark & $\mathbf{0.0617\pm0.0009}$ & $\mathbf{0.0202 \pm 0.0005}$ & $\mathbf{0.0781 \pm 0.0015}$ & $\mathbf{0.0538 \pm 0.0006}$ \\
\midrule
& Ours (w/o Reparam. $\sS$) & Expectation & \cmark & \cmark  & Exploded & Exploded & Exploded & Exploded\\
\bottomrule
\end{tabular}
\vskip -5pt
\label{tbl:inversion_mse}
\end{table*}





\subsection{Extension}
\label{ss_extension}
Besides the pixel-wise square loss $\sL(\mI',\mI)=\|\mI'-\mI\|_2^2$, our approach can easily be compatible with other loss functions on $\mI'$. The loss may even come from a downstream task. In~\autoref{ss_GAN}, we experiment with one such extension. Given a set of images $\{\mI_m\}_{m=1}^M$, we aim to learn a set of fractal parameters $\{\sS_j\}_{j=1}^J$ such that their generated images are distributionally similar to $\{\mI_m\}_{m=1}^M$. We apply a GAN loss~\cite{goodfellow2014generative}. In the optimization process, we train a discriminator to tell real images from generated images, and use it to provide losses to the generated images.
 