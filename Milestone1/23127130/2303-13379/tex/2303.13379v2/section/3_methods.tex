\section{Methods}

A systematic scoping review was conducted in this study as this method has been frequently used in emerging and rapidly evolving research areas to scope a body of literature and identify the key concepts, methods, evidence, and challenges \citep{munn2018systematic}. Consequently, the quality of the included studies was often not assessed as the aim is to provide a boarder picture of an emerging field.

\subsection{Review Procedures}
We followed the PRISMA \citep{page2021prisma} protocol to conduct the current systematic scoping review of LLMs. We searched four reputable bibliographic databases, including Scopus, ACM Digital Library, IEEE Xplore, and Web of Science, to find high-quality peer-reviewed publications. Additional searches were conducted through Google Scholar and Education Resources Information Center (ERIC) to identify peer-reviewed publications that have yet to be indexed by these databases, either recently published or not indexed (e.g., Journal of Educational Data Mining; prior to 2020). Our initial search query for the title, abstract, and keywords included terms such as "large language model", "pre*trained language model", "GPT-*", "BERT", "education", "student*", and "teacher*". A publication year constraint was also applied to restrict the search to studies published since 2017, specifically from 01/01/2017 to 12/31/2022, as the foundational architecture (Transformer) of LLMs was formally released in 2017 \citep{vaswani2017attention}. Only peer-reviewed publications were considered to enhance the scientific credibility of this review. The initial database search was conducted by two researchers independently. Any discrepancies between the search results were resolved through further discussion or consulting the librarian for guidance.

Two researchers independently reviewed the titles and abstracts of eligible articles based on five predetermined inclusion and exclusion criteria. First, we included studies that used large or pre-trained language models directly or built on top of such models, and excluded studies that used general machine-learning or deep-learning models with unspecified usage of LLMs. Second, we included empirical studies with detailed methodologies, such as a detailed description of the LLMs and research procedures, and excluded review, opinion, and scoping works. Third, we only included full-length peer-reviewed papers, and excluded short, workshop, and poster papers that were less than six and eight pages for double- and single-column layouts, respectively. Additionally, we included studies that used LLMs for the purpose of automating educational tasks (e.g., essay grading and question generation), and excluded studies that merely used LLMs as part of the analysis without educational implications. Finally, we only included studies that were published in English (both the abstract and the main text) and excluded studies that were published in other languages. Any conflicting decisions were resolved through further discussion between the two researchers or consulting with a third researcher to achieve a consensus.

The database search initially yielded 854 publications, with 191 duplicates removed, resulting in 663 publications for the title and abstract screening (see Figure \ref{fig:prisma}). After the title and abstract screening, 197 articles were included for the full-text review with an inter-rater reliability (Cohen's kappa) of 0.75, indicating substantial agreement between the reviewers during the title and abstract screening. A total of 118 articles were selected for data extraction after the full-text review with an inter-rater reliability (Cohen's kappa) of 0.73, indicating substantial agreement between the reviewers during the full-text review. Out of the initial 197 articles, 79 were excluded for various reasons, including not full paper (n=41), lack of educational automation (n=17), lack of pre-trained or LLMs (n=12), merely using pre-trained or LLMs as part of the analysis (n=3), non-English paper (n=2), and non-empirical paper (n=2). 

\begin{figure}[htbp]
    \centering
      \includegraphics[width=.99\textwidth]{figure/PRISMA.png}
    \caption{Systematic scoping review process following the PRISMA protocol.}
    \label{fig:prisma}
\end{figure}

\subsection{Data Analysis}
\label{sec:analysis}

For the first research question (RQ1), we conducted an inductive thematic analysis to extract information regarding the current state of research on using LLMs to automate educational tasks. Specifically, we extracted four primary types of contextual information from each included paper: educational tasks, stakeholders, LLMs, and machine-learning tasks. This contextual information would provide a holistic view of the existing research and inform researchers and practitioners regarding the viable directions to explore with the state-of-the-art LLMs (e.g., GPT-3.5 and Codex). A total of seven data extraction items were developed to address the second and third research questions. These items were developed as they are directly related to the definition of practicality (RQ2: Item 1-3) and ethicality (RQ3: Item 4-7), as defined in the Background section (Section \ref{Definitions}). The following list elaborates on the final set of items along with the corresponding guiding questions. For the thematic analysis and Items, two researchers independently coded 20 random samples of the included studies. Any conflicts were resolved through further discussion or consulting a third researcher. After reaching a Cohen's kappa of more than 0.80 (indicating almost perfect agreement), each researcher coded half of the remaining 98 studies (49 studies each) and cross-checked each other's work. The database of the studies included in this review and the extracted data for each item are available in the supplementary document.
\begin{enumerate}

    \item \textbf{Technology readiness} What levels of technology readiness are the LLMs-based innovations at? We adopted the assessment tool from the Australian government, namely the Australian Department of Defence's Technology Readiness Levels (TRL) \citep{TRL2020}, which has been used to assess the maturity of educational technologies in prior SLR \citep{yan2022scalability}. There are nine different technological readiness levels: Basic Research (TRL-1), Applied Research (TRL-2), Critical Function or Proof of Concept Established (TRL-3), Lab Testing/Validation of Alpha Prototype Component/Process (TRL-4), Laboratory Testing of Integrated/Semi-Integrated System (TRL-5), Prototype System Verified (TRL-6), Integrated Pilot System Demonstrated (TRL-7), System Incorporated in Commercial Design (TRL-8), and System Proven and Ready for Full Commercial Deployment (TRL-9), further explained in the Result section.  
    
    \item \textbf{Performance}: How accurate and reliable can the LLMs-based innovations complete the designated educational tasks? For example, what are the model performance scores for classification (e.g., AUC and F1 scores), generation (e.g., BLEU score), and prediction tasks (e.g., RMSE and Pearson's correlation)? 

    \item \textbf{Replicability}: Can other researchers or practitioners replicate the LLMs-based innovations without additional support from the original authors? This item evaluates whether the paper provided sufficient details about the LLMs (e.g., open-sourced algorithms) and the dataset (e.g., open-source data).      

    \item \textbf{Transparency}: What tiers of transparency index \citep{chaudhry2022transparency} are the LLMs-based innovations at? The transparency index proposed three tiers of transparency, including transparent to AI researchers and practitioners (Tier 1), transparent to educational technology experts and enthusiasts (Tier 2), and transparent to educators and parents (Tier 3). The tier of transparency increases as educational stakeholders become fully involved in developing and evaluating the AI system. These tiers were further elaborated on in the Results section.  

    \item \textbf{Privacy}: Has the paper mentioned or considered privacy issues of their innovations? This item explores potential issues related to informed consent, transparent data collection, individuals' control over personal data, and unintended surveillance \citep{ferguson2016guest,tsai2020privacy}.
    
    \item \textbf{Equality}: Has the paper mentioned or considered equal access to their innovations? This item explores potential issues related to limited access for students from low-income backgrounds or rural areas and the linguistic limitation of the innovations, such as their capability to analyse different languages \citep{ferguson2016guest}.
    
    \item \textbf{Beneficence}: Has the paper mentioned or considered potential issues that violate the ethical principle of beneficence? Such violations may include the risks associated with labelling and profiling students, inadequate usage of machine-generated content for assessments, and algorithmic biases \citep{ferguson2016guest,zawacki2019systematic}.
    
\end{enumerate}