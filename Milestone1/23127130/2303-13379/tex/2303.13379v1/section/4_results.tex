\section{Results}

\subsection{The Current State --- RQ1}

We identified nine different categories of educational tasks that prior studies have attempted to automate using LLMs (as shown in Table \ref{tab:educational-tasks}). Prior studies have used LLMs to automate the profiling and labelling of 17 types of education-related contents and concepts (e.g., forum posts, student sentiment, and discipline similarity), the detection of six latent constructs (e.g., confusion and urgency), the grading of five types of assessments (e.g., short answer questions and essays), the development of five types of teaching support (e.g., conversation agent and intelligent question-answering), the prediction of five types of student-orientated metrics (e.g., dropout and engagement), the construction of four types of knowledge representations (e.g., knowledge graph and entity recognition), the provision of four different forms of feedback (e.g., real-time and post-hoc feedback), the generation of four types of content (e.g., MCQs and open-ended questions), and the delivery of three types of recommendations (e.g., resource and course). Of the 118 reviewed studies, 85 studies aimed to automate educational tasks related to teachers (e.g., question grading and generation), 54 studies targeted student-related activities (e.g., feedback and resource recommendation), 20 studies focused on supporting institutional practices (e.g., course recommendations and discipline planning), and 14 studies empowered researchers with automated methods to investigate latent constructs (e.g., student confusion) and capture verbal data (e.g., speech recognition). 

We identified five categories of LLMs used in prior studies to automate educational tasks. BERT and its variations (e.g., RoBERTa, DistilBERT, multilingual BERT, LaBSE, EstBERT, and Sentence-BERT) were the most predominant model used in 109 reviewed studies. However, they often required manual effort for fine-tuning (n=90). GPT-2 and GPT-3 have been used in five and three studies, respectively. OpenAI's Codex has been used in two prior studies, specifically for code generation tasks. T5 has also been used in two prior studies for classification and generation purposes. In terms of machine-learning tasks, 74 studies used LLMs to perform classification tasks. Generation and prediction tasks were investigated in 24 and 23 prior studies, respectively. In sum, LLMs-based innovations have already been used to automate a range of educational tasks, but most of these innovations were developed on older models, such as BERT and GPT-2. Although state-of-the-art models, such as GPT-3, have been introduced for over two years \cite{brown2020language}, they have yet to be widely applied to automate educational tasks. A potential reason for this lack of adoption could be these models' commercial and close-sourced nature, increasing the financial burdens of developing and operating educational technology innovations on top of such models. 


\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{3}
    \caption{Educational Tasks in LLMs Research}
    \begin{tabular}{|p{3cm}|p{9.5cm}|}
    \hline
    Categories &
      Educational Tasks \\
      \hline
    Profiling and Labelling &
      Forum post classification, dialogue act classification, classification of learning designs, review sentiment analysis, topic modelling, pedagogical classification of MOOCs, collaborative problem-solving modelling, paraphrase quality, speech tagging, labelling educational content with knowledge components, key sentence and keyword extraction, reflective writing analysis, multimodal representational thinking, discipline similarity, concept classification, cognitive level classification, essay arguments segmentation \\
    Detection &
      Semantic analyses, detecting off-task messages, confusion detection, urgency detection, conversational intent detection, teachers' behaviour detection \\
    Assessment and Grading &
      Formative and summative assessment grading, short answer grading, essay grading, subjective question grading, student self-explanation \\  
    Teaching Support &
      Classroom teaching, learning community support, online learning conversation agent, intelligent question-answering, teacher activity recognition \\
    Prediction &
      Student performance prediction, student dropout prediction, emotional and cognitive engagement detection, growth and development indicators for college students, at-risk student identification \\
    Knowledge Representation &
      Knowledge graph construction, knowledge entity recognition, knowledge tracing, cause-effect relation extraction \\  
    Feedback &
      Real-time feedback, post-hoc feedback, aggregated feedback, feedback on feedback (peer-review comments) \\
    Content Generation &
      MCQs generation, open-ended question generation, code generation, reply (natural language) generation \\
    Recommendation &
      English reference selection and recommendation, resource recommendation, course recommendation \\
    \hline
    \end{tabular}
    \label{tab:educational-tasks}
\end{table}

\subsection{Practical Challenges --- RQ2}

\subsubsection{Technology readiness}

According to the Technology Readiness Level scale \cite{TRL2020}, the LLMs-based innovations are still in the early development and testing stage. Over three-quarters of the LLMs studies (n=89) are in the applied research stage (TRL-2), which aims to experiment with the capability of LLMs in automating different educational tasks by developing different models and combining LLMs with other machine-learning and deep-learning techniques (e.g., RCNN \cite{shang2022representation}). Thirteen studies have established a proof of concept and demonstrated the feasibility of using LLMs-based innovations to automate certain processes of educational tasks (TRL-3). Nine studies have developed functional prototypes and conducted preliminary validation under controlled laboratory settings (TRL-4), often involving stakeholders (e.g., students and teachers) to test and evaluate the output of their innovations. Only seven studies have taken a further step and conducted validation studies in authentic learning environments, with most functional components integrated into the educational tasks (TRL-5), such as an intelligent virtual standard patient for medical students training \cite{song2022intelligent} and an intelligent chatbot for university admission \cite{nguyen2021neu}. Yet, none of the existing LLMs-based innovations has been verified through successful operations (TRL-6). Together, these findings suggest although existing LLMs-based innovations can be used to automate certain educational tasks, they have yet to show evidence regarding improvements to teaching, learning, and administrative processes in authentic educational practices.

\subsubsection{Performance}

The performance of LLMs-based innovations varies across different machine-learning and educational tasks. For classification tasks, LLMs-based innovations have shown high performance for simple educational tasks, such as modelling the topics from a list of programming assignments (best F1 = 0.95) \cite{fonseca2020automatic}, analysing the sentiment of student feedback (best F1 = 0.94) \cite{truong2020sentiment}, constructing subject knowledge graph from teaching materials (best F1 = 0.94) \cite{su2020automatic}, and classifying educational forum posts \cite{sha2022latest} (best F1 = 0.92). However, the classification performance of LLMs-based innovations decreases for other educational tasks. For example, the F1 scores for detecting student confusion in the course forum \cite{geller2021new} and students' off-task messages in game-based collaborative learning \cite{carpenter2020detecting} are around 0.77 and 0.67, respectively. Likewise, the F1 score for classifying short-answer responses varies between 0.61 to 0.82, with the lower performance on out-of-sample questions (best F1 = 0.61) \cite{condor2021automatic}. Similar performances were also observed in classifying students' argumentative essays (best F1 = 0.66) \cite{ghosh2020exploratory}. 

For prediction tasks, LLMs-based innovations have demonstrated reliable performance compared to ground truth or human raters. For example, LLMs-based innovations have achieved high scores of quadratic weighted kappa (QWK) in essay scoring, specifically for off-topic (QWK = 0.80), gibberish (QWK = 0.80), and paraphrased answers (QWK = 0.94), indicating substantial to almost perfect agreements with human raters \cite{doewes2021limitations}. Similar performances on essay scoring have been observed in several other studies (e.g., 0.80 QWK in \cite{beseiso2021novel} and 0.81 QWK in \cite{sharma2021feature}). Likewise, LLMs-based innovations' performances on automatic short-answer grading were also highly correlated with human ratings (Pearson's correlation between 0.75 to 0.82) \cite{ahmed2022application,sawatzki2022deep}.

Regarding generation tasks, LLMs-based innovations demonstrated high performance across different educational tasks. For example, LLMs-based innovations have achieved an F1 score of 0.92 for generating MCQs with single-word answers \cite{kumar2022identification}. Educational technologies developed by fine-tuning Codex also demonstrated the capability of resolving 81\% of the advanced mathematics problems \cite{drori2022neural}. Text summaries generated using BERT had no significant differences compared with student-generated summaries and can not be differentiated by graduate students \cite{merine2022risks}. Similarly, BERT-generated doctor-patient dialogues were also found to be indistinguishable from actual doctor-patient dialogues, which can be used to create virtual standard patients for medical students' diagnosis practice training \cite{song2022intelligent}. Additionally, for introductory programming courses, the state-of-the-art LLMs, Codex, could generate sensible and novel exercises for students along with an appropriate sample solution (around three out of four times) and accurate code explanation (67\% accuracy) \cite{sarsa2022automatic}. 

In sum, although the classification performance of LLMs-based innovations on complex educational tasks is far from suitable for practical adoption, LLMs-based innovations have already shown high performance on several relatively simple classification tasks that could potentially be deployed to automatically generate meaningful insights that could be useful to teachers and institutions, such as navigating through numerous student feedback and course review. Likewise, LLMs-based innovations' prediction and generation performance reveals a promising future of potentially automating the generation of educational content and the initial grading of student assessments. However, ethical issues must be considered for such implementations, which we covered in the findings for RQ3. 

\subsubsection{Replicability}

Most reviewed studies (n=107) have not disclosed sufficient details about their methodologies for other researchers and practitioners to replicate their proposed LLMs-based innovations. Among these studies, 12 studies have open-sourced the original code for developing the innovations but failed to open-source the data they used. In contrast, 20 studies have open-sourced the data they used but failed to release the actual code. Around two-thirds of the reviewed studies (n=75) have failed to release both the original code and the data they used, leaving only 11 studies publicly available for other researchers and practitioners to replicate without needing to conduct the original authors. This lack of replicability could become a vital barrier to adoption, as 87 out of the 107 non-replicable studies required fine-tuning the LLMs to achieve the reported performance. This replication issue also limits others from further evaluating the generalisability of the proposed LLMs-based innovations in other datasets, constraining potential practical utilities.

\subsection{Ethical Challenges --- RQ3}

\subsubsection{Transparency}

Based on the transparency index and the three tiers of transparency \cite{chaudhry2022transparency}, most of the reviewed study reached at-most Tier 1 (n=109), which is merely considered transparent to AI researchers and practitioners. Although these studies reported details regarding their machine learning models (e.g., optimisation and hyperparameters), such information is unlikely to be interpretable and considered transparent for individuals without a strong background in machine learning. For the remaining nine studies, they reached at-most Tier 2 as they often involved some form of human-in-the-loop elements. Specifically, making the LLMs innovations available for student evaluation has been found in three studies \cite{nguyen2021neu,song2022intelligent,merine2022risks}. Such evaluations often involved students differentiating AI-generated from human-generated content \cite{song2022intelligent,merine2022risks} and assessing student satisfaction with AI-generated responses \cite{nguyen2021neu}. Likewise, two studies have involved experts in evaluating specific features of the content generated by the LLMs-based innovations, such as informativeness \cite{maheen2022automatic} and cognitive level \cite{moore2022assessing}. Surveys have been used to evaluate students' experience with LLMs-based innovations from multiple perspectives, such as the quality and difficulty of AI-generated questions \cite{drori2022neural,li2021natural} and potential learning benefits of the systems \cite{jayaraman2022effectiveness}. Finally, semi-structured interviews have been conducted to understand students' perception of the LLM system after using the system in authentic computer-supported collaborative learning activities \cite{zheng2022effects}. Although these nine studies had some elements of human-in-the-loop, stakeholders were often involved in a post-hoc evaluation manner instead of throughout the development process, and thus, have limited knowledge regarding the operating principle and potential weakness of the systems. Consequently, none of the existing LLMs-based innovations can be considered as being at Tier 3, which describes an AI system that is considered transparent for educational stakeholders (e.g., students, teachers, and parents).

\subsubsection{Privacy} 

The privacy issues related to LLMs-based innovations were rarely attended to or investigated in the reviewed studies. Specifically, for studies that have fine-tuned LLMs with textual data collected from students, none of these studies has explicitly explained their consenting strategies (e.g., whether students acknowledge the collection and intended usage of their data) and data protection measures (e.g., data anonymisation and sanitisation). This lack of attention to privacy issues is particularly concerning as LLMs-based innovations work with stakeholders' natural languages that may contain personal and sensitive information regarding their private lives and identities \cite{brown2022does}. It is possible that stakeholders might not be aware of their textual data (e.g., forum posts or conversations) on digital platforms (e.g., MOOCs and LMS) being used in LLMs-based innovations for different purposes of automation (e.g., automated reply and training chatbots) as the consenting process is often embedded into the enrollment or signing up of these platforms \cite{tsai2017learning}. This process can hardly be considered informed consent. Consequently, if stakeholders shared their personal information on these platforms in natural language (e.g., sharing phone numbers and addresses with group members via digital forums), such information could be used as training data for fine-tuning LLMs. This usage could potentially expose private information as LLMs are incapable of understanding the context and sensitivity of text, and thus, could return stakeholders' personal information based on semantic relationships \cite{brown2022does}.

%We only identified one study explicitly mentioning the privacy issues related to LLMs-based innovations \cite{merine2022risks}. In this study, the authors mentioned the need to consider privacy issues as such issues could expose students and teachers to cybercrime, where their personal information can be misused for malicious purposes (e.g., identity theft). Such privacy issues are particularly concerning as more than a half of the LLMs-based innovations (n=64) involved some form of data labelling during the development process, which increases the exposure of potentially personal and sensitive data (e.g., demographics and dialogues). In the Discussion section, we further elaborated on the lack of privacy considerations in the literature.

\subsubsection{Equality} 

Although most of the studies (n=95) used LLMs that only apply to English content, we also identified application scenarios of LLMs in automating educational tasks in 12 other languages. Specifically, 19 studies used LLMs that can be applied to Chinese content. Ten prior studies used LLMs for Vietnamese (n=3), Spanish (n=3), Italian (n=2), and German (n=2) contents. Additionally, seven studies applied LLMs to Croatian, Indonesian, Japanese, Romanian, Russian, Swedish, and Hindi content. While the dominance of English-based innovations remains a concerning equality issue, the availability of innovations that support a variety of other languages, specifically in none western, educated, industrialized, rich and democratic (WEIRD) societies (e.g., Indonesia and Vietnam), may indicate a promising sign for LLMs-based innovations to have potential global impacts and levels such equality issues in the future. However, the financial burdens from adopting the state-of-the-art models (e.g., OpenAI's GPT-3 and Codex) could potentially exacerbate the equality issues, making the best-performing innovations only accessible and affordable to WEIRD societies.

\subsubsection{Beneficence}

A total of seven studies have discussed potential issues related to the violation of the ethical principle of beneficence. For example, one study has discussed the potential risk of adopting underperforming models, which could negatively affect students' learning experiences \cite{li2021natural}. Such issues could be minimised by deferring decisions made by such models \cite{schneider2022towards} and labelling the AI-generated content with a warning message (e.g., teachers' manual revision is mandatory before determining the actual correctness) \cite{angelone2022improved}. Apart from issues with adopting inaccurate models, two studies have suggested that potential bias and discrimination issues may occur if adopting a model that is accurate but unfair \cite{sha2021assessing,merine2022risks}. This issue is particularly concerning as most existing studies focused solely on developing an accurate model. Only nine reviewed studies released information regarding the descriptive data of different sample groups, such as gender and ethicality (e.g., \cite{pugh2021say}). Two studies have proposed potential approaches that could address such fairness issues. Specifically, using sampling strategies, such as balancing demographic distribution, has been found as an effective approach to improve both model fairness and accuracy \cite{sha2022leveraging,sha2022bigger}. These approaches are essential for ensuring that LLMs-based innovations will not perpetuate problematic and systematic biases (e.g., gender biases), especially as the best-performing LLMs are often black-boxed with little interpretability, traceability, and justification of the results \cite{wu2022analysis}. 

 