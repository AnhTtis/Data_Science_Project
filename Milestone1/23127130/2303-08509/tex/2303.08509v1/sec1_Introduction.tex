\section{Introduction}


Occupying about $85\%$ of the global mobile operating system market, Android has become the main target of mobile malware in the world. A recent security report shows that on average, about 10000 new mobile malware samples were captured per day \cite{gdata}. The rapidly increasing of malware poses severe threats to Android users \cite{DBLP:conf/uss/SunSLM21,DBLP:conf/icssa/KimL18,DBLP:journals/tmc/LiuLZWZ20}, e.g., privacy leakage and economic losses. To tackle this problem, a variety of machine learning based Android malware detection methods have been designed to identify malware based on their features \cite{DBLP:conf/ccs/ZhangZZDCZZY20,DBLP:conf/ndss/MaricontiOACRS17,DBLP:conf/kbse/WuLZYZ019,DBLP:journals/tsmc/YuanJLC21,DBLP:conf/ndss/ArpSHGR14,DBLP:journals/tii/LiSYLSY18,DBLP:conf/kdd/HouYSA17,DBLP:journals/tifs/WangWFLHZ14,DBLP:conf/ccs/ZhangDYZ14,DBLP:conf/icccn/HuTMZZH14}. As a common feature for Android malware detection, Function Call Graph (FCG)  \cite{DBLP:conf/ccs/ZhangZZDCZZY20,DBLP:conf/ndss/MaricontiOACRS17,DBLP:conf/kbse/WuLZYZ019,DBLP:conf/kdd/HouYSA17,DBLP:journals/tifs/WangWFLHZ14,DBLP:conf/ccs/ZhangDYZ14,DBLP:conf/icccn/HuTMZZH14} (e.g., frequent subgraph \cite{DBLP:journals/tifs/Fan0LCTZL18} and E-FCG \cite{DBLP:journals/ijon/CaiJGLY21}) provides important clues for understanding how Android apps work. {In an FCG}, every node represents a function or an \textit{abstracted} function (e.g., class, package or family), and every edge denotes the calling relationship between caller and callee. 
\begin{figure}[bhtp]   
	\centering   
	\includegraphics[width=1\linewidth]{framework.pdf}
	\caption{{FCG based Android malware detection framework.} }  
	\label{fig:motivation}   
\end{figure} 
As depicted in Fig. \ref{fig:motivation}, {the FCG} based Android malware detection usually consists of three steps. First, the {FCG} feature (e.g., frequent subgraph) is extracted from the Android Package (APK) file. Second, the {FCG} is transformed into a feature vector, i.e., graph embedding. Third, the feature vector is processed for malware prediction. Existing studies \cite{DBLP:conf/ccs/ZhangZZDCZZY20,DBLP:conf/ndss/MaricontiOACRS17,DBLP:conf/kbse/WuLZYZ019} demonstrate that the {FCG} based Android malware detection methods can achieve promising performance. 

Unfortunately, the {FCG} based malware detection is susceptible to {adversarial examples (AEs) \cite{DBLP:journals/corr/SzegedyZSBEGF13,DBLP:conf/uss/SuyaC0020,DBLP:conf/sp/Carlini017,DBLP:conf/ijcai/XuC0CWHL19,DBLP:conf/www/SunWTHH20,DBLP:conf/kdd/0001WDWT21}}, which are generated by imposing well-crafted adversarial perturbations on normal examples to induce misclassification. To evade detection, an adversary just needs to manipulate a malicious app  by elaborately modifying (e.g., inserting non-functional function calls) and repackaging its code. Although malware manipulation takes place in problem space (depicted by the {first} box in Fig. \ref{fig:motivation}), it changes the {FCG} (e.g., adding new edges) and perturbs the feature vector in feature space (described by the {second} box in Fig. \ref{fig:motivation}). Once the perturbation helps the feature vector stride over the target classifier's decision boundary, the repackaged malware will evade detection. {Up to now, a variety of AE attacks towards Android malware detection have been proposed to produce evasive Android malware}. Most of them \cite{huang2018adversarial,grosse2017adversarial,hu2017generating,2019lh,DBLP:journals/tifs/LiL20} direct at non-graph features (i.e., syntax features) based detection models that use binary feature vectors  for app classification. % For instance, \cite{hu2017generating,2019lh} apply generative adversarial net (GAN) to modify malware's syntax features and design adversarial examples. 
 {Recently, increasing attention has been paid to} the AE attacks towards graph feature  (i.e., semantic feature) based detection models \cite{DBLP:journals/corr/abs-2110-03301}\cite{chen2020android}. {For example,  Bostani \emph{et al.} \cite{DBLP:journals/corr/abs-2110-03301} leverage random search to find optimal perturbation for APK files in a black-box setting. Chen \emph{et al.} \cite{chen2020android} propose a method to exert optimal perturbations on Android APK files.}
% To the best of our knowledge, only one AE attack method, i.e., \textit{Android HIV} [], has been developed against the graph-based Android malware detection. This method focuses on the scenario where Markov chains constructed from FCGs are used as the feature vectors for classification. It first figures out the adversarial perturbations (e.g., enlarging the transition probability between two states) that help the feature vector deceive its target classifier, and then modifies the FCG (e.g., adding more function calls) to match the perturbed feature vector. Guided by the modified FCG, it manipulates the malware sample to evade detection. As a pioneering method, however, Android HIV has two limitations. First, it requires the knowledge about feature space, including 1) Markov chains are used for classification, and 2) how to obtain Markov chains from FCGs. Second, it also requires the knowledge about problem space, i.e., which kind of FCGs (e.g., frequent subgraph or XXX) is used as the input of target system. Only with this knowledge can Android HIV accurately locate the function calls that needs to be increased or reduced. Unfortunately, these requirements are hard to satisfy in practice, since the process of graph construction and feature vector generation are invisible to adversaries. 

{Up to now, how to produce Android malware to circumvent the FCG based  detection is still an open issue. This motivates us to investigate the generation of AEs to fight against the FCG based Android malware detection. In practice, building evasive malware needs to consider the following realistic problems that have not been well addressed.}

\noindent (1) \textit{Malware functionality preservation}. {The malware manipulation should be able to} mislead its target classifier {in the premise of malware functionality preservation}. 
% The functionality preservation requirement strictly restricts the manipulation on malware. 

\noindent (2) \textit{Problem-feature space gap}. 
{Since the feature vector in feature space cannot be directly perturbed, adversaries have to modify malware code in problem space} and expect their modification brings about the desired adversarial perturbation on feature vector. 

\noindent(3) \textit{Strict black-box setting}. For adversaries, the target classifier is a strict black box and its architecture, parameters and output probabilities are all unknown. 

\noindent(4) \textit{{Feature information absence}}. Adversaries cannot get the {feature used by their target classifier, i.e., the FCG and the feature vector obtained by graph embedding (denoted in the second box of Fig. \ref{fig:motivation}).} {Moreover, a detection system may use one of several possible feature granularities, e.g., class level, package level and family level (as discussed in Subsection \ref{subsec: features}). In practice, the feature granularity information is often unavailable to adversaries. }


% {In realistic applications, an FCG based Android malware detection system may use one of several possible feature granularities, e.g., class level, package level and family level, which will be discussed in Subsection \ref{subsec: features}. For practical consideration, we assume that adversaries have to generate real evasive malware under incomplete feature information. Here incomplete feature information means adversaries does not know the granularity of the FCG feature.} 

{To overcome the above challenges}, we design a \textbf{b}lack-box \textbf{a}ttacks towards {FC\textbf{G}} based \textbf{A}ndroid \textbf{m}alware detection with \textbf{m}ulti-population co-ev\textbf{o}lution, termed {\ourtool}. {{\ourtool} works under the \textit{incomplete feature information} condition, which means adversaries do not know the granularity of the FCG feature used by their target system. }% The manipulation technique is used to modify the codes of malware samples for detection evasion, according to the derived adversarial perturbation. 
Our main tasks include designing a malware manipulation technique used in problem space, and developing an algorithm to derive adversarial perturbation in feature space. {{\ourtool} constructs a dedicated Generative Adversarial Network (GAN) and employs its \textit{generator}} to generate candidate manipulations under the guidance of its \textit{discriminator}. The generator is implemented by our proposed \textbf{A}dversarial multi-\textbf{p}opulation c\textbf{o}-\textbf{e}volution algorith\textbf{m} ({\ouralgorithm}). {\ourtool} iteratively queries its target detection system with manipulated samples, and gradually learns the desired manipulation from a sequence of query-reply pairs. {{\ourtool} uses the following techniques to overcome the above challenges.}

\noindent(1) {\ourtool} leverages a novel malware manipulation method "\textit{try-catch trap}" to insert never-executed function calls into malware code {for functionality preservation}.

\noindent(2) {{\ourtool} maps the FCG into a feature vector, which transfers the impacts of malware manipulation into feature space and hence bridges the problem-feature space gap.}

\noindent(3) To overcome the challenge of strict black box, the discriminator substitutes the target {classifier} and guides the generator to figure out the desirable manipulation rapidly.

\noindent(4) {In {\ouralgorithm}, every population corresponds to a possible feature granularity.} Owning to the cooperative evolution among populations, {\ouralgorithm} converges to the real feature granularity under incomplete feature information.


Our main contributions are summarized as follows.

\noindent$\bullet$ We propose a novel black-box AE attack {\ourtool} towards the {FCG} based Android malware detection. {{\ourtool} does not require  complete information about feature space, and hence it is a broad-spectrum attack with strong generalizability. }

\noindent$\bullet$  We  theoretically analyze why {\ouralgorithm} can mitigate the prematurity problem {that often plagues the evolution algorithms}. 

\noindent$\bullet$ {We conduct extensive experiments on three state-of-the-art (SOTA) malware detection methods MaMaDroid \cite{DBLP:conf/ndss/MaricontiOACRS17},  APIGraph    \cite{DBLP:conf/ccs/ZhangZZDCZZY20} and GCN\cite{DBLP:journals/corr/abs-2009-05602} with five classifiers (e.g., RF and DNN) under three feature granularities. {\ourtool} surpasses 
{the SOTA attack (i.e., reinforcement learning based method SRL) in our experiments.}} It achieves an average attack success rate of over
\textbf{99.9\%} on all \textbf{32} target detection systems. Our experiments also confirm the {\ourtool}'s attack efficiency and resilience to concept drift and data imbalance.
%the SOTA attack SRL in our experiments.}

% \noindent$\bullet$ Based on {\ourtool}, we develop a practical tool\footnote{We publish part of the code on https://github.com/USENIX497/USENIX-497} to {lay a try-catch trap} and produce a real evasive Android malware. This tool unpacks, manipulates and rebuilds APK files in an automated way, making it easy to use in practice.

{\textbf{Roadmap.} The remainder of the paper is organized as follows: §\ref{subsec: Preliminaries} introduces preliminaries; §\ref{subsec: Problem} presents the problem formulation; §\ref{sec:Adversarial manipulation design} discusses how to manipulate  the malware; §\ref{sec:method} describes the algorithm of perturbation generation; §\ref{SEC:EXP} gives the performance evaluation; §\ref{sec:Related} reviews relevant work; §\ref{sec:Limitations} provides {the limitations and discussion.}}
%the  potential limitations; §\ref{sec:Conclusion} concludes our work.}
