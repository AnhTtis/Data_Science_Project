
\section{Problem formulation} \label{subsec: Problem}
 {Here we first introduce the system and threats considered in our work, and then propose an attack formulation to guide the design of black-box AE attacks.}
\subsection{{System \& Threat}}

{Fig. \ref{fig:motivation} depicts the {FCG} based Android malware detection system considered in this work. Suppose an adversary launches a black-box AE attack towards this system to produce real evasive malware. To this end, the adversary first gets the classes.dex file from an APK file, and further decompiles it into a series of smali files, as shown in Fig. \ref{fig:manipulation overview}. The adversary manipulates the smali code according to its perturbation, and rebuilds the code to obtain a new APK file. The adversary then queries the detection system with the generated malware sample, utilizes the received binary decision (i.e., benign or malicious) to update its perturbation, and then rebuilds a new malware sample. The above procedure is repeated until a real evasive malware is obtained. }

The adversary only knows that the target system uses FCG feature for malware detection. However, the adversary does not know the feature granularity and the graph embedding method used by the target system. Moreover, the adversary has no information about the architecture, the parameters and the output probabilities of the target classifier. As for the defender, it can use static analysis and white list based defenses to resist evasive malware. In addition, the defender may raise alarms once the number of queries from a user is unusually large\footnote{ {
Our experiments indicate that our method only needs dozens of queries to generate the perturbations that can successfully attack the target model. Moreover, our method can further reduce the number of perturbations by conducting more queries (e.g., several hundreds of queries). To accelerate the attack process, we provide a substitute network to fit the target model. The related experiments can be found in  Section \ref{sec:RQ2}}}.

%如果还有camera ready的版本我建议直接删掉这句话。。。。因为查询次数确实不占优。特别是我们通过多粒度把问题复杂化了，需要的次数更多。

% To find adversarial perturbation, the adversary needs to query the target system with its currently generated malware sample, and utilize the reply (i.e., the binary decision on whether the received malware sample is malicious or not) to update its generated perturbation and then rebuild a new malware sample. The above procedure is repeated until a desired adversarial perturbation is found and a real evasive malware is obtained. 

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.8]{manipulation_overview.pdf}
	\caption{Overview of the AEs generation.}
	\label{fig:manipulation overview}
\end{figure}



\subsection{Attack formulation}
% Now we propose our attack formulation, and point out how to launch AE attacks inspired by this formulation. 
For convenience, {we first use $s$ and $m$ to refer to the malware sample and the manipulation}, respectively. We then use two functions $\mathcal M_G(\cdot)$ and $\mathcal M_V(\cdot)$ to denote the code-to-graph mapping and the graph-to-vector mapping shown in Fig. \ref{fig:motivation}, respectively. Through manipulating the malware sample $s$ with $m$, the adversary changes the input graph from $G=\mathcal M_G(s)$ to $\widetilde{G}=\mathcal M_G(s+m)$, where $G$ and $\widetilde{G}$ represent the original input {FCG} and the perturbed input {FCG}, respectively. Suppose $L(\cdot)$ denotes the label (i.e., benign or malicious) predicted by the target classifier. Then, the desired adversarial manipulation $m^*$ can be derived by solving the following problem:
\begin{equation}
L(\mathcal M_V(\mathcal M_G(s))) \neq L(\mathcal M_V(\mathcal M_G(s+m^*))) \label{objective: misleading}
\end{equation}
% s.t.,
% \begin{equation}
% 	F(s)=F(s+m^*) \label{constraint:preserving}
% \end{equation}
%  {Here $F(\cdot)$ represents the functionality set of malware, and the constraint \eqref{constraint:preserving} is introduced to guarantee the malware functionality is not changed by the manipulation. }
 {under the constraint of malware functionality preservation. }


{The above formulation points out two tasks for us: 1) designing a manipulation technique to modify malware code while preserving malware functionality, and 2) developing an adversarial perturbation generation algorithm to realize $m^*$. Due to the challenges of problem-feature space gap and strict black-box setting, $\mathcal M_G(\cdot)$ and $\mathcal M_V(\cdot)$ are actually unknown to the adversary. Hence it is extremely hard to derive the desired adversarial perturbation in one shot.} This motivates us to develop an evolutionary algorithm (i.e., {\ouralgorithm}) to gradually find the desired perturbation. {We will discuss how to fulfill  the above two tasks in Sections \ref{sec:Adversarial manipulation design} and \ref{sec:method}, respectively. } 

{Furthermore, it is noted that a variety of graph adversarial attack models \cite{DBLP:conf/ijcai/XuC0CWHL19,DBLP:conf/icml/BojchevskiG19,DBLP:conf/ijcai/Wu0TDLZ19,DBLP:conf/iclr/ZugnerG19,DBLP:conf/icml/DaiLTHWZS18,DBLP:conf/kdd/0001WDWT21,DBLP:conf/www/SunWTHH20} have been proposed in the community of machine learning. 
Although these methods offer inspirations to us, they cannot be directly applied to our attack for two reasons. First, graph adversarial attack models launch attacks from feature space. However, the attack against Android malware detection cannot directly access feature space, and has to indirectly affect feature space through manipulating malware code in problem space. Second, our attack needs to meet practical requirements (i.e., \textbf{R1}-\textbf{R4} discussed in Subsection \ref{subsec: four R}), which are absent in existing graph adversarial attacks. Therefore, specialized study is needed for malware adversarial attack design. } 




% (1) Adversarial manipulation needs to satisfy the realistic requirements for real-world application. For example, the adversary needs to appropriately choose the type of manipulation operations to preserve the functionality of the malware and to avoid the static analysis.  In Section \ref{sec:Adversarial manipulation design}, we will first discuss the requirements for adversarial manipulation, then analyze existing manipulation methods (e.g., adding or removing function calls) and introduce our method.

% (2) In our settings, $\mathcal M_G(\cdot)$ and $\mathcal M_V(\cdot)$ are unknown to the adversary. Hence it is almost impossible to derive the desired adversarial manipulation $m^*$ in one shot. A more promising way is to iteratively improve or update a manipulation $m$ until the objective (i.e., Eq. \ref{objective: misleading}) and the constraint (i.e., Eq. \ref{constraint:preserving}) are both satisfied. This motivates us to develop an evolutionary algorithm (i.e., {\ouralgorithm}) to find the desired perturbation.  

% In the following two sections, we first introduce our malware manipulation technique, and then propose our black-box attack method (including perturbation generation algorithm), respectively. 





% \subsection{Overview of {\ourtool}}
% The main task of designing {\ourtool} can be divided into two parts: 1) manipulation technique design, and 2) perturbation generation method. The manipulation technique is used in problem space, and the perturbation generation method is used in 

% Now in this section, we  summarizes our approach to generate  adversarial perturbations to the Android malware detection method on problem space. Our method can be divided into two parts, one is design an adversarial manipulation to modify the Android APK file, and the other one is to  study how to generate  appropriate perturbation for the adversarial manipulation.

% For adversarial manipulation,  it needs to  satisfy the some requirements for real-world application . For example, the adversary needs to appropriately choose the type of manipulation operations to preserve the functionality of the malware and to avoid the static analysis. In Section \ref{sec:Adversarial manipulation design}, we will first discuss the requirements for the adversarial manipulation. Then we analyse existing manipulation methods (e.g., adding or removing function calls) and introduce our method.


% For adversarial manipulation, the adversary need to solve the inequality \eqref{objective: misleading}. However, the code-to-graph mapping $\mathcal M_G(\cdot)$ and the graph-to-vector mapping $\mathcal M_V(\cdot)$ usually are unknown to the adversary. Hence it is almost impossible to obtain the desired adversarial manipulations in one shot. A promising alternative method is to iteratively update (or improve) the manipulations $m$ until the inequality and the constraint are both satisfied. In the above iterative operations, the adversary needs to evaluate the effects of current manipulations. To this end, the adversary sends the manipulated malware to the target system for malware detection, and uses the returned detection outcome (i.e., the predicted label) to help improve manipulations. A main challenge is that the detection outcome only tells whether the current manipulations induce misclassification or not, but it provides little information for manipulation improvement. To overcome this challenge, we will propose to simulate the target detection with a GCN in Section \ref{sec:method}. 