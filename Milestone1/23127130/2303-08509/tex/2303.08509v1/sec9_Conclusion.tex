	% \section{Conclusion}\label{sec:Conclusion}
% Under the realistic assumption on incomplete feature information, we design a novel black-box AE attack {\ourtool} against the graph based Android malware detection, called {\ourtool}. {\ourtool} uses a novel GAN model to automatically generate real evasive malware. Its generator is implemented with a multi-population co-evolution algorithm. Its discriminator acts as a substitute model and stimulates the generator to improve its generated malware. We provide theoretical analyses for the proposed algorithm. To evaluate {\ourtool}, we implement two SOTA graph based Android malware detection methods  MaMaDroid and APIGraph, use three feature granularities and five classifiers (e.g., RF and DNN), and build 30 different target detection systems. Extensive experiments on these target models confirm the advantages of {\ourtool} in attack effectiveness, attack efficiency and resilience to concept drift. 

% {In this paper, we propose a black-box AE attack {\ourtool} towards the FCG based Android malware detection. {\ourtool} works without knowing the feature granularity of its target detection system. {\ourtool} leverages a multi-population co-evolution algorithm to find the desired perturbation, and uses a new technique, i.e., try-catch trap, to manipulate malware. Extensive experiments confirm the effectiveness, efficiency and resilience of {\ourtool}, and demonstrate the advantage of {\ourtool} over the SOTA attack SRL. }

% {
% We hope that our work possesses reference value to the study of Android malware detection, and raises concern of malware detection research community for the threats posed by AE attacks. Moreover, our proposed method can be used to evaluate the robustness of  existing Android malware detection methods.
% }

%  {Recently, more and more works focus on the adversarial examples generation on graph data. Different from the image data consisting of continuous
% features, graph data  and the nodes features are often discrete. Therefore, gradient based approaches  for finding perturbations are not suited \cite{DBLP:conf/kdd/ZugnerAG18}. What's more,  it's easy to perturb image data by increasing or decreasing pixel values. However, it's difficult to perturb graph data( e.g.,  edge or node usually has  actual physical meaning and can't be modified  arbitrarily ). \cite{DBLP:conf/kdd/ZugnerAG18} propose the first work of adversarial attack on attributed graphs. By applying perturbations on  graph structure (add or subtract edges) or node attributes (add, subtract attributes), it can successfully attack node classification model.
% Develop up to now, adversarial example generation methods can be divided into two categories: gradient-based and non-gradient-based according to the  knowledge level  of the attacker. 
% The gradient-based method usually used in white-box attack, and adversarial examples generated   along the direction of gradient descent of loss function \cite{DBLP:conf/ijcai/XuC0CWHL19,DBLP:conf/icml/BojchevskiG19,DBLP:conf/ijcai/Wu0TDLZ19,DBLP:conf/iclr/ZugnerG19}. 
% The non-gradient-based attack usually applies to black box attack scenarios, and it attackers usually use heuristics method to solve the perturbation \cite{DBLP:conf/icml/DaiLTHWZS18,DBLP:conf/kdd/0001WDWT21,DBLP:conf/www/SunWTHH20} .} 

%  {
% Although these methods offer a lot  inspiration to adversarial examples generation in Android malware detection. But there's still a long way to go. 
% The main challenge of adversarial example attacks for Android malware detection stems from the fact that the problem space and the feature space of Android malware detection are different and separated. Attackers need to modify the normal samples (i.e., malicious apps) in problem space (instead of feature space) under some strict and practical constraints (e.g., R1-R4 proposed in our manuscript). The problem-feature separation and the consideration for practical constrains make our problem different from the problem of graph classification oriented adversarial example attacks. The existing adversarial example attacks developed for graph classification tasks cannot be directly transferred to Android malware detection, since they are launched in feature space and their perturbations may be impossible to realize in problem space.
% }


% \cite{DBLP:conf/icml/DaiLTHWZS18} proposes a reinforcement learning based attack method that learns the generalizable attack policy in  black-box attack (i.e., only requiring prediction labels from the target classifier.)  