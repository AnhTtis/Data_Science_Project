
\section{Experiments}\label{SEC:EXP}

In this section, we conduct extensive experiments to evaluate {\ourtool} by answering the following research questions:\\
\textbf{RQ1: Effectiveness.} { Does {\ourtool} successfully attack  the SOTA Android malware detection methods? }\\
%How effective is {\ourtool} in attacking the SOTA Android malware detection methods? \  
%How efficient is {\ourtool} in attacks?\\
\textbf{RQ2: Evolution.} How do multiple populations in {\ouralgorithm} evolve?\\
\textbf{RQ3: Efficiency.} { Does the substitute model help to decrease
queries and improve attack efficiency? }\\
\textbf{RQ4: Overhead.} Is there a trade-off between manipulation overhead and attack success rate?\\
\textbf{RQ5: Resilience.} {Is {\ourtool} still effective when there exists concept drift or data imbalance?}\\
\textbf{RQ6: Functionality.} {Does our adversarial perturbation change the functionality of malware?}

%\subsection{Setting}\label{SEC:Setting}
\textbf{Datasets}.
{ Our dataset contains 21399 benign samples and 22975 malicious samples, which come from Androzoo\footnote{https://androzoo.uni.lu/}, Faldroid dataset \cite{DBLP:journals/tifs/Fan0LCTZL18} and Drebin dataset \cite{DBLP:conf/ndss/ArpSHGR14}. Every sample collected from  Androzoo is detected by VirusTotal \cite{VirusTotal}. Only when a sample is detected to be malicious by more than four antivirus systems, we label it as malware. The details of our dataset  are provided in Appendix \ref{appendix: dataset}.} 

Furthermore, our experiments adopt two configurations to evaluate {\ourtool}. According to the first configuration,  we use 10-fold cross-validation to train the target models. To evaluate the attack methods, we randomly choose $100$ malicious examples (not included in the training data of target model) that can be correctly classified by target models for evasive malware generation. For the second, we divide the dataset according to the years that Android apps emerge as discussed in Section \ref{subsection: resilience}. The newly-emerged malware samples are used for test, while the old data are used in training.


% (1) \textit{Training target models}.   {The dataset is randomly divided into 10 fold, and 10-fold cross-validation is used to train and evaluate the existing FCG based Android malware detection methods. Every model's detection performance is shown in Appendix \ref{appendix:performance}.}

% % We randomly select 7000 benign examples and 7000 malicious examples to train the existing FCG based Android malware detection methods.

% (2) \textit{Generating adversarial examples}. In our experiments, we randomly choose $100$ malicious examples  {(not be selected as  training data by target model)} that can be correctly classified by the target model from our dataset, and use them for adversarial example generation.

 {Finally, we also consider the scenarios of concept drift and data imbalance in Subsection \ref{subsection: resilience}. In the scenario of concept drift, 17685 samples (8,017 benign examples and 9,668 malicious examples) from Androzoo are grouped by production year (from 2016 to 2020) and used to train target models. In the scenario of data imbalance, we randomly disarrange samples and set the benign-malicious ratio to 10:1, following the experimental setting in \cite{277204}.     }

%Our dataset contains 9,668 benign examples and 8,017 malicious examples. All the data come from Androzoo\footnote{https://androzoo.uni.lu/}, a growing collection of Android applications collected from several sources including the official Google Play app market \cite{Allix:2016:ACM:2901739.2903508}. At present, AndroZoo has been widely used in Android malware detection \cite{DBLP:conf/ccs/ZhangZZDCZZY20}\cite{DBLP:journals/tsmc/YuanJLC21} and Android adversarial malware generation \cite{DBLP:conf/www/LiZYLGC21}. The details of the dateset used in our experiments are provided in Appendix \ref{appendix: dataset}. In our experiments, the dataset is used to train the target model and to evaluate attack performance of {\ourtool}.

\begin{table*}[!ht]
	\caption{ {Effectiveness of {\ourtool} towards MaMaDroid, APIGraph and GCN.}}
	\centering
	\scalebox{1}{
		\begin{tabular}{c|c|ccc|ccc|ccc}
			\hline
			\multicolumn{2}{c|}{\multirow{2}{*}{Classifier\textbackslash{}Level}} & \multicolumn{3}{c|}{Family} & \multicolumn{3}{c|}{Package} & \multicolumn{3}{c}{Class} \\ \cline{3-11} 
			\multicolumn{2}{c|}{}                                                       & ASR     & APR     & IR      & ASR     & APR     & IR       & ASR    & APR    & IR      \\ \hline
			\multirow{5}{*}{MaMaDroid}                      
			& \cellcolor{cyan!60!gray!10}RF                        & \cellcolor{cyan!60!gray!10}1.000    & \cellcolor{cyan!60!gray!10}0.021    & \cellcolor{cyan!60!gray!10}8.670    & \cellcolor{cyan!60!gray!10}1.000    & \cellcolor{cyan!60!gray!10}0.049    & \cellcolor{cyan!60!gray!10}13.640    & \cellcolor{cyan!60!gray!10} 1.000  & \cellcolor{cyan!60!gray!10}	0.083  & \cellcolor{cyan!60!gray!10}	12.490    \\   

			& DNN                       & 0.990    & 0.149    & 11.130    & 1.000    & 0.134    & 16.730    & 1.000   & 	0.153  & 15.907    \\  	

			& \cellcolor{cyan!60!gray!10}AB                        & \cellcolor{cyan!60!gray!10}1.000   & \cellcolor{cyan!60!gray!10}0.066    & \cellcolor{cyan!60!gray!10}10.270    & \cellcolor{cyan!60!gray!10}1.000    & \cellcolor{cyan!60!gray!10}0.072    & \cellcolor{cyan!60!gray!10}14.300    & \cellcolor{cyan!60!gray!10}1.000  & \cellcolor{cyan!60!gray!10}0.118  & \cellcolor{cyan!60!gray!10}15.460   \\     		

			& 1-NN                      & 1.000    & 0.031    & 7.000    & 1.000    & 0.109    & 11.630     &1.000   & 0.060  & 10.960    \\  	 	

			& \cellcolor{cyan!60!gray!10}3-NN                      & \cellcolor{cyan!60!gray!10}1.000   & \cellcolor{cyan!60!gray!10}0.037    & \cellcolor{cyan!60!gray!10}9.390    & \cellcolor{cyan!60!gray!10}1.000    & \cellcolor{cyan!60!gray!10}0.142    & \cellcolor{cyan!60!gray!10}13.380     & \cellcolor{cyan!60!gray!10} 1.000  & \cellcolor{cyan!60!gray!10}	0.072  & \cellcolor{cyan!60!gray!10}10.770   \\ \hline  	

			\multirow{5}{*}{APIGraph}                       
			& RF                        & 1.000    & 0.039    & 11.260    & 1.000    & 0.098    & 14.930     & 1.000  &0.040  & 	9.530    \\  	 

			& \cellcolor{cyan!60!gray!10}DNN                       & \cellcolor{cyan!60!gray!10}1.000    & \cellcolor{cyan!60!gray!10}0.132    & \cellcolor{cyan!60!gray!10}14.370    & \cellcolor{cyan!60!gray!10}1.000    & \cellcolor{cyan!60!gray!10}0.096    & \cellcolor{cyan!60!gray!10}18.630   &  \cellcolor{cyan!60!gray!10}	1.000      &    \cellcolor{cyan!60!gray!10}0.168 
	     &  \cellcolor{cyan!60!gray!10}12.566 
      \\  	 

			& AB                        & 1.000    & 0.093    & 14.510    & 0.990    & 0.131    & 18.350     &  1.000  & 0.067   &	12.250   \\ 	 

			& \cellcolor{cyan!60!gray!10}1-NN                      & \cellcolor{cyan!60!gray!10}1.000   & \cellcolor{cyan!60!gray!10}0.058    & \cellcolor{cyan!60!gray!10}11.190    & \cellcolor{cyan!60!gray!10}1.000   & \cellcolor{cyan!60!gray!10}0.089    & \cellcolor{cyan!60!gray!10}14.040     & \cellcolor{cyan!60!gray!10}1.000  & \cellcolor{cyan!60!gray!10}0.012  & \cellcolor{cyan!60!gray!10}6.910    \\ 		

			& 3-NN                      & 1.000   & 0.085    & 11.570    & 1.000   & 0.105    & 13.770     & 1.000   & 	0.019 & 7.780   \\ \hline  	

			\multirow{1}{*}{GCN} & \cellcolor{cyan!60!gray!10}DNN                        & \cellcolor{cyan!60!gray!10}1.000   & \cellcolor{cyan!60!gray!10}0.205  & \cellcolor{cyan!60!gray!10}11.610   & \cellcolor{cyan!60!gray!10}1.000  & \cellcolor{cyan!60!gray!10}0.104   & \cellcolor{cyan!60!gray!10}17.320   & \cellcolor{cyan!60!gray!10}-  & \cellcolor{cyan!60!gray!10}-  & \cellcolor{cyan!60!gray!10}-   \\  \hline
			
		\end{tabular}%
	}
	\label{tab:effectiveness}
\end{table*}


\textbf{Target Model}.
We choose {three SOTA malware detection methods (i.e., MaMadroid \cite{DBLP:conf/ndss/MaricontiOACRS17}, APIGraph \cite{DBLP:conf/ccs/ZhangZZDCZZY20} and GCN \cite{DBLP:journals/corr/abs-2009-05602})} as our target system. 
% Note that they do not consider the function-level FCG in order to avoid overly high input dimensionality. Hence we use class-level, family-level and package-level FCGs as the features in our experiments, respectively.
% \textcolor{green}{Due to its extremely high dimensionality, the function-level FCG is not suitable for these methods \cite{}, and thus are not considered in our experiments.}
In MaMadroid and APIGraph, we employ Random Forest (RF) \cite{Breiman2001Random}, AdaBoost (AB) \cite{2002Logistic}, 1-Nearest Neighbor (1-NN) \cite{1952Discriminatory}, 3-Nearest Neighbor (3-NN) and Dense Neural Network (DNN) as the target classifier, respectively. Similar to  \cite{DBLP:journals/corr/abs-2009-05602}, we use a two-layer DNN as the target classifier in the GCN-based method.
% To ensure these target classifiers work well, we evaluate their detection performance and show the results in Appendix \ref{appendix:performance}.


\textbf{Metric}.
We  use attack success rate (\textbf{ASR}), average perturbation ratio (\textbf{APR}), and the number of interaction rounds (\textbf{IR}) to evaluate {\ourtool}.
{
ASR corresponds to the ratio of the number of successfully generated AEs (denoted by $N_{success}$) to the number of malicious examples used for AE generation (denoted by $N_{total}$ ), i.e., $ASR=N_{success} / N_{total}$.
% \begin{equation}
% 	ASR=N_{success} / N_{total}
% \end{equation}
APR is the ratio of the number of added edges (denoted by $E_{added}$) to the total number of edges (denoted by $E_{total}$), i.e., $APR = E_{added} / E_{total}$. 
% \begin{equation}
% 	APR = E_{added} / E_{total}
% \end{equation}
}
IR is defined as the number of interactions between our attack model and the target model.

\subsection{RQ1: Effectiveness}\label{sec:RQ1}

\textbf{Experimental Setup.} To verify the attack effectiveness of {\ourtool}, we use {\ourtool} to attack the {32} target models\footnote{{ Our experiments use 2 traditional 
FCG-based feature extraction methods (MaMaDroid and APIgraph), 3 feature levels (class, family and package) and 5 target classifiers (RF, AB, etc.). Furthermore, 1 GCN feature extraction method is considered with 2 feature levels (family and package). Hence, there are $32= 2\times 3\times5+1\times2 $ classifiers.}} mentioned above, and calculate ASR, APR and IR on every target model. 

% {For illustration, we supply an instance of AE generated by {\ourtool} }in Appendix \ref{appendix:instance}.

{Furthermore, we also compare {\ourtool} with three attack methods, i.e., SRL \cite{DBLP:journals/corr/abs-2009-05602}, SRL\_N and Random Insertion (RI). To our knowledge, SRL is the SOTA malware AE generation method \footnote{{Note that SRL works on control flow graph instead of FCG. To apply SRL to the FCG based Android malware detection, we design a non-functional API list (instead of non-functional instruction list), which contains 17 non-functional APIs.}}.  Since SRL requires knowing the class probabilities outputted by the target model, we modify its reward function and create a variant of SRL (i.e., SRL\_N) that only relies on binary outputs. The RI attack method is also introduced from \cite{DBLP:journals/corr/abs-2009-05602}, and it randomly inserts non-functional functions. }


% First, we consider a state-of-the-art black-box attack algorithm SRL \cite{DBLP:journals/corr/abs-2009-05602}. SRL is proposed to  generate adversarial example for control flow graph \footnote{ {To make this method work well in function call graph based Android malware detection system, we design a  non-functional API list instead of non-functional instruction list. This list contains 17 non-functional APIs.}} based Android malware detection method. It uses  deep reinforcement learning algorithm to choose nops instruction and insert them into the APK file. 
% However, SRL needs the output probability of the sample to calculate the reward in it's algorithm, so we modify it's reward function\footnote{ {Only when an adversarial example  attacks successfully, the algorithm give a positive reward.}} and introduce a new method called SRL\_N which only rely on the binary output result to get reward. 
% Moreover, we design a Random Insertion(RI) attack which randomly insert the non-functional function mentioned in SRL.}
% When we begin the attack, we use $N$ malicious Android APKs as the seed to generate adversarial examples. Then $M$ adversarial examples are classified as benign by the target classifeir. The attack success rate can be calculate as $ASR = M/N$. ASR shows the attack effectiveness.
% The APR is the ratio of added edges to the total edges. We discover that with the growth of an APK's API calls, it needs more perturbation  to generate an adversarial example. So APR can better describe the perturbation generated by an attack than average perturbation (\textbf{AP}). 
% IR is defined as the interaction rounds between the {\ourtool} and black-box classifier. 




\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{sota.pdf}
	\caption{{Comparison with SOTA methods.}}
	\label{fig:SOTA}
\end{figure}



\textbf{Results \& Analyses}.
{Table \ref{tab:effectiveness} reflects the attack performance of {\ourtool} on MaMaDroid, APIGraph and GCN under various feature granularities.}
{First, {\ourtool} achieves an average ASR of 99.9\% over 32 target models, hence confirming the effectiveness of {\ourtool}.} { Second, when attacking the family-granularity classifier, {\ourtool} achieves the lowest APR and IR (i.e., 0.071 and 10.936).} {This indicates that although the family-granularity FCG speeds up malware detection through reducing input complexity, it still improves the efficiency of {\ourtool} by reducing search space.} 

% First, {\ourtool} achieves an average ASR of 97.6\% over all 30 target models, hence confirming the effectiveness of {\ourtool}. Second, the DNN based target models exhibit stronger robustness against our attacks (average ASR is 92.0\%) than the other target models (average ASR is 99.0\%).
% It is because deep learning classifiers generally have better generalization capability and possess better decision boundaries. Therefore, it is more difficult to generate AEs to mislead deep learning classifiers. 


Fig. \ref{fig:SOTA} compares {\ourtool}, SRL, SRL\_N and RI with respect to ASR under various APRs. Not surprisingly, RI performs worst in our experiments due to its poor search strategy. SRL performs better than SRL\_N, because SRL has access to class probabilities, which is more valuable than binary decisions. It is worth noting that {\ourtool} still outperforms SRL (e.g., its ASR is 4\% higher when APR is 0.2), although {\ourtool} cannot utilize class probabilities. 
% The above results confirm the advantage of {\ourtool} in attack effectiveness. 
 {The above results confirm that, under a certain number of perturbations, our method generates a better combination of the added edges that are more deceptive to detectors, as compared to the other methods.}
%这里是审稿人5的Q5，同理，我不也不太想放实验，而且，我感觉这个better combination of the added edges不好理解,但是不提这个关键词，又怕那个审稿人不高兴。老师看看要不要换个词，比如说 the edges selected for insertion are more aggressive to the target model.本质上就是想说我们的方法效果更好，但是绕来绕去变复杂了。  

% under different APR of {\ourtool} and several SOTA methods. It can be observed that, although SRL and our method can achieve similar ASR (e.g., with 0.2 APR, our method achieves 0.81 average attack success rate, whihe SRL achieves 0.77), SRL requires the output probability to train it's model. If without this information, SRL\_N only  achieves 0.50  average attack success rate with 0.2 APR. This phenomenon proves that  our model can still train well and generate threatening adversarial examples even without the information of output probability.

% Moreover, it is usually more costly to generate adversarial examples for DNN based target models, since these target models often result in higher APR and IR than the others.  

\subsection{RQ2: Evolution }
\textbf{Experimental Setup.}
In this subsection, we use experiments to analyze the effects of multi-population co-evolution mechanism.
First, we want to show that this mechanism can overcome the challenge of unknown feature granularity. To this end, we compare our method with the single-population methods in attacking MaMaDroid. The single-population methods rely on one single population corresponding to class, package and family  level, denoted by {\ourtool}-C, {\ourtool}-P  and {\ourtool}-F, respectively. {We also randomly select a malware sample and take a close look at these methods' attack processes.}


 Second, we want to know whether the correct feature granularity is found by our method.  We then record the survival number of each population and analyze how these populations evolve. { In this experiment, we choose MaMaDroid with an RF classifier as our target model, and use family-level feature granularity in malware detection. } 
  


% In this subsection, we use experiments to analyze the effects of multi-population co-evolution mechanism. First, we want to show that multi-population co-evolution can overcome the challenge of unknown feature granularity. To this end, we use our method and the single-population methods to attack MaMaDroid. The single-population methods rely on one single population corresponding to class, package and family  level, denoted by {\ourtool}-C, {\ourtool}-P  and {\ourtool}-F, respectively. Second, we want to know whether the correct feature granularity is found by our method. 
%   {For this purpose, we record the average ASE of aboves methods. What's more, to have a  detail look of the process of above methods,  }
%   we randomly select a malware sample and use {\ourtool},  {\ourtool}-C, {\ourtool}-P  and {\ourtool}-F to manipulate it. We then record the survival number of each population and analyze how these populations evolve.In this experiment, we choose MaMaDroid with RF classifier as our target model. Furthermore, target models use package-level features for malware detection in all the experiments of this subsection. 

\begin{figure}[h]

	\centering
	\includegraphics[scale=0.55]{multi-alldata.pdf}
	\caption{ {Performance comparison with single-population.}}
	\label{fig:multi}
\end{figure}


\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.55]{mult-P4.pdf}
	\caption{{Multi-population vs. single-population. }}
	\label{fig:exp_3}
\end{figure*}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{living_number_change.pdf}
	\caption{{The changing trend of survival proportion.}}
	\label{fig:living_number_change}
\end{figure}










% \begin{table}[]
% 	\caption{  {Attack performace of Multi-population vs. single-population.}}
% 	\centering
% 	\scalebox{1}{
% \begin{tabular}{c|ccc}
% \hline
%  & ASR & APR & IR \\ \hline
% \cellcolor{cyan!60!gray!10}BagAmmo-C & \cellcolor{cyan!60!gray!10}0.770 & \cellcolor{cyan!60!gray!10}0.190 & \cellcolor{cyan!60!gray!10}13.232 \\
% BagAmmo-P & 0.800 & 0.206 & 12.361 \\
% \cellcolor{cyan!60!gray!10}BagAmmo-F & \cellcolor{cyan!60!gray!10}0.970 & \cellcolor{cyan!60!gray!10}0.155 & \cellcolor{cyan!60!gray!10}\textbf{9.082} \\ \hline
% BagAmmo & \textbf{0.990} & \textbf{0.149} & 11.130 \\ \hline
% \end{tabular}%
% }
% 	\label{tab:multi}
% \end{table}

\textbf{Results \& Analyses.} 
{ For comparison, we choose family-level feature granularity and evaluate {\ourtool} and single-population methods on \textbf{all}  test samples. The results are shown in Fig. \ref{fig:multi}. It can be seen that {\ourtool} performs best and achieves the highest ASR with the lowest APR. {\ourtool}-C and {\ourtool}-P perform worst since they use a false feature granularity. Surprisingly, {\ourtool} performs better than {\ourtool}-F (i.e., 2\% higher in ASR and 0.06 lower in APR). This is because the introduction of multiple populations helps to avoid premature convergence and approach a global optimum. However, it may result in more interactions with the target model. This accounts for why {\ourtool} has a higher IR than {\ourtool}-F.} 

% this also accounts for why {\ourtool} has the highest ASR with lowest APR. This phenomenon will be discussed in next experiment too. Besides, if an attacker choose a false granularity(i.e., {\ourtool}-P and  {\ourtool}-C in  Fig.  \ref{fig:multi}) to attack the target model, it will lose a lot attack success rate(e.g., 22\% in {\ourtool}-C and 19\% in {\ourtool}-P) and  increase a lot perturbation ratio(e.g., 0.41 in {\ourtool}-C and 0.57 in {\ourtool}-P). }

{Now we randomly select a malware sample, and use it to generate an AE to attack 5 classifiers under family-level feature granularity. The attack processes of all methods are depicted in Fig. \ref{fig:exp_3}.} In this figure, the vertical axis represents the perturbation ratio of all methods, and the horizontal axis shows the IR values. {If a curve exhibits an evident decreasing trend and falls below a low threshold, we can conclude that the corresponding method succeeds in generating an AE and defeating the target model. As for those curves keeping horizontal (e.g., the green curve in the first subfigure), the corresponding methods fail to generate AEs. }
% Therefor, we can evaluate the effect of multi-population based on the decease trend of perturbation ratio.
% Moreover, an anomalous decease trend often represents attack failure. For example, as shown in  Fig.\ref{fig:exp_3}-(1), the perturbation of {\ourtool}-P always maintains a maximum value. Our experimental results verify that the corresponding attack is unsuccessful.  
Fig.\ref{fig:exp_3} shows that the perturbation ratio of multi-population always has a satisfactory decreasing trend, hence confirming the effects of multi-population co-evolution.
Furthermore, using a single population may cause {premature convergence to a local optimum}, as indicated by Fig. \ref{fig:exp_3}-(1). However, {\ourtool} effectively mitigates this problem using  multiple populations. { Theoretical analyses are given in Appendix \ref{appendix: theoretical analyses}.}

Finally, we verify the multi-population  co-evolution method converges to the real feature granularity from a different perspective. We show the survival proportion (i.e., the ratio between the number of alive individuals and the total number of individuals) of different populations in Fig. \ref{fig:living_number_change}. At the beginning, the perturbations are randomly added, and the survival proportion of different populations are irregular. However, as the number of queries increases, the family population and class population gradually fall to a low level. Contrarily, the survival proportion of the population corresponding to the correct feature granularity (i.e., {family} level) gradually rises to a high level. This phenomenon also confirms the effects of multi-population co-evolution.


\subsection{RQ3: Efficiency}\label{sec:RQ2}
\textbf{Experimental Setup}. 
We conduct \textit{ablation studies} to verify the effects of the substitute model in decreasing queries and improving attack efficiency. For comparison, we remove the substitute model and guide the multi-population co-evolution algorithm only using the target model. This method is called {\ourtool}-Without-S. Then we use {\ourtool} and {\ourtool}-Without-S to manipulate the same APK file, {and compare their performance.} 
%Due to space limit, we only show the  experiments on MaMaDroid with family-level and package-level features.

% To be specific, we design three attack strategies by reserving a single population in {\ourtool} (i.e., family, package and class population), which are termed as {\ourtool}-F, {\ourtool}-P and {\ourtool}-C, respectively. We conduct experiments on the MaMaDroid  with  package feature granularity. 
\begin{figure*}[h]
	\centering
	\includegraphics[scale=0.6]{with_and_without2.pdf}
	\caption{{With substitute model vs. without substitute model.}}
	\label{fig:with_and_without}
\end{figure*}



% we modify our algorithm in single-population setting, and through comparing the rate of convergence  of these  algorithms with the multi-population algorithm to analyze the multi-population's influence on efficiency. To be specific, we only reserve a single population in {\ourtool} (i.e., family, package and class population), which are termed as {\ourtool}-F, {\ourtool}-P and {\ourtool}-C, respectively. We conduct experiments on the MaMaDroid  with  package feature granularity. 



\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{withvswithout.pdf}
	\caption{{Can substitute model reduce queries?}}
	\label{fig:exp_2_alldata}
\end{figure}




% As show in Fig. \ref{fig:exp_3}, we conduct experiments among  MaMaDroid with 5 classifier on package granularity API call graph. In each subfigure, we compare {\ourtool} with the 3 single-populations algorithm (i.e., {\ourtool}-F, {\ourtool}-P and {\ourtool}-C), the horizontal axis represents the interactive rounds, and the vertical axis is the perturbations on an APK file. Through the perturbations' decrease speed, we can evaluate the effect of the muti-population. From Fig.\ref{fig:exp_3}, it can be observed that no matter  in which case (i.e, the type of the classifier), muti-population can gain the fastest  perturbations' decrease speed. What's more, the wrong granularity population may  cause the  slow convergence speed of the algorithm and even cause failure of the attack. For example, in some cases (e.g., {\ourtool}-F in RF classifier), the perturbation maintain a   maximum value, which means the attack is unsuccessful.  Finally, as the Fig. \ref{fig:exp_3}.(3) shows that, single population may converges prematurely to local optimum which is a common  phenomenon in genetic algorithm. However, {\ourtool} effectively relieve this  phenomenon through multi-population. We supply the theoretical proof in Sec????.

\textbf{Results \& Analyses}.
Substitute model's effects are shown in  Fig. \ref{fig:with_and_without}, where the solid and the dotted lines represent {\ourtool} and {\ourtool}-Without-S, respectively. {The vertical axis reflects perturbation ratio, and the horizontal axis indicates the number of queries}. It can be seen that in all cases, {\ourtool} always has a higher convergence speed. Moreover, {\ourtool} always requires fewer queries before the perturbation ratio is kept below a certain threshold (e.g., 0.1). 
% For example, to keep perturbation ratio below $0.1$, the amount of queries required by {\ourtool} is about $79$\% of that required by {\ourtool}-Without-S.
Note that the difference between two methods in the initial phase is relatively small. It is because the substitute model has not been well trained in this phase. However, after the substitute model is well trained with sufficient data\footnote{ {In general,  the training accuracy of the substitute model arises as
the number of iteration rounds increase. However, the increasing trend of training accuracy is not strictly monotonic, because the training data used in the iterations are different. }}, {\ourtool} performs more efficiently and exhibits its advantage.
%%%%%%%老师帮忙看看这里，我感觉审稿人5的意见4和5不太好放到论文里，感觉比较占空间，而且加上去也没有意义，虽然说放附录里也可以，但是附录已经很多内容了，我不知道是否应该加进去。所以审稿人5的Q4这里提了一下,而且这里我不知道其他人看不看得明白。。。。或者不要however后面的那一句话。



{Fig. \ref{fig:exp_2_alldata} compares {\ourtool } and {\ourtool }-Without-S in terms of IR. Its top-half part gives the results on the family-level FCG based MaMadroid, while the bottom-half part shows the results on the package-level FCG based MaMadroid. The horizontal axis indicates various classifiers (e.g., AB, RF and 1-NN). We can draw two conclusions from this figure. First, the package-level classifier is more difficult to attack. This is because package-level FCGs contain much more nodes than family-level FCGs, resulting in a larger search space for {\ourtool }. Second, using the substitute model reduces the number of queries in almost all cases and helps enhance  the attack efficiency.}



\subsection{RQ4:  {Manipulation Overhead}}
\textbf{Experimental Setup.}
Here we study the number of code modifications (i.e., manipulation overhead) required to generate a real evasive malware. We use experimental results to reflect the relationship between ASR and {the allowed perturbation ratio}. 
%More specifically, we use the cumulative distribution function (CDF)  to  compare the attack performance on different classifier. 
% \st{What's more, we compare the attack success rate on APIGraph and MaMaDroid under the same amount of code modifications. }


\textbf{Result \& Analysis.}
Experimental results are shown in Fig. \ref{fig:perturbation}. In this figure, the horizontal axis represents the allowed perturbation ratio, and the vertical axis gives the cumulative distribution function (CDF) of ASR. It can be observed that the ASR keeps rising with the increase of the allowed perturbation ratio. In practice, a larger perturbation ratio results in larger computational overhead for adversaries. Therefore, there exists a trade-off between manipulation overhead and attack success ratio. Moreover, 
% {Fig. \ref{fig:perturbation} reveals that the deep learning method has stronger robustness} against AE attacks than the traditional machine learning method. Finally,
 the 3-NN classifier is more robust than the 1-NN classifier. {It is because} the 3-NN classifier considers more data than 1-NN classifier when classifying a sample, which makes it distinguish benign and malicious apps easier. 

% \st{Now we turn to Fig. , which provides the comparison results between APIGraph and MaMaDroid. It can be observed that APIGraph is more easily to be beaten than MaMaDroid under the same condition. Our explanation is given below. APIGraph is an excellent method that can improve the classification performance of Android malware detection system, especially when concept drift occurs. APIGraph groups similar APIs into one cluster, hence reducing the number of nodes in FCG. As a result, the difficulty of attacking the classifiers enhanced by APIGraph is also decreased. This accounts for why APIGraph is more susceptible to our AE attacks. And this indicates there exists an interesting trade-off between accuracy and robustness in APIGraph.}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{perturbation.pdf}
	\caption{{CDF of ASR under different perturbation ratios.}}
	\label{fig:perturbation}
\end{figure}

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[scale=0.6]{APIGraph-mama.pdf}
% 	\caption{APIGraph vs. MaMaDroid.}
% 	\label{fig:APIGraph-mama}
% \end{figure}

\subsection{RQ5: Resilience}
\label{subsection: resilience}
\textbf{Experimental Setup.}
Concept drift \cite{263854} is often observed in the realistic applications of Android malware detection. 
% When a classifier trained with old training samples is used to classify newly-emerged samples, concept drift may take place and detection performance may heavily degrade. 
 {Concept drift undermines existing AE generation methods that insert APIs selected from a pre-determined white list, if the white list is not updated accordingly. Hence we want to know whether {\ourtool} is also susceptible to concept drift}. To this end, we use newly-emerged malware samples to generate AEs and attack the classifiers trained over old data. We divide the dataset into training sets and testing sets according to the years that Android app emerge. We construct four new datasets to evaluate {\ourtool} under concept drift, as depicted in Table \ref{tab:setting}. {The first row of Table \ref{tab:setting} is the year of training samples used for training target classifiers. The second row is the year of testing samples used for generating AEs. The third row is the accuracy of the classifier.}

{Data imbalance is another practical problem worthy of consideration\cite{277204,DBLP:conf/uss/PendleburyPJKC19}. Since malicious samples are more difficult to collect than benign samples, malware detection models are usually trained over imbalanced data. We want to know whether data imbalance negatively impacts the attack performance of {\ourtool}. Hence we evaluate {\ourtool} on the target model trained with imbalanced data (the benign-malicious ratio is 10:1). }

\begin{table}
	\caption{{The attack performance under concept drift.}}
	\renewcommand\arraystretch{1.5}
	\centering
	\scalebox{0.7}{
		\begin{tabular}{ccccc}
			\hline		
			Training set (year) & \cellcolor{cyan!60!gray!10}2016    & \cellcolor{cyan!60!gray!10}2016-2017 & \cellcolor{cyan!60!gray!10}2016-2018 & \cellcolor{cyan!60!gray!10}2016-2019 \\ 
			Testing set (year)  & 2017    & 2018      & 2019      & 2020      \\ 
			ACC          & 92.92\% & 94.08\% & 94.58\% & 95.47\% \\  \hline
			ASR          & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%  \\ \hline
		\end{tabular}%
		\label{tab:setting}
	}
\end{table}






\begin{table}[]
	\caption{{The APR on balanced and imbalanced data.}}
	\renewcommand\arraystretch{1.5}
	\centering
	\scalebox{0.7}{
\begin{tabular}{ccccccc}
\hline
Level & Case & AB & RF & 1NN & 3NN & Average \\ \hline
\multirow{2}{*}{Family} & \cellcolor{cyan!60!gray!10}Balance & \cellcolor{cyan!60!gray!10}0.066 & \cellcolor{cyan!60!gray!10}0.021 & \cellcolor{cyan!60!gray!10}0.031 & \cellcolor{cyan!60!gray!10}0.037 & \cellcolor{cyan!60!gray!10}0.039 \\
 & Imbalance & 0.050 & 0.024 & 0.021 & 0.021 & 0.029 \\ \hline
\multirow{2}{*}{Package} & \cellcolor{cyan!60!gray!10}Balance & \cellcolor{cyan!60!gray!10}0.072 & \cellcolor{cyan!60!gray!10}0.049 & \cellcolor{cyan!60!gray!10}0.109 & \cellcolor{cyan!60!gray!10}0.142 & \cellcolor{cyan!60!gray!10}0.093 \\
 & Imbalance & 0.041 & 0.040 & 0.094 & 0.105 & 0.070 \\ \hline
\end{tabular}%
}
\label{fig:imbalance}
\end{table}





% \begin{table}[H] 
% 	\caption{The partition of dataset.}
% 	\renewcommand\arraystretch{1.5}
% 	\centering
% 	\scalebox{0.7}{
% 		\begin{tabular}{ccccc}
% 			\hline		
% 			Training set (year) & \cellcolor{cyan!60!gray!10}2016    & \cellcolor{cyan!60!gray!10}2016-2017 & \cellcolor{cyan!60!gray!10}2016-2018 & \cellcolor{cyan!60!gray!10}2016-2019 \\ 
% 			Testing set (year)  & 2017    & 2018      & 2019      & 2020      \\ 
% 			Setting name       & \cellcolor{cyan!60!gray!10}DSY2017 & \cellcolor{cyan!60!gray!10}DSY2018   & \cellcolor{cyan!60!gray!10}DSY2019   & \cellcolor{cyan!60!gray!10}DSY2020   \\
% 			ACC          & 92.92\% & 94.08\% & 94.58\% & 95.47\% \\  \hline
% 			ASR          & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%  \\ \hline
% 		\end{tabular}%
% 		\label{tab:setting}
% 	}
% \end{table}



\textbf{Result \& Analysis.}
%We use the old dataset for model training, and use the samples in the next year for model testing. 
{In the experiments on concept drift, we use {\ourtool} to manipulate the test samples, and the results are used to attack the MaMaDroid model with the family-level feature and the classifier of RF}. The ASR of {\ourtool} in every scenario is presented in the last row of Table \ref{tab:setting}. First, this table indicates that with more training samples, the accuracy of the target classifier becomes higher. No matter how high the accuracy is, however, {\ourtool} always achieves a perfect ASR of 100\%. %This phenomenon tells us that {\ourtool} performs well under concept drift. 
 {This shows that {\ourtool} performs well under concept drift and is still efficient when the malware detection models learn more with the new data.}
Note that {\ourtool} uses the functions coming from the malware itself (instead of a static function set).  {\ourtool} reduces the risk of using functions that become outdated due to concept drift. As a result, {\ourtool} poses a persistent threat to malware detectors. 
%这里要提对抗样本吗
     {Finally, we also discuss how {\ourtool} performs when the defender has the knowledge of adversarial example in Appendix \ref{app:ar}}


{Table \ref{fig:imbalance} shows the experimental results of {\ourtool} in the cases of balanced and imbalanced data. Our experiments demonstrate that the DNN model performs very poorly when trained with imbalanced data. Therefore, we do not choose DNN as our target model. 
%In our experiments, {\ourtool} achieves an attack success rate of 100\% in almost all cases. Hence we only show APR in Table \ref{fig:imbalance}.
 {In both cases (i.e., balanced dataset and imbalance dataset), {\ourtool} achieves an attack success rate (i.e., ASR) of 100\%. Here we only show the values of APR in Table \ref{fig:imbalance}.}
A higher APR means a more difficult attack task. It can be seen that in the vast majority of cases, {\ourtool} needs fewer perturbations (i.e., has a lower APR) to attack the target model trained with imbalanced data. That is, data imbalance does not bring troubles to {\ourtool}. This is because that training with imbalanced data makes the target model more likely to classify malware as benign apps. Accordingly, this reduces the degree of difficulty in generating AEs.
% modifying malware to mislead the classification model.


% In family granularity, balance scene less need  0.01 APR than imbalance scene, and  in package granularity, balance scene less need  0.017 APR than imbalance scene. The explanation is follow: 
% With an imbalanced dataset (i.e., malware is much less than benign sample), a classification model will be more likely tends to classify malware as benign apps. And this reduces the degree of difficulty in generating adversarial examples through
% modifying malware to mislead the classification model.

% \begin{table}[]
% 	\renewcommand\arraystretch{1.5}
% 	\caption{The partition of dataset.}
% 	\centering
% 	\label{tab:concept drift}
% 	\scalebox{1}{
% 		\begin{tabular}{ccccc}
% 			\hline			
% 			Setting name & \cellcolor{cyan!60!gray!10}DSY2017 & \cellcolor{cyan!60!gray!10}DSY2018 & \cellcolor{cyan!60!gray!10}DSY2019 & \cellcolor{cyan!60!gray!10}DSY2020 \\
% 			ACC          & 92.92\% & 94.08\% & 94.58\% & 95.47\% \\
% 			ASR          & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%   & \cellcolor{cyan!60!gray!10}100\%  \\
% 			\hline		
% 		\end{tabular}%
% 	}
% \end{table}


\subsection{{RQ6: Functionality} }
{\textbf{Experimental Setup.}
In this section, we first use static analysis to verify whether the perturbations generated by {\ourtool} are successfully imposed on malware. We then employ dynamic analysis to check whether the perturbation changes the functionality of the malware.}

{\textbf{Result \& Analysis.}
To know whether our perturbations are injected, we add a unique log statement when a perturbation (i.e., a try-catch trap) is injected. This log statement helps us to find the perturbation in the smali file. We then check if the found function calls in the smali file coincide with the perturbations generated by {\ourtool}. In our experiments, we evaluate 50 APK files, and we realize that all the generated perturbations are correctly injected into the smali file.
}

{ In our experiments of dynamic analysis, we first install and run 50 pairs of original and perturbed malware samples in Android Virtual Device (AVD). It is observed that every malware pair performs the same and has the same run-time UI. For further analysis, we insert three log statements, denoted by LOG1, LOG2 and LOG3,  into every try-catch block to record execution information. LOG1 is in front of the runtime exception, LOG2 is in front of the inserted function, and LOG3 is at the beginning of the catch block. We analyze 50 APK files aided by  Android Studio's log analysis tool (i.e., LogCat). We realize that either LOG1 or LOG3 of every APK file is normally executed, but no LOG2 is executed. This phenomenon means that all manipulated malware samples run properly, and the inserted functions are not invoked, hence posing no impact on the malware functionality.
}

%  {
% What's more, we use dynamic analysis to show that the functionality of a malware is not be influenced. First, we install and run the pairs of original and perturbed malware samples in
% Android Virtual Device (AVD). We  evaluate 50 pairs of APPs and every malware pair performs same and has the same run-time UI. Second, we  insert  log statement  into every try block to record execution information and use it to further confirm that the perturbed malware works the same as
% the original one. To be specific, we insert three log statements for every try-catch block, \textbf{LOG1} is before the runtime exception, \textbf{LOG2} is behind the  runtime exception and \textbf{LOG3} is in the beginning of the catch block. With the help of the  android Studio's log analysis tool to track the performance of every log statements.  We aslo evaluate 50 APPs, and all the LOG1 and LOG3  are normally  executed, and LOG2 never be executed. It means that all apK runs properly,and the inserted callee functions are not executed and do not affect the functionality of the original APK.}




