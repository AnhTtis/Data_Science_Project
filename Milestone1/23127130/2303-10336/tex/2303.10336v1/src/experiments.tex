
\section{Experiments}
\label{sec:experiments}
In order to train the neural network models, we rely on user data, collected using the circuit described in Section~\ref{sec:measurement_circuit}. We used a touchpad sensor like the one shown in Figure~\ref{fig:capturing_gesture}, connected to a Keysight 66322A Series Waveform Generator and a Keysight InfiniiVision MSO-X 3024T mixed signal Oscilloscope, for signal generation and acquisition respectively. A group of 8 total college-age subjects participated in data collection. All of the participants were in good health. For a single trial, participants drew one of the 12 gestures included in the study onto the touchpad sensor, as the oscilloscope was capturing the resulting waveform. The gesture pathways captured are illustrated in Figure~\ref{fig:gesture_vis}. For each gesture, a baseline reading was also captured without any input to the touchpad, to be further used for data processing as described in Section~\ref{sec:wavelet_filtering}. A few of the subjects completed trials over different sittings. In those cases, a baseline measurement was taken for each sitting to account for outside conditions. 

Training was accomplished using leave-one-out cross validation for each of 5 subjects. Additionally, we evaluated the performance of the trained models, by testing their accuracies against data from 3 new subjects, to further investigate the robustness and reliability of the results. Each participant performed at least 20 trials for each of the 12 gestures included in the experiment, with an average of $45$ samples. A total of $2700$ samples were collected for cross-validation, with an average of $225$ samples per class. The evaluation set was composed of a total of $720$ samples, or $60$ per class, with each subject performing $20$ trials per gesture type. We investigate whether our gesture recognition model is capable of accurately distinguishing among 12 different language character gestures. The collected and processed data is used as an input into the CNN-LSTM gesture recognition network, which outputs one of the 12 possible gestures. Accuracy, precision, recall, and F1-score are used to determine its performance. More details about the process of training and evaluation are provided in the section below.

\newsavebox{\gesturebox}
\begin{figure*}[h]
    \centering
    \subfigure[]{\raisebox{3mm}
    {\includegraphics[width=0.22\textwidth]{src/figures/gesture_pad_connected-min.pdf} \label{fig:capturing_gesture}}}
    \hfill
    \subfigure[]
    {\includegraphics[width=0.7\textwidth]{src/figures/Gesture_Pathways_Dark_Background.pdf} \label{fig:gesture_vis}}
    \caption[Gesture pathways]{Pathways of the collected gestures. \emph{(a):} The knitted component of the sensor on which the gestures were performed; \emph{(b):} All the gesture pathways that were collected and analysed.}
    \label{fig:Gesture_Pathways}
    \Description{This figure provides information regarding the data collection process. In (a), there is a picture of the knitted sensor with four connected electrodes. Figure (b) shows the pathways for all the 12 types of gestures performed. They are the English language characters: ‘3’, ‘5’, ‘I’, ‘J’, ‘L’, ‘M’, ‘O’, ‘S’, ‘V’, ‘W’, ‘Z’, ‘?’. Each sketch shows the shape of the gesture with an arrow pointing outward at the end of the gesture to clarify the direction of the performance.}
\end{figure*}
