\section{System Design}
\label{sec:system_design}
In this section, we describe our gesture recognition system, composed of a few main components: the knitted fabric containing conductive and non-conductive yarn, the measurement hardware and circuit, the algorithmic components which produce a trained machine learning model, and ultimately the NVIDIA\textregistered~Jetson Xavier\TM~NX, a powerful embedded system-on-module (SoM) on which the model is deployed. Currently, signal generation, acquisition, and processing occur offline through the use of an arbitrary waveform generator and digital oscilloscope. After the data from multiple users is collected, we train a classification model to distinguish between 12 different gestures. Subsequently, we deploy the model on the embedded CPU and test its ability to recognize a gesture event in real-time. In the future, we plan to perform signal generation, acquisition, and pre-processing on a standalone embedded micro-processor capable of communicating with the embedded CPU, which would allow this system to be fully portable, while providing real-time interaction. 

The selected gestures are the alphanumeric characters: \emph{'3'}, \emph{'5'}, \emph{'I'}, \emph{'J'}, \emph{'L'}, \emph{'M'}, \emph{'O'}, \emph{'S'}, \emph{'V'}, \emph{'W'}, \emph{'Z'}, with the addition of character \emph{'?'}. All of these gestures can be performed in one motion and have a distinct onset and offset. Some are more basic, such as \emph{'I'}, others more complex, such as \emph{'?'}, and some gestures are more similar to each other, such as \emph{'5'} and \emph{'S'}, which could cause incorrect classification. This character subset contains the basic motions upon which other alphanumeric characters can be built. Our choice of several alphanumeric characters serves to strike a balance between prototyping this system, and the complexity it has the potential to offer. Instead of testing its recognition ability with more basic gestures, such as taps and directional swipes, we opted for a more comprehensive set, including gestures with curvatures, to explore the limits of its capabilities. On the other hand, including the whole alphanumeric character set of the English language would increase its complexity even more, requiring more training data. It is worth noting that the focus of this work is not the specific application, but the gesture-enabling technology illustrated through some examples. Ultimately, we would like to build a recognition model capable of detecting numbers and the letters of the alphabet, which could serve as a communication system. Our system is a first step in that direction, but more generally, it unlocks the potential of gesture recognition on knitted sensors with many possible applications. Below we describe its components in more detail.

\begin{figure*}[ht]
    \centering
    \hfill
    \subfigure[]
    {\includegraphics[width=0.4\textwidth]{src/figures/Single-wire_Touchpad_Diagram.pdf}\label{fig:Single-wire_Touchpad_Diagram}}
    \hfill
    \subfigure[]
    {\includegraphics[width=0.4\textwidth]{src/figures/Planar_Touchpad_Diagram.pdf}\label{fig:Planar_Touchpad_Diagram}}
    \hfill
    \caption{Diagrams of the knitted touchpad designs. \emph{(a)}: A single-wire touchpad created using conductive and non-conductive yarns that serpentine across the textile surface. The points \emph{A} and \emph{B} connect to external sensing hardware. \emph{(b)}: A touchpad created as a planar conductive area with four connections points, \emph{A, B, C} and \emph{D}.}
    \label{fig:Touchpad_Designs}
    \Description{The two figures, (a) and (b) illustrate how weft-knitted sensors are constructed. In both cases, there is an illustrated hand touching somewhere on the sensor. In (a), the sensing structure is a linear serpentine, and there are two electrode points A and B. An additional view of the construction is provided with intertwined gray and black lines which represent conductive and non-conductive yarns combined to form the fabric. Figure (b) shows a similar sensor; however, the sensing area is a black rectangle in the middle, and there are four connection points A, B, C, and D. The additional view of the construction shows black lines intertwined to form the fabric, representing conductive yarn.}
\end{figure*}

\subsection{Knitted Sensor}
\label{sec:knitted_sensor}

The knitted sensor design we introduce is a continuous conductive rectangular sensing area, constructed using one conductive yarn and four electrode connections points, illustrated in Figure~\ref{fig:Planar_Touchpad_Diagram}. The yarn used is outwardly conductive, since it is composed of carbon-suffused nylon, and forms a resistive mesh when knitted. The inter-row yarn loop connections form an approximate uniform resistance gradient across the planar surface. This property is beneficial to measuring touch gesture as the resistance gradient is uniform in all directions. Touch location and capacitance magnitude are inferred through voltage measurements recorded at the corner locations. Capacitive touch draws current from the current sources attached at the corners, which affect the differential charge of the circuit (Figure~\ref{fig:Planar_CTS_Touch_Sensing_Circuit}).

Patterns developed in previous work~\cite{Vallett2016a,Vallett2019a,mcdonald2020knitted}, as seen in Figure~\ref{fig:Single-wire_Touchpad_Diagram}, utilized a single linear conductive pathway that covered a planar area while following the serpentine pathway of weft knitting. In those sensors, location was inferred as a linear distance along the pathway mapped along a 2-dimensional surface. Their circuit required two connections at the endpoints of the pathway (A and B). This design strategy reduces the dimension of the measured voltage outputs to what is required to decouple linear touch location and capacitance, and improves the simplicity of connecting the textile to measurement hardware. This strategy is acceptable for inferring static touch location or tap gestures, but it is non-ideal for inferring complex gesture pathways across the surface of the sensor. The measured output response is non-uniform between geometrically adjacent touch-points which complicates precise localization. Additionally, touch-points are spaced apart into a sparse sensitive area to prevent shunting when two or more touch-points are contacted. This spacing induces loss of contact when touch is transferred to another location, thus breaking the gesture’s continuity.

\begin{figure}[ht]
    \centering
    \hfill
    \subfigure[]
    {\includegraphics[width=0.3\textwidth]{src/figures/Planar_CTS_Touch_Sensing_Circuit.pdf}\label{fig:Planar_CTS_Touch_Sensing_Circuit}}
    \hfill
    \subfigure[]
    {\includegraphics[width=0.6\textwidth]{src/figures/Planar_CTS_Gesture_Data_Example.pdf}\label{fig:Planar_CTS_Gesture_Data_Example}}
    \caption{Illustrations of the measurement circuit and example recorded gesture data. \emph{(a)}: Illustration of the measurement circuit and resistor-capacitor network circuit formed during touch. \emph{(b)}: Plot of example measured data showing the changes in gain measurement during onset, gesture, and offset.}
    \Description{This figure illustrates the measurement circuit in (a), and the signal during different stages of a gesture event. In (a), an illustrated waveform generator, a digital oscilloscope, and a knitted sensor with four connection points are shown. Additionally, there is a 5-point star circuit illustrated, which includes the four sensor connections points A, B, C, D, and a user’s point of touch on the sensor. In (b), we see a graph on top with time ranging from 0 to 1 seconds on the X axis, and voltage gain values on the Y axis. There are four lines in the graph, each representing the signal measured from one of the electrodes. The plot is split into three areas in the time axis: before the gesture event, the gesture event, and after the gesture event. During the first and third phases, the behavior of the four lines is very similar, and their voltage gain values are high. However, during the gesture event, we notice a drop in the voltage gain value for all these values, and the differences among the individual lines becoming more pronounced. Below the plot, there are three illustrations of the four-connection knitted sensor with a hand touching the sensor. In the first illustration, we see the onset of touch, in the second one, the gesture being performed, and in the third, the offset.}
  \label{fig:Planar_CTS_Gesture_Data_Example}
\end{figure}

\subsection{Measurement Circuit}
\label{sec:measurement_circuit}
The capacitive touch circuit formed by the planar conductive area is modeled as a mesh resistor-capacitor ladder network. Figure~\ref{fig:Planar_CTS_Touch_Sensing_Circuit} illustrates the circuit diagrams of the resistances formed between the location of touch and the sensing points ($A, B, C, D$), represented as a star graph. The values of the resistances vary depending on touch location. The value $C_{t}$ represents the magnitude of capacitance induced by touch, which is used as a pseudo-pressure. The voltage measurement locations ($A, B, C, D$) have associated parasitic capacitances ($C_{p_{A}}$, $C_{p_{B}}$, $C_{p_{C}}$, $C_{p_{D}}$) which affect the voltages at the measurement locations. The excitation waveform passes through current-limiting resistors ($R_{A}$, $R_{B}$, $R_{C}$, $R_{D}$) that reduce the current entering and exiting the fabric, which allows voltage measurements to be discerned.

An example gesture and processed measurement are shown in Figure~\ref{fig:Planar_CTS_Gesture_Data_Example}. Figure~\ref{fig:Measured_Data_Processing_Pipeline} further explains the waveform processing steps. All measured gestures consist of a distinct touch onset, gesture action, and touch offset. During onset, the measured waveform gains decrease due to the increase in touch capacitance. The gain at each measurement point attenuates as a function of touch location, where attenuation increases as touch proximity increases. This phenomenon can be seen during the gesture action, where the motion trajectory and gain attenuation pf each sensing point correlate. Once the gesture is complete, the offset action returns the gain measurements to a baseline.

Waveform generation and acquisition are performed using a Keysight 33622A arbitrary waveform generator and a Keysight MSO-X-3024T oscilloscope. The waveform generator produces a 2 MHz sine wave used as input to the circuit (output 1) and 250 Hz square wave used to trigger oscilloscope sampling (output 2). The sine wave passes through current-limiting resistors attached to the four corners of the fabric circuit. The resistance values are approximately equal to the touchpad resistance of 4 kOhm/sq. A parasitic capacitance of approximately 60 pF is observed at each measurement point. Gesture data is collected over the span of 1 second at a rate of 250 frames per second. Each frame window consists of approximately 4000 samples collected at 625 MSamples/s. The frames are processed using Bode analysis to return the magnitude ratio (gain) of the input and measured waveforms. The output data is of dimension $250\times 4$ corresponding to the 250 measurement frames and 4 measurement channels.

\subsection{Signal Filtering}
\label{sec:wavelet_filtering}

After the data is captured by the four electrodes as changes of gain values over time, we subtract a baseline event, captured while no gesture was being performed on the knitted touchpad. The purpose of baseline subtraction is to remove any elements of the representation that stay the same across different measurements, therefore highlighting the differences between gestures, which makes classification easier. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{src/figures/Measured_Data_Processing_Pipeline.pdf}
    \caption{Diagram of the data processing pipeline: The waveform data is sampled in segmented record windows over the duration of the gesture. The FFT of the window is processed and the magnitude of the bin at the input frequency is used to determine the window gain. The measured baseline gain is subtracted from the gesture gain data and a wavelet transform is applied to smooth high-frequency changes.}
    \label{fig:Measured_Data_Processing_Pipeline}
    \Description{This figure illustrates the signal processing steps after its acquisition. There are three major steps: the first one is recording the event window, while the input and output waveforms are continuously sampled. The second is converting the recorded time-domain window into frequency domain via Fast Fourier Transform. Subsequently, the baseline value is subtracted, and then the signal is filtered through wavelet transform.}
\end{figure}


Subsequently, in order to further reduce noise from the signal, we filter it using \emph{wavelet transform}, a powerful and flexible analytic tool which allows us to obtain signal information from both its time and frequency domains. It does not only indicate which frequencies are present in the signal, like Fourier Transform, but also when those frequencies occur in time. Wavelet Transform achieves this by calculated compromises: by low frequencies having a high resolution in the frequency domain, and low resolution in the time domain; and high frequencies having a low resolution in the frequency domain, and high resolution in the time domain. Wavelet Transform analyses the signal in different scales: first, working with larger windows of the signal for elements stretched in time, like low frequencies. Then, after that information is acquired, we can progressively make the window of interest smaller to look for information in those scales. As we shrink the window, we lose the ability to capture low frequencies, however, that information should have been obtained in the previous step. Ultimately, we are able to use the collected information from different scales to reconstruct the signal. 

In order to filter the signal, we first deconstruct the signal through a \emph{Symlet} mother wavelet with 4 vanishing moments. After having obtained all the elements necessary for reconstruction through analysis at different levels, we reconstruct the signal using only a subset of them. Typically, the higher frequencies correspond to noise, so they are removed. For our analysis, we keep only the first 4 levels of components. These settings were selected such that noise is removed, but the filtered signal follows the original closely. Figure~\ref{fig:Measured_Data_Processing_Pipeline} illustrates an example of the original signal after the baseline has been subtracted, as explained above, together with its filtered version through wavelet reconstruction.  

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{src/figures/CNN-LSTM_Network_Architecture_2.pdf}
    \caption{CNN-LSTM neural network architecture: the four-signal representation of a gesture event is used as an input after filtering, and the output is a predicted gesture category. The CNN component captures spatial relationships in the signal pattern, while the LSTM component models time-dependency.}
    \label{fig:CNN-LSTM_Network_Architecture}
    \Description{This figure shows the neural network architecture. The input to the network is the gesture performed on the sensor after it has been processed. The output is a prediction about the gesture performed, out of 12 different classes of gestures. The input first goes through the convolutional neural network, which includes functions such as convolution, max-pooling, batch normalization and dropout. Then the output of the convolutional network becomes the input to a long-short term memory network with three hidden layers of 100 nodes each. The final layer is a fully connected layer, which then maps the values to the 12-node output layer through a Log SoftMax function.}
\end{figure}

\subsection{Neural Network Architecture}
\label{sec:classification_network}
The neural network architecture used to build the recognition model relies on a combination of CNNs and LSTMs. We chose deep learning over traditional machine learning techniques based on the strengths that both CNNs and LSTMs have shown in representing time-series signal data. Additionally, these networks automatically capture the important aspects of the data without needing feature construction, typically required by most non-neural network algorithms to represent a time-series signal.

The neural network takes as an input the data after being processed as described above. The architecture, illustrated in Figure~\ref{fig:CNN-LSTM_Network_Architecture}, starts with a one-dimensional convolution layer with a rectified linear unit (ReLU) activation function, followed by a batch normalization layer and a dropout layer for regularization purposes. It continues with another one-dimensional convolution layer, again with an ReLU activation function, followed by a max-pooling layer. The function of convolution is capturing local features at different scales in the signal. Subsequently, three layers of LSTM cells follow, each with a hidden size of 100. LSTMs have been widely used for sequential data analysis, including time-series signal data, since they capture the signal's temporal dependencies. Among recurrent layer variants, they are chosen since they are capable of handling long-term dependencies in sequential data~\cite{graves2013speech,sutskever2014sequence}.

Each input to the system consists of one gesture event as captured by the four electrodes connected to the sensor, and after being transformed to filtered frequency gain values. The sequence is 250 time steps long, and there are 4 values per time step, corresponding to an input layer of size 4 to the neural network. The temporally-related frequency gain values are transformed to one of 12 possible gesture classes through a 12-output linear layer at the end of the network. In order for the network to predict a correct gesture, it needs to be trained with user data, which we discuss in Sections~\ref{sec:experiments} and~\ref{sec:results}, together with the training hyper-parameter choices.


\subsection{Model Deployment}
\label{sec:model_deployment}
For an interactive system, after the above architecture is used for training, the resulting model needs to be deployed to lightweight hardware. We use a NVIDIA® Jetson Xavier™ NX Developer Kit  (Figure~\ref{fig:nvidia_picture}), running Ubuntu 18.04 with 8GB of RAM, a 6-core NVIDIA Carmel ARM CPU, and a NVIDIA Volta GPU for gesture classification (Table~\ref{tab:nvidia_specs}). The NVIDIA board's ability to do fast parallel math is important, as it allows running pre-trained models. In addition, its form factor is relatively compact, an important feature for many of the potential applications of the sensor.

Currently, the data used as the model input is a \emph{.csv} file containing one user-captured touchpad gesture. First, we subtract the baseline readings and perform waveform filtering to prepare the data for evaluation. Subsequently, the resulting waveform is input to the model for classification. The model then returns its prediction as an output, which is one of the 12 gesture classes. 

In the future, we plan to adapt the model deployment to handle continuous or real-time signal classification, as opposed to only pre-recorded discrete signals. A barrier to real-time signal classification is the practical lack of hardware that can perform the signal generation, acquisition, and filtering at the speeds necessary to continuously supply the model with new inputs. Custom hardware, which we aim to design, would be able to stream the acquired signal to the NVIDIA Jetson board for classification. Once the Jetson board would acquire the signal, it would need to parse it to isolate individual gesture windows, since the model would have been trained on inputs containing only a single gesture. An additional model may be necessary to accurately distinguish gestures. The main gesture classification model could then take gestures as inputs as they are returned from the parser.


\vspace{4mm}
\noindent
\begin{minipage}{\textwidth}
  \begin{minipage}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=0.7\linewidth]{src/figures/jetson_nvidia.jpg}
      \captionof{figure}{NVIDIA Jetson Xavier NX Developer Kit~\cite{jetsonxaviernx}}
      \label{fig:nvidia_picture}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \centering
    \captionof{table}{Hardware Specifications}
    \label{tab:nvidia_specs}
    \begin{tabular}[b]{c|p{0.7\textwidth}}
        \small{GPU} & NVIDIA Volta Architecture with 384 CUDA cores and 48 Tensor cores \\
        \hline
        \small{CPU}  &  6-core NVIDIA Carmel ARM Processor \\
        \hline
        \small{RAM} & 8GB LPDDR4x \\
        \hline
        \small{Size} & 70 mm x 45 mm\\
    \end{tabular}
    
  \end{minipage}
\end{minipage}
\vspace{4mm}




