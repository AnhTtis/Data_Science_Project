
\section{Methods and Results} 
\label{sec:results}

Data from 5 subjects was used for training and cross-validation, while data from 3 other subjects was used to evaluate the trained models. During cross-validation, at any time, data from 4 subjects was used for training, and data from the one other subject was used for validation. Therefore, 5 total trained models were produced, each of which was evaluated using evaluation data. Any one of those models could be deployed for use in a real-time interactive system.

The subject data was acquired as described in Section~\ref{sec:experiments}, and processed as detailed in Section~\ref{sec:wavelet_filtering}. Before training, we balanced the data by oversampling within each subject, such that each class had the same number of samples, helping to achieve model stability. The balanced training dataset resulted in an average of $277$ samples per class, while the evaluation dataset was already balanced at $60$ samples per class. Our model, whose architecture was defined in Section~\ref{sec:classification_network}, was trained using PyTorch 1.4 \cite{paszke2017automatic} with CUDA 10.0, using a learning rate $\alpha = 0.001$, a dropout rate = $0.6$ and a batch size $= 128$, over $2000$ epochs. We used \emph{Adam} as an optimizer, and a negative log likelihood loss function. The network weights were initialized using Xavier initialization~\cite{glorot2010understanding}.

These hyper-parameters were the same as the ones in~\cite{mcdonald2020knitted}, where an LSTM-based neural network model was used to identify location of touch on a 36-button knitted sensor. In addition to using the subject data to train our CNN-LSTM gesture recognition model, we also use it to train an LSTM-based model as a baseline comparison. The only difference between the two methods is the neural network architecture. The technique used in in~\cite{mcdonald2020knitted} also involves a statistical feature construction step over the 92 analyzed frequency gains, which was not possible to apply to our current data, since, due to hardware constraints, only relies on a single frequency. The results of using the baseline architecture are included in Table~\ref{tab:results}, together with those of our neural network model.   

\subsection{Cross-Validation and Evaluation Results}

Our model's cross-validation \emph{accuracy}, reported in Table~\ref{tab:results}, is 78.6\%. This accuracy was calculated as the average accuracy of the 5 cross-validation folds. This means that the data from one subject at a time was tested against the model trained on the data from the other 4 subjects, and subsequently, the average of the results is reported. The accuracy for each fold was calculated as the average of the last 50 epochs. The test dataset performance, which aims to capture how generalizable the model is, shows an even greater accuracy of 88.5\%. This accuracy was calculated as the average of testing the 3 test subject data against the 5 trained models produced during cross-validation. 

\begin{table}[h]
  \centering
  \caption{Classification results for model cross-validation and evaluation.}~\label{tab:results}
  \vspace{0.5cm}
  \begin{tabular}{l|cccc|cccc}
  \vspace{3mm}
    & \multicolumn{4}{c}{\small \textit{Cross-Validation}} & \multicolumn{4}{c}{\small \textit{Evaluation}} \\
    
    {\small\textit{Method}} & {\small \textit{Accuracy}} & {\small \textit{F1-Score}} & {\small \textit{Precision}} & {\small \textit{Recall}} & {\small \textit{Accuracy}} & {\small \textit{F1-Score}} & {\small \textit{Precision}} & {\small \textit{Recall}}\\
    
    \midrule
    \small{LSTM}         & 41.7\% & 22.4\% & 25.4\% & 22.0\% & 52.5\% & 53.9\% & 61.6\% & 52.5\% \\
    \small{CNN + LSTM}   & 78.6\% & 73.5\% & 73.9\% & 73.7\% & 89.8\% & 89.7\% & 90.3\% & 89.8\% \\
    % \small{CNN + LSTM}   & 78.6\% & 73.5\% & 73.9\% & 73.7\% & 88.5\% & 88.3\% & 89.3\% & 88.5\% \\
  \end{tabular}
  \Description{This table shows the cross-validation and evaluation performance measures for the baseline method, which is a long-short term memory network (LSTM), and for the method proposed in this work, a combination of convolutional and long-short term memory network (CNN-LSTM). The performance measures are accuracy, precision, recall, and F1-score. The CNN-LSTM network significantly outperforms the LSTM one in all measure, resulting in an accuracy of 78.6\% for cross-validation and 89.8\% for evaluation.}
\end{table}

Other measures of performance we calculate are \emph{macro-precision, macro-recall}, and \emph{macro-F1-score}. Precision refers to the ratio of true positives to the combined number of true and false positives. Recall is the ratio of true positives to the combined true positives and false negatives. The F1-score is the harmonic mean of precision and recall: $F1 = 2 * (precision * recall) / (precision + recall)$. Macro-precision, macro-recall, and macro-F1 refer to the balanced respective scores per class, and those values are obtained by averaging all respective class scores. In Table~\ref{tab:results} these values are simply referred to as: \emph{precision}, \emph{recall}, and \emph{F1-score}. These scores give greater insight into how specific accuracies were achieved. Additionally, we provide the classification matrices of all gestures in Figure~\ref{fig:Confusion_Matrices}, for both cross-validation and evaluation of our CNN-LSTM model.

As seen from Figure~\ref{fig:Confusion_Matrices}, there are gestures that incorrectly get classified as others, with pairs such as \emph{'S'} and \emph{'5'}, or \emph{'V'} and \emph{'W'} being noticeable. This can be expected as the trajectories between the two gestures are very similar. It should be noted that these results are not rotation-invariant, due to the nature of the gestures selected. Otherwise, the users' intention regarding entering a \emph{M} versus a \emph{W} would be obscured. Overall however, the model accurately identifies gestures performed on it, and generalizes very well, most probably in part, because the architecture includes batch normalization and a relatively high dropout rate of 0.6. If we compare the performance of this model to the baseline, we notice that the CNN-LSTM model significantly out-performs the baseline in all measures. 

\begin{figure*}[ht]
    \centering
    \subfigure[]
    {\includegraphics[width=0.495\textwidth]{src/figures/cv_heatmap.pdf}\label{fig:cv_confusion}}
    \hfill
    \subfigure[]
    {\includegraphics[width=0.495\textwidth]{src/figures/final_test_heatmap.pdf}\label{fig:test_confusion}}
    \hfill
    \caption{Classification matrices produced with results from cross-validation \emph{(a)}, and evaluation \emph{(b)}. The matrix rows denote the true row categories of the gestures, while the columns show the ones predicted during evaluation. Each category is denoted by the respective gesture pathway performed on the knitted touchpad. A higher-value diagonal, with the rest of the matrix having lower values, would be a desirable combination indicating a high accuracy. This figure illustrates how most gestures are correctly classified - if gestures are similar to each-other however, it is easy to incorrectly classify them as each-other.}
    \label{fig:Confusion_Matrices}
    \Description{This figure shows two classification matrices, one for the results of cross-validation in (a), and the other for the results of evaluation in (b). Both the rows and the columns of each matrix are labeled by the gesture types; therefore, the size of each matrix is 12x12. In both cases the diagonal along each is a much darker color (dark purple to black) compared to the surrounding values (yellow to orange).}
\end{figure*}

The accuracies reported here are subject-independent, even though subject-specific calibration is expected to increase overall accuracy. If a knitted pattern is designed to be used by a single user, calibration is likely to increase the gesture detection accuracy, since it removes variations that are caused by different users' physiological states, or specific ways of performing gestures. However, designing a system that does not rely on calibration makes the experience of multiple users interacting with the same sensor much more intuitive and non-intrusive. Additionally, we expect that provided more data is available, the model's accuracy would increase, since neural network models typically require a large amount of data to produce accurate results. Moreover, training under a variety of conditions, such as different environmental condition and other possible distortions, would mean that the model would incorporate the signal variations induced, and be more robust when exposed to them.   

\subsection{Resources and Time Performance}
The machine on which our models were trained through cross-validation, and later evaluated was running Ubuntu 18.04, with an Intel\textsuperscript{\textregistered}~Core\textsuperscript{\texttrademark} i9-9960X CPU @ 3.10GHz, 128 GB RAM, and 2x RTX 2080 Ti Blowers with NVLink cards. The trained models were deployed and evaluated on a NVIDIA Jetson Xavier NX Development Kit, described in Section~\ref{sec:model_deployment}, which was also running Ubuntu 18.04. We measured the total runtime of 3 different stages of the evaluation process: the time to convert the input from a numpy array to a CUDA tensor, the time taken for the loaded model to evaluate the input tensor, and the total time taken for one trial including the first two measurements. The results of these measurements are included in Table~\ref{tab:timing}.

\begin{table}[ht]
    \centering
    \caption{Timing measurements of model execution}~\label{tab:timing}
    \begin{tabular}{l|ccc}
         &  \small \textit{Conversion to CUDA} & \small \textit{Sample Evaluation} & \small \textit{Total Trial}\\
         \midrule
        \small \textit{Bootstrap and First Trial Time} & 3.7s & 3.6s & 7.3s\\
        \small \textit{Avg. Trial Time after first} & $1.9\times10^{-4}$s & $1.7\times10^{-2}$s & $3.0\times10^{-2}$s\\
        \Description{This table shows the time response for processing the first sample, as well as the average of several samples after the first, measured while running on the NVIDIA® Jetson Xavier NX™ Developer Kit. There are three columns to it: ‘conversion to CUDA’, ‘sample evaluation’, and ‘total trial’. The total trial time is 30ms for an average trial.}
    \end{tabular}
\end{table}

The purpose of this evaluation was to determine the feasibility of building a responsive real-time interactive system given a trained gesture recognition model. From the results in Table~\ref{tab:timing}, we can see that the average response time of a typical evaluation is approximately 30 ms, a very reasonable response time for real-time systems. The first trial's response time is much greater since it involves bootstrap and the NVIDIA CUDA® Deep Neural Network library (cuDNN) performing cache allocations. Typically, that first trial in real-world applications can be substituted with a sample that is not relevant to the gesture signal evaluation. For a fully-interactive system, the response time of the signal acquisition and processing controller would need to be added to that response time. Even though that component is not implemented in this work, prior work~\cite{mcdonald2020knitted} has shown that it is possible to have a responsive micro-controller performing those functionalities. Such systems could enable many applications, some of which are discussed in the following section, together with other considerations for real-world interactivity.

