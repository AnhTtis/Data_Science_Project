\section{Kernel Regularized Learning of LPV Feedforward Parameters}
\label{sec:kernel}
In this section, the functions $\theta_i(\rho)$ in \eqref{eq:FFModel} are identified using kernel regularization, which models the functions without a specified structure or order, since the solution is in the infinite-dimensional Reproducing Kernel Hilbert Space (RKHS). Second, kernel design for LPV feedforward parameters is described. Finally, the developed approach is summarized in a procedure.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Kernel Regularized Identification}
A cost function is defined using input-output data as \citep{Pillonetto2014,Blanken2020}
\begin{equation}
	\label{eq:minimization}
	\hat{\Theta} = \arg \min_\Theta \left\| \overline{w}-\Phi\Theta\right\|^2 + \gamma\|\Theta\|^2_\mathcal{H},
\end{equation}
with Euclidean norm $\|\cdot\|$, $\Phi\Theta$ equal to $w_{ff}$ in \eqref{eq:FFModel}, and measurement data vector $\overline{w}$, that is constructed as
\begin{equation}
	\label{eq:defineWbar}
	\overline{w}= \begin{bmatrix}w(0T_s) & w(1T_s) & \cdots & w((N-1)T_s)\end{bmatrix}^\top.
\end{equation}
The squared induced norm on the RKHS $\mathcal{H}$ is denoted as $\|\Theta\|^2_\mathcal{H}$, that is given by \citep{Pillonetto2014},
\begin{equation}
	\label{eq:RKHS}
	\|\Theta\|^2_\mathcal{H} = \Theta^\top K^{-1} \Theta,
\end{equation}
with kernel $K$. The parameter vector $\Theta$ and basis function matrix $\Phi$ are built up as
\begin{equation}
	\label{eq:ThetaPhi}
	\begin{aligned}
		\Theta = \begin{bmatrix}
			\overline{\theta}_1^\top &
			\overline{\theta}_2^\top &
			\cdots &
			\overline{\theta}_{n_\theta}^\top
		\end{bmatrix}^\top, &&
		\Phi = \begin{bmatrix}
			\overline{\phi}_1 & \overline{\phi}_2 & \cdots & \overline{\phi}_{n_\theta}
		\end{bmatrix},
	\end{aligned}
\end{equation}
where the individual parameter vector $\overline{\theta}_i$ and $\overline{\phi}$ are constructed by gathering the values over a training period as
\begin{equation}
	\label{eq:defineThetaPhibar}
			\resizebox{0.85\hsize}{!}{
		$
	\begin{aligned}
		\overline{\theta}_i &= \begin{bmatrix}
			\left( \theta_i(\rho)\right)\left( 0T_s\right)  \\
			\left(\theta_i(\rho)\right)\left( 1T_s\right)  \\
			\vdots \\
			\left(\theta_i(\rho)\right)\left( \left(N-1 \right) T_s)\right) 	\end{bmatrix} \\
		\overline{\phi}_{i}&=\begin{bmatrix}
			\left(\psi_{i} y\right)\left(0 T_{s}\right) & 0 & \cdots & 0 \\
			0 & \left(\psi_{i} y\right)\left(1 T_{s}\right) & \cdots & \vdots \\
			\vdots & \ddots & \ddots & \vdots \\
			0 & \cdots & \cdots & \left(\psi_{i} y\right)\left((N-1) T_{s}\right)
		\end{bmatrix}
	\end{aligned}
$}
\end{equation}
where $(\frac{d}{dt},I)$ has been left out for brevity. 

The solution to the cost function in \eqref{eq:minimization} is given by \citep{Pillonetto2014,Blanken2020}
\begin{equation}
	\label{eq:solTheta}
	\hat{\Theta} =K \Phi^{\top}\left(\Phi K \Phi^{\top}+\gamma I_{N}\right)^{-1} \overline{w},
\end{equation}
where parameters $\theta$ are estimated at any $\rho^*$ using the representer theorem \cite[Section~9.2]{Pillonetto2014}.
 \begin{rem}
	\label{rem:bias}
Note that \eqref{eq:minimization} is an open-loop solution, while a closed-loop control structure is assumed as shown in \figRef{fig:controlStructure}, hence measurement noise introduces bias \Citep{Blanken2020}. The addition of instrumental variables is capable of removing this bias, and is reported elsewhere.
\end{rem}
The kernel $K$ can be designed to incorporate prior knowledge on the feedforward parameters, such as smoothness or periodicity, and will be discussed in the next section.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Kernels for LPV Feedforward Parameters}
The kernel incorporates prior knowledge on the feedforward parameters, hence is important to design carefully. The optimal kernel for solving \eqref{eq:minimization}, when treating feedforward parameters as random variables, is equal to
\begin{equation}
	\label{eq:optKernel}
		\resizebox{0.88\hsize}{!}{
		$
	\Pi = \mathbb{E}\left(\Theta \Theta^\top \right) = \begin{bmatrix}
		\mathbb{E}( \overline{\theta}_1\overline{\theta}_1^\top) & 	\mathbb{E}( \overline{\theta}_1\overline{\theta}_2^\top) & 	\cdots & 	\mathbb{E}( \overline{\theta}_1\overline{\theta}_{n_\theta}^\top) \\
		\mathbb{E}( \overline{\theta}_2\overline{\theta}_1^\top) & \mathbb{E}( \overline{\theta}_2\overline{\theta}_2^\top) & \cdots & \vdots \\
		\vdots & \vdots & \ddots & \vdots \\
		\mathbb{E}( \overline{\theta}_{n_\theta}\overline{\theta}_1^\top) & \cdots & \cdots & \mathbb{E}( \overline{\theta}_{n_\theta}\overline{\theta}_{n_\theta}^\top)
	\end{bmatrix}. $}
\end{equation}
For LPV motion systems, parameters may correlate, i.e., $\mathbb{E}\left( \overline{\theta}_i\overline{\theta}_j\right) \neq 0 \, \forall j\neq i$. For example, when looking at \eqref{eq:exampleFFPara}, parameters $\theta_3$ and $\theta_4$ are scaled versions of each other. Hence, the framework is capable of incorporating correlation between feedforward parameters.


The optimal kernel is approximated by a kernel matrix,
\begin{equation}
	\label{eq:covker}
	\mathbb{E}\left( \overline{\theta}_i\overline{\theta}_j^\top\right) = K_{ij}(\overline{\rho},\overline{\rho}),
\end{equation}
which only has a static dependency on $\rho$, while the framework produces feedforward which is dynamically dependent on the scheduling sequence as shown in \secRef{sec:BFFF}. 

The kernel matrix $K_{ij}$ is determined by evaluating a kernel function, such as the squared exponential kernel function 
\begin{equation}
	\label{eq:SEKernel}
	k_{ij,SE}(\rho,\rho^\prime) = \sigma_{ij}^2\exp\left( -\frac{\left( \rho-\rho^\prime\right) ^2}{2\ell_{ij}^2}\right) .
\end{equation}
The hyperparameters of the kernel, i.e., for the squared exponential kernel in \eqref{eq:SEKernel} the output variances $\sigma_{ij}^2$ and length scales $\ell_{ij}$ can be tuned using marginal-likelihood optimization. The kernel choice provides the user to apply prior knowledge on the feedforward parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Developed Procedure}
The developed procedure is summarized in Procedure~\ref{proc:1}.
\vspace{3pt}\hrule\begin{proced} \textit{(Kernel regularized LPV feedforward identification)} \hfill \vspace{0.5mm} \hrule
	\label{proc:1}
	\begin{enumerate}
		\item Apply reference $r$ to closed-loop system in \figRef{fig:controlStructure} and record $y$, $\rho$ and $u$.
		\item Construct kernel matrices by evaluating \eqref{eq:covker}. 
		\item Calculate matrix $\Phi$ using \eqref{eq:ThetaPhi} and \eqref{eq:defineThetaPhibar}.
		\item Compute $\bar{w}$ from \eqref{eq:defineWbar} using the second integral of the input $w=\iint u \: dt^2$.
		\item Estimate the feedforward parameters $\hat{\Theta}$ using \eqref{eq:solTheta}.
	\end{enumerate}
	\vspace{0pt} 	\hrule 	\vspace{-2pt}
\end{proced}
To conclude, kernel regularized identification is capable of identifying LPV feedforward parameters with input-output data of a system, without specifying a structure or order. In the following section, an example is shown that validates the developed framework.