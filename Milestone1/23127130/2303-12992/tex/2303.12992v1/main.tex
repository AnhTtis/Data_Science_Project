

\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{arydshln}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{times}
\usepackage{array}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{bbm}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{float}
\usepackage{color}
\usepackage{stfloats}
\usepackage{mathtools}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{siunitx}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{boldline}
\usepackage{lipsum}
\usepackage{setspace}
\usepackage{caption}
%\usepackage{supertabular}
%\usepackage{longtable}
\usepackage{flushend,cuted}
%\usepackage{supertabular,booktabs}
\usepackage{xtab,booktabs}
%\usepackage[font=small,justification=justifying]{caption}
\usepackage{balance}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	filecolor=blue,      
	urlcolor=black,
	citecolor=green,
    breaklinks=true
}
\usepackage{cite}




\usepackage{multicol}
\usepackage{lipsum}% just to automatically generate some text

\newcommand{\wuge}[1]{{\textcolor{blue}{#1}}} % blue
\newcommand{\wugerewrite}[1]{{\textcolor{red}{#1}}} % red
\newcommand{\wugetable}[1]{{\textcolor{black}{#1}}} % yellow 若要检查新添加论文，重新改回黄色即可
\newcommand{\wugegpt}[1]{{\textcolor{purple}{#1}}} % purple
\newcommand{\implus}[1]{{\textcolor{cyan}{#1}}}
\newcommand{\ylfChecked}[1]{{\textcolor{black}{#1}}}
%\newcommand{\ylfChecked}[1]{{\textcolor[rgb]{0.24,0.74,1.00}{#1}}}
\newcommand{\implusChecked}[1]{\textcolor{gray}{#1}}

% *** CITATION PACKAGES ***
%
%\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
%  \usepackage[nocompress]{cite}
%\else
  % normal IEEE
%  \usepackage{cite}
%\fi

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Survey of Historical Learning:\\ Learning Models with Learning History}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.
\author{
        Xiang Li$^*$,
	    Ge Wu$^*$,
	    Lingfeng Yang,
        Wenhai Wang,
        Renjie Song,
		Jian Yang$^\#$\thanks{$*$ Equal contribution. $\#$ Corresponding author.}
		\IEEEcompsocitemizethanks{
		    %\IEEEcompsocthanksitem X. Li and J. Yang are from College of Computer Science, Nankai University. Email: xiang.li.implus@qq.com
		    
		    \IEEEcompsocthanksitem X. Li, G. Wu and J. Yang are from IMPlus@PCALab, College of Computer Science, Nankai University. Email: xiang.li.implus@nankai.edu.cn, gewu.nku@gmail.com, csjyang@nankai.edu.cn
		    %are from College of Computer Science, Nankai University. Email: xiang.li.implus@qq.com
		    \IEEEcompsocthanksitem L. Yang is with PCALab, School of Computer Science and Engineering, Nanjing University of Science and Technology. Email: yanglfnjust@njust.edu.cn
            \IEEEcompsocthanksitem W. Wang is with Shanghai AI Laboratory. Email: wangwenhai362@gmail.com
            \IEEEcompsocthanksitem R. Song is from Megvii Technology, Nanjing. Email: songrenjie@megvii.com
		}
	}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{}%IEEE Transactions on Pattern Analysis and Machine Intelligence}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%



\begin{abstract}
New knowledge originates from the old. The various types of elements, deposited in the training history, are a large amount of wealth for improving learning deep models. In this survey, we comprehensively review and summarize the topic--``Historical Learning: Learning Models with Learning History'', which learns better neural models with the help of their learning history during its optimization, from three detailed aspects: Historical Type (what), Functional Part (where) and Storage Form (how). To our best knowledge, it is the first survey that systematically studies the methodologies which make use of various historical statistics when training deep neural networks. The discussions with related topics like recurrent/memory networks, ensemble learning, and reinforcement learning are demonstrated. We also expose future challenges of this topic and encourage the community to pay attention to the think of historical learning principles when designing algorithms. The paper list related to historical learning is available at \url{https://github.com/Martinser/Awesome-Historical-Learning.}
%\href{https://github.com/Martinser/Awesome-Historical-Learning}{https://github.com/Martinser/Awesome-Historical-Learning}
%\url{https://github.com/Martinser/Awesome-Historical-Learning}

%systematically formulate the topic ``Historical Learning: Learning Models with Learning History'' by 1) reviewing the already existing approaches that utilize the statistical information in different aspects during the learning history; 2) organizing elaborate future directions and discussions related to this topic; 3) showcasing a new and effective method that exploits the historical input-prediction-label triplets (termed ``RecursiveMix'') to enhance the generalization of deep computer vision models. We hope this topic can serve as a meaningful, inspiring, and valuable designing principle for the community to explore continuously. Codes and pretrained models are available at XXXXXXX.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Learning history, historical type, functional part, storage form, deep learning.
\end{IEEEkeywords}}


% make the title area
\maketitle


\begin{figure*}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=0.98\textwidth]{./figs/HL_concept_V3.pdf}}
	\end{center}	
	\vspace{-10pt}
	\caption{ 
	\textbf{The concept of ``Learning Models with Learning History''.} During optimization, deep models can be improved potentially by exploiting the historical statistics, including the past labels, predictions, feature representations, inputs, model parameters, gradients, losses, and their combinations. ``iter.'' denotes iteration.
	}
	\label{fig_hl_concept}
	%\vspace{-10pt}
\end{figure*}

% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


%%%%%%%%% BODY TEXT
\section{Introduction}
%``Learn the new by reviewing the past, you can be a teacher''--Kong Zi, 
\noindent\emph{``He who by reviewing the old can gain knowledge of the new and is fit to be a teacher.''}--Confucius, China.

%Human acquires new knowledge and makes advances by keeping cherishing their past experiences. The diverse old knowledge may consist of hard mistakes, novel perspectives, and valuable introspection of certain topics. Such wealth enables people to make continuous progress and become experts in specific fields.

{People gain new knowledge and make advances by continuously valuing their past experiences. These varied experiences may include hard-won lessons, innovative perspectives, and valuable introspection on certain topics. This wealth of knowledge allows individuals to make ongoing progress and become experts in their fields.}

%This phenomenon is considerably similar in machine intelligence. When deep models are trained, a large quantity of various historical statistics can be accumulated. The diverse statistics include historical gradients, model parameters, predictions, intermediate feature representations (at any scale), labels, inputs (with possibly different augmentations) and losses. Analogous to the human system, the cumulative past information of learning models could have extraordinary potentials to boost their intelligence.

{This phenomenon is also present in machine intelligence. When deep learning models are trained, they accumulate a large amount of diverse historical data, including gradients, model parameters, predictions, intermediate feature representations, labels, inputs, and losses. Just like in the human system, the accumulation of past information during the learning process has the potential to significantly enhance the intelligence of these models.}

%In this paper, we first specifically summarize and formulate the topic -- ``Historical Learning: Learning Models with Learning History'' (Fig.~\ref{fig_hl_concept}), aiming at providing a comprehensive and systematic view of utilizing historical learning statistics and opening up new perspectives to expose what, where, and how to exploit, given the above diverse historical knowledge. 

{In this paper, we summarize and formulate the topic of ``Historical Learning: Learning Models with Learning History'' (Fig.~\ref{fig_hl_concept}). Our goal is to provide a comprehensive and systematic view of using historical elements during optimization and to explore the potential for utilizing this diverse knowledge to benefit machine learning models. We aim to answer the questions of what, where, and how this information can be exploited. Additionally, we discuss the relationships between historical learning and other related topics and outline the challenges that must be addressed in order to effectively use historical data to improve learning systems.
}
% Meanwhile, we demonstrate that there is a considerably large room of this topic, which is worthy of being dug deeply.



%It is worth noting that there is already a series of existing works that ``learn models with learning history''. The previous methods mainly focus on five historical types: 
{There have been a number of previous works that explore the use of historical data in machine learning models. These methods typically focus on five types of historical data:} 
(1) \textbf{Gradients}. All the optimizers~\cite{liu2019variance,loshchilov2017decoupled,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive, dozat2016incorporating, chen2018closing, heo2020adamp, xie2022adan,ding2019adaptive} using momentum-based operators have made good use of the past gradients of model parameters to improve the training convergence. Equalization loss~\cite{tan2021equalization,li2022equalized} and~\cite{kwon2020backpropagated} adopt the historical gradient-guided mechanism in long-tailed training or abnormal sample detection. 
(2) \textbf{Model parameters}. The historical architectures are generally utilized as discrete ensemble checkpoints~\cite{huang2017snapshot,chen2017checkpoint,chen2018short,ilg2018uncertainty,poggi2020uncertainty,wolterink2018automatic,yu2019deep,liu2020loss,xu2020multi,gong2022uncertainty,zhang2020swa,athiwaratkun2018there,izmailov2018averaging,yang2019swalp,cha2021swad,guo2022stochastic,garipov2018loss,maddox2019simple,von2020neural} for inference or as moving-averaged/direct teachers~\cite{caron2021emerging,chen2021empirical,feng2021temporal,grill2020bootstrap,he2020momentum,chen2020improved,el2021training,chen2021transferrable,luo2021coco,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography,xie2021detco,tomasev2022pushing,deng2021unbiased,cai2019exploring,
mitrovic2020representation,yuan2020revisiting,li2019learning,yang2019snapshot,furlanello2018born,wang2022efficient,wang2022learn,wang2022self,tarvainen2017mean,cai2021exponential, zhang2023robust,
cai2022semi,wang2022unsupervised,shi2018transductive,wang2021multi, kim2022pose, yu2019uncertainty, meng2021spatial, wang2021self,liu2022perturbed, cui2019semi, perone2018deep,zhang2022semi, xu2021end, yang2021interactive , liu2021unbiased, liu2022unbiased,wang2022semi} to provide extra meaningful supervisions. 
(3) \textbf{Feature representations}. The stored old feature maps efficiently augment data points to construct supplementary/critical training pairs~\cite{li2021spatial,zhuang2019local,zhou2022regional,zheng2021group,sprechmann2018memory,wang2020cross,deng2022insclr,yang2022online,deng2021variational,li2020unsupervised,feng2021exploring,liu2022memory,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,yang2018learning,yang2019visual,el2021training,liu2022densely,wang2022contrastive,wen2021false,liu2022learning,liu2021noise,ortego2021multi,jin2021mining,yang2022cross,zhong2019invariance,wu2018improving,wu2018unsupervised,misra2020self, tian2020contrastive, yin2021instance, dai2021dual,wang2022semi,ko2021learning,he2020momentum, chen2020improved,chen2021transferrable,luo2021coco,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography, xie2021detco
} or class centers~\cite{wen2016discriminative,he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,chen2021free,li2021frequency, keshari2020generalized, jing2021cross,fan2018associating,liu2020leveraging}. Their statistics (e.g., mean and variance) are sometimes accumulated for better training stability/convergence~\cite{ioffe2015batch,caron2021emerging,ioffe2017batch, li2016revisiting, salimans2016weight, arpit2016normalization}. 
(4) \textbf{Probability predictions}. The recorded probability predictions of specific instances are adopted for ensembling~\cite{laine2016temporal,yang2022recursivemix, yao2020deep,chen2020semi} and self-teaching~\cite{kim2021self,nguyen2019self, shen2022self}. %temporal ensembling
(5) \textbf{Loss values}. The statistics of loss values are used to identify
anomalous data~\cite{chu2020neural} and biased data~\cite{jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample}. 
%Beyond the perspective of historical type, we elaborate another two aspects (functional part and storage form) to better understand where and how these historical statistics are applied. These existing methods also provide extensive evidences that the learning history is of great importance for training stronger deep models.
{In addition to these historical types, we also consider the functional part (where the data is used) and the storage form (how it is stored) of historical statistics to better understand how it is applied in these methods. These existing works provide strong evidence that historical statistic is important for training more effective deep learning models.}

%In summary, our contributions are as follows:
%To our best knowledge, we are the first to systematically review and formulate the topic ``Learning Models with Learning History''. We attribute and summarize the existing representative works from three detailed aspects, discuss its relation to several related topics, and elaborate future challenges that encourage the community to explore continuously.
{To the best of our knowledge, we are the first to provide a systematic review and formulation of the topic ``Historical Learning: Learning Models with Learning History.'' We thoroughly summarize and attribute the existing representative works in this field, examining them from three detailed aspects. We also discuss the relationships between this topic and several related areas and outline the challenges that will encourage the community to continue exploring this topic in the future.}

%$\bullet$ We further develop one novel direction (i.e., the historical input-prediction-label triplets) of the topic and propose RecursiveMix (RM) which iteratively augments the new training data with the old ones. Under the negligible cost of additional storage/computing complexity and model adjustment, RM consistently improves the performance of vision models on the competitive benchmarks. 

\section{Learning Models with Learning History}
In this section, we summarize and describe the concept of 
%``Learning Models with Learning History'' 
{``Historical Learning: Learning Models with Learning History''}
in details.

Assume a deep model has $n$ hierarchical layers with total $\{W_1, ...,  W_n\}$ parameters, and it takes input $x_0$ to produce the prediction $p$, i.e., $p=\sigma(x_n)$, where $x_n=f(x_0, \{W_1, ...,  W_n\})$ and $\sigma$ is the output activation function. The loss $l$ is obtained from $p$ under the current supervision $y$. The feature $x_i$ generated by each layer $i\ (0 < i \le n)$ constitutes the intermediate representations $\{x_1, ...,  x_n\}$ at all scales. Based on the chain rule of backpropagation~\cite{lecun1988theoretical}, we have two groups of the gradients calculated on loss $l$: $\{\nabla_{W_1}, ..., \nabla_{W_n}\}$ w.r.t. the parameters and $\{\nabla_{x_0}, ..., \nabla_{x_n}\}$ w.r.t. the input and all-level features maps. Specifically, we use the superscript $t$ (e.g., $p^t$) to mark the moment of each variable in the training process. The superscript $t$ can be epoch-wise, iteration-wise, or customized period-wise.

Given moment $t$, the historical information $H$ is then defined as a superset of all the past statistics: 
\begin{equation}
\begin{aligned}
	\vspace{0pt}
    H = \Big\{ x_0^j, y^j, p^j, l^j,  \{x_1^j, ...,  x_n^j \},  \{W_1^j, ...,  W_n^j\}, \\ \{\nabla_{x_0}^j, ..., \nabla_{x_n}^j\}, \{\nabla_{W_1}^j, ..., \nabla_{W_n}^j\} |  \forall j < t \Big\}.
    \label{eqn_history}
\end{aligned}
\end{equation}

%Learning deep models usually involves three major ingredients: Data, Model and Loss~\cite{lecun2015deep}. Any type of the designed algorithm $\mathcal{A}(\cdot)$, which adopts the statistical usage of the history $H$ (i.e., $\mathcal{A}(H)$) in the three components to improve the model performance, can be formally regarded as ``Learning Models with Learning History''. 
% {The process of learning deep learning models typically involves three main components: data, model, and loss~\cite{lecun2015deep}. Any algorithm $\mathcal{A}(\cdot)$ that uses the statistical information from the history $H$ (i.e., $\mathcal{A}(H)$) to improve model performance can be considered ``Learning Models with Learning History.''}
\ylfChecked{Any algorithm $\mathcal{A}(\cdot)$ that uses the statistical information from the defined history $H$ (i.e., $\mathcal{A}(H)$) to improve model performance can be considered ``Learning Models with Learning History.'' Meanwhile, the historical statistical information can be categorized into three primary elements - data, model, and loss~\cite{lecun2015deep} - based on their functional role in the typical process of training deep learning models.}

%It is worth noting that the full content of $H$ is space consuming and it is usually impossible and inefficient to store all of them in GPU or CPU devices. There are two typical ways dealing with this issue in the literature: 1) {Discrete Elements (DE)} and 2) {Moving Average (MA)}. DE attempts to memorize a limited number of historical checkpoints for later operation while MA applies momentum or summary updates to the recorded variables. 
% {It is important to note that the complete history $H$ can take up a lot of storage space and is often impractical or inefficient to keep all of it on GPU or CPU devices. There are two common approaches to addressing this issue in the literature: (1) Discrete Elements (DE), which involve storing a limited number of historical checkpoints for later use, and (2) Moving Average (MA), which involves applying momentum or summary updates to the recorded variables.}
\ylfChecked{It is important to note that the entire history $H$ can occupy a substantial amount of storage space, and it may be impractical or inefficient to store all of it on GPU or CPU devices. To address this issue, there are two commonly used methods in the literature: (1) Discrete Elements (DE), which entails storing a limited number of historical checkpoints for later use, and (2) Moving Average (MA), which involves applying momentum or summary updates to the stored variables.}
%In summary, the topic of ``Historical Learning: Learning Models with Learning History'' seeks to boost deep models in the part of Data, Model or Loss, relying on the historical statistics $H$ in a storage form of DE or MA. In the next section, we demonstrate a detailed review of existing representative approaches from the above-mentioned three perspectives.

{In summary, the topic of ``Historical Learning: Learning Models with Learning History'' involves using historical data related to the model during its optimization in the form of Discrete Elements (DE) or Moving Average (MA), to improve the generalization and performance of the model. In the next section, we provide a detailed review of existing approaches to this topic from the three perspectives mentioned above.}


\section{Review of Existing Approaches}
Table~\ref{table1} summarizes the representative works related to ``Learning Models with Learning History'', and the timeline of these works is shown in Fig.~\ref{fig_timeline}. We elaborate on their details in the following subsections from multiple aspects.  % Table 1 如果需要调整或拆分，这里的文字也要相应改变


%3.1chatgpt已全刷
\subsection{Aspect of Historical Type~\label{sec_3.1}}
\noindent\textbf{Prediction.} It is known that the past predictions of specific instances are from architectures with diverse parameters and distinct augmented inputs. Therefore, the prediction can carry informative and complementary probability distributions, making it potential to be a teacher that can guide the learning of the original model. {Following this intuition, Kim et al.~\cite{kim2021self} adopt the prediction only from the last epoch. Thus, the new target~$p^{t}$ of PS-KD at epoch $t$ can be written as:
\begin{equation}
\begin{aligned}
p^{t}=\left(1-\alpha^{t}\right) {y}^t+\alpha^{t} p^{t-1},
\end{aligned}
\label{eq_target}
\end{equation}
\begin{equation}
\begin{aligned}
\alpha^{t}=\alpha^{T} \times \frac{t}{T}.
\label{eq_alpha}
\end{aligned}
\end{equation}
Eq.~\eqref{eq_target} represents that the current soft target~$p^{t}$ is a combination of the past~$(t-1)$-th epoch prediction~$p^{t-1}$ and the one-hot target~$y$. Moreover, the hyperparameter~$\alpha$ changes continuously with epoch $t$ in Eq.~\eqref{eq_alpha}. Through the use of historical predictions, the model can implicitly change the weight of hard samples, thereby paying more attention to these samples. {Chen et al.~\cite{chen2020semi} also integrate historical predictions to produce a smoother adaptive distribution for the current iteration. Besides, employing historical predictions has been effectively implemented in the temporal ensembling method by Laine et al.~\cite{laine2016temporal},}
% which uses the Exponential Moving Average (EMA) among all past epochs:}
\ylfChecked{which uses the Exponential Moving Average (EMA) to update predictions among all past epochs:}
%Laine et al.~\cite{laine2016temporal} successfully exploit the temporal ensembling among all the past epochs via Exponential Moving Average (EMA):
\begin{equation}
\begin{aligned}
p^t  =\alpha p^{t-1}+(1-\alpha) p ^t.
\end{aligned}
\end{equation}
%The prediction of the network~$ p ^t$ ensembles with the historical prediction~$p^{t-1}$ to perform the cumulative calculation. 
The prediction of the network~$p^t$ is cumulatively calculated with the historical prediction~$p^{t-1}$. Additionally,~$\alpha$~represents the momentum term, which determines the proportion of historical prediction participating in the $t$-th iteration. Shen et al.~\cite{shen2022self} use half of each mini-batch for extracting smoothed labels from previous iterations and the other half for providing soft targets for self-regularization.}

{Motivated by the work in~\cite{laine2016temporal}, some researchers have used temporal ensembling to improve network performance. Yao et al.~\cite{yao2020deep} use historical predictions of previous epochs to obtain more accurate confidence levels for different labels. Nguyen et al.~\cite{nguyen2019self} solve the issue of network overfitting in noisy datasets by filtering out incorrect labels during training and using EMA to ensemble historical predictions. Different from above work, Yang et al.~\cite{yang2022recursivemix} propose RecursiveMix, which takes the prediction from historical input as the target and aligns it with the semantics of the corresponding ROI area in the current training input.}
%{Motivated by~\cite{laine2016temporal}, some researchers have used temporal ensembling to improve network performance. Yao et al.~\cite{yao2020deep} also temporally ensemble the historical predictions of previous epochs. As a result, obtaining more accurate confidence levels for different labels is possible. To solve the problem of network over-fitting when training datasets with noise labels, Nguyen et al.~\cite{nguyen2019self} filter out the wrong labels during training, which also employs EMA to ensemble historical predictions. Shen et al.~\cite{shen2022self} use half of each mini-batch to correspond with the previous iteration in order to extract generated smoothed labels, while the other half corresponds with the next iteration and provides soft targets for self-regularization. Different from above work, Yang et al.~\cite{yang2022recursivemix} propose RecursiveMix, which takes the prediction corresponding to the historical input as the target, and aligns the semantics of the ROI corresponding area in the current training input.}

%Wu et al.~\cite{wu2019mutual} cumulatively integrate the predictions based on the complementary correction network, producing more accurate forecasts.
\iffalse
\begin{figure*}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=0.98\textwidth]{./figs/RM.pdf}}
	\end{center}	
	\vspace{-10pt}
	\caption{
	The illustration of the proposed RecursiveMix. It leverages the historical input-prediction-label triplets. The historical input images are resized and then mixed into the current ones where the labels are fused proportionally (i.e., $\lambda$ and $1-\lambda$) to the area of the operated patches, formulating a recursive paradigm.
	}
	\label{fig_rm}
	\vspace{-10pt}
\end{figure*}
\fi





\begin{strip}
\centering
\phantomsection{\label{table_review}}
%\setlength{\LTcapwidth}{\textwidth}
\footnotesize
\setlength{\tabcolsep}{4.pt}
\vspace{0pt}
\renewcommand\arraystretch{1.2}
\topcaption{\label{table1}Review of the representative works which ``learn models with learning history'' from the aspects of Historical Type (what), Functional Part (where), and Storage Form (how). }
\vspace{-10pt}
\begin{xtabular}{l|l||c|c|c|c|c|c|c|c||c|c|c||c|c}
        %\hline
        %\multirow{2}{*}{Reference} & \multirow{2}{*}{Method} & \multicolumn{8}{c||}{Historical Type}                    & \multicolumn{3}{c||}{Functional Part}     & \multicolumn{2}{c}{Storage Form} \\
        %\cline{3-15}
        %&                                                    & $p$ & $\{x_i\}$ & $x_0$ & $y$ & $\{W_i\}$ & $\{\nabla_{x_i}\}$ & $\{\nabla_{W_i}\}$ & $l$     & Data & Model & Loss                       &\ \ DE\ \ & MA  \\
        %\hline
        %\endfirsthead
        \hline
        \multirow{2}{*}{Reference} & \multirow{2}{*}{Method} & \multicolumn{8}{c||}{Historical Type}                    & \multicolumn{3}{c||}{Functional Part}     & \multicolumn{2}{c}{Storage Form} \\
        \cline{3-15}
        &                                                    & $p$ & $\{x_i\}$ & $x_0$ & $y$ & $\{W_i\}$ & $\{\nabla_{x_i}\}$ & $\{\nabla_{W_i}\}$ & $l$     & Data & Model & Loss                       &\ \ DE\ \ & MA  \\
        %\endhead
        \hline
        \wugetable{CVPR22} & Shen et al.~\cite{shen2022self}  &  $\checkmark$ & &  &  &  & & & & & & $\checkmark$ &    $\checkmark$ &   \\ % Self-Distillation from the Last Mini-Batch for Consistency Regularization
        ICCV21 & Kim et al.~\cite{kim2021self}  &  $\checkmark$ & &  &  &  & & & & & & $\checkmark$ &    $\checkmark$ &   \\ % Self-Knowledge Distillation with Progressive Refinement of Targets
        \wugetable{AAAI20} & Chen et al.~\cite{chen2020semi} &  $\checkmark$ & &  &  &  & & & & & & $\checkmark$ &  &  $\checkmark$    \\ % Semi-supervised learning under class distribution mismatch
        \wugetable{AAAI20} & Yao et al.~\cite{yao2020deep} & $\checkmark$ &  &  & &   & & & &  & & $\checkmark$ &  &  $\checkmark$  \\ %Deep discriminative CNN with temporal ensembling for ambiguously-labeled image classification
        \wugetable{Arxiv19} & Nguyen et al.~\cite{nguyen2019self} & $\checkmark$ &  &  & &   & & & &  & & $\checkmark$ &  &  $\checkmark$  \\ %Self: Learning to filter noisy labels with self-ensembling
        ICLR17 & Laine et al.~\cite{laine2016temporal} & $\checkmark$ &  &  & &   & & & &  & & $\checkmark$ &  &  $\checkmark$  \\ %Temporal Ensembling for Semi-supervised Learning. In ICLR, 2017
        \hline
        %memory bank
        \wugetable {AAAI22} & Liu et al.~\cite{liu2022memory} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &   \\%Memory-Based Jitter: Improving Visual Recognition on Long Tailed Data with Diversity in Memory
        \wugetable {AAAI22 }& Wang et al.~\cite{wang2022contrastive} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &   \\ %Contrastive quantization with code memory for unsupervised image retrieval
        %head
        \wugetable {AAAI22} & Deng et al.~\cite{deng2022insclr} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ % InsCLR: Improving instance retrieval with self-supervision $tail$
        \wugetable{ CVPR22}&  Zhou et al.~\cite{zhou2022regional} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  $\checkmark$\\
        \wugetable{ CVPR22}& Liu et al.~\cite{liu2022learning} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Learning Memory-Augmented Unidirectional Metrics for Cross-Modality Person Re-Identification $tail$
       %\wugetable{ CVPR22}& Bogolin et al.~\cite{bogolin2022cross} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Cross Modal Retrieval with Querybank Normalisation $tail$
       \wugetable{CVPR22} & Yang et al.~\cite{yang2022cross} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Cross-Image Relational Knowledge Distillation for Semantic Segmentation
       \wugetable{CVPR22} & Li et al.~\cite{li2022recurrent} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Cross-Image Relational Knowledge Distillation for Semantic Segmentation
       \wugetable{ ECCV22} & Liu et al.~\cite{liu2022densely} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\% DAS: Densely-Anchored Sampling for Deep Metric Learning $tail$
       %\wugetable {Arxiv22}& Wu et al.~\cite{wu2022memorizing} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Memorizing transformers $tail$
      %\wugetable {Arxiv22}& Wang et al.~\cite{ wang2022learning} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Learning Equivariant Segmentation with Instance-Unique Querying $tail$
       \wugetable {Arxiv22}& Yang et al.~\cite{yang2022online} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition  $tail$       
       \wugetable {AAAI21} & Yin et al.~\cite{yin2021instance} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Instance mining with class feature banks for weakly supervised object detection $tail$
       \wugetable{CVPR21} & Zheng et al.~\cite{zheng2021group} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ & $\checkmark$  \\ % Group-aware Label Transfer for Domain Adaptive Person Re-identification
       \wugetable{ CVPR21} & Liu et al.~\cite{liu2021one} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\% Exploring classification equilibrium in long-tailed object detection $tail$
        \wugetable{ CVPR21} & Feng et al.~\cite{feng2021exploring} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\% Exploring classification equilibrium in long-tailed object detection $tail$
        \wugetable{ CVPR21} & Deng et al.~\cite{deng2021variational} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\% Variational prototype learning for deep face recognition $tail$
        %\wugetable{ CVPR21} & Zheng et al.~\cite{zheng2021group} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%Group-aware label transfer for domain adaptive person re-identification $tail$
        \wugetable{ CVPR21} & Jin et al.~\cite{jin2021mining} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Mining contextual information beyond image for semantic segmentation
       %tail
        %\wugetable{CVPR21} & Li et al.~\cite{li2021spatial} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Spatial assembly networks for image representation learning %check
        \wugetable{CVPR21} & Li et al.~\cite{li2021spatial}&  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ & $\checkmark$ \\ % Spatial Assembly Networks for Image Representation Learnin
        \wugetable{CVPR21} & Liu et al.~\cite{liu2021noise} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Noise-resistant deep metric learning with ranking-based instance selection
        \wugetable{ICCV21} & Ko et al.~\cite{ko2021learning}&  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &   \\ %Learning with memory-based virtual classes for deep metric learning
        \wugetable{ ICCV21} & Alonso et al.~\cite{alonso2021semi} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\% Exploring classification equilibrium in long-tailed object detection $tail$
       \wugetable{ ICCV21} & Lee et al.~\cite{lee2021weakly} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\% Exploring classification equilibrium in long-tailed object detection $tail$
       \wugetable{ICCV21} & Feng et al.~\cite{feng2021exploring} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ % Exploring classification equilibrium in long-tailed object detection
        \wugetable{ NeurIPS21} & Wen et al.~\cite{wen2021false} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\%When False Positive is Intolerant: End-to-End Optimization with Low FPR for Multipartite Ranking
        \wugetable{TIP21} & Dai et al.~\cite{dai2021dual} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification
        CVPR20 & Wang et al.~\cite{wang2020cross} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ % Cross-Batch Memory for Embedding Learning
        \wugetable{CVPR20} &Misra et al.~\cite{misra2020self} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Self-supervised learning of pretext-invariant representations
        \wugetable{ECCV20} & Li et al.~\cite{li2020unsupervised} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ % Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss
        \wugetable{ECCV20} & Tian et al.~\cite{tian2020contrastive} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Contrastive multiview coding
        CVPR19 & Zhong et al.~\cite{zhong2019invariance} &  & $\checkmark$ &  & &  & & &  &  &  & $\checkmark$ &  & $\checkmark$ \\  
        \wugetable{ICCV19} & Zhuang et al.~\cite{zhuang2019local}  &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &$\checkmark$  \\
         &                                                    & $p$ & $\{x_i\}$ & $x_0$ & $y$ & $\{W_i\}$ & $\{\nabla_{x_i}\}$ & $\{\nabla_{W_i}\}$ & $l$     & Data & Model & Loss                       &\ \ DE\ \ & MA  \\\hline
        \wugetable{TPAMI19} & Yang et al.~\cite{yang2019visual} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Contrastive multiview coding
%Zhong_Invariance_Matters_Exemplar_Memory_for_Domain_Adaptive_Person_Re-Identification_CVPR_2019
        %CVPR19 & Wu et al.~\cite{} &  & $\checkmark$ &  & & &  & $\checkmark$ & & &  $\checkmark$ &  \\ % Long-Term Feature Banks for Detailed Video Understanding
        CVPR18 & Wu et al.~\cite{wu2018unsupervised}  &  & $\checkmark$ &  & & &  & & & $\checkmark$ &  & $\checkmark$ & $\checkmark$ &  \\  %Unsupervised Feature Learning via Non-Parametric Instance Discrimination
        ECCV18 & Wu et al.~\cite{wu2018improving} &  & $\checkmark$  &   & & & & & & $\checkmark$ &  & $\checkmark$ &   & $\checkmark$  \\ %Improving Generalization via Scalable Neighborhood Component Analysis
        \wugetable{ECCV18} & Yang et al.~\cite{yang2018learning} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ %Contrastive multiview coding
        \wugetable{ICLR18} & Sprechmann et al.~\cite{sprechmann2018memory}  &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &   \\  %Memory-based Parameter Adaptation
        \wugetable{CVPR17} & Xiao et al.~\cite{xiao2017joint} &  & $\checkmark$ &  & &  &  & & & $\checkmark$ & & $\checkmark$ &  $\checkmark$ &  \\ % Joint detection and identification feature learning for person search
        %center loss
        \wugetable{CVPR21}  & Li et al.~\cite{li2021frequency} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
        \wugetable{CVPR21}  & Jing et al.~\cite{jing2021cross} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
         \wugetable{ICCV21 } & Chen et al.~\cite{chen2021free} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
        \wugetable{ CVPR20}  & Keshari et al.~\cite{keshari2020generalized} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
        \wugetable{ECCV20}  & Min et al.~\cite{min2020adversarial} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ % A Discriminative Feature Learning Approach for Deep Face Recognition
        %\wugetable{Neu. Com.20}  & Zhu et al.~\cite{zhu2020hetero} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
        \wugetable{TPAMI20}  & Liu et al.~\cite{liu2020leveraging} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\
        \wugetable{TMM20}  & Zhao et al.~\cite{zhao2020deep} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
        \wugetable{ AAAI19} & Li et al.~\cite{li2019angular} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
       \wugetable{ ICCV19}  & Narayan et al.~\cite{narayan20193c} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\
        \wugetable{CVPR18} & He et al.~\cite{he2018triplet} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\ 
        \wugetable{ECCV18} & Fan et al.~\cite{fan2018associating} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\
        ECCV16 & Wen et al.~\cite{wen2016discriminative} &  & $\checkmark$ &  & &  & & & & $\checkmark$  &  & $\checkmark$ &  & $\checkmark$   \\
        %bn
%        &                                                    & $p$ & $\{x_i\}$ & $x_0$ & $y$ & $\{W_i\}$ & $\{\nabla_{x_i}\}$ & $\{\nabla_{W_i}\}$ & $l$     & Data & Model & Loss                       &\ \ DE\ \ & MA  \\
        \hline  
        \wugetable{ICLR17} & Li et al.~\cite{li2016revisiting} &  & $\checkmark$  &  & &   & & & &  & $\checkmark$ & & &  $\checkmark$  \\
        \wugetable{NeurIP17} & Ioffe et al.~\cite{ioffe2017batch} &  & $\checkmark$  &  & &   & & & &  & $\checkmark$ & & &  $\checkmark$  \\     
        \wugetable{ICML16} & Arpit et al.~\cite{arpit2016normalization} &  & $\checkmark$  &  & &   & & & &  & $\checkmark$ & & &  $\checkmark$  \\ 
        \wugetable{NeurIP16} & Salimans et al.~\cite{salimans2016weight} &  & $\checkmark$  &  & &   & & & &  & $\checkmark$ & & &  $\checkmark$  \\ 
        ICML15 & Ioffe et al.~\cite{ioffe2015batch} &  & $\checkmark$  &  & &   & & & &  & $\checkmark$ & & &  $\checkmark$  \\  %Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
        \hline
            \wugetable{CVPR22} & Wang et al.~\cite{wang2022semi} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ %Cross-Image Relational Knowledge Distillation for Semantic Segmentation
       %\wugetable{ ECCV22} & Byun et al.~\cite{byun2022grit} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$  \\% GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training
       \wugetable{ACM MM21} & Chen et al.~\cite{chen2021transferrable}&  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$   \\ %Transferrable contrastive learning for visual domain adaptation
       \wugetable{ACM MM21} & Luo et al.~\cite{luo2021coco} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$  \\ %Coco-bert:Improving video-language pre-training with contrastive cross-modalmatching and denoising
        %\wugetable{CVPR21} & Li et al.~\cite{li2021spatial} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$  \\ % Spatial Assembly Networks for Image Representation Learning
       %\wugetable{CVPR21} & Zheng et al.~\cite{zheng2021group} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$   \\ % Group-aware Label Transfer for Domain Adaptive Person Re-identification
       \wugetable{CVPR21} & Xie et al.~\cite{xie2021propagate} &  & $\checkmark$ & & & $\checkmark$ & &   & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning
       %\wugetable{CVPR21} & Xie et al.~\cite{xie2021propagate} &  & $\checkmark$ & & & $\checkmark$ & &   & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning
       ICCV21 & Caron et al.~\cite{caron2021emerging} &  & $\checkmark$ &  & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & & $\checkmark$   \\ % Emerging Properties in Self-Supervised Vision Transformers
        %\wugetable{ICCV21} & Ko et al.~\cite{ko2021learning}&  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$   \\ %Learning with memory-based virtual classes for deep metric learning
        \wugetable{ICCV21} & Van et al.~\cite{van2021unsupervised} &  & $\checkmark$ & & & $\checkmark$ & &   & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals
        \wugetable{ICCV21} & Ayush et al.~\cite{ayush2021geography} &  & $\checkmark$ &  & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ &$\checkmark$ & $\checkmark$ \\ %Geography-Aware Self-Supervised Learning
         \wugetable{ICCV21} & Xie et al.~\cite{xie2021detco} &  & $\checkmark$ &  & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ &$\checkmark$ & $\checkmark$  \\ %PROTOTYPICAL CONTRASTIVE LEARNING OF UNSUPERVISED REPRESENTATIONS
       \wugetable{ICLR21} & Li et al.~\cite{li2020prototypical} &  & $\checkmark$ &  & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ &$\checkmark$ & $\checkmark$  \\ %PROTOTYPICAL CONTRASTIVE LEARNING OF UNSUPERVISED REPRESENTATIONS
       \wugetable{Arxiv21} & El et al.~\cite{el2021training}  &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$   \\ % Training Vision Transformers for Image Retrieval
        CVPR20 & He et al.~\cite{he2020momentum} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Momentum contrast for unsupervised visual representation learning
        \wugetable{NeurIPS20} & Chen et al.~\cite{chen2020big} &  & $\checkmark$ & & & $\checkmark$ & &   & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Big Self-Supervised Models are Strong Semi-Supervised Learners
        Arxiv20 & Chen et al.~\cite{chen2020improved} &  & $\checkmark$ & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Improved baselines with momentum contrastive learning
        %\wugetable{CVPR19} & Zhong et al.~\cite{zhong2019invariance} &  & $\checkmark$ & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$   \\ % Invariance matters: Exemplar memory for domain adaptive person re-identification
        %\wugetable{ICCV19} & Zhuang et al.~\cite{zhuang2019local} &  & $\checkmark$ &  & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ &$\checkmark$ & $\checkmark$  \\ %PROTOTYPICAL CONTRASTIVE LEARNING OF UNSUPERVISED REPRESENTATIONS
        \hline
        %wang2021multi, kim2022pose,, meng2021spatial, wang2021self
        
        %model param1
        \wugetable{ Arxiv23} & Zhang et al.~\cite{zhang2023robust} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
        \wugetable{ AAAI22 } & Zhang et al.~\cite{zhang2022semi} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models
        \wugetable{ CVPR22} & Liu et al.~\cite{liu2022perturbed} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
        \wugetable{ CVPR22} & Liu et al.~\cite{liu2022unbiased } &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
        \wugetable{ ECCV22} & Wang et al.~\cite{wang2022unsupervised} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models
        %\wugetable{ICLR22} & Li et al.~\cite{li2021efficient} &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % EFFICIENT SELF-SUPERVISED VISION TRANSFORMERS FOR REPRESENTATION LEARNING
        \wugetable{ICML22} & Wang et al.~\cite{wang2022self} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ &  &  $\checkmark$   \\ % Self-ensemble adversarial training for improved robustness
        \wugetable{NeurIPS22} & Wang et al.~\cite{wang2022efficient} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ &    \\ % Effcient knowledge distillation from model checkpoints
        \wugetable{Arxiv22} & Wang et al.~\cite{wang2022learn} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ &   \\ % Learn from thepast: Experience ensemble knowledge distillation
        \wugetable{Arxiv22} & Tomasev et al.~\cite{tomasev2022pushing} &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % REPRESENTATION LEARNING VIA INVARIANT CAUSAL MECHANISMS
        \wugetable{ Arxiv22} & Cai et al.~\cite{cai2022semi} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models
        \wugetable{ Arxiv22} & Kim et al.~\cite{kim2022pose} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
        \wugetable{ CVPR21} & Cai et al.~\cite{cai2021exponential} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
         \wugetable{ CVPR21} & Wang et al.~\cite{wang2021self} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
         \wugetable{ CVPR21} & Yang et al.~\cite{yang2021interactive } &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
         \wugetable{ CVPR21} & Deng et al.~\cite{deng2021unbiased } &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
         &                                                    & $p$ & $\{x_i\}$ & $x_0$ & $y$ & $\{W_i\}$ & $\{\nabla_{x_i}\}$ & $\{\nabla_{W_i}\}$ & $l$     & Data & Model & Loss                       &\ \ DE\ \ & MA  \\\hline
        ICCV21 & Feng et al.~\cite{feng2021temporal} &  &  & & & $\checkmark$ & &   & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Temporal Knowledge Consistency for Unsupervised Visual Representation Learning
        ICCV21 & Chen et al.~\cite{chen2021empirical} &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % An Empirical Study of Training Self-Supervised Visual Transformers
        \wugetable{ ICCV21} & Meng et al.~\cite{meng2021spatial} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
         \wugetable{ ICCV21} & Wang et al.~\cite{wang2021multi} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
         ICCV21 & Xu et al.~\cite{xu2021end} &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % An Empirical Study of Training Self-Supervised Visual Transformers
        \wugetable{ICLR21} & Mitrovic et al.~\cite{mitrovic2020representation} &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % REPRESENTATION LEARNING VIA INVARIANT CAUSAL MECHANISMS
        \wugetable{Arxiv21} & Liu et al.~\cite{liu2021unbiased} &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % REPRESENTATION LEARNING VIA INVARIANT CAUSAL MECHANISMS
        CVPR20 & Yuan et al.~\cite{yuan2020revisiting} &  &  & & & $\checkmark$ & & &  &  & $\checkmark$ & $\checkmark$ & $\checkmark$ & \\ % Revisiting Knowledge Distillation via Label Smoothing Regularization
       %CVPR20 & Xie et al.~\cite{xie2020self} &  &  & & & $\checkmark$ & &   & & & $\checkmark$ & $\checkmark$ & $\checkmark$ &  \\ % Self-training with Noisy Student improves ImageNet classification
        NeurIPS20 & Grill et al.~\cite{grill2020bootstrap} &  &  & & & $\checkmark$  &  & &  &  & $\checkmark$ & $\checkmark$ & & $\checkmark$  \\ % Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
        CVPR19 & Li et al.~\cite{li2019learning}  &  &  & & & $\checkmark$ &   & & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\ % Learning to Learn from Noisy Labeled Data
        CVPR19 & Yang et al.~\cite{yang2019snapshot} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ &    \\ % Snapshot Distillation： Teacher-Student Optimization in One Generation
        \wugetable{CVPR19} &Cai et al.~\cite{cai2019exploring } &  &  & & & $\checkmark$ &  & &  & & $\checkmark$ & $\checkmark$ & & $\checkmark$ \\ % REPRESENTATION LEARNING VIA INVARIANT CAUSAL MECHANISMS
        %\wugetable{ IPMI19} & Cui et al.~\cite{cui2019semi} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Uncertainty-awareself-ensembling model for semi-supervised 3d left atrium segmention
         %\wugetable{ MICCAI19} & Yu et al.~\cite{yu2019uncertainty} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Uncertainty-awareself-ensembling model for semi-supervised 3d left atrium segmention
         DLMIA18 & Perone et al.~\cite{perone2018deep} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models
        \wugetable{ ECCV18} & Shi et al.~\cite{shi2018transductive} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
        ICML18 & Furlanello et al.~\cite{furlanello2018born} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & $\checkmark$ & $\checkmark$ &   \\ % Born-Again Neural Networks
        NeurIPS17 & Tarvainen et al.~\cite{tarvainen2017mean} &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & $\checkmark$  & &  $\checkmark$  \\ % Mean teachers are better role models:
        
        %model param2
%        \wugetable{WCVPR22} & Gong et al.~\cite{gong2022uncertainty} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        \wugetable{CVPR20} & Poggi et al.~\cite{poggi2020uncertainty} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        \wugetable{NeurIPS20} & Liu et al.~\cite{liu2020loss} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        %\wugetable{PR20} & Xu et al.~\cite{xu2020multi} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREEE
        \wugetable{CVPR19} & Yu et al.~\cite{yu2019deep} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
         \wugetable{ECCV18} & Ilg et al.~\cite{ilg2018uncertainty} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        %\wugetable{TSG18} & Chen et al.~\cite{chen2018short} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        ICLR17 & Huang et al.~\cite{huang2017snapshot} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        %\wugetable{MICCAI17} & Wolterink et al.~\cite{wolterink2018automatic} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &    \\ %SNAPSHOT ENSEMBLES: TRAIN 1, GET M FOR FREE
        \wugetable{Arxiv17} & Chen et al.~\cite{chen2017checkpoint} &  &  & & & $\checkmark$ & &  & & & $\checkmark$ & & $\checkmark$ &\\ %Checkpoint Ensembles: Ensemble Methods from a Single Training Process

        %model param3
        \wugetable{Arxiv22} & Guo et al.~\cite{guo2022stochastic} &  &  & & & $\checkmark$ & & &  &  & $\checkmark$ & & & $\checkmark$\\% Stochastic Weight Averaging Revisited
        \wugetable{ICML21} & Von et al.~\cite{von2020neural} &  &  & & & $\checkmark$ & &   & & & $\checkmark$ & &  & $\checkmark$ \\ % Neural networks with late-phase weights
        \wugetable{NeurIPS21} & Cha et al.~\cite{cha2021swad} &  &  & & & $\checkmark$ & & &  &  & $\checkmark$ & & & $\checkmark$\\% SWAD: Domain generalization by seeking flat minima
        %&                                                    & $p$ & $\{x_i\}$ & $x_0$ & $y$ & $\{W_i\}$ & $\{\nabla_{x_i}\}$ & $\{\nabla_{W_i}\}$ & $l$     & Data & Model & Loss                       &\ \ DE\ \ & MA  \\
        \hline
        Arxiv20 & Zhang et al.~\cite{zhang2020swa}  &  &  & & & $\checkmark$ &  & & &  & $\checkmark$ & &  & $\checkmark$ \\% Swa object detection
        ICLR19 & Athiwaratkun et al.~\cite{athiwaratkun2018there} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & & & $\checkmark$\\% There are many consistent explanations of unlabeled data: Why you should average
        \wugetable{ICML19} & Yang et al.~\cite{yang2019swalp} &  &  & & & $\checkmark$ & & &  &  & $\checkmark$ & & & $\checkmark$\\% SWALP: Stochastic Weight Averaging in Low-Precision Training
         NeurIPS19 & Maddox et al.~\cite{maddox2019simple} &  &  & & & $\checkmark$ & &  & &  & $\checkmark$ & & & $\checkmark$\\ % A Simple Baseline for Bayesian Uncertainty in Deep Learning
         NeurIPS18 & Garipov et al.~\cite{garipov2018loss} &  &  & & &  $\checkmark$ & &  & & & $\checkmark$ & & & $\checkmark$\\% Loss surfaces, mode connectivity, and fast ensembling of dnns.
        %UAI18 & Izmailov et al.~\cite{izmailov2018averaging} &  &  & & & $\checkmark$ & & &  &  & $\checkmark$ & & & $\checkmark$\\% Averaging Weights Leads to Wider Optima and Better Generalization
        \hline
		Arxiv22 & Li et al.~\cite{li2022equalized} &  &  &  & & &$\checkmark$ & & & &  & $\checkmark$ &  $\checkmark$ &   \\ %Equalized Focal Loss for Dense Long-Tailed Object Detection
         CVPR21 & Tan et al.~\cite{tan2021equalization} &  &  &  & & &$\checkmark$ & & & &  & $\checkmark$ & $\checkmark$  &   \\ %Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection
        \wugetable{ECCV20} & Kwon et al.~\cite{kwon2020backpropagated} &  &  &  & & &$\checkmark$ & & & &  & $\checkmark$ &     &$\checkmark$  \\%Backpropgated gradient representations for anomaly detection
        \hline
        %TODO momentum sgd, Adam, AdamW ....
		\wugetable{ NeurIPS22} & Xie et al.~\cite{xie2022adan} &  &  &  & & & &  $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$  \\ % Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models
		\wugetable{ ICLR21} & Heo et al.~\cite{heo2020adamp} &  &  &  & & & &  $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$  \\ % Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights
        ICLR20 & Liu et al.~\cite{liu2019variance} &  &  &  & & & &  $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$  \\ % RAdam ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND
       \wugetable{ IJCAI20} & Chen et al.~\cite{chen2018closing} &  &  &  & & & &  $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$  \\ % Closing the generalization gap of adaptive gradient methods in training deep neural networks
       ICLR19 & Loshchilov et al.~\cite{loshchilov2017decoupled} &  &  & & &  &  & $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$  \\ % Decoupled Weight Decay Regularization ADAMW
       Arxiv19 & Ding et al.~\cite{ding2019adaptive} &  &  & & &  &  & $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$  \\ % Decoupled Weight Decay Regularization ADAMW
        \wugetable{ICLR16} & Dozat et al.~\cite{dozat2016incorporating} &  &  & & &  &  & $\checkmark$ & & & $\checkmark$ &  &   & $\checkmark$ \\ % Incorporating nesterov momentum into adam
        ICLR15 & Kingma et al.~\cite{kingma2014adam}  &  &  & & &  &   & $\checkmark$ & & &$\checkmark$ &   & & $\checkmark$ \\ % ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION
        ICML13 & Sutskever et al.~\cite{sutskever2013importance} &  &  & & & &   & $\checkmark$&  & &$\checkmark$ &   & & $\checkmark$ \\ % On the importance of initialization and momentum in deep learning, (nesterov) momentum sgd
        %Arxiv12 & Zeiler~\cite{zeiler2012adadelta} &  &  & & & &   & $\checkmark$ & & &$\checkmark$ &  &  & $\checkmark$\\ %ADADELTA: AN ADAPTIVE LEARNING RATE METHOD 
        JMLR11 & Duchi et al.~\cite{duchi2011adaptive} &  &  & & &  &  & $\checkmark$ & & &$\checkmark$ &  &   & $\checkmark$\\ % Adagrad  Adaptive Subgradient Methods for Online Learning and Stochastic Optimization
        \hline
        \wugetable{AAAI22} & Jiang et al.~\cite{jiang2021delving} &  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$&  \\%Delving into sample loss curve to embrace noisy and imbalanced data
        \wugetable{ Arxiv22} & Yue et al.~\cite{yue2022ctrl}  &  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$&  $\checkmark$  \\%Ctrl: Clustering training losses for label error detection
        \wugetable{ECCV20} & Chu et al.~\cite{chu2020neural}  &  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$& \\%Neural batch sampling with reinforcement learning for semi-supervised anomaly detection%&  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  &   $\checkmark$& \\
         \wugetable{ECML21} &Xu et al.~\cite{xu2021small} &  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$& $\checkmark$  \\ 
         \wugetable{ICCV21} & Lazarou et al.~\cite{lazarou2021iterative}&  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$&   $\checkmark$ \\%Iterative label cleaning for transductive and semi-supervised few-shot learning
         \wugetable{Arxiv21} & Xia et al.~\cite{xia2021sample}&  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$& $\checkmark$   \\%Sample selection with uncertainty of losses for learning with noisy labels
         \wugetable{CVPR19} & Huang et al.~\cite{huang2019o2u} &  &  & & &  &  &  & $\checkmark$& $\checkmark$& &  $\checkmark$ &  $\checkmark$&  $\checkmark$  \\ % O2u-net: A simple noisy label detection approach for deep neural networks
        \hline
        NeurIPS22 & Yang et al.~\cite{yang2022recursivemix} &  $\checkmark$  &  &  $\checkmark$ & $\checkmark$ &  & &  & & $\checkmark$  &  & $\checkmark$ & $\checkmark$ &   \\%\multicolumn{2}{c||}{ This Work (Ours)} &    &  &  $\checkmark$ & $\checkmark$ &  &  & $\checkmark$  &  & $\checkmark$ & $\checkmark$ &   \\
        \hline
\end{xtabular}
%\vspace{-10pt}
\end{strip}



%\end{multicols}

\noindent\textbf{Intermediate Feature Representation.} There are mainly two streams  and the specific structure comparison is shown in Fig.~\ref{fig_intermediate_feature_representation}.

{
% (1) One stream is to directly record the instance-level feature representations, usually aiming at constructing sufficient and critical training pairs 
\ylfChecked{(1) One stream is to record the instance-level feature representations aiming at constructing sufficient and critical training pairs}
(see Fig.~\ref{fig_intermediate_feature_representation} (a)). 
%This method can be further categorized into three categories based on their storage scope, content, and update method: (a) store some instance representations~\cite{sprechmann2018memory,wang2020cross}. (b) store all image or class representations~\cite{wu2018improving,wu2018unsupervised}. (c) store some instance representations updated by momentum~\cite{he2020momentum,chen2020improved}. 
The common practices of obtaining feature representations via computing the current mini-batch lack diversity and efficiency. 
Sprechmann et al.~\cite{sprechmann2018memory} introduce an episodic memory module to store examples and modify network parameters based on the current input context. Wang et al.~\cite{wang2020cross} extend this concept with the introduction of a cross-batch memory (XBM) that records the instance embeddings of past iterations, allowing the algorithm to gather more hard negative pairs. 
Influenced by XBM, some work has embedded the memory bank into the network to adapt various tasks~\cite{deng2022insclr, yang2022online, deng2021variational, li2020unsupervised,feng2021exploring, liu2022memory,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,yang2018learning, yang2019visual,el2021training,liu2022densely,wang2022contrastive,wen2021false,liu2022learning,liu2021noise,ortego2021multi,jin2021mining,yang2022cross,zhong2019invariance}. To enhance the diversity of semantic differences, Liu et al.~\cite{liu2022densely} construct a memory bank using historical intra-class embedding representations. Motivated by the issue of slow feature drift, Wang et al.~\cite{wang2022contrastive} propose the quantization code memory bank to lower feature drift to use historical feature representation effectively. Wen et al.~\cite{wen2021false} offer a delayed update memory bank that ranks instances in the current mini-batch for machine learning multi-partite ranking tasks. Liu et al.~\cite{liu2022learning} also employ slow feature drift to accumulate the historical modality-specific proxies (MS-Proxies) stored by the memory bank, thereby enhancing the diversity of these MS-Proxies. 
% However, storing instances or features in memory banks may be affected by noise labels~\cite{liu2021noise,ortego2021multi}. 
\ylfChecked{Methods have also been proposed to address the issue of label noise that arises from storing instances or features in memory banks~\cite{liu2021noise,ortego2021multi}.}
% Liu et al.~\cite{liu2021noise} take a different approach by calculating the probability of clean labels and choosing high scores to extract features into the memory bank, resulting in having higher quality historical representation and improving noise resistance. 
\ylfChecked{Liu et al.~\cite{liu2021noise} take a different approach by calculating the probability of having clean labels and choosing high-scoring ones to extract features into the memory bank, resulting in producing superior historical representation and increasing resistance to noise.}
Some works skillfully combine the memory banks with specific downstream tasks, which promote the performance of the model. 
% In the field of semantic image segmentation, Jin et al.~\cite{jin2021mining} use a memory bank to store different class representations, while Yang et al.~\cite{yang2022cross} inspired by~\cite{fang2021seed} employ a pixel queue and a region queue to construct the memory bank, which utilizes historical feature representation to strengthen global semantic relations. The pixel queue models pixel dependencies, and the region queue models relations between pixels and class region embeddings. 
\ylfChecked{
In the field of semantic image segmentation, Jin et al.~\cite{jin2021mining} use a memory bank to store different class representations. Yang et al.~\cite{yang2022cross} employ a pixel queue and a region queue to construct the memory bank for modeling pixel dependencies and feature relations between pixels and class region, respectively. 
The work enhances global semantic relations through historical feature representation.}
In domain adaptation, Zhong et al.~\cite{zhong2019invariance} use a memory bank to store the feature maps of unlabeled data from the target domain and apply an exemplar-invariance objective from the source domain to bridge the gap.}

%\wugerewrite{The memory bank is divided into three types according to its storage scope, content, and update method: (a) partial instance representations updated by forward propagation~\cite{sprechmann2018memory,wang2020cross,li2020unsupervised,feng2021exploring,wen2021false,liu2021noise,ortego2021multi}. (b) all image representations updated by forward propagation~\cite{wu2018improving,wu2018unsupervised,dai2021dual}. (c) partial instance representations updated by momentum forward propagation~\cite{he2020momentum,chen2020improved,li2021spatial, el2021training, zheng2021group,fang2021seed,ko2021learning,liu2022memory,jin2021mining,yang2022cross}.} To address the deficiency, 
%\wuge{Sprechmann et al.~\cite{ sprechmann2018memory } deposit examples in the episodic memory module and then exploit the current input context to modify the network’s parameters. Following that, Wang et al.~\cite{wang2020cross} propose a cross-batch memory (XBM) that records the instance embeddings of past iterations, allowing the algorithm to collect adequate hard negative pairs. Later, XBM is extended to unsupervised learning by~\cite{li2020unsupervised}, letting the memory bank visit all training data rather than only the samples in the current mini-batch. Besides, there is the same question in long-tailed detection. Feng et al.~\cite{ feng2021exploring } propose Memory-augmented Feature Sampling (MFS) to first store various instance features and then sample more weak classes’ instance features. }
%{Sprechmann et al.~\cite{ sprechmann2018memory } propose using the episodic memory module to store examples and exploit the current input context to modify the network's parameters. Wang et al.~\cite{wang2020cross} extend this concept with the introduction of a cross-batch memory (XBM) that records the instance embeddings of past iterations, allowing the algorithm to gather more hard negative pairs. Li et al.~\cite{li2020unsupervised} further extend the use of XBM to unsupervised learning, allowing the memory bank to access all training data rather than just samples in the current mini-batch. Additionally, Feng et al.~\cite{ feng2021exploring } address the issue of long-tailed detection through the proposal of Memory-augmented Feature Sampling (MFS), which involves storing iterative instance features and sampling more weak class instance features.}
%\wuge{Motivated by the slow feature drift issue, Wen et al.~\cite{wen2021false} delayed update memory bank to rank the instances in the current mini-batch, which aims to solve the multipartite ranking task in machine learning. n addition, memory banks that store instances or features may be affected by noise labels~\cite{ liu2021noise,ortego2021multi}. Ortego et al.~\cite{ortego2021multi} take the memory bank to store feature representations, then compute the contrast loss between minibatch samples and memory samples. Further, Liu et al.~\cite{liu2021noise} calculate the probability of clean labels, then choose the high scores to extract features into the memory bank. Better noise resistance performance may result from this. To strengthen the within-class diversity of the tail classes, Liu et al.~\cite{liu2022memory} store and accumulate the different features and parameters of the tail sample and tail class on each training iteration, which attempts to re-sampling between head and tail classes. In semantic image segmentation, Jin et al.~\cite{jin2021mining} take the memory bank to store the different classes’ representations. Motivated by~\cite{fang2021seed}, Yang et al.~\cite{yang2022cross} employ a pixel queue and a region queue to construct the memory bank to strengthen global semantic relations. The pixel queue models dependencies among pixels, while the region queue models the relations between pixels and class region embeddings. }
%{Motivated by the issue of slow feature drift, Wen et al.~\cite{wen2021false} propose a delayed update memory bank that ranks instances in the current mini-batch to solve the multi-partite ranking task in machine learning. However, memory banks that store instances or features may be affected by noise labels~\cite{ liu2021noise,ortego2021multi}. Ortego et al.~\cite{ortego2021multi} address this issue by storing historical feature representations in the memory bank and computing the contrast loss between mini-batch samples and memory samples. Liu et al.~\cite{liu2021noise} take a different approach by calculating the probability of clean labels and choosing high scores to extract features into the memory bank, resulting in improved noise resistance. To increase the within-class diversity of tail classes, Liu et al.~\cite{liu2022memory} propose storing and accumulating the different features and parameters of tail samples and tail classes by memory bank on each training iteration, effectively re-sampling between head and tail classes. In the field of semantic image segmentation, Jin et al.~\cite{jin2021mining} use a memory bank to store different class representations, while Yang et al.~\cite{yang2022cross}, inspired by Fang et al.~\cite{fang2021seed}, employ a pixel queue and a region queue to construct the memory bank in order to strengthen global semantic relations. The pixel queue models dependencies among pixels, and the region queue models the relations between pixels and class region embeddings.}
\begin{figure}[htp]
	\vspace{0pt}
	%\begin{center}
		\setlength{\fboxrule}{0pt}
%\fbox{\includegraphics[width=0.47\textwidth,height=0.8\textheight,\fbox{\includegraphics[width=0.46\textwidth]
%\fbox{\includegraphics[width=0.48\textwidth,height=0.91\textheight,keepaspectratio]
\fbox{\includegraphics[width=0.5\textwidth,height=0.9\textheight,keepaspectratio] %,height=0.91\textheight
%\fbox{\includegraphics[width=0.5\textwidth,height=1\textheight,keepaspectratio]
%\fbox{\includegraphics[width=0.5\textwidth]
{./figs/timeline_single.pdf}}
	%\end{center}	
	\vspace{0pt}
	\caption{\textbf{The timeline of the works related to historical learning}. Different colors represent different historical information or types involved. For example, the red color is related to past predictions, while the light orange involves historical predictions, data, and labels. It demonstrates the tendency of using different historical types.}
	\label{fig_timeline}
	\vspace{0pt}
\end{figure}%\fbox{\includegraphics[width=0.98\textwidth]{./figs/timelines.pdf}}
{Different from storing some instance representations~\cite{wang2020cross}, Wu et al.~\cite{wu2018improving,wu2018unsupervised} develop the memory bank mechanism to memorize the representations for all images to perform Neighborhood Component Analysis (NCA)~\cite{wu2018improving} or Noise-Contrastive Estimation (NCE)~\cite{wu2018unsupervised} optimization, instead of exhaustively computing these embeddings every time. Following~\cite{ wu2018unsupervised}, 
some works~\cite{ misra2020self, tian2020contrastive, yin2021instance, dai2021dual,wang2022semi,ko2021learning} also employ the memory bank to store all image or class representations. Yin et al.~\cite{yin2021instance} propose Class Feature Banks (CFB), which use historical feature representation to record and update each class's diverse representation. Notably, previous works on memory bank~\cite{wu2018improving,wu2018unsupervised} may not be suitable for online feature learning, as the stored features in the memory bank cannot be updated through the back-propagation of gradients. To address this, Dai et al.~\cite{dai2021dual} propose an immediate memory bank that stores instant features of all the samples and updates them regularly. However, these methods~\cite{wang2020cross,wen2021false} primarily focus on categories that have already occurred and have a weaker impact on categories that have not yet appeared during testing. To address this issue, Ko et al.~\cite{ko2021learning} propose storing both embedding features and category weights in the memory bank to reduce the emphasis on seen classes. 
}
%{Similarly, Wu et al.~\cite{wu2018improving,wu2018unsupervised} develop the memory bank mechanism to memorize the representations for all images to perform Neighborhood Component Analysis (NCA)~\cite{wu2018improving} or Noise-Contrastive Estimation (NCE)~\cite{wu2018unsupervised} optimization, instead of exhaustively computing these embeddings every time. Following the~\cite{ wu2018unsupervised },~\cite{misra2020self} and~\cite{ tian2020contrastive } also employ the memory bank.}
%\wuge{However, previous works ~\cite{wu2018improving,wu2018unsupervised} might not be appropriate for online feature learning. Because the stored feature of the memory bank can’t be updated by back-propagating gradients. Dai et al.~\cite{dai 2021dual} propose an immediate memory bank that stores instant features and is renewed on time. Shen et al. ~\cite{shen2022self} take half of each mini-batch to correspond with the previous iteration to extract generated smoothed labels. The rest will coincide with the next iteration, providing soft targets to regularize itself.}
%{However, previous works~\cite{wu2018improving,wu2018unsupervised} may not be suitable for online feature learning, as the stored features in the memory bank cannot be updated through the back-propagation of gradients. To address this issue, Dai et al.~\cite{dai2021dual} propose an immediate memory bank that stores instant feature updates regularly.}
%\wuge{ He et al.~\cite{he2020momentum} and Chen et al.~\cite{chen2020improved} implement the memory dictionary named momentum memory bank as a queue of data samples where the features are progressively replaced. Some work~\cite{li2021spatial, el2021training, zheng2021group} is improved based on the two of them. Li et al. \cite{li2021spatial} use the momentum memory bank to design the novel spatial assembly network, which could learn different spatial variations’ features. El et al. \cite{el2021training} follow~\cite{ wang2020cross, he2020momentum} to use offline memory bank and momentum encoder to improve the performance of image retrieval baselines. In addition, Weighted contrastive loss is introduced by zheng2021group et al. to calculate the similarity of characteristics among mini-batch and memory bank, which expedites the optimization of the model. Moreover, Fang et al.~\cite{fang2021seed} take a shared memory bank between the teacher and student model, storing the encoding output of the teacher model. But these methods~\cite{he2020momentum, chen2020improved }primarily concentrate on those that have already occurred and have a poorer impact on categories that have not yet emerged during testing. Therefore, Ko et al.~\cite{ko2021learning} deposit both embedding features and category weights in order to reduce the emphasis on seen classes. Differently in domain adaptation, Zhong et al.~\cite{zhong2019invariance} store the feature maps of \emph{unlabeled data} from the target domain, introducing the exemplar-invariance objective with the source domain to bridge the domain gap.  Further,  Chen et al.~\cite{chen2021transferrable} employ source memory and target memory to record the class characteristics of the source domain and the pseudo-label characteristics of the target domain, respectively. }

It is noteworthy that these representations~\cite{wang2020cross} will not be updated with momentum encoder before inputting into the memory bank. In contrast, He et al.~\cite{he2020momentum} and Chen et al.~\cite{chen2020improved} introduce the concept of the momentum memory bank, a queue of data samples in which features are progressively replaced and updated by momentum. Some subsequent work~\cite{li2021spatial, el2021training ,chen2021transferrable , zhou2022regional, luo2021coco,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography, xie2021detco,zheng2021group,zhuang2019local} has built upon these ideas. Li et al.~\cite{li2021spatial} use the momentum memory bank to design a novel spatial assembly network, which can learn features of different spatial variations.
Following~\cite{wang2020cross, he2020momentum}, El et al.~\cite{el2021training} use an offline memory bank and a momentum encoder to improve the performance of image retrieval baselines. In addition, Zheng et al.~\cite{zheng2021group} introduce a weighted contrastive loss to calculate the similarity of characteristics between the mini-batch and memory bank, which speeds up the optimization of the model. 
%However, these methods~\cite{he2020momentum, chen2020improved} primarily focus on categories that have already occurred and have a weaker impact on categories that have not yet appeared during testing. To address this issue, Ko et al.~\cite{ko2021learning} propose storing both embedding features and category weights in the memory bank to reduce the emphasis on seen classes. 
In the context of domain adaptation, Chen et al.~\cite{chen2021transferrable}  address the problem of domain adaptation by using source memory and target memory to record the class characteristics of the source domain and the pseudo-label characteristics of the target domain, respectively.
%{He et al.~\cite{he2020momentum} and Chen et al.~\cite{chen2020improved} introduce the concept of the momentum memory bank, a queue of data samples in which features are progressively replaced. Some subsequent work~\cite{li2021spatial, el2021training, zheng2021group} has built upon these ideas. Li et al.~\cite{li2021spatial} use the momentum memory bank to design a novel spatial assembly network, which can learn different spatial variations' features. El et al.~\cite{el2021training} use an offline memory bank and a momentum encoder to improve the performance of image retrieval baselines, following~\cite{wang2020cross, he2020momentum}. In addition, Zheng et al.~\cite{zheng2021group} introduce a weighted contrastive loss to calculate the similarity of characteristics between the mini-batch and memory bank, which speeds up the optimization of the model. Fang et al.~\cite{fang2021seed} take a different approach by using a shared memory bank between a teacher and student model, storing the encoding output of the teacher model. However, these methods~\cite{he2020momentum, chen2020improved} primarily focus on categories that have already occurred and have a weaker impact on categories that have not yet appeared during testing. To address this issue, Ko et al.~\cite{ko2021learning} propose storing both embedding features and category weights in the memory bank to reduce the emphasis on seen classes. In the context of domain adaptation, Zhong et al.~\cite{zhong2019invariance} store the feature maps of \emph{unlabeled data} from the target domain and introduce the exemplar-invariance objective from the source domain to bridge the domain gap. Chen et al.~\cite{chen2021transferrable} also address the problem of domain adaptation by using source memory and target memory to record the class characteristics of the source domain and the pseudo-label characteristics of the target domain, respectively.

\begin{figure*}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=1\textwidth]{./figs/Intermediate_Feature_Representation.pdf}}
	\end{center}	
	\vspace{-15pt}
	\caption{ \textbf{Two streams of employing historical intermediate feature representation.} (a) Record the historical instance-level feature representations by memory bank or queue to construct/update sufficient and critical training pairs. (b) Memorize feature statistics which are further utilized in training loss or inference.
	}
	\label{fig_intermediate_feature_representation}
	%\vspace{-17pt}
\end{figure*}

%Intermediate Feature Representation2已用chatgpt刷过
(2)  {
% Another stream is to memorize feature statistics
\ylfChecked{Another stream is to memorize statistical-level feature representations for utilizing in training loss or inference}
~(see Fig.~\ref{fig_intermediate_feature_representation} (b)). 
The center loss~\cite{wen2016discriminative} and its different variants~\cite{he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,chen2021free,li2021frequency, keshari2020generalized, jing2021cross,fan2018associating,liu2020leveraging} attempt to record and update the category representations related to history. The goal is to minimize the intra-class distance and maximize the inter-class distance through the historical category representation. Center loss~\cite{wen2016discriminative} updates the category-level feature centers and penalizes the distances between the deep features and their corresponding class centers, promoting stronger intra-class compactness. The specific formula is as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {center }}=\frac{1}{2} \sum_{i=1}^{b}\left\|x_{i}-c_{y_{i}}\right\|_{2}^{2}.
\end{aligned}
\end{equation}
%$\mathcal{L}_{Center}$~represents the center loss, and $b$ denotes the size of mini-batch. Similarly, the $i$-th feature map of $n$-th layer~$x_{n_{i}}$ subtracts the  class center~$c_{k_{i}}$ whose category is $k$. $\left\|x_{n_{i}}-c_{k_{i}}\right\|_{2}^{2}$ means the intra-class distance. Then~$c_{k_{i}}$ is updated on each iteration, which condenses the class characteristics of different historical periods.
%The center loss, represented by $\mathcal{L}{center}$, is calculated over a mini-batch of size $b$. The calculation involves subtracting the class center $c{k_{i}}$ from the $i$-th feature vector of the $n$-th layer, $x_{n_{i}}$, where $k$ is the category. The intra-class distance is then computed as~$\left|x_{n_{i}}-c_{k_{i}}\right|_{2}^{2}$. On each iteration, the class center~$c{k_{i}}$ is updated, condensing the class characteristics over different historical periods. 
The center loss, represented by~$\mathcal{L}_{center}$, is calculated over a mini-batch of size $b$. The calculation involves subtracting the class center~$c_{y_{i}}$ from the~$i$-th feature vector~$x_{{i}}$, and~$y_{i}$ represents class index. The intra-class distance is then computed as~$\left\|x_{i}-c_{y_{i}}\right\|_{2}^{2}$. On each iteration, the class center~$c_{y_{i}}$ is updated, condensing the class characteristics over different historical periods. To further balance the distance between intra-class and inter-class, He et al.~\cite{he2018triplet} combine the advantage of triplet loss and center loss, which expressly seeks to enhance both inter-class separability and intra-class compactness. Li et al.~\cite{li2019angular} use angular distance instead of Euclidean distance used by~\cite{he2018triplet}, which gives more distinct discriminating limitations in the angle space. Later, Min et al.~\cite{min2020adversarial} combine the adversarial approach ACoL~\cite{ zhang2018adversarial } with center loss, which utilizes the majority of historical features statistics and also avoids the domination by some historical features statistics. Motivated by~\cite{wen2016discriminative} and~\cite{he2018triplet}, Zhao et al.~\cite{zhao2020deep} propose Hard Mining Center-Triplet Loss (HCTL). HCTL chooses the hardest positive/negative samples on whether it has the same label as each center, which uses the historical feature statistics to perform hard sample mining. Later, Li et al.~\cite{li2021frequency} selectively compress intra-class features into various embedding spaces, attempting to handle separately historical feature statistics for better optimization.
However, the center loss is designed for a single-label sample, making it difficult to extend to weakly-supervised multi-label tasks. Narayan et al.~\cite{narayan20193c} address this problem using attention-based per-class feature aggregation, calculating an attention center loss that employs more historical information and effectively reduces the intra-class distance of multi-label. %Furthermore, earlier studies could have ignored the similarities among different class centers in favor of the intra-class disparity. Zhu et al.\cite{zhu2020hetero} propose Hetero-Center loss, which limits the distance between two centers distributions by using the historical inter-class features. 
Besides,~\cite{keshari2020generalized} and~\cite{chen2021free} expand center loss to zero-shot learning. Notably, ~\cite{chen2021free} introduce the self-adaptive margin center (SAMC) loss to learn the historical fine-grained and visual-semantic embedding class centers. 
Jing et al.~\cite{jing2021cross} show that it is also possible to learn class centers from cross-modal samples. Cross-modal characteristics are transferred into the same representation space, then calculate the center loss, which employs the historical cross-modal feature to reduce the gaps.}
%In addition to recording historical category feature statistics, there are also historical characteristics of other locations in the network.}
%\wugerewrite{Later, some improvements ~\cite{liu2016large, liu2017sphereface, wang2017normface, deng2019arcface, meng2021magface} to Softmax were proposed. Liu et al.\cite{liu2016large} propose L-Softmax, which takes cosine similarity to separate various classes. L-Softmax takes the $θ$ as the angle between the samples and class, representing the intra-class distance in different historical periods. As the training progresses, $θ$ will gradually become smaller, which means the angle between the sample vector and the category vector becomes smaller. Then L-Softmax sets the margin parameter m to control the margin to reduce the distance of different classes. Motivated by L-Softmax, Liu et al. \cite{ liu2017sphereface } propose A-Softmax, which changes the weight $W_{i}$ and bias $b_{i}$ of Softmax. Specifically, A-Softmax set $||W_{1}||$~=~$||W_{2}||$ = 1 and~$||b_{1}||$~=~$||b_{2}||$~= 0, which lets the model learn and optimize the angular $θ$ between samples and classes directly. To accelerate the center shift within the historical intra-class center and gather different samples of the same class, Wang et al.~\cite{wang2018additive} introduced the inter-class interval t and scale factor s into L-Softmax. The t can increase the interval between different classes, and s accelerate the overall convergence of the model. Later, Deng et al.~\cite{deng2019arcface} let angle $θ$ add the margin m straightforward, aiming to improve Softmax's convergence difficulty. This operation makes the same convergence effect possible only when the angle converges smaller between inter-classes. Besides, Adaptive Margin Softmax ~\cite{liu2019adaptiveface} think margin should be adapted to different classes. It makes the margin m learnable and adjusts it for various classes adaptively. The adaptive margin m implicitly includes each class's historical representation, which can closely represent the real distribution of the datasets.} 

{Aside from statistics for classes, there are other ways to utilize historical statistics for features, e.g., moving mean and variance. Caron et al.~\cite{caron2021emerging} maintain the first-order batch statistics by adding a bias term to the teacher network, successfully avoiding the training collapse. {According to Ioffe et al.~\cite{ioffe2015batch}, during model inference, the mean and variance statistics computed and accumulated inside each Batch Normalization (BN) layer across all levels of features are utilized. The various variations of~\cite{ioffe2015batch, ioffe2017batch, li2016revisiting, salimans2016weight,arpit2016normalization} can lead to faster convergence and improved generalization of the model.}}

\begin{figure*}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=1\textwidth]{./figs/Model_Parameter.pdf}}
	\end{center}	
	\vspace{-15pt}
	\caption{ \textbf{Three types of using historical model parameters.} (a) Construct teachers based on historical student parameters to supervise the current student effectively. (b) Utilize ensemble results by inferring from several historical models. (c) Create a unitary ensemble architecture using the historical model parameters for efficient inference. 
	}
	\label{fig_hl_model_parameters}
	%\vspace{-10pt}
\end{figure*}

%Model Parameter已用chatgpt刷过
\noindent\textbf{Model Parameter.} The historical usage of model parameters can be divided into three major groups, and a comparison of these three structures is shown in Fig.~\ref{fig_hl_model_parameters}.

%caron2021emerging,he2020momentum,chen2021empirical,feng2021temporal,grill2020bootstrap,chen2020improved,li2021spatial, el2021training, zheng2021group, ko2021learning ,chen2021transferrable, zhong2019invariance,fang2021seed, byun2022grit , zhou2022regional,luo2021coco,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography
%caron2021emerging, chen2020improved, he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,li2021efficient, chen2020big, tomasev2022pushing, mitrovic2020representation, van2021unsupervised, xie2021propagate, ayush2021geography, li2020prototypical
(1) Constructing the teachers from past models  {to supervise the students effectively (see Fig.~\ref{fig_hl_model_parameters} (a)).} The popular practice is to build a teacher network by leveraging the historical student parameters in unsupervised~\cite{caron2021emerging,he2020momentum,chen2021empirical,feng2021temporal,grill2020bootstrap,chen2020improved, el2021training ,chen2021transferrable ,luo2021coco,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography,mitrovic2020representation,cai2019exploring, deng2021unbiased, xie2021detco,tomasev2022pushing}, supervised~\cite{yuan2020revisiting,li2019learning,yang2019snapshot,furlanello2018born,wang2022efficient,wang2022learn,wang2022self} and semi-supervised~\cite{tarvainen2017mean,cai2022semi,wang2022unsupervised,shi2018transductive,wang2021multi, kim2022pose, yu2019uncertainty, meng2021spatial, wang2021self ,wang2022semi,liu2022perturbed, cui2019semi, perone2018deep,zhang2022semi, xu2021end, yang2021interactive, liu2021unbiased, liu2022unbiased, cai2021exponential, zhang2023robust} learning. In the unsupervised learning, previous works~\cite{he2020momentum,chen2020improved} have shown the effectiveness of unsupervised learning exploiting historical parameters. 
%Grill et al.~\cite{grill2020bootstrap} focus on using ViT to build a self-supervised framework and achieve good results. Feng et al.~\cite{feng2021temporal} integrate instance temporal consistency into the teacher model and proposed a Temporal Knowledge Consistency (TKC) method for dynamic ensembling the knowledge of temporal teacher models and adaptively selecting valuable information. Chen et al.~\cite{chen2021empirical} propose a new self-supervised approach called Bootstrap Your Own Latent (BYOL), in which the target network's representation is predicted using the trained online network. After more in-depth research, Caron et al. ~\cite{caron2021emerging} find self-supervised ViT features provide more explicit information about down tasks of the image, such as semantic segmentation. Specifically,~\cite{caron2021emerging} employ a momentum encoder to forecast the teacher network's output directly.
Chen et al.~\cite{chen2021empirical} focus on building a self-supervised framework using ViTs and achieve good results. Feng et al.~\cite{feng2021temporal} propose a method called Temporal Knowledge Consistency (TKC), which integrates instance temporal consistency into the teacher model and dynamically combines the knowledge from temporal teacher models to select valuable information. Grill et al.~\cite{grill2020bootstrap} introduce a self-supervised approach called Bootstrap Your Own Latent (BYOL), in which the representation of the target network is predicted using a trained online network. Caron et al.~\cite{caron2021emerging} find that self-supervised ViT features provide more explicit information about tasks such as semantic segmentation, which employ a momentum encoder to predict the teacher network's output. %Other studies~\cite{li2021efficient, chen2020big, tomasev2022pushing, mitrovic2020representation, van2021unsupervised, xie2021propagate, ayush2021geography, li2020prototypical} also have applied the technique of constructing teacher network by historical parameters with the EMA to a range of unsupervised networks and produced positive outcomes.
%Feng et al.~\cite{feng2021temporal} integrate instance temporal consistency into the teacher model and propose a Temporal Knowledge Consistency (TKC) method to dynamically ensemble the knowledge of temporal teacher models and adaptively select valuable information. Chen et al.~\cite{chen2021empirical} propose a new self-supervised approach named Bootstrap Your Own Latent (BYOL). The same image representation of the target network is predicted by relying on the trained online network. Grill et al.~\cite{grill2020bootstrap} mainly focus on using ViT to build the self-supervised framework and achieve better results. Also, in the supervised learning, Yuan et al.~\cite{yuan2020revisiting} let the student model learn from itself or manually designed regularization distribution without the teacher model. Li et al.~\cite{li2019learning} train the models using gradient update to learn from various synthetic noisy labels, which aims to use the generated label noise to optimize the model. Yang et al.~\cite{yang2019snapshot} take each cycle's last snapshot as the next cycle teacher model to extract the same generation information. Furlanello et al.~\cite{furlanello2018born} offer ensemble the multiple student generations to get better performance which each identical model is initialized by using different random seeds and trained from the supervision of the earlier generation. Finally in semi-supervised learning, Xie et al.~\cite{xie2020self} train the teacher model on labeled data first and get the label from unlabeled data to train the student model, then iterate the student model as the teacher model. Tarvainen et al.~\cite{tarvainen2017mean} average model weight instead of label predictions to form a target-generating teacher model. Chen et al.~\cite{chen2020semi} let the teacher model take all the preceding network's predictions to averages to update the student model. Radosavovic et al.~\cite{radosavovic2018data} take omni-supervised learning as a special regime of semi-supervised and use a single model to ensemble predictions from multiple transformations of unlabeled data to automatically generate new training annotations. Yalniz et al.~\cite{yalniz2019billion} use a teacher model trained on labeled data to rank the unlabeled images, and the top-K images are picked to train the student model.
%\implusChecked{In the realm of supervised learning, Yuan et al.~\cite{yuan2020revisiting} propose letting the student model learn from itself or a manually designed regularization distribution without the need for a teacher model. Li et al.~\cite{li2019learning} train models using gradient updates to learn from various synthetic noisy labels, with the aim of using the generated label noise to optimize the model. Yang et al.~\cite{yang2019snapshot} use each cycle's last snapshot as the next cycle's teacher model to extract the same generation information. Furlanello et al.~\cite{furlanello2018born} propose ensembling multiple student generations to achieve better performance by initializing each identical model using different random seeds and training them using the supervision of the previous generation. 
%In the context of semi-supervised learning, Xie et al.~\cite{xie2020self} train the teacher model on labeled data first, then use the labels from unlabeled data to train the student model, iterating this process by using the student model as the teacher model. Tarvainen et al.~\cite{tarvainen2017mean} average model weights rather than label predictions to form a target-generating teacher model. Chen et al.~\cite{chen2020semi} use the teacher model to take the average of all preceding network predictions to update the student model. Radosavovic et al.~\cite{radosavovic2018data} treat omni-supervised learning as a special regime of semi-supervised learning and use a single model to ensemble predictions from multiple transformations of unlabeled data to automatically generate new training annotations. Yalniz et al.~\cite{yalniz2019billion} use a teacher model trained on labeled data to rank the unlabeled images, and the top-K images are selected to train the student model.}

%{In the realm of supervised learning, Yuan et al.~\cite{yuan2020revisiting} propose letting the student model learn from itself or a manually designed regularization distribution without the need for a teacher model. Li et al.~\cite{li2019learning} train models using gradient updates to learn from various synthetic noisy labels, with the aim of using the generated label noise to optimize the model. Yang et al.~\cite{yang2019snapshot} use each cycle's last snapshot as the next cycle's teacher model to extract the same generation information. Furlanello et al.~\cite{furlanello2018born} propose ensembling multiple student generations to achieve better performance by initializing each identical model using different random seeds and training them using the supervision of the previous generation. {Wang et al. propose in their works~\cite{wang2022efficient,wang2022learn} to train the student model by ensembling historical snapshots from the same training trajectory. This approach enhances distillation performance and reduces training costs. They also introduce a self-attention module to adapt the weights of the historical snapshots. In another work~\cite{wang2022self}, Wang et al. employ historical parameters as the teacher to update the current model through an Exponential Moving Average (EMA) during adversarial training.}}
{In the realm of supervised learning, researchers have proposed various approaches for employing historical parameters. Yuan et al.~\cite{yuan2020revisiting} suggest allowing the student model to learn from itself or a predefined regularization distribution, without relying on a teacher model. 
Li et al.~\cite{li2019learning} train models with synthetic noisy labels to optimize the model through the generated noise. Yang et al.~\cite{yang2019snapshot} use the last snapshot of each cycle as the teacher for the next cycle, in order to capture consistent information. Furlanello et al.~\cite{furlanello2018born} propose to improve performance by training multiple identical student models using different random seeds and the supervision of the previous generation. Wang et al.~\cite{wang2022efficient,wang2022learn} enhance distillation performance and reduce training costs by training the student model with historical snapshots from the same training trajectory, and by incorporating a self-attention module to adapt the weights of the snapshots. Another work~\cite{wang2022self} uses historical parameters as the teacher to update the current model through EMA during adversarial training.}

{In the context of semi-supervised learning, Tarvainen et al.~\cite{tarvainen2017mean} propose Mean teacher, which updates the teacher model based on the EMA of the historical student model’s parameters during training. Later, several  work uses~\cite{ cai2022semi, wang2022unsupervised, shi2018transductive,wang2021multi, kim2022pose, yu2019uncertainty, meng2021spatial, wang2021self ,wang2022semi,liu2022perturbed, cui2019semi, perone2018deep,zhang2022semi, xu2021end, yang2021interactive , liu2021unbiased, liu2022unbiased} or improves~\cite{ cai2021exponential, zhang2023robust} on the Mean teacher.
Cai et al.~\cite{cai2021exponential} propose exponential moving average normalization (EMAN) to replace the BN of the teacher model. EMAN aims to reduce the cross-sample dependency and model parameter mismatch to utilize historical student parameters efficiently. Then, Cai et al.~\cite{cai2022semi} extend the Mean teacher to the pure ViT for semi-supervised learning, having more scalability when using historical parameters and reducing the performance gap with fully supervised. 
Notably, Zhang et al.~\cite{zhang2023robust} propose the adaptive exponential moving average (AEMA) by adding an adjustment parameter~$\delta$ on the basis of EMA as follows:
\begin{equation}
\begin{aligned}
W_{i}^{t}  =\alpha\delta W_{i}^{t-1}+(1-\alpha\delta) W_{i}^{t}.
\end{aligned}
\end{equation}
The model parameters of~$t$-th iteration~$W_{i}^{t}$ are updated through the combination of historical model parameters~$W_{i}^{t-1}$ by momentum term~$ \alpha$. In general, AEMA generates better quality pseudo labels by adaptively adjusting historical and current model parameters.}
The teacher models can provide informative supervision to the original model, leading to significant improvement in the performance of student models.
%The resulted teacher models can provide sufficient and informative supervisions for the original model to distill, significantly boosting the performance of the student models.

%Model Parameter已用chatgpt刷过
{(2) {Directly exploiting ensemble results in inference from multiple past models (see Fig.~\ref{fig_hl_model_parameters} (b)).}
Huang et al.~\cite{huang2017snapshot} and Chen et al.~\cite{chen2017checkpoint} use a set of model snapshots (or checkpoints) to perform multiple evaluations for each test sample and ensemble the predictions as the final result. 
Later, some works extend~\cite{huang2017snapshot} to various tasks or fields: short-term load forecasting~\cite{ chen2018short }, uncertainty estimation ~\cite{ilg2018uncertainty, poggi2020uncertainty}, MR images segmentation~\cite{ wolterink2018automatic}, image denoising~\cite{yu2019deep},  adversarial training~\cite{liu2020loss}, object detection~\cite{ xu2020multi }, and medical image registration~\cite{gong2022uncertainty}. 
It is noteworthy that Chen et al.~\cite{ chen2018short } expand the~\cite{huang2017snapshot} to two stages, which involves integrating  models with different initial parameters. After obtaining all the snapshot models, the outputs of the models are averaged and produce the final forecast, using historical predictions from distinct training periods to reduce the standard deviation and improve generalization capability.}


%Huang et al.~\cite{huang2017snapshot} and Chen et al.~\cite{chen2017checkpoint} store a set of model snapshots (or checkpoints) to perform multiple evaluations for each test sample, then ensemble the predictions as the final results with improved accuracy. Behind, Xu et al.~\cite{xu2020multi} take~\cite{huang2017snapshot} expand to object detection. Meanwhile, ~\cite{xu2020multi} substitutes the cyclical learning rate for the cosine cyclical learning rate to keep the historical snapshots. Gong et al. \cite{gong2022uncertainty} then randomly choose $n$ samples form $m$ stored historical snapshots to calculate the empirical mean and variance, which aims to estimate the empirical uncertainty in medical image registration. 
%Huang et al.\cite{huang2017snapshot} and Chen et al.\cite{chen2017checkpoint} use a set of model snapshots (or checkpoints) to perform multiple evaluations for each test sample and ensemble the predictions as the final result. Xu et al.\cite{xu2020multi} extend this approach to object detection, and replace the cyclical learning rate with a cosine cyclical learning rate to preserve historical snapshots. Gong et al.\cite{gong2022uncertainty} randomly choose $n$ samples from $m$ stored historical snapshots to calculate the empirical mean and variance, which are used to estimate the empirical uncertainty in medical image registration.
%In object detection, some works~\cite{} attempt to ensemble the various results of the same or distinct models. Weighted Boxes Fusion (WBF) is a method suggested by ~\cite{solovyev2021weighted} to integrate the predictions of several item identification models. WBF first adds all kinds of predictions into the list, then ensembles them into new results. WBF can considerably enhance the merged rectangles' quality by aggregating historical predictions.}
%\wuge {Ruiz et al.~\cite{ruiz2019adaptative} employ a probabilistic framework that embeds an exponential number of CNNs to effectively adapt the computational cost necessary for inference, which prevents the independent evaluation of various networks at test time. Yang et al.~\cite{yang2020resolution} ensemble the multi-scale networks to allow different samples to select various resolution networks in an adaptive manner to infer efficiently. It aims to enable simple samples input into low-resolution and early network exit without additional processing. Furthermore, hard samples are fed into high-resolution networks for recognition. Wang et al.~\cite{wang2020glance} use reinforcement learning to ensemble multi-scale networks and down-sample simple samples to predict by using global information. However, hard samples are cropped to obtain class-discriminative image regions to localize and recognize iteratively. Park et al.~\cite{park2019feed} use parallel or recursive methods to introduce knowledge into the student network ensemble using multi-teacher models at the feature-map level.}
% , , 

(3) \ylfChecked{Building unitary ensemble architecture from previous models in inference}
(see Fig.~\ref{fig_hl_model_parameters} (c)). {Different from~\cite{huang2017snapshot,chen2017checkpoint} where multiple networks are adopted for inference,~\cite{zhang2020swa,athiwaratkun2018there,izmailov2018averaging,yang2019swalp,cha2021swad,guo2022stochastic,garipov2018loss,maddox2019simple,von2020neural} attempt to average/ensemble the historical model parameters directly and obtain a final single structure for efficient deployment.} 
%\wuge{Zhang et al.~\cite{zhang2020swa} use cyclical learning rates to train extra 12 epochs and average them, then can improve the performance. Wortsman et al.~\cite{wortsman2022robust} average the weights of the zero-shot and fine-tuned models to improve robustness and accuracy in the fine-tuning stage. Wortsman et al.~\cite{wortsman2022model} propose Model soup, a novel weight averaging technique that replaces traditional ensemble practices without adding to inference or memory costs by averaging the weights of independently fine-tuned models. By averaging the weights of history models, Wang et al.~\cite{wang2022self} suggeste a new ensemble technique called Self-Ensemble Adversarial Training (SEAT), which tries to integrate the states of every history model on the optimization trajectory through adversarial training. Recently, ~\cite{wang2022efficient,wang2022learn} ensemble the historical snapshots from the same training trajectory as the teacher model to train the student model. This method can considerably enhance the distillation performance of the ensemble model while lowering the training expense. Significantly, Wang et al.~\cite{ wang2022learn } not only ensemble the historical snapshots but also introduce the self-attention module to adapt the weights of historical snapshots. 
%Athiwaratkun et al.~\cite{athiwaratkun2018there} try to discover the high-accuracy pathways between the loss fuctions to get the high-performing ensembles in a short time. Maddox et al.~\cite{maddox2019simple} proposed Stochastic Weight Averaging (SWA), which can get a better generalization performance by following the trajectory of SGD to average the multiple points. Garipov et al.~\cite{garipov2018loss} aim to average multiple points to accelerate convergence than SWA further within each cycle of a cyclical learning rate schedule. Izmailov et al.~\cite{izmailov2018averaging} average the Bayesian model, which comes from the sampled Gaussian distribution. Von et al.~\cite{von2020neural} optimize independently and subsequently average the copies of a subset of neural network parameters, such as weights in the late processing of training, which can make SGD generalize well. Guo et al.~\cite{guo2022stochastic} try to discover the global geometric structures by using a series of WA operations achieving speed up the optimization effect than SGD. }
{For instance, Zhang et al.\cite{zhang2020swa} use cyclical learning rates to train extra epochs and average them to improve performance. Athiwaratkun et al.\cite{athiwaratkun2018there} try to identify high-accuracy pathways between loss functions to find high-performing ensembles in a short time. Izmailov et al.\cite{izmailov2018averaging} propose Stochastic Weight Averaging (SWA), which follows the trajectory of SGD to average multiple points and achieve better generalization performance. To adapt SWA to low precision training, Yang et al.~\cite{yang2019swalp} propose stochastic weight averaging in low-precision training (SWALP) to quantize gradient accumulators and velocity vectors during training. 
% On the other hand, Cha et al.~\cite{cha2021swad} extend SWA to Domain Generalization and address the issue of flat minima’s inaccurate approximation which is resulted by a small number of SWA sampling weights in high-dimensional parameter space. 
\ylfChecked{In another direction, Cha et al.~\cite{cha2021swad} extend SWA to Domain Generalization and aim to solve the problem of inaccurate approximation of flat minima that results from a small number of SWA sampling weights in high-dimensional parameter space.}
% Lastly, Guo et al.~\cite{guo2022stochastic} use a series of weight averaging (WA) operations to uncover global geometric structures, resulting in an improved optimization effect compared to SGD. 
\ylfChecked{Further, Guo et al.~\cite{guo2022stochastic} propose a method that utilizes a series of weight averaging (WA) operations to reveal global geometric structures, leading to improved optimization compared to SGD.}
Garipov et al.~\cite{garipov2018loss} use averaging to accelerate convergence within each cycle of a cyclical learning rate schedule. Maddox et al.~\cite{maddox2019simple} average Bayesian models that are sampled from a Gaussian distribution. \ylfChecked{Von et al.~\cite{von2020neural} optimize independently and average copies of a subset of neural network parameters, such as weights in the late processing of training, to improve generalization subsequently.}}
%{Zhang et al.\cite{zhang2020swa} use cyclical learning rates to train extra epochs and average them to improve performance. Wang et al.\cite{wang2022self} introduce a technique called Self-Ensemble Adversarial Training (SEAT), which averages the weights of historical models and integrates the states of each model on the optimization trajectory through adversarial training. Athiwaratkun et al.\cite{athiwaratkun2018there} try to identify high-accuracy pathways between loss functions to find high-performing ensembles in a short time. Maddox et al.\cite{maddox2019simple} propose Stochastic Weight Averaging (SWA), which follows the trajectory of SGD to average multiple points and achieve better generalization performance. Garipov et al.\cite{garipov2018loss} use averaging to accelerate convergence within each cycle of a cyclical learning rate schedule. Izmailov et al.\cite{izmailov2018averaging} average Bayesian models that are sampled from a Gaussian distribution. Von et al.\cite{von2020neural} optimize independently and subsequently average copies of a subset of neural network parameters, such as weights in the late processing of training, to improve generalization. Guo et al.\cite{guo2022stochastic} use a series of weight averaging (WA) operations to discover global geometric structures and improve the optimization effect compared to SGD.}
%{Zhang et al.\cite{zhang2020swa} use cyclical learning rates to train extra epochs and average them to improve performance. Wortsman et al.\cite{wortsman2022robust} average the weights of zero-shot and fine-tuned models to improve robustness and accuracy in the fine-tuning stage. They also propose a weight averaging technique called Model Soup~\cite{wortsman2022model}, which replaces traditional ensemble methods reducing inference or memory costs by averaging the weights of independently fine-tuned models. Wang et al.\cite{wang2022self} introduce a technique called Self-Ensemble Adversarial Training (SEAT), which averages the weights of historical models and integrates the states of each model on the optimization trajectory through adversarial training. Wang et al.\cite{wang2022efficient,wang2022learn} also propose ensembling historical snapshots from the same training trajectory to train the student model, which can improve distillation performance while reducing training costs. In addition, they introduce a self-attention module to adapt the weights of the historical snapshots. Athiwaratkun et al.\cite{athiwaratkun2018there} try to identify high-accuracy pathways between loss functions to find high-performing ensembles in a short time. Maddox et al.\cite{maddox2019simple} propose Stochastic Weight Averaging (SWA), which follows the trajectory of SGD to average multiple points and achieve better generalization performance. Garipov et al.\cite{garipov2018loss} use averaging to accelerate convergence within each cycle of a cyclical learning rate schedule. Izmailov et al.\cite{izmailov2018averaging} average Bayesian models that are sampled from a Gaussian distribution. Von et al.\cite{von2020neural} optimize independently and subsequently average copies of a subset of neural network parameters, such as weights in the late processing of training, to improve generalization. Guo et al.\cite{guo2022stochastic} use a series of weight averaging (WA) operations to discover global geometric structures and improve the optimization effect compared to SGD.}
% zhang2020swa, athiwaratkun2018there, maddox2019simple, garipov2018loss, izmailov2018averaging
\begin{table*}[htp]
	\vspace{0pt}
	\renewcommand\arraystretch{1.2}
	\centering
	\footnotesize \setlength{\tabcolsep}{4.pt}
    \caption{Related works of optimizers that rely on the statistics of historical gradients of the model parameters in a way of EMA for speeding up the training convergence. $W^{t-1}$ is not especially considered as any optimization step for neural networks involves updating the old network parameter.}
    %\vspace{-4pt}
	\resizebox{0.9\textwidth}{!}
	{
		%\begin{tabular}{|l|l|l|p{4cm}|}
        \begin{tabular}{l|l|l|l}
        \hline
        \multirow{1}{*}{Publication}& \multirow{1}{*}{Method}& \multirow{1}{*}{Update $W^{t}$} & \multirow{1}{*}{Historical Variable} \\ 

        \hline
       JMLR2011&Adagrad & $\begin{array}{l}
v^{t}=\sum_{0}^{t} (\nabla_{W_{i}^{t}})^2\\
W_{i}^{t}=W_{i}^{t-1}-\alpha \nabla_{W_{i}^{t}} / \sqrt{v^{t}+\epsilon}\\
\end{array}$  &${v^{t}}$ \\ 
        \hline
        ICML2013 &SGD+Momentum & $\begin{array}{l}
        v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) \nabla_{W_{i}^{t}} \\
        W_{i}^{t} =  W_{i}^{t-1}-\alpha^{t} v^{t} 
\end{array}$&${v^{t}}$  \\
        \hline 
        ICLR2015&Adam &$\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
\hat{m}^{t}=m^{t} /\left(1-\beta_{1}^{t}\right) \\
\hat{v}^{t}=v^{t} /\left(1-\beta_{2}^{t}\right) \\
W_{i}^{t} =  W_{i}^{t-1}-\hat{m}^{t}\alpha^{t} /\left(\sqrt{v^{t}}+\epsilon\right)
\end{array}$ &${m^{t}}$,~$v^{t}$  \\
        \hline 
        ICLR2016 &Nadam & $\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
\hat{m}^{t} =\left(\beta_{1}^{t+1} {m}^{t} /\left(1-\prod_{j=1}^{t+1} \beta_{1}^{j}\right)\right)+\left(\left(1-\beta_{1}^{t}\right) {g}^{t} /\left(1-\prod_{j=1}^{t} \beta_{1}^{j}\right)\right) \\
\hat{{v}^{t}} = \beta_{2}{v}^{t} /\left(1-\beta_{2}^{t}\right) \\
W_{i}^{t}=W_{i}^{t-1}-\hat{m}^{t}\alpha^{t} /\left(\sqrt{v^{t}}+\epsilon\right)
\end{array}$&${m^{t}}$,~$v^{t}$ \\
        \hline 
        ICLR2019&AdamW &$\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
\hat{m}^{t}=m^{t} /\left(1-\beta_{1}^{t}\right) \\
\hat{v}^{t}=v^{t} /\left(1-\beta_{2}^{t}\right) \\
\eta^{t} = \text { SetScheduleMultiplier }(t)\\
W_{i}^{t}=W_{i}^{t-1}-\eta^{t}(\hat{m}^{t}\alpha^{t} /\left(\sqrt{v^{t}}+\epsilon\right)+\lambda {\theta}^{t-1})
\end{array}$ &${m^{t}}$,~$v^{t}$  \\
        \hline 
       Arxiv2019& AdaMod& $\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
\hat{m}^{t}=m^{t} /\left(1-\beta_{1}^{t}\right) \\
\hat{v}^{t}=v^{t} /\left(1-\beta_{2}^{t}\right) \\
s^{t}=\beta_{3} s^{t-1}+\left(1-\beta_{3}\right) \alpha^{t} /(\sqrt{\hat{v}^{t}}+\epsilon)\\
W_{i}^{t}=W_{i}^{t-1}-\hat{m}^{t}\min \left(\alpha^{t} /\left(\sqrt{\hat{v}^{t}}+\epsilon\right), s^{t} \right)\\
\end{array}
$&${m^{t}}$,~$v^{t}$,~$s^{t}$\\
        \hline 
        IJCAI2020&Padam &$\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
\hat{v}^{t}=\max(\hat{v}^{t-1} ,{v}^{t})\\
W_{i}^{t}=W_{i}^{t-1}-{m}^{t}\alpha^{t} /\hat{v}^{t}_{p}\quad p \in(0,1 / 2]\\
\end{array}
$&${m^{t}}$,~$v^{t}$  \\
        \hline 
        ICLR2020&RAdam & $\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
\hat{m}^{t}=m^{t} /\left(1-\beta_{1}^{t}\right) \\
\rho^{\infty} =2 /\left(1-\beta_{2}\right)-1\\
\rho^{t} = \rho^{\infty}-2 t \beta_{2}^{t} /\left(1-\beta_{2}^{t}\right)\\
W_{i}^{t}=\left\{\begin{array}{ll}
l^{t} =\sqrt{\left(1-\beta_{2}^{t}\right) / v^{t}} \\
r^{t}=\sqrt{\frac{\left(\rho^{t}-4\right)\left(\rho^{t}-2\right) \rho^{\infty}}{\left(\rho^{\infty}-4\right)\left(\rho^{\infty}-2\right) \rho^{t}}} & \left(\rho^{t}>4\right) \\
W_{i}^{t-1}-\alpha^{t} r^{t} \hat{m}^{t} {l}^{t} & \\
\\
W_{i}^{t-1}-\alpha^{t} \hat{m}^{t} & \left(\rho^{t} \leq 4\right)
\end{array}\right.
\end{array}$&${m^{t}}$,~$v^{t}$  \\        
\hline 
        ICLR2021&Adamp & $\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}})^{2} \\
{p}^{t}= {m}^{t} /\left(\sqrt{{v}^{t}}+\varepsilon\right)\\
{q}^{t}=\left\{\begin{array}{ll}
\Pi_{W_{i}^{t-1}}\left({p}^{t}\right) & \text { if } \cos \left(W_{i}^{t-1}, \nabla_{W_{i}^{t}}\right)<\delta / \sqrt{\operatorname{dim}({W})} \\
{p}_{t} & \text { otherwise }
\end{array}\right.\\
W_{i}^{t}=W_{i}^{t-1}-\alpha {q}^{t}
\end{array}$&${m^{t}}$,~$v^{t}$\\
        \hline 
        NeurIPS22&Adan & $\begin{array}{l}
m^{t}=\beta_{1} m^{t-1}+\left(1-\beta_{1}\right) \nabla_{W_{i}^{t}} \\
v^{t}=\beta_{2} v^{t-1}+\left(1-\beta_{2}\right) (\nabla_{W_{i}^{t}}-\nabla_{W_{i}^{t-1}}) \\
n^{t}=\left(1-\beta_{3}\right) n^{t-1}+\beta_{3}\left[\nabla_{W_{i}^{t}}+\left(1-\beta_{2}\right)\left(\nabla_{W_{i}^{t}}-\nabla_{W_{i}^{t-1}}\right)\right]^{2} \\
\alpha^{t}=\alpha /\left(\sqrt{n^{t}+\varepsilon}\right) \\
W_{i}^{t}=\left(1+\lambda_{t} \eta\right)^{-1}\left[W_{i}^{t-1}-\alpha^{t}\left(m^{t}+\left(1-\beta_{2}\right) v^{t}\right)\right]
\end{array}$&${m^{t}}$,~$v^{t}$,~$n^{t}$  \\
        \hline 
		\end{tabular}
	}
	%\vspace{4pt}
	\vspace{-35pt}
	\label{tab_gradients_summary}
\end{table*}

%Gradient全部已用chatgpt刷过
\noindent\textbf{Gradient.} 
% There are two adoptions of this direction--the gradients of the model parameters and the all-level features. 
\ylfChecked{There are two approaches within this direction--operating the gradients of the model parameters or gradients of all-level features.}
The comparison of the two structures is illustrated in Fig.~\ref{fig_hl_gradient}.

(1) The advanced optimizers commonly rely on the statistics of historical gradients of the model parameters in a way of EMA for speeding up the training convergence (see Fig.~\ref{fig_hl_gradient} (a)), where momentum SGD~\cite{sutskever2013importance} calculates the first-order statistics, Adagrad~\cite{duchi2011adaptive} utilizes the second-order one. The specific information, formulas, and historical variables involved in different optimizers are shown in Table~\ref{tab_gradients_summary}. %\wuge {Adam~\cite{kingma2014adam} and its variants have been utilized for stochastic objective function optimization and are effective algorithms which make use of both. Moreover, Nesterov's accelerated gradient (NAG) is a technique used by NAdam\cite{dozat2016incorporating} to improve the learned model's quality. AdamW~\cite{loshchilov2017decoupled} adds decoupled weight decay yields for improved generalization performance. To hasten model convergence, Padam~\cite{chen2018closing} proposes the partial adaptive parameter. RAdam~\cite{liu2019variance} attempts to correct the adaptive learning rate so that the variance is constant. Adamp~\cite{heo2020adamp} alters the practical step sizes to prevent the weight standard from increasing.   Adan~\cite{xie2022adan} suggested a novel Nesterov momentum estimation (NME) method to lower the training cost and produce better outcomes.} %In addition, there is gradient accumulation (GradAccum) to accumulate multiple historical batch gradients to save memory~\cite{zhai2022scaling},~\cite{pham2021combined}. To do GradAccum work for concurrent learning and get around the memory limit, Pham et al~\cite{pham2021combined} developed a straightforward gradient accumulation method based on re-materialization. }%这个地方的zhai2022scaling需要check
Adam~\cite{kingma2014adam} and its variations are widely used for optimizing stochastic objective functions. They are effective algorithms that make use of both the mean and variance of the gradients. NAdam~\cite{dozat2016incorporating} incorporates Nesterov's accelerated gradient (NAG) technique to improve the quality of the learned model. AdamW~\cite{loshchilov2017decoupled} adds decoupled weight decay to improve generalization performance. 
AdaMod~\cite{ ding2019adaptive} uses adaptive upper bounds to limit the learning rates. 
Padam~\cite{chen2018closing} proposes the use of partial adaptive parameters to speed up model convergence. RAdam~\cite{liu2019variance} tries to correct the adaptive learning rate to maintain a constant variance. Adamp~\cite{heo2020adamp} modifies the practical step sizes to prevent the weight standard from increasing, and Adan~\cite{xie2022adan} introduces a Nesterov Momentum Estimation (NME) method to reduce training cost and improve performance.


(2) {The gradient-guided mechanism utilizes the historical gradients of various level features to reweight samples~(see Fig.~\ref{fig_hl_gradient} (b)). This method mainly focuses on dealing with long-tailed categories~\cite{tan2021equalization,li2022equalized} or abnormal samples~\cite{ kwon2020backpropagated} by using all levels of features. In order to reduce the accuracy gap between decoupled and end-to-end training techniques, Tan et al.~\cite{tan2021equalization} propose Equalization Loss v2 (EQL v2). EQL v2 rebalances the training process for each category independently and equally, which can be written as:
%2) \wuge{The gradient-guided mechanism mainly employs the historical gradients of various level features to reweight samples. Specially, the historical gradient-guided mechanism mainly deals with rare long-tailed categories indirectly~\cite{ tan2020equalization, hsieh2021droploss, wang2021adaptive, wang2021seesaw } or directly~\cite{ li2019gradient , tan2021equalization , li2022equalized } by using all level features. Tan et al.~\cite{tan2020equalization} propose a novel loss function named equalization loss (EQL), which indirectly uses gradients by introducing a weight term for each sample class. EQL prevents the learning of rare categories from being disadvantaged during the network parameter updating. To lessen disproportionate background gradients that inhibit tail class, DropLoss~\cite{hsieh2021droploss} is presented for rebalancing the background loss gradients adaptively between the rare class and the frequent class by via random sampling and reweighting. Wang et al.~\cite{wang2021adaptive} treat all object categories from the tail class to eliminate the boundary between the head class and tail class, which aims adaptively to balance the negative gradients by adding adaptive class suppression loss. However, blindly increasing/ reducing the gradients of positive/ negative samples will raise the probability of producing false positives of tail classes. Wang et al.~\cite{wang2021seesaw} propose Seesaw Loss to rebalance positive and negative gradients using mitigation and compensation factors. In terms of directly using the gradient, Li et al.~\cite{li2019gradient} first observe that the influence of imbalance between the head class and fail class can be described in terms of the gradient and employ a gradient harmonizing mechanism (GHM) to solve the problem. GHM calculates various samples with comparable gradient density and then adds a harmonizing parameter to the gradient of each sample in accordance with the density to balance the gradient contribution of different classes. In order to reduce the accuracy gap between decoupled and end-to-end training techniques, Tan et al.~\cite{tan2021equalization} propose Equalization Loss v2 (EQL v2). EQL v2 rebalances the training process for each category independently and equally, which can be written as:
\begin{equation}
\begin{aligned}
{g^{j}}=\frac{\sum_{t=0}^{j-1}\left|q^{t} \nabla_{z}^{\mathrm{pos}}\left(\mathcal{L}^{t}\right)\right|}{\sum_{t=0}^{j-1}\left|r^{t} \nabla_{z}^{\mathrm{neg}}\left(\mathcal{L}^{t}\right)\right|}.
\end{aligned}
\end{equation}
%$\nabla_{z}^{\mathrm{pos}}\left(\mathcal{L}^{t}\right)$ and~$\nabla_{z}^{\mathrm{neg}}\left(\mathcal{L}^{t}\right)$ represent the positive and negative gradients of the~$t$-th iteration. The~$t$-th iteration loss $\mathcal{L}^{t}$ is calculated by each classifier’s output~$z$.~$q^{t}$ and~$r^{t}$ are the weight of positive and negative gradients at the~$t$-th iteration. Then,~${g^{j}}$ define the cumulative ratio of positive and negative gradients for $j$-th iteration.
The positive and negative gradients of the~$t$-th iteration are represented by~$\nabla_{z}^{\mathrm{pos}}\left(\mathcal{L}^{t}\right)$ and~$\nabla_{z}^{\mathrm{neg}}\left(\mathcal{L}^{t}\right)$ respectively. These gradients are calculated based on the output of classifier~$z$ in the~$t$-th iteration. The weights of positive and negative gradients for the~$t$-th iteration are given by~$q^{t}$ and~$r^{t}$. The cumulative ratio of positive and negative gradients for the~$j$-th iteration is defined by~${g^{j}}$.
The EQL v2 balances the training process of each class independently by using the positive and negative gradients of the classifier's output to weight long-tailed samples. It has been shown to improve the performance of these samples on different baseline models. The idea has been extended to the one-stage object detection task by EFL~\cite{li2022equalized}, which uses two category-relevant factors to rebalance the loss contribution of positive and negative samples for each category. In general, the long-tailed distribution problem is better addressed by integrating the historical gradient of the long tail through the loss weighting method, either directly or indirectly. Additionally, in anomaly detection, abnormal samples have different gradients from normal samples. Kwon et al.~\cite{kwon2020backpropagated} use the gradient loss as a regularization term in the overall loss, which emphasizes the historical gradient of abnormal samples. The specific calculation process is as follows:
\begin{equation}
\begin{aligned}
\nabla_{x_{avg\_i}}^{j-1}=\frac{1}{(j-1)} \sum_{t=0}^{j-1} \nabla_{x_{i}}^{t}, 
\label{grad_avg}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {grad }}=-\underset{i}{\mathbb{E}}\left[{cos\_similarity}\left(\nabla_{x_{avg\_i}}^{j-1}, \nabla_{x_{i}}^{j}\right)\right].
\label{grad_cos}
\end{aligned}
\end{equation}
Eq.~\eqref{grad_cos} represents the cosine similarity between the average gradient~$\nabla_{x_{avg_i}}^{j-1}$ of~$(j-1)$-th iteration and the gradient~$\nabla_{x_{i}}^{j}$ of~$j$-th iteration, where~$i$ is the index of the layer in the decoder.
}
\begin{figure*}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=1\textwidth]{./figs/Gradients.pdf}}
	\end{center}	
	\vspace{-13pt}
	\caption{ \textbf{Two ways to use historical gradient}. (a) Optimizers update model parameters by EMA with historical gradients of the model parameters. (b) The gradient-guided mechanism reweights samples using the historical gradients of different level attributes.}
	\label{fig_hl_gradient}
	\vspace{-0pt}
\end{figure*}

%loss values 已用chatgpt刷过
\noindent\textbf{Loss Values.} The value of the loss may depict the training stability, learning difficulty, and other beneficial optimization signals. {Many historical parameters are used in the loss calculation and update process, therefore loss itself could be regarded as a historical collection. Some studies have used the unique properties of the loss to identify anomalous samples~\cite{chu2020neural} or biased samples~\cite{jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample}. For instance, Chu et al.~\cite{chu2020neural} use loss profiles as an informative cue for detecting anomalies, while~\cite{jiang2021delving} and \cite{huang2019o2u } record the loss curve of each sample as an additional attribute to help identify the biased samples. Notably,~\cite{huang2019o2u } proposes O2U-Net to transfer the network from overfitting to underfitting cyclically by adjusting the hyper-parameters. O2U-Net splits the training process into three parts: pre-training, cyclical training, and training on clean data. Inside, cyclical training records the historical loss of each sample, then calculates and ranks every sample's normalized average loss to recognize the noisy data with a higher score. Later, some works introduce the value of historical loss to determine whether the data is with noisy label~\cite{ xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample }. To achieve a better performance of cleaning samples, Xu et al.~\cite{xu2021small} also use the historical loss distribution to handle the issue of varying scales of loss distributions across different training periods. 
% Even if~\cite{huang2019o2u} the average loss can show good results, there may be some deviation when cleaning data due to the harm of some abnormal data. 
\ylfChecked{Although average loss has shown promising results~\cite{huang2019o2u}, the existence of outliers in the training data can result in deviations when cleaning the data.}
Hence Xia et al.~\cite{xia2021sample} construct two more robust mean estimators to reduce the uncertainty of small-loss and large-loss examples separately. Furthermore, Yue et al.~\cite{yue2022ctrl} explore the algorithm for cleaning the labels based on historical loss, efficiently using historical information to obtain high-quality and noise-free data.}
%\noindent\textbf{Loss Values.} The value of the loss may depict the training stability, learning difficulty and other beneficial optimization signals. {Many historical parameters are used in the loss calculation and update process, therefore loss itself could be regarded as a historical collection. Some studies use a special attribute of loss to identify anomalous samples~\cite{chu2020neural} or biased samples~\cite{jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample},  Chu et al.~\cite{chu2020neural} use loss profiles as an informative cue for detecting anomalies. \cite{jiang2021delving} and \cite{huang2019o2u } record the loss curve of each sample as an additional attribute to help identify the biased samples. Notably, \cite{huang2019o2u } proposes O2U-Net transfer the network from overfitting to underfitting cyclically by adjusting the hyper-parameters. O2U-Net splits the training process into three parts: pre-training, cyclical training, and training on clean data. Inside, cyclical training records the historical loss of each sample, then calculates and ranks every sample's normalized average loss to recognize the higher score noisy data. Later, some works also introduce the value of historical loss to determine whether the data is with noisy label~\cite{ xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample }. To achieve a better performance of cleaning samples, Xu et al.~\cite{xu2021small} also use the historical loss distribution to solve the issue of varying scales of loss distributions in training periods of history and the present. Even if~\cite{huang2019o2u} the average loss can show good results, there may be some deviation when cleaning data due to the harm of some abnormal data. Hence Xia et al.~\cite{xia2021sample} construct two more robust mean estimators to reduce the uncertainty of small-loss and large-loss examples separately. Special, Yue et al.~\cite{yue2022ctrl} further explore the algorithm of cleaning the labels based on the historical loss, efficiently using historical information to obtain high-quality noise-free data.}
% Historical statistics can tell potential attribute of samples, despite of the feature and prediction, 
  %The value of the loss may depict the training stability, learning difficulty and other beneficial optimization signals. \wuge {Many historical parameters are used in the loss calculation and update process, so loss itself is a historical collection. Some studies use a special attribute of loss to identify hard or easy samples~\cite{chu2020neural}, ~\cite{jiang2021delving},\cite{huang2019o2u},\cite{xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample},  while others use the value of loss to choose which samples to use for parameter updating.} Chu et al.~\cite{chu2020neural},  use loss profiles as an informative cue for detecting anomalies. Jiang et al.~\cite{jiang2021delving} record the loss curve of each sample as an additional attribute to help identify the biased samples. \wuge { Huang et al.~\cite{huang2019o2u} transfer the network from overfitting to underfitting cyclically by adjusting the hyper-parameters, then calculate and rank the normalized average loss of every sample to recognize the higher score noisy data. In order to enable the mentor network to deliver the student network with accurate label examples. Jiang et al.~\cite{jiang2018mentornet} treat a certain number of training examples as true-labeled examples by judging whether the loss is relatively small. In contrast, Han et al.~\cite{han2018co} and Yu et al.~\cite{yu2019does} aim to let two networks teach each other at every mini-batch, which select some small loss examples and feed each other. Wei et al.~\cite{wei2020combating} allow two networks to predict on the same mini-batch data and calculate each training example KL divergence, making predictions of the two networks closer and updating the parameters through small-loss examples. }
%Zhao et al.~\cite{zhao2021learning} use the similarities between features and memory centroids to compute each domain feature's identification loss.

\subsection{Aspect of Functional Part~\label{sec_3.2}}
{In Section~\ref{sec_3.1}, the background and details of the historical type are thoroughly discussed. Next, we elaborate on the appropriate functional parts in this subsection.
%and a detailed comparison of them is illustrated in Fig.~\ref{fig_hl_fp}.}

\noindent\textbf{Data.} 
\ylfChecked{The functional part of various data can be functionally categorized into four aspects. The first aspect addresses the problems of small numbers of valid positive and negative pairs and the imbalance of sample categories during training.} Adopting the memory bank or related variants~\cite{li2021spatial,zhuang2019local,sprechmann2018memory,wang2020cross, liu2022densely,wang2022contrastive,deng2022insclr,yang2022online,deng2021variational,li2020unsupervised,feng2021exploring, liu2022memory, wen2021false, liu2022learning, liu2021noise,ortego2021multi, jin2021mining, yang2022cross,wu2018improving,wu2018unsupervised, misra2020self, tian2020contrastive, yin2021instance, dai2021dual,zhou2022regional,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,yang2018learning,yang2019visual,zheng2021group} stores the past high-level instance embeddings to augment/formulate the critical training pairs to save feature computing budgets. %~\cite{tan2021equalization-kwon2020backpropagated}
It can be regarded as an efficient augmentation from the aspect of training data in the feature space, yet not the input (image) space. {Moreover, Zhang et al.~\cite{zhang2021rethinking} take hard negative mining with a rough similarity margin instead of easy negatives to improve the quality of the memory bank. Ge et al.~\cite{ge2020self} aim to learn features by taking all available data from source and target domains encoded using the hybrid memory module.} 
The second aspect involves gathering the feature statistics of the same class and dispersing the feature statistics of different classes. It is mainly achieved by reducing the intra-class distance and increasing the inter-class distance through the historical category representation. Center loss~\cite{wen2016discriminative} maintains the data center of each category by gradually accumulating the historical instance embeddings, and the data centers are utilized for learning the intra-class compactness. \ylfChecked{Later, diverse improvements~\cite{ he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,chen2021free,li2021frequency, keshari2020generalized, jing2021cross} are proposed based on the center loss to address its limitations, thus facilitating a faster reduction of intra-class distance and expansion of inter-class distance.}
For the third aspect, some studies employ the unique properties of the loss to clean the dataset. They~\cite{chu2020neural,jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample} leverage the loss curve to reweight the loss function so as to identify anomalous/biased samples, and ultimately improve data cleanliness. 
Last, the fourth aspect focuses on employing historical input to perform data augmentation. Yang et al.~\cite{yang2022recursivemix} zoom and paste the historical input onto the new round of training images to make mixed-sample data augmentation.}

% {The functional part of various data mainly focuses on three aspects: (1) solve the problems that the number of valid positive and negative pairs is small and the categories of samples are unbalanced during training. (2) gather the feature statistics of the same class and disperse the feature statistics of different classes. (3) use the unique properties of the loss to clean the dataset.}
%\ylfChecked{The functional part of various data mainly focuses on three aspects: (1) solving the problems of small numbers of valid positive and negative pairs and the imbalance of sample categories during training. (2) gathering the feature statistics of the same class and dispersing the feature statistics of different classes. (3) using the unique properties of the loss to clean the dataset. }\wuge{(4) employing the historical input to make data augmentation.}
%For the first series, some works adopt the memory bank or related variants~\cite{sprechmann2018memory,wang2020cross, liu2022densely, wang2022contrastive,deng2022insclr,yang2022online, deng2021variational,li2020unsupervised,feng2021exploring, liu2022memory, wen2021false, liu2022learning, liu2021noise,ortego2021multi, jin2021mining, yang2022cross, fang2021seed,wu2018improving,wu2018unsupervised, misra2020self, tian2020contrastive, yin2021instance, dai2021dual,he2020momentum, chen2020improved, li2021spatial, el2021training, zheng2021group, ko2021learning,chen2021transferrable,byun2022grit,zhou2022regional,luo2021coco,li2020prototypical,ayush2021geography,xie2021propagate,van2021unsupervised,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography,li2020prototypical,ayush2021geography,xie2021propagate,van2021unsupervised,chen2020big,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,yang2018learning, yang2019visual, zhuang2019local, xie2021detco} that store the past high-level instance embeddings to augment/formulate the critical training pairs to save feature computing budgets.
%It can be regarded as an efficient augmentation from the aspect of training data in the feature space, yet not the input (image) space. 
%{Moreover, Zhang et al.~\cite{zhang2021rethinking} take hard negative mining with a rough similarity margin instead of easy negatives to improve the quality of the memory bank. Ge et al.~\cite{ge2020self} aim to learn features by taking all available data from source and target domains encoded using the hybrid memory module.} The solution to the second problem is mainly achieved by reducing the intra-class distance and increasing the inter-class distance through the historical category representation. Center loss~\cite{wen2016discriminative} maintains the data center of each category by gradually accumulating the historical instance embeddings, and the data centers are utilized for learning the intra-class compactness. 
% Later, diverse improvements~\cite{ he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,zhu2020hetero,chen2021free,li2021frequency, keshari2020generalized, jing2021cross} based on center loss improve the shortcomings of the center loss, accelerating the contraction of the intra-class distance and the expansion of the inter-class distance. 
%\ylfChecked{Later, diverse improvements~\cite{ he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,zhu2020hetero,chen2021free,li2021frequency, keshari2020generalized, jing2021cross} are proposed based on the center loss to address its limitations, thus facilitating a faster reduction of intra-class distance and expansion of inter-class distance.}
% Finally, for the third point, some works~\cite{chu2020neural, jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample} utilize the loss curve to reweight the loss function and identify anomalous/biased samples, which aims to clean the data.
%\ylfChecked{For the third point, some works~\cite{chu2020neural, jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample} leverage the loss curve to reweight the loss function so as to identify anomalous/biased samples, and ultimately improve data cleanliness. } \wuge{At last, Yang et al.~\cite{yang2022recursivemix} zoom and paste the historical input onto the new round of training images to make mixed-sample data augmentation.}


%\noindent\textbf{Data.} To save feature computing budgets, Wang et al.~\cite{wang2020cross}, \wuge{Shen et al.~\cite{shen2022self}} and Wu et al.~\cite{wu2018unsupervised,wu2018improving} adopt the memory bank that stores the past high-level instance embeddings to augment/formulate the critical training pairs. It can be regarded as an efficient augmentation from the aspect of training data in the feature space, yet not the input (image) space. \wuge{Moreover, Zhang et al.~\cite{zhang2021rethinking} take hard negative mining with a rough similarity margin instead of easy negatives to improve memory bank data quality.  Ge et al.~\cite{ge2020self} aim to learn features by taking all available data from source and target domains encoded using the hybrid memory module.} Differently, Wen et al.~\cite{wen2016discriminative} maintain the data center of each category by gradually accumulating the historical instance embeddings, and the data centers are utilized for learning the intra-class compactness. Jiang et al.~\cite{jiang2021delving} utilizes the loss curve of each sample to re-weight the loss.

\noindent\textbf{Model.}
Existing works mainly focus on using parameters and gradients in the past model to improve the model's performance.
First, the previous model parameters are collected to build the teacher models~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,yuan2020revisiting,li2019learning,yang2019snapshot, furlanello2018born, tarvainen2017mean, chen2020big, tomasev2022pushing, mitrovic2020representation, van2021unsupervised, xie2021propagate, ayush2021geography, li2020prototypical,wang2022efficient,wang2022learn,wang2022self,cai2022semi, wang2022unsupervised, shi2018transductive,cai2021exponential, zhang2023robust,wang2021multi, kim2022pose, yu2019uncertainty, meng2021spatial, wang2021self,wang2022semi,liu2022perturbed, cui2019semi, perone2018deep,zhang2022semi, xu2021end, yang2021interactive , liu2021unbiased, liu2022unbiased, cai2019exploring, deng2021unbiased, xie2021detco,chen2021transferrable,luo2021coco,el2021training} that can offer effective supervisions. As a consequence, the performance of the student model is significantly boosted. 
In addition, model parameters are also performed by ensemble learning through averaging the model weights~\cite{zhang2020swa,athiwaratkun2018there,izmailov2018averaging,yang2019swalp,cha2021swad,guo2022stochastic,garipov2018loss,maddox2019simple,von2020neural} or predictions~\cite{huang2017snapshot,chen2017checkpoint,chen2018short,ilg2018uncertainty,poggi2020uncertainty,wolterink2018automatic,yu2019deep,liu2020loss,xu2020multi,gong2022uncertainty} during inference. The model weight is mainly constructed by unitary ensemble architecture in inference. \ylfChecked{For example, Zhang et al.~\cite{ zhang2020swa} average checkpoints from multiple training epochs trained with cyclical learning rates.}
% In contrast to model weights, handling predictions emphasize the exploitation of ensemble results in inference from various historical models.
\ylfChecked{Another way to use historical model parameters is to process model predictions, which emphasizes the exploitation of ensemble results from various historical models during inference.}
Both~\cite{ chen2017checkpoint} and~\cite{huang2017snapshot} employ a collection of model snapshots (or checkpoints) to assess each test sample and then combine the predictions as the final result. 
% There are some other slightly different usages introduced in Batch Normalization~\cite{ioffe2015batch} and its variants~\cite{ioffe2017batch, li2016revisiting, salimans2016weight, arpit2016normalization}, where the historical feature statistics are accumulated in training, but utilized in testing.
\ylfChecked{Batch Normalization~\cite{ioffe2015batch} and its variations~\cite{ioffe2017batch, li2016revisiting, salimans2016weight, arpit2016normalization} introduce some alternate applications, where the feature statistics are collected during training and subsequently used during testing.}
\ylfChecked{In contrast to using historical model parameters, optimization algorithms~\cite{liu2019variance,loshchilov2017decoupled,ding2019adaptive,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive, dozat2016incorporating, chen2018closing, heo2020adamp, xie2022adan} perform adaptive gradient descent based on the historical gradient statistics, successfully speeding up the training convergence.}
%\noindent\textbf{Model.} Existing works mainly focus on the past model gradients and model parameters. The optimizers~\cite{liu2019variance,loshchilov2017decoupled,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive} perform adaptive gradient descent based on the historical gradient statistics, successfully speeding up the training convergence. The previous model parameters are collected to build the teacher models~\cite{caron2021emerging, chen2020improved, he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,yuan2020revisiting,li2019learning,yang2019snapshot, furlanello2018born,xie2020self, tarvainen2017mean} offering effective supervisions or to perform ensemble learning  by averaging the model weights~\cite{zhang2020swa,athiwaratkun2018there,maddox2019simple,garipov2018loss,izmailov2018averaging} or predictions~\cite{huang2017snapshot,chen2017checkpoint} during inference. A slightly different usage is introduced in Batch Normalization~\cite{ioffe2015batch}, where the historical feature statistics are accumulated in training, but utilized in test.

%\noindent\textbf{Loss.} The historical predictions~\cite{kim2021self,laine2016temporal}, feature embeddings~\cite{wang2020cross,zhong2019invariance,wu2018improving,wu2018unsupervised,wen2016discriminative} and generated soft labels from self-constructed teachers~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,yuan2020revisiting,xie2020self,li2019learning,yang2019snapshot,furlanello2018born,tarvainen2017mean} finally become a part of the loss objectives, providing meaningful and promising learning signals.
\noindent\textbf{Loss.} The historical predictions~\cite{laine2016temporal,yang2022recursivemix, yao2020deep,kim2021self,nguyen2019self, shen2022self,chen2020semi}, feature embeddings~\cite{li2021spatial,zhuang2019local,sprechmann2018memory, wang2020cross, deng2022insclr, yang2022online, deng2021variational, li2020unsupervised,feng2021exploring, liu2022memory,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,yang2018learning, yang2019visual,liu2022densely,wang2022contrastive,wen2021false,liu2022learning,liu2021noise,ortego2021multi,jin2021mining,yang2022cross,zhong2019invariance,wu2018improving, wu2018unsupervised, misra2020self, tian2020contrastive, yin2021instance, dai2021dual,ko2021learning,zhou2022regional,zheng2021group,
wen2016discriminative,he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,chen2021free,li2021frequency, keshari2020generalized, jing2021cross,fan2018associating,liu2020leveraging
}, gradients of various level features~\cite{tan2021equalization,li2022equalized,kwon2020backpropagated} and generated soft labels from self-constructed teachers~\cite{caron2021emerging,he2020momentum,chen2021empirical,feng2021temporal,grill2020bootstrap,chen2020improved, el2021training ,chen2021transferrable ,luo2021coco,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography,mitrovic2020representation,cai2019exploring, deng2021unbiased, xie2021detco,tomasev2022pushing,
yuan2020revisiting,li2019learning,yang2019snapshot,furlanello2018born,wang2022efficient,wang2022learn,wang2022self,
tarvainen2017mean,cai2022semi,wang2022unsupervised,shi2018transductive,wang2021multi, kim2022pose, yu2019uncertainty, meng2021spatial, wang2021self ,wang2022semi,liu2022perturbed, cui2019semi, perone2018deep,zhang2022semi, xu2021end, yang2021interactive, liu2021unbiased, liu2022unbiased, cai2021exponential, zhang2023robust
} \ylfChecked{can both contribute to }the loss objectives, providing meaningful and promising learning signals. {Besides, some methods directly use the value of loss itself to determine whether the sample is abnormal or noisy~\cite{chu2020neural,jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample}. For example,~\cite{huang2019o2u} not only record the loss curve of each sample but also average the loss to recognize the higher score noisy data.
% Whether it is ultimately a part of the loss or utilizing historical loss for some tasks, 
% loss itself can be used as a collection of historical information to directly or indirectly use historical information in certain circumstances.
\ylfChecked{In summary, collecting historical information to construct the loss or using the loss function itself as historical information represents an indirect or direct approach to leveraging past knowledge functionally.}
}

\iffalse
\begin{figure*}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
\fbox{\includegraphics[width=0.8\textwidth]{./figs/Functional Part.pdf}}
	\end{center}	
	\vspace{-13pt}
	\caption{ \textbf{Three distinct functional parts.} (a) Utilize different historical information to augment, clean, cluster data, or create sample pairs, (b) Update or ensemble model parameters by employing diverse historical information. (c) Collect various historical information to construct the loss function or use it to clean data.}
	\label{fig_hl_fp}
	\vspace{-0pt}
\end{figure*}
\fi

\subsection{Aspect of Storage Form~\label{sec_3.3}}
{In Section~\ref{sec_3.2}, we outline the specific functional parts of historical learning. This section covers the storage form of historical information. The various components of history learning are either recorded as Discrete Elements in the medium to long term, or they are transitory and disappear upon updating the Moving Average.}

%gradient~\cite{li2022equalized,tan2021equalization}.Loss~\cite{chu2020neural,jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample}The latest single batch of predictions~\cite{kim2021self,shen2022self,chen2020semi,yang2022recursivemix}.
\noindent\textbf{Discrete Elements (DEs)}: {DEs are stored in various ways over a medium to long term. The existing literature primarily stores DE in four forms. Memory bank or queue stores and updates cross-batch DEs by creating a container with limited capacity. The instance embeddings~\cite{li2021spatial,zhuang2019local,sprechmann2018memory,wang2020cross,liu2022densely,wang2022contrastive,deng2022insclr,yang2022online,deng2021variational,li2020unsupervised,feng2021exploring, liu2022memory, wen2021false, liu2022learning, liu2021noise,ortego2021multi,jin2021mining,yang2022cross,wu2018unsupervised, misra2020self, tian2020contrastive, yin2021instance, dai2021dual,he2020momentum, chen2020improved, el2021training,chen2021transferrable,zhou2022regional,luo2021coco,li2020prototypical,ayush2021geography,xie2021propagate,van2021unsupervised,chen2020big,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,wang2022semi,yang2018learning, yang2019visual,xie2021detco,zheng2021group} are added to the memory bank or queue for further use. These past elements are recorded and will be replaced based on the first-in, first-out principle when the memory bank or queue reaches its capacity.
The iterative teacher model maintains historical DEs~\cite{wang2022efficient,wang2022learn,yuan2020revisiting,yang2019snapshot,furlanello2018born,li2019learning,feng2021temporal}. Feng et al.~\cite{feng2021temporal} perform teacher learning by incorporating multiple historical model checkpoints in a parallel manner. Yuan et al.~\cite{yuan2020revisiting} suggest training the model using a single pretrained teacher model.~\cite{yang2019snapshot,furlanello2018born,li2019learning} use iterative teacher-student learning by employing multiple checkpoints as the teacher model in sequence. \ylfChecked{Generally, the iterative teacher model retains and updates previous DEs and subsequently provides supervision to the current student model during the training process.}
In contrast, checkpoints or snapshots from the learning history can be directly saved to create robust ensemble results~\cite{huang2017snapshot,chen2017checkpoint,chen2018short,ilg2018uncertainty,poggi2020uncertainty,wolterink2018automatic,yu2019deep,liu2020loss,xu2020multi,gong2022uncertainty}. 
Finally, other DEs such as prediction~\cite{kim2021self,shen2022self,yang2022recursivemix}, gradient~\cite{li2022equalized,tan2021equalization}, and loss~\cite{chu2020neural,jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample} \ylfChecked{may incorporate memory by storing their respective historical values for use in various operations.}

%: (1) memory bank or queue. (2) teacher model. (3) ensemble results. (4) other elements (e.g., predictions, gradient, and loss). Memory bank or queue stores and updates cross-batch DE by creating a container with limited capacity. The instance embeddings~\cite{sprechmann2018memory,wang2020cross, liu2022densely, wang2022contrastive,deng2022insclr,yang2022online, deng2021variational,li2020unsupervised,feng2021exploring, liu2022memory, wen2021false, liu2022learning, liu2021noise,ortego2021multi, jin2021mining, yang2022cross, fang2021seed,wu2018unsupervised, misra2020self, tian2020contrastive, yin2021instance, dai2021dual,he2020momentum, chen2020improved, li2021spatial, el2021training, zheng2021group, ko2021learning,chen2021transferrable,byun2022grit,zhou2022regional,luo2021coco,li2020prototypical,ayush2021geography,xie2021propagate,van2021unsupervised,chen2020big,xiao2017joint,alonso2021semi,lee2021weakly,li2022recurrent,liu2021one,wang2022semi,yang2018learning, yang2019visual} are added to the memory bank or queue for further use. These past elements are recorded and will be replaced based on the first-in, first-out principle when the memory bank or queue reaches its capacity.
%In contrast, the iterative teacher model maintains historical DE~\cite{wang2022efficient,wang2022learn,yuan2020revisiting,yang2019snapshot,furlanello2018born,li2019learning,feng2021temporal}. Feng et al.~\cite{feng2021temporal} perform teacher learning by incorporating multiple historical model checkpoints in a parallel manner. Yuan et al.~\cite{yuan2020revisiting} suggest training the model using a single pretrained teacher model.~\cite{yang2019snapshot,furlanello2018born,li2019learning} use iterative teacher-student learning by employing multiple checkpoints as the teacher model in sequence. 
% Generally, the iterative teacher model retains and updates past DE, then supervises the current student model. 
%\ylfChecked{Generally, the iterative teacher model retains and updates previous DE, and subsequently provides supervision to the current student model during the training process.}
%In contrast, checkpoints or snapshots from the learning history can be directly saved to create robust ensemble results~\cite{huang2017snapshot,chen2017checkpoint,chen2018short,ilg2018uncertainty,poggi2020uncertainty,wolterink2018automatic,yu2019deep,liu2020loss,xu2020multi,gong2022uncertainty}. Besides, other elements such as predictions~\cite{kim2021self,shen2022self,chen2020semi,yang2022recursivemix}, gradient~\cite{li2022equalized,tan2021equalization}, and loss~\cite{chu2020neural,jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample} 
% also perform some additional operations by simply saving their corresponding historical values into memory.
%\ylfChecked{may incorporate memory by storing their respective historical values for use in various operations.}
%gradient~\cite{li2022equalized,tan2021equalization}.Loss~\cite{chu2020neural,jiang2021delving,huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample}The latest single batch of predictions~\cite{kim2021self,shen2022self,chen2020semi,yang2022recursivemix}.

%\noindent\textbf{Discrete Elements (DE).} \wuge{DE are stored in a relatively medium and long term in different ways. The existing work is mainly to store DE in the following four forms: (1) memory bank or queue; (2) teacher model; (3) prediction set as ensemble prediction; (4) model set as the unitary network. }The latest single batch of predictions~\cite{kim2021self}, instance embeddings~\cite{wang2020cross,wu2018unsupervised,chen2020improved,he2020momentum} is updated into the memory bank or queue for further operations. These past elements are recorded only once and will be replaced if a newer one comes up. Yuan et al.~\cite{yuan2020revisiting} propose to train the model by a single pretrained model of itself.~\cite{yang2019snapshot,xie2020self,furlanello2018born,li2019learning} perform iterative teacher-student learning by leveraging multiple checkpoints as the teacher model sequentially. Feng et al.~\cite{feng2021temporal} conduct the teacher learning via using multiple historical model checkpoints in a parallel way. The checkpoints or snapshots during the learning history can also be directly preserved as a model set to produce reliable ensemble predictions~\cite{huang2017snapshot,chen2017checkpoint}.

\noindent\textbf{Moving Average (MA).} There are typically two common forms of MA in the literature:

(1) Exponential Moving Average (EMA) is the most commonly used operator. {EMA calculates the local mean of a variable by considering its historical values over time. It typically updates momentum in predictions of probabilities~\cite{laine2016temporal,yao2020deep,nguyen2019self}, feature representations~\cite{li2021spatial,zhuang2019local,zhong2019invariance,wu2018improving,zhou2022regional,wen2016discriminative,he2018triplet,li2019angular,min2020adversarial,zhao2020deep,narayan20193c,chen2021free,li2021frequency, keshari2020generalized, jing2021cross,fan2018associating,liu2020leveraging,zheng2021group}, feature statistics~\cite{ioffe2015batch,ioffe2017batch, li2016revisiting, salimans2016weight,arpit2016normalization}, network parameters~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,tarvainen2017mean,chen2020big,van2021unsupervised,xie2021propagate,li2020prototypical,ayush2021geography,mitrovic2020representation,tomasev2022pushing,cai2022semi, wang2022unsupervised, shi2018transductive,cai2021exponential, zhang2023robust,wang2022self,li2019learning,wang2021multi, kim2022pose, yu2019uncertainty, meng2021spatial, wang2021self,wang2022semi,liu2022perturbed, cui2019semi, perone2018deep,zhang2022semi, xu2021end, yang2021interactive , liu2021unbiased, liu2022unbiased, cai2019exploring, deng2021unbiased, xie2021detco,chen2021transferrable,luo2021coco,el2021training}, and model gradients~\cite{liu2019variance,loshchilov2017decoupled,ding2019adaptive,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive, dozat2016incorporating, chen2018closing, heo2020adamp, xie2022adan}. The primary goal of combining past predictions with EMA is to provide soft targets that standardize the model and minimize the supervised learning objective. Batch Normalization~\cite{ioffe2015batch} and its variants use the mean and variance of historical statistics computed through EMA to standardize the data. Besides, leveraging historical feature representations~\cite{zhong2019invariance,wu2018improving,wen2016discriminative}, network parameters~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,tarvainen2017mean}, and gradients~\cite{liu2019variance,heo2020adamp, xie2022adan} by EMA give more weight and importance to the most recent data points while still tracking a portion of the history.}
%(1) Exponential Moving Average (EMA) is the most common operator. \wuge{The local mean of a variable can be calculated using EMA so that its update is related to its historical value over time. EMA usually applies momentum updates to probability predictions~\cite{ kim2021self, laine2016temporal, wu2019mutual, yao2020deep, nguyen2019self, shen2022self }, feature representations~\cite{zhong2019invariance,wu2018improving,wen2016discriminative}, feature statistics~\cite{ioffe2015batch}, network parameters~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,tarvainen2017mean} and model gradients~\cite{liu2019variance,loshchilov2017decoupled,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive}. The main purpose of combining past predictions with EMA is to provide soft targets standardizing itself and minimizing the supervised learning goal~\cite{ kim2021self, laine2016temporal, wu2019mutual, yao2020deep, nguyen2019self, shen2022self}. Additionally, BN and pertinent variants employ the mean and variance of the historical statistics in reference that have been gathered by EMA to standardize data. Moreover, historical feature representations~\cite{zhong2019invariance,wu2018improving,wen2016discriminative}, network parameters~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,tarvainen2017mean}, and gradients ~\cite{liu2019variance,loshchilov2017decoupled,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive} are leveraged in the way of EMA, which places a greater weight and significance on the most recent data points, and keeps tracking a certain range of the history.}
%1) Exponential Moving Average (EMA) is the most common operator. \wuge{The local mean of a variable can be calculated using EMA so that its update is related to its historical value over time. $V$ is recorded as $V_{t}$ at time $t$, $\theta _{t}$ is the value of $V$ at time $t$, and $β∈ [0,1]$ :} 
%\begin{equation}
%\begin{aligned}
%V_{t} =\beta V_{t-1}+ (1-\beta)\theta _{t}
%\end{aligned}
%\end{equation}
%EMA usually applies momentum updates to probability predictions~\cite{laine2016temporal}, feature representations~\cite{zhong2019invariance,wu2018improving,wen2016discriminative}, feature statistics~\cite{ioffe2015batch}, network parameters~\cite{caron2021emerging,chen2020improved,he2020momentum,feng2021temporal,chen2021empirical,grill2020bootstrap,tarvainen2017mean} and model gradients~\cite{liu2019variance,loshchilov2017decoupled,kingma2014adam,sutskever2013importance,zeiler2012adadelta,duchi2011adaptive}. EMA usually places a greater weight and significance on the most recent data points, and keeps tracking a certain range of the history. 

{(2) Simple Moving Average (SMA) is a type of arithmetic average that is calculated by adding recent data and dividing the sum by the number of time periods. The works in~\cite{zhang2020swa,wang2022efficient,wang2022learn,athiwaratkun2018there,maddox2019simple,garipov2018loss,izmailov2018averaging,von2020neural,guo2022stochastic,cha2021swad,yang2019swalp,chen2020semi,kwon2020backpropagated} all use SMA to obtain the average multiple checkpoints, predictions or gradients of features during the learning process. SMA has been proven to result in wider optima and better generalization.
Zhang et al.~\cite{zhang2020swa} focus on using SMA after additional training. This approach maximizes information retention but increases additional training costs.
Other works use SMA in different ways, such as following the optimization trajectory~\cite{wang2022self, wang2022efficient,wang2022learn, maddox2019simple, garipov2018loss, guo2022stochastic}, processing training later~\cite{von2020neural}, sampling from a Gaussian distribution~\cite{izmailov2018averaging} or finding high-accuracy pathways between loss functions~\cite{athiwaratkun2018there}. Additionally, some studies use the average of loss to recognize the noisy data~\cite{huang2019o2u,xu2021small,yue2022ctrl,lazarou2021iterative,xia2021sample}. 
% These methods reduce the cost of training and find suboptimal solutions that are close to the optimal solution when averaged. 
\ylfChecked{These techniques aim to decrease the training cost while seeking suboptimal solutions that are in close proximity to the optimal solution when averaged.}
}
%\wuge{(2) Simple Moving Average (SMA) is an arithmetic moving average calculated by adding recent data and then dividing that figure by the number of time periods in the calculation average. The works~\cite{zhang2020swa,wortsman2022robust,wortsman2022model,wang2022self,wang2022efficient,wang2022learn,athiwaratkun2018there,maddox2019simple,garipov2018loss,izmailov2018averaging,von2020neural,guo2022stochastic} all adopt SMA to obtain the target average model of multiple checkpoints during the learning process, which is proved to have wider optima and better generalization. Among them, some works focused on using SMA at different locations and times, which utilize it after completing routine~\cite{wortsman2022robust, wortsman2022model} or additional training~\cite{zhang2020swa}. The advantage is maximizing the retention of complete information, but the disadvantage is also apparent in increasing the additional reasoning or training costs. Other work is to select different periods: following the trajectory of optimization~\cite{wang2022self, wang2022efficient,wang2022learn, maddox2019simple, garipov2018loss, guo2022stochastic} and the late processing of training~\cite{von2020neural}, or methods: sampling from Gaussian distribution~\cite{izmailov2018averaging}, and high-accuracy pathways between loss functions~\cite{athiwaratkun2018there}, while reducing the cost of training, finding more suboptimal solutions, and then average close to the optimal solution.}
%2) Simple Moving Average (SMA) is an arithmetic moving average calculated by adding recent data and then dividing that figure by the number of time periods in the calculation average. The works~\cite{zhang2020swa,athiwaratkun2018there,maddox2019simple,garipov2018loss,izmailov2018averaging} all adopt SMA to obtain the target average model of multiple checkpoints during learning process, which is proved to have wider optima and better generalization.
%\wuge{ By using Stochastic Weight Averaging (SWA), which averages multiple points using the trajectory of SGD, Maddox et al. [13] claimed that SWA can achieve better generalization performance. SWA identifies the models with the lowest learning rate values when utilizing a cyclical learning rate. For constant learning rates, SWA captures models at each epoch. Then all the weights from the collected networks are averaged to obtain the final model $\text {SWA }$:} 


\iffalse
\begin{figure}[t]
	\vspace{0pt}
	\begin{center}
		\setlength{\fboxrule}{0pt}
		\fbox{\includegraphics[width=0.4\textwidth]{./figs/RM_good.pdf}}
	\end{center}	
	\vspace{-10pt}
	\caption{ 
	Three benefits of the proposed RecursiveMix which iteratively leverages the historical input-prediction-label triplets: 1) an enlarged diversity of data space and richer supervisions, 2) adequate training signals for an instance with multi-scale/-space views, {and 3) explicit learning on the spatial semantic consistency, which can further benefits the downstream tasks.}
	}
	\label{fig_rm_good}
	\vspace{-5pt}
\end{figure}
\fi


\section{Discussion}

\subsection{Relation to Related Topics}

\noindent\textbf{Relation to Recurrent Network.} Recurrent neural networks (RNNs)~\cite{zaremba2014recurrent,hochreiter1997long}  are neural sequence models that have played an important role in sequential tasks like language modeling~\cite{mikolov2010recurrent}, speech recognition~\cite{graves2013speech} and machine translation~\cite{kalchbrenner2013recurrent,bahdanau2014neural}. A well-known algorithm called Back Propagation Through Time (BPTT)~\cite{werbos1990backpropagation,guo2013backpropagation} is adopted to update weights in RNNs. When training, RNNs are expanded over time and have shared model parameters at each time step. Note that the ``time step'' here and  the ``time'' of BPTT in RNNs is quite different from the concept of time in learning history--they are in fact defined in the perspective of sequential dimension, whilst the time in learning history refers to a complete training iteration (or epoch). Therefore, the information from all expanded steps of RNNs can be together viewed as a set of historical knowledge at the same moment during learning. The hidden states of RNNs are sometimes called ``memory''~\cite{hochreiter1997long}, where the memory is actually the intermediate feature map across the input context, instead of the historical representations in our topic. 


\noindent\textbf{Relation to Memory Network.} The series of Memory Network aims at reasoning with a external memory component~\cite{weston2014memory,graves2014neural,sukhbaatar2015end,chandar2016hierarchical} and is widely applied in Question Answering (QA) tasks.
%or prior knowledge~\cite{miller2016key,chunseong2017attend}. 
The external memory is usually designed to be read and written to, which effectively maintains the long-term information. Different from the stored past feature representations during training history, the memory matrix acts as a global knowledge base that directly supports the inference over the entire sequence. It is more similar to the hidden states of RNNs but with a global long-term mechanism and shared read/write functions.

\noindent\textbf{Relation to Ensemble learning.} {Ensemble learning is an umbrella term for methods that combine multiple inducers to make a decision~\cite{sagi2018ensemble}. 
%\implus{Many techniques for historical learning involve methods from ensemble learning, but ensemble learning does not necessarily rely on historical information. These two areas are independent of each other, but can be used together.} 
The utilization of ensemble learning methods is prevalent in historical learning, however, ensemble learning doesn't necessarily require historical information. These two concepts are separate, but can be combined.
Ensemble learning has advanced significantly in the field of machine learning~\cite{hansen1990neural},~\cite{dietterich2000ensemble},~\cite{caruana2004ensemble}. To achieve the best ensemble effect at the lowest cost, some work investigates how various hyperparameters~\cite{saikia2020optimized, wenzel2020hyperparameter }, architectures~\cite{zaidi2021neural, gontijo2021no }, strategies~\cite{levesque2016bayesian, wenzel2020hyperparameter, fei2022meta}, and distribution shift~\cite{ovadia2019can,mustafa2020deep} affect ensemble models. Ensemble learning primarily makes use of historical information by ensembling the historical results~\cite{huang2017snapshot, chen2017checkpoint} and parameters~\cite{wang2022efficient,wang2022learn} to enhance the model’s performance during training and inference. 
%However, two special circumstances cannot be regarded as related to historical learning. Some works ensemble the multi classifiers~\cite{ sensoy2018evidential,lakshminarayanan2017simple} and model~\cite{ruiz2019adaptative, yang2020resolution, wang2020glance, park2019feed} into one network at once simple training process, which does not constitute the integration, utilization, and update of classifiers or branches on the iteration or epoch. Past parameters, models, or other information are only stored and used for a short time and are not precipitated and transmitted to the next iteration or epoch like history. This behavior can be seen as a short information integration before and after training or reasoning process, not as historical information integration. 
However, two exceptional cases cannot be considered as part of historical learning. Some works ensemble multiple classifiers~\cite{sensoy2018evidential,lakshminarayanan2017simple} and models~\cite{ruiz2019adaptative, yang2020resolution, wang2020glance, park2019feed,radosavovic2018data} into a single framework through the straightforward training process, which doesn't involve the integration, utilization, and updating of classifiers or branches during iterations or epochs. The past parameters, models, or other information are only temporarily stored and utilized, rather than being accumulated and transferred to the subsequent iteration or epoch, as is typical in historical information. This can be viewed as a brief integration of information before and after the training or reasoning process, not as an integration of historical information.
%Similarly, the results of ensembling independent or parallel training models cannot be considered as historical learning~\cite{ solovyev2021weighted, lakshminarayanan2017simple }. There is no historical association between the results, which does not constitute the iterative storage, utilization, and update. In contrast to the research mentioned above~\cite{ huang2017snapshot, chen2017checkpoint }, the current results are not updated by the previous results utilizing historical data, but other results of independent or parallel training models.}
{Likewise, the outcomes of combining independent or parallel training models cannot be considered as historical learning~\cite{solovyev2021weighted, lakshminarayanan2017simple, wortsman2022model, wortsman2022robust,gupta2020stochastic}. For example, WiSE-FT~\cite{ wortsman2022robust } average the weights of zero-shot and fine-tuned models, and Model Soup~\cite{ wortsman2022model } average the weights of independently fine-tuned models. There is no historical connection between the results, and thus, there is no iterative storage, utilization, and updating. Unlike previous research~\cite{huang2017snapshot, chen2017checkpoint}, the current results are not enhanced by utilizing previous results and historical data, but by combining other results of independent or parallel training models.}
%\noindent\textbf{Relation to Ensemble learning.} \wuge{Ensemble learning is an umbrella term for methods that combine multiple inducers to make a decision~\cite{sagi2018ensemble}. \implus{Many techniques for historical learning involve methods from ensemble learning, but ensemble learning does not necessarily rely on historical information. These two areas are independent of each other, but can be used together.} Ensemble learning primarily makes use of historical information by ensembling the results of the historical model, parameters, or feature representation in different periods to enhance the model's performance during training and inference.Ensemble learning has advanced significantly in the field of machine learning~\cite{hansen1990neural},~\cite{dietterich2000ensemble},~\cite{caruana2004ensemble}. In recent years, as research has become more in-depth, ensemble learning has been used to quantify model uncertainty~\cite{kendall2017uncertainties},~\cite{lakshminarayanan2017simple},~\cite{fort2019deep} and enhance model accuracy and robustness. To achieve the best ensemble effect at the lowest cost, some work investigates how various hyperparameters~\cite{saikia2020optimized}, architectures~\cite{zaidi2021neural}, strategies~\cite{levesque2016bayesian},~\cite{wenzel2020hyperparameter},~\cite{fei2022meta}, distribution shift~\cite{ovadia2019can}, and~\cite{mustafa2020deep} affect ensemble models. Zaidi et al.~\cite{zaidi2021neural} construct ensembles with varying architectures, which include higher ensemble diversity and can have better performance than deep ensembles. Wenzel et al.~\cite{wenzel2020hyperparameter} take the method of random searching to ensemble weights and hyperparameters over different hyperparameters, which reduces the computational and memory costs. Gontijo et al.~\cite{gontijo2021no} conduct a comprehensive empirical investigation of models spanning hyper-parameters, architectures, frameworks, and datasets which discover that more significant divergence in training approach produces uncorrelated mistakes and increases ensemble accuracy.Significantly, some works only simply ensemble the multi classifiers~\cite{ sensoy2018evidential,lakshminarayanan2017simple}, branches~\cite{}, and model~\cite{ruiz2019adaptative, yang2020resolution, wang2020glance, park2019feed} into one network at once simple training process, which does not constitute the integration, utilization, and update of classifiers or branches on the iteration or epoch. Past parameters, models, or other information are only stored and used for a short time and are not precipitated and transmitted to the next iteration or epoch like history. This behavior can be seen as a short information integration before and after training or reasoning process, not as historical information integration.Similarly, the results of ensembling various models of independent training cannot be considered as historical learning~\cite{ solovyev2021weighted}. There is no association between the previous checkpoint and the next checkpoint, which does not constitute the iterative storage, utilization, and update. In contrast to the research mentioned above~\cite{}, the current checkpoints are not updated by the checkpoints utilizing historical data, but another checkpoint for independent training.
%\implus{Many techniques for historical learning involve methods from ensemble learning, but ensemble learning does not necessarily rely on historical information. These two areas are independent of each other, but can be used together.}
%Therefore, compared to History learning, Ensemble learning covers a smaller extensive scope.
%(3) judging XXXX by calculating the gradient value
%计划把Relation to Reinforcement Learning的内容精简一下

\noindent{\textbf{Relation to Reinforcement Learning.}} 
{Reinforcement learning (RL) is a subfield of machine learning that deals with learning how to control a system to maximize a long-term target value~\cite{szepesvari2010algorithms}. The concept of an experience replay buffer was first introduced by Lin et al.~\cite{lin1992self} and has since seen significant development and advancements in the field of RL~\cite{mnih2015human, lillicrap2015continuous, schaul2015prioritized, wang2016sample, andrychowicz2017hindsight}. The buffer is initialized first and network training is conducted once the number of data stored in the buffer exceeds a certain threshold. It has been extended to various RL tasks~\cite{haarnoja2018soft, savinov2018episodic, pong2019skew, bohmer2019exploration} and improved with related variants~\cite{schaul2015prioritized, horgan2018distributed, badia2020never}. {The experience replay buffer and memory bank~\cite{wang2020cross} have similarities and differences. The similarities lie in the fact that they both store information for training; the differences lie in that the target stored in the replay buffer can exist independently of the network~\cite{zhang2017deeper}, being related only to the RL environment and essentially serving as training data for RL. However, memory bank usually stores representations outputted by the network in its learning history.}
%are similar in that they both store information, but the experience replay buffer stores quadruples $(s, a, r, s^{\prime})$, where $s$ and $s^{\prime}$ represent the present and next states, and $r$ is the reward obtained for carrying out action $a$ in state $s$. The experience replay buffer stores quadruples sampled from previous buffers before network training~\cite{zhang2017deeper}, unlike the memory bank which stores instance-level feature representations related to the history generated during network training. 
Thus, the experience replay buffer is typically not considered as historical learning. Some studies use ensembling different information~\cite{wiering2008ensemble, pathak2019self, shyam2019model, chua2018deep, henaff2019explicit} to make better decisions, but these works cannot be considered as historical learning since they fail to utilize historical information during the optimization process of each individual model.}
%do not have a strict historical relationship between the integrated information and the storage, utilization, and update process of information in historical learning.}
%\noindent{\textbf{Relation to Reinforcement Learning.}} \wuge{Reinforcement learning refers to a subfield of machine learning, which refers to learning how to control a system to maximize some long-term target value~\cite{szepesvari2010algorithms}.  Lin et al.~\cite{lin1992self} first propose the experience replay buffer, then it has been developed for a long time and made some achievements in reinforcement learning~\cite{ mnih2015human, lillicrap2015continuous,schaul2015prioritized,wang2016sample,andrychowicz2017hindsight}. The specific process is to initialize itself first, and then conduct network training when the number of buffer data exceeds a certain value. In addition, some researches extend it to various tasks of reinforcement learning~\cite{ haarnoja2018soft, savinov2018episodic, pong2019skew, bohmer2019exploration }, and related variants~\cite{ schaul2015prioritized, horgan2018distributed, badia2020never} constantly improve its shortcomings. It is noteworthy that there are similarities and distinctions between experience replay buffer and memory bank~\cite{wang2020cross}. The same is that they both store information. The difference is that they store the content from several periods. Memory bank tends to store instance-level feature representations, which are related to the history generated in the process of network iterative training. On the contrary, the experience replay buffer tends to store the quadruple $\left(s, a, r, s^{\prime}\right)$. $s$ and $s^{\prime}$ represent the present and next states respectively. Besides, $r$ is the reward obtained for carrying out the action $a$ in state $s$. It pertains to storing the quadruple sampled from the previous buffer before the network iterative training~\cite{zhang2017deeper}, rather than the historical representation created during the network iterative training. As a result, the experience replay buffer is unrelated to history learning.Significantly, some studies ensemble different information~\cite{ wiering2008ensemble, pathak2019self, shyam2019model,chua2018deep , henaff2019explicit} to make better decision. However, there is no historical relationship between the integrated information, which cannot be strictly satisfied with the storage, utilization, and update process of information in history learning. Therefore, these works cannot be considered related to history learning.}
%The main works related to reinforcement learning and history learning are to take experience replay buffer to store and employ historical information. The experience replay buffer is cross iteration or episode in reinforcement learning, which is similar to the memory bank~\cite{wang2020cross}. The specific training process is to initialize itself first, and then conduct network training when the number of buffer data exceeds a certain value. Lin et al.~\cite{lin1992self} first propose the experience replay buffer, and it has been developed for a long time and made some achievements in reinforcement learning~\cite{ mnih2015human, lillicrap2015continuous,schaul2015prioritized,wang2016sample,andrychowicz2017hindsight}. In addition, some researches extend it to various tasks of reinforcement learning~\cite{ haarnoja2018soft, savinov2018episodic, pong2019skew, bohmer2019exploration }. The related variants~\cite{ schaul2015prioritized, horgan2018distributed, badia2020never} have gradually emerged and improved the shortcomings of the experience replay buffer. 
%\noindent{\textbf{Relation to Reinforcement Learning.}} \wuge{There are many overlapping parts between Reinforcement Learning (RL) and history learning. The use history of reinforcement learning mainly depends on the following two aspects: (1) off-policy algorithms use experience replay buffer to store and use historical information~\cite{savinov2018episodic,bohmer2019exploration,badia2020never,pong2019skew}. (2) making the most favorable decision based on ensembling the results of different models. First, the replay buffer is mainly updated by interacting with this part of the data. Schaul  et al.~\cite{schaul2015prioritized} prioritize experience to learn from experience replay more efficiently. By decoupling acting from learning, Horgan et al.~\cite{horgan2018distributed} let the actors interact with the environment and accumulate experience in a shared experience replay buffer. Haarnoja et al.~\cite{haarnoja2018soft} update experience replay buffer with mixed data interacting with the actual environment. Then Savinov et al.~\cite{savinov2018episodic} propose the Episodic Curiosity (EC) module to compute the state’s intra-episode novelty. EC constructs the replay buffer using episodic memory, which compares each state to those and rewards ones different from those in the replay buffer. In contrast, Badia et al.~\cite{badia2020never} build the episodic memory-based intrinsic reward using k-nearest neighbors over the visited states recorded in the replay buffer. To improve exploration efficiency, Bohmer et al.~\cite{bohmer2019exploration} construct a replay buffer to store experiences with the environment. Other decentralized agents also modify their local policies using this experience to enhance their exploratory behavior. Further, Pong et al.~\cite{pong2019skew} propose Skew-Fit to use states sampled from the replay buffer, giving rare states more weight. Skew-Fit can provide much better state space coverage and explore more effectively. If the real distribution under the present replay buffer is mutually independent of the dataset, policies may be invalid. Fujimoto et al.~\cite{fujimoto2019off} propose batch-constrained reinforcement learning to minimize the mismatch between the different batch pairs. In general, the experience replay buffer focuses on storing historical information and learning from it, while the ensembling models focus on ensemble historical results or decisions to get better performance.}
%\par \wuge{Ensemble different models can fully integrate the results of different agents, which better interact and explore the environment. Pathak et al.~\cite{pathak2019self} encourage the agent to explore in an entirely self-supervised manner by using ensemble dynamics models. This operation applies the mean of outputs as the final prediction and reduces the ensemble model's variance. Moreover, Model-Based Active eXploration(MAX)~\cite{shyam2019model} ensemble forward dynamics models to simulate and assess an exploration policy. The work of~\cite{henaff2019explicit} ensembles the disagreement between different models to encourage exploration. Then Sekar et al.~\cite{sekar2020planning} expand from low to high dimensional states by building a model-based zero-shot RL agent. In addition, Lakshminarayanan et al.~\cite{lakshminarayanan2017simple} ensemble multiple models predictions and adversarial training, which explore their use successfully for estimating predictive uncertainty. On this basis, Chua et al.~\cite{chua2018deep} propose a new probabilistic ensemble with trajectory sampling (PETS) method. According to PETS, models may have better performance by including uncertainty estimates. This can offset the gap in the asymptotic performance difference between model-based and model-free RL. Apart from this, Kalweit et al.~\cite{kalweit2017uncertainty} use an inaccurate ensemble model to assess the dynamics uncertainty, which mitigates the negative consequences of imaginary rollouts. Kurutach et al.~\cite{kurutach2018model} retain the model uncertainty while addressing the issue of model bias in model-based RL by assembling models. Buckman et al.~\cite{buckman2018sample} ensemble samples to substitute for both the model and Q-function, which calculate the ensemble samples’ variance to approximate the uncertainty. Further, Pan et al.~\cite{pan2020trust} assess the discrepancy between one model and the other models existing in an ensemble model to calculate the uncertainty for each sample.

%\noindent\textbf{Relation to Reinforcement Learning.}
%\footnote{Here the historical gradient w.r.t. model parameters that can be applied in momentum-based optimizers to any type of tasks is not considered.}

\subsection{Future Challenges}
\noindent\textbf{Advanced/Downstream Vision Tasks.} 
%\wuge{The majority of existing methods that use historical statistics concentrate on image classification and metric/representation learning, with little attention paid to downstream vision tasks such as visual tracking~\cite{ ma2015hierarchical, bertinetto2016fully, wang2018sint++ , danelljan2015convolutional}, object detection~\cite{ren2015faster,lin2017focal,tian2019fcos,li2020generalized}, and segmentation~\cite{noh2015learning,long2015fully,he2017mask,tian2020conditional,wang2020solo,wang2020solov2}. Despite this, history learning has not been widely utilized in these tasks. This paper will provide a brief overview of the current status of history learning in downstream tasks and highlight the challenges faced, with the aim of driving further advancement in this area.}
Most existing methods that utilize historical statistics focus on image classification and metric/representation learning, while giving little attention to downstream vision tasks such as visual tracking~\cite{ma2015hierarchical, bertinetto2016fully, wang2018sint++ , danelljan2015convolutional}, object detection~\cite{ren2015faster,lin2017focal,tian2019fcos,li2020Generalized}, and segmentation~\cite{noh2015learning,long2015fully,he2017mask,tian2020conditional,wang2020solo,wang2020solov2}. Although the potential benefits of historical learning in these tasks are significant, it has not been widely adopted. Semantically rich and task-specific elements or results can be generated during training and inference in many vision tasks, such as bounding boxes, keypoints, uncertainty, or other statistics. These pieces of information could be combined with learning history, such as improving the effectiveness of object detection based on the historical bounding boxes or improving semantic segmentation performance by leveraging the historical uncertainty. In short, incorporating task-specific elements with learning history can probably lead to a stronger model for these downstream vision tasks.
%Despite its potential, historical learning has not been widely adopted in these tasks. {Specifically, there are three major challenges in advanced and downstream vision tasks: (1) relying solely on a single source of historical information. (2) limited integration of various historical information sources. (3) scant research on the effective utilization of historical information.}
%This paper aims to provide an overview of the current state of history learning in downstream tasks, highlight the challenges faced, and encourage further advancements in this field.
%\noindent\textbf{Advanced/Downstream Vision Tasks.} \wuge{While existing methods using historical statistics mainly focus on the subjects of image classification and metric/representation learning, little efforts have been paid on the downstream vision tasks, e.g., visual tracking~\cite{ ma2015hierarchical, bertinetto2016fully, wang2018sint++ , danelljan2015convolutional}, object detection~\cite{ren2015faster,lin2017focal,tian2019fcos,li2020generalized}, and segmentation~\cite{noh2015learning,long2015fully,he2017mask,tian2020conditional,wang2020solo,wang2020solov2}. However, history learning is rarely used in these downstream tasks. Next, we will briefly analyze the progress of history learning in downstream tasks and focus on the current challenges, hoping to promote the development of history learning in downstream tasks.}

%\par {%The three primary challenges in advanced and downstream vision tasks are: (1) Using single historical information. (2) Limited combination of different historical information. (3) Lack of in-depth study on utilizing historical information.
%Most work on historical learning in some vision tasks only uses a specific historical module, especially in visual tracking where historical information is mainly applied by creating memory banks to store information or by improving the tracker's ability through ensemble historical representations~\cite{yang2018learning,yang2019visual,gao2020recursive,fu2021stmtrack}. However, there is little exploration of past prediction ensembling, previous model parameters, gradient-guided mechanisms, or identifying hard or easy samples by loss values. The use of a specific type of previous information and different combinations of historical information may enhance the model's performance and robustness.

%In object detection, various historical information such as intermediate feature representation~\cite{ zhuang2019local, xie2021detco, zhang2022semi }, model parameters~\cite{wang2022omni, xu2021end, yang2021interactive, liu2021unbiased, liu2022unbiased, cai2019exploring, deng2021unbiased}, and gradients~\cite{tan2021equalization, li2022equalized} have been used for training or inference. Although a wide range of historical information is used, there is limited work on combining multiple historical information to improve performance in different situations.

%Similarly, in the field of segmentation, most recent works primarily focus on memory banks of instance-level feature representations~\cite{ alonso2021semi, lee2021weakly, li2022recurrent, liu2021one, wang2022semi}, center loss of memorizing feature statistics~\cite{fan2018associating, liu2020leveraging}, and mean teacher methods of constructing teachers from past model parameters~\cite{liu2022perturbed, cui2019semi, perone2018deep}. 

%However, these approaches mostly utilize historical information at a surface level and embed only a specific historical learning module into the original network. Additionally, there have been attempts to utilize historical model parameters for various tasks such as 3d analysis~\cite{cheng2022autoregressive, yu2019uncertainty}, action proposal~\cite{ wang2021self}, crowd counting~\cite{meng2021spatial}, facial affective behavior analysis~\cite{wang2021multi}, and human pose estimation~\cite{kim2022pose}. However, most research still confines itself to utilizing only one or a few types of historical information, instead of utilizing diverse combinations of it. Given the growing complexity of these advanced tasks, there is room for improvement in the diversity of learning history and the ways in which it is used. %Future work should aim to explore new types of historical information, combine different historical information, and sincerely integrate historical learning.
}
%\par \wuge{Three primary issues plague advanced and downstream vision tasks: (1) employing single historical information. (2) a few combinations of different historical information. (3) no in-depth study on untiling historical information. The first issue is mainly to explain that most of the work related to history learning in some vision tasks only uses a certain historical module, which is particularly obvious in visual tracking. Visual tracking applies historical information mainly by building diverse memory banks to store historical information~\cite{yang2018learning},~\cite{yang2019visual},~\cite{gao2020recursive},~\cite{fu2021stmtrack} or improve the tracker's ability through ensemble historical representations. It can be inferred that the above works concentrate mainly on intermediate feature representation. But there is little research to explore past prediction ensembling, previous model parameter, radient-guided mechanism, or identifying hard or easy samples by loss values. The performance of the model may be enhanced by a particular kind of previous information, and distinct combinations of historical information may further improve performance and robustness. In object detection, some studies use various historical information for training or inference: intermediate feature representation~\cite{ zhuang2019local, xie2021detco, zhang2022semi }, model parameter~\cite{ wang2022omni, xu2021end, yang2021interactive, liu2021unbiased, liu2022unbiased, cai2019exploring, deng2021unbiased} and gradient~\cite{tan2021equalization, li2022equalized}. The range of historical information used in object detection is relatively wide. But there is little work to combine multiple historical information, which may help improve performance in different situations. Reviewing the existing achievements, although aspects of the historical type are used in all kinds of models, they have not further explored the use of history learning or improved on a deeper level. For example, the majority of the recent works on history learning in the realm of segmentation primarily focus on the memory bank of recording instance-level feature representations~\cite{ alonso2021semi, lee2021weakly, li2022recurrent, liu2021one, wang2022semi}, center loss of memorizing feature statistics~\cite{fan2018associating, liu2020leveraging} and Mean teacher of constructing the teachers from past models parameter~\cite{liu2022perturbed, cui2019semi, perone2018deep}. However, most of these approaches employ historical information at a surface level or embed a particular module related to history learning into the original network. We hope to explore new historical types, combine different historical information, and integrate with history learning sincerely in the foreseeable future.}
%\par \wuge{In object detection, some studies use various historical information for training or inference: intermediate feature representation~\cite{ zhuang2019local, xie2021detco, zhang2022semi }, model parameter~\cite{ wang2022omni, xu2021end, yang2021interactive, liu2021unbiased, liu2022unbiased, cai2019exploring, deng2021unbiased} and gradient~\cite{tan2021equalization, li2022equalized}.The range of historical information used in object detection is relatively wide. There is a lot of work to use or combine multiple historical information, which also helps improve performance in different situations. We hope to explore new historical types or combine different historical information in depth in future research.}
%\par  \wuge{The majority of the recent works on history learning in the realm of segmentation primarily focus on the memory bank of recording instance-level feature representations~\cite{ alonso2021semi, lee2021weakly, li2022recurrent, liu2021one, wang2022semi}, center loss of memorizing feature statistics~\cite{fan2018associating, liu2020leveraging} and Mean teacher of constructing the teachers from past models parameter~\cite{liu2022perturbed, cui2019semi, perone2018deep}. It can be inferred that the above works concentrate mainly on intermediate feature representation and model parameter. But there is little research to explore past prediction ensembling, radient-guided mechanism, and identifying hard or easy samples by loss values. }
%\par \wuge{Visual Tracking applies historical information mainly by building various memory banks to store historical information~\cite{yang2018learning},~\cite{yang2019visual},~\cite{gao2020recursive},~\cite{fu2021stmtrack} or improve the tracker's ability through ensemble historical representations. Yang et al.~\cite{yang2019visual} propose a dynamic memory network constructed by an external memory bank and LSTM. The network takes CNN to extract features and uses an external memory bank to store templates. LSTM controls the external memory bank to read out the matching template to get the final template. Then search image features convolve with the final template and predict the target bounding box. This network that dynamically controls template reading and writing can encourage the tracker to memorize and adapt to the change in target appearance. Previous works have employed historical representations to improve the network's performance in visual tracking. However, most of these approaches employ historical information at a surface level and do not go much further. Therefore, it is hoped that in the foreseeable future, more work will be deeply integrated with history learning in visual tracking.}
%\par \wuge{In addition, some works take the using the historical model parameter for different tasks such as 3D~\cite{cheng2022autoregressive, yu2019uncertainty }, action proposal ~\cite{ wang2021self}, crowd counting~\cite{meng2021spatial}, facial affective behavior analysis~\cite{wang2021multi} and human pose estimation~\cite{kim2022pose}. However, most of the research is still limited to using one or few historical information, rather than using different combinations of historical information. Considering the increasing complexity of these advanced tasks, the diversity of learning history can be richer, and the way to make use of these historical statistics can be more flexible.}

\noindent\textbf{Rarely Explored Directions.} Although several types of historical information (e.g., intermediate feature representations and model parameters) have been extensively explored, there is a considerably large room for many other directions--the historical predictions, inputs, labels, and gradients to the feature maps. These directions, along with the potential combinations of all types, lack deep discussion and further exploitation currently. 
%\wuge{The area of gradients and loss is in need of further exploration. Currently, the main focus is on using historical gradients of model parameters, but their utilization is limited to through optimizers. There is a need to explore more beneficial techniques. For example, Tuluptceva et al. \cite{tuluptceva2020perceptual} utilized the historical L2-norm of unique model parameters' derivatives to balance different losses. In contrast, historical gradients of all levels also remain underutilized. More work is needed to explore various methods of using the historical gradient of features, such as combining it with information on different categories. The same is true for historical loss, where most research has been focused on recording its value for data cleaning during training. Although some works have used the historical loss to clean data more effectively \cite{ xu2021small, huang2019o2u, xia2021sample, yue2022ctrl}, there is potential for further improvement by combining it with other feature representations, such as using historical loss to calculate the weight of feature maps between different channels.}
Typically, the field of gradients and loss requires further investigation. Currently, the emphasis is mainly on utilizing historical gradients of model parameters through optimizers, but there is room for more effective methods. Tuluptceva et al.~\cite{tuluptceva2020perceptual} leverage the historical L2-norm of unique model parameters' derivatives to balance losses. However, the potential of historical gradients of all levels has yet to be fully explored. Further research is needed to discover new ways to use historical gradients of features, such as incorporating information on different categories. The same is true for historical loss, where most studies have only focused on using it for data cleaning during training. Although some works have utilized historical loss for effective data cleaning~\cite{xu2021small, huang2019o2u, xia2021sample, yue2022ctrl}, there is scope for improvement by combining it with other feature representations, such as using historical loss to determine the weight of feature maps between channels.
%\wuge{This is especially true in the area of gradients and loss. Current research mainly focuses on using historical gradients of model parameters and features. The historical gradients of model parameters are only utilized in one way, mainly through optimizers. There is a need to explore more beneficial techniques. For example, Tuluptceva et al. \cite{tuluptceva2020perceptual} use the historical L2-norm of the unique model parameters’ derivatives to balance different losses. In contrast, historical gradients of all levels are underutilized. Little research has been done to determine how to use them to improve model performance. Further exploration into how to use historical gradient with other historical information is necessary. The same holds true for historical loss, where most research focuses on recording its value to clean data during training. Although some work has delved into cleaning data more effectively through historical loss~\cite{ xu2021small, huang2019o2u, xia2021sample, yue2022ctrl}, more techniques could be combined with other historical information for even better results.}
%\wuge{The above situations are particularly obvious in the study of gradients and loss. In the present research, the historical gradients of the model parameters and all-level features are primarily used. On the one hand, the model parameters' historical gradients are only employed in one way. The majority of their updates come from optimizers. It is necessary to expand more beneficial techniques. For instance, Tuluptceva et al. \cite{tuluptceva2020perceptual} retain the historical L2-norm of the unique model parameters’ derivative to balance the various losses. On the other hand, all-level historical gradients are rarely applied. Only a little research has to be done to determine how to use them to enhance model performance. We hope that future research can deeply explore how to use the historical gradient with other historical information.
%Similar situations also occur in the historical loss. The primary research is too single, which records the historical loss's value to clean the data during training. Even though some work has deeply studied how to better clean data through historical loss~\cite{ xu2021small, huang2019o2u, xia2021sample, yue2022ctrl}, it is expected that more techniques may be utilized in combination with other historical information.}

\noindent\textbf{Extensions to Other Fields.} Currently, it is observed that the majority of existing historical learning approaches focus on computer vision (CV) tasks. However, natural language processing (NLP) tasks~\cite{devlin2018bert,radford2018improving,radford2019language,brown2020language,ouyang2022training} have yet to heavily incorporate historical mechanisms\footnote{Here, we exclude advanced optimizers that use historical gradients, which are applicable to all fields.}. This presents a promising opportunity to adapt historical learning mechanisms to NLP. Nevertheless, several challenges arise when extending similar ideas from CV to NLP tasks, mainly because the data format of language differs significantly from that of images. For example, while images can be augmented with flexible operations such as mix, crop, and resize, language is the opposite. The flexibility of operations in CV make it easier for models to incorporate historical information. Therefore, the effective utilization of historical learning mechanisms in NLP is still an open area that has great potentials to be explored and developed.
%Currently, we have discovered that most historical learning approaches focus on the area of computer vision (CV) tasks. However, the natural language processing (NLP) tasks~\cite{devlin2018bert} seem not to heavily rely on the historical mechanism till today\footnote{Here we do not consider the advanced optimizers using historical gradients which can be applied to all fields.}. This certainly poses a great chance to adapt historical learning mechanism to NLP. Nevertheless, challenges still exist when extending the similar ideas from CV to NLP tasks because the data format of language is quite different from images. For example, the image is flexible in augmentations (e.g., mix, crop, resize) whilst the language is the opposite. The flexible operators make it easier for CV models to utilize the historical information.

\noindent\textbf{Learning Patterns in Learning History.} We suspect that the learning trajectory may have meaningful patterns to guide efficient and robust training of the original models. It can be possible to train, e.g., a meta network~\cite{munkhdalai2017meta} for discovering such patterns through the recorded historical variables, or treat them as meta data~\cite{vanschoren2018meta} to learn new tasks much faster than otherwise possible.  %For example, we may discover the hard example or complex feature point, via the pattern of its learning dynamics during the entire optimization history.










\section{Conclusion}
In this paper, we systematically formulate the topic ``Historical Learning: Learning Models with Learning History'' by reviewing the existing approaches from the detailed three perspectives (i.e., what, where, and how), discuss their relation to similar topics and show promising directions for future exploration. 
We hope this topic can serve as a meaningful, inspiring, and valuable design principle, and this survey can inspire the community to explore continuously. 






% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

\iffalse
%\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...
\fi

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


{\small
	\bibliographystyle{IEEEtran}
	\bibliography{IEEEtran}
}
% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}



% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

\iffalse

\begin{IEEEbiographynophoto}
{Xiang Li} is an Associate Professor in College of Computer Science, Nankai University. He obtained the Ph.D. degree from Nanjing University of Science and Technology, Jiangsu, China, in 2020. 
His research interests include CNN/Transformer backbone, object detection, knowledge distillation and self-supervised learning. He has published 20+ papers in top journals and conferences such as T-PAMI, CVPR, NeurIPS, etc.
\end{IEEEbiographynophoto}
\vspace{-10pt}   


\begin{IEEEbiographynophoto}
{Ge Wu} is currently pursuing Ph.D. in College of Computer Science,
Nankai University. 
%a B.S. student with the College of Software, Henan University.
His research interests include  historical learning and object detection.
\end{IEEEbiographynophoto}
\vspace{-10pt}    
%{Ge Wu} received his B.S. degrees from Henan University, China in 2023. He is currently a Ph.D. student with the College of Computer Science, Nankai University.His research interests include object detection.


\begin{IEEEbiographynophoto}
{Lingfeng Yang} received his B.S. degrees from Nanjing University of Science and Technology, China in 2020. He is currently a Ph.D. student with the Department of Computer Science and Engineering, Nanjing University of Science and Technology.
His research interests include object detection, defect detection and fine-grained visual categorization.
\end{IEEEbiographynophoto}
\vspace{-10pt}    


\begin{IEEEbiographynophoto}
{Wenhai Wang} is a research scientist at Shanghai AI Laboratory. Before that, he received his PhD degree from the Department of Computer Science, Nanjing University in 2021. His main research interests include CNN/Transformer backbone, object detection, semantic/instance/panoptic segmentation, vision-language model, autonomous driving perception, and optical character recognition. He has published 20+ papers in vision journals and conferences such as T-PAMI, CVPR, ICCV, ECCV, etc.
\end{IEEEbiographynophoto}
\vspace{-20pt}    


\begin{IEEEbiographynophoto}
{Renjie Song} received B.S. degree from the Nanjing University of Science and Technology in 2015 and M.S. degree from Nanjing University in 2018. He is currently a Researcher with the Megvii Research Nanjing.
His current research interests include deep learning, computer vision, and their applications.
\end{IEEEbiographynophoto}
\vspace{-10pt}    


\begin{IEEEbiographynophoto}
{Jian Yang} received the Ph.D. degree from Nanjing University of Science and Technology (NUST), on the subject of pattern recognition and intelligence systems in 2002. In 2003, he was a Postdoctoral researcher at the University of Zaragoza. From 2004 to 2006, he was a Postdoctoral Fellow at Biometrics Centre of Hong Kong Polytechnic University. From 2006 to 2007, he was a Postdoctoral Fellow at Department of Computer Science of New Jersey Institute of Technology. Now, he is a Chang-Jiang professor in the School of Computer Science and Technology of NUST. He is the author of more than 200 scientific papers in pattern recognition and computer vision. His papers have been cited more than 6000 times in the Web of Science, and 17000 times in the Scholar Google. His research interests include pattern recognition, computer vision and machine learning. Currently, he is/was an associate editor of Pattern Recognition, Pattern Recognition Letters, IEEE Trans. Neural Networks and Learning Systems, and Neurocomputing. He is a Fellow of IAPR. 
\end{IEEEbiographynophoto}
\vspace{-10pt}    

\fi
% \begin{IEEEbiographynophoto}
% {Xiang Li} is an Associate Professor in College of Computer Science, Nankai University. He obtained the Ph.D. degree from Nanjing University of Science and Technology, Jiangsu, China, in 2020. 
% His research interests include CNN/Transformer backbone, object detection, knowledge distillation and self-supervised learning. He has published 20+ papers in top journals and conferences such as T-PAMI, CVPR, NeurIPS, etc.
% \end{IEEEbiographynophoto}
% \vspace{-20pt}    


%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


