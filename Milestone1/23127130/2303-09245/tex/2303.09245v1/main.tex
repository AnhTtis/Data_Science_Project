% Template for ICASSP-2019 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
% \usepackage{indentfirst}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{color}
\usepackage{pifont}
\usepackage{hyperref}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\newcommand{\methodname}{CHS-Net\xspace}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\shan}[1]{\textcolor{blue}{HM: #1}}
\newcommand{\gao}[1]{\textcolor{green}{JG: #1}}
\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\ie}{\emph{i.e.}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\eg}{\emph{e.g.}\xspace}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1.5pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
% Title.
% ------
\title{cross-head supervision for crowd counting with noisy annotations}
%
% Single address.
% ---------------
\name{Mingliang Dai$^{1}$ \qquad Zhizhong Huang$^{1}$ \qquad Jiaqi Gao$^{1}$ \qquad Hongming Shan$^{2}$ \qquad Junping Zhang$^{1\dagger}$\thanks{$\dagger$: Corresponding author.  jpzhang@fudan.edu.cn. This work was supported in part by the National Natural Science Foundation of China (Nos. 62176059 and 62101136), the Shanghai Municipal Science and Technology
Major Project (No. 2018SHZDZX01), the Zhangjiang Laboratory (ZJLab) and the Shanghai Center for Brain Science and Brain-Inspired Technology.}}
\address{$^1$ Shanghai Key Lab of Intelligent Information Processing, School of Computer Science\\
$^2$ Institute of Science and Technology for Brain-inspired Intelligence\\
Fudan University, Shanghai 200433, China}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
% \ninept
%
\maketitle
%
\begin{abstract}
%Crowd counting is a challenging computer vision task due to multi-scale head sizes, high occlusion, etc. 
Noisy annotations such as missing annotations and location shifts often exist in crowd counting datasets due to multi-scale head sizes, high occlusion, etc.
%is a challenging computer vision task due to multi-scale head sizes, high occlusion, etc. 
These noisy annotations severely affect the model training, especially for density map-based methods. 
To alleviate the negative impact of noisy annotations, we propose a novel crowd counting model with one convolution head and one transformer head, in which these two heads can supervise each other in noisy areas, called \textbf{C}ross-\textbf{H}ead \textbf{S}upervision. The resultant model, \methodname, can synergize different types of inductive biases for better counting.
%novel cross-head supervision model, named \methodname, which can supervise each other in noisy areas by leveraging different types of inductive biases.
In addition, we develop a progressive cross-head supervision learning strategy to stabilize the training process and provide more reliable supervision.
%Despite continuous efforts to improve the counting performance, the labeling noise issues have not received sufficient attention. 
%However, the performance of crowd counting suffers from the potential noises in the labeling process, such as missing annotations and location shifts.
%, may have a significant negative impact on the counting performance. 
%To mitigate this issue, therefore, we propose a novel model, named \methodname, consisting of two complementary heads to supervise each other in noisy areas. In addition, we develop a progressive 
%cross-head supervision learning strategy to stabilize the training process and provide more reliable supervision.
Extensive experimental results on ShanghaiTech and QNRF datasets demonstrate superior performance over state-of-the-art methods. Code is available at \url{https://github.com/RaccoonDML/CHSNet}.
\end{abstract}

%\shan{some general suggestions: (1) this paper should be reviewed anonymously; no github url is required. (2) use commands to denote some abbreviations so that you can easily rename them; see \methodname. (3) use a tilde mark to connect the text and cite, ref.; for example, xxxx~\cite{bai2020adaptive}.  }

\begin{keywords}
Crowd counting, noisy annotations
% , cross-head supervision
\end{keywords}

\input{section/introduction}
\input{section/methodology}
\input{section/experiments}
\vspace{-10pt}
\section{Conclusion}
\vspace{-5pt}
Noisy annotations are common in crowd counting datasets. To alleviate the negative impact of noisy annotations, we propose \methodname, a network with a convolution head and a transformer head to mutually supervise each other in noisy areas. In addition, we develop a progressive cross-head supervision learning strategy to stabilize training process and provide more reliable supervision. Experimental results show the superior performance of our proposed approach.
For future work, we will explore more noise robust loss functions to further utilize the ability of \methodname. Besides, we may consider to enhance the model sustainable learning ability like~\cite{gao2023forget} so that the noise ratio of training samples from different domains can be adaptively adjusted.

\clearpage
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
