\section{Methodology}
% 因为卷积有很强的局部建模能力，而transformer能有效捕获全局上下文依赖关系，所以我们针对数据中存在的标签噪声，在训练后期让两个分支预测的密度图作为彼此的监督信息，互相监督。从而有效降低标注噪声对训练的影响，并且充分利用两个网络的学习偏好进行优势互补。
% In this section, we will illustrate the network structure of \methodname and progressive cross-head supervision learning strategy. % separately.

\subsection{Network Architecture}
Convolutions have strong local modeling ability while transformers~\cite{vaswani2017attention} can effectively capture global context dependencies. We tackle the noisy annotations present in the data and serve the predictions from two heads as each other's supervision in noisy areas. Thus, the negative impact of noisy annotations on training is effectively reduced. The inductive biases of the two heads are fully utilized to complement each other. 

Fig.~\ref{fig:model} presents the proposed \methodname consisting of the following components
% \shan{3 followed by 2 points?}
: 1) a shared encoder $E$ that extracts the features $E(\mat{X})\in \mathbb{R}^{C\times H\times W}$ from the input image $\mat{X}$; and 2) two regression heads including a convolution head $H_{\mathrm{conv}}$ and a transformer head $H_{\mathrm{tran}}$ to predict the density maps $H_{\mathrm{conv}}\left(E(\mat{X})\right)\in \mathbb{R}^{H\times W}$ and $H_{\mathrm{tran}}\left(E(\mat{X})\right)\in \mathbb{R}^{H\times W}$, where $C$, $H$ and $W$ are the channels, height, and width of the features maps, respectively.
At the end of each head, we adopt an upsampling layer, three convolution blocks, and ReLU activation function to aggregate features and output the density map.
The encoder $E$ adopts VGG16~\cite{simonyan2014very} without the last maxpooling layer and fully-connected layers pre-trained on ImageNet~\cite{deng2009imagenet} for initialization. 

On the one hand, the convolution head $H_{\mathrm{conv}}$ consists of a series of convolution blocks. Each block is stacked by dilated convolution, batch normalization, and ReLU activation layers, which models the local contextual information.
On the other hand, the transformer head $H_{\mathrm{tran}}$ contains several transformer layers, whose inputs are the flattened features of $E(\mat{X})$ along $H$ and $W$, to capture global contextual dependencies of current features.
% \blue{please add a sentence like supposed to model the local context relationship.}
% ~\gao{confused with this sentence. CNN feature embedding as the transformer input? Need clarify, cannot understand}
For the sake of simplification, we omit the intermediate results and use $\mathcal{F}_\mathrm{conv}(\mat{X})$ and $\mathcal{F}_\mathrm{tran}(\mat{X})$ to denote the predictions of two heads.

\subsection{Cross-head Supervision}
% 由于两个分支网络在前期并不能生成可靠的密度图用于互相监督，所以采用渐进的互监督学习策略，其中每个分支的监督信息由互补分支预测结果和GT加权组合而成。并且随着训练的进行，逐渐增大互补分支的权重。同时为了使前期的训练过程稳定，噪声比例也是随着训练的进行逐渐增大到预设的最大噪声比例。
% 形式化的:
Since the two regression heads generate unreliable density maps at the early stage, we propose a progressive cross-head supervision learning strategy to stabilize the training process. The refined supervision of each head is a weighted combination of another prediction and the ground truth density map. The weights of the complementary head are gradually increased as the training proceeds. Formally, the refined supervision of convolution head $\widehat{\mat{D}}_\mathrm{conv}$ is defined as follows:
\begin{equation}
    \widehat{\mat{D}}_\mathrm{conv}=\alpha\mathcal{F}_\mathrm{tran}(\mat{X})+(1-\alpha) \mat{D}_\mathrm{gt},
\end{equation}
where $\alpha$ is the combination coefficient to control the importance of two terms and $\mat{D}_\mathrm{gt}$ is the ground truth density map which is usually mislabeled in some area.

Considering that noisy annotations only account for a small part of all annotations, in our method, only a specific mislabeled area uses the refined supervision, while the other areas should use the original ground truth. 
% , which is controlled by noise ratio $\delta$
Typically, the mislabeled examples usually have a large loss~\cite{arazo2019unsupervised,huang2023twin}, as the model would predict the correct labels if it is trained well. Therefore, to select those mislabeled areas, in practice, we first sort the deviation $\mat{\epsilon}=| \mathcal{F}_\mathrm{conv}(\mat{X})-\mat{D}_\mathrm{gt} | \in \mathbb{R}^{W\times H} $ in descending order and obtain the top $\delta\in [0,1]$ value as the mask threshold, denoted as $t_\delta$. In a sense, $\mat{\epsilon}$ describes the discrepancy between ground truth and model prediction. 
% $W$ and $H$ represent the width and height of output density map respectively.
Then the selection mask of the convolution head is given by:
\begin{align}
    \mat{M}_\mathrm{conv} = \mathbb{I}(\mat{\epsilon} \geq t_\delta) \in \{0,1\}^{W\times H},
\end{align}
where $\mathbb{I}(\cdot)$ is an indicator function. 
% \blue{In a sense, }
% \begin{equation}
% M_{ij} =\begin{cases}
% 1 & E_{ij} \geqslant t_\delta\\
% 0 & E_{ij} < t_\delta
% \end{cases}.
% \end{equation}

Once we have the selection mask, the final supervision for convolution head can be calculated by:
\begin{align}
    \mat{D}_\mathrm{conv}=\mat{M}_\mathrm{conv}\odot & \widehat{\mat{D}}_\mathrm{conv}+(1-\mat{M}_\mathrm{conv})\odot \mat{D}_\mathrm{gt} \\
    =\alpha \mat{M}_\mathrm{conv} \odot &\mathcal{F}_\mathrm{tran}(\mat{X}) + (1-\alpha \mat{M}_\mathrm{conv}) \odot  \mat{D}_\mathrm{gt},
\end{align}
where $\odot$ represents the element-wise multiplication. Similarly, the final supervision for transformer head $\mat{D}_\mathrm{tran}$ can be calculated using the prediction result by the convolution head. Finally, the overall loss function for optimization is 
\begin{align}
    \mathcal{L}={\| \mathcal{F}_\mathrm{conv}(\mat{X})-\mat{D}_\mathrm{conv} \|}^2_2+
    {\| \mathcal{F}_\mathrm{tran}(\mat{X})-\mat{D}_\mathrm{tran} \|}^2_2.
\end{align}
As a result, the supervision of mislabeled areas is refined from another head.

\subsection{Progressive Learning Strategy}
The predicted density maps of \methodname are unstable in the early stage of training. Therefore, they cannot be directly used for cross-head supervision. To make the early training process stable, we develop a progressive cross-head supervision learning strategy, that is, 
% Besides, to make the early training process stable,
the noise ratio $\delta$ and the combination coefficient $\alpha$ are linearly increased to the preset maximum value as the training process goes on. Formally, the noise ratio and the combination coefficient at the $i$-epoch are calculated as follows:
\begin{align}
    \delta_i=\delta_\mathrm{max}* i/T,\quad
    \alpha_i=\alpha_\mathrm{max}* i/T,
\end{align}
where $\delta_\mathrm{max}$ and $\alpha_\mathrm{max}$ are the predefined maximum noise ratio and maximum combination coefficient, respectively. $T$ denotes the maximum epoch for training.
% \shan{revised}

