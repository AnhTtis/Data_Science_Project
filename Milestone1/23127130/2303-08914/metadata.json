{
    "arxiv_id": "2303.08914",
    "paper_title": "MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge",
    "authors": [
        "Wei Lin",
        "Leonid Karlinsky",
        "Nina Shvetsova",
        "Horst Possegger",
        "Mateusz Kozinski",
        "Rameswar Panda",
        "Rogerio Feris",
        "Hilde Kuehne",
        "Horst Bischof"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Large scale Vision-Language (VL) models have shown tremendous success in aligning representations between visual and text modalities. This enables remarkable progress in zero-shot recognition, image generation & editing, and many other exciting tasks. However, VL models tend to over-represent objects while paying much less attention to verbs, and require additional tuning on video data for best zero-shot action recognition performance. While previous work relied on large-scale, fully-annotated data, in this work we propose an unsupervised approach. We adapt a VL model for zero-shot and few-shot action recognition using a collection of unlabeled videos and an unpaired action dictionary. Based on that, we leverage Large Language Models and VL models to build a text bag for each unlabeled video via matching, text expansion and captioning. We use those bags in a Multiple Instance Learning setup to adapt an image-text backbone to video data. Although finetuned on unlabeled video data, our resulting models demonstrate high transferability to numerous unseen zero-shot downstream tasks, improving the base VL model performance by up to 14\\%, and even comparing favorably to fully-supervised baselines in both zero-shot and few-shot video recognition transfer. The code will be released later at \\url{https://github.com/wlin-at/MAXI}.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08914v1"
    ],
    "publication_venue": null
}