\begin{table*}[!tb]%[!htbp]
\footnotesize
\centering
% \begin{tabular}{@{}c M{0.8cm} M{0.8cm} M{0.8cm} M{0.7cm} M{0.7cm}M{0.7cm}@{}}
% \begin{tabular}{@{}l @{\hspace{2em}}cccccccccccccc}
\begin{tabular}{ccccccccccccccc}
\toprule
Temp. attention layers & UCF101 & HMDB51 & K600 & MiniSSv2 & Charades & UAV Human & Moments-in-time \\
% \midrule
% \multicolumn{2}{c}{CLIP~\cite{clip} (w/o finetune) Zero-Shot} & 69.93 / 92.7 & 38.02 / 66.34 & 63.48 / 86.80 & 3.96 / 14.42 & 19.80 & 1.79 / 7.05 & 20.11 / 40.81 \\
\midrule
None & \textbf{78.17} & \textbf{52.24} & \textbf{71.43} & \textbf{6.37} & \textbf{23.79} & \underline{2.72} & \textbf{22.91}  \\
% MiniKinetics & 200 & 75.10 / 95.82 & \underline{48.34} / \underline{76.95} & \underline{69.23} / 90.92 & \textbf{6.50} / \textbf{18.76} & \underline{22.70} & \underline{2.40} / \underline{8.04} & \textbf{22.50} / \textbf{46.01} \\
2 & \underline{77.38} & 51.83 & \underline{70.41} & 5.98 & \underline{22.87} & \textbf{2.90} & 22.51 \\
6 & 75.91 & \underline{51.92} & 69.23 & \underline{6.09} & 21.78 & 2.52 & \underline{22.52}  \\
% UCF101 & 101 & -  \\

\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{
Cross-frame temporal attention modules. we report the zero-shot transfer performance after finetuning CLIP on K400. We train with text bags of GPT3 verbs and BLIP verbs. We set the text bag filtering ratio $p=90\%$. Adding temporal attention module does not lead to performance improvement. 
}
\label{tab:supp_tempt_att}
% \label{tab:yti_state_of_the_art}
\end{table*}