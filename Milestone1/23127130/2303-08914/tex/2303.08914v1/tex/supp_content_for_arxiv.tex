For further insights into our approach \OurMethod, we introduce more dataset statistics (Sec.~\ref{sec:dataset_statistics}) and implementation details (Sec.~\ref{sec:implementation_detials}) of \OurMethod. 

In the additional results, we provide comparison of visualizations of attention heatmaps across several approaches in Sec.~\ref{sec:attention-heatmaps}. Furthermore, we report more results of finetuning with noisy action dictionary (Sec.~\ref{sec:robustness_noisy_dictionary_more}), and provide more examples of language sources used for training (Sec.~\ref{sec:examples_of_language_sources}). Lastly, we explore a cross-frame attention temporal module in Sec.~\ref{sec:parameter_free_temp_module}. 

% \section{Source Code}\label{sec:source_code}
% We provide the source code in the directory \verb$supplementary_3246/source_code/MAXI$. Check \verb$README.md$ for instructions of re-evaluation. The source code will be released upon acceptance. 

\input{fig/supp_attn_in_dict_samples}

\section{Dataset Statistics}\label{sec:dataset_statistics}
\textbf{Kinetics-400} (K400)~\cite{kay2017kinetics}  is the most popular benchmark for action recognition tasks, containing around 240K training videos in 400 classes. The dataset consists of YouTube videos with an average length of 10 seconds. We use the training set of K400 for finetuning CLIP. 

\textbf{UCF101}~\cite{soomro2012ucf101} is collected from YouTube videos, consisting of 13K videos from 101 classes. There are three splits of training data ($\sim$ 9.4K) and validation data ($\sim$3.6K). Following XLCIP~\cite{ni2022expanding} and ViFi-CLIP~\cite{rasheed2022fine}, we report the average performance on the three validation splits. 

\textbf{HMDB51}~\cite{kuehne2011hmdb} consists of 7K videos comprised of 51 action classes, collected from YouTube videos and movie clips. There are three splits of training data ($\sim$ 3.5K, 70 videos per class) and validation data ($\sim$1.5K, 30 videos per class). Following~\cite{ni2022expanding,rasheed2022fine}, we report the average performance on the three validation splits. 

\textbf{Kinetics-600} (K600)~\cite{carreira2018short} is an extension of K400, consisting of 650K videos in 600 classes. Following~\cite{chen2021elaborative, ni2022expanding, rasheed2022fine}, we use the 220 classes\footnote{In the evolution from K400 to K600, there are renamed, removed and split classes. See details in Appendix B in~\cite{chen2021elaborative}.} that are not included in K400 for zero-shot action recognition. There are three validation splits, each containing 160 classes randomly sampled from these 220 classes. We report the average performance on the three validation splits, each containing around 14K videos. 

\textbf{MiniSSv2}~\cite{chen2021deep} (87 classes, 93K videos) is a subset of Something-Something v2 (SSv2)~\cite{goyal2017something} (174 classes, 220K videos). SSv2 is an egocentric motion-based action dataset, which has a large visual domain shift to K400. Furthermore, the action classes are detailed descriptions of fine-grained movements, in a largely different language style than the K400 action dictionary, \eg \textit{Failing to put something into something because something does not fit}, and \textit{Lifting a surface with something on it but not enough for it to slide down}. For zero-shot action recognition, we evaluate on the validation split of MiniSSv2 (12K videos). For few-shot action recognition, we follow~\cite{rasheed2022fine} and evaluate on the validation split of SSv2 (25K videos).  


\textbf{Charades}~\cite{sigurdsson2016hollywood} is a long-range activity dataset recorded by people in their homes based on provided scripts for home activities. There are $\sim$10K videos in 157 classes. The average video length is 30 seconds. Each video has annotations of an average of 6.8 action instances, often in complex co-occurring cases. The validation split consists of 1.8K videos. We report the mean Average Precision (mAP) for the multi-label classification task. 

\textbf{Moments-in-Time} (MiT)~\cite{monfort2019moments} is a large-scale action dataset of 3-second YouTube video clips, which cover actions in 305 classes, performed by both humans and animals. The validation split consists of 30K videos. 

\textbf{UAV Human} (UAV)~\cite{li2021uav} is an action dataset recorded with an Unmanned Aerial Vehicle in unique camera viewpoints. There are 155 action classes. Actions in different categories are performed by a fixed group of subjects in the same background scenes. This leads to an extremely low object-scene bias and a large shift to the domain of K400 and CLIP. We evaluate on the RGB videos and report the average performance on the two official validation splits, each consisting of $\sim$ 6.2K videos. 

% \wei{explain the issue with UCF class name, with space} we add space to have human-readable class names on UCF101.

\input{fig/supp_attn_out_of_dict_samples}
\input{fig/supp_attn_general_action_samples}

\section{Implementation Details}\label{sec:implementation_detials}
In addition to the details mentioned in the main manuscript, we cover more implementation specifics here. 

\textbf{CLIP matching.} 
The CLIP matching step is for consuming the language source of the predefined action dictionary $D$. 
We use CLIP\footnote{CLIP model \href{https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt}{source} }~\cite{clip} with the ViT-B/16 visual encoder~\cite{dosovitskiy2021image} to match each video with texts in the predefined action dictionary. To improve the matching quality for Text Bag Construction, we perform prompt ensembling over the 28 prompt templates\footnote{\url{https://github.com/openai/CLIP/blob/main/data/prompts.md}} which are proposed by CLIP for Kinetics videos. Important to note, during inference we follow the exact protocol of ViFi-CLIP \cite{rasheed2022fine} and use only a single prompt.
% . 

\textbf{GPT-3 text expansion.}
We employ the GPT-3 \texttt{text-davinci-003} model~\cite{brown2020language}. We set the temperature to 0.4. We generate 5 verb phrases using the input instruction - \textit{Generate 5 phrases to describe the action of $<$action$>$ in simple words}. Here for a video $x_i$, \textit{$<$action$>$} is the best matched text $\hat t_i$ from the predefined action dictionary. 

\textbf{BLIP captioning.}
We use BLIP model \footnote{BLIP model \href{https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth}{source} }~\cite{blip} with ViT-L/16 as the image captioner. For each video, the image captioning is performed on 8 uniformly sampled frames. The frames are resized into 384.

\textbf{Text augmentation}. We use the natural language processing tool spaCy\footnote{spaCy \url{https://spacy.io/}} to parse the verbs and verb phrases from the descriptions. We perform augmentation by converting the verbs into forms of lemma and gerund (present participle) and include results in the text bag. 

\textbf{Training.} We employ CLIP with the ViT-B/16 visual encoder. We follow the full-finetuning configuration of~\cite{rasheed2022fine} to finetune both the visual and text encoder. 
During training, we follow the configuration of~\cite{rasheed2022fine,ni2022expanding} for visual augmentation of multi scale crop, random flipping, color jitering and gray scaling. We do not perform augmentations of MixUp or CutMix. 

As different videos have varying numbers of texts in their bags, we randomly sample $N_{\text{bag}}$ texts from the originally constructed bag in each training iteration. 
For multiple instance learning, we use all the $N_{\text{bag}}$ words in a text bag to form $N_{\text{bag}}$ text prompts for each video. 
The text prompt is in the format of \textit{$<$text1$>$ + $<$text2$>$}.
The first part \textit{$<$text1$>$} is uniform for all the $N_{\text{bag}}$ text prompts. Specifically, we use a hand-crafted prompt template \textit{a photo of $<$action$>$}, where \textit{$<$action$>$} is the best-matched text $\hat t_i$ from the predefined action dictionary (see Eq.~1 in the main manuscript). \textit{$<$text2$>$} is an individual text from the text bag. 
To avoid duplication, we do not use $\hat t_i$ as \textit{$<$text2$>$}.
% For \textit{$<$text2$>$}, we do not consider $\hat t_i$ in the text bag to avoid duplication. 

\textbf{Inference.} We follow~\cite{ni2022expanding,rasheed2022fine} and sample a single view via sparse temporal sampling and spatial center crop. The same single prompt template is used in inference. 

% Train on 4$\times$ A6000

\input{tab/supp_noisy_class_space}
\input{fig/caption_gpt_examples}
\input{tab/supp_temp_att}

\section{Additional Results}




\subsection{Attention Heatmaps}\label{sec:attention-heatmaps}
To gain more insights into the performance improvement of \OurMethod, we compare the visualizations of attention heatmaps across several approaches in Fig.~\ref{fig:supp_attn_in_dict_samples}, Fig.~\ref{fig:supp_attn_out_of_dict_samples} and Fig.~\ref{fig:supp_attn_general_action_samples}. \textit{CLIP} is the original CLIP~\cite{clip} without any finetuning. \textit{ViFi-CLIP}~\cite{rasheed2022fine} finetunes CLIP via supervised classification on K400 with ground truth annotations. \textit{\OurMethod} is our approach of unsupervised finetuning with language knowledge. 

We obtain the attention maps by computing the cosine similarity between the patch token features from the visual encoder and the text feature from the text encoder. We visualize the attention maps in several action classes from the downstream datasets used for the zero-shot action recognition task. Based on the relationship between the zero-shot action class and the K400 action dictionary used for training, we categorize the visualizations into 3 groups: (1) In-dictionary action classes which have a verb form (lemma or gerund) directly included in the K400 action dictionary, \eg \textit{clap} and \textit{kick ball} in Fig.~\ref{fig:supp_attn_in_dict_samples}; (2) Novel actions classes which do not have any verb form included in the K400 action dictionary, \eg \textit{wave} and \textit{chew} in Fig.~\ref{fig:supp_attn_out_of_dict_samples}; (3) General actions whose verb form is a basic component of several actions in the K400 action dictionary, \eg \textit{catch}, \textit{hit}, \textit{jump} and \textit{run} in Fig.~\ref{fig:supp_attn_general_action_samples}.

\myparagraph{In-dictionary action classes.} In Fig.~\ref{fig:supp_attn_in_dict_samples}, we visualize two samples of the action \textit{clap} and \textit{kick ball}. \textit{clap} has the same lemma as \textit{clapping} in the K400 dictionary, while \textit{kick ball} has related actions of \textit{kicking field goal} and \textit{kicking soccer ball} in the K400 dictionary. We see that CLIP has incorrectly high attention on object (Fig.~\ref{fig:supp_attn_in_dict_samples}(a), 2nd row) or background scene (Fig.~\ref{fig:supp_attn_in_dict_samples}(b), 2nd row). ViFi-CLIP has cluttered high attention on both the subjects and the background scenes. On the contrary, \OurMethod has more focused attention on the hands (for \textit{clap}) and legs (for \textit{kick ball}). 

In our GPT-3 text bag of \textit{clapping}, related words such as \textit{clap}, \textit{smacking hands}, \textit{slapping palms} and \textit{clapping hands} are included. This strengthens the association between the action \textit{clap} and the body part of hands, and leads to more accurate attention. Furthermore, in BLIP caption verb text bags, the verb \textit{clap} appears several times in frame captions of K400 videos of \textit{clapping}, \textit{giving or receiving award} and \textit{applauding}. This further improves the understanding of \textit{clap}. Similarly, in BLIP frame captions, \textit{kick} is an even more basic verb with large amount of occurrences.

\myparagraph{Novel action classes.} In Fig.~\ref{fig:supp_attn_out_of_dict_samples}, we compare the attention maps for the novel verbs \textit{wave} and \textit{chew} that do not appear in the K400 action dictionary. We see that for \textit{wave}, CLIP and ViFi-CLIP have attention on the background scene or on the head, while \OurMethod has correct attention on the hand and arm. For \textit{chew}, CLIP has more attention on the hair and ViFi-CLIP has attention on a large area of the face. On the contrary, \OurMethod has consistent focused attentions on the area of the mouth where the action  \textit{chew} happens. 

The verb \textit{wave} appears in BLIP caption verb text bags of several K400 videos of \textit{clapping}, \textit{applauding}, \textit{celebrating}. The verb \textit{chew} appears in captions of K400 videos of \textit{eating carrots}, \textit{eating spaghetti}, \textit{eating watermelon} and \textit{baby waking up}. The additional language source improves the knowledge of actions that never appear in the K400 action dictionary. 

\myparagraph{General actions.} In Fig.~\ref{fig:supp_attn_general_action_samples}, we illustrate the attention maps for four general verbs \textit{catch}, \textit{hit}, \textit{jump} and \textit{run}. These verbs are basic components of several actions in the K400 dictionary, \eg \textit{catching fish}, \textit{catching or throwing frisbee}, \textit{hitting baseball}, \textit{jumping into pool} and \textit{running on treadmill}. In these samples, CLIP and ViFi-CLIP have cluttered attention on the background scene or objects. \OurMethod has more concentrated attention on the part where the action happens, \eg catching ball with hands (Fig.~\ref{fig:supp_attn_general_action_samples}(a), last row), hitting drum with stick (Fig.~\ref{fig:supp_attn_general_action_samples}(b), last row), legs and feet jump on stairs (Fig.~\ref{fig:supp_attn_general_action_samples}(c), last row), and attention on the running body (Fig.~\ref{fig:supp_attn_general_action_samples}(d), last row).

These verbs are very general and could have highly diverse instantiations. \Eg \textit{hit} (drum) in Fig.~\ref{fig:supp_attn_general_action_samples}(b) is not close to \textit{hitting baseball} on K400. \textit{jump} (on stairs) in Fig.~\ref{fig:supp_attn_general_action_samples}(c) is not close to \textit{jumping into pool} or \textit{bungee jumping} on K400, even if they share the same verb. In our GPT-3 verb bag and BLIP caption verb bag, there is a large amount of these verb instances that facilitate the comprehensive understanding of these general verbs. This leads to better focus even in unusual complex scenes, \eg jumping on stairs (Fig.~\ref{fig:supp_attn_general_action_samples}(c)). 




% CLIP has object bias and tends to have attention on the distinct objects. 


\subsection{Robustness Against Noisy Action Dictionary}\label{sec:robustness_noisy_dictionary_more}
In Table~5 in the main manuscript, we explored the robustness of our finetuning pipeline against noisy action dictionaries. In case of an over-specified dictionary, we added noisy verbs and verb phrases into the original K400 action dictionary. The noisy verbs are parsed from the captions in the WebVid2.5M dataset~\cite{bain2021frozen}. Here we further increase the ratio of noisy verbs, and add 800 and 1200 verbs into the dictionary, resulting in 1200-class and 1600-class spaces. 

In Table~\ref{tab:supp_noisy_class_space}, we report the zero-shot transfer performance of models finetuned with the resulted 1200-class and 1600-class space. We set the text bag filtering ratio $p=50\%$ for improved text bag quality. We see that even with extremely noisy dictionary where 50\% to 75\% of words do not match with the video data, our finetuning still results in a robust zero-shot transfer performance to unseen datasets. 
The robustness is the consequence of the fact that we collect knowledge from multiple language sources and learn from them via Multiple Instance Learning.
Note that the zero-shot transfer does not have consistent change in performance across the downstream datasets, as different datasets have different language domain shift to the action dictionary used for training. 



\subsection{Examples of Language Sources}\label{sec:examples_of_language_sources}
Similar to the \textit{cooking egg} example in Fig.~2 in the main manuscript, we illustrate more examples of video frames, BLIP frame captions, GPT-3 phrases, together with the derived BLIP verb bag and GPT-3 verb bag in Fig.~\ref{fig:caption_gpt_examples}. The videos are from the unlabeled K400 dataset which we use for training. 



\subsection{Parameter-Free Temporal Module}\label{sec:parameter_free_temp_module}
As mentioned in Sec.~3.1 in the main manuscript, we explore a parameter-free temporal-aware module on the CLIP model. We modify the multi head attention module~\cite{vaswani2017attention} in the visual encoder of CLIP to be temporal aware. Originally, the attention on the frame $t$ is computed via 
$A_t(Q_t, K_t, V_t)= \text{softmax}\frac{Q_t {K_t}^\top }{d_k}V_t$, where $Q_t$, $K_t$ and $V_t$ are the query, key and value from frame $t$. 

We explore to compute the cross-frame attention via 
\begin{equation}
\small
A'_t(Q_t, K_{t+i}|_{i\in I}, V_t)= 
\text{softmax}
\frac{ \sum_{i\in I} (Q_t \cdot {K_{t+i}}^\top) / |I|}
{d_k}V_t
\end{equation}
where we set $I=\{-1, 0, 1\}$. In this case, we use the keys from the frame $t-1$, $t$ and $t+1$ to compute the attention for frame $t$. 

We apply the cross-frame attention on the last 2 and on the last 6 transformer layers in the visual encoder of CLIP. In Table~\ref{tab:supp_tempt_att}, we report the zero-shot transfer performance. We see that in comparison to the variant without any temporal attention module, using cross-frame attention does not lead to performance improvement. K400 is of far smaller scale in comparison to the original CLIP domain. Finetuning from the CLIP model weights with a modified architecture could result in the case that the model drifts far away from the wise CLIP source domain. The results are consistent with the claims in \cite{rasheed2022fine} that a sophisticated temporal module does not necessarily lead to performance improvement.
% (2) K400 contains YouTube videos where the temporal reasoning does not have a big impact on recognizing actions. 




% \textbf{We do not add additional parameters onto CLIP}. 

% Explanation: K400 is of a lot smaller scale in comparison to the original CLIP source domain. as we finetune the entire model, optimization is sensitive, the model could easily drift away. on online user videos, the temporal reasoning does not have a big impact. 
