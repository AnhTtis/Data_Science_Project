\section{Introduction}
% \wei{For co-authors: Careful, introduction needs to be re-written to be coherent with the method section. Method section is already pretty complete. }
% \wei{Some bullet points for the introduction flow: \\
% - People have done Vl adaptation for video, but only with fully supervised data (annotated clips\\
% - People have done VL finetuning from unpaired data\\
% - we proposed to combine those two ideas and propose the first approach that does finetune a VL model from unpaired videos and given class information without any connection between the two \\
% - the scenario in this case work as follows: During training, we're given a set of unlabeled videos as well as a target dictionary with possible labels for the video set\\
% - and then describe method ... text bag to augment class labels and improve unpaired data matching\\
% }

 \vlfull{} (\vl{}) models \cite{clip, blip, jia2021scaling} have met unprecedented success in terms of unlocking many vision applications \cite{clip} to work with potentially unlimited output vocabularies through the promise of \zsfull{} transfer \cite{zhang2021tip,zhou2022conditional,zhou2022learning,gu2021open,rasheed2022bridging,zhou2022detecting,li2022language,rao2022denseclip} enabled by the alignment between visual and language (text) representation spaces effectively attained by those models leveraging huge amounts of paired image and text data. Incorporating a \vl{} model as a source (base) model or as an architectural component has allowed scaling fine-tuning on relatively small, compared to the vast \vl{} pretraining, datasets (e.g. limited in terms of the number of observed objects or other visual concepts) towards \zsfull{} transfer at inference time. That is recognizing \cite{zhang2021tip,zhou2022conditional,zhou2022learning}, detecting \cite{gu2021open,rasheed2022bridging,zhou2022detecting}, segmenting \cite{li2022language,rao2022denseclip}, and even generating \cite{dreambooth} objects unseen during the finetuning stage and only encountered for the first time during the inference. 

 % However, despite all this progress, when applied directly to video data \vl{} models have been observed to suffer from several shortcomings \cite{a,b,c}. Being trained on enormous image + text paired datasets commonly makes them somewhat more vulnerable to video artifacts such as video compression and motion blur. More importantly, as has been extensively studied in several works \cite{winoground,vl-checklist,aro-benchamrk} the \vl{} models have a tendency to mostly represent the objects (nouns) appearing on the images (or video frames) resulting in a kind of `bag of object' representation while paying much less attention to visual details that would correspond to attributes (adjectives) or actions (verbs) in the language counterpart. Intuitively, both of these factors negatively affect direct \vl{} model applications to \zsfull{} action recognition resulting in significant performance drops compared to common image-based more `object-oriented' tasks. 

However, despite all this progress in \zsfull{} image tasks, \vl{} models have been observed to underperform  when applied to \zsfull{} action recognition video data without any finetuning \cite{wang2021actionclip,ni2022expanding,ju2022prompting,wu2023revisiting,castro2022fitclip,rasheed2022fine}. 
% Being trained on enormous image + text paired datasets commonly makes them somewhat more vulnerable to video artifacts such as video compression and motion blur. More importantly, 
A possible reason for this might be that,
as has been extensively studied in several works \cite{winoground,vlc,aro}, the \vl{} models have a tendency to mostly represent objects (nouns) and not actions (verbs or verb phrases).
%
Therefore, to deal with these shortcomings of \vl{} models w.r.t. \zsfull{} action recognition, previous works \cite{wang2021actionclip,ni2022expanding,ju2022prompting,wu2023revisiting,castro2022fitclip,rasheed2022fine} have used (expensive to obtain) supervised datasets (e.g. K400 \cite{kay2017kinetics}) for fine-tuning of \vl{} models (e.g. the most popular CLIP \cite{clip}) towards improved video \zsfull{} recognition performance. The potential downsides of this approach are: (i) reliance on large-scale supervised action datasets that are expensive to collect; (ii) the exposure of the model to only the limited action vocabulary during the supervised fine-tuning (e.g. 400 actions of K400 vs. over 8K possible single verb actions and much more possible general actions in English language) limiting the performance of \zsfull{} transfer to unseen action categories. 
% So the questions we ask in this work are: (i) is it possible to tune strong large-scale pre-trained \vl{} models towards effective \zs{} transfer for action recognition while only leveraging unlabeled video data? (ii) is it possible to harness the knowledge of language to go beyond the limited action vocabulary available for action datasets?

 % In our work, we answer both questions in the affirmative. 
We propose `\oursfull{}' (\OurMethod) -- the first approach to allow finetuning on completely unlabeled video data (e.g. unlabeled K400 \cite{kay2017kinetics}) and a set of language sources, such as unpaired action dictionaries, Large Language Models (e.g. GPT \cite{brown2020language}), and \vl{} models for matching (e.g. CLIP \cite{clip}) and captioning (e.g. BLIP \cite{blip}). As illustrated on Figure \ref{fig:teaser}, \OurMethod effectively leverages the different language sources for collecting and refining noisy bags of potential text that may correspond to each video in the unlabeled set and then applies Multiple Instance Learning (MIL) for finetuning the \vl{} model using those bags.
We extensively evaluate \OurMethod on 7 downstream \zsfull{} and few-shot transfer action recognition benchmarks completely unseen during training. We show that \OurMethod is effective in leveraging unlabeled video data, not only significantly (up to 14\%) improving the source \vl{} model performance on all of those tasks, but also favorably competing with state-of-the-art supervised methods trained on fully supervised counterparts of the same finetuning data, and even improving upon them in some zero-shot and few-shot action recognition transfer tasks.
% \zsfull{} and few-shot transfer action recognition performance to video benchmarks unseen during training. 
%
% Moreover, we show that to achieve this, we can effectively fuse unlabeled video data with `bags' of language knowledge mined from either vision-conditioned and/or pure language-conditioned sources. Specifically, for vision-conditioned, we match video representations to (large) action (language) vocabulary using CLIP \cite{clip} and/or produce a sequence of frame captions using BLIP \cite{blip}. For language-conditioned, we query an LLM (e.g. GPT 3.5 \cite{gpt3.5}) for possible expanded descriptions of texts obtained in vision-conditioned ways. Applying, Multiple Instance Learning (MIL) strategies (e.g. the recent MIL-NCE \cite{milNCE}) using these `language knowledge bags' as pseudo-labels for the unlabeled videos results in the aforementioned gains.

 To summarize, the contributions of this work are as follows: (i) we propose a new task of leveraging unlabled video collection and a set of language sources to improve downstream \zsfull{} and few-shot transfer action recognition performance for strong pre-trained \vl{} models; (ii) we propose \OurMethod{} -- a first approach for this task that effectively matches each video in the unlabeled collection with the corresponding `text bag' of language knowledge mined from the language sources and employs Multiple Instance Learning for finetuning a \vl{} model using these `text bags'; (iii) we extensively evaluate our approach on 7 action recognition benchmarks unseen during training and demonstrate up to 14\% absolute \zsfull{} performance improvements over the source \vl{} model, as well as significant improvements over baseline models trained on fully supervised versions of the same data; (iv) additionally, we show a significant (up to 2.8\%) improvements in the few-shot action recognition transfer scenarios compared to fully supervised baselines improving upon them in 10 out of 12 standard evaluations; and (v) we provide extensive ablations to validate our approach from multiple perspectives.