\section{Introduction\wei{For co-authors: Careful, introduction needs to be re-written to be coherent with the method section. Method section is already pretty complete. }}

\wei{Some bullet points for the introduction flow: \\
- People have done Vl adaptation for video, but only with fully supervised data (annotated clips\\
- People have done VL finetuning from unpaired data\\
- we proposed to combine those two ideas and propose the first approach that does finetune a VL model from unpaired videos and given class information without any connection between the two \\
- the scenario in this case work as follows: During training, we're given a set of unlabeled videos as well as a target dictionary with possible labels for the video set\\
- and then describe method ... text bag to augment class labels and improve unpaired data matching\\
}

 Large scale \vlfull{} (\vl{}) models \cite{clip, blip, ...,jia2021scaling} \wei{is ALIGN~\cite{jia2021scaling} large-scale VL?} have met unprecedented success in terms of unlocking many vision applications \cite{a,b,c} to work with potentially unlimited output vocabularies through the promise of \zsfull{} (\zs{}) transfer \cite{a,b,c,zhang2021tip,zhou2022conditional,zhou2022learning,gu2021open,rasheed2022bridging,zhou2022detecting,li2022language,rao2022denseclip} enabled by the alignment between visual and language (text) representation spaces effectively attained by those models leveraging huge amounts of paired image and text data. Incorporating a \vl{} model as a source (base) model or as an architectural component have allowed scaling fine-tuning on relatively small, compared to the vast \vl{} pretraining, datasets (e.g. limited in terms of the number of observed objects or other visual concepts) towards \zs{} transfer at inference time. That is recognizing \cite{a,b,c,zhang2021tip,zhou2022conditional,zhou2022learning}, detecting \cite{a,b,c,gu2021open,rasheed2022bridging,zhou2022detecting}, segmenting \cite{a,b,c,li2022language,rao2022denseclip}, and even generating \cite{dreambooth,others,like,it} objects unseen during the finetuning stage and only encountered for the first time during the inference. 

 However, despite all this progress, when applied directly to video data \vl{} models have been observed to suffer from several shortcomings \cite{a,b,c}. Being trained on enormous image + text paired datasets commonly makes them somewhat more vulnerable to video artifacts such as video compression and motion blur. More importantly, as has been extensively studied in several works \cite{winoground,vl-checklist,aro-benchamrk} the \vl{} models have a tendency to mostly represent the objects (nouns) appearing on the images (or video frames) resulting in a kind of `bag of object' representation while paying much less attention to visual details that would correspond to attributes (adjectives) or actions (verbs) in the language counterpart. Intuitively, both of these factors negatively affect direct \vl{} model applications to \zsfull{} action recognition resulting in significant performance drops compared to common image-based more `object-oriented' tasks. 

 Therefore, to deal with these shortcomings of \vl{} models w.r.t. \zs{} action recognition, previous works \cite{a,b,c} have proposed leveraging supervised fine-tuning of these models (e.g. the most popular CLIP \cite{clip}) on action datasets (e.g. K400 \cite{kinetics}) with the hopes that it would improve alignment to the video data on the visual representation side, while at the same time enhance the significance at least of a part of the (initially under-represented) action language vocabulary (verbs) in the \vl{} model's visual representation. The potential downsides of this approach are: (i) reliance on large-scale supervised action datasets that are expensive to collect; (ii) the exposure of the model to only the limited action vocabulary during the supervised fine-tuning (e.g. 400 actions of K400 vs. over 8K possible single verb actions and much more possible general actions in English language) limiting the performance of \zs{} transfer to unseen action categories. So the questions we ask in this work are: (i) is it possible to tune strong large-scale pre-trained \vl{} models towards effective \zs{} transfer for action recognition while only leveraging unlabeled video data? (ii) is it possible to harness the knowledge of language to go beyond the limited action vocabulary available for action datasets?

 In our work, we answer both questions in the affirmative. We show that unlabeled video data can be effectively used to significantly improve \zs{} transfer action recognition performance to unseen video benchmarks (e.g. unlabeled K400 \cite{k400} $\rightarrow$ HMDB \cite{hmdb}, UCF \cite{ucf}, and others). Moreover, we show that to achieve this, we can effectively fuse unlabeled video data with `bags' of language knowledge mined from either vision-conditioned and/or pure language-conditioned sources. Specifically, for vision-conditioned, we match video representations to (large) action (language) vocabulary using CLIP \cite{clip} and/or produce a sequence of frame captions using BLIP \cite{blip}. For language-conditioned, we query an LLM (e.g. GPT 3.5 \cite{gpt3.5}) for possible expanded descriptions of texts obtained in vision-conditioned ways. Applying, Multiple Instance Learning (MIL) strategies (e.g. the recent MIL-NCE \cite{milNCE}) using these `language knowledge bags' as pseudo-labels for the unlabeled videos results in the aforementioned gains.

 To summarize, the contributions of this work are as follows: (i) we propose a new task of leveraging unlabled video data to improve downstream \zs{} transfer action recognition performance for strong pre-trained \vl{} models; (ii) we propose a first method for approaching this task via effective fusing of the unlabeled data with the corresponding language knowledge mined via pre-trained large language and \vl{} models and employed through Multiple Instance Learning on the resulting corresponding text `bags'; (iii) we demonstrate significant relative improvements of up to XXX\% over strong baselines including the original pre-trained models and models tuned using large-scale supervised data, as well as show a significant (up to YYY\%) improvements in the few-shot tuning scenarios, also providing extensive ablations to validate our approach.