\section{Method}
\mk{I know understand that I should have written `data set' where I wrote `batch';}
We adopt the classical approach to zero-shot classification with a vision-language model:
During inference, given a video $x$, a set of classes $\mathcal{C}$, and a text prompt $t_c$ for each $c\in \mathcal{C}$, we compute a prompt embedding using a text encoder $\phi_t$, and a video embedding using a video encoder $\phi_v$.
The zero-shot classification is performed by selecting the class $\hat c$ with the maximum similarity between its prompt and the video representation,
\begin{equation} \label{eq:classificationAtTestTime}
\hat c= \argmax_c \phi_v(x)^\top \phi_t(t_c).
\end{equation}
\mk{you do not need to normalize this }
% The zero-shot classification is performed by selecting the class prompt with the maximum similarity to the video representation, \ie, $\hat c= \argmax_c \bar \phi_v(x)^\top \bar \phi_t(t_c)$, 
% where $\bar \phi_v(x)$ and $\bar  \phi_t(t_c)$ denote the normalized video and text feature, respectively.

Like a number of existing methods~\cite{}, we build on top of CLIP.
We directly adapt CLIP's text encoder for $\phi_t$ and construct $\phi_v$ using CLIP's image encoder, as described in Sec.~\ref{sec:clip_on_video}.
Since CLIP is trained on 2D images, it does not capture video-specific cues, which compromises its performance in video recognition.
We remedy this by fine-tuning the model on a collection of unlabelled videos.
Our fine-tuning concept is based on a combination of pseud-labeling and Multiple Instance Learning: we align each video encoding $\phi_v(x)$ to \textit{text bags} --- possibly noisy collections of prompts --- representing other videos with the same pseudo label.
We describe our fine-tuning approach in Sec.~\ref{sec:mil-nce}.
The right composition of the text bags is essential for performance.
In Sec.~\ref{sec:text_bag_construction}, we propose to compose instance-specific, video-conditioned text bags by mixing prompts from different language sources.
An overview of our fine-tuning method is presented in Fig.~\ref{fig:pipeline}. 

\subsection{CLIP for Video Data}\label{sec:clip_on_video}

The network architectures employed by CLIP are designed for two-dimensional images.
To fully harness their power for video classification, we first need to adapt them to processing temporal frame sequences.
Previous efforts to extend CLIP to video data included adding modules dedicated to learning temporal relationships between frames~\cite{}.
However, it has been shown recently that simply pooling frame-level encodings makes the network implicitly model the temporal cues and outperforms more sophisticated architectures~\cite{rasheed2022fine,castro2022fitclip}.
We therefore follow this approach.
Given a video $x$, we pass its $M$ frames $(x[m])_{1\le m \le M}$ through the CLIP image encoder $\phi_\textrm{img}$ and compute the video representation
\begin{equation} \label{eq:videoEncoder}
\phi_{v}(x) = \sum_m \phi_\textrm{img}(x[{m}]) / M . 
\end{equation}
Since we introduce no additional parameters to the original CLIP encoder, pre-trained on a very large number of text-image pairs,
this delivers sufficient initial performance for fine-tunning with the approach we describe in Sec.~\ref{sec:mil-nce}. 
We also explored extending this architecture with a parameter-free temporal-aware module, but found out that it had negligible impact on performance.
The results can be found in the supplementary material.

\wei{as ViFiCLIP~\cite{rasheed2022fine} already outperforms related work with additional learnable components, maybe we do not have to explore this in supp anymore. need to discuss. }

\subsection{Fine-tuning the Network}\label{sec:mil-nce}

\mk{
I tried to write why you designed the loss like this, but it is not easy to see the intention behind this design. 
Have you tried aligning video encodings to encodings of the prompts corresponding to that same video? 
This is how Miech et al do it.
Also, I think Fig. 2 does not convey this video-to-video alignment
}

Fine tuning an \emph{image} vision-language model for \emph{video} classification is difficult, because 
some actions can be performed in different contexts, which makes their visual encoding align with different textual prompts, depending on the context.
For example, a photo of an archer at the Olympic Games is likely to be aligned with different text than a photo of a child handling a toy bow at a birthday party.
Moreover, the text space of a model trained on text-image pairs is biased towards objects as opposed to actions~\cite{}, but objects alone often provide insufficient cue for identification of certain actions.
To address these challenges, we employ a combination of pseudo-labeling with multiple instance learning.

\myparagraph{Pseudo-labeling.}
In a practical scneario, we usually have a coarse prior knowledge of the set of action classes $\mathcal{C}$ for an unannotated video collection.
For each action class $c\in\mathcal{C}$, we created the prompt $t_c$ using the names of the actions.
We compute the pseudo label for video $x_i$ as
\begin{equation}
\hat c_i= \argmax_{c\in D} {\phi_v(x_i)}^\mathsf{T}, \phi_t(t_c) .
\end{equation}
The pseudo-labels are noisy, especially at the beginning of the fine-tuning.
To reduce the influence of this noise on training, for each batch we only keep a fraction of $p$ videos with the highest cosine similarity to their matching prompt encoding.
As we will explain below, our use of the pseudo-labels is unorthodox: we do not enforce them on the network, but instead align representations of videos with the same pseudo-labels.

\myparagraph{Representing videos with text bags.}
We propose to represent each video $x_i$ in the batch with a \emph{text bag} --- a collection of diverse prompts, that represent both the pseudo label $\hat c_i$,
and the context in which the action appears in a given video.
Formally, the text bag for video $x_i$ is a set $T_i$ of prompts.
We detail the construction of $T_i$ in Sec.~\ref{sec:text_bag_construction}.
Each $t\in T_i$ is encoded by $\phi_t$, and we therefore obtain two representations of the unlabelled video $x_i$: its visual encoding $\phi_v(x_i)$ and the set of encodings of it text bag $\{\phi_t(t)\}_{t\in T_i}$.
The advantage of the text bag is that they represent many aspects of the video and the recorded action.
We will next argue that aligning text bags 
gives the network the liberty to represent different context of the same action with different visual encodings, while simultaneously requiring that the encoding is discriminative of the performed action.

\myparagraph{Aligning representations.}
We fine-tune the network by aligning the visual representation of each video to the text bag representations of all other videos with the same pseudo label in the batch.
In order to softly associate a bag of texts $T_i$ with each video $x_i$, in which one or multiple texts are a positive match to the video, 
we adapt the combination of Multiple Instance Learning and Noise Contrastive Estimation (MIL-NCE) by Miech et.\ al.~\cite{miech2020end}.
The loss function 
\begin{equation}
\small
    - \sum_{i} 
    \log 
    \frac{\sum_{j} \sum_{t\in T_j} \exp(\bar \phi_v(x_i)^\top \bar \phi_t (t)/\sigma)\cdot \mathbbm{1}(\hat c_i = \hat c_j)}
    {\sum_k\sum_{t\in T_k} \exp(\bar \phi_v(x_i)^\top \bar \phi_t (t)/\sigma)}
    % -l(i_j) \cdot \log (C^I(E(i_j; \theta^I_E) ; \theta^I_C ))
\end{equation}
where $i,j,k$ range over the batch elements, the bar denotes normalization to unit norm.
and $\sigma$ is a temperature parameter.
$\mathbbm{1}(\hat c_i = \hat c_j)$ is an indicator that $x_i$ and $x_j$ have the same pseudo-label.

\mk{note that you cannot really compute this loss if the summation is over the entire data set; Are you aware how it works? It could just be aligning each $\phi_v(x_i)$ to a single prompt.}

The advantage of this formulation is twofold.
First, it aligns representations softly, enabling aligning each video encoding to the encoding of one of many prompts attributed to other videos with the same pseudo-labels.
In other words, it lets the network preserve the diversity of its encodings, which is beneficial for zero-shot learning.
Second, since the text bags for different videos may be different even when their pseudo-labels are the same, it promotes alignment to prompts refering to concepts that may not appear in a given video, but are semantically related to the captured action.

\subsection{Text Bag Construction}\label{sec:text_bag_construction}
The composition of the text bags is essential to the performance of our fine-tuning technique. 
We initialize each text bag $T_i=\{ t_{\hat c_i}\}$, where $t_{\hat c_i}$ is the prompt obtained from the action name corresponding to the pseudo-label of $x_i$.
We then expand the text bags using GPT-3 and BLIP captioning.

\myparagraph{GPT-3 text expansion.} We expand the text bag by leveraging the large-scale language model (LLM) GPT-3~\cite{brown2020language}. This is similar to in-context learning \cite{} (\wei{is this correct?}) where few examples of desired output are provided in the context window of the input. By providing the best matched text $\hat t_i$ in the input prompt for LLM, we result in a collection of expanded alternative descriptions of the action. The descriptions contain hallucinating details leveraging the collective world knowledge of the LLM. 
We collect the verbs and verb phrases extracted from the generated expanded action descriptions. Furthermore, we perform text augmentation by including both the lemma and gerund (present participle) forms of the verbs. We add the collection of words to the text bag $T_i$. 
\wei{need to further check this paragraph}.

\myparagraph{BLIP captioning.} We employ the vision-language model BLIP~\cite{li2022blip} for generating captions of static frames on a video. Note that the image captioning model is not pretrained on any video domain. The frame captions provide instance-level descriptions that are dependent on the visual content of frames of the unlabeled videos. Similarly, we further collect verbs in the descriptions, and perform text augmentation. We include the collection of words to the text bag $T_i$. 

