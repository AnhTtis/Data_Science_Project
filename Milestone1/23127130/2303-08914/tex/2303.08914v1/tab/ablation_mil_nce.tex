% \begin{table*}[!tb]%[!htbp]
% \small
% \centering
% % \begin{tabular}{@{}c M{0.8cm} M{0.8cm} M{0.8cm} M{0.7cm} M{0.7cm}M{0.7cm}@{}}
% % \begin{tabular}{@{}l @{\hspace{2em}}cccccccccccccc}
% \begin{tabular}{ccccccccccccccc}
% \toprule
% % Model & \multicolumn{3}{c}{TANet} & \multicolumn{3}{c}{Swin} \\
% % \cmidrule(r){2-4}\cmidrule{5-7}
% % Dataset & \multirow{2}*{pretrain} & \multirow{2}*{sett.} & \multicolumn{4}{c}{HMDB51}  & \multicolumn{4}{c}{UCF101} & \multicolumn{4}{c}{SSv2}  \\
% % \cmidrule(r){4-7}\cmidrule(r){8-11}\cmidrule(r){12-15}
% Text bags & UCF101 & HMDB51 & K600 & MiniSSv2 & Charades & UAV Human & Moments-in-time \\
% \midrule
% Cross entropy & 74.48 & 48.69 & 65.09 & 5.24 & 22.91 & 2.60 & 21.64 \\
% NCE & 77.26 & 49.85 & 70.08 & 4.75 & 23.45 & 2.62 & 21.71\\
% MIL-Max & 77.24 & 49.85 & 70.71 & 5.09 & 23.49 & 2.46 & 21.84\\
% MIL-NCE w/o label mask & 76.96 & 48.48 & 70.14 \\
% MIL-NCE & 77.88 & 51.09 & 71.24 & 5.46 & 23.52 & 2.53 & 22.44 \\
% \bottomrule
% \end{tabular}
% \caption{
% Different strategies of learning from text bags.  We use text bags of GPT3 verbs + BLIP verbs. 100\% pseudo labels. \wei{only show the first 3 datasets}.
%  }
% \label{tab:ablation_mil_nce}
% % \label{tab:yti_state_of_the_art}
% \end{table*}



\begin{table}[!tb]%[!htbp]
\footnotesize
\centering
% \begin{tabular}{@{}c M{0.8cm} M{0.8cm} M{0.8cm} M{0.7cm} M{0.7cm}M{0.7cm}@{}}
% \begin{tabular}{@{}l @{\hspace{2em}}cccccccccccccc}
\begin{tabular}{cccc}
\toprule
% Model & \multicolumn{3}{c}{TANet} & \multicolumn{3}{c}{Swin} \\
% \cmidrule(r){2-4}\cmidrule{5-7}
% Dataset & \multirow{2}*{pretrain} & \multirow{2}*{sett.} & \multicolumn{4}{c}{HMDB51}  & \multicolumn{4}{c}{UCF101} & \multicolumn{4}{c}{SSv2}  \\
% \cmidrule(r){4-7}\cmidrule(r){8-11}\cmidrule(r){12-15}
Objective & UCF101 & HMDB51 & K600 \\
\midrule
Cross entropy & 74.48 & 48.69 & 65.09  \\
NCE & \underline{77.26} & 49.85 & 70.08 \\
MIL-Max & 77.24 & 49.85 & \underline{70.71} \\
MIL-NCE only instance-level & 76.96 & \underline{50.48} & 70.14 \\
MIL-NCE & \textbf{77.88} & \textbf{51.09} & \textbf{71.24} \\

\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{
Different strategies of learning from text bags. 
% We use text bags of GPT3 verbs + BLIP verbs. 
We report the zero-shot transfer performance on UCF, HMDB and K600.
For a thorough ablation, we set the text bag filtering ratio $p=100\%$ to keep the full noisy text bag property.
 }
\label{tab:ablation_mil_nce}
% \label{tab:yti_state_of_the_art}
\end{table}