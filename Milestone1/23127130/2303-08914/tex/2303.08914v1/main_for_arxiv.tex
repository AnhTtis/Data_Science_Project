\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\input{tex/defs.tex}

\iccvfinalcopy % *** Uncomment this line for the final submission
\def\iccvPaperID{3246} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

% \input{macros}

\begin{document}

%%%%%%%%% TITLE
% \title{Verb Injection, Transfer learning unlabeled videos, zero-shot action recognition}
% \title{\oursfull{}: finetuning on unlabeled video collection for zero-shot and few-shot action recognition leveraging language knowledge }
% \wei{Match, Expand and Improve? Term "Unpaired"- need to keep in mind that BLIP verbs are paired}
% }
\title{\oursfull{}: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge 
% \wei{Match, Expand and Improve? Term "Unpaired"- need to keep in mind that BLIP verbs are paired}
}
% Taming CLIP with verbs: zero-shot ... 

% \title{Finetuning action recognition via unpaired class dictionaries}
% \title{Finetuning VL models for action recognition via unpaired class dictionaries}
% \title{Finetuning VL models for action classification via unpaired class dictionaries}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }


\author{
Wei Lin$^{\dagger 1}$ \and
Leonid Karlinsky$^{2}$ \and 
Nina Shvetsova$^3$ \and 
Horst Possegger$^1$ \and 
Mateusz Kozinski$^1$ \and 
Rameswar Panda$^2$ \and
Rogerio Feris$^2$ \and
Hilde Kuehne$^{2,3}$ \and
Horst Bischof$^{1}$\and\\
$^1$Institute of Computer Graphics and Vision, Graz University of Technology, Austria\\
$^2$MIT-IBM Watson AI Lab, USA\\
$^3$Goethe University Frankfurt, Germany\\
}



\maketitle
\blfootnote{
% $\ast$ Equally contributing authors.\\ 
{
%\indent\indent\phantom{.} 
$\dagger$ Correspondence: \tt\small{wei.lin@icg.tugraz.at}}
}
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
% \wei{For co-authors: Careful, abstrac and introduction (current version is from some time ago) needs to be re-written to be coherent with the method section. Method section is already pretty complete. }
    Large scale \vlfull{} (\vl{}) models have shown tremendous success in aligning representations between visual and text modalities. This enables remarkable progress in \zsfull{} recognition, image generation \& editing, and many other exciting tasks. 
    %However, these models still have some fundamental weaknesses requiring additional training before applying them to \zsfull{} action recognition tasks. 
    However, \vl{} models tend to over-represent objects while paying much less attention to verbs, and require additional tuning on video data for best \zsfull{} action recognition performance.
    While previous work relied on large-scale, fully-annotated data, in this work we propose an unsupervised approach. We adapt a \vl{} model for zero-shot and few-shot action recognition using a collection of unlabeled videos and an unpaired action dictionary. %, Large Language Models and \vl{} models, for matching, text expansion and captioning. 
    Based on that, we leverage Large Language Models and \vl{} models to build a text bag for each unlabeled video via matching, text expansion and captioning. We use those bags in a Multiple Instance Learning setup to adapt an image-text backbone to video data. 
    % We attain improvement of up to XXX\% in \zs{} transfer to novel action recognition tasks, in some cases even improving upon supervised baselines by up to YYY\%.
    Although finetuned on unlabeled video data, our resulting models demonstrate high transferability to numerous unseen zero-shot downstream tasks, improving the base \vl{} model performance by up to 14\%, and even comparing favorably to fully-supervised baselines in both zero-shot and few-shot video recognition transfer. 
    The code will be released later at \url{https://github.com/wlin-at/MAXI}.
    
   % The ABSTRACT is to be in fully-justified italicized text, at the top
   % of the left-hand column, below the author and affiliation
   % information. Use the word ``Abstract'' as the title, in 12-point
   % Times, boldface type, centered relative to the column, initially
   % capitalized. The abstract is to be in 10-point, single-spaced type.
   % Leave two blank lines after the Abstract, then begin the main text.
   % Look at previous ICCV abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{fig/teaser}
\section{Introduction}
% \wei{For co-authors: Careful, introduction needs to be re-written to be coherent with the method section. Method section is already pretty complete. }
% \wei{Some bullet points for the introduction flow: \\
% - People have done Vl adaptation for video, but only with fully supervised data (annotated clips\\
% - People have done VL finetuning from unpaired data\\
% - we proposed to combine those two ideas and propose the first approach that does finetune a VL model from unpaired videos and given class information without any connection between the two \\
% - the scenario in this case work as follows: During training, we're given a set of unlabeled videos as well as a target dictionary with possible labels for the video set\\
% - and then describe method ... text bag to augment class labels and improve unpaired data matching\\
% }

 \vlfull{} (\vl{}) models \cite{clip, blip, align} have met unprecedented success in unlocking many vision applications \cite{clip} to work with potentially unlimited open vocabularies, through the promise of \zsfull{} transfer \cite{zhang2021tip,zhou2022conditional,zhou2022learning,gu2021open,rasheed2022bridging,zhou2022detecting,li2022language,rao2022denseclip}. This is empowered by the alignment between visual and language representation spaces, which is effectively attained by \vl{} models leveraging huge amounts of paired image and text data. Incorporating a \vl{} model as a source (base) model or as an architectural component has allowed scaling finetuning on relatively small datasets (e.g. limited in terms of the number of observed objects or other visual concepts compared to the vast \vl{} pretraining) towards \zsfull{} transfer at inference time. Such zero-shot transfer includes recognizing \cite{zhang2021tip,zhou2022conditional,zhou2022learning}, detecting \cite{gu2021open,rasheed2022bridging,zhou2022detecting}, segmenting \cite{li2022language,rao2022denseclip}, and even generating \cite{dreambooth} objects unseen during the finetuning stage and only encountered for the first time at the inference stage. 

 % However, despite all this progress, when applied directly to video data \vl{} models have been observed to suffer from several shortcomings \cite{a,b,c}. Being trained on enormous image + text paired datasets commonly makes them somewhat more vulnerable to video artifacts such as video compression and motion blur. More importantly, as has been extensively studied in several works \cite{winoground,vl-checklist,aro-benchamrk} the \vl{} models have a tendency to mostly represent the objects (nouns) appearing on the images (or video frames) resulting in a kind of `bag of object' representation while paying much less attention to visual details that would correspond to attributes (adjectives) or actions (verbs) in the language counterpart. Intuitively, both of these factors negatively affect direct \vl{} model applications to \zsfull{} action recognition resulting in significant performance drops compared to common image-based more `object-oriented' tasks. 
\vspace{4mm}

However, despite the progress in \zsfull{} image tasks, \vl{} models have been observed to underperform  when applied to \zsfull{} action recognition on video data without any finetuning \cite{wang2021actionclip,ni2022expanding,ju2022prompting,wu2023revisiting,castro2022fitclip,rasheed2022fine}. 
% Being trained on enormous image + text paired datasets commonly makes them somewhat more vulnerable to video artifacts such as video compression and motion blur. More importantly, 
% A possible reason for this might be that,
% as has been extensively studied in several works \cite{winoground,vlc,aro}, the \vl{} models have a tendency to mostly represent objects (nouns) and not actions (verbs or verb phrases).
A possible reason, as extensively studied in several works \cite{winoground,vlc,aro,hendricks2021probing}, is that \vl{} models have a tendency to mostly represent objects (nouns) and not actions (verbs or verb phrases).
%
Therefore, to deal with these shortcomings of \vl{} models w.r.t.~\zsfull{} action recognition, previous works \cite{wang2021actionclip,ni2022expanding,ju2022prompting,wu2023revisiting,castro2022fitclip,rasheed2022fine} have used datasets with full annotation (e.g. K400 \cite{kay2017kinetics}) to finetune \vl{} models (e.g. the most popular CLIP~\cite{clip}) towards improved video \zsfull{} recognition performance. The potential downsides of this approach are: (i) reliance on full annotation of large-scale action datasets that is time-consuming and cost-intensive, and (ii) the exposure of the model to only the limited action vocabulary during the supervised finetuning (e.g. 400 actions of K400 vs. over 8K possible single verb actions and much more possible general actions in English language) limiting the performance of \zsfull{} transfer to unseen action categories. 
% So the questions we ask in this work are: (i) is it possible to tune strong large-scale pre-trained \vl{} models towards effective \zs{} transfer for action recognition while only leveraging unlabeled video data? (ii) is it possible to harness the knowledge of language to go beyond the limited action vocabulary available for action datasets?
%
 % In our work, we answer both questions in the affirmative. 
In this context, we propose `\oursfull{}' (\OurMethod) -- to allow finetuning on completely unlabeled video data (e.g. unlabeled K400 \cite{kay2017kinetics}) and a set of language sources, such as unpaired action dictionaries, Large Language Models (LLM) (e.g. GPT-3 \cite{brown2020language}), and \vl{} models for matching (e.g. CLIP \cite{clip}) and captioning (e.g. BLIP \cite{blip}). 
To this end, \OurMethod relies on individual bags of potential texts, collected and refined based on the different language sources, that correspond to each video in the unlabeled set. It then applies Multiple Instance Learning (MIL) for finetuning the \vl{} model using those bags as illustrated in Figure \ref{fig:teaser}.
%
%As illustrated in Figure \ref{fig:teaser}, \OurMethod effectively leverages the different language sources for collecting and refining noisy bags of potential texts that may correspond to each video in the unlabeled set. It then applies Multiple Instance Learning (MIL) for finetuning the \vl{} model using those bags.
%
We extensively evaluate \OurMethod on seven downstream \zsfull{} and few-shot transfer action recognition benchmarks completely unseen during training. We show that \OurMethod is effective in leveraging unlabeled video data, not only significantly (up to 14\%) improving the source \vl{} model performance on all of those tasks, but also favorably competing with state-of-the-art supervised methods trained on fully supervised counterparts of the same finetuning data, and even improving upon them in some zero-shot and few-shot action recognition transfer tasks.
% \zsfull{} and few-shot transfer action recognition performance to video benchmarks unseen during training. 
%
% Moreover, we show that to achieve this, we can effectively fuse unlabeled video data with `bags' of language knowledge mined from either vision-conditioned and/or pure language-conditioned sources. Specifically, for vision-conditioned, we match video representations to (large) action (language) vocabulary using CLIP \cite{clip} and/or produce a sequence of frame captions using BLIP \cite{blip}. For language-conditioned, we query an LLM (e.g. GPT 3.5 \cite{gpt3.5}) for possible expanded descriptions of texts obtained in vision-conditioned ways. Applying, Multiple Instance Learning (MIL) strategies (e.g. the recent MIL-NCE \cite{milNCE}) using these `language knowledge bags' as pseudo-labels for the unlabeled videos results in the aforementioned gains.

Our contributions are as follows: (i) we propose \OurMethod{}, an approach that leverages an unlabeled video collection and a set of language sources to improve downstream \zsfull{}  action recognition; (ii) we propose to match each unlabeled video with \textit{text bags} of knowledge mined from the language sources,
 % \mk{this seems not to be true - maybe just substitute `the corresponding text bag' with `text bags'}
 and employ Multiple Instance Learning for finetuning a \vl{} model using these text bags; (iii) we extensively evaluate our approach on seven unseen action recognition benchmarks, and demonstrate up to 14\% absolute \zsfull{} performance improvements over the source \vl{} model, and even outperform baseline models trained in a fully supervised manner on the same data.
%
% To summarize, the contributions of this work are as follows: (i) we propose a new task of leveraging unlabled video collection and a set of language sources to improve downstream \zsfull{} and few-shot transfer action recognition performance for large-scale pre-trained \vl{} models; (ii) we propose \OurMethod{} -- a first approach for this task that effectively matches each unlabeled video with \textit{text bags} of knowledge mined from the language sources,
 % \mk{this seems not to be true - maybe just substitute `the corresponding text bag' with `text bags'}
 % and employs Multiple Instance Learning for finetuning a \vl{} model using these text bags; (iii) we extensively evaluate our approach on seven action recognition benchmarks unseen during training, and demonstrate up to 14\% absolute \zsfull{} performance improvements over the source \vl{} model, as well as substantial outperformance over baseline models trained in a fully supervised manner on the same data; (iv) additionally, we show a significant improvement (up to 2.8\%) in the few-shot action recognition transfer scenarios, improving upon fully supervised baseline in 10 out of 12 standard evaluations; and (v) we provide extensive ablations to validate our approach from multiple perspectives.






\input{fig/pipeline}

\section{Related Work}

\noindent\textbf{Vision-language (VL) Models} revolution started with CLIP~\cite{clip} and ALIGN~\cite{align} which demonstrated that very large scale (in hundreds of millions) pre-training, on a dataset with massive amount of noisy image-text pairs collected from the web, leads to significant advances in many diverse downstream zero-shot tasks. VL models optimize for image-text alignment via contrastive learning objectives. Earlier methods, such as \cite{tan2019lxmert,chen2020uniter,li2020oscar}, relied on pre-trained object detectors to extract region features. To relax this limitation, cross-attention layers with self-supervised learning objectives, image-text matching, and masked/autoregressive language modeling were proposed in \cite{kim2021vilt,align,yang2022vision,blip}. 
%\subsection{Vision and Language (VL) Models}
BLIP~\cite{blip} combined several techniques for multi-task VL pre-training, achieving strong results in several downstream VL tasks, such as image retrieval, visual question answering~(VQA), image captioning, and reasoning tasks. Finer-level text-image alignment was attempted in~\cite{cyclip,yao2021filip,furst2021cloob,declip,gao2022pyramidclip}, employing additional losses and logic on top of the base contrastive loss of CLIP. FILIP focuses on fine-grained contrastive learning, maximizing the token-wise similarity between image and text tokens. CyClip~\cite{cyclip} employs geometrical consistency between the image and text embeddings. DeCLIP~\cite{declip} retrieves nearest neighbors for expanding the set of positive contrastive matches. While these methods have strong zero-shot results on many image benchmarks, such as ImageNet~\cite{russakovsky2015imagenet} and  MS-COCO~\cite{lin2014microsoft}, recent studies such as VL-CheckList~\cite{vlc}, the Winoground Challenge~\cite{winoground} and ARO~\cite{aro}, show that these models cannot well distinguish fine-grained language details or understand more structured concepts such as actions that commonly require understanding temporal concepts, movement, and relations between objects. In this paper, we show how \vl{} models can be adapted to better understand actions given unlabeled video data.

\myparagraph{Zero-shot action recognition} is the task of recognizing actions that have not been seen during training. This requires the bridging between visual features and semantic representations. Previous works use manually defined attributes~\cite{liu2011recognizing,zellers2017zero}, and word embeddings of action names~\cite{brattoli2020rethinking,mandal2019out,qin2017zero,shao2020temporal} or action descriptions~\cite{chen2021elaborative,qian2022rethinking,wang2017alternative} as the semantic representation. 
ER-ZSAR~\cite{chen2021elaborative} and JigsawNet~\cite{qian2022rethinking} leverage crawled descriptions of action classes with manual correction, which require efforts of human annotators for modifying the descriptions. The class descriptions are assigned to the videos based on ground truth labels. On the contrary, our text bag construction requires neither manual correction efforts nor ground truth annotation of videos. 

Recent work contributes to adapting large-scale VL model for video understanding, including zero-shot action recognition tasks~\cite{wang2021actionclip,ni2022expanding,ju2022prompting,wu2023revisiting,castro2022fitclip,rasheed2022fine}. ActionCLIP~\cite{wang2021actionclip}, Ju \etal~\cite{ju2022prompting} and XCLIP~\cite{ni2022expanding} adapt CLIP for video data with additional components for spatio-temporal modeling, and demonstrate performance improvements on video tasks. The most recent ViFi-CLIP~\cite{rasheed2022fine} shows that frame-level processing with feature pooling achieves better visual-language alignment, and outperforms sophisticated related approaches with additional learnable spatio-temporal components. In this work, we follow the architecture and finetuning paradigm of ViFi-CLIP. 

Despite the various contributions in architecture design and optimization, the related approaches still rely on ground truth annotations in finetuning CLIP for zero-shot action recognition tasks. Furthermore, no additional language source other than simple action names is explored during finetuning. \OurMethod overcomes these two limitations by finetuning CLIP (1) without any ground truth labels, and (2) expanding action names by LLM text expansion and visual captioning. 

\input{tex/method_old}

\section{Experiments}

\subsection{Datasets}
We perform the self-supervised finetuning on Kinetics 400 (K400)~\cite{kay2017kinetics} without any ground truth labels. K400 is the most popular benchmark for action recognition tasks, containing around 240K training videos for 400 classes. We evaluate action recognition zero-shot transfer and few-shot transfer on several benchmark datasets: UCF101~\cite{soomro2012ucf101}, HMDB51~\cite{kuehne2011hmdb}, MiniSSv2~\cite{chen2021deep} (subset of SSv2~\cite{goyal2017something}), Kinetics600 (K600)~\cite{carreira2018short}, Charades~\cite{sigurdsson2016hollywood}, UAV Human (UAV)~\cite{li2021uav}, and Moments-in-Time (MiT)~\cite{monfort2019moments}. 
UCF, HMDB and K600 are collections of online user videos, which are closer in terms of style to K400. 
% are in style of online user videos which is closer to K400. 
The remaining datasets cover larger domain shifts to K400, varying from egocentric motions (MiniSSv2), human and animal videos (MiT), drone videos with small subject in frame (UAV) and 30-second long-term home videos (Charades).  
% \wei{Datasets of different levels of object bias}
% \wei{Divide the datsets into two groups: UCF, HMDB and K600, used in related work, are closer to K400. The rest has larger domain shift ... }
% \wei{Explain the challenge in each action datsaet, SSv2 - egocentric videos,  Moments-in-time: animals, UAVHuman - Drone videos with small subject in frame,   Charades is even long-term video dataset with average length of 30s. } 
More details about datasets are given in the supplementary.
% \wei{Explain that on K600, we only take the 220 classes not included in K400.}
% Experiments on Mini-Kinetics400??
% MiniSSv2 is a subset of SSv2~\cite{goyal2017something}, (87 randomly chosen classes) from~\cite{chen2021deep}

We follow the evaluation protocol of zero-shot and few-shot action recognition from~\cite{rasheed2022fine,ni2022expanding}. We report mAP for multi-label classification on Charades and Top1/Top5 accuracy for single-label classification on the remaining datasets.  
% \leonid{perhaps mention that you replicate ZS transfer and FS transfer protocolos from ViFi-CLIP, further extending it by adding supervised FS transfer (from K400 pretrain) too}

% evaluation protocol, average over three  splits on UCF, HMDB, K600. UAV has two splits. 
% We report top1 and top5 acc. mAP for Charades (multi-label classification)

\subsection{Implementation Details}
We employ CLIP with the ViT-B/16~\cite{dosovitskiy2021image} visual encoder. We follow the full-finetuning configuration of \cite{rasheed2022fine} to finetune both the visual and text encoder. 
We consistently set the temperature $\sigma$ to 0.02. For zero-shot setting, we finetune on K400 without any ground truth labels. We use the AdamW optimizer~\cite{loshchilov2017decoupled} with an initial learning rate of $5\times10^{-6}$ and a cosine decay scheduler. We sample 16 frames from each video and train with a batch size of 256 for 10 epochs. For few-shot learning, we sample 32 frames per video. We set the learning rate to $2\times10^{-6}$, and train with a batch size of 64 for 50 epochs. During inference, we sample 1 view from each video. Inspired by~\cite{wortsman2022robust, ilharco2022patching}, we perform linear weight-space ensembling between the original CLIP (with ratio of 0.2) and the finetuned model. In the main results, we set the text bag filtering ratio $p$ to 90\% and bag size to 16.




\subsection{Zero-Shot Action Recognition}\label{sec:zero_shot_action_recognition}
\input{tab/zero_shot_sota}
\input{tab/zero_shot_sota_part2}
We finetune CLIP on the large-scale K400 dataset stripped of the original ground truth labels. We perform zero-shot action recognition on seven different datasets to verify that cross-dataset model generalizability transfer after the finetuning. In zero-shot setting, the model is evaluated directly on downstream datasets with unseen classes, without being trained on any samples of these datasets.
% via text queries of \textit{A video of $<$action label$>$}, without being trained on any samples of these datasets.

In Table~\ref{tab:zs_action_recog_sota}, we first compare to other state-of-the-art methods, all of which use K400 to adapt CLIP models for zero-shot recognition tasks on UCF, HMDB and K600. Following~\cite{rasheed2022fine,ni2022expanding,chen2021elaborative}, we report the mean and standard deviation of results on three official validation sets. ER-ZSAR~\cite{chen2021elaborative} and JigsawNet~\cite{qian2022rethinking} are zero-shot action recognition approaches that train with K400 ground truth annotations. They leverage crawled descriptions of action classes with manual correction, which requires efforts from human annotators. Afterwards, the class descriptions are assigned to videos based on ground truth annotations.
% is a language-enhanced action recognition approach prior to CLIP, and uses separate TSM spatio-temporal visual encoder~\cite{lin2019tsm} and a BERT text encoder~\cite{devlin2018bert}. It explores language sources of Wikipedia and WordNet. 
We see that the original CLIP has good direct zero-shot performance across the three datasets, which performs better or on par with ER-ZSAR~\cite{chen2021elaborative} and JigsawNet~\cite{qian2022rethinking}. 
%
The rest of the compared approaches all adapt CLIP models on video-text pairs with the K400 ground truth class labels as texts. 
% The text is from the K400 action dictionary. 
Among them, the most recent ViFi-CLIP~\cite{rasheed2022fine} achieves the best result, outperforming all the other approaches, without adding any learnable spatio-temporal modules (as done by other approaches such as~\cite{wang2021actionclip,ju2022prompting,ni2022expanding}). 

In a similar full finetuning paradigm to ViFi-CLIP, \OurMethod achieves favorable results without using any ground truth annotation. We report the performance of \OurMethod with different combinations of language sources. Simply with the original K400 action dictionary, we already outperform most of the related work across the three datasets. With the additional GPT-3 verbs and BLIP verbs in the text bag, we further boost the performance, achieving the state-of-the-art among the three datasets. 

For a thorough analysis of the model generalizibility, we further report the performance of \OurMethod on four datasets (Charades, MiT, MiniSSv2 and UAV) with larger domain shift to K400 in Table~\ref{tab:zs_action_recog_sota_part2}. 
%In Table~\ref{tab:zs_action_recog_sota_part2}, for a thorough analysis of the model generalizibility, we further report the performance of \OurMethod on four datasets (Charades, MiT, MiniSSv2 and UAV) with larger domain shift to K400. 
In comparison to the original CLIP, our finetuned model has improved zero-shot transfer on all datasets. With the additional language sources of GPT-3 and BLIP, we even outperform ViFi-CLIP trained with ground truth of K400, on the challenging MiT and MiniSSv2 datasets. 

\subsection{Few-Shot Action Recognition}\label{sec:few_shot_action_recog}
We perform few-shot all-way action recognition to evaluate the model learning capacity in a low data regime. 
In this setting, we specifically verify whether our self-supervised finetuning on K400 provides a proper initialization for few-shot learning. 
We follow the few-shot configuration of ViFi-CLIP~\cite{rasheed2022fine} and XCLIP~\cite{ni2022expanding}, and use the same training samples in 2, 4, 8 and 16-shot experiments without additional language source for a fair comparison. We train with 32 frames per video. We use the best backbone of self-supervised finetuning (from Sec.~\ref{sec:zero_shot_action_recognition}) as the model initialization for few-shot training. In Table~\ref{tab:few_shot_table}, we report few-shot results of~\OurMethod on three datasets, and also the zero-shot performance of our initialization as a reference. We compare with related approaches that directly perform few-shot learning on CLIP. For a fair comparison, we include the result of few-shot training with a CLIP model that is pre-trained with ground truth labels in the ViFi-CLIP paradigm. 

We see that few-shot learning using a \OurMethod-pretrained backbone leads to best performance in most settings, even outperforming the fully-supervised pretrained backbone of ViFi-CLIP. The performance gap is significant in the more challenging extremely limited data scenarios (\eg 2-shot on HMDB and UCF). Pretraining with full supervision as an initialization might lead to degraded performance in the following few-shot learning (\eg 8-shot on HMDB, 4-shot on UCF), while our self-supervised finetuned model mitigates this problem, indicating improved generalizability. 

\input{tab/few_shot_table}

\input{tab/pseudo_label_ratio}
\subsection{Ablation Study}\label{sec:ablation_study}
\subsubsection{Text bag filtering}\label{sec:text_bag_filtering}
% \myparagraph{Text bag filtering.} 
To improve the quality of text bags used in training, we set a threshold $\delta_p$ on the similarity score from CLIP matching, such that $p\times 100\%$ of videos with highest similarity scores remain after the thresholding (see Sec.~\ref{sec:text_bag_construction}). We perform CLIP matching between unlabeled K400 videos and the K400 action dictionary, and use the filtered videos and text bags for finetuning CLIP. In Table~\ref{tab:ps_analysis}, we report the matching accuracy (after filtering), and zero-shot transfer performance of models finetuned with the filtered K400 videos and text bags. As a reference, we also report CLIP zero-shot performance, and the case of finetuning on 100$\%$ accurate video-textbag pairs using ground truth annotation, which leads to the best zero-shot transfer on most datasets. 

In Table~\ref{tab:ps_analysis}, we notice that the CLIP matching accuracy increases continuously with decreasing filtering ratio $p$. Setting $p=90\%$ leads to consistent improvement of zero-shot transfer, in comparison to the case of $p=100\%$ due to improved quality of matched texts. 
Setting $p=50\%$ leads to partial improvement compared to $p=100\%$. Further reducing $p$ to $50\%$ leads to performance degradation due to the limited amount of data. This indicates that selecting text bags that CLIP is confident about ensures improved finetuning for more effective zero-shot transfer. However, there is a trade-off between the quality of the filtered data and the amount of data used for training. 



% \myparagraph{Robustness of finetuning with noisy action dictionary.} 
\subsubsection{Robustness against noisy action dictionary}\label{sec:robustness_noisy_dict}
In a practical scenario, we have coarse prior knowledge of the potential action types in an unannotated video collection, which defines an action dictionary. However, such knowledge might be noisy. We explore the robustness of our finetuning pipeline against such a noisy action dictionary. We consider two cases of noisy action dictionaries: (1) an under-specified dictionary consisting of only half of the words of the original K400 action dictionary. Specifically, we use the 200 action names from MiniKinetics~\cite{chen2021deep} (a 200-class subset of K400). (2)~An over-specified dictionary by adding noisy verbs and verb phrases into the original K400 action dictionary. We parse verbs from the captions in the validation set of the WebVid2.5M dataset~\cite{bain2021frozen}, and randomly sample 400 verbs to add to the dictionary, resulting in a dictionary of 800 verbs or verb phrases. 

In Table~\ref{tab:noisy_class_space}, we report the zero-shot transfer performance of models finetuned with these noisy dictionaries. Here we set the text bag filtering $p=50\%$ for improved text bag quality. We also report the results with the original K400 action dictionary as a reference. Apparently, using the clean original K400 action dictionary leads to the best zero-shot transfer on most of the downstream datasets. However, using noisy action dictionaries still leads to significant performance boost compared to the CLIP zero-shot results without finetuning. This indicates the robustness of our pipeline with different cases of noisy predefined dictionaries. 

\input{tab/noisy_class_space}

% \myparagraph{What words to include in the text bag?} 
\subsubsection{What words to include in the text bag?}\label{sec:words_in_text_bag}
In Table~\ref{tab:bag_of_words}, we investigate different combinations of words to include in the text bag. Besides the original K400 action dictionary (\textit{K400 dict.}), we explore: (1) \textit{BLIP verbs}: verbs parsed from BLIP captions; (2) \textit{BLIP object nouns}: nouns of objects parsed from BLIP captions; (3) \textit{GPT3 verbs}: verbs and verb phrases from GPT3 text expansion. For a thorough ablation, we set the text bag filtering ratio $p=100\%$ to keep the full noisy text bag property.

In Table~\ref{tab:bag_of_words}, we notice that additional language source upon the original K400 action dictionary leads to further improvement in zero-shot transfer. Interestingly, using BLIP verbs has slightly better results than the case of BLIP object nouns. We assume this is because CLIP has a high object bias and is less sensitive to the language of verbs. Finetuning CLIP by injecting verbs leads to better zero-shot performance in action recognition. Consequently, combining BLIP verbs and GPT3 verbs in the text bag leads to the best zero-shot transfer. 

\input{tab/bag_of_words}

% \myparagraph{How to learn from words in text bags?} 
\subsubsection{How to learn from words in text bags?}\label{sec:how_to_learn_from_text_bag}
In Table~\ref{tab:ablation_mil_nce}, we explore different strategies of learning from words in a text bag: (1) \textit{Cross entropy}: classification in a fixed class space. (2) \textit{NCE}: contrastive learning to encourage instance-level match between a pair of video and text. In this case, we randomly sample one text from the text bag in each iteration. (3) \textit{MIL-Max}: in each iteration, among words in a text bag, we choose the word with the maximum similarity to the video, and pass the similarity in the contrastive loss. 
(4) \textit{MIL-NCE}: as explained in Sec.~\ref{sec:mil-nce}, we softly associate a bag of texts with the video, and sum up the similarities of texts in a bag (5)  \textit{MIL-NCE only instance-level}: the \textit{MIL-NCE} on instance-level match between video and text bag, without encouraging videos and text bags with the same best matched text to be close to each other (see Sec.~\ref{sec:mil-nce}). 
%
In Table~\ref{tab:ablation_mil_nce}, we see that cross entropy of classification in a fixed class space leads to the most inferior result, while our MIL-NCE achieves the best improvement. Encouraging videos and text bags with the same best matched text to be close to each other also leads to some performance boost in contrast to only instance-level matching.
%upon case of only instance-level match. 

\input{tab/ablation_mil_nce}

% \myparagraph{Bag size.} 
\subsubsection{Bag size}\label{sec:bag_size}
We perform an ablation on the bag size in Table~\ref{tab:bag_size}. A bag size of 1 is the same as \textit{NCE} loss with random word sampling in Table~\ref{tab:ablation_mil_nce}. Increasing the bag size from lower numbers to 8 leads to consistent performance improvements. Using bag size 16 has further slight performance boost. We report our main results with a bag size of 16. 

\input{tab/bag_size}
\section{Conclusion}
In this work, we consider the task of leveraging unlabeled video collections and a set of language sources to finetune the \vl{} model for improved zero-shot action recognition. To our best knowledge, our approach `\oursfull{}' (\OurMethod) is the first of this kind. Specifically, we leverage a set of language sources (unpaired action dictionaries, Large Language Models and VL models) to construct a text bag for each unlabeled video. Then we use the unlabeled videos and text bags to finetune the \vl{} model with the objective of Multiple Instance Learning. Our extensive evaluation for zero-shot and few-shot action recognition across several unseen action benchmarks demonstrate significant performance improvement over the source \vl{} model, as well as improvement over baselines trained in a fully supervised manner. 


% \newpage
\appendix
\section*{Supplementary}
\input{tex/supp_content_for_arxiv}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
