@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@inproceedings{kryscinski-etal-2019-neural,
    title = "Neural Text Summarization: A Critical Evaluation",
    author = "Kryscinski, Wojciech  and
      Keskar, Nitish Shirish  and
      McCann, Bryan  and
      Xiong, Caiming  and
      Socher, Richard",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1051",
    doi = "10.18653/v1/D19-1051",
    pages = "540--551",
    abstract = "Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overfit to layout biases of current datasets and offer limited diversity in their outputs.",
}

@inproceedings{gao-wan-2022-dialsummeval,
    title = "{D}ial{S}umm{E}val: Revisiting Summarization Evaluation for Dialogues",
    author = "Gao, Mingqi  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.418",
    doi = "10.18653/v1/2022.naacl-main.418",
    pages = "5693--5709",
    abstract = "Dialogue summarization is receiving increasing attention from researchers due to its extraordinary difficulty and unique application value. We observe that current dialogue summarization models have flaws that may not be well exposed by frequently used metrics such as ROUGE. In our paper, we re-evaluate 18 categories of metrics in terms of four dimensions: coherence, consistency, fluency and relevance, as well as a unified human evaluation of various models for the first time. Some noteworthy trends which are different from the conventional summarization tasks are identified. We will release DialSummEval, a multi-faceted dataset of human judgments containing the outputs of 14 models on SAMSum.",
}

@inproceedings{vasilyev-etal-2020-fill,
    title = "Fill in the {BLANC}: Human-free quality estimation of document summaries",
    author = "Vasilyev, Oleg  and
      Dharnidharka, Vedant  and
      Bohannon, John",
    booktitle = "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.eval4nlp-1.2",
    doi = "10.18653/v1/2020.eval4nlp-1.2",
    pages = "11--20",
    abstract = "We present BLANC, a new approach to the automatic estimation of document summary quality. Our goal is to measure the functional performance of a summary with an objective, reproducible, and fully automated method. Our approach achieves this by measuring the performance boost gained by a pre-trained language model with access to a document summary while carrying out its language understanding task on the document{'}s text. We present evidence that BLANC scores have as good correlation with human evaluations as do the ROUGE family of summary quality measurements. And unlike ROUGE, the BLANC method does not require human-written reference summaries, allowing for fully human-free summary quality estimation.",
}



@inproceedings{zeng-etal-2020-meddialog,
    title = "{M}ed{D}ialog: Large-scale Medical Dialogue Datasets",
    author = "Zeng, Guangtao  and
      Yang, Wenmian  and
      Ju, Zeqian  and
      Yang, Yue  and
      Wang, Sicheng  and
      Zhang, Ruisi  and
      Zhou, Meng  and
      Zeng, Jiaqi  and
      Dong, Xiangyu  and
      Zhang, Ruoyu  and
      Fang, Hongchao  and
      Zhu, Penghui  and
      Chen, Shu  and
      Xie, Pengtao",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.743",
    doi = "10.18653/v1/2020.emnlp-main.743",
    pages = "9241--9250",
    abstract = "Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets {--} MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System",
}

@inproceedings{post-2018-call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}


@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@InProceedings{kumar+byrne:2004,
  author = 		 {Shankar Kumar and William Byrne},
  title = 		 {Minimum Bayes-Risk Decoding for Statistical Machine Translation},
  booktitle = 	 {Proceedings of HLT-NAACL},
  year =		 2004
}

@article{unilm,
  author    = {Li Dong and
               Nan Yang and
               Wenhui Wang and
               Furu Wei and
               Xiaodong Liu and
               Yu Wang and
               Jianfeng Gao and
               Ming Zhou and
               Hsiao{-}Wuen Hon},
  title     = {Unified Language Model Pre-training for Natural Language Understanding
               and Generation},
  journal   = {CoRR},
  volume    = {abs/1905.03197},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.03197},
  archivePrefix = {arXiv},
  eprint    = {1905.03197},
  timestamp = {Mon, 05 Oct 2020 12:53:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-03197.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{cnndm,
author = {Hermann, Karl Moritz and Ko\v{c}isk\'{y}, Tom\'{a}\v{s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
title = {Teaching Machines to Read and Comprehend},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1693–1701},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{mrini-etal-2021-gradually,
    title = "A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding",
    author = "Mrini, Khalil  and
      Dernoncourt, Franck  and
      Yoon, Seunghyun  and
      Bui, Trung  and
      Chang, Walter  and
      Farcas, Emilia  and
      Nakashole, Ndapa",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.119",
    doi = "10.18653/v1/2021.acl-long.119",
    pages = "1505--1515",
    abstract = "Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss. We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer. We show through ablation studies that our proposed novelties improve performance. Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation. Finally, we show that our method fares better than single-task learning under 4 low-resource settings.",
}

@inproceedings{yuan-etal-2022-biobart,
    title = "{B}io{BART}: Pretraining and Evaluation of A Biomedical Generative Language Model",
    author = "Yuan, Hongyi  and
      Yuan, Zheng  and
      Gan, Ruyi  and
      Zhang, Jiaxing  and
      Xie, Yutao  and
      Yu, Sheng",
    booktitle = "Proceedings of the 21st Workshop on Biomedical Language Processing",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bionlp-1.9",
    doi = "10.18653/v1/2022.bionlp-1.9",
    pages = "97--109",
    abstract = "Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks.",
}

@inproceedings{lavie-agarwal-2007-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with High Levels of Correlation with Human Judgments",
    author = "Lavie, Alon  and
      Agarwal, Abhaya",
    booktitle = "Proceedings of the Second Workshop on Statistical Machine Translation",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-0734",
    pages = "228--231",
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{McHugh2012InterraterRT,
  title={Interrater reliability: the kappa statistic},
  author={Mary L. McHugh},
  journal={Biochemia Medica},
  year={2012},
  volume={22},
  pages={276 - 282}
}

@inproceedings{beltagy-etal-2019-scibert,
    title = "{S}ci{BERT}: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Cohan, Arman",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1371",
    doi = "10.18653/v1/D19-1371",
    pages = "3615--3620",
    abstract = "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
}


@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@inproceedings{
pillutla2021mauve,
title={{MAUVE}: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
author={Krishna Pillutla and Swabha Swayamdipta and Rowan Zellers and John Thickstun and Sean Welleck and Yejin Choi and Zaid Harchaoui},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=Tqx7nJp7PR}
}

@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}



@inproceedings{popovic-2015-chrf,
    title = "chr{F}: character n-gram {F}-score for automatic {MT} evaluation",
    author = "Popovi{\'c}, Maja",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3049",
    doi = "10.18653/v1/W15-3049",
    pages = "392--395",
}


@inproceedings{grusky-etal-2018-newsroom,
    title = "{N}ewsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
    author = "Grusky, Max  and
      Naaman, Mor  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1065",
    doi = "10.18653/v1/N18-1065",
    pages = "708--719",
    abstract = "We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.",
}


@inproceedings{dialogpt,
    title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
    author={Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
    year={2020},
    booktitle={ACL, system demonstration}
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang2019bridging,
    title = "Bridging the Gap between Training and Inference for Neural Machine Translation",
    author = "Zhang, Wen and Feng, Yang and Meng, Fandong and You, Di and Liu, Qun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    url = "https://www.aclweb.org/anthology/P19-1426",
    doi = "10.18653/v1/P19-1426",
    pages = "4334--4343",
}

@inproceedings{och-2003-minimum,
    title = "Minimum Error Rate Training in Statistical Machine Translation",
    author = "Och, Franz Josef",
    booktitle = "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2003",
    address = "Sapporo, Japan",
    url = "https://www.aclweb.org/anthology/P03-1021",
    doi = "10.3115/1075096.1075117",
    pages = "160--167",
}

@misc{sgd,
      title={Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset}, 
      author={Abhinav Rastogi and Xiaoxue Zang and Srinivas Sunkara and Raghav Gupta and Pranav Khaitan},
      year={2020},
      eprint={1909.05855},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{topical_chat,
  author={Karthik Gopalakrishnan and Behnam Hedayatnia and Qinlang Chen and Anna Gottardi and Sanjeev Kwatra and Anu Venkatesh and Raefer Gabriel and Dilek Hakkani-Tür},
  title={{Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={1891--1895},
  doi={10.21437/Interspeech.2019-3079},
  url={http://dx.doi.org/10.21437/Interspeech.2019-3079}
}


@inproceedings{MetaLWOZ,
  author={Sungjin Lee and Hannes Schulz and Adam Atkinson adn Jianfeng Gao and Kaheer Suleman and Layla El Asri and Mahmoud Adada and Minlie Huang and Shikhar Sharma and Wendy Tay and Xiujun Li},
  title={{Multi-Domain Task-Completion Dialog Challenge}},
  year=2019,
  booktitle={Dialog System Technology Challenges 8},
}


@misc{taskmaster1,
      title={Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset}, 
      author={Bill Byrne and Karthik Krishnamoorthi and Chinnadhurai Sankar and Arvind Neelakantan and Daniel Duckworth and Semih Yavuz and Ben Goodrich and Amit Dubey and Andy Cedilnik and Kyu-Young Kim},
      year={2019},
      eprint={1909.05358},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{li2018microsoft,
  title={Microsoft Dialogue Challenge: Building End-to-End Task-Completion Dialogue Systems},
  author={Li, Xiujun and Panda, Sarah and Liu, Jingjing and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1807.11125},
  year={2018}
}

@misc{roberta_wd,
      title={An End-to-End Dialogue State Tracking System with Machine Reading Comprehension and Wide \& Deep Classification}, 
      author={Yue Ma and Zengfeng Zeng and Dawei Zhu and Xuan Li and Yiying Yang and Xiaoyuan Yao and Kaijie Zhou and Jianping Shen},
      year={2020},
      eprint={1912.09297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  archivePrefix = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{listrank,
  acmid = {1273513},
  added-at = {2012-10-21T11:40:26.000+0200},
  address = {New York, NY, USA},
  author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
  biburl = {https://www.bibsonomy.org/bibtex/223529cb5975d43bf3f0a95176449af6f/nosebrain},
  booktitle = {Proceedings of the 24th international conference on Machine learning},
  doi = {10.1145/1273496.1273513},
  interhash = {b944a97bdfce90bb455e9d8998caa4ea},
  intrahash = {23529cb5975d43bf3f0a95176449af6f},
  isbn = {978-1-59593-793-3},
  keywords = {ListNet learning listwise ranking},
  location = {Corvalis, Oregon},
  numpages = {8},
  pages = {129--136},
  publisher = {ACM},
  series = {ICML '07},
  timestamp = {2012-12-25T15:48:32.000+0100},
  title = {Learning to rank: from pairwise approach to listwise approach},
  url = {http://doi.acm.org/10.1145/1273496.1273513},
  year = 2007
}


[Budzianowski et al. 2018]
@inproceedings{budzianowski2018large,
    Author = {Budzianowski, Pawe{\l} and Wen, Tsung-Hsien and Tseng, Bo-Hsiang  and Casanueva, I{\~n}igo and Ultes Stefan and Ramadan Osman and Ga{\v{s}}i\'c, Milica},
    title={MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling},
    booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    year={2018}
}

[Ramadan et al. 2018]
@inproceedings{ramadan2018large,
  title={Large-Scale Multi-Domain Belief Tracking with Knowledge Sharing},
  author={Ramadan, Osman and Budzianowski, Pawe{\l} and Gasic, Milica},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  volume={2},
  pages={432--437},
  year={2018}
}

[Eric et al. 2019]
@article{eric2019multiwoz,
  title={MultiWOZ 2.1: Multi-Domain Dialogue State Corrections and State Tracking Baselines},
  author={Eric, Mihail and Goel, Rahul and Paul, Shachi and Sethi, Abhishek and Agarwal, Sanchit and Gao, Shuyag and Hakkani-Tur, Dilek},
  journal={arXiv preprint arXiv:1907.01669},
  year={2019}
}

[Zang et al. 2020]
@inproceedings{zang2020multiwoz,
  title={MultiWOZ 2.2: A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines},
  author={Zang, Xiaoxue and Rastogi, Abhinav and Sunkara, Srinivas and Gupta, Raghav and Zhang, Jianguo and Chen, Jindong},
  booktitle={Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, ACL 2020},
  pages={109--117},
  year={2020}
}

@article{DSTC9,
   title={The Ninth Dialog System Technology Challenge},
   author={Chulaka Gunasekara and Abhinav Rastogi and Yun-Nung Chen and Luis Fernando D'Haro and Seokhwan Kim and Mihail Eric and Behnam Hedayatnia and Karthik Gopalakrishnan and Yang Liu and Chao-Wei Huang and Dilek Hakkani-tur and Baolin Peng and Jianfeng Gao and Jinchao Li and Lars Liden and Minlie Huang and Qi Zhu and Runze Liang and Ryuichi Takanobu and Shahin Shayandeh and Swadheen Shukla and Zheng Zhang and Shikib Mehri and Yulan Feng and Carla Gordon and Seyed Hossein Alavi and David Traum and Maxine Eskenazi and Ahmad Beirami and Eunjoon (EJ) Cho and Paul A. Crook and Ankita De and Alborz Geramifard and Satwik Kottur and Seungwhan Moon and Shivani Poddar and Rajen Subba},
   journal={arXiv preprint},
   year={2020}
}

@misc{kim2020domain,
      title={Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access}, 
      author={Seokhwan Kim and Mihail Eric and Karthik Gopalakrishnan and Behnam Hedayatnia and Yang Liu and Dilek Hakkani-Tur},
      year={2020},
      eprint={2006.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{topical_chat_asr,
  author={Gopalakrishnan, Karthik and Hedayatnia, Behnam and Wang, Longshaokan and Liu, Yang and Hakkani-Tür, Dilek},
  title={{Are Neural Open-Domain Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study}},
  year={2020},
  booktitle={INTERSPEECH}
}

@misc{dstc9overview,
      title={Overview of the Ninth Dialog System Technology Challenge: DSTC9}, 
      author={Chulaka Gunasekara and Seokhwan Kim and Luis Fernando D'Haro and Abhinav Rastogi and Yun-Nung Chen and Mihail Eric and Behnam Hedayatnia and Karthik Gopalakrishnan and Yang Liu and Chao-Wei Huang and Dilek Hakkani-Tür and Jinchao Li and Qi Zhu and Lingxiao Luo and Lars Liden and Kaili Huang and Shahin Shayandeh and Runze Liang and Baolin Peng and Zheng Zhang and Swadheen Shukla and Minlie Huang and Jianfeng Gao and Shikib Mehri and Yulan Feng and Carla Gordon and Seyed Hossein Alavi and David Traum and Maxine Eskenazi and Ahmad Beirami and Eunjoon and Cho and Paul A. Crook and Ankita De and Alborz Geramifard and Satwik Kottur and Seungwhan Moon and Shivani Poddar and Rajen Subba},
      year={2020},
      eprint={2011.06486},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{goodwin-etal-2020-flight,
    title = "Flight of the {PEGASUS}? Comparing Transformers on Few-shot and Zero-shot Multi-document Abstractive Summarization",
    author = "Goodwin, Travis  and
      Savery, Max  and
      Demner-Fushman, Dina",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.494",
    doi = "10.18653/v1/2020.coling-main.494",
    pages = "5640--5646",
    abstract = "Recent work has shown that pre-trained Transformers obtain remarkable performance on many natural language processing tasks including automatic summarization. However, most work has focused on (relatively) data-rich single-document summarization settings. In this paper, we explore highly-abstractive multi-document summarization where the summary is explicitly conditioned on a user-given topic statement or question. We compare the summarization quality produced by three state-of-the-art transformer-based models: BART, T5, and PEGASUS. We report the performance on four challenging summarization datasets: three from the general domain and one from consumer health in both zero-shot and few-shot learning settings. While prior work has shown significant differences in performance for these models on standard summarization tasks, our results indicate that with as few as 10 labeled examples there is no statistically significant difference in summary quality, suggesting the need for more abstractive benchmark collections when determining state-of-the-art.",
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@inproceedings{ben-abacha-demner-fushman-2019-summarization,
    title = "On the Summarization of Consumer Health Questions",
    author = "Ben Abacha, Asma  and
      Demner-Fushman, Dina",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    url = "https://www.aclweb.org/anthology/P19-1215",
    doi = "10.18653/v1/P19-1215",
    pages = "2228--2234",
    abstract = "Question understanding is one of the main challenges in question answering. In real world applications, users often submit natural language questions that are longer than needed and include peripheral information that increases the complexity of the question, leading to substantially more false positives in answer retrieval. In this paper, we study neural abstractive models for medical question summarization. We introduce the MeQSum corpus of 1,000 summarized consumer health questions. We explore data augmentation methods and evaluate state-of-the-art neural abstractive models on this new task. In particular, we show that semantic augmentation from question datasets improves the overall performance, and that pointer-generator networks outperform sequence-to-sequence attentional models on this task, with a ROUGE-1 score of 44.16{\%}. We also present a detailed error analysis and discuss directions for improvement that are specific to question summarization.",
}

@Inproceedings{MEDIQA2021,
author = {Asma {Ben Abacha} and Yassine Mrabet and Yuhao Zhang and Chaitanya Shivade and Curtis Langlotz and Dina Demner-Fushman},
title = {Overview of the MEDIQA 2021 Shared Task on Summarization in the Medical Domain},
booktitle = {Proceedings of the 20th SIGBioMed Workshop on Biomedical Language Processing, NAACL-BioNLP 2021},  
year = {2021}}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    url = "https://www.aclweb.org/anthology/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
}

@article{raffel2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@InProceedings{zhang2019pegasus,
  title = 	 {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author =       {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11328--11339},
  year = 	 {2020},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf},
  url = 	 {http://proceedings.mlr.press/v119/zhang20ae.html}
}

@article{see17pg,
  author    = {Abigail See and
               Peter J. Liu and
               Christopher D. Manning},
  title     = {Get To The Point: Summarization with Pointer-Generator Networks},
  journal   = {CoRR},
  volume    = {abs/1704.04368},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04368},
  archivePrefix = {arXiv},
  eprint    = {1704.04368},
  timestamp = {Mon, 13 Aug 2018 16:46:08 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SeeLM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{narayan18xsum,
  author =      "Shashi Narayan and Shay B. Cohen and Mirella Lapata",
  title =       "Don't Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization",
  booktitle =   "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ",
  year =        "2018",
  address =     "Brussels, Belgium",
}

@inproceedings{bi20palm,
    title = "{PALM}: Pre-training an Autoencoding{\&}Autoregressive Language Model for Context-conditioned Generation",
    author = "Bi, Bin  and
      Li, Chenliang  and
      Wu, Chen  and
      Yan, Ming  and
      Wang, Wei  and
      Huang, Songfang  and
      Huang, Fei  and
      Si, Luo",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.700",
    doi = "10.18653/v1/2020.emnlp-main.700",
    pages = "8681--8691",
}

@inproceedings{dong19unilm,
    title={Unified Language Model Pre-training for Natural Language Understanding and Generation},
    author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
    year={2019},
    booktitle = "33rd Conference on Neural Information Processing Systems (NeurIPS 2019)"
}

@inproceedings{charton-etal-2014-improving,
    title = "Improving Entity Linking using Surface Form Refinement",
    author = "Charton, Eric  and
      Meurs, Marie-Jean  and
      Jean-Louis, Ludovic  and
      Gagnon, Michel",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/899_Paper.pdf",
    pages = "4609--4615",
}

@inproceedings{zhang20bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@inproceedings{mrabet-demner-fushman-2020-holms,
    title = "{HOLMS}: Alternative Summary Evaluation with Large Language Models",
    author = "Mrabet, Yassine  and
      Demner-Fushman, Dina",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    url = "https://www.aclweb.org/anthology/2020.coling-main.498",
    doi = "10.18653/v1/2020.coling-main.498",
    pages = "5679--5688",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@article{soualmia2012matching,
  title={Matching health information seekers' queries to medical terms},
  author={Soualmia, Lina F and Prieur-Gaston, Elise and Moalla, Zied and Lecroq, Thierry and Darmoni, St{\'e}fan J},
  journal={BMC bioinformatics},
  volume={13},
  number={14},
  pages={1--15},
  year={2012},
  publisher={BioMed Central}
}

@inproceedings{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I},
  booktitle={Soviet physics doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966},
}

@article{Zhou2015ContextSensitiveSC,
  title={Context-Sensitive Spelling Correction of Consumer-Generated Content on Health Care},
  author={X. Zhou and An Zheng and Jiaheng Yin and R. Chen and Xianyang Zhao and Wei Xu and Wenqing Cheng and T. Xia and S. Lin},
  journal={JMIR Medical Informatics},
  year={2015}
}

@article{jin2021biomedical,
  title={Biomedical Question Answering: A Comprehensive Review},
  author={Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng},
  journal={arXiv preprint arXiv:2102.05281},
  year={2021}
}

@inproceedings{mi2021dstc,
  title={Towards Generalized Models for Beyond Domain API Task-oriented Dialogue},
  author={Haitao Mi and Qiyu Ren and Yinpei Dai and Yifan He and Jian Sun and Yongbin Li and Jing Zheng and Peng Xu},
  booktitle={Proceedings of the 9th Dialog System Technology Challenge},
  year={2021},
}

@inproceedings{gliwa-etal-2019-samsum,
    title = "{SAMS}um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
    author = "Gliwa, Bogdan  and
      Mochol, Iwona  and
      Biesek, Maciej  and
      Wawer, Aleksander",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    url = "https://aclanthology.org/D19-5409",
    doi = "10.18653/v1/D19-5409",
    pages = "70--79",
}

@inproceedings{ben-abacha-etal-2021-overview,
    title = "Overview of the {MEDIQA} 2021 Shared Task on Summarization in the Medical Domain",
    author = "Ben Abacha, Asma  and
      Mrabet, Yassine  and
      Zhang, Yuhao  and
      Shivade, Chaitanya  and
      Langlotz, Curtis  and
      Demner-Fushman, Dina",
    booktitle = "Proceedings of the 20th Workshop on Biomedical Language Processing",
    month = jun,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.bionlp-1.8",
    doi = "10.18653/v1/2021.bionlp-1.8",
    pages = "74--85",
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}


@inproceedings{graham_re-evaluating_2015,
	location = {Lisbon, Portugal},
	title = {Re-evaluating Automatic Summarization with {BLEU} and 192 Shades of {ROUGE}},
	url = {https://aclanthology.org/D15-1013},
	doi = {10.18653/v1/D15-1013},
	eventtitle = {{EMNLP} 2015},
	pages = {128--137},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Graham, Yvette},
	urldate = {2022-06-25},
	date = {2015},
}

@article{deutsch_re-examining_2022,
	title = {Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics},
	url = {http://arxiv.org/abs/2204.10216},
	abstract = {How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by system-level correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice. This allows us to demonstrate that our best estimate of the correlation of {ROUGE} to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.},
	journaltitle = {{arXiv}:2204.10216 [cs]},
	author = {Deutsch, Daniel and Dror, Rotem and Roth, Dan},
	urldate = {2022-06-25},
	date = {2022},
	year = "2022",
	eprinttype = {arxiv},
	eprint = {2204.10216},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{bhandari_re-evaluating_2020,
	location = {Online},
	title = {Re-evaluating Evaluation in Text Summarization},
	url = {https://aclanthology.org/2020.emnlp-main.751},
	doi = {10.18653/v1/2020.emnlp-main.751},
	abstract = {Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years {ROUGE} has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).},
	eventtitle = {{EMNLP} 2020},
	pages = {9347--9359},
	year = "2020",
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bhandari, Manik and Gour, Pranav Narayan and Ashfaq, Atabak and Liu, Pengfei and Neubig, Graham},
	urldate = {2022-06-25},
	date = {2020},
}

@article{fabbri_summeval_2021,
	title = {Summeval: Re-evaluating summarization evaluation},
	volume = {9},
	shorttitle = {Summeval},
	pages = {391--409},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	year = "2021",
	author = {Fabbri, Alexander R. and Kryściński, Wojciech and {McCann}, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	date = {2021},
	note = {Publisher: {MIT} Press},
}

@article{nallapati_abstractive_2016,
	title = {Abstractive text summarization using sequence-to-sequence rnns and beyond},
	journaltitle = {{arXiv} preprint {arXiv}:1602.06023},
	author = {Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing},
	year = "2016",
	date = {2016},
}

@inproceedings{roy_reassessing_2021,
	title = {Reassessing automatic evaluation metrics for code summarization tasks},
	pages = {1105--1116},
	booktitle = {Proceedings of the 29th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	year = "2021",
	author = {Roy, Devjeet and Fakhoury, Sarah and Arnaoudova, Venera},
	date = {2021},
}

@article{chen_thorough_2016,
	title = {A thorough examination of the cnn/daily mail reading comprehension task},
	journaltitle = {{arXiv} preprint {arXiv}:1606.02858},
	author = {Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
	date = {2016},
	year = "2016",
}

@inproceedings{jacquenet_meeting_2019,
	title = {Meeting summarization, a challenge for deep learning},
	pages = {644--655},
	booktitle = {International Work-Conference on Artificial Neural Networks},
	publisher = {Springer},
	author = {Jacquenet, François and Bernard, Marc and Largeron, Christine},
	date = {2019},
	year = "2019",
}