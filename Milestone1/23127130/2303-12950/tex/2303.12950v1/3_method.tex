\section{Method}
In this section, we describe the framework of LightPainter. As shown in Figure~\ref{fig2}, LightPainter consists of a frontend interactive application for the user to scribble and a backend performing relighting with respect to the user input. A detailed walk-through of the frontend interface can be found in the supplementary. Here we focus our discussion on the backend. Specifically, the backend comprises two conditional neural networks: a skin-tone-conditioned delighting module and a scribble-conditioned relighting module. The delighting stage recovers the geometry representing per-pixel surface normal, and an albedo image that optionally follows a user-selected skin tone. After delighting, estimated normal and albedo are transmitted to the relighting module, which renders the portrait under the lighting condition that respects the user's scribbles. In Section~\ref{sec:delight} and~\ref{sec:relight}, we describe each stage in detail. We define our training objectives in Section~\ref{sec:loss}. 

\begin{figure}[t!]
	\centering
	\includegraphics[clip, trim=0cm 11cm 12cm 0cm, width=1\columnwidth]{figures/simulation.pdf}
% 	\vskip-8pt	
	\caption{\textbf{An example of the simulated scribbles.} (a) A complete shading obtained from the Phong shading~\cite{phong1975illumination}. (b) Segmented superpixels after quantization and color/intensity average. Each segment is coarse in both shape and intensity. (c) Simulated scribbles generated by sampling from (b). We fill in Gaussian noise into the empty region and background.}\label{simulation}
\end{figure}
\subsection{Delighting Module}\label{sec:delight}
 Inspired by Total Relighting~\cite{pandey2021total}, LightPainter uses two networks to separately estimate geometry and reflectance. For geometry, we use the existing algorithm~\cite{bae2021estimating} and fine-tune on our dataset. Our main difference from prior works is the reflectance prediction, where we propose a skin-tone-conditioned albedo model.

\subsubsection{Skin-tone-conditioned Albedo Net} 
Data-driven albedo prediction is challenging as it requires a fully comprehensive and balanced dataset to avoid any skin tone bias. To address this challenge and recover an accurate albedo, we explore an alternative solution by leveraging user control. This is built upon the observation that the skin tone of a subject can be easily specified in practice, for example, using the skin swatches on a cosmetics website\footnote{For example,
\href{https://www.sephora.com/contentimages/productpages/pdfs2017/StudioUndertoneFinder_v3.pdf}{skin-tone finder} provided by Sephora}. We thus propose to leverage user interaction to aid our albedo generation. Specifically, we ask the user to optionally provide a skin color to the system. As shown in Figure~\ref{fig2}, the network then generates the albedo conditioned on the received color vector {$\hat{v}$}, along with the estimated normal and the source portrait. At training time, $\hat{v}$ can be extracted as the mean skin color from the ground truth albedo. For inference, if {$\hat{v}$} is not provided by the user, the albedo generation scheme falls back to a standard unconditional method.

It is crucial to determine how to best leverage {$\hat{v}$}. Intuitively, the skin tone should be accessible by all pixels in the skin region for guidance. The network must also be designed to follow the guidance so as the generated albedo matches the user's desire. In the following, we introduce a new technique, dubbed \textit{SkinFill}, by drawing inspiration from the makeup routine.
\vspace{-4mm}
\paragraph{SkinFill.} In standard makeup routine, skin color can be modified by blending foundation smoothly over the facial skin, followed by local retouching. This inspires two design choices of SkinFill: (1) matching the exact skin tone by uniformly shifting the pixel value in the skin region, (2) recovering local facial details from the input to ensure fidelity and realism. Specifically, SkinFill first creates a per-pixel skin-tone representation $T$, the \textit{tone map}, by filling in the skin-parsing mask $M_{skin}$ with color $\hat{v}$, i.e. $T = M_{skin}\odot \hat{v}$. The tone map $T$ is then used to condition the network for better facial detail recovery, and directly added to the network prediction to shift the pixel value in the skin region (see the Delighting part in Figure \ref{fig2}). SkinFill brings several benefits:  (1) the tone map makes user guidance easily accessible at all skin pixels. (2) uniformly shifting the skin color towards $\hat{v}$ enforces the prediction to follow the user's intention. (3) The network can focus on recovering the local facial details without the need of regressing the skin tone.

\subsection{Relighting Module}~\label{sec:relight}
In this section, we describe the scribble-based relighting module in detail. To begin with, we introduce a scribble simulation algorithm that enables training a network with synthetic scribbles that resemble real users' inputs. We also describe the shading network that renders a realistic relit portrait conditioned on the input scribbles.
\vspace{-3mm}
\subsubsection{Scribble Simulation} 
In order to train a relighting network that is conditioned on scribbles, we propose a scribble simulation algorithm that automatically generates scribbles that mimic real user inputs and with large variations. 
% The simulation algorithm must be tractable with standard relighting data and mimic the users' input as much as possible so that the trained network can generalize to real scenarios. 

As aforementioned, we use ``shading stripes" to represent user scribbles, which reflect local shading patterns. Different shading levels naturally correspond to different scribble intensities. Hence the first step in our simulation algorithm is to obtain a full shading map. This is accomplished by rendering with the Phong shading~\cite{phong1975illumination} model using the ground truth geometry and environment map. As shown in Figure ~\ref{simulation} (a), the rendered shading map can be viewed as an ``ideal'' scribble containing detailed and complete lighting information. However, drawings from novices are usually noisy -- irregular and incomplete. To make our system robust to these imperfect inputs, we apply a series of augmentations to model these defects in real scribbles. Specifically, we first convert the shading into $Lab$ color space and ``coarsen'' the luminance channel $L$ by randomly quantizing it into multiple bins. Then we perform a superpixel segmentation using SEEDS~\cite{bergh2012seeds} and average the color within each segment. As shown in Figure \ref{simulation} (b), each segment ends up being coarse in intensity and shape, similar to the noisy scribble that novices tend to draw. Finally, we ``sparsify'' the simulated scribbles by randomly sampling a small subset of segments at each training step. The sampling rate is drawn from a truncated exponential distribution with $\lambda=3$, which results in mostly sparse inputs. In addition, we always keep the segments of top $5\%$ brightest and darkest intensity to make sure our sampling captures the full dynamic range of shading. On the other hand, this also ensures that the most representative lighting information is preserved to help the network reasonably complete the full shading map. An example of simulated scribbles is illustrated in Figure~\ref{simulation} (c). More details and hyper-parameter choices can be found in the supplement. 

\subsubsection{Scribble-conditioned Shading Net}
\paragraph{Two-Step Relighting Pipeline.} In prior works, relighting is often performed in a single step by taking the lighting condition as a conditional input. In our interactive setting, the lighting condition is from the user scribbles, which can be local, sparse and coarse. We found that directly predicting the relit images from the scribbles does not perform well, where the network would struggle at generating a plausible completion of shading.

To address this challenge, we introduce a two-step relighting pipeline: The first step completes shading following the subject geometry, and the second stage refines the completed shading to render a relit image. As shown in Figure~\ref{fig2}, we use the bottom branch to complete a low-resolution shading map conditioned on the normal and scribbles. The output is supervised with the ground truth shading, which enforces the network to learn to propagate sparse lighting information following the surface geometry, which encourages geometry-consistent relighting. The shading feature is then concatenated with the albedo feature (encoded by the upper brunch), and transmitted to the decoder. The decoder then refines the completed shading and renders the final image.
\vspace{-3mm}
\paragraph{Improved Network Architecture.} We adapt our network architecture to better tolerate ``incomplete" user scribbles. Intuitively, the receptive field of the network should be sufficiently large so that the network can leverage the global context given sparse and local user input. To achieve this, we build our network with a U-Shaped structure~\cite{ronneberger2015u} and further adapt it with additional dilated convolutions~\cite{yu2016dilated} and non-local modules~\cite{wang2018non}. These improvements allow a global receptive field and enhance the information flow among distant locations, thus are more suitable for our task. We will demonstrate that our architectural design is crucial for faithful relighting from sparse scribbles.
\subsection{Training Objective}~\label{sec:loss}
To ensure both realism and fidelity of the relit image,
our neural network optimizes the following training objectives: 

\textbf{Reconstruction loss} $\m{L}_{R_{alb}}$ and $\m{L}_{R_{relit}}$: the standard $L1$ distance between the generated albedo/portrait and the ground truth albedo/portrait to ensure content fidelity. 

\textbf{Perceptual loss~\cite{zhang2018unreasonable}} $\m{L}_{P_{alb}}$ and $\m{L}_{P_{relit}}$: the features-wise distance of the predicted albedo/portrait and ground truth albedo/portrait extracted by a pre-trained VGG~\cite{vgg}. This loss is used to improve visual quality.

\textbf{Shading reconstruction loss} $\m{L}_{R_{shad}}$: the standard $L1$ distance between the completed low-resolution shading and the downscaled ground truth to enforce shading completion.

The overall loss can be expressed
as follows:
\begin{align}
\vspace{-2mm}
    \m{L} = \m{L}_{R_{alb}}+\m{L}_{P_{alb}}+\m{L}_{R_{relit}}+\m{L}_{P_{relit}}+\m{L}_{R_{shad}}
\end{align}
