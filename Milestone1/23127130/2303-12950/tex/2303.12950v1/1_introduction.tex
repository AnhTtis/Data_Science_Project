\section{Introduction}

Lighting is a fundamental aspect of portrait photograph, as lights shape the reality, and give the work depth, colorfulness and excitement. Professional photographers~\cite{schriever1909complete,grey2014master} spend hours designing lighting such that shadow and highlight are distributed accurately on the subject to achieve the desired photographic look. Getting the exact lighting setups requires years of training, expensive equipment, environment setup, timing, and costly teamwork.
Recently, portrait relighting techniques~\cite{pandey2021total, shu2017portrait, sun2019single, zhou2019deep, hou2022face,zhang2020portrait,wang2020single,yeh2022learning,Hou_2021_CVPR} allow users to apply a different lighting condition to a portrait photo. These methods require a given lighting condition: some use an exemplar image \cite{shu2017portrait, shih2014style}, which lacks precise lighting control and requires exhaustive image search to find the specific style; some use a high dynamic range (HDR) environment maps~\cite{wang2020single,sun2019single,pandey2021total,yeh2022learning} that is difficult and unintuitive to interpret or edit.

Hand-drawn sketches and scribbles have been shown to be good for user interaction and thus are widely used in various image editing applications\cite{chen2018sketchygan,zeng2022sketchedit,nazeri2019edgeconnect,elder1998image,dekel2018sparse,olszewski2020intuitive}.
Inspired by this, we propose \textit{\textbf{LightPainter}}, a scribble-based interactive portrait relighting system. As shown in Figure ~\ref{fig:teaser}, LightPainter is an intuitive and flexible lighting editing system that only requires casual scribbles drawn on the input. 
% We demonstrate that LightPainter is able to interpret these freehand scribbles as desired lighting effects.
Unlike widely-used lighting representations such as environment maps and spherical harmonics, it is non-trivial to interpret free-hand scribbles as lighting effects for a number of challenges. %In this work, we design LightPainter to address the following three main challenges via carefully designed neural networks and training strategies. %In this work, we design LightPainter to address the following three main challenges via carefully designed neural networks and training strategies. 
%We observe three major challenges to design a reliable scribble-based relighting system, and propose solutions for each. \zs{Do we have some remarks on user experience?}



The first challenge is simulating scribbles to mimic real free-hand input as it is impractical to collect a large number of human inputs. In addition, unlike other sketch-based editing tasks~\cite{chen2018sketchygan,zeng2022sketchedit,nazeri2019edgeconnect, elder1998image,dekel2018sparse,olszewski2020intuitive} where sketches can be computed from edges or orientation maps, there is no conventional way to connect scribbles with lighting effects. To address such challenge, we propose a scribble simulation algorithm that can generate a diverse set of synthetic scribbles that mimic real human inputs. For an interactive relighting task, scribbles should be flexible and expressive: easy to draw and accurately reflecting the lighting effect, such as changes in local shading and color. Compared to a shading map, scribbles are often ``incomplete'': users tend to sparsely place the scribbles on a few key areas on the face. Therefore, we propose to use a set of locally connected ``shading stripes'' to describe local shading patterns, including shape, intensity, and color, and use them to simulate scribbles. To this end, we simulate scribbles by starting from a full shading map and applying a series of operations to generate coarse and sparse shading stripes. We show that training with our synthetic scribbles enables the system to generalize well to real user scribbles from human inputs, with which our model can generate high-quality results with desirable lighting effects.

%First, \textbf{A better lighting representation for interaction: } unlike other editing  tasks~\cite{chen2018sketchygan,zeng2022sketchedit,nassssssszeri2019edgeconnect,elder1998image,dekel2018sparse,olszewski2020intuitive} where sketches can be naturally defined as edges or orientation maps, there is not an established way to properly interpret user scribbles in the context of relighting. A good interpretation must be intuitive for user to draw while reflecting meaningful lighting information so as to allow model to perform this task. We observed that a set of shading "stripes" describing local lighting patterns is a decent way to represent user scribbles, where different intensity of scribbles naturally corresponds to different shading levels. Based on this interpretation, we introduce a scribble simulation algorithm that can generate different types of synthetic scribbles. including sparse and incomplete ones to mimic imperfect user input. Specifically, we begin with a full shading map obtained from the traditional rendering pipeline, and apply a series of coarsening and sparsifying operations. Training with synthetic scribbles enables the system to tolerate creative but imperfect scribbles from users at the inference time, while leaving the challenging task of generating plausible lighting effects to the model.

% Second challenge
The second challenge is how to effectively use local and noisy scribbles to robustly represent portrait lighting that is often a global effect. LightPainter uses a carefully designed network architecture and training strategy to handle these discrepancies. Specifically, we introduce a two-step relighting pipeline to process sparse scribbles. The first stage produces a plausible completion of the shading map from the input scribbles and the geometry; the second stage refines the shading and renders the appearance with a learned albedo map. We propose a carefully designed neural network with an augmented receptive field. Compared with commonly-used UNet for portrait relighting~\cite{ pandey2021total,wang2020single,Hou_2022_CVPR,nestmeyer2020learning}, our design can better handle the sparse scribbles and achieve geometry-consistent relighting.

%Second, even though input scribbles from novices can be sparse and coarse, the relit images must still be complete and delicate. To overcome this challenge, we carefully design the training strategy and network architecture. We first introduce a two-step relighting pipeline to deal with sparse scribbles. The first step dedicates to produce a plausible completion of the shading by following the subjects' geometry, whereas the second stage refine the completed shading and fill in the appearance. We solve this two-step relighting through a carefully designed neural network with augmented receptive field. Compared with standard UNet~\cite{ronneberger2015u} (which is commonly used in the literature~\cite{pandey2021total,wang2020single,zhou2019deep,sengupta2018sfsnet,Hou_2021_CVPR,Hou_2022_CVPR,nestmeyer2020learning}), operating at larger spatial scale handles the sparse input better, resulting in a more geometry-consistent relighting.

% Third challenge
Last, there is one major challenge in portrait relighting that originates from the ill-posed nature of the intrinsic decomposition problem. That is to decouple albedo and shading from an image. It is also difficult to address with a learning framework due to the extreme scarcity of realistic labeled data and infinite possible lighting conditions for a scene. In the context of portrait relighting, it means recovering the true skin tone of a portrait subject is very challenging~\cite{weir2022deep,Feng:TRUST:ECCV2022}. Instead of trying to collect a balanced large-scale light-stage~\cite{debevec2000acquiring} dataset to capture the continuous and subtle variations in different skin tones, we propose an alternative solution dubbed \textit{SkinFill}. We draw inspiration from the standard makeup routine and design SkinFill to allow users to specify skin tone in our relighting pipeline. We use a \textit{tone map}, a per-pixel skin tone representation, to condition the albedo prediction to follow the exact skin tone as desired. This also naturally enables additional user control at inference time. 

%Third, the system needs to fully respect the skin tone of the portrait subject. One solution is to collect a comprehensive and balanced dataset. However, it is hardly practical to capture the continuous and subtle variations in different skin tones. We propose an alternative solution, dubbed \textit{SkinFill}, by drawing inspirations from the makeup industry. We use a \textit{tone map}, a per-pixel skin tone representation, to condition the albedo prediction to follow the exact skin tone as desired. This also naturally enables additional user control at inference time. 
%In addition, our system can also work automatically without skin tone input, but this SkinFill can add more controllability if users want to correct the skin color.

% \jz{May worth mentioning that our system can also work automatically without skin tone input, but this SkinFill can add more controllability if users want to correct the skin color.}
% A real application to should works equitably good for everyone. However, due to the data bias, it remains challenging for current deep models to represent each community of color fairly. Rather than collecting a comprehensive dataset, which is often impractical and time consuming, we propose an alternative solution via simple user interactions, where users can make correction based on their own knowledge. This is accomplished by technique named \textit{SkinFill}. In real-world, makeup artists adjust the skin-tone by blending foundation smoothly over their face. SkinFill is designed to "embed" this wisdom at the delight step. 
%by adding the filled mask with skin texture estimated from a albedo network. 
% We will show that it not only allows bias correction and but also offers extra freedom for skin-tone adjustment at inference time.

%With all the above components integrated, LightPainter enables creative and interactive lighting editing that has been infeasible before. We also demonstrate the simplicity and intuitive workflow of LightPainter through a thorough user study. We summarize our contributions as follows,
Similar to prior work~\cite{pandey2021total,zhang2021neural,sun2019single}, we train our system with a light stage~\cite{debevec2000acquiring} dataset. With our novel designs, LightPainter is a user-friendly system that enables creative and interactive portrait lighting editing. We demonstrate the simple and intuitive workflow of LightPainter through a thorough user study. We show it generates relit portraits with superior photo-realism and higher fidelity compared to state-of-the-art methods.  We summarize our contributions as follows:

% Combining with additional module for normal estimation and foreground matting, the unified system manifests a simpler and more intuitive workflow for users to express their creative minds, allowing them to freely achieve complex, but more creative, artistic and fascinating illumination effects than previous possible. In summary, the main contributions of our paper are:
\begin{itemize}%[noitemsep]
    \item We propose LightPainter, a novel scribble-based portrait relighting system that offers flexible user control, allowing users to easily design portrait lighting effects.
    \item We introduce a novel scribble simulation algorithm that can automatically generate realistic scribbles for training. Combining it with a carefully designed neural relighting module, our system can robustly generalize to real user input.
    \item We introduce SkinFill to allow users to specify skin tone in the relighting pipeline, which allows data-efficient training and offers additional control to address potential skin tone data bias.

    
\end{itemize}


%

%On the other hand, there is no paired data with switched skin-tone to learn from. Retouching skin-tone is therefore only possible in a self-supervised manner and should not harm the original rendering quality.  