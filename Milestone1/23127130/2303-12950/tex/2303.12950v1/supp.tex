
% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\newcommand{\m}[1]{\mathcal{#1}}
\usepackage{xcolor}
\usepackage{enumitem}
\newcommand{\HZ}[1]{\textcolor{cyan}{[HZ: {#1}]}} 
\newcommand{\jz}[1]{\textcolor{cyan}{[JZ: {#1}]}} 
\newcommand{\ZS}[1]{\textcolor{orange}{[ZS: {#1}]}}
\newcommand{\redbf}[1]{{\textbf{\color{red}{#1}}}} % red bold for table entries
\newcommand{\blueud}[1]{{\underline{\color{blue}{#1}}}} % blur underlined for table entries
\newcommand{\redud}[1]{{\underline{\color{red}{#1}}}} %
\makeatletter
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[accsupp]{axessibility} 
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2170} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Intuitive, Interactive Portrait Relighting with LightPainter

% }
\title{Supplementary File:
\\ LightPainter: Interactive Portrait Relighting with Freehand Scribble}

\author{%
  Yiqun~Mei$^{1}$\quad He Zhang$^{2}$\quad Xuaner Zhang$^{2}$\quad Jianming Zhang$^{2}$\quad Zhixin Shu$^{2}$\quad Yilin Wang$^{2}$\quad\\ Zijun Wei$^{2}$\quad Shi Yan$^{2}$\quad HyunJoon Jung$^{2}$\quad Vishal M.~Patel$^{1}$ \\ \\
  {\small$^{1}$Johns Hopkins University\quad \quad $^{2}$Adobe Inc.}
}

\maketitle
\begin{figure}[t]
    \centering
    \includegraphics [width=0.47\textwidth]{figure_supp/lightstage.jpg}
    \vspace{-3mm}
    \caption{An illustration of our light stage system.}
    \vspace{-3mm}
    \label{fig:lightstage}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.47\textwidth]{figure_supp/lightstage.png}
    \vspace{-1mm}
    \caption{The LED light distribution of our light stage system. The image is shown in the panoramic format with $x$-axis denoting the longitude and $y$-axis denoting the latitude.}
    \vspace{-5mm}
    \label{fig:lightstage_light}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[clip, trim=0 13.9cm 15.8cm 0, width=0.5\textwidth]{figure_supp/olat_exp.pdf}
     \vspace{-8mm}
    \caption{Examples of the captured OLAT images from 4 frontal-view cameras}
     \vspace{-5mm}
    \label{fig:olat_exp}
\end{figure}




\begin{figure}[ht]
    \centering
    \includegraphics[clip, trim=0 11cm 7.8cm 0, width=0.48\textwidth]{figure_supp/skin_tone_adjust.pdf}
    \vspace{-7mm}
    \caption{Examples of skin-tone control. Given a source image (left), LightPainter can predict the albedo that respects user-specified skin color (bottom left).}
    \vspace{-3mm}
    \label{fig:skin}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[clip, trim=0 8.9cm 10cm 0cm, width=0.49\textwidth]{figure_supp/limitations.pdf}
    \vspace{-6mm}
    \caption{Limitations of our method. While novice users can easily transfer the lighting patterns from a target image by simple scribbling, the scribble-based interface does not allow professional users to specify the exact shape/boundary of shadows. In LightPainter, these details are handled by the network to tolerate imperfect scribbles from novices. In addition, as shown in the relit image, our network also failed to produce non-Lambertian reflections in the subject's eyes.}
    \label{fig:limitation}
\end{figure}
\section{Interactive Application}
 The interactive application of LightPainter implements drawing tools for users to draw and edit their scribbles with ease. In the supplement, we provide a ~\textbf{recorded video} of using LightPainter to perform in-the-wild portrait relighting to demonstrate our interface and workflow.

\section{Implementation Details}

\begin{figure*}[ht]
    \centering
    \includegraphics[clip, trim=0 40.5cm 26cm 0, width=\textwidth]{figure_supp/supp_figure_wild.pdf}
    \vspace{-10mm}
    \caption{Examples of relighting in-the-wild portraits from user scribbles}
    \vspace{-2mm}
    \label{fig:portrait_visual_draw}
    \end{figure*}
    
    
\begin{figure*}[ht]
    \centering
    \includegraphics[clip, trim=0 5.5cm 6cm 0, width=\textwidth]{figure_supp/supp_more_visual_results_for_re.pdf}
    \vspace{-10mm}
    \caption{More visual results on images with eyeglasses, light hair color and three color lighting.}
    \vspace{-2mm}
    \label{fig:portrait_visual_draw_more}
    \end{figure*}
    
\paragraph{Unconditional Albedo Net.} We also train an unconditional
albedo net using the same architecture, which takes only
the portrait image and its normal as input. If the skin color
is not provided, our backend will automatically switch to
this unconditional network for inference.

\paragraph{Network Architecture.}
In LightPainter, both Albedo Net and Shading Net adopt a U-Shaped structure. Specifically, Albedo Net is a standard UNet~\cite{ronneberger2015u} and Shading Net has one additional non-local layer~\cite{wang2018non} and dilated convolution~\cite{yu2016dilated} in the bottleneck of both the encoder (shading completion branch) and the decoder. For both networks, we use 3 upsampling layers in the encoder and 3 downsampling layers in the decoder, resulting in 64-128-256-512 and 512-256-128-64 hidden channels respectively. All latent features in the bottleneck have 512 channels.  The upsampling operation is implemented using transposed convolution with a kernel size of $6\times6$. The dilated convolutions have a dilation rate of $7\times7$. And the kernel size for standard convolutions is set to $3\times3$. 




    
\begin{figure*}[ht]
    \centering
    \includegraphics[clip, trim=0 2cm 15.5cm 0, width=\textwidth]{figure_supp/main_fig_supp_2.pdf}
    \vspace{-7mm}
    \caption{Visual comparisons on environment-map-based portrait relighting. We compare our method with Total Relighting~\cite{pandey2021total} (TR) and SIPR-W~\cite{wang2020single}.}
    \label{fig:portrait_visual_1}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[clip, trim=0 2cm 15.5cm 0, width=\textwidth]{figure_supp/main_fig_supp_1.pdf}
    \vspace{-7mm}
    \caption{Visual comparisons on environment-map-based portrait relighting. We compare our method with Total Relighting~\cite{pandey2021total} (TR) and SIPR-W~\cite{wang2020single}.}
    \label{fig:portrait_visual_2}
\end{figure*}


\paragraph{More Scribble Simulation Details.} To compute shading maps, we follow Total Relighting~\cite{pandey2021total} and use the prefiltering technique for environment-map-based Phong shading. We pre-compute the diffuse and specular irradiance maps for each environment map by filtering it with cosine lobe functions corresponding to Lambertian and
Phong specular BRDF (where the Phong exponent is set to 32). This bakes the \textit{ambient} light term into the diffuse
term. The \textit{diffuse} and \textit{specular} shading of the subject are obtained by looking up into these irradiance maps according to the normal or reflection directions, respectively. The final shading
is: $0.85 \times \textit{diffuse} + 0.15 \times \textit{specular}$. 


We provide more details about the quantization process in our scribble simulation algorithm. As discussed, we first convert the obtained shading into $Lab$ space and quantize the luminance into multiple bins. We empirically set the number of bins to 25 to ensure the simulated scribbles are sufficiently coarse in terms of intensity so that can mimic causal user input. However, quantization inherently limits the network to be trained on only those 25 predefined bin levels and may result in poor generalization for other scribble intensities. To address this, we propose to shift the bin levels by a random value $p$ at each training step, where $p$ is drawn from $0$ to the bin width. With the random shift, the simulated scribbles can cover the full range of input intensity. 





\paragraph{Light Stage Configurations.}
As discussed, we rely on the light stage~\cite{debevec2000acquiring} to prepare our training data. Specifically, we construct a light stage illumination system with a diameter of 3.6m and 160 LED lights. The system structure and the distribution of lights are illustrated in Figure ~\ref{fig:lightstage} \& \ref{fig:lightstage_light}. We use the MER2-502-79U3M high-speed camera to record the reflectance field of the subject at 5 megapixel resolution and exposure time of 20ms. We provide some examples of the captured OLAT images from 4 frontal camera views in Figure \ref{fig:olat_exp}.
\section{More Study on Skin-tone Control}
As discussed, LightPainter can generate an albedo image that respects the user-specified skin color. We demonstrate its effectiveness in terms of user control in Figure ~\ref{fig:skin}. As one can see, our approach can predict realistic albedo images that match the user's intent.

\section{Limitations}
LightPainter is not without limitations. First, similar to previous relighting methods~\cite{pandey2021total,sun2019single,zhang2021neural, zhang2021neural2}, our method is trained on the data rendered with image-based relighting~\cite{debevec2000acquiring}. This inherently limits the expressiveness of the model to the lighting patterns that can be represented by the environment maps, indicating it cannot handle lighting effects and cast shadows caused by occlusions. Second, similar to Total Relighting~\cite{pandey2021total}, it fails to generate specular reflections in the eyes of the subjects (see Figure~\ref{fig:limitation}). This is because the eye regions only contribute to a small portion of the overall loss function. One possible solution is to add explicit supervision on the eye regions and we leave this extension for future work.  Third, in a few examples, we found that some fine details of the hair region are smoothed out. We suspect it may be due to simply fine-tuning the off-the-shelf normal net is not sufficient to estimate the fine and dense geometry of hair strands, and it is also difficult to learn sophisticated light
transport in hairs without explicit supervision. Finally, as discussed, by design our method is robust to noisy and incomplete scribbles from novices, where the plausible completion of the lighting effects, such as sharpness of highlights and hardness of shadow boundary, are left to the network. Therefore, as shown in Figure~\ref{fig:limitation}, although our method is good at producing realistic light patterns that match the user's desires, the scribble-based interface does not allow professional users to specify the lighting details, for example, the exact shape and boundary of shadows. We believe extending the current scribble-based interface to support more fine-grained control for professional users will be an interesting direction for future exploration.
%%%%%%%%% REFERENCES

\section{More Visual Results}
In Figure ~\ref{fig:portrait_visual_draw} \&~\ref{fig:portrait_visual_draw_more}, we provide more examples of in-the-wild portrait relighting to better demonstrate the effectiveness of LightPainter. In Figure \ref{fig:portrait_visual_1} \& \ref{fig:portrait_visual_2}, we also provided additional qualitative comparisons with state-of-the-art methods~\cite{wang2020single,pandey2021total} on environment-map-based portrait relighting. Note that we did not compare SIPR-W~\cite{wang2020single} on this task in the main paper as it yields relit results with strong artifacts (because SIPR-W~\cite{wang2020single} is only trained for face images). These results are also included here. 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
