\section{Data and Implementation Details}
\paragraph{Data Preparation.} Following prior practice ~\cite{pandey2021total,sun2019single,zhang2021neural}, we use a light stage~\cite{debevec2000acquiring} to collect training and testing OLAT data. Our capture system is structurally similar to that used in~\cite{sun2019single}. Our light stage contains 160 programmable LED-based lights and 4 frontal-viewing high-speed cameras. The detailed configuration can be found in the supplementary material. Our dataset contains 59 subjects. Each subject is photographed with 5-15 different poses and accessories, resulting in 2123 OLAT sequences in total. A subset of 13 subjects with diverse races and different genders are used for testing. The ground truth normal is computed following the algorithm described in~\cite{pandey2021total}. The ground truth diffuse albedo is acquired by capturing the subject in a flat unidirectional lighting condition with all LED lights turned on.

To obtain a paired dataset for supervised training, we render our OLAT images under diverse lighting environments, which are collected from the Laval Indoor \cite{gardner2017learning} and Outdoor HDR datasets and PolyHaven \cite{ployhaven}. We collect in total 2571 real environment maps. We in addition create 2000 synthetic environment maps by placing colored eclipse shapes on a black canvas. We randomly select a subset of 450 environment maps for testing. We also augment lighting by randomly rotating the environment maps when rendering data, resulting in 870K training samples. For the test set, we randomly pair each test OLAT sequence with two lighting environments to form input and output pairs, resulting in 757 testing pairs.
\begin{figure*}[t!]
	\centering
	\includegraphics[clip, trim=0cm 2.5cm 11cm 0cm, width=0.97\textwidth]{figures/visual_user_v3.pdf}
\vskip-8pt	\caption{\textbf{Visual comparisons on user-generated images using three relighting systems.} User-drawn scribbles and environment maps are shown as insets. Lighting effects that are produced by our method are the most faithful and consistent with the target. \textbf{Best viewed by zooming to 4X.}}\label{fig:user}
\vspace{-5mm}
\end{figure*}
\vspace{-4mm}
\paragraph{Implementation and Training Details.} Both Shading Net and Albedo Net have an encoder-decoder structure with three downsampling and upsampling layers. Multiple standard convolutions, dilated convolutions~\cite{yu2016dilated} and non-local attentions~\cite{wang2018non} are inserted at the bottleneck of each network. Further details can be found in the supplementary material. For training, we resize the rendered data to $800\times 600$ resolution and randomly crop $512\times512$ region from 32 images to form a mini-batch. The network is trained using Adam optimizer~\cite{adam}. The learning rate is set to 1e-4 for the first 2 epochs, then reduced to a half after each epoch. We stop the training after 5 epochs. The proposed model is implemented using PyTorch and the training takes about 1 day on 8 Nvidia A100 GPUs.
\section{Experiments}
%In this section, 
\vspace{-1mm}
We now demonstrate the high-quality portrait relighting capability of LightPainter via extensive evaluations, comparisons, and user studies. We also provide ablation studies to demonstrate the benefits of our system design.
%In section 5.1, we show that our sketch-based system is easy to use and improves user experience, with which novice users can produce a desirable lighting effect in minutes; 
%In section 5.2, we show LightPainter can adjustment skin-tone and thus is useful for skin-tone retouching;
%and de-bias; 
%In section 5.3, we compare our method with current state-of-the-arts methods. 
%In section 5.3, we provide quantitative and qualitative comparisons with state-of-the-art relighting methods. 
%Finally, we conduct ablation study to validate our design choices.  
%\ZS{This paragraph is not necessary, can be deleted if we need more space.}


\textbf{Evaluation Metrics.} We report perceptual metrics LPIPS~\cite{zhang2018unreasonable}, NIQE~\cite{zhang2015feature}, and pixel similarity metrics PSNR and SSIM~\cite{wang2004image}. In addition, we use the Deg (cosine similarity between LightCNN~\cite{wu2018light} features) to evaluate identity preservation capability. All results are computed within the subject mask pre-computed using~ \cite{yu2021mask}.

%\subsection{Does LightPainter Improve User Workflow?} 
\subsection{User Study}
%Given the scribble-based lighting representation, we argue that LightPainter is more simple and intuitive for novices to operate. 
%We conduct a user study to demonstrate LightPainter enables easy pursuit of desired portrait lighting effects within minutes of use. 
To demonstrate LightPainter can
benefit general users on portrait lighting editing, we perform a user study to evaluate the quality and user experience of LightPainter.
%We directly compare our system with ClipDrop~\cite{ClipDrop}, one of the state-of-the-art lighting editing applications supporting virtual light placement. We also compare with conventional environment-map-based system (denoted as ``Env''), where we re-implement Total Relighting~\cite{pandey2021total} as the backend and ask users to edit lighting by drawing a environment map. 
We recruit general users to conduct portrait relighting with three systems that use different interactive approaches:
\begin{itemize}[noitemsep]
\vspace{-1.5mm}
    \item ClipDrop~\cite{ClipDrop}\footnote{www.clipdrop.co/relight}, a commercial lighting editing web service, where users can place virtual lights into the scene with a chosen light color, intensity, distance and radius.
    \item Env: a re-implementation of Total Relighting~\cite{pandey2021total}, the state-of-the-art portrait relighting method. With this tool, the user provides a hand-drawn ``environment map'' to perform lighting editing.
    \item LightPainter, our scribble-based system.
\vspace{-1mm}
\end{itemize}

\begin{figure}[t]
\begin{center}
\includegraphics[clip, trim=0cm 10cm 19.5cm 0cm, width=0.48\textwidth]{figures/skin_fill.pdf}
\end{center}
\vspace{-14pt}
\caption{\textbf{Examples of how user leverage SkinFill on skin color retouching.} ``Auto'' denotes albedo generated by the automatic prediction mode of the proposed LightPainter. The selected skin color swatch is appended at the end of each row. Both TR \cite{pandey2021total} and ``Auto'' suffer from skin-tone bias. In contrast, with SkinFill, LightPainter can predict the correct skin color that follows the user's desire.}
\label{fig:skin}
\vspace{-5mm}
\end{figure}
\input{tables/user_quant}

%We instruct the users to use the three systems to reproduce the lighting effects in a target image by operating on a source image of the same view. Each task is limited to two minutes and the order each system presented to the user is randomized. Since ClipDrop cannot remove the existing lighting of the photo, we instead use the albedo (drawn from our test set) as the source image for fair comparisons. The target image are rendered with an environment map sampled from our test sets. We provide minimal training for 20 subjects, briefly walking them through each interface for 2 minutes. We ask each participant to use 3 systems to sequentially complete 2 tasks, resulting in $3\times 2$ user-generated relit images per-person.

For this experiment, we provide a \textit{target image} with a random lighting effect and assign a task to the users to reproduce this lighting effect on an input \textit{source image} with each tool individually. The source image and target image are from our testing set and only differ in lighting. We impose a time limit of 2 minutes per task for each tool.
Note that ClipDrop does not remove existing lighting effects on source images, we use the albedo image (from our test set) as the source image for fair comparisons. 

%In Figure~\ref{fig:user}, we show a random subset of visual results from the user study. Using Env or ClipDrop, users often fail to reproduce the lighting effects in the target image. Specifically, results from usiNG Env exhibit significant mismatch in both brightness and lighting patterns. This can be explained by inspecting the drawn environment maps, which makes it difficult to find the correct lighting position and intensity. Therefore, many results contain artifacts and lighting patterns do not match. While ClipDrop improves the relit photos, the detailed lighting patterns such as the shadow and highlight, are still different from the target images.
% which implies its difficulty in achieving the precise control. 


In Figure~\ref{fig:user}, we show a random subset of visual results from the user study. Using Env or ClipDrop~\cite{ClipDrop}, users are often unable to faithfully reproduce the target lighting effects. We notice that results from using Env exhibit a significant mismatch in both brightness and shading patterns. This shows the difficulty for users to interpret the lighting position and intensity, and associate lighting effects on an image with environment map representations. While ClipDrop~\cite{ClipDrop} is easier for users to interact with, the desired detailed lighting patterns in the target image, such as the highlight, are still largely absent in the results. 
%In contrast, userwith just two minutes of use, novices can produce the most accurate lighting effects with LightPainter.
In contrast, LightPainter allows users to faithfully reproduced the target lighting effect, with convincing details, within two minutes.

We also provide quantitative evaluations (computed between user-generated results and target images) on each method, as shown in Table~\ref{tab:user}. The quantitative results are collected on 40 trials performed by 20 users.
%LightPainter achieves the best performance on all metrics, which is consistent with the visual comparison. 
LightPainter achieves the best performance on all metrics, which not only produces the target lighting most closely, but also shows the best image quality and identity preservation capability.
%\ZS{How many images are used for quantitative evaluation?}
% These results indicate LightPainter is intuitive and easy to use, where novices can pursue the very convincing portrait lighting effects with just two minutes of use.

\begin{figure}[t]
\begin{center}
\includegraphics[clip, trim=0cm 1.5cm 19.5cm 0.3cm, width=0.49\textwidth]{figures/bg_figure.pdf}
\end{center}
\vspace{-7mm}
\caption{\textbf{Qualitative comparisons on environment-map-based portrait relighting.} Our method can produce on-par or better results comparing with TR~\cite{pandey2021total}. Inputs and targets (ground truth) are generated using the test set of light stage subjects and environment maps not seen during training. 
}
\vspace{-5mm}
\label{fig: pr}
\end{figure}

%In total, 20 users participated in our user study. 
We gathered feedback from the 20 participated users on their experience with each relighting system. We summarize the findings as follows: 1. All users are satisfied with our scribble-based relighting scheme, and feel that drawing over the portrait is very easy to operate. 2. Most users (17/20) feel that editing/drawing environment map is confusing. 3. More than half of users (12/20) feel tuning virtual lights (i.e. guessing the color, intensity and distance of each light associated with the scene) is very difficult and it is hard to obtain the desirable lighting effect. %These user feedback are consistent with the quantitative and qualitative results.

\begin{figure}[h]
\begin{center}
\includegraphics[clip, trim=0.1cm 23.5cm 16cm 0.0cm, width=0.49\textwidth]{figures/ffhq_update.pdf}

\end{center}
\vspace{-6mm}
\caption{\textbf{Qualitative comparisons on in-the-wild face relighting.} We compare relighting results with SIPR-W \cite{wang2020single} (row-2) and TR~\cite{pandey2021total} (row-3). The environment maps are shown as insets (row-1). We provide a reference image (row-5) rendered with OLAT data as guidance of the lighting effect under the input environment.%Since there is no ground truth as how the subject looks under the given lighting condition, we render a pseudo-ground-truth using our test subject, where we have the OLAT capture, under the given environment map.
}
\vspace{-6mm}
\label{fig: ffhq}
\end{figure}

%\subsection{Can LightPainter Modify Skin-tone?}
\subsection{Skin-tone Control}
With SkinFill, LightPainter can predict the albedo that respects the user-specified skin color. This grants the user with the flexibility of tuning skin tone.
% adjusting foundation for makeup. 
We demonstrate its use case in Figure~\ref{fig:skin}. This also helps resolve the potential data bias and ambiguity in predicting an accurate skin tone from a single image. As already shown in machine learning based approaches, the skin color of the predicted albedo from both total relighting~\cite{pandey2021total} and the automatic mode of LightPainter is inaccurate and appears washed-out. In contrast, with additional user interference with SkinFill, LightPainter can produce a more faithful albedo that respects the user's intent. More experiments can be found in the supplement.





\subsection{Comparisons with State-of-the-art Methods}
To further demonstrate the relighting quality of LightPainter, we compare it with the state-of-the-art methods SIPR-W~\cite{wang2020single} and Total Relighting~\cite{pandey2021total} (TR) on environment-map-based relighting. Since LightPainter is not designed for using environment map as its lighting representation, we perform relighting with ``estimated" scribbles instead of user scribbles.
Specifically, we render the shading map using the Phong model~\cite{phong1975illumination} with the estimated normal map to replace the user scribbles. 
%Note that LightPainter is not trained with these "ideal" inputs. 
These estimated scribbles are in fact ``ideal'' inputs for LightPainter, with more complete shading information than either the simulated scribbles we used for training or the real user scribbles.
We adopt the off-the-shelf image matting method~\cite{yu2021mask} to extract the foreground portrait and composite it with the new background.
We report (1) quantitative and qualitative results on portrait/upper-body relighting on our testing data, and (2) qualitative comparisons on in-the-wild images from FFHQ~\cite{karras2019style}. 
%Since neither of SIPR-W~\cite{wang2020single} and TR~\cite{pandey2021total} released their code, the results are obtained by asking the authors to run their models on our test data.
Results of SIPR-W~\cite{wang2020single} and TR~\cite{pandey2021total} are directly obtained from their authors.


\vspace{-4mm}
\paragraph{Evaluation with Light Stage Data.} For this experiment, 757 ground truth images are rendered with environment maps and OLAT data from the test set using Image-base Relighting~\cite{debevec2000acquiring}. We report the quantitative results in Table~\ref{tab:compare} and provide comparisons. Our approach performs better in perceptual quality, identity preservation, and image similarity.
%and visual results in Figure~\ref{fig: pr}. 
%Because SIPR-W is only trained on face crops, the produced relit portraits exhibit apparent artifacts (see supplemental). 
In Figure~\ref{fig: pr}, we show qualitative comparison with~\cite{pandey2021total}. Our method produces photo-realistic results and respects the target lighting more faithfully.
%it can be seen that our method can achieve on-par on better compared TR and are consistent with the target lighting pattern. The superiority results of LightPainter is further verified with quantitative results.
\vspace{-4mm}
\paragraph{Evaluation with In-the-wild Images.} 
%Our method can generalize to in-the-wild images. To show this, we compare the performance on real face images from
We demonstrate in-the-wild portrait relighting capability with images from FFHQ~\cite{shih2014style} dataset.
We show qualitative results in Figure~\ref{fig: ffhq}. Since there is no relighting ground truth, we provide a reference image by rendering a face under the target lighting environment using OLAT and Image-base Relighting~\cite{debevec2000acquiring}.
%It can be see that LightPainter yields realistic relit images with convincing lighting patterns. Our method achieves comparable with TR~\cite{pandey2021total} that explicitly designed for environment-map-based relighting.
LightPainter generates high-quality relighting results with convincing lighting effects. Our results are more consistent with lighting effects in the reference images compared to TR~\cite{pandey2021total}. Compare to~\cite{wang2020single}, our results are more robust and exhibit no noticeable artifacts.

\input{tables/compare}
\input{tables/ab_arch}

\subsection{Ablation Study}
%In this section, we conduct controlled experiments to 
We now provide ablation studies to demonstrate the benefit of our key designs in LightPainter. All results are evaluated on our test set using synthetic scribbles, ground-truth albedo and normal with a fixed sampling ratio of $0.3$.
\vspace{-4mm}
\paragraph{Effectiveness of Two-step Relighting Scheme.}
Our system adopts a two-step relighting scheme to enforce geometry-consistent shading. We investigate its effectiveness by comparing it with a single-step baseline, which directly predicts the relit images from the scribbles without shading completion. The results for one-step baseline are reported in the first row of Table \ref{tab:arch}. As shown, two-step approach (i.e. LightPainter) significantly improves performance.
\vspace{-4mm}
\paragraph{Augmented Receptive Field.} %To handle sparse user input, the shading net enlarges its receptive field by adopting non-local blocks and dilated convolutions. Here we conduct experiments to validate its effectiveness. As shown in Table~\ref{tab:arch}, removing either non-local attention or dilate convolution harms performance, demonstrating larger receptive fields is indeed necessary for our system.
To handle sparse user input, we adopt non-local blocks~\cite{wang2018non} and dilated convolutions~\cite{yu2016dilated} to enlarge the receptive field of the Shading Net. We conduct experiments to validate this design choice. As shown in Table~\ref{tab:arch}, removing either non-local attention or dilate convolution harms performance.



