%File: formatting-instructions-latex-2023.tex
%release 2023.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage{bbding}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{natbib}
\setcitestyle{authoryear,round}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color}
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{GAM : Gradient Attention Module of Optimization for Point Clouds Analysis}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Haotian Hu\textsuperscript{\rm 1}, Fanyi Wang\textsuperscript{\rm 2}\thanks{Corresponding author}, Jingwen Su\textsuperscript{\rm 2}, Hongtao Zhou\textsuperscript{\rm 1}, Yaonong Wang\textsuperscript{\rm 1}, Laifeng Hu\textsuperscript{\rm 1}, Yanhao Zhang\textsuperscript{\rm 2}, Zhiwang Zhang\textsuperscript{\rm 3}\thanks{Corresponding author}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1} Zhejiang Leapmotor Technology CO., LTD.\\
    \textsuperscript{\rm 2} OPPO Research Institute\\
    \textsuperscript{\rm 3} The University of Sydney\\
    
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar, \textsuperscript{\rm 2}
    % J. Scott Penberthy, \textsuperscript{\rm 3}
    % George Ferguson,\textsuperscript{\rm 4}
    % Hans Guesgen, \textsuperscript{\rm 5}.
    % Note that the comma should be placed BEFORE the superscript for optimum readability
    \{hu\_haotian, zhou\_hongtao, wang\_yaohong, hu\_laifeng\}@leapmotor.com, \{wangfanyi,sujingwen,zhangyanhao\}@oppo.com,\\  zhiwang.zhang@sydney.edu.au 
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1,\rm 2}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
In point cloud analysis tasks, the existing local feature aggregation descriptors (LFAD) are unable to fully utilize information in the neighborhood of central points.  Previous methods rely solely on Euclidean distance to constrain the local aggregation process, which can be easily affected by abnormal points and cannot adequately fit with the original geometry of the point cloud.  We believe that fine-grained geometric information (FGGI) is significant for the aggregation of local features.  Therefore, we propose a gradient-based local attention module, termed as Gradient Attention Module (GAM), to address the aforementioned problem.  Our proposed GAM simplifies the process that extracts gradient information in the neighborhood and uses the Zenith Angle matrix and Azimuth Angle matrix as explicit representation, which accelerates the module by 35X.  Comprehensive experiments were conducted on five benchmark datasets to demonstrate the effectiveness and generalization capability of the proposed GAM for 3D point cloud analysis.  Especially on S3DIS dataset~\citep{armeni20163d}, GAM achieves the best performance among current point-based models with mIoU/OA/mAcc of 74.4\%/90.6\%/83.2\%, respectively. Code to reproduce our results is available at \url{https://github.com/hht1996ok/GAM }.

\end{abstract}

\section{Introduction}
In recent years, point cloud analysis has become a hot topic in academia and industry due to the rapid development of autonomous driving and indoor robotics. Considering that point cloud is unordered, sparse, and irregular, traditional methods for 2D image processing cannot be directly applied to point clouds. PointNet~\citep{qi2017pointnet} is a pioneering work that uses Multi-Layer Perceptron (MLP) to learn point features independently. Qi et al. proposed PointNet++~\citep{qi2017pointnet++}, which introduces local features to point cloud analysis models for further performance improvement of point cloud analysis models. Recent works present some promising results, using convolutional layers \citep{boulch2020convpoint, thomas2019kpconv, xiang2021walk}, graph structures~\citep{wang2019dynamic, xu2020grid}, MLP~\citep{ma2022rethinking}, or attention mechanisms~\citep{guo2021pct, zhao2021point} for point cloud analysis. Among them, local feature aggregation descriptors (LFAD) play an important role. 

However, existing LFAD cannot effectively distinguish points in a point cloud neighborhood, and thus are unable to learn finer semantic information of the point cloud. This observation motivates us to consider the attention mechanism within a point cloud neighborhood. The previous works treat all points in the neighborhood as equally important~\citep{wang2019dynamic}, or only use distance information to constrain aggregation process~\citep{lan2019modeling, thomas2019kpconv, ma2022rethinking}, ignoring deeper geometric relationships within the neighborhood. These operations include too much outlier information in the local feature aggregation process and impede the model to conform the original geometry of the point cloud. Therefore, we propose a novel gradient attention module (GAM) that utilizes neighboring gradient information to better constrain the aggregation process of the neighborhood features. As shown in Figure~\ref{fig: fig1}, with gradient information, our proposed method is enabled to predict clearer object boundaries.

In addition, we find that the gradient calculation method~\citep{pauly2003point} based on the local surface fitting method is very slow, and hinders real-time inference after adding gradient information. To solve this problem, we propose to simplify the calculation of gradient information in the neighborhood, by converting gradient information to an explicit representation of the Zenith Angle and Azimuth Angle between the center point and its neighboring points. Our proposed method can accelerate computation speed by 35 times. As shown in Figure~\ref{fig: fig2}, GAM is a plug-and-play module, which effectively improves the performance of baseline methods while maintaining a similar inference speed.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure1.pdf}
    \caption{S3DIS benchmark visualization results, from left to right are ground truth, PointNet++~\citep{qi2017pointnet++} and the results after adding the gradient attention module (GAM).}
    \label{fig: fig1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure2.pdf}
    \caption{Overall Accuracy (OA) and throughput comparison plots of 3D shape classification experiment results on  PointMLP~\citep{ma2022rethinking}, PointMLP-Elite~\citep{ma2022rethinking}, PointTNT~\citep{berg2022points} and RepSurf 2x~\citep{ran2022surface} models before and after adding GAM in ScanObjectNN~\citep{uy2019revisiting}.}
    \label{fig: fig2}
\end{figure}

%GAM结构示意图。其中N表示点的个数，${{N}_{s}}$为中心点个数，$K$表示每个中心点的邻域点个数，$p$代表中心点，$q$代表$p$的某一邻域点，LFE表示不同开源模型的局部特征提取器(Local feature extractor)。该模块的输入（绿色框）是点云原始位置信息($N\times 3$)和特征($N\times C$)。步骤(a)搜索了点云的中心点和其对应邻域点；步骤（b）建立了一个有向图，得到中心点和其邻域点之间的相对位置向量，随后GAM计算了邻域内中心点和各邻域点之间的天顶角和方位角，并利用它们构建了邻域内的梯度注意力矩阵。
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure3.pdf}
    \caption{Schematic diagram of GAM structure, where $p$ represents one center point, $q$ represents a particular neighbor point of $p$, and LFE denotes the local feature extractor of different baselines. Module inputs are original positional information of the point cloud and features. Step (a) searches the center points of the point cloud and their corresponding neighborhood points; step (b) builds a directed graph to obtain the relative position vector between the center point and its neighborhood points. Then GAM calculates the zenith and azimuth angles in the neighborhood and uses them to construct a gradient attention matrix.}
    \label{fig: fig3}
\end{figure*}

%GAM非常容易移植，仅需几行代码就可以将其添加到点云分析模型中。本文在S3DIS(Armeni et al. 2016) 3D语义分割、ScanObjectNN(Uy et al. 2019) 3D形状分类、ShapeNet (Yi et al. 2016) 3D部件分割(3D part segmentation)、ModelNet40 (Wu et al. 2015) 3D形状分类和KITTI (Geiger, A.; Lenz, P. and Urtasun, R. 2012) 3D目标检测实验中验证了各开源模型插入GAM前后的精度，并报告了部分基准中加入GAM前后模型速度和参数量的变化情况。值得一提的是，GAM在ScanObjectNN、ShapeNet、S3DIS基准中分别提高了包括SOTA 模型在内的baseline的性能。实验结果证明了GAM是一个很有前途的模块，其可以有效的应用于多种点云任务，且对不同的模型都有着良好的性能提升。本文的贡献总结如下: 
Our proposed GAM is portable and can be added to previous state-of-the-art methods with a few lines of code. We conduct experiments on 3D semantic segmentation task using S3DIS dataset~\citep{armeni20163d}, 3D shape classification task using ScanObjectNN dataset~\citep{uy2019revisiting} and ModelNet40 dataset~\citep{wu20153d}, 3D part segmentation task using ShapeNet~\citep{yi2016scalable}, and 3D object detection task on KITTI dataset~\citep{geiger2013vision}. Experiment results demonstrate that GAM is an effective module and applicable to a wide range of point cloud analysis tasks with good performance improvements for various models. 

The contributions of this paper are summarised as follows: 


%1.	建立了一种轻量高效的梯度注意力卷积模块GAM，并首次将梯度信息引入点云邻域特征聚合描述符（vector of locally aggregated descriptors）中。
%2.	通过数学表示建立了点云邻域内梯度信息与天顶角、方位角之间的关系，从而将梯度计算过程简化为了计算邻域点的天顶角和方位角，有效提高了GAM的运行速度。 
%3.	在多个大规模数据集上的实验表明，GAM在增加有限的额外显存消耗和推理时间的情况下能够有效提高各开源模型性能，且可以在运用在多种点云任务中。

\begin{itemize}
\item We propose a lightweight and efficient gradient attentive module (GAM). To the best of our knowledge, gradient information is the first time to be introduced into the vector of locally aggregated descriptors of point cloud neighborhood features.

\item The relationship between gradient information and zenith angle and azimuth angle in the point cloud neighborhood is established through mathematical representation. The gradient calculation process is simplified to the calculation of the zenith angle and azimuth angle of neighborhood points. Thus, the computation speed of GAM is effectively improved.

\item Comprehensive experiments on five benchmarks demonstrate that our proposed gradient attention module can effectively boost performances of state-of-the-art methods within limited additional memory consumption and inference time. In addition, our proposed GAM can be used in various 3D tasks such as 3D semantic segmentation, 3D shape classification, 3d object detection, and 3D part segmentation.


\end{itemize}

%\section{Copyright}
%All papers submitted for publication by AAAI Press must be accompanied by a valid signed copyright form. They must also contain the AAAI copyright notice at the bottom of the first page of the paper. There are no exceptions to these requirements. If you fail to provide us with a signed copyright form or disable the copyright notice, we will be unable to publish your paper. There are \textbf{no exceptions} to this policy. You will find a PDF version of the AAAI copyright form in the AAAI AuthorKit. Please see the specific instructions for your conference for submission details.

\section{Related Work}\label{Sec:Literature}

%--------------------------------------------------------------
%点云分析任务中首先要学习每个点的嵌入，随后使用局部聚合方法从整个点云中提取出全局嵌入，最后将全局嵌入（global embedding）输入到各任务的分支中。由于点云的无序性，一部分研究人员尝试将点云投影到规则的体素（voxel）中(Zhou, Y. and Tuzel, O. 2018; Wu et al. 2015; Yan, Y.; Mao, Y. and Li, B. 2018)或者多视图中(Su et al. 2015; Wei, X.; Yu R. and Sun, J. 2020; Liang et al. 2018)，这些方法虽然大大提高了点云模型的运算速度，但是在投影过程中造成的信息损失会严重影响模型的精度。而基于点的方法则直接使用点云原始信息作为输入，并利用各种精心设计的局部特征聚合描述符(Thomas et al. 2019; Lan et al. 2019; Yang et al. 2019; Komarichev, A.; Zhong, Z. and Hua, J. 2019)来聚合局部特征。同时，一些模型利用注意力机制更好的提取了点云特征。
The point cloud analysis task starts with learning the embedding of each point, then extracts global embedding from the whole point cloud using local aggregation methods, and finally feeds global embedding to branches of each task. Due to the disorderly nature of point clouds, some previous works attempt to project point clouds into regular voxels~\citep{zhou2018voxelnet, wu20153d, yan2018second} or multiple views~\citep{su2015multi, wei2020view, liang2018deep}. These methods significantly improve the computational speed, but lose information during the projection process and undermine model accuracy severely. In contrast, point-based methods directly use original point cloud information as input and employ various well-designed local feature aggregation descriptors~\citep{thomas2019kpconv, lan2019modeling, yang2019modeling, komarichev2019cnn}. Meanwhile, some previous works~\citep{wang2019graph, chen2021GAPointNet, wang2021PointAttN, cui2021Geometric, wu2022casa} use the attention mechanism to extract the feature of the point cloud.


%2.1	 基于体素和多视图的方法
\subsection{Multi-view and Voxel-based Approaches}
%早期的工作将非结构化的点云投影到多个二维视图中，利用二维卷积提取不同的视图中的特征，并使用精心设计的方法来有效融合多个视图的特征。MVCNN(Su et al. 2015)是一项开创性的工作，它利用最大池化层将多视图信息聚合为全局特征。但最大池化层只保留了最大的元素，这将不可避免地造成信息的丢失。针对这个问题，Wei等人(Wei, X.; Yu R. and Sun, J. 2020)提出了使用有向图来寻找各个视图的关系的View-GCN，其将每个视图视为一个图节点，并对所有级别的图节点进行最大池化，获得全局形状描述符。
%基于体素的方法将点云划分为统一的三维空间网格，然后利用三维卷积神经网络进行特征提取。Maturana等人(Zhou, Y. and Tuzel, O. 2018)引入了VoxelNet来实现鲁棒的3D目标检测。Wu等人提出了3D shapeNets(Wu et al. 2015)，它使用DBN卷积网络来学习不同三维形状点的分布。虽然这些方法已经取得了不错的效果，但是随着点云分辨率的提高，运行时间和内存的消耗将会激增。为了解决这一问题， SECOND(Yan et al. 2018)提出了3D稀疏卷积，其有效的减少了内存损耗和计算消耗，但大部分设备依然难以负担如此庞大的计算量。
Early works~\citep{su2015multi,wei2020view} project unstructured point clouds into multiple 2D views, extract features from different views using 2D convolution, and then use sophisticated methods to fuse features from multiple views. MVCNN~\citep{su2015multi} is a pioneering work that uses a maximum pooling layer to aggregate multi-view information into global features. But the maximum pooling layer retains only the largest elements, which inevitably leads to information loss. To address this problem, Wei et al. proposed View-GCN~\citep{wei2020view} using directed graphs to find the relationship between individual views. Each individual view is regarded as a graph node, and the global shape descriptor is obtained by max pooling graph nodes of all levels.

Voxel-based approaches divide the point cloud into a uniform 3D spatial grid and use 3D convolutional neural networks for feature extraction. Zhou et al. introduced VoxelNet~\citep {zhou2018voxelnet} for robust 3D target detection. Although this method has achieved high detection performance, computation time and memory consumption sharply increase as the point cloud resolution is enhanced. To solve this problem, SECOND~\citep{yan2018second} proposed 3D sparse convolution that effectively reduces memory and computation costs, but most devices still have difficulty affording such a large amount of computation.

%2.2	基于点的方法
\subsection{Point-based Approaches}
%PointNet的提出为点云研究人员铺平了道路，其利用MLP和max-pooling提取并聚合全局特征。PointNet++将局部特征的概念引入到三维点云分析模型中，其利用Farthest point sampling (FPS)和球查询(ball query)对点云进行中心点采样和邻域点搜索，得到了不同层次的local-globala feature。后续的三维点云分析工作主要集中在点云局部特征聚合描述符的研究上。DGCNN(Wang et al.bb 2019)在中心点和相邻点之间建立有向图，利用EdgeConv提取各边的特征，最后通过最大池化层聚合局部特征。与此同时，出现了大量通过精心设计卷积核来模拟卷积运算以提取邻域信息的模型。Geo-CNN(Lan et al. 2019)中每个方向的边缘特征（edge feature）分别由一个方向相关的可学习矩阵加权，然后根据相对向量与三个坐标轴之间的角度聚合局部特征。A-CNN(Komarichev, A.; Zhong, Z. and Hua, J. 2019)提出了一种环形卷积方法来学习相邻点之间的关系，改进了PointNet++中Multi-Scale Grouping(MSG)模块存在的局部区域重叠问题。本文构建了梯度注意力模块GAM，由于近些年来愈来愈复杂的局部特征聚合描述符导致点云分析模型的效率不高，因此本文更希望寻找一种简单且高效的方法来约束聚合过程。
PointNet~\citep{qi2017pointnet} has paved the way for relevant research studies on the point cloud, using MLP and max-pooling to extract and aggregate global features. PointNet++~\citep{qi2017pointnet++} introduces the concept of local features into 3D point cloud analysis. It uses Farthest Point Sampling (FPS) and ball query to perform centroid sampling, and neighborhood point search on point clouds to obtain various levels of local-global features. Subsequent work on 3D point cloud analysis focuses on the study of point cloud local feature aggregation descriptors. DGCNN~\citep{wang2019dynamic} builds a directed graph between centroids and neighboring points, extracts features of each edge using EdgeConv, and finally aggregates local features through max-pooling layer. In Geo-CNN~\citep{lan2019modeling}, the edge feature of each direction is weighted by a learnable matrix that relates to the direction. Then local features are aggregated according to the angle between the relative vector and three axes. 

Different from the above methods, the proposed GAM utilizes fine-grained geometric information to aggregate finer local features, which helps to improve the accuracy of subsequent tasks. Besides, compared to sophisticated local feature aggregation descriptors that lead to inefficiency in point cloud analysis models, the proposed GAM does not burden the computing device.

\section{Proposed Method}
%要准确地提取复杂点云中的全局嵌入，不仅要考虑每个点的信息，还应该考虑各点与其相邻点集之间的关系。受FR3DNet(Gilani, S.Z. and Mian, A. 2018)的启发，本文提出了梯度注意力模块GAM。该模块利用中心点和其邻域点之间的梯度信息和距离信息来生成各邻域点相应的重要性权重，使得中心点与其各邻域点建立更深且更细粒度的几何联系。3.1节阐述了GAM的整体结构，3.2节中详细解释了点云邻域的梯度信息与天顶角、方位角之间的数学关系。
%To accurately extract the global embedding in complex point clouds, not only the information of each point should be considered, but also the relationship between each point and its neighboring point set. Inspired by FR3DNet (Gilani, S.Z. and Mian, A. 2018),  
The proposed gradient attention module (GAM) uses both gradient information and distance information between the center point and its neighboring points to generate corresponding importance weights of each neighboring point. The mathematical representation of gradient information of point cloud neighborhoods is given in Section 3.1, followed by the overall structure of GAM in Section 3.2. 

\subsection{Mathematical Representation of Point Cloud Neighborhood Gradients}\label{sec:math}
%在三维空间中，三元函数的梯度和三维等值面的法向量虽然有着不同的几何含义，但它们在本质上是相同的。为了方便理解，本小节仅以某一中心点和其邻域内某一点的梯度求解为例进行介绍。在中心点集合$\{{{p}_{s}}|s=1,...,{{N}_{s}}\}\in {{R}^{{{N}_{s}}\times 3}}$中取一中心点$p$，其笛卡尔坐标记为$({{x}_{i}},{{y}_{i}},{{z}_{i}})$，在点$p$的邻域内取一邻域点$q$，其笛卡尔坐标记为$({{x}_{j}},{{y}_{j}},{{z}_{j}})$。在点云的深度图(range image)中，$p$、$q$两点可以表示为离散点$f({{u}_{j}},{{v}_{j}})={{z}_{j}}$和$f({{u}_{i}},{{v}_{i}})={{z}_{i}}$，转换公式如下：
%In 3D space, the gradient of a \textcolor{red}{ternary function} and the \textcolor{red}{normal vector} of a 3D equivalent surface have different geometric meanings, but they are essentially the same. In this subsection, we takes the gradient solution of a certain centroid and a point in its neighborhood as an example. 
We use a center point and a point in its neighborhood as an example to illustrate the mathematical relation between gradient information, zenith angle, and azimuth angle. Given a set of N cloud points $\{{{p}_{i}}\} \in \mathbb{R}^{{N}\times 3}$,  where $i=1,2,... ,{N}$. A central point $p_i$ has the coordinate of $({{x}_{i}},{{y}_{i}},{{z}_{i}})$, and another point ${q}_j$ is the neighborhood point of ${p}_i$, whose coordinate is $({{x}_{j}},{{y}_{j}},{{z}_{j}})$. In the range image of the point cloud, two points $p_i$, $q_j$ can be represented as discrete points $f({{u}_{i}},{{v}_{i}})={{z}_{i}}$ and $f({{u}_{j}},{{v}_{j}})={{z}_{j}}$, and the conversion equation is written as follows.

%Given a set of N cloud points $\{{{p}_{i}}\} \in {{R}^{{{N}\times 3}}$,  where $i=1,2,... ,{N}$.  $\{{{p}_{s}}\} \in {{R}^{{{N}_{s}}\times 3}}$ is the set of ${{N}_{s}}$ selected central points of $\{{{p}_{i}}\}$.

\begin{equation}\label{eq:range}
\left\{\begin{aligned}
  &{{u}_{j}} =\frac{l}{d}{{x}_{j}}+{{u}_{0}}\\
  &{{v}_{j}} =\frac{l}{d}{{y}_{j}}+{{v}_{0}}
\end{aligned}\right.
\end{equation}

%$d$为当前像素值，$f$为相机焦距，${{u}_{0}}$、${{v}_{0}}$分别为深度图中心点的$X$、$Y$轴坐标。传统方法大多利用深度图（range image）中的当前像素与其X、Y轴方向相邻像素的像素值之差来表示该点的深度梯度，然而本文更加关注中心点$p$和邻域点$q$之间的关联，因此在深度图中将$p$点看作是$q$的相邻像素。我们计算这两点在边缘向量$\vec{b}=({{u}_{j}}-{{u}_{i}},{{v}_{j}}-{{v}_{i}})$方向上的梯度来表示$q$点的深度梯度$\nabla {{d}_{b}}$，同时计算得到其在$X$、$Y$轴的深度梯度分量$\nabla {{d}_{x}}$、$\nabla {{d}_{y}}$。
where $d$ is the depth of current point, $l$ is the camera focal length, ${{u}_{0}}$ and ${{v}_{0}}$ are the $X$, $Y$ coordinates of center point of the range image respectively.

Traditional method uses the difference of pixel value in depth map between current pixel and its adjacent pixels in the X and Y axis directions to represent depth gradient of the point. However, we focus on the association between the center point $p_i$ and one of its neighboring points $q_j$ in 3D space. Hence the point $q_j$ is regarded as a neighboring pixel of $p_i$ in the range image. We calculate the pixel value difference of these two points in the direction of an edge vector $\bm{\vec{b}}=({{u}_{j}}-{{u}_{i}},{{v}_{j}}-{{v}_{i}})$ to represent depth gradient of the point. The depth gradient $\nabla {{d}_{b}}$ is defined as $\nabla {{d}_{b}}=\frac{{{z}_{ji}}}{\sqrt{u_{ji}^{2}+v_{ji}^{2}}} $. And depth gradient along $X$, $Y$ axes $\nabla {{ d}_{x}}$, $\nabla {{d}_{y}}$ are defined as follows,

\begin{equation} \label{eq:gradient}
\left\{ \begin{aligned}
    \nabla {{d}_{x}} &=\frac{{{z}_{ji}}}{\sqrt{u_{ji}^{2}+v_{ji}^{2}}}*\frac{{{u}_{ji}}}{\sqrt{u_{ji}^{2}+v_{ji}^{2}}} \\ 
    \nabla {{d}_{y}}&=\frac{{{z}_{ji}}}{\sqrt{u_{ji}^{2}+v_{ji}^{2}}}*\frac{{{v}_{ji}}}{\sqrt{u_{ji}^{2}+v_{ji}^{2}}}. \\ 
\end{aligned} \right.
\end{equation}
where ${{u}_{ij}}$, ${{v}_{ij}}$, ${{z}_{ij}}$ denote ${{u}_{j}}-{{u}_{i}}$, ${{v}_{j}}-{{v}_{i}}$, ${{z}_{j}}-{{z}_{i}}$, respectively. Combining Eq.~\ref{eq:range} and Eq.~\ref{eq:gradient}, components of depth gradient along  $X$ and $Y$ axis are defined as$\nabla {{ d}_{x}}$, $\nabla {{d}_{y}}$.

\begin{equation}
\left\{ \begin{aligned}
 &\nabla {{d}_{x}}=\frac{d}{f}\frac{{{z}_{ji}}{{x}_{ji}}}{x_{ji}^{2}+y_{ji}^{2}}, \\ 
&\nabla {{d}_{y}}=\frac{d}{f}\frac{{{z}_{ji}}{{y}_{ji}}}{x_{ji}^{2}+y_{ji}^{2}}. \\    
\end{aligned} \right.  
\end{equation}


%由于本文探究的是三维空间中的点云几何结构，故需将$q$点深度梯度分量转换到世界坐标系中，得到点云$Z$轴关于$X$、$Y$轴的梯度$\nabla {{z}_{x}}$、$\nabla {{z}_{y}}$以及$q$点的近似法向量$\vec{n}$。
Since we explore the geometric structure of point clouds in 3D space, it is necessary to convert  depth gradient to the world coordinate system, in order to obtain gradients $\nabla  {{z}_{x}}$ and $\nabla {{z}_{y}}$ respectively. 

\begin{equation}
\left\{
\begin{aligned}{}
& \nabla {{z}_{x}}=\frac{d}{f}\nabla {{d}_{x}}=\frac{{{x}_{ji}}{{z}_{ji}}}{x_{ji}^{2}+y_{ji}^{2}},&\\
& \nabla {{z}_{y}}=\frac{d}{f}\nabla {{d}_{y}}=\frac{{{y}_{ji}}{{z}_{ji}}}{x_{ji}^{2}+y_{ji}^{2}}. \\
\end{aligned}\right.
\end{equation}

The approximate gradient $\bm{\vec{g}}$ of the neighboring point $q_j$ is $\bm{\vec{g}} =({{g}_{x}},{{g}_{y}},{{g}_{z}})$, where
\begin{equation}
\begin{aligned}
\left\{
\begin{array}{lll}
{g}_{x} & = \frac{{{z}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}+z_{ji}^{2}}} \frac{{{x}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}}},\\
{{g}_{y}} & = \frac{{{z}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}+z_{ji}^{2}}}\frac{{{y}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}}},\\
{{g}_{z}} & = \frac{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}+z_{ji}^{2}}}.
\end{array}\right.
\end{aligned}
\end{equation}
%其中$\frac{{{z}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}+z_{ji}^{2}}}$和$\frac{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}+z_{ji}^{2}}}$分别为邻域点天顶角的正弦值和余弦值，$\frac{{{x}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}$和$\frac{{{y}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}$分别为邻域点方位角的正弦值和余弦值。至此，得到了$q$点近似法向量$\vec{n}$，其与$q$点梯度在数值上相同。GAM将求解邻域点梯度问题简化为了求解邻域点天顶角和方位角，在减少该模块的运行时间的同时，有效的将梯度信息引入局部特征中。
Where $\frac{{{z}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}+z_{ji}^{2}}}$ and $\frac{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2} +z_{ji}^{2}}}$ denote sine and cosine of the zenith angle of the neighborhood point respectively, $\frac{{{x}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}$ and $\frac{{{y}_{ji}}}{\sqrt{x_{ji}^{2}+y_{ji}^{2}}}$ denote sine and cosine of the azimuth angle of the neighborhood point respectively. 

We simplify gradient calculation of neighborhood points by using the zenith angles and azimuth angles, and effectively reduce computation time.

\subsection{Gradient Attention Module} \label{sec:module}
%虽然Lan Shiyi等人(Lan et al. 2019、Thomas et al. 2019)使用中心点到其各邻域点的距离来加权聚合局部特征，但我们注意到，由于局部几何结构的不规则性，单纯的利用距离信息来计算各邻域点的权重分数会降低模型的精度和鲁棒性。为了解决这一问题，本文基于邻域点的天顶角和方位角建立了一种轻量且高效的梯度注意力模块，并首次探究了邻域内梯度信息在点云邻域聚合过程中的作用，该模块通过将细粒度几何信息引入局部特征聚合描述符中，使模型能更准确的捕捉到点云局部特征。我们在表1中给出了GAM的简略步骤。
%Although Lan Shiyi et al. (Lan et al. 2019, Thomas et al. 2019) use the distance from the centroid to each of its neighborhood points to weight the aggregated local features, we note that due to the irregularity of the local geometric structure, simply using the distance information to calculate the weight fraction of each neighborhood point reduces the accuracy and robustness of the model. 

In this section we introduce our proposed gradient attention module (GAM) based on the zenith and azimuth angles of neighborhood points, which includes gradient information of neighborhood points during neighborhood aggregation process. Therefore the model is enabled to capture more accurate local features, by using more fine-grained geometric information in the local feature aggregation descriptors. Details of our proposed GAM are given in Algorithm 1.

%表1  GAM的简略步骤
%输入：输入点位置信息$P=\{{{p}_{i}}|i=1,...,N\}\in {{R}^{N\times 3}}$，输入特征$\{{{f}_{s}}|s=1,...,{{N}_{s}}\}\in {{R}^{N\times C}}$，输出平衡权重$\lambda $，采样半径r，中心点采样数${{N}_{s}}$，邻域点采样数$K$，局部特征提取器$\phi (\cdot )$。
%输出：输出特征${{F}_{s}}$
%1.在N个输入点中采样${{N}_{s}}$个点作为点云的中心点，中心点的位置信息表示为$\{{{p}_{s}}|s=1,...,{{N}_{s}}\}\in {{R}^{{{N}_{s}}\times 3}}$；
%2.为每个中心点搜索$K$个点作为其邻域，邻域点位置信息和邻域点的局部特征表示为：$\{{{p}_{sj}}|s=1,...,{{N}_{s}},j=1,...,K\}\in {{R}^{{{N}_{s}}\times K\times 3}}$、$\{{{f}_{sj}}|s=1,...,{{N}_{s}},j=1,...,K\}\in {{R}^{{{N}_{s}}\times K\times C}}$；
%3.在每个中心点的邻域内建立有向图，并计算得到相对位置向量$$、距离信息$di{{s}_{sj}}$和邻域点的梯度信息$Gr{{a}_{sj}}$；
%4.利用MLP学习$di{{s}_{sj}}$和$Gr{{a}_{sj}}$中的细粒度几何结构，生成邻域点的权重分数矩阵。
%5.将$\phi (\cdot )$提取的局部深层特征与对应权重分数矩阵相乘，并通过$\lambda $对特征进行加权聚合，得到${{F}_{s}}$。.

% Input: input point position information $P=\{{{p}_{i}}|i=1,... ,N\\}\in {{R}^{N\times 3}}$, input features $\{{{{f}_{s}}}|s=1,... ,{{{N}_{s}}\}\\in {{R}^{N\times C}}$$, output balanced weights $\lambda $, sampling radius r, number of centroid samples ${{N}_{s}}$, number of neighborhood point samples $K$, local feature extractor $\phi (\cdot )$.
%output: output features ${{F}_{s}}$
%1. Sampling ${{N}_{s}}$ points out of N input points as the centroids of the point cloud, the position information of the centroids is denoted as $\{{{p}_{s}}|s=1,... ,{{{N}_{s}}}\}\\in {{R}^{{{N}_{s}}\times 3}}$.
%2. Search $K$ points for each centroid as its neighborhood, and the information on the location of the neighborhood points and the local features of the neighborhood points are expressed as: $\{{{p}_{sj}}|s=1,... ,{{{N}_{s}},j=1,... ,K\\}\in {{R}^{{{N}_{s}}\times K\times 3}}$,$\{{{f}_{sj}}|s=1,... ,{{{N}_{s}},j=1,... ,K\\}\in {{R}^{{{N}_{s}}\times K\times C}}$.
%3. create a directed graph in the neighborhood of each centroid and compute the relative position vector $$, distance information $$di{{s}_{sj}}$$ and gradient information $$Gr{{a}_{sj}}$$ of the neighborhood points.
%4. Use MLP to learn the fine-grained geometric structure in $$di{{s}_{sj}}$$ and $$Gr{{a}_{sj}}$$ to generate the weight score matrix of neighborhood points.
%5. Multiply the local deep features extracted by $\phi (\cdot )$ with the corresponding weight score matrix and weight the features by $\lambda $ to obtain ${{F}_{s}}$.

\begin{algorithm}[tb]
\caption{Gradient Attention Module}
\label{alg:algorithm}
\textbf{Input}: point cloud $\bm{P}=\{{\bm{p}_{i}}|i=1,... ,N \} \in {\mathbb{R}^{N\times 3}}$, with corresponding features $\bm{F}=\{{{\bm{f}_{i}}}|i=1,... ,N \} \in {\mathbb{R}^{N\times C}}$\\
\textbf{Parameter}: local feature extractor $\phi (\cdot )$, balanced weights $\lambda $, sampling radius r, number of centroid samples ${{N}_{s}}$, number of neighborhood point samples $K$\\
\textbf{Output}: output features $\bm{F}^{out}$
\begin{algorithmic}[1] %[1] enables line numbers
\STATE {Sample ${{N}_{s}}$ points as the center points of the point cloud, with corresponding coordinates denoted as $\{{\bm{p}^{center}_{s}}|s=1,... ,{{{N}_{s}}}\} \in {\mathbb{R}^{{{N}_{s}}\times 3}}$.}
\STATE {Search $K$ points for each center point as its neighborhood, with corresponding coordinates $\bm{Q}^{NBR} =\{\bm{q}^{NBR}_{s,j}|s=1,... ,{{N}_{s}},j=1,... ,K \}\in {\mathbb{R}^{{{N}_{s}}\times K\times 3}}$, and corresponding features $\bm{F}^{NBR} = \{\bm{f}^{NBR}_{s,j}|s=1,... ,{{N}_{s}},j=1,... ,K \}\in {\mathbb{R}^{{{N}_{s}}\times K\times C}}$.}
\STATE {Create a directed graph in the neighborhood of each center point and compute relative position vector, distance information $\bm{d}_{s,j}$ (Eq.~\ref{eq:6}) and gradient information $\bm{g}_{s,j}$ (Eq.~\ref{eq:7}) of the neighborhood points. }
\STATE {Calculate weighted score matrix of neighborhood points $\bm{A}$ by using fine-grained geometric information $\bm{d}_{s,j}$ and $\bm{g}_{s,j}$.(Eq.~\ref{eq:8})}
\STATE {Local features $\bm{F}^{NBR}$ are fed to local feature extractor $\phi (\cdot )$, multiplied with the corresponding weight score matrix $\bm{A}$ and then weighted by $\lambda$ to obtain $\bm{F}^{out}$ (Eq.~\ref{eq:9}). }

\STATE \textbf{Return} $\bm{F}^{out}$
\end{algorithmic}
\end{algorithm}

%如图2所示，给定一组点$P=\{{{p}_{i}}|i=1,...,N\}\in {{R}^{N\times 3}}$，N表示在笛卡尔坐标系$(x,y,z)$中的点个数，输入特征表示为$\{{{f}_{s}}|s=1,...,{{N}_{s}}\}\in {{R}^{N\times C}}$。在每个下采样层（down sampling layer）中，使用随机采样、Farthest point sampling (FPS) (Qi et al. 2017b)等算法采样 个点作为点云的中心点。随后使用ball query(Qi et al. 2017b)、KNN(K-Nearest Neighbor)(Wang et al. 2019)等算法为 个中心点$\{{{p}_{s}}|s=1,...,{{N}_{s}}\}\in {{R}^{{{N}_{s}}\times 3}}$.寻找 个邻域点作为该中心点的邻域，得到邻域点原始位置信息$\{{{p}_{sj}}|s=1,...,{{N}_{s}},j=1,...,K\}\in {{R}^{{{N}_{s}}\times K\times 3}}$和邻域点的局部特征$\{{{f}_{sj}}|s=1,...,{{N}_{s}},j=1,...,K\}\in {{R}^{{{N}_{s}}\times K\times C}}$，其中 表示输入特征的通道数。
As shown in Figure~\ref{fig: fig3}, there are a set of N points $ \bm{P}=\{{\bm{p}_{i}}|i=1,... ,N\}\in {\mathbb{R}^{N\times 3}} $ in the Cartesian coordinate system $(x,y,z)$, with their corresponding features  $\bm{F}=\{\bm{f}_{i}|i=1,... ,N \} \in {\mathbb{R}^{N\times C}}$. We use the baseline method~\citep{qi2017pointnet++, wang2019dynamic} to find center point and search for neighbor points. $\bm{P}^{center}=\{\bm{p}^{center}_{s}|s=1,... ,{{{N}_{s}}}\} \in {\mathbb{R}^{{{{N}_{s}}\times 3}}}$ represents the selected set of center point. For each center point, $K$ neighboring points are searched. In total there are $K*{N_s}$ neighboring points $\bm{Q}^{NBR}=\{  \bm{q}^{NBR}_{s,j}|s=1,... ,{{N}_{s}},j=1,... ,K \} \in {\mathbb{R}^{{{N}_{s}}\times K\times 3}}$ with corresponding features $\bm{F}^{NBR}=\{ \bm{f}^{NBR}_{s,j} |s=1,... ,{{{N}_{s}},j=1,... ,K\\}\}\in {\mathbb{R}^{{{N}_{s}}\times K\times C}}$, where $C$ denotes the number of channels of input features.

%在建立起中心点和其各邻域点之间的有向图后，可以得到它们之间的相对位置向量$={{p}_{sj}}-{{p}_{s}}$，向量长度$di{{s}_{sj}}$可表示为： 
After establishing the directed graph between center points and each of its neighboring points, the relative position matrix is represent as $\bm{E} = \{ \bm{e}_{s,j} |s=1,... ,{{{N}_{s}},j=1,... ,K\\}\}$  .  $\bm{e}_{s,j}$ is represented as $\bm{q}^{NBR}_{s,j}-\bm{p}^{center}_{s}$, where $\bm{p}^{center}_s$ is the center point and $\bm{q}^{NBR}_{s,j}$ is one of its neighboring points. Vector length $\bm{{d}_{s,j}}$ is also expressed as follows,

\begin{equation} \label{eq:6}
\begin{aligned}
\left\{
\begin{array}{ll}
{\bm{e}_{s,j}}& =(\overrightarrow{\bm{x}_{s,j}},\overrightarrow{\bm{y}_{s,j}},\overrightarrow{\bm{z}_{s,j}}) \\ 
{\bm{d}_{s,j}}& =\sqrt{{{(\overrightarrow{\bm{x}_{s,j}})}^{2}}+{{(\overrightarrow{\bm{y}_{s,j}})}^{2}}+{{(\overrightarrow{\bm{z}_{s,j}})}^{2}}} \\
\end{array}\right.
\end{aligned}
\end{equation}
%其中$(\overrightarrow{{{x}_{sj}}},\overrightarrow{{{y}_{sj}}},\overrightarrow{{{z}_{sj}}})$为相对位置向量$$的笛卡尔坐标系表示。随后计算各邻域点方位角正弦值和余弦值之和，并将之与天顶角的正弦值相乘作为邻域点的梯度信息$Gr{{a}_{sj}}$：
where $(\overrightarrow{\bm{x}_{s,j}},\overrightarrow{\bm{y}_{s,j}},\overrightarrow{\bm{z}_{s,j}})$ represent the relative position vector in the Cartesian coordinate system. 

Then the sum of azimuthal sine and cosine values of each neighboring point is calculated, and multiplied with sine of zenith angle, to represent the gradient information of the neighborhood points $\bm{{g}_{s,j}}$ written as follows,
\begin{equation} \label{eq:7}
\bm{g}_{s,j}=(\frac{\overrightarrow{\bm{z}_{s,j}}}{\bm{d}_{s,j}}\frac{\overrightarrow{\bm{x}_{s,j}}+\overrightarrow{\bm{y}_{s,j}}}{\sqrt{(\overrightarrow{\bm{x}_{s,j}})^{2}+(\overrightarrow{\bm{y}_{s,j}})^{2}}}).
\end{equation}
%GAM合并了邻域点的梯度信息和距离信息， MLP被设计用来从合并后的细粒度几何信息中学习邻域点的权重 ，每个MLP由一个FC层、归一化层和激活层（ReLU）组成，计算过程可用下式表示：
Our proposed GAM uses MLP to fuse gradient information and distance information of neighborhood points to obtain attentive weight calculated as follows,
\begin{equation} \label{eq:8}
a_{s,j}=Sigmoid(MLP([\bm{g}_{s,j};\bm{d}_{s,j}])).
\end{equation}
%其中Sigmoid为激活函数，[;]表示concat操作。具体来说，无论各种点云模型是如何聚合局部信息的，GAM的工作就是计算权重矩阵并将其与输入的点云特征相乘，这样MLP就可以根据每个邻域点的重要程度来提取点云的深层特征。完整的局部特征聚合过程可以被简略表示为：
where Sigmoid is the activation function and [;] denotes the concatenation operation. The weight matrix is represented as $\bm{A} = \{ a_{s,j} |s=1,... ,{{{N}_{s}},j=1,... ,K\\}\}$

After obtaining the weight matrix $\bm{A}$, it is multiplied with the input point cloud features. Thus GAM can extract  deep features of the point cloud according to importance of each neighborhood point. A complete local feature aggregation process can be expressed as follows, 
\begin{equation} \label{eq:9}
\bm{F}^{out}=MLP(\frac{\lambda \phi ({\bm{F}^{NBR})\cdot {\bm{A}}+\phi ({\bm{F}^{NBR}}})}{1+\lambda})
\end{equation}
%其中${{f}_{sj}}$表示局部特征，${{F}_{s}}$为输出特征，$\lambda $为输出平衡权重，$\phi (\cdot )$表示现有工作的局部特征提取器，其被用来提取深度聚合的特征。GAM展现出了一些突出的优点：1) 该模块将邻域内的细粒度几何信息引入邻域中，更好地聚合了邻域特征；2)在几乎不增加模型的显存需求和推理时间的基础上，进一步提升模型的性能；3) 即插即用，即插即涨。
where $ \bm{F}^{out} \in {\mathbb{R}^{{{N}_{s}}\times K\times C_{out}}}$, $C_{out}$ denotes the number of channels of output features.  $\lambda$ is the balance weight and $\cdot$ represents the element-wise multiplication. $\phi (\cdot )$ denotes the local feature extractor used in previous works~\citep{ma2022rethinking, qi2017pointnet++, wang2019dynamic} to extract deeply aggregated features.

%GAM exhibits some outstanding advantages: 1) the module brings the in-neighborhood fine-grained geometric information into the neighborhood and better aggregates neighborhood features; 2) it further improves the performance of the model with little increase in the model's explicit memory requirements and inference time; and 3) it is plug-and-play module.

%3.2	点云邻域梯度的数学表示

\section{Experiments}
%GAM是即插即用的局部聚合注意力模块，为了全面的验证其有效性，本文分别在S3DIS(Armeni et al. 2016)、ScanObjectNN(Uy et al. 2019)、ShapeNet (Yi et al. 2016) 、ModelNet40 (Wu et al. 2015)和KITTI (Geiger, A.; Lenz, P. and Urtasun, R. 2012)数据集上对现有模型进行了3D点云形状分类、3D部件分割实验、3D语义分割实验和3D目标检测实验，其中KITTI 3D目标检测实验和ModelNet40 3D形状分类实验详见附录A和附录B。实验环境为NVIDIA GTX 3090 GPU和AMD EPYC 7402 CPU。
To fully evaluate the effectiveness and generality of our proposed GAM, we apply our proposed method on several state-of-the-art methods for 3D shape classification, 3D part segmentation, 3D semantic segmentation, and 3D object detection. Experiments are conducted on S3DIS dataset~\citep{armeni20163d}, ScanObjectNN dataset~\citep{uy2019revisiting}, ShapeNet dataset~\citep{yi2016scalable}, KITTI dataset~\citep{geiger2013vision} and ModelNet40 dataset~\citep{wu20153d}, respectively. 
The same training strategy used in each baseline is employed in our experiments, except that only GAM is added in each downsampling layer of the model. And $\lambda$ is set to 1. The number of channels of the two-layer MLP in GAM is set to (1,16), (16,1). Experiments are run on NVIDIA GTX 3090 GPU and AMD EPYC 7402 CPU.

\begin{table}[]
\small
\setlength\tabcolsep{3pt}
\centering

\begin{tabular}{c|ccc|ccc}
\hline
\makebox[0.08\textwidth][c]{Method} & \makebox[0.04\textwidth][c]{mIoU} & \makebox[0.04\textwidth][c]{OA} & \makebox[0.04\textwidth][c]{mAcc} & \makebox[0.033\textwidth][c]{TP} \\
\hline
PointNet~\cite{qi2017pointnet}            & 47.6      & 78.5    & 66.2      & -            \\
PointCNN~\cite{li2018pointcnn}            & 65.4      & 88.1    & 75.6      & -                  \\
PointWeb~\cite{zhao2019pointweb}          & 66.7      & 87.3    & 76.2      & -                        \\
RandLA-Net~\cite{hu2020randla}          & 70.0      & 88.0    & 82.0      & -                     \\
KPConv~\cite{thomas2019kpconv}        & 70.6      & -       & 79.1      & -                          \\
BAAF-Net~(Qiu et al. 2021)         & 72.2      & 88.9    & 83.1         & -          \\
\hline
PointNet++~\cite{qi2017pointnet++}        & 54.5      & 81.0    & 67.1      & 130                     \\
+ GAM                      & 56.6      & 81.8    & 71.7      & 127                      \\
\hline
DGCNN~\cite{wang2019dynamic}           & 56.1      & 84.1    & -         & 32                   \\
+ GAM                           & 58.8      & 85.5    & 69.1      & 31                        \\
\hline
Point Trans.~\cite{zhao2021point} & 73.5      & 90.2    & 81.9      & 26                       \\
+ GAM               & 73.9      & 89.9    & 83.0      & 25                        \\
 + GAM*              & \textbf{74.4}      & \textbf{90.6}    & \textbf{83.2}      & -          \\ 
\hline      
\end{tabular}
\caption{Comparison results on S3DIS dataset for 3D semantic segmentation with 6-fold cross-validation. Mean Inter-over-Union (mIoU), overall accuracy (OA) and mean accuracy (mAcc) are used as evaluation metrics. * represents the voting strategy and throughput using the test result in Area5. } 
\label{tab:tab1}
\end{table}

\subsection{Results on 3D Semantic Segmentation}
%本文在S3DIS(Armeni et al. 2016)数据集上进行了3D语义分割实验。该数据集包含了6个室内区域的3D扫描点云，一共有272个房间。每个点都属于13个语义类别中的一个，包含木板、书柜、椅子、天花板等。训练中保持与各baseline相同的训练策略，仅在模型的每一个下采样层中加入一个GAM，并将设为1。
We conduct the 3D semantic segmentation experiment on S3DIS dataset~\citep{armeni20163d}, which contains 3D scanned point clouds of six interior regions with 272 rooms in total. Each point belongs to one of the 13 semantic categories containing wood panels, bookcases, chairs, ceilings, etc. We compare the proposed GAM with state-of-the-art methods, including PointNet~\citep{qi2017pointnet}, PointCNN~\citep{li2018pointcnn}, PointWeb~\citep{zhao2019pointweb}, RandLA-Net~\citep{hu2020randla}, KPConv~\citep{thomas2019kpconv}, BAAF-Net~\citep{qiu2021semantic}, PointNet++~\citep{qi2017pointnet++}, DGCNN~\citep{wang2019dynamic}, and Point Trans.~\citep{zhao2021point}. We select PointNet++, DGCNN, and Point Trans. as baselines to evaluate the effectiveness of adding our proposed method GAM.

%S3DIS 3D语义分割6折交叉验证（6-fold cross validation）实验结果。为了更好的阐述结果，本文对比了各baseline和其添加GAM后的速度。使用mean Inter-over-Union (mIoU)、overall accuracy(OA)和mean accuracy(mAcc)作为评价指标，*代表Voting。Throughput使用Area5中的测试结果。粗体为加入GAM后的实验结果。


%表1中报告了3D语义分割6折交叉验证（6-fold cross validation）的精度、吞吐量和参数量。PointNet++、DGCNN和Point Transformer被选择来验证GAM的性能，其中PointNet++利用MLP进行特征提取，DGCNN建立在图卷积网络上，Point Transformer则是SOTA。具体来说，对于三个加入GAM的baseline，其mIoU分别显著提高了2.1%、2.7%和0.4%，同时OA和mAcc也有着显著的提升。在使用Voting后，GAM以74.4/90.6/83.2的mIoU/OA/mAcc达到目前point-base模型中的最高指标，证明GAM轻量但是高效。三组对比实验中，baseline的推理时间仅分别增加了2.3%、3.1%和3.8%，而参数量和FLOPs方面的计算成本增加几乎可以忽略不计。因此， GAM对点云分析领域的更进一步发展具有一定的指导价值，通过设计高效精巧的模块，在控制计算成本的基础上进一步提升模型的性能。附录C和附录D中展示了S3DIS Area5的结果和分割效果图。
In Table~\ref{tab:tab1}, we report mean Inter-over-Union (mIoU), overall accuracy (OA) and mean accuracy (mAcc), and throughput(TP) for the 6-fold cross-validation of S3DIS dataset. We can find that after adding GAM into the three baselines PointNet++, DGCNN, and Point Trans, mIoU scores increase 2.1\%, 2.7\%, and 0.4\% respectively, OA scores and mAcc scores are also significantly increased. After using Voting, GAM with Point Trans achieves the best performance, where mIoU score, OA score, and mAcc score are 74.4\%, 90.6\%, and 83.2\%, respectively. Inference time of three baselines after adding GAM only increases by 2.3\%, 3.1\%, and 3.8\%, respectively. Increment on computational costs in terms of throughput is almost negligible. These experimental results demonstrate that our proposed GAM is lightweight and efficient. Figure~\ref{fig: fig4} shows visualization result of 3D semantic segmentation of S3DIS dataset.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure4.pdf}
    \caption{3D semantic segmentation experiment visualization results. (a) represents ground truth and (b) represents forecast results of point transformer~\cite{zhao2021point} after GAM is inserted.}
    \label{fig: fig4}
\end{figure*}

\begin{table}[]
\small
\setlength\tabcolsep{3pt}
\centering

\begin{tabular}{c|cc|ccc}
\hline
\makebox[0.08\textwidth][c]{Method} & \makebox[0.05\textwidth][c]{OA} & \makebox[0.05\textwidth][c]{mAcc} & \makebox[0.04\textwidth][c]{TP} \\
\hline
PointNet++~\cite{qi2017pointnet++}    & 77.9      & 75.4      & -                          \\
DGCNN~\cite{wang2019dynamic}      & 78.1      & 73.6      & -                             \\
GBNet~\cite{qiu2022geometric}   & 80.5      & 77.8      & -                            \\
\hline
PRA-Net(1K)~\cite{cheng2021net} & 81.0      & 77.9      & 1152                       \\
+ GAM                & 81.6      & 77.9      & 1094                        \\
\hline
Point-TNT~(Berg et al. 2022)  & 83.6 & 82.3 & 240                      \\
+ GAM                  & 85.0 & 82.8 & 238                          \\
\hline
PointMLP-elite~\cite{ma2022rethinking} & 84.4 & 82.6 & 749                     \\
+ GAM             & 84.8 & 88.24 & 708                         \\
\hline
PointMLP~\cite{ma2022rethinking}    & 85.7 & 84.4 & 213                       \\
+ GAM                   & 86.1 & 84.7 & 204                        \\
\hline
RepSurf-2x~\cite{ran2022surface}  & 86.0      & -         & 420                         \\
+ GAM                 & 86.4      & -         & 418                          \\
\hline
PointNeXt-s~\cite{qian2022pointnext} & 88.1 & 86.4 & 1628                       \\
+ GAM                & \textbf{88.4} & \textbf{86.5} & 1544                 \\
\hline
\end{tabular}
\caption{Comparison result on ScanObjectNN dataset for 3D shape classification. For a fair comparison, all methods in the table use 1024 points as the input. Overall accuracy (OA) and mean accuracy (mAcc) are used as evaluation metrics. }

\label{tab:tab2}
\end{table}

\subsection{3D Shape Classification Experimental Results on ScanObjectNN}
%ScanObjectNN (Uy et al. 2019) 数据集包含15,000个对象，这些对象被分为15个类，由于存在背景、噪声和遮挡，该基准对现有的点云分析方法提出了重大挑战。本文在PB_T50_RS上进行实验，这是目前ScanObjectNN最困难和最常用的变体。实验使用与各baseline相同的训练策略，仅在模型的每个下采样层中加入一个GAM，并将 设为1。实验的评价指标为Overall accuracy(OA) 和 Mean class accuracy（mAcc）。表2报告了各开源模型加入GAM前后的精度、推理时间和参数量。RepSurf、PointMLP等最近提出的模型均在ScanObjectNN数据集中取得了极佳的表现。在增加了GAM后，参数量差异较大的PointMLP、PointMLP-elite的OA平均提高了0.5%和0.6%，这表明该模块在不同规模的baseline中都可以适用。Prior SoTA模型RepSurf的OA从86.0%提高到86.4%。与baseline相同，在实验中我们没有使用多尺度推理策略。实验结果表明，GAM可以有效的增强不同baseline性能，这使得其有潜力成为一个广泛使用于点云领域的注意力模块。同时，根据推理时间和参数量的报告结果表明，加入GAM后仅增加了可忽略不计的parmas和FLOPs，且推理时间仅平均增加了3.5%。
We conduct the 3D shape classification experiment on  ScanObjectNN dataset~\citep{uy2019revisiting} that contains 15,000 objects of 15 different classes. The most difficult and commonly used variant of ScanObjectNN $PB_T50_RS$ is implemented for experiments. We compare our proposed GAM with state-of-the-art methods, including PointNet++~\citep{qi2017pointnet++}, DGCNN~\citep{wang2019dynamic}, GBNet~\citep{qiu2022geometric}, PRA-Net(1k)~\citep{cheng2021net}, Point-TNT~\citep{berg2022points}, PointMLP-elite~\citep{ma2022rethinking}, PointMLP~\citep{ma2022rethinking}, RepSurf-2x~\citep{ran2022surface} and PointNeXt-s~\citep{qian2022pointnext}. Among these methods, PRA-Net(1k), Point-TNT, PointMLP-elite, PointMLP, RepSurf-2x, and PointNeXt-s are used as baselines to evaluate the effectiveness of adding our proposed GAM. 

In Table~\ref{tab:tab2}, we report the overall accuracy (OA) score, mean accuracy (mAcc), and throughput (TP) on the ScanObjectNN dataset. The voting strategy is not used in each baseline. After adding GAM, the OA score and mAcc score of each baseline method are increased significantly. Experiment results demonstrate that our proposed GAM has the potential to be widely used in the point cloud domain. Besides, results in terms of throughput (TP) indicate that computation cost barely increases after adding GAM.

\begin{table}[]
\centering
\begin{tabular}{c|ccc}
\hline
\makebox[0.06\textwidth][c]{Method} & \makebox[0.04\textwidth][c]{Cars} & \makebox[0.06\textwidth][c]{Pedestrians} & \makebox[0.06\textwidth][c]{Cyclists} \\
\hline
IA-SSD~\cite{zhang2022not} & \textbf{79.57}    & 58.91           & 71.24        \\
+GAM    & 79.16    & \textbf{59.31}           & \textbf{72.58}        \\
\hline
\end{tabular}
\caption{Comparison results of 3D object detection on KITTI 3D target detection validation set. Mean Inter-over-Union (mIoU) score is used as the evaluation metric. }

\label{tab:tab3}
\end{table}

\subsection{Result on 3D Object Detection} 
%GAM不仅可以在室内数据集中取得良好的效果，在大尺度的室外点云场景中依然能够提升模型的性能。本文在KITTI (Geiger et al. 2012)数据集中进行3D目标检测实验，该数据集将检测目标分为汽车、行人和自行车三类，并且根据困难程度将每一类分为“容易”、“中等”和“困难”三个等级。“中等”的结果通常作为模型性能的主要评价指标。训练中保持与IA-SSD  (Zhang et al. 2022)相同的训练策略，仅在每个下采样层中加入GAM，并将 设为1。表7  KITTI 3D目标检测验证集实验结果。使用mean Inter-over-Union (mIoU)作为评价指标。粗体为加入GAM后的实验结果。表7报告了在KITTI数据集的验证集上取得的结果，可以看出，行人和自行车这两类的mIoU分别提升了0.4%和1.34%，而汽车下降了0.41%，这表明加入GAM后的模型对小目标的检测性能得到提高，而由于汽车这一类型的分布情况较为复杂，导致这个类型的精度下降
For 3D object detection task, experiments are conducted on the KITTI dataset~\citep{geiger2013vision}, which has three detection categories, cars, pedestrians, and bicycles. Each category has three subsets, "easy", "medium" and "difficult", basing on the detection difficulty. The "medium" subset is the most commonly used for evaluation. 

In Table~\ref{tab:tab3}, we report mean Inter-over-Union (mIoU) score on validation set of the KITTI dataset. After adding GAM, mIoU scores of pedestrian and bicycle category are improved by 0.4\% and 1.34\% respectively, while for car category mIoU score decreases by 0.41\%. These results indicate that detection performance for small targets can be effectively improved with GAM.

\subsection{3D Shape Classification Experimental Results on ModelNet40}
\begin{table}[]
\begin{tabular}{c|c|cc}
\hline
\makebox[0.02\textwidth][c]{Method} & \makebox[0.022\textwidth][c]{Input} & \makebox[0.025\textwidth][c]{OA} & \makebox[0.025\textwidth][c]{mAcc} \\
\hline
KPConv~\cite{thomas2019kpconv}      & 7k   & 92.9      & -         \\
Point Trans.~\cite{zhao2021point}             & 1k    & 93.7      & 90.6      \\
CurveNet~\cite{xiang2021walk}                 & 1k    & 94.2      & -         \\
\hline
PointNet++(S)~\cite{qi2017pointnet++}  & 1k   & 92.2      & 89.1      \\
+ GAM                              & 1k   & 92.8      & 91.5      \\
\hline
PointNet++(M)~\cite{qi2017pointnet++} & 1k+N & 92.8      & 90.7      \\
+ GAM & 1k+N & 93.3      & 91.4      \\
\hline
DGCNN~\cite{wang2019dynamic}                        & 1k   & 92.9      & 90.2      \\
+ GAM & 1k   & 93.3      & 90.5      \\
\hline
PointMLP*~\cite{ma2022rethinking}                    & 1k   & 94.5      & 91.4      \\
+ GAM*& 1k    & \textbf{94.7}      & \textbf{91.9} \\
\hline
\end{tabular}
\caption{Comparison results on ModelNet40 dataset on 3D shape classification tasks. N indicates that the input point cloud contains normal vector information, * indicates that voting is used, S represents Single-Scale Grouping, and M represents Multi-Scale Grouping. }
\label{tab:tab4}
\end{table}

We conduct 3D shape classification experiments on ModelNet40 dataset~\citep{wu20153d}, which has 12311 CAD samples, including 9843 training samples and 2468 test samples. The proposed GAM is compared with the state-of-the-art methods, which are KPConv~\citep{thomas2019kpconv}, Point Transformer~\citep{zhao2021point}, CurveNet~\citep{xiang2021walk}, PointNet++(SSG)~\citep{qi2017pointnet++}, PointNet++(MSG)~\citep{qi2017pointnet++}, DGCNN~\citep{wang2019dynamic}, and PointMLP~\citep{ma2022rethinking}. We select PointNet++(SSG), PointNet++(MSG), DGCNN, and PointMLP as baselines to evaluate effectiveness of our proposed method. 

In Table~\ref{tab:tab4}, we report overall accuracy (OA) score and mean accuracy (mAcc) on the ModelNet40 dataset. After adding GAM, OA score and mAcc are increase significantly.

\begin{table}[]
\setlength\tabcolsep{4pt}
\centering
\begin{tabular}{c|cc}
\hline
\makebox[0.28\textwidth][c]{Method} & \makebox[0.07\textwidth][c]{Ins} & \makebox[0.07\textwidth][c]{TP}\\
\hline
PointNet~\citep{qi2017pointnet}     &83.7     &-\\
Point Tran.~\citep{zhao2021point}  &86.6  &-\\
PointMLP~\citep{ma2022rethinking}    &86.1  &-\\
\hline
PointNet++~\citep{qi2017pointnet++}  &85.1  &370\\
+ GAM       &85.5  &368\\
\hline
DGCNN~\citep{wang2019dynamic}       &85.2  &257\\
+ GAM       &85.5  &235\\
\hline
CurveNet~\citep{xiang2021walk}*   &86.8  &104\\
+ GAM*      &\textbf{87.0}  &99\\   
\hline
\end{tabular}
%ScanObjectNN 3D点云形状分类结果， Vote表示多尺度推理。表格中的各开源模型的OA和mAcc以论文中的结果为准，且表格中模型的输入均为1024个点。部分模型进行三次训练，并报告了标准偏差+-。粗体为加入GAM后的实验结果。
\caption{Comparison results on the ShapeNet dataset for 3D part segmentation on different classes. Ins. represent the instance average Inter-over-Union. * denotes the use of voting.}
\label{tab:tab5}
\end{table}

\subsection{Result on 3D Part Segmentation}
%本文在ShapeNet (Yi et al. 2016)数据集上进行了三维零件分割实验。该数据集收集16个类别的16881个形状模型。其中的大多数对象被标记为少于6个部分，总共有50个不同的部分。每个模型被采样2048个点进行训练。训练中，保持与各baseline相同的训练策略，仅在模型的每个下采样层中加入一个GAM，并将 设为1。

%表3报告了各baseline和其插入GAM后的Ins.mIoU、Cls.mIoU和吞吐量。PointNet++、DGCNN和SOTA CurveNet的Ins.mIoU在添加GAM后分别提高了0.4%、0.3%和0.2%，这不仅表明本文提出的GAM 在3D部件分割任务的不同baseline中也有着良好的提升效果，同时也证明了该模块并非只对单一的数据集有效，而可以作为一个在point-base方法中通用的邻域注意力模块，良好的应用于3D点云形状分类、3D点云语义分割、3D点云部件分割和3D目标检测等多种任务中。

We conduct 3D part segmentation experiments on the ShapeNet dataset~\citep{yi2016scalable} that has 16881 3D objects with 50 different categories of segmentation masks.  We compare our GAM with the state-of-the-art methods, including  PointNet~\citep{qi2017pointnet}, Point Tran.~\citep{zhao2021point}, PointMLP~\citep{ma2022rethinking}, PointNet++~\citep{qi2017pointnet++}, DGCNN~\citep{wang2019dynamic} and CurveNet~\citep{xiang2021walk}. Each object was sampled to 2048 points. 

In Table~\ref{tab:tab5}, we report the instruction mIou (Ins.) and throughput (TP) for each baseline method before and after adding GAM. The Ins. scores of baseline methods (i.e. PointNet++, DGCNN, and SOTA CurveNet) increase by 0.4\%, 0.3\%, and 0.2\% after adding GAM, respectively. 

%which not only shows that the proposed GAM in this paper has a good performance in 3D part segmentation 
%This not only shows that the proposed GAM has a good improvement in different baselines of 3D part segmentation task, but also proves that the module is not only effective for a single dataset, but can be used as a generalized neighborhood attention module in point-base methods, which is well applied in various tasks such as 3D point cloud shape classification, 3D point cloud semantic segmentation, 3D point cloud part segmentation and 3D target detection.

%在ShapeNet上对3D部件分割的实验结果，评价指标为instance average Inter-over-Union(Cls.mIoU)和class average mean Inter-over-Union (Ins.mIoU)。粗体为加入GAM后的实验结果， *表示使用Voting。

\subsection{Ablation Study}
%GAM加入了邻域梯度信息而使得在局部特征聚合描述符可以更有效的聚合点云邻域信息（Point cloud neighborhood information），为了证明梯度信息的重要性，本文创建了两个GAM的变体，其中分别仅加入距离信息和仅加入梯度信息。本文在ScanObjectNN和S3DIS基准中进行了消融实验，在PointNet++、DGCNN和PointMLP中插入不同的GAM变体，并在表4和表5中报告了它们的性能。 

In order to verify the effectiveness of gradient information, two variants of GAM are created, one with distance information and the other with gradient information. Results of ablation experiments conducted on the ScanObjectNN dataset and S3DIS dataset are present in Table~\ref{tab:tab6} and Table~\ref{tab:tab7}, respectively.
\begin{table}[]
\setlength\tabcolsep{4pt}
\centering
\begin{tabular}{cc|cc|cc}
\hline
\makebox[0.05\textwidth][c]{ } & \makebox[0.05\textwidth][c]{  } & \multicolumn{2}{c|}{PointNet++} & \multicolumn{2}{c}{PointMLP} \\
\hline
Distance & Gradient & OA      & mA       & OA        & mA     \\
\hline
\XSolid         & \XSolid         & 86.2          & 84.3           & 85.7     & 84.4    \\
\Checkmark        & \XSolid         & 86.1          & 84.2           & 84.3     & 82.4    \\
\XSolid         & \Checkmark        & 86.5          & 84.5           & 85.8     & 84.6    \\
\Checkmark        & \Checkmark        &  \textbf{87.0}          &  \textbf{85.8}           &  \textbf{86.1}     &  \textbf{84.7}    \\
\hline
\end{tabular}
\caption{Ablation study of our proposed GAM using different kinds of information on ScanObjectNN dataset.}
\label{tab:tab6}
\end{table}


\begin{table}[]
\centering
\begin{tabular}{cc|cc|cc}
\hline
         &          & \multicolumn{2}{c|}{PointNet++} & \multicolumn{2}{c}{DGCNN} \\
\hline
Distance & Gradient & mIoU        & OA       & mIoU        & OA     \\
\hline
\XSolid        & \XSolid        & 53.5          & 83.0           & 47.9     & 83.6    \\

\Checkmark        & \XSolid        & 54.1         &  \textbf{83.2}           & 49.3     & 84.4    \\
\XSolid        & \Checkmark        & 54.5          & 83.0           & 49.7     & 84.5    \\
\Checkmark        & \Checkmark        &  \textbf{54.8}         &  \textbf{83.2}           &  \textbf{50.0}     &  \textbf{84.6} \\
\hline
\end{tabular}
\caption{Ablation study of our proposed GAM using different kinds of information on S3DIS dataset Area5.}
\label{tab:tab7}
\end{table}


%如表4所示，我们在PointNet++和PointMLP后插入仅使用距离信息的GAM变体后，他们的OA分别下降了0.1\%和1.3\%，这表明单一的距离信息并不足以约束邻域的聚合过程，这样的简单约束可能会退化模型的性能。仅使用梯度信息的GAM变体使两个baseline的性能分别增加了0.3\%和0.1\%，说明相较于距离信息，梯度信息可以更好地约束局部特征的提取过程。当我们合并距离和梯度信息后，GAM的性能相较于仅加入单一细粒度特征的变体有了显著的提升。本文认为原因在于，不同的细粒度几何信息的组合可以从多个几何维度对局部聚合过程进行约束，有助于注意力模块更好地挖掘点云的原始结构信息。

%表5中的S3DIS area5实验结果也证明了上述观点。在两个baseline中添加距离信息后，它们的mIoU分别提高了0.6\%和1.4\%。而加入梯度信息使得baseline的mIoU分别提高1.0\%和1.8\%。在合并距离和梯度信息后，baseline的性能又有了一定的提高，相较于仅增加梯度信息，PointNet++和PointMLP的mIoU分别提高了0.3\%。残差实验证明了梯度信息相较于距离信息有着一定的优越性，并且我们注意到，通过对多种细粒度几何信息的叠加，注意力模块可以有更好地性能。希望这一结论可以启发研究人员探索更多细粒度几何信息在邻域特征提取过程中的作用。

As shown in Table~\ref{tab:tab6}, after adding the GAM variant with only distance information for PointNet++ and PointMLP, OA scores decrease by 0.1\% and 1.3\%. After adding the GAM variant with only gradient information, the OA score increases by 0.3\% and 0.1\%, indicating that gradient information can constrain the local feature aggregation process more effectively. While with both distance and gradient information, the OA score can be significantly increased than single fine-grained information. Therefore, we conclude that the combination of various fine-grained geometric information is more effective in constraining the local aggregation process by using multiple geometric dimensions.

In Table~\ref{tab:tab7}, we report the results of the ablation study for our proposed GAM using different kinds of information on the S3DIS dataset Area5. It also demonstrates the superiority of gradient information over distance information, and the effectiveness of combining various fine-grained geometric information.

%GAM简化前后单次运行时间对比。Normal表示直接计算各邻域点法向量，Zenith & Azimuth表示计算天顶角和方位角的显式表示。
\begin{table}[ht]
\centering
\begin{tabular}{c|c|c}
\hline
       & Normal & Zenith \&   Azimuth \\
\hline
time (ms) & 18.6   & 0.522 \\
\hline
\end{tabular}
\caption{Computation time of a single run before and after GAM simplification. Normal represents direct calculation of the normal vector for each neighborhood point, Zenith \& Azimuth represents calculation of the explicit representation using the zenith angles and azimuth angles.}
\label{tab:tab8}
\end{table}

%为了进一步验证将梯度信息简化为天顶角和方位角的显式表示带来的模块加速，本文将GAM中计算天顶角和方位角的步骤替换为使用局部表面拟合法(Pauly et al. 2003)计算邻域内法向量。结果如表6所示，简化后的模块速度相较于直接求解法向量快了约57倍，这有效的减少了加入GAM后可能造成的推理时间增长。

In three-dimensional space, the gradient of a three-dimensional function and the normal vector of a three-dimensional isosurface have different geometric meanings, but they are essentially the same. Therefore, we modify the local surface fitting method~\citep{pauly2003point} to calculate the normal vector of a plane, which is formed by the projection of the center point on the X and Y circles and neighboring points. It is the same as the object meaning of the gradient calculated in GAM. The results are shown in Table~\ref{tab:tab8}. After simplification, calculation speed is about 35 times faster than the original method, which effectively alleviates the problem of slow inference speed after adding GAM.

%===================================================

\section{Conclusion}
%本文针对局部特征聚合过程中各邻域点重要性不同的问题，提出了一个高效、轻量且即插即用的梯度注意力模块GAM，并首次将梯度信息引入到局部特征聚合符中，证明了细粒度几何信息在局部聚合过程中的有效性和重要性。在3D点云形状分类、3D部件分割实验、3D语义分割实验和3D目标检测四个任务上进行对比实验，验证了GAM的高效性能。通过消融实验，验证了梯度信息在点云分析任务中的优越性。GAM在几乎不增加模型显存和推理时间的前提下可以有效的提高各开源模型的性能，并且可以适用于绝大多数的点云分析任务，希望本文的工作可以推动社区对局部特征聚合描述符的研究。
In this paper, we propose an efficient, lightweight, and plug-and-play gradient attention module (GAM), in which gradient information is introduced into the local feature aggregator for the first time. Our proposed GAM solves the problem of the different importance of each neighborhood point in the local feature aggregation process, and brings fine-grained geometric information to the local aggregation process. The effective and efficient performance of our proposed GAM is verified by conducting comparison experiments on four tasks, including 3D point cloud shape classification, 3D part segmentation, 3D semantic segmentation, and 3D object detection. It is our expectation that this work can promote further research on local feature aggregation descriptors.

\section{Acknowledgements}
This work was supported by a grant from Zhejiang Leapmotor Technology CO., LTD, China.
% \bibliographystyle{unsrtnat}
\bibliography{aaai23}
\end{document}