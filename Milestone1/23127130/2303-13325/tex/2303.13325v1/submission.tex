% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tabularx}
%\usepackage{layouts}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{pgfplots}

\DeclareMathOperator*{\argmin}{\arg\min}
\usepackage{color, colortbl}
\definecolor{LightCyan}{rgb}{0.9, 0.9, 0.98}

\usepackage{comment}

%\usepackage{algorithm} 
%\usepackage{algpseudocode} 
\newcommand{\mynorm}[1]{ \left\| #1 \right\| }

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%\pgfplotsset{compat=1.18}
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{3848} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{DARE-GRAM :  Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices}

\author{Ismail Nejjar\\
EPFL, Switzerland\\
{\tt\small ismail.nejjar@epfl.ch}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Qin Wang\\
ETH Zurich, Switzerland\\
{\tt\small qwang@ethz.ch}
\and
Olga Fink\\
EPFL, Switzerland\\
{\tt\small olga.fink@epfl.ch}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
%We present a simple yet effective domain adaptive regression method. 
Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap between a labeled source dataset and an unlabelled target dataset for regression problems. Recent works mostly focus on learning a deep feature encoder by minimizing the discrepancy between source and target features. 
In this work, we present a different perspective for the DAR problem by analyzing the closed-form ordinary least square~(OLS) solution to the linear regressor in the deep domain adaptation context.  Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features, which is motivated by its presence in the OLS solution and the Gram matrix's ability to capture the feature correlations. Specifically, we propose a simple yet effective DAR method which leverages the pseudo-inverse low-rank property to align the scale and angle in a selected subspace generated by the pseudo-inverse Gram matrix of the two domains. We evaluate our method on three domain adaptation regression benchmarks. Experimental results demonstrate that our method achieves state-of-the-art performance. Our code is available at \url{https://github.com/ismailnejjar/DARE-GRAM}.
%$Consequently, we propose the Domain Adaptation Regression by matching inversed GRAM matrices (DARE-GRAM). Rather than aligning the original feature embedding space, we propose to used the pseudo-inverse low-rank property to align in scale and angle a selected subspace generated by the Gram matrix of the two domains. We evaluate the DARE-GRAM method on three domain adaptation regression benchmarks. Experimental results demonstrate that our method outperforms the state-of-the-art DAR method. 

% Unsupervised Domain Adaptation Regression (DAR) aims to bridge domain shifts when transferring knowledge from a labeled (source) dataset to a new unlabelled (target) dataset for regression tasks. Although DAR is an essential task in a wide range of applications, it has not been sufficiently addressed, especially when compared to domain adaptation for classification problems. While previous research has also applied domain adaptation methods for classification on regression tasks, recent research has demonstrated that regression is more sensitive to feature scaling. Therefore, many domain adaptation methods may fail without considering the scaling. In this paper, we
% highlight the importance of feature interaction for regression
% tasks. We introduce a simple formulation for unsupervised domain adaptation regression that takes advantage of the Gram matrix in the closed-form ordinary least squares solution.
% Consequently, we propose the Domain Adaptation Regression by matching inversed GRAM matrices (DARE-GRAM). Rather than aligning the original embedding space, we propose to used the pseudo-inverse low-rank property to align in scale and angle a selected subspace generated by the Gram matrix of the two domains. We evaluate the DARE-GRAM method on three domain adaptation regression benchmarks. Experimental results demonstrate that our method outperforms the state-of-the-art DAR method. 
\begin{comment}
We propose a new knowledge adaptation framework for regression based on the Least-Squares solution, called Domain Adaptation Regression, by matching GRAM matrices (DARE-GRAM) to solve this problem. We first introduce a simple formulation of the problem that allows us to close the domain gap by maximizing the similarity between the Gram matrix of both the source and target domains. We expect that if we can make the two domains have a small domain gap at the feature level, they would also have a small domain discrepancy at the regression head. Our method computes a cosine similarity matrix between the pseudo-inverse with respect to the k most important predictors of the Gram matrix for the source feature and the target feature.
Moreover, we introduce a scaling penalization term to match the distance between the k-eigenvalues of the two matrices.
\end{comment}

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Regression problems, in which models learn to predict continuous variables, are one fundamental paradigm in machine learning. 
%Regression methods aim to predict continuous variables with a defined ordinal relationship from observed data. 
Regression problems are omnipresent in many different applications, including computer vision tasks, such as head-pose estimation \cite{Yang_2019_CVPR}, facial landmark detection \cite{li2022towards}, human pose estimation \cite{zheng20213d}, depth estimation~\cite{godard2019digging} and eye-tracking problems\cite{shenoy2021r}, and also widely in industrial applications, such as product quality prediction and condition monitoring \cite{8756463}. Nevertheless, real-world applications are often subject to the environmental conditions under which the data are collected and other influencing factors, hence domain gaps between datasets are inevitable. 

\begin{comment}
Deep neural networks (DNN) have made considerable progress across various tasks. However, collecting enough labeled data for each new task is expensive and inefficient. Therefore, particular attention has been paid to the field of transfer learning \cite{zhuang2020comprehensive}, especially for unsupervised domain adaptation (UDA) \cite{ben2006analysis,csurka2017domain}, where one or more similar but distinct labeled datasets are collected as source domain(s) to assist in the identification of unlabelled instances in the new target domain.
\end{comment}

\begin{figure}
    \centering
    \includegraphics[scale=0.69]{images/Intro/figure_one.pdf}
    \caption{Illustration of the UDA for regression setup and our main motivation. (a) Deep domain adaptation networks commonly use a shared deep feature encoder and a shared linear regression layer. We propose to pay close attention to the linear regressor, where the ordinary least square~(OLS) solution is well-known.   (b) Given a trained source linear regressor $\beta_s$, the target features $Z_t$ may not be calibrated to $\beta_s$.  (c)  Unlike previous adaptation methods which align in the original feature embedding space, we propose to align the inverse Gram matrix of the features $(Z^TZ)^{-1}$, which is motivated by its presence in the OLS solution and the Gram matrix's ability to capture the feature correlations. } %Aligning the two distributions amounts to finding a representation where $\beta_s = \beta_t$. However, in the context of UDA, $\beta_t$ can not be calculated because $y_t$ is unavailable to us. Therefore, we motivate the alignment of the left-hand side of the equation, also known as the inverse of the Gram matrix $(Z^TZ)^{-1}$, to train a shared encoder, where both domains are aligned for a given regressor.}
    \vspace{-0.7cm}
    \label{fig:edge}
\end{figure}

%Due to the diversity of object characteristics, the environmental conditions under which the data are collected, and other influencing factors, domain gaps between two datasets are inevitable. 
Unsupervised Domain adaptation (UDA) aims to overcome the distributional shift between a labeled source domain and an unlabelled target domain. Many UDA methods have been proposed to alleviate the domain shift problem. One common UDA direction is feature alignment by adversarial learning \cite{long2018conditional} or explicit losses such as maximum mean discrepancy \cite{long2015learning} to learn domain-invariant representations. Input alignment \cite{yang2020fda} and self-training using pseudo-label refinement\cite{lian2019constructing} are also popular UDA directions. While many DA methods have been developed and evaluated for classification and segmentation problems, some are not directly transferable to DA regression~\cite{chen2021representation}.
% As a result, Regression problems are often overlooked in DA.
%Few attempts have been made to understand the theory of domain adaptive regression (DAR). 
Pioneer works in Domain Adaptation Regression~(DAR)~\cite{cortes2011domain,mansour2009domain} introduced theoretical analysis for the problem. A few algorithms were proposed to tackle DAR.  For example, importance weighting~\cite{demathelin2021adversarial,yamada2014domain} and feature alignment~\cite{cao2010adaptive, pan2010domain} have shown improved results over learning only from the source. %Many of them require labeled target samples. %Under the UDA setup, \cite{singh2020deep} propose to use maximum mean discrepancy to align the feature distributions. Recently, Representation Subspace Distance (RSD)~\cite{chen2021representation} demonstrated that unlike classification tasks, regression is sensitive to feature scaling. They propose to keep the source feature scales and only align the orthogonal bases of the features spaces.%highlighted that domain adaptation regression networks are sensitive to feature scaling, from which classification problems are less sensitive to. Although some UDA approaches for regression tasks have been proposed, it is still a challenging task. 
Most recent unsupervised DAR methods~\cite{singh2020deep,chen2021representation} use the deep learning framework and focus on learning a shared deep feature extractor by directly minimizing the discrepancy between source and target features. By doing so, it is implicitly assumed that if the feature discrepancy is small, a shared linear regressor can be easily learned from the source supervision. This formulation used by existing works focuses solely on the feature extractor. % and does not take the discrimination ability of the linear regressor into account. 

In this work, we propose to look at the DAR problem from a different perspective. In particular, we pay close attention to the linear regressor, which is attached directly after the feature extractor. Motivated by the closed-form ordinary least squares~(OLS) regression solution, we analyze the potential optimal regressor for each domain. We reveal in Section~\ref{sec:Motivation} that even when the discrepancy between source and target features is small, the learning of a shared linear regressor could still be difficult because of the \textit{inverse Gram matrix} term in the OLS solution. %We will show in This is because the OLS solution contains an important term, the inversed Gram matrix. Even if the distance between source and target features is small, the distance between the domains in terms of the inversed Gram matrix can still be large, as seen in Figure~\ref{fig:Toy}. This could further lead to distinct optimal regressor weights for the two domains and makes it hard to learn a common linear regressor that performs well for both domains.
%Intuitively, regression analysis aims at estimating the relationships between the output variable to the dimensional features. Given a calibrated linear regressor on the source domain, and without hurting the model generalizability the target features should 'behave' similarly to the source features. The Gram matrix can retrieve this information  since it describes the activation for each variable of the embedding dimensions and summarises the pairwise interactions between the different features. 
%The Gram matrix has some interesting properties, such as describing the activation for each variable of the embedding dimensions and summarising the pairwise interactions between the different features. This data representation has been previously explored for Domain Style Transfer~\cite{gatys2016image} to align to distributions between a source and a target style by directly minimizing the Euclidean norm of the source and target Gram matrix. We argue that the intensity and pairwise interactions between features should remain similar for source and target variables, given a calibrated linear regressor on the source domain. Unlike previous approaches in UDA for regression that regularize the distance between source and target data in the feature representation subspace, we propose to use a more robust representation of the features to mitigate the shift between these two domains, as illustrated in Figure~\ref{fig:edge}.

In light of this, we propose an ordinary least squares inspired deep domain adaptation method for regression called Domain Adaptation Regression by aligning the inverse GRAM matrices (DARE-GRAM). As shown in Figure \ref{fig:edge}, unlike previous methods, which directly align the features, we align the inverse Gram matrix of the features. This is motivated by its presence in the closed-form solution of the ordinary least squares. More specifically, we leverage the low-rank property of the pseudo-inverse to align a selected subspace in scale and angle engendered by the Gram Matrix, which 
represents the intensity and pairwise interactions between different features for the source and target domains. The scale and angle alignment based on the Gram matrix can lead to a better-calibrated regressor with regard to both source and target data. The contributions of this work are as follows:

\begin{itemize}
    \item We offer a new perspective to understand the UDA for regression problems by leveraging the well-known closed-form solutions to the linear regression problem.
    \item Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features.
    \item Empirical results on three benchmarks validate the superiority of the DARE-GRAM over baseline methods.
\end{itemize}

\section{Related Work}
\label{sec:Related Work}

\noindent\textbf{Unsupervised Domain Adaptation.} The goal of unsupervised domain adaptation (UDA) \cite{patel2015visual} is 
to address the domain-shift problem between a labeled source and an unlabeled target domain.
UDA has been widely studied for classification and segmentation problems \cite{tsai2018learning,vu2019advent} to mitigate the gap between features across different domains. Early works addressed this problem via instance weighting \cite{huang2006correcting,sugiyama2007direct}, feature transformation \cite{pan2010domain}, and feature space alignment \cite{fernando2013unsupervised}. More recently, unsupervised domain adaptation has shown impressive results \cite{10.1145/3400066,na2022contrastive,hoyer2022hrda}. Discrepancy minimization \cite{long2015learning,kang2019contrastive} and domain adversarial learning \cite{ganin2015unsupervised,hoffman2018cycada} have been widely used within UDA methods to mitigate the gap between features across different domains. Moreover, feature regularization-based approaches \cite{chen2019transferability} and domain-specific normalization-based methods \cite{chen2019domain,li2016revisiting} have also demonstrated good performance. While most approaches perform feature alignment in the encoding feature space, some works proposed to carry out alignment in the input space \cite{yang2020fda}.  More recently, self-training has also demonstrated encouraging results by training the network with gradually improved target pseudo-label \cite{Liang_2022_CVPR, Wang_2022_CVPR,zhang2021prototypical,zhang2021efficient}. Existing UDA methods mostly focus on classification and segmentation problems. While some UDA techniques can directly be applied to regression problems, recent works have shown that many do not perform  well in the regression setup~\cite{chen2021representation,Bao_2022_CVPR}. %Unsupervised Domain Adaptation for Classification techniques may easily be applied to regression tasks.
%In the results section, we will test their effectiveness in test contexts but show their limited capabilities for regression problems.
%It is mainly because the strategies mentioned above are, for the majority of them, developed for classification rather than regression tasks.
%Although the strategies mentioned above achieve significant improvement, the majority of them are developed for classification problems rather than regression tasks.

\noindent\textbf{Domain Adaptation for Regression.} Domain Adaptation for Regression (DAR) has received relatively little attention in comparison to classification problems. Early theoretical properties for DAR were introduced in \cite{cortes2011domain,mansour2009domain}. Different algorithms were proposed to tackle DAR \cite{redko2020survey}. Unfortunately, most algorithms require access to a labeled target domain and are unsuitable for UDA regression. For instance, Boosting strategies have been explored  \cite{pardoe2010boosting,wang2019transfer} to extend previous classification domain adaptation methods based on AdaBoost~\cite{margineantu1997pruning} to regression tasks. Other instance weighting methods in the shallow regime, \cite{yamada2014domain,yamada2012no,demathelin2021adversarial} have been explored for a different range of applications. 
Some specific vision applications have been explored in the context of UDA \cite{jiang2021regressive,li2021synthetic,ohkawa2022domain,kim2022unified} , such as monocular depth estimation 
 \cite{tonioni2019unsupervised,lo2022learning, Akada_2022_WACV,Bhattacharjee_2022_WACV} or gaze estimation \cite{bao2022generalizing,guo2020domain}. However, these methods aim at improving upon a specific task and not for regression tasks in general. Recent works for UDA regression were proposed  \cite{chen2021representation,singh2020deep,wu2022distribution}. A key finding in RSD \cite{chen2021representation} is that in regression problems, deep neural networks are less robust to feature scaling than classification, and aligning the distributions of deep representations will alter feature scale and impede domain adaptation regression. To tackle this challenge, the authors of \cite{chen2021representation} proposed to match the orthogonal bases of both domains to close domain shifts without altering their feature scale by introducing a new geometrical distance. While RSD-based methods have shown improved results for DAR, matching only the eigenvectors can have some disadvantages, such as more loose numerical error bound \cite{anderson1999lapack} and may not satisfy the more strict conditions for distribution estimation \cite{knowles2013eigenvector}. In contrast to RSD, we propose using the inverse Gram Matrix, which carries the necessary information to align the source and target features while being less sensitive to the batch size.

\noindent\textbf{Gram Matrix and Subspace Alignment.} Distribution alignment approaches have been used for domain adaptation \cite{Wei_2021_CVPR,sun2016deep,fernando2013unsupervised}. Subspace-based domain adaptation has demonstrated good performance in visual domain adaptation \cite{gong2012geodesic,gopalan2011domain}, modeling distribution change by finding the best intermediate subspaces. The methods first independently compute a domain-specific d-dimensional subspace for the source and target data. Then project the source and target data into intermediate ones along the shortest geodesic path connecting the two d-dimensional subspaces on the Grassmann manifold.
Instead of computing a large number of intermediate subspaces, the authors of \cite{fernando2013unsupervised} directly aligns the two subspaces. Furthermore, the authors in \cite{sun2015subspace} proposed to incorporate distribution alignment into subspace adaptation to align the source and target features.
Given their close relation, distribution alignment approaches have also been used for Neural Style Transfer (NST)  \cite{bousmalis2017unsupervised,kalischek2021light}.  Early works on NST \cite{gatys2016image} introduced the Gram Matrix as the statistics of feature maps to extract style-specific attributes. Although the connection between aligning distributions and NST may not be straightforward, it was demonstrated in \cite{li2017demystifying} that the style loss in \cite{gatys2016image} may be expressed as an unbiased empirical estimate of the Maximum Mean Discrepancy (MMD)\cite{gretton2012kernel} with a quadratic kernel.  Unlike previous works in neural style transfer which directly aligns the Gram matrix, we propose to align the inverse Gram matrix as it is presented in the OLS solution. We will show in our method and our ablation study that this is critical for regression problems.%As highlighted in  \cite{kalischek2021light}, using directly the Gram matrix has some limitations. The Gram matrix violates the identity of indiscernible since the mapping of the polynomial kernel is not injective. Hence the distribution has no unique embedding in the reproducing kernel Hilbert space.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{images/main_methode.pdf}
\caption{An overview of the proposed DARE-GRAM approach for domain adaptive regression problems. Rather than aligning the features $Z$, we align the inverse Gram matrix, which is motivated by the ordinary least square solution. To achieve this, we compute the pseudo-inverse Gram Matrices for source and target features and align their angle and scale. }
\label{fig:tab}
\end{figure*}

\section{Methods}
\label{sec:Methods}

%In this section, we first introduce the unsupervised domain adaptation for regression problem. We then present the motivation %behind DARE-GRAM and describe the step-by-step algorithm and the optimization of the framework, which consists of the proposed subspace alignment in angle and scale. 

\subsection{Problem Definition}
\label{sec:Problem formulation}
In UDA, we are given labeled samples $\raisebox{2pt}{$\chi$}_s = \{(x_s^i,y_s^i)\}_{i=1}^{N_s}$ from the source domain and unlabeled samples $\raisebox{2pt}{$\chi$}_t = \{(x_t^i)\}_{i=1}^{N_t}$ from the target domain, where $N_s$ and $N_t$ denote the number of samples in $\raisebox{2pt}{$\chi$}_s$ and $\raisebox{2pt}{$\chi$}_t$. In contrast to the discrete labels $\mathcal{Y}$ in classification problems, this work focuses on the regression problem where $\mathcal{Y} \subset \mathbb{R}^{N_r}$ is multidimensional and continuous, and $N_r$ correspond to the number of regression tasks. The discrepancy between $P(\raisebox{2pt}{$\chi$}_s)$ and $P(\raisebox{2pt}{$\chi$}_t)$ is one of the main challenges for UDA. We aim to learn a model $F: x \mapsto y$, which can generalize well on the target domain. Formally, we want to minimize the expected error on the target data:
%We aim to make the information gained from the source domain (ordinality and output's range) well-generalized in the target domain. Formally, our goal is to learn a predictive function $F: x \rightarrow y$ on the source domain to achieve a minimum error on the target domain as : 
\begin{equation}
    \argmin_{F} \mathbb{E}_{(x^t,y^t)}\lVert F(x^t),y^t\rVert_2^2,
\end{equation} 
where $y^t$ is not known during the training. 

A source-only baseline can be learned by using the supervision from the source data by minimizing the Mean square error loss (MSE) between the prediction and the ground truth label on the source samples:
%We elaborate on the training procedure using the source domain as follows. The goal is to learn a function $F$ such that $\Tilde{y} = F(x)$ matches the real output $y$ as close as possible for any given $x$ from the source, by minimizing the Mean square error (MSE) between the predicted and the ground truth label on the source images:
\begin{equation}
    \mathcal{L}_{src} = \frac{1}{N_s} \sum_{i=1}^{N_s} \lVert \Tilde{y}_s^i-y_s^i \rVert_2^2,
\end{equation}

\noindent where $\Tilde{y}_s^n=F(x_s^i)$ is the predicted value for the training source image $x_s^i$.  To overcome the distribution gap between the source and target, additional constraints should be given.

\subsection{Motivation}
\label{sec:Motivation}
In deep domain adaptation models, given an input image $x$, a feature encoder $h_\theta$ is used to learn the deep representation $z=h_\theta(x)$ of $p$ dimensions. A linear layer $g_\beta$ is then applied on ${z}$ to make the final prediction:
\begin{equation}
    \tilde{y} = F(x)  = g_\beta(h_\theta(x)) = g_\beta(z) .%(g_\phi \circ h_\theta)(x) .%+\epsilon
\label{eq:zab}
\end{equation}
During training, the feature matrix is $Z = [z^1,...,z^b]$ where $Z \in \mathbb{R}^{b\times p}$ for a batch of $b$ images.  For many adaptation methods\cite{chen2021representation,singh2020deep}, the focus has been on minimizing the distribution difference between source features $Z_s$ and target features $Z_t$. Given the aligned features, it is often assumed that they will then lead to a good performance on the target domain.  However, this formulation focuses solely on the feature extractor $h_\theta$ and does not take the discrimination ability of the final linear layer $g_{\beta}$ into account. Target features aligned with the source domain may not be adapted to the linear layer. This can be especially dangerous for regression problems because it has been demonstrated empirically~\cite{chen2021representation} that in the DA for regression context, the models can be sensitive to feature scale differences.


In this work, we propose to take the linear prediction layer $g_\beta$ into account for the distribution alignment in domain adaptation regression problems. The proposed research is motivated by the question \textit{How to find a feature space, on which a shared linear regressor can easily learn}?

Fortunately, for the linear regression problem, a closed-form solution exists and is well-studied. Given the feature $Z$ and regression ground truth label $Y$, the problem of estimating the parameter ${{\beta}}$ for a linear layer $Y = Z{\beta}$ has %\epsilon$ is 
%This formulation allows us to leverage the many procedures developed for parameter estimation and inference in linear regression and the presence of closed-form solutions. The optimal solution in the sense of solving the quadratic minimization problem is:
%\begin{equation}
%    \boldsymbol{\hat{\beta}} = \operatorname*{argmin}_{\boldsymbol{{\beta}}} S(\boldsymbol{{\beta}})
%\end{equation}
the ordinary least-squared~(OLS) closed-form solution~\cite{goldberger1964econometric}:
\begin{equation}
    {\hat{\beta}} = (Z^TZ)^{-1}Z^TY
\label{eq:ols_opti}
\end{equation}

%%%%%%%%%%%%%%%% Add the discussion of the different parts. 
\noindent where $(Z^TZ)^{-1} \in \mathbb{R}^{p\times p}$ is the inverse of the Gram Matrix. Entries are then the inner products of the basis functions of the finite-dimensional subspace. $Z^TY \in \mathbb{R}^{p\times N_r}$ projects features to the label space.

The final linear prediction layer $g_\beta$ is shared by the source and target domains. Therefore, the estimated value from the two domains should be similar ${\hat{\beta_s}} \sim {\hat{\beta_t}}$ where
\begin{equation}
\begin{split}
    {\hat{\beta_s}} &= (Z_s^TZ_s)^{-1}\  Z_s^TY_s,  \\
    {\hat{\beta_t}} &= (Z_t^TZ_t)^{-1}\ Z_t^TY_t .
\end{split}\label{eq:zst}
\end{equation}

Most of the previous DAR works regularize the neural network by minimizing the distance between source and target data in the feature representation subspace of $Z$, i.e. aligning $Z_s$ and $Z_t$. However, because of the inverse operation in Equation~\ref{eq:zst}, even if the distance between $Z_s$ and $Z_t$ is small, the distance in terms of $(Z^TZ)^{-1}$ can be large, as seen in Figure \ref{fig:Toy}. This can further lead to distinct $\hat{\beta_s}$ and $\hat{\beta_t}$ and makes it potentially infeasible to learn a common regressor that performs well for both domains. Given this observation and motivated by the closed-form OLS solution, we propose to focus on the subspace of inverse Gram matrix $(Z^TZ)^{-1}$ for the alignment. 
More specifically, we propose to align the angle between the source and target pseudo-inverse Gram Matrix, formed by a subset of the eigenspace. In addition, we propose to ensure the same scale of $Z$ for the source and target reflected by the Gram matrix $(Z^TZ)$ by minimizing the distance between selected eigenvalues of both domains.%We will show in the following sections that the common subspace choice of $Z$ for alignment can lead to , such a change in the choice of subspace for alignment can lead to improvement in 1)   2)  3) .
%If all terms are well aligned, then a good shared regressor is guaranteed to be learned. Given the above analysis, in this work, we propose to align the terms in Equation~\ref{eq:zst} for the two domains. Specifically, we propose to align the first term~(the inverse of the Gram matrix $(Z^TZ)^{-1}$ ) and second terms~(the scale of the feature $Z$) to .... The first term can be seem as . The second term ... 

%The main difference between our work and existing DAR is ... unlike ....we ...

%Given the presented motivation, we highlighted the importance of the Gram Matrix in regression problems, and we motivated the choice of matching the Gram Matrix of the source and target domain would lead to an aligned feature distribution for both domains. 

%Furthermore, to ensure the same measurement scale of $Z$, for source and target reflected by the Gram matrix $(Z^TZ)$, we also propose to minimize the distance between $k$ the sorted eigenvalues of both domains. 

\begin{comment}
\subsection{Recap of Pseudo-Inverse for Gram-Matrix}

%In Ordinary least squares, $Z$ is usually considered an over-determined system, $n>>p$. However, 
During training of deep learning models, the batch size $n$ is generally smaller than the embedding dimension $p$. The feature matrix $Z\in \mathbb{R}^{n\times p}$ is therefore not of full rank. Consequently, the Gram Matrix $(Z^TZ)$ is also not fully ranked, thus not invertible. 

Given the singular value decomposition (SVD)~\cite{van1996matrix} of a data matrix $Z \in \mathbb{R}^{n\times p}$ defined by $Z = UDV^T$. The second moment Gram matrix $(Z^TZ) \in \mathbb{R}^{p\times p}$, can be decomposed using the SVD of $Z$ as:

\begin{equation}
\begin{split}
    Z^TZ &= (UDV^T)^T(UDV^T) \\
    &= V \Lambda V^T,
\end{split}
\label{eq:lambda_square}
\end{equation}

where the orthogonal matrix $V  \in \mathbb{R}^{p\times p}$ is identical to the matrix in the SVD of $Z$ and
$\Lambda \in \mathbb{R}^{p\times p}$ is the diagonal matrix containing the eigenvalues:
\begin{equation}
    \lambda_k := \Lambda_{k,k} = D_{k,k}^2 \quad \text{for} \quad k = 1,...,p
\end{equation}

The ordered Eigenvalues are non-negative :
\begin{equation}
    \lambda_1 \geq \lambda_2 \geq ... \geq \lambda_p \geq 0
\end{equation}

Since in reality the number of samples in a batch $n$ is smaller than the number of feature dimension $<p$, the matrix $(Z^TZ)$ is not fully ranked, meaning that one or more of the smaller eigenvalues get very close or become exactly equal to $0$ under such situations. The variance of the unbiased estimator $\hat{\beta}$ can be expressed as the inverse of the eigenvalues of $(Z^TZ)$. Hence, small eigenvalues have the maximum inflation effect on the variance of the least squares estimator, thereby destabilizing the estimator significantly when they are close to $0$. One approximation of the inverse of the Gram matrix is to use the Moore-Penrose pseudo-inverse \cite{pseudo_inverse} that can be defined as :
\begin{equation}
    (Z^TZ)^+ = V^T\Lambda^+V = V^T\left(\begin{array}{ccc|cc} \frac{1}{\lambda_1} &\\
    & \ddots &&&0 \\
    && \frac{1}{\lambda_{k}}\\
    \hline
    &&&  \\
    &0&&& 0 \\
    &&&& \end{array}\right)V
\end{equation}

Where $\Lambda^+$ is formed from $\Lambda$ by taking the reciprocal of the first $k$ non-zero eigenvalues. With $k \in \mathbb{N} \cap [1,r]$ and $r$ the rank of the normal matrix defined as $r = min(b,p)$. 

This work proposes to align the feature distributions by maximizing the cosine similarity between the pseudo inverse $(Z^TZ)^+ \in \mathbb{R}^{p\times p} $ of two domains. Hence, we compute a cosine similarity matrix to measure how the most important k-eigenvectors of the target are similar, spatial-wise, to the most important k-eigenvectors of the source. Furthermore, to ensure the same measurement scale of $Z$ for the source and target reflected by the matrix $(Z^TZ)$, we also propose minimizing the euclidean distance between the k most important eigenvalues of both domains. 
\end{comment}

\begin{comment}
\begin{figure}
\begin{center}
\begin{subfigure}{0.4\textwidth}
\begin{center}
    \scalebox{0.65}{\input{cvpr2023-author_kit-v1_1-1/latex/images/toy_example/toy_distribution.pgf}}
    \end{center}
    \caption{Two distributions: a Gaussian distribution for the source feature and an exponential distribution for the target features}
    \label{fig:toy_distribution}
\end{subfigure}\\
 \hspace{\bibindent}\raisebox{\dimexpr 1.1cm-3.\height}{Before Adapation}\\
\begin{subfigure}{0.2\textwidth}
\begin{center}
    \scalebox{0.55}{\input{cvpr2023-author_kit-v1_1-1/latex/images/toy_example/toy_rsd.pgf}}
\end{center}
\caption{RSD}
\label{fig:toy_rsd_bsp}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\begin{center}
    \scalebox{0.55}{\input{cvpr2023-author_kit-v1_1-1/latex/images/toy_example/toy_DAREGRAM.pgf}}
\end{center}
   \caption{DARE-GRAM}
    \label{fig:toy_DARE-GRAM}
\end{subfigure}
\end{center}
\caption{Illustration of distribution mismatch between source and target domain. A comparison between different methods before adaption is highlighted at the bottom. Previous methods using the representation $Z$, show that both distributions are almost aligned. Our approach leverages the inverse Gram matrix representation $(Z^TZ)^{-1}$ to measure the distribution shifts in high-dimension representation and reveals in this example the difference in scale and angle between the source and target representation.}
\label{fig:Toy}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[t]{0.35\linewidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/toy_example/toy_distribution.pdf}
    \caption{Two distributions}
        \label{fig:toy_distribution}
\end{subfigure}
\begin{subfigure}[t]{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/toy_example/toy_rsd.pdf}
\caption{RSD}
\label{fig:toy_rsd_bsp}
\end{subfigure} 
\begin{subfigure}[t]{0.3\linewidth}
\centering
\includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/toy_example/toy_DAREGRAM.pdf}
   \caption{DARE-GRAM}
    \label{fig:toy_DARE-GRAM}
\end{subfigure}
\end{figure}
\end{comment}
\begin{figure}
\centering
\begin{subfigure}[t]{0.35\linewidth}
    \includegraphics[width=1.\textwidth]{images/toy_2/zab.pdf}
    \caption{Two distributions}
        \label{fig:toy_distribution}
\end{subfigure}
\begin{subfigure}[t]{0.3\linewidth}
\centering
\includegraphics[width=0.8\textwidth]{images/toy_2/zab_rsd.pdf}
\caption{$Z$}
\label{fig:toy_rsd_bsp}
\end{subfigure} 
\begin{subfigure}[t]{0.3\linewidth}
\centering
\includegraphics[width=1\textwidth]{images/toy_2/daregram.pdf}
   \caption{$(Z^TZ)^{-1}$}
    \label{fig:toy_DARE-GRAM}
\end{subfigure}

\caption{Illustration of the impact the inverse Gram operation in the OLS solution. (a) Assume that the features of the source and target follow two Gaussian distributions with slightly different mean and variance. (b) Under the representation subspace distance, the feature subspaces of $Z_s$ and $Z_t$ are well aligned with a very small angle difference. (c) However, because of the inverse Gram operation, the difference in terms of the inverse Gram matrix $(Z^TZ)^{-1}$ can still be large in terms of both angle and scale. }
\label{fig:Toy}
\vspace{-0.3cm}
\end{figure}

\subsection{Angle Alignment for Gram Matrix Inverse}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% My writing is just a draft of my understanding. Overwrite whenever possible.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Overview and motivation of the first term. Give some additional insights and tastes on why we align the inverse of the Gram matrix.
%%% For example, variance, and maybe also from the style transfer paper. And also a bit why the inverse is necessary. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first term in Equation~\ref{eq:zst} concerns the Gram matrix. The Gram matrix is sometimes regarded as a style representation as it calculates the correlations between the different features. It can also be seen as an unbiased empirical estimate of the MMD with a quadratic kernel \cite{gatys2016image}. The inverse operation is also essential because it first relates the variance of the unbiased estimator $\beta$ to the eigenvalues of $(Z^TZ)$. Particular attention must be paid to the small eigenvalues, which have a maximum inflationary effect on the variance of the least squares estimator by significantly destabilizing the estimator
when it approaches zero. Secondly, the ill-conditioned Gram matrix motivates using a low-rank inverse approximation \cite{chung2015optimal}, which allows obtaining regularised basis to be aligned for the source and target domain.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% First explain inverse is non-trivial and one solution is to do a pseudo-inverse.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
However, such an alignment is non-trivial because the Gram matrix can be non-invertible in deep learning models. During training, the batch size $b$ is generally smaller than the embedding dimension $p$. Given a feature matrix $Z\in \mathbb{R}^{b\times p}$, with $b<p$, the Gram Matrix $(Z^TZ)\in\mathbb{R}^{p\times p}$, has rank $r$ smaller or equal to $b$. Hence the Gram Matrix is not fully ranked and thus not invertible. The Moore-Penrose pseudo-inverse in this case can generalize the concept of matrix inverse when the matrix may not be invertible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Then motivate the selective part, removing the dimensions with the small eigenvalues can avoid numerical issues in the inverse. It fits the need of the pseudo-inverse. This also suppress the large terms in the inversed matrix, thus suppress the stand out values, similar to "http://proceedings.mlr.press/v97/chen19i/chen19i.pdf", 
%%% explain the selection procedure. and again , exaplain, why we do the selection along the way. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%In order to get a close similarity between $\hat{\beta_s}$ and $\hat{\beta_t}$, while having the smallest variances on both terms, 

We propose to consider only a selected subspace of the Gram matrix to solve this problem. As not all basis vectors contribute equally, the basis vectors with the highest eigenvalues are the most influential. Therefore, we only consider the most dominant basis vectors in the alignment process.
%We propose to only consider the most dominant dimensions in the Gram matrix with the largest eigenvalues to tackle this issue. Since all basis vectors do not contribute equally, the basis vectors with higher eigenvalues are the most influential. Therefore, we consider only the most dominant basis vectors in the alignment process. 
This step has two main objectives : (i) maximize the mutual information between the two distributions by considering only a selected subset, (ii) avoid numerical instability when not considering degenerate eigenspace.

Concretely, given the singular value decomposition (SVD)~\cite{van1996matrix} of the feature matrix $Z$ defined by $Z = UDV^T$. The Gram matrix $(Z^TZ)$, can be decomposed using the SVD of $Z$ as :
\vspace{-0.1cm}
\begin{equation}
\begin{split}
    (Z^TZ) = (UDV^T)^T(UDV^T) = V \Lambda V^T,\\
    \lambda_k := \Lambda_{k,k} = D_{k,k}^2 \quad \text{for} \quad k = 1,...,p.
    \end{split}
\end{equation}

\noindent where the orthogonal matrix $V \in \mathbb{R}^{p\times p}$ is identical to the matrix in the SVD of $Z$ and $\Lambda \in \mathbb{R}^{p\times p}$ is the diagonal matrix containing the squared eigenvalues of $Z$.

Given the ordered eigenvalues of the Matrix $(Z^TZ)$ $\lambda_1 \geq  ...\geq \lambda_k \geq  ... \geq \lambda_p \geq 0$, the Moore-Penrose pseudo-inverse~\cite{pseudo_inverse} can be derived by discarding the singular values that are below $\lambda_k$ and treating them as zero. The pseudo-inverse of $(Z^TZ)$ can be expressed as:

\begin{equation}
    (Z^TZ)^+ = V^T\Lambda^+V = V^T\left(\begin{array}{ccc|cc} 
    \frac{1}{\lambda_1} &&&&\\
    & \ddots &&&0 \\
    && \frac{1}{\lambda_{k}}\\
    \hline
    &&&  \\
    &0&&& 0 \end{array}\right)V
\end{equation}

The operation is equivalent to removing the dimensions with the largest singular value in the inverse matrix. This is in line with \cite{chen2019transferability} as it has been shown that penalizing high eigenvalues is beneficial in domain adaptation. 

The selection of $k$~(the number of principal components used) can be achieved through a threshold on the cumulative sum of the eigenvalues of $(Z^TZ)$. Since the smaller eigenvalues do not contribute significantly to the cumulative sum, the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded. Given $\lambda_s$ and $\lambda_t$ respectively the eigenvalues of the matrix $(Z_s^TZ_s)$ and $(Z_t^TZ_t)$, the goal is to find k, s.t.

\begin{equation}
    \frac{\sum_{i=0}^k \lambda_{s,i}}{\sum_{i=0}^p\lambda_{s,i}} > T \quad \text{and} \quad \frac{\sum_{i=0}^k \lambda_{t,i}}{\sum_{i=0}^p\lambda_{t,i}} > T ,
\end{equation}
\vspace{0.1cm}

\noindent where $T$ is a threshold controlling the proportion of explained variance by the first $k$ principal components.
In the following, the pseudo-inverse with respect to $k$ of the Gram matrix for source and target is denoted as $G_s^+ = (Z_s^TZ_s)^+$ and  $G_t^+ = (Z_t^TZ_t)^+$, respectively. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Finally shortly give the similarity form. Since it is nothing new, make it concise.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Following \cite{chen2021representation}, the cosine similarity is used to calculate the angle difference between source and target. Unlike previous methods, the angle calculation directly uses the column space of $G_s^+$ and $G_t^+$, forming a subspace of $\mathbb{R}^{p}$ spanned by the column vectors of $G_s^+$ and $G_t^+$. A direct measurement of the principal angles is defined as follows:
\vspace{-0.1cm}
\begin{equation}
    \cos(\theta_i^{S \leftrightarrow T}) = \frac{G_{s,i}^+ \cdot G_{t,i}^+}{\lVert G_{s,i}^+\lVert \cdot\lVert G_{t,i}^+\lVert}
    % \cos(\theta_2^{S \leftrightarrow T}) &= \frac{(Z_s^TZ_s)^+_2\cdot(Z_t^TZ_t)^+_2}{\lVert (Z_s^TZ_s)^+_2\lVert \cdot\lVert (Z_t^TZ_t)^+_2 \lVert},\\
    % & \qquad  \qquad  \vdots\\
    % \cos(\theta_p^{S \leftrightarrow T}) &= \frac{(Z_s^TZ_s)^+_p\cdot(Z_t^TZ_t)^+_p}{\lVert (Z_s^TZ_s)^+_p\lVert \cdot\lVert (Z_t^TZ_t)^+_p \lVert}
\end{equation}

\noindent where $i\in[1,p]$, and $G_{i}^+$ represent the $i$th column of the inverse Gram matrix $G^+$.  The cosine similarity between the span of the subspace for both the source and target feature are stored in $M = [\cos(\theta_1^{S \leftrightarrow T}),\dots, \cos(\theta_p^{S \leftrightarrow T}) ]$. %Every element of $M$ is a cosine similarity; thus, each row represents
% how one of the eigenvectors for the target feature vector is similar to every selected eigenvector of the source feature. Since cosine similarity is normalized between $1$ and $-1$, the closer the element is to 1, the more similar the two subspaces of target and source are aligned.
The loss to align the selected basis from the pseudo-inverse of the Gram matrix can be written as: 
\begin{equation}
    \mathcal{L}_{cos}(Z^S,Z^T) = \mynorm{\mathbb{I} -M}_1^1
    \label{eq:ismai}
\end{equation}
with $\mathbb{I}$ a vector of ones, of shape $p$. 
Minimizing the above term maximizes the cosine similarity between the source and target representation subspace by reducing the angle between the basis of both domains. %the selected elements to 1 by minimizing L1 loss between $M$ and $\boldsymbol{1}$. %Directly minimizing the cosine similarity introduced in Equation~(\ref{eq:ismai}) can reduce the distance between the source representation and the target representation subspace. 
\paragraph{Discussion}
%%%%%%%%%%% Some points in the discussion here to add 
%%%% more stable by the selective pseudo-inverse, the arguments you gave in the analysis, give some references here, for example, the longmingsheng paper on using the largest eigenvalue

%%%%%%%%%%% link to variance, another perspective to see it.

%%%%%%%%%% Difference to the RSD, we do alignment in the p*p space which is batch invariant

% First les sensitive to the batch size
%Second the algorithms in pytorch is not svd 

The proposed method is also more robust and stable compared to the direct feature alignment of $Z$ (\eg RSD~\cite{chen2021representation}). An important difference between RSD and DARE-GRAM lies on the choice of subspace for the alignment. RSD relies on the $U$ basis derived from the SVD decomposition of $Z$. However, the vectors $U$ are first, not unique for a matrix with repeated singular values and, secondly, may be numerically unstable since the gradient depends on $\frac{1}{\lambda_i-\lambda_j}$. %DARE-GRAM uses the algebraically defined pseudo-inverse and does not rely on the SVD returned basis. We have presented the pseudo-inverse in our method through SVD because it is more intuitive to illustrate the underlying concepts, but the algebraic details are beyond the scope of this paper. %Another point to note is that the number of basis for alignment in RSD grows linearly with the batch size. Our choice of basis for alignment, on the other side, always remains the same dimension and models only the domain-specific characteristics. This is because the inversed Gram matrix $G^+$ is a square matrix of dimension $p\times p$, where $p$ is equal to the number of features.  
 Morever, A drawback of RSD is that a large batch size $b \geq p$ can result in full space, causing the principal angles(RSD~\cite{chen2021representation}-Eq.2) between two subspace to become zero. In this case, no alignment can be performed by RSD. Our method does not have this drawback.
%Toy example


\subsection{Scale Alignment}
\label{ssec:Scale}
Preserving the source feature scale is critical in domain adaptive regression problems~\cite{chen2021representation}. In addition to the angle alignment presented in the previous section, we propose to explicitly align the scale of the target subspace to the source. 

More specifically, the scale of the matrix $Z$  can be estimated by 
%Eigenvectors carry plausibly critical information and were carefully examined in the previous section to align the two domains. However, it does not impose any similarity in scale between the source and target domains. Hence, we should also ensure that the matrices $Z_s$ and $Z_t$ are similar. 
its trace norm $\lVert Z \lVert_1 = \text{Tr}(\sqrt{Z ^TZ }) = \sum_{i=1}^N\sqrt{\lambda_i}$, where the last term is the sum of the singular values of $Z$. % and $\sigma_i^2=\lambda_i$. 
The scale of $Z$ is therefore the sum of the diagonal elements of the Gram-Matrix. The scale distance between source and target feature is regularized by minimizing the difference between the k-principal eigenvalues: 

%% Furthermore as show in Equation~(\ref{eq:lambda_square}), the eigenvalues of $(Z^TZ)$ are the squared eigenvalues of $Z$. We propose to penalize the variations between the sorted eigenvalues of the two domains. More precisely we minimize the distance between the k-principal eigenvalues for the source and target feature vector: 
\vspace{-0.5cm}
\begin{equation}
    \mathcal{L}_{scale}(Z^S,Z^T) = \lVert\lambda_{s,i={1,\dots,k}}-\lambda_{t,i={1,\dots,k}}\rVert_2 .
\label{eq:scale_alignement}
\end{equation}
%add l2 norm instead with correct notation and don't call it sim

%The proposed loss $\mathcal{L}_{scale}$ is a scalar-valued score for measuring high-dimensional distribution shifts.
%%%%%%%%%%%%%%


Unlike the previous methods~\cite{chen2021representation}, which explicitly avoid aligning the feature scale, the pseudo-inverse Gram columns that form the basis of our subspace are not necessarily orthonormal. Therefore, matching the source and target basis scale is also essential to complete the alignment process. As a note, the eigenvectors from the SVD decomposition are orthonormal, and the length of the vectors is fixed and set to one, as shown in Figure \ref{fig:Toy}(b). 


%%% discussion for our contribution here.

%%%%%%%%%%%%%%
%% A question to be answered here is that why are we aligning the scale of the features while RSD avoids it. 
%% This can be to some extent be demonstrated in the ablation study, so the remove one-term ablation is quite necessary here.
%%%%%%%%%%%%%%

\subsection{Overview}
 
Combining our angle alignment for the inverse gram and scale alignment, the total loss used for the end-to-end training can be written as:% total loss function minimizes the supervised regression loss on source data, our cosine similarity loss and the similarity loss. %Note that we do not employ any adversarial adaptation loss.

\begin{equation}
\begin{split}
     \mathcal{L}_{total}(Z^S,Z^T) = &\mathcal{L}_{src} + \alpha_{cos}\mathcal{L}_{cos}(Z^S,Z^T) + \\
     &\gamma_{scale}\mathcal{L}_{scale}(Z^S,Z^T),
\end{split}
\end{equation}
\noindent where $\alpha_{cos},\gamma_{scale}$ are hyper-parameters controlling the effect of the angle and scale alignment. An overall of our method is presented in  Figure \ref{fig:tab}.  %The algorithm is also presented in the Pseudocode form in Listing~\ref{}.

%We have shown how we bridge the distribution gap between domains by matching the angle of . Cautious readers may notice that we did not take the third term $y$ into account. The reason  If we assume that data from the two domains follow the same distribution in the label space, we can safely disregard the third term. In addition, $y_t$ is not available to us, leading us to only consider the Gram Matrix $(Z^TZ)$ to ensure that  $\boldsymbol{\hat{\beta_s}} \sim \boldsymbol{\hat{\beta_s}}$.

\begin{comment}
\begin{lstlisting}[language=Python,caption=Python code of DARE-GRAM loss based on PyTorch][mathescape]
def cum_sum_eig(X,T):
    _,eig, _ = torch.linalg.svd(X)
    cum_sum = torch.cumsum(eig, dim = 0)
    importance_eig = cum_sum/eig.sum() 
    k = torch.argwhere(importance_eig <=T)[-1]
    return eig, k

def gram_rep(X,T):
    b,p = X.shape
    ones_b = torch.ones(b,1)
    X = torch.cat((ones_b, X), 1) #add a one vector to have same consistency as OLS solution
    Gram = (X.t()@X)
    eig,k = cum_sum_eig(Gram,T)
    return Gram, eig, k, p

def DARE-GRAM_loss(Z_s,Z_t,T):
# Z_s, Z_t: embeddings representation for source and target respectively [b,p]
# T, Threshold for desired variance 
    Gram_s, eig_s, k_s, p_s = gram_rep(Z_s,T)
    Gram_t, eig_t, k_t, p_t = gram_rep(Z_t,T)
    k = max(k_s, k_t)[0] #to match the same number of dimensions 
    inv_Gram_s = torch.linalg.pinv(Gram_s ,rtol = (eig_s[k]/eig_s[0]))
    inv_Gram_t = torch.linalg.pinv(Gram_t ,rtol = (eig_t[k]/eig_t[0]))
    #Max cosine distance 
    cos_sim = nn.CosineSimilarity(dim=0,eps=1e-6)
    cos_dist = cos_sim(inv_Gram_s,inv_Gram_t)
    ones_p = torch.ones(p+1,1)
    cos = torch.dist(ones_p,cos_dist),p=1)/(p+1)
    #Min distance between the eigenvalues
    eig_dist = torch.dist(eig_s[:k],eig_s[:k])/k
    return cos, eig_dist
\end{lstlisting}

A natural question arises : \textit{Should we consider the matching of the Gram-Matrix or its inverse?}

Previously, \cite{risser2017stable} already identified instabilities during training, as different distributions result in the same MMD.

As highlighted in  \cite{kalischek2021light}, the Gram matrix violates the identity of indiscernible because the mapping of the polynomial kernel is not injective, and the distribution has no unique embedding in the reproducing kernel Hilbert space (RKHS).

DARE-GRAM first consider DAR problem as finding a feature space which can be sovled by a shared linear layer regressor. This formulation allows us to leverage the many procedures developed for parameter estimation and inference in linear regression and the presence of closed-form solutions. Motivated by the closed form solution of the Ordinary least squares (OLS) estimators to mitigate the gap between the source and target domains. The product of the batched feature of source and target $(X^TX)$ is denoted as the Gram matrix. Its inverse $(X^TX)^{-1}$, if it exists, is the co-factor matrix of the unbiased estimator. However, DNNs tend to have a large embedding dimension and are trained using a smaller batch size than the embedding dimension of the encoder. Hence, making the matrix $(X^TX)$ not fully ranked, thus not invertible. 
This work uses the subspaces (composed of k eigenvectors induced by an SVD~\cite{van1996matrix}), one for each domain, to compute MoorePenrose pseudo-inverse \cite{pseudo_inverse} matrix of $(X^TX)$, denoted as $(X^TX)^+$. Furthermore, we propose to align the feature distributions by maximizing the cosine similarity between the pseudo inverse $(X^TX)^+$ of two domains. Hence, we compute a cosine similarity matrix to measure how the most important k-eigenvectors of the target are similar, spatial-wise, to the most important k-eigenvectors of the source. Furthermore, to ensure the same measurement scale of $X$, for source and target reflected by the matrix $(X^TX)$, we also propose minimizing the L2 loss between the k most essential eigenvalues of both domains. 


\subsection{Motivation}

In general, regression models aim the predict the dependent variable $y_i$ as a function of the independent variables $\boldsymbol{x}_i$, with $e_{i}$ representing an additive error term or random statistical noise:

\begin{equation}
    y_i = f(\mathbf{x}_i) + \epsilon
\end{equation}

The goal is to best estimate the function $f(.)$ that closely fits the data. In deep Learning, the function $f(.)$ is usually represented as a deep neural network. One possible formulation of the problem could be :  
\begin{equation}
    y_i = f(\mathbf{x}_i)  = (g \circ h)(\mathbf{x}_i) +\epsilon
\label{eq:zab}
\end{equation}

Where $h(.)$ is the model encoder and $g(.)$ is a single linear network represented by the parameters $W$ in a matrix form.

The equation \ref{eq:zab} could be re-wrote into the following form: 
\begin{equation}
\begin{split}
    y_{i} = g(h(\mathbf{x}_i)) &= W_{0}+W _{1}h(\mathbf{x}_{i})_1+\cdots +W _{p}h(\mathbf{x}_{i})_p+\varepsilon _{i}= W_0 + \sum _{j=1}^{p}W_{j}h(\mathbf{x}_{i})_j \\
    & = h(x_i)^{\mathsf {T}}{\boldsymbol {W }}+\varepsilon _{i} = \mathbf{z}_i^{\mathsf {T}}{W} + \varepsilon _{i}\qquad i={1,\ldots ,n}
\end{split}
\end{equation}
\noindent where $\mathbf{z}_i^T W$ is the inner product between vectors $\mathbf{z}_i$ and $W$.

Given n embedding samples ${z_{i,1},...,z_{i,p}}^n_{i=1}$ of $p$ dimensions, the matrix notation of the problem become:

\begin{equation}
    y = ZW + \epsilon
\end{equation}

With: 
\begin{equation}
    Z=\begin{pmatrix} \mathbf {z} _{1}^{\mathsf {T}}\\\mathbf {z} _{2}^{\mathsf {T}}\\\vdots \\\mathbf {z} _{n}^{\mathsf {T}}\end{pmatrix} =\begin{pmatrix}1&z_{11}&\cdots &z_{1p}\\1&z_{21}&\cdots &z_{2p}\\\vdots &\vdots &\ddots &\vdots \\1&z_{n1}&\cdots &z_{np}\end{pmatrix} \quad 
    \text{and}
     \quad  \boldsymbol {W} =\begin{pmatrix}W _{0}\\W _{1}\\W _{2}\\\vdots \\W _{p}\end{pmatrix} 
\end{equation}

In Ordinary least squares, Z is usually considered as an overdetermined system, $n>>p$, and the closed-form expression for the estimated value of the unknown parameter vector $\beta \in \mathbb{R}^p$:

\begin{equation}
    \hat{\beta} = (Z^TZ)^{-1}X^Ty
\end{equation}

In Unsupervised domain adaptation (UDA), we are usually access to a labeled source domain, denoted as $\mathcal{D}_S = \{(\mathbf {x}_{i,s},y_{i,s})\}_{i=1}^{n_s}$ with $n_s$ the number of labeled samples, and to an unlabeled target domain, denoted as  $\mathcal{D}_T = \{(\mathbf {x}_{i,t})\}_{i=1}^{n_t}$, with $n_t$ is the number of unlabeled sampled. Both domains are sampled from the different underlying distributions. Given the deep representation learned by the feature extractor h(.) from source $\mathbf {z}_{i,s}$ and for target $\mathbf {z}_{i,t}$. 

We assume in order to lower the generalization error in regression tasks and to mitigate the domain gap between the source and target that, the unbiased estimator should be similar for both domains  $\hat{\beta_s} \sim \hat{\beta_t}$ where :
\begin{equation}
\begin{split}
    \hat{\beta_s} &= (Z_s^TZ_s)^{-1}Z_s^Ty_s  \\
    \hat{\beta_t} &= (Z_t^TZ_t)^{-1}Z_t^Ty_t
\end{split}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%
% Another issue to be addressed is that if we want to put emphasis on the Gram matrix XTX or the so called co-factor matrix (XTX)^(-1). This is be dependent on our explanation on why our method make sense. But this choice will also be an important 
%%%%%%%%%%%%%%%%%%%%%%%%%

Whoever in unsupervised domain adaptation, $y_t$ is not available to us. During training, a batch of $b$ input samples is fed to the model, usually $b<<p$, resulting in a non-invertible matrix. 

Given the singular value decomposition (SVD) of a data matrix $Z \in \mathbb{R}^{b\times p}$ defined by $Z = UDV^T$. The second moment $Z^TZ \in \mathbb{R}^{p\times p}$, also referred to as the Gram matrix,  is equal to the empirical covariance if the columns of $Z$ are mean-centered and can be decomposed using the SVD of $Z$ as:

\begin{equation}
\begin{split}
    Z^TZ &= (UDV^T)^T(UDV^T) \\
    &=VD^TDV^T \\
    &= V \Lambda V^T
\end{split}
\end{equation}

Where the orthogonal matrix $V  \in \mathbb{R}^{p\times p}$ is identical to the matrix in the SVD of $Z$ and
$\Lambda \in \mathbb{R}^{p\times p}$ is the diagonal matrix containing the eigenvalues:
\begin{equation}
    \lambda_k := \Lambda_{k,k} = D_{k,k}^2 \quad \text{for} \quad k = 1,...,p
\end{equation}

The ordered Eigenvalues are :
\begin{equation}
    \lambda_1 \geq \lambda_2 \geq ... \geq \lambda_p \geq 0
\end{equation}

Since $b<<p$, the matrix $Z^TZ$ is not fully ranked, meaning that one or more of the smaller eigenvalues get very close or become exactly equal to $0$ under such situations. The variance of the unbiased estimator $\hat{\beta}$ can be expressed as the inverse of the eigenvalues of $Z^TZ$. Hence, small eigenvalues have the maximum inflation effect on the variance of the least squares estimator, thereby destabilizing the estimator significantly when they are close to $0$. One approximation of the inverse of the Gram matrix is to use the Moore-Penrose pseudo-inverse that can be defined as :
\begin{equation}
    (Z^TZ)^+ = V^T\Lambda^+V = V^T\left(\begin{array}{ccc|cc} \frac{1}{\lambda_1} &\\
    & \ddots &&&0 \\
    && \frac{1}{\lambda_{r}}\\
    \hline
    &&&  \\
    &0&&& 0 \\
    &&&& \end{array}\right)V
\end{equation}

Where $\Lambda^+$ is formed from $\Lambda$ by taking the reciprocal of the first r non-zero eigenvalues, with  $r$ the rank of the normal matrix and $r = min(b,p)$. 

Proposed approach:

\subsection{Least-square Domain Adaptation (LSDA)}

In order to get a close similarity between $\hat{\beta_s}$ and $\hat{\beta_t}$, while having the smallest variances on both terms, we first propose to match the portion of the base formed by $(Z_s^TZ_s)$ and $(Z_t^TZ_t)$ that is the most representative, spanning the subspace for both source and target. This is possible by choosing $k$ largest eigenvalues associated with the Gram matrix in each mini-batch for the source and target feature representation. Consequently, when computing the Moore-Penrose pseudo-inverse, only the $k \in \{1,...,r\}$ eigenvalues are kept, and the others are set to 0. 

The potential dimension reduction may be achieved by choosing $k$, the number of principal components used, through an appropriate threshold on the cumulative sum of the eigenvalues of $(Z^TZ)$. Since the smaller eigenvalues do not contribute significantly to the cumulative sum, the corresponding principal components may be continued to be dropped as long as the desired threshold limit is not exceeded. Given $\lambda_s$ and $\lambda_t$ respectively the eigenvalues of the matrix $(Z_s^TZ_s)$ and $(Z_t^TZ_t)$, the goal is to find k such as :

\begin{equation}
    \frac{\sum_{i=0}^k \lambda_{i,s}}{\sum_{i=0}^r\lambda_{i,s}} > T \quad \text{and} \quad \frac{\sum_{i=0}^k \lambda_{i,t}}{\sum_{i=0}^r\lambda_{i,t}} > T 
\end{equation}

With $T$, a threshold explaining the proportion of variation explained by the first $k$ principal components.

For sake of simplicity we will refer to the pseudo-inverse of $(Z_s^TZ_s)$ and $(Z_t^TZ_t)$ with respect to $k$ to $\Sigma_{k,s}^+$ and $\Sigma_{k,t}^+$.

The next step is to compute a cosine similarity matrix between the pseudo-inverse with respect to the $k$ $\Sigma_{k}^+$ of the target features and source features.

\begin{equation}
    M = \frac{\Sigma_{k,s}^{+T}.\Sigma_{k,t}^+}{\Vert\Sigma_{k,s}^+\Vert_2.\Vert\Sigma_{k,t}^+\Vert_2}
\end{equation}

$\Vert\Sigma_{k,s}^+\Vert_2$ and $\Vert\Sigma_{k,t}^+\Vert_2$ are L2-norms of $\Sigma_{k,s}^+$ and  $\Sigma_{k,t}^+$ with respect to the feature dimension. Therefore, $\Sigma_{k,s}^+$ still has the shape of $[p\times p]$, which shows the cosine similarity between the span of the subspace for both the source and target feature. Every element of $M$ is a cosine similarity; thus, each row represents
how one of the eigenvectors for the target feature vector is similar to every selected eigenvector of the source feature. Since cosine similarity is normalized between $1$ and $-1$, the closer the element is to 1, the more similar the two subspaces of target and source are aligned. 

This approach is to have a more robust and stable training since we do not backpropagate the gradient on the very small eigenvalues of the ill-conditioned Gram matrix.

\begin{equation}
    \mathcal{L}_{cos}(Z^t,Z^s) = \mynorm{M-\boldsymbol{1}}_1^1
    \label{eq:ismai}
\end{equation}

We maximize the selected elements to 1 by minimizing L1 loss between $M$ and $\boldsymbol{1}$. Furthermore, because our method compares all the vectors from the target matrix $\Sigma_{k,t}^+$ with every vector from the source matrix $\Sigma_{k,s}^+$ source, it can learn a representation dominated by very few principal components and also maximize the similarity of target features to the source features, making the target feature distribution and the source feature distribution aligned in the feature space.

Directly minimizing the cosine similarity introduced in \ref{eq:ismai} can reduce the distance between the source representation and the target representation subspace. It only focuses on the $k$ principal components. This is necessary but not sufficient. In order to get a close similarity between $\hat{\beta_s}$ and $\hat{\beta_t}$, we should also ensure that the matrices $(Z_s^TZ_s)$ and $(Z_t^TZ_t)$ are also similar. To force the similarity between the covariance matrices of the source and target features is to compare their trace since the two matrices are similar if their trace is similar. 

Since the trace of matrix $A$ is the sum of the diagonal element of the same matrix and is equal to the sum of eigenvalues of $A$, we aim to minimize the distance between the diagonal element of the matrix $(Z_s^TZ_s)$ and $(Z_t^TZ_t)$ : 

\begin{equation}
    \mathcal{L}_{sim}(Z^t,Z^s) = \mynorm{diag(Z_s^TZ_s)-diag(Z_t^TZ_t)}_2^2
\end{equation}

The total loss function minimizes the regression loss along with our cosine similarity loss and the similarity loss. Note that we do not employ any adversarial adaptation loss.


\begin{equation}
    \mathcal{L}_{total}(Z^t,Z^s) = \mynorm{g(Z_s)-y_s}_2^2 + \lambda_{cos}\mathcal{L}_{cos}(Z^t,Z^s)+ \lambda_{sim}\mathcal{L}_{sim}(Z^t,Z^s)
\end{equation}

\begin{algorithm}
	\caption{LSDA} 
	\begin{algorithmic}[1]
	\State \textbf{Input} : $Z_s\in \mathbb{R}^{p\times p}$ \text{and} $Z_t\in \mathbb{R}^{p\times p}$
	\State \textbf{Output} : $\mathcal{L}_{cos}$ \text{and} $\mathcal{L}_{sim}$
	\State Zmean$_{s}$ = $Z_s$.mean(dim=0)
	
		\For {$iteration=1,2,\ldots$}
			\For {$actor=1,2,\ldots,N$}
				\State Run policy $\pi_{\theta_{old}}$ in environment for $T$ time steps
				\State Compute advantage estimates $\hat{A}_{1},\ldots,\hat{A}_{T}$
			\EndFor
			\State Optimize surrogate $L$ wrt. $\theta$, with $K$ epochs and minibatch size $M\leq NT$
			\State $\theta_{old}\leftarrow\theta$
		\EndFor
	\end{algorithmic} 
\end{algorithm}

\subsection{Subspace generation}

Even though both the source and target data lie in the same $D$-dimensional space, they have been drawn according to different marginal distributions. Consequently, rather than working on the original data themselves, we suggest to handle more robust representations of the source and target domains and to learn the shift between these two domains. First, we transform every source and target data in the form of a $D$-dimensional z-normalized vector. Then, using PCA, we select for each domain $d$ eigenvectors corresponding to the $d$ largest eigenvalues. These eigenvectors are used as bases of the source and target subspaces.


https://arxiv.org/pdf/2010.03978.pdf

Subspace-based adaptation aims to discover a common intermediate representation that is shared between domains.
Many techniques have been proposed to construct this representation from the low-dimensional representation of the
source and target data. Most of the adaptation approaches in this category first create a low-dimensional representation of original data in the form of a linear subspace for each domain, and then reduce the discrepancy between the
subspaces to construct the intermediate representation. A dimensionality reduction technique such as principal component analysis (PCA) can be used to construct the subspaces as two points, one for each domain, in a low-dimensional Grassmann manifold. The distance between the points in Grassmann manifold indicates the domain shift which can
be reduced by applying different methods. Goplan et al. [26], proposed Sampling Geodesic Flow (SGF) that first
finds a geodesic path between the source and target points On a Grassmann manifold and then samples a set of points,
subspaces, including the source and the target points along this path. In the next step, the data from both domains are
projected onto all sampled subspaces along the geodesic path and will be concatenated to create a high-dimensional
vector. Finally, A discriminative classifier can learn from the source projected data to classify the unlabeled samples.
Sampling more points from the geodesic path would help to map the source subspace into the target subspace more
precisely. However, sampling more subspaces extends the dimensionality of the feature vector, which makes this
technique computationally expensive. Geodesic Flow Kernel [45], was proposed to extend and improve SGF. GFK is
a kernel-based domain adaptation method that deals with shift across domains. It aims to represent the smoothness
of transition from a source to a target domain by integrating an infinite number of subspaces to find a geodesic line
between domains in a low-dimensional manifold space. Fernando et al. [46], proposed a subspace alignment technique (SA) to directly reduce the discrepancy between domains by learning a linear mapping function that projects
the source point directly into the target point in the grassmann manifold. The projection matrix M can be learned by
minimizing the divergence in the Bergman matrix:
M = argmin ||XSM  XT ||2
F = XT
S XT , (8)
where XS, XT are the low-dimensional representation, bases vectors, of The source and the target data in the Grassmann manifold respectively, and ||.||2
F
is the Frobenius norm. SA only aligns the subspace bases and ignores the difference between subspace distributions. Subspace distribution alignment (SDA) [47], extends the work in SA by aligning
both subspace distributions and the bases at the same time. In SDA the projection matrix M can be formulated as
M = XT
S XT Q, where Q is a matrix to align the discrepancy between distributions

\end{comment}

\section{Experiments}
\label{sec:Experiments}

\subsection{Experimental setup}
\label{sec:Experimental setup}

We evaluate our proposed method on three domain adaptations for regression benchmark datasets:  dSprites~\cite{dsprites17}, 
MPI3D~\cite{NEURIPS2019_d97d404b} and  Biwi Kinect \cite{fanelli_IJCV}. 

\noindent\textbf{dSprites} \cite{dsprites17}  is a synthetic 2D dataset generated from five ground truth independent latent factors.
%, detailed in Table~\ref{tab:dSprites_datast}
Following common practice~\cite{chen2021representation}, we treat the three variants of the datasets as three different domains. They are generated by adding Color (\textbf{C}) or background noise such as Scream (\textbf{S}) and Noise (\textbf{N}), shown in Figure \ref{fig:Dsprites}. These three domains comprise 737,280 images each. Dsprites can be used as a benchmark for regression domain adaptation, especially if we consider scale, position X, and Y. Similarly to the setup in \cite{chen2021representation}, the orientation factor is excluded from consideration. We evaluate all methods on the three sub-regression tasks  on six adaptation directions: \textbf{C~$\rightarrow$~N}, \textbf{C~$\rightarrow$~S}, \textbf{N~$\rightarrow$~C}, \textbf{N~$\rightarrow$~S}, \textbf{S~$\rightarrow$~C}, and \textbf{S~$\rightarrow$~N}.% Results are provided in sum of MAE on the three regression tasks. 

\begin{figure}[h!]
\centering
\begin{subfigure}{0.14\textwidth}
    \includegraphics[width=\textwidth]{images/Dsprites/noise.png}   \caption{Noise}
    \label{fig:noise}
\end{subfigure}
\hspace{1em}%
\begin{subfigure}{0.14\textwidth}
    \includegraphics[width=\textwidth]{images/Dsprites/color.png}
    \caption{Color}
    \label{fig:Color}
\end{subfigure}
\hspace{1em}%
\begin{subfigure}{0.14\textwidth}
\includegraphics[width=\textwidth]{images/Dsprites/scream.png}
    \caption{Scream}
    \label{fig:Scream}
\end{subfigure}
\hfill
\vspace{-0.3cm}
\caption{Sample example of different domains in dSprites.}
\label{fig:Dsprites}
\vspace{-0.3cm}
\end{figure}
\begin{comment}
\begin{table}[h]
  \centering
  \begin{tabular}{ c | c | c }
    \hline
    Factor & Values & Task\\
    \hline
    Scale & 6 values in [0.5, 1] & Regression \\
    Orientation & 40 values in [0, 2$\pi$] & Regression \\
    Position X & 32 values in [0, 1] & Regression\\
    Position Y & 32 values in [0, 1] & Regression \\
    \hline\noalign{\smallskip}
  \end{tabular}
  \caption{Latent factor values on dSprites}
  \label{tab:dSprites_datast}
\end{table}
\end{comment}

\noindent\textbf{MPI3D} \cite{NEURIPS2019_d97d404b} is a benchmark dataset that consists of 1,036,800 examples of 3D objects from three different domain : Toy (\textbf{T}), RealistiC (\textbf{RC}) and ReaL (\textbf{RL}), as shown in Figure \ref{fig:MPI3D}. This real-world robotics dataset allows the investigation of the domain gap between real data and simulated ones. This dataset was recorded in a controlled environment, defined by seven factors of variation such as object color, shape, size and position, camera height, background color, and two degrees of freedom of motion of a robotic arm %(Table \ref{tab:MPI3D_datast}). 
The task is to predict these intrinsic factors from the input image. For this paper, we evaluate our method on six transfer tasks: \textbf{RL~$\rightarrow$~RC}, \textbf{RL~$\rightarrow$~T}, \textbf{RC~$\rightarrow$~T},\textbf{RC~$\rightarrow$~RL}, \textbf{T~$\rightarrow$~RL} and \textbf{T~$\rightarrow$~RC}. We only considered the two regression tasks, rotation about a vertical and horizontal axis.
    
\begin{figure}[h]
\centering
\begin{subfigure}{0.14\textwidth}
    \includegraphics[width=\textwidth]{images/mpi3d/toy.png}   \caption{Toy}
    \label{fig:TOY}
\end{subfigure}
\hspace{1em}%
\begin{subfigure}{0.14\textwidth}
    \includegraphics[width=\textwidth]{images/mpi3d/realistic.png}
    \caption{Realistic}
    \label{fig:Realistic}
\end{subfigure}
\hspace{1em}%
\begin{subfigure}{0.14\textwidth}
\includegraphics[width=\textwidth]{images/mpi3d/real.png}
    \caption{Real}
    \label{fig:REAL}
\end{subfigure}
\hfill
\vspace{-0.3cm}
\caption{Sample example of different domains in MPI3D.}
\label{fig:MPI3D}
\vspace{-0.3cm}
\end{figure}
\begin{comment}
\begin{table}[h]
  \centering
  \begin{tabular}{ c | c | c }
    \hline
    Factor & Values & Task\\
    \hline
    Horizontal Axis & 40 values in $[0, 39]$ & Regression \\
    Vertical Axis & 40 values in $[0, 39]$ & Regression \\
    \hline\noalign{\smallskip}
  \end{tabular}
  \caption{Variations Factors in MPI3D}
  \label{tab:MPI3D_datast}
\end{table}
\end{comment}

\noindent\textbf{Biwi kinect} \cite{fanelli_IJCV} is a real-word dataset containing over 15K images of 20 people, 6 Females (\textbf{F}) with 5874 images and 14 Males (\textbf{M}) with 9804 images, recorded with a Microsoft Kinect sensor while turning their heads around freely. The example images are shown in Figure \ref{fig:BIWIKINECT}. The three factors of variations used to evaluate our method are yaw, pitch, and roll angles. 
%The details of the variations factors are shown in Table~\ref{tab:biwi_kinet}. 
We evaluate our method on two transfer tasks: \textbf{M~$\rightarrow$~F} and \textbf{F~$\rightarrow$~M}.

\begin{comment}
\begin{figure}[h]
\centering
\begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/biwi/male.png}   \caption{Male}
    \label{fig:Male}
\end{subfigure}
\hspace{1em}%
\begin{subfigure}{0.22\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/biwi/female.png}
    \caption{Female}
    \label{fig:Female}
\end{subfigure}
\caption{Sample example of different domains in Biwi kinect.}
\label{fig:BIWIKINECT}
\end{figure}
\end{comment}

\begin{figure}[h]
\centering
\begin{subfigure}{0.22\textwidth}
{\includegraphics[width=0.45\textwidth]{images/biwi/male_1.png}}
{\includegraphics[width=0.45\textwidth]{images/biwi/male_2.png}}%
    \caption{Male}
    \label{fig:Male}
\end{subfigure}
\begin{subfigure}{0.22\textwidth}
  {\includegraphics[width=0.45\textwidth]{images/biwi/femal_1.png}}
{\includegraphics[width=0.45\textwidth]{images/biwi/female_2.png}}%
    \caption{Female}
    \label{fig:Female}
\end{subfigure}
\vspace{-0.3cm}
\caption{Sample example of different domains in Biwi kinect}
\label{fig:BIWIKINECT}
\vspace{-0.3cm}
\end{figure}
\begin{comment}
\begin{table}[h]
  \centering
  \begin{tabular}{ c | c | c }
    \hline
    Factor & Values & Task\\
    \hline
    Yaw & Values in [-92.044,231.352]   &  Regression\\
    Pitch & Values in [-87.7066,246.684] & Regression \\
    Roll angle & Values in [754.182,1297.45] & Regression \\
    \hline\noalign{\smallskip}
  \end{tabular}
  \caption{Variations Factors in Biwi Kinect}
  \label{tab:biwi_kinet}
\end{table}
\end{comment}

\noindent\textbf{Evaluation metrics.} Following previous works~\cite{chen2021representation,Li_2021_CVPR}, Mean Absolute Error (MAE) is used as our evaluation metric across all the regression tasks. Each experiment is repeated three times, and the average results are reported.
\begin{table*}[ht!]
\centering
\small
\setlength{\tabcolsep}{0.05\columnwidth}
\begin{tabular}{lcccccc|c}
\toprule
Method                      & C $\rightarrow$ N & C $\rightarrow$ S & N $\rightarrow$ C & N $\rightarrow$ S & S $\rightarrow$ C & S $\rightarrow$ N  & Avg           \\
\midrule
Resnet-18  \cite{https://doi.org/10.48550/arxiv.1512.03385}                 & 0.94              & 0.90              & 0.16              & 0.65              & 0.08              & 0.26             & 0.498          \\
TCA    \cite{pan2010domain}                   & 0.94               & 0.87              & 0.19              & 0.66              & 0.10              & 0.23             & 0.498 \\
MCD     \cite{saito2018maximum}                & 0.81              & 0.81              & 0.17              & 0.65              & 0.07              & 0.19             & 0.450          \\
JDOT \cite {courty2017joint}  &0.86 & 0.79 & 0.19 & 0.64 & 0.10 & 0.23 & 0.468 \\
AFN     \cite {xu2019larger}                 & 1.00              & 0.96              & 0.16              & 0.62              & 0.08              & 0.32              &0.523          \\
DAN     \cite{long2015learning}                  & 0.70              & 0.77              & 0.12              & 0.50              & 0.06              & 0.11              &0.377          \\
DANN     \cite{ganin2016domain}                   & 0.47              & 0.46              & 0.16              & 0.65              & \textbf{0.05}     & 0.10              &0.315          \\
RSD    \cite{chen2021representation}.                    & 0.31              & 0.31              & 0.12              & 0.53 & 0.07              & 0.08              & 0.237          \\
\textbf{DARE-GRAM (ours)} &\textbf{0.30} & \textbf{0.20} & \textbf{0.11} & \textbf{0.25} & \textbf{0.05} & \textbf{0.07} & \textbf{0.164}\\
\bottomrule
\end{tabular}
\caption{Comparisons with previous works on the dSprites regression tasks. All results are shown in sum of MAE with the ResNet-18.}
\label{tab:results_dsprites}
\end{table*}

\begin{table*}
\centering
\small
\setlength{\tabcolsep}{0.038\columnwidth}
\begin{tabular}{lcccccc|c}
\toprule
Methods                     & RL $\rightarrow$ RC & RL $\rightarrow$ T & RC $\rightarrow$ RL & RC $\rightarrow$ T & T $\rightarrow$ RL & T $\rightarrow$ RC & Avg\\
\midrule
Resnet-18  \cite{https://doi.org/10.48550/arxiv.1512.03385}                 & 0.17                & 0.44               & 0.19                & 0.45               & 0.51                                       & 0.50               & 0.377          \\
TCA  \cite{pan2010domain}                 & 0.17                & 0.42               & 0.19                & 0.42              & 0.50                                       & 0.50               & 0.373          \\
MCD      \cite{saito2018maximum}                   & 0.13                & 0.40               & 0.15                & 0.45               & 0.52                                       & 0.50               & 0.358          \\
JDOT        \cite {courty2017joint}                  & 0.16                & 0.41               & 0.16                & 0.41               & 0.47                                       & 0.47               & 0.353         \\
AFN   \cite {xu2019larger}                     & 0.18                & 0.45               & 0.20                & 0.46               & 0.53                                       & 0.53              & 0.390          \\
DAN     \cite{long2015learning}                    & 0.12                & 0.35               & 0.12                & 0.27               & 0.40                                       & 0.41               & 0.278          \\
DANN    \cite{ganin2016domain}                     & 0.09                & 0.24               & 0.11                & 0.41               & 0.48                                       & 0.37               & 0.283          \\
RSD       \cite{chen2021representation}.                  & \textbf{0.09}                & 0.19               & \textbf{0.08}               & 0.15               & 0.36                                       & 0.36               & 0.205          \\
\textbf{DARE-GRAM (ours)} & \textbf{0.09} & \textbf{0.15} & 0.10 & \textbf{0.14} & \textbf{0.24} & \textbf{0.24} & \textbf{0.160}\\
\bottomrule
\end{tabular}
\caption{Comparisons with related works on the MPI3D regression tasks. All results are shown in sum of MAE with the ResNet-18.}
\label{tab:results_MPI3D}
\end{table*}

%A_df[(A_df['tradeoff'] == 0.5) & (A_df['tradeoff_2'] == 0.001) & (A_df['threshold'] == 0.999)] t->rc & t->rl

% df[(df['tradeoff'] == 0.06) & (df['tradeoff_2'] == 0.001) & (df['threshold'] == 0.96)] for rl->t , rc->t
 
\noindent\textbf{Implementation Details.} A pre-trained ResNet-18 \cite{https://doi.org/10.48550/arxiv.1512.03385} on ImageNet is used as the backbone for all methods. For all the experiments, the different tasks share the same encoder but a separated single linear regressor with a Sigmoid activation function. The source and target labels were scaled in the range $[0, 1]$ to eliminate the effects of diverse scales in regression values. We use the SGD~\cite{https://doi.org/10.48550/arxiv.1609.04747} optimizer with
a momentum of 0.9. The weight decay is set to $1e^{-3}$ for the loss optimization. The newly added layers are trained with a learning rate ten times that of the pre-trained layers, which is initialized to 	$\eta_0 = 1e^{-2}$. 
We further adopt the same learning rate scheduler $\eta = \eta_0 \cdot(1 + 0.0001 \cdot p)^{-0.75}$ as \cite{ganin2015unsupervised,long2018conditional}, where $p$ is the number of iterations changing from 0 to the maximum number of iterations. The images are resized to 224  224 and concatenated into batches of size $b=36$. The number of iterations was set as in \cite{chen2021representation} to 20,000, 10,000, and 1,500 iterations for dSprites, MPI3D, and Biwi Kinect, respectively. These setup choices are identical to RSD~\cite{chen2021representation}. An NVIDIA RTX 3090 GPU was used for all the experiments. 

\noindent\textbf{Compared Methods}  We compare our method with a range of adaptation methods: (i) Domain Adaptation via Transfer Component Analysis (\textbf{TCA})~\cite{pan2010domain}
(ii) Maximum Classifier Discrepancy (\textbf{MCD}) \cite{saito2018maximum}
(iii) Joint Distribution Optimal Transportation for Domain Adaptation (\textbf{JDOT}) \cite {courty2017joint} (iv) Adaptive Feature Norm (\textbf{AFN}) \cite {xu2019larger}
(v) Deep Adaptation Network (\textbf{DAN}) \cite{long2015learning}
(vi) Deep Adaptation Neural Network (\textbf{DANN}) \cite{ganin2016domain} and (vii) Representation Subspace Distance for Domain Adaptation Regression (\textbf{RSD}) \cite{chen2021representation}.

%Please add a sentence to tell that we took the results from rsd paper.

\subsection{Results}
\begin{comment}
\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/discussion/rsd_embeddings.png}
    \caption{RSD}
    \label{fig:second}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/discussion/ours_embeddings.png}
    \caption{DARE-GRAM (ours)}
    \label{fig:third}
\end{subfigure}
\caption{PCA Projection of embedding from different techniques on the dSprites data set \textbf{C~$\rightarrow$~S}.}
\label{fig:test_image_1}
\end{figure}
\end{comment}


% df[(df['tradeoff'] == 0.05) & (df['tradeoff_2'] == 0.001) & (df['threshold'] == 0.97)] for c->n , c->s

% df[(df['tradeoff'] == 0.05) & (df['tradeoff_2'] == 0.001) & (df['threshold'] == 0.96)] for n->s,n->c

% df[(df['tradeoff'] == 0.01) & (df['tradeoff_2'] == 0.0001) & (df['threshold'] == 0.999)] for s->n , s->c

\noindent\textbf{Evaluation on dSprites:} As shown in Table \ref{tab:results_dsprites}, our model achieves the best performance among all competing methods. Specifically, our method outperforms the previous state-of-the-art regression-based method RSD\cite{chen2021representation} by $30.8\%$ in terms of average MSE over all directions. %The different tasks on this data set were quite easy to perform. However, 
On the three difficult adaptation directions \textbf{C~$\rightarrow$~N}, \textbf{C~$\rightarrow$~S}, \textbf{N~$\rightarrow$~S}. \textbf{DARE-GRAM} also improves the performance over the RSD. The improvement is especially significant on the direction \textbf{C~$\rightarrow$~S} and \textbf{N~$\rightarrow$~S}. The improvement is by $33.3\%$ and $52.8\%$, respectively. %While being in part with \cite{chen2021representation} on the task \textbf{C~$\rightarrow$~N}. 
\noindent\textbf{Evaluation on MPI3D:} We further evaluate the effectiveness of our method on this more complex simulation-real data set. As shown in Table \ref{tab:results_MPI3D}, in average, our method outperforms all previous methods. The improvement over the previous state-of-the-art RSD is more than 21.9\%.%our model performs notably better than other domain adaptation techniques on all the scenarios except for \textbf{RC~$\rightarrow$~RL}, where our method is more competitive than previous domain-adaptation techniques expect RSD.
The improvement is especially significant in the four hard adaptation directions \textbf{T~$\rightarrow$~RL}, \textbf{T~$\rightarrow$~RC}, \textbf{RL~$\rightarrow$~T}, and \textbf{RC~$\rightarrow$~T}. The performance is comparable with RSD on the \textbf{RC/RL} pair. This might be because the domain gap is relatively small between the pair and the performance~($\approx 0.1$) could be close to saturation. 


\noindent\textbf{Evaluation on Biwi Kinect:} Given the much smaller size of the dataset (15,000 images compared to the number of samples in the scale of millions in the other two datasets), the Biwi Kinect regression task is particularly challenging. Additionally, the high imbalance between the two domains and the lack of separation of training and testing sets makes it more difficult and closer to real-world scenarios for DAR. The results reported in Table \ref{tab:results_biwi} demonstrate that our model can also consistently improve over previous methods on both directions on this more challenging task. 

The performance improvement on the three datasets of very different natures demonstrates the effectiveness of our proposed method.

\begin{table}
\centering
\small
\begin{tabular}{lcc|c}
\toprule
Method   & M $\rightarrow$ F & F $\rightarrow$ M & Avg\\
\midrule
Resnet-18 \cite{https://doi.org/10.48550/arxiv.1512.03385} & 0.29 & 0.38 & 0.335\\
TCA \cite{pan2010domain} & 0.31 & 0.39 & 0.350 \\
MCD \cite{saito2018maximum} & 0.31 & 0.37 & 0.340 \\
JDOT \cite {courty2017joint} & 0.29 & 0.39  & 0.340 \\
AFN \cite {xu2019larger} & 0.32 & 0.41 & 0.365 \\
DAN \cite{long2015learning} & 0.28 & 0.37 & 0.325 \\
DANN \cite{ganin2016domain}  & 0.30 & 0.37 & 0.335 \\
RSD \cite{chen2021representation} & 0.26 & 0.30 & 0.280 \\
\textbf{DARE-GRAM (ours)} & \textbf{0.23} & \textbf{0.29} & \textbf{0.260} \\
\bottomrule
\end{tabular}
\caption{Comparisons with previous works on Biwi Kinect.}
\label{tab:results_biwi}
\vspace{-0.5cm}
\end{table}

\begin{comment}
    
\begin{figure*}
\centering
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/discussion/resnet_18_embeddings.png}
    \caption{Resnet-18}
    \label{fig:Resnet-18_emb}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/discussion/rsd_embeddings.png}
    \caption{RSD}
    \label{fig:rsd_emb}
\end{subfigure}
\hfill
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{cvpr2023-author_kit-v1_1-1/latex/images/discussion/ours_embeddings.png}
    \caption{DARE-GRAM (ours)}
    \label{fig:ours_emb}
\end{subfigure}
        
\caption{PCA Projection of embedding from different techniques on the dSprites data set \textbf{C~$\rightarrow$~S}.}
\label{fig:pca}
\end{figure*}
\end{comment}

\subsection{Discussion and Analysis}
To provide more insights on the proposed Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices, we provide a detailed analysis of the different components of the methodology. %perform extensive analytical experiments and ablations to verify our method's effectiveness further.

\noindent\textbf{Angle Alignment and Scale Alignment} In the first ablation, we study the impact of our angle alignment on the inverse Gram matrix and our scale alignment on the eigenvalues. The $\textbf{C~$\rightarrow$~S}$ and $\textbf{N~$\rightarrow$~S}$ tasks in dSprites are used for the ablation here. As shown in Table~\ref{tab:loss_ablataion}, both components of the proposed methodology are able to improve over the baseline. Minimizing the angle between the pseudo-inverse of the gram Matrix can reduce the MSE over the source-only baseline by 70\% on the $\textbf{C~$\rightarrow$~S}$ task. We also compare the alternate angle alignment on the Gram matrix and truncated Gram matrix without considering the inverse. The MAE in both cases is significantly worst than our inverse version. This demonstrates the significant impact of the inverse operation~(as in Equation~\ref{eq:zst}) on the regression layer and verifies our motivation for considering the closed-form solution of OLS regression problems. In addition, as discussed in Section~\ref{ssec:Scale}, the scaling constraints provide essential additional supervision on the alignment and further improve the model performance. Both terms are effective in improving the performance. %Surprisingly, scale loss can achieve exceptional performance on its own for the $\textbf{C~$\rightarrow$~S}$ task, while a less pronounced improvement is noticed on the $\textbf{N~$\rightarrow$~S}$ task. This provides a further evidence that scale should be taken more into account  for the UDA in the regression parameters.

\begin{table}
\centering
\small
\begin{tabular}{lcc}
\hline
Method   & C $\rightarrow$ S & N $\rightarrow$ S \\
\hline
Resnet-18 (source only) & 0.90 & 0.65\\
RSD & 0.31 & 0.53\\
Angle Alignment for Gram   & 0.88 & 0.55\\
Angle Alignment for truncated Gram & 0.89 & 0.52 \\
Angle Alignment for Gram  Inverse~(ours) & 0.27 & 0.36\\
Scale Alignment~(ours) & 0.23 & 0.60 \\
\textbf{DARE-GRAM (ours, angle + scale)} & \textbf{0.20} & \textbf{0.25}\\
\hline
\end{tabular}

\caption{Ablation study of different components in our proposed method on C $\rightarrow$ S and N $\rightarrow$ S task from dSprites. All results are shown in sum of MAE.}
\label{tab:loss_ablataion}
\vspace{-0.5cm}
\end{table}


\noindent\textbf{Effect of a Larger Batch Size} Given the direct relationship between the number of samples in a batch to the variance of the estimated $\beta$ in Equation~\ref{eq:zst}, we studied the effect of the different training batch sizes in Figure \ref{fig:batch} (using the same hyperparameters). As the batch size increases, our approach results in lower MAE.  Furthermore, the RSD approach is more sensitive to the batch size and leads to numerical errors for batches bigger than $64$. In this paper, we used only the same batch size of 36 to have a fair comparison with RSD. However, better results can be achieved with our method by further increasing the batch size to 256.

\begin{figure}
\begin{center}
    \scalebox{0.4}{\input{images/ablation/batch_ablation_1.pgf}}
    \caption{Batch size sensitivity on transfer task $\textbf{C~$\rightarrow$~S}$. For RSD, larger batch sizes lead to numerical errors, thus results not shown. Our method DARE-GRAM is able to achieve better performance with large batch sizes because the feature correlations can be better captured by the Gram matrix with larger number of samples.}
    \label{fig:batch}
\end{center}
\vspace{-0.5cm}
\end{figure}


\noindent\textbf{Effect of alignment factors:} We conduct additional experiments to evaluate the impact on the performance when using different values of hyperparameter $\alpha_{cos}$, $\gamma_{scale}$ and the threshold $T$. As shown in Figure \ref{fig:hyper}, results confirm that our method is not sensitive to hyperparameters. 

\noindent\textbf{Alignment performance of Z and $({Z^TZ})^{-1}$} To further validate the proposed method, we examined the cosine similarity of the k-principal components of ${Z_s, Z_t}$, as well as ${(Z^T_s Z_s)^{-1}, (Z^T_t Z_t)^{-1}}$, after applying our proposed method and RSD. The results are shown in Table A2 in the supplementary material. Our results demonstrate that aligning $Z$ by RSD can lead to poorly aligned inverse Gram matrix $\boldsymbol({Z^TZ})^{-1}$. In contrast, by aligning $({Z^TZ})^{-1}$, our proposed method leads to a well-aligned $Z$,  providing further empirical support for the effectiveness of our proposed method. 

\begin{figure}
\begin{center}
\begin{subfigure}{0.23\textwidth}
    \scalebox{0.45}{\input{images/ablation/sensitivity_s2n.pgf}}
    \caption{Impact of $\alpha_{cos}$ and $\gamma_{scale}$ in log scale on the sum of MAE. }
    \label{fig:alpha_gamma}
\end{subfigure}
\hfill
\begin{subfigure}{0.21\textwidth}
    \scalebox{0.45}{\input{images/ablation/thresold_s2n.pgf}}
    \caption{Impact of Threshold $T$ on the sum of MAE}
    \label{fig:Threshold}
\end{subfigure}
\end{center}
\vspace{-0.4cm}
\caption{Hyperparameter sensitivity of our method on dSprites transfer task $\textbf{S~$\rightarrow$~N}$.}
\label{fig:hyper}
\vspace{-0.5cm}
\end{figure}


\begin{comment}
\begin{figure}
\centering
\begin{minipage}{.48\linewidth}
    \includegraphics[scale=0.26]{cvpr2023-author_kit-v1_1-1/latex/images/ablation/batch_ablation.png}   
    \caption{Batch size sensitivity on transfer task $\textbf{C~$\rightarrow$~S}$}
    \label{fig:batch}
\end{minipage}
\hfill
\begin{minipage}{.48\linewidth}
    %\vspace{-10pt}
    \includegraphics[scale=0.26]{cvpr2023-author_kit-v1_1-1/latex/images/ablation/A_distance.png}
    \caption{ A-distance on transfer task $\textbf{C~$\rightarrow$~S}$}
    \label{fig:a_dist}
\end{minipage}
\end{figure}
\end{comment}



%%%%%%
%%% 1. two loss terms 
%%%% source only, soutce only + term1, sourceonly+term2, full method
%%% Batch
%%% Thresholding
%%% Computer
%%%%%%
%\noindent\textbf{Transferability of Representation:} We plot the trend of Resnet-18, RSD, and DARE-GRAM on transfer task $\textbf{C~$\rightarrow$~S}$ regarding the number of principal components expressing $90\%$ of variance alongside the L2 norm of the respective eigenvalues. DARE-GRAM and RSD share a common attribute. Both methods converge to the same principal components at the end of training three, equal to the number of regression tasks. Even if RSD does not align the scale, the L2-norm is smaller than the one from Resnet-18 but higher than ours. DARE-GRAM show to converge faster than RSD.

% \begin{figure}
% \begin{center}
% \begin{subfigure}{0.23\textwidth}
%     \scalebox{0.5}{\input{cvpr2023-author_kit-v1_1-1/latex/images/ablation/eigenvalues.pgf}}
%     \caption{Number of k principal component.}
%     \label{fig:eigen}
% \end{subfigure}
% \hfill
% \begin{subfigure}{0.23\textwidth}
%     \scalebox{0.5}{\input{cvpr2023-author_kit-v1_1-1/latex/images/ablation/L2_distance.pgf}}
%     \caption{L2-norm between the k principal eigenvalues.}
%     \label{fig:l2_norm}
% \end{subfigure}
% \end{center}
% \caption{Trends comparison on transfer task $\textbf{C~$\rightarrow$~S}$}
% \label{fig:comparaison}
% \end{figure}

\begin{comment}
\begin{figure}
\centering

\includegraphics[scale = 0.45]{cvpr2023-author_kit-v1_1-1/latex/images/ablation/batch_ablation.png}   \caption{Batch size sensitivity on transfer task $\textbf{C~$\rightarrow$~S}$}
\label{fig:batch}
\end{figure}
\end{comment}
\section{Conclusion}

In this paper, we have presented a new domain adaptative regression method called DARE-GRAM. We tackled the domain adaptation for regression problems from a different perspective analyzing the ordinary least square solution to the linear regressor in the deep domain adaptation context. Rather than aligning the original feature embedding space, we aligned a selected subspace of the pseudo-inverse Gram matrix, leveraging the pseudo-inverse low-rank property. Finally, two new regularization terms were proposed to align the scale and angle in a selected subspace generated by the Gram matrix of the two domains. Experimental results show that DARE-GRAM achieves significant improvement in three benchmark regression datasets while ensuring the stability and robustness of the training procedure.

%of its presence in the OLS solution. To this end, we used the pseudo-inverse low-rank property to align the scale and angle in a selected subspace generated by the Gram matrix of the two domains. Experiments on three benchmark regression datasets demonstrated the effectiveness of our approach.
%We focus on UDA for regression tasks. To address this challenge, we propose to take advantage of the well-known closed-form solution for regression problems and highlight the presence and utility of the Gram representation. Furthermore, we mitigate the gap between the domains by exploiting the low-rank property of the pseudo-inverse to align a selected subspace generated by the Gram matrix in scale and angle for both domains. Finally, two new regularisations are proposed to achieve this objective.
%DARE-GRAM achieves state-of-the-art adaptation results on regression tasks.
%The main limitation is that this paper does not cover UDA with missing output values in the target domain which has to be investigated in the future.
\noindent\textbf{Acknowledgments:} This work was supported by
the Swiss National Science Foundation under Grant PP00P2$\_$176878.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib.bib}
}

\end{document}
