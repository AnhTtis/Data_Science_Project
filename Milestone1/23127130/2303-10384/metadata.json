{
    "arxiv_id": "2303.10384",
    "paper_title": "Powerful and Extensible WFST Framework for RNN-Transducer Losses",
    "authors": [
        "Aleksandr Laptev",
        "Vladimir Bataev",
        "Igor Gitman",
        "Boris Ginsburg"
    ],
    "submission_date": "2023-03-18",
    "revised_dates": [
        "2023-05-10"
    ],
    "latest_version": 1,
    "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
    ],
    "abstract": "This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) \"Compose-Transducer\", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) \"Grid-Transducer\", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and end of utterances. All RNN-T losses are implemented with the k2 framework and are available in the NeMo toolkit.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10384v1"
    ],
    "publication_venue": "To appear in Proc. ICASSP 2023, June 04-10, 2023, Rhodes island, Greece. 5 pages, 5 figures, 3 tables",
    "doi": "10.1109/ICASSP49357.2023.10096679"
}