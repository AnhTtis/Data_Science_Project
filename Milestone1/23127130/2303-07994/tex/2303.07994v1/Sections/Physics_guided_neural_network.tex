\section{Physics-Guided Neural Network}
\label{sec:physics_guided_neural_network}%
%This section describes the PGNN feedforward parametrization, constituting contribution C1.
\subsection{Parallel Feedforward Parametrization}
To compensate hard-to-model dynamics, the feedforward controller $\mathcal{F}_{\zeta,\phi}$ is parametrized as the parallel combination of a physical model $\mathcal{M}_\zeta$ and complementary neural network $\mathcal{C}_\phi$ according to
\begin{equation}
	f(k) = \mathcal{F}_{\zeta,\phi}(\theta_d(k)) = \mathcal{M}_\zeta(\theta_d(k)) + \mathcal{C}_\phi(\theta_d(k)),
	\label{eq:parallel_feedforward}
\end{equation}
and is visualized in Fig. \ref{fig:PGNN_feedforward}.
The physical model $\mathcal{M}_\zeta$ with parameters $\zeta$ represents prior knowledge of the physical process through encapsulating quantitatively known dynamics. The physical model is complemented by a universal function approximator $\mathcal{C}_\phi$ with parameters $\phi$, here chosen as a neural network, such that it can learn dynamics not included in the physical model.
% with physical parameters $\zeta$, and a universal function approximator complement $\mathcal{C}_\phi$ with parameters $\phi$, represented here by a neural network.
%More specifically, the feedforward control signal $f(k) \in \mathbb{R}$ is generated based on the reference $\theta_d(k) \in \mathbb{R}$ according to 
%\begin{equation}
%	f(k) = \mathcal{F}_{\zeta,\phi}(\theta_d) = \mathcal{M}_\zeta(\theta_d) + \mathcal{C}_\phi(\theta_d).
%	\label{eq:parallel_feedforward}
%\end{equation}

\subsection{Physical Model}
The physical model $\mathcal{M}_\zeta$ constitutes an equation of motion describing the relation between the generalized coordinate $\theta$ and the input $u$ derived from first-principles modeling \citep{deVos2022}. More specifically, assuming that inputs are confined within actuator limits and ignoring drivetrain flexibilities, the equation of motion for $\theta$ is given by
\begin{equation}
	u = M \ddot{\theta} + H(\theta,\dot{\theta}) + d\dot{\theta}.
	\label{eq:physical_model}
\end{equation}
Here, $d \in \mathbb{R}_{\geq 0}$ is the viscous damping coefficient,
\begin{equation}
	M = m (y^2 + z^2) + J_{xx} \in \mathbb{R}_{\geq 0},
\end{equation}
the inertia of the roll axis, and
\begin{equation}
	H(\theta,\dot{\theta}) = m g (y \cos(\theta) - z \sin(\theta))\cos(\phi) \in \mathbb{R},
\end{equation}
the gravity contribution. Coordinates $y,z\in\mathbb{R}$ represent the offset of the center of mass with respect to the point of rotation, and $\phi$ the known orientation of the roll axis out of the vertical plane.
%inverting the physical model \eqref{eq:physical_model}, i.e., 

The required feedforward signal can be obtained by evaluating the right-hand-side for a given reference $\theta_d$ and discrete-time derivatives $\dot{\theta}_d$. Thus, the physical-model-based feedforward controller $\mathcal{M}_\zeta: \theta_d \rightarrow f_\mathcal{M}$ is given by
\begin{equation}
	f_\mathcal{M}(k) = M \ddot{\theta}_d(k) + H(\theta_d(k),\dot{\theta}_d(k)) + d\dot{\theta}_d(k),
	\label{eq:f_M}
\end{equation}
with physical parameters 
\begin{equation}
	\zeta = \begin{bmatrix} m & J_{xx}& d & y & z	\end{bmatrix}^\T \in \mathbb{R}^{N_\zeta}.
\end{equation}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{./Figures/Parallel_feedforward_parametrization.pdf}
	\caption{PGNN feedforward controller consisting of physical-model-based feedforward controller $\mathcal{M}_\zeta$ with parameters $\zeta$ and neural network feedforward controller $\mathcal{C}_\phi$. $\mathcal{C}_\phi$ consists of a feedforward neural network $g_\phi$ and physics-guided input $T(\theta_d(k))$, here with $L=3$ hidden layers of size $4, 3, 4$, and a linear output layer.  }
	\label{fig:PGNN_feedforward}
\end{figure}

\subsection{Neural Network}
%\begin{figure}
%	\centering
%	\includegraphics[width=\linewidth]{./Figures/Neural_network_feedforward_parametrization.pdf}
%	\caption{Neural network feedforward controller $\mathcal{C}_\phi$ consisting of feedforward neural network $g_\phi$ and physics-guided input $T(\theta_d(k))$, here with $L=3$ hidden layers of size $4, 3, 4$, and a linear output layer.  }
%	\label{fig:NN_feedforward}
%\end{figure}

The neural network $\mathcal{C}_\phi$ consists of a feedforward neural network (FNN) acting on a physics-guided input vector. By the universal approximation theorem \citep{Goodfellow2016}, $\mathcal{C}_\phi$ can represent any continuous function on a bounded interval up to arbitrary precision, such that it can learn hard-to-model dynamics, such as cable forces, from data.

More specifically, the neural network feedforward controller $\mathcal{C}_\phi: \theta_d \rightarrow f_\mathcal{C} $ is defined as
\begin{equation}
	f_\mathcal{C}(k) = g_\phi(T(\theta_d(k))).
	\label{eq:f_C}
\end{equation}
The FNN $g_\phi: T(\theta_d(k)) = x \rightarrow f_\mathcal{C}(k)$ is given by a fully connected multilayer perceptron (MLP) of $L$ hidden layers and a linear output layer without bias, i.e.,
%\begin{equation}
%	g_\phi(x(k)) = W_L \sigma( W_{L-1} \sigma( \hdots \sigma( W_0 x(k) + b_0 ) \hdots + b_{L-1} ),
%\end{equation} 
\begin{align}
	h^l(x) &= x	 & \textrm{if}\ \ & l = 0 \nonumber \\
	h^l(x) &= \sigma \left( W^{l-1} h^{l-1}(x) + b^l \right) & \textrm{if}\ \  & l = {1, \ldots, L} \nonumber \\
	g_{\phi}(x) &= W^l h^{l}(x) & \textrm{if}\ \ & l = L, \label{eq:FNN}
\end{align}
in which $W^l,b^l$ are appropriately sized weight matrices and bias vectors defining affine mappings, and $\sigma$ is an element-wise activation function acting on this affine mapping, such as a sigmoid, hyperbolic tangent ($\tanh$) or rectified linear unit (ReLU). The full MLP $g_\phi$ then is the repeated application of affine and nonlinear transformations. 

The physics-guided input transformation $T$ encodes prior qualitative knowledge on the hard-to-model dynamics and is defined as
\begin{equation}
	T(\theta_d(k)) = \begin{bmatrix} \theta_d(k) & \dot{\theta}_d(k) & \ddot{\theta}_d(k) & \mathrm{relay}(\dot{\theta}_d(k))	\end{bmatrix}^\T,
\end{equation}
with 
\begin{equation}
\mathrm{relay}(x(k)) = \begin{cases}
1 & \textrm{if } x(k) > 0 \\
-1 & \textrm{if } x(k) < 0 \\
\mathrm{relay}(x(k-1)) & \textrm{if } x(k) = 0,
\end{cases}
\label{eq:relay}
\end{equation}
and $\mathrm{relay}(x(0)) = 0$.
It transforms $\theta_d$ such that the input to $g_\phi$ contains all relevant physical quantities to make predictions about the required force.
%as opposed to trying to also learn these quantities from data using recurrent neural networks, which would increase complexity. 
For example, $T(\theta_d(k))$  contains the reference velocity $\dot{\theta}_d$ to learn complex friction characteristics, and a relay of the velocity to approximate hysteresis characteristics without requiring a recurrent network architecture, thereby greatly simplifying optimization.
