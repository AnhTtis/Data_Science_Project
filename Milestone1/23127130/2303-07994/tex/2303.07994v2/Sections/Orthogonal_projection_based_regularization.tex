\section{Separating Physical Model and Neural Network Contributions}
\label{sec:orthogonal_projection_based_regularization}
The neural network $\mathcal{C}_\phi$ is also able to learn and compensate dynamics included in the physical model $\mathcal{M}_\zeta$ and thus there exists multiple parameters $\zeta,\phi$ that produce the same input-output behaviour of $\mathcal{F}_{\zeta,\phi}$, i.e., the PGNN parametrization \eqref{eq:parallel_feedforward} is unidentifiable. Consequently, the physical model and neural network need not be complementary. By regularizing the neural network contribution $f_\mathcal{C}$ in the output space of the physical model, a specific solution on this manifold is selected, namely the one in which modeled dynamics are compensated by the physical model component and not by the neural network.

\subsection{Least-Squares And Consequences of Unidentifiability}
To illustrate unidentifiability, the least-squares cost function is decomposed in two orthogonal subspaces, one spanned by the physical model (locally, given some fixed parameter) and one as its orthogonal complement, from which it is apparent that the neural network can have a contribution in the subspace spanned by the physical model, resulting in unidentifiability.

Consider the least-squares criterion $J_{LS}$ penalizing residuals between the output of $\mathcal{F}_{\zeta,\phi}(\theta(k))$ and the required input $\hat{u}(k)$ for this $\theta(k)$, i.e., 
\begin{equation}
J_{LS} = \sum_{k=1}^N \norm{\hat{u}(k) - (f_\mathcal{M}(k) + f_\mathcal{C}(k))}_2^2 \in \mathbb{R}_{\geq 0}.
\label{eq:LS_costs}
\end{equation}
To decompose this cost function into orthogonal subspaces, the physical model parameters are split into two subsets $\zeta_l$, $\zeta_n$, such that, given $\zeta_n$, $f_\mathcal{M}$ is linear in $\zeta_l$, i.e.,
\begin{equation}
	\begin{aligned}
		\zeta_l &= \begin{bmatrix} m & J_{xx} & d \end{bmatrix}^\T \in \mathbb{R}^{N_{\zeta_l}} & & & \zeta_n &= \begin{bmatrix} y & z	\end{bmatrix} \in \mathbb{R}^{N_{\zeta_n}}.
	\end{aligned}
\end{equation}
Then, the physical model response can be written as
\begin{align}
f_\mathcal{M}(k) &= \begin{bmatrix} x_{1,\zeta_n}(\theta_d(k)) & x_2(\theta_d(k)) & x_3(\theta_d(k)) \end{bmatrix} \begin{bmatrix} m & J_{xx} & d \end{bmatrix}^\T \nonumber \\
&= X_{\zeta_n}(\theta_d(k)) \zeta_l
\end{align}
%f_\mathcal{M}(k) &= x_{1,\zeta_n}(\theta_d(k)) m + x_2(\theta_d(k)) J_{xx} + x_3(\theta_d(k)) d \nonumber \\
with ($\zeta_n$-dependent) basis functions
\begin{equation}
	\begin{gathered}
		x_{1,\zeta_n}(\theta) = (y^2 + z^2) \ddot{\theta} + g(y\cos(\theta) - z \sin(\theta)) \cos(\phi) \\
		\begin{aligned}
		x_2(\theta) &= \ddot{\theta} & & & x_3(\theta) &= \dot{\theta}.
		\end{aligned}
	\end{gathered}
\end{equation}
Now, represent finite-time signal $\hat{u}(k)$, $k=1,\ldots,N$ as a vector, i.e., 
\begin{equation}
	\underline{\hat{u}} = \begin{bmatrix} \hat{u}(1) & \hat{u}(2) & \hdots & \hat{u}(N) \end{bmatrix}^\T \in \mathbb{R}^N,
\end{equation}
and similarly for $\underline{\theta}_d$, $\underline{f}_\mathcal{M}$, $\underline{f}_\mathcal{C}$. Then, $\underline{f}_\mathcal{M}$ can be written as 
\begin{equation}
	\underline{f}_\mathcal{M} = X_{\zeta_n}(\underline{\theta}_d) \zeta_l,
	\label{eq:physical_model_pseudo_LIP}
\end{equation}
with finite-time basis function matrix
\begin{equation}
	X_{\zeta_n}(\underline{\theta}_d) = \begin{bmatrix} X_{\zeta_n}^\T (\theta_d(1)) & \hdots	& X_{\zeta_n}^\T (\theta_d(N)) \end{bmatrix}^\T \in \mathbb{R}^{N \times N_{\zeta_l}}.
\end{equation}
Based on \eqref{eq:physical_model_pseudo_LIP}, the output space of the physical model for any $\zeta_l$, given $\zeta_n$, is formed by the image of the basis function matrix $X_{\zeta_n}$ evaluated for the data $\underline{\theta}_d$, and can be represented through, e.g., a singular value decomposition (SVD). More specifically, consider the SVD of $X_{\zeta_n}$ as
\begin{equation}
	X_{\zeta_n}(\underline{\theta}_d) = \begin{bmatrix} U_{1, \zeta_n} & U_{2, \zeta_n} \end{bmatrix} \begin{bmatrix} \Sigma_{\zeta_n} & 0 \\ 0 & 0\end{bmatrix} \begin{bmatrix} V_{1, \zeta_n}^T \\ V_{2, \zeta_n}^\T	\end{bmatrix},
\end{equation}
with $\Sigma_{\zeta_n} \in \mathbb{R}^{r \times r}$, $r = \textrm{rank}(X_{\zeta_n})$, and $U_{1, \zeta_n} \in \mathbb{R}^{N \times r}$, $U_{2, \zeta_n} \in \mathbb{R}^{N \times N-r}$
such that $\begin{bmatrix} U_{1, \zeta_n} & U_{2, \zeta_n} \end{bmatrix}$ is an orthonormal matrix, i.e., 
\begin{equation}
	\begin{aligned}
		U_{1, \zeta_n}^\T U_{1, \zeta_n} &= I_{r} & U_{2, \zeta_n}^\T U_{1, \zeta_n} &= 0 & U_{2, \zeta_n}^\T U_{2, \zeta_n} = I_{N-r}.
	\end{aligned}
\end{equation}
%and similarly for $V_{1, \zeta_n}$, $V_{2, \zeta_n}$.
Then, $U_{1, \zeta_n}$ forms a basis for the image of $X_{\zeta_n}$, i.e., the output $\underline{f}_\mathcal{M}$ of the physical model for any $\zeta_l$, given $\zeta_n$, lies in the subspace spanned by $U_{1, \zeta_n}$.

Consider again $J_{LS}$ in \eqref{eq:LS_costs}, which with above finite-time notation can also be written as
\begin{equation}
J_{LS} = \norm{\underline{\hat{u}} - X_{\zeta_n}(\underline{\theta}_d) \zeta_l - g_\phi(T(\underline{\theta}_d))}_2^2.
\label{eq:J_LS_lifted}
\end{equation} 
Given basis $U_{1, \zeta_n}$ of the physical model output space, \eqref{eq:J_LS_lifted} is decomposed into $U_{1, \zeta_n}$ and complement $U_{2, \zeta_n}$ as \citep{Kon2022PhysicsGuided}
\begin{equation}
	J_{LS} = \norm{
	\begin{bmatrix}
	U_{1, \zeta_n}^\T \underline{\hat{u}} \\
	U_{2, \zeta_n}^\T \underline{\hat{u}}
	\end{bmatrix}
	-
	\begin{bmatrix}
	\Sigma_{\zeta_n} V_{1, \zeta_n}^\T \zeta_l + U_{1, \zeta_n}^\T g_\phi(T(\underline{\theta}_d)) \\
	U_{2, \zeta_n}^\T g_\phi(T(\underline{\theta}_d))
	\end{bmatrix}
	}_2^2.
	\label{eq:J_LS_decomposed}
\end{equation}
in which it is used that i) multiplying by $U_{1, \zeta_n}$ does not change the norm, ii) $X_{\zeta_n}(\underline{\theta}_d) = U_{1, \zeta_n} \Sigma_{\zeta_n} V_{1, \zeta_n}^\T$, iii) $U_{2, \zeta_n}^\T U_{1, \zeta_n}^\T=0$, and iv) $U_{1, \zeta_n} U_{1, \zeta_n}^\T + U_{2, \zeta_n}^\T U_{2, \zeta_n} = I$. In the decomposed cost function \eqref{eq:J_LS_decomposed}, $U_{1, \zeta_n}^\T g_\phi(T(\underline{\theta}_d))$ represents the neural network contribution in the subspace spanned by the physical model, such that dynamics can be freely interchanged as long as $g_\phi(T(\underline{\theta}_d))$ can have a contribution in $U_{1, \zeta_n}^\T$, revealing the unidentifiability. 

\subsection{Ensuring Complementarity through Regularization}
To ensure that the physical model learns all dynamics that fit in \eqref{eq:f_M}, the neural network contribution \eqref{eq:f_C} in the subspace of the physical model $U_{1, \zeta_n}$ is penalized through orthogonal projection-based regularization, ensuring complementarity between the physical model and neural network.

More specifically, the orthogonal projection-based regularization (OP-regularization) is defined as
\begin{equation}
R(\phi) = \norm{U_{1, \zeta_n^0}^\T g_\phi(T(\underline{\theta}_d))}_2^2 \in \mathbb{R}_{\geq 0},
\label{eq:OP_regularization}
\end{equation}
where $\zeta_n^0$ is an initial estimate of $\zeta_n$, obtained here from the best physical model fit. This regularization penalizes neural network contributions $g_\phi(T(\underline{\theta}_d))$ in the output space of the physical model $U_{1, \zeta_n^0}$, such that the neural network focuses on learning unmodeled dynamics in $U_{2, \zeta_n^0}$. As such, regularization \eqref{eq:OP_regularization} can be seen as targeted $L_2$-regularization, where only the weight directions generating a contribution in $U_{1, \zeta_n^0}$ are shrunk.

The feedforward parametrization \eqref{eq:parallel_feedforward} is then optimized according to
\begin{equation}
	\min_{\zeta, \phi} J_{OP} = \min_{\zeta, \phi} J_{LS} + \lambda R(\phi),
	\label{eq:J_OP}
\end{equation}
with user-defined regularization parameter $\lambda \in \mathbb{R}_{\geq 0}$ creating a spectrum between the least-squares case for $\lambda = 0$ and full orthogonality for $\lambda \rightarrow \infty$. Since $g_\phi$ only has finitely many degrees of freedom, in practice there exists a trade-off between not having a contribution in $U_{1, \zeta_n^0}$ and capturing unmodeled dynamics in $U_{2, \zeta_n^0}$, and $\lambda = 10^{-1}$ works well for most problems.

Although $\zeta_n$ is also updated during optimization of \eqref{eq:J_OP}, thus changing $U_{1, \zeta_n}$, recursively updating $U_{1, \zeta_n}$ in \eqref{eq:OP_regularization} to more accurately match the true physical model output space does not yield different results from a fixed initialization in practice: the best physical model fit provides a good enough approximation to ensure complementarity.