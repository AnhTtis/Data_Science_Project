\section{Experimental Evaluation}
\label{sec:Evaluation}

%This section presents the experimental evaluation results.

\subsection{Experiment Setup}

%In this section, we introduce the experiment settings, including the ISPC data set, the monitor system setting when we collect our data, and the ML algorithm implementation details.

\subsubsection{ISPC dataset}
The ISPC data set contains one channel of ECG data, 2 channels of PPG data, and 3-axis acceleration data. There are 12 healthy subjects and each of them is required to do a series of exercises to generate 5-minute long recordings with a PPG sampling rate of 125 Hz. Although there are two channels of PPG signals, we only used one channel that has lower noise for HR estimation for better reproduction accuracy. 80\% of the data are used for training, and 20\% are used for testing.
% There is an ECG HR every 8s with 6s overlap.
% Their PPG is installed on the wrist.

\subsubsection{Our dataset}

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{pic/pi.png}
  \caption{Connections between Raspberry Pi and PPG sensor.}
  \label{fig:pi}
\end{figure}

Our data set contains one channel of PPG data and 3-lead ECG signals.
%We collected data in three typical usage scenarios: when the subject is sitting, sleeping, and conducting daily activities such as walking and typing keyboard. 
%Each recording lasts for about 2 hours.
The subject simultaneously wore the PPG sensor on the fingertip and ECG electrodes on the torso. The PPG sensor is connected to a Raspberry Pi 4B through an I2C bus to record the data, as shown in Fig.\ref{fig:pi}.
%To ensure the stability of acquiring the sensor data, the connections between the communication pins on these 2 sensors and the DuPont wires are fixed by using solder, and then another end of the DuPont wires is pinned to the breadboard. 
The hardware components include: 1) a Raspberry Pi 4B with 4 GB RAM; 2) a Maxim MAXREFDES117\footnote{https://a.co/d/1VQGxm8} HR monitor with a MAX30102 PPG sensor; and 3) a TLC5007 Dynamic ECG (not shown in the figure).

% \paragraph{Edge device}
% A Raspberry Pi 4B with 4 GB RAM and a 1.5 GHz quad-core Arm A72 CPU is utilized to communicate with the sensors and run the monitor system. 
% \paragraph{PPG sensor}
% MAXREFDES117 is chosen as the PPG sensor since it provides a reliable light signal. 
% The MAXREFDES117 contains a MAX30102 which is in charge of monitoring the pulse oximeter and heart rate, and it has other associated parts which can filter out some noise. 
% It also provides a convenient pins interface for easy data accessing through the i2C bus.

% \paragraph{ECG device}
% TLC5007 Dynamic ECG is selected to record ECG signals of the subjects and used as the ground truth.
% It has a 3-lead portable holter that can collect ECG waveform. After recording, it can be connected to a computer to export the recording file. %which is a bin file.
% During our experiment, 5 disposable electrodes are mounted on the subject's torso and connected to the device through cables. 
% It also came with analysis software that can extract the RR intervals based on the recording file.

\subsubsection{ML Configurations}
ML algorithms are implemented using Scikit-Learn~\cite{scikit-learn}.
We used the random search function {\em RandomizedSearchCV} in Scikit-Learn to tune the model hyperparameters. The hyperparameters are: 
1) For DT, the maximum depth of the tree ranges from 1 to 20. 
2) For RF, the number of trees is between 1 to 30, and the maximum tree depth is between 3 to 7. 
3) For KNN, the number of neighbors is between 1 to 30, and the distance can be Manhattan or Euclidean. 
4) For SVM, the kernel may be among RBF, sigmoid and polynomial, and regularization (i.e., $C$) may be between 0.00001 to 10. 
%For polynomial kernels, the degree is between 3 to 8.
%For, we searched the hyper-parameter $C$ from 0.00001 to 10.
5) For MLP, there are 3 hidden layers with 2 to 15 neurons in each layer.
The activation function can be {\em relu} or {\em tanh}. 
The L2 regularization hyper-parameter $\alpha$ search range is from 0.00001 to 10.

% to do:  \caption{HR estimation trace for subject 11 in the ISPC data set (MLP model's feature size is \TODO{10}). The vertical line is the dividing line for training and testing. \TODO{only this has a vertical line?}}
% change both {fig:hr_ispc_singleSubject} and {fig:hr_mape_subject} to using 2 features to keep consistent

\begin{table*}
\caption{MAPE and Standard Deviation (SD) for different number of features when estimating ISPC HR (MAPE±SD).}
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{}}& \multicolumn{6}{c|}{Number of features}  \\ \hline
\multicolumn{1}{|l|}{} & 2             & 4             & 6             & 8             & 10            & 15            \\ \hline\
DT                     & 3.86\%±5.26\% & 3.77\%±4.91\% & 3.58\%±4.14\% & 3.4\%±3.64\%  & 3.41\%±3.53\% & \textbf{2.76\%±1.89\%} \\ \hline
RF                     & 3.14\%±3.45\% & 3.29\%±3.4\%  & 2.9\%±2.37\%  & 2.79\%±2.08\% & 2.7\%±1.87\%  & \textbf{2.62\%±1.91\%} \\ \hline
KNN                    & 2.89\%±2.6\%  & 2.89\%±2.2\%  & 2.9\%±1.87\%  & 3.08\%±1.75\% & 3.55\%±2.33\% & 3.69\%±1.94\% \\ \hline
SVM                    & 3.81\%±2.5\%  & 5.14\%±6.18\% & 4.71\%±5.49\% & 3.83\%±2.2\%  & 5.11\%±5.41\% & 4.06\%±2.49\% \\ \hline
MLP                    & 3.33\%±5.86\% & 3.26\%±5.2\%  & 3.38\%±5.49\% & 4.53\%±5.78\% & 3.4\%±4.41\%  & 4.81\%±4.46\% \\ \hline
\end{tabular}
\label{tab:hr_numofFeature_ispc}
\end{table*}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{pic/ispc_hr_plot220728mlp_dim2_mape-test.pdf}
  \caption{HR estimation MAPE for different subjects in the ISPC data set (MLP model's feature size is 2). "MLP" stands for our method that combines signal processing and MLP, and "Sig-proc" stands for the signal-processing-only method reproduced based on the ISPC paper~\cite{zhang2014troika}.}
%   (when using MLP and the number of features is 2). The ML predicted HR has lower error compared with only the signal processing method for most subjects.}
  \label{fig:hr_mape_subject}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{pic/ispc_hr_plot220909DATA_11_TYPE02_PPG_mlp_search2ppg1result.pdf}
  \caption{HR estimation trace for subject 11 in the ISPC dataset (the feature size of 2 is used for the MLP model).}
  \label{fig:hr_ispc_singleSubject}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{pic/ispc_hr_plot220909_dim_filesizeKBW0.13.pdf}
  \caption{Model sizes for different ML models with different number of features when estimating HRs using the ISPC dataset.}
  \label{fig:hr_modelsize_ispc}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{pic/ispc_hr_plot220909_dim_predictTW0.13.pdf}
  \caption{Model inference time for each HR reading with different ML models and number of features when estimating HRs using the ISPC dataset.}
  \label{fig:hr_time_ispc}
\end{figure}

\subsection{Heart Rate Estimation Results}
% In this section, we present the HR estimation results on ISPC data and our collected data.

\subsubsection{HR Estimation Accuracy for ISPC Dataset}
Figure~\ref{fig:hr_mape_subject} gives the HR estimation errors for the ISPC dataset, including the MAPEs of our method that combines signal processing and MLP model, and the MAPEs of the signal-processing-only method. 
Note that, the MLP model here has two features (i.e., two PPG-HRs as features). 
As Figure~\ref{fig:hr_mape_subject} shows, our combined method usually had lower errors than the signal-processing-only method.
% data for this graph is  [[0.00828584 0.20243997 0.00562585 0.00736061 0.00407525 0.02001565 0.01246988 0.00436353 0.01900087 0.09282047 0.0057757  0.01732338 0.03329642] [0.01428032 0.4094858  0.00931877 0.00943381 0.00673844 0.01562828 0.00724112 0.01015999 0.00864288 0.50075197 0.00888993 0.0206278 0.08509992]]
On average, our method had only 3.33\% error, which is 5.17\% lower than the signal-processing-only method. 
To further compare the accuracy differences, Figure~\ref{fig:hr_ispc_singleSubject} gives the HR estimations of our combined method, the signal-processing-only method, along with the ECG ground truth for subject 11. 
As Figure~\ref{fig:hr_ispc_singleSubject} shows, our HR estimations are more close to ECG readings, and less fluctuating than those from the signal-processing-only method, suggesting that the MLP model in our method could indeed reduce the random errors from the HRs generated by signal processing.


Figure~\ref{fig:hr_mape_subject} also indicates that, for most subjects, the HR estimation errors are below 2\%, suggesting that PPG-based HR monitoring can have high accuracy. 
The only two exceptions are subject 2 and subject 10, where the PPG signals were more noisy and random, likely due to worse MA effects during data collection. 
The same issue was also observed by other studies~\cite{zhang2015photoplethysmography}. 
Nonetheless, even for subjects 2 and 10, our combined method is still significantly more accurate than the signal-processing-only approach.

\subsubsection{ML Model Exploration for ISPC Dataset}
Besides MLP with two features, we also explored ML models and feature sizes to search for a configuration that can provide good accuracy with small model size and less inference time. 
Table~\ref{tab:hr_numofFeature_ispc} gives the MAPEs for different ML models and feature sizes (i.e., PPG-HR input sizes). Note that, the MAPEs in Table~\ref{tab:hr_numofFeature_ispc} is the average MAPEs for all 12 subjects for each configuration along with the standard deviation. 
As Table~\ref{tab:hr_numofFeature_ispc} shows, while all configurations have similar low errors, RF with 15 features has the lowest average MAPE at 2.62\%, followed closely by DT with 15 features at 2.76\%.

Figure~\ref{fig:hr_modelsize_ispc} gives the average model sizes for all subjects with different ML configurations, which shows that DT usually has smaller model sizes. 
While KNN and SVM have smaller model sizes when the number of features is small, due to their algorithmic philosophy, their model sizes grow rapidly when the number of features increases.
% Considering the lower error from DT, these results suggest that DT with 15 features is a better configuration that provides both high accuracy and low model size.

Figure~\ref{fig:hr_time_ispc} gives the average model inference time for all subjects with different ML configurations, which shows that DT usually has shorter inference time. The inference time did not increase much when the number of features increased from 2 to 15.
Considering the lower error from DT, these results suggest that DT with 15 features is a better configuration that provides both high accuracy, smaller model sizes, and short inference time.

\subsubsection{HR estimation Accuracy for Our Dataset}

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{pic/mydata_hr_plot220726_alg10mape-test.pdf}
  \caption{HR estimation MAPEs for different scenarios and ML algorithms using our data set (the feature size of 10 is used for ML models).}
  %when estimating HR using \10 HR as features. The sleep scenario has a slightly higher error.}
  \label{fig:hr_alg_mape}
\end{figure}

% more picture see plot_result/mydata_hr_plot220901
\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{pic/mydata_hr_plot220901Bdaily1-dis00_mlp10result5700to7028.pdf}
  \caption{Partial testing set HR estimation trace for the daily activities scenario (MLP model's feature size is 10). "Sig-proc" stands for the signal-processing-only method described in Section \ref{sec:stage2}.}% The ML model is MLP and the number of features is 10.}
  \label{fig:hr_ecgSensorPredicted}
\end{figure}

\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{pic/mydata_hr_plot220728daily_dim_mape-testsub.pdf}
  \caption{HR estimation MAPE for different number of features with our collected daily scenario data.} %The accuracy of a different number of features and different ML models are similar.}
  \label{fig:hr_numofFeature}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{pic/mydata_hr_plot220727daily_dim_filesizeKB.pdf}
  \caption{ML model size for different number of features when estimating HR using our collected daily scenario data.} %DT, MLP, and RF have smaller model size compared with KNN and SVM.}
  \label{fig:hr_modelsize}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{pic/mydata_hr_plot220909daily_dim_predictT.pdf}
  \caption{ML inference time for different number of features when estimating HR using our collected daily scenario data.} %DT, MLP, and RF have shorter inference time compared with KNN and SVM.}
  \label{fig:hr_time}
\end{figure*}
Figure~\ref{fig:hr_alg_mape} gives the HR estimation error using our data set. 
Note that, the ML models here have 10 features (i.e., 10 PPG-HRs as features). 
As Figure~\ref{fig:hr_alg_mape} shows, our combined method with either DT, RF, KNN, SVM, and MLP models always has lower error than the signal-processing-only model (Sig-proc) for all activity scenarios. Moreover, the errors of our method are always below 6\%, showing that our method is highly accurate. 
Figure~\ref{fig:hr_ecgSensorPredicted} further gives the HR traces for our method with an MLP model, the signal-processing-only method, and the ECG ground truth. 
Similar to the ISPC evaluation results, HRs from our combined method in Figure~\ref{fig:hr_ecgSensorPredicted} are usually more close to ECG readings. Our HR estimations are also less fluctuating than those from the signal-processing-only method, suggesting that our method could reduce the random noises in the PPG signals when the PPG signal is only at 25Hz.

\subsubsection{ML Model Exploration for Our Dataset}

Again, we also explored ML models and feature sizes using our data set to search for a configuration with good accuracy, small model size, and fast inference time. 
Since our data duration is longer than ISPC data, we took advantage of this and explored up to 300 features to further investigate how much historical data is preferable as ML input.
Figure~\ref{fig:hr_numofFeature} gives the MAPEs for different ML models and feature sizes (i.e., PPG-HR input sizes) for the daily activity scenario. We can see from Figure~\ref{fig:hr_numofFeature}, that the errors for all types of ML models are similarly low when there are more than 10 features, suggesting that 10 to 20 features could be enough for our data set.

Figure~\ref{fig:hr_modelsize} gives the model sizes of different ML configurations, which shows that DT models are consistently the smallest. 
% Compared to ISPC results, KNN and SVM have larger model sizes, probably because our data contains more samples compared to ISPC data.
% The model size increases when the number of features increases.
Figure~\ref{fig:hr_time} provides the model inference time with different ML configurations, which reveals that DT models are consistently the fastest. 
All the inference times increased slightly when the number of features increased from 1 to 300, due to more features and bigger models. %, but this trend is not obvious when the number of features is smaller than 60.
Considering the lower error from DT, these results suggest that DT with 10 to 20 features is a better configuration that provides high accuracy with a small model size and fast inference time.

% %For the ISPC data, we split the PPG data into 8:2 ratios and use the 80\% data to train and the rest 20\% to test.
% % Since there are using a two-channel PPG, we named the two PPG signals PPG1 and PPG2.
% % In this paper, we use PPG1 only to estimate ECG HR since there is much noise in PPG2.
% For example, the HR results for subject 11 are shown in Fig. \ref{fig:hr_ispc}.
% We can find that PPG1 HR is closer to ECG HR than PPG2 HR.
% The blue line shows the estimated HR from an MLP model using 10 PPG1 HRs as features.
% As shown in Fig. \ref{fig:hr_ispc}, the estimated HR is closer to ECG HR than PPG1 HR, which shows the effectiveness of the estimation algorithm.

% For 12 different ISPC subjects, MAPE varies considerably among them, and for most subjects, the ML predicted HR has less error compared with the HR before ML.
% For example, Fig. \ref{fig:hr_mape_sub} shows the RF models' MAPE when the number of features is 2.
% MAPEs for most subjects are below 2\%.
% But there are subject that has MAPE over 10\%.
% The reason might be the input for ML models contains more noise, which is related to the experiment collection process.
% The trends are similar for the other number of features and other ML algorithms.

% To evaluate the HR estimation method quantitatively, we present the HR estimation mean absolute percentage error (MAPE) in Table \ref{tab:hr_numofFeature_ispc}. 
% The average MAPE and standard deviation (SD) are the average or SD for the 12 subjects in ISPC.
% % The HR estimation of \cite{tor} is 1.82\%±2.07\%).
% We use different numbers of HR as the features to predict.
% For example, let $P_t^{hr}$ denote PPG HR at time $t$.
% To estimate HR at time $t$, we use $x$ PPG HR ($P_{t-x+1}^{hr} \thicksim P_t^{hr}$) as features for ML models.
% From the table, we can find that as the number of features increases, MAPE does not increase significantly.
% Therefore, smaller numbers of features are preferable to balance the estimation accuracy and the ML model complexity.
% Besides, The MAPE between different ML models is similar, except for SVM, which is the only ML algorithm that has MAPE higher than 5\%.

% The ML model sizes are shown in Fig. \ref{fig:hr_modelsize_ispc}.
% The model size is the average model size for the 12 ISPC subjects. 
% It can be easily found that the model sizes of DT, RF, and MLP have no major increase when adding more features. 
% However, KNN and SVM sizes change proportionally when increasing the number of features. 
% Usually, with more features, more buffers are required to store them, and more complex logic is needed for predication which also requires more memory.
% The model size of DT depends on the depth of the tree, consequently, the DT model sizes have limited changes. 
% Similarly, the RF model relies on the configuration of the forest, like the number of trees and the size of each tree, hence the model size may not change much if the configuration is fixed, and due to hyper-parameter searching, The size of RF happen to varies a little for the different number of features cases. 

% Considering the prediction accuracy and model size jointly, we recommend DT, RF, and MLP. 
% Besides, we recommend using fewer PPG HR'  as features since fewer features do not affect accuracy much but contribute to a smaller model size.






% \subsubsection{HR Estimation Results for Our Dataset}

% Our collected PPG HR, estimated HR, and collected ECG HR are shown in Fig. \ref{fig:hr_ecgSensorPredicted}. 
% In this figure, the estimated HR is obtained with an MLP model when the number of features is 10.
% As we can see, the MLP model obtained a high accuracy that helps to make the estimated HR closer to the ECG HR. 
% For example, when the time index is around 5800, the PPG HR contains some outliers but the ML model predicted results correctly and the estimated HR is closer to the label.

% We also compared the performance in different scenarios and the results are shown in Fig. \ref{fig:hr_alg_mape}.
% The number of features is 10. 
% The sit scenario and daily (activity) scenario have lower MAPE while the sleep scenario has higher MAPE.
% However, the MAPE difference between different scenarios is within 2\%, which is not too high.
% The trend for the other number of feature cases is similar.
% For all scenarios, The MAPE of HR after ML is about 1\% to 2\% lower than that of HR before ML.

% To further analyze the effect of the number of input features on the HR estimation, the results of MLP models with the different numbers of features are provided in Fig. \ref{fig:hr_numofFeature}.
% This figure shows results in the sitting scenario and the x-axis is the number of features.
% With a larger number of features, the MAPE decreases but the ML model needs more memory and time to train and run. 
% Hence, there is a trade-off between the number of features and the estimation accuracy.
% According to the results, 10 historical PPG HRs as features are preferable as they achieve good accuracy while consuming fewer resources.
% Besides, the MAPE difference between different ML models is small (mostly within 1\%).

% We also investigate the model size changes for different numbers of features and the results are shown in Fig. \ref{fig:hr_modelsize}.
% This figure used the sitting scenario data but the trend is similar for other scenarios.
% The model size is mainly decided by the ML algorithm and the number of features.
% With more features, the ML algorithm needs more complex logic and a larger buffer to estimate HR, as a result, the ML model size might be larger. 
% From the figure, we can conclude that DT needs less memory while KNN and SVM require more memory.
% Therefore, much the same as the ISPC conclusion, we recommend DT, RF, and MLP as the estimation algorithms by making a trade-off between accuracy and memory consumption.
