%Metaphors and similes have been studied previously primarily in terms of interpretation, generation, and detection in text  \citep{aghazadeh-etal-2022-metaphors, stowe-etal-2021-metaphor, He2022CanPL, Zeng2019NeuralSR, chakrabarty-etal-2020-generating, NotRocketScience2020}. 
Recently there have been several works exploring the ability of VL-PTMs to understand similes and metaphors, and several datasets have been introduced \cite{zhang-etal-2021-multimet,Liu-and-Geigle-2022,chakrabarty2023i,hwang2023memecap}. These datasets often focus on different types of images (memes, politics, advertising), sometimes containing syntethic images \cite{MetaCLUE-2022}. In contrast, we use natural images from a search engine. In addition, our tasks introduce the new aspect of retrieval. 

\remove{
\citet{zhang-etal-2021-multimet} introduced the first large-scale multimodal dataset of metaphors. \citet{Liu-and-Geigle-2022} presented FigMemes, a dataset for figurative language classification in politically-opinionated memes. \citet{MetaCLUE-2022} annotated a visual advertisement dataset with similes as captions to introduce MetaCLUE, a set of vision tasks on visual metaphor. \citet{chakrabarty2023i} introduced a novel method to generate visual metaphors combining Large Language Models (LLMs) and Diffusion Models with Chain-of-Thought prompting. 
%Using this method, they created a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. 
\citet{hwang2023memecap} introduced the meme captioning task and released a new dataset named MemeCap, which comprises 6.3K memes with comprehensive, detailed metadata. They found that state-of-the-art Visual and Language models struggle with visual metaphors and perform substantially worse than humans.
}

%We find MetaCLUE the closest to ours concerning similes, but MetaCLUE's images are synthetic, while ours are more natural. Additionally, our tasks introduce the new aspect of ranking. 