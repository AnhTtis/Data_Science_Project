% \yonatan{is it correct to say that it's an automatic generation followed by human ratings? If so I'll start by that. Also I suggest to say something similar to what we said in VASR: this is a process that contains several heuristics and implementation decisions. We evaluate the end2end dataset generation later on, and the fact that human achieve high agreement helps to verify the correctness of the end2end process. }
%\yonatan{is it correct to say that it's an automatic generation followed by human ratings? If so I'll start by that} \ron{We didn't started like this in VASR, why is it important here?}
\begin{figure}[t!]
\includegraphics[width=0.44\textwidth ,height=\textheight,keepaspectratio]{figures/up_a_tree_bigger.JPG}
\includegraphics[width=0.44\textwidth ,height=\textheight,keepaspectratio]{figures/blanket_of_snow_bigger.jpg}
\includegraphics[width=0.44\textwidth ,height=\textheight,keepaspectratio]{figures/car_cheetah_bigger.JPG}
\caption{Examples of the multimodal figurative language detection task for idiom, metaphor, and simile. The input is a figurative phrase and four candidate images (for idiom, we also show the definition). The correct answer is marked with an orange square.}
\label{fig:first-task-idiom-figurative}
\end{figure}

\input{tables/relation-categories.tex}
Our goal is to create a dataset with idioms, metaphors, and similes paired with figurative and literal images. This dataset can then serve as a benchmark to evaluate Vision and Language models on multimodal figurative language. 

\xhdr{Labels} Initially, we intended to have our annotators label images ``literal'' or ``figurative''. However, after initial experimentation with the data generated by our pipeline, we realized the necessity of a more nuanced classification system. Hence, we introduced two additional categories.

The first new category, ``Figurative+Literal,'' encompasses images that express the figurative meaning of an expression while also maintaining some aspects of the literal interpretation. The second, ``Partial Literal,'' includes images that visualize some (literal) elements or objects from the expression. 

 Table~\ref{tab:relation-categories} illustrates our categories for the expression ``Touch wood''. For example, an image of someone literally touching wood while crossing his fingers for luck is classified as Figurative+Literal.   
This distinction also allows us to later perform a richer analysis of model performance. 

%To create multimodal idioms, we gathered idiomatic expressions from the MAGPIE corpus \citep{haagsma-etal-2020-magpie}. We then utilized a semi-automatic pipeline we developed to find figurative and literal images (\S\ref{sec:idioms-collection}). For multimodal metaphors, we collected similes and metaphors from various online sources. Subsequently, we manually collected and annotated the corresponding figurative and literal images (\S\ref{sec:metaphors_and_similes}).


\subsection{Pipeline: Idioms}
\label{sec:idioms-collection}
\input{sections/04B_collecting_idioms}


\subsubsection{Searching for Images}
\label{sec:enriching_similes_and_idioms}
\begin{figure}[b!]
%\begin{center}
\includegraphics[width=0.48\textwidth,keepaspectratio]{figures/pipeline-figure.JPG}
%\end{center}
\caption{The flow of our idiom pipeline: getting definitions, looking for image candidates using the idiom and its definitions, filtering an selecting candidate images. In the human annotation stage, blue represents Literal, Green -- Figurative, and red -- None.}
\label{fig:figurative-pipeline}
\end{figure}
\input{sections/04BA_enriching_similes_and_idioms}
\subsubsection{Image Filtering}
\label{sec:choosing_images}
\input{sections/04BB_choosing_images}
\subsubsection{Human Annotation}
\label{sec:human_annotation}
\input{sections/04BC_human_annotation}

\subsection{Pipeline: Metaphors and Similes}
\label{sec:metaphors_and_similes}
\input{sections/04C_metaphors_and_similes}

% We collected $35$ textual metaphors and $142$ textual similes along with their definitions from online sources. Next, we used the metaphors and similes definitions as search queries to search for figurative and literal images. We manually annotated the resulting images into ``Figurative'' and ``Partial Literal'' categories. In total, we obtained $1107$ figurative images and $1816$ partial literal images for similes, and $333$ figurative images and $729$ literal images for metaphors. 








% We verify the correctness of our dataset on different tasks in the human evaluation section \ref{sec:human_evaluation}. 

% We collected $628$ idioms from the MAGPIE corpus \citep{haagsma-etal-2020-magpie} of idiomatic expressions. The MAGPIE corpus contains $56,622$ crowdsourced potentially idiomatic expressions, covering $1,756$ unique idioms that appear in at least two of the following dictionaries: Wiktionary, Oxford Dictionary of English Idioms, and UsingEnglish.com. After collecting the idioms, we feed them into the pipeline as input. The first step is to collect the idioms' definitions from Wiktionary and Oxford dictionaries and construct search queries to find literal and figurative candidate images (\S\ref{sec:enriching_similes_and_idioms}). The next step is to select the best literal and figurative candidates for annotation using various heuristics and implementation decisions elaborated at (\S\ref{sec:choosing_images}). After collecting the best figurative and literal candidate images for the idioms, AMT workers annotated the different relations (Table~\ref{tab:relation-categories}) between each idiom and its candidate image, thus creating the IRFL dataset (\S\ref{sec:human_annotation}).

% We evaluate the end-to-end dataset generation, and the fact that humans achieve high agreement helps verify the end-to-end process's correctness. \\\\
% To collect metaphors and similes images, we collected $35$ textual metaphors and $142$ textual similes along with their definitions from the internet. Next, we used the metaphors and similes definitions as search queries and adapted the method used in (\S\ref{sec:choosing_images}) to search for figurative and literal images. We manually annotated the resulting images into ``Figurative'' and ``Literal'' categories. In total, we obtained $1107$ figurative images and $1816$ literal images for similes, and $333$ figurative images and $729$ literal images for metaphors. We verify the correctness of our dataset on different tasks in the human evaluation section \ref{sec:human_evaluation}. 



% Older
% Our goal is to generate the IRFL dataset of idioms, metaphors, and similes with matching figurative and literal images and evaluate the figurative understanding and preference of Vision and Language models. To collect figurative and literal images for idioms, we developed an automatic pipeline that takes a list of idioms as input and outputs figurative and literal candidate images. We collected $628$ idioms from the MAGPIE corpus \citep{haagsma-etal-2020-magpie} of idiomatic expressions. The MAGPIE corpus contains $56,622$ crowdsourced potentially idiomatic expressions, covering $1,756$ unique idioms that appear in at least two of the following dictionaries: Wiktionary, Oxford Dictionary of English Idioms, and UsingEnglish.com. After collecting the idioms, we then feed them into the pipeline as input. First, we collect the definitions of these idioms from Wiktionary and Oxford dictionaries and construct search queries to find possible literal and figurative images (\S\ref{sec:enriching_similes_and_idioms}). Next, we select the best literal and figurative candidates for annotation using various heuristics and implementation decisions elaborated at (\S\ref{sec:choosing_images}). AMT workers annotated the different relations between each idiom and its candidate images, creating the IRFL dataset (\S\ref{sec:human_annotation}). We evaluate the end-to-end dataset generation, and the fact that humans achieve high agreement helps to verify the correctness of the end-to-end process. The relation categories can be seen with corresponding explanations and images in Table~\ref{tab:relation-categories}.

% To collect metaphors and similes' images, we collected $35$ textual metaphors and $142$ textual similes from the internet. First, we collected metaphors and similes definitions and used them as search queries and adapted the method used to search images in (\ref{sec:choosing_images}). Next, we annotated these images into ``Figurative'' and ``Literal'' categories. In total, we obtained $1107$ figurative images and $1816$ literal images for similes, and $333$ figurative images and $729$ literal images for metaphors. We verify the correctness of our dataset on different tasks in the human evaluation section \ref{sec:human_evaluation}. 





%\subsection{Dataset Analysis}
%\label{sec:dataset_statistics}
%\input{sections/04E_dataset_statistics}





