\input{tables/dataset_statistics.tex}
We hired Amazon Mechanical Turk (AMT) workers to annotate the relation between each idiom and its candidate images using the user interface seen in Appendix~\ref{sec:annotation_ui} (Figure~\ref{fig:image-task-ui}). Five workers annotated each image in batches of five images per sample. They received a payment of \$$0.15$ per sample, which resulted in an average hourly wage of \$$15$. We created a qualification test\footnote{https://irfl-dataset.github.io/mturk/image/qualification} to select quality annotators and provided them with an interactive training platform\footnote{https://irfl-dataset.github.io/mturk/image/train} to understand the task and the different categories better. 

We split the annotation process into batches with an average size of $60$ idioms per batch. After each batch, we provided each worker with a personal profile page (Appendix~\ref{sec:annotation_ui}, Figure~\ref{fig:profile-page-ui}) to view their statistics and some examples where their choice was different from the majority of workers. 
%We also set up a leaderboard (Figure~\ref{fig:image-task-leaderboard}, Appendix~\ref{sec:annotation_ui})  that was updated after each batch to improve their competitiveness. 

Full annotation results and statistics are presented in Table \ref{tab:dataset-statistics}. 
%The nature of this task is very subjective. 
%We further discuss this in (Appendix~\ref{sec:annotation_task_discussion}).
% \dnote{why do you have discussion in the appendix? this doesn't make much sense}
Despite the subjective nature of the task, in $94\%$ of the instances, there was a majority of $3$ workers or more out of $5$ compared to a random chance of $29\%$. %This shows that different people can see the same connection most of the time. 







