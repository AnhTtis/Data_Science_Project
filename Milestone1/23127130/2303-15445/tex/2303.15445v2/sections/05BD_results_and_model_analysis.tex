% \yonatan{add paragraphs with the main title findings of each main finding}
Zero-shot results on the ``mixed'' multimodal figurative language detection task are presented in Table \ref{tab:mixed-single-choice-results}. The best model achieved $22\%$, $30\%$, and $66\%$ accuracy on the idioms\footnote{Idioms were passed along with their definitions as input.}, metaphors, and similes tasks compared to a random chance of $25\%$. These results suggest that \textbf{models do not understand the connection between a figurative phrase and an image as humans do.} We next conduct a fine-grained analysis to examine if models failed because they do not see any connection to the figurative images or rather because they prioritize literal connections over figurative ones.

\input{tables/mixed_single_choice_task_results_table.tex}

\textbf{Models prefer partially literal images over figurative ones.}
We analyze the models' choices on the ``mixed'' multimodal figurative language detection task and found that in all models, a partially literal distractor was selected in $92\%-100\%$ of the instances where the models failed across all figures of speech (idioms, metaphors, and similes). This shows that models prefer partially literal images over figurative ones. { We find the case of idioms to be particularly interesting. Models receive a relatively long prompt (idiom+definitions), and often choose an image that is a literal interpretation of only 1-2 words from the prompt.}

\input{tables/random-idioms-single-choice.tex}

\textbf{Models partially understand the figurative connection between idioms and images.}
To examine whether models can comprehend a figurative connection between an image and an idiom, we experiment with random candidates and several configurations of the multimodal figurative language detection task (Table~\ref{tab:random-idiom-single-choice-results}). When provided with an idiom and its definitions as input, the accuracy on the Figurative category ranges between $75\%-87\%$ with $2$ candidates and $58\%-71\%$ with $4$ candidates. These results are above chance level but still below human performance on the ``mixed'' task. 

When given the idiom alone as input, the accuracy ranges between $56\%-67\%$ with $2$ candidates and $25\%-46\%$ with $4$ candidates. These results suggest that models partially understand the figurative connection between idioms and images. We see a significant performance drop with all models when the number of candidates increases.

In the Figurative+Literal category, with only the idiom as input, models registered an accuracy of $65\%-78\%$ with $4$ candidates. This performance significantly exceeds the accuracy recorded on the Figurative category with $2$ and $4$ candidates. The performance increase can be explained by the fact that Figurative+Literal images have both a literal and figurative connection to the phrase.

\input{tables/random-metaphors-single-choice.tex}
\textbf{Models understand metaphors but fail to reach human performance.}   
Table~\ref{tab:random-similes-metaphors-single-choice-results} shows the models' performance on metaphors with random candidates. The accuracy of all models on the Figurative category with $2$ candidates is $72\%-88\%$, and $53\%-76\%$ with $4$ candidates. We see a significant performance drop with all models when the number of candidates increases. The results suggest that models can understand metaphors but fail to reach human performance.

\textbf{Models understand similes well.}
Table~\ref{tab:random-similes-metaphors-single-choice-results} shows the models' performance on the similes with random candidates. The accuracy of all models on the Figurative category with $2$ candidates is $95\%-99\%$, and $88\%-98\%$ with $4$ candidates. Models' performance is competitive with that of humans, and the models maintain their performance when increasing the number of candidates. 
In contrast to the multimodal figurative language detection task with random images, the ``mixed'' task shows a performance gap between closed and open similes due to open similes concealing the compared property, making it harder for the model to choose the figurative image. Analyzing the ``mixed'' task results on closed similes, we found that figurative images scored higher than source concept images in $52\%-74\%$ of cases across all models. 

Additionally, source concept images scored higher than target concept distractor images in $51\%-70\%$ of cases. This pattern suggests a model prioritization for simile images: firstly, target concept images with the compared property, then source concept images, and finally, target concept images lacking the compared property.

%OLD -  As we analyzed the ``mixed'' X results on closed similes in more depth, we found that across all models excluding LiT, $55\%-61\%$ of the figurative images received a higher matching score than the source concept images. In addition, $50\%-66\%$ of the source concept images received a higher matching score than the target concept distractor image. These suggest that models prioritize simile images in the following order: 1) images of the target concept with the compared property, 2) images of the source concept, 3) images of the target concept without the compared property.
\input{tables/understanding_task_supervision.tex}

\textbf{Fine-tuning improves figurative understanding and reduces literal preference.} The supervised model results are presented in Table~\ref{tab:understanding_task_supervision}. Previously we did not display the models' performance on the ``mixed'' task when taking the idiom alone as input due to their poor performance ($5\%-7\%$ accuracy). However, when training on idioms alone,  the supervised model scored a mean accuracy of $46.2\%$, $9\times$ the zero-shot score of $5\%$. This large performance increase might suggest that VL-PTMs representation of an idiom encodes its definitions. 

Training and testing with the idiom and its definitions as input resulted in a mean accuracy of $58\%$ compared to $16\%$ in the Zero-shot configuration. After analyzing the supervised model results, we found that its literal preference has improved significantly. In $41\%\pm4.3$ of the instances where the model failed, a partially literal distractor was selected compared to $96\%$ in the zero-shot configuration. Along with this improvement in literal preference, Figurative+Literal category accuracy raised from $41\%$ in zero-shot to $49\%$. These results show that models can improve their preference for partially literal images and recognize idiomatic figurative connections better via training. Moreover, the results suggest that the data is a useful training signal for our task.

We have discovered that VL-PTMs tend to prefer partially literal images. In the next section, we design a task to tackle this issue. 


