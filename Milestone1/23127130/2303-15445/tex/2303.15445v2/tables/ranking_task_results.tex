\begin{table}[tp]
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
Categories  &     \multicolumn{2}{c}{Idioms}                   &  Metaphors         & \multicolumn{2}{c}{Similes} \\ \midrule
                & \begin{tabular}[c]{@{}l@{}}Fig. \\ Lit.\end{tabular}  & Fig.          &                   & Cl.        &       Op. \\ \midrule
CLIP-VIT-L/14   &       57            &    37                 & 26               & 44          &        \textbf{34} \\
CLIP-VIT-B/32   &       54            &    36                 & 22               & 38          &        30 \\
CLIP-RN50       &       54            &    37                 & 25               & 38          &        31\\
CLIP-RN50x64    &  \textbf{61}        &    \textbf{39}        & \textbf{29}      & 43          &        32\\
BLIP            &       58            &    \textbf{39}        & 24               & \textbf{54} &        33\\
BLIP2           &       57            &    \textbf{39}        & 22               & 42          &        29\\
CoCa ViT-L-14   &       56            &    36                 & 20               & 39          &        24\\ \midrule
ViLT            &        -            & -                     & 25               & 34          &        28\\ \midrule
\# Phrases   & 94                 & 149                    & 35               & 142            &        137\\ \bottomrule
\end{tabular}
\caption{Models performance on the multimodal figurative language retrieval task, the scoring metric is mean precision at $k$, where $k$ is the number of figurative images. There are two columns for idioms and similes. "Closed" and "Open" refers to the simile type. "Figurative" and "Figurative+Literal" refer to the correct image category. The results are low, we expect better results from models with proper figurative preferences.}
\label{tab:ranking-task}
\end{table}
