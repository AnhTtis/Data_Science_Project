We noted that many of the retrieved images contained the search query in textual form. We used optical character recognition (OCR) tool EasyOCR to extract text from the images, and TextBlob to correct spelling errors the OCR made. We then  filtered images that contained objects or entities from the idiom  or its definitions in textual form (50\% of the images). Such images are problematic because they may cause the model to select an image solely based on its textual signal. Following this filter, 15\% of the resulting images contained mostly text. To tackle this problem, we used OCR (See Appendix~\ref{sec:documents_filter}) to remove images with more than a couple of words, as well as images with more than 30\% of their space containing text. 

For the remaining images, we calculated the matching score of each image with its phrase and search query using ViLT. Top-$k$ images with a high ``phrase-image'' score (that passed a threshold, see Appendix~\ref{sec:literal_threshold}) were tagged as potentially literal.  We chose the top $k$ images with the highest ``definition-image'' score as Figurative candidates. 

%AMT workers then annotated the relation between the figurative phrase and its Figurative and Literal candidate images using the user interface (UI) seen in Figure~\ref{fig:image-task-ui},  Appendix~\ref{sec:annotation_ui}.
% Old_v2
% To find figurative images for our search queries, we searched Google images \footnote{Images were searched with ``SafeSearch'' flag ``on'', and in ``United States'' region.}, taking up to $20$ images per search query. About 50\% of the resulting images contained part of the idiom they were derived from or its definitions. Such images are problematic because they may cause the model to select an image solely based on its textual signal. We filtered out these images using the optical character recognition (OCR) tool EasyOCR and TextBlob library to correct any spelling errors the OCR had. Following this filter, 15\% of the resulting images were ``garbage'' images, mainly containing text, including artworks, letters, postcards, newspapers, and articles. To tackle this problem, we used OCR and an additional method (See Appendix~\ref{sec:documents_filter}) to remove images with more than a couple of words and images with text size that exceeds more than 30\% of the image size. For the remaining images, we calculated the matching score of each image with its phrase and search query using ViLT. Images with a ``phrase-image'' score that passed a certain literal threshold (Appendix~\ref{sec:literal_threshold}) were tagged as ``literal'', and from these images, we chose the top $K$ images as literal candidates. From the non ``literal'' images, we chose the top $K$ images with the highest ``search query-image'' score as Figurative candidates. AMT workers then annotated the relation between the figurative phrase and its Figurative and Literal candidate images using the user interface (UI) seen in Figure~\ref{fig:image-task-ui},  Appendix~\ref{sec:annotation_ui}.

% Old
% To find figurative images for our search queries, we searched Google images \footnote{Images were searched with ``SafeSearch'' flag ``on'', and in ``United States'' region.}, taking up to $20$ images per search query. The resulting images often included problematic images with part of the idiom they were derived from or its definitions were written. These images were problematic because a model may see a connection between an idiom and an image solely based on the textual signal that appears in it. Such images were filtered out by using the optical character recognition (OCR) tool EasyOCR and TextBlob library to correct any spelling errors the OCR had. Many ``garbage'' images with mostly text also appeared in the search results, including letters, postcards, newspapers, and articles. To tackle this problem, we used OCR to remove images with more than a couple of words and images with a text size bigger than 30\%. In addition, we removed images of documents that the OCR failed to detect using ViLT model (Appendix~\ref{sec:documents_filter}). Next, we calculated the matching score of each image that passed these filterers with its phrase and search query using ViLT. Images with a ``phrase-image'' score that passed a certain literal threshold (Appendix~\ref{sec:literal_threshold}) were tagged as ``literal'', and from these images, we chose the top K images as literal candidates. From the non ``literal'' images, we chose the top K images with the highest ``search query-image'' score as Figurative candidates. AMT workers then annotated the relation between the figurative phrase and its Figurative and Literal candidates using the user interface (UI) seen in Figure~\ref{fig:image-task-ui},  Appendix~\ref{sec:annotation_ui}.
