%Previous work on idioms focused on the detection, interpretation, and representation of textual idioms \citep{FalzyEtAl2009, verma-vuppuluri-2015-new, peng-feldman-2016-experiments, salton-etal-2016-idiom, LiuEtAl2017, li-sporleder-2009-classifier, liu-etal-2017-idiom, liu-hwa-2016-phrasal, zhou-etal-2021-pie}. 
Several papers have examined pre-trained LMs' ability to represent idioms. \citet{ShwartzandDagan2019} found that LMs' representation of idiomatic expressions was of lower quality than that of literal ones. \citet{NotRocketScience2020} introduced a narrative understanding benchmark focused on interpreting figurative language and found that pre-trained LMs struggle to perform well in zero-shot and few-shot settings. To the best of our knowledge, Vision and Language Pre-trained models (VL-PTMs) understanding of idioms has not been investigated until this work.