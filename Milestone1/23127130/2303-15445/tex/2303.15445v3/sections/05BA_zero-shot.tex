We evaluate several state-of-the-art vision-and-language models. We use four versions of CLIP models \cite{radford2021learning}: RN50, ViT-B/32, ViT-L/14, and RN50x64/14 with 100M, 150M, 430M, and 620M parameters, respectively. We use the official implementations of ViLT \citep{kim2021vilt}, BLIP \citep{li2022blip}, CoCa ViT-L-14 \citep{yu2022coca}, and BLIP2 \citep{li2023blip2}. We evaluate all models with their default hyper-parameters, except ViLT on idioms, due to its maximum sequence length of 40 tokens. 

The models encode the figurative phrase and the image, producing a matching score for each pair. We choose the image with the highest score as the one best matches the expression.

We also experimented with multimodal chatbot models, including LLaVA \cite{liu2023visual}, InstructBLIP \cite{dai2023instructblip}, OpenFlamingo \cite{awadalla2023openflamingo}, and Otter \cite{li2023otter}. We found that the first two do not support our setting, as they can not handle questions about multiple images; the latter two do support the setting, but did not seem to understand the task, returning mostly nonsense answers. 

% \begin{enumerate}
%     \item CLIP \citep{radford2021learning} is pre-trained with a contrastive objective that can be used without directly optimizing for the task. We use four versions of models with different amounts of parameters: RN50, ViT-B/32, ViT-L/14 and RN50x64/14 with 100M, 150M, 430M and 620M parameters, respectively (RN50 was used during data collection).
%     \item CLIP-ViL \citep{shen2021much}, with 290M parameters, is a pre-trained vision-and-language model that uses CLIP as a visual backbone, rather than CNN based visual encoders that are trained on a small set of manually annotated data.
%     \item ViLT \citep{kim2021vilt}, with 111M parameters, incorporates text embeddings into a Vision Transformer (ViT).
    
% \end{enumerate} 