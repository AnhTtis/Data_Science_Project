We introduce two novel tasks of multimodal figurative language detection and retrieval to evaluate humans and state-of-the-art models' ability to understand figurative language (\S\ref{sec:understanding_task}).

The {\bf  Multimodal Figurative Language Detection Task} evaluates VL-PTMsâ€™ ability to understand the relation between an image and a figurative phrase. The task is to choose the image that best visualizes the figurative phrase out of X candidates. Figure~\ref{fig:first-task-idiom-figurative} shows an example of the task for an idiom, a metaphor, and a simile.

The {\bf Multimodal Figurative Retrieval Task} examines VL-PTMs' preference for figurative images. The task is to rank the figurative and partially literal images using the model-matching score and calculate the precision at $k$, where $k$ is the number of figurative images. Figure~\ref{fig:second-task-idiom-figurative-vs-caption} shows an example of the task for the idiom ``ruffle someone's feathers''.

We show that IRFL tasks are easy for humans (97\% accuracy) and challenging for models (<22\%). Additionally, we provide a detailed analysis per figure of speech, experiments with idioms and their definitions as input, and with different candidate types. We find that models fail the IRFL task due to their preference for partially literal images over figurative images and introduce a retrieval task to tackle this problem (\S\ref{sec:ranking Task Analysis}). In addition, we examine the ability of generative models such as Dall-E and Stable Diffusion to generate figurative images for idioms (\S\ref{sec:genearive_models_analysis}). We find that they are unable to generate figurative images given idiomatic phrases. However, given the definitions of an idiom, generative models can generate figurative images.




