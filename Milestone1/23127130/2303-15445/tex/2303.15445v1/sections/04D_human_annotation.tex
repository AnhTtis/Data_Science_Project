We hired Amazon Mechanical Turk workers to annotate the relation between each idiom and its candidate images. Five workers annotated each image, the images were annotated in batches of five for the reward of \$0.15 for batch. We created a difficult qualification test \footnote{https://irfl-dataset.github.io/mturk/image/qualification} to select quality annotators and provided them with an interactive training platform \footnote{https://irfl-dataset.github.io/mturk/image/train} to understand the task and the different categories better. We split the annotation process into batches with an average size of 60 idioms per batch. After each batch, We provided each worker with a personal profile page \footnote{https://irfl-dataset.github.io/profile/example} to view its statistics and some handily picked examples where his choice was distant from a majority of four workers. We also provided workers with a leaderboard \footnote{https://irfl-dataset.github.io/mturk/leaderboard} that was updated after each batch to improve their competitiveness. Full annotation results and statistics are presented in Table \ref{tab:dataset-statistics}.
\\\\
The nature of this task is very subjective, and often the relation worker A sees between an idiom, and an image differs from the relation worker B see. We provide further discussion about this aspect of the task in (Appendix~\ref{sec:annotation_task_discussion}). Despite the subjective aspect of the task and its complexity in distinguishing between the various categories, in 94\% of the instances, there was a majority of 3 workers or more compared to a random chance of 29\%. This shows that different people can see the same connection most of the time. 

\input{tables/dataset_statistics.tex}




