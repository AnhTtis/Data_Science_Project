We evaluate several diverse state-of-the-art vision-and-language models. Due to ViLT's maximum sequence length of 40, we do not evaluate it on idioms. In all cases described below (except CLIP-ViL), the model encodes the figurative phrase and the image and produces a matching score for each pair. We chose the image that results the highest matching score as the image that best matches the figurative expression. 
\begin{enumerate}
    \item CLIP \cite{radford2021learning} is pre-trained with a contrastive objective that can be used without directly optimizing for the task. We use four versions of models with different amounts of parameters: RN50, ViT-B/32, ViT-L/14 and RN50x64/14 with 100M, 150M, 430M and 620M parameters respectively (RN50 was used during data collection).
    \item CLIP-ViL \cite{shen2021much}, with 290M parameters, is a pre-trained vision-and-language model that uses CLIP as a visual backbone, rather than CNN based visual encoders that are trained on a small set of manually annotated data.
    \item ViLT \cite{kim2021vilt}, with 111M parameters, incorporates text embeddings into a Vision Transformer (ViT).
    
\end{enumerate} 