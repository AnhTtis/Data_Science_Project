% \yonatan{add paragraphs with the main title findings of each main finding}
Zero-shot results on the ``mixed'' figurative understanding task are presented in Table \ref{tab:mixed-single-choice-results}. The best model achieved $27\%$, $30\%$, and $52\%$ accuracy on the idioms\footnote{Idioms were passed along with their definitions as input.}, metaphors, and similes tasks compared to a random chance of $25\%$. \textbf{These results suggest that models do not understand the connection between a figurative phrase and an image like humans do.} We conduct a fine-grained analysis to examine if models failed the ``mixed'' understanding task because they do not see any connection to the figurative images or rather because they prioritize ``weak'' literal connections over figurative ones. \\
\input{tables/mixed_single_choice_task_results_table.tex}
\noindent \textbf{Models prefer partially literal images over figurative ones.}
We analyzed the models' choices on the ``mixed'' figurative understanding task and found that in all models (excluding LiT on idioms and similes), a partially literal distractor was selected in $92\%-100\%$ of the instances where the models failed across all figures of speech (idioms, metaphors, and similes) . This shows that models prefer partially literal images over figurative ones.\textbf{ We find the case of idioms to be particularly interesting in this regard. Models receive a relatively long prompt containing both the idiom and its definitions as input. Instead of picking an image that fits the prompt semantically, they choose an image that is literal to one or two words.}\\
%\ron{@Dafna, should I add here a full table of this data? maybe in Appendix? (It will be 6X4)}% 
\input{tables/random-idioms-single-choice.tex}
\noindent \textbf{Models partially understand the figurative connection between idioms and images.}
To examine whether models can comprehend a figurative connection between an image and an idiom, we experiment with random candidates and several configurations of the understanding task (Table~\ref{tab:random-idiom-single-choice-results}). The accuracy score on the Figurative category with $2$ candidates is $61\%-87\%$, and $22\%-71\%$ with $4$ candidates. These results are marginally above random chance but still below human performance on the ``mixed'' task. When given the idiom alone as input, most models achieved $80\%-84\%$ with $2$ candidates and $30\%-46\%$ with $4$ candidates compared to random chance of $50\%$ and $25\%$. These results suggest that models partially understand the figurative connection between idioms and images. Moreover, we see a significant performance drop with all models when increasing the number of candidates.\\\\
In the Figurative Literal category, models achieve a $65\%-78\%$ accuracy score with 4 candidates, significantly higher than the performance in the Figurative category with $2$ and $4$ candidates. These results can be explained by the fact that Figurative Literal images possess a literal connection to the phrase in addition to a figurative one. \\
\input{tables/random-metaphors-single-choice.tex}
\noindent \textbf{Models understand metaphors, but fail to reach human performance.}   
Table~\ref{tab:random-similes-metaphors-single-choice-results} shows the models' performance on the metaphors figurative understanding task with random candidates. The accuracy score of all models, excluding LiT, on the Figurative category with $2$ candidates is $72\%-88\%$, and $53\%-76\%$ with $4$ candidates. We see a significant performance drop with all models when increasing the number of candidates. The results suggest that models understand metaphors but fail to reach human performance. \\\\
\noindent \textbf{Models understand similes as well as humans.}
Table~\ref{tab:random-similes-metaphors-single-choice-results} shows the models' performance on the similes figurative understanding task with random candidates. The accuracy score of all models, excluding LiT, on the Figurative category with $2$ candidates is $96\%-99\%$, and $91\%-97\%$ with $4$ candidates. Models' performance is competitive with that of humans, and the models maintain their performance when increasing the number of candidates. We note that we experiment with open similes where the compared property is explicitly mentioned in the simile. Thus the Figurative images can be seen as Figurative Literal. As we analyzed the ``mixed'' understanding task results in more depth, we found that across all models excluding LiT, $55\%-61\%$ of the figurative images received a higher matching score than the source concept images. In addition, $50\%-66\%$ of the source concept images received a higher matching score than the target concept distractor image. These suggest that models prioritize simile images in the following order: 1) images of the target concept with the compared property, 2) images of the source concept, 3) images of the target concept without the compared property.\\\\
\noindent \textbf{Fine-tuning improves figurative understanding and reduces partially literal preference.} Fine-tuning results are presented in Table~\ref{tab:understanding_task_supervision}. The mean Figurative category accuracy is $58\%$ compared to $13\%$ in the Zero-shot configuration. We analyzed the fine-tuned model results and compared them to the zero-shot configuration and found that in $41\%\pm4.3$ of the instances where the model failed, a partially literal distractor was selected compared to $96\%$ in the zero-shot configuration. Along with this improvement in literal preference, Figurative Literal category accuracy raised from $41\%$ in zero-shot to $49\%$. These results show that models can moderate their preference for partially literal images and recognize idiomatic figurative connections better, using extensive training. Moreover, the results suggest that the data is a valuable training signal for this task.
\input{tables/understanding_task_supervision.tex}



