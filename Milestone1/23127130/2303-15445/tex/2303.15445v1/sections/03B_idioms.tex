Previous work on idioms focused on the detection, interpretation, and representation of textual idioms \cite{FalzyEtAl2009, verma-vuppuluri-2015-new, peng-feldman-2016-experiments, salton-etal-2016-idiom, LiuEtAl2017, li-sporleder-2009-classifier, liu-etal-2017-idiom, liu-hwa-2016-phrasal, zhou-etal-2021-pie}. Recently, several papers have examined the ability of pre-trained LMs to represent idioms. Shwartz and Dagan \cite{ShwartzandDagan2019} found that LMs' representation of idiomatic expressions was of lower quality than that of literal ones. Chakrabarty at el. \cite{NotRocketScience2020} introduced a narrative understanding benchmark focused on interpreting figurative language and found that pre-trained LMs irrespective of their size, struggle to perform well in zero-shot and few-shot settings. However, to the best of our knoweldge, Vision and Language Pre-trained models (VL-PTMs) understanding of idioms has not been investigated until this work.