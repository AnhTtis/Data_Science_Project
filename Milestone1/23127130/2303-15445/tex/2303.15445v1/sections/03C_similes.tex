Metaphors and similes have been studied previously primarily in terms of interpretation, generation, and detection of textual metaphors and similes \cite{aghazadeh-etal-2022-metaphors, stowe-etal-2021-metaphor, He2022CanPL, Zeng2019NeuralSR, chakrabarty-etal-2020-generating, NotRocketScience2020}. Recently there have been several works focusing on the ability of VL-PTMs to understand similes and metaphors. Zhang et al. \cite{zhang-etal-2021-multimet} introduced the first large-scale multimodal dataset of metaphors. Liu and Giegle \cite{Liu-and-Geigle-2022} presented FigMemes, a dataset for figurative language classification in politically-opinionated memes. Akula et al. \cite{MetaCLUE-2022} annotated a visual advertisement dataset with similes as captions to introduce MetaCLUE, a set of vision tasks on visual metaphor. We find MetaCLUE the closest to ours concerning similes. The key difference between IRFL and MetaCLUE is the tasks and images. MetaCLUE's images are synthetic, while ours are more natural. Additionally, our tasks introduce a new aspect of preference (literal vs. figurative) into multimodal metaphorical understanding. 