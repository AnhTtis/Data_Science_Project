% Modified version
Figures of speech such as metaphors, similes, and idioms allow language to be expressive, invoke emotion, and communicate abstract ideas that might otherwise be difficult to visualize. These figurative forms are often conveyed through multiple modes, such as text and images, and frequently appear in advertising, news, social media, etc. Understanding multimodal figurative language is an essential component of human communication, and it plays a significant role in our daily interactions. While humans can intuitively understand multimodal figurative language, this poses a challenging task for machines that requires the cognitive ability to map between domains, abstraction, commonsense, and profound language and cultural knowledge. In this work, we propose the Image Recognition of Figurative Language dataset to examine vision and language models' understanding of figurative language. We leverage human annotation and an automatic pipeline we created to generate a multimodal dataset and introduce two novel tasks as a benchmark for multimodal figurative understanding. We experiment with several baseline models and find that all perform substantially worse than humans. We hope our dataset and benchmark will drive the development of models that will better understand figurative language.

% Previous version + Yonatan notes
%Figures of speech such as metaphors, similes, and idioms allow language to be expressive, invoke emotion, and communicate abstract ideas that might otherwise be difficult to visualize. These figurative forms are often conveyed through multiple modes, such as text and images, and frequently appear in advertising, news, social media, etc. Understanding multimodal figurative language is an essential component of human communication, and it plays a significant role in our daily interactions. While humans can intuitively understand multimodal figurative language, this poses a challenging task for machines that requires the cognitive ability to map between domains, abstraction, commonsense, and profound language and cultural knowledge. \yonatan{I think this part should come earlier, try to make the previous part shorter and more dense (consider chatgpt)}In this work, we propose the IRFL dataset to examine vision and language models' understanding of figurative language. We leverage human annotation and an automatic pipeline we created to generate a multimodal dataset and introduce two novel tasks as a benchmark for multimodal figurative understanding. We experiment with several baseline models and find that all perform substantially worse than humans. \yonatan{Try to add something meaningful / interseting about the results, it currently looks a bit generic} We hope our dataset and benchmark will drive the development of models with better figurative understanding.


% First iteration with notes:%
%Figures of speech allow language to be expressive, invoke emotion, and communicate abstract ideas that might otherwise be difficult to visualize. Figurative language is frequently accompanied by images in advertising, news, social media, etc. Vision-Language pre-trained models’ understanding of images and figurative language has not been thoroughly examined \dnote{you never said why it should have}. In this work, we leverage human annotation, ViLT \dnote{?}, and other tools \dnote{?} to generate a multimodal dataset and introduce two novel tasks as benchmark of figurative understanding and figurative preference \dnote{nobody knows what this means} to examine the understanding of such models. We experiment with several baseline models and find that all perform substantially worse than humans. We hope our dataset and benchmark will drive the development of models with better figurative understanding. \dnote{not enough motivation}
 
 % First iteration orignal:%
 %\noindent Figures of speech allow language to be expressive, communicate abstract Ideas that might otherwise be difficult to visualize, and invoke emotion. Figurative language frequently appears together with the vision modal in advertising, newspapers, news websites, social media, memes, and cartoons. Vision-Language Pre-Trained Models’ (VL-PTMs) understanding of figurative language combined with vision has not been thoroughly examined, if at all. In this work, we leverage human annotation, ViLT, and other tools to generate a multimodal figurative dataset and introduce two novel tasks as benchmark of figurative understanding and figurative preference to examine the understanding of such models. We experiment with several baseline models and find that all perform substantially worse than humans. We hope our dataset and benchmark will drive the development of models with better figurative understanding.

%OLD%
%Figurative language allow language to be expressive, communicate abstract ideas that might otherwise be difficult to visualize, and invoke emotion. Metaphors and similes map across domains to enable the concept of thinking of the source domain in terms of the target domain. Furthermore, understanding idioms require a profound language and cultural familiarity since their meaning cannot be deduced from the definitions of the component words. Humans intuitively understand these forms of figurative language, and widely employ them in everyday communication.  Recently, Language Pre-trained Models (PLMS) have been extensively studied for their capability to understand and recognize figurative forms. However, Vision-Language Pre-Trained Models' (VL-PTMs) understanding of figurative language combined with vision has not been thoroughly examined, if at all. We introduce a novel task of figurative text-image matching. Given a figurative phrase (idiom, simile or metaphor), choose the image that best visualizes it from X candidates. Unlike previous work on figurative language. We leverage CLIP and other tools to generate over 5000 silver-label tasks. We crowd-source annotations to create a gold-standard test-set of 4000 annotations, achieving high human accuracy (95\%). We experiment with several baseline models and find that all perform substantially worse (~50\%) than humans (~90\%) on these tasks. We hope our dataset will drive the development of models with better figurative understanding.%