% \yonatan{is it correct to say that it's an automatic generation followed by human ratings? If so I'll start by that. Also I suggest to say something similar to what we said in VASR: this is a process that contains several heuristics and implementation decisions. We evaluate the end2end dataset generation later on, and the fact that human achieve high agreement helps to verify the correctness of the end2end process. }
%\yonatan{is it correct to say that it's an automatic generation followed by human ratings? If so I'll start by that} \ron{We didn't started like this in VASR, why is it important here?}
Our goal is to introduce the IRFL dataset of idioms, metaphors, and similes with matching figurative and literal images and evaluate the figurative understanding and preference of Vision and Language models. To collect figurative and literal images for idioms, we developed an automatic pipeline that takes a list of idioms as input and outputs figurative and literal candidate images. We collected idioms from the MAGPIE corpus \cite{haagsma-etal-2020-magpie} of idiomatic expressions collected from Wiktionary, Oxford Dictionary of English Idioms, and UsingEnglish.com. The MAGPIE corpus contains 56,622 crowdsourced potentially idiomatic expressions, covering 1,756 unique idioms that appear in at least two of the dictionaries mentioned above. After collecting the idioms, we then feed them into the pipeline as input. First, we collect the definitions of the idioms from Wiktionary and Oxford dictionaries and construct search queries to find possible literal and figurative images (\ref{sec:enriching_similes_and_idioms}). The process of choosing figurative and literal candidates involves several heuristics and implementation decisions elaborated at (\ref{sec:choosing_images}). %At this point, the IRFL dataset contains figurative candidate images with a $41\%$ probability of being figurative. 
We annotate the different relations between each idiom and its candidate images, thus creating the IRFL dataset (\ref{sec:human_annotation}). We evaluate the end2end dataset generation, and the fact that humans achieve high agreement helps to verify the correctness of the end2end process. The relation categories can be seen with a corresponding explanation in Table~\ref{tab:relation-categories}.
\\\\
To collect metaphors and similes' images, we collected $35$ textual metaphors and $142$ textual similes from the internet. First, we constructed manual search queries and adapted the method used to search images in (\ref{sec:choosing_images}). Next, we annotated these images into ``Figurative'' and ``Literal'' categories. In total, we obtained $1107$ figurative images and $1816$ literal images for similes, and $333$ figurative images and $729$ literal images for metaphors. We verify the correctness of our dataset on different tasks in human evaluation section \ref{sec:human_evaluation}. 
\input{tables/relation-categories.tex}


\subsection{Search Queries}
\label{sec:enriching_similes_and_idioms}
\input{sections/04B_enriching_similes_and_idioms}
\subsection{Choosing Images}
\label{sec:choosing_images}
\input{sections/04C_choosing_images}
\subsection{Human Annotation}
\label{sec:human_annotation}
\input{sections/04D_human_annotation}
%\subsection{Dataset Analysis}
%\label{sec:dataset_statistics}
%\input{sections/04E_dataset_statistics}





