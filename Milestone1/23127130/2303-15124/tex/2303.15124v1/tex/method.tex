\section{Method}
\subsection{Overview}

The blind image inpainting task can be described as follows. Given an input corrupted image $I$ with artificial markers, we aim to learn a reconstruction network $f_{\theta}$ to obtain a clean image $\hat{I}$ with artificial markers removed, where $\theta$ are the network parameters to be learned. This blind inpainting task is different from the general inpainting task since the masks of corrupted regions are not provided in the inference stage. 

In this paper, we introduce a novel blind inpainting framework for artificial marker removal in medical images, as shown in Fig.~\ref{fig2}. It contains a mask-free reconstruction network and an object-aware discriminator. The reconstruction network has the ability to automatically localize the corrupted regions and inpaint the missing information simultaneously, thereby relaxing the constraint of using specific masks for target areas. In addition, the object-aware discriminator incorporates an object detector to enhance discrimination performance, demonstrating the feasibility of integrating object detectors into discriminative models. Details are described in following subsections.

\begin{figure}[!t]
\includegraphics[width=\textwidth]{fig/framework.pdf}
\caption{The proposed blind inpainting model, which consists of a two-branch reconstruction network $f_{\theta}$ and an object-aware discriminator $d_{\omega}$. In the two-branch reconstruction network, one branch $f_{\theta_1}$ implements the inpainting task, while the other branch $f_{\theta_2}$ estimates mask of corrupted regions. The object-aware discriminator follows the structure of dense object detectors to ensure the localization of artificial markers.}\label{fig2}
\end{figure}

\subsection{Mask-free Reconstruction Network}
The mask-free reconstruction network follows the setting of blind inpainting framework that no masks are provided. To guide the reconstruction network to focus on the inpainting in the corrupted regions (which are unknown to the network), we use a two-branch architecture in the reconstruction network, which localizes corrupted regions and fills in the missing information, respectively. 
The reconstruction network $f_{\theta}$ is divided into two branches, $f_{\theta_1}$ and $f_{\theta_2}$, with the same structure and different parameters. Each branch has an upsampler-convolution-downsampler structure, based on
gated convolution~\cite{yu2019free}. Specifically, $f_{\theta_1}$ implements the inpainting task, while $f_{\theta_2}$ estimates mask of corrupted regions. The reconstruction can be formulated as follows,
\begin{equation}
\begin{aligned}
& \hat{I}_g = f_{\theta_1}(I), \\
& \hat{M} = f_{\theta_2}(I), \\
& \hat{I} = \hat{M} \odot \hat{I}_g+(1-\hat{M}) \odot I,\\
\end{aligned}
\end{equation}
where $\odot$ represents the elementwise product.
The mask of corrupted regions is implicitly learned and the reconstruction is supervised by the clean image $I^{*}$ with the $l_1$ loss as follows,
\begin{equation}
\mathcal{L}_{\text{rec}}(\theta)=\Vert I^*-\hat{I}_g \Vert_1 + \Vert I^*-\hat{I} \Vert_1,
\end{equation}
where $\theta=\{\theta_1, \theta_2\}$.

In addition, we also constrain the feature maps of the reconstructed image with perceptual loss as follows,
\begin{equation}
\mathcal{L}_{\text{per}}(\theta)= \Vert \phi(I^*)-\phi(\hat{I}_g) \Vert_2 + \Vert \phi(I^*)-\phi(\hat{I}) \Vert_2,
\end{equation}
where $\phi(\cdot)$ denotes the layer activation of a pre-trained VGG-16~\cite{simonyan2014very} network.



\subsection{Object-aware Discrimination}
Since the artificial markers in the corrupted image are usually with different sizes in local regions, we use the dense object detector such as YOLOv5~\cite{glenn_jocher_2022_6222936} as the discriminator, relying on its powerful recognition capabilities for pixel-based classification in local regions. For the object-aware discriminator, we define a new category, namely ``fake marker", for the marker region in the reconstructed images to enhance the discrimination capability. On one hand, the object-aware discriminator should detect the artificial markers in the reconstructed image as much as possible. On the other hand, the reconstruction network should recover the missing regions to fool the discriminator to ensure that the reconstructed regions cannot be easily detected as objects.


Denote the object-aware discriminator as $d_{\omega}$, where $\omega$ are the parameters to be learned. Then the output of the discriminator contains two parts, \textit{i.e.},


\begin{equation}
     \hat{F}_{\text{cls}}^{\Omega},\hat{F}_{\text{loc}}^{\Omega} =  d_{\omega}(\Omega),\quad  \Omega \in \{I^*, \hat{I}_g,\hat{I}\},
\end{equation}
where $\hat{F}_{\text{cls}}$ represents the feature maps of the classification
and $\hat{F}_{\text{loc}}$ represents the localization results, including offsets and sizes.

To ensure the discriminator can be fooled, we add an adversarial loss for both $\hat{I}_g$ and $\hat{I}$, generated from the reconstruction network, \textit{i.e.},


\begin{equation}
\mathcal{L}_{\text{adv}}(\theta)= - \mathbb{E}_{\Omega \in \{\hat{I}_g,\hat{I}\}}\text{log}(1 -\hat{F}_{\text{cls}}^{\Omega})
\end{equation}
which guarantees that the reconstructed image should be close to the background of the medical images without artificial markers (objects). The values of $\lambda_1 \sim \lambda_3$ are set with reference to~\cite{yu2019free}.

We follow the conventional classification loss $\mathcal{L}_{\text{cls}}$ and localization loss $\mathcal{L}_{\text{loc}}$ of an anchor-based detector~\cite{glenn_jocher_2022_6222936} to train the object-aware discriminator, \textit{i.e.},
\begin{equation}
\mathcal{L}_{\text{d}}(\omega)= \sum\limits_{\Omega \in \{I^*, \hat{I}_g,\hat{I}\}} \mathcal{L}_{\text{cls}}(\hat{F}_{\text{cls}}^{\Omega};\omega)+\mathcal{L}_{\text{loc}}(\hat{F}_{\text{loc}}^{\Omega};\omega) .
\end{equation}
For the original corrupted image $I$ and the reconstructed image $\hat{I}_g$ and $\hat{I}$, the discriminator should detect the artificial markers as much as possible with the detection loss $\mathcal{L}_{\text{d}}(\omega)$. Then the total loss used for training is as follows,
\begin{equation}
\mathcal{L} = \lambda_1 \mathcal{L}_{\text{rec}}(\theta)+\lambda_2 \mathcal{L}_{\text{per}}(\theta)+\lambda_3 \mathcal{L}_{\text{adv}}(\theta)+\mathcal{L}_{\text{d}}(\omega),
\end{equation}
where $\theta$ and $\omega$ are updated iteratively.