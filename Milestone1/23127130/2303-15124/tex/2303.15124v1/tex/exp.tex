\section{Experiments and Results}
\subsection{Datasets}

Three datasets are used in our study, covering most commonly used medical imaging modalities including ultrasound (\textbf{US}), electron microscopy (\textbf{EM}), and magnetic resonance imaging (\textbf{MRI}). Specifically, the thyroid US dataset from Zhejiang University School of Medicine Sir Run Run Shaw Hospital contains 414 training images, 117 validation images and 69 test images (1024×768 pixels). Each image is marked with crosshairs and forks at the location of the lesion by doctors, and corresponding clean ground truth images are also available. 
The EM dataset is provided by the MICCAI 2015 gland segmentation challenge (GlaS)~\cite{sirinukunwattana2015stochastic}. It consists of 165 calibrated pathological images, which are split into 160 training images and 5 test images.  
The MRI dataset is provided by Prostate MR Image Segmentation Challenge~\cite{litjens2014evaluation}. It has 50 training images and 30 test images. As there are no markers in the EM and MRI datasets, we added artificial markers to these two datasets to mimic doctors' process and validate our method.

\subsection{Implementation}
We choose YOLOv5~\cite{glenn_jocher_2022_6222936} as the object-aware discriminator and Deepfillv2~\cite{yu2019free} as the basis for our improved two-branch reconstruction network design. Other dense object detection structures are also suitable for this task.
We set $\lambda_1=10, \lambda_2=1, \lambda_3=0.1$ and batch size as 4.
To enhance the model's performance, we added pseudo markers to the input images randomly as a form of data augmentation.

Our experiments are implemented with Pytorch. We evaluate the results using PSNR, SSIM and MSE. Adam is used as the optimizer with a learning rate of 1e-4. 
All the experiments are performed using one NVIDIA RTX 3090.

\subsection{Main Results}
We compare the performance of our method with recent blind inpainting framework VCNet~\cite{wang2020vcnet} and several SOTA reconstruction networks including MPRNet~\cite{zamir2021multi} and UNet~\cite{ronneberger2015u}.

\begin{table}[!t]
\renewcommand\tabcolsep{10pt}
\caption{Quantitative comparison between our method and recent blind inpainting framework VCNet and SOTA reconstruction networks including MPRNet and UNet. Boldface indicates the best results.}
\label{tab1}
\centering
\begin{tabular}{l|l|cccc}
\toprule
Datasets &
Methods &
  \begin{tabular}[c]{@{}c@{}}PSNR$\uparrow$ \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SSIM$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSE$\downarrow$\end{tabular} \\ \midrule
 & MPRNet~\cite{zamir2021multi}   & $37.877_{\pm3.289}$ &$0.995_{\pm0.002} $ & $13.027_{\pm10.201}$ \\
US & UNet~\cite{ronneberger2015u}    & $35.262_{\pm1.319}$ &$0.985_{\pm0.004} $ & $20.499_{\pm9.442}$\\
 & VCNet~\cite{wang2020vcnet}  & $36.891_{\pm1.425}$ &$0.971_{\pm0.012}$  &$14.442_{\pm6.910}$\\

 & \textbf{Ours}  & $\textbf{47.673}_{\pm5.415}$
 & $\textbf{0.999}_{\pm0.001}$ & $\textbf{2.633}_{\pm5.856}$  \\ \midrule
 & MPRNet~\cite{zamir2021multi}  & $34.860_{\pm1.992}$ &$0.991_{\pm0.001} $ & $23.298_{\pm9.599}$ \\
MRI  & UNet~\cite{ronneberger2015u}     & $29.736_{\pm2.004}$ &$0.961_{\pm0.012} $ & $75.659_{\pm29.296}$\\
  & VCNet~\cite{wang2020vcnet}     & $31.315_{\pm1.405}$ &$0.947_{\pm0.029} $ & $63.405
_{\pm18.734}$\\
  & \textbf{Ours}  & $\textbf{40.049}_{\pm7.004}$ & $\textbf{0.994}_{\pm0.003}$
& $\textbf{7.153}_{\pm9.627}$  \\ \midrule

& MPRNet~\cite{zamir2021multi}   & $35.184_{\pm1.368}$ &$0.991_{\pm0.002} $ & $20.505_{\pm6.460}$ \\
CM   & UNet~\cite{ronneberger2015u}    & $34.239_{\pm0.847}$ &$0.984_{\pm0.001} $ & $24.881_{\pm4.931}$\\
  & VCNet~\cite{wang2020vcnet}    & $32.230_{\pm0.350}$ &$0.956_{\pm0.007} $ & $39.016_{\pm3.098}$\\
  & \textbf{Ours} & $\textbf{41.419}_{\pm1.902}$& $\textbf{0.997}_{\pm0.001}$ & $\textbf{2.595}_{\pm1.284}$\\
\bottomrule
\end{tabular}
\end{table}

%arxiv:
\begin{table}[!h]
\renewcommand\tabcolsep{18pt}
\caption{Further quantitative comparison between ours, VCNet, MPRNet and UNet. Metrics are calculated \textbf{solely in mask areas}, represented by \textbf{rectangular boxes} derived from ground-truth labels.}
\label{tabA}
\centering
\begin{tabular}{l|l|cccc}
\toprule
Dataset &
Methods &
  \begin{tabular}[c]{@{}c@{}}PSNR$\uparrow$ \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SSIM$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSE$\downarrow$\end{tabular} \\ \midrule
 & MPRNet   & $13.478$ &$0.429 $ & $3213.933$ \\
US & UNet  & $14.899$ &$0.419 $ & $2280.374$ \\
 & VCNet  & $28.988$ &$0.801 $ & $\textbf{87.293}$ \\
 & \textbf{Ours}  & $\textbf{30.016}$ & $\textbf{0.855}$ & $103.111$  \\ \midrule
 
 & MPRNet  & $17.692$ &$0.627 $ & $1226.490$ \\
MRI  & UNet   & $18.021$ &$0.625 $ & $1003.576$ \\
  & VCNet    & $21.117$ &$0.705 $ & $423.108$\\
  & \textbf{Ours}  & $\textbf{26.159}$ & $\textbf{0.821}$ & $\textbf{203.967}$ \\ \midrule

& MPRNet  & $18.354$ &$0.702 $ & $1004.690$\\
CM   & UNet  & $19.472$ &$0.707 $ & $1015.378$\\
  & VCNet  & $22.268$ &$0.718 $ & $387.710$\\
  & \textbf{Ours}&  $\textbf{28.437}$ & $\textbf{0.839}$ & $\textbf{165.442}$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab1} demonstrates a comparison of MPRNet, UNet, VCNet, and our method measured by PSNR, SSIM, MSE (mean±s.d). It can be observed that our model obtains better restoration ability than all the baselines. The improvements are statistically significant.
Table~\ref{tabA} further shows these metrics calculated solely in mask areas, represented by rectangular boxes derived from ground-truth labels, which still illustrates the effectiveness of our method.




\begin{figure}[!t]
\includegraphics[width=\textwidth]{fig/methods3.png}
\caption{The qualitative comparison among our method, VCNet, UNet, and MPRNet on US, MRI and CM dataset. Our model generates visually appealing results. Results from other three models have varying degrees of restoration failure.}\label{fig3}
\end{figure}

Fig.~\ref{fig3} shows a qualitative comparison among MPRNet, UNet, VCNet, and our method. The comparison demonstrates that our method is better than VCNet in terms of the restoration performance. Moreover, the results from UNet and MPRNet indicate that the denoising and general reconstruction methods are unsuitable for this task.


\begin{figure}
\includegraphics[width=\textwidth]{fig/learn.png}
\caption{The learning process of the two-branch generator in several typical epochs. The first line represents the learning process of one branch for mask prediction, while the second line shows the learning process of the other branch for inpainting.}\label{figC}
\end{figure}

Fig.~\ref{figC} shows the learning process of mask prediction and inpainting of the two-branch generator. 


\subsection{Ablation Study }
In this section, we conduct experiments on US dataset to sort out which components of our framework contribute the most to the final performance. The original YOLOv5 + Deepfillv2~\cite{yu2019free,glenn_jocher_2022_6222936} two-stage inpainting network is also compared as a baseline.
Table~\ref{tab2} and Fig.~\ref{fig4} show Quantitative and qualitative comparisons.

\begin{table}[!t]
\renewcommand\tabcolsep{3pt}
\caption{Ablation studies of individual components. ``A" represents our complete model. ``B" replaces the object-aware discriminator with the one in SN-PatchGAN from Deepfillv2. ``C" replaces our two-branch reconstruction network with a single branch one. ``D" represents a two-stage inpainting solution with YOLOv5 and Deepfillv2 trained separately. }
\label{tab2}
\centering
\begin{tabular}{c|l|ccc}
\toprule
Type &
  \begin{tabular}[c]{@{}c@{}}Model \end{tabular} &

  \begin{tabular}[c]{@{}c@{}}PSNR$\uparrow$ \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SSIM$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSE$\downarrow$\end{tabular} \\ \midrule

\textbf{A} & \textbf{Complete Model}  & $\textbf{47.673}_{\pm5.415}$
 & $\textbf{0.999}_{\pm0.001}$ & $\textbf{2.633}_{\pm5.856}$
 \\ \midrule
B & w/o Object-aware Discriminator  &$33.283_{\pm2.023}$ & $0.984_{\pm0.006}$  &$33.948_{\pm16.306}$\\
C & w/o Two-branch Reconstruction   &$29.306_{\pm2.131}$ & $0.883_{\pm0.038}$  &$87.551_{\pm52.855}$ \\
D & Two-Stage (YOLOv5+Deepfillv2)  & $43.551_{\pm3.014}$ &$0.998_{\pm0.001}$  &$4.583_{\pm9.094}$\\
\bottomrule
\end{tabular}
\end{table}



\begin{figure}[!t]
\includegraphics[width=\textwidth]{fig/ablation.png}
\caption{The qualitative comparisons of variants of the model. Our complete model ``A" produces visually appealing results, supporting high-quality image completion. Model ``B" appears to be losing fine texture details. Model ``C" has low-quality resolution. Model ``D" demonstrates a degradation in texture restoration.}\label{fig4}
\end{figure}


\vspace{5pt}
\noindent \textbf{Object-aware Discrimination.}
Our study investigates whether a discriminator based on dense object detector could enhance high-quality adversarial training. We replaced the YOLOv5-based discriminator with the one in SN-PatchGAN from Deepfillv2 (model ``B" in Table~\ref{tab2}), and observed performance degradation in all metrics, particularly in MSE and PSNR. These findings suggest some loss of fidelity in the inpainting images. Also we present visualisations in Fig.~\ref{fig4}. Compared to model ``B", our complete model utilizes the powerful recognition capabilities of dense object detectors to identify artificial markers.

\vspace{5pt}
\noindent \textbf{Two-branch Reconstruction Network Structure.}
We also compared our two-branch reconstruction network with a single branch one. The results, presented in Table~\ref{tab2}, indicate that our complete model (model ``A") achieved superior performance compared to model ``C" with a 62.67\% improvement in PSNR. Additionally, as seen in Fig.~\ref{fig4}, model ``C" lost texture details while our complete model produced more visually appealing results, supporting high-quality image completion. This is because the mask prediction branch of the reconstruction network helps the model focus on marker regions in the fusion process. 

\vspace{5pt}
\noindent \textbf{Comparison with the Two-Stage Baseline.}
Model ``D" represents a two-stage inpainting solution that utilizes separately trained YOLOv5 and Deepfillv2 models. 
However, this approach demonstrates a degradation in texture restoration compared to our end-to-end blind inpainting method. Our findings are supported by both quantitative and qualitative results, which confirm the effectiveness of our approach for blind inpainting.