\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/framework.pdf}
    \caption{The proposed blind inpainting model consisted of a two-branch reconstruction network $f_{\theta}$ and an object-aware discriminator $d_{\omega}$. In $f_{\theta}$, one branch $f_{\theta_1}$ implements the inpainting task, while the other branch $f_{\theta_2}$ estimates mask of corrupted regions. $d_{\omega}$ follows the structure of dense object detectors to ensure the localization of corrupted regions.}
    \label{fig2}
\end{figure*}

\section{Method}

\subsection{Overview}



The blind image inpainting task can be described as follows. Given an input corrupted image $I$ with artificial markers, we aim to learn a reconstruction network $f_{\theta}$ to obtain a clean image $\hat{I}$ with markers removed, where $\theta$ are the network parameters to be learned. This blind inpainting task is different from the general inpainting task since the masks of corrupted regions are not provided in the inference stage. 

In the following, we minutely introduce a novel blind inpainting framework for medical imaging, as shown in Fig.~\ref{fig2}. It contains a mask-free reconstruction network and an object-aware discriminator. The reconstruction network can autonomously identify the corrupted regions and simultaneously inpaint the missing contents, eliminating the need for specific masks for target areas. In addition, the object-aware discriminator incorporates an object detector to enhance adversarial training and demonstrates the feasibility of integrating object detectors into discriminative models. 
% Details are described in following subsections.


\subsection{Mask-free Reconstruction Network}
% We employ a two-branch architecture in the reconstruction network to guide inpainting process to focus on the corrupted regions, which are unknown to the network. The two branches can localize corrupted regions and restore the missing information respectively, and thus eliminate dependency on a manual mask input.
% Specifically, the reconstruction network $f_{\theta}$ consists of two branches, $f_{\theta_1}$ for inpainting task and $f_{\theta_2}$ for mask prediction of corrupted regions. Each branch utilizes an same upsampler-convolution-downsampler structure based on gated convolution ~\cite{yu2019free}, with distinct parameters. 

We employ a two-branch architecture in the reconstruction network $f_{\theta}$ to guide the inpainting process to focus on corrupted regions, which are unknown to the network. The branch $f_{\theta_1}$ is for inpainting missing content in corrupted regions localized by the other branch $f_{\theta_2}$. This eliminats dependency on a manual mask input. Each branch utilizes an same upsampler-convolution-downsampler structure based on gated convolution ~\cite{yu2019free}, but is with distinct parameters. 
The reconstruction can be formulated as follows,

\begin{equation}
\begin{aligned}
& \hat{I}_g = f_{\theta_1}(I), \\
& \hat{M} = f_{\theta_2}(I), \\
& \hat{I} = \hat{M} \odot \hat{I}_g+(1-\hat{M}) \odot I,\\
\end{aligned}
\end{equation}
where $\odot$ represents the elementwise product.
The mask of corrupted regions is implicitly learned and the reconstruction is supervised by the clean image $I^{*}$ with the $l_1$ loss as follows,
\begin{equation}
\mathcal{L}_{\text{rec}}(\theta)=\Vert I^*-\hat{I}_g \Vert_1 + \Vert I^*-\hat{I} \Vert_1,
\end{equation}
where $\theta=\{\theta_1, \theta_2\}$.

In addition, we also constrain the feature maps of the reconstructed image with perceptual loss as follows,
\begin{equation}
\mathcal{L}_{\text{per}}(\theta)= \Vert \phi(I^*)-\phi(\hat{I}_g) \Vert_2 + \Vert \phi(I^*)-\phi(\hat{I}) \Vert_2,
\end{equation}
where $\phi(\cdot)$ is the layer activation of pre-trained VGG-16~\cite{simonyan2014very}.



\subsection{Object-aware Discrimination}
To accommodate markers of different relative sizes in corrupted images, we utilize and enhance an dense object detector such as YOLOv5~\cite{glenn_jocher_2022_6222936} to build our discriminator. This leverages the detector's powerful recognition capabilities for pixel-based classification in local regions. 
During adversarial training, the object-aware discriminator should detect artificial markers in reconstructed images as much as possible. Meanwhile, the reconstruction network should inpainting corrupted regions to blend naturally with background texture, making them less detectable as objects by the discriminator.
To enhance the discrimination in this supervision process, we define a new object category in ground-truth labels, namely ``fake marker", for marker regions in reconstructed images. 


Denote the object-aware discriminator as $d_{\omega}$, where $\omega$ are the parameters to be learned. Then the output of the discriminator contains two parts, \textit{i.e.},


\begin{equation}
     \hat{F}_{\text{cls}}^{\Omega},\hat{F}_{\text{loc}}^{\Omega} =  d_{\omega}(\Omega),\quad  \Omega \in \{I^*, \hat{I}_g,\hat{I}\},
\end{equation}
where $\hat{F}_{\text{cls}}$ represents the feature maps of the classification
and $\hat{F}_{\text{loc}}$ is the localization results, including offsets and sizes.

To ensure the discriminator can be fooled, we add an adversarial loss for both $\hat{I}_g$ and $\hat{I}$, generated from the reconstruction network, \textit{i.e.},


\begin{equation}
\mathcal{L}_{\text{adv}}(\theta)= - \mathbb{E}_{\Omega \in \{\hat{I}_g,\hat{I}\}}\text{log}(1 -\hat{F}_{\text{cls}}^{\Omega})
\end{equation}
which guarantees the reconstructed image to smoothly blend with the background texture without artificial markers (objects). Set values of $\lambda_1 \sim \lambda_3$ referencing~\cite{yu2019free}.

We follow the conventional classification loss $\mathcal{L}_{\text{cls}}$ and localization loss $\mathcal{L}_{\text{loc}}$ of an anchor-based detector~\cite{glenn_jocher_2022_6222936} to train the object-aware discriminator, \textit{i.e.},
\begin{equation}
\mathcal{L}_{\text{d}}(\omega)= \sum\limits_{\Omega \in \{I^*, \hat{I}_g,\hat{I}\}} \mathcal{L}_{\text{cls}}(\hat{F}_{\text{cls}}^{\Omega};\omega)+\mathcal{L}_{\text{loc}}(\hat{F}_{\text{loc}}^{\Omega};\omega) .
\end{equation}
For the original corrupted image $I$ and the reconstructed image $\hat{I}_g$ and $\hat{I}$, the discriminator should detect the artificial markers as much as possible with the detection loss $\mathcal{L}_{\text{d}}(\omega)$. Then the total loss used for training is as follows,
\begin{equation}
\mathcal{L} = \lambda_1 \mathcal{L}_{\text{rec}}(\theta)+\lambda_2 \mathcal{L}_{\text{per}}(\theta)+\lambda_3 \mathcal{L}_{\text{adv}}(\theta)+\mathcal{L}_{\text{d}}(\omega),
\end{equation}
where $\theta$ and $\omega$ are updated iteratively.