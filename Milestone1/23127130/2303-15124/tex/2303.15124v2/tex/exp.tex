\section{Experiments}
\subsection{Datasets}

Our study utilizes three datasets of various medical imaging modalities. The thyroid ultrasound (\textbf{US}) dataset provided by Sir Run Run Shaw Hospital of Zhejiang University contains 414 training images, 117 validation images and 69 test images (1024×768 pixels). The images feature crosshairs and forks as doctor-added markers at lesion locations, with corresponding clean ground truth images and location labels.
The electron microscopy (\textbf{EM}) dataset sourced from the MICCAI 2015 gland segmentation challenge (GlaS)~\cite{sirinukunwattana2015stochastic} consists of 160 training images and 5 test images.  
The magnetic resonance imaging (\textbf{MRI}) dataset obtained from Prostate MR Image Segmentation Challenge~\cite{litjens2014evaluation} has 50 training images and 30 test images. To replicate the doctors' process and validate our method, we add artificial markers to EM and MRI, which initially lack them.

\subsection{Implementation Details}
We enhance the object detector YOLOv5 ~\cite{glenn_jocher_2022_6222936} to form our object-aware discriminator. And modify the generator of the non-blind inpainting model Deepfillv2 ~\cite{yu2019free} to build an improved two-branch blind reconstruction network. 
Weight factors is set as $\lambda_1=10, \lambda_2=1, \lambda_3=0.1$. Data augmentation include adding more pseudo markers randomly to input images.
To ensure a fair comparison, we maintain parameters of compared baseline models in accordance with the respective papers or codes and train until loss functions converges. Data preprocessing methods are also same.
Experiments employ a single NVIDIA RTX 3090 GPU with PyTorch. Evaluation metrics include PSNR, SSIM, and MSE. Models are optimized by Adam with learning rate $1e^{-4}$ and batch size 4.
\vspace{-10pt}
\subsection{Motivation Verification}


% \vspace{\baselineskip}
% Results show that $M_{clean}$ outperforms $M_{unclean}$.
\begin{table}[!t]
\renewcommand\tabcolsep{3pt}
\renewcommand\arraystretch{0.9}
\caption{Motivation verify: Quantitative comparison. }
\label{motivation_tab}
\centering
\begin{tabular}{l|l|cccc}
\hline
Models &
Test sets &
  \begin{tabular}[c]{@{}c@{}}P \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}R\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}mAP@.5\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}mAP@.5:.95\end{tabular} \\ \hline
 & $V_{unclean}$   & 0.875  & 0.860 & 0.860 &  0.844\\
$M_{unclean}$ & $V_{clean}$  &0.500 &0.594 &0.556 &0.248\\
 & $V_{inpaint}$  & 0.583 &0.429 & 0.511 &0.221\\ \hline

& $V_{unclean} $  &0.780 &0.754 &0.773  &0.442\\
$M_{clean}$   & $V_{clean}$ & \textbf{0.770} & \textbf{0.696} & \textbf{0.734} & \textbf{0.425}\\
  & $V_{inpaint}$ &\textbf{0.664} &\textbf{0.719} &\textbf{0.676}  &\textbf{0.389}\\ \hline
\end{tabular}
\vspace{-6pt}
\end{table}

 % between AI diagnosis models $M_{\cdot}$ trained on unclean and clean data respectively. $M_{clean}$ outperforms $M_{unclean}$ on test set $V_{\cdot}$
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/motivation1.png}
    \caption{Motivation verify: Qualitative comparison.}
    \label{motivation}
    \vspace{-10pt} 
\end{figure}
We verify the motivation of our work by YOLOv5 for lesion detection on US dataset. First train YOLOv5 models $M_{\cdot}$ on unclean data with artificial markers and clean data respectively. Use $V_{\cdot}$ as test sets and process $V_{unclean}$ by our inpainting model to obtain $V_{inpaint}$.
Shown in Fig.~\ref{motivation} and Table~\ref{motivation_tab}, 
% excluding the case where $M_{unclean}$ performs well on $V_{unclean}$ just because conspicuous markers simplify feature learning, 
$M_{unclean}$ detects lesions relying on marker recognition, rather than understanding medical  semantics as $M_{clean}$. 
It proves the negative impact of unclean data on AI diagnostics.  

% We verify our work's motivation by YOLOv5 for lesion detection on US dataset. First train YOLOv5 models $M_{unclean}$, $ M_{clean}$ on unclean data with artificial markers and clean data respectively. Then use $V_{\cdot}$ as test sets. $V_{inpaint}$ is obtained by processing $V_{unclean}$ using our inpainting model. As shown in Fig.~\ref{motivation} and Table~\ref{motivation_tab}, $M_{unclean}$ underperforms $M_{clean}$. It proves the negative impact of unclean data on AI diagnostics when testing and extensive historical data training. 

\subsection{Main Results}


% \begin{table}[!t]
% \renewcommand\tabcolsep{0.5pt}
% \caption{Quantitative comparison between our method and recent blind inpainting framework VCNet and SOTA reconstruction networks including MPRNet and UNet. Boldface indicates the best results.}
% \label{tab1}
% \centering
% \begin{tabular}{l|l|cccc}
% \toprule
% Datasets &
% Methods &
%   \begin{tabular}[c]{@{}c@{}}PSNR~$\uparrow$ \end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}SSIM~$\uparrow$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}MSE~$\downarrow$\end{tabular} \\ \midrule
%  & MPRNet~\cite{zamir2021multi}   & $37.877_{\pm3.289}$ &$0.995_{\pm0.002} $ & $13.027_{\pm10.201}$ \\
% US & UNet~\cite{ronneberger2015u}    & $35.262_{\pm1.319}$ &$0.985_{\pm0.004} $ & $20.499_{\pm9.442}$\\
%  & VCNet~\cite{wang2020vcnet}  & $36.891_{\pm1.425}$ &$0.971_{\pm0.012}$  &$14.442_{\pm6.910}$\\

%  & Ours  & $\textbf{47.673}_{\pm5.415}$
%  & $\textbf{0.999}_{\pm0.001}$ & $\textbf{2.633}_{\pm5.856}$  \\ \midrule
%  & MPRNet~\cite{zamir2021multi}  & $34.860_{\pm1.992}$ &$0.991_{\pm0.001} $ & $23.298_{\pm9.599}$ \\
% MRI  & UNet~\cite{ronneberger2015u}     & $29.736_{\pm2.004}$ &$0.961_{\pm0.012} $ & $75.659_{\pm29.296}$\\
%   & VCNet~\cite{wang2020vcnet}     & $31.315_{\pm1.405}$ &$0.947_{\pm0.029} $ & $63.405
% _{\pm18.734}$\\
%   & Ours  & $\textbf{40.049}_{\pm7.004}$ & $\textbf{0.994}_{\pm0.003}$
% & $\textbf{7.153}_{\pm9.627}$  \\ \midrule

% & MPRNet~\cite{zamir2021multi}   & $35.184_{\pm1.368}$ &$0.991_{\pm0.002} $ & $20.505_{\pm6.460}$ \\
% CM   & UNet~\cite{ronneberger2015u}    & $34.239_{\pm0.847}$ &$0.984_{\pm0.001} $ & $24.881_{\pm4.931}$\\
%   & VCNet~\cite{wang2020vcnet}    & $32.230_{\pm0.350}$ &$0.956_{\pm0.007} $ & $39.016_{\pm3.098}$\\
%   & Ours & $\textbf{41.419}_{\pm1.902}$& $\textbf{0.997}_{\pm0.001}$ & $\textbf{2.595}_{\pm1.284}$\\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[!t]
\renewcommand\tabcolsep{3.1pt}
\renewcommand\arraystretch{0.9}
\caption{Quantitative comparison between our method, VCNet~\cite{wang2020vcnet}, MPRNet~\cite{zamir2021multi} and UNet~\cite{ronneberger2015u} (mean±s.d). In parentheses are metrics further calculated \textbf{only within mask areas}.}
\label{tab1}
\centering
\begin{tabular}{l|l|cccc}
\hline
Data &
Methods &
  \begin{tabular}[c]{@{}c@{}}PSNR~$\uparrow$ \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SSIM~$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSE~$\downarrow$\end{tabular} \\ \hline
 & MPRNet   & $37.877_{\pm3.289}$ &$0.995_{\pm0.002} $ & $13.027_{\pm10.201}$ \\
  &    & ($13.478$) &($0.429) $ & ($3213.933$) \\
& UNet    & $35.262_{\pm1.319}$ &$0.985_{\pm0.004} $ & $20.499_{\pm9.442}$\\
US &   & ($14.899$) &($0.419) $ & ($2280.374$)\\
 & VCNet  & $36.891_{\pm1.425}$ &$0.971_{\pm0.012}$  &$14.442_{\pm6.910}$\\
  &   & ($28.988$) &($0.801 $) & ($\textbf{87.293}$) \\
 & Ours  & $\textbf{47.673}_{\pm5.415}$ & $\textbf{0.999}_{\pm0.001}$ & $\textbf{2.633}_{\pm5.856}$  \\ 
 &   & ($\textbf{30.016}$) & ($\textbf{0.855}$) & ($103.111$)  \\ \hline
 
 & MPRNet  & $34.860_{\pm1.992}$ &$0.991_{\pm0.001} $ & $23.298_{\pm9.599}$ \\
 &   & ($17.692$) &($0.627) $ & ($1226.490$) \\
& UNet     & $29.736_{\pm2.004}$ &$0.961_{\pm0.012} $ & $75.659_{\pm29.296}$\\
MRI  &    & ($18.021$) &($0.625) $ & ($1003.576$) \\
  & VCNet     & $31.315_{\pm1.405}$ &$0.947_{\pm0.029} $ & $63.405_{\pm18.734}$\\
  &     & ($21.117$) &($0.705 $) & ($423.108$)\\
  & Ours  & $\textbf{40.049}_{\pm7.004}$ & $\textbf{0.994}_{\pm0.003}$& $\textbf{7.153}_{\pm9.627}$  \\
  &   & ($\textbf{26.159}$) & ($\textbf{0.821}$) & ($\textbf{203.967}$) \\ \hline

& MPRNet   & $35.184_{\pm1.368}$ &$0.991_{\pm0.002} $ & $20.505_{\pm6.460}$ \\
&   & ($18.354$) &($0.702 $) & ($1004.690$)\\
& UNet    & $34.239_{\pm0.847}$ &$0.984_{\pm0.001} $ & $24.881_{\pm4.931}$\\
CM   &   & ($19.472$) &($0.707) $ & ($1015.378$)\\
  & VCNet    & $32.230_{\pm0.350}$ &$0.956_{\pm0.007} $ & $39.016_{\pm3.098}$\\
  &   & ($22.268$) &($0.718 $) & ($387.710$)\\
  & Ours & $\textbf{41.419}_{\pm1.902}$& $\textbf{0.997}_{\pm0.001}$ & $\textbf{2.595}_{\pm1.284}$\\
   &  &  ($\textbf{28.437}$)& ($\textbf{0.839}$) & ($\textbf{165.442}$) \\ \hline
\end{tabular}
\end{table}


% %arxiv:
% \begin{table}[!h]
% \renewcommand\tabcolsep{7pt}
% \caption{Further quantitative comparison between ours, VCNet, MPRNet and UNet. Metrics are calculated \textbf{solely in mask areas}, represented by \textbf{rectangular boxes} derived from ground-truth labels.}
% \label{tabA}
% \centering
% \begin{tabular}{c|l|cccc}
% \toprule
% Dataset &
% Methods &
%   \begin{tabular}[c]{@{}c@{}}PSNR~$\uparrow$ \end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}SSIM~$\uparrow$\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}MSE~$\downarrow$\end{tabular} \\ \midrule
%  & MPRNet   & $13.478$ &$0.429 $ & $3213.933$ \\
% US & UNet  & $14.899$ &$0.419 $ & $2280.374$ \\
%  & VCNet  & $28.988$ &$0.801 $ & $\textbf{87.293}$ \\
%  & Ours  & $\textbf{30.016}$ & $\textbf{0.855}$ & $103.111$  \\ \midrule
 
%  & MPRNet  & $17.692$ &$0.627 $ & $1226.490$ \\
% MRI  & UNet   & $18.021$ &$0.625 $ & $1003.576$ \\
%   & VCNet    & $21.117$ &$0.705 $ & $423.108$\\
%   & Ours  & $\textbf{26.159}$ & $\textbf{0.821}$ & $\textbf{203.967}$ \\ \midrule

% & MPRNet  & $18.354$ &$0.702 $ & $1004.690$\\
% CM   & UNet  & $19.472$ &$0.707 $ & $1015.378$\\
%   & VCNet  & $22.268$ &$0.718 $ & $387.710$\\
%   & Ours &  $\textbf{28.437}$ & $\textbf{0.839}$ & $\textbf{165.442}$ \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/methods31.png}
    \caption{Qualitative comparison. Our model generates visually appealing results. Other models exhibit varying levels of restoration failure.}
    \label{fig3}
\end{figure}

\begin{figure}[!t]
\includegraphics[width=0.47\textwidth]{fig/learn1.png}
\caption{Results of two-branch generator included mask prediction branch $f_{\theta_2}$ and inpainting branch $f_{\theta_1}$ when training.}\label{figC}
\vspace{-10pt}
\end{figure}

We evaluate our method through comparisons with recent blind inpainting framework VCNet~\cite{wang2020vcnet} and SOTA reconstruction networks MPRNet~\cite{zamir2021multi} and UNet~\cite{ronneberger2015u}. 
Table~\ref{tab1} quantitatively compares our model to baselines, demonstrating superior restoration ability with statistically significant improvements. Metrics are further calculated within mask areas determined by ground-truth location labels, confirming our method's effectiveness.
Fig.~\ref{fig3} demonstrates a qualitative superiority of our method over VCNet in terms of restoration. Additionally, results from UNet and MPRNet suggest that denoising and general reconstruction methods are inadequate for this task.
And Fig.~\ref{figC} depicts the learning process of the two-branch generator for mask prediction and inpainting. 


\subsection{Ablation Study }
We compared our implementation with other different structures on US dataset, as shown in Table~\ref{tab2} and Fig.~\ref{fig4}.

\begin{table}[!t]
\renewcommand\tabcolsep{8pt}
\renewcommand\arraystretch{0.883}
\caption{Ablation study on US dataset. ``A" is our complete model. ``B" replaces our object-aware discriminator with the one in Deepfillv2. ``C" replaces our two-branch reconstruction network with a single branch one. ``D" is a two-stage non-blind inpainting solution with YOLOv5 and Deepfillv2.}
\label{tab2}
\centering
\begin{tabular}{c|ccc}
\hline
Type &

  \begin{tabular}[c]{@{}c@{}}PSNR~$\uparrow$ \end{tabular} &
  \begin{tabular}[c]{@{}c@{}}SSIM~$\uparrow$\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}MSE~$\downarrow$\end{tabular} \\ \hline

A & $\textbf{47.673}_{\pm5.415}$
 & $\textbf{0.999}_{\pm0.001}$ & $\textbf{2.633}_{\pm5.856}$
 \\ \hline
B &$33.283_{\pm2.023}$ & $0.984_{\pm0.006}$  &$33.948_{\pm16.306}$\\
C &$29.306_{\pm2.131}$ & $0.883_{\pm0.038}$  &$87.551_{\pm52.855}$ \\
D  & $43.551_{\pm3.014}$ &$0.998_{\pm0.001}$  &$4.583_{\pm9.094}$\\
\hline
\end{tabular}
\vspace{-6pt}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/ablation1.png}
    % \vspace{-6pt}
    \caption{Qualitative ablation study. Complete ``A" gives visually appealing results. ``B" loses fine texture details. ``C" has low-quality resolution. ``D" shows restoration degradation.}
    \label{fig4}
    \vspace{-8pt}
\end{figure}

\vspace{5pt}
\noindent \textbf{Object-aware Discrimination.}
We replace our discriminator with the one in SN-PatchGAN from Deepfillv2 as ``B" in Table~\ref{tab2}. Performance degrades in all metrics, particularly in MSE and PSNR, suggesting loss of fidelity. Fig.~\ref{fig4} highlight our complete model’s success with robust recognition capability to identify markers after enhanced adversarial training.
 

\vspace{5pt}
\noindent \textbf{Two-branch Reconstruction Network Structure.}
Replace our two-branch reconstruction network with a single branch one as model ``C". Table~\ref{tab2} indicates that our complete model ``A" outperforms model ``C" with a 62.67\% improvement in PSNR. Fig.~\ref{fig4} illustrates that ``C" loses texture details, while ``A" produces visually superior results, thanks to the mask prediction branch focusing on corrupted region during fusion. 

\vspace{5pt}
\noindent \textbf{Comparison with the Two-Stage Non-blind Baseline.}
The original YOLOv5~\cite{glenn_jocher_2022_6222936} + Deepfillv2~\cite{yu2019free} two-stage non-blind inpainting network is compared as a baseline ``D".
Both quantitative and qualitative results depict an obvious degradation in texture restoration compared to our end-to-end blind inpainting model. It confirms the superiority of our approach.