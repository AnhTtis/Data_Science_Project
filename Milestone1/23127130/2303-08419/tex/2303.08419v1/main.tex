\documentclass{article}
\pdfoutput=1

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}    
\usepackage{hyperref}       
\usepackage{url}           
\usepackage{booktabs}       
\usepackage{amsfonts}      
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{lipsum}
\usepackage{fancyhdr}       
\usepackage{graphicx}       
\graphicspath{{media/}}     
\usepackage{multirow} 
\usepackage{bbding} 
\usepackage{caption} 
\usepackage{subcaption} 
	
\usepackage{xcolor}
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 


\fancyhead[LO]{Running Title for Header}



  

\title{Multi-Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling}

\author{
  JUN-HWA KIM \\
  Department of Electronics and Electrical Engineering \\
  Dongguk University \\
  Seoul, Korea\\
  \texttt{jhkim414@dongguk.edu} \\
    \And
  NAMHO KIM \\
  Department of Electronics and Electrical Engineering \\
  Dongguk University \\
  Seoul, Korea\\
  \texttt{namho96@dgu.ac.kr} \\
   \And
  CHEE SUN WON \\
  Department of Electronics and Electrical Engineering \\
  Dongguk University \\
  Seoul, Korea\\
  \texttt{cswon@dongguk.edu} \\
}


\begin{document}
\maketitle


\begin{abstract}
Facial expression recognition is important for various purpose such as emotion detection, mental health analysis, and human-machine interaction. In facial expression recognition, incorporating audio information along with still images can provide a more comprehensive understanding of an expression state. This paper presents the Multi-modal facial expression recognition methods for Affective Behavior in-the-wild (ABAW) challenge at CVPR 2023. We propose a Modal Fusion Module (MFM) to fuse audio-visual information. The modalities used are image and audio, and features are extracted based on Swin Transformer to forward the MFM. Our approach also addresses imbalances in the dataset through data resampling in training dataset and leverages the rich modal in a single frame using dynmaic data sampling, leading to improved performance. 
\end{abstract}



\keywords{Facial Expression Recognition (FER) \and Deep Learning \and Swin Transformer \and Audio-Visual}


\section{Introduction}

Facial Expression Recognition (FER) is challenging task in computer vision that has received much attention in recent years due to the increasing demand for human-machine interaction. With the advent of deep learning, the performance of facial expression recognition has been significantly improved, leading to its increased importance in various fields, such as emotion detection, mental health analysis, and human-machine interaction\cite{muhammad2017facial,davoudi2019intelligent}. While previous studies have shown significant improvements in facial expression recognition using image-based features, there is a growing need for more precise recognition that incorporates multi-modal information, including not only images but also audio data. This is because facial expressions are not just visual, they contain vocal and auditory cues such as tone of voice, rate of speech and loudness that can provide important information about emotional state. To address this issue, we proposed a approach for facial expression recognition that fuses image and audio features through a Multi-Fusion-Module (MFM). The MFM is designed to adaptively learn the importance of each modality in the fusion process. Each image and audio feature is extracted using a Swin Transformer\cite{liu2021swin}. 

To evaluate our proposed approach, we conduct experiments on Aff-Wild2 dataset. Aff-Wild2 dataset is a large-scale in-the-wild dataset of the 5th Affective Behavior Analysis in-the-wild (ABAW 2023) competition by Kollias et al. \cite{kour2014real, kour2014fast,hadash2018estimate,kollias2022abaw, kollias2021analysing, kollias2020analysing, kollias2021distribution, kollias2021affect, kollias2019expression, kollias2019face, kollias2019deep, zafeiriou2017aff} in conjunction with CVPR 2023.  

\section{Methodology}

\subsection{Multi Modal Fusion Network}

\begin{figure}[h]
    \includegraphics[width=15cm]{src/overall_network.png}
    \caption{Overall structure.}
    \label{network}
\end{figure}


The proposed Multi Modal Fusion Network for facial expression recognition involves four main steps. The overall proposed structure is shown in Figure \ref{network}. Firstly, the image and audio data are trained separately using a Swin Transformer\cite{liu2021swin} to obtain features ($f_{I}$, and $f_{A}$). Next, these features are fed into  Multi-Fusion-Module (MFM) to fuse different modals. Multi modsal fusion is based on the Transformer Layer. The Transformer Layer fuses through the process of co-attention of different modals. Forward $f_{A}$ for Query and $f_{I}$ for Key and Value to fuse audio information based on image. Similarly, forward $f_{I}$ for Query and $f_{A}$ for Key and Value to fuse image information based on audio. The fused attention output vector $Y_{IA}$ and $Y_{AI}$ can be represented as follows:



\begin{equation}
    Y_{IA} = Attention(f_{A}, f_{I}, f_{I}) = softmax(\frac{f_{A}f_{I}^{T}}{\sqrt{d_{k}}})f_{I},
\end{equation}


\begin{equation}
    Y_{AI} = Attention(f_{I}, f_{A}, f_{A}) = softmax(\frac{f_{I}f_{A}^{T}}{\sqrt{d_{k}}})f_{A},
\end{equation}
where, $d_{k}$ denotes the dimensionality of the key vector, and $T$ denotes the Transpose.

\begin{equation}
    Y = Concat(Y_{IA}, Y_{AI}),
\end{equation}

where, $Y$ denotes the Fused two modality. Then $Y$ is passed through two transformer layer to obtain $Y_{T}$. Thirdly, image and audio features are separately fed through linear layers to obtain the output vectors $Y_{I}$ and $Y_{A}$, respectively. Finally, the three outputs, $Y_{I}$, $Y_{T}$, and $Y_{A}$, are ensembled to obtain the final prediction.

\subsection{Dynamic sampling and resampling}

Facial expressions are change abruptly with a change in frames, but usually have strong influence among adjacent frames. Therefore, the audio data was sampled from the dynamic range, as shown in Figure \ref{dynamic}, to capture the temporal dynamics of facial expressions. We defined the ranges based on the current frame $t_{c}$ and extracted three dynamic audio features accordingly. The ranges of three dynamic ranges are $t_{s} = \{t_{c-16}, ..., t_{c}, ... t_{c+15}\}$, $t_{m} = \{t_{c-24}, ..., t_{c}, ... t_{c+23}\}$, and $t_{l} = \{t_{c-32}, ..., t_{c}, ... t_{c+31}\}$. Dynamically including adjacent frames can include multiple ground truth labels different from $t_{c}$. Therefore, in order to exclude the above case, we selected thresholds of 0.8 for $t_{s}$, 0.65 for $t_{m}$, and 0.5 for $t_{l}$ as training data if they were identical to the ground truth label of $t_{c}$.

\begin{figure}[h]
    \includegraphics[width=15cm]{src/dynamic_sampling.png}
    \caption{Dynamic Sampling.}
    \label{dynamic}
\end{figure}

Table \ref{table_dataset_stat} shows the number and ratio of images for each class in the Aff-wild2 dataset. As shown in the Table \ref{table_dataset_stat}, there is a severe distribution imbalance among different classes. When training on a imbalanced distribution, there is a high risk of overfitting to the majiority class due to bias. Therefore, we used all data from the minority classes Anger, Disgust, Fear, Sadness, and Suprise. Neutral, Happiness, and Other, which are the majority classes, used the number reduced by the number of Happiness.

\begin{table*}[h]
  \caption{Number of image distribution among expression classes for the Aff-Wild2 dataset.}
  \centering
  \begin{tabular}{ccccccccccc}
    \toprule
    \multicolumn{2}{c}{\multirow{2}{*}{Dataset}} & \multicolumn{8}{c}{Expression} & \multirow{2}{*}{Total} \\ \cmidrule{3-10}
    {} & {} & Neutral & Anger & Disgust & Fear & Happiness & Sadness & Surprise & Other & {} \\
    
    \midrule
    \multirow{4}{*}{Train} & Number & \multirow{2}{*}{180227} & \multirow{2}{*}{17153} & \multirow{2}{*}{10978} & \multirow{2}{*}{9110} & \multirow{2}{*}{97302} & \multirow{2}{*}{80671} & \multirow{2}{*}{32033} & \multirow{2}{*}{169887} & \multirow{2}{*}{597361}\\ 
    &  of Data & \\
    \cmidrule{2-11}
    &  Ratio to & \multirow{2}{*}{0.302} & \multirow{2}{*}{0.029} & \multirow{2}{*}{0.018} & \multirow{2}{*}{0.015} & \multirow{2}{*}{0.163} & \multirow{2}{*}{0.135} & \multirow{2}{*}{0.053} & \multirow{2}{*}{0.284} & \multirow{2}{*}{1.000}\\
    &  total & \\
    \midrule
    \multirow{4}{*}{Validation} & Number & \multirow{2}{*}{83176} & \multirow{2}{*}{6127} & \multirow{2}{*}{5322} & \multirow{2}{*}{8473} & \multirow{2}{*}{34941} & \multirow{2}{*}{25594} & \multirow{2}{*}{12338} & \multirow{2}{*}{108259} & \multirow{2}{*}{284230} \\
    &  of Data & \\
    \cmidrule{2-11}
    &  Ratio to & \multirow{2}{*}{0.293} & \multirow{2}{*}{0.022} & \multirow{2}{*}{0.019} & \multirow{2}{*}{0.030} & \multirow{2}{*}{0.123} & \multirow{2}{*}{0.090} & \multirow{2}{*}{0.043} & \multirow{2}{*}{0.381} & \multirow{2}{*}{1.000} \\
    &  total & \\
    \bottomrule
  \end{tabular}
  
  \label{table_dataset_stat}
\end{table*}


\section{Experiment results}

\subsection{Implementation details}

Our experiments were conducted on a desktop computer with the following specifications: implemented in PyTorch, Ubuntu 20.4 operating System, 32GB RAM, and GPU GeForce RTX 1080ti with 11GB memory. The data set for evaluating the proposed method is the Aff-wild2 data set provided by the competition. The Aff-Wild2 basically consists of 8 classes with 7 emotions (’Neutral’, ’Anger’, ’Disgust’, ’Fear’, ’Happiness’, ’Sadness’, ’Surprise’) and ’Other’ classes. Table 1 shows the number of images and imbalance ratio to total for 8 emotions of Aff-Wild2. For Aff-wild2, we used only cropped aligned images provided by organizers and resize them to $224\times 224$ for network input. We adopted Swin Transformer \cite{liu2021swin} Tiny model for feature extraction network.

\subsection{Evaluation metric}

In the Affective Behavior Analysis in-the-wild (ABAW) Expression Classification competition, F1 score is used as an evaluation metric to assess the performance. F1 score is a commonly used metric that combines precision and recall into a single measure, and it is particularly useful in imbalanced datasets where the number of samples in each class is different. 
The $F_1$ score is defined as 
\begin{equation}
    F_{1} = \frac{1}{n}\sum_{i}^{n}{\frac{2 \times precision_{i} \times recall_{i}}{precision_{i} + recall_{i}}},
\end{equation}
where $n$ is the number of emotion class, $precision_{i}$ is the precision of the $i$-th class, and $recall_{i}$ is the recall of the $i$-th class.
\subsection{Results}

Table \ref{table_results} shows the results from our experiment on the validation set of Aff-Wild2. Table \ref{table_results} 

contains the F1-score of each validation set of image,  audio 3 streams. It also includes score fusion results of all streams.

\begin{table}[h]

  \caption{Experimental results for Aff-Wild2 validation set.}
  \centering
  \begin{tabular}{cccccc}
    \toprule
    Model & Stream   & F1-score \\ \midrule
    Baseline & - & 23.00 \\
    \midrule
    \midrule
    Swin-Tiny & Visual  &   \\ 
    \midrule
    Swin-Tiny &   & \\ 
    \midrule
    Swin-Tiny & Audio  &\\ 
    \midrule
    Swin-Tiny & Visual   & \\


    \bottomrule
  \end{tabular}
  
  \label{table_results}
\end{table}

\section{Conclusion}

In this paper, we have exploited multi-modal data with a Multi Modal Fusion Network, including cropped face images, and audio log spectrogram obtained from the Aff-Wild2 dataset, to solve the task of classifying eight facial expressions. Based on the recently introduced Swin Transformer with dynamic sampling, it showed better performance than the baseline.

\begin{thebibliography}{10}

\bibitem{muhammad2017facial}
Ghulam Muhammad, Mansour Alsulaiman, Syed~Umar Amin, Ahmed Ghoneim, and
  Mohammed~F Alhamid.
\newblock A facial-expression monitoring system for improved healthcare in
  smart cities.
\newblock {\em IEEE Access}, 5:10871--10881, 2017.

\bibitem{davoudi2019intelligent}
Anis Davoudi, Kumar~Rohit Malhotra, Benjamin Shickel, Scott Siegel, Seth
  Williams, Matthew Ruppert, Emel Bihorac, Tezcan Ozrazgat-Baslanti, Patrick~J
  Tighe, Azra Bihorac, et~al.
\newblock Intelligent icu for autonomous patient monitoring using pervasive
  sensing and deep learning.
\newblock {\em Scientific reports}, 9(1):1--13, 2019.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\bibitem{kollias2022abaw}
Dimitrios Kollias.
\newblock Abaw: Valence-arousal estimation, expression recognition, action unit
  detection \& multi-task learning challenges.
\newblock {\em arXiv preprint arXiv:2202.10659}, 2022.

\bibitem{kollias2021analysing}
Dimitrios Kollias and Stefanos Zafeiriou.
\newblock Analysing affective behavior in the second abaw2 competition.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3652--3660, 2021.

\bibitem{kollias2020analysing}
D~Kollias, A~Schulc, E~Hajiyev, and S~Zafeiriou.
\newblock Analysing affective behavior in the first abaw 2020 competition.
\newblock In {\em 2020 15th IEEE International Conference on Automatic Face and
  Gesture Recognition (FG 2020)(FG)}, pages 794--800.

\bibitem{kollias2021distribution}
Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou.
\newblock Distribution matching for heterogeneous multi-task learning: a
  large-scale face study.
\newblock {\em arXiv preprint arXiv:2105.03790}, 2021.

\bibitem{kollias2021affect}
Dimitrios Kollias and Stefanos Zafeiriou.
\newblock Affect analysis in-the-wild: Valence-arousal, expressions, action
  units and a unified framework.
\newblock {\em arXiv preprint arXiv:2103.15792}, 2021.

\bibitem{kollias2019expression}
Dimitrios Kollias and Stefanos Zafeiriou.
\newblock Expression, affect, action unit recognition: Aff-wild2, multi-task
  learning and arcface.
\newblock {\em arXiv preprint arXiv:1910.04855}, 2019.

\bibitem{kollias2019face}
Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou.
\newblock Face behavior a la carte: Expressions, affect and action units in a
  single network.
\newblock {\em arXiv preprint arXiv:1910.11111}, 2019.

\bibitem{kollias2019deep}
Dimitrios Kollias, Panagiotis Tzirakis, Mihalis~A Nicolaou, Athanasios
  Papaioannou, Guoying Zhao, Bj{\"o}rn Schuller, Irene Kotsia, and Stefanos
  Zafeiriou.
\newblock Deep affect prediction in-the-wild: Aff-wild database and challenge,
  deep architectures, and beyond.
\newblock {\em International Journal of Computer Vision}, pages 1--23, 2019.

\bibitem{zafeiriou2017aff}
Stefanos Zafeiriou, Dimitrios Kollias, Mihalis~A Nicolaou, Athanasios
  Papaioannou, Guoying Zhao, and Irene Kotsia.
\newblock Aff-wild: Valence and arousal ‘in-the-wild’challenge.
\newblock In {\em Computer Vision and Pattern Recognition Workshops (CVPRW),
  2017 IEEE Conference on}, pages 1980--1987. IEEE, 2017.

\end{thebibliography}



\end{document}
