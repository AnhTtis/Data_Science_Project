\let\latexarabic\arabic
\let\latexdocument\document
\let\latexenddocument\enddocument



\documentclass{biometrika}

\let\document\latexdocument
\let\enddocument\latexenddocument
\AtEndDocument{\printhistory}
\let\arabic\latexarabic
\def\rm{}

\usepackage{amsmath}

\usepackage{times}
\usepackage{bm}
\usepackage{natbib}

\usepackage[plain,noend]{algorithm2e}
\usepackage{macros}

\makeatletter
\renewcommand{\algocf@captiontext}[2]{#1\algocf@typo. \AlCapFnt{}#2} %
\renewcommand{\AlTitleFnt}[1]{#1\unskip}%
\def\@algocf@capt@plain{top}
\renewcommand{\algocf@makecaption}[2]{%
  \addtolength{\hsize}{\algomargin}%
  \sbox\@tempboxa{\algocf@captiontext{#1}{#2}}%
  \ifdim\wd\@tempboxa >\hsize%
    \hskip .5\algomargin%
    \parbox[t]{\hsize}{\algocf@captiontext{#1}{#2}}%
  \else%
    \global\@minipagefalse%
    \hbox to\hsize{\box\@tempboxa}%
  \fi%
  \addtolength{\hsize}{-\algomargin}%
}
\makeatother

\def\Bka{{\it Biometrika}}
\def\AIC{\textsc{aic}}
\def\T{{ \mathrm{\scriptscriptstyle T} }}
\def\v{{\varepsilon}}

\DeclareMathOperator{\Thetabb}{\mathcal{C}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\begin{document}

\jname{Preprint}
\jyear{}
\jvol{}
\jnum{}
\accessdate{March 15, 2023}



\title{Mean-variance constrained priors have finite maximum Bayes risk in the normal
location model}

\author{JIAFENG CHEN}
\affil{Department of Economics, Harvard University, Cambridge, Massachusetts, U.S.A. \email{jiafengchen@g.harvard.edu}}

\maketitle

\begin{abstract}

Consider a normal location model $X \mid \theta \sim \Norm(\theta, \sigma^2)$ with known
$\sigma^2$. Suppose $\theta \sim G_0$, where the prior $G_0$ has zero mean and unit
variance. Let $G_1$ be a possibly misspecified prior with zero mean and unit variance. We
show that the squared error Bayes risk of the posterior mean under $G_1$ is bounded,
uniformly over $G_0, G_1,
\sigma^2 > 0$. 

\end{abstract}

\begin{keywords}
Bayes risk; normal location model; Bayesian robustness; moments
\end{keywords}

\section{Introduction and main result}

We consider the estimation problem in the normal location model. For a known
$\sigma^2>0$, let $X \mid \theta \sim
\Norm(\theta, \sigma^2)$ be a Gaussian signal on some unknown parameter $\theta \in \R$.
We assume that $\theta \sim G_0$ is itself random, and we refer to $G_0$ as the true
prior. We can view $G_0$ either as some latent subjective beliefs or as some objective
sampling process for $\theta$. We additionally assume $G_0$ has zero mean and unit
variance. However, suppose the data analyst does not know $G_0$, but they do know that the
true prior has zero mean and unit variance. They instead use some possibly misspecified
prior $G_1$. Given $G_1$, the analyst's Bayes decision rule, for estimating $\theta$ under
squared error loss, is given by the posterior mean under $G_1$\[
\mu_1(x) = \frac{\int_{-\infty}^\infty \theta \varphi \pr{
  \frac{x-\theta}{\sigma} 
} \sigma^{-1} G_1(d\theta)}{\int_{-\infty}^\infty \varphi \pr{
  \frac{x-\theta}{\sigma} 
} \sigma^{-1} G_1(d\theta)}, \quad \varphi(t) = \frac{1}{\sqrt{2\pi}} e^{-t^2/2}.
\]
The Bayes risk of $\mu_1(x)$, under the true prior $G_0$, is \[
R(G_0, G_1, \sigma^2) = \E[(\mu_1(X) - \theta)^2],
\]
where the expectation integrates over $\theta \sim G_0$ and $X \mid \theta \sim \Norm
(\theta, \sigma^2)$. Let $\mathcal M$ denote the set of distributions on $\R$ with zero
mean and unit variance. The worst-case Bayes risk, over $G_0, G_1, \sigma^2$, is given by
\[
\bar R = \sup \br{R(G_0, G_1, \sigma^2) : {G_0 \in \mathcal M, G_1 \in \mathcal M, \sigma > 0}}.
\]
This paper proves the following bound on $\bar R$.

\begin{theorem}
\label{thm1}
Under the preceding setup, $\bar R < 535$. 
\end{theorem}

\section{Discussion}

Gaussian decision theory is a canonical problem in statistics. Our result upper bounds the
Bayes risk of misspecified Bayes decision rules in this setting. Misspecification refers
to the fact that the prior $G_1$ used to derive Bayes decision rules may be different from
the prior $G_0$ used to evaluate their Bayes risk. This risk quantity features in some
classical results, for instance, in Lemma 4.8 of \citet{johnstone}.

Our result relates to the Bayesian literature on partial prior information \citep[see
chapter 3 in][and references therein]{robert2007bayesian}, where information is available
for some features of a decision-maker's beliefs, but the full prior is too difficult to
elicit.  With only partial information on the prior, this literature is interested in
robustness of the posterior distribution with respect to the prior choice. Viewed through
this lens, we think of $G_0$ as some true subjective beliefs that are hard to elicit, and
$G_1$ as some convenient approximation of $G_0$. In our case, the decision-maker has
access to the first two moments of the prior distribution $G_0$, which are normalized to
zero and one without loss of generality. When the prior distribution admits constraints in
terms of moments, the closest result to our knowledge is \cite{goutis1994ranges}, who
shows that the posterior mean is pointwise finite. That is, in our notation, for all $x$,
\[
\sup_{G_1 \in \mathcal M} |\mu_1(x)| < \infty.
\]
Compared to \citet{goutis1994ranges}, we restrict to the normal location model, but our
bound for the Bayes risk, which integrates over $x$, is not directly implied by the
pointwise-in-$x$ finiteness of the posterior mean.

Our interest in the result is motivated by empirical Bayes procedures
\citep{efron2019bayes}. Empirical Bayes in the Gaussian model envisions a setup where $X_i
\mid \theta_i, \sigma_i^2 \sim \Norm (\theta_i, \sigma_i^2)$ for $i = 1,\ldots,n$ and
$\theta_i \sim G_0$. Since we have repeated measurements over $i$, empirical Bayesians can
estimate the prior distribution $G_0$ and report posterior means relative to an estimate
$G_1$ of $G_0$. Here, $G_0$ no longer denotes a subjective belief, but instead some
assumed physical sampling process for the parameters $\theta_i$. Empirical Bayes
procedures face the challenge that the distribution $G_0$ itself is often much harder to
estimate than its moments \citep{dedecker2013minimax,pensky2017minimax}. In this context,
$\bar R$ puts a ceiling on the worst-case behavior of empirical Bayes procedures
corresponding to $G_1$, assuming that $G_1$ has the same moments as $G_0$. Specifically,
\citet{chen2023empiricalbayes} considers an empirical Bayes model where $(\theta_i,
\sigma_i) \sim P$, but $\theta_i$ may not be independent with $\sigma_i$ under $P$.
\citet{chen2023empiricalbayes} proposes to model $P$ as a location-scale family for
$\theta_i \mid \sigma_i$: \[P (\theta_i \le t \mid \sigma_i) = G_1\pr{\frac{t -
\E[\theta_i \mid \sigma_i]}{\var(\theta_i \mid \sigma_i)^{1/2}}},\] for $G_1$ of mean zero
and variance one. When this location-scale assumption fails, empirical Bayes procedures
based on this assumption use some common $G_1$ to compute posterior means for $\tau_i =(
\theta_i - \E[\theta_i \mid \sigma_i]) / \var(\theta_i \mid \sigma_i)^ {1/2}$, whose true
prior distribution may be different from $G_1$, but obeys the moment restrictions by
construction. Our result would then bound the worst-case behavior of this procedure.

To be clear, if we are solely concerned about worst-case behavior and are free to choose
$G_1$, then we do not need \cref{thm1}. The choice $G_1 = \Norm(0,1)$ minimizes the
maximum risk $\sup
\br{R(G_0, G_1, \sigma^2): G_0 \in \mathcal M}$ for any $\sigma^2 > 0$. This minimax risk
is the posterior variance $
\sigma^2/(\sigma^2 + 1)$, which is at most 1. However, note that, under $G_1 = \Norm
(0,1)$, $R(G_0, G_1, \sigma^2) = \sigma^2/(\sigma^2 + 1)$ is constant over $G_0 \in
\mathcal M$, and so the Gaussian prior $G_1$ is pessimistic in the sense that every
underlying $G_0$ attains $G_1$'s worst-case behavior. In contrast, using a non-Gaussian
$G_1$ may be advantageous when $G_1$ is close to $G_0$. In this case, \cref{thm1} is
useful in providing a worst-case assurance.

The specific value of the bound, $535$, is not tight. We suspect our proof method can be
refined in various places to obtain a tighter constant. We conjecture that the tightest
possible upper bound is 2, which is obtained by the construction used to prove the
following lemma. Unfortunately, an early step in our proof already pushes the upper bound
over 2, and it is thus unlikely that we could achieve this upper bound with our line of
argument.

\begin{lemma}
\label{lowerbound}
Under the preceding setup, $\bar R \ge 2$.
\end{lemma}

\begin{proof}
Suppose $G_1$ puts $1/2$ mass each on $\br{-1, +1}$. Let $\epsilon > 0$. Suppose $G_0$
puts mass $\epsilon^2 / (1+\epsilon^2)$ on $-1/\epsilon$, and mass $1/(1+\epsilon^2)$ on
$\epsilon$. We verify that $G_0, G_1 \in \mathcal M$. The posterior distribution under
prior $G_1$ has mass for $\theta=1$ equal to \[ p_1(X) = \P_{G_1}( \theta = 1 \mid X) =
\Lambda\pr{
  \frac{1}{2\sigma^2} \bk{ \pr{-1 - X}^2 - (1 - X)^2}
}, \quad  \Lambda(t) = \frac{1}{1+e^{-t}}.
\]
Thus the posterior mean is $2p_1(X) - 1$. The Bayes risk is \begin{align*}
R(G_0, G_1, \sigma^2) &= \frac{\epsilon^2}{1+\epsilon^2} \E\bk{\pr{2p_1(X) - 1 + \frac{1}
{\epsilon}}^2 \mid
\theta = -\frac{1}{\epsilon}} \\ &\quad + \frac{1}{1+\epsilon^2} \E\bk{\pr{2p_1(X) - 1 - 
\epsilon}^2 \mid
\theta = \epsilon}.
\end{align*}
Now, taking $\sigma \to 0$ and then $\epsilon \to 0$ shows that \[
\lim_{\epsilon \to 0} \lim_{\sigma \to 0} R(G_0, G_1, \sigma^2) = \lim_{\epsilon \to 0} 
\frac{\epsilon^2}
{1+\epsilon^2} (1/\epsilon -
1)^2 + \frac{1}{1+\epsilon^2} (1-\epsilon)^2 = 2.
\]
This completes the proof.
\end{proof}

There may be reasons to suspect the upper bound is indeed $2$. Note that if we simply
ignored the data and sampled $\hat\theta \sim G_1$ independently as our decision, then we
would achieve Bayes risk of 2. Conditioning on the data $X$ should bring the posterior
under $G_1$ closer to $G_0$, and so it should not hurt the Bayes risk.\footnote{Note that
this heuristic leads to an incorrect conclusion if we report the prior mean under $G_1$
when we ignore the data, instead of sampling $\hat\theta \sim G_1$. In that case, we
achieve a risk of 1 but the worst-case risk is at least 2.} \citet{zellner1988optimal}
provides one information-theoretic sense in which this is true, as the posterior minimizes
a weighted average of the Kullback--Leibler divergence to the prior and the integrated
log-likelihood. However, since we make no support or continuity restrictions on $G_0,
G_1$, it is not immediate how to use these results in terms of information-theoretic
divergences.

\section*{Acknowledgement}
The author is particularly grateful to Isaiah Andrews, Xiao-Li Meng, Natesh Pillai, Neil
Shephard, and Elie Tamer for their comments. In early conversations, Alexander Frankel
suggested the configuration in the proof of \cref{lowerbound}.



\bibliographystyle{biometrika}
\bibliography{main}


\appendix

\appendixone
\section*{Proof of \cref{thm1}}

The main idea of the proof is as follows: Using a result of \citet{goutis1994ranges}, any
distribution in $\mathcal M$ may be represented by a mixture of discrete distributions in
$\mathcal M$ with at most three support points. Jensen's inequality then shows that the
optimal for $\bar R$ is achieved at a distribution with at most three support points, and
so we may assume $G_1$ satisfies this restriction. Chebyshev's inequality yields a bound
on the mass points of $G_1$ that are far away from zero, and implies that some mass point
is close to zero. The posterior mass ratio between any mass point far away from zero and
the mass point close to zero is tractable, and the expected squared error of $\mu_1(x)$
can be bounded in terms of the posterior mass ratio. We obtain an upper bound on $\bar R$
through further bounding various expressions.

\begin{proof}[of \cref{thm1}]
We first recall Lemma 1 in \citet{goutis1994ranges}, which states that $G_1$ can be
represented as a mixture over mean-zero, variance-one distributions with at most three
support points. Observe that a mean-zero, variance-one distribution supported on at most
three points is uniquely determined by its support points, since the probability masses
assigned to the support points must satisfy moment constraints. We let $a$ denote a
particular arrangement of the support points, and $\mathcal A$ denote the set of all such
arrangements. \citet{goutis1994ranges} states that there exists some mixing distribution
$\Pi$ over $\mathcal A$ such that, for any Borel set $A \subset \R$, \[
\P_{G_1}(A) = \int \P_{G_a}(A) \,d\Pi(a),
\]
where $G_a$ is a mean-zero, variance-one distribution supported on $a$. 

For a given arrangement of support points $a$, let $\theta_k(a)$ denote the individual
support points, where $k$ ranges from $1$ to either $2$ or $3$. Then, the posterior
mean over $G_1$ is an average over the posterior
means for $G_a$, which can further be written as an average of $\theta_k(a)$:\[
\mu_1(x) = \int \sum_{k} \P_{G_a}\pr{\theta = \theta_k(a) \mid X=x} \theta_k(a) \,d\Pi(a).
\]
By Jensen's inequality, \[
\E[(\mu_1(X) - \theta)^2 ] \le \int \sum_k \E\bk{\P_{G_a}\pr{\theta = \theta_k(a) \mid
X=x} (\theta_k(a) - \theta)^2 } \,d\Pi(a).
 \numberthis \label{eq:decompose}
\]
We can write $X = \theta + \sigma Z$, where $Z \sim \Norm(0,1)$. Correspondingly, we can
define the posterior mass in terms of $\sigma, \theta,$ and $Z=z$: Let $
\pi_{k,a} (\sigma, \theta, z) = \P_{G_a}(\theta = \theta_k(a) \mid X = \theta + \sigma z).
$
We can also define its expectation over $Z$: Let $
\bar \pi_{k,a}(\sigma, \theta) = \int_{-\infty}^\infty \pi_{k,a} (\sigma, \theta, z)
\varphi(z) \,dz.
$
Substituting these into \eqref{eq:decompose}, we have that \[
\E[(\mu_1(X) - \theta)^2] \le \int \sum_k \E\bk{\bar\pi_{k,a}(\sigma, \theta) (\theta_k
(a) - \theta)^2} \,d\Pi(a)
\]
where the expectation $\E[\cdot]$ integrates solely over $\theta \sim G_0$. 

It suffices to bound $\sum_k \E\bk{\bar\pi_{k,a}(\sigma, \theta) (\theta_k (a) -
\theta)^2}$ over
choices of $G_0, \sigma,$ and the support point arrangement $a$. To that end, consider a
fixed but arbitrary $a$ with support points $\br{\theta_k}$. Since we have fixed some $a$,
we suppress it from notation. 

\textbf{Case 1: All of $G_a$'s support points are within $[-2,2]$.} 
In this case,  we can expand \begin{align*}
\sum_k \E\bk{\bar\pi_{k, a}(\sigma, \theta) (\theta_k -
\theta)^2} &= \E[\theta^2] + \sum_k \E[\bar\pi_{k, a}(\sigma,\theta)] \theta_k^2 - 2
\E\bk{\theta\sum_k \bar\pi_{k, a}(\sigma, \theta) \theta_k} \\ 
&\le 1 + 2^2 +  2 \E[|\theta| \cdot 2] \\ 
&\le 1 + 4 + 4  \\ 
&= 9.
\end{align*}

\textbf{Case 2: At least one of $G_a$'s support points are outside of $[-2,2]$.} In this
case, there are at least one and at most two support points within $[-2,2]$. By
Chebyshev's inequality, then, there is at least one support point within $[-2,2]$ with
mass  exceeding $\pi_0 = \frac{1}{2}\pr{1-\frac{1}{4}} = \frac{3}{8}$. We can denote that
support point as $\theta_0$. For a given $\theta_k$, we shall compare its posterior
probability to $\theta_0$. As a result, it will be convenient to translate by $-\theta_0$
so that we can normalize this comparison point to zero. 

Precisely speaking, let $H \sim \theta - \theta_0$ when $\theta \sim G_a$. Let $H_0 \sim
\theta - \theta_0$ when $\theta \sim G_0$. By construction, $\P_H(\br{0}) \ge \pi_0$.
Denote the mass points of $H$ by $\tau_k = \theta_k - \theta_0$. Observe that \[
\P_H(\tau=\tau_k \mid X = \theta + \sigma z) = \P_{G_a}(\theta = \theta_k \mid X=  \theta
+ \sigma z) = \pi_{k, a}(\sigma, \theta, z).
\]
Thus, we define \[
\pi_{k}(\sigma, \tau, z) = \pi_{k,a} (\sigma, \tau + \theta_0, z), \quad \bar\pi_k
(\sigma, \tau) = \int \pi_k(\sigma,\tau,z)\varphi(z)\,dz.
\]

We are interested in $\sum_k \E\bk{\bar\pi_{k, a}(\sigma, \theta) (\theta_k -
\theta)^2}$, which is preserved by the translation: \[
\sum_k \E\bk{\bar\pi_{k, a}(\sigma, \theta) (\theta_k -
\theta)^2} = \sum_k \E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2}
\]
where the $\E[\cdot]$ on the right-hand side integrates over $\tau \sim H_0$. As a result,
we shall bound $\sum_k \E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2}$. 

After the translation, we note that the second moments of $H$ and $H_0$ are now bounded by
$5 = 1 + 2^2$. Since there are at most two non-zero support points $\tau_k \neq 0$, we
note that \begin{align*}
\sum_k \E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2} &\le 2 \max_{k: \tau_k \neq 0} \E[(\tau_k - \tau)^2 \bar\pi_k(\sigma, \tau)] +
\E
[\tau^2] \\&= 5 + 2 \max_{k: \tau_k \neq 0} \E[(\tau_k - \tau)^2 \bar\pi_k(\sigma, \tau)].
\numberthis \label{eq:boundtau}
\end{align*}
Thus, it suffices to bound $ \E[(\tau_k - \tau)^2 \bar\pi_k(\sigma, \tau)]$ for some
nonzero $\tau_k$.  Without loss of generality, assume $\tau_k > 0$. Define the constants
$C_0 = \pi_0/5 = 3/40 < 1$ and $s_0 = \sqrt{1/C_0} = 2\sqrt{10/3} > \sqrt{5}  > 2$. 

\textbf{Case 2.1: $\tau_k \le s_0$.} In this case, \begin{align*}
\E[(\tau_k - \tau)^2 \bar\pi_k(\sigma, \tau)] \le \E[(\tau_k - \tau)^2] = \E[(\theta_k
- \theta)^2] = 1 + \theta_k^2
\end{align*}
Note that $|\theta_k| = |\tau_k + \theta_0| \le s_0 + 2$. Thus, plugging into the
preceding display, in this case,
\[
\E[(\tau_k - \tau)^2 \bar\pi_k(\sigma, \tau)] \le 33,
\]
and thus \[
\sum_k \E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2} \le 71. 
\]

\textbf{Case 2.2: $\tau_k > s_0$.} We decompose $\E[(\tau_k - \tau)^2 \bar\pi_k(\sigma,
\tau)]$ on four intervals for $\tau$, where \[
\R = (-\infty, - \tau_k] \cup [\tau_k/2, \infty) \cup (-\tau_k, 0) \cup (0, \tau_k / 2).
\]
Note that if $\tau \in (-\infty, - \tau_k]$, then $
(\tau_k - \tau)^2 \le 4\tau^2$. If $\tau \in
[\tau_k/2,\infty)$, then $(\tau_k - \tau)^2  \le \tau^2$. Thus, \begin{align*}
\E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2} \le \,&\E[4\tau^2 \one\pr{\tau \le -\tau_k}] + \E[\tau^2 \one\pr{\tau \ge
\tau_k/2}] \\&\quad + \int_{-\tau_k}^0 (\tau_k - \tau)^2 \bar\pi_k(\sigma,\tau) H_0(d\tau)
 + \int_0^{\tau_k/2} (\tau_k - \tau)^2 \bar\pi_k(\sigma,\tau) H_0(d\tau).
\end{align*}
The first two terms in the preceding display are bounded by $
\E[4\tau^2 \one(\tau < -\tau_k \cup \tau > \tau_k/2)].
$
The next two terms are respectively bounded by the ensuing \cref{lemma:first_k,lemma:second_k}. The key to doing so is
 a bound of $\bar\pi_k(\sigma,\tau) $ in terms of the posterior ratio (\cref{lemma:develop_integral}). Note that \[
\pi_k(\sigma,\tau, z) \le \min\pr{\frac{\P_H(\tau = \tau_k \mid \sigma z + \tau)}{\P_H
(\tau = 0
\mid \sigma z + \tau)}, 1},
 \]
 where the right-hand side can be integrated tractably.

Collecting the results in \cref{lemma:first_k,lemma:second_k}, we
find that \[
\E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2} \le
\pr{4\maxwith \pr{1.5+\frac{3}{\sqrt{C_0}}} \maxwith \frac{2.92}{C_0}} \E[\tau^2] + 
\frac{2.92}
{C_0} + \frac{3}
{2C_0} +
\frac{3}{\sqrt{C_0}} \le 264.6
\]
where $\E[\tau^2] \le 5$. Plugging into \eqref{eq:boundtau}, we obtain an upper bound \[
\E\bk{\bar\pi_{k}(\sigma, \tau) (\tau_k -
\tau)^2} \le 535.
\]
This is worse than the bounds obtained from Case 2.1 and Case 1. Hence, $\bar R \le 535.$
\end{proof}


\begin{lemma}
\label{lemma:develop_integral}
Let $\Phi(t) = \int_{-\infty}^t \varphi(x)\,dx$ be the cumulative distribution function of
a standard Gaussian random variable. Let $\bar \Phi(t) = 1-\Phi(t)$ be its complement.  
If $\tau_k \ge \sqrt{5}$, then \begin{align*}
\bar \pi_k (\sigma,\tau) &\le \frac{1}{C_0 \tau_k^2} \exp\pr{
\frac{\tau_k \tau}{\sigma^2}} \bar \Phi\pr{
    \frac{\tau_k/2 + \tau}{\sigma} - \frac{\sigma} {\tau_k} \log (C_0 \tau_k^2)
} + \bar \Phi\pr{
    \frac{\tau_k/2 - \tau}{\sigma} + \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
    }\numberthis \label{eq:integral_bound}
\end{align*}
where we recall $C_0 = \frac{\pi_0}{5} = \frac{3}{40}$.
\end{lemma}
\begin{proof}
Note that, \[
\pi_k(\sigma, \tau, z) = \pr{\P_H(\tau=0 \mid \sigma z + \tau) \frac{ \P_H(\tau=\tau_k
\mid
\sigma
z + \tau)}{ \P_H(\tau=0 \mid \sigma z + \tau)}} \minwith 1 \le R(\sigma, \tau, z) \minwith
1. 
\]
where $R = \frac{ \P_H(\tau=\tau_k \mid
\sigma
z + \tau)}{ \P_H(\tau=0 \mid \sigma z + \tau)} $ is the posterior mass ratio. Note that,
since $
\P(\tau = \tau_k)\tau_k^2 \le \E_H[\tau^2] \le 5,
$
the prior mass is bounded: $\P(\tau = \tau_k) \le \frac{5}{\tau_k^2}$. We can thus compute 
the posterior mass ratio \[
R(\sigma, \tau, z) \le \frac{1}{C_0} \frac{1}{\tau_k^2}
\exp
\pr{
    \frac{\tau_k}{\sigma} z + \frac{\tau_k (\tau - \tau_k/2)}{\sigma^2}
},
\]
where the inequality stems from bounding the prior mass ratio with $\frac{5/\tau_k^2}
{\pi_0}.$
Define the right-hand side of the above display as $\bar R(\sigma, \tau, z)$.
We note that
$\bar R(\sigma,\tau,z)\le 1$ if and only if $z \le u$  where  $u = \frac{\sigma}
{\tau_k} \log (C_0
\tau_k^2) + \frac{\tau_k/2 - \tau}{\sigma}.
$ Hence, letting $\bar \Phi(t) = 1-\Phi(t)$
denote the complementary normal CDF, we compute that \begin{align*}
\bar \pi_k(\sigma, \tau) &\le \int_{-\infty}^u \bar R(\sigma, \tau,
z)
\varphi(z) \,dz + (1-\Phi(u)) \\ 
&= \frac{1}{C_0 \tau_k^2} \exp\pr{\frac{\tau_k \tau}{\sigma^2}} \bar \Phi\pr{
    \frac{\tau_k/2 + \tau}{\sigma} - \frac{\sigma} {\tau_k} \log (C_0 \tau_k^2)
} + \bar \Phi\pr{
    \frac{\tau_k/2 - \tau}{\sigma} + \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2) }.
\end{align*}
This completes the proof. 
\end{proof}



\begin{lemma}
\label{lemma:first_k}
Recall that $s_0 = \sqrt{1/C_0} = \sqrt{40/3}$. Define $K_{(-\tau_k, 0)} = \int_
{-\tau_k}^0 (\tau_k - \tau)^2 \bar\pi(\sigma,\tau)\,H_0(d\tau)$.
For $\tau_k \ge s_0$, \[
K_{(-\tau_k, 0)}  \le \frac{3}{2C_0} + \frac{3}{\sqrt{C_0}} + 
\pr{\frac{3}{2} + \frac{3}{\sqrt{C_0}}} \E[\tau^2 \one(-\tau_k \le \tau \le 0)]
\numberthis
\label{eq:first_k}.
\]
\end{lemma}
\begin{proof}

Define $c = c(\tau) = \tau/\tau_k \in [-1,0]$ when $\tau \in [-\tau_k,0]$. Then, plugging
in
\eqref{eq:integral_bound}, 
\[
K_{(-\tau_k, 0)} \le  \int_{-\tau_k}^0 \frac{(1-c)^2}{C_0} + 
(1-c)^2 \tau_k^2 \bar\Phi\pr{
    (1/2-c) \frac{\tau_k}{\sigma} + \frac{\sigma}{\tau_k} \log (C_0\tau_k^2)
}
\,H(d\tau)
\]
where we use the fact that $\exp(\tau_k\tau/\sigma^2) \le 1$ and $\bar\Phi(\cdot) < 1$ to
simplify the first term in \eqref{eq:integral_bound}. These are true since we integrate
over $(-\tau_k, 0)$.

Observe that, by \cref{lemma:AMGM}, \[
\bar\Phi\pr{
    (1/2-c) \frac{\tau_k}{\sigma} + \frac{\sigma}{\tau_k} \log (C_0\tau_k^2)
} \le \frac{1}{2} \exp\pr{
    -(1-2c) \log (C_0\tau_k^2)
} = \frac{1}{2} (C_0 \tau_k^2)^{2c-1} = \frac{1}{2} C_0^{2c-1} \tau_k^{4c-2}.
\]
Hence, \begin{align*}
K_{(-\tau_k, 0)} &\le  \int_{-\tau_k}^0 \frac{(1-c)^2}{C_0} + \frac{(1-c)^2}{2} 
C_0^{2c-1}
\tau_k^{4c}\,H_0(d\tau) \\
&= \frac{1}{C_0} \int_{-\tau_k}^0 (1-c)^2 \bk{1 + \frac{1}{2} (\sqrt{C_0} \tau_k)^{4c}}\,
H_0(d\tau) \\
&\le \frac{1}{C_0} \int_{-\tau_k}^0 \frac{(\tau_k-\tau)^2}{\tau_k^2} \pr{1 + \frac{1}
{2}}\,H_0(d\tau) \tag{Note that $(\sqrt{C_0}\tau_k)^{4c} \le 1$}\\
&= \frac{3}{2}\E\bk{\one(-\tau_k \le \tau \le 0) \frac{(\tau_k-\tau)^2}{C_0\tau_k^2}} 
\\
&\le \frac{3}{2C_0} \pr{
    1 + \frac{2}{\tau_k} \E[|\tau| \one(-\tau_k \le \tau \le 0)] + \frac{1}{\tau_k^2}
    \E[\tau^2 \one(-\tau_k \le \tau \le 0)]
}  \\ 
&\le \frac{3}{2C_0} + \frac{3}{\sqrt{C_0}}\E[|\tau| \one(-\tau_k \le \tau \le 0)] + 
\frac{3}{2} \E[\tau^2 \one(-\tau_k \le \tau \le 0)] \tag{$\tau_k > 1/\sqrt{C_0}$}.
\end{align*}
Conclude by noting that \[
\E[|\tau| \one(-\tau_k \le \tau \le 0)] \le \E[(\tau^2 \vee 1) \one(-\tau_k \le \tau \le
0)] \le \E[\tau^2 \one(-\tau_k \le \tau \le
0)] + 1. 
\]
This completes the proof.
\end{proof}


\begin{lemma}
\label{lemma:second_k}
Assume that $\tau_k > s_0$. 
Define $K_{(0, \tau_k/2)} = \int_{0}^{\tau_k/2} (\tau_k - \tau)^2 \bar\pi(\sigma,\tau)\,H_0(d\tau)$.
Then \[
K_{(0,\tau_k/2)} \le \frac{2.92}{C_0} + \frac{2.92}{C_0}\E[\tau^2 \one(\tau \in (0,
\tau_k/2))].
\]
\end{lemma}
\begin{proof}
Let $c = 
\frac{\tau}{\tau_k} \in [0, 1/2]$. Then we can
write \eqref{eq:integral_bound}, multiplied by $(\tau_k - \tau)^2 = (1-c)^2 \tau_k^2$, as 
\[
\frac{(1-c)^2}{C_0} \exp\pr{\frac{c\tau_k^2}{\sigma^2}} \bar\Phi\pr{
    (c+1/2) \frac{\tau_k}{\sigma} - \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
} + (1-c)^2 \tau_k^2 \bar\Phi\pr{
    (1/2-c) \frac{\tau_k}{\sigma} + \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
}
\numberthis
\label{eq:csubbed}
\]
This quantity upper bounds the integrand in $K_{(0,\tau_k/2)}$. Let us consider the first
term.

\textbf{First term, case a: } In particular, let us consider the case where \[
\tau_k^2 \le (1/2 + c)^{-1} \sigma^2 \log (C_0\tau_k^2).
\]
In this case, the first term in \eqref{eq:csubbed} is bounded by the following, via
ignoring the $\bar\Phi(\cdot)$ term: \begin{align*}
\frac{(1-c)^2}{C_0} \exp\pr{
    \frac{c}{1/2 + c} \log (C_0 \tau_k^2)
} &= (1-c)^2 C_0^{-\frac{1}{1+2c}} \tau_k^{
    \frac{4c}{1+2c}
} \\
&\le \frac{1}{C_0} (1-c)^2 \tau_k^{\frac{4c}{1+2c}} \\
&\le \frac{1}{C_0} \tau^{4c/(1+2c)} (1-c)^2 \pr{1/c}^{\frac{4c}{1+2c}} \\
&\le 
\frac{1.76}
{C_0}
(\tau \maxwith 1)^2.
\end{align*}
The last inequality follows from the bound \[
\sup_{c \in [0,1/2]} (1-c)^2 (1/c)^{\frac{4c}{1+2c}} \le 1.76.
\]
and $\tau^{4c/(1+2c)} \le \tau \vee 1 \le (\tau \vee 1)^{2}$

\textbf{First term, case b: } Now, assume that \[
\tau_k^2 > (1/2 + c)^{-1} \sigma^2 \log (C_0\tau_k^2) \iff 
(1/2+c) \tau_k/\sigma >  \frac{\sigma}{\tau_k} \log(C_0\tau_k^2)
\]
so that the argument in $\bar\Phi$ in the first term is positive. Now, \begin{align*}
&\frac{(1-c)^2}{C_0} \exp\pr{c\tau_k^2/\sigma^2} \bar\Phi\pr{
    (c+1/2) \frac{\tau_k}{\sigma} - \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
} \\
&\le \frac{1}{2C_0}(1-c)^2 \exp\bk{
    c \frac{\tau_k^2}{\sigma^2} - \frac{1}{2} \pr{
        (c+1/2) \frac{\tau_k}{\sigma} - \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
    }^2
} \tag{\cref{lemma:mills}}
\end{align*}
We compute that, by expanding and applying the inequality $(a+b) \ge 2\sqrt{ab}$ for
nonnegative $a, b$,
\begin{align*}
&c \frac{\tau_k^2}{\sigma^2} - \frac{1}{2} \pr{
        (c+1/2) \frac{\tau_k}{\sigma} - \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
    }^2 
\\
&=c\frac{\tau_k^2}{\sigma^2} - \frac{1}{2}(c+1/2)^2 \frac{\tau_k^2}{\sigma^2} - 
\frac{\sigma^2}{2\tau_k^2} \log(C_0 \tau_k^2)^2 + (c+1/2)\log(C_0 \tau_k^2) \\
&=(c+0.5) \log(C_0 \tau_k^2) - \br{
  (0.5(c+0.5)^2-c) \frac{\tau_k^2}{\sigma^2} + \frac{1}{2}\log(C_0\tau_k^2)^2 
  \frac{\sigma^2}
  {\tau_k^2}
}
    \\&\le -\br{2\sqrt{ \frac{1}{2}\pr{
        \frac{1}{2}(c+0.5)^2 - c
        }} - (c+0.5)} \log (C_0\tau_k^2) \\&= 2c \log(C_0 \tau_k^2).
\end{align*}
Hence, the first term in \eqref{eq:csubbed} is bounded by \[
\frac{1}{2C_0} (1-c)^2 (C_0 \tau_k^2)^{2c} = (1-c)^2\frac{\tau^{4c}}{2C_0} C_0^{2c} (1/c)^
{4c}  \le \frac{1.16}{C_0}(\tau \vee 1)^2.
\]
The last inequality comes from maximizing the expression over $c \in [0,1/2]$. \textbf{The
two cases imply that the first term is bounded by $1.76 C_0^{-1} (\tau \vee 1)^2$.}


\textbf{Second term: } The second term in \eqref{eq:csubbed} is bounded by 
\begin{align*}
&(1-c)^2 \tau_k^2 \bar\Phi\pr{
    (1/2-c) \frac{\tau_k}{\sigma} + \frac{\sigma}{\tau_k} \log (C_0 \tau_k^2)
} \\ 
&\le (1-c)^2 \tau_k^2 \frac{1}{2} \exp\pr{(2c-1) \log(C_0 \tau_k^2)} \tag{\cref{lemma:AMGM}}\\
&\le \frac{1}{2C_0} \tau^{1+2c} c^{-4c} (1-c)^2 \\
&\le \frac{1.16}{C_0} (\tau \maxwith 1)^2 \tag{Maximizing over $c$ and using $\tau^
{1+2c}\le (\tau\vee 1)^2$}
\end{align*}

Hence, overall, the integrand \eqref{eq:csubbed} is bounded by $
\frac{2.92}{C_0} (\tau \vee 1)^2.
$
We use $\tau^2 \vee 1 \le \tau^2 + 1$ to conclude the proof. 
\end{proof}



\begin{lemma}[Mill's ratio bound]
\label{lemma:mills}
For all $t >0$, 
\[\bar\Phi(t) \le 
\frac{\varphi(t)}{t \maxwith \sqrt{2/\pi}} \le \sqrt{\pi/2}\varphi(t) = \frac{1}{2} e^
{-t^2/2}.
\]
\end{lemma}
\begin{proof}
It is well known that $\bar\Phi(t) \le \frac{\varphi(t)}{t}$ for all positive $t$, which
is tighter when $t$ is large. At $t = 0$, the truncation $\sqrt{2/\pi}$ is such that both
sides are equal to $1/2$. We can differentiate for $t < \sqrt{2/\pi}$ to verify that
$\bar\Phi'(t) < \sqrt{\pi/2}\varphi'(t)$. 
\end{proof}


\begin{corollary}
\label{lemma:AMGM}
For nonnegative $a,b \in \R$, $\bar\Phi(a+b) \le \frac{1}{2}
\exp\pr{-2ab}.$
\end{corollary}
\begin{proof}
Immediate by noting that the arithmetic-mean--geometric-mean inequality implies $\bar\Phi
(a+b)
\le \bar\Phi(2\sqrt{ab})$ and applying \cref{lemma:mills}.
\end{proof}
\end{document}
