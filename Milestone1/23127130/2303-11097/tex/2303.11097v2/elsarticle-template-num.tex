
\documentclass[final,3p,times,sort&compress]{elsarticle}




\usepackage{amssymb}
\usepackage{amsthm}
\newdefinition{rem}{Remark}


\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{aligned-overset} %
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{external}
\tikzexternalize[prefix=externalized_figs/] %

\usepackage[hidelinks]{hyperref} %
\usepackage{cleveref}	%
\crefname{equation}{}{}

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{assum}{Assumption}
\newtheorem{prop}{Proposition}

\newcommand{\TTT}{\ensuremath{T_{\mathrm{TT}}}}
\newcommand{\TET}{\ensuremath{T_{\mathrm{ET}}}}
\newcommand{\E}[1]{\ensuremath{\mathbb{E}\!\left\lbrack#1\right\rbrack}}
\newcommand{\V}[1]{\ensuremath{\mathbb{V}\!\left\lbrack#1\right\rbrack}}
\newcommand{\Prob}{\ensuremath{\mathbb{P}}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\Eset}{\ensuremath{\mathcal{E}}}
\newcommand{\Vset}{\ensuremath{\mathcal{V}}}
\newcommand{\trace}[1]{\ensuremath{\mathrm{tr}\!\left(#1\right)}}

\journal{Nonlinear Analysis: Hybrid Systems}

\begin{document}

\begin{frontmatter}


    \title{Time- versus Event-Triggered Consensus of a Single-Integrator Multi-Agent System\tnoteref{t1}}

    \tnotetext[t1]{%
        F. Allgöwer thanks the German Research Foundation (DFG) for support of this work within grant AL 316/13-2 and within the German Excellence Strategy under grant EXC-2075 - 285825138; 390740016.
        F. Aurzada thanks the DFG for support within grant AU 370/7.
        For the cooperation until Jan.\ 2022, M. Lifshits thanks for the support within grant RFBR 20-51-12004.
    }


    \author[label1]{David Meister\corref{cor1}}\ead{meister@ist.uni-stuttgart.de}
    \author[label2]{Frank Aurzada}\ead{aurzada@mathematik.tu-darmstadt.de}
    \author[label3]{Mikhail A. Lifshits}\ead{mikhail@lifshits.org}
    \author[label1]{Frank Allgöwer}\ead{allgower@ist.uni-stuttgart.de}

    \affiliation[label1]{organization={University of Stuttgart, Institute for Systems Theory and Automatic Control}, %
        country={Germany}}

    \affiliation[label2]{organization={Technical University of Darmstadt}, %
        country={Germany}}

    \affiliation[label3]{organization={St. Petersburg State University}, %
        country={Russia}}

    \cortext[cor1]{Corresponding author}    %

    \begin{abstract}
        Event-triggered control has shown the potential for providing improved control performance at the same average sampling rate when compared to time-triggered control.
        While this observation motivates numerous event-triggered control schemes, proving it from a theoretical perspective has only been achieved for a limited number of settings.
        Inspired by existing performance analyses for the single-loop case, we provide a first fundamental performance comparison of time- and event-triggered control in a distributed multi-agent consensus setting.
        For this purpose, we consider undirected connected network topologies and the long-term average of the quadratic deviation from consensus as a performance measure.
        The main finding of our analysis is that time-triggered control provably outperforms event-triggered control beyond a certain number of agents in our particular setting.
        We thereby provide an exemplary distributed problem setup in which event-triggered control results in a performance disadvantage when compared to time-triggered control in the case of large networks.
        Moreover, we derive the asymptotic orders of the performance measure under both triggering schemes which give more insights into the cost relationship for large numbers of agents.
        Thus, by presenting an analysis for a particular setup, this work points out that the often presumed superiority of event-triggered control over time-triggered control might not generally be provided if we consider distributed settings.
    \end{abstract}



    \begin{keyword}
        Event-triggered control \sep
        Multi-agent systems \sep
        Networked control systems \sep
        Sampled-data systems


    \end{keyword}


\end{frontmatter}


\section{Introduction}\label{sec:intro}


Event-triggered control (ETC) schemes have shown the potential to be more performant than time-triggered control (TTC) schemes when communication channels are loss- and delay-free, and average triggering rates are equal, as demonstrated in \cite{Astrom2002} for single-integrator systems.
In ETC, the system only initiates communication when a triggering condition is met, while in TTC, it establishes communication at fixed time intervals.
Findings like the one from \cite{Astrom2002} have led to a variety of ETC schemes aiming at the reduction of the sampling frequency while still fulfilling a certain control goal, such as maintaining a performance level.
The reduction in "unnecessary" communication often appears as an argument for ETC also being advantageous for communication channels with limited bandwidth.
The idea of using ETC to decrease shared medium utilization has been adopted from the field of networked control systems (NCS), e.g., \cite{Henningsson2008,Heemels2008}, to the field of multi-agent systems (MAS), as seen in works such as \cite{Seyboth2013,Nowzari2019}.
To distinguish between NCS that are only coupled through their usage of a shared communication medium and MAS in which agents also cooperate to achieve a common goal, the former will be referred to as non-cooperative NCS throughout this paper.

The setup from \cite{Astrom2002} with impulsive inputs has been extended in various ways in order to find ETC schemes that are optimal with respect to a defined performance measure.
Most works in this paragraph use a performance measure that is quadratic in the system state and linear in the triggering rate including a scalar trade-off factor.
For first-order linear systems, \cite{Henningsson2008} introduces a minimum inter-event time and aims to find the optimal triggering condition.
The work \cite[Paper~II]{Henningsson2012} establishes a closed-form solution for optimal triggering rules for the multidimensional integrator case and shows simulation-based results for the generalization to linear time-invariant systems.
The authors in \cite{Andren2017} provide a numerical design method for optimal triggering rules in an LQG setting with output feedback.
Their work builds upon \cite{Mirkin2017,Braksmayer2017,Goldenshluger2017} which provide an $\mathcal{H}_2$-optimal controller design method for any given uniformly bounded sampling pattern in a linear system setup.
Moreover, they prove that the design of optimal triggering rule and optimal controller are separable, which allows \cite{Andren2017} to focus on the former.
For discrete time systems, \cite{Cogill2009} proposes an optimal periodic ETC design method for linear time-invariant systems.

Since finding optimal ETC schemes remains challenging, \cite{Antunes2016,Antunes2018,Antunes2020,Balaghiinaloo2021,Balaghiinaloo2022} have introduced and evaluated a so-called consistency property of ETC schemes in various LQ- and $\mathcal{L}_2/\ell_2$-settings.
In short, an ETC scheme is considered consistent with respect to the chosen performance criterion (LQ, $\mathcal{L}_2/\ell_2$) if it guarantees the same performance level as any periodic TTC scheme while having a smaller (or equal) average triggering rate.
An analogous definition of consistency is that the ETC scheme results in a better (or equal) performance level compared to any periodic TTC scheme while having the same average triggering rate.
Thus, one can consider the work in \cite{Astrom2002} as an evaluation of LQ-consistency in a single-integrator setup with a particular choice of cost matrices.
Moreover, considering the optimality perspective from the previous paragraph together with the consistency viewpoint, \cite{Mi2022} presents an $\mathcal{H}_\infty$-optimal ETC design method for continuous-time LTI systems.
Their method co-designs the controller and triggering rule according to their $\mathcal{H}_\infty$-performance and guarantees consistency with respect to the corresponding optimal periodic sampled-data controller.

Following another research direction, \cite{Rabi2009} extended the results from \cite{Astrom2002} to incorporate also network effects such as packet loss in the comparison of TTC and ETC for the non-cooperative single-integrator NCS case.
They point out that ETC can perform worse than TTC above a certain packet loss probability.
In \cite{Blind2011} and \cite{Blind2011a}, transmission delays are included into the comparison and the packet loss probability is determined based on the medium access protocol.
At last, \cite{Blind2013} provides a performance comparison of TTC and ETC schemes for single-integrator systems considering various medium access protocols.
The authors demonstrate the impact of the network load on the performance of the single-integrator NCS for various triggering schemes and medium access protocols.
Thereby, they establish the importance of taking the properties of the communication network into consideration when designing triggering schemes for NCS.
Another performance comparison in this realm is presented in \cite{Balaghiinaloo2022} which analyzes linear discrete time systems and contrasts purely stochastic with stochastic event-based triggering rule performance.
Analyzing more general NCS and their behavior under (periodic) ETC schemes is an active field of research, e.g., \cite{Gleizer2020, Postoyan2022}.

Although some fundamental considerations have shown that TTC can sometimes outperform ETC if network effects are taken into account, ETC is still very popular for NCS.
As discussed previously in this section, many settings not suffering from network losses provably yield a performance improvement under ETC when compared to TTC.
This also led to various ETC approaches for MAS while there exists no work on the fundamental characteristics of TTC compared to ETC in this case.
As pointed out by \cite{Nowzari2019}, the event-triggered consensus literature is still missing performance analyses that quantify the benefit of ETC over TTC schemes.
This work aims to close this gap in order to understand whether qualitative results are the same for MAS as in the non-cooperative NCS case, or whether new effects might arise.
With the previously discussed works in mind, we provide a first theoretical evaluation of ETC and TTC performance by analyzing a simple MAS problem.
Our main contribution is the finding that, for this particular setup, ETC is not always superior to TTC even without considering packet loss or transmission delays.
The performance relationship turns out to depend on the number of participating agents.
Moreover, we provide the asymptotic order of the performance measure for ETC and TTC as a function of the number of agents.
This gives further insights into the relationship between ETC and TTC in our particular MAS setup.
Compared to the corresponding conference paper \cite{Meister2022}, we provide extensive proof details for all our results.
In addition, we extend our statements to more general communication topologies than all-to-all networks.
What is more, we generalize our setup to a class of provably optimal control inputs.
Firstly, this allows us to show that our results hold for a broader class of problems.
Secondly, further potential performance improvements via a better choice of the control input can be ruled out in our analysis.
Furthermore, we improve our simulation results by increased sample numbers, simulations for more network sizes and by complementing the obtained results with a confidence interval.

Our paper is structured as follows:
We start by stating some preliminaries on background knowledge, especially graph theory, and on our notation in Section~\ref{sec:prelim}.
In Section~\ref{sec:probform}, we introduce the setup and formulate the considered problem.
After that, we present our theoretical results in Section~\ref{sec:analysis}, while we demonstrate our findings in a numerical simulation in Section~\ref{sec:sim}.
We conclude this work in Section~\ref{sec:conclusion} and provide additional proofs and details in the appendix.


\section{Preliminaries}\label{sec:prelim}

In this section, we introduce relevant notation, especially but not exclusively regarding graph theory.
A graph $\mathcal{G}=(\Vset,\Eset)$ consists of a set of vertices $\Vset = \lbrace1,\dots,N\rbrace$, also referred to as nodes, and a set of edges $\Eset \subset \Vset\times\Vset$.
The graph $\mathcal{G}$ is called undirected if $(i,j)\in\Eset \Leftrightarrow (j,i)\in\Eset$ for any $i,j\in\Vset$.
We will focus on definitions for undirected graphs throughout the rest of this section.
If an edge $(i,j)\in\Eset$ exists between two nodes, they are called adjacent.
All nodes that are adjacent to node $i$ are also referred to as node $i$'s neighbors $j\in\mathcal{N}_i = \lbrace j\in\Vset\mid (i,j)\in\Eset\rbrace$.
Note that we generally exclude self-loops, i.e., $(i,i)\notin\Eset$.
In the MAS context, the adjacency of nodes $i$ and $j$ indicates that agents $i$ and $j$ are able to communicate with each other.
Nodes $i$ and $j$ are referred to as connected if there exists a path between those nodes, i.e., a sequence of distinct nodes, starting at $i$ and ending at $j$, such that each pair of consecutive nodes is adjacent.
If all pairs of nodes in graph $\mathcal{G}$ are connected, the graph is called connected.

Furthermore, the adjacency matrix $A$ consists of elements $a_{ij}=1$ if $i$ and $j$ are adjacent and $a_{ij}=0$ otherwise.
For an undirected graph $\mathcal{G}$, the adjacency matrix is symmetric.
In addition, the degree $d_i$ of a node $i$ denotes the number of neighbors of node $i$, i.e., the cardinality of the neighbor set $\lvert\mathcal{N}_i\rvert$.
The degree matrix $D$ of graph $\mathcal{G}$ is the diagonal matrix $D=\mathrm{diag}(d_1,\dots,d_N)$.
With the definitions of adjacency and degree matrix, the Laplace matrix $L$ of graph $\mathcal{G}$ is defined as $L=D-A$.
For an undirected graph $\mathcal{G}$, the Laplace matrix $L$ is symmetric and positive semi-definite.
Moreover, it is column and row stochastic and, thus, has the eigenvector of all ones corresponding to the eigenvalue $0$.
If $\mathcal{G}$ is also connected, the Laplace matrix $L$ has exactly one zero eigenvalue.
Note that we can compute the cardinality of the edge set as $\lvert\Eset\rvert = \trace{L} = \trace{D}$.
Due to the definition via directed edges, the cardinality of $\Eset$ is twice as large as the number of undirected edges in the graph $\mathcal{G}$.

Beyond graph theory, we utilize two notation alternatives regarding the series of transmission events in this paper:
On the one hand, we refer to the series of triggering time instants with the notation $(t^j_k)_{k\in\mathbb{N}}$ for agent $j\in\lbrace1,\dots,N\rbrace$ where $\mathbb{N}$ represents the set of positive integers.
On the other hand, we denote the event series of the complete MAS by $(t_k)_{k\in\mathbb{N}}$.
For some formulations in this work, let us additionally define $t_0=0$.
Naturally, ordering the event series $(t^j_k)_{k\in\mathbb{N}}$ for all agents $j\in\lbrace1,\dots,N\rbrace$ in an increasing fashion yields the sequence $(t_k)_{k\in\mathbb{N}}$.
If any elements in $(t^j_k)_{k\in\mathbb{N}}$ for all agents $j\in\lbrace1,\dots,N\rbrace$ should be equal, we subsume them in a single $t_k$ in the series $(t_k)_{k\in\mathbb{N}}$.

Finally, let us denote the expected value and the variance of a random variable by $\E\cdot$ and $\V\cdot$, respectively.
In addition, let $\delta(\cdot)$ refer to the Dirac delta impulse and $\mathds{1}_{(\cdot)}$ denote the indicator function.
Moreover, let $\mathbb{R}$ abbreviate the set of all real numbers and $\lim_{\epsilon\downarrow0}$ indicate the right-sided limit.

\section{Problem Formulation}\label{sec:probform}

In this section, we introduce the considered setup and derive the optimal control input for the formulated problem.

\subsection{Setup} %

We consider an MAS consisting of $N$ single-integrator agents that are perturbed by noise
\begin{equation}\label{eq:agent}
    \mathrm{d}x_i = u_i \mathrm{d}t + \mathrm{d}v_i,
\end{equation}
starting in consensus, i.e., initial states $x_i(0)=0$ for all $i\in \lbrace 1,\dots,N\rbrace$, and with $v_i(t)$ referring to a standard Brownian motion and $u_i(t)$ to the control input.
Let the agents be able to communicate according to an undirected connected communication graph with $N$ nodes representing the agents and Laplacian $L$.
Therefore, an agent $i$ is able to communicate with its neighbors $j\in\mathcal{N}_i$.

Furthermore, we presume that the agents can continuously monitor their own state and trigger discrete transmission events in order to share information with their neighbors.
The shared information is then used to preserve consensus between the agents as well as possible.
Thus, the control inputs $u_i(t)$ are required to be causal in the sense that they only depend on information transmitted up to time $t$.
As explained in the introduction, we aim at comparing TTC and ETC schemes for triggering transmissions.
For that purpose, let us consider the cost functional
\begin{equation}\label{eq:cost}
    J \coloneqq \limsup_{M\to \infty} \frac{1}{M} \int_{0}^{M} \E{x(t)^\top Lx(t)} \diff t
\end{equation}
as a performance measure where $x(t)=\lbrack x_1(t), \dots, x_N(t)\rbrack^\top$.
It quantifies the expected quadratic deviation from consensus and can also be written as
\begin{equation*}
    J = \limsup_{M\to \infty} \frac{1}{M} \int_{0}^{M} \E{\frac{1}{2}\sum_{(i,j)\in\Eset} \left(x_i(t)-x_j(t)\right)^2} \diff t.
\end{equation*}

\begin{rem}
    The quadratic term $x^\top Lx$ is a typical measure for the deviation of an MAS from consensus and, for example, also often used as a Lyapunov function, see, e.g., \cite{Dimarogonas2009}.
    From an optimal control viewpoint, we consider a quadratic state cost with positive semi-definite weight matrix and no input cost.
\end{rem}
\begin{rem}
    We do not incorporate a cost term on the triggering rate in \eqref{eq:cost} since we will compare TTC and ETC under equal average triggering rates, cf.\ Section~\ref{sec:ET} and, e.g., \cite{Antunes2020}.
    Any cost component related to the average triggering rate can therefore be neglected for the comparison.
\end{rem}

We are well aware that the considered setup is simple and does not cover the vast variety of practically relevant settings available in the literature on cooperative control.
However, the simplicity of the setup allows for its detailed analysis and understanding.
The motivation behind this work is to provide theoretical results for the performance comparison between TTC and ETC in such a simple cooperative setup and, thereby, uncover new phenomena and differences in the outcome when compared to existing results.
For that purpose, note additionally that we study the same setup as in \cite{Astrom2002} except for the fact that we consider a cooperative control goal in a distributed setting.
This will allow us to contrast the findings later on.

\subsection{Optimal Control Input}\label{sec:optinput}

Given the setup described in the previous section, we can now also characterize the optimal control input with respect to \eqref{eq:cost}.
As it turns out to be independent of the deployed triggering scheme, we consider it to be part of the problem formulation.

\begin{prop}\label{prop:optinput}
    Given the performance measure \eqref{eq:cost}, the agents \eqref{eq:agent} are optimally controlled with a causal impulsive control input $u(t)=\lbrack u_1(t),\dots,u_N(t) \rbrack^\top$ that ensures
    \begin{equation*}
        \lim_{\epsilon\downarrow0} x(t_k+\epsilon)^\top Lx(t_k+\epsilon) = 0 \quad \forall (t_k)_{k\in\mathbb{N}},
    \end{equation*}
    and is zero otherwise, i.e., $u(t)$ resets all agents instantaneously to consensus at each triggering time instant.
\end{prop}
\begin{proof}
    Can be found in the appendix.
\end{proof}

The resulting consensus point is irrelevant for our analysis.
The key point is that any impulsive control input that resets the agents to any consensus configuration instantaneously when an event is triggered is optimal under the considered performance measure \eqref{eq:cost}.
Between those triggering time instants, the agents behave according to standard Brownian motions.

Let us give a few examples of control schemes that belong to the class in Proposition~\ref{prop:optinput} to provide some intuition:
\begin{enumerate}
    \item \emph{All-to-all communication topology and one-to-all broadcast:}
          The agents are controlled by the impulsive control input
          \begin{equation}\label{eq:input_all2all}
              u_i(t) = \sum_{k\in\mathbb{N}} \sum_{j\in\mathcal{N}_i} \delta(t-t^j_{k})(x_j(t^j_{k})-x_i(t^j_{k})),
          \end{equation}
          where $\mathcal{N}_i = \lbrace1,\dots,N\rbrace\backslash\lbrace i\rbrace$ and $t^j_{k}$ denotes the transmission time instant of packet $k$ from agent $j$.
          Thus, the system is reset to consensus by transmitting an agent's state to all other agents.
          We can therefore also consider this as a leader-follower consensus problem, potentially with time-varying leader assignment.
    \item \emph{Multi-hop communication and network flooding:}
          The same scheme also works for arbitrary connected graphs if all agents pass on received messages to their respective neighbors.
          This is referred to as multi-hop communication and distributes transmitted information within the network beyond local neighbor clusters, cf.\ \cite{Jin2006} for a multi-hop protocol involving continuous communication.
          As long as the information is spread throughout the complete network of agents, we are able to apply \eqref{eq:input_all2all} analogously in this case.
          A related method is the flooding algorithm in networks which refers to nodes passing on received information until it is known to all network participants, see, e.g., \cite{Ferrari2011} and, for an advanced scheme, \cite{Herrmann2018}.

          Note that the proposed methods usually induce a significant communication delay.
          In this work, we consider an idealized setup without delays such that consensus can be achieved instantaneously.
          We leave the incorporation of communication delays in the analysis for future research.
    \item \emph{Reset to the origin:}
          The setup in which all agents are reset to the origin at each triggering instant also belongs to the defined control input class.
          In this special case, only the reset time instants need to be communicated to all agents in the network.
\end{enumerate}

In Fig.~\ref{fig:TT}, we provide an exemplary state evolution of an MAS with 3 agents under a TTC scheme.
The MAS is reset to consensus at two time instants.

\begin{figure}
    \centering
    \input{figs/plot_TT.tex}
    \setlength{\abovecaptionskip}{0pt}
    \caption{Exemplary MAS state evolution under TTC with constant inter-event time $\TTT=1$.}
    \label{fig:TT}
\end{figure}

The specified examples are not an exhaustive list of possible schemes that belong to the described control input class.
The following performance analysis holds for any scheme that fits into this class of control inputs.

\begin{rem}
    Note that the performance analysis even remains the same if we have an unconnected graph since the performance measure would then also only require cluster consensus for minimal cost.
\end{rem}

\section{Main Results}\label{sec:analysis}

In this section, we introduce the two triggering schemes and derive and compare the related cost according to \eqref{eq:cost}.

\subsection{Preliminaries}\label{sec:pre}
Let us first establish some facts on the considered problem which we can build upon in the following analysis.
Similar to \cite{Rabi2009}, we find
\begin{fact}\label{fact:0T}
    If the sequence of inter-event times is independent and identically distributed, it suffices to evaluate the cost over the first sampling interval
    \begin{equation*}
        J(T) = \frac{\E{\frac{1}{2}\sum_{(i,j)\in\Eset} \int_{0}^{T} \left(x_i(t)-x_j(t)\right)^2\diff t}}{\E{T}},
    \end{equation*}
    where $T = t_1$ is the inter-event time determined by the respective triggering scheme introduced in Sections~\ref{sec:TT} and \ref{sec:ET}.
\end{fact}
\begin{proof}
    Can be found in the appendix.
\end{proof}

Denoting $Q(T) \coloneqq \E{\frac{1}{2}\sum_{(i,j)\in\Eset} \int_{0}^{T} \left(x_i(t)-x_j(t)\right)^2\diff t}$, we can express the numerator of the cost as follows.
\begin{fact}\label{fact:Q}
    Let $T$ be a symmetric stopping time, i.e., if one replaces $v_i$ by $-v_i$ for any $i\in\lbrace1,\dots,N\rbrace$ the value of $T$ does not change, as well as independent of the direction, i.e., $T$ does not change if $v_i$ is interchanged with $v_j$ for any $i,j\in\lbrace1,\dots,N\rbrace$.
    Then, given Fact~\ref{fact:0T} being applicable, we can establish
    \begin{equation*}
        Q(T) = \lvert\Eset\rvert \cdot \E{\int_{0}^{T} v_1(t)^2 \diff t}.
    \end{equation*}
\end{fact}
\begin{proof}
    We start with the expression
    \begin{align*}
        Q(T) ={} \E{ \frac{1}{2}\sum_{(i,j)\in\Eset} \int_0^{T} (v_i(t)-v_j(t))^2 \diff t}
            ={} \E{ \frac{1}{2}\sum_{(i,j)\in\Eset} \int_0^{T} (v_i(t)^2 -2 v_i(t) v_j(t) +v_j(t)^2) \diff t}.
    \end{align*}
    By assumption, the stopping time $T$ is symmetric.
    Observe that the distribution of the random variable $\int_0^{T} v_i(t) v_j(t)\diff t$ is symmetric as well since replacing $v_i$ by $-v_i$ only changes the sign of the integrand.
    Therefore, the expectation of the mixed term is zero for any $i\neq j$.
    This shows
    \begin{align*}
        Q(T) ={} \E{\int_0^{T} \sum_{\substack{(i,j)\in\Eset: \\ i<j}} (v_i(t)^2+v_j(t)^2) \diff t}
            ={} \E{ \int_0^{T} \sum_{i=1}^{N}  d_i v_i(t)^2 \diff t}
            ={} \lvert\Eset\rvert \cdot \E{ \int_0^{T}   v_1(t)^2  \diff t},
    \end{align*}
    using that $T$ is independent of the direction.
\end{proof}

\subsection{Time-Triggered Control}\label{sec:TT}
As a comparison benchmark for ETC, we choose a TTC scheme in which the transmission events are scheduled periodically with a constant inter-event time $\TTT = t_{k+1} - t_k = \mathrm{const.}$ for all $k\in\mathbb{N}_0$.
This is in line with the consistency definition for ETC introduced and considered in \cite{Antunes2016,Antunes2018,Antunes2020,Balaghiinaloo2021,Balaghiinaloo2022}.
Given the first setup example from Section~\ref{sec:optinput}, an all-to-all communication topology and one-to-all broadcast, this implies that the transmission of one agent's state to all the others takes place with a fixed frequency.
This state information is then used by all other agents to reset their states to consensus.
How the transmitting agent is chosen in this particular setting plays no role for the performance analysis to come.
An exemplary MAS state evolution under TTC is shown in Fig.~\ref{fig:TT}.
Deploying this triggering scheme in the considered setup leads to the following theorem.
\begin{thm}\label{thm:cost_TT}
    Suppose agents \eqref{eq:agent} are controlled by the impulsive input from Proposition~\ref{prop:optinput} with constant inter-event times $\TTT$.
    Then, the cost \eqref{eq:cost} is given by
    \begin{equation*}
        J_{\mathrm{TT}}(\TTT) = \lvert\Eset\rvert \cdot \frac{\TTT}{2}.
    \end{equation*}
\end{thm}
\begin{proof}
    Since the inter-event times $\TTT$ are identical and constant, it suffices to analyze the interval between two transmissions and Facts~\ref{fact:0T} and \ref{fact:Q} hold.
    Thus, we can write \eqref{eq:cost} as $J_{\mathrm{TT}}(\TTT) = Q(\TTT)/\TTT$ with
    \begin{align*}
        Q(\TTT) = \lvert\Eset\rvert \cdot \int_{0}^{\TTT} \E{v_1(t)^2} \diff t
        = \lvert\Eset\rvert \cdot \int_{0}^{\TTT} t \diff t = \lvert\Eset\rvert \cdot \frac{\TTT^2}{2},
    \end{align*}
    as required.
\end{proof}

\begin{rem}
    The result for the cost in the TTC case is the same as in \cite{Astrom2002} but scaled by twice the number of connected agent pairs $\lvert\Eset\rvert$.
    This is also related to the results for non-cooperative NCS in \cite{Blind2011} and related papers where the cost scales with the number of network participants $N$.
\end{rem}

\subsection{Event-Triggered Control}\label{sec:ET}
In ETC, the necessity to communicate is captured by a continuously evaluated triggering condition.
Once the condition is fulfilled, a transmission event is initiated by the respective agent.
Since we are operating in a distributed setting, each agent evaluates its triggering condition locally.
Consequently, only local information is to be used in the respective triggering rule.
As the agents are for example incorporating local state information in the triggering decision, ETC is often argued to lower the communication rate while maintaining the same performance level as TTC, see, e.g., \cite{Seyboth2013}.

For this work, we use
\begin{equation}\label{eq:ET_cond}
    \lvert x_i(t)-x_i(t_{\hat{k}}) \rvert \geq \Delta
\end{equation}
as the triggering condition where $\hat{k} = \max \left\lbrace k\in\mathbb{N}_0 \mid t_{k} \leq t \right\rbrace$ and $\Delta>0$.
It compares the local state deviation from the state at the last event $x_i(t_{\hat{k}})$ to a threshold $\Delta$.
This form of triggering rule is quite common in distributed setups, see, for example, \cite{Dimarogonas2009}.
Note that we use a triggering condition that is analogous to the one in \cite{Astrom2002,Rabi2009,Blind2013}.
\begin{rem}
    We choose the same threshold $\Delta$ for all agents.
    Note that strictly speaking this is only the best choice if the contribution of each agent's state to the cost is equal, namely if all agents $i$ have the same degree $d_i$.
    For heterogeneous degrees $d_i$, a heterogeneous choice of $\Delta_i$ might be advantageous.
    Deriving the optimal choice for $\Delta_i$ in this case is beyond the scope of this paper and, thus, the analysis for heterogeneous $\Delta_i$ is not considered in the remainder of this paper.
\end{rem}

Considering the first setup example from Section~\ref{sec:optinput} with an all-to-all communication topology and one-to-all broadcast, the described ETC scheme leads to one agent broadcasting its state to the others once the respective local triggering condition is fulfilled.
As in the time-triggered case, this state information is then used by all other agents to reset their states to consensus.
The triggering agent is thus chosen as the transmitting agent resulting in a distributed scheme.
An exemplary MAS state evolution under the analyzed ETC scheme is shown in Fig.~\ref{fig:ET}.

\begin{figure}
    \centering
    \input{figs/plot_ET.tex}
    \setlength{\abovecaptionskip}{0pt}
    \caption{Exemplary MAS state evolution under ETC with $\Delta=2$.}
    \label{fig:ET}
\end{figure}


Utilizing Facts~\ref{fact:0T} and \ref{fact:Q} again allows us to analyze the cost on the first sampling interval, also in the ETC case.
In contrast to the TTC analysis, the length of this time interval is described by a probabilistic stopping time $\TET(\Delta) = \inf\lbrace t>0 \mid \exists i\in\lbrace1,\dots,N\rbrace: \lvert x_i(t)\rvert = \Delta \rbrace$.
While we are not able to derive an explicit expression for the cost $J_\mathrm{ET}(\Delta):=J(\TET(\Delta))$ for the ETC case, we can still arrive at results on its relationship to the TTC cost $J_\mathrm{TT}(\TTT)$ derived in Section~\ref{sec:TT}.
Note that the latter relationship is also what we are primarily interested in for this work.

In order to facilitate a fair comparison between $J_\mathrm{ET}(\Delta)$ and $J_\mathrm{TT}(\TTT)$, we require $\TTT=\E{\TET(\Delta)}$ which results in the same average triggering frequency for both schemes.
This is again inspired by the line of thought for the consistency property of ETC schemes considered in \cite{Antunes2016,Antunes2018,Antunes2020,Balaghiinaloo2021,Balaghiinaloo2022}.
Note that this constraint embodies the bridge between the triggering threshold $\Delta$ determining $\E{\TET(\Delta)}$ for the ETC scheme and the constant inter-event time $\TTT$ in the TTC case.

Let us first establish the following fact and lemma, which will enable us to focus on the case $\Delta=1$ for the derivations to come.
\begin{fact}\label{fact:scaling}
    We can show that the following scaling relationships hold true
    \begin{align*}
        Q_\mathrm{ET}(\Delta) = \Delta^4 Q_\mathrm{ET}(1), \qquad
        \E{\TET(\Delta)}      = \Delta^2 \E{\TET(1)}\!,    \qquad
        \V{\TET(\Delta)}      = \Delta^4 \V{\TET(1)}\!,
    \end{align*}
    where $Q_\mathrm{ET}(\Delta):=Q(\TET(\Delta))$.
\end{fact}
\begin{proof}
    Let us show the first equality.
    Indeed,
    \begingroup		%
    \allowdisplaybreaks
    \begin{align*}
        Q_\mathrm{ET}(\Delta)
        ={} & \lvert\Eset\rvert \cdot \E{\int_0^{\TET(\Delta)} v_1(s)^2 \diff s}
        \\
        ={} & \lvert\Eset\rvert \cdot \E{\int_0^{\inf\{ t>0 \mid \exists k : \lvert v_k(t)\rvert = \Delta\}} v_1(s)^2 \diff s}
        \\
        ={} & \lvert\Eset\rvert \cdot \E{\int_0^{\inf\{ t>0 \mid \exists k : \lvert v_k(\Delta^2 t / \Delta^2)\rvert = \Delta\}} v_1(\Delta^2 s/\Delta^2)^2 \diff s}
        \\
        ={} & \lvert\Eset\rvert \cdot \E{\int_0^{\inf\{ t>0 \mid \exists k : \Delta\lvert v_k( t / \Delta^2)\rvert = \Delta\}} \Delta^2 v_1(s/\Delta^2)^2 \diff s}
        \\
        ={} & \Delta^2 \lvert\Eset\rvert \cdot  \E{\int_0^{\Delta^{-2} \inf\{ \Delta^2 t' >0 \mid \exists k : \lvert v_k( t')\rvert = 1\}} v_1(s')^2 \Delta^2 \diff s'}
        \\
        ={} & \Delta^4 \lvert\Eset\rvert \cdot  \E{\int_0^{\inf\{ t' >0 \mid \exists k : \lvert v_k( t')\rvert = 1\}}  v_1(s')^2 \diff s'}
        \\
        ={} & \Delta^4 Q_\mathrm{ET}(1).
    \end{align*}
    \endgroup
    In the fourth step, we used the scaling property of Brownian motions and, in the fifth step, we applied linear integral substitution.
    All other formulas are proved similarly.
\end{proof}

Thus, we can derive relevant quantities for the considered setup for $\Delta=1$ and use Fact~\ref{fact:scaling} to generalize the found expressions to arbitrary choices of $\Delta$.
Moreover, we obtain the following lemma as a direct consequence of Fact~\ref{fact:scaling}.
\begin{lem}\label{lem:cost_scaling}
    Let $J_\mathrm{TT}(\E{\TET(\Delta)})$ denote the cost under constant inter-event times $\TTT=\E{\TET(\Delta)}$.
    Then, the cost comparison between $J_\mathrm{ET}(\Delta)$ and $J_\mathrm{TT}(\E{\TET(\Delta)})$ is not influenced by the choice of $\Delta$.
\end{lem}
\begin{proof}
    Due to Fact~\ref{fact:scaling} together with Fact~\ref{fact:0T} and Theorem~\ref{thm:cost_TT}, we have
    \begin{align*}
        J_\mathrm{ET}(\Delta)           & = \Delta^2 J_\mathrm{ET}(1),           \\
        J_\mathrm{TT}(\E{\TET(\Delta)}) & = \Delta^2 J_\mathrm{TT}(\E{\TET(1)}).
    \end{align*}
    Thus, we can neglect the scaling factor $\Delta^2$ when comparing the two costs or computing their ratio.
\end{proof}

In summary, Fact~\ref{fact:scaling} and Lemma~\ref{lem:cost_scaling} allow us to concentrate on the case $\Delta=1$ for the remainder of this section.
In addition, they enable us to focus on $\Delta=1$ in the simulation in Section~\ref{sec:sim}.
Before arriving at the main result of this section, we need to characterize the asymptotic order of the moments of $\TET(1)$.

\begin{lem}\label{lem:asymp_order}
    We have
    \begin{align} %
        \E{\TET(1)}   & \sim \frac{1}{2\ln N}, \label{eq:expTET}            \\
        \E{\TET(1)^2} & \sim \frac{1}{(2\ln N)^2},                          \\
        \V{\TET(1)}   & \sim \frac{\pi^2/24}{ (\ln N)^4}, \label{eq:varTET}
    \end{align}
    where $a_n \sim b_n$ means that $\lim_{n\to \infty}a_n/b_n = 1$ for arbitrary series $(a_n)_{n\in\mathbb{N}}$, $(b_n)_{n\in\mathbb{N}}$.
\end{lem}
\begin{proof}
    Throughout the proof, we drop the arguments indicating $\Delta=1$ to simplify notation.
    Let $T_j \coloneqq \inf\lbrace t>0 : \lvert x_j(t)\rvert = 1 \rbrace$ for all $j\in\lbrace1,\dots,N\rbrace$ and, thus, $\TET = \inf_{1\leq j\leq N} T_j$.
    Using the tail behavior derived from \cite{Moerters2010}, Theorem~7.45,
    \begin{align*}
        \Prob(T_j\le w)  = \Prob( \sup_{0\leq t\leq w} |v_j(t)| \geq 1 )
        = \Prob( \sup_{0\leq t\leq 1} |v_j(t)| \geq w^{-1/2} )
        \overset{w\to 0}{\sim} \frac{\kappa}{w^{-1/2}}\, \exp( -w^{-1}/2),     %
    \end{align*}
    for $\kappa=\sqrt{2/\pi}$, and the independence of the exit times $T_j$, one can derive the limit theorem
    \begin{equation} \label{eq:limittheorem}
        2(\ln N)^2 \left({\TET}- a_N\right) \Rightarrow G,  \qquad \textrm{as } N\to\infty,
    \end{equation}
    with
    \begin{equation*}
        a_N\coloneqq\frac{1}{2\ln N}-\frac{\ln \frac{\kappa}{(2\ln N)^{1/2}}}{2(\ln N)^2},
    \end{equation*}
    and where $\Rightarrow$ stands for convergence in distribution.
    Moreover, $G$ is a Gumbel-distributed random variable,
    \begin{equation*}
        \Prob( G\ge r) = \exp( - \exp(r)).
    \end{equation*}
    Equation \eqref{eq:limittheorem} can be derived from \cite{Galambos1978}, Theorem~2.1.6.
    A direct proof is given here: Indeed, for any $r\in\mathbb{R}$, we have
    \begingroup		%
    \allowdisplaybreaks
    \begin{align*}
        \Prob( 2(\ln N)^2 \left({\TET}- a_N\right)\geq r)
        ={}    & \Prob( {\TET} \geq \frac{r}{2(\ln N)^2} + a_N)
        ={}    \Prob( \forall j=1,\ldots, N : T_j \geq \frac{r}{2(\ln N)^2} + a_N)
        \\
        ={}    & \Prob( T_1 \geq \frac{r}{2(\ln N)^2} + a_N)^N
        ={}    \left( 1 - \Prob( T_1 < \frac{r}{2(\ln N)^2} + a_N )\right)^N
        \\
        \sim{} & \left(1 - \frac{\kappa}{c_N} \exp\!\left( -\frac{1}{2} \left( \frac{r -\ln \frac{\kappa}{c_N}}{2(\ln N)^2}+ \frac{1}{2\ln N} \right)^{-1} \right) \right)^N
        \\
        ={}    & \left(1-\frac{\kappa}{c_N} \exp\!\left( -\ln N \left( \frac{r -\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{\ln N}+ 1 \right)^{-1} \right) \right)^N
        \\
        \sim{} & \left(1-\frac{\kappa}{c_N} \exp\!\left( -\ln N \left(1- \frac{r -\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{\ln N} \right) \right) \right)^N
        \\
        ={}    & \left(1 - \frac{\kappa}{c_N} \frac{1}{N} \exp\!\left( r  -\ln \frac{\kappa}{c_N}\right) \right)^N
        ={}    \left(1 - \frac{1}{N} \exp( r ) \right)^N  \sim  e^{-e^r},
    \end{align*}
    \endgroup
    as required and with $c_N=(2\ln N)^{1/2}$.

    The limit theorem \eqref{eq:limittheorem} is accompanied by the convergence of the first and second moment.
    The proof for this is provided in the appendix to allow for a more concise presentation of the results.
    It builds upon Lebesgue's dominated convergence theorem where we need to show that $\Prob( 2(\ln N)^2 \left({\TET}- a_N\right)\geq r)$ and $2r\Prob( 2(\ln N)^2 \left({\TET}- a_N\right)\geq r)$ are upper bounded by integrable functions.

    Taking expectations in \eqref{eq:limittheorem} gives
    \begin{equation*}
        2 (\ln N)^2 ( \E{\TET} - a_N) \to \E{G}.
    \end{equation*}
    This shows
    \begin{align}
        \E{\TET} & = a_N + \frac{\E{G}}{2 (\ln N)^2} (1+o(1)) \label{eq:expTET_1}                      \\
                 & = \frac{1}{2\ln N} + \mathcal{O}\!\left( \frac{\ln \ln N}{(\ln N)^2}\right). \notag
    \end{align}
    Similarly, taking second moments in \eqref{eq:limittheorem} gives
    \begin{equation*}
        4 (\ln N)^4 \E{( \TET - a_N)^2} \to \E{G^2}.
    \end{equation*}
    This shows
    \begin{equation*}
        \E{\TET^2} - 2 a_N \E{\TET} + a_N^2 = \frac{\E{G^2}}{4(\ln N)^4} (1+o(1)),
    \end{equation*}
    which, together with \eqref{eq:expTET_1}, yields
    \begin{align*}
        \E{\TET^2} ={} a_N^2 + 2 a_N \frac{\E{G}}{2 (\ln N)^2} (1+o(1)) + \frac{\E{G^2}}{4(\ln N)^4} (1+o(1))
        ={} \frac{1}{4(\ln N)^2} + \mathcal{O}\!\left(\frac{1}{(\ln N)^3}\right).
    \end{align*}
    Finally, the limit theorem can be re-written as
    \begin{equation*}
        2 (\ln N)^2 ( \TET - \E{\TET}) + 2(\ln N)^2 ( \E{\TET} - a_N) \Rightarrow G.
    \end{equation*}
    Squaring, taking expectations, and dividing by $4(\ln N)^4$ gives
    \begin{equation*}
        \E{( \TET - \E{\TET})^2}  +  ( \E{\TET} - a_N)^2 = \frac{\E{G^2}}{4(\ln N)^4} (1+o(1)).
    \end{equation*}
    This implies
    \begin{align*}
        \V{\TET} = \frac{\E{G^2}}{4(\ln N)^4} (1+o(1)) -   ( \E{\TET} - a_N)^2
        = \frac{\E{G^2}}{4(\ln N)^4} (1+o(1)) -   \frac{\E{G}^2}{4 (\ln N)^4} (1+o(1))
        = \frac{\V{G}}{4 (\ln N)^4} (1+o(1)),
    \end{align*}
    which proves \eqref{eq:varTET} because $\V{G}=\pi^2/6$.
\end{proof}

With this result, we have also shown a logarithmic dependence of the moments of $\TET(\Delta)$ on the number of agents.
This is a crucial difference to the non-cooperative NCS case, e.g., studied in \cite{Astrom2002,Rabi2009,Blind2013}, and caused by the distributed nature of the considered problem.
Leveraging the previous lemma, we arrive at the following main theorem.

\begin{thm}\label{thm:cost_ET}
    Suppose agents \eqref{eq:agent} are controlled by the impulsive input from Proposition~\ref{prop:optinput} with inter-event times $\TET(\Delta) = \inf\lbrace t>0 \mid \exists i\in\lbrace1,\dots,N\rbrace: \lvert x_i(t)\rvert = \Delta \rbrace$.
    Then, there exists an $N_0$ such that for all $N\geq N_0$, we have
    \begin{equation*}
        J_\mathrm{ET}(\Delta) > J_\mathrm{TT}(\E{\TET(\Delta)}),
    \end{equation*}
    i.e., TTC outperforms ETC for all $N\geq N_0$ under equal average triggering rates.
\end{thm}
\begin{proof}
    Due to Lemma~\ref{lem:cost_scaling}, we can concentrate on the case $\Delta=1$.
    Thus, we again use simplified notation and do not state $\Delta=1$ explicitly in our formulas.
    As before, let $T_j \coloneqq \inf\lbrace t>0 : \lvert x_j(t)\rvert = 1 \rbrace$ for all $j\in\lbrace1,\dots,N\rbrace$ and, thus, $\TET = \inf_{1\leq j\leq N} T_j$.
    Moreover, let $\tau \coloneqq \inf_{2\le j\le N} T_j \ge {\TET}$.

    The key estimate is
    \begin{equation} \label{e10}
        \int_0^{\TET} v_1(t)^2 \diff t
        \ge   \int_0^\tau v_1(t)^2 \diff t \, (1 - \mathds{1}_{\tau\not ={\TET}}).
    \end{equation}
    Let us evaluate the expectations.
    By independence of $\tau$ and $v_1$, we have
    \begin{align}
        \E{\int_0^\tau v_1(t)^2 \diff t} =  \int_0^\infty \E{ \mathds{1}_{t\leq \tau} v_1(t)^2} \diff t
        = \int_0^\infty \E{ \mathds{1}_{t\leq \tau}} \E{ v_1(t)^2} \diff t
        = \E{\int_0^\tau t \diff t} = \frac{\E{\tau^2}}{2} > \frac{\E{\TET^2}}{2}, \label{e11}
    \end{align}
    while since $\tau\leq T_2$ and using the Cauchy-Schwarz inequality
    \begingroup		%
    \allowdisplaybreaks
    \begin{align}
        \E{ \int_0^\tau v_1(t)^2 \diff t \cdot \mathds{1}_{\tau\not ={\TET}} }
        \le{} & \E{ \int_0^{T_2} v_1(t)^2 \diff t \cdot \mathds{1}_{\tau\not ={\TET}} }
        \le{} \E{ \left(\int_0^{T_2} v_1(t)^2 \diff t \right)^2 }^{1/2} \cdot \E{ \mathds{1}^2_{\tau\not ={\TET}} }^{1/2} \notag \\
        ={}   & C \cdot \Prob (\tau\not ={\TET})^{1/2} = C \, N^{-1/2}, \label{e12}
    \end{align}
    \endgroup
    where $C=\mathbb{E}[ (\int_0^{T_2} v_1(t)^2 \diff t)^2 ]^{1/2}$ does not depend on the number of agents $N$.
    The last step holds because $\tau\neq\TET$ if and only if the process $v_1$ is the first to exit $[-1,1]$.
    By symmetry, this has probability equal to $1/N$.
    Putting \eqref{e10}, \eqref{e11}, and \eqref{e12} together, one obtains
    \begin{align}
        \E{ \int_0^{\TET} v_1(t)^2 \diff t }  > \frac{\E{\TET^2}}{2} -  C \, N^{-1/2}
        = \frac{\E{\TET}^2}{2} + \frac{\V{\TET}}{2}  -  C \, N^{-1/2}. \label{e13}
    \end{align}
    Next, the definition of the limit shows that \eqref{eq:varTET} implies
    \begin{equation*}
        \frac{\V{\TET}}{(\pi^2/24)/(\ln N)^4} > \frac{1}{2}
    \end{equation*}
    for all $N\geq N_1$.
    Furthermore, let $N_2$ be such that $\frac{1}{4}\cdot \frac{\pi^2/24}{(\ln N)^{4}} -  C \, N^{-1/2} > 0$ for all $N\geq N_2$ and set $N_0\coloneqq\max(N_1,N_2)$.
    Plugging the inequalities into \eqref{e13}, we see that for $N\geq N_0$
    \begin{align*}
        \frac{1}{\lvert\Eset\rvert}\, Q_\mathrm{ET} = \E{ \int_0^{\TET} v_1(t)^2 \diff t }
        >	\frac{\E{{\TET}}^2}{2} + \frac{1}{4}\cdot \frac{\pi^2/24}{(\ln N)^{4}} -  C \, N^{-1/2}
        > \frac{\E{{\TET}}^2}{2}
        =  \frac{1}{\lvert\Eset\rvert}\, Q(\TTT=\E{\TET}),
    \end{align*}
    where we also used Fact~\ref{fact:Q} in the first step and Theorem~\ref{thm:cost_TT} in the last step.
    Multiplying both sides with $\lvert\Eset\rvert/\E{\TET}$ gives the desired inequality.
\end{proof}

Thus, we have proved that ETC is not necessarily outperforming TTC if we consider a distributed setup with cooperative agents.
The common result supported by works like \cite{Astrom2002}, that ETC schemes outperform TTC, can therefore not be simply transferred to this distributed setting.
We have also shown that this finding holds for any undirected connected communication topology in the considered setting.
Note that the independence from the topology is rooted in the infinitely fast control capabilities by impulsive control inputs and instantaneous communication.
The communication topology might indeed play a role under more realistic assumptions like bounded inputs or non-zero communication delays.

Building upon the results so far, we are also able to characterize the asymptotic order of the performance measure in the following corollary.

\begin{cor}
    The asymptotic order of the cost \eqref{eq:cost} as a function of the number of agents under both triggering schemes can be expressed as
    \begin{equation*}
        J_\mathrm{ET}(1) \sim J_\mathrm{TT}(\E{\TET(1)}) \sim \frac{\lvert\Eset\rvert}{4\ln N}.
    \end{equation*}
\end{cor}
\begin{proof}
    Let us again omit the arguments indicating $\Delta=1$.
    Utilizing Theorem~\ref{thm:cost_TT} and plugging in \eqref{eq:expTET} shows the relationship for $J_\mathrm{TT}(\E{\TET})$.

    The lower bound for $J_\mathrm{ET}$ follows from Theorem~\ref{thm:cost_ET}.
    For the upper bound, observe that
    \begin{align*}
        \E{\int_0^{\TET} v_1(t)^2 \diff t} \leq \E{\int_0^{\tau} v_1(t)^2 \diff t} = \frac{\E{\tau^2}}{2} \sim \frac{1}{2(2 \ln (N-1))^2} \sim \frac{1}{2(2 \ln N)^2},
    \end{align*}
    where we used the notation from the last proof and the fact that $\tau$ has the same distribution as $\TET$ for the dimension $N-1$.
    Utilizing $J_\mathrm{ET} = \lvert\Eset\rvert \cdot \mathbb{E}[\int_0^{\TET} v_1(t)^2 \diff t]/\mathbb{E}[\TET]$ yields the desired result.
\end{proof}

Thus, on the one hand, the cost for ETC and TTC grows with the same order for large numbers of agents.
On the other hand, this does not imply that the difference between $J_\mathrm{ET}(\Delta)$ and $J_\mathrm{TT}(\E{\TET(\Delta)})$ vanishes for large $N$.

Given the results in this section, especially Theorem~\ref{thm:cost_ET}, we can thus conclude that, in the considered distributed setting, the ETC scheme is inferior to the TTC scheme for large numbers of agents and equal average triggering rates.
This result is in contrast with the findings of similar analyses for non-cooperative setups under the assumption of delay- and loss-free communication such as \cite{Astrom2002}.
Moreover, the relationship between TTC and ETC performance might well depend on the number of agents or network participants $N$ when considering cooperative settings.
While it remains unclear how these results generalize to other distributed problems, they still point out that performance advantages of ETC can behave differently in some distributed settings when compared to their non-cooperative counterpart.
This work might therefore serve as a starting point for a careful evaluation of ETC performance for a wider range of distributed settings.

\section{Simulation}\label{sec:sim}

In this section, we perform simulations to support our theoretical findings.
A simulative performance comparison involving ETC schemes is in general challenging since closed-form expressions for the expected inter-event time are rarely obtainable.
Therefore, the constraint of equal expected inter-event times for different triggering schemes is hard to enforce exactly in simulation.
In our case though, we can resolve this problem by estimating the expected inter-event time of the ETC scheme from simulation results and then using this estimate as inter-event time in the closed-form expression for the cost of the TTC scheme.
Consequently, we are able to enforce the constraint $\TTT = \E{\TET(\Delta)}$ exactly in our setup when comparing costs of the triggering schemes.

Thus, we simulate the described MAS including the impulsive control law under the ETC scheme.
With the aforementioned strategy, this allows us to estimate the cost ratio $J_\mathrm{ET}(\Delta)/J_\mathrm{TT}(\E{\TET(\Delta)})$ for a varying number of agents $N$ and thereby relate the two schemes in terms of performance.
To be precise, we can use the simulation estimate of $\E{\TET(\Delta)}$ for a given $N$ with the result from Theorem~\ref{thm:cost_TT} to compute $J_\mathrm{TT}(\E{\TET(\Delta)})$ exactly.
Therefore, only the cost $J_\mathrm{ET}(\Delta)$ needs to be estimated based on simulation results.
We refer the reader to the appendix for additional information on how to achieve this.

For the simulation, we set $\Delta=1$.
We can do so without loss of generality due to Lemma~\ref{lem:cost_scaling}.
Moreover, we simulate the MAS with the Euler-Maruyama method for $N\in\lbrace2,3,\dots,9,10,12,15,20,25,\dots,80\rbrace$ with a step size of $10^{-4}\,\mathrm{s}$.
For each $N$, we perform $10\,000$ Monte Carlo runs for estimating $\E{\TET(1)}$ and $250\,000$ Monte Carlo runs for estimating $Q_\mathrm{ET}(1)$.
As an analysis of the first sampling interval suffices for the cost, we can terminate a Monte Carlo run when the first event occurs.
The resulting cost ratios with a minimum $95\%$-confidence interval are shown in Fig.~\ref{fig:cost_ratio}.
The derivations regarding the confidence interval can also be found in the appendix.

\begin{figure}
    \centering
    \input{figs/plot_cost_ratio.tex}
    \setlength{\abovecaptionskip}{0pt}
    \caption{Cost ratio of ETC over TTC.}
    \label{fig:cost_ratio}
\end{figure}

As predicted by our theoretical results, we find that the ETC scheme is outperformed by TTC for larger numbers of agents.
For low numbers of agents $N$, we observe a clear performance advantage of the ETC scheme in simulation.
Note that this advantage is not guaranteed by the theoretical findings in this paper.
In addition, the simulation results indicate that the critical number of agents $N_0$ from Theorem~\ref{thm:cost_ET} are likely between 30 and 55 agents, also supported by the depicted confidence intervals.
Thus, for this particular setting, the critical number of agents $N_0$ beyond which the TTC scheme outperforms the ETC scheme might well lie in a practically relevant range.

\section{Conclusion}\label{sec:conclusion}

In this work, we examined TTC and ETC performance in an MAS consensus setup with single-integrator agents, optimal control inputs and arbitrary undirected connected communication topologies.
For this particular setting, we provided a complete proof that TTC outperforms ETC beyond a certain number of agents given any communication topology in the considered class.
This is in striking contrast to the outcome of similar analyses for non-cooperative NCS.
In addition, we characterized the asymptotic order of the performance measure in the number of agents and evaluated our results in a numerical simulation.

This work points out that consistency considerations for ETC of MAS can be influenced by additional factors when compared to the non-cooperative NCS case.
In particular, performance advantages of ETC over TTC might provably vanish in a distributed setting if the number of agents is large enough.
The transfer of experience from the non-cooperative NCS to the MAS field and the creation of new event-triggering schemes should therefore be accompanied by a careful consideration of the impact of the number of agents on the performance relationship between TTC and ETC.

In future work, we plan to investigate the root of the discovered performance disadvantage of ETC compared to TTC for sufficiently large agent numbers more closely.
Only this will allow examining and arguing about options to overcome or alleviate the found phenomenon.
Moreover, we aim to incorporate network effects such as communication delays and packet loss in the analysis.
Previous work in the non-cooperative NCS field has shown the relevance of such considerations.

\appendix

\section{Proof of Proposition~\ref{prop:optinput}}

First, note that the considered performance measure \eqref{eq:cost} is minimized if $\E{x(t)^\top Lx(t)}$ is minimized for all $t\geq0$.
Second, the control input $u(t)$ can only utilize state information up to the last triggering time instant, i.e., up to time $t_{\hat{k}}$ with $\hat{k}=\max\{k\in\mathbb{N}_0\mid t_k\leq t\}$.
Consequently, we can compute as follows
\begingroup		%
\allowdisplaybreaks
\begin{align}
    \E{x(t)^\top Lx(t)}
    ={} & \E{\left( v(t) + \int_0^t u(s) \diff s \right)^\top L (\ast)} \notag                    \\
    ={} & \E{(v(t)-v(t_{\hat{k}}))^\top L (\ast)}
    + \sum_{k\in\mathbb{N}} H(t-t_k) \E{(v(t_k)-v(t_{k-1}))^\top L (\ast)} \notag                 \\
        & + 2\,\E{(v(t)-v(t_{\hat{k}}))^\top L \int_0^t u(s) \diff s}
    + 2\sum_{k\in\mathbb{N}} H(t-t_k) \E{(v(t_k)-v(t_{k-1}))^\top L \int_0^t u(s) \diff s} \notag \\
        & + \E{\left(\int_0^t u(s) \diff s\right)^\top L (\ast)} \notag                           \\
    ={} & \E{(v(t)-v(t_{\hat{k}}))^\top L (\ast)}
    + \E{\left(v(t_{\hat{k}}) + \int_0^t u(s) \diff s\right)^\top L (\ast)}, \label{eq:prop1_1}
\end{align}
\endgroup
where $(\ast)$ abbreviates the respective counterpart in the quadratic form, $v(t)=\lbrack v_1(t),\dots,v_N(t) \rbrack$, $H(\cdot)$ denotes the Heaviside step function, and $v(t_{\hat{k}})=\sum_{k\in\mathbb{N}} H(t-t_k) (v(t_k)-v(t_{k-1}))$.
Moreover, we utilized the integrated version of the agent dynamics \eqref{eq:agent}, the fact that the increments of a standard Brownian motion are independent, and that $u(s)$ must be independent of $v(\tilde{s})$ for $s,\tilde{s}\in(t_{\hat{k}},t\rbrack$.
The latter yields $\E{(v(t)-v(t_{\hat{k}}))^\top L \int_0^t u(s) \diff s} = 0$.

As both terms in \eqref{eq:prop1_1} are non-negative, we arrive at the following optimality condition
\begin{align}
    \E{\left(v(t_{\hat{k}}) + \int_0^t u(s) \diff s\right)^\top L (\ast)}
    ={} \E{\left(x(t_{\hat{k}}) + \int_{t_{\hat{k}}}^t u(s) \diff s\right)^\top L (\ast)}	\overset{!}{=} 0 \quad \forall t\geq0. \label{eq:prop1_optcond}
\end{align}
Evaluating the condition at $(t_k)_{k\in\mathbb{N}}$ yields $\E{x(t_k)^\top L x(t_k)} = 0$ which can be satisfied by ensuring
\begin{equation}
    x(t_k)^\top L x(t_k)=0 \quad \forall (t_k)_{k\in\mathbb{N}} \label{eq:prop1_optcond_ontrig}
\end{equation}
through the control inputs $u(t_k)$.

As explained in Section~\ref{sec:prelim}, the Laplace matrix $L$ is positive semi-definite in the considered setup.
Thus, we can decompose it according to $L = \tilde{L}^\top \tilde{L}$ which allows us to satisfy \eqref{eq:prop1_optcond} by requiring $\tilde{L}\left(x(t_{\hat{k}}) + \int_{t_{\hat{k}}}^t u(s) \diff s\right) = 0$ for all $t\geq0$ or, equivalently,
\begin{equation*}
    L \left(x(t_{\hat{k}}) + \int_{t_{\hat{k}}}^t u(s) \diff s\right) = 0 \quad \forall t\geq0.
\end{equation*}
Evaluating at $(t_k)_{k\in\mathbb{N}}$ gives us $L x(t_{\hat{k}}) = 0$.
Thus, for any $t\in\{t>0\mid t\notin(t_k)_{k\in\mathbb{N}}\}$, we have $L \int_{t_{\hat{k}}}^t u(s) \diff s = 0$, and, consequently,
\begin{equation}
    Lu(t) = 0 \quad \forall t\in\{t>0\mid t\notin(t_k)_{k\in\mathbb{N}}\}. \label{eq:prop1_optcond_betwtrig}
\end{equation}
While we have now characterized a class of optimal control inputs with conditions \eqref{eq:prop1_optcond_ontrig} and \eqref{eq:prop1_optcond_betwtrig}, we decide to fulfill the latter by choosing $u(t) = 0$ for all $t\in\{t>0\mid t\notin(t_k)_{k\in\mathbb{N}}\}$.
This appears as the practically most relevant case and reduces the complexity in Section~\ref{sec:optinput}.
Given this special case, we arrive at the result stated in Proposition~\ref{prop:optinput}.
Examples for such control inputs are provided in Section~\ref{sec:optinput}.
Note that all results shown in this paper generally hold for the class of optimal control inputs characterized by \eqref{eq:prop1_optcond_ontrig} and \eqref{eq:prop1_optcond_betwtrig}.


\section{Proof of Fact~\ref{fact:0T}}

First, we compute as follows
\begingroup		%
\allowdisplaybreaks
\begin{align}
    \E{\int_{0}^{M} \frac{1}{2}\sum_{(i,j)\in\Eset} \left(x_i(t)-x_j(t)\right)^2\diff t}
    ={} & \frac{1}{2}\sum_{(i,j)\in\Eset} \E{\int_{0}^{M} \left(x_i(t)-x_j(t)\right)^2\diff t} \notag                                                                                                                  \\
    ={} & \frac{1}{2}\sum_{(i,j)\in\Eset} \left(\E{\sum_{k=1}^{m(M)}\int_{t_{k-1}}^{t_{k}}\left(x_i(t)-x_j(t)\right)^2\diff t} + \E{\int_{t_{m(M)}}^{M}\left(x_i(t)-x_j(t)\right)^2\diff t}\right), \label{eq:fact1_1}
\end{align}
\endgroup
where $(m(M))_{M\in\lbrack0,\infty)}$ is a renewal process for the renewal time series $(t_k)_{k\in\mathbb{N}}$.
As the sequence of inter-event times is independent and identically distributed, the quantities
\begin{equation*}
    y_k^{(i,j)} \coloneqq \int_{t_{k-1}}^{t_{k}}\left(x_i(t)-x_j(t)\right)^2\diff t
\end{equation*}
are independent and identically distributed as well.
To see this, let $\bar{v}_i(t) = v_i(t) - v_i(t_k)$ for all $t\in\lbrack t_k,t_{k+1})$ and $i\in\lbrace1,\dots,N\rbrace$.
Note that $x_i(t) = x_i(t_k) + \bar{v}_i(t)$ for all $t\in\lbrack t_k,t_{k+1})$, $i\in\lbrace1,\dots,N\rbrace$ and $x_i(t_k)=x_j(t_k)$ for all $i,j\in\lbrace1,\dots,N\rbrace$.

By Wald's equation, we have $\mathbb{E}[\sum_{k=1}^{m(M)} y_k^{(i,j)}] = \E{m(M)} \mathbb{E}[y_1^{(i,j)}]$.
In addition, the second term in \eqref{eq:fact1_1} has the upper bound
\begin{equation*}
    \int_{t_{m(M)}}^{M}\left(x_i(t)-x_j(t)\right)^2\diff t \leq y_{m(M)+1}^{(i,j)}.
\end{equation*}
Dividing by $M$ and letting $M\to \infty$ yields
\begin{align*}
    J = \frac{1}{2}\sum_{(i,j)\in\Eset} \lim_{M\to \infty} \frac{\E{m(M)}}{M}\cdot\E{y_1^{(i,j)}}
    = \frac{1}{\E{T}} \cdot \frac{1}{2}\sum_{(i,j)\in\Eset} \E{\int_{0}^{T} \left(x_i(t)-x_j(t)\right)^2\diff t},
\end{align*}
since, by the renewal theorem, $\lim_{M\to \infty} \frac{\E{m(M)}}{M} = \frac{1}{\E{T}}$.

\section{Convergence of moments - Proof of Lemma~\ref{lem:asymp_order}}

We still have to show the following lemma to complete the proof of Lemma~\ref{lem:asymp_order}.

\begin{lem} \label{lem:mom_conv}
    The limit theorem \eqref{eq:limittheorem} also implies the convergence of the first and second moment, namely
    \begin{equation*}
        \E{X_N} \to \E{G}\quad\text{and}\quad \E{X_N^2} \to \E{G^2},
    \end{equation*}
    where
    \begin{equation*}
        X_N\coloneqq2(\ln N)^2 \left({\TET} - a_N\right)
    \end{equation*}
    and $G$ is a Gumbel-distributed random variable.
    Furthermore, $\kappa$ and $a_N$ are defined as in \eqref{eq:limittheorem}.
\end{lem}

In the language of Lemma~\ref{lem:mom_conv}, the limit theorem \eqref{eq:limittheorem} says that $X_N$ converges weakly to $G$.

\begin{proof}
    The proof builds upon Lebesgue's dominated convergence theorem where we need to show that $\Prob(X_N\geq r)$ and $2r\Prob(X_N\geq r)$ are upper bounded by integrable functions.
    We will show the existence of integrable majorants for $r\in\mathbb{R}$ by showing their existence on different ranges in $r$ whose union covers $\mathbb{R}$.
    Thereby, we arrive at a majorant for $r\in\mathbb{R}$ as the sum of the majorants for the subregions in $r$.

    \subsubsection*{Preliminaries}

    As in the beginning of the proof for Lemma~\ref{lem:asymp_order}, we have
    \begin{align*}
        \Prob(T_j<w) = \Prob( \sup_{0\leq t\leq w} |v_j(t)| \geq 1 ) = \Prob( \sup_{0\leq t\leq 1} |v_j(t)| \geq w^{-1/2} ).
    \end{align*}
    Using
    \begin{equation*}
        \Prob( \sup_{0\leq t\leq 1} v_j(t) \geq w^{-1/2} ) \leq
        \Prob( \sup_{0\leq t\leq 1} |v_j(t)| \geq w^{-1/2} ) \leq
        2 \Prob( \sup_{0\leq t\leq 1} v_j(t) \geq w^{-1/2} ),
    \end{equation*}
    as well as the fact that $\sup_{0\leq t\leq 1} v_j(t)$ has the same distribution as $|v_j(1)|$, and the standard Gaussian tail estimate, we see that there exist $0<\kappa_1< \kappa_2 < \infty$ and $w_0>0$ such that, for all $0<w<w_0$, we have
    \begin{equation} \label{eqn:tbd1}
        \frac{\kappa_1}{w^{-1/2}}\, e^{-w^{-1}/2} \leq \Prob(T_j<w) \leq \frac{\kappa_2}{w^{-1/2}}\, e^{-w^{-1}/2}.
    \end{equation}
    Furthermore, we will use the inequalities
    \begin{equation} \label{eqn:tbd2}
        1 - x \leq (1+x)^{-1} \leq  1 - x + x^2,\qquad x\geq 0,
    \end{equation}
    and
    \begin{equation} \label{eqn:tbd3}
        \left(1-\frac{\alpha}{N}\right)^N \leq e^{-\alpha}, \qquad \alpha\in\mathbb{R}.
    \end{equation}

    \subsubsection*{The range $0\leq r\leq\frac{1}{2}\ln N$}

    For $r>0$, we have for large $N$ (uniformly in $r$)
    \begin{align}
        w \coloneqq{} \frac{r}{2(\ln N)^2} + a_N
        ={} \frac{r -\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{2(\ln N)^ 2}+ \frac{1}{2\ln  N}  \geq \frac{1}{2\ln  N}, \label{eqn:tbd5}
    \end{align}
    i.e., $\quad w^{-1/2} \leq (2\ln N)^{1/2}$.
    Furthermore, since $r\leq \frac{1}{2} \ln N$, we have for large $N$ (uniformly in $r$)
    \begin{align}
        (r -\ln  \frac{\kappa}{(2\ln N)^{1/2}})^2 \leq (r+\ln \ln N)^2
        = r^2 +  2 r \ln \ln N + (\ln \ln N)^2
        \leq  \frac{3}{4} r \ln N + (\ln \ln N)^2. \label{eqn:tbd6}
    \end{align}
    This shows that
    \begingroup		%
    \allowdisplaybreaks
    \begin{align}
        \Prob( X_N\geq r)
        ={} & \Prob( T_1 \geq \frac{r}{2(\ln N)^2} + a_N)^N
        ={} \Prob( T_1 \geq w)^N \label{eqn:tbdcont1}                                                                                                                            \\
        ={} & \left( 1 - \Prob( T_1 < w)\right)^N
        \notag                                                                                                                                                                   \\
        \overset{\eqref{eqn:tbd1}}{\leq}
            & \left(1 - \frac{\kappa_1}{w^{-1/2}}\, \exp\!\left( -\frac{w^{-1}}{2} \right) \right)^N
        \notag                                                                                                                                                                   \\
        \overset{\eqref{eqn:tbd5}}{\leq}
            & \left(1 - \frac{\kappa_1}{c_N}\, \exp\!\left( -\frac{1}{2} \left( \frac{r -\ln \frac{\kappa}{c_N}}{2(\ln N)^ 2} + \frac{1}{2\ln  N} \right)^{-1} \right) \right)^N
        ={} \left(1 - \frac{\kappa_1}{c_N}\, \exp\!\left( -(\ln N) \left( \frac{r -\ln \frac{\kappa}{c_N}}{\ln N} + 1 \right)^{-1} \right) \right)^N
        \notag                                                                                                                                                                   \\
        \overset{\eqref{eqn:tbd2}}{\leq}
            & \left(1 - \frac{\kappa_1}{c_N}\, \exp\!\left( -(\ln N) (1- b_N + b_N^2) \right) \right)^N
        ={} \left(1 - \frac{\kappa_1}{c_N}\, \frac{1}{N} \exp\!\left( r  -\ln  \frac{\kappa}{c_N}-  \frac{(r -\ln  \frac{\kappa}{c_N})^2}{\ln N} \right) \right)^N
        \notag                                                                                                                                                                   \\
        ={} & \left(1 - \frac{\kappa_1/\kappa}{N} \exp\!\left( r -  \frac{(r -\ln  \frac{\kappa}{c_N})^2}{\ln N}\right) \right)^N
        \notag                                                                                                                                                                   \\
        \overset{\eqref{eqn:tbd3}}{\leq}
            & \exp\!\left( - \frac{\kappa_1}{\kappa}\,\exp\!\left( r -  \frac{(r -\ln  \frac{\kappa}{c_N})^2}{\ln N} \right) \right)
        \notag                                                                                                                                                                   \\
        \overset{\eqref{eqn:tbd6}}{\leq}
            & \exp\!\left( - \frac{\kappa_1}{\kappa}\,\exp\!\left(\frac{r}{4} -  1\right) \right), \label{eqn:sum1}
    \end{align}
    \endgroup
    where $b_N = \frac{r -\ln(\kappa/c_N)}{\ln N}$ and $c_N = (2\ln N)^{1/2}$.
    In addition, we utilized $r\in\lbrack 0, \frac{1}{2} \ln N\rbrack$ for applying \cref{eqn:tbd1,eqn:tbd5,eqn:tbd2,eqn:tbd6}.

    \subsubsection*{The range $r>(\ln N)^2$}

    On this range, we have that for $N$ large enough (uniformly in $r$)
    \begin{equation} \label{eqn:tbd7}
        w = \frac{r -\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{2(\ln N)^2}+ \frac{1}{2\ln  N}  \geq  \frac{r}{2(\ln N)^2},
    \end{equation}
    and $r>(\ln N)^2$ implies $w\geq 1/2$.
    Here, we can use the standard small deviation estimate as in \cite[(1.3)]{Li2001}:
    For a constant $c>0$ and all $w\geq 1/2$, we have
    \begin{equation}  \label{eqn:tbd8}
        \Prob( T_1 \geq w ) = \Prob( \sup_{0\leq t\leq w} |v_1(t)| \leq 1 ) \leq e^{-cw}.
    \end{equation}
    Therefore, continuing in \eqref{eqn:tbdcont1}, we obtain
    \begin{align}
        \Prob( X_N\geq r) = \Prob( T_1 \geq  w )^N
        \overset{\eqref{eqn:tbd8}}{\leq} e^{-cwN}
        \overset{\eqref{eqn:tbd7}}{\leq} \exp\left(-c\frac{rN}{2(\ln N)^2} \right) \leq e^{-r}, \label{eqn:sum2}
    \end{align}
    where we utilized $r>(\ln N)^2$ to apply \eqref{eqn:tbd8}.

    \subsubsection*{The range  $\frac{1}{2}\ln N \leq r\leq (\ln N)^2$}

    Here, for $N$ large enough (uniformly in $r$), we have
    \begin{equation} \label{eqn:tbd10}
        w = \frac{r -\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{2(\ln N)^ 2}+ \frac{1}{2\ln  N}  \geq  \frac{3}{4\ln N}.
    \end{equation}
    Therefore, we obtain
    \begingroup		%
    \allowdisplaybreaks
    \begin{align}
        \Prob( X_N\geq r)           = \Prob(T_1 \geq w)^N
        \overset{\eqref{eqn:tbd10}} & {\leq} \Prob\left( T_1 \geq \frac{3}{4\ln N} \right)^N
        = \left(1-\Prob\left(T_1 <  \frac{3}{4\ln N}\right) \right)^N
        \notag                                                                                                                   \\
        \overset{\eqref{eqn:tbd1}}  & {\leq} \left(1- \frac{\kappa_1}{(\frac{4}{3}\ln N)^{1/2}} e^{-\frac{2}{3} \ln N} \right)^N
        = \left(1- \frac{\kappa_1}{N^{2/3} (\frac{4}{3}\ln N)^{1/2}} \right)^N
        = \left(1- \frac{\kappa_1 N^{1/3}}{N  (\frac{4}{3}\ln N)^{1/2}} \right)^N
        \notag                                                                                                                   \\
        \overset{\eqref{eqn:tbd3}}  & {\leq} \exp\!\left( - \frac{\kappa_1 N^{1/3}}{(\frac{4}{3}\ln N)^{1/2}}\right)
        \leq \exp\!\left( - N^{1/6}\right)
        \leq \exp\!\left( - e^{\sqrt{r}/6}\right),\label{eqn:sum3}
    \end{align}
    \endgroup
    where we utilized $\frac{1}{2}\ln N\leq r$ to apply \eqref{eqn:tbd10} and $r\leq (\ln N)^2$ for the last step.

    Putting the three estimates \cref{eqn:sum1,eqn:sum2,eqn:sum3} together shows that we can find an integrable majorant for $r\mapsto \Prob( X_N\geq r)$. Therefore, using the limit theorem \eqref{eq:limittheorem}
    \begin{equation} \label{eqn:new+}
        \E{X_N \mathds{1}_{X_N\geq 0}} = \int_0^\infty \Prob( X_N\geq r) \diff r
        \to \int_0^\infty \Prob( G\geq r) \diff r = \E{G \mathds{1}_{G>0}}.
    \end{equation}

    Similarly, the three estimates above show that an integrable majorant for $r\mapsto 2 r\Prob( X_N\geq r)$ can be found giving
    \begin{equation}\label{eqn:new++}
        \E{X_N^2 \mathds{1}_{X_N\geq 0}} = \int_0^\infty 2r \Prob( X_N\geq r) \diff r
        \to \int_0^\infty 2r \Prob( G\geq r) \diff r = \E{G^2 \mathds{1}_{G>0}}.
    \end{equation}

    \subsubsection*{The range $r<0$}

    We finally handle $\E{X_N \mathds{1}_{X_N<0}}$.
    Note that
    \begin{align*}
        - \E{X_N \mathds{1}_{X_N<0}} = \int_0^\infty \Prob( - X_N > r ) \diff r
        = \int_{-\infty}^0 \Prob( X_N< r ) \diff r,
    \end{align*}
    and since $X_N\geq -\ln N + \ln(\kappa/(2\ln N)^{1/2}) \eqqcolon r_{\min}$, the integral is actually on the range $[r_{\min},0]$.
    This time, we will find an integrable majorant for $r \mapsto \Prob( X_N< r )$ on this range.

    Recall from \eqref{eqn:tbdcont1} that
    \begin{align*}
        \Prob( X_N< r ) = 1 - \Prob( X_N\geq  r )
        = 1 - \Prob( T_1 \geq w)^N
        = 1 - (1-\Prob(T_1<w))^N,
    \end{align*}
    where, as above,	%
    \begin{align}
        w = w_r = \frac{r -\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{2(\ln N)^2}+ \frac{1}{2\ln  N}
        \in \left[0,\frac{1}{2\ln N}- \frac{\ln  \frac{\kappa}{(2\ln N)^{1/2}}}{2(\ln N)^2}\right]. \label{eqn:tbdnew2}
    \end{align}
    Thus, it suffices to find an integrable majorant for $r\mapsto 1 - (1-\Prob(T_1<w_r))^N$.

    Recall that for $x\in[0,1]$ we have $1-(1-x)^N\leq N x$.
    Therefore, we obtain
    \begingroup		%
    \allowdisplaybreaks
    \begin{align*}
        1 - (1-\Prob(T_1\leq w))^N    & \leq
        N \Prob(T_1< w)                                                                                                                                        \\
        \overset{\substack{\eqref{eqn:tbd1}                                                                                                                    \\\eqref{eqn:tbdnew2}}} & {\leq} N \frac{\kappa_2}{w^{-1/2}} \, \exp\!\left(-\frac{w^{-1}}{2}\right)
        ={}
        N \frac{\kappa_2}{w^{-1/2}} \,\exp\!\left( - \frac{1}{2} \left( \frac{r -\ln  \frac{\kappa}{c_N}}{2(\ln N)^ 2}+ \frac{1}{2\ln  N} \right)^{-1} \right) \\
                                      & ={}
        N \frac{\kappa_2}{w^{-1/2}} \,\exp\!\left( - \ln N \left( \frac{r -\ln  \frac{\kappa}{c_N}}{\ln N}+ 1 \right)^{-1} \right)                             \\
        \overset{\eqref{eqn:tbd2}}    & {\leq}
        N \frac{\kappa_2}{w^{-1/2}} \,\exp\!\left( - \ln N \left(1 - \frac{r -\ln  \frac{\kappa}{c_N}}{\ln N}\right) \right)
        ={}
        \frac{\kappa_2}{w^{-1/2}} \,\exp\!\left(  r -\ln  \frac{\kappa}{c_N} \right)                                                                           \\
                                      & ={}
        \frac{\kappa_2}{w^{-1/2}} \, e^r  \frac{c_N}{\kappa}
        ={}
        e^r \, \frac{\kappa_2}{\kappa} \,  (w \cdot 2\ln N )^{1/2}                                                                                             \\
        \overset{\eqref{eqn:tbdnew2}} & {\leq}
        e^r \, \frac{\kappa_2}{\kappa} \, \sqrt{2},
    \end{align*}
    \endgroup
    for large $N$ (uniformly in $r$) and $c_N = (2\ln N)^{1/2}$.

    This gives an integrable majorant for $r\mapsto \Prob(X_N<r)$ for $r\in(-\infty,0]$.
    Therefore, we have
    \begin{equation*}
        - \E{X_N \mathds{1}_{X_N<0}} = \int_{-\infty}^0 \Prob( X_N < r) \diff r
        \to \int_{-\infty}^0 \Prob( G < r) \diff r = - \E{G \mathds{1}_{G<0}}.
    \end{equation*}

    The same argument applies to $\E{X_N^2 \mathds{1}_{X_N<0}}$ since the above estimate can also be used for $r \mapsto 2(-r)\Prob( X_N < r)$.

    This together with \eqref{eqn:new+} and \eqref{eqn:new++} shows that $\E{X_N} \to \E{G}$ and $\E{X_N^2} \to \E{G^2}$, as required.
\end{proof}

\section{Estimation of the cost ratio in simulation}

Let us first provide details on the utilized approach to estimate the event-triggered cost in the cost ratio and, subsequently, to obtain the confidence interval depicted in Fig.~\ref{fig:cost_ratio}.

Consider the Bessel process $R(t):=\sqrt{\sum_{i=1}^N v_i(t)^2}$ of dimension $N$ started at $R(0) = 0$.

\begin{fact}\label{fact:Q_sim}
    For any stopping time $T$ satisfying the assumptions in Facts~\ref{fact:0T} and \ref{fact:Q}, we have
    \begin{equation*}
        Q(T) = \frac{\lvert\Eset\rvert}{2N(N+2)} \, \E{R(T)^4}.
    \end{equation*}
\end{fact}
\begin{proof}
    The stochastic differential equation solved by $(R(t))_{t\in\lbrack 0,\infty)}$ is given by
    \begin{equation*}
        \diff R(t) = \frac{N-1}{2R(t)} \diff t + \diff v(t),
    \end{equation*}
    where $v(t)$ is a standard Brownian motion.
    Let $Af(x):= \frac{N-1}{2x} f'(x) +\frac{1}{2} f''(x)$ be the infinitesimal generator of the Markov process $(R(t))_{t\in\lbrack 0,\infty)}$.
    Then, Dynkin's formula says that, for any stopping time $T$,
    \begin{equation*}
        \E{f(R(T))} = \E{\int_0^{T} A f(R(t)) \diff t}.
    \end{equation*}
    Let $f(x)=x^4$.
    Then, $Af(x)= \frac{N-1}{2x} \, 4 x^3 + \frac{4 \cdot 3}{2} \, x^2 = 2(N+2) x^2$.
    This gives
    \begin{equation*}
        \E{R(T)^4} = 2(N+2) \E{\int_0^{T} R(t)^2 \diff t}.
    \end{equation*}
    Utilizing
    \begin{equation*}
        \E{\int_0^{T} R(t)^2 \diff t} = N \E{\int_0^{T} v_1(t)^2 \diff t} = \frac{N}{\lvert\Eset\rvert} Q(T),
    \end{equation*}
    based on Fact~\ref{fact:Q} gives the stated formula for $Q(T)$.
\end{proof}

We can leverage this fact to estimate $Q_\mathrm{ET}(\Delta)$ in simulation and thereby arrive at an estimate for $J_\mathrm{ET}(\Delta)$.
This is required to obtain the cost ratio results in Section~\ref{sec:sim}.
In particular, note that this fact allows for the explicit cancellation of $\lvert\Eset\rvert$ from the cost ratio.
Thus, the obtained simulation results hold for any graph topology in the given framework.

At last, let us present the methodology to obtain the confidence interval in Fig.~\ref{fig:cost_ratio}.
\begin{fact}\label{fact:confidence}
    Let the estimates for $\E{R(\TET)^4}$ and $\E\TET$ be
    \begin{equation*}
        g_1 = \frac{1}{n_1} \sum_{i=1}^{n_1} R(\TET)^4_{i}, \quad g_2 = \frac{1}{n_2} \sum_{i=1}^{n_2} \TET{}_{,i},
    \end{equation*}
    respectively, where $n_1, n_2\in\mathbb{N}$ denote the number of samples $R(\TET)^4_{i}$ and $\TET{}_{,i}$ obtained from independent Monte Carlo simulations.
    Moreover, let $\lbrack g_1^\mathrm{L},g_1^\mathrm{R}\rbrack$, $\lbrack g_2^\mathrm{L},g_2^\mathrm{R}\rbrack$ be the corresponding confidence intervals with confidence level $\gamma$.
    Then, we can estimate the confidence interval $\lbrack\alpha^\mathrm{L},\alpha^\mathrm{R}\rbrack$ with
    \begin{equation*}
        \Prob\left( \alpha^\mathrm{L} \leq \frac{J_\mathrm{ET}(\Delta)}{J_\mathrm{TT}(\E{\TET(\Delta)})} \leq \alpha^\mathrm{R} \right) \geq \gamma^2
    \end{equation*}
    according to
    \begin{equation*}
        \alpha^\mathrm{L} = \frac{g_1^\mathrm{L}}{N(N+2) (g_2^\mathrm{R})^2}, \quad
        \alpha^\mathrm{R} = \frac{g_1^\mathrm{R}}{N(N+2) (g_2^\mathrm{L})^2}.
    \end{equation*}
\end{fact}
\begin{proof}
    With Fact~\ref{fact:Q_sim}, we can estimate the cost ratio as
    \begin{equation*}
        \frac{J_\mathrm{ET}(\Delta)}{J_\mathrm{TT}(\E{\TET(\Delta)})} \approx \frac{g_1}{N(N+2)g_2^2}.
    \end{equation*}
    Given the confidence intervals $\lbrack g_1^\mathrm{L},g_1^\mathrm{R}\rbrack$, $\lbrack g_2^\mathrm{L},g_2^\mathrm{R}\rbrack$, we find
    \begin{align*}
        \Prob\left( \alpha^\mathrm{L} \leq \frac{J_\mathrm{ET}(\Delta)}{J_\mathrm{TT}(\E{\TET(\Delta)})} \leq \alpha^\mathrm{R} \right)
        \geq{} & \Prob(g_1\in\lbrack g_1^\mathrm{L},g_1^\mathrm{R}\rbrack \mathrm{~and~} g_2\in\lbrack g_2^\mathrm{L},g_2^\mathrm{R}\rbrack)          \\
        ={}    & \Prob(g_1\in\lbrack g_1^\mathrm{L},g_1^\mathrm{R}\rbrack)\cdot\Prob(g_2\in\lbrack g_2^\mathrm{L},g_2^\mathrm{R}\rbrack) =  \gamma^2,
    \end{align*}
    which finishes the proof.
\end{proof}

Since we have large numbers of samples $n_1, n_2$, we can assume the samples $R(\TET)^4_{i}, \TET{}_{,i}$ to be normally distributed.
This allows us to apply standard statistical methods to determine the confidence intervals $\lbrack g_1^\mathrm{L},g_1^\mathrm{R}\rbrack$, $\lbrack g_2^\mathrm{L},g_2^\mathrm{R}\rbrack$, e.g., for a confidence level $\gamma=0.975$.
Consequently, the confidence interval $\lbrack\alpha^\mathrm{L},\alpha^\mathrm{R}\rbrack$ according to Fact~\ref{fact:confidence} corresponds to a confidence level of at least $\gamma^2=0.975^2 \approx 0.95$.

\section*{Acknowledgment}

D. Meister thanks the Stuttgart Center for Simulation Science (SimTech) for supporting him.

\bibliographystyle{elsarticle-num}
\bibliography{references}





\end{document}
