% ------------------------------------------------------------------------------------------------------------
\section{Current Standards for Contrast Quantification}
\label{sec:current_standards}
To this date, detection limits in HCI are not uniformly quantified, but are based on a wide range of standards (see  Table \ref{tab:family} for an overview). Many of these standards are used along with particular data reduction algorithms, which in turn are specialized for certain types of observing strategies. 
%
Most common observing strategies include angular differential imaging \citep[ADI][]{maroisAngularDifferentialImaging2006}, spectral differential imaging \citep[SDI][]{racineSpeckleNoiseDetection1999, sparksImagingSpectroscopyExtrasolar2002}, reference star differential imaging \citep[RDI][]{lafreniereHSTNICMOSDETECTION2009}, multi-reference star differential imaging \citep[mRDI][]{ruaneReferenceStarDifferential2019} and polarimetric differential imaging \citep[PDI][]{kuhnImagingPolarimetricObservations2001, quanzVERYLARGEESCOPE2011}. 

The data-reduction techniques can be categorized into three families \citep{cantalloubeExoplanetImagingData2021}: 
%
% PSF-subtraction
First, subtraction-based methods that try to model and subtract the stellar point spread function (PSF). Famous examples are PCA / KLIP \citep{amaraPYNPOINTImageProcessing2012, soummerDETECTIONCHARACTERIZATIONEXOPLANETS2012}, LOCI and its variations \citep{maroisExoplanetImagingLOCI2010, maroisGPIPSFSubtraction2014, wahhajImprovingSignaltonoiseDirect2015, thompsonImprovedContrastImages2021}, LLSG \citep{gonzalezLowrankSparseDecomposition2016} and the recently proposed HSR \citep{gebhardHalfsiblingRegressionMeets2022}. The output of these techniques is a \emph{residual image} whose values are related to flux. The quantification of the detection limits is carried out in a separate step. 
%
% Forward modeling
Second, forward-modeling techniques aim at tracking potential planetary signals during the observing sequence. Their result is not a residual, but a \emph{detection map} showing the model's belief about the presence of a planet. Contrast and detection uncertainy are defined as the match of expected signal and the data. Forward-modeling for HCI was introduced in \cite{cantalloubeDirectExoplanetDetection2015} with the ANDROMEDA algorithm and extended in various ways; see, for example FMMF \citep{ruffioImprovingAssessingPlanet2017}, PACO \citep{flasseurExoplanetDetectionAngular2018} and TRAP \citep{samlandTRAPTemporalSystematics2021}. 
%
Third, supervised machine learning (ML) methods \citep{gomezgonzalezSupervisedDetectionExoplanets2018} try to learn the specific signatures of signal and noise in the data. Their output is again a \emph{detection map}. 

Because numerous methods are used to post-process the data, the metrics differ fundamentally in their definition of what a detection is. This starts with the \emph{research question} they address: Some metrics focus only on the detection problem, that is whether a potential candidate is real (Q1). Other metrics try to constrain detection limits (Q2). Yet other methods specialize solely on comparing post-processing algorithms \citep[e.g. ROC-curves][]{gonzalezLowrankSparseDecomposition2016}. 
%
% A metric in this context is a measure for the reliability of planet detection.
%
Even more fundamentally, there is no uniform standard to define \emph{what is signal and what is noise}. For example, some methods calculate the strength of the noise based on areas around the signal \citep{maroisConfidenceLevelSensitivity2008, mesaPerformanceVLTPlanet2015, ottenONSKYPERFORMANCEANALYSIS2017, golombPlanetEvidencePlanetNoise2021} while others consider noise with the same distance from the star \citep{cantalloubeDirectExoplanetDetection2015, mawetFUNDAMENTALLIMITATIONSHIGH2014, jensen-clemNewStandardAssessing2017}. Other variants use the opposite angle rotated residual \citep{pairetSTIMMapDetection2019, wahhajGEMININICIPLANETFINDING2013} or estimates along time \citep{dahlqvistRegimeswitchingModelDetection2020}. The noise statistics is sometimes calculated directly on pixel values \citep{maroisConfidenceLevelSensitivity2008, mesaPerformanceVLTPlanet2015, cantalloubeDirectExoplanetDetection2015, ruffioImprovingAssessingPlanet2017}, while others average multiple pixel inside apertures \citep{mawetFUNDAMENTALLIMITATIONSHIGH2014, jensen-clemNewStandardAssessing2017}.
Moreover, different types of \emph{statistics} are used. While some authors use classical definitions of signal-to-noise ratios \citep{rameauDISCOVERYPROBABLE452013, meshkatFURTHEREVIDENCEPLANETARY2013, mesaPerformanceVLTPlanet2015, uyamaSEEDSHighContrastImaging2017}, others address the quantification using frequentist hypothesis testing \citep{mawetFUNDAMENTALLIMITATIONSHIGH2014, jensen-clemNewStandardAssessing2017} or Bayesian methods \citep{ruffioBayesianFrameworkExoplanet2018, golombPlanetEvidencePlanetNoise2021}.

% We have to place this figure here in oder to have it on top of the page covering the detection topic
\begin{figure*}[t!]
	\epsscale{1.2}
	\plotone{01_Hypothesis_testing.pdf}
	\caption{The three main steps of the hypothesis testing framework introduced by \cite{mawetFUNDAMENTALLIMITATIONSHIGH2014} which we generalize in this paper. \textit{Step 1} shows a typical example of a residual image obtained after data post-processing with PCA. A previously inserted artificial companion appears as a spot slightly brighter than the remaining speckle noise. More details on the dataset are given in \mbox{Section \ref{sec:critical}}. In order to determine if the planet is real we extract values for noise $X$ and signal $Y$ by averaging pixel values inside apertures. In \textit{Step 2} we formulate two competing hypotheses $H_0$ (top) and $H_1$ (bottom) for detection and non-detection, respectively. The orange and blue crosses correspond to the signal and noise values obtained from the residual image on the left. Is the signal bright enough to show that $\mathcal{X}$ and $\mathcal{Y}$ differ significantly in their means? In order to answer this question we calculate the test statistic of the two sample t-test $T_{\text{obs}}=2.28$ (Equation \ref{eq:SNR}). \textit{Step 3}: The detection uncertainty or false-positive-fraction (FPF) is given by the shaded red area under the t-distribution with $\nu = n-1 =9$ degrees of freedom.}
	\label{fig:hypothesis_testing}
\end{figure*}

Due to these differences, results calculated with different standards are often not comparable. 
%The same applies to all scientific analyses founded upon the calculated detection limits, e.g. planetary occurrence rates. 
But which of the presented metrics gives the "right" scientific answers? On the one hand, an ideal metric should be \emph{universally applicable} irrespective of the observing strategy or data reduction used. On the other hand, it should be \emph{robust} under different data characteristics and provide reliable estimates for the achieved contrast. 
% The metric should not facilitate more detections or higher contrast.
%
% Limits of current standards
Current standards are limited with respect to both criteria: First, they are often specialized to be used with specific post-processing algorithms. For example, FMMF is used after post-processing with PCA.
%
Second, they are reliant on fixed assumptions about the noise making them non-robust under varying conditions.

In the following \mbox{Section \ref{sec:what_is_a_detection}} we revisit the metric presented in \cite{mawetFUNDAMENTALLIMITATIONSHIGH2014} and assess its limitations. Afterwards, we propose modifications and extensions to this approach in order to improve the robustness and universal applicability of the metric.
