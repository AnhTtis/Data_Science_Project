\section{Beyond Gaussian Noise}
\label{sec:beyond_gaussian_noise}
\begin{figure*}[t!]
	\epsscale{1.05}
	\plotone{05_Parametric_BS.pdf}
	\caption{The parametric bootstrap test introduced in this paper. The left side of the figure illustrates the key steps of the test, exemplary for Gaussian noise. The values used are identical to those in Figure \mbox{\ref{fig:hypothesis_testing}}. The numbers of the steps match with those used in the text. On the right, three histograms are given, which show the empirical distribution of $p(T^* = t | H_0)$ for different numbers of bootstrap samples $B$. The actual test does not require to compute histograms and the FPF is computed from the bootstrap samples \mbox{$T_{(1)}^*\leq  ... \leq T_{(B)}^*$} directly (see \mbox{Equations \ref{eq:fpf_bs_simple} and \ref{eq:FPF_para_bs}}). The results are compared with the t-distribution with $\nu = n -1 = 9$ degrees of freedom. Compared to the t-test, bootstrapping is much more versatile, since any distribution, not necessary Gaussian, can be assumed in step 3.}
	\label{fig:para_bs_workflow}
\end{figure*}

% How to fix the assumption about Gaussian noise:
In general, we can distinguish three concepts to deal with non-Gaussian noise:
%
% Find a better assumption
First, we can assume a different noise distribution which better describes the speckle behavior. The RSM presented in \cite{dahlqvistRegimeswitchingModelDetection2020} explores this idea, though along the time domain and not in the residual image. 
%
% Estimate the noise from the data
Second, we can try to estimate the distribution of the noise directly from the data. This idea was previously studied by \cite{maroisConfidenceLevelSensitivity2008} and \cite{pairetSTIMMapDetection2019} who tried to estimate the noise probability density function (PDF) based on pixel values in the residuals. As mentioned before, this can be problematic since the pixels in the residuals are not independent. Further, extensive extrapolation is required in order to describe how the noise behaves in the tails of the PDF. The concept is therefore better suited for large surveys as e.g. done by \cite{ruffioImprovingAssessingPlanet2017}.
%
% No assumption about the noise
Finally, we can try to estimate detection limits by not making any assumptions about the noise. An example for this are ROC-curves \citep{gonzalezLowrankSparseDecomposition2016, jensen-clemNewStandardAssessing2017} which are often used to benchmark post-processing algorithms.

In the following, we propose a new metric based on bootstrapping which follows the first concept and allows us to compute the detection uncertainty for any type of noise. The other two concepts are subject of future work.
%
In order to retain the interpretability of the test statistic $T$, we need to find its distribution $p(T = t | H_0)$ . In principle this could be done empirically. That is, we repeat our observation a lot of times on identical stars that do not host a planet and compute values for $T$ for every observation. The probability $p(T = t | H_0)$  is then given by the frequency that a value $T$ is observed. 
%
While repeating the observation that many times is not feasible in practice, it gives the key idea of the bootstrap: we aim at finding a distribution $\hat{F}$ that is close to the true but unknown distribution of the noise $F$ and use it to \emph{repeat the experiment}.
%
Two main concepts can be distinguished: first, the non-parametric bootstrap which makes no further assumptions about the noise and tries to approximate $F$ directly from the data. Second, the parametric bootstrap which assumes that $F$ follows a parametric distribution with unknown parameters.
%
In practice, if only a few noise observations are available, the estimated noise model $\hat{F}$ of the non-parametric bootstrap might be inaccurate w.r.t. the true $F$. This can lead to less accurate estimates of the test statistic distribution $p(T = t | H_0)$ .
 %
In this paper we study how to compute detection uncertainties using the parametric bootstrap, that is, under the assumption that the noise distribution is known. The data-driven non-parametric bootstrap will be explored in future work.

\subsection{The Parametric Bootstrap}
\label{sec:para_bs}
% The steps of the parametric bootstrap used in this paper
We adopted the bootstrap test discussed in example 3.4.1 of \cite{zoubirBootstrapTechniquesSignal2004} and chapter 16.2 of \cite{efronIntroductionBootstrap1993} to the special case of HCI where only a single value for $\mathcal{Y}$ is available. The test is similar to the two sample t-test explained in \mbox{Section \ref{sec:what_is_a_detection}} except that we can assume any noise distribution. The following procedure describes the main steps of the test, which are the same irrespective of the assumed noise distribution. For demonstration purposes, we give examples of how the test is carried out in the case of Gaussian noise (see \mbox{Figure \ref{fig:para_bs_workflow}}):
%
%If the assumed noise distribution satisfies some additional properties, the bootstrap procedure can be simplified significantly. We will discuss this topic in \mbox{Section \ref{sec:pivot}}. 
%The main steps of the parametric bootstrap test are:
%
\begin{enumerate}
	\item Extract the noise sample $\mathcal{X} = \{X_1, ..., X_n\}$ and the signal sample $\mathcal{Y} = \{Y_1\}$ from the residual image\footnote{We write $\mathcal{X} = \{X_1, ..., X_n\}$ and $\mathcal{Y} = \{Y_1\}$ instead of $\mathcal{X} = \{\overline{X}_1, ..., \overline{X}_n\}$ and $\mathcal{Y} = \{\overline{Y}_1\}$ to emphasize that we use spaced pixel instead of apertures (see discussion in \mbox{Section \ref{sec:independence}}).}. As for the t-test we assume that the noise is independent and identically distributed.
	\item Compute $T_{obs}$ by using \mbox{Equation \ref{eq:SNR}}.
	\item Assume a parametric distribution for the noise and list its unknown parameters. In case of Gaussian noise this would be $F=\mathcal{N}(\mu_\mathcal{X}, \sigma)$.
	\item Compute the maximum likelihood estimate of the unknown parameters. For Gaussian noise we compute the mean $\hat{\mu}_\mathcal{X} = \overline{\mathcal{X}}$ and the standard deviation $\hat{\sigma} = \text{std}(\mathcal{X})$ and set $\hat{F}=\mathcal{N}(\hat{\mu}_\mathcal{X}, \hat{\sigma})$. 
	\item \emph{Repeat the experiment} $B$ times under $H_0$ using numerical simulation based on $\hat{F}$. For every repetition we resample the noise \mbox{$\mathcal{X}^* = \{X_1^*, ..., X_n^*\} \sim \hat{F}$} and the signal \mbox{$\mathcal{Y}^* = \{Y_1^*\} \sim \hat{F}$} from the same distribution. It is important to draw the same number of noise observations for $\mathcal{X}^*$ as we have in $\mathcal{X}$. The results are $B$ so called bootstrap samples 
	\begin{eqnarray*}
		&(\{X_1^*, ..., X_n^*\}, \{Y_1^*\})_1, ..., (\{X_1^*, ..., X_n^*\}, \{Y_1^*\})_B \\ &= (\mathcal{X}^*_1, \mathcal{Y}^*_1), ... , (\mathcal{X}^*_B, \mathcal{Y}^*_B)
	\end{eqnarray*}
	\item For every bootstrap sample $(\mathcal{X}^*_i, \mathcal{Y}^*_i)$ compute the test statistic $T_i^*$ by using \mbox{Equation \ref{eq:SNR}}. Rank the results with increasing order \mbox{$T_{(1)}^*\leq  ... \leq T_{(B)}^*$}.
	\item Under the assumption that $\hat{F}$ is close to $F$ the distribution of $p(T^*=t | H_0)$ will converge to $p(T = t | H_0)$  if $B$ becomes large. The convergence depends on the maximum likelihood estimates calculated in step 4. If this estimate is inaccurate the bootstrap results will be biased. However, if the assumed noise distribution satisfies some additional properties explained in \mbox{Section \ref{sec:pivot}}, we are able to obtain the exact distribution of \mbox{$p(T = t | H_0)$}  irrespective of how close $\hat{F}$ is to $F$. 
	
	The false positive fraction is given by the fraction of values in $\{T_1^*, ..., T_B^*\}$ which are larger than $T_{\text{obs}}$:
	\begin{equation}
		\label{eq:fpf_bs_simple}
		\text{FPF} = \#\{T_i^* > T_\text{obs}\} / B \, .
	\end{equation}
	More accurate results can be obtained by linear interpolation. For this purpose we use the sorted bootstrap results \mbox{$T_{(1)}^*\leq  ... \leq T_{(B)}^*$} from the previous step and search for the two values of $T^*$ which are adjacent to $T_{\text{obs}}$: $T^*_{(a)} \leq T_{\text{obs}} \leq T^*_{(b)}$, where $a$ and $b$ are the corresponding indices. The detection uncertainty can be computed with  
	\begin{eqnarray}
	\label{eq:FPF_para_bs}
	1 -\text{FPF} = &\frac{a}{B-1} \cdot \frac{T_{\text{obs}} - T^*_{(a)}}{T^*_{(b)} - T^*_{(a)}} \\ \nonumber
				    + &\frac{b}{B-1} \cdot \frac{T^*_{(b)} - T_{\text{obs}}}{T^*_{(b)} - T^*_{(a)}} \, .
	\end{eqnarray}
\end{enumerate}
%
As shown in \mbox{Figure \ref{fig:para_bs_workflow}} the calculated FPF of the bootstrap test converges to $\text{FPF} = 0.0243$. This is exactly the same value that we obtained with the t-test in \mbox{Section \ref{sec:what_is_a_detection}}. This means that the parametric bootstrap is equivalent to the t-test if we assume Gaussian noise.

\subsection{The Parametric Bootstrap for Laplacian Noise}
\label{sec:para_bs_laplace}
In contrast to the t-test, the parametric bootstrap allows us to compute the FPF for any type of noise. This can be done by swapping out the Gaussian assumption in step 3.
% Example for the Laplace
In coronagraphic images with small static residuals, the modified Rician distribution of the speckle noise \citep{soummerSpeckleNoiseDynamic2007,aimeUsefulnessLimitsCoronagraphy2004} is reduced to a one-sided exponential \citep{fitzgeraldSpeckleStatisticsAdaptively2006}. This applies for example in the case of coronagraphic imaging with small non-common path aberrations or after some basic data post-processing. If we subtract two such images from each other, as done in the ANDROMEDA algorithm \citep{cantalloubeDirectExoplanetDetection2015}, we expect the noise to follow a two sided-exponential distribution i.e. a Laplacian. Similarly, if a PCA-model tries to subtract bright speckles, it sometimes erroneously induces negative speckles leading to a similar type of noise. Even after averaging several frames along the temporal dimension, the residual noise of some HCI datasets is often better described\footnote{Often it is not possible to find sufficient evidence that the residual noise is indeed Laplacian or Gaussian. More details on the topic are given in Appendix \ref{sec:testing_gaussian}.} by Laplacian and not by Gaussian noise \citep[see][for a detailed analysis on this topic]{pairetSTIMMapDetection2019}. 
%
In order to extend the bootstrap procedure to Laplacian noise we first assume $F = \mathcal{L}(\mu_{\mathcal{X}}, b)$ with PDF:

\begin{equation}
	\label{eq:laplace_pdf}
	f(x| \mu_{\mathcal{X}}, b) = \frac{1}{2b}\exp \left(-\frac{|x - \mu_{\mathcal{X}}|}{b}\right) \, .
\end{equation}
%
The maximum likelihood parameters \mbox{(step 4)} for $\mu_{\mathcal{X}}$ and $b$ are given by \citep{nortonDoubleExponentialDistribution1984}:
\begin{eqnarray}
	\hat{\mu}_{\mathcal{X}} = &\text{median}(\mathcal{X}) \\
	\hat{b} = &\frac{1}{n} \sum_{i=1}^{n} |X_i - \hat{\mu}_{\mathcal{X}}|
\end{eqnarray}
This gives us  $\hat{F} = \mathcal{L}(\hat{\mu}_{\mathcal{X}}, \hat{b})$ which we use to repeat the experiment by resampling (compare left side of \mbox{Figure \ref{fig:para_bs_workflow}}).
The remaining steps of the test are identical to those in \mbox{Section \ref{sec:para_bs}}\footnote{
The test statistic in \mbox{Equation \ref{eq:SNR}} is not necessarily optimal for all types of noise. It might be possible to derive a new test statistic for specific situations which still gives correct FPF estimates and at the same time offers higher power. This could be done by utilizing the Neyman-Pearson lemma \citep[Theorem 3.1.1 in][]{zoubirBootstrapTechniquesSignal2004}. We leave this idea open for future work and focus our analysis on the calculation of the error budget using the test statistic in \mbox{Equation \ref{eq:SNR}}.}.
%
By changing $F$ from a Gaussian to a Laplacian, we are able to compute the FPF under the assumption of Laplacian noise and the distribution of the test statistic $p(T = t | H_0)$  (compare right side of \mbox{Figure \ref{fig:para_bs_workflow}}) will no longer be a t-distribution. A comparison of $p(T = t | H_0)$  under the assumption of Gaussian and Laplacian noise is given in \mbox{Figure \ref{fig:bs_convergence}}.
%
If the assumed noise distribution is Gaussian, the distribution of $p(T = t | H_0)$  resulting from bootstrapping agrees with the t-distribution. This applies to separations close to the star as well as further out. If the assumed noise distribution is Laplacian, the bootstrap converges to a distribution with even heavier tails than the t-distribution. These heavy tails allow us to correct for the high occurrence of large noise values in Laplacian noise. We note, the distributions shown in \mbox{Figure \ref{fig:bs_convergence}} are not the distributions of the noise (here Gaussian and Laplacian), but the distribution of the test statistic $p(T = t | H_0)$ . 
%
\begin{figure}[t]
	\epsscale{1.12}
	\plotone{06_Convergence_distribution_BS.pdf}
	\caption{The convergence distributions of the parametric bootstrap, shown as histograms, in comparison with the normal and t-distribution. The top two plots show $p(T = t | H_0)$  under the assumption that $F$ is Gaussian, once at a separation of $2 \, \text{FWHM}$ i.e. $n=11$ and once at $8 \, \text{FWHM}$ i.e. $n=49$. The plots show the same result as the right side of \mbox{Figure \ref{fig:para_bs_workflow}} but for different $n$. The bottom two plots show $p(T = t | H_0)$  under the assumption of Laplacian noise. We use $B=10^{8}$ bootstrap iterations.}
	\label{fig:bs_convergence}
\end{figure}

In step 5 of the bootstrap algorithm we sample the same number of noise observations $n$ as in the original \mbox{sample $\mathcal{X}$}. This allows us to simultaneously consider the effects of the small sample statistics and non-Gaussian noise. The procedure can be extended to any parameterized noise \mbox{distribution $F$} for which the maximum likelihood estimates in step 4 are known.

\subsection{The Importance of Pivoting}
\label{sec:pivot}
In general, if we make no further restrictions on the type of noise distribution assumed, the accuracy of the bootstrap test depends on the maximum likelihood estimates calculated in step 4 in \mbox{Section \ref{sec:para_bs}}. Any deviation of the estimated $\hat{F}$ from the true $F$ will change \mbox{$p(T^*=t | H_0)$} and with it bias the FPF. However, for some specific types of noise distributions, we can overcome this limitation by taking advantage of so-called pivotal quantities \citep[see definition 9.2.6 in][]{casellaStatisticalInference2002}: "a random variable $Q(\mathcal{X}, \theta)$ is a pivotal quantity if the distribution of $Q(\mathcal{X}, \theta)$ is independent of all parameters. That is, if $\mathcal{X} \sim F(x|\theta)$, then $Q(\mathcal{X}, \theta)$ has the same distribution for all values $\theta$."

% Explain what this means in case of the t-test and Gaussian noise
In case of the t-test the noise and planet sample follow a normal distribution $\{X_1, ..., X_n \} \sim \mathcal{N}(\mu_\mathcal{X}, \sigma)$, \mbox{$\{Y_1\} \sim \mathcal{N}(\mu_\mathcal{Y}, \sigma)$} with unknown parameters $\theta=\{\mu_{\mathcal{X}}, \mu_{\mathcal{Y}}$, $\sigma\}$. Under the null hypothesis, the test statistic $T$ \mbox{ in Equation \ref{eq:SNR}} is a pivotal quantity. That is, the distribution of $p(T = t | H_0)$  is always the same, irrespective of the unknown parameters $\mu_{\mathcal{X}}, \mu_{\mathcal{Y}}, \sigma$. This means that no matter which maximum likelihood estimates $\hat{\mu}_{\mathcal{X}}$ and $\hat{\sigma}$ we calculate, the bootstrap procedure will always converge to the same distribution. For the t-test this distribution is the t-distribution. The fact that $T$ is a pivot is crucial for the t-test: it allows us to compute the exact FPF based on the t-distribution without knowing $\mu_{\mathcal{X}}, \mu_{\mathcal{Y}}$ and $\sigma$. 

In \mbox{Appendix \ref{sec:pivot_proof}} we prove that $T$ is a pivot not only under the assumption of Gaussian noise but for all distributions from a location-scale family. A location-scale family is characterized by two unknown parameters: a scale parameter $q$ and a shift parameter $w$. Let $Z \sim \mathcal{F}$ be a random variable following a standard distribution with no unknown parameters. By shifting and scaling $Z$ we obtain the location-scale family with random variables $X = Z q + w$. In case of the normal distribution $Z$ is the standard normal distribution $Z \sim \mathcal{N}(0, 1)$ with $q = \sigma, w=\mu$. Also the Laplacian distribution is a location-scale family were $Z \sim \mathcal{L}(0, 1)$ with PDF

\begin{equation}
	f(x) = \frac{1}{2}\exp \left(-|x|\right) 
\end{equation}
and $q = b, w=\mu$ (compare \mbox{Equation \ref{eq:laplace_pdf}}). 
%
The result that $T$ under $H_0$ is a pivotal quantity for location-scale family distributions has direct implications for the bootstrap procedure explained in \mbox{Section \ref{sec:para_bs}}:
%
The computation of the maximum likelihood parameters in step 4 is no longer needed. The resampling in steps 5 and 6 is independent of $\hat{\mu}_{\mathcal{X}}$ and $\hat{\sigma}$ (or $\hat{b}$) and always converges to the same $p(T^*=t | H_0)$. No matter how close $\hat{\mu}_{\mathcal{X}}$ and $\hat{\sigma}$ (or $\hat{b}$) are to the true values of $\mu_{\mathcal{X}}$ and $\sigma$ (or $b$), we always obtain the exact FPF.
%
The estimation of the FPF becomes a simple lookup. We can calculate $p(T^*=t | H_0)$ once and reuse it for future calculations, which drastically reduces the computation time.

The distribution of $p(T^*=t | H_0)$ depends on the sample size $n$ which is why we have to compute separate lookups for different separations from the star. In order to achieve accurate results for low FPF, the number of bootstrap samples $B$ has to be large. For example, if we aim for a FPF of $2.87 \cdot 10^{-7}$ ($5\sigma_{\mathcal{N}}$), we expect to observe one false-positive every \mbox{$B=1/2.87 \cdot 10^{-7} = 3.48 \cdot 10^{6}$} iterations. To constrain $p(T^*=t | H_0)$ with sufficient accuracy, we need about $B=10^8$ bootstrap samples.
%
In our \texttt{python} package \texttt{applefy} we provide pre-computed lookups within $1 - 20 \, \lambda /D$ for $B=10^8$ and interfaces which can be used to extend the code to other noise distributions.
