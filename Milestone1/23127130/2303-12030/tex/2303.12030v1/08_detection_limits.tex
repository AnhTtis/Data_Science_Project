\section{How to quantify detections with parametric bootstrapping?}
\label{sec:quantify_detections_with_bs}
Current standards for contrast quantification in HCI (compare \mbox{Section \ref{sec:current_standards}}) address two main research questions: First, the detection problem, i.e. whether a potential candidate is real (Q1), and second, the quantification of the detection limits (Q2). The following two subsections give a detailed recipe of how parametric bootstrapping can be used to address these two questions. An implementation is given in our \texttt{python} package \texttt{applefy}.

\subsection{How to compute the Uncertainty of a Detection?}
\label{sec:how_to_detection}
In \mbox{Section \ref{sec:critical}} we identified two limitations of the t-test when used in HCI: 1. The violation of the independence assumption if we use apertures  2. The non-Gaussian residual noise. To overcome these limitations, we propose the following procedure to quantify whether a signal in the data is a real planet candidate:
\begin{enumerate}
	\item Make an assumption about the type of noise present in the residual. For a conservative choice and in the case of speckle noise we recommend to choose Laplacian noise over Gaussian noise. If the noise is from a location scale family, use parametric bootstrapping to pre-compute the distribution of the test statistic under the null hypothesis $p(T^*=t | H_0)$ (see \mbox{Section \ref{sec:pivot}}). In case of Gaussian noise use the t-distribution. For surveys the use of the non-parametric bootstrap briefly mentioned in \mbox{Section \ref{sec:beyond_gaussian_noise}} represents an alternative to a fixed assumption about the noise. 
	
	\item The planet sample $\mathcal{Y}$ and noise sample $\mathcal{X}$ need to be extracted in the same way. Since we use spaced pixel for the noise, we have to use one pixel as the planet signal to get commensurable quantities. We search for the position of the planet by fitting a 2D Gaussian. The flux integrated over the circular area of one pixel around the best fit position gives us the planet signal $\mathcal{Y}$.	
	
	\item Extract noise values $\mathcal{X}$ with the same separation from the star. To ensure that the values in $\mathcal{X}$ are approximately independent, we do not use apertures but integrated over the circular area of one pixel spaced by one FWHM. In the background limited regime the use of apertures might be preferable for this and the previous step.

	\item Use \mbox{Equation \ref{eq:SNR}} to compute the test statistic $T_{\text{obs}}$\footnote{The assumption made in step 1 only influences the distribution of the test statistic $p(T = t | H_0)$ . The test statistic itself is the same.}.
	
	\item Translate $T_{\text{obs}}$ into the detection uncertainty (FPF) by using \mbox{Equation \ref{eq:FPF_ttest}} or \mbox{Equation \ref{eq:FPF_para_bs}}.
	
	\item Repeat steps 3. to 5. for different noise positions. This can be done by rotation of the initial noise positions (compare discussion in \mbox{Appendix \ref{sec:rotation}}  and \mbox{Figure \ref{fig:rotation}}).
	
	\item Report the median FPF over all noise positions. The mean absolute deviation from the median can be used as a measure of the uncertainties introduced by the placement of the noise values.
\end{enumerate}

\subsection{How to compute Detection Limits?}
\label{sec:detection_limits}
If no planet candidate is found, we can compute detection limits to constrain which planets existence we can confidently rule out. To this end, artificial planets are usually inserted in order to determine the minimum planet brightness still detectable \citep{maroisExoplanetImagingLOCI2010, morzinskiMAGELLANADAPTIVEOPTICS2015}. Hereby, the calculation of the detection limit has to be consistent with the calculation of the FPF in the previous section. That is, if we insert an artificial planet above the detection limit we expect it to be counted as a detection. Vice versa, if we insert an artificial planet below the detection limit it should not be detectable. 

In reality detection limits are never a hard limit and only give the point at which approximately 50\% of the planets are detected. There is no guarantee that all planets above the contrast curve will be detected and no planet below the curve will not be detected. At the position of the planet we always observe a combination of planet signal and speckle noise. 
If we are lucky and a faint planet falls on top of a speckle, we might be able to detect it although it is below our calculated limits. Conversely, if a negative speckle caused by the data post-processing is added to the planet signal, it may fall below the detection threshold. This effect can be quantified through calculations of the true-positive rate (TPR) also called the power of the test. A detailed discussion about the topic is presented in \cite{jensen-clemNewStandardAssessing2017}. In this paper we focus only on the FPF and leave power calculations for the parametric bootstrap open for future work. 

In the following we introduce a new approach to compute detection limits which is independent of the data post-processing. We call this approach the \emph{contrast grid}.
%
The calculation starts with the insertion of artificial planets at different separations e.g. \mbox{$s = 1, 1.5, ..., 7.5, 8 \quad \text{FWHM}$} and planet brightnesses e.g. \mbox{$f_p/f_* = 5, 5.5, ..., 10.5, 11 \quad \text{mag}$} into the raw dataset. We use the unsaturated PSF scaled by $f_p/f_*$ and potential attenuation due to a coronagraph as the planetary signal. 
%
\begin{figure}[t!]
	\epsscale{1.23}
	\plotone{07_contrast_grid.pdf}
	\caption{Detection limits for the $\beta$-Pictoris dataset under the assumption of Gaussian noise. The results under the assumption of Laplacian noise are presented later. The contrast grid (top pannel) gives the detection uncertainty (FPF) as a function of separation and contrast. Each value in the grid is based on 6 fake planet residuals. Below three example residuals are shown which were used to compute the contrast grid at the positions marked in orange and yellow. The $\text{FPF}_{\text{median}}$ given for each residual is the median FPF over different noise sample positions (see details in \mbox{Section \ref{sec:how_to_detection}}). The mean of all six $\text{FPF}_{\text{median}}$ values gives the final FPF value of the contrast grid. An interactive version of the plot is available on the documentation page of \texttt{applefy}: \url{https://applefy.readthedocs.io/}.}
	\label{fig:contrast_grid}
\end{figure}
%
In order to account for azimuthal variations we insert 6 artificial planets for each separation, one every 60 degrees. Some post-processing algorithms like the subtraction-based half-sibling regression presented in \cite{gebhardHalfsiblingRegressionMeets2022} make use of spatial correlations in the data. If we insert multiple planets at the same time these methods might learn to subtract the planet based on the movement of other planets in the data. Therefore, we insert only one planet at a time. For every inserted fake planet we run the data post-processing, in our case PCA, and save the residual images. Next, we estimate the FPF for each fake planet using the procedure explained in \mbox{Section \ref{sec:how_to_detection}}. Depending on the type of noise we can choose between the t-test or parametric bootstrapping.
%
The contrast grid, shown in \mbox{Figure \ref{fig:contrast_grid}}, combines all FPF estimates into single two dimensional grid.
%
The $5 \sigma_{\mathcal{N}}$ and $3 \sigma_{\mathcal{N}}$ contrast curves can be obtained by thresholding and interpolation of the grid.
%
As shown in the figure, no fake planet within 2 FWHM exceeds the $5 \sigma_{\mathcal{N}}$ detection threshold, no matter how bright it is. The reason for this is planet self-subtraction: At close separations a large fraction of the planetary signal is subtracted by the PCA noise model. If the planet gets brighter a progressively larger fraction of the planet flux is subtracted. In extreme cases an increase in planet brightness does not lead to a stronger signal in the residual. A detailed discussion of the effect is given in \mbox{Appendix \ref{sec:analytical_contast_curves}}.

The number of PCA components used during data post-processing affects the achieved contrast. While a higher number of PCA components leads to a stronger reduction of the noise, it also causes more loss of planetary signal. For the given dataset fewer components give higher contrast at close separations and more components provide better results for large separations. We recommend to compute contrast curves for a range of PCA components and report the overall best \citep{xuanCharacterizingPerformanceNIRC22018}. Our \texttt{python} package \texttt{applefy} provides the code needed to automate and parallelize these computations.

Under additional assumptions and for some data post-processing algorithms it is possible to compute the contrast curve analytically. This way the computation time can be reduced significantly. Current implementations in HCI packages like VIP \citep{gonzalezVIPVortexImage2017} or PynPoint \citep{stolkerPynPointModularPipeline2019} make use of the alternative. Also \texttt{applefy} allows to compute analytical contrast curves. A detailed discussion of this approach and how it can be used with parametric bootstrapping is presented in \mbox{Appendix \ref{sec:analytical_contast_curves}}.
