\section{Error mitigation}
\label{sec:mitigation}


\subsection{Rescaling of survival probabilities}

Without mitigation, noise will frustrate any naive attempts to observe prethermal plateaus on current quantum hardware. As we show in Appendix~\ref{sec:diff_TE}, noise provides an additional heating source to the Floquet driving already discussed; one that we expect to be far stronger with today's error rates, and one without favourable scaling in the system size. It is therefore desirable to develop an error mitigation technique to estimate the result of a noiseless quantum circuit from multiple measurements in a noisy circuit~\cite{Temme2017, Endo2018a, Cai2022}. However, we do not see a reliable method for extracting the desired noiseless results from measurements of the noisy state as this would imply the ability of inferring low-temperature results from high-temperature ones.

To circumvent this issue, we avoid direct tomography of the time-evolved observables on the noisy state. Instead, we convert observable estimation into a survival probability circuit, in a manner similar to that used in out-of-time-order correlators (OTOC)~\cite{mi2021} or echo verification circuits~\cite{obrien2021,huo2022}. Following forward evolution, we \emph{apply} the observable and then evolve backwards in time, followed by a projection onto the initial state (see Fig.~\ref{fig:scaling}a). This yields a survival probability of the form
\begin{eqnarray}
    L_{A, \psi}(t) = \left| \braket{\psi | e^{iHt} A e^{-i H t} | \psi} \right|^2 = \braket{\psi | A(t) | \psi}^2.
\end{eqnarray}
In the following, we drop the label $\psi$ for notational simplicity. For this procedure to work, $A$ must be a (local) unitary. For spin systems, it is possible to write any observable as a sum of products of unitary Pauli operators and to measure each Pauli operator separately. Although $L_{A}(t)$ only gives the expectation value of an observable up to a sign, one can infer the sign by tracking it from the known initial value, assuming $\braket{\psi | A(t) | \psi }$ is a smooth function~\cite{Lu2020}.
This simplifies previous Loschmidt-echo style methods for learning $\braket{\psi | A(t) |\psi}$, which required ancilla qubits, the preparation of large Greenberger-Horne-Zeilinger (GHZ) states~\cite{obrien2021} or intermediate re-preparation and measurement of qubits~\cite{huo2022}.

As we will now demonstrate, a simple rescaling is remarkably effective at mitigating errors in the estimation of the survival probability. The strategy is based on the observation that the survival probability is approximately proportional to the probability of no error occurring. The reason is that the state becomes highly entangled during the evolution, at which point a single-qubit error results in an orthogonal state with high probability. To be more concrete, consider a single Pauli error $\sigma^{\mu}_i$ occurring at time $t^{\prime} < t$ at site $i$ and set the observable $A$ to be identity. The survival probability is then given by $[\tr (\rho_i(t^{\prime}) \sigma^{\mu}_i)]^2$, where $\rho_i(t^{\prime})$ is the reduced density matrix of $\ket{\psi(t^{\prime})}$ at site $i$. If this site is entangled with the other parts of the system, the reduced density matrix will be close to the identity (completely mixed) and the survival probability will be close to zero. 

The above discussion suggests that the survival probability with noise is related to the noiseless value, times the probability that no error has occurred. For concreteness, we consider error models in which a single-qubit noise channel $\mathcal{N}_p$ is applied to each qubit after every layer of unitary gates. Here, $p$ is the probability that the channel causes an error on the qubit. The state of art gate error rate is around $0.5\%$ for two-qubit gates~\cite{Mi2022b, Wei2022a}, motivating our choice of $p = 0.3\%$ per qubit per gate as the reference value in our model~\footnote{In experiments, XY rotations are sometimes compiled into more than one two-qubit gate. The value of $p$ should then be increased accordingly.}.

Denoting the survival probability in the presence of noise by $L_A^{\mathcal{N}_p}(t)$, we then expect that
\begin{eqnarray}
    L_{A}^{\mathcal{N}_p}(t) / L_{A}(t) \approx (1 - p)^{ND},
    \label{eq:scaling}
\end{eqnarray}
where $N$ is the number of qubits and $D$ is the circuit depth including both forward and backward evolutions. Crucially, no independent knowledge of the noise channel is required to estimate $L_A(t)$. By setting $A = \id$, we obtain $L_\id^{\mathcal{N}_p}(t) \approx (1 - p)^{ND}$ since the noiseless survival probability satisfies $L_\id(t) = 1$. Hence,
\begin{eqnarray}
    L_A(t) \approx L_A^{\mathcal{N}_p}(t) / L_\id^{\mathcal{N}_p}(t),
    \label{eq:rescale}
\end{eqnarray}
where the right-hand side can be obtained from measurements on the noisy quantum device.

We can make this argument more rigorous for channels that can be represented in terms of unitary Kraus operators. For such channels, the probability that a particular error occurs is independent of the state. This class of channels includes depolarizing and dephasing noise as well as all other Pauli channels~\footnote{Even though amplitude damping error is not included in this class of channels, we find that the conclusions of this section nevertheless hold to a good approximation. See Appendix~\ref{sec:supp_plot} for numerical results.}. The survival probability after the noisy circuit can be expressed as
\begin{eqnarray}
    L_{A}^{\mathcal{N}_p}(t) = \tr \left[ \left( A \rho^{\mathcal{N}_p}_{\psi}(t) \right)^2\right],
    \label{eq:noisy_sp}
\end{eqnarray}
where $\rho^{\mathcal{N}_p}_{\psi}(t)$ is the mixed state after the noisy forward evolution~\footnote{To obtain this equation, the circuit in Fig.~\ref{fig:scaling}a has to be slightly modified: during backward evolution, the error gates occur before each evolution unitary gate instead of after it.}. We write the state $\rho^{\mathcal{N}_p}_{\psi}(t)$ as
\begin{eqnarray}
    \rho^{\mathcal{N}_p}_{\psi}(t) = q \ket{\psi_t}\bra{\psi_t} + (1-q)\tilde{\rho},
    \label{eq:dm_noisy}
\end{eqnarray}
where $\ket{\psi_t} = U_{\mathrm{Trotter}}^{t/\tau}(\tau) \ket{\psi}$ is the state after noiseless forward evolution and $q = (1 - p)^{ND / 2}$ is the probability that no error occurred during the forward evolution. The density matrix $\tilde{\rho}$ is the state conditioned on at least one error having occurred. The survival probability in noisy simulation then becomes
\begin{eqnarray}
    \begin{aligned}
        L_{A}^{\mathcal{N}_p}(t)
        = & q^2 |\braket{\psi_t | A | \psi_t }|^2 + (1-q)^2\tr \left[ (\tilde{\rho} A)^2 \right]\\
        &  + 2q(1-q) \braket{\psi_t | A \tilde{\rho} A | \psi_t}.
    \end{aligned}
    \label{eq:expansion_sp}
\end{eqnarray}
Defining $r = \sqrt{ \tr \left[\tilde{\rho}^2 \right] }$, we can use Cauchy-Schwarz inequality to obtain (see Appendix~\ref{app:proof})
\begin{eqnarray}
    \left | \frac{L_{A}^{\mathcal{N}_p}(t)}{q^2} - L_{A}(t) \right| \le (1-q)^2\left(\frac{r}{q}\right)^2 + 2(1-q)\frac{r}{q}.
    \label{eq:scaling_math}
\end{eqnarray}
Since $0 < q, r \le 1$, $L_{A}^{\mathcal{N}_p}(t) / q^2$ serves as a good approximation of $L_{A}(t)$ when $q \gg r$. This condition can be satisfied over a broad range of parameters because $r$ typically decays with the system size. In the most extreme case of global depolarizing noise, $\tilde{\rho}$ is a completely mixed state, for which $r^2 = 2^{-N}$. The condition $q \gg r$ then gives rise to
\begin{eqnarray}
    (1-p)^{ND} > \frac{C}{2^N} \Rightarrow  ND < \frac{N\log{2} + \log (1 / C)}{\log[1 / (1-p)]}
    \label{eq:limit_depth}
\end{eqnarray}
for some constant $C$. For $p=0.3\%$, this evaluates to $D < 230$ in the thermodynamic limit. For more general types of noise, we similarly expect the scaling with $q^2$ to hold up to some constant circuit depth in the thermodynamic limit. The noisy survival probability at this constant circuit depth will, however, decay exponentially when increasing the system size such that exponentially many measurements are required to resolve the signal. Nevertheless, we will show below that the number of measurements remains experimentally feasible in superconducting quantum devices for moderately sized systems with realistic error rates.

Two situations where Eq.~(\ref{eq:rescale}) fails directly follow from our argument. One is the case when $q$ approaches $r$, as already discussed. The other is when the initial state does not thermalize. For example, the product state $\ket{Z+} = \ket{0}^{\otimes N}$ is invariant under the (Floquet) XY Hamiltonian and thus will not get entangled. However, even in this case Eq.~(\ref{eq:rescale}) works well for many practical channels because two independent errors are unlikely to cancel each other.

    
\subsection{Numerical results}
\begin{figure}
    \centering
    \xincludegraphics[width=0.499\textwidth,trim={1.2cm 0.2cm 1.cm 0.cm},clip,label=(a)]{figs/03a_circuit.pdf}
    \\\ \\
    \xincludegraphics[width=0.25\textwidth, label=(b)]{figs/03b_TELoschmidt_alphaPi8.0_angle8_dep0.003_scaling.pdf}
    \xincludegraphics[width=0.2\textwidth, label=(c)]{figs/03c_TELoschmidt_alphaPi8.0_angle8_dep0.003_XX_scaling.pdf}
    \caption{\textbf{(a)} Quantum circuit to map the expectation value of a (unitary) observable onto a survival probability. The initial state is prepared with $V$, $U= U_1, U_2, U_3$ or $ U_4$ is a single step in the Trotter decomposition, and $\mathcal{N}$ denotes a local noise channel.
    \textbf{(b)}, \textbf{(c)} Dependence of $L_{\id}^{\mathcal{N}_p}(t)$ and $L_{A}^{\mathcal{N}_p}(t)$ on the circuit depth $D$ and system size $N$ in the presence of depolarizing noise with error probability $p = 0.3\%$. The initial state is $\ket{\psi} = \ket{X+}$. The observable $A = 4S^x_{i}S_{i+1}^x$ is a correlator in the center of the lattice. The black dashed lines represent the scaling predicted by Eq.~(\ref{eq:scaling}).
    }
    \label{fig:scaling}
\end{figure}


We now numerically verify these considerations for the Floquet evolution of the XY model described in Sec.~\ref{sec:xy} in the presence of local depolarizing noise. For each qubit, the noise channel is given by
\begin{eqnarray}
    \mathcal{N}_{p}(\rho) = (1 - p)\rho + \sum_{\mu = 1}^{3} \frac{p}{3} \sigma^{\mu} \rho \sigma^{\mu}.
\end{eqnarray}
Other types of noise are discussed in the Appendix~\ref{sec:supp_plot}. In Fig.~\ref{fig:scaling}b and c, we respectively show $L_{\id}^{\mathcal{N}_p}(t)$ and  $L_{A}^{\mathcal{N}_p}(t)$ for the initial state $\ket{\psi} = \ket{X+}$ for different system sizes. The computations were performed using the Monte Carlo wavefunction method with the Cirq library~\cite{cirq_developers_2022_6599601}. Each data point in the figure corresponds to an average over 2000 quantum trajectories. This number of trajectories is sufficient to observe convergence of the mean value in the region of our interest. The results agree well with Eq.~(\ref{eq:scaling}). This also holds for different types of noise as we show in Appendix~\ref{sec:supp_plot}. We note that the data points start to deviate from the estimated black dashed lines at $ND$ approximately linear in $N$, in line with the expectation from Eq.~(\ref{eq:limit_depth}).


\begin{figure}
    \centering
    \xincludegraphics[width=0.235\textwidth, label=(a)]{figs/04a_TELoschmidt_alphaPi8.0_angle8_dep0.003_XX_scaling_s_runs2000_cutoff0.010.pdf}
    \xincludegraphics[width=0.235\textwidth, label=(b)]{figs/04b_TELoschmidt_alphaPi8.0_angle8_cutoff0.010_new_depolarizing.pdf}
    \caption{\textbf{(a)} The mitigation error $s_A^{\mathcal{N}_p}$ for the range of data in Fig.~\ref{fig:scaling} where $L_{\id}^{\mathcal{N}_p}(t) > 0.01$. We choose this cutoff due to the limited number of trajectories in the simulation, which limits the significant digits. \textbf{(b)} The root-mean-square of $s$, $\sqrt{\sum_{\mathrm{data}} s^2 / \sum_{\mathrm{data}}}$, evaluated over the window of circuit depth $[D - 16, D + 16]$ in \textbf{(a)}.}
    \label{fig:error_scaling}
\end{figure}

To quantify the error of the mitigation strategy, we define 
\begin{eqnarray}
    s_{A}^{\mathcal{N}_p}(t) = L_{A}^{\mathcal{N}_p}(t) \big/  L_{\id}^{\mathcal{N}_p}(t) - L_{A}(t).
\end{eqnarray}
Figure~\ref{fig:error_scaling}a shows the distribution of $s$ of the mitigated data from Fig.~\ref{fig:scaling}. The error remains small for depths up to $D \approx 100$. To compare different noise rates, we plot in Fig.~\ref{fig:error_scaling}b the square root of the moving average of $s^2$ for different values of $p$. Similar plots for types of noise other than depolarizing noise are presented in Appendix~\ref{sec:supp_plot}. For reference, the typical value of $L_{A}(t)$ in the simulation is around $0.3$, which indicates that for circuit depth $D=80$, the relative error is around 10\% for $p = 0.3\%$.

Although these results confirm the effectiveness of our error mitigation strategy, we also observe a systematic shift of $s$ towards positive values. This can be explained by the error terms in Eq.~(\ref{eq:expansion_sp}). Let us assume for simplicity that $\tilde{\rho} = \id / 2^N$, from which it follows that 
\begin{eqnarray}
    \begin{aligned}
    \frac{L_{A}^{\mathcal{N}_p}(t)}{L_{\id}^{\mathcal{N}_p}(t)} & = \frac{q^2 L_{A}(t) + (1 - q^2)/2^N}{q^2 + (1 - q^2)/2^N},
    \end{aligned}
\end{eqnarray}
where we used the fact that $A^2 = \id$ since $A$ is hermitian and unitary. Hence,
\begin{eqnarray}
    s_{A}^{\mathcal{N}_p}(t) = \left[1 - L_{A}(t)\right] \frac{(1 - q^2)}{q^2\cdot 2^N + (1 - q^2)} > 0.
    \label{eq:error_evaluation}
\end{eqnarray}
For certain error models, it may be possible to remove this systematic error by using a more complicated rescaling formula instead of \eqref{eq:rescale}. Nevertheless, the systematic error remains small as long as $q^2 \gg \tr(\tilde \rho^2)$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.38\textwidth]{figs/05_Spec_XX_Lx4_Ly4_alphaPi8.0_new_p0.003.pdf}
    \caption{The time average of $L_{\sigma_i^x \sigma_{i+1}^x, \psi}(t)$ at $J t \approx 7.85$ with $\omega = 8J$, corresponding to 10 Trotter steps. The black crosses show the noiseless result. The red points were obtained by applying our error mitigation strategy to noisy simulations with a single-qubit depolarization rate $p = 0.3\%$. Error bars indicate the statistic errors due to fluctuations of different Monte Carlo trajectories, propagated from the standard deviations of $L_{A}^{\mathcal{N}_p}(t)$ and $L_{\id}^{\mathcal{N}_p}(t)$. The system size $N = 4 \times 4$.}
    \label{fig:full_exp} 
\end{figure}

We will now argue that our mitigation strategy enables the observation of prethermalization on current and near-term quantum devices. After Trotterization, the total required circuit depth $D$ to simulate time evolution of the two-dimensional XY model up to time $t_{\max}$ is
\begin{eqnarray}
	D = 4 \cdot 2 \cdot t_{\max} / \tau,
\end{eqnarray}
which, from left to right, represents the number of layers per Trotter step, back and forward evolution, and the number of Trotter steps. To see prethermalization of the Floquet XY model, Fig.~\ref{fig:prethermal} indicates that $t_{\max}$ should be at least $8 / J$ for $\omega = 8J$, which yields $D \approx 80$. The estimation is within the limit of the maximum circuit depth from Eq.~(\ref{eq:limit_depth}) and Fig.~\ref{fig:error_scaling} for $p =0.3\%$, showing that our proposal is suitable for current and near-term quantum devices.

We have now gathered all the ingredients for the full simulation of the PEVP on a noisy quantum device. We consider the two-dimensional XY model on a $4 \times 4$ square lattice in the presence of depolarizing noise with noise rate $p = 0.3 \%$. For the observable, we focus on the correlator $A = 4S_i^x S_{i+1}^x$ of a pair of neighboring sites at the center of the lattice.  In Fig.~\ref{fig:full_exp}, we plot the time averages of $\langle A(t) \rangle^2$ at driving frequency $\omega = 8J$ as a function of the initial state energy $E$ up to $t = 10 \tau$, corresponding to circuit depth $D=80$. The initial states were chosen from the same set as in Fig.~\ref{fig:prethermal}b. The black crosses represent the noise-free results, whereas for the red points the experiment was simulated including noise and error mitigation. The error bars show statistical errors due to fluctuations of different Monte Carlo trajectories, propagated from the standard deviations of $L_{A}^{\mathcal{N}_p}(t)$ and $L_{\id}^{\mathcal{N}_p}(t)$. Note that the sign of $\braket{\psi | A (t) | \psi }$ turns out to be constant during the Floquet time evolution in our range of simulations. In the long-time limit, the time average of the square is therefore equivalent to the square of the time average, given that they converge to a constant.

We find that the noise-free results lie within the error bars for all initial states and that the trend of the observable is well reproduced. This shows that our error mitigation procedure is viable to solve the PEVP. We note that the deviation between the noisy and noise-free results is biased since the red points are systematically above the black crosses, consistent with the expectation from Eq.~(\ref{eq:error_evaluation}).


\subsection{Implementation\label{sec:implementation}}

The results of the previous section show that our error mitigation strategy enables the solution of the PEVP for the XY model at a depolarizing noise rate of $p = 0.3 \%$. One more step remains to assess the experimental viability: an estimate of the number of required measurements.

In experiments, the survival probabilities are estimated from binary outcomes (success / failure). This gives rise to shot noise, which in turn sets a lower bound on the necessary number of samples. To achieve a statistical uncertainty of $\epsilon$, roughly $1/\epsilon^2$ samples are needed. For the error mitigation scheme to work, the shot noise must be smaller than the survival probability. As the noisy survival probability is suppressed by the factor $(1-p)^{ND}$, it follows that the number of needed measurements scales as $(1-p)^{-2ND}$. We note that this number of samples is typically orders of magnitude larger than the number needed to suppress the fluctuations in Monte Carlo trajectories due to noisy dynamics.

Since the sample complexity scales exponentially with the number of qubits, this is an important limitation to the system size that can realistically be reached. Nevertheless, classically hard regimes are accessible with realistic parameters. For instance, setting $N = 50$ while keeping $p = 0.3 \%$ and $D = 80$, we find that $(1-p)^{-2ND} \approx 3 \times 10^{10}$ samples are needed. This is inconveniently large as current superconducting quantum devices can collect millions of samples on the time scale of minutes. However, a modest improvement in the error rate to $p = 0.2\%$ reduces the number of samples to a much more realistic value of $9 \times 10^6$.

We have so far neglected the role of measurement errors, which occur with probability $p_{m} \approx 1 \% - 2\%$ for each single qubit measurement in current devices~\cite{Satzinger2021, Wei2022a}. Fortunately, these errors are automatically remedied by our error mitigation strategy. The measurement errors simply suppress the survival probability by another factor $(1-p_{m})^N$, which is independent of the circuit depth. For system sizes up to $N=50$, this increases the required number of measurements by at most an order of magnitude.