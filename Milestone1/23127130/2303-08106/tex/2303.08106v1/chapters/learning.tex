\section{DG Methods: Learning Paradigms}\label{sec:learning-paradigms}
\subsection{Meta-learning}\label{subsec:meta-learning}
Meta-learning \cite{DBLP:journals/air/HuismanRP21}, also known as learning-to-learn is a research area that has attracted much interest in recent years. The main goal of meta-learning is to learn a general model using samples from multiple tasks to quickly adapt to new unseen tasks. The learned meta-model encompasses the general knowledge from all the different training tasks which makes it a better model initialization to adapt for new tasks \cite{DBLP:conf/icml/FinnAL17}. 

Traditional supervised learning (SL) methods learn a model $f_\theta$ that maps inputs to outputs. The model's parameters $\theta$ are learned by minimizing a loss function given a dataset $\mathcal{D} = \{(x_i,y_i)\}_{i=1}^{m}$ as follows:
\begin{align*}
    \theta^*_{\textrm{SL}} = \argmin_\theta \mathcal{L}(\mathcal{D}, \theta)
\end{align*}
At each iteration, the parameters are updated based on a specific optimization procedure $g_\omega$ where $\omega$ denotes all the pre-defined assumptions about the learning algorithm such as the function class of $f$ (e.g., DNN), the initial model initialization, the choice of the optimizer, etc. In the literature, $\omega$ is also called \emph{pre-defined meta-knowledge} \cite{DBLP:journals/pami/HospedalesAMS22}. It is straightforward to observe that the model's performance depends drastically on $\omega$. 

In addition, it is common to split the dataset $\mathcal{D}$ into training and testing sets. The model is first learned using the training samples, and the generalization of the model is subsequently evaluated on the test set with unseen samples and known outputs. Consequently, the learned parameters $\theta_{\textrm{SL}}$ are specific to the dataset $\mathcal{D}$ and are not guaranteed to generalize to samples different from the ones in $\mathcal{D}$. 

Different from the supervised learning setting, meta-learning aims to learn a meta-knowledge $\omega$ over a distribution of tasks $p(\mathcal{T})$. A task $i$ can be defined by a loss function and a dataset (i.e., $\mathcal{T}_i=\{\mathcal{L}_i, \mathcal{D}_i\}$). Learning the meta-knowledge from multiple tasks enables the quick learning of new tasks from $p(\mathcal{T})$. In meta-learning, different choices of the meta-knowledge $\omega$ are proposed such as parameter initialization, optimizer, hyperparameters, task-loss functions, etc. We refer the interested reader to \cite{DBLP:journals/pami/HospedalesAMS22} for a detailed discussion about the different choices for $\omega$.

Meta-learning algorithms also involve two stages, namely meta-training followed by meta-testing. The objective of meta-training is to learn the ``best'' meta-knowledge $\omega$ across multiple tasks. To do so, a set of training tasks $\mathcal{T}_{\textrm{train}} \sim p(\mathcal{T})$ is used where each task $i$ has training and validation datasets (i.e., $\mathcal{D}_i=\{\mathcal{D}_i^{\textrm{train}}, \mathcal{D}_i^{\textrm{val}}\}$). The meta-training phase is commonly presented as a bi-level optimization problem \cite{DBLP:journals/pami/HospedalesAMS22} as follows:
\begin{align}
   \omega^{*} &= \overbrace{\mathop{\argmin}\limits_{\omega} \sum_{i=1}^{|\mathcal{T}_{\textrm{train}}|} 
    \mathcal{L}\left(\theta_i^*(\omega), \mathcal{D}_i^{\textrm{val}}\right) }^{\text{outer level}}, \label{eq:outer-level}\\
    \text{s.t. } & \underbrace{\theta_i^*(\omega) = \argmin_\theta \mathcal{L}_i\left(\mathcal{D}^{\textrm{train}}_i, \theta,\omega\right)}_{\text{inner level}} \label{eq:inner-level}.
\end{align}

The inner level consists in learning task-specific learners conditioned on the meta-knowledge $\omega$. Note that the inner level only optimizes the task-specific parameters $\theta$ using the task train datasets $\mathcal{D}^{\textrm{train}}_i$ and does not change $\omega$. Whilst, the outer level learns $\omega$ that minimizes the aggregated losses from all the train tasks on their validation datasets. 

In the literature, it is common to divide meta-learning methods into three families: optimization-based, model-based, and metric-based. Optimization-based methods, promoted by the Model Agnostic Meta-Learning (MAML) algorithm \cite{DBLP:conf/icml/FinnAL17} have been recently adopted for domain generalization. The general idea is to consider the different domains as different tasks. Hence, data from multiple source domains are divided into meta-training and meta-testing sets. By training with data from different domains, the meta-learner is exposed to domain shift and is required to learn a meta-knowledge that quickly adapts to domain shift in new unseen domains \cite{DBLP:conf/aaai/LiYSH18}.

\subsection{Self-Supervised Learning}
Self-supervised learning (SSL) is a learning paradigm that generates labels from data and subsequently uses these labels as ground truth. SSL is useful in real-world applications where abundant unlabeled data is available, especially when the labeling process is cumbersome and expensive. Another motivation behind SSL is to learn rich and general representations, unlike supervised learning methods that learn biased representations via the supervision signal or the type of annotations \cite{DBLP:conf/cvpr/LiV19a}. In supervised learning, labels serve as the supervision signal to learn a specific task. However, in SSL, a model is learned using the data as a supervision signal. In other words, the labels in SSL are generated from the data itself. The SSL pipeline can be divided into two parts: 
\begin{itemize}
    \item learn feature representations by solving a \emph{pretext} task. An example of a pretext task is to retain part of the input data to be predicted by a model that is trained on the other part of the data \cite{MaskedAutoencoders2021}. Another pretext task consists in learning the relationship between data instances (e.g., similarity) or reconstruct an input from its shuffled parts (also known as the jigsaw puzzle). Note that the labels (or supervision signal) for the pretext task is generated from the input data, thus no human intervention is needed;
    \item solve a downstream task using the learned representations and a few annotated data.
\end{itemize}

SSL is applied in DG to learn domain-invariant features that help in avoiding overfitting on domain-specific biases while aligning features from different source domains. As discussed in Section \ref{subsec:data-invariant-representation-learning}, these invariant features can be leveraged in unseen target domains to achieve better generalization \cite{DBLP:conf/icml/MuandetBS13}. In this context, contrastive learning is a well-known SSL method that aims to learn latent representations such that positive instances are close and negative samples are pushed away. Therefore, in the learned embedding space, the distance between similar instances is reduced while the distance between negative pairs is increased. For instance, the authors in \cite{selfReg2021} proposed two self-supervised contrastive losses to measure feature dissimilarities in the embedding space. For dissimilarities across domains, the authors used a Mix-up layer \cite{mixup, DBLP:conf/icassp/WangLK20} (i.e., a convex combination of samples' embeddings from different domains) to compute the interpolated feature representation across domains. Thus, the regularisation loss is defined as the distance between the individual representations and the interpolated one using the mix-up layer. One caveat of this method is that it assumes the label space does not change for all the domains.

\subsection{Ensemble Learning}
Ensemble learning \cite{Zhou2012EnsembleMF} is a famous technique in traditional and modern machine learning where multiple models are learned and combined for prediction/classification. The same idea was also exploited for DG. The most straightforward approach is to learn a model for each source domain and average the individual predictions to compute the final ensemble prediction \cite{DBLP:journals/tip/ZhouYQX21, DBLP:conf/dagm/DInnocenteC18}. Instead of learning separate models for each source domain, it is common to design the ensemble as a shared feature extractor and different domain-specific heads \cite{DBLP:journals/tip/ZhouYQX21}. Another line of work focuses on the weighting of the individual models' predictions. For instance, the domain-specific models can be weighted differently depending on the similarity of the target domain to the source domain. The authors in \cite{DBLP:conf/icip/ManciniBC018} proposed to learn a domain predictor that predicts the probability that a target sample belongs to a source domain. These probabilities can be used to fuse the models' predictions at test time.

An alternative solution proposes to train domain-invariant classifiers for each source domain by learning domain-specific normalization \cite{DBLP:conf/eccv/SeoSKKHH20, DBLP:journals/pr/SeguTT23}. All the classifiers share the same parameters except the ones in the normalization layers. The objective of learning domain-specific normalization is to obtain domain-agnostic latent feature space that can be used to map samples from unknown domains to the source domains. This idea is related to the feature alignment methods reviewed in Section \ref{subsubsec:explicit-feature-alignment}.

Alternatively, the stochastic weight averaging (SWA) method \cite{SWA} aggregates weights at different training epochs to form an ensemble model instead of combining the predictions of multiple learners. Starting from a pre-trained model, SWA trains a single model using a cyclic learning rate schedule (or a constant high learning rate) and saves model snapshots corresponding to different local minima. Averaging these points leads to better solutions in flatter regions of the loss landscape. Intuitively, flatter minima are more robust than sharp minima to changes in the loss landscape between the training and testing datasets \cite{SWA}. Consequently, this weight averaging idea was extended to the DG proving that flat minima lead to better generalization on unseen domains \cite{SWAD}. 

\subsection{Hypernetwork-Based Learning}
Hypertnetwork-based learning \cite{hypernetworks} is an approach that learns a network (i.e. the hypernetwork) to generate weights for another network called the main network. The latter represents the usual model that maps raw data to their targets or labels. The goal of the hypernetwork is to generate a specific set of weights depending on inputs about the structure of the weights or tasks. Different from the usual supervised learning setting, only the hypernetwork's parameters are learned during training whilst keeping the main network's parameters unchanged. At inference, the main network is evaluated based on the weights generated by the hypernetwork.  

Recent work proposed hypernetwork-based algorithms for DG in natural language processing \cite{DBLP:journals/corr/abs-2203-14276} and vision \cite{hmoe}. For vanilla DG, a straightforward application of hypernetworks is to train a hypernetwork on data samples from different source domains to produce the model's weights for each domain. On the other hand, for compound DG, the appropriate approach is to first learn a latent embedding space for the different domains, then the hypernetwork learns to map the latent features to a set of weights so as to compute model predictions. 

In the next sections, we will overview the different applications of the techniques detailed above to wireless communication problems.  
