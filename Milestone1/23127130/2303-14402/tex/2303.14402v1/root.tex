%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Comments a

%   Reformulate Sec II in DT
%   Restructure Sec III possibly?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command
\pdfminorversion=4
\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{subcaption}

\usepackage{xcolor}
%\usepackage{framed}
%\usepackage{enumitem} 
\usepackage{epsfig} % for
%font selection scheme installed
\usepackage{amsmath, bm}
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{arydshln}
\usepackage{amsfonts}
%postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
%\usepackage{arydshln}
%\usepackage{amsfonts}
%\usepackage{newtxtext, newtxmath}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\TR}[1]{{\color{magenta}#1}}

\usepackage[noadjust]{cite}
\title{\LARGE \bf
Exploring the use of deep learning in task-flexible ILC*
}


\author{Anantha Sai Hariharan Vinjarapu, Yorick Broens, Hans Butler, {and Roland T\'{o}th}
\thanks{*This work has received funding from the ECSEL Joint Undertaking under grant agreement No 875999 and from the European Union within the framework of the National Laboratory for Autonomous Systems (RRF-2.3.1-21.2022-00002).}% <-this % stops a space
\thanks{A.S.H.Vinjarapu, Y.Broens, R.T\'oth {and H.Butler} are with the Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands. R.T\'oth is also affiliated with the Systems and Control Laboratory, Institute for Computer Science and Control, Hungary. {H.Butler is also affiliated with ASML, Veldhoven, The Netherlands}. Email: {\tt\small y.l.c.broens@tue.nl}.}
}

% <-this % stops a space
% <-this % stops a space
\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
%Feedforward control plays a crucial role in motion systems when accurate position tracking is desired. In the scenario of high precision positioning systems where repetitive motion is expected in the performed tasks, Iterative Learning Control (ILC) is a suitable choice for feedforward compensation, since it is able to achieve superior performance compared to classical model based feedforward alone. In this paper, we aim to explore the use of deep learning to address the task flexibility constraint of conventional ILC. For this purpose, two controllers based on deep learning are developed, each assuming a different perspective of the problem. Among these, a Task Analogy based Imitation Learning (TAIL)-ILC approach for addressing the problem is proposed in this work as one of the two controllers while the second approach is inspired from a previous work. A simulation study is subsequently performed on a single axis of a 6 degree of freedom magnetic levitation system to validate and compare the performances of combinations of the two controllers with classical model based feedforward strategy. Additionally, a broader level comparison of the perspectives assumed by the two controllers is made to provide a more general reasoning of the differences in certain aspects of the performances.     

Growing demands in today's industry results in increasingly stringent performance and throughput specifications. For accurate positioning of high-precision motion systems, feedforward control plays a crucial role. Nonetheless, conventional model-based feedforward approaches are no longer sufficient to satisfy the challenging performance requirements. An attractive method for systems with repetitive motion tasks is iterative learning control (ILC) due to its superior performance. However, for systems with non-repetitive motion tasks, ILC is {generally} not applicable, {despite of some recent promising advances}. In this paper, we aim to explore the use of deep learning to address the task flexibility constraint of ILC. For this purpose, a novel Task Analogy based Imitation Learning (TAIL)-ILC approach is developed. To benchmark the performance of the proposed approach, a
simulation study is presented which compares the TAIL-ILC to classical model-based feedforward strategies and existing learning-based approaches, such as neural network based feedforward learning.

%Feedforward control plays a crucial role in accurate positioning of high-precision motion systems. For systems where repetitive motion profiles are expected, Iterative Learning Control (ILC) is an attractive method for feedforward control due to its superior performance compared to conventional model-based feedforward design approaches. However, for systems with non-repetitive motion trajectories, conventional ILC is not a viable option.

%In this paper, we aim to explore the use of deep learning to address the task flexibility constraint of conventional ILC. For this purpose, a novel Task Analogy based Imitation Learning (TAIL)-ILC approach is developed. To benchmark the performance of the proposed approach, a
%simulation study is presented which compares the TAIL-ILC to conventional model-based feedforward strategies and existing learning-based approaches, such as Neural Network (NN) based feedforward learning.

\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Introduction-under construction}
%High-precision positioning systems are essential components in modern manufacturing machines and scientific equipment, see \cite{Butler,411117,HEERTJES20161, 6225187}. To achieve high-throughput and high-accuracy position tracking, a two-degree-of-freedom controller structure comprising a feedback controller and a feedforward controller is typically employed, see \cite{1223174,Oomen,190Steinbuch}. While the feedback controller ensures closed-loop stability and disturbance rejection, the feedforward controller is responsible for over 99 $\%$ of the position tracking performance. Due to the increasingly stringent specifications in industry, conventional model-based feedforward approaches, see \cite{oomen2020model} , are no longer sufficient.

%to meet the increasingly stringent performance specifications required in today's industry.

%Iterative Learning Control (ILC) is a suitable feedforward control method for motion systems that perform repetitive tasks. However, ILC has limitations, including its assumption that the tracking error is repetitive from one iteration to the next and its restriction to a single task. To address these issues, machine learning and system identification techniques have been employed, but they often sacrifice local ILC performance for task flexibility.

%In this paper, we propose a novel approach called Task Analogy based Imitation Learning (TAIL)-ILC, which uses imitation learning to mimic the behavior of conventional ILC policies over multiple trajectories. We view the problem from a data science perspective and learn the spatial feature analogies of the trajectories and corresponding control signals. Abstract lower dimensional representations of the signals are used for efficient training of the network, providing advantages in training and prediction time efficiencies, handling of high sampling rates, and the amount of data that can be used for training. The resulting feedforward controller is a cascade interconnection of an encoding policy, a learning policy, and a decoding policy. Dual principal component analysis is employed for the encoding and decoding policies, while a deep neural network is used for the learning policy.
\section{Introduction}
\label{intro}
%High-precision positioning systems play a crucial role in manufacturing machines and state-of-the-art scientific equipment. The typical high performance requirements in such motion systems involve high accelerations and small settling times which mandate the use of a two degree of freedom controller structure consisting of a feedback controller and a feedforward controller \cite{model based ffd for motion systems}. A typical example of a positioning system is a wafer stage used in the lithography industry for the production of silicon wafers. A wafer stage primarily performs repetitive motion to appropriately position the silicon wafers in an x-y grid for printing. Because of the nature of patterns being printed on the wafers, these systems are required to deliver sub-nanometer accurate positioning while operating at extremely high accelerations. Other examples of such positioning systems include wire bonders and desktop printers \cite{amcprecisionmech.}. In the scenario of such industrial systems, assuming that the tracking error is repetitive from one iteration to the next, Iterative Learning Control (ILC) is a suitable candidate for feedforward control \cite{a survey of ilc} for achieving close to perfect position tracking. However, ILC has some important diadvantages: 
%\begin{enumerate}
%	\item The tracking error being addressed by ILC is assumed to be repeating from one iteration to the next, limiting the scope of applicability of ILC. 
%	\item The performance of conventional ILC is restricted to a single task. 
%\end{enumerate}
%Several attempts exist in literature addressing the task flexibility issue of ILC using ideas from different concepts such as machine learning and system identification, for example \cite{rational basis functions in ilc},\cite{bt-ilc},\cite{nn-ilc}. However, an analysis of the results presented in the corresponding literature of these variants shows a trend in terms of performance of the ILC variant for any individual task relative to the deviation of the corresponding variant from the fundamental idea of conventional ILC. This trend is summarised in Figure \ref{perfvsdesign}.
%\begin{}[b]
%    \centering
%    \includegraphics[width=250pt]{Pics/perf vs ILC design.png}
%    \caption{Performance of the ILC variant vs deviation from the fundamental ILC idea. In this, DD-ILC using RL \cite{dd-ilc using rl}, AC-ILC \cite{ac-ilc}, Conventional ILC \cite{ilc for industrial systems},FF parameterization ILC \cite{b2b feedforward parameterized ilc}, ILC for varying tasks \cite{ilc for varying tasks}}
%    \label{perfvsdesign}
%\end{figure}
%\indent Here, the fundamental idea of conventional ILC is considered as the direct iterative manipulation of signals (control input signals or error signals) rather than applying the iterative learning law to a reframed problem. Figure \ref{perfvsdesign} shows that the greater the deviation between the considered ILC variant and the conventional ILC design, the poorer is the performance. In the light of this observation, we would like to have a task flexible ILC variant where the ILC design does not deviate from the conventional design procedures. In other words, it could be more desirable to treat the problem of task flexibility independently of the ILC design inorder to maintain the superior tracking performance for multiple tasks. Such an ILC variant can be imagined to make use of imitation learning in order to mimic the behaviour of a conventional ILC for multiple trajectories. \\                
%\indent In this paper, we explore the use of deep learning models in obtaining such imitation learning based task-flexible ILC variants. First, we treat the problem from a data science perspective and propose an idea to imitate the performance of a conventional ILC by learning the spatial feature analogies of the trajectories and the corresponding control signals generated by the ILC. We call this approach as Task Analogy based Imitation Learning (TAIL)-ILC. We make use of abstract lower dimensional representations of the trajectory and control signal features for efficient learning of a mapping between the two. The use of abstract representations provides multiple advantages in terms of (i) training and prediction time efficiencies (ii) the amount of training data that could be used and (iii) dealing with high sampling rates. The resulting controller is a cascade of an encoding policy, a learning policy and a decoding policy. Here, we use a standard linear dimensionality reduction technique called dual principal component analysis (DPCA) for the encoding and decoding policies and a deep neural network for the learning policy. \\
%\indent Next, we consider a perspective similar to that of data-driven system modelling within the context of deep learning. Here, we train a deep neural network to learn a sample-wise mapping between the reference trajectories and the feedforward control signals. This perspective is primarily inspired from a previous work, \cite{nn-ilc} and is called as Neural Network (NN)-ILC. Both the controllers are trained to imitate the performance of a conventionally designed ILC which has access to process dynamics, for a set of reference trajectories. We subsequently demonstrate the performances of the trained controllers for compensation of residual dynamics in a single axis of a magnetic levitation system in simulation. The performances of the developed controllers are compared with that of a classical model based feedforward controller.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Revised introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
High-precision positioning systems are essential components in modern manufacturing machines and scientific equipment, see \cite{Butler,411117,HEERTJES20161, 6225187}. To ensure high-throughput and high-accuracy position tracking, a two-degree-of-freedom controller structure, consisting of a feedback controller and a feedforward controller, is commonly utilized, see \cite{1223174,Oomen,190Steinbuch}.  The feedback controller maintains closed-loop stability and disturbance rejection, while the feedforward controller is primarily responsible for achieving optimal position tracking performance, see \cite{articleHH}.
%Typically, these type of systems are characterized by two key features: \emph{high-throughput} and \emph{high-accuracy}, thus mandating the use of a two degrees of freedom controller structure consisting of a feedback controller and a feedforward controller, see \cite{1223174,Oomen}. Conventionally, the feedback controller provides disturbance rejection and ensures closed-loop stability while the feedforward controller is responsible for over  99\% of the position tracking performance, see \cite{articleHH}.
Nonetheless, with the increasingly stringent demands in contemporary industry, conventional model-based feedforward techniques, e.g. \cite{oomen2020model}, are no longer adequate to meet the desired performance specifications, thus necessitating for alternative feedforward approaches.

\emph{Iterative Learning Control} (ILC), see \cite{ahn2007iterative}, has emerged as a viable choice for feedforward control in motion systems that execute recurring tasks, enabling accurate position tracking. Despite its advantages, ILC exhibits significant limitations. Primarily, ILC is dependent on the assumption that the tracking error recurs from one iteration to the next, limiting its general applicability. Additionally, conventional ILC performance is constrained to a single task, see \cite{BLANKEN2016213}. 

%For motion systems that perform repetitive tasks, \emph{Iterative Learning Control} (ILC), see \cite{ahn2007iterative}, is a suitable candidate for feedforward control as it allows for close to perfect position tracking. Nonetheless, ILC has some important drawbacks. First, the tracking error being addressed by ILC is assumed to be repetitive from one iteration to the next, limiting the scope of applicability of ILC. Secondly, the performance of conventional ILC is restricted to a single task, see \cite{BLANKEN2016213}.

%In the literature, several attempts have been made to address the task flexibility issue of ILC using ideas from machine learning and system identification, see \cite{HulstMSc,5559384,Bosma-msc}. Nonetheless, by analyzing the results presented in the corresponding literature (see \cite{HulstMSc,5559384,Bosma-msc}) it is observed that there is a trade-off between  attainable position tracking performance and the deviation between the fundamental idea of ILC, i.e. direct iterative manipulation of signals. Rather than sacrificing local ILC performance for task flexibility, we aim at developing a learning-based feedforward approach which is able to archive position tracking performance independently of the severeness of the variation of the compensatory signal over the tasks.


Several studies have attempted to address the task flexibility limitations of ILC by drawing on concepts from machine learning and system identification, as reported in the literature \cite{HulstMSc,5559384,Bosma-msc}. However, the findings from the related literature suggest that there exists a trade-off between the achievable position tracking performance and the degree of deviation from the core principle of ILC, i.e., direct iterative manipulation of signals. Instead of compromising local ILC performance to enhance task flexibility, the aim is to develop a learning-based feedforward strategy that can deliver superior position tracking performance regardless of the severity of the variation of the compensatory signal across tasks.
%In the light of this observation,
%we would like to have a task flexible ILC variant where the ILC design does not deviate from the conventional design procedures. In other words, 
%it could be more desirable to treat the problem of task flexibility independently of the ILC design in order to maintain the superior tracking performance.
%for multiple tasks. 
Such an ILC variant can be imagined to make use of imitation learning in order to mimic the behaviour of conventional ILC policies generalized over multiple trajectories. 

This paper introduces a novel approach to ILC, termed Task Analogy based Imitation Learning (TAIL)-ILC, from a data science perspective. By acquiring spatial feature analogies of the trajectories and their corresponding control signals, performance of conventional ILC policies  can be replicated. To facilitate efficient network training, abstract lower-dimensional representations of signals are utilized. This approach offers numerous benefits in terms of training and prediction time efficiency, utilization of large datasets, and high sampling rate handling. The resulting feedforward controller comprises an encoding policy, a learning policy, and a decoding policy arranged in a cascade interconnection. Dual principal component analysis (DPCA), a standard linear dimensionality reduction technique, is utilized for the integration of the encoding and decoding policies, while a deep neural network is employed for the learning policy.

%This paper presents a novel \emph{Task Analogy based Imitation Learning} (TAIL)-ILC approach by viewing the problem from a data science based perspective. By learning the spatial feature analogies of the trajectories and the corresponding control signals, the performance of the conventional ILC policies can be imitated. For efficient training of the network, abstract lower dimensional representations of the signals are employed. The use of abstract representations provides multiple advantageous properties in terms of training and prediction time efficiencies, the amount of data that can be used for training, and the handling of high sampling rates. The resulting feedforward controller is a cascade interconnection of an encoding policy, a learning policy and a decoding policy. For integration of the feedforward controller, a standard linear dimensionality reduction technique called \emph{dual principal component analysis} (DPCA) is employed for the encoding and decoding policies while for the learning policy, a deep neural network is employed.
 
 The main contributions of this paper are:
 \vspace*{-1mm}
 \begin{itemize}
     \item [(C1)] {A novel TAIL-ILC approach that tackles the task extension problem of ILC via learning spatial feature analogies of trajectories and their {compensation} %corresponding control 
     signals, enabling direct imitation of %conventional 
     ILC policies.} 
     %A novel Task Analogy based Imitation Learning ILC approach which views the task extension problem of ILC from a data science perspective. By this method, the conventional ILC policies are directly imitated by learning the spatial feature analogies of the trajectories in combination with the corresponding control signals.
     \item [(C2)] 
     {An efficient implementation strategy is devised for the learning-based feedforward controller in terms of constructing it through the cascade interconnection of an encoder, a deep neural network, and a decoder.}
     %A computationally efficient implementation strategy for the corresponding learning-based feedforward controller, which is constructed from the cascade interconnection of an encoder, a deep neural network and a decoder.
 \end{itemize}
  \vspace*{-1mm}
 This paper is organized as follows. First, the problem formulation is presented in Section \ref{Section_Problem_formulation}. Next, Section \ref{Section_Conventional_Feedforward} 
 presents the proposed novel TAIL-ILC approach which aims  at generalizing ILC performance across various tasks through imitation learning strategies.  Section \ref{Section_SimulationStudy} provides a simulation study of the proposed approach with respect to existing feedforward strategies using a high-fidelity model of a moving-magnet planar actuator. In Section \ref{TAILPERSPE}, detailed comparison between the proposed TAIL-ILC approach and neural-network-based feedforward strategies is presented. Finally, conclusions on the proposed approach are presented in Section \ref{Section_Conclusions}.
 
 \section{Problem statement}
 \label{Section_Problem_formulation}
 \subsection{Background}
 \label{PF}

 Consider the conventional frequency domain ILC configuration illustrated by Figure \ref{fig:ILC_interconnection}, where $P\in \mathcal{R}^{n_\mathrm{y} \times n_\mathrm{u}}$ corresponds to the proper {transfer matrix representation} of a \emph{discrete time} (DT) \emph{linear-time-invariant} (LTI) 
\emph{multiple-input multiple-output} (MIMO) plant {with $\mathcal{R}$ denoting the set of real rational functions in the complex variable $z\in\mathbb{C}$.} Furthermore, the proper $K\in \mathcal{R}^{n_\mathrm{u} \times n_\mathrm{y}}$ represents a LTI stabilizing DT feedback controller, which is typically constructed using rigid-body decoupling strategies, see \cite{Steinbuch2013}.
The aim of the conventional frequency domain ILC framework is to construct an optimal feedforward policy $f$, which minimizes the position tracking error $e$ in the presence of the motion trajectory $r$. Under the assumption that the reference trajectory is trial invariant, the error propagation per trial $k \in \mathbb{N}_{\geq 0}$ is given by:
\vspace*{-.2cm}
 \begin{equation}
     e_k = Sr-Jf_k,
          \label{ILC_Formula_1}
 \end{equation}
 
 \vspace*{-.1cm}

 \noindent where $S=(I+PK)^{-1}$ and $J=(I+PK)^{-1}P$. Generally, the update law for the feedforward policy is in accordance with the procedure outlined in \cite{blanken2019multivariable}:
 \vspace*{-.2cm}
 \begin{equation}
     f_{k+1} = Q\left(Le_k+f_k \right),
     \label{ILC_Formula_2}
 \end{equation}
 
 \vspace*{-.1cm}
 
 \noindent where $L\in \mathcal{RL}_{\infty}^{n_\mathrm{u} \times n_\mathrm{y}} $ is a learning filter and $Q \in \mathcal{RL}_\infty^{n_\mathrm{u} \times n_\mathrm{u}}$ denotes a robustness filter {with $\mathcal{RL}_\infty$ corresponding to set of real rational functions in $z$ that have bounded singular value on the unit circle $\mathbb{D}=\{ e^{\mathrm{i} \omega} \mid \omega \in [0,2\pi] \}$, i.e., finite $\mathcal{L}_\infty (\mathbb{D} )$ norm.} Both $L$ and $Q$ are required to be designed for the ILC task at hand. Furthermore, by combining (\ref{ILC_Formula_1}) and (\ref{ILC_Formula_2}), the progression of the error and feedforward update is reformulated as: 
 \vspace*{-.2cm}
 \begin{subequations}  \label{ILC_Formula_3}
 \begin{align}
     e_{k+1} &= (I-JQJ^{-1} )Sr + JQ( J^{-1} -L ) e_k,\\ 
     f_{k+1} &= QLSr + Q(I-LJ)f_k,
     \end{align} 
     \end{subequations}
     
     \vspace*{-.1cm}

 \noindent which can be reduced to: 
 \vspace*{-.2cm}
 \begin{subequations}  \label{ILC_Formula_4}
 \begin{align}
     e_{k+1} &= (I-Q)Sr+Q(I-JL)e_k, \\
      f_{k+1} &= QLSr + Q(I-LJ)f_k,
  \end{align} 
     \end{subequations}
 
 \vspace*{-.1cm}
 \noindent under the assumption that $Q$ is diagonal and $J$ is approximately diagonal, which holds in case of rigid-body decoupled systems. 
 
 From (\ref{ILC_Formula_4}), several observations can be made. First, it can be observed that the contribution of $r$ to the position tracking error is dependent on the robustness filter $Q$, which is optimally chosen as identity to negate the contribution of the reference trajectory towards the tracking error. Secondly, learning filter $L$ aims to minimize the criterion $\|Q(I-JL) \|_\infty <1$, {where $\|\centerdot\|_\infty$ stands for the $\mathcal{H}_\infty$ norm,} such that the tracking error is steered to zero, which is optimally achieved when $L=J^{-1}$. Note that these assumptions on $Q$ and $L$ yield the optimal feedforward update $f_{k+1} = P^{-1}r$, which results in perfect position tracking. Moreover, when the convergence criterion is satisfied, the limit policies, i.e. $e_\infty = \lim_{k \rightarrow \infty }e_k$, $f_\infty = \lim_{k \rightarrow \infty }f_k$, correspond to:  \vspace*{-.2cm}
 \begin{subequations}  \label{ILC_Formula_5}
 \begin{align}
         e_\infty &= \bigl(I-J\bigl(I-Q(I-LJ)\bigr)^{-1}QL\bigr)Sr, \\
         f_\infty &= \left(I-Q(I-LJ)\right)^{-1}QLSr,
\end{align}
     \end{subequations}
 \vspace*{-.1cm}

 
 
 
%Consider the conventional ILC control interconnection illustrated in Figure \ref{fig:ILC_interconnection}, where $P$ corresponds to a \emph{linear time-invariant} (LTI) mechanical system, which can be expressed in state-space form as:
%\begin{equation}
%    P:=
%    \begin{cases}
%    \begin{split}
%        \dot{x}(t) &= Ax(t) + Bu(t)\\
%        y(t) &= Cx(t) + Du(t)
%    \end{split}
%    \end{cases},
%\end{equation}

%\noindent 
%where $x:\mathbb{R}\rightarrow \mathbb{X}\subseteq\mathbb{R}^{n_\mathrm{x}}$ corresponds to the state vector, $u:\mathbb{R}\rightarrow \mathbb{U}\subseteq\mathbb{R}^{n_\mathrm{u}}$ is the control input and $y:\mathbb{R}\rightarrow \mathbb{Y}\subseteq\mathbb{R}^{n_\mathrm{y}}$ is the plant output. $K$ denotes a dynamic feedback controller, which is of form:
%\begin{equation}
%    K:=
%    \begin{cases}
%    \begin{split}
%        \dot{x}_k(t) &= A_kx_k(t) + B_ke(t)\\
%        u(t) &= C_kx_k(t) + D_ke(t)
%    \end{split}
%    \end{cases},
%\end{equation}

%\noindent 
%where $x_k\in \mathbb{R}^{n_{x_k}}$ is the state vector of the controller and $e\in \mathbb{R}^{n_\mathrm{e}}$ denotes the position tracking error. Furthermore, $L$ corresponds to a learning filter and $Q$ denotes a robustness filter. The main idea of conventional ILC is to construct a feedforward signal $f_{k+1}$ in an iterative manner, such that the effects of \emph{residual dynamics} are completely removed from the tracking error signal, thus allowing for maximal performance with respect to position tracking in case of repetitive motion profiles. Moreover, using the ILC interconnection illustrated by Figure \ref{fig:ILC_interconnection}, the following expression is obtained for the position tracking error per iteration $k$:
%\begin{equation}
%    e_k = \left(I+PK \right)^{-1}r_k - %\left(I+PK \right)^{-1}P f_{k+1} 
%    \label{ILC_\mathrm{e}xpression_1}
%\end{equation}

%\noindent Additionally, the feedforward update rule $f_{k+1}$ is given by:
%\begin{equation}
%    f_{k+1} = Q\left(f_k+Le_k \right)
%    \label{ILC_\mathrm{e}xpression_2}
%\end{equation}

%\noindent By using a small-gain type argument for convergence analysis, for which it is assumed that $r = 0$ and all initial conditions are zero, see \cite{STEINBUCH2000899}, the progression of the error dynamics is reformulated as:
%\begin{equation}
%    e_{k+1} = P_\mathrm{S}\cdot Q\cdot \left(P_\mathrm{S}^{-1}-L \right)e_k,
%        \label{ILC_\mathrm{e}xpression_3}
%\end{equation}

%\noindent where $P_\mathrm{S} =\left(I+PK \right)^{-1}P $. In industrial applications, motion control design is typically simplified by application of rigid body decoupling strategies, see \cite{190Steinbuch,BLANKEN2016213}, thus allowing for individual control design of the mechanical \emph{degrees of freedom} (DoF), since the dynamics as seen by the controller, are diagonalized in the frequency spectrum of interest. In the light of this observation, (\ref{ILC_\mathrm{e}xpression_3}) is reformulated, such that the tracking error is reduced in the frequency spectrum of interest, resulting in the following progression of the error signal:
%\begin{equation}
%    e_{k+1} = Q\left(I-L\cdot P_\mathrm{S} \right)e_k
%    \label{ILC_\mathrm{e}xpression_4}
%\end{equation}

%\noindent Moreover,  the criterion $\big| \big| Q\left[I-L\cdot P_\mathrm{S} \right]\big| \big|_\infty < 1$ results in minimization of the error, while enforcing stability. Note that the criterion is minimized  if $L \approx P_\mathrm{S}^{-1}$. The robustness filter $Q$ is typically chosen with the shape of a low-pass filter to focus on the specific frequency range of interest for which the position tracking error must be minimized. Additionally, $Q$ can be shaped in a desirable manner to account for model uncertainty and channel interaction of the particular MIMO system. When the convergence criterion is satisfied, the limit policy $f^*$ is given by:

%%\begin{equation}
 %   f^* = \left(I-Q\left(I-LP_\mathrm{S} \right) \right)^{-1} QL\mathrm{S}r, 
 %   \label{fixedptff}
%\end{equation}

%\noindent where $\mathrm{S}=\left(I+PK \right)^{-1}$. The corresponding optimal error is given by:
%\begin{equation}
%    e^* = \left(I-P_\mathrm{S}\left(I-Q\left(I-LP_\mathrm{S} \right) \right)^{-1}QL \right)\mathrm{S}r
%\end{equation}

In spite of its simplicity and efficacy, the conventional ILC is hindered by significant limitations, the most notable of which is its confinement to a single task. Consequently, its practical utility is restricted to particular types of machinery. %To overcome this limitation, we have adopted an imitation learning based feedforward approach that emulates the policies of the conventional ILC feedforward for a generalized set of motion trajectories. This strategy enhances the task-flexibility of the feedforward controller, while simultaneously elevating the closed-loop performance beyond that offered by current methods, including the model-based and neural network based feedforward approaches.

 \begin{figure}[t]
    \centering
    \includegraphics[trim={0cm 0cm 0cm 0cm},width=.7\linewidth]{Pics/ILC_Figure_1_ext.pdf} \vspace{-3mm}
     \caption{Control structure with the conventional ILC configuration.}
     \label{fig:ILC_interconnection} \vspace{-10mm}
 \end{figure}

%\begin{figure}[b]
%    \centering
 %   \includegraphics[width=250]{Pics/Position Reference trajectories sample.png}
 %   \caption{Example of motion trajectories that belong to a single class.}
 %   \label{example trajectories in a class}
%\end{figure}
 \subsection{Problem formulation}
 %The problem that is being addressed in this paper is to extend the task-flexibility of conventional ILC by employing an imitation learning based controller which generalizes the optimal feedforward policy, constructed by conventional ILC, for a generalization of motion profiles. The objective of this paper is to design a learning-based feedforward controller, such that the following requirements are satisfied:
 The aim of this paper is to address the challenge of augmenting the task-flexibility of the conventional ILC by utilizing an imitation learning based controller. This approach facilitates the generalization of the optimal feedforward policy, created by the conventional ILC, for a wider range of motion profiles. The primary objective of this paper is to devise a feedforward controller that employs a learning-based mechanism, which satisfies the following requirements:
 \begin{itemize}
     \item [(R1)] The learning-based feedforward approach enables the generalization of the performance of the conventional ILC across multiple trajectories.
     \item [(R2)] 
     The scalability of the learning-based feedforward approach is imperative for its implementation in systems with a high sampling rate.
     %The learning-based feedforward approach should be scalable to systems with high sampling rate. 
 \end{itemize}

\section{TAIL-ILC}
\label{Section_Conventional_Feedforward}
\vspace*{-.2cm}
\subsection{Approach}
\label{3A_approach}
For a given dynamic system with a proper discrete transfer function $G\in\mathcal{R}$ under a sampling time $T_\mathrm{s} \in \mathbb{R}_{+}$, 
a reference trajectory $r$ of duration $T=n_\mathrm{d} T_\mathrm{s}$ seconds can be defined as %$r(t) \in \mathbb{R} \ ,t \in \mathbb{R}_{+}$ such that $t \in [0,T]$, whereas a sampled version of $r$ becomes:
\vspace*{-.3cm}
\begin{equation}
r = \begin{bmatrix}r(0) & \cdots & r(n_\mathrm{d}) \end{bmatrix}^\top ,
\end{equation}

\vspace*{-.1cm}
\noindent where $n_\mathrm{d}$ corresponds to the length of the signal in DT. This reference trajectory for example can correspond to a $n^{th}$ order motion profile. A trajectory class $C \subset  \mathbb{R}^{n_\mathrm{d} \times n_\mathrm{t}}$ is defined as a collection of reference trajectories such that each trajectory shares certain prominent spatial features (motion profile order, constant velocity interval length, etc.) with the others, where $n_\mathrm{t}$ is the number of trajectories:
\vspace*{-.2cm}
\begin{equation}
C = \{r_1,r_2,r_3 . . . . . ,r_{n_\mathrm{t}}\}.
\end{equation}

\vspace*{-.2cm}

%\noindent Figure \ref{example trajectories in a class} illustrates four example trajectories that belong to the same class according to this definition where all the trajectories are closed polygons with four edges and four corners in the x-y plane as the common spatial features.\\
\noindent 
%\textbf{Definition 1:} An $n^{th}$ order motion profile for a reference trajectory $r_i \sim C$ is defined as,
%
%\begin{equation}
%    r_\ast: \mathbb{R}_+ \rightarrow \mathbb{R} \text{ and } r(k)=r_\ast(kT_\mathrm{s}), k\in[0,n_\mathrm{d}]
%\end{equation}

		%$$u_i(t) = [u_{0i}(t),u_{1i}(t),u_{2i}(t),u_{3i}(t),.,.,u_{ni}(t)]^{\top}$$
%where, $u_{ni}(t) = \frac{d^nr_i(t)}{dt^n} \in \mathbb{R}_{+} \ \forall\ n$. Similar to the reference trajectories, the sampled versions become $u_{ni} = [u_{ni}(0),u_{ni}(1),.,.,.,u_{ni}(n_\mathrm{d} - 1)]^{\top}\ \forall\ n \in \mathbb{N}$. 
 Given a specific combination of the $L$ and $Q$ filters, consider that an ILC policy $\pi^*$ exists which maps a given reference trajectory $r$ to the optimal feedforward compensation signal $f^*$, see  \eqref{ILC_Formula_5}. This can be formally expressed as:
 \vspace*{-.4cm}
\begin{equation} \pi^*: r_i \rightarrow f^*_i. \end{equation}

\vspace*{-.1cm}



%From here on, $\pi^*$ would be referred to as the expert policy which has access to the learning and robustness filters designed based on a process model. Here we aim to develop an optimal student policy $\pi_\mathrm{s}^*$ that has a performance close to the optimal policy $\pi^*$ for a range of trajectories belonging to the considered trajectory class. Mathematically put, the goal is find $\pi_\mathrm{s}^*$ as a solution to the following optimiation problem:
\noindent 
Henceforth, $\pi^*$ shall be denoted as the expert policy, which is equipped with learning and robustness filters established through a process model. Our objective is to formulate an optimal student policy $\pi_\mathrm{s}^*$ that approximates the performance of the optimal policy $\pi^*$ over a set of trajectories from the pertinent trajectory class. To this end, we endeavor to determine $\pi_\mathrm{s}^*$ as a solution to the optimization problem:
\vspace*{-.1cm}
\begin{equation}
\label{mes}
\pi_\mathrm{s}^* = \underset{\pi_\mathrm{s}}{\arg\min}\ \eta(\pi^*(r_i),\pi_\mathrm{s}(r_i)),\quad \forall i \in [1,n_\mathrm{t}]
\end{equation}

\vspace*{-.2cm}

\begin{figure}[t]
\vspace*{7pt}
    \centering
    \includegraphics[trim={1.5cm 0cm 3.5cm 0cm}    ,width = \linewidth]{Pics/ACC.drawio.pdf} 
    \vspace*{-.6cm}
    \caption{Offline TAIL-ILC with $r_i,f_i \ \in \mathbb{R}^{n_\mathrm{d} \times 1}$.}
    \label{taililc} 
    \vspace*{-5mm}
\end{figure}

\noindent where $r_i \sim C$ and $\eta(\cdot,\cdot)$ is a performance quantification measure, and $\pi_\mathrm{s}$ are parameterized student policy candidates. The expert policy $\pi^*$ is a conventionally designed frequency domain ILC as described in Section \ref{PF}. In TAIL-ILC, the idea is to structure $ \pi_\mathrm{s}$ as :
\vspace*{-.2cm}
\begin{equation}
\label{pis}
    \pi_\mathrm{s} =  \pi_\mathrm{D,Y} \circ  \pi_\mathrm{C} \circ  \pi_\mathrm{E,U}.
\end{equation}

\vspace*{-.1cm}

\noindent
which is visualised in Figure \ref{taililc}. %shows a schematic of the TAIL-ILC controller. 
The TAIL-ILC controller is capable of generating a feedforward control signal based on a given reference trajectory. This process is carried out through a series of three sub-policies outlined in equation \eqref{pis}. The first sub-policy, $\pi_\mathrm{E,U}$, projects the reference trajectory $r_i \in \mathbb{R}^{n_\mathrm{d} \times 1}$ into a lower-dimensional space referred to as the \textit{latent space}. Next, the second sub-policy, $\pi_\mathrm{C}$, predicts a latent space representation of the feedforward signal, which is then fed into the third sub-policy, $\pi_\mathrm{D,Y}$, to project the latent space feedforward signal back into the higher-dimensional output space, resulting in $f_i \in \mathbb{R}^{n_\mathrm{d} \times 1}$. Notably, the successful application of TAIL-ILC requires that all reference trajectories share certain spatial features with each other. The prediction sub-policy, $\pi_\mathrm{C}$, is trained on a set of reference trajectories and their corresponding feedforward control signals obtained using $\pi^*$, which are projected into the latent space. The use of abstract representations enables the preservation of the most significant information of the signals while simultaneously reducing the amount of data used for making predictions, resulting in several advantages, such as increased training and prediction time efficiencies. The subsequent sub-section will delve into the development of each sub-policy in further detail.

%As can be seen in this, the TAIL-ILC controller takes a reference trajectory as input and produces a corresponding feedforward control signal. Within the TAIL-ILC controller, the input reference trajectory goes through three stages which are shown as three sub-policies in \eqref{pis}. First, the reference trajectory $r_i \in \mathbb{R}^{n_\mathrm{d} \times 1}$ is projected into a lower-dimensional space which is referred to as the \textit{latent space} using the sub-policy $ \pi_\mathrm{E,U}$. Following this, the sub-policy $ \pi_\mathrm{C}$ predicts a latent space representation of the feedforward signal. Finally, the output of $ \pi_\mathrm{C}$ is fed to the sub-policy $ \pi_\mathrm{D,Y}$ which projects the latent space feedforward signal into the higher dimensional space outputting $f_i \in \mathbb{R}^{n_\mathrm{d} \times 1}$. A key requirement for using TAIL-ILC is that all the reference trajectories share certain prominent spatial features with each other. Here, the prediction sub-policy $ \pi_\mathrm{C}$ is a learning model trained on a set of reference trajectories (training inputs) and the corresponding feedforward control signals (target outputs) obtained using $\pi^*$, \textit{projected} into the latent space. The use of abstract representations allows us to preserve the most prominent information of the signals while being able to reduce the total amount of data used for making predictions. This feature provides  several important advantages in terms of training and prediction time efficiencies. The next sub-section focuses on the construction of the individual sub-policies.   
\vspace*{-.1cm}
\subsection{Student policy $\pi_\mathrm{s}$}
The three-part student policy $\pi_\mathrm{s} : r \rightarrow f_{\pi_\mathrm{s}}$ can be decomposed into three distinct components:
\vspace{-3mm}
\begin{subequations}  
 \begin{align}
	     \pi_\mathrm{E,U} &: r \rightarrow r_l\\
	 \pi_\mathrm{C} &: r_l \rightarrow f_l\\
	 \pi_\mathrm{D,Y} &: f_l \rightarrow f_{\pi_\mathrm{s}}
  \end{align} 
     \end{subequations}
     
     \vspace*{-.1cm} \noindent
where, $r,f_{\pi_\mathrm{s}} \in \mathbb{R}^{n_\mathrm{d} \times 1}$ and $r_l,f_l \in \mathbb{R}^{n_l \times 1}$ and $n_l$ is the latent space dimensionality such that $n_l \ll n_\mathrm{d}$. As mentioned in Section \ref{3A_approach}, the training data for the sub-policy $ \pi_\mathrm{C}$, namely the pairs $\{r_{i,l},f_{i,l}\}$, are in the latent space.
\noindent This shows that the ideal outputs of $\pi_\mathrm{s}$ are of the form:
\vspace*{-.2cm}
\begin{equation}
\label{idealop}
f_{\pi_\mathrm{s}} =  \pi_\mathrm{D,Y}(f_l) = f' \approx f^*,
\end{equation}

\vspace*{-.1cm}
\noindent 
where, an approximation error may exist between $f'$ and $f^*$. Additionally, we aim at:
\vspace*{-.2cm}
\begin{equation}
 \pi_\mathrm{C}(r_l) = \widehat{f_l} \approx f_l,
\end{equation}

\vspace*{-.1cm}
\noindent 
where, in case of using a deep neural network, $\widehat{f_l}$ is the output of the network and the prediction error $e_\mathrm{pred}$ is defined as:
\vspace*{-.2cm}
\begin{equation}
	e_\mathrm{pred} = \|f_l - \widehat{f_l}\|_2,
	\end{equation}
	
	\vspace*{-.1cm}

\noindent where, $\|\cdot\|_2$ denotes the $\ell_2$ norm. Moreover, this implies that \eqref{idealop} becomes:
\vspace*{-.1cm}
\begin{equation}
	f_{\pi_\mathrm{s}} =  \pi_\mathrm{D,Y}(\widehat{f_l}) = \widehat{f'}.
\end{equation}

\vspace*{-.1cm}

\noindent In order to quantify the gap between performance of $\pi^*$ and that of $\pi_\mathrm{s}$, a distance measure is used as the performance quantification measure $\eta$ in  \eqref{mes}. This is expressed as:
\vspace*{-.1cm}
\begin{equation}
\label{newmetric}
\eta(\pi^*,\pi_\mathrm{s}) = \frac{1}{n_\mathrm{t}}\sum_{i = 1}^{n_\mathrm{t}}\|f_i - \widehat{f'_i}\|_2.
\end{equation}

\vspace*{-.1cm}

\noindent Assuming that $\mu$ represents the set of weights and biases of the deep neural network, improving the performance of $\pi_\mathrm{s}$ can be posed as the following optimization problem:
\vspace*{-.1cm}
\begin{equation}
\label{optimizationprob}
\underset{n_l,\mu}{\arg\min} \ \eta(\pi^*,\pi_\mathrm{s})
\end{equation}

\vspace*{-.1cm}

\noindent %Here, the idea would be to back propagate the value of $\eta$ through all the three sub-policies thus optimizing $n_l,\mu$ together iteratively by solving  \eqref{optimizationprob}. However, it is desirable to have a simpler alternative or a reformulation of this problem in view of the computational cost involved. To this end, the following definitions of \emph{Expert space} and \emph{Student space} are introduced. 

The proposed approach involves propagating the parameter $\eta$ through the three sub-policies, with the aim of iteratively optimizing both $n_l$ and $\mu$ via \eqref{optimizationprob}. However, given the significant computational burden associated with this approach, there is a need for a more straightforward alternative or a reformulation of the problem. With this goal in mind, we introduce the concepts of the \emph{Expert space} and \emph{Student space} to provide alternative perspectives for addressing the optimization problem at hand.

\begin{definition}
%\noindent \textbf{Definition 2:} 
The expert space is defined as the space of all real policies denoted by superscript$\ ^\mathrm{e} $ having the form
\vspace*{-.2cm}
$$\pi^\mathrm{e}:\mathbb{R}^{n_\mathrm{x} \times 1} \rightarrow \mathbb{R}^{n_\mathrm{d} \times 1}\quad  \forall n_\mathrm{x} \in \mathbb{N}$$
\end{definition}

\noindent 
\textbf{Example:}
\begin{enumerate}
	\item Expert policy in expert space:
	\vspace*{-.2cm}
\begin{equation}\pi_\mathrm{e}^\mathrm{e}:r \rightarrow f'\end{equation}

\vspace*{-.2cm}
\noindent 
where $r,f' \in \mathbb{R}^{n_\mathrm{d} \times 1}$
	\item Student policy in expert space:
	\vspace*{-.2cm}
\begin{equation}\pi_\mathrm{s}^\mathrm{e}:r \rightarrow \widehat{f'}\end{equation} 

\vspace*{-.2cm}
\noindent 
where $r,\widehat{f'} \in \mathbb{R}^{n_\mathrm{d} \times 1}$
\end{enumerate}
\begin{definition} %\noindent \textbf{Definition 3:} 
The student space is defined as the space of all real policies denoted by superscript $(\ ^\mathrm{s})$ having the form
\vspace*{-.2cm}
$$\pi^\mathrm{s}:\mathbb{R}^{n_\mathrm{x} \times 1} \rightarrow \mathbb{R}^{n_l \times 1}\ \forall \ n_\mathrm{x} \in \mathbb{N}$$
\end{definition}
\textbf{Example:}
\begin{enumerate}
	\item Expert policy in student space:
	\vspace*{-.2cm}
\begin{equation}\pi_\mathrm{e}^\mathrm{s}:r_l \rightarrow f_l\end{equation}

\vspace*{-.2cm}
\noindent 
where $r_l,f_l \in \mathbb{R}^{n_l \times 1}$
	\item Student policy in student space:
	\vspace*{-.2cm}
\begin{equation}\pi_\mathrm{s}^\mathrm{s}:r_l \rightarrow \widehat{f_l}\end{equation}

\vspace*{-.2cm}
\noindent 
where $r_l,\widehat{f_l} \in \mathbb{R}^{n_l \times 1}$
\end{enumerate}

\noindent Table \ref{policies} summarizes these definitions.

%\begin{remark} %\noindent \textbf{Remark 1:} 
Stated differently, the expert space is comprised of all the decoding policies, $\pi_\mathrm{D}$, which project signals into $n_\mathrm{d}$ dimensions, while the student space is composed of all the encoding policies, $\pi_\mathrm{E}$, which project signals into $n_l$ dimensions.
%In other words, the expert space consists of all the decoding policies ($\pi_\mathrm{D}$, projecting signals to $n_\mathrm{d}$ dimensions) and the student space consists of all the encoding policies ($\pi_\mathrm{E}$, projecting signals to $n_l$ dimensions).
%\end{remark}
\begin{table}[t]
\vspace{4.5pt}
\centering
    \caption{Expert and student policies in expert and student spaces}
	\begin{tabular}{|c|c|c|}
		\hline
	        &\textbf{Expert space $(^\mathrm{e})$} & \textbf{Student space $(^\mathrm{s})$}\\
		\hline
		\hline
		\textbf{Expert policy $(\pi_\mathrm{e})$} & $\pi_\mathrm{e}^\mathrm{e}:r \rightarrow f'$ & $\pi_\mathrm{e}^\mathrm{s}:r_l \rightarrow f_l$ \\
		\textbf{Student policy $(\pi_\mathrm{s})$} & $\pi_\mathrm{s}^\mathrm{e}:r \rightarrow \widehat{f'}$ & $\pi_\mathrm{s}^\mathrm{s}:r_l \rightarrow \widehat{f_l}$ \\
		\hline
	\end{tabular}
\label{policies} \vspace{-3mm}
\end{table}
Based on the preceding definitions, it is worth noting that our primary objective is to determine the student policy in the expert space, $\pi_\mathrm{s}^\mathrm{e}$. In light of these definitions, the distance metric specified in \eqref{newmetric} can be reformulated as:
%From the above definitions, note that we are essentially interested in finding the student policy in expert space, $\pi_\mathrm{s}^\mathrm{e}$. In the light of these definitions, the distance measure in \eqref{newmetric} can be redefined as follows:
\vspace*{-.2cm}
\begin{equation}
\label{modifiedupperbound}
\eta(\pi^*,\pi_\mathrm{s}^\mathrm{e}) = \eta(\pi^*,\pi_\mathrm{e}^\mathrm{e}) + \eta(\pi_\mathrm{e}^\mathrm{e},\pi_\mathrm{s}^\mathrm{e})
\end{equation}
\begin{equation*}
\label{modifiedmetric1}
\implies	\eta(\pi^{*},\pi_\mathrm{s}^\mathrm{e}) = \frac{1}{n_\mathrm{t}}\sum_{i = 1}^{n_\mathrm{t}}\|f_i - f'_i\|_2 + \frac{1}{n_\mathrm{t}}\sum_{i = 1}^{n_\mathrm{t}}\|f'_i - \widehat{f'_i}\|_2
\end{equation*}

\vspace*{-.1cm}

\noindent where, $\eta(\pi^*,\pi_\mathrm{e}^\mathrm{e})$ corresponds to the optimization of $n_l$ and $\eta(\pi_\mathrm{e}^\mathrm{e},\pi_\mathrm{s}^\mathrm{e})$ corresponds to the optimization of $\mu$. This separation of the distance measure \eqref{newmetric} allows the optimization problem in \eqref{optimizationprob} to be segmented as:
\vspace*{-.1cm}
\begin{equation*}
	\underset{n_l,\mu}{\arg\min}\ \eta(\pi^*,\pi_\mathrm{s}^\mathrm{e}) = \underset{n_l}{\arg\min}\  \eta(\pi^*,\pi_\mathrm{e}^\mathrm{e}) + \underset{\mu}{\arg\min}\ \eta(\pi_\mathrm{e}^\mathrm{e},\pi_\mathrm{s}^\mathrm{e})
\end{equation*}

\vspace*{-.1cm}

\noindent This segregation allows us to optimize $n_l$ independently of $\mu$, thus simplifying the optimization problem defined by \eqref{optimizationprob}.
\subsection{Choice of encoding and decoding sub-policies}
%For the encoding and decoding sub-policies, here we use DPCA which is a standard linear dimensionality reduction technique. This choice was made due of the simplicity of the computations involved. Other popular linear and non-linear dimensionality reduction techniques can be found in \cite{osti_15002155}.\\

The encoding and decoding sub-policies in this work employ DPCA, a well-established linear dimensionality reduction technique, due to its computational simplicity. Other commonly-used linear and non-linear dimensionality reduction methods are also available and have been reviewed in \cite{osti_15002155}. 
DPCA involves the identification of a linear subspace with $n_l$ dimensions in an $n_\mathrm{d}$ dimensional space, where $n_l$ is significantly smaller than $n_\mathrm{d}$. This subspace is defined by a set of orthonormal bases that maximize the variance of the original data when projected onto this subspace. The orthonormal bases computed through this process are commonly referred to as \emph{principal components}.
%In DPCA, the aim is to find a linear $n_l$ dimensional subspace in an $n_\mathrm{d}$ dimensional space ($n_l \ll n_\mathrm{d}$) which is computed as a set of orthonormal bases that maximize the variance of the original data when projected onto this subspace. The computed orthonormal bases are referred to as \emph{principal components}. %Consider a dataset $H \in \mathbb{R}^{n_\mathrm{d} \times n_\mathrm{t}}$
\begin{definition}%\noindent \textbf{Definition 3:} 
A \emph{data point} in an arbitrary dataset $H \in \mathbb{R}^{n_\mathrm{x} \times n_\mathrm{t}}$ is defined as a vector $r_i \in \mathbb{R}^{n_\mathrm{x} \times 1}$ $\forall i \in [1,n_\mathrm{t}]$.
\end{definition}

The selection of the principal components for an $n_l$ dimensional latent space for the data points in $C$ involves choosing the right eigenvectors that correspond to the first $n_l$ singular values of $H$. It should be emphasized that the projection of a data point onto the latent space can be computed through the following method:
%The principal components of an $n_l$ dimensional latent space for the data points in $C$ are chosen as the right eigen vectors corresponding to the first $n_l$ singular values of $H$. In this context, the projection of a data point into the latent space can be obtained as:
\vspace*{-.2cm}
\begin{equation}
\label{fe}
	r_l = T_\mathrm{E}  r,
\end{equation}

\vspace*{-.3cm}

\noindent where:

\vspace*{-.3cm}
\begin{equation}
\label{te}
T_\mathrm{E} = \widehat{\Sigma}^{-1} V^\top H^\top 
\end{equation}

\vspace*{-.1cm}

\noindent In this context, $r_l \in \mathbb{R}^{n_l \times 1}$, $V \in \mathbb{R}^{n_\mathrm{t} \times n_\mathrm{t}}$ denotes the matrix of right eigenvectors of $H$ and $\widehat{\Sigma} \in \mathbb{R}^{n_l \times n_\mathrm{t}}$ contains the first $n_l$ singular values of $H$ along its diagonal elements. It is worth noting that the value of $n_l$ is constrained by the number of data points in $H$. This feature of DPCA is particularly advantageous in situations where $n_{\mathrm{d}} >> n_{\mathrm{t}}$. Given the latent space representation $r_l$, a reconstructed data point $r'$ can be obtained as:
%Here $r_l \in \mathbb{R}^{n_l \times 1}$, $V \in \mathbb{R}^{n_\mathrm{t} \times n_\mathrm{t}}$ represents the matrix of right eigen vectors of $H$ and $\widehat{\Sigma} \in \mathbb{R}^{n_l \times n_\mathrm{t}}$ contains the first $n_l$ singular values of $H$ on its diagonal elements. The value of $n_l$ is upper-bounded by the number of data points in $H$. This feature of DPCA is particularly attractive in cases where $n_\mathrm{d} \gg n_\mathrm{t}$. A reconstruction $r'$ of the original data point $r$ from the latent space representation $r_l$ can then be obtained as:
\vspace*{-.1cm}
\begin{equation}
\label{fd}
	r' = T_D  r_l, \qquad r' \in \mathbb{R}^{n_\mathrm{d} \times 1},
\end{equation} 

\vspace*{-.1cm}

\noindent 
where:
\vspace*{-.1cm}
\begin{equation}
\label{td}
T_\mathrm{D} = HV\widehat{\Sigma}^{-1}.
\end{equation}

\vspace*{-.1cm}

%\noindent 
%\textbf{Remark 2:} 
\begin{remark} \label{rem:2}
The computation of transformations $T_\mathrm{E}$ and $T_\mathrm{D}$ depend on $n_l$. Additionally, considering that we have access to the dataset $H$, the matrix transformations $\widehat{\Sigma}^{-1} V^TH^T$ and $HV\widehat{\Sigma}^{-1}$, the right hand side of \eqref{fe} and \eqref{fd}, become constant for a specific problem for a given choice of $n_l$. 
\end{remark}

In the light of Remark \ref{rem:2}, for a given dataset $H \in \mathbb{R}^{n_\mathrm{d} \times n_\mathrm{t}}$ the encoding ($ \pi_\mathrm{E,U}$) and decoding ($ \pi_\mathrm{D,Y}$) sub-policies for using in the student policy $\pi_\mathrm{s}$ can be defined as follows:
\vspace*{-.2cm}
\begin{subequations}
\begin{align}
\label{pie}
	 \pi_\mathrm{E,U}(r) &= T_\mathrm{E}r  \\
\label{pid}
	 \pi_\mathrm{D,Y}(\widehat{f_l}) &= T_\mathrm{D} \widehat{f_l} 
  \end{align}
\end{subequations}

\vspace*{-.1cm}

%\subsection{Approach}
%In this section, the task flexibility constraint is addressed from a perspective similar to data-driven system modelling. This is primarily inspired from a previous work, see \cite{Bosma-msc}. Here, the idea is to consider the reference trajectory time samples as points in an n-dimensional space for an $n^{th}$ order motion profile and the corresponding feedforward signal time samples as points in a one-dimensional space. We then train a deep neural network learning policy $\pi_\mathrm{NN}$ in a supervised manner with the reference trajectory time samples as the training inputs and the corresponding feedforward signal time samples as the target outputs.  Here, the target output data for training the learning policy is obtained for a set of reference trajectories using the expert policy $\pi^*$. 
%\subsection{Student policy $\pi_\mathrm{NN}$}
%For a given reference trajectory $r_i \sim C$, the feedforward signal generated by $\pi^*$ corresponds to:
%\begin{equation}
% f^*_i = \pi^*(r_i), \qquad \text{where:} \ \ \ \ \  r_i,f^*_i \ \in \mathbb{R}^{n_\mathrm{d} \times 1}
%\end{equation}
 
%\noindent Furthermore, each trajectory $r \sim C$ is composed of an $n^{th}$-order motion profile according to \emph{Definition 1}. In this scenario, for a given reference trajectory $r \sim C$, the goal of NN-ILC is to obtain a student policy $\pi_\mathrm{NN}$ such that:
%\begin{equation}
%\label{nnilcpolicy}
%\pi_\mathrm{NN}: r(t) \rightarrow f_{\pi_\mathrm{NN}}(t)
%\end{equation}
%\noindent In this context, the idea is to obtain a generalising control policy $\pi_\mathrm{NN}$ such that the mean-squared error (MSE) minimization is framed as follows:
%\begin{equation}
%\label{mseerr}%
%	\underset{\pi_\mathrm{NN}}{\arg\min} \frac{1}{n_\mathrm{t} \times n_\mathrm{d}} \sum_{i = 1}^{n_\mathrm{t}} \sum_{t = 1}^{n_\mathrm{d}} \|f_i^*(t) - \pi_\mathrm{NN}(r_i(t))\|_{2}
%\end{equation}
%\noindent Similar to Eq. \eqref{mes}, this can be alternatively written as, 
%\begin{equation}
 %   \pi_\mathrm{NN} = \underset{\pi_\mathrm{NN}}{\arg\min}\ \eta(f_i^*(t),\pi_\mathrm{NN}(r_i(t)))
%\end{equation}
%\noindent Here, the hypothesis is that with access to sufficient amount of data, the performance of $\pi_\mathrm{NN}$ can be generalised for trajectories outside the $n_\mathrm{t}$ training set trajectories. However, there exist multiple heuristic learning methods that can achieve this objective with varying degrees of accuracy. In \cite{Bosma-msc}, the authors explore two such methods namely genetic programming and neural networks. Here, the use of deep learning models is further considered for $\pi_\mathrm{NN}$. Since the trajectories and feedforward signals are considered per time sample, the resulting controller is implementable in an online manner.

%\section{TAIL-ILC}
%\label{Section_\mathrm{t}AIL_ILC}
%\subsection{Approach}
%In this section, the task flexibility constraint of conventional ILC is addressed primarily from a data science perspective. The idea is to model a three-part student policy $\pi_\mathrm{s}$ 
%\begin{equation}
 %   \pi_\mathrm{s} = \pi_D \circ  \pi_\mathrm{C} \circ \pi_\mathrm{e},
%\end{equation}
%\noindent where $\pi_\mathrm{e}$ is the encoding sub-policy, $ \pi_\mathrm{C}$ corresponds to the learning sub-policy and $\pi_D$ denotes the decoding sub-policy. Here, the sub-policy $ \pi_\mathrm{C}$ is trained in a supervised manner to predict abstract representations of the feedforward control signals for multiple trajectories sharing certain prominent spatial features with each other. These abstract control signal representations are subsequently projected back into the actual signal space using the sub-policy $\pi_D$ for obtaining meaningful control signals for a system. The training and testing inputs for $ \pi_\mathrm{C}$ are also abstract lower dimensional representations of the trajectories. In this case, the input trajectories are first projected into the abstract lower dimensional space, which we call \emph{latent space}, using the sub-policy $\pi_\mathrm{e}$. The inputs to $\pi_\mathrm{e}$ consist of discrete time position domain trajectories $r \in \mathbb{R}^{n_\mathrm{d} \times 1}$. The outputs from $\pi_D$ are the discrete time-domain control signals of the form $f \in \mathbb{R}^{n_\mathrm{d} \times 1}$ where $n_\mathrm{d} = \frac{T}{T_\mathrm{s}}$ is the total number of samples. Because of the way the trajectories and signals are considered, the final TAIL-ILC controller is implementable only offline which is closer to a conventional ILC setting. Figure \ref{taililc} shows a schematic of the TAIL-ILC controller.
%\begin{figure}
    %\centering
    %\includegraphics[width = \linewidth,height = %130]{Pics/tail controller.png}
    %\caption{Offline TAIL-ILC. $r_i,f_i \ \in %\mathbb{R}^{n_\mathrm{d} \times 1}$}
%    \label{taililc}
%\end{figure}
%\indent The control signals obtained by the expert policy are used as the target outputs while the corresponding reference trajectories are used as the training inputs for training the student policy. \\
%\indent We first obtain the training input and output data sets for a set of reference trajectories using $\pi^*$. Then, these are first projected into the latent space to obtain the actual training data for $ \pi_\mathrm{C}$. This sub-policy subsequently learns a mapping between abstraction representations of trajectories and control signals. The use of abstract representations allows us to preserve the most prominent information of the signals while being able to reduce the total amount of data used for making predictions. This feature provides us with several important advantages in terms of the training and prediction time efficiencies.
%\subsection{Student policy $\pi_\mathrm{s}$}
%The three-part student policy $\pi_\mathrm{s}$ can be broken down as follows:
%\begin{equation}
%	\pi_\mathrm{s} : r \rightarrow f_{\pi_\mathrm{s}},
%	\end{equation}
	
%	\noindent
%	such that,
%	\begin{equation}
%	\begin{split}
%	    \pi_\mathrm{e} &: r \rightarrow r_l\\
%	 \pi_\mathrm{C} &: r_l \rightarrow f_l\\
%	\pi_D &: f_l \rightarrow f_{\pi_\mathrm{s}}
%	\end{split}
%	\end{equation}
% where, $r,f_{\pi_\mathrm{s}} \in \mathbb{R}^{n_\mathrm{d} \times 1}$ and $r_l,f_l \in \mathbb{R}^{n_l \times 1}$ and $n_l$ is the latent space dimensionality such that $n_l << n_\mathrm{d}$. Since the training data for the learning model in $ \pi_\mathrm{C}$, namely the pairs $\{r_{i,l},f_{i,l}\}$ are in the latent space, these are obtained as:
%\begin{equation}
%	r_{i,l} = \pi_\mathrm{e}(r_i)
%	\end{equation}
%	\noindent and 
%	\begin{equation}
%	f_{i,l} = \pi_\mathrm{e}(f_i) 
%	\end{equation}
%	\noindent 
%This shows that the ideal outputs of $\pi_\mathrm{s}$ are of the form:
%\begin{equation}
%\label{idealop}
%f_{\pi_\mathrm{s}} = \pi_D(f_l) = \pi_D \circ \pi_\mathrm{e}(f) = f' %\approx f^*
%\end{equation}
%where, an approximation error exists between $f'$ and $f^*$. Additionally, we have that:
%\begin{equation}
% \pi_\mathrm{C}(r_l) = \widehat{f_l} \approx f_l,
%\end{equation}
%where, in case of use of a deep neural network, the prediction error $e_\mathrm{pred}$ is defined as:
%\begin{equation}
%	e_\mathrm{pred} = \|f_l - \widehat{f_l}\|_2,
%	\end{equation}
%\noindent where, $\|\cdot\|_2$ denotes the $L_2$ norm. Moreover, this implies that Eq. \eqref{idealop} becomes:
%\begin{equation}%
%	f_{\pi_\mathrm{s}} = \pi_D(\widehat{f_l}) = \widehat{f'}
%\end{equation}	
%\noindent In order to quantify the gap between performance of $\pi^*$ and that of $\pi_\mathrm{s}$, a distance measure is used as the performance quantification measure $\eta$ in Eq. \eqref{mes}. This is expressed as:
%\begin{equation}
%\label{newmetric}
%\eta(\pi^*,\pi_\mathrm{s}) = \frac{1}{n_\mathrm{t}}\sum_{i = %1}^{n_\mathrm{t}}\|f_i - \widehat{f'_i}\|_2
%\end{equation}
%\noindent Assuming that $\mu$ represents the set of weights and biases of the deep neural network, improving the performance of $\pi_\mathrm{s}$ can be posed as the following optimization problem:
%\begin{equation}
%\label{optimizationprob}
%\underset{n_l,\mu}{\arg\min} \ \eta(\pi^*,\pi_\mathrm{s})
%\end{equation}
%\noindent Here, the idea would be to back propagate the value of $\eta$ through all the three sub-policies thus optimizing $n_l,\mu$ together iteratively by solving Eq. \eqref{optimizationprob}. However, it is desirable to have a simpler alternative or a %reformulation of this problem in view of the computational cost involved. To this end, the following definitions of \emph{Expert space} and \emph{Student space} are introduced. \\
%\noindent \textbf{Definition 1:} The expert space is defined as the space of all real policies denoted by superscript $(\ ^\mathrm{e})$ having the form
%$$\pi^\mathrm{e}:\mathbb{R}^{n_\mathrm{x} \times 1} \rightarrow \mathbb{R}^{n_\mathrm{d} \times 1}\ \forall \ n_\mathrm{x} \in %\mathbb{N}$$
%\textbf{Example:}
%\begin{enumerate}
	%\item Expert policy in expert space: $$\pi_\mathrm{e}^\mathrm{e}:r %\rightarrow f'$$ where $r,f' \in \mathbb{R}^{n_\mathrm{d} %\times 1}$
	%\item Student policy in expert space: $$\pi_\mathrm{s}^\mathrm{e}:r %\rightarrow \widehat{f'}$$ where $r,\widehat{f'} %\in \mathbb{R}^{n_\mathrm{d} \times 1}$
%\end{enumerate}
%\noindent \textbf{Definition 2:} The student space is defined as the space of all real policies denoted by %superscript $(\ ^\mathrm{s})$ having the form
%$$\pi^\mathrm{s}:\mathbb{R}^{n_\mathrm{x} \times 1} \rightarrow \mathbb{R}^{n_l \times 1}\ \forall \ n_\mathrm{x} \in %\mathbb{N}$$
%\textbf{Example:}
%\begin{enumerate}
	%\item Expert policy in student space: %$$\pi_\mathrm{e}^\mathrm{s}:r_l \rightarrow f_l$$ where $r_l,f_l \in %\mathbb{R}^{n_l \times 1}$
	%\item Student policy in student space: %$$\pi_\mathrm{s}^\mathrm{s}:r_l \rightarrow \widehat{f_l}$$ where %$r_l,\widehat{f_l} \in \mathbb{R}^{n_l \times 1}$
%\end{enumerate}
%\noindent \textbf{Remark 1:} In the current context, signals from student space can be projected into the expert space using the mapping $\pi_D$ and the opposite can be achieved using the mapping $\pi_\mathrm{e}$.
%Table \ref{policies} summarizes these definitions:
%\begin{table}[h] 
%\centering
%    \caption{Definitions of expert and student policies in expert and student spaces}
%	\begin{tabular}{|c|c|c|}
%		\hline
%	        &\textbf{Expert space$(^\mathrm{e})$} & \textbf{Student space$(^\mathrm{s})$}\\
%		\hline
%		\hline
%		\textbf{Expert policy $(\pi_\mathrm{e})$} & $\pi_\mathrm{e}^\mathrm{e}:r \rightarrow f'$ & $\pi_\mathrm{e}^\mathrm{s}:r_l \rightarrow f_l$ \\
%		\textbf{Student policy $(\pi_\mathrm{s})$} & $\pi_\mathrm{s}^\mathrm{e}:r \rightarrow \widehat{f'}$ & $\pi_\mathrm{s}^\mathrm{s}:r_l \rightarrow \widehat{f_l}$ \\
%		\hline
%	\end{tabular}
%\label{policies}
%\end{table}\\
%\noindent Note here that the policy $\pi_\mathrm{s}^\mathrm{e}$ is equivalent to the student policy $\pi_\mathrm{s}$ of interest. 
%\noindent In the light of these definitions, the distance measure in Eq. \eqref{newmetric} can be redefined as follows:
%\begin{equation}
%\label{modifiedupperbound}
%\eta(\pi^*,\pi_\mathrm{s}^\mathrm{e}) = \eta(\pi^*,\pi_\mathrm{e}^\mathrm{e}) + \eta(\pi_\mathrm{e}^\mathrm{e},\pi_\mathrm{s}^\mathrm{e})
%\end{equation}
%\begin{equation*}
%\label{modifiedmetric1}
%\implies	\eta(\pi^{*},\pi_\mathrm{s}^\mathrm{e}) = %\frac{1}{n_\mathrm{t}}\sum_{i = 1}^{n_\mathrm{t}}\|f_i - f'_i\|_2 + %\frac{1}{n_\mathrm{t}}\sum_{i = 1}^{n_\mathrm{t}}\|f'_i - %\widehat{f'_i}\|_2
%\end{equation*}
%\noindent where, $\eta(\pi^*,\pi_\mathrm{e}^\mathrm{e})$ corresponds to the optimization of $n_l$ and $\eta(\pi_\mathrm{e}^\mathrm{e},\pi_\mathrm{s}^\mathrm{e})$ corresponds to the optimization of $\mu$. This segmentation of the distance measure Eq. \eqref{newmetric} allows the optimization problem in %Eq. \eqref{optimizationprob} to be segmented as follows:
%\begin{equation*}
%	\underset{n_l,\mu}{\arg\min}\ \eta(\pi^*,\pi_\mathrm{s}^\mathrm{e}) = \underset{n_l}{\arg\min}\  \eta(\pi^*,\pi_\mathrm{e}^\mathrm{e}) + \underset{\mu}{\arg\min}\ \eta(\pi_\mathrm{e}^\mathrm{e},\pi_\mathrm{s}^\mathrm{e})
%\end{equation*}
%\noindent This segregation allows us to optimize $n_l$ independently of $\mu$ thus simplifying the optimization problem defined by Eq. \eqref{optimizationprob}.
%\subsection{Choice of encoding and decoding sub-policies}
%For the encoding and decoding sub-policies, here we use DPCA which is a standard linear dimensionality reduction technique. This choice was made because of the simplicity of the computations involved. Other popular linear and non-linear dimensionality reduction techniques can be found in \cite{osti_15002155}.\\
%\indent In DPCA, the aim is to find a linear $n_l$ dimensional subspace in an $n_\mathrm{d}$ dimensional space ($n_l << n_\mathrm{d}$) which is computed as a set of orthonormal bases that maximize the variance of the original data when projected onto this subspace. The orthonormal bases thus computed are referred to as \emph{principal components}. Consider a dataset $C \in \mathbb{R}^{n_\mathrm{d} \times n_\mathrm{t}}$\\
%\noindent \textbf{Definition 3:} A \emph{data point} in an arbitrary dataset $C \in \mathbb{R}^{n_\mathrm{x} \times n_\mathrm{t}}$ is defined as a vector $r_i \in \mathbb{R}^{n_\mathrm{x} \times 1} \ \forall \ i \in [1,n_\mathrm{t}] \ \forall \ n_\mathrm{x} \in \mathbb{N}$.\\             
%\noindent The principal components of an $n_l$ dimensional latent space for the data points in $C$ are chosen as the right eigen vectors corresponding to the first $n_l$ singular values of $C$. In this context, the projection of a data point $r \sim C$ into the latent space can be obtained as:
%\begin{equation}
%\label{fe}
%	r_l = T_\mathrm{e} \cdot r,
%\end{equation}
%where,
%\begin{equation}
%\label{te}
%T_\mathrm{e} = \widehat{\Sigma}^{-1} V^TC^T 
%\end{equation}
%\noindent Here $r_l \in \mathbb{R}^{n_l \times 1}$, $V \in \mathbb{R}^{n_\mathrm{t} \times n_\mathrm{t}}$ represents the matrix of right eigen vectors of $C$ and $\widehat{\Sigma} \in \mathbb{R}^{n_l \times n_\mathrm{t}}$ contains the first $n_l$ singular values of $C$ on its diagonal elements. The value of $n_l$ is upperbounded by the number of data points in $C$. This feature of DPCA is particularly attractive in cases where $n_\mathrm{d} >> n_\mathrm{t}$. A reconstruction $r'$ of the original data point $r$ from the latent space representation $r_l$ can then be obtained as:
%\begin{equation}
%\label{fd}%
%	r' = T_D \cdot r_l, \qquad r' \in \mathbb{R}^{n_\mathrm{d} \times 1}
%\end{equation}        
%where,
%\begin{equation}
%\label{td}
%T_D = CV\widehat{\Sigma}^{-1}
%\end{equation}

%\noindent 
%\textbf{Remark 2:} The computation of transformations $T_\mathrm{e}$ and $T_D$ depends on $n_l$. Additionally, considering that we have access to the dataset $C$, the matrix transformations $\widehat{\Sigma}^{-1} V^TC^T$ and $CV\widehat{\Sigma}^{-1}$, in the R.H.S of Eq. \eqref{fe} and Eq. \eqref{fd}, become constant for a specific problem for a given choice of $n_l$. \\
%In the light of \emph{Remark 2}, for a given dataset $C \in \mathbb{R}^{n_\mathrm{d} \times n_\mathrm{t}}$ the encoding($\pi_\mathrm{e}$) and decoding($\pi_D$) sub-policies for using in the student policy $\pi_\mathrm{s}$ can be defined as follows:
%\begin{equation}
%\label{pie}%
%	\pi_\mathrm{e}(r) = T_\mathrm{e}.r 
%\end{equation}
%\begin{equation}
%\label{pid}
%	\pi_D(r_l) = T_D.r_l 
%\end{equation}
\vspace*{-.1cm}

\section{Simulation study}
\label{Section_SimulationStudy}
%This section presents a comparison study of the proposed TAIL-ILC approach with respect to another artificial neural network based approach, called here as NN-ILC, see \cite{Bosma-msc}. Additionally, performance of the TAIL-ILC controller is compared to traditional feedforward control based on the rigid body dynamics, see \cite{190Steinbuch}. For simulation purposes, a high-fidelity model of a 
%moving-magnet planar actuator, depicted in Figure \ref{FIGUUR}, is considered. A detailed description of this high-precision motion system is given in \cite{9566789}.

This section presents a comparison study of the TAIL-ILC approach in comparison with classical ILC, an artificial neural network (ANN) based ILC, referred to as NN-ILC, see \cite{Bosma-msc}, and conventional rigid body feedforward, see \cite{190Steinbuch,Proimadis-phd}, which is obtained by multiplication of the acceleration profile and the inverted rigid body dynamics of the system: 
\vspace*{-.2cm}
\begin{equation}
    C_\mathrm{FF} = m\Ddot{r_i}
\end{equation}

\vspace{-.1cm}
\noindent 
To facilitate simulation, a high-fidelity model of a moving-magnet planar actuator (MMPA), depicted in Figure \ref{FIGUUR}, is considered. A detailed description of a MMPA system is given in \cite{9566789}.  
\begin{figure}[b]
\vspace*{-.3cm}
    \centering
    \includegraphics[width=.9\linewidth]{Pics/MeasSystem.pdf}
  \vspace*{-2.5mm}
    \caption{Schematic representation of a MMPA model.}
    \label{FIGUUR}
\end{figure}
%The simulation study is performed on a single axis of a 6 DOF magnetic levitation system called NAPAS. The single axis corresponds to the 1st DOF of the rigid body decoupled system. Figure \ref{system} shows the frequency domain behaviour of the 1st DOF of the rigid body decoupled system.
%\begin{figure}
%    \centering
%    \includegraphics[width=250pt]{Pics/system bode.png}
%    \caption{Bode plot of the 1st DOF of NAPAS system}
%    \label{system}
%\end{figure}
%The system is sampled at a frequency of 8.3 kHz which is standard as per the physical system. 
Table \ref{combined-table} provide a concise overview of the network architecture and training specifics for sub-policy $\pi_\mathrm{C}$ in TAIL-ILC and policy $\pi_\mathrm{NN}$ in NN-ILC, respectively. For the sake of comparability, the training parameters are kept consistent between the two networks. The networks are designed and trained using the Deep Learning toolbox in MATLAB 2019b, employing the default random parameter initialization.

%The architecture and training details of the deep neural networks corresponding to sub-policy $ \pi_\mathrm{C}$ in TAIL-ILC and the policy $\pi_\mathrm{NN}$ corresponding to NN-ILC are summarised in Tables \ref{archi} and \ref{training} respectively. For the purpose of comparison, the training parameters are taken the same for both the networks. The design and training of the networks is carried out using the MATLAB 2019b Deep Learning toolbox with the default random initialisation of network parameters. 

The training set consists of 618 trajectories, while the test set includes 42 trajectories, each of which is 2.5 seconds long with a total of 20833 time samples. Each trajectory corresponds to a fourth-order motion profile, designed based on the approach presented by \cite{208Lambrechts}, and is parameterized with five parameters in the spatial domain. Individual trajectories are then generated by sweeping over a grid of values for each of these parameters. The objective of this study is to evaluate and compare the performance of the previously mentioned feedforward approaches against the expert ILC policy $\pi^*$, which is the traditional ILC optimized for multiple trajectories of the same class. The primary aim of ILC in this context is to mitigate any unaccounted-for residual dynamics in the system and enhance classical model-based feedforward. Consequently, we also compare the combined performance of student policies with classical feedforward controllers. We demonstrate the tracking ability of TAIL-ILC and NN-ILC on two reference trajectories, namely $r_1$ and $r_2$, which belong to the same class and are shown in Figure \ref{reftraj1dof}. $r_1$ is a randomly chosen trajectory from the training set, while $r_2$ is a previously unseen trajectory.

\begin{table}[t]
\vspace*{4.5pt}
\centering
\caption{Architecture and training details of the NNs}
\vspace*{-.2cm}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Parameter} & \textbf{TAIL-ILC} & \textbf{NN-ILC}\\
\hline
No. of neurons in the input layer & $618$ & $4$\\
\hline
No. of hidden layers & $3$ & $3$\\
\hline
No. of neurons in hidden layers & $800$ & $6$\\
\hline
Activation & Relu & Relu\\
\hline
No. of neurons in the output layer & $618$ & $1$\\
\hline
Learning rate & $10^{-3}$ & $10^{-3}$\\
\hline
Epochs & $5000$ & $5000$\\
\hline
Optimizer & adam & adam\\
\hline
Minibatch size & $128$ & $128$\\
\hline
Train set & $618$ trajectories & $618$ trajectories\\
\hline
Test set & $42$ trajectories & $42$ trajectories\\
\hline
\end{tabular}}
\vspace*{-.4cm}
\label{combined-table}
\end{table}
%\begin{table}[t]
%\vspace{-.4cm}
%\centering
%\caption{Architecture details of the deep neural networks}
%	\begin{tabular}{|c|c|c|}%
%	    \hline
	  %  \textbf{Parameter} & \textbf{TAIL-ILC} & \textbf{NN-ILC}\\
%		\hline
%		No. of neurons in the input layer & $618$ & $4$\\
%		\hline
 %       No. of hidden layers & $3$ & $3$\\
%		\hline
%		No. of neurons in hidden layers & $800$ & $6$\\
%		\hline
%		Activation & Relu & Relu\\
%		\hline
%		No. of neurons in the output layer & $618$ & $1$\\
%		\hline
%	\end{tabular}
%\label{archi}
%\end{table}

%\begin{table}[t]
%\centering
%\caption{Training parameter values for the deep neural networks}
%	\begin{tabular}{|c|c|}
%	    \hline
%	    \textbf{Parameter} & \textbf{Value}\\
%		\hline
%		Learning rate & $10^{-3}$\\
%		\hline
%	        Epochs & $5000$\\
%		\hline
%		Optimizer & adam\\
%		\hline
%		Minibatch size & $128$\\
%		\hline
%		Train set & $618$ trajectories\\
%		\hline
%		Test set & $42$ trajectories\\
%		\hline
%	\end{tabular} \vspace{-4mm}
%\label{training}
%\end{table}
%The training set has a total of 618 trajectories and the test set has 42 trajectories. Each of the training and test set trajectories are of 2.5 seconds in length with a total of 20833 time samples. Each trajectory corresponds to a fourth-order motion profile and is parameterized with five parameters in the spatial domain. Individual trajectories are then generated by sweeping over a grid of values for each of these parameters. The aim of the study is to show the performance of the TAIL-ILC student policy $\pi_\mathrm{s}$ and the NN-ILC policy $\pi_\mathrm{NN}$ and compare them with the expert ILC policy $\pi^*$, which is the conventional ILC optimised for multiple trajectories of the same class. In this context, ILC is primarily employed to compensate for unmodeled residual dynamics in the system, as an augmentation to the classical model-based feedforward. Therefore, comparisons are also drawn between combinations of the student policies with the classical feedforward controller.  Here, we demonstrate the trajectory tracking performance of the TAIL-ILC and NN-ILC on the NAPAS system for two reference trajectories, namely $r_1$ and $r_2$ belonging to the same class. Among the two trajectories, $r_1$ is a randomly selected training set trajectory while $r_2$ is a previously unseen trajectory. These are shown in Figure \ref{reftraj1dof}.

%\begin{figure}[b]
%    \centering
%    \includegraphics[width=250pt]{Pics/seen and %unseen traj.png}
%    \caption{$r_1$ and $r_2$ to be tracked by the NAPAS system in x-y plane}
%    \label{napasreftraj}
%\end{figure}
\begin{figure}[b]
\vspace*{-.6cm}
    \centering
    \includegraphics[width=\linewidth]{Pics/Ananth1-eps-converted-to.pdf} 
    \vspace*{-.6cm}
    \caption{x-direction of the references $r_1$ and $r_2$.}
    \label{reftraj1dof}
\end{figure}
%A rigid body feedforward controller obtained by making use of the acceleration profile and directly inverting the rigid body dynamics of the system, having the form (see \cite{Proimadis-phd}):


\vspace{-.1cm}
\subsection{Time domain performance of TAIL-ILC and NN-ILC}
A silicon wafer scanning application is considered where the scanning takes place during the constant velocity interval of the motion profile, see \cite{Butler}. In this context, Figure \ref{trkerrseen} illustrates the position tracking error in $x$-direction during the constant velocity interval of the reference trajectories $r_1$ and $r_2$ respectively. In addition to the performance of mass feedforward, TAIL-ILC and NN-ILC, the figure also indicates the performance of the expert ILC policy. This is to facilitate the comparison of the performance of the two deep learning based ILC variants with the baseline. As demonstrated in the left Figure, i.e. the performance of the feedforward controllers on $r_1$, the expert ILC policy exhibits the highest overall performance. Nonetheless, it is noteworthy that the TAIL-ILC policy outperforms in terms of the peak tracking error achieved compared to the alternative feedforward approaches, whereas the NN-ILC policy demonstrates a superior performance in terms of the convergence time of the error. Nonetheless, when analyzing the right Figure, i.e. the performance of the feedforward approaches for a previously unseen trajectory $r_2$, the expert ILC policy needs to re-learn the relevant feedforward signal. Conversely, the TAIL-ILC and NN-ILC policies are capable of achieving similar performance to the re-learned expert ILC policy without any further training. Additionally, when combined with a classical mass feedforward controller, both the TAIL-ILC and NN-ILC policies are observed to yield superior performance in terms of peak error and settling time compared to the classical mass feedforward controller alone.

%Here, as can be seen, while the expert ILC policy gives the overall best performance, the TAIL-ILC policy gives a better performance in terms of the peak tracking error achieved whereas the NN-ILC policy gives a better performance in terms of the convergence time of the error.



%However, as can be seen in Figure \ref{trkerrseen}, in order to deliver similar performance for $r_2$, the expert ILC policy has to re-learn the corresponding feedforward signal. In contrast, both TAIL-ILC and NN-ILC policies deliver a performance close to that of the re-learnt expert ILC policy without any additional training. 
%In both cases, the combinations of trained student policies and classical mass feedforward controller are observed to deliver a better performance compared to the classical mass feedforward alone, in terms of peak error as well as the settling time.

\begin{figure}[t]
\vspace*{4.5pt}
\centering
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[trim={1cm 0.5cm .2cm 0cm}
    ,width=\linewidth]{Pics/trackingerror1stDOFw-CFFaftercontrolzoomin.png}
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
  \centering
  \includegraphics[trim={1cm 0.5cm .2cm 0cm}
    ,width=\linewidth]{Pics/trackingerror1stDOFw-CFFaftercontrolunseenzoomin.png}

\end{subfigure}
\caption{{Tracking error for $r_1$ (left) and $r_2$ (right) during constant velocity.}}
\label{trkerrseen}
\vspace*{-.5cm}
\end{figure}

%\begin{figure}[t]
%    \centering
%    \includegraphics[trim={2cm 0cm 2cm 0cm},width=\linewidth]{Pics/Combofig.eps} 
%        \vspace*{-.6cm}

 %   \caption{{Tracking error for $r_1$ (left) and $r_2$ (right) during constant velocity.}}
  %  \label{trkerrseen}
  %  \vspace*{-.5cm}
%\end{figure}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=\linewidth]{Pics/Ananth2.eps}
 %   \caption{Tracking error for $r_2$ in constant velocity phase}
 %   \label{trkerrunseen}
%\end{figure}
\vspace{-.1cm}
\subsection{TAIL-ILC vs NN-ILC}
Table \ref{comparison results} provides a comparison of the training and prediction properties of the TAIL-ILC and NN-ILC student policies. Here, we compare the following parameters:
\begin{enumerate}
    \item $T_\text{train}$: Time to train the neural network
    \item $T_\text{predict}$: Time to make predictions for 10 randomly selected test set trajectories.
    \item $e_\text{train}$: Control signal prediction error averaged over 10 randomly selected train set trajectories.
	\item $e_\text{test}$: Control signal prediction error averaged over 10 randomly selected test set trajectories.
	\item $e_\text{peak \ tracking}$: Peak tracking error achieved with the predicted control signals averaged over 10 randomly selected train set trajectories.
\end{enumerate}
\begin{table}[b] 
\vspace*{-.3cm}
		\centering
		\caption{Performance comparison for the $1^{st}$ degree of freedom}
		\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Criterion} & \textbf{NN-ILC} & \textbf{TAIL-ILC}\\
			\hline
	        $T_\text{train}$ & $2.5$ hr & $20$ min\\
	        \hline
			$T_\text{predict}(\text{per sample})$ & $0.005$ sec & $0.064$ sec\\
			\hline
			$T_\text{predict}(\text{full signal})$ & $86$ sec & $0.064$ sec\\
			\hline
			$e_\text{train}$ & $0.0055$ N & $0.0011$ N\\
			\hline
			$e_\text{test}$ & $0.0013$ N & $0.0064$ N\\
			\hline
			$e_\text{peak\ tracking}$ & $1.3 \cdot 10^{-7}$ m & $8.3 \cdot 10^{-8}$ m\\
			\hline
		\end{tabular}
		\label{comparison results} 
\end{table}
\noindent Here, the average control signal prediction errors of the train and the test set trajectories are calculated as the values of the performance measure $\eta$ in \eqref{modifiedupperbound}. As can be seen, though the original signals and trajectories are extremely high dimensional, the projection of these signals into the latent space using the proposed TAIL-ILC approach has resulted in significant improvement in training and prediction time compared to that of the NN-ILC approach.  

%Since the TAIL-ILC is primarily offline implementable, the prediction times per sample and for full signal are the same whereas this is not the case for NN-ILC because of the online implementation.

Moreover, as observed in Figure \ref{trkerrseen}, the average signal prediction error has decreased for TAIL-ILC in case of previously seen trajectories whereas the NN-ILC has improved performance for previously unseen trajectories.

\vspace{-.2cm}
\section{TAIL-ILC vs NN-ILC PERSPECTIVES}
\label{TAILPERSPE}

\vspace*{-.1cm}
In the previous Section, we have seen a comparison of the performance of TAIL-ILC and NN-ILC controllers for a specific use case. However, it is more natural to view these controllers as individual instances of two fundamentally different perspectives of the problem. Hence, it is important to reflect upon the perspectives that these controllers convey and the consequences for various aspects of the resulting controllers. This is expected to provide us with a more generalised reasoning to some of the differences observed in performances of these two controllers.

\vspace{-.1cm}
\subsection{Time duration of trajectories}
%As NN-ILC treats reference trajectories and feedforward signals sample-wise, dealing with trajectories of different lengths is not an issue. In contrast to this, TAIL-ILC treats trajectories and signals in full length. As a result, because of the fixed input-output dimensionality of a neural network learning model, dealing with trajectories of different durations is now a non-trivial issue requiring the use of additional encoding and decoding transformations. Moreover, an advantage of NN-ILC over TAIL-ILC is the fact that it can accomodate for instantaneous changes in the reference trajectories which is not possible with TAIL-ILC. Alternatively, a middle ground between both the perspectives can be pursued by using a different learning model class, for example a recurrent neural network (RNN). %In such a case, we would be able to deal with more than a single time sample at a time unlike NN-ILC but not the entire signal at a time unlike TAIL-ILC, while being able to extract prominent spatial features from the data. 

\vspace*{-.1cm}

The NN-ILC and TAIL-ILC are two approaches of ILC that differ in their treatment of reference trajectories and feedforward signals. NN-ILC is capable of handling trajectories of different lengths, as it deals with them sample-wise. In contrast, TAIL-ILC processes trajectories and signals in their entirety, making it challenging to manage trajectories of varying durations due to the fixed input-output dimensionality of neural network learning models. Additionally, NN-ILC is better equipped to handle instantaneous changes in reference trajectories compared to TAIL-ILC. A possible solution to reconcile these perspectives is to use a different class of learning models, such as a recurrent neural network.
\subsection{Training and prediction time efficiencies}
\vspace*{-.1cm}


In NN-ILC, the training dataset used for $\pi_\mathrm{NN}$ encompasses all the samples from all the trajectories in the training set, along with their associated feedforward signals. Conversely, TAIL-ILC employs a training dataset for $\pi_\mathrm{C}$ that solely includes the parameters of the trajectories and feedforward signals within the latent space, resulting in a significantly smaller dataset in comparison to the total number of samples. This characteristic leads to TAIL-ILC presenting shorter training and prediction times when compared to NN-ILC, as demonstrated by the results presented in Table \ref{comparison results}.
%In NN-ILC, the training data for $\pi_\mathrm{NN}$ includes all samples of all the training set trajectories and the corresponding feedforward signals. In contrast to this, the training data for $ \pi_\mathrm{C}$ in TAIL-ILC consists of only the parameters of the trajectories and feedforward signals in the latent space which is much less compared to the total number of samples. This feature ensures that the TAIL-ILC approach has less training and prediction times compared to NN-ILC as observed in Table \ref{comparison results}. %Alternatively, additional techniques such as sub-sampling can be used in case of NN-ILC to aleviate this constraint. However, sub-sampling could lead to compromise in performance due to the loss of some training data. Moreover, this would have effect only on the training time while the prediction time would still remain high.
\subsection{Generalizability to previously unseen trajectories}
%The generalizability of performance to previously unseen trajectories is better with NN-ILC compared to TAIL-ILC as is evident from tracking results in Figure \ref{trkerrseen}. This is because, NN-ILC treats the time samples of reference trajectories as points in an $n$-dimensional space corresponding to the $n^\mathrm{th}$ order motion profiles and learns a mapping to the corresponding feedforward signal time samples. Consequently, the trained network is able to regress better in the space of possible reference trajectories, thus extrapolating the performance to previously unseen points in this space. In contrast to this, since the TAIL-ILC approach primarily relies on analogies between individual tasks on a higher level, the sample-wise performance for previously unseen trajectories might not be as good as for NN-ILC. 

Figure \ref{trkerrseen} demonstrates that NN-ILC outperforms TAIL-ILC in terms of generalizing performance to previously unobserved trajectories. The improved performance can be attributed to NN-ILC's treatment of reference trajectories as points in an $n$-dimensional space corresponding to $n^\mathrm{th}$ order motion profiles, which allows it to learn a mapping to the corresponding feedforward signal time samples. As a result, the trained network can more accurately extrapolate performance to previously unobserved points in the space of possible reference trajectories. In contrast, TAIL-ILC relies primarily on analogies between individual tasks on a higher level, which may result in suboptimal performance when confronted with previously unobserved trajectories at the sample level.
\vspace{-.1cm}
\section{CONCLUSION}
\label{Section_Conclusions}
\vspace*{-.1cm}

In this work, we have primarily explored two different perspectives within the context of deep learning of the task-flexibility constraint of conventional ILC. While each of the considered approaches has its own advantages and disadvantages, it has been observed that the use of deep learning techniques in general could be a useful direction for future research in designing task-flexible ILC variants.  


\vspace*{-.1cm}

\bibliographystyle{ieeetr}       
\bibliography{MyBib} 


%\begin{thebibliography}{99}
%Steinbuch, M., Merry, R. J. E., Boerlage, M. L. G., Ronde, M. J. C., & Molengraft, van de, M. J. G. (2010). Advanced Motion Control Design. In W. S. Levine (editor), The Control Handbook, Control System Applications, Second edition (blz. 27-1/25). CRC Press. https://doi.org/10.1201/b10382-35
%\bibitem{model based ffd for motion systems}
%Boerlage, M., Steinbuch, M., Lambrechts, P., \& van de Wal, M. (2003, June). Model-based feedforward for motion systems. In Proceedings of 2003 IEEE Conference on Control Applications, 2003. CCA 2003. (Vol. 2, pp. 1158-1163). IEEE.

%\bibitem{amcprecisionmech.}
%Oomen, T. (2018). Advanced motion control for precision mechatronics: Control, identification, and learning of complex systems. IEEJ Journal of Industry Applications, 7(2), 127-140.

%\bibitem{a survey of ilc}
%Bristow, D. A., Tharayil, M., \& Alleyne, A. G. (2006). A survey of iterative learning control. IEEE control systems magazine, 26(3), 96-114.

%\bibitem{rational basis functions in ilc}
%Bolder, J., \& Oomen, T. (2014). Rational basis functions in iterative learning controlWith experimental verification on a motion system. IEEE Transactions on Control Systems Technology, 23(2), 722-729.

%\bibitem{bt-ilc}
%Hoelzle, D. J., Alleyne, A. G., \& Johnson, A. J. W. (2010). Basis task approach to iterative learning control with applications to micro-robotic deposition. IEEE Transactions on Control Systems Technology, 19(5), 1138-1148.

%\bibitem{nn-ilc}
%S. Bosma, The generalization of feedforward control for a periodic motion system. MSc thesis. Delft University of Technology, 2019.

%\bibitem{dd-ilc using rl}
%Song, B. (2019). From model-based to data-driven discrete-time iterative learning control. Columbia University.

%\bibitem{ac-ilc}
%Poot, M., Portegies, J., \& Oomen, T. (Accepted/In press). On the Role of Models in Learning Control: Actor-Critic
%Iterative Learning Control. Paper presented at 21st IFAC World Congress 2020, Berlin, Germany.

%\bibitem{ilc for industrial systems}
%Steinbuch, M., \& van de Molengraft, R. (2000). Iterative learning control of industrial motion systems. IFAC Proceedings Volumes, 33(26), 899-904.

%\bibitem{b2b feedforward parameterized ilc}
%Blanken, L., Boeren, F., Bruijnen, D., \& Oomen, T. (2016). Batch-to-batch rational feedforward control: from iterative learning to identification approaches, with application to a wafer stage. IEEE/ASME Transactions on Mechatronics, 22(2), 826-837.

%\bibitem{ilc for varying tasks}
%Van Zundert, J., Bolder, J., \& Oomen, T. (2015, July). Iterative learning control for varying tasks: Achieving optimality for rational basis functions. In 2015 American Control Conference (ACC) (pp. 3570-3575). IEEE.

%\bibitem{survey of dimred tech}
%Fodor, I. K. (2002). A survey of dimension reduction techniques (No. UCRL-ID-148494). Lawrence Livermore National Lab., CA (US).

%\bibitem{alighodse lecture1}
%Lecture 3 and 4 \textit{Data Visualization} STAT 442 / 890, CM 462, Lecture: Ali Ghodsi.

%\bibitem{alighodse lecture2}
%Lecture 6 \textit{Data Visualization} STAT 442 / 890, CM 462, Lecture: Ali Ghodsi.

%\bibitem{ioannesthesis}
%Proimadis, I. (2020). Nanometer-accurate motion control of moving-magnet planar motors. Technische Universiteit Eindhoven.

%\end{thebibliography}
\end{document}
