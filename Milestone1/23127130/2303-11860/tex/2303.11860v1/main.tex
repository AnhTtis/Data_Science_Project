\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% ready for submission
%\usepackage{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:

\PassOptionsToPackage{numbers, sort}{natbib}
\usepackage[preprint]{neurips_2022}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2022}

% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2022}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath} 
\usepackage{comment}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{mathdots}
\usepackage{verbatim}
\usepackage{wrapfig}
\usepackage{float}
%\usepackage{sidecap}


\title{Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  N. Leroux \\%\thanks{}
  Peter Gr\"unberg Institute, \\
  Forschungszentrum J\"ulich, \\
  Aachen, Germany \\
  \texttt{n.leroux@fz-juelich.de}
  % examples of more authors
  \And
  J. Finkbeiner \\
  Peter Gr\"unberg Institute, \\
  Forschungszentrum J\"ulich, \\
  RWTH\\
  Aachen, Germany \\
  \texttt{j.finkbeiner@fz-juelich.de}
  \AND
  E. Neftci \\
  Peter Gr\"unberg Institute, \\
  Forschungszentrum J\"ulich, \\
  RWTH,\\
  Aachen, Germany \\
  \texttt{e.neftci@fz-juelich.de}
}

\begin{document}


\maketitle

\begin{abstract}
    Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we increase the sparsity of the network using Leaky-Integrate and Fire (LIF) units, a bio-inspired neuron model that activates sparsely in time solely when crossing a threshold. We thus reduce the number of synaptic operations up to a factor of $\times5.3$ without loss of accuracy. Our results hold great promises for accurate and fast online processing of sEMG signals for smooth prosthetic hand control and is a step towards Transformers and Spiking Neural Networks (SNNs) co-integration for energy efficient temporal signal processing.
\end{abstract}

\section{Introduction}
    Surface Electromyography (sEMG) is a technique that senses currents running through muscular fibers’ membrane to measure muscular activity \cite{zheng_surface_2022}. As sEMG signals are triggered by electrical stimuli from the central nervous system, this method is gaining a strong interest as a mean for Human-Machine Interfacing \cite{zheng_surface_2022}. Since sEMG measurements only require electrodes positioned on the forearm skin, this technique is very promising for future non-invasive wearable prosthetic hand control system \cite{zheng_surface_2022}.
    
    Transformers, which are the state-of-the-art networks for sequence processing \cite{vaswani_attention_2017, lin_survey_2022}, can be very efficient to process sEMG signals \cite{burrello_bioformers_2022}. However, the self-attention mechanism \cite{vaswani_attention_2017} used in conventional transformers requires to wait for large time windows, which induces a delay preventing fast online processing of continuous signals. Moreover, memory and computation of the self-attention mechanism scales quadratically with the sequence length.  
    
     In contrast, Recurrent Neural Networks (RNNs) integrate the concept of time into their operating model and are thus suited for continuous signals online processing.  Spiking Neural Networks (SNNs) \cite{zenke_superspike_2018, tavanaei_deep_2019} are a bio-inspired type of RNNs. They are very promising for low power applications because their neurons only transmit information when their membrane potential (an internal state of each neuron) reaches a threshold, and these events happen sparsely in time \cite{tavanaei_deep_2019}. Many research focus on building new hardware that leverage the inherent temporal sparsity of SNNs \cite{merolla_million_2014, liu_memory-efficient_2018, orchard_efficient_2021, pehle_brainscales-2_2022}. 
%However, RNNs are hard to train to learn long-range dependencies because of the 
%vanishing gradient problem \cite{The Vanishing Gradient Problem During Learning 
%Recurrent Neural Nets and Problem Solutions}.

  In this paper, we propose an online transformer that makes use of a linearized sliding window attention mechanism \cite{beltagy_longformer_2020}. We adapt this attention mechanism for online processing of continuous signals by making it forward in time and serialized. Our online transformer thus performs inference for each token as they are generated. In order to leverage information from past inputs, we store information in the keys and the values of the attention mechanism, and we update this memory dynamically as the tokens are generated. The length of the sequences stored in the keys and the values is a hyper-parameter that we can tune to change the temporal depth of the information used in the attention mechanism, as well as the computational complexity and the memory usage.
  
    We test our model on a finger position regression through sEMG signals using the Non-Invasive Adaptive Hand Prosthetics Database 8 (NinaProDB8) dataset. First, we show that our online transformer allows users to process sEMG signals with high accuracy using solely very short time windows of 3.5 ms, which permits a very fine granularity in time of prosthetic hand control. Secondly, we show that selecting the temporal depth of the attention improves the results of signal processing and makes our model outperform a self-attention-based transformer, as well as previous state-of-the-art models. Finally, we show how our custom online attention mechanism allows us to SNNs inside the transformer architecture to increase the network sparsity, which in turn results in a reduction of the required number of synaptic operations by a factor of $\times5.3$ without loss of accuracy.

\section{Related work}
\paragraph{Deep Learning for Surface Electromyography processing}
    Although sEMG signals and muscle activity are correlated, their relation is unknown and processing sEMG signals remains very challenging because of electrical noise (e.g., interference, ground noise, crosstalk between electrodes), inter-subject variability (e.g., different forearm circumferences, muscle characteristics), and intra-subject variability (e.g., variation of the electrodes position or the skin conductivity from one day to the next) \cite{krasoulis_effect_2019}.
    
    Deep learning methods can leverage large datasets to extract the more relevant features despite noise or variability \cite{jaramillo-yanez_real-time_2020}. They can thus outperform conventional machine learning techniques like Support Vector Machine (SVM) \cite{milosevic_exploring_2018}. Moreover, deep networks can process raw sEMG signals whereas conventional networks require prior pre-processing like Principal Component Analysis \cite{phukpattaranont_evaluation_2018}, Linear Discriminant Analysis (LDA) \cite{phukpattaranont_evaluation_2018}, Fourier transforms \cite{taghizadeh_finger_2021}, and others.
    
    Deep learning has already been applied to sEMG signals processing using Temporal Convolutions \cite{tsinganos_improved_2019,zanghieri_robust_2020,zanghieri_semg-based_2021} and Recurrent Neural Networks (RNNs) \cite{anam_estimation_2020, koch_regression_2020,ilyas_evaluation_2022, li_approach_2022}. While the  ability to compute on the edge with restricted memory capacity and low power consumption are essential to the deployment of autonomous wearable prosthetic hand control systems, most deep learning techniques are computationally intensive. \citet{mukhopadhyay_classification_2018} have shown that the inherent sparsity of SNNs can be leveraged to reduce drastically the computational intensity of sEMG signals processing. \citet{burrello_bioformers_2022}  have shown that a transformer network can process sEMG signals with a limited memory usage and reduced number of Multiply-And-Accumulate (MAC) operations.
    
\paragraph{Transformers}
    Unlike RNNs, Transformers do not suffer from the vanishing gradient problem for learning features in time \cite{lin_survey_2022}, they do not have inductive biases made from assumptions about the data structure, and they can be trained very fast on GPUs since they can process an entire temporal sequence in parallel. The workhorse of Transformers is the self-attention mechanism, an operation that allows all the elements of a sequence to be compared with each other’s. For Natural Language Processing (NLP), the strength of self-attention is that is allows one token to be compared with present, past, and future tokens \cite{vaswani_attention_2017}. However, depending on the application,  conventional self-attention is not always the best choice. It has been shown that using local attention and sliding windows attention can lead to better results for long sequences in NLP \cite{beltagy_longformer_2020} and in Machine Vision \cite{hassani_dilated_2022}.
    
    
\paragraph{Transformers with Spiking Neural Networks.}
    SNNs, which mimic biological neural networks, are very promising for low power applications because their neurons only transmit information when their membrane potential (an internal state of each neuron) reaches a threshold, and these events happen sparsely in time \cite{tavanaei_deep_2019}. Integrating SNNs in a transformer architecture is challenging and not intuitive. Just as RNNs, SNNs have a temporal dynamic. Thus, each element of a sequence must be fed to RNNs or SNNs sequentially. In contrast, since the self-attention mechanism compares all the different elements of a sequence in parallel, Transformers require to wait for the completion of a sequence before computing. For instance, the transformer used in \cite{burrello_bioformers_2022} for sEMG classification used time windows of 150 ms. Naively stacking conventional self-attention layers and recurrent layers would then lead to undesirable delays due to the alteration between waiting time windows and processing sequences sequentially.
    
    \citet{yao_temporal-wise_2021} have used a type of attention mechanism to select the importance of event frames, and then process the events with a SNN. \citet{sabater_event_2022} have shown that a transformer can be used to process event-based vision sensor data more efficiently and accurately than convolutional neural networks.
    \citet{zhou_spikformer_2022} have used binarized self-attention to integrate sparsity in Transformers. 
    \citet{li_spikeformer_2022} have used a SNN as a pre-processing step for a transformer. It was also shown by \citet{gehrig_recurrent_2022} that Long-Term Short-Term (LSTM) units can be integrated inside a transformer architecture, but in this work the attention mechanism was spatial and not temporal. Finally, \citet{zhu_spikegpt_2023} have integrated spiking neurons inside a transformer architecture, but by using a custom attention mechanism that cannot be computed online.   
    
    In this paper, we introduce a transformer model that can perform attention in time online, and is compatible with spiking neurons at every layer of the architecture.     
    
\section{Methods}
\subsection{NinaproDB8: A Finger Position Regression Dataset}
    %
    \begin{figure}
        \includegraphics[width=\linewidth]{img/emg+cyberglove+regression.png}
        \caption{(a) Surface electromyography signal acquired using a 16 channel Delsys Trigno IM Wireless EMG system (see \citet{krasoulis_effect_2019}). The signal of only one out of the 16 channels is plotted. (b) The Cyberglove II is used for the acquisition of the ground truth finger-joint angles \cite{pizzolato_comparison_2017}. (c) Ground truth finger-joint angles and reconstruction with our Online Transformer model.}
        \label{fig:emg}
    \end{figure}
    %
    In this work, we used the Non-Invasive Adaptive Hand Prosthetics Database 8 (NinaProDB8) \cite{krasoulis_effect_2019}, a public sEMG database made as a benchmark for estimation of kinematic finger position. Many deep learning efforts applied to sEMG focus on simple functional movement classification \cite{tsinganos_improved_2019, burrello_bioformers_2022, zheng_surface_2022,zanghieri_robust_2020}. However, sequence-to-sequence regression of finger position can lead to a wider range of gestures and can be more easily coupled to sensory feedback from robotic hands for a closed-loop precise control \cite{markovic_myocontrol_2018}. 
    
    The measurements of the database were made on 10 able-bodied subjects and two right trans-radial amputees. The sEMG signal, that is the input of our neural network (see Fig. \ref{fig:emg} (a)), are recorded using 16 electrodes (Delsys Trigno IM Wireless EMG system) positioned around the right forearm of the participants. The finger positions were measured using a dataglove, the Cyberglove II, 18-Degrees of Freedom (DoF) model, that measures the finger-joint angles that correspond to the dots in Fig. \ref{fig:emg} (b). The sEMG signals and the dataglove signals were up sampled to 2 kHz and post-synchronized. The details of the dataset can be found in \cite{krasoulis_effect_2019}.
    
    In order to disregard the irrelevant degrees of freedom and focus directly on motions relevant for prosthetic hand control, it has been shown by \citet{krasoulis_effect_2019} that we can convert the 18-DoF recorded by the dataglove into 5-Degrees of Actuation (DoA) using a simple linear transformation. The matrix used for this linear transformation can be found in the supplementary materials of \citet{krasoulis_effect_2019}. We used the DoA as targets of our neural network.
    
    Three datasets were recorded for each participant: the first two datasets (acquisition 1 and 2) comprised 10 repetitions of each movement and the third dataset (acquisition 3) comprised only 2 repetitions. We used both acquisition 1 and 2 as training set and acquisition 3 as testing set. In Fig. \ref{fig:emg} (a) and (c) we show the example of the testing set for subject 1 (target). 
    
    To facilitate the training of our neural network, we normalize each set of repetition by subtracting the sEMG signals by their mean and dividing by their standard deviation.

\subsection{Online Inference with a Custom Attention Mechanism}
\label{sec:online_inference}
    %
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{img/pipeline.png}
        \caption{(a) Online Transformer neural network architecture. (b) Online Attention sketch: The different tokens are created by a temporal convolution (with a kernel size 3 and a stride 2 in this example). The tokens are linearly projected toward the queries, the keys and the values ($Q$, $K$, $V$). $Q$ matches only the present token whereas $K$ and $V$ store multiple previous tokens. The length $M$ of this memory is 3 in this example. At each time step, $K$ and $V$ forget the projection of the oldest token and store the projection of the new one. The mathematical operations of the online attention mechanism are described in section \ref{sec:online_inference}.}
        \label{fig:pipeline}
    \end{figure}
    %
    In conventional transformers \cite{vaswani_attention_2017}, the entire self-attention stage is calculated in parallel. The elements of the input sequence of a self-attention layer are called tokens, and the operation of self-attention is described as
    %
    \begin{equation}\label{eq:self_attention}
        \mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{Q\circ K^T}{\sqrt d}\right)\circ V \\
    \end{equation}
    %
    where $\circ$ is the dot-product operator, $Q\in\mathbb{R}^{N\times d}$, $K\in\mathbb{R}^{N\times d}$, and $V\in\mathbb{R}^{N\times d}$ are respectively called the queries, the keys, and the values and are three different projections of the same sequence of tokens:
    %
    \begin{subequations}
    \begin{align}
        Q&=W_{Q}x \label{eq:q_proj} \\
        K&=W_{K}x \label{eq:k_proj} \\
        V&=W_{V}x. \label{eq:v_proj}
    \end{align}
    \end{subequations}
    %
    The attention dimension $d$ is the size of each token projection and $N$ is the sequence length. $W_{Q}\in\mathbb{R}^{d\times D}$, $W_{K}\in\mathbb{R}^{d\times D}$, and $W_{V}\in\mathbb{R}^{d\times D}$ are learnable weights matrices with with $D$ the embedding dimension, and $x\in\mathbb{R}^{N\times D}$ the input of the attention mechanism. 
    
    In the case of continuous signals (such as bio-medical signals), it is possible to split the input signal into finite time windows, and to wait for the end of each time window before carrying-out the inference (as in \citet{burrello_bioformers_2022}). However, this method induces delays due to waiting for the end of the time windows. Our online transformer uses a custom attention mechanism that can be computed online for each element of the sequence without delays. 
    
    To avoid waiting for future tokens, the tokens of time step $t_0$ are not compared with future tokens of time steps $t>t_0$. The information from previous tokens is stored in the keys and the values $K \in\mathbb{R}^{M\times d}$ and $V \in\mathbb{R}^{M\times d}$. Unlike for self-attention, here the size of $K$ and $V$ does not depend on the full sequence length, but solely on $M$, which is the number past time steps we choose to store.
    
    $K$ and $V$ are initially zeroed. Then, the elements $K_{i} \in\mathbb{R}^{d}$ and $V_{i} \in\mathbb{R}^{d}$ are iteratively replaced token-wise using the projections of Eqs. \ref{eq:k_proj} and \ref{eq:v_proj}. At each time step, a single query $Q_{t} \in\mathbb{R}^{d}$ is also computed with Eq. \ref{eq:q_proj}, and the attention is computed as 
    %
    \begin{equation}\label{eq:online_attention}
        \mathrm{Attention}_{t}=\mathrm{softmax}\left(\frac{Q_{t} \circ K}{\sqrt{d}}\right) \circ V
    \end{equation}
    %
    The softmax is computed on the memory length dimension $M$. Since one different element of $K$ and $V$ is updated at each time step, and since the length of $K$ and $V$ is $M$, all their elements are updated with a frequency $\frac{1}{M}$. The above-mentioned procedure is summarized as the Algorithm \ref{alg:online_attention}.
    
    \begin{figure}     
    \begin{minipage}{.5\linewidth}
        \begin{algorithm}[H]
            \caption{Inference with Online Attention}
            \begin{algorithmic}
            \State $K=\mathbf{0}$ \Comment{$K\in\mathbb{R}^{M\times d}$}
            \State $V=\mathbf{0}$ \Comment{$V\in\mathbb{R}^{M\times d}$}
            \State $t=0$
            \State $i=0$
            \Repeat 
                \State ${Q_t} \gets W_{Q}x_{t}$ \Comment{$Q_{t}\in\mathbb{R}^{d}$}
                \State ${K}_{i} \gets W_{K}x_{t}$\Comment{$K_i\in\mathbb{R}^{d}$}
                \State ${V}_{i} \gets W_{V}x_{t}$\Comment{$V_i\in\mathbb{R}^{d}$}
                \State $\mathrm{Attention}_{t} \gets \mathrm{softmax}\left(\frac{Q_{t} \circ K}{\sqrt{d}}\right) \circ V$
                \State $t \gets t+1$ \Comment{New time step}
                \State $i \gets i+1$ 
                \State $i \gets i\;(\bmod{M})$ 
            \Until{end of sequence}
            \end{algorithmic}
        \label{alg:online_attention}
        \end{algorithm}
    \end{minipage}
    \hspace{1em}
    \begin{minipage}{.4\linewidth}
      \includegraphics{img/sliding_window.png}
        \caption{Sliding window attention. The same $K$ values are represented by the same colors.}
        \label{fig:sliding_attention}
    \end{minipage}
    \end{figure}
    
    In contrast to the quadratic dependence with respect to sequence length for conventional self-attention ($O\left(N^2\right)$), the computational complexity of the sliding window attention mechanism is linear ($O\left(M N\right)$).

    Eq. \ref{eq:online_attention} shows that our attention mechanism can be computed time step wise instead of waiting for the end of large time windows to compute the attention in parallel. Now, we will show how this attention mechanism fits in our full neural network.

    \subsection{Neural Network Architecture}
    Our neural network consists of three blocks as depicted in Fig. \ref{fig:pipeline} (a): an embedding block that converts the raw EMG signal into a sequence of tokens, an encoder block that uses attention to find correlation between sequence elements, and a regression block that converts the output into five degrees of actuation. 

    The embedding is made of a temporal convolution layer. Convolutional layers have overlaps between input time windows, which means that unlike linear layers, they have an intrinsic order, and thus do not require positional embeddings \cite{vaswani_attention_2017}. The convolutional layer has $C=16$ input channels matching the 16 electrodes, and $D=64$ output channels. The network is tested with kernels of various sizes to vary the length of the input time window that matches one token. We chose to make an overlap of two time steps between time windows, which makes the convolutional layer stride be $s=k-2$, where $k$ is the kernel size. With a padding $p=1$, the number of tokens generated thus depends on the stride as 
    %
    \begin{equation}\label{eq:n_tokens}
        N=\lfloor\frac{N_{\mathrm{samples}}}{s}\rfloor \\
    \end{equation}
    %
    where $N_{\mathrm{samples}}$ is the number of processed samples of the input signal.
    
    The encoder is described as 
    %
    \begin{equation}\label{eq:encoder}
    	\begin{array}{cll}
        	f\left(x\right)&=&x+\mathrm{MHA}\left(\mathrm{LN}\left(x\right)\right)\\z\left(f\left(x\right)\right)&=&f\left(x\right)+\mathrm{FNN}\left(\mathrm{LN}\left(f\left(x\right)\right)\right)\\
    	\end{array}
	\end{equation}
%
    where $\mathrm{LN}$ is a layer norm layer. $MHA$ is the multi-head attention layer with $h=8$ heads computed in parallel using Eq. \ref{eq:online_attention}, with an attention dimension $d=32$. After that attention is computed, the $h$ heads are concatenated and projected into dimension $D=64$. $FNN$ is a Feedforward Neural Network with one hidden layer of 128 GeLU units \cite{hendrycks_gaussian_2020} and a dropout layer with probability 0.2. The linear projections of $\mathrm{FNN}$ are applied token-wise so that that they can be computed online. The entire encoder block can be repeated and stacked $L$ times, but here we chose to keep $L=1$. The backbone of our neural network is inspired from \citet{burrello_bioformers_2022}. 
    
    Finally, the regression block consists of a linear layer that projects each token from a dimension $D = 64$ to a dimension 5 (the number of degrees of actuation we perform the regression on), and an up sampling layer that duplicates the output of each token to generate as many samples as there are in the target signal (an example of target signal is shown in Fig. \ref{fig:emg} (c)). The up sampling factor is equal to the stride that we use in the convolutional embedding layer (see Eq. \ref{eq:n_tokens}). 
    
    \subsection{Increasing the network sparsity with binarization and spiking neurons}
    
    In order to reduce the number of required operations, we increase the network sparsity by using binarization and Leaky Integrate and Fire (LIF) units \cite{zenke_superspike_2018,tavanaei_deep_2019} units. We test two sparse models. In the first one, we binarize the output of the convolutional embedding, we binarize the projections $Q$, $K$, and $V$, and we replace the FNN by a SNN with a first layer of 128 LIF units, and a second of $D=64$ LIF units. The second sparse model is similar to the first one, but instead of binarizing $Q$, $K$, and $V$, we replace each projection of Eqs. \ref{eq:q_proj}, \ref{eq:k_proj}, and  \ref{eq:v_proj} by a single spiking layer of $d=32$ LIF units, which adds an additional dynamic to the model.
    
    Binarization is done by applying a Heaviside function.
    The dynamics of the LIF units are defined by
    %
    \begin{subequations}
        \begin{align}            U_t&=\alpha\left(1-S_{t-1}\right)U_{t-1}+\left(1-\alpha\right)I_{t-1} \label{eq:U} \\
            I_t&=\beta I_{t-1}+\left(1-\beta\right)Wx_t \label{eq:I} \\
            S_t&=H\left(U_{t-1}-\mathrm{\Theta}\right) \label{eq:S}
        \end{align}
    \end{subequations}
    %
    where $t$ is the index of the tokens, $U$ is the membrane potential, $I$ is the synaptic current, S is the spike response, H is the Heaviside function, $\alpha=0.95$, $\beta=0.9$, and $\mathbf{\Theta}=1$. The outputs of the $Q$, $K$, and $V$ projections are the spike responses $S_t$ (see Eq. \ref{eq:S}). The outputs of the first layer of the SNN replacing the FNN are the spike responses $S_t$, and the outputs of the second layer are the membrane potentials $U_t$ (see Eq. \ref{eq:U}).
    
    Because the Heaviside function is not differentiable, during training the gradient of the different Heaviside functions (used for binarization and LIF units) are replaced by the SuperSpike surrogate gradient \cite{zenke_superspike_2018, neftci_surrogate_2019}. To preserve the sparsity between the embedding and the encoder block, we remove the layer norm layer that precedes the embedding when the embedding is binarized. In addition, we remove the dropout layers in the two sparse models. The softmax of the attention mechanism is only computed on non-zero elements.
    
    \subsection{Training}
    To speed up training, the attention block is computed in parallel. Projections  $Q\in\mathbb{R}^{N\times d}$, $K\in\mathbb{R}^{N\times d}$, and $V\in\mathbb{R}^{N\times d}$ are computed for an entire time window with $N$ tokens. The keys and values are then unfolded into sliding windows of size $M$ and stride 1, similarly as for a convolution (see in Fig. \ref{fig:sliding_attention} an example of sliding window attention). The product between queries and keys is thus computed as 
    %
    \begin{equation}\label{eq:attention_training_matrix}
            \left[\begin{matrix}
            Q_{0}K_{1-M} & \cdots & Q_{0}K_{-2} & Q_{0}K_{-1} & Q_{0}K_{0} \\
            Q_{1}K_{2-M} & \cdots & Q_{1}K_{-1} & Q_{1}K_{0} & Q_{1}K_{1} \\
            Q_{2}K_{3-M} & \cdots & Q_{2}K_{0} & Q_{2}K_{1} & Q_{2}K_{2} \\
            \vdots & \iddots & \vdots & \vdots & \vdots \\
            Q_{N}K_{N-M} & \cdots & Q_{N-1}K_{N-3} & Q_{N-1}K_{N-2} & Q_{N-1}K_{N-1} \\
            \end{matrix}\right].
    \end{equation}
    %
    Since the keys $K_{i<0}$ are forbidden values, we mask them by replacing them with $-\infty$ as in \citet{vaswani_attention_2017}, so that they are not computed in the softmax (see Eq. \ref{eq:online_attention}). The values $V_{i<0}$ are simply zeroed.

    To improve training, we developed a simple data augmentation protocol: first, the training set signals are sliced into time windows of $N_\mathrm{samples} = 2000$ samples (which corresponds to $1\,$s since the sampling rate is $2\,$kHz). Then, each time window is duplicated 64 times. For data augmentation, the beginning of each of this duplicated time window is shifted with a random number sampled in a uniform distribution between 0 and 2000. Finally, the resulting time windows are shuffled to create the training dataset.
    
    We trained each network for each subject for 10 epochs using the Adam optimizer \cite{kingma_adam_2017}, a learning rate of ${10}^{-3}$, batch sizes of 64, and since the metric we want to minimize is the mean average error (MAE) over the 5 degrees of freedom (DoA), we used the L1 loss function.
    
    For the sparse models, we added a sparsity loss function term \cite{yan_backpropagation_2022} to the global loss to increase the sparsity of the embedding, the queries, keys and values such that the total loss is:
    %
    \begin{equation}\label{eq:sparsity_loss}
        \mathcal{L}\left(y,\hat{y}\right)=\parallel y_{i,j}-\hat{y}_{i,j} \parallel_1
        -\frac{1}{2}\lambda\left(\parallel x \parallel_{2}+
        \parallel \mathrm{Concat}\left(Q,K,V\right) \parallel_{2}\right)
    \end{equation}
    %
    with $y$ the network outputs, $\hat{y}$ the targets, $x$ the embeddings and $\lambda=1$.
    
    \begin{comment}
    \begin{equation}\label{eq:L1_loss}
        MAE\left(y,\hat{y}\right)=\frac{1}{5N_{samples}} \sum^{5}_{i} \sum^{N_{samples}}_{j} \parallel y_{i,j}-\hat{y}_{i,j} \parallel_1
        .
    \end{equation}
    \end{comment}
    
    In this study, we simply trained and tested datasets independently for each subject. To improve accuracy and repeatability in future studies, it is also possible to use transfer learning: the network can learn from multiple subjects before fine-tuning and testing on a new subject, as in \cite{lehmler_deep_2022}.

\section{Results}
    
    In Fig. \ref{fig:emg} (c) we show an example of the regression results for a sparse online transformer with a embedding convolution kernel size $k = 7$ and a memory length $M = 150$.
    We first investigate how $k$ and $M$ affect the final accuracy. For this study, we use the network without sparsity. Since $s=k-2$, we simultaneously change $s$ and $k$, and thus the number of tokens $N$ generated for a given time window (see Eq. \ref{eq:n_tokens}). The memory length $M$ defines how many past tokens are used in the  attention mechanism. The time length of the signal used to store information in $K$ and $V$ is thus:
    %
    \begin{equation}\label{eq:t_memory}
        \tau_{\mathrm{memory}}=\frac{M\times s}{\mathrm{SamplingRate}}
        .
    \end{equation}
    %
    In Fig. \ref{fig:svs_sweep} we plot the mean absolute error (MAE) over the different degrees of actuation for values of $M$ swept between 10 and 150 with intervals of 20, and for five different values $k=$ 7, 15, 20, 25, and 30 (which correspond to $s=$ 5, 13, 18, 23 and 28). While sweeping $M$, we see for $k=$ 15, 20, 25, and 30 that the MAE reaches a minimum and then increases. It shows that there is then an optimum value of memory length for each kernel size, and we see that this optimum value decreases with the kernel size and thus with the stride. Using Eq. \ref{eq:t_memory}, this result indicates that there is an optimum length of information $\tau_\mathrm{memory}$ used in the attention mechanism, and that past that point increasing the stored information does not increase the accuracy. 
    %
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{img/results_svs_sweep_merged_MAE_degrees_test.png}
        \caption{Mean absolute error averaged over the 12 subjects versus the number of stored tokens $M$ for kernel embedding sizes of 7, 15, 20, 25, and 30 (from black to light yellow curves).}
        \label{fig:svs_sweep}
    \end{figure}
    %
    Then, we compare our different models using each time a kernel size $k = 7$ and a memory length is $M=150$. As we see in Fig \ref{fig:svs_sweep}, these parameters lead to the best accuracy using the shortest time window for each token. For this study we also measure the 10°-accuracy and the 15°-accuracy, which are respectively the proportion of time samples that lead to mean average errors inferior to ten and fifteen degrees \cite{koch_regression_2020}. These additional metrics are important to measure the accuracy of the prediction within a margin of error. The different results are shown in Table \ref{table:ninaprodb8_results}. The mean and standard deviation of each metric are computed over the 12 subjects of the NinaproDB8 dataset.
    
    To see the impact on  our online transformer with custom sliding window attention mechanism, we compare it to a conventional Transformer with self-attention. For the three metrics, our online transformers outperform the transformer with conventional self-attention (see Table \ref{table:ninaprodb8_results}). This results further reveals the importance of selecting relevant information, and that for sEMG signal processing, it is likely more important to use local information from the past than global information from both past and future. 
    
    % Note: this figure could be totally removed is space needed: redundant with the results table
    \begin{comment}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{img/results_networks.png}
        \caption{Regression results for three different transformer networks. Results are averaged over the 12 subjects and error bars represent the standard deviation.}
        \label{fig:3_metrics}
    \end{figure}
    \end{comment}
          
    \begin{table}\label{table:ninaprodb8_results}
      \caption{Results of regression on the Ninapro DB8 database with different models}
      \centering
      {\small  
      \begin{tabular}{llllll}
        \toprule
        %\multicolumn{2}{c}{Part}                   \\
        %\cmidrule(r){1-2}
        \textbf{Model} & \textbf{MAE (°)}$\downarrow$ & \textbf{10°-accuracy}$\uparrow$ & \textbf{15°-accuracy}$\uparrow$ & $\mathbf{\tau_{min}}\downarrow$ & \textbf{MMAC ops}$\downarrow$\\&&&&&\textbf{/inference}\\
        \toprule
        SVM$^{\ast}$ \cite{zanghieri_semg-based_2021} & 7.28 & 0.79 & 0.88 & 60 ms & --\\
        \midrule
        MLP$^{\ast}$ \cite{zanghieri_semg-based_2021} & 7.14 & 0.80 & 0.89 & 60 ms &  --\\
        \midrule
        TempConv \cite{zanghieri_semg-based_2021} & 6.89 & 0.81 & 0.90 & 128 ms & 3.2 \\
        \midrule
        LSTM \cite{koch_regression_2020} & 7.04 &  --&  --& 10 ms &  --\\
        \midrule
        Transformer$^{\dagger}$ & 6.62$\pm$1.52 & 0.87$\pm$0.07 & 0.92$\pm$0.05 & 2 s & 3,769 \\
        \midrule
        Online Transformer$^{\dagger}$ & \textbf{6.10$\pm$1.50} & \textbf{0.86$\pm$0.07} & \textbf{0.94$\pm$0.05} & \textbf{3.5 ms}& 5.3 \\
        \midrule
        Online Transformer with \\
        binary embedding and QKV, & \textbf{6.08$\pm$1.27} & \textbf{0.87$\pm$0.06} & \textbf{0.94$\pm$0.04} & \textbf{3.5 ms} & \textbf{1.4} \\ spiking FNN$^{\dagger}$ \\
        \midrule
        Online Transformer with \\
        binary embedding, & \textbf{6.16$\pm$1.39} & \textbf{0.87$\pm$0.07} & \textbf{0.94$\pm$0.04} & \textbf{3.5 ms} & \textbf{1.0} \\ spiking QKV and FNN$^{\dagger}$ \\
        \midrule
        $^{\ast}$ With prior feature extraction. & & & &\\
        $^{\dagger}$ This work.\\
        \bottomrule
      \end{tabular}}
    \end{table}
    %h
    Our two sparse models reach similar accuracy than our non-sparse online transformer (and thus also better accuracy than equivalent conventional transformer), and respectively lead to a reduction of the number of required Multiply-And-Accumulate operations (MAC ops) by factors of $3.8 \times$ and $5.3 \times$ compared to the non-sparse online transformer (the activation function operations are not included in these calculations). 
    The method used to compute the number of required operations is described in the Appendix.
    
    Moreover, we see that our three online transformer models outperform LSTMs \cite{koch_regression_2020} by at least 0.88° of MAE, outperform Temporal Convolutions \cite{zanghieri_semg-based_2021} with at least 0.76° of MAE (previous SoTA on NinaproDB8 dataset). To compare the inference speed of the different methods, we define the minimum time of computing as the length of the time windows used for each inference step, which for our online transformer is $ \tau_{\mathrm{min}}=\frac{k}{\mathrm{SamplingRate}}$,
    with $k$ the embedding convolution kernel size. Since $k=7$ and the sampling rate is 2 kHz, our network can compute with a minimum latency of 3.5 ms, which is shorter than any previous methods and in particular more than $30\times$ shorter than the Temporal Convolutional network \cite{zanghieri_semg-based_2021} which was the previous SoTA for the Ninapro DB8 dataset.
    
\section{Conclusion}
    In this work, we developed an online transformer model that leverages sliding window attention to process tokens one at the time. We have shown that the locality of the sliding window makes it more efficient than self-attention. The proposed method makes sEMG signal processing with very short time windows (3.5 ms) possible, and sets the new state-of-the-art on the prosthetic hand control NinaproDB8 dataset. 
    Using sliding window attention, our model also solves the problem of the integration of SNNs temporal dynamics in Transformers. 
    We used a combination of binarization and SNNs to increase the network sparsity, thus reducing the number of required operation up to a factor $5.3\times$. 
    In conclusion, this work is a step toward precise, smooth, and low-power Human-Machine Interfacing, and holds great promises for future neuromorphic transformer models. \blfootnote{Python codes available at https://github.com/NathanLeroux-git/OnlineTransformerWithSpikingNeurons}

\medskip

%\bibliography{biblio}
%\bibliographystyle{unsrtnat}

\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Zheng et~al.()Zheng, Crouch, and Eggleston]{zheng_surface_2022}
Mingde Zheng, Michael~S. Crouch, and Michael~S. Eggleston.
\newblock Surface electromyography as a natural human–machine interface: A
  review.
\newblock 22\penalty0 (10):\penalty0 9198--9214.
\newblock ISSN 1558-1748.
\newblock \doi{10.1109/JSEN.2022.3165988}.
\newblock Conference Name: {IEEE} Sensors Journal.

\bibitem[Vaswani et~al.()Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani_attention_2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc.
\newblock URL
  \url{https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Lin et~al.()Lin, Wang, Liu, and Qiu]{lin_survey_2022}
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
\newblock A survey of transformers.
\newblock 3:\penalty0 111--132.
\newblock ISSN 2666-6510.
\newblock \doi{10.1016/j.aiopen.2022.10.001}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S2666651022000146}.

\bibitem[Burrello et~al.()Burrello, Morghet, Scherer, Benatti, Benini, Macii,
  Poncino, and Pagliari]{burrello_bioformers_2022}
Alessio Burrello, Francesco~Bianco Morghet, Moritz Scherer, Simone Benatti,
  Luca Benini, Enrico Macii, Massimo Poncino, and Daniele~Jahier Pagliari.
\newblock Bioformers: Embedding transformers for ultra-low power {sEMG}-based
  gesture recognition.
\newblock In \emph{2022 Design, Automation \& Test in Europe Conference \&
  Exhibition ({DATE})}, pages 1443--1448.
\newblock \doi{10.23919/DATE54114.2022.9774639}.
\newblock {ISSN}: 1558-1101.

\bibitem[Zenke and Ganguli()]{zenke_superspike_2018}
Friedemann Zenke and Surya Ganguli.
\newblock {SuperSpike}: Supervised learning in multilayer spiking neural
  networks.
\newblock 30\penalty0 (6):\penalty0 1514--1541.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco_a_01086}.
\newblock URL \url{https://doi.org/10.1162/neco_a_01086}.

\bibitem[Tavanaei et~al.()Tavanaei, Ghodrati, Kheradpisheh, Masquelier, and
  Maida]{tavanaei_deep_2019}
Amirhossein Tavanaei, Masoud Ghodrati, Saeed~Reza Kheradpisheh, Timothée
  Masquelier, and Anthony Maida.
\newblock Deep learning in spiking neural networks.
\newblock 111:\penalty0 47--63.
\newblock ISSN 0893-6080.
\newblock \doi{10.1016/j.neunet.2018.12.002}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0893608018303332}.

\bibitem[Merolla et~al.()Merolla, Arthur, Alvarez-Icaza, Cassidy, Sawada,
  Akopyan, Jackson, Imam, Guo, Nakamura, Brezzo, Vo, Esser, Appuswamy, Taba,
  Amir, Flickner, Risk, Manohar, and Modha]{merolla_million_2014}
Paul~A. Merolla, John~V. Arthur, Rodrigo Alvarez-Icaza, Andrew~S. Cassidy, Jun
  Sawada, Filipp Akopyan, Bryan~L. Jackson, Nabil Imam, Chen Guo, Yutaka
  Nakamura, Bernard Brezzo, Ivan Vo, Steven~K. Esser, Rathinakumar Appuswamy,
  Brian Taba, Arnon Amir, Myron~D. Flickner, William~P. Risk, Rajit Manohar,
  and Dharmendra~S. Modha.
\newblock A million spiking-neuron integrated circuit with a scalable
  communication network and interface.
\newblock 345\penalty0 (6197):\penalty0 668--673.
\newblock ISSN 0036-8075, 1095-9203.
\newblock \doi{10.1126/science.1254642}.
\newblock URL \url{https://science.sciencemag.org/content/345/6197/668}.
\newblock Number: 6197 Publisher: American Association for the Advancement of
  Science Section: Report.

\bibitem[Liu et~al.()Liu, Bellec, Vogginger, Kappel, Partzsch, Neumärker,
  Höppner, Maass, Furber, Legenstein, and Mayr]{liu_memory-efficient_2018}
Chen Liu, Guillaume Bellec, Bernhard Vogginger, David Kappel, Johannes
  Partzsch, Felix Neumärker, Sebastian Höppner, Wolfgang Maass, Steve~B.
  Furber, Robert Legenstein, and Christian~G. Mayr.
\newblock Memory-efficient deep learning on a {SpiNNaker} 2 prototype.
\newblock 12.
\newblock ISSN 1662-453X.
\newblock URL
  \url{https://www.frontiersin.org/articles/10.3389/fnins.2018.00840}.

\bibitem[Orchard et~al.()Orchard, Frady, Rubin, Sanborn, Shrestha, Sommer, and
  Davies]{orchard_efficient_2021}
Garrick Orchard, E.~Paxon Frady, Daniel Ben~Dayan Rubin, Sophia Sanborn,
  Sumit~Bam Shrestha, Friedrich~T. Sommer, and Mike Davies.
\newblock Efficient neuromorphic signal processing with loihi 2.
\newblock In \emph{2021 {IEEE} Workshop on Signal Processing Systems ({SiPS})},
  pages 254--259.
\newblock \doi{10.1109/SiPS52927.2021.00053}.
\newblock {ISSN}: 2374-7390.

\bibitem[Pehle et~al.()Pehle, Billaudelle, Cramer, Kaiser, Schreiber,
  Stradmann, Weis, Leibfried, Müller, and Schemmel]{pehle_brainscales-2_2022}
Christian Pehle, Sebastian Billaudelle, Benjamin Cramer, Jakob Kaiser,
  Korbinian Schreiber, Yannik Stradmann, Johannes Weis, Aron Leibfried, Eric
  Müller, and Johannes Schemmel.
\newblock The {BrainScaleS}-2 accelerated neuromorphic system with hybrid
  plasticity.
\newblock 16:\penalty0 795876.
\newblock ISSN 1662-4548.
\newblock \doi{10.3389/fnins.2022.795876}.
\newblock URL \url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8907969/}.

\bibitem[Beltagy et~al.()Beltagy, Peters, and Cohan]{beltagy_longformer_2020}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock URL \url{http://arxiv.org/abs/2004.05150}.

\bibitem[Krasoulis et~al.()Krasoulis, Vijayakumar, and
  Nazarpour]{krasoulis_effect_2019}
Agamemnon Krasoulis, Sethu Vijayakumar, and Kianoush Nazarpour.
\newblock Effect of user practice on prosthetic finger control with an
  intuitive myoelectric decoder.
\newblock 13.
\newblock ISSN 1662-453X.
\newblock URL
  \url{https://www.frontiersin.org/articles/10.3389/fnins.2019.00891}.

\bibitem[Jaramillo-Yánez et~al.()Jaramillo-Yánez, Benalcázar, and
  Mena-Maldonado]{jaramillo-yanez_real-time_2020}
Andrés Jaramillo-Yánez, Marco~E. Benalcázar, and Elisa Mena-Maldonado.
\newblock Real-time hand gesture recognition using surface electromyography and
  machine learning: A systematic literature review.
\newblock 20\penalty0 (9):\penalty0 2467.
\newblock ISSN 1424-8220.
\newblock \doi{10.3390/s20092467}.
\newblock URL \url{https://www.mdpi.com/1424-8220/20/9/2467}.
\newblock Number: 9 Publisher: Multidisciplinary Digital Publishing Institute.

\bibitem[Milosevic et~al.()Milosevic, Farella, and
  Benatti]{milosevic_exploring_2018}
Bojan Milosevic, Elisabetta Farella, and Simone Benatti.
\newblock Exploring arm posture and temporal variability in myoelectric hand
  gesture recognition.
\newblock In \emph{2018 7th {IEEE} International Conference on Biomedical
  Robotics and Biomechatronics (Biorob)}, pages 1032--1037.
\newblock \doi{10.1109/BIOROB.2018.8487838}.
\newblock {ISSN}: 2155-1782.

\bibitem[Phukpattaranont et~al.()Phukpattaranont, Thongpanja, Anam, Al-Jumaily,
  and Limsakul]{phukpattaranont_evaluation_2018}
Pornchai Phukpattaranont, Sirinee Thongpanja, Khairul Anam, Adel Al-Jumaily,
  and Chusak Limsakul.
\newblock Evaluation of feature extraction techniques and classifiers for
  finger movement recognition using surface electromyography signal.
\newblock 56\penalty0 (12):\penalty0 2259--2271.
\newblock ISSN 1741-0444.
\newblock \doi{10.1007/s11517-018-1857-5}.
\newblock URL \url{https://doi.org/10.1007/s11517-018-1857-5}.

\bibitem[Taghizadeh et~al.()Taghizadeh, Rashidi, and
  Shalbaf]{taghizadeh_finger_2021}
Zahra Taghizadeh, Saeid Rashidi, and Ahmad Shalbaf.
\newblock Finger movements classification based on fractional fourier transform
  coefficients extracted from surface {EMG} signals.
\newblock 68:\penalty0 102573.
\newblock ISSN 1746-8094.
\newblock \doi{10.1016/j.bspc.2021.102573}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S1746809421001701}.

\bibitem[Tsinganos et~al.()Tsinganos, Cornelis, Cornelis, Jansen, and
  Skodras]{tsinganos_improved_2019}
Panagiotis Tsinganos, Bruno Cornelis, Jan Cornelis, Bart Jansen, and
  Athanassios Skodras.
\newblock Improved gesture recognition based on {sEMG} signals and {TCN}.
\newblock In \emph{{ICASSP} 2019 - 2019 {IEEE} International Conference on
  Acoustics, Speech and Signal Processing ({ICASSP})}, pages 1169--1173.
\newblock \doi{10.1109/ICASSP.2019.8683239}.
\newblock {ISSN}: 2379-190X.

\bibitem[Zanghieri et~al.({\natexlab{a}})Zanghieri, Benatti, Burrello, Kartsch,
  Conti, and Benini]{zanghieri_robust_2020}
Marcello Zanghieri, Simone Benatti, Alessio Burrello, Victor Kartsch, Francesco
  Conti, and Luca Benini.
\newblock Robust real-time embedded {EMG} recognition framework using temporal
  convolutional networks on a multicore {IoT} processor.
\newblock 14\penalty0 (2):\penalty0 244--256, {\natexlab{a}}.
\newblock ISSN 1940-9990.
\newblock \doi{10.1109/TBCAS.2019.2959160}.
\newblock Conference Name: {IEEE} Transactions on Biomedical Circuits and
  Systems.

\bibitem[Zanghieri et~al.({\natexlab{b}})Zanghieri, Benatti, Burrello,
  Kartsch~Morinigo, Meattini, Palli, Melchiorri, and
  Benini]{zanghieri_semg-based_2021}
Marcello Zanghieri, Simone Benatti, Alessio Burrello, Victor~Javier
  Kartsch~Morinigo, Roberto Meattini, Gianluca Palli, Claudio Melchiorri, and
  Luca Benini.
\newblock {sEMG}-based regression of hand kinematics with temporal
  convolutional networks on a low-power edge microcontroller.
\newblock In \emph{2021 {IEEE} International Conference on Omni-Layer
  Intelligent Systems ({COINS})}, pages 1--6, {\natexlab{b}}.
\newblock \doi{10.1109/COINS51742.2021.9524188}.

\bibitem[Anam et~al.()Anam, Avian, Swasono, Muttaqin, and
  Ismail]{anam_estimation_2020}
Khairul Anam, Cries Avian, Dwiretno~Istiyadi Swasono, Aris~Zainul Muttaqin, and
  Harun Ismail.
\newblock Estimation of finger joint movement based on electromyography signal
  using long short-term memory.
\newblock In \emph{2020 International Conference on Computer Engineering,
  Network, and Intelligent Multimedia ({CENIM})}, pages 86--90.
\newblock \doi{10.1109/CENIM51130.2020.9298023}.

\bibitem[Koch et~al.()Koch, Dreier, Larsen, Parbs, Maass, Phan, and
  Mertins]{koch_regression_2020}
Philipp Koch, Mark Dreier, Anna Larsen, Tim~J. Parbs, Marco Maass, Huy Phan,
  and Alfred Mertins.
\newblock Regression of hand movements from {sEMG} data with recurrent neural
  networks.
\newblock In \emph{2020 42nd Annual International Conference of the {IEEE}
  Engineering in Medicine \& Biology Society ({EMBC})}, pages 3783--3787.
\newblock \doi{10.1109/EMBC44109.2020.9176278}.
\newblock {ISSN}: 2694-0604.

\bibitem[Ilyas et~al.()Ilyas, Anam, {Widjonarko}, Avian, Muttaqin, and
  Ramadhan]{ilyas_evaluation_2022}
Zamroni Ilyas, Khairul Anam, {Widjonarko}, Cries Avian, Aris~Zainul Muttaqin,
  and Mochammad~Edoward Ramadhan.
\newblock Evaluation of gated-recurrent unit for estimating finger-joint angle
  using surface electromyography signal.
\newblock In \emph{2022 9th International Conference on Electrical Engineering,
  Computer Science and Informatics ({EECSI})}, pages 25--28.
\newblock \doi{10.23919/EECSI56542.2022.9946461}.

\bibitem[Li et~al.({\natexlab{a}})Li, Wei, Wen, Liu, and
  Wang]{li_approach_2022}
Jun Li, Lixin Wei, Yintang Wen, Xiaoguang Liu, and Hongrui Wang.
\newblock An approach to continuous hand movement recognition using {SEMG}
  based on features fusion.
\newblock {\natexlab{a}}.
\newblock ISSN 0178-2789, 1432-2315.
\newblock \doi{10.1007/s00371-022-02465-7}.
\newblock URL \url{https://link.springer.com/10.1007/s00371-022-02465-7}.

\bibitem[Mukhopadhyay et~al.()Mukhopadhyay, Chakrabarti, and
  Sharad]{mukhopadhyay_classification_2018}
Anand~Kumar Mukhopadhyay, Indrajit Chakrabarti, and Mrigank Sharad.
\newblock Classification of hand movements by surface myoelectric signal using
  artificial-spiking neural network model.
\newblock In \emph{2018 {IEEE} {SENSORS}}, pages 1--4.
\newblock \doi{10.1109/ICSENS.2018.8589757}.
\newblock {ISSN}: 2168-9229.

\bibitem[Hassani and Shi()]{hassani_dilated_2022}
Ali Hassani and Humphrey Shi.
\newblock Dilated neighborhood attention transformer.
\newblock URL \url{http://arxiv.org/abs/2209.15001}.

\bibitem[Yao et~al.()Yao, Gao, Zhao, Wang, Lin, Yang, and
  Li]{yao_temporal-wise_2021}
Man Yao, Huanhuan Gao, Guangshe Zhao, Dingheng Wang, Yihan Lin, Zhaoxu Yang,
  and Guoqi Li.
\newblock Temporal-wise attention spiking neural networks for event streams
  classification.
\newblock pages 10221--10230.
\newblock URL
  \url{https://openaccess.thecvf.com/content/ICCV2021/html/Yao_Temporal-Wise_Attention_Spiking_Neural_Networks_for_Event_Streams_Classification_ICCV_2021_paper.html}.

\bibitem[Sabater et~al.()Sabater, Montesano, and Murillo]{sabater_event_2022}
Alberto Sabater, Luis Montesano, and Ana~C. Murillo.
\newblock Event transformer. a sparse-aware solution for efficient event data
  processing.
\newblock In \emph{2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern
  Recognition Workshops ({CVPRW})}, pages 2676--2685. {IEEE}.
\newblock ISBN 978-1-66548-739-9.
\newblock \doi{10.1109/CVPRW56347.2022.00301}.
\newblock URL \url{https://ieeexplore.ieee.org/document/9857382/}.

\bibitem[Zhou et~al.()Zhou, Zhu, He, Wang, Yan, Tian, and
  Yuan]{zhou_spikformer_2022}
Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian,
  and Li~Yuan.
\newblock Spikformer: When spiking neural network meets transformer.
\newblock URL \url{http://arxiv.org/abs/2209.15425}.

\bibitem[Li et~al.({\natexlab{b}})Li, Lei, and Yang]{li_spikeformer_2022}
Yudong Li, Yunlin Lei, and Xu~Yang.
\newblock Spikeformer: A novel architecture for training high-performance
  low-latency spiking neural network, {\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/2211.10686}.

\bibitem[Gehrig and Scaramuzza()]{gehrig_recurrent_2022}
Mathias Gehrig and Davide Scaramuzza.
\newblock Recurrent vision transformers for object detection with event
  cameras.
\newblock URL \url{http://arxiv.org/abs/2212.05598}.

\bibitem[Zhu et~al.()Zhu, Zhao, and Eshraghian]{zhu_spikegpt_2023}
Rui-Jie Zhu, Qihang Zhao, and Jason~K. Eshraghian.
\newblock {SpikeGPT}: Generative pre-trained language model with spiking neural
  networks.
\newblock URL \url{http://arxiv.org/abs/2302.13939}.

\bibitem[Pizzolato et~al.()Pizzolato, Tagliapietra, Cognolato, Reggiani,
  Müller, and Atzori]{pizzolato_comparison_2017}
Stefano Pizzolato, Luca Tagliapietra, Matteo Cognolato, Monica Reggiani,
  Henning Müller, and Manfredo Atzori.
\newblock Comparison of six electromyography acquisition setups on hand
  movement classification tasks.
\newblock 12\penalty0 (10):\penalty0 e0186132.
\newblock ISSN 1932-6203.
\newblock \doi{10.1371/journal.pone.0186132}.
\newblock URL \url{https://dx.plos.org/10.1371/journal.pone.0186132}.

\bibitem[Markovic et~al.()Markovic, Schweisfurth, Engels, Farina, and
  Dosen]{markovic_myocontrol_2018}
Marko Markovic, Meike~A. Schweisfurth, Leonard~F. Engels, Dario Farina, and
  Strahinja Dosen.
\newblock Myocontrol is closed-loop control: incidental feedback is sufficient
  for scaling the prosthesis force in routine grasping.
\newblock 15\penalty0 (1):\penalty0 81.
\newblock ISSN 1743-0003.
\newblock \doi{10.1186/s12984-018-0422-7}.
\newblock URL \url{https://doi.org/10.1186/s12984-018-0422-7}.

\bibitem[Hendrycks and Gimpel()]{hendrycks_gaussian_2020}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units ({GELUs}).
\newblock URL \url{http://arxiv.org/abs/1606.08415}.

\bibitem[Neftci et~al.()Neftci, Mostafa, and Zenke]{neftci_surrogate_2019}
Emre~O. Neftci, Hesham Mostafa, and Friedemann Zenke.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the
  power of gradient-based optimization to spiking neural networks.
\newblock 36\penalty0 (6):\penalty0 51--63.
\newblock ISSN 1558-0792.
\newblock \doi{10.1109/MSP.2019.2931595}.
\newblock Number: 6 Conference Name: {IEEE} Signal Processing Magazine.

\bibitem[Kingma and Ba()]{kingma_adam_2017}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Yan et~al.()Yan, Chu, Jin, Huan, Zou, and
  Zheng]{yan_backpropagation_2022}
Yulong Yan, Haoming Chu, Yi~Jin, Yuxiang Huan, Zhuo Zou, and Lirong Zheng.
\newblock Backpropagation with sparsity regularization for spiking neural
  network learning.
\newblock 16.
\newblock ISSN 1662-453X.
\newblock URL
  \url{https://www.frontiersin.org/articles/10.3389/fnins.2022.760298}.

\bibitem[Lehmler et~al.()Lehmler, Saif-ur Rehman, Glasmachers, and
  Iossifidis]{lehmler_deep_2022}
Stephan~Johann Lehmler, Muhammad Saif-ur Rehman, Tobias Glasmachers, and
  Ioannis Iossifidis.
\newblock Deep transfer learning compared to subject-specific models for {sEMG}
  decoders.
\newblock ISSN 1741-2560, 1741-2552.
\newblock \doi{10.1088/1741-2552/ac9860}.
\newblock URL
  \url{https://iopscience.iop.org/article/10.1088/1741-2552/ac9860}.

\end{thebibliography}


\appendix

\section{Appendix}

To compute the number of synaptic MAC operations of our different models, we used the following set of equations, that represent the different operations (activation function excluded) of our models:

\begin{subequations}
    \begin{align}  
        \mathrm{\#EmbeddingMACs}     &= k\times C \times D \times 32 \label{eq:shsb} \\
        \mathrm{\#QKVProjectionMACs} &= \left(1-\mathrm{EmbeddingSparsity}\right) \times 3 \times D \times d \times h \times 32 \\
        \mathrm{\#QKProductMACs}     &=\parallel QK \parallel_1 \times M \times h \times 32 \\
        \mathrm{\#VProductMACs}      &=\left(1-\mathrm{VSparsity}\right) \times d \times M \times h \times 32 \\
        \mathrm{\#ConcatMACs}        &=\left(1-\mathrm{AttentionSparsity}\right) \times d \times h \times D \times 32 \\
        \mathrm{\#TotalAttentionMACs}    &=\mathrm{\#QKVProjectionMac}+ \mathrm{\#QKProductMacs}  \\
        &                     +\mathrm{\#VProductMacs}+\mathrm{\#ConcatMACs} \nonumber \\    
        \mathrm{\#FFL1MACs}           &=D \times H \times 32 \\
        \mathrm{\#FFL2MACs}           &=\left(1-\mathrm{FFL1Sparsity}\right) \times H \times D \times 32 \\
        \mathrm{\#RegressionMACs}     &=D \times 5 \times 32  \\    
        \mathrm{\#TotalMACs}          &=\mathrm{\#EmbeddingMacs}+\mathrm{\#TotalAttentionMACs} \\
        &                     +\mathrm{\#FFL1MACs}+\mathrm{\#FFL2MACs}+\mathrm{\#RegressionMACs} \nonumber
    \end{align}
\end{subequations}


\begin{comment}
The sparsity of a tensor $x$ of a number of elements $N_{elements}$ is defined by

\begin{equation}
    sparsity(x) = 1-\frac{\sum_{i}^{N_{elements}}x_{i}}{N_{elements}}
\end{equation}
\end{comment}

\end{document}
