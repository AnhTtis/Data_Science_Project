\section{Experiments}

\begin{table}[thb]
\centering
\caption{Results on CIFAR-10, CIFAR-100, and TinyImageNet datasets.}
\label{tab:results-combined}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|cc|cc|cc@{}}
\toprule
\multirow{3}{*}{Method} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c|}{CIFAR-100} & \multicolumn{2}{c}{TinyImageNet} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
                        & 200 epochs & 400 epochs       & 200 epochs & 400 epochs       & 200 epochs & 400 epochs       \\ \midrule
SimCLR (repro.)         & $88.13$    & $90.59$          & $62.67$    & $66.23$          & $34.03$    & $37.86$          \\
\midrule
Laplacian Kernel        & $89.31$    & $91.05$          & $63.17$    & $66.06$          & $35.92$    & $38.76$          \\
$\gamma=0.5$ Exponential Kernel & $89.00$ & $91.23$      & $63.47$    & $65.71$          & $34.21$    & $38.70$          \\
\textbf{Simple Sum Kernel} & $89.80$   & $\mathbf{91.76}$ & $\mathbf{66.73}$ & $\mathbf{68.62}$ & $\textbf{36.60}$ & $\textbf{39.38}$ \\
Concatenation Sum Kernel & $\mathbf{89.89}$ & $91.28$   & $66.09$    & $68.53$          & $35.92$    & $38.76$          \\
\midrule
\end{tabular}
}
\end{table}



In our experiments, we reproduce the baseline algorithm SimCLR ~\citep{chen2020simple}, and replace SimCLR's Gaussian kernel with other kernels. We then test against SimCLR using Kernel-InfoNCE loss on various benchmark vision datasets.

\paragraph{CIFAR-10 and CIFAR-100} CIFAR-10 ~\citep{krizhevsky2009learning} and CIFAR-100 ~\citep{krizhevsky2009learning} are well-known classic image classification datasets. Both CIFAR-10 and CIFAR-100 contain a total of 60k $32 \times 32$ labeled images of different classes, with 50k for training and 10k for testing. CIFAR-10 is similar to CIFAR-100, except there are 10 different classes in CIFAR-10 and 100 classes in CIFAR-100.

\paragraph{TinyImageNet} TinyImageNet ~\citep{le2015tiny} is a subset of ImageNet ~\citep{deng2009imagenet}. There are 200 different object classes in TinyImageNet, with 500 training images, 50 validation images, and 50 test images for each class. All the images in TinyImageNet are colored and labeled with a size of $64 \times 64$.

For each algorithm, we first train an encoder $f$ on the training dataset to minimize the empirical loss function generated by the kernel. Then, following the standard linear evaluation protocol, we freeze the encoder $f$ and train a supervised linear classifier, which takes the output representation of $f$ as input. Additional experimental details can be found in Appendix~\ref{section: experiment_details}.

\paragraph{Experimental Results}
We summarize our empirical results on various benchmark datasets in Table~\ref{tab:results-combined}. It is evident that we have achieved better performance than SimCLR on all three benchmark datasets, with the Simple Sum Kernel reaching the best average performance.

