\section{Introduction}
Contrastive learning has emerged as one of the most prominent self-supervised learning methods, especially in the realm of vision tasks~\citep{chen2020simple,he2019momentum}. This approach trains a neural network to map a set of objects into an embedding space, ensuring that similar objects are closely positioned while dissimilar objects remain distanced. The InfoNCE loss, exemplified by SimCLR~\citep{chen2020simple}, is a widely employed loss function for achieving this goal.

In their inspiring work, \citet{haochen2021provable} demonstrated that by replacing the standard InfoNCE loss with their spectral contrastive loss, contrastive learning performs spectral clustering on the population augmentation graph. However, the spectral contrastive loss is seldom utilized in practice and is not applicable for analyzing the performance of various similarity functions in the embedding space. Furthermore, when employing the spectral contrastive loss, the final embedding constitutes a combination of standard spectral clustering and an additional linear transformation. Consequently, existing results do not establish a connection between the original InfoNCE loss and standard spectral clustering.



\begin{figure*}
\input{contents/1.tex}
\caption{An illustration of our analysis.}
\label{fig:illustration}
\end{figure*}

In this paper, we prove that SimCLR, the standard contrastive learning method, performs spectral clustering without modifying the InfoNCE loss or applying additional transformations to the embeddings. Our analysis involves a collection of $n$ objects $\bfX=[\bfX_1, \cdots, \bfX_n]$ within space $\cX$. For these objects, we define a similarity graph with an adjacency matrix $\bfpi$, such that $\bfpi_{i,j}$ represents the probability of $\bfX_i$ and $\bfX_j$ being paired together in the data augmentation step of contrastive learning. Notice that $\bfpi$ can be in general asymmetric. 

Given this similarity graph, we aim to discover an embedding function $f:\cX\rightarrow \cZ$. Denote $\bfZ \triangleq f(\bfX)$ as the embedding of $\bfX$, and our objective is to ensure that the Gram matrix $\bfK_\bfZ$ with kernel $k$ representing the similarities for $\bfZ$ closely approximates $\bfpi$. Please refer to Figure~\ref{fig:illustration} for an illustration.

However, directly comparing $\bfpi$ with $\bfK_\bfZ$ can be difficult, as there are too many edges in both graphs. Therefore, we define two Markov random fields (MRFs) based on $\bfpi$ and $\bfK_\bfZ$ and compare the MRFs instead. Each MRF introduces a probability distribution of unweighted directed subgraphs on $n$ objects~\citep{van2022probabilistic}, denoted as $\bfW_\bfX$ and $\bfW_\bfZ$ respectively. As a natural approximation to the ideal loss between $\bfpi$ and $\bfK_\bfZ$, we employ the cross-entropy loss between $\bfW_\bfX$ and $\bfW_\bfZ$. Our paper's surprising discovery is that the InfoNCE loss is equivalent to the cross-entropy loss when each subgraph is constrained to have an out-degree of exactly one. Furthermore, when $k$ is the Gaussian kernel, optimizing the cross-entropy loss corresponds to executing spectral clustering on $\bfpi$. By combining these two observations, we conclude that employing the InfoNCE loss is equivalent to performing spectral clustering.


Our characterization of contrastive learning hinges on two crucial factors: the augmentation step that defines a similarity graph, and the InfoNCE loss that measures the distance between two MRFs. Consequently, any other models incorporating these two factors can be similarly analyzed. Notably, the CLIP~\citep{radford2021learning} model for multi-modal learning fits within this paradigm. Utilizing the same framework, we establish a representation theorem for CLIP, demonstrating that it performs spectral clustering on the bipartite graph induced by the paired training data.

Is it possible to improve the InfoNCE loss by using a different kernel? Based on the maximum entropy principle, we demonstrate that the exponential kernels are the natural choices for capturing the local similarity structure for contrastive learning.
Empirically, we observe that 
taking the mixture of Gaussian and Laplacian kernels, which maintain the aforementioned properties, can achieve better performance than the Gaussian kernel on several benchmark vision datasets. 

In summary, our main contributions include:
\begin{itemize} [itemsep=0\baselineskip, topsep=0\baselineskip, leftmargin=0.5cm]
\item We prove the equivalence of SimCLR and spectral clustering on the similarity graph.

\item We extend our analysis to the multi-modal setting and prove the equivalence of CLIP and spectral clustering on the multi-modal similarity graph.

\item Inspired by theory, we propose a new Kernel-InfoNCE loss with mixture of kernel functions that achieves better performance than the standard Gaussian kernel (SimCLR) empirically on the benchmark vision datasets. 
\end{itemize}

