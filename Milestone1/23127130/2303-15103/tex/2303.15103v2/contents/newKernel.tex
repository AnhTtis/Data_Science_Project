\section{Use New Kernels}
%We have seen that the InfoNCE loss is equivalent to Eqn.~(\ref{eqn:spectral}) for various settings. Can we replace the Gaussian kernel  in Eqn.~(\ref{eqn:spectral}) with new kernels for better performance? In this section, we examine InfoNCE-like losses with the maximum entropy principle, and then consider the mixture of Gaussian and Laplacian kernels as potential alternatives.

\subsection{Maximum entropy principle}
\label{sec:max-entropy}

Can we replace the Gaussian kernel in Eqn.~(\ref{eqn:spectral}) with new kernels for improved performance? In this subsection, we offer an interpretation of InfoNCE-like loss, suggesting that exponential kernels are natural choices for our setting. Given a contrastive sample $\mathbf{q}$, let $\psi_i$ represent the similarity between $\mathbf{q}$ and the contrastive sample $\mathbf{p}_i$ for $i \in [n]$, computed by a kernel $k$. Without loss of generality, assume $\mathbf{p}_1$ is the neighbor of $\mathbf{q}$ according to prior knowledge, but $\psi_1$ is not necessarily the largest value in ${\psi_i}$. Ideally, we desire $\psi_1$ to be the largest or at least among the few largest similarities, which indicates that our kernel properly aligns with the prior knowledge of $\cX$.

To optimize toward this goal, we must design a loss function that captures the ranking of $\psi_1$. Since the ordering function is discrete and lacks gradient information, we need to convert it into a soft and continuous function that enables gradient-based optimization. Specifically, we employ a probability distribution $\boldsymbol{\alpha}$ to represent the neighborhood structure of $\mathbf{q}$ in relation to $\psi_1$, satisfying $\psi_{1}\leq \sum_{i=1}^n \alpha_{i} \psi_{i}$, and $\forall i, \alpha_i\geq 0$. If $\psi_1$ is the largest, $\boldsymbol{\alpha}=e_1$ is the sole solution; otherwise, $\boldsymbol{\alpha}$ can be more diverse. For instance, when all $\psi_i$ values are equal, $\boldsymbol{\alpha}$ can be a uniform distribution.

Intuitively, if there are numerous other $\psi_i$ values similar to $\psi_1$, the neighborhood structure of $\mathbf{q}$ is not as optimal as when $\psi_1$ is the only object close to $\mathbf{q}$. Formally, this means $\boldsymbol{\alpha}$ should have fewer non-zero entries or at least concentrate on $\boldsymbol{\alpha}1$. We use its entropy $H(\boldsymbol{\alpha})= - \sum{i=1}^n \alpha_{i}\log \alpha_{i}$ to represent this diversity, which results in the following optimization problem.
$$
\begin{array}{ccl}
(\text{P1})& \max _{\boldsymbol{\alpha}} & H(\boldsymbol{\alpha}) \\
&\text { s.t. } & \boldsymbol{\alpha}^{\top} \mathbf{1}_n=1, \alpha_1, \ldots, \alpha_n \geq 0 \\
&& \psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i} \leq 0
\end{array}
$$
By minimizing the solution of (P1), we can discover an embedding that more accurately approximates the prior knowledge. However, how can we solve (P1)? By introducing the Lagrangian dual variable $\tau>0$, we obtain the subsequent program (P2) that generates an upper bound of (P1). Consequently, minimizing (P2) simultaneously produces a smaller upper bound of (P1) as well, indirectly aiding us in achieving our objective.
$$
\begin{array}{ccl}
(\text{P2})& \max _{\boldsymbol{\alpha}} & -E(\boldsymbol{\alpha}) \\
&\text { s.t. } & \boldsymbol{\alpha}^{\top} \mathbf{1}_n=1, \alpha_1, \ldots, \alpha_n \geq 0
\end{array}
$$
where
$
E(\boldsymbol{\alpha})=\psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i}+\tau \sum_{i=1}^n \alpha_{i}\log \alpha_{i}.
$

We present the following theorem for solving (P2).

\begin{theorem}[Exponential kernels are natural]
\label{thm:exponential}
The solution of (P2) satisfies: 
$$-E\left(\boldsymbol{\alpha}^*\right)= -\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}.$$
\end{theorem}

For proof, please refer to Appendix~\ref{proof:exponential}.

Using this framework, we can derive results akin to the max-min optimization formulation from \cite{tian2022understanding} as a corollary.

Theorem~\ref{thm:exponential} suggests that the loss function of the form $-\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}$ is a natural choice for characterizing the neighborhood similarity structure. In other words, we should employ exponential kernels defined as follows:

\begin{equation}
K_{\exp }^{\gamma, \sigma}(x, y) \triangleq \exp \left(-\frac{\|x-y\|^\gamma}{\sigma}\right) \quad (\gamma, \sigma>0)
\end{equation}

We define our kernel-based contrastive loss, Kernel-InfoNCE, as follows:
\begin{equation}
\label{eqn:kernel-infonce}
\mathcal{L}^{\gamma, \sigma}_{\text{Kernel-InfoNCE}}(\mathbf{q}, \mathbf{p}_1, \{\mathbf{p}_i\}_{i=2}^{N})\triangleq-\log \frac{K^{\gamma, \sigma}_{\exp}(\mathbf{q}, \mathbf{p}_1)}{\sum_{i=1}^N K^{\gamma, \sigma}_{\exp}(\mathbf{q}, \mathbf{p}_i)
}
\end{equation}

%Below we further investigate the kernels from  expressive power and kernel mixture. 

% \subsection{Expressive power}
% Based on Definition~\ref{def:rkhs}, we know the size of $\mathcal{H}$ depends on how $k$ is defined. For different exponential kernels, which one is more expressive? Specifically, 
% can we find another kernel $k'$ so that $\mathcal{H}_{k'}$ is larger than  $\mathcal{H}_{\mathrm{Gauss}}$?
% The answer is affirmative by the following Theorem. 

% \begin{theorem}[\citet{chen2020deep}]
% Consider exponential kernels restricted to the unit sphere $\mathbb{S}^{d-1}$ and defined on the entire $\mathbb{R}^d$, respectively. Then we have the following RKHS inclusions:
% If $0<\gamma\leq 1$, for any $\sigma>0$,
% $
% \mathcal{H}_{\mathrm{Gauss}}\left(\mathbb{S}^{d-1}\right) \subseteq \mathcal{H}_{K_{\mathrm{exp}}^{\gamma, \sigma}}\left(\mathbb{S}^{d-1}\right)
% $ and $
% \mathcal{H}_{\mathrm{Gauss}}\left(\mathbb{R}^d\right) \subseteq \mathcal{H}_{K_{\mathrm{exp}}^{\gamma, \sigma}}\left(\mathbb{R}^d\right)
% $. 
% \end{theorem}

% This implies, at least from the perspective of expressive power, picking a smaller number on the exponential is better. Note that when $\gamma=1$, we get the Laplacian kernel. 

\subsection{Kernel Mixture}
%In the previous subsection, we have seen that exponential kernels have many attractive properties. 

In this subsection, we consider taking the mixture of two exponential kernels as potential candidates for replacing the Gaussian kernel. There are two kinds of mixing methods. The first one is taking the weighted average of two positive definite kernels. 
%by the following fact.  
%\begin{fact}
%If $k_i: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{C}(i=1,2, \ldots)$ are strictly positive definite kernels, then 
%the kernel $ak_1+bk_2$ for $a, b \geq 0, ab > 0$, is also strictly positive definite. 
%\end{fact}
The other mixing method is concatenation, which splits the input vectors into two parts, where the first part uses the first kernel, and the second part uses the second kernel. It is easy to see that both mixing methods maintain the strictly positive definite property of the base kernels. We list the two types of kernel mixtures below.

\textbf{Simple Sum Kernel:}
\begin{align*}
K(x_i, x_j) \triangleq & \exp(-\|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|^{2}_{2} / \tau_2) + \exp(-\|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|^{1}_{2} / \tau_1)
\end{align*}
\textbf{Concatenation Sum Kernel:}
\begin{align*}
K(x_i, x_j) \triangleq & \exp(-\|f(\mathbf{x}_i)[0:n] - f(\mathbf{x}_j)[0:n]\|^{2}_{2} / \tau_2) + \exp(-\|f(\mathbf{x}_i)[n:2n] - f(\mathbf{x}_j)[n:2n]\|^{1}_{2} / \tau_1)
\end{align*}




