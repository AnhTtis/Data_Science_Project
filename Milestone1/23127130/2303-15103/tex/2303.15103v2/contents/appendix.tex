%%%%%%%%%%%%%%%%%%%%
%%%
% \section{Augmented CLIP}
% \label{sec:augmented}
% \begin{algorithm}[tb]
%    \caption{Augmented CLIP}
%    \label{alg:augment-clip}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} 
%    $\bbfpi_{\bA, \bB} $
%    \REPEAT
%    \STATE For every object $\ai$, sample its neighbor $\bi'$ from $\bbfpi_{\bA, \bB}$.
%    \STATE For every object $\bi$, sample its neighbor $\ai'$ from $\bbfpi_{\bA, \bB}$.
%    \STATE Train $f$ once on $\{(\ai,\bi')_{i=1}^{n_\bA}, 
%    (\ai',\bi)_{i=1}^{n_\bB}\}$ with loss~(\ref{eqn:infonce}).
%    \UNTIL{$f$ Converges}
% \end{algorithmic}
% \end{algorithm}



% Can we modify CLIP so that it can produce semantically meaningful spectral clustering results, rather than depending on empirically verified network structures? This is possible if we have the data augmentation prior knowledge for both modalities. 
% Formally, we define the augmented pair graph with the prior knowledge as follows. 
% \begin{definition}[Augmented pair graph]
% Given two similarity graphs $\bfpi_\mathbf{A}\in \mathbb{R}_+^{n_\mathbf{A}\times n_\mathbf{A}}$,  
% $\bfpi_\mathbf{B}\in \mathbb{R}_+^{n_\mathbf{B}\times n_\mathbf{B}}$ representing the similarities in $\mathbf{A}$ and $\bB$ respectively, we augment each edge $\mathbf{E}_i=(\ai, \bi, p_i)$ in $\bfpi_{\bA, \bB}\in \mathbb{R}_+^{(n_\bA+n_\bB)\times(n_\bA+n_\bB)}$ to be $ \mathbf{\bar E}_i\triangleq \{
% (\bai, \bbi, p_i\cdot \bfpi_{\bA}(\ai, \bai)\cdot \bfpi_{\bB}(\bi, \bbi))~|~ 
% \bai \sim \bfpi_{\bA}(\ai), 
% \bbi \sim \bfpi_{\bB}(\bi) 
% \}$, where $\bfpi_{\bA}(\ai)$ denotes the probability distribution of augmenting $\ai$ in $\bA$,  
% $\bfpi_{\bA}(\ai, \bai)$ denotes the sampling probability of pick $\bai$ given $\ai$. $\bfpi_\bB(\bi)$ and $\bfpi_\bB(\bi, \bbi)$ are defined similarly. $\bbfpi_{\bA, \bB}$ is the combination of all $\mathbf{\bar E}_i$.
% \end{definition}

% After augmentation, each pair will connect two groups of objects with similar semantic meanings, instead of just two isolated objects. Running CLIP on the augmented pair graph, we immediately get the following corollary. 
% \begin{corollary}[Augmented CLIP]
% \label{cor:augmented}
% Denote the pair graph as $\bbfpi_{\mathbf{A}, \mathbf{B}}$, 
% then Algorithm~\ref{alg:augment-clip} 
% is equivalent to running the generalized spectral
% clustering on $\bbfpi_{\mathbf{A}, \mathbf{B}}$.
% \end{corollary}



% \begin{algorithm}[tb]
%    \caption{Multi-modal contrastive learning}
%    \label{alg:Multi-modal-contrastive}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} $\bfpi^*$
%    \REPEAT
%    \STATE For every object $\bfX_i$, sample its neighbor $\mathbf{X}_i'$ (either in $\bA$ or $\bB$) from $\bfpi^*$. 
%    \STATE Train $f$ once on $\{(\bfX_i, \mathbf{X}_i')\}_{i=1}^n$ with loss~(\ref{eqn:infonce}). 
%    \UNTIL{$f$ Converges}
% \end{algorithmic}
% \end{algorithm}


% Algorithm~\ref{alg:augment-clip} runs spectral clustering on the bipartite graph $\bbfpi_{\bA, \bB}$, and contrastive learning runs spectral clustering on $\bfpi_\bA$ and $\bfpi_\bB$. Can we combine these algorithms to run spectral clustering on all the graphs glued together? This naturally leads to the following definition. 

% \begin{definition}[Multi-modal similarity graph]
% Given two similarity graphs $\bfpi_\mathbf{A},
% \bfpi_\bB$, and an augmented pair graph $\bbfpi_{\bA, \bB}$, we glue all these graphs together to get the multi-modal similarity graph $\bfpi^*$, with adjusted probabilities. 
% \end{definition}

% Here adjusted probability  means we scale all the probabilities in $\bfpi_\bA$ and $\bfpi_\bB$ by say, $0.3$, and the probabilities in $\bbfpi_{\bA, \bB}$ by $0.4$, so that each row of $\bfpi^*$ is still a valid probability distribution. The scaling factors can be set as a hyperparameter empirically. 

% $\bfpi^*$ is a hybrid graph with inter- and intra-modal edges, which is a different setting from CLIP or standard contrastive learning. However, if we treat the two encoders as a single function $f$ that can encode the objects from both $\bA$ and $\bB$, we can still run the InfoNCE loss on the graph, see Algorithm~\ref{alg:Multi-modal-contrastive}. We have the following corollary, which can be naturally generalized to more than two modalities.

% \begin{corollary}[Multi-modal contrastive learning]
% \label{cor:multi-modal-contrastive}
% Denote the multi-modal similarity graph as $\bfpi^*$, 
% then Algorithm~\ref{alg:Multi-modal-contrastive} 
% is equivalent to running the generalized spectral
% clustering on $\bfpi^*$.
% \end{corollary}



\section{Proofs}

\paragraph{Proof of Theorem \ref{thm:exponential}}

\begin{proof}
\label{proof:exponential}
Since the objective function consists of a linear term combined with an entropy regularization, which is a strongly concave function, the maximization problem is a convex optimization problem. Owing to the implicit constraints provided by the entropy function, the problem is equivalent to having only the equality constraint. We then introduce the Lagrangian multiplier $\lambda$ and obtain the following relaxed problem:

$$
\widetilde{E}(\boldsymbol{\alpha})=\psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i}+\tau \sum_{i=1}^n \alpha_{i}\log \alpha_{i}+\lambda\left(\boldsymbol{\alpha}^{\top} \mathbf{1}_n-1\right).
$$

As the relaxed problem is unconstrained, taking the derivative with respect to $\alpha_{i}$ yields

$$
\frac{\partial \widetilde{E}(\boldsymbol{\alpha})}{\partial \alpha_{i}}=-\psi_{i}+\tau\left(\log \alpha_{i}+\alpha_{i} \frac{1}{\alpha_{i}}\right)+\lambda=0.
$$

Solving the above equation implies that $\alpha_{i}$ takes the form
$
\alpha_{i}=\exp \left(\frac{1}{\tau} \psi_{i}\right) \exp \left(\frac{-\lambda}{\tau}-1\right).
$ Since $\alpha_{i}$ lies on the probability simplex, the optimal $\alpha_{i}$ is explicitly given by
$
\alpha^{*}_{i}=\frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)} .
$ Substituting the optimal point into the objective function, we obtain
$$
\begin{aligned}
E\left(\boldsymbol{\alpha}^*\right)  &=\psi_1-\sum_{i=1}^n \frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)} \psi_{i}+\tau \sum_{i=1}^n \frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)}\log \frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)} \\
& =\psi_1 - \tau \log \left(\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)\right).
\end{aligned}
$$
Thus, the Lagrangian dual function is given by
\begin{equation*}
-E\left(\boldsymbol{\alpha}^*\right)= -\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}.\qedhere
\end{equation*}
\end{proof}



\section{Experiment Details} \label{section: experiment_details}

\textbf{Pseudo-code.} Algorithm \ref{alg:Training Procedure} presents the pseudo-code for our empirical training procedure.

\begin{algorithm}[!htbp]
\caption{Training Procedure}
\label{alg:Training Procedure}
\begin{algorithmic}[1]
\REQUIRE trainable encoder network $f$, batch size $N$, augmentation strategy \textit{aug}, loss function $L$ with hyperparameters \textit{args}
\FOR {sampled minibatch ${x_i}_{i=1}^N$}
\FORALL{$i \in { 1, ..., N }$}
\STATE draw two augmentations $t_i = \textit{aug}\left(x_i\right) $, $t_i' = \textit{aug}\left(x_i\right) $
\STATE $z_i = f\left(t_i\right)$, $z_i' = f\left(t_i'\right)$
\ENDFOR
\STATE compute loss $\mathcal{L} = L(N, z, z', \textit{args})$
\STATE update encoder network $f$ to minimize $\mathcal{L}$
\ENDFOR
\STATE \textbf{Return} encoder network $f$
\end{algorithmic}
\end{algorithm}

We also provide the pseudo-code for our core loss function used in the training procedure in Algorithm \ref{alg:Core loss}. The pseudo-code is almost identical to SimCLR's loss function, with the exception of an extra parameter $\gamma$.

\begin{algorithm}[!htbp]
\caption{Core loss function $\mathcal{C}$}
\label{alg:Core loss}
\begin{algorithmic}[1]
\REQUIRE batch size $N$, two encoded minibatches $z_1, z_2$, $\gamma$, temperature $\tau$
\STATE $z = \textit{concat}\left(z_1, z_2\right)$
\FOR {$i \in {1, ..., 2N }, j \in {1, ..., 2N}$ }
\STATE $s_{i,j} = \Vert z_i - z_j \Vert_2^{\gamma}$
\ENDFOR
\STATE \textbf{define} $l(i, j)$ \textbf{as} $l(i, j) = - \log \frac{exp\left(s_{i,j}/\tau \right)}{\sum_{k=1}^{2N} \mathbf{1}{[k \ne i]} exp\left(s{i, j} / \tau \right)} $
\STATE \textbf{Return} $\frac{1}{2N} \sum_{k=1}^N\left[l(i, i+N) + l(i+N, i)\right]$
\end{algorithmic}
\end{algorithm}

Utilizing the core loss function $\mathcal{C}$, we can define all kernel loss functions used in our experiments in Table \ref{table: loss definition}. For all $z_i \in z$ with even dimensions $n$, we define $z_{L_i} = z_i\left[0:n/2\right]$ and $z_{R_i} = z_i\left[n/2:n\right]$.

\begin{table}[ht]
\centering
\begin{tabular}{{@{}l|l@{}}}
Kernel  &  Loss function \\ \midrule
Laplacian & $\mathcal{C}\left(N, z, z', \gamma=1, \tau\right)$\\ \midrule
Sum       & $\lambda * \mathcal{C}\left(N, z, z', \gamma=1, \tau_1\right) + (1-\lambda) * \mathcal{C}\left(N, z, z', \gamma=2, \tau_2\right)$  \\ \midrule
Concatenation Sum&$\lambda * \mathcal{C}\left(N, z_L, z'_L, \gamma=1, \tau_1\right) + (1-\lambda) * \mathcal{C}\left(N, z_R, z'_R, \gamma=2, \tau_2\right)$\\ \midrule
$\gamma = 0.5$ & $\mathcal{C}\left(N, z, z', \gamma=0.5, \tau\right)$          \\ 

\end{tabular}

\caption{Definition of kernel loss functions in our experiments}
\label {table: loss definition}
\end{table}

\textbf{Baselines.} We reproduce the SimCLR algorithm using PyTorch Lightning~\citep{PytorchLightning}.

\textbf{Encoder details.}
The encoder $f$ consists of a backbone network and a projection network. We employ ResNet50~\citep{ResNet} as the backbone and a 2-layer MLP (connected by a batch normalization~\citep{ioffe2015batch} layer and a ReLU \cite{nair2010rectified} layer) with hidden dimensions 2048 and output dimensions 128 (or 256 in the concatenation kernel case).

\textbf{Encoder hyperparameter tuning.}
For each encoder training case, we randomly sample 500 hyperparameter groups (sample details are shown in Table \ref{table: Hyperparameter sample}) and train these samples simultaneously using Ray Tune ~\citep{RayTune}, with the ASHA scheduler~\citep{li2018massively}. Ultimately, the hyperparameter group that maximizes the online validation accuracy (integrated in PyTorch Lightning) within 5000 validation steps is chosen for the given encoder training case.

\begin{table}[ht]
\centering

\begin{tabular}{@{}l|l|l@{}}
\midrule
Hyperparameter  & Sample Range & Sample Strategy \\ \midrule
start learning rate & $\left[10^{-2}, 10\right]$ & log uniform \\ \midrule
$\lambda$       & $\left[0, 1\right]$ & uniform \\ \midrule
$\tau$, $\tau_1$, $\tau_2$ & $\left[0, 1\right]$ & log uniform \\ \midrule
\end{tabular}

\caption{Hyperparameters sample strategy}
\label {table: Hyperparameter sample}
\end{table}

\textbf{Encoder training.} 
We train each encoder using the LARS optimizer~\citep{LARSOptimizer}, LambdaLR Scheduler in PyTorch, momentum 0.9, weight decay $10^{-6}$, batch size 256, and the aforementioned hyperparameters for 400 epochs on a single A-100 GPU.

\textbf{Image transformation.} The image transformation strategy, including augmentation, is identical to the default transformation strategy provided by PyTorch Lightning.

\textbf{Linear evaluation.}
The linear head is trained using the SGD optimizer with a cosine learning rate scheduler, batch size 64, and weight decay $10^{-6}$ for 100 epochs. The learning rate starts at $0.3$ and ends at $0$.
