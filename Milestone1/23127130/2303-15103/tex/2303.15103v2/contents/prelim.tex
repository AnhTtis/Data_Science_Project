\section{Background}
In this paper, we use objects to denote data points like images or texts. 
Given a matrix $\bfX$, we use $\bfX_i$ to denote its $i$-th row, and $\bfX_{i,j}$ to denote its $(i,j)$-th entry. Same holds for matrices like $\bfW_\bfX$, where we use $\bfW_{\bfX,i}$ and $\bfW_{\bfX,i,j}$, respectively. 


\subsection{Contrastive learning: SimCLR}
Given a query object $\mathbf{q}\in \cX$, 
\textbf{one} similar object $\mathbf{p}_1$ for $\mathbf{q}$, and $N-1$ other objects $\{\mathbf{p}_i\}_{i=2}^{N}$, 
SimCLR finds a function $f$ (usually a neural network) that maps these objects to $\cZ$, to minimize the InfoNCE loss of $\mathbf{q}$:
\begin{equation}
\label{eqn:infonce}
\mathcal{L}(\mathbf{q}, \mathbf{p}_1, \{\mathbf{p}_i\}_{i=2}^{N})=-\log \frac{\exp(\tsim(f(\mathbf{q}), f(\mathbf{p}_1))/\tau)}{\sum_{i=1}^N
\exp(\tsim(f(\mathbf{q}), f(\mathbf{p}_i))/\tau)
}
\end{equation}
Here, the actual loss of $f$ takes the summation over different $\mathbf{q}$, and $\tau$ is a temperature hyperparameter. 
The $\tsim(\bfZ_i, \bfZ_j)$ function measures the similarity between $\bfZ_i, \bfZ_j$ in $\cZ$, and is commonly  defined as $\tsim(\bfZ_i,  \bfZ_j)=\frac{\bfZ_i^\top \bfZ_j }{\|\bfZ_i\|\|\bfZ_j\|} $, 
or $\bfZ_i^\top \bfZ_j$, 
or $- \|\bfZ_i- \bfZ_j\|^2/2 $. 
In this paper, we consider the case that $\cZ$ is the unit sphere, i.e., $\|\bfZ_i\|=\|\bfZ_j\|=1$. 
This is because both SimCLR and CLIP have a normalization step in the implementation~~\citep{chen2020simple, radford2021learning}.
Hence, $
\frac{\bfZ_i^\top \bfZ_j}{\|\bfZ_i\|\|\bfZ_j\|}=
\bfZ_i^\top \bfZ_j$, and 
\begin{equation}
\label{eqn:sim_equivalence}
-\|\bfZ_i- \bfZ_j\|^2/2= -\bfZ_i^2/2 - \bfZ_j^2/2 + \bfZ_i^\top \bfZ_j = 
-1+\bfZ_i^\top \bfZ_j. 
\end{equation}
Therefore, these losses are the same up to a constant. 

\subsection{Multi-modal learning: CLIP}
CLIP~~\citep{radford2021learning} is a multi-modal model  with a dataset containing millions of (image, text) pairs. 
During pretraining, for each batch of $N$ pairs of data points, CLIP uses an image encoder and a text encoder to get $N$ pairs of embeddings, and use the InfoNCE loss to compute the correct $N$ pairs out of  $N\times N$ possible connections. Specifically, given an image $\ai$, we compare the its matching score of the paired text $\bi$, with the matching scores of other $N-1$ texts $\{\mathbf{b}_j\}_{j\neq i}$, using the loss $\ell(\ai, \bi, \{\mathbf{b}_j\}_{j\neq i})$ defined in Eqn.~(\ref{eqn:infonce}) by setting $\tsim(\bfZ_i,  \bfZ_j)=\frac{\bfZ_i^\top \bfZ_j }{\|\bfZ_i\|\|\bfZ_j\|} $.


One can define the loss similarly for text, and the actual loss of the embedding network $f$ takes the summation over all the images and  texts. 



\subsection{Reproducing Kernel Hilbert Space}
\label{sec:rkhs}
Given two objects $\bfZ_i, \bfZ_j 
\in \cZ$, consider a feature map $\varphi: \cZ\rightarrow \cH$, where the feature space $\cH$ is usually much larger than $\cZ$. We may define a kernel $k$ that measures the similarity of $\bfZ_i, \bfZ_j$ as $k(\bfZ_i, \bfZ_j)\triangleq \langle \varphi(\bfZ_i), \varphi(\bfZ_j)\rangle_\cH$, i.e., the inner product between the two objects after mapping them to the feature space. For any vector $h\in \cH$, it also corresponds to a function $h(\cdot): \cZ\rightarrow \mathbb{R}$, defined as $h(\bfZ_i)=\langle h, \varphi(\bfZ_i)\rangle_\cH$. Specifically,
$\varphi(\bfZ_j)$ as a vector in $\cH$ represents the function $k(\cdot, \bfZ_j): \cZ\rightarrow \mathbb{R}$, because for any $\bfZ_i\in \cZ$, we have $k(\bfZ_i, \bfZ_j)=\langle \varphi(\bfZ_i), \varphi(\bfZ_j)\rangle_\cH$. Formally, we have:
\begin{definition}[Reproducing kernel Hilbert space]
\label{def:rkhs}
Let $\cH$ be a Hilbert space of $\mathbb{R}$-valued functions defined on a non-empty set $\cZ$. A function $k:\cZ\times \cZ \rightarrow \mathbb{R}$ is called a reproducing kernel of $\cH$, and $\cH$ is a
reproducing kernel Hilbert space, if $k$ satisfies
\begin{itemize}[parsep=0cm, topsep=0cm]
    \item $\forall \bfZ_i\in \cZ, k(\cdot, \bfZ_i)\in \cH$,
    \item $\forall \bfZ_i\in \cZ, \forall h\in \cH, \langle h, k(\cdot, \bfZ_i)\rangle_\cH=h(\bfZ_i).$
\end{itemize}    
\end{definition}

We focus on the translation-invariant kernel in our paper, where the kernel $k(\bfZ_i,\bfZ_j)$ can always be written as $k'(\bfZ_i-\bfZ_j)$ for $k'\in \cZ\rightarrow \mathbb{R}$. The Mooreâ€“Aronszajn's theorem states that if $k$ is a symmetric, positive definite kernel on $\cZ$, there is a unique Hilbert space of functions $\cH$ on $\cZ$ for which $k$ is a reproducing kernel.

For instance, the Gaussian kernel is a symmetric, positive definite kernel that yields an RKHS with infinite dimensions. One of the advantages of a reproducing kernel is that the similarity can be computed directly in $\cZ$ without using the feature map to go to the potentially infinite dimensional Hilbert space. However, a reproducing kernel's similarity structure should ideally align with the semantic meanings of specific tasks. For example, it is unlikely to calculate the semantic similarity of two images directly using a predefined reproducing kernel in the pixel space.

Consequently, we ask if it is possible to find an embedding function $f:\cX\rightarrow \cZ$, where $\cZ$ can compute the similarity of two objects in $\cX$ with a predefined kernel function, i.e., whether $\bfK_\bfZ$ matches with $\bfpi$ in Figure~\ref{fig:illustration}. 
In other words, we hope to map
the objects to a space where the semantic similarity in $\cX$ is naturally embedded. This is the starting point of our paper.


\subsection{Markov random field}
 In this subsection, we present the framework (without proofs) of MRF for dimension reduction~~\citep{van2022probabilistic}. 
We have modified some definitions and lemmas for our learning scenarios, and the readers may check the paper for more details on this framework. 

Consider $n$ objects $\bfZ=[\bfZ_1, \cdots, \bfZ_n]$ in $\mathcal{Z}$. We use a symmetric and translation invariant kernel  $k: \cZ\rightarrow \mathbb{R}_+$ to represent the similarities in $\cZ$, where
symmetric means $k(\mathbf{x})=k(-\mathbf{x})$. Given $\bfZ$ and $k$, 
we define the gram matrix  as $\bfK_\bfZ\triangleq (k(\bfZ_i-\bfZ_j))_{(i,j)\in [n]^2}$, which is also the adjacency matrix representing the similarities of objects in $\bfZ$. 

As discussed previously, directly comparing $\bfK_\bfZ$ and $\bfpi$ can be difficult, so we treat them as MRFs and compare the induced probability distributions on subgraphs instead.
In our paper, 
subgraphs are directed unweighted graphs from the set $S_\bfW\triangleq 
\{\bfW\in \{0,1\}^{n\times n} ~|~ \forall (i,j)\in [n]^2, \bfW_{i,i}=0\}$. 
The distribution of $\bfW$ is generally defined as follows. 

\begin{definition}[Distribution of $\bfW$]
\label{def:prior_w}
Let $\bfpi\in \mathbb{R}_+^{n\times n}$, we define the distribution $
\mathbb{P} (\bfW;\bfpi) \propto 
\Omega(\bfW) 
\Pi_{(i,j)\in [n]^2} \bfpi_{i,j}^{\bfW_{i,j}}$, 
where
$\Omega(\bfW)\triangleq \Pi_i \mathbb{I}_{\sum_j \bfW_{i,j}=1}$.
\end{definition}


\begin{figure*}
\begin{center}
\input{contents/2.tex}
\end{center}
\caption{Sampling probabilities of the subgraphs defined by $\mathbb{P} (\bfW;\bfpi)$. 
The first subfigure represents the underlying graph $\bfpi$, the next three subfigures represent three different subgraphs with their sampling probabilities. The last subfigure has sampling probability $0$ because the purple node has out-degree larger than $1$. 
}
\label{fig:mrf-distribution}
\end{figure*}


To provide a clearer interpretation of the definition, we can break down the expression $\Omega(\bfW) 
\Pi_{(i,j)\in [n]^2} \bfpi_{i,j}^{\bfW_{i,j}}$ into two parts. 
Firstly, $\Omega(\bfW)$ checks if each row $i$ of $\bfW$ has exactly one out-going edge. Therefore, only subgraphs with a unitary out-degree will be preserved, while subgraphs with other out-degree values will be filtered out. As we will see later, this exactly corresponds to the setting that the InfoNCE loss uses exactly one positive neighbor. 
Secondly, $\Pi_{(i,j)\in [n]^2} \bfpi_{i,j}^{\bfW_{i,j}}$ multiplies the scores of each edge in $\bfpi$ compared with $\bfW_{i,j}$. This multiplication results in the unnormalized likelihood of $\bfW$ under $\bfpi$.  See Figure~\ref{fig:mrf-distribution} for an illustration. 

By applying Definition~\ref{def:prior_w} to $\bfK_\bfZ$, we obtain the following expression for $\mathbb{P} (\bfW;\bfK_\bfZ)$: $\mathbb{P} (\bfW;\bfK_\bfZ) \propto \Omega(\bfW) \Pi_{(i,j)\in [n]^2}k(\bfZ_i-\bfZ_j)^{\bfW_{i,j}}$. This expression represents the prior probability of $\bfW$ under $\bfK_\bfZ$. 

Due to the unitary out-degree filter, $\mathbb{P} (\bfW;\bfpi)$  has the following property.

\begin{lemma}
\label{lem:multinomial}
For $\bfW\sim \mathbb{P}(\cdot; \bfpi)$, 
$\forall i \in [n], \bfW_i\sim \mathcal{M}(1, \bfpi_i/\sum_j \bfpi_{i,j})$, where $\mathcal{M}$ is the multinomial distribution. Moreover, given any $i,i'\in [n]$, $\bfW_i$ is independent to~$\bfW_{i'}$.  Where $\bfW_i$ is the i-th row of $\bfW$, $\bfpi_{i}$ is the $i$-th row of $\bfpi$.
\end{lemma}

Below we define the cross entropy loss given distribution $\bfpi$ and the similarity matrix $\bfK_\bfZ$. 
\begin{equation}
\label{eqn:cross-entropy}
\mathcal{H}^k_{\bfpi}(\bfZ)\triangleq -\mathbb{E}_{\bfW_\bfX \sim \mathbb{P}(\cdot; \bfpi)}[\log \mathbb{P}(\bfW_\bfZ=\bfW_\bfX;\bfK_\bfZ)]
\end{equation}
The following lemma will be helpful in analyzing the cross-entropy loss, which states that when the two distributions can be aligned and decomposed, their cross entropy loss can also be decomposed. 
\begin{lemma}
\label{lem:cross_split}
Assume $\mathcal{X}= \mathcal{X}_1 \times \cdots \times \mathcal{X}_k$ and there are two probability distributions $\mathbf{P}$ and $\mathbf{Q}$ supported on $\mathcal{X}$. Suppose $\mathbf{P} = \mathbf{P}_1 \otimes \cdots \otimes \mathbf{P}_k$ and $\mathbf{Q} = \mathbf{Q}_1 \otimes \cdots\otimes \mathbf{Q}_k$, with $\mathbf{P}_i$ and $\mathbf{Q}_i$ supported on $\mathcal{X}_i$. 
Let $\mathcal{H}(\mathbf{P},\mathbf{Q})\triangleq - 
\mathbb{E}_{x\sim \mathbf{P}} 
[\log \mathbf{Q}(x)]$.
Then $\mathcal{H}(\mathbf{P},\mathbf{Q})=\sum^{k}_{i=1} \mathcal{H}(\mathbf{P}_i,\mathbf{Q}_i)$.
\end{lemma}

The cross-entropy loss can be converted to the combination of repulsion and attraction terms. 

\begin{lemma}
\label{lem:convert_to_spectral}
$\min_{\bfZ} \mathcal{H}^k_{\bfpi}(\bfZ)$ is equivalent to 
\begin{equation}
\label{eqn:spectral}
\min_{\bfZ}
-\sum_{(i,j)\in [n]^2} \mathbf{P}_{i,j}
\log k (\bfZ_i-\bfZ_j)+\log \mathbf{R}(\bfZ), 
\end{equation}
where $\mathbf{P}=\mathbb{E_{\bfW_\bfX\sim \mathbb{P}(\cdot; \bfpi)}}[\bfW_\bfX]$, and 
$\mathbf{R}(\bfZ)=\sum_{\bfW \in 
S_\bfW} \mathbb{P}(\bfZ,\bfW)
$ with $\mathbb{P}(\bfZ,\bfW)\propto f_k(\bfZ, \bfW)\Omega(\bfW)$. 
\end{lemma}
The second term in Eqn.~(\ref{eqn:spectral}) punishes trivial solutions like $\bfZ=\mathbf{0}$, as $\mathbf{0}$ is a mode for $f_k(\cdot, \bfW)$ for any $\bfW$, which incurs large $\log \mathbf{R}(\bfZ)$. The first term can be interpreted using the graph Laplacian operator, defined below.

\begin{definition}[Graph Laplacian operator]
The graph Laplacian operator is a function  $\mathbf{L}$ that maps a $n\times n$ non-negative matrix to a positive semi-definite matrix such that:
\[
\forall i,j \in [n]^2, \mathbf{L}(\bfW)_{i,j}=
\left\{
\begin{aligned}
-\bfW_{i,j}&~~~~ \mathrm{if} ~i\neq j  \\ 
\sum_{k\in [n]} \bfW_{i,k}&~~~~  \mathrm{o.w.}
\end{aligned}
\right.
\]
\end{definition}

By simple calculation, when $k$ is the Gaussian kernel, the first term in Eqn.~(\ref{eqn:spectral}) becomes $\mathrm{tr}(\bfZ^\top \mathbf{L}^* \bfZ)$ where $\mathbf{L}^* =\mathbb{E}_{\bfW_\bfX\sim \mathbb{P}(\cdot; \bfpi)}[\mathbf{L}(\bfW_\bfX)]$. 
In other words, 
Eqn.~(\ref{eqn:spectral}) is equivalent to doing spectral clustering with a repulsion regularizer $\log \mathbf{R}(\bfZ)$.
