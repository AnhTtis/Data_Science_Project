\section{Using New Kernels}
\subsection{Maximum entropy principle}
\label{sec:max-entropy}

{In this subsection, we offer an interpretation of InfoNCE-like loss, suggesting that exponential kernels are natural choices to use in this type of loss.} Given a {query} sample $\mathbf{q}$, let $\psi_i$ represent the similarity between $\mathbf{q}$ and the contrastive sample $\mathbf{p}_i$ for $i \in [n]$, computed by a kernel $k$. Without loss of generality, assume $\mathbf{p}_1$ is the (positive) neighbor of $\mathbf{q}$ according to prior knowledge, but $\psi_1$ is not necessarily the largest value in ${\psi_i}$. Ideally, we desire $\psi_1$ to be the largest or at least among the few largest similarities, which indicates that our kernel properly aligns with the prior knowledge of $\cX$.

To optimize toward this goal, we must design a loss function that captures the ranking of $\psi_1$. Since the ordering function is discrete and lacks gradient information, we need to convert it into a soft and continuous function that enables gradient-based optimization. Specifically, we employ a probability distribution $\boldsymbol{\alpha}$ to represent the neighborhood structure of $\mathbf{q}$ in relation to $\psi_1$, satisfying $\psi_{1}\leq \sum_{i=1}^n \alpha_{i} \psi_{i}$, and $\forall i, \alpha_i\geq 0$. If $\psi_1$ is the largest, $\boldsymbol{\alpha}=e_1$ is the sole solution; otherwise, $\boldsymbol{\alpha}$ can be more diverse. For instance, when all $\psi_i$ values are equal, $\boldsymbol{\alpha}$ can be a uniform distribution.

Intuitively, if there are numerous other $\psi_i$ values similar to $\psi_1$, the neighborhood structure of $\mathbf{q}$ is not as optimal as when $\psi_1$ is the only object close to $\mathbf{q}$. Formally, this means $\boldsymbol{\alpha}$ should have fewer non-zero entries or at least concentrate on $\boldsymbol{\alpha}_1$. We use its entropy $H(\boldsymbol{\alpha})= - \sum_{i=1}^n \alpha_{i}\log \alpha_{i}$ to represent this diversity, which results in the following optimization problem.
$$
\begin{array}{ccl}
(\text{P1})& \max _{\boldsymbol{\alpha}} & H(\boldsymbol{\alpha}) \\
&\text { s.t. } & \boldsymbol{\alpha}^{\top} \mathbf{1}_n=1, \alpha_1, \ldots, \alpha_n \geq 0 \\
&& \psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i} \leq 0
\end{array}
$$
By minimizing the solution of (P1), we can discover an embedding that more accurately approximates the prior knowledge. However, how can we solve (P1)? By introducing the Lagrangian dual variable $\tau>0$, {we obtain the subsequent program (P2)'s solution upper bound $\tau$ times the solution of (P1).} Consequently, minimizing (P2) simultaneously produces a smaller upper bound of (P1) as well, indirectly aiding us in achieving our objective.
$$
\begin{array}{ccl}
(\text{P2})& \max _{\boldsymbol{\alpha}} & -E(\boldsymbol{\alpha}) \\
&\text { s.t. } & \boldsymbol{\alpha}^{\top} \mathbf{1}_n=1, \alpha_1, \ldots, \alpha_n \geq 0
\end{array}
$$
where
$
E(\boldsymbol{\alpha})=\psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i}+\tau \sum_{i=1}^n \alpha_{i}\log \alpha_{i}.
$

We present the following theorem for solving (P2).

\begin{theorem}[Exponential kernels are natural]
\label{thm:exponential}
The solution of (P2) satisfies: 
$$-E\left(\boldsymbol{\alpha}^*\right)= -\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}.$$
\end{theorem}
\begin{proof}
    Please refer to Appendix~\ref{proof:main}.
\end{proof}

Using this framework, we can derive results akin to the max-min optimization formulation from \cite{tian2022understanding} as a corollary.

\subsection{Kernel-InfoNCE Loss}


{In this subsection, we will show how to use the derivation above to improve InfoNCE loss.} Theorem~\ref{thm:exponential} suggests that the loss function of the form $-\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}$ is a natural choice for characterizing the neighborhood similarity structure. {When the similarity between the query sample $\mathbf{p}$ and neighbourhood sample $\mathbf{p}_i$ is defined as $\psi_i = C - \| f(\mathbf{q}) - f(\mathbf{p}_i)\|^{\gamma}$, where $C$ is a large positive constant and $\gamma>0$.} We find it recovers the exponential kernels defined as follows:
\begin{equation}
K_{\exp }^{\gamma, \textcolor{blue}{\tau}}(\mathbf{x}, \mathbf{y}) := \exp \left(-\frac{\|\mathbf{x}-\mathbf{y}\|^\gamma}{\textcolor{blue}{\tau}}\right) \quad (\gamma, \textcolor{blue}{\tau}>0)
\end{equation}

We then define our kernel-based contrastive loss, Kernel-InfoNCE, as follows:
\begin{equation}
\label{eqn:kernel-infonce}
\mathcal{L}^{\gamma, \tau}_{\text{Kernel-InfoNCE}}(\mathbf{q}, \mathbf{p}_1, \{\mathbf{p}_i\}_{i=2}^{N}):=-\log \frac{K^{\gamma, \tau}_{\exp}(\mathbf{q}, \mathbf{p}_1)}{\sum_{i=1}^N K^{\gamma, \tau}_{\exp}(\mathbf{q}, \mathbf{p}_i).
}
\end{equation}


Note equation (\ref{eqn:kernel-infonce}) can be easily derived by setting the kernel $k$ in equation (\ref{eqn:cross-entropy}) to exponential kernel. Our framework in Section \ref{simclr spectral clustering} is suitable for explaining losses that are adapted from InfoNCE by changing kernels. We consider generalizing the exponential kernel a bit. We propose to use the mixture of two exponential kernels as potential candidates for replacing the Gaussian kernel. There are two kinds of mixing methods. The first one is taking the weighted average of two positive definite kernels. 

The other mixing method is concatenation, which splits the input vectors into two parts, where the first part uses the first kernel, and the second part uses the second kernel. It is easy to see that both mixing methods maintain the strictly positive definite property of the base kernels. We list the two types of kernel mixtures below.

\textbf{Simple Sum Kernel:}
\begin{equation*}
\resizebox{0.7\hsize}{!}{$
\begin{aligned}
K(x_i, x_j) := & \exp(-\|\boldsymbol{f}(\mathbf{x}_i) - \boldsymbol{f}(\mathbf{x}_j)\|^{2}_{2} / \tau_2) + \exp(-\|\boldsymbol{f}(\mathbf{x}_i) - \boldsymbol{f}(\mathbf{x}_j)\|^{1}_{2} / \tau_1)
\end{aligned}
$
}
\end{equation*}
\textbf{Concatenation Sum Kernel:}
\begin{equation*}
\resizebox{1.0\hsize}{!}{$
\begin{aligned}
K(x_i, x_j) := & \exp(-\|\boldsymbol{f}(\mathbf{x}_i)[0:n] - \boldsymbol{f}(\mathbf{x}_j)[0:n]\|^{2}_{2} / \tau_2) + \exp(-\|\boldsymbol{f}(\mathbf{x}_i)[n:2n] - \boldsymbol{f}(\mathbf{x}_j)[n:2n]\|^{1}_{2} / \tau_1)
\end{aligned}
$
}
\end{equation*}
