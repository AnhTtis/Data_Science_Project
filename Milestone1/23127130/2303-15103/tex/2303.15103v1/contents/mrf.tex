\section{Constrastive Learning: SimCLR}
We assume that there are finitely many objects in $\cX$, denoted as $n$. This is the same assumption used by~\citet{haochen2021provable}, who also demonstrated that the finite case can be easily extended to the infinite case by 
replacing sum by
integral, adjacency matrix by adjacency operator, etc.
For continuous augmentation methods like adding Gaussian noise, we can discretize it in a natural way. 
Assuming a finite number of objects can help us avoid non-essential technical jargon. 

With $n$ objects in $\cX$, consider a similarity graph defined on these objects, which gives a similarity matrix $\bfpi$ of size  $n\times n$. However, for real scenarios like learning images, it is extremely difficult to obtain such $\bfpi$ from human labeling. Therefore, we compute $\bfpi$ using the prior knowledge of the dataset.  For example, in the original SimCLR paper~\citep{chen2020simple}, 
there are $9$ different augmentation methods. Each augmentation method may generate many different augmented images that look similar to the original image. 
For every original image $\bfX_i$, we define a probability distribution $\bfpi_i$, such that each object $\bfX_j$ gets sampled with probability $\bfpi_{i, j}$. Therefore, $\bfpi_i$ can be represented as a vector in $\mathbb{R}^n_+$. 


Stacking all probability distributions $\bfpi_i$ together for $i\in [n]$, we get a matrix $\bfpi \in \mathbb{R}_+^{n\times n}$.  
We assume the sampling process is symmetric, i.e.,  $\bfpi_{i,j}=\bfpi_{j,i}$. 
The stochastic data augmentation samples $\bfW_\bfX$ based on $\bfpi$, i.e., $\bfW_\bfX\sim \mathbb{P}(\cdot;\bfpi)$. 

\subsection{Main Theorem}
\begin{theorem}
\label{thm:main}
For the SimCLR algorithm, denote $f$ as the neural network, $\bfZ\triangleq f(\bfX)$, and 
$\bfpi$ as the similarity graph defined by the data augmentation process. 
Then SimCLR is equivalent to solving the following program:
\[
\min_\bfZ \mathrm{tr}(\bfZ^\top \mathbf{L}(\bfpi) \bfZ)
+\log \mathbf{R}(\bfZ)
\]
which runs spectral clustering on $\bfpi$. 
\end{theorem}
\begin{proof}
Our proof has two steps. In Step 1, we will show that SimCLR is equivalent to minimizing the cross entropy loss defined in Eqn.~(\ref{eqn:cross-entropy}). 
In Step 2, we will show  that minimizing the cross entropy loss 
is equivalent to spectral clustering on $\bfpi$. 
Combining the two steps together, we have proved our theorem. 

\textbf{Step 1: } SimCLR is equivalent to minimizing the cross entropy loss.

The cross entropy loss takes expectation over 
$\bfW_\bfX\sim \mathbb{P}(\cdot ; \bfpi)$, 
which means $\bfW_\bfX$ has exactly one non-zero entry in each row $i$. By Lemma~\ref{lem:multinomial}, we know every row $i$ of $\bfW_\bfX$ is independent of other rows. Moreover, 
$\bfW_{\bfX,i}\sim \mathcal{M}(1, \bfpi_i/\sum_j \bfpi_{i,j})=\mathcal{M}(1, \bfpi_i)$, because $\bfpi_i$ itself is a probability distribution.
Similarly, we know $\bfW_\bfZ$ also has the row-independent property by sampling over $\mathbb{P}(\cdot;\bfK_\bfZ)$.
Therefore, by Lemma~\ref{lem:cross_split}, we know Eqn.~(\ref{eqn:cross-entropy}) is equivalent to:
\[
 -\sum_{i=1}^n \mathbb{E}_{\bfW_{\bfX,i}}[\log \mathbb{P}(\bfW_{\bfZ,i}=\bfW_{\bfX,i};\bfK_\bfZ)]
\]

This expression  takes expectation over $\bfW_{\bfX,i}$ for the given row $i$. Notice that 
$\bfW_{\bfX,i}$ has exactly one non-zero entry, which equals $1$ (same for $\bfW_{\bfZ,i}$). 
As a result
we expand the above expression to be:
\begin{equation}
 -\sum_{i=1}^n \sum_{j\neq i} \Pr(\bfW_{\bfX,i,j}=1)\log \Pr(\bfW_{\bfZ,i,j}=1)
\label{eqn:detailed-expansion}    
\end{equation}


By Lemma~\ref{lem:multinomial}, $\Pr(\bfW_{\bfZ,i,j}=1)=\bfK_{\bfZ,i,j}/\|\bfK_{\bfZ,i}\|_1$ for $j\neq i$. Recall that $\bfK_\bfZ=(k(\bfZ_i-\bfZ_j))_{(i,j)\in[n]^2}$, which means 
$\bfK_{\bfZ,i,j}/\|\bfK_{\bfZ,i}\|_1=\frac{\exp(-\|\bfZ_i-\bfZ_j\|^2/{2\tau})}{\sum_{k\neq i}
\exp(-\|\bfZ_i-\bfZ_k\|^2/{2\tau})
}$ for $j\neq i$, when $k$ is the Gaussian kernel with variance $\tau$. 

Notice that $\bfZ_i=f(\bfX_i)$, so we know
\begin{equation}
-\log \Pr(\bfW_{\bfZ,i,j}=1)=
-\log \frac{\exp(-\|f(\bfX_i)-f(\bfX_j)\|^2/{2\tau})}{\sum_{k\neq i}
\exp(-\|f(\bfX_i)-f(\bfX_k)\|^2/{2\tau})
}
\label{eqn:infonce-equivalence}    
\end{equation}


The right hand side is exactly the InfoNCE loss defined in Eqn.~(\ref{eqn:infonce}).
Inserting Eqn.~(\ref{eqn:infonce-equivalence}) into Eqn.~(\ref{eqn:detailed-expansion}), we get the SimCLR algorithm, which first samples augmentation pairs $(i,j)$ with $\Pr(\bfW_{\bfX,i,j}=1)$ for each row $i$, and then optimize the InfoNCE loss. 

\textbf{Step 2: } minimizing the cross entropy loss 
is equivalent to spectral clustering on $\bfpi$.


By Lemma~\ref{lem:convert_to_spectral}, we may further convert the loss to 
\begin{equation}
\label{eqn:main-theorem-repul-attr}
\min_{\bfZ}
-\sum_{(i,j)\in [n]^2} \mathbf{P}_{i,j}
\log k (\bfZ_i-\bfZ_j)+\log \mathbf{R}(\bfZ)
\end{equation}
Since $k$ is the Gaussian kernel, this reduces to \[
\min_\bfZ \mathrm{tr}(\bfZ^\top \mathbf{L}(\bfpi) \bfZ)
+\log \mathbf{R}(\bfZ)
\]

where we use the fact that $\mathbb{E}_{\bfW_\bfX\sim \mathbb{P}(\cdot; \bfpi)}[\mathbf{L}(\bfW_\bfX)]
=\mathbf{L}(\bfpi)
$, because the Laplacian operator is linear and $
\mathbb{E}_{\bfW_\bfX\sim \mathbb{P}(\cdot; \bfpi)}(\bfW_\bfX)=\bfpi
$.
\end{proof}


\textbf{Discussions. } 
Empirically, the InfoNCE loss is applied to a large batch of the object, rather than all the $n$ objects that Theorem~\ref{thm:main} requires. This explains why SimCLR benefits from larger batch size, e.g. \citet{chen2020simple} uses a batch size of 4096, and \citet{he2019momentum} uses an even large memory bank for storing more samples. 

While using the same framework from ~\citep{van2022probabilistic}, our Theorem~\ref{thm:main} is significantly different from their results on dimension reduction from at least two aspects. Firstly, in the object space $\cX$, we have a predefined similarity graph $\bfpi$, but they were using a kernel matrix $\bfK_\bfX$ based on  $\bfX$ and a kernel $k_\cX$. This is because in the dimension reduction setting, the input objects are assumed to be well-structured data points, but in the self-supervised learning setting, the input objects are images or texts, where a translation invariant kernel cannot be used for computing the similarities. Secondly, their cross-entropy loss is directly computed from $\bfK_\bfX$, while $\bfW_\bfX$ is never explicitly sampled. 
In contrast, in our method, $\bfpi$ is never explicitly used, and the cross entropy loss is indirectly computed from the randomly sampled $\bfW_\bfX$. 

The equivalence we proved is exact. 
Therefore, after learning,  the embedding space contains different components, corresponding to various (sub-) classes of the objects. 
This characterization naturally explains why contrastive learning works well for classification related downstream tasks.

\section{Multi-modal Learning}

\subsection{Original CLIP}
In this subsection, we extend Theorem~\ref{thm:main} to the multi-modal setting by analyzing CLIP, which applies the contrastive loss to the image-text pairs. 
The image-text pairs can be represented with the following pair graph. 
\begin{definition}[Pair graph]
Consider two modalities of objects $\mathbf{A}, \mathbf{B}$, 
and undirected unit-weight edges $\mathbf{E}=\{(\ai, \bi)~|~\ai\in \mathbf{A}, 
\bi\in \mathbf{B}\}_{i=1}^M$. 
The pair graph between $\mathbf{A}, \mathbf{B}$ is a directed bipartite graph $\bfpi_{\mathbf{A}, \mathbf{B}}=(\mathbf{A},\mathbf{B},\mathbf{E})$, with the weight of each outgoing edge normalized by the out-degree of the node. 
\end{definition}

By definition, $\bfpi_{\bA, \bB}$ is not necessarily symmetric.  
Consider the case where the dataset contains $10$ images of ``dog'', all of them are connected to the same text ``dog''. In this case, the text dog has $1/10$ probability to each image, while each image has only one edge with $100\%$ probability to the text. However, since each row of $\bfpi_{\bA, \bB}$ is still a probability distribution, we still have the next theorem. 

\begin{theorem}[CLIP's objective]
\label{thm:clip}
For the CLIP algorithm, denote 
$\bfpi_{\mathbf{A}, \mathbf{B}}$ as the pair graph. 
Then CLIP is equivalent to running the generalized spectral
clustering on $\bfpi_{\mathbf{A}, \mathbf{B}}$.
\end{theorem}
\begin{proof}
Since $\bfW_\bfX\sim \mathbb{P}(\cdot;\bfpi_{\mathbf{A}, \mathbf{B}})$, we know 
$\bfW_\bfX$ has exactly one non-zero entry in each row, denoting the pair got sampled. 
A notable difference compared to the previous proof, is we now have $n_\mathcal{A}+n_\mathcal{B}$ objects in our graph. CLIP deals with this by taking a mini-batch of size $2N$, 
such that $n_\mathcal{A}=n_\mathcal{B}=N$, and add the $2N$  InfoNCE losses together. We label the objects in $\mathcal{A}$ as $[n_\mathcal{A}]$, and the objects in $\mathcal{B}$ as $\{n_\mathcal{A}+1, \cdots, n_\mathcal{A}+n_\mathcal{B}\}$. 

Notice that $\bfpi_{\mathbf{A}, \mathbf{B}}$ is a bipartite graph, so the edges of objects in $\mathcal{A}$ will only connect to object in $\mathcal{B}$ and vice versa. We can define the similarity matrix in $\cZ$ as $\bfK_\bfZ$, 
where $\bfK_\bfZ(i, j+n_\mathcal{A})=\bfK_\bfZ(j+n_\mathcal{A},i)= k(\bfZ_i-\bfZ_j)$ for $i\in [n_\mathcal{A}], j\in [n_\mathcal{B}]$, and otherwise we set $\bfK_\bfZ(i,j)=0$. 
The rest is same as the previous proof. 
\end{proof}

\textbf{Discussions.} 
In Theorem~\ref{thm:clip}, we say CLIP runs the generalized spectral clustering because 
$\mathbf{L}(\bfpi_{\mathbf{A}, \mathbf{B}})$ is not necessarily the Laplacian of a symmetric graph, although one can still compute the optimal embedding $\bfZ$ following Eqn.~(\ref{eqn:spectral}).

Theorem~\ref{thm:clip} also assumes that all the objects are sampled in $\bfW_\bfX$, while empirically a really big batch size of 32,768 is used in \citet{radford2021learning}. Moreover, 
the probability distribution $\mathbb{P}(\cdot;\bfpi_{\mathbf{A}, \mathbf{B}})$ used in 
Theorem~\ref{thm:clip} is slightly different from the implementation of CLIP, in the sense that CLIP uniformly samples the edges in $\mathbf{E}$, but here we uniformly sample the objects in $\mathbf{A}\cup \mathbf{B}$. 
When the image-text pairs dataset has high quality, 
the difference between these two sampling schemes becomes negligible as 
the variance of object out-degrees is extremely small.

However, the pair graph usually contains a huge number of isolated edges (thus isolated components), which means the outcome of spectral clustering is less interesting. Empirically, CLIP picks 
strong image and text encoders with good prior knowledge about the dataset. Such prior knowledge may bias towards a better embedding for grouping the isolated edges with more semantics. 
Inspired by Theorem~\ref{thm:clip}, we also propose two augmented CLIP algorithms for better theoretical guarantees, see Appendix~\ref{sec:augmented}.
