%%%%%%%%%%%%%%%%%%%%
%%%
\section{Augmented CLIP}
\label{sec:augmented}
\begin{algorithm}[tb]
   \caption{Augmented CLIP}
   \label{alg:augment-clip}
\begin{algorithmic}
   \STATE {\bfseries Input:} 
   $\bbfpi_{\bA, \bB} $
   \REPEAT
   \STATE For every object $\ai$, sample its neighbor $\bi'$ from $\bbfpi_{\bA, \bB}$.
   \STATE For every object $\bi$, sample its neighbor $\ai'$ from $\bbfpi_{\bA, \bB}$.
   \STATE Train $f$ once on $\{(\ai,\bi')_{i=1}^{n_\bA}, 
   (\ai',\bi)_{i=1}^{n_\bB}\}$ with loss~(\ref{eqn:infonce}).
   \UNTIL{$f$ Converges}
\end{algorithmic}
\end{algorithm}



Can we modify CLIP so that it can produce semantically meaningful spectral clustering results, rather than depending on empirically verified network structures? This is possible if we have the data augmentation prior knowledge for both modalities. 
Formally, we define the augmented pair graph with the prior knowledge as follows. 
\begin{definition}[Augmented pair graph]
Given two similarity graphs $\bfpi_\mathbf{A}\in \mathbb{R}_+^{n_\mathbf{A}\times n_\mathbf{A}}$,  
$\bfpi_\mathbf{B}\in \mathbb{R}_+^{n_\mathbf{B}\times n_\mathbf{B}}$ representing the similarities in $\mathbf{A}$ and $\bB$ respectively, we augment each edge $\mathbf{E}_i=(\ai, \bi, p_i)$ in $\bfpi_{\bA, \bB}\in \mathbb{R}_+^{(n_\bA+n_\bB)\times(n_\bA+n_\bB)}$ to be $ \mathbf{\bar E}_i\triangleq \{
(\bai, \bbi, p_i\cdot \bfpi_{\bA}(\ai, \bai)\cdot \bfpi_{\bB}(\bi, \bbi))~|~ 
\bai \sim \bfpi_{\bA}(\ai), 
\bbi \sim \bfpi_{\bB}(\bi) 
\}$, where $\bfpi_{\bA}(\ai)$ denotes the probability distribution of augmenting $\ai$ in $\bA$,  
$\bfpi_{\bA}(\ai, \bai)$ denotes the sampling probability of pick $\bai$ given $\ai$. $\bfpi_\bB(\bi)$ and $\bfpi_\bB(\bi, \bbi)$ are defined similarly. $\bbfpi_{\bA, \bB}$ is the combination of all $\mathbf{\bar E}_i$.
\end{definition}

After augmentation, each pair will connect two groups of objects with similar semantic meanings, instead of just two isolated objects. Running CLIP on the augmented pair graph, we immediately get the following corollary. 
\begin{corollary}[Augmented CLIP]
\label{cor:augmented}
Denote the pair graph as $\bbfpi_{\mathbf{A}, \mathbf{B}}$, 
then Algorithm~\ref{alg:augment-clip} 
is equivalent to running the generalized spectral
clustering on $\bbfpi_{\mathbf{A}, \mathbf{B}}$.
\end{corollary}



\begin{algorithm}[tb]
   \caption{Multi-modal contrastive learning}
   \label{alg:Multi-modal-contrastive}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\bfpi^*$
   \REPEAT
   \STATE For every object $\bfX_i$, sample its neighbor $\mathbf{X}_i'$ (either in $\bA$ or $\bB$) from $\bfpi^*$. 
   \STATE Train $f$ once on $\{(\bfX_i, \mathbf{X}_i')\}_{i=1}^n$ with loss~(\ref{eqn:infonce}). 
   \UNTIL{$f$ Converges}
\end{algorithmic}
\end{algorithm}


Algorithm~\ref{alg:augment-clip} runs spectral clustering on the bipartite graph $\bbfpi_{\bA, \bB}$, and contrastive learning runs spectral clustering on $\bfpi_\bA$ and $\bfpi_\bB$. Can we combine these algorithms to run spectral clustering on all the graphs glued together? This naturally leads to the following definition. 

\begin{definition}[Multi-modal similarity graph]
Given two similarity graphs $\bfpi_\mathbf{A},
\bfpi_\bB$, and an augmented pair graph $\bbfpi_{\bA, \bB}$, we glue all these graphs together to get the multi-modal similarity graph $\bfpi^*$, with adjusted probabilities. 
\end{definition}

Here adjusted probability  means we scale all the probabilities in $\bfpi_\bA$ and $\bfpi_\bB$ by say, $0.3$, and the probabilities in $\bbfpi_{\bA, \bB}$ by $0.4$, so that each row of $\bfpi^*$ is still a valid probability distribution. The scaling factors can be set as a hyperparameter empirically. 

$\bfpi^*$ is a hybrid graph with inter- and intra-modal edges, which is a different setting from CLIP or standard contrastive learning. However, if we treat the two encoders as a single function $f$ that can encode the objects from both $\bA$ and $\bB$, we can still run the InfoNCE loss on the graph, see Algorithm~\ref{alg:Multi-modal-contrastive}. We have the following corollary, which can be naturally generalized to more than two modalities.

\begin{corollary}[Multi-modal contrastive learning]
\label{cor:multi-modal-contrastive}
Denote the multi-modal similarity graph as $\bfpi^*$, 
then Algorithm~\ref{alg:Multi-modal-contrastive} 
is equivalent to running the generalized spectral
clustering on $\bfpi^*$.
\end{corollary}






\section{Experiment Details} \label{section: experiment_details}



\textbf{Pseudo-Code.} Algorithm \ref{alg:Training Procedure} shows the pseudo-code for our empirical training procedure. 
\begin{algorithm}[!htbp]

\caption{Training Procedure} 
\label{alg:Training Procedure} 
\begin{algorithmic}[1] 
   \REQUIRE   trainable encoder network $f$, batch size $N$, augmentation strategy \textit{aug}, loss function $L$ with hyperparameters \textit{args}     
    \FOR {sampled minibatch $\{x_i\}_{i=1}^N$}
        \FORALL{$i \in \{ 1, ..., N \}$}
            \STATE draw two augmentations $t_i = \textit{aug}\left(x_i\right) $, $t_i' = \textit{aug}\left(x_i\right) $
            \STATE $z_i = f\left(t_i\right)$, $z_i' = f\left(t_i'\right)$
        \ENDFOR
        \STATE compute loss $\mathcal{L} = L(N, z, z', \textit{args})$
        \STATE update encoder network $f$ to minimize $\mathcal{L}$
    \ENDFOR
    \STATE \textbf{Return} encoder network $f$
\end{algorithmic}
\end{algorithm}

We also provide the pseudo-code of our core loss function used in training procedure in Algorithm \ref{alg:Core loss}. The pseudo-code  is almost the same with SimCLR's loss function, except we have an extra parameter $\gamma$.

\begin{algorithm}[!htbp]

\caption{Core loss function $\mathcal{C}$} 
\label{alg:Core loss} 
\begin{algorithmic}[1] 
   \REQUIRE batch size $N$, two encoded minibatches $z_1, z_2$, $\gamma$, temperature $\tau$
    \STATE $z = \textit{concat}\left(z_1, z_2\right)$
    \FOR {$i \in \{1, ..., 2N \}, j \in \{1, ..., 2N\}$ }
        \STATE $s_{i,j} = \Vert z_i - z_j \Vert_2^{\gamma}$ 
    \ENDFOR
    \STATE \textbf{define} $l(i, j)$ \textbf{as} $l(i, j) = - \log \frac{exp\left(s_{i,j}/\tau \right)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \ne i]} exp\left(s_{i, j} / \tau \right)} $ 
    \STATE \textbf{Return} $\frac{1}{2N} \sum_{k=1}^N\left[l(i, i+N) + l(i+N, i)\right]$ 
\end{algorithmic}
\end{algorithm}

With the help of core loss function $\mathcal{C}$, we can define all kernel loss function used for our experiments in Table \ref{table: loss definition}. Here for all $z_i \in z$ with even dimensions $n$, we define $z_{L_i} = z_i\left[0:n/2\right]$ and $z_{R_i} = z_i\left[n/2:n\right]$


    
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|}
\hline
Kernel  &  Loss function \\ \hline
Laplacian & $\mathcal{C}\left(N, z, z', \gamma=1, \tau\right)$\\ \hline
Sum       & $\lambda * \mathcal{C}\left(N, z, z', \gamma=1, \tau_1\right) + (1-\lambda) * \mathcal{C}\left(N, z, z', \gamma=2, \tau_2\right)$  \\ \hline
Concatenation Sum&$\lambda * \mathcal{C}\left(N, z_L, z'_L, \gamma=1, \tau_1\right) + (1-\lambda) * \mathcal{C}\left(N, z_R, z'_R, \gamma=2, \tau_2\right)$\\ \hline
% Concatenation Product& ${C}\left(N, z_L, z'_L, \gamma=1, \tau_1\right) *  \mathcal{C}\left(N, z_R, z'_R, \gamma=2, \tau_2\right)$ \\ \hline
$\gamma = 0.5$ & $\mathcal{C}\left(N, z, z', \gamma=0.5, \tau\right)$          \\ \hline

\end{tabular}

\caption{Definition of kernel loss functions in our experiments}
\label {table: loss definition}
\end{table}


\textbf{Baselines.} We reproduce SimCLR algorithm based on PyTorch Lightning\cite{PytorchLightning}. 

\textbf{Encoder Details.}
The encoder $f$ consists of a backbone network and a projection network. We use ResNet50\cite{ResNet} as the backbone, and a 2-layer MLP (linked by a batch normalization \cite{ioffe2015batch} layer and a ReLU \cite{nair2010rectified} layer) with hidden dimensions 2048 and output dimensions 128(or 256 in concatenation kernel case). 

\textbf{Encoder Hyperparameter Tuning.}
For each encoder training case, we sample 500 hyperparameter groups randomly (sample detail is shown in Table \ref{table: Hyperparameter sample}), and train these samples simultaneously using Ray Tune\cite{RayTune}, with ASHA scheduler \cite{li2018massively}. Finally, the hyperparameter group that maximizes the online validation accuracy (integrated in PyTorch Lightning) in 5000 validation steps is chosen to be used in this encoder training case.

    
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|}
\hline
Hyperparameter  & Sample Range & Sample Strategy \\ \hline
start learning rate & $\left[10^{-2}, 10\right]$ & log uniform \\ \hline
$\lambda$       & $\left[0, 1\right]$ & uniform \\ \hline
$\tau$, $\tau_1$, $\tau_2$ & $\left[0, 1\right]$ & log uniform \\ \hline
\end{tabular}

\caption{Hyperparameters sample strategy}
\label {table: Hyperparameter sample}
\end{table}
\textbf{Encoder Training.} 
We train each encoder using LARS optimizer \cite{LARSOptimizer}, LambdaLR Scheduler in PyTorch, momentum 0.9, weight decay $10^{-6}$, batch size 256 together with above hyperparameters for 800 epochs, in one A-100 GPU. 

\textbf{Image Transformation.} The image transformation strategy, includes argumentation, is the same as the default transformation strategy given by PyTorch Lightning


\textbf{Linear Evaluation.}
The linear head is trained using SGD optimizer with cosine learning rate scheduler, batch size 64, and weight
decay $10^{-6}$ for 100 epochs, and the learning rates starts at $0.3$, ends at $0$.

