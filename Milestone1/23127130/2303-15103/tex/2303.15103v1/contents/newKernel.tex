\section{With New Kernels}
We have seen that the InfoNCE loss is equivalent to Eqn.~(\ref{eqn:spectral}) for various settings. Can we replace the Gaussian kernel  in Eqn.~(\ref{eqn:spectral}) with new kernels for better performance? 
In this section, 
we examine InfoNCE-like losses with the maximum entropy principle and subsequently evaluate the expressive power of different kernel functions. Our analysis leads us to conclude that the exponential kernel is a promising candidate for replacing the Gaussian kernel. Specifically, we consider a mixture of Gaussian and Laplacian kernels as potential alternatives for experimentation purposes.





\subsection{Maximum entropy principle}
\label{sec:max-entropy}

In this subsection, we provide an interpretation of InfoNCE-like loss, which indicates that exponential kernels are the natural choices for our setting. 
Given a contrastive sample $\mathbf{q}$, denote $\psi_i$
as the similarity between the $\mathbf{q}$ and the contrastive sample $\mathbf{p}_i$ for $i\in [n]$, computed by a kernel $k$. WLOG, assume $\mathbf{p}_1$ is the neighbor of $\mathbf{q}$ by the prior knowledge, but $\psi_1$ is not necessarily the largest one in $\{\psi_i\}$. Ideally, 
 we hope $\psi_1$ is the largest, or at least among one of the few largest similarities, which indicates our kernel properly matches with the prior knowledge of $\cX$.

 To optimize towards this goal, we shall design a loss function to capture how $\psi_1$ ranks. 
 Since the ordering function is discrete without gradient information, we have to convert it into a soft and continuous function that makes gradient based optimization possible. 
 Specifically,  we use a probability distribution  $\boldsymbol{\alpha}$ to represent the neighborhood structure of $\mathbf{q}$ related to $\psi_1$, which satisfies $\psi_{1}\leq \sum_{i=1}^n \alpha_{i} \psi_{i}$, and $\forall i, \alpha_i\geq 0$. If $\psi_1$ is the largest, $\boldsymbol{\alpha}=e_1$ is the only solution, otherwise $\boldsymbol{\alpha}$ can be more diverse. For example, when all $\psi_i$'s are equal to each other, $\boldsymbol{\alpha}$ can be the uniform distribution. 

Intuitively, if there are many other $\psi_i$'s with similar values as $\psi_1$, the neighborhood structure of $\mathbf{q}$ is not as good as~$\psi_1$ being the only close object to $\mathbf{q}$. Formally, it means $\boldsymbol{\alpha}$ should  have fewer non-zero entries, or at least concentrate at~$\boldsymbol{\alpha}_1$. We use its entropy $H(\boldsymbol{\alpha})= - \sum_{i=1}^n \alpha_{i}\log \alpha_{i}$  to represent this diversity, which gives the following optimization problem.
$$
\begin{array}{ccl}
(\text{P1})& \max _{\boldsymbol{\alpha}} & H(\boldsymbol{\alpha}) \\
&\text { s.t. } & \boldsymbol{\alpha}^{\top} \mathbf{1}_n=1, \alpha_1, \ldots, \alpha_n \geq 0 \\
&& \psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i} \leq 0
\end{array}
$$
By minimizing the solution of (P1), we can find an embedding that better approximates the prior knowledge. However, how can we solve (P1)?
By introducing the Lagrangian dual variable $\tau>0$, we get the following program~(P2) that generates an upper bound of (P1). As a result, minimizing~(P2) simultaneously 
generates a smaller upper bound of~(P1) as well, which helps us achieve our goal indirectly.
$$
\begin{array}{ccl}
(\text{P2})& \max _{\boldsymbol{\alpha}} & -E(\boldsymbol{\alpha}) \\
&\text { s.t. } & \boldsymbol{\alpha}^{\top} \mathbf{1}_n=1, \alpha_1, \ldots, \alpha_n \geq 0
\end{array}
$$
where
$
E(\boldsymbol{\alpha})=\psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i}+\tau \sum_{i=1}^n \alpha_{i}\log \alpha_{i}.
$

We have the following theorem for solving (P2).

\begin{theorem}[Exponential kernels are natural]
\label{thm:exponential}
The solution of (P2) satisfies: 
$$-E\left(\boldsymbol{\alpha}^*\right)= -\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}.$$
\end{theorem}

\begin{proof}
Because the objective function is a linear term adding an entropy regularization, which is a strongly concave function, the maximization problem is a convex optimization problem.
Thanks to the implicit constraints given by the entropy function, the problem is equivalent to having only the equality constraint. We shall then introduce the Lagrangian multiplier $\lambda$ and get the following relaxed problem:
$$
\widetilde{E}(\boldsymbol{\alpha})=\psi_{1}-\sum_{i=1}^n \alpha_{i} \psi_{i}+\tau \sum_{i=1}^n \alpha_{i}\log \alpha_{i}+\lambda\left(\boldsymbol{\alpha}^{\top} \mathbf{1}_n-1\right).
$$


As the relaxed problem is an unconstrained problem, taking the derivative with respect to $\alpha_{i}$, we shall get 
$$
\frac{\partial \widetilde{E}(\boldsymbol{\alpha})}{\partial \alpha_{i}}=-\psi_{i}+\tau\left(\log \alpha_{i}+\alpha_{i} \frac{1}{\alpha_{i}}\right)+\lambda=0.
$$
Solving the above equation implies that $\alpha_{i}$ has the form
$
\alpha_{i}=\exp \left(\frac{1}{\tau} \psi_{i}\right) \exp \left(\frac{-\lambda}{\tau}-1\right).
$ As $\alpha_{i}$ lying on the probability simplex, the optimal $\alpha_{i}$ is explicitly given by
$
\alpha^{*}_{i}=\frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)} .
$ Substituting the optimal point into the objective function, we have
$$
\begin{aligned}
E\left(\boldsymbol{\alpha}^*\right)  &=\psi_1-\sum_{i=1}^n \frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)} \psi_{i}+\tau \sum_{i=1}^n \frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)}\log \frac{\exp \left(\frac{1}{\tau} \psi_{i}\right)}{\sum_{i^{\prime}=1}^n \exp \left(\frac{1}{\tau} \psi_{i^{\prime}}\right)} \\
& =\psi_1 - \tau \log \left(\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)\right).
\end{aligned}
$$
Thus, the Lagrangian dual function is given by
\begin{equation*}
-E\left(\boldsymbol{\alpha}^*\right)= -\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}.\qedhere
\end{equation*}
\end{proof}

With this framework, we can derive results similar to \cite{tian2022understanding}'s max-min optimization formulation as a corollary.

Theorem~\ref{thm:exponential} indicates that the loss function of the form  $-\tau \log \frac{\exp \left(\frac{1}{\tau} \psi_{1}\right)}{\sum_{i=1}^n \exp \left(\frac{1}{\tau} \psi_{i}\right)}$ is a natural choice for characterizing
the neighborhood similarity structure. In other words, we should use the exponential kernels defined as follows. 
\begin{equation}
K_{\exp }^{\gamma, \sigma}(x, y) \triangleq \exp \left(-\frac{\|x-y\|^\gamma}{\sigma}\right) \quad (\gamma, \sigma>0)
\end{equation}

We define our kernel-based contrastive loss Kernel-InfoNCE as below:
\begin{equation}
\label{eqn:kernel-infonce}
\mathcal{L}^{\gamma, \sigma}_{\text{Kernel-InfoNCE}}(\mathbf{q}, \mathbf{p}_1, \{\mathbf{p}_i\}_{i=2}^{N})\triangleq-\log \frac{K^{\gamma, \sigma}_{\exp}(\mathbf{q}, \mathbf{p}_1)}{\sum_{i=1}^N K^{\gamma, \sigma}_{\exp}(\mathbf{q}, \mathbf{p}_i)
}
\end{equation}

Below we further investigate the kernels from  expressive power and kernel mixture. 

\subsection{Expressive power}
Based on Definition~\ref{def:rkhs}, we know the size of $\mathcal{H}$ depends on how $k$ is defined. For different exponential kernels, which one is more expressive? Specifically, 
can we find another kernel $k'$ so that $\mathcal{H}_{k'}$ is larger than  $\mathcal{H}_{\mathrm{Gauss}}$?
The answer is affirmative by the following Theorem. 

\begin{theorem}[\citet{chen2020deep}]
Consider exponential kernels restricted to the unit sphere $\mathbb{S}^{d-1}$ and defined on the entire $\mathbb{R}^d$, respectively. Then we have the following RKHS inclusions:
If $0<\gamma\leq 1$, for any $\sigma>0$,
$
\mathcal{H}_{\mathrm{Gauss}}\left(\mathbb{S}^{d-1}\right) \subseteq \mathcal{H}_{K_{\mathrm{exp}}^{\gamma, \sigma}}\left(\mathbb{S}^{d-1}\right)
$ and $
\mathcal{H}_{\mathrm{Gauss}}\left(\mathbb{R}^d\right) \subseteq \mathcal{H}_{K_{\mathrm{exp}}^{\gamma, \sigma}}\left(\mathbb{R}^d\right)
$. 
\end{theorem}

This implies, at least from the perspective of expressive power, picking a smaller number on the exponential is better. Note that when $\gamma=1$, we get the Laplacian kernel. 

\subsection{Kernel Mixture}
In the previous subsections, we have seen that exponential kernels have many attractive properties. In this subsection, we consider taking the mixture of two exponential kernels. 

There are two kinds of mixing methods. The first one is taking the weighted average of two kernels, which maintains the positive definite property by the following fact.  
\begin{fact}
If $k_i: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{C}(i=1,2, \ldots)$ are strictly positive definite kernels, then 
the kernel $ak_1+bk_2$ for $a, b \geq 0, ab > 0$, is also strictly positive definite. 
\end{fact}

The other mixing method is concatenation, which splits the input vectors into two parts, where the first part uses the first kernel, and the second part uses the second kernel. It is also easy to see that the concatenation of two strictly positive definite kernels is still strictly positive definite. We list the two types of kernel mixtures below.

\textbf{Simple Sum Kernel:}
\begin{align*}
K(x_i, x_j) \triangleq & \exp(-\|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|^{2}_{2} / \tau_2) + \exp(-\|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|^{1}_{2} / \tau_1)
\end{align*}
\textbf{Concatenation Sum Kernel:}
\begin{align*}
K(x_i, x_j) \triangleq & \exp(-\|f(\mathbf{x}_i)[0:n] - f(\mathbf{x}_j)[0:n]\|^{2}_{2} / \tau_2) + \exp(-\|f(\mathbf{x}_i)[n:2n] - f(\mathbf{x}_j)[n:2n]\|^{1}_{2} / \tau_1)
\end{align*}




