\section{Experiments}
\begin{table}[thb]
\centering
\begin{tabular}{@{}c|cc@{}}
\toprule
\multirow{2}{*}{Method}                       & \multicolumn{2}{c}{CIFAR-10}                                             \\ \cmidrule(l){2-3} 
                    & 200 epochs & 400 epochs                         \\ \midrule
SimCLR (repro.)                     & $88.13$ & $90.59$ \\
 \midrule
%Gaussian Kernel                     & $88.13$ & $90.59$ \\ 
Laplacian Kernel                    & $89.31$ & $91.05$ \\
$\gamma=0.5$ Exponential Kernel     & $89.00$ & $91.23$ \\ 
\textbf{Simple Sum Kernel}          & $89.80$ & $\mathbf{91.76}$ \\
\textbf{Concatenation Sum Kernel}            & $\mathbf{89.89}$ & $91.28$ \\ \midrule
\end{tabular}
\caption{Results on CIFAR-10 dataset}
\label{tab:results-cifar10}
\end{table}

\begin{table}[thb]
\centering
\begin{tabular}{@{}c|cc@{}}
\toprule
\multirow{2}{*}{Method}                       & \multicolumn{2}{c}{CIFAR-100}                                             \\ \cmidrule(l){2-3} 
                    & 200 epochs & 400 epochs                         \\ \midrule
SimCLR (repro.)                     & $62.67$ & $66.23$ \\
 \midrule
%Gaussian Kernel                     & $62.67$ & $66.23$ \\
Laplacian Kernel                    & $63.17$ & $66.06$ \\
$\gamma=0.5$ Exponential Kernel     & $63.47$ & $65.71$ \\ 
\textbf{Simple Sum Kernel}          & $\mathbf{66.73}$ & $\mathbf{68.62}$ \\
Concatenation Sum Kernel   & $66.09$ & $68.53$ \\ \midrule
\end{tabular}
\caption{Results on CIFAR-100 dataset}
\label{tab:results-cifar100}
\end{table}

\begin{table}[thb]
\centering
\begin{tabular}{@{}c|cc@{}}
\toprule
\multirow{2}{*}{Method}                       & \multicolumn{2}{c}{TinyImageNet}                                             \\ \cmidrule(l){2-3} 
                    & 200 epochs & 400 epochs                         \\ \midrule
SimCLR (repro.)                     & $34.03$ & $37.86$ \\
 \midrule
%Gaussian Kernel                    & $34.03$ & $37.86$ \\
Laplacian Kernel                    & $35.92$ & $38.76$ \\
$\gamma=0.5$ Exponential Kernel     & $34.21$ & $38.70$ \\ 
\textbf{Simple Sum Kernel}          & $\textbf{36.60}$ & $\textbf{39.38}$  \\
Concatenation Sum Kernel            & $35.92$ & $38.76$ \\ \midrule
\end{tabular}
\caption{Results on TinyImageNet dataset}
\label{tab:results-tinyimagenet}
\end{table}


In our experiments, we reproduce the baseline algorithm SimCLR ~\citep{chen2020simple}, and replace SimCLR's Gaussian kernel with other kernels, then test against SimCLR on different benchmark vision datasets. 

\paragraph{CIFAR-10 and CIFAR-100} CIFAR-10 ~\citep{krizhevsky2009learning} and CIFAR-100 ~\citep{krizhevsky2009learning} are popular classical image classification datasets. Both CIFAR-10 and CIFAR-100 contain in total 60k  $32 \times 32$ labeled images of different classes, which 50k for training and 10k for test. CIFAR-10 is similar to CIFAR-100, except there are 10 different classes in CIFAR-10, and 100 classes in \mbox{CIFAR-100}. 


\paragraph{TinyImageNet} TinyImageNet ~\citep{le2015tiny} is a subset of ImageNet ~\citep{russakovsky2015imagenet}. There are 200 different object classes in TinyImageNet, and 500 training images, 50 validation images, and 50 test images for each class. All the images in TinyImageNet are colored and labeled images with a size of $64 \times 64$. 

For each algorithm, we first train an encoder $f$ on the training dataset, to minimize the empirical loss function generated by the kernel. Then following the standard linear evaluation protocol, we freeze the encoder $f$ and train a supervised linear classifier, which takes the output representation of $f$ as input. More experimental details can be found in Appendix~\ref{section: experiment_details}.

\paragraph{Experimental Results} 
We summarize our empirical results on different benchmark datasets in Table \ref{tab:results-cifar10}, \ref{tab:results-cifar100}, and \ref{tab:results-tinyimagenet}. It is clear that we have achieved better performance than SimCLR in all three benchmark datasets, and the Simple Sum Kernel reaches the best average performance. 

