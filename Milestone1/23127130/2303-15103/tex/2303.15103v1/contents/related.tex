\section{Related Work}

Contrastive learning is a classical method that was widely used in representation learning ~\citep{hadsell2006dimensionality,becker1992self}. 
Since its recent applications in self-supervised learning, 
it has received prevalent attention  and achieved state-of-the-art results on many downstream tasks in computer vision ~\citep{tian2020makes, cui2021parametric}, graph representation learning ~\citep{you2020graph, hassani2020contrastive}, multi-modality ~\citep{radford2021learning} and so on.

Contrastive predictive coding ~\citep{oord2018representation} is one of the first methods to apply the idea of contrastive learning in self-supervised learning. After that, various methods seek to improve the performance. SimCLR ~\citep{chen2020simple} and MoCo ~\citep{chen2020mocov2github} propose to use a large batch size and momentum update mechanism to ensure the effectiveness of learning. 
To alleviate the influence of false negative sampling, the hard negative sampling method ~\citep{kalantidis2020hard} has also been discussed. 

Although contrastive learning has reached empirical success, the theoretical understanding of its mechanism is still limited. \citet{oord2018representation} show that InfoNCE loss can be seen as a surrogate loss for maximizing the mutual information. \citet{haochen2021provable} introduce the concept of augmentation graph to help analyze contrastive learning. \citet{hu2022your} connect contrastive learning with stochastic neighbor embedding. \citet{wang2020understanding} 
show that the quality of embedding may decompose into an alignment part and a uniformity part, by taking 
both the loss function and the embedding space into consideration.
