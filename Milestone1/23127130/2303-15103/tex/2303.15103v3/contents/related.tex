\section{Related Work}

Contrastive learning constitutes a classical method extensively employed in representation learning~\citep{hadsell2006dimensionality,becker1992self}. Owing to its recent applications in self-supervised learning, contrastive learning has garnered widespread attention and achieved state-of-the-art results in numerous downstream tasks within computer vision~\citep{tian2020makes, cui2021parametric}, graph representation learning~\citep{you2020graph, hassani2020contrastive, deng2022neural}, multi-modality~\citep{radford2021learning}, and beyond.

Contrastive predictive coding~\citep{oord2018representation} represents one of the pioneering methods to incorporate the concept of contrastive learning in self-supervised learning. Subsequently, various methods have sought to enhance performance. SimCLR~\citep{chen2020simple} and MoCo~\citep{chen2020mocov2github} advocate for utilizing a large batch size and momentum update mechanism to guarantee effective learning. Moreover, the hard negative sampling method~\citep{kalantidis2020hard} has been explored to mitigate the impact of false negative sampling.

Despite the empirical success of contrastive learning, the theoretical comprehension of its underlying mechanisms remains limited. \citet{oord2018representation} demonstrate that the InfoNCE loss can be regarded as a surrogate loss for maximizing mutual information. \cite{arora2019theoretical} give the generalization bound for contrastive learning under a latent class assumption.\citet{haochen2021provable} incorporate the concept of augmentation graph to facilitate the analysis of contrastive learning and propose a surrogate loss spectral contrastive loss. They show that this surrogate loss is equivalent to spectral clustering on augmentation graph. \cite{wang2022chaos} propose that aggressive data augmentations lead to overlapping support of intra-class samples, allowing for the clustering of positive samples and the gradual learning of class-separated representations, providing new insights into understanding contrastive learning. \cite{balestriero2022contrastive} link a variant of contrastive loss to the ISOMAP algorithm. \citet{hu2022your} connect contrastive learning with stochastic neighbor embedding. \citet{wang2020understanding} reveal that the quality of embedding can be decomposed into an alignment component and a uniformity component, considering both the loss function and the embedding space.