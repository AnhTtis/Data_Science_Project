\section{Constrastive Learning: SimCLR} \label{simclr spectral clustering}
In this section, we will prove our main theorem that contrastive learning is spectral clustering on a similarity graph. We assume that there are finitely many objects in $\cX$, denoted as $n$. This is the same assumption used by~\citet{haochen2021provable}, who also demonstrated that the finite case can be easily extended to the infinite case by 
replacing sum by
integral, adjacency matrix by adjacency operator, etc.
For continuous augmentation methods like adding Gaussian noise, we can discretize it in a natural way. 
Assuming a finite number of objects can help us avoid non-essential technical jargon. 

With $n$ objects in $\cX$, consider a similarity graph defined on these objects, which gives a similarity matrix $\bfpi$ of size  $n\times n$. However, for real scenarios like learning images, it is extremely difficult to obtain such $\bfpi$ from human labeling. Therefore, we compute $\bfpi$ using the prior knowledge of the dataset.  For example, in the original SimCLR paper~\citep{chen2020simple}, 
there are $9$ different augmentation methods. Each augmentation method may generate many different augmented images that look similar to the original image. 
For every original image $\bfX_i$, we define a probability distribution $\bfpi_i$, such that each object $\bfX_j$ gets sampled with probability $\bfpi_{i, j}$. For example, during the augmentation process, suppose $\bfX_j$ has a probability, say $1/9$, to be an augmentation of $\bfX_i$, then $\bfpi_{i, j}=1/9$.
Therefore, $\bfpi_i$ can be represented as a vector in $\mathbb{R}^n_+$. 


Stacking all probability distributions $\bfpi_i$ together for $i\in [n]$, we get a matrix $\bfpi \in \mathbb{R}_+^{n\times n}$.  
In this section, we assume the sampling process is symmetric, i.e.,  $\bfpi_{i,j}=\bfpi_{j,i}$. 
The stochastic data augmentation samples $\bfW_\bfX$ based on $\bfpi$, i.e., $\bfW_\bfX\sim \mathbb{P}(\cdot;\bfpi)$. 

\subsection{Main Theorem}
\begin{theorem}
\label{thm:main}
For the SimCLR algorithm, denote $f$ as the neural network, $\bfZ:= f(\bfX)$, and 
$\bfpi$ as the similarity graph defined by the data augmentation process where objects are connected iff they are positive samples of each other. 
Then SimCLR is equivalent to solving the following program:
\[
\min_\bfZ \mathrm{tr}(\bfZ^\top \mathbf{L}(\bfpi) \bfZ)
+\log \mathbf{R}(\bfZ),
\]
which runs spectral clustering on $\bfpi$. 
\end{theorem}

\begin{proof}
    If not specified, we present all the proofs in  Appendix~\ref{proof:main}.
\end{proof}

\textbf{Discussions.} 
Empirically, the InfoNCE loss is applied to a large batch of the object, rather than all the $n$ objects that Theorem~\ref{thm:main} requires. This explains why SimCLR benefits from larger batch size, e.g. \citet{chen2020simple} use a batch size of 4096, and \citet{he2019momentum} use an even large memory bank for storing more samples. 

While using the same framework from ~\citep{van2022probabilistic}, our Theorem~\ref{thm:main} is significantly different from their results on dimension reduction from at least two aspects. Firstly, in the object space $\cX$, we have a predefined similarity graph $\bfpi$, but they were using a kernel matrix $\bfK_\bfX$ based on  $\bfX$ and a kernel $k_\cX$. This is because in the dimension reduction setting, the input objects are assumed to be well-structured data points, but in the self-supervised learning setting, the input objects are images or texts, where a translation invariant kernel cannot be used for computing the similarities. Secondly, their cross-entropy loss is directly computed from $\bfK_\bfX$, while $\bfW_\bfX$ is never explicitly sampled. 
In contrast, in our method, $\bfpi$ is never explicitly used, and the cross entropy loss is indirectly computed from the randomly sampled $\bfW_\bfX$. 

The equivalence we proved is exact. 
Therefore, after learning, the embedding space contains different components, corresponding to various (sub-) classes of the objects. 
This characterization naturally explains why contrastive learning works well for classification-related downstream tasks.

\section{Multi-modal Learning: CLIP}
In this subsection, we extend Theorem~\ref{thm:main} to the multi-modal setting by analyzing CLIP, which applies the contrastive loss to the image-text pairs. 
The image-text pairs can be represented with the following pair graph. 
\begin{definition}[Pair graph]
Consider two modalities of objects $\mathbf{A}, \mathbf{B}$, 
and undirected unit-weight edges $\mathbf{E}=\{(\ai, \bi)~|~\ai\in \mathbf{A}, 
\bi\in \mathbf{B}\}_{i=1}^M$. 
The pair graph between $\mathbf{A}, \mathbf{B}$ is a directed bipartite graph $\bfpi_{\mathbf{A}, \mathbf{B}}=(\mathbf{A},\mathbf{B},\mathbf{E})$, with the weight of each outgoing edge normalized by the out-degree of the node. 
\end{definition}

By definition, $\bfpi_{\bA, \bB}$ is not necessarily symmetric.  
Consider the case where the dataset contains $10$ images of ``dog'', all of them are connected to the same text ``dog''. In this case, the text dog has $1/10$ probability to each image, while each image has only one edge with $100\%$ probability to the text. However, since each row of $\bfpi_{\bA, \bB}$ is still a probability distribution, we still have the next theorem. 

\begin{theorem}[CLIP's objective]
\label{thm:clip}
For the CLIP algorithm, denote 
$\bfpi_{\mathbf{A}, \mathbf{B}}$ as the pair graph. 
Then CLIP is equivalent to running the generalized spectral
clustering on $\bfpi_{\mathbf{A}, \mathbf{B}}$.
\end{theorem}
\begin{proof}
    Please refer to Appendix~\ref{proof:main}.
\end{proof}


\textbf{Discussions.} 
In Theorem~\ref{thm:clip}, we say CLIP runs the generalized spectral clustering because 
$\mathbf{L}(\bfpi_{\mathbf{A}, \mathbf{B}})$ is not necessarily the Laplacian of a symmetric graph, although one can still compute the optimal embedding $\bfZ$ following Eqn.~(\ref{eqn:spectral}). The pair graph may contain a huge number of isolated edges. Empirically, CLIP picks 
strong image and text encoders with good prior knowledge about the dataset. Such prior knowledge may be biased towards a better embedding for grouping the isolated edges with more semantics. 

Theorem~\ref{thm:clip} also assumes that all the objects are sampled in $\bfW_\bfX$, while empirically a really big batch size of 32,768 is used in \citet{radford2021learning}. Moreover, 
the probability distribution $\mathbb{P}(\cdot;\bfpi_{\mathbf{A}, \mathbf{B}})$ used in 
Theorem~\ref{thm:clip} is slightly different from the implementation of CLIP, in the sense that CLIP uniformly samples the edges in $\mathbf{E}$, but here we uniformly sample the objects in $\mathbf{A}\cup \mathbf{B}$. 
When the image-text pairs dataset has high quality, 
the difference between these two sampling schemes becomes negligible as 
the variance of object out-degrees is extremely small.


\subsection{Applying to LaCLIP}

Due to computation resources limitations, we haven't implemented an improved CLIP algorithm ourselves. Interestingly, a direct improvement to CLIP based on our theory was recently conducted. We shall present this algorithm LaCLIP carefully
\citep{fan2023improving} and discuss why it can be seen as a direct application of our theory.


Roughly speaking, LaCLIP is a direct extension of CLIP by not only incorporating image augmentations but using text augmentation as well. Specifically, we can treat language rewrites as text augmentations. For each image text pair $(x_{I}, x_{T})$, the text augmentation can be derived as follows:
\begin{equation}
\operatorname{aug}_T\left(x_T\right) \sim \operatorname{Uniform}\left(\left[x_{T_0}, x_{T _1} \ldots, x_{T_M}\right]\right),    
\end{equation}
where $x_{T_i}$ is the text $x_{T}$ itself or its rewrite.

Then training loss over the images in LaCLIP becomes:
$$
\mathcal{L}_I := -\sum_{i=1}^N \log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{f}_I\left(\operatorname{aug}_I\left(x_I^i\right)\right), \boldsymbol{f}_T\left(\operatorname{aug}_T\left(x_T^i\right)\right)\right) / \tau\right)}{\sum_{k=1}^N \exp \left(\operatorname{sim}\left(\boldsymbol{f}_I\left(\operatorname{aug}_I\left(x_I^i\right)\right), \boldsymbol{f}_T\left(\operatorname{aug}_T\left(x_T^k\right)\right)\right) / \tau\right)},
$$
where $\boldsymbol{f}_I$ and $\boldsymbol{f}_T$ are image and text encoders respectively.


From the pair graph point of view, LaCLIP expands the nodes in the "text side" of the pair graph by including the nodes of the rewrite (augmented) texts. Moreover, as the augmented images are connected to augmented texts, this augmented pair graph will have more clusters between similar objects than the original CLIP pair graph. Thus, from the spectral clustering, it will be natural to expect LaCLIP shall cluster similar objects across modalities better than CLIP. Indeed, the zero-shot transfer ability of LaCLIP significantly improves \citep{fan2023improving}.