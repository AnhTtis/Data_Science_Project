\section{Introduction}
\label{sec:intro}
Learning a 3D generative model from 2D images has recently drawn much interest~\cite{sitzmann2019srn, niemeyer2021giraffe, gu2021stylenerf}.
{%
Since the introduction of Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf}, the quality of images rendered from a 3D model~\cite{gu2021stylenerf, Chan2022eg3d} has improved drastically, becoming as photorealistic as those rendered by a 2D model~\cite{karras2020stylegan2}.
}%
While some of them~\cite{chan2021pigan, rebain2022lolnerf} rely {purely} on 3D representations to {deliver}
3D consistency and pay the price of {decreased} photorealism, more recent work~\cite{Chan2022eg3d} has further shown {that this can be avoided and extreme photorealism can be obtained through a hybrid setup}.
However, {even so,} a shortcoming of these models is that the components that constitute the scene---the geometry, appearance, and the lighting---are all \emph{entangled} and are thus not controllable using user defined inputs.

Methods have been proposed to break this entanglement~\cite{boss2021neural, boss2021nerd, zhang2021nerfactor},
however, they require multiview image collections of the scene being modeled and are thus inapplicable to images in-the-wild where such constraint cannot be satisfied easily.
Boss et al.~\cite{boss2022samurai} loosens this constraint to images of different scenes, but they still require the same object to be seen from multiple views at the end.
Moreover, these methods are not generative and therefore need to be trained for each object and cannot generate new objects.
For generative methods~\cite{gu2021stylenerf, Chan2022eg3d, chan2021pigan}, geometry and illumination remains entangled.
In this work, we demonstrate that {one does not require multiple views, and} the variability and the volume of already existing datasets~\cite{karras2019stylegan, karras2018progressive, karras2020training} are enough to learn a disentangled 3D generative model.

We propose \emph{\ours}, a framework that learns a \emph{disentangled} 3D model of a face, purely from images; see~\Figure{teaser}.
The high-level idea behind our method is to build a rendering pipeline that is \emph{forced} to respect physical lighting models~\cite{ramamoorthi2001efficient, phong1975illumination}, similar to \cite{zhang2021nerfactor} but in a framework friendly to 3D generative modeling, and one that can leverage off-the-shelf lighting and pose estimators~\cite{feng2021deca}.
In more detail, we embed the {physics-based} illumination model using Spherical Harmonics~\cite{ramamoorthi2001efficient} within the recent generative Neural Volume Rendering pipeline, EG3D~\cite{Chan2022eg3d}.
We then simply train for realism, and since the framework has to then obey physics to generate realistic images, it naturally learns a disentangled 3D generative model.

Importantly, the way we embed physics-based rendering into neural volume rendering is the core enabler of our method.
As mentioned, to allow easy use of existing off-the-shelf illumination estimators~\cite{feng2021deca}, we base our method on Spherical Harmonics.
We then model the diffuse and specular components of the scene via the Spherical Harmonic coefficients associated with the surface normals and the reflectance vectors, where the {diffuse reflectance}, the normal vectors and the material {specular reflectance} are generated by a neural network. 
While simple, our setup allows for effective disentanglement of illumination from the rendering process.

We show the effectiveness of our method using three datasets FFHQ~\cite{karras2019stylegan}, CelebA-HQ~\cite{karras2018progressive} and MetFaces~\cite{karras2020training} and obtain state-of-the-art FID scores among 3D aware generative models.
Furthermore, to the best of our knowledge, our method is the very first generative method that can generate 3D faces with controllable scene lighting. {Our code is available for research purposes at \url{https://github.com/apple/ml-facelit/}.}

To summarize, our contributions are:
\vspace{-0.7em}
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-.3em}
    \item we propose a novel framework that can learn a disentangled 3D generative model of faces from single views, with which we can render the face with different views and under various lighting conditions;
    \item we introduce how to embed an illumination model in the rendering framework that models the effects of diffuse and specular reflection.
    \item we show that our method can be trained \emph{without} any manual label and simply with 2D images and an off-the-shelf pose/illumination estimation method.
    \item we achieve state-of-the-art FID score of 3.5 among 3D GANs on the FFHQ dataset improving the recent work~\cite{Chan2022eg3d} by 25\%, relatively. 
\end{itemize}

