


\section*{A. Network Architecture}
\label{sec:architecture}

We show the architecture of the diffuse Decoder and the specular decoder as described in Section 3 in Fig.~\ref{fig:decoders}. We follow a feed-forward architecture with fully connected layers and softplus activation~\cite{zhao2018softplus}. The fully connected layers have a hidden dimension of 64 for diffuse decoder and 32 for specular decoder.

\begin{figure}
    \centering
    \includegraphics[width=0.12\textwidth]{figs/diffuse_decoder.png} \qquad \qquad
    \includegraphics[width=0.12\textwidth]{figs/specular_decoder.png} \\
    \begin{tabular}{cc}
        (a) Diffuse Decoder \quad & \quad (b) Specular Decoder
    \end{tabular}
    \caption{Architecture of Diffuse Decoder and Specular Decoder }
    \label{fig:decoders}
\end{figure}

\section*{B. Training Time}
Our training time is slightly higher than EG3D due to addition of illumination model. EG3D takes 4 days, FaceLit-d takes 4.5 days and FaceLit-f takes 4.8 days on 8 NVIDIA A100 GPUs to train at a neural rendering resolution of $64^3$. 

\section*{C. Additional Qualitative Results}

\paragraph{Albedo.} In Fig.~\ref{fig:albedo}, we show the visualization of albedo images of the corresponding generated samples. The albedo images are obtained by rendering the images before the superresolution module under constant illumination.

\paragraph{Uncurated Samples.} In Fig.~\ref{fig:uncurated_ffhq_psi_1}, we show uncurated samples generated by our model with a truncation factor of $\psi = 1.0$. In Fig.~\ref{fig:uncurated_ffhq_psi_07}, we show uncurated samples generated by our model with a truncation factor of $\psi = 0.7$. In both case, we use the FaceLit-F model trained on the FFHQ dataset.


\paragraph{Latent Space Interpolations.} In Fig.~\ref{fig:latent_interp}, we show interpolation across generated samples from our model. In each row, we interpolate the latent code linearly and observe that the generated samples vary smoothly and generalize to wide variety of faces, even with accessories such as eyeglasses.

\paragraph{Videos.} For visuals, please open the \texttt{README.html}. The webpage contains all the video results. The webpage is tested to run on a MacOS and Ubuntu and supports Safari, Chrome, and Firefox browsers. All videos are encoded with \texttt{h264} encoding and the raw files are present in the \texttt{data} directory. 

\section*{D. Additional Quantitative Results}
We compare our method with VoLux-GAN~\cite{tan2022volux} and ShadeGAN~\cite{pan2021shadegan} by evaluating the face identity consistency of the generated samples by measuring the cosine similarity between the samples using Arcface~\cite{deng2019arcface}. In Tab.~\ref{tab:voluxgan_id}, we compute the cosine similarity of the face rendered with yaw $\in [-0.5, -0.25, 0.25, 0.5]$ radians with yaw $ = 0$. The methods are trained on different datasets therefore it does not reflect a direct comparison. However, it shows that our method preserves identity similar to the baseline methods.
 
In Tab.~\ref{tab:voluxgan_ill_id}, we evaluate the identity similarity under different lighting conditions as compared to the rendered albedo of the face. Similar to previous evaluation, the methods are trained on different datasets and have different models of illumination. ShadeGAN uses point light sources, and the face consistency is measured under different point light sources and the albedo. In VoLux-GAN, the face consistency is measured under different environment maps and the albedo. In our case, we measure it using 3 different spherical harmonics lights and the albedo. We observe that the face similarity is preserved under different lighting conditions. 


\begin{table*}
    \centering
    \scalebox{1.}{
    \begin{tabular}{lccccc}
    \toprule
     & Dataset & \multicolumn{4}{c}{Yaw changes in radians} \\
    & &  -0.5 & -0.25 & 0.25 & 0.5 \\ 
    \midrule
    ShadeGAN~\cite{pan2021shadegan}  & CelebA & 0.481 & 0.751 & 0.763 & 0.500 \\
    VoLux-GAN~\cite{tan2022volux}  & CelebA & 0.606 & 0.774 & {0.800} & 0.599 \\
    FaceLit-D  & FFHQ & {0.615} & {0.817} & 0.781 & 0.596 \\
    FaceLit-F  & FFHQ & 0.581 & 0.795 & 0.783 & {0.626} \\
    \bottomrule
    \end{tabular}}
    \caption{Face consistency metrics with yaw changes.}
    \label{tab:voluxgan_id}
\end{table*}

\begin{table*}
    \centering
    \scalebox{1.}{
    \begin{tabular}{lcccccc}
    \toprule
     & Dataset & Illumination Type & \multicolumn{3}{c}{Setting} \\
    & & & 1 & 2 & 3\\ 
    \midrule
    ShadeGAN~\cite{pan2021shadegan}  &  CelebA & Point Source & 0.581 & 0.649 & 0.666 \\
    VoLux-GAN~\cite{tan2022volux}  & CelebA & Environment Map &  0.760 & {0.890} & 0.808\\
    FaceLit-D  & FFHQ & Spherical Harmonics & 0.837 & {0.911} & 0.811   \\
    FaceLit-F  & FFHQ & Spherical Harmonics & {0.839}  & 0.906  & {0.815} \\
    \bottomrule
    \end{tabular}}
    \caption{Face consistency metrics with illumination changes.}
    \label{tab:voluxgan_ill_id}
\end{table*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/albedo_img.jpg}
    \caption{\textbf{Albedo Visualization:} Generated images on top with their corresponding albedo images at the bottom.}
    \label{fig:albedo}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/uncurated_ffhq_psi_1.jpg}
    \caption{Uncurated FFHQ samples at $\psi = 1.0$}
    \label{fig:uncurated_ffhq_psi_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/uncurated_ffhq_psi_07.jpg}
    \caption{Uncurated FFHQ samples at $\psi = 0.7$}
    \label{fig:uncurated_ffhq_psi_07}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/latent_interp_image.jpg}
    \caption{\textbf{Latent Space Interpolation:} In each row, we smoothly vary the latent code from left to right.}
    \label{fig:latent_interp}
\end{figure*}
