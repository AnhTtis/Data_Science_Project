\section{Related work}
\label{sec:related}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/pipeline.png}
    \caption{\textbf{Overview:} Our generation process samples a 3D face from a latent $\bf z$ conditioned on pose $\bf p$ and illumination $\ill$. The generator uses a StyleGAN2 backbone with a tri-plane feature representation $\bf f$ introduced by EG3D~\cite{Chan2022eg3d} and introduces illumination modeling using diffuse and specular decoders. Green modules are non-trainable fixed differentiable functions. See \S \ref{sec:generator} for details.}
    \label{fig:pipeline}
\end{figure*}

We first discuss works that focus on decomposing a scene into shape, appearance, and lighting, then discuss those that focus on 3D generative models.

\paragraph{Decomposing a scene into shape, appearance and lighting.}
Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} learn a {5D radiance field}
where the aspects of the scene such as shape, appearance and lighting are jointly modeled by a neural network.
{While effective, this results in an entangled representation of the scene where only the camera pose can be controlled.
In fact, besides controllability, in Ref-NeRF~\cite{verbin2022refnerf}, it was demonstrated that explicitly allowing diffuse and specular rendering paths within the deep architecture, thus having in a sense architectural disentanglement leads to improved rendering quality.
Thus, it is unsurprising that various methods have been proposed for disentanglement.}

Recent works~{\cite{boss2021nerd, zhang2021nerfactor} use}
a Bidirectional Reflectance Distribution function (BRDF) to learn a scene representation using multiview images. 
Neural-PIL~\cite{boss2021neural} computes a pre-integrated lighting map from multiview image collections and models diffuse and specular reflectance in the scene.
SAMURAI~\cite{boss2022samurai} operates on image collections of an object under different views and lighting conditions {with various backgrounds thus reducing the strictness of the multiview constraint.}
{%
However, even with reduced constraint, they still require the \emph{same} object of interest to be in the scene from multiple views. 
In our case, we are interested in the problem setting when only single views are available, which none of these methods can be trivially extended to.
}%


\paragraph{3D generative models.}
Generative Adversarial Networks (GAN)~\cite{goodfellow2020generative} trained from single-view images have been shown to be successful in  generating photorealistic face images~\cite{karras2019stylegan, karras2020stylegan2}.
Early works to enforce 3D consistency on generated images relied on conditioning the GAN with pose~\cite{nguyen2019hologan} and other attributes~\cite{ghosh2020gif, ruiz2020morphgan} such as expression and lighting.
However, these methods do not model the physical rendering process like NeRFs.
%
This leads to inconsistent 3D geometry---rotating images generated from these 2D-based models result in change of shape and appearance and not just the view point.
Follow up work~\cite{chan2021pigan, rebain2022lolnerf, gu2021stylenerf, zhao2022gmpi, schwarz2022voxgraf} thus uses volume rendering---which brings 3D consistency by construction---on top of GANs to force 3D consistent representation. {Furthermore, recent work~\cite{tan2022volux, pan2021shadegan, prao2022vorf} use illumination modeling on top of volume rendering.}
While this allowed the models to be 3D consistent, their generated image quality is not as photorealistic as their 2D counterparts~\cite{karras2020stylegan2}.
%
{%
EG3D~\cite{Chan2022eg3d} thus proposes a tri-plane representation, and a hybrid framework that achieves the level of photorealism similar to 2D GAN frameworks.
}%
% 
{%
In a different direction, Cao et al.~\cite{Cao22} utilize a massive dataset of multiple views of human faces to build a generative model that are conditioned on facial expressions, that are then adapted to a subject of interest for a controllable 3D model.
}%


{%
Regardless of whether these models aim for unconditional generation or controllability, they, however, do not disentangle geometry from illumination and thus cannot be relighted, limiting their application towards a fully controllable generative 3D model.
}%


\subsection{Preliminaries: The EG3D framework}

As we base our framework on the EG3D framework~\cite{Chan2022eg3d}, we first briefly explain the pipeline in more detail before discussing our method.
The core of the EG3D pipeline is the use of tri-plane features, which allows the use of well-studied 2D CNNs for generating deep features to be used for volume rendering.

As shown in \Figure{pipeline}, EG3D uses a tri-plane generator $\triplaneG$ with a StyleGAN2~\cite{karras2020stylegan2} backbone conditioned on camera pose $\pose$ to generate feature maps.
These feature maps are rearranged to obtain tri-plane features $\feat^{XY}, \feat^{YZ}, \feat^{XZ}$ along 3 orthogonal planes.
A decoder neural network is then used to regress color $\bc$ and density $\density$ and additional features $\bw$ at a given location $\bx \in \real^3$ from the tri-plane features.
A color image $\bI_\bc$ is then obtained by aggregating the values $\bc, \sigma$ by volume rendering along the ray $\ray$ given by
% 
\begin{equation}
\bI_\bc (\ray) = \int_{t_n}^{t_f} T(t) \density(\ray(t)) \bc(\ray(t), \bd) dt
,
\label{eq:volume_rendering}
\end{equation}
% 
where spatial locations are sampled within the near and far plane locations as $t \in [t_n, t_f]$, $\bd$ is the viewing direction, and the transmittance 
% 
\begin{equation}
    T(t) = \exp \left ( - \int_{t_n}^t \density(\ray(u)) du \right )
    .
\end{equation}
% 
{This volume rendering of $\bI_\bc$ is performed at a relatively low resolution to be memory efficient, and an upsampling is performed for higher resolution images.
Hence, an additional} $n_w$-channel feature image $\bI_\bw$ is rendered by tracing over the features $\bw \in \mathbb{R}^{n_w}$, {which is then used for generating} the final image $\bI^+_\bc = \upsampleU(\bI_\bc, \bI_\bw)$, with the super-resolution network $\upsampleU$.
{%
The framework is then trained to make $\bI^+_\bc$ as realistic as possible through GAN training setup, with the discriminator being conditioned on the poses $\pose$.
}%

{%
Note here that, as the discriminator is conditioned on the pose, the pose must be provided for each training image; which has been shown to be effective in delivering better 3D consistency~\cite{Chan2022eg3d}.
In the case of EG3D~\cite{Chan2022eg3d}, these poses are obtained using Deng et al.~\cite{deng2019facerecon}.
Similarly, we will rely on DECA~\cite{feng2021deca} since it provides the estimates of poses, as well as illumination.
}%


