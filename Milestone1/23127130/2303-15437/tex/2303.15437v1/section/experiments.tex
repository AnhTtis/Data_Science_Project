
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental setup}


\paragraph{Datasets.}
We use three datasets for our experiments: FFHQ~\cite{karras2019stylegan}, MetFaces~\cite{karras2020training}  and CelebA-HQ~\cite{karras2018progressive}.
FFHQ contains 70,000 samples and CelebA-HQ contains 30,000 samples of real human faces {as both datasets have been used traditionally to evaluate GAN methods}.
MetFaces contains 1,336 samples of faces taken from museum art images, {which is a small dataset that we use to demonstrate that our method can be applied beyond real face photos}.


\paragraph{Implementation details.}
As in Chan~\etal~\cite{Chan2022eg3d}, we mirror samples in each of the datasets to double the number of training samples.
We estimate the camera poses $\pose$ and illumination coefficients $\ill$ using DECA~\cite{feng2021deca}.

{We apply slightly varying training strategies for each dataset to account for their image resolution and volume.}
For FFHQ, we follow a strategy of EG3D~\cite{Chan2022eg3d} and train in two stages with a batch size of 32 on 8 GPUs.
In the first stage, we train for 750k iterations where
we volume render the images at $64^2$ and super resolve them to $512^2$.
In the second stage, we adjust the rendering resolution to $128^2$
{and super resolve them to $512^2$,} and train them further for 750k iterations.
% 
For the CelebA-HQ dataset, we train only using the first stage at a rendering resolution of $64^2$ that are superresolved to $512^2$, for {500k iterations}.
% 
{%
For the MetFaces dataset, as the sample size is small, we use a model pretrained on FFHQ and fine tune it on MetFaces via ADA augmentation~\cite{karras2020training}.
{We train for 15k iterations.}
}
We detail the network architectures in the supplementary material.

\input{figs/ffhq_samples.tex}
\input{figs/pose_light_interp.tex}
\input{figs/metfaces_celeba_samples}


\subsection{Qualitative results}

{%
We first qualitatively demonstrate the effectiveness of our method.
}%

\paragraph{Randomly drawn samples.}
We demonstrate the quality of our generated samples from the FFHQ dataset in \Figure{ffhq_samples}.
% 
For our results, we also visualize the 3D reconstruction and the illumination used when generating these samples via a matt sphere for easy verification of the illumination consistency of each sample.
As shown, our results are photorealistic, also with fine details. Furthermore, the 2D images rendered from our model is visually consistant with the 3D shapes.
Note especially the regions around the lips and the teeth where our model provides improved 3D shape compared to EG3D, benefitting {from the specular modeling.}

We further show samples from CelebA-HQ and MetFaces datasets in \Figure{metfaces_celeba_samples}.
{%
For both datasets our results deliver rendering quality that photorealistic, or indistinguishable form actual paintings, while still providing explicit control over illumination and camera pose. We provide additional visualizations in the supplementary material.
}%







\paragraph{Controlling pose and illumination.}
In \Figure{pose_light_interp}, we fix the latent code for generating a face and vary the camera pose and illumination. Each row corresponds to a different illumination condition. We note that the lighting matches over two different persons. We also see the effect of strong lights in the middle row versus weak directional lights on the top and the bottom row.
Our model provides rendering that remains consistent regardless of the pose and illumination change.
We show additional results 
in the supplementary material.

\input{figs/specular.tex}

\paragraph{Specular highlights.}
In \Figure{specular}, we show the effect of varying the specular component on the generation process. We vary the strength of specular component across each row in \Figure{specular}.
Note how the generated results for lower specularity seem matt, which is to be expected. Under normal conditions of specularity, highlights on the nose and cheeks are pronounced adding to the realism of the generated faces. Furthermore, using higher specularity results in glare.


\subsection{Quantitative results}
We further evaluate our results quantitatively through various metrics.

\paragraph{Evaluation metrics.}
We benchmark our generation quality on key metrics: FID score~\cite{heusel2017fid}, KID score~\cite{binkowski2018demystifying, heusel2017fid}, identity consistency (ID), depth, pose and light consistency.
\vspace{-0.7em}
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-.3em}
    \item {\bf FID~\cite{heusel2017fid} and KID~\cite{binkowski2018demystifying, heusel2017fid}:} As in~\cite{Chan2022eg3d}, we sample 50,000 faces from the model and compute the score against the full dataset. 
    The FID and KID score captures the photorealism in the generated samples.
    \item {\bf Identity consistency (ID):}
    As in \cite{Chan2022eg3d, shi2021lifting, zhao2022gmpi, chan2021pigan}, we compute ID with the mean Arcface~\cite{deng2019arcface} cosine similarity, after rendering a face with two random camera views.
    The measure highlights the consistency of the face under different rotations.
    
    \item {\bf Consistency:}
    We further evaluate our model on camera pose, depth and illumination consistency.
    We sample 1024 faces from the generator along with their camera pose, illumination and depth using the our model.
    We then use DECA~\cite{feng2021deca} to obtain psuedo ground truths for camera pose, illumination of the generated samples. {This differs from baseline methods~\cite{Chan2022eg3d,zhao2022gmpi, chan2021pigan, shi2021lifting} that use Deng et al.~\cite{deng2019facerecon} for pose estimation and preprocessing. Since DECA has better performance, this also contributes to lower error in our evaluation.} For depth consistency, following  previous work~\cite{Chan2022eg3d, zhao2022gmpi, shi2021lifting}, we estimate pseudo ground truth depth from Deng et al.~\cite{deng2019facerecon}. We report the mean square error between our estimates and the pseudo ground truth estimates.
\end{itemize}





\vspace{-0.6em}
\paragraph{Variants.}
{%
For the quantitative study we evaluate four different variants of our method.
}%
We introduce two sets of models \ours-d and \ours-f using the diffuse-only model and the full model respectively with volume rendering resolution of $64^2$ and superresolved to $512^2$.
We further train the models \ours-D and \ours-F at volume rendering resolution of $128^2$ and superresolved to $512^2$.



\paragraph{FFHQ.}
We report the quatitative evaluations on the FFHQ dataset in~\Table{ffhq_eval} and compare with previous work on 3D aware GANs.
We observe that \ours-D and \ours-F obtain state-of-the-art in photorealism metrics---FID, KID and competitive performance on ID and depth consistency metrics. Amongst methods that generate at $512^2$ resolution, we achieve state-of-the-art accuracy on depth consistency metrics. We also note that although diffuse models achieve state-of-the-art on photorealism metrics, the full models that also model the specular components have better depth.
On the pose consistency metrics, we achieve state-of-the-art performance.

\input{tables/ffhq_eval.tex}


\paragraph{MetFaces.}
In \Table{metfaces_eval}, we compare the performance of our model to a 2D GAN --  StyleGAN2~\cite{karras2020stylegan2}, and a 3D aware GAN, StyleNeRF~\cite{gu2021stylenerf}. We obtain better photorealism than both the methods on FID and KID scores. We further provide face consistency metrics and 3D consistency metrics -- depth and pose. We note that the depth metrics obtained on this dataset are worse than those of FFHQ. A reason for this could be that art images do not strictly respect the physical model of illumination.

\input{tables/metfaces_eval}
\input{tables/celeba_eval.tex}

\paragraph{CelebA-HQ.}
In \Table{celeba_eval}, we evaluate our models trained on a smaller dataset, CelebA-HQ with 30,000 samples. We note that our models can achieve better photorealism scores than 2D GANs such as StyleGAN and has good performance on consistency metrics. Furthermore, we also note that the full model,~\ours-f provides better depth accuracy than diffuse only,~\ours-d model.


\subsection{Illumination accuracy}
\label{sec:ablation}



{%
We further report how accurate our model learns the illumination effects by generating random samples and running DECA~\cite{feng2021deca} on them to see how well the estimated illumination agree with the conditioned illumination. We use mean square error of the SH coefficients averaged over 1024 random samples from our model. The samples are conditioned on pose and illumination randomly sampled from the training dataset.
While imperfect, as these results will be limited by the accuracy of DECA~\cite{feng2021deca}, it allows us to roughly gauge the {accuracy of our illuminations}.
We report these results in \Table{light_accuracy}.
}%
{%
As reported, our model generates images that are well inline with the DECA estimates.
}%
We show the visualizations in \Figure{illumination_visuals}. 
\input{tables/light_accuracy.tex}
\input{figs/illumination_visuals.tex}


