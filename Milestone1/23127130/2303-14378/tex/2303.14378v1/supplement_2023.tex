\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version


% For camera ready (Optional, but recommended)
\usepackage{axessibility}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{balance}
\usepackage{multirow}
\usepackage[usestackEOL]{stackengine}

\usepackage[normalem]{ulem}   % sout (strikethrough)
\usepackage[table]{xcolor}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\algrenewcommand\algorithmicfor{\textbf{For}}
\algrenewcommand\algorithmicwhile{\textbf{While}}

\renewcommand{\thesubsection}{\Alph{subsection}}
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}


% To use \degree command
\usepackage{gensymb}

\newcommand\secondbest[1]{\textbf{\textcolor{blue}{#1}}}
\newcommand{\OursAcronym}{LiDomAug}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\definecolor{brightmaroon}{rgb}{0.76, 0.13, 0.28}
\definecolor{burgundy}{rgb}{0.5, 0.0, 0.13}

\input{misc/macro}

\appendix
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4665} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Instant Domain Augmentation for LiDAR Semantic Segmentation \\
\emph{Supplementary Material}}

\author{Kwonyoung Ryu$^{1*}$
\hspace{8mm}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
Soonmin Hwang$^{2*}$
\hspace{8mm}
Jaesik Park$^1$
\vspace{2mm}\\
$^1$POSTECH 
\hspace{8mm}
$^2$Carnegie Mellon University\\
{\tt \small \url{http://cvlab.postech.ac.kr/research/LiDomAug}}
}
\maketitle
\def\thefootnote{*}\footnotetext{Equal contribution}\def\thefootnote{\arabic{footnote}}

\renewcommand\thefootnote{\textcolor{red}{\arabic{footnote}}}


This material provides detailed information on handling dynamic objects, label consistency and propagation, detailed LiDAR configurations, and experiment settings. 
We also present per-class domain adaptation results, examples of the generated LiDAR scans, and more qualitative results from 3D semantic segmentation models trained with \OursAcronym.



\section{Dynamic Objects}
\vspace{-2mm}

\begin{algorithm}[h]
    \caption{Dynamic object accumulation}
    \begin{algorithmic}[1]
        \Require{Sequence of point clouds $\{\mathcal{P}_n\}_{n=1}^N$,
        3D bounding boxes $\{\mathbf{b}_n^k\}_{n=1}^N$ of an object $k$, and
        its frame-to-frame transformation matrices $\{\mathbf{T}_n^{k}\}_{n=1}^N$} 
        \Ensure{Accumulated points $\mathcal{P}^k$ of an object $k$}
        \State{$\mathcal{P}^k \longleftarrow \{ \ \}$}
        \State{$\mathbf{T} \longleftarrow \mathbf{I}$}
        \For{$n=1:N$}
        \State{$\mathbf{T} \longleftarrow \mathbf{T}\circ(\mathbf{T}_n^{k})^{-1}$}
        \State{$ \mathcal{P}^k \longleftarrow \{ \mathcal{P}^k \cup \mathbf{T}(\mathcal{P}_n \cap \mathbf{b}_n^k)) \}$}
        \EndFor
    \end{algorithmic}
    \label{alg:dynamic_object_accumulation}
\end{algorithm}

As described in the method section, object-wise motion causes an error, so-called flying points, in constructing the world model if we only use global ego-motion in the aggregation step. If the trajectories of each object over time are provided, the object-wise motion could be canceled out by applying the inverse of them, described in Algorithm \ref{alg:dynamic_object_accumulation}.

As nuScene-lidarseg~\cite{nuscenes_lidarseg} dataset provides the information of object-wise motion in the form of 3D bounding box annotations\footnotemark[1], we apply the Algorithm \ref{alg:dynamic_object_accumulation} to build a better world model.
On the other hand, the object-wise motion information is not available in SemanticKITTI~\cite{semantic_kitti}. In this case, we set a small temporal adjacency for dynamic objects to minimize the error while maximizing the density of the world model, then accumulate the 3D points on dynamic objects by applying global inverse ego-motion.

\footnotetext[1]{We downloaded 3D bounding box annotations from nuScenes full dataset~\cite{nuscenes2019}, not from the nuScenes-lidarseg subset~\cite{nuscenes_lidarseg}.}

\begin{algorithm}[h]
    \caption{Label consistency check and propagation}
    \begin{algorithmic}[1]
        \Require{Point clouds $\{\mathcal{P}_n\}_{n=1}^{N}$}, its semantic label $\{\mathcal{L}_n\}_{n=1}^{N}$, ego-motion $\{\mathbf{T}_n\}_{n=1}^N$, and unlabeled point cloud $\mathcal{P}_u$
        \Ensure{labels $\mathcal{L}$ for $\mathcal{P}_u$}        
        \State{$\mathcal{P}_{world} \longleftarrow \bigcup_{n=1}^N \mathbf{T}_n^{-1}(\mathcal{P}_n)$}        
        \State{$\mathcal{L}_{world} \longleftarrow \bigcup_{n=1}^N \mathcal{L}_n$}
        \State{$\{\mathbf{v}_i\}_{i=1}^{V}\longleftarrow$ Voxelization($\mathcal{P}_{world}$)}
        \For{$i=1 : V$}
        \State{Make $\mathcal{J}=\{j\}$, s.t. $\mathbf{p}_j \in \mathbf{v}_i$ and $\mathbf{p}_j \in \mathcal{P}_{world}$}
        \State{Make $\mathcal{K}=\{k\}$, s.t. $\mathbf{p}_k \in \mathbf{v}_i$ and $\mathbf{p}_k \in \mathcal{P}_{u}$}        
        \State{$l^*_i$ = MostFrequent$(\{l_j|j\in\mathcal{J}\})$, s.t. $l\in\mathcal{L}_{world}$}
        \State{$\mathcal{L}(\mathcal{J})\leftarrow l^*_i$} \Comment{Label consistency}
        \State{$\mathcal{L}(\mathcal{K})\leftarrow l^*_i$} \Comment{Label propagation}        
        \EndFor
    \end{algorithmic}
    \label{alg:label_consistency}
\end{algorithm}



\section{Label consistency check and propagation}

Whereas SemanticKITTI~\cite{semantic_kitti} provides 3D semantic labels for every frame, nuScene-lidarseg~\cite{nuscenes_lidarseg} dataset provides labels only for key-frames sampled at 2Hz. Also, it is difficult for human labelers to label 3D points accurately when the semantics change (e.g., the boundary of the object) or the 3D points are too sparse, so labeling errors occur. As described in the method section, we prepare an examination step for label consistency check and propagation. The 3D points in a world model are divided by a small voxel grid ($0.1m \times 0.1m \times 0.1m$), and the representative labels for each voxel are determined by majority voting. Based on the representative labels, we can correct the semantic labels of 3D points or assign a new label to the unlabeled 3D points. The detailed description of this procedure is shown in Algorithm \ref{alg:label_consistency}. During this procedure, 6.4 points (nuScenes-lidarseg~\cite{nuscenes_lidarseg}) or 8.8 points (SemanticKITTI~\cite{semantic_kitti}) are assigned to a single voxel in average.


\begin{table*}[t]
    \begin{center}    
    \scalebox{0.85}{
        \begin{tabular}{l|c|c|c|c|c}
            \toprule
            \multirow{2}{*}{Model}
            & \multirow{2}{*}{\Centerstack{Velodyne \\ HDL-64E}} 
            & \multirow{2}{*}{\Centerstack{Velodyne \\ HDL-32E}} 
            & \multirow{2}{*}{\Centerstack{Velodyne \\ VLP-16}} 
            & \multirow{2}{*}{\Centerstack{Ouster\\OS-1 64}} 
            & \multirow{2}{*}{\Centerstack{Ouster \\ OS-1 128}} \\ 
            &&&&& \\ \midrule

            {Channels ($H$)}                        & {64}              & {32}                & {16}  & {64}  & {128}              \\ 
            {Range ($r$)}                           & {120 m}           & {100 m}             & {100 m} & {120 m}& {120 m}            \\ 
            {Field of View ($f_{up}, f_{down}$)}        & {+2.0\degree to -24.9\degree } & {+10.67\degree to 30.67\degree } & {+15.0\degree to -15.0\degree } & {+22.5\degree to -22.5\degree } & {+22.5\degree to -22.5\degree } \\ 
            Horizontal angular resolution ($W$) & {2048}  & {2048}    & {2048} & {1024} & {1024} \\
            Clock-wise rotation rate ($\omega_{0}$)    & {20Hz}             & {20Hz}                & {20Hz}  & {20Hz} & {20Hz} \\
            \bottomrule
        \end{tabular}
    }
    \end{center}
    \vspace{-3mm}
    \caption{Detailed configurations of various LiDARs used in our experiments}
    \label{tab:LiDAR_config}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figure/supplementary/supple_MinkNet42_v2.pdf}
    \caption{Architecture of MinkowskiNet42(Res16UNet34)\cite{Minkowski}. `Sparse Conv Tr' means transposed sparse convolution.}
    \label{fig:MinkNet42}
\end{figure*}



\vspace{5mm}
\section{Details of LiDAR Configuration}

As mentioned in the experimental section (Table 3), we consider five popular LiDARs from different vendors such as Velodyne\cite{velodyne2007} and Ouster\cite{ouster2018}. Table \ref{tab:LiDAR_config} shows the specifications of the considered LiDARs. 
\vspace{5mm}



\section{Experiment Settings}
\vspace{-6mm}

\noindent \paragraph{Architecture.}
We use MinkNet42~\cite{Minkowski}, and a variant of MinkNet from C\&L~\cite{complete_and_label_cvpr21} and SPVCNN~\cite{SPVNAS_ECCV20} as our baselines. We implement the sparse-convolution-based SPVCNN backbone using MinkowskiEngine~\cite{Minkowski} for a NAS-based backbone experiment. And the MinkNet42 consists of five planes of the encoder and four planes of the decoder. Each res-blocks is set to [32, 64, 128, 256, 256, 128, 96, 96] dimensions with two convolution layers each. Figure \ref{fig:MinkNet42} shows the MinkNet42 architecture we use throughout the experiments. For C\&L backbone, we follow the details in C\&L paper~\cite{complete_and_label_cvpr21} to implement the U-Net style custom MinkNet, which has [24, 32, 48, 64, 80, 96, 112] dimensions for the encoder blocks and [96, 80, 64, 48, 32, 16] dimensions for the decoder blocks. Please refer to the original paper~\cite{complete_and_label_cvpr21} for more details. We use voxel size as $d=5cm$ for all three backbones.
\vspace{2mm}

\noindent \paragraph{Training.}
We use SGD optimizer with the momentum of $0.9$ and weight decay of $10^{-4}$. The initial learning rate is set to $10^{-1}$, decayed by $0.1$ at [3, 8, 15] epochs. We use a batch size of four and use the cross-entropy loss. The training and evaluation are done with a single NVIDIA GeForce RTX 3090 GPU.



\section{Additional Domain Adaptation Scenarios}

\begin{table}[t]
    \newcommand{\minus}[1]{{\textcolor{purple}{$\downarrow$ #1}}}
    \newcommand{\plus}[1]{{\textcolor{teal}{$\uparrow$ #1}}}
    \newcommand{\ccgreen}{\cellcolor{yellow!10}}
    
    \newcommand{\xmark}{\textcolor{teal}{\ding{55}}}%
    \newcommand{\cmark}{\textcolor{purple}{\ding{51}}}%

    \newcolumntype{L}{>{\hspace{0.7em}}l}
    \newcolumntype{R}{>{\hspace{0.6em}}r}

    \centering
    \resizebox{1.0\columnwidth}{!}{
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{c|L|R@{\hspace{0.5em}}lR@{\hspace{0.5em}}l}    
            \multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{4}{r}{Unit: mIoU (Rel.\%)}\\
            \toprule
            \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Backbone\\(\# of params)\end{tabular}}}
            &\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Methods\end{tabular}}}
            &\multicolumn{4}{c}{Source $\rightarrow$ Target} \\
            \cline{3-6}         
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &\multicolumn{2}{c}{$K \rightarrow P$} & \multicolumn{2}{c}{$N \rightarrow P$} \\                 
            \cmidrule[0.8pt]{1-6}
            \multicolumn{1}{c|}{\multirow{3}{*}{\Centerstack{MinkNet42~\cite{Minkowski}\\(37.8M)}}} 
            & Baseline & \multicolumn{2}{c}{9.5} & \multicolumn{2}{c}{11.3} \\        
            &\ccgreen&\ccgreen&\ccgreen&\ccgreen&\ccgreen\\
            &\multirow{-2}{*}{\ccgreen\shortstack[l]{Baseline\\+ \OursAcronym{}}} & \multirow{-2}{*}{\ccgreen \textbf{11.5}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{21.1}})}& \multirow{-2}{*}{\ccgreen \textbf{28.1}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{148.6}})}\\
            
            \bottomrule
        \end{tabular}
    }
    
    \caption{                        
        Training with SemanticKITTI~\cite{kitti} and testing with Pandaset~\cite{pandaset} (K$\rightarrow$P) and vice versa (N$\rightarrow$P) with MinkNet42~\cite{Minkowski}
    }        
    \label{tab:mink_pandaset}
\end{table}


We conduct an additional experiment on \textit{Pandaset}~\cite{pandaset}, which consists of one forward-facing \textit{solid-state LiDAR} and one \textit{spinning Hesai LiDAR}, to investigate whether \OursAcronym{} is effective beyond a single cylindrical LiDAR setup. We train models on SemanticKITTI or nuScenes and evaluate them on \textit{Pandaset}. The results, as shown in Table.~\ref{tab:mink_pandaset}, reveal that our method is effective in both $K\rightarrow P$ and $N\rightarrow P$ scenarios, indicating the potential advantages of our method in complex LiDAR setups.


\begin{table*}[t]
    \newcommand{\ccgreen}{\cellcolor{yellow!10}}
    \resizebox{2.1\columnwidth}{!}{
        \begin{tabular}{c|l|rr|rrrrrrrrrr|r}
        \toprule
        \multicolumn{1}{c|}{Backbone} & Methods & \multicolumn{1}{c}{mIoU} & \multicolumn{1}{c|}{mAcc} &
        \multicolumn{1}{c}{\rotatebox{90}{car}} & 
        \multicolumn{1}{c}{\rotatebox{90}{bicycle}} & 
        \multicolumn{1}{c}{\rotatebox{90}{motorcycle}} & 
        \multicolumn{1}{c}{\rotatebox{90}{truck}} & 
        \multicolumn{1}{c}{\rotatebox{90}{\shortstack{other\\vehicle}}} & 
        \multicolumn{1}{c}{\rotatebox{90}{pedestrian}} & 
        \multicolumn{1}{c}{\rotatebox{90}{\shortstack{drivable\\surface}}} & 
        \multicolumn{1}{c}{\rotatebox{90}{sidewalk}} & 
        \multicolumn{1}{c}{\rotatebox{90}{terrain}} & 
        \multicolumn{1}{c|}{\rotatebox{90}{vegetation}} &
        \multicolumn{1}{c}{\rotatebox{90}{Avg. rank}} \\ \midrule
        \multirow{6}{*}{\rotatebox{90}{MinkNet42}}
        & Baseline & 37.8 & 48.1 & 50.7 & 5.65 & 5.96 & 21.7 & 24.8 & 29.2 & \textbf{89.1} & 42.0 & 23.1 & 85.8 & 3.5\\ 
        \cline{2-15}
        &  CutMix~\cite{Cutmix}  &  37.1 &  46.4 &  75.5 &   0.05 & 14.0 &  26.6 &  22.6 &  3.92 &  86.6 &  36.5 &  19.7 &  85.6 &  4.7\\ 
        &   Copy-Paste~\cite{Copy-Paste} &  38.5 &  48.8 &  77.9 &  3.11 &  11.1 &  21.7 &  31.2 &  7.81 &  88.0 &  38.8 &  19.6 &  86.2 &  3.6\\ 
        &   Mix3D~\cite{Mix3d}   &  43.1 &  52.7 &  72.1 &  0.00 &  34.8 &  11.7 &  26.4 &  28.5 & 83.3 &  41.0 & \textbf{46.4} & \textbf{86.5} & 3.7\\ 
        & Polarmix~\cite{Polarmix} &  45.8 &  54.4 &  74.1&  1.68 &  \textbf{41.9} &  26.9&  23.8 &  \textbf{30.5} &  85.1 &  \textbf{42.7} &  45.3 &  86.2 &  2.7\\ 
        &\ccgreen Baseline+\OursAcronym~ & \ccgreen \textbf{45.9} & \ccgreen \textbf{55.0}& \ccgreen \textbf{79.2}& \ccgreen \textbf{5.78} & \ccgreen 28.0 & \ccgreen \textbf{49.3}& \ccgreen \textbf{32.1} & \ccgreen 13.8 & \ccgreen 88.0 & \ccgreen 42.0 & \ccgreen 35.4 & \ccgreen 85.1 & \ccgreen \textbf{2.4}\\ \bottomrule        
    \end{tabular}}
    \caption{Per-class accuracy from various data augmentation methods. All the models are trained on SemanticKITTI, and tested on nuScene-lidarseg (K$\rightarrow$N).}
    \label{tab:S2N}
\end{table*}


\begin{table*}[!h]
    \newcommand{\ccgreen}{\cellcolor{yellow!10}}
    \resizebox{2.1\columnwidth}{!}{
        \begin{tabular}{c|l|rr|rrrrrrrrrr|r}
        \toprule
        \multicolumn{1}{c|}{Backbone} & Methods & \multicolumn{1}{c}{mIoU} & \multicolumn{1}{c|}{mAcc} &
        \multicolumn{1}{c}{\rotatebox{90}{car}} & 
        \multicolumn{1}{c}{\rotatebox{90}{bicycle}} & 
        \multicolumn{1}{c}{\rotatebox{90}{motorcycle}} & 
        \multicolumn{1}{c}{\rotatebox{90}{truck}} & 
        \multicolumn{1}{c}{\rotatebox{90}{\shortstack{other\\vehicle}}} & 
        \multicolumn{1}{c}{\rotatebox{90}{pedestrian}} & 
        \multicolumn{1}{c}{\rotatebox{90}{\shortstack{drivable\\surface}}} & 
        \multicolumn{1}{c}{\rotatebox{90}{sidewalk}} & 
        \multicolumn{1}{c}{\rotatebox{90}{terrain}} & 
        \multicolumn{1}{c|}{\rotatebox{90}{vegetation}} &
        \multicolumn{1}{c}{\rotatebox{90}{Avg. rank}} \\ \midrule
        \multirow{6}{*}{\rotatebox{90}{MinkNet42}}
        & Baseline & 36.1 & 46.9 & 78.5 & 0.00 & 8.17 & 3.37 & {11.1} & \textbf{34.5} & 66.3 & 35.8 & 39.4 & {84.2} & 4.5\\ 
        \cline{2-15}
        &  CutMix~\cite{Cutmix}  &  37.6 &  51.4 &  81.2 &   0.00 &  5.28 &  9.09 &  \textbf{17.4} &  11.8 &  73.6 &  45.5 &  46.8 &  85.7 &  3.9\\ 
        &   Copy-Paste~\cite{Copy-Paste} &  41.1 &  56.6 &  85.7 &  0.00 &  8.17 &  12.8 &  6.46 &  28.6 &  \textbf{80.8} &  \textbf{47.4} &  53.8 &  87.2 &  2.8\\ 
        &   Mix3D~\cite{Mix3d}   &  44.7 &  63.9 &  \textbf{93.1} &  10.4 &  31.3 &  17.0 &  14.1 &  {34.2} &  {71.8} &  40.7 &  {44.6}&  \textbf{89.5} &  {2.8}\\ 
        & Polarmix~\cite{Polarmix} &  {39.1} &  {61.2}&  {75.9}&  19.4 &  19.7 &  {9.60}&  {3.03} &  18.3 &  75.0 &  {43.1} &  48.9 &  77.8 &  {4.0}\\ 
        &\ccgreen Baseline+\OursAcronym~ & \ccgreen \textbf{48.3} & \ccgreen \textbf{69.0}& \ccgreen 92.6& \ccgreen \textbf{31.6} & \ccgreen \textbf{42.5} & \ccgreen \textbf{21.6}& \ccgreen 6.43 & \ccgreen 34.4 & \ccgreen 70.0 & \ccgreen 47.1 & \ccgreen \textbf{59.4} & \ccgreen 77.5 & \ccgreen \textbf{2.6}\\ \bottomrule
    \end{tabular}}
    \caption{Per-class accuracy from various data augmentation methods. All the models are trained on nuScene-lidarseg, and tested on SemanticKITTI (N$\rightarrow$K).
    \label{tab:N2S}
    }
\end{table*}



\section{Per-class Domain Adaptation Results}

In Table \ref{tab:S2N} (K$\rightarrow$N) and Table \ref{tab:N2S} (N$\rightarrow$K), we present per-class IoU numbers from our experiments, and Baseline + \OursAcronym{} achieves the best performances in mIoU, mAcc, and avg. rank metrics. Particularly, as shown in Table \ref{tab:N2S} (N$\rightarrow$K), our Baseline + \OursAcronym{} setting, in which the label propagation and dynamic object handling are applied, significantly improves the performance of moving object classes (bicycle: $0.00 \rightarrow 31.6$, motorcycle: $8.17 \rightarrow 42.5$, truck: $3.37 \rightarrow 21.6$). Note that our final model, Baseline+\OursAcronym{}, achieves the best performance in average rank across classes, implying that \OursAcronym{} helps the model generalize over multiple classes beyond sensor-shift.
 
\newpage

\section{More Qualitative Results}

We included a video file, LiDomAug.mp4, to showcase (1) augmented frames using \OursAcronym{}, (2) dynamic object accumulation, and (3) label consistency check and propagation. We also present the qualitative results of (4) the evaluation of generated LiDAR frames and (5) the evaluation of two different datasets. Figure \ref{fig:simulation} and \ref{fig:augmentation} represent the qualitative examples from \OursAcronym{} in SemanticKITTI. Our data generation can imitate the LiDAR geometric patterns with various configurations. Figure \ref{fig:k2k_comparison_qual}, \ref{fig:k2n_n2k_comparison_qual}, \ref{fig:k2n_n2k_comparison_qual_spvcnn} shows the qualitative comparison between the model trained with and without \OursAcronym{}, and show significant improvement. 



\begin{figure*}[b]
    \centering
    \includegraphics[width=1\textwidth]{figure/supplementary/simulation_qual_v3.pdf}

    \caption{Augmented LiDAR frames using SemanticKITTI dataset. We augment 5 types of LiDARs (Table~\ref{tab:LiDAR_config}). We mark V64 as the original frame because SemanticKITTI is captured with Velodyne HDL-64E.}
    \label{fig:simulation}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figure/supplementary/qual_augmentation_v3.pdf}

    \caption{Data augmentation using \OursAcronym on SemanticKITTI.  (a) Original frames (b) Generated frames after pose augmentation (c) Rendered frames with the consideration of frame distortion (d) Mixed two scenes with various LiDAR configurations. \textcolor{red!80}{The red cross} means the center of the original frames, and \textcolor{brightmaroon}{the dark red box} highlights the changes. Note that the motion distortion exists in the original frames(1st,3rd row: distorted by LiDAR move straight, 2nd row : distorted by LiDAR rotate counter clock-wise, 4th row : distorted by LiDAR rotate counter clock-wise) as \textcolor{blue!80}{the blue box} highlighted.}
    \label{fig:augmentation}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figure/supplementary/Towards_sensoragnostic.pdf}
    
    \caption{Evaluation of inconsistent LiDAR configuration (corresponds to Table 3 in the main paper). All the models here are trained with/without \OursAcronym{} on SemanticKITTI. \textcolor{red!80}{The red points} indicate the wrong predictions. Train$\rightarrow$Test: (a) V64 (original; w/o \OursAcronym{})$\rightarrow$V32 (b) \OursAcronym{}$\rightarrow$V32 (c) V64 (original; w/o \OursAcronym{})$\rightarrow$V16 (d) \OursAcronym{} $\rightarrow$V16. \textcolor{brightmaroon}{The dark red box} highlights the changes. As shown in columns (b) and (d), \OursAcronym{} helps relieve erroneous predictions from inconsistent LiDAR types in training and testing.}
    \label{fig:k2k_comparison_qual}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figure/supplementary/qual_mink.pdf}
    
    \caption{Evaluation on domain adaptation settings with MinkNet42~\cite{Minkowski} backbone (corresponds to Table 1 in main paper). \textcolor{red!80}{The red points} indicate the wrong predictions. Train$\rightarrow$Test: (a) Baseline MinkNet42 (source LiDAR setting V64 $\rightarrow$ target LiDAR setting V32) (b) Trained with our \OursAcronym{} (c) Baseline MinkNet42 (source LiDAR setting V32 $\rightarrow$ target LiDAR setting V64) (d) Trained with our \OursAcronym{}. \textcolor{brightmaroon}{The dark red box} highlights the changes. \OursAcronym{} (b, d) helps to relieve erroneous predictions from dataset shifts in training and testing.}
    \label{fig:k2n_n2k_comparison_qual}
\end{figure*}

\begin{figure*}[t]    
    \centering
    \includegraphics[width=1\textwidth]{figure/supplementary/qual_spvcnn.pdf}
    
    \caption{Evaluation on domain adaptation settings with SPVCNN~\cite{SPVNAS_ECCV20} backbone (corresponds to Table 5 in main paper). \textcolor{red!80}{The red points} indicate the wrong predictions. Train$\rightarrow$Test: (a) Baseline SPVCNN (source LiDAR setting V64 $\rightarrow$ target LiDAR setting V32) (b) Trained with our 
    \OursAcronym{} (c) Baseline SPVCNN (source LiDAR setting V32 $\rightarrow$ target LiDAR setting V64) (d) Trained with our 
    \OursAcronym{}. \textcolor{brightmaroon}{The dark red box} highlights the changes. \OursAcronym{} (b, d) helps to relieve erroneous predictions from dataset shifts in training and testing.}    
    \label{fig:k2n_n2k_comparison_qual_spvcnn}
\end{figure*}



\clearpage
%%%%%%%%% REFERENCES
\balance
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document} 