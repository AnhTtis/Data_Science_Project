\section{Introduction}
\label{sec:introduction}

LiDAR (Light Detection And Ranging) is a modern sensor that provides reliable range measurements of environments sampled from 3D worlds and has become crucial for intelligent systems such as robots~\cite{behley2018rss,LOAM}, drones~\cite{drone}, or autonomous vehicles~\cite{LiDAR_autonomous_article, kitti}. Therefore, developing resilient 3D perception algorithms for LiDAR data~\cite{RandLANet_CVPR20,PointPillars,PseudoLiDAR} is becoming more crucial. 


With the growing interest in LiDAR sensors, various LiDAR sensors from multiple manufacturers have become prevalent. As a result, popular 3D datasets~\cite{nuscenes2019,chang2019argoverse,kitti,geyer2020a2d2,huang2018apolloscape} are captured by different LiDAR configurations, which are defined by vertical/horizontal resolutions, a field of view, and a mounting pose. Due to the difference in sampling patterns from various LiDAR configurations, the \emph{sensor-bias problem} arises in 3D perception algorithms~\cite{complete_and_label_cvpr21,triess2021survey}. For example, as shown in Figure~\ref{teaser_introduction}, we observe a severe performance drop in LiDAR semantic segmentation task if the LiDAR used to collect the test set differs from the LiDAR used for the training set.

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=1.0\linewidth]{figure/Introduction/teaser_cam_ready_2.pdf}
    \end{center}
    \vspace{-3mm}
    
    \caption{
        Data-driven LiDAR semantic segmentation methods often show an accuracy drop when they are applied to unseen LiDAR configurations. (Left) A result of a baseline approach, where inaccurate predictions are highlighted with orange arrows. (Right) A result of the baseline approach aided with the proposed~\OursAcronym{}. For both results, we use nuScenes~\cite{nuscenes2019} (32 ch.) for the training and use SemanticKITTI~\cite{semantic_kitti} (64 ch.) for the testing.        
    }
    \vspace{-3mm}
    \label{teaser_introduction} 
\end{figure}

Although the sensor-bias problem is crucial, an existing solution, such as domain adaptation, is tuned for a specific LiDAR configuration, which is suboptimal to designing a sensible 3D perception method. Specifically, Supervised Domain Adaptation requires massive labeling costs to learn to adapt to the new data captured with a target sensor. Hence, such an approach is often not viable in practice. Unsupervised Domain Adaptation~\cite{jaritz2020xmuda,jiang2021lidarnet,complete_and_label_cvpr21} aims to make a model adapt to a target domain without using direct annotations. However, there is an accuracy degradation, and such approaches require enough collection of target domain data. Thus, it is demanding to design a new approach that can be applied instantly to an unseen target domain without requesting any target domain data.


By focusing on the widely used cylindrical LiDARs, this paper presents a new approach to alleviate the sensor-bias problem. The proposed method, called \emph{\OursAcronym{}}, augments the training data based on arbitrary cylindrical LiDAR configurations, mounting pose, and motion distortions. The proposed on-demand augmentation module runs at 330 FPS, which can be regarded as an \emph{instant domain augmentation}. This flexibility, which is a key strength of our method, enables us to train a sensor-agnostic model that can be directly applied to multiple target domains. 


We demonstrate our method on the task of LiDAR semantic segmentation. In particular, we tackle the domain discrepancy problem when the LiDAR sensors used for making the training and the test data are not consistent. Interestingly, learning-based approaches aided with the proposed \OursAcronym{} outperform the state-of-the-art Unsupervised Domain Adaptation approaches~\cite{FeaDA,SWD,OutDA,3DGCA,complete_and_label_cvpr21} without access to any target domain data. Our method also beats Domain Mapping~\cite{domain_transfer_iros20, bevsic2021unsupervised} and Domain Augmentation approaches~\cite{Cutmix,Copy-Paste, Mix3d, Polarmix}, showing the practicality of the proposed approach. In addition, we show a semantic segmentation model trained with \OursAcronym{} that works faithfully on the various cylindrical LiDAR configurations.


Our contributions can be summarized as follows:
\begin{itemize}
    \renewcommand{\labelitemi}{$\bullet$}
    \item We present an \textit{instant LiDAR domain augmentation} method, called \OursAcronym{}, for LiDAR semantic segmentation task. Our on-demand augmentation module runs at 330 FPS. 
    \item Our method can augment arbitrary cylindrical LiDAR configurations, mounting pose, and entangled motions of LiDAR spin and moving platform just from the input data. We empirically validate that such flexible modules are helpful in learning sensor-agnostic LiDAR frameworks.
    \item Experiments show that LiDAR semantic segmentation networks trained with the proposed \OursAcronym{} outperform the state-of-the-art Unsupervised Domain Adaptation, Domain Mapping, and LiDAR Data Augmentation approaches.     
\end{itemize}
