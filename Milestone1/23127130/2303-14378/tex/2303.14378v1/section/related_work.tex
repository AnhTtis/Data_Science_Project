\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[width=1\linewidth]{figure/Method/overview_v19.pdf}
    \end{center}
    \vspace{-12pt}
    
    \caption{
        \textbf{Overview of the proposed \OursAcronym{} pipeline.} 
        \ding{192} A dense 3D world model is constructed from raw LiDAR frames with the consideration of unlabeled frames and dynamic objects (Sec.~\ref{sec:construct_a_world_model}). \ding{193} Range maps of arbitrary LiDAR configuration are rendered by projecting the world model after applying random pose augmentation (Sec.~\ref{subsec:sensor-agnostic LiDAR projection}). \ding{194} The distortion induced by LiDAR spin and ego-motion is applied (Sec.~\ref{subsec:Apply Situation-aware Transformation}). \ding{195} Range maps are mixed using random azimuth ranges and back-projected to make an output 3D point cloud (Sec.~\ref{sec:sensor_level_mix}). Note that the on-demand augmentation module that comprises \ding{193}, \ding{194}, and \ding{195} runs at 330FPS.
    }        
    \label{fig:overview}        
\end{figure*}



\section{Related Work}
\label{sec:related work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2.1 LiDAR Domain Adaptation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LiDAR Domain Adaptation and Mapping}
\textbf{Domain Adaptation.}
A representative direction to alleviate the sensor-to-sensor domain shift issue is to adopt domain adaptation approaches~\cite{triess2021survey}. Cross-modal learning~\cite{jaritz2020xmuda} is exploited to enable controlled information exchange between image predictions and 3D scans. Adversarial domain adaptation methods are introduced for output space~\cite{OutDA} or feature space alignment~\cite{FeaDA} by employing sliced Wasserstein discrepancy~\cite{SWD} or boundary penalty~\cite{jiang2021lidarnet}. 3DGCA\cite{3DGCA} aligns the statistics between batches from source and target data with geodesic distance. A sparse voxel completion network~\cite{complete_and_label_cvpr21} is proposed to learn a mapping from the source domain to a canonical domain that contains complete and high-resolution point clouds. LiDAR semantic segmentation is performed on the canonical domain, and the result is projected to the target domain. ConDA\cite{ConDA} and CoSMix\cite{CoSmix} also construct an intermediate domain by mixing or concatenating the source and the target domains using pseudo-labeled target data to mitigate the domain shift issue. GIPSO\cite{Saltori2022GIPSOGI}, a recent online adaptation method, requires an optimization process on target domain data using geometric propagation and temporal regularization with pseudo labels inferred from a source domain model. A common limitation of the above methods is to require additional optimization with access to target domain data, which hinders their practicality. On the other hand, our method only adds slight augmentation overhead in the training phase and circumvents the need for target domain data.
\vspace{2mm}

\noindent\textbf{Domain Mapping.}
Our most relevant approach is domain mapping that directly transforms the source domain data to the target-like LiDAR scan~\cite{bevsic2021unsupervised,domain_transfer_iros20} and uses the transformed data for the training. However, the approach by Bešić~\etal~\cite{bevsic2021unsupervised} requires access to target domain data, and the method proposed by Langer~\etal~\cite{domain_transfer_iros20} is computationally heavy due to mesh operations that recover surfaces and check occlusions. Instead, our method can produce various LiDAR scans considering multiple LiDAR configurations in 330 FPS. Our experiment shows the efficacy of our LiDAR scans on the LiDAR semantic segmentation task.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2.2 LiDAR Data Augmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LiDAR Data Augmentation}

Approaches for LiDAR data augmentation have been explored in various ways. Inspired by seminal work in image augmentation~\cite{Cutmix}, augmentation methods for the LiDAR object detection task~\cite{Augmented_RA_L_2020, Cheng2020Improving3O,Choi2021PartAwareDA,LiDARAug_CVPR2021,Pattern-aware, Vfield} are proposed. However, these works are crafted for the detection task and assume bounding box labels are provided. For the 3D semantic segmentation task, CutMix~\cite{Cutmix} and Copy-Paste~\cite{Copy-Paste} extend the successful ideas applied for 2D image augmentation. Mix3D~\cite{Mix3d} aggregates the two 3D scenes to make objects implicitly placed into a novel out-of-context environment, which encourages the model to focus more on the local structure. Recently, PolarMix~\cite{Polarmix} introduces the scene- and object-level mix in cylindrical coordinates. PolarMix shows an impressive performance gain in domain adaptation tasks but is limited to demonstrating its synthetic-to-real adaptation capability. To the best of our knowledge, our approach is the first work on comprehensive LiDAR data augmentation to address the sensor-bias issue, and it shows superior performance compared with existing 3D data augmentation approaches. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2.3 LiDAR Semantic Segmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LiDAR Semantic Segmentation}

Existing approaches for 3D semantic segmentation can be categorized into three groups: 2D projection-based, point-based, and voxel-based methods. The 2D projection-based approaches~\cite{kochanov2020kprnet,milioto2019rangenet++,xu2020squeezesegv3} project 3D point clouds to 2D space and apply a neural network architecture crafted for image perception. Point-based methods directly work on unstructured and scattered point cloud data. Approaches in this category utilize point-wise multi-layered perceptron~\cite{Pointnet,Pointnet++}, point convolution~\cite{PointCNN,liu2019pvcnn,KPConv}, or lattice convolution~\cite{Splatnet}. Voxel-based methods handle voxelized 3D points. Early work~\cite{Volumetric_CVPR16,3dshapenet_CVPR15} adopts dense 3D convolutions, but recent approach~\cite{Minkowski} regards voxel as a sparse tensor and presents an efficient semantic segmentation framework.

Among these approaches, we select KPConv~\cite{KPConv} (point-based) and MinkowskiNet~\cite{Minkowski} (voxel-based) for the semantic segmentation experiments due to their efficiency and fidelity in the field. We apply the proposed \OursAcronym{} to the selected networks to see the improvements.
