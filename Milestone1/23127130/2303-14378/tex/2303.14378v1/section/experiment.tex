\section{Experiment}
\label{sec:experiment}

We conduct a series of experiments to test the generalization ability over the sensor-bias issue in the domain adaptation setting, in which a different LiDAR sensor is used at test time. We compare our method with domain adaptation and data augmentation approaches (Sec.~\ref{sec:exp:domain_adaptation}). Next, we demonstrate the effectiveness of our method in training a sensor-unbiased model (Sec.~\ref{sec:exp:towards_unbiased_model}). Last, we perform an ablation study on each technical contribution (Sec.~\ref{sec:exp:ablation}).


\subsection{Implementation details}
\label{sec:implementation}

In our experiment, as described in Sec.~\ref{subsec:sensor-agnostic LiDAR projection}, the yaw angle in the rotation matrix is randomly sampled from a uniform distribution, $\theta_{yaw}\sim U(-\frac{\pi}{6}, \frac{\pi}{6})$, and each element in the translation vector is drawn from another uniform distribution, $\textbf{t}_{aug}=[x, y, z]^T$ where $x\sim U(-1, 1)$m, $y\sim U(-0.5, 0.5)$m, and $z\sim U(-0.1, 0.1)$m. In addition, we set random LiDAR configuration parameters that are described in Sec.~\ref{subsec:Apply Situation-aware Transformation} as follows: $H \in \{1024, 2048\}$px., $W\in[16, 128]$px., $f_{up}\in[0,\pi/12)$, $f_{down}\in[-\pi/6,0)$. Note that we sample $W$, $f_{up}$, and $f_{down}$ from certain ranges to render arbitrary configurations of LiDARs. The forward movement velocity $V$ is sampled from $U(0, 60)$~km/h, and the rotation angular velocity of the vehicle $\omega$ is sampled from $U(-\frac{\pi}{8}, \frac{\pi}{8})$. We mix two augmented LiDAR frames in our experiment. 

We implement every module with GPU primitives for speed gain. As a result, our method can be seamlessly plugged into the data loader in the training pipeline due to its efficiency. For example, our method integrated into the data loader of MinkNet42 network training adds just 3ms to render a new LiDAR frame.\footnote{We use a workstation equipped with AMD EPYC 7452 CPU and Nvidia GeForce RTX 3090 GPU.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.1 Experimental Setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}


\noindent\textbf{SemanticKITTI}~\cite{semantic_kitti} is a large-scale dataset for LiDAR semantic segmentation task built upon the popular KITTI Vision Odometry Benchmark~\cite{kitti}. It consists of 22 sequences with 19 annotated classes. The dataset was collected by a Velodyne HDL-64E that has 64 vertical beams for 26.9$\degree$ of vertical field of view (+2.0$\degree$ to -24.9$\degree$) corresponding to $64\times2048$ range map. Following the standard protocol, we use sequences 00 to 10 (19k frames) for training except sequence 08 (4k frames), reserved for a validation set. Since SemanticKITTI does not provide the 3D bounding boxes, we treat the dynamic objects as a part of the static scene when constructing the world models described in Sec.~\ref{sec:construct_a_world_model}. 
\vspace{2mm}


\noindent\textbf{nuScenes-lidarseg}~\cite{nuscenes_lidarseg} is another large dataset providing 1,000 driving scenes (850 for training and validation, 150 for testing), including per-point annotation for 16 categories. However, as only the keyframes sampled at 2Hz are annotated, the label propagation scheme described in Sec.~\ref{sec:construct_a_world_model} is applied. This dataset was captured with a Velodyne HDL-32E, providing 32 vertical beams for 41.33$\degree$ of vertical field of view (+10.67$\degree$ to -30.67$\degree$), resulting in $32\times2048$ range map. We consider each motion of dynamic objects in constructing world models using the given 3D bounding box trajectory information.
\vspace{2mm}


\noindent\textbf{Label Mapping.} 
Since the annotated classes in Semantic KITTI\cite{semantic_kitti} and nuScene\cite{nuscenes_lidarseg} differ, we evaluate only ten overlapping categories in our experiments: \{Car, Bicycle, Motorcycle, Truck, Other vehicles, Pedestrian, Drivable surface, Sidewalk, Terrain, and Vegetation\} as suggested by \cite{complete_and_label_cvpr21}. We use mean Intersection-over-Union(mIoU) as our evaluation metric.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.2 Comparison with Domain Adaptation Approaches
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
    \newcommand{\minus}[1]{{\textcolor{purple}{$\downarrow$ #1}}}
    \newcommand{\plus}[1]{{\textcolor{teal}{$\uparrow$ #1}}}
    \newcommand{\ccgreen}{\cellcolor{yellow!10}}
    
    \newcommand{\xmark}{\textcolor{teal}{\ding{55}}}%
    \newcommand{\cmark}{\textcolor{purple}{\ding{51}}}%

    \newcolumntype{L}{>{\hspace{0.7em}}l}
    \newcolumntype{R}{>{\hspace{0.6em}}r}

    \centering
    \resizebox{1\columnwidth}{!}{
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{c|L|R@{\hspace{0.5em}}lR@{\hspace{0.5em}}l}    
        \multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{4}{r}{Unit: mIoU (Rel.\%)}\\
        \toprule
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Backbone\\(\# of params)\end{tabular}}
        &\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Methods\end{tabular}}}
        &\multicolumn{4}{c}{Source $\rightarrow$ Target} \\
        \cline{3-6} 
        \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &\multicolumn{2}{c}{$K(64) \rightarrow N(32)$} & \multicolumn{2}{c}{$N(32) \rightarrow K(64)$} \\         
        \cline{1-6}
        \multicolumn{1}{c|}{\multirow{7}{*}{\shortstack{Complete\\ \&Label~\cite{complete_and_label_cvpr21}\\(8.39M)}}} 
        & Baseline & \multicolumn{2}{c}{27.9} & \multicolumn{2}{c}{23.5} \\
        \cline{2-6}
        & FeaDA~\cite{FeaDA}  & 27.2 & (\minus{2.5}) & 21.4 & (\minus{8.9}) \\
        & OutDA~\cite{OutDA}  & 26.5 & (\minus{5.0}) & 22.7 & (\minus{3.4}) \\
        & SWD~\cite{SWD}      & 27.7 & (\minus{0.7}) & 24.5 & (\minus{4.3}) \\
        & 3DGCA~\cite{3DGCA}  & 27.4 & (\minus{1.8}) & 23.9 & (\plus{1.7}) \\
        &\Centerstack{C\&L~\cite{complete_and_label_cvpr21}} & 31.6 & (\textbf{\plus{13.3}}) & 33.7 & (\plus{43.4})\\        
        &\ccgreen&\ccgreen&\ccgreen&\ccgreen&\ccgreen\\
        &\multirow{-2}{*}{\ccgreen \shortstack[l]{Baseline\\+ \OursAcronym{}}} & \multirow{-2}{*}{\ccgreen \textbf{39.2}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{40.5}})}& \multirow{-2}{*}{\ccgreen \textbf{37.9}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{61.3}})} \\
        \cmidrule[0.8pt]{1-6}
        \multicolumn{6}{c}{(a) Comparison with unsupervised domain adaptation approaches} \\
        \multicolumn{6}{c}{} \\
        
        \multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{4}{r}{Unit: mIoU (Rel.\%)}\\
        \toprule
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Backbone\\(\# of params)\end{tabular}}
        &\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Methods\end{tabular}}}
        &\multicolumn{4}{c}{Source $\rightarrow$ Target} \\
        \cline{3-6}
        \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &\multicolumn{2}{c}{$K(64) \rightarrow N(32)$} & \multicolumn{2}{c}{$N(32) \rightarrow K(64)$} \\         
        \cline{1-6}
        \multicolumn{1}{c|}{\multirow{6}{*}{\Centerstack{MinkNet42~\cite{Minkowski}\\(37.8M)}}} 
        & Baseline & \multicolumn{2}{c}{37.8} & \multicolumn{2}{c}{36.1} \\
        \cline{2-6}
        & CutMix~\cite{Cutmix} & 37.1 & (\minus{1.9}) & 37.6 & (\plus{4.2}) \\
        & Copy-Paste~\cite{Copy-Paste} & 38.5 & (\plus{1.9}) & 41.1 & (\plus{13.9}) \\
        & Mix3D~\cite{Mix3d} & 43.1 & (\plus{14.0}) & 44.7 & (\plus{23.8}) \\
        & PolarMix~\cite{Polarmix} & 45.8 & (\plus{21.2}) & 39.1 & (\plus{8.3}) \\        
        &\ccgreen&\ccgreen&\ccgreen&\ccgreen&\ccgreen\\
        & \multirow{-2}{*}{\ccgreen \shortstack[l]{Baseline\\+ \OursAcronym{}}}  & \multirow{-2}{*}{\ccgreen \textbf{45.9}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{21.4}})}& \multirow{-2}{*}{\ccgreen \textbf{48.3}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{33.8}})}\\
        \cmidrule[0.8pt]{1-6}
        \multicolumn{6}{c}{(b) Comparison with data augmentation approaches}
        \end{tabular}
    }    
    
    \caption{                
        An experiment with domain adaptation settings. We train networks with SemanticKITTI (64 ch.)~\cite{kitti} and test with nuScenes (32 ch.)~\cite{nuscenes_lidarseg} (K$\rightarrow$N) and vice versa (N$\rightarrow$K). Baseline with the proposed \OursAcronym{} is more effective than state-of-the-art Domain Adaptation and Data Augmentation approaches.         
    }    
    \label{tab:Domain_adaptation}
\end{table}


\subsection{Results}
\label{sec:exp:domain_adaptation}

We compare our method with unsupervised domain adaptation and domain mapping methods. All the methods are trained on SemanticKITTI\cite{semantic_kitti} and evaluated on nuScenes\cite{nuscenes_lidarseg} (K$\rightarrow$N) or vice versa (N$\rightarrow$K). Note that our approach \emph{does not utilize the target dataset nor utilize the target LiDAR sensor information}. As stated in Sec.~\ref{subsec:sensor-agnostic LiDAR projection}, our approach is trained with randomized LiDAR configurations described in Sec.~\ref{sec:implementation} for this experiment.
\vspace{2mm}


\noindent\textbf{Unsupervised Domain Adaptation.} 
As shown in Table~\ref{tab:Domain_adaptation}-(a), our method shows consistent improvement by a large margin over the state-of-the-art methods in both adaptation settings (K$\rightarrow$N and N$\rightarrow$K). In the N$\rightarrow$K setting, for example, even though the model is trained on sparse data (32-ch) than the target domain (64-ch), our augmentation method provides more density-varied examples than what is available in the source domain, which helps improve the learning of sensor-agnostic representations.

Adversarial domain alignment methods, FeaDA~\cite{FeaDA}, OutDA~\cite{OutDA}, SWD~\cite{SWD}, and 3DGCA~\cite{3DGCA}, show similar performance with the baseline and reveal the limitation in learning sensor-unbiased representations.\footnote{As reported in~\cite{complete_and_label_cvpr21}, previous domain adaptation methods, namely FeaDA, OutDA, SWD, and 3DGCA, are ineffective in handling 3D LiDAR data. For more details, please refer to Sec. 4.2 in ~\cite{complete_and_label_cvpr21}.} Compared with C\&L~\cite{complete_and_label_cvpr21} that requires additional back-and-forth mapping to the canonical domain at the test time, our augmentation method is just applied at the training time, and it does not add additional computational burdens at the test time. 

\begin{table}[bt]

    \newcommand{\ccgreen}{\cellcolor{yellow!10}}
    \newcommand{\minus}[1]{{\textcolor{purple}{$\downarrow$ #1}}}
    \newcommand{\plus}[1]{{\textcolor{teal}{$\uparrow$ #1}}}
    
    \newcolumntype{L}{>{\hspace{0.7em}}l}
    \newcolumntype{R}{>{\hspace{1em}}r}
    
    \centering
    
    \resizebox{0.95\columnwidth}{!}{
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{L|c|R@{\hspace{0.1em}}L@{\hspace{0.3em}}}
        \multicolumn{4}{r}{Unit: mIoU (Rel.\%)}\\
        \toprule
        \multirow{2}{*}{Method} & \multirow{2}{*}{Retraining} & \multicolumn{2}{c}{Source $\rightarrow$ Target} \\
        \cline{3-4}
        & & \multicolumn{2}{c}{$K(64) \rightarrow N_{0103}(32)$} \\
        \midrule    
        CP\cite{domain_transfer_iros20} & \textcolor{teal}{No} & \multicolumn{2}{c}{28.8}\\
        MB\cite{domain_transfer_iros20} & \textcolor{teal}{No} & 30.0 & (\plus{4.2}) \\
        MB+GCA\cite{domain_transfer_iros20} & \textcolor{purple}{Required} & 32.6 & (\plus{13.2}) \\
        CP+GCA\cite{domain_transfer_iros20} & \textcolor{purple}{Required} & 35.9 & (\plus{24.7})\\    
        BonnetalPS+AdaptLPS\cite{bevsic2021unsupervised} & \textcolor{purple}{Required} & 37.5 & (\plus{30.2})\\
        EfficientLPS+AdaptLPS\cite{bevsic2021unsupervised} & \textcolor{purple}{Required} & 38.5 & (\plus{33.7})\\    
        \ccgreen MinkNet42 + \OursAcronym{} & \ccgreen \textcolor{teal}{No} & \ccgreen \textbf{52.4} & \ccgreen (\plus{81.9}) \\
        \bottomrule
        \end{tabular}
    }
        
    \caption{
        Comparison with domain mapping approaches. We follow the evaluation protocol used in \cite{domain_transfer_iros20}, i.e., trained on SemanticKITTI (K) and evaluated on a subset of the nuScene (N-0103), to ensure a fair comparison. Note that our approach does not require retraining and achieves performance improvement.        
    }
    \label{tab:domain_mapping}

\end{table}
\vspace{2mm}


\noindent\textbf{Domain Mapping.}
Domain Mapping methods~\cite{bevsic2021unsupervised,domain_transfer_iros20} try to convert the source domain data to target domain data as closely as possible, so they are required to access the target domain data. Some approaches, such as GCA~\cite{domain_transfer_iros20} and AdaptLPS~\cite{bevsic2021unsupervised}, as shown in Table~\ref{tab:domain_mapping}, even require retraining networks. On the other hand, as discussed in Sec.~\ref{sec:related work}, our method is an effective instant domain augmentation approach, which provides diverse LiDAR patterns beyond the target domain patterns during training, so it is a good alternative to the domain mapping approaches. Table~\ref{tab:domain_mapping} shows the comparison result. We follow the same evaluation protocol used in \cite{domain_transfer_iros20} for a fair comparison, and our model shows superior performance over the state-of-the-art (BPS+AdaptLPS) by a large margin (38.5 vs. 52.4 mIOU), without re-training process required by the other approaches. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.3 Comparison with Data Augmentation Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[bt]
    \setlength{\tabcolsep}{3pt}
    \newcommand{\cboxgray}[1]{{\setlength{\fboxsep}{1pt}\colorbox{gray!20}{\setlength{\fboxsep}{1pt}#1}}}
    \newcommand{\cboxblue}[1]{{\setlength{\fboxsep}{1pt}\colorbox{blue!10}{\setlength{\fboxsep}{1pt}#1}}}
    
    \newcommand{\ccblue}{\cellcolor{blue!10}}
    \newcommand{\ccgray}{\cellcolor{gray!20}}
    \newcommand{\ccgreen}{\cellcolor{yellow!20}}
    
    \newcommand{\minus}[1]{{\textcolor{purple}{$\downarrow$ #1}}}
    \newcommand{\plus}[1]{{\textcolor{teal}{$\uparrow$ #1}}}
    \newcommand{\boldminus}[1]{{\textcolor{purple}{\boldsymbol{$\downarrow$} #1}}}
    \newcommand{\boldplus}[1]{{\textcolor{teal}{\boldsymbol{$\uparrow$} #1}}}
    \centering
    
    \resizebox{1.995\columnwidth}{!}{
        \begin{tabular}{l|c|cc|cc|cc|cc|cc|c}
            \multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{3}{r}{Unit: mIoU (Rel.\%)}\\
            \toprule
            \multirow{2}{*}{Backbone} &
            \multicolumn{1}{c|}{\multirow{2}{*}{\shortstack{Training data}}} &
            \multicolumn{10}{c|}{Testing data} &
            \multirow{2}{*}{\shortstack{Avg. rank}} \\
            \cline{3-12}
            & \multicolumn{1}{c|}{} 
            & \multicolumn{2}{c|}{V64} 
             & \multicolumn{2}{c}{V32} 
            & \multicolumn{2}{c}{V16} 
            & \multicolumn{2}{c}{O64} 
            & \multicolumn{2}{c|}{O128} & \\
            \midrule
            % Rank: 3, 7, 7, 7, 7 -> avg.rank: 6.2
            KPConv\cite{KPConv} & \multicolumn{1}{c|}{V64} & \multicolumn{2}{c|}{\ccgray 54.70} & 0.02& (\minus{99.95}) & 0.01& (\minus{99.98}) & 0.01& (\minus{99.98}) & 0.01& (\minus{99.98}) & 6.2 \\    
            \cmidrule[0.8pt]{1-13}
            % Rank: 1, 6, 5, 6, 6 -> avg.rank: 4.8
            \multirow{7}{*}{\shortstack{MinkNet42~\cite{Minkowski}}}& \multicolumn{1}{c|}{V64} & \multicolumn{2}{c|}{\ccgray \textbf{62.80}} & 24.32& (\minus{43.65}) & 13.77& (\minus{59.64})& 25.80& (\minus{36.76}) & 24.05& (\minus{46.14})& 4.8 \\
            \cmidrule[0.8pt]{2-13}
            % Rank: 4, 2, 4, 5, 5 -> avg.rank: 4
            & V32  & 45.59& (\minus{27.40})& \multicolumn{2}{c|}{\ccgray 43.16} & 25.35& (\minus{25.70})& 29.05& (\minus{28.80})& 29.28& (\minus{34.42})& 4.0 \\
            % Rank: 7, 4, 1, 4, 4 -> avg.rank: 4
            & V16  & 33.70& (\minus{46.33})& 29.66& (\minus{31.27})& \multicolumn{2}{c|}{\ccgray \textbf{34.12}}& 34.07& (\minus{16.50})& 31.48& (\minus{29.50})& 4.0 \\
            % Rank: 5, 3, 3, 2, 3 -> avg.rank: 3.2
            & O64  & 43.01& (\minus{31.51}) & 39.13& (\minus{9.34}) & 27.96& (\minus{18.05}) & \multicolumn{2}{c|}{\ccgray \uline{40.80}} & 43.08& (\minus{3.516}) & 3.2 \\
            % Rank: 6, 5, 6, 3, 2 -> avg.rank: 4.4
            & O128 & 42.25& (\minus{32.72}) & 27.41& (\minus{36.49}) & 10.54& (\minus{69.11}) & 37.81& (\minus{7.33}) & \multicolumn{2}{c|}{\ccgray \uline{44.65}} & 4.4 \\
            % Rank: 2, 1, 2, 1, 1 -> avg.rank: 1.4
            & \ccgreen \OursAcronym{} (Rand) & \ccgreen \uline{61.51}& \ccgreen (\minus{2.05}) & \ccgreen \textbf{44.73}& \ccgreen (\textbf{\plus{3.64}}) & \ccgreen \uline{33.38} & \ccgreen (\minus{2.17}) & \ccgreen \textbf{46.54} & \ccgreen (\textbf{\plus{14.07}}) & \ccgreen \textbf{48.34} & \ccgreen (\textbf{\plus{8.26}}) & \ccgreen \textbf{1.4} \\
            \bottomrule
        \end{tabular}
    }

    \caption{
        Experiment on the sensor-bias issue of LiDAR semantic segmentation models. We tested KPConv~\cite{KPConv} and MinkowskiNet~\cite{Minkowski} models on SemanticKITTI~\cite{semantic_kitti} by generating multiple LiDAR configurations using the proposed \OursAcronym{}. `\OursAcronym{} (Rand)' denotes that the model is trained with our final method, i.e., with pose-augmentation, randomized LiDAR config., random distortion, and scene-level \& sensor-level mixing. For each test configuration, \textbf{the best} and \uline{the second-best} performances are highlighted, the reference cases (the LiDAR scans come from the same LiDAR configurations are used at the training and the testing time) are \cboxgray{colored in gray}.         
    }    
    
    \label{tab:simulated_data_experiment}    
\end{table*}
\vspace{2mm}


\noindent\textbf{Data Augmentation.}
Although the 3D augmentation methods~\cite{Cutmix, Copy-Paste, Mix3d, Polarmix} have shown their effectiveness in learning a good representation on a \emph{single domain}, it is rarely studied to show the effectiveness of domain adaptation settings particularly caused by sensor discrepancy. We experiment to see whether the existing LiDAR augmentation methods and our approach are good at domain adaptation. 

As shown in Table~\ref{tab:Domain_adaptation}-(b), interestingly, the 3D augmentation methods~\cite{Cutmix, Copy-Paste, Mix3d, Polarmix} are helpful in domain adaptation settings, even though they are not designed for adapting to an unseen domain. In particular, Mix3D~\cite{Mix3d} shows impressive improvements (37.8 $\rightarrow$ 43.1 and 36.1 $\rightarrow$ 44.7) by simply aggregating two 3D scenes. However, we speculate that the point cloud aggregation by Mix3D can induce unusual local structures (e.g., two cars overlapped perpendicularly), which may result in a suboptimal model.

Furthermore, PolarMix~\cite{Polarmix}, works well in the K$\rightarrow$N setting (37.8 $\rightarrow$ 45.8). Our conjecture for the success of PolarMix in the K$\rightarrow$N setting is that it can provide patterns of sparse 3D points similar to those found in nuScenes (32-ch) when faraway portions of a KITTI frame (64-ch) are selected for mixing. However, if the source domain does not provide enough diversity as in the opposite N$\rightarrow$K scenario, PolarMix shows reduced improvement (36.1 $\rightarrow$ 39.1). This result shows that learning a rich sensor-agnostic representation is challenging. Our method aims to reduce the domain gap induced by sensor discrepancy by explicitly rendering various LiDAR patterns. As a result, our method achieves superior performances in both K$\rightarrow$N and N$\rightarrow$K settings by a large margin (37.8 $\rightarrow$ 45.9 and 36.1 $\rightarrow$ 48.3).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.4 Towards Sensor-unbiased Semantic Segmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Towards Sensor-agnostic Model}
\label{sec:exp:towards_unbiased_model}

Our method encourages models to learn a sensor-agnostic representation, and no data from the target domain is required during training. In this experiment, we discuss the effectiveness of our approach in training a model unbiased to any LiDAR configuration. This experiment is challenging to conduct because there is no real-world dataset captured by different kinds of LiDARs at once\footnote{Carballo~\etal~\cite{carballo2020libre} proposed a multi-lidar dataset, but the dataset is not released to the public domain.}. Therefore, it is not straightforward to configure a dataset of the same scene captured with different LiDARs. 

To proceed with this experiment, we use the proposed~\OursAcronym{} to create LiDAR frames of various LiDAR configurations from the SemanticKITTI dataset. These frames of specific LiDAR configurations were then mix-and-matched for training and testing datasets. Specifically, we create the frames of 16-, 32-, and 64-ch Velodyne LiDARs~\cite{velodyne2007} (denoted by V16, V32, and V64) and the frames of 64- and 128-ch Ouster LiDARs~\cite{ouster2018} (denoted by O64 and O128) based on the manufacturer-provided LiDAR specification\footnote{More detailed configurations are described in the supplement.}. We tested KPConv~\cite{KPConv} and MinkowskiNet~\cite{Minkowski}, which are representative methods in point- and voxel-based approaches, respectively.

In Table~\ref{tab:simulated_data_experiment}, we present evaluation results on various LiDAR patterns (columns) of a model trained on a specific LiDAR pattern (row). As shown in rows 1-2, the models show severe performance drops if the LiDAR got changed at test time. For instance, a MinkNet42~\cite{Minkowski} model trained on V64 LiDAR pattern in the second row achieves 62.80 mIoU if tested on the same LiDAR. However, the model shows significant performance drops if evaluated on different LiDAR patterns (24.32 mIoU on V32, 13.77 mIoU on V16, etc.). Especially, KPConv~\cite{KPConv} fails under this sensor-discrepancy scenario while MinkowskiNet~\cite{Minkowski} model is less affected. We speculate that the U-Net style of architectural design makes it resilient to variations of the geometric patterns of 3D points. Therefore, we choose MinkowskiNet~\cite{Minkowski} as the backbone model for the rest of the experiment.

We also trained MinkowskiNet~\cite{Minkowski} models on the other LiDAR configurations, such as V32, V16, O64, and O128, shown in rows 3-6 of Table~\ref{tab:simulated_data_experiment}. As expected in the sensor-discrepancy evaluation scenarios, the best performances are achieved when the same LiDAR is applied at test time (colored in gray). Otherwise, the performances fluctuate a lot. This result indicates that a data-driven model tends to be biased towards a specific LiDAR configuration of the training data, which could be a hurdle in deploying them to real-world applications.

As a remedy for the sensor-bias issue, we propose to train models with the proposed \OursAcronym{} using \emph{randomized LiDAR configurations}, shown in row 7 of Table~\ref{tab:simulated_data_experiment}. Our model gets to learn sensor-agnostic representations since \OursAcronym{} provides various LiDAR patterns with realistic distortions. Given the \emph{no free lunch theorem}~\cite{shalev2014understanding}, \OursAcronym{} (Rand) does not beat all the test settings, especially when the LiDAR configuration used for the training and testing is the same. Our model, however, achieves the highest generalization ability across the diverse LiDAR configurations we tested, measured by average rank in the last column of Table~\ref{tab:simulated_data_experiment}. This result shows that the proposed method helps alleviate the sensor bias.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4.5 Ablation Studies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Study}
\label{sec:exp:ablation}

\begin{table}[t]
    \setlength{\tabcolsep}{3pt}
    \newcommand{\ccgreen}{\cellcolor{yellow!10}}    
    \newcommand{\minus}[1]{{\textcolor{purple}{$\downarrow$ #1}}}
    \newcommand{\plus}[1]{{\textcolor{teal}{$\uparrow$ #1}}}
    \centering
    
    \resizebox{1.0\columnwidth}{!}{
        \begin{tabular}{cccc|c}
        &&&\multicolumn{2}{r}{Unit: mIoU (Rel.\%)}\\
        \toprule
        Training data & Pose-Aug & Distortion & S\&S Mix & \multirow{2}{*}{$K(64) \rightarrow N(32)$} \\
        (Sec~\ref{subsec:sensor-agnostic LiDAR projection})
        & (Sec~\ref{subsec:sensor-agnostic LiDAR projection}) 
        & (Sec~\ref{subsec:Apply Situation-aware Transformation})
        & (Sec~\ref{sec:sensor_level_mix}) & \\
        \midrule
        K(64) &&&& 36.57$\pm$0.56 \\    
        \midrule
        K(32) &&&& 37.05$\pm$1.15 (\plus{1.31}) \\
        Random &&&& 40.05$\pm$1.03 (\plus{9.51}) \\ 
        Random &\checkmark &&& 42.70$\pm$0.91 (\plus{16.8}) \\
        Random &\checkmark &\checkmark && 43.04$\pm$0.56 (\plus{17.7}) \\    
        \ccgreen Random &\ccgreen \checkmark &\ccgreen \checkmark &\ccgreen \checkmark & \ccgreen \textbf{44.98$\pm$1.42} (\plus{\textbf{23.0}}) \\
        \bottomrule
        \end{tabular}
    }
    
    \vspace{-2mm}
    \caption{
        Ablation study on the proposed augmentation modules. The results are averaged over three runs.
    }    
    \label{tab:ablation}
    \vspace{-2mm}
\end{table}


\begin{table}[t]
    \newcommand{\minus}[1]{{\textcolor{purple}{$\downarrow$ #1}}}
    \newcommand{\plus}[1]{{\textcolor{teal}{$\uparrow$ #1}}}
    \newcommand{\ccgreen}{\cellcolor{yellow!10}}
    \newcommand{\xmark}{\textcolor{teal}{\ding{55}}}%
    \newcommand{\cmark}{\textcolor{purple}{\ding{51}}}%

    \newcolumntype{L}{>{\hspace{0.7em}}l}
    \newcolumntype{R}{>{\hspace{0.6em}}r}

    \vspace{-2mm}
    \centering
    \resizebox{1.0\columnwidth}{!}{
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{c|L|R@{\hspace{0.5em}}lR@{\hspace{0.5em}}l}    
            \multicolumn{1}{c}{}&\multicolumn{1}{c}{}&\multicolumn{4}{r}{Unit: mIoU (Rel.\%)}\\
            \toprule
            \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Backbone\\(\# of params)\end{tabular}}}
            &\multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Methods\end{tabular}}}
            &\multicolumn{4}{c}{Source $\rightarrow$ Target} \\
            \cline{3-6} 
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} &\multicolumn{2}{c}{$K(64) \rightarrow N(32)$} & \multicolumn{2}{c}{$N(32) \rightarrow K(64)$} \\                 
            \cmidrule[0.8pt]{1-6}
            \multicolumn{1}{c|}{\multirow{3}{*}{\Centerstack{SPVCNN~\cite{SPVNAS_ECCV20}\\(37.9M)}}} 
            & Baseline & \multicolumn{2}{c}{43.4} & \multicolumn{2}{c}{41.9} \\
            &\ccgreen&\ccgreen&\ccgreen&\ccgreen&\ccgreen\\
            &\multirow{-2}{*}{\ccgreen\shortstack[l]{Baseline\\+ \OursAcronym{}}} & \multirow{-2}{*}{\ccgreen \textbf{51.7}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{19.1}})}& \multirow{-2}{*}{\ccgreen \textbf{51.2}} & \multirow{-2}{*}{\ccgreen(\textbf{\plus{22.2}})}\\
            \bottomrule
        \end{tabular}
    }

    \vspace{-2mm}
    \caption{                
        Training with SemanticKITTI~\cite{kitti} and testing with nuScenes-lidarseg~\cite{nuscenes_lidarseg} (K$\rightarrow$N). The vice versa is denoted as (N$\rightarrow$K). We utilize state-of-the-art NAS-based 3D neural network architecture (SPVCNN~\cite{SPVNAS_ECCV20}) for this experiment.
    }    
    \label{fig:spvcnn_experiment}    
\end{table}

We perform an ablation study on the impact of each proposed contribution. In this experiment, we train MinkNet42 models~\cite{Minkowski} on SemanticKITTI (64 ch.)~\cite{semantic_kitti} and test them on nuScenes (32 ch.)~\cite{nuscenes_lidarseg}, i.e., $K(64)\rightarrow N(32)$ scenario.
\vspace{2mm}


\noindent\textbf{Training with randomized LiDARs.}
We compare models trained on three types of LiDAR patterns: (1) the original SemanticKITTI dataset, denoted as K(64). (2) 32 ch. LiDAR, created by \OursAcronym{} using SemanticKITTI dataset and target nuScene LiDAR specification, denoted as K(32). (3) LiDAR scans created by \OursAcronym{} using randomized configurations, denoted as Random. As shown in rows 1-3 of Table~\ref{tab:ablation}, K(32) shows improvement over K(64) because K(32) resembles the data in the target domain, denoted as N(32). Random data provides examples with abundant patterns to help learn a better representation, achieving additional performance gain (36.57 $\rightarrow$ 40.05).
\vspace{2mm}


\noindent\textbf{Pose augmentation} is another effective source of providing a diversity of the scan patterns. 
As shown in Table~\ref{tab:ablation}, adding pose augmentation to the Random LiDAR inputs leads to extra improvement (40.05 $\rightarrow$ 42.70).
\vspace{2mm}


\noindent\textbf{Distortion induced by entangled motion.}
In the real world, LiDAR data have distortions due to the entangled motion of the vehicle and LiDAR. Our distortion module implements various degrees of distortion from randomly selected forward and angular velocities of the vehicle. As this module enhances the realism of LiDAR frames, we achieve another enhancement (42.70 $\rightarrow$ 43.04), shown in row 5 of Table~\ref{tab:ablation}.
\vspace{2mm}


\noindent\textbf{Scene-level \& Sensor-level mix} module provides extra diversity to a single LiDAR frame by swapping scenes captured with different LiDAR configurations. Eventually, the final model shown in row 6 of Table~\ref{tab:ablation} learns a better representation from the rich LiDAR frames (43.04 $\rightarrow$ 44.98).
\vspace{2mm}


\noindent\textbf{NAS-based backbone.} We additionally validate that our approach can be applied to an advanced 3D neural network, SPVCNN~\cite{SPVNAS_ECCV20} that was found by extensive neural architecture search (NAS). We use the same experimental setting used in Sec.~\ref{sec:exp:domain_adaptation}, and we utilize the pre-trained network provided by the authors. After fine-tuning the network with the proposed~\OursAcronym{}, we observe the prediction accuracy on the unseen target domain significantly enhanced in both $K(64)\rightarrow N(32)$ and $N(32)\rightarrow K(64)$ scenarios.
