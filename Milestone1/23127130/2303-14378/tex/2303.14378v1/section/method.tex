\section{Fast LiDAR Data Augmentation} 
\label{sec:method}

We introduce a new augmentation method, called \textbf{\OursAcronym{}}, that instantly creates a new LiDAR frame considering LiDAR mounting positions, various LiDAR configurations, and distortion caused by LiDAR spin and ego-motion. In this work, we craft our augmentation approach for cylindrical LiDARs. As shown in Fig.~\ref{fig:overview}, our method consists of four steps: \ding{192} Constructing a world model from LiDAR frames, \ding{193} Creating a range map of arbitrary LiDAR configurations and poses, \ding{194} Applying motion distortion to the augmented frames caused by ego-motion, and \ding{195} Scene-level \& sensor-level mix. The proposed method is flexible enough to produce a combined LiDAR frame having multiple LiDAR configurations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.1 World Model Construction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constructing a 3D World Model}
\label{sec:construct_a_world_model}

A LiDAR frame is partial geometric capture of a 3D world. Therefore, we can aggregate multiple LiDAR frames of similar regions to build a rough 3D world model. In this step, we separately care for static scenes and dynamic objects by utilizing semantic label annotations on 3D points and trajectories of moving objects in the scene. Such information is available in standard LiDAR datasets~\cite{nuscenes_lidarseg}.
\vspace{2mm}


\noindent\textbf{Static scene.}
 We construct a static world model by aggregating multiple LiDAR frames using ego-motion. Specifically, a set of motion-compensated LiDAR frames $\mathcal{P}_t ^{world}$ is built as a world model at time $t$ by aggregating $N$ adjacent LiDAR frames. We determine the adjacent LiDAR frames using geometric adjacency (based on the LiDAR center coordinates) rather than temporal adjacency (based on frame indices) to cover the 3D scene better. This scheme helps to build a denser world map when the ego vehicle revisits the same place, formulated as follows:
\begin{equation}
        \mathcal{P}_t^{world} =\bigcup_{k\in {K}_{t}}{T_{t}\circ T_{t+k}^{-1}(\mathcal{P}_{t+k})},        
    \label{equ:world_model_from_static_scene}    
\end{equation}
where the geometrically adjacent set of frames ${K}_{t} = \argmin_{K} \sum_{k\in K}{||\mathbf{R}_{t+k}^\top \mathbf{t}_{t+k}- \mathbf{R}_{t}^\top \mathbf{t}_{t}||_2}$ $\text{s.t.}~|K| = N$, $T_t(\mathbf{x})=\mathbf{R}_t\mathbf{x}+\mathbf{t}_t$ is the ego-motion from the world origin at time $t$, and $\mathcal{P}_t$ is 3D points captured at time $t$.
\vspace{2mm}


\noindent\textbf{Dynamic objects.} 
When we aggregate 3D points on dynamic objects in the world model, we should avoid unintended flying points occurring by object-wise motion. To alleviate the issue, we leverage temporally consecutive LiDAR frames, not the geometrically adjacent frames, and we consider trajectory information of the dynamic objects over time. In short, the sparse observations of dynamic objects across multiple frames are aggregated by applying inverse motions of each dynamic object and ego-motions.
\vspace{2mm}


\noindent\textbf{Label consistency and label propagation.}
After the world model construction, we examine the labeling consistency for all the aggregated 3D points in $\mathcal{P}_t^{world}$. This verification step is a safeguard to remove noisy points from various sources of errors, such as incorrect annotation and inaccurate ego-motion. To make consistent labels, we examine a set of 3D points assigned to a single voxel in the voxel grid (10cm). The majority voting determines a representative semantic label for each voxel, and we can get clean labels. Note that the majority label in a voxel can be propagated to the unlabeled points in the same voxel. This step helps assign pseudo labels to sparsely annotated datasets like nuScenes~\cite{nuscenes_lidarseg} that only provides dense annotations for keyframes selected at 2Hz.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.2 Random LiDAR Projection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Creating a Range Map}
\label{subsec:sensor-agnostic LiDAR projection}

\noindent\textbf{Pose augmentation.}
Once we have a world model, the LiDAR pose is augmented by applying a rigid transformation $T_{aug}(\mathbf{x})=\textbf{R}_{aug}\mathbf{x}+\textbf{t}_{aug}$ to give variations of the LiDAR frames. In our experiments, random rotation along the z-axis, i.e., $\textbf{R}_{aug}=\textbf{R}_z(\theta_{yaw})$, and random translation are considered\footnote{While our method is capable of incorporating arbitrary rotations, it is worth noting that most public datasets utilize \emph{upright LiDARs}. Therefore, applying full rotations may result in an unintended severe domain gap.}. The yaw angle $\theta_{yaw}$ and the translation vector $\textbf{t}_{aug}$ are drawn from uniform distributions.
\vspace{2mm}


\noindent\textbf{Randomized LiDAR configurations.}
A LiDAR frame can be expressed as a range map, and the configuration of LiDAR is defined by the vertical field of view ($f_{up}$, $f_{down}$) and the resolution of the range map ($H$, $W$). In the case of cylindrical LiDARs, the projection $\Pi (\mathbf{x}) \rightarrow [u, v, r]^{\top}$ of the 3D points is calculated\footnote{We set the x-axis as the vehicle's forward direction, the y-axis as the left from the vehicle, and the z-axis as the top direction from the ground.} as follows~\cite{behley2018rss,domain_transfer_iros20}:

\begin{equation}
    \Pi(\begin{bmatrix} x \\ y \\ z \end{bmatrix}) 
    =
    \begin{bmatrix} \frac{1}{2}[1-(\arctan{\frac{y}{x}})/\pi]W \\ [1-(\arcsin{\frac{z}{r}}-f_{down})/f]H \\ ||\mathbf{x}||_2 \end{bmatrix}
    =
    \begin{bmatrix} u \\ v \\ r \end{bmatrix},
    \label{equ:projection}
\end{equation}
where $f=|f_{up}| + |f_{down}|$. With $\Pi(\cdot)$, we can project the world model $\mathbf{x}\in T_{aug}(\mathcal{P}_t^{world})$ using a given LiDAR configuration. Here, we \emph{randomize LiDAR configuration $(H,W,f_{up},f_{down})$} to augment LiDAR frames further. With this procedure, a range map of random LiDAR configuration can be rendered, and LiDAR patterns not observed in the training data can be provided. This step is shown to be very effective in our experiment.

The world models are constructed by aggregating LiDAR frames of different viewpoints, which can result in occlusion from a desired viewpoint. To filter out these points, we employ z-buffer-based raycasting~\cite{Z-buffer} that selects the nearest 3D points to the desired viewpoint. Therefore, we formulate the step for range image rendering as follows:
\begin{equation}
    Prj(\mathcal{P}_{t}) = \mathcal{Z}(\Pi(T_{aug}(\mathcal{P}_{t}^{world}))),
\end{equation}
where the $\mathcal{Z}$ means the z-buffer-based ray-casting. 


\begin{figure}[t]
    \begin{center}
        \includegraphics[width=1\linewidth]{figure/Method/velocity_distortion_with_legend_2pi_v3.pdf}
    \end{center}
    \vspace{-10pt}
    
    \caption{
        Implementation of LiDAR distortion induced by entangled motion. Given movement/rotation velocities, distortion maps are generated and used to implement the motion distortion.         
    }
    \label{fig:distortion_and_occlusion}    
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.3 Distortion induced by entangled motion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Adding Motion Distortion}
\label{subsec:Apply Situation-aware Transformation}

Cylindrical LiDARs have a spinning motion with a fixed rate for omnidirectional capture. Also, LiDARs are often mounted on a moving platform, such as a vehicle. The two entangled motions, i.e., movement of the vehicle and spinning motion of LiDAR, result in distortion on framed data. We observe such distortions from the real LiDAR frames (See the supplement). 

More specifically, the rotation of the platform affects the effective LiDAR angular velocity, resulting in a gap or overlap between starting and ending points of a single LiDAR frame shown in the middle of Fig.~\ref{fig:distortion_and_occlusion}. If the platform has a forward movement, as depicted at the bottom of Fig.~\ref{fig:distortion_and_occlusion}, the starting and ending points are not aligned because each 3D point has a different travel distance. Although this distortion could significantly change the coordinates of 3D points in a LiDAR frame, this phenomenon is rarely addressed in the literature.

We formulate the distortion with LiDAR spin angular velocity $\omega_0$, platform rotation angular velocity $\omega$, and platform forward movement velocity $V$ under constant velocity assumption. 
\begin{align}
    M(u', v) = M(\frac{(\omega_0+\omega)}{\omega_0}\cdot u, v),
    \label{equ:distortion_rotation} \\
    M(u, v) + d(u) = M(u, v) + V\cdot\frac{u}{\omega_0},
    \label{equ:distortion_movement}
\end{align}

where $M$ is a range map projection of a LiDAR frame.
The effective angular velocity is $\omega+\omega_0$ for distortion by rotation, which results in a resampling of each 3D point in the range map along $u$-axis, as shown in Eq.~\ref{equ:distortion_rotation}. The travel distance compensation due to the forward movement is given by Eq.~\ref{equ:distortion_movement}. These equations lead us to an efficient implementation of the \emph{distortion in the range map} by applying coordinate resampling and depth adjustment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3.4 Sensor-level Mix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scene-level \& Sensor-level Mix}
\label{sec:sensor_level_mix}

PolarMix~\cite{Polarmix} shows a scene-level mix and demonstrates strong data augmentation performance. Inspired by this work, we propose an extended augmentation module that mixes frames of \emph{different scenes captured by different LiDARs}. As described in Fig.~\ref{fig:overview}, after rendering range maps with random LiDAR configurations, we mix the range maps using random azimuth angle ranges. The mixed range map is transformed to a 3D point cloud using the inverse of the projection model $\Pi^{-1}$. Generally, the diversity of training data in a single training step is proportional to the batch size. The mixing module helps data-driven approaches by providing diverse LiDAR patterns in a single batch, reducing efforts to keep a large batch size.
