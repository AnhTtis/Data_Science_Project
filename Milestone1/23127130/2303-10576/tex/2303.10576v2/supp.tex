%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}
%\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{colortbl}
\usepackage{ulem} % for delete line \sout

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% ready for submission
%\usepackage[nonatbib]{neurips_2023}
\usepackage{iclr2024_conference,times}
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

%\usepackage{natbib}
%\setcitestyle{numbers,square}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\zy}[1]{\textcolor{magenta}{[Zuoyu: #1]}}
\newcommand{\jr}[1]{\textcolor{black}{[Junru: #1]}}
\newcommand{\mh}[1]{\textcolor{red}{[Muhan: #1]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



\title{An Efficient Subgraph GNN with Provable Substructure Counting Power\\ 
-- Supplementary Material --}


\author{Zuoyu Yan \\
Wangxuan Institute of Computer Technology \\
Peking University \\
\texttt{yanzuoyu3@pku.edu.cn}
\And 
Junru Zhou \\
Institute for Artificial Intelligence\\
Peking University \\
\texttt{@126.com}
\AND 
Liangcai Gao \\
Wangxuan Institute of Computer Technology \\
Peking University \\
\texttt{glc@pku.edu.cn}
\And
Zhi Tang \\
Wangxuan Institute of Computer Technology \\
Peking University \\
\texttt{tangzhi@pku.edu.cn}
\AND
Muhan Zhang\thanks{Correspondence to Muhan Zhang, and Liangcai Gao} \\
Beijing Institute for General Artificial Intelligence\\
Peking University \\
\texttt{muhan@pku.edu.cn}
}


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\begin{document}
\maketitle

In the supplementary material, we provide:
\begin{enumerate}
%\item the related works;
\item the introduction of the Weisfeiler-Leman algorithm (WL) and Message Passing Neural Network (MPNN);
\item the proof of Theorem 3.2 in the main paper (the upper bound of $m$-WL in terms of counting substructures);
\item the proof of Theorem 3.4 in the main paper (the lower bound of subgraph GNN in terms of counting connected substructures)
\item the proof of Theorem 4.4 in the main paper (ESC-GNN's ability to differentiate non-isomorphic graphs);
\item the proof of Theorem 4.2 and Theorem 4.3 in the main paper (ESC-GNN's ability for subgraph counting and induced subgraph counting); 
\item the proof of Theorem 4.5 (ESC-GNN's ability to differentiate regular graphs);
\item additional discussion on the expressive power of ESC-GNN;
\item experimental details;
\item additional experiments on real-world benchmarks and ablation study;
\item limitations and the assets we used.
\end{enumerate} 

\section{The Weisfeiler-Leman algorithm and Message Passing Neural Network}
\textbf{$1$-WL.} We first describe the classic Weisfeiler-Leman algorithm (1-WL). For a given graph $G$, 1-WL aims to compute the coloring for each node in $V$. It computes the node coloring for each node by aggregating the color information from its neighborhood iteratively. In the 0-th iteration, the color for node $v \in V$ is $c_v^0$, denoting its initial isomorphic type\footnote{The term "isomorphic type" is based on previous work~\citep{morris2019weisfeiler}, which gives each $k$-tuple of nodes an initial feature such that two $k$-tuples receive the same initial feature if and only if their induced subgraphs (indexed by node order in the $k$-tuple) are isomorphic.}. For labeled graphs, the isomorphic type of a node is simply its node feature. For unlabeled graphs, we give the same 1 to all nodes. In the $t$-th iteration, the coloring for $v$ is computed as $$c_v^t = \text{HASH}(c_v^{t-1}, \{\!\!\{c_u^{t-1}:u \in N(v)\}\!\!\})$$,
%\begin{equation}
%\label{eq:1WL}
%c_v^t = \text{HASH}(c_v^{t-1}, \{\!\!\{c_u^{t-1}:u \in N(v)\}\!\!\})
%\end{equation}
where HASH is a bijective hashing function that maps the input to a specific color. The process ends when the colors for all nodes between two iterations are unchanged. If two graphs have different coloring histograms in the end (e.g., different numbers of nodes with the same color), then $1$-WL detects them as non-isomorphic.

\textbf{$k$-WL.} For each $k\geq 2, k \in \mathbb{N}$, the $k$-dimensional WL algorithm ($k$-WL) colors $k$-tuples instead of nodes. In the 0-th iteration, the isomorphic type of a $k$-tuple $\vec{v}$ is given by the hashing of 1) the tuple of colors associated with the nodes of the $\vec{v}$, and 2) the adjacency matrix of the subgraph induced by $\vec{v}$ ordered by the node ordering within $\vec{v}$. %its color in the 0-th iteration, $c_{\vec{v}}^0$, is the initial isomorphic type of $\vec{v}$. 
In the $t$-th iteration, its coloring is updated by: %\jr{Reviewer hXix says it should be $t$ instead of $t-1$ in the colored part of the following formula, which I think is right}
%\begin{equation}
%\label{eq:kWL1}
%c_{\vec{v}}^t = \text{HASH}(c_{\vec{v}}^{t-1}, c_{\vec{v}, (1)}^{\textcolor{black}{t}},...,c_{\vec{v}, (k)}^{\textcolor{black}{t}})
%\end{equation} 
$$c_{\vec{v}}^t = \text{HASH}(c_{\vec{v}}^{t-1}, c_{\vec{v}, (1)}^{\textcolor{black}{t}},...,c_{\vec{v}, (k)}^{\textcolor{black}{t}})$$,
where $$c_{\vec{v}, (i)}^t = \{\!\!\{c_{\vec{u}}^{t-1}|\vec{u} \in N_i(\vec{v})\}\!\!\}, i \in [k]$$. 
%\begin{equation}
%\label{eq:kWL2}
%c_{\vec{v}, (i)}^t = \{\!\!\{c_{\vec{u}}^{t-1}|\vec{u} \in N_i(\vec{v})\}\!\!\}, i \in [k]
%\end{equation}
Here, $N_i(\vec{v}) = \{(v_1,...,v_{i-1}, w, v_{i+1},...,v_k) | w \in V\}$ is the $i$-th neighborhood of $\vec{v}$. Intuitively, $N_i(\vec{v})$ is obtained by replacing the $i$-th component of $\vec{v}$ by each node from $V$. Besides the updating function, other procedures of $k$-WL are analogous to 1-WL. In terms of distinguishing graphs, 2-WL is as powerful as 1-WL, and for $k\geq 2$, $(k+1)$-WL is strictly more powerful than $k$-WL.

\textbf{MPNNs.} MPNNs are a class of GNNs that learns node representations by iteratively %encoding and 
aggregating messages from neighboring nodes. Let $h_v^t$ be the node representation for $v \in V$ in the $t$-th iteration. It is usually initialized with the node's intrinsic attributes. In the $(t+1)$-th iteration, it is updated by $$h_v^{t+1} = W_1^t(h_v^t, \sum_{u \in N(v)}W_2^t(h_u^t, h_v^t, e_{uv}))$$,
%\begin{equation}
%\label{eq:mpnn}
%h_v^{t+1} = W_1^t(h_v^t, \sum_{u \in N(v)}W_2^t(h_u^t, h_v^t, e_{uv}))
%\end{equation}
where $W_1^t$ and $W_2^t$ are two learnable functions. MPNN's power in terms of distinguishing non-isomorphic graphs is upper bounded by 2-WL~\citep{xu2018powerful}.

%\section{Related Works}
%\label{sec:rel}

%\subsection{Representation power of GNNs}
%There are two major perspectives to evaluate the representation power of GNNs: the ability to distinguish non-isomorphic graphs, and the ability to approximate specific functions. In terms of differentiating graphs, existing works~\citep{xu2018powerful, morris2019weisfeiler} showed that MPNNs are at most as powerful as the WL test~\citep{weisfeiler1968reduction}. Following works improve the expressiveness of GNNs by using high-order information~\citep{morris2019weisfeiler, morris2020weisfeiler, maron2019provably, bodnar2021weisfeiler, bodnar2021weisfeiler1, vignac2020building} or augmenting node features~\citep{bouritsas2022improving, barcelo2021graph, dwivedi2021graph, loukas2020hard, abboud2021surprising, kreuzer2021rethinking, lim2022sign}. 

%In terms of approximating specific functions, some works use GNNs to approximate graph algorithms~\citep{velivckovic2019neural, xhonneux2021transfer, yan2022neural}. We also notice that a recent trend is to evaluate the representation power through the bi-connectivity of GNNs~\citep{zhang2023rethinking}. In this paper, we focus on GNN's ability to count substructures, especially connected substructures. Some previous works~\citep{furer2017combinatorial, arvind2020weisfeiler} relate the counting power to the expressiveness of GNNs, by providing substructures that WL can count. Following works count substructures with powerful networks, such as using the local relational pooling~\citep{murphy2019relational, chen2020can} or high-order GNNs \citep{tahmasebi2020counting}. However these works either suffer from high computational cost or requires truncation methods that lack theoretical guarantees. There are several other studies in the literature that count subgraph isomorphisms~\citep{liu2020neural, liu2022graph, yu2023learning}. However, they do not provide theoretical guarantees for counting substructures in general or in specific cases. Therefore we cannot trust their performance on substructure-counting-related tasks, which are abundant in chemistry and biology.â€”%Analyzing what substructures GNNs can count has significant implications in understanding their representation power and generalization. For example, being able to count 6-cycles enables a wide range of chemical prediction tasks involving benzenes.

 %However, these works bring in much computational cost, leading to works that focus on the local counting power of GNNs. 

%\subsection{Subgraph GNNs}
%To improve the representation power of GNNs, subgraph GNNs extract the input graph into a collection of subgraphs, and use the subgraph information to enhance the representations of graph elements (e.g., nodes, edges). The subgraph selection policies vary among different works, such as graph element deletion~\citep{bevilacqua2022equivariant, cotta2021reconstruction, papp2021dropgnn}, k-hop subgraph extraction~\citep{abu2019mixhop, sandfelder2021ego, nikolentzos2020k, feng2022powerful}, node identity augmentation~\citep{you2021identity}, and rooted subgraph extraction~\citep{zhang2021nested, zhao2022stars, frascaunderstanding2022, papp2022theoretical, zhang2021labeling, huang2023boosting, qianordered2022}. Most of these works need to run message passing and aggregation over all subgraphs, therefore performing much slower than classic MPNNs. This prevents the use of subgraph GNNs in large real-world datasets. Furthermore, there are other studies that focus on extracting structural information within subgraphs and integrating it with MPNNs~\citep{zhang2018link, yan2021link}, but these works are beyond the scope of this paper.

%\subsection{Positional/Structural Encodings on Graphs}
%To leverage the spectral properties of graphs, many works~\citep{dwivedi2020benchmarking, lim2022sign, dwivedi2021graph, kreuzer2021rethinking, mialon2021graphit, park2022grpe} introduce the eigenvectors of the graph Laplacian as augmented node features. Other approaches introduce positional encodings such as random walks~\citep{li2020distance}, diffusion kernels~\citep{feldman2022weisfeiler}, shortest path distance~\citep{ying2021transformers}, and unsupervised node embedding methods~\citep{wang2022equivariant}. Our work can be viewed as a structural encoding based on subgraph GNN.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% supplementary material
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
%\supplementary material
%\onecolum


\section{The Proof of Theorem 3.2}

We begin by introducing the \textit{graph isomorphism}. For a pair of graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, if there exists a bijective mapping $f: V_1 \rightarrow V_2$, so that for any edge $(u_1, v_1) \in E_1$, it satisfies that $(f(u_1), f(v_1)) = (u_2, v_2) \in E_2$, then $G_1$ is isomorphic to $G_2$, otherwise they are not isomorphic. Up to now, there is no polynomial algorithm for solving the graph isomorphism problem. One popular method is to use the $k$-order Weisfeiler-Leman~\citep{weisfeiler1968reduction} algorithm ($k$-WL). It is known that  1-WL is as powerful as 2-WL, and for $k \geq 2$, $(k+1)$-WL is more powerful than $k$-WL. 


We then prove that $k$-WL can't count all connected substructures with $(k+1)$ nodes (specifically, $(k+1)$-cliques). We restate the result as follows: 

%\begin{theorem}
%	For any $k\geq 2$, there exists a pair of graphs $G$ and $H$, such that $G$ contains a $(k+1)$-clique as its subgraph while $H$ does not, and that $k$-WL can't distinguish $G$ from $H$.
%\end{theorem}

\begin{theorem}
\label{th:disab}
For any $k\geq 2$, there exists a pair of graphs $G$ and $H$, such that $G$ contains a $(k+1)$-clique as its subgraph while $H$ does not, and that $k$-WL can't distinguish $G$ from $H$.
\end{theorem}

\begin{proof}
	The counter-example is inspired by the well-known Cai-F\"urer-Immerman (CFI) graphs \citep{cai1992optimal}. We define a sequence of graphs $G_k^{(\ell)}, \ell = 0, 1, \ldots, k+1$ as following,
	\begin{equation}
		\begin{split}
			V_{G_k^{(\ell)}} =& \Big\{u_{a, \vec{v}}\Big| a\in[k+1], \vec{v}\in\{0,1\}^k \text{ and }\\
			&\quad\begin{array}{ll}
				\vec{v} \text{ contains an even number of } 1 \text{'s}, & \text{if }a=1,2,\ldots, k-\ell+1, \\
				\vec{v} \text{ contains an odd number of } 1 \text{'s},  & \text{if }a=k-\ell+2,\ldots, k+1.
			\end{array}\Big\}
		\end{split}
	\end{equation}
	Two nodes $u_{a,\vec{v}}$ and $u_{a',\vec{v}'}$ of $G_k^{(\ell)}$ are connected iff there exists $m\in [k]$ such that $a' \mod (k+1) = (a+m) \mod (k+1)$ and $v_m = v'_{k-m+1}$. We have the following lemma.

	\begin{lemma}
		\emph{(a)} For each $\ell = 0, 1, \ldots, k+1$, $G_k^{(\ell)}$ is an undirected graph with $(k+1)2^{k-1}$ nodes;

		\emph{(b)} The set of graphs $G_k^{(\ell)}$ with an odd $\ell$ are mutually isomorphic; similarly, the set of graphs $G_k^{(\ell)}$ with an even $\ell$ are mutually isomorphic.
	\end{lemma}

	It's easy to verify (a). To prove (b), it suffices to prove $G_k^{(\ell)}$ is isomorphic to $G_k^{(\ell+2)}$ for all $\ell = 0,1,\ldots, k-1$. We apply a \emph{renaming} to the nodes of $G_k^{(\ell)}$: we flip the $1^\mathrm{st}$ bit of $\vec{v}$ in every node named $u_{k-\ell, \vec{v}}$, and flip the $k^\mathrm{th}$ bit of $\vec{v}$ in every node named $u_{k-\ell+1, \vec{v}}$. Since this is a mere renaming of nodes, the resulting graph is isomorphic to $G_k^{(\ell)}$. However, it's also easy to see that the resulting graph follows the construction of $G_k^{(\ell+2)}$. Therefore, we assert that $G_k^{(\ell)}$ must be isomorphic to $G_k^{(\ell+2)}$.

	Now, let's ask $G=G_k^{(0)}$ and $H=G_k^{(1)}$. Obviously there is a $(k+1)$-clique in $G$: nodes $u_{j,0^k}, j=1,2, \ldots, k+1$ are mutually adjacent by definition of $G_k^{(0)}$. On the contrary, we have
	\begin{lemma}
		There's no $(k+1)$-clique in $H$.
	\end{lemma}

	The proof is given below. Assume there is a $(k+1)$-clique in $H$. Since there's no edge between nodes $u_{a, \vec{v}}$ with an identical $a$, the $(k+1)$-clique must contain exactly one node from every node set $\{u_{a,\vec{v}}\}$ for each fixed $a\in[k+1]$. We further assume that the $(k+1)$ nodes are $u_{a, b_{a1}b_{a2}\ldots b_{ak}}, a=1,2,\ldots, k+1$. Using the condition for adjacency, we have
	\begin{align}
		       & b_{2k} = b_{11},                                                               \\
		       & b_{3k} = b_{21}, b_{3(k-1)} = b_{12},                                          \\
		       & b_{4k} = b_{31}, b_{4(k-1)} = b_{22}, b_{4(k-2)} = b_{13},                     \\
		\notag & \cdots\cdots\cdots\cdots                                                       \\
		       & b_{(k+1)k} = b_{k1}, b_{(k+1)(k-1)} = b_{(k-1)2}, \ldots, b_{(k+1)1} = b_{1k}.
	\end{align}
	Applying the above identities to the summation
	\begin{align}
		\label{contradiction}
		\sum_{a=1}^{k+1}\sum_{j=1}^k b_{aj} = 2\sum_{j=1}^k\left(b_{1j}+b_{2j}+\cdots+b_{(k-j+1)j}\right),
	\end{align}
	we see that it should be even. However, by definition of $G_k^{(1)}$, there are an even number of $1$'s in $b_{a1}b_{a2}\ldots b_{ak}$ when $a\in [k]$, and an odd number of $1$'s when $a=k+1$. Therefore, the sum in \eqref{contradiction} should be odd. This leads to a contradiction.

	Finally, to prove the $k$-WL equivalence of $G$ and $H$, we have
	\begin{lemma}
		$k$-WL can't distinguish $G$ and $H$.
	\end{lemma}

	By virtue of the equivalence between $k$-WL and pebble games~\citep{grohe_otto_2015}, it suffices to prove that Player II will win the $\mathcal{C}_{k}$ bijective pebble game on $G$ and $H$. We state the winning strategy for Player II as following. Since $G$ and $H$ are isomorphic with nodes $\{u_{k+1,*}\}$ deleted, Player II can always choose an isomorphism $f:G-\{u_{k+1,*}\}\rightarrow H-\{u_{k+1,*}\}$ to survive if Player I never places a pebble on nodes $u_{k+1,*}$. Furthermore, since $k$ pebbles can occupy nodes with at most $k$ different values of $a$ (in $u_{a,\vec{v}}$), there's always a set of pebbleless nodes $\{u_{a_0,\vec{v}}\}$ with some $a_0\in[k+1]$. Therefore, Player II only needs to do proper renaming on $H$ between $u_{k+1,*}$ and $u_{a_0,*}$ as stated above. This makes every $\vec{v}$ in $u_{a_0,\vec{v}}$ have an odd number of $1$'s. Player II then chooses the isomorphism on $G-\{u_{a_0,*}\}$ and $H^{\mathrm{renamed}}-\{u_{a_0,*}\}$. This way, Player II never loses since there are not enough pebbles for Player I to make use of the oddity at the currently pebbleless set of nodes.

\end{proof}

\begin{remark}
    Notice that if we take $k=2$, then $G$ is two 3-cycles while $H$ is a 6-cycle, which 2-WL cannot differentiate; if we take $k=3$, then $G$ is the 4*4 Rook's graph while $H$ is the Shrikhande graph, which 3-WL cannot differentiate. In these special cases, the above construction complies with our well-known examples.
\end{remark}

\section{The Proof of Theorem 3.4}

We restate the theorem as follows:
%\begin{theorem}
%\label{th:sub1}
%For any connected substructure with no more than $k+m$ ($m\geq 2, k > 0$) nodes, there exists a subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL that can count it. 
%\end{theorem}

\begin{theorem}
\label{th:sub}
For any connected substructure with no more than $k+m$ ($m\geq 2, k > 0$) nodes, there exists a subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL that can count it. 
\end{theorem}


\begin{proof}
Based on Remark 3.3 in the main paper, there exists a type of connected $k$-tuple that satisfies the decomposition of the connected substructure. Then the substructure can be separated into 2 subgraphs: the nodes that belong to the $k$-tuple (we call them rooted nodes) and the nodes that do not belong to the $k$-tuple (we call them non-rooted nodes). Formally, for the given two substructures $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, we define the subgraph that formed by the rooted nodes of $G_1$ ($G_2$, resp.) as $G_{1,r} = (V_{1, r}, E_{1,r})$ ($G_{2,r} = (V_{2,r}, E_{2,r})$, resp.). Similarly, define the subgraph that formed by the non-rooted nodes of $G_1$ ($G_2$, resp.) as $G_{1,n} = (V_{1, n}, E_{1,n})$ ($G_{2,n} = (V_{2,n}, E_{2,n})$, resp.). We can also define the subgraph formed between the rooted nodes and non-rooted nodes of $G_1$ ($G_2$, resp.) as $G_{1,c} = (V_{1, c}, E_{1,c})$ ($G_{2,c} = (V_{2,c}, E_{2,c})$, resp.). Then it is easy to find that for $G_1$, $V_1 = (V_{1,r} \cup V_{1, n})$, $V_{1,c} \subseteq V_1$, $E_1 = E_{1,r} \cup E_{1, n} \cup E_{1, c}$. There is no intersection between $V_{1,r}$ and $V_{1, n}$, and there is no intersection between $E_{1,r}$, $E_{1,c}$, and $E_{1, n}$. The same holds for $G_2$.

If $G_1$ and $G_2$ are non-isomorphic, then there can be three potential situations: (1) $G_{1,r}$ and $G_{2,r}$ are non-isomorphic; (2) $G_{1,n}$ and $G_{2,n}$ are non-isomorphic; (3) $G_{1,c}$ and $G_{2,c}$ are non-isomorphic. In the following section, we will prove that in all these situations, the subgraph GNN can distinguish between $G_1$ and $G_2$.

\textbf{$G_{1,r}$ and $G_{2,r}$ are non-isomorphic.}  It denotes that for $G_1$ and $G_2$, the selected $k$-tuples are different. Then of course the subgraph GNN can differentiate $G_1$ and $G_2$.

\textbf{$G_{1,n}$ and $G_{2,n}$ are non-isomorphic.} Recall that we use a backbone GNN as powerful as $m$-WL to encode the information within the subgraph, and $V_{1,n}$ and $V_{2,n}$ contain nodes no more than $m$ nodes. Therefore if $G_{1,n}$ and $G_{2,n}$ are non-isomorphic, then they will have different isomorphic types, and thus can be distinguished by the backbone GNN.

\textbf{$G_{1,c}$ and $G_{2,c}$ are non-isomorphic.} We define the label of all nodes in $G_1$ (the same holds for $G_2$) as its distance to the nodes in the $k$-tuple. Formally, let $V_{1,r} = \{v_{1,r,1},...,v_{1,r,k}\}$, then $\forall u \in V_1$, its label $f_1(u) = (d(u, v_{1,r,1}),...,d({u,v_{1,r,k}}), I(u))$, where $d(.)$ denotes the shortest path distance between two nodes, and $I(u)$ denotes the label that reflects the isomorphic type of $u$ encoded by the subgraph GNN within $G_{1,n}$.

Since the substructures are connected, there exists at least a node $u_1 \in V_{1,n}$, whose label contains at least an index with the value ``1". For $f(u_1)$, the indices with value ``1" denotes that there exist edges between $u_1$ and the corresponding nodes in $V_{1,r}$. While the indices with value larger than 1 denote that there is no edge between $u_1$ and the corresponding nodes in $V_{1,r}$. The same holds for $u_2$ and $V_{2,r}$. Therefore, if the subgraph formed by $u_1$ and $V_{1,r}$ and the subgraph formed by $u_2$ and $V_{2,r}$ are non-isomorphic, then $f_1(u_1)$ and $f_2(u_2)$ are different, and the subgraph GNN can differentiate the two substructures. Also, if the $I(u_1)$ and $I_(u_2)$ are different, then the subgraph GNN can also distinguish $G_1$ and $G_2$. We can then consider the next nodes, and continue the process inductively.

Therefore, if for all nodes in $V_{1,n}$, we can find a unique node in $V_{2,n}$ that has the same label as it. Then $G_{1,c}$ and $G_{2,c}$ are isomorphic. Reversely, if $G_{1,c}$ and $G_{2,c}$ are non-isomorphic, then there exists at least a node in $V_{1,n}$, that we cannot find a unique node in $V_{2,n}$ that has the same label as it. Therefore the subgraph GNN can differentiate $G_1$ and $G_2$.

Then we can assign each substructure a unique color according to its isomorphic type, and use the color histogram of the graph as the output function. If two graphs have different numbers of certain substructures, then the color histogram will be different.

Based on the above results, the subgraph GNN can count the given type of connected substructure.

\end{proof}


\section{The Proof of Theorem 4.4}
We state the result as follows.
%\begin{theorem}
%\label{th:iso1}
%ESC-GNN is strictly more powerful than 2-WL, while not less expressive than 3-WL.
%\end{theorem}

\begin{theorem}
\label{th:iso}
ESC-GNN is strictly more powerful than 2-WL, while not less expressive than 3-WL.
\end{theorem}

\begin{figure}[btp]
	\centering
	\subfigure[]{
		\begin{minipage}[t]{0.43\linewidth}
			\centering
			\includegraphics[width=\columnwidth]{figure/Rook.png}
			%\caption{fig1}
		\end{minipage}%
	}%
	\subfigure[]{
		\begin{minipage}[t]{0.37\linewidth}
			\centering
			\includegraphics[width=\columnwidth]{figure/Sh.png}
			%\caption{fig1}
		\end{minipage}%
	}%
	\centering
	\vspace{-.1in}
	\caption{(a) the 4*4 Rook Graph and (b) the Shrikhande Graph}
	\label{fig:3wl}
	\vspace{-.15in}
\end{figure}



 \begin{proof}
 \textbf{ESC-GNN is not less powerful than 3-WL.} As shown in Theorem~\ref{th:count}, ESC-GNN is able to count 4-cliques. In the pair of graphs called the 4*4 Rook Graph and the Shrikhande Graph (shown in Figure~\ref{fig:3wl}), there exist several 4-cliques in the 4*4 Rook Graph, while there is no 4-clique in the Shrikhande Graph. Therefore ESC-GNN can differentiate the pair of graphs. Considering that 3-WL cannot differentiate them~\citep{arvind2020weisfeiler},  ESC-GNN is not less powerful than 3-WL.
 
 \textbf{ESC-GNN is more powerful than 2-WL.} Using the MPNN~\citep{xu2018powerful} as the backbone network, it can be as powerful as 2-WL in terms of distinguishing non-isomorphic graphs. However, there exist pairs of graphs, e.g., the 4*4 Rook Graph and the Shrikhande Graph that can be distinguished by ESC-GNN but not 2-WL. Therefore, ESC-GNN is strictly more powerful than the 2-WL.
 

 \end{proof}
 
\section{The Proof of Theorem 4.2}
Below we show the counting power of ESC-GNN in terms of subgraph counting. %We decompose Theorem~\ref{th:count} according to different substructures.

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figure/4cycle.png}}
\caption{Examples of 4-cycles that pass the rooted edges. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:4cycle}
\end{center}
%\vskip -0.2in
\end{figure*}

% \begin{figure*}[btp]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/4path.png}}
% \caption{Examples of 4-paths that may be potentially computed between the rooted nodes. In these figures, the rooted nodes are colored blue.}
% \label{fig:4path}
% \end{center}
% \end{figure*}

%\begin{theorem}
%\label{th:count1}
%In terms of subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques.
%\end{theorem}


\begin{theorem}
\label{th:count}
In terms of subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) stars with arbitrary sizes; (4) up to 3-paths.
\end{theorem}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figure/5cycle_sub.png}}
\caption{Examples where ESC-GNN cannot subgraph-count 5-cycles. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:5cycle_sub}
\end{center}
%\vskip -0.2in
\end{figure*}

\begin{proof}
\textbf{Clique counting.} The number of 3-cliques is the number of nodes with the shortest path distance ``1" to both rooted nodes; the number of 4-cliques is the number of edges labeled $(1,1,1,1)$ (definition see the edge-level distance encoding in Section 4 in the main paper, and example see Figure~\ref{fig:4cycle}(d)). Therefore, ESC-GNN can count these types of cliques. In terms of 5-cliques, 4-WL cannot count them according to Theorem~\ref{th:disab}, therefore subgraph GNNs rooted on edges with MPNN as the backbone GNN cannot count 5-cliques according to Proposition 3.5 in the main paper. Then according to Proposition 4.1 in the main paper, ESC-GNN also cannot count 5-cliques.

\textbf{Cycle counting.} the counting of 3-cycles is the same as 3-cliques. In terms of counting 4-cycles, there are basically 4 different situations where 4-cycles exist, examples are shown in Figure~\ref{fig:4cycle}. Note that figures (a),(b), and (c) contain one 4-cycle, respectively, while figure (d) contains two 4-cycles that pass the rooted edge. Therefore the number of 4-cycles is the weighted sum of the number of edges with labels $(1,2,2,1)$, $(1,1,2,1)$, $(1,2,1,1)$, $(1,1,1,1)$.

In terms of 5-cycle subgraph-counting, we provide a counter-example in Figure~\ref{fig:5cycle_sub}. In Figure~\ref{fig:5cycle_sub}(a), there is one 5-cycle $ABEDC$ that passes $AB$, while there is no 5-cycle that passes $AB$ in Figure~\ref{fig:5cycle_sub}(b). Considering that the degree information and the distance information is the same for the pair of graphs, ESC-GNN cannot differentiate them. Therefore, ESC-GNN cannot subgraph-count 5-cycles.


\textbf{Path Counting.} Note that the definition of paths here is different from other substructures. It denotes the number of edges within the path instead of the number of nodes within the path. Here, we slightly extend the use of 2-tuples in ESC-GNN by considering not only 2-tuples with edges but also 2-tuples without edges.

In terms of counting 2-paths or 3-paths between 2 nodes, it is equal to counting 3-cycles or 4-cycles, between edges, respectively. We have proven that ESC-GNN can count these cycles, therefore ESC-GNN can count such edges.

\textbf{Star counting.} We can decompose the graph-level star counting problem to 2-tuples by considering the first node of each 2-tuple as the root of stars. Examples are shown in Figure~\ref{fig:star}. We advocate that the number of stars is easily encoded by the number of nodes whose shortest path distance is 1 to the first rooted node. Denote the number of nodes with the shortest path distance 1 to the first rooted node as $N'$ (including the second node), then the number of $p$-stars is $C_{N'-1}^{p-2}$. A similar proof is provided by~\citep{chen2020can}.

\end{proof}

%Then we show the counting power of ESC-GNN in terms of counting paths.

%\begin{theorem}
%\label{th:path}
%In terms of subgraph counting, ESC-GNN can count up to 3-paths.
%\end{theorem}

%\begin{proof}
%Note that the definition of paths here is different from other substructures. It denotes the number of edges within the path instead of the number of nodes within the path. Here, we slightly extend the use of 2-tuples in ESC-GNN by considering not only 2-tuples with edges but also 2-tuples without edges.

%In terms of counting 2-paths or 3-paths between 2 nodes, it is equal to counting 3-cycles or 4-cycles, between edges, respectively. We have proven that ESC-GNN can count these cycles, therefore ESC-GNN can count such edges.
%\end{proof}

%\begin{theorem}
%In terms of subgraph counting, ESC-GNN can count stars of arbitrary shapes.
%\end{theorem}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figure/star.png}}
\caption{Examples of stars that pass the rooted edges. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:star}
\end{center}
%\vskip -0.2in
\end{figure*}

%\begin{proof}
%We can decompose the graph-level star counting problem to 2-tuples by considering the first node of each 2-tuple as the root of stars. Examples are shown in Figure~\ref{fig:star}. We advocate that the number of stars is easily encoded by the number of nodes whose shortest path distance is 1 to the first rooted node. Denote the number of nodes with the shortest path distance 1 to the first rooted node as $N'$ (including the second node), then the number of $p$-stars is $C_{N'-1}^{p-2}$. A similar proof is provided by~\citep{chen2020can}.
%\end{proof}

\section{The Proof of Theorem 4.3}

Below we show the counting power of ESC-GNN in terms of induced subgraph counting. We restate the Theorem 4.3 as follows.


\begin{theorem}
\label{th:count_ind}
In terms of induced subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) up to 4-stars; (4) up to 3-paths.
\end{theorem}

%\begin{theorem}
%In terms of induced subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) up to 3-paths; (4) up to 4-stars.
%\end{theorem}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{figure/5star.png}}
\caption{Examples where ESC-GNN cannot induced-subgraph-count 5-stars. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:5star}
\end{center}
%\vskip -0.2in
\end{figure*}


\begin{proof}
\textbf{Clieque counting.} Since cliques are fully connected substructures, the proof of cliques is the same as the proof of subgraph counting.

\textbf{Cycle counting.} For cycles, the number of 3-cycles is the same as 3-cliques. As for 4-cycles, we only need to consider the situation shown in Figure~\ref{fig:4cycle}(a), where the number of $(1,2,2,1)$ edges reflects the number of 4-cycles. For induced-subgraph-counting 5-cycles, in Figure~\ref{fig:5cycle}(a), there is no 5-cycle that pass $AB$, while in Figure~\ref{fig:5cycle}(b), there is one 5-cycle $ABEDC$ that passes $AB$. However, ESC-GNN cannot differentiate the two graphs since the degree information and the distance information is the same. Therefore, ESC-GNN cannot induced-subgraph-count 5-cycles. It can serve as the same example for not counting 4-paths.

\textbf{Path counting.} The proof of paths is actually the same as Theorem~\ref{th:count}.

\textbf{Star counting.} In terms of 3-stars and 4-stars, we only need to consider the situation shown in Figure~\ref{fig:star}(a), where the number of $(1,2)$ nodes encodes the number of stars, i.e., denote the number of $(1,2)$ nodes as $N'$, the number of $p$-stars ($p \leq 4$) is $C_{N'}^{p-2}$. 

For 5-stars, a pair of examples are shown in Figure~\ref{fig:5star}. These two graphs will be assigned the same structural embedding since the node degree information, and the distance information among these two graphs are the same. Therefore, ESC-GNN cannot differentiate the two graphs. However, Figure~\ref{fig:5star}(a) contains no 5-star that passes the rooted 2-tuple $AB$, while Figure~\ref{fig:5star}(b) contains one 5-star ($BAFDG$) that passes the rooted 2-tuple $AB$.
\end{proof}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.55\columnwidth]{figure/5cycle.png}}
\caption{Examples where ESC-GNN cannot induced-subgraph-count 5-cycles. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:5cycle}
\end{center}
%\vskip -0.2in
\end{figure*}

\section{The Proof of Theorem 4.5}

Here we restate Theorem 4.5 as follows: 

%\begin{theorem}
%\label{th:reg1}
%Consider all pairs of $r$-regular graphs with $n$ nodes, let $3\leq r < (2log2n)^{1/2}$ and $\epsilon$ be a fixed constant. With the hop parameter $h$ set to $\lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$, there exists a ESC-GNN that can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
%\end{theorem}

\begin{theorem}
\label{th:reg}
Consider all pairs of $r$-regular graphs with $n$ nodes, let $3\leq r < (2log2n)^{1/2}$ and $\epsilon$ be a fixed constant. With the hop parameter $h$ set to $\lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$, there exists an ESC-GNN that can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
\end{theorem}


Recall that we encode the distance information as augmented structural features for ESC-GNN. For the input graph $G$, denote the node-level distance encoding on $h$-hop subgraphs on 2-tuple $(u,v)$ as $D_{(u,v),G}^{h,\text{node}}$. Note that we can naturally transfer the encoding on 2-tuples to node by: ($D_{v,G}^{h,\text{node}} = D_{(v,v),G}^{h,\text{node}}$). We then introduce the following lemma.

\begin{lemma}
\label{lemma:reg}
For two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V^2, E^2)$ that are randomly and independently sampled from $r$-regular graphs with n nodes ($3 \leq r < (2log2n)^{1/2}$). Select two nodes $v_1$ and $v_2$ from $G_1$ and $G_2$ respectively. Let $h = \lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$ where $\epsilon$ is a fixed constant, $D_{v_1,G_1}^{h,\text{node}} = D_{v_2, G_2}^{h, \text{node}}$ with probability at most $o(n^{-3/2})$.
\end{lemma}

\begin{proof}
The proof follows from~\citep{bollobas1982distinguishing, feng2022powerful}. As $D_{v,G}^{h,\text{node}}$ stores exactly the same information as the node configuration used in~\citep{feng2022powerful}, therefore the proof is exactly the same as the proof of Lemma 1 in~\citep{feng2022powerful}. %Briefly speaking, the proof states that with the increasing of $h$, the number of neighboring nodes will almost be the same, thus $D_{v_1,G_1}^{h,\text{node}} = D_{v_2, G_2}^{h, \text{node}}$ with large probability.

Based on Lemma~\ref{lemma:reg}, we can prove Theorem~\ref{th:reg}. Given node $v_1 \in V_1$, compare $D_{v_1,G_1}^{h,\text{node}}$ with each $D_{v_2,G_2}^{h,\text{node}}$ where $v_2 \in V_2$. The probability that $D_{v_1,G_1}^{h,\text{node}} \neq D_{v_2,G_2}^{h,\text{node}}$ for all possible $v_2 \in V_2$ is $1-o(n^{-3/2})*n = 1 - o(n^{-1/2})$. Therefore, ESC-GNN can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
\end{proof}



\section{Additional Discussion on the Expressive Power of ESC-GNN}

In previous sections, we mainly discuss the expressive power enhanced by the proposed structural encoding. In this section, we shift to the integration of the global message passing layer and elucidate its substantial contribution to the expressive power. This is exemplified below through a toy example: consider graph $G$, composed of a 4-cycle and a 1-path (edge), and graph $H$, comprising a 5-path (a path with 6 nodes). When extracting 1-hop subgraphs for each node, both $G$ and $H$ yield two 1-paths and four "V graphs" (3 nodes and 2 edges). Consequently, the subgraph representation fails to distinguish between these two graphs. However, the introduction of a global message passing layer enables differentiation due to the distinct connections among these subgraphs. This distinction is applicable to both subgraph GNNs and our framework. A more comprehensive analysis can be found in~\citep{rattan2023weisfeiler}, which substantiates that the global message passing can enhance expressiveness regardless of the choice of the hop parameter.

Additionally, We conduct an ablation study on the substructure counting dataset by removing the global message passing layer from ESC-GNN. The resulting new model, denoted as "(-MP)", relies solely on aggregating the structural encoding to derive the final prediction. Given the absence of additional node attributes, the new model can be easily implemented as ESC-GNN with a single message passing layer that aggregates the edge-level structural embedding to obtain the node-level prediction. The result is presented in Table~\ref{tab:count_ab_mp}. As evident from the table, ESC-GNN consistently outperforms the new model across all benchmarks, demonstrating that the global message passing layer significantly boosts the representation power of ESC-GNN.

\begin{table*}
	\centering
	\caption{Ablation study on the proposed structural embedding (norm MAE).} 
    \label{tab:count_ab_mp}
	\scalebox{0.72}{
	\begin{tabular}{lccccccccc}
	\hline\noalign{\smallskip}
	Dataset & Tailed Triangle & Chordal Cycle & 4-Clique & 4-Path & Triangle-Rectangle & 3-cycles & 4-cycles & 5-cycles & 6-cycles \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	MPNN & 0.3631 & 0.3114 & 0.1645 & 0.1592 & 0.2979 &  0.3515 & 0.2742 & 0.2088 & 0.1555\\
 	\noalign{\smallskip}\hline\noalign{\smallskip}
	ESC-GNN & \cellcolor{yellow}0.0052 & 0.0169 & \cellcolor{yellow}0.0064 & 0.0254 & 0.0178 & \cellcolor{yellow}0.0074 &\cellcolor{yellow}0.0044 & 0.0356& 0.0337\\
    \noalign{\smallskip}\hline\noalign{\smallskip}
     (- MP) & 0.062 & 1.9355 & 0.4124& 0.0271 &0.0563 & 0.1508 & \cellcolor{yellow}0.0050 & 0.0996 &  0.0443 \\
	\hline\noalign{\smallskip}
    \end{tabular}}
\end{table*}


\section{Experimental Details}

\textbf{Stastics of Datasets.} The statistics of all used datasets are available in Table~\ref{tab:stat}.

\begin{table*}
	%\vspace{-0.1 in}
	\centering
	\caption{Statistics of the used datasets.}
	\label{tab:stat} 
	\scalebox{1.0}{
		\begin{tabular}{lccccc}
			\hline\noalign{\smallskip}
			Dataset & Graphs & Avg Nodes & Avg Edges & Task Type & Metric\\
			\noalign{\smallskip}\hline\noalign{\smallskip}
			MUTAG & 188 & 17.9 & 19.8 & Graph Classification & ACC\\
   			PTC-MR &  349 & 14.1 & 14.5 & Graph Classification & ACC\\
			ENZYMES &  600 & 32.6 & 62.1 & Graph Classification & ACC \\
			PROTEINS & 1113 & 39.1 & 72.8 & Graph Classification & ACC \\
			IMDB-BINARY & 1000 & 19.8 & 96.5 & Graph Classification & ACC\\
			ZINC-12k & 12000 & 23.2 & 24.9 & Graph Regression & MAE\\
			ogbg-molhiv & 41127 & 25.5 & 27.5 & Graph Classification & AUC-ROC\\
            ogbg-molpcba & 437929 & 26.0 & 28.1 & Graph Classification & AP\\
            QM9 & 129433 & 18.0 & 18.6 & Graph Regression & MAE\\
            Synthetic & 5000 & 18.8 & 31.3 & Node Regression & MAE\\
			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
	\end{tabular}}
	%\vspace{-0.1 in}
\end{table*}

\textbf{Experimental details.} The baselines and data splittings of our experiments follow from existing works~\citep{zhang2021nested, huang2023boosting} and the standard data split setting. For ESC-GNN, we adopt GIN~\citep{xu2018powerful} as the backbone GNN. In the structural embedding, we use both the shortest path distance and the resistance distance~\citep{lu2011link} as the distance feature. For the hop parameter $h$ we search between 1 to 4, and report the best results. Following existing works~\citep{huang2023boosting}, we use Adam optimizer as the optimizer, and use plateau scheduler with patience 10 and decay factor 0.9. On most datasets, the learning rate is set to 0.001, and the hidden embedding dimension is set to 300. The training epoch is set to 2000 for counting substructures, 400 for QM9, 1000 for ZINC, and 150 for the OGB dataset. Most of the experiments are implemented with two Intel Core i9-7960X processors and 2 NVIDIA 3090 graphics cards. Others (e.g., experiments on the TU dataset) are implemented with two Intel Xeon Gold 5218 processors and 10 NVIDIA 2080TI graphics cards. 

\section{Evaluation on Real-World Datasets}
\label{subsec:real}

\textbf{Molecule Dataset.} We evaluate ESC-GNN on various popular real-world molecule datasets, including ZINC~\citep{dwivedi2020benchmarking} and the OGB dataset~\citep{hu2020open}. ZINC is a dataset of chemical compounds, and the task is graph regression. For the OGB dataset, we use ogbg-molhiv and ogbg-molpcba for evaluation. ogbg-molhiv contains 41K molecules with 2 classes, and ogbg-molpcba contains 438 molecules with 128 classes. The task is to predict to which these molecules belong. We follow the standard evaluation metric and the dataset split, and report the result in Table~\ref{tab:mol}.

%\begin{table*}
%\centering
%\caption{Evaluation on QM9 (MAE)} 
%\label{tab:qm9}
%\begin{tabular}{lcccccccc}
%\hline\noalign{\smallskip}
%Dataset & 1-GNN & 1-2-3-GNN & DTNN & Deep LRP & PPGN & NGN & I$^2$-GNN & ESC-GNN \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%$\mu$ & 0.493 & 0.476 & 0.244 & 0.364 & \textbf{0.231} & 0.428 & 0.428 & \textbf{0.231}\\
%%$\alpha$ & 0.78 & 0.27 & 0.95 & 0.298 & 0.382 & 0.29 & \textbf{0.230} & 0.265\\
%$\epsilon_{\text{homo}}$  &0.00321 &0.00337& 0.00388& 0.00254& 0.00276 &0.00265 &0.00261& \textbf{0.00221}\\
%$\epsilon_{\text{lumo}}$  &0.00355& 0.00351 &0.00512& 0.00277 &0.00287 &0.00297& 0.00267&\textbf{0.00204}\\
%$\Delta_{\epsilon}$ & 0.0049 &0.0048 &0.0112& 0.00353& 0.00406& 0.0038& 0.0038&\textbf{0.0032}\\
%$R^2$ & 34.1& 22.9& 17.0& 19.3 &16.07 &20.5 &18.64&\textbf{7.28}\\
%ZPVE & 0.00124& 0.00019 &0.00172 &0.00055 &0.0064& 0.0002 &\textbf{0.00014}&0.00033\\
%$U_0$ & 2.32& 0.0427 &2.43 &0.413 &0.234 &0.295& \textbf{0.211}&0.645\\
%%$U$ & 2.08& \textbf{0.111}& 2.43 &0.413 &0.234 &0.361 &0.206&0.380\\
%$H$ & 2.23 &\textbf{0.0419} &2.43 &0.413 &0.229 &0.305 &0.269&0.427\\
%$G$ & 1.94 &\textbf{0.0469} &2.43 &0.413 &0.238& 0.489 &0.261&0.384\\
%$C_v$ &0.27& 0.0944& 2.43& 0.129 &0.184 &0.174 &\textbf{0.0730} &0.105\\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%\end{tabular}\end{table*}


\begin{table}
\centering
\caption{Evalation on ZINC and OGB datasets.} 
\label{tab:mol}
\scalebox{0.8}{
\begin{tabular}{lccc}
\hline\noalign{\smallskip}
Dataset & OGBG-HIV (AUCROC) & OGBG-PCBA (AP) & ZINC \\
\noalign{\smallskip}\hline\noalign{\smallskip}
%GCN & 75.99$\pm$1.19* & 24.24$\pm$0.34* \\
GIN~\citep{xu2018powerful} & 77.07$\pm$1.49 & 27.03$\pm$0.23 & 0.163 \\
PNA~\citep{corso2020principal} & 79.05$\pm$1.32 & 28.38$\pm$0.35 & 0.188\\
GSN~\citep{bouritsas2022improving} & 77.99$\pm$0.01 & - & 0.115\\
DGN~\citep{beaini2021directional} & 79.70$\pm$0.97 & 28.85$\pm$0.30 & 0.168 \\
CIN~\citep{bodnar2021weisfeiler1} & \textbf{80.94$\pm$0.57} & - & \textbf{0.079} \\
NGNN~\citep{zhang2021nested} & 78.34$\pm$1.86 & 28.32$\pm$0.41 & 0.111\\
GIN-AK+~\citep{zhao2022stars} & 79.61$\pm$1.19 & \textbf{29.30$\pm$0.44} & 0.080\\
OSAN~\citep{qianordered2022} & - & - & 0.126\\ 
SUN~\citep{frascaunderstanding2022} & 80.03$\pm$0.55 &  - & 0.083 \\
DSS-GNN~\citep{bevilacqua2022equivariant} & 76.78$\pm$1.66 & - & 0.102 \\
I$^2$-GNN~\citep{huang2023boosting} & 78.68$\pm$0.93 & - & 0.083 \\
Graph Transformer~\citep{rampavsek2022recipe} & 77.40$\pm$1.77 & 27.51$\pm$0.28 & 0.113 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
ESC-GNN & 78.62$\pm$1.06 & 28.16$\pm$0.31 & 0.075$\pm$0.002\\%0.096\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}}
\end{table}

\textbf{TU datasets.} We evaluate the performance of ESC-GNN on the TU datasets~\citep{morris2020tudataset}. The experimental settings follow~\citep{zhang2021nested} for more consistent evaluation standards. Specifically, we uniformly use the 10-fold cross validation framework, with the split ratio of training/validation/test set 0.8/0.1/0.1. The results are available in Table~\ref{tab:tu}. In the table, ESC-GNN (h2) and ESC-GNN (h3) denote ESC-GNN with the hop parameters setting to 2 and 3. 

\subsection{Results} 

\textbf{Comparison with backbone models.} On OGBG-MolHIV, OGBG-MolPCBA, and the TU dataset, we use GIN~\citep{xu2018powerful} as the backbone model. While on ZINC, we use a plain graph transformer without positional encodings from~\citep{rampavsek2022recipe} as the backbone model. The experimental results compared with these backbones demonstrate that the proposed structural information not only enhances the theoretical representation power but also improves the empirical representation power.

\textbf{Comparison with subgraph GNNs.} When compared to subgraph GNNs rooted at 2-tuples, such as I$^2$-GNN, ESC-GNN performs slightly worse or comparably on these benchmark tasks. This empirical observation provides evidence that the proposed structural embedding effectively captures valuable information from subgraph GNNs, benefiting downstream tasks. It is worth noting that I$^2$-GNN generally outperforms node-based subgraph GNNs like NGNN and GIN-AK+, suggesting that the theoretical advantages gained from subgraph selection and aggregation policies contribute to the empirical representation power. Furthermore, among the subgraph GNNs, those that introduce interactions between subgraphs, such as SUN, GIN-AK+, and OSAN, tend to perform better compared to subgraph GNNs that do not incorporate such interactions, like NGNN and ID-GNN. This observation indicates that subgraph interaction also plays a role in enhancing the representation power of subgraph GNNs. Exploring additional techniques to incorporate this interaction can be considered for future research.

\textbf{Comparison with GNNs that incorporate the substructure information.} GSN~\citep{bouritsas2022improving} incorporates the number of specific substructures as augmented features. In contrast to GSN, our framework does not explicitly encode the number of substructures. Instead, we encode the more general distance information of subgraphs, which enables plain GNNs to count many substructures without the need to manually determine which substructures to include. Regarding the real-world dataset, GSN achieves a MAE score of 0.115 on ZINC, whereas ESC-GNN attains a MAE score of 0.096. Similarly, utilizing the same backbone (GIN), GSN records an AUCROC score of 77.99 on OGBG-HIV, whereas ESC-GNN demonstrates a higher AUCROC score of 78.62. These outcomes underscore the superiority of our proposed model. 



\begin{table}
\centering
\caption{Experiments on TU, Accuracy as the evaluation metric.} 
\label{tab:tu}
\begin{tabular}{lcccccccc}
\hline\noalign{\smallskip}
Dataset & MUTAG & PTC-MR & PROTEINS & ENZYMES & IMDB-B \\
\noalign{\smallskip}\hline\noalign{\smallskip}
GIN & 84.5$\pm$8.9 & 51.2$\pm$9.2 & 70.6$\pm$4.3 & 38.3$\pm$6.4 & 73.3$\pm$4.7 \\
PPGN & 84.7$\pm$8.2 & 55.0$\pm$6.4 & 74.8$\pm$3.3 & 55.0$\pm$6.4 & 71.5$\pm$5.4\\
NGNN & 87.9$\pm$8.2 & 54.1$\pm$7.7 & 73.9$\pm$5.1 & 29.0$\pm$8.0 & 73.1$\pm$5.7 \\
GIN-AK+ & \textbf{88.8$\pm$4.0} & 60.5$\pm$8.0 & 75.5$\pm$4.4 & \textbf{58.9$\pm$6.2} & 72.4$\pm$3.7\\ 
SUN & 86.1$\pm$6.0 & 60.2$\pm$7.2 & 72.1$\pm$3.8 & 16.7$\pm$0.0 & \textbf{73.7$\pm$2.9}\\
I$^2$-GNN & 87.9$\pm$4.3 & \textbf{61.4$\pm$8.7} & 74.8$\pm$2.9 & 40.3$\pm$6.7 & 73.6$\pm$4.0\\
%nest PPGN & \textbf{91.0$\pm$7.0} & 59.6$\pm$10.7& 75.0$\pm$3.3 & 40.0$\pm$7.4& \textbf{74.7$\pm$3.9}\\
%edge PPGN & 85.2$\pm$7.7 & 57.6$\pm$9.0 & \textbf{76.8$\pm$3.6} & 51.1$\pm$5.5 & OOM \\
\noalign{\smallskip}\hline\noalign{\smallskip}
ESC-GNN (h2) & 86.2$\pm$7.9 & 52.9$\pm$6.4 & 73.3$\pm$4.1 & 53.2$\pm$8.1 & 72.0$\pm$6.0\\
ESC-GNN (h3) & 85.6$\pm$7.9 & 56.4$\pm$6.9 & \textbf{76.0$\pm$4.5} & 43.3$\pm$6.0 & \textbf{73.7$\pm$4.8}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

\section{Ablation Study}
\label{subsec:ablation}
We evaluate the effectiveness of each part of the proposed structural embedding on the substructure counting dataset. For the proposed three types of structural embedding (the degree encoding, the node-level distance encoding, and the edge-level distance encoding), we delete one of them from the original structural embedding every time and report the results in Table~\ref{tab:count_ab}. We observe that after removing the three types of embedding, ESC-GNN performs worse compared with its original version, especially after removing the edge-level distance encoding. This is consistent with our theoretical results: in Theorem~\ref{th:sub}, we show that the proposed structural embedding contains key information for the counting power of subgraph GNNs; in Theorem~\ref{th:count} and Theorem~\ref{th:count_ind}, we show that the edge-level distance information directly encodes the number of certain types of substructures.

\begin{table*}
	\centering
	\caption{Ablation study on the proposed structural embedding (norm MAE).} 
    \label{tab:count_ab}
	\scalebox{0.72}{
	\begin{tabular}{lccccccccc}
	\hline\noalign{\smallskip}
	Dataset & Tailed Triangle & Chordal Cycle & 4-Clique & 4-Path & Triangle-Rectangle & 3-cycles & 4-cycles & 5-cycles & 6-cycles \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	MPNN & 0.3631 & 0.3114 & 0.1645 & 0.1592 & 0.2979 &  0.3515 & 0.2742 & 0.2088 & 0.1555\\
 	\noalign{\smallskip}\hline\noalign{\smallskip}
	ESC-GNN & \cellcolor{yellow}0.0052 & 0.0169 & \cellcolor{yellow}0.0064 & 0.0254 & 0.0178 & \cellcolor{yellow}0.0074 &\cellcolor{yellow}0.0044 & 0.0356& 0.0337\\
    \noalign{\smallskip}\hline\noalign{\smallskip}
     (- degree) &0.0121 & 0.0492& 0.0106& 0.0322 & 0.0349 &0.0342 &0.0144 &0.0513 & 0.0652\\
    (- node-level dist) & 0.0382 & 0.0344 & 0.0222 &0.0428 & 0.0256 & 0.0157 & 0.0261 & 0.0492 &0.0608  \\
    (- edge-level dist) & 0.0208&0.2811 &0.0497 & 0.0584&0.185 & 0.2617&0.2244 & 0.1654& 0.1364\\
	\hline\noalign{\smallskip}
    \end{tabular}}
\end{table*}

\section{Evaluation on the Space Cost}

With regards to the preprocessing time, our approach's preprocessing time is comparable to that of I$^2$-GNN as shown in Table 3 in the main paper since we use their code to extract subgraph information. Although we require a small amount of extra time to extract and preprocess the structural embeddings, we believe that this is a reasonable trade-off since the actual model running time is reduced by orders. When compared to the total running time of subgraph GNNs, the preprocessing time is negligible. Therefore, our ESC-GNN's total running time is less than 1\% of that of I$^2$-GNN on ogbg-hiv.

%Furthermore, we note that I$^2$-GNNâ€™s preprocessing code is not parallel currently. Since the preprocessing of subgraphs are independent of each other, we can expect a further 10-100 times of speedup (depending on cores) in preprocessing time if parallel computing is used.

As for the storage of structural embeddings, we only need to store a vector of integer indices for each structural embedding, as illustrated in Figure 1 in the main paper, without really storing any dense high-dimensional vectors. Therefore, the storage is still manageable. Specifically, when feeding the indices to the backbone model, we use a learnable matrix (which is a model parameter) to transform the integer index vector into dense embeddings. For example, to extract the degree information of the first subgraph in Figure 1 in the main paper, we use a learnable matrix $W \in \mathbb{R}^{4\times h}$ and compute the degree information with $W * [0;0;4;0]$, where $*$ denotes sparse matrix multiplication (which is very fast for sparse matrices), and the sparse vector $[0;0;4;0]$ is what we need to store. This approach requires relatively small storage space.

We have also included the space cost of our approach in Table~\ref{tab:space}. In these datasets, ESC-GNN requires much less storage space than I$^2$-GNN while slightly more space than NGNN. These results demonstrate that our approach for storing structural embeddings offers excellent storage performance.


\begin{table*}
	%\vspace{-0.1 in}
	\centering
	\caption{Evaluation on the space cost.}
	\label{tab:space} 
	\scalebox{1.0}{
		\begin{tabular}{lccccc}
			\hline\noalign{\smallskip}
			Dataset & OGBG-HIV &ZINC & QM9\\
			\noalign{\smallskip}\hline\noalign{\smallskip}
			Original & 159MB & 16.2MB & 281MB\\
   			NGNN & 2.32GB & 218MB & 2.90GB\\
			I$^2$-GNN & 5.95GB & 627MB & 8.25GB \\
			ESC-GNN & 2.57GB & 427MB & 4.46GB\\


			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
	\end{tabular}}
	%\vspace{-0.1 in}
\end{table*}


\section{Counting Substructures on Other Datasets}

In order to evaluate the counting power of the proposed model, we conducted experiments on the ZINC dataset by generating the cycle counting task and reporting the MAE result in Table~\ref{tab:cnt_zinc}. To count cycles, we used the simple-cycle function from \url{https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.cycles.simple_cycles.html}. Surprisingly, we found that all methods achieved much better MAE scores than those reported in Table 1 of the main paper. This may be due to the fact that many graphs in ZINC contain few cycles, resulting in a small MAE value.

In general, the reported results are consistent with Table 1 of the main paper. ESC-GNN performs better in counting 3-cycles and 4-cycles, and performs worse on 5-cycles and 6-cycles. This observation is consistent with Theorem~\ref{th:count}. Furthermore, it outperforms MPNN, performs comparably to GIN-AK+, and is outperformed by I$^2$-GNN, which is consistent with our theoretical results, showing that the proposed structural embedding can extract valuable information from the subgraph GNNs.

\begin{table*}
	%\vspace{-0.1 in}
	\centering
	\caption{Evaluation on Counting Substructures on ZINC (norm MAE).}
	\label{tab:cnt_zinc} 
	\scalebox{1.0}{
		\begin{tabular}{lccccc}
			\hline\noalign{\smallskip}
			Tasks & 3-cycle & 4-cycle & 5-cycle & 6-cycle \\


			\noalign{\smallskip}\hline\noalign{\smallskip}
			GNN & 0.0016 & 0.0030 & 0.0394 & 0.1442\\
   			GIN-AK+ & 0.0009 & 0.0064 & 0.0036 & 0.0057\\
			NGNN & 	0.0002 &	0.0001	& 0.0007 & 0.0012 \\
            I$^2$-GNN &0.0003 & 0.0001 & 0.0003 & 0.0007 \\


			ESC-GNN & 0.0008	&0.0004	&0.0058 &	0.0047\\


			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
	\end{tabular}}
	%\vspace{-0.1 in}
\end{table*}

\section{Limitations and the Assets We Used}

\textbf{Limitations of the paper.} First, we have shown that the representation power of our model is bounded by 4-WLs and subgraph GNNs rooted on 2-tuples in terms of distinguishing non-isomorphic graphs and counting substructures.

Second, the proposed model may not reach a satisfying performance on benchmarks where the encoded substructures are of no use. Also, the proposed model may not suit high-order graphs where the neighbors of the nodes and edges are defined differently from the simple graphs. 

\textbf{The assets we used.} Our model is experimented on benchmarks from~\citep{dwivedi2020benchmarking, hu2020open, zhao2022stars, morris2020tudataset, abboud2021surprising, balcilar2021breaking, murphy2019relational, ramakrishnan2014quantum, wu2018moleculenet} under the MIT license.


% In terms of counting 4-paths, we follow the inspiration from~\citep{furer2017combinatorial}. Let $p_{uv}^l$ be the number of paths of length $l$ from node $u$ to $v$. As proven above, we can extract $p_{uv}^1$ and $p_{uv}^2$ for all node pairs. Therefore, for all $l_1, l_1' \in \{0,1\}$ and $l_2,l_2' \in N$, we can compute:
% $$n_{l_1l_2l_1'l_2'} = |\{w|w \notin \{u,v\} \wedge p_{uw}^i = l_i \wedge p_{wv}^i = l_i' \text{ for } i \in \{1,2\}\}|.$$

% Then, we can compute the number of 4-paths between $u$ and $v$ by first combining all paths of length 2 from $u$ to $w$ and $w$ to $v$. We then remove two cases: (1) the 2-paths between $u$ and $w$ through $v$, or the 2-paths between $v$ and $w$ through $u$ (Figure~\ref{fig:4path}(c)); (2) walks in the form of $u,x,w,x,v$ where for each $x$ (Figure~\ref{fig:4path}(b)). Therefore, the final number of 4-paths between $u$ and $v$ can be computed as:
% $$\sum_{l_1,l_2,l_1',l_2'}n_{l_1l_2l_1'l_2'} (l_2 - p_{uv}^1l_1')(l_2'-p_{uv}^1l_1) - \sum_{x \in V\\\{u,v\}} p_{ux}^1(d(x)-2)p_{xv}^1$$
% \end{proof}

% And based on the above result, we can compute the number of 5-cycles that pass node pair $(u,v)$ because we know whether there exist an edge between $u$ and $v$ or not. Therefore, we have
% \begin{theorem}
% \label{th:count2}
% ESC-GNN can count 5-cycles.
% \end{theorem}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nocite{langley00}

%\clearpage
\bibliography{example_paper}
%\bibliographystyle{plain}
%\bibliographystyle{icml2023}
\bibliographystyle{iclr2024_conference}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
