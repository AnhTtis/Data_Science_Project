%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{colortbl}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage[]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\zy}[1]{\textcolor{magenta}{[Zuoyu: #1]}}
\newcommand{\jr}[1]{\textcolor{violet}{[Junru: #1]}}
\newcommand{\mh}[1]{\textcolor{red}{[Muhan: #1]}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Efficiently Counting Substructures by Subgraph GNNs without Running GNN on Subgraphs}

\begin{document}

\twocolumn[
\icmltitle{Efficiently Counting Substructures by Subgraph GNNs\\  without Running GNN on Subgraphs}
% \icmltitle{Enhancing the Substructure Counting Ability of Subgraph GNNs\\ without Running GNN on Subgraphs}
% \icmltitle{Counting Substructures with Subgraph GNNs \\ without Running GNN on Subgraphs}
%\icmltitle{Subgraph GNNs without really running GNN on subgraphs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zuoyu Yan}{wict}
\icmlauthor{Junru Zhou}{pkuai}
\icmlauthor{Liangcai Gao}{wict}
\icmlauthor{Zhi Tang}{wict}
\icmlauthor{Muhan Zhang}{pkuai,bai}

%\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
%\icmlauthor{Firstname3 Lastname3}{comp}
%\icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{wict}{Wangxuan Institute of Computer Technology, Peking University}
\icmlaffiliation{pkuai}{Institute for Artificial Intelligence, Peking University}
\icmlaffiliation{bai}{Beijing Institute for General Artificial Intelligence}
%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Liangcai Gao}{glc@pku.edu.cn}
\icmlcorrespondingauthor{Muhan Zhang}{muhan@pku.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Count substructures, Subgraph network, Graph Neural Networks}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Using graph neural networks (GNNs) to approximate specific functions such as counting graph substructures is a recent trend in graph learning. Among these works, a popular way is to use subgraph GNNs, which decompose the input graph into a collection of subgraphs and enhance the representation of the graph by applying GNN to individual subgraphs. Although subgraph GNNs are able to count complicated substructures, they suffer from high computational and memory costs. In this paper, we address a non-trivial question: can we \textit{count substructures efficiently} with GNNs? To answer the question, we first theoretically show that the distance to the rooted nodes within subgraphs is key to boosting the counting power of subgraph GNNs. We then encode such information into structural embeddings, and precompute the embeddings to avoid extracting information over all subgraphs via GNNs repeatedly. Experiments on various benchmarks show that the proposed model can preserve the counting power of subgraph GNNs while running orders of magnitude faster. 
\end{abstract}

\section{Introduction}
Message Passing Neural Networks (MPNNs) are the most commonly used Graph Neural Networks (GNNs). They have achieved remarkable success on graph representation learning~\cite{KipfW17, xu2018powerful}, and have been widely used in various downstream tasks~\cite{wu2020comprehensive, zhou2020graph}. However, the representation power of MPNNs is limited~\cite{xu2018powerful}, thus increasing efforts have been spent on designing more powerful GNNs.

There are two main perspectives to evaluate the representation power of GNNs. One is the ability to differentiate whether a pair of graphs are isomorphic or not. \citet{xu2018powerful} show that MPNNs are at most as powerful as the Weisfeiler-Leman (WL) test~\cite{weisfeiler1968reduction} in terms of differentiating graphs. However, WL test is powerful enough to distinguish almost all pairs of non-isomorphic graphs~\cite{babai1980random}, leading to the concern about whether it is worth sacrificing extra computational cost only for distinguishing more graphs. 

Another way to evaluate the representation power of GNNs is to approximate specific functions, such as counting certain graph substructures. Substructure counting is important in various domains, such as chemistry~\cite{deshpande2002automated, jin2018junction}, biology~\cite{koyuturk2004efficient}, and sociology~\cite{jiang2010finding}. Besides, substructure counting is also related to graph kernels~\cite{shervashidze2009efficient} and spectral computation~\cite{preciado2010local}. Nevertheless, MPNNs' substructure counting ability is shown to be very limited, failing to count even triangles~\citep{chen2020can}. Consequently, the ability to count substructures can serve as a more intuitive and more practically useful approach to evaluate the representation power of GNNs.

In this paper, we focus on the counting power of GNNs. Specifically, whether GNNs can count the number of given connected substructures in a graph. These global graph-level counting problems can be decomposed into local tuple-level (e.g., node-level, edge-level) counting problems. For example, to count 3-cycles (triangles) over a graph, we can first count the number of 3-cycles that pass each node, sum the number over all nodes, and then divide the number by 3. The reason for such decomposition is that we can use a less expressive model to count such substructures locally, leading to much better efficiency. In the case of counting 3-cycles, one may need a model as powerful as 3-WL~ \citep{maron2019provably, tahmasebi2020counting} to reach a global-level expressiveness. However, if we decompose it into a node-level counting problem, we can extract the subgraph of each node, and use faster models such as~\citet{zhang2021nested, you2021identity} to count 3-cycles within the subgraphs.

These models are called subgraph GNNs~\cite{you2021identity, zhang2021nested,bevilacqua2022equivariant,zhao2022stars,frascaunderstanding2022,huang2023boosting}. For an input graph, they first decompose the input graph into a collection of subgraphs (overlap is allowed) based on certain subgraph selection policies. Then some base GNNs are applied to the extracted subgraphs whose representations are used to enhance the graph representation. Although faster than globally expressive GNNs, they still need to run GNNs over all subgraphs. Therefore they are much slower compared with classic MPNNs, and suffer from high computational cost when encoding large and dense graphs. 

Based on the above observation, we raise a nontrivial question: can we count substructures \textbf{efficiently} with GNNs, ideally using a similar cost to MPNNs? To answer the above, we decompose it into two sub-questions: (1) what provides the extra representation power of subgraph GNNs compared with classic MPNNs? (2) can we utilize such information efficiently without running GNNs on subgraphs? To answer the first question, we show that the key to boosting the counting power of subgraph GNNs is the \textbf{distance} to the rooted nodes within the subgraph. Take subgraph GNNs with distance encoding~\citep{li2020distance} as an example. They first extract the subgraph rooted at each node, and then enhance the node feature with its distance to the rooted node. Given a node $u$ and its subgraph $G_u$, denote the label of any node $v$ within the subgraph as its distance to $u$: $f(v) = d(u, v)$. Then the number of 3-cycles/3-cliques that pass $u$ is exactly the number of edges with both nodes labeled one. Note that the distance information is not restricted to models using distance encoding~\cite{zhang2021nested, huang2023boosting}. It can also be learned by models such as~\cite{you2021identity, papp2021dropgnn, cotta2021reconstruction, kreuzer2021rethinking}. 
%Classic MPNNs such as~\cite{xu2018powerful} can extract such information, thus subgraph GNNs are able to count 3-cycles while MPNNs cannot.

To answer the second question, we find that such distance information can be encoded into a precomputed structural embedding, without the need to run GNNs repeatedly on each subgraph. For example, in the above case, we can directly use the number of edges with both nodes labeled one as the structural embedding of the rooted node. In this way, we only need to run GNNs on the original graph (augmented with precomputed structural embeddings), while being able to efficiently count substructures. %In this paper, we theoretically show that for any given connected substructure, there exists a corresponding structural embedding that can count such substructures. 

In summary, our contributions are listed as follows:
\begin{itemize}
\item Previous works have connected subgraph GNNs to substructure counting problems, but few work answers why they are so naturally connected. In this paper, we try to provide insights to the question, by theoretically showing that subgraph GNNs are \textit{much more efficient yet nearly as powerful} as globally expressive models in terms of counting substructures.%with backbone GNN as expressive as $k$-WL ($k \geq 2$) and subgraphs rooted at $m$-tuples, can count all connected substructures with no more than $k+m$ nodes. In other words, it can be nearly as expressive as $(k+m)$-WL in terms of counting substructures while being much more efficient.
\item Compared with classic MPNNs, subgraph GNNs need to learn representations over all subgraphs, and thus are much slower. To accelerate them, we first theoretically characterize the general substructure counting power of subgraph GNNs, and show that the distance to the rooted nodes within subgraphs is one key to boosting their counting power. We then propose a structural embedding to encode such distance information. 

\item We propose a model, Efficient Substructure Counting GNN (\textit{ESC-GNN}), which enhances a basic GNN model with the structural embedding. It only needs to run message passing on the whole graph, and thus is much more efficient than subgraph GNNs. We evaluate ESC-GNN on various real-world and synthetic benchmarks. Experiments show that ESC-GNN performs comparably with subgraph GNNs on real-world tasks and counting substructures, while running much faster.
\end{itemize}

\section{Related Works}
\label{sec:rel}

\subsection{Representation power of GNNs}
There are two major perspectives to evaluate the representation power of GNNs: the ability to distinguish non-isomorphic graphs, and the ability to approximate specific functions. In terms of differentiating graphs, \citet{xu2018powerful} and \citet{morris2019weisfeiler} showed that MPNNs are at most as powerful as the WL test~\cite{weisfeiler1968reduction}. Following works improve the expressiveness of GNNs by using high-order information~\cite{morris2019weisfeiler, morris2020weisfeiler, maron2019provably, bodnar2021weisfeiler, bodnar2021weisfeiler1, vignac2020building} or augmenting node features~\cite{bouritsas2022improving, barcelo2021graph, dwivedi2021graph, loukas2020hard, abboud2021surprising, kreuzer2021rethinking, lim2022sign}. 

In terms of approximating specific functions, some works use GNNs to approximate graph algorithms~\cite{velivckovic2019neural, xhonneux2021transfer, yan2022neural}. In this paper, we focus on GNN's ability to count substructures, especially connected substructures. Some previous works~\cite{furer2017combinatorial, arvind2020weisfeiler} relate the counting power to the expressiveness of GNNs, by providing substructures that WL can count. Following works count substructures with powerful networks, such as using the local relational pooling~\cite{murphy2019relational, chen2020can} or high-order GNNs \cite{tahmasebi2020counting}. However these works either suffer from high computational cost or requires truncation methods that lack theoretical guarantees. There are several other studies in the literature that count subgraph isomorphisms~\cite{liu2020neural, liu2022graph, yu2023learning}. However, they do not provide theoretical guarantees for counting substructures in general or in specific cases. Analyzing what substructures GNNs can count has significant implications in understanding their representation power and generalization. For example, being able to count 6-cycles enables a wide range of chemical prediction tasks involving benzenes.

 %However, these works bring in much computational cost, leading to works that focus on the local counting power of GNNs. 

\subsection{Subgraph GNNs}
To improve the representation power of GNNs, subgraph GNNs extract the input graph into a collection of subgraphs, and use the subgraph information to enhance the representations of graph elements (e.g., nodes, edges). The subgraph selection policies vary among different works, such as graph element deletion~\cite{bevilacqua2022equivariant, cotta2021reconstruction, papp2021dropgnn}, k-hop subgraph extraction~\cite{abu2019mixhop, sandfelder2021ego, nikolentzos2020k, feng2022powerful}, node identity augmentation~\cite{you2021identity}, and rooted subgraph extraction~\cite{zhang2021nested, zhao2022stars, frascaunderstanding2022, papp2022theoretical, zhang2021labeling, huang2023boosting, qianordered2022}. Most of these works need to run message passing and aggregation over all subgraphs, therefore performing much slower than classic MPNNs. This prevents the use of subgraph GNNs in large real-world datasets. Furthermore, there are other studies that focus on extracting structural information within subgraphs and integrating it with MPNNs~\cite{zhang2018link, yan2021link}, but these works are beyond the scope of this paper.

\section{Preliminaries}
Let $G = (V,E)$ be a simple, undirected graph, where $V = \{1,2,...,N\}$ is the node set, and $E$ is the edge set. We use $x_v$ to represent the node attribute for $v \in V$, and $e_{uv}$ to represent the edge attribute for $uv \in E$. Denote the $h$-hop neighborhood of node $v$ as $V_{v}^h = \{u \in V|d(u,v) \leq h\}$, where $d(u,v)$ denotes the shortest path distance between node $u$ and $v$. For a special case where $h = 1$, we call it the neighborhood of node $v$: $N(v) = V_v^1$.

Define a subgraph of $G$ as any graph $G^S = (V^S, E^S)$ with $V^S \subseteq V$ and $E^s \subseteq E$. And an induced subgraph of $G$ is any graph $G^I = (V^I, E^I)$ where $V^I \in V$, and $E^I = E \cap (V^I)^2$ is the set of all edges in $E$ where both nodes belong to $V^I$. For a $k$-tuple $\vec{v} = (v_1, ..., v_k) \in V^k$, define its rooted $h$-hop subgraph as $G_{\vec{v}}^h = (V_{\vec{v}}^h, E_{\vec{v}}^h)$, where $V_{\vec{v}}^h$ is the union of the $h$-hop neighborhoods of all vertices in $\vec{v}$: $V_{\vec{v}}^h = V_{v_1}^h \cup ... \cup V_{v_k}^h$, and $E_{\vec{v}}^h = E\cap (V_{\vec{v}}^h)^2$ are edges whose two nodes both belong to $V_{\vec{v}}^h$. Later, We will omit the hop parameter $h$ for simplicity.

In this paper, we focus on graph-level counting of connected substructures\footnote{Some works~\cite{huang2023boosting} focus on node-level counting, which can also be transferred to graph-level counting.}. Connected substructures are substructures whose nodes belong to a connected component. We study four types of connected substructures that are widely used in existing works: cycles, cliques, stars, and paths. An $L$-path is a sequence of edges $[(v_1, v_2),...,(v_L,v_{L+1})]$ such that all nodes are distinct; an $L$-cycle is an $L$-path except that $v_1 = v_{L+1}$; an $L$-clique is a fully connected graph with $L$ nodes; and an $L$-star denotes a set of edges $[(v, v_1), (v, v_2),...,(v, v_{L-1})]$ where all nodes with different symbols are distinct. Two substructures are called equivalent if their sets of edges are equal. Given a substructure $S$ and a graph $G$, the \textbf{subgraph counting} is defined as counting the number of inequivalent substructures $C_S(S,G)$ that are subgraphs of $G$. The \textbf{induced subgraph counting} is defined as counting the number of inequivalent substructures that are induced subgraphs of $G$. In this paper, we focus on subgraph counting, but we also provide theoretical results on induced subgraph counting.


Following existing works~\cite{chen2020can, huang2023boosting}, we formally define our task as:

\begin{definition}
Let $\mathcal{G}$ be the set of all graphs and $\mathcal{F}$ be a function class over graphs. We say $\mathcal{F}$ can count connected substructure $S$ on $\mathcal{G}$ if for all $G_1$, $G_2 \in \mathcal{G}$ such that $C_S(S, G_1) \neq C_S(S, G_2)$, their exists $f \in \mathcal{F}$ that $f(G_1) \neq f(G_2)$.
\end{definition}

Using the Stone-Weierstrass  theorem, the definition is equivalent to approximating subgraph-counting functions~\cite{chen2020can}. If replacing $C_S$ to $C_I$, then the task will be naturally turned into induced subgraph counting.


\section{Counting Power of Subgraph GNNs}

Subgraph GNNs have been widely used to count substructures~\cite{chen2020can,you2021identity,zhao2022stars, huang2023boosting}. Existing works do not fully answer why it is more suitable to use subgraph GNNs instead of globally expressive models to count substructures. In this section, we provide insight into the question, by showing that subgraph GNNs are nearly as powerful as globally expressive models in terms of counting substructures while running much faster. We show that the distance information within the subgraph is the key to boosting the counting power of GNNs from two perspectives. \textbf{First}, if not adding any distance-related information, the expressiveness of subgraph GNNs will be strongly limited. For example, the $k$-hop rooted subgraph uses the distance information to select subgraphs; if not using the distance to restrict the subgraphs, all subgraphs will be the same (equal to the whole graph), resulting in the same representation power as MPNNs. Similarly, most existing subgraph GNNs use an unsymmetric treatment to rooted node and other nodes in a subgraph. This different treatment is also based on distance (whether $>$ 0). Without it, subgraphs rooted at different nodes will again be the same.
\textbf{Second}, we will theoretically show in Section~\ref{subsec:count} that the distance information effectively enhances the counting power of subgraph GNNs. Such information can be encoded into a structural embedding, providing the basis for our proposed efficient substructure-counting model.


\subsection{Subgraph GNNs}
In this section, we first introduce the globally expressive $k$-WL test, and then introduce MPNNs and subgraph GNNs.
%~\cite{weisfeiler1968reduction}

\textbf{$1$-WL.} We first describe the classic Weisfeiler-Leman algorithm (1-WL). For a given graph $G$, 1-WL aims to compute the coloring for each node in $V$. It computes the node coloring for each node by aggregating the color information from its neighborhood iteratively. In the 0-th iteration, the color for node $v \in V$ is $c_v^0$, denoting its initial isomorphic type\footnote{The term "isomorphic type" is based on previous work~\cite{morris2019weisfeiler}, which gives each $k$-tuple of nodes an initial feature such that two $k$-tuples receive the same initial feature if and only if their induced subgraphs (indexed by node order in the $k$-tuple) are isomorphic.}. For labeled graphs, the isomorphic type of a node is simply its node feature. For unlabeled graphs, we give the same 1 to all nodes. In the $t$-th iteration, the coloring for $v$ is computed as:
\begin{equation}
\label{eq:1WL}
c_v^t = \text{HASH}(c_v^{t-1}, \{\!\!\{c_u^{t-1}:u \in N(v)\}\!\!\})
\end{equation}
where HASH is a bijective hashing function that maps the input to a specific color. The process ends when the colors for all nodes between two iterations are unchanged. If two graphs have different coloring histograms in the end (e.g., different numbers of nodes with the same color), then $1$-WL detects them as non-isomorphic.

\textbf{$k$-WL.} For each $k\geq 2, k \in \mathbb{N}$, the $k$-dimensional Weisfeiler-Leman algorithm ($k$-WL) colors $k$-tuples instead of nodes. In the 0-th iteration, the isomorphic type of a $k$-tuple $\vec{v}$ is given by the hashing of 1) the tuple of colors associated with the nodes of the $\vec{v}$, and 2) the adjacency matrix of the subgraph induced by $\vec{v}$ ordered by the node ordering within $\vec{v}$. %its color in the 0-th iteration, $c_{\vec{v}}^0$, is the initial isomorphic type of $\vec{v}$. 
In the $t$-th iteration, its coloring is updated by: %\jr{Reviewer hXix says it should be $t$ instead of $t-1$ in the colored part of the following formula, which I think is right}
\begin{equation}
\label{eq:kWL1}
c_{\vec{v}}^t = \text{HASH}(c_{\vec{v}}^{t-1}, c_{\vec{v}, (1)}^{\textcolor{violet}{t}},...,c_{\vec{v}, (k)}^{\textcolor{violet}{t}})
\end{equation} 
where 
\begin{equation}
\label{eq:kWL2}
c_{\vec{v}, (i)}^t = \{\!\!\{c_{\vec{u}}^{t-1}|\vec{u} \in N_i(\vec{v})\}\!\!\}, i \in [k]
\end{equation}
Here, $N_i(\vec{v}) = \{(v_1,...,v_{i-1}, w, v_{i+1},...,v_k) | w \in V\}$ is the $i$-th neighborhood of $\vec{v}$. Intuitively, $N_i(\vec{v})$ is obtained by replacing the $i$-th component of $\vec{v}$ by each node from the node set $V$. Besides the updating function, other procedures of $k$-WL are analogous to 1-WL. In terms of distinguishing graphs, 2-WL is as powerful as 1-WL, and for $k\geq 2$, $(k+1)$-WL is strictly more powerful than $k$-WL.

\textbf{MPNNs.} MPNNs are a class of GNNs that learns node representations by iteratively encoding and aggregating messages from neighboring nodes. Let $h_v^t$ be the node representation for $v \in V$ in the $t$-th iteration. It is usually initialized with the node's intrinsic attributes. In the $(t+1)$-th iteration, it is updated by:
\begin{equation}
\label{eq:mpnn}
h_v^{t+1} = W_1^t(h_v^t, \sum_{u \in N(v)}W_2^t(h_u^t, h_v^t, e_{uv}))
\end{equation}
where $W_1^t$ and $W_2^t$ are two learnable functions. MPNN's expressive power in terms of distinguishing non-isomorphic graphs is upper bounded by 2-WL~\cite{xu2018powerful}.

%\textbf{$k$-GNN.} $k$-GNN is a generalization of $k$-WL algorithm. In this paper, we directly follow the 

\textbf{Subgraph GNNs.} Subgraph GNNs first represent the input graph by a collection of subgraphs based on certain subgraph selection policies. They then encode the subgraphs using backbone GNNs and aggregate subgraph representations into the graph representation. We note that there exist some other variants of subgraph GNNs~\cite{qianordered2022, zhao2022stars, frascaunderstanding2022, bevilacqua2022equivariant}, but in this paper, we focus on a specific type of subgraph GNNs without information exchange between subgraphs, which covers reconstruction GNNs~\cite{papp2021dropgnn, cotta2021reconstruction}, ID-GNNs~\cite{you2021identity}, and nested GNNs~\cite{ zhang2021nested,huang2023boosting}. We will show that this type of subgraph GNNs is powerful enough in terms of counting connected substructures.

We call their subgraph selection policy as \textit{rooted subgraph extraction policy}. Existing works select subgraphs rooted at either nodes~\cite{zhang2021nested, you2021identity} or $k$-tuples~\cite{huang2023boosting, qianordered2022}, and typically use a $1$-WL equivalent GNN as the backbone. In this paper, we propose a more general framework with \textit{$m$-WL (or its equivalent GNN) as the backbone on subgraphs rooted at connected $k$-tuples}, i.e., $k$-tuples in which their induced subgraphs are connected. For a connected $k$-tuple $\vec{v}$, the selected subgraph is its rooted subgraph $G_{\vec{v}}=(V_{\vec{v}}, E_{\vec{v}})$.

Denote $c^t_{\vec{v},\vec{u}}$ as the color for an $m$-tuple $\vec{u} = (u_1,...,u_m)$ in $G_{\vec{v}}$ at iteration $t$. It is computed by:%\jr{Similar here}
\begin{equation}
\label{eq:submpnn}
c_{\vec{v}, \vec{u}}^t = \text{HASH}(c_{\vec{v}, \vec{v}}^{t-1}, c_{\vec{v}, \vec{u}}^{t-1}, c_{\vec{v}, \vec{u}, (1)}^{\textcolor{violet}{t}},...,c_{\vec{v}, \vec{u}, (k)}^{\textcolor{violet}{t}})
\end{equation}
where 
\begin{equation}
\label{eq:submpnn2}
c_{\vec{v}, \vec{u}, (i)}^t = \{\!\!\{c_{\vec{v},\vec{q}}^{t-1}|\vec{q} \in N_{\vec{v},i}(\vec{u})\}\!\!\}, i \in [m]
\end{equation}
Here $N_{\vec{v}, i}(\vec{u}) = \{(u_1,...,u_{i-1},w,u_{i+1},...,u_m)| w\in V_{\vec{v}}\}$ denotes the $i$-th neighborhood of $\vec{u}$ in $G_{\vec{v}}$.

%As for the backbone GNN, most existing works use MPNNs as the backbone GNNs. Actually, there can be various potential choices, e.g., PPGN~\cite{maron2019provably}, k-GNN~\cite{morris2019weisfeiler}, and so on. Here we use MPNN as the backbone GNN for illustration. Denote $h_{\vec{v}, u}^t$ as the representation of node $u$ in $G_{\vec{v}}$ at iteration $t$. Similar to normal MPNNs, $h_{\vec{v}, u}^0$ is usually initialized as the intrinsic node attribute. In the $(t+1)$-th iteration, it is updated by:
%\begin{equation}
%\label{eq:submpnn}
%h_{\vec{v},u}^{t+1}=W_1^t(h_{\vec{v},u}^t, \sum_{w \in N_{\vec{v}}(u)} W_2^t(h_{\vec{v}, v_1}^t,...,h_{\vec{v}, v_k}^t,h_{\vec{v}, w},e_{uw}))
%\end{equation}
%where $N_{\vec{v}}(u)$ denotes the 1-hop neighborhood of node $u$ in $G_{\vec{v}}$. Other symbols are similar to Equation~\ref{eq:mpnn}. 


Denote the final iteration as iteration $T$. The color of $\vec{v}$ after the final iteration will be the combination of all $m$-tuples' colors inside $G_{\vec{v}}$. Formally, 
\begin{equation}
c_{\vec{v}} = Readout(\{\!\!\{c_{\vec{v},\vec{u}}^T|\vec{u} \in (V_{\vec{v}})^m\}\!\!\})
\end{equation}
where $Readout$ is a readout function, e.g., the sum function. Intuitively, compared with GNNs, subgraph GNNs (1) update the representation using not only the neighboring information but also the information from the rooted nodes; (2) the neighborhood is restricted to the subgraph level.


\subsection{Counting Power of Subgraph GNNs}
\label{subsec:count}
Subgraph GNNs have long been used to count substructures. Existing works mainly focus on counting certain types of substructures, e.g., walks~\cite{you2021identity} and cycles~\cite{huang2023boosting} and do not relate subgraph GNNs with substructure counting in a holistic perspective. In this section, we provide insights into this question by showing that subgraph GNNs are nearly as powerful as globally expressive models, e.g., high-dimensional WL, in terms of counting connected substructures while running much faster. We first theoretically characterize $m$-WL's power for counting \textbf{any} substructures.

\textbf{Counting power of $m$-WL.} Different substructures with no more than $m$ nodes have different initial isomorphic types. We can assign each isomorphic type a unique color, and define the color histogram of the graph as the output function. Then we can easily come up with a lower bound of the counting power of $m$-WL:
\begin{remark}
\label{rm:wl}
$m$-WL ($m \geq 2$) can count all connected substructures with no more than $m$ nodes.
\end{remark}

%As for the upper bound, MPNNs (or 2-WL) cannot count 3-cliques (i.e., cliques with 3 nodes). In addition, there exist 4-cliques in the 4*4 Rook Graph, while there are none in the Shrikhande Graph. Since 3-WL cannot distinguish the pair of graphs~\cite{arvind2020weisfeiler}, they cannot count 4-cliques. 

%Therefore, we can provide a upper bound for the counting power of $m$-WL: %\zy{The conclusion here is a little bit weak, can it be extended to a more strict one?, e.g., $m$-WL cannot count $m+1$-cliques. }\jr{An affirmative result has been established. See Theorem \ref{th:disab}.}
As for the upper bound, we show that for any $m \geq 2$, there exists a type of connected substructure with $m + 1$ nodes that $m$-WL cannot count. The theorem is formally stated below, and the proof is provided in the appendix.

%Therefore, we can provide a upper bound for the counting power of $m$-WL
\begin{theorem}
    \label{th:disab}
	For any $m\geq 2$, there exists a pair of graphs $G$ and $H$, such that $G$ contains an $(m+1)$-clique as its subgraph while $H$ does not, and that $m$-WL cannot distinguish $G$ from $H$.
\end{theorem}

Theorem~\ref{th:disab} makes the lower bound in Remark~\ref{rm:wl} tight.

%Therefore, 
%\begin{remark}
%For all $m \geq 2$ $(m \in \mathbb{N})$, there exist connected substructures with $m+1$ nodes that $m$-WL cannot count.
%\end{remark}

\textbf{Decomposition of counting connected substructures.} Before discussing the counting power of subgraph GNNs, we first provide the basis for the discussion: any graph-level substructure counting can be naturally decomposed into a collection of local substructure counting. For example, to count 3-cliques/3-cycles in a given graph, we can first compute the number of 3-cycles that pass each node, sum the number among all nodes, and then divide the number by 3 to compute the graph-level result. We can extend the observation to a more general version:

\begin{remark}
\label{rm:de}
To count a certain type of connected substructures with no more than $m+k$ ($m \geq 2, k > 0$) nodes in a given graph, we can decompose it into counting over $k$-tuples. First, select a specific type of connected $k$-tuple whose induced subgraph is a subgraph of the target substructure. Then count the substructures that pass each $k$-tuple. Finally, the result is the sum of the numbers over all $k$-tuples divided by a constant dependent on the substructure.
\end{remark} 

\textbf{Counting power of subgraph GNNs.} We first give the lower bound of the counting power of subgraph GNNs.

\begin{theorem}
\label{th:sub}
For any connected substructure with no more than $m+k$ ($m\geq 2, k > 0$) nodes, there exists a subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL that can count it. 
\end{theorem}

%We provide a sketch of proof here, leaving the whole proof in the appendix.

\textit{Proof sketch.} Based on Remark~\ref{rm:de}, there exists a type of connected $k$-tuple that satisfy the decomposition of the connected substructure. Then the substructure can be separated into 2 subgraphs: the nodes that belong to the $k$-tuple (we call them rooted nodes) and the nodes that do not belong to the $k$-tuple (we call them non-rooted nodes). The subgraph GNNs can distinguish between the subgraphs formed only by rooted nodes and those formed only by non-rooted nodes. Also, the distance information between the rooted nodes and the non-rooted nodes directly reflects the connection between these two parts. Therefore, if the subgraphs formed by the edges between these two parts are non-isomorphic, the subgraph GNNs can differentiate them, and thus can count the substructure.

Based on the proof and the insights gained from popular subgraph GNNs, we can safely conclude that \textit{the distance from the rooted nodes to nodes within the subgraph provides valuable information for substructure counting}. As for the upper bound of the counting power of subgraph GNNs, existing works~\cite{geerts2020expressive, frascaunderstanding2022} show that for $m \geq 2$, (1) $m$-WL is as powerful as a specific GNN, called $m$-IGN~\cite{maron2018invariant}; (2) a subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL can be implemented by $(m+k)$-IGN. Therefore, we have:

\begin{proposition}
\label{pro:sub}
Any subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL ($m \geq$ 2) is not more powerful than $(m+k)$-WL.
\end{proposition}
Combining with Theorem~\ref{th:disab}, we obtain a tight characterization of subgraph GNNs' counting power for any substructures, which is the same as $(m+k)$-WL.
This suggests that subgraph GNNs are nearly as powerful as globally expressive models in terms of graph-level general substructure counting. Note that globally expressive models might still be more powerful at counting specific substructures.

\textbf{Efficiency of subgraph GNNs.}  The computational cost for GNNs as powerful as $(m+k)$-WL is $O(|V|^{m+k})$, where $|V|$ denotes the number of nodes in the input graph. However, for subgraph GNNs rooted at $k$-tuples with backbone GNN as powerful as $m$-WL, the computational cost can be $O(|V|^{k}|V_s|^{m})$, where $V_s$ is the largest number of nodes among all subgraphs. Here we point out that $|V_s|$ is usually much smaller than $|V|$. For example, when counting cliques and stars, the hop parameter can be set to 1; when counting cycles and paths, the hop parameter can be set to $m/2$. Therefore subgraph GNNs are much more efficient. In conclusion, subgraph GNNs rooted at $k$-tuples with backbone GNN as powerful as $m$-WL can reach a similar counting power to $(m+k)$-WL while being much more efficient. This can be a key motivation for the use of subgraph GNNs in counting substructures.

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.8\columnwidth]{figure/ESCGNN.pdf}}
\vskip -0.1in
\caption{Framework of ESC-GNN. The rooted 2-tuples are colored red in the subgraph extraction procedure.}
\label{fig:ESC-GNN}
\end{center}
\vskip -0.4in
\end{figure*}

\section{Efficient Substructure Counting GNN (ESC-GNN)}
\label{sec:ESC-GNN}
%In the previous section, we show that subgraph GNNs are comparably efficient since they focus on local representation power which is enough for counting substructures. 
Despite the strong substructure counting ability, subgraph GNNs are still much slower than MPNNs since they need to run backbone GNNs on all subgraphs. In this section, we propose a model, Efficient Substructure Counting GNN (ESC-GNN), which can count substructures efficiently and effectively, while, more importantly, does not need to run GNN on subgraphs.
ESC-GNN encodes the distance information within subgraphs into structural embeddings of edges in a preprocessing step. After that, it only needs to \textbf{run the backbone GNN on the input graph once rather than over all subgraphs}. We first introduce ESC-GNN, and then show its representation power theoretically. The code is available at %\href{http://www.overleaf.com}
\url{https://github.com/pkuyzy/ESC-GNN}.

\subsection{Framework of ESC-GNN}
\label{subsec:frame}
\textbf{Basic framework.} In this section, we first introduce the framework of ESC-GNN, which is illustrated in Figure~\ref{fig:ESC-GNN}. In ESC-GNN, we adopt subgraphs rooted at 2-tuples, and use MPNN as the backbone GNN. Subgraph GNNs rooted at 2-tuples are more expressive than those rooted at nodes, but at the cost of even higher computational cost~\cite{huang2023boosting}. Thus, instead of running backbone GNN over subgraphs to extract subgraph features (like distances), we directly encode them into some carefully designed structural embeddings, which are used as additional edge features of the input graph. An MPNN is then applied to this augmented graph. %The framework can be viewed as an MPNN with an augmented structural encoding on edges. It can be naturally extended to $k$-tuples with more powerful backbone GNNs.
%It is usually initialized with the node's intrinsic attributes. 
Specifically, let $h_v^t$ be the node representation for $v\in V$ in the $t$-th iteration. The update function is given by:
\begin{equation}
\label{eq:ESC-GNN}
h_v^{t+1} = W_1^t(h_v^t, \sum_{u \in N(v)}W_2^t(h_u^t, h_v^t, e_{uv}, s_{uv}))
\end{equation}
where $s_{uv}$ is the structural embedding for edge $uv$. %The choice of $s_{uv}$ is discussed in the following paragraphs.

%$W_1^t$ and $W_2^t$ are learnable functions, $e_{uv}$ is the edge representation,

%\textbf{Optimal counting power of ESC-GNN.} 
%\begin{remark}
%For any connected substructures with at most $m+k$ ($m \geq 2, k > 0$) nodes, there exist a type of structural encoding rooted at $k$-tuples that encodes the number of such substructures.
%\end{remark}
%For example, to count 4-cliques in a subgraph $G_v = (V_v, E_v)$ rooted at node $v$, define the label of each node within the subgraph as its shortest  path distance to $v$: $\forall u \in V_v, f(u) = d(u,v)$. If we define the structural embedding as the number of 3-cycles with all nodes labeled ``1", then it can encodes the number of 4-cliques within the subgraph. Therefore,
%\begin{remark}
%For any connected substructures with at most $m+k$ ($m \geq 2, k > 0$) nodes, there exist a type of ESC-GNN rooted at $k$-tuples that can count it.
%\end{remark}


%\zy{cancel the remark}
\textbf{Choice of structural embedding.} Recall Theorem~\ref{th:sub} implies that the distance information within subgraphs is  key to boosting the counting power of subgraph GNNs. Therefore, we encode such distance information, as well as some degree information, as follows. An example is shown in Figure~\ref{fig:ESC-GNN}. 
%The isomorphic type of nodes also provides valuable information. 
%Based on Theorem~\ref{th:sub}, we propose a structural embedding that (1) is easy to extract; (2) contains valuable information for substructure counting.
%\zy{distance encoding is enough to distinguish almost all regular graphs?}
\begin{itemize}
\item The degree encoding: for each subgraph, we first compute the degree of all nodes within the subgraph, and then use the degree histogram as the encoding. For example, the degree histogram for the first subgraph (the subgraph rooted at edge $v_1v_3$) of Figure~\ref{fig:ESC-GNN} is $(0,0,4,0)$, since there are 4 nodes with degree 2 in the subgraph.

\item The node-level distance encoding: for the subgraph rooted at edge $uv$, we use the shortest path distance histograms to the rooted nodes as the distance encoding. Take the first subgraph of Figure~\ref{fig:ESC-GNN} as an example. The distance histogram for $v_1$ is $(1,2,1,0)$, since there are one node with distance $0$ ($v_1$), two nodes with distance $1$ ($v_2$ and $v_3$), and one node with distance $2$ ($v_4$) to node $v_1$. The same holds for the other rooted node $v_3$.

\item The edge-level distance encoding: for the subgraph rooted at edge $uv$, define the label for each node as its shortest path distances to all the rooted nodes, i.e., $\forall u_1, f(u_1) = (d(u_1, u), d(u_1,v))$. Then we can define the label of edges in the subgraph as the concatenation of the label of its two end nodes, e.g., for edge $u_1v_1$, its label is $f(u_1v_1) = (f(u_1),f(v_1))$. We then use the edge-level distance histogram as the distance encoding. For example, for the subgraphs shown in Figure~\ref{fig:ESC-GNN}, there are seven types of edges: (0,1,1,0), (0,1,1,1), (0,1,1,2), (1,0,1,1), (1,0,2,1), (1,2,2,1), (2,1,2,1). Therefore the edge-level distance encoding for the first subgraph is (1,0,1,0,1,1,0), since there are one edge ($v_1v_3$) with label (0,1,1,0), one edge ($v_1v_2$) with label (0,1,1,2), one edge ($v_3v_4$) with label (1,0,2,1), and one edge $(v_2v_4)$ with label (1,2,2,1).
\end{itemize}

Finally, we concatenate the three encodings to get the final structural embedding $s_{uv}$. 

\textbf{Analysis on the structural embedding.} In terms of representation power, since all these distance encodings can be extracted using an MPNN within the subgraph, we first conclude that:

\begin{proposition}
\label{rm:esc}
ESC-GNN is not more powerful than subgraph GNNs rooted at edges with MPNN as backbone GNN.
\end{proposition}

In terms of algorithm efficiency, we can precompute these structural embeddings during subgraph extraction (the preprocessing cost can be amortized into each epoch/iteration), and ESC-GNN only needs to run the backbone GNN on the input graph. Therefore in every iteration, its computational cost is only $O(|E|)$, and its memory cost is $O(|V|)$, both the same as MPNN. As for subgraph MPNNs rooted at edges, they need to run the process of subgraph extraction and run the backbone GNN over all subgraphs. In every iteration, their computational cost is $O(|E||E_s'|)$, and their memory cost is $O(|E||V_s'|)$, where $|V_s'|$ and $|E_s'|$ are the average numbers of nodes and edges among all subgraphs. Even if considering subgraph MPNNs rooted at nodes, in every iteration, their computational cost is $O(|V||E_s|)$, and their memory cost is $O(|V||V_s|)$. Note that $|V||E_s| >> |V|D / 2 = |E|$, where $D$ is the average node degree. Therefore we can safely conclude that ESC-GNN is more efficient than subgraph GNNs. We will empirically evaluate its efficiency in the experiment.




\subsection{Representation Power of ESC-GNN}
\label{subsec:escgnn}
In this section, we analyze the representation power of ESC-GNN from two perspectives: its counting power and its ability to distinguish non-isomorphic graphs.

\textbf{Counting power of ESC-GNN.} Existing works~\cite{you2021identity,huang2023boosting} mainly focus on subgraph counting. In this section, we provide results on both subgraph counting and induced subgraph counting. We use four types of widely used substructures: cycles, cliques, stars, and paths, as examples to show the counting power of ESC-GNN. The proof is provided in the appendix.

\begin{theorem}
\label{th:count}
In terms of subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) stars with arbitrary sizes; (4) up to 3-paths.
\end{theorem}

\begin{theorem}
\label{th:count_ind}
In terms of induced subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) up to 4-stars; (4) up to 3-paths.
\end{theorem}

\textbf{Compared with subgraph GNNs.} As shown in Proposition~\ref{rm:esc}, ESC-GNN is less powerful than subgraph MPNNs rooted at 2-tuples~\cite{huang2023boosting}. As for subgraph MPNNs rooted at nodes~\cite{zhang2021nested,you2021identity}, they can only count up to 4-cycles, 3-cliques, and 3-paths~\cite{huang2023boosting}. Therefore, ESC-GNN is more powerful than subgraph MPNNs rooted at nodes in terms of counting these substructures.

\textbf{The ability to distinguish non-isomorphic types.} 

\begin{theorem}
\label{th:iso}
ESC-GNN is strictly more powerful than 2-WL, while not less expressive than 3-WL.
\end{theorem}

\textbf{Compared with subgraph GNNs.} In terms of distinguishing non-isomorphic graphs, subgraph MPNNs rooted at nodes~\cite{zhang2021nested,you2021identity} are strictly less powerful than 3-WL~\cite{frascaunderstanding2022}, while ESC-GNN is not less expressive than 3-WL. %We will empirically show its expressiveness in the experiment.

In addition, although able to distinguish most pairs of non-isomorphic graphs~\cite{babai1980random}, 2-WL fails to distinguish any pairs of non-isomorphic $r$-regular graphs with equal size. In this paper, we prove that ESC-GNN can distinguish almost all $r$-regular graphs:

\begin{theorem}
\label{th:reg}
Consider all pairs of $r$-regular graphs with $n$ nodes, let $3\leq r < (2log2n)^{1/2}$ and $\epsilon$ be a fixed constant. With the hop parameter $h$ set to $\lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$, there exists an ESC-GNN that can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
\end{theorem}

% The theorem suggests that ESC-GNN is powerful enough to differentiate almost all pairs of regular graphs.



%We provide a sketch of proof here, leaving the whole proof in the appendix. 

%\textit{Proof Sketch.} As shown in Theorem~\ref{th:count}, ESC-GNN is able to count 4-cliques, thus able to distinguish the pair of graphs called the 4*4 Rook Graph and the Shrikhande Graph. Considering that 2-WL and 3-WL cannot differentiate them~\cite{arvind2020weisfeiler},  ESC-GNN is not less powerful than 3-WL. Also, given any pair of graphs that 2-WL can distinguish, ESC-GNN can distinguish them using the backbone GNNs~\cite{xu2018powerful}. Therefore ESC-GNN is strictly more powerful than 2-WL.

\begin{table}
	\centering
	\caption{Test Accuracy on EXP/SR25/CSL} 
     \label{tab:exp}
	\scalebox{0.9}{
    \begin{tabular}{lcccccc}
	\hline\noalign{\smallskip}
	Dataset & EXP & SR25 & CSL \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	MPNN & 50 & 6.67 & 10 \\
	NGNN  & 100 & 6.67 & - \\
	GIN-AK+ & 100 & 6.67 & -\\
	PPGN & 100 & 6.67 & -\\
	3-GNN & 99.7 & 6.67 & 95.7 \\
	I$^2$-GNN & 100 & 100 & 100 \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	ESC-GNN & 100 & 100 & 100 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
	\end{tabular}}
 \vskip -0.2in
\end{table}

\section{Experiment}

To thoroughly analyze the property of ESC-GNN, we evaluate it from the following perspectives: (1) we evaluate its representation power in Section~\ref{subsec:rep}, to show whether it can reach the representation power as shown in Section~\ref{subsec:escgnn}; (2) we evaluate its performance on real-world benchmarks in Section~\ref{subsec:real}, to show whether the increased representation power can boost its performance on real-world tasks; (3) we evaluate its efficiency in Section~\ref{subsec:eff}.% to show that ESC-GNN is much faster while not much less expressive than subgraph GNNs.

\textbf{Baselines.} We compare with various baseline methods including (1) a basic MPNN model~\cite{xu2018powerful, KipfW17}; (2) subgraph GNNs including NGNN~\cite{zhang2021nested}, IDGNN~\cite{you2021identity}, GIN-AK+~\cite{zhao2022stars}, SUN~\cite{frascaunderstanding2022}, DSS-GNN~\cite{bevilacqua2022equivariant}, OSAN~\cite{qianordered2022}, and I$^2$-GNN~\cite{huang2023boosting}; (3) high-order GNN models including 1-2-3-GNN~\cite{morris2019weisfeiler} and PPGN~\cite{maron2019provably}.

\begin{table*}
	\centering
	\caption{Evaluation on Counting Substructures (norm MAE), cells with MAE less than 0.01 are colored yellow.} 
    \label{tab:count}
	\scalebox{0.9}{
	\begin{tabular}{lccccccccc}
	\hline\noalign{\smallskip}
	Dataset & Tailed Triangle & Chordal Cycle & 4-Clique & 4-Path & Triangle-Rectangle & 3-cycles & 4-cycles & 5-cycles & 6-cycles \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	MPNN & 0.3631 & 0.3114 & 0.1645 & 0.1592 & 0.3018 &  0.3515 & 0.2742 & 0.2088 & 0.1555\\
	ID-GNN & 0.1053 & 0.0454 & \cellcolor{yellow}0.0026 & 0.0273 & 0.0564 & \cellcolor{yellow}0.0006 & \cellcolor{yellow}0.0022 & 0.0490 & 0.0495 \\
	NGNN & 0.1044 & 0.0392 & \cellcolor{yellow}0.0045 & 0.0244 & 0.0519 & \cellcolor{yellow}0.0003 & \cellcolor{yellow}0.0013 & 0.0402 & 0.0439\\
	GIN-AK+ & \cellcolor{yellow}0.0043 & 0.0112 &\cellcolor{yellow} 0.0049 & \cellcolor{yellow}0.0075 & 0.0317 & \cellcolor{yellow}0.0004 & \cellcolor{yellow}0.0041 & 0.0133 & 0.0238\\
	PPGN & \cellcolor{yellow}0.0026 & \cellcolor{yellow}0.0015 & 0.1646 & \cellcolor{yellow}0.0041 & \cellcolor{yellow}0.0081 &\cellcolor{yellow}0.0005 &\cellcolor{yellow}0.0013 & \cellcolor{yellow}0.0044& \cellcolor{yellow}0.0079\\ 
	I$^2$-GNN & \cellcolor{yellow}0.0011 & \cellcolor{yellow}0.0010 & \cellcolor{yellow}0.0003 & \cellcolor{yellow}0.0041 &\cellcolor{yellow} 0.0026 &\cellcolor{yellow} 0.0003 & \cellcolor{yellow}0.0016 & \cellcolor{yellow}0.0028 & \cellcolor{yellow}0.0082\\ 
	\noalign{\smallskip}\hline\noalign{\smallskip}
	ESC-GNN & \cellcolor{yellow}0.0052 & 0.0169 & \cellcolor{yellow}0.0064 & 0.0254 & 0.0748& \cellcolor{yellow}0.0074 &\cellcolor{yellow}0.0096 & 0.0356& 0.0578\\
	\hline\noalign{\smallskip}
    \end{tabular}}
    \vskip -0.2in
\end{table*}

\subsection{Representation Power of ESC-GNN}
\label{subsec:rep}

\textbf{Datasets.} We evaluate the representation power of ESC-GNN from two perspectives: 
\begin{itemize}
\item Its ability to differentiate non-isomorphic graphs. We use (1) EXP~\cite{abboud2021surprising}, which contains 600 pairs of non-isomorphic graphs that 1-WL/2-WL fails to distinguish; (2) SR25~\cite{balcilar2021breaking}, which contains 150 pairs of non-isomorphic strongly regular graphs that cannot be differentiated by 3-WL; (3) CSL~\cite{murphy2019relational}, which contains 150 regular graphs that 1-WL/2-WL fails to distinguish. These graphs are classified into 10 isomorphism classes. Classification accuracy is adopted as the evaluation metric.
\item Its counting ability. We %evaluate its ability to count various substructures 
use the synthetic dataset from~\cite{zhao2022stars}. The task is to predict the number of substructures that pass each node in the given graph. The Mean Absolute Error (MAE) is adopted as the evaluation metric.
\end{itemize}


\textbf{Results.} In Table~\ref{tab:exp}, ESC-GNN achieves 100\% accuracy on all datasets. Considering that models as powerful as 3-WL (PPGN and 3-GNN) fail the SR25 dataset, the results serve as empirical evidence that ESC-GNN can effectively differentiate regular graphs (Theorem~\ref{th:reg}), and not less powerful than 3-WL (Theorem~\ref{th:iso}). In Table~\ref{tab:count}, ESC-GNN reaches less-than-0.01 MAE in terms of counting tailed triangles, 4-cliques, 3-cycles, and 4-cycles, serving as the empirical evidence for Theorem~\ref{th:count}. Generally speaking, ESC-GNN performs much better than MPNNs, and slightly beats or performs comparably with node-based subgraph GNNs such as ID-GNN, NGNN and GIN-AK+. Also, it performs inferior to subgraph GNNs rooted at edges (I$^2$-GNN). This serves as the empirical evidence for Proposition~\ref{rm:esc}. 


\begin{table}
\centering
\caption{Evaluation on QM9 (MAE)} 
\label{tab:qm9}
\scalebox{0.85}{
\begin{tabular}{lcccccccc}
\hline\noalign{\smallskip}
Dataset & 1-GNN & 1-2-3-GNN & NGNN & I$^2$-GNN & ESC-GNN \\
\noalign{\smallskip}\hline\noalign{\smallskip}
$\mu$ & 0.493 & 0.476 & 0.428 & 0.428 & \textbf{0.231}\\
$\alpha$ & 0.78 & 0.27 & 0.29 & \textbf{0.230} & 0.265\\
$\epsilon_{\text{homo}}$  &0.00321 &0.00337& 0.00265 &0.00261& \textbf{0.00221}\\
$\epsilon_{\text{lumo}}$  &0.00355& 0.00351  &0.00297& 0.00267&\textbf{0.00204}\\
$\Delta_{\epsilon}$ & 0.0049 &0.0048 & 0.0038& 0.0038&\textbf{0.0032}\\
$R^2$ & 34.1& 22.9 &20.5 &18.64&\textbf{7.28}\\
ZPVE & 0.00124& 0.00019& 0.0002 &\textbf{0.00014}&0.00033\\
$U_0$ & 2.32& 0.0427 &0.295& \textbf{0.211}&0.645\\
$U$ & 2.08& \textbf{0.111}& 0.361 &0.206&0.380\\
$H$ & 2.23 &\textbf{0.0419} &0.305 &0.269&0.427\\
$G$ & 1.94 &\textbf{0.0469} & 0.489 &0.261&0.384\\
$C_v$ &0.27& 0.0944& 0.174 &\textbf{0.0730} &0.105\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}}
\vskip -0.2in
\end{table}

\subsection{Real World Tasks.}

We present the results on QM9~\cite{ramakrishnan2014quantum, wu2018moleculenet} in Table~\ref{tab:qm9}, and refer the readers to Section~\ref{subsec:real} in the appendix for additional real-world experiments. QM9 contains 130k small molecules, and the task is to perform regression on twelve graph properties. 

Generally speaking, on all real-world datasets, ESC-GNN performs much better than classic MPNNs, slightly better or comparably with subgraph GNNs rooted at nodes, and worse than subgraph GNNs rooted at 2-tuples. However, we are surprised to find that in certain situations (e.g., shown in Table~\ref{tab:qm9}), we beat subgraph GNNs rooted at 2-tuples. This may be due to the fact that the framework of ESC-GNN is simple enough to avoid problems such as overfitting. We also observe that our performance on $U_0$, $U$, and $H$ in Table~\ref{tab:qm9} is the second-worst. These targets represent the Internal energy at 0K, the Internal energy at 298.15K, and the Enthalpy at 298.15K, respectively. To calculate these targets, computational methods are typically used, taking into account the interactions between all the atoms in the molecule and their surroundings, including any heat or work exchanged with the environment. As a result, globally expressive models (1-2-3 GNN) can achieve the best performance, while subgraph GNNs, which utilize local information to enhance graph representation, perform much worse.


  
\subsection{Algorithm Efficiency.}
\label{subsec:eff}

In terms of algorithm efficiency, we compare ESC-GNN with three baselines: an MPNN, NGNN whose subgraphs are rooted at node, and I$^2$-GNN whose subgraphs are rooted at 2-tuples. We report the data preprocessing time and the standard running time (100 epochs for ogbg-hiv, and 1000 epochs for ZINC) in Table~\ref{tab:eff}. As shown in the table, ESC-GNN is nearly as efficient as MPNN, while running much faster than NGNN and I$^2$-GNN. This is consistent with our observation in Section~\ref{subsec:frame}.

\begin{table}
\centering
\caption{Evaluation on Algorithm Efficiency.} 
\label{tab:eff}
\scalebox{0.9}{
\begin{tabular}{|l|cc|cc|}
\hline\noalign{\smallskip}
Dataset & \multicolumn{2}{c|}{ogbg-hiv} & \multicolumn{2}{c|}{ZINC} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Model & Pre & Run & Pre & Run \\
\noalign{\smallskip}\hline\noalign{\smallskip}
MPNN & 2.7 & 6296.8 &6.2& 1945.0\\
NGNN & 1288.0 & 14862.9 & 300.3 & 8368.8 \\
I$^2$-GNN & 2806.5& 1042963.7  & 677.7 &  18607.5 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
ESC-GNN & 3586.0 & 6301.0 & 761.0 & 2872.2\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}}
\vskip -0.15in
\end{table}

\section{Conclusion}
%Subgraph GNNs have been used to count substructures. However, they suffer from high computational costs since they need to run backbone GNNs among all subgraphs. To efficiently count substructures with GNNs, we first show that the distance information within subgraphs is the key to boosting the counting power of subgraph GNNs. We then encode such information into a structural embedding, and enhance GNN models with such embedding. In this way, the proposed model does not need to learn representations over all subgraphs, and can be much more efficient. Experiments on various benchmarks show that the proposed model preserves the representation power of subgraph GNNs while performing much faster.

The huge computational cost is associated with subgraph GNNs due to the requirement of running backbone GNNs among all subgraphs. To address this challenge and enable efficient substructure counting with GNNs, we theoretically show that the distance information within subgraphs is key to boosting the counting power of GNNs. We then encode this information into a structural embedding and enhance standard GNN models with this embedding, eliminating the need to learn representations over all subgraphs. Experiments on various benchmarks demonstrate that the proposed model retains the representation power of subgraph GNNs while running much faster. It can potentially enhance the utility of subgraph GNNs in a variety of applications that require efficient substructure counting.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

%\clearpage
\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix.}

In the appendix, we provide (1) the proof of Theorem~\ref{th:disab} (the upper bound of $m$-WL in terms of counting substructures); (2) the proof of Theorem~\ref{th:sub} (the lower bound of subgraph GNN in terms of counting connected substructures); (3) the proof of Theorem~\ref{th:iso} (ESC-GNN's ability to differentiate non-isomorphic graphs); (4) the proof of Theorem~\ref{th:count} and Theorem~\ref{th:count_ind} (ESC-GNN's ability to count substructures); (5) the proof of Theorem~\ref{th:reg} (ESC-GNN's ability to differentiate regular graphs); (6) experimental details; (7) additional experiments on real-world benchmarks and ablation study.

\subsection{The proof of Theorem~\ref{th:disab}}

In this part, we prove that $k$-WL can't count all connected substructures with $(k+1)$ nodes (specifically, $(k+1)$-cliques). We restate the result as follows: 

%\begin{theorem}
%	For any $k\geq 2$, there exists a pair of graphs $G$ and $H$, such that $G$ contains a $(k+1)$-clique as its subgraph while $H$ does not, and that $k$-WL can't distinguish $G$ from $H$.
%\end{theorem}

\begingroup
\def\thetheorem{\ref{th:disab}}
\begin{theorem}
For any $k\geq 2$, there exists a pair of graphs $G$ and $H$, such that $G$ contains a $(k+1)$-clique as its subgraph while $H$ does not, and that $k$-WL can't distinguish $G$ from $H$.
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup

\begin{proof}
	The counter-example is inspired by the well-known Cai-F\"urer-Immerman (CFI) graphs \cite{cai1992optimal}. We define a sequence of graphs $G_k^{(\ell)}, \ell = 0, 1, \ldots, k+1$ as following,
	\begin{equation}
		\begin{split}
			V_{G_k^{(\ell)}} =& \Big\{u_{a, \vec{v}}\Big| a\in[k+1], \vec{v}\in\{0,1\}^k \text{ and }\\
			&\quad\begin{array}{ll}
				\vec{v} \text{ contains an even number of } 1 \text{'s}, & \text{if }a=1,2,\ldots, k-\ell+1, \\
				\vec{v} \text{ contains an odd number of } 1 \text{'s},  & \text{if }a=k-\ell+2,\ldots, k+1.
			\end{array}\Big\}
		\end{split}
	\end{equation}
	Two nodes $u_{a,\vec{v}}$ and $u_{a',\vec{v}'}$ of $G_k^{(\ell)}$ are connected iff there exists $m\in [k]$ such that $a' \mod (k+1) = (a+m) \mod (k+1)$ and $v_m = v'_{k-m+1}$. We have the following lemma.

	\begin{lemma}
		\emph{(a)} For each $\ell = 0, 1, \ldots, k+1$, $G_k^{(\ell)}$ is an undirected graph with $(k+1)2^{k-1}$ nodes;

		\emph{(b)} The set of graphs $G_k^{(\ell)}$ with an odd $\ell$ are mutually isomorphic; similarly, the set of graphs $G_k^{(\ell)}$ with an even $\ell$ are mutually isomorphic.
	\end{lemma}

	It's easy to verify (a). To prove (b), it suffices to prove $G_k^{(\ell)}$ is isomorphic to $G_k^{(\ell+2)}$ for all $\ell = 0,1,\ldots, k-1$. We apply a \emph{renaming} to the nodes of $G_k^{(\ell)}$: we flip the $1^\mathrm{st}$ bit of $\vec{v}$ in every node named $u_{k-\ell, \vec{v}}$, and flip the $k^\mathrm{th}$ bit of $\vec{v}$ in every node named $u_{k-\ell+1, \vec{v}}$. Since this is a mere renaming of nodes, the resulting graph is isomorphic to $G_k^{(\ell)}$. However, it's also easy to see that the resulting graph follows the construction of $G_k^{(\ell+2)}$. Therefore, we assert that $G_k^{(\ell)}$ must be isomorphic to $G_k^{(\ell+2)}$.

	Now, let's ask $G=G_k^{(0)}$ and $H=G_k^{(1)}$. Obviously there is a $(k+1)$-clique in $G$: nodes $u_{j,0^k}, j=1,2, \ldots, k+1$ are mutually adjacent by definition of $G_k^{(0)}$. On the contrary, we have
	\begin{lemma}
		There's no $(k+1)$-clique in $H$.
	\end{lemma}

	The proof is given below. Assume there is a $(k+1)$-clique in $H$. Since there's no edge between nodes $u_{a, \vec{v}}$ with an identical $a$, the $(k+1)$-clique must contain exactly one node from every node set $\{u_{a,\vec{v}}\}$ for each fixed $a\in[k+1]$. We further assume that the $(k+1)$ nodes are $u_{a, b_{a1}b_{a2}\ldots b_{ak}}, a=1,2,\ldots, k+1$. Using the condition for adjacency, we have
	\begin{align}
		       & b_{2k} = b_{11},                                                               \\
		       & b_{3k} = b_{21}, b_{3(k-1)} = b_{12},                                          \\
		       & b_{4k} = b_{31}, b_{4(k-1)} = b_{22}, b_{4(k-2)} = b_{13},                     \\
		\notag & \cdots\cdots\cdots\cdots                                                       \\
		       & b_{(k+1)k} = b_{k1}, b_{(k+1)(k-1)} = b_{(k-1)2}, \ldots, b_{(k+1)1} = b_{1k}.
	\end{align}
	Applying the above identities to the summation
	\begin{align}
		\label{contradiction}
		\sum_{a=1}^{k+1}\sum_{j=1}^k b_{aj} = 2\sum_{j=1}^k\left(b_{1j}+b_{2j}+\cdots+b_{(k-j+1)j}\right),
	\end{align}
	we see that it should be even. However, by definition of $G_k^{(1)}$, there are an even number of $1$'s in $b_{a1}b_{a2}\ldots b_{ak}$ when $a\in [k]$, and an odd number of $1$'s when $a=k+1$. Therefore, the sum in \eqref{contradiction} should be odd. This leads to a contradiction.

	Finally, to prove the $k$-WL equivalence of $G$ and $H$, we have
	\begin{lemma}
		$k$-WL can't distinguish $G$ and $H$.
	\end{lemma}

	By virtue of the equivalence between $k$-WL and pebble games~\cite{grohe_otto_2015}, it suffices to prove that Player II will win the $\mathcal{C}_{k}$ bijective pebble game on $G$ and $H$. We state the winning strategy for Player II as following. Since $G$ and $H$ are isomorphic with nodes $\{u_{k+1,*}\}$ deleted, Player II can always choose an isomorphism $f:G-\{u_{k+1,*}\}\rightarrow H-\{u_{k+1,*}\}$ to survive if Player I never places a pebble on nodes $u_{k+1,*}$. Furthermore, since $k$ pebbles can occupy nodes with at most $k$ different values of $a$ (in $u_{a,\vec{v}}$), there's always a set of pebbleless nodes $\{u_{a_0,\vec{v}}\}$ with some $a_0\in[k+1]$. Therefore, Player II only needs to do proper renaming on $H$ between $u_{k+1,*}$ and $u_{a_0,*}$ as stated above. This makes every $\vec{v}$ in $u_{a_0,\vec{v}}$ have an odd number of $1$'s. Player II then chooses the isomorphism on $G-\{u_{a_0,*}\}$ and $H^{\mathrm{renamed}}-\{u_{a_0,*}\}$. This way, Player II never loses since there are not enough pebbles for Player I to make use of the oddity at the currently pebbleless set of nodes.

\end{proof}

\begin{remark}
    Notice that if we take $k=2$, then $G$ is two 3-cycles while $H$ is a 6-cycle, which 2-WL cannot differentiate; if we take $k=3$, then $G$ is the 4*4 Rook's graph while $H$ is the Shrikhande graph, which 3-WL cannot differentiate. In these special cases, the above construction complies with our well-known examples.
\end{remark}

\subsection{The Proof of Theorem~\ref{th:sub}}

Here we restate the theorem as follows:
%\begin{theorem}
%\label{th:sub1}
%For any connected substructure with no more than $k+m$ ($m\geq 2, k > 0$) nodes, there exists a subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL that can count it. 
%\end{theorem}

\begingroup
\def\thetheorem{\ref{th:sub}}
\begin{theorem}
For any connected substructure with no more than $k+m$ ($m\geq 2, k > 0$) nodes, there exists a subgraph GNN rooted at $k$-tuples with backbone GNN as powerful as $m$-WL that can count it. 
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup


\begin{proof}
Based on Remark~\ref{rm:de}, there exists a type of connected $k$-tuple that satisfy the decomposition of the connected substructure. Then the substructure can be separated into 2 subgraphs: the nodes that belong to the $k$-tuple (we call them rooted nodes) and the nodes that do not belong to the $k$-tuple (we call them non-rooted nodes). Formally, for the given two substructures $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, we define the subgraph that formed by the rooted nodes of $G_1$ ($G_2$, resp.) as $G_{1,r} = (V_{1, r}, E_{1,r})$ ($G_{2,r} = (V_{2,r}, E_{2,r})$, resp.). Similarly, define the subgraph that formed by the non-rooted nodes of $G_1$ ($G_2$, resp.) as $G_{1,n} = (V_{1, n}, E_{1,n})$ ($G_{2,n} = (V_{2,n}, E_{2,n})$, resp.). We can also define the subgraph formed between the rooted nodes and non-rooted nodes of $G_1$ ($G_2$, resp.) as $G_{1,c} = (V_{1, c}, E_{1,c})$ ($G_{2,c} = (V_{2,c}, E_{2,c})$, resp.). Then it is easy to find that for $G_1$, $V_1 = (V_{1,r} \cup V_{1, n})$, $V_{1,c} \subseteq V_1$, $E_1 = E_{1,r} \cup E_{1, n} \cup E_{1, c}$. There is no intersection between $V_{1,r}$ and $V_{1, n}$, and there is no intersection between $E_{1,r}$, $E_{1,c}$, and $E_{1, n}$. The same holds for $G_2$.

If $G_1$ and $G_2$ are non-isomorphic, then there can be three potential situations: (1) $G_{1,r}$ and $G_{2,r}$ are non-isomorphic; (2) $G_{1,n}$ and $G_{2,n}$ are non-isomorphic; (3) $G_{1,c}$ and $G_{2,c}$ are non-isomorphic. In the following section, we will prove that in all these situations, the subgraph GNN can distinguish between $G_1$ and $G_2$.

\textbf{$G_{1,r}$ and $G_{2,r}$ are non-isomorphic.}  It denotes that for $G_1$ and $G_2$, the selected $k$-tuples are different. Then of course the subgraph GNN can differentiate $G_1$ and $G_2$.

\textbf{$G_{1,n}$ and $G_{2,n}$ are non-isomorphic.} Recall that we use a backbone GNN as powerful as $m$-WL to encode the information within the subgraph, and $V_{1,n}$ and $V_{2,n}$ contain nodes no more than $m$ nodes. Therefore if $G_{1,n}$ and $G_{2,n}$ are non-isomorphic, then they will have different isomorphic types, and thus can be distinguished by the backbone GNN.

\textbf{$G_{1,c}$ and $G_{2,c}$ are non-isomorphic.} We define the label of all nodes in $G_1$ (the same holds for $G_2$) as its distance to the nodes in the $k$-tuple. Formally, let $V_{1,r} = \{v_{1,r,1},...,v_{1,r,k}\}$, then $\forall u \in V_1$, its label $f_1(u) = (d(u, v_{1,r,1}),...,d({u,v_{1,r,k}}), I(u))$, where $d(.)$ denotes the shortest path distance between two nodes, and $I(u)$ denotes the label that reflects the isomorphic type of $u$ encoded by the subgraph GNN within $G_{1,n}$.

Since the substructures are connected, there exists at least a node $u_1 \in V_{1,n}$, whose label contains at least an index with the value ``1". For $f(u_1)$, the indices with value ``1" denotes that there exist edges between $u_1$ and the corresponding nodes in $V_{1,r}$. While the indices with value larger than 1 denote that there is no edge between $u_1$ and the corresponding nodes in $V_{1,r}$. The same holds for $u_2$ and $V_{2,r}$. Therefore, if the subgraph formed by $u_1$ and $V_{1,r}$ and the subgraph formed by $u_2$ and $V_{2,r}$ are non-isomorphic, then $f_1(u_1)$ and $f_2(u_2)$ are different, and the subgraph GNN can differentiate the two substructures. Also, if the $I(u_1)$ and $I_(u_2)$ are different, then the subgraph GNN can also distinguish $G_1$ and $G_2$. We can then consider the next nodes, and continue the process inductively.

Therefore, if for all nodes in $V_{1,n}$, we can find a unique node in $V_{2,n}$ that has the same label as it. Then $G_{1,c}$ and $G_{2,c}$ are isomorphic. Reversely, if $G_{1,c}$ and $G_{2,c}$ are non-isomorphic, then there exists at least a node in $V_{1,n}$, that we cannot find a unique node in $V_{2,n}$ that has the same label as it. Therefore the subgraph GNN can differentiate $G_1$ and $G_2$.

Then we can assign each substructure a unique color according to its isomorphic type, and use the color histogram of the graph as the output function. If two graphs have different numbers of certain substructures, then the color histogram will be different.

Based on the above results, the subgraph GNN can count the given type of connected substructure.

\end{proof}


\subsection{The proof of Theorem~\ref{th:iso}.}
Below, we state the result as follows.
%\begin{theorem}
%\label{th:iso1}
%ESC-GNN is strictly more powerful than 2-WL, while not less expressive than 3-WL.
%\end{theorem}

\begingroup
\def\thetheorem{\ref{th:iso}}
\begin{theorem}
ESC-GNN is strictly more powerful than 2-WL, while not less expressive than 3-WL.
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup

\begin{figure}[btp]
	\centering
	\subfigure[]{
		\begin{minipage}[t]{0.43\linewidth}
			\centering
			\includegraphics[width=\columnwidth]{figure/Rook.png}
			%\caption{fig1}
		\end{minipage}%
	}%
	\subfigure[]{
		\begin{minipage}[t]{0.37\linewidth}
			\centering
			\includegraphics[width=\columnwidth]{figure/Sh.png}
			%\caption{fig1}
		\end{minipage}%
	}%
	\centering
	\vspace{-.1in}
	\caption{(a) the 4*4 Rook Graph and (b) the Shrikhande Graph}
	\label{fig:3wl}
	\vspace{-.15in}
\end{figure}



 \begin{proof}
 \textbf{ESC-GNN is not less powerful than 3-WL.} As shown in Theorem~\ref{th:count}, ESC-GNN is able to count 4-cliques. In the pair of graphs called the 4*4 Rook Graph and the Shrikhande Graph (shown in Figure~\ref{fig:3wl}), there exist several 4-cliques in the 4*4 Rook Graph, while there is no 4-clique in the Shrikhande Graph. Therefore ESC-GNN can differentiate the pair of graphs. Considering that 3-WL cannot differentiate them~\cite{arvind2020weisfeiler},  ESC-GNN is not less powerful than 3-WL.
 
 \textbf{ESC-GNN is more powerful than 2-WL.} Using the MPNN~\cite{xu2018powerful} as the backbone network, it can be as powerful as 2-WL in terms of distinguishing non-isomorphic graphs. However, there exist pairs of graphs, e.g., the 4*4 Rook Graph and the Shrikhande Graph that can be distinguished by ESC-GNN but not 2-WL. Therefore, ESC-GNN is strictly more powerful than the 2-WL.
 

 \end{proof}
 
\subsection{The proof of Theorem~\ref{th:count}}
Below we show the counting power of ESC-GNN in terms of subgraph counting. %We decompose Theorem~\ref{th:count} according to different substructures.

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figure/4cycle.png}}
\caption{Examples of 4-cycles that pass the rooted edges. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:4cycle}
\end{center}
%\vskip -0.2in
\end{figure*}

% \begin{figure*}[btp]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figure/4path.png}}
% \caption{Examples of 4-paths that may be potentially computed between the rooted nodes. In these figures, the rooted nodes are colored blue.}
% \label{fig:4path}
% \end{center}
% \end{figure*}

%\begin{theorem}
%\label{th:count1}
%In terms of subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques.
%\end{theorem}

\begingroup
\def\thetheorem{\ref{th:count}}
\begin{theorem}
In terms of subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) stars with arbitrary sizes; (4) up to 3-paths.
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figure/5cycle_sub.png}}
\caption{Examples where ESC-GNN cannot subgraph-count 5-cycles. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:5cycle_sub}
\end{center}
%\vskip -0.2in
\end{figure*}

\begin{proof}
\textbf{Clique counting.} The number of 3-cliques is the number of nodes with the shortest path distance ``1" to both rooted nodes; the number of 4-cliques is the number of edges labeled $(1,1,1,1)$ (definition see the edge-level distance encoding in Section~\ref{sec:ESC-GNN}, and example see Figure~\ref{fig:4cycle}(d)). Therefore, ESC-GNN can count these types of cliques. In terms of 5-cliques, 4-WL cannot count them according to Theorem~\ref{th:disab}, therefore subgraph GNNs rooted on edges with MPNN as the backbone GNN cannot count 5-cliques according to Proposition~\ref{pro:sub}. Then according to Proposition~\ref{rm:esc}, ESC-GNN also cannot count 5-cliques.

\textbf{Cycle counting.} the counting of 3-cycles is the same as 3-cliques. In terms of counting 4-cycles, there are basically 4 different situations where 4-cycles exist, examples are shown in Figure~\ref{fig:4cycle}. Note that figures (a),(b), and (c) contain one 4-cycle, respectively, while figure (d) contains two 4-cycles that pass the rooted edge. Therefore the number of 4-cycles is the weighted sum of the number of edges with labels $(1,2,2,1)$, $(1,1,2,1)$, $(1,2,1,1)$, $(1,1,1,1)$.

In terms of 5-cycle subgraph-counting, we provide a counter-example in Figure~\ref{fig:5cycle_sub}. In Figure~\ref{fig:5cycle_sub}(a), there is one 5-cycle $ABEDC$ that passes $AB$, while there is no 5-cycle that passes $AB$ in Figure~\ref{fig:5cycle_sub}(b). Considering that the degree information and the distance information is the same for the pair of graphs, ESC-GNN cannot differentiate them. Therefore, ESC-GNN cannot subgraph-count 5-cycles.


\textbf{Path Counting.} Note that the definition of paths here is different from other substructures. It denotes the number of edges within the path instead of the number of nodes within the path. Here, we slightly extend the use of 2-tuples in ESC-GNN by considering not only 2-tuples with edges but also 2-tuples without edges.

In terms of counting 2-paths or 3-paths between 2 nodes, it is equal to counting 3-cycles or 4-cycles, between edges, respectively. We have proven that ESC-GNN can count these cycles, therefore ESC-GNN can count such edges.

\textbf{Star counting.} We can decompose the graph-level star counting problem to 2-tuples by considering the first node of each 2-tuple as the root of stars. Examples are shown in Figure~\ref{fig:star}. We advocate that the number of stars is easily encoded by the number of nodes whose shortest path distance is 1 to the first rooted node. Denote the number of nodes with the shortest path distance 1 to the first rooted node as $N'$ (including the second node), then the number of $p$-stars is $C_{N'-1}^{p-2}$. A similar proof is provided by~\cite{chen2020can}.

\end{proof}

%Then we show the counting power of ESC-GNN in terms of counting paths.

%\begin{theorem}
%\label{th:path}
%In terms of subgraph counting, ESC-GNN can count up to 3-paths.
%\end{theorem}

%\begin{proof}
%Note that the definition of paths here is different from other substructures. It denotes the number of edges within the path instead of the number of nodes within the path. Here, we slightly extend the use of 2-tuples in ESC-GNN by considering not only 2-tuples with edges but also 2-tuples without edges.

%In terms of counting 2-paths or 3-paths between 2 nodes, it is equal to counting 3-cycles or 4-cycles, between edges, respectively. We have proven that ESC-GNN can count these cycles, therefore ESC-GNN can count such edges.
%\end{proof}

%\begin{theorem}
%In terms of subgraph counting, ESC-GNN can count stars of arbitrary shapes.
%\end{theorem}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figure/star.png}}
\caption{Examples of stars that pass the rooted edges. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:star}
\end{center}
%\vskip -0.2in
\end{figure*}

%\begin{proof}
%We can decompose the graph-level star counting problem to 2-tuples by considering the first node of each 2-tuple as the root of stars. Examples are shown in Figure~\ref{fig:star}. We advocate that the number of stars is easily encoded by the number of nodes whose shortest path distance is 1 to the first rooted node. Denote the number of nodes with the shortest path distance 1 to the first rooted node as $N'$ (including the second node), then the number of $p$-stars is $C_{N'-1}^{p-2}$. A similar proof is provided by~\cite{chen2020can}.
%\end{proof}

\subsection{The proof of Theorem~\ref{th:count_ind}}

Below we show the counting power of ESC-GNN in terms of induced subgraph counting. We restate the Theorem~\ref{th:count_ind} as follows.


\begingroup
\def\thetheorem{\ref{th:count_ind}}
\begin{theorem}
In terms of induced subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) up to 4-stars; (4) up to 3-paths.
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup

%\begin{theorem}
%In terms of induced subgraph counting, ESC-GNN can count (1) up to 4-cycles; (2) up to 4-cliques; (3) up to 3-paths; (4) up to 4-stars.
%\end{theorem}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\columnwidth]{figure/5star.png}}
\caption{Examples where ESC-GNN cannot induced-subgraph-count 5-stars. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:5star}
\end{center}
%\vskip -0.2in
\end{figure*}


\begin{proof}
Since cliques are fully connected substructures, the proof of cliques is the same as the proof of subgraph counting.

For cycles, the number of 3-cycles is the same as 3-cliques. As for 4-cycles, we only need to consider the situation shown in Figure~\ref{fig:4cycle}(a), where the number of $(1,2,2,1)$ edges reflects the number of 4-cycles. For induced-subgraph-counting 5-cycles, in Figure~\ref{fig:5cycle}(a), there is no 5-cycle that pass $AB$, while in Figure~\ref{fig:5cycle}(b), there is one 5-cycle $ABEDC$ that passes $AB$. However, ESC-GNN cannot differentiate the two graphs since the degree information and the distance information is the same. Therefore, ESC-GNN cannot induced-subgraph-count 5-cycles. It can serve as the same example for not counting 4-paths.

The proof of paths is actually the same as Theorem~\ref{th:count}.

In terms of 3-stars and 4-stars, we only need to consider the situation shown in Figure~\ref{fig:star}(a), where the number of $(1,2)$ nodes encodes the number of stars, i.e., denote the number of $(1,2)$ nodes as $N'$, the number of $p$-stars ($p \leq 4$) is $C_{N'}^{p-2}$. 

For 5-stars, a pair of examples are shown in Figure~\ref{fig:5star}. These two graphs will be assigned the same structural embedding since the node degree information, and the distance information among these two graphs are the same. Therefore, ESC-GNN cannot differentiate the two graphs. However, Figure~\ref{fig:5star}(a) contains no 5-star that passes the rooted 2-tuple $AB$, while Figure~\ref{fig:5star}(b) contains one 5-star ($BAFDG$) that passes the rooted 2-tuple $AB$.
\end{proof}

\begin{figure*}[btp]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.55\columnwidth]{figure/5cycle.png}}
\caption{Examples where ESC-GNN cannot induced-subgraph-count 5-cycles. In these figures, the rooted 2-tuples are colored blue.}
\label{fig:5cycle}
\end{center}
%\vskip -0.2in
\end{figure*}

\subsection{The proof of Theorem~\ref{th:reg}}

Here we restate Theorem~\ref{th:reg} as follows: 

%\begin{theorem}
%\label{th:reg1}
%Consider all pairs of $r$-regular graphs with $n$ nodes, let $3\leq r < (2log2n)^{1/2}$ and $\epsilon$ be a fixed constant. With the hop parameter $h$ set to $\lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$, there exists a ESC-GNN that can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
%\end{theorem}

\begingroup
\def\thetheorem{\ref{th:reg}}
\begin{theorem}
Consider all pairs of $r$-regular graphs with $n$ nodes, let $3\leq r < (2log2n)^{1/2}$ and $\epsilon$ be a fixed constant. With the hop parameter $h$ set to $\lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$, there exists an ESC-GNN that can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup


Recall that we encode the distance information as augmented structural features for ESC-GNN. For the input graph $G$, denote the node-level distance encoding on $h$-hop subgraphs on 2-tuple $(u,v)$ as $D_{(u,v),G}^{h,\text{node}}$. Note that we can naturally transfer the encoding on 2-tuples to node by: ($D_{v,G}^{h,\text{node}} = D_{(v,v),G}^{h,\text{node}}$). We then introduce the following lemma.

\begin{lemma}
\label{lemma:reg}
For two graphs $G_1 = (V_1, E_1)$ and $G_2 = (V^2, E^2)$ that are randomly and independently sampled from $r$-regular graphs with n nodes ($3 \leq r < (2log2n)^{1/2}$). Select two nodes $v_1$ and $v_2$ from $G_1$ and $G_2$ respectively. Let $h = \lfloor(1/2 + \epsilon)\frac{log2n}{log(r-1)}\rfloor$ where $\epsilon$ is a fixed constant, $D_{v_1,G_1}^{h,\text{node}} = D_{v_2, G_2}^{h, \text{node}}$ with probability at most $o(n^{-3/2})$.
\end{lemma}

\begin{proof}
The proof follows from~\cite{bollobas1982distinguishing, feng2022powerful}. As $D_{v,G}^{h,\text{node}}$ stores exactly the same information as the node configuration used in~\cite{feng2022powerful}, therefore the proof is exactly the same as the proof of Lemma 1 in~\cite{feng2022powerful}. %Briefly speaking, the proof states that with the increasing of $h$, the number of neighboring nodes will almost be the same, thus $D_{v_1,G_1}^{h,\text{node}} = D_{v_2, G_2}^{h, \text{node}}$ with large probability.

Based on Lemma~\ref{lemma:reg}, we can prove Theorem~\ref{th:reg}. Given node $v_1 \in V_1$, compare $D_{v_1,G_1}^{h,\text{node}}$ with each $D_{v_2,G_2}^{h,\text{node}}$ where $v_2 \in V_2$. The probability that $D_{v_1,G_1}^{h,\text{node}} \neq D_{v_2,G_2}^{h,\text{node}}$ for all possible $v_2 \in V_2$ is $1-o(n^{-3/2})*n = 1 - o(n^{-1/2})$. Therefore, ESC-GNN can distinguish $1-o(n^{-1/2})$ such pairs of graphs.
\end{proof}




\subsection{Experimental Details}

\textbf{Stastics of Datasets.} The statistics of all used datasets are available in Table~\ref{tab:stat}.

\begin{table*}
	%\vspace{-0.1 in}
	\centering
	\caption{Statistics of the used datasets.}
	\label{tab:stat} 
	\scalebox{1.0}{
		\begin{tabular}{lccccc}
			\hline\noalign{\smallskip}
			Dataset & Graphs & Avg Nodes & Avg Edges & Task Type & Metric\\
			\noalign{\smallskip}\hline\noalign{\smallskip}
			MUTAG & 188 & 17.9 & 19.8 & Graph Classification & ACC\\
   			PTC-MR &  349 & 14.1 & 14.5 & Graph Classification & ACC\\
			ENZYMES &  600 & 32.6 & 62.1 & Graph Classification & ACC \\
			PROTEINS & 1113 & 39.1 & 72.8 & Graph Classification & ACC \\
			IMDB-BINARY & 1000 & 19.8 & 96.5 & Graph Classification & ACC\\
			ZINC-12k & 12000 & 23.2 & 24.9 & Graph Regression & MAE\\
			ogbg-molhiv & 41127 & 25.5 & 27.5 & Graph Classification & AUC-ROC\\
            ogbg-molpcba & 437929 & 26.0 & 28.1 & Graph Classification & AP\\
            QM9 & 129433 & 18.0 & 18.6 & Graph Regression & MAE\\
            Synthetic & 5000 & 18.8 & 31.3 & Node Regression & MAE\\
			\noalign{\smallskip}
			\hline
			\noalign{\smallskip}
	\end{tabular}}
	%\vspace{-0.1 in}
\end{table*}

\textbf{Experimental details.} The baselines and data splittings of our experiments follow from existing works~\cite{zhang2021nested, huang2023boosting} and the standard data split setting. For ESC-GNN, we adopt GIN~\cite{xu2018powerful} as the backbone GNN. In the structural embedding, we use both the shortest path distance and the resistance distance~\cite{lu2011link} as the distance feature. For the hop parameter $h$ we search between 1 to 4, and report the best results. Following existing works~\cite{huang2023boosting}, we use Adam optimizer as the optimizer, and use plateau scheduler with patience 10 and decay factor 0.9. On most datasets, the learning rate is set to 0.001, and the hidden embedding dimension is set to 300. The training epoch is set to 2000 for counting substructures, 400 for QM9, 1000 for ZINC, and 150 for the OGB dataset. Most of the experiments are implemented with two Intel Core i9-7960X processors and 2 NVIDIA 3090 graphics cards. Others (e.g., experiments on the TU dataset) are implemented with two Intel Xeon Gold 5218 processors and 10 NVIDIA 2080TI graphics cards. 

\subsection{Evaluation on Real-World Datasets.}
\label{subsec:real}

\textbf{Molecule Dataset.} We evaluate ESC-GNN on various popular real-world molecule datasets, including ZINC~\cite{dwivedi2020benchmarking} and the OGB dataset~\cite{hu2020open}. ZINC is a dataset of chemical compounds, and the task is graph regression. For the OGB dataset, we use ogbg-molhiv and ogbg-molpcba for evaluation. ogbg-molhiv contains 41K molecules with 2 classes, and ogbg-molpcba contains 438 molecules with 128 classes. The task is to predict to which these molecules belong. We follow the standard evaluation metric and the dataset split, and report the result in Table~\ref{tab:mol}.

%\begin{table*}
%\centering
%\caption{Evaluation on QM9 (MAE)} 
%\label{tab:qm9}
%\begin{tabular}{lcccccccc}
%\hline\noalign{\smallskip}
%Dataset & 1-GNN & 1-2-3-GNN & DTNN & Deep LRP & PPGN & NGN & I$^2$-GNN & ESC-GNN \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%$\mu$ & 0.493 & 0.476 & 0.244 & 0.364 & \textbf{0.231} & 0.428 & 0.428 & \textbf{0.231}\\
%%$\alpha$ & 0.78 & 0.27 & 0.95 & 0.298 & 0.382 & 0.29 & \textbf{0.230} & 0.265\\
%$\epsilon_{\text{homo}}$  &0.00321 &0.00337& 0.00388& 0.00254& 0.00276 &0.00265 &0.00261& \textbf{0.00221}\\
%$\epsilon_{\text{lumo}}$  &0.00355& 0.00351 &0.00512& 0.00277 &0.00287 &0.00297& 0.00267&\textbf{0.00204}\\
%$\Delta_{\epsilon}$ & 0.0049 &0.0048 &0.0112& 0.00353& 0.00406& 0.0038& 0.0038&\textbf{0.0032}\\
%$R^2$ & 34.1& 22.9& 17.0& 19.3 &16.07 &20.5 &18.64&\textbf{7.28}\\
%ZPVE & 0.00124& 0.00019 &0.00172 &0.00055 &0.0064& 0.0002 &\textbf{0.00014}&0.00033\\
%$U_0$ & 2.32& 0.0427 &2.43 &0.413 &0.234 &0.295& \textbf{0.211}&0.645\\
%%$U$ & 2.08& \textbf{0.111}& 2.43 &0.413 &0.234 &0.361 &0.206&0.380\\
%$H$ & 2.23 &\textbf{0.0419} &2.43 &0.413 &0.229 &0.305 &0.269&0.427\\
%$G$ & 1.94 &\textbf{0.0469} &2.43 &0.413 &0.238& 0.489 &0.261&0.384\\
%$C_v$ &0.27& 0.0944& 2.43& 0.129 &0.184 &0.174 &\textbf{0.0730} &0.105\\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%\end{tabular}\end{table*}


\begin{table}
\centering
\caption{Evalation on ZINC and OGB datasets.} 
\label{tab:mol}
\scalebox{0.8}{
\begin{tabular}{lccc}
\hline\noalign{\smallskip}
Dataset & OGBG-Hiv (AUCROC) & OGBG-PCBA (AP) & ZINC \\
\noalign{\smallskip}\hline\noalign{\smallskip}
%GCN & 75.99$\pm$1.19* & 24.24$\pm$0.34* \\
GIN & 77.07$\pm$1.49 & 27.03$\pm$0.23 & 0.163 \\
PNA & 79.05$\pm$1.32 & 28.38$\pm$0.35 & 0.188\\
DGN & 79.70$\pm$0.97 & 28.85$\pm$0.30 & 0.168 \\
GSN & 77.99$\pm$1.00 & - & 0.115 \\
CIN & \textbf{80.94$\pm$0.57**} & - & 0.079 \\
NGNN & 78.34$\pm$1.86* & 28.32$\pm$0.41* & 0.111\\
GIN-AK+ & 79.61$\pm$1.19** & 29.30$\pm$0.44** & 0.080\\
OSAN & - & - & 0.126\\ 
SUN & 80.03$\pm$0.55 &  - & 0.083 \\
DSS-GNN & 76.78$\pm$1.66 & - & 0.102 \\
I$^2$-GNN & 78.68$\pm$0.93 & - & 0.083 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
ESC-GNN & 78.36$\pm$1.06 & 28.16$\pm$0.31 & 0.096\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}}
\end{table}

\textbf{TU datasets.} We evaluate the performance of ESC-GNN on the TU datasets~\cite{morris2020tudataset}. The experimental settings follow~\cite{zhang2021nested} for more consistent evaluation standards. Specifically, we uniformly use the 10-fold cross validation framework, with the split ratio of training/validation/test set 0.8/0.1/0.1. The results are available in Table~\ref{tab:tu}. In the table, ESC-GNN (h2) and ESC-GNN (h3) denote ESC-GNN with the hop parameters setting to 2 and 3. 

\textbf{Results.} From the tables, we observe that ESC-GNN performs much better than normal MPNNs, comparably with or slightly better than subgraph GNNs rooted at nodes, while performing comparably or slightly less than I$^2$-GNN. This shows that while running much faster than these subgraph GNNs (shown in Table~\ref{tab:eff}), ESC-GNN manages to preserve the representation power of subgraph GNNs.

\begin{table}
\centering
\caption{Experiments on TU, Accuracy as the evaluation metric.} 
\label{tab:tu}
\begin{tabular}{lcccccccc}
\hline\noalign{\smallskip}
Dataset & MUTAG & PTC-MR & PROTEINS & ENZYMES & IMDB-B \\
\noalign{\smallskip}\hline\noalign{\smallskip}
GIN & 84.5$\pm$8.9 & 51.2$\pm$9.2 & 70.6$\pm$4.3 & 38.3$\pm$6.4 & 73.3$\pm$4.7 \\
PPGN & 84.7$\pm$8.2 & 55.0$\pm$6.4 & 74.8$\pm$3.3 & 55.0$\pm$6.4 & 71.5$\pm$5.4\\
NGNN & 87.9$\pm$8.2* & 54.1$\pm$7.7* & 73.9$\pm$5.1* & 29.0$\pm$8.0* & 73.1$\pm$5.7 \\
GIN-AK+ & \textbf{88.8$\pm$4.0} & 60.5$\pm$8.0 & 75.5$\pm$4.4 & \textbf{58.9$\pm$6.2} & 72.4$\pm$3.7\\ 
SUN & 86.1$\pm$6.0 & 60.2$\pm$7.2 & 72.1$\pm$3.8 & 16.7$\pm$0.0 & \textbf{73.7$\pm$2.9}\\
I$^2$-GNN & 87.9$\pm$4.3 & \textbf{61.4$\pm$8.7} & 74.8$\pm$2.9 & 40.3$\pm$6.7 & 73.6$\pm$4.0\\
%nest PPGN & \textbf{91.0$\pm$7.0} & 59.6$\pm$10.7& 75.0$\pm$3.3 & 40.0$\pm$7.4& \textbf{74.7$\pm$3.9}\\
%edge PPGN & 85.2$\pm$7.7 & 57.6$\pm$9.0 & \textbf{76.8$\pm$3.6} & 51.1$\pm$5.5 & OOM \\
\noalign{\smallskip}\hline\noalign{\smallskip}
ESC-GNN (h2) & 86.2$\pm$7.9 & 52.9$\pm$6.4 & 73.3$\pm$4.1 & 53.2$\pm$8.1 & 72.0$\pm$6.0\\
ESC-GNN (h3) & 85.6$\pm$7.9 & 56.4$\pm$6.9 & \textbf{76.0$\pm$4.5} & 43.3$\pm$6.0 & \textbf{73.7$\pm$4.8}\\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

\subsection{Ablation Study.}
\label{subsec:ablation}
We evaluate the effectiveness of each part of the proposed structural embedding on the substructure counting dataset. For the proposed three types of structural embedding (the degree encoding, the node-level distance encoding, and the edge-level distance encoding), we delete one of them from the original structural embedding every time and report the results in Table~\ref{tab:count_ab}. We observe that after removing the three types of embedding, ESC-GNN performs worse compared with its original version, especially after removing the edge-level distance encoding. This is consistent with our theoretical results: in Theorem~\ref{th:sub}, we show that the proposed structural embedding contains key information for the counting power of subgraph GNNs; in Theorem~\ref{th:count} and Theorem~\ref{th:count_ind}, we show that the edge-level distance information directly encodes the number of certain types of substructures.

\begin{table*}
	\centering
	\caption{Ablation study on the proposed structural embedding (norm MAE).} 
    \label{tab:count_ab}
	\scalebox{0.85}{
	\begin{tabular}{lccccccccc}
	\hline\noalign{\smallskip}
	Dataset & Tailed Triangle & Chordal Cycle & 4-Clique & 4-Path & Triangle-Rectangle & 3-cycles & 4-cycles & 5-cycles & 6-cycles \\
	\noalign{\smallskip}\hline\noalign{\smallskip}
	MPNN & 0.3631 & 0.3114 & 0.1645 & 0.1592 & 0.3018 &  0.3515 & 0.2742 & 0.2088 & 0.1555\\
 	\noalign{\smallskip}\hline\noalign{\smallskip}
	ESC-GNN & \cellcolor{yellow}0.0052 & 0.0169 & \cellcolor{yellow}0.0064 & 0.0254 & 0.0748& \cellcolor{yellow}0.0074 &\cellcolor{yellow}0.0096 & 0.0356& 0.0578\\
    \noalign{\smallskip}\hline\noalign{\smallskip}
     (- degree) &0.0121 & 0.0492& 0.0106& 0.0322 & 0.0841 &0.0342 &0.0144 &0.0513 & 0.0652\\
    (- node-level dist) & 0.0382 & 0.0344 & 0.0222 &0.0428 & 0.1120 & 0.0157 & 0.0261 & 0.0492 &0.0608  \\
    (- edge-level dist) & 0.0208&0.2811 &0.0497 & 0.0584& 0.2438& 0.2617&0.2244 & 0.1654& 0.1364\\
	\hline\noalign{\smallskip}
    \end{tabular}}
\end{table*}
% In terms of counting 4-paths, we follow the inspiration from~\cite{furer2017combinatorial}. Let $p_{uv}^l$ be the number of paths of length $l$ from node $u$ to $v$. As proven above, we can extract $p_{uv}^1$ and $p_{uv}^2$ for all node pairs. Therefore, for all $l_1, l_1' \in \{0,1\}$ and $l_2,l_2' \in N$, we can compute:
% $$n_{l_1l_2l_1'l_2'} = |\{w|w \notin \{u,v\} \wedge p_{uw}^i = l_i \wedge p_{wv}^i = l_i' \text{ for } i \in \{1,2\}\}|.$$

% Then, we can compute the number of 4-paths between $u$ and $v$ by first combining all paths of length 2 from $u$ to $w$ and $w$ to $v$. We then remove two cases: (1) the 2-paths between $u$ and $w$ through $v$, or the 2-paths between $v$ and $w$ through $u$ (Figure~\ref{fig:4path}(c)); (2) walks in the form of $u,x,w,x,v$ where for each $x$ (Figure~\ref{fig:4path}(b)). Therefore, the final number of 4-paths between $u$ and $v$ can be computed as:
% $$\sum_{l_1,l_2,l_1',l_2'}n_{l_1l_2l_1'l_2'} (l_2 - p_{uv}^1l_1')(l_2'-p_{uv}^1l_1) - \sum_{x \in V\\\{u,v\}} p_{ux}^1(d(x)-2)p_{xv}^1$$
% \end{proof}

% And based on the above result, we can compute the number of 5-cycles that pass node pair $(u,v)$ because we know whether there exist an edge between $u$ and $v$ or not. Therefore, we have
% \begin{theorem}
% \label{th:count2}
% ESC-GNN can count 5-cycles.
% \end{theorem}

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
