\section{Data Depth}
\label{sec:depth}
Depth functions have been initially introduced in the setting of non-parametric multivariate analysis to define affine invariant versions of median, quantiles, and ranks in higher dimensional spaces where there is no natural order  (see historical overviews by~\citet{mosler2012multivariate,Nieto-Reyes:Battey:2015}).  The key idea of the depth approach is to offer a center-outward ordering of all observations by assigning a numeric score in $[0,1]$ range to each data point with respect to its position within a cloud of multivariate or functional observations or a probability distribution. Nowadays, data depth is a rapidly developing field that gains increasing momentum due to the wide applicability of depth concepts to classification, visualization, high dimensional and functional data analysis~\cite{Hyndman:Shang:2010, Narisetty:Nair:2016, mozharovskyi2020nonparametric, sguera2020notion, zhang2021depth}.
Most recently, depth approaches have found novel applications in density-based clustering and space-time data mining~\cite{Jeong:etal:2016, HuangGel2017, vinue2020robust}, shape recognition and uncertainty quantification in computer graphics~\cite{Whitaker:etal:2013, sheharyar2019visual}, ordinal data analysis~\cite{Kleindessner:vonLuxburg:2017} and computational geometry for privacy-preserving data analysis~\cite{mahdikhani2020achieve}.
Nevertheless, data depth is yet a largely unexplored concept in network sciences~\cite{Fraiman:et:2015, raj2017path,Tian:Gel:2017, tian2019fusing}.


\begin{table}
\fontsize{9}{12}\selectfont
\centering
\caption{Example node property functions. Most functions are adopted from the related work. Functions that encode the community around a node, such as cycles, can help bringing a higher ordered structure of the network into use.}
\label{tab:node_property_functions}
\begin{tabular}{ll}
\hline
Function & Value / number of ... \\
\hline
$N(u)$ & neighbors of $u$ \\
$N_{out}(u)$ & neighbors reachable with outgoing edges from $u$ \\
$N_{in}(u)$ & neighbors reachable with incoming edges to $u$ \\
$deg(u)$ & edges to/from $u$ (Degree) \\
$deg_{out}(u)$ & outgoing edges from $u$ (Out-Degree) \\
$deg_{in}(u)$ & incoming edges to $u$ (In-Degree) \\
$\bigcirc(u, l)$ & undirected cycles of length $l$ that $u$ is part of \\
$\circlearrowright(u, l)$ & directed cycles of length $l$ that $u$ is part of \\
$t(u, l)$ & length $l$ timeframes that $u$ has edges in \\
$S(u)$ & sum of edge weights incident to a node (Strength)\\
$S_{out}(u)$ & sum of outgoing edge weights (Out-Strength)\\
$S_{in}(u)$ & sum of incoming edge weights (In-Strength)\\
\hline
\end{tabular}
\end{table}

\begin{definition}[Data Depth] Formally, let $E$ be a Banach space (e.g., $E=\mathbb{R}^d$), $\mathcal B$ its Borel sets in $E$, and $\mathcal P$ be a set of probability distributions on $\mathcal B$. We view $\mathcal P$ as the class of empirical distributions giving equal probabilities $1/n$ to $n$ data points in $E$. Then a data depth function is a function $\mathbb{D}: E\times \mathcal P \longrightarrow [0,1]$, $(x,P) \longrightarrow \mathbb{D}(x|P)$, $x\in E, P\in \mathcal P$ that shall satisfy the following desirable properties: \textit{affine invariant}, \textit{upper semi-continuous} in $x$,
\textit{quasiconcave} in $x$ (i.e., having convex upper level sets) and \textit{vanishing as} $||x||\to \infty$. Specifically, a data depth   function $\mathbb{D}(x)$ measures how closely an observed point $x \in \mathbb{R}^d$, $d\geq 1$, is located to the center of a finite set $\mathcal{X}\in \mathbb{R}^d$, or relative to $F$, which is a probability distribution in $\mathbb{R}^d$. In complex network analysis, these points may correspond to the features of nodes or edges.
\end{definition}



Among many depth functions formulated to date, the Mahalanobis depth is one of the most prominent in the current practice.

\begin{definition}[Mahalanobis (MhD) depth]
Let $x\in \mathbb{R}^d$ be an observed data point, then Mahalanobis (MhD) depth of $x$ in respect to a $d$-variate probability distribution $F$ with mean vector $\mu_F \in \mathbb{R}^d$ and covariance matrix $\Sigma_F \in \mathbb{R}^{d\times d}$ is given by
 \begin{equation}
MhD_{\mu_F}(x)=\bigl(1+(x-\mu_F)^\top\Sigma^{-1}_F(x-\mu_F)\bigr)^{-1}.
 \end{equation}
Here $^\top$ denotes matrix transpose. The MhD depth measures the \textit{outlyingness} of the point with respect to the deepest point of the distribution (here $\mu_F$), and allows to easily handle the elliptical family of distributions, including a Gaussian case.
\end{definition}
MhD offers flexibility in changing the reference point with respect to which we compute data rankings. For instance, instead of $\mu_F$ we can select an arbitrary point $x_0\in \mathbb{R}^d$ and compute MhD in respect to this new reference point $x_0$
\begin{equation}
\label{MhD_arb}
MhD_{x_0}(x)=\bigl(1+(x-x_0)^\top\Sigma^{-1}_F(x-x_0)\bigr)^{-1}.
 \end{equation}
Furthermore, $\Sigma_F$ can be substituted by any empirical estimator of covariance matrix $\hat{\Sigma}$ obtained from the observed data sample $x_1, x_2, \ldots, x_n$.
