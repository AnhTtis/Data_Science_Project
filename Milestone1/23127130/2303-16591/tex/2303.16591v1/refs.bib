 @misc{johnson_2022, title={The state of open source vulnerabilities 2021 - whitesource}, url={https://www.mend.io/resources/research-reports/the-state-of-open-source-vulnerabilities/}, journal={Mend}, author={Johnson , Patricia}, year={2022}, month={Jun}}
 
 @article{MCKINNEL2019175,
title = {A systematic literature review and meta-analysis on artificial intelligence in penetration testing and vulnerability assessment},
journal = {Computers \& Electrical Engineering},
volume = {75},
pages = {175-188},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618315489},
author = {Dean Richard McKinnel and Tooska Dargahi and Ali Dehghantanha and Kim-Kwang Raymond Choo},
keywords = {Penetration testing, Vulnerability assessment, Artificial intelligence, Systematic literature review, Machine learning, Meta-analysis},
abstract = {Vulnerability assessment (e.g., vulnerability identification and exploitation; also referred to as penetration testing) is a relatively mature industry, although attempting to keep pace with the diversity of computing and digital devices that need to be examined is challenging. Hence, there has been ongoing interest in exploring the potential of artificial intelligence to enhance penetration testing and vulnerability identification of systems, as evidenced by the systematic literature review performed in this paper. In this review, we focus only on empirical papers, and based on the findings, we identify a number of potential research challenges and opportunities, such as scalability and the need for real-time identification of exploitable vulnerabilities.}
}

@INPROCEEDINGS{johnson,

  author={Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},

  booktitle={2013 35th International Conference on Software Engineering (ICSE)}, 

  title={Why don't software developers use static analysis tools to find bugs?}, 

  year={2013},

  volume={},

  number={},

  pages={672-681},

  doi={10.1109/ICSE.2013.6606613}}

@INPROCEEDINGS{antunes,  author={Antunes, Nuno and Vieira, Marco},  booktitle={2010 IEEE International Conference on Web Services},   title={Benchmarking Vulnerability Detection Tools for Web Services},   year={2010},  volume={},  number={},  pages={203-210},  doi={10.1109/ICWS.2010.76}}

@article{hydara,
title = {Internet of Things security and forensics: Challenges and opportunities},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {544-546},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316667},
author = {Mauro Conti and Ali Dehghantanha and Katrin Franke and Steve Watson},
keywords = {Internet of Things, Cyber security, Digital forensics},
abstract = {The Internet of Things (IoT) envisions pervasive, connected, and smart nodes interacting autonomously while offering all sorts of services. Wide distribution, openness and relatively high processing power of IoT objects made them an ideal target for cyber attacks. Moreover, as many of IoT nodes are collecting and processing private information, they are becoming a goldmine of data for malicious actors. Therefore, security and specifically the ability to detect compromised nodes, together with collecting and preserving evidences of an attack or malicious activities emerge as a priority in successful deployment of IoT networks. In this paper, we first introduce existing major security and forensics challenges within IoT domain and then briefly discuss about papers published in this special issue targeting identified challenges.}
}

@article{LOMIO,
title = {Just-in-time software vulnerability detection: Are we there yet?},
journal = {Journal of Systems and Software},
volume = {188},
pages = {111283},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111283},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000437},
author = {Francesco Lomio and Emanuele Iannone and Andrea {De Lucia} and Fabio Palomba and Valentina Lenarduzzi},
keywords = {Software vulnerabilities, Machine learning, Empirical SE},
abstract = {Background:
Software vulnerabilities are weaknesses in source code that might be exploited to cause harm or loss. Previous work has proposed a number of automated machine learning approaches to detect them. Most of these techniques work at release-level, meaning that they aim at predicting the files that will potentially be vulnerable in a future release. Yet, researchers have shown that a commit-level identification of source code issues might better fit the developer’s needs, speeding up their resolution.
Objective:
To investigate how currently available machine learning-based vulnerability detection mechanisms can support developers in the detection of vulnerabilities at commit-level.
Method:
We perform an empirical study where we consider nine projects accounting for 8991 commits and experiment with eight machine learners built using process, product, and textual metrics.
Results:
We point out three main findings: (1) basic machine learners rarely perform well; (2) the use of ensemble machine learning algorithms based on boosting can substantially improve the performance; and (3) the combination of more metrics does not necessarily improve the classification capabilities.
Conclusion:
Further research should focus on just-in-time vulnerability detection, especially with respect to the introduction of smart approaches for feature selection and training strategies.}
}

@inproceedings{NEUHAUS,
author = {Neuhaus, Stephan and Zimmermann, Thomas and Holler, Christian and Zeller, Andreas},
title = {Predicting Vulnerable Software Components},
year = {2007},
isbn = {9781595937032},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1315245.1315311},
doi = {10.1145/1315245.1315311},
abstract = {Where do most vulnerabilities occur in software? Our Vulture tool automatically mines existing vulnerability databases and version archives to map past vulnerabilities to components. The resulting ranking of the most vulnerable components is a perfect base for further investigations on what makes components vulnerable.In an investigation of the Mozilla vulnerability history, we surprisingly found that components that had a single vulnerability in the past were generally not likely to have further vulnerabilities. However, components that had similar imports or function calls were likely to be vulnerable.Based on this observation, we were able to extend Vulture by a simple predictor that correctly predicts about half of all vulnerable components, and about two thirds of all predictions are correct. This allows developers and project managers to focus their their efforts where it is needed most: "We should look at nsXPInstallManager because it is likely to contain yet unknown vulnerabilities.".},
booktitle = {Proceedings of the 14th ACM Conference on Computer and Communications Security},
pages = {529–540},
numpages = {12},
keywords = {software security, prediction},
location = {Alexandria, Virginia, USA},
series = {CCS '07}
}

@article{LIVHITS,
author = {Cadar, Cristian and Sen, Koushik},
title = {Symbolic Execution for Software Testing: Three Decades Later},
year = {2013},
issue_date = {February 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2408776.2408795},
doi = {10.1145/2408776.2408795},
abstract = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
journal = {Commun. ACM},
month = {feb},
pages = {82–90},
numpages = {9}
}

@inproceedings{PERL,
author = {Perl, Henning and Dechand, Sergej and Smith, Matthew and Arp, Daniel and Yamaguchi, Fabian and Rieck, Konrad and Fahl, Sascha and Acar, Yasemin},
title = {VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813604},
doi = {10.1145/2810103.2813604},
abstract = {Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. However, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is sufficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flawfinder, to flag potentially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the amount of false alarms by over 99 \% at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {426–437},
numpages = {12},
keywords = {static analysis, machine learning, vulnerabilities},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@ARTICLE{KAMEI,  author={Kamei, Yasutaka and Shihab, Emad and Adams, Bram and Hassan, Ahmed E. and Mockus, Audris and Sinha, Anand and Ubayashi, Naoyasu},  journal={IEEE Transactions on Software Engineering},   title={A large-scale empirical study of just-in-time quality assurance},   year={2013},  volume={39},  number={6},  pages={757-773},  doi={10.1109/TSE.2012.70}}

@INPROCEEDINGS{ZHANG,  author={Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},   title={A Novel Neural Source Code Representation Based on Abstract Syntax Tree},   year={2019},  volume={},  number={},  pages={783-794},  doi={10.1109/ICSE.2019.00086}}

@article{HARER,
  author    = {Jacob A. Harer and
               Louis Y. Kim and
               Rebecca L. Russell and
               Onur Ozdemir and
               Leonard R. Kosta and
               Akshay Rangamani and
               Lei H. Hamilton and
               Gabriel I. Centeno and
               Jonathan R. Key and
               Paul M. Ellingwood and
               Marc W. McConley and
               Jeffrey M. Opper and
               Peter Chin and
               Tomo Lazovich},
  title     = {Automated software vulnerability detection with machine learning},
  journal   = {CoRR},
  volume    = {abs/1803.04497},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.04497},
  eprinttype = {arXiv},
  eprint    = {1803.04497},
  timestamp = {Thu, 29 Sep 2022 15:06:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-04497.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MORRISON,
author = {Morrison, Patrick and Herzig, Kim and Murphy, Brendan and Williams, Laurie},
title = {Challenges with Applying Vulnerability Prediction Models},
year = {2015},
isbn = {9781450333764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746194.2746198},
doi = {10.1145/2746194.2746198},
abstract = {Vulnerability prediction models (VPM) are believed to hold promise for providing software engineers guidance on where to prioritize precious verification resources to search for vulnerabilities. However, while Microsoft product teams have adopted defect prediction models, they have not adopted vulnerability prediction models (VPMs). The goal of this research is to measure whether vulnerability prediction models built using standard recommendations perform well enough to provide actionable results for engineering resource allocation. We define 'actionable' in terms of the inspection effort required to evaluate model results. We replicated a VPM for two releases of the Windows Operating System, varying model granularity and statistical learners. We reproduced binary-level prediction precision (~0.75) and recall (~0.2). However, binaries often exceed 1 million lines of code, too large to practically inspect, and engineers expressed preference for source file level predictions. Our source file level models yield precision below 0.5 and recall below 0.2. We suggest that VPMs must be refined to achieve actionable performance, possibly through security-specific metrics.},
booktitle = {Proceedings of the 2015 Symposium and Bootcamp on the Science of Security},
articleno = {4},
numpages = {9},
keywords = {complexity, coverage, prediction, vulnerabilities, churn, dependencies, metrics},
location = {Urbana, Illinois},
series = {HotSoS '15}
}

@article{SHIN,
author = {Shin, Yonghee and Williams, Laurie},
year = {2011},
month = {02},
pages = {},
title = {Can traditional fault prediction models be used for vulnerability prediction?},
volume = {18},
journal = {Empirical Software Engineering},
doi = {10.1007/s10664-011-9190-8}
}

@ARTICLE{ZAGANE,  author={Zagane, Mohammed and Abdi, Mustapha Kamel and Alenezi, Mamdouh},  journal={IEEE Access},   title={Deep Learning for Software Vulnerabilities Detection Using Code Metrics},   year={2020},  volume={8},  number={},  pages={74562-74570},  doi={10.1109/ACCESS.2020.2988557}}

@article{Scandariato2014PredictingVS,
  title={Predicting Vulnerable Software Components via Text Mining},
  author={Riccardo Scandariato and James Walden and Aram Hovsepyan and Wouter Joosen},
  journal={IEEE Transactions on Software Engineering},
  year={2014},
  volume={40},
  pages={993-1006}
}

@inproceedings{ZHU,
author = {Zhou, Yaqin and Sharma, Asankhaya},
title = {Automated Identification of Security Issues from Commit Messages and Bug Reports},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117771},
doi = {10.1145/3106237.3117771},
abstract = {The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55\% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {914–919},
numpages = {6},
keywords = {machine learning, commit, vulnerability identification, bug report},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{ZIMMERMANN,
author = {undefinedliwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
title = {When Do Changes Induce Fixes?},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1083147},
doi = {10.1145/1082983.1083147},
abstract = {As a software system evolves, programmers make changes that sometimes cause problems. We analyze CVS archives for fix-inducing changes---changes that lead to problems, indicated by fixes. We show how to automatically locate fix-inducing changes by linking a version archive (such as CVS) to a bug database (such as BUGZILLA). In a first investigation of the MOZILLA and ECLIPSE history, it turns out that fix-inducing changes show distinct patterns with respect to their size and the day of week they were applied.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–5},
numpages = {5}
}

@inproceedings{10.1145/3106237.3117771,
author = {Zhou, Yaqin and Sharma, Asankhaya},
title = {Automated Identification of Security Issues from Commit Messages and Bug Reports},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117771},
doi = {10.1145/3106237.3117771},
abstract = {The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55\% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {914–919},
numpages = {6},
keywords = {vulnerability identification, bug report, commit, machine learning},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@INPROCEEDINGS{MINH,  author={Minh Le, Triet Huynh and Hin, David and Croft, Roland and Ali Babar, M.},  booktitle={2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},   title={DeepCVA: Automated Commit-level Vulnerability Assessment with Deep Multi-task Learning},   year={2021},  volume={},  number={},  pages={717-729},  doi={10.1109/ASE51524.2021.9678622}}

@inproceedings{VCCFinder,
author = {Perl, Henning and Dechand, Sergej and Smith, Matthew and Arp, Daniel and Yamaguchi, Fabian and Rieck, Konrad and Fahl, Sascha and Acar, Yasemin},
title = {VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813604},
doi = {10.1145/2810103.2813604},
abstract = {Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. However, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is sufficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flawfinder, to flag potentially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the amount of false alarms by over 99 \% at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {426–437},
numpages = {12},
keywords = {static analysis, machine learning, vulnerabilities},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@article{Riom2021RevisitingTV,
  title={Revisiting the VCCFinder approach for the identification of vulnerability-contributing commits},
  author={Timoth{\'e}e Riom and Arthur D. Sawadogo and Kevin Allix and Tegawend{\'e} F. Bissyand{\'e} and Naouel Moha and Jacques Klein},
  journal={Empir. Softw. Eng.},
  year={2021},
  volume={26},
  pages={46}
}

@inproceedings{DEFREEZ,
author = {DeFreez, Daniel and Thakur, Aditya V. and Rubio-Gonz\'{a}lez, Cindy},
title = {Path-Based Function Embedding and Its Application to Error-Handling Specification Mining},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236059},
doi = {10.1145/3236024.3236059},
abstract = {Identifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2<pre>vec</pre>, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2<pre>vec</pre> at identifying function synonyms in the Linux kernel. Finally, we apply Func2<pre>vec</pre> to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2<pre>vec</pre> result in error-handling specifications with high support.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {423–433},
numpages = {11},
keywords = {program analysis, program embeddings, program comprehension, specification mining, error handling},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@misc{DEVLIN,
  doi = {10.48550/ARXIV.1710.11054},
  
  url = {https://arxiv.org/abs/1710.11054},
  
  author = {Devlin, Jacob and Uesato, Jonathan and Singh, Rishabh and Kohli, Pushmeet},
  
  keywords = {Artificial Intelligence (cs.AI), Programming Languages (cs.PL), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Semantic Code Repair using Neuro-Symbolic Transformation Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@Article{PAN,
AUTHOR = {Pan, Cong and Lu, Minyan and Xu, Biao and Gao, Houleng},
TITLE = {An Improved CNN Model for Within-Project Software Defect Prediction},
JOURNAL = {Applied Sciences},
VOLUME = {9},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {2138},
URL = {https://www.mdpi.com/2076-3417/9/10/2138},
ISSN = {2076-3417},
ABSTRACT = {To improve software reliability, software defect prediction is used to find software bugs and prioritize testing efforts. Recently, some researchers introduced deep learning models, such as the deep belief network (DBN) and the state-of-the-art convolutional neural network (CNN), and used automatically generated features extracted from abstract syntax trees (ASTs) and deep learning models to improve defect prediction performance. However, the research on the CNN model failed to reveal clear conclusions due to its limited dataset size, insufficiently repeated experiments, and outdated baseline selection. To solve these problems, we built the PROMISE Source Code (PSC) dataset to enlarge the original dataset in the CNN research, which we named the Simplified PROMISE Source Code (SPSC) dataset. Then, we proposed an improved CNN model for within-project defect prediction (WPDP) and compared our results to existing CNN results and an empirical study. Our experiment was based on a 30-repetition holdout validation and a 10 * 10 cross-validation. Experimental results showed that our improved CNN model was comparable to the existing CNN model, and it outperformed the state-of-the-art machine learning models significantly for WPDP. Furthermore, we defined hyperparameter instability and examined the threat and opportunity it presents for deep learning models on defect prediction.},
DOI = {10.3390/app9102138}
}

@misc{ALADICS,
  doi = {10.48550/ARXIV.2110.04951},
  
  url = {https://arxiv.org/abs/2110.04951},
  
  author = {Aladics, Tamás and Jász, Judit and Ferenc, Rudolf},
  
  keywords = {Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Bug Prediction Using Source Code Embedding Based on Doc2Vec},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DOC2VEC,
  doi = {10.48550/ARXIV.1405.4053},
  
  url = {https://arxiv.org/abs/1405.4053},
  
  author = {Le, Quoc V. and Mikolov, Tomas},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Distributed Representations of Sentences and Documents},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DWF,
title = {Deep-water framework: The Swiss army knife of humans working with machine learning models},
journal = {SoftwareX},
volume = {12},
pages = {100551},
year = {2020},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2020.100551},
url = {https://www.sciencedirect.com/science/article/pii/S2352711019303772},
author = {Rudolf Ferenc and Tamás Viszkok and Tamás Aladics and Judit Jász and Péter Hegedűs},
keywords = {Deep-learning, Machine learning, Software analysis, Model management, Data visualization},
abstract = {Working with machine learning models has become an everyday task not only for software engineers, but for a much wider spectrum of researchers and professionals. Training such models involves finding the best learning methods and their best hyper-parameters for a specific task, keeping track of the achieved performance measures, comparing the results visually, etc. If we add feature extraction methods – that precede the learning phase and depend on many hyper-parameters themselves – into the mixture, like source code embedding that is quite common in the field of software analysis, the task cries out for supporting tools. We propose a framework called Deep-Water that works similarly to a configuration management tool in the area of software engineering. It supports defining arbitrary feature extraction and learning methods for an input dataset and helps in executing all the training tasks with different hyper-parameters in a distributed manner. The framework stores all circumstances, parameters and results of training, which can be filtered and visualized later. We successfully used the tool in several software analysis based prediction tasks, like vulnerability or bug prediction, but it is general enough to be applicable in other areas as well, e.g. NLP, image processing, or even other non-IT fields.}
}

@inproceedings{CC2VEC,
	doi = {10.1145/3377811.3380361},
  
	url = {https://doi.org/10.1145\%2F3377811.3380361},
  
	year = 2020,
	month = {jun},
  
	publisher = {{ACM}
},
  
	author = {Thong Hoang and Hong Jin Kang and David Lo and Julia Lawall},
  
	title = {{CC}2Vec},
  
	booktitle = {Proceedings of the {ACM}/{IEEE} 42nd International Conference on Software Engineering}
}

@ARTICLE{MCCABE,  author={McCabe, T.J.},  journal={IEEE Transactions on Software Engineering},   title={A Complexity Measure},   year={1976},  volume={SE-2},  number={4},  pages={308-320},  doi={10.1109/TSE.1976.233837}}

@conference{VULNDB,
author={Tamás Aladics. and Péter Hegedűs. and Rudolf Ferenc.},
title={A Vulnerability Introducing Commit Dataset for Java: An Improved SZZ based Approach},
booktitle={Proceedings of the 17th International Conference on Software Technologies - ICSOFT,},
year={2022},
pages={68-78},
publisher={SciTePress},
organization={INSTICC},
doi={10.5220/0011275200003266},
isbn={978-989-758-588-3},
issn={2184-2833},
}

@article{CODE2VEC,
 author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
 title = {Code2Vec: Learning Distributed Representations of Code},
 journal = {Proc. ACM Program. Lang.},
 issue_date = {January 2019},
 volume = {3},
 number = {POPL},
 month = jan,
 year = {2019},
 issn = {2475-1421},
 pages = {40:1--40:29},
 articleno = {40},
 numpages = {29},
 url = {http://doi.acm.org/10.1145/3290353},
 doi = {10.1145/3290353},
 acmid = {3290353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Big Code, Distributed Representations, Machine Learning},
}

@article{CTX_AWARE,
author = {Lin, Bo and Wang, Shangwen and Wen, Ming and Mao, Xiaoguang},
year = {2021},
month = {12},
pages = {1},
title = {Context-Aware Code Change Embedding for Better Patch Correctness Assessment},
journal = {ACM Transactions on Software Engineering and Methodology},
doi = {10.1145/3505247}
}

@inproceedings{
    CODE2SEQ,
    title={code2seq: Generating Sequences from Structured Representations of Code},
    author={Uri Alon and Shaked Brody and Omer Levy and Eran Yahav},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=H1gKYo09tX},
}


@InProceedings{STRUCTURAL_MODELS,
  title = 	 {Structural Language Models of Code},
  author =       {Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {245--256},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/alon20a/alon20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/alon20a.html},
  abstract = 	 {We address the problem of any-code completion - generating a missing piece of source code in a given program without any restriction on the vocabulary or structure. We introduce a new approach to any-code completion that leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling (SLM). SLM estimates the probability of the program’s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary code in any programming language. Our model significantly outperforms both seq2seq and a variety of structured approaches in generating Java and C# code. Our code, data, and trained models are available at http://github.com/tech-srl/slm-code-generation/. An online demo is available at http://AnyCodeGen.org.}
}

@misc{PATH_BASED_REPR,
  doi = {10.48550/ARXIV.1803.09544},
  
  url = {https://arxiv.org/abs/1803.09544},
  
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  
  keywords = {Programming Languages (cs.PL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A General Path-Based Representation for Predicting Program Properties},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{githubCorpus2013,
	author={Allamanis, Miltiadis and Sutton, Charles},
	title={{Mining Source Code Repositories at Massive Scale using Language Modeling}},
	booktitle={The 10th Working Conference on Mining Software Repositories},	
	year={2013},
	pages={207--216},
	organization={IEEE}
}

@article{rehurek2011gensim,
  title={Gensim--python framework for vector space modelling},
  author={Rehurek, Radim and Sojka, Petr},
  journal={NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic},
  volume={3},
  number={2},
  year={2011}
} 

@misc{nvd,
  author = {Harold Booth and Doug Rike and Gregory Witte},
  title = {The National Vulnerability Database (NVD): Overview},
  year = {2013},
  month = {2013-12-18},
  publisher = {ITL Bulletin, National Institute of Standards and Technology, Gaithersburg, MD},
  url = {https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=915172},
  language = {en},
}

@inproceedings{ponta2019msr,
    author={Serena E. Ponta and Henrik Plate and Antonino Sabetta and Michele Bezzi and
    C´edric Dangremont},
    title={A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software},
    booktitle={Proceedings of the 16th International Conference on Mining Software Repositories},
    year=2019,
    month=May,
}

@inproceedings{Borg_2019,
	doi = {10.1145/3340482.3342742},
  
	url = {https://doi.org/10.1145%2F3340482.3342742},
  
	year = 2019,
	publisher = {{ACM} Press},
  
	author = {Markus Borg and Oscar Svensson and Kristian Berg and Daniel Hansson},
  
	title = {{SZZ} unleashed: an open implementation of the {SZZ} algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
  
	booktitle = {Proceedings of the 3rd {ACM} {SIGSOFT} International Workshop on Machine Learning Techniques for Software Quality Evaluation  - {MaLTeSQuE} 2019}
}

@article{10.1145/1082983.1083147,
author = {undefinedliwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
title = {When Do Changes Induce Fixes?},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1083147},
doi = {10.1145/1082983.1083147},
abstract = {As a software system evolves, programmers make changes that sometimes cause problems. We analyze CVS archives for fix-inducing changes---changes that lead to problems, indicated by fixes. We show how to automatically locate fix-inducing changes by linking a version archive (such as CVS) to a bug database (such as BUGZILLA). In a first investigation of the MOZILLA and ECLIPSE history, it turns out that fix-inducing changes show distinct patterns with respect to their size and the day of week they were applied.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–5},
numpages = {5}
}

@inproceedings{szz,
author = {undefinedliwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
title = {When Do Changes Induce Fixes?},
year = {2005},
isbn = {1595931236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083142.1083147},
doi = {10.1145/1083142.1083147},
abstract = {As a software system evolves, programmers make changes that sometimes cause problems. We analyze CVS archives for fix-inducing changes---changes that lead to problems, indicated by fixes. We show how to automatically locate fix-inducing changes by linking a version archive (such as CVS) to a bug database (such as BUGZILLA). In a first investigation of the MOZILLA and ECLIPSE history, it turns out that fix-inducing changes show distinct patterns with respect to their size and the day of week they were applied.},
booktitle = {Proceedings of the 2005 International Workshop on Mining Software Repositories},
pages = {1–5},
numpages = {5},
location = {St. Louis, Missouri},
series = {MSR '05}
}

@article{commit2vec,
	doi = {10.1007/s42979-021-00566-z},
  
	url = {https://doi.org/10.1007%2Fs42979-021-00566-z},
  
	year = 2021,
	month = {mar},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {2},
  
	number = {3},
  
	author = {Roc{\'{\i}}o Cabrera Lozoya and Arnaud Baumann and Antonino Sabetta and Michele Bezzi},
  
	title = {Commit2Vec: Learning Distributed Representations of Code Changes},
  
	journal = {{SN} Computer Science}
}

@article{gan,
	doi = {10.1109/tnnls.2020.2978386},
  
	url = {https://doi.org/10.1109%2Ftnnls.2020.2978386},
  
	year = 2021,
	month = {jan},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {32},
  
	number = {1},
  
	pages = {4--24},
  
	author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  
	title = {A Comprehensive Survey on Graph Neural Networks},
  
	journal = {{IEEE} Transactions on Neural Networks and Learning Systems}
}

@inproceedings{NGUYEN,
author = {Nguyen, Viet Hung and Tran, Le Minh Sang},
title = {Predicting Vulnerable Software Components with Dependency Graphs},
year = {2010},
isbn = {9781450303408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1853919.1853923},
doi = {10.1145/1853919.1853923},
abstract = {Security metrics and vulnerability prediction for software have gained a lot of interests from the community. Many software security metrics have been proposed e.g., complexity metrics, cohesion and coupling metrics. In this paper, we propose a novel code metric based on dependency graphs to predict vulnerable components. To validate the efficiency of the proposed metric, we conduct a prediction model which targets the JavaScript Engine of Firefox. In this experiment, our prediction model has obtained a very good result in term of accuracy and recall rates. This empirical result is a good evidence showing dependency graphs are also a good option for early indicating vulnerability.},
booktitle = {Proceedings of the 6th International Workshop on Security Measurements and Metrics},
articleno = {3},
numpages = {8},
keywords = {vulnerability, prediction},
location = {Bolzano, Italy},
series = {MetriSec '10}
}

@ARTICLE{SHIN_EVAL,  author={Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A.},  journal={IEEE Transactions on Software Engineering},   title={Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities},   year={2011},  volume={37},  number={6},  pages={772-787},  doi={10.1109/TSE.2010.81}}