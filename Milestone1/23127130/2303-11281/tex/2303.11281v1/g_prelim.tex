%!TEX root = main.tex

\paragraph{\textbf{Graph Terminology}}
We begin with a brief introduction to the graph terminology we use in this paper.
Let $G=(V,E)$ be a graph.
For a subgraph $G'=(V',E')$ of $G$ we use $V(G')$ and $E(G')$ to denote $V'$ and $E'$, respectively.
We define the \emph{size} of a subgraph $G' \subseteq G$ as the number of its vertices, where we denote the size of $G$ by $n$.
For $v \in V$ we define $N(v)$ as its neighborhood, and $d(v)$ as the degree of $v$.
For a vertex subset $V' \subseteq V$ we define $G[V']$ as the induced subgraph of $V'$, $G-V' := G[V \setminus V']$ and $N(V') := \left(\bigcup_{v \in V'} N(v)\right) \setminus V'$.
% $G - V'$ as the subgraph obtaining after removing $V'$ from $G$, and $G[V']$ as the induced subgraph of $V'$ in $G$, i.e.~$G[V'] := G - (V \setminus V')$. 
% We define $N(V') := \left(\bigcup_{v \in V'} N(v)\right) \setminus V'$.
Finally, in the context of this work, we also use directed graphs in the sense of flow networks, where we move the corresponding terminology to the appendix next to the proofs.
%Here we use the well-known Ford-Fulkerson framework~\cite{ford1956maximal}.
%We denote a network $H$ by $(V,\ora{E},c)$, where $V$ is its set of vertices, $\ora{E}$ is the set of arcs, and $c \colon \ora{E} \to \N$ its capacity function.
%The main characteristic of a network is that its vertices contain $s$ and $t$, called the source and sink, respectively.
%The source vertex is characterized by having only outgoing arcs, while the sink vertex has only incoming arcs.
%We denote a flow by $Z$, i.e.~$Z := \left\{z_{\ora{e}} \in \N_{\geq 0} \mid \ora{e} \in \ora{E}\right\}$.
%A flow $Z$ is called feasible if $z_{\ora{e}} \leq c(\ora{e})$ for all $\ora{e} \in \ora{E}$ and if $\sum_{\ora{uv} \in  \ora{E}} z_{\ora{uv}} = \sum_{\ora{vw} \in  \ora{E}} z_{\ora{vw}}$ for all $v \in V \setminus \{s,t\}$.
%Usually, the challenge is to find a feasible flow that sends as much flow as possible from $s$ to $t$, where the total flow value can be extracted from $\sum_{\ora{ut} \in \ora{E}} z_{\ora{ut}} = \sum_{\ora{sv} \in \ora{E}} z_{\ora{sv}}$.
%The residual network $R = (V, \ora{E}')$ is defined with respect to a flow $Z$.
%For our purposes, we can simplify its definition in comparison as it is usually known: for $z_{\ora{uv}} \in Z$ we have $\ora{uv} \in \ora{E}'$ if $c(\ora{uv}) - z_{\ora{uv}} > 0$ and $\ora{vu} \in \ora{E}'$ if $z_{\ora{uv}} > 0$.
%An $s$-$t$-path in $R$ is called an augmenting path and gives the way how an $s$-$t$-flow with respect to $Z$ can be increased.  
%It is a well-known result that if there is no $s$-$t$-path in $R$, then $Z$ is a maximum flow for $s$ and $t$.

\paragraph{\textbf{Parameterized Terminology}}
We use the standard terminology for parameterized complexity, which is also used, for example, in \cite{downey2012parameterized,fomin2019kernelization}.
A \emph{parameterized problem} is a decision problem with respect to certain instance parameters.
Let $I$ be an instance of a parameterized problem with an instance parameter $k$, usually given as a pair $(I,k)$.
If for each pair $(I,k)$ there exists an algorithm that solves the decision problem in time $f(k) \cdot |I|^c$, where $f$ is a computable function and $c$ is a constant, then the parameterized problem is \emph{fixed-parameter tractable}.
We say $(I,k)$ is a \emph{yes-instance} if the answer to the decision problem is positive, otherwise we say $(I,k)$ is a \emph{no-instance}.

Of particular interest in this work are kernelizations, which can be roughly described as formalized preprocessings.
More formally, given an instance $(I,k)$ of a parameterized problem, a polynomial algorithm is called a kernelization if it maps any $(I,k)$ to an instance $(I',k')$ such that $(I',k')$ is a yes-instance if and only if $(I,k)$ is a yes-instance, $|I'| \leq g(k)$, and $k' \leq g'(k)$ for computable functions $g,g'$.

The idea of parameterized complexity can be extended by combining multiple parameters.
That is, if we consider an instance $I$ with parameters $k_1, \dots, k_m$, then we are interested in algorithms that solve the corresponding decision problem in a runtime of $f(k_1, \dots, k_m) \cdot |I|^c$, where $f$ is a computable function and $c$ is a constant.
We refer to runtimes that satisfy this type of form as \emph{FPT-times}. % $f(k_1, \dots, k_m) \cdot |I|^c$.


\paragraph{\textbf{Problem Statement and Objectives}}
First we introduce the \textit{$W$-separator problem}.
Given is a graph $G=(V,E)$ and two positive integers $k$ and $W$.
The challenge is to find a vertex subset $V' \subseteq V$, such that $V'$ has cardinality at most $k$ and the removal of $V'$ in $G$ leads to a graph that contains only connected components of size at most $W$.
The minimization problem is to find $V'$ with the smallest cardinality, where we denote the optimal objective value by $\opt$.
Note that we can reformulate the problem statement by demanding that $V'$ intersects with each connected subgraph of $G$ of size $W+1$.
In the case $W=1$ a separator needs to cover each edge, which shows that the $W$-separator problem is a natural generalization of the well-known vertex cover problem.

In terms of evolutionary algorithms, a solution to the $W$-separator problem can be represented in a bit sequence of length $n$. 
Each vertex has value zero or one, where one stands for the vertex being part of the $W$-separator.
Let $\{0,1\}^{n}$ be our solution space.
We work with multi-objective evolutionary algorithms, which evaluate each search point $X \in \{0,1\}^{n}$ using a fitness function $f \colon \{0, 1\}^{n} \to \R^m$ with $m$ different objectives.
The goal is to minimize each of the objectives.
Denote by $f^i(X)$ the $i$-th objective, evaluated at a search point $X$.
For two search points $X_1$ and $X_2$, we say $X_1$ \emph{weakly dominates} $X_2$ if $f^i(X_1) \leq f^i(X_2)$ for every $i \in [m]$, where $[m]$ is defined as the set $\{1,\dots,m\}$.
In this case, we simply write $f(X_1) \leq f(X_2)$.
If additionally $f(X_1) \neq f(X_2)$, then we say that $X_1$ \emph{dominates} $X_2$. 
We distinguish between pareto optimal search points $X$ and vectors $f(X)$. 
A pareto optimal search point is a search point that is not even weakly dominated by any other search point, whereas a pareto optimal vector is not dominated by any other vector.
That is, if $f(X_1)$ is a pareto optimal vector, then there can be a vector $X_2 \neq X_1$ with $f(X_2) = f(X_2)$, whereas if $X_1$ is a pareto optimal search point, then there is no search point $X_2 \neq X_1$ with $f(X_1) = f(X_2)$.

%For this purpose, we define $\sw_q(G')$
%for a subgraph $G' \subseteq G$ 
%as the set of all vertex sets that induces a connected subgraph of size $q$ in $G'$.
For some fitness functions we investigate, we use a linear program to evaluate the search points.
Let $G=(V,E)$ be an instance of the $W$-separator and let $y_v \in \{0,1\}$ be a variable for each $v \in V$.
An integer program (IP) that solves the $W$-separator problem can be formulated as follows:
%\begin{figure}
%	\caption{Linear program of the $W$-separator problem for a graph $G=(V,E)$}
\begin{align*}
	\min &\sum_{v \in V} y_v\\
	&\sum_{v \in S} y_v \geq 1, \forall S \subseteq V \colon  |S| = W+1 \text{ and } G[S] \text{ is connected }
	%& y_v \geq 0, \forall v \in V 
\end{align*}
%	\label{fig::LPWSep}
%\end{figure}
We will consider the relaxed version of the IP by allowing fractional solutions and consider the corresponding linear program (LP).
That is, instead of $y_v \in \{0,1\}$ we have $y_v \geq 0$ for all $v \in V$.
In the rest of this paper we will call it the \emph{$W$-separator LP}.
We define $\lpPrim(G')$ for a subgraph $G' \subseteq G$ as the objective of the $W$-separator LP with $G'$ as input graph.
If we put every connected subgraph of size $W+1$ as constraint in the LP formulation of the $W$-separator, then we end up with a running time of $n^{\O(W)}$.
However, as mentioned already in Fomin et.~al.~\cite{fomin2019kernelization}~(Section 6.4.2) finding an optimal solution for the LP can be sped up to a running time of $2^{\O(W)} n^{\O(1)}$.
Roughly speaking, the idea is to use the ellipsoid method with separation oracles to solve the linear program, where the separation oracle uses a method called color coding that makes it tractable in $W$.

%An optimal solution for the LP can be also found in time $2^{W} \cdot n^{\O(1)}$ by using a method called color coding.
%Roughly, the idea is that we do not need any connected subgraph of size $W+1$, where the right ones can be filtered by using separation oracles and the ellipsoid method to solve the LP.
%By applying color coding we can capture the violated constraints through a dynamic program tractable in $W$.
% \zienain{
% 	In the book it is unclear to me how this should work, maybe: "Roughly, the idea is that we do not need any connected subgraph of size $W+1$, where the right ones can be filtered by using separation oracles and the ellipsoid method to solve the LP. By applying color coding we can capture the violated constraints through a dynamic program tractable in $W$." 
	
% 	Or they mean a different technique. If we want to take it out we should say either that $W$ is a constant or that an equivalent instance guarantees that we need at most $(W+1)^{\opt}$ constraints.
% }

Next, we define few additional terms before we get to the multiobjective fitness functions.
Let $X \in \{0,1\}^n$ be a search point.
For $v \in V$ we define $x_v \in \{0,1\}$ as the corresponding value in the bit-string $X$. 
We denote by $X_1 \subseteq V$ the vertices with value one.
We define $u(X)$ as the set of vertices that are in components of size at least $W+1$ after the removal of $X_1$ in $G$.
The function $u(X)$ can be interpreted as the uncovered portion of the graph with respect to the vertices $X_1$.
The fitness functions we work with are the following:

\begin{itemize}
	\item $f_1(X) := \left(|X_1|, |u(X)|, -\sum_{v \in X_1} d(v)\right)$
	\item $f_2(X) := \left(|X_1|, |u(X)|, \lpPrim(G[u(X)])\right)$
	\item $f_3(X) := \left(|X_1|, \lpPrim(G[u(X)])\right)$
\end{itemize}

As the names suggest, we use \emph{one-objective}, \emph{uncovered-objective}, \emph{degree-objective} and \emph{LP-objective} to denote $|X_1|,|u(X)|, -\sum_{v \in X_1} d(v)$ and $\lpPrim(G[u(X)])$ respectively.
Note that the fitness $f_3$ is same as $f_2$ without the uncovered-objective.
Furthermore, we use $*$ to denote that an objective can be chosen arbitrarily, for instance in $(|X_1|,*,-\sum_{v \in X_1} d(v))$ the uncovered-objective $u(X)$ is arbitrarily.

\paragraph{\textbf{Algorithms}}
We proceed by presenting the algorithms that we study.
All of them are based on \algGlobalSemo (see Algorithm \ref{alg:GlobalSemo}), which maintains a population $\P \subseteq \{0, 1\}^{n}$ of $n$-dimensional bit strings.

%\ziena{Marcus: Maybe reference to first appearance of the algo in the literature?}

\begin{algorithm}
	\caption{\algGlobalSemo} \label{alg:GlobalSemo}
	
	Choose $X \in \{0,1\}^n$ uniformly at random
	
	$\P \gets \{X\}$
	
	\While{stopping criterion not met}
	{
		Choose $X  \in \P$ unformly at random
		
		$Y \gets$ flip each bit of $X$ independently with probability $1/n$
		\label{algGlobSemo::Mutation}
		
		
		If $Y$ is not dominated by any other search point in $\P$, include $Y$ into $\P$ and delete all other bit strings $Z \in \P$ which are weakly dominated by $X$ from $\P$, i.e.~those with $f(Y) \leq f(Z)$.
	}	
\end{algorithm}


\begin{algorithm}
	\caption{\algAltMut} \label{alg:altMut}
	
	Choose $b \in \{0,1,2\}$ uniformly at random
	
	\uIf{$b=2$ \textbf{and} $u(X) \neq \varnothing$}
	{
		$Y \gets$ for $v \in u(X)$ flip each bit $x_v$ with probability $1/2$ 
	}
	
	\uElseIf{$b=1$ \textbf{and} $X_1 \neq \varnothing$}
	{
		$Y \gets$ for $v \in X_1$ flip each bit $x_v$ with probability $1/2$ 
	}
	
	\Else
	{
		$Y \gets$ flip each bit of $X$ independently with probability $1/n$
	}
\end{algorithm}

We define the Algorithm \algGlobalSemoAlt similarly to the Algorithm \algGlobalSemo (see Algorithm~\ref{alg:GlobalSemo})  
with the difference that the mutation in line~\ref{algGlobSemo::Mutation} is exchanged by \algAltMut (see Algorithm~\ref{alg:altMut}). 
The following two lemmas will be useful throughout the whole paper.
Their proofs are similar to some appearing in~\cite{DBLP:journals/algorithmica/KratschN13}, and due to space constraints we have moved them to the appendix (Section~\ref{appendix::prelim}).

\begin{lemma}
	\label{lemma::singleFlipAndBoundedPop}
	Let $\P \neq \varnothing$ be a population for the fitness functions $f_1$ and $f_2$.
	In the Algorithms \algGlobalSemo and \algGlobalSemoAlt,
	selecting a certain search point $X \in \P$ has probability $\Omega(1/n^2)$, and additionally flipping only one single bit in it has probability $\Omega(1/n^3)$.
\end{lemma}
%\begin{proof}
%	We start to bound the size of the population $\P$.
%	Given $f_1$ or $f_2$ for the bit sequences $\P$, the first and second entries each have at most $(n+1)$ distinct values, and we keep at most one for each possibility.
%	Therefore, we have $|\P| \leq (n+1)^2$ and thus selecting a certain search point $X \in \P$ has probability $\Omega(1/n^2)$.
%	In \algGlobalSemo we mutate every bit in $X$ with probability $1/n$.
%	That is, we obtain a probability of $1/n \cdot (1 - 1/n)^{n-1} \geq 1/ne \in \Omega(1/n)$ to flip a certain bit in $X$.
%	This probability decreases only by a constant factor of $1/3$ in \algGlobalSemoAlt and keeps therefore the probability by $\Omega(1/n)$ for this event.
%	As a result, selecting a certain search point $X \in \P$ and flipping only one single bit in it has probability $\Omega(1/n^3)$ for both algorithms.
%\end{proof}

Let $0^n$ be the search point that contains only zeroes.
Note that once $0^n$ is in the population it is pareto optimal for all fitness functions because of the one-objective.
%, since one objective in all of them is to minimize the number of selected ones. 



\begin{lemma}
	\label{lemma::zeroSol}
	Using the the fitness functions $f_1$ or $f_2$, the expected number of iterations of \algGlobalSemo or \algGlobalSemoAlt until the population $\P$ contains the search point $0^n$ is $\O(n^3 \log n)$.
\end{lemma}

%\begin{proof}
%	Let $\P \neq \varnothing$ and let $X^{\min} \in \P$ be the bit string in $\P$ with the minimal number of ones, where $i := |X^{\min}_1|$.
%	By \cref{lemma::singleFlipAndBoundedPop} the probability that $X^{\min}$ is chosen in one iteration of both algorithms is $\Omega(1/n^2)$.
%	If we consider \algGlobalSemo, then the probability that the mutation of $X^{\min}_1$ results in a bit string $X$ with $|X_1| < i$ is at least 
%	$i/n \cdot (1 - 1/n)^{n-1} \geq i/ne$, which is the probability that only one 1-bit is flipped and nothing else.
%	Note that the probability changes by a factor of $1/3$ in the algorithm \algGlobalSemoAlt.
%	Thus, the probability for both algorithms that the population gets after an iteration a bit string that has less number of ones compared to the previous population is $\Omega(i/n^3)$.
%	This in turn means that the expected number of iterations this happens is $\O(n^3/i)$.  
%	Using the method of fitness based partitions \cite{DBLP:conf/ppsn/Sudholt10} and summing up over the different values of $i$ we obtain an expected time of $\sum_{i=1}^{n} \O(n^3/i) = \O(n^3 \log n)$ that $0^n$ is in $\P$.
%\end{proof}

The following lemmas are proven analogously to \cref{lemma::singleFlipAndBoundedPop,lemma::zeroSol} by observing that the worst-case bounds on the population size decrease by a factor of $n$ when using fitness function $f_3$ instead of $f_1$ or $f_2$.

\begin{lemma}
	\label{corollary::singleFlipAndBoundedPop}
	Let $\P \neq \varnothing$ be a population for the fitness function $f_3$.
	In the Algorithms \algGlobalSemo and \algGlobalSemoAlt,
	selecting a certain search point $X \in \P$ has probability $\Omega(1/n)$, and additionally flipping only one single bit in it has probability $\Omega(1/n^2)$.
\end{lemma}

\begin{lemma}
	\label{corollary::zeroSol}
	Using the the fitness function $f_3$, the expected number of iterations of \algGlobalSemo or \algGlobalSemoAlt until the population $\P$ contains the search point $0^n$ is $\O(n^2 \log n)$.
\end{lemma}