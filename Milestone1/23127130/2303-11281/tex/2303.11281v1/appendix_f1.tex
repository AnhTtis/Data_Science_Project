%!TEX root = main.tex

\subsection*{Proof of \cref{lemma::reducedInstance_f1}}
\begin{proof}
	Let $V' = \{v_1, \dots, v_\ell\} \subseteq V$ be the vertices with degree larger than $k+W$ (\emph{reducible vertices}), such that $d(v_i) \geq d(v_j)$ for $i>j$.
	Observe that if $V' = \varnothing$, then $0^n$ is already the desired search point and we are done by \cref{lemma::zeroSol}.
	For $i \in \{0,1,\dots,\ell\}$ let $X^i$ be a search point with $|X^i_1| = i$ and $\sum_{v \in X^i_1} d(v) = \sum_{j=1}^i d(v_j)$.
	In particular, $X^\ell$ is the desired search point according to the lemma.
	First observe that $X^i$ can only be dominated by a search point $X$ if $|X_1| \leq |X^i_1|$ and $-\sum_{v \in X_1} d(v) \leq - \sum_{v \in X^i_1} d(v)$,
	which is only possible if $|X_1| = i = |X^i_1|$ and $-\sum_{v \in X} d(v) = - \sum_{v \in X'_1} d(v)$ as $X^i_1$ contains only the vertices of largest degree.
	Consequently, $X^i \subseteq V'$ and once $X^i$ is in the population $\P$ the vector $(|X^i_1|, *, -\sum_{v \in X_1} d(v))$ is pareto optimal.
	
	Let $\P$ be a population with $0^n \in \P$ and let $s < \ell$ be the largest integer such that $X^s \in \P$.
	Note that $X^0 = 0^n$.
	Let $v \in u(X^s)$ be a vertex that satisfies $d(v) = \max_{u \in u(X^s)} d(u)$.
	By \cref{lemma::singleFlipAndBoundedPop} \algGlobalSemoAlt flips only a certain bit from a certain search point of $\P$ with probability $\Omega(1/n^3)$ and
	thus, mutating $X^s$ to a search point $X^{s+1}$ takes in expectation $\O(n^3)$ iterations.
	Using the method of fitness based partitions \cite{DBLP:conf/ppsn/Sudholt10} and summing up over the different values of $s$ leads to $\sum_{i=1}^{\ell} \O(n^3) \leq \sum_{i=1}^{\opt} \O(n^3) = \O(\opt \cdot n^3)$ expected number of iterations having $X^\ell$ in the population $\P$, once $0^n \in \P$.
	By \cref{lemma::zeroSol} the expected number of iterations such that $0^n$ is in the population $\P$ is $\O(n^3 \log n)$.
	As a result, we have $X^\ell \in \P$ after $\O(n^3(\opt + \log n))$ iterations in expectation. 
\end{proof}

\subsection*{Proof of \cref{thm::fitness1Opt}:}
\begin{proof}
First observe that once we have a search point $X$ according to \cref{lemma::reducedInstance_f1} in the population that this is pareto optimal, since there is no other search point with same or less number of selected vertices that yields to a smaller value of $-\sum_{v \in X_1} d(v)$.
On the other hand, by \cref{thm::degreeKernel} we know that the uncovered-objective $u(X)$ has size at most $q=\opt \cdot W(\opt+W)+\opt$
while $X_1$ contains only vertices that have to be in the optimal solution.
In expectation, $X$ is in the population after $\O(n^3(\opt + \log n))$ iterations of \algGlobalSemoAlt (cf.~\cref{lemma::reducedInstance_f1}).
Thus, given $X$ is selected in the Algorithm~\algGlobalSemoAlt we obtain a probability of $1/3 \cdot 2^{-q}$ flipping $m \leq \opt$ vertices of $u(X)$ that lead to an optimal solution. % and therefore, given $X$ is selected this event happens in expectation after $2^{kW(k+W)+k}$ iterations of \algGlobalSemoAlt.
That is, if $X \in \P$ we have a probability of $\Omega\left(1/n^2 \cdot 2^{-q}\right)$ reaching the optimal solution in one iteration, where the additional factor $1/n^2$ comes from selecting $X$ (cf.~\cref{lemma::singleFlipAndBoundedPop}).
Consequently, the expected number of iterations reaching an optimal solution is upper bounded by $\O\left(n^3(\opt + \log n) + n^2 \cdot 2^q \right)$.
\end{proof}