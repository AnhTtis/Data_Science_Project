%!TEX root = main.tex

\paragraph{\textbf{Graph Terminology}}
We begin with a brief introduction to the graph terminology we use in this paper.
Let $G=(V,E)$ be a graph.
For a subgraph $G'=(V',E')$ of $G$ we use $V(G')$ and $E(G')$ to denote $V'$ and $E'$, respectively.
We define the \emph{size} of a subgraph $G' \subseteq G$ as the number of its vertices, where we denote the size of $G$ by $n$.
For $v \in V$ we define $N(v)$ as its neighborhood, and $d(v)$ as the degree of $v$.
For a vertex subset $V' \subseteq V$ we define $G[V']$ as the induced subgraph of $V'$, $G-V' := G[V \setminus V']$ and $N(V') := \left(\bigcup_{v \in V'} N(v)\right) \setminus V'$.
Finally, in the context of this work, we also use directed graphs in the sense of flow networks, where we move the corresponding terminology to the appendix next to the proofs.

\paragraph{\textbf{Parameterized Terminology}}
We use the standard terminology for parameterized complexity, which is also used, for example, in \cite{downey2012parameterized,fomin2019kernelization}.
A \emph{parameterized problem} is a decision problem with respect to certain instance parameters.
Let $I$ be an instance of a parameterized problem with an instance parameter $k$, usually given as a pair $(I,k)$.
If for each pair $(I,k)$ there exists an algorithm that solves the decision problem in time $f(k) \cdot |I|^c$, where $f$ is a computable function and $c$ is a constant, then the parameterized problem is \emph{fixed-parameter tractable}.
We say $(I,k)$ is a \emph{yes-instance} if the answer to the decision problem is positive, otherwise we say $(I,k)$ is a \emph{no-instance}.

Of particular interest in this work are kernelizations, which can be roughly described as formalized preprocessings.
More formally, given an instance $(I,k)$ of a parameterized problem, a polynomial algorithm is called a kernelization if it maps any $(I,k)$ to an instance $(I',k')$ such that $(I',k')$ is a yes-instance if and only if $(I,k)$ is a yes-instance, $|I'| \leq g(k)$, and $k' \leq g'(k)$ for computable functions $g,g'$.

The idea of parameterized complexity can be extended by combining multiple parameters.
That is, if we consider an instance $I$ with parameters $k_1, \dots, k_m$, then we are interested in algorithms that solve the corresponding decision problem in a runtime of $f(k_1, \dots, k_m) \cdot |I|^c$, where $f$ is a computable function and $c$ is a constant.
We refer to runtimes that satisfy this type of form as \emph{FPT-times}. % $f(k_1, \dots, k_m) \cdot |I|^c$.


\paragraph{\textbf{Problem Statement and Objectives}}
First we introduce the \textit{$W$-separator problem}.
Given is a graph $G=(V,E)$ and two positive integers $k$ and $W$.
The challenge is to find a vertex subset $V' \subseteq V$, such that $V'$ has cardinality at most $k$ and the removal of $V'$ in $G$ leads to a graph that contains only connected components of size at most $W$.
The minimization problem is to find $V'$ with the smallest cardinality, where we denote the optimal objective value by $\opt$.
Note that we can reformulate the problem statement by demanding that $V'$ intersects with each connected subgraph of $G$ of size $W+1$.
In the case $W=1$ a separator needs to cover each edge, which shows that the $W$-separator problem is a natural generalization of the well-known vertex cover problem.

In terms of evolutionary algorithms, a solution to the $W$-separator problem can be represented in a bit sequence of length $n$. 
Each vertex has value zero or one, where one stands for the vertex being part of the $W$-separator.
Let $\{0,1\}^{n}$ be our solution space.
We work with multi-objective evolutionary algorithms, which evaluate each search point $X \in \{0,1\}^{n}$ using a fitness function $f \colon \{0, 1\}^{n} \to \R^m$ with $m$ different objectives.
The goal is to minimize each of the objectives.
Denote by $f^i(X)$ the $i$-th objective, evaluated at a search point $X$.
For two search points $X_1$ and $X_2$, we say $X_1$ \emph{weakly dominates} $X_2$ if $f^i(X_1) \leq f^i(X_2)$ for every $i \in [m]$, where $[m]$ is defined as the set $\{1,\dots,m\}$.
In this case, we simply write $f(X_1) \leq f(X_2)$.
If additionally $f(X_1) \neq f(X_2)$, then we say that $X_1$ \emph{dominates} $X_2$. 
We distinguish between pareto optimal search points $X$ and vectors $f(X)$. 
A pareto optimal search point is a search point that is not even weakly dominated by any other search point, whereas a pareto optimal vector is not dominated by any other vector.
That is, if $f(X_1)$ is a pareto optimal vector, then there can be a vector $X_2 \neq X_1$ with $f(X_2) = f(X_2)$, whereas if $X_1$ is a pareto optimal search point, then there is no search point $X_2 \neq X_1$ with $f(X_1) = f(X_2)$.

For some fitness functions we investigate, we use a linear program to evaluate the search points.
Let $G=(V,E)$ be an instance of the $W$-separator and let $y_v \in \{0,1\}$ be a variable for each $v \in V$.
An integer program (IP) that solves the $W$-separator problem can be formulated as follows:
\begin{align*}
	\min &\sum_{v \in V} y_v\\
	&\sum_{v \in S} y_v \geq 1, \forall S \subseteq V \colon  |S| = W+1 \text{ and } G[S] \text{ is connected }
	%& y_v \geq 0, \forall v \in V 
\end{align*}
We will consider the relaxed version of the IP by allowing fractional solutions and consider the corresponding linear program (LP).
That is, instead of $y_v \in \{0,1\}$ we have $y_v \geq 0$ for all $v \in V$.
In the rest of this paper we will call it the \emph{$W$-separator LP}.
We define $\lpPrim(G')$ for a subgraph $G' \subseteq G$ as the objective of the $W$-separator LP with $G'$ as input graph.
If we put every connected subgraph of size $W+1$ as constraint in the LP formulation of the $W$-separator, then we end up with a running time of $n^{\O(W)}$.
However, as mentioned already in Fomin et.~al.~\cite{fomin2019kernelization}~(Section 6.4.2) finding an optimal solution for the LP can be sped up to a running time of $2^{\O(W)} n^{\O(1)}$.
Roughly speaking, the idea is to use the ellipsoid method with separation oracles to solve the linear program, where the separation oracle uses a method called color coding that makes it tractable in $W$.

Next, we define few additional terms before we get to the multiobjective fitness functions.
Let $X \in \{0,1\}^n$ be a search point.
For $v \in V$ we define $x_v \in \{0,1\}$ as the corresponding value in the bit-string $X$. 
We denote by $X_1 \subseteq V$ the vertices with value one.
We define $u(X)$ as the set of vertices that are in components of size at least $W+1$ after the removal of $X_1$ in $G$.
The function $u(X)$ can be interpreted as the uncovered portion of the graph with respect to the vertices $X_1$.
The fitness functions we work with are the following:

\begin{itemize}
	\item $f_1(X) := \left(|X_1|, |u(X)|, -\sum_{v \in X_1} d(v)\right)$
	\item $f_2(X) := \left(|X_1|, |u(X)|, \lpPrim(G[u(X)])\right)$
	\item $f_3(X) := \left(|X_1|, \lpPrim(G[u(X)])\right)$
\end{itemize}

As the names suggest, we use \emph{one-objective}, \emph{uncovered-objective}, \emph{degree-objective} and \emph{LP-objective} to denote $|X_1|,|u(X)|, -\sum_{v \in X_1} d(v)$ and $\lpPrim(G[u(X)])$ respectively.
Note that the fitness $f_3$ is same as $f_2$ without the uncovered-objective.
Furthermore, we use $*$ to denote that an objective can be chosen arbitrarily, for instance in $(|X_1|,*,-\sum_{v \in X_1} d(v))$ the uncovered-objective $u(X)$ is arbitrarily.

\paragraph{\textbf{Algorithms}}
We proceed by presenting the algorithms that we study.
All of them are based on \algGlobalSemo (see Algorithm \ref{alg:GlobalSemo}), which maintains a population $\P \subseteq \{0, 1\}^{n}$ of $n$-dimensional bit strings.

\begin{algorithm}
	\caption{\algGlobalSemo} \label{alg:GlobalSemo}
	
	Choose $X \in \{0,1\}^n$ uniformly at random
	
	$\P \gets \{X\}$
	
	\While{stopping criterion not met}
	{
		Choose $X  \in \P$ unformly at random
		
		$Y \gets$ flip each bit of $X$ independently with probability $1/n$
		\label{algGlobSemo::Mutation}
		
		
		If $Y$ is not dominated by any other search point in $\P$, include $Y$ into $\P$ and delete all other bit strings $Z \in \P$ which are weakly dominated by $X$ from $\P$, i.e.~those with $f(Y) \leq f(Z)$.
	}	
\end{algorithm}


\begin{algorithm}
	\caption{\algAltMut} \label{alg:altMut}
	
	Choose $b \in \{0,1,2\}$ uniformly at random
	
	\uIf{$b=2$ \textbf{and} $u(X) \neq \varnothing$}
	{
		$Y \gets$ for $v \in u(X)$ flip each bit $x_v$ with probability $1/2$ 
	}
	
	\uElseIf{$b=1$ \textbf{and} $X_1 \neq \varnothing$}
	{
		$Y \gets$ for $v \in X_1$ flip each bit $x_v$ with probability $1/2$ 
	}
	
	\Else
	{
		$Y \gets$ flip each bit of $X$ independently with probability $1/n$
	}
\end{algorithm}

We define the Algorithm \algGlobalSemoAlt similarly to the Algorithm \algGlobalSemo (see Algorithm~\ref{alg:GlobalSemo})  
with the difference that the mutation in line~\ref{algGlobSemo::Mutation} is exchanged by \algAltMut (see Algorithm~\ref{alg:altMut}). 
The following two lemmas will be useful throughout the whole paper.
Their proofs are similar to some appearing in~\cite{DBLP:journals/algorithmica/KratschN13}, and due to space constraints we have moved them to the appendix (Section~\ref{appendix::prelim}).

\begin{lemma}
	\label{lemma::singleFlipAndBoundedPop}
	Let $\P \neq \varnothing$ be a population for the fitness functions $f_1$ and $f_2$.
	In the Algorithms \algGlobalSemo and \algGlobalSemoAlt,
	selecting a certain search point $X \in \P$ has probability $\Omega(1/n^2)$, and additionally flipping only one single bit in it has probability $\Omega(1/n^3)$.
\end{lemma}

Let $0^n$ be the search point that contains only zeroes.
Note that once $0^n$ is in the population it is pareto optimal for all fitness functions because of the one-objective.

\begin{lemma}
	\label{lemma::zeroSol}
	Using the the fitness functions $f_1$ or $f_2$, the expected number of iterations of \algGlobalSemo or \algGlobalSemoAlt until the population $\P$ contains the search point $0^n$ is $\O(n^3 \log n)$.
\end{lemma}

The following lemmas are proven analogously to \cref{lemma::singleFlipAndBoundedPop,lemma::zeroSol} by observing that the worst-case bounds on the population size decrease by a factor of $n$ when using fitness function $f_3$ instead of $f_1$ or $f_2$.

\begin{lemma}
	\label{corollary::singleFlipAndBoundedPop}
	Let $\P \neq \varnothing$ be a population for the fitness function $f_3$.
	In the Algorithms \algGlobalSemo and \algGlobalSemoAlt,
	selecting a certain search point $X \in \P$ has probability $\Omega(1/n)$, and additionally flipping only one single bit in it has probability $\Omega(1/n^2)$.
\end{lemma}

\begin{lemma}
	\label{corollary::zeroSol}
	Using the the fitness function $f_3$, the expected number of iterations of \algGlobalSemo or \algGlobalSemoAlt until the population $\P$ contains the search point $0^n$ is $\O(n^2 \log n)$.
\end{lemma}