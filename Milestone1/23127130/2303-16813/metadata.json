{
    "arxiv_id": "2303.16813",
    "paper_title": "Optimal approximation of $C^k$-functions using shallow complex-valued neural networks",
    "authors": [
        "Paul Geuchen",
        "Felix Voigtlaender"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "math.FA",
        "cs.LG",
        "stat.ML"
    ],
    "abstract": "We prove a quantitative result for the approximation of functions of regularity $C^k$ (in the sense of real variables) defined on the complex cube $Ω_n := [-1,1]^n +i[-1,1]^n\\subseteq \\mathbb{C}^n$ using shallow complex-valued neural networks. Precisely, we consider neural networks with a single hidden layer and $m$ neurons, i.e., networks of the form $z \\mapsto \\sum_{j=1}^m σ_j \\cdot φ\\big(ρ_j^T z + b_j\\big)$ and show that one can approximate every function in $C^k \\left( Ω_n; \\mathbb{C}\\right)$ using a function of that form with error of the order $m^{-k/(2n)}$ as $m \\to \\infty$, provided that the activation function $φ: \\mathbb{C} \\to \\mathbb{C}$ is smooth but not polyharmonic on some non-empty open set. Furthermore, we show that the selection of the weights $σ_j, b_j \\in \\mathbb{C}$ and $ρ_j \\in \\mathbb{C}^n$ is continuous with respect to $f$ and prove that the derived rate of approximation is optimal under this continuity assumption. We also discuss the optimality of the result for a possibly discontinuous choice of the weights.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16813v1"
    ],
    "publication_venue": null
}