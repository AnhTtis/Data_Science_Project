% !TeX encoding = UTF-8
% !TeX spellcheck = en_US
% !TeX root = main_paper.tex

\section{Divided Differences}
Divided differences are well-known in numerical mathematics as they are for example used to calculate the coefficients of an interpolation polynomial in its Newton representation. In our case, we are interested in divided differences, since they can be used to obtain a generalization of the classical mean-value theorem for differentiable functions: Given an interval $I \subseteq \RR$ and $n+1$ pairwise distinct data points $x_0, ..., x_n \in I$ as well as an $n$-times differentiable real-valued function $f : I \to \RR$, there exists $\xi \in \left( \text{min}\left\{x_0 ,..., x_n\right\}, \text{max}\left\{x_0 ,..., x_n\right\}\right)$, such that
\begin{equation*}
    f\left[x_0, ..., x_n\right] = \frac{f^{(n)}(\xi)}{n!},
\end{equation*}
where the left-hand side is a divided difference of $f$ (defined below). The classical mean-value theorem is obtained in the case $n=1$. Our goal in this section is to generalize this result to a multivariate setting by considering divided differences in multiple variables. Such a generalization is probably well-known, but since we could not locate a convenient reference and to make the paper more self-contained, we provide a proof.

Let us first define divided differences formally. Given $n+1$ data points $\left(x_0, y_0\right), ..., \left(x_n, y_n\right) \in \RR \times \RR$ with pairwise distinct $x_k$, we define the associated divided differences recursively via
\begin{align*}
    \left[ y_k\right] &\defeq y_k, \ k \in \{0,...,n\}, \\
    \left[y_k ,..., y_{k+j}\right] &\defeq \frac{\left[y_{k+1},..., y_{k+j}\right] - \left[y_k,..., y_{k+j-1}\right]}{x_{k+j}-x_k}, \ j \in \{1,...,n\}, \ k \in \{0, ..., n-j\}.
\end{align*}
If the data points are defined by a function $f$ (i.e. $y_k = f\left(x_k\right)$ for all $k \in \{0,...,n\}$), we write
\begin{equation*}
    \left[x_k,...,x_{k+j}\right] f \defeq \left[y_k, ..., y_{k+j}\right].
\end{equation*}
We first consider an alternative representation of divided differences, the so-called \emph{Hermite-Genocchi-Formula}. To state it, we introduce the notation $\Sigma^k$ for a certain $k$-dimensional simplex.
\begin{definition}
    Let $s \in \NN$. Then we define
    \begin{equation*}
        \Sigma^s \defeq \left\{ x \in \RR^s : \ x_\ell \geq 0 \ \mathrm{ for} \ \mathrm{all}  \ \ell \ \mathrm{ and  } \sum_{\ell = 1}^s x_\ell \leq 1 \right\}.
    \end{equation*}
    The identity $\lambda^s\left(\Sigma^s\right)= \frac{1}{s!}$ holds true.
\end{definition}
A proof for the fact that the volume of $\Sigma^s$ is indeed $\frac{1}{s!}$ can be found for example in \cite{stein_note_1966}.
We can now consider the alternative representation of divided differences.
\begin{lemma}[Hermite-Genocchi-Formula]
    For real numbers $a,b \in \RR$, a function $f \in C^k([a,b]; \RR)$ and pairwise distinct $x_0, ..., x_k \in [a,b]$, the divided difference of $f$ at the data points $x_0, ..., x_k$ is given as
\begin{equation}
\label{hg}
    \left[x_0, ..., x_k\right]f = \int_{\Sigma^k} f^{(k)}\left(x_0 + \sum_{\ell=1}^{k}s_\ell\left(x_\ell-x_0\right)\right) ds.
\end{equation}
\end{lemma}
\begin{proof}
    See \cite[Theorem 3.3]{atkinson_introduction_1989}.
\end{proof}
We need the following easy generalization of the mean-value theorem for integrals.
\begin{lemma}
Let $D \subseteq \RR^s$ be a connected and compact set with positive Lesbesgue measure and furthermore $f : D \to \RR$ a continuous function. Then there exists $\xi \in D$ such that
\begin{equation*}
    f(\xi) = \frac{1}{\lambda(D)} \cdot \int_D f(x) dx.
\end{equation*}
\end{lemma}
\begin{proof} 
Since $D$ is compact and $f$ continuous, there exist $x_{\text{min}} \in D$ and $x_{\text{max}} \in D$ satisfying
\begin{equation*}
    f\left(x_{\text{min}}\right) \leq f(x) \leq f\left(x_{\text{max}} \right)
\end{equation*}
for all $x \in D$. Thus, one gets
\begin{equation*}
    f\left( x_{\text{min}}\right) \leq \frac{1}{\lambda(D)} \int_D f(x) dx \leq  f\left( x_{\text{max}}\right)
\end{equation*}
so the claim follows using the fact that $f(D) \subseteq \RR$ is connected, i.e., an interval.
\end{proof}
We also get a convenient representation of divided differences if the data points are equidistant.
\begin{lemma}
Let $f: \RR \to \RR$, $x_0 \in \RR$ and $h > 0$. We consider the case of equidistant data points, meaning $x_{j} \defeq x_0 + jh$ for all $j = 1,...,k$ for a fixed $h > 0$. In this case, we have the formula
\begin{equation}
\label{alternativdarstellung}
    \left[x_0, ..., x_k\right]f = \frac{1}{k!h^k} \cdot \sum_{r=0}^k (-1)^{k-r}\binom{k}{r} f\left(x_r\right). 
\end{equation}
\end{lemma}
\begin{proof}
We prove the result via induction over the number $j$ of considered data points, meaning the following: For all $j \in \{0,...,k\}$ we have
\begin{equation*}
    \left[x_\ell, ..., x_{\ell+j}\right]f = \frac{1}{j!h^j} \cdot \sum_{r=0}^j (-1)^{j-r}\binom{j}{r} f\left(x_{\ell+r}\right)
\end{equation*}
for all $\ell \in \{0, ..., k\}$ such that $\ell + j \leq k$. The case $j = 0$ is trivial. Therefore, we assume the claim to be true for a fixed $j \in \{0,...,k-1\}$ and choose an arbitrary $\ell \in \{0,...,k\}$ such that $\ell+j+1 \leq k$. We then get
\begin{align*}
    \left[x_\ell, ..., x_{\ell+j+1}\right]f &= \frac{\left[x_{\ell+1}, ..., x_{\ell+j+1}\right]f - \left[x_\ell, ..., x_{\ell+j}\right]f}{x_{\ell+j+1}-x_\ell} \\
    &= \frac{1}{j!h^j}\cdot \frac{\sum_{r=0}^j (-1)^{j-r}\binom{j}{r} \left(f\left(x_{\ell+r+1}\right) - f\left(x_{\ell+r}\right)\right)}{(j+1)h} \\
    &= \frac{1}{(j+1)!h^{j+1}}\sum_{r=0}^j (-1)^{j-r}\binom{j}{r} \left(f\left(x_{\ell+r+1}\right) - f\left(x_{\ell+r}\right)\right).
\end{align*}
Using an index shift we deduce
\begin{align*}
    & \norel \sum_{r=0}^j (-1)^{j-r}\binom{j}{r}f\left(x_{\ell+r+1}\right) - \sum_{r=0}^j (-1)^{j-r}\binom{j}{r}f\left(x_{\ell+r}\right) \\
    &= \sum_{r=1}^{j+1} (-1)^{j+1-r}\binom{j}{r-1}f\left(x_{\ell+r}\right) + \sum_{r=0}^j (-1)^{j+1-r}\binom{j}{r}f\left(x_{\ell+r}\right) \\
    &= (-1)^{j+1} f\left(x_\ell\right) + \sum_{r=1}^{j} \left((-1)^{j+1-r}f\left(x_{\ell+r}\right) \left[\binom{j}{r-1} + \binom{j}{r}\right]\right) + f\left(x_{\ell+j+1}\right) \\
    &= \sum_{r=0}^{j+1} (-1)^{j+1-r}\binom{j+1}{r} f\left(x_{\ell+r}\right)
\end{align*}
which yields the claim.
\end{proof}
The final result for divided differences is stated as follows:
\begin{theorem}
\label{div_differences_mainresult}
Let $f: \RR^s \to \RR$ and $k \in \NN_0, r>0$, such that $\fres{f}{(-r,r)^s} \in C^k \left((-r,r)^s; \RR\right)$. For $\textbf{p} \in \NN_0^s$ with $\vert \pp \vert \leq k$ and $h>0$ let
\begin{equation*}
    f_{\pp,h} \defeq (2h)^{-\vert \pp \vert} \sum_{0 \leq \textbf{r} \leq \textbf{p}} (-1)^{\vert \pp \vert -\vert \rr \vert} \binom{\textbf{p}}{\textbf{r}} f \left( h(2\rr-\pp)\right).
\end{equation*}
Let $m \defeq \underset{j}{\mathrm{max}} \  \pp_j$. Then, for $0 <h < \frac{r}{\max\{1,m\}}$ there exists $\xi \in h[-m,m]^s$ satisfying
\begin{equation*}
    f_{\pp,h} = \partial^\pp f(\xi).
\end{equation*}
\end{theorem}
\begin{proof}
We may assume $m > 0$, since $\pp=0$ implies $f_{\pp,h} = f(0)$ and hence, the claim follows with $\xi = 0$.

We prove via induction over $s \in \NN$ that the formula
\begin{equation}
\label{to_prove}
    f_{\pp,h}= \pp! \int_{\Sigma^{\pp_s}}\int_{\Sigma^{\pp_{s-1}}} \cdot\cdot\cdot \int_{\Sigma^{\pp_1}} \partial^\pp f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\sigma_\ell^{(1)}, ..., -h\pp_s + 2h\sum_{i=1}^{\pp_s}\ell\sigma_\ell^{(s)}\right)d\sigma^{(1)} \cdot \cdot \cdot d\sigma^{(s)}
\end{equation}
holds for all $\pp \in \NN_0^s$ with $1 \leq \vert \pp \vert \leq k$ and all $0 < h < \frac{r}{m}$. The case $s=1$ is exactly the Hermite-Genocchi-Formula (\ref{hg}) combined with (\ref{alternativdarstellung}) applied to the data points $-hp, -hp + 2h, ..., hp-2h, hp$. 

By induction, assume that the claim holds for some $s \in \NN$.
For a fixed point $y \in (-r,r)$, let 
\begin{equation*}
    f_y: \quad (-r,r)^s \to \RR, \quad x \mapsto f(x,y).
\end{equation*}
For $\pp \in \NN_0^{s+1}$ with $\vert \pp \vert \leq k$ and $\pp' := \left(p_1,...,p_s\right)$ we define
\begin{equation*}
    \Gamma: \quad (-r,r) \to \RR, \quad y \mapsto \left( f_y\right)_{\pp',h} = (2h)^{- \vert \pp' \vert} \sum_{0 \leq \rr' \leq \pp'} (-1)^{\vert  \pp' \vert - \vert \rr' \vert} \binom{\pp'}{\rr'} f\left( h(2\rr' - \pp'),y\right).
\end{equation*}
Using the induction hypothesis, we get
\begin{align*}
\label{IV}
    &\Gamma(y) \\
    =&\pp'! \int\limits_{\Sigma^{\pp_s}}\int\limits_{\Sigma^{\pp_{s-1}}} \cdot\cdot\cdot \int\limits_{\Sigma^{\pp_1}} \partial^{\left(\pp',0 \right)} f \left( -h\pp_1 + 2h\sum_{i=1}^{\pp_1}i\sigma_i^{(1)}, ..., -h\pp_s + 2h\sum_{i=1}^{\pp_s}i\sigma_i^{(s)},y\right)d\sigma^{(1)} \cdot \cdot \cdot d\sigma^{(s)}
\end{align*}
for all $y \in (-r,r)$. Furthermore, we calculate
\begin{align*}
    &\norel \pp_{s+1}! \cdot [-h \cdot \pp_{s+1}, -h \cdot \pp_{s+1} + 2h, ..., h \cdot \pp_{s+1}]\Gamma  \\
    \overset{\eqref{alternativdarstellung}}&{=} (2h) ^{- \pp_{s+1}} \sum_{r' = 0}^{\pp_{s+1}} (-1)^{\pp_{s+1}-r'}\binom{\pp_{s+1}}{r'} \Gamma\left(h\left(2r'-\pp_{s+1}\right)\right) \\
    &= (2h) ^{- \pp_{s+1}} \sum_{r' = 0 }^{\pp_{s+1}} (-1)^{\pp_{s+1}-r'}\binom{\pp_{s+1}}{r'} (2h)^{- \vert \pp' \vert} \sum_{0 \leq \rr' \leq \pp'}(-1)^{\vert \pp' \vert -\vert \rr' \vert} \binom{\pp'}{\rr'} f\left( h(2\rr' - \pp'), h(2r' - \pp_{s+1})\right) \\
    &= (2h)^{-\vert \pp \vert} \sum_{0 \leq \textbf{r} \leq \textbf{p}} (-1)^{\vert \pp \vert -\vert \rr \vert} \binom{\textbf{p}}{\textbf{r}} f \left( h(2\rr-\pp)\right) \\
    &= f_{\pp, h}.
\end{align*}
On the other hand, we get
\begin{align*}
    &\norel [-h \cdot \pp_{s+1}, -h \cdot \pp_{s+1} + 2h, ..., h \cdot \pp_{s+1}]\Gamma \\
    \overset{\eqref{hg}}&{=} \int_{\Sigma^{\pp_{s+1}}}\Gamma^{(\pp_{s+1})}\left(-h\pp_{s+1} + 2h\sum_{\ell=1}^{\pp_{s+1}}\ell t_\ell\right)dt \\
    \overset{}&{=}\pp'! \int\limits_{\Sigma^{\pp_{s+1}}} \cdot\cdot\cdot \int\limits_{\Sigma^{\pp_1}} \partial^{\pp} f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\sigma_\ell^{(1)}, ..., -h\pp_{s+1}+2h\sum_{\ell=1}^{\pp_{s+1}}\ell\sigma^{(s+1)}_\ell\right)d\sigma^{(1)} \cdot \cdot \cdot d\sigma^{(s+1)}.
\end{align*}
Changing the order of integration and derivative is possible, since we integrate on compact sets and only consider continuously differentiable functions.

We have thus proven (\ref{to_prove}) using the principle of induction. The claim of the theorem then follows directly using the mean-value theorem for integrals and by the fact that all the simplices $\Sigma^{\pp_\ell}$ are compact and connected (in fact convex).
\end{proof}