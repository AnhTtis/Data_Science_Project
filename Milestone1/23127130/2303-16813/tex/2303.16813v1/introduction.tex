% !TeX encoding = UTF-8
% !TeX spellcheck = en_US
% !TeX root = main_paper.tex

\section{Introduction}
Up to now, Deep Learning has predominantly relied on real-valued neural networks, which have led to breakthroughs in fields like image or speech recognition \cite{6638947, krizhevsky_imagenet_2017}. However, recent work has uncovered several application areas such as MRI fingerprinting in which the use of complex-valued neural networks leads to better results than the use of their real-valued counterparts \cite{8939560,virtue_better_2017, 8039431, arjovsky_unitary_2016}. The mathematical theory of these complex-valued neural networks (CVNNs), however, is still in its infancy. There is therefore a great mathematical interest in uncovering the differences and commonalities between CVNNs and their real-valued counterparts. 
A prominent example highlighting the differences between both network classes is the universal approximation theorem for neural networks, whose real-valued version was proven in 1993  and which was recently generalized to the case of complex-valued networks \cite{leshno_multilayer_1993,voigtlaender_universal_2022}. The two theorems describe necessary and sufficient conditions on an activation function which guarantee that arbitrarily wide neural networks of a fixed depth can approximate any continuous function on any compact set arbitrarily well with respect to the uniform norm. Already here it was shown that complex-valued networks behave significantly different from real-valued networks: While real-valued networks  are universal if and only if the activation function is non-polynomial, complex-valued networks with a single hidden layer are universal if and only if the activation function is non-polyharmonic. Furthermore there are continuous activation functions for which deep --- but not shallow (i.e., networks with only a single hidden layer) --- CVNNs are universal, whereas the same cannot happen for real-valued neural networks. This example shows that it is of great importance to study the properties of CVNNs and to examine which of the fundamental properties of real-valued networks extent to complex-valued networks.

Essentially the only existing \emph{quantitative} result regarding the approximation theoretical properties of CVNNs is \cite{caragea_quantitative_2022}, which provides results for deep CVNNs using the modReLU activation function. However, for real-valued NNs it is known that already \emph{shallow} NNs can approximate $C^k$-functions at the optimal rate. Precisely, Mhaskar showed in \cite{mhaskar_neural_1996} that one can approximate $C^k$-functions on $[-1,1]^n$ with an error of order $m^{-k/n}$ as $m \to \infty$, where $m$ is the number of neurons in the hidden layer of the network. Here he assumed that the activation function is smooth on an open interval, where at a point of this interval no derivative vanishes. This is equivalent to the activation function being smooth and non-polynomial on that interval \cite[p. 53]{donoghue_distributions_1969}.

We are going to generalize this result to the setting of complex-valued networks, by proving that one can approximate every function in $C^k \left( \Omega_n; \CC \right)$ with an error of the order $m^{-k/(2n)}$ using shallow complex-valued neural networks with $m$ neurons in the hidden layer. Here we define the set $\Omega_n$ as $\Omega_n \defeq [-1,1]^n + i [-1,1]^n$. The result holds whenever the activation function is smooth and non-polyharmonic on some non-empty open set. This is a very natural condition, since for polyharmonic activation functions there exist $C^k$-functions that cannot be approximated at all below some error threshold using neural networks with this activation function \cite{voigtlaender_universal_2022}.

Furthermore, the present paper studies in how far the approximation order of $m^{-k/(2n)}$ is \emph{optimal}, meaning that an order of $m^{-(k/2n) - \alpha}$ (where $\alpha > 0$) cannot be achieved. Here it turns out that the derived order of approximation is indeed optimal in the setting that the weight selection is \emph{continuous}, meaning that the map that assigns to a function in $C^k \left( \Omega^n;\CC\right)$ its corresponding network weights is continuous with respect to some norm on $C^k (\Omega_n ; \CC)$. We are also going to investigate this optimality result further by dropping the continuity assumption and constructing two special smooth and non-polyharmonic activation functions with the first one having the property that the order of approximation can indeed be strictly increased via a discontinuous selection of the related weights. For the second function we are going to show that the order of $m^{-k/(2n)}$ cannot be improved, even if one allows a discontinuous weight selection. This in particular shows that in the given generality of arbitrary smooth, non-polyharmonic activation functions, the approximation rate $m^{-k/(2n)}$ cannot be improved.
\subsection{Main Results}
\label{main_results}
In this work we are going to deal with so-called shallow complex-valued neural networks, meaning complex-valued neural networks with a single hidden layer. Furthermore, we are only considering networks where the map connecting the hidden with the final layer is not just affine-linear but even linear, and where the output dimension of the network is $1$. Precisely, this means that we consider functions of the form
\begin{equation*}
    \CC^n \ni z \mapsto \sum_{j=1}^{m} \sigma_j \phi\left(\rho_j^T \cdot z + \eta_j\right) \in \CC,
\end{equation*}
with $\rho_1, ..., \rho_{m} \in \CC^{n}$, $\sigma_1, ..., \sigma_{m}, \eta_1,..., \eta_{m} \in \CC$ and an \emph{activation function} $\phi:\CC \to \CC$. Here, $m$ denotes the number of neurons of the network.

Our first main result is that it is possible to approximate complex polynomials in $z$ and $\overline{z}$ using shallow complex-valued neural networks. Precisely, for $m \in \NN$ let
\begin{equation*}
    \mathcal{P}_m \defeq \left\{ \CC^n \to \CC, \ z \mapsto \sum_{ \m  \leq m} \sum_{ \elll  \leq m} a_{\m,\elll} z^\m \overline{z}^{\elll} : \ a_{\m, \elll} \in \CC\right\},
\end{equation*}
where we are summing over all multi-indices $\m, \elll \in \NN_0^n$ with $\m_j, \elll_j \leq m$ for every $j \in \{1,...,n\}$. Note that $\mathcal{P}_m$ is a finite-dimensional space, so that it makes sense to talk about bounded subsets of $\mathcal{P}_m$. Then we show the following:
\begin{theorem}
\label{main_1}
     Let $m,n \in \NN$, $\varepsilon > 0$ and $\phi: \CC \to \CC$ be smooth and non-polyharmonic on an open set $\emptyset \neq U \subseteq \CC$.
     Let $\PP' \subseteq \PP_m$ be bounded and set $N := (4m+1)^{2n}$. Then there exist certain coefficients $\rho_1, ..., \rho_N \in \CC^n$ and $b_m \in U$ with the following property: For each polynomial $p \in \mathcal{P}'$ there are coefficients $\sigma_1, ..., \sigma_N \in \CC$, such that
    \begin{equation*}
        \left\Vert p - f\right\Vert_{L^\infty \left(\Omega_n; \CC\right)}  \leq \varepsilon,
    \end{equation*}
    where 
    \begin{equation}
        f: \quad \Omega_n \to \CC, \quad z \mapsto \sum_{j=1}^N \sigma_j \phi\left(\rho_j^T z + b_m\right). 
    \end{equation}
\end{theorem}
    Note that $N$ only depends on the degree of the polynomials, not on the desired degree of approximation and that the coefficients $\rho_j$ are chosen independently from the specific polynomial $p$.

Using this approximation result and the fact that $C^k$-functions can be approximated at a certain rate by complex polynomials in $z$ and $\overline{z}$, we can then study the approximation of arbitrary $C^k$-functions. 
\begin{theorem}
\label{main_2}
    Let $n,k \in \NN$. Then there exists a constant $c = c(n,k) > 0$ with the following property: For any function $\phi:\CC \to \CC$ that is smooth and non-polyharmonic on an open set $\emptyset \neq U \subseteq \CC$ and for any $m \in \NN$ there are coefficients $\rho_1, ... , \rho_m \in \CC^n$ and $b_m \in U$ with the following property: For any $f \in C^k \left(\Omega_n; \CC\right)$ there are complex numbers $\sigma_1 = \sigma_1(f), ..., \sigma_m = \sigma_m(f) \in \CC$, such that
    \begin{equation*}
        \left\Vert f - g \right\Vert_{L^\infty \left(\Omega_n; \CC\right)} \leq c \cdot m^{-k/(2n)} \cdot \left\Vert f \right\Vert_{C^k \left(\Omega_n; \CC\right)},
    \end{equation*}
    where $g$ is defined as
    \begin{equation*}
        g : \quad \Omega_n \to \CC, \quad g(z) \defeq \sum_{j=1}^m \sigma_j \phi \left(\rho_j^T z + b_m\right).
    \end{equation*}
    Furthermore, the map $f \mapsto \sigma_j(f)$ is a continuous linear functional with respect to the $L^\infty$-norm for every $j \in \{1,...,m\}$.
\end{theorem}
Discussing the optimality of this result, based on work by DeVore et al. \cite{devore_optimal_1989} we prove the following result. 
\begin{theorem}
\label{main_3}
    Let $n \in \NN$ and $k \in \NN$. Then there exists a constant $c=c(n,k) > 0$ with the following property: For any $m \in \NN$, $\phi \in C(\CC; \CC)$ and any map
    \begin{equation*}
        \eta : \quad C^k \left( \Omega_n ; \CC\right) \to \left(\CC^n\right)^m \times \CC^m \times \CC^m, \quad g \mapsto \left(\eta_1(g), \eta_2(g), \eta_3(g)\right)
    \end{equation*}
which is continuous with respect to some norm on $C^k (\Omega_n ; \CC)$ there exists $f \in C^k\left(\Omega_n; \CC\right)$ satisfying $\Vert f \Vert_{C^k (\Omega_n ; \CC)} \leq 1$ and
    \begin{equation*}
        \left\Vert f - \Phi(f)\right\Vert_{L^\infty \left(\Omega_n ; \CC\right)} \geq c \cdot m^{-k/(2n)},
    \end{equation*}
    where $\Phi(f) \in C(\Omega_n; \CC)$ is given by
\begin{equation*}
\Phi(f)(z) \defeq \sum_{j=1}^m \left(\eta_3(f)\right)_j \phi \left(\left[\eta_1 (f)\right]_j^T z + \left(\eta_2(f)\right)_j\right).
\end{equation*}
\end{theorem}
This result shows that the approximation order of $m^{-k/(2n)}$ is optimal if one insists on a continuous selection of the weights. For the case of possibly \emph{discontinuous} weight selection we deduce the following two results:
\begin{theorem}
\label{main_4}
    There exists a function $\phi \in C^\infty(\CC;\CC)$ which is non-polyharmonic with the following additional property: For every $k \in \NN$ and $n \in \NN$ there exists a constant $c = c(n,k)>0$ such that for any $m \in \NN$ and $ f \in C^k \left(\Omega_n; \CC\right)$ there are $\rho_1, ..., \rho_m \in \CC^n$ and $\ell_1, ..., \ell_m \in \NN$, such that
    \begin{equation*}
        \left\Vert f(z) - \sum_{j=1}^m \phi \left( \rho_j^T   z + 3\ell_j\right)\right\Vert_{L^\infty \left(\Omega_n; \CC\right)} \leq c \cdot m^{-k /(2n-1)} \cdot \left\Vert f \right\Vert_{C^k \left( \Omega_n; \CC\right)}.
     \end{equation*}
\end{theorem}
\begin{theorem}
\label{main_5}
     Let $n \in \NN$, $k \in \NN$ and
\begin{equation*}
    \phi: \quad \CC \to \CC, \quad \phi(z) \defeq \frac{1}{1+e^{-\mathrm{Re}(z)}}.
\end{equation*}
In particular, $\phi$ is smooth but non-polyharmonic. Then there exists a constant $c = c(n,k) > 0$ with the following property: For any $m \in \NN^{\geq 2}$ there is a function $f \in C^k \left(\Omega^n ; \CC\right)$ with $\Vert f \Vert_{C^k (\Omega_n; \CC)} \leq 1$, such that for every choice of coefficients $\rho_1, ..., \rho_m \in \CC^n$, $\eta_1, ..., \eta_m \in \CC$ and $\sigma_1, ..., \sigma_m \in \CC$ we have
\begin{equation*}
    \left\Vert f(z) - \sum_{j=1}^m \sigma_j \cdot \phi \left( \rho_j^T  z + \eta_j\right)\right\Vert_{L^\infty \left(\Omega_n ; \CC\right)} \geq c \cdot \left(m \cdot \ln (m)\right)^{-k/2n}.
\end{equation*}
\end{theorem}
The significance of these results is as follows: The approximation rate provided by \Cref{main_2} can be strictly improved for some special activation functions (e.g., for the function $\phi$ from \Cref{main_4}), but this improvement is not generally possible for arbitrary (smooth, non-polyharmonic) activation functions; there are such activation functions for which the order of $m^{-k/(2n)}$ is optimal (up to logarithmic factors), even if discontinuous weight selection is allowed.
\subsection{Related Work}
The main inspiration for the present work is the paper \cite{mhaskar_neural_1996} by Mhaskar, where the real-valued counterpart of \Cref{main_2} is stated and proven. Many techniques in the present paper are in fact adaptations of what has been done in \cite{mhaskar_neural_1996}. In particular noteworthy are the multidimensional divided differences formula 
 used to approximate polynomials and the approximation of $C^k$-functions using Fourier-analytic results and Chebyshev polynomials. 
 
 In addition, the work \cite{MAIOROV199981} should be mentioned, which serves as an inspiration for \Cref{main_4}. In \cite{MAIOROV199981} it was proven that there exists a real activation function such that one can approximate continuous functions on the unit cube arbitrarily well using neural networks of \emph{constant size} (only depending on the input dimension) which have two hidden layers. Suitable adaptation of the techniques used in \cite{MAIOROV199981} in combination with a result from \cite{gordon_best_2001} is the basis for our proof of \Cref{main_4}.

Another important inspiration for the present work is \cite{yarotsky_phase_2021}, in which the approximation properties of real-valued networks using the ReLU activation function are considered. In particular, this paper also distinguishes between a continuous and discontinuous selection of network weights, but generally considers networks with more than one hidden layer. In the setting of \cite{yarotsky_phase_2021}, the differences between continuous and discontinuous weight assignment are significant: while with continuous weight assignment at most a rate of $W^{-k/n}$ is possible (and in fact achievable), with discontinuous weight assignment an improvement of the rate to $W^{-2k/n}$ is possible. Here $k$ is, as usual, the regularity of the functions under consideration, $n$ the input dimension and $W$ the number of parameters of the network. 

 When it comes to general literature about mathematical properties of complex-valued neural networks, surprisingly little work can be found. The most prominent result in the theory of CVNNs is the \emph{Universal Approximation Theorem for Complex-Valued Neural Networks} \cite{voigtlaender_universal_2022}, discussing for which activation functions CVNNs are uniform approximators on compact sets. In particular, it has been shown that shallow CVNNs have this property if and only if the activation function $\phi$ is not polyharmonic. Thus, the condition assumed in this work is quite natural.

Another result from the field of approximation theory of CVNNs can be found in \cite{park_qualitative_2022}. Here it is proven that shallow CVNNs with \emph{entire} functions as activation functions can uniformly approximate exactly those continuous functions on compact sets which can also be uniformly approximated by complex polynomials on compact sets. Note that, unlike in the real setting, this does not correspond to the entirety of continuous functions. For example, it is not possible to uniformly approximate the function $z \mapsto \overline{z}$ on compact sets with non-empty interior using holomorphic functions (in particular with polynomials).

  Regarding quantitative approximation results for CVNNs, the only existing work of which we are aware is \cite{caragea_quantitative_2022}, analyzing the approximation capabilities of \emph{deep} CVNNs where the modReLU is used as activation function. Since the modReLU satisfies our condition on an admissible activation function (as shown in \Cref{concrete_activation_functions}), the present work can be seen as an improvement to \cite{caragea_quantitative_2022}. The main differences can be summarized as follows: 
\begin{itemize}
\item We consider very general activation functions including but not limited to the modReLU.
\item We improve the approximation order by a log factor.
\item We show that the order of approximation can in fact be achieved using shallow networks instead of the deep networks used in \cite{caragea_quantitative_2022}.
\end{itemize}
\subsection{Notation}
\label{notation}
Throughout the paper, multi-indices (i.e. elements of $\NN_0^n$) are denoted using boldface. For $\m \in \NN_0^n$ we have the usual notation $\vert \m \vert = \sum_{j=1}^n \m_j$. For a number $m \in \NN_0$ and another multi-index $\pp \in \NN_0^n$ we write $\m \leq m$ iff $\m_j \leq m$ for every $j \in \{1,...,n\}$ and $\m \leq \pp$ iff $\m_j \leq \pp_j$ for every $j \in \{1,...,n\}$. Furthermore we write
\begin{equation*}
    \binom{\pp}{\rr} = \prod_{j = 1}^n \binom{\pp_j}{\rr_j}
\end{equation*}
for two multi-indices $\pp,  \rr \in \NN_0^n$ with $\rr \leq \pp$. For a complex vector $z \in \CC^n$ we write 
\begin{equation*}
    z^\m = \prod_{j=1}^n z_j^{\m_j} \quad \text{and} \quad \overline{z}^\m = \prod_{j=1}^n \overline{z_j}^{\m_j}.
\end{equation*}
For a point $x \in \RR^n$ and $r>0$ we define
\begin{equation*}
    B_r(z) \defeq \left\{ y \in \RR^n : \ \left\Vert x - y \right\Vert < r\right\}
\end{equation*}
with $\Vert \cdot \Vert$ denoting the usual Euclidian distance. This definition is analogously transferred to $\CC^n$. Furthermore, for any function $f: \RR^n \to \RR^m$ and a subset $A \subseteq \RR^n$ we denote
\begin{equation*}
\left\Vert f \right\Vert_{L^\infty\left(A; \RR^m \right)} \defeq \underset{x \in A}{\sup} \left\Vert f(x) \right\Vert.
\end{equation*}
Note that this is finite if $f$ is continuous and $A$ compact. This definition is also analogously transferred to $\CC^n$ and $\CC^m$.

For a function $f: U \to \RR^m$ with an open set $U \subseteq \RR^n$ we write 
\begin{equation*}
    f \in C^k\left(U ; \RR^m\right) \ \Leftrightarrow \ \partial^{ \pp} f \text{ exists and is continuous for every } \pp \in \NN_0^n \text{ with } \vert \pp \vert \leq k. 
\end{equation*}
$\CC^n$ is canonically isomorphic to $\RR^{2n}$ by virtue of the isomorphism
\begin{equation} \label{isomorphism_intro}
    \varphi_n: \quad \RR^{2n} \to \CC^n, \quad \left(x_1, ..., x_n, y_1, ..., y_n\right) \mapsto \left(x_1 +iy_1, ..., x_n +iy_n\right).
\end{equation}
Let $U \subseteq \CC^n$ be any subset of $\CC^n$ and $f: U \to \CC$. Then the function $\widetilde{f}: \varphi_n^{-1}(U) \to \RR^2$ associated to $f$ is defined as 
\begin{equation*}
\widetilde{f} \defeq \varphi_1^{-1} \circ f \circ \fres{\varphi_n}{\varphi_n^{-1}(U)}. 
\end{equation*}
If $U$ is open, we can study real differentiability of $\widetilde{f}$. We write 
\begin{equation*}
    f \in C^k(U; \CC) \defequiv \widetilde{f} \in C^k\left(\varphi_n^{-1}(U); \RR^2\right).
\end{equation*}
If $f \in C^k(U; \CC)$ and $\pp \in \NN_0^{2n}$ is a multi-index with $\vert \pp \vert \leq k$, we denote
\begin{equation*}
    \partial^\pp f \defeq \varphi_1 \circ \partial^\pp \widetilde{f} \circ \fres{\varphi_n^{-1}}{U}.
\end{equation*}

In the present work, the goal is to study the approximation capabilities of complex-valued neural networks on the complex cube
\begin{equation*}
    \Omega_n \defeq \left\{\left(z_1, ..., z_n\right) \in \CC^n : \ \RE\left(z_j\right), \ \IM\left(z_j\right) \in [-1,1], \ j=1,...,n\right\}.
\end{equation*}
Therefore, we need to introduce $C^k$-functions on this cube: We define
\begin{equation*}
    C^k \left([-1,1]^n; \RR^m\right) \defeq \left\{ f \in C([-1,1]^n ; \RR^m) :  \ \begin{matrix}\fres{f}{(-1,1)^n} \in C^k \left((-1,1)^n;\RR^m\right)  \text{ and } \\ \partial^{\pp}\big(\fres{f}{(-1,1)^n}\big) \text{ uniformly continuous } \forall \ \vert \pp \vert \leq k \end{matrix}\right\}.
\end{equation*}
For a function $f \in C^k \left([-1,1]^n; \RR^m\right)$ and any multi-index $\pp \in \NN_0^n$ with $\vert \pp \vert \leq k$ we define $\partial^\pp f$ as the unique continuous extension of $\partial^\pp \big(\fres{f}{(-1,1)^n}\big)$ to $[-1,1]^n$ which exists because of the uniform continuity of $\partial^\pp \big(\fres{f}{(-1,1)^n}\big)$. We then define the $C^k$-norm as 
\begin{equation*}
    \left\Vert f \right\Vert_{C^k \left( [-1,1]^n; \RR^m\right)} \defeq \underset{\vert \pp \vert \leq k}{\max} \ \underset{x \in [-1,1]^n}{\max} \left\Vert \left(\partial^\pp f \right)(x)\right\Vert_{2}.
\end{equation*}
We translate this definition to the setting of complex-valued functions by letting 
\begin{equation*}
    C^k \left(\Omega_n ; \CC\right) \defeq \left\{ f: \Omega_n \to \CC : \ \tilde{f} \in C^k \left([-1,1]^{2n}; \RR^2\right) \right\}.
\end{equation*}
Additionally, as in the setting of open sets, we denote 
\begin{equation*}
    \partial^\pp f \defeq \varphi_1 \circ \partial^\pp \widetilde{f} \circ \fres{\varphi_n^{-1}}{\Omega_n}
\end{equation*}
for any $\pp \in \NN_0^{2n}$ with $\vert \pp \vert \leq k$ and 
\begin{equation*}
    \left\Vert f \right\Vert_{C^k \left(\Omega_n ; \CC\right)} \defeq \underset{\vert \pp \vert \leq k}{\max} \ \underset{z \in \Omega_n}{\max} \left\vert \left(\partial^\pp f \right)(z)\right\vert.
\end{equation*}
Note that by definition we have 
\begin{equation*}
    \left\Vert f \right\Vert_{C^k \left(\Omega_n ; \CC\right)} = \Vert \tilde{f} \Vert_{C^k \left([-1,1]^{2n} ; \RR^2\right)}.
\end{equation*}

For $f \in C^1(U; \CC)$, where $U \subseteq \CC$ is an open subset of $\CC$, we denote 
\begin{equation*}
    \wirt f \defeq \frac{1}{2}\left(\partial^{(1,0)}f - i\partial^{(0,1)}f\right) , \quad \wirtq f \defeq \frac{1}{2}\left(\partial^{(1,0)}f + i\partial^{(0,1)}f\right).
\end{equation*}
These operators are called the \emph{Wirtinger derivatives}. They have the following properties that we are going to use, which can be found for example in \cite[E. 1a]{kaup_holomorphic_1983}:
\begin{enumerate}
    \item $\wirt$ and $\wirtq$ are both $\CC$-linear operators on the set $C^1(U; \CC)$.
    \item $f$ is complex-differentiable in $z \in U$ iff $\wirtq f(z) = 0$ and in this case the equality
    \begin{equation*}
        \wirt f(z) = f'(z)
    \end{equation*}
    holds true, with $f'(z)$ denoting the complex derivative of $f$ at $z$. 
    \item We have the conjugation rules
    \begin{align*}
        \overline{\wirt f} = \wirtq \overline{f} \text{ and } \overline{\wirtq f} = \wirt \overline{f}.
    \end{align*}
    \item If $g \in C^1(U; \CC)$, the following product rules for Wirtinger derivatives hold for every $z \in U$:
    \begin{align*}
        \wirt (f \cdot g)(z)&= \wirt f (z) \cdot g(z) + f(z) \cdot \wirt g(z), \\
        \wirtq (f \cdot g)(z)&= \wirtq f (z) \cdot g(z) + f(z) \cdot \wirtq g(z).
    \end{align*}
    \item If $V \subseteq \CC$ is an open set and $g \in C^1(V;\CC)$ with $g(V) \subseteq U$, then the following chain rules for Wirtinger derivatives hold true:
    \begin{align*}
        \wirt(f \circ g) &= \left[(\wirt f) \circ g\right]\cdot \wirt g + \left[\left(\wirtq f\right)\circ g\right]\cdot \wirt \overline{g}, \\
        \wirtq(f \circ g) &= \left[(\wirt f) \circ g\right]\cdot \wirtq g + \left[\left(\wirtq f\right)\circ g\right]\cdot \wirtq \overline{g}.
    \end{align*}
    \item \label{laplace_ident} If $f \in C^2(U; \CC)$ then we have
    \begin{equation*}
        \Delta f (z) = 4\left(\wirt \wirtq f \right) (z)
    \end{equation*}
    for every $z \in U$, with $\Delta$ denoting the usual Laplace-Operator $\Delta = \partial^{(2,0)} + \partial^{(0,2)}$ (cf. \cite[equation (1.7)]{balk_polyanalytic_1991}).
\end{enumerate}
A function $f \in C^\infty(U ; \CC)$ is called \emph{polyharmonic} (on $U$ with an open set $U \subseteq \CC$) if there is $k \in \NN$ such that $\Delta^k \left(f\right) \equiv 0$. 

Furthermore, we can also define partial Wirtinger derivatives of functions in multiple variables, meaning $f \in C^1(U; \CC)$, where $U \subseteq \CC^n$ is an open set. The formal definition of the partial Wirtinger derivative with respect to the $j$-th variable is
\begin{equation*}
    \wirt^{e_j}f := \frac{1}{2}\left(\partial^{e_j}f - i \partial^{e_{n+j}}f\right), \quad \wirtq^{e_j}f := \frac{1}{2}\left(\partial^{e_j}f + i \partial^{e_{n+j}}f\right).
\end{equation*}
It is easy to see that since Wirtinger derivatives are just a complex linear combination of real derivatives, the symmetry of derivatives also holds in the case of Wirtinger derivatives, i.e., all Wirtinger derivatives commute for functions of sufficient regularity. 
Thus, we introduce the following notation: For an open set $U \subseteq \CC^n$, a function $f \in C^k(U; \CC)$ and a multi-index $\elll \in \NN_0^n$ with $\vert \elll \vert \leq k$ we write $\wirt^{\elll} f$ and $\wirtq^{\elll} f$ for the iterated Wirtinger derivatives according to the multi-index $\elll$. 
\begin{proposition}
\label{wirtreal}
    Let $U \subseteq \CC^n$ be an open set and $f \in C^k(U; \CC)$. Then for any multi-indices $\m, \elll \in \NN_0^{n}$ with $\vert \m + \elll \vert \leq k$ we have the representation
    \begin{equation*}
        \wirt^\m \wirtq^{\elll} f(a) = \underset{\pp' + \pp'' = \m + \elll}{\underset{\pp = (\pp', \pp'') \in \NN_0^{2n}}{\sum}} b_{\pp} \left(\partial^{\pp} f \right)(a) \quad \forall a \in U
    \end{equation*}
    with complex numbers $b_{\pp}  \in \CC$ only depending on $\pp, \m$ and $\elll$ and writing $\pp = (\pp', \pp'')$ for the concatenation of the multi-indices $\pp'$ and $\pp'' \in \NN_0^n$. In particular, the coefficients do not depend on $f$.
\end{proposition}
\begin{proof}
    The proof is by induction over $\m$ and $\elll$. We first assume $\m=0$ and show the claim for all $\elll \in \NN_0^n$ with $\vert \elll \vert \leq k$. In the case $\elll = 0$ there is nothing to show, so we assume the claim to be true for fixed $\elll \in \NN_0^n$ with $\vert \elll \vert < k$ and take $j \in \{1,...,n\}$ arbitrarily. Then we get
    \begin{align*}
        \wirtq^{\elll + e_j} f(a) &= \wirtq^{e_j} \wirtq ^{\elll} f(a) \overset{\text{IH}}{=} \underset{\pp' + \pp'' = \elll}{\sum_{\pp = (\pp', \pp'')\in \NN_0^{2n}}} b_{\pp}\wirtq^{e_j} \left(\partial^{\pp} f\right)(a) \\
&= \underset{\pp' + \pp'' = \elll}{\sum_{\pp= (\pp', \pp'') \in \NN_0^{2n}}} \frac{b_{\pp}}{2} \left(\partial^{(\pp' + e_j, \pp'')} f\right)(a) + \frac{ib_{\pp}}{2} \left(\partial^{(\pp', \pp'' + e_{j})}f\right)(a) \\
&=\underset{\pp' + \pp'' = \elll + e_j}{\underset{\pp = (\pp', \pp'')\in \NN_0^{2n}}{\sum}} b_{\pp} \left(\partial^{\pp} f\right)(a) , 
    \end{align*}
    as was to be shown.

    Since we have shown the case $\m = 0$ we may assume the claim to be true for fixed $\m, \elll \in \NN_0^s$ with $\vert \m + \elll\vert < k$. Then we deduce
    \begin{align*}
        \wirt^{\m + e_j}\wirtq^{\elll} f (a) &= \wirt^{e_j} \wirt^\m \wirtq ^{\elll} f(a) \overset{\text{IH}}{=} \underset{\pp' + \pp'' = \m + \elll}{\sum_{\pp = (\pp', \pp'') \in \NN_0^{2n}}} b_{\pp}\wirt^{e_j} \left(\partial^{\pp} f\right)(a) \\
&= \underset{\pp' + \pp'' = \m + \elll}{\sum_{\pp = (\pp', \pp'') \in \NN_0^{2n}}} \frac{b_\pp}{2} \left(\partial^{(\pp' + e_j, \pp'')} f\right)(a) - \frac{ib_\pp}{2} \left(\partial^{(\pp', \pp'' + e_{j})}f\right)(a) \\
&=\underset{\pp' + \pp'' = \m + \elll + e_j}{\underset{\pp = (\pp', \pp')' \in \NN_0^{2n}}{\sum}} b_{\pp} \left(\partial^{\pp} f\right)(a).
    \end{align*}
   Hence the claim follows using the principle of induction.
\end{proof}
\Cref{wirtreal} is technical but crucial: In the course of the paper it will be useful to be able to approximate Wirtinger derivatives of certain functions. In fact, however, we will approximate real derivatives and take advantage of the fact that Wirtinger derivatives are linear combinations of these.
\subsection{Structure of the Paper}
The work is divided into five main sections (excluding the introduction) and the appendix. In \Cref{admissibility} we introduce the class of admissible activation functions $\phi: \CC \to \CC$. The definition is stated in a very general way but we show that every function which is smooth and non-polyharmonic on a non-empty open subset of $\CC$ is admissible.  

\Cref{approx_polynomials} is devoted to the approximation of complex polynomials in $z$ and $\overline{z}$ and in particular to the proof of \Cref{main_1}. An extension of the Mean Value Theorem to multivariate functions using divided differences, which is important for proving the theorem, is proven in the appendix.

In \Cref{ck_functions}, we consider general $C^k$-functions on the complex unit cube $\Omega_n$ and prove \Cref{main_2}. Essential to the proof is a quantitative approximation statement for $C^k$-functions using linear combinations of Chebyshev polynomials. This approximation is based on Fourier analytic methods, which are well-known in principle. Nevertheless, we discuss these methods in the appendix in order to make the paper more self-contained. 

Furthermore, \Cref{concrete_activation_functions} deals with concrete examples of admissible functions and in particular proves the admissibility of the modReLU and the complex cardioid function, which are the two most commonly used activation functions in applications of complex-valued neural networks.

Finally, \Cref{optimality_section} deals with the optimality of the proven approximation statement for $C^k$-functions and in particular proves Theorems \ref{main_3} - \ref{main_5}.
