
\subsection{Divided Differences} \label{sec:div_diff_reordered}

Divided differences are well-known in numerical mathematics as they are for example used to calculate the coefficients of an interpolation polynomial in its Newton representation. In our case, we are interested in divided differences since they can be used to obtain a generalization of the classical mean-value theorem for differentiable functions: Given an interval $I \subseteq \RR$ and $n+1$ pairwise distinct data points $x_0, ..., x_n \in I$ as well as an $n$-times differentiable real-valued function $f : I \to \RR$, there exists $\xi \in \left( \text{min}\left\{x_0 ,..., x_n\right\}, \text{max}\left\{x_0 ,..., x_n\right\}\right)$ such that
\begin{equation*}
    f\left[x_0, ..., x_n\right] = \frac{f^{(n)}(\xi)}{n!},
\end{equation*}
where the left-hand side is a divided difference of $f$ (defined below). The classical mean-value theorem is obtained in the case $n=1$. Our goal in this section is to generalize this result to a multivariate setting by considering divided differences in multiple variables. Such a generalization is probably well-known, but since we could not locate a convenient reference and to make the paper more self-contained, we provide a proof.

Let us first define divided differences formally. Given $n+1$ data points $\left(x_0, y_0\right), ..., \left(x_n, y_n\right) \in \RR \times \RR$ with pairwise distinct $x_k$, we define the associated divided differences recursively via
\begin{align*}
    \left[ y_k\right] &\defeq y_k, \ k \in \{0,...,n\}, \\
    \left[y_k ,..., y_{k+j}\right] &\defeq \frac{\left[y_{k+1},..., y_{k+j}\right] - \left[y_k,..., y_{k+j-1}\right]}{x_{k+j}-x_k}, \ j \in \{1,...,n\}, \ k \in \{0, ..., n-j\}.
\end{align*}
If the data points are defined by a function $f$ (i.e. $y_k = f\left(x_k\right)$ for all $k \in \{0,...,n\}$), we write
\begin{equation*}
    \left[x_k,...,x_{k+j}\right] f \defeq \left[y_k, ..., y_{k+j}\right].
\end{equation*}
We first consider an alternative representation of divided differences, the so-called \emph{Hermite-Genocchi-Formula}. To state it, we introduce the notation $\Sigma^k$ to denote a certain $k$-dimensional simplex.
\begin{definition}\label{def:simplex}
    Let $s \in \NN$. Then we define
    \begin{equation*}
        \Sigma^s \defeq \left\{ x \in \RR^s : \ x_\ell \geq 0 \ \mathrm{ for} \ \mathrm{all}  \ \ell \ \mathrm{ and  } \sum_{\ell = 1}^s x_\ell \leq 1 \right\}.
    \end{equation*}
    We further set $\Sigma^0 \defeq \{0\} \subseteq \RR$. Denoting by $\lambda^s$ the $s$-dimensional Lebesgue-measure (and by $\lambda^0$ the counting measure), the identity $\lambda^s\left(\Sigma^s\right)= \frac{1}{s!}$ holds true; see for instance \cite{stein_note_1966} and note that the case $s=0$ can be seen directly.
\end{definition}
In the following, we consider integrals over the sets $\Sigma^s$. These integrals are always formed with respect to the measure $\lambda^s$. However, we only write, e.g., $\int_{\Sigma^s} f(x) dx$, with the implicit understanding that the integral is formed with respect to the measure $\lambda^s$. Using this convention, we can now consider the alternative representation of divided differences.
\begin{lemma}[Hermite-Genocchi-Formula]
    Let $k \in \NN_0$. For real numbers $a,b \in \RR$ with $a < b$, a function $f \in C^k([a,b]; \RR)$ (where $C^0([a,b]; \RR)$ denotes the set of continuous functions) and pairwise distinct $x_0, ..., x_k \in [a,b]$, the divided difference of $f$ at the data points $x_0, ..., x_k$ satisfies the identity
\begin{equation}
\label{hg}
    \left[x_0, ..., x_k\right]f = \int_{\Sigma^k} f^{(k)}\left(x_0 + \sum_{\ell=1}^{k}s_\ell\left(x_\ell-x_0\right)\right) ds.
\end{equation}
\end{lemma}
\begin{proof}
    The case $k=0$ follows directly from $[x_0]f = f(x_0)$. For the case $k>0$, see \cite[Theorem 3.3]{atkinson_introduction_1989}.
\end{proof}
We will make use of the above formula by combining it with the following generalization of the mean-value theorem for integrals.
\begin{lemma}\label{lem:mean_val}
Let $D$ be a connected and compact topological space. Let $\mathcal{A}$ be some $\sigma$-algebra over $D$ and let $\mu: \mathcal{A} \to [0, \infty)$ be a finite measure on $D$ with $\mu(D) > 0$. Let $f : D \to \RR$ be a continuous function with respect to the standard topology on $\RR$ and the topology on $D$. Moreover, let $f$ be measurable with respect to $\mathcal{A}$ and the Borel $\sigma$-algebra on $\RR$. Then there exists $\xi \in D$ such that
\begin{equation*}
    f(\xi) = \frac{1}{\mu(D)} \cdot \int_D f(x) d\mu(x).
\end{equation*}
\end{lemma}
\begin{proof} 
Since $D$ is compact and connected and $f$ continuous, it follows that $f(D) \subset \RR$ is compact and connected, and hence $f(D)$ is a compact interval in $\RR$. Therefore, there exist $x_{\text{min}} \in D$ and $x_{\text{max}} \in D$ satisfying
\begin{equation*}
    f\left(x_{\text{min}}\right) \leq f(x) \leq f\left(x_{\text{max}} \right)
\end{equation*}
for all $x \in D$. Thus, one gets
\begin{align*}
    f\left( x_{\text{min}}\right) &=  \frac{1}{\mu(D)} \int_D f(x_{\min}) d\mu(x)  \leq \frac{1}{\mu(D)} \int_D f(x) d\mu(x) \\
    &\leq \frac{1}{\mu(D)} \int_D f(x_{\max}) d\mu(x) =  f\left( x_{\text{max}}\right).
\end{align*}
The claim now follows from the fact that $f(D)$ is an interval.
\end{proof}
We also get a convenient representation of divided differences for the case of equidistant data points.
\begin{lemma}
Let $f: \RR \to \RR$, $x_0 \in \RR$, $k \in \NN_0$ and $h > 0$. We consider the case of equidistant data points, meaning $x_{j} \defeq x_0 + jh$ for all $j = 1,...,k$. In this case, we have the formula
\begin{equation}
\label{alternativdarstellung}
    \left[x_0, ..., x_k\right]f = \frac{1}{k!h^k} \cdot \sum_{r=0}^k (-1)^{k-r}\binom{k}{r} f\left(x_r\right). 
\end{equation}
\end{lemma}
\begin{proof}
We prove the result via induction over the number $j$ of considered data points, meaning the following: For all $j \in \{0,...,k\}$ we have
\begin{equation*}
    \left[x_\ell, ..., x_{\ell+j}\right]f = \frac{1}{j!h^j} \cdot \sum_{r=0}^j (-1)^{j-r}\binom{j}{r} f\left(x_{\ell+r}\right)
\end{equation*}
for all $\ell \in \{0, ..., k\}$ satisfying $\ell + j \leq k$. The case $j = 0$ is trivial. Therefore, we assume the claim to be true for a fixed $j \in \{0,...,k-1\}$, and let $\ell \in \{0,...,k\}$ be arbitrary with $\ell+j+1 \leq k$. We then get
\begin{align*}
    \left[x_\ell, ..., x_{\ell+j+1}\right]f &= \frac{\left[x_{\ell+1}, ..., x_{\ell+j+1}\right]f - \left[x_\ell, ..., x_{\ell+j}\right]f}{x_{\ell+j+1}-x_\ell} \\
    \overset{\text{I.H.}}&{=} \frac{1}{j!h^j}\cdot \frac{\sum_{r=0}^j (-1)^{j-r}\binom{j}{r} \left(f\left(x_{\ell+r+1}\right) - f\left(x_{\ell+r}\right)\right)}{(j+1)h} \\
    &= \frac{1}{(j+1)!h^{j+1}}\sum_{r=0}^j (-1)^{j-r}\binom{j}{r} \left(f\left(x_{\ell+r+1}\right) - f\left(x_{\ell+r}\right)\right).
\end{align*}
Using an index shift, we deduce
\begin{align*}
    & \norel \sum_{r=0}^j (-1)^{j-r}\binom{j}{r}f\left(x_{\ell+r+1}\right) - \sum_{r=0}^j (-1)^{j-r}\binom{j}{r}f\left(x_{\ell+r}\right) \\
    &= \sum_{r=1}^{j+1} (-1)^{j+1-r}\binom{j}{r-1}f\left(x_{\ell+r}\right) + \sum_{r=0}^j (-1)^{j+1-r}\binom{j}{r}f\left(x_{\ell+r}\right) \\
    &= (-1)^{j+1} f\left(x_\ell\right) + \sum_{r=1}^{j} \left((-1)^{j+1-r}f\left(x_{\ell+r}\right) \left[\binom{j}{r-1} + \binom{j}{r}\right]\right) + f\left(x_{\ell+j+1}\right) \\
    &= \sum_{r=0}^{j+1} (-1)^{j+1-r}\binom{j+1}{r} f\left(x_{\ell+r}\right),
\end{align*}
which yields the claim.
\end{proof}
The final result for divided differences, which is the result that is actually used in the proof of \Cref{main_1} in \Cref{approx_polynomials_reordered}, reads as follows:
\begin{theorem}
\label{div_differences_mainresult}
Let $f: \RR^s \to \RR$ and $k \in \NN_0, r>0$, such that $\fres{f}{(-r,r)^s} \in C^k \left((-r,r)^s; \RR\right)$. For $\textbf{p} \in \NN_0^s$ with $\vert \pp \vert \leq k$ and $h>0$ let
\begin{equation*}
    f_{\pp,h} \defeq (2h)^{-\vert \pp \vert} \sum_{0 \leq \textbf{r} \leq \textbf{p}} (-1)^{\vert \pp \vert -\vert \rr \vert} \binom{\textbf{p}}{\textbf{r}} f \left( h(2\rr-\pp)\right).
\end{equation*}
Let $m \defeq \underset{j}{\mathrm{max}} \  \pp_j$. Then, for $0 <h < \frac{r}{\max\{1,m\}}$ there exists $\xi \in h[-m,m]^s$ satisfying
\begin{equation*}
    f_{\pp,h} = \partial^\pp f(\xi).
\end{equation*}
\end{theorem}
\begin{proof}
We may assume $m > 0$, since $m=0$ implies $\pp=0$ and thus $f_{\pp,h} = f(0)$, so that the claim holds for $\xi = 0$.

We prove via induction over $s \in \NN$ that the formula
\begin{equation}
\label{to_prove}
    f_{\pp,h}= \pp! \int_{\Sigma^{\pp_s}}\int_{\Sigma^{\pp_{s-1}}} \cdot\cdot\cdot \int_{\Sigma^{\pp_1}} \partial^\pp f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\sigma_\ell^{(1)}, ..., -h\pp_s + 2h\sum_{\ell=1}^{\pp_s}\ell\sigma_\ell^{(s)}\right)d\sigma^{(1)} \cdot \cdot \cdot d\sigma^{(s)}
\end{equation}
holds for all $\pp \in \NN_0^s$ with $1 \leq \vert \pp \vert \leq k$ and all $0 < h < \frac{r}{m}$. The case $s=1$ is exactly the Hermite-Genocchi-Formula (\ref{hg}), combined with (\ref{alternativdarstellung}) applied to the data points \[-hp, -hp + 2h, ..., hp-2h, hp.\] 

By induction, assume that the claim holds for some $s \in \NN$.
For a fixed point $y \in (-r,r)$, let 
\begin{equation*}
    f_y: \quad (-r,r)^s \to \RR, \quad x \mapsto f(x,y).
\end{equation*}
For $\pp \in \NN_0^{s+1}$ with $\vert \pp \vert \leq k$ and $\pp' := \left(p_1,...,p_s\right)$, we define
\begin{equation*}
    \Gamma: \quad (-r,r) \to \RR, \quad y \mapsto \left( f_y\right)_{\pp',h} = (2h)^{- \vert \pp' \vert} \sum_{0 \leq \rr' \leq \pp'} (-1)^{\vert  \pp' \vert - \vert \rr' \vert} \binom{\pp'}{\rr'} f\left( h(2\rr' - \pp'),y\right).
\end{equation*}
Using the induction hypothesis, we get
\begin{align*}
\label{IV}
    &\Gamma(y) \\
    =&\pp'! \int\limits_{\Sigma^{\pp_s}}\int\limits_{\Sigma^{\pp_{s-1}}} \cdot\cdot\cdot \int\limits_{\Sigma^{\pp_1}} \partial^{\left(\pp',0 \right)} f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\sigma_\ell^{(1)}, ..., -h\pp_s + 2h\sum_{\ell=1}^{\pp_s}\ell\sigma_\ell^{(s)},y\right)d\sigma^{(1)} \cdot \cdot \cdot d\sigma^{(s)}
\end{align*}
for all $y \in (-r,r)$.
Furthermore, we calculate
\begin{align*}
    &\norel \pp_{s+1}! \cdot [-h \cdot \pp_{s+1}, -h \cdot \pp_{s+1} + 2h, ..., h \cdot \pp_{s+1}]\Gamma  \\
    \overset{\eqref{alternativdarstellung}}&{=} (2h) ^{- \pp_{s+1}} \sum_{r' = 0}^{\pp_{s+1}} (-1)^{\pp_{s+1}-r'}\binom{\pp_{s+1}}{r'} \Gamma\left(h\left(2r'-\pp_{s+1}\right)\right) \\
    &= (2h) ^{- \pp_{s+1}} \Bigg[\sum_{r' = 0 }^{\pp_{s+1}} (-1)^{\pp_{s+1}-r'}\binom{\pp_{s+1}}{r'} (2h)^{- \vert \pp' \vert}\\
    &\hspace{1.8cm}\norel \times \sum_{0 \leq \rr' \leq \pp'}(-1)^{\vert \pp' \vert -\vert \rr' \vert} \binom{\pp'}{\rr'} f\left( h(2\rr' - \pp'), h(2r' - \pp_{s+1})\right)\Bigg] \\
    &= (2h)^{-\vert \pp \vert} \sum_{0 \leq \textbf{r} \leq \textbf{p}} (-1)^{\vert \pp \vert -\vert \rr \vert} \binom{\textbf{p}}{\textbf{r}} f \left( h(2\rr-\pp)\right) \\
    &= f_{\pp, h}.
\end{align*}
On the other hand, we get
\begin{align*}
    &\norel [-h \cdot \pp_{s+1}, -h \cdot \pp_{s+1} + 2h, ..., h \cdot \pp_{s+1}]\Gamma \\
    \overset{\eqref{hg}}&{=} \int_{\Sigma^{\pp_{s+1}}}\Gamma^{(\pp_{s+1})}\left(-h\pp_{s+1} + 2h\sum_{\ell=1}^{\pp_{s+1}}\ell \sigma^{(s+1)}_\ell\right)d\sigma^{(s+1)} \\
    \overset{}&{=} \pp'! \!
                   \int\limits_{\Sigma^{\pp_{s+1}}}
                    \cdots
                    \int\limits_{\Sigma^{\pp_1}}
                      \partial^{\pp} f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\sigma_\ell^{(1)}, ..., -h\pp_{s+1}+2h\sum_{\ell=1}^{\pp_{s+1}}\ell\sigma^{(s+1)}_\ell\right)
                    d\sigma^{(1)} \cdots d\sigma^{(s+1)}.
\end{align*}
Interchanging the order of integration and derivative is possible, since we integrate on compact sets and only consider continuously differentiable functions (see, e.g., \cite[Lemma 16.2]{bauer_measure_2001}).

We have thus proven (\ref{to_prove}) using the principle of induction. The claim of the theorem then follows directly using the mean-value theorem for integrals (\Cref{lem:mean_val}) applied to the topological space
\begin{equation*}
D \defeq \Sigma^{\pp_1} \times \cdots \times \Sigma^{\pp_{s+1}}
\end{equation*}
equipped with the product topology where each factor is endowed with the standard topology on $\Sigma^{\pp_\ell}$ (where the standard topology on $\Sigma^0$ is the discrete topology), the measure
\begin{equation*}
\mu \defeq \lambda^{\pp_1} \otimes \cdots \otimes \lambda^{\pp_{s+1}}
\end{equation*}
defined on the product of the Borel $\sigma$-algebras on $\Sigma^{\pp_\ell}$ (and the $\sigma$-algebra of $\{0\}$ is its power set) and to the function
\begin{equation*}
(\sigma^{(1)}, ..., \sigma^{(s+1)}) \mapsto  \partial^{\pp} f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\sigma_\ell^{(1)}, ..., -h\pp_{s+1}+2h\sum_{\ell=1}^{\pp_{s+1}}\ell\sigma^{(s+1)}_\ell\right).
\end{equation*}
Moreover, note that all the simplices $\Sigma^{\pp_\ell}$ are compact and connected (in fact convex) with
\begin{equation*}
\lambda^{\pp_1}(\Sigma^{\pp_1}) \cdots \lambda^{\pp_{s+1}}(\Sigma^{\pp_{s+1}}) = \frac{1}{\pp!},
\end{equation*} 
see \Cref{def:simplex}.
Therefore, \Cref{lem:mean_val} yields the existence of a certain $(\xi^{(1)}, ..., \xi^{(s+1)}) \in D$ with
\begin{equation*}
f_{\pp, h} = \partial^{\pp} f \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\xi_\ell^{(1)}, ..., -h\pp_{s+1}+2h\sum_{\ell=1}^{\pp_{s+1}}\ell\xi^{(s+1)}_\ell\right).
\end{equation*}
Hence, the claim follows letting
\begin{equation*}
\xi \defeq \left( -h\pp_1 + 2h\sum_{\ell=1}^{\pp_1}\ell\xi_\ell^{(1)}, ..., -h\pp_{s+1}+2h\sum_{\ell=1}^{\pp_{s+1}}\ell\xi^{(s+1)}_\ell\right) \in h[-m,m]^s. \qedhere
\end{equation*}
\end{proof}
