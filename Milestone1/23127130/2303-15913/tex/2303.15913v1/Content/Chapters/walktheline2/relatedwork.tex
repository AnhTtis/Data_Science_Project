\section{Related Work}
\label{sec:walktheline:rw}

Chapter \ref{ch:relatedwork} discusses the related works on interaction techniques with \acp{HMD} and section \ref{sec:cheesyfoot:rw} discusses foot-based interaction techniques. In addition to this, the following section presents a set of requirements for hands-free interaction with \acp{HMD} while walking and, afterward, categorizes relevant research with regards to the requirements. 

\subsection{Requirements}

The following section presents a set of requirements for interactions with \acp{HMD} using the \emph{lower limbs} while walking derived from the related works. The requirements are then used to compare the most relevant related work (see table \ref{tab:req:walktheline}).

\begin{description}
	\item[R6.1: Adapted Input] The input methods of the system must compensate for the situationally-induced impairments of walking.
	\item[R6.2: Adapted Output] The output methods of the system must compensate for the situationally-induced impairments of walking.
	\item[R6.3: Generalizability of the Interaction Technique] The interaction technology must be suitable for general interaction with \acp{HMD}, not only for a specific use case.
	\item[R6.4: No Interruption of the Locomotion] The interaction with the system must occur completely during walking and may not require to interrupt the process of locomotion.
	\item[R6.5: Eyes-Free Operation] To avoid danger, a system must not capture the entire (visual) attention of the user. The user must retain awareness of his environment while interacting with the system.
\end{description}










\subsection{Interfaces for Use While Walking}
\label{sec:rw:whilewalking}

The proliferation of smartphones and the increasing usage during walking~\ncite{Yoshiki2017} led to a stream of research to mitigate the situationally-induced impairments~\ncite{sears03} that are introduced through walking~\ncite{Sarsenbayeva2018, Saulynas2018} and additional encumbrances such as carrying objects~\ncite{Ng2013, Ng2015} or ambient noises~\ncite{Sarsenbayeva2018a}.

\typcite{Kane2008} introduced the term \acp{WUI} for interfaces that are explicitly designed \enquote{to compensate for the effects of walking on mobile device usability.} The authors proposed the usage of increased button and text sizes to compensate for the reduced input performance. Following this stream of research, \typcite{Rahmati2009} used content stabilization to compensate for the shaking introduced from walking. Further examples to help users to overcome the situational impairments include the usage of other keyboard layouts~\ncite{Clawson2014} or text input modalities beyond touch-typing~\ncite{Fitton2013}.

Focusing on the safety aspects of usage, \typcite{Beuck2017} found that mobile applications actively interrupting smartphone usage when entering a potentially dangerous situation can help to prevent such situations. \typcite{Shikishima2018} showed how texting while walking can be detected.
Following the same path, \typcite{Hincapie-Ramos2013} presented an integrated alarm system that warns users of dangerous situations while being engaged with their smartphone. Further examples include other warning systems~\ncite{Wen2015,Tang2016a,Vinayaga-Sureshkanth2018a}, obstacle detection~\ncite{Wang2017}, or specialized support for texting~\ncite{Kong2017} or video watching~\ncite{Ahn2013}.

While most of the research on interfaces for use while walking focused on smartphones, this paper argues that \acp{HMD} are a better fit for the requirements of a truly mobile user interface to be operated while walking: Such devices do not require the user to look down to operate. Further, the user’s visual attention is not captured on an opaque screen, keeping the connection to the real world~\ncite{Lucero2014}.

Highly related, \typcite{Lages2019} explored how different adaptation strategies of user interfaces can support the user in interacting with \acp{HMD} during walking. However, this very inspirational work focused on adapting the \emph{output} to accommodate for the effects of walking. The authors did not address the changing requirements for \emph{input} while walking.



\subsection{Locomotion as Input}
\label{sec:rw:movement}

Research showed how the movement of the user’s body during locomotion, as well as the changing spatial relationships between users and objects, could be used as an input dimension for both, implicit and explicit interactions. 

Popular examples of implicit interaction using body motion as an input dimension can be found in the area of context-aware computing systems~\ncite{Chen2000}, e.g., for mobile navigation. Such systems use the (global) spatial position of the user as input and present navigation instructions through a variety of output modalities such as screen-based~\ncite{Kruger2004}, augmented~\ncite{Narzt2006}, vibrotactile~\ncite{Erp2005, Tsukada2004} or audio~\ncite{Holland2002}, or a combination of these. As another example, \typcite{Dow2005} showed how the spatial location of a user could be used to start the playback of location-specific content, allowing users to interactively explore content through walking. Further, \typcite{Vogel2004} showed how to use the spatial position of users relative to a public display to switch between different modes of interaction implicitly. However, the use of implicit interaction techniques is limited to specific application areas and, therefore, cannot be generalized to general interaction.

In recent years, research proposed more explicit methods for interacting with \acp{HMD} through walking. As the most prominent example, it is a widespread interaction paradigm in \ac{VR} and \ac{AR} to approach virtual objects in order to interact with them in place \ncite{Argelaguet2013}. In such systems, the user’s spatial movement acts as means for \emph{selection} of virtual objects. However, to the best of our knowledge, prior work did not explore how the locomotion of users during walking itself can be leveraged as a generic \emph{input} dimension for interaction with \acp{HMD} yet.

\input{tables/requirements_walktheline}