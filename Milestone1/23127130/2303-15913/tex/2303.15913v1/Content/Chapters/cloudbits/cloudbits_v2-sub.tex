\subsection{CloudBits: Interaction Techniques and Prototype}
\label{sec:cloudbits:cloudbits}

\cptteaser{cloudbits/teaser}{CloudBits provides users with auxiliary information based on the topics of their conversation, proactively retrieved by means of zero-query search. The information is visualized as augmented \emph{information bits} falling from the \emph{cloud}.}

Based on the findings from the exploratory study and the related work, this section presents the CloudBits concepts and prototype implementation.

CloudBits leverages the metaphor of \emph{cloud} and \emph{drops}, where retrieved units of information are visualized as small drops of information, gracefully falling from an imaginary cloud above the users, in sync with the flow of the conversation (see figure \ref{fig:cloudbits/teaser}). The usage of \acp{HMD} allows to present both, 1) a public shared information space visualized as jointly visible information drops, and 2) individual private information visualized as only privately visible information drops, and supports direct and immediate interaction right in front of the eyes.

The \emph{world-stabilized} nature of the visualization enables spatial interaction with information: Users can sort and group information in the complete 3D space provided by the physical location, leveraging affordances of the real world (e.g. placing something on the table). The spatial dimension of public information is shared between all users so that information appears at the same physical location, allowing for collaborative interaction with information.

The remainder of this section discusses the CloudBits concepts with regards to the established requirements (see section \ref{sec:cloudbits:study1:results}).

\subsubsection{Augmented Zero-Query Information Drops for Unsolicited and Real-Time Service}
\label{sec:cloudbits:cloudbits:focuspluscontext}

CloudBits is an augmented reality system for \acp{HMD} that supports users through small units of information. To fulfill requirement R1, these information units appear in real-time and time-varying to support the current context of the conversation. The information units are visualized as information drops, depicting a preview image, dropping slowly from the metaphorical \emph{cloud} above the users (see fig. \ref{fig:cloudbits/cloudbits} b). Information drops exist in a shared information space, i.e., position and movement of information bits are synchronized between the users. Thus, the information drops appear at the same real-world coordinates but rotated towards each user, allowing users to naturally refer to individual information drops (\emph{Look, there!}, see figure \ref{fig:cloudbits/cloudbits} a). If interested, users can interact with the information drops or, if not, just let them drop slowly to the ground. Once an information bit hits the ground, which means that its lifespan is over, it disappears without further interaction from the user. 

CloudBits unobtrusively transcribes conversations in the background through several microphones and a voice recognition system. Based on the transcribed text, the ambient voice search engine deduces the topics of the conversation. CloudBits uses those topics as zero-query search terms to proactively retrieve information for the users from public (e.g., map data, websites) and private (e.g., e-mail, calendar) information sources. The individual spawn position of the information drops is calculated to be in the peripheral vision of the users in order to lessen the visual clutter and the imposed distraction~\ncite{Kruijff2010}.

\subsubsection{Supporting Fluid Transition between Focus and Context}

To support the fluid transition between the conversation and the process of information retrieval (R3), CloudBits proposes a focus+context~\ncite{Card1999} approach for interaction with information in a conversation setting. 

While the conversation is the \emph{focus} of the user, CloudBits provides \emph{context} through small information drops visualizing the course of the conversation. Vice versa, when interacting with information, CloudBits becomes the \emph{focus} of the user. In contrast to information retrieval using a mobile device, which restricts the participation in the conversation to the auditory channel, the augmented reality nature of CloudBits still allows audio-visual participation as \emph{context}, as the other persons of the conversation are still in the peripheral vision. The tight integration and synchronization of CloudBits with the conversation allows for a fast and smooth transition of the focus between the actual conversation and the information retrieval.

The presented focus+context nature of CloudBits supports users in re-engaging with the content of the conversation through the always-available context of the conversation. Furthermore, as we will introduce in the next section, the vital information drops can be pinned in the information space and always accessible just by a quick glance.

\subsubsection{Immediate Interaction with Information}

\textfig{cloudbits/cloudbits}{The interaction techniques of CloudBits: Users can (b) grab\&move information drops freely in the space. To access information, users can grab an information bit and (c) open the hand with the palm facing upwards. Private information drops can (d,e) be shared with other users through grabbing and moving them towards another user. The color encodes if they are public or private.}

CloudBits provides a set of interaction techniques that allow for easy and immediate interaction with the information. All interactions with public information (see section \ref{sec:cloudbits:concept:sharing}) are shared between the users, i.e., if one user changes the position of an information bit or shows its content, this is visualized for all users. This provides mutual awareness as users can understand (1) other users' interactions with the system and (2) the context of their interactions as they can also see the information they are interacting with.  


\begin{description}
	\item[Grab \& Move] Users can grab (see fig. \ref{fig:cloudbits/cloudbits}, b) information drops and drag them from the stream of falling drops. Bits can be freely moved around in the real world. \vspace{1em}
	\item[Grab \& Pin] Interesting information drops can be kept for future access through pinning them to a real-world position. Pinning is initiated by moving an information bit to the desired position and releasing the Grab \& Move gesture. Users can unpin an information bit through tapping. \vspace{1em}
	\item[Grab \& Throw] When no longer needed, users can discard information drops by grabbing and throwing them away.\vspace{1em}
	\item[Grab \& Show] To access the content of an information bit, users can unfold it through dragging the information bit into the center of their vision and opening the hand with the palm facing upwards (see fig. \ref{fig:cloudbits/cloudbits} b,c). Similar to the closed drops, the expanded information is presented at the same world coordinate but individually rotated towards each user. To close information drops, users can perform the reversed gesture. 
\end{description}

\subsubsection{Selective Information Sharing from the Public-Private Information Spectrum}
\label{sec:cloudbits:concept:sharing}

To fulfill requirement R3 and to overcome the privacy issues of information sharing on personal smart devices~\ncite{Karlson2009} and in traditional ambient voice search systems, CloudBits supports private information that is only visualized for the respective user. Users can distinguish private and public information through a color-coding (orange for private, blue for public, see figure \ref{fig:cloudbits/cloudbits} d,e). Private information drops provide the same interaction techniques, as outlined in the last section. Additionally, private information can be selectively shared with other users through the \emph{Grab \& Share} gesture.

\begin{description}
	\item[Grab \& Share] Users can share information by grabbing (see figure \ref{fig:cloudbits/cloudbits}, d) and moving it towards another user (see figure \ref{fig:cloudbits/cloudbits}, e), resembling the natural gesture of handing an object to another person. 
\end{description}

\subsubsection{Prototype Implementation}
\label{sec:cloudbits:prototype}

As mentioned earlier, the CloudBits prototype is based upon the implementation of an ambient voice search engine presented by~\typcite{Radeck-Arneth}. The system implementation is based on two main components: (1) a centralized server and (2) a client visualization application for the \acp{HMD}.

The centralized server receives the topics from the ambient voice search engine. The server then orchestrates the spawn positions of information drops in world coordinates and distributes those to the client applications. The server selects the spawn positions such that the direct line of sight between the users remains clear, and the drops appear in their peripheral area.

The CloudBits client application was implemented for the Microsoft HoloLens using Unity3D. All interactions from users are synchronized with the centralized server in real-time (delay <0.2s, 20 fps) and, in the case of public information drops, broadcasted to the other connected clients.

