\section{Use Cases and Applicability}
\label{sec:proximity:applicability}

This section presents use cases for the usage of one-handed proximity-based interaction for \acp{HMD} and, as an example for the applicability beyond \acp{HMD}, for interacting with a smartwatch.

\subsection{Use Cases for Interacting with Head-Mounted Displays}

A real-world system supporting proximity-based one-handed interaction can leverage the space in front of the user to present a layered information space (virtually augmented on the user's palm). Similar to \ncite{Ha2014}, the hand's 3D features can be extracted from an RGB-D attached to the head-mounted display. This section presents four possible use cases to demonstrate the design space of such proximity-based interaction.

\input{Content/Figures/proximity/usecases_graphic}

\subsubsection{Exploring Information Layers}

Proximity-based interaction can be used to explore multidimensional information structures in which different information layers refer to the same underlying data set. 

A map application can serve as an example of this type of interaction. While the underlying data set to be displayed does not change, a different view of the data can be selected by moving the hand. This process allows the user to switch through different information layers quickly - e.g., elevation map, topographic map, traffic intensity - without needing a second hand (see figure \ref{fig:proximity/usecases} a). According to guideline 2 and 4 (see section \ref{sec:proximity:discussion:guideline2} and \ref{sec:proximity:discussion:guideline4}), the most frequently used information layers are presented in the near and medium zones and occupy a larger part of the interaction space then the other layers.

\subsubsection{Browsing Lists}

As a second example, proximity-based interaction can be used to scroll through list structures by moving the hand, following the metaphor of scrolling through a file cabinet. 

Two implementations are possible, depending on the size of the list:  For short lists, list elements can be bound directly to layers of the information space, allowing direct access of elements.  For longer lists, a mapping can be used, where the inner (near) layers are used to select a list element, while the outer (medium and far) layers are used for slow or fast scrolling in the respective direction (see figure \ref{fig:proximity/usecases} b). According to guideline 1 (see section \ref{sec:proximity:discussion:guideline1}), the number of layers displayed depends on the arm length of the user to ensure fast and accurate interactions.

\subsubsection{Combination with Touch-Input}

In addition to the selection of elements through proximity-based interaction, methods of on-body interaction can be used to realize additional touch-input on the respective layers.

As an example, imagine a proximity-enabled keyboard. The proximity-dimension can be used to select different layers (e.g., small letters, capital letters, numbers) of the keyboard. On each layer, the opposing hand can provide touch input to select keys (see figure \ref{fig:proximity/usecases} c).

\subsubsection{Discrete Interaction for Shortcuts}

In addition to the continuous interaction by the movement of the hand through the interaction space, this thesis explored discrete interactions by raising the hand at specific distances from the user's body. Such interactions can be performed directly and are based only on raising the hand; no additional interaction is necessary. Therefore, this type of interaction is suitable for fast shortcuts. According to guideline 3 (see section \ref{sec:proximity:discussion:guideline3}), the position of the shortcut layer is tailored to the user in order to adapt to their personal interaction area.

As an example, such discrete shortcuts can be used to accept or decline incoming calls without the need of a second hand to interact (see figure \ref{fig:proximity/usecases} d).

\subsubsection{Conclusion}

This section presented four use cases to demonstrate the applicability of proximity-based interaction with \acp{HMD} in different situations. In combination with other input modalities such as touch or further proximity dimensions, further styles of interaction ar possible. Section \ref{sec:proximity:limitations} discusses these possible extensions.

\subsection{Use Cases for Smartwatches}

The two styles of interaction introduced - discrete and continuous - can also support other device classes beyond \acp{HMD} for immediate and joyful interactions. As an example, this section presents a proof of concept for the applicability of the concepts for interaction with smartwatches.

A closer look reveals that there are great similarities between the challenges found in interacting with smartwatches with the challenges of interacting with \acp{HMD}: Interface elements cannot be touched (or only with difficulty), the interaction takes place in a mobile context with a possibly occupied hand. Therefore, this section explores how the presented proximity-based interaction techniques can be transferred from the field of \acp{HMD} to Smartwatches. Based on the findings presented earlier in this chapter, this section introduces ProxiWatch: A one-handed proximity-based hand input modality for smartwatches along with two main interaction techniques. Further, this section presents the design of two example applications to show the usefulness for varying scenarios.

\subsubsection{Motivation and Background}

Highly capable smartwatches have become an emerging class of wearable devices that allow ubiquitous and mobile interaction with digital contents. Such devices usually consist of small multi-touch displays, bundled together with computing and sensing hardware of a smartphone, worn on the user's wrist. Therefore, smartwatches allow users to see, access, and modify information right at their wrist, anytime, and anywhere. The screen size of such devices is a trade-off between wearing comfort and interaction space. On the one hand, the display should be big enough for meaningful touch interaction. On the other hand, bigger display sizes result in bulky devices and reduced wearing comfort. The evolution of wearable devices shows a trend towards small and elegant devices with small interaction space~\ncite{Ni2009}.

Therefore, traditional interaction techniques are not directly applicable to smartwatches. Current consumer devices (e.g., Apple Watch, Android Wear, Pebble Watch) mainly focus on 1) touch-based interfaces, 2) physical input controls on the frame of the device (e.g., a digital crown, buttons) and 3) off-device input modalities such as voice input. While practical and useful, those styles of interaction have various drawbacks in the context of smartwatches. Traditional touch-based interaction techniques suffer from small screen sizes as the user's interacting finger occludes a big part of the screen. %
Physical input controls such as a digital crown allow interacting with the content without occluding the screen; however, these approaches do not support direct targeting and selecting of UI elements. In addition, touch interfaces, as well as physical input controls, require both hands of the user and, thus, may diminish the user experience in situations where the user is encumbered~\ncite{Ng2014}. Voice input lacks direct manipulation and is difficult to use in noisy environments~\ncite{Starner2002} (see also section \ref{sec:rw:hmd:voice}).

In recent years, work is emerging that addresses these interaction challenges: Research presented on-device input modalities beyond traditional touch-based interfaces using a finger-mounted stylus~\ncite{Xia2015} or tapping gestures~\ncite{Oakley2015} on the device. As another approach, off-device input modalities have been proposed that increase the interaction space of such devices by leveraging the space around the device: Using infrared, acoustic or magnetic sensors embedded into the frame, the device can track the location of the fingertip of the user's dominant hand. This tracking allows users to perform off-screen (multi-)touch~\ncite{Butler2008, Harrison2010} or air gestures~\ncite{Harrison2009, Kim2007, Knibbe2014} around the device or on surrounding surfaces~\ncite{VanVlaenderen2015} and, thus, without occluding the content on the screen. Despite the advantages, the presented approaches still require both hands of the user. As another approach, one-handed interfaces have been proposed that allow users to trigger a set of actions by performing gestures with their finger or hand~\ncite{Kerber2015, Knibbe2014, Rekimoto2001, Xu2015a} of the arm wearing the watch. While such interfaces can be operated with one hand, they do not support continuous interactions.



\input{Content/Chapters/proxiwatch/prototype}
