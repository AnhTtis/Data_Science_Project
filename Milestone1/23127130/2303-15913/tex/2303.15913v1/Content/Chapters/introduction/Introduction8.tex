\citebox{Imagine a time far into the future, when all knowledge about our civilization has been lost. Imagine further, that in the course of planting a garden, a fully stocked computer store from the 1980s was unearthed, and that all of the equipment and software was in working order. Now, based on this find, consider what a physical anthropologist might conclude about the physiology of the humans of our era?}{buxton1986more}

\section{Motivation}

\acresetall

In his 1986 essay, \typcite{buxton1986more} wondered what conclusions a future anthropologist would draw from the current computer technology on the physiology of its users. With a humorous tone, he concluded that \enquote{My best guess is that we would be pictured as having a well-developed eye, a long right arm, uniform-length fingers, and a \enquote{low-fi} ear. But the dominating characteristic would be the prevalence of our visual system over our poorly developed manual dexterity.}

Thirty years have passed since Buxton’s description of the State-Of-The-Art in interacting with information. During the time of his writing, indirect interaction through mediator devices such as mouse and keyboard dominated interaction with computing systems. Since then, more direct multi-touch interaction~\ncite{Shneiderman1982} has found its way into stationary devices such as desktop PCs and has taken the market for mobile devices by storm. 

However, the utilization of our body for interacting with information might be even more limited today than it was when Buxton’s essay was written: While we use all of our fingers to interact with a keyboard, interaction with today’s touch devices is often limited to just the thumb tapping and sliding on the device, while the remaining fingers are degraded to hold and stabilize~\ncite{Xiong2014}. Therefore, such interfaces even further ignore the highly developed skills and dexterity of our body that we show in our daily interaction with the real world, increasing the mismatch between human physical abilities and the design of the computer systems we surround ourselves with. This style of interaction is largely based on the inherent limitations of such devices: Information is visualized as \emph{Pictures Under Glass}~\ncite{victor_2011}, bound to a small 2D surface.%

As a possible solution, the advent of see-through \acp{HMD} and \ac{AR} technologies allows information to \emph{break the glass} and spread into the real world. Such devices consist of a head-mounted combination of two semi-transparent displays (one per eye) to enable stereoscopic output, as well as sensors that allow tracking of the position and orientation of the user’s head~\ncite{Shibata2002}. To allow for natural movement in a (partially) virtual environment~\ncite{Sutherland1968}, such devices use the tracking data to calculate and display a perspectively accurate image. This allows digital information to appear as homogeneous members of reality, thus, enabling and affording a more physical interaction with information.

However, looking at how we interact with today’s \acp{HMD}, we mainly find touch-input 1) on the frame of the \ac{HMD}~\ncite{Islam2018} or on 2) accessories~\ncite{Ashbrook2011} or 3) voice-based input~\ncite{He2018}. Although these interaction styles are often practical and useful, they still largely ignore the degrees of freedom of our body and have various other disadvantages: Touch input on \acp{HMD} and accessories such as the \emph{Hololens Clicker} do not support direct manipulation of content. Voice input is difficult to use in noisy environments and imposes privacy and social acceptability concerns in public areas~\ncite{Starner2002}. Therefore, despite many recent advances that turned \acp{HMD} into \emph{versatile output devices}, there is still a lack of appropriate interaction techniques to transform such devices into \emph{versatile input devices}.

In recent years, more physical styles of interaction with \acp{HMD} and in general have emerged in the field of \emph{body-based} or \emph{body-centric} interaction. As the most prominent example, \emph{on-body interfaces}~\ncite{Harrison2012} gained wide-spread attention. Such on-body interfaces allow (multi-) touch input -- often combined with visual output -- on the surface of our body (e.g., \ncite{Harrison2010}). In most of these systems, the user’s non-dominant hand or arm acts as a two-dimensional interactive surface on which the opposing hand interacts with the content, mimicking the interaction with a hand-held or body-worn touchscreen device. Besides all the advantages, the interaction space is bound to the two-dimensional surface of the body. Moreover, this style of interaction requires both hands and, therefore, hardly supports situations where users’ hands are busy.

\section{Around-Body Interaction}
\label{sec:introduction:aroundbodyinteraction}

In order to overcome the limitations connected with the interaction \emph{on} the body, this thesis focuses on interaction in the space \emph{around} our body using our limbs, leveraging the learned skills and the dexterity that we show while interacting with the real world for interaction with \acp{HMD}.

\input{Content/Chapters/relatedwork/definition_aroundbody}

Such \aroundbodyinteraction{} techniques stand in a long tradition of works that leverage the degrees of freedom of our limbs for interaction. As the most prominent example for the upper limbs, mid-air gestures~\ncite{Colaco2013} received considerable attention in the field of body-based interaction with \acp{HMD}. Such gestures cover a far greater amount of the possible movements of our limbs, making it possible to use not only the surface of our body but also the area \emph{around} our body for interaction. 

However, despite a variety of examples of such interaction techniques around the body, challenges remain: Most of the presented approaches focus only on the upper limbs neglecting the degrees of freedom of our legs and feet. Thus, such approaches cannot support situations where the user’s hands are not available. In addition, most of these approaches only provide a fixed gesture set, limiting the expressiveness of the interaction. Further, these interaction techniques have always been considered independently, limiting their applicability to specific use cases. 

This thesis, in contrast, contributes to the vision of a concept that supports a variety of interaction techniques that leverage the degrees of freedom of our \emph{upper and lower} limbs for fast and natural interaction with information in the space around our body. Therefore, this work proposes to conceptualize these interactions in a common framework as \aroundbodyinteraction{}s to build a comprehensive concept for interaction with \acp{HMD}.

\subsection{Research Challenges}
\label{chp:introduction:challenges}

To illustrate the vision of ubiquitous interaction with information in a digitally augmented physical world, we consider a day in Alice’s life. In the following chapters, this thesis will revisit Alice’s day to demonstrate the suitability of the presented interaction techniques. 

\interactionbox{lbl}{Introduction}{Alice spends a day in the city. She will go shopping, meet friends, and roam the streets. Throughout the day, she is time and again faced with situations where she needs support from technology in various areas, from communication and navigation to information retrieval and entertainment. The interaction with the information takes place in various situations: While sitting, standing or walking, alone or during (local or remote) conversations with other people and while being hampered in her interaction because she carries her shopping.}

Based on the vision of ubiquitous \aroundbodyinteraction{} in a digitally augmented world, a number of research challenges arise that lead to the contributions of this thesis.

\begin{description}
	\item[Interaction Techniques for Upper and Lower Limbs] Depending on the context of use, one or more body parts and, thus, interaction techniques tailored to these body parts may not be available because of \emph{situational hindrances}. For example, when we carry things in our hands, we cannot use our hands for interaction. Or when we walk, the feet are not available for interaction. Therefore, a single interaction technique with a fixed set of limbs is not capable of supporting interaction in every situation. As a consequence of these situational hindrances, it is necessary to support interaction situations in which upper or lower limbs are not available for interaction. Therefore, interaction techniques for both limb-groups are necessary.
	
	\item[Support for Different Visualizations] Suitable interaction techniques are not only dependent on the limbs used for input, but must also be adapted to the output of the system. Different tracking technologies of \acp{HMD} allow different types of visualization that 1) move as the user moves (\emph{body-stabilized}) or 2) are anchored in the real world (\emph{world-stabilized}) and, thus, allow the user to move independently of the visualization. These different visualization techniques impose different requirements on the interaction with the system and, thus, require specific interaction techniques. 
	
	\item[Support for Interaction Situations] In addition to the general challenges of input and output, there are also special challenges that arise from specific interaction situations:
	
	\begin{description}
		\textfigH{relatedwork/interaction_situations}{A design space for \aroundbodyinteraction{} based on the 1) location of the interaction (\inplaceUpper{} or \mobilityUpper{}), 2) number of user (\singleuserUpper{} or \multiuserUpper{}), and 3) interaction style (\discreteUpper{} and \continuousUpper{}).}
		
		\item[\protect\inlinegraphicsame{mobility} Mobility] \acp{HMD} are inherently mobile devices. Due to their unique placement on the user’s head, such devices will be ultimately - once today’s technical limitations are overcome - always available and can support many situations without having to reach for a device. However, mobile use also poses particular challenges, especially during locomotion, which require interaction techniques to support such situations optimally.
		\item[\protect\inlinegraphicsame{in-place} In-Place] 
		Despite the inherent mobility of \acp{HMD}, interaction during stationary periods (e.g., while sitting or standing) will continue to take place in the future. Therefore, interaction techniques are required that support users in such situations.
		\item[\protect\inlinegraphicsame{single-user} Single-User] \acp{HMD} are radically private devices: Visual output and interaction with information is only available to the wearing user. Therefore, interaction techniques for single users are required.
		\item[\protect\inlinegraphicsame{multi-user} Multi-User] 
		While the focus on individual users given by the shape of \acp{HMD} offers a multitude of advantages (e.g., increased privacy), this also deprives \acp{HMD} of inherent opportunities for collaboration which, due to their physical design, are naturally available on other device classes. Therefore, interaction techniques are necessary that support multi-user collaboration.
		\item[\protect\inlinegraphicsame{continuous} Continuous Interaction] Continuous control like the gradual adjustment of a slider is important for fine-grained manipulation, especially in complex interaction situations. Therefore, interaction techniques supporting continuous interaction are necessary.
		\item[\protect\inlinegraphicsame{discrete} Discrete Interaction] In addition to fine-grained and precise continuous value changes, discrete interactions can offer the possibility for fast and immediate interactions. Such discrete interactions can, for example, support shortcuts to trigger frequently used interactions or allow to traverse cascading menus.
	\end{description}
	
\end{description}

\subsection{Contributions}

\textfigH{introduction/contributions}{The main contributions of this thesis, structured by \emph{limb used for input} and \emph{output stabilization}. Further, each contribution focuses on specific interaction situations (\inplaceUpper{}, \mobilityUpper{}, \singleuserUpper{}, \multiuserUpper{}, \continuousUpper{}, \discreteUpper{}). \protect\inlinegraphics{Content/Figures/General/published} depicts published papers, \protect\inlinegraphics{Content/Figures/General/submitted} depicts papers currently under submission.}%

Based on the research challenges, this thesis contributes four interaction techniques for \aroundbodyinteraction{}, each focusing on a unique combination of the \emph{limbs used for input} and the \emph{output stabilization} (see section \ref{chp:introduction:challenges}). Within the respective quadrant of the design space, each contribution focuses on a subset of the identified interaction situations (see figure \ref{fig:relatedwork/interaction_situations}). Further, each contribution discusses possible extension points and directions for future work to support further interaction situations. Based on this design space (see figure \ref{fig:introduction/contributions}), the contributions of this thesis are:

The first contribution is \projProximity{} and focuses on interaction using the \emph{upper limbs} with \emph{body-stabilized} interfaces that are augmented to the user’s body. This contribution focuses on how the degree of freedom offered by the elbow joint, i.e., \emph{flexion} by moving the hand towards and \emph{extension} by moving the hand away from the body on the user’s line of sight can be leveraged for interacting with \acp{HMD}. For this, the interaction space in front of the user is divided into multiple parallel planes where each plane corresponds to a layer with visual content. When moving the hand through the interaction space, the visual content is augmented to the user’s palm, allowing the user to browse through successive layers.

The second contribution is \projCloudbits{} and explores interacting using the \emph{upper limbs} with \emph{world-stabilized} interfaces. The contribution focuses on how the world-stabilization of information can be leveraged for new use cases that give meaning to the spatial location of information: Like documents in the real world, users can sort and group information and use the spatial layout to add meta-information to the actual information implicitly. Further, the chapter explores how the context-aware and proactive retrieval of information can support users.

The third contribution, \projCheesyfoot{}, focuses on using the \emph{lower limbs} with \emph{body-stabilized} interfaces, proposing foot-tapping as an input modality for interaction with \acp{HMD}. More precisely, the contribution explores the interaction with a semi-circular grid in the reachability of the user’s feet while standing. Part of this contribution is also the comparison of two different visualization techniques and their influence on the performance of the users: 1) direct interaction with interfaces that are displayed on the floor and require the user to look down to interact and 2) indirect interaction with interfaces that, although operated by the user’s feet, are displayed as a floating window in front of the user.

The fourth contribution, \projCheesyfootToGo{}, focuses on interacting using the \emph{lower limbs} with \emph{world-stabilized} interfaces. The focus of this contribution is on how users can interact with \acp{HMD} while walking without losing the connection to reality and, thus, getting themselves into potentially dangerous situations. Therefore, this thesis investigates the idea of using minimal shifts of the user’s walking path to interact with a visualization augmented to the ground.



\subsection{Publications}

All main contributions of this thesis have been published at international peer-reviewed conferences. This thesis uses parts of the content of the respective publications verbatim.

\projProximity is based on the publications 

\begin{small}
	\begin{addmargin}[25pt]{0pt} 
		\longfullcite{muller2015a}
		
		\longfullcite{muller2016proxiwatch}
	\end{addmargin}
\end{small}

\projCloudbits is based on the publication

\begin{small}
	\begin{addmargin}[25pt]{0pt} 
		\longfullcite{muller2017cloudbits}
	\end{addmargin}
\end{small}

\projCheesyfoot is based on the publication

\begin{small}
	\begin{addmargin}[25pt]{0pt} 
		\longfullcite{Muller2019}
		
	\end{addmargin}
\end{small}

\projCheesyfootToGo is based on the publication

\begin{small}
	\begin{addmargin}[25pt]{0pt} 
		\longfullcite{Muller2020}
	\end{addmargin}
\end{small}

\input{Content/Chapters/relatedwork/Methodology}

\section{Structure}

This thesis is structured as follows:

\begin{description}
	\item[Chapter \ref{ch:relatedwork}] discusses definitions and approaches from related works for interaction with \acp{HMD} (section \ref{sec:relatedwork:hmds}), body-based interaction (section \ref{sec:relatedwork:bodybased}) and \aroundbodyinteraction{} (section \ref{sec:rw:aroundbody}) and, further, establishes requirements.
	\item[Chapter \ref{ch:proximity:merged}] presents \projProximity, exploiting the proximity-dimension between the user's hand and head as an input modality for interacting with \acp{HMD}.
	\item[Chapter \ref{ch:cloudbits}] proposes \projCloudbits, leveraging the spatial dimension of world-stabilized output to support collaborative interaction with information in a shared information space.
	\item[Chapter \ref{ch:cheesyfoot}] contributes \projCheesyfoot, exploring direct and indirect interaction with augmented information using foot-taps.
	\item[Chapter \ref{ch:walktheline}] introduces \projCheesyfootToGo, leveraging lateral shifts of the walking path as an input modality for \acp{HMD}.
	\item[Chapter \ref{ch:conclusion}] integrates the previous contributions of this thesis (section \ref{sec:conclusion:summary}) and presents an outlook to future directions for research (section \ref{sec:conclusion:futurework}). 
\end{description}