\section{Body-based Interaction}
\label{sec:relatedwork:bodybased}

This section presents the related work in body-based interaction techniques. While many of the presented approaches are not directly tailored to the interaction with \acp{HMD}, the concepts can be transferred to this class of devices as \acp{HMD} can mimic the visual output of other systems (e.g., by virtually projecting visual output). Therefore, this section does not distinguish between interaction techniques for \acp{HMD} and other device classes.

First, this section gives an overview of the definition and history of body-based interaction (section \ref{sec:relatedwork:body:def}). Second, the section introduces a set of requirements for the body-based interaction with \acp{HMD} with respect to the vision of ubiquitous interaction in a digitally augmented physical world (section \ref{sec:relatedwork:body:requirements}). Finally, the section discusses three different streams of research on body-based interaction with \acp{HMD}: 1) \nameref{sec:rw:body:onbody}, 2) \nameref{sec:rw:body:headgaze}, and 3) \nameref{sec:rw:body:whole} (section \ref{sec:rw:body}).

\subsection{Definitions and Background}
\label{sec:relatedwork:body:def}

With the increasing proliferation of (low-cost) sensor hardware and advances in computer vision, research began to incorporate the human body as an input modality. These works belong to the area of \emph{body-based} (also \emph{body-centric}) interaction. Although not a new phenomenon, body-based interaction was first conceptualized and used as a term in a work in the field of input control for games in 2009. \typcite{Silva2009} described an approach which \enquote{we call body-based interaction [...]. The use of real-world movements to control virtual actions can make use of players’ physical skills from previous experiences with the world (e.g., muscle memory). Body movements allow players to utilize input modalities beyond the hand and fingers. [..] In VR, body-based interaction has promoted benefits such as better spatial understanding, and higher sense of presence.}

During the first wave of \ac{VR} in the mid-90s, such systems typically used expensive and complicated hardware setups for human motion tracking, e.g., the A/C magnetic sensors used by the CAVE project~\ncite{Cruz-Neira1992}. In the following years, research moved towards human motion recognition using only visual information using color and edge feature detection on 2D images~\ncite{Wang2018}. With the availability of inexpensive integrated color and depth cameras (RGB-D cameras, e.g., Microsoft Kinect), the focus of research has shifted to the utilization of such devices~\ncite{Zhang2012}. The progress in computer vision algorithms led to the possibility of body joint estimation~\ncite{Shotton2011} and was further improved by fitting it into a model of human kinesthetics, excluding unlikely positions and movements~\ncite{Corazza2006}, to allow for novel input techniques as discussed below. In recent years, progress in the field of computer vision and machine learning techniques~\ncite{Omran2018} has led to a decline in interest to the analysis of depth images. Instead, the focus again moved to the mere analysis of RGB images.

Many body-based interfaces are built upon the sense of proprioception: This proprioception, the \enquote{sensation of body position and movement}~\ncite{Tuthill2018}, gives us an innate understanding of the relative position and orientation of our body parts to each other. We can move our limbs without visual attention. This sense allows building user interfaces that minimize the interference with the user's interaction with the real world, as the (visual) attention is not entirely captured by the interaction. 

This sense allows for interactions without explicit feedback: \typcite{Lopes2015} leveraged this sense of proprioception and presented eyes-free interaction techniques, shaping the term \emph{Proprioceptive Interaction}. Subsequent research showed different use cases for such interactions, from controlling a music player~\ncite{Lissermann2013} or a TV set~\ncite{Dezfuli2012} to operating a phone~\ncite{Gustafson2011a}.

\subsection{Requirements}
\label{sec:relatedwork:body:requirements}

The following subsection establishes requirements for body-based interaction with \acp{HMD} with respect to the vision of ubiquitous interaction in a digitally augmented physical world, which are then used to classify existing research streams and the interaction concepts used in today’s commercially available \acp{HMD}.

\def\reqBlocking{Minimize Blocking of Body Parts}
\def\reqNatural{Minimize unnatural movements}
\def\reqInputSpace{Support for 3D-Interaction}
\def\reqExpressiveness{Maximize the expressiveness of Input}

\subsubsection{The Tension Between Interaction and Reality}

We use our body and especially our limbs to interact with the world: Our hands grasp and carry things, we walk and pedal with our feet, and we use our head to look around and point at things. Adding the ubiquitous interaction with an computing system imposes additional tasks on our body, entailing additional burdens. 

This becomes a particular challenge when an interaction with the \acp{HMD} coincides with an interaction with the real world. Any part of the body involved in an interaction with the \acp{HMD} is no longer (or, at least, no longer entirely) available for interaction with the real world. 

Therefore, interaction techniques should minimize the blocking of individual body parts.

\reqbox{body:blocking}{\reqBlocking}{Interaction techniques should require as few body parts as possible at the same time in order not to complicate regular interaction with the real world.}

We perform the above interactions with the world in very specific ways that respect the characteristics of our body. This is also required for interaction with HMDs: For natural interaction, it is essential not to force users to make uncomfortable and unnatural movements.

Therefore, interaction techniques should accept the natural boundaries of the human locomotor system.

\reqbox{body:natural}{\reqNatural}{Interaction techniques should not require unnatural and uncomfortable movements.}

\subsubsection{Interaction after Breaking the Glass}

As discussed in section \ref{sec:relatedwork:hmds:requirements:physicality}, \acp{HMD} allow information to leave the limited display area of today’s devices and spread throughout the real world, adding a third dimension. This dispersion of the output in the world also requires appropriate interaction techniques. 

Therefore, in particular, interaction techniques are required which support three-dimensional information spaces.

\reqbox{body:inputspace}{\reqInputSpace}{Interaction techniques should leverage the entire interaction space available, which is created by the output in the three-dimensional space.}

Today’s smartphones have developed into mobile all-rounders: We use them - just as examples - for communication, navigation, entertainment, and information retrieval. \acp{HMD} meet the technical requirements to perform all of these tasks in possibly exciting new ways due to the additional output capabilities. 

However, communicating these tasks as well as the necessary information to complete the tasks to the system can require a variety of necessary interaction steps. 

Therefore, expressive interaction techniques are required.

\reqbox{body:expressiveness}{\reqExpressiveness}{Interaction techniques should provide a high level of expressiveness to transfer a maximum of information in a minimum of time.}

\subsection{Related Work}
\label{sec:rw:body}

In the following, this section discusses three different streams of research for body-based interaction techniques for \acp{HMD} using 1) \nameref{sec:rw:body:onbody}, 2) \nameref{sec:rw:body:headgaze}, and 3) \nameref{sec:rw:body:whole}.

\subsubsection{On-Body Interaction}
\label{sec:rw:body:onbody}

Recent advances in input and output technology have led to the emergence of so-called \emph{on-body}~\ncite{Harrison2012} interfaces, leveraging the human body as an interactive surface for both, input and output. In these systems the input is performed by touch-based interactions on projected~\ncite{Winkler2014, Mistry2009, Wilson2010}, augmented~\ncite{Ha2014} or imaginary~\ncite{Dezfuli2012, Gustafson2013, Oh2014} user interfaces.

Harrison et al. paved the way for such on-body interfaces in their two groundbreaking works, Skinput~\ncite{Harrison2010}, and Omnitouch~\ncite{Harrison2011a}. The papers presented approaches for the acoustic and, respectively, optical localization of touch events on the arm and hand of the user together with projected visual output on these body parts. Focussing again on the topic of on-body interfaces, \typcite{Harrison2014} further explored the implications of different locations for input and output on the human body. Building on this work, \typcite{Weigel2014} increased the input space for on-body interactions by analyzing how additional input modalities, such as pulling, pressing, and squeezing, can be used for more expressive interactions. \typcite{Mehta2016} added itching and scratching to the input space. \typcite{Bostan2017} contributed a user-elicitation study, collecting and comparing on-body gestures.

The vast majority of the work concentrated on the hands and forearms of the users due to the anatomically easy accessibility and because they are often unclothed and socially acceptable to touch~\ncite{Wagner2013}. Yet, other parts of the body, such as the abdomen~\ncite{Vo2014} or the ear~\ncite{Lissermann2013}, were also examined for input.

\paragraph{Further Sensing Techniques}

Beyond the approaches presented by Harrison et al. and adopted by the other works cited, research proposed other sensing techniques. For example, \typcite{Saponas2009b} proposed to sense touch through sensing human muscle activity. Further, \typcite{Wang2016} showed how to reconstruct finger movement on the body using the skin stretch, and \typcite{Matthies2015} analyzed how unique electric signatures of different body parts can be used to localized touch events on any part of the body.

\paragraph{Accessories}

With the packaging of sensor techniques into body-worn accessories, research developed several approaches for supporting on-body interaction on the move while, at the same time, realizing Mark Weisser’s demand for disappearing technologies~\ncite{weiser1991computer}. These accessories range from attachable skin buttons~\ncite{Laput2014} and clothing~\ncite{Heller2014, Ueda2018}, to belts~\ncite{Dobbelstein2015}, wristbands~\ncite{Dobbelstein2018} and rings~\ncite{Ashbrook2011}.



\paragraph{Body Parts as Public Displays}

Visual output on body parts can also be interpreted as a visualization on a public display~\ncite{Olberding2013}. \typcite{Hoang2018} used such on-body visual output to use the human body as a canvas to communicate information. Such visual output can also reflect the use of tattoos as a means of communicating information intended for the public. This resulted in a stream of research focusing on integrating sensing technology into tattoo-styled skin-worn accessories ~ \ncite{Weigel2015, Lo2016, Kao2016}. \typcite{Strohmeier2016} analyzed the tension between the input on and with one’s own body, perceived as something very personal, and the perception of the skin as a public display that is also (passively) accessible to other people which must be considered in the design of such systems. 

\paragraph{Conclusion}

Such on-body interfaces have shown considerable advantages over other approaches for the mobile use of computing systems (e.g., during sports~\ncite{Hamdan2017, Vechev2018}): No second device is necessary for interaction, which can be forgotten or lost. In addition, the limited interaction surface of today’s mobile devices - tablets, smartphones, or smartwatches - is replaced with the much larger surface of the user’s body. Leveraging the sense of proprioception, such interfaces can be even operated without visual attention (see section \ref{sec:relatedwork:body:def}).

However, a closer look reveals challenges which remain unsolved, especially in the context of mobile use with \acp{HMD}. In most of these systems, the user’s non-dominant hand acts as a two-dimensional interactive surface on which the opposing hand interacts with content through (multi)-touch gestures. While useful and practical, the interaction space is bound to the two-dimensional surface of the hand, imitating the interaction with a hand-held mobile device and ignoring the benefits of three-dimensional output and input. Moreover, this style of interaction requires both hands and, therefore, hardly supports situations, where users are encumbered.

\subsubsection{Head- and Gaze-based Interaction}
\label{sec:rw:body:headgaze}

As another approach to body-based interaction, research proposed the usage of gaze-input as a hands-free input modality for \acp{HMD}~\ncite{Lukander2013}. Such systems face the challenge of distinguishing between intentional input and regular body movements, known as the \emph{Midas Touch Problem}~\ncite{Jacob1995}. To overcome this challenge, research proposed to exploit dwell time~\ncite{Wobbrock2008}, active pupil size manipulation~\ncite{Ekman2008} or gaze gestures~\ncite{Rantala2015}. 

However, introducing dwell times increases interaction times and gaze gestures reduce the general applicability of the approaches. In addition, research showed that gaze interactions feel unnatural since the natural task of the eyes is to capture sensory information, and it is fatiguing to use the gaze for continuous manipulation tasks~\ncite{Zhai1999}, decreasing the expressiveness of gaze~\ncite{Chatterjee2015}. Also, today’s sensors require time-consuming calibration steps and suffer from decreasing accuracy because of movements of the glasses ~\ncite{Kyto2018}.

Recently, also head-pointing~\ncite{Morris2000} has emerged as a promising interaction techniques for \acp{HMD} in both industry (e.g., Microsoft Hololens) and research~\ncite{Clifford2017}. 

However, research showed that, while being fast and precise~\ncite{Qian2017}, head-pointing can require unnatural movements and, further, cause fatigue~\ncite{Kyto2018}.

\subsubsection{Whole-body and Embodied Interaction}
\label{sec:rw:body:whole}

As another approach, research proposed to harness all available information about the current state of the user for \emph{Whole-Body Interaction}. \typcite{England2009} defined \emph{Whole-Body Interaction} as \enquote{The integrated capture and processing of human signals from physical, physiological, cognitive and emotional sources to generate feedback to those sources for interaction in a digital environment}~\ncite{England2009}. In a later publication, \typcite{England2011} further added that \enquote{The key word here is integration [...] to use two or more [...] input categories in combination so that we can [...] get a richer picture of what the user intends when they move their body [...]}.

The central idea behind \emph{Whole-Body} interaction is to include as much information as possible about the user in the interpretation of actions. This does not only involve physical movements but also - with references to the field of affective computing~\ncite{Picard1997} - the emotional state of the user.

As a highly related field of research, \emph{Embodied Interaction} has attracted much attention in recent years. In embodied interaction, the concept of embodiment has not only references to the involvement of the body but stands in the philosophical tradition of phenomenology as proposed by \typcite{Husserl1913} and \typcite{heidegger1977sein}. \typcite{Dourish2004} defined such embodied interactions as \enquote{the creation, manipulation, and sharing of meaning through engaged interaction with artifacts} and further explained that \enquote{When I talk of \emph{embodied interaction}, I mean that interaction is an embodied phenomenon. It happens in the world, and that world (a physical world and a social world) lends form, substance, and meaning to the interaction}~\ncite{Dourish1999}. %

Having all this physical, physiological, and cognitive information at hand, research applied the concept of whole-body and embodied interaction to a plethora of application areas. \typcite{Maes1997} showed how whole-body interaction could be used to interact with agents in a virtual world. \typcite{Walter2015} proposed techniques for interacting with public displays. \typcite{Price2016} and \typcite{Freeman2013} explored whole-body interaction to engage visitors in museums. \typcite{Cafaro2012} proposed allegories to encourage data exploration using whole-body interaction. \typcite{Dezfuli2012} used implicit and pose-based embodied interactions to control a TV set. Further examples of whole-body and embodied interaction include virtual training~\ncite{Reidsma2011}, assisted living~\ncite{Altakrouri2014}, support for elderly~\ncite{Ferron2019}, interaction with virtual environments~\ncite{Hasenfratz2004}, supporting collaboration~\ncite{Malinverni2015}, interaction with smartphones~\ncite{Khalilbeigi2011} or human-robot interaction~\ncite{Peternel2013}.  More abstracted from specific application domains, \typcite{Fogtmann2008} described how whole-body interaction could serve as a foundation for designing interactive systems.

Further, whole-body interfaces received particular attention in the area of games and gamified~\ncite{Deterding2011} systems. \typcite{Gerling2012} used whole-body and motion-based game controls for the elderly and found that such controls \enquote{can accommodate a variety of user abilities, have a positive effect on mood and, by extension, the emotional well-being of older adults}. Following this thread of research, \typcite{schonauer2015full} showed how whole-body interaction with games can help patients in motor rehabilitation following, e.g., strokes or traumatic brain injuries. \typcite{Bianchi-Berthouze2007} analyzed the effect of whole-body interaction on the engagement levels of players of games and found a significant increase in engagement compared to controller-based input. %

By including such a large amount of information about the user and his context in the evaluation and interpretation of interactions, both positive and negative effects emerge. Such interactions can communicate a large amount of information with small interactions and, thus, show a high expressiveness. Further, the interactions are not limited to a specific area and fully support interaction in the 3D space.

However, monitoring such a multitude of information (not only physiological but also cognitive) also requires a large amount of sensors and, thus, poses technical challenges that are not yet been finally solved. Further, the existing solutions are always tailored to a specific application, a general interaction framework for use in all situations such as WIMP for the desktop~\ncite{VanDam1997} is not yet in sight. This lack of a general basis of interaction primitives can lead to overstraining users, who have to remember a different style of interaction for each device or application. Additionally, since whole-body interaction considers the entire body, such interaction techniques can hardly support situations in which body-parts are not available, for example, because of carrying something in hand.






\subsection{Conclusion}

\input{tables/requirements_body}

Comparing the basic concepts for body-based interaction with \acp{HMD} to the established requirements, none of the presented approaches can fulfill all of the requirements \refreq{req:body:blocking} - \refreq{req:body:expressiveness} (see table \ref{tab:req:body}). The following section introduces \aroundbodyinteraction{} as an alternative interaction concept for \acp{HMD} and demonstrates that this type of interaction can fulfill all of the requirements.


