\section{Related work}
\noindent \textbf{Implict Neural Representation}.
Implicit neural representations (also known as coordinate-based representations) are a popular way to parameterize content of all kinds, such as audio, images, video, or 3D scenes~\cite{FFL, siren, srn, NeRF}.
Recent works \cite{NeRF, DeepSDF, occnet, srn} build neural implicit fields for geometric reconstruction and novel view synthesis achieving outstanding performance.
The implicit neural representation is continuous, resolution-independent, and expressive, and is capable of reconstructing geometric surface details and rendering photo-realistic images. 
%
While explicit representations like point clouds\cite{points1, NHR}, meshes\cite{NT}, and voxel grids\cite{deepvoxels, occnet, NeuralVolume, voxel1} are usually limited in resolution due to memory and topology restrictions.
%
One of the most popular implicit representations - Neural Radiance Field (NeRF) \cite{NeRF} -  proposes to combine the neural radiance field with differentiable volume for photo-realistic novel views rendering of static scenes. However, NeRF requires optimizing the 5D neural radiance field for each scene individually, which usually takes hours to converge. Recent works\cite{PixelNeRF, ibrnet, MVSNeRF} try to extend NeRF to generalization with sparse input views.
%
In this work, we extend the neural radiance field to a general human reconstruction scenario by introducing conditional geometric code and appearance code. 


\noindent \textbf{3D Model-based Human Reconstruction}
With the emergence of human parametric models like SMPL\cite{SMPL,SMPLX} and SCAPE\cite{SCAPE}, many model-based 3D human reconstruction works have attracted wide attention from academics. Benefiting from the statistical human prior, some works\cite{tex2shape, Multi-Garment, expose, VIBE} can reconstruct the rough geometry from a single image or video. 
However, limited by the low resolution and fixed topology of statistical models, these methods cannot represent arbitrary body geometry, such as clothing, hair, and other details well. 
To address this problem, some works\cite{PIFu, pifuhd} propose to use pixel-aligned features together with neural implicit fields to represent the 3D human body, but still have poor generalization for unseen poses. To alleviate such generalization issues, \cite{pamir, arch, doublefield} incorporate the human statistical model SMPL\cite{SMPL, SMPLX} into the implicit neural field as a geometric prior, which improves the performance on unseen poses. 
Although these methods have achieved stunning performance on human reconstruction, high-quality 3D scanned meshes are required as supervision, which is expensive to acquire in real scenarios. Therefore, prior works\cite{PIFu, pifuhd, pamir, arch} are usually trained on synthetic datasets and have poor generalizability to real scenarios due to domain gaps. To alleviate this limitation, 
some works\cite{neuralbody, Anim-NeRF, animnerf_zju, humannerf, arah, a-nerf}  combine neural radiance fields\cite{NeRF} with SMPL\cite{SMPL} to represent the human body, which can be rendered to 2D images by differentiable rendering. 
Currently, some works\cite{gpnerf, genebody, NHP, keypointNeRF, doublediffuse, doublefield} can quickly create neural human radiance fields from sparse multi-view images without optimization from scratch.
While these methods usually rely on accurate SMPL estimation which is not always applicable in practical applications. 
% We introduce a xxx

% identity-specific models, like NeuralBody\cite{neuralbody}
% generalizable models, 
% SMPL\cite{SMPL}, SMPLX\cite{SMPLX}, SCAPE\cite{SCAPE}, Tex2Shape\cite{tex2shape}, Multi-Garment Net\cite{Multi-Garment}, VIBE\cite{VIBE}, Expose\cite{expose}, NeuralBody\cite{neuralbody}, Anim-NeRF\cite{animnerf_zju, animnerf}, Neural Actor\cite{neuralactor}, SelfRecon\cite{selfrecon}, HumanNeRF\cite{humannerf}, PIFu\cite{PIFu}, PIFuHD\cite{pifuhd}, Pamir\cite{pamir}, Arch\cite{arch}, Double Field\cite{doublefield}, GNR\cite{genebody}, NHP\cite{NHP}, GPNeRF\cite{gpnerf}, KeypointNeRF\cite{keypointNeRF}, DoubleDiffuse\cite{doublediffuse}











\begin{figure*}[ht]
    \centering
    \vspace{-1em}
    \includegraphics[width=1.0\linewidth]{figures/method/pipeline.png}
    \vspace{-1.5em}
    \caption{\textbf{The architecture of our method}. Given $m$ calibrated multi-view images and registered SMPL, we build the generalizable model-based neural human radiance field. First, we utilize the image encoder to extract multi-view image features, which are used to provide geometric and appearance information, respectively. In order to adequately exploit the geometric prior, we propose the visibility-based attention mechanism to construct a structured geometric body embedding, which is further diffused to form a geometric feature volume. For any spatial point $\mathbf{x}$, we trilinearly interpolate the feature volume $\mathcal{G}$ to obtain the geometric code $\mathbf{g}(\mathbf{x})$. In addition, we also propose geometry-guided attention to obtain the appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$ directly from the multi-view image features. We then feed the geometric code $\mathbf{g}(\mathbf{x})$ and appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$ into the MLP network to build the neural feature field $(\mathbf{f}, \sigma) = F(\mathbf{g}(\mathbf{x}), \mathbf{a}(\mathbf{x}, \mathbf{d}))$. Finally, we employ volume rendering and neural rendering to generate the novel view image.
    % \Liqian{1) Add section ref. 2) add detailed caption. 3) Modulate the fig, each module corresponds to a sub-section. 4) keep fig text  consistent with method text}
    }
    \vspace{-1em}
    \label{fig:architecture}
\end{figure*}
