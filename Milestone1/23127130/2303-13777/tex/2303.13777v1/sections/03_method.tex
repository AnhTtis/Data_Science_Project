% \section{Preliminaries: Neural Radiance Fields}

% Neural Radiance Field (NeRF\cite{NeRF}) is a popluar learning-based  3D reconstruction method, which uses a MLP network $F(\mathbf{x}, \mathbf{d})=(\mathbf{c}, \sigma)$ to map 3D position $\mathbf{x}$ and view direction $\mathbf{d}$ to color $\mathbf{c}$ and density $\sigma$. 
% However, the architecture is designed for a single scene and cannot generalize to unseen scenes. To address such limitation, we extend it to general scenes by introducing geometric code $\mathbf{g}(\mathbf{x})$ and appearance code $\mathbf{a}(\mathbf{x},\mathbf{d})$ instead of the query point $\mathbf{x}$ and view direction $\mathbf{d}$ as inputs.
% \begin{equation}
% F(\mathbf{g}(\mathbf{x}), \mathbf{a}(\mathbf{x},\mathbf{d}))=(\mathbf{c}, \sigma)
% \end{equation}



\section{Method}




% \Liqian{We introduce an effective framework - X - for human 3d reconstruction. X learns generalizable model-based neural radiance fields from calibrated multi-view images ...}

% \Liqian{
% Q:\\
% 1) Maybe use another preliminary subsection for NeRF?\\
% 2) The generalizability in our case is unseen pose but not unseen identity, right? If so, it is necessary to claim it clearly in the beginning, as the reader may expect too much.
% }

% \Liqian{
% Neural Radiance Field (NeRF\cite{NeRF}) is a popular learning-based 3D reconstruction method which ...
% However, ...
% To address such limitation, we propose ...
% }

% Our work is aimed to build generalizable model-based neural radiance fields from calibrated multi-view images that can be used for novel view synthesis and 3D human reconstruction, which introduce a parametric model as a geometric prior.
We introduce an effective framework GM-NeRF for novel view synthesis and 3D human reconstruction as illustrated in ~\cref{fig:architecture}. GM-NeRF learns generalizable model-based neural radiance fields from calibrated multi-view images by introducing a parametric model SMPL as a geometric prior, which can generalize to unseen identity and unseen pose.


Given $m$ calibrated multi-view images $\left\{I_{k}\right\}_{k=1}^{m}$ of a person, we use Easymocap\cite{EasyMocap} to obtain the SMPL\cite{SMPL} parameters $\mathbf{M}(\theta, \beta)$ of the person. We feed the multi-view images into the encoder network $\mathbf{E}$ to extract multi-view feature maps,
\begin{equation}
\begin{aligned}
H_{k} &= \mathbf{E}(I_{k}), \quad k=1,2,\ldots,m .
\end{aligned}
\end{equation}

For any 3D position $\mathbf{p}$, we can project it onto the feature map $H_{k}$ according to the corresponding camera parameters, which is defined as $\pi _{k}(\cdot)$, then use bilinear interpolation $\Psi(\cdot)$ to obtain the pixel-aligned feature $\mathbf{h}_{k}(\mathbf{p})$ and pixel-aligned color $\mathbf{r}_{k}(\mathbf{p})$ as follows,
\begin{equation}
\begin{aligned}
% H_{k} &= \mathbf{E}(I_{k}), \quad k=1,2,\ldots,m \\ 
\mathbf{h}_{k}(\mathbf{p}) &= \Psi (H_{k}, \pi _{k}(\mathbf{p})), \\
\mathbf{r}_{k}(\mathbf{p}) &= \Psi (I_{k}, \pi _{k}(\mathbf{p})).
\end{aligned}
\end{equation}
In order to adequately exploit the geometric prior, we propose the visibility-based attention mechanism to construct a structured geometric body embedding, which is further diffused to form a geometric feature volume (\cref{Structured_Geometric_Body_Embedding}). Afterward, we trilinear interpolate each spatial point $\mathbf{x}$ in the feature volume $\mathcal{G}$ to obtain the geometric code $\mathbf{g}(\mathbf{x})$. 
To avoid the misalignment between the appearance code and geometry code, we utilize the geometry code as a proxy and then register the appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$ directly from the multi-view image features with a novel geometry-guided attention mechanism (\cref{Multi-View_Appearance_Blending}).
% In addition, we also propose geometry-guided attention to obtain the appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$ directly from the multi-view image features (\cref{Multi-View_Appearance_Blending}). 
We then feed the geometric code $\mathbf{g}(\mathbf{x})$ and appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$ into the MLP network to build the neural feature field $(\mathbf{f}, \sigma) = F(\mathbf{g}(\mathbf{x}), \mathbf{a}(\mathbf{x}, \mathbf{d}))$ followed by volume rendering and neural rendering for novel view image generation (\cref{Differential_Rendering}). 
To obtain high-quality results, we carefully design an optimization objective including a novel normal regularization. (\cref{Loss_Functions}) as well as an efficient training strategy (\cref{Efficient_Training}).
% For stable training, we use multiple loss functions to optimize the network (\cref{Loss_Functions}).


\subsection{Structured Geometric Body Embedding}
\label{Structured_Geometric_Body_Embedding}
Different from neural radiance fields on general scenes, we introduce a parametric body model to provide the geometric prior for constructing the neural human radiance field, which can enhance generalizability under unseen poses. In our experiments, we choose the SMPL\cite{SMPL} model as the parametric model. The SMPL\cite{SMPL} model $\mathbf{M}(\theta, \beta)$ is a mesh with $N=6,890$ vertices $\left\{\mathbf{v}_{i}\right\}_{i=1}^{N}$, where it is mainly controlled by the pose parameter 
$\theta$, and the shape parameter $\beta$. 
NeuralBody\cite{neuralbody} optimizes a set of structured latent codes from scratch on vertices of the SMPL model for each specific identity. However, not only does it fail to represent a new identity but also has poor generalizability on unseen poses. To address such limitation, we extract the structured latent codes $\mathcal{Z}=\left\{\mathbf{z}_{i}\right\}_{i=1}^{N}$ from the multi-view feature map $H_{k}$ as a geometric embedding to represent arbitrary identities. 
% \Liqian{Is it the main difference between our Structured Geometric Body Embedding and neuralBody? If so, we should claim it more clearly, and describe the motivation and intuition of our design.}
For vertex $\mathbf{v}_{i}$, we design a visibility-based attention mechanism as shown in \cref{fig:architecture} to fuse multi-view features.
\begin{equation}
\begin{aligned}
\mathbf{Q}_{g}(\boldsymbol{v}_{i}) &=F_{Q}^{g}\left(\boldsymbol{n}_{i}\right) \\
\mathbf{K}_{g}(\boldsymbol{v}_{i}) &=F_{K}^{g}(\{\mathbf{h}_{k}(\boldsymbol{v}_{i}) \oplus \mathbf{d}_{k}\}_{k=1}^{m}) \\
\mathbf{V}_{g}(\boldsymbol{v}_{i}) &=F_{V}^{g}(\{\mathbf{h}_{k}(\boldsymbol{v}_{i})\}_{k=1}^{m}) \\
\mathbf{z}_{i} &=F^{g}\left(Att\left(\mathbf{Q}_{g}(\boldsymbol{v}_{i}), \mathbf{K}_{g}(\boldsymbol{v}_{i}), \mathbf{V}_{g}(\boldsymbol{v}_{i})\right)\right)
\end{aligned}
\end{equation}
where $\oplus$ is the concatenation operator, and $\boldsymbol{n}_{i}$ is the normal of the vertex $\boldsymbol{v}_{i}$. $F_{Q}^{g}$, $F_{K}^{g}$, $F_{V}^{g}$ denote the geometric linear layers producing the query, key, and value matrices $\mathbf{Q}_{g}(\boldsymbol{v}_{i})$, $\mathbf{K}_{g}(\boldsymbol{v}_{i})$, $\mathbf{V}_{g}(\boldsymbol{v}_{i})$, respectively. $Att$ is the attention mechanism proposed by \cite{attention}. $F^{g}$ is the geometric feed-forward layer. The intuition of this visibility-based attention mechanism is that the closer the input camera direction $\mathbf{d}_{k}$ is to the normal $\boldsymbol{n}_{i}$, the more the corresponding feature contributes. As shown in \cref{fig:att_vis}, the visualization result demonstrates the plausibility of this design.

Similar to NeuralBody\cite{neuralbody}, we use SparseConvNet\cite{spconv} $\mathbf{D}$ to diffuse the structured latent codes $\{\mathbf{z}_{i}\}_{i=1}^{N}$ into the nearby space to form a 3D feature volume $\mathcal{G}$.

\begin{equation}
\begin{aligned}
    \mathcal{G} = \mathbf{D} (\{\mathbf{z}_{i}\}_{i=1}^{N}) \\
    \mathbf{g}(\mathbf{x}) = \Phi (\mathbf{x}, \mathcal{G} )
\end{aligned}
\end{equation}
where $\Phi(\cdot)$ is the trilinear interpolation operation, which is applied to obtain the geometric code $\mathbf{g}(\mathbf{x})$ for any 3D position $\mathbf{x}$ during volume rendering.




\subsection{Multi-View Appearance Blending}
\label{Multi-View_Appearance_Blending}

% \Liqian{We need to write more about the misalignment. In Method, I write this sentence "To avoid the misalignment between appearance code and geometry code, we utilize the geometry code as a proxy and then register the the appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$ directly from the multi-view image features with a novel geometry-guided attention mechanism". We shall write something consistent here.}




Although the structured geometric body embedding provides a robust geometric prior, high-frequency appearance details such as wrinkles and patterns are lost, due to the low resolution and the minimally-clothed topology of the parametric model. 
% \Liqian{An illustration figure is necessary here to visualize the attention. Is there any specific design or choice for the QKV of the attention?}
In practice, inaccurate SMPL estimation will lead to the misalignment between the 3D geometry and pixel space, which will cause blur and distortion when fusing the geometry and pixel-aligned feature.
To solve this problem, we design a geometry-guided attention mechanism as shown in \cref{fig:architecture}, which utilizes the geometry code as a proxy and then registers the appearance code $\mathbf{a}(\mathbf{x}, \mathbf{d})$  directly from the multi-view image features for any 3D position $\mathbf{x}$ and view direction $\mathbf{d}$.
\begin{equation}
\begin{aligned}
\mathbf{Q}_{a}(\mathbf{x}) &=F_{Q}^{a}\left(\mathbf{g}(\mathbf{x}) \oplus \mathbf{d}\right) \\
\mathbf{K}_{a}(\mathbf{x}) &=F_{K}^{a}(\{\mathbf{h}_{k}(\mathbf{x}) \oplus \mathbf{d}_{k}\}_{k=1}^{m}) \\
% V(\mathbf{x}) &=F_{V}^{a}(\{\Psi (H_{k}, \Pi _{k}(\mathbf{x})) \oplus \Psi (I_{k}, \Pi _{k}(\mathbf{x}))\}_{k=1}^{m}) \\
\mathbf{V}_{a}(\mathbf{x}) &=F_{V}^{a}(\{\mathbf{h}_{k}(\mathbf{x}) \oplus \mathbf{r}_{k}(\mathbf{x})\}_{k=1}^{m}) \\
\mathbf{a}(\mathbf{x}, \mathbf{d}) &=F^{a}\left(Att\left(\mathbf{Q}_{a}(\mathbf{x}), \mathbf{K}_{a}(\mathbf{x}), \mathbf{V}_{a}(\mathbf{x})\right)\right)
\end{aligned}
\end{equation}
% \lipsum[13-14]
where $F_{Q}^{a}$, $F_{K}^{a}$, $F_{V}^{a}$ denote the appearance layers producing the query, key, and value matrices $\mathbf{Q}_{a}(\mathbf{x})$, $\mathbf{K}_{a}(\mathbf{x})$, $\mathbf{V}_{a}(\mathbf{x})$, respectively. $F^{a}$ is the appearance feed-forward layer.








\subsection{Differential Rendering}
\label{Differential_Rendering}

After we get the geometric code and appearance code of any 3D point, we design a two-stage MLP network $F(\cdot)$ to build the neural feature field.
\begin{equation}
(\mathbf{f}, \sigma) = F(\mathbf{g}(\mathbf{x}), \mathbf{a}(\mathbf{x}, \mathbf{d}))
\end{equation}

Unlike classical NeRF\cite{NeRF}, which regresses color $\mathbf{c}$ and density $\sigma$, our decoder outputs the intermediate feature $\mathbf{f}$ and density $\sigma$. However, the original volume rendering process is memory-consuming, we use a combination of volume rendering and neural rendering to get the final image. 



\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/GeNeRFs_v1.png}
    \vspace{-2.5em}
    \caption{{\bf Qualitative comparison with generalizable NeRFs}. We input $m = 4$ multi-view images of unseen identity, and our method produces a more photo-realistic novel view image compared to other state-of-the-art generalizable human NeRFs\cite{ibrnet, NHP, genebody, keypointNeRF}. The first two rows are from Multi-Garment\cite{Multi-Garment}, the third row from THuman2.0\cite{THuman2.0} and the last row from GeneBody\cite{genebody}.}
    \label{fig:results_on_GeNeRFs}
    \vspace{-0.5em}
\end{figure*}

\input{tables/generalizable_nerf}








{\bf 3D Volume Rendering}. We use the same volume rendering techniques as in NeRF~\cite{NeRF} to render the neural radiance field into a 2D image. Then the pixel colors are obtained by accumulating the colors and densities along the corresponding camera ray $\mathbf{\tau }$. In practice, the continuous integration is approximated by summation over sampled $N$ points $\left\{\mathbf{x}_{i}\right\}_{i=1}^{N}$ between the near plane and the far plane along the camera ray $\mathbf{\tau }$. 
\begin{equation}
\label{eq:volume rendering}
\begin{aligned}
\mathbf{\mathcal{F} }(\mathbf{\tau }) &= \sum_{i=1}^{N} \alpha_{i}\left(\mathbf{x}_{i}\right) \prod_{j<i}\left(1-\alpha_{j}\left(\mathbf{x}_{j}\right)\right) \mathbf{f}\left(\mathbf{x}_{i}\right) \\
\mathbf{\mathcal{M} }(\mathbf{\tau }) &= \sum_{i=1}^{N} \alpha_{i}\left(\mathbf{x}_{i}\right) \prod_{j<i}\left(1-\alpha_{j}\left(\mathbf{x}_{j}\right)\right) \\
\alpha_{i}(\mathbf{x}) &= 1-\exp \left(-\sigma(\mathbf{x}) \delta_{i}\right)  \\
\end{aligned}
\end{equation}
where $\delta_{i}=\left\|\mathbf{x}_{i+1}-\mathbf{x}_{i}\right\|_{2}$ is the distance between adjacent sampling points. $\alpha_{i}(\mathbf{x})$ is the alpha value for $\mathbf{x}$. The intermediate feature image $I_{\mathcal{F}}\in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times M_{\mathcal{F}}}$ and the silhouette image $I_{\mathcal{M}}\in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times 1}$ is obtained by \cref{eq:volume rendering}.

{\bf 2D Neural Rendering}.
\label{Neural_Rendering}
We utilize a 2D convolutional network $G_{\theta}$ to convert the intermediate feature image $I_{\mathcal{F}} \in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times M_{\mathcal{F}}}$ rendered by volume rendering into the final synthesized image $I_t \in \mathbb{R}^{H \times W \times 3}$.

\begin{equation}
    I_t \longleftarrow G_{\theta}\left( I_{\mathcal{F}} \right)
\end{equation}
where $\theta$ is the parameters of the 2D neural rendering network $G$, which means the rendering procedure is learnable.




\subsection{Loss Functions}
\label{Loss_Functions}

To stabilize the training procedure, we adopt the pixel-wise L2 loss widely used in \cite{PixelNeRF,NHP,genebody} to constrain  the rendered image $I_{t}$ and the alpha image $I_{\mathcal{M}}$.
% {\bf Reconstruction Loss}. The reconstruction loss aims to minimize the error between the rendered image $I_{t}$ and the target image $\tilde{I}_{t}$, which is defined as 

\begin{equation}
\mathcal{L}=\lambda_{r}\left\|\tilde{I}_{t} - I_{t} \right\|_{2}^{2} + \lambda_{s}\left\|\tilde{I}_{\mathcal{M}} - I_{\mathcal{M}} \right\|_{2}^{2}
\end{equation}
where $\tilde{I}_{t}$, $\tilde{I}_{\mathcal{M}}$ are the ground-truth of the RGB image and silhouette image, respectively and $\lambda_{r}$, $\lambda_{s}$ are the weights. Beyond that, we also introduce the following loss functions to optimize the networks together,
% {\bf Silhouette Loss}. We minimize the difference between the rendered alpha map $I_{\mathcal{M}}$ and the target silhouette map $\tilde{I}_{\mathcal{M}}$.

% \begin{equation}
%     \mathcal{L}_{s}=\left\|\tilde{I}_{\mathcal{M}} - I_{\mathcal{M}} \right\|_{2}^{2}
% \end{equation}

{\bf Perceptual Loss}
\label{Perceptual_Loss}. We use a perceptual loss\cite{Perceptual_Loss} based on the VGG Network\cite{VGG}. It is more effective when the size of the images is closer to the network input, while it is memory intensive to render the whole image by volume rendering. To address these limitations, we adapt both neural rendering as well as partial gradient backpropagation.
% \Liqian{We should emphasize it more and connect it with other designs in both architecture (2d render) and training (partial gradient backpropagation).}

\begin{equation}
    \mathcal{L}_{p}=\sum \frac{1}{N^{j}}\left|p^{j}\left(\tilde{I}_{t}\right)-p^{j}\left(I_{t}\right)\right|
\end{equation}
where $p^j$ is the activation function and $N^j$ is the number of elements of the $j$-th layer in the pretrained VGG network.

% \Liqian{I suppose the norm reg is our contribution for loss part. If so, we can emphasize this reg, and formulate the full objective as $L_{full}=L + L_{reg}$. The rest loss $L=L_{r} + L_{m} + L_{p}$ can be discussed briefly if they are wildly used in related works.}





% \input{tables/ablation_study}


% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/Finetune_v1.png}
%     \vspace{-1.5em}
%     \caption{{\bf Qualitative results of our method on ZJUMocap\cite{neuralbody} and GeneBody\cite{genebody} datasets}. Fs denotes training from scratch, Ft indicates fine-tuning the model after pretraining on THuman2.0\cite{THuman2.0} dataset.}
%     \label{fig:Finetune}
% \end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/perscene_nerf_4view.png}
    \vspace{-2.5em}
    \caption{{\bf Qualitative results of novel pose synthesis on ZJUMocap\cite{neuralbody} datasets}. Fs denotes training from scratch, Ft indicates fine-tuning the model after pretraining on Multi-Garment\cite{Multi-Garment} dataset.}
    \label{fig:novel_pose_on_zju}
    \vspace{-1.0em}
\end{figure*}





{\bf Normal Regularization}. Although NeRF\cite{NeRF} can produce realistic images, the geometric surfaces generated by Marching Cubes\cite{Marchingcubes} are extremely coarse and noisy. To alleviate it, we introduce normal regularization to constrain the normal among adjacent points.
\begin{equation}
\begin{aligned}
\mathcal{L}_{n}&=\sum_{\mathbf{x}_{s} \in \mathcal{S}}\left\|\mathbf{n}\left(\mathbf{x}_{s}\right)-\mathbf{n}\left(\mathbf{x}_{s}+\epsilon\right)\right\|_{2} \\
&\mathbf{n}\left(\mathbf{x}_{s}\right)=\frac{\nabla_{\mathbf{x}_{s} }\sigma \left(\mathbf{x}_{s}\right)}{\left\|\nabla_{\mathbf{x}_{s}}\sigma \left(\mathbf{x}_{s}\right)\right\|_{2}} 
\end{aligned}
\end{equation}
where $\mathcal{S}$ is the points set randomly sampled near the SMPL mesh surface. $\mathbf{n}\left(\mathbf{x}_{s}\right)$ is the normal of the sampled point $\mathbf{x}_{s}$ and $\epsilon$ is a gaussian random noise with a variance of $0.1$.

The final loss can be summarized as
\begin{equation}
\mathcal{L}_{full}= \mathcal{L} + \lambda_{p}\mathcal{L}_{p} + 
\lambda_{n}\mathcal{L}_{n}
\end{equation}
 where $\lambda_{p}$ and $\lambda_{n}$ are the weights of the perceptual loss and the normal regularization, respectively. 





\subsection{Efficient Training}

\input{tables/persence_nerf}

\label{Efficient_Training}During training, we select $m$ multi-view images $\left\{I_{k}\right\}_{k=1}^{m}$ as inputs to build the generalizable model-based neural radiance fields and synthesize the target image $I_{t}$ with given camera pose. It is memory-consuming to synthesize the whole image at the same time by volume rendering, so we only generate an image patch of the resolution $H_{p}\times W_{p}$ sampled randomly from the whole target image, which means we only need to synthesize the half intermediate feature image of the resolution $\frac{H_{p}}{2} \times \frac{W_{p}}{2}$ by volume rendering. Meanwhile, since the perceptual loss requires a large enough image as input, we use partial gradient backpropagation introduced in CIPS-3D\cite{CIPS-3D} to further reduce the memory cost caused by volume rendering. Specifically, we randomly choose $n_{p}$ camera rays to participate in the gradient calculation, and the remaining rays $\frac{H_{p}}{2} \times \frac{W_{p}}{2} - n_{p}$ are not involved in gradient backpropagation.




