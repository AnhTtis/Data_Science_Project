\section{Experiments}
\subsection{Datasets}











We conduct experiments on two synthesized datasets Thuman2.0\cite{THuman2.0} and Multi-garment\cite{Multi-Garment} and real-world datasets Genebody\cite{genebody} and ZJUMocap\cite{neuralbody} for the generalizable scene task.
% For the generalizable scene task, we perform the experiments on Thuman2.0, Multi-garment, ZJUMocap, and Genebody. 
The Thuman2.0 dataset contains $525$ human scan meshes, of which we selected $400$ for training and the remaining $125$ for testing. For the Multi-garment dataset, we used $70$ meshes for training and $25$ meshes for evaluation. For each scanned mesh, we rendered it into $66$ multi-view images of resolution $1024\times 1024$. 
\begin{figure}[ht]
    \vspace{-0.5em}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/att_vis.png}
    \vspace{-2.0em}
    \caption{
    % {\bf The visualization of  visibility-based attention}. Red indicates high confidence, while blue represents low confidence.
    {\bf The visualization of  visibility-based attention confidence}. We visualize the contribution of different input views to the SMPL vertices. (Red indicates high confidence, while blue represents low confidence.)
    }
    \vspace{-1.0em}
    \label{fig:att_vis}
\end{figure}
Specifically, we first place each scanned mesh into the center of a unit sphere at a distance of $5.4m$, with the camera orientation always pointing towards the center of the sphere. We move the camera around the sphere, sample a yaw angle from $0^{\circ}$ to $60^{\circ}$ with an interval of $30^{\circ}$, and sample a roll angle from
$0^{\circ}$ to $360^{\circ}$ with an interval of $30^{\circ}$. 
The Genebody consists of $50$ sequences at a $48$ synchronized cameras setting, each of which has $150$ frames.
Specifically, we choose $40$ sequences for training and another $10$ sequences for testing. For ZJUMocap, which captures $10$ dynamic human sequences with $21$ synchronized cameras, we use 7 sequences
for training and the rest 3 sequences for testing. 
To compare with the case-specific methods, we conduct experiments about novel view synthesis and novel pose synthesis on ZJUMocap. Following the evaluation protocols used in NB\cite{neuralbody}, we select $4$ fixed view videos for training.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/Ablation.png}
    \vspace{-2.5em}
    \caption{{\bf Qualitative results of ablation studies on Multi-Garment dataset}.
    }
    \vspace{-1em}
    \label{fig:Alabtion_study}
\end{figure*}




% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/results/Specific_case.png}
%     \caption{{\bf Specifc Case.}}
%     \label{fig:specifc_case}
% \end{figure}










% \lipsum[1-2]
\subsection{Evaluation Metrics}

We evaluate our method with state-of-the-art generalizable or per-scene optimized methods to verify the superiority of our performance. We formulate comparative experiments on both geometric reconstruction and novel view synthesis. 
\input{tables/ablation_study}
For quantitative comparison, we adopt peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and learned perceptual image patch similarity (LPIPS\cite{LPIPS}) to evaluate the similarity between the rendered image and the ground-truth. Meanwhile, we also adopt chamfer distance (Chamfer) and point-to-surface distance (P2S) for geometric quality evaluation.



\subsection{Implementation Details}





In our experiments, we choose $m = 4$ multi-view images $\left\{I_{k}\in \mathbb{R}^{512 \times 512 \times 3}\right\}_{k=1}^{m} $ as input to synthesize the target image $I_{t}\in \mathbb{R}^{512 \times 512 \times 3}$.
% During training, the input views are selected randomly, but for testing the input views are taken from four views uniformly distributed around the person. 
During training, the input multi-view images are selected randomly, while selected uniformly surrounding the person (\ie, the front, back, left, and right views) for evaluation.
The resolution of the patch image during training is $H_{p} = W_{p} = 224$. 
The SMPL parameters are obtained using EasyMocap\cite{neuralbody}. 
The size of our 3D feature volume $\mathcal{G}$ is $224^{3}$.
For partial gradient backpropagation, we randomly sample $n_{p} = 4,096$ camera rays from the target image patch to improve memory efficiency.
We then uniformly query $N = 64$ samples from our feature volume along the camera ray.
We train our network end-to-end by using the Adam\cite{adam} optimizer, and the base learning rate
is $5\times 10^{-4}$ which decays exponentially along with the optimization. We train $200,000$ iterations on two Nvidia RTX3090 GPUs with a batch size of $4$. The loss weights $\lambda _{r} = 1$, $\lambda _{s} = 0.1$, $\lambda _{p} = 0.01$, $\lambda _{n} = 0.01$.




\subsection{Evaluation.}



\textbf{Comparison with generalizable NeRFs}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/Ablation_on_attention.png}
    \vspace{-2.5em}
    \caption{{\bf Qualitative results of different multi-view fusion mechanisms}.}
    \label{fig:Ablation_on_attention}
    \vspace{-1.0em}
\end{figure}
\input{tables/ablation_on_attention.tex}
We compare our method with state-of-the-art generalizable methods IBRNet\cite{ibrnet}, NHP\cite{NHP}, GNR\cite{genebody} and KeypointNeRF\cite{keypointNeRF}.
We retrain all aforementioned networks with the official training protocols on GeneBody\cite{genebody}, Multi-Garment\cite{Multi-Garment}, and THuman2.0\cite{THuman2.0} datasets. Specially, we also use $m=3$ views as input on ZJUMocap\cite{neuralbody} dataset following the evaluation protocol used in KeypointNeRF. The result can be seen in \cref{tab:generalization} and \cref{fig:results_on_GeNeRFs}, which shows our method generalizes to unseen identities well and outperforms the methods compared. IBRNet, which learns a general view interpolation function to synthesize the novel view from a sparse set of nearby views, is able to render high-fidelity images for views close to the input views while having very poor generalization for views far from the input views. Our method has better generalization of novel view synthesis and generates higher quality geometry due to the use of the geometry prior SMPL. KeypointNeRF utilizes sparse 3D keypoints
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/results/Geometry_v1.png}
    \vspace{-2.5em}
    \caption{{\bf Visualization results of 3D geometry reconstruction compared with different methods}.}
    \label{fig:geometry_results}
    \vspace{-0.5em}
\end{figure}
\input{tables/geometry}
as pose priors and has weak expressiveness for unseen poses when the pose diversity in the training set is insufficient. In our experiment, we choose the 3D joints of SMPL as the input of KeypointNeRF.
Compared to NHP and GNR, although we both employ SMPL as the geometry prior and suffer from inaccurate SMPL estimation, our method can alleviate the ambiguity of misalignment between geometry and pixel-aligned appearance. Meanwhile, benefiting from perceptual loss, our generated images have more photo-realistic 
 local details.
 For 3d reconstruction, the mesh surface extracted by Marching Cubes is smoother and more precise due to normal regularization compared with others as shown in \cref{fig:geometry_results} and \cref{tab:geometry}.

\textbf{Comparison with case-specific Methods}. We also compare with per-scene optimization methods
% NV\cite{NeuralVolume}, NT\cite{NT}, NHR\cite{NHR}, NB\cite{neuralbody}.
NB\cite{neuralbody}, Ani-NeRF\cite{animnerf_zju}, A-NeRF\cite{a-nerf}, ARAH\cite{arah}.
NB optimizes a set of structured latent codes associated with SMPL vertices, which are diffused into the observation space by using SparseConvNet. Since the 3D convolution in SparseConvNet is not rotation-invariant, NB has poor generalization on out-of-distribution poses. Ani-NeRF learns a backward LBS network to warp the observation space into the canonical space, which is not sufficient to model non-rigid deformations in complex poses. A-NeRF uses skeleton-relative embedding to model pose dependence deformation, which requires seeing the subjects from all views in varying poses. ARAH uses iterative root-finding for simultaneous ray-surface intersection search and correspondence search, which generalizes well to unseen poses.
As shown in Tab~\ref{tab:novel_view_pose_on_zju}, the performance of novel view synthesis is comparable with these methods, and it is reasonable since our network has more parameters($13.6$M) and struggles with overfitting when the training data is so limited without any pretraining. After pretraining on the Multi-Garment and finetuning $5,000$ steps on ZJUMocap, our results achieve a noticeable improvement.
Anyway, our method has superior generalization on novel pose synthesis, which is a more challenging task. Our results are more photorealistic and preserve more details like wrinkles and patterns as shown in Fig~\ref{fig:novel_pose_on_zju}, which benefit from the sparse multi-view input. 
 
% NT uses a traditional graphics pipeline to render the neural texture and uses 2D neural rendering to generate the target image. NHR utilizes a 2D CNN to extract features from the input 3D point clouds to render the target image. NT and NHR lack multi-view geometry consistency due to the 2D neural rendering network.
% NV utilizes an encoder-decoder architecture to learn the dynamic latent volume to produce novel content by interpolation and NB combines SMPL and NeRF together to represent the dynamic human video. NV and NB have weak expressiveness for unseen poses. Compared with these per-scene optimization methods, we can finetune the pretrained model about in 20 min and has a better generalization to novel pose as shown in \cref{tab:persence} and \cref{fig:Finetune}.
 


% \lipsum[1-2]


% \lipsum[1-2]


\subsection{Ablation studies}

The baseline ({\bf Base}) is an extended version of NB
% .  NeuralBody optimizes a set of structured latent codes from scratch on vertices of the SMPL model for each specific identity. NB uses the SparseConvNet\cite{spconv} to diffuse the structured latent codes into the nearby space to obtain the feature volume. For any spatial point, its feature is obtained by interpolating the feature volume trilinearly, and then is fed into the MLP network to obtain its density and color. NB can not generalize to unseen identities and has poor generalization for unseen poses. Therefore, we extend it
to express arbitrary identities as our baseline. Specifically, our structured latent codes are obtained by fusing multi-view input, rather than optimizing from scratch for a specific identity.
Beyond that, we introduce multi-view appearance blending ({\bf MAB}), perception loss ({\bf PL}), and neural rendering ({\bf NR}). The experimental results prove the effectiveness of each component as shown in \cref{tab:albation_study} and \cref{fig:Alabtion_study}. In addition, we explore the effects of different multi-view fusion mechanisms, and the experiments prove that our proposed visibility-based attention and geometry-guided attention are more effective than AvgPool\cite{PIFu, PixelNeRF} and IBRAtt\cite{ibrnet}.