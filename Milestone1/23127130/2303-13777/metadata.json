{
    "arxiv_id": "2303.13777",
    "paper_title": "GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images",
    "authors": [
        "Jianchuan Chen",
        "Wentao Yi",
        "Liqian Ma",
        "Xu Jia",
        "Huchuan Lu"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "In this work, we focus on synthesizing high-fidelity novel view images for arbitrary human performers, given a set of sparse multi-view images. It is a challenging task due to the large variation among articulated body poses and heavy self-occlusions. To alleviate this, we introduce an effective generalizable framework Generalizable Model-based Neural Radiance Fields (GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a geometry-guided attention mechanism to register the appearance code from multi-view 2D images to a geometry proxy which can alleviate the misalignment between inaccurate geometry prior and pixel space. On top of that, we further conduct neural rendering and partial gradient backpropagation for efficient perceptual supervision and improvement of the perceptual quality of synthesis. To evaluate our method, we conduct experiments on synthesized datasets THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The results demonstrate that our approach outperforms state-of-the-art methods in terms of novel view synthesis and geometric reconstruction.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13777v1"
    ],
    "publication_venue": "Accepted at CVPR 2023"
}