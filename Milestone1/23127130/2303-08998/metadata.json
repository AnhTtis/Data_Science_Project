{
    "arxiv_id": "2303.08998",
    "paper_title": "Unified Visual Relationship Detection with Vision and Language Models",
    "authors": [
        "Long Zhao",
        "Liangzhe Yuan",
        "Boqing Gong",
        "Yin Cui",
        "Florian Schroff",
        "Ming-Hsuan Yang",
        "Hartwig Adam",
        "Ting Liu"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "This work focuses on training a single visual relationship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning different datasets could be challenging due to inconsistent taxonomies. The issue is exacerbated in visual relationship detection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Unified Visual Relationship Detection by leveraging vision and language models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are optimized to be close to each other for semantic unification. Our bottom-up design enables the model to enjoy the benefit of training with both object detection and visual relationship datasets. Empirical results on both human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the current best bottom-up HOI detector by 60% relatively. More importantly, we show that our unified detector performs as well as dataset-specific models in mAP, and achieves further improvements when we scale up the model.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08998v1"
    ],
    "publication_venue": "Technical report (14 pages)"
}