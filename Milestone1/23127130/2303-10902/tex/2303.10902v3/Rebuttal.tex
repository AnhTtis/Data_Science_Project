\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=blue]{hyperref}
\usepackage[dvipsnames]{xcolor}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5514} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Rebuttal for: Feature Alignment and Uniformity for Test Time Adaptation}  % **** Enter the paper title here

% \maketitle
% \thispagestyle{empty}
% \appendix
% \vspace{25}

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
Thanks for the comments from all the reviewers. 
% Responses for reviewer Cd9q, nx7w, ioKd are marked in \textcolor{Maroon}{Cd9q} / \textcolor{ForestGreen}{nx7w} / \textcolor{RoyalBlue}{ioKd}.

\textbf{\textcolor{Maroon}{[Cd9q]} Q1: Results of Table 1\&2}. It is a common phenomenon in TTA. It also happens in T3A[21], and \cite{anonymous2023testtime}. As presented in Table. \ref{tab:results_t3a}, nearly all the compared methods before T3A yield worse performance than ERM does. This phenomenon is consistent with our paper. We notice that the label distribution is severely different among domains in VLCS compared to other datasets, which is probably why fewer methods show performance gain in VLCS compared to other datasets. Thus the cause of performance degradation can be summarized as follows: The label distribution shift in different domains of the dataset can significantly reflect the model's performance and thus can be seen as a justification of the model's robustness. By this, our model is efficient and robust under all the settings.
\vspace{-1em}
\begin{table}[!h]
  \centering
  \caption{Average results(\textbf{Including TerraIncognita dataset!}) reported in T3A[21]. Results worse than ERM are \textcolor{red}{highlighted}.}
  \vspace{-0.5em}
  \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccc}
    \toprule
          & ERM   & SHOT-IM & PL    & Tent  & BN    & T3A \\
    \hline
    Resnet18 & 64.0  & \textcolor{red}{59.8}  & \textcolor{red}{57.9} & \textcolor{red}{64.0}   & \textcolor{red}{62.9}  & 64.6 \\
    Resnet50 & 67.8  & \textcolor{red}{62.0}  & \textcolor{red}{60.4}  & 68.0   & \textcolor{red}{66.5}  & 68.5 \\
    \bottomrule
    \end{tabular}
   }
  \label{tab:results_t3a}
  \vspace{-1.25em}
\end{table}%
% For the implementation of compared methods, we strictly follows the settings in T3A[21] and \cite{anonymous2023testtime}. we use the same public code [21,43,5] of all the corresponding methods to reproduce the results.
% (2)
% (3) The reasons can be concluded as follows: OfficeHome includes more classes (OfficeHome: 65 classes; PACS: 7 classes), and domains in VLCS have more severe domain shifts, which make them more challenging than PACS for the TTA task.

\textbf{\textcolor{Maroon}{[Cd9q]} Q2: Compared with domain generalization methods}. 
We highlight that our method outperforms \textbf{all} the existing DG methods (\eg, PCL, DNA in Table 3\&4). To be specific, our method outperforms PCL (PCL is also based on SWAD) \textbf{2.3\%} on PACS and \textbf{3.9\%} on DomainNet.

\textbf{\textcolor{Maroon}{[Cd9q]} Q3: Update strategies for the memory bank}. Two strategies are proposed in our paper: (1) we adopted two filters, \ie, entropy filter (line 310) and consistency filter (line 357), to stop the gradient backpropagation of unreliable samples in the memory bank. Ablation studies are presented in Table 5. (2) Furthermore, we propose memorized spatial local clustering (line 374) to align $K$ nearest features instead of all the features for feature alignment, which mitigates the impact of low-quality features and logits. Ablation studies are presented in Figure. 3.

\textbf{\textcolor{Maroon}{[Cd9q]} Q4: Details about segmentation task}. For the segmentation task, we regard it as a pixel-wise classification task. We store the features generated by the backbone and the logits of \textit{every pixel} rather than \textit{every image} in the classification task. Thus, the retrieve of \textit{image-wise} information becomes to retrieve the K-nearest \textit{pixel-wise} information in the memory bank. So it is with the entropy \& consistency filter. Other implementation details are the same as the classification task except that we enlarge $K$ from 3 to 64. We will add the missing information in the revision.

\textbf{\textcolor{ForestGreen}{[nx7w]} Q1: Paper organization and Figure 1}. Thanks for your comments. We will polish the writing and improve the clarity of Figure 1 in the revised version.

\textbf{\textcolor{ForestGreen}{[nx7w]} Q2: Experiment results on CIFAR-C and ImageNet-C}. We evaluated our methods on CIFAR-10/100-C and ImageNet-C. The results are listed in Table \ref{tab:exp_cifar_imagenet}. It is noticed that our method still outperforms existing TTA methods on CIFAR-10/100-C and achieves competitive results on ImageNet-C (rank2, 0.1\% less than ETA). Experiments show that our method is efficient and robust under most settings. We can not provide results currently due to the limited time for the Kitty object detection task, but we will add it in the revision.

\textbf{\textcolor{RoyalBlue}{[ioKd]} Q1: About uniformity}. Keeping a memory bank is \textit{not} the reason to maintain the uniformity. Different from the \textit{label-free} contrastive learning, which has no target distribution to simulate uniformity, the TTA task has access to the trained model. Thus we can have a target distribution (though not very accurate) to simulate and adjust the uniformity step by step. As mentioned in (Lines 133-139), the prediction distributions of the linear classifier should be as uniform as the prediction distributions of prototype-based classification (can be seen as an ideal uniformed distribution of all the previous data). As for the impact, our TSD strategy guarantees uniformity, and the MSLC strategy guarantees alignment. We show the ablation studies in Table 5. The performance will degrade if we remove one of them.

\textbf{\textcolor{RoyalBlue}{[ioKd]} Q2: Differences with AdaContrast (AdaC)}. Our method is fundamentally different from AdaC. 
(1)AdaC utilize the logits of $k$-nearest neighbours to soften the pseudo-labels while we use \textit{both} features and logits to encourage features from the same class to be closed or features from different classes to be far away from each other. (2) As presented in Eq. \ref{eq:entropy}, the use of entropy minimization in AdaC aims to encourage class diversification in the batch ($N$ is the batch size) while we aim to filter the high-entropy noisy labels. (3) AdaC simply adopts a MoCo strategy to boost the performance without any analysis while we decouple and explain every part of our model for the view of uniformity and alignment. Our paper provides a new perspective on TTA and makes TTA strategies more explanatory. (4) Moreover, we compared our method with AdaC. Results are presented in Table. \ref{tab:exp_cifar_imagenet}. Our simple and efficient model outperforms AdaC in all three datasets.
\vspace{-1em}
\begin{equation}
    \mathcal{L}_{div} = \sum_{c=1}^{C}\hat{p}\log \hat{p}, \hat{p} = \frac1N \sum_{i=1}^{N}p,p\in R^{N\times C}
    \label{eq:entropy}
    \vspace{-2.5em}
\end{equation}

\begin{table}[t]
  \centering
  \caption{Accuracy on CIFAR-10/100-C and ImageNet-C. We use ResNet26 for CIFAR-10/100-C, and ResNet50 for ImageNet-C.}
  \vspace{-0.5em}
    \begin{tabular}{lccc}
    \toprule
          & CIFAR-10-C & CIFAR-100-C & ImageNet-C \\
    \midrule
    ERM [61] & 70.7 & 41.4 & 18.0 \\
    T3A [21] & 77.4 & 44.6 & 36.5 \\
    %SHOT-IM [34] & 80.7 & 52.1  & 43.1 \\
    LAME [5] & 79.4  & 50.6  & 47.6 \\
    ETA [43] & 80.6  & \underline{52.4}  & \textbf{48.1} \\
    AdaC [8] & \underline{81.1}  & 51.8 & 47.6 \\
    \hline
    Ours  & \textbf{81.7}  & \textbf{52.6}  & \underline{48.0}\\
    \bottomrule
    \end{tabular}%
  \label{tab:exp_cifar_imagenet}%
  \vspace{-2em}
\end{table}%
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{rebuttal_ref}
}
\end{document}
