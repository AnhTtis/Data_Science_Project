@article{Federmeier1999ARB,
  title={A Rose by Any Other Name: Long-Term Memory Structure and Sentence Processing},
  author={Kara D. Federmeier and Marta Kutas},
  journal={Journal of Memory and Language},
  year={1999},
  volume={41},
  pages={469-495}
}

@article{Battig1969CategoryNO,
  title={Category norms of verbal items in 56 categories A replication and extension of the Connecticut category norms},
  author={William F. Battig and William Edward Montague},
  journal={Journal of Experimental Psychology},
  year={1969},
  volume={80},
  pages={1-46}
}

@inproceedings{belinkov-etal-2020-interpretability,
    title = "Interpretability and Analysis in Neural {NLP}",
    author = "Belinkov, Yonatan  and
      Gehrmann, Sebastian  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-tutorials.1",
    doi = "10.18653/v1/2020.acl-tutorials.1",
    pages = "1--5",
    abstract = "While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis of modern neural networks can be difficult for newcomers to the field. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, such as structural analyses using probing classifiers, behavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches, in order to inform participants where to focus future efforts.",
}



@article{Srivastava2022BeyondTI,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}

@article{Sanh2021MultitaskPT,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel et. al},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.08207}
}

@article{Zhang2022OPTOP_OPT,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Susan Zhang and Stephen Roller and Naman Goyal et. al},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01068}
}

@article{Chowdhery2022PaLMSL_PALM,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin et. al},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311}
}


@inproceedings{Lialin2022LifeAB,
    title = "Life after {BERT}: What do Other Muppets Understand about Language?",
    author = "Lialin, Vladislav  and
      Zhao, Kevin  and
      Shivagunde, Namrata  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.227",
    doi = "10.18653/v1/2022.acl-long.227",
    pages = "3180--3193",
    abstract = "Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model{'}s linguistic capabilities.",
}

@article{chow2016AM,
  title={A “bag-of-arguments” mechanism for initial verb predictions},
  author={Wing-Yee Chow and Cybelle Smith and Ellen F. Lau and Colin Phillips},
  journal={Language, Cognition and Neuroscience},
  year={2016},
  volume={31},
  pages={577 - 596}
}


@article{talmor2019olmpics,
  title={oLMpics-On What Language Model Pre-training Captures},
  author={Alon Talmor and Yanai Elazar and Yoav Goldberg and Jonathan Berant},
  journal={Transactions of the Association for Computational Linguistics},
  year={2019},
  volume={8},
  pages={743-758}
}

@inproceedings{kobayashi2020attention,
  title={Attention is not only a weight: Analyzing transformers with vector norms},
  author={Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7057--7075},
  year={2020}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{brown2020language_gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, et. al},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{michel2019sixteen,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{prasanna2020bert,
  title={When BERT Plays the Lottery, All Tickets Are Winning},
  author={Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={3208--3229},
  year={2020}
}

@inproceedings{liu2019linguistic,
  title={Linguistic Knowledge and Transferability of Contextual Representations},
  author={Liu, Nelson F and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E and Smith, Noah A},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1073--1094},
  year={2019}
}

@inproceedings{tenney2019bert_rediscovers,
  title={BERT Rediscovers the Classical NLP Pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}

@inproceedings{clark2019does,
  title={What Does {B}{E}{R}{T} Look at? An Analysis of {B}{E}{R}{T}’s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019}
}


@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{ding2017visualizing,
  title={Visualizing and understanding neural machine translation},
  author={Ding, Yanzhuo and Liu, Yang and Luan, Huanbo and Sun, Maosong},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1150--1159},
  year={2017}
}

@article{lin2019open,
  title={Open sesame: Getting inside bert's linguistic knowledge},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  journal={arXiv preprint arXiv:1906.01698},
  year={2019}
}

@article{vig2019analyzing,
  title={Analyzing the structure of attention in a transformer language model},
  author={Vig, Jesse and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:1906.04284},
  year={2019}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={arXiv preprint arXiv:1905.03197},
  year={2019}
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{wallace2019nlp,
  title={Do nlp models know numbers? probing numeracy in embeddings},
  author={Wallace, Eric and Wang, Yizhong and Li, Sujian and Singh, Sameer and Gardner, Matt},
  journal={arXiv preprint arXiv:1909.07940},
  year={2019}
}

@article{durrani2021transfer,
  title={How transfer learning impacts linguistic knowledge in deep NLP models?},
  author={Durrani, Nadir and Sajjad, Hassan and Dalvi, Fahim},
  journal={arXiv preprint arXiv:2105.15179},
  year={2021}
}

article{tenney2019you,
  title={What do you learn from context? probing for sentence structure in contextualized word representations},
  author={Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and Van Durme, Benjamin and Bowman, Samuel R and Das, Dipanjan and others},
  journal={arXiv preprint arXiv:1905.06316},
  year={2019}
}

@article{mccoy2019right,
  title={Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  journal={arXiv preprint arXiv:1902.01007},
  year={2019}
}

@inproceedings{ross2019well,
  title={How well do NLI models capture verb veridicality?},
  author={Ross, Alexis and Pavlick, Ellie},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2230--2240},
  year={2019}
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@article{jain2019attention,
  title={Attention is not explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  journal={arXiv preprint arXiv:1902.10186},
  year={2019}
}

@inproceedings{serrano2019attention,
  title={Is Attention Interpretable?},
  author={Serrano, Sofia and Smith, Noah A},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2931--2951},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{kovaleva2021busters,
    title = "{BERT} Busters: Outlier Dimensions that Disrupt Transformers",
    author = "Kovaleva, Olga  and
      Kulshreshtha, Saurabh  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.300",
    doi = "10.18653/v1/2021.findings-acl.300",
    pages = "3392--3405",
}



@inproceedings{kovaleva2019revealing,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga  and
      Romanov, Alexey  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1445",
    doi = "10.18653/v1/D19-1445",
    pages = "4365--4374",
    abstract = "BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT{'}s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.",
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1905.00537},
  year={2019}
}

@article{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  pages={1693--1701},
  year={2015}
}

@misc{schick2020fewshot,
    title={Few-Shot Text Generation with Pattern-Exploiting Training},
    author={Timo Schick and Hinrich Schütze},
    year={2020},
    eprint={2012.11926},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{kaplan2020scaling,
    title={Scaling Laws for Neural Language Models},
    author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
    year={2020},
    eprint={2001.08361},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{ettinger2020bert,
  title={What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models},
  author={Allyson Ettinger},
  journal={Transactions of the Association for Computational Linguistics},
  year={2019},
  volume={8},
  pages={34-48}
}

@article{Soler2020BertKnows,
   title={BERT Knows Punta Cana is not just beautiful, it’s gorgeous: Ranking Scalar Adjectives with Contextualised Representations},
   url={http://dx.doi.org/10.18653/v1/2020.emnlp-main.598},
   DOI={10.18653/v1/2020.emnlp-main.598},
   journal={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
   publisher={Association for Computational Linguistics},
   author={Garí Soler, Aina and Apidianaki, Marianna},
   year={2020}
}

@inproceedings{ravichander2020systematicity,
    title = "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}",
    author = "Ravichander, Abhilasha  and
      Hovy, Eduard  and
      Suleman, Kaheer  and
      Trischler, Adam  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.starsem-1.10",
    pages = "88--102",
}

@inproceedings{Zagoury2021WhatsTB,
  title={What's the best place for an AI conference, Vancouver or \_\_\_\_\_\_: Why completing comparative questions is difficult},
  author={Avishai Zagoury and Einat Minkov and Idan Szpektor and William W. Cohen},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{Kassner2020ArePL,
  title={Are Pretrained Language Models Symbolic Reasoners over Knowledge?},
  author={Nora Kassner and Benno Krojer and Hinrich Sch{\"u}tze},
  booktitle={CONLL},
  year={2020}
}

@inproceedings{Mohebbi2021ExploringTR,
  title={Exploring the Role of BERT Token Representations to Explain Sentence Probing Results},
  author={Hosein Mohebbi and Ali Modarressi and Mohammad Taher Pilehvar},
  booktitle={EMNLP},
  year={2021}
}

@article{Clark2020TransformersAS,
  title={Transformers as Soft Reasoners over Language},
  author={Peter E. Clark and Oyvind Tafjord and Kyle Richardson},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05867}
}


@inproceedings{Liu2021ProbingAT,
    title = "Probing Across Time: What Does {R}o{BERT}a Know and When?",
    author = "Liu, Zeyu  and
      Wang, Yizhong  and
      Kasai, Jungo  and
      Hajishirzi, Hannaneh  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.71",
    doi = "10.18653/v1/2021.findings-emnlp.71",
    pages = "820--842",
    abstract = "Models of language trained on very large corpora have been demonstrated useful for natural language processing. As fixed artifacts, they have become the object of intense study, with many researchers {``}probing{''} the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Recent work applied several probes to intermediate training stages to observe the developmental process of a large-scale model (Chiang et al., 2020). Following this effort, we systematically answer a question: for various types of knowledge a language model learns, when during (pre)training are they acquired? Using RoBERTa as a case study, we find: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.",
}

@inproceedings{Zhou2021RICAER,
  title={RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms},
  author={Pei Zhou and Rahul Khanna and Bill Yuchen Lin and Daniel Ho and Jay Pujara and Xiang Ren},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{Ilharco2021ProbingCL,
  title={Probing Contextual Language Models for Common Ground with Visual Representations},
  author={Gabriel Ilharco and Rowan Zellers and Ali Farhadi and Hannaneh Hajishirzi},
  booktitle={NAACL},
  year={2021}
}

@article{Rajaee2021HowDF,
  title={How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy},
  author={S. Rajaee and Mohammad Taher Pilehvar},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.04740}
}

@inproceedings{Mosbach2020OnTI,
  title={On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers},
  author={Marius Mosbach and Anna Khokhlova and Michael A. Hedderich and Dietrich Klakow},
  booktitle={FINDINGS},
  year={2020}
}

@article{Phang2021FineTunedTS,
  title={Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers},
  author={Jason Phang and Haokun Liu and Samuel R. Bowman},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.08406}
}

@article{Jiang2021HowCW,
  title={How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering},
  author={Zhengbao Jiang and J. Araki and Haibo Ding and Graham Neubig},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={9},
  pages={962-977}
}


@misc{Gokaslan2019OpenWeb,
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@misc{trinh2018simple,
    title={A Simple Method for Commonsense Reasoning},
    author={Trieu H. Trinh and Quoc V. Le},
    year={2018},
    eprint={1806.02847},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}


@inproceedings{gptneo,
  title={GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author={Sid Black and Leo Gao and Phil Wang and Connor Leahy and Stella Rose Biderman},
  year={2021}
}



@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{shazeer2020glu,
    title={GLU Variants Improve Transformer},
    author={Noam Shazeer},
    year={2020},
    eprint={2002.05202},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Hendrycks2016BridgingNA,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@inproceedings{kobayashi2021IncorporatingRA,
  title={Incorporating Residual and Normalization Layers into Analysis of Masked Language Models},
  author={Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentarou Inui},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{luo2021positional,
  title={Positional artefacts propagate through masked language model embeddings},
  author={Luo, Ziyang and Kulmizev, Artur and Mao, Xiaoxi},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5312--5327},
  year={2021}
}

@article{brunner2020OnIdentifiability,
  title={On Identifiability in Transformers},
  author={Gino Brunner and Yang Liu and Damian Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},
  journal={arXiv: Computation and Language},
  year={2020}
}

@inproceedings{Howard2018UniversalLM,
  title={Universal Language Model Fine-tuning for Text Classification},
  author={Jeremy Howard and Sebastian Ruder},
  booktitle={ACL},
  year={2018}
}

@inproceedings{Peters2018DeepCW,
  title={Deep Contextualized Word Representations},
  author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle={NAACL},
  year={2018}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@inproceedings{mccoy-etal-2019-right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, Tom  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
}

@article{Goldberg2019AssessingBS,
  title={Assessing BERT's Syntactic Abilities},
  author={Yoav Goldberg},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.05287}
}

@inproceedings{Conneau2018WhatYC,
  title={What you can cram into a single \$\&!\#* vector: Probing sentence embeddings for linguistic properties},
  author={Alexis Conneau and Germ{\'a}n Kruszewski and Guillaume Lample and Lo{\"i}c Barrault and Marco Baroni},
  booktitle={ACL},
  year={2018}
}

@article{federmeier1999rose,
  title={A rose by any other name: Long-term memory structure and sentence processing},
  author={Federmeier, Kara D and Kutas, Marta},
  journal={Journal of memory and Language},
  volume={41},
  number={4},
  pages={469--495},
  year={1999},
  publisher={Elsevier}
}

@article{chow2016bag,
  title={A “bag-of-arguments” mechanism for initial verb predictions},
  author={Chow, Wing-Yee and Smith, Cybelle and Lau, Ellen and Phillips, Colin},
  journal={Language, Cognition and Neuroscience},
  volume={31},
  number={5},
  pages={577--596},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{fischler1983brain,
  title={Brain potentials related to stages of sentence verification},
  author={Fischler, Ira and Bloom, Paul A and Childers, Donald G and Roucos, Salim E and Perry Jr, Nathan W},
  journal={Psychophysiology},
  volume={20},
  number={4},
  pages={400--409},
  year={1983},
  publisher={Wiley Online Library}
}


@inproceedings{card-etal-2020-little_power,
    title = "With Little Power Comes Great Responsibility",
    author = "Card, Dallas  and
      Henderson, Peter  and
      Khandelwal, Urvashi  and
      Jia, Robin  and
      Mahowald, Kyle  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.745",
    doi = "10.18653/v1/2020.emnlp-main.745",
    pages = "9263--9274",
}


@article{aroger2020,
  title={A Primer in BERTology: What We Know About How BERT Works},
  author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
  volume={8},
  pages={842-866}
}

@article{zhao2021,
  doi = {10.48550/ARXIV.2102.09690},
  url = {https://arxiv.org/abs/2102.09690},
  author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Whitfield2021,
  doi = {10.48550/ARXIV.2104.10658},
  url = {https://arxiv.org/abs/2104.10658},
  author = {Whitfield, Dewayne},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Using GPT-2 to Create Synthetic Data to Improve the Prediction Performance of NLP Machine Learning Classification Models},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{datagenSchick2010,
  doi = {10.48550/ARXIV.2104.07540},
  url = {https://arxiv.org/abs/2104.07540},
  author = {Schick, Timo and Schütze, Hinrich},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generating Datasets with Pretrained Language Models},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{AnabyTavor2019NotED,
  title={Not Enough Data? Deep Learning to the Rescue!},
  author={Ateret Anaby-Tavor and Boaz Carmeli and Esther Goldbraich and Amir Kantor and George Kour and Segev Shlomov and N. Tepper and Naama Zwerdling},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.03118}
}

@article{Smith2022,
  doi = {10.48550/ARXIV.2205.02318},
  url = {https://arxiv.org/abs/2205.02318},
  author = {Smith, Ryan and Fries, Jason A. and Hancock, Braden and Bach, Stephen H.},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Language Models in the Loop: Incorporating Prompting into Weak Supervision},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Gao2022gpt2,
  doi = {10.48550/ARXIV.2205.12679},
  url = {https://arxiv.org/abs/2205.12679},
  author = {Gao, Jiahui and Pi, Renjie and Lin, Yong and Xu, Hang and Ye, Jiacheng and Wu, Zhiyong and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ZeroGen$^+$: Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Bonifacio2022gpt3,
  doi = {10.48550/ARXIV.2202.05144},
  url = {https://arxiv.org/abs/2202.05144},
  author = {Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {InPars: Data Augmentation for Information Retrieval using Large Language Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Han2021gpt3translation,
  title={Unsupervised Neural Machine Translation with Generative Language Models Only},
  author={Jesse Michael Han and Igor Babuschkin and Harrison Edwards and Arvind Neelakantan and Tao Xu and Stanislas Polu and Alex Ray and Pranav Shyam and Aditya Ramesh and Alec Radford and Ilya Sutskever},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.05448}
}

@misc{Liu2022gpt3nli,
  doi = {10.48550/ARXIV.2201.05955},
  url = {https://arxiv.org/abs/2201.05955},
  author = {Liu, Alisa and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Anaby2020,
author = {Anaby-Tavor, Ateret and Carmeli, Boaz and Goldbraich, Esther and Kantor, Amir and Kour, George and Shlomov, Segev and Tepper, Naama and Zwerdling, Naama},
year = {2020},
month = {04},
pages = {7383-7390},
title = {Do Not Have Enough Data? Deep Learning to the Rescue!},
volume = {34},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6233}
}

@article{kumar2020,
  author    = {Varun Kumar and
               Ashutosh Choudhary and
               Eunah Cho},
  title     = {Data Augmentation using Pre-trained Transformer Models},
  journal   = {CoRR},
  volume    = {abs/2003.02245},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.02245},
  eprinttype = {arXiv},
  eprint    = {2003.02245},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-02245.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Meng2020,
  title={Generating Training Data with Language Models: Towards Zero-Shot Language Understanding},
  author={Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{Biswesh2020,
  author    = {Biswesh Mohapatra and
               Gaurav Pandey and
               Danish Contractor and
               Sachindra Joshi},
  title     = {Simulated Chats for Task-oriented Dialog: Learning to Generate Conversations
               from Instructions},
  journal   = {CoRR},
  volume    = {abs/2010.10216},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.10216},
  eprinttype = {arXiv},
  eprint    = {2010.10216},
  timestamp = {Mon, 26 Oct 2020 15:39:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-10216.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Papanikolaou2020,
  author       = {Yannis Papanikolaou and
                  Andrea Pierleoni},
  title        = {{DARE:} Data Augmented Relation Extraction with {GPT-2}},
  journal      = {CoRR},
  volume       = {abs/2004.13845},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.13845},
  eprinttype    = {arXiv},
  eprint       = {2004.13845},
  timestamp    = {Sat, 02 May 2020 19:17:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-13845.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Yiben2020,
  author    = {Yiben Yang and
               Chaitanya Malaviya and
               Jared Fernandez and
               Swabha Swayamdipta and
               Ronan Le Bras and
               Ji{-}Ping Wang and
               Chandra Bhagavatula and
               Yejin Choi and
               Doug Downey},
  title     = {{G-DAUG:} Generative Data Augmentation for Commonsense Reasoning},
  journal   = {CoRR},
  volume    = {abs/2004.11546},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.11546},
  eprinttype = {arXiv},
  eprint    = {2004.11546},
  timestamp = {Sat, 23 Jan 2021 01:17:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-11546.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Yian2020ett,
  author    = {Yian Zhang and
               Alex Warstadt and
               Haau{-}Sing Li and
               Samuel R. Bowman},
  title     = {When Do You Need Billions of Words of Pretraining Data?},
  journal   = {CoRR},
  volume    = {abs/2011.04946},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.04946},
  eprinttype = {arXiv},
  eprint    = {2011.04946},
  timestamp = {Thu, 12 Nov 2020 15:14:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-04946.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{li2022systematicity,
      title={Systematicity in GPT-3's Interpretation of Novel English Noun Compounds}, 
      author={Siyan Li and Riley Carlson and Christopher Potts},
      year={2022},
      eprint={2210.09492},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wu2022generating,
      title={Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets}, 
      author={Yuxiang Wu and Matt Gardner and Pontus Stenetorp and Pradeep Dasigi},
      year={2022},
      eprint={2203.12942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{west2022symbolic,
      title={Symbolic Knowledge Distillation: from General Language Models to Commonsense Models}, 
      author={Peter West and Chandra Bhagavatula and Jack Hessel and Jena D. Hwang and Liwei Jiang and Ronan Le Bras and Ximing Lu and Sean Welleck and Yejin Choi},
      year={2022},
      eprint={2110.07178},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wanli2023,
    title = "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
    author = "Liu, Alisa  and
      Swayamdipta, Swabha  and
      Smith, Noah A.  and
      Choi, Yejin",
    month = jan,
    year = "2022",
    url = "https://arxiv.org/pdf/2201.05955",
}
