\section{EXPERIMENTAL SETUP}\label{sec:exp_setup}

\looseness=-1
In this section, we describe the tasks, environment, and data collection process used for training the policy~\policy\ and optimization of the robot morphology. We also describe the training process of the targeted policy, which serves as a true performance evaluator for a given robot morphology.

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{figs/task_motion_strip.png}
  \vspace{-8mm}
  \caption{\textbf{Manipulation Task} - Robot motion key-frame visualization for object grasping, object placing, and cabinet closing sub-tasks.}
  \label{fig:task_motion_strip}
  \vspace{-5mm}
\end{figure}

\subsection{Tasks}
\looseness=-1
We use two tasks for training the policy: reaching and manipulation. The reaching task involves moving the end-effector to a specific goal position, while the manipulation task involves picking an object, placing it in a cabinet, and closing the cabinet door (Fig. \ref{fig:task_motion_strip}).

\subsubsectioncol{Reaching task}
\looseness=-1
The reaching task involves moving the robot's end-effector to a specific goal position within the workspace.
The goal position is randomly sampled at the beginning of each trial, and the robot's joint positions are also randomly placed in a different initial configuration.
The robot's end-effector is required to finish in a fixed orientation used across all trials.
The task is considered successful if the end-effector is within 3cm and 10 degrees from the goal pose for more than 20 time steps.
This task is used to evaluate the robot's ability to reach specific points within the workspace and helps test the onboard policies robot's arm/end-effector pose inference capabilities.

\subsubsectioncol{Manipulation task}
\looseness=-1
The manipulation task is a harder task, which is broken down into three sub-tasks: picking, placing, and closing.
The goal of this task is to evaluate the robot's ability to perform more complex manipulation actions, such as picking, manipulating objects, and interacting with the environment.
Unlike reaching, this task additionally requires that the network infers the object position and the cabinet states.

\subsubsubsection{Picking.}
\looseness=-1
For this task, the robot is required to pick up an object off the table surface.
The position of the object is randomized at the beginning of each trial, and the robot arm is reset into a randomized location in near proximity to the object.
The task is considered successful if the object is raised 20 cm above the table for more than 20 time steps.

\subsubsubsection{Placing.}
\looseness=-1
The robot must place the object in one of two cabinets. One of the cabinets is randomly selected as the goal at the beginning of each trial with its door being set as open.
The robot arm is initialized in the object-holding pose, with the goal to bring and release that object inside the target cabinet.


\begin{figure*}[!ht]
  \centering
  \begin{minipage}[b]{0.32\linewidth}
    \centering
    \includegraphics[height=6.5cm]{figs/optim_vs_human.png}
    \caption{\textbf{Targeted vs. Morphology-Agnostic Policies.}
    Policies \policyR\ and \targetedpolicy\ have a $14.44\%$ performance gap on the \textit{human-expert} design, which shrinks to $1\%$ on the \ourmorph\ design.
    We omit the targeted performance evaluation on random morphologies due to computational cost.
    }
    \label{fig:optimized_vs_human}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.35\linewidth}
    \centering
    \includegraphics[height=6.5cm]{figs/better_learner.png}
    \caption{\textbf{Robot Performance \& Learning Efficiency.}
    \textbf{(a)} \textit{Learning oriented} morphology design outperforms the \textit{human-expert} design by $18.95\%$ when trained with the same amount of data.
    \textbf{(b)} \textit{Learning oriented} design can achieve the same performance as \textit{human-expert} design using $25$x less data.
    }
    \label{fig:better_learner}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.29\linewidth}
    \centering
    \includegraphics[height=6.5cm]{figs/best_priv_vs_best_real_axis_flipped.png}
    \caption{\textbf{Onboard and privileged design objective comparison.}  
    Morphology optimized using privileged objective has a major performance drop when evaluated using onboard information, emphasizing the importance of evaluation with onboard sensing.
    }
    \label{fig:best_priv_vs_best_real}
  \end{minipage}
  \label{fig:figures}
\end{figure*}

\subsubsubsection{Closing.}
\looseness=-1
The robot must close the door of the cabinet that the object was placed in.
The goal is to retract the arm from the post object placing pose and fully close the door of the cabinet.
The task is considered successful if the door is closed with a threshold of five degrees.

\subsection{Robot Data Collection}\label{subsec:data_collection}
\looseness=-1
Next, we describe the robot control and data collection procedures used for training the policy \policy\ and optimizing the robot morphology.
We collect trajectories using motion planning, which uses ground-truth simulation information, for various robot designs.
During training (Section \ref{sec:universal_pi}), \policyR\ is trained to produce these motions using only onboard observations.

\subsubsectioncol{Robot Control}
\looseness=-1
For our tasks, we use a robot manipulator with a seven degrees of freedom arm and a fixed base.
The robot arm is actuated through end-effector displacement control and Inverse Kinematics (IK) with collision avoidance.
During data collection, we use a set of predefined end-effector poses, which, IK guides the robot arm through avoiding any collisions.

\subsubsectioncol{Data Collection}
\looseness=-1
The data collection process involves initializing a morphology with uniformly sampled link lengths and collecting synthetic trajectories using the steps outlined in Algorithm \ref{alg:collection}.
\input{46_collection_algo}
\vspace{-3mm}
All of the sub-tasks are timed, and the trajectory lengths are capped at 200 time steps ($\sim$10 seconds).
The data is collected using a planner which follows through a designated set of end-effector poses.
We use PyBullet~\cite{coumans2021} simulator to collect the demonstration data.
The data collection process is conducted in parallel by hundreds of workers to efficiently collect a large amount of data.
In total, we collect $N=500$k successful trajectories for training the policy \policy.

\subsection{Targeted Policy}\label{subsec:targeted}
A targeted policy \targetedpolicy\ is a controller designed to control a specific robot, unlike a \shortname~policy \policy, which is designed to perform well across multiple robots.
We use targeted policy as an ablation to evaluate how much the performance of our proposed \shortname~policy \policy is compromised to accommodate for multi-morphology capability.

To evaluate the true performance without multi-morphology compromise, we train the \textit{targeted} policy~\targetedpolicy\ using the procedure described in Section \ref{sec:universal_pi} but with a fixed robot design.
For a particular robot, we collect $100$k successful targeted demonstrations while keeping morphology~$m_i$ intact (skip Algorithm \ref{alg:collection}:\textit{line} 3) during the data collection process.
We use targeted demonstration to train the targeted policy~$\pi^T$ from scratch until convergence.