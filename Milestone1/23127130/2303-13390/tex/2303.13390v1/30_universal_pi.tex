\section{\cleanfullpolicyname}\label{sec:universal_pi}

\subsection{Overview}
We present a \fullpolicyname (\shortname) capable of operating across a variety of environments, tasks, and robot morphologies.
The controller is a neural network policy \policy\ trained to control a wide range of robot morphologies using the robot's onboard camera sensing.
To achieve this challenging goal, we introduce a \onestagehalfname framework (detailed in Section \ref{subsec:training}) to bootstrap the behavioral learning from demonstrations.
The data is collected across a spectrum of robot designs (detailed in Section \ref{sec:exp_setup}) to make the policy compatible with morphological variations.
The \policy\ training pipeline and policy architecture are summarized in Fig. \ref{fig:architecture}.
The \policy\ will be later used as a surrogate function to rapidly assess the quality of different morphologies in Section \ref{sec:hw_optim}.
Below, we introduce the policy structure and training protocols used for obtaining such a policy.

\subsection{States and Observation}
We train our policy using Behavior Cloning, which has been proven effective for a variety of manipulation tasks~\cite{jang2022bc, khansari2022practical, rt12022arxiv}. In this paper, we use the following notation to describe the states and observations: \stateblue{\textit{onboard}} observations~${o_t}$, \statered{\textit{privileged}} states ${s_t}$, \stateyellow{\textit{morphology}} configuration ${m_i}$.
\stateblue{\textit{Onboard}} observations consist of sensor readings which are available during robot deployment. \statered{\textit{Privileged}} states provide a comprehensive representation that is only used to bootstrap the learning efficiency of the policy. These states do not suffer from sensor limitations or occlusions, notably, they are unobtainable during the policy deployment. \stateyellow{\textit{Morphology}} configuration is used to inform the policy about the robot design parameters.

States and observations used in our tasks of robot manipulation are the following:
\begin{itemize}
    \item \stateblue{$I \in R^{[150\times150\times3]}$} (onboard) - image observations rendered from the robot's onboard camera.
    \item \statered{$J \in R^{8}$} (privileged) - positions of the robot's arm joints.
    \item \statered{$E^{curr/goal} \in R^{11}$} (privileged)  - current and goal states of the robot's end-effector, including Cartesian coordinates, quaternion orientation, and finger positions.
    \item \statered{$B \in R^{7}$} (privileged) - position and orientation of the object being manipulated during the task.
    \item \statered{$F \in R^{2}$}  (privileged) - an indicator of contact between the fingers and the object, and the distance between them.
    \item \statered{$W \in R^{2}$}  (privileged) - openness state of the cabinet, used to track the progress of the closing task.
    \item \stateyellow{$m \in R^{7}$} (morphology)- configuration parameters that define the robot's physical structure and configuration.
\end{itemize}
These observations and states provide a comprehensive understanding of the robot's environment and actions, which are used to train the \policy.

The policy output controls the displacement of robot end-effector position, orientation, and two-fingers.

\subsection{Privileged Single-stage Learning via Latent Alignment}\label{subsec:training}
In this section, we describe the \onestagefullnamecased (\onestageshortname) used for training of~\policy.
Privileged learning has been shown to be effective for a number of applications~\cite{chen2020learning, sorokin2022learning, miki2022learning, agarwallegged}.
Generally in this paradigm, two policies are trained sequentially, a student policy with limited observations, and a privileged policy with full observations.
The privileged policy's sole role is to guide the student during the transfer process, leading to improved learning efficiency and improved behavior due to a stronger learning signal.

We propose a novel training procedure, which unifies the traditional two-stage approach into one stage by using a latent space alignment loss during the optimization process.
This unification allows us to discourage the student policy from learning \textit{supernatural} behaviors which incorrectly exploit the information only present in privileged states, which is important to learn realistic vision-based policies.
To make this approach possible, we use a combination of loss functions to train the policy network.
We use Behavioral Cloning (BC) for action and Latent Alignment loss for the alignment of information in the encoder latent space.
We utilize this framework for training \policy\ that is agnostic of the robot morphology in both observation and control space.

\vspace{1mm}
\subsubsectioncol{Network Architecture} Our network architecture consists of four main components: image encoder \stateblue{$R$}, a privileged encoder \statered{$P$}, morphology encoder \stateyellow{$M$}, and the policy head~\stategreen{$\pi$}.
Encoders produce unit-vector embeddings
$\colorzR$,
$\colorzP$,
$\colorzM$
processing the respective inputs:
$\colorobs$\footnote[1]{For reaching $o_t$ consists on $E^{goal}$ in addition to $I$},
$\colorstate$, and
$\colormorph$.
%
Onboard policy action $\coloractR$ and privileged policy action $\coloractR$ are produced by querying policy head with corresponding inputs:
$\stategreen{\pi}(\colorzR,\colorzM)$ and
$\stategreen{\pi}(\colorzP,\colorzM)$.
For the rest of the paper, we refer to
onboard policy $\stategreen{\pi}(\colorzR,\colorzM)$ as \policyR, and to
privileged policy $\stategreen{\pi}(\colorzP,\colorzM)$ as \policyP.
%
For a detailed overview of the architecture refer to Fig. \ref{fig:architecture}.

\vspace{1mm}
\subsubsectioncol{Losses} We use a combination of three loss function terms for training the policy network: two behavioral cloning terms for actions and one latent alignment term for the alignment of information in the encoder latent space.

\vspace{1mm}
\subsubsubsection{Behavioral Cloning loss.} $L_{BC}$ consists of Mean Squared Error (MSE) for the Cartesian ($\mathbf{xyz}$) and finger ($\mathbf{f}$) actions, and the unsigned relative rotation angle for the quaternion orientation ($\mathbf{q}$) action:
\vspace*{-2mm}
\begin{multline*}
L_{BC}(a,\hat{a}) = MSE(a_{xyz/f},\hat{a}_{xyz/f}) + 2\arccos({\hat{a}_{q}}^Ta_{q}).
\end{multline*}

$L_{BC}$ is calculated for both actions produced from privileged and onboard information encoder latents, against the demonstrated action $\hat{a}$.

\vspace{1mm}
\subsubsubsection{Latent Alignment loss.} $L_{align}$ is used to align the unit vector outputs of the privileged and onboard information encoders. We empirically find that Huber loss~\cite{huber1992robust} works better than MSE loss for the alignment on our tasks.

\vspace{1mm}
\subsubsubsection{Total loss.}
Overall, single-stage privileged training loss looks as follows:
\vspace*{-2mm}
\begin{multline*}
L_{total} = L_{BC}(\coloractR,\mathbf{\hat{a}}) + L_{align}(\colorzR,\colorzP) + L_{BC}(\coloractP,\mathbf{\hat{a}}).
\end{multline*}

\subsubsubsection{Training regimes.}
The combination of all loss terms simulates a single-stage privileged co-training regime, where:
\vspace{1mm}
\begin{enumerate}
    \setlength\itemsep{5pt}
	\item $L_{BC}(\coloractP,\mathbf{\hat{a}})$ trains the privileged \policyP~(a.k.a. \textit{Stage I})
	\item $L_{align}(\colorzR,\colorzP)$ emulates the transfer (a.k.a. \textit{Stage II})
	\item $L_{BC}(\coloractR,\mathbf{\hat{a}})$ adapts the behavior of  \policyR~for $\colorobs$.
\end{enumerate}

\vspace{1mm}
Through hyper-parameter search, we find that weighting all of the loss terms equally leads to the most efficient training regime of the onboard policy \policyR.

We also explore the variations of $L_{align}(\mathbf{sg(\colorzR}),\colorzP)$ and $L_{align}(\colorzR,\mathbf{sg(\colorzP}))$, to only allow for alignment in one direction by stopping the gradients ($\mathbf{sg()}$), however, we find no significant difference in the performance of \policyR.

\subsubsectioncol{Regularization} To regularize the network and make it robust to occlusions from different robot morphologies, we use image augmentation during the training process. We use the \textit{imgaug} library~\cite{imgaug} to augment camera images during training. For each image, we apply either \textit{Cutout} (random patch dropout) or \textit{CoarseDropout} (smaller but more frequent patch dropout) augmentation with a $50$\% chance.
We find these augmentations to be of high importance for training all our policies.

\subsubsectioncol{Training Parameters}
For all experiments in this paper we use the Adam optimizer with a learning rate of $3e-4$.
The total training time of the \shortname~policy \policy~takes five~days on a single V100 GPU.