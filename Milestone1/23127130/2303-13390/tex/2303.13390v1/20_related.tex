\section{RELATED WORK}

\subsection{Morphology Optimization}

\looseness=-1
Robot's physical design is an important factor in its performance and ability to complete assigned tasks.
Traditionally, robot designs are made by human experts who rely on heuristics to optimize the structure.
A number of approaches have been developed to algorithmically assist designers in making better physical structures~\cite{coello1998using, kouritem2022multi}, improving reachability~\cite{seraji1995reachability, vahrenkamp2009humanoid, makhal2018reuleaux}, and observability of the workspace~\cite{triggs1995automatic, chen2004automatic, nikolaidis2009optimal}.
However, heuristic approaches don't offer any insight on the design's performance on actual tasks.
This is particularly true for tasks that involve vision and learning, where the robot's physical structure can significantly impact learning efficiency and performance. 

\looseness=-1
Recently, researchers have aimed to address the limitations of robot designs by optimizing their physical structures in an informed and systematic way.
A number of works have proposed computational approaches for co-optimizing the design parameters and motion trajectories of robotic systems~\cite{ha2018computational, ha2018computational2}.
Typically these works model the relationship between form and function as solutions to an optimal control problem, often showing that such frameworks are effective in optimizing robotic design for a variety of tasks.
Recently, a number of methods were proposed for the joint optimization of physical structures using policy optimization methods~\cite{wampler2009optimal, ha2019reinforcement, zhao2020robogrammar}.
While these approaches bring us closer to optimizing the designs for performance on the actual tasks, so far they tackle cases with proprioceptive sensor information and do not take into account vision, which is often critical in deep learning approaches~\cite{saycan2022arxiv, rt12022arxiv}.
In this work, we incorporate exteroceptive information obtained from the onboard camera sensor to account for an interplay between the design and the robot's ability to sense task-relevant information.

\subsection{Vision-based Robot Learning}

\looseness=-1
Visual sensing plays a crucial role in the field of robotics, enabling the robots' perception and understanding of their surroundings. 
The field has come a long way in terms of explicitly understanding the world through pixels ~\cite{he2016deep, He_2017_ICCV, dosovitskiy2020image} and 3D ~\cite{monodepth2, philion2020lift, guizilini20203d}, as well as, implicitly through learned features that enhance downstream task performance~\cite{grill2020bootstrap, shah2021rrl}.

\looseness=-1
Utilizing the advances of learning, the field of robotics has been booming with pipelines that leverage Behavioral Cloning~\cite{pomerleau1988alvinn} and Reinforcement Learning~\cite{sutton2018reinforcement} methods.
Many systems that are designed to be deployed autonomously in the real-world leverage both exteroceptive sensing and learning.
Notably, a number of projects that were deployed in the real-world leveraged learning and vision, some examples of such systems include robot locomotion~\cite{yu2021visual, miki2022learning, agarwallegged}, navigation~\cite{tolani2021visual, hoeller2021learning, sorokin2022learning}, and manipulation~\cite{kalashnikov2018qt, xia2020relmogen, jang2022bc, khansari2022practical}.
These works focus on improvements to the training pipelines and algorithms while keeping the robot characteristics intact.
In this work, we leverage findings from vision-based robotics research and use manipulation problems as a testing ground for design optimization.


\subsection{Surrogate Evaluation}

\looseness=-1
A surrogate function is a model that approximates the behavior of the true function that is costly to produce~\cite{sobester2008engineering}.
In the context of robot design optimization, a true function would measure the quality of the design through a full cycle of data collection, training, and evaluation.
Alas, with a large number of designs to rate, it is infeasible to go through the whole process from collection to evaluation for each design.
Thus, we explore the idea of a morphology-agnostic controller~\cite{Yu-RSS-17,zhao2020robogrammar,gupta2022metamorph} which is capable of controlling a range of morphologies.
Zhao~et~al.~\cite{zhao2020robogrammar} used a model predictive control to evolve terrain traversal creatures. 
Such controllers can be used as a \textit{proxy} to evaluate the performance of actual robot designs.

\subsection{Universal Policy}
\looseness=-1
A universal policy can be deÔ¨Åned as a controller which is able to operate in different environments while performing various tasks.
In the context of learning, universal controllers are made possible through the exposure of the controller to a diverse set of environments during training~\cite{tobin2017domain}.
Yu~et~al.~\cite{Yu-RSS-17} showed that it is possible to effectively adapt the controller to unknown task environments.
Gupta~et~al.~\cite{gupta2022metamorph} leveraged learning and demonstrated cross-morphology policy transfer in the context of locomotion.
In~RT-1~\cite{rt12022arxiv}, it was recently shown how a large-scale training could enable effective multi-task learning in different environments and help with cross-robot transfer.
These approaches provide a strong foundation to enable generalization to unseen environments and tasks.
In this work, we propose a recipe to enable the training of a universal controller that works across different morphologies while using the robot's \textit{onboard} sensing.


\begin{figure*}[!ht]
  \centering
  \includegraphics[width=1.0\textwidth]{figs/architecture.png}
  \vspace{-7mm}
  \caption{
  \textbf{Overview of policy \policy\ a \cleanfullpolicyname trained via \onestagefullname (\onestageshortname)}
  \protect\\
  \textit{\stateorange{Data Collection}} - demonstrates a few robot designs randomly generated during the collection process. Robot trajectories are collected using motion planning and only successful trajectories are stored in the Trajectory Buffer.
  \textit{\stateblue{Encoder $R$}} processes information obtained from onboard sensors (e.g., cameras) to generate embedding $\colorzR$.
  \textit{\statered{Encoder $P$}} extracts $\colorzP$ from privileged state which contains a comprehensive information about the robot and workspace.
  $\colorzP$ and $\colorzR$ are explicitly aligned through a loss function to produce mutual information.
  Then, \textit{\stategreen{policy head $\pi$}} generates onboard and privileged actions $\coloractP$/ $\coloractR$ using morphology embedding $\colorzM$, and $\colorzR$/$\colorzP$.
  As such onboard (\policyP$\gets\pi(\colorzP,\colorzM$)) and privileged (\policyR$\gets\pi(\colorzR,\colorzM$)) policies are trained jointly via latent alignment and share \textit{\stategreen{policy head $\pi$}}. For details refer to Section \ref{sec:universal_pi}.
  }
  \vspace{-4mm}
  \label{fig:architecture}
\end{figure*}

\looseness=-1
To better guide the controller during training, we leverage ideas of privileged learning~\cite{chen2020learning} to bootstrap the training.
A privileged pipeline enables a more capable and sample efficient policy training that can control different morphologies.
Overall, using the morphology-agnostic controller as a surrogate function allows us to efficiently explore the design candidate space and find an improved morphology design.
