@inproceedings{belz-etal-2020-disentangling,
    title = "Disentangling the Properties of Human Evaluation Methods: A Classification System to Support Comparability, Meta-Evaluation and Reproducibility Testing",
    author = "Belz, Anya  and
      Mille, Simon  and
      Howcroft, David M.",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    month = dec,
    year = "2020",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.inlg-1.24",
    pages = "183--194",
    abstract = "Current standards for designing and reporting human evaluations in NLP mean it is generally unclear which evaluations are comparable and can be expected to yield similar results when applied to the same system outputs. This has serious implications for reproducibility testing and meta-evaluation, in particular given that human evaluation is considered the gold standard against which the trustworthiness of automatic metrics is gauged. {\%}and merging others, as well as deciding which evaluations should be able to reproduce each other{'}s results. Using examples from NLG, we propose a classification system for evaluations based on disentangling (i) what is being evaluated (which aspect of quality), and (ii) how it is evaluated in specific (a) evaluation modes and (b) experimental designs. We show that this approach provides a basis for determining comparability, hence for comparison of evaluations across papers, meta-evaluation experiments, reproducibility testing.",
}

@misc{gehrmann2022repairing,
      title={Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text}, 
      author={Sebastian Gehrmann and Elizabeth Clark and Thibault Sellam},
      year={2022},
      eprint={2202.06935},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{VANDERLEE2021101151,
	abstract = {Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated, with a particularly high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how (mostly intrinsic) human evaluation is currently conducted and presents a set of best practices, grounded in the literature. These best practices are also linked to the stages that researchers go through when conducting an evaluation research (planning stage; execution and release stage), and the specific steps in these stages. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG.},
	author = {van der Lee, Chris and Albert Gatt and Emiel {van Miltenburg} and Emiel Krahmer},
	doi = {https://doi.org/10.1016/j.csl.2020.101151},
	issn = {0885-2308},
	journal = {Computer Speech \& Language},
	keywords = {Natural Language Generation, Human evaluation, Recommendations, Literature review, Open science, Ethics},
	pages = {101151},
	title = {Human evaluation of automatically generated text: Current trends and best practice guidelines},
	url = {https://www.sciencedirect.com/science/article/pii/S088523082030084X},
	volume = {67},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S088523082030084X},
	bdsk-url-2 = {https://doi.org/10.1016/j.csl.2020.101151}}


@inproceedings{van-miltenburg-etal-2021-underreporting,
    title = "Underreporting of errors in {NLG} output, and what to do about it",
    author = {van Miltenburg, Emiel  and
      Clinciu, Miruna  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Gkatzia, Dimitra  and
      Inglis, Stephanie  and
      Lepp{\"a}nen, Leo  and
      Mahamood, Saad  and
      Manning, Emma  and
      Schoch, Stephanie  and
      Thomson, Craig  and
      Wen, Luou},
    booktitle = "Proceedings of the 14th International Conference on Natural Language Generation",
    month = aug,
    year = "2021",
    address = "Aberdeen, Scotland, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.inlg-1.14",
    pages = "140--153",
    abstract = "We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by {`}state-of-the-art{'} research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting.",
}

@article{DBLP:journals/corr/abs-2006-14799,
  author    = {Asli Celikyilmaz and
               Elizabeth Clark and
               Jianfeng Gao},
  title     = {Evaluation of Text Generation: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2006.14799},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.14799},
  eprinttype = {arXiv},
  eprint    = {2006.14799},
  timestamp = {Wed, 03 Mar 2021 07:40:24 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-14799.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1145/3485766,
author = {Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.},
title = {A Survey of Evaluation Metrics Used for NLG Systems},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3485766},
doi = {10.1145/3485766},
abstract = {In the last few years, a large number of automatic evaluation metrics have been proposed for evaluating Natural Language Generation (NLG) systems. The rapid development and adoption of such automatic evaluation metrics in a relatively short time has created the need for a survey of these metrics. In this survey, we (i) highlight the challenges in automatically evaluating NLG systems, (ii) propose a coherent taxonomy for organising existing evaluation metrics, (iii) briefly describe different existing metrics, and finally (iv) discuss studies criticising the use of automatic evaluation metrics. We then conclude the article highlighting promising future directions of research.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {26},
numpages = {39},
keywords = {Automatic evaluation metrics, question generation, image captioning, correlations, abstractive summarization, question answering, data-to-text generation}
}

@inproceedings{shimorina-belz-2022-human,
    title = "The Human Evaluation Datasheet: A Template for Recording Details of Human Evaluation Experiments in {NLP}",
    author = "Shimorina, Anastasia  and
      Belz, Anya",
    booktitle = "Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.humeval-1.6",
    doi = "10.18653/v1/2022.humeval-1.6",
    pages = "54--75",
    abstract = "This paper presents the Human Evaluation Datasheet (HEDS), a template for recording the details of individual human evaluation experiments in Natural Language Processing (NLP), and reports on first experience of researchers using HEDS sheets in practice. Originally taking inspiration from seminal papers by Bender and Friedman (2018), Mitchell et al. (2019), and Gebru et al. (2020), HEDS facilitates the recording of properties of human evaluations in sufficient detail, and with sufficient standardisation, to support comparability, meta-evaluation,and reproducibility assessments for human evaluations. These are crucial for scientifically principled evaluation, but the overhead of completing a detailed datasheet is substantial, and we discuss possible ways of addressing this and other issues observed in practice.",
}

@inproceedings{van-miltenburg-etal-2021-preregistering,
    title = "Preregistering {NLP} research",
    author = "van Miltenburg, Emiel  and
      van der Lee, Chris  and
      Krahmer, Emiel",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.51",
    doi = "10.18653/v1/2021.naacl-main.51",
    pages = "613--623",
    abstract = "Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.",
}

@article{reiter-2018-structured,
    title = "A Structured Review of the Validity of {BLEU}",
    author = "Reiter, Ehud",
    journal = "Computational Linguistics",
    volume = "44",
    number = "3",
    month = sep,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-3002",
    doi = "10.1162/coli_a_00322",
    pages = "393--401",
    abstract = "The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique{---}in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.",
}

@BOOK{Emerson1982-zx,
  title     = "Selected Essays",
  author    = "Emerson, Ralph Waldo",
  publisher = "Penguin Books",
  series    = "American Library",
  month     =  sep,
  year      =  1982,
  address   = "Harlow, England",
  language  = "en"
}
