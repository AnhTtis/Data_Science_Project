% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% \usepackage{subfigure}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{threeparttable}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.



%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6382} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}
%%%%%%%%% TITLE - PLEASE UPDATE
% \title{\LaTeX\ Author Guidelines for \confName~Proceedings}
\title{Adaptive Sparse Convolutional Networks with Global Context Enhancement for Faster Object Detection on Drone Images}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

% \author{Bowei Du\\
% Beihang University\\
% % Institution1 address\\
% {\tt\small boweidu@buaa.edu.cn}
% \and
% Yecheng Huang\\
% Beihang University\\
% % Institution1 address\\
% {\tt\small ychuang@buaa.edu.cn}
% \and
% Jiaxin Chen\\
% Beihang University\\
% % Institution1 address\\
% {\tt\small jiaxinchen@buaa.edu.cn}
% \and
% Di Huang\\
% Beihang University\\
% % Institution1 address\\
% {\tt\small dhuang@buaa.edu.cn}
% }



% \author{
%     Bowei Du\authornote{*},
%     Yecheng Huang\thanks{indicates equal contribution.}\ ,
%     Jiaxin Chen,
%     Di Huang\thanks{indicates the corresponding author.}\\
%     Beihang University\\
%     {\tt\small \{boweidu, ychuang, jiaxinchen, dhuang\}@buaa.edu.cn}
% }
% \author{
%     Bowei Du,
%     Yecheng Huang\thanks{indicates equal contribution.}\ ,
%     Jiaxin Chen,
%     Di Huang\thanks{indicates the corresponding author.}\\
%     Beihang University\\
%     {\tt\small \{boweidu, ychuang, jiaxinchen, dhuang\}@buaa.edu.cn}
% }
% \author{
%     \IEEEauthorblockN{Bowei Du$^{1,3*}$, Yecheng Huang$^{1,3*}$, Jiaxin Chen$^{2,3}$, Di Huang$^{1,3\dag}$}\\
%     \IEEEauthorblockA{$^1$ State Key Laboratory of Software Development Environment, Beihang University, Beijing, China}\\
%     \IEEEauthorblockA{$^2$ State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China}\\
%     \IEEEauthorblockA{$^3$ School of Computer Science and Engineering, Beihang University, Beijing, China}\\
%     \IEEEauthorblockA{\{boweidu, ychuang, jiaxinchen, dhuang\}@buaa.edu.cn}
% }
\author{
    \textbf{Bowei Du}$^{1,2\dag}$, \textbf{Yecheng Huang}$^{1,2\dag}$, \textbf{Jiaxin Chen}$^{2}$, \textbf{Di Huang}$^{1,2,3*}$\\
    {$^1$ State Key Laboratory of Software Development Environment, Beihang University, Beijing, China}\\
    {$^2$ School of Computer Science and Engineering, Beihang University, Beijing, China}\\
    {$^3$ Hangzhou Innovation Institute, Beihang University, Hangzhou, China}\\
    {\{boweidu, ychuang, jiaxinchen, dhuang\}@buaa.edu.cn}
}


\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
%Most drone object detectors spend excessive resources on every pixel and focus on improving accuracy without considering model complexity and inference speed. Some previous work introduces sparse convolution for faster inference to object classification and object detection. However, the regions ignored by the sparse network are not effectively processed, and there is no practical method to dynamically constrain the sparsity degree of the network, which can be detrimental to accuracy and inference speed. To address the above issues, we first apply the sparse network structure to base object detectors and then propose instances aware based focal-global dynamic head structure. On the one hand, we introduce a focal and global information fusion method to enhance features extracted by sparse convolution to consolidate the information carried by features and restore the relationship between pixels. On the other hand, we propose a instances aware resource budget estimation method through ground truth label assignment in the training phase, which further develops flexibility of the dynamic head and promotes accuracy and inference speed. In the popular drone image dataset VisDrone, Our method based on GFL baseline reduces GFLOPS by 70.7\% and improves the inference speed on GPU by 56.2\%. In contrast, our method only reduces mAP by 0.25 compared to baseline. Similar results are obtained from experiments on other datasets and baselines. Code will be available soon.
%Object detection on drone images with low-latency is an important but challenging task on the resource-constrained Unmanned Aerial Vehicle (UAV) platform, which however is not fully resolved by existing works. In this paper, we investigate the detection head optimization based on the sparse convolution, which proves effective in balancing the accuracy and efficiency. Nevertheless, it suffers from the insufficient integration of contextual information of tiny objects, as well as the clumsy control of mask ratio in the presence of foregrounds with varying scales. To address the issues above, we propose a novel global Context-Enhanced Adaptive Sparse Convolutional network (CEASC). CEASC first develops a  context-enhanced group normalization (CE-GN) layer, by replacing the statistics based on sparsely sampled features with the global contextual ones. Subsequently, CEASC designs an adaptive multi-layer masking scheme to generate optimal mask ratios at distinct scales for compact foreground coverage, promoting both the accuracy and efficiency. Extensive experimental results on two benchmarks, \ie VisDrone and UAVDT, demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the inference speed when plugging into the state-of-the-art detection frameworks (\eg RetinaNet and GFL V1) with a competitive accuracy. \textcolor{red}{Code is available at https://github.com/Cuogeihong/CEASC.}
Object detection on drone images with low-latency is an important but challenging task on the resource-constrained unmanned aerial vehicle (UAV) platform. This paper investigates optimizing the detection head based on the sparse convolution, which proves effective in balancing the accuracy and efficiency. Nevertheless, it suffers from inadequate integration of contextual information of tiny objects as well as clumsy control of the mask ratio in the presence of foreground with varying scales. To address the issues above, we propose a novel global context-enhanced adaptive sparse convolutional network (CEASC). It first develops a context-enhanced group normalization (CE-GN) layer, by replacing the statistics based on sparsely sampled features with the global contextual ones, and then designs an adaptive multi-layer masking strategy to generate optimal mask ratios at distinct scales for compact foreground coverage, promoting both the accuracy and efficiency. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the inference procedure when plugging into the typical state-of-the-art detection frameworks (e.g. RetinaNet and GFL V1) with competitive performance. Code is available at https://github.com/Cuogeihong/CEASC.
\end{abstract}

\renewcommand{\thefootnote}{}
% \footnotetext{$\dag$ indicates equal contribution. $*$ refers to the corresponding author.}
\footnotetext{$\dag$ indicates equal contribution.}
\footnotetext{$*$ refers to the corresponding author.}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
% Introduce the progress of CNNs on aerial small object detection and current methods' feedback: excessive computing resource consumption. 

% In recent years, the continuous development of neural networks has greatly improved the accuracy and efficiency of classical vision tasks[]. Under the circumstance, the object detection task also achieves quite excellent results on the classical dataset COCO[] and several other applications[]. However, object detection on drone images still has difficulties in accuracy and efficiency as instances in drone images usually have relatively smaller scale than instances in COCO. While there exists plenty of previous improvements promoting drone object detector's accuracy, they usually follow a coarse-to-fine pipeline and thus consuming excessive computing resource, preventing its widespread use in practice. For drone object detection, the optimization of object detection efficiency is more important, because it usually consumes more computation costs to introduce features with higher resolution or more channels to detect relatively smaller objects but with an application requirement to deploy object detectors on devices with poor computing ability.

%In recent years, the continuous development of neural networks has greatly improved the accuracy and efficiency of classical vision tasks. Under the circumstance, the object detection task also achieves quite excellent results on the classical dataset \emph{e.g.} COCO~\cite{COCO2014} and several other applications. However, object detection on drone images still has difficulties in accuracy and efficiency as instances in drone images usually have relatively smaller scale than instances in COCO. While there exists plenty of previous improvements promoting drone object detector's accuracy, they usually follow a coarse-to-fine pipeline and thus consuming excessive computing resource. For drone object detection, the optimization of object detection efficiency is more important, because it usually consumes more computation costs to introduce features with higher resolution or more channels to detect relatively smaller objects but with an application requirement to deploy object detectors on devices with poor computing ability.

%Recent progress of deep neural networks has greatly improved the performance of object detection for general purpose on conventional benchmarks such as COCO~\cite{COCO2014}. However, object detection based on the unmanned aerial vehicle (UAV) platform remains a challenging task. On the one hand, in order to detect tiny objects on high-resolution drone imagery with a high accuracy, most existing works usually adopt complicated models, thus being computationally consuming. On the other hand, the hardware equipped with UAVs is often resource-constrained, raising the concern about lightweight deployed model for fast inference and low latency. 

Recent progress of deep neural networks (\emph{e.g.} CNNs and Transformers) has significantly boosted the performance of object detection on public benchmarks such as COCO~\cite{COCO2014}. By contrast, building detectors for unmanned aerial vehicle (UAV) platforms currently remains a challenging task. On the one hand, existing studies are keen on designing complicated models to reach high accuracies of tiny objects on high-resolution drone imagery, which are computationally consuming. On the other hand, the hardware equipped with UAVs is often resource-constrained, raising an urgent demand in lightweight deployed models for fast inference and low latency. 
%\begin{figure}[!t]
%\centering
%\subfigure[]{
%\begin{minipage}[b]{\linewidth}
%\centering
%\includegraphics[width=\linewidth]{introduce-l-4-v2.pdf}
%\end{minipage}
%%\caption{fig1}
%}
%\subfigure[]{
%\begin{minipage}[b]{\linewidth}
%\centering
%\includegraphics[width=\linewidth]{introduce-r-3-v2.pdf}
%\end{minipage}
%}
%\caption{(a) Comparison of foreground proportions on COCO and drone-based datasets; %(b)Visualization of the foreground areas (highlighted by yellow shapes) on VisDrone and %UAVDT.}\label{fig:challenge}
%%\vspace{-0.3cm}
%\end{figure}
\begin{figure}[!t]
    \centering
    % \subfigure[]{
    %     \begin{minipage}[b]{\linewidth}
    %         \centering
    %         \includegraphics[width=\linewidth]{introduce-l-4-v2.pdf}
    %     \end{minipage}
    %     %\caption{fig1}
    % }
    % \subfigure[]{
    %     \begin{minipage}[b]{\linewidth}
    %         \centering
    %         \includegraphics[width=\linewidth]{introduce-r-3-v2.pdf}
    %     \end{minipage}
    % }
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{introduce-l-4-v9.pdf}
        \caption{}
        \label{fig:short-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
    \includegraphics[width=1\linewidth]{introduce-r-4.pdf}
        \caption{}
        \label{fig:short-b}
    \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
    %\caption{(a) Comparison of foreground proportions on COCO and drone-based datasets; (b) Visualization of the foreground areas (highlighted by yellow shapes) on VisDrone and UAVDT.}
    %\caption{(a) Comparison of foreground proportions on the COCO and drone imagery databases; (b) Visualization of foregrounds (highlighted by yellow) on samples from VisDrone and UAVDT.}
    \caption{(a) Comparison of foreground proportions on the COCO and drone imagery databases; and (b) visualization of foregrounds (highlighted in yellow) on samples from VisDrone and UAVDT.}
    \label{fig:challenge}
    % \vspace{-0.1cm}
\end{figure}

%To handle with the dilemma above, many approaches have been proposed, most of which concentrate on reducing the computational cost of the backbone networks~\cite{MobileNetV12017,ShufflenetV12018,DistillationDetection2017}. Nevertheless, these methods fail to take into account the heavy detection head, which is widely used by the state-of-the-art detectors for promoting the accuracy, especially on drone image detectors~\cite{ufpmpdet2022}. For instance, RetinaNet~\cite{RetinaNet2017} with ResNet18~\cite{ResNet2016} backbone network and 512 input channels adopts a detection head that occupies 82.3\% of the overall GFLOPs. Recently, several methods have been proposed to optimize the detection head, including network pruning~\cite{groupfisherprune2021,slimyolov32019} and lightweight network structure~\cite{yolov42020,yolox2021}, showing its potential in accelerating the inference speed. However, pruning methods often fail to maintain accuracy after significant computation decline and lightweight networks adopt low input resolution ($640 \times 640$) and lack of structure design for aerial images, thus cannot be effectively applied in lightweight aerial image detector.

%To handle the dilemma of balancing accuracy and efficiency, a number of efforts are made on general object detection, which mainly concentrate on reducing the complexity of the backbone networks~\cite{MobileNetV12017, ShufflenetV12018, DistillationDetection2017}. Despite some potential, these methods leave much room for improvement since they fail to take into account the heavy detection heads which are widely used by the state-of-the-art detectors~\cite{RetinaNet2017,ATSS2020,GFLv12020,ufpmpdet2022}. For instance, RetinaNet~\cite{RetinaNet2017} taking ResNet18~\cite{ResNet2016} as backbone with 512 input channels adopts a detection head that occupies 82.3\% of the overall GFLOPs. Recently, several methods have been presented to solve this problem, including network pruning~\cite{groupfisherprune2021, slimyolov32019} and structure redesigning~\cite{yolov42020, yolox2021}, and prove effective in accelerating inference. However, the former is criticized by the sharp performance drop when computations are greatly decreased, evidenced by the attempt on detection for UAVs \cite{slimyolov32019}, and the latter is basically optimized for low-resolution input (\emph{e.g.} $640 \times 640$), making it not straightforward to adapt to high-resolution aerial images.

To deal with the dilemma of balancing the accuracy and efficiency, a number of efforts are made, mainly on general object detection, which basically concentrate on reducing the complexity of the backbone networks~\cite{MobileNetV12017, ShufflenetV12018, DistillationDetection2017}. Despite some potential, these methods leave much room for improvement since they fail to take into account the heavy detection heads which are widely used by the state-of-the-art detectors~\cite{RetinaNet2017,ATSS2020,GFLv12020,ufpmpdet2022}. For instance, RetinaNet~\cite{RetinaNet2017} taking ResNet18~\cite{ResNet2016} as backbone with 512 input channels adopts a detection head that occupies 82.3\% of the overall GFLOPs. Recently, several methods have been presented to solve this problem, including network pruning~\cite{groupfisherprune2021, slimyolov32019} and structure redesigning~\cite{yolov42020, yolox2021}, and prove effective in accelerating inference. However, the former is criticized by the sharp performance drop when computations are greatly decreased, evidenced by the attempt on detection for UAVs \cite{slimyolov32019}, and the latter is primarily optimized for low-resolution input (\emph{e.g.} $640 \times 640$), making it not straightforward to adapt to high-resolution aerial images.

%optimizing the head of the object detector according to the characteristics of the aerial image has become an important way to enhance the efficiency of the aerial object detector. As Fig.1 (b) shows, the proportion of foreground in the drone image dataset is much lower than that in common datasets, which makes researchers tend to detect objects by only computing foreground regions through sparse convolution.

%has difficulties in accuracy and efficiency as instances in drone images usually have relatively smaller scale than instances in COCO. While there exists plenty of previous improvements promoting drone object detector's accuracy, they usually follow a coarse-to-fine pipeline and thus consuming excessive computing resource. For drone object detection, the optimization of object detection efficiency is more important, because it usually consumes more computation costs to introduce features with higher resolution or more channels to detect relatively smaller objects but with an application requirement to deploy object detectors on devices with poor computing ability.

%%%image

% Introduce traditional quantitative methods, point out that most of them only refine the backbone state and they are not designed for aerial object detection, while the aerial images have more background areas than common images and these backgrounds' information can be collected and generated by a simple way.

%Several methods has been proposed to reduce computing cost like lightweight models~\cite{MobileNetV12017,ShufflenetV12018}, knowledge distillation~\cite{DistillationDetection2017}. Most of them focus on optimization to backbone network, while state-of-the-art detectors prefer to use the heavy detection head to enhance the detection performance, as the detection head of RetinaNet~\cite{RetinaNet2017} with ResNet18~\cite{ResNet2016} as the backbone network and 512 input channels occupies 82.3\% of GFLOPs for instance. As a result, optimizing the head of the object detector according to the characteristics of the aerial image has become an important way to enhance the efficiency of the aerial object detector. As Fig.1 (b) shows, the proportion of foreground in the drone image dataset is much lower than that in common datasets, which makes researchers tend to detect objects by only computing foreground regions through sparse convolution. 

%However, its application to drone-based object detection is seldomly investigated, leaving much room for performance improvement. The sparse convolution is one of the representative detection head optimization methods, which have shown its effectiveness in drone-based object detection. However, the current detection head optimization method using sparse convolution cannot adapt to the following two problems in drone object detection effectively, the usage of background information and the dynamic control of foreground area division. As Fig.1 (a) shows, drone images usually have a large number of small and scattered foreground areas, make it difficult to effectively synthesize the information only by processing the foreground features and also difficult to learn the information between each pixel and even each region. For instance, understanding instances within an area are on roads or pedestrian streets can help the network learn classification and shape preferences. In addition, less information can easily lead to fluctuations in the feature distribution, which affects commonly used regularization methods~\cite{GN2018} and reduces training accuracy. Meanwhile, we can also find that the foreground area of the UAV image has a larger variance on scale than the conventional image, which renders it inefficient to control the division of the foreground area by simply setting the expected foreground area as a fixed value like ~\cite{DynamicConvolution2020,DynamicHead2020}. This approach cannot calculate appropriate foreground proportion for each inference image, resulting in incomplete inference area or too much background information and affecting the accuracy.

%Sparse convolution \cite{SECOND2018,SACT2017} is another representative detection head optimization method, reducing the computational cost by only operating on sparsely sampled regions or channels via learnable masks, suitable for small object detection scenario like aerial images. Application of sparse convolution often requires optimal trade-off between accuracy and efficiency by accurate selection of the concerned region. As the focal region of the learned mask in sparse convolution is prone to locate in the foreground area, when the mask ratio is inadequately low, the focal part becomes excessively large, brings in a large amount of unnecessary computations on backgrounds and affects both efficiency and accuracy. On the contrary, a high mask ratio will make focal region too small to fully cover foregrounds, greatly deteriorating the accuracy. What's more, as shown in Fig.~\ref{fig:challenge}(a), a large portion of objects in drone images are with small scales, heavily relying on the context, which proves effective in detecting tiny objects. Placing these context into the focal region greatly affects detection efficiency, inspiring us to extract global information to enhance sparse convolution results to realize efficient utilization of the foreground context.

%Sparse convolutions \cite{SECOND2018, SACT2017} show another promising alternative, which limit computations by only operating convolutions on sparsely sampled regions or channels via learnable masks. While theoretically attractive, their results are highly dependent on the selection of meaningful areas, because the focal region of the learned mask in sparse convolutions is prone to locate within foreground. Regarding drone images, the vast majority of objects are of small scales (as shown in Fig.~\ref{fig:challenge} (a)) and the scale of foreground areas varies along with flying altitudes and observing viewpoints (as shown in Fig.~\ref{fig:challenge} (b)), and this issue becomes even more prominent. An inadequate mask ratio enlarges the focal part and more unnecessary computations are consumed on background, which tends to simultaneously deteriorate efficiency and accuracy. On the contrary, an exaggerated one shrinks the focal part and incurs the difficulty in fully covering foreground and crucial context, thus leading to performance degradation. DynamicHead~\cite{DynamicHead2020} and QueryDet~\cite{QueryDet2022} indeed apply sparse convolutions to the detection head; unfortunately, the primary goal is to offset the increased computational cost when additional feature maps are jointly used for performance gain on general object detection. They both follow the traditional way in original sparse convolutions that set fixed mask ratios or focus on foreground only and are thus far from reaching the trade-off between accuracy and efficiency required by UAV detectors. Therefore, it is still an open question to leverage sparse convolutions to facilitate lightweight detection for UAVs.

Sparse convolutions \cite{SECOND2018, SACT2017} show another promising alternative, which limit computations by only operating convolutions on sparsely sampled regions or channels via learnable masks. While theoretically attractive, their results are highly dependent on the selection of meaningful areas, because the focal region of the learned mask in sparse convolutions is prone to locate within foreground. Regarding drone images, the vast majority of objects are of small scales (as shown in Fig.~\ref{fig:challenge} (a)) and the scale of foreground areas varies along with flying altitudes and observing viewpoints (as shown in Fig.~\ref{fig:challenge} (b)), and this issue becomes even more prominent. An inadequate mask ratio enlarges the focal part and more unnecessary computations are consumed on background, which tends to simultaneously deteriorate efficiency and accuracy. On the contrary, an exaggerated one shrinks the focal part and incurs the difficulty in fully covering foreground and crucial context, thus leading to performance degradation. DynamicHead~\cite{DynamicHead2020} and QueryDet~\cite{QueryDet2022} indeed apply sparse convolutions to the detection head; unfortunately, their primary goal is to offset the increased computational cost when additional feature maps are jointly used for performance gain on general object detection. They both follow the traditional way in original sparse convolutions that set fixed mask ratios or focus on foreground only and are thus far from reaching the trade-off between accuracy and efficiency required by UAV detectors. Therefore, it is still an open question to leverage sparse convolutions to facilitate lightweight detection for UAVs.

% Recently, QueryDet \cite{QueryDet2022} makes the first attempt to investigate the effectiveness of sparse convolution in drone-based object detection. It promotes the accuracy by introducing low-level high-resolution feature maps, of which the additional computation cost is offset by applying the sparse convolution with fixed mask ratios. However, QueryDet doesn't deal with the following two prevailing issues in drone imagery, thus failing to fully explore the advantage of sparse convolution in accelerating the inference. First, as shown in Fig.~\ref{fig:challenge}(a), a large portion of objects in drone images are with small scales, heavily relying on the context, which proves effective in detecting tiny objects. However, the visible part of the learned mask in sparse convolution is prone to locate in the foreground area, thus inclining to abandon useful global contextual information. Second, as displayed in Fig.~\ref{fig:challenge}(b), due to the fluctuating viewpoints and flying altitudes of drones, the scale of foreground areas changes significantly. Nevertheless, the conventional sparse convolution often utilizes a fixed mask ratio~\cite{DynamicConvolution2020,DynamicHead2020}, incurring the following two problems: (1) when the mask ratio is improperly high, the area of the visible part would become too small to fully cover large foregrounds; (2) when the mask ratio is inadequately low, the visible part becomes excessively large such that a lot of background areas are enclosed for convolution. The former results in incomplete object representation, which deteriorates the accuracy; the later brings in a large amount of unnecessary computations on backgrounds, thus impeding efficient inference. 

%Recently, QueryDet\cite{QueryDet2022} and DynamicHead~\cite{DynamicHead2020} apply sparse convolution to the detect head and reduce the computation cost. However, they do not deal with the prevailing issues above and cannot adapt to the characteristics of aerial images. As displayed in Fig.~\ref{fig:challenge}(b), due to the fluctuating viewpoints and flying altitudes of drones, the scale of foreground areas changes significantly. DynamicHead does not design suitable mask rate generated for different training images and QueryDet only focus on small objects, difficult to adapt to fickle foreground distribution of aerial images. Moreover, the lack of utilization of context area leads to the performance degradation after introduction of sparse convolution. 

%In this paper, we propose a novel plug-and-play detection head optimization approach to efficient object detection on drone images, namely global Context-Enhanced Adaptive Sparse Convolution (CEASC). Concretely, we first develop a Context-Enhanced Sparse Convolution (CESC) to capture global information and enhance focal features, which consists of a residual structure with a Context-Enhanced Group Normalization (CE-GN) layer. Since CE-GN specifically preserves a set of holistic features and applies their statistics for normalization, it compensates the loss of context caused by sparse convolutions and stabilizes the distribution of foreground areas, thus bypassing the sharp drop on accuracy. We then propose an Adaptive Multi-layer Masking (AMM) scheme, and it separately estimates an optimal mask ratio by minimizing an elaborately designed loss at distinct levels of Feature Pyramid Networks (FPN), balancing the detection accuracy and efficiency. It is worth noting that CESC and AMM can be easily extended to various detectors, indicating that CEASC is generally applicable to existing state-of-the-art object detectors for accelerating object detection on drone imagery.

In this paper, we propose a novel plug-and-play detection head optimization approach to efficient object detection on drone images, namely global context-enhanced adaptive sparse convolution (CEASC). Concretely, we first develop a context-enhanced sparse convolution (CESC) to capture global information and enhance focal features, which consists of a residual structure with a context-enhanced group normalization (CE-GN) layer. Since CE-GN specifically preserves a set of holistic features and applies their statistics for normalization, it compensates the loss of context caused by sparse convolutions and stabilizes the distribution of foreground areas, thus bypassing the sharp drop on accuracy. We then propose an adaptive multi-layer masking (AMM) scheme, and it separately estimates an optimal mask ratio by minimizing an elaborately designed loss at distinct levels of feature pyramid networks (FPN), balancing the detection accuracy and efficiency. It is worth noting that CESC and AMM can be easily extended to various detectors, indicating that CEASC is generally applicable to existing state-of-the-art object detectors for acceleration on drone imagery.

%To overcome their limitations and address the two issues above \ie accurate selection of the concerned region and efficient utilization of the foreground context, we propose a novel plug-and-play detection head optimization approach for efficient object detection on drone images, namely global Context-Enhanced Adaptive Sparse Convolution (CEASC). Concretely, we first develop a Context-Enhanced Sparse Convolution (CESC) to extract global information and enhance focal feature, which adopts a residual structure with a Context-Enhanced Group Normalization (CE-GN) layer. Since CE-GN specifically maintains a set of global contextual features and applies their statistics for normalization, it compensates the loss of context information caused by sparse convolutions as well as stabilizing the distribution of the foreground areas, thus avoiding sharp drop of detection accuracy. In the mean time, we propose an Adaptive Multi-layer Masking (AMM) scheme. AMM separately estimates an optimal mask ratio by minimizing an elaborately designed loss at distinct levels of Feature Pyramid Networks (FPN), thus balancing the detection accuracy and efficiency. It is worth noting that CESC and AMM can be easily extended to various detectors, indicating that CEASC is generally applicable to existing state-of-the-art object detectors for accelerating object detection on drone imagery.

The contribution of our work lies in three-fold:

%1) We propose a novel detection head optimization approach based on sparse convolutions, \emph{i.e.} CEASC, for efficient object detection for UAVs.
1) We propose a novel detection head optimization approach based on sparse convolutions, \emph{i.e.} CEASC, to efficient object detection for UAVs.

%2) We develop a context-enhanced sparse convolution layer as well as an adaptive multi-layer masking scheme to optimize the mask ratio, delivering an optimal balance between the detection accuracy and efficiency.
2) We introduce a context-enhanced sparse convolution layer and an adaptive multi-layer masking scheme to optimize the mask ratio, delivering an optimal balance between the detection accuracy and efficiency.

%3) We extensively evaluate the proposed approach on two public benchmarks on drone imagery by integrating CEASC to various state-of-the-art detectors (\eg RetinaNet and GFL V1), significantly reducing their computational costs while maintaining competitive accuracies. 
3) We extensively evaluate the proposed approach on two major public benchmarks of drone imagery by integrating CEASC to various state-of-the-art detectors (\eg RetinaNet and GFL V1), significantly reducing their computational costs while maintaining competitive accuracies. 

% Describe our methods: using a light network to detect foreground and background area, and train the network with Gumbel-softmax trick. We achieve a foreground and background area dynamically division method by the loss function design.

% The significant and global information:  A sparse CNN is applied to the foreground area to get significant information. We use a light network to collect global information and use normalization method to generate significant and global information. 

%To address the aforementioned two issues, we propose a novel global Context-Enhanced Adaptive Sparse Convolutional (CEASC) network to serve as a plug-and-play network to improve efficiency of the baseline network with comparable accuracy. To improve the sparse network's utilization of background information, we propose a context-enhanced group normalization (CE-GroupNorm) layer to stabilize the distribution of the foreground area and reconstruct the data relationship of the pixels in each region with the help of global information while normalizing the features. And we also adopt a residual structure to fill background information for regions not computed by sparse convolution. An instances-based adaptive control of computation resource method is developed to control expected foreground area dynamically. For each image, the result of label assignment to the ground-truth of training data reflects the proportion of positive and negative samples and pixels located in the foreground or background area in the features extracted from the image. And we process the ground-truth data separately at different levels of the FPN to achieve precise control over the target foreground ratio for different scales in the image.

% Our method reduces the total GFLOPS by over 50\%, improves the inference speed by XXX\%. While the mAP increases from XXX to XXX on the baseline GFL. And other baselines are also tested to prove our method can be used universally.


%We evaluate our proposed method on two publicly available drone datasets \emph{i.e.} VisDrone and UAVDT as well as multiple state-of-the-art baselines \emph{e.g.} GFL v1, RetinaNet. In the popular drone image dataset VisDrone, Our method based on GFL baseline reduces GFLOPs by 70.7\% and improves the inference speed on GPU by 56.2\%. In contrast, our method only reduces mAP by 0.25 compared to baseline while similar results are obtained from experiments on other dataset and baselines, which demonstrates the efficiency and ease of use of our method.


% Contributions

% 1. Apply the quantitative method on the dense heads, use foreground and background area division to specially reduce the computing consumption of aerial small object detection

% 2. Use sparse CNN to get significant information and use normalization method to generate significant and global information.

% 3. We use a dynamic resource load calculation method to control the foreground and background area division.

%-------------------------------------------------------------------------

%\section{Related work}
\section{Related Work}
\subsection{General Object Detection}
% traditional object detection methods like two stages: Faster-RCNN, Mask-RCNN. One stage: GFL, FCOS, YOLOX

% Most traditional object detection methods can be divided into two-stage detectors or one-stage detectors according to their detection process. Two-stage detectors usually performs better than one-stage detectors as their second stage can calculate the category and position of the targets in more detail, but at the cost of greater computational consumption, such as R-CNN~\cite{RCNN2014}, Faster-RCNN~\cite{Faster-RCNN2015}, Mask RCNN~\cite{MaskRCNN2017}. On the contrary, one-stage detectors can achieve better computing resource trade-off. RetinaNet introduces FPN~\cite{FPN2017} to achieve multi-layer feature learning and proposes FocalLoss to deal with positive-negative instances imbalance problem. Some anchor-free detectors like FCOS~\cite{FCOS2019} and FSAF~\cite{FSAF2019} help network learn knowledge from ground truth without anchors. ATSS~\cite{ATSS2020} and GFL V1/V2~\cite{GFLv12020,GFLv22021} further improve detectors on training sample selection and loss function design.
%Most general object detection methods can be divided into anchor-based detectors and anchor-free detectors depending on whether they use preset sliding windows or anchors to locate object proposals. In anchor-based detectors, the multi-stage detectors, including R-CNN~\cite{RCNN2014}, Faster-RCNN~\cite{Faster-RCNN2015}, Mask RCNN~\cite{MaskRCNN2017}, generate the proposal regions through sliding windows or anchors, and subsequently classify as well as locate the target objects within the proposal regions. On the contrary, the classification and localization of objects can be conducted directly in the whole feature on the one-stage detectors such as RetinaNet~\cite{RetinaNet2017} and GFL V1/V2~\cite{GFLv12020,GFLv22021}, which treat the anchors as the final bounding box targets. As for the anchor-free detectors (\eg Centernet~\cite{centernet2019}, FCOS~\cite{FCOS2019} and FSAF~\cite{FSAF2019}), the anchors that requires heavy computation cost are replaced by the efficient alternatives such as the centerness constraints or the object heatmaps.

General object detection methods can be mainly divided into anchor-based detectors and anchor-free detectors depending on whether they use preset sliding windows or anchors to locate object proposals. In anchor-based detectors, the multi-stage detectors, including R-CNN~\cite{RCNN2014}, Faster-RCNN~\cite{Faster-RCNN2015}, Mask RCNN~\cite{MaskRCNN2017}, first generate proposal regions and subsequently classify and localize target objects within them. On the contrary, classification and localization of objects can be directly conducted in the whole feature on the one-stage detectors such as RetinaNet~\cite{RetinaNet2017} and GFL V1/V2~\cite{GFLv12020,GFLv22021}, which treat anchors as final bounding box targets. As for the anchor-free ones (\eg Centernet~\cite{centernet2019}, FCOS~\cite{FCOS2019} and FSAF~\cite{FSAF2019}), the anchors that incur heavy computational burden are replaced by efficient alternatives such as centerness constraints or object heatmaps. Although gains are consistently delivered, it is not so straightforward to adapt such methods to the case on UAVs.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{framework_v16.pdf}%{framework.png}
%\caption{Framework of CEASC. Given a base detector such as GFL V1, CEASC replaces the detection head by Context-Enhanced Sparse Convolution (CESC) in each FPN layer, via generating a mask feature $\mathbf{H}_{i}$ and a global feature $\mathbf{G}_{i}$ for context enhancement. The mask ratio of $\mathbf{H}_{i}$ is automatically optimize by the Adaptive Multi-layer Masking (AMM) scheme, promoting both the accuracy and efficiency.} % $\mathcal{L}_{norm}$ and $\mathcal{L}_{cost}$ will be used for optimization together with GFL's training loss.} 
\caption{Framework of CEASC. Given a base detector such as GFL V1, CEASC replaces the detection head by context-enhanced sparse convolution (CESC) in each FPN layer, via generating a mask feature $\mathbf{H}_{i}$ and a global feature $\mathbf{G}_{i}$ for context enhancement. The mask ratio of $\mathbf{H}_{i}$ is automatically optimized by the adaptive multi-layer masking (AMM) scheme, promoting both the accuracy and efficiency.} 
\label{fig:framework}
\vspace{0.1cm}
\end{figure*}
% small object detection methods: ClusDet, DMNet, QueryDet, Focus\&Detect. 
% They usually follow to a coarse-to-fine pipeline, which require several times of inference with high computing consumption
\subsection{Object Detection on Aerial Images}

%For drone image object detection, current studies usually follow a coarse-to-fine pipeline where a coarse detector is launched to locate the large-scale instances and sub-regions that contain densely distributed small ones and a fine detector applied to those regions to find instances of small sizes. ClusDet~\cite{ClusDet2019} employs this pipeline and proposes an effective scale estimation network(ScaleNet) for better fine detection. DMNet~\cite{DMNet2020} further optimal the region select algorithm by employing a density map method with connected possible blocks generation algorithm. UFPMP-Det~\cite{ufpmpdet2022} merges sub-regions generated by a coarse detector into a unified images and employ the Multi-Proxy Detection Network to improve the detection accuracy for tiny objects. Focus\&Detect~\cite{FocusAndDetect2022} uses Gaussian Mixture Model to generate focal regions and introduce the incomplete box suppression to solve the problem of overlapping focal areas.

%Despite of their high accuracy achieved on drone-based object detection, the coarse-to-fine methods need to perform inference on one image for multiple times, which are not efficient, limiting their applications on the resource-constrained UAV platforms.

For object detection on drone imagery, current studies usually follow a coarse-to-fine pipeline where a coarse detector is launched to locate large-scale instances and sub-regions that contain densely distributed small ones and a fine detector is further applied to those regions to find instances of small sizes. For example, ClusDet~\cite{ClusDet2019} employs a scale estimation network (ScaleNet) for better fine detection; DMNet~\cite{DMNet2020} optimizes region selection by conducting a density map guided connected crop generation; UFPMP-Det~\cite{ufpmpdet2022} merges sub-regions generated by a coarse detector into a unified image and designs the multi-proxy detection network to improve the detection accuracy of tiny objects; and Focus\&Detect~\cite{FocusAndDetect2022} makes use of the Gaussian mixture model to estimate focal regions and introduces incomplete box suppression to deal with overlapping focal areas. Despite of high accuracies achieved, these methods need to perform inference on one image for multiple times, which are not efficient, limiting their applications on the resource-constrained UAV platforms. 

%\subsection{Light-weight Models for Object Detection}
\subsection{Lightweight Models for Object Detection}

% MobileNet and ShuffleNet. 
% They only focos on the backbone, and usually lack of high resolution. While our method can obtain high resolution. 

%Along with the advance of the deep learning, the complexity of deep object detection models has increased sharply, incurring the heavy computational cost and slow inference speed. Several typical kinds of solutions are proposed in parallel for reducing the complexity and accelerating the inference, including neural architecture search~\cite{efficientnet2019,nasfcos2020}, model compression~\cite{networkslimming2017,groupfisherprune2021}, knowledge distillation~\cite{DistillationDetection2017,FGD2022} as well as the light-weight model~\cite{MobileNetV22018,thundernet2019}, which is the main focus of our work.

Along with the advancement of deep learning, the complexity of object detection models has sharply increased, incurring heavy computational cost and slow inference speed. Several typical solutions are proposed in parallel to reduce computations for acceleration, including neural architecture search~\cite{efficientnet2019, nasfcos2020}, network pruning~\cite{networkslimming2017, groupfisherprune2021}, knowledge distillation~\cite{DistillationDetection2017, FGD2022} and lightweight model design~\cite{MobileNetV22018, thundernet2019}. Among them, lightweight model design is in the lead for detection on UAVs for its good potential in speed-accuracy trade-off.


% Deep learning models designed for vision tasks are continuously enhanced recent years, which also makes computation costs increase heavily,
% A great deal of methods is proposed to make the model lightweight without losing too much accuracy. MobileNet series~\cite{MobileNetV12017,MobileNetV22018,MobileNetV32019} modify network's convolution to depthwise separable convolution to reduce computation costs and adopt linear bottleneck and inverted residuals for better accuracy. ShuffleNet~\cite{ShufflenetV12018} use pointwise group convolution to reduce model weight and propose channel shuffle method to improve network's information extraction capability and ShuffleNetV2~\cite{ShufflenetV22018} further optimizes model structure according to the performance on different computing devices in practical. YOLO series~\cite{yolox2021, yolov42020} develop darknet as backbone and design light-weight detect head for better efficiency on object detection while introducing a variety of improvements for continuous updates \emph{i.e.} CSP module~\cite{cspnet2020}, CIoU loss~\cite{ciou2020}, obtaining great result both in precision and speed.
% One branch of existing works on light-weight models focuses on introducing light-weight backbones to reduce the computational cost, where  the MobileNet~\cite{MobileNetV12017,MobileNetV22018,MobileNetV32019} and ShuffleNet~\cite{ShufflenetV12018,ShufflenetV22018} are the representatives, which employ the depth-wise separable convolution and group convolution, respectively. Another branch of works concentrates on designing light-weight detection head, including the methods based on YOLO series~\cite{yolox2021, yolov42020}, which adopt the darknet as the backbone by developing a variety of improvements such as the CSP module~\cite{c spnet2020} and CIoU loss~\cite{ciou2020}. \textcolor{red}{Most lightweight network can only optimize the speed of backbone for object detection detector, making it difficult to promote efficiency for current state-of-the-art detectors with heavy head. At the same time, the detection head optimized for traditional object detection cannot provide higher acceleration through drone images lower foreground proportion, needs further optimization.}

% Sparse CNN has also been introduced to 2D vision tasks to accelerate inference speed as not all regions in the image needs invovle in computation. ~\cite{DynamicConvolution2020} and ~\cite{SpatiallyAdaptiveInference2020} both introduce a mask network trained by Gumbel-Softmax trick to generate sample mask for every pixel and reduce the computational complexity of backbone. For object detector head, to strengthen multi-scale feature representation ability, ~\cite{DynamicHead2020} conducts a combination of FPN features from different scales by spatial gates. QueryDet~\cite{QueryDet2022} utilizes high-resolution images and $P_{2}$ features from FPN to improve accuracy on tiny object and adopts a cascade sparse query structure in the head. Comparing with our method, these methods do not efficiently process the information ignored by sparse convolution. Meanwhile, \textcolor{red}{most of} these methods need to control the mask ratio by hand, so they cannot cope with the challenging scenes in drone object detection.
%One branch of existing works on light-weight models focuses on introducing light-weight backbones to reduce the computational cost, where the MobileNet~\cite{MobileNetV12017,MobileNetV22018,MobileNetV32019} and ShuffleNet~\cite{ShufflenetV12018,ShufflenetV22018} are the representatives, which employ the depth-wise separable convolution and group convolution, respectively. Another branch of works concentrates on designing light-weight detection head, including the methods based on YOLO~\cite{yolox2021, yolov42020}, which adopt the darknet as the backbone by develop a variety of improvements such as the CSP module~\cite{cspnet2020} and CIoU loss~\cite{ciou2020}. 

% Some methods focus on introducing lightweight backbones, where MobileNet~\cite{MobileNetV12017,MobileNetV22018,MobileNetV32019} and ShuffleNet~\cite{ShufflenetV12018,ShufflenetV22018} are the representatives, which employ depth-wise separable convolutions and group convolutions, respectively. Some methods design lightweight detection heads, \emph{e.g.} the YOLO series~\cite{yolov62022, yolov72022}, which adopt Darknet as the backbone by developing a variety of improvements such as the CSP module~\cite{cspnet2020} and CIoU loss~\cite{ciou2020}. 

Some methods focus on lightweight backbones, where MobileNet~\cite{MobileNetV12017,MobileNetV22018,MobileNetV32019} and ShuffleNet~\cite{ShufflenetV12018,ShufflenetV22018} are the representatives, which employ depth-wise separable convolutions and group convolutions, respectively. Some methods design lightweight detection heads, \emph{e.g.} in the YOLO series, YOLO v6~\cite{yolov62022} presents an efficient decoupled head, while YOLO v7~\cite{yolov72022} plans re-parameterized convolutions.


%Recently, the Sparse CNN has emerged as a promising way in accelerating the inference by generating pixel-wise sample masks for the backbone. Some works \cite{DynamicHead2020,QueryDet2022} have also attempted to employ the sparse convolution on the detection head.  ~\cite{DynamicHead2020} conducts a pixel-level combination of FPN features from different scales via spatial gates to reduce the computation costs. QueryDet~\cite{QueryDet2022} performs on high-resolution images and utilizes the $P_{2}$ features from FPN to improve the accuracy on tiny object. It leverages a cascade sparse query structure trained by focal loss \cite{RetinaNet2017} to accelerate the inference speed. Nevertheless, these methods suffer from the missing global contexts, since the sparse convolutions often skip a large portion of features in order to accomplish the computational efficiency. Meanwhile, these methods usually use a fixed mask ratio, which fails to handle with the severe fluctuations in foreground rations on drone images, thus not being fully optimized for drone-based object detection. In contrast, our method employs a global feature as well as the CE-GroupNorm to compensate the missing contexts, and develops the AMM scheme, enabling the model to adaptively adjust the mask ratio. As a consequence, our method performs better in balancing the efficiency and accuracy, compared to existing sparse convolution based approaches. 

Sparse CNN has recently emerged as a promising way to accelerate inference by generating pixel-wise sample masks for convolutions. In particular, \cite{DynamicHead2020, QueryDet2022} have attempted to apply sparse convolutions to the detection head. ~\cite{DynamicHead2020} conducts a pixel-level combination of FPN features from different scales via spatial gates to reduce the computational cost. QueryDet~\cite{QueryDet2022} works on high-resolution images and utilizes the $P_{2}$ features from FPN to improve the accuracy on tiny objects, while a cascade sparse query structure is built and trained by the focal loss \cite{RetinaNet2017} for acceleration. Nevertheless, as these methods usually use a fixed mask ratio without capturing global context, they fail to handle severe fluctuations of foreground regions, leading to insufficiently optimized detection results on drone images. In contrast, our method adaptively adjusts the mask ratio with global feature captured to balance the efficiency and accuracy. 


%YOLO series~\cite{yolox2021, yolov42020} develop darknet as backbone and design light-weight detect head for better efficiency on object detection while introducing a variety of improvements for continuous updates \ie CSP module, CIoU loss, obtaining great result both in precision and speed. Most lightweight network can only optimize the speed of backbone for object detection detector, making it difficult to promote efficiency for current state-of-the-art detectors with heavy head. At the same time, the detection head optimized for traditional object detection cannot provide higher acceleration through drone images lower foreground proportion, needs further optimization.

%\textcolor{red}{Sparse CNN has also been introduced to 2D vision tasks to accelerate inference speed as not all regions in the image are equally important. ~\cite{DynamicConvolution2020} and ~\cite{SpatiallyAdaptiveInference2020} both introduce a mask network trained by Gumbel-Softmax trick to generate sample mask for every pixel and reduce the computational complexity of backbone. For object detector head, to strengthen multi-scale feature representation ability, ~\cite{DynamicHead2020} select a pixel-level combination of FPN features from different scales through spatial gates to reduce computation costs. QueryDet~\cite{QueryDet2022} introduces high-resolution images and $P_{2}$ features from FPN to improve accuracy on small object and adopts a cascade sparse query structure trained by focal loss function~\cite{RetinaNet2017} on the head to reduce computation costs. Comparing with our method, these methods do not efficiently process the information ignored by sparse convolution. Meanwhile, these methods need to control the mask ratio by hand, so they cannot cope with the challenging scenes in drone object detection.}

%\section{The Proposed Method}
\section{Method}

%In this section, we present the framework of the proposed Context-Enhanced Adaptive Sparse Convolutional (CEASC) network, and elaborate the details of the main components, including the Context-Enhanced Sparse Convolution and the Adaptive Multi-layer Masking scheme. Without loss of generality, we describe our method based on the GFL V1 base detector, which can be easily extended to other detectors by replacing the detection head with ours.  

%In this section, we describe the Context-Enhanced Adaptive Sparse Convolutional (CEASC) network.  

%As displayed in Fig.~\ref{fig:framework}, given a base detector, the proposed CEASC aims to optimize the detection heads of different layers in the Feature Pyramid Network (FPN), by developing a novel Context-Enhanced Sparse Convolution, which integrates the focal information with the global context by using a light-weight convolutional module as well as a CE-GroupNorm normalization. An Adaptive Multi-layer Masking (AMM) module is proposed to facilitate the model adaptively generating masks with an adequate mask ratio, thus reaching a better balance in accuracy and efficiency. 

As Fig.~\ref{fig:framework} shows, given a base detector, the entire CEASC network aims to optimize the detection head at different layers in FPN, by developing a context-enhanced sparse convolution (CESC), which integrates focal information with global context through a lightweight convolutional module as well as a context-enhanced group normalization (CE-GN) layer. An adaptive multi-layer masking (AMM) module is designed to enable the model adaptively generating masks with an adequate mask ratio, thus reaching a better balance in accuracy and efficiency. 

%The details of the above components are described in Sec.~\ref{sec:CESC} and Sec.~\ref{sec:amm}.

The details of the components aforementioned are described in Sec.~\ref{sec:CESC} and Sec.~\ref{sec:amm}.

\subsection{Context-Enhanced Sparse Convolution}\label{sec:CESC}
\subsubsection{Sparse Convolution}

%Most existing detectors on drone images utilize the dense detection head, convolving on the whole feature maps. Although fully exploring the visual information facilitates detecting tiny objects, the dense head requires much more computations, which is not applicable to the resource-constrained UAV platform. In the mean time, the foreground area only occupies a small part of a frame acquired by a drone as shown in Fig.~\ref{fig:challenge}, indicating that the dense head conducts a lot of computational operations on the background area, which contains much less useful information for object detection than the foreground area. This observation reveals the potential to accelerate the detection head by only computing on the foreground area. 

Most existing detectors on drone images work with dense detection heads, convolving on the whole feature maps. Although fully exploring visual clues facilitates detecting tiny objects, the dense head requires much more computations, which is not applicable to the resource-constrained UAV platform. In the mean time, the foreground area only occupies a small part of a frame acquired by a drone as shown in Fig.~\ref{fig:challenge}, indicating that the dense head conducts a lot of computational operations on background, which contains much less useful information for object detection. This observation reveals the potential to accelerate the detection head by only computing on the foreground area. 

%The recently proposed sparse convolution (SC) \cite{SECOND2018,SACT2017} learns to operate on foreground areas by employing a sparse mask, proving effective in speeding up the inference on a variety of vision tasks. Inspired by these works, we construct our method based on the sparse convolution.

Sparse convolution (SC) \cite{SECOND2018, SACT2017} have recently been proposed, which learn to operate on foreground areas by employing a sparse mask and prove effective in speeding up the inference phase on a variety of vision tasks. Inspired by them, we construct our network based on SC.

%, which consume unnecessary computing resources on drone images' background area. Although calculating the whole image will help discover some difficult targets and improve the robustness, it is more practical to focus on the foreground regions rather than the whole image. Based on this inspiration, we propose a light mask network to distinguish foreground and background areas, and Gumbel-Softmax trick is adopted to optimize the network. Then a sparse convolution will replace the dense convolution to generate foreground information.

Specifically, given a feature map $\mathbf{X}_{i} \in \mathbb{R}^{B \times C \times H \times W}$ from the $i$-th layer of FPN, SC adopts a mask network consisting of a shared kernel $\mathbf{W}_{mask} \in \mathbb{R}^{C \times 1 \times 3 \times 3}$, where $B$, $C$, $H$, $W$ refers to the batch size, channel size, height and width, respectively. Convolving on $\mathbf{X}_{i}$ based on $\mathbf{W}_{mask}$ generates a soft feature $\mathbf{S}_{i} \in \mathbb{R}^{B \times 1 \times H \times W}$, which is further turned to a mask matrix $\mathbf{H}_{i} \in \{0,1\}^{B \times 1 \times H \times W}$ by using the Gumbel-Softmax trick \cite{DynamicConvolution2020} formulated as below:
\begin{equation}
\mathbf{H}_{i} =
\begin{cases}
%\sigma\Bigl(\frac{\mathbf{S}_{i} + g_{1} - g_{2}}{\tau}\Bigr) > 0.5, & \text{ during training } \\
%\mathbf{S}_{i} > 0, &  \text{ during inference }
\sigma\Bigl(\frac{\mathbf{S}_{i} + g_{1} - g_{2}}{\tau}\Bigr) > 0.5, & \text{ For training } \\
\mathbf{S}_{i} > 0, &  \text{ For inference }
\end{cases}
\label{eq:hard-mask}
\end{equation}
%where $g_{1}, g_{2} \in \mathbb{R}^{B \times 1 \times H \times W}$ are two random gumbel noises, $\sigma$ refers to the sigmoid function, and $\tau$ is the corresponding temperature parameter for Gumbel-Softmax.
where $g_{1}, g_{2} \in \mathbb{R}^{B \times 1 \times H \times W}$ denote two random gumbel noises, $\sigma$ refers to the sigmoid function, and $\tau$ is the corresponding temperature parameter in Gumbel-Softmax.

%According to $\mathbf{H}_{i}$, only the areas with the mask value 1 will involve in  computation during inference, thus reducing the overall computational cost. The sparsity of $\mathbf{H}_{i}$ is controlled by a mask ratio $r\in [0,1]$, which is often set larger than 0.9 by hand in existing works. Since the base detector GFL V1 has a classification head and a regression head in the detection framework, we separately introduce a mask network for each head considering that they often focus on different areas. Each detection head in GFL V1 adopts four Convolution-GN-ReLU layers and a single convolution layer to generate the final prediction. We replace the original conventional convolution layers with the aforementioned sparse convolution. 

According to Eq. \eqref{eq:hard-mask}, only the area with the mask value 1 involves in convolutions during inference, thus reducing the overall computational cost. The sparsity of $\mathbf{H}_{i}$ is controlled by a mask ratio $r\in [0,1]$, which is often set larger than 0.9 by hand in existing studies. Since the base detector (here we take GFL V1 as an example) has a classification head and a regression head in the detection framework, we separately introduce a mask network for each head considering that they often focus on different areas. Each detection head adopts four Convolution-GN-ReLU layers and a single convolution layer to make prediction, where we replace the conventional convolution layers with the SC ones. 

% Current implementation of the sparse convolution is not fully optimized on CUDA, since they convert the convolution operation to a matrix multiplication operation by the `im2col' operator \cite{pytorch2019}. \textcolor{red}{As shown in Fig.~\ref{fig:framework}, we modify this process by develop a sparse `im2col' implementation, which converts the active pixels into column vectors for subsequent matrix multiplications.} 

% \begin{algorithm}[t]
%     \caption{Sparse im2col method}
%     \hspace*{0.02in} {\bf Input:} 
%     Input $\mathcal{X}$, Mask $\mathcal{M}$; Kernel $k$, Stride $s$, Padding $p$;\\
%     \hspace*{0.02in} {\bf Output:} 
%     Tensors columns $col$;
%     \begin{algorithmic}[1]
%     \State $\mathcal{MW}$,\ $\mathcal{MH}$\ =\ nonzero($\mathcal{M}$)
%     \Statex\ //\ to\ get\ the\ coordinates\ of\ the\ active\ mask areas
%     \State $n$\ =\ length\ of\ $\mathcal{MH}$;\ $c, h, w$\ =\ size\ of\ $\mathcal{X}$
    
%     \For {$idx=0$\ to\ $n$\ $\times$\ $c$}
%         \State $I$\ =\ $idx$\ \%\ $n$
%         \State $\mathcal{W}$,\ $\mathcal{H}$\ =\ $\mathcal{MW}$[$I$]\ $\times$\ $s$\ -\ $p$,\ $\mathcal{MH}$[$I$]\ $\times$\ $s$\ -\ $p$
%         \State $C$\ =\ $idx$\ /\ n\ $\times$\ $k^{2}$
%         \For {$i=0$ to kernel}
%             \For {$j=0$ to kernel}
%                 \State $W_{cur}$, \ $H_{cur}$ = $\mathcal{W}$ + j, $\mathcal{H}$ + i    
%                 \If{0\ $\leq$\ $W_{cur}$\ $\le$\ $w$\ and\ 0\ $\leq$\ $H_{cur}$\ $\le$\ $h$}
%                 \State $col[C][I]\ =\ \mathcal{X}[idx\ /\ n][i][j]$
%                 \EndIf
%                 \State $C$\ =\ $C$\ +\ 1
%             \EndFor
%         \EndFor
%     \EndFor
%     \State \Return columns
%     \end{algorithmic}
% \end{algorithm}

\subsubsection{Context Enhancement}\label{sec:CE}

% Introduce the 1x1 convolution network to generate global information and how to fuse focal and global information by normalization.

% Introduce the training method, use the perfect information generated by dense convolution to guide the study of the fusion process.

%As claimed in \cite{FGD2022}, the contextual cues (\eg background surrounding the objects) facilitates object detection. However, the sparse convolution tends to only perform on foreground areas, thus abandoning a large portion of contextual information, which undermines the overall accuracy, especially in the presence of tiny objects prevailing in drone images. \cite{SpatiallyAdaptiveInference2020} attempts to recover the surrounding context by interpolation, which however is unreliable as the focal and background areas exhibit large discrepancy. To tackle with this problem, we propose a light-weight context-enhanced sparse convolution module by fusing the focal and global contextual information, which boosts the stability of subsequent computation in the meantime. As in Fig.~\ref{fig:framework}, we adopt a point-wise convolution on the feature map $\mathbf{X}_{i}$, generating the global contextual feature $\mathbf{G}_{i}$. Since only a small portion of elements in $\mathbf{X}_{i}$ are processed by the sparse convolution, $\mathbf{G}_{i}$ tends to become stable after multiple rounds of sparse convolution without taking much extra computational cost.

As claimed in \cite{FGD2022}, contextual clues (\eg background surrounding target objects) benefit object detection; however, SC performs convolutions only on foreground and abandons background with useful information, which probably undermines the overall accuracy, especially in the presence of tiny objects prevailing in drone images. To tackle with this problem, \cite{SpatiallyAdaptiveInference2020} attempts to recover surrounding context by interpolation, but it is not reliable as the focal and background areas exhibit large discrepancy. In this work, we propose a lightweight CESC module, jointly making use of focal information and global context for enhancement and simultaneously boosting the stability of subsequent computations. As displayed in Fig.~\ref{fig:framework}, we apply a point-wise convolution to the feature map $\mathbf{X}_{i}$, generating the global contextual feature $\mathbf{G}_{i}$. Since only a few elements in $\mathbf{X}_{i}$ are processed by SC, $\mathbf{G}_{i}$ tends to become stable after multiple rounds of SC without taking much extra computational cost.

%As an important part of the sparse convolution, we attempt to embed the global contextual information $\mathbf{G}_{i}$ into the SparseConvolution-GN-ReLU layers, which takes the feature map $\mathbf{X}_{i,j}$, the mask $\mathbf{H}_{i}$ and the global feature $\mathbf{G}_{i}$ as inputs, where $j$ indicates the $j$-th SparseConvolution-GN-ReLU layer. Instead of using the activated elements to compute the statistics for group normalization as in conventional sparse convolutions, we adopt the  \textcolor{red}{mean value and standard deviation} of the global feature $\mathbf{G}_{i}$ for normalization, thus compensating the missing context. Supposing that $\mathbf{L}_{i,j}$ is the output feature map after applying sparse convolution on $\mathbf{X}_{i,j}$, the context-enhanced feature $\mathbf{F}_{i,j}$ is obtained by the Context-Enhanced Group Normalization (CE-GN) as below

As an important part of SC, we embed the global contextual information $\mathbf{G}_{i}$ into the SparseConvolution-GN-ReLU layers, which takes the feature map $\mathbf{X}_{i,j}$, the mask $\mathbf{H}_{i}$, and the global feature $\mathbf{G}_{i}$ as inputs, where $j$ indicates the $j$-th SparseConvolution-GN-ReLU layer. Instead of using the activated elements to compute the statistics for group normalization as in conventional SC, we adopt the mean value and standard deviation of $\mathbf{G}_{i}$ for normalization, aiming to compensate the missing context. Supposing that $\mathbf{L}_{i,j}$ is the output feature map after applying SC on $\mathbf{X}_{i,j}$, the context-enhanced feature $\mathbf{F}_{i,j}$ is obtained by CE-GN as below

\begin{table*}[!t]
    \centering
    \normalsize
    % \small
    %\resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c|ccc|cccc|cc}
    \hline
    %\multicolumn{2}{c|}{Method} & mAP & AP_{50} & AP_{75} & AR_{1} & AR_{10} & AR_{100} & AR_{500} & GFLOPs & FPS \\ \hline
    Base Detector & Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & $\text{AR}_{1}$ & $\text{AR}_{10}$ & $\text{AR}_{100}$ & $\text{AR}_{500}$ & GFLOPs & FPS \\ \hline
    \multirow{2}*{GFL V1~\cite{GFLv12020}}        & Baseline & 28.4 & 50.0 & 27.8 & 0.62 & 6.36 & 35.6 & 44.9 & 524.95 & 13.46 \\
                                 & \textbf{Ours (CEASC)}  & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{0.65} & \textbf{6.56} & 35.6 & \textbf{45.0} & \textbf{150.18} & \textbf{21.55} \\ \hline
    \multirow{2}*{RetinaNet~\cite{RetinaNet2017}}     & Baseline & \textbf{21.8} & 39.3 & \textbf{21.1} & 0.54 & 5.82 & \textbf{29.1} & \textbf{35.3} & 529.81 & 13.41 \\
                                 & \textbf{Ours (CEASC)}  & 21.6 & \textbf{39.6} & 20.6 & \textbf{0.59} & 5.82 & 28.9 & 34.7 & \textbf{157.41} & \textbf{20.10} \\ \hline
     \multirow{2}*{Faster-RCNN~\cite{Faster-RCNN2015}}  & Baseline & \textbf{24.8} & \textbf{43.6} & \textbf{25.0} & 0.64 & \textbf{5.97} & \textbf{33.0} & 34.9 & 322.25 & 18.17 \\
                                 & \textbf{Ours (CEASC)}  & 24.6 & 43.4 & 24.7 & 0.64 & 5.91 & 32.8 & \textbf{35.1} & \textbf{132.91} & \textbf{21.71} \\ \hline
     \multirow{2}*{FSAF~\cite{FSAF2019}}         & Baseline & \textbf{26.3} & \textbf{50.3} & \textbf{23.7} & 0.53 & 5.25 & \textbf{32.5} & \textbf{43.5} & 518.25 & 14.06 \\
                                 & \textbf{Ours (CEASC)}  & 25.0 & 48.9 & 22.0 & \textbf{0.56} & \textbf{5.65} & 31.1 & 41.5 & \textbf{153.92} & \textbf{19.43} \\ \hline
    
    \end{tabular}%}
    \caption{Comparison of AP/AR (\%) and GFLOPs/FPS on VisDrone by using our approach with various base detectors.}
    \vspace{0.1cm}
    \label{tab:sota_diff_basedet}
\end{table*}

\begin{equation}
\mathbf{F}_{i,j} = w \times \frac{\mathbf{L}_{i,j} - mean[\mathbf{G}_{i}]}{{std}[\mathbf{G}_{i}]} + b
\label{eq:ce-gn}
\end{equation}
%where $mean[\mathbf{G}_{i}]$ and  $std[\mathbf{G}_{i}]$ are the mean and \textcolor{red}{standard deviation} of $\mathbf{G}_{i}$, respectively. $w$ and $b$ are learnable parameters.
%where $mean[\cdot]$ and  $std[\cdot]$ indicate the mean and \textcolor{red}{standard deviation}, respectively. $w$ and $b$ are learnable parameters.
where $mean[\cdot]$ and $std[\cdot]$ denote the mean and standard deviation, respectively, and $w$ and $b$ are learnable parameters.


%In order to further mitigate the information loss caused by sparse convolution and make the training process more stable, we additionally maintain the normal dense convolution besides the sparse convolution during training, generating a feature map $\mathbf{C}_{i,j}$ \textcolor{red}{convolved} on the full input feature map. We employ $\mathbf{C}_{i,j}$ to enhance the sparse feature map $\mathbf{F}_{i,j}$ by optimizing the following MSE loss 
To further mitigate the information loss in SC and make the training process more stable, we additionally maintain the normal dense convolution besides the sparse one during training, generating a feature map $\mathbf{C}_{i,j}$ convolved on the full input feature map. We then employ $\mathbf{C}_{i,j}$ to enhance the sparse feature map $\mathbf{F}_{i,j}$ by optimizing the MSE loss as: 
\begin{equation}
\mathcal{L}_{norm} = \frac{1}{4L}\sum_{i=1}^{L}\sum_{j=1}^{4}\|\mathbf{C}_{i,j} \times \mathbf{H}_{i}\ -\ \mathbf{F}_{i,j}\|^{2},\label{eq:norm_loss}
\end{equation}
where $L$ is the amount of layers in FPN.

%Finally, we adopt a residual structure before the activation layer by adding $\mathbf{G}_{i}$ to $\mathbf{F}_{i,j}$, \ie $\mathbf{F}_{i,j}:=\mathbf{F}_{i,j}+\mathbf{G}_{i}$, which further preserves the contextual information. The complete structure of the context-enhanced sparse convolution as well as the CE-GN layer are displayed in Fig.~\ref{fig:framework}.

We finally adopt a residual structure before the activation layer by adding $\mathbf{G}_{i}$ to $\mathbf{F}_{i,j}$, \ie $\mathbf{F}_{i,j}:=\mathbf{F}_{i,j}+\mathbf{G}_{i}$, which strengthens context preservation. The complete architecture of the CESC module and the CE-GN layer are displayed in Fig.~\ref{fig:framework}.


\subsection{Adaptive Multi-layer Masking}
\label{sec:amm}
% To avoid a sparse convolution degenerates into a dense convolution, we need a Complexity Loss Function to restrict the networks calculated quantity

% We design a dynamic function to measure the complexity of the network based on the number ratio of positive and negative samples or the area ratio of positive and negative samples.

%Without extra constraints, the sparse detector tends to generate masks with large activation ratio (or small mask ratio) for higher accuracy, thus increasing the overall computational cost. To deal with this issue, most existing works attempt to use a fixed activation ratio. However, as the foregrounds of aerial images exhibit severe fluctuations, a fixed ratio is prone to either incur significant increase in computation or decrease in accuracy due to insufficient coverage over the foreground areas. To handle the dilemma of balancing accuracy and efficiency, we propose the Adaptive Multi-layer Masking (AMM) scheme to adaptively control the activation ratio (or reversely the mask ratio). 

Without any extra constraint, the sparse detector tends to generate the mask with a large activation ratio (or a small mask ratio) for a higher accuracy, thus increasing the overall computational cost. To deal with this issue, most existing attempts use a fixed activation ratio. However, as the foreground of aerial images exhibits severe fluctuations, a fixed ratio is prone to incur either significant increase in computation or decrease in accuracy due to insufficient coverage over foreground areas. For the trade-off between accuracy and efficiency, we propose the AMM scheme to adaptively control the activation ratio (or reversely the mask ratio). 

%which act  converge to the dense convolutional network to achieve higher accuracy. As the foreground ratio of aerial images have more pronounced fluctuations than conventional images, previous methods which set the target mask rate of sparse networks to a fixed value will cause images with different foreground proportions cannot obtain appropriate foreground information in training, thus affecting the training accuracy. Meanwhile, fixing the target computational complexity will also lead to the difficulty in achieving the best trade-off between accuracy and speed in inference. Therefore, we propose a dynamic computation resource control method based on the object detection model's positive and negative instances.

%Specifically, AMM firstly estimates an optimal mask ratio based on the ground-truth labels. By leveraging the label assignment technique, for the $i$-th FPN layer, we obtain the ground-truth classification results $\mathcal{C}_{i} \in \mathbb{R}^{h_{i} \times w_{i} \times c}$, where $c$ represents the number of categories including the background, $h_{i}$ and $w_{i}$ indicate the height and width of the feature map, respectively. The optimal activation ratio $\mathcal{P}_{i}$ in the $i$-th FPN layer is estimated as

Specifically, AMM firstly estimates an optimal mask ratio based on the ground-truth label. By leveraging the label assignment technique, for the $i$-th FPN layer, we obtain the ground-truth classification results $\mathcal{C}_{i} \in \mathbb{R}^{h_{i} \times w_{i} \times c}$, where $c$ represents the number of categories including the background; $h_{i}$ and $w_{i}$ indicate the height and width of the feature map, respectively. The optimal activation ratio $\mathcal{P}_{i}$ in the $i$-th FPN layer is estimated as

% \begin{center}
% \begin{table*}[t]
%     \centering
%     \label{uncertain}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|c|c|ccc|ccc}
%     \hline
%     \multicolumn{2}{c|}{Method} & Backbone & mAP & AP_{50} & AP_{75} & GFLOPs & GPU-FPS & CPU-FPS \\ \hline
%     \multirow{3}*{RetinaNet} & Baseline  & ResNet50 & 20.2 & \textbf{36.9} & 19.5 & 586.77 & 10.27 & 10 \\
%      & QueryDet                          & ResNet50 & 19.6 & 35.7 & 19.0 & - & 10.65 & 10 \\
%      & QueryDet-CSQ                      & ResNet50 & 19.3 & 35.0 & 18.9 & - & 11.71 & 10 \\
%      & Ours                             & ResNet50 & \textbf{20.8} & 35.0 & \textbf{27.7} & \textbf{201.96} & \textbf{14.27} & 10 \\ \hline
%     \multirow{4}*{GFL V1} & Baseline     & ResNet18 & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 & 10 \\
%      & Baseline                          & MobileNet V2 & 28.5 & 50.2 & 28.1 & 491.47 & 13.63 & 10 \\ 
%      & Baseline                          & ShuffleNet V2 & 26.2 & 46.6 & 25.7 & 488.94 & 13.92 & 10 \\  
%      & Ours                              & ResNet18 & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} & 10 \\ \hline
%     \end{tabular}}
%     \caption{Comparison of different light-weight models and sparse-convolution based methods in accuracy (AP) and efficiency (GFLOPs, GPU-FPS, CPU-FPS) on VisDrone. We removed QueryDet's precision structures to verify performance on lightweight scenario and remove data augmentation on our method for equal comparison.}
% \end{table*}
% \end{center}

\begin{equation}
\mathcal{P}_{i} = \frac{Pos(\mathcal{C}_{i})}{{Numel(\mathcal{C}_{i})}},
\label{eq:p}
\end{equation}
%where $Pos(\mathcal{C}_{i})$ and ${Numel(\mathcal{C}_{i})}$ indicate the number of pixels belonging to the positive (foreground) instances, and the number of all pixels, respectively. 
where $Pos(\mathcal{C}_{i})$ and ${Numel(\mathcal{C}_{i})}$ indicate the number of pixels belonging to the positive (foreground) instances and that of all pixels, respectively. 

To guide the network adaptively generating a mask with an adequate mask ratio, we employ the following loss  
\begin{equation}
\mathcal{L}_{amm} = \frac{1}{L}\sum_{i}\Bigl(\frac{Pos(\mathbf{H}_{i})}{Numel(\mathbf{H}_{i})} - \mathcal{P}_{i}\Bigr)^{2},
\label{eq:loss_cost}
\end{equation}
%where $\frac{Pos(\mathbf{H}_{i})}{Numel(\mathbf{H}_{i})}$ indicates the activation ratio of the mask $\mathbf{H}_{i}$. By minimizing $\mathcal{L}_{amm}$, the mask $\mathbf{H}_{i}$ is forced to abide by the same activate ratio as the ground-truth foreground ratio $\mathcal{P}_{i}$, thus facilitating generating adequate mask ratios.
where $\frac{Pos(\mathbf{H}_{i})}{Numel(\mathbf{H}_{i})}$ indicates the activation ratio of the mask $\mathbf{H}_{i}$. By minimizing $\mathcal{L}_{amm}$, $\mathbf{H}_{i}$ is forced to abide by the same activation ratio as the ground-truth foreground ratio $\mathcal{P}_{i}$, thus facilitating the generation of adequate mask ratios.

% Finally, by adding the conventional training loss $\mathcal{L}_{gfl}$ of GFL V1, we formulate the overall training loss as follows:
By adding the conventional detection loss $\mathcal{L}_{det}$, we formulate the overall training loss as follows:
\begin{equation}
\mathcal{L} = \mathcal{L}_{det} + \alpha \times \mathcal{L}_{norm} + \beta \times \mathcal{L}_{amm}\label{eq:overall_loss},
\end{equation}
where $\alpha$, $\beta$ are hyper-parameters balancing the importance of $\mathcal{L}_{norm}$ and $\mathcal{L}_{amm}$.


%\section{Experimental Results and Analysis}
\section{Experiments}
%In this section, we evaluate the effectiveness of the proposed CEASC approach by combining with distinct prevailing detectors, and comparing to the state-of-the-art light-weight methods, as well as extensive ablation studies.
%In this section, we evaluate the effectiveness of CEASC by comparing to the state-of-the-art light-weight approaches, as well as extensive ablation studies.
We evaluate the effectiveness of CEASC by comparing it to the state-of-the-art lightweight approaches and conducting comprehensive ablation studies.


%\subsection{Datasets and Evaluation Metrics}
\subsection{Datasets and Metrics}

%We conduct experiments on two widely used benchmarks for drone-based object detection, \ie VisDrone~\cite{VisDrone2018} and UAVDT~\cite{UAVDT2018}. VisDrone includes 7019 high-resolution (2000$\times$1500) aerial images from 10 categories. Following previous works \cite{ClusDet2019,QueryDet2022}, we select 6471 images for training and 548 images for testing. UAVDT contains 23258 training images and 15069 test images with a 1024$\times$540 resolution from three classes. 

We adopt two major benchmarks for evaluation in drone-based object detection, \ie VisDrone~\cite{VisDrone2018} and UAVDT~\cite{UAVDT2018}. VisDrone consists of 7,019 high-resolution (2,000$\times$1,500) aerial images belonging to 10 categories. Following previous work \cite{ClusDet2019, QueryDet2022}, we use 6,471 images for training and 548 images for testing. UAVDT contains 23,258 training images and 15,069 testing images with a resolution of 1,024$\times$540 from 3 classes. 

%We report the mean Average Precision (mAP), Average Precision (AP) and Average Recall (AR) as the evaluate metrics on accuracy, as well as GFLOPs and FPS as the evaluation metrics on efficiency. 

We employ the mean Average Precision (mAP), Average Precision (AP) and Average Recall (AR) as the evaluation metrics on accuracy, as well as GFLOPs and FPS as the ones on efficiency.

\subsection{Implementation Details}
%We implement our method based on PyTorch~\cite{pytorch2019} and MMDetection~\cite{mmdetection2019}. On VisDrone, all models are trained for 15 epochs with the SGD optimizer. The learning rate is initialized as 0.01 with a linear warm-up and decreased by 10 times after 11 and 14 epochs. On UAVDT, we train models for 6 epochs with an initial learning rate 0.01, decreased by 10 times after 4 and 5 epochs. Th trade-off hyper-parameters $\alpha$ and $\beta$ in Eq. \eqref{eq:overall_loss} are set to 1 and 10, respectively, and the temperature parameter $\tau$ for Gumbel Softmax is fixed as 1. We use GFL v1 as the base detector and ResNet18 as the backbone with 512 feature channels by default. The input image sizes are set to 1333$\times$800 and 1024$\times$540 on VisDrone and UAVDT, respectively. All experiments in this paper are conducted on two Nvidia RTX 2080Ti GPUs, except that the inference speed is tested on one RTX 2080Ti GPU. 

We implement our network based on PyTorch~\cite{pytorch2019} and MMDetection~\cite{mmdetection2019}. On VisDrone, all models are trained for 15 epochs with the SGD optimizer, and the learning rate is initialized as 0.01 with a linear warm-up and decreased by 10 times after 11 and 14 epochs. On UAVDT, we train models for 6 epochs with an initial learning rate at 0.01, decreased by 10 times after 4 and 5 epochs. The trade-off hyper-parameters $\alpha$ and $\beta$ in Eq. \eqref{eq:overall_loss} are set to 1 and 10, respectively, and the temperature parameter $\tau$ in Gumbel Softmax is fixed as 1. We make use of GFL V1 as the base detector and ResNet18 as the backbone with 512 feature channels by default. The input image sizes are set to 1,333$\times$800 and 1,024$\times$540 on VisDrone and UAVDT, respectively. All experiments are conducted on two NVIDIA RTX 2080Ti GPUs, except that the inference speed is test on a single RTX 2080Ti GPU. 

%\begin{table}[!t]
%    \centering
%    \label{uncertain}
%    \resizebox{\linewidth}{!}{
%    \begin{tabular}{c|ccc|ccc|cc}
%    \hline
%    Method & mAP & AP_{50} & AP_{75} & AP_{S} & AP_{M} & AP_{L} & GFLOPs & FPS \\ 
%    \hline
%    Baseline & 16.9 & 29.5 & \textbf{17.9} & \textbf{12.0} & 27.5 & \textbf{31.5} & 271.66 & 20.49 \\
%    Ours  & \textbf{17.1} & \textbf{30.9} & 17.8 & 11.7 & \textbf{28.7} & 20.0 & \textbf{64.12} & \textbf{28.47} \\ \hline
%    \end{tabular}}
%    \caption{Comparison between GFL v1 baseline method and our methods in accuracy (AP) and efficiency (GFLOPs, FPS) on UAVDT dataset.}
%\end{table}

%\subsection{Effectiveness of the Proposed Approach using Distinct Detectors}
\subsection{Evaluation on Different Detectors}
\label{sec:performance-sota-detector}

%It is worth noting that our CEASC approach is plug-and-play. To validate its effect in a wide range of base detectors, we report the performance of our method by combining with four prevailing base detectors: GFL V1 \cite{GFLv12020}, RetinaNet \cite{RetinaNet2017}, Faster-RCNN \cite{Faster-RCNN2015} and FSAF \cite{FSAF2019}. As shown in Table \ref{tab:sota_diff_basedet}, by integrating CEASC, the GFLOPs of all base detectors are reduced by at least 60\%, and the FPS is promoted by 20\%$\sim$60\% with slight fluctuations in mAP, indicating the effectiveness and generalizability of our method in accelerating detectors without sacrificing their accuracy. 

It is worth noting that the proposed CEASC network is plug-and-play. To validate its effect in a wide range of base detectors, we report the performance by combining CEASC with four prevailing base detectors: GFL V1 \cite{GFLv12020}, RetinaNet \cite{RetinaNet2017}, Faster-RCNN \cite{Faster-RCNN2015} and FSAF \cite{FSAF2019}. As shown in Table \ref{tab:sota_diff_basedet}, by integrating CEASC, the GFLOPs of all the base detectors are reduced by at least 60\%, and the FPS is promoted by 20\%$\sim$60\% with slight fluctuations in mAP, indicating its effectiveness and generalizability in accelerating detectors without sacrificing their accuracies. 


\begin{table}[!t]
    \centering
        
    \normalsize
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccc|cc}
    \hline
    CESC & AMM & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
      &  & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
    \checkmark & & 28.6 & 50.6 & 28.2 & 158.23 & 19.26 \\
    %   & \checkmark &  &  & & & \\
    \checkmark  & \checkmark & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55}\\
     \hline
    \end{tabular}
    }
    %\caption{Effect of CESC and AMM on the proposed method on VisDrone with GFL V1.}
    \caption{Ablation on CESC and AMM with GFL V1 as the base detector on VisDrone.}
    \vspace{0.1cm}
    \label{tab:ab_study}
\end{table}

% \begin{figure}{\linewidth}
\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{ce-gn_v6.pdf}
%\caption{Visualization on correlation between the features generated by dense convolutions and those by sparse convolutions with distinct normalizations on VisDrone, (a) and (b) use CE-GN and GN, respectively.}
\caption{Visualization on correlation between features generated by dense convolutions and those by sparse convolutions using distinct normalization schemes on VisDrone, (a) and (b) use CE-GN and GN on sparse convolutions, respectively.} 
% $\mathcal{L}_{norm}$ and $\mathcal{L}_{cost}$ will be used for optimization together with GFL's training loss.} 
\label{fig:ce-gn}
\vspace{0.1cm}
\end{figure}



\subsection{Ablation Study}\label{sec:ab_study}
%In this section, we evaluate the effect of the main components of CEASC. Without loss of generality, we adopt GFL V1 as the base detector in all ablation studies.  
We validate the main components of CEASC, where we also adopt GFL V1 as the base detector in all the ablation studies.  

%As reported in Table \ref{tab:ab_study}, by employing the CESC component, the base detector GFL V1 saves about 70\% of GFLOPs, and runs 1.43 times faster without drop in accuracy, since the sparse convolution reduces the complexity and the CE-GN layer together with the residual structure compensates the loss of contextual information. By adopting the dynamic mask ratio to obtain a compact foreground coverage, the proposed AMM component further increases the accuracy and accelerates the inference speed by 11.9\% as well as saving 5.1\% of the GFLOPs. Note that the training process of GFL V1 becomes extremely unstable when directly applying the sparse convolution without CESC. We therefore can not provide the result by individually evaluating AMM based on GFL V1. 

\subsubsection{On CESC and AMM} 

As Table \ref{tab:ab_study} reports, by employing the CESC component, the base detector saves about 70\% of GFLOPs and runs 1.43 times faster without any drop in accuracy, since SC reduces the complexity and the CE-GN layer together with the residual structure compensates the loss of context. By adopting the dynamic mask ratio to obtain a compact foreground coverage, the AMM component further increases the accuracy and accelerates the inference speed by 11.9\% while saving 5.1\% of GFLOPs. Note that the training process of GFL V1 becomes extremely unstable when directly applying SC without CESC, and we thus do not provide the result by individually evaluating AMM on GFL V1. 

\begin{table}[!t]
    \centering
        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccc|ccc|cc}
    \hline
     SC & Res. & CE-GN & $\mathcal{L}_{norm}$ & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    & &  &  & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
%     & \checkmark & \checkmark & 28.4 & 50.5 & 28.0 & 150.58 & 21.32 \\
    \checkmark & \checkmark &  &  & 26.1 & 47.2 & 25.3 & 151.66 & 21.51 \\
    \checkmark & \checkmark & \checkmark &  & 28.5 & 50.5 & 28.3 & 155.90 & 19.91 \\
    \checkmark & \checkmark & \checkmark & \checkmark & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\ \hline
    \end{tabular}}
    %\caption{Ablation study of the Context-Enhanced Sparse Convolution (CESC) on VisDrone based on GFL V1.}
    \caption{Ablation on detailed designs in CESC with GFL V1 on VisDrone.}
    % \vspace{-0.1cm}
    \label{tab:ab_cesc}
\end{table}

\begin{table}[!t]
    \centering
        
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    Dense Conv. & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
%     & \checkmark & \checkmark & 28.4 & 50.5 & 28.0 & 150.58 & 21.32 \\
    w/o Normalization & 26.1 & 47.2 & 25.3 & 151.66 & 21.51 \\
    GN~\cite{GN2018} & 28.0 & 49.9 & 27.7 & 154.49 & 18.82 \\
    BN~\cite{BatchNorm2015} & 26.1 & 47.0 & 25.4 & 150.81 & 19.55 \\
    IN~\cite{InstanceNorm2016} & 27.9 & 49.7 & 27.6 & 160.91 & 19.30 \\
    CE-GN (\textbf{Ours}) & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\ \hline
    \end{tabular}}
    %\caption{Ablation study of the Context-Enhanced GroupNorm (CE-GN) on VisDrone based on GFL V1.}
    \caption{Ablation on CE-GN with GFL V1 on VisDrone.}
    % \vspace{-0.1cm}
    \label{tab:ab_cegn}
\end{table}

\begin{table}[!t]
    \centering
       
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
%     & \checkmark & \checkmark & 28.4 & 50.5 & 28.0 & 150.58 & 21.32 \\
    $3\times3$ convolution & 28.5 & 50.1 & 28.1 & 262.38 & 17.12 \\
    GhostModule~\cite{ghostnet2020} & 28.3 & 50.1 & 27.8 & 194.66 & 19.22 \\
    CBAM~\cite{cbam2018} & 28.4 & 50.3 & 27.8 & \textbf{148.08} & 16.20 \\
    Criss-Cross Attn.~\cite{ccnet2019} & 28.4 & 50.3 & 27.8 & 159.27 & 15.40 \\
    Point-wise (\textbf{Ours}) & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & 150.18 & \textbf{21.55}\\ \hline
    \end{tabular}}
    %\vspace{-0.2cm}
     %\caption{Comparison of distinct methods for encoding the global contextual information on VisDrone based on GFL V1.}
     \caption{Comparison of distinct methods to encode global context with GFL V1 on VisDrone.}
    \label{tab:ab_global_info}
\end{table}

\begin{figure*}[!t]
\centering
\includegraphics[width=1\linewidth]{vision_2.pdf}
\caption{Visualization of the dynamic masks estimated by AMM for different layers (from `P3' to `P7') in FPN of GFL V1. Highlighted areas are activated for computation.}
\label{fig:vis}
\vspace{0.1cm}
\end{figure*}

% \begin{table}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|ccc|cc}
%     \hline
%     Method & mAP & AP_{50} & AP_{75} & GFLOPs & FPS \\ \hline
% %     & \checkmark & \checkmark & 28.4 & 50.5 & 28.0 & 150.58 & 21.32 \\
%     with P3 & 26.9 & 48.6 & 26.3 & \textbf{143.03} & \textbf{27.78} \\
%     with P3-P4 & 28.5 & 50.5 & 28.1 & 149.09 & 24.60 \\
%     with P3-P5 & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & 150.01 & 21.79 \\
%     \textbf{Ours} (with P3-P7) & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & 150.18 & 21.55\\ \hline
%     \end{tabular}}
%     %\caption{Ablation of FPN on VisDrone with GFL-V1.}
%     \caption{Ablation on FPN with GFL-V1 on VisDrone.}
%     % \caption{Ablation of FPN on VisDrone with our method(CEASC).}
%     %\caption{Ablation study of FPN on VisDrone with GFL-V1.}
%     % \vspace{-0.1cm}
%     \label{tab:ab_fpn}
% \end{table}
\begin{table}[!t]
    \centering
    \small  
    %\resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    Global & 28.4 & 50.2 & 28.1 & 162.53 & 19.84 \\
    Layer-wise & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\ \hline
    \end{tabular}%}
    %\caption{Comparison of different ways for estimating the mask ratio by AMM on VisDrone.}
    \caption{Comparison of estimating the mask ratio in different ways by AMM on VisDrone.}
    \label{tab:layerwise}
    % \vspace{-0.1cm}
\end{table}

\begin{figure}[!tpb]
\centering
% \subfigure[On VisDrone]{
% \begin{minipage}[b]{\linewidth}
% \centering
% \includegraphics[width=\linewidth]{result_11_A.pdf}
% \end{minipage}
% % \caption{VisDrone}
% }
% \subfigure[On UAVDT]{
% \begin{minipage}[b]{\linewidth}
% \centering
% \includegraphics[width=\linewidth]{result_11_B.pdf}
% \end{minipage}
% % \caption{UAVDT}
% }
\begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{result_test_A.pdf}
        \caption{VisDrone}
        \label{fig:short-res-a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{result_test_B.pdf}
        \caption{UAVDT}
        \label{fig:short-res-b}
        % \vspace{-0.1cm}
    \end{subfigure}
%\caption{Comparison of fixed mask ratios and the dynamic mask ratio estimated by AMM.}\label{fig:ab_amm}
\caption{Comparison of the fixed mask ratio and the dynamic one estimated by AMM.}\label{fig:ab_amm}
\vspace{0.2cm}
\end{figure}

%\subsubsection{On Context-Enhanced Sparse Convolution} We separately evaluate the effect of the residual structure (Res. for short), CE-GN as well as the normalization loss $\mathcal{L}_{norm}$ in Eq.~\eqref{eq:norm_loss} on the performance of CESC. Recall that directly applying sparse convolution to GFL V1 makes the training process unstable. As summarized in Table~\ref{tab:ab_cesc}, when employing the residual structure, GFL V1 with sparse convolution turns to be stable and requires much less GFLOPs, but the mAP drops sharply due to the loss of context information. By adding the context information via CE-GN, the accuracy is promoted significantly with a slight increase in GFLOPs. $\mathcal{L}_{norm}$ further boosts the accuracy and efficiency, since it implicitly strengthens the sparsity of the features. 

\subsubsection{On Detailed Designs in CESC} We separately evaluate the effect of the residual structure (Res. for short), CE-GN and the normalization loss $\mathcal{L}_{norm}$ in Eq.~\eqref{eq:norm_loss} on the performance of CESC. Recall that directly applying SC to GFL V1 makes the training process unstable. As summarized in Table~\ref{tab:ab_cesc}, when employing the residual structure, GFL V1 with SC turns to be stable and requires much less GFLOPs, but the mAP sharply drops due to the loss of context. By adding the context information via CE-GN, the accuracy is significantly promoted with a slight increase in GFLOPs. $\mathcal{L}_{norm}$ further boosts the accuracy and efficiency, since it implicitly strengthens the sparsity of the features. 

%We further evaluate the performance of the proposed CE-GroupNorm (CE-GN) by comparing to the counterparts including the one without using normalization as in QueryDet~\cite{QueryDet2022}, GroupNorm (GN)~\cite{GN2018} as in DynamicHead~\cite{DynamicHead2020}, BatchNorm (BN)~\cite{BatchNorm2015} and InstanceNorm (IN)~\cite{InstanceNorm2016}. We also report the results by the original GFL V1 denoted as `Dense Conv.'. As displayed in Table~\ref{tab:ab_cegn}, CE-GN substantially promotes the accuracy of the model without using normalization by 2.6\%. Comparing to other normalization methods, the accuracy of CE-GN is 0.7\%, 2,6\% and 0.8\% higher than GN, BN and IN, respectively. It is worth noting that CE-GN also performs the best in efficiency in regards of the GFLOPs and FPS. To further show the advantages of CE-GN, we visualize the cosine similarities between the features generated by dense convolutions and CE-GN, as well as GN for comparison. As Fig.~\ref{fig:ce-gn} illustrates, the features using CE-GN exhibit higher correlations with those using dense convolutions, showing the advantages of CE-GN in enhancing the global context for sparse convolutions.
We further evaluate the performance of CE-GN by comparing it to the counterparts including the one without using normalization as in QueryDet~\cite{QueryDet2022}, GroupNorm (GN)~\cite{GN2018} as in DynamicHead~\cite{DynamicHead2020}, BatchNorm (BN)~\cite{BatchNorm2015} and InstanceNorm (IN)~\cite{InstanceNorm2016}. We also report the results by the original GFL V1 detector denoted as `Dense Conv.'. As displayed in Table~\ref{tab:ab_cegn}, CE-GN substantially promotes the accuracy of the model without normalization by 2.6\%. In comparison to the other normalization schemes, CE-GN achieves the best accuracy, 0.7\%, 2,6\% and 0.8\% higher than GN, BN and IN, respectively. It is worth noting that CE-GN performs the best in efficiency in regards of GFLOPs and FPS as well. To highlight the advantages of CE-GN, we visualize the cosine similarities between the features generated by dense convolutions and sparse convolutions, where CE-GN and GN are separately utilized to normalize SC. As Fig.~\ref{fig:ce-gn} illustrates, the features using CE-GN exhibit higher correlations than those using GN, showing the superiority of CE-GN in enhancing global context for SC.


\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
%     & \checkmark & \checkmark & 28.4 & 50.5 & 28.0 & 150.58 & 21.32 \\
    with P3 & 26.9 & 48.6 & 26.3 & \textbf{143.03} & \textbf{27.78} \\
    with P3-P4 & 28.5 & 50.5 & 28.1 & 149.09 & 24.60 \\
    with P3-P5 & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & 150.01 & 21.79 \\
    \textbf{Ours} (with P3-P7) & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & 150.18 & 21.55\\ \hline
    \end{tabular}}
    %\caption{Ablation of FPN on VisDrone with GFL-V1.}
    \caption{Ablation on FPN with GFL-V1 on VisDrone.}
    % \caption{Ablation of FPN on VisDrone with our method(CEASC).}
    %\caption{Ablation study of FPN on VisDrone with GFL-V1.}
    % \vspace{-0.1cm}
    \label{tab:ab_fpn}
\end{table}
% \begin{table}[!t]
%     \centering
%     \small  
%     %\resizebox{\linewidth}{!}{
%     \begin{tabular}{c|ccc|cc}
%     \hline
%     Method & mAP & AP_{50} & AP_{75} & GFLOPs & FPS \\ \hline
%     Global & 28.4 & 50.2 & 28.1 & 162.53 & 19.84 \\
%     Layer-wise & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\ \hline
%     \end{tabular}%}
%     %\caption{Comparison of different ways for estimating the mask ratio by AMM on VisDrone.}
%     \caption{Comparison of estimating the mask ratio in different ways by AMM on VisDrone.}
%     \label{tab:layerwise}
%     % \vspace{-0.1cm}
% \end{table}


\begin{table*}[!t]
    \centering 
    \normalsize
    % \small
    %\resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|ccc|ccc}
    \hline
%    \multicolumn{2}{c|}{Method} & Backbone & mAP & AP_{50} & AP_{75} & GFLOPs & FPS \\ \hline
     Base Detector & Method & Backbone & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    \multirow{4}*{GFL V1~\cite{GFLv12020}} & Baseline     & ResNet18 & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
     & MobileNet V2~\cite{MobileNetV22018}                          & MobileNet V2 & 28.5 & 50.2 & 28.1 & 491.47 & 13.63 \\ 
     & ShuffleNet V2~\cite{ShufflenetV22018}                         & ShuffleNet V2 & 26.2 & 46.6 & 25.7 & 488.94 & 13.92 \\  
     & \textbf{Ours (CEASC)}                              & ResNet18 & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\ \hline
    \multirow{3}*{RetinaNet~\cite{RetinaNet2017}} & Baseline  & ResNet50 & 20.2 & \textbf{36.9} & 19.5 & 586.77 & 10.27 \\
     & QueryDet~\cite{QueryDet2022}                          & ResNet50 & 19.6 & 35.7 & 19.0 & - & 10.65 \\
     & QueryDet-CSQ~\cite{QueryDet2022}                      & ResNet50 & 19.3 & 35.0 & 18.9 & - & 11.71 \\
     & \textbf{Ours (CEASC)}                             & ResNet50 & \textbf{20.8} & 35.0 & \textbf{27.7} & \textbf{201.96} & \textbf{14.27} \\ \hline
    \end{tabular}%}
    %\caption{Comparison of mAP/AP (\%) and GFLOPs/FPS with the state-of-the-art approaches on VisDrone. `-' indicates that the result is not reported or not public available.}
    \caption{Comparison of mAP/AP (\%) and GFLOPs/FPS with the state-of-the-art approaches on VisDrone. `-' indicates that the result is not reported or not publicly available.}
    \vspace{0.1cm}
    \label{tab:sota_on_visdrone}
    
\end{table*}


\begin{table}[!t]
    \centering
    
    \normalsize
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ 
    \hline
    Baseline & 16.9 & 29.5 & \textbf{17.9} & 271.66 & 20.49 \\
    \textbf{Ours (CEASC)}  & \textbf{17.1} & \textbf{30.9} & 17.8 & \textbf{64.12} & \textbf{28.47} \\ \hline
    \end{tabular}}
    % \vspace{-0.2cm}
    %\caption{Comparison of mAP/AP (\%) and GFLOPs/FPS on UAVDT with GFL V1.}
    \caption{Comparison of mAP/AP (\%) and GFLOPs/FPS with GFL V1 on UAVDT.}
    % \vspace{-0.5cm}
    \label{tab:sota_uavdt}
\end{table}


%To encode the global context, we utilize the point-wise convolution, and compare to existing approaches including the plain $3\times 3$ convolution, GhostModule~\cite{ghostnet2020}, as well as several attention-based methods such as CBAM~\cite{cbam2018} and Criss-Cross Attention~\cite{ccnet2019}. As summarized in Table ~\ref{tab:ab_global_info}, the point-wise convolution outperforms compared methods in regards of the detection accuracy. %%Meanwhile, the attention-based methods cannot utilize the advantage of low FLOPs while point-wise convolution reaches the lowest FLOPs in convolution-based methods, thus achieving the highest efficiency in all methods, indicating point-wise convolution attains the best performance among them.
%Meanwhile, the point-wise convolution reaches the lowest GFLOPs in the convolution-based approaches, and achieves the highest FPS among all compared methods, clearly demonstrating its advantage in balancing the accuracy and efficiency.

To encode global context, we utilize the point-wise convolution, and make comparison to existing techniques including the plain $3\times 3$ convolution, GhostModule~\cite{ghostnet2020}, and several attention-based methods such as CBAM~\cite{cbam2018} and Criss-Cross Attention~\cite{ccnet2019}. As summarized in Table ~\ref{tab:ab_global_info}, the point-wise convolution outperforms the counterparts in detection accuracy. Meanwhile, it reaches the lowest GFLOPs in the convolution-based approaches and achieves the highest FPS among all the methods, clearly demonstrating its advantage in balancing the accuracy and efficiency.

%We conduct ablation studies on several major components in Table 4 for their effects on model performance. The baseline first achieve 28.4 mAP with 13.46 FPS. After adding the sparse convolution acceleration, the overall training fails to converge due to little proportion of pixels processed by sparse convolution. Adding a residual structure or our proposed CE-GroupNorm structure with $\mathcal{L}_{norm}$ both could make the training run smoothly and promote the inference speed to around 21 FPS. However, the introduction of residual structure will lead to a large drop in mAP to 26.1, while our CE-GroupNorm structure perfectly maintains the original accuracy. Further on, it can be concluded from the last three rows of Table 4 that CE-GroupNorm greatly improves the accuracy on the basis of residual structure with a slight drop on speed because more pixels are processed for better context enhancement, and $\mathcal{L}_{norm}$ add guidance to CE-GroupNorm thus further improves the accuracy and speed of our method to 28.7 mAP and 21.55 fps respectively, highlighting the effectiveness of our proposed method.



%\begin{table*}[!t]
%    \centering
%    \label{uncertain}
%    \resizebox{0.75\linewidth}{!}{
%    \begin{tabular}{ccccc|ccc|cc}
%    \hline
%    Base & Mask & Residual & CE-GN & $\mathcal{L}_{norm}$ & mAP & AP_{50} & AP_{75} & GFLOPs & FPS \\ \hline
%    \checkmark &  &  &  &  & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
    % \checkmark & \checkmark &  &  &  & \multicolumn{3}{c|}{-} & \multicolumn{2}{c}{-} \\
    % \checkmark & \checkmark & & \checkmark & \checkmark & \multicolumn{3}{c|}{-} & \multicolumn{2}{c}{-} \\
%    \checkmark & \checkmark &  &  &  & \multicolumn{3}{c|}{Failed} & - & - \\
%    \checkmark & \checkmark & & \checkmark & \checkmark & 28.4 & 50.5 & 28.0 & 150.58 & 21.32 \\
%    \checkmark & \checkmark & \checkmark &  &  & 26.1 & 47.2 & 25.3 & 151.66 & 21.51 \\
%    \checkmark & \checkmark & \checkmark & \checkmark &  & 28.5 & 50.5 & 28.3 & 155.90 & 19.91 \\
%    \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\ \hline
%    \end{tabular}}
%    \caption{Ablation Studies in terms of accuracy (AP) and efficiency (GFLOPs, FPS) on VisDrone, Mask means sparse convolution acceleration, '-' indicates could not be tested for some reason.}
%\end{table*}


%\begin{table}[h]
%    \centering
%    \label{uncertain}
%    \resizebox{1.05\linewidth}{!}{
%    \begin{tabular}{c|ccc|cc}
%    \hline
%    Method & mAP & AP_{50} & AP_{75} & GFLOPs & FPS \\ \hline
%    Baseline & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
%    FPN only P3+P4 & 14.4 & 22.8 & 15.3 & 494.42 & 15.63 \\
%    DepthWiseSeparable & 24.5 & 43.2 & 23.9 & 157.74 & 20.84 \\
%    Ours & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\  \hline
%    \end{tabular}}
%    \caption{Comparing different accelerate methods for detection head on VisDrone. 'DepthWiseSeparable' indicates Sequence of Depth-wise convolution and Point-wise convolution proposed by MobileNetV1.}.
%\end{table}

%\begin{table}[h]
%    \centering
%    \label{uncertain}
%    \resizebox{1.05\linewidth}{!}{
%    \begin{tabular}{c|ccc|cc}
%    \hline
%    Method & mAP & AP_{50} & AP_{75} & GFLOPs & FPS \\ \hline
%    Baseline & 28.37 & 50.01 & 27.80 & 524.95 & 13.46 \\
%    Input & 28.12 & 49.78 & 27.37 & 153.52 & 19.98 \\
%    Random & 28.35 & 50.49 & 27.99 & 150.45 & 19.25 \\
%    Zero & 28.43 & 50.50 & 28.05 & 150.58 & 21.32 \\
%    Ours & \textbf{28.73} & \textbf{50.70} & \textbf{28.36} & \textbf{150.18} & \textbf{21.55} \\  \hline
%    \end{tabular}}
%    \caption{Comparing different residual structure on VisDrone. 'Random' indicates plus with randomly generated feature with the same distribution as $G_{i}$.}.
%\end{table}

% \begin{figure}[!tpb]
% \centering
% \includegraphics[width=0.5\linewidth]{result_4.pdf}
% \caption{Comparison between baseline and several mask ratio control methods including fix mask ratio at different values and adaptive dynamic mask in accuracy at left graph. At the right graph, we present these method's trade-off between efficiency and accuracy.}
% \label{fig:example}
% \end{figure}

%We compare different detection head optimization methods in Table 5. Although Fig.4 shows that the dependence on $P_{5}$-$P_{7}$ feature levels in drone object detection is extremely low, the simple removal of these three layers will still lead to a significant drop in accuracy but cannot provide a significant acceleration effect. As to the classical approach replacing 3 $\times$ 3 convolution with DepthWise Separable convolution, it shares a similar GFLOPs with our method and achieves a great acceleration. However, this method still loses accuracy a lot because it simplifies the calculation of features as a whole. In contrast, our approach captures the information through dynamic control and feature enhancement and thus maintaining high accuracy without much calculation on them.

%In Table 6, we compare the impact of different residual structures on our method. Adding input to $F_{i,j}$ will cause a even greater loss than adding a random feature with the same distribution as $G_{i}$, showing the importance in keeping a balance distribution on features, which assist in maintaining the relationship between active pixels. While adding an identity mapping or a random feature will do harm to the method, adding $G_{i}$ promotes the mAP by 0.3, indicating further enhancement from background information to features. 

%\subsubsection{On Adaptive Multi-layer Masking} We compare the proposed AMM method with fixed mask ratios ranging from 0.50 to 0.95, on VisDrone and from 0.50 to 0.975 on UAVDT, respectively. As shown in Fig.~\ref{fig:ab_amm}, we can observe that more features will be involved in convolution when reducing the mask ratio, resulting in higher computational cost and slower FPS. In the mean time, the detection accuracy is sensitive to the mask ratio, not being consistently improved as the ratio increases. Moreover, the optimal fixed mask ratios are distinct on different datasets, \eg 0.9 on VisDrone and 0.95 on UAVDT in regards of mAP. In contrast, the proposed AMM approach can automatically determine an appropriate mask ratio, with which the base detector reaches the highest accuracy with the fastest inference speed, demonstrating the effectiveness of AMM. 

\subsubsection{On Detailed Analysis of AMM} We compare the AMM module with a fixed mask ratio ranging from 0.50 to 0.95 on VisDrone and from 0.50 to 0.975 on UAVDT, respectively. As Fig.~\ref{fig:ab_amm} shows, more features are involved in convolution when reducing the mask ratio, resulting in higher computational cost and lower FPS. In the mean time, we can see that the detection accuracy is sensitive to the mask ratio, which is not consistently improved as the ratio increases. Moreover, the optimal fixed mask ratio varies on different datasets, \eg 0.9 on VisDrone and 0.95 on UAVDT in regards of mAP. In contrast, AMM adaptively determines an appropriate mask ratio, with which the base detector reaches the best accuracy and the highest inference speed, demonstrating its necessity. 

%We compare the difference between setting the target mask rate of sparse networks to a fixed value with our proposed adaptive mask generation method in Fig.3. It can be concluded from the left graph that the accuracy does not always increase when more pixels are processed in sparse convolution, but achieves a maximum value at a low pixel processing rate. Meanwhile, the accuracy around the maximum value has a large range of drops, which requires the sparse algorithm to correctly predict the required pixel processing rate for every image. Our proposed method successfully predicts the extreme point on different datasets and achieves better performance than fixing mask ratio to the same value. The right graph shows the trade-off between accuracy and speed of the above methods, and the points in the upper right of the graph have better efficiency. As shown in the graph, unlike the conventional downward-right trend, the efficiency of the fixed ratio method is significantly improved after reaching the extreme point, supporting the conclusion above. In addition, the methods we propose on different datasets are both located in the upper right corner, demonstrating their high efficiency.

%Note that AMM separately computes the mask ratio for different layers in a ``Layer-wise'' way. We compare it to the ``Global'' way, which estimates a global mask ratio for all layers. As summarized in Table \ref{tab:layerwise}, the ``Layer-wise'' method clearly performs better than the ``Global'' one in regards of mAP and FPS. The reason lies in that the optimal mask ratio varies in different layers of FPN as displayed in Fig.~\ref{fig:vis}, and the ``Layer-wise'' method estimates the mask ratio more precisely than the ``Global'' one, thus promoting both the accuracy and efficiency. \textcolor{red}{We also evaluate the effect of different FPN layers in Table~\ref{tab:ab_fpn}. With less FPN layers, GFLOPs and FPS are improved. Abandoning P6-P7 does not affect much as they are less informative. Removing P4 incurs a sharp drop in mAP, indicating that P4 is crucial, which is consistent with the visualization.}

Note that AMM separately computes the mask ratio for different layers in a ``Layer-wise'' way. We compare it to a ``Global'' version, which estimates a global mask ratio for all layers. As demonstrated in Table \ref{tab:layerwise}, the ``Layer-wise'' method clearly performs better than the ``Global'' one in terms of mAP and FPS. The reason lies in that the optimal mask ratio varies in different layers of FPN as displayed in Fig.~\ref{fig:vis}, and the ``Layer-wise'' method estimates the mask ratio more precisely than the ``Global'' one, thus promoting both the accuracy and efficiency. We also evaluate its effect at different FPN layers in Table~\ref{tab:ab_fpn}. With less FPN layers, GFLOPs and FPS are improved. Abandoning P6-P7 does not affect much as they are less informative. Removing P4 incurs a sharp drop in mAP, indicating that P4 is crucial, which is consistent with the visualization.


%\subsection{Comparison with the State-of-the-art Methods}
\subsection{Comparison to SOTA}
%We compare our method with the state-of-the-art approaches: 1) the lightweight models including MobileNet V2~\cite{MobileNetV12017}, ShuffleNet V2~\cite{ShufflenetV12018}; 2) the detection head optimization method for drone images including QueryDet~\cite{QueryDet2022} and its acceleration part QueryDet-CSQ~\cite{QueryDet2022}. Since GFL V1 \cite{GFLv12020} with ResNet18 as the backbone network is widely used and proves effective in drone-based object detection, we select it as the base detector, and denote the original version as the `Baseline' method. We also report the result of our method by using RetinaNet~\cite{RetinaNet2017} with ResNet50 as the backbone network, since QueryDet and QueryDet-CSQ utilize RetinaNet~\cite{RetinaNet2017} as the base detector. \textcolor{red}{Note that the same data augmentation used in QueryDet is adopted in our implementation} for fair comparison.

We compare our network with the state-of-the-art ones: 1) the lightweight methods including MobileNet V2~\cite{MobileNetV12017} and ShuffleNet V2~\cite{ShufflenetV12018}; 2) the detection head optimization methods for drone imagery including QueryDet~\cite{QueryDet2022} and its acceleration part QueryDet-CSQ~\cite{QueryDet2022}. Since GFL V1 \cite{GFLv12020} with ResNet18 as the backbone is widely used and proves effective in drone-based object detection, we select it as the base detector, and denote the original version as the `Baseline' method. We also report the result by using RetinaNet~\cite{RetinaNet2017} with ResNet50 as the backbone, since it is used as the base detector in QueryDet and QueryDet-CSQ. Note that the same data augmentation technique used in QueryDet is adopted in our implementation for fair comparison.

%As summarized in Table \ref{tab:sota_on_visdrone}, our method remarkably reduces the GFLOPs of the base detector GFL V1 and RetinaNet, reaching a slightly higher mAP in the mean time. For instance, CEASC decreases the GFLOPS of the Baseline GFL V1 by 71.4\% and achieves 60\% speedup in terms of FPS during inference, with a 0.3\% improvement in mAP. Since the lightweight models, \ie MobileNet V2 and ShuffleNet V2, seeks for efficiency by simplifying the network structure, their mAPs are lower than ours. Moreover, they utilize dense detection heads, thus requiring much more GFLOPs than ours. Though QueryDet-CSQ considers optimizing the detection head by using the CSQ module based on sparse convolution, it only concentrates on small objects and ignores the loss of contextual information. Meanwhile, QueryDet employs an extra heavy query head for promoting accuracy, which introduces more computation cost. In contrast, our method develops the context-enhanced sparse convolution and designs an adaptive multi-layer masking scheme, thus clearly outperforming QueryDet and QueryDet-CSQ, both in accuracy and efficiency. 

As summarized in Table \ref{tab:sota_on_visdrone}, CEASC remarkably reduces the GFLOPs of the base detectors (GFL V1 and RetinaNet), reaching a slightly higher mAP in the mean time. For instance, CEASC decreases the GFLOPS of the Baseline GFL V1 by 71.4\% and achieves 60\% speedup in terms of FPS during inference, with a 0.3\% improvement in mAP. Since the lightweight models, \ie MobileNet V2 and ShuffleNet V2, quest for efficiency by simplifying the network structures, their mAPs are lower than ours. Moreover, they apply dense detection heads, thus requiring much more GFLOPs. Though QueryDet-CSQ considers to optimize the detection head by the CSQ module with sparse convolutions, it only concentrates on small objects and ignores the loss of contextual information. Besides, QueryDet introduces an extra heavy query head to promote performance, which inevitably incurs more computational cost. In contrast, CEASC newly develops the context-enhanced sparse convolution module and designs an adaptive multi-layer masking scheme, thus clearly outperforming QueryDet and QueryDet-CSQ, both in accuracy and efficiency. 

%We also evaluate CEASC on UAVDT. As reported in Table \ref{tab:sota_uavdt}, our method reduces the GFLOPs by 76.3\% and boosts the inference speed by 38.9\% with a 0.2 mAP improvement, compared with Baseline.

We also evaluate CEASC on UAVDT. As reported in Table \ref{tab:sota_uavdt}, our method reduces the GFLOPs by 76.3\% and boosts the inference speed by 38.9\% with a gain of 0.2\% in mAP, compared with the Baseline.

%We also test two ways to calculate $\mathcal{L}_{cost}$ introduced in Section 3. In Table 7, it can be found that calculating the average proportion of activated pixels globally is worse than calculating at each level both in accuracy and inference speed, indicating that the hierarchical calculation can allocate more appropriate results for each layer.


\section{Conclusion}

%We propose a novel plug-and-play detection head optimization approach, namely the global Context-Enhanced Adaptive Sparse Convolutional network (CEASC), for object detection on drone images. CEASC employs the CE-GroupNorm for normalization, which substantially compensates the loss of global contextual information. In addition, CEASC develops the AMM module to adaptively adjust the mask ratio for distinct foreground ratios. Extensive experiments on VisDrone and UAVDT demonstrate that CEASC remarkably accelerates the inference speed of various base detectors with competitive accuracy.

We propose a novel plug-and-play detection head optimization approach, namely CEASC, to object detection on drone imagery. It develops the CESC module with CE-GN, which substantially compensates the loss of global context and stabilizes the distribution of foreground. Furthermore, it designs the AMM module to adaptively adjust the mask ratio for distinct foreground areas. Extensive experimental results achieved on VisDrone and UAVDT demonstrate that CEASC remarkably accelerates the inference speed of various base detectors with competitive accuracies.

%\section*{Acknowledgement}
\section*{Acknowledgment}

%This work is supported by the National Key R\&D Program of China (2021ZD0110503), the National Natural Science Foundation of China (No. 62022011 and No. 62202034), the Research Program of State Key Laboratory of Software Development Environment (SKLSDE-2021ZX-04), and the Fundamental Research Funds for the Central Universities.
This work is partly supported by the National Key R\&D Program of China (2021ZD0110503), the National Natural Science Foundation of China (62022011 and 62202034), the Research Program of State Key Laboratory of Software Development Environment (SKLSDE-2021ZX-04), and the Fundamental Research Funds for the Central Universities.

% \section{Acknowledgement}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix

\renewcommand{\thefigure}{\Alph{figure}}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thetable}{\Alph{table}}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}

\section*{Supplementary Material}

In this document, we provide more implementation details, additional ablation studies as well as more visualization results. 

\section{More Implementation Details}\label{sec:impl}
%In Table 1 of the main body, we evaluate the performance of our method with various base detectors, including RetinaNet, FSAF, Faster-RCNN besides GFL V1. Since both RetinaNet and FSAF are the one-stage detectors as GFL V1, we  use the same setting as GFL V1 for these two detectors. Faster-RCNN is the two-stage detector, we follow \cite{QueryDet2022}, modifying its RPN Head to 4 Convolution-GN-ReLU layers instead of 1 convolution layer and using 256 feature channels, which proves effective in balancing the accuracy and efficiency in \cite{QueryDet2022}. 
%In Table 1 of the main body, we evaluate the performance of our method with various base detectors, including RetinaNet, FSAF, Faster-RCNN besides GFL V1. Since both RetinaNet and FSAF are one-stage detectors as GFL V1, we use the same setting as used by GFL V1. Regarding the two-stage Faster-RCNN detector, we follow \cite{QueryDet2022}, modifying its RPN head to 4 Convolution-GN-ReLU layers instead of 1 convolution layer and using 256 feature channels, which proves effective in balancing the accuracy and efficiency \cite{QueryDet2022}. 
In Table~\ref{tab:sota_diff_basedet} of the main body, we evaluate the performance of our approach with various base detectors, including RetinaNet, FSAF, Faster-RCNN besides GFL V1. Since both RetinaNet and FSAF are one-stage detectors as GFL V1, we adopt the same setting as used by GFL V1. Regarding the two-stage Faster-RCNN detector, we follow \cite{QueryDet2022}, modifying its RPN head to 4 Convolution-GN-ReLU layers instead of 1 convolution layer and using 256 feature channels, which proves effective in balancing the accuracy and efficiency \cite{QueryDet2022}. 

%In Table 7 of the main body, we compare our method with MobileNet V2~\cite{MobileNetV22018}, ShuffleNet V2~\cite{ShufflenetV22018} and QueryDet~\cite{QueryDet2022}. We select the feature map generated by the layers (2, 4, 6) and (0, 1, 2) as the inputs of FPN for MobileNet V2 and ShuffleNet V2, respectively. In regards of QueryDet, we reimplement it by using the same setting as MobileNet V2 and ShuffleNet V2 as well as ours, for fair comparisons. Concretely, we utilize the unified input size as (1333$\times$800), and omit the calculation of the $P_{2}$ layer. 
In Table~\ref{tab:sota_on_visdrone} of the main body, we compare our network to MobileNet V2~\cite{MobileNetV22018}, ShuffleNet V2~\cite{ShufflenetV22018} and QueryDet~\cite{QueryDet2022}. We select the feature maps generated from the layers (2, 4, 6) and (0, 1, 2) as the input of FPN for MobileNet V2 and ShuffleNet V2, respectively. Regarding QueryDet, we re-implement it by using the same setting for fair comparison. Particularly, we utilize the unified input size as 1,333$\times$800 and omit the calculation of the $P_{2}$ layer. 

\section{Additional Ablation Studies}\label{sec:ab}
%In this section, we provide more results about the performance of our method with different residual structures, accelerating strategies, \textcolor{red}{context information enhancement strategies} and training epochs. 
%We provide additional results by our method using different residual structures, accelerating strategies, context clues and training epochs. 
We show additional results by our approach using different residual structures, accelerating strategies, context clues and training epochs.

%\subsection{On Different Residual Structures}
\subsection{On Residual Structures}
%As illustrated in Fig. 2 of the main body, we adopt a residual structure to compensate the loss of the contextual information due to sparse convolutions. We therefore compare the performance of the proposed CEASC by using different residual structures, including: 1) ``w.o. Res.'' without residual structure; 2) ``Focal Res.'' using the raw input for skip connection, \emph{i.e.} $\mathbf{F}:=\mathbf{F}+\mathbf{X}$; 3) ``Ours'' using the global contextual feature for skip connection, \emph{i.e.} $\mathbf{F}:=\mathbf{F}+\mathbf{G}$.
As shown in Fig.~\ref{fig:framework} of the main body, we adopt a residual structure to compensate the loss of contextual information due to sparse convolutions. We therefore compare the performance of the proposed CEASC network by using different residual structures, including: 1) ``w.o. Res.'' without using the residual structure; 2) ``Focal Res.'' using the raw input for skip connection, \emph{i.e.} $\mathbf{F}:=\mathbf{F}+\mathbf{X}$; and 3) ``Ours'' using the global contextual feature for skip connection, \emph{i.e.} $\mathbf{F}:=\mathbf{F}+\mathbf{G}$.

%As shown in Table \ref{tab:ab_res}, our method reaches the best performance, showing the advantage of using the global contexts.
%As displayed in Table \ref{tab:ab_res}, our method reaches the best performance, showing the advantage of using the global context. 
As displayed in Table \ref{tab:app_ab_res}, our approach reaches the best performance, highlighting its advantage in capturing global context. 

%\subsection{On Distinct Acceleration Strategies}
\subsection{On Acceleration Strategies}
%In Tables 7 of the main body, we have compared our method with the state-of-the-art lightweight models for drone-based object detection. In this document, we provide ablation study on our approach by using distinct acceleration strategies including: 1) ``FPN on P3+P4 only'' that only adopts the P3 and P4 layer for FPN, since the P5-P7 layer are unlikely to be activated for sparse convolution as obseved in Fig.~\ref{fig:vis};  2) ``DWS'' that utilizes the Depth-Wise Separable (DWS) convolution used in MobileNet for ligh-weighting, instead of the normal 3 $\times$ 3 convolution in the ``Baseline'', \emph{i.e.} the original GFL V1.
%In Table 7 of the main body, we compare our method with the state-of-the-art lightweight models for drone-based object detection. Here, we carry out the ablation study on our method by using distinct acceleration strategies including: 1) ``FPN on P3+P4 only'' that only adopts the P3 and P4 layers for FPN, since the P5-P7 layers are unlikely to be activated for sparse convolutions as observed in Fig.~\ref{fig:vis};  and 2) ``DWS'' that utilizes the Depth-Wise Separable (DWS) convolution as in MobileNet, instead of the normal 3 $\times$ 3 convolution in the ``Baseline'', \emph{i.e.} the original GFL V1 detector.
In Table~\ref{tab:sota_on_visdrone} of the main body, we compare our approach with the state-of-the-art lightweight models for drone-based object detection. Here, we carry out the ablation study on our approach using distinct acceleration strategies including: 1) ``FPN on P3+P4 only'' that only adopts the P3 and P4 layers for FPN, since the P5-P7 layers are unlikely to be activated for sparse convolutions as observed in Fig~\ref{fig:vis} and Fig.~\ref{fig:app_visdrone};  and 2) ``DWS'' that utilizes the Depth-Wise Separable (DWS) convolution as in MobileNet, instead of the normal 3 $\times$ 3 convolution in the ``Baseline'', \emph{i.e.} the original GFL V1 detector.

%The results are summarized in Table \ref{tab:ab_acc}, showing that our method outperforms the compared method both in accuracy and efficiency, due to the sparse convolution optimized by the context-enhancement and adaptive multi-layer masking. 
%The results are summarized in Table \ref{tab:ab_acc}, indicating that our method outperforms the counterparts both in accuracy and efficiency, due to the sparse convolutions optimized by the context-enhancement and adaptive multi-layer masking.
The results are summarized in Table \ref{tab:app_ab_acc}, indicating that our approach outperforms the counterparts both in accuracy and efficiency, due to the sparse convolutions optimized by context-enhancement and adaptive multi-layer masking.


\begin{table}[!t]
    \centering
   % \resizebox{1.05\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    w.o. Res. & 28.4 & 50.5 & 28.1 & 150.58 & 21.32 \\
    Focal Res. & 28.1 & 49.8 & 27.4 & 153.52 & 19.98 \\
%    Random & 28.35 & 50.49 & 27.99 & 150.45 & 19.25 \\
    \textbf{Ours} & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\  \hline
    \end{tabular}%}
    %\caption{Comparison with different residual structures on VisDrone.}.
    \caption{Comparison in terms of mAP (\%) and GFLOPs/FPS with different residual structures on VisDrone.}
    \label{tab:app_ab_res}
\end{table}


\begin{table}[!t]
    \centering
    %\caption{Comparing with different  acceleration methods for detection head on VisDrone. 'DepthWiseSeparable' indicates Sequence of Depth-wise convolution and Point-wise convolution proposed by MobileNetV1.}.
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    Baseline & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
    FPN on P3+P4 only & 28.1 & 49.9 & 27.5 & 494.42 & 15.63 \\
    DWS & 24.5 & 43.2 & 23.9 & 157.74 & 20.84 \\
    \textbf{Ours} & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\  \hline
    \end{tabular}}
    %\caption{Comparing with different acceleration strategies on VisDrone.}
    \caption{Comparison in terms of mAP (\%) and GFLOPs/FPS with different acceleration strategies on VisDrone.}
    \label{tab:app_ab_acc}
\end{table}

\begin{table}[!t]
    \centering
    %\caption{Comparing with different  acceleration methods for detection head on VisDrone. 'DepthWiseSeparable' indicates Sequence of Depth-wise convolution and Point-wise convolution proposed by MobileNetV1.}.
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{c|ccc|cc}
    \hline
    Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & GFLOPs & FPS \\ \hline
    Baseline & 28.4 & 50.0 & 27.8 & 524.95 & 13.46 \\
    Interpolation\cite{SpatiallyAdaptiveInference2020} & 26.9 & 48.5 & 26.1 & 212.15 & 13.80 \\
    \textbf{Ours} & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{150.18} & \textbf{21.55} \\  \hline
    \end{tabular}}
    %\caption{Comparing with different context information supplement strategies on VisDrone.}
    \caption{Comparison in terms of mAP (\%) and GFLOPs/FPS with different context clues on VisDrone.}
    \label{tab:app_ab_context}
\end{table}


\begin{figure*}[!thb]
\centering
\includegraphics[width=\linewidth]{ce-gn-all-v3.pdf}
%\caption{Visualization on correlation between the features generated by dense convolutions and those by sparse convolutions with distinct normalizations on VisDrone.}
\caption{Visualization of correlation between features generated by dense convolutions and sparse convolutions with distinct normalization schemes on VisDrone.}
\label{fig:app_ce-gn}
\end{figure*}




\begin{figure*}[!thb]
\centering
\includegraphics[width=\linewidth]{vision_visdrone.pdf}
%\caption{Visualization of the dynamic masks estimated by AMM for different layers (from `P3' to `P7') in FPN of GFL V1 on VisDrone. Highlighted areas are activated for computation.}
\caption{Visualization of dynamic masks estimated by AMM at different layers (from `P3' to `P7') in FPN of GFL V1 on VisDrone. Highlighted areas are activated for computation.}
\label{fig:app_visdrone}
\end{figure*}

\begin{table*}[!t]
    \centering
    \normalsize
    %\resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c|ccc|cccc|cc}
    \hline
    %\multicolumn{2}{c|}{Method} & mAP & AP_{50} & AP_{75} & AR_{1} & AR_{10} & AR_{100} & AR_{500} & GFLOPs & FPS \\ \hline
    Epoch & Method & mAP & $\text{AP}_{50}$ & $\text{AP}_{75}$ & $\text{AR}_{1}$ & $\text{AR}_{10}$ & $\text{AR}_{100}$ & $\text{AR}_{500}$ & GFLOPs & FPS \\ \hline
    \multirow{2}*{12}        & Baseline & 27.8 & 49.2 & 27.3 & 0.63 & 6.27 & 34.7 & 44.2 & 524.95 & 13.48 \\
                                 & \textbf{Ours (CEASC)} & 27.8 & \textbf{49.3} & \textbf{27.4} & \textbf{0.67} & \textbf{6.47} & \textbf{34.8} & \textbf{44.4} & \textbf{151.93} & \textbf{21.52} \\\hline
    \multirow{2}*{15}   & Baseline & 28.4 & 50.0 & 27.8 & 0.62 & 6.36 & 35.6 & 44.9 & 524.95 & 13.46 \\\
                                 & \textbf{Ours (CEASC)} & \textbf{28.7} & \textbf{50.7} & \textbf{28.4} & \textbf{0.65} & \textbf{6.56} & 35.6 & \textbf{45.0} & \textbf{150.18} & \textbf{21.55}\\\hline
    \multirow{2}*{24}   & Baseline  & 28.9 & 50.9 & 28.4 & \textbf{0.72} & 6.53 & 35.7 & 45.2 & 524.95 & 13.41 \\
                                 & \textbf{Ours (CEASC)}  & \textbf{29.1} & \textbf{51.3} & \textbf{28.7} & 0.70 & \textbf{6.90} & \textbf{36.0} & \textbf{45.4} & \textbf{151.42} & \textbf{21.49} \\\hline
    \end{tabular}%}
    %\caption{Comparison of AP/AR (\%) and GFLOPs/FPS on VisDrone with GFL V1 base detector training for different epochs.}
    \caption{Comparison in terms of AP/AR (\%) and GFLOPs/FPS with the GFL V1 base detector using different training epochs on VisDrone.}
    \label{tab:app_ab_epoch}
\end{table*}

\begin{figure*}[!thb]
\centering
\includegraphics[width=\linewidth]{vision_uavdt.pdf}
%\caption{Visualization of the dynamic masks estimated by AMM for different layers (from `P3' to `P7') in FPN of GFL V1 on UAVDT. Highlighted areas are activated for computation.}
\caption{Visualization of dynamic masks estimated by AMM at different layers (from `P3' to `P7') in FPN of GFL V1 on UAVDT. Highlighted areas are activated for computation.}
\label{fig:app_uavdt}
\end{figure*}



%We compare different detection head optimization methods in Table B. Although visualization results shows that the dependence on $P_{5}$-$P_{7}$ feature levels in drone object detection is extremely low, the simple removal of these three layers will still lead to a significant drop in accuracy but cannot provide a significant acceleration effect. As to the classical approach replacing 3 $\times$ 3 convolution with DepthWise Separable convolution, it shares a similar GFLOPs with our method and achieves a great acceleration. However, this method still loses accuracy a lot because it simplifies the calculation of features as a whole. In contrast, our approach captures the information through dynamic control and feature enhancement and thus maintaining high accuracy without much calculation on them.

%\subsection{On Context Information Enhancement Strategies}
\subsection{On Context Clues}
%\textcolor{red}{We have mentioned an interpolation method to generate ignored pixels from focal information~\cite{SpatiallyAdaptiveInference2020} in Sec. 3.1.2 of the main body, and an ablation study is conducted to compare~\cite{SpatiallyAdaptiveInference2020} with our approach in Table~\ref{tab:ab_context}. The results reveal that interpolation have a negative impact on the accuracy, while more computation is needed to satisfy the interpolation requirements.}
%In Sec. 3.1.2 of the main body, we mention an interpolation method to generate ignored pixels from focal areas~\cite{SpatiallyAdaptiveInference2020}, and an ablation study is conducted for comparison. The results in Table~\ref{tab:ab_context} reveal that interpolation incurs a drop on the accuracy but consumes more computation.
In Sec.~\ref{sec:CE} of the main body, we mention an interpolation method to generate ignored pixels from focal areas~\cite{SpatiallyAdaptiveInference2020}, and an ablation study is conducted for comparison. The results in Table~\ref{tab:app_ab_context} reveal that interpolation incurs a drop on the accuracy but consumes more computations.

\subsection{On Training Epochs}
%In Tabel 1 and 2 of the main body, we compare our method with the state-of-the-p

%In literature, some works utilize the 12 and 24 training epochs, besides the 15 epochs adopted in our work. We therefore provide more results by using different training epochs. As displayed in Table \ref{tab:ab_epoch}, our method consistently significantly boosts the accuracy without decreasing the accuracy. As more training epochs are used for training, our method can reach higher accuracy. 
%In the literature, some studies train their models for varying epochs (\emph{e.g.} 12 or 24). We thus provide more results by using such numbers of training epochs in addition to 15 adopted in this work. As displayed in Table \ref{tab:ab_epoch}, our method consistently boosts the performance by a large margin with different training epochs. When more training epochs are used, our method reaches a higher accuracy, where 15 is a good trade-off. 
In the literature, some studies train their models for varying epochs (\emph{e.g.} 12 or 24). We thus provide more results by using such numbers of training epochs in addition to 15 adopted in this work. As displayed in Table \ref{tab:app_ab_epoch}, our approach consistently boosts the performance by a large margin with different training epochs. When more training epochs are used, our approach reaches a higher accuracy, where 15 is a good trade-off. 

\subsection{More Visualized Results}
%\textcolor{red}{More visualization results are presented in Fig.~\ref{fig:ce-gn} as supplements to Fig. 3 of the main body. The features normalized by CE-GN obtain higher correlation with the dense convolution than GN, indicating CE-GN successfully enhances focal features with the assistance of global contexts.}
%More visualized results are presented in Fig.~\ref{fig:ce-gn} as supplements to Fig. 3 of the main body. The features normalized by CE-GN have higher correlation with dense convolutions than GN, indicating that CE-GN enhances focal features with the assistance of global context.
More results are visualized in Fig.~\ref{fig:app_ce-gn} as supplements to Fig.~\ref{fig:ce-gn} of the main body. The features normalized by CE-GN have higher correlation with dense convolutions than GN, indicating that CE-GN enhances focal features with the assistance of global context.

%In Fig.~\ref{fig:vis} and Fig.~\ref{fig:uavdt}, we provide more visualization results as supplements to Fig. 4 of the main body. As illustrated, the mask generated by our approach successfully captures the foreground areas, indicating that the sparse convolution spends most of the computation on the foregrounds, thus promoting the efficiency without sacrificing much accuracy.
%In Fig.~\ref{fig:vis} and Fig.~\ref{fig:uavdt}, we visualize more results as supplements to Fig. 4 of the main body. As illustrated, the mask generated by our method well covers foreground areas, indicating that sparse convolutions spend most computations on foreground, thus promoting the efficiency without sacrificing much accuracy.
In Fig.~\ref{fig:app_visdrone} and Fig.~\ref{fig:app_uavdt}, we visualize more results as supplements to Fig.~\ref{fig:vis} of the main body. As illustrated, the mask generated by our approach well covers foreground areas, indicating that sparse convolutions spend most computations on foreground, thus promoting the efficiency without sacrificing much precision.

\end{document}
