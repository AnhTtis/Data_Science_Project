\section{Introduction}
\label{sec:introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}
One of the challenges to realizing autonomous mobile robots should be the perception problem.
As the front-end module of a robotic system, perception offers essential information to high-level navigation.
Nowadays, the fusion of a LiDAR and a camera, called \textit{LC-Fusion}, provides a promising solution for robust perception.
LC-Fusion is able to overcome individual limitations of both LiDARs and cameras, producing confident results to boost various tasks such as simultaneous localization and mapping (SLAM) \cite{lin2022r,yu2022accurate} and pattern recognition \cite{qi2018frustum}.
Overall, LC-Fusion enjoys the following advantages:
\begin{itemize}
    \item \textbf{Accessibility}:
          Benefiting from the rapid development of sensory technologies, both cameras and LiDARs are portable and accessible for various commercial mobile robots, including drones \cite{zhang2019maximum}, quadrupedal robots \cite{jiao2022fusionportable}, and self-driving cars \cite{cao2021learning}.
    \item \textbf{Information Sufficiency}:
          LC-Fusion provides multi-modal information that is typically sufficient for robot navigation.
          Specifically, LiDARs directly offer sparse but accurate 3D measurements of surrounding objects.
          In contrast, cameras capture dense and high-resolution 2D images, which is beneficial to object recognition.
    \item \textbf{Wide Usage}:
          Research on LiDARs, cameras, and their fusion has attracted much attention from the research community.
          Algorithms targeting at different perception tasks have immediate solutions and are applicable to many robotic navigation tasks.
\end{itemize}

\begin{figure}[t]
    \centering
    \subfigure[]
    {\label{fig:lidar_garden}\centering\includegraphics[width=0.110\textwidth]{figure/methodology/fig1_pointcloud-crop}}
    \subfigure[]
    {\label{fig:image_garden}\centering\includegraphics[width=0.115\textwidth]{figure/methodology/fig1_img-crop}}
    \subfigure[]
    {\label{fig:image_event_garden}\centering\includegraphics[width=0.115\textwidth]{figure/methodology/fig1_event-crop}}
    \subfigure[]
    {\label{fig:image_event_garden}\centering\includegraphics[width=0.115\textwidth]{figure/methodology/fig1_event_reconstruction-crop}}
    \caption{Sensor measurements in a garden at night:
        (a) a LiDAR point cloud,
        (b) a frame image,
        (c) events that are printed on a frame image, and
        (d) reconstructed images using events.
        We can distinguish a human from the point cloud and event image except for the frame image. This indicates that the frame image is sensitive to the weak light. Please refer to the color version for the better visualization.}
    \label{fig:sensor}
\end{figure}

However, traditional frame cameras are commonly sensitive to changing illumination (e.g., darkness and glare).
Cameras cannot capture scene information completely and may fail several vision-based algorithms.
LC-Fusion may degenerate into a LiDAR-only configuration since most images are noisy.
This issue motivates us to explore a novel type of sensor, event cameras \cite{gallego2020event}, to complement the traditional LiDAR-camera fusion in challenging environments.
The event camera augments the original sensor setup, and we can fuse three types of sensor input for perception.
This new sensor fusion mode is called \textit{LCE-Fusion}.

Event cameras are bio-inspired sensors.
Different from frame cameras that capture images at a fixed rate, event cameras asynchronously capture the per-pixel \textit{intensity changes} and output a stream of \textit{2D events}.
Each event is encoded with information, including the triggered time, pixel localization, and the sign of the intensity change.
Event cameras have high temporal resolution ($\mu s$-level), high dynamic range ($140$dB v.s. $60$dB of frame cameras), and low power consumption.
They have great potential for several computer vision and robotic tasks, e.g.,
high-speed motion estimation \cite{bryner2019event} and high dynamic range perception \cite{rebecq2019high}.
Fig. \ref{fig:sensor} visualizes the enhancement brought by an event camera in a dark garden.
% as well as feature tracking \cite{gentil2020idol} 

% \begin{figure}[t]
%     \centering
%     \subfigure[]
%     {\label{fig:lidar_garden}\centering\includegraphics[width=0.210\textwidth]{figure/methodology/lidar_garden-crop}}     
%     \subfigure[]
%     {\label{fig:image_garden}\centering\includegraphics[width=0.13\textwidth]{figure/methodology/image_garden-crop}}
%     \subfigure[]
%     {\label{fig:image_event_garden}\centering\includegraphics[width=0.1275\textwidth]{figure/methodology/image_event_garden-crop}}
%     \caption{Sensor measurements in a garden at night: (a) a LiDAR point cloud, 
%     (b) a frame image, 
%     and (c) events that are rendered on a frame image.
%     We can distinguish a human in the point cloud and event images (consist of red and blue events) except for the frame image.
%     This means that the frame image is sensitive to the weak light.
%     Please refer to the color version for the better visualization.}
%     \label{fig:sensor}  
% \end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Challenges}
The extrinsic calibration, estimating the relative rotational and translational offset from the reference frame to the target frame, is an indispensable step in using the LCE-Fusion.
Our goal is to design a general and automatic extrinsic calibration approach for the LCE-Fusion. However, as emphasized in \cite{choi2015extrinsic}, challenges of automatic calibration arise from the three aspects: feature extraction given noisy data, data association across multi-modal sensors, and parameter estimation.

A standard checkerboard is desirable in calibration, offering distinctive features (e.g., corners, boundaries, and a plane) and known geometry for feature matching. Although existing methods \cite{verma2019automatic} have demonstrated the validity of this paradigm, several issues, including feature extraction, data association, event representation, and global optimization, have not been addressed well:
\begin{itemize}
    \item \textbf{Automatic Feature Extraction}: It is straightforward to detect the checkerboard from images by the off-the-shelf softwares such as OpenCV. However, this is not applicable to point clouds whose data model is fundamentally different. The detection of the checkerboard from point clouds is a nontrivial problem and is particularly hard if point clouds contain doors, tables, and walls with the planar shape. Several works \cite{verma2019automatic} have to finish the feature extraction manually.
    \item \textbf{Automatic Feature Matching}: An automatic approach to match checkerboard features between LiDARs and cameras are needed. However, this raises the issue that the symmetric shape of the checkerboard may lead to ambiguous data association, resulting in suboptimal or unreliable extrinsics.
    \item \textbf{Event Representation}: The asynchronous and sparse nature of event cameras makes the feature selection difficult. We need a method to convert a group of events into an image-type representation since traditional methods can be directly applied.
    \item \textbf{Globally Optimal Solution}: The extrinsic optimization problem is generally non-convex. This implies that the typical Gauss-Newton solution that linearizes the objective is approximate to the globally optimal solution. Thus, introducing a globally optimal solver in calibration given noisy sensor measurements is important.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Contributions}
To tackle these challenges, we propose \textit{LCE-Calib}, a unified extrinsic calibration method for the LCE sensor configuration.
In overall, LCE-Calib presents the following \textit{contributions}.
\begin{enumerate}
    \item We propose an automatic checkerboard extraction and tracking method that is robust to external noisy objects from point clouds (see Section \ref{sec:methodology_fsl}). This method is also noise-aware, since we reduce the bias of LiDAR points by projecting points onto a reference plane and model the uncertainty of normal vectors.
    \item We introduce the learning-based approach to reconstruct frame images from event streams for the downstream extrinsic calibration task. The resulting images allow the usage of traditional corner detectors (see Section \ref{sec:methodology_ire}).
    \item We design the calibration process with an initialization-refinement philosophy to utilize point-to-plane and point-to-line constraints in a coarse-to-fine manner. This avoids the ambiguity issue in data association caused by the board's symmetric shape (see Section \ref{sec:ext_calibration}).
    \item We introduce a general solver to globally solve two optimization problems in calibration: the Perspective-n-Point (PnP) and Point-to-Plane registration (PtPL) problems. The benefits from it is that the resulting extrinsics are accurate even if measurements are noisy.
\end{enumerate}

The proposed method is evaluated extensively on different sensor devices in various calibration scenes from indoor offices to outdoor grounds. The proposed LCE-Calib outperforms the state-of-the-art (SOTA) calibration method, achieving an extrinsic accuracy of centimeters in translation and deci-degrees in rotation. To benefit the community, we publicly release the experimental data and code.\footnote{\url{https://github.com/HKUSTGZ-IADC/LCECalib}}


\begin{comment}
\subsection{Organization}
The rest of the article is organized as follows.
Section \ref{sec:related_work} reviews the relevant literature.
Section \ref{sec:problem_statement} formulates the problem and residual functions.
Section \ref{sec:measure_process} describes the preprocessing module on raw sensor measurements.
Section \ref{sec:ext_calibration} introduces the initialization, data association, and refinement schemes.
Section \ref{sec:experiment} shows experimental results.
Finally, Section \ref{sec:conclusion} concludes this article.
\end{comment}
