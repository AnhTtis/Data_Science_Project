\section{Measurement Processing}
\label{sec:measure_process}

\begin{figure}[t]
  \centering
  \subfigure[]
  {\label{fig:mp_pipeline_result_cloud}\centering\includegraphics[width=0.19\textwidth]{figure/methodology/mp_cloud_3-crop}}
  \subfigure[]
  {\label{fig:mp_pipeline_result_frame_image}\centering\includegraphics[width=0.14\textwidth]{figure/methodology/mp_frame_image-crop}}
  \subfigure[]
  {\label{fig:mp_pipeline_result_event_image}\centering\includegraphics[width=0.14\textwidth]{figure/methodology/mp_event_image-crop}}
  \caption{Features of the checkerboard are extracted from (a) the point cloud, (b) frame image, and (c) reconstructed image from events.}
  \label{fig:mp_pipeline_result}
\end{figure}

This section explains how raw sensor data from LiDARs, frame cameras, and event cameras are preprocessed before estimating their extrinsics.
We can take advantage of the structural prior of the checkerboard for reliable feature selection and matching.
Fig. \ref{fig:mp_pipeline_result} shows the feature extraction results.

\subsection{Automatic Feature Extraction From LiDARs}
\label{sec:methodology_fsl}
We are interested in extracting three types of features from the checkerboard point cloud: edge points, planar points, and planar coefficients.
These features are useful in LiDAR-camera data association.
Since we work on mechanical scanning LiDARs, the following sections use \textit{ring} to denote the points set from the same emitter.
We propose a two-stage method for automatic feature extraction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Checkerboard Point Selection}
\label{sec:methodology_fsl_selection}
We need to select board points from the raw point cloud.
We have these observations to design the point selection method:
\textit{1)} the width and height of the board are known as a prior;
\textit{2)} points from the same ring form a straight line segment if they lie on the board;
\textit{3)} points from different rings form a planar patch if they lie on the board and stay near.

Firstly, we compute the pitch angle $\phi = \arctan(\frac{z}{\sqrt{x^2 + y^2}})$ for each point and split out points into different rings.
Some LiDARs directly provide the ``ring'' as a property of each point.
We then cluster each ring into multiple line segments based on the distance of two consecutive points.
The principle component of each line segment is computed by PCA: $[\sigma_1, \sigma_2, \sigma_3]$ where $\sigma_1 \geq \sigma_2 \geq \sigma_3$. The curvature is measured as $\sigma_1/\sigma_2$.
We keep line segments if their curvature is bigger than the predefined threshold $\mu_1$ and the length of the line segment smaller than an empirical threshold $\mu_2$.
Secondly, we utilize the DBScan algorithm \cite{kriegel2011density} to segment the remaining points as several clusters. Small clusters are then removed.
Thirdly, we verify each cluster by registering it with a board point template.
We compute the registration error for each cluster and keep the only one with the lowest error. The selected cluster is regarded as the candidate board points.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Checkerboard Feature Extraction}
\label{sec:methodology_fsl_feature_extraction}

After obtaining board points, we use the RANSAC-based plane fitting method \cite{rusu20113d} to compute the normal vector $\bm{n}^{l}$, filter out outliers, and obtain a set of planar points $\mathcal{P}^{l}_{pl}$ (inliers).
Due to uncorrected bias and noise of LiDARs' measurements, we observe that board points form a $3cm$-thick plane approximately.
To reduce this effect in the subsequent calibration process, we project each planar point $\bm{p}$ onto the fitted plane as
\begin{equation}
  \label{equ:projection_on_plane}
  \bm{p}_{proj}^{l}
  =
  \bm{p}^{l} - a\bm{n}^{l},
  \ \ \
  a = \bm{n}^{l}\cdot(\bm{p}^{l} - \bm{g}^{l}),
\end{equation}
where $\bm{g}^{l}$ is a point on the plane.
The edge points $\mathcal{P}^{l}_{edge}$ are then extracted as the starting and ending points of each ring.

% We will show how this projection can improve the accuracy in Section \ref{sec:experiment}.}

% pts_on_plane = lidar_board_edge_pts(1:3, 1);
% pts_on_plane(3) = -((d + n(1:2)' * pts_on_plane(1:2)) / n(3));

% for ptid = 1 : size(lidar_board_pts, 2)
%   pt = lidar_board_pts(1:3, ptid);
%   v = pt - pts_on_plane;
%   pt_onboard = pts_on_plane + v - (v' * n * n);
%   lidar_board_pts(1:3, ptid) = pt_onboard;
% end

% In contrast, the determination of corners $\mathcal{P}^{l}_{cor}$ is a little complex. 
% A primary solution was proposed in \cite{zhou2018automatic}, where boundary equations are found via the RANSAC-based line fitting algorithm.
% But this scheme presents two limitations: 
% 1) the RANSAC-based fitting process only succeeds if more than three boundary points are provided, 
% and 2) the fitted lines are slightly unreliable since boundary points suffer from large non-Gaussian noise \cite{pomerleau2012noise}.

% Instead, our method fully utilizes the prior knowledge of the checkerboard.
% We formulate the point-to-edge distance minimization problem by computing the optimal transformation from the template board to data $\bm{x}^{l}_{b}$.
% The coarse initialization of orientation and translation of the board is insufficient for a nonlinear optimization problem. 
% We thus employ the global search method based on the scatter-search mechanism \cite{ugray2007scatter} to generate multiple start points around this initial value. 
% For each start point, the corresponding line of each boundary point can be found by the Nearest Neighbor Search (NNS), and we then minimize the sum of all point-to-edge residuals to optimize the transformation as iteratively
% \begin{equation}
% \begin{aligned}
%   \label{equ:methodology_board_pose}
%   &\hat{\bm{x}}^{l}_{b}
%   =
%   \underset{\bm{x}^{l}_{b}}{\arg\min}
%   \sum_{\bm{p}\in{\mathcal{P}_{edge}^{l}}}{}
%   \rho
%   \big(
%     ||\bm{r}_{edge}(\bm{x}^{l}_{b}, \bm{p}, L)||^{2}_{\bm{\Sigma}_{\bm{p}}}
%   \big).
% \end{aligned}
% \end{equation}

% Finally, we can select the optimal result with the lowest error from a set of candidate transformations. It helps us to avoid local minima caused by incorrect data association.
% We can use this resulting transformation to transform original corner points to obtain the observed board corners $\mathcal{P}^{l}_{cor}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Checkerboard Tracking}

Estimating the board's position allows us to track the board at the next frame.
The time-consuming point selection stage (costs $50-60ms$) is skipped.
We create a virtual bounding box around the checkerboard with a slightly larger size.
In the next frame, only points inside the box are kept.
This tracking stage also improves the success rate of the board point selection.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Automatic Feature Extraction From Frame Cameras}
\label{sec:methodology_fsc}

The coefficients of the board observed by the frame camera, denoted by $[\bm{n}^{c},d^{c}]^{\top}$, can be easily obtained.
We use the off-the-shelf software to detect the inner patterns of the checkerboard automatically.
The board's pose $SE_{3}(\bm{R}^{c}_{b}, \bm{t}^{c}_{b})$ is estimated by the \textit{QPEP-PnP} algorithm, where we minimize the point-wise distance between 3D inner patterns and their corresponding 2D points.
Along the transformed boundaries of the board, we can generate many ``fake'' edge points $\mathcal{P}_{edge}^{c}$.
We can also obtain the covariance matrix $\bm{\Sigma}_{\bm{\delta\phi}}$ of the checkerboard's rotation from the \textit{QPEP-PnP} algorithm.
Based on the uncertainty representation of 6-DoF poses in Section \ref{sec:ps_notation}, we can propagate the uncertainty of the rotated normal vector $\bm{n}^{c}\approx\bar{\bm{R}}^{c}_{b}(\bm{I} + \delta\phi^{\wedge})\bm{n}^{b}$ as
\begin{equation}
  \begin{split}
    % \bm{n}^{c}
    % &\approx
    % \bar{\bm{R}}^{c}_{b}(\bm{I} + \delta\phi^{\wedge})\bm{n}^{b}, \\
    \bm{\Sigma}_{\bm{n}^{c}}
    \approx
    [\bar{\bm{R}}^{c}_{b}(\bm{n}^{b})^{\wedge}]
    \bm{\Sigma}_{\bm{\delta\phi}}
    [\bar{\bm{R}}^{c}_{b}(\bm{n}^{b})^{\wedge}]^{\top},
  \end{split}
\end{equation}
where the derived covariance will be used in the optimization objective in Section \ref{sec:ext_calibration}.
% The derivation is detailed in the supplementary material \cite{}.
% \begin{equation}
%   % \underset{\bm{R}^{c}_{b}\in SO(3),\ \bm{t}^{c}_{b}\in\mathbb{R}^{3}}{\arg\min}
%   % \sum_{\bm{p}\in\mathcal{P}_{pat}^{b}}
%   % ||\pi(\bm{p}, \bm{R}^{c}_{b}, \bm{t}^{c}_{b}) - \bm{u}||^{2}_{\bm{\Sigma}},
%   \underset{\bm{R}^{c}_{b}\in SO(3),\bm{t}^{c}_{b}\in\mathbb{R}^{3}}{\arg\min}
%   \sum_{\bm{p}\in\mathcal{P}_{pat}^{b}}
%   f_{pnp}(\bm{p}, \bm{R}^{c}_{b}, \bm{t}^{c}_{b}).
%   % ||\bm{r}(\bm{p}, \bm{R}^{c}_{b}, \bm{t}^{c}_{b}) - \bm{u}||^{2}_{\bm{\Sigma}},  
% \end{equation}
% where $\bm{u}$ is the corresponding 2D projected point of $\bm{p}$ and $\pi(\cdot)$ is the camera projection function.}


% The transformation and planar parameters of the checkerboard are calculated by solving the Perspective-n-Point (PnP) problem \cite{Hartley2004}. 
% Thus, we can determine corner features in the camera coordinate system as $\mathcal{P}^{c}_{cor}$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image Reconstruction From Events}
\label{sec:methodology_ire}
\subsubsection{Event Data}
An event camera has independent pixels that respond to logarithmic intensity change $L$.
In a noise-free scenario, an event $\bm{e}_{k}=[\bm{u}_{k}^{\top},t_{k},p_{k}]^{\top}$ is triggered at pixel $\bm{u}_{k} = [x_{k},y_{k}]^{\top}$ and time $t_{k}$ as soon as the logarithmic intensity increment reaches a contrast threshold $\pm C$ since the last event at the pixel, i.e.
\begin{equation}
  \Delta L(\bm{u}_{k},t_{k})
  \doteq
  L(\bm{u}_{k}, t_{k}) - L(\bm{u}_{k}, t_{k}-\Delta t_{k})
  \geqslant
  p_{k}C,
\end{equation}
where $C > 0$, $\Delta t_{k}$ is the time elapsed since the last event at the same pixel, and the polarity $p_{k}\in\{+1,-1\}$ is the sign of the intensity change \cite{gallego2020event}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
  \caption{LiDAR-Camera Extinsic Calibration}
  \label{alg:ext_refinement}
  \LinesNumbered
  \KwIn{Number of iteration: $I$ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
    Number of LiDAR-camera data pairs: $N$
    Features: $\{\bm{n}^{c}\}$, $\{\mathcal{P}^{c}_{edge}\}$,
    $\{\mathcal{P}^{l}_{pl}\}$, $\{\mathcal{P}^{l}_{edge}\}$\\}
  \KwOut{Estimated extrinsics $\bm{T}^{c}_{l}$;}
  Initialize extrinsics $\bm{T}^{c}_{l,ini}=
    \textit{QPEP-PtPL}(\{\bm{n}^{c}\},\{\mathcal{P}^{l}_{pl}\})$\\
  \ForEach{$M\in\{1,2, \dots, N\}$}
  {
  The set of all candidate extrinsics: $\mathcal{T}=\emptyset$\\
  \While{count of iteration $<I$}
  {
  Randomly select $M$ data pairs: $\{\bm{n}^{c}\}^{\#}$, $\{\mathcal{P}^{c}_{edge}\}^{\#}$,
  $\{\mathcal{P}^{l}_{pl}\}^{\#}$, $\{\mathcal{P}^{l}_{edge}\}^{\#}$\\
  Find corresponding edge points of $\{\mathcal{P}^{l}_{edge}\}^{\#}$:
  $\{\mathcal{E}^{c}\}\subseteq\{\mathcal{P}^{c}_{edge}\}^{\#}$ given $\bm{T}^{c}_{l,ini}$\\
  Optimize extrinsics: $\bm{T}=$
  $\textit{QPEP-PtPL}(\{\bm{n}^{c}\}^{\#},$
    $\{\mathcal{P}^{l}_{pl}\}^{\#},\{\mathcal{E}^{c}\}, \{\mathcal{P}^{l}_{edge}\}^{\#})$\\
  $\mathcal{T}=\mathcal{T}\cup\bm{T}$\\
  }
  Compute the mean of $\mathcal{T}$: $\bar{\bm{T}}$\\
  $\bm{T}^{c}_{l}=\bar{\bm{T}}$ if the mean geometric error is smaller\\
  }
\end{algorithm}

\begin{figure}[t]
  \centering
  \subfigure[]
  {\label{fig:exp_rlfs01_cloud_align}\centering\includegraphics[width=0.257\textwidth]{figure/experiment/version2/rlfs01_aligned-crop.pdf}}
  \subfigure[]
  {\label{fig:exp_rles03_cloud_align}\centering\includegraphics[width=0.22\textwidth]{figure/experiment/version2/rles03_aligned-crop.pdf}}
  \caption{The LiDAR's planar points and edge points are aligned with the board plane of images with the estimated extrinsiscs.}
  \label{fig:exp_cloud_align}
\end{figure}


\subsubsection{Image Reconstruction}
One of the fundamental building blocks for camera calibration is the detection of checkerboard corners.
However, these corner detectors originally designed for frame images are not directly applicable to events due to their intrinsically asynchronous and sparse nature.
Inspired by the work done by Muglikar \textit{et al.} \cite{muglikar2021calibrate}, we resort to a learning-based method called E2VID \cite{rebecq2019high} to reconstruct high-quality frame images from the asynchronous and sparse event stream.
This method encodes events in a spatio-temporal voxel grid and uses a recurrent convolutional neural network based on the UNet architecture \cite{ronneberger2015u} to process events.
The network is already pretrained using a large number of simulated event sequences.
After the reconstruction, we can directly apply the approach in Section \ref{sec:methodology_fsc} on these images for calibration.

The reconstruction procedure is summarized in three steps:
1) Move the checkerboard before the event camera to trigger events.
2) Divide events into chunks of constant time duration ($50ms$ in our experiments). But the time duration of these chunks does not have to be constant. One could choose to define the chunks by the number of events.
3) Reconstruct images from events using E2VID. The corresponding LiDAR frame specifies the reconstruction timestamp.

Besides the above approach, we also have two possible solutions to detect corners from events, but they present several limitations.
The first solution is to aggregate events within a local spatial-temporal window to create an event map that is also an image-type representation \cite{jiao2021comparing}.
But event maps do not record intensity and contain several noisy pixels, making the traditional corner detectors inaccurate.
Another approach is to detect corners from pure events.
However, existing event-based corner detectors \cite{manderscheid2019speed} are not specifically designed for checkerboard corners, and most of them are not publicly released, inducing difficulties in calibration.
In contrast, the proposed reconstruction-based calibration scheme is easy to implement and does not require much parameter tuning.
Experimental results have demonstrated the effectiveness of the proposed reconstruction-based calibration method.

