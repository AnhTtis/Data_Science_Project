\section{Research Method}
Because our study focuses on \textit{why} and \textit{how} companies contribute to OSS, we performed semi-structured interviews \cite{baltes2022sampling, ralph2020empirical} targeting interviewees with different roles that are knowledgeable about the company's involvement in OSS (e.g., OSPO Lead, CEO, Ecosystem Strategist) working at companies of different sizes (7 small, 1 medium, 9 large) \cite{businessSize} from startups to large technology companies such as Microsoft, Google, RedHat, and Spotify. The following two sections detail our data collection, analysis, and member-checking approach.  

\textbf{Recruitment.} We recruited 20 interview participants from 17 different companies of different sizes. We emailed and sent direct messages via Twitter to participants whose contact information was publicly available. Our selection criteria consisted of participants who were either in a leadership position (e.g., Founder, CEO) or in an OSS-related position (e.g., OSPO Lead, Ecosystem Strategist). We first conducted 12 interviews and used a snowball approach to recruit more interviewees. We conducted 8 additional interviews. Table \ref{table:participants} shows the participants, their roles, and their respective company's details. 

\textbf{Data collection and availability.} We arranged meeting dates and times according to each interviewee's availability. All interviews were conducted online. The first author conducted the interviews and started by introducing themselves and the project, and getting permission to record the interview. The interviews were 30 to 60 minutes long. After this, we thanked our participants and compensated them with a 50-dollar gift card as a token of appreciation. The data collection resulted in 20 interviews which were then transcribed. This is in line with the anthropology literature, which states that a set of 10-20 knowledgeable people is sufficient to uncover the core categories in any cultural domain or study of lived experience \cite{bernard2017research}. 
%
The interview transcripts are confidential as per our institutional IRB restrictions. However, to aid verifiability, we have included the interview guide, the codebook with example quotes, and code networks in our supplemental material \cite{suppdoc}. Also, Table \ref{tab:motivationTable} gives examples of how the findings mapped to the observations.

\textbf{Data Analysis.} 
We conducted and analyzed the data iteratively and we used ATLAS.ti\footnote{https://atlasti.com/} to perform the data analysis. Atlas.ti is a software for qualitative research that assists with analytic procedures by providing tagging capabilities and visualizations (mind maps) allowing the researcher to visually examine features and relationships in the “coded” texts (transcripts). We qualitatively analyzed the transcripts by inductively applying open coding, whereby we identified the motivations and contributions that each participant reported. We built post-formed codes as the analysis progressed and associated them to respective parts of the transcribed interview text. We reached code saturation after 13 interviews where no new codes emerged. However, we decided to finish the rest of the 7 interviews as they were already scheduled and made via personal introductions (snowball). Next, we grouped all the codes into categories using ATLAS.ti network. We held weekly meetings with three researchers experienced in the domain and in qualitative research to discuss and adjust codes and categories until we reached agreement. The first author presented and described each category and the researchers went through the codes and the quotes backing them. We discussed the grouped categories, merged categories into higher-level themes, discussed the relationship between them, and adjusted codes and categories until we reached a consensus. This process took 36 weeks. 
For instance, we merged the codes ``engineer-to-engineer communication'' and ``timely user feedback'' under the subcategory name ``closer channels''. When we found the same meaning for a concept that had been coded differently for more than one excerpt, we discussed it until we found the appropriate concept that represented all the coded excerpts. For instance, within the higher-level category ``business advantage,''  we merged the subcategory, ``avoiding technical debt'' with ``business dependency on OSS'' as the need to avoid technical debt stems from the fact that the business depends on OSS. 
In addition to the motivations and contributions mentioned by interviewees, we also collected lessons learned that interviewees shared with us. 

\textbf{Member checking.}
After analyzing all interviews, we performed member checking to evaluate the validity of our findings by emailing our participants an editable document with our findings on company motivations and ways to contribute to OSS. Eight participants (P1, P2, P6, P7, P11-13, P20) provided their feedback. All eight participants verified our findings and did so via the Google document as well as an email response. They all agreed with our results, and P1 and P20 provided only rewording suggestions. P20 suggested rewording ``joining'' dependencies' leadership and governance to ``aspiring to join'' as they felt these roles need to be earned and voted in by the community (see section \ref{sec:developertime}). P1 suggested rephrasing ``gaining user fidelity'' in the description of ``building verifiable trust'' to ``gaining community trust'' (see section \ref{sec:reputation}) which to them then might result in user fidelity. We addressed the feedback from both participants by specifically rewording the respective parts of the results using their suggestions.

 
\input{tables/participants}


