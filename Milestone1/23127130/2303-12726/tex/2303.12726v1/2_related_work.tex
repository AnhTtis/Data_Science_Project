\section{Related Work}

\subsection{Hand Manipulation in Animation}
Generating animation of realistic hand manipulation skills has been a long-studied topic in animation.\revised{Wheatland and colleagues ~\cite{wheatland2015state} summarizes the state of the art in modeling and animation in hand manipulation problems.} Early work exploited traditional optimal control theory to generate hand manipulation skills for rigid objects~\cite{liu2009dextrous,mordatch2012contact,bai2014dexterous} or soft bodies~\cite{bai2016dexterous} by planning contact forces. Instead of solving an optimal control problem, Ye and colleagues~\cite{ye2012synthesis} proposed a randomized sampling algorithm to find a set of diverse strategies.  Zhang~\etal~\cite{zhang2021manipnet} developed a data-driven approach for synthesizing a variety of object manipulation motions via deep learning, but the generated motions are purely kinematic \revised{instead of} simulated with physics. \revised{Reinforcement learning has also been used in generate manipulation animations. Andrews ~\etal ~\cite{andrews2013goal} trains goal directed policies for one-handed manipulation without guidance from motion capture data.} Another line of work has focused on synthesizing grasping motions for simulated hands~\cite{li2007data,christen2021d,hwang2021primitive,pollard2005physically} or whole-body characters~\cite{wu2021saga}. Inspired by prior work, we tackle the problem of hand manipulation by leveraging recent advances in DRL and physics-based simulation.

Our approach of adapting an existing manipulation motion to different objects can also be cast as a motion retargeting problem. Retargeting of complex interactions often considers the spatial relationship of two people \cite{ho2010interaction,jin2018auramesh}, or between the person and the environment \cite{alasqhar2013descriptor}, or through analysis of the object geometry and affordance in the case of manipulations \cite{simeonov2022ndf,taheri2020grab}, without enforcing physical correctness in the results. Recent work starts to utilize geometry representation to improve the learning of manipulation control \cite{she2022highdof}. \revised{More recently, Yang ~\etal ~\cite{yang2022learning} trains policies to control hands to manipulate chopsticks through reconstructing hand motions then then retarget to finish the manipulation tasks.} We instead show that direct imitation learning can be successful in retargeting to different objects with a well designed curriculum. 

\subsection{Robotic Hand Manipulation}
Dexterous hand manipulation has also been an essential topic in robotics. Early work~\cite{huang2000mechanics,sawasaki1991tumbling,han1997dextrous,sundaralingam2018geometric} typically casts manipulation into motion planning problems, which allow robots to achieve various motions, such as tapping, tumbling, or rolling manipulations. However, these motion planning approaches often generate open-loop trajectories which cannot handle dynamic perturbations. This limitation has been overcome by incorporating tactile sensor feedbacks during manipulation~\cite{tahara2010dynamic,li2013rotary}. These methods can allow certain amounts of deviation from the planned trajectory, but they often require accurate dynamic models for both hands and objects. 

Because of hand manipulation's complex and discontinuous nature, researchers have investigated deep reinforcement learning (DRL) algorithms that can solve challenging motor control problems. A common approach~\cite{zhu2019dexterous,nagabandi2020deep,charlesworth2021solving,andrychowicz2020learning,akkaya2019solving} is to cast manipulation into an MDP directly to learn skills without human demonstrations. Andrychowicz~\etal~\cite{andrychowicz2020learning} demonstrated the successful learning of dexterous cube manipulation on a real robot hand by combining pose estimation and deep reinforcement learning. The work is extended to solve a Rubik's cube with a humanoid robot hand by applying proper randomization to system dynamics and designing a curriculum with increasing difficulty. Note that training times for these policies are high, with the Rubik's cube policy requiring several months of training on 64 V100 GPUs and 920 worker machines.

While focusing on a single type of object, other work~\cite{chen2021system,huang2021generalization} trained a control policy in simulation to reorient a wide variation of geometries in hand, including the YCB dataset~\cite{calli2017yale} and various toys. However, this approach of finding a manipulation policy without human demonstrations often leads to unnatural and jerky motion. Many researchers~\cite{gupta2016learning,rajeswaran2017learning,radosavovic2021state,jeong2020learning,kumar2016learning} also have utilized human demonstration to complete dexterous manipulation tasks. For instance, Garcia~\etal~\cite{garcia2020physics} developed a learning framework by estimating the needed contacts to accomplish the tasks on a physics simulator from human motions. Our approach also uses deep reinforcement learning to solve a dexterous hand manipulation problem from the captured hand and object movements.

% \vspace{-1cm}


\subsection{Learning to Imitate Reference Motions}
Since the pioneering work of \revised{Liu~\etal \cite{liu2010sampling,liu2015improving} and }Peng~\etal~\cite{peng2018deepmimic}, learning to imitate a given reference motion has been a promising approach for developing natural and effective physics-based motion controllers. Starting from a short motion clip of walking, running, or kicking, researchers~\cite{peng2018sfv,lee2019scalable,peng2020learning,park2019learning,chentanez2018physics,merel2018neural} have extended the motion imitation framework from various perspectives, including interactivity, data diversity, and character complexity. For instance, researchers trained an interactive controller to complete locomotion tasks from a wide range of walking motions. Won~\etal~\cite{won2020scalable} invented a scalable motion imitation framework by employing an ensemble of expert networks. Lee~\etal~\cite{lee2019scalable} trained a full-body muscular skeleton character to track the motion capture data by introducing a hierarchical controller. Researchers~\cite{won2021control} also developed a technique to solve control of multiple characters in competitive games with a manually designed multi-stage learning. Some prior work~\cite{lee2021learning,won2019learning} have discussed how to adapt policies to variants of the training mocap sequences, rather than simply following the given reference motion. Our work also employs the motion imitation framework while applying it to dexterous hand manipulation, which has \revised{not yet} been fully investigated with motion imitation.

\subsection{Curriculum learning}
Curriculum learning~\cite{bengio2009curriculum} is one of the common techniques for solving challenging problems. It generates tasks with increasing difficulties until it finds solutions for the most difficult scenarios. Many prior works~\cite{karpathy2012curriculum,heess2017emergence,yin2008continuation,tang2021learning} design a curriculum manually using prior knowledge to train locomotion controllers for complex tasks. Instead of a naive curriculum, recent works \cite{won2019learning,xie2020allsteps} propose adaptive curriculum methods based on the value function of the trained policy. In another closely related line of work, researchers \cite{wang2019paired,wang2020enhanced} propose algorithms to design an open-ended curriculum for policies and environments without specifying any target task. In \revised{our} work, we take inspiration from these ideas and propose a new curriculum learning framework, Greedy Shape Curriculum, to generate complex manipulation trajectories with various objects.
% When a target task is too difficult to solve with direct training, curriculum-based learning methods \cite{bengio2009curriculum} are often applied to gradually solve tasks with increasing difficulties, and eventually reaches solution for the most difficult scenario. Using naive human specified curriculum, prior research \cite{karpathy2012curriculum,heess2017emergence,yin2008continuation} are able to train locomotion controllers for complex tasks. Instead of naive curriculum, recent works \cite{won2019learning,xie2020allsteps} propose adaptive curriculum methods based on the value function of the trained policy. In another closely related line of work \cite{wang2019paired,wang2020enhanced}, there is no target task specified, and the goal is to generate both the policy and the environment based on an open-end curriculum. In out work, we take some inspirations from these ideas and propose the new curriculum learning framework to generating complex manipulation trajectories with various objects.
