\section{Results}

\def \fw {0.18}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=\fw\textwidth]{figures/sequence_still_frames/teapot_morph_0.png}
    \includegraphics[width=\fw\textwidth]{figures/sequence_still_frames/elephantcube3_cube.png}
    \includegraphics[width=\fw\textwidth]{figures/sequence_still_frames/cube_bunny_transfer_fig8_top.png}
    \includegraphics[width=\fw\textwidth]{figures/sequence_still_frames/cube_train_transfer_fig8_top.png}
    \includegraphics[width=\fw\textwidth]{figures/sequence_still_frames/cylinder_transfer_fig8.png}
    \begin{subfigure}{\fw\textwidth}
    \includegraphics[width=\textwidth]{figures/sequence_still_frames/teapot_morph_5.png}
    \caption{}
    \end{subfigure}
    \begin{subfigure}{\fw\textwidth}
    \includegraphics[width=\textwidth]{figures/sequence_still_frames/elephantcube3_elephant.png}
    \caption{}
    \end{subfigure}
    \begin{subfigure}{\fw\textwidth}
    \includegraphics[width=\textwidth]{figures/sequence_still_frames/cube_bunny_transfer_fig8_bottom.png}
    \caption{}
    \end{subfigure}
    \begin{subfigure}{\fw\textwidth}
    \includegraphics[width=\textwidth]{figures/sequence_still_frames/cube_train_transfer_fig8_bottom.png}
    \caption{}
    \end{subfigure}
    \begin{subfigure}{\fw\textwidth}
    \includegraphics[width=\textwidth]{figures/sequence_still_frames/cylinder_transfer_fig8_bottle.png}
    \caption{}
    \end{subfigure}
    \caption{Examples of transferring the manipulation of an original shape (top) to a target shape (bottom). Left to right, the examples shown are: cube to teapot, cube to elephant, cube to bunny, cube to toy train, and cylinder to bottle.}
    \label{fig:greedy_shape_curriculum}
\end{figure*}

To validate our approach, we evaluate the performance of our policy on a variety of mocap sequences. These include single-handed manipulations of several distinct objects, as well as passing and rotating an object between two hands. We show that the resulting policies can endure a moderate amount of dynamic perturbations. We also demonstrate successful transfer of manipulations to novel everyday objects using our greedy shape curriculum. Please refer to the accompanying video for visual evaluation.

For all of our results, we use Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, a common on-policy reinforcement learning algorithm. The policy architecture is a fully connected network with two hidden layers, each of which has $256$ units with $tanh$ activation functions. Each policy is trained with 32 million observation/action pairs over 800 iterations. This training setting is held consistently across all trainings for original object sequences and shape morphs.

Because our source shapes are convex, we can use a simple projection-based shape morphing technique in order to generate intermediate shapes. To create the morphs, we project all vertices from the target shape to the surface of the source shape. We then use the intermediate positions along the projection paths as the vertex positions for each morph. For each shape pair, we create four intermediate morphs with vertices linearly interpolated between the corresponding positions on the source and target shapes. To create Mujoco compatible models, we use the commonly used convex decomposition method V-HACD\cite{mamou2016volumetric} with a maximum voxel resolution of $40 \times 40 \times 40$ to generate a collection of convex parts for each of the intermediate morphs. All the morphing operations are retrieved from a pre-processing stage in Blender~\cite{blender}. Figure \ref{fig:collision_shape_morphs} shows an example of the collision shapes of the source, target, and all intermediate morphs between a cube model and a bunny model. In later sections, we refer to each shape by its interpolation distance from the original shape along with the shape name. For any intermediate shape, the shape name is referred to as "Morph". For example, the sequence of morphs from the cube to the bunny are denoted as Cube(0.0), Morph(0.2), Morph(0.4),Morph(0.6),Morph(0.8), and Bunny(1.0).


We conduct our experiments on AWS C4 compute nodes with $36$ virtual cores on each machine. Each learner process executes $800$ iterations of PPO, collecting $32$ million rollout samples in total with $8$ parallel worker processes at a rate of $350$ frames per second. This results in an end-to-end policy training duration of about $40$ hours, depending on the complexity of the object's geometry and the number of contacts.



% \begin{table}[t!]
% \vspace{3mm}
% \caption{Success rate on example sequences with stochastic policy execution (500 samples). If tracking is perfect, cumulative reward for short-horizon sequences will be 500, and that for long-horizon sequences will be 1000.}
% \vspace{-3mm}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% Experiment & Sequence & Success \% & Mean Reward \\
% \hline
% \multirow{9}{*}{\makecell{Single Hand \\ (short horizon)}}& Cube 1 & 97.8 & 490.63\\
% \hhline{~---}
% & Cube 2 & 87.2 & 459.87\\
% \hhline{~---}
% & Cylinder 1 & 95 &  477.65\\
% \hhline{~---}
% & Tetrahedron 1 & 95.2 &  478.51\\
% \hhline{~---}
% & Tetrahedron 2 & 99.8 &  497.06\\
% \hhline{~---}
% & Hemisphere 1 & 91.4 &  461.84\\
% \hhline{~---}
% & Torus 1 & 83.8 &  445.97\\
% \hhline{~---}
% & Wineglass 1 & 68.4 &  394.54\\
% \hhline{~---}
% & Wineglass 2 & 99.2 &  491.73\\
% \hline
% \makecell{Single Hand \\ (long horizon)} & Long Cube Single & 97.0 & 956.69 \\
% \hline
% \multirow{2}{*}{\makecell{Two Hand \\ (short horizon)}} &Cube Passing & 93.4 & 471.74 \\
% \hhline{~---}
% &Cube Rotation & 75.6 & 403.32 \\
% \hline
% \makecell{Two Hand \\ (long horizon)} & Long Cube Both & 30.6 & 363.05 \\
% \hline

% \end{tabular}
% \end{center}
% \label{tbl:success_rate_spcecialized}
% \end{table}

\subsection{Reproducing single dynamic sequences}
We first demonstrate the success of our approach across a diverse set of sequences involving a variety of objects and manipulation skills. First, we train a set of policies to track single-handed manipulation sequences, involving different objects over a $4$-second horizon. These results indicate that our approach is successful for sequences involving both primitive convex objects such as cubes, cylinders, and hemispheres, as well as more complex, concave geometries such as a torus and a wineglass. Figure~\ref{fig:hemisphere_large1_single} (\textbf{top}) shows still frames from the manipulation of a wineglass, an example of a non-convex object, and (\textbf{bottom}) shows still frames from rotating a hemisphere in the hand. Additionally, we show that our approach can be trivially extended to control two-handed manipulations that require coordination, such as passing and cooperative rotation shown in Figure ~\ref{fig:teaser}.

\begin{table}[b!]
\vspace{3mm}
\caption{Success rate on example sequences with stochastic policy execution (500 samples). The maximum cumulative reward is 500.}
\vspace{-3mm}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Experiment & Sequence & Success \% & Cumulative Reward \\
\hline
\multirow{9}{*}{\makecell{Single Hand }}& Cube 1 & 94 & 463.16\\
\hhline{~---}
& Cube 2 & 81.6 & 415.18\\
\hhline{~---}
& Cube 3 & 98.6 & 480.45\\
\hhline{~---}
& Cylinder 1 & 75.6 &  403.88\\
\hhline{~---}
& Hemisphere 1 & 88.6 &  431.58\\
\hhline{~---}
& Torus 1 & 85 &  432.55\\
\hhline{~---}
& Wineglass 1 & 94.2 &  458.99\\
\hline
\multirow{2}{*}{\makecell{Two Hand}} &Cube Passing & 82.6 & 417.66 \\
\hhline{~---}
&Cube Rotation & 99.6 & 482.63 \\
\hline

\end{tabular}
\end{center}
% \vspace{-0.15in}
\label{tbl:success_rate_spcecialized}
\end{table}

Even though each policy is trained to follow one object manipulation sequence, it is able to endure moderate dynamic perturbations. To demonstrate this, at random frames through the simulation, we apply forces of $8N$ on all fingertips for $0.25$ second. Despite these perturbations, the policies adapt to these forces and still successfully track the motion. Similarly, a policy can easily track a motion sequence when modifying the masses and friction coefficients of the manipulated objects. These results are best seen in the supplementary video.

We summarize a quantitative analysis of success rates and mean reward of our final policies across the full set of motions in Table~\ref{tbl:success_rate_spcecialized}. To compute each entry in the table, a sample population of $500$ episode rollouts are initialized from the first frame of each motion capture sequence, and stochastic policy actions are applied until the final frame is reached successfully or the object tracking error crosses a threshold value. For a more detailed analysis and comparison, we cache the episode length of each trial, and plot the percentage of the rollouts that meet or exceed a range of rollout length thresholds.
In our experience, a success rate was too simple to capture the performance of the policy for our problem because it does not capture various difficulties of different frames. To this end, we plot ``completion percentage'' that shows the percentage of successful policies (Y-axis) to reach the given frame (X-axis). We summarized completion percentages in Figure \ref{fig:no_variation_threshold}. The figure shows how difficult each sequence is, and how often a final trained policy can successfully track the object to the end of the sequence. 
For instance, the cylinder 1 motion shows a significant drop of the completion percentage around frame $300$, where thumbs are having trouble rotating a standing cylinder back into the palm, and thus where the cylinder may fall out of the hand.
\begin{figure}[t]
    \centering
    % \includegraphics[width=0.49\linewidth]{figures/no_variation/Single Variation Short.png}
    % \includegraphics[width=0.49\linewidth]{figures/no_variation/Single Variation Long.png}
    \includegraphics[width=\linewidth]{figures/no_variation/sensor.png}

    \caption{Completion percentage of all sequences. This plot illustrates the relationships between the difficulty of a given frame and the associated success rate. The least performing sequence (Cylinder1) has more than 75\% success rate to complete the entire rollout.}
    \label{fig:no_variation_threshold}
    \vspace{-0.18in}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=0.46\textwidth]{figures/BunnyCube1MoreSeedGSC_end_point_Seed0.png}
\includegraphics[width=0.46\textwidth]{figures/ElephantCube3MoreSeedGSC_end_points_Seed84.png}
\begin{subfigure}{0.46\textwidth}
\includegraphics[width=\textwidth]{figures/BunnyCube1MoreSeedNaiveCurr_end_point_Seed0.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.46\textwidth}
\includegraphics[width=\textwidth]{figures/ElephantCube3MoreSeedNaiveCurr_end_point_Seed84.png}
\caption{}
\end{subfigure}

\caption{
Comparison between our greedy shape curriculum (top) and naive curriculum (bottom) method. Part (a) shows policy training for cube-to-bunny, and part (b) shows cube-to-elephant. Note that our method often trains more quickly than the naive approach. The graph at the bottom of (a) shows failure of the naive approach to even find a solution. The blue dashed lines show which shape is being trained at each iteration. Black dots indicate which shapes can receive higher goodness score when using the currently trained policy. Green dots indicate the first successful iteration with the corresponding goodness score, and red dots means the shape was not able to be solved when training completes.
}

\vspace{-0.18in}
\label{fig:dependency_plot_comparison}
\end{figure*}
\subsection{Transferring to Complex Objects using a Greedy Shape Curriculum}\label{ssec:policy_transferring}

We demonstrate that using our greedy shape curriculum, we can easily train policies that can manipulate both the source object and the target object by tracking a provided motion clip. For example, Figure \ref{fig:greedy_shape_curriculum} shows the simulation result after adapting the manipulation of a simple shape (e.g. cube or cylinder) to a more complex object (teapot, elephant, bunny, toy train, bottle).

%\begin{figure*}[h!]
%    \centering
%    \includegraphics[height=\high]{figures/sequence_still_frames/torus_0.png}
%    \hfill
%    \includegraphics[height=\high]{figures/sequence_still_frames/torus_1.png}
%    \hfill
%    \includegraphics[height=\high]{figures/sequence_still_frames/torus_2.png}
%    \hfill
%    \includegraphics[height=\high]{figures/sequence_still_frames/torus_3.png}
%    \hfill
%    \includegraphics[height=\high]{figures/sequence_still_frames/torus_4.png}
%    \hfill
%\end{figure*}

%Defining width ratio as a variable to more easily add/remove images here:


We compare our method with a naive curriculum learning method where a policy will only proceed to the next shape morph when the previous one receives a goodness score above the threshold. Two examples are shown in figure~\ref{fig:dependency_plot_comparison}. In the figures, blue dashed lines indicate the policy training progression, and the black dots indicate which shapes are receiving a better score when evaluating the current policy. We highlight the first success of each shape in green dots labeled with the respective score. Greedy shape curriculum results are shown on the top row, and the naive curriculum results are on the bottom row.

Figure~\ref{fig:dependency_plot_comparison} (a) shows an example in which the intermediate morphs do not introduce a smooth linear curriculum.  The naive curriculum (bottom row) spends the majority of its compute budget on Morph(0.2) and eventually stops at Morph(0.8) without reaching the target shape. In contrast, the greedy shape curriculum (top row) skips training on these two difficult morphs, yet still finds a successful policy for the target shape by utilizing the other intermediate morphs. Training starts on Morph($0.4$) and finishes much sooner than the $800$ iteration budget at iteration $320$, by alternating among Cube(0.0), Morph(0.6), and the target Bunny(1.0). The two difficult shapes, Morph(0.2) and Morph(0.8), do not get trained on and are not successful at the end.

%top row, our successful policy for the Bunny is inherited from the policy trained for Morph($0.6$) at iteration $120$ without utilizing other intermediate morphs. It is able to reach the goodness threshold at iteration $320$ much sooner than the $800$ iteration compute budge. In contrast, the naive linear curriculum in figure~\ref{fig:dependency_plot_comparison} (a) bottom row first spent a lot of compute on a difficult shape Morph(0.2), then got stuck again on Morph(0.8) and failed to reach the target shape. Our curriculum skips both difficult shapes yet is still successful. 

%, where the next morph is selected once the previous morph is successful. When an intermediate morph does not introduce a smooth curriculum, or it may even be more difficult than the target shape, this naive method will easily fail to reach a successful policy for the target shape.

Our greedy shape curriculum can also be more efficient than the naive curriculum in easy cases when both methods succeed. In Figure~\ref{fig:dependency_plot_comparison} (b), the greedy shape curriculum picks the target shape Elephant(1.0) for training at iteration $80$ by inheriting the policy trained on Morph($0.6$) at iteration $20$. After further training, it is able to successfully manipulate the elephant at iteration $120$. On the other hand, the naive curriculum has to receive high goodness score on all five previous shapes before succeeding on the target shape Elephant(1.0) at iteration $240$. 
%As shown in Figure~\ref{fig:dependency_plot_comparison} (a), the greedy shape curriculum (top row) quickly finds a policy for the bunny.  In contrast, the naive curriculum (bottom row) gets stuck at Morph(0.8) and never succeeds. 

To highlight the benefits of our method, we compare greedy shape curriculum with four different baselines: 1) Training a single policy on the target shape; 2) Training a single policy over both the source and the target shapes; 3) Training a single policy over the collection of morphs from the source and the target shapes; 4) Training a collection of policies using a naive curriculum over all morphs. For each of the baselines, we take the mean policies trained from four random seeds, apply them on the target shape, and record whether or not they can successfully complete the rollout without hitting the early termination criterion. All trials are trained using 32 million samples over 800 iterations. As shown in Table ~\ref{tbl:baselines_comp}, Greedy Shape Curriculum has the highest likelihood of producing working policies for all the target shapes under a fixed sample budget. In comparison, training on the target shape directly has a lower chance of success. Using a naive curriculum may be helpful in some cases but could be harmful in others, because only a subset of the sample budget is allocated to improve the target shape. On the other hand, training one generalized policy on multiple shapes is making the problem more challenging and is therefore more difficult to succeed, especially when the compute budget is limited.

%On the other hand, if we train the policies using naive curriculum, it is less likely for the target shape policy to be successfully trained. For the other three baselines, where a generalized policy is trained on one or multiple shapes, we see that it is very difficult for one policy to adapt to multiple shapes for a given mocap sequence.

\begin{table*}[t!]
\vspace{3mm}
\caption{Comparing the success rates of our method against four different baseline methods. Each method is trained with four random seeds.}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Original Sequence - Target Shape  & Direct Target & Source + Target & All morphs & Naive Curriculum & Greedy Shape Curriculum \\
\hline
Cube1 - Teapot & 50\% & 0\% & 0\% & 50\% & \textbf{75\%} \\
\hline
Cube1 - Bunny & \textbf{100\%} & 0\% & 0\% & 50\% & \textbf{100\%} \\
\hline
Cube1 - Train & \textbf{100\%} & 50\% & 0\% & 75\% & \textbf{100\%} \\
\hline
Cube2 - Bunny & 50\% & 0\% & 0\% & 0\% & \textbf{100\%} \\
\hline
Cube2 - Elephant & \textbf{25\%} & 0\% & 0\% & 0\% & \textbf{25\%} \\
\hline
Cube3 - Elephant & 50\% & 0\% & 0\% & \textbf{100\%} & \textbf{100\%} \\
\hline
\end{tabular}
\end{center}
\vspace{-0.15in}
\label{tbl:baselines_comp}
\end{table*}
% \begin{table*}[t!]
% \vspace{3mm}
% \caption{Comparison of five different methods by evaluating the policies from trainings using four random seeds. The values indicate the number of policies that can successfully manipulate the original shape (left value) and the target shape (right value).}
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|c|}
% \hline
% Original Sequence - Target Shape  & Direct Target & Source + Target & All morphs & Naive Curriculum & Greedy Shape Curriculum \\
% \hline
% Cube1 - Teapot & 1,2 & 4,0 & 4,0 & 4,2 & 4,3 \\
% \hline
% Cube1 - Bunny & 1,4 & 4,0 & 4,0 & 4,2 & 4,4 \\
% \hline
% Cube1 - Train & 1,4 & 4,2 & 4,0 & 4,3 & 4,4 \\
% \hline
% Cube2 - Bunny & 0,2 & 4,0 & 4,0 & 1,0 & 2,4 \\
% \hline
% Cube2 - Elephant & 0,1 & 4,0 & 4,0 & 1,0 & 3,1 \\
% \hline
% Cube3 - Elephant & 0,2 & 4,0 & 4,0 & 4,4 & 4,4 \\
% \hline
% \end{tabular}
% \end{center}
% \vspace{-0.15in}
% \label{tbl:baselines_comp}
% \end{table*}

% To further compare between naive curriculum and greedy shape curriculum and show our goodness score serves as a good heuristic on actual policy success, we record the iteration number it takes for a training to have the goodness score passing the pre-specified threshold, and evaluate both the original-shape-policy and the target-shape-policy on whether or not they will survive the rollouts. We collect such data from the training using each random seed for each motion sequence. As shown in Table ~\ref{tbl:curriculum_comp}, greedy shape curriculum is able to produce successful policies for target shape using fewer samples and with a higher success rate.

% \begin{table*}[t!]
% \vspace{3mm}
% \caption{Comparison between the naive curriculum method and greedy shape curriculum method. The first iteration of a policy that is above the goodness score threshold is recorded. For those that are not exceeding the prescribed threshold, policies at iteration 800 are used for the evaluation.}
% \begin{center}
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% Original Sequence - Target Shape & Above Threshold (Naive) & Succeed Target (Naive) & Above Threshold (Ours) & Succeed Target (Ours)\\
% \hline
% Cube1 - Teapot  & ---,---,---,800 & T,F,F,T & 680,---,---,--- & T,F,T,T\\
% \hline
% Cube1 - Bunny  & ---,380,---,--- & F,T,F,F & 320,200,200,320 & T,T,T,T\\
% \hline
% Cube1 - Train & 300,240,300,--- & T,T,T,F & 380,220,260,240 & T,T,T,T \\
% \hline
% Cube2 - Bunny & ---,---,---,--- & F,F,F,F & 580,440,180,360 & T,T,T,T \\
% \hline
% Cube2 - Elephant  & ---,720,---,600 & F,F,F,F & ---,700,---,700 & T,T,T,T \\
% \hline
% Cube3 - Elephant & 200,200,240,160 & F,T,T,F & 80,160,120,140 & T,T,T,T  \\
% \hline
% \end{tabular}
% \end{center}
% \vspace{-0.15in}
% \label{tbl:curriculum_comp}
% \end{table*}

\subsection{Refining successful policies}
\revised{Due to the simplification on disabling the self collision within the hand, the generated animation sometimes have the hand interpenetrating between the fingers. To mitigate the issue, we take the trained policy on target morph from the result of Greedy shape curriculum, and refine the training with a hand model with self collision enabled. This simple refinement can often result in more physically realistic motion qualities for the manipulation. A comparison is shown in Figure \ref{fig:comparing_refinement}}.

\begin{figure}[h!]
    \centering
    \includegraphics[height=\high]{figures/refinement/teapot_penetration_no_refinement_236.png}
    \hfill
    \includegraphics[height=\high]{figures/refinement/teapot_no_penetration_refinement_236.png}
    \caption{Interpenetration between fingers (\textbf{left}) is resolved after refining the policy with self-collisions enabled (\textbf{right}).}
    \label{fig:comparing_refinement}
    \vspace{-0.1in}
\end{figure}