\section{Limitations}

% \alex{Just listing possible topics here:
% - No Memory
% - Better geometry representation for object transfer  generalization
% - Each sequence has its own challenges and possibly different optimal contact parameters
% - how to combine skills or generalize across objects?
% - longer sequences and number of distinct manipulation types?
% - Round objects are harder?
% - Inherent gap between real and simulated dynamics (e.g. deformable vs. rigid manipulator)
% - Manipulation tasks with environmental interactions (e.g. bouncing a ball, picking/placing, turning key in a key-hole, etc...)
% }

%\greg{I took some of your ideas, Alex, and including them as future work in Conclusions.}
Although we have been successful in tracking reference manipulations, we still sometimes observe noticeable penetrations between the hand and the object. Such artifacts are due to the contact handling mechanism in the physics engine. They are more pronounced when the object has a narrow or thin feature, such as the stem of the wine glass. 
\revised{In addition, we found that having a self-collision enabled hand makes the physics simulation very fragile that rollouts can easily get early terminated during training. Successful policies on target morphs can hardly be trained under this setup, and we end up disabling the self-collisions within the hand, and lead to some finger interpenetration in our results. Even though further refining the resulting policies may mitigate the artifacts, it's not guaranteed to reach an interpenetration-free rollout.}
%In all the cases that we have tried, we have been able to produce a policy that successfully mimics the motion of the specific object used during motion capture.  Sometimes, however, the resulting animation can show unrealistic intersection of the object with the fingers.  This limitation in realism comes from the  physics engine allowing some between-object intersection even with hard contact settings. Such cases are more pronounced when the object has a narrow part, such as the stem of the wine glass.


%For example, trying to adapt the motion for one object to another that is very different in size will often result in failure.
%One potential approach for supporting a wider variety of shapes is to make policies aware of a detailed description of the shape, perhaps using a voxel grid or a distance field \cite{zhang2021manipnet,christen2021d,akkaya2019solving}.  We leave such possibilities for future work.

% One possible way to do this would be to give the policy some history information, so it might deduce the shape of an object early in the sequence, and then continue to use this information throughout the rollout.

We are still limited in how much the target shape can deviate from the source shape, and we cannot guarantee to always find a successful policy. With extreme changes in shape or size, the original manipulation strategy as captured may no longer be suitable or feasible. Related to this, our method cannot discover and explore novel manipulation skills that deviate significantly from the input. Even when successful (eg. the object is not dropped), the resulting finger motions may at times appear unnatural. The simple imitation term in the reward function and in the goodness score trades off exploration and exploitation, and could become a limiting factor in inventing new ways to interact with an object. An Adversarial Motion Prior~\cite{peng2021amp} could be a promising mitigation.

Lastly, we found that some mocap sequences are particularly difficult to simulate because of noisy or erroneous frames. Although the motion imitation can learn a robust policy and ``correct'' these invalid frames via physics simulation, they tend to significantly slow down the learning process.
%One interesting future research direction would be to develop a general pre-processing technique on mocap data to correct those challenging frames and help the learning to find a more robust policy.

% Lastly, we learned that different mocap sequences gives different difficulty level, and if the sequence is not picked wisely, a policy might give lower success rate as shown in Figure \ref{fig:no_variation_threshold}. A challenging direction would be to find a general pre-processing technique on mocap data so that it can lower the difficulty of a sequence, and make a robust policy easier to train. 
%An important part of the reward function is the term that penalizes deviation from the original motion.  This approach does not allow enough latitude to find entirely new ways to interact with a given object.
