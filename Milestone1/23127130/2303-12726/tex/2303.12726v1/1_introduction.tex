\section{Introduction}

%The wide variety of skills that people show when manipulating objects with hands is a crucial ingredient in creating animation of realistic human avatars. We aim to replicate the sophistication of a human hand in a physically simulated environment. Human hands, with their characteristic opposable thumbs, allow us to perform a number of different dexterous tasks, including picking up, rolling, turning, re-gripping, passing, and putting down objects of various shapes and sizes. However, synthesizing such dexterous manipulation animations has been challenging because they must be robust to geometry variations of the object and frequent contact changes.

Why do cartoon characters have four digits instead of five \cite{bbc}? It is because the hand is notoriously time-consuming to animate, whether in drawing or digitally, owing to its high degree of articulation. While finger motions can now be tracked by consumer devices as reference \cite{mediapipe,han2020megatrack}, hand-object manipulation remains challenging to capture accurately and conveniently. Moreover, once captured, fixing and editing the result can often be as difficult and tedious as animating from scratch, because both the hand and the object need to move in coordination and convey the physical realism of the interaction. Among all hand-object interactions, in-hand manipulation is the hardest to animate and edit (when compared to other motions such as grasping and pick-n-place), due to frequent contact changes and the dynamic nature of the interaction.

In this work, we apply an imitation learning framework based on deep reinforcement learning (DRL) to physically simulate a reference motion of in-hand object manipulation. We further extend this learning framework to adapt the reference motion to target objects of different shapes and sizes than the source object. As a result, we can capture manipulations of simple primitive objects with the desired styles and actions, and then adapt the hand motion to manipulate more complex objects with various physical properties in a physically realistic way.

\begin{figure}[ht]
\vspace{3mm}
\centering
\includegraphics[width=0.46\textwidth]{figures/fig_1.png}
\caption{Example of motion transfer from a cube to three different objects (left) and a two-hand manipulation (right).}
\label{fig:teaser}
\vspace{-0.5cm}
\end{figure}

%Our research uses deep reinforcement learning (DRL) to produce hand motion policies that we can use to create animation of various forms of object manipulation.
Our choice of DRL is inspired by its explosive success in simulating full body motions, including walking, running, jumping, ball bouncing and catching~\cite{won2020scalable,yu2018learning,liu2018learning,merel2020catch}. However, it has not been applied as widely to animations of object manipulation. The robotics community is successful in applying DRL to functional manipulation tasks with manipulators, such as object re-orientation or peg-in-hole \cite{rajeswaran2017learning,akkaya2019solving,chebotar2019closing}, but with less emphasis on naturalness or realism of the hand motion. To achieve natural manipulations, the computer vision community instead captures the kinematic motions of hands interacting with objects from human performance \cite{taheri2020grab,chao2021dexycb,Brahmbhatt_2020_ECCV,brahmbhatt2019contactdb,han2018online}. The resulting datasets enable the synthesis of convincing grasping poses of a rich set of objects \cite{taheri2022goal,wu2021saga,christen2021d,grady2021contactopt}, as well as dexterous and skillful finger maneuvers \cite{zhang2021manipnet}. However, these results are focusing on the kinematic motion and do not enforce proper physical constraints of the interaction. Our work benefits from high-quality and realistic motions of object manipulation, and we further enable physical simulation of the interactions such that our results are responsive to dynamic situations and are physically consistent.

An important challenge of object manipulation stems from the limitless possibility of object shapes. While human hands can effortlessly adapt to geometry variations, control policies have a harder time adjusting. Naive fine-tuning of an existing policy to new shapes is rarely successful. A sensible strategy is then to design a curriculum of increasingly difficult tasks to progressively guide the learning towards solving the target shape. A natural choice is then to use a series of shape morphs between the source and the target objects. Unfortunately, the task difficulty and training progress do not correlate with shape progression, which makes a naive linear curriculum ineffective. Instead, we take inspiration from the work of Wang et al.~\cite{wang2019paired} that generates an open-ended curriculum with increasing complexity by co-evolving the policies and training environments. To this end, we propose a \emph{Greedy Shape Curriculum} across the morph geometry variations. During training, we keep track of the most successful policy for each shape, and pick the most promising shape-policy pair for further training. After a few training epochs, we update the shape-policy pairs by testing the new policy on all shape morphs, then repeat the process. This strategy can effectively adapt an example manipulation of a simple primitive object to a diverse set of everyday objects automatically.

% \original{Methods for object manipulation from computer vision and robotics do not necessarily provide satisfactory solutions for computer animation.  Related methods from computer vision typically considers just a single grasping pose and do not enforce hard physics constraints. Robotics approaches emphasize functional tasks such as object re-orientation or peg-in-hole with no regard to naturalness or realism.  Realistic hand motions are relaxed and fluid, and we seek such natural motions for our animation sequences.} 

% For instance, the reference state initialization strategy~\cite{peng2018deepmimic} cannot be applied as-is due to erroneous and sometimes severe contact penetrations, especially when the simulated object differs significantly from the capture object.

% We take as our starting point the motion imitation framework of Peng et al.~\shortcite{peng2018deepmimic}.  Using motion capture (mocap) data of both the hand and an object in the hand, we train a hand animation policy that is capable of performing the same object manipulation.  There are several issues that make this seemingly straightforward approach challenging.  In-hand object manipulations are contact rich, and handling these contacts necessitates both carefully tuning the contact model of the physics simulator, and providing the appropriate contact states for the policy to act on.  The motion of the hand and objects are closely coupled, and the object is solely actuated through hand contacts.  And much of the time, the object is not in static equilibrium.  Interesting interactions between hand and object are often substantially longer in duration than motions such as a walking gait cycle. Moreover, there are many different sizes and shapes of objects that we handle day-to-day, and we want our hand motion policies to be able to handle this kind of variety. That is, we ask for our system to produce motion policies that can handle novel objects that are different than those that were used in the given mocap sequence.  

%Our imitation learning formulation is based on the imitation framework proposed by Peng et al.~\shortcite{peng2018deepmimic} mostly applied to locomotion tasks. However, the problem of dexterous hand manipulation raises a few unique challenges in terms of simulation and learning. For instance, simulation of object manipulation requires careful tuning of parameters to illustrate various interactions between fingers and objects, including pushing, rotating, sliding, and gaiting. Even worse, it often involves ``squeezing’’ motions that create multiple contacts that constrain the object's position completely. In addition, learning a robust policy to imitate a non-periodic, contact-rich sequence with separate unactuated objects is often more challenging than periodic locomotion tasks. For instance, it is difficult to determine when a rollout should be terminated because slight drifting of the object may easily lead to unrecoverable states. 

%To achieve successful learning, we carefully design our simulation and learning frameworks. We identify that rigid contacts parameters could affect both the difficulty and realism for the motion sequence to be learned. We are able to tune the set of rigid contact parameters in the simulator that allows easier training for dexterous manipulation controllers, and the resulting motions do not suffer from sinking or sliding contacts between the hand and the objects. We also choose to let the policy output deltas from the reference motion as used in \cite{bergamin2019drecon} instead of the other common choice of outputting raw PD targets \cite{peng2018deepmimic,won2020scalable}.

The success of a curriculum hinges on a capable learning framework. Ours is based on the popular  DeepMimic formulation \cite{peng2018deepmimic} on locomotion tasks. When applying it to dexterous manipulations, however, we encountered several unique challenges. For instance, parameters of the contact model can greatly affect both the difficulty and realism of a learned policy. In our case, we carefully chose a set of stiff parameters so the resulting motions do not suffer from sinking or sliding contacts between the hand and the objects, and they work across all our examples without adjustments. We also found it more effective to let the policy output deltas from the reference motion as used in \cite{bergamin2019drecon}, instead of the common choice of direct PD targets \cite{peng2018deepmimic,won2020scalable}. In addition, defining an early termination criterion is more challenging for the control of an unactuated object, because small deviations in the object state could still become unrecoverable. With some deliberate design choices, we arrive at a robust imitation learning framework that can simulate natural and dexterous manipulation of a variety of objects. This framework serves as a solid foundation for the greedy shape curriculum.

% In order to successfully train a hand animation policy, we found several techniques to be important.  To produce long non-repetitive sequences, we found it necessary to provide the reference position of the hand and the object as input to the policy.  In addition, rather than providing absolute target positions as the output of our policy, we instead output deltas from the mocap positions.  We also found that a careful choice of the representation of the hand and object orientation is necessary to avoid potential singularities in the rotation space.  Finally, detecting and using object/finger contacts as part of the observation proved to generate more robust controllers when varying the shape of the object.

%\revised{We even further achieve dexterous manipulation of more challenging objects by inventing a novel automated policy transfer mechanism with shape morphing. We take inspiration from real-world human manipulation of unseen objects that often starts from the previously learned manipulation strategy [\sehoon{Citation? not sure..}]. We co-train policies on a list of the morphed objects that span from the original shape to the target shape. Note that the morphing progression is not necessarily associated with the difficulty of the task. At each epoch, we train a policy on the associated shape for a short period and cross-test it on different morphologies at the end. If it is more promising than the existing policy, we transfer it to a new shape and resume fine-tuning later. In our experience, this automated policy transfer allows us to effectively overcome the learning saturation for challenging objects.}

% Using the above-mentioned techniques, we demonstrate a number of animation results.  First, we can create a hand policy that can manipulate a wide array of size and shape variations of the original object.  Moreover, we can create animation sequences as long as 1,000 frames of non-repetitive motion.  Finally, we can produce animations in which both hands participate in the manipulation of an object (e.g. turning, passing).


Our three main contributions are:
\begin{itemize}
\item We create a framework with well designed simulation and learning configurations for robust dexterous hand manipulation.

\item We propose a novel Greedy Shape Curriculum to automatically transfer an example manipulation of simple shapes to more challenging objects.

\item We showcase that the proposed framework can learn robust policies on various manipulation tasks, even with dynamics variations. 

% \item A simulation and DRL formulation that allows us to train DRL policies that demonstrate challenging in-hand manipulation tasks.
% \item Manipulation of a variety of object shape variations using a single policy, in which contact sensors provide an important boost to training quality.
\end{itemize}


%(1) implicitly inferring it with contact sensors at hands (2) adding full geom info (3) curriculum learning. We also demonstrate that we can infer full geom info, which is required for robotics...

%\sehoon{IMHO, the contributions are better to be closely connected with the challenges. And I would not treat "delta" and "representation of orientation" as our main contributions, which have been discussed a lot before. Here are potential contributions:}

% \begin{itemize}
%  \item Creation of a hand animation policy that manipulates an object using physics simulation.
%  \item Generate animation sequences as long as 1,000 frames of non-repetitive motion.
%  \item Create animations in which two hands cooperate to manipulate an object.
%  \item Create a hand policy that can manipulate a wide array of size and shape variations of the original object.
% \end{itemize}

