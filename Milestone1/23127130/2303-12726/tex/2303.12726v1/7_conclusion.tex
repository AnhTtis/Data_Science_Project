\section{Conclusion}

%By training a policy that is aware of contacts between the hand and the object, we can successfully learn to manipulate objects across a wide range of shapes and sizes. 

We have demonstrated training of policies for in-hand manipulation of objects based on motion capture data. Moreover, using a greedy shape curriculum, we can also adapt a given motion to a new shape. Because we use physics simulation, any disturbances or changes in physical properties are reflected in the object and hand motions. Our results closely mimic the fluidity and naturalness of real hands in the mocap examples.
%Because our policies follow the high-quality input mocap data, the resulting animations show the fluidity and naturalness of real hands.

%We see several exciting directions that could build upon our current hand animation techniques. We would like to see whether adding temporal information to the observation might aid in control that is more robust to changes in object size and shape. Possible directions includes adding memory mechanism (e.g. LSTM), giving history trajectory segments, or providing future motion capture states to better guide the learning.

One interesting direction for future work would be to reuse the existing motion clips for generating novel manipulation motions.
% One interesting direction for future work would be to further extend the duration of the manipulation tasks. 
Building a manipulation motion graph~\cite{lee2019scalable} to smoothly join manipulation mocap clips would be one possible approach. Unlike locomotion data where activities are highly repetitive, it is much harder to find suitable transition points with similar object and contact states, making this a challenging direction to explore.

\revised{Currently, policies generated from our method are only specialized to the target shape that it is trained on. We believe it is a first step towards solving a universal policy that can generalize to arbitrary shapes and task goals, which would be fruitful future work}
%An alternative direction for producing long motion controllers would be to utilize generative motion models to synthesize realistic manipulation references~\cite{wu2021saga,zhang2021manipnet,christen2021d,taheri2021goal}.



% We would like to see whether adding a form of memory to the policy (e.g. LSTM) might aid in control that is even more robust to changes in object size and shape. Perhaps such an approach would allow the policy to recognize object shape similar to how some locomotion policies can handle terrain variation.  Another challenge would be to learn how to smoothly switch between different policies in order to produce extended animations. Such a capability would then allow higher level animation control, either from a planning algorithm or under interactive human input.