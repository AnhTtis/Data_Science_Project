\section{Maximum Mean Discrepancy (MMD) and Two Sample Test}

To discover how long before the interlocks the precursors appear, we take the input data at $t_0$ and $t_1$ seconds respectively before all interlocks, and group them together into two sets. The problem is then formulated as a statistical problem --- to discover whether two random variables are sampled from an identical distribution, without imposing any assumption on the unknown true distribution. To achieve such a goal, we perform \emph{two sample test}~\cite{gretton2012kernel} on the two sets taken at different time before the interlocks, and statistically compare their \emph{Maximum Mean Discrepancy} (MMD). MMD could be interpreted as the distance between two distributions, and a large MMD implies a large difference between the two sets. Figure~\ref{fig:test_outline} generally describes the process.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{figs/twosampletest.png}
\caption{An outline of the two sample MMD test that shows the procedure of taking two dataset from $t_0$ and $t_1$ seconds before interlocks.}
\label{fig:test_outline}
\end{figure}

Two sample tests are a group statistical tests that aim to decide whether two sets are drawn from the same distribution. Namely, consider samples $X=\{x_1,\cdots, x_m\}$ that are independently and identically drawn (i.i.d.) from distribution $p$, and samples $Y=\{y_1,\cdots,y_n\}$ i.i.d. from distribution $q$, the null hypothesis of two sample test is $H_0: p = q$, and the alternative hypothesis is thus $H_1: p \neq q$. Examples of widely applied two sample tests include the two sample Kolmogorov-Smirnov (K-S) test~\cite{massey1951kolmogorov}, which compares the empirical cumulative distribution functions between two samples; Anderson-Darling (A-D) test~\cite{scholz1987k}, which is based on K-S test and gives more weights on the tail of distribution; and Cram\'{e}r-von Mises test~\cite{darling1957kolmogorov}, which employs a slightly different measure than A-D test. The tests mentioned above are only suitable for univariate problems; otherwise a multivariate generalisation step is necessary, as \citet{friedman1979multivariate} have shown using Minimum Spanning Trees or, more broadly, Nearest Neighbors approaches.

We choose to apply the two sample test based on MMD~\cite{gretton2012kernel} in our high dimensional data. Its application has been expanding recently, together with the growing trend of data complexity and development of machine learning, for instance the identification of lung cancer as in~\cite{zhao2019identification} and in the training of adversarial neural networks~\cite{dziugaite2015training}.
MMD is a distance metric for the closeness of two probability distributions. Specifically, it is the largest difference between expectations of functions in the unit ball of a reproducing kernel Hilbert space (RKHS)~\cite{sriperumbudur2010hilbert, sriperumbudur2011universality}, which is a Hilbert space of functions that fulfils certain continuity conditions.

Formally, we have observations $X_0$ and $X_1$ from a topological space $\mathcal{X}$, while $p$ and $q$ are their Borel probability measures, respectively. (In our practical case, $\mathcal{X}=\mathbb{R}^d$ where $d=376$ is the input dimension or number of channels.) The maximum mean discrepancy (MMD) is defined as
\begin{equation}\label{eq:mmd1}
    \text{MMD}(\mathcal{F}, p, q) := \sup_{f\in \mathcal{F}}\left(\mathbf{E}_{x_0}[f(x_0)]-\mathbf{E}_{x_1}[f(x_1)]\right)
\end{equation}
where $\mathcal{F}$ is a class of functions $f: \mathcal{X} \to \mathbb{R}$, and $\mathbf{E}_{x_0}[f(x_0)]$ is simplified from $\mathbf{E}_{{x_0}\sim p}[f(x_0)]$, denoting the expectation taking over the distribution $p$. To serve as a test metric capable of distinguishing between distributions, the function class $\mathcal{F}$ needs to be comprehensive enough to ensure a unique value when $p=q$ while being constrained enough to allow for practical finite estimates. \citet{gretton2012kernel} propose $\mathcal{F}$ to be the unit ball in an RKHS~$\mathcal{H}$. Based on the properties of $\mathcal{H}$, the \emph{mean embedding} $\mu_p \in \mathcal{H}$ of a probability distribution $p$ is introduced as
\begin{equation}
    \mathbf{E}_{x_0} f = \langle f, \mu_p \rangle_{\mathcal{H}}
\end{equation}
where $\langle\cdot,\cdot\rangle_{\mathcal{H}}$ denotes the inner product in $\mathcal{H}$. It can then be shown that MMD equals the distance between the two mean embeddings
\begin{equation}\label{eq:mmd2}
    \text{MMD}(\mathcal{F}, p, q) = \|\mu_p - \mu_q\|_{\mathcal{H}}.
\end{equation}

It can be further proved that if $\mathcal{F}$ is the unit ball in a universal RKHS~$\mathcal{H}$ on the compact metric space $\mathcal{X}$ with associated continuous kernel $k(\cdot, \cdot)$, then MMD$(\mathcal{F}, p,q)=0$ iff $p=q$ and the uniqueness is guaranteed~\cite{gretton2012kernel, sriperumbudur2010hilbert}. A detailed derivation of MMD is given in the appendix.

The mean embedding $\mu_p$ is an infinite-dimensional representation of the distribution $p$. Calculating MMD as the RKHS norm of the difference between the mean embeddings contains an essential Fourier transform step, which would require evaluating at infinite frequencies. Heuristic pseudo-distances seem like a plausible solution, such as the difference between values of characteristic functions at a single frequency~\cite{heathcote1972test}. However such methods are highly dependent on prior knowledge of the distributions to compare; and local agreement at some frequency intervals might cover up the globally distinct characteristic functions, thus causing the consistency issue.

Therefore in practice, the actual statistical test we adapt and implement is a variation called the \emph{mean embeddings} (ME) test, based on differences between analytic mean embeddings~\cite{chwialkowski2015fast}. Instead of integrating over the whole frequency domain, the ME test guarantees that it is \emph{almost surely} sufficient to evaluate only at a single random frequency. Theoretically, it is proved to be \emph{almost surely} consistent for all distributions; on the computational side, it has only linear time complexity with respect to the sample size, compared to quadratic time for the original MMD test~\cite{gretton2012kernel}. 

The process of the ME test is described as follows
\begin{enumerate}
    \item Randomly take a set of $J$ test locations $\{\mathbf{v}_j\}_{j=1}^{J} \subset \mathcal{X}$ (i.e. $\mathbb{R}^d$) according to certain optimization schemes~\cite{chwialkowski2015fast};
    \item Take a positive definite kernel $k: \mathcal{X}\times \mathcal{X} \to \mathbb{R}$ (normally use the Gaussian kernel);
    \item For each pair of original samples from the two sets $\{x_{0,i}, x_{1,i}\}_{i=1}^n$, calculate the \emph{differences between kernels}
    \begin{align}
        \vec{z}_i &:= \bigg(k(x_{0,i}, v_1)-k(x_{1,i}, v_1), \dots, \nonumber\\
        &k(x_{0,i}, v_J)-k(x_{1,i}, v_J)\bigg) \in \mathbb{R}^J
    \end{align}
    which is a $J-$dimensional vector.
    \item Calculate the \emph{mean empirical differences}
    \begin{equation}
        \vec{w}_n = \frac{1}{n}\sum_i  \vec{z}_i
    \end{equation}
    and its \emph{covariance matrix}
    \begin{equation}
        \mathbf{\Sigma}_n = \frac{1}{n-1}\sum_i(\vec{z}_i-\vec{w}_n)(\vec{z}_i-\vec{w}_n)^T.
    \end{equation}
    \item The test statistics is computed as
    \begin{equation}
        \widehat{\lambda}_n := n\vec{w}_n\mathbf{\Sigma}_n^{-1}\vec{w}_n
    \end{equation}
    which follows a $\chi^2$ distribution with $J$ degrees of freedom under the null hypothesis $H_0$;
    \item Choose a threshold $t_\alpha$ corresponding to the $1-\alpha$ quantile of the $\chi^2(J)$ distribution, and reject $H_0$ whenever $\widehat{\lambda}_n$ is larger than $t_\alpha$.
\end{enumerate}

To examine when the difference in the input data is emerging, we take samples $X_t=\{x_t^{I_0}, x_t^{I_1}, x_t^{I_2}, \cdots\}$ at $t$ seconds before all the interlocks. Each sample is a $d=376$ dimensional vector, where $t = \SI{0.2}{\second}, \SI{0.4}{\second}, \dots$ denotes the time before interlocks when we take the sample, and $I_0, I_1, \dots$ denotes the interlock number. The sample size for each $X_t$ is the number of available interlocks in the training data. We perform two sample ME test on each pair of samples, namely calculate the ME test statistics between $X_{t=\SI{0.2}{\second}}$ with $X_{t=\SI{0.4}{\second}}, X_{t=\SI{0.6}{\second}}, \dots$, then perform the test between $X_{t=\SI{0.4}{\second}}$ with $X_{t=\SI{0.6}{\second}}, X_{t=\SI{0.8}{\second}}, \dots$ and so forth. Table~\ref{tab:mmd} lists values of the test statistics with $J=5$ and $\alpha=0.01$ for the two sets taken at different $t_0$ and $t_1$, and Figure~\ref{fig:mmd2d} shows the full matrix of test statistics values of all pairs of $t_0$ and $t_1$ from \SI{0.2}{\second} to \SI{10}{\second} on log scale. It is clearly visible that samples taken at \SI{0.2}{\second} before interlocks are notably different, while \SI{0.4}{\second} also shows some slight difference.

\begin{table}
\centering
\caption{Value of $\widehat{\lambda}_n$ ($J=5$, $\alpha=0.01$) for various pair of two sets taken at $t_0$ and $t_1$ seconds before interlocks. The bold numbers denote that $H_0$ is rejected, i.e. the two sets are considered different.}
\begin{tabular}{lrrrrrr}
\toprule
$\mathbf{t_1\big\backslash t_0}$ & \SI{0.2}{\second} & \SI{0.4}{\second} & \SI{0.6}{\second} &  \SI{0.8}{\second} & $\dots$ &\SI{10.0}{\second}\\
\midrule
\SI{0.2}{\second} & 1.2 & \textbf{4935.6} & \textbf{8102.1} & \textbf{7999.4} & $\dots$ & \textbf{7330.0}\\
\SI{0.4}{\second} & - & 5.6 & \textbf{177.0} & \textbf{165.3} & $\dots$ & \textbf{215.8}\\
\SI{0.6}{\second} & - & - & 2.6 & 3.7 & $\dots$ & \textbf{103.9} \\
\SI{0.8}{\second} & - & - & - & 3.6 & $\dots$ & \textbf{93.7}\\
\bottomrule
\end{tabular}
\label{tab:mmd}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/mmd2d.png}
    \caption{Pairwise ME test statistics comparing sample sets taken at $t_0$ and $t_1$ seconds before interlocks, ranging from \SI{0.2}{\second} to \SI{10}{\second}. The values are transformed on log scale to emphasize the difference. A darker blue color implies more significant discrepancy between the two compared distributions. The lower triangle is masked as gray due to symmetry.}
    \label{fig:mmd2d}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/mmd10.png}
    \caption{The test statistics value of the set $t_0$ fixed at \SI{10}{\second} compared with data taken at \SI{9.8}{\second} towards \SI{0.2}{\second} before all interlocks in 53 days of 2019. With a log scale for the test statistics, we see that changes are only apparent about \SI{0.2}{\second} and \SI{0.4}{\second} before the interlocks.
}
    \label{fig:mmd10}
\end{figure}

Figure~\ref{fig:mmd10} shows the ME test statistics between samples taken at $t_0=\SI{10}{\second}$ before interlocks and samples taken from $t_1=\SI{9.8}{\second}$ towards \SI{0.2}{\second} before interlocks. With the log scale in y-axis, it is clearly seen that noticeable changes are only present less than \SI{0.4}{\second}, or even only \SI{0.2}{\second} before interlocks. The result implies that interlocks are mostly \emph{abrupt} events and the time scale of changes are much less than \SI{1}{\second}. This explains the weak classification power of RPCNN, since its input data are taken earlier than \SI{1}{\second} before interlocks, which fails to capture the abrupt changes. Since the data remain quite similar most of the time, the model actually sees similar inputs for the two different classes, and its large number of model parameters may only capture spurious patterns specific to the training set. This might as well explain the issue of over-fitting in RPCNN.
