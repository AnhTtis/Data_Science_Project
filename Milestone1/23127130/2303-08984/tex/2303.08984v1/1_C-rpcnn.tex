\subsection{The previous RPCNN model}
In a previous study~\cite{li2021novel}, we have presented the \emph{Recurrence Plot - Convolutional Neural Network} (RPCNN) model that takes window input, transforms time series into images and then performs the classification. However, the problems of a high False Positive rate and unstable performance need to be addressed.

\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{figs/rpcnn.png}
\caption{The RPCNN model workflow. Two classes of samples are taken as windows from the original 376 channels, with positive samples (orange, indicating interlocks) as sliding windows and negative samples (green, indicating stable operation) as non-overlapping windows. Then each 1-dimensional time series of the windows are transferred into an RP, and in total 376 RPs forms one sample to be fed into the CNN, which gives out a probability score.}
\label{fig:rpcnn}
\end{figure*}

The workflow of the RPCNN model is sketched in Fig.~\ref{fig:rpcnn}. First, we take two classes of time windows either close to or far from the interlocks, where the window length is a tunable model parameter. Explicitly, the \emph{interlock samples} are taken as sliding windows from \SI{1}{\second} to \SI{15}{\second} before interlocks. Second, we transform each 1-dimensional time series of the 376 channels to a 2-dimensional \emph{Recurrence Plot}, which could be interpreted as a pairwise distance measure of the time series and is capable of extracting finer dynamical patterns. Afterwards, we train the plots with \emph{Convolutional Neural Network}, which is proved to be powerful in image-driven pattern recognition~\cite{o2015introduction}. The output is a probability score inside the range $[0,1]$ indicating the likelihood of a sample belonging to the positive (i.e. close to interlock) class. More details such as model architecture are published in~\cite{li2021novel}.

% metrics
Typical binary classification metrics in a confusion matrix are defined and applied in our setting. A True Positive (TP) means an interlock sample --- a sample less than 15~s before an interlock --- being classified as an interlock. A False Positive (FP) means a stable sample --- a sample at least 10 minutes away from an interlock --- being mistaken as an interlock. A True Negative (TN) means a stable sample being reported as stable. And the remaining False Negative (FN) means that the model fails to identify an interlock.

For the predictions of a binary classification model, the ROC curve shows its true positive rate ($TPR=\frac{TP}{TP+FN}$) against false positive rate ($FPR=\frac{FP}{FP+TN}$) as a function of varying classification threshold~\cite{fawcett2006introduction}. The Area Under Curve (AUC) is a score satisfies $AUC \in [0,1]$ which represents the probability that a random positive (i.e. interlock) sample receives a higher value than a random negative (i.e. stable) sample. It shows the general ability of a model to distinguish between the two classes when taken all classification thresholds into account. The model parameters are chosen according to highest mean AUC value (AUC=0.71) of 25 model instances with random initialization. The classification threshold --- which directly determines the confusion matrix --- is generally chosen at a point on the ROC curve that leverages between high true positive rate and low false positive rate. 

Since the long term goal is to integrate the model into real-time operation, a way to quantify the model performance in real-time setting is needed in addition to standard classification metrics. We develop a custom metric called \emph{average Beam Time Saved per interlock} $\overline{T_s}$, taking real beam time loss and potential recovering methods into consideration. It is assumed that an interlock could be circumvented by reducing 10\% of the beam current, based on experiments and expert consultation. The classification threshold is then chosen according to largest $\overline{T_s}$. However, this metric imposes strict constraint on false positives. FPR is brought down to below 0.2\% at a cost of true positive rate reaching only 4.9\%. As shown later in Fig.~\ref{fig:roc} and Table~\ref{tab:realtime}, the classification result is neither as expected nor stable. There are still a large number of unwanted false positives; and over-fitting might also have occurred due to the non-negligible uncertainty in results as well as unsatisfactory convergence in hyperparameter scan. We examine the input data more in-depth with visualization, expert knowledge and statistical tests to find out the reason, and the customized real-time metric is also revised.
