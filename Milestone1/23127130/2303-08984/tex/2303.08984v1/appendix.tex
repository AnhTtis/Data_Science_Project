\section{Detailed derivation about  MMD}\label{appendix}

We start from the RKHS $\mathcal{H}$ and function $f:\mathcal{X} \to \mathbb{R} \in \mathcal{F} \in \mathcal{H}$, where $\mathcal{F}$ is a unit ball. Due to the continuity of the evaluation operator $\delta_x$~
\cite{scholkopf2002learning}
\begin{equation}
    \delta_x: \mathcal{H} \to \mathbb{R}, f \mapsto f(x),
\end{equation}
by the Riesz representation theorem~\cite{reed1980functional}, there is a feature mapping $\phi_x$ i.e.
\begin{equation}\label{eq:kernel}
    \phi_x = k(x, \cdot): \mathcal{X} \to \mathbb{R}
\end{equation}
that satisfies
\begin{equation}
    \langle f, \phi_x\rangle_{\mathcal{H}} = f(x).
\end{equation}
where $\langle\cdot, \cdot\rangle_{\mathcal{H}}$ is the inner product between two functions in $\mathcal{H}$. The kernel $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ in Eq.\eqref{eq:kernel} has one argument fixed at $x$, and the other free argument serves as the input argument for the feature map $\phi_x(\cdot)$. In particular, we have
\begin{equation}
    \langle\phi_{x_0}, \phi_{x_1}\rangle_{\mathcal{H}} = k(x_0,x_1).
\end{equation}

\begin{theorem}[MMD as the distance between mean embeddings in $\mathcal{H}$~\cite{gretton2006kernel}]\label{mmddef}

If mean embeddings $\mu_p$ and $\mu_q$ exist, where
\begin{equation*}
    \mathbf{E}_{x_0\sim p}f = \langle f, \mu_p\rangle_{\mathcal{H}}, \quad
    \mathbf{E}_{x_1\sim q}f = \langle f, \mu_q\rangle_{\mathcal{H}}, \quad \forall f \in \mathcal{F}
\end{equation*}
then
\begin{equation}
    \text{MMD}(\mathcal{F}, p, q) = \|\mu_p - \mu_q\|_{\mathcal{H}}.
\end{equation}
\end{theorem}

\begin{proof}
From Eq.~\eqref{eq:mmd1} we have
\begin{align*}
    \text{MMD}(\mathcal{F}, p, q) &= \sup_{f\in \mathcal{F}}\left(\mathbf{E}_{x_0}[f(x_0)]-\mathbf{E}_{x_1}[f(x_1)]\right) \\
    &= \sup_{\|f\|_ \mathcal{H}\leq 1}\left(\langle f, \mu_p\rangle_{\mathcal{H}}-\langle f, \mu_q\rangle_{\mathcal{H}}\right) \\
    &= \sup_{\|f\|_ \mathcal{H}\leq 1}\left(\langle f, \mu_p-\mu_q\rangle_{\mathcal{H}}\right)\\
    &= \left\langle \frac{\mu_p-\mu_q}{\|\mu_p-\mu_q\|_{\mathcal{H}}}, \mu_p-\mu_q\right\rangle_{\mathcal{H}} \\
    &= \|\mu_p-\mu_q\|_{\mathcal{H}}
\end{align*}
\end{proof}

We know that mean embeddings exist when $k(\cdot, \cdot)$ is measurable and $\mathbf{E}_x\sqrt{k(x,x)}<\infty$~\cite{gretton2012kernel}. On top of those, we now set up conditions for MMD to become a metric, i.e. uniquely equals zero if and only if two distributions are the same.
\begin{theorem}[MMD as a metric]
If $\mathcal{H}$ is a \textbf{universal} RKHS defined on a \textbf{compact} metric space $\mathcal{X}$, and the associated kernel $k(\cdot, \cdot)$ is \textbf{continuous}, then
\begin{equation}
    \text{MMD}(\mathcal{F}, p, q) = 0 \iff p=q
\end{equation}
where $\mathcal{F}$ is a unit ball on $\mathcal{H}$ and $p, q$ are two distributions from $\mathcal{X}$.
\end{theorem}

\begin{proof}
It is clear from the expression of MMD (Eq.~\eqref{eq:mmd1} and Eq.~\eqref{eq:mmd2}) that if $p=q$, MMD$(F, p, q)$ is zero. Therefore we only need to prove the rightward arrow.

Since $\mathcal{H}$ is universal, the kernel $k$ is required to be continuous, and $\mathcal{H}$ is also dense with regard to the $L_{\infty}$ norm in the bounded continuous function space $C(\mathcal{X})$~\cite{micchelli2006universal}. In other words, any function in $C(\mathcal{X})$ can be arbitrarily well-approximated by functions in $\mathcal{H}$. Therefore, for any given $\epsilon > 0$ and any function $g \in C(\mathcal{X})$, there exists a function $h \in \mathcal{H}$ such that
\begin{equation}\label{eq:dense}
    \|g-h\|_{\infty} < \epsilon.
\end{equation}
Then we have
\begin{align*}
    |\mathbf{E}_{x_0} g(x_0) - \mathbf{E}_{x_1} g(x_1)|&\leq |\mathbf{E}_{x_0} g(x_0) - \mathbf{E}_{x_0} h(x_0)|\\
    &+ |\mathbf{E}_{x_0} h(x_0) - \mathbf{E}_{x_0} h(x_1)|\\
    &+ |\mathbf{E}_{x_1} h(x_1) - \mathbf{E}_{x_1} g(x_1)| \\
    &\leq \mathbf{E}_{x_0} |g(x_0)-h(x_0)|\\
    &+|\langle h, \mu_p-\mu_q\rangle|_{\mathcal{H}}\\
    &+\mathbf{E}_{x_1}|h(x_1)-g(x_1)|
\end{align*}
and from Eq.~\eqref{eq:mmd2} we know that $\text{MMD}=0$ implies $\mu_p - \mu_q = 0$; from Eq.~\eqref{eq:dense} we have $\mathbf{E}_{x}|g(x)-h(x)| < \epsilon$. Therefore
\begin{equation}
    |\mathbf{E}_{x_0} g(x_0) - \mathbf{E}_{x_1} g(x_1)| \leq \epsilon + 0 + \epsilon = 2\epsilon
\end{equation}
for all $\epsilon > 0$ and $g\in C(\mathcal{X})$. This indicates that the two distributions or Borel probability measures $p$ and $q$ are equal~\cite{gretton2012kernel, dudley2018real}.
\end{proof}