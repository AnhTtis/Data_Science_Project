\section{Methods}
In the study, 12 participants interacted with an android robot. The robot looked like a young woman and it was able to display four primary emotions – joy, sadness, anger and surprise. %with light skin, brown hair and eyes and realistic proportions. 
The model of the android was an A-Lab Android Standard Model AL-G109ST-F.
%It has 18 DoFs (degrees of freedom) of which    more than half are positioned within the face of the android. These allow it to blink, move and widen its eyes, raise the lower part of its eyes, open its mouth, smile and shift its eyebrows inward or upward. It can also tilt and move its head.
  

%\begin{figure} [h!]
% \centering
% \includegraphics[width=8.5cm]{111.jpg}
% \caption{Android Robot during the interaction withparticipant}
% \label{fig:andoroid}
%\end{figure}

\begin{wrapfigure}{R}{0.35\textwidth}
\includegraphics[width=1\linewidth]{img_usertesting_mit_AndroidRoboter.jpg} 
\caption{Android Robot \& participant}
\label{fig:andoroid}
\end{wrapfigure}

A living lab situation was created in which participants had the opportunity to interact with the android for about thirty minutes. The study was designed as a Wizard-of-Oz interaction, meaning that the android’s phrases were controlled and written by the researchers \cite{dahlback1993wizard, carros2022ethical}. The facial expression was a mix of the Wizard-of-Oz simulating emotions and an automatic mode making micro movements such as simulating breathing or looking around the room. During the interaction, the android was seated across from the participant. Meanwhile, two researchers sat behind them out of view, and a conversation was started through a series of loosely defined questions and statements. The topics started with casual small talk like “What is your favorite season?” to more personal questions like “What are you looking forward to today?” to very deep questions like “What is your greatest fear?”.
Eight participants who were all above the age of 60 were invited as well as four stakeholders, whose ages ranged from 48 to 67, who could offer a professional opinion on the effect a robot displaying emotion could have on older adults. In the following, these stakeholder participants will be denoted with SH and regular participants with P.  
Directly after the interactions, semi-structured interviews were conducted on how the interaction was perceived and how the participants would evaluate potential future uses, especially in regard to care homes. Interviews lasted 30-60 minutes and were later transcribed and deductively coded. Using a reflexive thematic analysis \cite{braun2021can} initial categories were formulated and inductively expanded while reviewing the interviews. The codes were also discussed among researchers. As main categories, it was looked at the way the participants rated the speech, gestures, and facial expressions of the android. Making a distinction of whether an aspect was seen as positive or negative in each one. It was also coded how often the participants mentioned an emotion. Further main categories were potential problems, visions for the future, and the usage of the android within a care home – each of which was further divided into subcategories.