\section{Introduction \& Related Research
}
Human-Robot interaction has made great advances in the last decade, with social robots being put to practice in different social situations (e.g., care settings \cite{carros2020exploring,10.1145/3491102.3517463, helmsocial}, religion \cite{trovato2021religion, trovato2019communicating, loffler2021blessing}, rehabilitation \cite{langer2021emerging, kellmeyer2018social}. Still, emotions are seen as one of the aspects that separate humans from machines.
Emotions are integral to developing empathy and understanding the intent of the interaction partner. Sadness, joy, and fear are used by humans to determine that the creatures they interact with are deserving of respect and care \cite{torre2020if}. Still, we also developed fine-tuned abilities to notice signs of disingenuousness \cite{reed2018face}.
Next to tone and gestures, we mainly use facial expressions to detect and infer the emotions of our human counterparts \cite{crivelli2018facial}.
A phenomenon called emotional contagion describes the fact that emotions can even be transmitted to another person \cite{hatfield2014new}. This often includes facial mimicry, the copying of a facial expression that was displayed by a counterpart \cite{rymarczyk2019empathy}. Primary emotions are joy, sadness, anger, fear, disgust, and surprise. They often describe a spontaneous and intuitive reaction toward an event and have specific facial expressions associated with them across cultures \cite{ekman1992argument}. 
%There are conflicting views on what the purpose of emotions - or rather their display - is. One of these theories is the behavioral ecology view (BECV) that sees emotions as social tools that are used consciously to influence social situations \cite{crivelli2018facial}.

There is insightful research shedding light on the beneficial effect of displayed emotions in human-robot interaction. For example, Chuah and Yu could show that the display of joy and surprise by service robots positively influenced potential customers \cite{chuah2021future}. As it is rather complicated to artificially create realistic facial expressions, many researchers explored different channels to convey emotions \cite{haring2011creation}. Haring et al. tried communicating emotions through sounds, body movements, and even eye color using a Nao robot but concluded that the latter was ineffective, as it was too far removed from the way emotions are naturally displayed \cite{haring2011creation}. Torre et al. found that not only visual expressions of happiness like smiles increased trust, but also conversational agents that spoke with a tone implying a smile were met with a higher level of trust \cite{torre2020if}. Likewise, Beck et al. could show that even a Nao robot that lacks the ability to alter its face could communicate emotions using its body language \cite{beck2010towards}.
Another way to create realistic emotions is through using animated avatars \cite{noel2009interpreting}. These avatars can be portrayed on a display or projected into a three-dimensional space \cite{matovelle2018interaction}.
There are also robots that try to very closely mimic natural human faces. These so-called androids can mimic facial expressions very closely as they have controllable joints in their faces \cite{ishiguro2007android}. Nishio et al. even built a robot that exactly resembled one of the researchers to directly compare the effect this robot would have on people to the one the real person has \cite{nishio2007geminoid}.
In order to shed light on how older adults react to the display of emotion by a human-like robot, we let people interact with a very realistic android. Through interviews and surveys, we captured the impression these basic emotions caused and their effects on trust.
