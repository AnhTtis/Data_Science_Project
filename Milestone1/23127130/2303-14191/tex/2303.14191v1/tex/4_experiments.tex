\section{Experiments}
\input{table/combine_ablation.tex}
We conduct extensive experimental evaluations to validate the capability of our framework, built upon the point cloud perception codebase \textit{Pointcept}~\cite{pointcept2023}. We first ablate our designs with an efficient pre-training pipeline that only utilizes ScanNet point cloud in \secref{sec:ablation}, while without compromising performance. Then we explore large-scale pre-training across multiple datasets and compare our performance with previous results in \secref{sec:results_comparison}.

\subsection{Main Properties}
\label{sec:ablation}
We ablate the main designs and intriguing properties of our MSC in \tabref{tab:ablation}, and the default setting is available in the caption. We enable \textit{efficient pre-training} by introducing our view generation pipeline, which is ablated in \tabref{tab:ablation_view_generation}. All of our ablation experiments only require 20G ScanNet point cloud data and around 14 hours pertaining on a single machine containing 8 NVIDIA RTX3090.

\mypara{View generation.}
In \tabref{tab:ablation_view_generation}, we show the results with different generation strategies of contrastive views. Unlike PointContrast~\cite{xie2020pointcontrast}, which builds on the raw RGB-D frames, our strategy directly utilizes the scene-level point cloud.
This significantly reduces storage requirements (96\% less) and allows more efficient use of the training data.
With a 30\% equivalent number of training iterations, our method can attain 120$\times$ training epochs, making full use of the training data for the first time.
This results in 0.4 points higher FT mIoU with 4.4$\times$ speedup.
When combined with an additional mask point modeling strategy, the performance can be further boosted by 0.6 points, yet still with a notable speedup.

\mypara{Number of positive pairs.}
In \tabref{tab:ablation_ft_bs}, we show the effects of different numbers of positive pairs. Our method sees consistent improvements with an increasing number of positive pairs, while for PointContrast~\cite{xie2020pointcontrast}, the information in a large batch of positive pairs cannot be effectively utilized. 
Our intuition is that the ability to process scene-level views, which are larger in scale and contain much more information than frame-level views, enables our method to take advantage of a larger number of positive pairs.

\mypara{Data augmentation.}
In \tabref{tab:ablation_augmentation}, we analyze the effect of different data augmentation combinations.
Adopting either spatial augmentation or photometric augmentation leads to sub-optimal performance, and the combination of both helps make our view generation pipeline come to work.
Concerning relative significance, since the spatial augmentation is highly coupled with the masking strategy, the gain from it is higher than the photometric augmentation.
But still, both are necessary.

\mypara{View mixing.}
In \tabref{tab:ablation_mix}, we explore different configs for the view mixing strategy.
Randomly mixing the query views while leaving the key views unchanged yields the best performance.
Our intuition is that the key views, which form the vocabulary of the InfoNCE loss, should be relatively stable, thus reducing the ambiguity of the learning target.
In contrast, introducing more diversity to the queries can be more helpful.

\begin{figure}[t]\centering
\includegraphics[width=.95\linewidth]{figure/ablation_masking_ratio.pdf} \\
\scriptsize masking ratio (\%) \\
\caption{\textbf{Masking ratio}. A masking ratio ranging from 30\% to 40\% works well with our design, and a higher mask rate negatively influences contrastive learning. The y-axes represent ScanNet semantic segmentation validation mIoU (\%).}
\vspace{-6mm}
\label{fig:ablation_mask_ratio}
\end{figure}

\mypara{Cross mask.}
In \tabref{tab:ablation_cross_mask}, we study the cross mask strategy.
This strategy ensures that the two augmented views have no overlapping tokens.
As reported in the table, whether with or without the contrastive learning target, this strategy ensures fewer shortcuts to the task and enables consistent downstream improvements.

\mypara{Mask grid size.}
In \tabref{tab:ablation_mask_grid_size}, we show the results ablating the grid size for producing masks on point clouds.
Our design works with a grid size larger than 0.1m, and we consider 0.15m as a default setting. As shown in \figref{fig:vis_mask}, our reconstruction module is robust to extending mask grid size, which indicates that high quilty representation is captured during pre-training.

\mypara{Masking ratio.}
In \figref{fig:ablation_mask_ratio}, we depict the effect of the ratio of masked tokens.
A masking ratio ranging from 30\% to 40\% works well with our design, and a higher masking rate has a negative impact on the overall performance.
This varies from the conclusion of MAE~\cite{he2022masked}, in which a higher masking ratio of 75\% achieves top performance.
Our hypothesis is that the contrastive learning objective built on the masked point clouds might favor a lower masking ratio, and the results in \figref{fig:ablation_mask_ratio} reflect a trade-off between the contrastive learning objective and the reconstructive objective.
And pure reconstructive learning on point clouds is an interesting direction for future explorations.

\mypara{Reconstruction target.}
In \tabref{tab:ablation_reconstruction_target}, we ablate the effects of two components of our reconstruction target: color reconstruction and normal reconstruction. 
Given the premise that indoor scenes are used, both targets show a positive effect on overall performance, while color reconstruction has a higher impact.
The intuition is that the difference in texture reflected by color has a higher impact on the task of semantic segmentation, while the normal is helpful but has less influence (consider the same task on a 2D image).

\input{table/combine_results.tex}

\subsection{Results Comparison}
\label{sec:results_comparison}
In this section, we extend the scale of pre-training by merging multiple datasets and compare downstream task fine-turning performance with previous unsupervised pre-training frameworks~\cite{xie2020pointcontrast, hou2021exploring}.
Specifically, we adopt the default model setting ablated in \secref{sec:ablation} and train on both ScanNet~\cite{dai2017scannet} and ArkitScenes~\cite{dehghan2021arkitscenes} point clouds, extending pre-training assets from 1,513 scenes to 6,560 scenes.

\mypara{Semantic segmentation.}
In \tabref{tab:results_sem_seg}, we report the semantic segmentation results on ScanNet and ScanNet200~\cite{rozenberszki2022language} benchmark with SparseUNet and compare them with previous results.
Our improvements are consistent and significant with larger pre-training assets for both benchmarks.
With SparseUNet backbone, we outperform the current state-of-art pretraining framework by 1.7 points on ScanNet and 2.4 points on ScanNet200. Meanwhile, it is worth noting that driven by powerful MSC, we set a new best validation result on ScanNet semantic segmentation and pushed the previous SOTA to 75.5\% with a baseline model.

\mypara{Instance segmentation.}
In \tabref{tab:results_ins_seg}, we report the instance segmentation results on ScanNet and ScanNet200~\cite{rozenberszki2022language} with SparseUNet backbone driven by \textit{PointGroup}~\cite{jiang2020pointgroup}. Comparing them with previous results, we still see consistent improvements. Specifically, our framework achieves 59.6\% on the ScanNet validation set, which is 3.3 points higher than training from scratch and 0.2 points promotion compared with the previous state-of-art performance. The boost is more significant on ScanNet200, which is 1.6 points higher than the previous SOTA.

\mypara{Data efficiency.} In \tabref{tab:results_data_efficient}, we compare the ScanNet Data Efficient~\cite{hou2021exploring} results with previous methods. Our MSC shows consistently superior performance even compared with the latest data-efficient learning framework\cite{tian2022vibus}.