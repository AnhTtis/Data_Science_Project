\begin{figure}[t]\centering
\includegraphics[width=0.87\linewidth]{figure/teaser.pdf}
\vspace{-4mm}
\caption{Comparison of unsupervised 3D representation learning. The previous method~\cite{xie2020pointcontrast} (top) relies on raw RGB-D frames with restricted views for contrastive learning, resulting in low efficiency and inferior versatility. Our approach (bottom) directly operates on scene-level views with contrastive learning and masked point modeling, leading to high efficiency and superior generality, further enabling large-scale pre-training across multiple datasets.}
\label{fig:teaser}
\vspace{-6mm}
\end{figure}

\section{Introduction}
Unsupervised visual representation learning aims at learning visual representations from vast amounts of unlabeled data. The learned representations are proved to be beneficial for various downstream tasks like segmentation and detection. It has attracted lots of attention and achieved remarkable progress in 2D image understanding, exceeding the upper bound of human supervision~\cite{grill2020bootstrap,henaff2020data}.

Despite the impressive success of unsupervised visual representation learning in 2D, it is underexplored in 3D. Modern 3D scene understanding algorithms~\cite{choy20194d, wu2022point} are focused on supervised learning, where models are trained directly from scratch on targeted datasets and tasks. Well-pre-trained visual representations can undoubtedly boost the performance of these algorithms and are currently in urgent demand. Recent work PointContrast~\cite{xie2020pointcontrast} conducts a preliminary exploration in 3D unsupervised learning. However, it is limited to raw RGB-D frames with an inefficient learning paradigm, which is not scalable and applicable to large-scale unsupervised learning. To address this essential and inevitable challenge, we focus on building a scalable framework for large-scale 3D unsupervised learning.

One technical stumbling block towards large-scale pre-training is the inefficient learning strategy introduced by matching RGB-D frames as contrastive views. PointContrast~\cite{xie2020pointcontrast} opens the door to pre-training on real indoor scene datasets and proposes frame matching to generate contrastive views with natural camera views, as in Figure~\ref{fig:teaser} top. However, frame matching is inefficient since duplicated encoding exists for matched frames, resulting in limited scene diversity in batch training and optimization. Meanwhile, not all of the 3D scene data contains raw RGB-D frames, leading to failure deployments of the algorithm. Inspired by the great success of SimCLR~\cite{chen2020simple}, we investigate generating strong contrastive views by directly applying a series of well-curated data augmentations to scene-level point clouds, eliminating the dependence on raw RGB-D frames, as in Figure~\ref{fig:teaser} bottom. Combined with an effective mechanism that mixes up query views, our contrastive learning design accelerates the pre-training procedure by 4.4$\times$ and achieves superior performance with purely point cloud data, compared to PointContast with raw data. The superior design also enables large-scale pre-training across multiple datasets like ScanNet~\cite{dai2017scannet} and ArkitScenes~\cite{dehghan2021arkitscenes}.

Another obstacle is the mode collapse phenomenon that occurs when scaling up the optimization iterations. We owe the culprit for this circumstance to the insufficient difficulty of unsupervised learning tasks. To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked autoencoders~\cite{he2022masked, xie2022simmim}, we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively. We incorporate the mask point modeling strategy into our contrastive learning framework via an exquisite design of contrastive cross masks, leading towards a scalable unsupervised 3D representation learning framework, namely Masked Scene Contrast~(MSC).

Our framework is efficient, effective, and scalable. We conduct extensive experimental evaluations to validate its capability. On the popular point cloud dataset ScanNet, our algorithm accelerates the pre-training procedure by more than 3$\times$, and achieves better performance on downstream tasks, when compared to the previous representative PointContrast. Besides, our method also enables large-scale 3D pre-training across multiple datasets, leading to state-of-the-art fine-tuning results on several downstream tasks, e.g. 75.5\% mIoU on ScanNet semantic segmentation validation set. In conclusion, our work opens up new possibilities for large-scale unsupervised 3D representation learning.