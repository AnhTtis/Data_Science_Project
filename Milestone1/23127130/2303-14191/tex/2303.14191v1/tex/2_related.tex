\section{Related Work}
\mypara{2D Image contrastive learning.}
Based on the instance discrimination~\cite{dosovitskiy2015discriminative} pretext task, and combined with the contrastive learning~\cite{oord2019representation,hjelm2018learning} paradigm, modern variants of 2D image contrastive representation learning have shown strong abilities in learning transferable visual representations~\cite{wu2018unsupervised,chen2020simple,he2020momentum}. With the learning objective built on the similarity between randomly augmented image views, this line of work strongly depends on a large batch size~\cite{chen2020simple,he2020momentum} and a finely designed data augmentation pipeline~\cite{chen2020simple,tian2020makes,grill2020bootstrap} to achieve better performance. We find these two points also hold true for 3D contrastive learning.

\mypara{2D Image reconstructive learning.}
In 2D unsupervised learning, there is also a recent trend of switching the pretext task from instance discrimination~\cite{chen2020simple,he2020momentum,grill2020bootstrap,caron2021emerging,bardes2022vicreg} to masked image modeling~\cite{bao2022beit,he2022masked,xie2022simmim,zhou2022image,wei2022masked}. Based on a denoising autoencoder~\cite{vincent2010stacked}-style architecture, the task is to reconstruct the RGB value~\cite{he2022masked,xie2022simmim}, discrete token~\cite{bao2022beit,zhou2022image}, or feature~\cite{wei2022masked} of masked pixels. This line of work has shown strong potential in learning representations on large-scale datasets and is less prone to model collapsing like instance-discrimination-based methods (\eg, contrastive learning). When combined with contrastive learning~\cite{el2021large,assran2022masked,wu2022extreme}, the performance can be further boosted, yet with less dependence on data scale~\cite{el2021large}.

\begin{figure*}[!t]
    \raggedleft
    \subfloat[
        \textbf{Frame matching~\cite{xie2020pointcontrast, hou2021exploring}.} 1. Extract raw RGB-D frames and camera positions from raw data. 2. Project each 2D frame into 3D space produces frame-level point cloud views. 3. Calculate pairwise overlapping rates among each frame of a single view and select pairs with overlapping rates larger than 30\% as pairs of contrastive views. 
        \label{fig:frame_matching}
    ]{
        \includegraphics[width=0.485\textwidth]{figure/frame_matching.pdf}
    }
    \hspace{0.9mm}
    \subfloat[
        \textbf{Scene augmentation~(ours).} 1. Apply spatial augmentations containing rotation, flipping, and scaling. 2. Apply photometric augmentations containing brightness, contrast, saturation, hue, and gaussian noise jittering. 3. Generate and apply contrastive cross masks to the two views after sampling augmentations containing cropping and voxelization.
        \label{fig:scene_augmentation}
    ]{
        \includegraphics[width=0.485\textwidth]{figure/scene_augmentation.pdf}
    }
    \vspace{-1mm}
    \caption{\textbf{View generation.} Compared with frame matching (FM), our scene augmentation (SA) is efficient and effective. 1. SA can end-to-end produce contrastive views on the original point cloud with ignorable latency, while FM requires preprocessing devouring enormous storage resources (\eg additional 1.5 TB storage for ScanNet) in step 2 and pairwise matching is time-consuming. 2. SA produces scene-level views, while FM can only produce frame-level views containing limited information. Benefiting from advanced photometric augmentations, SA has the capacity to simulate the same scene under different lighting.}
    \label{fig:view_generation}
    \vspace{-2mm}
\end{figure*}


\mypara{3D Scene understanding.}
The deep neural architectures for understanding 3D scenes can be roughly categorized into three paradigms according to the way they model point clouds: projection-based, voxel-based, and point-based.
Projection-based works project 3D points into various image planes and adopt 2D CNN-based backbones to extract features\cite{su15mvcnn,li2016vehicle,chen2017multi,lang2019pointpillars}.
On the other hand, the voxel-based stream transforms point clouds into regular voxel representations to operate 3D convolutions~\cite{maturana2015voxnet,song2017semantic}.
Their efficiency is then improved thanks to sparse convolution~\cite{graham20183d,choy20194d,chu2022twist}.
Point-based methods, in contrast, directly operate on the point cloud~\cite{qi2017pointnet,qi2017pointnet++,zhao2019pointweb,thomas2019kpconv,chu2021icm}, and see a recent transition towards transformer-based architectures~\cite{guo2021pct,zhao2021point,wu2022point}.
Following~\cite{xie2020pointcontrast}, we mainly pre-train on the voxel-based method SparseUNet~\cite{choy20194d} implemented with SpConv~\cite{spconv2022}.

\mypara{3D Representation learning.}
Unlike the 2D counterparts, where large-scale unsupervised pre-training has been a common choice for facilitating downstream tasks~\cite{caron2021emerging}, 3D representation learning is still not mature, and most works still train from scratch on the target data directly~\cite{hou2021exploring}.
While earlier works in 3D representation learning simply build on a single object~\cite{wang2019deep,hassani2019unsupervised,sauder2019self,sanghi2020info3d}, recent works start to train on scene-centric point clouds~\cite{xie2020pointcontrast,hou2021exploring}.
However, unlike in 2D that scene-centric representation learning has been well-studied~\cite{liu2020self,xie2021unsupervised,wen2022self}, the pre-training on 3D scenes, which relies on raw frame data~\cite{xie2020pointcontrast,hou2021exploring}, still faces inefficiency issues and finds it hard to scale up to larger scale datasets.
In contrast, we explore directly learning at the scene level, which shows significantly higher efficiency in processing scene data, and opens the possibility for pre-training with larger-scale point clouds, for the first time ever.