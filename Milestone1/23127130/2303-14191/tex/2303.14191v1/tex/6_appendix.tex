\begin{figure*}[t!]\centering
\includegraphics[width=0.95\linewidth]{figure/augmentation.pdf}
    \caption{\textbf{Photometric augmentation.}}
    \label{fig:sup_augmentation}
\end{figure*}

\section{Implementation Details}
This section introduces the implementation details of our proposed \textit{Masked Scene Contrast} (MSC), which is crucial to making these novel designs work.
\subsection{Backbone Architecture}
We adopt \textit{SparseUNet}~\cite{choy20194d}, which is widely applied by previous works as ablation studies and result comparisons. \textit{SparseUNet} adopt a U-Net style architecture, and the config details follow previous works~\cite{xie2020pointcontrast, hou2021exploring, wu2022point}. The main config is available in \tabref{tab:sup_backbones}, and the name of the backbone is marked in \marktext{gray}{gray}.

\subsection{View Generation Pipeline.}
The specific constitution of our generation pipeline is concluded in \tabref{tab:sup_augmentation}. For a given point cloud input, we first dedicate two copies of the original point cloud for separated random view generation. Then we apply the augmentation sequence in \tabref{tab:sup_augmentation} to produce differentiated views of a single scene. The original coordinates (w/o rotation) are saved for both views, and both grid sampling and point matching are performed on this original coordinate system. Spatial augmentations, photometric augmentations, and sampling augmentations are marked in \marktext{green!5}{green}, \marktext{yellow!10}{yellow} and \marktext{blue!5}{blue}.

\mypara{Spatial augmentations.} We simulate different orientations of point cloud scenes by randomly rotating around the z-axis. Slight rotations around the x-axis and y-axis are also applied to simulate the unavoidable slope of the ground. Additional random flipping also adds geometric diversity to objects in the scenes and is thus also applied.

\begin{table}[t!]
\begin{minipage}{.48\textwidth}
    \centering
    \tablestyle{7pt}{1.08}
    \input{table/sup_backbones.tex}
    \vspace{-2mm}
    \caption{\textbf{Backbone setting.}}
    \vspace{2mm}
    \label{tab:sup_backbones}
\end{minipage} \\
\begin{minipage}{.48\textwidth}
    \centering
    \tablestyle{7pt}{1.08}
    \input{table/sup_augmentation.tex}
    \vspace{-2mm}
    \caption{\textbf{View generation pipeline.}}
    \vspace{2mm}
    \label{tab:sup_augmentation}
\end{minipage} \\
\begin{minipage}{.48\textwidth}
    \centering
    \tablestyle{5pt}{1.08}
    \input{table/sup_pre_training.tex}
    \vspace{-2mm}
    \caption{\textbf{Pre-training setting.}}
    \vspace{2mm}
    \label{tab:sup-pre-training}
\end{minipage} \\
\begin{minipage}{.48\textwidth}
    \centering
    \tablestyle{5pt}{1.08}
    \input{table/sup_fine_tuning.tex}
    \vspace{-2mm}
    \caption{\textbf{Fine-tuning setting.}}
    \label{tab:sup-fine-tuning}
\end{minipage}
\end{table}


\mypara{Photometric augmentations.}
Our photometric augmentations contain brightness, contrast, saturation, and hue adjusting from 2D images to 3D point clouds. These augmentations enhance the chromatic augmentations scheme introduced by Choy \etal~\cite{choy20194d} three years ago, and we hope these advanced photometric augmentations can also benefit future works. As for the augmentation parameters, we follow BYOL~\cite{grill2020bootstrap}, a reputed unsupervised representation learning framework for 2D images. We shrink the boundary of hue adjustment since the hue diversity of 3D indoor scenes is limited compared with image datasets. These stochastic photometric augmentations can effectively simulate diverse light conditions. A visualization of these augmentations is available in \figref{fig:sup_augmentation}.

\mypara{Sampling augmentations.} Grid sampling is a necessary process that both reduces point redundancy and increases data diversity. Combined with random rotation, the grid sampling is applied to different grids and points from the original point cloud, which adds to the data diversity. Further, random cropping is also applied to simulate the occlusion relationship and enforce the model to differentiate the visible region of contrastive views, which is also an important component.

\subsection{Training Setting.}
\mypara{Pre-training.} The default setting is in \tabref{tab:sup-pre-training}. We only utilize ScanNet point cloud scene data for efficient pre-training. And we adopt both ScanNet~\cite{dai2017scannet} and ArkitScene~\cite{dehghan2021arkitscenes} for large-scale pretraining.

\mypara{Fine-tuning.} The default setting for fine-tuning on ScanNet semantic segmentation is in \tabref{tab:sup-fine-tuning}. It is worth noting that good fine-tuning results rely on higher batch size. And the conclusion holds for most of our experimented downstream tasks. We use the same setting proposed by CSC~\cite{hou2021exploring} and adopt 48 as the fine-tuning batch size for downstream tasks.