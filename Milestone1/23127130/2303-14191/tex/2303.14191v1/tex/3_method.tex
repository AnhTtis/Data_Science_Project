\section{Pilot Study}
\label{sec:pilot}
This section analyzes the two main obstacles towards large-scale pre-training with point clouds. Our proposed design is based on the conclusion of the pilot study.

\mypara{Is matching RGB-D frames a good choice?} \\
As a seminal work in 3D representation learning, PointContrast~\cite{xie2020pointcontrast} first enables pre-training in real-world indoor scenes with matched raw RGB-D frames as contrastive views. A visualization of the frame matching procedure is illustrated in \figref{fig:frame_matching}. This protocol seems natural for indoor scenes since point clouds of indoor scenes are usually derived from RGB-D videos~\cite{armeni2016s3dis,dai2017scannet,dehghan2021arkitscenes}, where the raw frames are extracted from. However, this framework has multiple drawbacks that can hinder the scalability of training:
\begin{itemize}
    \vspace{-2mm}
    \item \textit{Redundant frame encoding.} The pairwise matching strategy that PointContrast adopts allows one frame to be matched multiple times. As a result, each frame can be encoded multiple times in one step, adding to redundancy in training.
    \vspace{-2mm}
    \item \textit{Low learning efficiency.} In one training step, the frame matching strategy only allows the framework to process several views of a single scene. Therefore the amount of information that PointContrast can process in one step is rather limited, and the overall time for one training cycle is also notably high.
    \vspace{-2mm}
    \item \textit{Dependency on raw RGB-D frames.} The whole framework is built on the assumption that RGB-D videos are available, yet this is not true for every publicly-available point cloud dataset. Even when available, the storage cost of RGBD frames is also significantly higher than the reconstructed point cloud data.
    \vspace{-2mm}
\end{itemize}
Consequently, pre-training frameworks~\cite{xie2020pointcontrast,hou2021exploring} based on matching frames as contrastive samples require enormous computing and storage resources. For example, PointContrast sub-samples RGB-D scans from the raw ScanNet videos every 25 frames, consuming $\sim$30 times the storage of the processed point clouds, and takes 80 GPU hours to process an epoch of 1500 scenes. Even at such cost, our experiments in \tabref{tab:ablation_view_generation} verify that the raw RGB-D frames cannot bring additional information over the processed point clouds to achieve a better representation.

\textit{We will explore the possibility of pre-training on the point clouds directly.}

\begin{figure*}[!t]\centering
\includegraphics[width=0.95\linewidth]{figure/framework.pdf}
\caption{\textbf{Our MSC framework}. (1) Generating a pair of contrastive views with a well-curated data augmentation pipeline consisting of photometric, spatial, and sampling augmentations. (2) Generating a pair of complementary masks and applying them to the pair of contrastive views. Replacing masked point features with a learnable mask token vector. (3) Extracting point representation with a given U-Net style backbone for point cloud understanding. (4) Reassembling masked contrastive views to masked points combination and unmasked points combination. (5) Matching points share similar positional relationships in the two views as positive sample pairs and computing InfoNCE loss to optimize contrastive objective. (6) Predicting masked point color and normal and computing Mean Squared Error loss and Cosine Similarity Loss with ground truth respectively, to optimize the reconstruction objective.}
\label{fig:framework}
\end{figure*}

\mypara{What's the revelation behind mode collapse?} \\
Mode collapse, defined as the phenomenon that all features collapse to a single vector, remains an unsolved problem accompanying the development of 3D representation learning~\cite{choy2019fully,xie2020pointcontrast}.
To alleviate this problem, PointContrast introduced InfoNCE loss~\cite{oord2018representation}, which has been shown to stabilize training, to replace the hardest-contrastive loss. Yet, the problem of mode collapse can still occur when the amount of training data and the length of the training schedule increase.
Given the empirical conclusion of 2D contrastive learning~\cite{chen2021exploring}, the occurrence of mode collapse is unusual under the premise that a large number of negative samples are already adopted. 
Interestingly, we notice that the mean negative pair cosine similarity  of previous works is mostly close to 0, indicating that the negative samples are mostly easy and thus have little penalty towards the trivial solution. Although the InfoNCE loss alleviates this problem with an alternated optimization objective, we argue that a more desirable solution can be achieved by raising the difficulty of the unsupervised pretext task.

\textit{We will further raise the difficulty of the pretext task to solve the mode collapse problem.}

\section{Approach}
Based on the analysis of the stumbling blocks for large-scale pre-training in \secref{sec:pilot}, we first introduce our optimized contrastive learning design in \secref{sec:cl} to make the process more efficient. Then we solve the long-term problem of mode collapse with an additional reconstructive learning design in \secref{sec:rl}. The final optimization target is described in \secref{sec:loss}. Combining these exquisite designs, we build the whole framework, namely \textit{Masked Scene Contrast} (MSC), and a visual illustration of our MSC is available in \figref{fig:framework}.
\subsection{Contrastive Learning}
\label{sec:cl}
\mypara{Framework.}
Different from the previous protocol of matching RGB-D frames decomposed from indoor scenes, our contrastive learning framework directly operates on the point cloud data. Given a point cloud $\mX = (\mP, \mC)$, where $\mP \in \mathbb{R}^{n \times 3}$ represents the spatial features (coordinate) of the points and $\mC \in \mathbb{R}^{n \times 3}$ represents the photometric features (color) of the points, the contrastive learning framework can be summarized as follows:

\begin{itemize}
    \vspace{-2mm}   
    \item \textit{View generation.} For a given point cloud $\mX$, we generate query view $\mX_r$ and key view $\mX_k$ of the original point cloud with a sequence of stochastic data augmentations, which includes photometric, spatial, and sampling augmentations.
    \vspace{-2mm}
    \item \textit{Feature extraction.} Encoding point cloud features $\mF_r$ and $\mF_k$ with a U-Net style backbone $\zeta(\cdot)$ to $\hat{\mF}_r$ and $\hat{\mF}_k$ respectively.
    \vspace{-2mm}
    \item \textit{Point matching.} The positive samples of contrastive learning are point pairs with close spatial positions in the two views. For each point belonging to the query view, we calculate the correspondence mapping $\sP = \{(i, j)\}_{n'}$ to points of the key view. If $(i, j) \in \sP$ then point $(\vp_i, \vc_i)$ and point $(\vp_j, \vc_j)$ constructs a pair across two views.
    \vspace{-2mm}
    \item \textit{Loss computation.} Computing the contrastive learning loss on the representation of two views $\hat{\mF}_r$ and $\hat{\mF}_k$ and the correspondence mapping $\sP$. An encoded query view should be similar to its key view.
    \vspace{-2mm}
\end{itemize}

\mypara{Data augmentation.} As a pioneering work in image contrastive learning, SimCLR~\cite{chen2020simple} reveals that a well-curated data augmentation pipeline is crucial for learning strong representations. Unlike supervised learning, contrastive learning requires much stronger data augmentations to prevent trivial solutions.
However, an effective data augmentation recipe is still absent in 3D representation learning. The frame matching scheme in prior works~\cite{xie2020pointcontrast,hou2021exploring} simply applies a randomly rotating operator to contrastive targets. Even if the RGB-D frames can be viewed as a natural random crop, the augmentation space is still far from diverse enough. The pretext task it forms is not yet challenging enough to facilitate the contrastive learning framework to learn robust representations for downstream tasks.

As presented in \figref{fig:scene_augmentation}, our well-designed stochastic data augmentation pipeline includes photometric augmentations, spatial augmentations, and sampling augmentations.
Inspired by the advanced photometric augmentation validated by our 2D counterparts~\cite{chen2020simple,grill2020bootstrap}, we further strengthen the photometric augmentation component introduced by Choy \etal~\cite{choy20194d} with random brightness, contrast, saturation, hue, and gaussian noise jittering for photometric augmentation. Besides that, random rotating, flipping, and scaling constitute our spatial augmentations, and the sampling augmentation is composed of random cropping and grid sampling.

Empirically, the order of data augmentations is also a key component of our recipe.
For example, grid sampling after random rotation leads to cross grids for sampling, which further increases the distinction between contrastive views and has a better augmentation effect. The specific data augmentation settings are available in the appendix, and a comparison with previous methods is presented in \figref{fig:view_generation}.

\mypara{View mixing.} Recently, Nekrasov \etal~\cite{Nekrasov21mix3d} proposes a data augmentation technique for 3D understanding models by mixing two scenes as a hybrid training sample, which can significantly suppress model overfitting. Inspired by the mixing mechanism, we integrate the logic of mixing as part of the contrastive learning objective. As illustrated in \figref{fig:mixing}, for a batch of pairwise views, we randomly mix up the query views while maintaining the key views unchanged before the \textit{feature extraction} process. The simple operation can effectively increase the robustness of the backbone and improve the robustness of the point cloud representation.

\begin{figure}[t]\centering
\includegraphics[width=\linewidth]{figure/view_mixing.pdf}
\caption{\textbf{View Mixing}. Randomly mix up query views while keeping key views unmixed for a given batch of pairwise contrastive views. Detaching mixed query view after feature extraction for contrastive comparison with matched key views.}
\label{fig:mixing}
\end{figure}

\mypara{Contrastive target.} We follow the design of PointContrast on the contrast target and apply InfoNCE loss to the matched points. Given correspondence mapping $\sP = \{(i, j)\}_{n'}$  produced by \textit{point matching} and points representation $\hat{\mF}_r$ and $\hat{\mF}_k$ embedded during \textit{feature extraction}, the contrastive loss is:
\begin{align}
    s_{ij} &= \frac{\vf_{i'}^{rT} \vf_{j'}^{k}}{||\vf_{i'}^{rT}||\cdot||\vf_{j'}^{k}||},
    \label{eq:cosine-similarity}
    \\
    \mathcal{L}_{\text{InfoNCE}} &= \sum_i^n - \text{log} \frac{\text{exp}(s_{ii} / \tau)}{\sum_j^n \text{exp}(s_{ij} / \tau)},
    \label{eq:info-nce-loss}
\end{align}
note that $\mS=\{s_{ij}\} \in \mathbb{R}^{n\times n}$ is the pairwise cosine similarity matrix between positive samples and negative samples, while $\tau$ is the temperature factor scaling cosine similarity. In practice, we control temperature factor $\tau$ as 0.4, which is the same as previous works~\cite{xie2020pointcontrast,hou2021exploring}.

\subsection{Reconstructive Learning}
\label{sec:rl}
As is mentioned in \secref{sec:pilot}, one of the stumbling blocks for large-scale representation is mode collapse, and our solution is to scale up the difficulty of the unsupervised pre-training task. Motivated by the success of masked image modeling~\cite{he2022masked, xie2022simmim} in 2D representations, we propose masked point modeling, which can be naturally integrated into our contrastive learning framework. Benefiting from this design, our framework can fully use non-overlapped regions of contrastive views that cannot be utilized by contrastive learning.

\mypara{Contrastive cross mask.}
The key design that enables additional construction learning in our contrastive learning framework is the contrastive cross mask. For a given query view and key view of a single point cloud, we partite the unioned point set into non-overlapping grid partitions by their original position before spatial augmentation. Given a mask rate $r$ range from 0 to 0.5, we randomly generate a pair of masks $\mM_r, \mM_k \in \mathbb{R}^{1\times n_{r,k}}$, in which there are no shared masked patches. Then, we follow the practice of SimMIM~\cite{xie2022simmim} to apply the pair of masks to the two views respectively by replacing the input feature with a learnable mask token vector $\vt \in \mathbb{R}^c$. Consequently, the feature extraction process can be rewritten as follows:
\begin{align}
    \hat{\mF}_{r,k} &= \zeta((1 - \mM_{r,k}) \mF_{r,k} + \mM_{r,k}\mT_{r,k}),
    \label{eq:feature-extract-with-mask}
\end{align}
where $\mT_{r,k}\in\mathbb{R}^{n_r,n_k\times c}$ is the expand matrix of mask token vector $\vt$ to fit the feature dimensions.

\mypara{Reconstruction target.}
The features of the point cloud are composed of two parts, the coordinates that determine the geometric structure and the colors that represent the texture features. We build up reconstruction targets for the two groups of features separately.

The reconstruction of point cloud texture is straightforward, we predict the photometric value of each point with a linear projection. We compute the mean squared error (MSE) between the reconstructed and original color of masked points as the color reconstruction loss:
\begin{align}
    \mathcal{L}_{\text{c}} = \frac{\sum_i^{n_r}m_i^r||\vx_i^r - \hat{\vx}_i^r||_2^2 + \sum_i^{n_k}m_i^k||\vx_i^k - \hat{\vx}_i^k||_2^2}{n_r' + n_k'},
    \label{eq:color-reconstruction-loss}
\end{align}
where $n_r'$ and $n_k'$ represent the number of mask points belonging to refer view and key view, $m_r^i$ and $m_k^i$ mean the $i$-th element of $M_r$ and $M_k$ respectively.

Point coordinates play an important role in describing the geometric structure of point clouds, and it is worth noting that directly reconstructing the coordinates of masked points is not reasonable since masked points are only sampled from 3D object surface rather than the continuous surface itself. Reconstructing points coordination would lead to an overfitted representation. To overcome the challenge, we introduce the concept of surfel reconstruction. Surfel is an abbreviation for a \textit{surface element} or \textit{surface voxel} in the discrete topology literature~\cite{herman1992discrete} and primitives rendering~\cite{pfister2000surfels}. For each masked point, we reconstruct the normal vector of the corresponding surfel and compute the  mean cosine similarity between estimations and surfel normals as a contrastive loss:
\begin{align}
    \mathcal{L}_{\text{n}} = \frac{\sum_i^{n_r}m_i^r\vx_i^{rT}\hat{\vx}_i^{r} + \sum_i^{n_k}m_i^k\vx_i^{kT}\hat{\vx}_i^{k}}{n_r' + n_k'},
    \label{eq:normal-reconstruction-loss}
\end{align}
where $n_r'$ and $n_k'$ represent the number of mask points belonging to refer view and key view, $m_r^i$ and $m_k^i$ mean the $i$-th element of $M_r$ and $M_k$ respectively.

\subsection{Loss Function}
\label{sec:loss}
Our framework combines the contrastive target, the color reconstruction target, and the surfel reconstruction to make the unsupervised task more scalable. The overall loss function is a weighted sum of \eqref{eq:info-nce-loss}, \eqref{eq:color-reconstruction-loss}, and \eqref{eq:normal-reconstruction-loss} which is written as follow:
\begin{align}
    \mathcal{L}_{\text{overall}} = \mathcal{L}_{\text{InfoNCE}} + \lambda_c\mathcal{L}_{\text{c}} + \lambda_n\mathcal{L}_{\text{n}},
    \label{eq:combined-loss}
\end{align}
where $\lambda_c$ and $\lambda_n$ are the weight parameters that balance the three loss components. Empirically we find that performance is robust to the choice of weight parameters, and we make $\lambda_c = \lambda_n = 1$ in practice.

\begin{figure}[t]\centering
\includegraphics[width=.98\linewidth]{figure/vis_mask.pdf} \\
\caption{\textbf{Masked scenes and color reconstructions}. We visualize one of the cross masks of each scene with a mask rate of 50\% (left) and color reconstruction of the masked point combinations (right). We pre-train our MSC with a mask patch size of 10cm and generalize to results to different mask sizes. Compared with the original point clouds, the loss of detail cannot be avoided, while boundary and texture are well preserved by our model.}
\label{fig:vis_mask}
\end{figure} 