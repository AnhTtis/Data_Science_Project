In this paper, we tackle the problem of scalable unsupervised 3D representation learning. To this end, we present Masked Scene Contrast (MSC), an efficient, effective, and scalable framework that directly operates on scene-level views with contrastive learning and masked point modeling. Benefiting from the efficient scene-level point cloud processing pipeline and the effective training objectives, our method harvests high efficiency and superior generality, and enables large-scale pre-training across multiple datasets.

The key factor that empowers our method's scalability to larger-scale pre-training lies in the efficient pipeline that can directly learn from point cloud data, rather than the raw RGB-D frames. The efficiency, however, does not only mean processing data at scale. When only the standard dataset ScanNet is used for pre-training, our method still achieves uncompromised performance, yet with at least 3$\times$ speedup over previous works. This is especially meaningful considering the exhaustively long experimental time in the pre-training community, and it can better facilitate the verification of new ideas in future works.

It should also \textbf{}be noted that given the limit in computing resources, and the absence of a pre-training dataset that is sustainably at scale, the scalability of our method is not fully presented. In other words, our method opens the possibility of large-scale pre-training on 3D point cloud data for the first time, and we call for a large-enough 3D scene dataset that can fully unleash this potential. We hope our method can inspire future works that take a first step to real large-scale 3D pre-training, as the 2D community does.