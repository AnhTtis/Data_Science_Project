


@article{dumitrache2018crowdsourcing,
  title={Crowdsourcing Ground Truth for Medical Relation Extraction},
  author={Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={8},
  number={2},
  pages={1--20},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{kilicoglu2020broad,
  title={Broad-coverage biomedical relation extraction with SemRep},
  author={Kilicoglu, Halil and Rosemblat, Graciela and Fiszman, Marcelo and Shin, Dongwook},
  journal={BMC bioinformatics},
  volume={21},
  pages={1--28},
  year={2020},
  publisher={Springer}
}

@inproceedings{zeng2020copymtl,
  title={Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning},
  author={Zeng, Daojian and Zhang, Haoran and Liu, Qianying},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={9507--9514},
  year={2020}
}



@inproceedings{nayak2020effective,
  title={Effective modeling of encoder-decoder architecture for joint entity and relation extraction},
  author={Nayak, Tapas and Ng, Hwee Tou},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8528--8535},
  year={2020}
}

@article{bekoulis2018joint,
  title={Joint entity recognition and relation extraction as a multi-head selection problem},
  author={Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},
  journal={Expert Systems with Applications},
  volume={114},
  pages={34--45},
  year={2018},
  publisher={Elsevier}
}

@article{peng2018extracting,
  title={Extracting chemical--protein relations with ensembles of SVM and deep learning models},
  author={Peng, Yifan and Rios, Anthony and Kavuluru, Ramakanth and Lu, Zhiyong},
  journal={Database},
  volume={2018},
  year={2018},
  publisher={Oxford Academic}
}

@article{tran2019neural,
  title={Neural metric learning for fast end-to-end relation extraction},
  author={Tran, Tung and Kavuluru, Ramakanth},
  journal={arXiv preprint arXiv:1905.07458},
  year={2019}
}

@inproceedings{miwa2016end,
  title={End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures},
  author={Miwa, Makoto and Bansal, Mohit},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1105--1116},
  year={2016}
}

@article{rink2011automatic,
  title={Automatic extraction of relations between medical concepts in clinical texts},
  author={Rink, Bryan and Harabagiu, Sanda and Roberts, Kirk},
  journal={Journal of the American Medical Informatics Association},
  volume={18},
  number={5},
  pages={594--600},
  year={2011},
  publisher={BMJ Group BMA House, Tavistock Square, London, WC1H 9JR}
}

@inproceedings{kavuluru2017extracting,
  title={Extracting drug-drug interactions with word and character-level recurrent neural networks},
  author={Kavuluru, Ramakanth and Rios, Anthony and Tran, Tung},
  booktitle={2017 IEEE International Conference on Healthcare Informatics (ICHI)},
  pages={5--12},
  year={2017},
  organization={IEEE}
}

@inproceedings{liu2016dependency,
  title={Dependency-based convolutional neural network for drug-drug interaction extraction},
  author={Liu, Shengyu and Chen, Kai and Chen, Qingcai and Tang, Buzhou},
  booktitle={2016 IEEE international conference on bioinformatics and biomedicine (BIBM)},
  pages={1074--1080},
  year={2016},
  organization={IEEE}
}


@book{slpbook3,
 author = {Jurafsky, Daniel and Martin, James H.},
 title = {Speech and Language Processing (3rd Edition)},
note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
 year = {2023}
} 


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{zeng2022natural,
  title={Natural language processing--assisted literature retrieval and analysis for combination therapy in cancer},
  author={Zeng, Jia and Cruz Pico, Christian X and Saridogan, Tur{\c{c}}in and Shufean, Md Abu and Kahle, Michael and Yang, Dong and Shaw, Kenna and Meric-Bernstam, Funda},
  journal={JCO Clinical Cancer Informatics},
  volume={6},
  pages={e2100109},
  year={2022},
  publisher={Wolters Kluwer Health}
}

@article{klauschen2014combinatorial,
  title={The combinatorial complexity of cancer precision medicine},
  author={Klauschen, Frederick and Andreeff, Michael and Keilholz, Ulrich and Dietel, Manfred and Stenzinger, Albrecht},
  journal={Oncoscience},
  volume={1},
  number={7},
  pages={504},
  year={2014},
  publisher={Impact Journals, LLC}
}

@inproceedings{tiktinsky-etal-2022-dataset,
    title = "A Dataset for N-ary Relation Extraction of Drug Combinations",
    author = "Tiktinsky, Aryeh  and
      Viswanathan, Vijay  and
      Niezni, Danna  and
      Meron Azagury, Dana  and
      Shamay, Yosi  and
      Taub-Tabib, Hillel  and
      Hope, Tom  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.233",
    doi = "10.18653/v1/2022.naacl-main.233",
    pages = "3190--3203",
    abstract = "Combination therapies have become the standard of care for diseases such as cancer, tuberculosis, malaria and HIV. However, the combinatorial set of available multi-drug treatments creates a challenge in identifying effective combination therapies available in a situation.To assist medical professionals in identifying beneficial drug-combinations, we construct an expert-annotated dataset for extracting information about the efficacy of drug combinations from the scientific literature. Beyond its practical utility, the dataset also presents a unique NLP challenge, as the first relation extraction dataset consisting of variable-length relations. Furthermore, the relations in this dataset predominantly require language understanding beyond the sentence level, adding to the challenge of this task. We provide a promising baseline model and identify clear areas for further improvement. We release our dataset (https://huggingface.co/datasets/allenai/drug-combo-extraction), code (https://github.com/allenai/drug-combo-extraction) and baseline models (https://huggingface.co/allenai/drug-combo-classifier-pubmedbert-dapt) publicly to encourage the NLP community to participate in this task.",
}

@article{10.1093/bib/bbac282,
    author = {Luo, Ling and Lai, Po-Ting and Wei, Chih-Hsuan and Arighi, Cecilia N and Lu, Zhiyong},
    title = "{BioRED: a rich biomedical relation extraction dataset}",
    journal = {Briefings in Bioinformatics},
    volume = {23},
    number = {5},
    year = {2022},
    month = {07},
    abstract = "{Automated relation extraction (RE) from biomedical literature is critical for many downstream text mining applications in both research and real-world settings. However, most existing benchmarking datasets for biomedical RE only focus on relations of a single type (e.g. protein–protein interactions) at the sentence level, greatly limiting the development of RE systems in biomedicine. In this work, we first review commonly used named entity recognition (NER) and RE datasets. Then, we present a first-of-its-kind biomedical relation extraction dataset (BioRED) with multiple entity types (e.g. gene/protein, disease, chemical) and relation pairs (e.g. gene–disease; chemical–chemical) at the document level, on a set of 600 PubMed abstracts. Furthermore, we label each relation as describing either a novel finding or previously known background knowledge, enabling automated algorithms to differentiate between novel and background information. We assess the utility of BioRED by benchmarking several existing state-of-the-art methods, including Bidirectional Encoder Representations from Transformers (BERT)-based models, on the NER and RE tasks. Our results show that while existing approaches can reach high performance on the NER task (F-score of 89.3\\%), there is much room for improvement for the RE task, especially when extracting novel relations (F-score of 47.7\\%). Our experiments also demonstrate that such a rich dataset can successfully facilitate the development of more accurate, efficient and robust RE systems for biomedicine.Availability: The BioRED dataset and annotation guidelines are freely available at https://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbac282},
    url = {https://doi.org/10.1093/bib/bbac282},
    note = {bbac282},
    eprint = {https://academic.oup.com/bib/article-pdf/23/5/bbac282/45936115/bbac282.pdf},
}


@article{krallinger2015chemdner,
  title={The CHEMDNER corpus of chemicals and drugs and its annotation principles},
  author={Krallinger, Martin and Rabal, Obdulia and Leitner, Florian and Vazquez, Miguel and Salgado, David and Lu, Zhiyong and Leaman, Robert and Lu, Yanan and Ji, Donghong and Lowe, Daniel M and others},
  journal={Journal of cheminformatics},
  volume={7},
  number={1},
  pages={1--17},
  year={2015},
  publisher={BioMed Central}
}

@article{peng-etal-2017-cross,
    title = "Cross-Sentence N-ary Relation Extraction with Graph {LSTM}s",
    author = "Peng, Nanyun  and
      Poon, Hoifung  and
      Quirk, Chris  and
      Toutanova, Kristina  and
      Yih, Wen-tau",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1008",
    doi = "10.1162/tacl_a_00049",
    pages = "101--115",
    abstract = "Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.",
}

@inproceedings{zhong-chen-2021-frustratingly,
    title = "A Frustratingly Easy Approach for Entity and Relation Extraction",
    author = "Zhong, Zexuan  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.5",
    doi = "10.18653/v1/2021.naacl-main.5",
    pages = "50--61",
    abstract = "End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7{\%}-2.8{\%} absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16{\mbox{$\times$}} speedup with a slight reduction in accuracy.",
}

@inproceedings{giorgi-etal-2022-sequence,
    title = "A sequence-to-sequence approach for document-level relation extraction",
    author = "Giorgi, John  and
      Bader, Gary  and
      Wang, Bo",
    booktitle = "Proceedings of the 21st Workshop on Biomedical Language Processing",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bionlp-1.2",
    doi = "10.18653/v1/2022.bionlp-1.2",
    pages = "10--25",
    abstract = "Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https://github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.",
}

@inproceedings{segura2013semeval,
  title={Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013)},
  author={Segura-Bedmar, Isabel and Mart{\'\i}nez Fern{\'a}ndez, Paloma and Herrero Zazo, Mar{\'\i}a},
  year={2013},
  organization={Association for Computational Linguistics}
}

@article{jia2019document,
  title={Document-Level $ N $-ary Relation Extraction with Multiscale Representation Learning},
  author={Jia, Robin and Wong, Cliff and Poon, Hoifung},
  journal={arXiv preprint arXiv:1904.02347},
  year={2019}
}

@inproceedings{zeng-etal-2018-extracting,
    title = "Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism",
    author = "Zeng, Xiangrong  and
      Zeng, Daojian  and
      He, Shizhu  and
      Liu, Kang  and
      Zhao, Jun",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1047",
    doi = "10.18653/v1/P18-1047",
    pages = "506--514",
    abstract = "The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.",
}

@article{Rosenman2020ExposingSH,
  title={Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data},
  author={Shachar Rosenman and Alon Jacovi and Yoav Goldberg},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.03656}
}

@article{sabo-etal-2021-revisiting,
    title = "Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes",
    author = "Sabo, Ofer  and
      Elazar, Yanai  and
      Goldberg, Yoav  and
      Dagan, Ido",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.42",
    doi = "10.1162/tacl_a_00392",
    pages = "691--706",
    abstract = "We explore few-shot learning (FSL) for relation classification (RC). Focusing on the realistic scenario of FSL, in which a test instance might not belong to any of the target categories (none-of-the-above, [NOTA]), we first revisit the recent popular dataset structure for FSL, pointing out its unrealistic data distribution. To remedy this, we propose a novel methodology for deriving more realistic few-shot test data from available datasets for supervised RC, and apply it to the TACRED dataset. This yields a new challenging benchmark for FSL-RC, on which state of the art models show poor performance. Next, we analyze classification schemes within the popular embedding-based nearest-neighbor approach for FSL, with respect to constraints they impose on the embedding space. Triggered by this analysis, we propose a novel classification scheme in which the NOTA category is represented as learned vectors, shown empirically to be an appealing option for FSL.",
}

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}


@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}


@inproceedings{gururangan-etal-2020-dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
}

