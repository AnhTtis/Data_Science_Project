\section{Bounds and Code Constructions of \BGC Schemes}
For the non-trivial case of $\localcomp<\ngrad$, i.e., the \master does not compute all the partial gradients locally, the replication factor of any \BGC scheme is bounded from below by
$\replfact \geq {\nmalicious+1}$.
In addition, we note that if $\localcomp = 0$, then the replication factor of any \BGC scheme is bounded from below as $\replfact \geq {2\nmalicious+1}$ and can be achieved through DRACO \cite{chenDRACOByzantineresilientDistributed2018}. Conversely, if $\replfact \geq 2\nmalicious+1$, then $\localcomp=\commoh=0$ was shown to be achievable. Thus, we are most interested in the case $\nmalicious+1 \leq \replfact \leq 2\nmalicious+1$, and we investigate the fundamental tradeoff between $\localcomp$, $\replfact$ and $\commoh$ for any $\nround$. 

In particular, we show that for $\localcomp = \nmalicious$ the replication factor $\replfact = {\nmalicious+1}$ is achievable. For $0<\localcomp<\nmalicious$, we show that for any $1 \leq \nhonest \leq \nmalicious+1$, if $\replfact \leq {\nmalicious + \nhonest}$, then $\localcomp\geq \compbound$. For $\replfact = {\nmalicious + \nhonest}$ and $\localcomp = \compbound$, we give a lower bound on $\commoh$. We then construct an \BGC scheme that requires $\localcomp \leq \compbound$ local computations, $\replfact = {\nmalicious+\nhonest}$, $\nround \leq \left(\nmalicious+1-\nhonest\right) \left\lceil \log_2\left(\frac{\ngrad}{\ngroup}\right) \right\rceil$ rounds and  $\commoh \leq \left(\nmalicious+1-\nhonest\right) \left( 2 \left\lceil \log_2\left( \frac{\ngrad}{\ngroup}\right) \right\rceil+ \frac{\nmalicious+3\nhonest}{2\log_2{\card{\galpha}}} \right)$.
\input{local_comp_vs_total_comm.tex}
As depicted in \cref{fig:comm_vs_localcomp}, for realistic parameter ranges, the local computations drastically reduce the required communication. The communication overhead of the protocol $\commoh$ is outweighed by the initial transmission of $\iresp_{0, \wind}$. For a comparison between our achievability result and converse for $\commoh$, cf. \cref{app:achievability_vs_converse}. 

We prove our lower bounds by considering the case where the adversary chooses the following behaviour for the malicious workers.

\subsection{Symmetrization Attack for Bounds}\label{sec:attack}
    The adversary chooses a (potentially corrupted) value for each malicious worker and each partial gradient. 
    We denote worker $\worker$'s \emph{claimed} partial gradient results for $\pgrad$ as $\wpgrad{\gind}{\wind}\in\galpha^\graddim$ for all $\wind \in \range{\nworker}$ and $\gind\in\range{\ngrad}$.
    For honest workers we say $\wpgrad{\gind}{\wind} = \pgrad[\gind]$.
    Each worker $\worker$ computes their responses consistently based on those values, i.e.,
    \begin{align*}
        \noisyiresp_{\roundind, \wind} &= \encfun[\wind,\encind_{\wind,\roundind}]\left( \wpgrad{1}{\wind},\dots,\wpgrad{\ngrad}{\wind} \right),\\ 
        &\forall \roundind\in \{0, \dots, \nround\}, \wind \in \range{\nworker}, \encind_\wind \in \range{\nencfun}.
    \end{align*}
    For clarity of exposition, we first lay out how the adversary chooses the claimed gradient values for $\ngroup=1$ groups of size $\nmalicious+1$, i.e., $\nhonest=1$, before we generalize to arbitrary $\ngroup, \nhonest \in \N$.

    For $\nhonest=1$ and $\ngroup=1$, the adversary draws a set $\disagreegradset \subseteq \range{\ngrad}$ of size $|\disagreegradset|=\lfloor \frac{\nmalicious}{\nhonest} \rfloor = \nmalicious$ uniformly at random and assigns each malicious worker $\worker$ a unique gradient index from $\disagreegradset$.
    For each index, it sets the corresponding worker's claimed value to a distinct random vector unequal the true value and all other malicious worker's values to the true partial gradient value.
    With probability $\frac{1}{2}$, it picks a single gradient index from $\disagreegradset$ uniformly at random and sets all malicious workers' values to the same erroneous random partial gradient value.
    The resulting claimed gradients take on the form as depicted in \cref{tab:key_error_pattern}.
\begin{table}[h]
    \vspace{-0.3cm}
    \begin{tabularx}{\columnwidth}{c|ccccccc}
        & $\wpgrad{1}{\wind}$ & $\wpgrad{2}{\wind}$ 
        & $\dots$  & $\wpgrad{\nmalicious}{\wind}$ & $\wpgrad{\nmalicious+1}{\wind}$ & $\dots$ & $\wpgrad{\ngrad}{\wind}$ \\ 
        \toprule
        $\worker[1]$ & $\pgrad[1]^{\prime\prime}$ & $\pgrad[2]^\prime$ & $\dots$  & $\pgrad[\nmalicious]^\prime$ & $\pgrad[\nmalicious+1]^\prime$ & $\dots$ & $\pgrad[\ngrad]^\prime$ \\ 
        $\worker[2]$ & $\pgrad[1]^\prime$ & $\pgrad[2]^{\prime\prime}$ & $\dots$  & $\pgrad[\nmalicious]^\prime$ & $\pgrad[\nmalicious+1]^\prime$ & $\dots$ & $\pgrad[\ngrad]^\prime$ \\ 
        $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ & $\vdots$ & $\dots$ & $\vdots$ \\
        $\worker[\nmalicious]$ & $\pgrad[1]^\prime$ & $\pgrad[2]^\prime$ & $\dots$ & $\pgrad[\nmalicious]^{\prime\prime}$ & $\pgrad[\nmalicious+1]^\prime$ & $\dots$ & $\pgrad[\ngrad]^\prime$ \\ 
       $\worker[\nmalicious+1]$ & $\pgrad[1]^\prime$ & $\pgrad[2]^\prime$ & $\dots$  & $\pgrad[\nmalicious]^\prime$ & $\pgrad[\nmalicious + 1]^\prime$ & $\dots$ & $\pgrad[\ngrad]^\prime$ \\ 
    \end{tabularx}
    \caption{claimed partial gradients for symmetrization attack.}
    \vspace{-0.5cm}
    \label{tab:key_error_pattern}
\end{table}
    For partial gradients with index in $\disagreegradset$ there are two competing values $\pgrad^{\prime}$ and $\pgrad^{\prime\prime}$ whereas for all other gradient indices the claimed values by all workers agree.

    For $\nhonest > 1$, the adversary randomly partitions the malicious workers into $\lfloor \nmalicious / \nhonest \rfloor -1$ sets of size $\nhonest$ and (depending on divisibility) one group of size $\nmalicious \mod \nhonest$.
    Each of the sets of size $\nhonest$ behaves like one malicious worker in the case $\nhonest=1$.
    The workers in the remaining set of less than $\nhonest$ workers (if non-empty) pick their claimed gradients randomly either like a random other set of workers or like the honest workers.
    The resulting claimed gradients take on the form as depicted in \cref{tab:key_error_pattern_generalized} in Appendix~\ref{app:large_table}.

For $\ngroup > 1$, the adversary chooses the first group and chooses the claimed gradients according to the attack strategy for $\ngroup^\prime=1$ and $\ngrad^\prime=\ngrad / \ngroup$ in that group. In all other groups, the claimed gradients are equal to their true values.

\subsection{Fundamental Limits}

\begin{theorem}[Lower bound on $\localcomp$ and $\replfact$]
Suppose that $\nworker = \ngroup (\nmalicious + \nhonest)$ for integers $\ngroup,\nhonest \geq 1$. For any tuple (\allocmat, \encfunset, \decfun, \proto), with $\allocmat$ as in \eqref{eq:fractional_repetition}, to be a (\nround,\localcomp,\replfact,\commoh)-\BGC scheme, it holds that if $\replfact \leq \nmalicious + \nhonest$, then $\localcomp\geq \compbound$ (conversely,  if $\localcomp < \compbound$ then $\replfact > {\nmalicious + \nhonest}$) for any number of rounds $\nround$ and any communication overhead $\commoh$. 
\label{thm:converse_C}
\end{theorem}
\begin{IEEEproof}
    We demonstrate that any tuple $(\allocmat, \encfunset, \decfun, \proto)$ with fractional repetition data allocation which uses $\localcomp \leq \compbound - 1$ local computations cannot be an \BGC scheme. Specifically, we show that the malicious workers can always perform a symmetrization attack preventing the main node from deterministically computing the full gradient. 
    Let the malicious workers behave as explained in \cref{sec:attack}. 

    We abstract $\encfunset$ and $\proto_1$ by assuming that the values of all partial gradients $\wpgrad{\gind}{\wind}$ computed by the workers are available at the \master. Note that regardless of $\encfunset$ and $\proto_1$ the \master cannot gain any additional information from the workers' responses.
    We now show that for $\localcomp \leq \compbound-1$ there exists no choice of a decoding function $\decfun$ and $\proto_2$ for which the \master deterministically outputs the true full gradient. To that end, for any possible $\proto_2$, we give two cases for which the inputs to the decoding function are identical but the true full gradients differ.

    The claimed partial gradients take on values as in \cref{tab:key_error_pattern} for $\nhonest=1$ and \cref{tab:key_error_pattern_generalized} for $\nhonest>1$. There are $\compbound$ partial gradients, say $\pgrad[1],\ldots,\pgrad[\compbound]$, such that for each partial gradient $\pgrad[\gind]$ worker $\worker[\gind]$ sends a value $\wpgrad{\gind}{\gind} = \pgrad[\gind]^{\prime\prime}$ that is different from the value $\pgrad[i]^\prime$ sent by all other workers.
For any list $\lgindset^{(\nround)}$ of locally computed gradients of size $|\lgindset^{(\nround)}| \leq \nmalicious-1$ (produced by any $\proto_2$) there exists an index $\widetilde{\gind} \in [\nmalicious]$ such that $\widetilde{\gind} \notin \lgindset^{(\nround)}$.
Consider the following two cases:\begin{enumerate}[label=Case \arabic{*}:, wide=1.5\parindent, leftmargin=*]
    \item $\pgrad[\gind] = \pgrad[\gind]^\prime \;\forall \gind \in [\frac{\ngrad}{\ngroup}]$ and 
    \item $\pgrad[\gind] = \pgrad[\gind]^\prime \;\forall \gind \in [\frac{\ngrad}{\ngroup}] \setminus \{\widetilde{\gind}\}$ and $\pgrad[\widetilde{\gind}] = \pgrad[\widetilde{\gind}]^{\prime\prime}$.
\end{enumerate}
It is easy to see that both cases occur with non-zero probability according to the attack strategy in \cref{sec:attack}. In both cases, the inputs to $\decfun$ only depend on the fixed $\lgindset^{(\nround)}$, the $\wpgrad{\gind}{\wind},\;\wind \in [\nmalicious +1],\;\gind\in[\frac{\ngrad}{\ngroup}]$ and the $\pgrad[\gind],\; \gind \in \lgindset^{(\nround)}$, all of which take on identical values in both cases.
The value of the full gradient $\tgrad$, however, is $\sum_{\gind \in [\ngrad]} \pgrad[\gind]^\prime$ in Case 1 and $\pgrad[\widetilde{\gind}]^{\prime\prime} + \sum_{\gind \in [\ngrad] \setminus \{\widetilde{\gind}\}} \pgrad[\gind]^\prime$ in Case 2. Hence, for $\localcomp < \compbound$ no decoding function can deterministically produce the correct full gradient.
\end{IEEEproof}

\begin{theorem}[Lower bound on $\commoh$ for fixed $\localcomp$ and $\replfact$]
Suppose that $\nworker = \ngroup (\nmalicious + \nhonest)$ for integers $\ngroup,\nhonest \geq 1$. For any tuple (\allocmat, \encfunset, \decfun, \proto), with $\allocmat$ as in \eqref{eq:fractional_repetition}, to be a (\nround,\localcomp,\replfact,\commoh)-\BGC scheme with $\replfact = {\nmalicious + \nhonest}$ and $\localcomp = \compbound$, then it must hold that
  \vspace{-0.6em}
    \begin{align*}
        \commoh \geq {\log_{\alphasize} \binom{\ngrad/\ngroup}{\lfloor \nmalicious / \nhonest \rfloor}}.%
    \end{align*}
  \vspace{-0.6em}
    \label{thm:converse_T}
\end{theorem}

\vspace{-0.3cm}
\begin{IEEEproof}
The proof is given in \cref{sec:proof_lower_bound_kappa_rho}
\end{IEEEproof}

\subsection{Construction of an \BGC Scheme}

\begin{theorem}
  The scheme constructed below is an \BGC scheme with a parameter $\nhonest$ such that $1\leq \nhonest \leq \nmalicious+1$ and requires $\nround \leq \left(\nmalicious+1-\nhonest\right) \left\lceil \log_2\left(\frac{\ngrad}{\ngroup}\right) \right\rceil$ rounds, $\localcomp \leq \compbound$ local gradient computations, $\replfact = {\nmalicious+\nhonest}$, and $\commoh \leq \left(\nmalicious+1-\nhonest\right) \left( 2 \left\lceil \log_2\left( \frac{\ngrad}{\ngroup}\right) \right\rceil+ \frac{\nmalicious+3\nhonest}{2\log_2{\card{\galpha}}} \right)$. 
    \label{thm:scheme}
\end{theorem}
\begin{IEEEproof}
    We defer the proof to Appendix~\ref{app:scheme_proof}.
\end{IEEEproof}

We construct an $(\nround, \localcomp, \replfact, \commoh)$-\BGC scheme that has a replication factor $\replfact={\nmalicious+\nhonest}$, $\nhonest \geq 1$, and requires the optimal local computation load $\localcomp \leq \compbound$ at the \master. The protocol $\proto$ runs for $\nround \leq \left(\nmalicious+1-\nhonest\right) \left\lceil \log_2\left(\frac{\ngrad}{\ngroup}\right) \right\rceil$ rounds and achieves a communication overhead \mbox{$\commoh \leq \left(\nmalicious+1-\nhonest\right) \left( 2 \left\lceil \log_2\left( \frac{\ngrad}{\ngroup}\right) \right\rceil+ \frac{\nmalicious+3\nhonest}{2\log_2{\card{\galpha}}} \right)$}. Our scheme uses a fractional repetition data assignment with $\ngroup$ groups each of size $\nmalicious+\nhonest$. Informally, in each group, the \master runs an elimination tournament consisting of matches (similar to the one explained in~\cref{ex:toy_problem}) between pairs of workers that return contradicting responses. An algorithmic description of the elimination tournament is given in \cref{alg:elimination_tournament} for the general case. For clarity of exposition, we explain the idea of our scheme for the special case of $\nhonest=1$. The general case for $\nhonest\geq 2$ follows similar steps and is given in \cref{app:general_case}.

The tournament consists of a series of matches between two workers. 
During a match, each worker constructs a binary tree based on their computed partial gradients, which we refer to as the match tree.
The root of the tree is labeled by the sum of all partial gradients computed at that worker ($\noisyiresp_{0,\wind}$).
The child nodes are constructed based on the partial gradients that are contained in the parent node.
Each node has two children: the first one is labeled by the sum of the first half of the parent node's partial gradients; the second one is labeled by the sum of the second half of the parent node's partial gradients.
Thus, the labeling is done such that the sum of the labels of any two siblings gives the label of their parent node.

Proceeding in this way recursively, each worker ends up with the leaves of the tree being labeled by individual partial gradients.
For example, when $\ngroup = 1$ and $\ngrad=4$, the tree is depicted in \cref{fig:binary_add_tree}.
During a match, the \master requests the labels for particular nodes in this tree from the two competing workers, and compares them. 
If the root labels of two match trees differ, then by definition there must be a child node for which the corresponding label differs in those trees. By induction, it is clear that there has to be a path from the root to a leaf, such that the corresponding labels of all involved nodes differ between the two match trees. Applying this observation to the example in \cref{fig:binary_add_tree}, if $\wpgrad{1}{\wind}+\wpgrad{2}{\wind}+\wpgrad{3}{\wind}+\wpgrad{4}{\wind}$ is different for two workers, then $\wpgrad{1}{\wind}+\wpgrad{2}{\wind}$ or $\wpgrad{3}{\wind}+\wpgrad{4}{\wind}$ must also differ (or both). In the latter case, we end up with $\wpgrad{3}{\wind}$ or $\wpgrad{4}{\wind}$ being different between the workers.
\begin{figure}[h]
  \vspace{-0.8em}
    \centering
    \resizebox{0.9\linewidth}{!}{
    \input{matchtree.tex}
    }
  \vspace{-1em}
    \caption{Example of a match tree for $\worker$ and parameters $\ngroup=1,\ngrad=4$.}
    \label{fig:binary_add_tree}
\end{figure}

The match starts at the root. In this case, each worker $\worker$ would have already sent the node label in $\iresp_{0,\wind}$ to the \master, i.e.,
  \vspace{-0.6em}
\begin{equation*}
    \iresp_{0,\wind} = \encfun[\wind,1]\left( \pgrad[1],\dots,\pgrad[\ngrad] \right) = \sum_{\substack{ \gind \in \range{\ngrad} \\ \allocmat[\gind, \wind] = 1}} \pgrad[\gind].
  \vspace{-0.8em}
\end{equation*}
Note that, without errors, all workers' messages agree within a group.
In case of discrepancies between the responses $\noisyiresp_{0,\wind}$ of the workers within a group, the \master selects a pair of disagreeing workers $\worker[\wind_1],\worker[\wind_2]$ and further descends in the tree as follows.

For every node in the tree, the \master requests and compares the left child's label from both workers, encoded in $\iresp_{\roundind,\indone}$ and $\iresp_{\roundind,\indtwo}$. The \master then moves on to a child whose label the workers disagree on.
Note that, based on the current node's label and the left child's label, the \master can infer the right child's label.
If the competing workers agree on the left child's label, they must disagree on the right child's label.
This procedure is repeated until a leaf is reached.
Note that, even if the workers send inconsistent labels at each round, this procedure is guaranteed to reach a leaf node for which the (sent or inferred) values of the individual gradient is different for the two workers. \cref{alg:match} formalizes this procedure for $\ngroup=1$.

Note that it is possible to reduce the communication load by picking a coordinate $\compind$ in which the workers' initial responses disagree, i.e., $\comp{ \iresp_{0,\indone} } \neq \comp{ \iresp_{0,\indtwo} }$. That is, the workers only encode the $\compind$-th coordinate of the node labels. By this method, it is still guaranteed that the \master can identify a partial gradient for which the competing workers disagree.

Having identified disagreeing values $\wpgrad{\gind}{\wind_1}$ and $\wpgrad{\gind}{\wind_2}$ of a partial gradient $\pgrad[\gind]$, the \master computes the correct of this partial gradient locally. It then marks the worker(s) whose values differ from the value computed locally as malicious.
The algorithm ends up with disagreeing leaf labels by design. Therefore, each match is guaranteed to eliminate at least one malicious worker and no honest workers.
After performing at most $\nmalicious$ matches, the \master is guaranteed to identify all malicious workers.
The \master takes the encoded gradient of one worker identified as being honest from each group and sums up the group-wise results to obtain $\gestim=\tgrad$.

\subsection{Discussion}
According to \cref{thm:converse_C}, the interactive protocol of our scheme achieves the lowest possible number of locally computed partial gradients for the fractional repetition data assignment and $\replfact = \nmalicious+\nhonest$. Note that for $\nhonest \geq \nmalicious+1$, i.e., $\replfact \geq 2\nmalicious+1$, no local computation is necessary. In fact, since there is only one set of consistent workers $\workerset$ that has $\card{\workerset} \geq \nhonest$ per fractional repetition group, the scheme immediately identifies $\workerset$ as the honest set and terminates without any additional computation or communication. Thus, as shown in~\cite{chenDRACOByzantineresilientDistributed2018}, our scheme achieves the optimal performance in this case. Although we consider $1 \leq \nhonest \leq \nmalicious+1$ in \cref{thm:scheme} for technical reasons, the scheme works for any $\nhonest \geq 1$.
Note that the achievable bound on $\commoh$ in \cref{thm:scheme} is off from the converse bound in \cref{thm:converse_T} by a constant factor asymptotically, see \cref{fig:achievability_vs_converse_datacolumns} in Appendix~\ref{app:achievability_vs_converse}. The reason is that we use a rather simple and conservative lower bound on the amount of information that is required to be transmitted by the workers in \cref{thm:converse_T}. Among others, we assume that all workers have knowledge about the malicious worker's identities and also the malicious workers contribute useful information. Improvement of the converse bound is left for future work.
