\section{Problem Setting}
We first set the notation. 
Matrices and vectors are denoted by upper-case and lower-case bold letters, respectively. $\mathbf{A}_{i,j}$ refers to the element in row $i$ and column $j$ of the matrix $\mathbf{A}$. Scalars are denoted by lower-case letters, sets by calligraphic letters, and lists by fractal letters, respectively, e.g., $a$, $\mathcal{A}$ and $\lis{A}$. For an integer $a \geq 1$, we define $\range{a} \defeq \left\{ 1,2,\dots,a \right\}$.
Let $\lis{A}_i, \, i=1,\dots,t$, be a collection of lists, we define $\lis{A}^{(t)}$ to be their concatenation.
We use $\boldsymbol{1}_{m \times n}$ and $\boldsymbol{0}_{m \times n}$ to denote the all-one and all-zero matrices of dimension $m \times n$. 
\ARXIVonly{For a list of symbols, cf. \cref{app:notations}.}

We consider a \emph{synchronous distributed gradient descent} setting, in which the goal is to fit the parameters $\params \in \R^\graddim$ of a model to a dataset consisting of $\ngrad$ samples $\sample \in \R^\graddim, \gind \in
\range{\ngrad}$. This is done by finding (local) optima for the problem
$\displaystyle \argmin_{\params \in \R^\graddim} \sum_{\gind \in \range{\ngrad}} \loss(\params, \sample)$ 
for a per-sample loss function $\loss(\params, \sample)$.
The gradient descent algorithm starts with a random initialization for the 
parameter vector, defined as $\params^{(0)}$, and then iteratively applies the update rule $
    \params^{(\gditer+1)} = \params^{(\gditer)} - \frac{\learningrate}{\ngrad} \sum_{\gind \in
    \range{\ngrad}} \nabla \loss(\params^{(\gditer)}, \sample),
$ where $\gditer$ is the iteration index and $\learningrate \in \R$ is referred to as the learning rate. For notational convenience, we define the evaluation of the gradient of the loss function at individual samples as $
    \pgrad^{(\gditer)} \defeq \nabla \loss(\params^{(\gditer)}, \sample)
$ and call them \emph{partial gradients}.
\Revision{%
Since in practice data 
is quantized to a finite set of values, we 
take the partial gradients to be
vectors over a finite alphabet $\galpha$, i.e., $\pgrad^{(\gditer)} \in \galpha^\graddim$.}

Consider a system comprising a \master and $\nworker$ worker nodes, $\nmalicious$ of which might be malicious.
The malicious workers can send arbitrarily corrupted information to the \master. At the start of the procedure, the \master distributes the samples to the workers with some redundancy.
Then, each iteration $\gditer$ starts with the \master broadcasting the current parameter vector $\params^{(\gditer)}$. The workers then compute the partial gradients $\pgrad^{(\gditer)}$ corresponding to the samples they store. At the end of the iteration, the \master must obtain the \emph{full gradient} $\tgrad^{(\gditer)} \defeq \sum_{\gind \in \range{\ngrad}} \pgrad^{(\gditer)}$ irrespective of the actions of the $\nmalicious$ malicious workers.

In this work, we are concerned with the problem of reliably reconstructing $\tgrad^{(\gditer)}$ \emph{exactly} at the \master in each iteration $\gditer$.
In the sequel, we only consider a single iteration of gradient descent and omit the superscript $\gditer$.


\section{Byzantine-Resilient Gradient Coding}
For a better grasp of our ideas, we start with a toy example that captures the concepts introduced and studied.

\subsection{How to Catch Liars Efficiently?}
\input{sections/toy_example.tex}

\vspace{-0.1cm}
\subsection{The Framework}

Carrying over this idea to the gradient coding framework, we next define gradient coding schemes resilient against an  adversary controlling $\nmalicious$ malicious workers.

\begin{definition}[Byzantine-resilient gradient coding scheme]
    A Byzantine-resilient gradient coding scheme tolerating $s$ malicious workers, referred to as \BGC, is a tuple $\left( \allocmat, \encfunset, \decfun, \proto \right)$ where
\begin{itemize}
    \item $\allocmat \in \{0, 1\}^{\ngrad \times \nworker}$ is a \textbf{data assignment matrix} in which $\allocmat[{\gind, \wind}]$ is equal to $1$ if the $\gind$-th data sample is given to the $\wind$-th worker and $0$ otherwise,
    \item $\encfunset \defeq \left(\encfun[\wind,\encind] \mid \wind\in \range{\nworker}, \encind \in \range{\nencfun}\right)$ is the list of $\nworker\nencfun$ \textbf{encoding functions} used by the workers such that $\encfun[\wind,1]$ corresponds to a gradient code dictated by $\allocmat$ and $\encfun[\wind,\encind]$ depends only on the gradients assigned to $\worker$, %
    \item $\proto = (\proto_1,\proto_2)$ is a \textbf{multi-round protocol} in which $\proto_1$ selects the indices of the encoding functions to be used by the workers and $\proto_2$ selects gradients to be locally computed at the \master,
    \item and $\decfun$ is a \textbf{decoding function} used by the \master after running the protocol $\proto$ to always output the correct full gradient if the number of malicious workers is at most $\nmalicious$.
\end{itemize}
\end{definition}

Each worker initially ($\roundind=0$)
sends a vector $\iresp_{0,\wind} \defeq \encfun[\wind,1]\left( \pgrad[1],\dots,\pgrad[\ngrad] \right) \in \galpha^{\graddim}$ that is a codeword symbol of a gradient code~\cite{tandonGradientCoding2017}. The protocol $\proto$ then runs for $\nround \in \N$ rounds.
In each round $\roundind \in \range{\nround}$, the \master uses $\proto_1$ to select an encoding function $\encfun[\wind,\encind_{\roundind,\wind}]$ for each worker $\worker$ and communicates its index $\encind_{\roundind,\wind}$ to the respective worker. 
Each worker $\worker$ then computes a response $\iresp_{\roundind,\wind} = \encfun[\wind,\encind_{\wind,\roundind}]\left( \pgrad[1],\dots,\pgrad[\ngrad] \right) \in \galpha^{\respdim{\wind}{\roundind}}$ for some $\respdim{\wind}{\roundind} \in \N_0$ and sends a vector $\noisyiresp_{\roundind,\wind} \in \galpha^{\respdim{\wind}{\roundind}}$ to the \master.
For honest workers $\noisyiresp_{\roundind,\wind} = \iresp_{\roundind,\wind}$, while for malicious workers, $\noisyiresp_{\roundind,\wind}$ may be chosen arbitrarily.

In every round, the \master uses $\proto_2$ to choose a set of partial gradients to compute locally. We denote the list of indices of the locally computed partial gradients in round $\roundind$ by $\lgindset_\roundind$ and the corresponding list of partial gradient values by $\locgradset_\roundind \defeq (\pgrad[\gind] \mid \gind \in \lgindset_\roundind )$.
Analogously, we define $\irespset_\roundind \defeq (\iresp_{\roundind, \wind} \mid \forall \wind \in [\nworker])$ and 
$\recresset_\roundind \defeq (\noisyiresp_{\roundind, \wind} \mid \forall \wind \in [\nworker])$.

The protocol $\proto_1$ selects the indices of the encoding functions to be used in the current round $t$ based on the received results and locally computed gradients from previous rounds.
After receiving the results in the current round, the \master uses $\proto_2$ to select the gradients to compute locally in this round, i.e.,
\vspace{-0.2em}
\begin{align}
    \encind_{1,\roundind}, \dots, \encind_{\nworker, \roundind} &= \proto_1\left(\roundind, \recresset^{(\roundind-1)}, \lgindset^{(\roundind-1)}, \locgradset^{(\roundind-1)} \right), \label{eq:encind}\\
    \lgindset_\roundind &= \proto_2\left(\roundind, \recresset^{(\roundind)}, \lgindset^{(\roundind-1)}, \locgradset^{(\roundind-1)} \right) \label{eq:indset}.
\end{align}
After round $\nround$, the main node computes an estimate $\gestim$ of $\tgrad$ using the decoding function 
    $\gestim = \decfun\left(\recresset^{(\nround)}, \lgindset^{(\nround)}, \locgradset^{(\nround)} \right).$
The total number of partial gradients computed at the \master is defined as $\localcomp \defeq \card{\lgindset^{(\nround)}}$.

A valid \BGC scheme must output $\gestim = \tgrad$ if the number of malicious workers is at most $\nmalicious$.


\subsection{Figures of Merit}
An \BGC scheme is evaluated by the maximum number of rounds $\nround$ and the maximum number of local computations $\localcomp$ required by $\proto$, and its replication factor and communication overhead, which we define next.

\begin{definition}[Replication factor and communication overhead]
The \textbf{replication factor} of an \BGC scheme is the average number of workers to which each sample is assigned, i.e.,
\vspace{-1em}
\begin{equation*}
    \replfact \defeq \frac{\sum_{\gind \in \range{\ngrad}, \wind \in \range{\nworker}} \allocmat[{\gind, \wind}]}{\ngrad}.
\end{equation*}

The \textbf{communication overhead} is the maximum number of symbols from $\galpha$ transmitted from the workers to the \master during $\proto$, i.e.,
\begin{equation*}
    \commoh \defeq {\sum_{\roundind \in \range{\nround}, \wind \in \range{\nworker}} \respdim{\wind}{\roundind}}.
\end{equation*}
\end{definition}



We say that a tuple $(\allocmat, \encfunset, \decfun, \proto)$ is a $(\nround, \localcomp, \replfact, \commoh)$-\BGC scheme if in the presence of at most $\nmalicious$ malicious workers, the scheme always outputs $\gestim = \tgrad$ by requiring at most $\nround$ communication rounds, at most $\localcomp$ local computations, and has replication factor $\replfact$ and communication overhead less than or equal to $\commoh$.

We study settings in which the number of workers $\nworker$ divides $\nmalicious+1$, i.e., $\nworker = \ngroup (\nmalicious + 1)$ for some integer $\ngroup$ and only consider balanced data assignments, i.e., every worker computes the same number of gradients.
We focus on the particular case of a fractional repetition data assignment~\cite{tandonGradientCoding2017}. That is, the \master partitions the workers into $\ngroup$ groups of size $\frac{\nworker}{\ngroup}$ each and assigns the same data samples to all workers within a group.
The data assignment matrix is constructed as 
\vspace{-0.8em}
\begin{equation}
\label{eq:fractional_repetition}
\allocmat = \begin{bmatrix}
        \boldsymbol{1}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} & \boldsymbol{0}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} & \dots & \boldsymbol{0}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} \\
        \boldsymbol{0}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} & \boldsymbol{1}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} &   \dots & \boldsymbol{0}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} \\
        \vdots & \vdots & \ddots & \vdots \\
        \boldsymbol{0}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} & \boldsymbol{0}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}} & \dots & \boldsymbol{1}_{\frac{\ngrad}{\ngroup} \times \frac{\nworker}{\ngroup}}
\end{bmatrix}.
\end{equation}
  \vspace{-1.1em}

Since we focus on this particular data assignment, the initial worker responses are given by the sum of all computed gradients
\smash{$\iresp_{0,\wind} =
\sum_{
\substack{\gind \in \range{\ngrad} \\ \allocmat[\gind,\wind]=1}
} \pgrad[\gind]$}, which form a valid gradient code.
