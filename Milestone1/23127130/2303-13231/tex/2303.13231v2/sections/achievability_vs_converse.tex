For convenience, we reproduce our achievability and converse bound on $\commoh$ from \cref{thm:scheme} and \cref{thm:converse_T}, respectively.
For fractional repetition data allocation, with $\ngroup$ groups, $\nworker = \ngroup(\nmalicious+\nhonest)$ workers, $\nmalicious$ of which are malicious, our scheme requires data transmission of at most
\begin{align*}
  \commoh[achieve] = \left(\nmalicious+1-\nhonest\right) \left( 2 \left\lceil \log_2\left( \frac{\ngrad}{\ngroup}\right) \right\rceil+ \frac{\nmalicious+3\nhonest}{2\log_2{\card{\galpha}}} \right)  
\end{align*}
during the interactive protocol as measured in symbols from $\galpha$.
Our lower bound shows, that for any computation optimal \BGC, \commoh is at least
\begin{align*}
  \commoh[bound] = {\log_{\alphasize} \binom{\ngrad/\ngroup}{\lfloor \nmalicious / \nhonest \rfloor}}.%
\end{align*}

\begin{figure}[h]
    \centering
    \resizebox{0.98\linewidth}{!}{
    \input{tikz/kappa_vs_p}
    }
    \caption{Comparison of converse and achievability for $\commoh$ over the dataset size $\ngrad$. We consider a system of $\nworker=10$ workers, $\ngroup=1$ group and an alphabet size $|\galpha|=2^{16}$. As the percentage of malicious workers rises from \SI{50}{\percent} to \SI{90}{\percent} the communication overhead of the scheme increases. }
    \label{fig:achievability_vs_converse_datacolumns}
\end{figure}

The gap between our scheme and the bound is depicted in~\cref{fig:achievability_vs_converse_datacolumns}.
It shows that for any parameter $\nmalicious$, the scheme is a constant factor away from the bound.
Typically, in distributed gradient descent applications the number of parameters $\graddim$ and the number of samples $\ngrad$ are very large, whereas the number of workers $\nworker$ and as a consequence $\nmalicious$, $\nhonest$ and $\ngroup$ are small by comparison.
For large numbers of samples $\ngrad$, the ratio $\commoh[achieve]/\commoh[bound]$ tends to 
\begin{align}
  \label{eq:conv_limit}
\lim_{\ngrad \to \infty} \frac{\commoh[achieve]}{\commoh[bound]} &= 2 \log_2(|\galpha|) \frac{(\nmalicious - \nhonest + 1)}{\lfloor \nmalicious / \nhonest 
\rfloor}.
\end{align}

The convergence behavior can be observed in \cref{fig:convergence}.
\begin{figure}[h]
    \centering
    \resizebox{0.98\linewidth}{!}{
    \input{tikz/kappa_relative_vs_p}
    }
    \caption{Convergence of the ratio $\frac{\commoh[achieve]}{\commoh[bound]}$ to the limit given in $\eqref{eq:conv_limit}$ for large numbers of samples.
      The parameters are $\nworker=10$, $\ngroup=10$, $|\galpha|=2^{16}$.
    For $\nmalicious=5$ and $\nmalicious=9$ the limits as in \eqref{eq:conv_limit} yield the same value.}
    \label{fig:convergence}
\end{figure}
Note that depending on the alphabet the communication overhead of our scheme can be slightly improved as stated in the following.
\begin{remark}[Compression Beyond the Alphabet Size]
  If for every pair of elements $a, b \in \galpha$ there exists a
  function $f: \galpha \mapsto \mathcal{B}$, that maps from $\galpha$ to a
smaller alphabet $\mathcal{B}$, such that $f(a) \neq f(b)$ and an operation $\oplus$ with the property $\forall c, d, e, g \in \galpha: f(c+d) \neq f(e+g) \implies f(c) \neq f(d)\text{ or }f(e) \neq f(g)$, then 
our scheme can be improved to $\commoh \geq \left(\nmalicious+1-\nhonest\right) \left( 2 \lceil \log_{|\galpha|}(|\mathcal{B}|)\rceil \left\lceil \log_2\left( \frac{\ngrad}{\ngroup}\right) \right\rceil+ \frac{\nmalicious+3\nhonest}{2\log_2{\card{\galpha}}} \right)$.
At the start of each match, the main node chooses not only the index $\compind$ but also the appropriate function $f$ and communicates it to the two workers. They then use $f$ to compress their transmitted symbols during the match. 
\end{remark}



