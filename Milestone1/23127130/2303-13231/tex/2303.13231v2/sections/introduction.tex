\section{Introduction}

Consider the setting of a \master possessing large amounts of data on which a machine learning model shall be trained using gradient descent. To speed up the learning process, the \master distributes the computations to several worker nodes~\cite{lian2017can,abadi2016tensorflow}.
One of the main vulnerabilities of distributed gradient descent is the presence of Byzantine errors corrupting some workers' computation results~\cite{lamportByzantineGeneralsProblem}. Even a single corrupted computation result can drastically deteriorate the performance of the algorithm~\cite{damaskinos2019aggregathor}.

The problem of tolerating Byzantine errors in distributed computing has been considered in different settings. For example, for linear computations\RevisionRemove{, i.e., matrix-matrix or matrix-vector multiplications,}~\cite{hofmeisterSecurePrivateAdaptive2022,tangAdaptiveVerifiableCoded2022} use Freivalds' algorithm to detect Byzantine errors with high probability and exclude them in further processing. For polynomial computations, \cite{yuLagrangeCodedComputing2019} uses properties of error-correcting codes to correct \Revision{errors}\RevisionRemove{the erroneous results}. Other approaches towards the mitigation of erroneous results include \RevisionRemove{the use of} group testing and Reed-Solomon codes~\cite{solankiNonColludingAttacksIdentification2019}, and homomorphic hash functions~\cite{keshtkarjahromiSecureCodedCooperative2019}. For a more comprehensive review of the existing literature, we refer the interested reader to~\cite{hofmeisterSecurePrivateAdaptive2022}. %

For distributed gradient descent each worker computes a gradient of a so-called loss function for local training data. The \master aggregates \RevisionRemove{the locally computed}\Revision{these partial} gradients into a total gradient. The problem of Byzantine errors in this context has been first tackled using robust aggregation functions, which select only a subset of the workers' results. The selection is based on minimizing the distance to other results~\cite{guerraoui2018hidden,chenDistributedStatisticalMachine2017,blanchardMachineLearningAdversaries,rajputDETOXRedundancybasedFramework,damaskinos2019aggregathor,el2020fast}, by assigning a sanity score to each result~\cite{xieZenoDistributedStochastic}, or by general consistency checks~\cite{konstantinidisRobustDetection}. Using such aggregation functions, however, the resulting gradient estimate may be only an inexact approximation of the desired total gradient in the error-free case. This increases the runtime for the gradient descent algorithm, see e.g.~\cite{bitarStochasticGradientCoding2020} and references therein, and might perform poorly in some particular settings, e.g., when the distribution of the training data is not identical among the workers~\cite{chenRevisitingDistributedSynchronous2017, tandonGradientCoding2017}. 
Moreover, advanced gradient descent techniques, such as the momentum method~\cite{sutskeverImportanceInitializationMomentum}, are in general not compatible with approximate schemes~\cite{tandonGradientCoding2017}.

Due to the latter, the problem of tolerating Byzantine errors in distributed gradient descent with exact recovery has been approached from a coding-theoretic perspective. For linear and polynomial computations, coding over the input data at the \master has been proposed in~\cite{yuLagrangeCodedComputing2019} and~\cite{dataDataEncodingByzantineResilient2021}. Since the computations in gradient descent are highly non-linear in general, these approaches might not be applicable. Instead,\RevisionRemove{ the work of}~\cite{chenDRACOByzantineresilientDistributed2018} introduces DRACO, a framework that performs coding over the computation results at the workers. The authors build on the idea of gradient coding~\cite{tandonGradientCoding2017}, which \RevisionRemove{had}\Revision{was} originally\RevisionRemove{ been} designed to mitigate the effect of stragglers, i.e., slow or unresponsive workers. By replicating each gradient computation to $\nmalicious+1$ different workers, gradient coding can tolerate $\nmalicious$ stragglers by treating them as erasures. Applying the same ideas, DRACO can tolerate $\nmalicious$ malicious workers instead, treating the computations of malicious workers as errors. This comes at the cost of increasing the replication of each gradient computation to $2\nmalicious+1$, hence, causing a large computation overhead. Both DRACO and gradient coding are shown to achieve an optimal replication for the respective problem settings.

In this work, we also consider the problem of exact gradient coding in the presence of an adversary controlling $\nmalicious$ malicious workers, that introduce Byzantine errors in their computation results.
In contrast to~\cite{chenDRACOByzantineresilientDistributed2018}, we define a more general framework in which the \master can run a small number of computations itself to aid in decoding. We propose a scheme that requires a replication of only $\nmalicious+1$ at the expense of running $\nmalicious$ local gradient computations\footnote{A replication of $\nmalicious+1$ means that \Revision{\emph{each}} gradient computation is run $\nmalicious+1$ times; the cost of $\nmalicious$ local computations \Revision{\emph{in total}} is very small in comparison.}. The idea of running local computations at the \master is also used in~\cite{caoDistributedGradientDescent2019,prakash2020secure}, where the \master computes an estimate of the true gradient from only few data samples and discards worker results that have a large distance to this estimate. 
In contrast to our work, those solutions do not guarantee exact recovery of the total gradient at the \master. We design additional light communication between the workers and the \master to help identify which gradients should be computed locally. We generalize the framework to the case where less than $\nmalicious$ local computations are allowed at the \master and explore the tradeoff between replication and local computation.
