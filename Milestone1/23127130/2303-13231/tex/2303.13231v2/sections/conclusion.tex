\section{Conclusion}
We considered the problem of distributed learning in the presence of Byzantine computation errors. We introduced a framework that extends the known gradient coding framework by adding an interactive light communication between the \master and the workers and verifying local computations at the \master.
In the scope of this framework, we proposed a new scheme that can tolerate $\nmalicious$ malicious workers with a computational redundancy of $\nmalicious+\nhonest$ for any $\nhonest \geq 1$. We showed that with a fractional repetition data assignment, the scheme achieves the optimal number of local computations at the \master. Future work includes the improvements of the converse and achievability bounds, the generalization of the fundamental limits to a broader class of data assignments, and the investigation into optimal Byzantine-resilient gradient coding schemes.
