% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{color}
\usepackage{multirow}
\usepackage{indentfirst}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\def\xnote#1{\textcolor{red}{WQ: #1}}
\def\rnote#1{\textcolor{blue}{RUOLIN: #1}}
\def\hnote#1{\textcolor{green}{XH: #1}}
\def\jnote#1{\textcolor{yellow}{JY: #1}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4898} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Appendix for GarmentTracking: Category-Level Garment Pose Tracking}

\maketitle

\section{VR-Folding Dataset}
\subsection{Task Definition}
In this section, we will describe the details of our task definition in our VR-Folding dataset. 
\subsubsection{Flattening}
\cref{fig:flattening_example} gives some examples of \textit{Flattening} task. The goal of this task is making a garment in randomly crumpled state into a canonical flattened T-pose (see \cref{fig:flattening_example}) by a series of actions (\eg grasp, fling). Inspired by GarmentNets\cite{chi2021garmentnets} and FlingBot\cite{ha2022flingbot}, we firstly grasp the garment on one randomly selected point to simplify its initial configuration and increase its visibility, then fling the garment with both hands to flatten it efficiently. Specifically, this task can be divided into the following steps:
\begin{enumerate}
    \item \textbf{Grasp with a single hand}: The volunteer will grasp one randomly selected point on the garment with a single hand, and lift it in the air (see sub-figure [1] for Shirt in \cref{fig:flattening_example}). This initial configuration is similar to the definition in GarmentNets\cite{chi2021garmentnets}.
    \item \textbf{Grasp with both hands}: The volunteer will try to grasp two separate points on the garment with two hands simultaneously, and get ready for the following \textit{fling} action (see sub-figure [2] for Shirt in \cref{fig:flattening_example}).
    \item \textbf{Fling}: The volunteer will fling the garment with both hands to erase the wrinkles (see sub-figure [3-7] for Shirt in \cref{fig:flattening_example}).
    \item Repeat Step 2 and Step 3 until the garment is in flattened T-pose (see sub-figure [7-12] for Shirt in \cref{fig:flattening_example}). 
\end{enumerate}

\begin{figure*}[ht!]
  \centering
    %   \fbox{\rule{0pt}{8.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.93\linewidth]{fig/flattening_example.pdf}
   \caption{The examples of \textbf{Flattening} task.}
   \label{fig:flattening_example}
\end{figure*}

\subsubsection{Folding}
\cref{fig:folding_example} gives some examples of \textit{Folding} task. This task can be achieved by a series of bimanual pick-and-place actions. We make some general rules for the whole folding process. In order to enhance data variety, we encourage volunteers to fold garments in different ways (\eg fold from the left part or fold from the right part) without violating these rules. 
Here is the detailed rules of \textit{Folding} for each category:
\begin{enumerate}
    \item \textbf{Shirt}: The volunteer performs 2 (short-sleeve) or 3 (long-sleeve) pick-and-place actions with both hands for \textit{Shirt} (see examples for \textit{Shirt} in \cref{fig:folding_example}). For long-sleeved shirts, the first two actions will make the two sleeves folded, and the last action will make the trunk part folded. For short-sleeved shirts, the first action will fold in half along the left and right direction, and the last action will fold in half along the up and down direction.
    \item \textbf{Pants}: The volunteer performs 2 pick-and-place actions with both hands for \textit{Pants} (see examples for \textit{Pants} in \cref{fig:folding_example}). The first action will fold in half along the left and right direction, and the last action will fold in half along the up and down direction.
    \item \textbf{Top}: The volunteer performs 2 pick-and-place actions with both hands for \textit{Top} (see examples for \textit{Top} in \cref{fig:folding_example}). The first action will fold in half along the left and right direction, and the last action will fold in half along the up and down direction.
    \item \textbf{Skirt}: The volunteer performs 2 pick-and-place actions with both hands for \textit{Skirt} (see examples for \textit{Skirt} in \cref{fig:folding_example}). The first action will fold in half along the left and right direction, and the last action will fold in half along the up and down direction.
\end{enumerate}
\begin{figure*}[th!]
  \centering
        % \fbox{\rule{0pt}{8.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.93\linewidth]{fig/folding_example.pdf}
   \caption{The examples of \textbf{Folding} task.}
   \label{fig:folding_example}
\end{figure*}

\subsection{Data Statistics}

\begin{table}[h!]
\resizebox{0.48\textwidth}{!}{
  \centering
  \begin{tabular}{c|c|c|c|c|c}
    \toprule
    & & Shirt & Pants & Top & Skirt\\
    \midrule
    \multirow{2}{*}{Folding} & \# of instances & 989 & 1538 & 903 & 461\\
    & \# of videos & 993 & 1551 & 889 & 463 \\
    % double check here!!
    & \# of frames & 66830 & 77479 & 37214 & 23427\\
    % & \# of frames & 101,227 & 117303 & 56341 & 35468\\
    \midrule
    \multirow{2}{*}{Flattening} & \# of instances & 1037 & 1631 & 1040 & 467 \\
    & \# of videos & 2145 & 1720 & 1063 & 943 \\
    % double check here!!
    & \# of frames & 243460 & 136411 & 132580 & 73357\\ 
    % & \# of frames & 337192 & 188929 & 183623 & 101599\\ 
    \bottomrule
  \end{tabular}
}
  \caption{Statistics of VR-Folding dataset.}
  \label{tab:data_stat}
\end{table}
The detailed data statistics of VR-Folding dataset is shown in \cref{tab:data_stat}. In the re-rendering process, we capture frames in Unity at the speed of 10 FPS. Each video only contains one instance, but one instance could occur in multiple videos. Each frame contains 4-view RGB-D images along with mask and complete garment mesh with NOCS labels. The multi-view RGB-D frames will be filtered with masks and transformed into point cloud. We split the whole dataset into \textit{train}, \textit{val}, \textit{test} subset on ratio $[0.8, 0.1, 0.1]$ by instances, which means that all the instances in videos of \textit{val} or \textit{test} subset are \textbf{unseen} during training. 
\section{GarmentTracking Network}
\subsection{3D Feature Extractor}
The 3D feature extractor (sparse ResUNet3D) used in our GarmentTracking network is based on FCGF\cite{choy2019fully}. The sparse 3D convolution operation is implemented by MinkowskiEngine\cite{MinkowskiEngine}. In our design, the output feature channels of each 3D convolution layer and transpose 3D convolution layer are $[64, 64, 128, 256]$ and $[64, 64, 64, 128]$ respectively. The feature dimension of output per-point feature is 64. Besides, we remove the L2-normalized operation for the final feature, because the relation attention module described below will perform L2-normalization for the feature later. Other hyper-parameters of the network structure are the same as FCGF\cite{choy2019fully}.

\subsection{Relation Attention Module (RAM)}
The self-attention module and cross-attention module are both based on the same Transformer-like structure called \textit{Relation Attention Module (RAM)} proposed by PTTR\cite{zhou2022pttr}. RAM can be formulated as $\operatorname{Att}(\mathbf{Q}, \mathbf{K}, \mathbf{V})$, which firstly projects input feature vectors of "Query (Q)", "Key (K)" and "Value (V)" into a latent feature space and then estimates the attention matrix between "Query" and "Key". The attention matrix is then applied to the "Value" feature to obtain the final attention product. 
Specifically, the self-attention and cross-attention operation can be formulated as \cref{eq:att1}, \cref{eq:att2} and \cref{eq:att3}:
\begin{align}
     \overline{\mathbf{X}}_1=\operatorname{Att}\left(\mathbf{X}_1, \mathbf{X}_1, \mathbf{X}_1\right) \label{eq:att1} \\ \overline{\mathbf{X}}_2=\operatorname{Att}\left(\mathbf{X}_2, \mathbf{X}_2, \mathbf{X}_2\right) \label{eq:att2} \\
    \hat{\mathbf{X}}=\operatorname{Att}\left(\overline{\mathbf{X}}_2, \overline{\mathbf{X}}_1, \overline{\mathbf{X}}_1\right) \label{eq:att3}
\end{align}
where $\mathbf{X}_1$ and $\mathbf{X}_2$ stands for features of previous frame and current frame respectively. $\overline{\mathbf{X}}_1, \overline{\mathbf{X}}_2$ are the self-attention features and $\hat{\mathbf{X}}$ is the cross-attention fusion feature. 
It is worth noting that RAM is a lightweight transformer, which only brings small computing overhead for the whole pipeline. 
The layer number and the head number of RAM are both 1. The feature dimension in the middle layer of RAM is 64. The MLP channels in the final feature layer is $[64, 128, 128]$. The dimension of the final fusion per-point feature is 128.

\subsection{NOCS Refiner}
The detailed network structure of NOCS Refiner is shown in \cref{tab:nocs_refiner}. We use ReLU as  activation function and use batch normalization in all the MLPs. During inference, the \textit{PC Refiner} will refine the raw NOCS class logits for each frame. However, we find that the \textit{Mesh Refiner} only needs very few steps to achieve reasonable results during tracking. So we only enable the \textit{Mesh Refiner} for the first few frames during long-term tracking for inference, which could increase the stability of the final outputs. For \textit{Folding} task, we enable it only in the first frame. For \textit{Flattening} task, we enable it in the first 15 frames in the video. 
\begin{table}[h!]
  \centering
  \begin{tabular}{c|c}
    \toprule
    Name & Feature Dimensions of MLP \\
    \midrule
    PC-PointNet & [326, 256, 256, 1024] \\
    Mesh-PointNet & [3, 64, 128, 1024] \\
    Mesh Fusion MLP & [2112, 512, 512, 1024] \\
    Mesh Refine MLP & [1024, 512, 256, 6] \\
    PC Refine MLP & [2304, 1024, 512, 192] \\
    \bottomrule
  \end{tabular}
  \caption{The detailed network structure of NOCS Refiner.}
  \label{tab:nocs_refiner}
\end{table}

\subsection{Details of Training and Inference}
The batch size in training is 16 for all of our experiments. During training, we randomly select two continuous frames in the same video as input. During inference, we use the predicted per-point NOCS coordinate and the refined canonical mesh of the previous frame as the input of the current frame. We remove the static frames (\ie frames without garment movement) and only track the moving frames (\ie frames that contain garment movement) in videos during inference.

\section{Additional Experiment Results}
\subsection{Noise Parameters for Robustness Experiment}
In the robustness experiment (Sec. 5.3.3) of the main paper, we augment the point-cloud NOCS coordinates of the first frame with a global scaling factor $\mathbf{s}_{pc}$, a global offset $\mathbf{o}_{pc}$, and Gaussian noise standard deviation $\delta$. Besides, we also augment the canonical mesh with global scaling factor $\mathbf{s}_{mesh}$. The detailed setting of these noise parameters is shown in \cref{tab:noise_params}.
\begin{table}
  \centering
  \begin{tabular}{c|c|c|c|c}
    \toprule
    Noise Level & $\mathbf{s}_{pc}$ & $\mathbf{o}_{pc}$ & $\delta$ & $\mathbf{s}_{mesh}$\\
    \midrule
    1x & $[0.8, 1.2]^3$ & $[0, 0.1]^3$ & 0.05 & $[0.8, 1.2]^3$ \\
    2x & $[0.6, 1.4]^3$ & $[0, 0.2]^3$ & 0.10 & $[0.6, 1.4]^3$ \\
    3x & $[0.4, 1.6]^3$ & $[0, 0.3]^3$ & 0.15 & $[0.4, 1.6]^3$ \\
    \bottomrule
  \end{tabular}
  \caption{The noise parameters in the robustness experiment. $[a, b]^3$ indicates a 3-D vector (\ie x, y, z axis) in which each dimension is uniformly sampled from $[a, b]$.}
  \label{tab:noise_params}
\end{table}
\subsection{Qualitative Results on VR-Folding Dataset}
\cref{fig:vis_sample_sim} shows additional qualitative results on \textbf{unseen} instances in VR-Folding dataset. We use GarmentNets\cite{chi2021garmentnets} as the baseline here. \textbf{Please watch the video in the supplementary files for more elaborate examples}.
\begin{figure*}[th!]
  \centering
        % \fbox{\rule{0pt}{8.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.94\linewidth]{fig/vis_samples_sim.pdf}
   \caption{The additional qualitative results on \textbf{unseen} instances in VR-Folding dataset. \textbf{Please watch the video in the supplementary files for more elaborate examples}.}
   \label{fig:vis_sample_sim}
\end{figure*}

\subsection{Real-world Experiments}
We collect some real-world RGB-D ($640\times480$, 30FPS) videos of garment manipulation with 4 Realsense L515\cite{L515} LiDAR cameras. Before recording, We perform calibration (\ie intrinsic and extrinsic parameters) for the four L515 cameras. During recording, we ask the volunteer to repeat the same garment manipulation actions in VR-Folding dataset on \textbf{novel} garments in real world. After recording, we uniformly drop $3/4$ of the frames in the raw videos which makes the final videos all at 7.5FPS. Nextly, we perform post-processing for the recorded data. We firstly generate partial point cloud from the depth map of each camera then merge the multi-view partial point cloud into one single point cloud. Finally, we ask volunteers to segment the merged point cloud and only keep the garment part in the point cloud.

In order to narrow the gap between simulation and real world, we re-train our model and GarmentNets with pure depth information (\ie no RGB in point cloud) and perform zero-center operation for the input point cloud during training. During inference, we directly use the GarmentNets prediction (\ie canonical coordinates and mesh) as the first-frame pose. 

\cref{fig:vis_sample_real} and \cref{fig:vis_sample_real_2} show some qualitative results on \textbf{unseen} garments in our collected real-world data. Our method can directly track pose for novel garments in the real-world with a model trained only on our simulated data, and exhibit more stability compared to the baseline (\ie GarmentNets). 
\textbf{Please watch the video in the supplementary files for more elaborate examples}.
\begin{figure*}[th!]
  \centering
        % \fbox{\rule{0pt}{8.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.94\linewidth]{fig/vis_samples_real.pdf}
   \caption{The qualitative results on \textbf{unseen} instances for \textit{Folding} task in \textbf{real-world} data. \textbf{Please watch the video in the supplementary files for more elaborate examples}.}
   \label{fig:vis_sample_real}
\end{figure*}
\begin{figure*}[th!]
  \centering
        % \fbox{\rule{0pt}{8.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.94\linewidth]{fig/vis_samples_real_2.pdf}
   \caption{The qualitative results on \textbf{unseen} instances for \textit{Flattening} task in \textbf{real-world} data. \textbf{Please watch the video in the supplementary files for more elaborate examples}.}
   \label{fig:vis_sample_real_2}
\end{figure*}
\section{Broader Impact}
\begin{itemize}
    \item Since our dataset only contains the tasks associated with the daily garments, our work will not facilitate injury to living beings directly.
    \item We paid great attention to protect our volunteers' privacy in the process of collecting data.
    \item We respect human rights in all parts of our work.
    \item Our work only focuses on two tasks about deformable garments, so it's impossible to use our work to develop or extend harmful forms of surveillance.
    \item Our work causes no damage to the environment, because the dataset we built and used is colleted with a VR system.
    \item The garments appear in our VR-Folding dataset are all generated by software, so our dataset is unlikely to deceive people in real life.
\end{itemize}
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\end{document}