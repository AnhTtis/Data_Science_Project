\RequirePackage{fix-cm}

\documentclass[twocolumn]{svjour3}        

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}
\newcommand{\ymark}{\ding{51}}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\begin{document}

\title{Multi-modal Prompting for Low-Shot Temporal Action Localization}

\titlerunning{Multi-modal Prompting for low-shot TAL} 

\author{Chen Ju \and
        Zeqian Li \and 
        Peisen Zhao \and 
        Ya Zhang \and    
        Xiaopeng Zhang \and  \\
        Qi Tian \and  
        Yanfeng Wang \and  
        Weidi Xie }

\authorrunning{Chen Ju et al.} 

\institute{{Chen Ju, Zeqian Li, Ya Zhang, Yanfeng Wang, Weidi Xie}  \at
              CMIC, Shanghai Jiao Tong University, Shanghai \\
              \email{\{ju\_chen, lzq0103, ya\_zhang\}@sjtu.edu.cn   \\ \{wangyanfeng, weidi\}@sjtu.edu.cn}           
           \and
           {Peisen Zhao, Xiaopeng Zhang, Qi Tian} \at
              Huawei Cloud \& AI, Shenzhen, Guangdong \\
             \email{\{pszhao93, zxphistory, tianqi1\}@gmail.com}
}

\date{Received: date / Accepted: date}


\maketitle


\begin{abstract}
\hspace{1pt} In this paper, we consider the problem of temporal action localization under low-shot~(zero-shot \& few-shot) scenario, with the goal of detecting and classifying the action instances from arbitrary categories within some untrimmed videos, even not seen at training time. We adopt a Transformer-based two-stage action localization architecture with class-agnostic action proposal, followed by open-vocabulary classification. 
We make the following contributions. 
\textbf{First}, to compensate image-text foundation models with temporal motions, we improve category-agnostic action proposal by explicitly aligning embeddings of optical flows, RGB and texts, which has largely been ignored in existing low-shot methods. 
\textbf{Second}, to improve open-vocabulary action classification, we construct classifiers with strong discriminative power, {\em i.e.}, avoid lexical ambiguities. To be specific, we propose to prompt the pre-trained CLIP text encoder either with detailed action descriptions~(acquired from large-scale language models), or visually-conditioned instance-specific prompt vectors.
\textbf{Third}, we conduct thorough experiments and ablation studies on THUMOS14 and ActivityNet1.3, demonstrating the superior performance of our proposed model, outperforming existing state-of-the-art approaches by one significant margin. 


\keywords{Vision-Language Foundation Models \and Prompt Tuning \and Low-shot Video Understanding}
\end{abstract}



\input{sec/1-introduction.tex}
\input{sec/2-related-work.tex}
\input{sec/3-method.tex}
\input{sec/4-experiment.tex}
\input{sec/5-conclusion.tex}


\bibliographystyle{spmpsci}     
\bibliography{egbib} 


\end{document}

