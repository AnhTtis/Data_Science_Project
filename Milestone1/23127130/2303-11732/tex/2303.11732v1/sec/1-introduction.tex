%---------------------------------------------------------------
%---------------------------------------------------------------
\begin{figure*}[t]
\begin{center}
\vspace{0.2cm}
\includegraphics[width=\textwidth] {./fig/intro.pdf}
\end{center}
\vspace{-0.2cm}
\caption{\textbf{Left}: Existing methods add prompt tuning and temporal layer to foundation models. \textbf{Middle}: The main challenge is lexical ambiguities in vanilla category names. To disambiguate text-based category names, we decompose actions by prompting large language models for various action attribute descriptions. For cases where it may be difficult to give comprehensive detail descriptions, we further propose vision-conditional prompting to learn from input videos. \textbf{Right}: Our overall framework.}
\vspace{0.2cm}
\label{fig:intro}
\end{figure*}
%------------------------------------------------------------
%------------------------------------------------------------



\section{Introduction}
\label{intro}
Temporal action localization~(TAL) aims to localise and classify action instances in untrimmed long videos and is an indispensable part of video understanding~\cite{shu2015joint,zhu2016learning,ke2010volumetric}. Despite great progress has been made by training deep neural networks on large-scale datasets~\cite{caba2015activitynet,zhao2019hacs}, existing research mainly considers a closed-set scenario, where action categories remain identical at training and inference stage. Such an assumption is clearly an oversimplification for deployment scenarios, hence limiting its practical uses.


In the recent literature, another line of research~\cite{ju2022prompting,nag2022zero} considers a more challenging problem, that requires the vision system to handle both seen and unseen categories, with low-shot (zero or only few) examples at inference time, this problem is often termed as open-vocabulary temporal action localization. To tackle the problem, existing studies~\cite{ju2022prompting,nag2022zero,luo2022clip4clip} take inspiration from large-scale foundational models~\cite{Radford21,Jia21,yao2021filip}, casting the problem of action classification in the form of cross-modal retrieval, {\em i.e.}, for one action videos, searching its closest category embedding in text form~({\em e.g.}, ``an action video of class''). However, such a design potentially suffers from the lexical ambiguities, as multiple actions may share category names, despite its differing visual appearance. For example in Figure~\ref{fig:intro}, ``fencing'' could either refer to ``the game of stabbing with slender steel swords in protective clothing'' or ``enclose or separate something using paling or hedgerow'', directly encoding the category names would therefore be unable to distinguish these two concepts. 



To alleviate the above challenge, in this paper, we consider two ideas to enrich the discriminative power of action classifiers constructed from a pre-trained text encoder, via natural language descriptions, or visually conditioned prompting. To be specific, (1) to disambiguate the text-based category names, we construct an automatic pipeline to source detailed attribute descriptions for action categories by prompting large-scale language models~(LLMs)~\cite{Brown20,zhang2022opt}. Along this line of idea, we propose three types of attribute question templates, to encode the salient objects, event fields, and interactions for the query actions. For instance, while prompting LLMs using the template: ``What tools are needed for [skiing]?'', it outputs ``ski board, ski stick, ski goggles, helmet'', hence giving additional cues to enhance the discrimination of the subsequently generated action classifier; 
(2) in cases that could be difficult to provide comprehensive detail descriptions, we further propose vision-conditional prompting, that enables to extract action details from RGB and Flow embeddings, as instance-specific prompt vectors for the pre-trained text encoder. As an example demonstrated in Figure~\ref{fig:intro}, to construct the action classifier for ``competitive artistic gymnastics'', we can either encode the detailed language descriptions~(handspring, walkover, leap straddle, scissors, somersault, etc.), or simply calculate the visual embeddings of the given video, and prompt the text encoder along with action categories.



Inheriting empirical observations that optical flows can generally bring impressive category-agnostic detection (action proposals), {\em i.e.}, large flow normally indicates the existence of actions, we also explicitly incorporate optical flows into the visual representations~\cite{carreira2017quo}, and then align RGB, Flow, and text modalities into one shared embedding space. When evaluating on two standard benchmarks: THUMOS14 and ActivityNet1.3, our model significantly outperforms existing competitive methods, sometimes by over 10\% across few-shot and zero-shot scenarios. We also conduct thorough ablation studies to reveal the effectiveness of each component, both quantitatively and qualitatively.

