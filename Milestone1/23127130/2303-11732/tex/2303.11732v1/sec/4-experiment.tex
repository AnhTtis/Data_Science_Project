\vspace{-0.3cm}
\section{Experiments}
We experiment on two public datasets across four data splits. In Sec.~\ref{subsec:ablation} and~\ref{subsec:choices}, we validate the effectiveness of each component. In Sec.~\ref{subsec:SOTA}, we compare with state-of-the-art low-shot methods. In Sec.~\ref{subsec:qualitative}, we visualize the localization results.


\vspace{-0.2cm}
\subsection{Datasets \& Metrics}   
\label{subsec:implementation}
\noindent {\bf THUMOS14~\cite{jiang2014thumos}} has $413$ untrimmed videos from $20$ categories, with an average of $15$ instances per video, and $20$ videos per category. 
\noindent {\bf ActivityNet1.3}~\cite{caba2015activitynet} covers 20k videos from $200$ categories, with an average of $1.5$ instances per video, and $100$ videos per category. 



\vspace{0.2cm}
\noindent \textbf{Splits.}
Following literature~\cite{ju2022prompting,nag2022zero}, we adopt two types of splits for zero-shot scenarios. 
{\bf The 75:25 split}: train on 75\% base categories and test on 25\% novel categories.
{\bf The 50:50 split}: train on 50\% base categories and test on 50\% novel categories.
The final results are calculated by averaging $10$ random splits. 

While for few-shot scenarios, as we are not aware of any existing benchmarks, we initiate the $N$-shot evaluation, {\em i.e.}, sample $N$ videos from the training set for each novel category, to form the few-shot support set, and then measure on the standard testing set. 


\vspace{0.2cm}
\noindent \textbf{Metrics.} To evaluate localization performance, we report mean Average Precision (mAP) under different intersections over union (IoU) thresholds, following standard protocols. To evaluate classification performance, we report the TOP1 accuracy. Note that one proposal is regarded as positive only if both the category prediction is correct and the IoU exceeds set thresholds.



\vspace{-0.3cm}
\subsection{Implementation Details}   
Our framework is implemented with PyTorch, and all experiments are conducted on one 24G GeForce RTX 3090 GPU. On all datasets, the models are optimized with Adam, using a learning rate of $10^{-4}$, and a batch size of $32$ videos. We warm up the model in the first $5$ epochs for better convergence, and continue to train $45$ epochs for full optimization. To deal with the large variety in video durations, we pad all videos with zeros to $T$ frames. $T$ is set to $192$ on ActivityNet1.3, and $2304$ on THUMOS14. For temporal resolution, we take $16$ consecutive frames as one basic input unit, and the stride of sliding windows is set to $4$ frames, following the literature~\cite{zhang2022actionformer,liu2019completeness,ju2022prompting}. For spatial resolution, we use center crop on each video frame to get $224\times224$ image. For the text stream, we employ the CLIP text encoder $D_{\mathrm{text}}=512$. For the Flow encoder, we utilize the I3D network $D_{\mathrm{flow}}=1024$. For the RGB stream, we explore two solutions: the CLIP image encoder $D_{\mathrm{rgb}}=512$ and the I3D network $D_{\mathrm{rgb}}=1024$. Architecture-wise, both the CLIP image and text encoders are ViT-B/16. We use GPT-3~\cite{Brown20} for $\Phi_{\text{LLM}}(\cdot)$; adopt fully convolutional networks for detector $\Phi_{\mathrm{det}}$ and regressor $\Phi_{\mathrm{reg}}(\cdot)$; employ MLPs for vision-conditional prompt mudule $\Phi_{\mathrm{prmp}}(\cdot)$.


All hyperparameters are set by the grid search: balancing ratios $\lambda_1=\lambda_2=1$, the temperature $\tau=0.07$, detection threshold $\theta_{cls}=0.85$, classification threshold $\theta_{loc}= 0.05$, and the soft-NMS threshold is set to $0.5$. The dimension of multi-modal shared space is $D_{\mathrm{align}}=1024$, and the pyramid layer $L=6$. 


\input{tab/Caption.tex}


\input{tab/Describe.tex}


\input{tab/Prompt.tex}


\input{tab/Align.tex}


\vspace{-0.2cm}
\subsection{Ablation Study}    
\label{subsec:ablation}
We here ablate key components to evaluate their effectiveness. Unless otherwise stated, experiments are conducted under the 75:25 zero-shot split on THUMOS14, using the I3D encoders for both RGB and Flow modalities, CLIP encoder for text modality. 



\vspace{0.2cm}
\noindent \textbf{Text-based classifiers from detailed language descriptions.} 
To alleviate lexical confusion for vanilla category names $\mathcal{C}_{\mathrm{name}}$, we decompose actions into attribute descriptions $\mathbf{M}_{\mathrm{desc}}$, with the help of Large-scale Language Models (GPT-3). Table~\ref{tab:caption} compares the performance resulting from these two options. 


Comparing to only using category names for action classification, incorporating detailed descriptions could enrich discriminative information for classifiers. As a result, $\mathbf{M}_{\mathrm{desc}}$ could bring around 10.5\% average mAP gains and 6.0\% accuracy gains over $\mathcal{C}_{\mathrm{name}}$, proving the effectiveness of category completion. On the one hand, attribute descriptions specify one category name from various aspects, hence giving additional action details. On the other hand, the descriptions from LLMs avoid manually spending time to search external knowledge sources, thus they are also efficient for use. 



\vspace{0.2cm}
\noindent \textbf{Optimal language descriptions.} 
\hspace{1pt} For descriptions $\mathbf{M}_{\mathrm{desc}}$ from LLMs, there are various prompt templates available. For full attribute descriptions of various aspects, we use three types of templates, {\em i.e.}, ``what tools?'', ``where takes place?'', and ``how to decompose steps?'', to obtain salient objects, event fields, and motion interactions. Table~\ref{tab:description} evaluates their effectiveness.


Single prompt only brings trivial results, and ``what tools?'' performs best among the three attributes. To integrate the diversity, we propose three simple strategies, {\em i.e.}, concatenate descriptions then text encoding, average their text encodings, and weighted sum the text encodings. Overall, prompt fusion brings better performance, {\em e.g.}, 2.8\% average mAP gains and 2.6\% accuracy gains over any single prompt. We finally choose weighted sum for its best results.


\input{tab/Usage.tex}


%---------------------------------------------------------------
%---------------------------------------------------------------
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.38\textwidth] {./fig/length.pdf}
\end{center}
\vspace{-0.4cm}
\caption{\textbf{Effect of prompt length.} The variance in prompt length has only slight effects on localization performance.}
\label{fig:length}
\end{figure}
%-------------------------------------------------------------
%-------------------------------------------------------------



\vspace{0.2cm}
\noindent \textbf{Vision-conditional prompt tuning.} 
\hspace{1pt} For the case on building classifiers with video-conditioned prompts, {\em i.e.}, feed RGB and Flow embeddings into the prompt module $\Phi_{\text{prmp}}(\cdot)$, to generate vision-conditional prompt vectors $\mathbf{M}_{\mathrm{cond}}$. Table~\ref{tab:prompt} compares $\mathbf{M}_{\mathrm{cond}}$ with $\mathbf{M}_{\mathrm{rand}}$, where the latter refers to the popular prompt tuning~\cite{zhu2022prompt,ju2022prompting,Lester21}, with the prompt vectors being randomly initialized and learnt on the base (seen) categories.


Comparing to the case without prompt, $\mathbf{M}_{\mathrm{rand}}$ has shown considerable improvements, for both detection and classification. While our vision-conditional prompt vectors $\mathbf{M}_{\mathrm{cond}}$ further boost the performance. For instance, when using only RGB modality, 2.3\% accuracy gains and 4.7\% average mAP gains over $\mathbf{M}_{\mathrm{rand}}$. Our $\Phi_{\text{prmp}}(\cdot)$ enables the prompt vectors $\mathbf{M}_{\mathrm{cond}}$ to be instance-specific, that enables to characterize rich visual details, effectively complementing the information acquired from only encoding the action category names.


Moreover, the prompt vectors learned from RGB-Flow dual modalities significantly outperform those from uni-modality. This is because, in terms of describing visual action details, the RGB modality focuses more on appearance or context; while the Flow modality mainly consists of motion information, the dual-modal prompt vectors could complement each other. 


\input{tab/Share.tex}


\input{tab/Fix_End.tex}


\vspace{0.2cm}
\noindent \textbf{Effectiveness of optical flow.} 
Table~\ref{tab:ablatalign} validates the efficacy of multi-modal alignment. For evaluation metrics, we also report the Oracle mAP using GT category labels, to decouple proposal and classification. 


\input{tab/ZeroShot.tex}


In general, making alignment with Text gives RGB or Flow the open-vocabulary ability, enabling them to deal with zero-shot classification. Comparing to adopting RGB only, introducing Flow for tri-modal alignment leads to impressive boosts in the performance of both proposal and classification. For example, 8.5\% gains on oracle average mAP and 3.8\% gains on TOP1 accuracy over only I3D RGB pre-training, revealing the essence of Flow. In terms of efficacy, RGB has more advantages on zero-shot classification, mainly because it gives valuable appearance or context to distinguish actions; while Flow is better at category-agnostic localization, consistent with the closed-set scenarios, {\em i.e.}, explicit motion inputs are critical clues for action discovery.



\vspace{0.2cm}
\noindent \textbf{Encoder generalization.} 
\hspace{1pt} Table~\ref{tab:ablatalign} also attempts to align two types of the RGB encoders with the Flow and Text encoders. As is evident, our method shows promising performance boosts with these two RGB encoders, validating the strong encoder generalization, {\em i.e.}, the same technique should be applicable to more pre-trained encoders. As far as RGB encoder is concerned, the I3D and CLIP have their own advantages. As CLIP is pre-trained using 400M image-text pairs, it shows better vision-language alignment, leading to strong zero-shot classification. While I3D is pre-trained with Kinetics-400 for better temporal continuity, resulting in superior category-agnostic detection, {\em i.e.}, Oracle mAP.
 


\vspace{-0.3cm}
\subsection{Detailed Comparison \& Module Choice}    \label{subsec:choices}
\vspace{-0.05cm}
In this section, we make detailed comparisons to further dissect model architectures and optimization designs. Experiments are conducted under 75:25 zero-shot splits on THUMOS14, using the I3D encoders for both RGB and Flow, while the CLIP encoder for Text.



\vspace{0.2cm}
\noindent \textbf{Prompt positions.} 
\hspace{1pt} For the learnable prompt vectors $\mathbf{M}_{\mathrm{cond}}$ from vision-conditional prompt module, they can be fed to the input or output of the text encoder, serving as some visual contexts functionally. We make comparisons of these two usage positions (denoting as input and output respectively) in Table~\ref{tab:position}, and observe similar performance. In general, these two positions are almost equivalent as they both extract fine-grained action information from the visual stream to textual stream. 



\input{tab/FewShot.tex}


\vspace{0.2cm}
\noindent \textbf{Prompt length \& format.} 
\hspace{1pt} For the learned prompt vectors $\mathbf{M}_{\mathrm{cond}}$, we prepend or append them in the format of $[\mathbf{M}_{\mathrm{cond}},\, \Phi_\textsc{tokenise}(\mathcal{C}_{\mathrm{name}}),\, \mathbf{M}_{\mathrm{cond}}]$. In practise, this format is equivalent with $[\mathbf{M}_{\mathrm{cond}},\, \Phi_\textsc{tokenize}(\mathcal{C}_{\mathrm{name}})]$ or $[\Phi_\textsc{tokenize}(\mathcal{C}_{\mathrm{name}}),\, \mathbf{M}_{\mathrm{cond}}]$, as revealed by existing studies~\cite{ju2022prompting,zhou2022conditional}. On the other hand, given $\mathbf{M}_{\mathrm{cond}} \in \mathbb{R}^{K \times D_{\mathrm{text}}}$, to validate the effect of prompt length $K$, we also experiment in Figure~\ref{fig:length}, {\em i.e.}, gradually increase the prompt number from $4$ to $64$. Overall, the variance in prompt length has marginal effects on performance. We therefore pick $32$ prompt vectors for its good trade-off between model performance and parameter efficiency. 



\vspace{0.2cm}
\noindent \textbf{Sharing visual backbone.} 
\hspace{1pt} In the closed-set action localization, there are two public backbone strategies: share one network (early fusion) or utilize separate networks (late fusion) for RGB and Flow. In Table~\ref{tab:share}, we explore these two strategies for low-shot scenarios. Generally speaking, sharing a visual backbone damages the performance to some extent, both for proposal and classification. This is possibly due to the premature fusion of RGB and Flow, hindering the effective alignment between these two visual modalities.



\vspace{0.2cm}
\noindent \textbf{Freeze encoders {\em vs.} end-to-end fine-tuning.} 
\hspace{1pt} To avoid heavy computational burdens, we freeze the pre-trained encoders for RGB-Flow-Text modalities, but only optimize lightweight modules (detector, regressor, etc.) in our proposed method. Here, in Table~\ref{tab:fixend}, we compare with end-to-end fine-tuning of the entire model on THUMOS14. Surprisingly, fine-tuning more parameters actually leads to lower performance. We conjecture this is because the large models can be overfitting to the training data, thus damaging the generalization towards unseen (novel) action categories. 



\vspace{-0.2cm}
\subsection{Comparison with state-of-the-art methods}  \label{subsec:SOTA}
\vspace{-0.1cm}
This section makes full comparisons with state-of-the-art methods on both THUMOS14 and ActivityNet1.3. For the sake of fairness, we employ the CLIP encoders for both RGB and Text modalities. While for the Flow modality, we adopt the I3D network pre-trained on the Kinetics-400 dataset~\cite{carreira2017quo}. For low-shot classifiers, we here use vision-conditional prompt tuning ($\mathbf{M}_{\mathrm{cond}}$).




%---------------------------------------------------------------
%---------------------------------------------------------------
\begin{figure}[t]
\begin{center}
\vspace{0.1cm}
\includegraphics[width=0.4\textwidth] {./fig/shot.pdf}
\end{center}
\vspace{-0.4cm}
\caption{\textbf{Effect of shot number.} More shots for novel categories bring greater gains, but also heavier annotation costs.}
\label{fig:shot}
\end{figure}
%-------------------------------------------------------------
%-------------------------------------------------------------



\vspace{0.2cm}
\noindent \textbf{The zero-shot performance} is reported in Table~\ref{tab:zeroshot}. We respectively list the results of RGB and Flow for clear understanding. In general, on all benchmarks, our framework achieves new state-of-the-art under most IoU regimes, using the single RGB modality, for example, comparing to concurrent work, we significantly surpass competitive methods by over 5\% average mAP on THUMOS14. Overall, existing zero-shot TAL methods usually adopt prompt tuning in the text stream, lacking the understanding of visual details. Instead, our vision-conditional method uses RGB or Flow embeddings to enrich the classifier generation, showing better generalization to the novel action categories.


Moreover, adding optical Flows for explicit motion inputs brings immediate improvements, proving the effectiveness of tri-modal alignment. Delightfully, our zero-shot results with RGB-Flow inputs are even comparable with several early methods~\cite{Chao18,Lin18,shou2017cdc} from closed-set scenarios, demonstrating the superiority. Additionally, comparing to the 75:25 data splits, the 50:50 data splits pose bigger challenges, yet our method still yields excellent results, showing powerful generalization.



%-------------------------------------------------------------
%-------------------------------------------------------------
\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.98\textwidth] {./fig/result.pdf}
\end{center}
\vspace{-0.2cm}
\caption{\textbf{Qualitative zero-shot results on THUMOS14.} For various videos from novel categories, our method all outputs good detection results, although action number and action duration vary frequently. Single RGB sometimes has large deviations, or even omits action instances. By bringing motion details, Flow could further correct or complete the RGB results.}
\vspace{0.2cm}
\label{fig:results}
\end{figure*}
%------------------------------------------------------------
%------------------------------------------------------------




{\bf Note that}, comparing to THUMOS14, ActivityNet has lower requirements for classification and detection, as there are only $1.5$ action instances per video, and most videos contain only one category; while for THUMOS14, there are an average of $15$ action instances per video, with significant variations. This phenomenon has become a consensus~\cite{Chao18,zhang2019adversarial,luo2020weakly} of temporal localization, and could somewhat limit our gains on ActivityNet.



\vspace{0.2cm}
\noindent \textbf{Few-shot performance.}
\hspace{1pt} For few-shot scenarios, several video exemplars are annotated for novel (unseen) categories as the support set. Since we are not aware of any existing benchmarks, we initiate evaluation settings based on data splits of zero-shot scenarios. Specifically, we label $1$ or $2$ videos (shots) each category for THUMOS14, while label $5$ or $10$ videos per category for ActivityNet1.3. The shot number is set according to the dataset scale. We conduct $3$ trials and report the average results to ensure statistical significance.



As shown in Table~\ref{tab:fewshot}, we also retrain E-Prompt~\cite{ju2022prompting} with its released codes to get few-shot results for comparison. In all settings of both datasets, these few-shot video exemplars bring considerable gains over the zero-shot counterparts, by providing explicit distribution of novel categories. In addition, more shots naturally bring higher performance gains, and optical flows still have immediate improvements in performance. 


To further evaluate the efficacy of shot number, Figure~\ref{fig:shot} reports some results under the 50:50 data splits on THUMOS14. As can be seen, more shots for novel categories indeed bring greater gains, and also enhance the model generalization. However, there is a price to pay, {\em i.e.}, annotation overheads continue to rise.


\vspace{0.2cm}
\noindent \textbf{Effectiveness of optical flow.} 
On both datasets, we can observe that adding Flow consistently improves the performance by a large margin, for both zero-shot and few-shot tasks. Especially on THUMOS14, introducing optical Flow boosts the average mAP for more than 10\%. The same phenomenon could be observed under various data splits, thus reflecting the significance and effectiveness of tri-modal alignment.



\vspace{-0.2cm}
\subsection{Qualitative Localization Results}  \label{subsec:qualitative}
\vspace{-0.05cm}
We visualize several detection results of novel categories in Figure~\ref{fig:results}, under the 75:25 zero-shot splits on THUMOS14. Note that one proposal is shown only if the predicted category is correct. As is evident, for various novel categories, both action number and action duration could vary frequently in these videos, posing great challenges to TAL models. Nevertheless, our method obtains good results in most cases, again proving the gratifying effectiveness. Besides, the single RGB modality sometimes outputs large deviations, or even omits action instances. By adding motion details, the Flow modality can further correct or complete results.

