\section{Related Work} \label{sec:related work}
\vspace{-0.2cm}
{\noindent \bf Vision-Language Pre-training.} In the early literature, \cite{Mori99,Frome13,Weston11} explore jointly training image-text embeddings using paired text documents. Recently, some studies have further scaled up the training with large-scale web data to form ``the \textbf{foundation} models'', {\em e.g.}, CLIP~\cite{Radford21}, ALIGN~\cite{Jia21}, Florence~\cite{yuan2021florence}, FILIP~\cite{yao2021filip}, VideoCLIP~\cite{xu2021videoclip}, and LiT~\cite{zhai2022lit}. These foundation models usually contain one visual encoder and one textual encoder, which are trained using simple noise contrastive learning for powerful cross-modal representations. They have shown promising potential in many tasks, such as image classification and detection, action recognition, and retrieval. In this paper, we use CLIP for low-shot temporal action localization, but the same technique should be applicable to other foundation models as well.



\vspace{0.1cm}
{\noindent \bf Prompting} refers to leveraging input instructions to steer foundation models for desired outputs. In the NLP domain, early papers~\cite{Gao21,Jiang20,Timo21,Shin20} focus on handcrafted prompt templates. To avoid labor and increase flexibility, some studies~\cite{Lester21,li21-prefixtuning,li2021prefix} propose learnable prompt tuning at the textual stream, showing strong low-shot generalization. In the CV domain, some recent papers~\cite{zhou2019learn,zhou2022conditional,ju2022prompting} introduce such randomly initialized prompt tuning to handle visual tasks, {\em e.g.}, image understanding~\cite{zhu2022prompt,lu2022prompt,yang2022learning,ma2023diffusionseg} and video understanding~\cite{jia2022visual,nag2022zero,ni2022expanding}. However, these studies ignore lexical ambiguity of category names, and cases that are not easy to describe in text. This paper designs novel conditional prompt tuning and language descriptions from LLMs, to solve these issues. 



\vspace{0.1cm}
{\noindent \bf Closed-set Temporal Action Localization} considers to detect and classify action instances from one pre-defined category list. Specifically, existing methods can be divided into two popular supervisions, {\em i.e.}, strong~\cite{zeng2019graph,lin2021learning,qing2021temporal} and weak~\cite{wang2017untrimmednets,ju2023constraint,ju2020point,yudistira2022weakly}. Strong supervision gives precise boundary labels and category labels for training. There are two detailed pipelines: the top-down framework~\cite{shou2016temporal,shou2017cdc,gao2017turn,chao2018rethinking,lin2017single,xu2017r,tan2021relaxed,zhu2021enriching,wang2022rcl,xu2020g} pre-defines extensive anchors, adopts fixed-length sliding windows to produce initial proposals, then regresses to refine boundaries; the bottom-up framework~\cite{zhao2017temporal,lin2018bsn,lin2019bmn,vo2023aoe,zhao2020bottom,bai2020boundary} learns frame-wise boundary detectors for the boundary frames, then groups extreme frames or estimates action lengths for proposal generation. In addition, several works~\cite{gao2018ctap,liu2019multi,yang2020revisiting} used various fusion strategies to complement these frameworks. On the other hand, weak supervision trains without boundary labels to alleviate annotation costs. The video-level setting learns from category labels~\cite{paul2018w,ju2022distilling}, the CAS-based framework~\cite{liu2019completeness,ju2021adaptive,min2020adversarial,narayan2021d2,lee2019background,lee2021weakly,zhao2021soda} and attention-based framework~\cite{nguyen2018weakly,luo2021action,nguyen2019weakly,shi2020weakly,gao2022fine,he2022asm,huang2021foreground,luo2020weakly,ma2022weakly} have been well studied. To generate better results from CAS or attention, some studies~\cite{shou2018autoloc,liu2019weakly} improved post-processing. To balance cost and performance, some papers introduced single-frame annotations~\cite{ju2021divide,ma2020sf,lee2021learning,yang2021background,mettes2019pointly} or instance-number annotations~\cite{narayan20193c,xu2019segregated}. 

Nevertheless, all the above methods assume that action categories remain identical for training and testing, which is an over-simplification of real application scenarios, limiting practical uses of the vision system.



\vspace{0.1cm} 
{\noindent \bf Low-Shot Temporal Action Localization} considers more realistic scenarios: generalize TAL towards action categories that are unseen (zero-shot) or with several support samples (few-shot). Existing methods~\cite{ju2022prompting,nag2022zero,zhang2022ow,bao2022opental} most rely on foundational models pre-trained on large-scale image-caption pairs for help. Typically, E-Prompt~\cite{ju2022prompting} is the first to construct wide baselines with popular prompt tuning~\cite{Lester21,li21-prefixtuning} and vanilla temporal modeling. STALE~\cite{nag2022zero} explores the one-stage framework to further simplify usage. Although promising, all above methods meet two main challenges: (1) For category semantics, the definition may be vague, inaccurate, or incomplete. (2) For visual motions, temporal modeling may be insufficient. In this paper, for detailed category understanding, we design novel language descriptions from LLMs and vision-conditional prompt tuning; for clearer motion understanding, we introduce optical flows to provide explicit motion inputs. 



