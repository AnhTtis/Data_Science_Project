\vspace{-0.1cm}
\section{Conclusion}
This paper considers low-shot temporal action localization, and handles one main challenge, {\em i.e.}, the lexical ambiguities in vanilla category names. To enrich the discriminative power of models, we decompose categories into adaptive attribute descriptions, by prompting large-scale language models, making text-based classifiers more detailed. Moreover, to tackle cases where it is difficult to give comprehensive descriptions, we design one novel vision-conditional prompt module, inputting RGB \& Flow embeddings to generate prompting with rich visual details, for powerful vision-based classifiers. Besides, we also inject optical flows for explicit motion inputs, bringing impressive category-agnostic detection. Extensive experiments and thorough ablations demonstrate the effectiveness of core components, and our superior performance over state-of-the-art methods. 
