
@article{commonvoice,
  title     = {Common Voice: A Massively-Multilingual Speech Corpus},
  author    = {Rosana Ardila and Megan Branson and Kelly Davis and Michael Henretty and Michael Kohler and Josh Meyer and Reuben Morais and Lindsay Saunders and Francis M. Tyers and Gregor Weber},
  journal   = {LREC},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/63a71de0dafc90910e37a2b07169ff486d9b5fe5}
}



@inproceedings{47393,
title	= {Rapid development of TTS corpora for four South African languages},
author	= {Daniel van Niekerk and Charl van Heerden and Marelie Davel and Neil Kleynhans and Oddur Kjartansson and Martin Jansche and Linne Ha},
year	= {2017},
URL	= {http://dx.doi.org/10.21437/Interspeech.2017-1139},
booktitle	= {Proc. Interspeech 2017},
pages	= {2178--2182}
}


 @misc{digitalumugandaDataDigital, author = {Digital Umuganda}, title={Kinyarwanda dataset}, url={https://digitalumuganda.com/dataset/}, journal={Digital Umuganda}} 


@misc{caitoMAILABSSpeech,
	author = {},
	title = {The MAILABS Speech Dataset},
	howpublished = {\url{https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/}},
	year = {},
	note = {[Accessed 01-Feb-2023]},
}


@misc{babel,
           month = {May},
          author = {M.J.F. Gales and K.M. Knill and A. Ragni and S.P. Rath},
            note = {{\copyright} 2014 ISCA. Reproduced in accordance with the publisher's self-archiving policy.},
       booktitle = {Fourth International Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU-2014)},
           title = {Speech recognition and keyword spotting for low-resource languages : Babel project research at CUED},
       publisher = {International Speech Communication Association (ISCA)},
            year = {2014},
         journal = {Fourth International Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU-2014)},
           pages = {16--23},
        keywords = {keyword spotting; deep neural network; low-resource languages; multi-lingual systems},
             url = {https://eprints.whiterose.ac.uk/152840/},
        abstract = {Recently there has been increased interest in Automatic Speech Recognition (ASR) and Key Word Spotting (KWS) systems for low resource languages. One of the driving forces for this research direction is the IARPA Babel project. This paper describes some of the research funded by this project at Cambridge University, as part of the Lorelei team co-ordinated by IBM. A range of topics are discussed including: deep neural network based acoustic models; data augmentation; and zero acoustic model resource systems. Performance for all approaches is evaluated using the Limited (approximately 10 hours) and/or Full (approximately 80 hours) language packs distributed by IARPA. Both KWS and ASR performance figures are given. Though absolute performance varies from language to language, and keyword list, the approaches described show consistent trends over the languages investigated to date. Using comparable systems over the five Option Period 1 languages indicates a strong correlation between ASR performance and KWS performance.}
}


@misc{voxforgeFreeSpeech,
	author = {},
	title = {{F}ree {S}peech... {R}ecognition ({L}inux, {W}indows and {M}ac) - voxforge.org --- voxforge.org},
	howpublished = {\url{http://www.voxforge.org/home}},
	year = {},
	note = {[Accessed 01-Feb-2023]},
}

@article{xlsr,
  title   = {XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale},
  author  = {Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick von Platen and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2111.09296}
}

@article{xls-r,
  title   = {Unsupervised Cross-lingual Representation Learning for Speech Recognition},
  author  = {Alexis Conneau and Alexei Baevski and Ronan Collobert and Abdelrahman Mohamed and Michael Auli},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2006.13979}
}

@inproceedings{DBLP:conf/coling/AlabiAMK22,
  author    = {Jesujoba O. Alabi and David Ifeoluwa Adelani and Marius Mosbach and Dietrich Klakow},
  editor    = {Nicoletta Calzolari and Chu{-}Ren Huang and Hansaem Kim and James Pustejovsky and Leo Wanner and Key{-}Sun Choi and Pum{-}Mo Ryu and Hsin{-}Hsi Chen and Lucia Donatelli and Heng Ji and Sadao Kurohashi and Patrizia Paggio and Nianwen Xue and Seokhwan Kim and Younggyun Hahm and Zhong He and Tony Kyungil Lee and Enrico Santus and Francis Bond and Seung{-}Hoon Na},
  title     = {Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17, 2022},
  pages     = {4336-4349},
  publisher = {International Committee on Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.coling-1.382},
  timestamp = {Thu, 13 Oct 2022 17:29:38 +0200},
  biburl    = {https://dblp.org/rec/conf/coling/AlabiAMK22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bibletts,
  title     = {BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus},
  author    = {Josh Meyer and David Ifeoluwa Adelani and Edresson Casanova and A. Oktem and Daniel Whitenack Julian Weber and Salomon Kabongo KABENAMUALU and Elizabeth Salesky and Iroro Orife and Colin Leong and Perez Ogayo and Chris C. Emezue and Jonathan Mukiibi and Salomey Osei and Apelete Agbolo and Victor Akinode and Bernard Opoku and S. Olanrewaju and Jesujoba Oluwadara Alabi and Shamsuddeen Hassan Muhammad},
  journal   = {INTERSPEECH},
  year      = {2022},
  doi       = {10.48550/arXiv.2207.03546},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/278fe1c4ccffc1a06e818a2d6154a37687545995}
}

@article{speechstew,
  title   = {SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network},
  author  = {William Chan and Daniel Park and Chris Lee and Yu Zhang and Quoc Le and Mohammad Norouzi},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2104.02133}
}
@inproceedings{Burget:203450,
      title = {Multilingual acoustic modeling for speech recognition  based on subspace Gaussian Mixture Models},
      author = {Burget, Lukas and Schwarz, Petr and Agarwal, Mohit and  Akyazi, Pinar and Feng, Kai and Ghoshal, Arnab and Glembek,  Ondrej and Goel, Nagendra and Karafiat, Martin and Povey,  Daniel and Rastrow, Ariya and Rose, Richard C. and Thomas,  Samuel},
      publisher = {IEEE},
      journal = {2010 IEEE International Conference on Acoustics, Speech  and Signal Processing},
      pages = {4334-4337},
      year = {2010},
      abstract = {Although research has previously been done on multilingual  speech recognition, it has been found to be very difficult  to improve over separately trained systems. The usual  approach has been to use some kind of “universal phone  set” that covers multiple languages. We report  experiments on a different approach to multilingual speech  recognition, in which the phone sets are entirely distinct  but the model has parameters not tied to specific states  that are shared across languages. We use a model called a  “Subspace Gaussian Mixture Model” where states'  distributions are Gaussian Mixture Models with a common  structure, constrained to lie in a subspace of the total  parameter space. The parameters that define this subspace  can be shared across languages. We obtain substantial WER  improvements with this approach, especially with very small  amounts of in-language training data.},
      url = {http://infoscience.epfl.ch/record/203450},
      doi = {10.1109/ICASSP.2010.5495646},
}


@article{6639348,
  journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages   = {8619-8623},
  doi     = {10.1109/ICASSP.2013.6639348},
  title   = {Multilingual acoustic models using distributed deep neural networks},
  year    = {2013},
  author  = {G. Heigold and V. Vanhoucke and A. Senior and P. Nguyen and M. Ranzato and M. Devin and J. Dean}
}

@Inbook{Monaco2013,
author="Monaco, Jane",
editor="Gellman, Marc D.
and Turner, J. Rick",
title="Weighted Sample",
bookTitle="Encyclopedia of Behavioral Medicine",
year="2013",
publisher="Springer New York",
address="New York, NY",
pages="2043--2044",
isbn="978-1-4419-1005-9",
doi="10.1007/978-1-4419-1005-9_1082",
url="https://doi.org/10.1007/978-1-4419-1005-9_1082"
}

@inproceedings{oyewusi2022tcnspeech,
  title     = {{TCNS}peech: A Community-Curated Speech Corpus for Sermons},
  author    = {Wuraola Fisayo Oyewusi and Sharon Ibejih and Soromfe Uzomah and Elizabeth Mawutin Joseph and Jon Cynthia and Folakunmi Ojemuyiwa and Benedicta Johnson-Onuigwe and Omolola Taiwo and Akintunde Akinpelumi and Olabisi Adesina and Ayodele Noutouglo and Adeola Adeleke Adeoba and Andrew Akoh and Chukwuemeka Nwachukwu and Opeyemi Agbabiaje and Itunu Falade and Olukemi Erhunmwunsee and Oluwatobiloba Dada and Ol{\'u}wat{\'o}bi David OSIBELUWO and Ehis Akene and Udim Akpan and Moira Amadi-Emina and Jaiyeola Marquis and Michael Senapon Bojerenu and Gbolahan Olumade and Oluwagbemi Lesi and Timothy Ezeh and Oluwadamilola Oguntoyinbo and Tosan Mogbeyiteren and Felicia Oresanya and Samuel Chika and Sodiq Akinjobi},
  booktitle = {3rd Workshop on African Natural Language Processing},
  year      = {2022},
  url       = {https://openreview.net/forum?id=r_-PYcf4LZc}
}


@data{DVN/YB9FWK_2022,
author = {Nweya, Gerald Okey and Akinola, Solomon Oluwole and Onwuegbuzia, Emeka Felix and Ejinwa, Samuel Obinna and Adiboshi, Anita and Nwokwu, Daniel Success and Ihunna Peter and Osuagwu, Amarachi Akudo},
publisher = {Harvard Dataverse},
title = {Replication Data for Igbo Natural Language Processing Tasks II},
year = {2022},
version = {DRAFT VERSION},
doi = {10.7910/DVN/YB9FWK},
url = {https://doi.org/10.7910/DVN/YB9FWK}
}


@data{DVN/RXBNCZ_2022,
author = {Nweya, Gerald Okey and Akinola Solomon Oluwole and Onwuegbuzia, Emeka Felix and Ejinwa, Samuel Obinna and Adiboshi, Anita and Nwokwu, Daniel Success and Ihunna Peter and Osuagwu, Amarachi Akudo},
publisher = {Harvard Dataverse},
title = {{Replication Data for Igbo Natural Language Processing Tasks I}},
UNF = {UNF:6:0RBYf/qoSeTZCEh9QKOxiQ==},
year = {2022},
version = {V1},
doi = {10.7910/DVN/RXBNCZ},
url = {https://doi.org/10.7910/DVN/RXBNCZ}
}

@inproceedings{babirye2022building,
  title     = {Building Text and Speech Datasets for Low Resourced Languages: A Case of Languages in East Africa},
  author    = {Claire Babirye and Joyce Nakatumba-Nabende and Andrew Katumba and Ronald Ogwang and Jeremy Tusubira Francis and Jonathan Mukiibi and Medadi Ssentanda and Lilian D Wanzare and Davis David},
  booktitle = {3rd Workshop on African Natural Language Processing},
  year      = {2022},
  url       = {https://openreview.net/forum?id=SO-U99z4U-q}
}

@article{kreutzer2021quality,
  title   = {Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets},
  author  = {Julia Kreutzer and Isaac Caswell and Lisa Wang and Ahsan Wahab and Daan van Esch and Nasanbayar Ulzii-Orshikh and Allahsera Tapo and Nishant Subramani and Artem Sokolov and Claytone Sikasote and Monang Setyawan and Supheakmungkol Sarin and Sokhar Samb and Benoît Sagot and Clara Rivera and Annette Rios and Isabel Papadimitriou and Salomey Osei and Pedro Ortiz Suarez and Iroro Orife and Kelechi Ogueji and Andre Niyongabo Rubungo and Toan Q. Nguyen and Mathias Müller and André Müller and Shamsuddeen Hassan Muhammad and Nanda Muhammad and Ayanda Mnyakeni and Jamshidbek Mirzakhalov and Tapiwanashe Matangira and Colin Leong and Nze Lawson and Sneha Kudugunta and Yacine Jernite and Mathias Jenny and Orhan Firat and Bonaventure F. P. Dossou and Sakhile Dlamini and Nisansa de Silva and Sakine Çabuk Ballı and Stella Biderman and Alessia Battisti and Ahmed Baruwa and Ankur Bapna and Pallavi Baljekar and Israel Abebe Azime and Ayodele Awokoya and Duygu Ataman and Orevaoghene Ahia and Oghenefego Ahia and Sweta Agrawal and Mofetoluwa Adeyemi},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2103.12028}
}
@article{dunn2020mapping,
  title     = {Mapping languages: The corpus of global language use},
  author    = {Dunn, Jonathan},
  journal   = {Language Resources and Evaluation},
  volume    = {54},
  pages     = {999-1018},
  year      = {2020},
  publisher = {Springer}
}

@inproceedings{caswell-etal-2020-language,
    title = "Language {ID} in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus",
    author = "Caswell, Isaac  and
      Breiner, Theresa  and
      van Esch, Daan  and
      Bapna, Ankur",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.579",
    doi = "10.18653/v1/2020.coling-main.579",
    pages = "6588--6608",
    abstract = "Large text corpora are increasingly important for a wide variety of Natural Language Processing (NLP) tasks, and automatic language identification (LangID) is a core technology needed to collect such datasets in a multilingual context. LangID is largely treated as solved in the literature, with models reported that achieve over 90{\%} average F1 on as many as 1,366 languages. We train LangID models on up to 1,629 languages with comparable quality on held-out test sets, but find that human-judged LangID accuracy for web-crawl text corpora created using these models is only around 5{\%} for many lower-resource languages, suggesting a need for more robust evaluation. Further analysis revealed a variety of error modes, arising from domain mismatch, class imbalance, language similarity, and insufficiently expressive models. We propose two classes of techniques to mitigate these errors: wordlist-based tunable-precision filters (for which we release curated lists in about 500 languages) and transformer-based semi-supervised LangID models, which increase median dataset precision from 5.5{\%} to 71.2{\%}. These techniques enable us to create an initial data set covering 100K or more relatively clean sentences in each of 500+ languages, paving the way towards a 1,000-language web text corpus.",
}

@article{arivazhagan2019massively,
  title   = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  author  = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and George Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1907.05019}
}

@article{team2022language,
  title   = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  author  = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  year    = {ai},
  journal = {META}
}

@article{fan2020beyond,
  title   = {Beyond English-Centric Multilingual Machine Translation},
  author  = {Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Edouard Grave and Michael Auli and Armand Joulin},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2010.11125}
}

@article{masakhaner,
  title     = {MasakhaNER: Named entity recognition for African languages},
  author    = {Adelani, David Ifeoluwa and Abbott, Jade and Neubig, Graham and D’souza, Daniel and Kreutzer, Julia and Lignos, Constantine and Palen-Michel, Chester and Buzaaba, Happy and Rijhwani, Shruti and Ruder, Sebastian and others},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {9},
  pages     = {1116-1131},
  year      = {2021},
  publisher = {MIT Press}
}

@article{masakhane,
  title     = {Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages},
  author    = {Wilhelmina Nekoto and V. Marivate and T. Matsila and Timi E. Fasubaa and T. Kolawole and T. Fagbohungbe and S. Akinola and Shamsuddeen Hassan Muhammad and Salomon Kabongo KABENAMUALU and Salomey Osei and Sackey Freshia and Andre Niyongabo Rubungo and Ricky Macharm and Perez Ogayo and Orevaoghene Ahia and Musie Meressa and Mofetoluwa Adeyemi and Masabata Mokgesi-Selinga and Lawrence Okegbemi and L. Martinus and Kolawole Tajudeen and Kevin Degila and Kelechi Ogueji and Kathleen Siminyu and Julia Kreutzer and Jason Webster and Jamiil Toure Ali and Jade Z. Abbott and Iroro Orife and I. Ezeani and Idris Abdulkabir Dangana and H. Kamper and Hady ElSahar and Goodness Duru and Ghollah Kioko and Espoir Murhabazi and Elan Van Biljon and Daniel Whitenack and Christopher Onyefuluchi and Chris C. Emezue and Bonaventure F. P. Dossou and Blessing K. Sibanda and B. Bassey and A. Olabiyi and A. Ramkilowan and A. Oktem and Adewale Akinfaderin and A. Bashir},
  journal   = {FINDINGS},
  year      = {2020},
  doi       = {10.18653/v1/2020.findings-emnlp.195},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/02c8d27b92a7159facb2e9446d26a5356d446ffe}
}

@article{schneider2019wav2vec,
  title   = {wav2vec: Unsupervised Pre-training for Speech Recognition},
  author  = {Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1904.05862}
}

@article{wolf2019huggingfaces,
  title   = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author  = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1910.03771}
}

@article{nllb,
  title   = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  author  = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2207.04672}
}

@article{adebara2022afrolid,
  title   = {AfroLID: A Neural Language Identification Tool for African Languages},
  author  = {Ife Adebara and AbdelRahim Elmadany and Muhammad Abdul-Mageed and Alcides Alcoba Inciarte},
  year    = {2022},
  journal = {arXiv preprint arXiv: Arxiv-2210.11744}
}


@inproceedings{joshi-etal-2020-state,
  title     = {The State and Fate of Linguistic Diversity and Inclusion in the {NLP} World},
  author    = {Joshi, Pratik and Santy, Sebastin and Budhiraja, Amar and Bali, Kalika and Choudhury, Monojit},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = {jul},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.560},
  doi       = {10.18653/v1/2020.acl-main.560},
  pages     = {6282-6293},
  abstract  = {Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the {``}language agnostic{''} status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.}
}


@article{russakovsky2015imagenet,
  title     = {Imagenet large scale visual recognition challenge},
  author    = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal   = {International journal of computer vision},
  volume    = {115},
  pages     = {211-252},
  year      = {2015},
  publisher = {Springer}
}


@article{librilight,
  title   = {Libri-Light: A Benchmark for ASR with Limited or No Supervision},
  author  = {Jacob Kahn and Morgane Rivière and Weiyi Zheng and Evgeny Kharitonov and Qiantong Xu and Pierre-Emmanuel Mazaré and Julien Karadayi and Vitaliy Liptchinsky and Ronan Collobert and Christian Fuegen and Tatiana Likhomanenko and Gabriel Synnaeve and Armand Joulin and Abdelrahman Mohamed and Emmanuel Dupoux},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1912.07875}
}

@misc{timit,
  doi = {10.35111/17GK-BN40},
  url = {https://catalog.ldc.upenn.edu/LDC93S1},
  author = {{Garofolo,  John S.} and {Lamel,  Lori F.} and {Fisher,  William M.} and {Pallett,  David S.} and {Dahlgren,  Nancy L.} and {Zue,  Victor} and {Fiscus,  Jonathan G.}},
  title = {TIMIT Acoustic-Phonetic Continuous Speech Corpus},
  publisher = {Linguistic Data Consortium},
  year = {1993}
}

@article{chen2020continuous,
  title   = {Continuous speech separation: dataset and analysis},
  author  = {Zhuo Chen and Takuya Yoshioka and Liang Lu and Tianyan Zhou and Zhong Meng and Yi Luo and Jian Wu and Xiong Xiao and Jinyu Li},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2001.11482}
}

@article{librispeech,
  journal = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages   = {5206-5210},
  doi     = {10.1109/ICASSP.2015.7178964},
  title   = {Librispeech: An ASR corpus based on public domain audio books},
  year    = {2015},
  author  = {Vassil Panayotov and Guoguo Chen and Daniel Povey and Sanjeev Khudanpur}
}

@article{workshop2022bloom,
  title   = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author  = {Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurençon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris Emezue and Christopher Klamm and Colin Leong and Daniel van Strien and David Ifeoluwa Adelani and Dragomir Radev and Eduardo González Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Bar Natan and Francesco De Toni and Gérard Dupont and Germán Kruszewski and Giada Pistilli and Hady Elsahar and Hamza Benyamina and Hieu Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jörg Frohberg and Joseph Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro Von Werra and Leon Weber and Long Phan and Loubna Ben allal and Ludovic Tanguy and Manan Dey and Manuel Romero Muñoz and Maraim Masoud and María Grandury and Mario Šaško and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Mohammad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto Luis López and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma and Shayne Longpre and Somaieh Nikpoor and Stanislav Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Davut Emre Taşar and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-shaibani and Matteo Manica and Nihal Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault Fevry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiangru Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Yallow Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre François Lavallée and Rémi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and Stéphane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aurélie Névéol and Charles Lovering and Dan Garrette and Deepak Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Jordan Clive and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and Shani Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdeněk Kasner and Alice Rueda and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ana Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ajibade and Bharat Saxena and Carlos Muñoz Ferrandis and Danish Contractor and David Lansky and Davis David and Douwe Kiela and Duong A. Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatima Mirza and Frankline Ononiwu and Habib Rezanejad and Hessie Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jesse Passmore and Josh Seltzer and Julio Bonis Sanz and Livia Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nour Fahmy and Olanrewaju Samuel and Ran An and Rasmus Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas Wang and Sourav Roy and Sylvain Viguier and Thanh Le and Tobi Oyebade and Trieu Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Singh and Benjamin Beilharz and Bo Wang and Caio Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Clémentine Fourrier and Daniel León Periñán and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Imane Bello and Ishani Dash and Jihyun Kang and John Giorgi and Jonas Golde and Jose David Posada and Karthik Rangasai Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc Pàmies and Maria A Castillo and Marianna Nezhurina and Mario Sänger and Matthias Samwald and Michael Cullan and Michael Weinberg and Michiel De Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patrick Haller and Ramya Chandrasekhar and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Bharati and Tanmay Laud and Théo Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yash Shailesh Bajaj and Yash Venkatraman and Yifan Xu and Yingxin Xu and Yu Xu and Zhe Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
  year    = {2022},
  journal = {BigScience Workshop, arXiv preprint arXiv: Arxiv-2211.05100}
}


@article{emezue2020lanfrica,
  title   = {Lanfrica: A Participatory Approach to Documenting Machine Translation Research on African Languages},
  author  = {Chris C. Emezue and Bonaventure F. P. Dossou},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2008.07302}
}


@inproceedings{10.1145/3442188.3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@inproceedings{adelani-etal-2022-thousand,
  title     = {A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for {A}frican News Translation},
  author    = {Adelani, David and Alabi, Jesujoba and Fan, Angela and Kreutzer, Julia and Shen, Xiaoyu and Reid, Machel and Ruiter, Dana and Klakow, Dietrich and Nabende, Peter and Chang, Ernie and Gwadabe, Tajuddeen and Sackey, Freshia and Dossou, Bonaventure F. P. and Emezue, Chris and Leong, Colin and Beukman, Michael and Muhammad, Shamsuddeen and Jarso, Guyo and Yousuf, Oreen and Niyongabo Rubungo, Andre and Hacheme, Gilles and Wairagala, Eric Peter and Nasir, Muhammad Umair and Ajibade, Benjamin and Ajayi, Tunde and Gitau, Yvonne and Abbott, Jade and Ahmed, Mohamed and Ochieng, Millicent and Aremu, Anuoluwapo and Ogayo, Perez and Mukiibi, Jonathan and Ouoba Kabore, Fatoumata and Kalipe, Godson and Mbaye, Derguene and Tapo, Allahsera Auguste and Memdjokam Koagne, Victoire and Munkoh-Buabeng, Edwin and Wagner, Valencia and Abdulmumin, Idris and Awokoya, Ayodele and Buzaaba, Happy and Sibanda, Blessing and Bukula, Andiswa and Manthalu, Sam},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {jul},
  year      = {2022},
  address   = {Seattle, United States},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.naacl-main.223},
  doi       = {10.18653/v1/2022.naacl-main.223},
  pages     = {3053-3070},
  abstract  = {Recent advances in the pre-training for language models leverage large-scale datasets to create multilingual models. However, low-resource languages are mostly left out in these datasets. This is primarily because many widely spoken languages that are not well represented on the web and therefore excluded from the large-scale crawls for datasets. Furthermore, downstream users of these models are restricted to the selection of languages originally chosen for pre-training. This work investigates how to optimally leverage existing pre-trained models to create low-resource translation systems for 16 African languages. We focus on two questions: 1) How can pre-trained models be used for languages not included in the initial pretraining? and 2) How can the resulting translation models effectively transfer to new domains? To answer these questions, we create a novel African news corpus covering 16 languages, of which eight languages are not part of any existing evaluation dataset. We demonstrate that the most effective strategy for transferring both additional languages and additional domains is to leverage small quantities of high-quality translation data to fine-tune large pre-trained models.}
}

@dataset{sautidb,
  author       = {Afonja, Tejumade and
                  Mbataku, Clinton and
                  Malomo, Ademola and
                  Okubadejo, Olumide and
                  Francis, Lawrence and
                  Nwadike, Munachiso and
                  Orife, Iroro},
  title        = {SautiDB: Nigerian Accent Dataset Collection},
  month        = feb,
  year         = 2021,
  publisher    = {Zenodo},
  version      = 1,
  doi          = {10.5281/zenodo.4561842},
  url          = {https://doi.org/10.5281/zenodo.4561842}
}

@article{afonja2021learning,
  title   = {Learning Nigerian accent embeddings from speech: preliminary results based on SautiDB-Naija corpus},
  author  = {Tejumade Afonja and Oladimeji Mudele and Iroro Orife and Kenechi Dukor and Lawrence Francis and Duru Goodness and Oluwafemi Azeez and Ademola Malomo and Clinton Mbataku},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2112.06199}
}
@article{dossou2021okwugb,
  title   = {OkwuGbé: End-to-End Speech Recognition for Fon and Igbo},
  author  = {Bonaventure F. P. Dossou and Chris C. Emezue},
  year    = {2021},
  journal = {arXiv preprint arXiv: Arxiv-2103.07762}
}

 @misc{muhire_2020, title={How Rwanda is making Voice Tech More Open}, url={https://foundation.mozilla.org/en/blog/how-rwanda-making-voice-tech-more-open/}, journal={Mozilla Foundation}, author={Muhire, Remy}, year={2020}, month={Sep}} 

@inproceedings{NIPS2012_c399862d,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@article{imagenet,
  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages   = {248-255},
  doi     = {10.1109/CVPR.2009.5206848},
  title   = {ImageNet: A large-scale hierarchical image database},
  year    = {2009},
  author  = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei}
}


@article{andrusenko2020towards,
  title   = {Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription},
  author  = {Andrei Andrusenko and Aleksandr Laptev and Ivan Medennikov},
  year    = {2020},
  journal = {arXiv preprint arXiv: Arxiv-2004.10799}
}
@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{rivire2020unsupervised,
  title     = {Unsupervised pretraining transfers well across languages},
  author    = {M. Rivière and Armand Joulin and Pierre-Emmanuel Mazar'e and Emmanuel Dupoux},
  journal   = {Icassp 2020 - 2020 Ieee International Conference On Acoustics, Speech And Signal Processing (icassp)},
  year      = {2020},
  doi       = {10.1109/ICASSP40776.2020.9054548},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cd1951c7286d11b2bc6bd662554e921456d55db9}
}
@article{SCHULTZ200131,
  title    = {Language-independent and language-adaptive acoustic modeling for speech recognition},
  journal  = {Speech Communication},
  volume   = {35},
  number   = {1},
  pages    = {31-51},
  year     = {2001},
  note     = {MIST},
  issn     = {0167-6393},
  doi      = {https://doi.org/10.1016/S0167-6393(00)00094-7},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167639300000947},
  author   = {Tanja Schultz and Alex Waibel},
  keywords = {Language portability, Multilingual acoustic models, Large vocabulary continuous speech recognition, Polyphone decision tree specialization (PDTS), GlobalPhone}
}



@article{1660022,
  journal = {2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings},
  volume  = {1},
  pages   = {I-I},
  doi     = {10.1109/ICASSP.2006.1660022},
  title   = {Cross-Domain and Cross-Language Portability of Acoustic Features Estimated by Multilayer Perceptrons},
  year    = {2006},
  author  = {A. Stolcke and F. Grezl and Mei-Yuh Hwang and Xin Lei and N. Morgan and D. Vergyri}
}

@article{6639081,
  journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages   = {7304-7308},
  doi     = {10.1109/ICASSP.2013.6639081},
  title   = {Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers},
  year    = {2013},
  author  = {Jui-Ting Huang and Jinyu Li and Dong Yu and Li Deng and Yifan Gong}
}

@article{wav2vec,
  title   = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author  = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {12449-12460},
  year    = {2020}
}




@article{devlin2018bert,
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author  = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1810.04805}
}
@article{oord2018representation,
  title   = {Representation Learning with Contrastive Predictive Coding},
  author  = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1807.03748}
}





@article{hsu2021robust,
  title     = {Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},
  author    = {Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and T. Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},
  journal   = {INTERSPEECH},
  year      = {2021},
  doi       = {10.21437/interspeech.2021-236},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/5f769c5df8de29d0a2cd9c020f78047013a87b34}
}

@article{DBLP:journals/corr/abs-1903-12262,
  author    = {Misha Benjamin and
               Paul Gagnon and
               Negar Rostamzadeh and
               Chris Pal and
               Yoshua Bengio and
               Alex Shee},
  title     = {Towards Standardization of Data Licenses: The Montreal Data License},
  journal   = {CoRR},
  volume    = {abs/1903.12262},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.12262},
  eprinttype = {arXiv},
  eprint    = {1903.12262},
  timestamp = {Tue, 02 Apr 2019 12:29:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-12262.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adelani2022thousand,
  title     = {A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation},
  author    = {David Ifeoluwa Adelani and Jesujoba Oluwadara Alabi and Angela Fan and Julia Kreutzer and Xiaoyu Shen and Machel Reid and Dana Ruiter and D. Klakow and Peter Nabende and Ernie Chang and Tajuddeen R. Gwadabe and Freshia Sackey and Bonaventure F. P. Dossou and Chris C. Emezue and Colin Leong and Michael Beukman and Shamsuddeen Hassan Muhammad and Guyo Dub Jarso and Oreen Yousuf and Andre Niyongabo Rubungo and Gilles Hacheme and Eric Peter Wairagala and Muhammad Umair Nasir and Benjamin Ayoade Ajibade and T. Ajayi and Yvonne Wambui Gitau and Jade Z. Abbott and Mohamed Ahmed and Millicent A. Ochieng and Anuoluwapo Aremu and Perez Ogayo and Jonathan Mukiibi and Fatoumata Kabore and Godson Kalipe and Derguene Mbaye and A. Tapo and Victoire Memdjokam Koagne and Edwin Munkoh-Buabeng and Valencia Wagner and Idris Abdulmumin and A. Awokoya and Happy Buzaaba and Blessing K. Sibanda and Andiswa Bukula and Sam Manthalu},
  journal   = {naacl},
  year      = {2022},
  doi       = {10.48550/arXiv.2205.02022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/fc9102cf16f8ad78a7d7d8abcfa8a776af553148}
}

@article{martinus2019focus,
  title   = {A Focus on Neural Machine Translation for African Languages},
  author  = {Laura Martinus and Jade Z. Abbott},
  year    = {2019},
  journal = {arXiv preprint arXiv: Arxiv-1906.05685}
}


@article{zoph2016transfer,
  title     = {Transfer Learning for Low-Resource Neural Machine Translation},
  author    = {Barret Zoph and Deniz Yuret and Jonathan May and Kevin Knight},
  journal   = {Conference On Empirical Methods In Natural Language Processing},
  year      = {2016},
  doi       = {10.18653/v1/D16-1163},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6}
}


@inproceedings{pmlr-v27-bengio12a,
  title     = {Deep Learning of Representations for Unsupervised and Transfer Learning},
  author    = {Bengio, Yoshua},
  booktitle = {Proceedings of ICML Workshop on Unsupervised and Transfer Learning},
  pages     = {17-36},
  year      = {2012},
  editor    = {Guyon, Isabelle and Dror, Gideon and Lemaire, Vincent and Taylor, Graham and Silver, Daniel},
  volume    = {27},
  series    = {Proceedings of Machine Learning Research},
  address   = {Bellevue, Washington, USA},
  month     = {02 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v27/bengio12a/bengio12a.pdf},
  url       = {https://proceedings.mlr.press/v27/bengio12a.html},
  abstract  = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution $P(x)$ is structurally related to some task of interest, say predicting $P(y|x)$. This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.}
}

@article{7415532,
  journal = {2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  pages   = {1225-1237},
  doi     = {10.1109/APSIPA.2015.7415532},
  title   = {Transfer learning for speech and language processing},
  year    = {2015},
  author  = {Dong Wang and Thomas Fang Zheng}
}


@inproceedings{10.1145/3531146.3534637,
author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534637},
doi = {10.1145/3531146.3534637},
abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2206–2222},
numpages = {17},
keywords = {data rights, technology governance, language data, datasets},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}



@software{zohar_jackson_2018_1342401,
  author       = {Zohar Jackson and
                  César Souza and
                  Jason Flaks and
                  Yuxin Pan and
                  Hereman Nicolas and
                  Adhish Thite},
  title        = {Jakobovski/free-spoken-digit-dataset: v1.0.8},
  month        = aug,
  year         = 2018,
  publisher    = {Zenodo},
  version      = {v1.0.8},
  doi          = {10.5281/zenodo.1342401},
  url          = {https://doi.org/10.5281/zenodo.1342401}
}

@article{mnist,
  journal = {Proceedings of the IEEE},
  volume  = {86},
  pages   = {2278-2324},
  doi     = {10.1109/5.726791},
  title   = {Gradient-based learning applied to document recognition},
  year    = {1998},
  author  = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner}
}


@article{radford2022robust,
  title   = {Robust Speech Recognition via Large-Scale Weak Supervision},
  author  = {Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
  year    = {2022},
  journal = {PREPRINT}
}