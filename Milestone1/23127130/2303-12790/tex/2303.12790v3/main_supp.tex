\clearpage

\setcounter{section}{0}
\maketitlesupplementary
\renewcommand{\thesection}{\Alph{section}}

% Restart figure and table numbering at each section
\numberwithin{figure}{section}
\numberwithin{table}{section}

% Include the section number in figure and table captions
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\renewcommand{\thetable}{\thesection.\arabic{table}}

\makeatletter
\renewcommand{\thealgorithm}{\thesection.\arabic{algorithm}}  
\@addtoreset{algorithm}{section}
\makeatother

\input{algorithms/pseudocode}

\section{Experimental details}
% Figure: network architecture
\begin{figure*}[!ht]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/methods/network.jpg}
\end{center}
    
    \caption{(a) {\bf Network architecture} for the denoising U-Net in conjunction with the count regression branch and the basic modules, (b) {\bf ResNet block}, and (c) {\bf Attention module}, used to construct the network. Each cuboid in a stack represents the functioning modules in the ResNet Block and whether the attention module is applied. {\bf Top stacks} are in the encoder. {\bf Bottom stacks} are in the decoder.}
    \label{figure: network architecture}
    
\end{figure*}

1. Denoising network architecture
\noindent {\bf{Denoising network}} has a U-Net architecture \cite{nichol2021improved}, and each downsampling and upsampling layer scales the features by a factor of two along each spatial dimension. We use average pooling for downsampling with a $2\times2$ kernel, a stride of 2, and nearest neighbor interpolation for upsampling. The 2-dimensional convolution layers are $3\times3$ kernels with a stride of 1, and the 1-dimensional convolution layers have a kernel size and a stride of 1. In the multi-head self-attention module, the channel dimension of each head is kept constant at 64, and the number of heads is varied according to the channel dimension of each depth level. The denoising network and the basic modules are illustrated in \cref{figure: network architecture}.

\noindent {\bf{Regression branch}} is a lightweight network with linear layers and a Rectified Linear Unit (ReLU) \cite{agarap2018deep} activation. We apply global average pooling to maintain compatibility along the spatial dimension for channel-wise concatenation.


\section{Evaluation metrics}
To evaluate crowd counting performance, we use the mean absolute error (MAE):
\[
    MAE = \frac{1}{N}\sum_{n=1}^{N} {\lVert c_n - \bar{c_n} \rVert}_1,
\]
and root mean squared error (MSE):
\[
    MSE = \sqrt{\frac{1}{N}\sum_{n=1}^{N} {\lVert c_n - \bar{c_n} \rVert}_2^2}
\]
as the performance metrics. Here, $N$ is the total number of test samples, $c_n$ is the ground truth count, and $\bar{c_n}$ is the prediction for the $n$\textsuperscript{th} sample. 

\section{Datasets}

We evaluate our method on five public datasets: \jhu \cite{sindagi2019pushing}, \shha \cite{zhang2016single}, \shhb \cite{zhang2016single}, \ucf \cite{idrees2013multi}, and \qnrf \cite{idrees2018composition} for crowd counting.\\
% JHU-CROWD++
{\bf \jhu} \cite{sindagi2019pushing} has 2,722 training images, 500 validation images, and 1,600 test images collected from diverse scenarios. The dataset consists of crowd images with numbers ranging up to 25,791 and images without any crowd.\\
% ShanghaiTech A
{\bf \shha} \cite{zhang2016single} contains 300 training images and 182 test images with annotations. We randomly select 30 samples from the training dataset as the validation dataset.\\
% ShanghaiTech B
{\bf \shhb} \cite{zhang2016single} contains 400 training images and 316 testing images with annotations. We create a validation dataset with randomly selected 40 crowd images from the training dataset.\\
% UCF_CC_50
{\bf \ucf} \cite{idrees2013multi} is a comparatively small crowd dataset for extremely dense crowd counting with just 50 samples. We perform a 5-fold cross-validation following the standard protocol in \cite{idrees2013multi}.\\
% UCF_QNRF
{\bf \qnrf} \cite{idrees2018composition} dataset contains 1,535 images of unconstrained crowd scenes, with approximately one million annotations in total. The dataset is split into a training set of 1,201 images and a testing set of 334 images.\\
% NWPU
\textbf{\nwpu} 
NWPU-Crowd \cite{wang2020nwpu} is a large-scale dataset collected from various scenes, consisting of 5,109 images. The images are randomly split into training, validation, and test sets containing 3109, 500, and 1500 images, respectively. This dataset provides box-level annotations.



\section{Additional qualitative results}
We provide a qualitative comparison between the feature maps of the denoising U-Net with and without the counting branch prediction for different time steps in \cref{figure: feature maps}. From \cref{figure: feature maps}, we can see that the decoder features are richer in detail for the case with the counting branch than without it. With the counting branch, the decoder generates features for the crowd starting from the initial time step. The performance of the counting branch further clarifies this, as the predicted count has not varied with time and deviated from the ground truth count significantly.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/feature_maps.png}
    \caption{Difference in feature maps without (\textit{top row}) and with (\textit{bottom row}) counting decoder.}
    \label{figure: feature maps}
\end{figure*}

\section{Number of inference steps}
Though the diffusion model was trained using 1,000 diffusion steps, we can perform inference with fewer steps using DDIM sampling \cite{song2020denoising}. However, selecting the number of sampling steps with a good compromise between the inference speed and MAE performance is pertinent. We considered different sampling steps and the corresponding inference speed and performance to decide the number of inference steps. We then compared it with the inference speed and performance of state-of-the-art methods to find an optimal number of sampling steps. We display the variation between performance and inference speed in \cref{figure: steps vs metrics} for MAE and MSE with \method. In \cref{figure: steps vs metrics}, the values are provided for the average from four realizations on \qnrf\ benchmark, and the shaded region marks the interval of the inference speed for the most recent crowd analysis methods: PET \cite{liu2023point}, STEERER \cite{han2023steerer}, and CrowdHat \cite{wu2023boosting}. Besides that, recently published ``consistency models" \cite{song2023consistency} could improve the sampling quality of few-step inference and facilitate single-step inference.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/steps_vs_metric.png}
    
    \caption{Performance variation with sampling steps}
    \label{figure: steps vs metrics}
\end{figure}


\section{Training setting}
The training parameters used for the denoising network and the counting decoder are presented in \tab{\ref{table: training setting}}.

\begin{table}[!ht]
\caption{Training parameters for the crowd counting network}
\label{table: training setting}

\begin{center}
\tiny
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{l c}
\hline \\[-2ex]
Configuration & setting \\[0.2ex]
\hline \\[-2ex]
Optimizer & Adamw\\
Optimizer betas & \{0.9, 0.999\}\\
Base learning rate & 1e-4\\
Warmup steps & 5000\\
Training steps & 2e5\\
Image size & 256$\times$256\\
Batch size &8\\
Diffusion steps & 1000\\
Noise schedule & Linear\\


\hline
\end{tabular}
}
\end{center}
\end{table}
% ********************************