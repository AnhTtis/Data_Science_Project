\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
% https://tex.stackexchange.com/questions/398223/tikz-gives-error-command-everyshipouthook-already-defined
\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother
\usepackage{tikz}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}

% Include other packages here, before hyperref.
\usepackage{mathtools}
\usepackage{xfrac}
% \sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
\usepackage{multirow, caption}
\usepackage{tikz}
\usepackage{todonotes}
\usepackage{siunitx}
% \usepackage{soul}
% \usepackage{lipsum}

\usepackage{algorithm}
\usepackage{bm}
\usepackage{algorithmic}
\usepackage{listings}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codekw}{RGB}{207,33,46}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\linespread{0.8}\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.5pt}{7.5pt}\color{codegreen},
  keywordstyle=\fontsize{7.5pt}{7.5pt}\color{codekw},
  escapechar={|}, 
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

%=========================================================================
\newcommand{\sect}[1]{Section {#1}}
\newcommand{\fig}[1]{Fig. {#1}}
\newcommand{\tab}[1]{Table {#1}}
\newcommand{\mapfont}[1]{\fontsize{#1}{#1}\selectfont}
\newcommand{\first}[1]{\bf #1}
\newcommand{\second}[1]{\underline{#1}}

% abbreviations
\newcommand{\sota}{SOTA}
\newcommand{\snr}{SNR}

% dataset abbreviations
\newcommand{\jhu}{JHU-CROWD++}
\newcommand{\shtc}[1]{ShanghaiTech #1}
\newcommand{\ucf}{UCF\_CC\_50}
\newcommand{\ucfq}{UCF-QNRF}


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\begin{document}

%%%%%%%%% TITLE
\title{Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models}

% A Multi-Realization Approach to Crowd-Counting with Denoising Diffusion Models and Contour Detection

\author{Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, and Vishal M. Patel \vspace{2mm}\\
Johns Hopkins University, Baltimore, USA\\
{\tt\small \{dranasi1, ngopala2, wbandar1, vpatel36\}@jhu.edu}\\
% Project page: \url{www.wgcban.com/research/adamae}
}


\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Crowd counting is a key aspect of crowd analysis and has been typically accomplished by estimating a crowd-density map and summing over the density values. However, this approach suffers from background noise accumulation and loss of density due to the use of broad Gaussian kernels to create the ground truth density maps. This issue can be overcome by narrowing the Gaussian kernel. However, existing approaches perform poorly when trained with such ground truth density maps. To overcome this limitation, we propose using conditional diffusion models to predict density maps, as diffusion models are known to model complex distributions well and show high fidelity to training data during crowd-density map generation. Furthermore, as the intermediate time steps of the diffusion process are noisy, we incorporate a regression branch for direct crowd estimation only during training to improve the feature learning. In addition, owing to the stochastic nature of the diffusion model, we introduce producing multiple density maps to improve the counting performance contrary to the existing crowd counting pipelines. Further, we also differ from the density summation and introduce contour detection followed by summation as the counting operation, which is more immune to background noise. We conduct extensive experiments on public datasets to validate the effectiveness of our method. Specifically, our novel crowd-counting pipeline improves the error of crowd-counting by up to 6\% on \jhu\ and up to 7\% on \ucfq. The project will be available at \href{https://github.com/dylran/DiffuseDenoiseCount}{\bf \texttt{ \url{github.com/dylran/DiffuseDenoiseCount}}}.
\end{abstract}

%------------------------------------------------------------------------

% Figure: density comparison with regression methods
% \input{latex/figures/density comparison}
\begin{figure}
    \centering
    \begin{tikzpicture}
    
    \node (img1) % image
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/intro_figure/crowd_image.jpg}}; 
    \node[below=of img1, anchor=center, yshift=0.85cm, ] {(a) Ground truth: 1155 (4)};
    
    \node[right=of img1,xshift=-1.2cm] (img2) % dfc map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/intro_figure/dfc_17.png}};
    \node[below=of img2, anchor=center, yshift=0.85cm, ] {(b) Ours: 1142 (4)};

    \node[below=of img1, yshift=0.75cm] (img3) % chfl map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/intro_figure/chfl_17.png}};
    \node[below=of img3, anchor=center, yshift=0.85cm, ] {(c) Chfl: 1187.6 (2.42)};

    \node[right=of img3,xshift=-1.2cm] (img4) % sua map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/intro_figure/sua_17.png}};
    \node[below=of img4, anchor=center, yshift=0.85cm, ] {(d) SUA: 1199.4 (3.20)};

    % crops
    % 1. frame
    \node[right=of img1,xshift=-3.573cm,yshift=.957cm] (frame1) % image crop
    {\includegraphics[height=0.15cm]{figures/intro_crops/box.pdf}};
    \node[right=of img2,xshift=-3.573cm,yshift=.957cm] (frame2) % image crop
    {\includegraphics[height=0.15cm]{figures/intro_crops/box.pdf}};
    \node[right=of img3,xshift=-3.573cm,yshift=.957cm] (frame3) % image crop
    {\includegraphics[height=0.15cm]{figures/intro_crops/box.pdf}};
    \node[right=of img4,xshift=-3.573cm,yshift=.957cm] (frame4) % image crop
    {\includegraphics[height=0.15cm]{figures/intro_crops/box.pdf}};

    % 2. enlarge crop
    \node[right=of img1,xshift=-2.235cm,yshift=-0.865cm] (frame1) % image crop
    {\includegraphics[height=1.0cm]{figures/intro_crops/image_crop.pdf}};
    \node[right=of img2,xshift=-2.235cm,yshift=-0.865cm] (frame1) % image crop
    {\includegraphics[height=1.0cm]{figures/intro_crops/dfc.pdf}};
    \node[right=of img3,xshift=-2.235cm,yshift=-0.865cm] (frame1) % image crop
    {\includegraphics[height=1.0cm]{figures/intro_crops/chfl.pdf}};
    \node[right=of img4,xshift=-2.235cm,yshift=-0.865cm] (frame1) % image crop
    {\includegraphics[height=1.0cm]{figures/intro_crops/sua.pdf}};
    
\end{tikzpicture}
    \caption{Predicted density results for (a) a dense crowd from (b) our method, (c) Chfl \cite{shu2022crowd}, and (d) SUA \cite{meng2021spatial}. The count of the enlarged crop is given in brackets.}
    \label{figure: density comparison}
\end{figure}

%*********************************************************************************************
\section{Introduction}
\label{sec: introduction}
Crowd counting has been a key element in surveillance, public safety, and crowd control. Various methods have been proposed in the literature to estimate the count including methods that directly predict the count \cite{liang2022transcrowd, zhang2019attentional, wang2019learning} or use a surrogate task such as density estimation \cite{hu2020count, jiang2020attention, wang2020distribution, song2021choose, wan2020kernel, tian2021cctrans}, object detection \cite{liu2019point, sam2020locate}, or point localization \cite{song2021rethinking, liang2022end, xu2022autoscale, gao2023application}. In recent state-of-the-art (\sota) methods, density estimation, and point localization have been frequently used.
% Behavior analysis \cite{shao2014scene} and scene understanding \cite{shao2015deeply} have also been explored as alternative approaches.

While density-based methods sum the estimated pixel density values for counting \cite{hu2020count}, localization-based methods count proposals with confidence scores higher than a threshold \cite{song2021rethinking}.
As a result, density-based methods are more susceptible to introducing background noise into the final count compared to localization-based methods \cite{wan2019adaptive}. 
Furthermore, density estimation methods are affected by variations in crowd density distributions, that arise due to different congestion levels of the crowd \cite{bai2020adaptive}. This could result in a loss of accuracy in the density estimation.
%Although adaptive kernels have been proposed as a remedy for dense crowd regions, the interference between adjacent kernels still leaves the aforementioned density deficit unresolved. 
In contrast, recent localization-based methods with point queries do not have the issue of background noise accumulation \cite{liang2022end}, like in density-based methods \cite{shu2022crowd}, as there is no interference between neighboring point proposals. However, localization-based methods require crowd-density heuristics for proposal setting \cite{song2021rethinking}, which is not required by density-based methods. Thus, if the premise of point supervision is translated into density-based methods, it is possible to circumvent the requirement for crowd-density heuristics as well as the flaws of conventional density-based methods, and a narrow-density kernel can be used to achieve this. However, Xu et al. \cite{xu2022autoscale} demonstrated that using a narrow kernel is ineffective with density regression methods.

Alternatively, it is feasible to use a generative model to predict the density map of a given crowd image that would learn the distribution of the values in the density map. Though GAN-based architectures have been used for density map prediction \cite{yao2020mask, duan2021mask, shen2018crowd}, these methods still rely upon broad kernel sizes and overlook the benefits of point supervision. Since the model learns the distribution of the density pixel values, it is advantageous to maintain the sample space of the density pixel values, and employing a broad kernel will only discourage it. 
%This is discussed in detail in Section \ref{sec: methods}.  Hence, there is a benefit to using a narrow kernel.
Furthermore, the use of both point supervision and crowd density prediction with generative models has not been fully investigated before. 
%and also it is yet to investigate the originally hypothesized merger between point supervision and crowd density prediction with generative models. 
Also, the aforementioned GAN-based approaches restrict to a single crowd-density map realization similar to the regression-based methods and jettison the stochastic nature of generative models to produce multiple density map realizations which could improve the counting performance.

To that end, we propose to use denoising diffusion probabilistic models (referred to as diffusion models), \cite{ho2020denoising, nichol2021improved} to generate crowd density maps for a given image. Though, diffusion models have been applied to segmentation \cite{amit2021segdiff, gu2022diffusioninst}, super-resolution \cite{li2022srdiff}, object detection \cite{chen2022diffusiondet}, etc., to the best of our knowledge, neither crowd-counting nor density map generation has been studied with diffusion models. Furthermore, we exercise our desired narrow Gaussian kernel into practice by constructing the ground truth crowd-density maps for training the denoising diffusion process. With the narrow kernel, we minimize the interference between adjacent densities which helps maintain the bounds and the distribution of density pixel values. This in turn simplifies the distribution learning for the diffusion model and improves density prediction as illustrated in \fig{\ref{figure: density comparison}} where the proposed method has reproduced the narrow kernel even in a dense region while the other two methods have failed.

Additionally, to eschew the loss of density that is probable with the density-based crowd-counting methods, we simply count the number of blobs observed in the predicted density map and this is done by detecting the contours in the map. Consequently, we eliminate the effect of background noise as there is no requirement to sum over the density pixel values. Then, we introduce the crowd map fusion mechanism where multiple dot maps constructed by detecting contours are combined together to improve the counting performance. It is noteworthy that this is only possible with generative models due to their stochastic nature. In addition, inspired by Deja \etal \cite{deja2023learning} on joint learning with diffusion models, we introduce an auxiliary regression branch only during training which estimates the crowd count based on encoder-decoder feature representations from the denoising network to improve feature learning. 

In summary, our contributions are:
\begin{itemize}[noitemsep]
    \item {\bf We formulate crowd-density map generation as a denoising diffusion process.} To the best of our knowledge, this is the first study to perform crowd counting with diffusion models.
    \item {\bf We promote the idea of using a narrow Gaussian kernel} to ease the learning process and to facilitate the high-quality density map generation with more fidelity to the ground truth Gaussian kernel.
    \item {\bf We introduce counting via contour detection} made possible by the use of narrow kernels as opposed to the summation of pixels.
    % \item We successfully replicate the background noise suppression capability of localization-based methods with the proposed counting alternative.
    \item {\bf We propose a mechanism to consolidate multiple crowd density realizations} into a single result to improve counting performance utilizing the stochastic nature of diffusion models.
    % \item We demonstrate the improvement from the addition of the count regression branch on both feature learning and counting.
    \item {\bf We show that the proposed method surpasses the \sota\ performance} on public crowd-counting datasets.
\end{itemize}

\section{Related works}
% In this section, we will present recent work on crowd-counting methods and adopting diffusion models for density-map generation.
\subsection{Crowd counting}

% \paragraph{Localization-based methods}
\noindent {\bf Localization-based methods} perform counting by predicting the locations of people. Primarily, it was performed as an object detection task \cite{lian2019density, liu2019point, sam2020locate} that involves predicting a bounding box about heads. Since most datasets provide head locations, ground truth bounding boxes are created using data heuristics \cite{zhong2022mask}. Hence, inaccurate bounding boxes will frustrate the learning process and increase false detection with non-maximum suppression \cite{song2021rethinking}. To circumvent this, other methods perform localization by points \cite{lian2019density} or blobs \cite{liu2019context}, yet do not remove duplicates in congested regions during post-processing. Hence, recent query-based localization methods \cite{song2021rethinking, liang2022end} for crowd-counting circumvent any post-processing with one-to-one matching to estimate the point locations of individuals inspired by DETR \cite{carion2020end}. However, these methods still require point proposals \cite{song2021rethinking} or queries \cite{liang2022end} which should be initialized using data statistics of the crowd density which does not exactly resolve the reliance on data heuristics.\\
% \vspace{-4mm}
% \paragraph{Density-based methods}
\noindent {\bf Density-based methods} \cite{liu2019counting, liu2020weighing, xiong2019open, li2018csrnet, miao2020shallow, liu2020adaptive, bai2020adaptive} attempt to produce a density map for a given crowd image and then obtain the count by summing over the predicted density maps. Unlike localization-based methods, density regression methods do not require any statistics other than the crowd locations \cite{wan2019adaptive}. But, density-based methods suffer from background noise and loss of density \cite{ma2021learning, oh2020crowd, luo2020hybrid} in congested regions due to the use of broad kernels during training. Nonetheless, it is ineffective to use a narrow Gaussian kernel to generate ground truths as experimentally shown by \cite{xu2022autoscale} with regression networks. Hence, we treat the prediction of the density map as a generative task where the distribution of the density values is learned by conditioning on different crowd images inspired by \cite{yao2020mask, shen2018crowd, duan2021mask}.


%********************************
% Figure: crowd counting pipeline
% \input{latex/figures/crowd counting pipeline}
\begin{figure}[!t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/methods/flow_chart.jpg}
\end{center}
    \caption{Overall crowd-counting pipeline. The crowd density maps are generated from the denoising diffusion process for a crowd image. Next, contour detection is performed on the resulting crowd density realizations to create crowd maps. The crowd maps are then fused into a single crowd map. The counting branch is trained in parallel using the encoder-decoder features of the denoising U-Net and discarded during inference.}
\label{figure: diffusion crowd count pipeline}
\end{figure}
%********************************



\subsection{Diffusion models for crowd-density generation}
\label{ssec: diffusion model}
Diffusion models are a class of likelihood-based models inspired by non-equilibrium thermodynamics \cite{sohl2015deep}. These generative models are defined based on a Markov chain with a forward and a reverse process. In the forward process, noise is gradually added to data; and is denoised in the reverse process. The forward process is formulated as,
\begin{equation}
    q(\mathbf{x_t}|\mathbf{x_{t-1}}) = \mathcal{N}(\mathbf{x_t}|\sqrt{1-\beta_t}\mathbf{x_{t-1}},\beta_t\mathbf{I}),
\end{equation}
where the sample datum $\mathbf{x_0}$ is gradually transformed to a noisy sample $\mathbf{x_t}$ for $t \in \{1,\dots,T\}$ by adding Gaussian noise according to a noise variance schedule $\beta_1,\dots,\beta_T$. Here $\mathbf{I}$ is the identity matrix. Nonetheless, $\mathbf{x_t}$ can be computed using $\mathbf{x_0}$ and a noise vector $\epsilon \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ and with the forward transformation,
\begin{equation}
    \mathbf{x_t} = \sqrt{\bar{\alpha_t}}\mathbf{x_0} + \left(1-\bar{\alpha_t} \right)\epsilon,
\end{equation}
where $\bar{\alpha_t} \coloneqq \prod_{\tau=1}^t \alpha_\tau = \prod_{\tau=1}^t (1-\beta_\tau)$ and $\beta_\tau$.

In this work, we aim to perform crowd-density map generation via the diffusion model. Hence, our data samples will be crowd-density maps $\mathbf{x_0} \in \mathbb{R}^{H\times W}$, where $H \text{ and } W$ are the height and width dimensions. However, in lieu of training a neural network to predict $\mathbf{x_0}$ from $\mathbf{x_t}$ for various time steps, we predict the amount of noise ($\hat{\epsilon}$) in $\mathbf{x_t}$ at each time step conditioned on the crowd image ($\mathbf{y}$), and apply the reverse diffusion process to obtain $\mathbf{x_0}$ ultimately. 

To that end, to train the denoising diffusion network, we use the hybrid loss ($L_{hybrid}$) function proposed by Nichol \etal\ in \cite{nichol2021improved}. To promote learning coarse features at lower \snr\ stages, we adopt the weighting scheme \cite{choi2022perception} defined as,
\begin{align}
    \label{equation: lambda}
    \lambda_t &= \frac{\sfrac{(1-\beta_t)(1-\bar{\alpha_t})}{\beta_t}}{{\left(k + \text{\snr}{\scriptstyle(t)}\right)}^\gamma}, \text{where}\;\; \text{\snr}{\scriptstyle(t)} = \frac{\bar{\alpha_t}}{1-\bar{\alpha_t}}
    %\\
    %\text{\snr}{\scriptstyle(t)} &= \frac{\bar{\alpha_t}}{1-\bar{\alpha_t}},
\end{align}
with $k$ and $\gamma$ as hyperparameters. Hence, the final loss over which the denoising network is optimized is defined as follows,
\begin{equation}
    L_{hybrid} = \mathbf{E}_{\mathbf{x_0},\mathbf{y},\epsilon} \left[ \lambda_t{\lVert\hat{\epsilon}_{(\mathbf{x_0},\mathbf{y},t)} - \epsilon\rVert}_2^2\right] + \lambda_{vlb}L_{vlb},
\end{equation}
where $L_{vlb}$ is the original variational lower bound defined in \cite{nichol2021improved} and $\lambda_{vlb}$ is its weighting factor.

\section{Proposed approach}
\label{sec: methods}

In this section, we first review the motivation for selecting an appropriate kernel size. Following, we present the joint learning of counting as an auxiliary task to improve the density map generation performance. Finally, we introduce a method to combine different realizations for density maps to improve crowd-counting performance. The overall crowd-counting pipeline is illustrated in \fig{\ref{figure: diffusion crowd count pipeline}}.

\subsection{An extremely narrow kernel}
The diffusion process requires a density map to learn the conditional distribution of crowd density.
The crowd density map can be acquired by convolving point information with a pre-defined Gaussian kernel. For that, it is important to select a proper kernel size and a variance as it governs the distribution of the pixel values of the crowd-density maps.

As demonstrated in \fig{\ref{figure: pixel value distribution}}, the divergence between the distribution of the Gaussian kernel (values) and the resulting density map increases as the kernel size and variance increase, especially for a densely crowded scene. While this might not be the case for sparse crowd scenes as there is minimal or no interference between density kernels. However, this implies that the density pixel value distribution is highly image dependent which hinders the learning of the crowd densities. This can be eschewed by narrowing the distribution of the Gaussian kernel as illustrated in \fig{\ref{figure: pixel value distribution}}. 
This also helps the denoising diffusion network to maintain the pixel values within a pre-defined range. The difference between the probability mass of a broad Gaussian kernel and the resulting density map is significant. This can lead to the clipping of many pixel values, resulting in a loss of information in congested scenes. 

The aforementioned issue can be solved by a narrow kernel. A narrow kernel provides an alternative path to crowd counting without summing over the density map values. As shown in \fig{\ref{figure: density comparison}}, the crowd count can be obtained by simply counting the observable kernels. For that, we perform contour detection on the density maps and obtain the location of each kernel.
Then, the crowd count is computed as the total number of locations. This provides the means to avoid background noise in the generated density maps and to obtain the crowd count by detecting these narrow kernels in the crowd-density maps. Unlike the local maxima detection strategy proposed in \cite{liang2022focal} to detect head locations from a crowd-density map, our method is not dependent on any hyperparameter tuning for contour detection.

%********************************
% Figure: density pixel value distribution
\begin{figure}[!t]
    \centering
        \begin{tikzpicture}
    \node (img1)
    {\includegraphics[width=4.0cm]{figures/pixel density distribution/narrow.png}};
    \node[below=of img1, anchor=center, yshift=1.0cm, font=\mapfont{5}] {(a) Narrow kernel (size:3$\times$3, $\sigma$:0.5)};
    \node[right=of img1,xshift=-1.2cm] (img2) 
    {\includegraphics[width=4.0cm]{figures/pixel density distribution/sparse.png}};
    \node[below=of img2, anchor=center, yshift=1.0cm, font=\mapfont{5}] {(b) Broad kernel (size:9$\times$9, $\sigma$:1.0)};
    
\end{tikzpicture}
    \caption{Change in the pixel values of the Gaussian kernel (red stems) and the resulting density map (blue stems) for a crowd image with a $3,547$ crowd count. The kernel size and variance increase from left to right.}
    \label{figure: pixel value distribution}
\end{figure}
%********************************

\subsection{Joint learning of counting}
Direct regression of the crowd count from image features is a harder task \cite{liang2022transcrowd} compared to counting with a surrogate task. To perform the direct computation of the crowd count we consider the intermediate features of the encoder and the decoder of the denoising U-Net. Let's denote the set of intermediate features from the denoising network for a particular timestep $t$ as $\mathcal{Z}_t = \{\mathbf{z}^1_t,\mathbf{z}^2_t,\dots,\mathbf{z}^d_t\}$, where $\mathbf{z}^{*}_t$ is the representation vector at the corresponding feature level of the decoder (see \fig{\ref{figure: network architecture}}). Since the spatial dimensions of the intermediate representations at different depth levels are incompatible, global average pooling is performed on each $\mathbf{z}^*_t$ which are then concatenated to construct a single feature vector $\mathbf{z}_t$. This is then passed through the regression network to estimate the crowd count.

However, for a sampled pair ($\mathbf{x_0}, \mathbf{y}$), only the density map $\mathbf{x_0}$ is diffused with noise according to a noise schedule. Hence, the noise level in the set of intermediate features $\mathcal{Z}_t$ will vary with the timestep, and \snr\ will be lower in the later stages of the diffusion process than in the earlier stages. Hence, we utilize the weighting scheme discussed in \sect{\ref{ssec: diffusion model}} during the training of the count regression network. 
% But, the motive for including a weighting scheme for the regression branch is to improve the crowd image features at different depth levels for different timesteps.
We utilize $L_1$-loss as follows, 
\begin{equation}
    L_1^t = \lambda_t{\lVert\bar{c_t} - c\rVert}_1
\end{equation}
to measure the difference between the prediction ($\bar{c_t}$) and the ground truth ($c$) for a given time step $t$ and a given sampled pair, where $\lambda_t$ is the same weighting factor used in Eqn. \eqref{equation: lambda}. As the training loss for the denoising model is the Monte-Carlo approximation of the sum over all timesteps, the training loss can be written as
\begin{equation}
    L_{count} = \mathbb{E}_{\mathbf{x_0}, \mathbf{y}, t}\left[ \lambda_t{\lVert\bar{c_t} - c\rVert}_1 \right].
\end{equation}

\subsection{Overall training objective}
The overall training includes optimization over the parameters of the denoising network and the regression branch. Hence, the overall training objective is as follows,
\begin{equation}
    L_{overall} = L_{hybrid} + \lambda_{count}L_{count},
\end{equation}
where $\lambda_{count}$ is a weightage on the counting task.


% ********************************
% Figure: Density fusion method
% \input{latex/figures/crowd counting pipeline}
\begin{figure}[!t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{figures/methods/fusion_method.jpg}
\end{center}
    \caption{Crowd map fusion criterion. The rejection radius is computed from the neighbors (black) inside the neighbor radius. New points (colored) that fall inside the rejection radius are removed (red) and the rest (green) are combined into the compound map.}
\label{figure: density fusion method}
\end{figure}
% ********************************



%*********************************
% Figure: network architecture
\begin{figure*}[h]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/methods/network.jpg}
\end{center}
    \caption{(a) {\bf Network architecture} for the denoising U-Net in conjunction with the count regression branch and the basic modules, (b) {\bf ResNet block} and (c) {\bf Attention module}, used to construct the network. Each cuboid in a stack represents the functioning modules in the ResNet Block and whether the attention module is applied. {\bf Top stacks} are in the encoder. {\bf Bottom stacks} are in the decoder}
    \label{figure: network architecture}
\end{figure*}
%*********************************

\subsection{Stochastic crowd map fusion}
\label{ssec: crowd density fusion}
The stochastic nature of the diffusion models could generate different realizations of the crowd-density map for the same crowd image. Therefore, the counting performance with diffusion models can be improved with multiple realizations contrary to the traditional crowd-counting methods as evidenced by other tasks based on diffusion models such as segmentation \cite{gu2022diffusioninst} and detection \cite{chen2022diffusiondet}. However, rather than averaging individual counts from different realizations, they could be combined to compute a more improved count because individual realizations could infer crowd densities that were not present in other realizations.

To combine different realizations for the crowd-density maps, only the new information should be transferred to the compound crowd-density map. For that, we first compute the locations of the density kernels by applying contour detection. Once these locations are found, a dot map is constructed for each density map, referred to as the `crowd map'. Then, we consider the dissimilarity between the crowd maps from different realizations and to measure that we consider the structural similarity index measure (SSIM) \cite{wang2004image}.
% which is defined as,
% \begin{equation}
%     SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}
% \end{equation}
% where $x$ and $y$ are coinciding patches from two different realizations for a given crowd image. Then, $\mu$ and $\sigma$ represent the mean and covariance operations on the image patches, while $c_1$ and $c_2$ are to avoid division by zero. 
For each crowd map we assign a similarity score measured as the cumulative of the SSIM with the remaining crowd map realizations. Then, the maps will be arranged in the ascending order of the SSIM before combining.

When fusing two crowd maps, it is necessary to reject repeating point locations. This is performed based on the locations of the new points compared to the points in the combined list. To start, we take the crowd map first in order and the head locations from that realization as the reference. 
Next, we define a rejection radius for each head location as:
\begin{equation}
    r_n = \beta \frac{\sum_{i=1}^{\Tilde{k}}r_{ni}}{2\Tilde{k}}
\end{equation}
by considering the {\it k}-nearest neighbors within a fixed range. Here $\beta$ is a scaling factor and $\Tilde{k}$ is the total nearest neighbors within the range. Next, we remove the head locations of the next crowd map locations that fall within the rejection radii in the reference map as illustrated in \fig{\ref{figure: density fusion method}}, and the remaining locations are added to the reference map. This procedure is performed until all realizations are exhausted.


\section{Experimental details}

%---------------------------------------------------------------------------------
% \subsection{Implementation details}
% \label{ssec: implementation details}

% 1. Denoising network architecture
%\paragraph{Denoising network} 
\noindent {\bf{Denoising network}} has a U-Net architecture \cite{nichol2021improved} and each downsampling and upsampling layer scales the features by a factor of two along each spatial dimension. We use average pooling for downsampling with a $2\times2$ kernel and a stride of 2, and nearest neighbor interpolation for upsampling. The 2-dimensional convolution layers are $3\times3$ kernels with a stride of 1 and the 1-dimensional convolution layers have a kernel size and a stride of 1. 
% In the multi-head self-attention module, the channel dimension of each head is kept constant at 64 and the number of heads is varied according to the channel dimension of each depth level. 
The denoising network and the basic modules are illustrated in \fig{\ref{figure: network architecture}}.\\
% 2. Regression branch architecture
%\paragraph{Regression branch} 
\noindent {\bf{Regression branch}} is a lightweight network with linear layers and a Rectified Linear Unit (ReLU) \cite{agarap2018deep} activation. We apply global average pooling to maintain compatibility along the spatial dimension for channel-wise concatenation.\\
% 3. Diffusion process parameters
%\noindent {\bf{Diffusion process}} 
\noindent {\bf{Diffusion process}} uses 1,000 timesteps and DDIM sampling \cite{song2020denoising} during inference. We use a linear noise schedule with noise variance ranging from \num{1e-3} to 0.02.\\
% 4. Hyperparameter values
%\paragraph{Hyperparameter values} 
\noindent {\bf{Hyperparameter values}} $\lambda_{count}$ is set as \num{5e-3} to match the range of the value for $L_{hybrid}$. The $\gamma$ and $k$ values are set as 0.5 and 1, respectively to compute the \snr-based weighting factors. We adopt the original scaling factor of \num{1e-3} for $\lambda_{vlb}$ following \cite{nichol2021improved}. For crowd map fusion, we set $\beta$ equal to 0.85 and the maximum nearest neighbors to four. The radius for the neighbor search was restricted to 0.05 of the minimum of the image dimensions.\\
% 5. Training parameters
\noindent {\bf{Training}} of the denoising network is initialized with the ImageNet pre-trained weights for the super-resolution \cite{rombach2022high} task except for the input and output layers. The network is trained for \num{2e5} iterations with a batch size of 8 for $256\times256$ images. We use an AdamW optimizer with a fixed learning rate \num{1e-4} and a linear warm-up schedule over \num{5e3} training steps.\\
% 6. Density map preparation
%\paragraph{Ground truth density maps} 
\noindent {\bf{Ground truth density maps}} are prepared by placing a 2-dimensional Gaussian kernel with a variance of 0.25 and a filter size of $3\times3$.\\
% 7. Augmentations
% \noindent {\bf{Augmentations}} such as color jittering, gray scaling, random scaling, and overlapping crops are applied to the crowd images. However, during random scaling, the ground truth locations are proportionately scaled before constructing the corresponding crowd-density map.\\
%8. Inference
\noindent {\bf{Inference}} for a given crowd image is performed by constructing non-overlapping image patches of size $256\times256$. For images, that are non-divisible by the crop dimension, overlapping crops are used for the remaining portions of the image, and these overlapping regions are removed when constructing the complete crowd-density maps. Further, we limit to four realizations for each crowd image to perform crowd map fusion.\\
%9. Evaluation and datasets
\noindent {\bf{Evaluations}} are performed on five public datasets:  \jhu \cite{sindagi2019pushing}, \shtc{A} \cite{zhang2016single}, \shtc{B} \cite{zhang2016single}, \ucf \cite{idrees2013multi}, and \ucfq \cite{idrees2018composition}. We use MAE and MSE as the performance metrics. 

% More details can be found in the supplementary document.


% **************************************
% Table: main results
% \input{latex/tables/sota results}
\begin{table*}
\begin{center}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{l l c c c c c c c c c c c}
\hline \\[-2ex]
\multirow{2}{*}{~} & \multirow{2}{*}{Method} & \multirow{2}{*}{Venue} &\multicolumn{2}{c}{\jhu} &\multicolumn{2}{c}{\shtc{A}} &\multicolumn{2}{c}{\shtc{B}} &\multicolumn{2}{c}{\ucf} &\multicolumn{2}{c}{\ucfq}\\
& & & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
\hline\hline \\[-2ex]

% & DM Count	& NeurIPS'20	& -	& -	& 59.70	& 95.70	& 7.40	& 11.80	& 211.00	& 291.50	& 85.60	& 148.30	\\[0.2ex]
% & AMSNet	& ECCV'20	& -	& -	& 56.70	& 93.40	& 6.70	& 10.20	& 208.40	& 297.30	& 101.80	& 163.20	\\[0.2ex]
% & AMRNet	& ECCV'20	& -	& -	& 61.59	& 98.36	& 7.02	& 11.00	& 184.00	& 265.80	& 86.60	& 152.20	\\[0.2ex]
& ADSCNET	& CVPR'20	& -	& -	& 55.40	& 97.70	& 6.40	& 11.30	& 198.40	& 267.30	& \second{71.30}	& 132.50	\\[0.2ex]
% & TopoCount	& AAAI'21	& 60.90	& 267.40	& 61.20	& 104.60	& 7.80	& 13.70	& 184.10	& 258.30	& 89.00	& 159.00	\\[0.2ex]
& SUA	& ICCV'21	& 80.70	& 290.80	& 68.50	& 121.90	& 14.10	& 20.60	& -	& -	& 130.30	& 226.30	\\[0.2ex]
& SASNet	& AAAI'21	& -	& -	& \second{53.59}	& \second{88.38}	& 6.35	& 9.90	& \second{161.40}	& \second{234.46}	& 85.20	& 147.30	\\[0.2ex]
& ChfL	& CVPR'22	& 57.00	& 235.70	& 57.50	& 94.30	& 6.90	& 11.00	& -	& -	& 80.30	& 137.60	\\[0.2ex]
& MAN	& CVPR'22	& \second{53.40}	& \second{209.90}	& 56.80	& 90.30	& -	& -	& -	& -	& 77.30	& \second{131.50}	\\[0.2ex]
& GauNet	& CVPR'22	& 58.20	& 245.10	& 54.80	& 89.10	& \second{6.20}	& \second{9.90}	& 186.30	& 256.50	& 81.60	& 153.70	\\[0.2ex]
& CLTR	& ECCV'22	& 59.50	& 240.60	& 56.90	& 95.20	& 6.50	& 10.60	& -	& -	& 85.80	& 141.30	\\[0.5ex]
& Ours	& 	& \first{49.44}	& \first{204.71}	& \first{52.87}	& \first{85.62}	& \first{6.08}	& \first{9.61}	& \first{157.12}	& \first{220.59}	& \first{65.79}	& \first{126.53}	\\[0.2ex]

\hline
\end{tabular}
}
\caption{Comparison with the SOTA methods on \jhu, \shtc{A}, \shtc{B}, \ucf, and \ucfq\ datasets. The best results are shown in {\bf bold}. The second-best results are \underline{underlined}.}
\label{table: crowd counting performance}
\end{center}
\end{table*}
% **************************************


% *********************************
% Figure: main crowd maps
\begin{figure*}[!t]
\centering
\begin{tikzpicture}
    \node (img1)
    {\includegraphics[height=2.7cm]{figures/jhu/0379/gt_99.jpg}};
    \node[right=of img1,xshift=-1.2cm] (img2) 
    {\includegraphics[height=2.7cm]{figures/jhu/4126/gt_768.jpg}};
    \node[right=of img2,xshift=-1.2cm] (img3) 
    {\includegraphics[height=2.7cm]{figures/shha/165/gt_1581.jpg}};
    \node[right=of img3,xshift=-1.2cm] (img4) 
    {\includegraphics[height=2.7cm]{figures/ucf qnrf/0331/gt_2607.jpg}};
    
    \node[below=of img1, yshift=1.2cm] (img5) 
    {\includegraphics[height=2.7cm, width=5.1cm]{figures/jhu/0379/final_101.png}};
    \node[below=of img5, anchor=center, yshift=0.9cm, font=\mapfont{8}] {GT: 99     Prediction: 101};
    \node[right=of img5,xshift=-1.2cm] (img6) 
    {\includegraphics[height=2.7cm, width=4.053cm]{figures/jhu/4126/final_766.png}};
    \node[below=of img6, anchor=center, yshift=0.9cm, font=\mapfont{8}] {GT: 768    Prediction: 766};
    \node[right=of img6,xshift=-1.2cm] (img7) 
    {\includegraphics[height=2.7cm]{figures/shha/165/final_1356.jpg}};
    \node[below=of img7, anchor=center, yshift=0.9cm, font=\mapfont{8}] {GT: 1581   Prediction: 1598};
    \node[right=of img7,xshift=-1.2cm] (img8) 
    {\includegraphics[height=2.7cm]{figures/ucf qnrf/0331/final_2420.jpg}};
    \node[below=of img8, anchor=center, yshift=0.9cm, font=\mapfont{8}] {GT: 2607   Prediction: 2566};    
\end{tikzpicture}
\caption{Qualitative results for the proposed method with the ground truths. The prediction is produced after combining multiple realizations.}
\label{figure: crowd maps}
\end{figure*}
% ***********************************


% *********************************
% Figure: gan ablation
\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
    
    \node (img1) % image
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/gan_ablation/IMG_104.jpg}}; 
    \node[below=of img1, anchor=center, yshift=0.85cm, ] {(a) Ground truth: 1174 };
    
    \node[right=of img1,xshift=-1.2cm] (img2) % dfc map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/gan_ablation/IMG_94.jpg}};
    \node[below=of img2, anchor=center, yshift=0.85cm, ] {(b) Ground truth: 70 };

    \node[below=of img1, yshift=0.75cm] (img3) % chfl map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/gan_ablation/dfc_104.png}};
    \node[below=of img3, anchor=center, yshift=0.85cm, ] {(c) Ours: 1168};

    \node[right=of img3,xshift=-1.2cm] (img4) % sua map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/gan_ablation/dfc_94.png}};
    \node[below=of img4, anchor=center, yshift=0.85cm, ] {(d) Ours: 69};

    \node[below=of img3, yshift=0.75cm] (img5) % chfl map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/gan_ablation/acscp_104.png}};
    \node[below=of img5, anchor=center, yshift=0.85cm, ] {(c) ACSCP: 1765.36};

    \node[right=of img5,xshift=-1.2cm] (img6) % sua map
    {\includegraphics[width=4.0cm,height=2.7cm]{figures/gan_ablation/acscp_94.png}};
    \node[below=of img6, anchor=center, yshift=0.85cm, ] {(d) ACSCP: 52.00};
    
    % crops
    % 1. frame
    \node[right=of img3,xshift=-3.375cm,yshift=1.08cm] (frame1) % image crop
    {\includegraphics[height=0.10cm]{figures/intro_crops/box.pdf}};

    % 2. enlarge crop
    \node[right=of img3,xshift=-2.235cm,yshift=-0.865cm] (frame1) % image crop
    {\includegraphics[height=1.0cm]{figures/gan_ablation/image_104_crop.png}};
    
\end{tikzpicture}
    \caption{Generation quality and crowd performance comparison with a narrow kernel between the diffusion models and a GAN-based \cite{shen2018crowd} method for different crowd scenes.}
    \label{figure: gan ablation}
\end{figure}
% *********************************

\section{Results}
%We performe experiments to evaluate the performance of the proposed crowd-counting method on five public datasets and benchmark against existing methods. 
We extensively evaluate the performance of our method on publicly available crowd-counting datasets and compare it with the recent \sota\ methods. We provide ablation studies for each aspect of the proposed crowd-counting pipeline.
% We divide the existing methods into regressive and generative methods while regressive methods are subdivided into localization-based and density-based methods.

\subsection{Crowd counting performance}

% \paragraph{Quantative results}
\noindent {\bf Quantitative results} for crowd counting are presented in \tab{\ref{table: crowd counting performance}} for the proposed method with other existing methods. The proposed method achieves \sota\ crowd-counting results on public crowd-counting datasets, and the improvement can be explained by two factors. First, the proposed use of a narrow kernel has improved the counting results in dense regions by mitigating the loss of density values in contrast to conventional density-based methods. Second, we eliminate the effect of background noise on the crowd count, which scales with the image dimensions, by replacing density summation with contour detection followed by summation. Since \jhu\ and \ucfq\ datasets contain dense crowd scenes and large image dimensions this explains the significant MAE margins of 6\% and 7\% for \jhu\ and \ucfq, respectively, compared to the \sota. This was possible due to the capability to produce accurate density maps with better resemblance to ground truth maps with diffusion models. 
\\
% \vspace{-5mm}
% \paragraph{Qualitative results}
\noindent {\bf Qualitative results} are presented in \fig{\ref{figure: density comparison}} and \fig{\ref{figure: crowd maps}} for density map generation with diffusion models and crowd map generation with the proposed pipeline. As depicted in \fig{\ref{figure: density comparison}}, the proposed method and the narrow kernel have the ability to accurately perform counting even in a dense region while the other two methods have suffered from loss of density. Further, our proposed pipeline has identified head locations accurately which is not possible with existing density-based methods, and without the need for data heuristics, unlike localization-based methods.\\

\subsection{Ablation study}
% We consider the effect of stochastic crowd map generation and the impact of the addition of the counting branch and the crowd map fusion method. First, we present qualitative results on stochastic crowd map generation as a validation for the crowd map fusion mechanism. Next, we provide comparisons for different realizations with and without the counting regression branch. Next, we present the possible ways of combining the crowd maps and the effect of each method on overall crowd-counting performance. 
% \paragraph{Diffusion models}
\noindent {\bf Diffusion models} are considered to have more fidelity to training data than GAN-based methods. From \fig{\ref{figure: gan ablation}} one can see that diffusion models have produced high-quality density maps with more accurate crowd counts while ASCSP \cite{shen2018crowd}, a GAN-based method, has failed. Furthermore, without the ability to produce narrow kernels in the predicted density maps, GAN-based methods have to use density summation as the counting operation which brings back the issues of noise accumulation and loss of density. This highlights the importance of using diffusion models for crowd-density map generation with contour detection as the counting operation.\\
% \vspace{-5mm}
% \paragraph{Stochastic crowd map}
\noindent {\bf Stochastic crowd map} generation is a key benefit of diffusion-based generative models. In \fig{\ref{figure: stochastic image generation}}, we provide qualitative results of two realizations for each crowd image. From  \fig{\ref{figure: stochastic image generation}} we can see that different realizations have information that is not present in other realizations. Furthermore, it is noteworthy that using a narrow kernel facilitates the ability to produce new knowledge which can be included in the final prediction. Otherwise, novel information captured by different realizations will be diluted by averaging the density maps had a larger kernel been used. Though this is a generative model, the proposed method has reassigned densities perfectly in certain instances and for some cases, there is a slight shift in the location between realizations. This necessitates the need for the proposed crowd map fusion method since simply combing these shifted dots results in double counts and worsens the counting performance otherwise.\\
% \vspace{-5mm}
% \paragraph{The counting branch}
\noindent {\bf The counting branch} is added to improve the counting performance with the crowd-density maps. We present the counting results for individual realizations with and without the regression network in \tab{\ref{table: counting branch ablation}}. In specific, we consider the average error performance from different realizations to identify characteristic effects of the counting branch before combining them into a single crowd map. The addition of the counting branch has improved the average counting results, and the variation in the counting results has been reduced. This is because the counting branch promotes feature learning for intermediate time steps with noisy features as well. Furthermore, we provide a qualitative comparison between the feature maps of the denoising U-Net with and without the counting branch prediction for different time steps in \fig{\ref{figure: feature maps}}. From \fig{\ref{figure: feature maps}} one can see that the decoder features are richer in detail for the case with the counting branch than without it. With the counting branch, the decoder generates features for the crowd starting from the initial time step. This is further clarified by the performance of the counting branch as the predicted count has not varied with time and deviated from the ground truth count significantly.\\
% \vspace{-5mm}


% ********************************
% \input{latex/tables/stochastic generation}
\begin{table}[t!]
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l l c c c c c c}
\hline \\[-2ex]
\multirow{2}{*}{~} & \multirow{2}{*}{Method} &\multicolumn{2}{c}{\jhu}  &\multicolumn{2}{c}{\shtc{B}} &\multicolumn{2}{c}{\ucfq}\\
&~ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
\hline\hline \\[-2ex]
\multirow{2}{*}{\rotatebox[origin=c]{90}{\small w/o}}
& Best	& 52.56	& 215.01	& 6.25	& 9.91	& 72.39	& 131.98	\\[0.0ex]
& Average	& 54.70	& 221.17	& 6.33	& 10.03	& 74.76	& 133.78	\\[0.0ex]
& Variance	& 1.6586	& 4.7822	& 0.0981	& 0.1528	& 2.2034	& 2.2348	\\[1ex]
% & 	& 	& 	& 	& 	& 	& 	& 	& 	& 	& 	\\[0.2ex]
\multirow{2}{*}{\rotatebox[origin=c]{90}{\small w/}}
& Best	& 50.46	& 209.05	& 6.17	& 9.76	& 68.86	& 128.49	\\[0.0ex]
& Average	& 50.80	& 209.66	& 6.20	& 9.81	& 69.73	& 129.53	\\[0.0ex]
& Variance	& 0.2664	& 0.8844	& 0.0222	& 0.0427	& 0.6106	& 0.6995	\\[0.1ex]
\hline
\end{tabular}
}
\caption{Error metrics for individual realizations without (top half) and with (bottom half) the counting decoder.}
\label{table: counting branch ablation}
\end{center}
\end{table}
% ********************************


% *******************************
% Figure: stochastic image generation
\begin{figure}[!t]
    \centering
    \begin{tikzpicture}
    \node (img1)
    {\includegraphics[height=2.8cm, width=3.8cm]{figures/stochastic_generation/stochastic_160.jpg}};
    
    \node[right=of img1,xshift=-1.2cm] (img2) 
    {\includegraphics[height=2.8cm]{figures/stochastic_generation/stochastic_171.jpg}};
    
    \node[below=of img1, yshift=1.22cm, xshift=0.7cm] (img3) 
    {\includegraphics[height=2.8cm]{figures/stochastic_generation/stochastic_0004.jpg}};
    
    \node[right=of img3,xshift=-1.2cm] (img4) 
    {\includegraphics[height=2.8cm]{figures/stochastic_generation/stochastic_58.jpg}};

    \node[below=of img3, yshift=1.22cm, xshift=1.42cm] (img5) 
    {\includegraphics[width=0.965\columnwidth]{figures/stochastic_generation/stochastic_154.jpg}};
    
    
    \end{tikzpicture}
    \caption{Qualitative results for stochastic crowd map generation from two realizations. Green boxes include new dots created at different realizations. Blue boxes include dots that are present in both realizations but with a shift and pink boxes include perfectly reassigned dots. ({\it best viewed in highest zooming level})}
    \label{figure: stochastic image generation}
\end{figure}
% *******************************


% *******************************
% Figure: feature maps
\begin{figure*}[!t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/feature_maps.png}
\end{center}
   \caption{Encoder-Decoder feature maps produced by the denoising U-Net at different time steps. The {\bf top row} feature maps are without the counting regression branch and the {\bf bottom row} feature maps are with the regression branch. The feature maps are from the feature levels which are connected to the regression branch. The ground truth count for the $256\times256$ image crop and the prediction from the counting branch at different time steps are given under each feature level.}
\label{figure: feature maps}
\end{figure*}
% *******************************


% *********************************
% Figure: density fusion ablation
\begin{figure*}[!t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figures/density_fusion_ablation.jpg}
\end{center}
    \caption{Qualitative results for the crowd-density map fusion method. Green dots represent the points combined to the final prediction and red dots represent the points removed from each realization.}
\label{figure: density fusion ablation}
\end{figure*}
% *********************************


% ********************************
% \input{latex/tables/density fusion}
% Table: density fusion method ablation
\begin{table}[t!]
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l c c c c c c}
\hline \\[-2ex]
\multirow{2}{*}{Method} &\multicolumn{2}{c}{\jhu} &\multicolumn{2}{c}{\shtc{B}} &\multicolumn{2}{c}{\ucfq}\\
& MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
\hline\hline \\[-2ex]
% \multirow{3}{*}{\rotatebox[origin=c]{90}{\small w/o}}
% & Random	& 52.98	& 215.64	& 6.24	& 9.89	& 73.76	& 130.83	\\[0.0ex]
% & Ascend	& 50.56	& 205.79	& 6.22	& 9.86	& 71.15	& 126.19	\\[0.0ex]
% & Descend	& 53.51	& 217.80	& 6.24	& 9.89	& 74.11	& 131.44	\\[1ex]
% & 	& 	& 	& 	& 	& 	& 	& 	& 	& 	& 	\\[0.2ex]
% \multirow{3}{*}{\rotatebox[origin=c]{90}{\small w/}}
Random	& 49.97	& 205.40	& 6.12	& 9.68	& 67.78	& 127.52	\\[0.0ex]
Ascend-SSIM	& 49.44	& 204.71	& 6.08	& 9.61	& 65.79	& 126.53	\\[0.0ex]
Descend-SSIM	& 50.32	& 208.79	& 6.15	& 9.73	& 68.44	& 127.98	\\[0.1ex]
\hline
\end{tabular}
}
\caption{Comparison for crowd map fusion methods.}
\label{table: fusion method ablation}
\end{center}
\end{table}
% ********************************


% ********************************
% Table: counting method ablation
% \input{latex/tables/counting method ablation}
\begin{table}[t!]
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l c c c c c c}
\hline \\[-2ex]
\multirow{2}{*}{Method} &\multicolumn{2}{c}{\jhu} &\multicolumn{2}{c}{\shtc{A}} &\multicolumn{2}{c}{\ucf}\\
& MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
\hline\hline \\[-2ex]
Contour detection	& 50.46	& 209.05	& 53.38	& 86.79	& 159.81	& 224.55	\\[0.0ex]
Density estimation	& 225.31	& 541.74	& 175.25	& 281.89	& 176.44	& 249.94	\\[0.0ex]
Noise residual	& 210.19	& 526.67	& 207.71	& 339.70	& 69.06	& 97.43	\\[0.1ex]
\hline
\end{tabular}
}
\caption{Comparison between crowd counting operations and the effect of noise.}
\label{table: counting method ablation}
\end{center}
\end{table}
% ********************************



% \paragraph{Crowd map fusion}
\noindent {\bf Crowd map fusion} leverages the stochastic nature of the crowd-density maps produced by the diffusion process, and we adopt a systematic way to fuse the maps. In Table \ref{table: fusion method ablation}, we present the error metrics for three different methods: Random, Descend-SSIM, and Ascend-SSIM. In the {\it Random} method, we combine the maps in the order in which they are produced. In the {\it Descend-SSIM} method, we combine the maps in the order of decreasing similarity, and in the {\it Ascend-SSIM} method, we combine the maps in the order of increasing similarity as described in \sect{\ref{ssec: crowd density fusion}}. The iterative improvement with stochastic generation and the proposed fusion method is shown in \fig{\ref{figure: density fusion ablation}}. From  \tab{\ref{table: fusion method ablation}} we see that the counting performance has improved with the Descend method where more locally dissimilar realizations are combined initially. This observation is validated by the performance degradation with the Descend-SSIM method compared to the both Ascend-SSIM and Random methods.


% \paragraph{Contour detection}
\noindent {\bf Contour detection} is proposed as an alternative to density summation for the counting operation. The performance comparison between the two methods is tabulated in \tab{\ref{table: counting method ablation}} for the best-performing realization of each dataset. From  \tab{\ref{table: counting method ablation}} we see that the density summation produces significantly inferior counting results than the contour detection despite both methods using the same density map. This is simply because of background noise accumulation with the summation operation and the proposed contour detection method displays better immunity to noise. 
% Furthermore, the effect of noise is dependent on the level of background details available which elucidates the smaller difference for \ucf.

% More results and details can be found in the supplementary document. 

%------------------------------------------------------------------------
\section{Conclusions}
In this work, we proposed a novel crowd-counting framework, where density map generation was treated as a denoising diffusion process. The new framework allows using extremely narrow density kernels with which noise can be suppressed more robustly in crowd-density maps. Furthermore, we implemented the counting operation by counting the density kernels rather than integrating the density map. For that, we performed contour detection on crowd-density maps which offered more immunity to noise than density summation. Also, the  proposed method could iteratively improve the counting performance via multiple realizations, unlike other crowd-counting frameworks due to the stochastic nature of the generative models. Further, our proposed method assigns density kernels at head positions, unlike existing density-based methods, and without the need for data heuristics, like localization-based methods. Hence, we hope our work will provide a baseline and a novel perspective for both localization-based and density-based crowd counting.

% References
\nocite{*} % to test all bib entrys
{
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\clearpage
\renewcommand\thesection{\Alph{section}}
\setcounter{section}{0}
\section{Video demonstrations}
The following links contain video demonstrations of density map generation as a denoising diffusion process. The density maps are overlayed on the crowd image and we only consider a single realization in the videos.
{\it We recommend to download the videos for better quality.}
% \vspace{-2mm}
\begin{itemize}[noitemsep]
    \item \shtc{A}\ video demo\\
        \href{https://drive.google.com/file/d/1i2IcFnXWi84uOfXknpwiPp6BBJqEFZJr/view?usp=share_link}{.avi}
        \href{https://drive.google.com/file/d/1gsPfvNwMcMQf-TWGAIBcCMyPHqYL0h8j/view?usp=share_link}{.mp4}
            
    \item \jhu\ video demo 1\\
        \href{https://drive.google.com/file/d/1khIV1ECAN0ltVQhBT8oVKcrr3N3bbifd/view?usp=share_link}{.avi}
        \href{https://drive.google.com/file/d/19HlepsUNUhwvWN3nwCmKl0b1gjiF4zjs/view?usp=share_link}{.mp4}
        
    \item \jhu\ video demo 2\\
        \href{https://drive.google.com/file/d/1kTR3PeVBQAX4TrxTUQaLX89i0GThxpLk/view?usp=share_link}{.avi}
        \href{https://drive.google.com/file/d/11ibHvgE-qO66kF2OYoW0lhHB2TI7mRjk/view?usp=share_link}{.mp4}
        
    \item \ucfq\ video demo\\
        \href{https://drive.google.com/file/d/1QlfpX0kU7jLWdG1kHtEDN430PfeMvouJ/view?usp=share_link}{.avi}
        \href{https://drive.google.com/file/d/18aRg7mrQxDBn9hiygPFtqDnu1icLToFJ/view?usp=share_link}{.mp4}
\end{itemize}


\section{Evaluation metrics}
To evaluate crowd-counting performance, we use the mean absolute error (MAE):
% \vspace{-2mm}
\begin{equation}
    MAE = \frac{1}{N}\sum_{n=1}^{N} {\lVert c_n - \bar{c_n} \rVert}_1,
% \vspace{-2mm}
\end{equation}
% \vspace{-2mm}
and root mean squared error (MSE):
\begin{equation}
    MSE = \sqrt{\frac{1}{N}\sum_{n=1}^{N} {\lVert c_n - \bar{c_n} \rVert}_2^2}
% \vspace{-2mm}
\end{equation}
as the performance metrics. Here, $N$ is the total number of test samples, $c_n$ is the ground truth count, and $\bar{c_n}$ is the prediction for the $n$\textsuperscript{th} sample. 



\section{Datasets}

% % **************************************
% % Table: main results
% % \input{latex/tables/sota results}
% \begin{table*}[!t]
% \begin{center}
% \resizebox{0.95\textwidth}{!}{
% \begin{tabular}{l l c c c c c c c c c c c}
% \hline \\[-2ex]
% \multirow{2}{*}{~} & \multirow{2}{*}{Method} & \multirow{2}{*}{Venue} &\multicolumn{2}{c}{\jhu} &\multicolumn{2}{c}{\shtc{A}} &\multicolumn{2}{c}{\shtc{B}} &\multicolumn{2}{c}{\ucf} &\multicolumn{2}{c}{\ucfq}\\
% & & & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
% \hline\hline \\[-2ex]

% & DM Count	& NeurIPS'20	& -	& -	& 59.70	& 95.70	& 7.40	& 11.80	& 211.00	& 291.50	& 85.60	& 148.30	\\[0.2ex]
% & AMSNet	& ECCV'20	& -	& -	& 56.70	& 93.40	& 6.70	& 10.20	& 208.40	& 297.30	& 101.80	& 163.20	\\[0.2ex]
% & AMRNet	& ECCV'20	& -	& -	& 61.59	& 98.36	& 7.02	& 11.00	& 184.00	& 265.80	& 86.60	& 152.20	\\[0.2ex]
% & ADSCNET	& CVPR'20	& -	& -	& 55.40	& 97.70	& 6.40	& 11.30	& 198.40	& 267.30	& \second{71.30}	& 132.50	\\[0.2ex]
% & DUBNet	& AAAI'20	& -	& -	& 64.60	& 106.80	& 7.70	& 12.50	& 243.80	& 329.30	& 105.60	& 180.50	\\[0.2ex]
% & HyGnn	& AAAI'20	& -	& -	& 60.20	& 94.50	& 7.50	& 12.70	& 184.40	& 270.10	& 100.80	& 185.30	\\[0.2ex]

% & TopoCount	& AAAI'21	& 60.90	& 267.40	& 61.20	& 104.60	& 7.80	& 13.70	& 184.10	& 258.30	& 89.00	& 159.00	\\[0.2ex]
% & UOT	& AAAI'21	& 60.50	& 252.70	& 58.10	& 95.90	& 6.50	& 10.20	& -	& -	& 83.30	& 142.30	\\[0.2ex]
% & SUA	& ICCV'21	& 80.70	& 290.80	& 68.50	& 121.90	& 14.10	& 20.60	& -	& -	& 130.30	& 226.30	\\[0.2ex]
% & SASNet	& AAAI'21	& -	& -	& \second{53.59}	& \second{88.38}	& 6.35	& 9.90	& \second{161.40}	& \second{234.46}	& 85.20	& 147.30	\\[0.2ex]
% & ChfL	& CVPR'22	& 57.00	& 235.70	& 57.50	& 94.30	& 6.90	& 11.00	& -	& -	& 80.30	& 137.60	\\[0.2ex]
% & MAN	& CVPR'22	& \second{53.40}	& \second{209.90}	& 56.80	& 90.30	& -	& -	& -	& -	& 77.30	& \second{131.50}	\\[0.2ex]
% & GauNet	& CVPR'22	& 58.20	& 245.10	& 54.80	& 89.10	& \second{6.20}	& \second{9.90}	& 186.30	& 256.50	& 81.60	& 153.70	\\[0.2ex]
% & CLTR	& ECCV'22	& 59.50	& 240.60	& 56.90	& 95.20	& 6.50	& 10.60	& -	& -	& 85.80	& 141.30	\\[0.5ex]
% & Ours	& 	& \first{49.44}	& \first{204.71}	& \first{52.87}	& \first{85.62}	& \first{6.08}	& \first{9.61}	& \first{157.12}	& \first{220.59}	& \first{65.79}	& \first{126.53}	\\[0.2ex]

% \hline
% \end{tabular}
% }
% \vspace{-2mm}
% \caption{Extended comparison with the SOTA methods on \jhu, \shtc{A}, \shtc{B}, \ucf, and \ucfq\ datasets. The best results are shown in {\bf bold}. The second-best results are \underline{underlined}.}
% \label{table: crowd counting performance}
% \vspace{-6mm}
% \end{center}
% \end{table*}
% % **************************************

We evaluate our method on five public datasets: \jhu \cite{sindagi2019pushing}, \shtc{A} \cite{zhang2016single}, \shtc{B} \cite{zhang2016single}, \ucf \cite{idrees2013multi}, and \ucfq \cite{idrees2018composition} for crowd counting.

% JHU-CROWD++
{\bf \jhu} has 2,722 training images, 500 validation images, and 1,600 test images, collected from diverse scenarios. The dataset consists of crowd images with numbers ranging up to 25,791 and images without any crowd.

% ShanghaiTech A
{\bf \shtc{A}} contains 300 training images and 182 test images with annotations. We randomly select 30 samples from the training dataset as the validation dataset.

% ShanghaiTech B
{\bf \shtc{B}} contains 400 training images and 316 testing images with annotations. We create a validation dataset with randomly selected 40 crowd images from the training dataset.

% UCF_CC_50
{\bf \ucf} is a comparatively small crowd dataset for extremely dense crowd counting with just 50 samples. We perform a 5-fold cross-validation following the standard protocol in \cite{idrees2013multi}.

% UCF_QNRF
{\bf \ucfq}  dataset contains 1,535 images of unconstrained crowd scenes, with approximately one million annotations in total. The dataset is split into a training set of 1,201 images and a testing set of 334 images, and we create a validation set of 60 images.





% \section{Extended result comparison}
% \tab{\ref{table: crowd counting performance}} compares the performance of the proposed crowd-counting pipeline with additional state-of-the-art (\sota) crowd counting pipilines.






\section{Pseudocodes}

The pseudocode for training is given in Algorithm \ref{alg: train code} and testing in Algorithm \ref{alg: test code}.
% ***********************
% Training code
\begin{algorithm}[!b]
\caption{Training phase}
\label{alg: train code}
\begin{lstlisting}[language=python]
def train(images, density_maps, gt_counts):
    """
        images: [B, H, W, 3]
        density_maps: [B, H, W]
        gt_counts: [B,]
    """
    
    # Density scaling
    density_maps = (2 * scale * density_maps - 1)

    # Corrupt density_maps
    t = randint(0, T)|~~~~~~~~~~~|# time step
    eps = normal(mean=0, std=1)  # noise: [B, H, W]
    crpt_density_maps = 
      diffusion_process(density_maps, eps, t)
    
    # Estimate noise and encoder-decoder features
    eps_pred, feats =
      denoising_network(images, crpt_density_maps, t)

    # Estimate crowd count
    count_est = counting_decoder(feats)
       
    # Compute denoising network loss
    loss = 
      l_hybrid(eps_pred, eps) + 
      count_scale * l1_loss(count_est, gt_count)
    
    return loss
\end{lstlisting}
% \vspace{-2mm}
\end{algorithm}

% ***********************
% Testing code
\begin{algorithm}[!t]
\caption{Testing phase}
\label{alg: test code}
\begin{lstlisting}[language=python]
def testing(images, realizations):
    """
    images: [B, H, W, 3]
    realizations: N
    """
    
    # Encode image features
    feats = image_encoder(images)
    
    # noisy density maps: [B, H, W]
    density_pred = normal(mean=0, std=1)
    
    # uniform sample step size
    times = reversed(
    linespace(diffusion_steps, sampling_steps))

    # Perform DDIM sampling
    for t in times:
        # Predict noise from density_pred
        eps_hat = denoising_network(images, noisy_density, t)
        # Compute posterior of noisy density
        density_pred = q_posterior(noisy_density, eps, t)

    # Detect head locations
    locations = contours(density_pred) # [B, N, *, 2]

    # Perform crowd map fusion: [B, *, 2]
    final_locations = crowd_map_fusion(locations)

    # Compute crowd count
    count_est = count(final_locations) # [B, ]
    
  return count_est
\end{lstlisting}
% \vspace{-2mm}
\end{algorithm}




\section{Performance for individual realizations}

In \tab{\ref{table: counting branch ablation individual}} we provide the counting performance for each of the four realizations used in the final crowd counting prediction with and without the counting decoder.

% ********************************
% \input{latex/tables/stochastic generation}
\begin{table}[!h]
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l l c c c c c c}
\hline \\[-2ex]
\multirow{2}{*}{~} & \multirow{2}{*}{Method} &\multicolumn{2}{c}{\jhu}  &\multicolumn{2}{c}{\shtc{B}} &\multicolumn{2}{c}{\ucfq}\\
&~ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
\hline\hline \\[-2ex]
\multirow{6}{*}{\rotatebox[origin=c]{90}{\small w/o}}
& Trial 1	& 54.23	& 219.76	& 6.31	& 10.00	& \first{72.39}	& \first{131.98}	\\[0.0ex]
& Trial 2	& 56.11	& 225.24	& \first{6.25}	& \first{9.91}	& 75.52	& 134.33	\\[0.0ex]
& Trial 3	& 55.91	& 224.65	& 6.47	& 10.25	& 73.67	& 132.09	\\[0.0ex]
& Trial 4	& \first{52.56}	& \first{215.01}	& 6.28	& 9.95	& 77.44	& 136.71	\\[0.5ex]
& Average	& 54.70	& 221.17	& 6.33	& 10.03	& 74.76	& 133.78	\\[0.0ex]
& Variance	& 1.6586	& 4.7822	& 0.0981	& 0.1528	& 2.2034	& 2.2348	\\[2ex]
% & 	& 	& 	& 	& 	& 	& 	& 	& 	& 	& 	\\[0.2ex]
\multirow{6}{*}{\rotatebox[origin=c]{90}{\small w/}}
& Trial 1	& 51.11	& 210.96	& 6.19	& 9.79	& 69.74	& 129.75	\\[0.0ex]
& Trial 2	& 50.83	& 209.45	& \first{6.17}	& \first{9.76}	& 70.17	& 129.97	\\[0.0ex]
& Trial 3	& 50.78	& 209.17	& 6.22	& 9.86	& \first{68.86}	& \first{128.49}	\\[0.0ex]
& Trial 4	& \first{50.46}	& \first{209.05}	& 6.21	& 9.82	& 70.14	& 129.91	\\[0.5ex]
& Average	& 50.80	& 209.66	& 6.20	& 9.81	& 69.73	& 129.53	\\[0.0ex]
& Variance	& 0.2664	& 0.8844	& 0.0222	& 0.0427	& 0.6106	& 0.6995	\\[0.1ex]
\hline
\end{tabular}
}
\caption{Error metrics for individual realizations without (top half) and with (bottom half) the counting decoder.}
\label{table: counting branch ablation individual}
\end{center}
\end{table}
% ********************************


\section{Training setting}
The training parameters used for the denoising network and the counting decoder are presented in \tab{\ref{table: training setting}}.

% ********************************
% \input{latex/tables/stochastic generation}
\begin{table}[t!]
\begin{center}
\tiny
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{l c}
\hline \\[-2ex]
% \multirow{2}{*}{~} & \multirow{2}{*}{Method} &\multicolumn{2}{c}{\jhu}  &\multicolumn{2}{c}{\shtc{B}} &\multicolumn{2}{c}{\ucfq}\\
% &~ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ & MAE$\downarrow$ & MSE$\downarrow$ \\[0.2ex]
Configuration & setting \\[0.2ex]
\hline \\[-2ex]
Optimizer & Adamw\\
Optimizer betas & \{0.9, 0.999\}\\
Base learning rate & 1e-4\\
Warmup steps & 5000\\
Training steps & 2e5\\
Image size & 256$\times$256\\
Batch size &8\\
Diffusion steps & 1000\\
Noise schedule & Linear\\


\hline
\end{tabular}
}
\caption{Training parameters for the crowd counting network}
\label{table: training setting}
\end{center}
\end{table}
% ********************************


\section{Additional qualitative results}
Here we provide more qualitative results for the proposed crowd-counting pipeline. The {\color{blue} ground truth} crowd map, the {\color{green} final prediction} along with individual trials are provided for labeled data. In each individual trial, the {\color{green} accepted points} and {\color{red} rejected points} are illustrated on the image.

Furthermore, we have provided more visual results for density maps and crowd maps for {\bf unlabeled data}. {\it Density maps are best viewed in the highest zoom level.} 

% ShanghaiTech images
\input{supplementary/shha/5/figure}
\input{supplementary/ucf/1/figure}
\input{supplementary/shha/6/figure}
\input{supplementary/qnrf/17/figure}
\input{supplementary/shha/20/figure}
\input{supplementary/shha/30/figure}

\input{supplementary/shha/31/figure}
\input{supplementary/qnrf/117/figure}
\input{supplementary/shha/46/figure}
\input{supplementary/ucf/10/figure}
\input{supplementary/shha/52/figure}

\input{supplementary/shha/76/figure}
\input{supplementary/qnrf/222/figure}
\input{supplementary/shha/86/figure}

\input{supplementary/shha/91/figure}
\input{supplementary/qnrf/331/figure}
\input{supplementary/shha/108/figure}
\input{supplementary/ucf/2/figure}
\input{supplementary/shha/159/figure}


% \section{Additional qualitative results on unlabeled data}
% Random images
\input{supplementary/random/4/figure}
\input{supplementary/random/1/figure}
\input{supplementary/random/6/figure}
\input{supplementary/random/7/figure}




\end{document}
