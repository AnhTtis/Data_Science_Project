%2multibyte Version: 5.50.0.2960 CodePage: 936
% not crucial - just used below for the URL
%\usepackage[nolists,tablesfirst]{endfloat}

\documentclass[12pt,a4paper]{article}%
\usepackage{amssymb,enumitem,booktabs,subfig,xcolor,todonotes,microtype,setspace}
\usepackage[top=1.4in, bottom=1.4in, left=1.35in, right=1.35in]{geometry}
\usepackage{natbib}
\usepackage{url}
\usepackage[pdftex,colorlinks=true,hypertexnames=false]{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amssymb}%
\setcounter{MaxMatrixCols}{30}
\usepackage{palatino,eulervm}
\usepackage[T1]{fontenc}
\usepackage{tipa}

\usepackage{multirow}

\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\definecolor{darkblue}{rgb}{0,0,.6}
\hypersetup{citecolor=darkblue,linkcolor=darkblue,urlcolor=darkblue}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\temptablewidth{0.95\textwidth}
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\renewcommand{\baselinestretch}{1.2}
\setlength{\textwidth}{18cm}
\setlength{\oddsidemargin}{-10mm}
\setlength{\evensidemargin}{-20mm}
\graphicspath{{plots/}}
\def \UrlFont{\tt}
\setlength{\parskip}{0.3em}
\allowdisplaybreaks[4]
\DeclareMathOperator{\plim}{plim}

\usepackage{amsthm,thmtools}
\usepackage{mathrsfs}
\declaretheorem{theorem}
\declaretheorem{lemma}
\makeatletter
\def\th@newremark{\th@remark\thm@headfont{\bfseries}}
\makeatletter
\theoremstyle{newremark}
\newtheorem{remark}{Remark}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\declaretheoremstyle[
  spaceabove=6pt, spacebelow=6pt,
  headfont=\bfseries,
  notefont=\mdseries, notebraces={(}{)},
bodyfont=\normalfont,
  postheadspace=0.5em,
  %qed=\qedsymbol
]{mystyle}
\declaretheorem[style=mystyle, name=Example \hspace{-.04in}]{callmeal}


\begin{document}

\title{Inference of Grouped Time-Varying Network Vector Autoregression Models}
\author{{\normalsize Degui Li\thanks{%
Department of Mathematics, University of York, UK. },\ \ \ Bin Peng\thanks{Department of Econometrics and Business Statistics, Monash University, Australia. },\ \ \ Songqiao Tang\thanks{%
School of Mathematical Sciences, Zhejiang University, China.},\ \ \ Weibiao Wu\thanks{Department of Statistics, University of Chicago, US.}}\\
{\normalsize\em University of York,\ \ Monash University,\ \ Zhejiang University,\ \ University of Chicago}}
\date{{\normalsize Version: \today}}
\maketitle

\centerline{\bf Abstract}

This paper considers statistical inference of time-varying network vector autoregression models for large-scale time series. A latent group structure is imposed on the heterogeneous and node-specific time-varying momentum and network spillover effects so that the number of unknown time-varying coefficients to be estimated can be reduced considerably. A classic agglomerative clustering algorithm with normalized distance matrix estimates is combined with a generalized information criterion to consistently estimate the latent group number and membership. A post-grouping local linear smoothing method is proposed to estimate the group-specific time-varying momentum and network effects, substantially improving the convergence rates of the preliminary estimates which ignore the latent structure. In addition, a post-grouping specification test is conducted to verify the validity of the parametric model assumption for group-specific time-varying coefficient functions, and the asymptotic theory is derived for the test statistic constructed via a kernel weighted quadratic form under the null and alternative hypotheses. Numerical studies including Monte-Carlo simulation and an empirical application to the global trade flow data are presented to examine the finite-sample performance of the developed model and methodology.

\bigskip

\noindent{\em Keywords}: cluster analysis, information criterion, latent groups, local linear estimator, network VAR, time-varying coefficients. 

\newpage

%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec1} 
\renewcommand{\theequation}{1.\arabic{equation}} \setcounter{equation}{0}

Modeling large-scale time series has been the main frontier of recent advances of time series analysis and is of fundamental importance in various fields of applications such as climatology, economics, finance and social networks. Since \cite{Si80}'s seminal work, the vector autoregression (VAR) model has become a commonly-used statistical tool to tackle multivariate time series, see \cite{Lu06} and \cite{KL17} for a comprehensive review of classic estimation and forecasting techniques. However, the VAR-based estimation and forecasting are challenging when the number of time series sequences $N$ diverges to infinity. In this case, the number of unknown parameters in VAR transition matrices is of order $O(N^2)$, which may be much larger than the time series length $T$. In order to construct sensible estimate and forecast, two dimension-reduction approaches are often employed: VAR with sparse transition matrices and regularised estimation \citep[e.g.,][]{SB11, BM15, KC15, DZZ16, MPS22} and factor-augmented VAR \citep[e.g.,][]{BBE05, BN06, BLL16}. Although some sound asymptotic properties have been developed for the sparse or factor-augmented VAR estimates, they often neglect possible network structures in large-scale time series and cannot directly capture dynamic network effects.   

Consider $X_{t}=\left(x_{1,t},\cdots,x_{N,t}\right)^{^\intercal}$ with $N$ being the number of nodes in the large-scale network, and denote an adjacency matrix by ${\boldsymbol W}=(w_{ij})_{N\times N}$, where $w_{ii}=0$, $w_{ij}=1$ for $i\neq j$ if there exists a directed edge from $i$ to $j$ and $w_{ij}=0$ otherwise. The matrix ${\boldsymbol W}$ is assumed to be observable and can be either directed (${\boldsymbol W}^{^\intercal}\neq {\boldsymbol W}$) or undirected (${\boldsymbol W}^{^\intercal}={\boldsymbol W}$). The network VAR model is defined by 
\begin{equation}\label{nVAR}
x_{i,t}=\beta_1 \sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}+\beta_2x_{i,t-1}+\varepsilon_{i,t},\ \ i=1,\cdots,N,
\end{equation}
where $\widetilde{w}_{ij}=w_{ij}/n_i$ with $n_i=\sum_{j\neq i} w_{ij}$,  $\beta_1$ and $\beta_2$ are unknown parameters, and $\varepsilon_t=(\varepsilon_{1,t},\cdots,\varepsilon_{N,t})^{^\intercal}$ is a sequence of independent and identically distributed (i.i.d.) error vectors. The above network VAR model formulation contains two regression components: $\beta_1 \sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}$ and $\beta_2x_{i,t-1}$, corresponding to the network (cross-lag) and momentum (own-lag) effects, respectively. \cite{ZPLLW17} discuss stationarity conditions for an extended network VAR model with extra nodal effects, propose the least squares estimation method and derive the relevant asymptotic theory. Although the linear network VAR model is easy to interpret and implement, it may be invalid in empirical applications. In particular, there often exist two practical issues: (i) the stable network VAR model cannot capture smooth structural changes in the underlying data generating process over a long time span; and (ii) it is often too restrictive to impose the homogeneity assumption on the autoregression coefficients over $N$ nodes. Consequently, the homogenous linear network VAR (\ref{nVAR}) may suffer from the model misspecification problem, resulting in biased estimates, inaccurate forecast and misleading inference. 

There have been some attempts in recent years to address one of the aforementioned two issues. To incorporate structural changes in the autoregressive structures, \cite{Su16}, \cite{SM18}, \cite{Wu19} and \cite{CLLL23} extend the linear network VAR models, allowing the coefficients to vary smoothly over time or with a stationary index variable. To relax the homogeneity restriction in linear VAR models, \cite{ZP20} introduce a grouped linear network VAR model via a mixture Gaussian distribution and use an EM estimation algorithm; \cite{GB21} propose a stochastic block VAR model and detect a latent group structure on the network spillover effects; \cite{CFZ22} study a community network VAR model and allow network effects to vary over different communities; and more recently \cite{ZXF22} introduce a least squares algorithm to simultaneously estimate the parameters and identify the group structure for heterogeneous network VAR models. As far as we know, however, there is virtually no work which tackles the two issues jointly. The present paper aims to fill in this gap by introducing a general time-varying network VAR model framework satisfying a latent group structure, i.e., the time-varying network autoregression relationships are invariant within a group of nodes, but change over different groups.      

With the latent group structure, we substantially reduce the number of unknown coefficient functions in the network autoregression for time-varying momentum and network spillover effects. Since neither the group number nor membership is known a priori, we combine a classic agglomerative clustering algorithm (with the normalized distance matrix estimator) and a generalized information criterion to consistently estimate the group structure. The developed estimation methodology complements recent developments on latent group estimation in the context of large panel data \citep[e.g.,][]{BM15, KLZ16, SSP16, AB17, VL17, VL20, Ch19, SWJ19}. However, since the underlying high-dimensional time series process is locally stationary, it is technically more challenging to derive the asymptotic property of the developed methodology. In addition, the normalized distance estimates used in our clustering algorithm have the approximate unit variance, making their sizes comparable over nodes and resulting in stable membership estimation.

To improve the convergence rates of preliminary time-varying coefficient estimation which only uses the sample information from one node and its direct neighbors (see Section \ref{sec3.1}), we propose a post-grouping local linear smoothing method to estimate the group-specific time-varying momentum, network and nodal effects by making use of the consistently estimated group structure. The asymptotic normal distribution theory is derived with the convergence rates comparable to those for homogenous time-varying coefficient estimation. We also conduct the post-grouping statistical inference by testing the hypothesis that some group-specific time-varying coefficient functions belong to a given parametric function family. For example, it is often interesting to test whether the time-varying coefficients can be approximated by some constant values or linear functions. The test statistic is constructed via the kernel-weighted quadratic form and the asymptotic theory is established for the developed test statistic under both the null and alternative hypotheses.

The finite-sample Monte-Carlo simulation study shows that the proposed clustering algorithm and information criterion can accurately estimate the latent group membership and number as long as the time series length $T$ is moderately large; and the post-grouping local linear estimates perform significantly better than the naive heterogeneous local linear estimates. The test statistic with the kernel-weighted quadratic form also has stable finite-sample performance in both the test size and power. The developed model and methodology are further applied to analyze the export flow data collected for 90 countries over the time period between January 1995 and December 2021. The empirical result reveals that there exist three groups over the node-specific time-varying coefficients, and the momentum effects tend to have negative and time-varying patterns which are sensible from the demand and supply perspective. 

The rest of the paper is organized as follows. Section \ref{sec2} introduces the heterogeneous time-varying network VAR model together with the latent group structure and provides some fundamental model assumptions. Section \ref{sec3} describes the estimation methodology for the group membership and number and establishes the consistency property. Section \ref{sec4} proposes the post-grouping nonparametric estimation and inference and derives the relevant asymptotic theory. Section \ref{sec5} reports both the simulation and empirical studies. Section \ref{sec6} concludes the paper. Appendix A contains proofs of the main asymptotic theorems and Appendix B contains some technical lemmas with their proofs. Throughout the paper, we let $\lfloor\cdot\rfloor$ and $\lceil\cdot\rceil$ be the floor and ceiling functions, respectively. For a vector $x=(x_1,\cdots,x_p)^{^\intercal}\in\mathscr{R}^p$, we write $\vert x\vert=\left(\sum_{i=1}^px_i^2\right)^{1/2}$ as the Euclidean norm; for a matrix $\boldsymbol{\Sigma}=\left(\sigma_{ij}\right)_{p\times p}$, write $\Vert\boldsymbol{\Sigma}\Vert=\left(\sum_{i=1}^{p}\sum_{j=1}^{p}\sigma_{ij}^2\right)^{1/2}$, $\vert\boldsymbol{\Sigma}\vert_\infty=\max_{1\leq i,j\leq p}\vert\sigma_{ij}\vert$ and $\vert\boldsymbol{\Sigma}\vert_2=\max_{x\in\mathscr{R}^p: \vert x\vert=1}\vert\boldsymbol{\Sigma} x\vert$; and for a $p$-dimensional random vector $Z$, write $Z\in\mathscr{L}^q$ if $\Vert Z\Vert_q:=\left[{\sf E}\left(\vert Z\vert^q\right)\right]^{1/q}<\infty, q\ge1$. Let $\boldsymbol{I}_k$ be a $k\times k$ identity matrix and $\boldsymbol{O}_{k\times l}$ a $k\times l$ zero matrix. For a square matrix, $\lambda_{\min}(\cdot)$ and $\lambda_{\max}(\cdot)$ denote the minimum and maximum eigenvalues and ${\sf det}(\cdot)$ denotes its determinant. Let $a_n=o(b_n)$, $a_n=o_P(b_n)$ and $a_n\propto b_n$ denote that $a_n/b_n\to 0$ as $n\to\infty$, $a_n/b_n\to 0$ with probability approaching one ({\em w.p.a.1}), and $0<\underline{c}\leq a_n/b_n\leq \overline{c}<\infty$, respectively.

%%%%%%%%%%%%%%%%%%%

\section{Time-varying network VAR and latent groups}\label{sec2} 
\renewcommand{\theequation}{2.\arabic{equation}} \setcounter{equation}{0}

In this section, we introduce the main model framework, i.e., the time-varying network VAR model with a latent group structure, and impose some fundamental assumptions, ensuring the network time series are locally stable. 

\subsection{heterogeneous time-varying network VAR}\label{sec2.1}

Consider the following time-varying network VAR model:
\begin{equation}\label{eq2.1}
x_{i,t}=\beta_{i,1}\left(\tau_t\right) \sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}+\beta_{i,2}\left(\tau_t\right)x_{i,t-1}+\varepsilon_{i,t},\ \ i=1,\cdots,N,\ \ t=1,\cdots,T,
\end{equation}
where $\tau_t=t/T$ denotes the scaled time point, $\beta_{i,1}(\cdot)$ and $\beta_{i,2}(\cdot)$ are node-specific smooth coefficient functions, and the remaining elements are defined as those in (\ref{nVAR}). In contrast to the linear network VAR model (\ref{nVAR}), the heterogeneous time-varying network VAR model (\ref{eq2.1}) provides a much more flexible framework, allowing the network and momentum effects to change over time and nodes. Although we only consider one lag in model (\ref{eq2.1}) for simplicity of exposition, the method and theory developed in Sections \ref{sec3} and \ref{sec4} below can be easily extended to the model setting with finite lags. Let $\widetilde{\boldsymbol W}$ be the row-normalized adjacency matrix with the $(i,j)$-entry being $\widetilde{w}_{ij}$, 
\[
{\boldsymbol B}_1(\tau_t)={\sf diag}\left\{\beta_{1,1}(\tau_t),\cdots,\beta_{N,1}(\tau_t)\right\}\ \ \ {\rm and}\ \ \ {\boldsymbol B}_2(\tau_t)={\sf diag}\left\{\beta_{1,2}(\tau_t),\cdots,\beta_{N,2}(\tau_t)\right\}.
\]
Then, we may rewrite model (\ref{eq2.1}) as
\begin{equation}\label{eq2.2}
X_t={\boldsymbol B}(\tau_t)X_{t-1}+\varepsilon_t,\ \ {\boldsymbol B}(\cdot)={\boldsymbol B}_1(\cdot)\widetilde{\boldsymbol W}+{\boldsymbol B}_2(\cdot),\ \ t=1,\cdots,T.
\end{equation}
Model (\ref{eq2.2}) falls within the high-dimensional time-varying VAR model framework which has received increasing attention in recent years. For instance, \cite{DQC17} propose a kernel-weighted $\ell_1$-regularised estimation for a time-varying VAR model when the number of time series variables diverges at a sub-exponential rate of $T$; \cite{XCW20} study a high-dimensional VAR model with multiple breaks and estimate smooth time-varying covariance and precision matrices between the break points; \cite{CLLL23} estimate dual network structures via directed Granger causality and undirected partial correlation linkages within the high-dimensional time-varying VAR framework. However, the aforementioned literature often assumes a sparsity condition on the time-varying VAR transition matrices to facilitate the use of the regularised estimation techniques and cannot directly capture time-varying network effects. In this paper, we do not require the sparsity restriction by decomposing the time-varying transition matrix ${\boldsymbol B}(\tau_t)$ into two components: ${\boldsymbol B}_2(\tau_t)$ capturing the momentum effects; and ${\boldsymbol B}_1(\tau_t)\widetilde{\boldsymbol W}$ capturing the dynamic network spillover effects.

In the homogenous or grouped linear network VAR \citep[e.g.,][]{ZPLLW17, CFZ22}, the so-called nodal effect is often incorporated in the model formulation. Hence, we may extend model (\ref{eq2.1}) to 
\begin{equation}\label{eq2.3}
x_{i,t}=\beta_{i,1}\left(\tau_t\right) \sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}+\beta_{i,2}\left(\tau_t\right)x_{i,t-1}+Z_i^{^\intercal}\gamma(\tau_t)+\varepsilon_{i,t},
\end{equation}
where $Z_i$ is a $p$-dimensional vector of node-specific exogenous covariates and $\gamma(\cdot)$ is a vector of smooth coefficient functions.  Since $Z_i$ are node-specific covariates, we do not need to further assume heterogeneity on $\gamma(\cdot)$. Letting $\{Z_i\}$ be independent of $\{\varepsilon_{i,t}\}$, the model framework and methodology developed in the sequel can be extended to tackle (\ref{eq2.3}) with slight modification. However, for notational simplicity, we mainly focus on model (\ref{eq2.1}) or (\ref{eq2.2}) throughout the paper. 

%%%%%%%%%%%%%%%%

\subsection{Fundamental assumptions and functional dependence measure}\label{sec2.2}

We impose the following assumption on model (\ref{eq2.1}) or (\ref{eq2.2}).

\begin{assumption}\label{ass:1}

{\em (i)\ Uniformly over $\tau\in[0,1]$, ${\sf det}({\boldsymbol I}_N-z{\boldsymbol B}(\tau))\neq0$ for all $|z|\leq 1$.}

{\em (ii)\ For $i=1,\cdots,N$, $\beta_{i,1}(\cdot)$ and $\beta_{i,2}(\cdot)$ are second-order continuously differentiable functions with
\[
\max_{1\leq i\leq N}\sup_{0\leq \tau\leq 1}\left\{\left\vert\beta_{i,1}^\prime(\tau)\right\vert+\left\vert\beta_{i,2}^\prime(\tau)\right\vert+\left\vert\beta_{i,1}^{\prime\prime}(\tau)\right\vert+\left\vert\beta_{i,2}^{\prime\prime}(\tau)\right\vert\right\}\leq c_\beta,
\]
where $\beta_{i,j}^{\prime}(\cdot)$ and $\beta_{i,j}^{\prime\prime}(\cdot)$ are the first- and second-order derivatives of $\beta_{i,j}(\cdot)$, $j=1,2$, and $c_\beta$ is a positive constant.
}

%{\em (iii)\ The node-specific covariates $Z_i$ have zero mean, covariance matrix ${\boldsymbol\Sigma}_Z$ and finite $q$-th moment, $q\geq8$. The homogenous coefficient function $\gamma(\cdot)$ is second-order continuously differentiable.}

{\em (iii)\ Let $\{\varepsilon_t\}$ be a sequence of i.i.d. random vectors with zero mean, covariance matrix ${\boldsymbol\Sigma}_\varepsilon$ and 
\[
\max_{1\leq i\leq N}{\sf E}\left[|\varepsilon_{i,t}|^{q}\right]\leq c_\varepsilon,
\]
where $q>8$ and $c_\varepsilon$ is a positive constant.}

\end{assumption}

Assumption \ref{ass:1}(i) is a natural extension of the stability assumption on linear network VAR, see Theorems 1 and 2 in \cite{ZPLLW17}. This condition ensures that the underlying time series process is locally stable and implies the following local Wold representation:
\begin{equation}\label{eq2.4}
X_t(\tau)=G(\tau,\mathscr{F}_t):=\sum_{m=0}^\infty \boldsymbol{B}_m(\tau)\varepsilon_{t-m}\ \ {\rm for}\ \ 0<\tau<1,
\end{equation}
where $\boldsymbol{B}_0(\tau)=\boldsymbol{I}_N$, $\boldsymbol{B}_m(\tau)=\prod_{k=\lfloor T\tau\rfloor-m+1}^{\lfloor T\tau\rfloor}\boldsymbol{B}(\tau_k)$ and $\mathscr{F}_t=(\cdots,\varepsilon_{t-1},\varepsilon_{t})$. By (\ref{eq2.2}) and (\ref{eq2.4}), we may write $X_t=X_t(\tau_t)$. The smoothness condition on $\beta_{i,1}(\cdot)$ and $\beta_{i,2}(\cdot)$ in Assumption \ref{ass:1}(ii) is common for the local linear estimation method and theory \citep[e.g.,][]{FG96}. We require a relatively strong moment condition  ($q>8$) in Assumption \ref{ass:1}(iii) to derive the uniform convergence of the kernel-weighted quantities, see Lemmas \ref{lem:B.1} and \ref{lem:B.2} in Appendix B. 

We next connect the proposed time-varying network VAR model to the nonlinear functional dependence measure introduced by \cite{Wu05}, which will facilitate the development of our main asymptotic theory. Let $\left\{\varepsilon_t^\ast\right\}$ be an independent copy of $\left\{\varepsilon_t\right\}$ and $\mathscr{F}_{t}^{\{l\}}=(\cdots,\varepsilon_{l-1},\varepsilon_l^\ast,\varepsilon_{l+1},\cdots,\varepsilon_t)$ be a coupled version of $\mathscr{F}_t$ replacing $\varepsilon_l$ by $\varepsilon_l^\ast$. Define $X_{t}^{\{l\}}(\tau)=G(\tau,\mathscr{F}_{t}^{\{l\}})$ and $X_{t}^{\{l\}}=X_{t}^{\{l\}}(\tau_t)$. As in \cite{XCW20} and \cite{ZW21}, we define the node-wise functional dependence measure:
\[
\delta_{i,t,q}=\sup_{\tau\in[0,1]}\left\Vert x_{i,t}(\tau)-x_{i,t}^{\{0\}}(\tau)\right\Vert_q,
\]
%and the uniform functional dependence measure:
%\[
%\omega_{t,q}=\sup_{\tau\in[0,1]}\left\Vert\left\vert X_t(\tau)-X_{t}^{\{0\}}(\tau)\right\vert_\infty\right\Vert_{q},
%\]
where $q$ is defined in Assumption \ref{ass:1}(iii), $x_{i,t}(\tau)$ is the $i$-th element of $X_t(\tau)$, and $x_{i,t}^{\{0\}}(\tau)$ is the $i$-th element of $X_{t}^{\{0\}}(\tau)$, and further construct the node-wise dependence adjusted norm:
\[
\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}=\sup_{m\ge0}(m+1)^\alpha\Delta_{i,m,q},~~ \Delta_{i,m,q}=\sum_{t=m}^\infty\delta_{i,t,q},
\]
%and
%\[
%{\sf DAM}_{q,\alpha}=\sup_{m\ge0}(m+1)^\alpha\Omega_{m,q},~~ \Omega_{m,q}=\sum_{t=m}^\infty\omega_{t,q},
%\]
where $\alpha\geq0$ depicts the decay rate of the cumulative dependence measure $\Delta_{i,m,q}$. 
%We further define the uniform dpendence adjusted norm
%\[
%\Phi_{q,\alpha}=\max_{1\le i\le N}\Vert X_{i\cdot}\Vert_{q,\alpha}.
%\]
Letting $\boldsymbol{B}_{m,i \bullet}(\tau)$ be the $i$-th row vector of $\boldsymbol{B}_{m}(\tau)$, assume that there exist $\alpha>1/2-2/q$ and $0<c<\infty$, such that for all $t\ge0$ and $1\le i\le N$, $\sup_{\tau\in[0,1]}\vert\boldsymbol{B}_{t,i \bullet}(\tau)\vert\le c(t+1)^{-(\alpha+1)}$. Then, using (\ref{eq2.4}) and following the argument of Example 2.2 of \cite{ZW21}, we have $\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}\le c_{q,\alpha}$, where $c_{q,\alpha}$ is a positive constant depending on $q$ and $\alpha$.

%%%%%%%%%%%%%%%%

\subsection{The latent group structure}\label{sec2.3}

Note that the number of unknown coefficient functions in (\ref{eq2.1}) for describing time-varying network and momentum effects is $2N$, which diverges to infinity as $N\rightarrow\infty$ and may be larger than $T$. Without further restriction on heterogeneity, we can only make use of the sample information from the $i$-th node and the average impact from its neighbors to nonparametrically estimate the node-specific coefficient functions $\beta_{i}(\cdot)=\left[\beta_{i,1}(\cdot),\beta_{i,2}(\cdot)\right]^{^\intercal}$ (see Section \ref{sec3.1}), leading to slow estimation convergence rates and unstable numerical performance in particular when $T$ is not sufficiently large. To address this problem, we impose a latent group structure on $\beta_i(\cdot)$, i.e., suppose there exists a partition of the index set $\{1,2,\cdots,N\}$, denoted by ${\mathscr{G}}=\{{\mathscr G}_1,{\mathscr G}_2,\cdots,{\mathscr G}_{K_0}\}$, such that
\begin{equation}\label{eq2.5}
{\mathscr G}_i\cap {\mathscr G}_j=\emptyset\ \ {\rm for} \ 1\leq i\neq j\leq K_0,\ \ {\rm and}\ \beta_i(\cdot)=\alpha_k(\cdot)\ \ {\rm for}\ i\in{\mathscr G}_k,
\end{equation}
where $\alpha_{k}(\cdot)=\left[\alpha_{k,1}(\cdot),\alpha_{k,2}(\cdot)\right]^{^\intercal}$ denotes a two-dimensional vector of group-specific coefficient functions. The time-varying network and momentum effects have the same functional patterns over nodes within the same group, but vary over different groups. In this paper, neither the group membership ${\mathscr G}$ nor the number $K_0$ is known a priori. 

With (\ref{eq2.1}) and (\ref{eq2.5}), we may re-write the main model as 
\begin{equation}\label{eq2.6}
x_{i,t}=\alpha_{k,1}\left(\tau_t\right) \sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}+\alpha_{k,2}\left(\tau_t\right)x_{i,t-1}+\varepsilon_{i,t},\ \ i\in{\mathscr G}_k,\ \ t=1,\cdots,T,
\end{equation}
which is called as the grouped time-varying network VAR model. Let ${\boldsymbol M}$ be an $N\times K_0$ membership matrix such that it has exactly one element taking the value of $1$ in each row and at least one element taking the value of $1$ in each column. Let $m_{i,k}\in\{0,1\}$ be the $(i,k)$-entry of ${\boldsymbol M}$ with $m_{i,k}=1$ if the $i$-th node belongs to the $k$-th group, and $M_i=(m_{i,1},\cdots,m_{i,K_0})^{^\intercal}$. Then, we may further write model (\ref{eq2.6}) in the vector/matrix form as in (\ref{eq2.2}), letting
\[
{\boldsymbol B}_1(\tau_t)={\sf diag}\left\{M_1^{^\intercal}{\boldsymbol\alpha}_{\bullet1}(\tau_t),\cdots,M_N^{^\intercal}{\boldsymbol\alpha}_{\bullet1}(\tau_t)\right\}\ \ {\rm and}\ \ {\boldsymbol B}_2(\tau_t)={\sf diag}\left\{M_1^{^\intercal}{\boldsymbol\alpha}_{\bullet2}(\tau_t),\cdots,M_N^{^\intercal}{\boldsymbol\alpha}_{\bullet2}(\tau_t)\right\},
\]
where ${\boldsymbol\alpha}_{\bullet 1}(\cdot)=\left[\alpha_{1,1}(\cdot),\cdots,\alpha_{K_0,1}(\cdot)\right]^{^\intercal}$ and ${\boldsymbol\alpha}_{\bullet 2}(\cdot)=\left[\alpha_{1,2}(\cdot),\cdots,\alpha_{K_0,2}(\cdot)\right]^{^\intercal}$. It follows from (\ref{eq2.6}) that the number of unknown coefficient functions is reduced from $2N$ to $2K_0$. Since $K_0$ is assumed to be fixed, we achieve dimension reduction and can improve the estimation convergence in the subsequent nonparametric estimation procedure (see Section \ref{sec4.1}) if the latent group structure is consistently estimated. 


%%%%%%%%%%%%%%%%%%%


\section{Estimation of the latent group structure}\label{sec3} 
\renewcommand{\theequation}{3.\arabic{equation}} \setcounter{equation}{0}
\renewcommand{\theprop}{3.\arabic{prop}}\setcounter{prop}{0}

In this section, we introduce the methodology for estimating the latent group membership and number, and present the relevant asymptotic properties. 


%%%%%%%%%%%%%%%%%%%

\subsection{Estimation of ${\mathscr G}$ when $K_0$ is pre-specified}\label{sec3.1}

\medskip

\noindent{\em 3.1.1.\ \ Point-wise distance estimation between nodes}

\smallskip

\noindent As there is no prior information on the latent group structure, we start with the fully heterogeneous time-varying network VAR model (\ref{eq2.1}). Letting 
\[
\widetilde{X}_{i,t-1}=\left(\sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1},\ x_{i,t-1}\right)^{^\intercal},
\] 
we re-write (\ref{eq2.1}) as 
\begin{equation}\label{eq3.1}
x_{i,t}=\beta_{i}^{^\intercal}(\tau_t) \widetilde{X}_{i,t-1} +\varepsilon_{i,t},\ \ i=1,\cdots,N,\ \ t=1,\cdots,T.
\end{equation}
With the smoothness condition in Assumption \ref{ass:1}(ii), the node-specific coefficient functions can be estimated by the local linear smoothing \citep[e.g.,][]{FG96}, only using the time series sample information from the $i$-th node and the average impact from its direct neighbors, i.e., $\sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}$. Letting $\beta_i^\prime(\cdot)=\left[\beta_{i,1}^\prime(\cdot),\beta_{i,2}^\prime(\cdot)\right]^{^\intercal}$, for any given $\tau\in[0,1]$, we have the following Taylor expansion:
\[
\beta_i(\tau_t)\approx \beta_i(\tau)+\beta_i^{\prime}(\tau)(\tau_t-\tau),
\]
when $\tau_t$ is in a small neighborhood of $\tau$. Define the node-specific local linear weighted objective function:
\begin{equation}\label{eq3.2}
{\cal L}_i(a,b)=\frac{1}{T}\sum_{t=1}^T\left[x_{i,t}-a^{^\intercal}\widetilde X_{i,t-1}-b^{^\intercal}\widetilde X_{i,t-1}(\tau_t-\tau)\right]^2K_h(\tau_t-\tau),
\end{equation}
where $a=(a_1,a_2)^{^\intercal}$, $b=(b_1,b_2)^{^\intercal}$, $K_h(\cdot)=\frac{1}{h}K(\cdot/h)$, $K(\cdot)$ is a kernel function and $h$ is a bandwidth. Minimizing ${\cal L}_i(a,b)$ with respect to the vectors $a$ and $b$, we obtain the solutions: $\widehat a=(\widehat a_1, \widehat a_2)^{^\intercal}$ and $\widehat b=(\widehat b_1, \widehat b_2)^{^\intercal}$, and then the local linear estimate of $\beta_i(\tau)$ as
\begin{equation}\label{eq3.3}
\widehat{\beta}_i(\tau)=\left[\widehat\beta_{i,1}(\tau), \widehat{\beta}_{i,2}(\tau)\right]^{^\intercal}=\left(\widehat a_1, \widehat a_2\right)^{^\intercal}.
\end{equation}

With the preliminary local linear estimates defined in (\ref{eq3.3}), we may compute the point-wise distance between nodes which is defined by
\[
\widehat{d}_{ij}(\tau)=\widehat{\beta}_i(\tau)-\widehat{\beta}_j(\tau)=\left[
\begin{array}{c}
\widehat{\beta}_{i,1}(\tau)-\widehat{\beta}_{j,1}(\tau)\\
\widehat{\beta}_{i,2}(\tau)-\widehat{\beta}_{j,2}(\tau)
\end{array}
\right].
\]
This is a natural estimate of $d_{ij}(\tau)=\beta_i(\tau)-\beta_j(\tau)$, $1\leq i\neq j\leq N$. Let $
{\boldsymbol\Delta}_i(\tau)={\sf E}[\widetilde{X}_{i,t}(\tau)\widetilde{X}_{i,t}^{^\intercal}(\tau)]$, where $\widetilde{X}_{i,t}(\tau)$ is defined similarly to $\widetilde{X}_{i,t}$ but with $x_{i,t}$ replaced by $x_{i,t}(\tau)$, the $i$-th element of $X_t(\tau)$ defined in (\ref{eq2.4}). The following conditions are required to establish the asymptotic property of $\widehat{d}_{ij}(\cdot)$.  

\begin{assumption}\label{ass:2}

{\em (i)\ The kernel function $K\left(\cdot\right)$ is a symmetric probability density function that is Lipschitz-continuous and has a compact support $\left[-1,1\right]$.}

{\em (ii)\ For each $1\le i\le N$, the matrix ${\boldsymbol\Delta}_i(\tau)$ is positive-definite uniformly over $\tau\in[0,1]$. In addition, there exist two finite positive constants: $\underline{\lambda}$ and $\overline{\lambda}$, such that
\[
0<\underline{\lambda}\le\min_{1\le i\le N}\inf_{0\leq\tau\leq1}\lambda_{\min}({\boldsymbol\Delta}_i(\tau))\le\max_{1\le i\le N}\sup_{0\leq\tau\leq1}\lambda_{\max}({\boldsymbol\Delta}_i(\tau))\le\overline{\lambda}<\infty.
\]
}

{\em (iii)\ The time series length $T$, the number of nodes $N$, and the bandwidth $h$ satisfy $h\rightarrow0$, $Th\rightarrow\infty$ and}
\begin{equation}\label{eq3.4}
\frac{N^2}{T^{\frac{q^2-6q-8}{4(q+2)}}\left[h\log(N\vee T)\right]^{q/4}}\to0.
\end{equation}


\end{assumption}

Assumption \ref{ass:2}(i) contains some commonly-used conditions imposed on the kernel function \citep[e.g.,][]{FG96}. Assumption \ref{ass:2}(ii) is crucial to ensure that the kernel-weighted random denominator in local linear estimation is non-singular (e.g., the proof of Proposition \ref{prop:3.1} in Appendix A). Note that $h\rightarrow0$ and $Th\rightarrow\infty$ in Assumption \ref{ass:2}(iii) are regular conditions for kernel-based smoothing, whereas the condition (\ref{eq3.4}) indicates that there is a trade-off between the network size and the required moment condition (i.e., when $q$ increases, $N$ may diverge at a faster rate). 

Write $\sigma_{ij}={\sf E}\left(\varepsilon_{i,t}\varepsilon_{j,t}\right)$, $\sigma_i^2=\sigma_{ii}$ and 
\[
{\boldsymbol\Delta}_{ij}(\tau)={\sf E}\left[\widetilde{X}_{i,t}(\tau)\widetilde{X}_{j,t}^{^\intercal}(\tau)\right],\ \ 1\leq i,j\leq N.
\] 
We next give the point-wise asymptotic distribution theory for $\widehat{d}_{ij}(\tau)$, which is similar to the conventional kernel-based estimation theory of the time-varying coefficient functions \citep[e.g.,][]{C07, ZW10, Vo12, ZW12}. 

\begin{prop}\label{prop:3.1}

 {\em Suppose that Assumptions \ref{ass:1} and \ref{ass:2} hold. For any given $\tau\in(0,1)$ and $1\leq i\neq j\leq N$, 
\begin{equation}\label{eq3.5}
\sqrt{Th}\left[\widehat{d}_{ij}(\tau)-d_{ij}(\tau)-\frac{1}{2}h^2\mu_2d_{ij}^{\prime\prime}(\tau)\right]\stackrel{d}\longrightarrow {\sf N}\left({\bf 0}, {\boldsymbol\Sigma}_{ij}(\tau)\right),
\end{equation}
where $d_{ij}^{\prime\prime}(\tau)=\beta_i^{\prime\prime}(\tau)-\beta_j^{\prime\prime}(\tau)$ with $\beta_{i}^{\prime\prime}(\tau)=\left[\beta_{i,1}^{\prime\prime}(\tau),\beta_{i,2}^{\prime\prime}(\tau)\right]^{^\intercal}$, and 
\[
{\boldsymbol\Sigma}_{ij}(\tau)=\nu_0\left[\sigma_i^2{\boldsymbol\Delta}_i^{-1}(\tau)+\sigma_j^2{\boldsymbol\Delta}_j^{-1}(\tau)-{\boldsymbol\Sigma}_{ij}^\star(\tau)\right]
\]
with 
\[{\boldsymbol\Sigma}_{ij}^\star(\tau)=\sigma_{ij}\left[{\boldsymbol\Delta}_i^{-1}(\tau){\boldsymbol\Delta}_{ij}(\tau){\boldsymbol\Delta}_j^{-1}(\tau)+{\boldsymbol\Delta}_j^{-1}(\tau){\boldsymbol\Delta}_{ji}(\tau){\boldsymbol\Delta}_i^{-1}(\tau)\right],
\] 
$\mu_k=\int_{-1}^1u^kK(u)du$ and $\nu_{k}=\int_{-1}^1u^kK^2(u)du$.}

\end{prop}

We next construct the consistent estimation of the asymptotic variance matrix ${\boldsymbol\Sigma}_{ij}(\tau)$to facilitate further inference. Let 
\[
{\boldsymbol\Sigma}_{ij,T}(\tau)=\nu_0\left[\sigma_i^2{\boldsymbol\Delta}_{i,T}^{-1}(\tau)+\sigma_j^2{\boldsymbol\Delta}_{j,T}^{-1}(\tau)-{\boldsymbol\Sigma}_{ij,T}^\star(\tau)\right],
\]
where ${\boldsymbol\Sigma}_{ij,T}^\star(\tau)$ is defined similarly to ${\boldsymbol\Sigma}_{ij}^\star(\tau)$ but with ${\boldsymbol\Delta}_{ij}(\tau)$ and ${\boldsymbol\Delta}_i(\tau)$ replaced by
\[
{\boldsymbol\Delta}_{ij,T}(\tau)=\frac{1}{T}\sum_{t=1}^T{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{j,t-1}^{^\intercal}\right]K_h(\tau_t-\tau)\ \ \ {\rm and}\ \  {\boldsymbol\Delta}_{i,T}(\tau)={\boldsymbol\Delta}_{ii,T}(\tau),
\]
respectively. Following the proof of Proposition \ref{prop:3.1}, we may show that ${\boldsymbol\Sigma}_{ij,T}(\tau)-{\boldsymbol\Sigma}_{ij}(\tau)=o(1)$. Hence, we next only need to consistently estimate ${\boldsymbol\Sigma}_{ij,T}(\tau)$.


Letting $\widehat{\varepsilon}_{i,t}=x_{i,t}-\widehat{\beta}_i^{^\intercal}(\tau_t) \widetilde{X}_{i,t-1}$, we may consistently estimate $\sigma_{ij}$ and $\sigma_i^2$ by
\[
\widehat{\sigma}_{ij}=\frac{1}{T}\sum_{t=1}^T \widehat{\varepsilon}_{i,t}\widehat{\varepsilon}_{j,t},\ \ 1\leq i,j\leq N,\ \ \ {\rm and}\ \ \ \widehat{\sigma}_i^2=\widehat{\sigma}_{ii},
\]
respectively. Define
\[
\widehat{\boldsymbol\Delta}_{ij}(\tau)=\frac{1}{T}\sum_{t=1}^T\widetilde{X}_{i,t-1}\widetilde{X}_{j,t-1}^{^\intercal}K_h(\tau_t-\tau),\ \ 1\leq i,j\leq N,\ \ \widehat{{\boldsymbol\Delta}}_i(\tau)=\widehat{{\boldsymbol\Delta}}_{ii}(\tau),
\]
and
\[
\widehat{\boldsymbol\Sigma}_{ij}(\tau)=\nu_0\left[\widehat\sigma_i^2\widehat{\boldsymbol\Delta}_i^{-1}(\tau)+\widehat\sigma_j^2\widehat{\boldsymbol\Delta}_j^{-1}(\tau)-\widehat{\boldsymbol\Sigma}_{ij}^\star(\tau)\right],
\]
where
\[
\widehat{\boldsymbol\Sigma}_{ij}^\star(\tau)=\widehat\sigma_{ij}\left[\widehat{\boldsymbol\Delta}_i^{-1}(\tau)\widehat{\boldsymbol\Delta}_{ij}(\tau)\widehat{\boldsymbol\Delta}_j^{-1}(\tau)+\widehat{\boldsymbol\Delta}_j^{-1}(\tau)\widehat{\boldsymbol\Delta}_{ji}(\tau)\widehat{\boldsymbol\Delta}_i^{-1}(\tau)\right].
\] 
In Appendix A, we prove that $\widehat{\boldsymbol\Sigma}_{ij}(\tau)$ is a uniformly consistent estimate of ${\boldsymbol\Sigma}_{ij,T}(\tau)$, see (\ref{eqA.11}).

\medskip


\noindent{\em 3.1.2.\ \ The clustering algorithm with normalized distance matrix}

\smallskip

\noindent Let $\tau_l^\ast$, $l=1,\cdots,L$, be a sequence of user-specified equidistant grid points between $0$ and $1$, where $L\rightarrow\infty$ and $L=O(T)$. Define the following normalized time-varying difference estimate between the $i$-th and $j$-th nodes at each of the grid points: 
\[
\widehat{D}_{ij}(\tau_l^\ast)=\widehat{\boldsymbol\Sigma}_{ij}^{-1/2}(\tau_l^\ast) \widehat{d}_{ij}(\tau_l^\ast),\ \ l=1,\cdots,L.
\] 
Then we construct
\begin{equation}\label{eq3.6}
\widehat{D}_{ij}=\frac{1}{L}\sum_{l=1}^L \widehat{D}_{ij}^{^\intercal}(\tau_l^\ast) \widehat{D}_{ij}(\tau_l^\ast)=\frac{1}{L}\sum_{l=1}^L \widehat{d}_{ij}^{^\intercal}(\tau_l^\ast) \widehat{\boldsymbol\Sigma}_{ij}^{-1}(\tau_l^\ast)\widehat{d}_{ij}(\tau_l^\ast).
\end{equation}
Let $\widehat{\boldsymbol D}$ be an $N\times N$ estimated distance matrix between nodes with the $(i,j)$-entry being $\widehat{D}_{ij}$. It is clear that the diagonal elements of $\widehat{\boldsymbol D}$ are zeros. With the distance matrix estimate, we may adopt the agglomerative clustering algorithm which is commonly used in unsupervised cluster analysis \citep[e.g.,][]{HTF09, ELLS11}. This clustering algorithm has been recently combined with the kernel-based estimation technique to identify the homogeneity/group structure in nonparametric panel regression models. For instance, \cite{Ch19} constructs the distance matrix using the unnormalized distance measure, i.e., $\widehat{D}_{ij}=\frac{1}{L}\sum_{l=1}^L \widehat{d}_{ij}^{^\intercal}(\tau_l^\ast) \widehat{d}_{ij}(\tau_l^\ast)$, and further estimate the latent group structure in time-varying coefficient panel data models; and \cite{VL20} introduce a bandwidth-free normalized distance measure in the clustering algorithm but assume the panel observations are independent over subjects. The latter assumption may be too restrictive for large-scale network time series data and is thus removed in this paper. Another relevant paper is \cite{Z13} which clusters nonlinear trend functions based on parallelism and allows the number of time series to grow at a slow polynomial rate of $T$. In contrast, the number of nodes $N$ can be much larger than $T$ in this paper, see (\ref{eq3.4}) in Assumption \ref{ass:2}(iii).  

We next describe the clustering algorithm when the true group number $K_0$ is known a priori. Start with $N$ clusters each of which corresponds to one node, search for the smallest off-diagonal entry in $\widehat{\boldsymbol D}$ which is the smallest estimated distance between nodes, and merge the two corresponding nodes. Consequently the cluster number reduces from $N$ to $N-1$. Use a linkage technique (such as the single or complete linkage) to calculate the distance between the merged cluster and the remaining ones and update the estimated distance matrix with size $(N-1)\times (N-1)$. Repeat the previous steps with the updated distance matrix, and stop the algorithm when the number of clusters reaches $K_0$. We denote the estimated clusters by $\widehat{\mathscr G}_k$, $k=1, \cdots,K_0$. 

The following theorem states the consistency property.

\renewcommand{\thetheorem}{3.\arabic{theorem}}\setcounter{theorem}{1}

\begin{theorem}\label{thm:3.2}

Suppose that Assumptions \ref{ass:1} and \ref{ass:2} hold and the group number $K_0$ is known a priori. In addition,
\begin{equation}\label{eq3.7}
\underline{D}:=\min_{1\le k\neq l\le K_0}\min_{i\in{\mathscr G}_k,j\in{\mathscr G}_l}D_{ij}>0,\ \ h^2+\sqrt{\frac{\log (N\vee T)}{Th}}+L^{-1}=o\left(\underline{D}\right),
\end{equation}
where $D_{ij}=\int_0^1D_{ij}^{^\intercal}(\tau)D_{ij}(\tau)d\tau$ with $D_{ij}(\tau)={\boldsymbol\Sigma}_{ij,T}^{-1/2}(\tau)d_{ij}(\tau)$, and ${\boldsymbol\Sigma}_{ij}(\tau)$ is positive definite uniformly over $i,j$ and $\tau$. Then we have
\begin{equation}\label{eq3.8}
{\sf P}\left(\big\{\widehat{\mathscr G}_k,\ 1\leq k\leq K_0\big\}=\big\{{\mathscr G}_k,\ 1\leq k\leq K_0\big\}\right)\rightarrow1.
\end{equation}

\end{theorem}

The consistency property (\ref{eq3.8}) is similar to the consistency results of group membership estimation in nonparametric panel/longitudinal data models, see Theorem 3.1 in \cite{VL17} and Theorem 4.1(a) in \cite{VL20}. The condition (\ref{eq3.7}) indicates that the minimum scaled distance between groups may converge to zero at a rate slower than $h^2+\sqrt{\frac{\log (N\vee T)}{Th}}$ (a typical nonparametric uniform convergence rate) if the grid number $L$ is of order $T$. In fact, to prove Theorem \ref{thm:3.2}, it suffices to show that
\[\max_{1\le k \le K_0}\max_{i,j\in{\mathscr G}_k}\widehat{D}_{ij}<\min_{1\le k\neq l\le K_0}\min_{i\in{\mathscr G}_k,j\in{\mathscr G}_l}\widehat{D}_{ij},\ \ w.p.a.1.\]
This can be proved by using the uniform convergence properties of $\widehat d_{ij}(\cdot)$ and $\widehat{\boldsymbol\Sigma}_{ij}(\cdot)$.

To further reduce the local linear estimation bias, we may adopt a bias-corrected normalized distance measure between nodes. Let $\widehat{\beta}_i(\tau\ |\ h)$ be the local linear estimate of $\beta_i(\tau)$, where we make the dependence on the bandwidth $h$ explicitly. As recommended by \cite{X98} and \cite{ZW10}, we may use the jackknife bias-corrected local linear estimates:
\[
\widehat{\beta}_i^{bc}(\tau)=2\widehat{\beta}_i(\tau\ |\ 2^{-1/2}h)-\widehat{\beta}_i(\tau\ |\ h)
\]
and
\[
\widehat{d}_{ij}^{bc}(\tau)=\widehat{\beta}_i^{bc}(\tau)-\widehat{\beta}_j^{bc}(\tau).
\]
It is easy to show that the bias-corrected estimation has a smaller bias order $h^4$, and implementing the bias correction in local linear estimation is equivalent to the conventional local linear estimation using the fourth-order kernel $K^\star(x)=2\sqrt{2}K(\sqrt{2}x)-K(x)$. The bias-corrected version of the normalized time-varying coefficient difference estimate can be constructed similarly, and Theorem \ref{thm:3.2} continues to hold under the following condition weaker than (\ref{eq3.7}):
\[
h^4+\sqrt{\frac{\log (N\vee T)}{Th}}+L^{-1}=o\left(\underline{D}\right).
\]

\subsection{Estimation of the group number $K_0$}\label{sec3.2}

In practice, the true group number $K_0$ is often unknown and a data-driven criterion is thus required to estimate $K_0$. Assuming the group number to be $K$, we may terminate the clustering algorithm in Section \ref{sec3.1} when the cluster number reaches $K$ and obtain the estimated clusters denoted by $\widehat{\mathscr G}_{k|K}$, $k=1,\cdots,K$. Note that the grouped time-varying network VAR model is either correctly- or over-fitted if $K\geq K_0$, and is under-fitted if $K<K_0$. For the latter scenario, at least two groups would be falsely merged, leading to biased estimation of some group-specific time-varying coefficient functions. When the group number is set as $K$, we may estimate the group-specific coefficient functions using the estimated groups $\widehat{\mathscr G}_{k|K}$, $k=1,\cdots,K$. For each $k$ and given $\tau\in[0,1]$, we define the local linear objective function by pooling the sample information over $i\in\widehat{\mathscr G}_{k|K}$, i.e.,
\begin{equation}\label{eq3.9}
\frac{1}{T|\widehat{\mathscr G}_{k|K}|}\sum_{i\in\widehat{\mathscr G}_{k|K}}\sum_{t=1}^T\left[x_{i,t}-a^{^\intercal}\widetilde{X}_{i,t-1}-b^{^\intercal}\widetilde{X}_{i,t-1}(\tau_t-\tau)\right]^2K_{h_1}(\tau_t-\tau),
\end{equation}
where $h_1$ is a bandwidth which may be different from $h$ used in the heterogeneous local linear estimation (\ref{eq3.2}) and (\ref{eq3.3}). Minimizing the pooled objective function in (\ref{eq3.9}) with respect to the vectors $a$ and $b$, we obtain the solutions denoted by $\widetilde a$ and $\widetilde b$, and define the pooled local linear estimates of group-specific coefficient functions (when the group number is assumed to be $K$): 
\begin{equation}\label{eq3.10}
\widehat\alpha_{k|K}(\tau)=\left[\widehat\alpha_{k|K,1}(\tau), \widehat\alpha_{k|K,2}(\tau)\right]^{^\intercal}=\widetilde a.
\end{equation}
When $K\geq K_0$, we may show that the local linear estimates defined in (\ref{eq3.10}) are uniformly consistent estimates of the true group-specific coefficient functions in (\ref{eq2.6}); and when $K<K_0$, as the model is under-fitted, the local linear estimates in (\ref{eq3.10}) are biased for at least one $k$. 

We next propose a generalized information criterion to estimate $K_0$. Define
\begin{equation}\label{eq3.11}
{\sf IC}(K)=\log \widehat{\sigma}_{NT}^2(K) + K\rho_{NT},
\end{equation}
where $\rho_{NT}$ is a user-specified tuning parameter and 
\begin{equation}\label{eq3.12}
\widehat{\sigma}_{NT}^2(K)=\frac{1}{NT}\sum_{k=1}^K\sum_{i\in\widehat{\mathscr G}_{k|K}}\sum_{t=1}^T\left[x_{i,t}-\widehat{\alpha}_{k|K}^{^\intercal}(\tau_t)\widetilde X_{i,t-1}\right]^2.
\end{equation}
We estimate the group number by 
\begin{equation}\label{eq3.13}
\widehat{K}=\argmin_{1\leq K\leq \overline K} {\sf IC}(K),
\end{equation}
where $\overline K$ is a pre-specified positive integer larger than $K_0$. Similar information criteria are used by \cite{SSP16}, \cite{Ch19} and \cite{SWJ19} to determine the group number in linear and time-varying coefficient longitudinal data models, see also \cite{ZXF22}. We require some further conditions to derive the consistency property of $\widehat K$.

\begin{assumption}\label{ass:3}
	
{\em (i)\ There exist two positive constants $\kappa_1\in(0,1)$ and $\kappa_2$ such that}
\[
\min_{1\le k\le K_0}\vert{\mathscr G}_k\vert\ge\kappa_1\cdot N,~\min_{1\le k_1\neq k_2\le K_0}\int_0^1\left\vert\alpha_{k_1}(\tau)-\alpha_{k_2}(\tau)\right\vert^2d\tau>\kappa_2.
\]

{\em (ii)\ The bandwidth $h_1$ satisfies $h_1\propto h$ and $\log(N\vee T)=O(Th_1^3)$, and the tuning parameter $\rho_{NT}$ satisfies $\rho_{NT}\to0$ and $\frac{1}{NTh_1}+h_1^4=o(\rho_{NT})$.}

{\em (iii)\ For any index set ${\mathscr G}\subset{\mathscr G}_k,k=1,\cdots,K_0$,
\begin{equation}\label{eq3.14}
\max_{i\in{\mathscr G}}\sum_{j\in{\mathscr G}}|\sigma_{ij}|\leq \overline c,\quad \max_{1\leq t\leq T}{\sf E}\left[\left\vert \sum_{i\in{\mathscr G}}\varepsilon_{i,t}\widetilde X_{i,t-1}\right\vert^4\right]\leq \overline{c} |{\mathscr G}|^2,
\end{equation}
where $\sigma_{ij}={\sf E}\left(\varepsilon_{i,t}\varepsilon_{j,t}\right)$ and $\overline{c}$ is a positive constant independent of the choice of ${\mathscr G}$.}

\end{assumption}

Assumption \ref{ass:3}(i) is crucial to derive a lower bound of $\widehat{\sigma}_{NT}^2(K)$ when $K<K_0$. The condition $\min_{1\le k\le K_0}\vert{\mathscr G}_k\vert\ge\kappa_1\cdot N$ indicates that the cardinality of groups ${\mathscr G}_k$ is of the same order. By Assumption \ref{ass:3}(ii), we may set $\rho_{NT}=\frac{\log (N\vee T)}{NTh_1}$, in which case the proposed information criterion is similar to \cite{S78}'s Bayesian information criterion by treating $NTh_1$ as the ``effective" sample size in the kernel-based estimation. Assumption \ref{ass:3}(iii) allows weak correlation between nodes and is sufficient to verify the high-level conditions such as Assumption 3.6 in \cite{Ch19}. In fact, the first condition in (\ref{eq3.14}) is slightly weaker than $\max_{1\leq i\leq N}\sum_{j=1}^N|\sigma_{i,j}|\leq \overline c$, a commonly-used sparsity restriction for large covariance matrix estimation. The second condition in (\ref{eq3.14}) can be easily verified when $\varepsilon_{i,t}$ is independent over $i$ \citep[e.g.,][]{ZPLLW17}.

\begin{theorem}\label{thm:3.3}

Suppose that Assumptions \ref{ass:1}--\ref{ass:3} are satisfied. Then we have
\begin{equation}\label{eq3.15}
{\sf P}\left(\widehat K=K_0\right)\rightarrow1.
\end{equation}

\end{theorem}

Finally, we use the estimated group number $\widehat K$ and terminate the clustering algorithm in Section \ref{sec3.1} when the cluster number reaches $\widehat K$. In order to avoid unnecessary notational burden, we still denote the estimated groups as $\widehat{\mathscr G}_k$, $k=1, \cdots,\widehat{K}$.


%%%%%%%%%%%%%%%%%%%


\section{Post-grouping estimation and inference}\label{sec4} 
\renewcommand{\theequation}{4.\arabic{equation}} \setcounter{equation}{0}

In this section we make use of the consistently estimated group structure to more efficiently estimate the group-specific time-varying network and momentum effects in (\ref{eq2.6}) and then conduct statistical inference. 

\subsection{Post-grouping local linear estimation}\label{sec4.1}

Note that the heterogeneous local linear estimates defined in (\ref{eq3.3}) only use the sample information from one node and its direct neighbors, resulting in rather slow convergence rates (see Proposition \ref{prop:3.1} and Lemma \ref{lem:B.3}) and unstable numerical performance if $T$ is not sufficiently large in finite samples. It follows from Theorems \ref{thm:3.2} and \ref{thm:3.3} that, for any $k=1,\cdots,K_0$, there exists $1\leq k_\circ\leq \widehat{K}$ such that ${\mathscr G}_k=\widehat{\mathscr G}_{k_\circ}$ {\em w.p.a.1}. Hence, without loss of generality, we may consider ${\mathscr G}_k=\widehat{\mathscr G}_{k}$ (conditioning on $\widehat{K}=K_0$) throughout this section. For each $k$ and given $\tau\in(0,1)$, similarly to (\ref{eq3.9}), we define the post-grouping local linear objective function: 
\begin{equation}\label{eq4.1}
\frac{1}{T|\widehat{\mathscr G}_k|}\sum_{i\in\widehat{\mathscr G}_k}\sum_{t=1}^T\left[x_{i,t}-a^{^\intercal}\widetilde{X}_{i,t-1}-b^{^\intercal}\widetilde{X}_{i,t-1}(\tau_t-\tau)\right]^2K_{h_2}(\tau_t-\tau),
\end{equation}
where $h_2$ is a bandwidth which may be different from $h$ used in the heterogeneous local linear estimation (\ref{eq3.2}) and (\ref{eq3.3}). Minimizing the post-grouping objective function with respect to the vectors $a$ and $b$, we obtain the solutions denoted by $\check a$ and $\check b$, and define the post-grouping local linear estimates as 
\begin{equation}\label{eq4.2}
\widehat\alpha_{k}(\tau)=\left[\widehat\alpha_{k,1}(\tau), \widehat\alpha_{k,2}(\tau)\right]^{^\intercal}=\check a.
\end{equation}


\renewcommand{\thetheorem}{4.\arabic{theorem}}\setcounter{theorem}{0}

\begin{theorem}\label{thm:4.1}

Suppose that Assumptions \ref{ass:1}--\ref{ass:3} are satisfied, $h_2\to0$, $NTh_2\to\infty$ and there exists a positive definite matrix ${\boldsymbol\Upsilon}_{{\mathscr G}_k}(\tau)$ such that
\begin{equation}\label{eq4.3}
\frac{1}{\vert{\mathscr G}_k\vert}\sum_{i,j\in{\mathscr G}_k}\sigma_{ij}{\boldsymbol\Delta}_{ij}(\tau)\rightarrow {\boldsymbol\Upsilon}_{{\mathscr G}_k}(\tau)
\end{equation}
as $\vert{\mathscr G}_k\vert\rightarrow\infty$, where $\sigma_{ij}$ and ${\boldsymbol\Delta}_{ij}(\tau)$ are defined in Section 3.1.1. For any $\tau\in\left(0,1\right)$,
\begin{equation}\label{eq4.4}
\sqrt{\vert{\mathscr G}_k\vert Th_2}\left[\widehat{\alpha}_k(\tau)-\alpha_k(\tau)-\frac{1}{2}h_2^2\mu_2\alpha_k^{\prime\prime}(\tau)\right]\stackrel{d}\longrightarrow {\sf N}\left(\boldsymbol{0},{\boldsymbol\Omega}_{{\mathscr G}_k}(\tau)\right)
\end{equation}
where
\[
{\boldsymbol\Omega}_{{\mathscr G}_k}(\tau)=\nu_0{\boldsymbol\Delta}_{{\mathscr G}_k}^{-1}(\tau){\boldsymbol\Upsilon}_{{\mathscr G}_k}(\tau){\boldsymbol\Delta}_{{\mathscr G}_k}^{-1}(\tau)
\]
with
\[
\boldsymbol{\Delta}_{{\mathscr G}_k}(\tau)=\frac{1}{\vert{\mathscr G}_k\vert}\sum_{i\in{\mathscr G}_k}{\boldsymbol\Delta}_i(\tau)=\frac{1}{\vert{\mathscr G}_k\vert}\sum_{i\in{\mathscr G}_k} {\sf E}\left[\widetilde{X}_{i,t}(\tau)\widetilde{X}_{i,t}^{^\intercal}(\tau)\right].
\]

\end{theorem}

Since $\vert{\mathscr G}_k\vert$ is of order $N$, Theorem \ref{thm:4.1} shows that the post-grouping local linear estimation $\widehat{\alpha}_k(\tau)$ has the point-wise convergence rate of $O_P\left(1/\sqrt{NTh_2}+h_2^2\right)$, which is substantially faster than that for the preliminary local linear estimates (see Lemma \ref{lem:B.3}). This is unsurprising since more sample information is used in the post-grouping estimation procedure.

\subsection{Post-grouping specification testing}\label{sec4.2}

In practice, one is often interested in testing the hypothesis that some group-specific time-varying coefficient functions belong to a given parametric function family, i.e., for $k=1,\cdots,K_0$,
\begin{equation}\label{eq4.5}
H_0:\ \alpha_k(\tau)=g_k(\tau; \theta_k),\ \ \tau\in[0,1],
\end{equation}
where $\theta_k$ is a $d_k$-dimensional vector of unknown parameters which lies in the interior of a compact set $\Theta$, and $g_k(\cdot;\cdot)$ is a pre-specified function. A typical example of (\ref{eq4.5}) is to test whether the time-varying coefficient functions are constants, i.e., for $k=1,\cdots,K_0$,
\begin{equation}\label{eq4.6}
H_0:\ \alpha_k(\tau)=\alpha_k,\ \ \tau\in[0,1],
\end{equation}
where $\alpha_k$ is a constant vector. Under the null hypothesis in (\ref{eq4.6}), model (\ref{eq2.6}) reduces to the grouped parametric network VAR model \citep[e.g.,][]{ZP20,CFZ22, ZXF22}. The statistical inference of the time-varying coefficient functions or trending functions have been extensively studied for either univariate or fixed dimensional time series \citep[e.g.,][]{WZ07, LCL11, ZW11, ZW12, DXZW12}. \cite{CW19} consider testing the trend functions in high-dimensional time series. We next make a further extension by developing a testing tool under a more general network VAR model framework.

\smallskip

Without loss of generality, we assume that ${\mathscr G}_k=\widehat{\mathscr G}_{k}$ (conditioning on $\widehat{K}=K_0$) as in the post-grouping local linear estimation. For $i\in\widehat{\mathscr G}_k$, we let $
\widetilde\varepsilon_{i,t}=x_{i,t}-g_k^{^\intercal}(\tau_t;\widetilde\theta_k)\widetilde X_{i,t-1}$, where $\widetilde\theta_k$ is the parametric estimate of $\theta_k$. Partly motivated by \cite{CW19}, we construct the test statistic via the following kernel weighted quadratic form:
\begin{equation}\label{eq4.10}
\widetilde{\cal Q}_T(k)=\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\widetilde\varepsilon_t^{^\intercal}(k)\widetilde\varepsilon_s(k),
\end{equation}
where $\widetilde\varepsilon_t(k)=(\widetilde{\varepsilon}_{i,t},\ i\in\widehat{\mathscr G}_k)$ and $h_3$ is a bandwidth. Here we simply use the kernel weight rather than a more complex local linear weight \citep[e.g.,][]{CW19}, avoiding possible random denominators in the test statistic. Similar test statistics have been proposed in the model specification testing of univariate or multiple time series \citep[e.g.,][]{LW98, L99, GG08, GKLT09} and high-dimensional time series \citep[e.g.,][]{BS96, CW19}. 

\begin{assumption}\label{ass:4}

{\em (i) There exist two positive constants $c_g^\dagger$ and $c_g^\ddagger$ such that
\[
\max_{1\le k\le K_0}\sup_{\tau\in[0,1]}\vert\partial_{\theta}g_k(\tau;\theta_k)\vert_2<c_g^\dagger,
\]
and
\[
\max_{1\le k\le K_0}\sup_{\tau\in[0,1]}\vert g_k(\tau;\theta)-g_k(\tau;\theta_k)-\partial_{\theta}g_k(\tau;\theta_k)(\theta-\theta_k)\vert_\infty\le c_g^\ddagger\vert\theta-\theta_k\vert^2.
\]
 }
 
 {\em (ii) Letting $\delta_\theta=\vert\widetilde{\theta}_k-\theta_k\vert$ and $\boldsymbol{\Sigma}_{\varepsilon}(k)={\sf E}\left[\varepsilon_t(k)\varepsilon_t^{^\intercal}(k)\right]$ with $\varepsilon_t(k)=(\varepsilon_{i,t},\ i\in{\mathscr G}_k)$, assume
 \[\frac{\delta_\theta^2NT}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\left(h_3^{1/2}+\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert^{-1}\right)\to0.\]
}

{\em (iii) Let $h_3\to0$ and $\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}\to\infty$. } 

\end{assumption}

The smoothness conditions on $g_k(\tau;\cdot)$ in Assumption \ref{ass:4}(i) are not uncommon in the literature \citep[e.g.,][]{GG08, CW19}. If $\delta_\theta$ has the root-$NT$ rate, $\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert$ is of the order $N^{1/2}$ (which is satisfied by imposing a sparsity restriction), $h_3\rightarrow0$ and $Th_3\rightarrow\infty$, it is easy to verify Assumption \ref{ass:4}(ii)(iii).

\begin{theorem}\label{thm:4.2}

Suppose that Assumptions \ref{ass:1}--\ref{ass:4} hold. Under $H_0$, 
\begin{equation}\label{eq4.8}
\frac{1}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}}\widetilde{\cal Q}_T(k)\stackrel{d}\longrightarrow{\sf N}\left(0,1\right).
\end{equation}

\end{theorem}

The normalization rate in Theorem \ref{thm:4.2} depends on $T$, $h_3$ and $N$ (implicitly via $\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert$). In particular, if $\boldsymbol{\Sigma}_{\varepsilon}(k)$ is sparse (see Assumption \ref{ass:3}(iii)), we may show that $\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert\propto N^{1/2}$ and the normalization rate in (\ref{eq4.8}) is $N^{1/2}Th_3^{1/2}$.

We next study the asymptotic power of the test under the alternative hypothesis. Denote 
\[\theta_k^\diamond=\argmin_{\theta_k\in\Theta}\frac{1}{\vert{\mathscr G}_k\vert}\sum_{i\in{\mathscr G}_k}\int_0^1\left\{\left[\alpha_k(\tau)-g_k(\tau;\theta_k)\right]^{^\intercal}\widetilde{X}_{i,t}(\tau)\right\}^2d\tau,\] 
where $\Theta$ is a compact set. Consider the alternative hypothesis as
\begin{equation}\label{eq4.12}
H_A: \alpha_k(\tau)=g_k(\tau;\theta_k^\diamond)+\varphi g_k(\tau),
\end{equation}
where $\varphi>0$ but may tend to zero at an appropriate rate, and $g_k(\cdot)$ is continuous over $[0,1]$. The following conditions are required to derive the asymptotic property of the test under $H_A$.


\begin{assumption}\label{ass:5}

{\em (i) There exist two positive constants $c_{g,\diamond}^\dagger$ and $c_{g,\diamond}^\ddagger$ such that
\[\max_{1\le k\le K_0}\sup_{\tau\in[0,1]}\vert\partial_{\theta}g_k(\tau;\theta_k^\diamond)\vert_2\le c_{g,\diamond}^\dagger\] 
and 
\[\max_{1\le k\le K_0}\sup_{\tau\in[0,1]}\vert g_k(\tau;\theta)-g_k(\tau;\theta_k^\diamond)-\partial_{\theta}g_k(\tau;\theta_k^\diamond)(\theta-\theta_k^\diamond)\vert_\infty\le c_{g,\diamond}^\ddagger\vert\theta-\theta_k^\diamond\vert^2.\]
 }
 
{\em (ii) Assumption \ref{ass:4}(ii) when $\delta_\theta$ is replaced by $\delta_\theta^\diamond=\vert\widetilde{\theta}_k-\theta_k^\diamond\vert$. There exists $\phi_k$ such that
 \begin{equation}\label{eq4.10}
\frac{1}{|{\mathscr G}_k|T^2h_3}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varpi_{{\mathscr G}_k,t}^{^\intercal}(g)\varpi_{{\mathscr G}_k,s}(g)\stackrel{P}\longrightarrow\phi_k,
\end{equation}
where $\varpi_{{\mathscr G}_k,t}(g)=\left\{g_k^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}, i\in{\mathscr G}_k\right\}$. }

{\em (iii) Let $\delta_\theta^\diamond=o(\varphi)$ and $\varphi^2 NTh_3\rightarrow\infty$.
}

\end{assumption}

The following theorem establishes the asymptotic distribution of $\widetilde{\cal Q}_T(k)$ under the local alternative hypothesis $H_A$ with $\varphi$ possibly converging to zero.


\begin{theorem}\label{thm:4.3}

Suppose that Assumptions \ref{ass:1}--\ref{ass:3}, \ref{ass:4}(iii) and \ref{ass:5} hold. Under $H_A$, (i) if $\frac{\varphi^2\phi_k |{\mathscr G}_k|Th_3^{1/2}}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\to\kappa$, 
\begin{equation}\label{eq4.11}
\frac{1}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}}\widetilde{\cal Q}_T(k)\stackrel{d}\longrightarrow{\sf N}\left(\kappa,1\right);
\end{equation}
and (ii) if $\frac{\varphi^2\phi_k |{\mathscr G}_k|Th_3^{1/2}}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\to\infty$, 
\begin{equation}\label{eq4.12}
\frac{1}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}}\widetilde{\cal Q}_T(k)\stackrel{P}\longrightarrow\infty.
\end{equation}
\end{theorem}

The above asymptotic distribution theory under the local alternative is comparable to Proposition 2 in \cite{CW19}. Let 
\[{\cal Q}_T(k)=\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)\varepsilon_s(k).\]
In fact, following the proofs of Theorems \ref{thm:4.2} and \ref{thm:4.3} in Appendix A, we may show that 
\[
\frac{1}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}}\widetilde{\cal Q}_T(k)= \frac{1}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}}\left[{\cal Q}_T(k)+\varphi^2\phi_k\cdot|{\mathscr G}_k|T^2h_3(1+o_P(1))\right]
\]
under $H_A$, where 
\[\frac{1}{(2\nu_0)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert Th_3^{1/2}}{\cal Q}_T(k)\stackrel{d}\longrightarrow {\sf N}(0,1).\] Hence, Theorem \ref{thm:4.2} can be viewed as a special case of Theorem \ref{thm:4.3}(i). Theorem \ref{thm:4.3}(ii) indicates that the null hypothesis is rejected {\em w.p.a.1} when $\frac{\varphi^2\phi_k |{\mathscr G}_k|Th_3^{1/2}}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\to\infty$.


%%%%%%%%%%%%%%%%%%%


\section{Numerical studies}\label{sec5} 
\renewcommand{\theequation}{5.\arabic{equation}} \setcounter{equation}{0}

In this section, we conduct both the simulation and empirical studies. Section \ref{sec5.1} assesses the finite-sample performance of the developed methodology and verifies the main theoretical properties via the Monte-Carlo simulation, whereas Section \ref{sec5.2} applies the model and methodology to the export flow data collected in 90 counties. Throughout this section, we use the Epanechnikov kernel as $K(\cdot)$ in the estimation and testing so that Assumption \ref{ass:2}(i) is satisfied.

\subsection{Simulation}\label{sec5.1} 

We consider the following data generating process (DGP):  
\begin{eqnarray}\label{Eq5.1}
x_{i,t}=\beta_{i,1}\left(\tau_t\right) \sum_{j\neq i}\widetilde{w}_{ij}x_{j,t-1}+\beta_{i,2}\left(\tau_t\right)x_{i,t-1}+\varepsilon_{i,t},\ \ i=1,\cdots,N,\ \ t=1,\cdots,T,
\end{eqnarray}
in which $\beta_{i}(\cdot)=[\beta_{i,1}(\cdot), \beta_{i, 2}(\cdot)]^{^\intercal}$ satisfies the following latent group structure:
\begin{eqnarray}
\beta_{i}(\tau)=\alpha_1 (\tau) &=&[\alpha_{1,1} (\tau) ,\alpha_{1,2}(\tau) ]^{^\intercal}=[-0.95\sin (\pi \tau), -0.6\cos (\pi \tau) -0.3]^{^\intercal},\ \ \ i\in{\mathscr G}_1,\nonumber \\
\beta_i(\tau)=\alpha_2 (\tau) &=&[\alpha_{2,1} (\tau) ,\alpha_{2,2}(\tau) ]^{^\intercal}=[-0.7\sin (\pi  \tau)+0.8, -0.5\cos (\pi \tau)+0.45 ]^{^\intercal} ,\ \ \ i\in{\mathscr G}_2,\nonumber \\
\beta_i(\tau)=\alpha_3 (\tau) &=&[\alpha_{3,1} (\tau) ,\alpha_{3,2}(\tau) ]^{^\intercal}=[ \sin (\pi  \tau)-0.2, 0.9\cos (\pi \tau) ]^{^\intercal},\ \ \ i\in{\mathscr G}_3.\nonumber
\end{eqnarray}
Clearly the number of groups is $K_0=3$. The group membership ${\mathscr G}=\left\{{\mathscr G}_1,{\mathscr G}_2, {\mathscr G}_3\right\}$ is generated by either of the following two partitions.

\begin{itemize}

\item Random group structure:\ Randomly assign node $i$ to one of the three groups (with equal probability) in each simulation replication.

\item Fixed group structure:\ Assign node $i$ to the three groups such that ${\sf P}(i\in \mathscr{G}_1) = 0.25$, ${\sf P}(i\in \mathscr{G}_2) = 0.25$ and ${\sf P}(i\in \mathscr{G}_3) = 0.5$, which is only done once and then fixed over replications.

\end{itemize}
Let $\widetilde{w}_{ij}=w_{ij}/n_i$ with $n_i=\sum_{j\neq i} w_{ij}$, where ${\sf P}(w_{ij} =1)=0.1$ and ${\sf P}(w_{ij} =0)=0.9$. The errors $\varepsilon_t=(\varepsilon_{1,t},\cdots,\varepsilon_{N,t})^{^\intercal}$ are independently generated from ${\sf N}({\bf 0}, {\boldsymbol\Sigma}_\varepsilon)$, where ${\boldsymbol\Sigma}_\varepsilon =\{\sigma_{ij} \}_{N\times N}$ with $\sigma_{ij}=0.3^{|i-j|}$. 

\medskip

\noindent{\em 5.1.1.\ \ Latent group estimation}

\smallskip

\noindent We first examine the numerical performance of the clustering algorithm and information criterion in finite samples. Consider $N =100, 200$, $T=200, 300, 400$, and the replication number $R=400$. In the local linear estimation procedure, there are three bandwidths involved: $h$ in the preliminary estimation, and $h_1$ and $h_2$ in the pooled and post-grouping estimation. To reduce the computational burden, we adopt the rule of thumb, setting the bandwidths as $h =2.34\sigma T^{-1/5}$ and $h_1=h_2 =2.34\sigma(NT)^{-1/5}$, where the constant $2.34$ is due to use of the Epanechnikov kernel in local linear smoothing and $\sigma =1/\sqrt{12}$ is due to the fact that the scaled time point sequence $\{ \tau_t\}$ behaves like the observations following a uniform distribution on $[0,1]$. The tuning parameter in the information criterion is set as $\rho_{NT}=\frac{\log (N\vee T)}{NTh_1}$ as discussed in Section \ref{sec3.2}.

To assess the performance of the information of the proposed information criterion, we compute the empirical probabilities of correct-, over- and under-estimation of the group number, i.e., 
\[
\sf{EP}_{\widehat{K}=K_0} = \frac{1}{R}\sum_{r=1}^R I\left\{\widehat{K}_r =K_0\right\},\quad \sf{EP}_{\widehat{K}>K_0} = \frac{1}{R}\sum_{r=1}^R I\left\{\widehat{K}_r >K_0\right\},\quad \sf{EP}_{\widehat{K}<K_0} = \frac{1}{R}\sum_{r=1}^R I\left\{\widehat{K}_r <K_0\right\},
\]
where $\widehat{K}_r$ is the group number estimate in the $r$-th replication, and $I\{\cdot\}$ denotes the indicator function. We use the so-called purity measurement to examine the accuracy of the group membership estimate, i.e.,
\[
\sf{Purity}=\frac{1}{R}\sum_{r=1}^R P_r, \quad P_r = \frac{1}{N} \sum_{k=1}^{\widehat{K}_r} \max_{1\le j\le K_0} \left|\widehat{\mathscr{G}}_{k, r} \cap \mathscr{G}_j \right|,
\]
where $\widehat{\mathscr{G}}_{k, r}$ denotes the group member estimate in the $r$-th replication. 

\begin{table}[h]
\centering
\caption{Estimation of the group number and membership}\label{tab:1}
\begin{tabular}{llrrrrrr}
\hline\hline
 &  & \multicolumn{3}{c}{Random groups} & \multicolumn{3}{c}{Fixed groups} \\
 & $N\setminus T$ & 200 & 300 & 400 & 200 & 300 & 400 \\
$\sf{EP}_{\widehat{K}=K_0}$ & 100 & 1.000 & 0.985 & \multicolumn{1}{r|}{0.950} & 1.000 & 0.995 & 1.000 \\
 & 200 & 0.725 & 0.995 & \multicolumn{1}{r|}{1.000} & 0.815 & 0.995 & 1.000 \\
 &  &  &  &  &  &  &  \\
$\sf{EP}_{\widehat{K}>K_0}$ & 100 & 0.000 & 0.015 & \multicolumn{1}{r|}{0.050} & 0.000 & 0.005 & 0.000 \\
 & 200 & 0.260 & 0.005 & \multicolumn{1}{r|}{0.000} & 0.160 & 0.005 & 0.000 \\
 &  &  &  &  &  &  &  \\
$\sf{EP}_{\widehat{K}<K_0}$ & 100 & 0.000 & 0.000 & \multicolumn{1}{r|}{0.000} & 0.000 & 0.000 & 0.000 \\
 & 200 & 0.015 & 0.000 & \multicolumn{1}{r|}{0.000} & 0.025 & 0.000 & 0.000 \\
 &  &  &  &  &  &  &  \\
\sf{Purity} & 100 & 1.000 & 1.000 & \multicolumn{1}{r|}{0.996} & 1.000 & 1.000 & 1.000 \\
 & 200 & 0.986 & 1.000 & \multicolumn{1}{r|}{1.000} & 0.979 & 1.000 & 1.000 \\
 \hline\hline
\end{tabular}
\end{table}

Table \ref{tab:1} presents the empirical probabilities and purity values over $400$ replications. In general, the information criterion can accurately estimate the group number as long as the time series length is moderately large. The performance of the group number estimation improves, i.e., $\sf{EP}_{\widehat{K}=K_0}$ increases whereas $\sf{EP}_{\widehat{K}>K_0}$ and $\sf{EP}_{\widehat{K}<K_0}$ generally decreases, as $T$ increases, but slightly deteriorates as $N$ increases from $100$ to $200$. When $N=200$ and $T=200$ or $300$, if $\widehat{K}\neq K_0$, the information criterion tends to over-estimate the group number, which is not as harmful as under-estimation of the group number. Note that the closer the values of purity are to 1, the more accurate the estimated groups are to the true ones. The obtained purity values are all larger than $0.975$, indicating that only a small proportion of nodes are misclassified and verifying the consistency property of the clustering algorithm in Theorem \ref{thm:3.2}. The estimation performance for the random group structure is similar to that for the fixed group structure.

\medskip

\noindent{\em 5.1.2.\ \ Time-varying coefficient estimation}

\smallskip

\noindent We next compare the numerical performance between the preliminary heterogeneous local linear estimation and post-grouping estimation and show the advantage of making use of the group structure to achieve dimension reduction and improve estimation efficiency. Define
\[
 {\sf RMSE}_{r}= \sqrt{\frac{1}{NT}  \sum_{i=1}^N \sum_{t=1}^T\left\Vert \widehat{\beta}_i^r(\tau_t)-\beta_i^r(\tau_t) \right\Vert^2}, 
\]
for each replication, where $\widehat{\beta}_i^r(\cdot)$ denotes the pre-grouping estimate of $\beta_i(\cdot)$. We further define ${\sf RMSE}_{{\rm pre}}$ as the average of ${\sf RMSE}_{r}$ over 400 replications. Let ${\sf RMSE}_{{\rm post}}$ and ${\sf RMSE}_{{\rm ora}}$ be defined similarly using the post-grouping local linear estimation, and using the oracle local linear estimation with the true group structure, respectively. The computed {\sf RMSE} values are reported in Table \ref{tab:2}, where numbers in the parentheses are the standard errors. As we can see from Table \ref{tab:2}, the ${\sf RMSE}_{{\rm post}}$ values are much smaller than the ${\sf RMSE}_{{\rm pre}}$ values, confirming the convergence improvement by the post-grouping estimation and verifying Theorem \ref{thm:4.1} and the subsequent discussion. In particular, the improvement becomes more significant when $N$ increases from $100$ to $200$. Meanwhile, the performance of the post-grouping estimation is similar to that of the oracle estimation which is infeasible in practice, and the difference between ${\sf RMSE}_{{\rm post}}$ and  ${\sf RMSE}_{{\rm ora}}$ is nearly invisible when $T=300$ or $400$.


\begin{table}[h]
\centering
\caption{Estimation performance of the time-varying coefficients}\label{tab:2}
\begin{tabular}{llcccccc}
 \hline\hline
  &  & \multicolumn{3}{c}{Random groups} & \multicolumn{3}{c}{Fixed groups} \\
 & $N\setminus T$ & 200 & 300 & 400 & 200 & 300 & 400 \\
${\sf RMSE}_{{\rm pre}}$ & 100 & 0.061 & 0.050 &  \multicolumn{1}{r|}{0.048} & 0.066 & 0.052 & 0.030 \\
 &  & (0.058) & (0.031) &  \multicolumn{1}{r|}{(0.052)} & (0.006) & (0.004) & (0.002) \\
 & 200 & 0.116 & 0.074 &  \multicolumn{1}{r|}{0.055} & 0.123 & 0.076 & 0.055 \\
 &  & (0.009) & (0.005) &  \multicolumn{1}{r|}{(0.003)} & (0.010) & (0.005) & (0.003) \\
${\sf RMSE}_{{\rm post}}$ & 100 & 0.048 & 0.041 &  \multicolumn{1}{r|}{0.043} & 0.049 & 0.040 & 0.036 \\
 &  & (0.007) & (0.011) & \multicolumn{1}{r|}{ (0.044)} & (0.007) & (0.006) & (0.005) \\
 & 200 & 0.068 & 0.043 &  \multicolumn{1}{r|}{0.039} & 0.064 & 0.043 & 0.038 \\
 &  & (0.058) & (0.007) & \multicolumn{1}{r|}{(0.006)} &  (0.005) & (0.006) & (0.005) \\
${\sf RMSE}_{{\rm ora}}$ & 100 & 0.048 & 0.040 &  \multicolumn{1}{r|}{0.038} & 0.049 & 0.040 & 0.036 \\
 &  & (0.007) & (0.006) &  \multicolumn{1}{r|}{(0.007)} & (0.007) & (0.006) & (0.005) \\
 & 200 & 0.052 & 0.043 &  \multicolumn{1}{r|}{0.039} & 0.051 & 0.043 & 0.038 \\
 &  & (0.008) & (0.006) &  \multicolumn{1}{r|}{(0.006)} & (0.007) & (0.006) & (0.005)  \\
  \hline\hline
\end{tabular}
\end{table}

\medskip

\noindent{\em 5.1.3.\ \ Post-grouping inference}

\smallskip

\noindent We next assess the size and power performance of the proposed kernel-weighted test statistic in finite samples. The DGP is similar to that used for estimation assessment, i.e., consider (\ref{Eq5.1}) but with the group-specific coefficients defined as 
\begin{eqnarray}
\alpha_1(\tau)  &=& [-0.7, -0.6 ]^{^\intercal},\nonumber \\
\alpha_2 (\tau) &=&[\alpha_{1,1} (\tau) ,\alpha_{1,2}(\tau) ]^{^\intercal}=[-0.7\sin (\pi \tau)+0.8,-0.5\cos (\pi \tau)+0.45 ]^{^\intercal},\nonumber \\
\alpha_3 (\tau) &=&[\alpha_{3,1} (\tau) ,\alpha_{3,2}(\tau) ]^{^\intercal}=[ \sin (\pi \tau)-0.2,0.9\cos (\pi \tau) ]^{^\intercal}.\nonumber
\end{eqnarray}
We aim to test the null hypothesis $H_0:\ \alpha_k(\tau)=\alpha_k,\ \ \tau\in[0,1]$ for $k=1,2,3$, where $\alpha_k$ is a bivariate vector of constants. It is clear that $H_0$ is only valid for $k=1$. Hence, the network time series sample in ${\mathscr G}_1$ is used to assess the finite-sample size performance, whereas those in ${\mathscr G}_2$ and ${\mathscr G}_3$ are used to assess the power performance.

Since the optimal bandwidth is of order $T^{-2/5}$ under the null (e.g., \citealp{Casas2021}), we adopt the rule of thumb for each group ${\mathscr G}_k$ by setting  $h_{3}=2.34\widetilde{\sigma}_{\varepsilon,k} T^{-2/5}$, where $\widetilde{\sigma}_{\varepsilon,k}^2 =\frac{1}{| {\mathscr G}_k | (T-1)}\sum_{t=2}^T\left\Vert\widetilde{\varepsilon}_t(k)\right\Vert^2$. We use the asymptotic null distribution in Theorem \ref{thm:4.2} to determine the test critical value. Conditioning on $\widehat{K}=K_0$, we compute the test rejection rates (at the significance level $\alpha$), which is defined as
\[
{\sf TRR}_{\widehat{K}=K_0} = \frac{1}{R_\ast}\sum_{r=1}^{R_\ast}  I\left(|\widehat{\cal Q}_T^r(k) |> z_{1-\alpha/2}\right),\quad\widehat{\cal Q}_T^r(k) = \frac{1}{\Vert \widetilde{\boldsymbol{\Sigma}}_{\varepsilon}^r(k)\Vert (2\nu_0T^2h_3)^{1/2}}\widetilde{\cal Q}_T^r(k)
\]
where $R_\ast$ denotes the number of replications when $\widehat{K}=K_0$, $z_{\alpha}$ is the $\alpha$-quantile of ${\sf N}(0,1)$ and $\widetilde{\boldsymbol{\Sigma}}_{\varepsilon}^r(k)$ is the sample version of $\boldsymbol{\Sigma}_{\varepsilon}(k)$  in the $r$-th replication (e.g., \citealp{CW19}). We also consider the oracle kernel-weighted test directly using the true group structure and compute
\[
{\sf TRR}_{\rm ora}= \frac{1}{R}\sum_{r=1}^R  I\left(|\widehat{\cal Q}_{\rm ora}^r(k) |> z_{1-\alpha/2}\right),
\]
where $\widehat{\cal Q}_{\rm ora}^r(k)$ is defined similarly to $\widetilde{\cal Q}_T^r(k)$, using the latent group structure when constructing $\widetilde{\cal Q}_T^r(k)$ and $\widetilde{\boldsymbol{\Sigma}}_{\varepsilon}^r(k)$. The finite-sample test results are summarized in Table \ref{tab:3}, where we consider $\alpha =1\%, 5\%$ and $10\%$. For ${\mathscr G}_1$, both ${\sf TRR}_{\widehat{K}=K_0}$ and ${\sf TRR}_{\rm ora}$ are generally close to the nominal rates, indicating that it is reasonable to choose the test critical value via the asymptotic null distribution. The test results for ${\mathscr G}_2$ and ${\mathscr G}_3$ show that the proposed test statistic has reliable power performance. In general, the ${\sf TRR}_{\widehat{K}=K_0}$ values are very close to the ${\sf TRR}_{\rm ora}$ values.



\begin{table}[hbt!]\centering
\caption{Rejection rates of the post-grouping tests under the levels $1\%$, $5\%$ and $10\%$ }\label{tab:3}
\begin{tabular}{llrrrrrrrrr}
  \hline\hline
 &  & \multicolumn{3}{c}{$\alpha=1\%$} & \multicolumn{3}{c}{$\alpha=5\%$} & \multicolumn{3}{c}{$\alpha=10\%$} \\
 & N/T & 200 & 300 & 400 & 200 & 300 & 400 & 200 & 300 & 400 \\ \\
  &  & \multicolumn{9}{c}{${\sf TRR}_{\widehat{K}=K_0}$  --- Random groups } \\
Group1 & 100 & 0.010 & 0.013 & \multicolumn{1}{r|}{0.014} & 0.040 & 0.043 & \multicolumn{1}{r|}{0.060} & 0.105 & 0.083 & 0.090 \\
 & 200 & 0.016 & 0.008 & \multicolumn{1}{r|}{0.008} & 0.056 & 0.045 & \multicolumn{1}{r|}{0.053} & 0.107 & 0.088 & 0.105 \\
Group2 & 100 & 0.341 & 0.806 & \multicolumn{1}{r|}{0.932} & 0.466 & 0.856 & \multicolumn{1}{r|}{0.943} & 0.554 & 0.881 & 0.957 \\
 & 200 & 0.273 & 0.499 & \multicolumn{1}{r|}{0.835} & 0.365 & 0.579 & \multicolumn{1}{r|}{0.868} & 0.429 & 0.642 & 0.893 \\
Group3 & 100 & 1.000 & 0.843 & \multicolumn{1}{r|}{0.546} & 1.000 & 0.874 & \multicolumn{1}{r|}{0.709} & 1.000 & 0.907 & 0.785 \\
 & 200 & 1.000 & 0.995 & \multicolumn{1}{r|}{0.895} & 1.000 & 0.995 & \multicolumn{1}{r|}{0.933} & 1.000 & 0.997 & 0.950 \\ \\
   &  & \multicolumn{9}{c}{${\sf TRR}_{\rm ora}$ --- Random groups } \\ 
 Group1 & 100 & 0.010 & 0.013 & \multicolumn{1}{r|}{0.013} & 0.040 & 0.043 & \multicolumn{1}{r|}{0.060} & 0.105 & 0.083 & 0.093 \\
 & 200 & 0.018 & 0.007 & \multicolumn{1}{r|}{0.008} & 0.058 & 0.045 & \multicolumn{1}{r|}{0.053} & 0.105 & 0.090 & 0.105 \\
Group2 & 100 & 0.343 & 0.808 & \multicolumn{1}{r|}{0.938} & 0.468 & 0.858 & \multicolumn{1}{r|}{0.948} & 0.555 & 0.883 & 0.960 \\
 & 200 & 0.283 & 0.498 & \multicolumn{1}{r|}{0.835} & 0.373 & 0.578 & \multicolumn{1}{r|}{0.868} & 0.435 & 0.640 & 0.893 \\
Group3 & 100 & 1.000 & 0.840 & \multicolumn{1}{r|}{0.528} & 1.000 & 0.873 & \multicolumn{1}{r|}{0.695} & 1.000 & 0.905 & 0.778 \\
 & 200 & 1.000 & 0.995 & \multicolumn{1}{r|}{0.895} & 1.000 & 0.995 & \multicolumn{1}{r|}{0.933} & 1.000 & 0.998 & 0.950 \\ \\
  &  & \multicolumn{9}{c}{${\sf TRR}_{\widehat{K}=K_0}$ --- Fixed groups} \\
Group1 & 100 & 0.025 & 0.020 & \multicolumn{1}{r|}{0.008} & 0.068 & 0.048 & \multicolumn{1}{r|}{0.063} & 0.111 & 0.083 & 0.106 \\
 & 200 & 0.011 & 0.008 & \multicolumn{1}{r|}{0.010} & 0.059 & 0.055 & \multicolumn{1}{r|}{0.040} & 0.111 & 0.113 & 0.088 \\
Group2 & 100 & 0.158 & 0.256 & \multicolumn{1}{r|}{0.808} & 0.296 & 0.421 & \multicolumn{1}{r|}{0.856} & 0.392 & 0.524 & 0.881 \\
 & 200 & 0.376 & 0.649 & \multicolumn{1}{r|}{0.735} & 0.469 & 0.711 & \multicolumn{1}{r|}{0.803} & 0.539 & 0.743 & 0.830 \\
Group3 & 100 & 1.000 & 1.000 & \multicolumn{1}{r|}{0.924} & 1.000 & 1.000 & \multicolumn{1}{r|}{0.952} & 1.000 & 1.000 & 0.957 \\
 & 200 & 1.000 & 1.000 & \multicolumn{1}{r|}{0.998} & 1.000 & 1.000 & \multicolumn{1}{r|}{0.998} & 1.000 & 1.000 & 0.998\\  \\
 &  & \multicolumn{9}{c}{${\sf TRR}_{\rm ora}$ --- Fixed groups} \\ 
 Group1 & 100 & 0.025 & 0.020 & \multicolumn{1}{r|}{0.008} & 0.068 & 0.048 & \multicolumn{1}{r|}{0.055} & 0.110 & 0.083 & 0.108 \\
 & 200 & 0.010 & 0.008 & \multicolumn{1}{r|}{0.010} & 0.060 & 0.055 & \multicolumn{1}{r|}{0.040} & 0.103 & 0.113 & 0.088 \\
Group2 & 100 & 0.160 & 0.255 & \multicolumn{1}{r|}{0.808} & 0.298 & 0.420 & \multicolumn{1}{r|}{0.855} & 0.395 & 0.523 & 0.880 \\
 & 200 & 0.375 & 0.665 & \multicolumn{1}{r|}{0.735} & 0.468 & 0.725 & \multicolumn{1}{r|}{0.803} & 0.538 & 0.753 & 0.830 \\
Group3 & 100 & 1.000 & 1.000 & \multicolumn{1}{r|}{0.925} & 1.000 & 1.000 & \multicolumn{1}{r|}{0.953} & 1.000 & 1.000 & 0.958 \\
 & 200 & 1.000 & 1.000 & \multicolumn{1}{r|}{1.000} & 1.000 & 1.000 & \multicolumn{1}{r|}{0.998} & 1.000 & 1.000 & 0.998 \\
 \hline\hline
\end{tabular}
\end{table}



\subsection{An Empirical Study}\label{sec5.2} 

Most countries are now connected to each other through varieties of bilateral trade. In order to provide a detailed insight into the most recent trading patterns, it is crucial to obtain reliable and up-to-date statistics on trade flows between different countries and explore possible dynamic network structures. There has been increasing interest in understanding such a complex network and developing different network models over the past decade \citep[e.g.,][]{MNP2013, BHM2016}. It is noteworthy that trade flows between a dyad represented by an origin and a destination country do not solely depend on these two countries directly involved, but also on how this relates to the opportunities of moving to other destinations. Hence, it is sensible to apply the proposed network time series model \eqref{eq2.1} to the trade flow data and further explore smooth structural changes and the latent group structure on time-varying coefficients. 

Let $x_{i,t}$ denote the growth rates of bilateral trade flows divided by GDP per capita \citep[e.g.,][]{R13} of 90 countries, i.e., $N=90$. The export flow data are collected from the Direction of Trade Statistics (DOTS) of International Monetary Fund (IMF) available at \url{https://www.imf.org/external/index.htm}. We use the FOB (free on board) value of exports of goods (denominated in U.S. dollars) and restrict the sample over a 324-month period between January 1995 and December 2021, i.e., $T=324$. The data of GDP per capita are available at \url{https://www.worldbank.org/en/home}. In the preliminary analysis, we standardize each time series observation to ensure zero mean and unit variance. For the adjacency matrix  $\boldsymbol{W}=(w_{ij})_{N\times N}$, we set $w_{ij} =1$ if there is a trade flow between $i$ and $j$, and $0$ otherwise. The row-normalized adjacency matrix $\widetilde{\boldsymbol{W}}$ is then used in the developed model.

With the estimation methodology developed in Section \ref{sec3}, we first estimate the heterogeneous node-specific time-varying coefficients (see Figure \ref{fig1}), and then determine the latent group structure. The tuning parameters involved are selected as in the simulation study. By the proposed information criterion, we identify 3 groups, i.e., $\widehat{K}=3$. The group membership details are summarized in Table \ref{tab:4}. Group 1 mainly consists of some Asia-Pacific countries such as CHN, KOR, MYS, THA, USA and VNM; whereas Group 3 includes some European countries such as DEU, ESP, GBR, POL and SWE as well as some members of Commonwealth countries (BGD, CAN, DOM, IND, NZL, TTO) which traditionally have close ties to GBR. It seems difficult to interpret Group 2 which contains 63 countries, and may be possible to further split it into some smaller groups via a different clustering technique.

We further conduct post-grouping local linear estimation and kernel-weighted inference. The estimated group-specific time-varying coefficients and their 95\% point-wise confidence intervals are plotted in Figures \ref{fig2}. It follows from Figure \ref{fig2} that the estimated momentum effects $\widehat{\alpha}_{k,2}(\cdot)$ have more significant time-varying patterns than the estimated network effects $\widehat{\alpha}_{k,1}(\cdot)$. The proposed kernel-weighted test reject the null hypothesis of constant coefficients at the $5\%$ significance level for all the groups with respective $p$-values 0.047, 0.033, 0.029. Note that the estimated time-varying momentum effect are generally negative, indicating that a fast trade growth rate at time $t$ often leads to a slow rate at $t+1$. This may be sensible from the demand and supply perspective, as a fast growth rate is often related to a strong demand which results in over-supply in the following time period.


\begin{table}[tbp]\centering
\caption{Group members for the empirical analysis of the trade flow data. }\label{tab:4}
\begin{tabular}{cccccccc}
\hline\hline
\multicolumn{1}{c}{Group 1} & \multicolumn{5}{c}{Group 2} & \multicolumn{2}{c}{Group 3} \\
\multicolumn{1}{c|}{CHL} & DZA & CYP & ITA & PAK & \multicolumn{1}{r|}{URY} & BGD & SVK   \\
\multicolumn{1}{c|}{CHN} & AGO & DNK & JPN & PAN & \multicolumn{1}{r|}{UZB} & CAN & SWE \\
\multicolumn{1}{c|}{COL} & ARG & ECU & KAZ & PNG & \multicolumn{1}{r|}{ZMB} & CIV & TTO  \\
\multicolumn{1}{c|}{KOR} & AUS & EGY & KWT & PRY & \multicolumn{1}{c|}{} & CZE &  \\
\multicolumn{1}{c|}{MYS} & AUT & EST & LAO & PER & \multicolumn{1}{c|}{} & DOM &  \\
\multicolumn{1}{c|}{THA} & AZE & FIN & LVA & PRT & \multicolumn{1}{c|}{} & DEU &  \\
\multicolumn{1}{c|}{ARE} & BLR & FRA & LBY & QAT & \multicolumn{1}{c|}{} & ESP  &  \\
\multicolumn{1}{c|}{USA} & BOL & GAB & LTU & RUS & \multicolumn{1}{c|}{} & GBR  &  \\
\multicolumn{1}{c|}{VNM} & BRA & CHA & MEX & SAU & \multicolumn{1}{c|}{} & HUN  &  \\
\multicolumn{1}{c|}{} & BRN & GRC & MNG & SVN & \multicolumn{1}{c|}{} & IND  &  \\
\multicolumn{1}{c|}{} & BGR & GTM & MAR & LKA & \multicolumn{1}{c|}{} & JOR  &  \\
\multicolumn{1}{c|}{} & KHM & IDN & MMR & CHE & \multicolumn{1}{c|}{} & NZL  &  \\
\multicolumn{1}{c|}{} & COG & IRN & NLD & TUN & \multicolumn{1}{c|}{} & OMN  &  \\
\multicolumn{1}{c|}{} & CRI & IRL & NGA & TUR & \multicolumn{1}{c|}{} & PHL &  \\
\multicolumn{1}{c|}{} & HRV & ISR & NOR & UKR & \multicolumn{1}{c|}{} & POL & \\
\hline\hline
\end{tabular}\\
{\em\small Note:\ We use the ISO 3166-1 alpha-3 codes to represent countries}.
\end{table}

\begin{figure}[tbp]\centering
\hspace*{-1cm}\includegraphics[scale=0.22]{Group}
\caption{\small Estimates of the node-specific time-varying coefficients}\label{fig1}
\end{figure}


\begin{figure}[tbp]\centering
\hspace*{-1cm}\includegraphics[scale=0.3]{figbeta}
\caption{\small Estimates of the group-specific time-varying coefficients}\label{fig2}
\end{figure}


%%%%%%%%%%%%%%%%%%%


\section{Conclusion}\label{sec6}
%\renewcommand{\theequation}{6.\arabic{equation}} \setcounter{equation}{0}

In this paper we have introduced a general nonlinear network VAR model for high-dimensional time series, where the momentum and network spillover effects are allowed to change over time and nodes. To achieve dimension reduction and obtain satisfactory estimation convergence rates, we impose a latent group structure on time-varying coefficients in the heterogeneous network VAR model. The unknown group number is determined by the extended information criterion whereas the group membership is estimated by the agglomerative clustering algorithm with normalized distance matrix estimates. Theorems \ref{thm:3.2} and \ref{thm:3.3} show that the developed methodology consistently estimates the latent group structure. To further improve the convergence rates of the time-varying coefficient estimation, we have proposed a post-grouping local linear smoothing to estimate the group-specific time-varying momentum and network effects. Meanwhile a post-grouping kernel-weighted test is developed to examine the parametric model specification for group-specific time-varying coefficients. The simulation study demonstrates that (i) the developed method can accurately estimate the latent group structure in finite samples; (ii) the post-grouping local linear estimation significantly outperforms the naive heterogeneous estimation which ignores the latent structure; and (iii) the kernel-weighted test statistic has stable finite-sample performance under either the null or alternative hypothesis.  The empirical study shows that there exist three groups over the node-specific time-varying coefficients for the export flow data collected over 90 countries and the momentum effects tend to evolve smoothly over time.  




%%%%%%%%%%%%%%%%%%%


\section*{\Large Appendix A:\ \ Proofs of the main asymptotic theorems}
\renewcommand{\theequation}{A.\arabic{equation}}
\setcounter{equation}{0}

{\small We next provide the detailed proofs of the main asymptotic theorems stated in Sections \ref{sec3} and \ref{sec4}. Let $C$ denote a generic positive constant throughout the proofs.

\smallskip

\noindent{\bf Proof of Proposition \ref{prop:3.1}}.\ \ For any $1\leq i\leq N$, we define 
\begin{eqnarray}
{\boldsymbol\Gamma}_{i,X}(\tau)&=&\left[
\begin{array}{cc}
\frac{1}{Th}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau) & \frac{1}{Th}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,1}(\tau) \\
\frac{1}{Th}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,1}(\tau) & \frac{1}{Th}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,2}(\tau)
\end{array}
\right]\nonumber\\
&=:&\left[
\begin{array}{cc}
{\boldsymbol\Gamma}_{i,X}(\tau,0) & {\boldsymbol\Gamma}_{i,X}(\tau,1)\\
{\boldsymbol\Gamma}_{i,X}(\tau,1) & {\boldsymbol\Gamma}_{i,X}(\tau,2)
\end{array}
\right],\notag\\
{\boldsymbol\Gamma}_{i,\beta}(\tau)&=&\left[
\begin{array}{c}
\frac{1}{Th}\sum_{t=1}^T\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\left[\beta_i(\tau_t)-\beta_i(\tau)-\beta_i^\prime(\tau)(\tau_t-\tau)\right]K_{t,0}(\tau) \\
\frac{1}{Th}\sum_{t=1}^T\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\left[\beta_i(\tau_t)-\beta_i(\tau)-\beta_i^\prime(\tau)(\tau_t-\tau)\right]K_{t,1}(\tau)
\end{array}
\right]\nonumber\\
&=:&\left[
\begin{array}{c}
{\boldsymbol\Gamma}_{i,\beta}(\tau,0) \\
{\boldsymbol\Gamma}_{i,\beta}(\tau,1)
\end{array}
\right],\notag\\
{\boldsymbol\Gamma}_{i,\varepsilon}(\tau)&=&\left[
\begin{array}{c}
\frac{1}{Th}\sum_{t=1}^T\widetilde{X}_{i,t-1}\varepsilon_{i,t}K_{t,0}(\tau) \\
\frac{1}{Th}\sum_{t=1}^T\widetilde{X}_{i,t-1}\varepsilon_{i,t}K_{t,1}(\tau)
\end{array}
\right]
=:\left[
\begin{array}{c}
{\boldsymbol\Gamma}_{i,\varepsilon}(\tau,0) \\
{\boldsymbol\Gamma}_{i,\varepsilon}(\tau,1)
\end{array}
\right],\notag
\end{eqnarray}
where $K_{t,j}(\tau)=\left(\frac{\tau_t-\tau}{h}\right)^jK\left(\frac{\tau_t-\tau}{h}\right)$. From the definition of $\widehat{\beta}_i(\cdot)$ in (\ref{eq3.3}), we write 
\begin{equation}\label{eqA.1}
\widehat{\beta}_i(\tau)-\beta_i(\tau)=\boldsymbol{E}_\ast\cdot{\boldsymbol\Gamma}_{i,X}^{-1}(\tau)\left[{\boldsymbol\Gamma}_{i,\beta}(\tau)+{\boldsymbol\Gamma}_{i,\varepsilon}(\tau)\right],
\end{equation}
where $\boldsymbol{E}_\ast=\left(\boldsymbol{I}_2,\boldsymbol{O}_{2\times2}\right)$. 


%Let 
%\[
%{\boldsymbol\Delta}_{i,T}^\ast(\tau)=\left[
%\begin{array}{cc}
%{\boldsymbol\Delta}_{i,T}^\ast(\tau,0) & {\boldsymbol\Delta}_{i,T}^\ast(\tau,1)\\
%{\boldsymbol\Delta}_{i,T}^\ast(\tau,1) & {\boldsymbol\Delta}_{i,T}^\ast(\tau,2)
%\end{array}
%\right],\ \ {\boldsymbol\Delta}_{i,T}^\ast(\tau,j)=\frac{1}{T}\sum_{t=1}^T{\sf E}\left[\widetilde{X}_{i,t-1}^\ast\widetilde{X}_{i,t-1}^{\ast^\intercal}\right]K_{t,j}(\tau),
%\]
Recall that ${\boldsymbol\Delta}_{ij}(\tau)={\sf E}\left[\widetilde{X}_{i,t}(\tau)\widetilde{X}_{j,t}^{^\intercal}(\tau)\right]$ and ${\boldsymbol\Delta}_i(\tau)={\boldsymbol\Delta}_{ii}(\tau)$. Define ${\boldsymbol\Lambda}_{i}(\tau)={\sf diag}\{1,\mu_2\}\otimes {\boldsymbol\Delta}_i(\tau)$. By Lemma \ref{lem:B.1}(i) and Assumption \ref{ass:1}, we have
\begin{equation}\label{eqA.2}
\max_{1\le i\le N}\left\Vert{\boldsymbol\Gamma}_{i,X}(\tau)-{\boldsymbol\Lambda}_{i}(\tau)\right\Vert=o_P\left(1\right).
\end{equation}
By Taylor's expansion and following the proof of Lemma \ref{lem:B.1}(i), we have for any $\tau\in(0,1)$,
\begin{equation}\label{eqA.3}
{\boldsymbol\Gamma}_{i,\beta}\left(\tau\right)=\left[
\begin{array}{c}
\frac{1}{2}h^2\mu_2 {\boldsymbol\Delta}_i(\tau)\beta_i^{\prime\prime}(\tau)\\
\boldsymbol{0}
\end{array}
\right]+o_P\left(h^2\right).
\end{equation}
By (\ref{eqA.1})--(\ref{eqA.3}), we may show that, for $i\neq j$,
\begin{eqnarray}
\widehat{d}_{ij}(\tau)-d_{ij}(\tau)&=&\boldsymbol{E}_\ast\cdot\left\{\left[{\boldsymbol\Gamma}_{i,X}^{-1}(\tau){\boldsymbol\Gamma}_{i,\beta}(\tau)-{\boldsymbol\Gamma}_{j,X}^{-1}(\tau){\boldsymbol\Gamma}_{j,\beta}(\tau)\right]+\left[{\boldsymbol\Gamma}_{i,X}^{-1}(\tau){\boldsymbol\Gamma}_{i,\varepsilon}(\tau)-{\boldsymbol\Gamma}_{j,X}^{-1}(\tau){\boldsymbol\Gamma}_{j,\varepsilon}(\tau)\right]\right\}\notag\\
&=&\boldsymbol{E}_\ast\cdot\Bigg\{\left[{\boldsymbol\Lambda}_{i}^{-1}(\tau){\boldsymbol\Gamma}_{i,\varepsilon}(\tau)-{\boldsymbol\Lambda}_{j}^{-1}(\tau){\boldsymbol\Gamma}_{j,\varepsilon}(\tau)\right]+\left[\frac{1}{2}h^2\mu_2d_{ij}^{\prime\prime}(\tau), {\bf 0}^{^\intercal}\right]^{^\intercal}\notag\\
&&+o_P\left((Th)^{-1/2}+h^2\right)\Bigg\}.\label{eqA.4}
\end{eqnarray}

We next only need to show that 
\begin{equation}\label{eqA.5}
\sqrt{Th}\cdot\left[{\boldsymbol\Lambda}_{i}^{-1}(\tau){\boldsymbol\Gamma}_{i,\varepsilon}(\tau)-{\boldsymbol\Lambda}_{j}^{-1}(\tau){\boldsymbol\Gamma}_{j,\varepsilon}(\tau)\right]\stackrel{d}\longrightarrow {\sf N}\left(\boldsymbol{0}, {\boldsymbol\Sigma}_{ij}^\ast(\tau)\right),
\end{equation}
where
${\boldsymbol\Sigma}_{ij}^\ast(\tau)={\sf diag}\left\{\nu_0,\nu_2\mu_2^{-2}\right\}\otimes {\boldsymbol\Sigma}_{ij}(\tau)$ with ${\boldsymbol\Sigma}_{ij}(\tau)$ defined in Proposition \ref{prop:3.1}. By the Cram\'er-Wold device \citep[e.g.,][]{BP95}, we only need to prove that for any four-dimensional vector $u$ of numbers, 
\begin{equation}\label{eqA.6}
\sqrt{Th}\cdot u^{^\intercal}\left[{\boldsymbol\Lambda}_{i}^{-1}(\tau){\boldsymbol\Gamma}_{i,\varepsilon}(\tau)-{\boldsymbol\Lambda}_{j}^{-1}(\tau){\boldsymbol\Gamma}_{j,\varepsilon}(\tau)\right]\stackrel{d}\longrightarrow {\sf N}\left(0,u^{^\intercal}{\boldsymbol\Sigma}_{ij}^\ast(\tau)u\right).
\end{equation}
Note that $u^{^\intercal}[{\boldsymbol\Lambda}_{i}^{-1}(\tau){\boldsymbol\Gamma}_{i,\varepsilon}(\tau)-{\boldsymbol\Lambda}_{j}^{-1}(\tau){\boldsymbol\Gamma}_{j,\varepsilon}(\tau)]$ is martingale with respect to $\mathscr{F}_{t}$. Write $u=\left(u_1^{^\intercal},u_2^{^\intercal}\right)^{^\intercal}$ with $u_1$ and $u_2$ being two-dimensional vectors of numbers. Let
\begin{eqnarray}
\sigma_{ij,T}^2&=&\frac{1}{Th}\sum_{t=1}^T{\sf E}\Bigg(\bigg\{\left[u_1^{^\intercal}{\boldsymbol\Delta}_i^{-1}(\tau)\widetilde{X}_{i,t-1}K_{t,0}(\tau)+u_2^{^\intercal}\mu_2^{-1}{\boldsymbol\Delta}_i^{-1}\widetilde{X}_{i,t-1} K_{t,1}(\tau)\right]\varepsilon_{i,t}\notag\\
&&-\left[u_1^{^\intercal}{\boldsymbol\Delta}_j^{-1}(\tau)\widetilde{X}_{j,t-1} K_{t,0}(\tau)+u_2^{^\intercal}\mu_2^{-1}{\boldsymbol\Delta}_j^{-1}(\tau)\widetilde{X}_{j,t-1} K_{t,1}(\tau)\right]\varepsilon_{j,t}\bigg\}^2\big|\mathscr{F}_{t-1}\Bigg),\notag
\end{eqnarray}
Using Assumption \ref{ass:1}(iii) and Lemmas \ref{lem:B.1}(i) and \ref{lem:B.2}(i), we may show that $\sigma_{ij,T}^2\stackrel{P}\rightarrow u^{^\intercal}{\boldsymbol\Sigma}_{ij}^\ast(\tau)u$ as $T\rightarrow\infty$. Then, by the martingale central limit theorem \citep[e.g., Corollary 3.1 in][]{HH80}, we prove (\ref{eqA.6}) and then (\ref{eqA.5}). By virtue of (\ref{eqA.4}) and (\ref{eqA.5}), we complete the proof of (\ref{eq3.5}). \hfill$\Box$

\smallskip


\noindent{\bf Proof of Theorem \ref{thm:3.2}}. To prove (\ref{eq3.8}), we only need to show that
\begin{equation}\label{eqA.7}
{\sf P}\left(\max_{1\le k \le K_0}\max_{i,j\in{\mathscr G}_k}\widehat{D}_{ij}<\min_{1\le k\neq l\le K_0}\min_{i\in{\mathscr G}_k,j\in{\mathscr G}_l}\widehat{D}_{ij}\right)\rightarrow1
\end{equation}
as $T$ tends to infinity. Notice that we have $D_{ij}=0$ if $i,j\in{\mathscr G}_k$, and $
\min_{1\le k\neq l\le K_0}\min_{i\in{\mathscr G}_k,j\in{\mathscr G}_l}D_{ij}>0$. Hence, to prove (\ref{eqA.7}), it is sufficient to show that
\begin{equation}\label{eqA.8}
\max_{1\le i,j\le N}\big\vert\widehat{D}_{ij}-D_{ij}\big\vert=o_P\left(\underline{D}\right),
\end{equation}
where $\underline{D}$ is defined in (\ref{eq3.7}).

Letting $\widetilde{D}_{ij}=\frac{1}{L}\sum_{l=1}^LD_{ij}^{^\intercal}(\tau_l^\ast)D_{ij}(\tau_l^\ast)$, we have
\[
\big\vert\widehat{D}_{ij}-D_{ij}\big\vert\le\big\vert\widehat{D}_{ij}-\widetilde{D}_{ij}\big\vert+\big\vert\widetilde{D}_{ij}-D_{ij}\big\vert.
\]
By the definition of the Riemann integral and $1/L=o\left(\underline{D}\right)$ in (\ref{eq3.7}), we readily have 
\begin{equation}\label{eqA.9}
\max_{1\le i,j\le N}\big\vert\widetilde{D}_{ij}-D_{ij}\big\vert=O_P\left(1/L\right)=o_P\left(\underline{D}\right).
\end{equation}
On the other hand, note that $1\le i,j\le N$,
\begin{eqnarray}
\big\vert\widehat{D}_{ij}-\widetilde{D}_{ij}\big\vert&=&\bigg\vert\frac{1}{L}\sum_{l=1}^L\left[\widehat{d}_{ij}^{^\intercal}(\tau_l^\ast)\widehat{{\boldsymbol\Sigma}}_{ij}^{-1}(\tau_l^\ast)\widehat{d}_{ij}(\tau_l^\ast)-d_{ij}^{^\intercal}(\tau_l^\ast){\boldsymbol\Sigma}_{ij,T}^{-1}(\tau_l^\ast)d_{ij}(\tau_l^\ast)\right]\bigg\vert\notag\\
&\le&\frac{1}{L}\sum_{l=1}^L\bigg\vert\widehat{d}_{ij}^{^\intercal}(\tau_l^\ast)\left[\widehat{{\boldsymbol\Sigma}}_{ij}^{-1}(\tau_l^\ast)-{\boldsymbol\Sigma}_{ij,T}^{-1}(\tau_l^\ast)\right]\widehat{d}_{ij}(\tau_l^\ast)\bigg\vert+\notag\\
&&\frac{1}{L}\sum_{l=1}^L\bigg\vert\left[\widehat{d}_{ij}(\tau_l^\ast)+d_{ij}(\tau_l^\ast)\right]^{^\intercal}{\boldsymbol\Sigma}_{ij,T}^{-1}(\tau_l^\ast)\left[\widehat{d}_{ij}(\tau_l^\ast)-d_{ij}(\tau_l^\ast)\right]\bigg\vert\notag\\
&=:&I_{ij}(1)+I_{ij}(2).\notag
\end{eqnarray}
Note that $\left\Vert{\boldsymbol\Sigma}_{ij,T}^{-1}(\tau)\right\Vert$ is uniformly bounded since ${\boldsymbol\Sigma}_{ij}(\tau)$ is positive definite uniformly over $i,j$ and $\tau$ and ${\boldsymbol\Sigma}_{ij,T}(\tau)$ uniformly converges to ${\boldsymbol\Sigma}_{ij}(\tau)$. Then, for $I_{ij}(2)$, using Lemma \ref{lem:B.3}, we readily have that
\begin{equation}\label{eqA.10}
\max_{1\leq i,j\leq N} I_{ij}(2)=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}+h^2\right).
\end{equation}
We next consider $I_{ij}(1)$. Note that 
\begin{eqnarray}
\widehat{\sigma}_{ij}-\sigma_{ij}&=&\frac{1}{T}\sum_{t=1}^T\widehat{\varepsilon}_{i,t}\widehat{\varepsilon}_{j,t}-\frac{1}{T}\sum_{t=1}^T\varepsilon_{i,t}\varepsilon_{j,t}+\frac{1}{T}\sum_{t=1}^T\varepsilon_{i,t}\varepsilon_{j,t}-{\sf E}\left(\varepsilon_{i,t}\varepsilon_{j,t}\right)\notag\\
&=&\frac{1}{T}\sum_{t=1}^T\left(\widehat{\varepsilon}_{i,t}\widehat{\varepsilon}_{j,t}-\varepsilon_{i,t}\varepsilon_{j,t}\right)+O_P\left(\sqrt{\frac{\log N}{T}}\right),\notag
\end{eqnarray}
and 
\[
\big\vert\widehat{\varepsilon}_{i,t}-\varepsilon_{i,t}\big\vert\le\bigg\vert\left[\widehat{\beta}_i(\tau_t)-\beta_i(\tau_t)\right]^{^\intercal}\widetilde{X}_{i,t-1}\bigg\vert.
\]
By Lemma \ref{lem:B.3}, we then have 
\[
\max_{1\le i,j\le N}\big\vert\widehat{\sigma}_{ij}-\sigma_{ij}\big\vert=O_p\left(\sqrt{\frac{\log (N\vee T)}{Th}}+h^2\right),
\]
which, together with Lemmas \ref{lem:B.1}(ii) and \ref{lem:B.2}(ii), leads to
\begin{equation}\label{eqA.11}
\max_{1\le i,j\le N}\sup_{\tau\in[0,1]}\left\Vert\widehat{\boldsymbol\Sigma}_{ij}(\tau)-{\boldsymbol\Sigma}_{ij,T}(\tau)\right\Vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}+h^2\right).
\end{equation}
This indicates that 
\begin{equation}\label{eqA.12}
\max_{1\leq i,j\leq N}I_{ij}(1)\le  C\cdot\frac{1}{L}\sum_{l=1}^L\left\Vert\widehat{\boldsymbol\Sigma}_{ij}^{-1}(\tau_l^\ast)-{\boldsymbol\Sigma}_{ij,T}^{-1}(\tau_l^\ast)\right\Vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}+h^2\right).
\end{equation}
By (\ref{eq3.7}), (\ref{eqA.10}) and (\ref{eqA.12}), we have
\begin{equation}\label{eqA.13}
\max_{1\le i,j\le N}\big\vert\widehat{D}_{ij}-\widetilde{D}_{ij}\big\vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}+h^2\right)=o_P\left(\underline{D}\right).
\end{equation}
With (\ref{eqA.9}) and (\ref{eqA.13}), we complete the proof of (\ref{eqA.8}). \hfill$\Box$

\smallskip

\noindent{\bf Proof of Theorem \ref{thm:3.3}}.\ \ By (\ref{eq3.13}), we only need to show that
\begin{equation}\label{eqA.14}
{\sf P}\left({\sf IC}(K_0)=\min_{1\le K\le \overline{K}}{\sf IC}(K)\right)\rightarrow1.
\end{equation}
To prove (\ref{eqA.14}), we next consider the following two scenarios: (i) $1\le K\le K_0-1$ and (ii) $K_0+1\le K\le \overline{K}$, corresponding to the under-fitted and over-fitted models, respectively. Let ${\cal E}({\mathscr G})$ denote the event that $\big\{\widehat{{\mathscr G}}_1,\cdots,\widehat{{\mathscr G}}_{K_0}\big\}=\big\{{\mathscr G}_1,\cdots,{\mathscr G}_{K_0}\big\}$. For scenario (i), by Theorem \ref{thm:3.2} and Lemma \ref{lem:B.4} in Appendix B, we have
\begin{eqnarray}
&&{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K),1\le K\le K_0-1\right)\notag\\
&=&{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K),1\le K\le K_0-1,{\cal E}({\mathscr G})\right)+o(1)\notag\\
&=&1+o(1).\label{eqA.15}
\end{eqnarray}
On the other hand, for scenario (ii), by Theorem \ref{thm:3.2} and Lemma \ref{lem:B.5}, we have 
\begin{eqnarray}
&&{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K),K_0+1\le K\le \overline{K}\right)\notag\\
&=&{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K),K_0+1\le K\le \overline{K},{\cal E}({\mathscr G})\right)+o(1)\notag\\
&=&1+o(1).\label{eqA.16}
\end{eqnarray}
Combine (\ref{eqA.15}) and (\ref{eqA.16}), we complete the proof of Theorem \ref{thm:3.3}. \hfill$\Box$

\smallskip

\noindent{\bf Proof of Theorem \ref{thm:4.1}}.\ \ By the consistency properties in Theorems \ref{thm:3.2} and \ref{thm:3.3}, we may prove the asymptotic distribution theory conditional on $\widehat{{\mathscr G}}_k={\mathscr G}_k$ and $\widehat{K}=K_0$. For $k=1,\cdots,K_0$, let
\begin{eqnarray}
{\boldsymbol\Xi}_{k,X}(\tau)&=&\left[
\begin{array}{cc}
\frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}^{\ddagger}\left(\tau\right) & \frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,1}^{\ddagger}\left(\tau\right)\\
\frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,1}^{\ddagger}\left(\tau\right)& \frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,2}^{\ddagger}\left(\tau\right)
\end{array}
\right],\notag\\
{\boldsymbol\Xi}_{k,\alpha}(\tau)&=&\left[
\begin{array}{c}
\frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\left[\alpha_k(\tau_t)-\alpha_k(\tau)-\alpha_k^\prime(\tau)(\tau_t-\tau)\right]K_{t,0}^{\ddagger}\left(\tau\right)
 \\
\frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\left[\alpha_k(\tau_t)-\alpha_k(\tau)-\alpha_k^\prime(\tau)(\tau_t-\tau)\right]K_{t,1}^{\ddagger}\left(\tau\right)
\end{array}
\right],\notag\\
{\boldsymbol\Xi}_{k,\varepsilon}(\tau)&=&\left[
\begin{array}{c}
\frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\varepsilon_{i,t}K_{t,0}^{\ddagger}(\tau) \\
\frac{1}{\vert{\mathscr G}_k\vert Th_2}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k}\widetilde{X}_{i,t-1}\varepsilon_{i,t}K_{t,1}^{\ddagger}(\tau) 
\end{array}
\right],\notag
\end{eqnarray}
where $K^\ddagger_{t,j}(\tau)=\left(\frac{\tau_t-\tau}{h_2}\right)^jK\left(\frac{\tau_t-\tau}{h_2}\right)$. By the definition of the post-grouping local linear estimation defined in (\ref{eq4.2}), we have
\[
\widehat{\alpha}_k(\tau)-\alpha_k(\tau)=\boldsymbol{E}_{\ast}\cdot{\boldsymbol\Xi}_{k,X}^{-1}(\tau)\left[{\boldsymbol\Xi}_{k,\alpha}(\tau)+{\boldsymbol\Xi}_{k,\varepsilon}(\tau)\right].
\]
In order to prove (\ref{eq4.4}) in Theorem \ref{thm:4.1}, we only need to show
\begin{eqnarray}
&&{\boldsymbol\Xi}_{k,X}(\tau)\stackrel{P}\longrightarrow{\sf diag}\left\{1,\mu_2\right\}\otimes \boldsymbol{\Delta}_{{\mathscr G}_k}(\tau),\label{eqA.17}\\
&&{\boldsymbol\Xi}_{k,\alpha}(\tau)=\left[
\begin{array}{c}
\frac{1}{2}h_2^2\mu_2\boldsymbol{\Delta}_{{\mathscr G}_k}(\tau)
\alpha_k^{\prime\prime}(\tau)\\
\boldsymbol{0} \\
\end{array}
\right]+o_P(h_2^2),\label{eqA.18}\\
&&\sqrt{\vert{\mathscr G}_k\vert Th_2}{\boldsymbol\Xi}_{k,\varepsilon}(\tau)\stackrel{d}\longrightarrow{\sf N}\left(\boldsymbol{0},\ {\sf diag}\left\{\nu_0,\nu_2\right\}\otimes{\boldsymbol\Upsilon}_{{\mathscr G}_k}(\tau)\right).\label{eqA.19}
\end{eqnarray}
As the proof of (\ref{eqA.17}) is similar to the proof of (\ref{eqA.2}), details are omitted here. By the smoothness condition in Assumption \ref{ass:1}(ii), Taylor's expansion of $\alpha_k(\cdot)$ and (\ref{eqA.17}), we can prove (\ref{eqA.18}). We next turn to the proof of (\ref{eqA.19}). Let 
\[
W_{{\mathscr G}_k,t}(\tau)=\left[W_{{\mathscr G}_k,t}^{^\intercal}(\tau,0), W_{{\mathscr G}_k,t}^{^\intercal}(\tau,1)\right]^{^\intercal}\quad {\rm with}\quad W_{{\mathscr G}_k,t}(\tau,j)=
\frac{1}{\sqrt{\vert{\mathscr G}_k\vert}}
\sum_{i\in{\mathscr G}_k}\varepsilon_{i,t}\widetilde{X}_{i,t-1}K^\ddagger_{t,j}(\tau),
\]
and write
\[
\sqrt{\vert{\mathscr G}_k\vert Th_2}{\boldsymbol\Xi}_{k,\varepsilon}(\tau)=\frac{1}{\sqrt{Th_2}}\sum_{t=1}^T W_{{\mathscr G}_k,t}(\tau).
\]
Using Lemma \ref{lem:B.2}(i), we may show that 
\begin{eqnarray}
\frac{1}{Th_2}\sum_{t=1}^T {\sf E}\left[W_{{\mathscr G}_k,t}(\tau,0)W_{{\mathscr G}_k,t}^{^\intercal}(\tau,0) | {\cal F}_{t-1}\right]&=&\frac{1}{\vert{\mathscr G}_k\vert}\sum_{i,j\in{\mathscr G}_k}\sigma_{ij}\left[\frac{1}{Th_2}\sum_{t=1}^T \widetilde X_{i,t-1}\widetilde X_{j,t-1}^{^\intercal}K^\ddagger_{t,0}(\tau)\right]\notag\\
&\stackrel{P}\longrightarrow&\frac{\nu_0}{\vert{\mathscr G}_k\vert}\sum_{i,j\in{\mathscr G}_k}\sigma_{ij}{\boldsymbol\Delta}_{ij}(\tau).\notag
\end{eqnarray}
Similarly, we can also prove that
\[
\frac{1}{Th_2}\sum_{t=1}^T {\sf E}\left[W_{{\mathscr G}_k,t}(\tau,1)W_{{\mathscr G}_k,t}^{^\intercal}(\tau,1) | {\cal F}_{t-1}\right]\stackrel{P}\longrightarrow \frac{\nu_2}{\vert{\mathscr G}_k\vert}\sum_{i,j\in{\mathscr G}_k}\sigma_{ij}{\boldsymbol\Delta}_{ij}(\tau),
\]
and 
\[
\frac{1}{Th_2}\sum_{t=1}^T {\sf E}\left[W_{{\mathscr G}_k,t}(\tau,0)W_{{\mathscr G}_k,t}^{^\intercal}(\tau,1) | {\cal F}_{t-1}\right]=o_P(1).
\]
Hence, we have
\[
\frac{1}{Th_2}\sum_{t=1}^T {\sf E}\left[W_{{\mathscr G}_k,t}(\tau)W_{{\mathscr G}_k,t}^{^\intercal}(\tau) | {\cal F}_{t-1}\right]\stackrel{P}\longrightarrow {\sf diag}\left\{\nu_0,\nu_2\right\}\otimes\frac{1}{\vert{\mathscr G}_k\vert}\sum_{i,j\in{\mathscr G}_k}\sigma_{ij}{\boldsymbol\Delta}_{ij}(\tau),
\]
which, together with (\ref{eq4.3}) and the martingale central limit theorem \citep[e.g.,][]{HH80}, leads to (\ref{eqA.19}). The proof of Theorem \ref{thm:4.1} is completed. \hfill$\Box$

\smallskip


\noindent{\bf Proof of Theorem \ref{thm:4.2}}.\ \ Denote 
\[{\cal Q}_T(k)=\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)\varepsilon_s(k),\] 
where $\varepsilon_{t}(k)$ is defined in Assumption \ref{ass:4}(ii). It suffices to prove that 
\begin{eqnarray}
&&\left(2\nu_0T^2h_3\right)^{-1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert^{-1}{\cal Q}_T(k)\stackrel{d}\longrightarrow{\sf N}\left(0,1\right),\label{eqA.20}\\
&&\left(T^2h_3\right)^{-1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert^{-1}\left[\widetilde{\cal Q}_T(k)-{\cal Q}_T(k)\right]=o_P(1).\label{eqA.21}
\end{eqnarray}

Letting $Y_t(k)=2\varepsilon_t^{^\intercal}(k)\sum_{s<t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_s(k)$, we readily have that
\[
{\cal Q}_T(k)=2\sum_{t=1}^T\sum_{s<t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)\varepsilon_s(k)=\sum_{t=1}^T Y_t(k),
\]
which is a martingale with respect to $\mathscr{F}_{t}$. Note that
\begin{eqnarray}
\frac{1}{T^2h_3}\sum_{t=1}^T{\sf E}\left[Y_t^2(k) \big|\mathscr{F}_{t-1}\right]
&=&\frac{4}{T^2h_3}\sum_{t=1}^T\sum_{s_1,s_2<t}K\left(\frac{\tau_t-\tau_{s_1}}{h_3}\right)K\left(\frac{\tau_t-\tau_{s_2}}{h_3}\right)\varepsilon_{s_1}^{^\intercal}(k){\sf E}\left[\varepsilon_{t}(k)\varepsilon_{t}^{^\intercal}(k)\big|\mathscr{F}_{t-1}\right]\varepsilon_{s_2}(k)\notag\\
&=&\frac{4}{T^2h_3}\sum_{t=1}^T\sum_{s<t}\varepsilon_s^{^\intercal}(k){\boldsymbol\Sigma}_{\varepsilon}(k)\varepsilon_{s}(k)K^2\left(\frac{\tau_t-\tau_s}{h_3}\right)+O_P\left(T^{-1/2}\right)\notag\\
&=&2\cdot {\sf trace}\left\{{\boldsymbol\Sigma}_{\varepsilon}(k){\boldsymbol\Sigma}_{\varepsilon}(k)\right\}\frac{2}{T^2h_3}\sum_{t=1}^T\sum_{s<t}K^2\left(\frac{\tau_t-\tau_s}{h_3}\right)+o_P(1)\notag\\
&=&2\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert^2 \frac{1}{T^2h_3}\sum_{t=1}^T\sum_{s\neq t}K^2\left(\frac{\tau_t-\tau_s}{h_3}\right)+o_P(1)\notag\\
&=&2\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert^2\nu_0 (1+o_P(1)).\label{eqA.22}
\end{eqnarray}
Then, by (\ref{eqA.22}) and the martingale central limit theorem, we prove (\ref{eqA.20}).

We next turn to the proof of (\ref{eqA.21}) conditioning on $\widehat{\mathscr G}_{k}={\mathscr G}_k$. Define 
\[h_{t}(k)=\left(\left[g_k(\tau_t;\widetilde{\theta}_k)-g_k(\tau_t;\theta_k)\right]^{^\intercal}\widetilde{X}_{i,t-1},i\in{\mathscr G}_k\right).\] 
Consider the following decomposition: 
\[
\widetilde{\cal Q}_T(k)-{\cal Q}_T(k)=\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)h_t^{^\intercal}(k)h_s(k)-2\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)h_s(k).
\]
Let 
\[\omega_{t}(k)=\left(\left[\partial_{\theta}g_k(\tau_t;\theta_k)(\widetilde{\theta}_k-\theta_k)\right]^{^\intercal}\widetilde{X}_{i,t-1},i\in{\mathscr G}_k\right)\] 
and $r_{t}(k)=h_{t}(k)-\omega_{t}(k)$. By Assumption \ref{ass:4}(i), we have 
\begin{equation}\label{eqA.23}
\vert\omega_{t}(k)\vert^2=O\left(\delta_\theta^2\right)\sum_{i\in{\mathscr G}_k}\vert\widetilde{X}_{i,t-1}\vert^2,\ \ \vert r_{t}(k)\vert^2=O\left(\delta_\theta^4\right)\sum_{i\in{\mathscr G}_k}\vert\widetilde{X}_{i,t-1}\vert^2.
\end{equation}
Hence, by (\ref{eqA.23}) and Assumption \ref{ass:4}(ii) we may show that
\begin{eqnarray}
&&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)h_t^{^\intercal}(k)h_s(k)\notag\\
&=&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\left[\omega_t^{^\intercal}(k)\omega_s(k)+\omega_t^{^\intercal}(k)r_s(k)+r_t^{^\intercal}(k)\omega_s(k)+r_t^{^\intercal}(k)r_s(k)\right]\notag\\
&=&\frac{O\left(\delta_\theta^2+\delta_\theta^4\right)}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\sum_{i\in{\mathscr G}_k}\vert\widetilde{X}_{i,t-1}\vert^2\notag\\
&=&O\left(\delta_\theta^2\right)\frac{O_P\left(|{\mathscr G}_k|T^2h_3\right)}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\notag\\
&=&O_P\left(\frac{\delta_\theta^2NTh_3^{1/2}}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\right)=o_P(1).\label{eqA.24}
\end{eqnarray}
On the other hand, using (\ref{eqA.23}) and noting that
\[{\sf E}\left[\left\vert\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\sum_{t\neq s}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\partial_{\theta}g_k(\tau_s;\theta_k)\widetilde X_{i,s-1}\varepsilon_{i,t}\right\vert^2\right]=O\left(|{\mathscr G}_k|T^3h_3\right)\]
which can be proved using the first condition in (\ref{eq3.14}), we may show that
\begin{eqnarray}
&&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)h_s(k)\notag\\
&\le&\frac{\left\vert \widetilde{\theta}_k-\theta_k\right\vert}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\left\vert\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\sum_{t\neq s}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\partial_{\theta}g_k(\tau_s;\theta_k)\widetilde X_{i,s-1}\varepsilon_{i,t}\right\vert\notag\\
&=&\frac{O\left(\delta_\theta\right)}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\cdot O_P\left(\vert{\mathscr G}_k\vert^{1/2}T^{3/2}h_3^{1/2}\right)\notag\\
&=&O_P\left(\frac{\delta_\theta(NT)^{1/2}}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\right)=o_P(1).\label{eqA.25}
\end{eqnarray}
By (\ref{eqA.24}) and (\ref{eqA.25}), we complete the proof of (\ref{eqA.21}).\hfill$\Box$

\smallskip


\noindent{\bf Proof of Theorem \ref{thm:4.3}}.\ \ Let ${\cal Q}_T^\diamond(k)$ be defined similarly to $\widetilde{\cal Q}_T(k)$ but with $\widetilde{\theta}_k$ replaced by $\theta_k^\diamond$. Denote 
\begin{eqnarray}
\varpi_t(k)&=&\left(\alpha_k^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}, i\in{\mathscr G}_k\right),\notag\\
\varpi_t^\diamond(k)&=&\left(g_k^{^\intercal}(\tau_t;\theta_k^\diamond)\widetilde{X}_{i,t-1}, i\in{\mathscr G}_k\right),\notag\\
\varpi_t^\dagger(k)&=&\left(g_k^{^\intercal}(\tau_t;\widetilde{\theta}_k)\widetilde{X}_{i,t-1}, i\in{\mathscr G}_k\right),\notag
\end{eqnarray}
and $\varpi_{t}^\ddagger(k)=\varpi_t^\dagger(k)-\varpi_t^\diamond(k)$. Write
\begin{eqnarray}
\widetilde{\cal Q}_T(k)-{\cal Q}_T^\diamond(k)&=&-2\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)[\varpi_t^\ddagger(k)]^{^\intercal}\varepsilon_s(k)+\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)[\varpi_t^\ddagger(k)]^{^\intercal}\varpi_s^\ddagger(k)-\notag\\
&&2\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)[\varpi_t^\ddagger(k)]^{^\intercal}\left[\varpi_s(k)-\varpi_s^\diamond(k)\right].\notag
\end{eqnarray}
Following the proofs of (\ref{eqA.24}) and (\ref{eqA.25}), we can similarly prove that
\begin{eqnarray}
&&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)[\varpi_t^\ddagger(k)]^{^\intercal}\varpi_s^\ddagger(k)=o_P(1),\label{eqA.26}\\
&&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)[\varpi_t^\ddagger(k)]^{^\intercal}\varepsilon_s(k)=o_P(1).\label{eqA.27}
\end{eqnarray}
By the Cauchy-Schwarz inequality and Assumption \ref{ass:5}(i), we may show that
\begin{eqnarray}
&&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\left\vert [\varpi_t^\ddagger(k)]^{^\intercal}\left[\varpi_s(k)-\varpi_s^\diamond(k)\right]\right\vert\notag\\
&\le&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T \left\vert \varpi_t^\ddagger(k)\right\vert \sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\left\vert\varpi_s(k)-\varpi_s^\diamond(k) \right\vert\notag\\
&=&O_P\left(\frac{\vert{\mathscr G}_k\vert T^2h_3\delta_\theta^\diamond\varphi}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\right)=O_P\left(\frac{N Th_3^{1/2}\delta_\theta^\diamond\varphi }{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\right).\label{eqA.28}
\end{eqnarray}
By (\ref{eqA.26})--(\ref{eqA.28}) and Assumption \ref{ass:5}(iii), we readily have that
\begin{equation}\label{eqA.29}
\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\left\vert \widetilde{\cal Q}_T(k)-{\cal Q}_T^\diamond(k)\right\vert=o_P\left(\frac{N Th_3^{1/2}\varphi^2}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\vee1\right).
\end{equation}

On the other hand, we note that
\begin{eqnarray}
{\cal Q}_T^\diamond(k)-{\cal Q}_T(k)&=&2\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)\left[\varpi_s(k)-\varpi_s^\diamond(k)\right]+\notag\\
&&\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\left[\varpi_t(k)-\varpi_t^\diamond(k)\right]^{^\intercal}\left[\varpi_s(k)-\varpi_s^\diamond(k)\right].\notag
\end{eqnarray}
Following the proof of (\ref{eqA.25}) and using the continuity condition on $g_k(\cdot)$, we can show that
\begin{equation}\label{eqA.30}
\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\varepsilon_t^{^\intercal}(k)\left[\varpi_s(k)-\varpi_s^\diamond(k)\right]=O_P\left(\frac{\varphi(NT)^{1/2}}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\right).
\end{equation}
By Assumption \ref{ass:5}(ii), we can prove that 
\begin{eqnarray}
&&\frac{1}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}\sum_{t=1}^T\sum_{s\neq t}K\left(\frac{\tau_t-\tau_s}{h_3}\right)\left[\varpi_t(k)-\varpi_t^\diamond(k)\right]^{^\intercal}\left[\varpi_s(k)-\varpi_s^\diamond(k)\right]\notag\\
&=&\frac{\varphi^2\phi_k\cdot|{\mathscr G}_k|T^2h_3}{\left(T^2h_3\right)^{1/2}\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}(1+o_P(1))=\frac{\varphi^2\phi_k\cdot|{\mathscr G}_k|Th_3^{1/2}}{\Vert\boldsymbol{\Sigma}_{\varepsilon}(k)\Vert}(1+o_P(1)).\label{eqA.31}
\end{eqnarray}
By (\ref{eqA.20}) and (\ref{eqA.29})--(\ref{eqA.31}), we complete the proof of Theorem \ref{thm:4.3}. \hfill$\Box$

}

%%%%%%%%%%%%%%%%%%%


\section*{\Large Appendix B:\ \ Technical lemmas with proofs}
\renewcommand{\theequation}{B.\arabic{equation}}\setcounter{equation}{0}
\renewcommand{\thelemma}{B.\arabic{lemma}}\setcounter{lemma}{0}

{\small

We next prove some technical lemmas which have been used to prove the main asymptotic theorems in Appendix A. As in Appendix A, we let $C$ denote a generic positive constant whose value may change from one place to another.

\smallskip

\begin{lemma}\label{lem:B.1}

Suppose that the conditions of Proposition \ref{prop:3.1} are satisfied. Then we have
	
(i) For any fixed $\tau\in(0,1)$,
\begin{equation}\label{eqB.1}
\max_{1\le i\le N}\left\Vert{\boldsymbol\Gamma}_{i,X}(\tau)-{\boldsymbol\Delta}_{i,T}^\ast(\tau)\right\Vert=O_P\left(\sqrt{\frac{\log N}{Th}}\right),
\end{equation}
and (ii)
\begin{equation}\label{eqB.2}
\max_{1\le i\le N}\sup_{\tau\in[0,1]}\left\Vert{\boldsymbol\Gamma}_{i,X}(\tau)-{\boldsymbol\Delta}_{i,T}^\ast(\tau)\right\Vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right),
\end{equation}
where 
\[{\boldsymbol\Delta}_{i,T}^\ast(\tau)=\left[
\begin{array}{cc}
\frac{1}{T}\sum_{t=1}^T{\sf E}\left(\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right)K_{t,0}(\tau) & \frac{1}{T}\sum_{t=1}^T{\sf E}\left(\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right)K_{t,1}(\tau)\\
\frac{1}{T}\sum_{t=1}^T{\sf E}\left(\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right)K_{t,1}(\tau) & \frac{1}{T}\sum_{t=1}^T{\sf E}\left(\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right)K_{t,2}(\tau)
\end{array}
\right].\]


\end{lemma}

\smallskip

\noindent{\bf Proof of Lemma \ref{lem:B.1}}.\ \ We only prove the uniform convergence for ${\boldsymbol\Gamma}_{i,X}(\cdot,0)$ since the proofs for the other block-matrices ${\boldsymbol\Gamma}_{i,X}(\cdot,1)$ and ${\boldsymbol\Gamma}_{i,X}(\cdot,2)$ are very similar. Let 
\[
{\boldsymbol\Delta}_{i,T}(\tau)=\frac{1}{T}\sum_{t=1}^T{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right]K_h(\tau_t-\tau),
\]
and note that $\widehat{\boldsymbol\Delta}_i(\tau)={\boldsymbol\Gamma}_{i,X}(\tau,0)$. Hence, we only show that for any fixed $\tau\in(0,1)$,
\begin{equation}\label{eqB.3}
\max_{1\le i\le N}\left\Vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\right\Vert=O_P\left(\sqrt{\frac{\log N}{Th}}\right)
\end{equation}
Noting that
\[
\widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)=\frac{1}{Th}\sum_{t=1}^T\left\{\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}-{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right]\right\}K_{t,0}(\tau),
\]
to prove (\ref{eqB.3}), it is sufficient to show that
\begin{equation}\label{eqB.4}
\max_{1\le i\le N}\bigg\vert\widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\bigg\vert_\infty=O_P\left(\sqrt{\frac{\log N}{Th}}\right).
\end{equation}
Letting $V_{i,t}(\tau)=\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau)$ and noting that $K(\cdot)$ has the support $[-1,1]$,
\[
\sum_{t=1}^T V_{i,t}(\tau)=\sum_{t=1}^T\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau)=\sum_{t=T_1(\tau)}^{T_2(\tau)}\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau),
\]
where $T_1(\tau)=\lfloor T\tau\rfloor-\lfloor Th\rfloor+1$ and $T_2(\tau)=\lfloor T\tau\rfloor+\lfloor Th\rfloor$. We next adopt some standard techniques in the literature on high-dimensional locally stationary processes \citep[e.g.,][]{XCW20,ZW21} to prove (\ref{eqB.4}).   

Let $M=2\lfloor Th\rfloor, M_\dagger=\lfloor\log M/\log 2\rfloor,u_l=2^l$ for $1\le l\le M_\dagger-1$ and $u_{M_\dagger}=M$. Define
$V_{i,t,u}(\tau)={\sf E}\left[V_{i,t}(\tau)\big|\mathscr{F}_{t-u}^{t-1}\right]$ with $\mathscr{F}_s^t=(\varepsilon_{s},\cdots,\varepsilon_{t})$, and
\[
S_{i,l,T}(\tau)=\sum_{t=T_1(\tau)}^{T_2(\tau)}\left[V_{i,t,u_l}(\tau)-V_{i,t,u_{l-1}}(\tau)\right],\ \ l=1,\cdots,M_\dagger.
\]
Then we may write
\begin{equation}\label{eqB.5}
\widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)=\frac{1}{Th}\left(\sum_{t=T_1(\tau)}^{T_2(\tau)}\left[V_{i,t}(\tau)-V_{i,t,M}(\tau)\right]+\sum_{l=2}^{M_\dagger} S_{i,l,T}(\tau)+\sum_{t\vartheta\phi=T_1(\tau)}^{T_2(\tau)}\left\{V_{i,t,2}(\tau)-{\sf E}\left[V_{i,t,2}(\tau)\right]\right\}\right).
\end{equation}

We next tackle the three terms on the right side of (\ref{eqB.5}) separately. Since 
\[V_{i,t}(\tau)-V_{i,t,M}(\tau)=\sum_{k=M+1}^\infty\left[V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right],\] 
we have
\begin{align*}
\left\Vert\Bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}\left[V_{i,t}(\tau)-V_{i,t,M}(\tau)\right]\Bigg\vert_\infty\right\Vert_{q/2}=\ \ &\left\Vert\Bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}\sum_{k=M+1}^\infty\left[V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right]\Bigg\vert_\infty\right\Vert_{q/2}\\
\le \ \ &C\sum_{k=M+1}^\infty\left\Vert\Bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}\left[V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right]\Bigg\vert_\infty\right\Vert_{q/2}.
\end{align*}
Note that, for any $1\leq i\leq N$ and $k\ge M+1$, ${\sf vec}\left(V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right),~T_1(\tau)\le t\le T_2(\tau)$, is a sequence of martingale difference vectors. By Lemma D.3 of \cite{ZW21}, we have 
\begin{eqnarray}
&&\sum_{k=M+1}^\infty\left\Vert\Bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}\left(V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right)\Bigg\vert_\infty\right\Vert_{q/2}\notag\\
&=&\sum_{k=M+1}^\infty\left\Vert\Bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}{\sf vec}\left(V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right)\Bigg\vert_\infty\right\Vert_{q/2}\notag\\
&\le&C\sum_{k=M+1}^\infty \sqrt{\sum_{t=T_1(\tau)}^{T_2(\tau)}\left\Vert\bigg\vert{\sf vec}\left( V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\right)\bigg\vert_{\infty}\right\Vert_{q/2}^2}\notag\\
&=&C\sum_{k=M+1}^\infty \sqrt{\sum_{t=T_1(\tau)}^{T_2(\tau)}\left\Vert\bigg\vert V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\bigg\vert_{\infty}\right\Vert_{q/2}^2}.\notag
\end{eqnarray}
With the triangle inequality, 
\begin{eqnarray}
&&\left\Vert\bigg\vert V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\bigg\vert_{\infty}\right\Vert_{q/2}=\left\Vert\bigg\vert{\sf E}\left(V_{i,t}(\tau)-V_{i,t}^{\{t-k\}}(\tau)\big|\mathscr{F}_{t-k}^{t-1}\right)\bigg\vert_\infty\right\Vert_{q/2}\notag\\
&\le&\left\Vert\bigg\vert\left(\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{t-k\}}\right)\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q/2}+\left\Vert\bigg\vert \widetilde{X}_{i,t-1}^{\{t-k\}}\left(\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{t-k\}}\right)^{^\intercal}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q/2},\notag
\end{eqnarray}
where $V_{i,t}^{\{l\}}(\tau)=\widetilde{X}_{i,t-1}^{\{l\}}\widetilde{X}_{i,t-1}^{\{l\}^{^\intercal}}K_{t,0}(\tau)$ and $\widetilde{X}_{i,t}^{\{l\}}=\left(\sum_{j\neq i}\widetilde{w}_{ij}x_{j,t}^{\{l\}},x_{i,t}^{\{l\}}\right)^{^\intercal}$ with $x_{i,t}^{\{l\}}$ being the $i$-th element of $X_{t}^{\{l\}}$. Following the discussion in Section \ref{sec2.2} and noting that $\sum_{j=1,\neq i}^N\widetilde w_{ij}=1$, we have
\[
\left\Vert\bigg\vert\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{t-k\}}\bigg\vert_\infty\right\Vert_{q}\leq \delta_{i,k-1,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,k-1,q},
\] 
and 
\begin{eqnarray}
\left\Vert\bigg\vert\widetilde{X}_{i,t-1}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_q&\le&\sum_{k=0}^\infty\left\Vert\bigg\vert \left[{\sf E}(\widetilde{X}_{i,t-1}\big|\mathscr{F}_{t-k-1})-{\sf E}(\widetilde{X}_{i,t-1}\big|\mathscr{F}_{t-k-2})\right]K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q}\notag\\
&\le&C\sum_{k=0}^\infty\left(\delta_{i,k,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,k,q}\right)\notag\\
&\leq&C\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}\notag
\end{eqnarray}
which is bounded. Then, by Jensen's inequality, applying the Cauchy-Schwarz inequality, we have 
\begin{eqnarray}
\left\Vert\bigg\vert\left(\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{t-k\}}\right)\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q/2}&\leq&\left\Vert\bigg\vert\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{t-k\}}\bigg\vert_\infty\right\Vert_q\cdot\left\Vert\bigg\vert\widetilde{X}_{i,t-1}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_q\notag\\
&\le& C\left(\delta_{i,k-1,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,k-1,q}\right).\notag
\end{eqnarray}
Similarly we can also prove that
\[
\left\Vert\bigg\vert\left(\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{t-k\}}\right)\left(\widetilde{X}_{i,t-1}^{\{t-k\}}\right)^{^\intercal}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q/2}\leq C\left(\delta_{i,k-1,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,k-1,q}\right).
\]
Since $M=2\lfloor Th\rfloor$, we have 
\begin{eqnarray}
&&\sum_{k=M+1}^\infty \sqrt{\sum_{t=T_1(\tau)}^{T_2(\tau)}\left\Vert\bigg\vert\notag V_{i,t,k}(\tau)-V_{i,t,k-1}(\tau)\bigg\vert_{\infty}\right\Vert_{q/2}^2}\notag\\
&\le&C(Th)^{1/2}\sum_{k=M+1}^\infty\left(\delta_{i,k-1,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,k-1,q}\right)\notag\\
&=&C(Th)^{1/2}\left(\Delta_{i,M,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\Delta_{j,M,q}\right)\notag\\
&\le&C(Th)^{1/2-\alpha}\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha},\label{eqB.6}
\end{eqnarray}
where the last inequality is due to the fact that 
\[
\max_{1\leq i\leq N}\Delta_{i,M,q}\leq M^{-\alpha}\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}.
\]
By (\ref{eqB.6}) and Markov's inequality, we have for any $z>0$,
\begin{equation}\label{eqB.7}
{\sf P}\left(\bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}\left(V_{i,t}(\tau)-V_{i,t,M}(\tau)\right)\bigg\vert_\infty>z\right)\le \frac{C(Th)^{(1-2\alpha)q/4}}{z^{q/2}}\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}^{q/2}.
\end{equation}

For each $2\le l\le M_\dagger$, define
\[
U_{i,k,l}(\tau)=\sum_{t=(k-1)u_l+T_1(\tau)}^{(ku_l)\wedge T_2(\tau)}\left[V_{i,t,u_l}(\tau)-V_{i,t,u_{l-1}}(\tau)\right],\ \ 1\le k\le \lceil M/u_l\rceil,
\]
and
\[
S_{i,l,T}^{\rm e}(\tau)=\sum_{k \text{ is even}}U_{i,k,l}(\tau), ~~ S_{i,l,T}^{\rm o}(\tau)=\sum_{k \text{ is odd}}U_{i,k,l}(\tau).
\]
Let $\lambda_l=(l-1)^{-2}/(\pi^2/3)$ if $2\le l\le M_\dagger/2$ and $\lambda_l=(M_\dagger+1-l)^{-2}/(\pi^2/3)$ if $M_\dagger/2<l<M_\dagger$. It is easy to verify that $\sum_{l=2}^{M_\dagger}\lambda_l\le1$. Since $U_{i,k,l}(\tau)$ and $U_{i,k^\prime,l}(\tau)$ are independent for $\vert k-k^\prime\vert>1$, by Lemma D.4 of \cite{ZW21}, we have for any $z>0$ and each $2\le l\le M_\dagger$,
\[
{\sf P}\left(\left\vert S_{i,l,T}^{\rm e}(\tau)\right\vert_\infty\ge\lambda_lz\right)\le C\left((\lambda_lz)^{-q/2}\sum_{k\text{ is even}}{\sf E}\left[\vert U_{i,k,l}(\tau)\vert_\infty^{q/2}\right]+\exp\left\{-\frac{(\lambda_lz)^2}{C\sum\limits_{k\text{ is even}}{\sf E}\left[\vert U_{i,k,l}(\tau)\vert_\infty^2\right]}\right\}\right).
\]
Similarly to  (\ref{eqB.6}), we may show that 
\[
\Vert \vert U_{i,j,l}(\tau)\vert_\infty\Vert_{q/2}\leq C\cdot u_l^{1/2-\alpha}\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha},\quad\Vert\vert U_{i,j,l}(\tau)\vert_\infty\Vert_2\leq C\cdot u_l^{1/2-\alpha}\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{4,\alpha}.
\]
Similar results also hold for $\left\vert S_{i,l,T}^{\rm o}(\tau)\right\vert_\infty$. A combination of these arguments yields that
\begin{eqnarray}
&&{\sf P}\left(\left\vert\sum_{l=2}^{M_\dagger}S_{i,l,T}(\tau)\right\vert_\infty\ge2z\right)\notag\\
&\le&\sum_{l=2}^{M_\dagger}{\sf P}\left(\vert S_{i,l,T}(\tau)\vert_{\infty}\ge2\lambda_l z\right)\notag\\
&\le&\sum_{l=2}^{M_\dagger}{\sf P}\left(\vert S_{i,l,T}^{\rm e}(\tau)\vert_\infty\ge\lambda_lz\right)+\sum_{l=2}^{M_\dagger}{\sf P}\left(\vert S_{i,l,T}^{\rm o}(\tau)\vert_\infty\ge\lambda_lz\right)\notag\\
&\le&C\left\{\frac{Th}{z^{q/2}}\cdot\sum_{l=2}^{M_\dagger}\frac{u_l^{(1-2\alpha)q/4-1}}{\lambda_l^{q/2}}\left(\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}^{q/2}\right)+\sum_{l=2}^{M_\dagger}\exp\left(-\frac{\lambda_l^2u_l^{2\alpha}z^2}{C(Th)\max\limits_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{4,\alpha}^2}\right)\right\}.\notag
\end{eqnarray}
Noting that $\sum_{l=2}^{M_\dagger}\lambda_l^{-q/2}u_l^{(1-2\alpha)q/4-1}$ is bounded (since $\alpha<1/2-2/q$) and $\min_{l\ge 1}\lambda_l^2u_l^{2\alpha}>0$, we have
\begin{equation}\label{eqB.8}
{\sf P}\left(\left\vert\sum_{l=2}^{M_\dagger}S_{i,l,T}(\tau)\right\vert_\infty\ge2z\right)\le C\left(\frac{Th}{z^{q/2}}\left(\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}^{q/2}\right)+\exp\left\{-\frac{z^2}{C(Th)}\left(\max\limits_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{4,\alpha}^2\right)^{-1}\right\}\right).
\end{equation}

We next consider $\sum_{t=T_1(\tau)}^{T_2(\tau)}\left\{V_{i,t,2}(\tau)-{\sf E}[V_{i,t,2}(\tau)]\right\}$. Note that
\[
V_{i,t,2}(\tau)-{\sf E}[V_{i,t,2}(\tau)]=\sum_{s=-\infty}^t\left\{{\sf E}\left[V_{i,t,2}(\tau)\big|\mathscr{F}_{s-1}\right]-{\sf E}\left[V_{i,t,2}(\tau)\big|\mathscr{F}_{s-2}\right]\right\}.
\]
For any $s$, by Jensen's inequality and the triangle inequality, we have
\begin{eqnarray}
&&\left\Vert\bigg\vert{\sf E}\left[V_{i,t,2}(\tau)\big|\mathscr{F}_{s-1}\right]-{\sf E}\left[V_{i,t,2}(\tau)\big|\mathscr{F}_{s-2}\right]\bigg\vert_\infty\right\Vert_{q/2}\notag\\
&\le&\left\Vert\bigg\vert\left(\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{s-1\}}\right)\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q/2}+\left\Vert\bigg\vert \widetilde{X}_{i,t-1,\{r-1\}}^{\{s-1\}}\left(\widetilde{X}_{i,t-1}-\widetilde{X}_{i,t-1}^{\{s-1\}}\right)^{^\intercal}K_{t,0}(\tau)\bigg\vert_\infty\right\Vert_{q/2}\notag\\
&\le&C\left(\delta_{i,t-s,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,t-s,q}\right).\notag
\end{eqnarray}
Consequently,
\[
\left\Vert\bigg\vert V_{i,t,2}(\tau)-{\sf E}[V_{i,t,2}(\tau)]\bigg\vert_\infty\right\Vert_{q/2}\le C\sum_{s=-\infty}^t \left(\delta_{i,t-s,q}+\sum_{j=1,\neq i}^N\widetilde w_{ij}\delta_{j,t-s,q}\right)\le C\max_{1\leq i\leq N}\Delta_{i,0,q},
\]
and
\begin{equation}\label{eqB.9}
\sum_{t=T_1(\tau)}^{T_2(\tau)}{\sf E}\bigg\vert V_{i,t,2}(\tau)-{\sf E}V_{i,t,2}(\tau)\bigg\vert_\infty^{q/2}\le  C(Th)\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}^{q/2}.
\end{equation}
By the independence of $\varepsilon_{i,t}$ over $t$, $V_{i,t,2}(\tau)$ and $V_{i,t^\prime,2}(\tau)$ are independent for $\vert t-t^\prime\vert>2$. Following the proof of (\ref{eqB.8}) and using (\ref{eqB.9}), we have
\begin{eqnarray}
&&{\sf P}\left(\bigg\vert\sum_{t=T_1(\tau)}^{T_2(\tau)}\left\{V_{i,t,2}(\tau)-{\sf E}[V_{i,t,2}(\tau)]\right\}\bigg\vert_\infty\ge z\right)\notag\\
&\le&C\left(\frac{Th}{z^{q/2}}\left(\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}^{q/2}\right)+\exp\left\{-\frac{z^2}{C(Th)}\left(\max\limits_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{4,\alpha}^{2}\right)^{-1}\right\}\right).\label{eqB.10}
\end{eqnarray}
A combination of (\ref{eqB.5}), (\ref{eqB.7}), (\ref{eqB.8}) and (\ref{eqB.10}) yields that 
\begin{equation}\label{eqB.11}
{\sf P}\left(\bigg\vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\bigg\vert_\infty>\frac{z}{Th}\right)\le C\left(\frac{Th}{z^{q/2}}\left(\max_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}^{q/2}\right)+\exp\left\{-\frac{z^2}{C(Th)}\left(\max\limits_{1\leq i\leq N}\left\Vert x_{i\bullet}\right\Vert_{4,\alpha}^{2}\right)^{-1}\right\}\right).
\end{equation}
By the discussion in Section \ref{sec2.2}, $\max_{1\leq i\leq N}\left(\left\Vert x_{i\bullet}\right\Vert_{q,\alpha}+\left\Vert x_{i\bullet}\right\Vert_{4,\alpha}\right)$ is bounded. Setting $z=\eta\sqrt{Th\log N}$ in (\ref{eqB.11}), we may show that
\begin{eqnarray}
&&{\sf P}\left(\max_{1\le i\le N}\bigg\vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\bigg\vert_\infty>\eta\sqrt{\frac{\log N}{Th}}\right)\notag\\
&\le&\sum_{i=1}^N{\sf P}\left(\bigg\vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\bigg\vert_\infty>\eta\sqrt{\frac{\log N}{Th}}\right)\notag\\
&\le&CN\left\{\exp\left(-\frac{\eta^2Th\log N}{C Th}\right)+\frac{CTh}{(\eta\sqrt{Th\log N})^{q/2}}\right\}\notag\\
&=&O\left(N\exp\left\{-(\eta^2/C)\log N\right\}\right)+O\left(\frac{N}{\eta^{q/2}(Th)^{q/4-1}(\log N)^{q/4}}\right).\label{eqB.12}
\end{eqnarray}
Choosing $\eta$ to be sufficiently large and using Assumption \ref{ass:2}(iii), we have
\[
{\sf P}\left(\max_{1\le i\le N}\bigg\vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\bigg\vert_\infty>\eta\sqrt{\frac{\log N}{Th}}\right)\rightarrow0.
\]
This completes the proof of (\ref{eqB.4}).

We next turn to the proof of Lemma \ref{lem:B.1}(ii). As in the proof of Lemma \ref{lem:B.1}(i), we only prove that
\begin{equation}\label{eqB.13}
\max_{1\le i\le N}\sup_{\tau\in[0,1]}\left\vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\right\vert_\infty=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right).
\end{equation}
Let $\xi_{NT}=T^{(q-2)/[2(q+2)]}\left[h\log(N\vee T)\right]^{1/2}$. We consider covering the closed interval $[0,1]$ by some disjoint sub-intervals ${\mathcal I}_b,1\le b\le B$, with centres $\tau_b$ and length $\gamma_{NT}=h^2\sqrt{\log(N\vee T)/{Th}}/\xi_{NT}$. The number $B$ is upper bounded by $\xi_{NT}/({h^2\sqrt{\log(N\vee T)/{Th}}})$. Then we have
\begin{eqnarray}
&&\max_{1\le i\le N}\sup_{\tau\in[0,1]}\left\vert \widehat{\boldsymbol\Delta}_i(\tau)-{\boldsymbol\Delta}_{i,T}(\tau)\right\vert_\infty\notag\\
&\le&\max_{1\le i\le N}\max_{1\le b\le B}\left\vert\frac{1}{Th}\sum_{t=1}^T\left\{\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}-{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right]\right\}K_{t,0}(\tau_b)\right\vert_\infty+\notag\\
&&\max_{1\le i\le N}\max_{1\le b\le B}\sup_{\tau\in{\cal I}_b}\left\vert\frac{1}{Th}\sum_{t=1}^T\left\{\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}-{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right]\right\}\left[K_{t,0}(\tau)-K_{t,0}(\tau_b)\right]\right\vert_\infty.\notag
\end{eqnarray}

We first prove that
{\small\begin{equation}\label{eqB.14}
\max_{1\le i\le N}\max_{1\le b\le B}\sup_{\tau\in{\cal I}_b}\left\vert\frac{1}{Th}\sum_{t=1}^T\left\{\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}-{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right]\right\}\left[K_{t,0}(\tau)-K_{t,0}(\tau_b)\right]\right\vert_\infty=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right).
\end{equation}}
By Assumption \ref{ass:2}(i), we readily have that
\[
\max_{1\le b\le B}\sup_{\tau\in{\cal I}_b}\left\vert K_{t,0}(\tau)-K_{t,0}(\tau_b)\right\vert=O_P\left(\gamma_{NT}h^{-2}\right).
\]
Define $V_{i,t}^X=\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}$, 
\[
\overline{V}_{i,t}^X=\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}I\left\{\Vert\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\Vert\le \xi_{NT}\right\}~~\text{and}~~\widetilde{V}_{i,t}^X=V_{i,t}^X-\overline{V}_{i,t}^X.
\] 
Then, we may show that
\begin{eqnarray}
&&\max_{1\le i\le N}\max_{1\le b\le B}\sup_{\tau\in{\cal I}_b}\left\vert\frac{1}{Th}\sum_{t=1}^T\left\{\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}-{\sf E}\left[\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right]\right\}\left[K_{t,0}(\tau)-K_{t,0}(\tau_b)\right]\right\vert_\infty\notag\\
&\le&\gamma_{NT}h^{-2}\left\{\max_{1\le i\le N}\frac{1}{T}\sum_{t=1}^T\left\vert \overline{V}_{i,t}^X-{\sf E}\big[\overline{V}_{i,t}^X\big]\right\vert_\infty+\max_{1\le i\le N} \frac{1}{T}\sum_{t=1}^T\left\vert\widetilde{V}_{i,t}^X-{\sf E}\big[\widetilde{V}_{i,t}^X\big]\right\vert_\infty\right\}\notag\\
&=&O_P\left(\sqrt{\frac{\log(N\vee T)}{Th}}\right)+\gamma_{NT}h^{-2}\max_{1\le i\le N} \frac{1}{T}\sum_{t=1}^T\left\vert\widetilde{V}_{i,t}^X-{\sf E}\big[\widetilde{V}_{i,t}^X\big]\right\vert_\infty,\label{eqB.15}
\end{eqnarray}
and, for any $\eta>0$,
\begin{eqnarray}
&&{\sf P}\left(\max_{1\le i\le N} \frac{1}{T}\sum_{t=1}^T\left\vert\widetilde{V}_{i,t}^X-{\sf E}\big[\widetilde{V}_{i,t}^X\big]\right\vert_\infty>\eta \xi_{NT}\right)\notag\\
&\le&\sum_{i=1}^N\sum_{t=1}^T{\sf P}\left(\left\Vert\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\right\Vert>\xi_{NT}\right)\notag\\
&=&O\left(NT\xi_{NT}^{-q/2}\right)=o(1)\label{eqB.16}
\end{eqnarray}
by Assumptions \ref{ass:1}(iii) and \ref{ass:2}(iii). With (\ref{eqB.15}) and (\ref{eqB.16}), we prove (\ref{eqB.14}).

It remains to show that
\begin{equation}\label{eqB.17}
\max_{1\le i\le N}\max_{1\le b\le B}\left\vert \widehat{\boldsymbol\Delta}_i(\tau_b)-{\boldsymbol\Delta}_{i,T}(\tau_b)\right\vert_\infty=O_p\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right).
\end{equation}
Following the same arguments as Lemma \ref{lem:B.1}(i), we have for any $\eta>0$,
\begin{eqnarray}
&&{\sf P}\left(\max_{1\le i\le N}\max_{1\le b\le B}\left\vert \widehat{\boldsymbol\Delta}_i(\tau_b)-{\boldsymbol\Delta}_{i,T}(\tau_b)\right\vert_\infty>\eta\sqrt{\frac{\log (N\vee T)}{Th}}\right)\notag\\
&\le&\sum_{i=1}^N\sum_{b=1}^B{\sf P}\left(\left\vert \widehat{\boldsymbol\Delta}_i(\tau_b)-{\boldsymbol\Delta}_{i,T}(\tau_b)\right\vert_\infty>\eta\sqrt{\frac{\log (N\vee T)}{Th}}\right)\notag\\
&=&O\left(NB\exp\left\{-C\eta^2\log (N\vee T)\right\}\right)+O\left(\frac{NB}{\eta^{q/2}(Th)^{q/4-1}(\log (N\vee T))^{q/4}}\right).\label{eqB.18}
\end{eqnarray}
Noting that $B$ diverge at a certain polynomial rate of $T$ and letting $\eta$ be sufficiently large, the first order on the right side of (\ref{eqB.18}) converges to zero. By Assumption \ref{ass:2}(iii), we may also show that the second order also converges to zero. Hence, we complete the proof of (\ref{eqB.17}). By virtue of (\ref{eqB.14}) and (\ref{eqB.17}), we prove (\ref{eqB.13}). The proof of Lemma \ref{lem:B.1} is completed.\hfill$\Box$

\smallskip

\begin{lemma}\label{lem:B.2}

Suppose that the conditions of Proposition \ref{prop:3.1} are satisfied. Then we have
	
(i) For any fixed $\tau\in(0,1)$,
\begin{equation}\label{eqB.19}
\max_{1\le i,j\le N}\left\Vert\widehat{\boldsymbol\Delta}_{ij}(\tau)-{\boldsymbol\Delta}_{ij,T}(\tau)\right\Vert=O_P\left(\sqrt{\frac{\log N}{Th}}\right),
\end{equation}
and (ii)
\begin{equation}\label{eqB.20}
\max_{1\le i,j\le N}\sup_{\tau\in[0,1]}\left\Vert\widehat{\boldsymbol\Delta}_{ij}(\tau)-{\boldsymbol\Delta}_{ij,T}(\tau)\right\Vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right),
\end{equation}
where $\widehat{\boldsymbol\Delta}_{ij}(\tau)$ and ${\boldsymbol\Delta}_{ij,T}(\tau)$ are defined in Section 3.1.1.

\end{lemma}

\smallskip

\noindent{\bf Proof of Lemma \ref{lem:B.2}}.\ \ The proofs of (\ref{eqB.19}) and (\ref{eqB.20}) are similar to the proofs of (\ref{eqB.4}) and (\ref{eqB.13}), respectively. Details are omitted here.\hfill$\Box$

\smallskip


\begin{lemma}\label{lem:B.3}

Suppose that the conditions of Theorem \ref{thm:3.2} are satisfied. Then we have
\begin{equation}\label{eqB.21}
\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert \widehat\beta_{i}(\tau_k^\ast)-\beta_{i}(\tau_k^\ast)\right\vert=O_P\left(\sqrt{\frac{\log N}{Th}}+h^2\right),
\end{equation}
where the number of grid points $L$ satisfies $L=O(T)$.

\end{lemma}

\smallskip

\noindent{\bf Proof of Lemma \ref{lem:B.3}}.\ \ Letting ${\boldsymbol\Gamma}_{i,X}(\cdot), {\boldsymbol\Gamma}_{i,\beta}(\cdot)$ and ${\boldsymbol\Gamma}_{i,\varepsilon}(\cdot)$ be defined as in the proof of Proposition \ref{prop:3.1},
\begin{equation}\label{eqB.22}
\widehat\beta_{i}(\tau_k^\ast)-\beta_{i}(\tau_k^\ast)={\boldsymbol E}_\ast\cdot{\boldsymbol\Gamma}_{i,X}^{-1}(\tau_k^\ast)\left[{\boldsymbol\Gamma}_{i,\beta}(\tau_k^\ast)+{\boldsymbol\Gamma}_{i,\varepsilon}(\tau_k^\ast)\right]
\end{equation} 
as in (\ref{eqA.1}). Following the proof of Lemma \ref{lem:B.1}(ii), we may show that 
\begin{equation}\label{eqB.23}
\max_{1\le i\le N}\max_{1\leq k\leq L}\left\Vert {\boldsymbol\Gamma}_{i,X}^{-1}(\tau_k^\ast)\right\Vert=O_P(1),\ \ \max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert {\boldsymbol\Gamma}_{i,\beta}(\tau_k^\ast)\right\vert=O_P\left(h^2\right).
\end{equation}
In view of (\ref{eqB.22}) and (\ref{eqB.23}), in order to prove (\ref{eqB.21}), we only need to show that 
\[
\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert {\boldsymbol\Gamma}_{i,\varepsilon}(\tau_k^\ast)\right\vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right).
\]
To save the space, we only provide the proof of
\begin{equation}\label{eqB.24}
\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert\frac{1}{Th}\sum_{t=1}^T\widetilde{X}_{i,t-1}\varepsilon_{i,t}K_{t,0}(\tau_k^\ast)\right\vert=O_P\left(\sqrt{\frac{\log (N\vee T)}{Th}}\right).
\end{equation}

We next use the truncation technique and \cite{Fr75}'s concentration inequality for martingale to prove (\ref{eqB.24}). Let $\xi_{NT}=T^{(q-2)/[2(q+2)]}\left[h\log(N\vee T)\right]^{1/2}$ be defined as in the proof of Lemma \ref{lem:B.1}(ii), $W_{i,t}^{X,\varepsilon}=\widetilde{X}_{i,t-1}\varepsilon_{i,t}$, 
\[
\overline{W}_{i,t}^{X,\varepsilon}=\widetilde{X}_{i,t-1}\varepsilon_{i,t}I\left\{\vert\widetilde{X}_{i,t-1}\vert\le \xi_{NT}^{1/2},\ \vert\varepsilon_{i,t}\vert\le\xi_{NT}^{1/2}\right\}~~\text{and}~~\widetilde{W}_{i,t}^{X,\varepsilon}=W_{i,t}^{X,\varepsilon}-\overline{W}_{i,t}^{X,\varepsilon}.
\] 
Note that
\begin{eqnarray}
&&\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert\frac{1}{Th}\sum_{t=1}^T\widetilde{X}_{i,t-1}\varepsilon_{i,t}K_{t,0}(\tau_k^\ast)\right\vert\notag\\
&=&\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert\frac{1}{Th}\sum_{t=1}^T\left[\overline{W}_{i,t}^{X,\varepsilon}-{\sf E}\left(\overline{W}_{i,t}^{X,\varepsilon}\right)\right]K_{t,0}(\tau_k^\ast)\right\vert+\notag\\
&&\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert\frac{1}{Th}\sum_{t=1}^T\left[\widetilde{W}_{i,t}^{X,\varepsilon}-{\sf E}\left(\widetilde{W}_{i,t}^{X,\varepsilon}\right)\right]K_{t,0}(\tau_k^\ast)\right\vert.\notag
\end{eqnarray}
For any $\eta>0$, by Markov's inequality, Assumption \ref{ass:2}(iii) and $L=O(T)$, 
\begin{eqnarray}
&&{\sf P}\left(\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert\frac{1}{Th}\sum_{t=1}^T\left[\widetilde{W}_{i,t}^{X,\varepsilon}-{\sf E}\left(\widetilde{W}_{i,t}^{X,\varepsilon}\right)\right]K_{t,0}(\tau_k^\ast)\right\vert>\eta \sqrt{\frac{\log (N\vee T)}{Th}}\right)\notag\\
&\leq&{\sf P}\left(\max_{1\le i\le N}\max_{1\leq k\leq L}\vert\widetilde{X}_{i,t-1}\vert> \xi_{NT}^{1/2}\right)+{\sf P}\left(\max_{1\le i\le N}\max_{1\leq k\leq L}\vert\varepsilon_{i,t}\vert> \xi_{NT}^{1/2}\right)\notag\\
&\leq&\sum_{i=1}^N\sum_{k=1}^L{\sf P}\left(\vert\widetilde{X}_{i,t-1}\vert> \xi_{NT}^{1/2}\right)+\sum_{i=1}^N\sum_{k=1}^L{\sf P}\left(\vert\varepsilon_{i,t}\vert> \xi_{NT}^{1/2}\right)\notag\\
&=&O\left(NL\xi_{NT}^{-q/2}\right)=o(1).\label{eqB.25}
\end{eqnarray}
On the other hand, by \cite{Fr75}'s concentration inequality for martingale, we have
\begin{eqnarray}
&&{\sf P}\left(\max_{1\le i\le N}\max_{1\leq k\leq L}\left\vert\frac{1}{Th}\sum_{t=1}^T\left[\overline{W}_{i,t}^{X,\varepsilon}-{\sf E}\left(\overline{W}_{i,t}^{X,\varepsilon}\right)\right]K_{t,0}(\tau_k^\ast)\right\vert>\eta \sqrt{\frac{\log (N\vee T)}{Th}}\right)\notag\\
&\leq&\sum_{i=1}^N\sum_{k=1}^L{\sf P}\left(\left\vert\frac{1}{Th}\sum_{t=1}^T\left[\overline{W}_{i,t}^{X,\varepsilon}-{\sf E}\left(\overline{W}_{i,t}^{X,\varepsilon}\right)\right]K_{t,0}(\tau_k^\ast)\right\vert>\eta \sqrt{\frac{\log (N\vee T)}{Th}}\right)\notag\\
&=&O\left(NL\exp\left\{-\frac{\eta^2(Th)\log(N\vee T)}{CTh}\right\}\right)=o(1),\label{eqB.26}
\end{eqnarray}
by letting $\eta$ be sufficiently large. With (\ref{eqB.25}) and (\ref{eqB.26}), we complete the proof of (\ref{eqB.24}).\hfill$\Box$

\smallskip

\begin{lemma}\label{lem:B.4}

 Suppose that the conditions of Theorem \ref{thm:3.3} are satisfied. Then we have
\begin{equation}\label{eqB.27}
{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K),1\le K\le K_0-1\right)\rightarrow1
\end{equation}
conditional on the event ${\cal E}({\mathscr G})$.

\end{lemma}

\smallskip

\noindent{\bf Proof of Lemma \ref{lem:B.4}}.\ \ Without loss of generality, we only consider $K=K_0-1$ because the other scenarios can be dealt with in the same manner. Conditional on ${\cal E}({\mathscr G})$, two clusters from ${\mathscr G}_1,\cdots,{\mathscr G}_{K_0}$ are falsely merged when the clustering algorithm terminates at $K_0-1$. Without loss of generality we assume that ${\mathscr G}_{K_0-1}$ and ${\mathscr G}_{K_0}$ are falsely merged, i.e., $\widehat{{\mathscr G}}_k={\mathscr G}_k,k=1,\cdots,K_0-2$ and $\widehat{{\mathscr G}}_{K_0-1}={\mathscr G}_{K_0-1}^\ast:={\mathscr G}_{K_0-1}\cup{\mathscr G}_{K_0}$. Let $\alpha_{k|K_0-1,1}(\cdot)=\alpha_{k,1}(\cdot), \alpha_{k|K_0-1,2}(\cdot)=\alpha_{k,2}(\cdot), k=1,\cdots,K_0-2$, and write $\alpha_{k|K_0-1}(\cdot)=[\alpha_{k|K_0-1,1}(\cdot),\alpha_{k|K_0-1,2}(\cdot)]^{^\intercal}$. Let $\alpha_{K_0-1}^\ast(\cdot)=[\alpha_{K_0-1,1}^\ast(\cdot),\alpha_{K_0-1,2}^\ast(\cdot)]^{^\intercal}$ be the vector of ``pseudo" coefficient functions associated with ${\mathscr G}_{K_0-1}^\ast$.

For $i\in{\mathscr G}_k,k=1,\cdots,K_0-2$, following the proof of Lemma \ref{lem:B.3}, we may show that
\[
\max_{1\le k\le K_0-2}\max_{1\leq t\leq T}\left\vert\widehat{\alpha}_{k|K_0-1}(\tau_t)-\alpha_{k|K_0-1}(\tau_t)\right\vert=O_P\left(h_1^2+\sqrt{\frac{\log (N\vee T)}{Th_1}}\right),
\]
indicating that
\begin{equation}\label{eqB.28}
\frac{1}{NT}\sum_{k=1}^{K_0-2}\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\left[x_{i,t}-\widehat{\alpha}_{k|K_0-1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}\right]^2=\frac{1}{NT}\sum_{k=1}^{K_0-2}\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\varepsilon_{i,t}^2+o_P(1).
\end{equation}

Conditional on ${\cal E}({\mathscr G})$, for $i\in{\mathscr G}_{K_0-1}$, we write
\begin{equation}\label{eqB.29}
x_{i,t}-\widehat{\alpha}_{K_0-1|K_0-1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}=\varepsilon_{i,t}-\left[\widehat{\alpha}_{K_0-1|K_0-1}(\tau_t)-\alpha_{K_0-1}^\ast(\tau_t)\right]^{^\intercal}\widetilde{X}_{i,t-1}
\end{equation}
and
\[
\widehat{\alpha}_{K_0-1|K_0-1}(\tau)-\alpha_{K_0-1}^\ast(\tau)=\boldsymbol{E}_\ast\cdot{\boldsymbol\Gamma}_{\ast,X}^{-1}(\tau)\left[{\boldsymbol\Gamma}_{\ast,\beta}(\tau)+{\boldsymbol\Gamma}_{\ast,\varepsilon}(\tau)\right],
\]
where $\boldsymbol{E}_\ast$ is defined as in (\ref{eqA.1}),
\begin{eqnarray}
{\boldsymbol\Gamma}_{\ast,X}(\tau)&=&\left[
\begin{array}{cc}
\frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,0}^{\dagger}(\tau) & \frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,1}^{\dagger}(\tau) \\
\frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,1}^{\dagger}(\tau) & \frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}K_{t,2}^{\dagger}(\tau)
\end{array}
\right],\notag\\
{\boldsymbol\Gamma}_{\ast,\beta}(\tau)&=&\left[
\begin{array}{c}
\frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\left[\beta_i(\tau_t)-\beta_i(\tau)-\beta_i^\prime(\tau)(\tau_t-\tau)\right]K_{t,0}^{\dagger}(\tau) \\
\frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}\left[\beta_i(\tau_t)-\beta_i(\tau)-\beta_i^\prime(\tau)(\tau_t-\tau)\right]K_{t,1}^{\dagger}(\tau)
\end{array}
\right],\notag\\
{\boldsymbol\Gamma}_{\ast,\varepsilon}(\tau)&=&\left[
\begin{array}{c}
\frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\varepsilon_{i,t} K_{t,0}^{\dagger}(\tau) \\
\frac{1}{Th_1}\sum_{i\in{\mathscr G}_{K_0-1}^\ast}\sum_{t=1}^T \widetilde{X}_{i,t-1}\varepsilon_{i,t} K_{t,1}^{\dagger}(\tau)
\end{array}
\right],\notag
\end{eqnarray}
$K_{t,j}^{\dagger}(\tau)=\left(\frac{\tau_t-\tau}{h_1}\right)^jK\left(\frac{\tau_t-\tau}{h_1}\right)$. Letting ${\boldsymbol\Delta}_{{\mathscr G}_k}(\tau)=\frac{1}{|{\mathscr G}_k|}\sum_{i\in{\mathscr G}_k}{\boldsymbol\Delta}_i(\tau),1\le k\le K_0$, we may explicitly write the ``pseudo" time-varying coefficient vector as
\[
\alpha_{K_0-1}^{\ast}(\tau)=\left[|{\mathscr G}_{K_0-1}|{\boldsymbol\Delta}_{{\mathscr G}_{K_0-1}}(\tau)+|{\mathscr G}_{K_0}|{\boldsymbol\Delta}_{{\mathscr G}_{K_0}}(\tau)\right]^{-1}\left[|{\mathscr G}_{K_0-1}|{\boldsymbol\Delta}_{{\mathscr G}_{K_0-1}}(\tau)\alpha_{K_0-1}(\tau)+|{\mathscr G}_{K_0}|{\boldsymbol\Delta}_{{\mathscr G}_{K_0}}(\tau)\alpha_{K_0}(\tau)\right],
\]
a weighted average of $\alpha_{K_0-1}(\tau)$ and $\alpha_{K_0}(\tau)$. Following the proof of Lemma \ref{lem:B.3}, we may show that
\begin{equation}\label{eqB.30}
\max_{1\leq t\leq T}\left\vert\widehat{\alpha}_{K_0-1|K_0-1}(\tau_t)-\alpha_{K_0-1}^\ast(\tau_t)\right\vert=O_P\left(h_1^2+\sqrt{\frac{\log (N\vee T)}{Th_1}}\right).
\end{equation}
By (\ref{eqB.29}) and (\ref{eqB.30}), letting $d_{\alpha,1}(\tau)=\alpha_{K_0-1}(\tau)-\alpha_{K_0-1}^\ast(\tau)$, we have
\begin{eqnarray}
&&\frac{1}{NT}\sum_{i\in{\mathscr G}_{K_0-1}}\sum_{t=1}^T\left[x_{i,t}^\dagger-\widehat{\alpha}_{K_0-1|K_0-1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}\right]^2\notag\\
&=&\frac{1}{NT}\sum_{i\in{\mathscr G}_{K_0-1}}\sum_{t=1}^T\varepsilon_{i,t}^2+\frac{1}{NT}\sum_{i\in{\mathscr G}_{K_0-1}}\sum_{t=1}^Td_{\alpha,1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}\widetilde{X}_{i,t-1}^{^\intercal}d_{\alpha,1}(\tau_t)+\notag\\
&&\frac{2}{NT}\sum_{i\in{\mathscr G}_{K_0-1}}\sum_{t=1}^T\varepsilon_{i,t}d_{\alpha,1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}+o_P(1)\notag\\
&\ge&\frac{1}{NT}\sum_{i\in{\mathscr G}_{K_0-1}}\sum_{t=1}^T\varepsilon_{i,t}^2+(\underline{\lambda}\cdot\kappa_1)\int_{0}^{1}\left\vert d_{\alpha,1}(\tau)\right\vert^2d\tau+o_P(1),\label{eqB.31}
\end{eqnarray}
where $\underline{\lambda}$ and $\kappa_1$ are defined in Assumptions \ref{ass:2}(ii) and \ref{ass:3}(i), respectively. Analogously, we can also prove that 
\begin{equation}\label{eqB.32}
\frac{1}{NT}\sum_{i\in{\mathscr G}_{K_0}}\sum_{t=1}^T\left[x_{i,t}-\widehat{\alpha}_{K_0-1|K_0-1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}\right]^2\ge\frac{1}{NT}\sum_{i\in{\mathscr G}_{K_0}}\sum_{t=1}^T\varepsilon_{i,t}^2+(\underline{\lambda}\cdot\kappa_1)\int_{0}^{1}\left\vert d_{\alpha,2}(\tau)\right\vert^2d\tau+o_P(1),
\end{equation}
where $d_{\alpha,2}(\tau)=\alpha_{K_0}(\tau)-\alpha_{K_0-1}^\ast(\tau)$. By (\ref{eqB.28}), (\ref{eqB.31}), (\ref{eqB.32}) and Assumption \ref{ass:3}(i)(ii), we have
\begin{eqnarray}
{\sf IC}(K_0-1)&=&\log \widehat{\sigma}_{NT}^2(K_0-1)+(K_0-1)\cdot\rho_{NT}\notag\\
&\ge&\log\left\{\frac{1}{NT}\sum_{k=1}^{K_0}\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\varepsilon_{i,t}^2+(\underline{\lambda}\kappa_1)\int_{0}^{1}\left[\left\vert d_{\alpha,1}(\tau)\right\vert^2+\left\vert d_{\alpha,2}(\tau)\right\vert^2\right]d\tau\right\}+\notag\\
&&(K_0-1)\cdot\rho_{NT}+o_P(1)\notag\\
&\ge&\log\left\{\frac{1}{NT}\sum_{k=1}^{K_0}\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\varepsilon_{i,t}^2+\frac{1}{2}\left(\underline{\lambda}\kappa_1\kappa_2\right)\right\}+(K_0-1)\cdot\rho_{NT}+o_P(1)\notag\\
&>&\log\left\{\frac{1}{NT}\sum_{k=1}^{K_0}\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\varepsilon_{i,t}^2\right\}+K_0\cdot\rho_{NT}+o_p(1)\notag\\
&=&\log\widehat{\sigma}_{NT}^2(K_0)+K_0\cdot\rho_{NT}+o_p(1)={\sf IC}(K_0)+o_P(1),\notag
\end{eqnarray}
completing the proof of Lemma \ref{lem:B.4}. \hfill$\Box$

\smallskip

\begin{lemma}\label{lem:B.5}

Suppose that the conditions of Theorem \ref{thm:3.3} are satisfied. Then we have
\begin{equation}\label{eqB.33}
{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K),K_0+1\le K\le \overline{K}\right)\rightarrow1
\end{equation}
conditional on the event ${\cal E}({\mathscr G})$.

\end{lemma}

\smallskip

\noindent{\bf Proof of Lemma \ref{lem:B.5}}.\ \ Without loss of generality, we only consider $K=K_0+1$ and prove that
\[
{\sf P}\left({\sf IC}(K_0)<{\sf IC}(K_0+1),{\cal E}({\mathscr G})\right)\rightarrow1.
\]
Conditional on ${\cal E}({\mathscr G})$, one of ${\mathscr G}_1,\cdots,{\mathscr G}_{K_0}$ is split into two sub-clusters when the clustering algorithm stops at $K_0+1$. Without loss of generality, we assume that ${\mathscr G}_{K_0}$ is divided into two sub-clusters and denote the resulting $K_0+1$ clusters as ${\mathscr G}_1^\diamond,\cdots,{\mathscr G}_{K_0}^\diamond,{\mathscr G}_{K_0+1}^\diamond$ with ${\mathscr G}_{k}^\diamond={\mathscr G}_k$ for $k=1,\cdots,K_0-1$ and ${\mathscr G}_{K_0}^\diamond\cup{\mathscr G}_{K_0+1}^\diamond={\mathscr G}_{K_0}$. Accordingly, we let $\alpha_k^\diamond(\cdot)=\alpha_k(\cdot)$ when $k=1,\cdots,K_0-1$ and $\alpha_k^\diamond(\cdot)=\alpha_{K_0}(\cdot)$ when $k=K_0,K_0+1$.

Observe that 
\[
\widehat{\sigma}_{NT}^2\left(K_0+1\right)=\frac{1}{NT}\sum_{k=1}^{K_0+1}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\left[x_{i,t}-\widehat{\alpha}_{k|K_0+1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}\right]^2.
\]
Let ${\boldsymbol\Gamma}_{k|K_0+1,X}(\tau), {\boldsymbol\Gamma}_{k|K_0+1,\beta}(\tau)$ and ${\boldsymbol\Gamma}_{k|K_0+1,\varepsilon}(\tau)$ be defined similarly to ${\boldsymbol\Gamma}_{i,X}(\tau),{\boldsymbol\Gamma}_{i,\beta}(\tau)$ and ${\boldsymbol\Gamma}_{i,\varepsilon}(\tau)$, respectively, but with $\sum_{t=1}^T$ and $h$ replaced by $\sum_{i\in{\mathscr G}_{k}^\diamond}\sum_{t=1}^T$ and $h_1$, respectively. Write ${\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}(\tau)=\frac{1}{\vert{\mathscr G}_k^\diamond\vert}\sum_{i\in{\mathscr G}_k^\diamond}{\boldsymbol\Delta}_i(\tau)$. For any $i\in{\mathscr G}_k^\diamond$, we write 
\begin{equation}\label{eqB.34}
x_{i,t}-\widehat{\alpha}_{k|K_0+1}^{^\intercal}(\tau_t)\widetilde{X}_{i,t-1}=\varepsilon_{i,t}-\left[\widehat{\alpha}_{k|K_0+1}(\tau_t)-\alpha_{k}^\diamond(\tau_t)\right]^{^\intercal}\widetilde{X}_{i,t-1}.
\end{equation}
By Lemma \ref{lem:B.1}(ii), (\ref{eqB.24}) and the bandwidth condition $\log(N\vee T)=O(Th_1^3)$ in Assumption \ref{ass:3}(ii), we may show that
\begin{eqnarray}
\widehat{\alpha}_{k|K_0+1}(\tau_t)-\alpha_k^\diamond(\tau_t)
&=&\boldsymbol{E}_\ast\cdot{\boldsymbol\Gamma}_{k|K_0+1,X}^{-1}(\tau_t)\left[{\boldsymbol\Gamma}_{k|K_0+1,\beta}(\tau_t)+{\boldsymbol\Gamma}_{k|K_0+1,\varepsilon}(\tau_t)\right]\notag\\
&=&{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{s=1}^T\widetilde{X}_{i,s-1}\varepsilon_{i,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]+O_P\left(h_1^2+\frac{\log (N\vee T)}{Th_1}\right)\notag\\
&=:&R_{k}(\tau_t)+O_P(h_1^2)\notag
\end{eqnarray}
uniformly over $\tau_t$, $t=1,\cdots,T$. By Assumption \ref{ass:2}(ii), we may show that
\begin{eqnarray}
&&\frac{1}{NT}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\left[\widetilde{X}_{i,t-1}^{^\intercal}R_{k}(\tau_t)\right]^2\notag\\
&\le&\frac{1}{NT}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\left\{\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s=1}^T\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]\right\}^2\notag\\
&\le&\underline{\lambda}^{-2}\cdot\frac{1}{NT}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\left\vert\widetilde{X}_{i,t-1}\right\vert^2\left\vert\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s=1}^T\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right\vert^2.\label{eqB.35}
\end{eqnarray}
By the Cauchy-Schwarz inequality, 
\begin{eqnarray}
&&\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T{\sf E}\left[\left\vert\widetilde{X}_{i,t-1}\right\vert^2\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s=1}^T\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right\vert^2\right]\notag\\
&\leq&\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\left\{{\sf E}\left[\left\vert\widetilde{X}_{i,t-1}\right\vert^4\right]\right\}^{1/2}\left\{{\sf E}\left[\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s=1}^T\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right\vert^4\right]\right\}^{1/2}\notag\\
&\leq&C\left\vert{\mathscr G}_k^\diamond\right\vert T \left({\sf E}\left[\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s=1}^T\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right\vert^4\right]\right)^{1/2}.\label{eqB.36}
\end{eqnarray}
By Burkholder's inequality for martingale differences \citep[e.g.,][]{B73}, the conditional H\"older's inequality and Assumption \ref{ass:3}(iii), we have
\begin{eqnarray}
&&{\sf E}\left[\left\vert\sum_{s=1}^T\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right\vert^4\right]\notag\\
&\leq&C\left({\sf E}\left\{\left[\sum_{s=1}^TK^2\left(\frac{\tau_s-\tau_t}{h_1}\right){\sf E}\left(\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s-1}\varepsilon_{j,s}\right\vert^2\big|{\mathscr F}_{s-1}\right)\right]^2\right\}+\right.\notag\\
&&\left.\sum_{s=1}^TK^4\left(\frac{\tau_s-\tau_t}{h_1}\right){\sf E}\left[\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s-1}\varepsilon_{j,s}\right\vert^4\right]\right)\notag\\
&\leq&C\sum_{s_1=1}^T\sum_{s_2=1}^TK^2\left(\frac{\tau_{s_1}-\tau_t}{h_1}\right)K^2\left(\frac{\tau_{s_2}-\tau_t}{h_1}\right){\sf E}\left[{\sf E}\left(\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s_1-1}\varepsilon_{j,s_1}\right\vert^2\big|{\mathscr F}_{s_1-1}\right)\right]\notag\\
&& \left[{\sf E}\left(\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s_2-1}\varepsilon_{j,s_2}\right\vert^2\big|{\mathscr F}_{s_2-1}\right) \right]+O\left(|{\mathscr G}_k^\diamond|^2Th_1\right)\notag\\
&\leq&C\left[\sum_{s=1}^TK^2\left(\frac{\tau_{s}-\tau_t}{h_1}\right)\left\{{\sf E}\left[{\sf E}\left(\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s-1}\varepsilon_{j,s}\right\vert^4\big|{\mathscr F}_{s-1}\right)\right]\right\}^{1/2}\right]^2+O\left(|{\mathscr G}_k^\diamond|^2Th_1\right)\notag\\
&=&C\left\{\sum_{s=1}^TK^2\left(\frac{\tau_{s}-\tau_t}{h_1}\right)\left[{\sf E}\left(\left\vert\sum_{j\in{\mathscr G}_k^\diamond}\widetilde{X}_{j,s-1}\varepsilon_{j,s}\right\vert^4\right)\right]^{1/2}\right\}^2+O\left(|{\mathscr G}_k^\diamond|^2Th_1\right)\notag\\
&=&O\left(|{\mathscr G}_k^\diamond|^2T^2h_1^2+|{\mathscr G}_k^\diamond|^2Th_1\right)=O\left(|{\mathscr G}_k^\diamond|^2T^2h_1^2\right),\notag
\end{eqnarray}
which, together with (\ref{eqB.35}) and (\ref{eqB.36}), leads to 
\begin{equation}\label{eqB.37}
\frac{1}{NT}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\left[\widetilde{X}_{i,t-1}^{^\intercal}R_{k}(\tau_t)\right]^2=O_P\left(\frac{\vert{\mathscr G}_k^\diamond\vert^2T^2h_1}{N\vert{\mathscr G}_k^\diamond\vert^2T^3h_1^2}\right)=O_P\left(\frac{1}{NTh_1}\right).
\end{equation}
On the other hand, note that
\begin{eqnarray}
&&\frac{1}{NT}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}R_{k}(\tau_t)\notag\\
&=&\frac{1}{NT}\sum_{t=2}^T\sum_{i\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s<t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]+\notag\\
&&\frac{1}{NT}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s>t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]+\notag\\
&&\frac{1}{N\vert{\mathscr G}_k^\diamond\vert T^2h_1}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k^\diamond}\sum_{j\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\varepsilon_{j,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\widetilde{X}_{j,t-1}K\left(0\right).\notag
\end{eqnarray}
As $\varepsilon_{i,t}$ is independent of $\widetilde{X}_{i,t-1}^{^\intercal}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s<t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}$, following the arguments in the proof of (\ref{eqB.37}) and using the first condition in (\ref{eq3.14}), we may show that 
\begin{eqnarray}
&&{\sf E}\left\{\frac{1}{NT}\sum_{t=2}^T\sum_{i\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s<t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]\right\}^2\notag\\
&=&\frac{1}{N^2\vert{\mathscr G}_k^\diamond\vert^2 T^4h_1^2}\sum_{t=2}^T{\sf E}\left\{\sum_{i\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s<t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]\right\}^2\notag\\
&=&\frac{1}{N^2\vert{\mathscr G}_k^\diamond\vert^2 T^4h_1^2}\sum_{i,j\in{\mathscr G}}|\sigma_{i,j}|\cdot O\left(|{\mathscr G}_k^\diamond|T^2h_1\right)=O\left(\frac{1}{N^2T^2h_1}\right),\notag
\end{eqnarray}
indicating that 
\[
\frac{1}{NT}\sum_{t=2}^T\sum_{i\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s<t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]=O_P\left(\frac{1}{NTh_1^{1/2}}\right).
\]
Similarly, we can also prove that 
\[
\frac{1}{NT}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k^\diamond}\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\left[\frac{1}{\vert{\mathscr G}_k^\diamond\vert Th_1}\sum_{j\in{\mathscr G}_k^\diamond}\sum_{s>t}\widetilde{X}_{j,s-1}\varepsilon_{j,s}K\left(\frac{\tau_s-\tau_t}{h_1}\right)\right]=O_P\left(\frac{1}{NTh_1^{1/2}}\right).
\]
By (\ref{eq3.14}), we have
\[
\frac{1}{N\vert{\mathscr G}_k^\diamond\vert T^2h_1}\sum_{t=1}^T\sum_{i\in{\mathscr G}_k^\diamond}\sum_{j\in{\mathscr G}_k^\diamond}{\sf E}\left[\vert\varepsilon_{i,t}\varepsilon_{j,t}\vert\right]{\sf E}\left[\left\vert\widetilde{X}_{i,t-1}^{^\intercal}{\boldsymbol\Delta}_{{\mathscr G}_k^\diamond}^{-1}(\tau_t)\widetilde{X}_{j,t-1}\right\vert\right]=O\left(\frac{1}{NTh_1}\right).
\]
Combining the above arguments, we prove that
\begin{equation}\label{eqB.38}
\frac{1}{NT}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\varepsilon_{i,t}\widetilde{X}_{i,t-1}^{^\intercal}R_{k}(\tau_t)=O_P\left(\frac{1}{NTh_1}\right).
\end{equation}

By (\ref{eqB.34}), (\ref{eqB.37}), (\ref{eqB.38}) and Assumption \ref{ass:3}(ii), we can prove that
\begin{eqnarray}
{\sf IC}\left(K_0+1\right)&= &\log\widehat{\sigma}_{NT}^2\left(K_0+1\right)+\left(K_0+1\right)\cdot\rho_{NT}\notag\\
&=&\log\left\{\frac{1}{NT}\sum_{k=1}^{K_0+1}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\varepsilon_{i,t}^2\right\}+O_P\left(\frac{1}{NTh_1}+h_1^4\right)+\left(K_0+1\right)\cdot\rho_{NT}\notag\\
&=&\log\left\{\frac{1}{NT}\sum_{k=1}^{K_0+1}\sum_{i\in{\mathscr G}_k^\diamond}\sum_{t=1}^T\varepsilon_{i,t}^2\right\}+\left(K_0+1\right)\cdot\rho_{NT}+o_P(\rho_{NT})\notag\\
&>&\log\left\{\frac{1}{NT}\sum_{k=1}^{K_0}\sum_{i\in{\mathscr G}_k}\sum_{t=1}^T\varepsilon_{i,t}^2\right\}+K_0\cdot\rho_{NT}+o_P\left(\rho_{NT}\right)\notag\\
&=&{\sf IC}\left(K_0\right)+o_P\left(\rho_{NT}\right),\notag
\end{eqnarray}
completing the proof of Lemma \ref{lem:B.5}. \hfill$\Box$}

\bigskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{xx}    

{\footnotesize

\harvarditem{Ando \harvardand\ Bai}{2017}{AB17} Ando, T. and Bai, J. (2017). Clustering huge number of time series: A panel data approach with high-dimensional predictors and factor structures. {\em Journal of the American Statistical Association} {\bf 112}, 1182--1198.

\harvarditem{Bai, Li \harvardand\ Lu}{2016}{BLL16} Bai, J., Li, K. and Lu, L. (2016). Estimation and inference of FAVAR models. {\em Journal of Business and Economic Statistics} {\bf 34}, 620--641.

\harvarditem{Bai \harvardand\ Ng}{2006}{BN06} Bai, J. and Ng, S. (2006). Confidence intervals for diffusion index forecasts and inference for factor-augmented regressions. {\em Econometrica} {\bf 74}, 1135--1150.

\harvarditem{Bai \harvardand\ Saranadasa}{1996}{BS96} Bai, Z. and Saranadasa, H. (1996). Effect of high dimension: By an example of a two sample problem. {\em Statistica Sinica} {\bf 6}, 311--329.

\harvarditem{Basu \harvardand\ Michailidis}{2015}{BM15} Basu, S. and Michailidis, G. (2015). Regularized estimation in sparse high-dimensional time series models. {\em The Annals of Statistics} {\bf 43}, 1535--1567.

\harvarditem{Beck, Hubrich \harvardand\ Marcellino}{2016}{BHM2016} Beck, G. W., Hubrich, K. and Marcellino, M. (2016). On the importance of sectoral and regional shocks for price-setting. {\em Journal of Applied Econometrics} {\bf 31}, 1234--1253.


\harvarditem{Bernanke, Boivin \harvardand\ Eliasz}{2005}{BBE05} Bernanke, B., Boivin, J. and Eliasz, P. S. (2005). Measuring the effects of monetary policy: a factor-augmented vector autoregressive (FAVAR) approach. {\em Quarterly Journal of Economics} {\bf 120}, 387--422.

\harvarditem{Billingsley}{1995}{BP95} Billingsley, P. (1995). {\em Probability and Measure} (3rd Edition). John Wiley and Sons. 

\harvarditem{Bonhomme \harvardand\ Manresa}{2015}{BM15} Bonhomme, S. and Manresa, E. (2015). Grouped patterns of heterogeneity in panel data. {\em Econometrica} {\bf 83}, 1147--1184.

\harvarditem{Burkholder}{1973}{B73} Burkholder, D. L. (1973). Distribution function inequalities for martingales. {\em The Annals of Probability} {\bf 1}, 19--42.

\harvarditem{Cai}{2007}{C07} Cai, Z. (2007). Trending time-varying coefficient time series models with serially correlated errors. {\em Journal of Econometrics} {\bf 136}, 163--188.

\harvarditem{Casas  {\em et al}}{2021}{Casas2021} Casas, I., Gao, J., Peng, B. and Xie, S. (2021). Trending time-varying coefficient time series models with serially correlated errors. {\em Journal of Applied Econometrics} {\bf 36}, 328--345.

\harvarditem{Chen, Fan \harvardand\ Zhu}{2022}{CFZ22} Chen, E., Fan, J. and Zhu, X. (2022). Community network autoregression for high-dimensional time series. Forthcoming in {\em Journal of Econometrics}. 

\harvarditem{Chen}{2019}{Ch19} Chen, J. (2019). Estimating latent group structure in time-varying coefficient panel data models. {\em Econometrics Journal} {\bf 22}, 223--240. 

\harvarditem{Chen {\em et al}}{2023}{CLLL23} Chen, J., Li, D., Li, Y. and Linton, O. (2023). Estimating time-varying networks for high-dimensional time series. Working paper available at \url{https://arxiv.org/abs/2302.02476}.

\harvarditem{Chen \harvardand\ Wu}{2019}{CW19} Chen, L. and Wu. W. (2019). Testing for trends in high-dimensional time series. {\em Journal of the American Statistical Association} {\bf 114}, 869--881.

\harvarditem{Davis, Zang \harvardand\ Zheng}{2016}{DZZ16} Davis, R., Zang, P. and Zheng, T. (2016). Sparse vector autoregressive modeling. {\em Journal of Computational and Graphical Statistics} {\bf 25}, 1077--1096.

\harvarditem{Degras {\em et al}}{2012}{DXZW12} Degras, D., Xu, Z., Zhang, T. and Wu, W. (2012). Testing for parallelism among trends in multiple time series. {\em IEEE Transactions on Signal Processing} {\bf 60}, 1087--1097.

\harvarditem{Ding, Qiu \harvardand\ Chen}{2017}{DQC17} Ding, X., Qiu, Z. and Chen, X. (2017). Sparse transition matrix estimation for high-dimensional and locally stationary vector autoregressive models. {\em Electronic Journal of Statistics} {\bf 11}, 3871--3902.

\harvarditem{Everitt {\em et al}}{2011}{ELLS11}Everitt, B. S., Landau, S., Leese, M. and Stahl, D. (2011). {\em Cluster Analysis} (5th Edition). Wiley Series in Probability and Statistics, Wiley.

\harvarditem{Fan \harvardand\ Gijbels}{1996}{FG96} Fan, J. and Gijbels, I. (1996). {\em Local Polynomial Modelling and Its Applications}. Chapman and Hall/CRC.

\harvarditem{Feng, Gao \harvardand\ Peng}{2022}{FGP2022} Feng, G., Gao, J. and Peng, B. (2022). An integrated panel data approach to modelling economic growth. {\em Journal of Econometrics} {\bf 228}, 379--397.

\harvarditem{Freedman}{1975}{Fr75} Freedman D.A. (1975). On tail probabilities for martingales. {\em The Annals of Probability} {\bf 3}, 100--118.

\harvarditem{Gao \harvardand\ Gijbels}{2008}{GG08} Gao, J. and Gijbels I. (2008). Bandwidth selection in nonparametric kernel testing. {\em Journal of the American Statistical Association} {\bf 103}, 1584--1594.

\harvarditem{Gao {\em et al}}{2009}{GKLT09} Gao, J., King, M., Lu, Z. and Tj\o stheim, D. (2009). Nonparametric specification testing for nonlinear time series with nonstationarity. {\em Econometric Theory} {\bf 25}, 1869--1892.

\harvarditem{Gu{\dh}mundsson \harvardand\ Brownlees}{2021}{GB21} Gu{\dh}mundsson, G. and Brownlees C. (2021). Detecting groups in large vector autoregressions. {\em Journal of Econometrics} {\bf 225}, 2--26.

\harvarditem {Hall \harvardand\ Heyde}{1980}{HH80} Hall, P. and Heyde, C.C. (1980). {\em Martingale limit theory and its application}. Academic press.

\harvarditem{Hastie, Tibshirani \harvardand\ Friedman}{2009}{HTF09} Hastie, T., Tibshirani, R. and Friedman, J. (2009). {\em The Elements of Statistical Learning: Data Mining, Inference and Prediction} (2nd Edition). Springer-Verlag, New York.

\harvarditem{Kilian and L\"utkepohl}{2017}{KL17} Kilian, K. and L\"utkepohl, H. (2017). {\em Structural Vector Autoregressive Analysis}. Cambridge University Press.

\harvarditem{Ke, Li \harvardand\ Zhang}{2016}{KLZ16}Ke, Y., Li, J. and Zhang, W. (2016). Structure identification in panel data analysis. {\em The Annals of Statistics} {\bf 44}, 1193--1233.

\harvarditem{Kock \harvardand\ Callot}{2015}{KC15} Kock, A.B. and Callot, L. (2015). Oracle inequalities for high dimensional vector autoregressions. {\em Journal of Econometrics} {\bf 186}, 325--344.

\harvarditem{Li, Chen \harvardand\ Lin}{2011}{LCL11} Li, D., Chen, J. and Lin, Z. (2011). Statistical inference in partially time-varying coefficient models. {\em Journal of Statistical Planning and Inference} {\bf 141}, 995--1013. 

\harvarditem{Li}{1999}{L99} Li, Q. (1999). Consistent model specification tests for time series econometric models. {\em Journal of Econometrics} {\bf 92}, 101--147.

\harvarditem{Li \harvardand\ Wang}{1998}{LW98} Li, Q. and Wang, S. (1998). A simple consistent bootstrap test for a parametric regression function. {\em Journal of Econometrics} {\bf 87}, 145--165.

\harvarditem{L\"utkepohl}{2006}{Lu06} L\"utkepohl, H. (2006). {\em New Introduction to Multiple Time Series Analysis}. Springer.

\harvarditem{Miao, Phillips \harvardand\ Su}{2022}{MPS22} Miao, K., Phillips, P.C.B. \harvardand\ Su, L. (2022). High-dimensional VARs with common factors. Forthcoming in {\em Journal of Econometrics}.

\harvarditem{Moench, Ng \harvardand\ Potter}{2013}{MNP2013} Moench, E., Ng, S. \harvardand\ Potter, S. (2013). Dynamic hierarchical factor models. {\em The Review of Economics and Statistics} {\bf 95}, 1811-1817.

\harvarditem{Rodrik}{2013}{R13} Rodrik, D. (2013). Unconditional convergence in manufacturing. {\em Quarterly Journal of Economics} {\bf 128}, 165--204.

\harvarditem{Schwarz}{1978}{S78} Schwarz, G. (1978). Estimating the dimension of a model. {\em The Annals of Statistics} {\bf 6}, 461--464.

\harvarditem{Sims}{1980}{Si80} Sims, C. A. (1980). Macroeconomics and reality. {\em Econometrica} {\bf 48}, 1--48.

\harvarditem{Song \harvardand\ Bickel}{2011}{SB11} Song, S. and Bickel, P. (2011). Large vector auto regressions. Working paper available at \url{https://arxiv.org/abs/1106.3915}.

\harvarditem{Su, Shi \harvardand\ Phillips}{2016}{SSP16}Su, L., Shi, Z. and Phillips, P. C. B. (2016). Identifying latent structures in panel data. {\em Econometrica} {\bf 84}, 2215--2264.

\harvarditem{Su, Wang \harvardand\ Jin}{2019}{SWJ19}Su, L., Wang, X. and Jin, S. (2019). Sieve estimation of time-varying panel data models with latent structures. {\em Journal of Business and Economic Statistics} {\bf 37}, 334--349.

\harvarditem{Sun}{2016}{Su16} Sun, Y. (2016). Functional-coefficient spatial autoregressive models with nonparametric spatial weights. {\em Journal of Econometrics} {\bf 195}, 134--153.

\harvarditem{Sun \harvardand\ Malikov}{2018}{SM18} Sun, Y. and Malikov, E. (2018). Estimation and inference in functional-coefficient spatial autoregressive panel data models with fixed effects. {\em Journal of Econometrics} {\bf 203}, 359--378.

\harvarditem{Vogt}{2012}{Vo12} Vogt, M. (2012). Nonparametric regression for locally stationary time series. {\em The Annals of Statistics} {\bf 40}, 2601--2633.

\harvarditem{Vogt \harvardand\ Linton}{2017}{VL17}Vogt, M. and Linton, O. (2017). Classification of nonparametric regression functions in longitudinal data models. {\em Journal of the Royal Statistical Society, Series B} {\bf 79}, 5--27.

\harvarditem{Vogt \harvardand\ Linton}{2020}{VL20}Vogt, M. and Linton, O. (2020). Multiscale clustering of nonparametric regression curves. {\em Journal of Econometrics} {\bf 216}, 305--325.

\harvarditem{Wu}{2019}{Wu19} Wu, B. (2019). Time-varying network vector autoregression model. Working paper available at \url{http://dx.doi.org/10.2139/ssrn.3491608}.

\harvarditem{Wu}{2005}{Wu05} Wu, W. (2005). Nonlinear system theory: Another look at dependence. {\em Proceedings of the National Academy of Sciences} {\bf102}, 14150--14154.

\harvarditem{Wu \harvardand\ Zhao}{2007}{WZ07} Wu, W. and Zhao, Z. (2007). Inference of trends in time series. {\em Journal of the Royal Statistical Society, Series B} {\bf 69}, 391--410.

\harvarditem{Xia}{1998}{X98} Xia, Y. (1998). Bias-corrected confidence bands in nonparametric regression. {\em Journal of the Royal Statistical Society Series B} {\bf 60}, 797--811.

\harvarditem{Xu, Chen \harvardand\ Wu}{2020}{XCW20} Xu, M., Chen, X. and Wu, W. (2020). Estimation of dynamic networks for high-dimensional nonstationary time series. {\em Entropy} {\bf 22}, 55.

\harvarditem {Zhang \harvardand\ Wu}{2021}{ZW21} Zhang, D. and Wu, W. (2021). Convergence of covariance and spectral density estimates for high-dimensional locally stationary processes. {\em The Annals of Statistics} {\bf 49}, 233--254.

\harvarditem {Zhang}{2013}{Z13} Zhang, T. (2013). Clustering high-dimensional time series based on parallelism. {\em Journal of the American Statistical Association} {\bf 108}, 577--588.

\harvarditem {Zhang \harvardand\ Wu}{2011}{ZW11} Zhang, T. and Wu, W. (2011). Testing parametric assumptions of trends of a nonstationary time series. {\em Biometrika} {\bf 98}, 599--614.

\harvarditem {Zhang \harvardand\ Wu}{2012}{ZW12} Zhang, T. and Wu, W. (2012). Inference of time varying regression models. {\em The Annals of Statistics} {\bf 40}, 1376--1402.

\harvarditem{Zhou \harvardand\ Wu}{2010}{ZW10} Zhou, Z. and Wu, W. (2010). Simultaneous inference of linear models with time varying coefficients. {\em Journal of the Royal Statistical Society, Series B} {\bf 72}, 513--531.

\harvarditem{Zhu \harvardand\ Pan}{2020}{ZP20} Zhu, X. and Pan, R. (2020). Grouped network vector autoregression. {\em Statistica Sinica} {\bf 30}, 1437--1462.

\harvarditem{Zhu {\em et al}}{2017}{ZPLLW17} Zhu, X., Pan, R., Li, G., Liu, Y. and Wang, H. (2017). Network vector autoregression. {\em The Annals of Statistics} {\bf 45}, 1096--1123.

\harvarditem{Zhu, Xu \harvardand\ Fan}{2022}{ZXF22} Zhu, X., Xu, G. and Fan, J. (2022). Simultaneous estimation and group identification for network vector autoregressive model with heterogeneous nodes. Working paper available at \url{https://arxiv.org/abs/2209.12229}.

}

\end{thebibliography}


\end{document}