\documentclass[11pt,final,twocolumn]{IEEEtran}
\renewcommand{\baselinestretch}{1.0}

\date{}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{steinmetz}
\usepackage[colorlinks=true, pdfborder={0 0 0}]{hyperref}
\usepackage{stmaryrd}
\usepackage{MnSymbol,  bbm} 
\usepackage{cite}
\usepackage{url}
\usepackage{svg}
\usepackage{amsfonts}
   
\usepackage{caption}

\usepackage{color}
\usepackage{float}
\usepackage{times,amsmath,epsfig}
\usepackage{xspace,latexsym,syntonly}
\usepackage{amssymb}
\usepackage{textcomp}

\usepackage{xcolor}
\usepackage{import}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}

\newcommand{\qed}{\hfill\blacksquare}
\DeclareMathOperator*{\argmax}{\arg\max} 
\DeclareMathOperator*{\argmin}{\arg\min} 

\setlength{\textfloatsep}{5pt}

\usepackage{algorithm}
\usepackage{algpseudocode}


% ---- TABLE & PLOTS ----
\usepackage{pgfplots, pgfplotstable}
\usepackage{array, boldline, makecell}
\newcommand\btrule[1]{\specialrule{#1}{0pt}{0pt}}

\usepackage{standalone}
\usepackage{tikz}
\pgfplotsset{compat=1.7}
\usepackage[utf8]{inputenc}



% ---------------
\pdfoutput=1
\begin{document}
\title{Multi-Flow Transmission in Wireless Interference Networks:  A Convergent Graph Learning Approach}
\author{Raz Paul, Kobi Cohen, Gil Kedar
\thanks{Raz Paul and Kobi Cohen are with the School of Electrical and Computer Engineering, Ben-Gurion University of the Negev, Beer Sheva 8410501 Israel. Email: razpa@post.bgu.ac.il, yakovsec@bgu.ac.il.}
\thanks{Gil Kedar is with Ceragon Networks Ltd., Tel Aviv, Israel. Email: gilke@ceragon.com.}
\thanks{This work was supported by the Israel Ministry of Economy under the Magnet consortium program.}
}

\maketitle

\begin{abstract}
We consider the problem of of multi-flow transmission in wireless networks, where data signals from different flows can interfere with each other due to mutual interference between links along their routes, resulting in reduced link capacities. The objective is to develop a multi-flow transmission strategy that routes flows across the wireless interference network to maximize the network utility. However, obtaining an optimal solution is computationally expensive due to the large state and action spaces involved. To tackle this challenge, we introduce a novel algorithm called Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. A centralized stage computes the multi-flow transmission strategy using a novel design of graph neural network (GNN) reinforcement learning (RL) routing agent. Then, a distributed stage improves the performance based on a novel design of distributed learning updates. We provide a theoretical analysis of DIAMOND and prove that it converges to the optimal multi-flow transmission strategy as time increases. We also present extensive simulation results over various network topologies (random deployment, NSFNET, GEANT2), demonstrating the superior performance of DIAMOND compared to existing methods.
\end{abstract}

\begin{IEEEkeywords}
Wireless interference networks, distributed learning, deep reinforcement learning (DRL), graph neural network (GNN). 
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

The increasing demand for wireless communication services has accompanied the fast development of communication network technology in 5G and beyond. Despite this, spectrum scarcity remains a major constraint in meeting this growing demand. Thus, developing algorithms for data transmission in wireless networks that utilize the available spectral resources and manage data transmissions efficiently is a key challenge in modern wireless communication networks. 

In this paper, we consider the multi-flow transmission problem in wireless interference networks. Specifically, let $\mathcal{V}$ denote the set of nodes (i.e., users) in the network, and $\mathcal{E}$ the set of edges (i.e., communication links). The wireless communication network is modeled by a directed connected graph $G=(\mathcal{V},\mathcal{E})$. A directed communication link between transmitter $v$ and receiver $u$ is modeled by a link $(v,u)\in\mathcal{E}$ from node $v\in\mathcal{V}$ to node $u\in\mathcal{V}$ in the directed graph (i.e., $v,u$ are neighbors). Links cause mutual interference due to the overlapping of their radio frequency signals in the same spatial region. We are given $N$ flows, each flow (say $f_i$) is indicated by source node $s_i\in\mathcal{V}$ and destination node $d_i\in\mathcal{V}$. The goal is to allocate multi-flow routes for data transmissions in the wireless network to maximize a certain network utility, where data signals from different flows can interfere with one another due to mutual interference between links along their routes, resulting in a degradation in link capacities. 

\subsection{Existing Approaches to Data Flow Transmission}

A well-known approach for data flow transmission is to solve the shortest path problem and use the shortest paths (or a candidate set of short paths) to route data flows. For instance, the popular Open Shortest Path First (OSPF) routing protocol transmits data flows in the network over shortest paths, where the computations of the paths are implemented by the well-known Dijkstra algorithm \cite{srikant2013communication, gong2016distributed}. OSPF-based protocols are widely used in communication companies today due to their simplicity and good performance when the network load is not too high, and the wireless interference is not strong. The drawback of using shortest paths to route data in communication networks is their tendency to increase congestion on those paths, which can decrease performance in highly-loaded networks and in environments with strong wireless interference. Alternatively, more recent approaches have suggested balancing between short paths and path congestion to distribute the load over the network more evenly \cite{ying2010combining, joo2011performance, amar2021online, liu2022routing}. 

In recent years, significant improvements have been made by developing machine learning algorithms for managing flow transmissions in order to tackle uncertainties in random channel and network conditions. Several studies \cite{tekin2011online, tekin2012online, liu2012learning, cohen2014restless, gafni2018learning, bistritz2018distributed, turgay2019exploiting, yemini2020restless, gafni2020learning, gafni2022distributed, gafni2022learning} have analyzed the long-term reward optimization of users in the network using a multi-armed bandit learning framework. The learning strategies have included various methods, such as reinforcement learning and upper confidence bound (UCB)-based algorithms \cite{agrawal1995sample, auer2002finite, tabei2021multi}, as well as deep reinforcement learning that uses deep neural networks in the optimization \cite{wang2018deep, yu2019deep, naparstek2018deep, bokobza2023deep}. While most of these online learning methods have focused on single-hop transmissions, in this paper we apply the learning to multi-hop link states to enable efficient path selection for flow transmission.

Existing learning methods for routing data flows in wireless networks have been developed in several studies \cite{liu2012adaptive, tehrani2013distributed, he2013endhost, talebi2017stochastic, xia2019reinforcement, DRL+GNN, huang2021tsor, Zhao2021, amar2022online}. In \cite{DRL+GNN}, the focus was on using deep reinforcement learning to schedule routes based on congestion levels, but without considering mutual interference between links and without providing theoretical guarantees on convergence. In \cite{liu2012adaptive, tehrani2013distributed, he2013endhost, talebi2017stochastic, xia2019reinforcement, huang2021tsor, Zhao2021, amar2022online}, the focus was on developing an online learning algorithm that efficiently trades off between exploring paths to learn the network state, at the cost of selecting sub-optimal paths during the exploration phase, and exploiting the information the algorithm has gained to solve the shortest paths and schedule data transmissions through these paths. However, these algorithms suffer from poor performance in terms of load balancing since they tend to increase congestion over short paths. This negative effect becomes particularly pronounced as network load increases and mutual interference between links reduces the capacity over selected paths. Our algorithm overcomes this limitation by learning a multi-flow strategy optimized with respect to the current network load, thereby minimizing mutual interference and dynamically adjusting the path selection to reduce congestion in a distributed manner. 

\subsection{Main Results}
\label{sec:main_resutls}

In this paper, we aim to solve the multi-flow transmission problem in  wireless interference networks, while overcoming the limitations of existing approaches as discussed earlier. Below, we summarize our main contributions.

Firstly, in terms of algorithm development, our aim is to find a multi-flow transmission strategy that efficiently routes data flows across wireless interference networks while maximizing the network utility. Existing approaches assumed deterministic (e.g., \cite{ying2010combining, srikant2013communication}) or stochastic random (e.g., \cite{liu2012adaptive, tehrani2013distributed, talebi2017stochastic, huang2021tsor, amar2022online}) link weights that allow for efficient learning with low-complexity computation of short paths. However, this assumption is not valid in wireless interference networks, as data signals from different flows can interfere with one another due to mutual interference between links along their routes, leading to a decrease in link capacities. Therefore, finding an optimal solution for this problem is generally computationally expensive due to the large state and action spaces involved in wireless interference networks. 

To tackle this problem, we develop a novel algorithm, dubbed Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. The centralized stage computes the multi-flow transmission strategy using a novel module design of Graph neural network Routing agent via Reinforcement Learning, dubbed GRRL. Specifically, the GRRL module is implemented on a centralized unit as OSPF, given the interference map and flow demands, as given in 5G networks. It optimizes the network utility and yields the corresponding path allocation vector, a single route per flow. GRRL is trained by Reinforcement Learning (RL), and uses a novel Graph Neural Network (GNN) architecture as its policy network, for efficient path decoding. This captures the ability of GNN to efficiently prune the large search space and approximate well the optimal solution. It is designed in a generic way that handles general parameter values, number of nodes, edges, and flows. Then, to avoid local maxima and update strategies dynamically, a distributed stage is implemented to refine the GRRL solution towards a global optimum. To this, we develop a novel module design of distributed Noisy Best-Response for Route Refinement, dubbed NB3R. In NB3R, each source node updates its path asynchronously in a probabilistic manner to improve a collaborative utility, designed to increase the global network utility. Figure \ref{fig:search_space} provides an illustration of DIAMOND's two-stage optimization logic in the search space.

\begin{figure}[ht]
\centering
\hspace{-0.5cm}
\includegraphics[scale=0.17]{figures/search_space.pdf}
  \caption{An illustration of DIAMOND's two-stage optimization logic in the search space. The first module, GRRL, prunes the search space and yields a solution that concentrates around the global optimum. The second module, NB3R, avoids local maxima and updates strategies dynamically in a distributed manner to refine the GRRL solution toward a global optimum.}
  \label{fig:search_space}
\end{figure}

Second, we provide a rigorous performance analysis, theoretically and numerically. In terms of theoretical performance analysis, we provide a rigorous convergence analysis of the DIAMOND algorithm. Specifically, we prove that DIAMOND converges to the optimal multi-flow path allocation that maximizes the global network utility as time increases. In terms of numerical performance analysis, we present extensive simulation results to evaluate the overall performance of DIAMOND in finite time. The simulations were conducted using various network topologies, namely random deployment, NSFNET, and GEANT2 networks. All simulations validated the significant performance improvements of DIAMOND compared to existing methods.

\section{Network Model and Problem Statement}
\label{sec:system}

We consider the multi-flow transmission problem in wireless interference networks as described next. Let $\mathcal{V}$ denote the set of nodes (i.e., users) in the network, and $\mathcal{E}$ the set of edges (i.e., communication links between nodes). The wireless communication network is modeled by a directed connected graph $G=(\mathcal{V},\mathcal{E})$. A directed communication link between transmitter $v$ and receiver $u$ is modeled by a link $(v,u)\in \mathcal{E}$ from node $v\in\mathcal{V}$ to node $u\in\mathcal{V}$ in the directed graph (i.e., $v,u$ are neighbors). Links cause mutual interference due to the overlapping of their radio frequency signals in the same spatial region. Let $\mathcal{F}$ be a set of $N$ data flows, where each flow (say $f_n\in\mathcal{F}$) is indicated by source node $s_n\in\mathcal{V}$ and destination node $d_n\in\mathcal{V}$. Let $\mathcal{A}_n=\{\varphi^{n}_1,\varphi^{n}_2,\ldots,\varphi^{n}_{k_n}\}$ be the set of all allowed routes for flow $n$. Let $\sigma_n\in\mathcal{A}_n$
be a selected route for flow $n$, $\sigma\triangleq(\sigma_1, \sigma_2, ..., \sigma_N)$ be the selected route vector for all flows, and $\sigma_{-n}\triangleq(\sigma_1, ..., \sigma_{n-1}, \sigma_{n+1},..., \sigma_N)$ be the selected route vector for all flows excluding flow $n$. Let $u_n(\sigma)$ be a bounded utility of flow $n$ (e.g., the achievable rate, a monotonically increasing function with the achievable rate, etc.), $u_n(\sigma)\leq u_{max}\;\forall n=1, 2, ..., N$. Note that the utility depends on the selected route of flow $n$ as well as the selected routes of other flows that interfere with flow $n$. We denote the set of flows that interfere with flow $n$ (i.e., interfering neighbors) by $\mathcal{N}_n$. All other flows $\mathcal{F}\setminus\mathcal{N}_n$ are transmitted through paths that cause interference below the noise floor (due to geographical distance, directional antennas, etc.). As a result, $u_n(\sigma)$ depends on the selected route of flow $n$, and the selected routes of flows $\mathcal{N}_n$, $u_n(\sigma)=u_n(\sigma_n, \left\{\sigma_i\right\}_{i\in\mathcal{N}_n})$. 

The objective is to find a multi-flow 
transmission path vector $\sigma$ that solves the network utility maximization (NUM) problem 
\cite{srikant2013communication}:  

\begin{equation}
\label{eq:objective}
    \sigma^* = \argmax_{\{\sigma_n\in\mathcal{A}_n\}_{n=1}^N} \sum_{n=1}^N u_{n}(\sigma). 
\end{equation}

The utility of flow $n$ is typically set to be a monotonically increasing function with the achievable rate $R_n(\sigma)$ \cite{srikant2013communication}. For instance, one can set $u_n(\sigma)=R_n(\sigma)$ to maximize the network sum rate, or $u_n(\sigma)=\log R_n(\sigma)$ to maximize the network sum log-rate (i.e., proportional fairness). The rate of flow $f_n$ transmitted through path $\sigma_n=(s_n, u_1, u_2, ..., d_n)$ is determined by the slowest link rate across the links in its path, say link $\ell$. The achievable rate of user $n$ is thus given by:
\begin{equation}
    R_n(\sigma)=R_{\ell}(\sigma)=
    B_\ell \cdot \log\left(1+\textrm{SINR}_\ell(\sigma)\right), 
    \label{eq:link_rate}
\end{equation}
where $B_\ell$ is the bandwidth of link $\ell$, and 
\begin{equation}
 \textrm{SINR}_\ell(\sigma)\triangleq\frac{P_\ell}{I_\ell(\sigma)
 +\tilde{\sigma}^2}   
\end{equation}
denotes the Signal to Interference plus Noise Ratio (SINR) at the reciever of link $\ell$. Here, $P_\ell$ is the received power of the signal, $\tilde{\sigma}^2$ is the power spectrum density (PSD) of the additive white Gaussian noise (AWGN), and 
\begin{equation}
I_\ell(\sigma) \triangleq \sum_{i\in\mathcal{N}_{\ell}}
I_{\ell,i}(\sigma_i)
\end{equation}
is the cumulative interference power at the receiver of link $\ell$, aggregated across all links within its interference range $\mathcal{N}_\ell$. Here, $I_{\ell,i}$ is the interference power at the receiver of link $\ell$ caused by flow $i$, greater than zero if active by strategy $\sigma_i$. The transmission power control mechanism considered in this paper is independent, where a predetermined transmission power is assigned to each link based on factors such as geographical distance and channel state. Consequently, the action space of the learning algorithm is solely determined by the selected paths that affect the network utility.

Finding the optimal multi-flow transmission path vector that solves the combinatorial optimization in (\ref{eq:objective}) is an NP-hard problem \cite{NP_hard_routing}. The exponential number of possible route allocations makes it intractable to find the optimal solution via naive brute-force methods, thus a computationally-efficient algorithm is needed. Moreover, the optimal allocation is often expensive to compute, even for a small network. Thus, developing learning-based methods by training a supervised model is not practical. In the next section, we develop a novel algorithm to solve (\ref{eq:objective}), while overcoming these limitations. 

\begin{figure*}
    \vspace{-1cm}
    \centering
    \includegraphics[scale=0.15]{figures/algorithm_overview.pdf}
    \caption{An overview of the proposed DIAMOND framework: (a) An illustration of the construction of the search space for flow $n$, as described in Subsection \ref{ssec:constructing} and Alg.\ref{alg:k_paths}. This stage is implemented for all flows $n=1, 2, ..., N$. (b)-(e) \emph{Centralized GRRL module:} (b) The RL agent receives $N$ flow demands, and the network state as link features. (c) The network state is processed with a graph encoder GNN to produce an embedding for each link as well as a global graph embedding. The flow demands, along side those embedding are processed by the path-embedding module that outputs an embedding for each of the possible routing options. (d) The RL agent makes an allocation decision based on the path embedding. (e) The transmission paths of all $N$ flows are allocated at once to the network. (f) \emph{Distributed NB3R module:} Each flow updates its path allocation distributedly, based on the NB3R policy, which refines the allocation made by GRRL to improve performance.}
    \label{fig:alg_overview}
\end{figure*}

\section{The Proposed DIAMOND Algorithm}
\label{sec:DIAMOND}

In this section we present the DIAMOND algorithm to solve the objective (\ref{eq:objective}). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. The centralized stage computes the multi-flow transmission strategy using deep reinforcement learning methodology that uses deep neural network (DNN) to capture the high-dimensional problem. Specifically, we design a novel module of GNN Routing agent via Reinforcement Learning, dubbed GRRL. The GRRL module is implemented on a centralized unit as OSPF, given the interference map and flow demands, as given in 5G networks. It optimizes the network utility and yields an initial path allocation vector with fast inference computation. GRRL is trained by RL offline, and uses a novel GNN architecture as its policy network, for efficient path decoding. This captures the ability of GNN to efficiently prune the large search space and approximate well the optimal solution. It is designed in a generic way that handles general parameter values (number of nodes, edges, and flows). Then, to avoid local maxima and update strategies dynamically, a distributed stage is implemented to refine the GRRL solution toward a global optimum. To this, we develop a novel module design of distributed Noisy Best-Response for Route Refinement, dubbed NB3R. In NB3R, each source node updates its path asynchronously in a probabilistic manner to approach the global NUM solution \eqref{eq:objective}. A high-level illustration of DIAMOND's two-stage optimization logic in the search space is presented in Fig.\ref{fig:search_space}. A detailed illustration of each module in DIAMOND's framework is presented in Fig. \ref{fig:alg_overview}. 

We will begin by discussing the construction of the search space in the DIAMOND algorithm, which enables efficient learning. Following that, we will provide a detailed explanation of the GRRL and NB3R modules utilized in DIAMOND.

\subsection{Constructing the Search Space in the DIAMOND Algorithm}
\label{ssec:constructing}

The number of possible routes for each flow is usually large, resulting in a large action space for multi-flow path allocation that increases exponentially with the number of flows $N$. Therefore, to prevent divergence of the learning algorithm, the search space (i.e., the allowed number of paths for each flow) must be restricted \cite{babaee2010cross}. Let $K$ be the number of allowed paths for each flow. Then, the the size of the action space  is $K^N$ that determines the search space of the algorithm. Generally, one can set $K_n$ as the number of allowed paths for flow $n$, which can vary across flows. In \cite{DRL+GNN}, the authors used the standard selection of $K$ shortest-paths to restrict the search space. However, this approach is not suitable for wireless interference networks, as the selection mechanism may choose $K$ short paths with high mutual interference. Therefore, to cope with the interference environment, we develop a novel heuristic search space reduction method, which is described next (the pseudocode is given in Algorithm \ref{alg:k_paths}). For each flow, the algorithm computes a set of $K$ short paths that have high mutual distance between them. This allows the algorithm to select paths with significantly different interference effects on the network to effectively balance the load across the network. Specifically, let $d(\ell, \varepsilon)$ be a certain distance measure between links $\ell$ and $\varepsilon$ (e.g., Manhattan distance, Euclidean distance). The links' weights are initialized by a unit cost, and the first shortest path (say $\varphi$) is computed by the Dijkstra algorithm (the function shortest-path$(s_n, d_n, G, \left\{W(\varepsilon)\right\}_{\varepsilon\in\mathcal{E}})$ in Algorithm \ref{alg:k_paths}). Then, the algorithm increases the weight of each link (say $\varepsilon$) by $\min_{\ell \in \varphi} d^{-1}(\ell, \varepsilon)$. As a result, the next selected shortest path by Dijkstra (say $\varphi'$) would have a large distance between $\varphi'$ and $\varphi$. The algorithm continues until $K_n$ paths are selected.

\begin{algorithm}[hbt!]
\caption{Construction of $\mathcal{A}_n$ for flow $n$}\label{alg:k_paths}
\begin{algorithmic}[1]
\State \textbf{Input:} $G=(\mathcal{V}, \mathcal{E}), \ s_n, d_n \in \mathcal{V}$ 
\State \textbf{Initialize:} $W(\varepsilon) \gets 1\;  \forall \varepsilon \in \mathcal{E}$, \; $P \gets [] $
\For{$i=1,2,\ldots,K_n$ }
\State $\varphi \gets \textrm{shortest-path}(s_n, d_n, G, \left\{W(\varepsilon)\right\}_{\varepsilon\in\mathcal{E}})$
\State $P \gets P \cup \{\varphi\}$
\State  $W(\varepsilon) \gets W(\varepsilon) + \min_{\ell \in \varphi} d^{-1}(\ell, \varepsilon), \forall \varepsilon \in \mathcal{E}$
\EndFor
\State \textbf{Return:} $P$
\State $\mathcal{A}_n\gets P$
\end{algorithmic}
\end{algorithm}

\subsection{The Centralized Stage (GRRL)}

We start by describing the DNN architecture used in the GRRL module. Then, we describe the DRL algorithm of the agent. 

\subsubsection{The DNN architecture} \label{sec:DNN_arch}

The agent's policy is implemented as a deep GNN, which is comprised of graph-encoder and path-encoder modules.

\noindent
\textbf{Graph-Encoder}: The first step involves implementing Message Passing Neural Networks (MPNN) with a customized message-passing procedure that considers both the uplinks and downlinks across the links in the network. The input is an $E \times h_{\textrm{in}}$ edge feature matrix, containing all feature vectors of the graph's edges, and the $N$ flow demands triplets. Here, $h_{\textrm{in}}=3$, consists of the interference on the link, the link's capacity (normalized), and the last action that was taken (one-hot representation). We treat the links in the communication graph as nodes in the GNN. Let $\ell = (u,v)\in\mathcal{E}$ be a link in the communication network. We define $E_{in}(\ell) = \{(w,u) \ | \ w\in\mathcal{V} \}$, and $E_{out}(\ell) = \{(v,w) \ | \ w\in\mathcal{V} \}$
to be the sets of \emph{incoming} and \emph{outgoing} edges for link $\ell$.
The term $h(\ell)$ denotes the embedding for link $\ell$, and superscripts denote the time-step indices of the message passing. A single message passing can be described as 
\begin{eqnarray}
    h^{(t)}(\ell) &=& \gamma \big(h^{(t-1)}(\ell)W_1^{t}+ \sum_{e\in E_{in}(\ell)}h^{(t-1)}(e)W_2^{(t)}  \nonumber \\ 
    &+& \sum_{e\in E_{out}(\ell)}h^{(t-1)}(e)W_2^{(t)} \big), \label{eq:custom-message-passing}
\end{eqnarray}
where $W_1, \ W_2, \ W_3$ are learned matrices, and $\gamma(\cdot)$ denotes a non-linear activation function. As implemented by MPNNs, \eqref{eq:custom-message-passing} is held for $T$ iterations. In our case, the \emph{update} step is implemented as a simple GRU \cite{GRU}. This module results with an embedding representation $h(\ell)\in\mathbb{R}^{d}, \ \forall \ell\in\mathcal{E}$, as well as a global graph embedding $h(G)\in\mathbb{R}^{d}$, as the mean embedding.

\noindent
\textbf{Path-Encoder}: Next, an embedding $u_i$ per each path $\varphi$ in the action space $\mathcal{A}_n$ is constructed. This module is implemented as a bi-directional GRU. Its hidden state is initialized by $h(G)$. For each path in the action space, it sequentially aggregates all the edge features along the path with the flow demand embedding, and outputs a path specific embedding $u_i, \ i=1,2,\ldots,K_n$.

Finally, the agent determines the path to be allocated, by sampling the probability computed by scaling $u_i$ using the softmax operator 
\begin{equation}
    p_i^{n} = \frac{\exp (u_i)}{\sum_{j=1}^{K_n} \exp(u_j)} \label{eq: softmax}
\end{equation}
based on those embeddings.


\subsubsection{The Operation of the DRL Agent}

To implement the DRL algorithm for the agent, we design the reward function judiciously so that to balance between the utility of a flow and its interference to other flows. Since we aim at maximizing the sum of flow-utility in the network, we design the reward signal as the difference between two consecutive reward, i.e., $R_t = r_t - r_{t-1}$, where $r_t$ is the reward given from the environment, and $R_t$ denotes the reward introduced to the agent at time-step $t$. We have achieved a return as a telescopic sum. By setting $r_{0}=0$, we have:
\begin{equation}
    \mathcal{R}(\sigma) = \sum_{n=1}^N R_n = \sum_{n=1}^N (r_{n}-r_{n-1}) = 
    r_{N}. \label{eq:trajectory_return}
\end{equation}
For example, let $a_n$, and $\Phi_n$ denote the action, and the set of all allocated paths at time step $t=n$. Then, a good reward function that balances between the individual utility and the interference level in the network can be defined by: $r_n(a_n)=\alpha\cdot \sum_{m=1}^{n} u_m(\sigma)-(1-\alpha)\cdot\sum_{\ell\in\mathcal{E}(\Phi_n)} I_{\ell}(\sigma)$.

We train the agent using REINFORCE with baseline algorithm \cite{REINFORCE}. 
REINFORCE is a policy gradient method, which uses Monte Carlo rollout to compute the rewards, i.e., play through the whole episode to produce an allocation to each flow, then receives the trajectory's reward $\mathcal{R}(\sigma)$, given in \eqref{eq:trajectory_return}, which we treat as the agent's loss $\mathcal{L}$. Given the multi-flow transmission problem, the DNN model parameterized by $\boldsymbol{\theta}$ produces probability distribution $p_{\boldsymbol{\theta}}(\pi)$ by \eqref{eq: softmax}. It is then sampled to obtain a path allocation $\sigma=(\varphi_n\in\mathcal{A}_n)_{n=1}^{N}$. With the actions $\sigma$ given a network state $s$, the REINFORCE gradient estimator minimizes $\mathcal{L}_{\boldsymbol{\theta}}$ using the policy gradient:
\begin{eqnarray}
\nabla_{\boldsymbol{\theta}}\mathcal{L}_{\boldsymbol{\theta}} = \mathbb{E}_{p_{\boldsymbol{\theta}}(\sigma)}[\nabla \log p_{\boldsymbol{\theta}}(\sigma) (\mathcal{R}(\sigma)-b(s))],
\end{eqnarray}
where $b(s)$ is the baseline. In practice, we update $\boldsymbol{\theta}$ by Adam optimizer according to $\nabla\mathcal{L}$. The baseline is set to be the best random choice (one path for each flow) out of $M$ i.i.d. trials, which is only a function of the graph state, and does not depend on the action taken by the agent. Thus, it does not introduce any bias to the gradient estimator \cite{BartoSutton}. The pseudocode code of the operation of the DRL agent is given in Algorithm \ref{alg:RL}. Given a set of $N$ flow demands, the DRL agent allocates all $N$ flow demands one after the other, in a random order, by interacting with the simulated network environment.
First, as an initialization (line 3), it observes the network's initial state $\mathcal{G}$ as the $E \times h_{\textrm{in}}$ edge feature matrix discussed above. For each flow, say $n$, the agent first calculates its action space $\mathcal{A}_n$, namely, the $K_n$ possible paths for allocation (line 5). Then, the agent generates an embedding for each possible path $\pi_n$ (line 6). The embedding is produced via the GNN in a two-stage process, where first the Graph-Encoder generates features for each edge in the graph given the current state $\mathcal{G}$, followed by the Path-decoder module which produces embedding for each possible path in $\mathcal{A}_n$. Hence, $\pi_n$ is a vector of size $K_n$. The agent then chooses the path to allocate for flow $n$ by sampling $\pi_n$ in \eqref{eq: softmax} (line 7). Lastly, it allocates the path to the environment, and receives in return a new state of the environment, following its allocation action (line 8).

\begin{algorithm}
\caption{The operation of the DRL agent}\label{alg:RL}
  \begin{algorithmic}[1]
    \State \textbf{Inputs:} $G=(\mathcal{V}, \mathcal{E}), \{s_n, d_n\}_{n=1}^{N}, \{K_n\}_{n=1}^{N}$
    \State \textbf{Output:} $\{\varphi_n\}_{n=1}^{N}$
    
    \State $\mathcal{G} \gets \textrm{env.reset}()$
    \For{$n=1, 2, \ldots, N$}
    \State $\mathcal{A}_n \gets \textrm{Alg}.\ref{alg:k_paths}(s_n, d_n, K_n)$
    \State $\textbf{$\pi_n$} \gets \textrm{GNN}(\mathcal{A}_n, \mathcal{G})$
    \State $\varphi_n \sim Softmax(\pi_n)$
    \State $\mathcal{G} \gets \textrm{env.allocate}(\varphi_n)$
    \EndFor
    \State \textbf{Return:} $\{\varphi_n\}_{n=1}^{N}$
\end{algorithmic}
  \end{algorithm}


\subsection{The Distributed Stage (NB3R)}
\label{subsec:analyze_stage_2}

After a route was chosen for each flow (as illustrated in Fig.\ref{fig:alg_overview}(e)), an iterative noisy best-response procedure is implemented distributedly to refine the path chosen by the GRRL stage. 

Theoretically, convergence analysis often requires flows to update their strategies sequentially to avoid conflicts that might arise from simultaneous updates. In communication systems, this is often achieved by allowing each source node to draw a random backoff time and update its strategy when the backoff time expires \cite{srikant2013communication, cohen2017distributed}. For simplicity, we assume a similar mechanism here. Specifically, we assume that flows hold a global clock and may update their strategies only at certain times $\tilde{t}_1, \tilde{t}2, ..., $ referred to as updating times. At each updating time, every flow draws a backoff time from a continuous uniform distribution over the range $[0, \tilde{T}]$ for some $\tilde{T}>0$. A flow whose backoff time expires may broadcast a pilot signal to its neighbors, indicating that its strategy has been updated. Then, all its neighbors keep their strategies fixed until the next updating time. As a result, neighbors will not update their strategies simultaneously, as desired. At each updating time, we refer to flows that update their strategies to improve the overall objective as \emph{optimizing flows}, denoted by $\mathcal{F}{opt}$ (which is time-varying across updating times). Algorithm \ref{alg:stage_2} presents the pseudocode of the NB3R module.

Let us define a collaborative utility for each flow as follows:   
\begin{equation}
    \mathcal{U}_{n}(\sigma) = u_n(\sigma) + \sum_{m\in\mathcal{N}(n)} u_m(\sigma). 
\label{eq:utility}
\end{equation}
Note that the collaborative utility $\mathcal{U}_{n}(\sigma)$ consists of the individual utility $u_{n}(\sigma)$ of flow $n$ plus the utilities of all flows that interfere with flow $n$.  

The NB3R procedure works as follows. The distributed learning stage is initialized by the path allocation computed by the GRRL module. Then, a set of optimizing flows $\mathcal{F}_{opt}$ is selected (line 5). Each optimizing flow (say flow $n$) in $\mathcal{F}_{opt}$ queries its neighbors $\mathcal{N}(n)$ to compute and send their individual utility $u_m(\varphi, \sigma_{-n})$ for $K_n$ possible values, $\sigma_n=\varphi_1, ..., \varphi_{K_n}$ given the current strategy profile of all other flows $\sigma_{-n}$ (line 7). Note that each neighbor only needs to know the strategy of its neighbors to compute the utility values. Then, flow $n$ computes the collaborative utility using (\ref{eq:utility}) (line 8). 

Next, the path is updated using noisy best response (NBR) dynamics \cite{young2020individual}. Unlike in best response (BR) dynamics, where a player updates its strategy to maximize its utility given the current strategy of all other players, in NBR a player updates its strategy in a probabilistic manner. This allows for the avoidance of local maxima when optimizing the overall objective. Here, optimizing flows construct a probability mass function (pmf) over their actions (i.e., possible paths) and choose their actions according to this distribution. Typically, NBR is designed so that the BR strategy is played with high probability, while other strategies are played with a probability that decays exponentially fast with the myopic utility loss in order to escape local maxima. Specifically, the pmf over the available actions is given by:
\begin{equation}
    \mathbb{P}(\varphi^n_i) = \frac{e^{\nu \cdot \mathcal{U}_n(\varphi^n_i, \varphi^{-n})}}{\sum_{j=1}^{|\mathcal{A}_n|} e^{\nu \cdot \mathcal{U}_n(\varphi^n_j, \varphi^{-n})}} 
    \label{eq:nb3r_pmf}
\end{equation}
for some $\nu>0$. 
Note that when $\nu=0$, the pmf assigns equal weights for all strategies, while the probability of playing BR approaches one as $\nu\rightarrow\infty$ (a discussion on the setting of $\nu$ over time based on simulated annealing analysis \cite{hajek1988cooling} is provided in the next section).

After updating the selected path (line 9), flow $n$ broadcasts this information to all its neighbors $m\in\mathcal{N}_n$ (line 10). The procedure repeats until convergence (convergence analysis is provided in the next section).   

\begin{algorithm}[hbt!]
\caption{NB3R Algorithm}\label{alg:stage_2}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $G=(\mathcal{V}, \mathcal{E}), \{s_n, d_n, \mathcal{A}_n\}_{n=1}^{N}$, path allocation $\{\varphi_n\}_{n=1}^{N}$ output by  \textrm{Alg. \ref{alg:RL}} \vspace{0.1cm}
\State \textbf{Output:} Path $\varphi^{*}_n$ for each flow $n$ (distributedly)
\State \textbf{Initialize:} Each flow (say $n$) transmits its data signal through path $\varphi_n$
\Repeat{\;(at each updating time)} 
\State $\mathcal{F}_{opt} \gets \textrm{updated set of optimizing flows}$
\For{$n\in\mathcal{F}_{opt}$}:
\State Query $K_n$ utility values: $u_m(\varphi_1^n, \sigma_{-n})$, ...,$u_m(\varphi_{K_n}^n, \sigma_{-n})$ from all neighbors\hspace{2cm} .\hspace{1cm} $m\in\mathcal{N}(n)$ 
\State Compute $\mathcal{U}_{n}(\varphi_1^n, \sigma_{-n})$,...,$\mathcal{U}_{n}(\varphi_{K_n}^n, \sigma_{-n})$ (\ref{eq:utility})
\State \textrm{Draw $\varphi_n$ from distribution \eqref{eq:nb3r_pmf}} 
\State \textrm{Inform all neighbors $m\in\mathcal{N}(n)$ of the updated $\varphi_n$}
\EndFor 
\Until all utilities converge
\State \textbf{Return:} $\{\varphi^{*}_n\}_{n=1}^{N}$
\end{algorithmic}
\end{algorithm}

\section{Performance Analysis}

In this section, we evaluate the performance of the DIAMOND algorithm. First, in Subsection \ref{ssec:theoretical}, we provide theoretical convergence analysis, demonstrating strong guarantees that the algorithm will converge asymptotically to the optimal multi-flow path allocation as time increases. Then, 
in Subsection \ref{ssec:numerical} we present extensive simulation results to evaluate the overall performance of DIAMOND numerically in finite time. The
simulations were conducted using various network topologies,
namely random deployment, NSFNET, and GEANT2 networks.

\subsection{Theretical Analysis}
\label{ssec:theoretical}

In the following theorem we show that DIAMOND converges to the optimal multi-flow path allocation asymptotically as time increases. Let $\sigma^{DIAMOND(\nu)}(t)$ be the strategy profile under DIAMOND at time $t$ given a parameter value $\nu$. Let 
$\Sigma^*$ be the set of strategy profile that solves \eqref{eq:objective}. The following theorem shows that by setting $\nu$ sufficiently large, the probability of reaching the optimal strategy profile is close to one as time increases.\vspace{0.1cm}   

\noindent
\begin{theorem}
\label{th:main_theorem}
For any $\epsilon>0$, there exists $\nu>0$ such that
\begin{equation}
\label{eq:prob_nb3r}
    \lim_{t\to\infty} \mathbb{P}\left(\sigma^{DIAMOND(\nu)}(t)\in\Sigma^*\right) \geq 1-\epsilon.\vspace{0.3cm} 
\end{equation}
\end{theorem}

\emph{Proof:}
We prove the theorem by using potential game analysis \cite{potential_games}. We refer to each flow as a player that can adjust its strategy (i.e., selected path). The game is initialized by the centralized solution (GRRL), and plays distributedly by the NB3R dynamics. In potential games, the incentive of  players to switch strategies can be
expressed by a global potential function. A Nash Equlibrium for the game is reached at any local maximum
of the potential function. Let $\sigma_{n}^{(1)}$, $\sigma_{n}^{(2)}$ be two possible strategies for flow $n$. Specifically, the DIAMOND dynamics follows an \emph{exact} potential game if there is an exact potential function $\phi: \sigma \to \mathbb{R}$ such that for every flow $n$ and for every $\sigma_{-n}=\{{\varphi^{i}}\}_{i\neq n}$, the following holds:
\begin{equation}
\begin{array}{l}
\label{eq:definition_exact}
\displaystyle
\mathcal{U}_n(\sigma_{n}^{(2)}, \sigma_{-n}) -\mathcal{U}_n(\sigma_{n}^{(1)}, \sigma_{-n}) \vspace{0.2cm}\\
\displaystyle\hspace{2.5cm}
=\phi(\sigma_{n}^{(2)}, \sigma_{-n}) - \phi(\sigma_{n}^{(1)}, \sigma_{-n}),
\vspace{0.2cm}\\
\displaystyle\hspace{4cm}
\forall\sigma_{n}^{(1)}, \sigma_{n}^{(2)}\in\mathcal{A}_n.
\end{array}    
\end{equation}

Next, we show that DIAMOND dynamics satisfies (\ref{eq:definition_exact}) with the following exact potential function:
\begin{equation}
    \phi(\sigma) = \sum_{n=1}^{N} u_{n}(\sigma), \label{eq:exact_potential}
\end{equation}
namely, the overall objective function that needs to be maximized. To show this, consider two strategies for flow $n_0$: $\sigma_{n_0}^{(1)}, \sigma_{n_0}^{(2)}\in\mathcal{A}_n$,  and fix the strategy profile for all other flows $\sigma_{-n_0}$. Then, the difference in the potential function \eqref{eq:exact_potential} is given by:
\begin{equation}
\begin{array}{l}
\displaystyle
\Delta\phi(\sigma_{n_0}^{(1)}, \sigma_{n_0}^{(2)}, \sigma_{-n_0})\vspace{0.2cm}\\
    \displaystyle
\triangleq 
    \phi(\sigma_{n_0}^{(2)}, \sigma_{-n_0}) - \phi(\sigma_{n_0}^{(1)}, \sigma_{-n_0})\vspace{0.2cm}\\
    \displaystyle
= 
    \left(u_{n_{0}}(\sigma_{n_0}^{(2)}, \sigma_{-n_0}) + \sum_{m\neq {n_0}} u_{m}(\sigma_{n_0}^{(2)}, \sigma_{-n_0})\right)\vspace{0.2cm}\\
    \displaystyle
    -\left(u_{n_{0}}(\sigma_{n_0}^{(1)}, \sigma_{-n_0}) + \sum_{m\neq {n_0}} u_{m}(\sigma_{n_0}^{(1)}, \sigma_{-n_0})\right)\vspace{0.2cm}\\
    \displaystyle
= 
    \left(u_{n_{0}}(\sigma_{n_0}^{(2)}, \sigma_{-n_0}) + \sum_{m\in \mathcal{N}(n_0)} u_{m}(\sigma_{n_0}^{(2)}, \sigma_{-n_0})\right)\vspace{0.2cm}\\
    \displaystyle
    -\left(u_{n_{0}}(\sigma_{n_0}^{(1)}, \sigma_{-n_0}) + \sum_{m\in\mathcal{N}(n_0)} u_{m}(\sigma_{n_0}^{(1)}, \sigma_{-n_0})\right)\vspace{0.2cm}\\
    \displaystyle
=\mathcal{U}_{n_0}(\sigma_{n_0}^{(2)}, \sigma_{-n_0}) -\mathcal{U}_{n_0}(\sigma_{n_0}^{(1)}, \sigma_{-n_0})\vspace{0.2cm}\\
\displaystyle
\triangleq
\Delta\mathcal{U}_{n_0}(\sigma^{(1)}, \sigma^{(2)}, \sigma_{-n_0}),
\end{array}    
\end{equation}
where the second equality follows since flow $n_0$ only interferes with its neighbors $\mathcal{N}(n_0)$. As a result, their utility might change, while the utilities of all other flows $\mathcal{F} \setminus \mathcal{N}(n_0)$ remain the same and are cancelled. Furthermore, since the utility $u_{n_0}$ is bounded by $u_{max}$, it follows that $\phi(\sigma)<N\cdot u_{max} < \infty$ holds. As a result, $\phi(\sigma)$ in \eqref{eq:exact_potential} is a bounded exact potential function of the game, which proves that DIAMOND dynamics is an exact potential game.

Finally, recall that DIAMOND plays noisy BR dynamics by the NB3R module with respect to $\mathcal{U}_n$. We now use the fact that a noisy BR dynamics following (\ref{eq:nb3r_pmf})
in the exact potential game converges to a stationary
distribution of the Markov chain corresponding to the game \cite{blume1993statistical}. Since DIAMOND is an exact potential game with an exact potential function $\phi$ given in (\ref{eq:exact_potential}), the stationary distribution of the strategy profile is given by:
 \begin{equation}
\displaystyle\mathbb{P}(\sigma^{DIAMOND(\nu)}=\sigma) = \displaystyle\frac{e^{\nu\cdot\phi(\sigma)}}{\sum_{\tilde{\sigma}} e^{\nu\cdot\phi(\tilde{\sigma})}}. 
\label{eq:prob_sigma}
 \end{equation}
Note that the strategy that maximizes \eqref{eq:exact_potential} is the one that solves  \eqref{eq:objective} as well. Thus, its global maximum lies inside the action space played by the NB3R module. Therefore, for any $\epsilon>0$ we can choose $\nu>0$ sufficiently large such that the stationary distribution puts
a sufficiently high weight on the strategy profile that maximizes \eqref{eq:exact_potential} (i.e., $\phi$ in \eqref{eq:prob_sigma}). Thus, \eqref{eq:prob_nb3r} is satisfied as time approaches infinity.
$\qed$ \vspace{0.2cm}

\emph{Corollary 1:} Let $\sum_{n=1}^{N} u_n\left(\sigma^{DIAMOND\left(\nu(t)\right)}\right)$ and  
$\sum_{n=1}^{N} u_n\left(\sigma^*\right)$ be the sum utility achieved by DIAMOND at time $t$ and the optimal strategy, respectively. Let $\Delta=N\cdot u_{max}$, and let the parameter $\nu$ in \eqref{eq:nb3r_pmf} increase with time as $\nu(t)=\log (t)/\Delta$. Then,
\begin{equation}
\begin{array}{l}
\displaystyle
\sum_{n=1}^{N} u_n\left(\sigma^{DIAMOND\left(\nu(t)\right)}\right)\longrightarrow\sum_{n=1}^{N} u_n\left(\sigma^*\right) \vspace{0.2cm}\\\hspace{6cm}
\mbox{as\;\;}t\longrightarrow\infty.\vspace{0.2cm}    
\end{array}
\end{equation}

\emph{Proof:}
Following the proof of Theorem \ref{th:main_theorem}, the stationary distribution of the Markov chain with a fixed $\nu$ is given by \eqref{eq:prob_sigma}. As a result, as the probability of playing BR increases, where BR is the strategy of each flow maximizing the improvement in the collaborative utility at each updating time given the current state of other flows (i.e., by increasing $\nu$), the probability of attaining the global maximum of the potential function \eqref{eq:exact_potential} increases with time. 
Achieving the optimal solution (i.e., letting $\epsilon\rightarrow 0$ in \eqref{eq:prob_nb3r}) requires $\nu$ to approach infinity. However, increasing $\nu$ too fast may push the algorithm into a local maximum for a long time (since the probability of not playing BR is too small). The process of increasing $\nu(t)$ during the algorithm is also known as cooling the system in simulated annealing analysis, where $1/\nu(t)$ represents the temperature. Following simulated annealing analysis \cite{hajek1988cooling}, convergence to the optimal solution is attained by increasing $\nu(t)$ as $\nu(t)=\log t/\Delta, t=1, 2, ...$, where it suffices to set the constant $\Delta$ to be greater than the maximal change in the objective function, which is upper bounded by $N\cdot u_{max}$ in our case. $\qed$\vspace{0.2cm} 

Using the DIAMOND dynamics described in Corollary 1, the flows explore strategy profiles at the beginning of the algorithm, and converge to their best response as time approaches infinity. In cases where multiple optimal operating points exist, the algorithm may converge to one of them. However, simulation results show that the optimal solution can be reached much faster with smaller values of $\Delta$ under typical scenarios.

\subsection{Numerical Analysis}
\label{ssec:numerical}

We now provide numerical examples to illustrate the performance of the proposed DIAMOND algorithm in various wireless interference network environments. We compared the following algorithms: (i) \emph{Random Baseline (RB)}: The random baseline is a heuristic method that chooses the best path allocation out of $100$ independent trails of random choices from the action set. A random baseline is commonly used in machine learning research to evaluate the ability of a learning algorithm to learn good strategies and perform better than exhaustive search over a limited number of trials. (ii) \emph{Open Shortest Path First (OSPF)} \cite{OSPF}: The OSPF protocol (and its variants) is widely used for routing data and is implemented in many real-world systems. OSPF uses Dijkstra algorithm to compute and transmit data through the shortest path. (iii) DQN+GNN \cite{DRL+GNN}: The recently suggested DQN+GNN algorithm uses a DRL algorithm, based on the well-known offline-RL DQN \cite{mnih2015human} algorithm. In the implementation of all algorithms, we used the same knowledge of the wireless network state and interference map to optimize the objective and compute the selected paths. For each simulation experiment, we averaged the results over $10$ random independent settings, i.e., flow demands and link capacities. 

First, we simulated a large-size dense cluster. We simulated a random network setting, where each trial involves randomizing $N=60$ node positions, $E=90$ random links between nodes, as well as random capacities and random flow demands. For convenience, we refer to the case of $N\leq 0.5V$ as a lightly-loaded network, the case of $0.5V < N\leq V$ as a moderately-loaded network, and the case of $N>V$ as a heavily-loaded network. The results are presented in Figs. \ref{fig:rand6090_topology_a}-\ref{fig:rand6090_delay_c}. In all cases, DIAMOND shows significantly better performance than all other algorithms, with an average flow rate (Fig. \ref{fig:rand6090_rate}) outperforming DQN+GNN (in second place) by $20\%$ to $70\%$ in lightly-loaded networks. The performance gain increases to $100\%$ or more as the load increases. It is important to notice that the superior performance of DIAMOND in achievable rate does not come at a cost of the packet delay. On the contrary, it can be seen in Fig. \ref{fig:rand6090_delay_c} that DIAMOND significantly outperforms all other algorithms in terms of packet delay as well for all network load values, while DQN+GNN, which comes in second place in terms of achievable rate, achieves the worst performance in terms of packet delay in heavily-loaded networks.
 
\begin{figure}[ht!] 
\begin{center}
% RandomTopology
    \subfigure[Random Topology $V=60, E=90$]{\scalebox{0.5}
    {
      \label{fig:rand6090_topology_a}
      \includegraphics      {figures/topologies/rand_graph_60_90.pdf}
    }}
    \hspace{-1cm}\hfill
    \subfigure[Average Flow Rate]{\scalebox{0.45}
    {
    \label{fig:rand6090_rate}      \input{figures/graphs/V60E90/Rates.tex}
    }}
    \hfill
    \subfigure[Network Delay]{\scalebox{0.45}
    {
        \label{fig:rand6090_delay_c}
      \input{figures/graphs/V60E90/Delay.tex}
    }}
    \caption{Performance evaluation of the algorithms as a function of the number of flows in a large-size dense cluster network.}
\label{fig:performance_topologies_random}
\end{center}
\end{figure}

Second, we evaluate the performance over NSFNET topology, which is a known topology with $V=14$ nodes and $E=21$ edges \cite{NSFNET}. This experiment corresponds to a small-size cluster of users. We randomize over trials the node positions, link capacities and flow demands. The results are presented in Figs. \ref{fig:nsfnet_topology1}-\ref{fig:nsfnet_delay1}. In all cases, DIAMOND shows significantly better performance than all other algorithms, with an average flow rate (Fig. \ref{fig:nsfnet_rate1}) outperforming DQN+GNN (in second place) by $10\%$ to $40\%$. It is important to note again that the superior performance of DIAMOND in achievable rate does not come at the cost of packet delay. As seen in Fig. \ref{fig:nsfnet_delay1}, DIAMOND outperforms all other algorithms in terms of packet delay for most network load values.

\begin{figure}[ht!] 
\begin{center}
% NSFNET
    \subfigure[NSFNET \cite{NSFNET}]{\scalebox{0.5}
    {
    \label{fig:nsfnet_topology1}
      \includegraphics{figures/topologies/NSFNet.pdf}
    }}
    \hspace{-1cm}\hfill
    \subfigure[Average Flow Rate]{\scalebox{0.45}
    {
    \label{fig:nsfnet_rate1}  \input{figures/graphs/NSFNET/Rates.tex}
    }}
    \hfill
    \subfigure[Network Delay]{\scalebox{0.45}
    {
    \label{fig:nsfnet_delay1}
      \input{figures/graphs/NSFNET/Delay.tex}
    }}
    \caption{Performance evaluation of the algorithms as a function of the number of flows in NSFNET network.}
\label{fig:performance_topologies_NSFNET}
\end{center}
\end{figure}

Third, we evaluate the performance over GEANT2 topology, which is a known topology with $V=24$ nodes and $E=37$ edges \cite{GEANT2}. This experiment corresponds to a medium-size cluster of users. We randomize over trials the node positions, link capacities and flow demands. The results are presented in Figs. \ref{fig:geant2}-\ref{fig:geant2_delay}. In terms of average flow, DIAMOND shows significantly better performance than all other algorithms, with an average flow rate (Fig. \ref{fig:geant2_rate}) outperforming DQN+GNN (in second place) by $10\%$ to $40\%$. As seen in Fig. \ref{fig:geant2_delay}, DIAMOND achieves good performance as compared to the other algorithms in terms of packet delay as well.

\begin{figure}[ht!] 
\begin{center}
% GEANT2
    \subfigure[GEANT2 \cite{GEANT2}]{\scalebox{0.5}
    {
    \label{fig:geant2}
      \includegraphics{figures/topologies/GEANT2.pdf}
    }}
    \hspace{-1cm}\hfill
    \subfigure[Average Flow Rate]{\scalebox{0.45}
    {
    \label{fig:geant2_rate}  
    \input{figures/graphs/Geant2/Rates.tex}
    }}
    \hfill
    \subfigure[Network Delay]{\scalebox{0.45}
    {
    \label{fig:geant2_delay}
      \input{figures/graphs/Geant2/Delay.tex}
    }}
    
    \caption{Performance evaluation of the algorithms as a function of the number of flows in GEANT2 network.}
\label{fig:performance_topologies}
\end{center}
\end{figure}

To further validate the robustness of DIAMOND in supporting various network configurations with changing numbers of flow demands ($N$), nodes ($V$), and edges ($E$), and to justify its usage in 5G topologies, which consist of dense networks with a large number of edges, we conducted additional experiments using randomized settings. The results are presented in Table \ref{tab:rand_performances}.

\begin{table*}[!ht]
	\caption{Algorithm comparison for various network configurations. 
}
	\label{tab:rand_performances}
	\begin{center}
		\scalebox{1.0}{ 
			\begin{tabular}{c c c || c c c c | c c c c}
                \hlineB{8}\\[-1em]
                & & & \multicolumn{4}{c|}{Average Flow Rates [Mbps] $\uparrow$} & \multicolumn{4}{c}{Max Delay [time-steps] $\downarrow$}\\
                \\[-1em]\hlineB{4}\\[-1em]
				$N$ & $V$ & $E$ & RB & OSPF & DQN+GNN & \textbf{DIAMOND (ours)} & RB & OSPF & DQN+GNN & \textbf{DIAMOND (ours)}  \\ \hline
    
				\hypertarget{r1}{}25 & 50 & 75 & 25.624 & 26.78 & 46.636 &  \textbf{80.216} & 31.3 & 72.0 & 70.4 &  \textbf{16.5}\\ 

                \hypertarget{r2}{}100 & 200 & 300 & 0.611 & 2.061 & 9.937 &  \textbf{17.239} & 314.2 & 372.1 & 339.9 &  \textbf{113.8}\\

                \hypertarget{r3}{}70 & 70 & 140 & 1.887 & 4.201 & 20.597 &  \textbf{30.594} & 179.2 & 147.7 & 155.5 &  \textbf{60.7}\\

                \hypertarget{r4}{}20 & 20 & 40 & 26.935 & 60.77 & 75.505 &  \textbf{108.14} & 55.2 & 10.8 & 35.6 &  \textbf{8.4}\\

                \hypertarget{r5}{}80 & 40 & 60 & 0.57625 & 1.841 & 9.85 &  \textbf{18.107} & 192.0 & 191.7 & 182.3 & \textbf{130.6}\\

                \hypertarget{r6}{}100 & 50 & 75 & 0.276 & 0.841 & 6.468 & \textbf{13.6} & 311.4 & 343.8 & 419.1 & \textbf{190.8}\\

                \hypertarget{r7}{}50 & 10 & 15 & 6.504 & 9.05 & 25.024 & \textbf{41.054} & 40.8 & 36.8 & 49.2 & \textbf{24.7}\\

                \hypertarget{r8}{}150 & 30 & 45 & 0.1113 & 0.285 & 4.026 & \textbf{7.894} & 544.6 & 684.5 & 577.2 & \textbf{195.2}\\

                \hypertarget{r9}{}140 & 70 & 105 & 0.1207 & 0.396 & 4.137 & \textbf{8.89} & 544.5 & 608.7 & 513.2 & \textbf{455.3}\\

                \hypertarget{r10}{}30 & 10 & 45 & 18.553 & 20.21 & 52.57 & \textbf{77.689} & 18.8 & 24.2 & 26.1 & \textbf{10.8}\\

            \\[-1em]\hlineB{8}
			\end{tabular}
		}
	\end{center}
\end{table*}

First, we simulated lightly-loaded networks where $V=2N$ and $E=1.5V$ (lines \hyperlink{r1}{1}, \hyperlink{r2}{2}). Second, we simulated moderately-loaded networks where $V=N$ and $E=2V$ (lines \hyperlink{r3}{3}, \hyperlink{r4}{4}). Third, we simulated heavily-loaded networks where $V=0.5N$ and $E=1.5V$ (lines \hyperlink{r5}{5}, \hyperlink{r6}{6}) and $V=0.2N$ and $E=1.5V$ (lines \hyperlink{r7}{7}, \hyperlink{r8}{8}). Lastly, we considered a scenario with a higher and lower ratio between nodes and edges (lines \hyperlink{r9}{9}, \hyperlink{r10}{10}, respectively). The results in Table \ref{tab:rand_performances} show that DIAMOND significantly outperforms all other algorithms in terms of both average flow rate and packet delay in all scenarios, which represent different realistic network configurations.

In summary, the simulation results demonstrate that DIAMOND consistently exhibits very strong performance when compared to all other algorithms.


\section{Conclusion}
\label{sec:conclusion}

The problem of multi-flow transmission in wireless networks, where mutual interference between links can reduce link capacities, has been addressed with the development of a novel algorithm called DIAMOND. The algorithm allows for a hybrid centralized-distributed implementation and is designed to maximize the network utility. The theoretical analysis has proven that DIAMOND converges to the optimal multi-flow transmission strategy over time. Extensive simulation results over various network topologies have demonstrated the superior performance of DIAMOND compared to existing methods. DIAMOND represents a promising approach to improving multi-flow transmission in wireless networks and has the potential to make valuable contributions in 5G and beyond technologies with centralized unit deployments.


\bibliographystyle{IEEEtran}
\bibliography{main}


\end{document}

