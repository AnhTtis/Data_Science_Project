\documentclass[12pt,onecolumn]{article}

\usepackage{times}
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow,multicol,makecell}
\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{enumitem}
\usepackage{comment}
\usepackage{tabularray}
\usepackage{amsmath,amssymb,amsthm,amsfonts,mathrsfs,bm,color}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{dsfont}
\usepackage{wrapfig}
% \usepackage{float}
% \usepackage{subfigure}

%\documentclass[11pt,a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}
\usepackage[a4paper, margin=1in]{geometry}

% Support for easy cross-referencing
%\usepackage[capitalize]{cleveref}
%\crefname{section}{Sec.}{Secs.}
%\Crefname{section}{Section}{Sections}
%\Crefname{table}{Table}{Tables}
%\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}




\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Spatial-Temporal Data Overfitting for High-Quality and Efficient Video Delivery}  % **** Enter the paper title here
\author{\mbox{*}Jie Ji, Gen Li, and Xiaolong Ma  \\
        \small PhD student, Clemson University,
        \small PhD student, Clemson University, and \small Professor, Clemson University \\     
}
%\author[1]{Jie Ji\thanks{jji@g.clemson.edu}}
%\author[1]{Gen Li\thanks{gen@g.clemson.edu}}
%\author[1]{Xiaolong Ma\thanks{xiaolom@clemson.edu}}
%\author[2]{Author D\thanks{D.D@university.edu}}
%\author[2]{Author E\thanks{E.E@university.edu}}
%\affil[1]{Department of Electrical and Computer Engineering, Clemson University}
%\affil[2]{Department of Mechanical Engineering, \LaTeX\ University}



\date{\vspace{-5ex}}
\maketitle

\textbf{Keywords}: super-resolution, machine learning, DNN model overfitting, real-time edge computing



\thispagestyle{empty}
\appendix


%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolution upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and overfitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency. However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. To reconcile such, we present a novel approach towards high-quality and efficient video resolution upscaling via \underline{\bf S}patial-\underline{\bf T}emporal \underline{\bf D}ata \underline{\bf O}verfitting, namely \textbf{STDO}, which for the first time, utilizes the spatial-temporal information to accurately divide video into chunks for super-resolution (SR) model overfitting tasks. 

\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth]{figs/necv_stdo_overview.pdf}
\vspace{-1.5em}
\caption{Overview of the proposed STDO method. Each video frame across time dimension is sliced into patches, then divided and grouped into chunks. Here we set the number of chunks to 2 for clear illustration. Then each chunk is overfitted by independent SR models, and delivered to end-user for video super-resolution.}
\label{fig:intro_2}
\vspace{-0.3cm}
\end{figure}


Figure~\ref{fig:intro_2} demonstrates the overview of our proposed method. By using spatial-temporal information for data overfitting, we reduce the number of chunks as well as the overfitting models since they are bounded by the nature of the content, which means our method can keep a minimum number of chunks regardless the duration of videos. Moreover, since each chunk has similar data patches, we can actually use smaller SR models without handcrafted modules for the overfitting task, which reduces the computation burden for devices of the end-user. 
We advance our method into a \emph{single} overfitting model by a data-aware joint training technique (\textbf{JSTDO}), which further reduces the storage requirement with negligible quality drop.
%Our experimental results demonstrate that our method achieves real-time video resolution upscaling from 270p to 1080p on an off-the-shelf mobile phone with high PSNR. Compared with the state-of-the-art, our method achieves 28 fps streaming speed with 41.6 PSNR, which is 14$\times$ faster and 2.29 dB better in the live video resolution upscaling tasks.



\begin{table}[t]
\linespread{0.9}
\small
\centering
\caption{Comparison results of STDO and JSTDO with two representative works on different SR backbones.}
\vspace{-0.5em}
\scalebox{0.9}{
\begin{tabular}{llccccc}
\toprule
 &\bf{Data} &\bf{\# of} &\bf{\# of}& \multicolumn{3}{c}{\bf{inter-45s PSNR}}    \\
\bf {Model} &\bf{Scale} &\bf{Chunks} &\bf{Models} & $\times$2 & $\times$3 & $\times$4  \\

\midrule 
\multirow{4}{*}{VDSR}
& NAS~\cite{yeo2018neural}&9&9& 42.40 & 34.53 & 31.10 \\
& CaFM\cite{liu2021overfitting} & 9&1& 42.86 & 34.49 & 30.95 \\
& \bf{STDO} &4&4 &\textbf{43.36}& \textbf{35.64} & \textbf{31.77}  \\
& \bf{JSTDO} & 4&1 & \textbf{42.74} & \textbf{35.01} &\textbf{31.03} \\
\midrule
\multirow{4}{*}{EDSR} 
& NAS~\cite{yeo2018neural} &9&9& 43.31 & 35.80 & 32.67 \\
&CaFM~\cite{liu2021overfitting}   &9&1& 43.37 & 35.62 & 32.35 \\
& \bf{STDO}  &4&4 & \textbf{44.52} & \textbf{38.28} & \textbf{35.51} \\ 
& \bf{JSTDO}& 4&1 & \textbf{44.06} & \textbf{37.59} &\textbf{34.81}  \\
\midrule
\multirow{4}{*}{WDSR}
& NAS~\cite{yeo2018neural}&9&9 & 43.41 & 36.05 & 33.11 \\
& CaFM\cite{liu2021overfitting}  &9&1 & 43.52 & 36.03 & 32.97 \\
& \bf{STDO} & 4&4& \textbf{44.54} & \textbf{38.72} & \textbf{36.03}  \\ 
& \bf{JSTDO} & 4&1& \textbf{44.96} & \textbf{38.05} &\textbf{35.24}   \\

\bottomrule
\end{tabular}}
\label{tab:different_backbones}
\vspace{-0.4cm}
\end{table}


We conduct extensive experiments to prove the advantages of our proposed methods. We compare our method with the state-of-the-art (SOTA) methods that either use general model overfitting or time-divided model overfitting on different SR backbones. We sample three video categories from VSD4K, and test on two different video time lengths as 15$s$ and 45$s$. We compare with the state-of-the-art neural network-based SR video delivery methods, such as awDNN~\cite{yeo2017will} where a video is overfitted by a model, NAS~\cite{yeo2018neural} that splits a video into multiple chunks in advance and overfit each of the time-divided chunks with independent SR models, and CaFM~\cite{liu2021overfitting} that uses time-divided video chunks and a single SR model with hand-crafted module to overfit videos. 
For our implementation of STDO, we divide the spatial-temporal data into 4 chunks.
We show the comparison by computing the PSNR of each method. Partial results that use inter-45$s$ video overfitted by NAS~\cite{yeo2018neural}, CaFM~\cite{liu2021overfitting}, STDO and JSTDO are shown in Table~\ref{tab:different_backbones}. It can be concluded from our results that that our method can exceed the SOTA works consistently on different backbones. 
\begin{table}[t]
% \linespread{1.1}
\small
\centering
% \vspace{0.4cm}
\caption{Computation cost for different backbones with VSD4K video game-45s. We include the computation cost for the models with different resolution upscaling factors.}
\scalebox{0.99}{
\begin{tabular}{lcccc}
\toprule 
\bf { Model } & \bf { Scale } &  \bf{FLOPs} &\textbf{CaFM~\cite{liu2021overfitting}} &\textbf{STDO}  \\
\midrule \multirow{3}{*}{\text { ESPCN }} & $\times$2 & 0.14G & 36.09 & 37.75   \\
&$\times$3 & 0.15G & 31.06 & 32.29  \\
&$\times$4  & 0.16G & 29.05 & 29.96   \\
\midrule \multirow{3}{*}{\text { SRCNN }} & $\times$2  & 0.64G &  35.49 & 36.74   \\
&$\times$3 & 1.45G & 30.63 & 31.46  \\
&$\times$4  & 2.58G & 28.66 & 29.37   \\
\midrule \multirow{3}{*}{\text { VDSR }} & $\times$2  & 6.15G & 41.92 & 42.65   \\
& $\times$3 & 13.85G & 35.56 & 36.23  \\
&  $\times$4  & 20.62G & 33.16 & 33.76   \\
\midrule \multirow{3}{*}{\text { EDSR }} & $\times$2  & 3.16G & 43.32 & 45.65   \\
&$\times$3 & 3.60G & 37.19 & 39.93 \\
&$\times$4  & 4.57G & 34.61 &  37.24  \\
\midrule \multirow{3}{*}{\text { WDSR }} & $\times$2  & 2.73G & 43.97 & 45.71   \\
&$\times$3 & 2.74G & 37.64 &40.33  \\
&$\times$4  & 2.76G & 35.12 & 37.76   \\
\bottomrule
\end{tabular}}
\label{tab:CaFM_compare}
% \vspace{-0.4cm}
\end{table}

With STDO, each SR model is only overfitting one video chunk that has similar information density, which makes it suitable to use smaller and low capacity SR models that has low computations.
In Table~\ref{tab:CaFM_compare}, we demonstrate the computation cost on each model. 
% We also implement our method on WDSR which is a compact model with good performance, and it shows higher PSNR than all other models. 
From the results, we notice that with the relatively new model such as VDSR, EDSR, and WDSR, when the computation drops below 3 GFLOPs, time-divided method experiences significant quality degradation, while STDO maintains its performance or even achieves quality improvements. When using extremely small networks such as ESPCN or SRCNN, time-divided methods PSNR drops quickly, while STDO still achieves 0.7 $\sim$ 1.7 dB better performance.



We deploy the video chunks alongside with the overfitting models of STDO on a Samsung Galaxy S21 with Snapdragon 888 to test execution performance. Our results are shown in Figure~\ref{fig:radar_edsr}, which demonstrates that our method achieves 28 FPS when super-resolving videos from 270p to 1080p by WDSR, and it is significantly faster in speed and better in quality than other models such as EDSR or VDSR that are originally used in other baseline methods.

\begin{figure}[h]
\centering
\vspace{-0.2cm}
\includegraphics[width=0.8\columnwidth]{figs/rader_two.pdf}
\vspace{-0.5em}
\caption{Execution speed and video quality comparison between STDO and \cite{liu2021overfitting}\cite{yeo2017will} respectively using an Samsung mobile phone.}
\label{fig:radar_edsr}
\end{figure}


%%%%%%%%% REFERENCES
\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
