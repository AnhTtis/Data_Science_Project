\begin{table*}
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l *{5}{c} }
\toprule
& \multicolumn{3}{c}{\textbf{STRUCT}} & \textbf{LEX} & \textbf{Overall} \\
\textbf{Model} & Obj PP $\rightarrow$ Subj PP & CP Recursion & PP Recursion &  & \% \\
\midrule
BART~\cite{lewis2019bart} & 0 & {\phz}0 & 12 & 91 & 79\rlap{$^\dagger$} \\
BART+syn~\cite{lewis2019bart} & 0 & {\phz}5 & {\phz}8 & 80  & 80\rlap{$^\dagger$} \\
T5~\cite{raffel2020exploring} & 0 & {\phz}0 & {\phz}9 & 97 & 83\rlap{$^\dagger$} \\ \midrule
\citealt{kim2020cogs} & 0 & {\phz}0 & {\phz}0 & 73 & 63 \\
\citealt{ontanon2021making} & 0 & {\phz}0 & {\phz}0 & 53 & 48 \\
\citealt{akyurek2021lexicon} & 0 & {\phz}0 & {\phz}1 & 96 & 82 \\
\citealt{conklin2021meta} & 0 & {\phz}0 & {\phz}0 & 88 & 75 \\
\citealt{csordas2021devil} & 0 & {\phz}0 & {\phz}0 & 95 & 81 \\ \midrule
\citealt{zheng2021disentangled} & 0 & 25 & 35 & 99 & 88\rlap{$^\ddagger$} \\
\bottomrule
\end{tabular}}
\caption{Results on the COGS benchmark for different generalization splits, including recent seq2seq models specialized for COGS. {}$^\dagger$Models use pretrained weights, and their results are copied from \citet{yao2022structural}. {}$^\ddagger$Model uses pretrained weights and is hyperparameter tuned using data sampled from the generalization splits. Our focus is on the factors behind the strikingly bad performance of all models, but especially the models that are not pretrained, on the structural generalization splits.} \label{tab:cogs-seq2seq-results}
\end{table*}
