% -----------------------------------------------
% Template for SMC 2023
% based on SMC 2022 template
% -----------------------------------------------

\documentclass{article}
\usepackage{smc}
\usepackage{times}
\usepackage{ifpdf}
\usepackage[english]{babel}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xpatch}

\makeatletter
\xpatchcmd{\algorithmic}
  {\ALG@tlm\z@}{\leftmargin\z@\ALG@tlm\z@}
  {}{}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%% Some useful packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%% See related documentation %%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{amsmath} % popular packages from Am. Math. Soc. Please use the 
%\usepackage{amssymb} % related math environments (split, subequation, cases,
%\usepackage{amsfonts}% multline, etc.)
%\usepackage{bm}      % Bold Math package, defines the command \bf{}
%\usepackage{paralist}% extended list environments
%%subfig.sty is the modern replacement for subfigure.sty. However, subfig.sty 
%%requires and automatically loads caption.sty which overrides class handling 
%%of captions. To prevent this problem, preload caption.sty with caption=false 
%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}


%user defined variables
\def\papertitle{Generating symbolic music using diffusion models}
\def\firstauthor{Lilac Atassi}
% \def\secondauthor{Second author}
% \def\thirdauthor{Third author}

% adds the automatic
% Saves a lot of output space in PDF... after conversion with the distiller
% Delete if you cannot get PS fonts working on your system.

% pdf-tex settings: detect automatically if run by latex or pdflatex
\newif\ifpdf
\ifx\pdfoutput\relax
\else
   \ifcase\pdfoutput
      \pdffalse
   \else
      \pdftrue
\fi

\ifpdf % compiling with pdflatex
  \usepackage[pdftex,
    pdftitle={\papertitle},
    pdfauthor={\firstauthor, 
    % \secondauthor,
    % \thirdauthor
    },
    bookmarksnumbered, % use section numbers with bookmarks
    pdfstartview=XYZ % start with zoom=100% instead of full screen; 
                     % especially useful if working with a big screen :-)
   ]{hyperref}
  %\pdfcompresslevel=9

  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are and their extensions so 
  %you won't have to specify these with every instance of \includegraphics
  \graphicspath{{./figures/}}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}

  \usepackage[figure,table]{hypcap}

\else % compiling with latex
  \usepackage[dvips,
    bookmarksnumbered, % use section numbers with bookmarks
    pdfstartview=XYZ % start with zoom=100% instead of full screen
  ]{hyperref}  % hyperrefs are active in the pdf file after conversion

  \usepackage[dvips]{epsfig,graphicx}
  % declare the path(s) where your graphic files are and their extensions so 
  %you won't have to specify these with every instance of \includegraphics
  \graphicspath{{./figures/}}
  \DeclareGraphicsExtensions{.eps}

  \usepackage[figure,table]{hypcap}
\fi

%setup the hyperref package - make the links black without a surrounding frame
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black
}


% Title.
% ------
\title{\papertitle}

% Authors
% Please note that submissions are NOT anonymous, therefore 
% authors' names have to be VISIBLE in your manuscript. 
%
% Single address
% To use with only one author or several with the same address
% ---------------
\oneauthor
  {\firstauthor} {University of California, San Diego \\ %
    {\tt \href{mailto:latassi@ucsd.edu}{latassi@ucsd.edu}}}

%Two addresses
%--------------
% \twoauthors
%   {\firstauthor} {Affiliation1 \\ %
%     {\tt \href{mailto:author1@smcnetwork.org}{author1@smcnetwork.org}}}
%   {\secondauthor} {Affiliation2 \\ %
%     {\tt \href{mailto:author2@smcnetwork.org}{author2@smcnetwork.org}}}

% Three addresses
% --------------
 % \threeauthors
 %   {\firstauthor} {Affiliation1 \\ %
 %     {\tt \href{mailto:author1@smcnetwork.org}{author1@smcnetwork.org}}}
 %   {\secondauthor} {Affiliation2 \\ %
 %     {\tt \href{mailto:author2@smcnetwork.org}{author2@smcnetwork.org}}}
 %   {\thirdauthor} { Affiliation3 \\ %
 %     {\tt \href{mailto:author3@smcnetwork.org}{author3@smcnetwork.org}}}


% ***************************************** the document starts here ***************
\begin{document}

\emergencystretch 1em

%
\capstartfalse
\maketitle
\capstarttrue
%
\begin{abstract}
Probabilistic Denoising Diffusion models have emerged as simple yet very powerful generative models. Diffusion models unlike other generative models do not suffer from mode collapse nor require a discriminator to generate high quality samples. In this paper, we propose a diffusion model that uses a binomial prior distribution to generate piano-rolls. The paper also proposes an efficient method to train the model and generate samples. The generated music has coherence at time scales up to the length of the training piano-roll segments. We show how such a model is conditioned on the input and can be used to harmonize a given melody, complete an incomplete piano-roll or generate a variation of a given piece. The code is shared publicly to encourage the use and development of the method by the community.
\end{abstract}
%

\section{Introduction}\label{sec:introduction}

One of the advantages of generating symbolic music compared to audio by machine learning (ML) is that manipulating the generated material is feasible using conventional tools for composition. This possibility allows a composer or a musician to collaborate with the ML model more easily. In addition, using specifically binary piano-rolls, i.e. without dynamics, requires less computational resources for training ML models compared to training ML models with spectrograms or even piano-rolls with dynamics. The shorter training and sampling time of light weight models allows more rapid experimentation. That is the reason that in this paper most of the attention is on the methods that can be applied to binary piano-rolls. Nonetheless, most of the presented methods can be generalized to models for other forms of piano-rolls, e.g. with dynamics.

The approaches to generative models can be divided into two broad categories. In the first category of approaches, the data distribution is estimated directly. For example, fitting a normal distribution on a set of samples falls in this category. This approach does not scale to high-dimensional and complex distributions. Meaning, to increase the accuracy of the estimation of the real data points distribution an impractical amount of computation is required.

The second approach does not estimate the data distribution directly. Instead, this approach estimates a function that transforms a sample from a prior distribution to the data distribution. The prior distribution is chosen to be simple, that is its probability distribution function has no more than few parameters. Therefore, sampling the prior distribution is straightforward. Generative Adversarial Networks (GANs)~\cite{smc3}, Variational Autoencoders (VAEs)~\cite{smc2}, and Denoising Diffusion Probabilistic (DDP) models~\cite{smc4} or for short diffusion models fall in this category. In particular, diffusion models transform a sample from the prior distribution to the data distribution in several gradual steps. 

Multiple methods based on GANs~\cite{smc5} and VAEs~\cite{smc6} have been proposed for music generation in the literature. Mittal et al. in~\cite{smc7} propose using a VAE to encode 32 two-bar segments. The diffusion model in the latent space of the VAE generates new samples. A similar model has also been explored for images~\cite{smc8}. One reason for using a VAE is that the discrete music data is transformed to a continuous latent space where using a diffusion model with a normal prior distribution would be feasible. Our proposed method simply generates piano-rolls using a binomial prior without the need of transforming the data to continuous space. One advantage of our direct approach is that it is possible to use the model to harmonize a given melody. Or more generally, the model can infill a given piano-roll that has some rows and columns masked. The method in~\cite{smc7} can infill only the masked columns. The second advantage of our approach is that the quality of the generated piano-rolls is not limited by the decoder of the VAE.

The following sections briefly review the forward and reverse process of the diffusion models, explain our proposed methods for the forwards process to generate noisy binary piano-rolls, and discuss our proposed sampling algorithm. The experiments and experimental results are presented at the end. The code used in the experiments is available at  \url{https://github.com/Lilac-Atassi}.

\section{Diffusion models}

Diffusion models  \cite{smc4} apply a diffusion transition kernel $T_\pi$,
\begin{align}
    q\left(x_{t}|x_{t-1}\right) &= T_\pi\left(x_{t} | x_{t-1}; \beta_{t}\right) \label{eq:kernel_shorthand},
\end{align}
repeatedly to the input data $x_{0}$ to transform the data distribution to the prior distribution,
\begin{align}
    q\left(x_{0:T}\right) &= q\left(x_{0}\right) \prod_{t=1}^{T} q\left(x_{t} | x_{t-1}\right) \label{eq:diffuse_forward}.
\end{align}
$\beta_t$ is the diffusion rate and $T$ is the number of diffusion steps. The kernel for a binomial prior is 
\begin{align}
B(x_{t}; x_{t-1} (1-\beta_t) + 0.5\beta_t), \label{eq:binomial_kernel}
\end{align}
which can be used for binary vectors such as binary piano-rolls. The data generated by the forward process is used to train a neural network which is used in the reverse process, also known as the sampling process. By training the model on pairs of $(x_{t}, x_{0})$, proposed in \cite{smc9}, simply L2 can be used as the training loss function. Then in the sampling process, the model alternates between predicting the noiseless sample $x_{0}$ and adding noise corresponding to $t-1$.

\section{Efficient forward process}
The authors in \cite{smc9} also propose a new equation for the Gaussian kernel in the forward process that depends only on the input $x_0$. This has two benefits. First, to compute the output at time step $t$ there is no need to compute the output at the time steps before $t$. This way the output at random $t$s can be computed and used to train the neural network for the reverse process. That means with the kernel from the original paper \cite{smc4}, the whole set of noisy training data should be generated and stored in memory. This is needed to shuffle the order of noisy training data in mini-batches, which is required for training neural networks using mini-batch gradient descent. With this updated kernel, it is possible to first pick a random order for the noisy piano-rolls and then generate them on the fly without generating the preceding noisy piano-rolls. Therefore, before training the network we do not need to generate all the noisy piano rolls, which is the case in \cite{smc4}. As a result the amount of memory required is just to hold a single mini-batch and not the whole training set for the neural network. Second, this simplification of the kernel equation simplifies the loss function for training the neural network. The normal kernel with dependency on $x_{0}$ is presented in \cite{smc9},
\begin{align}
    \alpha_t &= 1 - \beta_t\\
    \bar{\alpha}_t &= \prod_{s=1}^t \alpha_s\\
    q(x_t | x_{0}) &=N(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)
\end{align}

To derive a similar binomial kernel with dependency only on $x_{0}$, in Equation~\ref{eq:binomial_kernel}, the success probability can be substituted with the probability from the step before at $t-1$,
\begin{align}
q(x_t | x_{t-2}) =B\left(x_{t}; x_{t-2} \alpha_t \alpha_{t-1} + (1-\alpha_t \alpha_{t-1}) 0.5\right). 
\end{align}
By repeating the success probability substitution from the prior step until $t=0$, the binomial kernel that depends only on the input $x_0$ is proved to be 
\begin{equation}
    q(x_t | x_{0}) = B(x_t; \bar{\alpha}_t x_0 + (1-\bar{\alpha}_t) 0.5). \label{eq:binomial_kernel_jump}
\end{equation}
Therefore, it is possible to just change the diffusion rate $\beta_t$ schedule in Equation~\ref{eq:binomial_kernel} and arrive at Equation~\ref{eq:binomial_kernel_jump}. $0.5$ in the binomial kernel is the success probability of the prior binomial distribution. In an application, the average ratio of ones or successes to the dimensionality of $x_0$ replaces $0.5$.

\section{Sampling algorithm}

\begin{algorithm}
\caption{Generating new samples}\label{alg:sample}
\begin{algorithmic}
\State \textbf{Input:} A piano-roll sampled from a binomial distribution $x_T$
\For {$t=T,T-1,\ldots,1$}
    \State $\hat{x}_0 = \mathrm{UNet}(x_t)$
    \State $\delta = x_T \oplus \hat{x}_0 $
    \State $\mathrm{mask} \sim B(\delta \beta_t) $
    \State ${x}_{t-1} = \hat{x}_0 \odot (1-$mask$) + x_T \odot \mathrm{mask}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
\centering
\includegraphics[width=1.0\columnwidth]{sampling_same_noise.png}
\caption{Visualizing the the sampled piano-roll along the sampling path, $x_t$ in Algorithm~\ref{alg:sample} for $t=100,75,50,25,1$, from top to bottom. It is visually evident the original binomial noise is reduced in each step.\label{fig:sampling_same_noise}}
\end{figure}


The proposed sampling method in this section is inspired by the method in \cite{smc10}. It is shown in \cite{smc10} that with a neural network that cannot perfectly denoise a given sample, the usual sampling method used with diffusion models can be unstable. Consequently the generated samples are of poor quality. With an improved sampling method, the quality of the generated sample can improve more consistently.

In the simple or naive sampling method a sample $x_T$ from the binomial distribution is passed to the neural network to estimate the noiseless sample $\hat{x}_0$. Then using the binomial kernel, some noise is added to $\hat{x}_0$ to get $x_{T-1}$. This process of alternating between adding noise and denoising is repeated. This method is inefficient as the noise added by the kernel in each step is independent of the noise in the previous step. To see this effect, using the kernel twice with the same $\hat{x}_0$ we get two points that are far away from each other. Therefore, after some noise is added to $\hat{x}_0$, $x_{t-1}$ might be very far from $x_{t}$. In summary, $\hat{x}_0$ might move to new points far from each other in each of the first few steps. As a result, the computation in those first few steps would be wasted. 

An improved sampling method in each step would partially add back the noise from $x_T$ to $\hat{x}_0$. The proposed method is presented in Algorithm~\ref{alg:sample}. The neural network, this case a UNet (explained in the next section), is used to denoise the sample. The difference $\delta$ between the denoised sample $\hat{x}_0$ and noise sample $x_T$ is computed using the exclusive or operator. A gradually shrinking subset of $\delta$ in each step is used as a mask that selects what elements of the piano-roll should be picked from the noise sample $x_T$. The remaining elements in the piano-roll are taken from $\hat{x}_0$. Figure~\ref{fig:sampling_same_noise} shows a piano-roll $x_t$ at steps $t=100,75,50,25$ and $1$ generated by the improved sampling method. It is possible to see in the samples a subset of the noise remains in the piano-roll as the sampling method progresses. As parts of the reconstructed piano-roll are preserved in each step, no computation is wasted, and the generated sample quality is improved.


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{lunet.png}
\caption{This figure depicts the UNet architecture used in the experiments. The input piano-roll is a matrix of size $56\times384$. Each green arrow show a conv layer. The rectangles depict the featuremaps, with the number above each rectangle showing the depth of the featuremap tensor which is also the number of conv filters in the corresponding layer. The red arrows shows the conv layers with stride of two that reduce the size (rows and columns) of the featuremaps by 2. The purple arrows show the upsampling step using the nearest neighbor interpolation which doubles the size of the featuremaps. The grey arrows depict copying a featuremap from the left side of the network to the right side. On the right side, the copied featuremaps are stacked on the upsampled featuremaps, the result is a featuremap with double the depth of the featuremap tensor below. The last conv layer, top right, contains a single convolution filter, as a result the output is a single piano-roll. The number of rows and columns of the tensor in each row or level of UNet remain the same due to padding the input when applying convolution filters.\label{fig:lunet}}
\end{figure*}

\section{Neural network architecture}

The UNet architecture was introduced in 2015 for image segmentation \cite{smc11}. It resembles the typical architecture of autoencoders, called often the hourglass architecture, that in multiple steps subsamples the feature-maps in the first half the network and then in the second half the featuremaps are upsampled to finally have the identical size of the input. The difference between UNet and hourglass is that UNet has horizontal connections that connect the feature-maps at the same scale from the first half of the network to the second half. The UNet architecture that is used in this paper is shown in Figure~\ref{fig:lunet}.


The second main operation in UNet is downampling the featuremaps. Downsampling is done by using conv filters with stride of 2, skipping every other element of the input tensor horizontally and vertically. Using a conv layer with stride of 2 to downsample feature-maps has an advantage over just dropping or decimating every other column and row. The learnable weights of the conv filters have an opportunity to generate featuremaps that preserve information while reducing the size of featuremaps.

The third main operation in UNet is upsampling the featuremaps. On the right side of the network, the featuremaps are upsampled from higher scales to be combined with the featuremaps at lower scales. The upsampling process simply duplicates the values in the input tensor. Upsampling brings the information from higher scales to lower scales, which is used then to guide the finer details.


\section{Training the model}
To train the diffusion model, I used the Maestro dataset that contains MIDIs and audio files of recorded performances from piano performance competitions. In the dataset there are just over 2,000 MIDI files and the title and composer name for each MIDI is in a text file. I went through the list and removed the the performances of the same piece by keeping only one of the MIDI files per title. At this point, about 500 MIDI files were left out of 2,000. Then using a Python script, I found that there are 78 MIDIs that have pitches only between 33(A1) and 88 (E6) MIDI notes (56 MIDI notes).

The 78 MIDIs were converted to piano-rolls. The resolution used in the conversion was 1 beat in the MIDI file converted to 24 ticks. Each piano-roll was then divided into non-overlapping segments of 16 beats or 384 (16×24) ticks. This process at the end yielded 2,044 piano-roll segments that were used to train the model.

The number of diffusion steps, $T$, used in the experiments was set to 100. The UNet neural network was trained over 50 epochs. In each epoch, each one of the 2,044 piano-roll segments was used to generate 100 noisy piano-rolls that vary from no noise to all the way being a sample from the binomial distribution. Therefore, in each epoch there were 204,400 training samples. Training the UNet network on the 50 epochs took
about 48 hours on two NVidia A6000 GPUs.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\columnwidth]{max_act_l4_filter_3.png}
\includegraphics[width=1.0\columnwidth]{max_act_l4_filter_5.png}
\includegraphics[width=1.0\columnwidth]{max_act_l4_filter_6.png}
\caption{Using AM, the input that maximizes the activation of three convolution filters at the lowest resolution of the UNet trained on piano-rolls are visualized. The visualized paino-rolls reveal the filters at this level, after three downsampling steps have a high resolution of the input piano-roll without losing information along the pitch or time axes.}
\label{fig:max_act_1}
\end{figure}

\section{What the network sees}

One question regarding the choice of network architecture is whether the UNet network loses too much information along the pitch axis of the piano-rolls each time the input is downsampled. Given the UNet architecture is designed for processing images and not piano-rolls, it is a possibility some aspects of the architecture are just bad for processing a piano-roll with time along one axis and pitch along another versus an image that has identical units for the horizontal and vertical axes.

A method that to some degree reveals what each convolution filter in a network is looking at is Activation Maximization (AM) \cite{smc12}. The vanilla AM algorithm takes a random input. In my experiments, this is a piano-roll sampled from a binomial distribution. The neural network processes the input.
The gradient of a particular convolution filter’s average output with respect to the input is computed. Then with gradient ascent the input is updated. This in result increases the average of output feature-map of the convolution filter. This process is then repeated a fixed number of times.

In my experiments, I found after about 200 iterations the updated input does not change much. Applying activation maximization to three convolution filters of the last convolution layer at the bottom of UNet that is trained in my experiments, reveals the convolution filters have a high resolution view of the input piano-roll, Figure~\ref{fig:max_act_1}. Therefore, the three downsampling steps on the left side of UNet are not losing information along either pitch or time axis. If that was the case, and the convolution filters would have a low-resolution view of the piano-roll, the output of AM would have multiple elements along the time or pitch axis with the same color in the repeated patterns. This confirms using convolution filters with stride of two to downsample the feature-maps does not loose information as one expects from a simple decimation-based down-sampling method.


\begin{figure}[h]
\centering
\href{https://youtu.be/wu_kfcpzAPI}{\includegraphics[width=0.99\columnwidth]{feb8_scratch_8_90bpm.mov.jpg}}
\href{https://youtu.be/r3Rn91t3g5A}{\includegraphics[width=0.99\columnwidth]{feb8_scratch_10_90bpm.mov.jpg}}
\href{https://youtu.be/KEAtbzAScYY}{\includegraphics[width=0.99\columnwidth]{feb8_scratch_14_90bpm.mov.jpg}}
\caption{Three generated piano-rolls unconditionally. Each piano-roll in this figure is linked to the synthesized audio. To take a listen, click on one of the piano-rolls.}
\label{fig:uncondtional}
\end{figure}


\begin{figure}[h]
\centering
\href{https://youtu.be/-Z5FaaligNg}{\includegraphics[width=0.99\columnwidth]{feb8_variation_original.mov.jpg}}
\href{https://youtu.be/pgnU8VLQNyQ}{\includegraphics[width=0.99\columnwidth]{feb8_prompt_half_4.mov.jpg}}
\caption{Top: original piano-roll segment. In the following experiments the same piano-roll is used as well. Bottom: the diffusion model is prompted with the first half of the original piano-roll and the second half is generated by the model. As the sampling process is stochastic, many samples can be generated using the same prompt. Here a sample are presented. Each piano-roll in this figure is linked to the synthesized audio. To take a listen, click on one of the piano-rolls.}
\label{fig:prompt_first_half}
\end{figure}

\section{Experiments}

A diffusion model can be used to generate unconditional samples following the sampling method in Algorithm~\ref{alg:sample}. Three generated piano-rolls using this method are presented in Figure~\ref{fig:uncondtional}. Each piano-roll in this and following figures is linked to the synthesized audio. Each of the four bars in the piano-rolls are marked at the top of the pictures.

One of the interesting aspects of diffusion models is that they can be used as generative models conditioned on any part of the piano-roll. For instance, it is possible to prompt the model with the first half of the piano-roll and the model generates the remaining half. To achieve this, the sampling algorithm is modified by excluding the part containing the prompt from the process that adds noise to the piano-roll. We start with a sample piano-roll from the binomial distribution. Replace the beginning of the noise sample with the prompt. The network takes this piano-roll and attempts to remove the noise. Then a smaller amount of noise is added to piano-roll. After that, the original prompt segment is placed back on the noisy piano-roll and the process repeats. At the end, the model generates a piano-roll that is coherent with the prompt. The example in Figure~\ref{fig:prompt_first_half} demonstrates this approach to infilling piano-rolls. The original piano-roll that the model is conditioned on is not in the training set of the model. the same piano-roll is used in the following experiments with conditioning the model.


An experiment similar to the previous using the conditional aspect of diffusion models is to prompt the beginning and end of a piano-roll and the model fills the middle segment. I thought this would be an interesting experiment to see how close would the generated style be to the original score composed by the composer. The process is similar to the previous experiment that the prompt segments are overwritten on the piano-roll at the end of the sampling iterations. Figure~\ref{fig:prompt_first_last_quarter} presents a generated piano-roll using this process.

\begin{figure}[h]
\centering

\href{https://youtu.be/0_xM3XFjd1w}{\includegraphics[width=0.99\columnwidth]{feb8_prompt_qq_2.mov.jpg}}

\caption{The model fills the middle half of the piano-roll to be coherent with the first and last quarter. The piano-roll in this figure is linked to the synthesized audio.}
\label{fig:prompt_first_last_quarter}
\end{figure}

Another aspect of diffusion models is that in the latent space, which is the noisy piano-roll in my experiments, the pieces that are similar sounding are close in the latent space. Therefore, it is possible to add noise to a piano-roll, for instance the noise level of step 80 out of 100 in the forward process, and then denoise the model using the diffusion model sampling method. The difference is that the starting point of the sampling algorithm is not a sample from the binomial distribution, but it is a piano-roll with added noise at the level corresponding to the step 80 of the forward process, for instance. The output would have similarities to the original piano-roll and can possibly be called a variation of the original piano-roll. As the sampling process is stochastic, it is possible to generate multiple variations of a single piano-roll. Using the same original piano-roll as in the previous experiments, three variations of it are generated and shown in Figure~\ref{fig:variation}.


\begin{figure}[h]
\centering

\href{https://youtu.be/vvpXlKrablk}{\includegraphics[width=0.99\columnwidth]{feb8_variation_t_25.mov.jpg}}

\href{https://youtu.be/zukJpDQOUPI}{\includegraphics[width=0.99\columnwidth]{feb8_variation_t_75.mov.jpg}}

\href{https://youtu.be/KC4qDVuU2p0}{\includegraphics[width=0.99\columnwidth]{feb8_variation_t_95.mov.jpg}}

\caption{From top to bottom, the initial points are at $t=25,75,95$ in the sampling algorithm to generate variations of the original given piano-roll. The sampling method with fewer iterations generates output that is more similar to the the given noisy piano-roll. The bottom sample hardly resembles the original piano-roll. The top sample sounds similar to the original piano-roll. Each piano-roll in this figure is linked to the synthesized audio.}
\label{fig:variation}
\end{figure}

It is also possible to prompt the network along the pitch axis. For example, one can provide a melody line to be harmonized by the model. Here is an example of this experiment where an improvised melody is given to the model. The melody and the combined melody and harmony piano-rolls are presented in Figure~\ref{fig:harmonize}.


\begin{figure}[ht]
\centering
\href{https://youtu.be/unN9BBbpPOE}{\includegraphics[width=1.0\columnwidth]{Diffusion_expeirment1_harmonize_melody.mov.jpg}}
\href{https://youtu.be/QVDB1UfW9dc}{\includegraphics[width=1.0\columnwidth]{harmonize_exp1_sample2.mov.jpg}}
\caption{The trained diffusion model can generate harmony for a given melody. For this example, I improvised the melody line, the piano-roll at the top. The generated harmony, the piano-roll at the bottom, by the diffusion model sounds idiomatic in the style of common practice period composers utilising standard harmonic progressions. Each piano-roll in this figure is linked to the synthesized audio.\label{fig:harmonize}}
\end{figure}

\section{Conclusions}
Binomial diffusion models can be used directly to generate piano-rolls. This paper proposes an efficient forward binomial kernel for diffusion models. This kernel reduces the computational resources needed during the training process of the model. An improved sampling method for binomial diffusion models is also introduced that can more consistently produce high quality samples. Multiple generated piano-rolls using the proposed method are presented to demonstrate the model can be prompted with piano-roll segments and with an imposed melody can harmonize. The required computational resources by the method to generate piano-rolls longer than 32 beats are prohibitive. Extending the method to generate longer piano-rolls remains for future work. 

% \begin{acknowledgments}
% At the end of the Conclusions, acknowledgements to people, projects, funding agencies, etc. can be included after the second-level heading  ``Acknowledgments'' (with no numbering).
% \end{acknowledgments} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%bibliography here
\bibliography{smc2023bib}

\end{document}
