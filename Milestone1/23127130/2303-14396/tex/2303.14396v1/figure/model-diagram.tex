\begin{figure*}[h!]
\centering
\includegraphics[width=0.95\textwidth]{resource/segmentation-pipeline-new.pdf}
\vspace{0.05in}
\caption{\textbf{Illustration of the semantic segmentation in VL encoder-decoder.} 
Our method incorporates a transformer encoder-decoder ($f_\mathtt{enc}, f_\mathtt{dec}$) along with an external image backbone ($f_\mathtt{img}$) for tokenizing a given image. Given a pair of an image and a prompt sentence, the transformer generates contextualized embeddings through its self-attention layers. The decoder then sequentially predicts the probability distribution over the semantic categories in a region (\emph{e.g.,} $\mathbf{p}^{(i)}$), by transforming an input composed of the special begin-of-sequence (BOS) embedding and the contextualized embeddings at the preceding region indices (\emph{e.g.,} $[\mathbf{e}_\mathtt{BOS};{f}^{(0)}(\mathbf{e}_\mathtt{x});...;{f}^{(i-1)}(\mathbf{e}_\mathtt{x})]$) through its self-attention and cross-attention layers. Finally, bilinear interpolation is applied to obtain the final prediction in a desired spatial size.
} \label{fig2:segmentation_overview}
\vspace{-0.1in}
\end{figure*}