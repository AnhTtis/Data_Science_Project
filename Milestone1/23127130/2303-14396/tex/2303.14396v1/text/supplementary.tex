\newpage
\onecolumn
\clearpage
\begin{center}{\bf {\LARGE Supplementary Material}}
\end{center}
\vspace{0.05in}
\appendix

\section{More experiments with stronger supervision}
In this section, we consider two additional scenarios when a stronger level of supervision is available; external semantic categories (see \cref{sec:larger-set}), external training images and annotations (see \cref{sec:external-images}).
Lastly, we present a comparison with (weakly) supervised baselines using the self-training technique (see \cref{supple:selftraining}).

\subsection{External segmentation categories}\label{sec:larger-set}
We investigate the effect of (a) hierarchical semantic categories and (b) external semantic categories from other sources.

\vspace{0.02in}
\noindent
\textbf{Hierarchical semantic categories.} 
Hierarchical semantic categories can be a stronger supervision for our artificial image creation.
Specifically, we explore the semantic hierarchy as the language by supervising the model with multiple example words (\emph{i.e.}, fine-grained categories) per single semantic category (\emph{i.e.}, coarse categories). 
To this end, we first introduce the COCO Stuff benchmark~\cite{caesar2018coco} with the 27 coarse semantic categories, which remaps the original 171 fine-grained categories in the COCO stuff benchmark to the 27 coarse categories.\footnote{The full list of hierarchy between the coarse and fine-grained categories are given in \cref{tbl:coco-coarse-mapping}.} 
Then we augment each coarse category's words with those from its fine-grained categories for generating the artificial image;
we slightly alter the artificial image creation in \cref{sec2:ifseg} to sample $h \cdot w$ coarse categories first, then perform additional sampling that actually assigns a word among the fine-grained categories associated with each coarse category. 
In our experiments, we empirically found that such hierarchical supervision significantly improves the performance of our method from 21.2 to 31.0 (+ 9.8) mIoU on the 27 coarse categories of the COCO Stuff benchmark.
Furthermore, we provide a comparison with unsupervised semantic segmentation baselines on the coarse COCO Stuff benchmark. 
\cref{tbl:coco-word-augmentation} summarizes the results; our method consistently and significantly outperforms all the existing baselines. For example, our method significantly outperforms STEGO~\cite{hamilton2022unsupervised} by achieving 31.0 mIoU in an image-free manner, while STEGO does 26.8, despite it requires task-specific images for training.


\begin{table*}[h!]
\centering
\small
\begin{tabular}{l c l c c}
    \toprule
    Model & Text Backbone & Image Backbone & Image Dataset & mIoU \\
    \midrule
    IIC \cite{ji2019invariant} & \xmark & ResNet-18 & COCO (118k) & 2.4  \\
    PiCIE + H. \cite{cho2021picie} & \xmark & ResNet-18 & COCO (118k) &  14.4  \\
    TransFGU \cite{yin2022transfgu} & \xmark & ViT-S/8 & COCO (118k) &  17.5  \\
    STEGO \cite{hamilton2022unsupervised} & \xmark & ViT-S/8 & COCO (118k) & 26.8 \\
    \midrule
    CLIP$\dag$ \cite{radford2021learning, zhou2022extract}  & CLIP-ResNet \hfill \: & ResNet-101 & \xmark & 6.6 \\
    MaskCLIP$\dag$ \cite{zhou2022extract}  & CLIP-ResNet \hfill \: & ResNet-101 & \xmark & 6.9 \\
    OFA$\dag$ \cite{zhou2022extract}  & OFA-Base \hfill \: & ResNet-101 & \xmark & 2.2 \\
    IFSeg (ours)$\dag$ & OFA-Base \hfill \: & ResNet-101 & \xmark & \textbf{31.0} \\
    \bottomrule
\end{tabular}
\caption{\textbf{Comparison with unsupervised semantic segmentation baselines} on the COCO Stuff benchmark. 
We report the mIoU metric evaluated on the 27 coarse semantic categories of the COCO Stuff benchmark. $\dag$ denotes that our post-processing is applied.}
\label{tbl:coco-word-augmentation}
\end{table*}

\vspace{0.02in}
\noindent
\textbf{External semantic categories from other sources.} 
Here, we validate the effect of external semantic categories from other sources. 
To this end, we perform IFSeg using the 150 semantic categories of the ADE20K benchmark~\cite{zhou2017scene} and then evaluate it on the 15 unseen categories of the COCO Stuff benchmark.\footnote{We use the same vocabulary of the unseen semantic categories of the COCO Stuff in \cref{sec:image-free}: \emph{frisbee, skateboard, cardboard, carrot, scissors, suitcase, giraffe, cow, road, wall concrete, tree, grass, river, clouds, playing field}.}
Interestingly, even though only 4 semantic categories (\emph{road}, \emph{tree}, \emph{grass}, and \emph{river}) intersect between training and evaluation, our method still achieves a significant performance of 54.1 mIoU, which is close to 54.6 mIoU of ours in \cref{tbl1:coco_unseen}, which potentially indicates that our method with external categories from other sources could learn transferable representations to novel semantic categories.

\subsection{External training images and annotations}\label{sec:external-images}
In this section, we investigate further improvements of ours when external training images and annotations as we described in \cref{sec:ablation}. Overall, we empirically found that ours can achieve the best score compared to the baselines in both \cref{tbl2:cross2ade} and \cref{tbl3:unsup_coco} when such stronger supervision is available; 
for example, ours in the last row of \cref{tbl:unsup_selftraining} shows the best score by fine-tuning task-specific images with corresponding pseudo-labels with 8k additional training iterations. 
Here, we generate pseudo-labels via our pre-trained model following Zhou \emph{et al.} \cite{zhou2022extract}.
Moreover, we also observed that fine-tuning ours in \cref{tbl2:cross2ade} with class-agnostic masks gives further enhancements from 17.4\footnote{We empirically found that using hierarchical semantic categories for the ADE20K benchmark also improves the performance from 16.8 to 17.4 mIoU score. The hierarchy is publicly available at \url{https://groups.csail.mit.edu/vision/datasets/ADE20K/}.} to 20.9 mIoU with 60k additional training iterations following the configuration of ZSSeg~\cite{xu2022simple}, which is the strongest baseline and does 20.5 on the ADE20K benchmark (see \cref{tbl:cross_ade_selftraining}). 
We note that our values using training images are reported without the post-processing, including \cref{tbl4:ablation}.

\begin{table}[h]
\centering
\small
\scalebox{0.95}{
\begin{tabular}{llcc}
    \toprule
    Method & Backbone & Image Dataset & mIoU \\
    \midrule
    IIC \cite{ji2019invariant}& ResNet-18 & COCO (118k) & 0.6 \\
    PiCIE + H. \cite{cho2021picie}& ResNet-18 & COCO (118k) & 4.6  \\
    TransFGU \cite{yin2022transfgu}& ViT-S/8 & COCO (118k) & 11.9  \\
    MaskCLIP+ \cite{zhou2022extract} & ResNet-101 & COCO (118k) & {18.0} \\
    \cmidrule{1-4}
    CLIP$\dag$ \cite{radford2021learning,zhou2022extract}& ResNet-101 & \xmark & 4.5 \\
    MaskCLIP$\dag$ \cite{zhou2022extract}& ResNet-101 & \xmark & 13.7 \\
    OFA$\dag$ \cite{wang2022ofa} & ResNet-101 & \xmark & {1.5} \\ 
    IFSeg (ours)$\dag$ & ResNet-101 & \xmark & {16.9} \\ 
    \cmidrule{1-4}
    IFSeg (ours) & ResNet-101 & COCO (118k) & \textbf{18.4} \\ 
    \bottomrule
\end{tabular}
}
\caption{\textbf{Ablation study on the effect of external training images.} All models are evaluated on the 171 semantic categories of the COCO Stuff unsupervised segmentation benchmark. The last row indicates that fine-tuned result on training images of the COCO Stuff benchmark and corresponding pseudo labels generated by ours with 8k iterations. $\dag$ denotes that our post-processing is applied.}\label{tbl:unsup_selftraining}
\end{table}
\begin{table*}[h]
\centering
\small
\scalebox{0.95}{
\begin{tabular}{lllccc}
    \toprule
    Method & Text Backbone & Image Backbone & Image Dataset & Segmentation Label & mIoU \\
    \midrule
    LSeg+ \cite{li2022languagedriven, ghiasi2022scaling} & ALIGN-BERT-Large~\cite{jia2021scaling} & ResNet-101 & COCO (118k) & \cmark & 13.0 \\
    OpenSeg \cite{ghiasi2022scaling}& ALIGN-BERT-Large~\cite{jia2021scaling} & ResNet-101 & COCO (118k) & \cmark & 15.3 \\
    ZSSeg \cite{xu2022simple} & CLIP-ViT-B~\cite{radford2021learning} & ResNet-101 & COCO (118k) & \cmark & 20.5 \\
    \cmidrule{1-6}
    CLIP$\dag$ \cite{radford2021learning,zhou2022extract} & CLIP-ResNet~\cite{radford2021learning} & ResNet-101 & \xmark & \xmark & 3.9 \\
    MaskCLIP$\dag$ \cite{zhou2022extract} & CLIP-ResNet~\cite{radford2021learning} & ResNet-101 & \xmark & \xmark & 11.3 \\
    OFA$\dag$ \cite{wang2022ofa} & OFA-Base~\cite{wang2022ofa} & ResNet-101 & \xmark & \xmark & {0.5} \\
    IFSeg (ours)$\dag$ & OFA-Base~\cite{wang2022ofa} & ResNet-101 & \xmark & \xmark & {16.8} \\
    \cmidrule{1-6}
    IFSeg (ours) & OFA-Base~\cite{wang2022ofa} & ResNet-101 & COCO (118k) & \cmark & \textbf{20.9} \\
    \bottomrule
\end{tabular}
}
\caption{\textbf{Ablation study on the effect of external segmentation annotations.} All models are evaluated on the 150 semantic categories of the ADE20K benchmark. The last row indicates that our fine-tuned result on training images of the COCO Stuff benchmark and corresponding class-agnostic segmentation masks with 60k iterations following the configuration of ZSSeg. $\dag$ denotes that our post-processing is applied.}\label{tbl:cross_ade_selftraining}
\end{table*}



\subsection{Comparison on weakly-supervised zero-shot transfer scenario}\label{supple:selftraining}
In this section, we present a comparison between our method and (weakly) supervised baselines using the self-training technique \cite{bucher2019zero}, which has been widely used in VL-driven zero-shot segmentation literature.
Inspired by the weakly-supervised zero-shot transfer recipe proposed in MaskCLIP+ \cite{zhou2022extract}, we consider a weakly-supervised variant of our model, named IFSeg+, which is trained based on the ground truth segmentation labels for the 156 seen classes, the pseudo labels for the 15 unseen classes produced by the pre-trained IFSeg,\footnote{We use the pre-trained IFSeg checkpoint having 61.6 mIoU in \cref{tbl4:ablation}.} and an additional set of pseudo labels that are produced by IFSeg+ model itself during training.
To be specific, we train IFSeg+ using the pre-trained OFA-Base \cite{wang2022ofa} checkpoint, leveraging the ground truth segmentation labels (for the seen 156 classes) and the pseudo labels (for the unseen 15 classes) generated by
the pre-trained
IFSeg during initial 15k training iterations.
Subsequently, we replace the pseudo labels generated by IFSeg with those generated by the IFSeg+ itself. We then apply the self-training technique \cite{bucher2019zero} for the remaining 66k training iterations.

For evaluation, we follow the protocol of COCO Stuff seen $\to$ unseen zero-shot transfer scenario considered by prior works \cite{bucher2019zero,gu2020context,cheng2021sign,xian2019semantic,pastore2021closer,xu2022simple,zhou2022extract} where all 171 semantic categories of the COCO Stuff have to be predicted, then the mIoU metrics for the seen and the unseen categories are individually considered (\emph{i.e.}, mIoU(U) and mIoU(S)), as well as their harmonic mean (\emph{i.e.}, hIoU).
\cref{tbl:ablation-seen-to-unseen} summarizes the results; our method (\emph{i.e.}, IFSeg+) can achieve significant segmentation performances compared to all the baselines. For example, IFSeg+ scored 2.1, 3.7, and 3.2 higher points than MaskCLIP+ \cite{zhou2022extract} in terms of mIoU(U), mIoU(S), and hIoU, respectively.
We note that our post-processing technique is not applied to the weakly-supervised zero-shot models, as the effect of the technique diminishes after using the real images and annotations during training as discussed in \cref{sec2:ifseg}. 
For example, applying the post-processing ($K=3$ with 25 iterations)
even degrades the mIoU(U) scores of IFSeg+ and MaskCLIP+, dropping from 56.8 to 55.2, and from 54.7 to 54.5, respectively.


\begin{table*}[h]
\centering
\small
\scalebox{0.92}
{
\begin{tabular}{lllccccc}
    \toprule
    Method & Text Backbone & Image Backbone & Image Dataset & 
Segmentation Label & mIoU(U) & mIoU(S) & hIoU \\
    \midrule
    ZS5 \cite{bucher2019zero} & word2vec~\cite{mikolov2013distributed} & ResNet-101 & COCO (118k) & \cmark (156 seen)  & 10.6 & 34.9 & 16.2 \\
    CaGNet \cite{gu2020context} & word2vec~\cite{mikolov2013distributed}, fasttext~\cite{joulin2016fasttext} & ResNet-101 & COCO (118k) & \cmark (156 seen) & 13.4 & 35.3 & 32.6 \\
    SIGN \cite{cheng2021sign} & word2vec~\cite{mikolov2013distributed}, fasttext~\cite{joulin2016fasttext} & ResNet-101 & COCO (118k) & \cmark (156 seen) & 15.2 & 36.4 & 21.4 \\
    SPNet \cite{xian2019semantic} & word2vec~\cite{mikolov2013distributed}, fasttext~\cite{joulin2016fasttext} & ResNet-101 & COCO (118k) & \cmark (156 seen) & 26.9 & 34.6 & 30.3 \\
    STRICT \cite{pastore2021closer} & word2vec~\cite{mikolov2013distributed}, fasttext~\cite{joulin2016fasttext} & ResNet-101 & COCO (118k) & \cmark (156 seen) & 30.3 & 35.3 & 32.6 \\
    ZSSeg \cite{xu2022simple}  & ALIGN-BERT-Large~\cite{jia2021scaling} & ResNet-101 & COCO (118k) & \cmark (156 seen) & 43.6 & 39.6 & 41.5 \\
    MaskCLIP+ \cite{zhou2022extract} & CLIP-ResNet \cite{radford2021learning} & ResNet-101 & COCO (118k) & \cmark (156 seen) & 54.7 & 38.2 & 45.0 \\
    IFSeg+ (ours) & OFA-Base \cite{radford2021learning} & ResNet-101 & COCO (118k) & \cmark (156 seen) & \textbf{56.8} & \textbf{41.9} & \textbf{48.2} \\
    \bottomrule
\end{tabular}
}
\caption{\textbf{Comparison with (weakly) supervised baselines under the seen$\rightarrow$unseen transfer scenario.} We report the mIoU metric evaluated on the 15 unseen and the 156 seen semantic categories of the COCO Stuff benchmark and their harmonic mean, denoted by mIoU(U), mIoU(S), and hIoU, respectively.  All models are trained on segmentation labels of the 156 seen categories (supervised training) and pseudo-labels of the 15 unseen categories (self-training), where ``Image Dataset'' denotes the dataset required for training.}
\label{tbl:ablation-seen-to-unseen}
\end{table*}

\section{Ablation study on hyperparameters}
\label{supple:abl:hyperparameter}
In this section, we perform an ablation study to understand the effect of hyperparameters of our method, namely the iteration count and $K$-nearest neighbors used in the post-processing, the sampling range $S$ for the artificial image, and the use of cross-attention mechanism in our transformer decoder.

\vspace{0.02in}
\noindent
\textbf{Post processing.}
We first examine the effect of the iteration count and the number of nearest neighbor $K$ in our post-processing across an array of $\{0, 1, 10, 25, 50\}$ iteration count and $K\in\{2, 3, 5, 8\}$. As shown in \cref{tbl:ablation-postprocessing}, the effect of iteration counts becomes saturated after 25 iterations, and our method could be further improved with a larger $K$ (\emph{e.g.}, $K=8$). We note that the evaluations are performed under the zero-shot semantic segmentation on the 15 unseen semantic categories of the COCO Stuff. We also note that 0 iteration is equivalent to not performing the post-processing.


\begin{table*}[h]
\begin{subfigure}{0.5\linewidth}
\centering
    \begin{tabular}{lccccc}
	\toprule
	Iteration
	& 0 & 1 & 10 & 25 & 50\\\midrule
mIoU& 46.8 & 51.2 & 54.9 & 55.6 & 55.5 \\ 
	\bottomrule
    \end{tabular}
    \caption{Varying iteration counts with $K=3$.}
    \label{supp:detect}
\end{subfigure}
\begin{subfigure}{0.5\linewidth}
\centering
\begin{tabular}{lcccc}
    \toprule
        $K$     & 2 & 3 & 5 & 8  \\\midrule
    mIoU & 50.1 & 55.6 & 59.7 & 61.4 \\
        \bottomrule
\end{tabular}
    \caption{Varying $K$ with iteration counts of 25.}
\end{subfigure}
\caption{\textbf{Ablation studies on varying the iteration count and the number of nearest neighbor $K$.} All models are trained and evaluated on the 15 unseen semantic categories of the COCO Stuff benchmark.}\label{tbl:ablation-postprocessing}
\end{table*}

\begin{table}[h]
\centering
\small
\scalebox{0.95}{
\begin{tabular}{lclccc}
    \toprule
    Method & PP & Backbone & Zero-shot (mIoU) & Cross-dataset (mIoU) & Unsupervised (mIoU) \\
    \midrule
    CLIP \cite{radford2021learning,zhou2022extract} & \cmark & ResNet-101 & \textbf{12.3} & \textbf{3.7} & \textbf{4.6} \\
    CLIP \cite{radford2021learning,zhou2022extract} & \xmark & ResNet-101 & 11.6 & 3.6 & 4.4 \\
    \midrule
    MaskCLIP \cite{zhou2022extract} & \cmark & ResNet-101 & \textbf{24.8} & \textbf{10.3} & \textbf{12.7} \\
    MaskCLIP \cite{zhou2022extract} & \xmark & ResNet-101 & 23.7 & 8.8 & 10.8 \\
    \midrule
    IFSeg (ours) & \cmark & ResNet-101 & \textbf{54.6} & \textbf{16.8} & \textbf{16.9} \\
    IFSeg (ours) & \xmark & ResNet-101 & {47.0} & {14.0} & {14.3} \\
    \bottomrule
\end{tabular}
}
\caption{\textbf{Effects of the post-processing on varying image-free approaches.} We report the mIoU metric with and without the post-processing, evaluated on the zero-shot (the 15 unseen categories in COCO Stuff), the cross-dataset (COCO$\rightarrow$ADE20K), and the unsupervised (all the 171 categories in COCO Stuff) semantic segmentation scenarios. ``PP'' denotes our post-processing is applied.}
\label{tbl:ablation-basline-without-postprocessing}
\end{table}

Next, we present the mIoU of the image-free models (\emph{i.e.}, CLIP\cite{radford2021learning, zhou2022extract}, MaskCLIP\cite{zhou2022extract}, and IFSeg) without our post-processing evaluated on the zero-shot (the 15 unseen categories in COCO Stuff), the cross-dataset (COCO$\rightarrow$ADE20K), and the unsupervised semantic segmentation (all the 171 categories in the COCO Stuff) scenarios in \cref{tbl:ablation-basline-without-postprocessing}. 
Overall, our post-processing positively affects the mIoU of all baselines (\emph{e.g.}, 23.7 mIoU $\to$ 24.8 mIoU for MaskCLIP on the zero-shot semantic segmentation scenario). Regardless of whether or not the post-processing is applied, however, IFSeg is always the best-performing image-free model in all the scenarios.

\vspace{0.02in}
\noindent
\textbf{Artificial image.}
Here, we investigate the effect of varying sampling range $k$ for our artificial image generation.
\cref{tbl:ablation-artificial-image-k} summarizes results; interestingly, optimal values of $k=16$ and $K=8$ (of the post-processing) give our significant further improvements from 55.6 to 66.0 (+ 10.4) mIoU score on the 15 unseen semantic categories of the COCO Stuff benchmark. 
We remark that the values of $h$ and $w$ in \cref{eq:artificial_image_hw} are randomly sampled from $\{1, 2, ..., k\}$.
Regarding this, the last row in \cref{tbl:ablation-artificial-image-k} shows that removing randomness from sampling $h$ and $w$ harms overall improvements.

\begin{table}[h!]
\centering
\begin{tabular}{c l c c}
    \toprule
$S$&    $(h, w) \sim \{1, 2, ..., S\}$ & Post-processing with $K=3$ & Post-processing with $K=8$\\
    \midrule
8&    $(h, w) \sim \{1, 2, ..., 8\}$ & 55.8 & 64.3 \\
16&    $(h, w) \sim \{1, 2, ..., 16\}$ & \textbf{57.8} & \textbf{66.0}\\
32&    $(h, w) \sim \{1, 2, ..., 32\}$ & 55.6 & 61.4 \\\cmidrule{1-4}
32&    $(h, w) = (32, 32)$ & 47.7 & 56.1 \\
    \bottomrule
\end{tabular}
\caption{\textbf{Ablation studies on varying sampling range $S$ for our artificial image generation.} We also report two different nearest neighbor hyperparameters $K\in\{3,8\}$ of the post-processing. The last row reports the deterministic setup of $(h,w)=(32,32)$ for generating our artificial images. The reported values are mIoU scores on the 15 unseen semantic categories of the COCO Stuff benchmark.}
\label{tbl:ablation-artificial-image-k}
\end{table}
On the other hand, one may consider the recent VL prompt learning method \cite{zhou2022learning, zhou2022conditional} as an option for efficiently adapting a VL model to the semantic segmentation task. However, we would like to emphasize that our primary interest lies in image-free scenarios. Simply plugging the prompt learning into the image-free setting is non-trivial, as prompting cannot replace the training images and labels required to learn the segmentation task. 
Nonetheless, formulating image-free semantic segmentation within the context of the prompt learning framework could be an interesting direction for future research.


\vspace{0.02in}
\noindent \textbf{The cross-attention mechanism.}
We validate the effect of the cross-attention mechanism in our transformer decoder.
To this end, we train our model on the zero-shot (the 15 unseen categories in COCO Stuff) semantic segmentation scenario without providing the contextualized embedding (\cref{eq:transformer_encoder}) for the cross-attention mechanism.
As a result, we observed a significant degradation in segmentation performance, dropping from 55.6 $\rightarrow$ 22.6 mIoU after removing the cross-attention. 
We note that the use of cross-attention is a default setting during the VL pre-training in our framework, and maintaining the cross-attention during fine-tuning would be beneficial for stability. 


\section{Image-free baselines with ViT backbone}
In this section, we present the mIoU of the image-free baselines, CLIP\cite{radford2021learning,zhou2022extract} and MaskCLIP\cite{zhou2022extract}, with the stronger ViT-B/16 image backbones evaluated on the zero-shot (the 15 unseen categories in COCO Stuff) semantic segmentation scenario in \cref{tbl:ablation-vit-backbone}. Overall, ViT-B/16 brings performance improvements thanks to its advanced visual representation compared to the ResNet-101 backbone. Nonetheless, the performance of our IFSeg is superior to these baselines even if it uses the ResNet-101 as the image backbone model, unchanged from the trends observed in \cref{tbl1:coco_unseen}.


\begin{table}[h]
\centering
\small
\scalebox{0.95}{
\begin{tabular}{lllc}
    \toprule
    Method & Text Backbone & Image Backbone & mIoU \\
    \midrule
    CLIP$\dag$ \cite{radford2021learning,zhou2022extract} & CLIP-ViT-B/16~\cite{radford2021learning} & ViT-B/16 & 12.9 \\
    MaskCLIP$\dag$ \cite{zhou2022extract} & CLIP-ViT-B/16~\cite{radford2021learning}  & ViT-B/16 & 37.0 \\
    IFSeg (ours)$\dag$ & OFA-Base~\cite{wang2022ofa} & ResNet-101  & \textbf{55.6} \\
    \bottomrule
\end{tabular}
}
\caption{\textbf{Comparison with image-free baselines under the zero-shot semantic segmentation (the 15 unseen categories in COCO Stuff) scenario.} We report the mIoU metric evaluated on the 15 unseen semantic categories of the COCO Stuff benchmark. $\dag$ indicates models with our post-processing applied.}
\label{tbl:ablation-vit-backbone}
\end{table}


\section{Compatibility analysis}
We here validate the compatibility of our method with another encoder-decoder VL model, CLIPCap~\cite{mokady2021clipcap}. 
Note that CLIPCap is a fine-tuned CLIP-ViT-B/32 model for an image-to-text captioning task on the Conceptual Captions benchmark~\cite{sharma2018conceptual}. Specifically, CLIPCap utilizes GPT2~\cite{radford2019language} as a text generator, and we also do it as the segmentation decoder in our framework. 

In order to create our artificial image under CLIP's dual-encoder design, we utilize CLIP text encoder's sentence-level feature as the word embedding for semantic categories, directly following the prompt engineering procedure by MaskCLIP \cite{zhou2022extract}. For example, an artificial image patch for a \emph{dog} category is an ensemble of prompts like ``\emph{a photo of the dog}'' and ``\emph{a painting of a dog}.''\footnote{We refer the readers to the codebase of MaskCLIP \cite{zhou2022extract} for the full list of prompt templates; \url{https://github.com/chongzhou96/MaskCLIP}.} 
Then, similar to ours incorporated with the OFA framework, we fine-tune the text generator of ClipCap to predict semantic segmentation of the artificial image and evaluate the performance on the 15 unseen semantic categories of the COCO Stuff benchmark.
We note that, in order to deal with the prefix-based design of ClipCap (\emph{i.e.}, a single token in the CLIP representation space is mapped to multiple tokens in the text generator space), we decode each token individually.


\cref{tbl:clipcap-results} summarizes the compatibility experiments; our method is well-incorporated with CLIPCap and even significantly outperforms CLIP and MaskCLIP~\cite{zhou2022extract} baselines, which also have the same CLIP backbone. 
For example, our method achieves the best mIoU score of 25.8 on the 15 unseen semantic categories of the COCO Stuff benchmark compared to the baselines having the same CLIP backbone.
These results demonstrate the broad applicability of our method with various pre-trained VL models and lead them to perform semantic segmentation in an image-free manner.


\begin{table}[h]
\centering
\small
\begin{tabular}{lllcc}
    \toprule
    Method & Pretrain & Image Backbone & Text Deocder & mIoU \\
    \midrule
    CLIP$\dag$ \cite{zhou2022extract, radford2021learning}  & CLIP \cite{radford2021learning} & CLIP-ViT-B/32 & \xmark & 4.8 \\
    MaskCLIP$\dag$ \cite{zhou2022extract}  & CLIP \cite{radford2021learning} & CLIP-ViT-B/32 & \xmark & 20.7 \\
    IFSeg (ours)  & CLIPCap \cite{mokady2021clipcap} & CLIP-ViT-B/32 & GPT2~\cite{radford2019language} & \textbf{25.8} \\
    \bottomrule
\end{tabular}
\caption{\textbf{Ablation study on compatibility with other encoder-decoder VL models.} We denote that our image-free approach is applied to CLIPCap, which is an image captioning model built upon pre-trained CLIP. 
All models are evaluated on the 15 unseen semantic categories of the COCO Stuff benchmark. $\dag$ denotes that our post-processing is applied.}
\label{tbl:clipcap-results}
\end{table}


\section{Implementation details}
\vspace{0.02in}
\noindent \textbf{Image Pre-processing.} We preprocess images using the official codebase\footnote{\url{https://github.com/OFA-Sys/OFA}.} of OFA \cite{wang2022ofa} framework and $\tt {mmsegmentation}$\footnote{\url{https://github.com/open-mmlab/mmsegmentation}.}. Specifically, we normalize the image with the mean and standard deviation values of 0.5. We also resize the short sides of images keeping the aspect ratio. 
For all experiments, we resize the short sides to 512, in order to ensure a fair comparison with the strongest baselines MaskCLIP+~\cite{zhou2022extract} and DenseCLIP~\cite{rao2022denseclip}.

\vspace{0.02in}
\noindent \textbf{Text Pre-processing.} We generate prompt text following the ``\textit{task description} $+$ \textit{category enumeration}'' protocol of the VQA task~\cite{wang2022ofa}. Precisely, we use ``\textit{what is the segmentation of the image?}'' as the task description, and ``\textit{object: category1 category2 ... categoryN}'' as the category enumeration. For the tokenization and embedding, we directly incorporate the pre-trained BPE tokenizer and embedding matrix provided by the codebase of OFA \cite{wang2022ofa} framework.

\vspace{0.02in}
\noindent \textbf{Evaluation Details.} For a fair comparison, we perform the \emph{whole inference} evaluation protocol (\emph{i.e.}, predicting the rectangular-shaped output at once) for image-free based approaches (\emph{e.g.}, \cref{tbl1:coco_unseen,tbl2:cross2ade,tbl3:unsup_coco,tbl4:ablation}) following their strongest baseline, MaskCLIP+~\cite{zhou2022extract}, and the \emph{sliding inference} evaluation protocol (\emph{i.e.}, concatenating square-shaped crops of the original rectangular-shaped image) for supervised approaches (\emph{e.g.}, \cref{tbl5:supervised_ade}) following their strongest baseline, DenseCLIP \cite{rao2022denseclip}. 

\vspace{0.02in}
\noindent \textbf{Visual Feature-based Post-processing.} 
In our post-processing,
we utilize features of the image backbone network (\emph{i.e.}, ResNet) from the OFA \cite{wang2022ofa} encoder.
For a fair comparison, we also apply the post-processing for our re-implemented baselines of OFA \cite{wang2022ofa}, CLIP \cite{radford2021learning}, and MaskCLIP \cite{zhou2022extract} with their image backbone networks. 
For example, in the case of CLIP \cite{radford2021learning} and MaskCLIP \cite{zhou2022extract}), we utilize the final patch-wise outputs of the ViT-B/16 image backbone as the post-processing features.

\vspace{0.02in}
\noindent \textbf{Viusalization Details.} Exclusively for the visualizations of the image-free models (\cref{fig1:first-page,fig4:visualize_ifseg}), we introduce additional post-processing with DenseCRF \cite{krahenbuhl2011efficient} and its third-party implementation\footnote{\url{https://github.com/lucasb-eyer/pydensecrf}.}. Note that smoothing outputs with DenseCRF can provide qualitatively sharper segmentation results by clustering prediction outputs according to the edges of the raw RGB images. 
However, we remark that DenseCRF is \emph{never} used for the reported values of experimental results for a fair comparison.


\begin{table*}[h!]
\centering
\scalebox{0.9}{
\begin{tabular}{ll}
    \toprule
    Coarse category & Fine-grained category \\
    \midrule
    animal & giraffe, zebra, bear, elephant, cow, sheep, horse, dog, cat, bird \\
    sports & tennis racket, surfboard, skateboard, baseball glove, baseball bat, kite, sports ball, snowboard, skits, frisbee \\
    accessory & suitcase, tie, handbag, eye glasses, shoe, umbrella, backpack, hat \\
    outdoor & bench, parking meter, stop sign, street sign, fire hydrant, traffic light \\
    vehicle & boat, truck, train, bus, airplane, motorcycle, car, bicycle \\
    person & man, woman, child, boy, girl \\
    indoor & hair brush, toothbrush, hair drier, teddy bear, scissors, vase, clock, book \\
    appliance & blender, refrigerator, sink, toaster, oven, microwave \\
    electronic & cell phone, keyboard, remote, mouse, laptop, tv \\
    furniture (things) & door, toilet, desk, window, dining table, mirror, bed, potted plant, couch, chair \\
    food (things) & cake, donut, pizza, hot dog, carrot, broccoli, orange, sandwich, apple, banana \\
    kitchen & bowl, spoon, knife, fork, cup, wine glass, plate, bottle \\
    water & waterdrops, sea, river, fog, lake, ocean \\
    ground & playingfield, platform, railroad, pavement, road, gravel, mud, dirt, snow, sand \\
    solid & hill, mountain, stone, rock, wood \\
    sky & clouds \\
    plant & straw, moss, branch, flower, leaves, bush, tree, grass \\
    structural & railing, net, cage, fence \\
    building & roof, tent, bridge, skyscrapper, house \\
    food (stuff) & vegetable, salad, fruit \\
    textile & banner, pillow, blanket, curtain, cloth, clothes, napkin, towel, mat, rug \\
    furniture (stuff) & stairs, light, counter, mirror, cupboard, cabinet, shelf, table, desk, door \\
    window & blind window \\
    floor & stone floor, marble floor, wood floor, tile floor, carpet \\
    ceiling & tile ceiling \\
    wall & concrete wall, stone wall, brick wall, wood wall, panel wall, tile wall \\
    raw material & metal, plastic, paper, cardboard \\
    \bottomrule
\end{tabular}
}
\caption{\textbf{The full list of hierarchical semantic categories} of the COCO Stuff benchmark. Each coarse category is paired with given fine-grained categories, following the label hierarchy of Caesar \emph{et al.}~\cite{caesar2018coco}.}
\label{tbl:coco-coarse-mapping}
\end{table*}



\section{Additional qualitative results}
In this section, we present visualizations of segmentation results obtained by baselines and our method in different evaluation settings. Specifically, we first consider the weakly-supervised scenario (zero-shot transfer) in \cref{tbl:ablation-seen-to-unseen} and compare the result between our IFSeg+ and MaskCLIP+, the strongest baseline in the scenario. Next, we also consider the fully-supervised semantic segmentation scenario in \cref{tbl5:supervised_ade} and compare the result between the supervised IFSeg and DenseCLIP baseline.

\subsection{Weakly-supervised zero-shot transfer scenario}
The visualizations of segmentation results obtained by MaskCLIP+ and ours under the COCO Stuff seen$\to$unseen zero-shot transfer scenario are present in \cref{fig:visualization-coco-unseen}. Following the protocol in \cref{supple:selftraining} we evaluate and visualize the 15 unseen classes of the COCO Stuff benchmark. Overall, it shows that our method can predict the segmentation that is more consistent with the groud-truth (GT) segmentation than the MaskCLIP+ baseline.

\begin{figure*}[h!]
\vspace{-0.1in}
 \centering
 \scalebox{1.0}{
  \includegraphics[width=0.93\textwidth]{resource/supple_visualize_coco_unseen.pdf}
  }
\caption{\textbf{Visualization of segmentation results under the weakly-supervised zero-shot transfer scenario.} We visualize the segmentation results of IFSeg+ (ours) and MaskCLIP+ (baseline). Qualitatively observed, IFSeg+ can predict the segmentation that is more consistent with the groud-truth (GT) segmentation than the MaskCLIP+ baseline. Best viewed in color.}
\label{fig:visualization-coco-unseen}
\vspace{-0.1in}
\end{figure*}

\subsection{Fully-supervised semantic segmentation scenario}
The visualizations of segmentation results obtained by DenseCLIP and ours under the ADE20k semantic segmentation benchmark are present in \cref{fig:visualization-ade}. We evaluate and visualize the total 150 classes of the ADE20k dataset. As depicted by the quantitative mIoU score in \cref{tbl5:supervised_ade} and some visualization cases in \cref{fig:visualization-ade}, ours shows results that are more consistent with the groud-truth (GT) segmentation than the DenseCLIP baseline. However, we note that DenseCLIP and ours both tend to produce satisfactory prediction results for most samples since they are trained in a fully-supervised way using the ground-truth segmentation annotations.

\begin{figure*}[h!]
\vspace{-0.1in}
 \centering
 \scalebox{0.97}{
  \includegraphics[width=0.93\textwidth]{resource/supple_visualize_ade.pdf}
  }
\caption{{\textbf{Visualization of segmentation results under the fully-supervised semantic segmentation scenario.} We visualize the segmentation results of Supervised IFSeg (ours) and DenseCLIP (baseline). Although both the models are trained in a fully-supervised manner, our IFSeg tends to produce more accurate predictions than DenseCLIP. We utilize the class colors defined by the $\tt {mmsegmentation}$ in \url{https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/datasets/ade.py}. For clarity, we denote the 14 classes with the largest segmentation regions in this example.}
Best viewed in color.}
\label{fig:visualization-ade}
\end{figure*}

