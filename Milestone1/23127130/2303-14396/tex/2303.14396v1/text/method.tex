\section{Method}
In this section, we present a method for performing semantic segmentation tasks using vision-language (VL) encoder-decoder models and our image-free approach in a self-supervised manner. 
Inspired by the success of zero-shot transfer (\emph{e.g.}, zero-shot image classification~\cite{radford2021learning}) in the recent VL models, we aim to perform semantic segmentation only given a set of target semantic categories but without any task-specific images and annotations during training.
However, several prior works~\cite{zhou2022extract, ghiasi2022scaling} observed that it is challenging to directly segment semantic categories via VL models, \emph{e.g.}, CLIP~\cite{radford2021learning}, without any modifications and additional training.
Nonetheless, we address this challenging task using the pre-trained VL models with an encoder-decoder architecture.
In \cref{sec2:preliminaries}, we introduce the VL encoder-decoder architecture and describe how it operates in our method.
In \cref{sec2:segmentation}, we describe how the semantic segmentation task can be handled in the encoder-decoder VL model. In \cref{sec2:ifseg}, we present our image-free semantic segmentation method.

\subsection{VL Encoder-Decoder Architecture} \label{sec2:preliminaries}
Here, we introduce the VL model architecture in our framework and describe its operation step-by-step.

\vspace{0.02in}
\noindent \textbf{Data format. } Our method operates based on sequence data. For instance, let $\mathbf{x}$ be a sequence data of length $L_{\mathtt{x}}$ and let $\mathbf{e}_\mathtt{x}$ be its embedding in a $D$-dimensional vector space:
\begin{gather}
\mathbf{x} = \{{x}^{(0)},...,{x}^{(L_{\mathtt{x}}-1)}\}, \label{eq:x-sequence} \\
\mathbf{e}_\mathtt{x} = [{\mathbf{e}_\mathtt{x}}^{(0)};...;{\mathbf{e}_\mathtt{x}}^{(L_{\mathtt{x}}-1)}] \in \mathbb{R}^{L_{\mathtt{x}} \times D}.\label{eq:x-embedding}
\end{gather}
Specifically, we deal with the raw image-text $(\mathcal{X}_\mathtt{I}, \mathcal{X}_\mathtt{T})$ by tokenizing them into a sequence of tokens. The text $\mathcal{X}_\mathtt{T}$ is tokenized by a dictionary $\mathcal{V} = \{v_0, ..., v_{N-1} \}$ of $N$ pre-defined words\footnote{We utilize the bytes pair encoding (BPE) \cite{sennrich2016neural} words.} and the corresponding word embedding matrix $\mathbf{E} = [\mathbf{e}_0;...; \mathbf{e}_{N-1}] \in \mathbb{R}^{N \times D}$ that are related by the lookup operation $\mathbf{e}_{i} := \mathtt{Emb}({v}_{i})$. For example, we consider the following source text tokens and their embedding,
\begin{gather}
\mathbf{x}_\mathtt{T} = \{{x}_{\mathtt{T}}^{(0)},...,{x}_{\mathtt{T}}^{(L_{\mathtt{T}}-1)}\},\label{eq:text-tokens} \\
\mathbf{e}_\mathtt{T} = [\mathbf{e}_{\mathtt{T}}^{(0)};...;\mathbf{e}_{\mathtt{T}}^{(L_{\mathtt{T}}-1)}] \in \mathbb{R}^{L_{\mathtt{T}} \times D},\label{eq:text-embedding}
\end{gather}
where ${x}_{\mathtt{T}}^{(i)} \in \mathcal{V}$ and $\mathbf{e}_{\mathtt{T}}^{(i)} := \mathtt{Emb}({x}_{\mathtt{T}}^{(i)})$. To deal with the image $\mathcal{X}_{\mathtt{I}}$, an image backbone\footnote{Typical vision models (\emph{e.g.}, convolutional neural nets) are used.} is introduced to produce a 2D feature map of shape {$H \times W \times C$}, followed by a spatial flatten operation ($H \times W \to L_{\mathtt{I}}$), resulting in the sequence
\begin{align}
    f_\mathtt{img} (\mathcal{X}_{\mathtt{I}})=\widetilde{\mathbf{e}}_{\mathtt{I}}=[\widetilde{\mathbf{e}}_{\mathtt{I}}^{(0)};...;\widetilde{\mathbf{e}}_{\mathtt{I}}^{(L_\mathtt{I}-1)}] \in \mathbb{R}^{L_\mathtt{I} \times C}.\label{eq:image-embedding}
\end{align}
Additionally, a learnable linear layer is applied to fix the output channel size, $\mathbf{e}_{\mathtt{I}} = \mathtt{Linear}(\widetilde{\mathbf{e}}_{\mathtt{I}}) \in \mathbb{R}^{L_\mathtt{I} \times D}$, which we interpret as the embedding of the conceptual image tokens:
\begin{align}
\mathbf{x}_{\mathtt{I}}=\{x_{\mathtt{I}}^{(0)};...;x_{\mathtt{I}}^{(L_\mathtt{I}-1)}\}. \label{eq:image-tokens}
\end{align}
Concatenating them together, we assign the token sequence $\mathbf{x}:=\{\mathbf{x}_{\mathtt{I}}, \mathbf{x}_{\mathtt{T}} \}$ in \cref{eq:x-sequence} and the embedding representation $\mathbf{e}_\mathtt{x}=[\mathbf{e}_{\mathtt{I}}; \mathbf{e}_{\mathtt{T}}] \in \mathbb{R}^{L_\mathtt{x} \times D}$ in \cref{eq:x-embedding}, where $L_\mathtt{x}:=L_\mathtt{I}+L_\mathtt{T}$.

\input{figure/task-description}
\vspace{0.02in}
\noindent \textbf{VL model architecture.}
VL models predict
a target $\mathbf{y}=\{y^{(0)},...,y^{(L_\mathtt{y}-1)}\}$ based on a learned distribution $P(\mathbf{y} | \mathbf{x})$ given the multi-modal source $\mathbf{x}$. To be specific, we employ an encoder-decoder model\cite{sutskever2014sequence}, where an encoder produces a contextualized encoding of $\mathbf{x}$, and a decoder predicts the target distribution based on the encoding.
Specifically, the transformer architecture
\cite{vaswani2017attention, dosovitskiy2021vit} is adopted for implementing the modules, $f_\mathtt{enc}$ and $f_\mathtt{dec}$. The transformer encoder $f_\mathtt{enc}$ produces the contextualized embedding of $\mathbf{x}$ by transforming the embedding $\mathbf{e}_\mathtt{x}$ with the self-attention mechanism \cite{vaswani2017attention},
\begin{align}
    f_\mathtt{enc} (\mathbf{e}_\mathtt{x})
    = [ f^{(0)}_\mathtt{enc} ( \mathbf{e}_\mathtt{x} );...; f_\mathtt{enc}^{(L_{\mathtt{x}}-1)} ( \mathbf{e}_\mathtt{x} ) ] \in \mathbb{R}^{L_{\mathbf{x}} \times D}.\label{eq:transformer_encoder}
\end{align}
Then, the transformer decoder $f_\mathtt{dec}$ sequentially produces the output, by transforming a decoder input $\mathbf{d}_{i}=[\mathbf{d}^{(0)};...;\mathbf{d}^{(i)}] \in \mathbb{R}^{(i+1) \times D}$ with the self-attention and the cross-attention \cite{vaswani2017attention} mechanism with respect to $f_\mathtt{enc} (\mathbf{e}_\mathtt{x})$,
\begin{align}
\mathbf{h}^{(i)} 
= f_\mathtt{dec} (\mathbf{d}_{i}; f_\mathtt{enc} (\mathbf{e}_\mathtt{x})) \in \mathbb{R}^{D}.
\label{eq:transformer_decoder}
\end{align}
The formulation of the decoder input $\mathbf{d}_{i}$ would vary depending on the tasks. For example, the formulation during the pre-training is often the earlier targets, $\mathbf{d}^{(i)} := \mathtt{Emb} (y^{(i-1)})$ for $i>0$, and a special begin-of-sequence embedding $\mathbf{d}^{(0)} := \mathbf{e}_{\mathtt{BOS}}$. However, we will revisit and alter this formulation in \cref{sec2:segmentation} for the semantic segmentation task.

Finally, a linear transform
by the embedding matrix $\mathbf{E}$ produces a logit over the dictionary $\mathcal{V}$,
\begin{align}
{P}({y}^{(i)}|\mathbf{x}) \propto
\mathbf{E} \cdot \mathbf{h}^{(i)} \in \mathbb{R}^{N}. \label{eq:output-probability}
\end{align}
During the VL pre-training (\emph{e.g.}, image captioning),
all modules are trained end-to-end by maximizing the likelihood in \cref{eq:output-probability}.
We assume that \textit{the VL pre-training would align the image tokens with the word tokens in the contextualized embedding space} in \cref{eq:transformer_encoder}, which is the key idea in our framework introduced in \cref{sec2:ifseg}.

\subsection{Semantic Segmentation via Encoder-Decoder}\label{sec2:segmentation}
In this section, we formulate the semantic segmentation task in the VL encoder-decoder model and discuss the technical considerations. An overall pipeline is depicted in \cref{fig2:segmentation_overview}.

\vspace{0.02in}
\noindent \textbf{Task formulation.}\label{sec2:pipeline}
Given $M$ semantic categories of interest, we formulate a semantic segmentation task as decoding a category word for each dense region of the image. However, this design could be cumbersome in practice, since a certain semantic category word may be tokenized to multiple sub-words in the dictionary $\mathcal{V}$ (\emph{e.g.}, ``giraffe'' is tokenized to 2 sub-words: ``\_gir'' and ``affe'' in \cref{fig3:ifseg_overview}). As a remedy, we treat such a category as a temporary additional word and append the average embedding of the sub-word tokens to the embedding matrix $\mathbf{E}$. In this way, each semantic category is always treated as one distinct word, $\mathcal{V}_{\mathtt{seg}}=\{v'_{0},...,v'_{M-1}\}$.

To perform the task, we aim to produce spatially conditioned\footnote{We also replace the decoder's position embedding with the encoder's image position embedding for better visual understanding.} decoder outputs on the image tokens $x^{(i)}_{\mathtt{I}}$ (\emph{i.e.,} \cref{eq:image-tokens}). Specifically,
we enforce 
an alternative formulation of decoder input $\mathbf{d}_i$ in \cref{eq:transformer_decoder} such that the encoder output of the preceding index is used, \emph{i.e.}, $\mathbf{d}^{(i)} =  f^{(i-1)}_\mathtt{enc} ( \mathbf{e}_\mathtt{x} )$ for $i > 0$, where $\mathbf{d}^{(0)}=\mathbf{e}_{\mathtt{BOS}}$ without modification.
Then, we get $L_{\mathtt{I}}$ number of decoder outputs as
\begin{align}
\mathbf{h} =[\mathbf{h}^{(0)};...;\mathbf{h}^{(L_{\mathtt{I}}-1)}] \in \mathbb{R}^{L_{\mathtt{I}} \times D}. \label{eq:segmentation-latent-initial}
\end{align}
Next, we calculate the logit with \cref{eq:output-probability} and apply softmax after masking out the words that are not in $\mathcal{V}_{\mathtt{seg}}$ to get the normalized probability over the $M$ categories,
\begin{align}
\mathbf{p} =[\mathbf{p}^{(0)};...;\mathbf{p}^{(L_{\mathtt{I}}-1)}] \in \mathbb{R}^{L_{\mathtt{I}} \times M}. \label{eq:segmentation-probability-initial}
\end{align}
Then, we recover the spatial dimension of the image backbone $f_{\mathtt{img}}$ (\emph{i.e.}, $L_{\mathtt{I}} \to H \times W$) and up-sample it with bilinear interpolation to match a desired size $\widetilde{P} \times \widetilde{W}$ (\emph{e.g.}, an irregular shape of the image $\mathcal{X}_\mathtt{I}$). As a result, we obtain the output
\begin{align}
    \widetilde{\mathbf{p}} = [\widetilde{\mathbf{p}}^{(0)};...;\widetilde{\mathbf{p}}^{(\widetilde{H}\cdot \widetilde{W}-1)}] \in \mathbb{R}^{\widetilde{H} \times \widetilde{W} \times M}\label{eq:segmentation-probability-upsample},
\end{align}
and the predictive distribution is defined as:
\begin{align}
P(y^{(i)}|\mathbf{x}) := \widetilde{\mathbf{p}}^{(i)} \in \mathbb{R}^{M}.
\end{align}
Finally, we predict the category with the highest probability,
\begin{align}
\hat{y}^{(i)} = \argmax_{y \in \mathcal{V_{\mathtt{seg}}}}{P({y}^{(i)}=y | \mathbf{x})}. \label{eq:segmentation-prediction}
\end{align}
For fine-tuning given a segmentation label $y^{(i)}_{\mathtt{gt}}$ (represented by the semantic category words in $\mathcal{V_{\mathtt{seg}}}$), we consider the negative log-likelihood as the objective to minimize:
\begin{align}
\mathcal{L}_{\mathtt{seg}}(\mathbf{x}, \mathbf{y}_\mathtt{gt}) = \sum_{i} -\ln {P}(y^{(i)}={y}_{\mathtt{gt}}^{(i)}|\mathbf{x}).\label{eq:finetune-objective}
\end{align}

\vspace{0.02in}
\noindent \textbf{Prompt design.}
The text tokens $\mathbf{x}_\mathtt{T}$ in \cref{eq:text-tokens} can be provided as the prompt for instructing the details of the semantic segmentation task, namely the task description and the list of target classes. Specifically, we follow the ``\textit{task description} $+$ \textit{category enumeration}'' protocol in the VQA task~\cite{wang2022ofa} where the target classes are enumerated after the task description, \emph{e.g.}, ``what is the segmentation map of the image? object: giraffe, grass,'' in \cref{fig3:ifseg_overview}.
In this design, we expect the VL model to capture the cross-modal relationships between image tokens $\mathbf{x}_\mathtt{I}$ and the semantic categories.

\subsection{Image-free Semantic Segmentation}\label{sec2:ifseg}
In this section, we introduce a VL-driven self-supervised task, coined \emph{IFSeg} (\textbf{I}mage-\textbf{F}ree \textbf{Seg}mentation), to tackle the image-free semantic segmentation via the encoder-decoder VL model. 
Our main idea is that during the VL pre-training (in \cref{sec2:preliminaries}), the real image tokens and their corresponding semantic category word tokens can be considered interchangeable because they are both likely to be located in close proximity within the shared contextualized embedding space.
To this end, we generate artificial image tokens using given word tokens and update the VL model to segment the corresponding word tokens in a self-supervised manner. In other words, we generate artificial training data for an image-free semantic segmentation task.
We provide a brief overview of the proposed image-free approach in \cref{fig3:ifseg_overview}.

\vspace{0.02in}
\noindent
\textbf{Constructing artificial image tokens.}
We construct artificial training data (\emph{i.e.,} image-segmentation token pairs) from a set of $M$ unique category words $\mathcal{V}_{\mathtt{seg}}:=\{v'_{0},...,v'_{M-1}\}$.
Specifically, we randomly sample with replacement $U \times V$ number of words to construct a grid map $\widetilde{\mathbf{v}}_{\mathtt{IFSeg}}$ as follows:
\begin{gather}
    \widetilde{\mathbf{v}}_{\mathtt{IFSeg}} = \{\widetilde{v}_\mathtt{IFSeg}^{(0)}, ..., \widetilde{v}_\mathtt{IFSeg}^{(U\cdot V-1)}\}.\label{eq:artificial_image_uv}
\end{gather}
The initial grid sizes $U$, $V$ are randomly drawn from a range $\{1,2, ..., S\}$ with a hyper-parameter $S$.
Then, we up-scale the grid to have the spatial resolution of the image backbone (\emph{i.e.}, $H\times W$) via the nearest neighbor interpolation,
\begin{gather}
    {\mathbf{v}}_{\mathtt{IFSeg}} = \{{v}_\mathtt{IFSeg}^{(0)}, ..., {v}_\mathtt{IFSeg}^{(H\cdot W-1)}\}.\label{eq:artificial_image_hw}
\end{gather}
In our experiments, we use $H=W=32$ 
by following the configuration of the VL pre-training, 
and we also set $S=32$ as the size of the initial map, so it
can vary in the largest range
(see \cref{supple:abl:hyperparameter} for analysis on the effect of the initial grid range $S$).
The goal of using various random maps to up-sample our data is to bridge the gap between real images and our synthetic data by introducing a shape regularization effect. This effect allows objects to be depicted as a cluster of various sizes rather than being randomly scattered.
Finally, we train the model with the artificial image tokens $\mathbf{v}_{\mathtt{IFSeg}}$ (replacing the real image tokens in \cref{eq:image-tokens}) and their corresponding ground truths using the \textit{maximum likelihood} in \cref{eq:finetune-objective}.
We note that the image backbone, $f_\mathtt{img}$ (in \cref{eq:image-embedding}) is frozen during our self-supervised training.

\vspace{0.02in}
\noindent
\textbf{Post-processing for image-free segmentation.}
One challenge of the image-free segmentation task is the discrepancy in input modality between training and evaluation, which arises due to the absence of real training images.
For example, it is challenging to learn image-specific priors such as object shapes and label coherence in regions with similar textures.
To resolve this issue, we found that averaging the output probability based on the image feature (\emph{i.e.,} outputs of image backbone $f_\mathtt{img}$) significantly enhances the segmentation quality.
Specifically, we search $K$-nearest neighbors of the image features in \cref{eq:image-embedding} using the cosine similarity,
$\widetilde{\mathbf{e}}_{\mathtt{I}}^{(i)}
\cdot
\widetilde{\mathbf{e}}_{\mathtt{I}}^{(j)}
/
\|\widetilde{\mathbf{e}}_{\mathtt{I}}^{(i)}\|
\cdot
\|\widetilde{\mathbf{e}}_{\mathtt{I}}^{(j)}\|$.
Then, given a set of neighborhood indices $\mathcal{N}^{(i)}$, we iterate averaging the probability in \cref{eq:output-probability} with the neighborhood as follows,
\begin{align}
\mathbf{p}^{(i)}:=\sum_{j\in \mathcal{N}^{(i)}} \mathbf{p}^{(j)} / \:|\mathcal{N}^{(i)}|.
\end{align}
We empirically found that the effect of the post-processing diminishes when the real training images and annotations are available.
In our experiments, we apply this only for image-free approaches and use $K=3$ and 25 iterations unless stated otherwise (see \cref{supple:abl:hyperparameter} for ablation studies on varying $K$ and the iteration count).