\input{table/tbl4_ablation_st}
\subsection{Ablation study}\label{sec:ablation}
In this section, we perform an ablation study to understand further how the proposed method works when training images or segmentation annotations are available. 

\vspace{0.02in}
\noindent \textbf{Self-training.}
Self-training technique~\cite{bucher2019zero} has been widely used in the VL literature. It generates pseudo-labels of unseen segmentation categories for reducing the gap between seen and unseen semantic categories in a semi-supervised manner; it assumes the pixels of unseen categories could be present in the training images, while those pixels are not annotated. On this line, we also evaluate our method on the COCO Stuff benchmark when training images or the seen annotations are available.
Specifically, we fine-tune IFSeg with an additional 8k training iterations using 118k images and the seen annotations. We then evaluate the model on the 15 unseen categories of the COCO Stuff benchmark.
\cref{tbl4:ablation} shows the individual effects of training images and seen annotations in our framework.
After self-training, our method has improved significantly from 55.6 to 61.6 mIoU, which also largely surpasses the strongest baseline MaskCLIP+ of 48.7 on the COCO Stuff in \cref{tbl1:coco_unseen}.
Furthermore, we observe that ours can achieve outperforming performance compared to self-training baselines as presented in \cref{supple:selftraining}.

\vspace{0.02in}
\noindent
\textbf{Supervised semantic segmentation.}
Here, we perform supervised learning on the ADE20K benchmark varying model size of OFA~\cite{wang2022ofa} to demonstrate their effectiveness. For a fair comparison, we follow the training configuration of DenseCLIP~\cite{rao2022denseclip}, which incorporates cross-modal representations of CLIP to Semantic FPN~\cite{kirillov2019panoptic}, including input resolutions, batch size, and iterations.
We also compare with traditional image segmentation decoders like Semantic FPN and UPerNet~\cite{xiao2018unified} on pre-trained ImageNet~\cite{he2016deep}. 

As shown in \cref{tbl5:supervised_ade}, the encoder-decoder VL models can be successfully fine-tuned to segment semantic categories by surpassing the existing supervised approaches with a large margin, \emph{e.g.}, + 2.0 mIoU compared to the strongest baselines, DenseCLIP, on the ADE20K benchmark.
\input{table/tbl5_supervised_ADE}