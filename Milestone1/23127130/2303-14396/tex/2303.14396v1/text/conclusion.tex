\section{Conclusion}
We newly introduce a novel image-free semantic segmentation task, which has the goal of performing semantic segmentation without any task-specific images and annotations, except target semantic categories.
To tackle this, we propose a simple yet effective image-free framework via vision-language (VL) models in a self-supervised manner. 
The key idea is that words of semantic categories can act as an artificial image tokens on the cross-modal representation space of pre-trained VL models.
Specifically, we generate artificial image-segmentation pairs using word tokens to replace the real image-segmentation pairs for image-free semantic segmentation via the VL models.
Through extensive experiments, we demonstrate our models are not only effective baseline for this novel task but also show strong performances over existing methods acquiring the stronger supervision.
We believe our work would provide insights into the under-explored yet important problems for semantic segmentation via the pre-trained VL models.
