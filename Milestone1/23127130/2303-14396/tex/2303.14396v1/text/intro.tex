\section{Introduction}
\input{figure/first-page-figure}
Understanding a new concept with less cost (\emph{e.g.}, collecting data, annotations, or training) is a challenging yet essential problem in machine learning~\cite{vinyals2016matching}.
The most common practice is fine-tuning a foundation model, pre-trained on a large amount of data~\cite{he2016deep,devlin2018bert, chen2020big, brown2020_gpt3}, for downstream tasks.
In particular, such large-scale models have shown successful adaptation to downstream tasks with only little supervision across vision~\cite{chen2020big} and language~\cite{brown2020_gpt3} domains.
Recently, pre-training approaches in the vision-language (VL) domain have also achieved remarkable results in transferring to novel tasks (\emph{e.g.}, few-shot or zero-shot transfer~\cite{snell2017prototypical}) with various elaborate designs, including modality interaction between the dual encoders~\cite{radford2021learning, jia2021scaling}, the multi-modal encoder~\cite{kim2021vilt, wang2022image}, and the encoder-decoder~\cite{alayrac2022flamingo, wang2022simvlm,cho2021unifying, wang2022ofa, tsimpoukelli2021multimodal, yang2022empirical}.

Semantic segmentation is one of the crucial tasks in computer vision that requires understanding dense representations for pixel-wise classifications.
Inspired by the success of the contrastive VL pre-training, CLIP~\cite{radford2021learning}, several recent attempts~\cite{zhou2022extract, li2022languagedriven, ghiasi2022scaling, xu2022simple, liu2022open} have explored CLIP-based segmentation approaches for better transferability (\emph{e.g.}, zero-shot~\cite{xian2019semantic,bucher2019zero} and open-vocabulary segmentation~\cite{zhao2017open}).
However, the existing zero-shot or open-vocabulary segmentation approaches still suffer from a burden of training on additional image data, segmentation annotations~\cite{li2022languagedriven, ghiasi2022scaling, xu2022simple, zhou2022extract}, or natural language supervision~\cite{liu2022open, xu2022groupvit}, to adapt pre-trained VL models to downstream segmentation tasks. In the wild, however, such training data is not
readily available; \emph{e.g.}, there would be no task-specific training images or labels for novel web images like \cref{fig1:first-page}.
This limitation inspires us to investigate {how to fully utilize the VL models for semantic segmentation in a lightweight manner, even without any image data or human-annotated supervision}.

Meanwhile, the recent encoder-decoder VL models~\cite{alayrac2022flamingo, wang2022simvlm,cho2021unifying, wang2022ofa, tsimpoukelli2021multimodal, yang2022empirical} also have gained popularity with their unique characteristics of image-to-text generation via the VL decoder network.
Motivated by this, we explore the potential usability of the VL decoder to segment pixels in the text generation manner as an alternative to traditional vision segmentation decoders,
\emph{e.g.}, Semantic FPN~\cite{kirillov2019panoptic} and UperNet~\cite{xiao2018unified}.
Interestingly, we found that \emph{a solely given set of semantic categories enables the encoder-decoder VL models to perform semantic segmentation without any training images or annotations};
\cref{fig1:first-page} shows the quality of semantic segmentation results on the image-free segmentation task with a wild uncurated image downloaded from the web.

\vspace{0.02in}
\noindent
{\bf Contribution.} 
In this paper, we introduce a novel \textbf{I}mage-\textbf{F}ree \textbf{Seg}mentation task that aims to segment target semantic categories when only a set of the target semantic categories is given without any task-specific images and annotations.
Our core idea to tackle this challenge is that a word set of semantic categories can serve as an artificial image for the VL models on their cross-modal embedding space.
To this end, we propose a simple yet effective VL-driven self-supervised task, coined \emph{IFSeg}, that generates artificial image-segmentation pairs using word tokens and updates the VL models to segment them.
Specifically, we construct this artificial training data by creating a 2D map of random semantic categories (\emph{i.e.}, artificial image tokens) and another map of their corresponding word tokens.
We provide overall illustrations and the proposed method for semantic segmentation via the VL models in \cref{fig2:segmentation_overview,fig3:ifseg_overview}, respectively.

To demonstrate the effectiveness of our method for image-free semantic segmentation, we incorporate our method with the publicly available encoder-decoder VL model~\cite{wang2022ofa}.\footnote{
Our framework can be 
incorporated with any encoder-decoder VL models
and is expected to be improved by using even larger or better VL models, \emph{cf.},
pretraining OFA was performed on 22M image-text pairs, while the popular CLIP~\cite{radford2021learning} was pre-trained on 400M image-text pairs.}
In particular, the proposed method, albeit with weaker supervision (\emph{i.e.}, only segmentation categories), can even outperform the baselines that use much stronger supervision, such as task-specific images 
and segmentation masks.
For example, our method outperforms
MaskCLIP+~\cite{zhou2022extract} without 118k training images 
on a zero-shot segmentation scenario in the COCO Stuff benchmark
by achieving
+6.9 higher mIoU.
In addition, we conduct conventional scenarios having images and annotations available for further analysis, including supervised and semi-supervised approaches.
As a result, we demonstrate our method still outperforms the recent VL-driven supervised segmentation baselines. For example, our method has achieved an improved +2.0 mIoU compared to DenseCLIP~\cite{rao2022denseclip} on the ADE20K benchmark.

Overall, our work newly introduces image-free semantic segmentation, a challenging yet potentially crucial task for the computer vision domain, and also highlights the broad applicability of the recent tending VL models.
We hope our work could inspire researchers to
rethink a new research direction for segmentation tasks in a dataset-free manner.

\input{figure/model-diagram}