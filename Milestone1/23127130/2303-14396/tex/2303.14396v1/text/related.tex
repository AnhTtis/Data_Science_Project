\section{Related Works}
\noindent {\bf Vision-language pre-training.}
The recent vision-language models pre-trained on large-scale image-text data have shown successful results in zero-shot and few-shot adaptation to novel tasks across domains, \emph{e.g.}, image classification~\cite{deng2009imagenet}, captioning~\cite{lin2014microsoft} and visual question answering~\cite{antol2015vqa}.
To improve the quality of cross-modal representations, there have been extensive exploration in design of modality interaction, including the dual encoders~\cite{radford2021learning, jia2021scaling}, the multi-modal encoder~\cite{kim2021vilt, wang2022image}, and the encoder-decoder~\cite{alayrac2022flamingo, wang2022simvlm,cho2021unifying, wang2022ofa, tsimpoukelli2021multimodal, yang2022empirical}.
For example, CLIP~\cite{radford2021learning} introduced contrastive pre-training on the dual encoder (\emph{i.e.}, image and text encoder) and has shown impressive zero-shot image classification performances via a simple prompt engineering technique without training.
On the other hand, the encoder-decoder VL approaches~\cite{alayrac2022flamingo, wang2022simvlm,cho2021unifying, wang2022ofa, tsimpoukelli2021multimodal, yang2022empirical} also have gained much attention in image-to-text generation tasks such as image captioning and visual question answering. 
In this paper, we explore the potential usability of the VL decoder for image segmentation from the perspective of image-to-text generation.

\vspace{0.02in}
\noindent
{\bf Transferable image segmentation.}
Image segmentation is a core computer vision task, but it is still challenging to segment novel visual categories.
To this end, several attempts have been introduced, including unsupervised~\cite{yin2022transfgu, hamilton2022unsupervised, ji2019invariant, cho2021picie, liu2022open, zhou2022extract} and zero-shot segmentation~\cite{gu2020context, bucher2019zero, xian2019semantic, cheng2021sign, zhou2022extract, li2022languagedriven, ghiasi2022scaling, xu2022simple, pastore2021closer}.
First, unsupervised segmentation approaches~\cite{yin2022transfgu, hamilton2022unsupervised, ji2019invariant, cho2021picie, zhou2022extract} have been focused on clustering dense representations of an image, and then matching corresponding segmentation categories via the Hungarian-matching algorithm~\cite{doersch2015unsupervised}. 
On the other hand, the recent VL-driven approaches~\cite{liu2022open, zhou2022extract} replace the matching process via the text encoder of CLIP using segmentation vocabulary for better efficiency and transferability.
Meanwhile, early approaches in zero-shot segmentation~\cite{gu2020context, bucher2019zero, xian2019semantic, cheng2021sign, pastore2021closer} have utilized segmentation vocabulary via learned word embeddings like word2vec~\cite{mikolov2013distributed} and fast-text~\cite{joulin2016fasttext}.
Similar to the VL-driven unsupervised segmentation, the VL-driven zero-shot approaches~\cite{zhou2022extract, li2022languagedriven, ghiasi2022scaling, xu2022simple} also have been established on CLIP instead of word embeddings.
The zero-shot segmentation approaches often require class-agnostic segmentation masks~\cite{ghiasi2022scaling, xu2022simple} or class-specific segmentation annotations~\cite{gu2020context, bucher2019zero, xian2019semantic, cheng2021sign, li2022languagedriven, zhou2022extract, pastore2021closer}.
In this respect, we explore an image-free semantic segmentation task for more realistic scenarios with only given segmentation vocabulary, which can be easily collected than images or other annotations.