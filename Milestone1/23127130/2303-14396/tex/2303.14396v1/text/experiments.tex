\input{figure/zeroshot-good}
\input{table/tbl1_image_free_coco_unseen}
\section{Experiments}
\label{sec:experiments}
In this section, we demonstrate the effectiveness of the proposed image-free approach, IFSeg. Specifically, we incorporate our method with the recent VL encoder-decoder model, OFA~\cite{wang2022ofa}, which is publicly available,\footnote{\url{https://github.com/OFA-Sys/OFA}.} and evaluate its segmentation abilities on COCO Stuff~\cite{caesar2018coco} and ADE20K~\cite{zhou2017scene} semantic segmentation benchmarks.
Specifically, we compare our method with existing VL-driven segmentation baselines that target various scenarios: (a) zero-shot segmentation scenario~\cite{gu2020context, bucher2019zero, xian2019semantic, cheng2021sign, zhou2022extract, ghiasi2022scaling, xu2022simple}, (b) cross-dataset segmentation scenario~\cite{li2022languagedriven, ghiasi2022scaling, xu2022simple} and (c) unsupervised image segmentation~\cite{yin2022transfgu, hamilton2022unsupervised, ji2019invariant, cho2021picie, zhou2022extract}.
We consider CLIP~\cite{radford2021learning}, MaskCLIP~\cite{zhou2022extract}, and OFA~\cite{wang2022ofa} as baselines to evaluate the segmentation abilities of the pre-trained VL models without fine-tuning.
More details are described in each section and Appendix.

\vspace{0.02in}
\noindent
{\bf Datasets.}
COCO Stuff~\cite{caesar2018coco} is a large-scale dataset that contains 117k training, 5k validation images, and segmentation annotations of 171 semantic categories consisting of 80 objects and 91 stuff categories.
For the zero-shot image segmentation, we split COCO Stuff dataset into 156 seen categories and 15 unseen categories.\footnote{We report the specific vocabulary of unseen semantic categories in the COCO Stuff: \emph{frisbee, skateboard, cardboard, carrot, scissors, suitcase, giraffe, cow, road, wall concrete, tree, grass, river, clouds, playing field}.}
ADE20K~\cite{zhou2017scene} is a challenging semantic segmentation dataset including 20k training, 5k validation, and segmentation annotations of 150 fine-grained semantic categories that cover indoor and outdoor scenes.
In our image-free experiments in \cref{sec:image-free}, we use only semantic categories given by the segmentation benchmarks, without any training images and annotations.

\vspace{0.02in}
\noindent
{\bf Baselines.}
We consider a variety of existing VL-driven unsupervised, zero-shot, and the image-free segmentation baselines:
(a) unsupervised baselines:
IIC~\cite{ji2019invariant}, PiCIE+H.~\cite{cho2021picie}, TransFGU~\cite{yin2022transfgu},
(b) zero-shot baselines:
LSeg+\footnote{A re-implemented LSeg~\cite{li2022languagedriven} in the OpenSeg~\cite{ghiasi2022scaling}.}~\cite{li2022languagedriven}, ZSSeg~\cite{xu2022simple}, OpenSeg~\cite{ghiasi2022scaling}, and MaskCLIP+~\cite{zhou2022extract}, where ZSSeg, OpenSeg, and MaskCLIP+ are the recent VL-driven baselines that relied on CLIP~\cite{radford2021learning} or ALIGN~\cite{jia2021scaling},
and (c) image-free baselines: OFA~\cite{wang2022ofa}, CLIP~\cite{radford2021learning}, and MaskCLIP~\cite{zhou2022extract} which directly evaluate the segmentation abilities of the pre-trained VL models, OFA and CLIP.

\input{table/tbl2_transfer_supervised}
\vspace{0.02in}
\noindent
{\bf Implementation details.}
In our experiments, we implement our method on the OFA (encoder-decoder VL model) framework and generally follow the training and evaluation configuration of OFA~\cite{wang2022ofa}, $\tt {mmsegmentation}$\footnote{\url{https://github.com/open-mmlab/mmsegmentation}.} \cite{mmseg2020}, and MaskCLIP~\cite{zhou2022extract} (the strongest baseline) for a fair comparison.
We fine-tune our model from the OFA-Base pre-trained weights with the ResNet-101 backbone network. We optimize with AdamW optimizer~\cite{loshchilov2018adamw} with a weight decay of 0.1, a learning rate of 0.00005, and a batch size of 16 with 2k iterations unless stated otherwise. We generate $32\times 32$ grid-size of artificial image tokens with $S=32$ and use $K=3$ with 25 iterations for the post-processing for image-free baselines. 
We report a single-scale mean Intersection over Union (mIoU) score evaluated at the original irregular image sizes as the metric.
More details of experimental setups are described in Appendix.

 \input{table/tbl3_image_free_unsup}

\subsection{Image-free Adaptation for Segmentation}\label{sec:image-free}
\noindent {\bf Zero-shot image segmentation.}
We first evaluate the effectiveness of the proposed image-free approach, IFSeg, for adapting VL models toward semantic segmentation tasks.
We evaluate the mIoU scores of different models on segmenting the COCO Stuff 15 unseen semantic categories.
Specifically, we compare with the image-free baselines, CLIP\cite{radford2021learning}, OFA\cite{wang2022ofa}, and MaskCLIP\cite{zhou2022extract} in \cref{tbl1:coco_unseen}. 
In addition, we also compare with MaskCLIP+\cite{zhou2022extract} under the same evaluation setup as a baseline, 
which is trained on 118k COCO images using the pseudo-labels generated by MaskCLIP\cite{zhou2022extract}.
First of all, \cref{tbl1:coco_unseen} shows that our method can achieve significant improvement in mIoU metric compared to all the image-free baselines, \emph{e.g.}, +30.8 points higher than MaskCLIP. 
Somewhat surprisingly, our method outperforms MaskCLIP+\cite{zhou2022extract}, which is a stronger baseline trained on additional 118k images, despite our scarce training data regime that does not use any images and annotations except segmentation vocabulary.

\vspace{0.02in}
\noindent
{\bf Cross-dataset transfer.}
Again, we compare with VL-driven segmentation baselines in \cref{tbl2:cross2ade} under a cross-dataset scenario,
where the model is trained on the COCO Stuff and evaluated on the ADE20K benchmark.
To this end, we train our model using segmentation vocabulary of the COCO Stuff, and then evaluated on the ADE20K vocabulary. 

Similar to 
the above zero-shot scenario,
\cref{tbl2:cross2ade} shows 
that our method can achieve significant and comparable performance with the image-free baselines and the baselines with stronger supervision despite our image-free training regime. 
For example, ours achieved 1.5 points higher mIoU than OpenSeg\cite{ghiasi2022scaling} trained on the 118k training images and class-agnostic segmentation mask annotations.
Although the reported value of ours is lower than ZSSeg\cite{xu2022simple}, we note that there exists a huge gap between training scale; ZSSeg is trained on the COCO Stuff dataset with its natural language annotations (\emph{i.e.} captions), in a total $960\times$ larger training configuration ($15\times$ larger iterations with $64\times$ larger batch size).
Nevertheless, our method still consistently and significantly outperforms all the image-free baselines by a large margin; for example, ours achieves 5.5 higher points than MaskCLIP in terms of the mIoU metric. 

\vspace{0.02in}
\noindent
{\bf Unsupervised image segmentation.}
On the other hand, we also compare our method with unsupervised segmentation baselines in \cref{tbl3:unsup_coco}, which is another promising approach for learning transferable segmentation models.
Specifically, unsupervised baselines are trained on the COCO Stuff dataset and evaluated 171 semantic categories.

As shown in \cref{tbl3:unsup_coco}, our method consistently outperforms all the existing image-free segmentation baselines. For example, our method significantly outperforms MaskCLIP by achieving 16.9 mIoU, while MaskCLIP achieves 12.7. 
Also, ours shows comparable results to MaskCLIP+, which requires additional training with 118k images for transferring the knowledge of MaskCLIP via pseudo-labeling.

\vspace{0.02in}
\noindent
{\bf Qualitative Results.}
We present visualizations of segmentation results obtained by MaskCLIP and Ours in \cref{fig4:visualize_ifseg}, and it shows that our method even segments more fine-grained categories than the ground-truth labels; for example, the accessory category in the middle and bottom images are captured via ours, but not contained in the labels.
