\begin{table*}[t]
\centering
\small
{
\begin{tabular}{lllccc}
    \toprule
    Method & Text Backbone & Image Backbone & Image Dataset & Segmentation Label & mIoU \\
    \midrule
    LSeg+ \cite{li2022languagedriven, ghiasi2022scaling} & ALIGN-BERT-Large~\cite{jia2021scaling} & ResNet-101 & COCO (118k) & \cmark & 13.0 \\
    OpenSeg \cite{ghiasi2022scaling}& ALIGN-BERT-Large~\cite{jia2021scaling} & ResNet-101 & COCO (118k) & \cmark & 15.3 \\
    ZSSeg \cite{xu2022simple} & CLIP-ViT-B~\cite{radford2021learning} & ResNet-101 & COCO (118k) & \cmark & 20.5 \\
    \cmidrule{1-6}
    CLIP$\dag$ \cite{radford2021learning,zhou2022extract} & CLIP-ResNet~\cite{radford2021learning} & ResNet-101 & \xmark & \xmark & 3.7 \\
    MaskCLIP$\dag$ \cite{zhou2022extract} & CLIP-ResNet~\cite{radford2021learning} & ResNet-101 & \xmark & \xmark & 10.3 \\
    OFA$\dag$ \cite{wang2022ofa} & OFA-Base~\cite{wang2022ofa} & ResNet-101 & \xmark & \xmark & {0.5} \\
    IFSeg (ours)$\dag$ & OFA-Base~\cite{wang2022ofa} & ResNet-101 & \xmark & \xmark & \textbf{16.8} \\
    \bottomrule
\end{tabular}
}
\caption{\textbf{Comparison with VL-driven baselines under the cross-dataset  (COCO$\rightarrow$ADE20K) scenario.}
We report the mIoU metric evaluated on the ADE20K benchmark. 
We use the 150 fine-grained semantic categories of the ADE20K for image-free training.
``Image Dataset'' and ``Segmentation Label'' denote requirements for their training. $\dag$ denotes results that our post-processing is applied.}\label{tbl2:cross2ade}
\end{table*}