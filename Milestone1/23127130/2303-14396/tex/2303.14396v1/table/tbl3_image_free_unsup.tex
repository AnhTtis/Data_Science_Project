\begin{table}[t]
\centering
\small
{
\begin{tabular}{llcc}
    \toprule
    Method & Backbone & Image Dataset & mIoU \\
    \midrule
    IIC \cite{ji2019invariant}& ResNet-18 & COCO (118k) & 0.6 \\
    PiCIE + H. \cite{cho2021picie}& ResNet-18 & COCO (118k) & 4.6  \\
    TransFGU \cite{yin2022transfgu}& ViT-S/8 & COCO (118k) & 11.9  \\
    MaskCLIP+ \cite{zhou2022extract} & ResNet-101 & COCO (118k) & {18.0} \\
    \cmidrule{1-4}
    CLIP$\dag$ \cite{radford2021learning,zhou2022extract}& ResNet-101 & \xmark & 4.6 \\
    MaskCLIP$\dag$ \cite{zhou2022extract}& ResNet-101 & \xmark & 12.7 \\
    OFA$\dag$  \cite{wang2022ofa} & ResNet-101 & \xmark & {1.5} \\ 
    IFSeg (ours)$\dag$ & ResNet-101 & \xmark & \textbf{16.9} \\ 
    \bottomrule
\end{tabular}
}
\caption{\textbf{Comparison with unsupervised semantic segmentation (COCO$\rightarrow$COCO) baselines.}
We report the mIoU metric evaluated on the 171 semantic categories of the COCO Stuff benchmark. $\dag$ denotes results that our post-processing is applied.}\label{tbl3:unsup_coco}
\end{table}