% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
% \usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% SH
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{bm}
\usepackage{multirow}
\usepackage{float}
\usepackage{array}
\usepackage{duckuments}
\newcolumntype{P}[1]{>{\centering\arraybackslash}m{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{11803} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\usepackage{lipsum}  

\newcommand{\phseo}[1]{\textcolor{blue}{[Paul: #1]}}

\begin{document}

\title{IFSeg: Image-free Semantic Segmentation via Vision-Language Model}

\author{Sukmin Yun$^{13}\thanks{Equal contribution}\;\thanks{Work was done while at KAIST}$\quad\quad Seong Hyeon Park$^1$\footnotemark[1]\quad\quad Paul Hongsuck Seo$^2$\quad\quad Jinwoo Shin$^1$\\
$^1$Korea Advanced Institute of Science and Technology (KAIST) \quad $^2$Google Research\\
$^3$Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)\\
{\tt\small sukmin.yun@mbzuai.ac.ae, seonghyp@kaist.ac.kr, phseo@google.com, jinwoos@kaist.ac.kr}\\
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Vision-language (VL) pre-training has recently gained much attention for its transferability and flexibility in novel concepts (e.g., cross-modality transfer) across various visual tasks.
However, VL-driven segmentation has been under-explored, and the existing approaches still have the burden of acquiring additional training images or even segmentation annotations to adapt a VL model to downstream segmentation tasks.
In this paper, we introduce a novel image-free segmentation task where the goal is to perform semantic segmentation given only a set of the target semantic categories, but without any task-specific images and annotations.
To tackle this challenging task, our proposed method, coined IFSeg, generates VL-driven artificial image-segmentation pairs and updates a pre-trained VL model to a segmentation task. 
We construct this artificial training data by creating a 2D map of random semantic categories and another map of their corresponding word tokens.
Given that a pre-trained VL model projects visual and text tokens into a common space where tokens that share the semantics are located closely, this artificially generated word map can replace the real image inputs
for such a VL model.
Through an extensive set of experiments, our model not only establishes an effective baseline for this novel task but also demonstrates strong performances compared to existing methods that rely on stronger supervision, such as task-specific images and segmentation masks. Code is available at \url{https://github.com/alinlab/ifseg}.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{text/intro}
\input{text/method}
\input{text/related}
\input{text/experiments}
\input{text/ablation}
\input{text/conclusion}

\vspace{0.02in}
\noindent \textbf{Acknowledgements.} This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST); No.2021-0-02068, Artificial Intelligence Innovation Hub; No.2022-0-00959, Few-shot Learning of Casual Inference in Vision and Language for Decision Making).

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\input{text/supplementary}

\end{document}
