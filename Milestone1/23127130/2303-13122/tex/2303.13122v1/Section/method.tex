\section{Method}
Fig.~\ref{fig_overview} illustrates the proposed prompt learning framework, which consists of three primary steps: (\Rmnum{1}) MIL classifier training, (\Rmnum{2}) representative patch selection, and (\Rmnum{3}) prompt fine-tuning.
First, an ImageNet pre-trained ResNet~\cite{he2016deep} is used to extract patch features, which are then used to train the MIL classifier for the attention score of each patch. 
Second, representative patches in each WSI bag are chosen based on their attention scores to form a new bag. 
Third, the representative patches are used to fine-tune the prompt blocks plugged into the feature extractor and MIL classifier in an end-to-end manner.
we will elaborate on each step in the following sections.

\input{Figure/Overview} 

\subsection{Attention-based MIL Classifier with Frozen Feature Extractor}
In attention-based MIL for WSI classification, the standard training process first uses a frozen feature extractor $f(\cdot)$ pre-trained on ImageNet to extract all patch features.
Then all patch features in a WSI bag are aggregated to form the WSI feature using the attention mechanism, which learns an attention score $\alpha_{k}$ for each patch $k$ through the MIL classifier. 
The WSI feature $\bm{F}$ is obtained by computing the attention-weighted average of all patch features in a WSI as~\cite{ABMIL}: 
\begin{equation}
{\bm{F}} = \sum\limits_{k = 1}^{{K}} {{\alpha _{k}}{f(\bm{x}_{k})}},
\end{equation}  
where
\begin{equation}
{\alpha_k} = \dfrac{{\exp \left\{ {{{\bm{w}}^{\rm{T}}}\left( {\tanh \left( {{{\bm{V}}_1}{f(\bm{x}_{k})}} \right) \odot {\rm{sigmoid}}\left( {{{\bm{V}}_2}{f(\bm{x}_{k})}} \right)} \right)} \right\}}}{{\sum\limits_{j = 1}^K {\exp } \left\{ {{{\bm{w}}^{\rm{T}}}\left( {\tanh \left( {{{\bm{V}}_1}{f(\bm{x}_{k})}} \right) \odot {\rm{sigmoid}}\left( {{{\bm{V}}_2}{f(\bm{x}_{k})}} \right)} \right)} \right\}}},
\end{equation}  
where $\bm{w}$, $\bm{V}_1$ and $\bm{V}_2$ are learnable parameters in the MIL classifier, $\odot$ is the element-wise multiplication, and $\tanh(\cdot)$ and $\rm{sigmoid(\cdot)}$ denote the tanh and sigmoid activation function, respectively. \\
Finally, the MIL classifier head $h(\cdot)$ predicts the label of WSI from the WSI feature $\bm{F}$, represented as:
\begin{equation}
\tilde{\bm y} = h(\bm{F}),
\end{equation}  
where $\tilde{\bm y}$ denotes the prediction of the WSI label. 
During training, we minimize the prediction error using the cross-entropy (CE) loss.

\subsection{Representative Patch Selection.}
In WSI classification, it's common for only a small number of patches within a WSI to be associated with the disease of interest. 
For example, in the positive slides of Camelyon16~\cite{bejnordi2017diagnostic}, on average, less than 10\% of the patches in a WSI are tumor patches. 
Thus, only a few patches are sufficient to represent the entire WSI bag. 
Based on this observation, we propose a feasible solution that selects representative pathological images to fine-tune the pre-trained feature extractor for WSI classification.
In attention-based MIL, the patch with a higher attention score $\alpha_{k}$ in a WSI bag is more likely to have the same category semantics as the WSI.
Thus, based on the patches' attention score $\alpha_{k}$ calculated by the MIL classifier, we select the top-$K$ patches with the highest attention scores in each WSI as a new bag. 
Here, $K$ is set to 200, which will be discussed in Section~\ref{Implementation}. 
The new bag label is assigned as the original WSI bag label for prompt fine-tuning.
In this way, we reduce vast quantities of patches in each WSI to a small subset, enabling an end-to-end training for both feature extractor and MIL classifier.

\subsection{Prompt Fine-tuning.}
With the selected representative patches, we design a prompt learning framework to adapt the feature extractor from the natural image domain to the pathological image domain, while retaining the advantage of pre-training on the large and diverse ImageNet dataset.
Specifically, we design a prompt block that sequentially consists of a global average pooling (GAP), a multi-layer perceptron (MLP) with two layers, and a sigmoid activation, as shown in Fig.~\ref{fig_overview}. 
Given the intermediate feature map $\bm{f}_i$ from the $i$-th block of the feature extractor, the prompt block is added in parallel to the basic ResNet block $g_i(\cdot)$ to generate the visual prompt $\bm{p}_i \in {\mathbb{R}^{D}}$, where $D$ denotes the dimension of the prompt vector.
Subsequently, the generated prompts $\bm{p}_i$ are channel-wise multiplied with feature maps $\bm{f}_{i+1}$ in the next block, represented as:
\begin{equation}
\bm{p}_i = {\rm{sigmoid}}( \bm{W}_{i2} {\rm{ReLU}}(\bm{W}_{i1} {\rm{GAP}}(\bm{f}_i))),
\end{equation}
\begin{equation}
\bm{f}_{i+1} = g_i(\bm{f}_i) \odot \bm{p}_i,
\end{equation}
where $\text{ReLU}(\cdot)$ denotes rectified linear unit, $\bm{W}_{i1}$ and $\bm{W}_{i2}$ are the learnable parameters to be fine-tuned, and the parameters of $g_i(\cdot)$ remain frozen during training.
During the training process, only the parameters of prompt blocks and the lightweight MIL classifier are updated in an end-to-end manner, while the original pre-trained feature extractor is frozen. 

