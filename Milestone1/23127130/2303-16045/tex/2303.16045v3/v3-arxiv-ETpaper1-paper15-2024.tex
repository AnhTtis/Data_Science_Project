%Version 2.1 April 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-nature]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%\\usepackage[affil-it]{authblk}
\usepackage{comment}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{todonotes}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{caption}
\captionsetup{font=footnotesize}

\usepackage[symbol]{footmisc}

%\usepackage[number]{cite}

%\makeatletter
%% Redefine \@cite to remove brackets around citation numbers in text
%\def\@cite#1#2{#1\if@tempswa , #2\fi}
%% Redefine \@biblabel to remove brackets around numbers in the bibliography
%\def\@biblabel#1{#1.}
%\makeatother


%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%
%% as per the requirement new theorem styles can be included as shown below
%\theoremstyle{thmstyleone}%
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[lemma]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\newtheorem{corollary}[lemma]{Corollary}

%\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

%\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title{\vspace{-2cm}An Optimal, Universal and Agnostic Decoding Method for Message Reconstruction, Bio and Technosignature Detection}
\author[ 1,2,3,4,5]{Hector Zenil\footnote{Corresponding author. Email: \href{hector.zenil@cs.ox.ac.uk}{hector.zenil@cs.ox.ac.uk} }}
\author[5,6]{Alyssa Adams}
\author[3,7,8]{Felipe S. Abrah\~{a}o}


\affil[1]{ School of Biomedical Engineering and Imaging Sciences, King's College London, U.K}
\affil[2]{ The Alan Turing Institute, British Library, U.K}
\affil[3]{ Oxford Immune Algorithmics, U.K}
\affil[4]{ Algorithmic Dynamics Lab, Karolinska Institutet, Sweden}
%\affil[5]{\small Algorithmic Nature Group, LABORES, France.}
\affil[5]{ Cross Labs, Cross Compass, Kyoto, Japan}
\affil[6]{ John W. and Jeanne M. Rowe Center for Research in Virology, Morgridge Institute for Research, University of Wisconsinâ€“Madison, U.S}
\affil[7]{ Centre for Logic, Epistemology and the History of Science, University of Campinas, Brazil}
\affil[8]{ DEXL, National Laboratory for Scientific Computing, Brazil}


\abstract{
%Based on the principles of information theory, measure theory, and theoretical computer science, we introduce a univariate signal deconvolution method with a wide range of applications to coding theory, 
We present a signal reconstruction method for zero-knowledge one-way communication channels in which a receiver aims to interpret a message sent by an unknown source about which no prior knowledge is available and to which no return message can be sent. 
Our reconstruction method is agnostic vis-\`a-vis the arbitrarily chosen encoding-decoding scheme and other observer-dependent characteristics, such as the arbitrarily chosen computation model or underlying mathematical theory.
We investigate how non-random messages may encode information about the physical properties, such as dimension and length scales of the space in which a signal or message may have been originally encoded, embedded, or generated.
We argue that our results have applications to life and technosignature detection and to coding theory in general.
%The method derives from the principles of an approach to Artificial General Intelligence capable of building a general-purpose model of models independent of any arbitrarily assumed prior probability distribution.
}


\keywords{algorithmic information dynamics, causal deconvolution, zero-knowledge communication, technosignatures, signal processing, perturbation analysis, universal distribution, intelligent signal detection, biosignatures}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sectionIntro}


For the past 50 years, astronomers have been sweeping the skies using radio telescopes in hopes of stumbling across a message from an alien civilisation. More recently, space missions to collect samples and data from Mars, Titan, Europa, and others have been focused on detecting possible biosignatures \cite{abizenil,kiangPhotosynthesisAstrobiologyLooking2014, meadowsSearchHabitableEnvironments2009, nationalacademiesofsciencesengineeringandmedicineAstrobiologyStrategySearch2018, rojoCenterLifeDetection2022}. The James Webb Space Telescope has been outfitted with sensors and measurement devices to scout potentially habitable solar systems \cite{fridayHowJamesWebb, haqq-misraDetectabilityChlorofluorocarbonsAtmospheres2022}. For the first time in human history, the detection of extraterrestrial life is at the forefront of space exploration. In the novel Contact, written by Carl Sagan and later made into a film, an extraterrestrial signal is received that is encoded in three dimensions. A good deal of the story has to do with how scientists serendipitously deduce that the signal encodes an object in three dimensions after spending months or years trying to fit it in two dimensions.

Here we show that the more a message is removed from randomness the easier is to derive its native geometric and topological dimensions, and that such an approach derives its fruitful properties from the universality and limitations of algorithmic compression algorithms.
These properties include being agnostic (in the asymptotic limit when computational resources are unbounded) vis-\'a-vis: 
the encoding-decoding scheme used to encode the message by the emitter agent; 
computation model and
the programming language chosen in order to implement our proposed method;  
the computable (or semi-computable) method of approximation to algorithmic complexity; 
any arbitrarily chosen (computable) probability measure of the events;
and the formal theory able to formalize the mathematical statements.
While the arbitrary choice of each of these constitutes an observer dependency, this dependency is cancelled out in the long run without the need of resorting to special observers \cite{Abrahao2021b}.

%Some researchers worry that our current theoretical toolkit for data analysis may not be  sophisticated enough \cite{bottaStrategiesLifeDetection2008, enyaComparativeStudyMethods2022, neveuLadderLifeDetection2018}. 

%Our current signal processing analyses have been limited by what we know about life \textit{on Earth} in order to distinguish random noise from possible, meaningful messages or biosignatures from a distant, form of life \cite{smithFutilityExoplanetBiosignatures2022}. The true physical limits of what life can and cannot be are still largely unknown. 

All of our knowledge about life is based on what occurs here on Earth \cite{zenillife,bedauOpenProblemsArtificial2000, clelandDefiningLife2002, dupreMetaphysicsBiology2021, kempesMultiplePathsMultiple2021, mariscalHiddenConceptsHistory2019, ruiz-mirazoUniversalDefinitionLife2004, walkerAlgorithmicOriginsLife2013, witzanyWhatLife2020}. Some researchers consider technosignature detection a more promising method of detecting life than other biosignatures because of their longevity \cite{zengauch,haqq-misraSearchingTechnosignaturesExoplanetary2022a, wrightCaseTechnosignaturesWhy2022}.
However, some researchers worry that our current theoretical toolkit for data analysis may not be  sophisticated enough \cite{bottaStrategiesLifeDetection2008, enyaComparativeStudyMethods2022, neveuLadderLifeDetection2018}. 
%
%While alleged universal grammars have been proposed \cite{berwickWhyOnlyUs2015}, they have been criticised heavily by linguists \cite{evansDiversityMindFreeing2009} because languages are so diverse that universality may not be possible. Regardless, our current understanding of language is based on human-centric rules of communication. Questions about universal features of written, spoken, non-verbal, and internal languages remain largely unanswered beyond human communication \cite{bybeeLanguageUsageCognition2010, goldbergConstructionsWorkNature2006, tomaselloConstructingLanguageUsageBased2003}. For example, is there an optimal mapping between meaningful features in the environment and the number of unique words? 
%
%Is it possible to quantify what is meant by a "meaningful features"? What is the relationship between an agent capable of communication and its ability to sense its environment and process information?
%Claims of extraterrestrial radio signals have been made several times since 1899, including some presumed to be coming from Mars. But it became increasingly accepted that nothing but signals from natural (non-living) mechanisms emissions were actually the generators of the signals detected.
%The discussion involving SETI and the signals it can detect goes back to the 1970s.  The first chapter or two of the Project Cyclops report and much of the discussion in NASA's SP419 are still relevant for today's open questions about extraterrestrial life detection.  
Most of the discussion has been centred around the technical justification of hardware choices and the technicalities of detection (e.g. the frequency) and not the nature (i.e., original structural characteristics or underlying mathematical properties) of the signal itself. Thus, for example, the search for extraterrestrial signals has been mostly focused on narrow-band signals (mostly pulses) a few Hertz-wide (or narrower). 
Natural cosmic noisemakers, such as pulsars, quasars, and the turbulent, thin interstellar gas of our own Milky Way do not make radio signals that are this narrow, which is a reasonable justification of the bounds of the physical signal search. 
On Earth, we also find local signal-noise makers such as animals, organs, neurons, and cells. 
However, 
beyond identifying very basic statistical patterns, not much progress has been made on the qualitative semantic aspects of signals and communication, 
let alone on building a universal framework within which the question can be explored and analysed. 

Each different species on Earth has its own different set of sensors that it uses to interact with its environment. 
%Based on the philosophical essay "What is it like to be a bat?" \cite{nagelWhatItBe1974a}, the recent book "An Immense World" \cite{yongImmenseWorldHow2022} \todo{text}explores the unique slices of experience that are native to many different species. Because each creature is outfitted with its own, unique sensory mechanisms and capabilities, Yong illustrates in great detail how different species experience the world differently. 
To put this in terms of signals or messages between senders and receivers, different species emit signals based on the mechanisms that allow them to do so \cite{nagelWhatItBe1974a,yongImmenseWorldHow2022}. 
They also decode messages based on their sensory capabilities, and if each species has a unique set of senses, then we can say that each species decodes the same message differently. The 52Hz whale's messages are decoded differently between humans monitoring the oceans and other whales (who do not sense the messages at all).



One can say that that if a signal is intentional, it might be decipherable. 
In order to intentionally send or receive a signal over interstellar distances, it is reasonable to assume a civilisation must understand basic science and mathematics, or at least must evolve in such that these are somehow encoded into a message. Hence, a message from another civilisation might use some similar framework to science and mathematics to build up a common ground with respect to other societies. 
Signals sent by a civilisation for its own purposes may be impossible for us to unravel but our own mathematical tools may help.





% {\color{red} % \todo{erased in NatAstro. Present in the NatCompSci}
% There are myriad data types and formats that encode different types of data on multiple levels, from ASCII characters to video files to health record data. While it is possible to re-encode all data into a single format, the original message may be lost (imagine trying to extract voice-recorded messages of a sound file only by looking at the sound wave plot over time).
% Because of this diversity even within our own technology, the process of detecting, decoding, and interpreting terrestrial or extraterrestrial signals \textit{must} involve considering a wide range of possible encoding and decoding schemes. 

% %% from old section on encoding-decoding schemes \ref{sectionCompressionandrandomness}


% Without proper conditions that assure the algorithms can only pick up meaningful or sound features---given the prior assumption that a particular dataset actually contains a message---we know the presence of spurious correlations constitutes one of the dangers of naively applying any sort of statistics-based Big Data analysis to extract or find statistically significant patterns in sufficiently large datasets \cite{Calude2017,Smith2020}. 
% %A spurious correlation is a correlation found in statistical or computational analysis so that it would have occurred in a randomly generated database anyway, and therefore bears no meaningful information about the investigated phenomena nor reveals any underlying causal relation.
% %We start with the baseline assumption that
% Ideally, given a random signal such as white noise, no such signal interpretation should yield any meaningful message or interpretation.
% %Mathematically, this means that
% A random signal should not only look random, but also be interpreted as such, no matter how it is decoded, or how it was encoded.
% Thus, along with the data compression distortions known to occur from the application of statistics-based or entropy-based methods \cite{zkpaper,zenilreview2020}, the possibility of spurious findings in random messages is of major concern in decoding signal streams so as to reconstruct the original message.

% In this regard, if data is displayed in its most compressed form (therefore, random), there is little chance of recovering the multidimensional space in which the original object was embedded, let alone the original message itself.
% Such a message would be indistinguishable from an ideal fair-coin-toss sequence of events.
% %However, the schema that shortens any redundancies must be read properly by an interpreter to properly translate the shorthand redundancies back into their longer form.
% %This holds because of the basic properties in algorithmic information theory and classical information theory that a message in its most compressed form is incompressible (except for a constant that does not depend on the message) by any arbitrarily chosen universal machine, programming language, or formal theory.
% In the opposite case, a compressible (therefore non-random) message would have a large standard deviation in the range of information values for the different ways of decomposing the signal, and thus the schema that shortens any redundancies must be read properly by an interpreter to accurately translate the shorthand redundancies back into their longer form.
% %Intuitively, this is because compression is a mapping that translates the state of the original message into a different state aimed at minimizing any redundancies in the original messages in a shorter expression.
% Indeed, it is possible for a signal stream to arrive at a naive receiver in a format that contains sufficient information about how it ought to be decompressed. %e.g. a computer program and the computer itself.
% However, even in this case, the receiver must have prior knowledge about how to decode that signal into its original encoding so as to reveal the redundancies present in the original message that gave it meaning.
% Notice that it is the receiver's
% prior knowledge about how to decode the signal according to the emitter's previously chosen encoding scheme what 
% guarantees the validity of
% the classical noisy-channel coding theorem  \cite{Cover2005}. 
% It demonstrates that the emitter agent can introduce redundancies in the original message---thus making the signal data more compressible or non-random---to be correctly interpreted by the receiver so as to avoid decoding noise as meaningful information.}


In this paper, we introduce a method based upon the principles of (algorithmic) information theory that is aimed at sweeping over various possible encoding and decoding schemes to test our current limits on signal interpretation as we attempt to reconstruct the original partition (i.e., multidimensional space) in which the original message was given its ``meaning'' by the emitter agent.
This \emph{semantic} characteristic refers to the (algorithmic) information about the context or real-world correspondents of the emitter that the signal sent by this emitter is trying to convey to the receiver.
The real-world correspondents that we particularly investigate are the objects grounded in their respective contexts, that is, embedded into their respective multidimensional spaces as the emitter originally intended.
See the Sup. Inf. 
%and \cite{Zenil2024ETpaper2} 
for a formal introduction to these concepts.

\emph{One-way communication} channels are those for which the receiver cannot (in principle or in practice) send any signal back to the emitter in order to help or facilitate the decoding process of the first message sent by the emitter. 
%Thus, the phenomena discussed in the above paragraph strongly suggest that for effective communication, especially for one-way communication between any emitter agent and receiver agent, a message should not be displayed in its most compressed form, unless the receiver has knowledge of the compression schema and can uncompress the message back into its original state (disregarding efficiency).  
%%This problem may be equivalent to encoding a message for cryptographic purposes (modulo efficiency).
%Instead, a message should be received in an uncompressed form.
%%In this regard, the compressibility degree of a message is calculated by a comparison between its received encoded form and the original signal size (or length, if the signal stream is unidimensional).
%%See also Section~\ref{sectionZero-knowledge} for the theoretical framework underlying this intuition.
%See also the Sup. Inf..
We demonstrate how signals and messages may be reconstructed by deriving the number of dimensions and the scale of each length of an object from examples ranging from text to images embedded in multiple dimensions, showing a connection between irreducible information content, syntax, geometry, semantics, and topology.
In addition, this article also shows results that are not only agnostic vis-\'a-vis prior knowledge of encoding-decoding schemes, but also demonstrate sufficient conditions in this zero-knowledge scenario for enabling the reconstruction of the original \emph{message} (i.e., the original object embedded into the original multidimensional space to be conveyed to the receiver) in one-way communication channels.

\emph{Zero-knowledge communication} occurs when the receiver agent is able to correctly interpret the received signal as the originally intended message sent by the emitter agent, given that the receiver has no knowledge about the encoding-decoding scheme chosen by the emitter.
Notice that this condition of no prior information about the emitter agent only applies to encoding-decoding schemes, and consequentially also to the original multidimensional space and object that are unknown to the receiver before any communication takes place.
However, as discussed in the Sup. Inf., %and \cite{Zenil2024ETpaper2}, 
this zero-knowledge condition does not mean other assumptions regarding the unknown emitter agent are not being considered by the receiver agent.
%For example, as it is the case studied in the present article, the assumption that the emitter agent is somehow capable of performing an arbitrary enconding and compression of the message into a signal stream.
Also, the reader should not confuse zero-knowledge communication (ZKC) with zero-knowledge proof (ZKP), which is commonly studied in cryptography \cite{Buchanan2022Cryptographybook,Vadhan2023surveyonZKproofs,Allender2023AITstatisticalZK}.
Actually, %ZKP and ZKC are based upon opposing assumptions and goals regarding the zero knowledge.
%In the ZKP case, one looks for high enough probabilities of one agent correctly attesting the other agent's achievement while no information (or knowledge) is revealed about the actual content of this achievement (e.g., the proof of a theorem).
%In the ZKC case instead, one looks for high enough probabilities of one agent being correct about the actual content of what the other agent intended to communicate given that no information was known by the former about the latter.
%On the one hand, the resulting process in ZKPs should lead to no knowledge being acquired as a goal, once the acquisition of any knowledge is undesirable.
%On the other hand, the resulting process in ZKC should lead to maximum knowledge being acquired as a goal, once zero knowledge was a starting condition in the first place.
one may consider ZKP and ZKC as kindred mathematical problems but as diametrically opposing counterparts with regard to the acquisition of knowledge. %\cite{Zenil2024ETpaper2}.

The method is based on the principles of Algorithmic Information Dynamics (AID)~\cite{algodyn,nmi,Abrahao2021b}, and it consists of a perturbation analysis of a received signal. 
AID is based upon the principles of information theory and the mechanisms of algorithmic probability and the universal distribution, a formal approach to a type of Artificial General/Super Intelligence that requires the massive production of a universal distribution, the mother of all models~\cite{miracle}, to build a very large (semi-computable) model of computable approximate models.  
The underlying idea is that a computable model is a causal explanation of a piece of data.  
As a result of each perturbation applied to the received signal, a new computable approximate model is built, and then compared against the observation~\cite{nmi,maininfo}.
%As explained in the Sup. Inf., we estimate the algorithmic probability changes for a message under distinct partitions that result from perturbations.
This process then builds a large landscape of possible candidate models (along with their respective approximate complexity values) from which one can infer the best model.
In the particular context investigated in this article, the partition (i.e., multidimensional space) for which the message displays the lowest complexity indicates the original partition that the received signal stream encodes.
%For example in the case of bidimensional spaces, our method consists of finding the bidimensional space $ \mathcal{ S }_2 $ for which the algorithmic complexity of the object embedded into $ \mathcal{ S }_2 $ (i.e., the algorithmic complexity of the message) is minimised.

Overarching frameworks such as the one introduced here can be useful not only for signal detection but also for signal deconvolution from local information in, e.g., biology (what chemical signals among cells mean).  In previous work, we showed how the same technology can be used to disentangle the 3D structure of DNA and genomic information~\cite{nar}.

%{\todo{erased in the NatAstro, present in the CompSci}
%The present work applies AID to the problem of communicating messages by performing a perturbation analysis on the received signal.
%%Thereafter, the principles of AIT are applied through approximations to algorithmic complexity by using BDM.
%%These approximations are employed as a performance measure that characterizes the criterion to estimate the correct multidimensional space by distinguishing it from the wrong ones, which are the outcomes of the perturbations.
%In this manner, our present approach also contributes a universal method that is not reducible to a straightforward application of algorithmic information theory (AIT) \cite{Zenil2024ETpaper2}.
%As the number of perturbed messages strongly dominates the number of possible best message candidates, our theoretical and empirical results show that perturbation analysis enables the downward spikes displayed in the landscapes of complexities %(as shown Section~\ref{sectionResults}) 
%to be more likely to indicate the best message candidates, even though the exact values of the algorithmic complexities are semi-computable, i.e., they are uncomputable and one can only asymptotically approximate them from above.
%Although there is this intrinsically subjective ``deficiency'' in the perturbation analysis that forbids an arbitrarily precise approximation to algorithmic complexity, we demonstrate that this subjective aspect can be eventually cancelled out in the perturbation analysis phase.
%In fact, as discussed in the Sup. Inf. and \cite{Zenil2024ETpaper2}, it is this theoretical limitation given by semi-computability that, counter-intuitively, enables the perturbation analysis to overcome this and the other subjective aspects (such as an arbitrarily chosen programming language, probability measure $ \mathbf{P} $, or the computational appliances available) involved in the signal processing and interpretation process---that is, in finding an objective piece of information. }


%In what follows, we show how to reconstruct signals and messages by deriving the number of dimensions and the scale of each length of an object from examples ranging from text to images embedded in multiple dimensions, showing a connection between irreducible information content, syntax, geometry, semantics, and topology.
%See also Sections~\ref{sectionAIDistortions} and~\ref{sectionRetrievability} for a formalisation of the theoretical background of this method that applies to multidimensional spaces in general.

%% from old section \ref{sectionCompressionandrandomness}
%Our results are not only agnostic to prior knowledge of encoding-decoding schemes, but also demonstrate sufficient conditions in this zero-knowledge context for enabling the reconstruction of the original \emph{message} (i.e., the original object embedded into the original multidimensional space to be conveyed to the receiver) in one-way communication channels.
%%%

%\section{Results}\label{sectionResults}

%We start with the baseline assumption that given a random signal, such as white noise, no such signal interpretation should produce any meaningful message or interpretation. Mathematically, this means that a random signal will look random no matter how it is decoded. A non-random message would have a large standard deviation in the range of information values for its different arrangements. If data is compressed, however, there is little chance to recover the dimension, let alone the original message. This is because compression is a mapping that translates the state of the original message into a different state aimed at expressing any redundancies in the original messages as a shorter expression. However, the schema that shortens any redundancies must be read properly by an interpreter to properly translate the shorthand redundancies back into its longer form. This strongly suggests that for effective communication, especially for one-way communication between any sender and any receiver, data should not be compressed (unless the receiver has knowledge about the compression schema and can uncompress the message back into its original state). It is also possible for a compressed message to arrive to a naive receiver in a format that decompresses itself, e.g. a computer program and the computer itself. However, even in this case, the receiver must have knowledge about how to decode that signal into its original encoding.
%
%Imagine a sender encodes a flattened image as a sequence of bits and a receiver detects these bits as an incoming stream. Is there any way for the receiver to tell that the stream is a message delivered in a bidimensional array, i.e. that the message is actually an image? And, in the general case where the receiver doesn't know anything about the original message at all, can a lower-information transmission encode its original dimension such that the message is natively encoded? At present, there are at least two common ways to flatten an image, but we don't yet know if it is possible to reconstruct the image given any arbitrary encoding and no information about the original message or its encoding. Is this information somehow embedded in the data? If so, is it possible to recover it?
%
%Many compression algorithms linearise data as a part of the compression process, so why does exploring different before-compression dimensions matter? If there is a shift, change, or edit in the data, then re-flattening the data does not lead to the same compressed arrangement. Compression methods only work because they capitalise on the most redundant arrangement with a lower algorithmic complexity value (or more regularities, assuming that the data actually contains a message).

%Here is an example with a few different sequences. The first is a simple sequence of alternating zeroes and ones. If this sequence is perturbed by flipping a single bit, the size of the compressed sequence will be larger than the size of the compressed original sequence. Fig.~\ref{compression} shows this result for sequences up to size 200.

%\begin{figure}[ht!]
%\centering
%\includegraphics[scale=0.35]{example1.png}\includegraphics[scale=0.35]{example1_1.png}
%\caption{\label{compression}Left: Perturbing a single bit in a regular sequence of alternating 0's and 1's will cause the resulting size (in bits) of compression to increase when compared to the size of compression of the original sequence. Right: The block entropy value of perturbed sequences does not change, but the BDM increases for all but one sequence length.}
%\end{figure} 

\section{Perturbation analysis of compressed data}\label{sectionAIDonDarwins}



To showcase the relationship between compression, algorithmic complexity, and encoding methods, introducing the method put forward in this work, we took real sentences from Darwin's ``Origin of Species'' and binarised them in two different ways according to arbitrary ASCII-to-binary functions. 
In one encoding (left column in Fig.~\ref{sentences}), ASCII vowels are assigned a 1 and all others 0. For the other encoding (right column), spaces are assigned a 1 and the rest 0. The compression, block entropy, and block decomposition method (BDM) are measured for each string. Then, for each string, each value is perturbed (1 to a 0, or vice-versa) one at a time. The resulting change in values from the original binary sequence to the perturbed ones are shown as different coloured lines for each panel.


%\color{blue}As formalised in \cite{Abrahao2021b} any transformation or perturbation of a finite object into another finite object is equivalent to an algorithmic transformation that takes the former as input and outputs the latter.
%Thus, a perturbation with low algorithmic complexity is one whose algorithmic information content is preserved under such perturbations.
%Swapping all 0's and 1's, or swapping all black and white pixels in an image are examples of low-algorithmic-complexity transformations.
%Randomly inserting or deleting an element (such as creating or destroying an edge in a graph or perturbing a single bit in a string) is also an example of low-algorithmic-complexity transformation \cite{Abrahao2021b,Zenil2019c}.
%Note that a perturbation can change either the object or the multidimensional space independently, or can change both at the same time.\color{black}


The results in Fig.~\ref{sentences} demonstrate that most single-bit perturbations in a sequence that encodes a message result in a more random-looking sequence. These results vary depending on what mapping we choose to binarise the sequences, but in general, any perturbation generally increases randomness across various mappings (we also explored other arbitrary binarisation functions, with similar results). In other words, this example shows that sequences that carry meaningful content are quantitatively different according to various measures. Any deviation (i.e., perturbation) from the encoded sequences causes the randomness to increase, which suggests that an incoming signal with low randomness is more likely to encode a message than one that has more randomness.
%See Section~\ref{sectionZero-knowledge} for the theoretical investigation of this phenomenon.

%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.4]{Figures/newfig1.png}
%	\caption{\label{sentences}Algorithmic perturbation analysis on short sentences of only 400 characters from Darwin's ``Origin of Species'' are converted from ASCII into their corresponding binary values. Changing a single character with probability $1/n$ for growing $n$ from $n=1$ (original test, axis $x=0$) to $n=4$ (a quarter of the text is mapped to a random character from the same vocabulary of the original text, axis $x=100$ letters changed or perturbed) causes the resulting size of the compressed sequence index to increase, with BDM the most sensitive and Shannon entropy unable to capture any difference. This is because signals carrying meaning are far removed from randomness, and random perturbations make the text more random \cite{Zenil2019c} even when the methods know nothing about words, grammar or anything linguistic.}
%\end{figure}

Fig.~\ref{sound} demonstrates the same phenomenon shown in non-random text but in audio signals, the results using an audio recording of the words spoken from Apollo 13, ``Houston, we've had a problem'' transmitted on April 13, 1970 at a sample rate frequency of 11025 Hz on a single channel. When compressed, the message is 106\,250 bytes and 106\,320 characters long. Here, the message was scrambled several times. The histograms on the bottom left show the different compression lengths in bytes and string lengths of the scrambled messages. Each of these scramblings results in an increase in randomness. Additionally, if only the beginning and end of each word are perturbed, the resulting randomness increases smoothly with each perturbation, as shown in the bottom right plot. 
These results indicate that perturbations of the original message, including scrambling elements of the original message in a different order, will result in a message of higher complexity and thus greater randomness. As a result, the original encoding of the message (the one with an interpretable meaning) is the one with the lowest complexity.


%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.37]{Figures/sound.png}
%	\vspace{0.5cm}
%	
%	\centering
%	\includegraphics[scale=0.32]{Figures/soundhist.png}\hspace{0.3cm}\includegraphics[scale=0.31]{Figures/plot.png}
%	\caption{\label{sound}\textit{Top:} Sound waves of the words spoken from Apollo 13, ``Houston, we've had a problem.'' transmitted on April 13, 1970 at a sample rate frequency of 11025 Hz on a single channel. \textit{Bottom left:} Histograms of the change in compression lengths of scrambled versions of the same message from the length of the original compressed message. \textit{Bottom right:} Small perturbations of the original message by scrambling only the beginning and end of each word shows a smooth transition from low to high randomness. The lowest complexity signal indicates the correct (original) signal. The file was processed in FLAC format (Free Lossless Audio Codec) from a lossless file, with no audio data discarded.}
%\end{figure}

%A combination of perturbation analysis and information content, an area called Algorithmic Information Dynamics~\cite{algodyn,nmi}, based on algorithmic complexity, i.e. the minimal length of a binary-coded string that completely defines a system, can be a key asset for the search of meaningful signals due to its universality and independence from priors and inaccessible probability distributions. Thus, the recognition of bit strings of various complexity up to incompressible random sequences, will require a algorithmic information-based strategy to analyse cosmic signals.


\section{On the information content of the Arecibo message}\label{sectionArecibo}



In 1974, the bitmap image in Fig.~\ref{arecibo} was sent into space as a radio signal from the Arecibo radio telescope. At the left-hand end of the image is a version of the pattern of digits from page 117-- but it is distorted so it has no obvious nested structure. In the image, there are atomic numbers for various elements and bitvectors for components of DNA. Under these, there are rough pictorial representations of a DNA molecule, a human, and the telescope. All these images seem to depend almost completely on human visual encoding/decoding conventions. Without any sort of human context, including 
any indication that these are pictorial representations based on human vision, their meaning would be essentially impossible to recognise. This is especially true for message receivers who may not possess visual recognition capabilities, at least not visual capabilities that are similar to our own vision.

%\begin{figure}[ht!]
%	\centerline{\includegraphics[scale=0.28]{Figures/Fig1.png}
%		\hspace{1cm}
%		\includegraphics[scale=0.31]{Figures/Fig2.png}}
%	\caption{\label{arecibo}\textit{Left:} The original Arecibo message intended to be reconstructed, but sent as a linear stream from the radiotelescope in Arecibo, Puerto Rico. The 1,679 bits are meant to be arranged into 23 columns of 73 rows, 23 and 73 being two prime numbers which when multiplied together equal 1,679. \textit{Right:} If the stream is instead arranged into 23 rows and 73 columns, the original visual interpretations of the message are scrambled, which may result in a figure that is closer to being statistically random. What we show is that the message is still there, concealed, and can be deciphered by algorithmic deconvolution.}
%\end{figure} 
%
%
%
%\begin{figure}[ht!]
%	\centerline{\includegraphics[scale=0.44]{Figures/signaldeconv.png}}
%	\caption{\label{arecibosequence}\textit{Top left:} Most possible partitions result in random-appearing configurations with high corresponding complexity, indicating measurable randomness. \textit{Bottom:} Some partitions will approximate the originally encoded meaning (third from the right). Other configurations result in images with higher complexity values. This sequence of images shows the images in the approximate vicinity of the correct bidimensional configuration (i.e., partition) and illustrates fast convergence to low complexity. \textit{Top right:} By using different information indexes across different configurations, a downward-pointing spike will indicate message (image) configurations that correspond to low-complexity image(s). This allows a prior-knowledge-agnostic and objective method to infer a message's original encoding. Of the various measures, BDM, combining classical information (entropy) for long ranges and a measure motivated by algorithmic probability for short ranges, is the most sensitive and accurate in this regard. Traditional compression and entropy also indicate the right configuration amongst the top spiking candidates. The ratio of noise-to-signal was amplified in favour of the hidden structure by multiplying the original image size by 6 for both length and height.}
%\end{figure} 

In addition, this message was reformatted from its original bidimensional state and sent as a string of 1,679 binary digits. The reasoning was that an alien civilisation with a mathematical system that receives the message will recognise 1,679 as a semi-prime number--a multiple of 23 and 73. Even if the original message could not be reconstructed in its original and intended encoding, the receiver would recognise the message as having some mathematical significance that is unlikely to be a random signal from outer space.

Fig.~\ref{arecibosequence} shows how the method provides a robust approach to identifying the best way to decode a signal back to its original multidimensional space, by
 sweeping over a large variety of possible dimensional configurations and measuring the resulting information content and algorithmic complexity of the candidate messages by taking the lowest algorithmic complexity configurations under the assumption that the original message is algorithmically not random (i.e., that the original message is compressible).  
As shown in \cite{Zenil2019b} for graphs, this is because if the original message is random, then the likelihood that a rearrangement (i.e., a new partition) will lead to a low complexity configuration is very low. On the other hand, if the original message is sufficiently compressible, 
%(see \cite[Section $4.4$]{SupplInfo}), 
and therefore not random, most partitions will lead to configurations of greater algorithmic randomness.


%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.36]{Figures/additivenoise.png}
%	\caption{\label{noise}The method's resilience in the face of some noise. At 3\% of the bits of the original $23 \times 73=1679$-pixel image randomly flipped (which means about 1.5\% were binary negated), the method remains sensitive and displays a small downward spike at the 23 value, uncovering its length, but the signal gets lost when more bits are flipped.}
%\end{figure}

Fig.~\ref{noise} shows the method's noise resistance to indicate the candidate partition and the precise lengths of the original message. Successful amplification of the signal is shown in Fig.~\ref{ampnoise}, with BDM outperforming Compression and Shannon Entropy in the face of additive noise, with Compression showing insensitivity to the original signal at about 10\% of bits flipped, and Shannon Entropy diminishing faster than BDM but slower than Compression.  In all cases, different levels of robustness at deconvolving the image dimensions are shown.

Investigating this further, Fig.~\ref{moreexps} shows six bidimensional images along with their original numerical dimension. The values of the three complexity measures over possible partitions are shown below the original numerical dimension for ease of comparison.
The method is invariant with regard to linear transformations such as encoding, as shown in images 2 and 4 of Fig.~\ref{moreexps}. 
Drops in complexity (downward spikes) indicate candidate dimensions for the original encoding dimensions, and thus the decoding dimension that would process the signal such that it produces the lowest-complexity message. BDM outperforms compression and entropy and correctly identifies the original message (image), encoding for 4 out of 6 images. 

%\begin{figure}[H]
%	\centering
%	\includegraphics[scale=0.26]{Figures/noisedeconv.png}\hspace{.8cm}\includegraphics[scale=0.26]{Figures/amplifiedconvnoise.png}
%	\caption{\label{ampnoise}Applying 3 different quantitative methods, it is shown (left) that they are highly insensitive to signal and highly sensitive to noise (16.5\% of pixels randomly flipped). However, by growing the original image (right) by a factor of 6 on each dimension (i.e. 1 pixel becomes $6\times6$), the methods are less sensitive to noise and more sensitive to signal, with BDM significantly outperforming Compression, and Shannon Entropy showing sensitivity at up to 60\% pixels flipped (hence about 30\% of the original image) versus Compression and Shannon Entropy, that are about 50\%
% sensitive. Downward spikes (right) are shown at $23 \times 6 = 141$.}
%\end{figure}

%The method is invariant to linear transformations such as encoding, as shown in images 2 and 4 of Fig.~\ref{moreexps}. 
%Corroborating the theoretical predictions from Section~\ref{sectionRetrievability}, algorithmic complexity (as estimated via BDM) is also shown to be invariant to low-algorithmic-complexity transformations that preserve the data algorithmic content (such as swapping all 0's and 1's or swapping all black and white pixels in an image), as well as some non-linear but algorithmic-information-content-preserving transformations~\cite{kolmo2d,bdmpaper}.
%Indeed, Section~\ref{sectionRetrievability} also presents theoretical findings that demonstrate that if the original message is highly compressible, then the amount of low-complexity perturbations (whether introducing noise into the message or rearranging the dimensions into a different partition) on the message which drastically move the message toward randomness strongly dominates the amount of those which do not.
%Therefore, in case this hold, the original message can be estimated from the one that minimises algorithmic complexity with probability as high as one wishes, which predicts our empirical findings in the ideal scenario in which computational resources are unbounded.

%See Section~\ref{sectionRobustenesstoperturbations}. 
%As also demonstrated in Section~\ref{sectionRobustenesstoperturbations}, this method is invariant to linear transformations as well as some non-linear but algorithmic-information-content-preserving transformations~\cite{kolmo2d,bdmpaper}.


%As demonstrated in Section~\ref{sectionRetrievability}, the best way to decode a message is the one that produces the least-random-looking message. 

%\begin{figure}[ht]
%\centerline{\includegraphics[scale=0.34]{Figures/Fig10.png}}
%	\caption{\label{seqs}Sequence of lowest complexity (BDM value on top in bits) partitions of a black and white image of a space shuttle, with the top right partition as the correct reconstruction and other candidates cases in which an alignment occurs at some multiple of the correct dimension.}
%\end{figure}
%\pagebreak
Some of these image decodings for the space shuttle image are shown in Fig.~\ref{seqs}. 
Each panel shows random realignments of the correct dimensions along with the original image (i.e., object) embedded into these new partitions, with their respective BDM values displayed above each image. 
%The original (and correct) image has the lowest complexity value. 
When compared to the other bidimensional spaces, the one that is most similar to the original message will have a lower complexity value.

By translating (encoding) a colour image into binary black and white pixels, this method picks an image decoding that is similar enough to the original image (at least similar enough visually). Fig.~\ref{mandril} shows the original image on the left, the results of the method on the right, and the resulting image selected from the spike in BDM on the bottom. A mirror-like image is selected as the first candidate, followed by the correct one. This kind of spiking also suggests the original partition can be inferred via smaller, non-random spikes at multiples or divisors of the native partition (500). In this case, small spikes at half the original partition (250) provide clues to the original embedding in 2D. 


%\begin{figure}[H]
%	\centerline{\hspace{.8cm}\includegraphics[scale=0.55]{Figures/Fig6.png}}
%	
%	\begin{center}
%		196 \hspace{3.8cm} 82 \hspace{4cm} 215
%	\end{center}
%	
%	\centerline{\includegraphics[scale=0.27]{Figures/Fig7.png}}
%	
%	\vspace{0.5cm}
%	
%	\centerline{\hspace{.8cm}\includegraphics[scale=0.37]{Figures/Fig8.png}}
%	
%	\begin{center}
%		{276 \hspace{3.6cm} 360 \hspace{3.6cm} 220}
%	\end{center}
%	
%	\centerline{\includegraphics[scale=0.28]{Figures/Fig9.png}}
%	\caption{\label{moreexps}Six 2D images (labelled 1 through 6 going from left to right, top to bottom) of very different nature, including a demonstration of linear-transformation invariance conforming to the underlying theory (in this case, rotation of a mathematical formula). Size invariance has actually shown amplification of signal-to-noise difference. Under each image is its correct numerical (first) dimension. The values of the three complexity indexes over possible partitions are shown below the original numerical dimension for ease of comparison. Downward spikes indicate candidates for possible original partitions. In all cases, the correct dimension value is among the top three candidates, with BDM outperforming in 4 out of 6 cases at indicating the top candidate.}
%\end{figure}
%
%
%
%\begin{figure}[H]
%	
%	\flushleft\footnotesize{\hspace{2.4cm}500$\times$500}
%	\centerline{\includegraphics[scale=0.32]{Figures/Fig15.png}\includegraphics[scale=0.3]{Figures/Fig16.png}}
%	\flushleft\footnotesize{\hspace{6cm}750$\times$250}
%	\centerline{\includegraphics[scale=0.57]{Figures/Fig29.png}}
%	\caption{\label{mandril}\textit{Top Left:} An original 2D image message and \textit{top right:} the complexity results of its various image reconstructions via linear signal decomposition (in binary, similar to how the Arecibo message was sent) into distinct partitions. BDM spikes prominently at the correct right partition with bidimensional configuration of 500 $\times$ 500 pixels, contrasting with a mirror-like image at 250 $\times$ 750 pixels (\textit{bottom}, rotated 45 degrees counterclockwise).}
%\end{figure}


%When a message is represented in a different partition than the original message, the resulting plot shows a regular statistical pattern. 
%For example, encoding a tridimensional image into a bidimensional space will produce plots similar to the ones show on the bottom-left of the figure. 
%Fig.~\ref{bone} shows upward and downward spikes for an image in three dimensions. Downward spikes indicate partitions where the algorithm interprets the signal (image) as having low complexity, whereas upward spikes indicate high complexity.

\begin{comment}

\begin{figure}[H]
	\flushleft\footnotesize{\hspace{3cm}128 $\times$ 128
		$\times$ 128 \hspace{3.7cm} 64 $\times$ 128 $\times$  192}
	\centerline{\includegraphics[scale=0.28]{Figures/Fig31.png}\hspace{1.5cm}\includegraphics[scale=0.27]{Figures/Fig28.png}}
	\centerline{\includegraphics[scale=0.35]{Figures/Fig23.png}}
	\caption{\label{bone}\textit{Top left:} A reconstruction exercise of a 3D image of a Magnetic Resonance image of a knee embedded in a cube. \textit{Top right:} Reconstruction from the \textit{bottom:} perturbation analysis on various partitions. Spikes occur at the original dimension's multiples: 64, 128, and 192. When the linear signal stream is partitioned at the first candidate, the next dimensions are indicated by downward spikes, or upward spikes even on the first pass. A mirror image (top right) is indicated and reconstructed as the most likely candidate, and the correct knee configuration appears at the second spike (top left).}
\end{figure}

\end{comment}



\section{Discussion}

%As demonstrated in the Sup. Inf., algorithmic complexity (as estimated via BDM) is also shown to be invariant to low-algorithmic-complexity transformations that preserve the algorithmic content of the data (such as swapping all 0s and 1s or swapping all black and white pixels in an image), as well as some non-linear but algorithmic-information-content-preserving transformations~\cite{kolmo2d,bdmpaper}.
%Indeed, we also demonstrate in the Sup. Inf. that if the original message is highly compressible, then the number of low-complexity perturbations (whether introducing noise into the message or rearranging the dimensions into a different partition) on the message which drastically move the message toward randomness strongly dominates the amount of those which do not.
%Therefore, in cases where this holds, the original message can be estimated from the one that minimises algorithmic complexity with probability as high as one wishes, which predicts our empirical findings in the ideal scenario in which computational resources are sufficiently large.
%%See \cite[Sections~$4.2$, $4.3$, and $4.3$]{SupplInfo}.



%We have introduced and demonstrated a practical and reliable method for reconstructing properties of non-random signals or messages related to their physical properties. 
%Our methods show how the receiver can decode the (multidimensional) space into which the original message was sent via zero-knowledge one-way communication channels. 
%We have presented signal-amplification techniques that enable the methods to get a better signal-to-noise reading and investigated how the methods perform in the face of noise.

%Our method is based on algorithmic information dynamics (AID) which combines tools from classical information theory and algorithmic information theory, in which algorithmic complexity values are approximated and combined with Shannon entropy by using the block decomposition method (BDM), in order to perform a perturbation analysis so that this algorithmic-information-based method can be used to avoid encoding biases in signal detection and processing. 
%Our method is agnostic vis-\`a-vis endoding-decoding schemes, computation model, programming language, and the arbitrarily chosen formal theory.
%%and can be extended and used for bio- and technosignature detection and signal deconvolution.
%%Such results relate information theory to fundamental areas of mathematics, such as geometry and topology, by means of compression and algorithmic probability.
%%More specifically, this method is developed based on the discovery that the native dimension in which non-random data is originally produced and encoded can be inferred from the data itself. 

%By using 3 different methods of measuring complexity (BDM, block entropy, and compression), we found that BDM is the most reliable measure that indicates a message's original native dimension when applied to a multiplicity of data. 

This work advances a practical and theoretical framework that relates information, entropy, complexity, and semantics that can be extended and hence put to multiple uses in signal deconvolution, bio- and technosignature detection, cryptography, and coding theory. 
Our methods show how the receiver can decode the (multidimensional) space into which the original message was sent via zero-knowledge one-way communication channels.

We believe that the present work is only one example of how our mathematical and computational framework can be used in all areas of inverse problems as an approach to a large universal generative model able to instantiate Artificial General Intelligence from first principles, particularly in application to some specific cases of message reconstruction in which prior knowledge about the source is very limited.
Such results relate information theory to fundamental areas of mathematics, such as geometry and topology, by means of compression and algorithmic probability.

%Because we cannot assume any original encoding for any extraterrestrial signal, whether chemical, visual, spatial, or temporal, it is critical that astrobiologists adopt a signal detection method that is independent of any assumed encoding or decoding scheme for some incoming message.
%We should not a priori assume that any other type of intelligence has any bias toward a particular multidimensional space into which we already are used to giving meanings to the objects.
%In other words, we should not a priori assume that the multidimensional space of an encoded message in which we have zero knowledge about is isomorphic to ours.
%Otherwise, such assumptions would constitute either an anthropomorphic bias or a bias toward our current state of technology and knowledge about physics.
%As shown in this article, algorithmic information dynamics (AID) offers a way out of this kind of biases by estimating algorithmic complexity over a large variety of possible encodings and perturbed multidimensional spaces, so it is possible to recover the original physical meaning from a message that would otherwise be lost.



%\section{Methods}\label{sectionMethods}
%\todo{Section embedded into the new Sup. Inf.}
%
%
%
%
%\subsection{Basic concepts}\label{sectionBasicconcepts}
%
%
%For the purposes of the present article, we say an object has ``meaning'' when the object conveyed in the received signal is actually embedded into (or grounded on) the multidimensional space (i.e., the context) that the emitter agent originally intended.
%Each (finite and discrete) \emph{multidimensional space} $ \mathcal{ S } $---no matter how complex it is---can be univocally determined by the number of dimensions, the encoding of the set of elements for each dimension, and the ordering in which each dimension appears.
%For example: 
%the unidimensional space $ \mathcal{ S }_1 $ that takes values from the natural numbers can be univocally determined by only informing the length $ \left| \mathcal{ S }_1 \right| $ in $ \mathbf{O}\left( \log\left( \left| \mathcal{ S }_1 \right| \right) \right) $ bits; 
%the bidimensional space $ \mathcal{ S }_2 $ by a pair $ \left( \left| {  \mathcal{ S }_2 }_x \right| ,  \left| { \mathcal{ S }_2 }_y \right| \right) $ in $ \mathbf{O}\left( \log\left( \left| {  \mathcal{ S }_2 }_x \right| \right) \right) + \mathbf{O}\left( \log\left( \left| {  \mathcal{ S }_2 }_y \right| \right) \right) + \mathbf{O}\left( 1 \right) $ bits, 
%where $ { \mathcal{ S }_2 }_x $ is the first dimension and $ { \mathcal{ S }_2 }_y $ is the second dimension, and so on.
%
%In this manner, one is always able to (uniquely) decode the received message into sufficient information to completely determine the multidimensional space configuration.
%One example of an encoding scheme for an arbitrary multidimensional space $ \mathcal{ S } $ consists on encoding it in the form of a companion tuple $ \bm{ \tau } $ as in \cite{Abrahao2020c,Abrahao2021}.
%Finite and discrete multidimensional spaces with more intricate configurations other than whole number multiplication in the form of $ m_1 \times m_2 \times \dots \times m_k $, where $ m,k \in \mathbb{N} $, can, for example, be encoded by node-unaligned companion tuples $ \bm{ \tau }_{ ua } $ as in \cite[Definition~2]{Abrahao2021}.
%Nevertheless, as demonstrated in \cite{SupplInfo}, the theoretical framework of our method is agnostic vis-\`a-vis any arbitrarily chosen computational scheme for encoding multidimensional spaces.
%
%Each \emph{partition} of a $n$-bit-length linear signal stream corresponds to a distinct configuration of dimension lengths, and therefore to a distinct multidimensional space in which the total additive dimension lengths remains upper bounded by the constant $ n $.
%For example, a picture of 4$\times$4 pixels can be partitioned into other dimension-length configurations leading to different non-squared shapes, like 2$\times$8, 8$\times$2, or 1$\times$16.
%Once a row (or, alternatively, a column) is fixed in a bidimensional space, the remaining rows and columns are dependent on the initial row fixation. 
%In other words, each bidimensional partition is a single-variable dependency, which is the main rationale underlying the sweep in our algorithms employed to achieve the results in Section~\ref{sectionResults}.
%
%More formally, in the case of a (finite and discrete) bidimensional space $ \mathcal{ S }_2 $, 
%one may have different combinations of values $ \left| {  \mathcal{ S }_2 }_x \right| $ and $ \left| {  \mathcal{ S }_2 }_y \right| $ for each partition.
%However, for any partition one has it that both
%$
%1 \leq \left| {  \mathcal{ S }_2 }_x \right| \leq n
%$ and
%$
%\lfloor \frac{ n }{ \left| {  \mathcal{ S }_2 }_x \right| } \rfloor = \left| {  \mathcal{ S }_2 }_y \right|
%$
%hold.
%Thus, because each partition is single-variable dependent on $ { \mathcal{ S }_2 }_x $, for any $n$-bit-length linear signal stream one can encode each of its $ 2 $D-partitions in $ \mathbf{O}\left( \log\left( n \right) \right) $ bits.
%The same applies to $ 3 $D-partitions (or to any partition from a finite number of dimensions), except for an extra number of bits upper bounded by a partition-independent constant $ \mathbf{O}\left( 1 \right) $.
%
%
%As formalised in \cite{Abrahao2021b} any transformation or perturbation of a finite (encoded) object into another finite (encoded) object is equivalent to an algorithmic transformation that takes the former as input and outputs the latter.
%%(See also Section~\ref{sectionRobustenesstoperturbations}).
%Thus, a (algorithmic) perturbation with low algorithmic complexity is one of these transformations where the algorithmic information content of the original object is mostly preserved under such perturbations.
%More formally, a \emph{low-complexity perturbation} occurs when $ \mathbf{K}\left( y \middle\vert x \right) = \mathbf{O}\left( \log\left( \left| x \right| \right) \right) $, where $ \mathbf{K}\left( \cdot \right) $ denotes the algorithmic complexity, $ x $ is the original encoded object, $ y $ is the encoded object that results from the perturbation on $ x $, and $ \left| x \right| $ is the size of the object $ x $.
%
%Swapping \emph{all} 0s for 1s, or swapping \emph{all} black for white pixels in an image are examples of low-complexity perturbations (in these two cases in particular, one has it that $ \mathbf{K}\left( y \middle\vert x \right) = \mathbf{O}\left( 1 \right) $).
%Randomly inserting or deleting a finite number of elements (such as creating or destroying edges in a graph or flipping a finite number of bits in a string) are also examples of low-complexity perturbations \cite{Abrahao2021b,Zenil2019c,algodyn}.
%As we saw in the above paragraphs, changing the partition is also a low-complexity perturbation of the multidimensional space into which the object $ x $ is embedded. 
%This is because any new partition can always be encoded in $ \mathbf{O}\left( \log\left( \left| x \right| \right) \right) $ bits.
%Notice that a perturbation can change either the object (e.g., by flipping bits) or its multidimensional space (e.g., by reconfiguring the dimension lengths of a given partition to achieve another distinct partition) independently, or can change both at the same time.
%In any of these three options, the encoded form of the message resulting from the perturbation will be distinct from the encoded form of the original message.
%Both changing the partition into which the object is embedded and introducing noise into the object itself (e.g., by flipping bits) are particular examples of (algorithmic) perturbations \cite{Abrahao2021b,Zenil2019c,algodyn}. 
%Thus, notice that these properties of low-complexity perturbations underpin the methodology employed in Section~\ref{sectionResults}.
%
%
%
%\subsection{Distortions in arbitrarily complex multidimensional spaces}\label{sectionAIDistortions}
%
%
%
%
%
%
%\citet{Abrahao2020c,Abrahao2021} investigates how much of the irreducible information content of a multidimensional network and its isomorphic monoplex network (i.e., its isomorphic graph) is preserved during such an isomorphism transformation.
%In fact, isomorphisms are demonstrated to not preserve algorithmic information in the general case.
%The algorithmic information of a multidimensional network might be exponentially distorted with respect to the algorithmic information of its isomorphic (monoplex) network (see \cite[Theorem~12]{Abrahao2021}).
%We refer to an algorithmic information \emph{distortion} when the algorithmic information content of an object (e.g., measured by the prefix algorithmic complexity of an encoded form of the object) in a certain context (e.g., the multidimensional space the object is embedded into) sufficiently differs from the algorithmic information content of the same object in a distinct context.
%In this regard, algorithmic complexity \emph{oscillations} may arise from a series of distortions produced by a large enough collection of distinct contexts into which the same object is embedded.
%
%%From the well-known invariance in algorithmic information theory (AIT) \cite{Chaitin2004,Calude2002,Li1997,Downey2010}, one for example knows that every positive or negative oscillation resulting from changing the computation model (or universal programming language) is pairwise upper and lower bounded by a constant that does not depend on the object output by this pair of machines.
%In Section~\ref{sectionResults}, we investigate the oscillations measured by the plots presented in the figures, particularly those image plots for which one has the $x$-axis representing the distinct partitions we estimated the algorithmic complexity of the messages into.
%Notice that the downward spikes are examples of algorithmic complexity (negative) oscillations such that, for the decoding problem we are interested in, indicate the partition that corresponds to the correct multidimensional space the message intended. 
%
%The results in \cite{Abrahao2020c,Abrahao2021} demonstrate a fundamental limitation that mathematical and computational methods should be aware of in order to properly reconstruct the original multidimensional space from a receiver.
%%To demonstrate this limitation, take for example received signals that encode (either in $1$D or $2$D signal streams) adjacency matrices of the (monoplex) networks whose isomorphic counterparts are multidimensional.
%For any arbitrarily chosen formal theory and decoding algorithm, the theorems in \cite{Abrahao2020c,Abrahao2021} imply that there is a sufficiently large multidimensional network whose multidimensional space cannot be retrieved from the adjacency matrix of the network by the respective formal theory and/or decoding algorithm.
%%This occurs because the algorithmic information distortions (due to being exponential) simply outgrow any prediction or output capability of the formal theory together with the decoding algorithm, should the network be given as input \cite[Theorem 12]{Abrahao2021}.
%
%Such distortions demonstrate that in the general case where the multidimensional spaces may be much more complex than the objects themselves, there is a fundamental limitation for any formal theory with regard to the original multidimensional space to which a received message containing an object embedded into a very simple multidimensional space (e.g., a linear one, such as a signal stream) in fact corresponds. 
%This implies that even in the ideal scenario in which we correctly infer the actual object sent by a completely unknown emitter, we are still prone to being wrong about the original dimensions' configurations the emitter intended.
%It also poses a challenge to straightforward applications of the \emph{algorithmic coding theorem} \cite{Calude2002,Downey2010}, and also in the case of arbitrarily large messages \cite{Abrahao2021darxiv}. 
%
%Therefore, in order to circumvent this theoretical limitation, one needs to investigate additional conditions to be met so one can correctly infer the message, given that the receiver has no prior knowledge about the source.
%In other words, if one aims to develop methods for multidimensional reconstruction of received low-dimensional signals in which one does not have access to knowledge about the encoding-decoding scheme or the original multidimensional space, one should look for conditions or assumptions that avoid such distortions.
%%\color{blue}We will use the term distortion and oscillation capturing possible changes in the encoding of a message or even noise indistinguishably as equivalent terms.\color{black}
%One most obvious approach to avoiding them is to limit the algorithmic complexity of the multidimensional space itself with respect to the algorithmic complexity of the object.
%As we demonstrate in \cite{SupplInfo}, and discuss in the following Section~\ref{sectionTheoreticalresults}, a proper formalisation of this intuition is indeed sufficient for achieving an encoding/decoding-agnostic method for reconstructing the original multidimensional spaces. 
%%and the message length scales.
%
%
%\subsection{Zero-knowledge one-way communication}\label{sectionTheoreticalresults}
%
%
%We have demonstrated in \cite{SupplInfo} conditions for which decoding the message in the original multidimensional space $ \mathcal{ S } $ chosen from the one in which the message has the lowest algorithmic complexity (i.e., highest algorithmic probability, as assured by the universal distribution defined by $ \mathbf{P}_{ \mathbf{U} } $) becomes eventually more likely to be correct than decoding the message in a distinct multidimensional space $ \mathcal{ S }' $, with or without noise. 
%The underlying idea is that the larger the algorithmic complexity of the closer-to-randomness message candidate in the wrong multidimensional space with respect to the algorithmic complexity of the message in the correct multidimensional space, the larger the probability of the correct multidimensional space with respect to the probability of the wrong closer-to-randomness multidimensional space. 
%Thus, finding the multidimensional space that minimises the algorithmic complexity the most, eventually increases the probability of finding the correct one in comparison to the probability of finding the wrong closer-to-randomness one. 
%
%
%More specifically, in \cite[Theorem~4.10]{SupplInfo}, one can show that once certain sufficient conditions for the compressibility of objects, multidimensional spaces, and perturbations (e.g., noise) are met, one-way communication to the receiver becomes possible even if no prior knowledge about the encoding-decoding scheme chosen by the emitter is known.
%One can first simply look into the landscape of all possible partitions and low-complexity noise the receiver has access to while assuming no bias toward particular objects and partitions in this landscape. 
%Then, despite having no knowledge about the encoding-decoding schemes and thus a lack of biases, one can infer with probability as high as one wishes that the message with the lowest algorithmic complexity is arbitrarily more likely to be correct than any other arbitrary computable method for assigning probabilities to message candidates.
%
%
%One of the main conditions for this to hold is that the algorithmic complexity of the object (with or without noise) in the wrong multidimensional space should increase (i.e., move the message toward algorithmic randomness) sufficiently fast in comparison to the algorithmic complexity of the message in a correct multidimensional space.
%%In consonance with the discussion in Section~\ref{sectionCompressionandrandomness} and in addition to the assumption (see Lemma~\ref{thm1}) of relatively low algorithmic complexity of the multidimensional space with respect to that of the object, 
%To this end, one can assume a message should contain a large amount of redundancies, and thus should be highly compressible with respect to its linear signal stream length $ \left| y \right| $, such as the case in which $ \mathbf{K}\left( y \middle\vert \mathcal{S} \right) = \mathbf{O}\left( \log\log\left( \left| y \right| \right) \right) $ holds.
%In this manner, most low-complexity perturbations will transform the object and/or partition by sufficiently moving it toward algorithmic randomness so that the conditions of \cite[Theorem~4.10]{SupplInfo} are satisfied.
%%This is because we know from AIT that the number of low-complexity perturbations for which $ \mathbf{K}\left( y' , \mathcal{ S }' \right) 
%%= 
%%\bm{ \omega }\big( \log\log\left( \left| y \right| \right) \big) $ holds strongly dominates the number of those for which one has it that $ \mathbf{K}\left( y' , \mathcal{ S }' \right) 
%%= 
%%\mathbf{O}\left( \log\log\left( \left| y \right| \right) \right) $.
%
%
%%% From Section~\ref{sectionIntroanddefinitions} in \cite{SupplInfo}
%Our theoretical results in \cite{SupplInfo} show that the method employed to achieve the empirical findings in Section~\ref{sectionResults} is sound with regard to the infinite asymptotic limit when computational resources are unbounded; and it also shows that the method can be generalised to multidimensional spaces in addition to bi- or tridimensional ones.
%These empirical results suggest that AID is capable of harnessing (given the availability of adequate computational resources) some of the algorithmic-informational properties that we demonstrated in ideal and generalised scenarios.
%
%As shown in \cite{algodyn,algodyn2}, AID allows the investigation of causal analysis \cite{maininfo,Zenil2019b} and solution of inverse problems \cite{nmi} by taking advantage of a high convergence rate to the algorithmic probability brought along by BDM, probability values which remain stable across distinct models of computation, most prominently for high algorithmically probable objects.
%%%%
%In addition, although it may seem paradoxical at first glance---because one may initially think that the errors produced by the methods for approximating the algorithmic complexity would be inherited by the perturbation analysis phase itself---our theoretical and empirical results show that the perturbation analysis enables one to overcome the semi-computability limitation in algorithmic information theory, but only when, looking at the whole landscape of complexity values, one estimates with a sufficiently large amount of perturbations effected on the received signal.
%This is because the algorithmic complexities of the messages resulting from the perturbations eventually become sufficiently larger than the complexity of the original message.
%Hence, a powerful enough method for approximating algorithmic complexity, such as BDM, can harness this divergence in order to enable the perturbation analysis to statistically distinguish the correct one (i.e., more compressible) from the wrong ones (i.e., more random).
%That is, an approximating method with enough computational resources can produce smaller errors than the actual difference of complexity values between the correct candidate and the perturbed ones, even though the exact algorithmic complexity values remain ultimately unknown to the perturbation analyst (i.e., even though the analyst itself can never be sure it has enough computational resources in the first place).
%Further discussion of these and other topics can be found in \cite[Section~$5$]{SupplInfo}.



\bmhead{Acknowledgements}


Felipe S. Abrah\~{a}o acknowledges support from the SÃ£o Paulo Research Foundation (FAPESP), grants $2021$/$14501$-$8$ and $2023$/$05593$-$1$.

\bmhead{Author Contributions:}
%\section*{Author Contributions:}
 HZ conceived the theory, methods and experiments; HZ performed most of the experiments, with support from AA. HZ and FA developed the theoretical framework.

%\newpage

\bibliographystyle{plainnat}
\bibliography{references.bib}
%\printbibliography[title={References}]

%\begin{thebibliography}{47}
%\providecommand{\natexlab}[1]{#1}
%\providecommand{\url}[1]{\texttt{#1}}
%\expandafter\ifx\csname urlstyle\endcsname\relax
%  \providecommand{\doi}[1]{doi: #1}\else
%  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi
%
%\bibitem[Abrah{\~{a}}o et~al.(2023)Abrah{\~{a}}o, Zenil, Porto, Winter,
%  Wehmuth, and D'Ottaviano]{Abrahao2021darxiv}
%Felipe~S. Abrah{\~{a}}o, Hector Zenil, Fabio Porto, Michael Winter, Klaus
%  Wehmuth, and Itala M.~L. D'Ottaviano.
%\newblock A simplicity bubble problem in formal-theoretic learning systems.
%\newblock \emph{arXiv Preprints}, arXiv:2112.12275 [cs.IT], 2023.
%\newblock URL \url{https://doi.org/10.48550/arXiv.2112.12275}.
%
%\bibitem[AbrahÃ£o and Zenil(2022)]{Abrahao2021b}
%Felipe~S. AbrahÃ£o and Hector Zenil.
%\newblock Emergence and algorithmic information dynamics of systems and
%  observers.
%\newblock \emph{Philosophical Transactions of the Royal Society A:
%  Mathematical, Physical and Engineering Sciences}, 380\penalty0 (2227), 2022.
%\newblock ISSN 1364-503X.
%\newblock \doi{10.1098/rsta.2020.0429}.
%
%\bibitem[AbrahÃ£o et~al.(2021{\natexlab{a}})AbrahÃ£o, Wehmuth, Zenil, and
%  Ziviani]{Abrahao2020c}
%Felipe~S. AbrahÃ£o, Klaus Wehmuth, Hector Zenil, and Artur Ziviani.
%\newblock An {Algorithmic} {Information} {Distortion} in {Multidimensional}
%  {Networks}.
%\newblock In Rosa~M. Benito, Chantal Cherifi, Hocine Cherifi, Esteban Moro,
%  Luis~Mateus Rocha, and Marta Sales-Pardo, editors, \emph{Complex {Networks}
%  \& {Their} {Applications} {IX}}, volume 944 of \emph{Studies in Computational
%  Intelligence}, pages 520--531, Cham, 2021{\natexlab{a}}. Springer
%  International Publishing.
%\newblock ISBN 978-3-030-65351-4.
%\newblock \doi{10.1007/978-3-030-65351-4_42}.
%
%\bibitem[AbrahÃ£o et~al.(2021{\natexlab{b}})AbrahÃ£o, Wehmuth, Zenil, and
%  Ziviani]{Abrahao2021}
%Felipe~S. AbrahÃ£o, Klaus Wehmuth, Hector Zenil, and Artur Ziviani.
%\newblock Algorithmic information distortions in node-aligned and
%  node-unaligned multidimensional networks.
%\newblock \emph{Entropy}, 23\penalty0 (7), 2021{\natexlab{b}}.
%\newblock ISSN 1099-4300.
%\newblock \doi{10.3390/e23070835}.
%
%\bibitem[Allender et~al.(2023)Allender, Hirahara, and
%  Tirumala]{Allender2023AITstatisticalZK}
%Eric Allender, Shuichi Hirahara, and Harsha Tirumala.
%\newblock {Kolmogorov Complexity Characterizes Statistical Zero Knowledge}.
%\newblock In Yael Tauman~Kalai, editor, \emph{14th Innovations in Theoretical
%  Computer Science Conference (ITCS 2023)}, volume 251 of \emph{Leibniz
%  International Proceedings in Informatics (LIPIcs)}, pages 3:1--3:19,
%  Dagstuhl, Germany, 2023. Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r
%  Informatik.
%\newblock ISBN 978-3-95977-263-1.
%\newblock \doi{10.4230/LIPIcs.ITCS.2023.3}.
%
%\bibitem[Bedau et~al.(2000)Bedau, McCaskill, Packard, Rasmussen, Adami, Green,
%  Ikegami, Kaneko, and Ray]{bedauOpenProblemsArtificial2000}
%Mark~A Bedau, John~S McCaskill, Norman~H Packard, Steen Rasmussen, Chris Adami,
%  David~G Green, Takashi Ikegami, Kunihiko Kaneko, and Thomas~S Ray.
%\newblock Open problems in artificial life.
%\newblock 6\penalty0 (4):\penalty0 363--376, 2000.
%
%\bibitem[Botta et~al.(2008)Botta, Bada, Gomez-Elvira, Javaux, Selsis, and
%  Summons]{bottaStrategiesLifeDetection2008}
%Oliver Botta, Jeffrey~L. Bada, Javier Gomez-Elvira, Emmanuelle Javaux, Franck
%  Selsis, and Roger Summons, editors.
%\newblock \emph{Strategies of {{Life Detection}}}, volume~25 of \emph{Space
%  {{Sciences Series}} of {{ISSI}}}.
%\newblock {Springer US}, 2008.
%\newblock ISBN 978-0-387-77515-9 978-0-387-77516-6.
%\newblock \doi{10.1007/978-0-387-77516-6}.
%
%\bibitem[Buchanan(2022)]{Buchanan2022Cryptographybook}
%William Buchanan.
%\newblock \emph{Cryptography}.
%\newblock River Publishers, New York, 1 edition, September 2022.
%\newblock ISBN 978-1-00-333775-1.
%\newblock \doi{10.1201/9781003337751}.
%
%\bibitem[Calude(2002)]{Calude2002}
%Cristian~S. Calude.
%\newblock \emph{{Information and Randomness: An algorithmic perspective}}.
%\newblock Springer-Verlag, 2 edition, 2002.
%\newblock ISBN 3540434666.
%
%\bibitem[Calude and Longo(2017)]{Calude2017}
%Cristian~S. Calude and Giuseppe Longo.
%\newblock The {Deluge} of {Spurious} {Correlations} in {Big} {Data}.
%\newblock \emph{Foundations of Science}, 22\penalty0 (3):\penalty0 595--612,
%  September 2017.
%\newblock ISSN 1233-1821, 1572-8471.
%\newblock \doi{10.1007/s10699-016-9489-4}.
%
%\bibitem[Cleland and Chyba(2002)]{clelandDefiningLife2002}
%Carol~E. Cleland and Christopher~F. Chyba.
%\newblock Defining life.
%\newblock 32\penalty0 (4):\penalty0 387--393, 2002.
%
%\bibitem[Cover and Thomas(2005)]{Cover2005}
%Thomas~M. Cover and Joy~A. Thomas.
%\newblock \emph{{Elements of Information Theory}}.
%\newblock John Wiley {\&} Sons, Inc., Hoboken, NJ, USA, sep 2005.
%\newblock ISBN 9780471241959.
%\newblock \doi{10.1002/047174882X}.
%
%\bibitem[Downey and Hirschfeldt(2010)]{Downey2010}
%Rodney~G. Downey and Denis~R. Hirschfeldt.
%\newblock \emph{{Algorithmic Randomness and Complexity}}.
%\newblock Theory and Applications of Computability. Springer New York, New
%  York, NY, 2010.
%\newblock ISBN 978-0-387-95567-4.
%\newblock \doi{10.1007/978-0-387-68441-3}.
%
%\bibitem[DuprÃ©(2021)]{dupreMetaphysicsBiology2021}
%John DuprÃ©.
%\newblock \emph{The {{Metaphysics}} of {{Biology}}}.
%\newblock Cambridge University Press, 2021.
%\newblock \doi{10.1017/9781009024297}.
%
%\bibitem[Enya et~al.(2022)Enya, Yamagishi, Kobayashi, and
%  Yoshimura]{enyaComparativeStudyMethods2022}
%Keigo Enya, Akihiko Yamagishi, Kensei Kobayashi, and Yoshitaka Yoshimura.
%\newblock Comparative study of methods for detecting extraterrestrial life in
%  exploration mission of {{Mars}} and the solar system.
%\newblock 34:\penalty0 53--67, 2022.
%\newblock ISSN 2214-5524.
%\newblock \doi{10.1016/j.lssr.2022.07.001}.
%
%\bibitem[Friday et~al.()Friday, 22, and {2022}]{fridayHowJamesWebb}
%Chris Holt |~Published: Friday, April 22, and {2022}.
%\newblock How the {{James Webb Space Telescope}} will search for
%  extraterrestrial l.
%\newblock URL
%  \url{https://astronomy.com/news/2022/04/how-the-james-webb-space-telescope-will-search-for-extraterrestrial-life}.
%
%\bibitem[Haqq-Misra et~al.(2022{\natexlab{a}})Haqq-Misra, Kopparapu, Fauchez,
%  Frank, Wright, and
%  Lingam]{haqq-misraDetectabilityChlorofluorocarbonsAtmospheres2022}
%Jacob Haqq-Misra, Ravi Kopparapu, Thomas~J. Fauchez, Adam Frank, Jason~T.
%  Wright, and Manasvi Lingam.
%\newblock Detectability of {{Chlorofluorocarbons}} in the {{Atmospheres}} of
%  {{Habitable M-dwarf Planets}}.
%\newblock 3\penalty0 (3):\penalty0 60, 2022{\natexlab{a}}.
%\newblock ISSN 2632-3338.
%\newblock \doi{10.3847/PSJ/ac5404}.
%
%\bibitem[Haqq-Misra et~al.(2022{\natexlab{b}})Haqq-Misra, Schwieterman,
%  Socas-Navarro, Kopparapu, Angerhausen, Beatty, Berdyugina, Felton, Sharma,
%  De~la Torre, and Apai]{haqq-misraSearchingTechnosignaturesExoplanetary2022a}
%Jacob Haqq-Misra, Edward~W. Schwieterman, Hector Socas-Navarro, Ravi Kopparapu,
%  Daniel Angerhausen, Thomas~G. Beatty, Svetlana Berdyugina, Ryan Felton,
%  Siddhant Sharma, Gabriel~G. De~la Torre, and DÃ¡niel Apai.
%\newblock Searching for technosignatures in exoplanetary systems with current
%  and future missions.
%\newblock 198:\penalty0 194--207, 2022{\natexlab{b}}.
%\newblock ISSN 0094-5765.
%\newblock \doi{10.1016/j.actaastro.2022.05.040}.
%
%\bibitem[Kempes and Krakauer(2021)]{kempesMultiplePathsMultiple2021}
%Christopher~P. Kempes and David~C. Krakauer.
%\newblock The {{Multiple Paths}} to {{Multiple Life}}.
%\newblock 89\penalty0 (7):\penalty0 415--426, 2021.
%\newblock ISSN 1432-1432.
%\newblock \doi{10.1007/s00239-021-10016-2}.
%
%\bibitem[Kiang(2014)]{kiangPhotosynthesisAstrobiologyLooking2014}
%Nancy Kiang.
%\newblock Photosynthesis and astrobiology: {{Looking}} for life elsewhere.
%\newblock 36:\penalty0 24--30, 2014.
%\newblock \doi{10.1042/BIO03606024}.
%
%\bibitem[Kirchherr et~al.(1997)Kirchherr, Li, and VitÃ¡nyi]{miracle}
%W.~Kirchherr, M.~Li, and Paul VitÃ¡nyi.
%\newblock The miraculous universal distribution.
%\newblock 19:\penalty0 7â€“15, 1997.
%
%\bibitem[Mariscal et~al.(2019)Mariscal, Barahona, Aubert-Kato, Aydinoglu,
%  Bartlett, CÃ¡rdenas, Chandru, Cleland, Cocanougher, Comfort, Cornish-Bowden,
%  Deacon, Froese, Giovannelli, Hernlund, Hut, Kimura, Maurel, Merino, Moreno,
%  Nakagawa, PeretÃ³, Virgo, Witkowski, and
%  James~Cleaves]{mariscalHiddenConceptsHistory2019}
%Carlos Mariscal, Ana Barahona, Nathanael Aubert-Kato, Arsev~Umur Aydinoglu,
%  Stuart Bartlett, MarÃ­a~Luz CÃ¡rdenas, Kuhan Chandru, Carol Cleland,
%  Benjamin~T. Cocanougher, Nathaniel Comfort, Athel Cornish-Bowden, Terrence
%  Deacon, Tom Froese, Donato Giovannelli, John Hernlund, Piet Hut, Jun Kimura,
%  Marie-Christine Maurel, Nancy Merino, Alvaro Moreno, Mayuko Nakagawa, Juli
%  PeretÃ³, Nathaniel Virgo, Olaf Witkowski, and H.~James~Cleaves.
%\newblock Hidden {{Concepts}} in the {{History}} and {{Philosophy}} of
%  {{Origins-of-Life Studies}}: A {{Workshop Report}}.
%\newblock 49\penalty0 (3):\penalty0 111--145, 2019.
%\newblock ISSN 1573-0875.
%\newblock \doi{10.1007/s11084-019-09580-x}.
%
%\bibitem[Meadows et~al.(2009)Meadows, Claire, Domagal-Goldman, Allen, Anbar,
%  Barnes, Boss, Kasting, Kiang, Martin-Torres, Robinson, Sleep, Som, Sparks,
%  and Williams]{meadowsSearchHabitableEnvironments2009}
%Victoria~S. Meadows, Mark~W. Claire, Shawn~D. Domagal-Goldman, Mark Allen,
%  Ariel~D. Anbar, Rory Barnes, Alan Boss, Jim Kasting, Nancy Kiang, Javier
%  Martin-Torres, Tyler~D. Robinson, Norm Sleep, Sanjoy Som, William Sparks, and
%  Darren~M. Williams.
%\newblock The {{Search}} for {{Habitable Environments}} and {{Life}} in the
%  {{Universe}}.
%\newblock 2010:\penalty0 201, 2009.
%\newblock URL \url{https://ui.adsabs.harvard.edu/abs/2009astro2010S.201M}.
%
%\bibitem[{National Academies of Sciences, Engineering, and Medicine}
%  et~al.(2018){National Academies of Sciences, Engineering, and Medicine},
%  {Division on Engineering and Physical Sciences}, {Space Studies Board}, and
%  {Committee on Astrobiology Science Strategy for the Search for Life in the
%  Universe}]{nationalacademiesofsciencesengineeringandmedicineAstrobiologyStrategySearch2018}
%{National Academies of Sciences, Engineering, and Medicine}, {Division on
%  Engineering and Physical Sciences}, {Space Studies Board}, and {Committee on
%  Astrobiology Science Strategy for the Search for Life in the Universe}.
%\newblock \emph{An {{Astrobiology Strategy}} for the {{Search}} for {{Life}} in
%  the {{Universe}}}.
%\newblock {National Academies Press (US)}, 2018.
%\newblock URL \url{http://www.ncbi.nlm.nih.gov/books/NBK540091/}.
%
%\bibitem[Neveu et~al.(2018)Neveu, Hays, Voytek, New, and
%  Schulte]{neveuLadderLifeDetection2018}
%Marc Neveu, Lindsay~E. Hays, Mary~A. Voytek, Michael~H. New, and Mitchell~D.
%  Schulte.
%\newblock The {{Ladder}} of {{Life Detection}}.
%\newblock 18\penalty0 (11):\penalty0 1375--1402, 2018.
%\newblock ISSN 1531-1074.
%\newblock \doi{10.1089/ast.2017.1773}.
%
%\bibitem[Rojo(2022)]{rojoCenterLifeDetection2022}
%Sara Rojo.
%\newblock Center for life detection (cld) â€“ research and service, 2022.
%\newblock URL \url{http://www.nasa.gov/ames/cld}.
%
%\bibitem[Ruiz-Mirazo et~al.(2004)Ruiz-Mirazo, PeretÃ³, and
%  Moreno]{ruiz-mirazoUniversalDefinitionLife2004}
%Kepa Ruiz-Mirazo, Juli PeretÃ³, and Alvaro Moreno.
%\newblock A universal definition of life: Autonomy and open-ended evolution.
%\newblock 34\penalty0 (3):\penalty0 323--346, 2004.
%\newblock \doi{10.1023/B:ORIG.0000016440.53346.dc}.
%
%\bibitem[{Salil Vadhan}(2023)]{Vadhan2023surveyonZKproofs}
%{Salil Vadhan}.
%\newblock \emph{A study of statistical zero-knowledge proofs}.
%\newblock Springer, Berlin, 2023.
%\newblock ISBN 978-3-540-71373-9.
%\newblock URL \url{https://link.springer.com/book/9783540713739}.
%
%\bibitem[Smith(2020)]{Smith2020}
%Gary Smith.
%\newblock The paradox of big data.
%\newblock \emph{SN Applied Sciences}, 2\penalty0 (6):\penalty0 1041, 2020.
%\newblock ISSN 2523-3963, 2523-3971.
%\newblock \doi{10.1007/s42452-020-2862-5}.
%
%\bibitem[Uthamacumaran et~al.(2022)Uthamacumaran, S.~AbrahÃ£o, A.~Kiani, and
%  Zenil]{abizenil}
%Abicumaran Uthamacumaran, Felipe S.~AbrahÃ£o, Narsis A.~Kiani, and Hector
%  Zenil.
%\newblock On the salient limitations of the methods of assembly theory and
%  their classification of molecular biosignatures.
%\newblock \emph{arXiv Preprints}, arXiv:2210.00901 [cs.IT], 2022.
%\newblock URL \url{https://doi.org/10.48550/arXiv.2210.00901}.
%
%\bibitem[Walker and Davies(2013)]{walkerAlgorithmicOriginsLife2013}
%Sara~Imari Walker and Paul~CW Davies.
%\newblock The algorithmic origins of life.
%\newblock 10\penalty0 (79):\penalty0 20120869, 2013.
%
%\bibitem[Witzany(2020)]{witzanyWhatLife2020}
%Guenther Witzany.
%\newblock What is {{Life}}?
%\newblock Frontiers in Astronomy and Space Sciences, 7, 2020.
%
%\bibitem[Wright et~al.(2022)Wright, Haqq-Misra, Frank, Kopparapu, Lingam, and
%  Sheikh]{wrightCaseTechnosignaturesWhy2022}
%Jason~T. Wright, Jacob Haqq-Misra, Adam Frank, Ravi Kopparapu, Manasvi Lingam,
%  and Sofia~Z. Sheikh.
%\newblock The {{Case}} for {{Technosignatures}}: {{Why They May Be Abundant}},
%  {{Long-lived}}, {{Highly Detectable}}, and {{Unambiguous}}.
%\newblock 927\penalty0 (2):\penalty0 L30, 2022.
%\newblock ISSN 2041-8205.
%\newblock \doi{10.3847/2041-8213/ac5824}.
%
%\bibitem[Zenil(2020)]{zenilreview2020}
%Hector Zenil.
%\newblock A review of methods for estimating algorithmic complexity: Options,
%  challenges, and new directions.
%\newblock 22\penalty0 (612), 2020.
%
%\bibitem[Zenil and Minary(2019)]{nar}
%Hector Zenil and Peter Minary.
%\newblock Training-free measures based on algorithmic probability identify high
%  nucleosome occupancy in dna sequences.
%\newblock gkz750, 2019.
%
%\bibitem[Zenil et~al.(2012)Zenil, Gershenson, Marshall, and
%  Rosenblueth]{zenillife}
%Hector Zenil, Carlos Gershenson, James Marshall, and David Rosenblueth.
%\newblock Life as thermodynamic evidence of algorithmic structure in natural
%  environments.
%\newblock 14\penalty0 (11), 2012.
%
%\bibitem[Zenil et~al.(2013)Zenil, Delahaye, and Gaucherel]{zengauch}
%Hector Zenil, Jean-Paul Delahaye, and Cedric Gaucherel.
%\newblock Image information content characterization and classification by
%  physical complexity.
%\newblock 17\penalty0 (3):\penalty0 26--42, 2013.
%
%\bibitem[Zenil et~al.(2015)Zenil, Soler-Toscano, Delahaye, and
%  Gauvrit]{kolmo2d}
%Hector Zenil, Fernando Soler-Toscano, Jean-Paul Delahaye, and Nicolas Gauvrit.
%\newblock Two-dimensional {{Kolmogorov}} complexity and an empirical validation
%  of the coding theorem method by compressibility.
%\newblock 1, 2015.
%
%\bibitem[Zenil et~al.(2017)Zenil, Kiani, and Tegn{\'{e}}r]{zkpaper}
%Hector Zenil, Narsis~A. Kiani, and Jesper Tegn{\'{e}}r.
%\newblock {Low-algorithmic-complexity entropy-deceiving graphs}.
%\newblock \emph{Physical Review E}, 96\penalty0 (1):\penalty0 012308, jul 2017.
%\newblock ISSN 2470-0045.
%\newblock \doi{10.1103/PhysRevE.96.012308}.
%
%\bibitem[Zenil et~al.(2018)Zenil, HernÃ¡ndez-Orozco, Kiani, Soler-Toscano,
%  Rueda-Toicen, and TegnÃ©r]{bdmpaper}
%Hector Zenil, Santiago HernÃ¡ndez-Orozco, Narsis~A. Kiani, Fernando
%  Soler-Toscano, Antonio Rueda-Toicen, and Jesper TegnÃ©r.
%\newblock A {{Decomposition Method}} for {{Global Evaluation}} of {{Shannon
%  Entropy}} and {{Local Estimations}} of {{Algorithmic Complexity}}.
%\newblock 20\penalty0 (8):\penalty0 605, 2018.
%\newblock \doi{10.3390/e20080605}.
%
%\bibitem[Zenil et~al.(2019{\natexlab{a}})Zenil, Kiani, Marabita, Deng, Elias,
%  Schmidt, Ball, and Tegner]{maininfo}
%Hector Zenil, Narsis Kiani, Francesco Marabita, Yue Deng, Szabolcs Elias,
%  Angelika Schmidt, Gordon Ball, and Jesper Tegner.
%\newblock An {{Algorithmic Information Calculus}} for {{Causal Discovery}} and
%  {{Reprogramming Systems}}.
%\newblock 19, 2019{\natexlab{a}}.
%\newblock \doi{10.1016/j.isci.2019.07.043}.
%
%\bibitem[Zenil et~al.(2019{\natexlab{b}})Zenil, Kiani, and
%  Tegn{\'{e}}r]{Zenil2019b}
%Hector Zenil, Narsis~A. Kiani, and Jesper Tegn{\'{e}}r.
%\newblock {The Thermodynamics of Network Coding, and an Algorithmic Refinement
%  of the Principle of Maximum Entropy}.
%\newblock \emph{Entropy}, 21\penalty0 (6):\penalty0 560, jun
%  2019{\natexlab{b}}.
%\newblock ISSN 1099-4300.
%\newblock \doi{10.3390/e21060560}.
%
%\bibitem[Zenil et~al.(2019{\natexlab{c}})Zenil, Kiani, Zea, and
%  Tegn{\'{e}}r]{nmi}
%Hector Zenil, Narsis~A. Kiani, Allan~A. Zea, and Jesper Tegn{\'{e}}r.
%\newblock {Causal deconvolution by algorithmic generative models}.
%\newblock \emph{Nature Machine Intelligence}, 1\penalty0 (1):\penalty0 58--66,
%  jan 2019{\natexlab{c}}.
%\newblock ISSN 2522-5839.
%\newblock \doi{10.1038/s42256-018-0005-0}.
%
%\bibitem[Zenil et~al.(2020)Zenil, Kiani, AbrahÃ£o, and TegnÃ©r]{algodyn}
%Hector Zenil, Narsis~A. Kiani, Felipe~S AbrahÃ£o, and Jesper~N. TegnÃ©r.
%\newblock {A}lgorithmic {I}nformation {D}ynamics.
%\newblock \emph{Scholarpedia}, 15\penalty0 (7):\penalty0 53143, 2020.
%\newblock \doi{10.4249/scholarpedia.53143}.
%
%\bibitem[Zenil et~al.(2022)Zenil, Kiani, Adams, Abrah{\~{a}}o, Rueda-Toicen,
%  Zea, and Tegn{\'{e}}r]{Zenil2019c}
%Hector Zenil, Narsis~A. Kiani, Alyssa Adams, Felipe~S. Abrah{\~{a}}o, Antonio
%  Rueda-Toicen, Allan~A. Zea, and Jesper Tegn{\'{e}}r.
%\newblock {Minimal Algorithmic Information Loss Methods for Dimension
%  Reduction, Feature Selection and Network Sparsification}.
%\newblock \emph{arXiv Preprints}, arXiv:1802.05843 [cs.DS], 2022.
%\newblock URL \url{https://doi.org/10.48550/arXiv.1802.05843}.
%
%%\bibitem[Zenil et~al.(2023{\natexlab{a}})Zenil, Adams, and
%%  Abrah{\~{a}}o]{SupplInfo}
%%Hector Zenil, Alyssa Adams, and Felipe~S. Abrah{\~{a}}o.
%%\newblock {Supplementary Information of the paper ``\textit{Optimal Spatial
%%  Deconvolution and Message Reconstruction from a Large Generative Model of
%%  Models}''}.
%%\newblock 2023{\natexlab{a}}.
%
%\bibitem[Zenil et~al.(2023{\natexlab{b}})Zenil, Kiani, and Tegner]{algodyn2}
%Hector Zenil, Narsis Kiani, and Jesper Tegner.
%\newblock \emph{{Algorithmic Information Dynamics}}.
%\newblock Cambridge University Press, 1 edition, 2023{\natexlab{b}}.
%\newblock ISBN 1108497667.
%
%\end{thebibliography}

%\newpage
%\appendix

%\newpage
\begin{appendices}


\section{Figures}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{newfig1.png}
	\caption{\label{sentences}Algorithmic perturbation analysis on short sentences of only 400 characters from Darwin's ``Origin of Species'' are converted from ASCII into their corresponding binary values. Changing a single character with probability $1/n$ for growing $n$ from $n=1$ (original test, axis $x=0$) to $n=4$ (a quarter of the text is mapped to a random character from the same vocabulary of the original text, axis $x=100$ letters changed or perturbed) causes the resulting size of the compressed sequence index to increase, with BDM the most sensitive and Shannon entropy unable to capture any difference. This is because signals carrying meaning are far removed from randomness, and random perturbations make the text more random \cite{Zenil2019c} even when the methods know nothing about words, grammar or anything linguistic.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.37]{sound.png}
	\vspace{0.5cm}
	
	\centering
	\includegraphics[scale=0.28]{soundhist.png}\hspace{0.3cm}\includegraphics[scale=0.31]{plot.png}
	\caption{\label{sound}\textit{Top:} Sound waves of the words spoken from Apollo 13, ``Houston, we've had a problem.'' transmitted on April 13, 1970 at a sample rate frequency of 11025 Hz on a single channel. \textit{Bottom left:} Histograms of the change in compression lengths of scrambled versions of the same message from the length of the original compressed message. \textit{Bottom right:} Small perturbations of the original message by scrambling only the beginning and end of each word shows a smooth transition from low to high randomness. The lowest complexity signal indicates the correct (original) signal. The file was processed in FLAC format (Free Lossless Audio Codec) from a lossless file, with no audio data discarded.}
\end{figure}

\begin{figure}[ht!]
	\centerline{\includegraphics[scale=0.20]{Fig1.png}
		\hspace{1cm}
		\includegraphics[scale=0.21]{Fig2.png}}
	\caption{\label{arecibo}\textit{Left:} The original Arecibo message intended to be reconstructed, but sent as a linear stream from the radiotelescope in Arecibo, Puerto Rico. The 1,679 bits are meant to be arranged into 23 columns of 73 rows, 23 and 73 being two prime numbers which when multiplied together equal 1,679. \textit{Right:} If the stream is instead arranged into 23 rows and 73 columns, the original visual interpretations of the message are scrambled, which may result in a figure that is closer to being statistically random. What we show is that the message is still there, concealed, and can be deciphered by algorithmic deconvolution.}
\end{figure} 



\begin{figure}[ht!]
	\centerline{\includegraphics[scale=0.44]{signaldeconv.png}}
	\caption{\label{arecibosequence}\textit{Top left:} Most possible partitions result in random-appearing configurations with high corresponding complexity, indicating measurable randomness. \textit{Bottom:} Some partitions will approximate the originally encoded meaning (third from the right). Other configurations result in images with higher complexity values. This sequence of images shows the images in the approximate vicinity of the correct bidimensional configuration (i.e., partition) and illustrates fast convergence to low complexity. \textit{Top right:} By using different information indexes across different configurations, a downward-pointing spike will indicate message (image) configurations that correspond to low-complexity image(s). This allows a prior-knowledge-agnostic and objective method to infer a message's original encoding. Of the various measures, BDM, combining classical information (entropy) for long ranges and a measure motivated by algorithmic probability for short ranges, is the most sensitive and accurate in this regard. Traditional compression and entropy also indicate the right configuration amongst the top spiking candidates. The ratio of noise-to-signal was amplified in favour of the hidden structure by multiplying the original image size by 6 for both length and height.}
\end{figure} 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.30]{additivenoise.png}
	\caption{\label{noise}The method's resilience in the face of some noise. At 3\% of the bits of the original $23 \times 73=1679$-pixel image randomly flipped (which means about 1.5\% were binary negated), the method remains sensitive and displays a small downward spike at the 23 value, uncovering its length, but the signal gets lost when more bits are flipped.}
\end{figure}


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.23]{noisedeconv.png}\hspace{.8cm}\includegraphics[scale=0.23]{amplifiedconvnoise.png}
	\caption{\label{ampnoise}Applying 3 different quantitative methods, it is shown (left) that they are highly insensitive to signal and highly sensitive to noise (16.5\% of pixels randomly flipped). However, by growing the original image (right) by a factor of 6 on each dimension (i.e. 1 pixel becomes $6\times6$), the methods are less sensitive to noise and more sensitive to signal, with BDM significantly outperforming Compression, and Shannon Entropy showing sensitivity at up to 60\% pixels flipped (hence about 30\% of the original image) versus Compression and Shannon Entropy, that are about 50\%
 sensitive. Downward spikes (right) are shown at $23 \times 6 = 141$.}
\end{figure}

\begin{figure}[H]
	\centerline{\hspace{.8cm}\includegraphics[scale=0.55]{Fig6.png}}
	
	\begin{center}
		196 \hspace{3.8cm} 82 \hspace{4cm} 215
	\end{center}
	
	\centerline{\includegraphics[scale=0.27]{Fig7.png}}
	
	\vspace{0.5cm}
	
	\centerline{\hspace{.8cm}\includegraphics[scale=0.37]{Fig8.png}}
	
	\begin{center}
		{276 \hspace{3.6cm} 360 \hspace{3.6cm} 220}
	\end{center}
	
	\centerline{\includegraphics[scale=0.28]{Fig9.png}}
	\caption{\label{moreexps}Six 2D images (labelled 1 through 6 going from left to right, top to bottom) of very different nature, including a demonstration of linear-transformation invariance conforming to the underlying theory (in this case, rotation of a mathematical formula). Size invariance has actually shown amplification of signal-to-noise difference. Under each image is its correct numerical (first) dimension. The values of the three complexity indexes over possible partitions are shown below the original numerical dimension for ease of comparison. Downward spikes indicate candidates for possible original partitions. In all cases, the correct dimension value is among the top three candidates, with BDM outperforming in 4 out of 6 cases at indicating the top candidate.}
\end{figure}

\begin{figure}[ht]
\centerline{\includegraphics[scale=0.28]{Fig10.png}}
	\caption{\label{seqs}Sequence of lowest complexity (BDM value on top in bits) partitions of a black and white image of a space shuttle, with the top right partition as the correct reconstruction and other candidates cases in which an alignment occurs at some multiple of the correct dimension.}
\end{figure}






\begin{figure}[H]
	
	\flushleft\footnotesize{\hspace{2.4cm}500$\times$500}
	\centerline{\includegraphics[scale=0.28]{Fig15.png}\includegraphics[scale=0.25]{Fig16.png}}
	\flushleft\footnotesize{\hspace{6cm}750$\times$250}
	\centerline{\includegraphics[scale=0.50]{Fig29.png}}
	\caption{\label{mandril}\textit{Top Left:} An original 2D image message and \textit{top right:} the complexity results of its various image reconstructions via linear signal decomposition (in binary, similar to how the Arecibo message was sent) into distinct partitions. BDM spikes prominently at the correct right partition with bidimensional configuration of 500 $\times$ 500 pixels, contrasting with a mirror-like image at 250 $\times$ 750 pixels (\textit{bottom}, rotated 45 degrees counterclockwise).}
\end{figure}

\end{appendices}

\end{document}