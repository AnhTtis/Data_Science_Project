\documentclass[12pt,a4paper,sort&compress]{article}
\usepackage{amsmath, amssymb,amscd,amsthm, amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage[title]{appendix}

%% Packages added by Felipe
%\usepackage{todonotes}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{array}
\usepackage{authblk}
\usepackage{mathtools}
\usepackage{soul}
\usepackage[symbol]{footmisc}
\usepackage{bm}
\usepackage{caption}
\captionsetup{font=footnotesize}

%\usepackage{amsaddr}
%\oddsidemargin 0pt
%\evensidemargin 0pt
%\marginparwidth 40pt
%\marginparsep 10pt
%\topmargin -20pt
%\headsep 10pt
%\textheight 8.7in
%\textwidth 6.65in
%\linespread{1.2}

\newcommand{\fsa}[1]{\textcolor{blue}{\bf [Felipe]: #1}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\al}{\alpha}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\aff}{aff}


\title{\vspace{-3.5cm}Optimal Spatial Deconvolution and Message Reconstruction from a Large Generative Model of Models}
\author[1,2,3,4]{Hector Zenil\thanks{Corresponding author. Email: hector.zenil@cs.ox.ac.uk}}
\author[5,8]{Alyssa Adams}
\author[6,7]{Felipe S. Abrah\~{a}o}

%\footnotetext{$\dagger$ Email: a$\_$utham@live.concordia.ca}
%\footnotetext{$\ddagger$ Email: fsa@lncc.br}

\affil[1]{\footnotesize Machine Learning Group, Department of Chemical\\

Engineering and
	Biotechnology, University of Cambridge, U.K.}
\affil[2]{\footnotesize Oxford Immune Algorithmics, Reading, England, U.K.}
\affil[3]{\footnotesize Kellogg College, University of Oxford, U.K.}
\affil[4]{\footnotesize Algorithmic Dynamics Lab, Karolinska Institutet, Stockholm, Sweden.}
%\affil[5]{\footnotesize Algorithmic Nature Group, LABORES, France.}
\affil[5]{\footnotesize John W. and Jeanne M. Rowe Center for Research in Virology, 

Morgridge Institute for Research, University of Wisconsin–Madison, U.S.}
\affil[6]{\footnotesize Centre for Logic, Epistemology and the History of Science,
	
University of Campinas, Brazil.}
\affil[7]{\footnotesize DEXL, National Laboratory for Scientific Computing, Brazil.}
\affil[8]{\footnotesize Cross Labs, Cross Compass, Kyoto, Japan.}

\date{}

%%% End of Frontmatter with authblk package

%\title{On Assembly Theory and\\ the Molecular Complexity Classification\\ of Biosignatures}
%
%\author{Abicumaran Uthamacumaran$^{1,2}$, Felipe S. Abrah\~{a}o$^{3,7}$, and Hector Zenil$^{4,5,6,7}$\\
%$^{1}$Concordia University, Department of Physics (Alumni), Montreal, QC, Canada.\\
%$^{2}$ McGill University, McGill Genome Center, Montreal, QC, Canada.\\
%$^{3}$DEXL Lab, National Laboratory for Scientific Computing (LNCC), 25651-075,\\ Petropolis, RJ, Brazil.\\
%$^{4}$Alan Turing Institute, British Library, London, NW1 2DB, U.K..\\
%$^{5}$Oxford Immune Algorithmics, Reading, RG30 1EU, U.K..\\
%$^{6}$Algorithmic Dynamics Lab, Karolinska Institute, Stockholm, 171 77, Sweden.\\
%$^{7}$Algorithmic Nature Group, LABORES, Paris, 76006, France.
%}

\begin{document}

%% Mathematical environment added by Felipe:

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\theoremstyle{theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{claim}{Claim to prove}[section]

%%Possible journals:
%http://www.journals.elsevier.com/acta-astronautica/
%Information Sciences
%http://rsif.royalsocietypublishing.org/ %http://www.journals.elsevier.com/signal-processing/



\maketitle

\vspace{-1cm}
%Main question: Does information encode the native dimension of the object in any representation and what other properties? 

%\newpage 
\begin{abstract}
	We introduce a general-purpose univariate signal deconvolution method based on the principles of an approach to Artificial General Intelligence.  This approach is based on a generative model that combines information theory and algorithmic probability that required a large calculation of an estimation of a `universal distribution' to build a general-purpose model of models independent of probability distributions.  This was used to
	investigate how non-random data may encode information about the physical properties such as dimension and length scales in which a signal or message may have been originally encoded, embedded, or generated.
	This multidimensional space reconstruction method is based on information theory and algorithmic probability, and it is agnostic, but not independent, with respect to the chosen computable  or semi-computable approximation method or encoding-decoding scheme. 
	%	where computable classical statistical information approaches or estimations to semi-computable measures shed light.  
	%By reconstructing a signal in its original dimensions, any potential meaning, if any, can be interpreted as it originally intended.  
	The results presented in this paper are useful for applications in coding theory, particularly in zero-knowledge one-way communication channels, such as in deciphering messages sent by generating sources of unknown nature for which no prior knowledge is available. 
	We argue that this can have strong potential for cryptography, signal processing, causal deconvolution, life, and technosignature detection.\\
	
	\noindent \textit{Keywords:} Algorithmic information dynamics, causal deconvolution, intelligent signal detection, technosignatures, biosignatures, universal distribution, logical-depth, signal processing	
	
\end{abstract}


%Add:  Unlike Shannon’s, interested in an engineering problem related to the syntactic properties of communication that have driven many areas including SETI and on how to transform a received signal, distorted by the physical medium, to reconstruct the original as accurately as possible. We use a combination of entropy and algorithmic complexity and entropy alone to answer the question… 

%Shannon provided a formula for the maximum number of bits per second that can be reliably communicated in the face of noise, which he called the system’s capacity, C. This is the maximum rate at which the receiver can resolve the message’s uncertainty, effectively making it the speed limit for communication. The source Doesn’t have to be in binary 

\section{Introduction}\label{sectionIntro}

For the past 50 years, astronomers have been sweeping the skies using radio telescopes in the hopes of stumbling across a message from an alien civilisation. More recently, space missions to collect samples and data from Mars, Titan, Europa, and others have been focused on detecting possible biosignatures \cite{abizenil,kiangPhotosynthesisAstrobiologyLooking2014, meadowsSearchHabitableEnvironments2009, nationalacademiesofsciencesengineeringandmedicineAstrobiologyStrategySearch2018, rojoCenterLifeDetection2022}. The James Webb Space Telescope has been outfitted with sensors and measurement devices to scout potentially-habitable solar systems \cite{fridayHowJamesWebb, haqq-misraDetectabilityChlorofluorocarbonsAtmospheres2022}. For the first time in human history, life detection beyond Earth is at the forefront of space exploration. In the novel Contact written by Carl Sagan and later made into a film, an extraterrestrial signal is received that is encoded in three dimensions.  A good deal of the story goes into how scientists deduce that the signal encodes an object in three dimensions out of serendipity.  
Here we show that information away from randomness can convey its native physical dimension and that an encoding/decoding-agnostic methodological approach with no human intervention can derive it.

Some researchers worry that our current theoretical toolkit for data analysis may not be  sophisticated enough \cite{bottaStrategiesLifeDetection2008, enyaComparativeStudyMethods2022, neveuLadderLifeDetection2018}. 

%Our current signal processing analyses have been limited by what we know about life \textit{on Earth} in order to distinguish random noise from possible, meaningful messages or biosignatures from a distant, form of life \cite{smithFutilityExoplanetBiosignatures2022}. The true physical limits of what life can and cannot be are still largely unknown. 

All of our knowledge about life is based on what occurs here on Earth \cite{zenillife,bedauOpenProblemsArtificial2000, clelandDefiningLife2002, dupreMetaphysicsBiology2021, kempesMultiplePathsMultiple2021, mariscalHiddenConceptsHistory2019, ruiz-mirazoUniversalDefinitionLife2004, walkerAlgorithmicOriginsLife2013, witzanyWhatLife2020}. Some researchers consider technosignature detection as a more promising method of detecting life over other biosignatures because of their longevity \cite{zengauch,haqq-misraSearchingTechnosignaturesExoplanetary2022a, wrightCaseTechnosignaturesWhy2022}.
%
%While alleged universal grammars have been proposed \cite{berwickWhyOnlyUs2015}, they have been criticised heavily by linguists \cite{evansDiversityMindFreeing2009} because languages are so diverse that universality may not be possible. Regardless, our current understanding of language is based on human-centric rules of communication. Questions about universal features of written, spoken, non-verbal, and internal languages remain largely unanswered beyond human communication \cite{bybeeLanguageUsageCognition2010, goldbergConstructionsWorkNature2006, tomaselloConstructingLanguageUsageBased2003}. For example, is there an optimal mapping between meaningful features in the environment and the number of unique words? 
%
%Is it possible to quantify what is meant by a "meaningful features"? What is the relationship between an agent capable of communication and its ability to sense its environment and process information?
%Claims of extraterrestrial radio signals have been made several times since 1899, including some presumed to be coming from Mars. But it became increasingly accepted that nothing but signals from natural (non-living) mechanisms emissions were actually the generators of the signals detected.

%The discussion involving SETI and the signals it can detect goes back to the 1970s.  The first chapter or two of the Project Cyclops report and much of the discussion in NASA's SP419 are still relevant for today's open questions about extraterrestrial life detection.  

Most of the discussion has centred around the technical justification of the hardware choices and technicalities for detection (e.g. the frequency) and not about the nature of the signal itself. The search for extraterrestrial signals has, for example, mostly focused on narrow-band signals (mostly pulses) a few Hertz-wide (or less). 

Natural cosmic noisemakers, such as pulsars, quasars, and the turbulent, thin interstellar gas of our own Milky Way do not make radio signals that are this narrow, which is a reasonable justification of the bounds of the physical signal search. However, not much progress has been done on the qualitative semantic aspects of signal and communication beyond very basic mathematical statistical patterns, let alone building a universal framework in which the question can be explored and analysed. 

On Earth, we also find local signal-noise makers such as animals, organs, neurons, and cells. The tools here introduced can be useful not only for signal detection but also signal deconvolution from local information in, e.g. biology (what chemical signals among cells mean).  In previous work, we showed, for example, how the same technology can be used to disentangle the 3D structure of DNA and genomic information~\cite{nar}

%If a signal is intentional, it might be decipherable. In order to intentionally send or receive a signal over interstellar distances, it is reasonable to assume a civilisation must understand basic science and mathematics, or at least must evolve in such that these are somehow encoded into a message. Hence, a message from another civilisation might use some similar framework to science and mathematics to build up a common language with themselves and other societies. Signals sent by a civilisation for its own purposes may be impossible for us to unravel but our own mathematical tools may help.

%On earth, each different species has its own different set of sensors that it uses to interact with its environment. Based on the philosophical essay "What is it like to be a bat?" \cite{nagelWhatItBe1974a}, the recent book "An Immense World" \cite{yongImmenseWorldHow2022} explores the unique slices of experience that are native to many different species. Because each creature is outfitted with its own, unique sensory mechanisms and capabilities, Yong illustrates in great detail how different species experience the world differently. To put this in terms of signals or messages between senders and receivers, different species emit signals based on the mechanisms that allow them to do so. They also decode messages based on their sensory capabilities, and if each species has a unique set of senses, then we can say that each species decodes the same message differently. The 52Hz whale's messages are decoded differently between humans monitoring the oceans and other whales (who do not sense the messages at all).

There are myriad data types and formats that encode different types of data on multiple levels, from ASCII characters to video files to health record data. While it is possible to re-encode all data into a single format, the original message may be lost (imagine trying to extract voice-recorded messages of a sound file only by looking at the sound wave plot over time).

Because of this diversity even within our own technology, the process of detecting, decoding, and interpreting terrestrial or extraterrestrial signals \textit{must} involve considering a wide range of possible encoding and decoding schemes. 

For the present purposes, we say an object has ``meaning'' when the object conveyed in the received signal is actually embedded into (or grounded on) the multidimensional space (i.e., the context) that the emitter agent originally intended.
%The multidimensional space is completely determined (or specified) by the number of dimensions, the size of each dimension, and the ordering in which each dimension appears.
Each (finite and discrete) \emph{multidimensional space} $ \mathcal{ S } $---no matter how complex it is---can be univocally determined by the number of dimensions, the encoding of the set of elements for each dimension, and the ordering in which each dimension appears.
For example: 
the unidimensional space $ \mathcal{ S }_1 $ that takes values from the natural numbers can be univocally determined by only informing the length $ \left| \mathcal{ S }_1 \right| $ in $ \mathbf{O}\left( \log\left( \left| \mathcal{ S }_1 \right| \right) \right) $ bits; 
the bidimensional space $ \mathcal{ S }_2 $ by a pair $ \left( \left| {  \mathcal{ S }_2 }_x \right| ,  \left| { \mathcal{ S }_2 }_y \right| \right) $ in $ \mathbf{O}\left( \log\left( \left| {  \mathcal{ S }_2 }_x \right| \right) \right) + \mathbf{O}\left( \log\left( \left| {  \mathcal{ S }_2 }_y \right| \right) \right) + \mathbf{O}\left( 1 \right) $ bits, 
where $ { \mathcal{ S }_2 }_x $ is the first dimension and $ { \mathcal{ S }_2 }_y $ is the second dimension; and so on.
In this manner,
one is always able to (uniquely) decode the received message into sufficient information for completely determining the multidimensional space configuration.
One example of encoding scheme for an arbitrary multidimensional space $ \mathcal{ S } $ is by encoding it in the form of a companion tuple $ \bm{ \tau } $ as in \cite{Abrahao2020c,Abrahao2021}.
Finite and discrete multidimensional spaces with more intricate configurations other than whole numbers multiplication in the form of $ m_1 \times m_2 \times \dots \times m_k $, where $ m,k \in \mathbb{N} $, can for example be encoded by node-unaligned companion tuples $ \bm{ \tau }_{ ua } $ as in \cite[Definition~2]{Abrahao2021}.
Nevertheless, as demonstrated in Section~\ref{sectionRetrievability}, the theoretical framework of our method is agnostic to any arbitrarily chosen computational scheme to encode multidimensional spaces.

Each \emph{partition} of a $n$-bit-length linear signal stream corresponds to a distinct configuration of dimensions' lengths, and therefore to a distinct multidimensional space in which the total additive dimensions' lengths remains upper bounded by the constant $ n $.
For example, a picture of 4$\times$4 pixels can be partitioned into other dimension-length configurations leading to different non-squared shapes, like 2$\times$8, 8$\times$2, or 1$\times$16.
Once a row (or, alternatively, a column) is fixed in a bidimensional space, the remaining rows and columns are dependent on the initial row fixation. 
In other words, each bidimensional partition is a single-variable dependency.
More formally, in the case of a (finite and discrete) bidimensional space $ \mathcal{ S }_2 $, 
%determined by the pair  $ \left( \left| {  \mathcal{ S }_2 }_x \right| ,  \left| { \mathcal{ S }_2 }_y \right| \right) $ as in Figs.~\ref{arecibo},~\ref{arecibosequence},~\ref{seqs},~\ref{moreexps},~\ref{mandril}
%where $ { \mathcal{ S }_2 }_x $ is the first dimension and $ { \mathcal{ S }_2 }_y $ is the second dimension,
one may have different combinations of values $ \left| {  \mathcal{ S }_2 }_x \right| $ and $ \left| {  \mathcal{ S }_2 }_y \right| $ for each partition.
However, for any partition one has it that both
%the values of $ \left| {  \mathcal{ S }_2 }_x \right| $ and $ \left| {  \mathcal{ S }_2 }_y \right| $ obey the inequalities
$
1 \leq \left| {  \mathcal{ S }_2 }_x \right| \leq n
$ and
$
\lfloor \frac{ n }{ \left| {  \mathcal{ S }_2 }_x \right| } \rfloor = \left| {  \mathcal{ S }_2 }_y \right|
$
%$
%\lfloor \frac{ n }{ \left| {  \mathcal{ S }_2 }_x \right| } \rfloor \leq \left| {  \mathcal{ S }_2 }_y \right| \leq \frac{ n }{ \left| {  \mathcal{ S }_2 }_x \right| }
%$ 
hold.
%and
%$
%\frac{ n }{ 2 } < \left| {  \mathcal{ S }_2 }_x \right| \times \left| {  \mathcal{ S }_2 }_y \right|
%\leq
%n
%$.
%Note once a row or column is fixed in a bidimensional space, the remaining rows and columns are dependent on the initial row fixation. 
%Thus, each $2$D-partition is a single-variable dependency.
Thus, because each partition is single-variable dependent on $ { \mathcal{ S }_2 }_x $, for any $n$-bit-length linear signal stream one can encode each of its $ 2 $D-partitions in $ \mathbf{O}\left( \log\left( n \right) \right) $ bits.
The same applies to $ 3 $D-partitions (or to any partition from a finite number of dimensions), except for an extra number of bits upper bounded by a partition-independent constant $ \mathbf{O}\left( 1 \right) $.


As formalised in \cite{Abrahao2021b} any transformation or perturbation of a finite (encoded) object into another finite (encoded) object is equivalent to an algorithmic transformation that takes the former as input and outputs the latter.
(See also Section~\ref{sectionRobustenesstoperturbations}).
Thus, a (algorithmic) perturbation with low algorithmic complexity is one of these transformations whose algorithmic information content of the original object is mostly preserved under such perturbations.
More formally, a \emph{low-complexity perturbation} occurs when $ \mathbf{K}\left( y \middle\vert x \right) = \mathbf{O}\left( \log\left( \left| x \right| \right) \right) $, where $ \mathbf{K}\left( \cdot \right) $ denotes the algorithmic complexity, $ x $ is the original encoded object, $ y $ is the encoded object that results from the perturbation on $ x $, and $ \left| x \right| $ is the size of the object $ x $.

Swapping \emph{all} 0's and 1's, or swapping \emph{all} black and white pixels in an image are examples of low-complexity perturbations (in these two cases in particular, one has it that $ \mathbf{K}\left( y \middle\vert x \right) = \mathbf{O}\left( 1 \right) $).
Randomly inserting or deleting a finite number of elements (such as creating or destroying edges in a graph or flipping a finite number of bits in a string) are also examples of low-complexity perturbations \cite{Abrahao2021b,Zenil2019c,algodyn}.
As we saw in the above paragraph, changing the partition is also a low-complexity perturbation of the multidimensional space into which the object $ x $ is embedded. 
This is because any new partition can always be encoded in $ \mathbf{O}\left( \log\left( \left| x \right| \right) \right) $ bits.
Thus, note that a perturbation can change either the object (e.g., by flipping bits) or its multidimensional space (e.g., by reconfiguring the dimensions' lengths of a given partition to achieve another distinct partition) independently, or can change both at the same time.
In any of these three options, the encoded form of the message resulting from the perturbation will be distinct from the encoded form of the original message.
Both changing the partition into which the object is embedded and introducing noise into the object itself (e.g., by flipping bits) are particular examples of (algorithmic) perturbations \cite{Abrahao2021b,Zenil2019c,algodyn}. 

In this paper, we introduce a methodology that is aimed at sweeping over various possible encoding and decoding schemes to test our current limits on signal interpretation as one reconstructs the original partition (i.e., multidimensional space) into which the original message was given ``meaning'' by the emitter agent.

Our method is based on the principles of algorithmic information dynamics (AID)~\cite{algodyn,nmi}, and it consists of a perturbation analysis of a received signal. AID is based upon the mechanisms of algorithmic probability and the universal distribution, a formal approach to Artificial General Intelligence that required the massive production of a universal distribution, the mother of all models~\cite{miracle}, to build a very large (semi-computable) model of computable approximative models.  The underlying idea is that a computable model, or computer program, is a causal explanation of a piece of data.  For each perturbation, a new computable approximative model is built and compared against the observation~\cite{nmi,maininfo}.
Equipped with AID, we estimated the algorithmic probability changes for a message under distinct partitions as perturbations. 
The partition (i.e., multidimensional space) for which the message displays the lowest complexity indicates the original partition that the received signal stream encodes.
For example in the case of bidimensional spaces, our method then consists of finding the bidimensional space $ \mathcal{ S }_2 $ for which the algorithmic complexity of the object embedded into $ \mathcal{ S }_2 $ (i.e., a message) is minimised.

In what follows, we demonstrate how to reconstruct signals and messages by deriving the number of dimensions and the scale of each length of an object from examples ranging from text to images embedded in multiple dimensions, showing a connection between irreducible information content, syntax, geometry, semantics, and topology.

See also Sections~\ref{sectionAIDistortions} and~\ref{sectionRetrievability} for a formalisation of the theoretical background of this method that applies to multidimensional spaces in general.


\subsection{Detecting technosignatures and decoding signals}

A killer application of a signal deconvolution method is bio and technosignature detection. Much of the analysis performed on message detection and biosignatures so far has been focused on chemical and molecular detection based on what we know about life here on Earth. For example, non-chemical signals from stars and other celestial bodies are compared to signals from typical stars to search for potential technosignatures \cite{sheikhAnalysisBreakthroughListen2021}. Incoming radio signals as detected by SETI programs have traditionally been  analysed using simple and purely statistical approaches to distinguish signals from background noise \cite{deansComputationalProblemsSignal1991}. 
We have shown shortcomings of statistical and entropy-based approaches before \cite{zenilreview2020,zkpaper,bdmpaper} demonstrating how their limitations are enough to rethink the mathematical framework and methods for signals detection and deconvolution carrying information both of human or extraterrestrial origin. 

Beyond Shannon's theory of information and its results in channel capacity and maximum syntactic compression, most communication research has focused on exploring the possible \emph{syntactic} nature of signals. This includes the type of equipment, carrier, bandwidth, and medium on which that communication may happen.
While alleged universal grammars have been proposed \cite{berwickWhyOnlyUs2015}, they have also been criticised heavily by linguists \cite{evansDiversityMindFreeing2009}. Our current understanding of language is based on human-centric rules of communication. Questions about universal features of written, spoken, non-verbal, and internal languages remain largely unanswered beyond human communication \cite{bybeeLanguageUsageCognition2010, goldbergConstructionsWorkNature2006, tomaselloConstructingLanguageUsageBased2003}. Human-centred language is most certainly not similar to how neurons in a brain, cells in an organ, non-human animals, or aliens may communicate with each other.
%For example, is there an optimal mapping between meaningful features in the environment and the number of unique words?

%Little research has been done on the \emph{semantic} information content of a possible message from an unknown generative source for which no prior is available~\cite{kolchinskySemanticInformationAutonomous2018}.

%The technical discussion involving the detection of potentially intelligent-generated messages (the wavelength range of a scope, for example) historically leaves aside an in-depth analysis of the nature of the expected message in terms of its \textit{semantic} information content.
%For the present purposes of this article to reconstruct or predict the multidimensional space in which the emitter agent is embedded, note that the terminology ``semantic information'' is standing for the (algorithmic) information about the real-world correspondents of the emitter agent that the signal emitted by the very emitter agent is trying to convey.

%Along with the claim of universal computation being ubiquitous~\cite{wolfram,Riedel2018}, Wolfram's Principle of Computational Equivalence for example suggests that intelligent-generated and naturally-generated signals (non-intelligent and purely mechanical in nature) might have the same degree of complexity generation capabilities. This would make it difficult to discern one generative process from the other and therefore undermines our efforts in the search of intelligent-based communications.

Detecting the digits of $\pi$ in a radio signal is sometimes used as an undoubtable signal of intelligent life. However, $\pi$ is simply a property of a circle, so it is rather trivial to conceive a purely mechanical scenario that naturally generates an increasingly accurate approximation of $\pi$.  For example, a geometrical probability known as Buffon's needle, is known since the 18th century. A planet or celestial body capable of emitting radio waves can encode $\pi$ in a circular orbit, if its rotational period is exactly the same as the time it takes to traverse the circle's diameter at the same speed. So, $\pi$ and other mathematical objects of low algorithmic complexity can easily be encoded in a purely mechanistic sense.  However, streaming the digits of $\pi$ without specifying its ending points would appear as a stream of maximal entropy given its likely Borel normality \cite{Calude2002} hence appearing as random looking. In fact, Ziv and Lempel showed that a sequence is normal if and only if it is incompressible by any information lossless finite-state compressor~\cite{ziv}, meaning that any statistical compression algorithm will fail at recognising the mathematical (algorithmic) content of an object like $\pi$---which therefore renders any finite-length initial segment of $\pi$ a highly compressible string. In order to encode $\pi$ in a radio signal, humans might map each digit in binary on a radio wave. However, the reader of such an encoded message would have to know that a property of a circle is not only first encoded into base-ten digits, but each digit is encoded in some $n$-ary encoding and its ending points.

%This is where the field of algorithmic complexity is relevant~\cite{bdmpaper}. 
%In clear contrast to entropy-based approaches like statistical compression, an algorithmically random sequence will only appear random if it does so to any algorithm (i.e., including not only those finite-state machines from the Ziv and Lempel characterisation but also any possible Turing machine or computable function).

%Anthropologists have argued that base ten is useful for humans because we have ten fingers-- easy for counting \cite{ifrahUniversalHistoryNumbers2000}. %Would an intelligent form of life without ten fingers encode $\pi$ in the same digits? How would these digits then be mapped to a radio wave?

%So far, all the attempts to receive, process, and analyse extraterrestrial messages have focused on techniques that receive only linear data. 

%Because of this, receiving a sequence of prime numbers has also been cited as a possible sign of extraterrestrial intelligence \cite{catlingExoplanetBiosignaturesFramework2018}. But this assumption forgets the importance of the way signals are encoded and decided. We ourselves have deliberately sent data (and unintentionally non-deliberately) in many dimensions beyond linearly, so why should we make the assumptions that another intelligent form would do the same? The Arecibo message is sent in linear form but conveys its meaning in a specific bidimensional array.

%Due to these reasons, we argue that the likelihood of receiving data in some higher-order dimension should not be discounted. We should consider strategies to decode extraterrestrial messages or signals in ways that can provide multiple different message interpretations, starting with finding its original encoding dimension. 

%Some questions related to the detection of intelligent-generated signals include: What kind of messages can be sent without a common language? How could they/we send messages that the receiver would understand, regardless of any encoding or decoding? If we assume that the same laws of physics apply all over the universe and that mathematics described by other civilisations is fundamentally the same as our own (even if expressed in a fundamentally different encoding), it has been suggested that any message would be consistent with the language of mathematics \cite{walkerExoplanetBiosignaturesFuture2018}. 

%For these reasons, 
Some researchers have suggested that the best way to get an intelligent extraterrestrial's attention is to send it a significant numeric pattern, perhaps prime numbers or the never-ending stream of the digits of $\pi$. However, even these baseline assumptions can fail. Geometric-based numbers like $\pi$ can be broadly generated by natural-like processes in the universe as they can occur numerically (as an approximation) almost everywhere in physical phenomena due to their relationship to the nature of curved surfaces.

A simple Monte Carlo estimate for the value of $\pi$ can be found by generating random points on a square and counting the proportion that lies inside an inscribed circle. The probability of a point landing in the circle is proportional to the relative areas of the circle and square, so detecting $\pi$ up to several digits and attributing it to intelligent life is confounded by natural processes. A decoder might find it difficult recognising the ending points expecting $\pi$ to begin with and might just begin to encounter the segment past its starting point unless it used an algorithmic mechanism to pick up the signal in the first place.

Indeed, one way to encode $\pi$ is to send its generating mechanism, a formula, or its 2- and 3-dimensional geometric relationship among multidimensional objects.  This is similar to the approach here undertaken, which partially relies on building a distribution of computable models of non-random objects like $\pi$ that can be highly compressed and therefore identified assuming the signal received had information content like $\pi$.

%A frequent suggestion is the use of visual representations, especially when it is about sending messages from Earth to space in binary language. For example, in order to convey the dimensions of a picture to extraterrestrial observers, one may send a signal which repeats a prime number of times. Observers might realise that as the number of signals is a prime number because it can only be factored in one way and therefore would need to be the length or width of the array to decode the signal as a 2D picture. By sending a series of pictures, eventually, some sort of vocabulary could be constructed to eventually exchange semantic information. 
%However, the technical discussion involving the detection of potentially intelligent-generated messages historically leaves aside an in-depth analysis of the nature of the expected message in terms of its \textit{semantic} information content.
For the purposes of the discussion in this article, to reconstruct the multidimensional space in which a signal is embedded, the terminology ``semantic information'' is standing for the (algorithmic) information about the real-world correspondents of the emitter agent that the signal emitted by the very emitter agent is trying to convey. In this work,
the real-world correspondents that we investigate are the objects embedded into (or grounded on) their respective multidimensional spaces (i.e., their contexts) that the emitter agent originally intended.


%Here we study the relationship between a message and its physical embedding.


%\subsection{Background}

%The technical discussion involving the detection of potentially intelligent-generated messages (the wavelength range of a scope, for example) historically leaves aside an in-depth analysis of the nature of the expected message in terms of its \textit{semantic} information content. 
%Along with the claim of universal computation being ubiquitous~\cite{wolfram,Riedel2018}, Wolfram's Principle of Computational Equivalence for example suggests that intelligent-generated and naturally-generated signals (non-intelligent and purely mechanical in nature) might have the same degree of complexity generation capabilities.
%This would make it difficult to discern one generative process from the other and therefore undermines our efforts in the search of intelligent-based communications.
%
%As an example, detecting the digits of $\pi$ in a radio signal is sometimes used as an undoubtable signal of intelligent life. However, $\pi$ is simply a property of a circle, so it is rather trivial to conceive a purely mechanical scenario that naturally generates an increasingly accurate approximation of $\pi$.  For example, a geometrical probability known as Buffon's needle, is known since the 18th century. A planet or celestial body capable of emitting radio waves can encode $\pi$ in a circular orbit, if its rotational period is exactly the same as the time it takes to traverse the circle's diameter at the same speed. So, $\pi$ and other mathematical objects of low algorithmic complexity can easily be encoded in a purely mechanistic sense.  However, streaming the digits of $\pi$ without specifying its ending points would appear as a stream of maximal entropy given its likely Borel normality hence appearing as random looking. In fact, Ziv and Lempel showed that a sequence is normal if and only if it is incompressible by any information lossless finite-state compressor~\cite{ziv}, meaning that any statistical compression algorithm will fail at recognising the mathematical (algorithmic) content of an object like $\pi$. In order to encode $\pi$ in a radio signal, humans might map each digit in binary on a radio wave. However, the reader of such an encoded message would have to know that a property of a circle is not only first encoded into base-ten digits, but each digit is encoded in some $n$-ary encoding and its ending points.  
%This is where the field of algorithmic complexity is relevant~\cite{bdmpaper}: 
%in contrast to entropy and statistical compression, an algorithmically random sequence will only appear random if it does so to any algorithm (i.e., including not only those finite-state machines from the Ziv and Lempel characterisation but also any possible Turing machine or computable function).
%
%%Anthropologists have argued that base ten is useful for humans because we have ten fingers-- easy for counting \cite{ifrahUniversalHistoryNumbers2000}. %Would an intelligent form of life without ten fingers encode $\pi$ in the same digits? How would these digits then be mapped to a radio wave?
%
%So far, all the attempts to receive, process, and analyse extraterrestrial messages have focused on techniques that receive only linear data. 
%
%%Because of this, receiving a sequence of prime numbers has also been cited as a possible sign of extraterrestrial intelligence \cite{catlingExoplanetBiosignaturesFramework2018}. But this assumption forgets the importance of the way signals are encoded and decided. We ourselves have deliberately sent data (and unintentionally non-deliberately) in many dimensions beyond linearly, so why should we make the assumptions that another intelligent form would do the same? The Arecibo message is sent in linear form but conveys its meaning in a specific bidimensional array.
%
%%Due to these reasons, we argue that the likelihood of receiving data in some higher-order dimension should not be discounted. We should consider strategies to decode extraterrestrial messages or signals in ways that can provide multiple different message interpretations, starting with finding its original encoding dimension. 
%
%%Some questions related to the detection of intelligent-generated signals include: What kind of messages can be sent without a common language? How could they/we send messages that the receiver would understand, regardless of any encoding or decoding? If we assume that the same laws of physics apply all over the universe and that mathematics described by other civilisations is fundamentally the same as our own (even if expressed in a fundamentally different encoding), it has been suggested that any message would be consistent with the language of mathematics \cite{walkerExoplanetBiosignaturesFuture2018}. 
%
%For these reasons, some researchers have suggested that the best way to get an intelligent extraterrestrial's attention is to send it a significant numeric pattern, perhaps prime numbers or the never-ending stream of the digits of $\pi$. However, even these baseline assumptions are flawed. Geometric-based numbers like $\pi$ can be broadly generated by natural-like processes in the universe. $\pi$, for example, occurs numerically (as an approximation) almost everywhere in physical systems since it appears routinely due to its relationship to the nature of curved surfaces, in probability distributions, and other simple phenomena. In other words, $\pi$ is a property of a perfect circle. 
%
%For example, a simple Monte Carlo estimate for the value of can be found by generating random points on a square and counting the proportion that lie inside an inscribed circle. The probability of a point landing in the circle is proportional to the relative areas of the circle and square, so detecting $\pi$ up to several digits and attributing it to intelligent life is confounded by natural processes. A decoder might find it difficult recognising the ending points expecting $\pi$ to begin with and might just begin to encounter the segment past its starting point. Perhaps the only way to encode the exact idea of $\pi$ would be to send it generating mechanism, a formula or its 2- and 3-dimensional geometric relationship among multi dimensional objects.
%
%To this end, a frequent suggestion is the use of visual representations, specially when it is about sending messages from the Earth to the space in binary language. For example, in order to convey the dimensions of a picture to extraterrestrial observers, one may send a signal which repeats a prime number of times. Observers might realise that as the number of signals is a prime number because it can only be factored in one way and therefore would need to be the length or width of the array to decode the signal as a 2-D picture. By sending a series of pictures, eventually some sort of vocabulary could be constructed to eventually exchange semantic information. 

%Claims of extraterrestrial radio signals have been made several times since 1899, including some presumed to be coming from Mars. But it became increasingly accepted that nothing but signals from natural (non-living) mechanisms emissions were actually the generators of the signals detected.
%
%%When galactic radio emissions were first detected in 1931, they seemed too random to be from an intelligent origin. When radio astronomy began to develop, the field mostly ignored the existence of some celestial objects that were actually sending signals that could be interpreted as being intelligently encoded, such as signals from pulsars.  So one has to know the precise difference to distinguish between what is complex in the sense that being generated by an intelligent being, as opposed to something random nature-generated.
%
%The discussion involving SETI and the signals it can detect goes back to the 1970s.  The first chapter or two of the Project Cyclops report and much of the discussion in NASA's SP419 are still relevant for today's open questions about extraterrestrial life detection.  However, most of the discussion centres around the technical justification of the hardware/software used for the physical detection of signals, not about the nature of the expected signal. The search for signals is mostly focused on narrow-band signals (mostly pulses) a few Hertz-wide (or less) with some statistically simple humanly-recognisable pattern in its frequency variances. Natural cosmic noisemakers, such as pulsars, quasars, and the turbulent, thin interstellar gas of our own Milky Way do not make radio signals that are this narrow, which is a reasonable justification of the bounds of the physical signal search. However, not much progress has been done on the qualitative aspects of signal and communication beyond Shannon information and entropy. 
%
%If a signal is intentional, it might be decipherable. In order to intentionally send or receive a signal over interstellar distances, it is reasonable to assume a civilisation must understand basic science and mathematics, or at least must evolve in such that these are somehow encoded into a message. Hence, a message from another civilisation might use some similar framework to science and mathematics to build up a common language with themselves and other societies. Signals sent by a civilisation for its own purposes may be impossible for us to unravel but our own mathematical tools may help.
%
%\begin{comment}
%
%\subsection{Other non-human ways of encoding messages}
%
%Since the late 1980's, scientists have detected the call of a single, unidentified whale that calls at 52 Hz \cite{watkinsTwelveYearsTracking2004}. This is at a higher frequency than other whales and has been dubbed ``the world's loneliest whale'', although the signal has been detected in two separate locations at the same time in the 2010s, suggesting it may no longer be a single whale. 
%
%As a result, other whales cannot detect the calls made at these high frequencies and its calls go unheard. While the message this whale might by trying to communicate may be similar or even identical to messages produced by other whales at a lower frequency, its difference in encoding that message makes it impossible for other whale observers to decode. Whales within the usual frequency range of communication have evolved to detect and decode messages received in a particular encoding on a particular frequency. In order for the 52-Hz whale to be heard, other whales would need to suddenly be granted to capability to hear on much higher frequencies.
%
%Other cetaceans, such as dolphins, have evolved the ability to sense their environment through echolocation. The complex clicks and whistles that a dolphin makes are key for that dolphin's ability to sense and locate physical objects by detecting which sound waves are reflected back to the dolphin. 
%
%In the last decade, researchers have successfully been able to reproduce 3-D images from dolphin signals \cite{kassewitzPhenomenonDiscoveredImaging2016}. Using a novel instrument, researchers are able to capture dolphin sound waves that are emitted while the dolphin is presented with some objects in its vicinity. By using novel wave-processing techniques, 3-D reconstructions of the objects were recovered from the dolphin's emitted signals only. The instrument was designed based on understanding the physiological mechanisms that allow dolphins to sense their environment, including dolphin sound signals. Without understanding the physiological structure of a dolphin's ability to sense, such as the shape of its jaw and bones, it would be extremely difficult to build an instrument (or use an existing instrument) capable of decoding these signals to reveal their original message.
%
%More broadly, 
%
%Earth's biosphere is teeming with all sorts of encoders, decoders, and signals. Even within this system, messages and signals received from non-human species cannot be decoded and interpreted flatly across all message-sending sources. This is even true within human technological systems. 
%
%\end{comment}
%
%On earth, each different species has its own different set of sensors that it uses to interact with its environment. Based on the philosophical essay "What is it like to be a bat?" \cite{nagelWhatItBe1974a}, the recent book "An Immense World" \cite{yongImmenseWorldHow2022} explores the unique slices of experience that are native to many different species. Because each creature is outfitted with its own, unique sensory mechanisms and capabilities, Yong illustrates in great detail how different species experience the world differently. To put this in terms of signals or messages between senders and receivers, different species emit signals based on the mechanisms that allow them to do so. They also decode messages based on their sensory capabilities, and if each species has a unique set of senses, then we can say that each species decodes the same message differently. The 52Hz whale's messages are decoded differently between humans monitoring the oceans and other whales (who do not sense the messages at all).
%
%There are myriad data types and formats that encode different types of data on multiple levels, from ASCII characters to video files to health record data. While it is possible to re-encode all data into a single format, the original message may be lost (imagine trying to extract voice-recorded messages of a sound file only by looking at the sound wave plot over time).
%
%Because of this diversity within our own planet, and even within our own technology, the process of detecting, decoding, and interpreting extraterrestrial signals \textit{must} involve considering a wide range of possible encoding and decoding schemes. We \textit{cannot} decode extraterrestrial signals based on a single encoding mechanism or assume that any such signal was encoded in a way that some humans would. We introduce a methodology that is aimed at sweeping over various possible encoding and decoding schemes to test our current limits on extraterrestrial signal interpretation.


\subsection{On encoding-decoding schemes, compression, and randomness}\label{sectionCompressionandrandomness}

On the one hand, a non-random (i.e., compressible) message would have a large standard deviation in the range of information values for its different partitions, and thus the schema that shortens any redundancies must be read properly by an interpreter to properly translate the shorthand redundancies back into their longer form.
Intuitively, this is because compression is a mapping that translates the state of the original message into a different state aimed at expressing any redundancies in the original messages as a shorter expression.

Indeed, it is possible for a signal stream to arrive at a naive receiver in a format that contains sufficient information about how to decompress itself. %e.g. a computer program and the computer itself.
However, even in this case, the receiver must have prior knowledge about how to decode that signal into its original encoding so as to reveal the redundancies present in the original message that gave it meaning.
Note that the case in which the receiver must have prior knowledge about how to decode the signal according to the emitter's previously chosen encoding scheme---so as to introduce redundancies in the original message and avoid decoding noise as meaningful information---is what already allowed the classical noisy-channel coding theorem  \cite{Cover2005} to hold.

Imagine a sender encodes a flattened image as a sequence of bits and a receiver detects these bits as an incoming stream. Is there any way for the receiver to tell that the stream is a message delivered in a bidimensional array, i.e. that the message is actually an image? And, in the general case where the receiver does not know anything about the original message at all, can a lower-information transmission encode its original dimension such that the message is natively encoded? At present, there are at least two common ways to flatten an image, but we do not yet know if it is possible to reconstruct the image given any arbitrary encoding and no information about the original message or its encoding. Is this information somehow embedded in the data? If so, is it possible to recover it?

On the other hand, if data is displayed in its most compressed and linear form, there is little chance to recover the original multidimensional space in which the original message was embedded, let alone the original message itself.
Thus, such a message would be indistinguishable from an ideal fair-coin-toss sequence of events.
%However, the schema that shortens any redundancies must be read properly by an interpreter to properly translate the shorthand redundancies back into their longer form.
This holds because of the basic properties in algorithmic information theory and classical information theory that a message in its most compressed form is incompressible (except for a constant that does not depend on the message) by any arbitrarily chosen universal machine or programming language.

Without proper conditions that assure the algorithms can only pick meaningful or sound features once previously assumed that data actually contained a message, we know the presence of spurious correlations constitutes one of the dangerous of naively applying any sort of statistics-based data analysis to extract or find statistically significant patterns in sufficiently large datasets \cite{Calude2017,Smith2020}. 
A spurious correlation is a correlation found in statistical or computational analysis so that it would have occurred in a randomly generated database anyway, and therefore bears no meaningful information about the investigated phenomena nor reveals any underlying causal relation.
%We start with the baseline assumption that
Ideally, given a random signal, such as white noise, no such signal interpretation should produce any meaningful message or interpretation.
%Mathematically, this means that
A random signal should look random no matter how it is encoded or decoded.
Thus, along with the data compression distortions known to occur from the application of statistics-based or entropy-based methods \cite{zkpaper,zenilreview2020}, the possibility of spurious findings in random messages is of major concern in decoding signal streams so as to reconstruct the original message.



%Many compression algorithms linearise data as a part of the compression process, so why does exploring different before-compression dimensions matter? If there is a shift, change, or edit in the data, then re-flattening the data does not lead to the same compressed arrangement. 
%Assuming that the data actually contains a message, compression methods only work because they capitalise on the most redundant arrangement with a lower algorithmic complexity value (or more regularities). 
%However, 
%Without proper conditions that assure the algorithms can only pick meaningful or sound features once previously assumed that data actually contained a message, we know the presence of spurious correlations constitutes one of the dangerous of naively applying any sort of statistics-based data analysis to extract or find statistically significant patterns in sufficiently large datasets \cite{Calude2017,Smith2020}. 
%A spurious correlation is a correlation found in statistical or computational analysis so that it would have occurred in a randomly generated database anyway, and therefore bears no meaningful information about the investigated phenomena nor reveals any underlying causal relation.
%%We start with the baseline assumption that
%Ideally, given a random signal, such as white noise, no such signal interpretation should produce any meaningful message or interpretation.
%%Mathematically, this means that
%A random signal should look random no matter how it is encoded or decoded.
%Thus, along with the data compression distortions known to occur from the application of statistics-based or entropy-based methods \cite{zkpaper,zenilreview2020}, the possibility of spurious findings in random messages is of major concern in decoding signal streams so as to reconstruct the original message.

These phenomena strongly suggest that for effective communication, especially for one-way communication between any sender and receiver, a message should not be displayed in its most compressed form, i.e. incompressible, unless the receiver has knowledge about the compression schema and can uncompress the message back into its original state (disregarding efficiency).  This problem is equivalent to encoding a message for cryptographic purposes (modulo efficiency).
Instead, a message should be received in an uncompressed form in first place, compressibility which is compared to the signal original size (or length, if the signal stream is unidimensional).
See also Section~\ref{sectionZero-knowledge} for the theoretical framework underlying this intuition.
%Instead of trying to extract any type of pattern from noisy communication channels in which the received signal stream may be mostly constituted of white noise---and, therefore, prone to spurious correlations---, we analyze the compression of the whole signal stream. 
%\subsection{Algorithmic Information Dynamics}\label{sectionAID}

Combining perturbation analysis and irreducible information content, an area called algorithmic information dynamics (AID)~\cite{algodyn,nmi} (which is based on algorithmic information theory but applicable also to statistical approaches based on classical information) is a key asset for the search of properties of meaningful signals away from randomness due to its universal and independent (invariant in the limit) nature specially due to not being predicated upon priors and a priori given probability distributions. 
The recognition of bit strings of various complexity values up to incompressible random sequences will require an algorithmic-information-based strategy to analyse cosmic signals.
The present article shows that AID is indeed capable of harnessing the universality of algorithmic information. 

In this instance, AID may rely upon, but is independent of, an algorithm called BDM for Block Decomposition Method. BDM is a measure of algorithmic probability~\cite{bdmpaper}. It relies on a second algorithm called Coding Theorem Method or CTM~\cite{d5} that helps navigate model space~\cite{algodyn}. Model space is the space of all computable models generated by exhaustively running a large number of computer programs, in this case, 5\,562\,153\,742\,336 halting programs sorted from smallest to larger on a supercomputer as a numerical approximation to the semi-computable universal model of models~\cite{d5}.  This fundamental model of models learns incrementally from this large computation assumed to approximate a universal distribution~\cite{miracle} and is trained to spike upon the closest computer program that explains the data.  Other models of computation have been explored~\cite{ca,fco}, with some degree of correspondence~\cite{correspondence}. BDM uses CTM to build a sequence of approximate small models that in sequence closely explain an observation or perturbation~\cite{bdmpaper,maininfo}.

Our results are not only agnostic to prior knowledge of encoding-decoding schemes, but also demonstrate sufficient conditions in this zero-knowledge context for enabling the reconstruction of the original \emph{message} (i.e., the original object embedded into the original multidimensional space to be conveyed to the receiver) in one-way communication channels.
%In particular, we demonstrate that one of these conditions is the assumption that any message received via a signal stream should be logarithmically compressible.
%By \emph{logarithmically compressible} objects we refer to those objects that can be compressed up to a logarithmic term (see Corollary~\ref{thm2}) as a function of the size of the object---in other words, the object is either computable or $\mathbf{K}$-trivial \cite{Downey2010}.



%\section{Experiments}
\section{Results}\label{sectionResults}

%We start with the baseline assumption that given a random signal, such as white noise, no such signal interpretation should produce any meaningful message or interpretation. Mathematically, this means that a random signal will look random no matter how it is decoded. A non-random message would have a large standard deviation in the range of information values for its different arrangements. If data is compressed, however, there is little chance to recover the dimension, let alone the original message. This is because compression is a mapping that translates the state of the original message into a different state aimed at expressing any redundancies in the original messages as a shorter expression. However, the schema that shortens any redundancies must be read properly by an interpreter to properly translate the shorthand redundancies back into its longer form. This strongly suggests that for effective communication, especially for one-way communication between any sender and any receiver, data should not be compressed (unless the receiver has knowledge about the compression schema and can uncompress the message back into its original state). It is also possible for a compressed message to arrive to a naive receiver in a format that decompresses itself, e.g. a computer program and the computer itself. However, even in this case, the receiver must have knowledge about how to decode that signal into its original encoding.
%
%Imagine a sender encodes a flattened image as a sequence of bits and a receiver detects these bits as an incoming stream. Is there any way for the receiver to tell that the stream is a message delivered in a bidimensional array, i.e. that the message is actually an image? And, in the general case where the receiver doesn't know anything about the original message at all, can a lower-information transmission encode its original dimension such that the message is natively encoded? At present, there are at least two common ways to flatten an image, but we don't yet know if it is possible to reconstruct the image given any arbitrary encoding and no information about the original message or its encoding. Is this information somehow embedded in the data? If so, is it possible to recover it?
%
%Many compression algorithms linearise data as a part of the compression process, so why does exploring different before-compression dimensions matter? If there is a shift, change, or edit in the data, then re-flattening the data does not lead to the same compressed arrangement. Compression methods only work because they capitalise on the most redundant arrangement with a lower algorithmic complexity value (or more regularities, assuming that the data actually contains a message).

%Here is an example with a few different sequences. The first is a simple sequence of alternating zeroes and ones. If this sequence is perturbed by flipping a single bit, the size of the compressed sequence will be larger than the size of the compressed original sequence. Fig.~\ref{compression} shows this result for sequences up to size 200.

%\begin{figure}[ht!]
%\centering
%\includegraphics[scale=0.35]{example1.png}\includegraphics[scale=0.35]{example1_1.png}
%\caption{\label{compression}Left: Perturbing a single bit in a regular sequence of alternating 0's and 1's will cause the resulting size (in bits) of compression to increase when compared to the size of compression of the original sequence. Right: The block entropy value of perturbed sequences does not change, but the BDM increases for all but one sequence length.}
%\end{figure} 

\subsection{Perturbation analysis of compressed knowledge}\label{sectionAIDonDarwins}

To demonstrate the relationship between compression, algorithmic complexity, and encoding methods, we took real sentences from Darwin's ``Origin of Species'' and binarise them in two different ways according to arbitrary ASCII-to-binary functions. 
In one encoding (left column in Fig.~\ref{sentences}), ASCII vowels are assigned a 1 and all others 0. For the other encoding (right column), spaces are assigned a 1 and the rest 0. The compression, block entropy, and block decomposition method (BDM) are measured for each string. Then, for each string, each value is perturbed (1 to a 0, or vice-versa) one at a time. The resulting change in values from the original binary sequence to the perturbed ones are shown as different coloured lines for each panel.


%\color{blue}As formalised in \cite{Abrahao2021b} any transformation or perturbation of a finite object into another finite object is equivalent to an algorithmic transformation that takes the former as input and outputs the latter.
%Thus, a perturbation with low algorithmic complexity is one whose algorithmic information content is preserved under such perturbations.
%Swapping all 0's and 1's, or swapping all black and white pixels in an image are examples of low-algorithmic-complexity transformations.
%Randomly inserting or deleting an element (such as creating or destroying an edge in a graph or perturbing a single bit in a string) is also an example of low-algorithmic-complexity transformation \cite{Abrahao2021b,Zenil2019c}.
%Note that a perturbation can change either the object or the multidimensional space independently, or can change both at the same time.\color{black}


The results in Fig.~\ref{sentences} demonstrate that most single-bit perturbations in a sequence that encodes a message results in a more random-looking sequence. These results vary depending on what mapping we choose to binarise the sequences, but in general, any perturbation generally increases randomness across various mappings (we also explored other arbitrary binarisation functions, with similar results). In other words, this example shows that sequences that carry meaningful content (i.e., being logarithmically compressed\footnote{ See Section~\ref{sectionRetrievability}.}) are quantitatively different according to various measures. Any deviation (i.e., perturbation) from the encoded sequences causes the randomness to increase, which suggests that an incoming signal with low randomness is more likely to encode a message than one that has more randomness.
See Section~\ref{sectionZero-knowledge} for the theoretical investigation of this phenomenon.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{Figures/newfig1.png}
	\caption{\label{sentences}Algorithmic perturbation analysis on sentences of a short length of only 400 characters from Darwin's ``Origin of Species'' are converted from ASCII into their corresponding binary values. Changing a single character with probability $1/n$ for growing $n$ from $n=1$ (original test, axis $x=0$) to $n=4$ (a quarter of the text is mapped to a random character from the same vocabulary of the original text, axis $x=100$ letters changed or perturbed) causes the resulting size of the compressed sequence index to increase, with BDM the most sensitive and Shannon entropy unable to capture any difference. This is because signals carrying meaning are far away from randomness and random perturbations make the text more random \cite{Zenil2019c} even when the methods know nothing about words, grammar or anything linguistic.}
\end{figure}

Fig.~\ref{sound} demonstrates the same phenomenon shown in non-random text but in audio signals,  the results using an audio recording of the words spoken from Apollo 13, ``Houston, we've had a problem'' transmitted on April 13, 1970 at a sample rate frequency of 11025 Hz on a single channel. When compressed, the message is 106\,250 bytes and 106\,320 characters long. Here, the message was scrambled several times. The histograms on the bottom left show the different compression lengths in bytes and string lengths of the scrambled messages. Each of these scramblings results in an increase in randomness. Additionally, if only the beginning and end of each word are perturbed, the resulting randomness increases smoothly with each perturbation, as shown in the bottom right plot. These results indicate that perturbations from the original message, including scrambling elements of the original message in a different order will result in a message of higher complexity, and thus more randomness. As a result, the original encoding of the message (the one with interpretable meaning) is the one with the lowest complexity.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.37]{Figures/sound.png}
	\vspace{0.5cm}
	
	\centering
	\includegraphics[scale=0.32]{Figures/soundhist.png}\hspace{0.3cm}\includegraphics[scale=0.31]{Figures/plot.png}
	\caption{\label{sound}\textit{Top:} Sound waves of the words spoken from Apollo 13, ``Houston, we've had a problem.'' transmitted on April 13, 1970 at a sample rate frequency of 11025 Hz on a single channel. \textit{Bottom left:} Histograms of the change in compression lengths of scrambled versions of the same message from the length of the original compressed message. \textit{Bottom right:} Small perturbations of the original message by scrambling only the beginning and end of each word shows a smooth transition from low to high randomness. The lowest complexity signal indicates the correct (original) signal. The file was processed in FLAC format (Free Lossless Audio Codec) from a lossless file with no audio data discarded.}
\end{figure}

%A combination of perturbation analysis and information content, an area called Algorithmic Information Dynamics~\cite{algodyn,nmi}, based on algorithmic complexity, i.e. the minimal length of a binary-coded string that completely defines a system, can be a key asset for the search of meaningful signals due to its universality and independence from priors and inaccessible probability distributions. Thus, the recognition of bit strings of various complexity up to incompressible random sequences, will require a algorithmic information-based strategy to analyse cosmic signals.


\subsection{On the information content of the Arecibo message}\label{sectionArecibo}

In 1974, the bitmap image in Fig.~\ref{arecibo} was sent into space as a radio signal from the Arecibo radio telescope. At the left-hand end of the image is a version of the pattern of digits from page 117-- but it is distorted so it has no obvious nested structure. In the image, there are atomic numbers for various elements and bitvectors for components of DNA. Under these, there are rough pictorial representations of a DNA molecule, a human, and the telescope. All these images seem to depend almost completely on human visual encoding/decoding conventions. Without any sort of human context, including making the fact that these are pictorial representations based on human vision, their meaning would be essentially impossible to recognise. This is especially true for message receivers who may not possess visual recognition capabilities, at least not visual capabilities that are similar to our own vision.

\begin{figure}[ht!]
	\centerline{\includegraphics[scale=0.28]{Figures/Fig1.png}
		\hspace{1cm}
		\includegraphics[scale=0.31]{Figures/Fig2.png}}
	\caption{\label{arecibo}\textit{Left:} The original Arecibo message intended to be reconstructed, but sent as a linear stream from the radiotelescope in Arecibo, Puerto Rico. The 1,679 bits are meant to be arranged into 23 columns of 73 rows, 23 and 73 being two prime numbers, which when multiplied together equal 1,679. \textit{Right:} If the stream is instead arranged into 23 rows and 73 columns, the original visual interpretations of the message are scrambled, which may result in a figure closer to being statistically random. What we show is that the message is still there concealed and can still be brought forward by algorithmic deconvolution.}
\end{figure} 



\begin{figure}[ht!]
	\centerline{\includegraphics[scale=0.44]{Figures/signaldeconv.png}}
	\caption{\label{arecibosequence}\textit{Top left:} Most possible partitions result in random-appearing configurations with high corresponding complexity, indicating measurable randomness. \textit{Bottom:} Some partitions will approximate the originally-encoded meaning (third from the right). Other configurations result in images with higher complexity values. This sequence of images shows the images in the approximate vicinity of the correct bidimensional configuration (i.e., partition) and illustrates fast convergence to low complexity. \textit{Top right:} By using different information indexes across different configurations, a downwards-pointing spike will indicate message (image) configurations that correspond to low-complexity image(s). This allows a natural and objective method to infer a message's original encoding. Of the various measures, BDM, combining classical information (entropy) for long range and a measure motivated by algorithmic probability on short range, is the most sensitive and accurate in this. Traditional compression and entropy also indicate the right configuration amongst the top spiking candidates. The ratio of noise-to-signal was amplified in favour of the hidden structure by multiplying the original  image size by 6 in both length and height.}
\end{figure} 

In addition, this message was reformatted from its original bidimensional state and sent as a string of 1,679 binary digits. The reasoning was that an alien civilisation with a mathematical system that receives the message will recognise 1,679 as a semi-prime number--a multiple of 23 and 73. Even if the original message could not be reconstructed in its original and intended encoding, the receiver would recognise the message as having some mathematical significance that is unlikely to be a random signal from outer space.

Fig.~\ref{arecibosequence} shows how the method provides a robust approach to identifying the best way to decode a signal back to its original multidimensional space. 
By sweeping over a large variety of possible dimensional configurations and measuring the resulting information content and algorithmic complexity of the candidate messages by taking the lowest algorithmic complexity configurations under the assumption that the original message is algorithmically not random (i.e., that the original message is logarithmically compressible).  
As shown in \cite{Zenil2019b} for graphs, this is because if the original message is random, then the likelihood that a rearrangement (i.e., a new partition) will lead to a low complexity configuration is very low. On the other hand, if the original message is logarithmically compressible, and therefore not random, most partitions will lead to configurations of greater algorithmic randomness.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.36]{Figures/additivenoise.png}
	\caption{\label{noise}Method resilience to noise. At 3\% of the bits of the original $23 \times 73=1679$-pixel image randomly flipped (which means about 1.5\% were binary negated), the method remains sensitive and displays a small downward spike at the 23 value uncovering its length. The head of each line on the plot indicates whether the message is random or is encoded in the right partition and remains highly resilient to noise at about 50\% of bits flipped randomly (hence about 25\% different).}
\end{figure}

Fig.~\ref{noise}, shows the method's resilience to noise for indicating the candidate partition and the precise lengths of the original message. Successful amplification of the signal is shown in Fig.~\ref{ampnoise} with BDM outperforming Compression and Shannon Entropy in the face of additive noise with Compression showing insensitivity to the original signal at about 10\% of bits flipped and Shannon Entropy diminishing faster than BDM but slower than Compression.  In all cases, different levels of robustness at deconvolving the image dimensions are shown.

To investigate this further, Fig.~\ref{moreexps} shows six bidimensional images along with their original numerical dimension. The values of the three complexity measures over possible partitions are shown below the original numerical dimension for easy comparison.

Drops in complexity (downward spikes) indicate candidate dimensions for the original encoding dimensions, and thus the decoding dimension that would process the signal such that it produces the lowest-complexity message. BDM outperforms compression and entropy and correctly identifies the original message (image) encoding for 4 out of 6 images. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.26]{Figures/noisedeconv.png}\hspace{.8cm}\includegraphics[scale=0.26]{Figures/amplifiedconvnoise.png}
	\caption{\label{ampnoise}When applying 3 different quantitative methods, it is shown (left) that they are highly insensitive to signal and highly sensitive to noise (16.5\% of pixels randomly flipped).  However, by growing the original image (right) by a factor of 6 on each dimension (i.e. 1 pixel becomes $6\times6$), the methods are less sensitive to noise and more sensitive to signal with BDM significantly outperforming Compression and Shannon Entropy showing sensitivity at up to 60\% pixels flipped (hence about 30\% of the original image) versus Compression and Shannon Entropy half sensitive. Downward spikes (right) are shown at $23 \times 6 = 141$.}
\end{figure}

The method is invariant to linear transformations such as encoding, as shown in images 2 and 4 of Fig.~\ref{moreexps}. 
Corroborating the theoretical predictions from Section~\ref{sectionRetrievability}, algorithmic complexity (as estimated via BDM) is also shown to be invariant to low-algorithmic-complexity transformations that preserve the data algorithmic content (such as swapping all 0's and 1's or swapping all black and white pixels in an image), as well as some non-linear but algorithmic-information-content-preserving transformations~\cite{kolmo2d,bdmpaper}.
Indeed, Section~\ref{sectionRetrievability} also presents theoretical findings that demonstrate that if the original message is highly compressible, then the amount of low-complexity perturbations (whether introducing noise into the message or rearranging the dimensions into a different partition) on the message which drastically move the message toward randomness strongly dominates the amount of those which do not.
Therefore, in case this hold, the original message can be estimated from the one that minimises algorithmic complexity with probability as high as one wishes, which predicts our empirical findings but in the ideal limit when computational resources are unbounded.

%See Section~\ref{sectionRobustenesstoperturbations}. 
%As also demonstrated in Section~\ref{sectionRobustenesstoperturbations}, this method is invariant to linear transformations as well as some non-linear but algorithmic-information-content-preserving transformations~\cite{kolmo2d,bdmpaper}.
%\todo{Update this paragraph with the crossrefs to the theorems and respective figures.}


%As demonstrated in Section~\ref{sectionRetrievability}, the best way to decode a message is the one that produces the least-random-looking message. 

\begin{figure}[ht]
\centerline{\includegraphics[scale=0.34]{Figures/Fig10.png}}
	\caption{\label{seqs}Sequence of lowest complexity (BDM value on top in bits) partitions of a black and white image of a space shuttle, with the top right partition as the correct reconstruction and other candidates cases in which an alignment occurs at some multiple of the correct dimension.}
\end{figure}
\pagebreak
Some of these image decodings for the space shuttle image are shown in Fig.~\ref{seqs}. 
Each panel shows random realignments of the correct dimensions along with the original image (i.e., object) embedded into these new partitions, and their respective BDM values are displayed above each image. 
The original (and correct) image has the lowest complexity value. 
When compared to the other multidimensional space, the one that is most similar to the original message will have the lowest complexity value.

By translating (encoding) a colour image into binary black and white pixels, this method picks an image decoding that is similar enough to the original image (at least, similar enough visually). Fig.~\ref{mandril} shows the original image on the left, the results of the method on the right, and the resulting image selected from the spike in BDM on the bottom. A mirror-like image is selected as the first candidate, followed by the correct one. This kind of spiking also suggests the original partition can be inferred via smaller, non-random spikes at multiples or divisors of the native partition (500). In this case, small spikes at half the original partition (250) provide clues to the original embedding in 2D. 


\begin{figure}[H]
	\centerline{\hspace{.8cm}\includegraphics[scale=0.55]{Figures/Fig6.png}}
	
	\begin{center}
		196 \hspace{3.8cm} 82 \hspace{4cm} 215
	\end{center}
	
	\centerline{\includegraphics[scale=0.27]{Figures/Fig7.png}}
	
	\vspace{0.5cm}
	
	\centerline{\hspace{.8cm}\includegraphics[scale=0.37]{Figures/Fig8.png}}
	
	\begin{center}
		{276 \hspace{3.6cm} 360 \hspace{3.6cm} 220}
	\end{center}
	
	\centerline{\includegraphics[scale=0.28]{Figures/Fig9.png}}
	\caption{\label{moreexps}Six 2D images (named 1 through 6 going from left to right, top to bottom) of very different nature, including a demonstration of linear-transformation invariance conforming to the theory behind (in this case, rotation of a mathematical formula).  Size invariance has actually shown amplification of signal-to-noise difference. Under each image is its correct numerical (first) dimension. The values of the three complexity indexes over possible partitions are shown below the original numerical dimension for easy comparison. Downwards spikes indicate candidates for possible original partitions. In all cases, the correct dimension value is among the top three candidates, with BDM outperforming 4 out of 6 cases to indicate the top candidate.}
\end{figure}



\begin{figure}[H]
	
	\flushleft\footnotesize{\hspace{2.4cm}500$\times$500}
	\centerline{\includegraphics[scale=0.32]{Figures/Fig15.png}\includegraphics[scale=0.3]{Figures/Fig16.png}}
	\flushleft\footnotesize{\hspace{6cm}750$\times$250}
	\centerline{\includegraphics[scale=0.57]{Figures/Fig29.png}}
	\caption{\label{mandril}\textit{Top Left:} An original 2D image message and \textit{top right:} the complexity results of its various image reconstructions via linear signal decomposition (in binary, similarly to how the Arecibo message was sent) into distinct partitions. BDM spikes prominently at the correct right partition with bidimensional configuration of 500 $\times$ 500 pixels, contrasting with a mirror-like image at 250 $\times$ 750 pixels (\textit{bottom}, rotated 45 degrees counterclockwise).}
\end{figure}


When a message is represented in a different partition than the original message, the resulting plot shows a regular statistical pattern. 
For example, encoding a tridimensional image into a bidimensional space will produce plots similar to the ones show on the bottom-left of the figure. Fig.~\ref{bone} shows upward and downward spikes for an image in three dimensions. Downwards spikes indicate partitions that the algorithm interprets the signal (image) as having low complexity, whereas upwards spikes indicate high complexity.

\begin{figure}[H]
	\flushleft\footnotesize{\hspace{3cm}128 $\times$ 128
		$\times$ 128 \hspace{3.7cm} 64 $\times$ 128 $\times$  192}
	\centerline{\includegraphics[scale=0.28]{Figures/Fig31.png}\hspace{1.5cm}\includegraphics[scale=0.27]{Figures/Fig28.png}}
	\centerline{\includegraphics[scale=0.35]{Figures/Fig23.png}}
	\caption{\label{bone}\textit{Top left:} A reconstruction exercise of a 3D image of a Magnetic Resonance image of a knee embedded in a cube. \textit{Top right:} Reconstruction from the \textit{bottom:} perturbation analysis on various partitions. Spikes occur at the original dimension's multiples: 64, 128, and 192. When the linear signal stream is partitioned at the first candidate, the next dimensions are indicated by downward spikes, or spike upwards even on the first pass. A mirror image (top right) is indicated and reconstructed as the most likely candidate and the correct knee configuration appears at the second spike (top left).}
\end{figure}










\section{Methods and theoretical framework}\label{sectionMethods}


%\color{blue}The main method based on algorithmic information dynamics (AID) consists in a decomposition of the original object (for testing purposes) or a perturbation analysis (as the reverse actual method when the source is of unknown nature) by sweeping over a significant sample of all possible lengths (as a result of the partitions) and all $n$-dimensional arrangements  (i.e., different ways of decoding the same original message in distinct multidimensional spaces respectively) from $n$ smaller to larger, and estimating the resulting changes of the arrangement. 
%The arrangement(s) with the lowest complexity indicates the original dimension of the message or some multiple/division of it. 
%
%Each partition determines the different possible lengths of an object for which the original embedded dimensions is unknown. 
%
%Thus, each partition corresponds to a distinct dimensions' lengths configuration in a presumed multidimensional space with its total additive lengths constant.
%
%For example, a picture of 4$\times$4 pixels can be partitioned into multiple configurations leading to different non-squared shapes. All configurations of 16 pixels in different rows and columns are considered as all possible perturbations of the original message in whatever form it was sent. Once a row or column is fixed, the remaining rows and columns are dependent on the initial row fixation. 
%
%As a result, each partition is a single-variable dependency no matter if the object is embedded into a uni-, bi-, or tridimensional space.
%More formally, in the case of a (finite and discrete) bidimensional space $ \mathcal{ S }_2 $ determined by the pair  $ \left( \left| {  \mathcal{ S }_2 }_x \right| ,  \left| { \mathcal{ S }_2 }_y \right| \right) $ as in Figs.~\ref{arecibo} and~\ref{arecibosequence},
%where $ { \mathcal{ S }_2 }_x $ is the first dimension and $ { \mathcal{ S }_2 }_y $ is the second dimension,
%one may have different combinations of values $ \left| {  \mathcal{ S }_2 }_x \right| $ and $ \left| {  \mathcal{ S }_2 }_y \right| $ for each space.
%However, for any partition one has it that the total value $ \left| {  \mathcal{ S }_2 }_x \right| \times \left| {  \mathcal{ S }_2 }_y \right| $ remains the same.
%Our method then consists of finding the bidimensional space $ \mathcal{ S }_2 $ for which the algorithmic complexity of the object (i.e., the message) embedded into $ \mathcal{ S }_2 $ is minimised.
%See Sections~\ref{sectionAIDistortions} and~\ref{sectionRetrievability} for a formalisation of the theoretical background of this method that applies to any type of multidimensional space.\color{black}

%\color{green}To investigate this further, Fig.~\ref{moreexps} shows six bidimensional images along with their original numerical dimension. The values of the three complexity measures over possible partitions are shown below the original numerical dimension for easy comparison. Drops in complexity (downward spikes) indicate candidate dimensions for the original encoding dimensions, and thus the decoding dimension that would process the signal such that it produces the lowest-complexity message. BDM outperforms compression and entropy and correctly identifies the original message (image) encoding for 4 out of 6 images. 
%This method is invariant to linear transformations such as encoding, as shown in images 2 and 4 of Fig.~\ref{moreexps}. 
%In fact, corroborating the theoretical predictions from Section~\ref{sectionRetrievability}, algorithmic complexity (as estimated via BDM) is also shown to be invariant to low-algorithmic-complexity transformations that preserve the data algorithmic content (such as swapping all 0's and 1's or swapping all black and white pixels in an image).
%
%The method is even robust enough for colour images. By translating (encoding) a colour image into binary black and white pixels, this method picks an image decoding that is similar enough to the original image (at least, similar enough visually). Fig.~\ref{mandril} shows the original image on the left, the results of the method on the right, and the resulting image selected from the spike in BDM on the bottom. A mirror-like image is selected as the first candidate, followed by the correct one. This kind of spiking also suggests the original dimension can be inferred via smaller, non-random spikes at multiples or divisors of the native dimension (500). In this case, small spikes at half the original dimension (250) provide clues to the original embedding in 2D. 
%
%When a message is represented in a different dimension than the original message, the resulting plot shows a regular statistical pattern. 
%For example, encoding a tridimensional image into a bidimensional space will produce plots similar to the ones show on the bottom-left of the figure. Fig.~\ref{bone} shows upward and downward spikes for an image in three dimensions. Downwards spikes indicate partitions that the algorithm interprets the signal (image) as having low complexity, whereas upwards spikes indicate high complexity.
%
%\color{blue}Fig XXX Signature equivalence of random and simple configurations. The configuration of the lowest compression on a non-random signature is also similar to a measure of sophistication like logical depth.\color{black}

Our results in Section~\ref{sectionResults} are based on the assumption that the best strategy for one-way communication to a receiver with zero knowledge about the encoding-decoding scheme is to send compressible messages. 
Now, we investigate the mathematical conditions that enable these results and claims. 

The theoretical framework presented in Section~\ref{sectionRetrievability} demonstrates the method employed to achieve the empirical findings in Section~\ref{sectionResults} is sound in the infinite asymptotic limit when computational resources are unbounded and can be generalised to multidimensional spaces in addition to bi- or tridimensional ones.
These empirical results corroborate the theoretical predictions of the theorems in Section~\ref{sectionRetrievability}, which in turn evinces that AID is capable of harnessing (under feasible computational resources) some of the algorithmic-informational properties that the following theorems demonstrate in ideal and generalised scenarios.
As shown in \cite{algodyn,algodyn2}, AID allows the investigation of causal analysis \cite{maininfo,Zenil2019b} and solution of inverse problems \cite{nmi} by taking advantage of 
a high convergence rate to the algorithmic probability, which is stable across distinct models of computation, and most prominently for high algorithmically probable objects. 


In Section~\ref{sectionAIDistortions}, we present the theoretical limitations in the general case, which in turn highlights the sufficient conditions that should be satisfied in order to avoid these limitations as we will investigate in the theoretical framework demonstrated in Section~\ref{sectionRetrievability}.

\subsection{Avoiding algorithmic information distortions in arbitrarily complex multidimensional spaces}\label{sectionAIDistortions}





In \cite{Abrahao2020c,Abrahao2021} it is investigated how much the irreducible information content of a multidimensional network and its isomorphic monoplex network (i.e., its isomorphic graph) is preserved during such an isomorphism transformation.
In fact, isomorphisms are demonstrated to not preserve algorithmic information in the general case.
The algorithmic information of a multidimensional network might be exponentially distorted with respect to the algorithmic information of its isomorphic (monoplex) network (see \cite[Theorem~12]{Abrahao2021}).
We refer to an algorithmic information \emph{distortion} when the algorithmic information content of an object (e.g., measured by the prefix algorithmic complexity of an encoded form of the object) in a certain context (e.g., the multidimensional space the object is embedded into) sufficiently differs from the algorithmic information content of the same object in a distinct context.
In this regard, algorithmic complexity \emph{oscillations} may arise from a series of distortions produced by a large enough collection of distinct contexts into which the same object is embedded.
From the well-known invariance of algorithmic complexity, one for example knows that every positive or negative oscillation resulting from changing the computation model (or universal programming language) is pairwise upper and lower bounded by a constant that does not depend on the object output by this pair of machines.
In this article, other examples are the oscillations measured by the plots presented in the figures in Section~\ref{sectionResults}, particularly those image plots for which one has the $x$-axis representing the distinct partitions we estimated the algorithmic complexity of the messages into, respectively.
Note that the downward spikes we have seen in Section~\ref{sectionResults} are examples of algorithmic complexity (negative) oscillations that for the decoding problem we are interested in, indicate the partition that corresponds to the correct multidimensional space the message intended. 

The results in \cite{Abrahao2020c,Abrahao2021} demonstrate a fundamental limitation that mathematical and computational methods should be aware of in order to properly reconstruct the original multidimensional space from a receiver.
To demonstrate this limitation, take for example received signals that encode (either in $1$D or $2$D signal streams, respectively) adjacency matrices of the (monoplex) networks whose isomorphic counterparts are multidimensional.
For any arbitrarily chosen formal theory and decoding algorithm, the theorems in \cite{Abrahao2020c,Abrahao2021} imply that there is a sufficiently large multidimensional network whose multidimensional space cannot be retrieved from the adjacency matrix of the network by the respective formal theory and/or decoding algorithm.
This occurs because the algorithmic information distortions (due to being exponential) simply outgrow any prediction or output capability of the formal theory together with the decoding algorithm, should the network be given as input \cite[Theorem 5]{Abrahao2021}.

With the purpose of developing methods for multidimensional reconstruction of received low-dimensional signals in which one does not have access to knowledge about the decoding scheme or the original multidimensional space, one should look for conditions or assumptions that avoid such distortions.
%\color{blue}We will use the term distortion and oscillation capturing possible changes in the encoding of a message or even noise indistinguishably as equivalent terms.\color{black}
One most evident approach to avoid them is to limit the algorithmic complexity of the multidimensional space itself with respect to the algorithmic complexity of the object.
As we will demonstrate in Section~\ref{sectionRetrievability}, a formalisation of this intuition is sufficient for achieving an encoding/decoding-agnostic method for reconstructing the original multidimensional spaces. 
%and the message length scales.

%\section{Theoretical framework}

\subsection{Reconstruction of messages}\label{sectionRetrievability}

In Sections~\ref{sectionAIDonDarwins} and~\ref{sectionArecibo} the values of the (prefix) algorithmic complexities $ \mathbf{K}\left( \cdot \right) $ are approximated using BDM, traditional compression, and block entropy over distinct multidimensional spaces.
%Note that the multidimensional spaces (e.g., with one, two, or three dimensions) studied in these sections may be considered low dimension, or as simple multidimensional spaces without loss of generality.
%In fact, this property will be employed to prove our theorems (see Lemma~\ref{thm1}).
%\color{blue}In the general case, each (finite and discrete) multidimensional space $ \mathcal{ S } $---no matter how complex it is---can be univocally determined by the number of dimensions, the size (or set) of each dimension, and the ordering in which each dimension appears.
%For example: 
%the unidimensional space $ \mathcal{ S }_1 $ that takes values from the natural numbers can be completely determined by only informing the length $ \left| \mathcal{ S }_1 \right| $ in $ \mathbf{O}\left( \log\left( \left| \mathcal{ S }_1 \right| \right) \right) $ bits; 
%the bidimensional space $ \mathcal{ S }_2 $ by a pair $ \left( \left| {  \mathcal{ S }_2 }_x \right| ,  \left| { \mathcal{ S }_2 }_y \right| \right) $ in $ \mathbf{O}\left( \log\left( \left| {  \mathcal{ S }_2 }_x \right| \right) \right) + \mathbf{O}\left( \log\left( \left| {  \mathcal{ S }_2 }_y \right| \right) \right) + \mathbf{O}\left( 1 \right) $ bits, 
%where $ { \mathcal{ S }_2 }_x $ is the first dimension and $ { \mathcal{ S }_2 }_y $ is the second dimension; and so on.
%
%Once the multidimensional space is encoded, one is able to uniquely decode it into sufficient information for completely determining the multidimensional space configuration.
%One way to encode a multidimensional space $ \mathcal{ S } $ is by encoding it in the form of a companion tuple $ \bm{ \tau } $ as in \cite{Abrahao2020c,Abrahao2021}.
%Nevertheless, our results hold for any arbitrarily chosen computational method to encode the multidimensional space.
%Thus, \color{black} 

Now, let $ \mathbf{K}\left( \mathcal{ S } \right) $ denote the algorithmic complexity of the arbitrarily chosen encoded form of the multidimensional space $ \mathcal{ S } $;
$ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) $ denote the algorithmic complexity of the encoded object $ y $ once the multidimensional space $ \mathcal{ S } $ is a priori known;
and $ \mathbf{K}\left( y , \mathcal{ S } \right) $ denote the algorithmic complexity of the encoding of the object $ y $ univocally embedded into the multidimensional space $ \mathcal{ S } $.
Note that there is no loss of generality in our following proofs, nor any algorithmic information distortion (except for an independent constant), in employing here the usual notation $ \mathbf{K}\left( w , z \right) $ for the joint algorithmic complexity of a string $ w $ and a string $ z $.
This is because there is an algorithm that can always encode the object $ y $ univocally embedded into the multidimensional space $ \mathcal{ S } $, if the shortest program that generates the encoding of the pair $ \left( y , \mathcal{ S } \right) $ is given as input to this algorithm;
and there is another algorithm that can always return the encoded pair $ \left( y , \mathcal{ S } \right) $, if the shortest program that generates the encoded form of the object $ y $ univocally embedded into the multidimensional space $ \mathcal{ S } $ is given as input to this algorithm.
For example, as shown in \cite{Abrahao2020c,Abrahao2021}, the composite edge set string of a multidimensional network is always computably retrievable from its characteristic string and the encoded form of its companion tuple;
and both the characteristic string and the companion tuple of a multidimensional network are computably retrievable from the composite edge set string. 

As usual, let $ f(x) = \mathbf{O}( g(x) ) $, $ f(x) = \mathbf{o}( g(x) ) $, $ f(x) = \bm{\omega}( g(x) ) $, 
%$ f(x) = \bm{\Omega}( g(x) ) $, 
and $ f(x) \sim g(x)  $ denote asymptotic dominances.

%\subsubsection{Message's object complexity is invariant to embedding}\label{sectionRobustnesstooscillations}

The following Lemma~\ref{thm1} demonstrates a sufficient condition that assures that any algorithmic information distortion, which might occur due to changing the multidimensional space into which the object should be embedded, vanishes in the limit as the object itself becomes sufficiently complex.

%Here we show that algorithmic complexity of the same message encoded in distinct multidimensional spaces does not change the overall asymptotic dominance class. 

\begin{lemma}\label{thm1}
	Let $ \mathcal{ S } $ be an arbitrary multidimensional space.
	Let $ y $ be an arbitrary object embedded into $ \mathcal{ S } $. 
	%	such that $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) = \bm{\Theta}\left( \log\left( \left| y \right| \right) \right) $ (i.e., the algorithmic complexity of the message, given that one a priori knows the original multidimensional space, is logarithmically compressible).
	Therefore, if 
	\begin{equation}
		\mathbf{K}\left( \mathcal{ S } \right) = \mathbf{o}\left( \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) \right)
	\end{equation}
	holds, then for any $ y $ with sufficiently large $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) $, we have it that
	\begin{equation}
		\mathbf{K}\left( y , \, \mathcal{ S } \right) \sim \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right)
		\text{ .}
	\end{equation}
	i.e., 
	%	joint algorithmic complexity of both the object and the multidimensional space 
	the algorithmic complexity of the object $ y $ univocally embedded into the multidimensional space $ \mathcal{ S } $
	is of the same order of the algorithmic complexity of the object in the given multidimensional space.
\end{lemma}

By \emph{logarithmically compressible} objects we refer to those objects whose encoded form can be compressed up to a logarithmic term (i.e., $ \mathbf{K}\left( x \right) \leq \mathbf{O}\left( \log\left( \left| x \right| \right) \right) $, where $ x $ is an encoded object) as a function of the size of the object.
For the purposes of the present paper, $ \left| x \right| $ is the length of the linear signal stream.

Corollary~\ref{thm2} then shows that, in case the conditions of Lemma~\ref{thm1} hold, the logarithmic compressibility of the message is preserved: 
%should the complexity of the very multidimensional space be sufficiently lower than the one of the message itself. 

\begin{corollary}\label{thm2}
	%	Let $ \mathcal{ S } $ be an arbitrary multidimensional space.
	Let $ y $ and $ \mathcal{ S } $ satisfy Lemma~\ref{thm1} such that the length of the linear signal stream $ \left| y \right| $ is sufficiently large and $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) \leq \mathbf{O}\left( \log\left( \left| y \right| \right) \right) $. 
	%	(i.e., the algorithmic complexity of the object, given that one a priori knows the original multidimensional space, is logarithmically compressible).
	Then,
	\begin{equation}
		\mathbf{K}\left( y , \, \mathcal{ S } \right)
		=
		\mathbf{O}\left( \log\left( \left| y \right| \right) \right)
		%		\text{ ,}
	\end{equation}
	holds,
	i.e., 
	the algorithmic complexity of the object univocally embedded into the multidimensional space
	%	the joint algorithmic complexity of both the object and the multidimensional space 
	remains logarithmically compressible.
\end{corollary}




\subsubsection{Robustness of non-random messages to low-complexity perturbations}\label{sectionRobustenesstoperturbations}

We shall see in Corollary~\ref{thm3.2} (following from Lemma~\ref{thm3}) that these conditions in  Corollary~\ref{thm2} also guarantee that the algorithmic complexities indeed remain invariant to low-algorithmic-complexity perturbations.
%A perturbation with low algorithmic complexity is one whose algorithmic information content is preserved under such perturbations.
%Swapping all 0's and 1's, or swapping all black and white pixels in an image are examples of low-algorithmic-complexity transformations.
%Randomly inserting or deleting an element (such as creating or destroying an edge in a graph) is also an example of low-algorithmic-complexity transformation \cite{Abrahao2021b}.

We know from \cite{Abrahao2021b} that any transformation of a finite object into another finite object is an \emph{algorithmic perturbation} $ \mathcal{P} $ (i.e., a program) that takes the former as input and outputs the latter \cite{Abrahao2021b}.
In this way, $ \mathbf{U}\big( \left< \left< y , \, \mathcal{ S } \right> , \mathcal{P} \right> \big) $ denotes the outcome $ \left( y' , \mathcal{ S }' \right) $ of the computational transformation that corresponds to the algorithmic perturbation $ \mathcal{P} $ on the object $ y $ embedded into the multidimensional space $ \mathcal{ S } $ when such transformation is run on the universal machine $ \mathbf{U} $.
%Note that an algorithmic perturbation can change either the object or the multidimensional space independently, or can change both at the same time.

\begin{lemma}\label{thm3}
	Let $ y $ and $ \mathcal{ S } $ satisfy Lemma~\ref{thm1} such that $ \left| y \right| $ is sufficiently large.
	%	Let $ \mathcal{ S } $ be an arbitrary multidimensional space.
	%	Let $ y $ be an arbitrary object embedded into $ \mathcal{ S } $ such that $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) = \bm{\Theta}\left( \log\left( \left| y \right| \right) \right) $ and 
	%	$ \mathbf{K}\left( \mathcal{ S } \right) = \mathbf{o}\left( \mathbf{K}\left( y \right) \right) $.
	Then,
	\begin{equation}
		\begin{aligned}
			\mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right)
			\sim \\
			\mathbf{K}\left( y , \, \mathcal{ S } \right) 
			\leq \\
			\mathbf{K}\Big(
			\mathbf{U}\big( \left< \left< y , \, \mathcal{ S } \right> , \mathcal{P} \right> \big) \Big) 
			+
			\mathbf{K}\left( \mathcal{P}^{ -1 } \right)
			+ \mathbf{O}(1) 
			= \\
			\mathbf{K}\Big(
			y', \mathcal{S}' \Big) 
			+
			\mathbf{K}\left( \mathcal{P}^{ -1 } \right)
			+ \mathbf{O}(1)
			%	\leq \\
			%	\mathbf{K}\Big(
			%	\mathbf{U}\big( \left< \left< y , \, \mathcal{ S } \right> , \mathcal{P} \right> \big) \Big) 
			%	+
			%	\mathbf{o}\big( \log\left( \left| y \right| \right) \big)
			%	+ \mathbf{O}(1)
			%		\text{ ,}
		\end{aligned}
	\end{equation} 
	and
	\begin{equation}
		\begin{aligned}
			\mathbf{K}\Big(
			y', \mathcal{S}' \Big) 
			= \\
			\mathbf{K}\Big(
			\mathbf{U}\big( \left< \left< y , \, \mathcal{ S } \right> , \mathcal{P} \right> \big) \Big)
			\leq \\
			\mathbf{K}\left( y , \, \mathcal{ S } \right) 
			+
			\mathbf{K}\left( \mathcal{P} \right)
			+ \mathbf{O}(1) 
			\sim \\
			\mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right)
			+
			\mathbf{K}\left( \mathcal{P} \right)
			+ \mathbf{O}(1)
			%	\leq \\
			%	\mathbf{O}\left( \log\left( \left| y \right| \right) \right)
			%	+
			%	\mathbf{K}\left( \mathcal{P} \right)
			%	+ \mathbf{O}(1)
		\end{aligned}
	\end{equation}
	hold, where: 
	$ \mathcal{P} $ is an algorithmic perturbation that transforms $ y $ embedded into $ \mathcal{S} $ into another $ y' $ embedded into another $ \mathcal{S}' $;
	and
	$ \mathcal{P}^{ -1 } $ is the respectively inverse algorithmic perturbation that transforms $ y' $ embedded into $ \mathcal{S}' $ back into $ y $ embedded into $ \mathcal{S} $.
\end{lemma}

Corollary~\ref{thm3.1} then establishes that an algorithmic perturbation can only (in the worst case scenario of high-complexity perturbations) increase, but not decrease (except for a logarithmic term), the asymptotic dominance class of the algorithmic complexity of the original message. 
This is congruent with the forthcoming Corollary~\ref{thm4.2}, which assumes the condition of $ \mathbf{K}\Big( y', \mathcal{S}' \Big) $ being sufficiently larger than $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) $ in order to guarantee a strong asymptotic dominance.


\begin{corollary}\label{thm3.1}
	Let $ y $ and $ \mathcal{ S } $ satisfy Corollary~\ref{thm2} and Lemma~\ref{thm3} such that $ \left| y \right| $ is sufficiently large. 
	%	such that
	%	\begin{equation}
	%		\mathbf{K}\left( \mathcal{ S }' \right) = \mathbf{o}\left( \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) \right)
	%		\text{ .}
	%	\end{equation}
	%	Let $ \mathcal{ S } $ be an arbitrary multidimensional space.
	%	Let $ y $ be an arbitrary message (or object) encoded in $ \mathcal{ S } $ such that $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) = \bm{\Theta}\left( \log\left( \left| y \right| \right) \right) $ and 
	%	$ \mathbf{K}\left( \mathcal{ S } \right) = \mathbf{o}\left( \mathbf{K}\left( y \right) \right) $.
	Then,
	\begin{equation}
		\begin{aligned}
			\mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right)
			-
			\mathbf{O}\big( \log\left( \left| y \right| \right) \big)
			- \mathbf{O}(1)
			%	\sim \\
			%	\mathbf{K}\left( y , \, \mathcal{ S } \right) 
			%	\leq \\
			%	\mathbf{K}\Big(
			%	\mathbf{U}\big( \left< \left< y , \, \mathcal{ S } \right> , \mathcal{P} \right> \big) \Big) 
			%	+
			%	\mathbf{K}\left( \mathcal{P}^{ -1 } \right)
			%	+ \mathbf{O}(1)
			\leq 
			\mathbf{K}\Big(
			y', \mathcal{S}' \Big) 
			%		\text{ ,}
		\end{aligned}
	\end{equation} 
	%	and
	%	\begin{equation}
	%	\begin{aligned}
	%	\mathbf{K}\Big(
	%	\mathbf{U}\big( \left< \left< y , \, \mathcal{ S } \right> , \mathcal{P} \right> \big) \Big)
	%	\leq \\
	%	\mathbf{K}\left( y , \, \mathcal{ S } \right) 
	%	+
	%	\mathbf{K}\left( \mathcal{P} \right)
	%	+ \mathbf{O}(1) 
	%	\sim \\
	%	\mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right)
	%	+
	%	\mathbf{o}\big( \log\left( \left| y \right| \right) \big)
	%	+ \mathbf{O}(1)
	%	\leq \\
	%	\mathbf{O}\left( \log\left( \left| y \right| \right) \right)
	%	+
	%	\mathbf{o}\big( \log\left( \left| y \right| \right) \big)
	%	+ \mathbf{O}(1)
	%	\end{aligned}
	%	\end{equation}
	holds, 
	where the algorithmic perturbation $ \mathcal{P} $ transforms $ y $ embedded into $ \mathcal{S} $ into another $ y' $ embedded into another $ \mathcal{S}' $.
	%	and
	%	$ \mathcal{P}^{ -1 } $ is the respective inverse algorithmic perturbation that transforms $ y' $ embedded into $ \mathcal{S}' $ back into $ y $ embedded into $ \mathcal{S} $.
\end{corollary}


In addition, in the case of low-complexity perturbations, the asymptotic dominance class of the algorithmic complexity of the perturbed---and, therefore, possibly grounded on a slightly wrong multidimensional space---message remains as logarithmically compressible as the correct one:

\begin{corollary}\label{thm3.2}
	Let $ y $ and $ \mathcal{ S } $ satisfy Corollary~\ref{thm3.1} such that $ \left| y \right| $ is sufficiently large and
	$ \mathbf{K}\left( \mathcal{P} \right) = \mathbf{O}\left( \log\left( \left| y \right| \right) \right) $.
	%	\begin{equation}
	%		\mathbf{K}\left( \mathcal{P} \right) = \mathbf{O}\left( \log\left( \left| y \right| \right) \right)
	%		\text{ .}
	%	\end{equation}
	%	such that
	%	\begin{equation}
	%		\mathbf{K}\left( \mathcal{ S }' \right) = \mathbf{o}\left( \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) \right)
	%		\text{ .}
	%	\end{equation}
	%	Let $ \mathcal{ S } $ be an arbitrary multidimensional space.
	%	Let $ y $ be an arbitrary message (or object) encoded in $ \mathcal{ S } $ such that $ \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) = \bm{\Theta}\left( \log\left( \left| y \right| \right) \right) $ and 
	%	$ \mathbf{K}\left( \mathcal{ S } \right) = \mathbf{o}\left( \mathbf{K}\left( y \right) \right) $.
	Then,
	\begin{equation}
		\begin{aligned}
			\mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right)
			-
			\mathbf{O}\big( \log\left( \left| y \right| \right) \big)
			- \mathbf{O}(1)
			\leq \\
			\mathbf{K}\Big(
			y', \mathcal{S}' \Big)
			= \\
			\mathbf{O}\left( \log\left( \left| y \right| \right) \right)
			%	+
			%	\mathbf{K}\left( \mathcal{P} \right)
			%	+ \mathbf{O}(1)
		\end{aligned}
	\end{equation}
	holds, 
	where the algorithmic perturbation $ \mathcal{P} $ transforms $ y $ embedded into $ \mathcal{S} $ into another $ y' $ embedded into another $ \mathcal{S}' $.
	%	and
	%	$ \mathcal{P}^{ -1 } $ is the respective inverse algorithmic perturbation that transforms $ y' $ embedded into $ \mathcal{S}' $ back into $ y $ embedded into $ \mathcal{S} $.
\end{corollary}


\subsubsection{Robustness to arbitrary probability measures and to perturbations of objects and multidimensional spaces}\label{sectionRobustnesstoprobabilitiesandperturbations}



Let $ \mathbf{P} \left[ \cdot , \cdot \right] $ be an arbitrarily chosen (computable) probability measure on the space of all possible (computably constructible) objects $ y $ embedded into a multidimensional space $ \mathcal{ S } $.
For example, $ \mathbf{P} \left[ y , \mathcal{ S } \right] $ may be any probability measure value that is calculated for $ y $ embedded into $ \mathcal{ S } $ by an arbitrarily chosen formal theory $ \mathbf{F} $, encoding-decoding scheme, and decoding algorithm.

%Let $ \mathbf{P}_{ \mathbf{U} } \left[ \cdot \right] $ denote the universal a priori probability of an arbitrary event.
We know from the algorithmic coding theorem in algorithmic information theory that
\begin{equation}
	\mathbf{K}\left( x \right) = 
	- \log\left( \mathbf{P}_{ \mathbf{U} } \left[ \text{``event } x \text{ occurs''} \right] \right) \pm \mathbf{O}( 1 )
	= 
	- \log\left( \mathbf{m}\left( x \right) \right) \pm \mathbf{O}( 1 )
	\text{ ,}
\end{equation}
holds,
where:
%$ p $ denotes a program running on the universal prefix Turing machine $ \mathbf{U} $ that outputs $ x $;
%$ \left| p \right| $ is the length of a program $ p $;
$ \mathbf{m}\left( \cdot \right) $ is a maximal computably enumerable semimeasure; 
and $ \mathbf{P}_{ \mathbf{U} } \left[ \cdot \right] $ denotes the \emph{universal a priori probability} of an arbitrary event.
$ \mathbf{P}_{ \mathbf{U} } $ can be understood as the probability of randomly generating (by an i.i.d. stochastic process) a prefix-free (or self-delimiting) program that outputs $ x $. 
A computably enumerable semimeasure $ \mathbf{m}\left( \cdot \right) $ is said to be \emph{maximal} if, for any other computably enumerable semimeasure $ \mu\left( \cdot \right) $ with domain defined for possible encoded objects, where $ \sum\limits_{ x \in \left\{ 0 , 1 \right\}^* } \mu\left( x \right) \leq 1 $, there is a constant $ C > 0 $ (which does not depend on $ x $) such that, for every encoded object $ x $,
%\begin{equation*}
$ \mathbf{m}\left( x \right) \geq C \, \mu\left( x \right)\text{ .} $
%\end{equation*} 
%In particular, we know from AIT that any \emph{computable} measure (or semimeasure) $ \mu'\left( \cdot \right) $ of the infinite discrete space of all encoded finite objects loses the property of being maximal \cite{Li1997}, unlike the computably enumerable semimeasure $ \mathbf{m}\left( \cdot \right) $ which is maximal.
Also note that the algorithmic coding theorem applies analogously to the conditional algorithmic complexity $ \mathbf{K}( z \, | w ) $.
This guarantees that 
there is $ C > 0 $ such that
$ \mathbf{m}\left( y , \mathcal{ S } \right) \geq C \, \mathbf{P} \left[ y , \mathcal{ S } \right] $, which is one of the key steps that enables one to prove the forthcoming Theorem~\ref{thm4-1}.


We can explore the power and universality of the algorithmic coding theorem in order to investigate a lower bound for the universal (probability) distribution in comparison to an arbitrary probability measure $ \mathbf{P} $. 
A chosen probability measure $ \mathbf{P} $ may or may not assign a higher probability value to a correct multidimensional space.
Thus, it is important to evaluate how much a universal approach such as the universal a priori probability can be wrong in the asymptotic limit.
Indeed, the algorithmic coding theorem together with Lemma~\ref{thm3} enables the following Theorem~\ref{thm4-1} to guarantee that 
the multiplicative error of 
\[ 
\mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right] 
=
\mathbf{P}_{ \mathbf{U} } \left[ y \middle\vert \mathcal{S} \right] 
\sim
\mathbf{P}_{ \mathbf{U} } \left[ y , \, \mathcal{S} \right]
\]
with respect to 
\[ \mathbf{P} \left[ y' , \mathcal{ S }' \right] \]
is bounded by the algorithmic probability of the inverse algorithmic perturbation $ \mathcal{P}^{ -1 } $ itself.
This means that the easier, or less complex, it is to retrieve $ y $ embedded into $ \mathcal{S} $ from $ y' $ embedded into $ \mathcal{S}' $ the smaller the multiplicative error of employing $ \mathbf{P}_{ \mathbf{U} } $ instead of the arbitrarily chosen $ \mathbf{P} $.


\begin{theorem}\label{thm4-1}
	Let $ y' $ be an arbitrary object and $ \mathcal{ S }' $ be an arbitrary multidimensional space.
	Let an arbitrary $ y $ embedded into arbitrary $ \mathcal{ S } $  satisfy Lemma~\ref{thm3}.
	Then,
	\begin{equation}
	\begin{aligned}
	\mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right]
	\geq \\
	\frac{ 1 }{ 2^{ \mathbf{K}\left( y , \, \mathcal{ S } \right) \pm \mathbf{O}( 1 ) } }
	\geq \\ 
	\frac{ 1 }{ 2^{ \mathbf{K}\left( y' , \, \mathcal{ S }' \right)
			+ \mathbf{K}\left( \mathcal{P}^{ -1 } \right)	 \pm \mathbf{O}( 1 ) } }
	\geq \\
	\frac{ \mathbf{P} \left[ y' , \mathcal{ S }' \right] }{ 2^{ \mathbf{K}\left( \mathcal{P}^{ -1 } \right) 
			+ \mathbf{O}( 1 ) } }
	\end{aligned}
	\end{equation}
	holds,
	where
	$ \mathcal{P}^{ -1 } $ is the inverse algorithmic perturbation that transforms $ y' $ embedded into $ \mathcal{S}' $ back into $ y $ embedded into $ \mathcal{S} $.
%	That is, the multiplicative error of 
%	\[ \mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right] \]
%	with respect to 
%	\[ \mathbf{P} \left[ y , \mathcal{ S }' \right] \]
%	increases asymptotically slower than the algorithmic probability of $ y $ alone.
\end{theorem}

Now, by using $ \mathbf{P}_{ \mathbf{U} } $ instead of $ \mathbf{P} $ together with the compressibility assumption, Corollary~\ref{thm4} shows that one can only wrongly assign a probability for $ y $ up to $ 2^{ \mathbf{o}\left( \mathbf{K}\left( y  \right) \right)  + \mathbf{O}( 1 ) } $ less likely than $ \mathbf{P} $ can (except for the algorithmic probability of reverting the perturbation that might have introduced noise into $ y $, perturbation which results in $ y' $).
This is a multiplicative term that does not depend on $ \mathcal{ S } $ nor $ \mathcal{ S }' $.
And such a lower bound holds whether $ \mathcal{ S } = \mathcal{ S }' $ or not, i.e., whether $ \mathcal{ S }' $ is the correct multidimensional space $ \mathcal{ S } $, or any other wrong one.

\begin{corollary}\label{thm4}
	Let $ y' $ be an arbitrary object and $ \mathcal{ S }' $ be an arbitrary multidimensional space.
	Let an arbitrary $ y $ embedded into arbitrary $ \mathcal{ S } $  satisfy Lemma~\ref{thm3}.
	Then,
	%	there is a constant $ C $ (which does not depend on $ y $, $ \mathcal{ S } $ nor $ \mathcal{ S }' $) such that for every $ \mathcal{ S }' $, the inequality
	\begin{equation}
		\begin{aligned}
			\mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right]
			\geq \\
			\frac{ 1 }{ 2^{ \mathbf{K}\left( y , \, \mathcal{ S } \right) \pm \mathbf{O}( 1 ) } }
			\geq \\
			%	C \, 
			\frac{ 1 }{ 2^{ \mathbf{K}\left( y' , \, \mathcal{ S }' \right)
			+ \mathbf{K}\left( y \middle\vert y' 
			, \mathcal{S} 
			\right)	
			+ \mathbf{o}\left( \mathbf{K}\left( y \, \middle\vert \, \mathcal{ S } \right) \right) \pm \mathbf{O}( 1 ) } }
			%	\geq \\
			%	\frac{ 1 }{ 2^{ \mathbf{K}\left( y \middle\vert \mathcal{ S }' \right)
			%			+ \mathbf{K}\left( \mathcal{ S }' \right) + \mathbf{o}\left( \mathbf{K}\left( y \right) \right) \pm \mathbf{O}( 1 ) } }
			\geq \\
			\frac{ \mathbf{P} \left[ y' , \mathcal{ S }' \right] }{ 2^{ 
					\mathbf{K}\left( y \middle\vert y' 
					, \mathcal{S} 
					\right) 
					+ \mathbf{o}\left( \mathbf{K}\left( y \middle\vert \mathcal{S} \right) \right) 
					%			+ \mathbf{K}\left( \mathcal{ S }' \right) 
					+ \mathbf{O}( 1 ) } }
			%		\big)
		\end{aligned}
	\end{equation}
	holds.
	%	$ \mathbf{P}_{ \mathbf{U} } \left[ \cdot \right] $ denotes the universal a priori probability of an arbitrary event,
	%	$ \mathbf{P} \left[ \cdot \right] $ denotes any other computable probability measure of the space of all possible (computably constructible) objects;
	%	$ \left< y , \, \mathcal{ S } \right> $ is the original message (with $ y $ embedded in $ \mathcal{ S } $);
	%	$ y' $ denotes either a perturbed (i.e., $ y' \neq y $) or non-perturbed (i.e., $ y' = y $) message;
	%	and 
	%	$ \mathcal{ S }' $ is any distinct multidimensional space from $ \mathcal{ S } $ (i.e., 
	%	$ \mathcal{ S }' \neq \mathcal{ S } $.
	That is, the multiplicative error of 
	\[ \mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right] \]
	with respect to 
	\[ \mathbf{P} \left[ y' , \mathcal{ S }' \right] \]
	increases asymptotically slower than the algorithmic probability of $ y $,
	except for the algorithmic probability of generating $ y $ from $ y' $ in the given $ \mathcal{ S } $.
	%	\[ \mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs''}\right] \text{ .} \]
\end{corollary}


Therefore, along with Corollary~\ref{thm3.1}, one can show in Corollary~\ref{thm4.1} that the assumption of logarithmic compressibility ensures that searching for the multidimensional space that minimises the algorithmic complexity can only produce errors that vanishes linearly in the asymptotic limit in comparison to the algorithmic complexity of the object itself.
This holds because in this case the multiplicative error of employing the universal distribution instead of any other arbitrarily chosen probability measure (which assigns higher likelihood to the multidimensional space $ \mathcal{ S }' $) becomes strongly dominated by the algorithmic probability of the object $ y $ itself, which in turn decreases linearly with the size of $ y $.

\begin{corollary}\label{thm4.1}
	Let 
%	$ y' $ be an arbitrary object and 
	$ \mathcal{ S }' $ be an arbitrary multidimensional space.
	Let an arbitrary $ y $ embedded into arbitrary $ \mathcal{ S } $  satisfy Corollary~\ref{thm3.1}.
	%	Let $ y $ embedded into arbitrary $ \mathcal{ S }' $ satisfy Lemma~\ref{thm1}.
	Then, there is a constant $ C $ such that
	the multiplicative error of 
	\[ \mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right] \]
	with respect to 
	\[ \mathbf{P} \left[ y , \mathcal{ S }' \right] \]
	increases asymptotically slower than 
	\[ \frac{ C }{ \left| y \right| } \text{ .} \]
\end{corollary}

%In particular, Corollaries~\ref{thm3.1} and~\ref{thm4.1} also predict the behavior of the images in Fig.~\ref{moreexps}. 
%The higher the algorithmic complexity of the object itself in comparison to the algorithmic complexity of the multidimensional space itself, such as in the sixth figure (i.e., the bottom rightmost image) in Fig.~\ref{moreexps}, the less impactful are the algorithmic complexity perturbations from distinct configurations as the result of different partitions that does not change the algorithmic complexity and (algorithmic) information content of the object itself. 
%Thus, the clearer and more distinctive the downward spikes are when one finds the partition in which the object itself has the least algorithmic complexity among the other ones. 

\subsubsection{Bias toward low complexity in zero-knowledge one-way communication}\label{sectionZero-knowledge}

So far, we have investigated the worst-case scenarios 
%for the probability discrepancies when the algorithmic complexity for the wrong multidimensional space $ \mathcal{ S }' $ is in the same ballpark of the algorithmic complexity of the original multidimensional case $ \mathcal{ S } $ (i.e., when they differ by a term strongly dominated by the algorithmic complexity of the object).
%In these scenarios 
in which $ \mathbf{P}_{ \mathbf{U} } $ may be underestimating the probability measure $  \mathbf{P} \left[ y' , \mathcal{ S }' \right] $ of your choice up to a multiplicative term lower bounded by $ 2^{ - \mathbf{K}\left( \mathcal{P}^{ -1 } \right) 
		- \mathbf{O}( 1 ) }  $.
However, as indicated by our empirical results, we shall see in Corollary~\ref{thm4.2} that the original multidimensional space with the lowest algorithmic complexity can be set apart from the wrong ones, should the wrong ones sufficiently move (i.e., transform or perturb) the object and/or the multidimensional space toward randomness in comparison to $ \mathbf{K}\left( y \middle\vert \mathcal{S} \right) $. 
In addition, from the results demonstrated in the previous Section~\ref{sectionRobustnesstoprobabilitiesandperturbations}, note that this phenomenon holds for any arbitrarily chosen computable probability measure $  \mathbf{P} $ as long as the algorithmic complexity of the object $ y $ becomes sufficiently large. 


\begin{corollary}\label{thm4.2}
	Let an arbitrary $ y $ embedded into arbitrary $ \mathcal{ S } $  satisfy Corollary~\ref{thm3.2}.
	Let $ y' $ be an arbitrary object and $ \mathcal{ S }' $ be an arbitrary multidimensional space such that 
	\begin{equation}
		\mathbf{K}\left( y' , \mathcal{ S }' \right) 
		= 
		\bm{ \omega }\big( \mathbf{K}\left( y \middle\vert \mathcal{S} \right) \big)
		\text{ .}
	\end{equation}
	Then, 
%	for sufficiently large $ \left| y \right| $,
	\begin{equation}
		\begin{aligned}
			\mathbf{P} \left[ y' , \mathcal{ S }' \right]
			=
			\mathbf{o}\left( \mathbf{P}_{ \mathbf{U} } \left[ \text{``event }  y \text{ occurs in a given } \mathcal{ S } \, \text{''}\right] \right)
%			\text{ .} 
		\end{aligned}
	\end{equation}
	holds.
\end{corollary}

In summary, Corollary~\ref{thm4.2} tells us that decoding the message in the original multidimensional space $ \mathcal{ S } $ chosen from the one in which the message has the lowest algorithmic complexity (i.e., highest algorithmic probability) becomes eventually more probable to be correct than decoding the message in an distinct multidimensional space $ \mathcal{ S }' $, with or without noise. 
The key idea of Corollary~\ref{thm4.2} is that the larger the algorithmic complexity of the closer-to-randomness message candidate in the wrong multidimensional space with respect to the algorithmic complexity of the message in the correct multidimensional space, the larger the probability of the correct multidimensional space with respect to the probability of the wrong closer-to-randomness multidimensional space. 
Thus, finding the multidimensional space that minimises the algorithmic complexity the most, eventually increases the probability of one finding the correct one in comparison to the probability of finding the wrong closer-to-randomness one, even by assuming an arbitrarily different probability measure as starting point.

The condition for this to hold is that the algorithmic complexity of the object (with or without noise) in the wrong multidimensional space should increase (i.e., move the message toward algorithmic randomness) sufficiently fast in comparison to the algorithmic complexity of the message in a correct multidimensional space.
In consonance with the discussion in Section~\ref{sectionCompressionandrandomness} and in addition to the assumption (see Lemma~\ref{thm1}) of relatively low algorithmic complexity of the multidimensional space with respect to that of the object, one can assume a message should contain a large amount of redundancies, and thus should be highly compressible with respect to its linear signal stream length $ \left| y \right| $, such as the case in which $ \mathbf{K}\left( y \middle\vert \mathcal{S} \right) = \mathbf{O}\left( \log\log\left( \left| y \right| \right) \right) $ holds (which also satisfies the condition in Corollary~\ref{thm2}).
In this manner, most low-complexity perturbations (see also Section~\ref{sectionIntro}) that satisfy the condition in Corollary~\ref{thm3.2} will transform the object and/or partition by sufficiently moving it toward algorithmic randomness so that the conditions of our final Corollary~\ref{thm4.2} become satisfied.
This is because we know from AIT that the number of low-complexity perturbations for which $ \mathbf{K}\left( y' , \mathcal{ S }' \right) 
= 
\bm{ \omega }\big( \log\log\left( \left| y \right| \right) \big) $ holds strongly dominates the number of those for which one has it that $ \mathbf{K}\left( y' , \mathcal{ S }' \right) 
= 
\mathbf{O}\left( \log\log\left( \left| y \right| \right) \right) $.



\section{Conclusions}

We have introduced and demonstrated a practical and reliable method for reconstructing properties of non-random signals or messages related to their physical properties that serve the receiver to decode the embedded (multidimensional) space into which the original message was originally sent.  This related information theory to geometry and topology by means of compression and algorithmic probability.

The method is based on algorithmic information dynamics (AID) which uses the tools of classical information theory and algorithmic information theory (and their combination) and perturbation analysis, with algorithmic complexity values approximated and combined with Shannon entropy by using the block decomposition method (BDM). 
%More specifically, this method is developed based on the discovery that the native dimension in which non-random data is originally produced and encoded can be inferred from the data itself. 

We have shown signal-amplification techniques that help the methods to get a better signal-to-noise reading and investigated how the methods perform in the face of noise.

We have shown that these agnostic methods can be extended and used for bio and technosignature detection and signal deconvolution, and how these algorithmic-information-based methods can be used to avoid encoding biases in signal detection. 

%By using 3 different methods of measuring complexity (BDM, block entropy, and compression), we found that BDM is the most reliable measure that indicates a message's original native dimension when applied to a multiplicity of data. 

This work advances a practical and theoretical framework that relates information, Shannon entropy, complexity, and  semantics that can be extended to multiple uses in signal deconvolution, cryptography, and coding theory. 

We are convinced that this application is only an example of how this mathematical and computational framework can be used in all areas of inverse problems as an approach to a large universal generative model able to instantiate a strong form of super-human Artificial General Intelligence from first principles here in application to some specific cases of message reconstruction.

%Because we cannot assume any original encoding for any extraterrestrial signal, whether chemical, visual, spatial, or temporal, it is critical that astrobiologists adopt a signal detection method that is independent of any assumed encoding or decoding scheme for some incoming message.
%We should not a priori assume that any other type of intelligence has any bias toward a particular multidimensional space into which we already are used to giving meanings to the objects.
%In other words, we should not a priori assume that the multidimensional space of an encoded message in which we have zero knowledge about is isomorphic to ours.
%Otherwise, such assumptions would constitute either an anthropomorphic bias or a bias toward our current state of technology and knowledge about physics.
%As shown in this article, algorithmic information dynamics (AID) offers a way out of this kind of biases by estimating algorithmic complexity over a large variety of possible encodings and perturbed multidimensional spaces, so it is possible to recover the original physical meaning from a message that would otherwise be lost.


%\section{Methods}\label{sectionMethods}

\section*{Funding}

Felipe S. Abrah\~{a}o acknowledges the partial support from São Paulo Research Foundation (FAPESP), grant 2021/14501-8.

\section*{Author Contributions:} HZ conceived the theory, methods and experiments, HZ performed most of the experiments with support from AA. HZ and FA developed the theoretical framework.

\clearpage

\bibliographystyle{plain}
\bibliography{references.bib}

\end{document}

