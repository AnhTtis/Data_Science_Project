% \begin{table}[t]
%     \tablestyle{2pt}{1.05}
    
%     \centering
%     %\resizebox{1\columnwidth}{!}{
%     \begin{tabular}{@{}l|ccccccc}
%     	\toprule
%             \multicolumn{8}{c}{Untrimmed Spatial-Temporal Grounding}
%     	\toprule
%     	\multicolumn{1}{c}{} & \multicolumn{7}{c}{GroundingYouTube}  \\ 
%     	\cmidrule(lr){2-8} 
%     	\multirow{2}{*}{\textbf{Method}}    & \multirow{2}{*}{IoU+Point} &\multicolumn{6}{c}{mAP}  \\ 
%     	                                    &  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & 0.1:0.5 \\ 
%     	\midrule
%     	MIL-NCE \citep{miech2020end} & 4.67 & 33.94 & 25.16 & 12.65 & 3.42 & 0.41  & 15.11 \\
%          CoMMA* \citep{tan2021look}   & 1.02 & 2.18  & 1.72 & 1.11 & 0.93 & 0.37 & 1.26\\
%              %Ours S3D                         & 7.78 & 39.43 & 31.47 & 19.38 & 9.14 & 3.79  & 20.64  \\
%              Ours S3D                      & 9.12 & 42.70  & 35.49 & 25.16 & 16.22 & 10.05  & 25.92 \\
%              \midrule
%             CLIP \citep{radford2021learning}  & 3.59 & 29.54  & 22.15 & 9.16 & 2.48 & 0.39 & 12.74 \\
%             CoMMA$\dagger$              & 1.68 & 3.51 & 2.32 & 1.88 & 0.99 & 0.40 & 1.82 \\
%     	   Ours                        & 10.09 & 42.81  & 36.05 & 25.84 & 17.10 & 11.35  & 26.63 \\
%             \midrule
%             GLIP \citep{li2022grounded}      &  1.24 & 2.83 & 2.10 & 1.52 & 0.96 & 0.37 & 1.56 \\
%     	\bottomrule
%     \end{tabular}
%     %\vspace{+0.3cm}
%     \caption{\textbf{Spatio-temporal localization on full videos}. Since our model learned global representations encoding temporal information and spatial correspondences across modalities, it achieves the best performance in spatio-temporal evaluation.
%     % \caption{\textbf{Spatial-temporal localization on full videos}. Our model learned both global representation which encodes temporal information. It also learned spatial correspondence across modalities, which ends up with the best performance in spatial temporal evaluation.
%     \label{tab:st_long}
%     %\vspace{-0.7cm}
%     }
%     %}
% \end{table}
\begin{table*}[t]
    \tablestyle{4pt}{1.05}
    \tiny
    \centering
    \resizebox{2\columnwidth}{!}{
    \begin{tabular}{@{}l|ccccccccccc}
    	\toprule
    	\multicolumn{5}{c}{} &\multicolumn{7}{c}{GroundingYoutube}  \\ 
    	\cmidrule(lr){6-12} 
    	\multirow{2}{*}{\textbf{Method}}  & \multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{DataSet}} & \multirow{2}{*}{\textbf{Supervision}} & \multirow{2}{*}{\textbf{Modality}}  & \multirow{2}{*}{IoU+Point} &\multicolumn{6}{c}{mAP}  \\ 
    	  & & & & & & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & 0.1:0.5 \\ 
    	\midrule
    	
         CoMMA$\dagger$ \citep{tan2021look}  & S3D &HT250K & Self &VT& 1.02 & 2.18  & 1.72 & 1.11 & 0.93 & 0.37 & 1.26\\
         MIL-NCE \citep{miech2020end} & S3D* &HT100M & Self &VT& 4.67 & 33.94 & 25.16 & 12.65 & 3.42 & 0.41  & 15.11 \\
             %Ours S3D                         & 7.78 & 39.43 & 31.47 & 19.38 & 9.14 & 3.79  & 20.64  \\
             %\midrule
             Ours                   & S3D &HT100M & Self &VT  & \textbf{9.12} & \textbf{42.70}  & \textbf{35.49} & \textbf{25.16} & \textbf{16.22} & \textbf{10.05}  & \textbf{25.92} \\
             \midrule
            GLIP \citep{li2022grounded}   & Swin-L*  & Cap24M & Weak & IT &  1.24 & 2.83 & 2.10 & 1.52 & 0.96 & 0.37 & 1.56 \\
            CoMMA$\ddagger$   \citep{tan2021look} & CLIP &HT100M& Self & VT & 1.68 & 3.51 & 2.32 & 1.88 & 0.99 & 0.40 & 1.82 \\
            CLIP \citep{radford2021learning}& CLIP &HT100M & Self & IT& 3.59 & 29.54  & 22.15 & 9.16 & 2.48 & 0.39 & 12.74 \\
            RegionCLIP \citep{zhong2022regionclip}   & ResNet-101*  & CC3M & Weak & IT &  5.65 & 35.65 & 27.43 & 15.69 & 4.31 & 0.86 &  16.78 \\
            %\midrule
    	   Ours       & CLIP &HT100M & Self &VT  &10.09 & 42.81  & 36.05 & 25.84 & 17.10 & 11.35  & 26.63 \\
               Ours                    & CLIP* &HT100M & Self &VT  & \textbf{11.53} & \textbf{43.64}  & \textbf{36.94} & \textbf{26.78} & \textbf{19.45} & \textbf{14.61}  & \textbf{28.26} \\
               \midrule
               MIL-NCE(temp.)+RegionCLIP(spa.)   &  -  & - & - & VT  & 9.21  & 40.54  & 34.97  & 22.38  &  13.79 & 9.18  &  22.33  \\
    	\bottomrule
    \end{tabular}}
    %\vspace{+0.3cm}
    \caption{\textbf{Spatio-temporal grounding on GroundingYouTube full videos}.   
The proposed model learns global representations encoding global information and spatial correspondences across modalities, achieving a better performance in spatio-temporal evaluation compared to models trained on only spatial or temporal grounding. 
(V: video, I: image, T: text.) $^*$ indicates finetuned backbone.
    % \caption{\textbf{Spatial-temporal localization on full videos}. Our model learned both global representation which encodes temporal information. It also learned spatial correspondence across modalities, which ends up with the best performance in spatial temporal evaluation.
    \label{tab:st_long}
    \vspace{-0.3cm}
    }
    %}
\end{table*}