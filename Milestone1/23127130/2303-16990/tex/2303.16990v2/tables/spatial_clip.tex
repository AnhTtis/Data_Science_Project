% \begin{table}[t]
%     \tablestyle{2pt}{1.05}
    
%     \centering
%     %\resizebox{1\columnwidth}{!}{
%     \begin{tabular}{@{}l|ccccccc}
%     	\toprule
%     	\multicolumn{1}{c}{} &\multicolumn{7}{c}{GroundingYouTube}  \\ 
%     	\cmidrule(lr){2-8} 
%     	\multirow{2}{*}{\textbf{Method}}    & \multirow{2}{*}{IoU+Point} &\multicolumn{6}{c}{mAP}  \\ 
%     	                                    &  & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & 0.1:0.5 \\ 
%     	\midrule
%     	CoMMA* \citep{tan2021look}   & 1.10 & 7.46  & 5.84 & 4.20 & 2.65 & 1.53  & 4.93  \\
%     	MIL-NCE \citep{miech2020end} & 12.41 & 45.91 & 32.33 & 15.35 & 3.70 & 2.56   & 19.54  \\
%             Ours S3D                    & 19.46 & 51.95 & 40.31 & 26.81 & 16.27 & 7.81  & 28.63 \\
%     	Ours S3D w/ audio           & 22.35 & 55.82 & 45.71 & 32.33 & 20.29 & 11.29  & 25.92  \\
%             \midrule
%             CLIP \citep{radford2021learning}                &  &  &  &  &  &  &  \\
%             CoMMA$\dagger$                 &  &  &  &  &  &  &  \\
            
%     	Ours                        & 9.12 & 42.70  & 35.49 & 25.16 & 16.22 & 10.05  & 25.92 \\
%             \midrule
%             GLIP \citep{}                &  &  &  &  &  &  &  \\
%     	\bottomrule
%     \end{tabular}
%     %\vspace{+0.3cm}
%     \caption{\textbf{Spatial-temporal localization on short video clips.} The same evaluation is repeated on GroundingYouTube, except with shorter clips.
%     % \caption{\textbf{Spatial-temporal localization on short video clip}. Our model learned both global representation which encodes temporal information. It also learned spatial correspondence across modalities, which ends up with the best performance in spatial temporal evaluation.
%     \label{tab:st_clip}
%     %\vspace{-0.7cm}
%     }
%     %}
    
% \end{table}

\begin{table*}[t]
    \tablestyle{4pt}{1.05}
    \scriptsize
    \centering
    %\resizebox{1\columnwidth}{!}{
    \begin{tabular}{@{}l|ccccccccccc}
    	\toprule
    	\multicolumn{5}{c}{} &\multicolumn{7}{c}{GroundingYoutube}  \\ 
    	\cmidrule(lr){6-12} 
    	\multirow{2}{*}{\textbf{Method}}  & \multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{DataSet}} & \multirow{2}{*}{\textbf{Supervision}} & \multirow{2}{*}{\textbf{Modality}}  & \multirow{2}{*}{IoU+Point} &\multicolumn{6}{c}{mAP}  \\ 
    	  & & & & & & 0.1 & 0.2 & 0.3 & 0.4 & 0.5  & 0.1:0.5 \\ 
    	\midrule
    	
         CoMMA* \citep{tan2021look}  & S3D-word2vec &HT250K & Self &VT & 1.10 & 7.46  & 5.84 & 4.20 & 2.65 & 1.53  & 4.93  \\
         MIL-NCE \citep{miech2020end} & S3D-word2vec &HT100M & Self &VT& 12.41 & 45.91 & 32.33 & 15.35 & 3.70 & 2.56   & 19.54  \\
             %Ours S3D                         & 7.78 & 39.43 & 31.47 & 19.38 & 9.14 & 3.79  & 20.64  \\
             %\midrule
             Ours                   & S3D-word2vec &HT200K & Self &VT  & 19.46 & 51.95 & 40.31 & 26.81 & 16.27 & 7.81  & 28.63 \\
             \midrule
            CoMMA$\dagger$   \citep{tan2021look} & CLIP &HT200M& Self & VT & 2.64 & 8.94 & 6.89 & 5.47 & 4.18 & 2.67 & 5.63 \\
            CLIP \citep{radford2021learning}& CLIP &HT200K & Self & IT&  11.34 & 43.28 & 30.64 & 11.20 & 3.10 & 1.94 & 18.03 \\
            RegionCLIP \citep{zhong2022regionclip}   & ResNet-101  & CC3M & Weak & IT & 17.42 & 51.86 & 40.23 & 26.10 & 15.23 & 7.29 & 28.14 \\
            %\midrule
            GLIP \citep{li2022grounded}   & Swin-L  & Cap24M & Weak & IT & 18.15  & 52.61 & 41.83 & 26.93 & 17.23 & 8.46 & 29.41 \\
    	   Ours                    & CLIP &HT200K & Self &VT  & 20.81 & 53.24 & 42.96 & 29.17 & 20.36 & 11.84 & 31.51 \\
            \midrule
            {\color{gray}TubeDETR \citep{yang2022tubedetr}}    &  {\color{gray}MDETR} & {\color{gray}Vid-STG} & {\color{gray} Full} & {\color{gray}VT} & {\color{gray} 26.43 }    &   {\color{gray} 63.47} & {\color{gray} 50.95 } & {\color{gray} 38.23} & {\color{gray} 28.31 } & {\color{gray} 19.34 } & {\color{gray} 40.06 } \\
            {\color{gray}STCAT \citep{jin2022embracing} }    &  {\color{gray}ResNet-101} & {\color{gray}Vid-STG} & {\color{gray} Full} & {\color{gray}VT} & {\color{gray} 27.84 }    &   {\color{gray} 64.96 } & {\color{gray} 52.13} & {\color{gray} 40.61 } & {\color{gray} 30.49  } & {\color{gray} 20.55 } & {\color{gray} 41.75 } \\
    	\bottomrule
    \end{tabular}
    %\vspace{+0.3cm}
    \caption{\textbf{Single-action spatio-temporal grounding in short videos.} We compare spatio-temporal grounding approaches based on single phrase grounding. To this end, we construct a clip-level evaluation based on the action segments of GroundingYouTube, where each action segment varies from 9 sec to 60 sec. We append video segments before and after the annotated action with the same time length of the action step to form the final video clip. This allows us to directly compare with supervised spatio-temporal grounding methods \citep{yang2022tubedetr,jin2022embracing}.
    % \caption{\textbf{Spatial-temporal localization on full videos}. Our model learned both global representation which encodes temporal information. It also learned spatial correspondence across modalities, which ends up with the best performance in spatial temporal evaluation.
    \label{tab:st_clip}
    \vspace{-0.3cm}
    }
    %}
    
\end{table*}