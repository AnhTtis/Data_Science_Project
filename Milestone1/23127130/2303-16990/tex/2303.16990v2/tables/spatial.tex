\begin{table*}[h]
    \tablestyle{7pt}{1.05}
    \tiny
    \centering
    \resizebox{2\columnwidth}{!}{
    \begin{tabular}{@{}l| cccc | c |c c| c c | c c }
    	\toprule
    	\multicolumn{4}{c}{} & \multicolumn{1}{c}{ } & \multicolumn{1}{c}{YC-Inter} & \multicolumn{2}{c}{GroundingYT}  & \multicolumn{2}{c}{V-HICO}   & \multicolumn{2}{c}{Daly}\\ 
    	\cmidrule(lr){6-6} \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-12}  
    	Method  & Backbone &Data&Super.&Mod.& Acc &  Acc & mAP &  Acc & mAP  &  Acc & mAP \\ 
    	\midrule
        MIL-NCE \citep{miech2020end} & S3D* &HT100M & Self &VT& 23.67  & 27.45  & 8.21 & 12.65 & 11.23 & 13.84 & 24.23 \\
    	CoMMA$\dagger$ \citep{tan2021look} & S3D &HT250K & Self &VT& 48.63   & 47.68 & 23.38 & 40.97 & 21.45 & 54.48 & 33.39 \\
        %\midrule
        Ours                       & S3D &HT100M & Self &VT & \textbf{53.98}   & \textbf{60.62} & \textbf{44.93} & \textbf{44.32} & \textbf{24.31} & \textbf{66.35} & \textbf{45.93} \\
         \midrule
         CLIP   \citep{radford2021learning}            & CLIP&HT100M & Self &IT &    14.10    & 12.50  & 3.49 &  29.23 & 12.51  & 18.02 & 27.28  \\
         CoMMA$\ddagger$  \citep{tan2021look}            & CLIP  &HT100M & Self &VT&   52.65     & 47.56 & 36.42 & 55.20 &  34.54& 61.06 & 44.37  \\
             RegionCLIP   \citep{zhong2022regionclip}            & RN50x4* & CC3M & Weak &IT &   51.56     &   52.84 &  23.42 & 57.92 & 37.82 & 67.12 & 48.62 \\
            GLIP   \citep{li2022grounded}            & Swin-L*&Cap24M & Weak &IT &   52.84      &   53.62 & 24.73 & \textbf{66.05} & 41.17 & - & - \\
            %\midrule
            Ours         & CLIP &HT100M & Self &VT& 57.10    &   55.49 & 43.12 & 60.71& 39.28 & 70.08 & 50.56 \\
            Ours                       & CLIP* &HT100M & Self &VT& \textbf{58.35}    &   \textbf{56.98} & \textbf{45.32} & 62.34& \textbf{41.56} & \textbf{71.35} & \textbf{52.78} \\
            %V-HICO   \citep{}            &  Faster R-CNN &  -      &  - & - & & 67.21 & - & - \\
            \midrule
            {\color{gray}TubeDETR \citep{yang2022tubedetr}}    &  {\color{gray}MDETR} & {\color{gray}Vid-STG} & {\color{gray} Full} & {\color{gray}VT} & {\color{gray}51.63}    &   {\color{gray}53.24} & {\color{gray} 41.76} & {\color{gray}63.23} & {\color{gray}40.87 } & {\color{gray}84.21} & {\color{gray} 62.98} \\
            {\color{gray}STCAT \citep{jin2022embracing}}    &  {\color{gray}ResNet-101} & {\color{gray}Vid-STG} & {\color{gray} Full} & {\color{gray}VT} & {\color{gray}54.47}    &   {\color{gray} 55.90} & {\color{gray}44.21 } & {\color{gray}65.34} & {\color{gray} 41.10 } & {\color{gray}85.42} & {\color{gray} 63.94} \\
    	\bottomrule
    \end{tabular}
    }
    \vspace{-0.2cm}
    \caption{\textbf{Video spatial grounding}. We evaluate the accuracy of the pointing game and the mean average precision. 
    We listed CNN-based methods on top and transformer-based methods in the middle. 
    Models learning global representations (MIL-NCE, CLIP) don't perform well on localization tasks, while our model outperforms other grounding methods. $^*$ indicates finetuned backbone.
    %Models learning global representations (MIL-NCE, CLIP) don't perform well on localization tasks, while our model outperforms other grounding methods. %We listed CNN-based methods on top and transfomer-based methods at the bottom. 
    %(Mod. indicates the modality used, where V: video, I: image, T: text. Super. indicates supervision.)
    %Our method generalized well on both video and image architectures. 
    % Daly GLIP is not workable since every class is action. OOV. V-HICO dataset the CLIP  generalized better to OOV, while word2vec getting worse performance. \bc{maybe we can add supervision: weakly, SSL} \bc{add pretraining data}
    \label{tab:spatial}
    \vspace{-0.5cm}
    }
   
    
\end{table*}

% \begin{table}[t]
%     % \tablestyle{2pt}{1.05}
    
%     \centering
%     %\resizebox{1\columnwidth}{!}{
%     \begin{tabular}{@{}l|cc|cc}
%     	\toprule
%     	\multicolumn{1}{c}{} & \multicolumn{2}{c}{YouCook-Interaction} & \multicolumn{2}{c}{MiningYoutube Grounding}  \\ 
%     	\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
%     	Method  & Acc & IoU   & Acc & IoU \\ 
%     	\midrule
%     	CoMMA* \citep{tan2021look}   & 48.63 & -  & 47.68 & -  \\
%     	MIL-NCE \citep{miech2020end} & 23.67 & -  & 27.45 & -  \\
%     	Ours                        & 48.03 & -  & 47.35 & -  \\
%     	\bottomrule
%     \end{tabular}
%     \vspace{+0.3cm}
%     \caption{Evaluation on spatial-only evaluation using pointing game accuracy and attention heatmap IoU with GT bounding box. Models learning global representation doesn't perform well on localization tasks, while our model maintain comparable performance.
%     \label{tab:spatial}
%     %\vspace{-0.2cm}
%     }
%     %}
    
% \end{table}