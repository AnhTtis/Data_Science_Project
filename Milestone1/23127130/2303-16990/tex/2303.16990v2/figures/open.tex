% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.48\textwidth]{figures/Teaser_STG.pdf}
%     \caption{\textbf{Learning Spatio-temporal grounding in untrimmed videos:} 
%     In training, we learn from unlabeled videos without human annotation. In evaluation, we perform spatio-temporal grounding using an action description such as ``crack egg'' as a query. The model needs to localize both the action's temporal boundary and spatial region in the long untrimmed video. %At the bottom figure, each frame in the video consists of up to 5 points annotation (shown as $\oplus$ in the figure) representing the location of a given action step, e.g., Crack eggs. 
%     We visualize the heat-map from the annotation points as well as derived bounding boxes.% were derived from the point annotation using the union of annotated points with additional distance for later evaluation.  
%     }
%     \label{fig:open}
%     \vspace{-15pt}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.36\textwidth]{figures/Teaser_STG.pdf}
%     % \caption*{Learning Spatio-temporal grounding}
%     % \qquad
%     \resizebox{0.48\columnwidth}{!}{
%     \begin{tabular}[b]{@{}l|cc}
%     	\toprule
%             % Dataset  & \multicolumn{2}{c}{Annotation} & \multicolumn{2}{c}{Annotation Details}  \\
%     	  % & S & T & Spatial & Temporal\\ 
%             Dataset & \multicolumn{2}{c}{Annotation}  \\
%     	    & Spatial & Temporal \\ 
%     	\midrule
%     	% GroundingYoutube & $\checkmark$ & $\checkmark$ & action center point &  multi-action segmentation \\
%             V-HiCo & object bb + human bb  & - \\
%             AVA-Kinetics & object bb + human bb  &  - \\
%             THUMOS14 & - & action boundaries \\
%             ActivityNet & - & action boundaries \\
%             HACSSegment & - & action boundaries \\
%             YouCook II & - &  multi-action boundaries \\
%             Cross-Task & - &  multi-action boundaries \\
%             COIN & - &  multi-action boundaries \\
%             EPIC KITCHENS-100 & - &  multi-action boundaries \\
%             Ego4D & - &  multi-action boundaries \\
%             JHMDB51-21 & human tubes & - \\
%             UCF101-24 & human tubes & action boundaries \\
%             Daly & human tubes & action boundaries \\
%             Vid-STG & human tubes & action boundaries \\
%             HC-STVG & human tubes & action boundaries \\
%             AVA & human tubes & action boundaries \\
%             YouCook-Interactions & action bb &  - \\
%             GroundingYoutube (ours) & action bb + center points  &  multi-action boundaries \\
%     	\bottomrule
%     \end{tabular}}
%     \captionlistentry[table]{A table beside a figure}
%     \captionsetup{labelformat=andtable}
%     % \caption{A table beside a figure}
%     \caption{\textbf{Learning Spatio-temporal grounding in untrimmed videos (Figure 1).} 
%     In training, we learn from unlabeled videos without human annotation. In evaluation, we perform spatio-temporal grounding using an action description such as ``crack egg'' as a query. The model needs to localize both the action's temporal boundary and spatial region in the long untrimmed video. %At the bottom figure, each frame in the video consists of up to 5 points annotation (shown as $\oplus$ in the figure) representing the location of a given action step, e.g., Crack eggs. 
%     We visualize the heat-map from the annotation points as well as derived bounding boxes. 
%     % were derived from the point annotation using the union of annotated points with additional distance for later evaluation.
%     \textbf{Comparison of spatial/temporal/spatial-temporal grounding datasets (Table 1):}
%     V-HiCo~\citep{li2021weakly},
%     AVA-Kinetics~\citep{li2020ava},
%     THUMOS14~\citep{idrees2017thumos},
%     ActivityNet~\citep{caba2015activitynet},
%     HACSSegment~\citep{zhao2019hacs},
%     YouCook II~\citep{zhou2018towards},
%     Cross-Task~\citep{zhukov2019cross},
%     COIN~\citep{tang2019coin},
%     EPIC KITCHENS-100~\citep{damen2022rescaling},
%     Ego4D~\citep{grauman2022ego4d},
%     JHMDB51-21~\citep{jhuang2013towards},
%     UCF101-24~\citep{soomro2012ucf101}
%     Daly~\citep{weinzaepfel2016human},
%     Vid-STG~\citep{zhang2020does},
%     HC-STVG~\citep{tang2021human},
%     AVA~\citep{gu2018ava},
%     YouCook-Interactions~\citep{tan2021look}.
%     }
%     \label{fig:open}
%     \vspace{-15pt}
% \end{figure}




\begin{figure}[t]
    \centering
    %\begin{minipage}{\textwidth}
    %\begin{minipage}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/STG_Teaser_new.pdf}
     \vspace{-0.2cm}
    \captionof{figure}{\textbf{Learning Spatio-temporal grounding in untrimmed videos.} 
    In training, we learn from unlabeled videos without human annotation. In evaluation, we perform spatio-temporal grounding using an action description such as ``crack egg'' as a query. The model needs to localize both the action's temporal boundary and spatial region in the long untrimmed video. %At the bottom figure, each frame in the video consists of up to 5 points annotation (shown as $\oplus$ in the figure) representing the location of a given action step, e.g., Crack eggs. 
    We visualize the heat-map from the annotation points as well as derived bounding boxes. 
    % were derived from the point annotation using the union of annotated points with additional distance for later evaluation.
    % figure: https://docs.google.com/presentation/d/1umhBX31PMdMdCaqK8zB2Xj8T5w2jN6zRffYO2wu44wc/edit?usp=sharing
    \label{fig:open}
    \vspace{-0.5cm}
    }
    \end{figure}
    %\end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.49\textwidth}
%     \resizebox{1\columnwidth}{!}{
%     \begin{tabular}[b]{@{}l|cc}
%     	\toprule
%             % Dataset  & \multicolumn{2}{c}{Annotation} & \multicolumn{2}{c}{Annotation Details}  \\
%     	  % & S & T & Spatial & Temporal\\ 
%             Dataset & \multicolumn{2}{c}{Annotation}  \\
%     	    & Spatial & Temporal \\ 
%     	\midrule
%     	% GroundingYoutube & $\checkmark$ & $\checkmark$ & action center point &  multi-action segmentation \\
%             V-HiCo & object bb + human bb  & - \\
%             AVA-Kinetics & object bb + human bb  &  - \\
%             THUMOS14 & - & action boundaries \\
%             ActivityNet & - & action boundaries \\
%             HACSSegment & - & action boundaries \\
%             YouCook II & - &  multi-action boundaries \\
%             Cross-Task & - &  multi-action boundaries \\
%             COIN & - &  multi-action boundaries \\
%             EPIC KITCHENS-100 & - &  multi-action boundaries \\
%             Ego4D & - &  multi-action boundaries \\
%             JHMDB51-21 & human tubes & - \\
%             UCF101-24 & human tubes & action boundaries \\
%             Daly & human tubes & action boundaries \\
%             Vid-STG & human tubes & action boundaries \\
%             HC-STVG & human tubes & action boundaries \\
%             AVA & human tubes & action boundaries \\
%             YouCook-Interactions & action bb &  - \\
%             GroundingYoutube (ours) & action bb + center points  &  multi-action boundaries \\
%     	\bottomrule
%     \end{tabular}}
%     \captionof{table}{\textbf{Comparison of spatial, temporal, and spatio-temporal grounding datasets.}
%     \small{V-HiCo~\citep{li2021weakly},
%     AVA-Kinetics~\citep{li2020ava},
%     THUMOS14~\citep{idrees2017thumos},
%     ActivityNet~\citep{caba2015activitynet},
%     HACSSegment~\citep{zhao2019hacs},
%     YouCook II~\citep{zhou2018towards},
%     Cross-Task~\citep{Zhukov2019CrossTask},
%     COIN~\citep{tang2019coin},
%     EPIC KITCHENS-100~\citep{damen2022rescaling},
%     Ego4D~\citep{grauman2022ego4d},
%     JHMDB51-21~\citep{jhuang2013towards},
%     UCF101-24~\citep{soomro2012ucf101}
%     Daly~\citep{weinzaepfel2016human},
%     Vid-STG~\citep{zhang2020does},
%     HC-STVG~\citep{tang2021human},
%     AVA~\citep{gu2018ava},
%     YouCook-Interactions~\citep{tan2021look}.}
%     \label{tab:datasets}
%     }
%     \end{minipage}
%     \end{minipage}
%     \vspace{-0.4cm}
% \end{figure}