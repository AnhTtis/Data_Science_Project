
\input{tables/spatial_temporal_long}

\section{Experiments}

\subsection{Datasets} \label{dataset}
\noindent \textbf{Training Data:} \textbf{HowTo100M dataset} contains 1.2M instructional videos along with their corresponding automatically generated speech (ASR).
The narrations may be inaccurate and do not always accurately depict the video scene.
%We randomly selected 200K  video clips from the \textit{Food and Entertaining} category for training. %, and thus, we mainly focus on instructional videos in the area of cooking and kitchen tasks. 
%
%\input{tables/retrieval}
%\input{tables/spatio_vhico}

%\vspace{-0.3cm}
\noindent\textbf{Downstream Datasets:} %\textbf{YouCook2}: For the text-to-video retrieval downstream task, we use the common YouCook2 dataset , which provides a human-generated caption for 3.5K video clips for cooking instruction. 
%blah blah ... \hkc{add some details here?}
\textbf{GroundingYoutube (GYT)} is used to evaluate the task of multi-action spatio-temporal grounding as described in Section \ref{sec:dataset:annotation}.
% , we annotated the dense spatio-temporal location information as described in Section \ref{sec:dataset:annotation}.
%for 512 verb-noun phrases. All occurrences of the specific phrase in the test video are hence annotated, allowing us to evaluate spatio-temporal grounding in full untrimmed videos.
\noindent\textbf{MiningYoutube (MYT)} \citep{kuehne2019mining} %: To evaluate the temporal grounding abilities, we leverage the MiningYoutube \citep{kuehne2019mining} dataset, as it 
provides temporal annotation and is limited to the domain of cooking instruction videos. %The dataset features 250 full instructional videos, which are annotated with 512 action classes and temporal boundary information. 
%We use it to evaluate the temporal grounding abilities.
%Here, temporal alignment, the task of finding the right temporal boundaries given the sequences of actions, is used during evaluation to relax the task of temporal detection. 
\noindent\textbf{YouCook-Interaction (YC-Inter)} \citep{tan2021look} is an extension of the YouCook2 dataset \citep{zhou2018towards} for cooking instruction providing bounding boxes for 6K selected frames. The bounding boxes usually comprise the hand and the tool mentioned in the respective sentence-wise annotation. %We evaluate the spatial grounding abilities of models on this dataset.
% \noindent\textbf{YouCook2-Interaction (YC-Inter)}  To evaluate the spatial grounding abilities of our system, we use the YouCook2-Interaction dataset \citep{tan2021look}, an extension of a subset of the YouCook2 dataset \citep{zhou2018towards} for cooking instruction, which provides bounding boxes for 6K selected frames. The bounding boxes usually comprise the hand and the tool mentioned in the respective sentence-wise annotation.    
To further benchmark on general video domains on the \textbf{V-HICO} dataset~\citep{li2021weakly} with 6.5k videos with human-object interaction bounding boxes annotations, 
% that have been semi-automatically curated from sentence captions, 
and \textbf{Daly} action dataset~\citep{weinzaepfel2016human}, featuring videos consisting of daily actions such as ``brushing teeth''.% and ``cleaning windows''.



\subsection{Baseline methods}

%The proposed system is compared to various multimodal methods based on self- and weak supervision: 
\textbf{Temporal}: MIL-NCE~\citep{miech2020end} utilizes S3D~\citep{xie2018rethinking} and word2vec~\citep{mikolov2013efficient}. CLIP~\citep{radford2021learning}, an image-text model with transformer. 
\textbf{Spatial}:
CoMMA~\citep{tan2021look}, SSL model ($\dagger$ for weights shared by the author\footnote{We thank the authors for providing code and weights.} $\ddagger$ trained with CLIP);  
GLIP~\citep{li2022grounded}, RegionCLIP~\citep{zhong2022regionclip}, SOTA weakly supervised grounding model. % trained with image-text pairs.
\textbf{Spatio-temporal}: We construct MIL-NCE+RegionCLIP following the inference pipeline in Figure \ref{fig:inference}. 
TubeDETR~\citep{yang2022tubedetr} and STCAT \citep{jin2022embracing} are supervised. 
More descriptions of the baselines are given in the Appendix \ref{sup:baseline}.
Details of the implementation and experimental settings can be found in the appendix \ref{backbone_and_training}. Inference setups for each baseline are described in Section \ref{inference_sup}.

%\input{tables/spatial_temporal_long}

\input{tables/spatial}

\subsection{Downstream Tasks}


%We compare to the SOTA self-supervised method evaluated on spatial \citep{tan2021look} and temporal \citep{kuehne2019mining} grounding.

We considered the following downstream tasks to evaluate spatio-temporal grounding abilities of various models (detailed description is included in the appendix \ref{eval_metric}):

\noindent (i) \textbf{Spatio-temporal grounding in untrimmed video} is evaluated on the proposed Grounding Youtube dataset. The entire video and the respective pool of action instructions were provided. The model needs to localize each action step in time (start-time/end-time) and space (location in the video) as described in Figure \ref{fig:inference}. 
% We evaluate in two metrics: \textbf{IoU+Pointing game} combines spatial grounding~\citep{akbari2019multi} and temporal grounding~\citep{kuehne2019mining} metrics. %For each video frame, the prediction is correct when the model predicts the correct action for the frame. Also, given the predicted action as a query, the maximum point of the heatmap aims to lie within the desired bounding box. We then compute the Intersection over Union (IoU) over all the predictions with the GT to acquire the final score. 
% We also compute \textbf{video mAP} following previous evaluation~\citep{gu2018ava}, where we set IoU threshold between GT and predicted spatio-temporal tubes. A prediction is correct when it surpasses the IoU threshold. We compute the mAP over all classes. %We form a 3D prediction mask following Figure \ref{fig:inference} and compute IoU between our 3D heatmap and 3D tube.
We evaluate in two metrics: \textbf{IoU+Pointing game} combines the evaluation setting from the spatial grounding \citep{akbari2019multi} and temporal grounding \citep{kuehne2019mining} metrics. For each video frame, the prediction is correct when the model predicts the correct action for the frame. Also, given the predicted action as a query, the maximum point of the heatmap aims to lie within the desired bounding box. We then compute the Intersection over Union (IoU) over all the predictions with the GT to acquire the final score. 
We also compute \textbf{video mAP} following previous evaluation \citep{gu2018ava}, where we set IoU threshold between GT and predicted spatio-temporal tubes. A prediction is correct when it surpasses the IoU threshold. We then compute the mAP over all classes. We form a 3D prediction mask following Figure \ref{fig:inference} and compute IoU between our 3D heatmap and 3D tube.

\noindent (ii) \textbf{Spatial grounding} is given a text description to localize the region in the trimmed video. %We use GroundingYoutube, Youcook-Interaction, V-HICO, and Daly for evaluation. %Note that the evaluation is spatial only. It evaluates the results for each frame separately without considering the temporal information. 
It is evaluated using the \textbf{pointing game accuracy}. %Given the query text and video, we compute the attention heatmap on the video as described in Figure \ref{fig:inference}(b). 
If the predicted point lies in the ground truth bounding box, the result counts as a ``hit" and counts as ``miss" otherwise. The final accuracy is calculated as a ratio between hits to the total number of predictions $\frac{\text{\# hits}}{\text{\# hits} + \text{\# misses}}$. 
We also report the mean average precision \textbf{(mAP)} following the settings from V-HICO~\citep{li2021weakly}. %Given a human-object category as the text query, we aim to localize the spatial location in the video frame.
%The predicted location is correct if their Intersection over-Union (IoU) with ground truth bounding boxes is larger than 0.3. 
%Since we do not use any bounding box proposal tools or supervision, we create an attention heatmap as described in Figure \ref{fig:inference}(b) to create a mask for IoU computation. 
%We follow \citep{li2021weakly} and compute the mAP over all verb-object classes.


\noindent (iii) \textbf{Temporal grounding} \label{temporal_grounding}
provides videos with the respective actions and their ordering, including the background. The goal is to find the correct frame-wise segmentation of the video. We follow the inference procedure in \citep{kuehne2019mining} to compute the alignment given the similarity input matrix. The task is evaluated by intersection over detection (IoD), defined as $\frac{G \cap D}{D}$ the ratio between the intersection of ground-truth action $G$ and prediction $D$ to prediction $D$, and the Jaccard index, which is an (IoU) given as $\frac{G \cap D}{G \cup D}$.



\subsection{Comparison with state-of-the-art methods}\label{sota}
\noindent (i) \textbf{Spatio-temporal grounding in untrimmed video:}
We first compare the proposed method with other approaches designed for spatial or temporal grounding in Table \ref{tab:st_long}.
It shows that models without specific loss designs for spatial grounding (MIL-NCE~\citep{miech2020end}, CLIP~\citep{radford2021learning}) show good mAP scores but lower pointing game accuracy. Out of the two weakly supervised methods, GLIP~\citep{li2022grounded} and RegionCLIP~\citep{zhong2022regionclip}), trained with aligned image-text, RegionCLIP show significantly better performance in this setting, while both perform in a similar range in the spatial grounding scenario (see Table~\ref{tab:spatial}). We attribute this behavior to the fact that RegionCLIP distinguishes frames with relevant queries better from background than GLIP, leading to better temporal localization. 
We finally compare the strong baseline MIL-NCE+RegionCLIP, which combines two approaches specialized in temporal and spatial aspects, to our task. 
It shows that the proposed method improves over all other baselines underlining the need to incorporate global (temporal) and local (spatial) representations. 
%Experiments showed that combining a joint objective that learns spatial and temporal information jointly results in better performance than simply applying the best temporal and spatial model. 
% Also, such a combined objective also benefits more when the visual backbone is finetued as well. 
% We construct a split with single action shown in appendix \ref{single_action_stg}.
%Models designed for trimmed videos (CoMMA\citep{tan2021look}) or trained with aligned image-text (GLIP\citep{li2022grounded}, RegionCLIP\citep{zhong2022regionclip}) failed to capture the temporal dynamics, while models without specific loss designs for spatial grounding (MIL-NCE\citep{miech2020end}, CLIP\citep{radford2021learning}) were not able to ground the action in the correct region.
%Note that supervised spatio-temporal grounding approaches~\citep{yang2022tubedetr,jin2022embracing} are not directly applicable in this evaluation since such methods assume the given text query to be ground-truth. %The model must distinguish the correct text query from a pool of action lists. 
%We include an evaluation setting in the supplement where the GT-text queries were provided. \hkc{Do we? If not, we can probably comment the last 2 sentences}
%More experiment setting is in the supplement.

\input{tables/temporal}

\noindent (ii)~\textbf{Spatial grounding: } 
 %We do not report mAP on Youcook interaction since the input is sentence descriptions instead of class.
Second, we compare the performance of the proposed framework to other methods on the task of spatial grounding, including models with weak supervision, as well as models trained in a fully supervised setting in Table \ref{tab:spatial}.
%As shown in Table \ref{tab:spatial}, models trained with global representations such as MIL-NCE and CLIP were not able to localize the text description compared to models learning local representations such as CoMMA, GLIP, RegionCLIP and our approach. 
In the instruction video domain (GYT and YC-Inter), the proposed approach achieves the best result among all weakly and self-supervised trained methods. In the general domain (V-HICO and Daly), the method also achieves competitive results, showing the generalizability of the model to other domains. 
%We attribute this to the transformer architecture in the text branch inheriting knowledge from the open domain during large-scale training, while in contrast the model's performance using word2vec dropped in these datasets. 
Note that in the Daly dataset, the classes are verbs, which are not detectable by the object-focused model GLIP. 
Compared to their weakly trained counterparts, fully-supervised model (TubeDETER~\citep{yang2022tubedetr}, STCAT~\citep{jin2022embracing}) achieve competitive performance in the general domain (V-HICO, Daly) and slightly lower performance in instruction domain (GYT, YC-Inter) due to the domain gap with respect to the training data.
\input{tables/ablation}
\input{figures/visualization.tex}

\noindent (iii)~\textbf{Temporal grounding:}
We evaluate temporal grounding in Table \ref{tab:temporal}. Here, it shows that global representations also profit from local representation learning.%, achieving state-of-the-art results in temporally localizing actions in untrimmed videos. 
This hypothesis is further validated in the ablation studies in Table~\ref{tab:train_ablations}, where we ablate both losses for all three settings and show a consistent improvement in the joint loss formulation. 

%Our model achieved comparable results with 
%Called action step localization. Evaluated on Mining Youtube. 




%\input{tables/spatial_temporal_clip}






% \noindent (iv) \textbf{Spatio-temporal Clip :}
% \label{ST_clip}
% Following the current spatio-temporal datasets \citep{jiang2014thumos,gu2018ava} which aim to discriminate the action class from the background class in a short clip, we construct a clip level evaluation where the clip varies from 9 sec to 60 Section  Given an action step, we append the video segments before and after the steps with the same time length of the action step to form the final video clip. This results in 2,895 clips for the spatio-temporal clip grounding evaluation.
% For each clip, the  temporal action intervals occupy 33\% of corresponding videos, which demonstrates the difficulty of the setting. As shown in Table \ref{tab:st_clip}, we observe a similar trend as the full video evaluation where our model outperforms all the baselines. 




\subsection{Ablation study} 
%\vspace{-1mm}
We perform ablation studies with respect to all three settings, spatio-temporal grounding, as well as spatial and temporal grounding alone, reporting performance for spatio-temporal grounding on GroundingYT using mAP with IoU@0.4, on temporal grounding using MiningYT IoU, and on spatial grounding using YC-Inter. pointing game. Additional ablation are in appendix \ref{ablation_sup}. %For each setting, we use the same feature extractor for three modalities as described in Sec 4.1 for a fair comparison. 

% add summary here?
% as they are the less computational evaluation tasks.
%This subset of downstream tasks has been chosen for their simplicity of evaluation and because they cover a wide range of tasks.

\noindent\textbf{Frame selection strategy.} 
We perform an ablation on the possible frame selection strategies for our method (Figure \ref{fig:pipeline}(b) and Section \ref{sinkhorn_main}). In Table \ref{tab:frame_ablations}, \textit{None} uses all frames within the ASR boundary ($U=T$) as our video training data. 
\textit{Global} represents the [CLS] token in text and video. \textit{Local} uses the words and spatio-temporal tokens. In the setting Sinkhorn was not applied, the top $T$ frames with the highest similarity score were selected. When we set spatio-temporal tokens as the selection target, we sum over the scores with respect to each frame to acquire the frame similarity score.
%\textit{Global} utilizes the global sentence resp. frame [CLS] token as the query to rank the top $T$ similar frames as the selected frames for training. \textit{Local} uses the words resp spatial-temporal tokens instead of the CLS token as a query and selects the frames with the closest feature distance. 
It shows that selecting frames based on Sinkhorn selection leads to consistently better results as it enforces more variety of visual concepts but also captures frames with possible groundable objects. It further shows that word tokens are more suitable than the global text CLS token for frame selection. Finally, we see that depending on the task (spatial vs. temporal), a local resp. global representation is better, and a combination of both works best for spatio-temporal grounding. 
%, which improves overall performance.%, leading to better supervision.
We provide runtime analysis of such frame selection strategy in the appendix \ref{runtime}.
% \noindent\textbf{Number of frames for training.} We tested different video lengths $T$ used for training. As shown in Table \ref{subtab:ablations2}, selecting less frames for training significantly causes the performance to drop. We hypothesize that not only does the model fail to capture the temporal dynamics with less frames, but loses some frames with groundable objects in the sentence while training. We also found that when the number of frames increases, more irrelevant frames might be selected during training, which decreases the performance.
\input{tables/ablation_loss}

%\vspace{-0.1cm}
\noindent\textbf{Global and local loss.} As mentioned in the spatio-temporal evaluation, both features contribute to the final grounding result. We test the model by ablating out each loss. 
Table \ref{tab:train_ablations} shows that each loss not only contributes to the spatio-temporal grounding on the GYT, but also that the whole is more than the sum of its parts (losses) since this task requires both spatial and temporal detection. The reduced impact of the global loss in the case of YC-Inter is that this is a pure spatial grounding dataset (no background frames) without temporal detection, and the local loss plays a more critical role. We observe the same patterns in the temporal grounding result for MYT, where spatial localization is not directly contributing to the final performance. We tried out the same ablation using in the S3D backbone in supplement.
%We provide runtime analysis of different losses in the appendix \ref{runtime}.
%By comparing the results for spatio-temporal grounding in untrimmed videos (Table 1) vs. spatial grounding in trimmed videos (Table 3),  we can further see the impact of the proposed joint representation.

%\bc{to appendix}




% adding the global loss improves the ground performance. This results also shows that spatial grounding benefits from global representation learning. In the spatio-temporal setting, the performance without a global or local loss outperforms other baselines.

% \noindent\textbf{Dataset for training.} As mentioned in Section \ref{dataset}, we trained models with data with food categories. In Table \ref{subtab:ablations4}, we also tested our model trained with a larger set of food and entertaining called HowTo370K used in \citep{han2022temporal}. The full set of HowTo100M contains a total of 1M long videos, which is five times the size of our dataset. We found training with our 200K videos reaches similar performance with much less training hours.

% \noindent\textbf{Affect of audio in training and testing.} Unlike text which describes a discrete concept as a target to ground, audio serves as a continuous representation that is highly relevant to the temporal information. For example, we can determine an action started when we hear a ``cracking'' sound. In Table \ref{subtab:ablations5}, we tested our model using the additional audio modality by expanding our architecture and loss from VT to VAT. We found when training and testing with audio, the spatio-temporal result increases while the spatial-only result remains the same. This validates our assumption that audio contributes more to temporal understanding. When we trained on audio and tested without audio, the performance increases over the VT model, showing that the audio serves as useful supervision for better video/text representations. More details are presented in the supplement. 

\subsection{Qualitative results}
\vspace{-1mm}
We visualize our spatio-temporal result in Figure \ref{fig:visualization}. For the GLIP model, we output the bounding box with the highest confidence score and visualize its center point. We found GLIP model focuses on the salient object while our model focuses more on human-object interaction.

