\section{Conclusion}
\vspace{-2mm}
%Improving the spatial and temporal grounding of actions in video are important steps towards not only improved  applications such as retrieval, detection, and summarization on large video datasets where human annotation is infeasible, but also more explainable systems as it e.g. not only allows to retrieve videos, but also to visualize the regions relevant for this decision.  
% In this work, we proposed a method to learn spatio-temporal action grounding with self-supervision and a
% new dataset: the GroundingYoutube annotations. We propose a frame selection method that discovers frames with groundable objects to adjust the learning into untrimmed setting. In addition, we jointly learns global representations, which encodes temporal information and local representations while learning multimodal interaction between video and text.
% We extensively evaluate our method on various downstream tasks, including untrimmed video spatio-temporal grounding, video spatial grounding, and action step alignment. Our approach achieves state-of-the-art performance in instruction videos and generalized well with open vocabulary human object interaction datasets.
We presented an approach for learning spatio-temporal grounding with self-supervision and a new dataset: GroundingYoutube annotations, where we densely annotate spatio-temporal points/boxes from untrimmed multi-action videos. Our method includes a frame selection mechanism that identifies frames with groundable objects to adapt the learning process for untrimmed videos. Furthermore, we jointly learn global representations, which capture temporal information, and local representations learning fine-grained multimodal interactions between video and text.
We conducted extensive experiments and our approach shows state-of-the-art performance in spatio-temporal grounding, as well as temporal and spatial grounding alone. 
%We extensively evaluate our method on various downstream tasks, including untrimmed video spatio-temporal grounding, video temporal grounding, and video spatial grounding. Our approach achieves state-of-the-art performance in instruction videos and generalized well with open vocabulary human object interaction datasets.
%We conducted extensive experiments to evaluate the performance of our approach on various downstream tasks, including untrimmed multi-action video spatio-temporal grounding, video temporal grounding, and video spatial grounding. Our method achieves state-of-the-art in instruction videos and generalizes well to open vocabulary human-object interaction datasets.