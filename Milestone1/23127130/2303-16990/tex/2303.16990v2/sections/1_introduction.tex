\section{Introduction}
%
%
\input{figures/open}
Spatio-temporal grounding (STG) describes the challenging task of locating events in space and time within video data based on text referential expressions.
Methods in this field usually rely on a combination of spatio-temporal bounding box annotation, together with a human-generated caption, describing the visual content of the bounding box \citep{yang2022tubedetr, jin2022embracing}, which limits their generalizability beyond the given training scenario. 
Compared to that, as a second line of work, multimodal self-supervised learning tries to leverage ``free'' data sources, such as video and automatic speech recognition (ASR) captions from large-scale instructional videos to learn representations without human annotation \citep{miech2019howto100m,miech2020end,alayrac2020self,akbari2021vatt,chen2021multimodal}. The resulting models achieve state-of-the-art performance on zero-shot tasks such as cross-modal video retrieval or classification and also for zero-shot temporal action segmentation and detection based on free text queries~\citep{Zhukov2019CrossTask,kuehne2019mining,tang2019coin,chen2021multimodal,ShvetsovaCVPR22Everything}, but usually lack spatial localization abilities.
% They are thus able to detect events in videos without labeled training data and based on referential expressions only. 
% However, so far these approaches focus \nina{missed "only"?} on global frame wise processing, e.g. for temporal action segmentation/detection \citep{Zhukov2019CrossTask,kuehne2019mining,tang2019coin}. 
% \hkc{cite some more ... }, 
%
A third line of work focuses on label-free spatial grounding, e.g. by training on image-caption \citep{akbari2019multi,yang2022unitab,li2022grounded,wang2022cris,zhong2022regionclip} or video-caption pairs \citep{tan2021look,shi2019not}. The goal is to correctly localize a referential expression in an image or each video frame, e.g., via a bounding box or a heatmap. However, those methods are not optimized to detect whether an event is present in a video. The assumption is thus that the evaluated expression is visible in the image or in all video frames.

% We therefore aim to bring together those two streams of work by proposing/addressing (?) the task of spatio-temporal action grounding from multimodal supervision. 
The following work aims to bring together those ideas to address the task of spatio-temporal action grounding from multimodal supervision in untrimmed videos. We propose a grounding approach that uses video-text pairs based on ASR transcripts in instructional videos and learns the spatial representation of free-text events as well as their temporal extent, as shown in Figure~\ref{fig:open}. 
To this end, we leverage two different representations of the visual data: a global feature representation based on full-frame information to define the temporal extent of an event and a local representation based on frame-wise grid features for spatial localization.
The motivation for this dualism is that while the local representation captures the spatial correlations between vision and text input, this can be too fine-grained to learn a holistic representation of the frame, while the global representation can be assumed to capture a more compact, aggregated view compared to local data and thus to provide a more reliable cue for the task of temporal localization. 
%We are also motivated by recent systems showing the potential of text-image grounding \citep{arbelle2021detector,akbari2019multi}. 
% But ASR text is usually captured over a longer duration, e.g., 10-15 Section  \citep{}, and is not precisely aligned with the described activities. 
However, compared to the hand-annotated video-caption setup of most spatio-temporal grounding methods, the ASR text can be noisy as not all comments refer to visible events. Further, as there is only a loose temporal correlation, the described activities might not be precisely aligned, can be scattered over multiple frames, or not be present at all~\citep{miech2020end,han2022temporal}.
Therefore, we propose to specifically select frames to capture only those useful for training. To this end, we look for frames that match the vocabulary of the respective text, leveraging a selection strategy by Sinkhorn optimal transport \citep{cuturi2013sinkhorn}. 
%\hkc{We shoudl described inference and then close with this sentence> .. or remove it} 
This allows us to train a model that can localize actions in space and time within videos without labeling supervision.
% \hkc{Note that the audio channel does not suppress actions, even if there is no audio present, but instead attends temporal regions that seem more relevant than others.}
% Using the best of both worlds, we learn fine-grained projections both in space and time that allow us to localize action instructions / semantic concepts in space and time within a video volume. 
%Method 
%Input is test, output is region in the video ... from test to audio from audio to video ? Audio hard to contribute to the localization because of noisy data ... e.g. spoken words, bad camera quality, works only for instruments and speech, but not for multiple sounds sources. 
%\vspace{-0.1cm}

To evaluate spatio-temporal grounding in untrimmed videos, a new benchmark, GroundingYouTube, is proposed. It is based on the existing MiningYouTube dataset \citep{kuehne2019mining} and extended with spatio-temporal localization information.
This setup differs from other benchmarks such as \citep{tan2021look,chen-etal-2019-weakly,ZhLoCoBMVC18} in two ways: first, by using multiple center point annotations, it focuses on the grounding of referential actions itself instead of interacting humans or objects which are usually labeled; second, the dense annotations of multiple actions in the video allow us to benchmark action grounding in long, realistic untrimmed videos compared to existing, often pre-clipped benchmarks\citep{chen-etal-2019-weakly,zhang2020does}. The benchmark provides queries for 512 different event types and over $5K$ spatio-temporal annotations, as shown in Figure~\ref{fig:open}. A comparison of current datasets is shown in Table~\ref{tab:datasets}. 
%Our setting can be considered especially challenging as actions do not have a physical spatial outline. Thus, compared to object and temporal boundaries, boundaries are less defined. %Also, the output of the system is also usually not given in the form of a bounding box, but rather a heat map indicating the most relevant areas. 
%To address this problem, we borrow the idea of pointing game \citep{xiao2017weakly,zhang2018top} for annotation as well as for evaluation and ask annotators to annotate the presumed center point of the action rather than defining a bounding box outline, collecting five-point annotations per frame. This leads to a heatmap similar to the expected system output . 

% Evaluation
To evaluate the proposed approach as well as the new benchmark, the system is trained on the HowTo100M dataset \citep{miech2019howto100m} and compared to state-of-the-art methods based on full, weak, and self-supervision for spatial and temporal, as well as combined spatio-temporal grounding tasks. It shows that existing methods usually do well in one of the two aspects, spatial or temporal grounding. In contrast, the proposed method can combine spatial and temporal aspects.% of semantic concepts without label annotation. %, and it outperforms current state-of-the-art systems on the downstream tasks. 
%
% To evaluate our method as well as the annotations, we follow the standard benchmark protocol and train our system on the HowTo100M dataset \citep{miech2019howto100m}, resp. A subsection of it as used by Tan et al. \citep{tan2021look}, and compare it to state-of-the-art methods for the task of retrieval, spatial and temporal grounding, as well as the combined spatio-temporal grounding task. We further provide an evaluation of the gathered annotations considering various metrics such as pointing game accuracy and IoU MAP. Overall, it shows that the proposed method is able to detect action in space and time without the need for any labeled annotation during training and outperforms current state-of-the-art systems on this task.
% \hkc{... can learn action detection (spatially and temporally) based on semantic concepts without labels.}
%
%
% Both aspects, the spatial as well as the temporal grounding of actions in video can be considered important steps, not only on the way to build better models and allow for applications such as retrieval, detection, or summarization on large video datasets where human annotation is infeasible, but also to allow for a step towards more explainable systems in the area, as it e.g. not only allows retrieve a video, but also to visualize the regions relevant for this decision.  

% % Save some space this way.
We summarize the contributions of this work as follows\footnote{We will make the code and the annotations publicly available.}: 
% (1) We introduce a novel approach to learn spatio-temporal grounding in untrimmed videos using weakly aligned multimodal supervision, without requiring human annotation. Our proposed method is the first of its kind, addressing this challenging problem in an unsupervised manner.
% (2) To facilitate this task, we propose a novel frame selection strategy based on Sinkhorn that improves the quality of the acquired learning samples, leading to more effective supervision.
% (3) Our approach employs a combination of global and local representation learning, enabling the effective encoding of temporal information and the localization of spatio-temporal action extents in instructional videos.
% (4) We present a new benchmark and annotations that allow for the evaluation of this challenging problem using real-world instructional video data. Our benchmark provides an objective way to compare different methods and to encourage further research in this area.
(1) We propose a framework for spatio-temporal grounding in untrimmed videos based on weakly aligned multimodal supervision without human annotation, employing a combination of global and local representation learning to learn the spatio-temporal extent of actions.
(2) To facilitate this task, we propose a frame selection strategy based on Sinkhorn-Knopp Optimal transport that improves the quality of the acquired learning samples, leading to more effective supervision.
(3) We provide a new benchmark and annotations to evaluate this challenging problem on real-world multi-action instructional video data.  

% We summarize our contributions as follows\footnote{We will make the code and the annotations publicly available.}:
% \begin{itemize}
%     \item We propose the task of spatio-temporal grounding in untrimmed videos based on weakly aligned multimodal supervision. 
%     \item To address this task, we leverage a combination of global representation learning to encode temporal information and local representations to learn the spatio-temporal extent of actions in instructional videos.
%     \item We provide a new benchmark and annotations to evaluate this challenging problem on real-world instructional video data.  
%     %\vspace{-0.5cm}
% \end{itemize}


% \input{tables/datasets}
