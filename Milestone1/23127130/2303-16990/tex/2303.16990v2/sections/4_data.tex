
\section{GroundingYoutube Benchmark}
\label{sec:dataset:annotation}
\vspace{-0.1cm}
Current downstream datasets either provide spatial~\citep{tan2021look}, or temporal annotation~\citep{kuehne2019mining}, or spatio-temporal annotation~\citep{zhang2020does} but only for short video clips with few frames before and after the action takes place. These datasets do not provide the opportunity to evaluate both aspects, spatial and temporal grounding, in an untrimmed long video manner. 
We, therefore, extend one of the current benchmarks, MiningYouTube~\citep{kuehne2019mining}, with 250 untrimmed videos with a duration of around 3-5 minutes and an average of 41.6 labeled action instances per video. Note that each video contains various classes. 
As the dataset already provides dense temporal annotations, we annotate the respective temporal action segments in the dataset with spatial information. 
%Our annotation is based on the MiningYoutube dataset~\citep{kuehne2019mining}, 
%To extend the current benchmarks, we decided to go with the Mining YouTube dataset \citep{kuehne2019mining}. 
\input{figures/boxes}


% \subsection{Dataset annotation}
% \label{sec:dataset:annotation}
% \vspace{-0.1cm}
Annotating the spatio-temporal extent of actions can be challenging as there is no clear visible outline as, e.g., in object annotation, nor is there a unique signal to indicate the temporal begin and end points. 
Similarly, grounding systems do not produce pixel-exact bounding boxes, but rather indicate regions of interest. Detector-free spatial grounding models~\citep{arbelle2021detector} address this fuzziness by relying on pointing game accuracy, thus only using the center point of the heat map for evaluation. 
%
Lending on this idea, annotators were asked to mark the presumed center point of the action. 
Compared to bounding boxes, center point annotation can be advantageous because annotators are not visually distracted by object outlines, so it is more likely that the most important region will be selected. 
%Additionally, point annotations take less time, so it is more cost-effective compared to bounding boxes.
%Overall, we found that a point annotation took $5s$ in average per image, compared to $25s$ for bounding boxes a reported in \citep{su2012crowdsourcing}. 
We capture five annotations per frame, resulting in a density-based heatmap. 


Starting from 5,091 clips showing one of the 512 action classes, we adopt the methodology used for temporal action localization developed in~\citep{gu2018ava} and label one frame per second, resulting in $26,987$ labeled frames. We annotated all frames with five repeats per image, resulting in five annotations per frame and $134,935$ point labels in total. %Each video contains an average of 10 distinct actions.
%Our finding is that our annotation reaches high consistency and captures meaningful variety for evaluation. In general, we found in the initial phase of the annotation that the center point of action is more intuitive than the bounding box since it is hard to give any guidance to the scale of the bounding box, thus the physical outline for an action, which varies e.g. with respect to annotations and viewpoints of the hands, human, or instrument. %As an alternative view, if we predefined certain label criteria (e.g. always include the hands), the model would not be aware of those criteria without being trained on it, which would defy the idea of a weakly-supervised training without the need for annotation. Instead, we used the current spread of center points to define the outlines of the region that we want the model to localize on. }
To evaluate using bounding boxes~\citep{kalogeiton2017action}, we get the union of all annotated points with additional distance to construct the bounding box as shown in Figure \ref{fig:boxes}. More information on the annotation process, bounding box derivation, and dataset analysis is provided in the appendix \ref{GroundingYoutube_Annotation}. %Video examples are in the supplement. %\bc{Sec.X}.%\hkc{Refer to supplementary for further details ...?}

% \begin{figure}[t]
% \centering
% %\resizebox{.9\linewidth}{!}{
%     %%%%%%%%%%%%%%
%     \begin{subfigure}{.16\textwidth}
%     \centering
%     \subfloat[Number of labels generated][\scriptsize Wider shot]{
% \includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/answers_concentrated.png}
% \label{fig:answers_concentrated}}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
%     %%%%%%%%%%%%%%
%     \begin{subfigure}{.16\textwidth}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Flow action]{
% \includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/add_oil.png}
% \label{fig:answers_no_clear_action}}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
%     %%%%%%%%%%%%%%
%     \begin{subfigure}{.16\textwidth}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Split to two]{
% \includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/error_multiple_actions.png}
% \label{fig:answers_multiple_actions}}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
% \caption{Example of keypoint annotations under different conditions }
% \label{fig:quality_aspects}
% \vspace{-0.5cm}
% \end{figure}


%All labels were generated by $13$ professional annotators in total.  
%
% During the annotation, professional annotators were given a short instruction video at the beginning and then asked to click on the center of the given action without additional instructions. They further were free to choose ``can't answer" if they could not locate the action, e.g., at the beginning and end of the clip. Thus, the number of available keypoints per image differs and we choose majority voting to define if an action is present or not, resulting in new, refined temporal boundaries compared to the original annotation. 

% \begin{figure}
% %\vspace{-0.5cm}
%     \centering
%     \begin{subfigure}{.5\textwidth}
%     %%%%%%%%%%%%%
    
%     \centering
%     \subfloat[Can't solve][\scriptsize Can't solve]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/error_cant_solve.png}
%     \label{fig:answers_cant_solve}}
%     \qquad
%     \subfloat[Corrupt image][\scriptsize Single point]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/single_point.png}
%     \label{fig:single_point}}
%     \\
%     \subfloat[Corrupt image][\scriptsize Four points]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/four_points.png}
%     \label{fig:answers_corrupt}}
%     \qquad
%     \subfloat[Corrupt image][\scriptsize Five points]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/five_points.png}
%     \label{fig:answers_corrupt}}
%     %\caption{Illustration of an ``can't solve'' image \bc{4 examples: }}
%     %\label{fig:can_solve}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
%     %%%%%%%%%%%%%%
    
%     %%%%%%%%%%%%%%%%%%%
%     \caption{Sample annotations (left) and number of keypoints (right). The purple point represents the center point of the annotations in the frame. Approximately $48\%$ of the data has all 5 keypoints and $19\%$ has not a single annotation.
%     % It can be seen that the vast majority of answers were generated by five annotators. % \hkc{probably take out that one ... it's not favorable in times of crowd sourcing ... }-  
%       }
%     %\vspace{-0.4cm}
% \end{figure}
%
% \begin{figure}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Number of keypoints]{
%           \includegraphics[width=0.5\textwidth]{figures/dataset_annotation/distribution_number_of_keypoints.png}
%           \label{fig:answers_per_annotator}}
% \end{figure}%
%
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/dataset_annotation/distribution_number_of_keypoints.png}
%     \caption{Number of keypoints per image. It can be seen that $48\%$ of the data has all 5 keypoints and $19\%$ has not a single annotation}
%     \label{fig:answers_per_annotator}
% \end{figure}

% We found that the point-wise annotation resulted in roughly four distinct patterns, which depend on the captured scenario, as shown in Figure~\ref{fig:quality_aspects}. In the case of half portrait or even wider shots in Figure~\ref{fig:answers_concentrated}, annotations are highly locally centered, while frames with large object focus tend to have more widespread and less dense annotation in Figure~\ref{fig:answers_widespread}. We further found that in some cases, the point annotation can also result in representing the flow of the action, e.g., pouring oil in Figure~\ref{fig:answers_no_clear_action}, or even split in two separate clusters in Figure~\ref{fig:answers_multiple_actions}. 




% \subsection{Quality control}
% \label{sec:dataset:quality_control}
% Since the label quality of the datasets used is a critical factor in the performance of machine learning models, we verified the correctness of a subset of our images using an experienced annotation specialist for $1,026$ randomly selected frames. %\hkc{Sample size calculation in the rebuttal} % todo reference label quality effects outcome
% %
% To evaluate the quality of the data, we evaluate the agreement between the annotation specialist and the annotations provided by the annotators. To this end, we considered an annotation as false positive if three annotators or more have set a key point, although no action can be seen in the image and as false negative if three annotators or more have not set a key point, even though an action can be seen in the image. The entire sample was assessed using these criteria, with the specialist disagreeing with the annotators in only a total of $1.1\% \pm 3\%$ (FP: $0.7\% \pm 3\%$, FN: $0.4\% \pm 3\%$). We also found that annotations significantly diverted in terms of spread. Namely, for wider shots, they tend to be highly centered whereas zooming in together with the usage of larger objects such as a pan or a spatula results in more widespread key points. We therefore also analysed how often those cases occur and found that $14.0\%$ of the selected frames show a wide spread pattern. 


% \subsection{Dataset usage for evaluation}
% \label{sec:dataset:evaluation}
% %\hkc{we need to somehow describe how we turn point in bounding boxes ... } \\
% \noindent\textbf{Bounding box:} We get the union of all annotated points in a single frame with an additional distance for constructing the bounding box in evaluation. \\
% \noindent\textbf{Clip level evaluation construction: }
% Following the current spatio-temporal dataset \citep{kalogeiton2017action}, we construct clip level evaluation where the clip varies from 9 sec to 60 Section  Given an action step, we append the video segments before and after the steps with the same time length of the action step to form the final video clip. We result in 2895 clips for the spatio-temporal clip grounding evaluation.
%, which ends up with [previous segment, action step, later segment] as the final video clip.