
\section{Additional Experiments}
\subsection{Runtime analysis}
\label{runtime}
We analyze the computational costs of sampling and loss. We sample 16-second videos at a frame rate of 5 FPS (80 frames in total). We report the execution time for a single batch (batch size = $64$) averaged over 100 batches.
For the \textit{frame sampling strategy}:
(1) Random select 8 frames: 1.48s.
(2) Optimal transport based selection of 8 frames out of 64: 1.54s.
(3) Entire 64 frames: 1.74s.
The execution time of our method is close to traditional random sampling while capturing diverse visual concepts, which improves the training process. %However, using the entire video segment as input slows down the training.
% For the global and local components, (1) including only global loss requires 1.1s (2) only local loss requires  1.52s. (3) Including both losses requires 1.54s. We find that local loss computation is more expensive than global loss since it requires features with finer granularity, while the global component only requires mean-pooled features over time.
For the \textit{global and local} components:
(1) Global loss only: 1.1s.
(2) Local loss only: 1.52s.
(3) Both losses: 1.54s.
Computation of the local loss is more time-consuming than the global loss due to its requirement for features with finer granularity.

\input{tables/spatial_clip}


\subsection{Single-action spatio-temporal grounding.}
\label{single_action_stg}
%
Current spatio-temporal detection and grounding datasets \citep{jiang2014thumos,gu2018ava} usually aim to discriminate a single given action class from the background class in a short clip. This differs from our setup of spatio-temporal grounding in untrimmed videos, which usually comprises a set of phrases that need to be detected in a 3-5 min long video. To allow an evaluation of spatio-temporal grounding approaches based on single phrase grounding, we construct a clip-level evaluation where the clip varies from 9 sec to 60 sec. Given an action step, we append the video segments before and after the steps with the same time length of the action step to form the final video clip. This results in 2,895 clips for the spatio-temporal clip grounding evaluation.
For each clip, the temporal action intervals occupy 33\% of corresponding videos, which demonstrates the difficulty of the setting. 
%\bc{talk about GLIP performance increase, how its temporal prediction}
In this setting, instead of selecting the possible action step from a pool, the ground truth action step was given as the text query for spatio-temporal grounding. This allows us to directly compare with supervised spatio-temporal grounding methods \citep{yang2022tubedetr,jin2022embracing} as described in Section \ref{sota}.
As shown in Table \ref{tab:st_clip}, we observe that the baseline GLIP models achieve a much better performance compared to Table \ref{tab:st_long}. This is due to the fact that this setting does not require the model to select the text query from the pool, which the GLIP model was not trained to do. Moreover, we find that weakly supervised methods, GLIP and RegionCLIP, show only limited ability to differentiate the queried action from the background, which leads the model to ground the text query in most of the frames. However, both demonstrate powerful localization ability in foreground action segments, which results in a decent performance. The fully-supervised trained models (TubeDETR, STCAT) achieved a balance in localizing temporally and spatially, resulting in the best performance on this task.


\input{supplement/tables/architecture}
\input{supplement/tables/frames.tex}
\input{supplement/tables/frames_for_train}
\input{supplement/tables/audio}
\input{supplement/tables/threshold.tex}
\input{tables/ablation_meanpool}
\input{tables/ablation_loss_s3d}


\subsection{Ablation and decision choices}
\label{ablation_sup}
We performed additional ablation studies using the CLIP backbone without finetuning. \\
\noindent\textbf{Attention architecture.} We tested different architectures by stacking the self-attention or cross-attention block in the model to calculate contextualized local representations, as shown in Figure \ref{fig:pipeline}(d). As shown in Table \ref{subtab:architecture}, we found that the standard multimodal transformer architecture (self+cross) to have the worst performance. Using two cross-attention blocks was beneficial in incorporating more cross-modal interaction between local features. Finally, including a
self-attention layer slightly improves the final representations by encoding better single-modality representations.


\noindent \textbf{Frames used for selection.}
As shown in Table \ref{tab:frames}, we perform an ablation study on the number of candidates frames $U$ used for training. We found that selecting 80 frames (16 seconds) achieves the best performance, comprising the useful video information in training while not including too many irrelevant concepts that diverge from the action/object in the ASR sentence.



\noindent\textbf{Number of frames for training.} We further evaluated the impact of different numbers of frames $T$ used for training. As shown in Table \ref{subtab:ablations2}, selecting fewer frames for training significantly causes the performance to drop. We hypothesize that the model not only fail to capture the temporal dynamics with fewer frames but also loses some frames with groundable objects in the sentence while training. We also hypothesize that with a too large number of frames, more irrelevant frames might be selected during training, which decreases the performance.


\noindent\textbf{Effect of audio in training and testing.} Unlike text which describes a discrete concept as a target to ground, audio serves as a continuous representation that is highly relevant to the temporal information. For example, we can determine an action started when we hear a ``cracking'' sound. In Table \ref{subtab:ablations5}, we tested our model using the additional audio modality. For the audio branch, we compute log-mel spectrograms and use a DAVEnet model~\citep{harwath2018jointly} initialized by MCN on HowTo100M~\citep{chen2021multimodal} to extract audio features. We extend the global and local loss pairs from VT to VT, VA, and AT following \citep{ShvetsovaCVPR22Everything}.
We found when training and testing with audio, the spatio-temporal result increases the temporal performance while the spatial-only result remains the same. This validates our assumption that audio contributes more to temporal understanding. When we trained on audio and tested without audio, the performance increased over the VT model, showing that the audio serves as useful supervision for better video/text representations. 



\noindent \textbf{Threshold for attention mask.}
As shown in Figure \ref{fig:inference}(b), we apply a threshold to create a mask from the result of attention rollout. Note that this threshold $\tau$ is not a hyperparameter that affects the training or the model but simply serves as a means to an end to compute the \textit{mAP} scores. We did not systematically optimize this threshold, but instead, %chose it as giving the most plausible qualitative results. 
Test different thresholds for attention scores for all relevant models (COMMA, ours) using the spatio-temporal grounding \textit{mAP} IoU@0.4 on our GroundingYoutube dataset as shown in Table \ref{tab:thre}. We find 0.01 to be a reasonable threshold among all models, performing best on COMMA and giving at least the second best results for the proposed model. 

\noindent \textbf{Mean pooling for global features}
We also tried mean pooling over all tokens for CLIP to replace [CLS] for the global feature. As shown in the tab. \ref{tab:train_ablations_meanpool}, [CLS] outperforms mean pooling in our 3 datasets. We attribute this to the fact that [CLS] was calculated by self-attention, which will automatically select important tokens, whereas mean pooling treats all tokens with the same importance.

\noindent \textbf{Loss ablation with MIL-NCE(S3D)}
We tested the local vs. global loss on top of S3D (initialized by MIL-NCE, see Tab. \ref{tab:train_ablations_s3d}) showing similar behavior compared to CLIP.(shown in Table \ref{tab:train_ablations} in the main paper)
%Spatio-temporal grounding performance in different settings
%Saturated v.s. widespread
