\section{Experimental setup}
%\subsection{Implementation details}
\subsection{Baseline details}
\label{sup:baseline}
MIL-NCE~\citep{miech2020end}, which utilizes S3D~\citep{xie2018rethinking} and word2vec~\citep{mikolov2013efficient} to project two modalities into a common space, is chosen as the standard baseline for this task; 
CoMMA~\citep{tan2021look}, the best-performing model for spatial representations in self-supervised learning (we denote CoMMA$\dagger$ to represent the model that uses weights shared by the author\footnote{We thank the authors for providing code and weights.}); CLIP~\citep{radford2021learning}, an image-text model trained with transformer architecture, is further applied as the backbone and trained with~\citep{tan2021look} to construct CoMMA$\ddagger$; GLIP~\citep{li2022grounded} and RegionCLIP~\citep{zhong2022regionclip}, state-of-the-art image-text grounding models that combine large-scale image caption pretraining and object detection fine-tuning, which we consider weakly supervised as the bounding box proposal network was trained on other human-annotated data.
We further construct a strong baseline out of the best methods for temporal and spatial localization, MIL-NCE+RegionCLIP, where we use MIL-NCE for temporal localization and RegionCLIP for spatial grounding following the inference pipeline of Figure \ref{fig:inference} without additional training. 

\subsection{Backbones and Training}
\label{backbone_and_training}
We evaluate the proposed method on backbones, CLIP \citep{radford2021learning} and S3D-word2vec \citep{miech2020end}.
We described the detailed setup as well as the training in the following.

\noindent \textbf{CLIP models.} For both the visual and text backbone, we use the pretrained weights from CLIP \citep{radford2021learning} with transformer ViT-B/32 and fix the encoder. Both the visual and text encoder has a final embedding size of 512. We apply them to video segments with 12-28 seconds, processing 1 frame per second. An evaluation of how many frames to process (identical to the number of seconds) is shown in Table \ref{tab:frames}. We sampled the video with 5 fps. It shows the best results when we start with 80 possible frames $U$ (as described in Section \ref{frame_sampling}), from which $T$ = 16 frames are selected for training. Ablation of the number of frames $T$ used for training is shown in Table \ref{subtab:ablations2}.
We used a batch size of $B$ = 64 video clips.

\noindent \textbf{S3D-word2vec models.}
For the video backbone, we follow~\citep{tan2021look} and use S3D initialized by MIL-NCE on HowTo100M~\citep{miech2020end} at the rate of 5 frames per second and fix the video encoder. 
The global video clip features were max-pooled over time and projected into embeddings of dimension 512. We used the mean-pooled S3D spatio-temporal features to represent the global representation of the video following the S3D architecture \citep{xie2018rethinking}.
For the text feature, we follow ~\citep{miech2019howto100m} using a GoogleNews pre-trained word2vec model~\citep{mikolov2013efficient} and max-pooling over words in a given sentence to acquire the text global feature.  
We follow \citep{miech2020end} to use the max-pooled word embedding to represent the sentence (global representation) since there is no [CLS] token. Also, the sentence feature is used for the query word selection instead of the [CLS] token. 
We use a batch size of $B$ = 96 video clips.


\noindent \textbf{Training.} For the training of both backbone settings, we use an Adam optimizer~\citep{kingma2015adam} with a learning rate of $1\mathrm{e}{-4}$. 
In the setting of fintining CLIP, we set a learning rate of $1\mathrm{e}{-7}$ for the CLIP backbone. 
The model is trained for 10 epochs on 4 V100 GPUs, which takes about two days. 


\subsection{Inference}
\label{inference_sup}
\noindent \textbf{Inference for the proposed model and CoMMA.} For inference in the case of temporal grounding, as shown in Figure \ref{fig:inference}(a), we first normalize the global feature for video and text. We used a (temporal) threshold $\theta$ = 0.5 to separate detections from the background. In spatial grounding, we acquire an attention heatmap using the attention rollout \citep{abnar2020quantifying} described in Section \ref{inference_section}. We set a spatial threshold $\tau$ = 0.01 to create the mask, as shown in Figure \ref{fig:inference}(b). The choice of this spatial threshold is evaluated in Table \ref{tab:thre}. 



\noindent \textbf{GLIP, RegionCLIP baseline inference.} In spatial grounding, we are given a text query and need to localize it in the frame. GLIP and RegionCLIP predict multiple bounding boxes corresponding to the text query. We select the predicted bounding box with the highest confidence score as the prediction result. We use the center point of the predicted bounding box for the pointing game evaluation as the model prediction. For \textit{mAP} evaluation, we use the predicted bounding box to compute IoU with the ground truth bounding box. In spatio-temporal grounding, we input all possible action description labels as candidates similar to Figure \ref{fig:inference}(a). We pick the class with the highest confidence score as the predicted label. If the model made no prediction, we would predict it as ``background''. The spatial inference is the same as the spatial grounding setting.

\noindent \textbf{TubeDETR, STCAT baseline inference.} TubeDETR and STCAT are spatio-temporal grounding models trained to predict a single spatio-temporal tube per video. 
In both cases, TubeDETR and STCAT, we use models trained on the Vid-STG dataset with 448x448 resolution and evaluate them for the task of spatial grounding. 
Since this dataset contains mostly short videos ($<$30sec), we observed that both methods will also only predict a trajectory tube in this temporal range ($<$30sec), no matter how long the input video is. To allow us to apply them to longer videos ($>$30sec), we split the longer videos based on sliding windows of 5-sec for better performance.




\noindent \textbf{MIL-NCE, CLIP baseline inference.} Both models are trained based on global representations for both input modalities, videos/images and text. We can, therefore, directly compute a sentence-to-video-frame similarity to perform the temporal grounding for Figure \ref{fig:inference}(a), following the same process as the proposed method for temporal grounding. For spatial grounding, we compute sentence-to-region feature similarity. Both visual backbones produce a 7x7 grid feature. We normalize the sentence and region features, then select a spatial threshold $\tau$ = 0.5 to create the mask for the \textit{mAP} evaluation.

\subsection{Evaluation metrics}
\label{eval_metric}


\noindent (i) \textbf{Spatio-temporal grounding in untrimmed video} is evaluated on our annotated GroundingYoutube dataset. We combined the spatial and temporal grounding evaluation as before \citep{kuehne2019mining,akbari2019multi} to form the spatio-temporal evaluation. The entire video and the respective pool of action instructions were provided. The model needs to localize each action step in temporal (start-time/end-time) and spatial (location in the video) as described in Figure \ref{fig:inference}. 
% \noindent\textbf{Inferencing.} The model will need to predict the action label per frame by feature similarity between the video and action classes similar to \citep{Zhukov2019CrossTask}. Later, the model will use its predicted action label as the query to perform spatial grounding to localize the action in the video frame.
We evaluate in two metrics: \textbf{IoU+Pointing game} combines the evaluation setting from the spatial grounding \citep{akbari2019multi} and temporal grounding \citep{kuehne2019mining} metrics. For each video frame, the prediction is correct when the model predicts the correct action for the frame. Also, given the predicted action as a query, the maximum point of the heatmap aims to lie within the desired bounding box. We then compute the Intersection over Union (IoU) over all the predictions with the GT to acquire the final score. 
We also compute \textbf{video mAP} following previous evaluation \citep{gu2018ava}, where we set IoU threshold between GT and predicted spatio-temporal tubes. A prediction is correct when it surpasses the IoU threshold. We then compute the mAP over all classes. We form a 3D prediction mask following Figure \ref{fig:inference} and compute IoU between our 3D heatmap and 3D tube.

\noindent (ii) \textbf{Spatial grounding} is given a text query description to localize the corresponding region in the trimmed video. We use GroundingYoutube, Youcook-Interaction, V-HICO, and Daly for evaluation. %Note that the evaluation is spatial only. It evaluates the results for each frame separately without considering the temporal information. 
This task is evaluated using the \textbf{pointing game accuracy}. Given the query text and video, we compute the attention heatmap on the video as described in Figure \ref{fig:inference}(b). If the highest attention similarity score lies in the ground truth bounding box, the result counts as a ``hit" and counts as ``miss" otherwise. The final accuracy is calculated as a ratio between hits to the total number of predictions $\frac{\text{\# hits}}{\text{\# hits} + \text{\# misses}}$. 
We report the mean average precision \textbf{(mAP)} following the settings from V-HICO \citep{li2021weakly}. Given a human-object category as the text query, we aim to localize the spatial location in the video frame.
The predicted location is correct if their Intersection over-Union (IoU) with ground truth bounding boxes is larger than 0.3. 
Since we do not use any bounding box proposal tools or supervision, we create an attention heatmap as described in Figure \ref{fig:inference}(b) to create a mask for IoU computation. 
We follow \citep{li2021weakly} and compute the mAP over all verb-object classes.


\noindent (iii) \textbf{Temporal grounding} \label{temporal_grounding}
provides videos with the respective actions and their ordering, including the background. The goal is to find the correct frame-wise segmentation of the video. We follow the inference procedure in \citep{kuehne2019mining} to compute the alignment given our similarity input matrix. The task is evaluated by intersection over detection (IoD), defined as $\frac{G \cap D}{D}$ the ratio between the intersection of ground-truth action $G$ and prediction $D$ to prediction $D$, and the Jaccard index, which is an (IoU) given as $\frac{G \cap D}{G \cup D}$.

