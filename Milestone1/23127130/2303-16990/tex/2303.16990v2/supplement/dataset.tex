


\section{GroundingYoutube Annotation}
\label{GroundingYoutube_Annotation}

%We include more visualizations of our annotated GroundingYoutube dataset in video format at {\color{blue}\textbf{sup/Annotation visualization videos.pptx}}.

The data annotation was divided into three phases:
During \emph{Phase I} (Sec. \ref{sec:dataset:annotation:ui_task}, a graphical user interface (UI) and the task description were developed. In \emph{Phase II}, the dataset was given to the annotators to generate the key points (Sec. \ref{sec:dataset:annotation:ui_task}). In \emph{Phase III}, a manual quality control step was performed (Sec. \ref{sec:dataset:quality_control}).
%\hkc{Sample size calculation in the rebuttal}
\subsection{Development of the graphical user interface and task description}
\label{sec:dataset:annotation:ui_task}
The annotation of a large amount of data is often one of the most expensive aspects of a machine learning pipeline design, which is why the annotation time per datum should be kept as short as possible. 
There are two points that can be optimized, (1) the training or the task ``message'' for the annotators and (2) the graphical user interface by minimizing interaction times.


\begin{figure}[!h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/dataset_annotation/annotation_ui.png}
    \caption{\textbf{A screenshot of our simplified annotation interface.} On the top, the annotation task is described in simple and short words to save reading time. To make interacting with the UI as intuitive as possible, actions are limited to simple button clicks and setting the key point by clicking on the image.}
    \vspace{-0.1cm}
    \label{fig:annotation:annotation_ui}
\end{figure}


While tasks are usually formulated in such a way that no ambiguities arise, i.e. all possible edge cases are somehow covered, and simple words are used, in this case, we made a conscious decision to choose questions as short as possible, and that would give the annotator room for interpretation. We did this because it was hard to predict where people would actually locate actions in images. 
We also created a 1 min 30 sec long user training video where we demonstrate the task using exemplary keypoint annotations and explain how to use the UI.
% In order to understand the task even faster, we have created a 1 min 30 sec long user training video, where we demonstrate the task again using exemplary keypoint annotations and explain how to use the UI.


Our annotation UI was designed with a special focus to keep it as intuitive as possible and reducing the interaction time. 
Our UI only provided five functionalities (set/unset a keypoint, undo the last image, image can't be solved, and image is corrupt) which were clearly described in text buttons (see Figure  \ref{fig:annotation:annotation_ui}).
% This means, that our UI only provided five functionalities (set/unset a keypoint, undo the last image, image can't be solved and image is corrupt) in total, all clear described as text buttons (see Fig. \ref{fig:annotation:annotation_ui}).
Further, to reduce the cognitive load of our workers, images were presented in the form of work packages, each containing $25$ images. Hence, we could ensure that completing a task would take no longer than 6 minutes.



% \subsection{Dataset annotation}
% \label{sec:dataset:annotation:annotation}

The annotation of all $26,987$ images was performed with five distinct repeats per image, resulting in $134,935$ labels in total. All labels were generated by $13$ professional annotators in total, which took them $5s$ in average per image. However, it should be noted that the number of images where an annotator placed a keypoint differs along all the workers (see Figure \ref{fig:annotation:number_of_answers_per_annotator}) and that the vast majority of all images have been answered by five annotators only. Examples are shown in Figure \ref{point_ex}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/dataset_annotation/distribution_number_of_keypoints.png}
    \caption{\textbf{Number of keypoints per image}. It can be seen that $48\%$ of the data has all 5 key points and $19\%$ has not a single annotation}
    \label{fig:annotation:number_of_answers_per_annotator}
\end{figure}

% \begin{figure}[h]
% \centering
% \subfloat[Can't solve][can't solve]{
% \includegraphics[width=0.3\textwidth]{figures/dataset_annotation/quality_control_classes/error_cant_solve.png}
% \label{fig:annotation:answers_cant_solve}}
% \qquad

% \subfloat[Corrupt image][Corrupt image]{
% \includegraphics[width=0.3\textwidth]{figures/dataset_annotation/quality_control_classes/error_corrupt.jpg}
% \label{fig:annotation:answers_corrupt}}
% \caption{Illustration of an can't solve or corrupt image}
% \label{fig:quality_aspects}
% \end{figure}





%All labels were generated by $13$ professional annotators in total.  
During the annotation, professional annotators were given a short instruction video at the beginning and then asked to click on the center of the given action without additional instructions. They were further free to choose ``can't answer" if they could not locate the action, e.g., at the beginning and end of the clip. Thus, the number of available key points per image differs, and we choose majority voting to determine whether an action is present, resulting in new, refined temporal boundaries compared to the original annotation. 

\begin{figure}
%\vspace{-0.5cm}
    \centering
    \begin{subfigure}{.49\textwidth}
    %%%%%%%%%%%%%
    
    %\centering
    \subfloat[Can't solve][\scriptsize Can't solve]{
    \includegraphics[width=0.46\textwidth]{figures/dataset_annotation/quality_control_classes/error_cant_solve.png}
    \label{fig:answers_cant_solve}}
    %\qquad
    \subfloat[Corrupt image][\scriptsize Single point]{
    \includegraphics[width=0.46\textwidth]{figures/dataset_annotation/quality_control_classes/single_point.png}
    \label{fig:single_point}}
    \\
    \subfloat[Corrupt image][\scriptsize Four points]{
    \includegraphics[width=0.46\textwidth]{figures/dataset_annotation/quality_control_classes/four_points.png}
    \label{fig:answers_corrupt}}
    %\qquad
    \subfloat[Corrupt image][\scriptsize Five points]{
    \includegraphics[width=0.46\textwidth]{figures/dataset_annotation/quality_control_classes/five_points.png}
    \label{fig:answers_corrupt}}
    %\caption{Illustration of an ``can't solve'' image \bc{4 examples: }}
    %\label{fig:can_solve}
    \end{subfigure}%
    %%%%%%%%%%%%%%
    %%%%%%%%%%%%%%
    
    %%%%%%%%%%%%%%%%%%%
    \caption{\textbf{Sample annotations}. The purple point represents the center point of the annotations in the frame. $48\%$ of the data has all 5 key points, and $19\%$ has not had a single annotation. \label{point_ex}
    % It can be seen that the vast majority of answers were generated by five annotators. % \hkc{probably take out that one ... it's not favorable in times of crowd sourcing ... }-  
      }
    \vspace{-0.2cm}
\end{figure}

% \begin{figure}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Number of keypoints]{
%           \includegraphics[width=0.5\textwidth]{figures/dataset_annotation/distribution_number_of_keypoints.png}
%           \label{fig:answers_per_annotator}}
% \end{figure}%
\begin{figure}[h]
\centering
%\resizebox{.9\linewidth}{!}{
    %%%%%%%%%%%%%%
    \begin{subfigure}{.16\textwidth}
    \centering
    \subfloat[Number of labels generated][\scriptsize Wider shot]{
\includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/answers_concentrated.png}
\label{fig:answers_concentrated}}
    \end{subfigure}%
    %%%%%%%%%%%%%%
    %%%%%%%%%%%%%%
    \begin{subfigure}{.16\textwidth}
      \centering
      \subfloat[Number of labels generated][\scriptsize Flow action]{
\includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/add_oil.png}
\label{fig:answers_no_clear_action}}
    \end{subfigure}%
    %%%%%%%%%%%%%%
    %%%%%%%%%%%%%%
    \begin{subfigure}{.16\textwidth}
      \centering
      \subfloat[Number of labels generated][\scriptsize Split to two]{
\includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/error_multiple_actions.png}
\label{fig:answers_multiple_actions}}
    \end{subfigure}%
    %%%%%%%%%%%%%%
\caption{\textbf{Example of keypoint annotations under different conditions.} }
\label{fig:quality_aspects}
\vspace{-0.5cm}
\end{figure}
%\input{figures/boxes.tex}

We found that the point-wise annotation resulted in roughly three distinct patterns, which depend on the captured scenario, as shown in Figure~\ref{fig:quality_aspects}. In the case of half portrait or even wider shots in Figure~\ref{fig:answers_concentrated}, annotations are highly locally centered. % while frames with large object focus tend to have more widespread and less dense annotation in Figure~\ref{fig:answers_widespread}. 
We further found that in some cases, the point annotation can also represent the flow of the action, e.g., pouring oil in Figure~\ref{fig:answers_no_clear_action}, or even split into two separate clusters in Figure~\ref{fig:answers_multiple_actions}. 


\subsection{Quality control}
\label{sec:dataset:quality_control}
Since the label quality of the datasets used is a critical factor in the performance of machine learning models, we verified the correctness of a subset of our images using an experienced annotation specialist for $1,026$ randomly selected frames. %\hkc{Sample size calculation in the rebuttal} % todo reference label quality effects outcome
%
To evaluate the data quality, we evaluate the agreement between the annotation specialist and the annotations provided by the annotators. To this end, we considered an annotation as a false positive if three annotators or more have set a key point, although no action can be seen in the image, and as a false negative if three annotators or more have not set a key point, even though an action can be seen in the image. The entire sample was assessed using these criteria, with the specialist disagreeing with the annotators in only a total of $1.1\% \pm 3\%$ (FP: $0.7\% \pm 3\%$, FN: $0.4\% \pm 3\%$). We also found that annotations significantly diverted in terms of spread. Namely, wider shots tend to be highly centered, whereas zooming in together with the usage of larger objects such as a pan or a spatula results in more widespread key points. We also analyzed how often those cases occur and found that $14.0\%$ of the selected frames show a widespread pattern. 



\paragraph{Sample size calculation}

To this end, we first needed a representative subset of $N_S$ images of our data. We calculated the required sample size based on the following two formulas:
\begin{equation}
    N_0 = \frac{z^2}{\epsilon^2} \cdot p \cdot (1-p)
\end{equation}
where $\alpha$ is the confidence interval, $p$ the expected probability of the appearance of a quality aspect (e.g., widespread answers), $epsilon$ is the accepted error margin, and $Q(\alpha)$ is the percent point function of a normal distribution and $z=Q(1-\frac{\alpha}{2})$. %TODO REFERENCE PERCENT POINT FUNCTION FOR NORMAL DISTRIBUTION

As $N_0$ would be the required sample size for an infinitely large population, we applied the finite population factor that results from sampling without replacement from a finite population.  % TODO REFERENCE FOR FINITE POPULATION FACTOR
\begin{equation}
    N_S = \frac{(N_0 \cdot N)}{N_0 + (N-1)}
\end{equation}

where $N$ is the total number of images.

We set $\alpha=95\%$, $epsilon=3\%$, and our sample size of $N=26,987$. As the probability of the quality aspect is unknown, we set $p=50\%$, which resulted in $1,026$ being checked for quality control.

\input{supplement/tables/wide}


% \noindent\textbf{Task}
% \noindent\textbf{Dataset}
\subsection{Dataset usage for evaluation}
\label{sec:dataset:evaluation}
%\hkc{we need to somehow describe how we turn points in bounding boxes ... } \\
\noindent\textbf{How to derive bounding box?} 
%For evaluation purposes, we get the union of all annotated points in a single frame with additional distance respect to the height $H$ and width $W$ as shown in Figure \ref{fig:boxes}.
We derive bounding boxes by adding a constant distance (0.15$\times$frame\_width in width, 0.15$\times$frame\_height in height) to the boundaries of a union of all annotated points (as shown in Figure \ref{fig:boxes}).
Since point annotations may be scattered in the image or, conversely, gathered around a point, the output bounding boxes will vary in size over time and for different actions. 
% As reported in Supplement line 1236, we measure 14\% of widespread annotations (defined in line 1268) and provide the performance analysis of the widespread vs. saturated actions in Tab. 13.
We manually check the auto-generated bounding boxes and adjust the bounding box when needed. 



\noindent \textbf{Performance on widespread and saturated action.}
We evaluate the performance of different action distributions using the spatio-temporal grounding \textit{mAP} IoU@0.4 setting. We define widespread actions to have an area larger than a certain threshold $A$. Here, we set $A=60,000$ pixels. As shown in Table \ref{tab:wide}, the performance of the widespread actions was higher since it had a higher tolerance of spatial localization error. %Widespread action examples can be found in Figure \ref{fig:boxes}.

