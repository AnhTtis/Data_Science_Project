\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{tabularx}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}

\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4376} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\rem}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}
%\newcommand{\one}{\textcolor{purple}{R1}}
\newcommand{\one}{\textbf{\textcolor[HTML]{d35400}{R1}}}
\newcommand{\two}{\textbf{\textcolor[HTML]{2980b9}{R2}}}
\newcommand{\three}{\textbf{\textcolor[HTML]{16a085}{R3}}}
\def\red{\textcolor{red}}
\renewcommand{\thetable}{\Alph{table}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{ \normalsize		A Global-to-Local Approach for Multimodal Self-Supervised Spatio-Temporal Grounding in Untrimmed Videos}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix
\vspace{-0.7cm}

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\noindent %We thank the reviewers for their feedback.
We thank the reviewers for their constructive and detailed feedback. 
We will incorporate all feedback and release all data annotations and code for evaluation.


\noindent\textbf{[\one{}(C7pJ), \two{}(5JLB)(1,4), \three{}(qMcv)(1)] Supervision.} 
Spatio-temporal grounding in videos can be learned with different levels of supervision. In \textit{supervised} setting, the model is trained with video and a human-labeled sentence with the target bounding box annotation. The papers listed by R2 [1,2,3,4] fall in this category. Another line of work utilizes \textit{weakly supervised} data for training, where the human-annotated sentence was given without bounding box supervision. We follow the term \textit{self-supervised} setting as previous works cited in our main paper [4,5,34]. In this setting, we learn with the text generated from the video's audio using ASR tools that require no human label effort in training. 
We propose adding a paragraph about supervised grounding in our related work to clarify the difference.

% between previous works.
% Another aspect of difference in terms of annotation in testing is the difference between object-centric versus activity-centric annotation. Take the activity \textit{cracking egg} as an example. Previous work such as [1,3] will annotate multiple trajectories of different eggs while we annotate the intersection point of human and object throughout the video, which represents the same activity. 
% The last aspect of the difference in the testing annotation is the difference between trimmed and untrimmed videos. Unlike previous works that annotate trimmed videos with around 5~10 seconds, we annotate all the possible actions in the entire long video for around 3~5 minutes, including the background. This annotation will benefit learning relations or order across actions in future works.


% chatGPT
% Spatio-temporal grounding in videos can be learned at different levels of supervision. Our approach utilizes a \textit{self-supervised} setting following the term by previous works cited in our main paper [4,5,34], where the model is trained with video and text generated from the video itself using ASR tools, requiring no human label effort. This differs from the \textit{supervised} setting used in previous works such as [1,2,3,4] in R2's list, where the model is trained with video and human-labeled sentences with target bounding box annotations. This also differs from the \textit{weakly-supervised} setting where the model is trained with human-labeled sentences without bounding box annotation. 

\noindent\textbf{[\one{},\two{}(4),\three{}(1)] Dataset diff from previous works.} 
Our work utilizes activity-centric annotation in testing, as opposed to the object/person-centric annotation used in previous works such as [1,3]. Take the activity \textit{crack egg} as an example; prior work annotated the human and egg trajectories separately while we annotate the intersection point of human and object throughout the video, representing the same activity. We also annotate the entire untrimmed videos with 3-5 minutes, including the background, as opposed to the trimmed videos with around 5-10 seconds used in previous works. We hope this annotation will benefit learning temporal patterns across actions in future works. 
%We propose to add a paragraph about supervised grounding in our related work to make the difference to previous works clear.


\noindent\textbf{[\one{},\two{}(2b),\three{}(2)] Architecture and loss contribution. } 
Note that all elements, including the local representation module, are only parts of the proposed joint system. In Table \red{4c}, we show not only that each loss contributes to the task of spatio-temporal grounding on the GroundingYT but also that the whole is more than the sum of its parts (losses) since this task requires both spatial and temporal detection. 
The reduced impact of the global loss in the case of YouCook-Inter is based on the fact that this is a pure spatial grounding dataset (no background frames) without temporal detection, and the local loss plays a more critical role. 
By comparing the results for spatial grounding in trimmed videos (Table \red{1}) vs. spatio-temporal grounding in untrimmed videos (Table \red{3}), we can further see the impact of the proposed joint representation.



\noindent\textbf{[\two{}(1),\three{}(3)] Baseline comparisons.} 
To our best knowledge, our work and CoMMA are the only two models that learn grounding from self-supervision.
As suggested by reviewers, we compare our methods with the supervised approach TubeDETR [3] and STCAT [1] with trained models on the Vid-STG dataset with 448 resolution. We also include a weakly supervised approach, RegionCLIP. 
We test them on the spatial grounding and evaluate with pointing game accuracy. Since methods [1,3] will only predict one trajectory given a long video ($>$20sec), we perform sliding windows of 5-sec for better performance.
An initial comparison is shown in Table \ref{tab:baselines}. Note that all results are subject to domain gaps and based on different backbones. We will add an extended version of Table \ref{tab:baselines} 
 and also include evaluations on Vid-STG \& HC-STVG in the final version. 

%As shown in Table \ref{tab:baselines}, our approach outperforms the supervised method in the instruction domain (left two). This can be due to the domain gap between different videos and sentences. We also achieved competitive performance in the general domain (right two).
%Note that our method is also the first one to extend this task to perform well on untrimmed spatial-temporal grounding.

\begin{table}[t!]
     \tablestyle{2pt}{1.05}
    %\vspace{-0.3cm}
    \centering
    %\resizebox{1\columnwidth}{!}{
    \begin{tabular}{@{}l|c|cccc}
    	\toprule
    	%\multicolumn{4}{c}{} &\multicolumn{2}{c}{MiningYoutube}  \\ 
    	%\cmidrule(lr){5-6} 
    	Method   & Supervision & YT-Inter & GroundYT & V-HICO & DALY \\ 
    	\midrule
            TubeDETR [3]  &  Supervised & 51.63 & 53.24 & 63.23 & 84.21   \\
    	STCAT [1] &  Supervised & 54.47 & 55.90 & 65.34 & 85.42   \\
            RegionCLIP  & Weak & 51.56 & 52.84  &  57.92 &  67.12  \\
            Ours (HT200K)  & Self &  57.10 & 55.49  &  60.71 & 70.08 \\
            Ours (HT100M)  & Self &  57.04 & 55.87  &  61.75 & 71.31 \\
             %
            % MCN \cite{chen2021multimodal}      &VAT& R152+RX101   & 23.10 & 32.04    \\
    	\bottomrule
    \end{tabular}
    \vspace{-0.6cm}
    \caption{\textbf{Spatial grounding pointing game.} 
    \label{tab:baselines}
    \vspace{-0.7cm}
    }
\end{table}

\noindent\textbf{[\one{},\three{}(2)] $K$ words in frame selection?} 
Sorry for the typo. The $K$ in lines 321 \& 332 should be replaced by $T$ and is calculated between $T$ words and $U$ frames. We select $T$ words because we want to select a small number of frames ($T$) from the longer video clip (5-10 sec at 30fps) so that each relevant word in the sentence is assigned to a unique frame. Hence, we use the Sinkhorn algorithm, where we constrain the matrix $\mathbf{Q}\in\mathbb{R}_+^{U\times T}$ to output $T$ assignments. Thus $T$ words result in $T$ selected frames. 
%As shown in Table \red{4a}, with such a pre-sampling strategy, we increase 2\% in accuracy.
%Due to memory efficiency, we select $T$ frames since we are processing 5-10 sec video containing numerous frames with redundant information. 
% Frame selection
% 1 resource -sup table5
% , resource intensive lots of redundant. better start for 2 point plus, pre cleaning/ beter sample in table 4(a). We will make this clear in our revision.


%\input{tables/new_base.tex}

\noindent\textbf{[\one{}] Performance in general domain.} 
In Table \red{1} in our main paper, V-HICO and DALY datasets are general domain human-object interaction videos with various object and action classes. We will clarify this and expand the ablation studies (Table \red{4}) on these two datasets in our main paper.
We will also train our model on the two supervised datasets VidSTG[4] and HC-STVG[2], on camera ready.


\noindent\textbf{[\one{}] Optimal transport vs. bipartite matching.} 
SK optimal transport is a form of bipartite matching. %It was given the matrix representation of the given bipartite graph and scaled with a few steps of the Sinkhornâ€“Knopp algorithm.
We offer to include other bipartite matching algorithms, e.g., Hungarian matching, for comparison upon request. Numerically, the optimal assignment should not be drastically different.

\noindent\textbf{[\one{}] Generate temporal boundary.} 
Temporal boundary was generated using text-to-video similarity and a threshold described in supplement lines 44-48. 
%As shown in Figure \red{3}, the temporal boundary of "crack egg" will be $[t4,t5]$ given that the action has the highest similarity. 
We will move the description from the supplement to the main paper.

\noindent\textbf{[\two{}(2a)] Difference to cross-model similarity.} 
[5] utilize the SK algorithm to create pseudo-label prototypes as a self-supervised task for learning. %, which is in the line of self-supervised clustering paper SwAV. 
We do not perform any pseudo-label prediction in training. Instead, we utilize SK as a frame selection algorithm for clearer supervision.% in learning to ground.

% \noindent \textbf{\two{}(2b) Difference to CoMMA} We implement the local representation module with different transformer architecture. In direct comparison, our local only loss performs slightly better than CoMMA (Ours-local-only: YTI:54.3, GYT:5.7 (Table 4c) vs CoMMA: YTI:52.65, GYT:0.99 (Table 1 and 3)). The reduced impact of the global loss in case of YouCook-Inter is based on the fact that this is a pure spatial grounding dataset (no background frames), where the temporal information is not crucial, and the local loss plays a more important role. 

 
 %As shown in Table \red{1}, we demonstrate that our model achieved better localization ability than CoMMA guided by the frame selection using global representation. 
 
%also talk about table 3

\noindent\textbf{[\two{}(3)] Larger data drop performance?} 
%We trained the model on the three datasets with the same epochs. 
The slight decrease is due to an increased domain gap. The HT200K (20\% of HT100M) is from the food category (line 492), closer to the downstream YT-Inter and GroundingYT. HT100M contains more categories, which contribute less to the performance. Training with HT100M achieves performance gain in general domain videos (Table \ref{tab:baselines}) but still suffers from a domain gap between YT and daily activities.

%refer to table 1


% \noindent\textbf{[\one{},\two{}(2b),\three{}(2)] Dependency on local representation.} 
% We implement the local representation module with different transformer architecture. Also, the local representation module is part of the entire system, including our frame selection pipeline and global representation. As shown in Table \red{1}, we demonstrate that our model achieved better localization ability than CoMMA guided by the frame selection using global representation. 
% The YouCook-Inter dataset is pure spatial grounding, where the global (temporal) information is not crucial, and local loss plays a more important role. In Table \red{4c}, we show the global loss contributes more to the GroundingYT dataset since this task requires temporal information for the spatio-temporal grounding. By comparing Table \red{1} and \red{3}, we can see the effect of the global representation contributes most in the untrimmed setting.


% %%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
