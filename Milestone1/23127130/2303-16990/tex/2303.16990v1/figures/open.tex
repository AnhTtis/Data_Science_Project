\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/Teaser_STG.pdf}
    
    \caption{\textbf{Learning Spatio-temporal grounding in untrimmed videos:} 
    In training, we learn from unlabeled videos without human annotation. In evaluation, we perform spatio-temporal grounding using an action description such as ``crack egg'' as a query. The model needs to localize both the action's temporal boundary and spatial region in the long untrimmed video. %At the bottom figure, each frame in the video consists of up to 5 points annotation (shown as $\oplus$ in the figure) representing the location of a given action step, e.g., Crack eggs. 
    We visualize the heat-map from the annotation points as well as derived bounding boxes.% were derived from the point annotation using the union of annotated points with additional distance for later evaluation.  
    }
    \label{fig:open}
    \vspace{-15pt}
\end{figure}