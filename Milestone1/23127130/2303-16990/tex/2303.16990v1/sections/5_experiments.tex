\input{tables/spatial_temporal_long}


\section{Experiments}

\subsection{Datasets} \label{dataset}
\noindent \textbf{Training Data:} \textbf{HowTo100M dataset} contains 1.2M instructional videos along with their corresponding automatically generated speech (ASR) transcriptions. We randomly selected 200K  video clips from the \textit{Food and Entertaining} category for training. %, and thus, we mainly focus on instructional videos in the area of cooking and kitchen tasks. 
\\
%\input{tables/retrieval}
%\input{tables/spatio_vhico}

\vspace{-0.3cm}
\noindent\textbf{Downstream Datasets:} %\textbf{YouCook2}: For the text-to-video retrieval downstream task, we use the common YouCook2 dataset , which provides a human-generated caption for 3.5K video clips for cooking instruction. 
%blah blah ... \hkc{add some details here?}
\textbf{GroundingYoutube (GYT)} is used to evaluate the task of multi-action spatio-temporal grounding as described in Section \ref{sec:dataset:annotation}.
% , we annotated the dense spatio-temporal location information as described in Section \ref{sec:dataset:annotation}.
%for 512 verb-noun phrases. All occurrences of the specific phrase in the test video are hence annotated, allowing us to evaluate spatio-temporal grounding in full untrimmed videos.
\noindent\textbf{MiningYoutube (MYT)} \cite{kuehne2019mining} %: To evaluate the temporal grounding abilities, we leverage the MiningYoutube \cite{kuehne2019mining} dataset, as it 
provides temporal annotation and is limited to the domain of cooking instruction videos. The dataset features 250 full instructional videos, which are annotated with 512 action classes and temporal boundary information. 
We use it to evaluate the temporal grounding abilities.
%Here, temporal alignment, the task of finding the right temporal boundaries given the sequences of actions, is used during evaluation to relax the task of temporal detection. 
\noindent\textbf{YouCook-Interaction (YC-Inter)} \cite{tan2021look} is an extension of the YouCook2 dataset \cite{zhou2018towards} for cooking instruction, which provides bounding boxes for 6K selected frames. The bounding boxes usually comprise the hand and the tool mentioned in the respective sentence-wise annotation. We evaluate the spatial grounding abilities of models on this dataset.
% \noindent\textbf{YouCook2-Interaction (YC-Inter)}  To evaluate the spatial grounding abilities of our system, we use the YouCook2-Interaction dataset \cite{tan2021look}, an extension of a subset of the YouCook2 dataset \cite{zhou2018towards} for cooking instruction, which provides bounding boxes for 6K selected frames. The bounding boxes usually comprise the hand and the tool mentioned in the respective sentence-wise annotation.    
To further benchmark on general video domains with human-object interaction, we test spatial grounding on the \textbf{V-HICO} dataset \cite{li2021weakly} with 6.5k videos with human-object interaction bounding boxes annotations, 
% that have been semi-automatically curated from sentence captions, 
as well as on the \textbf{Daly} action dataset \cite{weinzaepfel2016human}, featuring videos consisting of daily actions such as ``brushing teeth''.% and ``cleaning windows''.

\input{tables/spatial}

\input{tables/temporal}

\subsection{Baseline methods}

The proposed system is compared to various multimodal methods based on self- and weak supervision to evaluate the approach and related data annotation. Namely, we choose MIL-NCE~\cite{miech2020end} as the standard baseline for this task, which utilizes S3D\cite{xie2018rethinking} and word2vec\cite{mikolov2013efficient} to project two modalities into a common space. We include CoMMA~\cite{tan2021look} as the best-performing model for spatial representations in self-supervised learning. We noted as CoMMA* to represent the model uses weights shared by the author\footnote{We thank the authors for providing code and weights.}.
CLIP \cite{radford2021learning} is an image-text model trained with transformer architecture on image caption pairs which shows great results on multimodal video tasks \cite{luo2022clip4clip}. We further apply CLIP as the backbone and train with \cite{tan2021look} to construct  CoMMA$\dagger$. GLIP\cite{li2022grounded} and RegionCLIP \cite{zhong2022regionclip} are state-of-the-art image-text grounding models to combine the large-scale image caption pretraining and object detection finetuning. Such supervision is considered weak supervision since the bounding box proposal network was trained on other human-annotated data.
We also include two fully supervised models, TubeDETR \cite{yang2022tubedetr} and STCAT \cite{jin2022embracing} trained on Vid-STG dataset \cite{zhang2020does} with 448 resolution for comparison. The two models utilize human-annotated referring expression sentences and corresponding bounding boxes during training.
For the models using S3D\cite{xie2018rethinking} visual backbones, we use the pre-trained weights from MIL-NCE~\cite{miech2020end} for initialization. For the models using word2vec features, we follow \cite{miech2020end} to use the max-pooled word embedding to represent the sentence (global representation) since there is no [CLS] token. Also, the sentence feature is used for the query word selection instead of the [CLS] token. We used the mean-pooled S3D spatio-temporal features to represent the global representation of the video following the S3D architecture \cite{xie2018rethinking}.
For CLIP\cite{radford2021learning} backbones, we use the pretrained transformer ViT-B/32. 
More implementation details and experimental settings are in the supplementary.

%\input{tables/spatial_temporal_long}

\subsection{Downstream Tasks}


%We compare to the SOTA self-supervised method evaluated on spatial \cite{tan2021look} and temporal \cite{kuehne2019mining} grounding.

We considered the following downstream tasks to evaluate spatio-temporal grounding abilities of various models:

\noindent (i) \textbf{Spatio-temporal grounding in untrimmed video} is evaluated on our annotated GroundingYoutube dataset. We combined the spatial and temporal grounding evaluation as before \cite{kuehne2019mining,akbari2019multi} to form the spatio-temporal evaluation. The entire video and the respective pool of action instructions were provided. The model needs to localize each action step in temporal (start-time/end-time) and spatial (location in the video) as described in Figure \ref{fig:inference}. 
% \noindent\textbf{Inferencing.} The model will need to predict the action label per frame by feature similarity between the video and action classes similar to \cite{Zhukov2019CrossTask}. Later, the model will use its predicted action label as the query to perform spatial grounding to localize the action in the video frame.
We evaluate in two metrics: \textbf{IoU+Pointing game} combines the evaluation setting from the spatial grounding \cite{akbari2019multi} and temporal grounding \cite{kuehne2019mining} metrics. For each video frame, the prediction is correct when the model predicts the correct action for the frame. Also, given the predicted action as a query, the maximum point of the heatmap aims to lie within the desired bounding box. We then compute the Intersection over Union (IoU) over all the predictions with the GT to acquire the final score. 
We also compute \textbf{video mAP} following previous evaluation \cite{gu2018ava}, where we set IoU threshold between GT and predicted spatio-temporal tubes. A prediction is correct when it surpasses the IoU threshold. We then compute the mAP over all classes. We form a 3D prediction mask following Figure \ref{fig:inference} and compute IoU between our 3D heatmap and 3D tube.

\noindent (ii) \textbf{Spatial grounding} is given a text query description to localize the corresponding region in the trimmed video. We use GroundingYoutube, Youcook-Interaction, V-HICO, and Daly for evaluation. %Note that the evaluation is spatial only. It evaluates the results for each frame separately without considering the temporal information. 
This task is evaluated using the \textbf{pointing game accuracy}. Given the query text and video, we compute the attention heatmap on the video as described in Figure \ref{fig:inference}(b). If the highest attention similarity score lies in the ground truth bounding box, the result counts as a ``hit" and counts as ``miss" otherwise. The final accuracy is calculated as a ratio between hits to the total number of predictions $\frac{\text{\# hits}}{\text{\# hits} + \text{\# misses}}$. 
We report the mean average precision \textbf{(mAP)} following the settings from V-HICO \cite{li2021weakly}. Given a human-object category as the text query, we aim to localize the spatial location in the video frame.
The predicted location is correct if their Intersection over-Union (IoU) with ground truth bounding boxes is larger than 0.3. 
Since we do not use any bounding box proposal tools or supervision, we create an attention heatmap as described in Figure \ref{fig:inference}(b) to create a mask for IoU computation. 
We follow \cite{li2021weakly} and compute the mAP over all verb-object classes.


\noindent (iii) \textbf{Temporal grounding} \label{temporal_grounding}
provides videos with the respective actions and their ordering, including the background. The goal is to find the correct frame-wise segmentation of the video. We follow the inference procedure in \cite{kuehne2019mining} to compute the alignment given our similarity input matrix. The task is evaluated by intersection over detection (IoD), defined as $\frac{G \cap D}{D}$ the ratio between the intersection of ground-truth action $G$ and prediction $D$ to prediction $D$, and the Jaccard index, which is an (IoU) given as $\frac{G \cap D}{G \cup D}$.

\input{figures/visualization.tex}


\subsection{Comparison with state-of-the-art methods}
\noindent (i) \textbf{Spatio-temporal grounding in untrimmed video:}
We first compare the proposed method to other approaches designed for either spatial or temporal grounding in Table \ref{tab:st_long}.
It shows that the proposed method improves over the other baselines demonstrating the model's ability to incorporate global (temporal) and local (spatial) representations. Further, models without specific loss designs for spatial grounding (MIL-NCE\cite{miech2020end}, CLIP\cite{radford2021learning}) show good mAP scores but lower pointing game accuracy. Out of the two weakly supervised methods, GLIP\cite{li2022grounded} and RegionCLIP\cite{zhong2022regionclip}), trained with aligned image-text, RegionCLIP show significantly better performance in this setting, while both perform in a similar range in the spatial grounding scenario (see Table~\ref{tab:spatial}) with GLIP even outperforming RegionCLIP on the V-HICO dataset. We attribute this behavior to the fact that RegionCLIP is rather able to distinguish frames with relevant queries from background frames than GLIP, leading to better temporal localization.
%Models designed for trimmed videos (CoMMA\cite{tan2021look}) or trained with aligned image-text (GLIP\cite{li2022grounded}, RegionCLIP\cite{zhong2022regionclip}) failed to capture the temporal dynamics, while models without specific loss designs for spatial grounding (MIL-NCE\cite{miech2020end}, CLIP\cite{radford2021learning}) were not able to ground the action in the correct region.
Note that supervised spatio-temporal grounding approaches \cite{yang2022tubedetr,jin2022embracing} are not directly applicable in this evaluation since such methods assume the given text query to be ground-truth. The model must distinguish the correct text query from a pool of action lists. We include an evaluation setting in the supplement where the GT-text queries were provided.
%More experiment setting is in the supplement.

\noindent (ii)~\textbf{Spatial grounding: } 
 %We do not report mAP on Youcook interaction since the input is sentence descriptions instead of class.
Second, we compare the performance of the proposed framework to other methods on the task of spatial grounding, including models with weak supervision, as well as models trained in a fully supervised setting.
%As shown in Table \ref{tab:spatial}, models trained with global representations such as MIL-NCE and CLIP were not able to localize the text description compared to models learning local representations such as CoMMA, GLIP, RegionCLIP and our approach. 
In the instruction video domain (GYT, YC-Inter), the proposed approach achieves the best result among all weakly and self-supervised trained methods. In the general domain, such as V-HICO and Daly, the method also achieves competitive results, showing the generalizability of the model to other domains. We attribute this to the transformer architecture in the text branch inheriting knowledge from the open domain during large-scale training, while in contrast the model's performance using word2vec dropped in these datasets. Note that in the Daly dataset, the classes are verbs, which are not detectable by the object-focused model GLIP. 
Compared to their weakly trained counterparts, fully-supervised model (TubeDETER \cite{yang2022tubedetr}, STCAT\cite{jin2022embracing}) achieve decent performance in the general domain (V-HICO, Daly) and slightly lower performance in instruction domain (GYT, YC-Inter) due to the domain gap with respect to the training data.

\noindent (iii)~\textbf{Temporal grounding:}
We finally compare the task of temporal grounding in Table \ref{tab:temporal}. It shows that global representations also profit from local representation learning, achieving state-of-the-art results in temporally localizing actions in untrimmed videos. This hypothesis is further validated in the ablation studies in Table~\ref{tab:train_ablations}, where we ablate both losses for all three settings and show a consistent improvement in the joint loss formulation. 

%Our model achieved comparable results with 
%Called action step localization. Evaluated on Mining Youtube. 




%\input{tables/spatial_temporal_clip}






% \noindent (iv) \textbf{Spatio-temporal Clip :}
% \label{ST_clip}
% Following the current spatio-temporal datasets \cite{jiang2014thumos,gu2018ava} which aim to discriminate the action class from the background class in a short clip, we construct a clip level evaluation where the clip varies from 9 sec to 60 Section  Given an action step, we append the video segments before and after the steps with the same time length of the action step to form the final video clip. This results in 2,895 clips for the spatio-temporal clip grounding evaluation.
% For each clip, the  temporal action intervals occupy 33\% of corresponding videos, which demonstrates the difficulty of the setting. As shown in Table \ref{tab:st_clip}, we observe a similar trend as the full video evaluation where our model outperforms all the baselines. 

\input{tables/ablation}


\subsection{Ablation study} 
\vspace{-1mm}
We perform ablation studies with respect to all three settings, spatio-temporal grounding as well as spatial and temporal grounding alone, reporting performance for spatio-temporal grounding on GroundingYT using mAP with IoU@0.4, on temporal grounding using MiningYT IoU, and on spatial grounding using YouCook-Inter. pointing game.  %For each setting, we use the same feature extractor for three modalities as described in Sec 4.1 for a fair comparison. 
%(More ablations are in the supplement.)
% add summary here?
% as they are the less computational evaluation tasks.
%This subset of downstream tasks has been chosen for their simplicity of evaluation and because they cover a wide range of tasks.

\noindent\textbf{Frame selection strategy.} 
We perform an ablation on the frame selection strategies in Figure \ref{fig:pipeline}(b). In Table \ref{tab:train_ablations}, \textit{None} uses the ASR boundary ($U=T$) as our video training data. \textit{Global} utilizes the global sentence feature [CLS] token as the query to rank the top $T$ similar frames as the selected frames for training. \textit{Local} uses the words instead of the sentence as a query and selects the frames with the closest feature distance. We have shown that selecting frames based on sinkhorn leads to more variety of visual concepts but also captures frames with possible groundable objects. %, which improves overall performance.%, leading to better supervision.

% \noindent\textbf{Number of frames for training.} We tested different video lengths $T$ used for training. As shown in Table \ref{subtab:ablations2}, selecting less frames for training significantly causes the performance to drop. We hypothesize that not only does the model fail to capture the temporal dynamics with less frames, but loses some frames with groundable objects in the sentence while training. We also found that when the number of frames increases, more irrelevant frames might be selected during training, which decreases the performance.

\noindent\textbf{Global and local loss.} As mentioned in the spatio-temporal evaluation, both features contribute significantly to the final grounding result. We test the model by ablating out each loss. 
As shown in Table \ref{tab:train_ablations}, not only that each loss contributes to the task of spatio-temporal grounding on the GYT, but also the whole is more than the sum of its parts (losses) since this task requires both spatial and temporal detection. The reduced impact of the global loss in the case of YC-Inter is based on the fact that this is a pure spatial grounding dataset (no background frames) without temporal detection, and the local loss plays a more critical role. We observe the same patterns in the temporal grounding result for MYT, where spatial localization wasn't directly contributing to the final performance. %By comparing the results for spatio-temporal grounding in untrimmed videos (Table 1) vs. spatial grounding in trimmed videos (Table 3),  we can further see the impact of the proposed joint representation.

\noindent\textbf{Attention architecture.} We tested different architectures by stacking the self-attention or cross-attention block in the model for computing contextualized local representations as shown in Figure \ref{fig:pipeline}(d). As shown in Table \ref{tab:train_ablations}, we found the standard multimodal transformer architecture (self+cross) to have the worst performance. Using two cross-attention blocks was beneficial in incorporating more cross-modal interaction between local features. Finally, including a
self-attention layer slightly improves the final representations by encoding better single-modality representations.
% adding the global loss improves the ground performance. This results also shows that spatial grounding benefits from global representation learning. In the spatio-temporal setting, the performance without a global or local loss outperforms other baselines.

% \noindent\textbf{Dataset for training.} As mentioned in Section \ref{dataset}, we trained models with data with food categories. In Table \ref{subtab:ablations4}, we also tested our model trained with a larger set of food and entertaining called HowTo370K used in \cite{han2022temporal}. The full set of HowTo100M contains a total of 1M long videos, which is five times the size of our dataset. We found training with our 200K videos reaches similar performance with much less training hours.

% \noindent\textbf{Affect of audio in training and testing.} Unlike text which describes a discrete concept as a target to ground, audio serves as a continuous representation that is highly relevant to the temporal information. For example, we can determine an action started when we hear a ``cracking'' sound. In Table \ref{subtab:ablations5}, we tested our model using the additional audio modality by expanding our architecture and loss from VT to VAT. We found when training and testing with audio, the spatio-temporal result increases while the spatial-only result remains the same. This validates our assumption that audio contributes more to temporal understanding. When we trained on audio and tested without audio, the performance increases over the VT model, showing that the audio serves as useful supervision for better video/text representations. More details are presented in the supplement. 

\subsection{Qualitative results}
\vspace{-1mm}
We visualize our spatio-temporal result in Figure \ref{fig:visualization}. For the GLIP model, we output the bounding box with the highest confidence score and visualize its center point. We found GLIP model focuses on the salient object while our model focuses more on human-object interaction.
