
\section{Related Work}

\subsection{Supervised Spatio-temporal Grounding.}

%\bc{training with bounding box, localization is fixed vocab}
% Given a video and a textual description of an object, 
% This task aims at localizing a spatio-temporal tube (i.e., a sequence of bounding boxes) for the target object described by the input text. 
% VidSTG~\cite{chen-etal-2019-weakly} and HC-STVG\cite{chen-etal-2019-weakly} datasets are oriented on tube bounding box prediction of the object performing the action. %Closer to the aim of our work is the 
% Something-STAR dataset~\cite{xu2020spatio} contains untrimmed videos with tube annotation of hands and objects involved in an interaction, such as the hands opening a bottle, but not an entire person. %This dataset contains 47 classes, but was not released publicly.
% Spatio-temporal localization datasets like J-HMDB~\cite{jhuang2013towards}, UCF101-24~\cite{jiang2014thumos}, AVA~\cite{gu2018ava}, V-HICO\cite{li2021weakly}, DAILY~\cite{weinzaepfel2016human}, and DIVA~\cite{gleason2019proposal}.
Spatio-temporal Grounding refers to the problem of localizing a sequence of bounding boxes (a spatio-temporal tube) for a target object described by an input text. This problem has been addressed by various approaches TubeDETR \cite{yang2022tubedetr}, STCAT \cite{jin2022embracing}, STVGBert\cite{su2021stvgbert}, STVGBert\cite{su2021stvgbert}, STGVT \cite{tang2021human}, STGRN\cite{zhang2020does}. 
These methods rely on proposal networks such as Faster R-CNN\cite{Faster_rcnn} or MDETR \cite{kamath2021mdetr} to predict bounding box coordinates for learning text-to-region interaction.
%\hkc{Write a bit more here ... more apporaches, more details about how they work ...} 
All those approaches rely on supervised training with the human-annotated sentence and bounding box supervision, provided, e.g., by datasets such as VidSTG ~\cite{zhang2020does} and HC-STVG\cite{chen-etal-2019-weakly}. 
While those datasets provide a temporal aspect, temporal detection is usually limited to identifying the start and end frame of a single action in a video. Compared to that, an untrimmed setting usually comprises multiple actions in a longer video that can be separated by longer background sequences. This conceptually differs from previous works~\cite{chen-etal-2019-weakly} that typically use short videos of around 5-10 seconds. %\hkc{Please check!}
%Other datasets, such as Something-STAR~\cite{xu2020spatio}, contain tube annotations of hands and objects involved in interactions. 
% These datasets annotate only bounding boxes around a single human that performs an action, regardless of where exactly that action (such as movement or interaction) was performed. 
%
%Instead, our task and data annotation specifically focus on grounding actions, and we annotate entire untrimmed videos with 3-5 minutes, including the background. 


% Another line of work, spatio-temporal localization, is a task that aims to determine a spatio-temporal location in the video segment corresponding to a predefined class.
% In this context, action detection datasets, such as J-HMDB~\cite{jhuang2013towards}, UCF101-24~\cite{jiang2014thumos},  AVA~\cite{gu2018ava}, V-HICO\cite{li2021weakly}, DAILY~\cite{weinzaepfel2016human}, DIVA~\cite{gleason2019proposal}, focus on spatio-temporal localization of a fixed number of actions. 
% Moreover, these datasets annotate only bounding boxes around a single human that performs an action, regardless of where exactly that action (such as movement or interaction) was performed. 
%Therefore, existing methods either evaluate specific tasks such as target person detection~\cite{tang2021human,arnab2020uncertainty}, grounding an object by given text description~\cite{chen-etal-2019-weakly,zhang2020does} or perform in controlled settings such as grounding with a limited number of predefined classes~\cite{kalogeiton2017action,xu2020spatio}. 
% Compared to previous works, our task and data annotation focuses on grounding action specifically. In addition, we annotate the entire untrimmed videos with 3-5 minutes, including the background, as opposed to the untrimmed videos with around 5-10 seconds used in previous works ~\cite{chen-etal-2019-weakly,chen-etal-2019-weakly}. 

%Our work is the first to extend learning spatio-temporal grounding with self-supervision (with auto-generated text and detector free).


% % in temporally trimmed or untrimmed videos~\cite{xu2020spatio}.
% % or performing frame level detection which will be linked across time later~\cite{peng2016multi}.  
% % /asli- back to non-action staff=?/Besides of action detection,/asli/ existing methods either evaluated on specific tasks such as target person detection~\cite{tang2021human,arnab2020uncertainty}, grounding declarative/interrogative sentence depicting an object~\cite{zhang2020does} or perform in controlled settings such as grounding on curated datasets with temporally trimmed videos with limited number of classes are predefined~\cite{kalogeiton2017action,chen-etal-2019-weakly} or performing frame level detection which will be linked across time later~\cite{peng2016multi}. 
\subsection{Multimodal Self-supervised Learning.}
%Using a large amount of unlabeled data, self-supervised learning methods obtain an effective data representation by leveraging useful pretext tasks. Various pretext tasks have been extensively studied for different modalities: for language -- the next token prediction~\cite{radford2018improving}, masked word prediction and next sentence prediction~\cite{devlin2018bert}, text denoising~\cite{lewis2019bart}; for images -- semantic inpainting~\cite{pathak2016context}, colorization~\cite{zhang2016colorful}, rotation prediction~\cite{gidaris2018unsupervised}, contrastive learning with augmented views~\cite{chen2020simple,he2020momentum,grill2020bootstrap,tian2020contrastive}; for video -- future frame prediction~\cite{srivastava2015unsupervised}, pace prediction~\cite{wang2020self,benaim2020speednet}, or temporal ordering~\cite{lee2017unsupervised,fernando2017self,misra2016shuffle}.
% Multimodal Self-supervised methods aim at learning data representations by leveraging a large amount of unlabeled data with multiple co-occurrences of modalities. Early work~\cite{weston2011wsabie,frome2013devise} proposed to project images and text into a joint visual-language embedding space where image and text embeddings of semantically-similar pairs are close. Following this idea, CLIP~\cite{radford2021learning} learned representations leveraging 400 million image-text pairs publicly available on the internet. %Other 

The field of multimodal self-supervised learning aims to learn data representations by leveraging large amounts of unlabeled data with multiple modalities. Early works ~\cite{weston2011wsabie,frome2013devise} started by projecting images and text into a joint visual-language embedding space, where embeddings of semantically-similar pairs are close. Those ideas have now grown into systems such as CLIP~\cite{radford2021learning} based on representations from 400 million internet image-text pairs, or MIL-NCE~\cite{miech2020end} using the HowTo100M dataset~\cite{miech2019howto100m} to train a video-language embedding space from 1.2 million instructional videos paired with text descriptions from ASR. Follow-up works~\cite{alayrac2020self,akbari2021vatt,rouditchenko2020avlnet,chen2021multimodal, ShvetsovaCVPR22Everything} show that using videos without annotation enables an effective multimodal embedding space via contrastive learning.
%approaches~\cite{arandjelovic2017look,arandjelovic2018objects,korbar2018cooperative,morgado2021audio} exploit correspondence between the visual and the audio streams to learn representations from unlabeled videos. 

% Miech et al.~\cite{miech2019howto100m} trained an effective video-language embedding space by introducing the HowTo100M dataset with 1.2 million instructional videos collected from YouTube paired with text descriptions from ASR. Learning representations from text, visual, and audio modalities has also been studied~\cite{aytar2017see,ma2019unpaired}. In this context, \cite{alayrac2020self,akbari2021vatt,rouditchenko2020avlnet,chen2021multimodal} recently showed that using videos without annotation enables an effective multimodal embedding space via contrastive learning. %Our method stands as a first work that leverages self-supervised multimodal learning with text, video, and audio modalities for spatio-temporal video grounding. 

Based on those advantages, approaches started to address the problem of \noindent\textbf{Spatial Video Grounding} from multimodal self-supervised aiming to identify spatial locations in a \textit{trimmed} video based on text descriptions without the need for bounding box annotation during training. 
% This task is mostly studied in the context of %video object detection in supervised learning~\cite{li2017tracking,s2,yang2022tubedetr} or
% weakly supervised learning scenarios without temporal tracking detection capability~\cite{ZhLoCoBMVC18}. 
% Among object grounding benchmarks, the YouCook2-BoundingBox~\cite{ZhLoCoBMVC18} dataset provides bounding box annotations for the visible objects in the YouCook2~\cite{zhou2018towards} dataset of cooking videos. 
% Recent work proposed the YouCook-Interactions dataset~\cite{tan2021look} together with an approach CoMMA\cite{tan2021look} for the spatial grounding of objects and actions with multimodal self-supervision from HowTo100M videos. All of those works focus on spatial grounding only and assume that the video is temporally clipped with respect to the grounding phrase. Recently, works such as GLIP\cite{li2022grounded} and RegionCLIP\cite{zhong2022regionclip} combines the spirit of large-scale vision language training with bounding box finetuneing on object detection datasets \cite{gupta2019lvis,lin2014microsoft}
One of the early works studied this task in the context of weakly supervised learning scenarios where we learn grounding with human-annotated captions of the video~\cite{ZhLoCoBMVC18}. In this context, works \cite{tan2021look,shi2019not} %\hkc{cite some here that use YC2 Boxes} 
have focused on object grounding benchmarks such as YouCook2-BoundingBox~\cite{zhou2018towards}, which provides bounding box annotations for visible objects in cooking videos. Other works such as GLIP\cite{li2022grounded} and RegionCLIP\cite{zhong2022regionclip} combine the principles of large-scale vision language training with bounding box fine-tuning on object detection datasets\cite{gupta2019lvis,lin2014microsoft}. Recently, the YouCook-Interactions dataset~\cite{tan2021look} and CoMMA\cite{tan2021look} have been proposed for the spatial grounding of objects and actions with multimodal self-supervision from HowTo100M videos. All these works assume that the video is temporally clipped with respect to the grounding phrase. %Instead, we propose a frame selection strategy to adjust this spatial learning to the untrimmed video setting.

% Unlike mentioned phrase grounding recent work in~\cite{tan2021look} performs spatial localization of multiple objects and actions with self-supervision from HowTo100M instructional videos. 
% \asli {or BU here?}
% \bc{dataset, then methods}
% \bc{include CoMMA}
% \bc{GLIp and region CLIP is hybrid}

Compared to that, \noindent\textbf{Temporal Video Grounding}  aims to determine the set of consecutive frames corresponding to a text query in an \textit{untrimmed} video~\cite{regneri2013grounding,jiang2014thumos,heilbron2019large,soldan2021mad}, thus predicting temporal boundaries of action instances. 
Recent work such as MIL-NCE~\cite{miech2020end},  MCN~\cite{chen2021multimodal}, and VideoCLIP~\cite{videoclip} utilize large-scale pretraining for grounding actions temporally via text-to-frame similarity matching on video datasets such as MiningYouTube~\cite{kuehne2019mining} or CrossTask~\cite{Zhukov2019CrossTask} without proposals.
However, the majority of methods neglect the word-to-spatial relation and lack spatial localization ability~\cite{t11,t12}. To address this limitation, we propose a novel approach that utilizes large-scale pretraining for grounding actions by leveraging word-to-spatial relations and improving spatial localization ability.
% video CLIP, hero
% \bc{MIL-NCE, CLIP}
% \bc{add more references from Slack}
%Previous work can be categorized into proposal-based and proposal-free approaches. Proposal-based approaches employ a propose-and-rank pipeline framework to localize the temporal boundaries of the target segment~\cite{shou2016temporal,escorcia2016daps}. These methods are computationally expensive and rely on proposal quality. Among proposal-free methods, \cite{t7,t8} uses attention-based grounding and~\cite{t9,t10} proposed reinforcement learning for regressing the start and end times of target video segments. 