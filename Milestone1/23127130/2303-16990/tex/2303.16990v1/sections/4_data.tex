

\section{GroundingYoutube Benchmark}
\label{sec:dataset:annotation}
\vspace{-0.1cm}
Current downstream datasets either provide spatial \cite{tan2021look,shi2019not}, temporal annotation \cite{Zhukov2019CrossTask,kuehne2019mining,tang2019coin}, or short video clip with spatio-temporal annotation\cite{zhang2020does,chen-etal-2019-weakly}. These datasets do not provide the opportunity to evaluate both aspects, spatial and temporal grounding, in an untrimmed long video manner. We, therefore, extend one of the current benchmarks, MiningYouTube~\cite{kuehne2019mining}, which already provides dense temporal annotations, and we annotate video clips in the dataset with spatial information. 
%To extend the current benchmarks, we decided to go with the Mining YouTube dataset \cite{kuehne2019mining}. 


% \subsection{Dataset annotation}
% \label{sec:dataset:annotation}
% \vspace{-0.1cm}
Annotating the spatio-temporal extent of actions can be challenging as there is no clear visible outline as in object annotation, nor is there a unique signal to indicate the temporal begin and end points. 
Similarly, grounding systems do not usually produce pixel-exact bounding boxes but rather indicate regions of interest. Detector-free spatial grounding models \cite{arbelle2021detector} address this fuzziness by relying on pointing game accuracy, thus only using the center point of the heat map for evaluation. 
%
Lending on this idea, annotators were asked to mark the presumed center point of the action. 
Compared to bounding boxes, center point annotation can be advantageous because annotators are not visually distracted by object outlines, so it is more likely that the most important region will be selected. 
%Additionally, point annotations take less time, so it is more cost-effective compared to bounding boxes.
%Overall, we found that a point annotation took $5s$ in average per image, compared to $25s$ for bounding boxes a reported in \cite{su2012crowdsourcing}. 
We capture five annotations per frame, resulting in a density-based heat map. 


Starting from 5,091 clips showing one of the 512 action classes, we adopt the methodology used for temporal action localization developed in \cite{gu2018ava} and label one frame per second, resulting in $26,987$ frames. We annotated all frames with five repeats per image, resulting in $134,935$ point labels in total. Each video contains an average of 10 actions.
Following the previous evaluation setting using bounding boxes \cite{kalogeiton2017action}, we get the union of all annotated points in a single frame with an additional distance for constructing the bounding box. We provide more information on the annotation process is available in the supplementary.%\hkc{Refer to supplementary for further details ...?}

% \begin{figure}[t]
% \centering
% %\resizebox{.9\linewidth}{!}{
%     %%%%%%%%%%%%%%
%     \begin{subfigure}{.16\textwidth}
%     \centering
%     \subfloat[Number of labels generated][\scriptsize Wider shot]{
% \includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/answers_concentrated.png}
% \label{fig:answers_concentrated}}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
%     %%%%%%%%%%%%%%
%     \begin{subfigure}{.16\textwidth}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Flow action]{
% \includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/add_oil.png}
% \label{fig:answers_no_clear_action}}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
%     %%%%%%%%%%%%%%
%     \begin{subfigure}{.16\textwidth}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Split to two]{
% \includegraphics[width=0.9\textwidth]{figures/dataset_annotation/quality_control_classes/error_multiple_actions.png}
% \label{fig:answers_multiple_actions}}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
% \caption{Example of keypoint annotations under different conditions }
% \label{fig:quality_aspects}
% \vspace{-0.5cm}
% \end{figure}


%All labels were generated by $13$ professional annotators in total.  
%
% During the annotation, professional annotators were given a short instruction video at the beginning and then asked to click on the center of the given action without additional instructions. They further were free to choose ``can't answer" if they could not locate the action, e.g., at the beginning and end of the clip. Thus, the number of available keypoints per image differs and we choose majority voting to define if an action is present or not, resulting in new, refined temporal boundaries compared to the original annotation. 

% \begin{figure}
% %\vspace{-0.5cm}
%     \centering
%     \begin{subfigure}{.5\textwidth}
%     %%%%%%%%%%%%%
    
%     \centering
%     \subfloat[Can't solve][\scriptsize Can't solve]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/error_cant_solve.png}
%     \label{fig:answers_cant_solve}}
%     \qquad
%     \subfloat[Corrupt image][\scriptsize Single point]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/single_point.png}
%     \label{fig:single_point}}
%     \\
%     \subfloat[Corrupt image][\scriptsize Four points]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/four_points.png}
%     \label{fig:answers_corrupt}}
%     \qquad
%     \subfloat[Corrupt image][\scriptsize Five points]{
%     \includegraphics[width=0.25\textwidth]{figures/dataset_annotation/quality_control_classes/five_points.png}
%     \label{fig:answers_corrupt}}
%     %\caption{Illustration of an ``can't solve'' image \bc{4 examples: }}
%     %\label{fig:can_solve}
%     \end{subfigure}%
%     %%%%%%%%%%%%%%
%     %%%%%%%%%%%%%%
    
%     %%%%%%%%%%%%%%%%%%%
%     \caption{Sample annotations (left) and number of keypoints (right). The purple point represents the center point of the annotations in the frame. Approximately $48\%$ of the data has all 5 keypoints and $19\%$ has not a single annotation.
%     % It can be seen that the vast majority of answers were generated by five annotators. % \hkc{probably take out that one ... it's not favorable in times of crowd sourcing ... }-  
%       }
%     %\vspace{-0.4cm}
% \end{figure}
%
% \begin{figure}
%       \centering
%       \subfloat[Number of labels generated][\scriptsize Number of keypoints]{
%           \includegraphics[width=0.5\textwidth]{figures/dataset_annotation/distribution_number_of_keypoints.png}
%           \label{fig:answers_per_annotator}}
% \end{figure}%
%
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/dataset_annotation/distribution_number_of_keypoints.png}
%     \caption{Number of keypoints per image. It can be seen that $48\%$ of the data has all 5 keypoints and $19\%$ has not a single annotation}
%     \label{fig:answers_per_annotator}
% \end{figure}

% We found that the point-wise annotation resulted in roughly four distinct patterns, which depend on the captured scenario, as shown in Figure~\ref{fig:quality_aspects}. In the case of half portrait or even wider shots in Figure~\ref{fig:answers_concentrated}, annotations are highly locally centered, while frames with large object focus tend to have more widespread and less dense annotation in Figure~\ref{fig:answers_widespread}. We further found that in some cases, the point annotation can also result in representing the flow of the action, e.g., pouring oil in Figure~\ref{fig:answers_no_clear_action}, or even split in two separate clusters in Figure~\ref{fig:answers_multiple_actions}. 




% \subsection{Quality control}
% \label{sec:dataset:quality_control}
% Since the label quality of the datasets used is a critical factor in the performance of machine learning models, we verified the correctness of a subset of our images using an experienced annotation specialist for $1,026$ randomly selected frames. %\hkc{Sample size calculation in the rebuttal} % todo reference label quality effects outcome
% %
% To evaluate the quality of the data, we evaluate the agreement between the annotation specialist and the annotations provided by the annotators. To this end, we considered an annotation as false positive if three annotators or more have set a key point, although no action can be seen in the image and as false negative if three annotators or more have not set a key point, even though an action can be seen in the image. The entire sample was assessed using these criteria, with the specialist disagreeing with the annotators in only a total of $1.1\% \pm 3\%$ (FP: $0.7\% \pm 3\%$, FN: $0.4\% \pm 3\%$). We also found that annotations significantly diverted in terms of spread. Namely, for wider shots, they tend to be highly centered whereas zooming in together with the usage of larger objects such as a pan or a spatula results in more widespread key points. We therefore also analysed how often those cases occur and found that $14.0\%$ of the selected frames show a wide spread pattern. 


% \subsection{Dataset usage for evaluation}
% \label{sec:dataset:evaluation}
% %\hkc{we need to somehow describe how we turn point in bounding boxes ... } \\
% \noindent\textbf{Bounding box:} We get the union of all annotated points in a single frame with an additional distance for constructing the bounding box in evaluation. \\
% \noindent\textbf{Clip level evaluation construction: }
% Following the current spatio-temporal dataset \cite{kalogeiton2017action}, we construct clip level evaluation where the clip varies from 9 sec to 60 Section  Given an action step, we append the video segments before and after the steps with the same time length of the action step to form the final video clip. We result in 2895 clips for the spatio-temporal clip grounding evaluation.
%, which ends up with [previous segment, action step, later segment] as the final video clip.