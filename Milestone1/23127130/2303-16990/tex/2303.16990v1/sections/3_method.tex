\input{figures/pipeline}

\section{A Global-Local Framework for STG}



\subsection{General setup}
%\vspace{-10pt}
%
%\bc{This paragraph was mostly copied from MCN, the basic idea is the same and will revise soon}
% We have built up a strong baseline that incorporates global and local representations. Also, we learned the multimodal interaction in three modalities.
The goal of the proposed method is to temporally and spatially localize actions based on free-text queries in untrimmed videos. To this end, two representation spaces should be learned from unlabeled videos, a local and a global one. 
We start with narrated video clips, each associated with a corresponding visual representation and text narration.
% Given this input, the joint embedding space for both global and local representations are learned via contrastive loss by bringing the embeddings of both, local and global representation, with semantically similar visual and text content closer together.
Given this input, the text-video embedding spaces for both global and local representations are learned via contrastive loss by bringing the embeddings of semantically similar visual and text content closer together for both representations.
%\hkc{Remove this as we're pooling? : }Also, the local features, which were represented by spatio-temporal region in the video and words in text will be similar when they share the same concept.
Namely, for each clip $\mathcal{X} = \{\mathcal{V},\mathcal{S}\}$, let $\mathcal{V}$ stand for the video clip and $\mathcal{S}$ for the text narration sentence generated by the automatic speech recognition (ASR) system.
Each clip $\mathcal{V}$ consists of $T \times N$ spatio-temporal tokens $\{v_{t,n}\}$, where $t\in \{1,...,T\}$ represents the number of frames in the video and $n\in \{1,...,N\}$ represents the number of spatial grid region tokens or features in a frame. 
The text sentence $\mathcal{S}$ consists of $K$ words $\{s_1,...,s_K\}$. 
We represent localized features by the tokens from each modality, and the global features $\{{V},{S}\}$ are acquired either by mean-pooling over the local features or by using the classifier token from the transformer as in \cite{radford2021learning}.
We learn transformations $f:V\to\mathbb{R}^d$ to a $d$-dimensional representation $f(V)\in\mathbb{R}^d$ from the global representation $V$, and $g:S\to\mathbb{R}^d$, to produce similar $d$-dimensional text global embeddings: $g(S)\in\mathbb{R}^d$. Similar to $\{f, g\}$, we note $\{f^{'}, g^{'}\}$ to be the transform for localized features, where local features $\{v,s\}$ are also projected as d-dimensional representations.
%Let $f:\mathcal{V}\to\mathbb{R}^d$ denote a transformation that derives a video clip $v$ into a $d$-dimensional vector $f(v)\in\mathbb{R}^d$, $g:\mathcal{A}\to\mathbb{R}^d$ maps an audio sample $a$ into the same $d$-dimensional vector space, $g(a)=z\in\mathbb{R}^d$, and $h:\mathcal{T}\to\mathbb{R}^d$ maps the text $t$ into the $d$-dimensional vector space, $h(t)\in\mathbb{R}^d$.
%Given these low dimensional embeddings, we assume that we can estimate up to a constant factor the joint probability of a pair of embeddings from any two modalities $(v,t),(t,a),(a,v)$ by exponentiating the dot product of the two embeddings. For example with a pair of video and text samples,
%\begin{align}
%\label{eq:prob_model}
%p(v,t; f, h) \propto e^{f(v)^\top h(t)}
%\end{align}
In this work, $f$ takes as input S3D \cite{xie2018rethinking} or CLIP \cite{radford2021learning} features from a fixed-length clip, and the inputs for $g$ are from a sentence-based neural model that transforms a set of words into a single vector.
Global contrastive loss $\mathcal{L}_{Global}$ is used to ensure that the representations from each of the modalities at the global level are comparable. A second localized attention contrastive loss $\mathcal{L}_{Local}$ encourages representations from finer granularity, e.g., spatial regions and words, to be close in the embedding space.
% %\vspace{-3mm}
% \begin{align}
%     \label{eq:total}
%     \resizebox{0.4\linewidth}{!}{$
%     \mathcal{L} = \mathcal{L}_{Global}+\mathcal{L}_{Local}
%     $}
% \end{align}
%Pairwise contrastive loss, why do we need 3 pairs (AV, VT, AT)?
%\vspace{-5mm}


\subsection{Representations guided frame sampling} \label{temporal sampling} 
%\bc{Temporal sampling separates image grounding vs video grounding}
Learning representations from multimodal self-supervision is challenging since the narration is very likely to be noisy, thus containing more information than the actual task descriptions. The relevant task will often not be present in all frames due to poor temporal alignment or cut scenes~\cite{han2022temporal,miech2020end}, which is one of the key differences between weakly supervised vision-caption grounding and multimodal self-supervised grounding. %\hkc{the last half sentence makes only sense when we discuss weakly supervised vision-caption grounding somewhere} %\bc{added }
%In \cite{han2022temporal}, authors find that only 15\% of the videos align with the ASR script in HowTo100M dataset. 
%\hkc{This might go:  In our work, we aim to do spatio-temporal grounding, where we need to find the accurate start time and end time of instruction. In addition, we want to find the spatial location in the video where certain objects or actions exist. }
Motivated by this setup, this work pursues a frame selection strategy to improve object grounding and temporal alignment during training.
%Namely, we want to select $T$ frames in the video for training. 
We start from a longer sequence $U$, where $U>T$, which includes the video frames before and after the ASR boundaries that might contain actions or objects in the sentence. Our goal is to find $T$ frames out of the $U$ frames that are most relevant to the actions and objects in the sentence $\mathcal{S}$.
%To this end, we start by selecting $T$ words from the sentence $\mathcal{S}$ and utilize each word as the query to pick the $T$ relevant frames in the untrimmed video. Hence, the selected frames contain certain object/action concepts from each word. All words are ranked by the feature similarity between each word in the sentence and the global sentence-level feature, e.g. the [CLS] token in case of a transformer model, selecting the top $T$ words that best represent the sentence for our grounding target as shown in Figure \ref{fig:pipeline}(a). 
We formalize it as an optimal transport problem utilizing the Sinkhorn-Knopp algorithm \cite{cuturi2013sinkhorn}.

\noindent\textbf{Optimal transport for word-to-frame assignment.}
\label{sinkhorn}
%We denote by $\mathbf{C}$ the matrix whose columns are the $\mathbf{c}_1, \dots, \mathbf{c}_k$. 
To acquire the optimal assignment from word features to video frames, an assignment matrix $\mathbf{Q}$ is computed from each video and ASR pair as shown in Figure \ref{fig:pipeline}(a). This cross-model optimal transport mechanism is applied to assignment $\mathbf{Q}$ from the projected cross-model similarity $\mathbf{P}$ between word tokens and each video frame, where $\mathbf{P}=g(\mathcal{S}) \bigotimes f(\mathcal{V})^\top \in \mathbb{R}^{K\times U}$.  
To compute the assignment matrix, the text and video projection layers from the global representation in Figure \ref{fig:pipeline}(c) are used to project multimodal features into a common space for feature similarity calculation.
To ensure that the word-to-frame assignment contains more diversity instead of just saturated assignments to a single video frame, we add a constraint that requires label assignments to be equally distributed across various video frames representing diverse object/action concepts. This is achieved by restricting $\mathbf{Q_v}$ to a \textit{transportation polytope} $\mathcal{Q}_v$: %where $\mathbf{Q} \mathbbm{1} = \frac{1}{K} \mathbbm{1}$:
%work on minibatches by restricting the transportation polytope to the minibatch:
\vspace{-3mm}
\begin{equation}
  \resizebox{0.8\linewidth}{!}{$\mathcal{Q} = \left \{\mathbf{Q}\in\mathbb{R}_+^{U\times K} ~|~\mathbf{Q} \mathbf{1}_K = \frac{1}{U} \mathbf{1}_U, \mathbf{Q}^\top \mathbf{1}_U = \frac{1}{K} \mathbf{1}_K \right \}$},
  \label{eq:equal}
\end{equation}
which enforces the soft-assignment distribution $\mathbf{Q}$ to assign an equal marginal probability to each of the $U$ frames instead of converging to a single frame. The vector $\mathbf{1}_U$ represents one vector with dimension $U\times1$.

The next goal is to enforce this \textit{transportation polytope} $\mathcal{Q}$. %The projection $\mathbf{C}$ that is used to compute $\mathbf{P}$ is also shared across different batches and different modalities. This further allows the proposed multimodal clustering network to scale to large multimodal datasets and to perform clustering jointly across modalities. 
A solution for $\mathbf{Q}$ is now computed using the optimal transport Sinkhorn-Knopp algorithm~\cite{caron2020unsupervised,cuturi2013sinkhorn} as shown in Figure ~\ref{fig:pipeline} (b). The Sinkhorn-Knopp algorithm also normalizes the distribution of $\mathbf{P}$ as: 
\begin{equation}
\label{eq:qstar}
\resizebox{0.5\linewidth}{!}{$
   \mathbf{Q}= \text{Diag}(\mathbf{\alpha}) \exp\left(\frac{ \mathbf{P}}{\varepsilon} \right) \text{Diag}(\mathbf{\beta}),
   $}
\end{equation}
where $\mathbf{\alpha}$ and $\mathbf{\beta}$ are scaling vectors that restrict $\mathbf{Q}$ to have a uniform distribution across region assignment. $\varepsilon$ is a parameter that controls the smoothness of the mapping \cite{caron2020unsupervised}. %Pseudo-labels  $\mathbf{Q_a}$ and $\mathbf{Q_t}$ for the audio and text domains, are similarly computed using this procedure.

%Given that it is unlikely for semantic patterns to be always consistent across different modalities, we use soft labels to represent the probability distribution over labels instead of assigning various feature representations to a single label class, similar to \cite{caron2020unsupervised}. 
%We do this by preserving the soft pseudo-label $\mathbf{Q}^*$, which of computing a single hard label. 
%All the computations involved in this algorithm are matrix multiplications and hence can be efficiently implemented on GPUs. This additionally makes the technique scalable to large multimodal data compared to traditional k-means clustering \cite{arthur2006k}.
The $T$ frames are then selected by the corresponding assignment $\mathbf{Q}$ from the frames with top $T$  aggregated similarity sum over each word for further training. Note that the selection part $\mathbf{P}$ is from a trainable projection. While acquiring a better word-to-region projection during training, we hypothesize that the frame selection also benefits. The respective frame selection strategy is evaluated in Table~\ref{tab:train_ablations}. %\hkc{just ref the full table or section?}

% How to select top 8 text?
% %Motivate by the ASR miss align, cite temporal alignment. learn to select better frames for grounding. Find frames with relevant objects in the video
% use local to select frame for better supervision
% we propose 3 sampling strategt: global, local, sinkhorn. More comparison can be found in the ablation experiments Table \ref{subtab:sampling}.

\subsection{Local representations for spatial localization} \label{localization training}
To capture multimodal interaction with finer granularity, we apply the widely used attention mechanism to 
%learn the correlation of the features. Instead of manipulating features globally by pooling the features in a single vector, we 
learn the projection between tokenized features as shown in Figure~\ref{fig:pipeline}(d). We extract spatio-temporal region features $v_{tn}$ from the video. Also, we extract word features $s_k$ which represents the feature from word $k$. %\hkc{- Does this sentence make sense?} 
All tokenized features are projected through a linear layer.
To compute attention between the tokenized features, we stacked two cross-modal attention layers with a self-attention layer in the middle, as illustrated in Figure~\ref{fig:pipeline} (d).
Cross-modal attention is computed similar to the standard attention mechanism \cite{lee2018stacked}. %, except we compute the attention across different batches. 
Given a spatio-temporal token $v_{tn}$ from a video, we compute the attention score to all of the words $s_k$, where $k \in \{1,...,K\}$ in the ASR sentence $\mathcal{S}$ by $\alpha_{tnk} = \frac{\exp(e_{tnk})}{ \sum_{k=1}^{K}\exp(e_{tnk})}$ in the same video clip, where $e_{tnk} = cosine (v_{tn}, s_k)$. We then acquire a contextual video token feature $\Bar{v}_{tn} = \sum_{k=1}^K \alpha_{tnk} s_k$, which encoded text contextual information. %We compute the contextual video token ${v_{tn}}$ using the aggregated text token features $\{s_k, k=1,\ldots,K\}$ within the same video-sentence pair. 
% \begin{align}
% \vspace{-0.5cm}
%     e_{tnk} =& sim (v_{tn}, s_k) \\
%     \label{eq:alpha}
%     \alpha_{tnk} =& \frac{\exp(e_{tnk})}{ \sum_{k=1}^{K}\exp(e_{tnk})} \\
%     \Bar{v}_{tn} = &  \sum_{k=1}^K \alpha_{tnk} s_k
% \end{align}
Note that the contextual vector is represented by aggregating the representations from the other modality.
Follow the standard self-attention computation \cite{vaswani2017attention} $K$, $Q$, $V$ represent the features for the keys, queries, and values as:
\begin{equation} \label{attn_func}
\resizebox{0.6\linewidth}{!}{$
\vspace{-3cm}
    Attn(K, Q, V) =\operatorname{softmax}\left(\frac{\left(Q^\top K\right) }{\sqrt{d_k}}\right) V 
    $}
\end{equation} 
where $d_k$ is the dimension of the key.
In our case, we feed each contextual features $\{\bar{v}_{tn}$,$\bar{s}_{k}\}$ right after the first cross-attention layer to be the $K$, $Q$, $V$ to acquire its self-attended representation.
The localized attention model was trained using contrastive loss.% similar to the global contrastive loss.
To represent the video clip $\mathcal{V}$ and ASR sentence $\mathcal{S}$, we mean-pool over the spatio-temporal tokens in video {$\Bar{V} = \frac{1}{TN}\sum_{r=1}^{TN}  \bar{v}_r $}, and words {$\Bar{S} = \frac{1}{K}\sum_{k=1}^{K} \bar{s}_k $} respectively. Let $\left(\Bar{V}^{(l)},\Bar{S}^{(l)}\right)$ be the $l$-th training example pair. We adopt the Noise Contrastive Estimation (NCE)  loss \cite{gutmann2010nce} and the localized attention losses  $\mathcal{L}_{Local}$ :
%\resizebox{1\linewidth}{!}{
% \begin{equation}
% \label{eq:video_objective_function_local}
% %\footnotesize
% \scriptsize
% {L}_{va} =
% -\sum_{i=1}^{B} \log \left(\frac{\displaystyle e^{\Bar{V}_i \cdot \Bar{A}_i - \delta}}{\displaystyle e^{\Bar{V}_i \cdot \Bar{A}_i - \delta} + \sum\limits_{m\sim \mathcal{N}_{i,A}} \displaystyle e^{\Bar{V}_i \cdot \Bar{A}_m} + \sum\limits_{m\sim \mathcal{N}_{i,V}} \displaystyle e^{\Bar{V}_m \cdot \Bar{A}_i}} \right)
% \end{equation}
%\vspace{-0.1cm}
\begin{align}
     \label{local_loss}
    \resizebox{0.5\linewidth}{!}{$
    -\frac{1}{B} \sum\limits_{l=1}^{B} \Bigg[ \Bigg( \log \frac{\displaystyle e^{\Bar{V}_l \cdot \Bar{S}_l - \delta}}{\displaystyle e^{\Bar{V}_l \cdot \Bar{S}_l - \delta} + \textstyle \sum\limits_{\substack{k=1 \\ k\neq l}}^{B}e^{\Bar{V}_k^{imp} \cdot \Bar{S}_l}} \Bigg)
    $} \resizebox{0.5\linewidth}{!}{$
    + \Bigg( \log \frac{\displaystyle e^{\Bar{V}_l \cdot \Bar{S}_l - \delta}}{\displaystyle e^{\Bar{V}_l \cdot \Bar{S}_l - \delta} + \textstyle \sum\limits_{\substack{k=1 \\ k\neq l}}^{B}e^{\Bar{V}_l \cdot {\Bar{S}_k}^{imp}})} \Bigg) \Bigg]
    \vspace{-0.1cm}
    $} 
\end{align}
% Similar to Equation \ref{eq:video_objective_function} except we feed the mean local contextual feature instead of the global feature.
where $B$ stands for the batch. $\Bar{V}_k^{imp}$ and ${\Bar{S}_k}^{imp}$ represent imposter samples, and $\delta$ is a margin hyperparameter.
%As shown in the figure, the learned localized feature was used to guide the frame sampling strategy. While training, our frame sampling strategy will become more accurate due to the better common-space features.
%we utilize audio as an additional bridge for video and text to learn localized features. Our hypothesis is that maintaining this consistency across modalities may help learn better-localized features. We will justify this design in the ablation study.
% The total localized attention loss $L_{Local}$ is the sum of pairwise losses using each of the three modalities:
% %\vspace{-1mm}
% \begin{equation}
%     \label{eq:mms}
%     %\vspace{-2mm}
%     \resizebox{0.5\linewidth}{!}{$
%     L_{Local} = L_{ta}  + L_{vt} + L_{va} 
%     $}
%     %\vspace{-2mm}
% \end{equation}


\subsection{Learning multimodal global representations}
\label{sec:contrastive}
We learn to project the global representation of a video clip and a sentence by contrastive loss, as shown in Figure~\ref{fig:pipeline}(c). %We use the video-to-text pair for the following example. 
This loss pulls the representations of the two modalities from the same instance closer while pushing the imposter modality pairs sampled from different videos further away. 
We use the NCE loss function~\cite{gutmann2010nce}.
The global contrastive loss $\mathcal{L}_{Global}$ follows the formulation as Equation \ref{local_loss} while using the global representations $V$ and $S$, which is the [CLS] tokens from both modalities, instead of the local representations. %The learned global feature is then used for further temporal alginment since it captures the entire content of a video or a sentence.
% %\vspace{-1mm}
% \begin{equation}
%     \label{eq:mms}
%     %\vspace{-2mm}
%     \resizebox{.5\linewidth}{!}{$
%     L_{Global} = L_{VT}  + L_{AT} + L_{VA} 
%     $}
%     %\vspace{-2mm}
% \end{equation}
% where $L_{VT}$, $L_{AT}$, $L_{VA}$ stands for the loss respective to the pairwise modalities.
% % where $L_{VT}$, $L_{AT}$, $L_{VA}$ stands for the loss respective to the pairwise modalities $(V,T),(A,T),(V,A)$. 
% Given the video and audio pair as example, the  loss $L_{VA}$ is as:
% % \begin{align}
% %     \resizebox{0.7\linewidth}{!}{$
% %     L_{VA} = -\frac{1}{B} \sum\limits_{i=1}^{B} \Bigg[ \Bigg( \log \frac{\displaystyle e^{f(\textbf{V}_i) \cdot g(\textbf{A}_i) - \delta}}{\displaystyle e^{f(\textbf{V}_i) \cdot g(\textbf{A}_i)  - \delta} + \textstyle \sum\limits_{\substack{k=1 \\ k\neq i}}^{B}e^{h(\textbf{V}_k^{imp}) \cdot g(\textbf{A}_i)}} \Bigg)
% %     $} 
% % \end{align}
% \begin{equation}
% \label{eq:video_objective_function}
% \resizebox{1\linewidth}{!}{$
% %\scriptsize
% \tiny
% {L}_{VA} =
% -\sum_{i=1}^{B} \log \left(\frac{\displaystyle e^{f(\textbf{V}_i) \cdot g(\textbf{A}_i) - \delta}}{\displaystyle e^{f(\textbf{V}_i) \cdot g(\textbf{A}_i) - \delta} + \sum\limits_{m\sim \mathcal{N}_{i,A}} \displaystyle e^{f(\textbf{V}_i) \cdot g(\textbf{A}_m)} + \sum\limits_{m\sim \mathcal{N}_{i,V}} \displaystyle e^{f(\textbf{V}_m) \cdot g(\textbf{A}_i)}} \right)  
% $}
% \end{equation}
% where $B$ stands for the batch and the negative sets $\mathcal{N}_{i, V}$ and $\mathcal{N}_{i,A}$ represents imposter samples, and $\delta$ is a margin hyperparameter.
%comprise indices for non-corresponding video clip and narration pairs for the $i$-th training sample, and $n$ denotes the total number of training samples.
% where  and $V_j^{imp}$ represents imposter pairs  sampled from a batch but not from the same video clip. 
Projecting the global features to the same space ensures that the features across different modalities are comparable. Since global representations encode information from the entire video, it is essential in encoding temporal information for the later downstream tasks. The final model is optimized by the sum of both losses.

\input{figures/inference.tex}
%\subsection{Inferencing for spatio-temporal grounding}
\subsection{Inference for spatio-temporal grounding.} \label{inference_section}
To perform spatio-temporal grounding on untrimmed videos, we start from temporal action detection as shown in Figure \ref{fig:inference}. Given a pool of possible action descriptions on the left and an untrimmed video, we perform feature similarity matching using the global representation ([CLS] token) per frame with a threshold $\tau$ to filter backgrounds. We pick the action class with the largest similarity score per frame. Later, we use the predicted action class and feed it into the local representation branch to compute spatial grounding. We follow attention rollout~\cite{abnar2020quantifying} to compute feature similarity between visual tokens and text tokens through the cross-attention and self-attention. In the end, we acquire an attention heatmap for later downstream evaluation.
% During inference, given a natural language sentence and a video clip, we aim to localize
% the salient spatial regions across the frames in the clip. Beginning from the attention weights of the
% last cross-modal attention layer, we apply attention rollout [1] to obtain the final attention heatmap
% over all spatiotemporal regions. In attention rollout, the attention weight matrices from all crossattention
% and self-attention layers are multiplied recursively to yield the output localization scores
% A for each spatiotemporal region, where A =
% QL
% l=0Wl for attention weights Wl from the l-th
% layer. The resulting localization scores aggregate the total amount of attention by the entire set of
% spatiotemporal and word features assigned to a query feature
%\noindent\textbf{Spatial localization}

