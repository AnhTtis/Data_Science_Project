\section{Experimental setup}
%\subsection{Implementation details}

\subsection{Backbones and Training}
We evaluate the proposed method on backbones, CLIP\cite{radford2021learning} and S3D-word2vec\cite{miech2020end}.
We described the detailed setup as well as the training in the following.

\noindent \textbf{CLIP models.} For both the visual and text backbone, we use the pretrained weights from CLIP \cite{radford2021learning} with transformer ViT-B/32 and fix the encoder. Both the visual and text encoder has a final embedding size of 512. We apply them to video segments with 12-28 seconds, processing 1 frame per second. An evaluation of how many frames to process (identical to the number of seconds) is shown in Table \ref{tab:frames}. It shows the best results when we start with 16 possible frames $U$ (as described in Section \red{3.1}), from which the $T$ = 8 frames are selected for training. Ablation on the numbers of frames $T$ used for training is shown in Table \ref{subtab:ablations2}.
We use a batch size of $B$ = 64 video clips. 

\noindent \textbf{S3D-word2vec models.}
For the video backbone, we follow~\cite{tan2021look} and use S3D initialized by MIL-NCE on HowTo100M~\cite{miech2020end} at the rate of 5 frames per second and fix the video encoder. 
The global video clip features were max-pooled over time and projected into embeddings of dimension 512.
For the text feature, we follow ~\cite{miech2019howto100m} using a GoogleNews pre-trained word2vec model~\cite{mikolov2013efficient} and max-pooling over words in a given sentence to acquire the text global feature.  
We use a batch size of $B$ = 96 video clips.


\noindent \textbf{Training.} For the training of both backbone settings, we use an Adam optimizer~\cite{kingma2015adam} with a learning rate of $1\mathrm{e}{-4}$. 
The model is trained for 10 epochs on 4 V100 GPUs, which takes about two days. 


\subsection{Inference}

\noindent \textbf{Inference for the proposed model and CoMMA.} For inference in the case of temporal grounding, as shown in Figure \red{3a}, we first normalize the global feature for video and text. We used a (temporal) threshold $\theta$ = 0.5 to separate detections from the background. In spatial grounding, we acquire an attention heatmap using the attention rollout \cite{abnar2020quantifying} described in Section \red{3.5}. We set a spatial threshold $\tau$ = 0.01 to create the mask, as shown in Figure \red{3b}. The choice of this spatial threshold is evaluated in Table \ref{tab:thre}. 



\noindent \textbf{GLIP, RegionCLIP baseline inference.} In spatial grounding, we are given a text query and need to localize it in the frame. GLIP and RegionCLIP predict multiple bounding boxes corresponding to the text query. We select the predicted bounding box with the highest confidence score as the prediction result. We use the center point of the predicted bounding box for the pointing game evaluation as the model prediction. For \textit{mAP} evaluation, we use the predicted bounding box to compute IoU with the ground truth bounding box. In spatio-temporal grounding, we input all possible action description labels as candidates similar to Figure \red{3a}. We pick the class with the highest confidence score as the predicted label. If the model made no prediction, we would predict it as ``background''. The spatial inference is the same as the spatial grounding setting.

\noindent \textbf{TubeDETR, STCAT baseline inference.} TubeDETR and STCAT are spatio-temporal grounding models trained to predict a single spatio-temporal tube per video. 
In both cases, TubeDETR and STCAT, we use models trained on the Vid-STG dataset with 448x448 resolution and evaluate them for the task of spatial grounding. 
Since this dataset contains mostly short videos ($<$30sec), we observed that both methods will also only predict a trajectory tube in this temporal range ($<$30sec), no matter how long the input video is. To allow us to apply them to longer videos ($>$30sec), we split the longer videos based on sliding windows of 5-sec for better performance.

\input{tables/spatial_clip}
\input{supplement/tables/frames.tex}
\input{supplement/tables/frames_for_train}
\input{supplement/tables/audio}
\input{supplement/tables/threshold.tex}


\noindent \textbf{MIL-NCE, CLIP baseline inference.} Both models are trained based on global representations for both input modalities, videos/images and text. We can, therefore, directly compute a sentence-to-video-frame similarity to perform the temporal grounding for Figure \red{3a}, following the same process as the proposed method for temporal grounding. For spatial grounding, we compute sentence-to-region feature similarity. Both visual backbones produce a 7x7 grid feature. We normalize the sentence and region features, then select a spatial threshold $\tau$ = 0.5 to create the mask for the \textit{mAP} evaluation.

