
\section{Additional Experiments}

\noindent \textbf{Single-action spatio-temporal grounding.}
Current spatio-temporal detection and grounding datasets \cite{jiang2014thumos,gu2018ava} usually aim to discriminate a single given action class from the background class in a short clip. This differs from our setup of spatio-temporal grounding in untrimmed videos, which usually comprises a set of phrases that need to be detected in a 3-5 min long video. To allow an evaluation of spatio-temporal grounding approaches based on single phrase grounding, we construct a clip-level evaluation where the clip varies from 9 sec to 60 sec. Given an action step, we append the video segments before and after the steps with the same time length of the action step to form the final video clip. This results in 2,895 clips for the spatio-temporal clip grounding evaluation.
For each clip, the temporal action intervals occupy 33\% of corresponding videos, which demonstrates the difficulty of the setting. 
%\bc{talk about GLIP performance increase, how its temporal prediction}
In this setting, instead of selecting the possible action step from a pool, the ground truth action step was given as the text query for spatio-temporal grounding. This allows us to directly compare with supervised spatio-temporal grounding methods \cite{yang2022tubedetr,jin2022embracing} as described in Section \red{5.4}.
As shown in Table \ref{tab:st_clip}, we observe that the baseline GLIP models achieve a much better performance compared to Table \red{1}. This is due to the fact that this setting does not require the model to select the text query from the pool, which the GLIP model was not trained to do. Moreover, we find that weakly supervised methods, GLIP and RegionCLIP, show only limited ability to differentiate the queried action from the background, which leads the model to ground the text query in most of the frames. However, both demonstrate powerful localization ability in foreground action segments, which results in a decent performance. The fully-supervised trained models (TubeDETR, STCAT) achieved a balance in localizing temporally and spatially, resulting in the best performance on this task.





\noindent \textbf{Frames used for selection.}
As shown in Table \ref{tab:frames}, we perform an ablation study on the number of candidates frames $U$ used for training. We found that selecting 16 frames achieves the best performance, comprising the useful video information in training while not including too many irrelevant concepts that diverge from the action/object in the ASR sentence.



\noindent\textbf{Number of frames for training.} We further evaluated the impact of different numbers of frames $T$ used for training. As shown in Table \ref{subtab:ablations2}, selecting fewer frames for training significantly causes the performance to drop. We hypothesize that the model not only fail to capture the temporal dynamics with fewer frames but also loses some frames with groundable objects in the sentence while training. We also hypothesize that with a too large number of frames, more irrelevant frames might be selected during training, which decreases the performance.


\noindent\textbf{Effect of audio in training and testing.} Unlike text which describes a discrete concept as a target to ground, audio serves as a continuous representation that is highly relevant to the temporal information. For example, we can determine an action started when we hear a ``cracking'' sound. In Table \ref{subtab:ablations5}, we tested our model using the additional audio modality. For the audio branch, we compute log-mel spectrograms and use a DAVEnet model~\cite{harwath2018jointly} initialized by MCN on HowTo100M~\cite{chen2021multimodal} to extract audio features. We extend the global and local loss pairs from VT to VT, VA, and AT following \cite{ShvetsovaCVPR22Everything}.
We found when training and testing with audio, the spatio-temporal result increases the temporal performance while the spatial-only result remains the same. This validates our assumption that audio contributes more to temporal understanding. When we trained on audio and tested without audio, the performance increased over the VT model, showing that the audio serves as useful supervision for better video/text representations. 



\noindent \textbf{Threshold for attention mask.}
As shown in Figure \red{3b}, we apply a threshold to create a mask from the result of attention rollout. Note that this threshold $\tau$ is not a hyperparameter that affects the training or the model but simply serves as a means to an end to compute the \textit{mAP} scores. We did not systematically optimize this threshold, but instead, %chose it as giving the most plausible qualitative results. 
Test different thresholds for attention scores for all relevant models (COMMA, ours) using the spatio-temporal grounding \textit{mAP} IoU@0.4 on our GroundingYoutube dataset as shown in Table \ref{tab:thre}. We find 0.01 to be a reasonable threshold among all models, performing best on COMMA and giving at least the second best results for the proposed model. 

%Spatio-temporal grounding performance in different settings
%Saturated v.s. widespread
