\begin{table*}[h]
    \tablestyle{7pt}{1.05}
    
    \centering
    %\resizebox{2\columnwidth}{!}{
    \begin{tabular}{@{}l| cccc | c |c c| c c | c c }
    	\toprule
    	\multicolumn{4}{c}{} & \multicolumn{1}{c}{ } & \multicolumn{1}{c}{YouCook-Inter} & \multicolumn{2}{c}{GroundingYoutube}  & \multicolumn{2}{c}{V-HICO}   & \multicolumn{2}{c}{Daly}\\ 
    	\cmidrule(lr){6-6} \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-12}  
    	Method  & Backbone &Data&Super.&Mod.& Acc &  Acc & mAP &  Acc & mAP  &  Acc & mAP \\ 
    	\midrule
        MIL-NCE \cite{miech2020end} & S3D &HT100M & Self &VT& 23.67  & 27.45  & 8.21 & 12.65 & 11.23 & 13.84 & 24.23 \\
    	CoMMA* \cite{tan2021look} & S3D &HT250K & Self &VT& 48.63   & 47.68 & 23.38 & 40.97 & 21.45 & 54.48 & 33.39 \\
        %\midrule
        Ours                       & S3D &HT200K & Self &VT & \textbf{53.98}   & \textbf{60.62} & \textbf{44.93} & \textbf{44.32} & \textbf{24.31} & \textbf{66.35} & \textbf{45.93} \\
         \midrule
         CLIP   \cite{radford2021learning}            & CLIP&HT200K & Self &IT &    14.10    & 12.50  & 3.49 &  29.23 & 12.51  & 18.02 & 27.28  \\
         CoMMA$\dagger$  \cite{tan2021look}            & CLIP  &HT200K & Self &VT&   52.65     & 47.56 & 36.42 & 55.20 &  34.54& 61.06 & 44.37  \\
             RegionCLIP   \cite{zhong2022regionclip}            & RN50x4 & CC3M & Weak &IT &   51.56     &   52.84 &  23.42 & 57.92 & 37.82 & 67.12 & 48.62 \\
            GLIP   \cite{li2022grounded}            & Swin-L&Cap24M & Weak &IT &   52.84      &   53.62 & 24.73 & \textbf{66.05} & \textbf{41.17} & - & - \\
            %\midrule
            Ours                       & CLIP &HT200K & Self &VT& \textbf{57.10}    &   \textbf{55.49} & \textbf{43.12} & 60.71& 39.28 & \textbf{70.08} & \textbf{50.56} \\
            %V-HICO   \cite{}            &  Faster R-CNN &  -      &  - & - & & 67.21 & - & - \\
            \midrule
            {\color{gray}TubeDETR \cite{yang2022tubedetr}}    &  {\color{gray}MDETR} & {\color{gray}Vid-STG} & {\color{gray} Full} & {\color{gray}VT} & {\color{gray}51.63}    &   {\color{gray}53.24} & {\color{gray} 41.76} & {\color{gray}63.23} & {\color{gray}40.87 } & {\color{gray}84.21} & {\color{gray} 62.98} \\
            {\color{gray}STCAT \cite{jin2022embracing}}    &  {\color{gray}ResNet-101} & {\color{gray}Vid-STG} & {\color{gray} Full} & {\color{gray}VT} & {\color{gray}54.47}    &   {\color{gray} 55.90} & {\color{gray}44.21 } & {\color{gray}65.34} & {\color{gray} 41.10 } & {\color{gray}85.42} & {\color{gray} 63.94} \\
    	\bottomrule
    \end{tabular}
    %}
    \vspace{-0.2cm}
    \caption{\textbf{Video spatial grounding}. We evaluate using pointing game accuracy and mean average precision. %Models learning global representations (MIL-NCE, CLIP) don't perform well on localization tasks, while our model outperforms other grounding methods. %We listed CNN-based methods on top and transfomer-based methods at the bottom. 
    %(Mod. indicates the modality used, where V: video, I: image, T: text. Super. indicates supervision.)
    %Our method generalized well on both video and image architectures. 
    % Daly GLIP is not workable since every class is action. OOV. V-HICO dataset the CLIP  generalized better to OOV, while word2vec getting worse performance. \bc{maybe we can add supervision: weakly, SSL} \bc{add pretraining data}
    \label{tab:spatial}
    \vspace{-0.3cm}
    }
   
    
\end{table*}

% \begin{table}[t]
%     % \tablestyle{2pt}{1.05}
    
%     \centering
%     %\resizebox{1\columnwidth}{!}{
%     \begin{tabular}{@{}l|cc|cc}
%     	\toprule
%     	\multicolumn{1}{c}{} & \multicolumn{2}{c}{YouCook-Interaction} & \multicolumn{2}{c}{MiningYoutube Grounding}  \\ 
%     	\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
%     	Method  & Acc & IoU   & Acc & IoU \\ 
%     	\midrule
%     	CoMMA* \cite{tan2021look}   & 48.63 & -  & 47.68 & -  \\
%     	MIL-NCE \cite{miech2020end} & 23.67 & -  & 27.45 & -  \\
%     	Ours                        & 48.03 & -  & 47.35 & -  \\
%     	\bottomrule
%     \end{tabular}
%     \vspace{+0.3cm}
%     \caption{Evaluation on spatial-only evaluation using pointing game accuracy and attention heatmap IoU with GT bounding box. Models learning global representation doesn't perform well on localization tasks, while our model maintain comparable performance.
%     \label{tab:spatial}
%     %\vspace{-0.2cm}
%     }
%     %}
    
% \end{table}