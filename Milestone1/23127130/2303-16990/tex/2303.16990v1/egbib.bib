@misc{Authors14,
	title        = {The frobnicatable foo filter},
	author       = {Authors},
	year         = 2014,
	note         = {{BMVC14} submission ID 324. Supplied as additional material {\tt bmvc14.pdf}}
}
@misc{Authors14b,
	title        = {Frobnication tutorial},
	author       = {Authors},
	year         = 2014,
	note         = {Supplied as additional material {\tt tr.pdf}}
}
@inproceedings{li2021weakly,
	title        = {Weakly Supervised Human-Object Interaction Detection in Video via Contrastive Spatiotemporal Regions},
	author       = {Li, Shuang and Du, Yilun and Torralba, Antonio and Sivic, Josef and Russell, Bryan},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {1845--1855}
}
@inproceedings{
jin2022embracing,
title={Embracing Consistency: A One-Stage Approach for Spatio-Temporal Video Grounding},
author={Yang Jin and yongzhi li and Zehuan Yuan and Yadong MU},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=NzFtM5Pzvm}
}
@inproceedings{kamath2021mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1780--1790},
  year={2021}
}
@article{Faster_rcnn,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{zhang2020does,
  title={Where does it exist: Spatio-temporal video grounding for multi-form sentences},
  author={Zhang, Zhu and Zhao, Zhou and Zhao, Yang and Wang, Qi and Liu, Huasheng and Gao, Lianli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10668--10677},
  year={2020}
}


@inproceedings{su2021stvgbert,
  title={Stvgbert: A visual-linguistic transformer based framework for spatio-temporal video grounding},
  author={Su, Rui and Yu, Qian and Xu, Dong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1533--1542},
  year={2021}
}
@inproceedings{arbelle2021detector,
  title={Detector-free weakly supervised grounding by separation},
  author={Arbelle, Assaf and Doveh, Sivan and Alfassy, Amit and Shtok, Joseph and Lev, Guy and Schwartz, Eli and Kuehne, Hilde and Levi, Hila Barak and Sattigeri, Prasanna and Panda, Rameswar and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1801--1812},
  year={2021}
}
@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={69--85},
  year={2016},
  organization={Springer}
}

@article{vaswani2017attention,
	title        = {Attention is all you need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@inproceedings{gutmann2010nce,
	title        = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
	author       = {Gutmann, Michael and Hyv{\"a}rinen, Aapo},
	year         = 2010,
	booktitle    = {AISTATS}
}
@inproceedings{shi2019not,
	title        = {Not all frames are equal: Weakly-supervised video grounding with contextual similarity and visual clustering losses},
	author       = {Shi, Jing and Xu, Jia and Gong, Boqing and Xu, Chenliang},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {10444--10452}
}
@inproceedings{tang2019coin,
	title        = {Coin: A large-scale dataset for comprehensive instructional video analysis},
	author       = {Tang, Yansong and Ding, Dajun and Rao, Yongming and Zheng, Yu and Zhang, Danyang and Zhao, Lili and Lu, Jiwen and Zhou, Jie},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {1207--1216}
}
@article{zhang2018top,
	title        = {Top-down neural attention by excitation backprop},
	author       = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
	year         = 2018,
	journal      = {International Journal of Computer Vision},
	publisher    = {Springer},
	volume       = 126,
	number       = 10,
	pages        = {1084--1102}
}
@inproceedings{xiao2017weakly,
	title        = {Weakly-supervised visual grounding of phrases with linguistic structures},
	author       = {Xiao, Fanyi and Sigal, Leonid and Jae Lee, Yong},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {5945--5954}
}
@article{Alpher02,
	title        = {Frobnication},
	author       = {A. Alpher},
	year         = 2002,
	journal      = {Journal of Foo},
	volume       = 12,
	number       = 1,
	pages        = {234--778}
}
@inproceedings{akbari2019multi,
	title        = {Multi-level multimodal common semantic space for image-phrase grounding},
	author       = {Akbari, Hassan and Karaman, Svebor and Bhargava, Surabhi and Chen, Brian and Vondrick, Carl and Chang, Shih-Fu},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {12476--12486}
}
@inproceedings{yang2022tubedetr,
  title={TubeDETR: Spatio-Temporal Video Grounding with Transformers},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16442--16453},
  year={2022}
}
@inproceedings{kuehne2019mining,
	title        = {Mining YouTube-A dataset for learning fine-grained action concepts from webly supervised video data},
	author       = {Kuehne, Hilde and Iqbal, Ahsan and Richard, Alexander and Gall, Juergen},
	year         = 2019,
	booktitle    = CVPR
}
@inproceedings{miech2020end,
	title        = {End-to-end learning of visual representations from uncurated instructional videos},
	author       = {Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {9879--9889}
}
@article{tan2021look,
	title        = {Look at what iâ€™m doing: Self-supervised spatial grounding of narrations in instructional videos},
	author       = {Tan, Reuben and Plummer, Bryan and Saenko, Kate and Jin, Hailin and Russell, Bryan},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34
}
@inproceedings{wang2022cris,
  title={Cris: Clip-driven referring image segmentation},
  author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11686--11695},
  year={2022}
}
@article{Alpher03,
	title        = {Frobnication revisited},
	author       = {A. Alpher and J.~P.~N. Fotheringham-Smythe},
	year         = 2003,
	journal      = {Journal of Foo},
	volume       = 13,
	number       = 1,
	pages        = {234--778}
}
@article{Alpher04,
	title        = {Can a machine frobnicate?},
	author       = {A. Alpher and J.~P.~N. Fotheringham-Smythe and G. Gamow},
	year         = 2004,
	journal      = {Journal of Foo},
	volume       = 14,
	number       = 1,
	pages        = {234--778}
}
@inproceedings{pathak2016context,
	title        = {Context encoders: Feature learning by inpainting},
	author       = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {2536--2544}
}
@inproceedings{zhang2016colorful,
	title        = {Colorful image colorization},
	author       = {Zhang, Richard and Isola, Phillip and Efros, Alexei A},
	year         = 2016,
	booktitle    = {European conference on computer vision},
	pages        = {649--666},
	organization = {Springer}
}
@article{gidaris2018unsupervised,
	title        = {Unsupervised representation learning by predicting image rotations},
	author       = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1803.07728}
}
@inproceedings{he2020momentum,
	title        = {Momentum contrast for unsupervised visual representation learning},
	author       = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {9729--9738}
}
@inproceedings{chen2020simple,
	title        = {A simple framework for contrastive learning of visual representations},
	author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year         = 2020,
	booktitle    = {International conference on machine learning},
	pages        = {1597--1607},
	organization = {PMLR}
}
@article{grill2020bootstrap,
	title        = {Bootstrap your own latent-a new approach to self-supervised learning},
	author       = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {21271--21284}
}
@inproceedings{tian2020contrastive,
	title        = {Contrastive multiview coding},
	author       = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	year         = 2020,
	booktitle    = {European conference on computer vision},
	pages        = {776--794},
	organization = {Springer}
}
@inproceedings{srivastava2015unsupervised,
	title        = {Unsupervised learning of video representations using lstms},
	author       = {Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
	year         = 2015,
	booktitle    = {International conference on machine learning},
	pages        = {843--852},
	organization = {PMLR}
}
@inproceedings{wang2020self,
	title        = {Self-supervised video representation learning by pace prediction},
	author       = {Wang, Jiangliu and Jiao, Jianbo and Liu, Yun-Hui},
	year         = 2020,
	booktitle    = {European conference on computer vision},
	pages        = {504--521},
	organization = {Springer}
}
@inproceedings{benaim2020speednet,
	title        = {Speednet: Learning the speediness in videos},
	author       = {Benaim, Sagie and Ephrat, Ariel and Lang, Oran and Mosseri, Inbar and Freeman, William T and Rubinstein, Michael and Irani, Michal and Dekel, Tali},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {9922--9931}
}
@inproceedings{lee2017unsupervised,
	title        = {Unsupervised representation learning by sorting sequences},
	author       = {Lee, Hsin-Ying and Huang, Jia-Bin and Singh, Maneesh and Yang, Ming-Hsuan},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {667--676}
}
@inproceedings{fernando2017self,
	title        = {Self-supervised video representation learning with odd-one-out networks},
	author       = {Fernando, Basura and Bilen, Hakan and Gavves, Efstratios and Gould, Stephen},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {3636--3645}
}
@inproceedings{misra2016shuffle,
	title        = {Shuffle and learn: unsupervised learning using temporal order verification},
	author       = {Misra, Ishan and Zitnick, C Lawrence and Hebert, Martial},
	year         = 2016,
	booktitle    = {European Conference on Computer Vision},
	pages        = {527--544},
	organization = {Springer}
}
@article{devlin2018bert,
	title        = {Bert: Pre-training of deep bidirectional transformers for language understanding},
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.04805}
}
@article{yang2019xlnet,
	title        = {Xlnet: Generalized autoregressive pretraining for language understanding},
	author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@article{radford2018improving,
	title        = {Improving language understanding by generative pre-training},
	author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year         = 2018
}
@article{lewis2019bart,
	title        = {Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
	author       = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1910.13461}
}
@inproceedings{weston2011wsabie,
	title        = {Wsabie: Scaling up to large vocabulary image annotation},
	author       = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
	year         = 2011,
	booktitle    = {Twenty-Second International Joint Conference on Artificial Intelligence}
}
@article{frome2013devise,
	title        = {Devise: A deep visual-semantic embedding model},
	author       = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc'Aurelio and Mikolov, Tomas},
	year         = 2013,
	journal      = {Advances in neural information processing systems},
	volume       = 26
}
@inproceedings{li2022grounded,
	title        = {Grounded language-image pre-training},
	author       = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {10965--10975}
}
@inproceedings{radford2021learning,
	title        = {Learning transferable visual models from natural language supervision},
	author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {8748--8763},
	organization = {PMLR}
}
@inproceedings{arandjelovic2017look,
	title        = {Look, listen and learn},
	author       = {Arandjelovic, Relja and Zisserman, Andrew},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision},
	pages        = {609--617}
}
@inproceedings{arandjelovic2018objects,
	title        = {Objects that sound},
	author       = {Arandjelovic, Relja and Zisserman, Andrew},
	year         = 2018,
	booktitle    = {Proceedings of the European conference on computer vision (ECCV)},
	pages        = {435--451}
}
@inproceedings{morgado2021audio,
	title        = {Audio-visual instance discrimination with cross-modal agreement},
	author       = {Morgado, Pedro and Vasconcelos, Nuno and Misra, Ishan},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {12475--12486}
}
@inproceedings{mettes2017spatial,
	title        = {Spatial-aware object embeddings for zero-shot localization and classification of actions},
	author       = {Mettes, Pascal and Snoek, Cees GM},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {4443--4452}
}
@article{korbar2018cooperative,
	title        = {Cooperative learning of audio and video models from self-supervised synchronization},
	author       = {Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
	year         = 2018,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 31
}
@inproceedings{mikolov2013efficient,
	title        = {Efficient estimation of word representations in vector space},
	author       = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year         = 2013,
	booktitle    = {arXiv preprint arXiv:1301.3781}
}
@inproceedings{kingma2015adam,
	title        = {Adam: A method for stochastic optimization},
	author       = {Kingma, Diederik P and Ba, Jimmy},
	year         = 2015,
	booktitle    = ICLR
}
@article{luo2022clip4clip,
	title        = {CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval and captioning},
	author       = {Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
	year         = 2022,
	journal      = {Neurocomputing},
	publisher    = {Elsevier},
	volume       = 508,
	pages        = {293--304}
}
@inproceedings{lee2018stacked,
	title        = {Stacked cross attention for image-text matching},
	author       = {Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
	year         = 2018,
	booktitle    = {Proceedings of the European conference on computer vision (ECCV)},
	pages        = {201--216}
}
@inproceedings{miech2019howto100m,
	title        = {Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
	author       = {Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {2630--2640}
}
@article{aytar2017see,
	title        = {See, hear, and read: Deep aligned representations},
	author       = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1706.00932}
}
@inproceedings{harwath2018jointly,
	title        = {Jointly discovering visual objects and spoken words from raw sensory input},
	author       = {Harwath, David and Recasens, Adria and Sur{\'\i}s, D{\'\i}dac and Chuang, Galen and Torralba, Antonio and Glass, James},
	year         = 2018,
	booktitle    = {Proceedings of the European conference on computer vision (ECCV)},
	pages        = {649--665}
}
@inproceedings{ma2019unpaired,
	title        = {Unpaired image-to-speech synthesis with multimodal information bottleneck},
	author       = {Ma, Shuang and McDuff, Daniel and Song, Yale},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {7598--7607}
}
@article{alayrac2020self,
	title        = {Self-supervised multimodal versatile networks},
	author       = {Alayrac, Jean-Baptiste and Recasens, Adria and Schneider, Rosalia and Arandjelovi{\'c}, Relja and Ramapuram, Jason and De Fauw, Jeffrey and Smaira, Lucas and Dieleman, Sander and Zisserman, Andrew},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {25--37}
}
@article{akbari2021vatt,
	title        = {Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
	author       = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34
}
@article{rouditchenko2020avlnet,
	title        = {Avlnet: Learning audio-visual language representations from instructional videos},
	author       = {Rouditchenko, Andrew and Boggust, Angie and Harwath, David and Chen, Brian and Joshi, Dhiraj and Thomas, Samuel and Audhkhasi, Kartik and Kuehne, Hilde and Panda, Rameswar and Feris, Rogerio and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.09199}
}
@inproceedings{zhou2018towards,
	title        = {Towards automatic learning of procedures from web instructional videos},
	author       = {Zhou, Luowei and Xu, Chenliang and Corso, Jason J},
	year         = 2018,
	booktitle    = {Thirty-Second AAAI Conference on Artificial Intelligence}
}
@inproceedings{chen2021multimodal,
	title        = {Multimodal clustering networks for self-supervised learning from unlabeled videos},
	author       = {Chen, Brian and Rouditchenko, Andrew and Duarte, Kevin and Kuehne, Hilde and Thomas, Samuel and Boggust, Angie and Panda, Rameswar and Kingsbury, Brian and Feris, Rogerio and Harwath, David and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {8012--8021}
}

@article{tang2021human,
	title        = {Human-centric spatio-temporal video grounding with visual transformers},
	author       = {Tang, Zongheng and Liao, Yue and Liu, Si and Li, Guanbin and Jin, Xiaojie and Jiang, Hongxu and Yu, Qian and Xu, Dong},
	year         = 2021,
	journal      = {IEEE Transactions on Circuits and Systems for Video Technology},
	publisher    = {IEEE}
}
@article{chen2019weakly,
	title        = {Weakly-supervised spatio-temporally grounding natural sentence in video},
	author       = {Chen, Zhenfang and Ma, Lin and Luo, Wenhan and Wong, Kwan-Yee K},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.02549}
}
@inproceedings{yang2022unitab,
  title={Unitab: Unifying text and box outputs for grounded vision-language modeling},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Ahmed, Faisal and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={European Conference on Computer Vision},
  pages={521--539},
  year={2022},
  organization={Springer}
}
@inproceedings{zhong2022regionclip,
  title={Regionclip: Region-based language-image pretraining},
  author={Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16793--16803},
  year={2022}
}
@inproceedings{chen-etal-2019-weakly,
    title = "Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video",
    author = "Chen, Zhenfang  and
      Ma, Lin  and
      Luo, Wenhan  and
      Wong, Kwan-Yee Kenneth",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1183",
    doi = "10.18653/v1/P19-1183",
    pages = "1884--1894",
    abstract = "In this paper, we address a novel task, namely weakly-supervised spatio-temporally grounding natural sentence in video. Specifically, given a natural sentence and a video, we localize a spatio-temporal tube in the video that semantically corresponds to the given sentence, with no reliance on any spatio-temporal annotations during training. First, a set of spatio-temporal tubes, referred to as instances, are extracted from the video. We then encode these instances and the sentence using our newly proposed attentive interactor which can exploit their fine-grained relationships to characterize their matching behaviors. Besides a ranking loss, a novel diversity loss is introduced to train our attentive interactor to strengthen the matching behaviors of reliable instance-sentence pairs and penalize the unreliable ones. We also contribute a dataset, called VID-sentence, based on the ImageNet video object detection dataset, to serve as a benchmark for our task. Results from extensive experiments demonstrate the superiority of our model over the baseline approaches.",
}
@inproceedings{kalogeiton2017action,
	title        = {Action tubelet detector for spatio-temporal action localization},
	author       = {Kalogeiton, Vicky and Weinzaepfel, Philippe and Ferrari, Vittorio and Schmid, Cordelia},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision},
	pages        = {4405--4413}
}
@inproceedings{peng2016multi,
	title        = {Multi-region two-stream R-CNN for action detection},
	author       = {Peng, Xiaojiang and Schmid, Cordelia},
	year         = 2016,
	booktitle    = {European conference on computer vision},
	pages        = {744--759},
	organization = {Springer}
}
@misc{jiang2014thumos,
	title        = {THUMOS challenge: Action recognition with a large number of classes},
	author       = {Jiang, Yu-Gang and Liu, Jingen and Zamir, A Roshan and Toderici, George and Laptev, Ivan and Shah, Mubarak and Sukthankar, Rahul},
	year         = 2014,
	howpublished = {\url{http://crcv.ucf.edu/THUMOS14/}}
}
@inproceedings{jhuang2013towards,
	title        = {Towards understanding action recognition},
	author       = {Jhuang, Hueihan and Gall, Juergen and Zuffi, Silvia and Schmid, Cordelia and Black, Michael J},
	year         = 2013,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {3192--3199}
}
@inproceedings{gu2018ava,
	title        = {Ava: A video dataset of spatio-temporally localized atomic visual actions},
	author       = {Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and others},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {6047--6056}
}
@article{xu2020spatio,
	title        = {Spatio-temporal action detection with multi-object interaction},
	author       = {Xu, Huijuan and Yang, Lizhi and Sclaroff, Stan and Saenko, Kate and Darrell, Trevor},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.00180}
}
@article{weinzaepfel2016human,
	title        = {Human action localization with sparse spatial supervision},
	author       = {Weinzaepfel, Philippe and Martin, Xavier and Schmid, Cordelia},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1605.05197}
}
@inproceedings{Zhukov2019CrossTask,
	title        = {Cross-task weakly supervised learning from instructional videos},
	author       = {Zhukov, Dimitri and Alayrac, Jean-Baptiste and Cinbis, Ramazan Gokberk and Fouhey, David and Laptev, Ivan and Sivic, Josef},
	year         = 2019,
	booktitle    = CVPR
}
@inproceedings{su2012crowdsourcing,
	title        = {Crowdsourcing annotations for visual object detection},
	author       = {Su, Hao and Deng, Jia and Fei-Fei, Li},
	year         = 2012,
	booktitle    = {Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence}
}
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@article{li2020hero,
  title={Hero: Hierarchical encoder for video+ language omni-representation pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  journal={arXiv preprint arXiv:2005.00200},
  year={2020}
}
@inproceedings{videoclip,
  title={VideoCLIP: A Cross-Attention Model for Fast Video-Text Retrieval Task with Image CLIP},
  author={Li, Yikang and Hsiao, Jenhao and Ho, Chiuman},
  booktitle={Proceedings of the 2022 International Conference on Multimedia Retrieval},
  pages={29--33},
  year={2022}
}
@inproceedings{gupta2019lvis,
  title={Lvis: A dataset for large vocabulary instance segmentation},
  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5356--5364},
  year={2019}
}
@inproceedings{gleason2019proposal,
	title        = {A proposal-based solution to spatio-temporal action detection in untrimmed videos},
	author       = {Gleason, Joshua and Ranjan, Rajeev and Schwarcz, Steven and Castillo, Carlos and Chen, Jun-Cheng and Chellappa, Rama},
	year         = 2019,
	booktitle    = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	pages        = {141--150},
	organization = {IEEE}
}
@inproceedings{arnab2020uncertainty,
	title        = {Uncertainty-aware weakly supervised action detection from untrimmed videos},
	author       = {Arnab, Anurag and Sun, Chen and Nagrani, Arsha and Schmid, Cordelia},
	year         = 2020,
	booktitle    = {European Conference on Computer Vision},
	pages        = {751--768},
	organization = {Springer}
}
@inproceedings{t1,
	title        = {Tall: Temporal activity localization via language query},
	author       = {Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {5267--5275}
}
@inproceedings{t2,
	title        = {Temporally grounding natural sentence in video},
	author       = {Chen, Jingyuan and Chen, Xinpeng and Ma, Lin and Jie, Zequn and Chua, Tat-Seng},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 conference on empirical methods in natural language processing},
	pages        = {162--171}
}
@article{t3,
	title        = {Semantic conditioned dynamic modulation for temporal sentence grounding in videos},
	author       = {Yuan, Yitian and Ma, Lin and Wang, Jingwen and Liu, Wei and Zhu, Wenwu},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 32
}
@inproceedings{t4,
	title        = {Cross-modal interaction networks for query-based moment retrieval in videos},
	author       = {Zhang, Zhu and Lin, Zhijie and Zhao, Zhou and Xiao, Zhenxin},
	year         = 2019,
	booktitle    = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
	pages        = {655--664}
}
@misc{mexaction2,
	title        = {MEXaction2: action detection and localization dataset},
	howpublished = {\url{http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset}}
}
@inproceedings{heilbron2019large,
	title        = {A large-scale video benchmark for human activity understanding},
	author       = {Heilbron, FC and Escorcia, V and Ghanem, B and Niebles, J},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, 2015. 961},
	volume       = 970
}
@article{zhou2018weakly,
  title={Weakly-supervised video object grounding from text by loss weighting and object interaction},
  author={Zhou, Luowei and Louis, Nathan and Corso, Jason J},
  journal={arXiv preprint arXiv:1805.02834},
  year={2018}
}

@article{soldan2021mad,
	title        = {Mad: A scalable dataset for language grounding in videos from movie audio descriptions},
	author       = {Soldan, Mattia and Pardo, Alejandro and Alc{\'a}zar, Juan Le{\'o}n and Heilbron, Fabian Caba and Zhao, Chen and Giancola, Silvio and Ghanem, Bernard},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2112.00431}
}
@article{regneri2013grounding,
	title        = {Grounding action descriptions in videos},
	author       = {Regneri, Michaela and Rohrbach, Marcus and Wetzel, Dominikus and Thater, Stefan and Schiele, Bernt and Pinkal, Manfred},
	year         = 2013,
	journal      = {Transactions of the Association for Computational Linguistics},
	publisher    = {MIT Press},
	volume       = 1,
	pages        = {25--36}
}
@inproceedings{t5,
	title        = {Multilevel language and vision integration for text-to-clip retrieval},
	author       = {Xu, Huijuan and He, Kun and Plummer, Bryan A and Sigal, Leonid and Sclaroff, Stan and Saenko, Kate},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	number       = {01},
	pages        = {9062--9069}
}
@inproceedings{t6,
	title        = {Temporally grounding language queries in videos by contextual boundary-aware prediction},
	author       = {Wang, Jingwen and Ma, Lin and Jiang, Wenhao},
	year         = 2020,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 34,
	number       = {07},
	pages        = {12168--12175}
}
@inproceedings{t7,
	title        = {Proposal-free temporal moment localization of a natural-language query in video using guided attention},
	author       = {Rodriguez, Cristian and Marrese-Taylor, Edison and Saleh, Fatemeh Sadat and Li, Hongdong and Gould, Stephen},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
	pages        = {2464--2473}
}
@inproceedings{t8,
	title        = {To find where you talk: Temporal sentence localization in video with attention based location regression},
	author       = {Yuan, Yitian and Mei, Tao and Zhu, Wenwu},
	year         = 2019,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	number       = {01},
	pages        = {9159--9166}
}
@article{t9,
	title        = {Tripping through time: Efficient localization of activities in videos},
	author       = {Hahn, Meera and Kadav, Asim and Rehg, James M and Graf, Hans Peter},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1904.09936}
}
@inproceedings{t10,
	title        = {Language-driven temporal activity localization: A semantic matching reinforcement learning model},
	author       = {Wang, Weining and Huang, Yan and Wang, Liang},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {334--343}
}
@inproceedings{t11,
	title        = {Dense regression network for video grounding},
	author       = {Zeng, Runhao and Xu, Haoming and Huang, Wenbing and Chen, Peihao and Tan, Mingkui and Gan, Chuang},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {10287--10296}
}
@inproceedings{t12,
	title        = {Cascaded prediction network via segment tree for temporal video grounding},
	author       = {Zhao, Yang and Zhao, Zhou and Zhang, Zhu and Lin, Zhijie},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {4197--4206}
}
@article{youcookbbx,
	title        = {Weakly-supervised video object grounding from text by loss weighting and object interaction},
	author       = {Zhou, Luowei and Louis, Nathan and Corso, Jason J},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.02834},
	booktitle    = {BMVC}
}
@inproceedings{ZhLoCoBMVC18,
    author={Zhou, Luowei and Louis, Nathan and Corso, Jason J},
    title={Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction},
    booktitle = {British Machine Vision Conference},
    year = {2018},
    url = {http://bmvc2018.org/contents/papers/0070.pdf}
  }
@inproceedings{s2,
	title        = {Video object grounding using semantic roles in language description},
	author       = {Sadhu, Arka and Chen, Kan and Nevatia, Ram},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {10417--10427}
}
@article{caron2020unsupervised,
	title        = {Unsupervised learning of visual features by contrasting cluster assignments},
	author       = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {9912--9924}
}
@inproceedings{cuturi2013sinkhorn,
	title        = {Sinkhorn distances: Lightspeed computation of optimal transport},
	author       = {Cuturi, Marco},
	year         = 2013,
	booktitle    = NIPS
}
@article{asano2019self,
	title        = {Self-labelling via simultaneous clustering and representation learning},
	author       = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year         = 2020,
	journal      = ICLR
}
@inproceedings{han2022temporal,
	title        = {Temporal alignment networks for long-term video},
	author       = {Han, Tengda and Xie, Weidi and Zisserman, Andrew},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {2906--2916}
}
@inproceedings{shou2016temporal,
	title        = {Temporal action localization in untrimmed videos via multi-stage cnns},
	author       = {Shou, Zheng and Wang, Dongang and Chang, Shih-Fu},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {1049--1058}
}
@inproceedings{escorcia2016daps,
	title        = {Daps: Deep action proposals for action understanding},
	author       = {Escorcia, Victor and Caba Heilbron, Fabian and Niebles, Juan Carlos and Ghanem, Bernard},
	year         = 2016,
	booktitle    = {European conference on computer vision},
	pages        = {768--784},
	organization = {Springer}
}
@inproceedings{li2017tracking,
	title        = {Tracking by natural language specification},
	author       = {Li, Zhenyang and Tao, Ran and Gavves, Efstratios and Snoek, Cees GM and Smeulders, Arnold WM},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages        = {6495--6503}
}

@inproceedings{xie2018rethinking,
	title        = {Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification},
	author       = {Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
	year         = 2018,
	booktitle    = {Proceedings of the European conference on computer vision (ECCV)},
	pages        = {305--321}
}
@inproceedings{ShvetsovaCVPR22Everything,
	title        = {Everything at Once--Multi-modal Fusion Transformer for Video Retrieval},
	author       = {Shvetsova, Nina and Brian Chen and Andrew Rouditchenko and Samuel Thomas and Brian Kingsbury and Rogerio Feris and David Harwath and James Glass and Hilde Kuehne},
	year         = 2022,
	booktitle    = {CVPR}
}
@article{abnar2020quantifying,
	title        = {Quantifying attention flow in transformers},
	author       = {Abnar, Samira and Zuidema, Willem},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2005.00928}
}
