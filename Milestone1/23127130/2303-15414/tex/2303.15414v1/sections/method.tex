\section{Graph Matching Network and GMTracker}
In this section, we will describe the details of our Graph Matching Network and our {\gm}. As shown in Fig.~\ref{pipeline}, the pipeline of our Graph Matching Network consists of three parts: (1) feature encoding in detection and tracklet graphs; (2) feature enhancement by cross-graph Graph Convolutional Network (GCN) and (3) differentiable graph matching layer. We will describe these three parts step by step and show how we integrate them into a tracker ({\gm}) in the following.

%\noindent{\bf Graph construction and feature encoding.}
%We construct two graphs, detection graph and tracklet graph. In detection graph, each vertex represents an object in current frame. We encode the appearance feature of the object into vertex. Tracklet graph contains the trajectories information of past frames. We aggregate the features of objects belong to the same tracklet as a vertex in tracklet graph. 
%
%\noindent{\bf Feature propagation using cross-graph GCN.}
%We adopt GCN cross tracklet graph and detection graph to propagation features between detection and tracklet.
%
%%The similarities of appearance and geometric information are encoded on edge between two graphs.
%
%\noindent{\bf Differential graph matching layer.}
%As derived in section \ref{sec:relax}, the graph matching layer can be regarded as a QP layer in our graph matching network.
%% To make graph matching module end-to-end learnable, we redefine a relaxed convex quadratic problem formulation for graph matching problem. 
%We implement the differential graph matching layer following OptNet~\cite{amos2017optnet}.
%\begin{comment}
%{\color{red}
%\noindent{\bf Optimization and loss function.}
%Weighted BCE loss is adopted in our method. Before fed  loss function, the graph matching score matrix is sharpened by a Softmax function with temperature.
%}
%\end{comment}
\subsection{Feature Encoding in Two Graphs}

%\subsection{Feature Encoding}
%\label{sec:fegc}

We utilize a pre-trained ReIDentification (ReID) network followed by a multi-layer perceptron (MLP) to generate the appearance feature $\mathbf{a}_D^{i}$ for each detection $D_i$.
The appearance feature $\mathbf{a}_T^{j}$ of the tracklet $T_{j}$ is obtained by averaging all the appearance features of detections before.

\begin{comment}
{\color{red}We also try other aggregation function like moving average, and find simple average can yield better performance.} 
\end{comment}
%Note that we do not associate edges in $\MCG_D^t$ and $\MCG_T^t$ with edge features currently. We will do this after cross-graph GCN.
\begin{comment}
\begin{equation}
  \begin{aligned}
    \mathbf{a}_T^{i,t}=\frac{1}{|K|}\sum_{k\in K}{\rm MLP_{e}}\comp {\rm Enc}(\mathbf{I}_{p,i}^k)
  \end{aligned}
\end{equation}
where $K=\{k|k<t\}$. 
\end{comment}
%We also try other feature aggregation methods, such as moving average in the whole tracklet, moving average in the last few frames, etc. 
%Comparing these methods, we finally choose the simplest but effective way, averaging the appearance feature during the whole tracklet. 
%For the comparison between these methods, see Table \ref{tbl:traagg}.
\par
\begin{comment}
Then, two complete graphs are constructed. Let $\mathcal{G}_T^t=(\mathcal{V}_T,\mathcal{E}_T)$ be a tracklet graph. The vertex $n_i$ in vertex set $\mathcal{V}_T$ corresponds to the tracklet $T_i^t$. The edges in edge set $\mathcal{E}_T=\{e_{i,j}\}$ are the fully connections between two arbitrary vertices in graph $\mathcal{G}_T^t$. Each vertex feature $\mathbf{h}_i$ of $n_i$ is the appearance feature embedding of the tracklet and the edge feature $\mathbf{h}_{i,j}$ on the edge $e_{i,j}$ contains two vertex features, as
\begin{equation}
  \begin{aligned}
    \mathbf{h}_i^{(0)}=\mathbf{a}_T^{i,t}, \ \ \mathbf{h}_{i,j}^{(0)}=\bm{[}\mathbf{a}_T^{i,t},\mathbf{a}_T^{j,t}\bm{]}
\end{aligned}
\end{equation}
where, $\bm{[\cdot]}$ denotes concatenation operation.
\par
The detection graph $\mathcal{G}_D^t=(\mathcal{V}_D,\mathcal{E}_D)$ is similar to the tracklet graph. Each vertex includes a detection appearance feature embedding, and each edge is the combination of two connected vertex features, as
\begin{equation}
  \begin{aligned}
    \mathbf{h}_p^{(0)}=\mathbf{a}_D^{p,t},\ \  \mathbf{h}_{p,q}^{(0)}=\bm{[}\mathbf{a}_D^{p,t},\mathbf{a}_D^{q,t}\bm{]}.
\end{aligned}
\end{equation}
\end{comment}

\subsection{Cross-Graph GCN}
\label{sec:gcn}
 Similar to \cite{braso2020learning, ma2019deep, Weng2020_GNN3DMOT}, we only adopt a GCN module between the graph $\MCG_D$ and graph $\MCG_T$ to enhance the feature, and thus it is called Cross-Graph GCN. 

 The initial vertex features on detection graph and tracklet graph are the appearance features on the vertices, i.e., let $\Mh_i^{(0)}=\mathbf{a}_D^{i}$ and $\Mh_j^{(0)}=\mathbf{a}_T^{j}$.
 \begin{comment}
 In the following, we describe how we enhance the feature of a detection vertex by aggregating the features from the tracklet graph. Similar procedure is also applied to the tracklet vertices.
 \end{comment}
Let $\Mh_i^{(l)}$ and $\Mh_j^{(l)}$ be the feature of vertex $i \in \MCG_D$ and vertex $j \in \MCG_T$ in the $l$-th propagation, respectively. 
\begin{comment}
Then the feature update for vertex $i$ can be formulated as:
\begin{align}
	&\Mm_{i}^{(l)} = \MCA(\{w_{i,j}^{(l)}\Mh_j^{(l)} \mid j \in \MCG_T\}), \\
	&\Mh_{i}^{(l + 1)} = \MCF(\Mh_i^{(l)}, \Mm_{i}^{(l)}),
\end{align}
where $w_{i,j}$ is a weight coefficient, $\MCA(\cdot)$ and $\MCF(\cdot)$ are the aggregation and the transformation function, respectively.
\end{comment}
We define the aggregation weight coefficient $w_{i, j}^{(l)}$ in GCN as the appearance and geometric similarity between vertex $i$ and vertex $j$:
\begin{equation}
w_{i,j}^{(l)} = \cos(\Mh_i^{(l)}, \Mh_j^{(l)}) + {\rm{IoU}}(\mathbf{g}_i, \mathbf{g}_j)
\end{equation}
where $\cos(\cdot, \cdot)$ means the cosine similarity between input features and $\rm{IoU(\cdot, \cdot)}$ denotes the Intersection over Union of two bounding boxes. For a detection vertex $i$, $\mathbf{g}_i$ is the corresponding detection bounding box defined in Section \ref{sec:construct}. As for a tracklet vertex $j$, we estimate the bounding box $\mathbf{g}_j$ in current frame $t$ by Kalman Filter \cite{kalman1960new} motion model with a constant velocity. Note that we only consider the appearance feature similarity in weight $w_{i,j}$ when the camera moves, since the motion model cannot predict reliable future positions in these complicated scenes.

We use summation as the aggregation function, i.e., $\Mm_{i}^{(l)} = \sum_{j \in \MCG_T} w_{i,j}^{(l)}\Mh_j^{(l)}$ and the vertex features are updated by:
\begin{equation}
\Mh_i^{(l + 1)} = {\rm{MLP}}(\Mh_i^{(l)} + \frac{\norm{\Mh_i^{(l)}}_2\Mm_{i}^{(l)}}{\norm{\Mm_{i}^{(l)}}_2}),
\end{equation}
where we adopt message normalization proposed in \cite{li2020deepergcn} to stabilize the training.

We apply $l_2$ normalization to the final features after cross-graph GCN and denote it as $\Mh_i$. Then we use $\Mh_i$ as the feature of vertex $i$ in graph $\MCG_D$, and construct the edge feature for edge $(i, i')$ with $\Mh_{i,i'} = l_2([\Mh_i, \Mh_{i'}])$, where $[\cdot]$ denotes concatenation operation. The similar operation is also applied to the tracklet graph $\MCG_T$. In our implementation, we only apply GCN once.

\subsection{Differentiable Graph Matching Layer}
\label{sec:diffgm}
\begin{comment}
\footnote{A key issue of the section is how does M connected to Eqn6? What is the meaning of Graph matching in the context of MOT? These points need to be clarified before going into the details.}
\end{comment}
After enhancing the vertex features and constructing the edge features on graph $\MCG_D$ and $\MCG_T$, we meet the core component of our method: the differentiable graph matching layer. By optimizing the QP in Eq.~\ref{finalQP} from quadratic affinity matrix $\mathbf{M}$ and vertex affinity matrix $\mathbf{B}$, we can derive the optimal matching score vector $\mathbf{x}$ and reshape it back to the shape $n_d \times n_t$ to get the matching score map $\mathbf{X}$. 


Since we finally formulate the graph matching problem as a QP, we can construct the graph matching module as a differentiable QP layer in our neural network. Since KKT conditions are the necessary and sufficient conditions for the optimal solution $\mathbf{x}^*$ and its dual variables, we could derive the gradient in backward pass of our graph matching layer based on the KKT conditions and implicit function theorem, which is inspired by OptNet~\cite{amos2017optnet}.
% \footnote{Put the details in supplemental material} 
In our implementation, we adopt the qpth library~\cite{amos2017optnet} to build the graph matching module.
% Besides solving the QP, in the neural network, the differential QP layer needs back propagation. According to the matrix
% differential calculus, the derivative of the solution can be obtained from KKT conditions, which is the sufficient and necessary conditions for the optima. In our graph matching module, the gradients required in back propagation, can be formulated ~\cite{amos2017optnet} as 
% \begin{equation}
%   \begin{aligned}
%     \nabla_\mathbf{Q} \ell = \frac{1}{2}(d_x x^\top + xd_x^\top),
%     \nabla_\mathbf{q} \ell = d_x 
%   \end{aligned}
%   \label{eq:grads}
% \end{equation}
% where, $\ell$ is the loss function.
In the inference stage, to reduce the computational cost and accelerate the algorithm, we solve the QP using the CVXPY library~\cite{diamond2016cvxpy} only for forward operation.
\begin{comment}
Following FGM~\cite{}, the edge affinity matrix between two graphs, can be formed as 
\begin{equation}
  \mathbf{M}=(\mathbf{G_2}\otimes\mathbf{G_1})[\text{vec}(\mathbf{M_e})](\mathbf{H_2}\otimes\mathbf{H_1})^T
\end{equation}

We can adopt the methods in OptNet~\cite{amos2017optnet} and Cvxpylayer~\cite{AgrawalABBDK19}, regarding the graph matching as a QP layer, which can be end-to-end trained. In the implementation, we use qpth library.
\par
\noindent{\bf Graph matching using vertex and edge features.} \ 
In this part, we expand the relaxed QP Eq.\ref{QP} formulation using vertex and edge features.

\noindent{\bf Solving the QP and back propagation.} \ 
In this part, we describe how to solve the QP and how to compute gradients in the process of back propagation.
\begin{equation}
  \begin{aligned}
    \nabla_Q \ell &= \frac{1}{2}(d_z z^T + zd_z^T) &
    \nabla_q \ell &= d_z \\
  \end{aligned}
  \label{eq:grads}
\end{equation}
\end{comment}
\par
For training, we use weighted binary cross entropy Loss: 
\begin{equation}
  \begin{aligned}
    \mathcal{L}=\frac{-1}{n_dn_t}\sum_{i=1}^{n_d}\sum_{j=1}^{n_t}\ ky_{i,j}\log(\hat{y}_{i,j})+(1-y_{i,j})\log(1-\hat{y}_{i,j}),
\end{aligned}
\end{equation}
where $\hat y_{i,j}$ denotes the matching score between detection $D_i$ and tracklet $T_j$, and $y_{i,j}$ is the ground truth indicating whether the object belongs to the tracklet. $k=(n_t-1)$ is the weight to balance the loss between positive and negative samples. Besides, due to our QP formulation of graph matching, the distribution of matching score map $\mathbf{X}$ is relatively smooth. We adopt softmax function with temperature $\tau$ to sharpen the distribution of scores before calculating the loss:
\begin{equation}
\hat{y}_{i,j} = {\rm Softmax}(x_{i,j},\tau)=\frac{e^{x_{i,j}/\tau}}{\sum_{j=1}^{n_t}e^{x_{i,j}/\tau}},
\label{eq:softmax}
\end{equation}
where $x_{i,j}$ is the original matching score in score map $\mathbf{X}$.

\begin{comment}
After matching, We follow DeepSORT \cite{wojke2017simple} to handle the born and death of tracklets. We start a new tracklet if a detection meets one of the three criteria: 1) all the appearance similarities between a detection and existing tracklets are below a threshold $\sigma$. 2) it is far away from all tracklets, i.e. its center location is out of motion gate $\kappa$.\footnote{This one is not clear to me.} 3) the bounding box has no overlap with any tracklets. The motion model we used \footnote{What for?}is Kalman Filter, but for the video taken by the moving camera, we apply an Enhanced Correlation Coefficient (ECC) \cite{ecc} model additionally to compensate the camera motion.\footnote{Then why not apply it to the construction of edge weight?} For the unmatched detections\footnote{Is it right?}, we apply IoU matching between the filtered detections\footnote{What does filtered mean?} and the unmatched tracklets.\footnote{Then when do we delete a tracklet?}
\end{comment}