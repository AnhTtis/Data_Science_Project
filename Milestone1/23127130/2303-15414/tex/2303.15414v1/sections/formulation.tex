\section{Graph Matching Formulation for Data Association}
\label{sec:relax}
In this section, we will formulate data association problem as a graph matching problem. Instead of solving the original Quadratic Assignment Problem (QAP), we relax the graph matching formulation as a convex quadratic programming (QP) and extend the formulation from the edge weights to the edge features. The relaxation facilitates the differentiable and joint learning of feature representation and combinatorial optimization.
\begin{comment}
\textcolor{red}{In this section, we will describe the formulation of graph matching and how to relax it as a convex quadratic programming (QP) problem. Then we will show how to extend the formulation from edge weights to edge features.}
\end{comment}
%In general, the \textbf{intuition} of our derivation below is that we relax the basic formulation of graph matching to a convex quadratic programming (QP), and it can become a QP layer in our learnable pipeline. To expand the formulation from edge weights to edge features, we finally have a formulation Eq.\ref{finalQP}.
\subsection{Basic Graph Matching Formulation for Data Association}
% Given the detection graph $\MCG_D$ and the tracklet graph $\MCG_T$, the graph matching problem is to maximize the similarities between the matched vertices and corresponding edges connected by these vertices. In the following derivation, we use the general notation $\mathcal{G}_1$ and $\mathcal{G}_2$ to obtain a general graph matching formulation.
We define the aim of data association is to match the vertices in graph $\mathcal{G}_1$ and $\mathcal{G}_2$ constructd in view 1 and view 2 respectively. So, it can be seen as a graph matching problem, which is to maximize the similarities between the matched vertices and corresponding edges connected by these vertices.
As defined in \cite{LawlerMS63}, the graph matching problem is a Quadratic Assignment Problem (QAP) . A practical mathematical form is named \emph{Koopmans-Beckmann's} QAP \cite{kbqap}:
\begin{equation}
\label{equ:KBQAP}
\begin{aligned}
& \underset{\MPI}{\text{maximize}}
&& \mathcal{J}(\MPI)=\text{tr}(\MA_1\MPI\MA_2\MPI^\top)+\text{tr}(\MB^\top\MPI),  \\
& \text {s.t.}
&& \MPI\mathbf{1}_{n}= \mathbf{1}_{n}, \MPI^\top\mathbf{1}_{n}= \mathbf{1}_{n},
\end{aligned}
\end{equation}
where $\MPI\in\{0,1\}^{n\times{n}}$ is a permutation matrix that denotes the matching between the vertices of two graphs, $\MA_1\in\MBR^{n\times n}$, $\MA_2\in\MBR^{n\times n}$ are the weighted adjacency matrices of graph $\mathcal{G}_1$ and $\mathcal{G}_2$ respectively, and $\MB\in\MBR^{n\times n}$ is the vertex affinity matrix between $\mathcal{G}_1$ and $\mathcal{G}_2$. $\mathbf{1}_{n}$ denotes an n-dimensional vector with all values to be 1.

\subsection{Reformulation and Convex Relaxation}
\label{sec:qp}
For \emph{Koopmans-Beckmann's} QAP, as $\MPI$ is a permutation matrix, i.e., $\MPI^\top\MPI=\MPI\MPI^\top=\mathbf{I}$. Following~\cite{facgm}, Eq.~\ref{equ:KBQAP} can be rewritten as
\begin{equation}
\begin{aligned}
\mathbf{\Pi}^*
&=\underset{\mathbf{\Pi}}{\arg\min} \ \frac{1}{2}||\mathbf{A_1}\mathbf{\Pi}-\mathbf{\Pi}\mathbf{A_2}||_F^2-\text{tr}(\mathbf{B}^\top\mathbf{\Pi}).
\end{aligned}
\label{K-B}
\end{equation}
%\begin{comment}
%    &=  \underset{\mathbf{\Pi}\in\mathcal{P}}{\arg\max} \ \text{tr}(\mathbf{A_1}\mathbf{\Pi}\mathbf{A_2}\mathbf{\Pi}^\top)+\text{tr}(\mathbf{K}^\top\mathbf{\Pi}) \\
%&=  \underset{\mathbf{\Pi}\in\mathcal{P}}{\arg\max} \ \text{tr}(\mathbf{A_1}\mathbf{\Pi}\mathbf{A_2}\mathbf{\Pi}^\top)  - \frac{1}{2} \text{tr}(\mathbf{A_1}\mathbf{A_1}\mathbf{\Pi}\mathbf{\Pi}^\top) \\
%&\ \ \ \ -\frac{1}{2}\text{tr}(\mathbf{A_2}\mathbf{A_2}\mathbf{\Pi}^\top\mathbf{\Pi})+\text{tr}(\mathbf{K}^\top\mathbf{\Pi}) \\
%&= \underset{\mathbf{\Pi}\in\mathcal{P}}{\arg\max} \ - \frac{1}{2} ||\mathbf{A_1}\mathbf{\Pi}-\mathbf{\Pi}\mathbf{A_2}||_F^2+\text{tr}(\mathbf{K}^\top\mathbf{\Pi}) \\
%\end{comment}
This formulation is more intuitive than that in Eq.~\ref{equ:KBQAP}. For two vertices $i, i' \in \MCG_1$  
and their corresponding vertices $j, j' \in \MCG_2$, the first term in Eq.~\ref{K-B} denotes the difference of the weight of edge $(i, i')$ and $(j, j')$, and the second term denotes the vertex affinities between $i$ and $j$. Then the goal of the optimization is to maximize the vertex affinities between all matched vertices, and minimize the difference of edge weights between all matched edges.
 
It can be proven that the convex hull of the permutation matrix lies in the space of the doubly-stochastic matrix. So, as shown in \cite{aflalo2015convex}, the QAP (Eq.~\ref{K-B}) can be relaxed to its tightest convex relaxation by only constraining the permutation matrix $\mathbf{\Pi}$ to be a double stochastic matrix $\mathbf{X}$, formed as the following QP problem:
\begin{equation}
\mathbf{X}^*=\underset{\mathbf{X}\in\mathcal{D}}{\arg\min} \ \frac{1}{2}||\mathbf{A_1}\mathbf{X}-\mathbf{X}\mathbf{A_2}||_F^2-\text{tr}(\mathbf{B}^\top\mathbf{X}),
\label{QP}
\end{equation}
where $\mathcal{D}=\{\mathbf{X}:\mathbf{X}\mathbf{1}_n= \mathbf{1}_n, \mathbf{X}^\top\mathbf{1}_n=\mathbf{1}_n,\mathbf{X}\geq\mathbf{0}\}$.
%By the property of doubly stochastic matrix, The optimal solution $\mathbf{X}^*$ lies on the vertices of the constraint set $\mathcal{D}$. Thus it is equivalent to the optimal solution $\MPI^*$ in the original problem in Eq.~\ref{K-B}.
%\begin{comment}
%{\color{red}
%Then, to formulate the QP(Eq. \ref{QP}) as a standard QP formulation, we should first vectorize the matrix $\mathbf{X}$, i.e. $\mathbf{x}=\text{vec}(\mathbf{X})$. So, the relaxed QP can be formed as 
%\begin{equation}
%\begin{aligned}
%\mathbf{x}^*
%&=\underset{\mathbf{x}\in\mathcal{D}^{'}}{\arg\min} \ ||\mathbf{B_1}\mathbf{x}-\mathbf{B_2}\mathbf{x}||_2^2-\mathbf{k}_p^\top\mathbf{x}  \\
%&=\underset{\mathbf{x}\in\mathcal{D}^{'}}{\arg\min} \ (\mathbf{B_1}\mathbf{x}-\mathbf{B_2}\mathbf{x})^\top(\mathbf{B_1}\mathbf{x}-\mathbf{B_2}\mathbf{x})-\mathbf{k}_p^\top\mathbf{x} \\
%&= \underset{\mathbf{x}\in\mathcal{D}^{'}}{\arg\min} \ \mathbf{x}^\top(\mathbf{B_1}^\top\mathbf{B_1}-\mathbf{B_2}^\top\mathbf{B_1}-\mathbf{B_1}^\top\mathbf{B_2} \\
%&\ \ \ \ +\mathbf{B_2}^\top\mathbf{B_2})\mathbf{x}-\mathbf{k}_p^\top\mathbf{x} \\
%&=\underset{\mathbf{x}\in\mathcal{D}^{'}}{\arg\min} \ \frac{1}{2}\mathbf{x}^\top\mathbf{Q}\mathbf{x}-\mathbf{k}_p^\top\mathbf{x}
%\label{vecQP}
%\end{aligned}
%\end{equation}
%where, $ \mathcal{D}^{'}=\{\mathbf{x}:\mathbf{A}\mathbf{x}=\mathbf{1},\mathbf{x}\geq\mathbf{0},\mathbf{A}=\left[\begin{array}{ccc}
%\mathbf{1}_{n}^\top\otimes{\mathbf{I}_{n}} \\
%\mathbf{I}_{n}\otimes{\mathbf{1}_{n}^\top}
%\end{array} 
%\right ]\}$, $\mathbf{B_1}=\mathbf{I}\otimes\mathbf{A_1}$ and $\mathbf{B_2}=\mathbf{A_2^\top}\otimes\mathbf{I}$, $\mathbf{k}_p=\text{vec}(\mathbf{K_p})$.
%}
%\end{comment}
\section{Graph Matching for MOT}
In this section, we introduce the problem definition of MOT and our graph matching formulation for the data association in MOT task.
\subsection{Detection and Tracklet Graphs Construction}
\label{sec:construct}
As an online tracker, we track objects frame by frame. 
In frame $t$, we define $\MCD^t=\{D_1^t, D_2^t,\cdots, D_{n_d}^t\}$ as the set of detections in current frame and $\MCT^t=\{T_1^t, T_2^t, \cdots, T_{n_t}^t\}$ as the set of tracklets obtained from past frames. $n_d$ and $n_t$ denote the number of detected objects and tracklet candidates. A detection is represented by a triple $D_p^t=(\mathbf{I}_p^t, \mathbf{g}_p^t, t)$, where $\mathbf{I}_p^t$ contains the image pixels in the detected area, $\mathbf{g}_p^t=(x_p^t,y_p^t,w_p^t,h_p^t)$ is a geometric vector including the central location and size of the detection bounding box. Each tracklet contains a series of detected objects with the same tracklet id. With a bit abuse of notations, the generation of $T_{id}^t$ can be represented as $T_{id}^{t} \gets T_{id}^{t-1} \cup \{D^{t-1}_{(id)}\}$, which means we add $D^{t-1}_{(id)}$ to the tracklet $T_{id}^{t-1}$.

Then we define the detection graph in frame $t$ as $\MCG_D^t=(\MCV_D^t, \MCE_D^t)$ and the tracklet graph up to the frame $t$ as $\MCG_T^t=(\MCV_T^t, \MCE_T^t)$. Each vertex $i \in \MCV_D^t$ and vertex $j\in \MCV_T^t$ represents the detection $D_i^t$ and the tracklet $T_j^t$, respectively. The $e_u=(i,i')$ is the edge in $\MCE_D^t$ and $e_v=(j,j')$ is the edge in $\MCE_T^t$. Both of these two graphs are complete graphs. Then the data association in frame $t$ can be formulated as a graph matching problem between $\MCG_D^t$ and $\MCG_T^t$. For simplicity, we will ignore $t$ in the following sections.
\subsection{From Edge Weights to Edge Features}
In the general formulation of graph matching, the element $a_{i,i'}$ in the weighted adjacency matrix $\MA \in\MBR^{n\times n}$ is a scalar denoting the weight on the edge $(i, i')$. To facilitate the application in our MOT problem, we expand the relaxed QP formulation by using an \emph{$l_2$-normalized} edge feature $\mathbf{h}_{i,i'} \in \MBR^d$ instead of the scalar-formed edge weight $a_{i,i'}$ in $\mathbf{A}$. We build a weighted adjacency tensor $\mathbf{H} \in \MBR^{d \times n \times n}$ where $\mathbf{H}^{\cdot, i,i'}$ = $\mathbf{h}_{i, i'}$, i.e., we consider the each dimension of $\mathbf{h}_{i,i'}$ as the element $a_{i,i'}$ in $\mathbf{A}$ and concatenate them along channel dimension. The $\mathbf{H_D}$ and $\mathbf{H_T}$ are the weighted adjacency tensors for $\MCG_D$ and $\MCG_T$, respectively. Then the optimization objective in Eq.~\ref{K-B} can be further expanded to consider the $l_2$ distance between two corresponding \emph{n-d} edge features other than the scalar differences:
\begin{equation}
\begin{aligned}
\mathbf{\Pi}^*
&=\underset{\mathbf{\Pi}}{\arg\min} \ \sum_{c=1}^{d}\frac{1}{2}||\mathbf{H}_D^c\mathbf{\Pi}-\mathbf{\Pi}\mathbf{H}_T^c||_F^2-\text{tr}(\mathbf{B}^\top\mathbf{\Pi}) \\
&=\underset{\mathbf{\Pi}}{\arg\min} \ \sum_{i=1}^{n}\sum_{i'=1}^{n}\sum_{j=1}^{n}\sum_{j'=1}^{n}  \frac{1}{2}||\mathbf{h}_{ii'}\pi_{ij}-\mathbf{h}_{jj'}\pi_{i'j'}||_2^2 \\
&\ \ \ \ -\text{tr}(\mathbf{B}^\top\mathbf{\Pi}) \\
&=\underset{\mathbf{\Pi}}{\arg\min} \ \sum_{i=1}^{n}\sum_{i'=1}^{n}\sum_{j=1}^{n}\sum_{j'=1}^{n}  \frac{1}{2}(\pi_{ij}^2-2\pi_{ij}\pi_{i'j'}\mathbf{h}_{ii'}^\top\mathbf{h}_{jj'} \\
&\ \ \ \ +\pi_{i'j'}^2)-\text{tr}(\mathbf{B}^\top\mathbf{\Pi}),
\end{aligned}
\label{before_edge}
\end{equation}
where $n$ is the number of vertices in graph $\mathcal{G}_D$ and $\mathcal{G}_T$, the subscript $i$ and $i'$ are the vertices in graph $\mathcal{G}_D$ and $j$ and $j'$ are in graph $\mathcal{G}_T$. We reformulate Eq.~\ref{before_edge} as: 
\begin{equation}
\bm{\pi}^*=\underset{\bm{\pi}}{\arg\min} \ 
\bm{\pi}^\top((n-1)^2\mathbf{I}-\mathbf{M})\bm{\pi}-\mathbf{b}^\top\bm{\pi},
\label{edge}
\end{equation}
where $\bm{\pi}=\text{vec}(\MPI)$, $\mathbf{b}=\text{vec}(\MB)$ and $\mathbf{M}\in \MBR^{n^2\times n^2}$ is the symmetric quadratic affinity matrix between all the possible edges in two graphs.

Following the relaxation in Section \ref{sec:qp}, the formulation Eq.~\ref{edge} using edge features can be relaxed to a QP:\\
\begin{equation}
\begin{aligned}
\mathbf{x}^*
&=\underset{\mathbf{x}\in\mathcal{D^{'}}}{\arg\min} \ 
\mathbf{x}^\top((n-1)^2\mathbf{I}-\mathbf{M})\mathbf{x}-\mathbf{b}^\top\mathbf{x}, \\
% &=\underset{\mathbf{x}\in\mathcal{D^{'}}}{\arg\min} \ 
% \frac{1}{2}\mathbf{x}^\top\mathbf{Q}\mathbf{x}+\mathbf{q}^\top\mathbf{x} 
\end{aligned}
\label{finalQP}
\end{equation}
where $ \mathcal{D}^{'}=\{\mathbf{x}:\mathbf{R}\mathbf{x}=\mathbf{1},\mathbf{U}\mathbf{x}\leq \mathbf{1},\mathbf{x}\geq\mathbf{0},\mathbf{R}=\mathbf{1}_{n_2}^\top\otimes{\mathbf{I}_{n_1}},\mathbf{U}=\mathbf{I}_{n_2}^\top\otimes{\mathbf{1}_{n_1}}\}$, $\otimes$ denotes Kronecker product.
\begin{figure}
          \centering
           \includegraphics[width=\linewidth]{figures/me2m.pdf}
             \caption{An example of the derivation from edge affinity matrix $\mathbf{M_e}$ to quadratic affinity matrix $\mathbf{M}$.}
             \label{fig:me2m}
  \end{figure}    

 In the implementation, we first compute the cosine similarity between the edges in $\mathcal{G}_D$ and $\mathcal{G}_T$ to construct the matrix $\mathbf{M_e}\in\MBR^{|\mathcal{E}_D|\times |\mathcal{E}_T|}$. 
 The element of the matrix $\mathbf{M_e}$ is the cosine similarity between edge features $\mathbf{h}_{i,i'}$ and $\mathbf{h}_{j,j'}$ in two graphs:
\begin{align}
  \mathbf{M}_e^{u,v}=\mathbf{h}_{i,i'}^\top\mathbf{h}_{j,j'},
  \label{eq:e2e}
\end{align}
where $e_u=(i,{i'})$ is the edge in $\mathcal{G}_D$ and $e_v=(j,{j'})$ is the edge in $\mathcal{G}_T$. 
 
 And following \cite{zanfir2018deep}, we map each element of matrix $\mathbf{M_e}$ to the \emph{symmetric} quadratic affinity matrix $\mathbf{M}$:
\begin{align}
  \mathbf{M}=(\mathbf{S_D}\otimes\mathbf{S_T})\text{diag}(\text{vec}(\mathbf{M_e}))(\mathbf{T_D}\otimes\mathbf{T_T})^\top,
  \label{eq:me2m}
\end{align}
where $\text{diag}(\cdot)$ means constructing a diagonal matrix by the given vector, $\mathbf{S_D} \in \{0,1\}^{|\MCV_D| \times |\MCE_D|}$ and $\mathbf{S_T} \in \{0,1\}^{|\MCV_T| \times |\MCE_T|}$, whose elements are an indicator function:
\begin{equation}
  \mathbb{I}_s(i,u):= \begin{cases}
    1 & \text{if $i$ is the start vertex of edge $e_u$}, \\
    0 & \text{if $i$ is not the start vertex of edge $e_u$},
  \end{cases}
\end{equation}
$\mathbf{T_D} \in \{0,1\}^{|\MCV_D| \times |\MCE_D|}$ and $\mathbf{T_T} \in \{0,1\}^{|\MCV_T| \times |\MCE_T|}$, whose elements are another indicator function:
\begin{equation}
  \mathbb{I}_t(i',u):= \begin{cases}
    1 & \text{if $i'$ is the end vertex of edge $e_u$}, \\
    0 & \text{if $i'$ is not the end vertex of edge $e_u$}.
  \end{cases}
\end{equation}
An example of the derivation from $\mathbf{M_e}$ to $\mathbf{M}$ is illustrated in Fig.~\ref{fig:me2m}. %In brief, each element in $\mathbf{M_e}$ is mapped to the matrix $\mathbf{M}$ to make the edge affinity matrix a symmetric matrix indexed by the vertex pairs.

Besides, each element in the vertex affinity matrix $\mathbf{B}$ is the cosine similarities between feature $\mathbf{h}_i$ on vertex $i \in \MCV_D$ and feature $\mathbf{h}_j$ on vertex $j\in \MCV_T$:
\begin{align}
  \mathbf{B}_{{i,j}}=\mathbf{h}_{i}^\top\mathbf{h}_{j}
  \label{eq:n2n}
\end{align}
\par