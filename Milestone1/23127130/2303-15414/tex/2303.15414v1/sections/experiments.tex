\section{Experiments on MOT task}

\subsection{Datasets and Evaluation Metrics}
%We carry out all experiments on MOTChallenge benchmark, which is a challenging multiple pedestrian tracking dateset. MOTChallenge benchmark includes four main datasets, 2DMOT15~\cite{leal2015motchallenge}, MOT16~\cite{milan2016mot16}, MOT17~\cite{milan2016mot16} and MOT20~\cite{dendorfer2020mot20}. 
We carry out the experiments on MOT16~\cite{milan2016mot16} and MOT17~\cite{milan2016mot16} benchmark. The videos in this benchmark were taken under various scenes, light conditions and frame rates. Occlusion, motion blur, camera motion and distant pedestrians are also crucial problems in this benchmark. Among all the evaluation metrics, Multiple Object Tracking Accuracy (MOTA)~\cite{kasturi2008framework} and ID F1 Score (IDF1)~\cite{ristanieccvw2016} are the most general metrics in the MOT task. Since MOTA is mostly dominated by the detection metrics false positive and false negative, and our graphing matching method mainly tries to tackle the associations between detected objects, we pay more attention to IDF1 than the MOTA metric. Moreover, a newly proposed metric Higher Order Tracking Accuracy (HOTA)~\cite{luiten21hota}, emphasizing the balance of object detection and association, becomes one of the official metrics of MOT benchmarks.\\ %We report our results on MOT16 and MOT17.

\input{tables/big-ablation.tex}
\subsection{Implementation Details}
\noindent\textbf{Training.}
Following other MOT methods \cite{braso2020learning,hornakova2020lifted}, we adopt Tracktor~\cite{bergmann2019tracking} to refine the public detections. We use a ResNet50 \cite{he2016deep} backbone followed by a global average pooling layer and a fully connected layer with 512 channels, as the ReID network used for feature extraction. The output ReID features are further normalized with the $l_2$ normalization. We pre-train the ReID network on Market1501~\cite{market_dataset}, DukeMTMC~\cite{ristanieccvw2016} and CUHK03~\cite{cuhk03_dataset} datasets jointly. The parameters of the ReID network will be frozen after pre-training. Then we add two trainable fully connected layers with 512 channels to get appearance features. All the ReID network training settings follow MPNTrack~\cite{braso2020learning}.
Our implementation is based on PyTorch~\cite{paszke2019pytorch} framework. We train our model on an NVIDIA RTX 2080Ti GPU. Adam~\cite{kingma2014adam} optimizer is applied with $\beta_1=0.9$ and $\beta_2=0.999$. The learning rate is  5$\times$10$^{-5}$ and weight decay is 10$^{-5}$. The temperature $\tau$ in Eq. \ref{eq:softmax} is 10$^{-3}$. 

\noindent\textbf{Inference.}
Our inference pipeline mostly follows DeepSORT \cite{wojke2017simple}, except that we use general graphing matching instead of bipartite matching for the association. As in DeepSORT, we set the motion gate $\kappa$ as 9.4877, which is at the 0.95 confidence of the inverse $\chi^2$ distribution. The feature similarity threshold $\sigma$ is set to 0.6 in the videos taken by the moving camera, and 0.7 when we use geometric information in the cross-graph GCN module for videos taken by the static camera. The \emph{max age} $\delta$ is 100 frames.
\par
\noindent\textbf{Post-processing.} To compare with other state-of-the-art offline methods, we perform a linear interpolation within the tracklet as post-processing to compensate for the missing detections, following \cite{braso2020learning,hornakova2020lifted}. This effectively reduces the false negatives introduced by upstream object detection algorithm.\par
% In ablation study, we compare our proposed components under the experiment settings both with and without the interpolation. In Table \ref{tab:mot}, we show both to compare with other state-of-the-art methods separately.
% \input{tables/big-ablation.tex}

\subsection{Ablation Study}
\label{sec:ablation}

\input{tables/tab-rmgmlayer.tex}
\input{tables/tracklet-abl.tex}
\input{tables/maxage-abl.tex}
% \input{tables/w&wo-ecc.tex}
We conduct ablation studies of the proposed components in our method on the MOT17 dataset. Following \cite{braso2020learning}, we divide the training set into three parts for three-fold cross-validation, called MOT17 \emph{val} set, and we conduct the experiments under this setting both in the ablation study section and the discussions section. We ablate each component we propose: (i) graph matching module built as a QP layer (GM); (ii) MLP trained on MOT dataset to refine the appearance features (App. Enc.); (iii) the cross-graph GCN module (GCN) with and without using geometric information (Geo); (iv) the linear interpolation method between the same object by the time (Inter.).

As shown in Table \ref{tbl-ablation}, compared with the DeepSORT baseline (the first row), which associates the detections and the tracklets based on Hungarian Algorithm, our method without training gets a gain of 1.9 IDF1, and a gain of 2.7 IDF1 and 1.1 MOTA with the linear interpolation. The results show the effectiveness of the second-order information in the graph. 
% Because DeepSORT filters the unconfirmed tracklets and keeps the unmatched tracklets for one more frame, it reduces ID switch at the cost of increasing FP and FN. After adopting linear interpolation, the number of ID Switch of our method reduces substantially. However, ID Switch of DeepSORT barely keeps unchanged.

Appearance feature refinement and GCN improve about 0.6 IDF1 compared to the untrained model. Geometric information provides about 1.0 additional gain on IDF1, which highlights the importance of geometric information in the MOT task. Finally, compared with the baseline, our method achieves about 3.4 and 0.2 improvements on IDF1 metric and MOTA metric, respectively. With interpolation, the gain becomes even larger: about 4.1 improvements on IDF1 and 0.9 on MOTA.

Table \ref{tab-rmgmlayer} shows the effectiveness of our differentiable graph matching layer the importance of training all components in our tracker jointly. We get the gain of 1.3 and 2.0 IDF1 compared with only removing the graph matching layer in training stage and in both training and inference stage, respectively. 
\begin{figure}
    \includegraphics[width=\columnwidth]{figures/thr.pdf}
    \caption{Results on IDF1, FP, FN and ID Switch metrics under different threshold $\sigma$ of the feature similarity to create a new tracklet.}
    \vspace{-0.2cm}
    \label{fig:thr}
    \vspace{-3pt}
\end{figure}
\input{tables/oracle.tex}

\subsection{Discussions}
In this part, we discuss two main design choices of our method on the MOT17 \emph{val} set. When we construct the tracklet graph, there are some different intra-tracklet feature aggregation methods. Moreover, how to create and delete a tracklet is important for an online tracker. Besides, the oracle experiment shows the upper bound performance of our learnable graph matching method.

\noindent{\bf Intra-tracklet feature aggregation.} In the tracklet graph $\MCG_T$, each vertex represents a tracklet. And the vertex feature $\mathbf{a}_T^{j}$ is the aggregation of the appearance features of all detections in tracklet $T_j$. Here, we compare several aggregation methods, including mean, moving average and only using the last frame of the tracklet. The results are shown in Table \ref{tbl:traagg}. The IDF1 is 7.2 lower when only using the last frame of the tracklet. The results also reveal that when we utilize all the frame information, no matter using the simple average or the moving average, their impact is not significant. To make our method simple and effective, we finally use the simple average method to aggregate the appearance features within a tracklet.


\noindent{\bf Tracklet born and death strategies.} 
In most of the online tracking methods, one of the core strategies is how to create and delete a tracklet. In our GMTracker, we mostly follow DeepSORT, but we also make some improvements to make these strategies more suitable for our approach, as described in Section \ref{sec:tarcker}.
 Among the three criteria to create a new tracklet, we find that the threshold $\sigma$ is the most sensitive hyperparameter in our method. We conduct experiments with different $\sigma$, and its influence on IDF1, FP, FN and ID Switch is shown in Fig.~\ref{fig:thr}. 
% With a higher $\sigma$, the number of false positive (FP) will decrease and the false negative (FN) will increase. However, the IDF1 and ID Switch have 
As for removing a trajectory from association candidates, our basic strategy is that if the tracklet has not been associated with any detections in $\delta$ frames, the tracklet will be removed and not be matched anymore.\par
Table \ref{tbl-ablation2} shows in our method, that larger \emph{max age} $\delta$, which means more tracklet candidates, yields a better IDF1 score. It shows the effectiveness of our method from another aspect that our GMTracker can successfully match the tracklets that disappeared about five seconds ago. On the contrary, when the \emph{max age} increases to 150 frames, the IDF1 will drop 0.8 using DeepSORT, which indicates our method can deal with long-term tracklet associations better.\par %The results show that there is an obvious gap between Hungarian algorithm and our graph matching when the \emph{max age} increases. 
\noindent{\bf Comparison with the Oracle Tracker.}
To explore the upper bound of the association method, we compare our method with the ground truth association, called the Oracle tracker. The results on MOT17 \emph{val} set are shown in Table \ref{tbl-oracle}. There is a gap of 5.7 IDF1 and about 1000 ID Switches between our online GMTracker and the Oracle tracker. Another observation is that on some metrics, which are extremely relevant to detection results, like MOTA, FP and FN, the gaps between the baseline, our method and the Oracle tracker are relatively small. That is why we mainly concern with the metrics reflecting the association results, such as IDF1 and ID Switch.
\begin{figure*}[!h]
    \centering
    \subfloat[Detection misses and resurfaces by occlusion.]{
    \begin{minipage}[]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/track1.pdf}\\
    \label{fig:tracking1}
    \end{minipage}%
    }%
    \subfloat[Two objects exchange their locations.]{
    \begin{minipage}[]{0.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figures/track2.pdf}\\
    \label{fig:tracking2}
    \end{minipage}%
    }%
    \caption{Examples of tracking results on MOT17 dataset. The top line is from \emph{DeepSORT}, and the bottom is from \emph{GMTracker}. (a) and (b) are two typical hard cases, in which our method is better than the baseline \emph{DeepSORT}.}
    \label{fig:tracking}
\end{figure*}
% \begin{figure}[!h]
%     \centering
%     \subfloat[]{
%     \begin{minipage}[]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \vspace{10pt}
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \end{minipage}%
%     }%
%     \subfloat[]{
%     \begin{minipage}[]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \vspace{10pt}
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \end{minipage}%
%     }\\

%     \subfloat[]{
%     \begin{minipage}[]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \vspace{10pt}
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \end{minipage}%
%     }%
%     \subfloat[]{
%     \begin{minipage}[]{0.48\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \vspace{10pt}
%     \includegraphics[width=\linewidth]{figures/scannet/scene0711_00_frame-001680_scene0711_00_frame-001995_evaluation.pdf}\\
%     \end{minipage}%
%     }%
%     \caption{Qualitative results}
%     \label{fig:imagematching}
% \end{figure}
\subsection{Inference Time}
We compare the running speed between our new GST algorithm and the original quadratic programming solved by the CVXPY library, as shown in Table~\ref{tbl-speed}. Using the new GST algorithm, with the performance under main metrics almost the same, the speed is about 21$\times$ faster than the original GMTracker, and the solver running time is two orders of magnitude lower than before. Note that the ID switches are much fewer because the matching candidates are filtered before solving the quadratic assignment problem, and the phenomenon of early termination of the tracklet is somewhat alleviated.\par
\input{tables/inference_speed.tex}
\input{tables/superglue.tex}
\input{tables/sota.tex}
\subsection{Comparison with State-of-the-Art Methods}
We compare our GMTracker with other state-of-the-art methods on MOT16 and MOT17 test sets. As shown in Table \ref{tab:mot}, when we apply Tracktor~\cite{bergmann2019tracking} to refine the public detection, the \emph{online} \gm\  achieves 63.8 IDF1 on MOT17 and 63.9 IDF1 on MOT16, outperforming the other online trackers. To compare with CenterTrack~\cite{zhou2020tracking}, we use the same detections, called GMT\_CT, and the IDF1 is 66.9 on MOT17 and 68.6 on MOT16.
% compared with other \emph{online} methods, we improve the state-of-the-art on the IDF1 metric by 6.0 and 5.7 on MOT17 and MOT16 datasets, respectively. 
With the simple linear interpolation, called GMT\_simInt in Table \ref{tab:mot}, we also outperform the other \emph{offline} state-of-the-art trackers on IDF1. With exactly the same visual inter- and extrapolation as LifT~\cite{hornakova2020lifted}, called GMT\_VIVE in Table~\ref{tab:mot},\ the MOTA is comparable with LifT. After utilizing the CenterTrack detections and linear interpolation, the GMTCT\_simInt improves the SOTA on both MOT16 and MOT17 datasets.
%  The online tracker  refines the public detections based on a better detector CenterNet~\cite{zhou2019objects}. To compare with it, we use the same detections as CenterTrack, called GMT\_CT in Table~\ref{tab:mot}, and we improve the IDF1 by 7.3 on the MOT17 test set. CenterTrack does not report the results on the MOT16 test set. But compared with other methods, the results on MOT16 test set also show the effectiveness of our method. 
%For a better performance, we can also derive our detection proposals from CenterTrack~\cite{zhou2020tracking} or add visual interpolation and extrapolation like \cite{hornakova2020lifted}. 

\subsection{Qualitative results on MOT17}
In Fig.~\ref{fig:tracking}, we show the hard cases, in which the baseline tracker \emph{DeepSORT} has ID switches and our \emph{GMTracker} tracks the objects with the right IDs. For example, in Fig.~\ref{fig:tracking1}, DeepSORT fails to track the person with ID-2 and creates a new tracklet ID-38, because the person is occluded by the streetlight and reappears. And in Fig.~\ref{fig:tracking2}, the people with ID-19 and ID-21 exchange their places. DeepSORT can not keep the IDs. The ID-19 drifts to the person with ID-21 and a new ID is assigned to the person with ID-21. However, our GMTracker tracks the objects with right IDs.

\section{Experiments on Image Matching task}
\subsection{Datasets and Evaluation Metrics}
We do the experiments about the Image Matching task on the mainstream indoor camera pose estimation dataset ScanNet~\cite{dai2017scannet}. ScanNet is a large-scale indoor dataset collected using RGB-D cameras. Because it is an indoor dataset, local patterns are always similar in a large area, such as on the white wall and ceramic tile. In image matching task, the general evaluation metrics focus on the camera pose estimation performance, such as the Area Under Curve (AUC) of the pose error with thresholds at $5^{\circ},10^{\circ},20^{\circ}$. Here, the pose error takes the maximum angle error of the rotation matrices and the transformation vectors. Besides, the match precision and the matching score are calculated, mainly reflecting the keypoint matching performance.
\subsection{Implementation Details}
 We take state-of-the-art keypoint-based image matching pipeline SuperGlue~\cite{sarlin2020superglue} as our baseline. Our implementation uses PyTorch~\cite{paszke2019pytorch} framework, and we train our model on 24 NVIDIA RTX 2080Ti GPUs. Adam~\cite{kingma2014adam} optimizer is applied, and the base learning rate is $4\times10^{-5}$ with Cosine Annealing as the learning rate scheduler. Like SuperGlue, we only sample the image pairs with an overlap in $[0.4, 0.8]$ for training. The images and the depth maps are resized to the resolution of $640\times480$. The batch size is 24 and train the model for 1.2M iterations. In the training stage, only 200 pairs are sampled randomly for each scene in an epoch. During the inference stage, the mutual constraint and the matching score threshold are adopted to filter the mismatch and the threshold is set to 0.2.
Other experiment settings not written out here are the same as SuperGlue. 
\subsection{Comparisons with SOTA keypoint-based methods}
In Table~\ref{tbl-sgsota}, we show the comparison with other SOTA methods using the same SuperPoint~\cite{detone2018superpoint} keypoint detector on ScanNet test set, which contains 1500 image pairs from \cite{sarlin2020superglue}. Note that we use about half training data and training iterations to outperform SOTA method SuperGlue~\cite{sarlin2020superglue} by 1 pose estimation AUC.


% \noindent{\bf Qualitative results.}\\

% \begin{figure}
% \begin{minipage}
%     \includegraphics[width=0.5\linewidth]{example-image-a}\qquad
% \includegraphics[width=0.5\linewidth]{example-image-a}\qquad
% \end{minipage}
% \end{figure}
% \includegraphics[width=0.5\linewidth]{example-image-a}\qquad
% \includegraphics[width=0.5\linewidth]{example-image-a}\qquad
\begin{comment}
Furthermore, our differentiable graph matching module can be easily adopted in other classical or deep learning methods to replace the traditional bipartite matching and improve their performance. 
\vspace{-5pt}
\end{comment}
\section{Conclusion}

In this paper, we propose a novel learnable graph matching method for data association. Our graph matching method focuses on the pairwise relationship within the view. To make the graph matching module end-to-end differentiable, we relax the QAP formulation into a convex QP and build a differentiable graph matching layer in our Graph Matching Network. We apply our method to the Multiple Object Tracking task, called GMTracker. Taking the second-order edge-to-edge similarity into account, our tracker is more accurate and robust in the MOT task. To speed up our algorithm, we design GST to shrink the area of the feasible region. The experiments of the ablation study and comparison with other state-of-the-art methods both show the efficiency and effectiveness of our method. Moreover, for the Image Matching task, we propose the end-to-end learnable graph matching algorithm based on the SOTA method SuperGlue, called GMatcher, which shows our ability to solve data association tasks generally. The experiments show that we only use half training data and training iterations to outperform SuperGlue by about 1 AUC.
% \section*{Acknowledgements}
% \noindent{This work was supported in part by the National Key R\&D Program of China(No. 2018YFB1004602), the National Natural Science Foundation of China (No. 61836014, No. 61773375). The authors would like to thank Roberto Henschel for running their post-processing code for us.}