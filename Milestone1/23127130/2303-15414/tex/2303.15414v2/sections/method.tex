\section{Graph Matching Network and GMTracker}
In this section, we will describe the details of our Graph Matching Network and our {\gm}. As shown in Fig.~\ref{pipeline}, the pipeline of our Graph Matching Network consists of three parts: (1) feature encoding in detection and tracklet graphs; (2) feature enhancement by cross-graph Graph Convolutional Network (GCN) and (3) differentiable graph matching layer. We will describe these three parts step by step and show how we integrate them into a tracker ({\gm}) in the following.

\subsection{Feature Encoding in Two Graphs}

We utilize a pre-trained ReIDentification (ReID) network followed by a multi-layer perceptron (MLP) to generate the appearance feature $\mathbf{a}_D^{i}$ for each detection $D_i$.
The appearance feature $\mathbf{a}_T^{j}$ of the tracklet $T_{j}$ is obtained by averaging all the appearance features of detections before.\par

\subsection{Cross-Graph GCN}
\label{sec:gcn}
 Similar to \cite{braso2020learning, ma2019deep, Weng2020_GNN3DMOT}, we only adopt a GCN module between the graph $\MCG_D$ and graph $\MCG_T$ to enhance the feature, and thus it is called Cross-Graph GCN. 

 The initial vertex features on detection graph and tracklet graph are the appearance features on the vertices, i.e., let $\Mh_i^{(0)}=\mathbf{a}_D^{i}$ and $\Mh_j^{(0)}=\mathbf{a}_T^{j}$.
Let $\Mh_i^{(l)}$ and $\Mh_j^{(l)}$ be the feature of vertex $i \in \MCG_D$ and vertex $j \in \MCG_T$ in the $l$-th propagation, respectively. 
We define the aggregation weight coefficient $w_{i, j}^{(l)}$ in GCN as the appearance and geometric similarity between vertex $i$ and vertex $j$:
\begin{equation}
w_{i,j}^{(l)} = \cos(\Mh_i^{(l)}, \Mh_j^{(l)}) + {\rm{IoU}}(\mathbf{g}_i, \mathbf{g}_j)
\end{equation}
where $\cos(\cdot, \cdot)$ means the cosine similarity between input features and $\rm{IoU(\cdot, \cdot)}$ denotes the Intersection over Union of two bounding boxes. For a detection vertex $i$, $\mathbf{g}_i$ is the corresponding detection bounding box defined in Section \ref{sec:construct}. As for a tracklet vertex $j$, we estimate the bounding box $\mathbf{g}_j$ in current frame $t$ by Kalman Filter \cite{kalman1960new} motion model with a constant velocity. Note that we only consider the appearance feature similarity in weight $w_{i,j}$ when the camera moves, since the motion model cannot predict reliable future positions in these complicated scenes.

We use summation as the aggregation function, i.e., $\Mm_{i}^{(l)} = \sum_{j \in \MCG_T} w_{i,j}^{(l)}\Mh_j^{(l)}$ and the vertex features are updated by:
\begin{equation}
\Mh_i^{(l + 1)} = {\rm{MLP}}(\Mh_i^{(l)} + \frac{\norm{\Mh_i^{(l)}}_2\Mm_{i}^{(l)}}{\norm{\Mm_{i}^{(l)}}_2}),
\end{equation}
where we adopt message normalization proposed in \cite{li2020deepergcn} to stabilize the training.

We apply $l_2$ normalization to the final features after cross-graph GCN and denote it as $\Mh_i$. Then we use $\Mh_i$ as the feature of vertex $i$ in graph $\MCG_D$, and construct the edge feature for edge $(i, i')$ with $\Mh_{i,i'} = l_2([\Mh_i, \Mh_{i'}])$, where $[\cdot]$ denotes concatenation operation. The similar operation is also applied to the tracklet graph $\MCG_T$. In our implementation, we only apply GCN once.

\subsection{Differentiable Graph Matching Layer}
\label{sec:diffgm}
After enhancing the vertex features and constructing the edge features on graph $\MCG_D$ and $\MCG_T$, we meet the core component of our method: the differentiable graph matching layer. By optimizing the QP in Eq.~\ref{finalQP} from quadratic affinity matrix $\mathbf{M}$ and vertex affinity matrix $\mathbf{B}$, we can derive the optimal matching score vector $\mathbf{x}$ and reshape it back to the shape $n_d \times n_t$ to get the matching score map $\mathbf{X}$. 


Since we finally formulate the graph matching problem as a QP, we can construct the graph matching module as a differentiable QP layer in our neural network. Since KKT conditions are the necessary and sufficient conditions for the optimal solution $\mathbf{x}^*$ and its dual variables, we could derive the gradient in backward pass of our graph matching layer based on the KKT conditions and implicit function theorem, which is inspired by OptNet~\cite{amos2017optnet}.
In our implementation, we adopt the qpth library~\cite{amos2017optnet} to build the graph matching module.
In the inference stage, to reduce the computational cost and accelerate the algorithm, we solve the QP using the CVXPY library~\cite{diamond2016cvxpy} only for forward operation.\par
For training, we use weighted binary cross entropy Loss: 
\begin{equation}
  \begin{aligned}
    \mathcal{L}=\frac{-1}{n_dn_t}\sum_{i=1}^{n_d}\sum_{j=1}^{n_t}\ ky_{i,j}\log(\hat{y}_{i,j})+(1-y_{i,j})\log(1-\hat{y}_{i,j}),
\end{aligned}
\end{equation}
where $\hat y_{i,j}$ denotes the matching score between detection $D_i$ and tracklet $T_j$, and $y_{i,j}$ is the ground truth indicating whether the object belongs to the tracklet. $k=(n_t-1)$ is the weight to balance the loss between positive and negative samples. Besides, due to our QP formulation of graph matching, the distribution of matching score map $\mathbf{X}$ is relatively smooth. We adopt softmax function with temperature $\tau$ to sharpen the distribution of scores before calculating the loss:
\begin{equation}
\hat{y}_{i,j} = {\rm Softmax}(x_{i,j},\tau)=\frac{e^{x_{i,j}/\tau}}{\sum_{j=1}^{n_t}e^{x_{i,j}/\tau}},
\label{eq:softmax}
\end{equation}
where $x_{i,j}$ is the original matching score in score map $\mathbf{X}$.
