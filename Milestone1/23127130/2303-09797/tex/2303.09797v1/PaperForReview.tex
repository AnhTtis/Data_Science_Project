% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{upgreek}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2303} % *** Enter the CVPR Paper ID here
\def\confName{ICCV}
\def\confYear{2023}


\begin{document}


%%%%%%%%% TITLE - PLEASE UPDATE
\title{MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for\\ Audio-Driven 3D Face Animation}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{
Haozhe Wu$^1$, 
Jia Jia$^1$, 
Junliang Xing$^1$, 
Hongwei Xu$^2$, 
Xiangyuan Wang$^2$, 
Jelo Wang$^2$\\
$^1$Department of Computer Science and Technology, Tsinghua University \quad $^2$FACEGOOD Inc.\\
{\tt\small wuhz19@mails.tsinghua.edu.cn} \quad {\tt\small \{jjia,jlxing\}@tsinghua.edu.cn}\\
}

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=\textwidth]{intro.pdf}
    \caption{An overview of the MMFace4D dataset. %
    We demonstrate (1) the capture setup and the recording environment, (2) one piece of the MMFace4D data, which contains multi-view RGB videos, multi-view depth videos, reconstructed meshes, and synchronized speech audio, (3) the diversified actors in the MMFace4D dataset. Note that all RGB facial images are mosaicked for privacy protection. }
    \label{fig:intro}  
\end{center}%
}]

% 论文实验部分的组织: (1) 主要侧重数据集的比较，侧重evaluation的提出，针对evaluation找各种极端的例子，然后用简单的baseline验证evaluation的有效性，避免复杂baseline的比较。可以移除掉condition，或者缩小网络层数，以此来验证benchmark。
% baseline部分: 验证resnet50，是否加dense template，验证resnet18，以及lstm等多个框架的效果
% 要缩写重建部分，加一部分metric的描述
% 要比较数据集质量，从多样性、variance的角度比较
% 与单目RGB比较

%%%%%%%%% ABSTRACT
\begin{abstract}
    % Audio-Driven Face Animation is an eagerly anticipated technique for applications such as VR/AR, games, and movie making. %
    % Especially with the rapid development of 3D engines, there is an increasing demand for driving 3D faces with audio. %
    % However, the currently available 3D face animation datasets are either scale-limited or quality-unsatisfied, which hampers further developments of audio-driven 3D face animation. %
    % To address this challenge, we propose \textbf{MMFace4D}, a large-scale multi-modal 4D (3D sequence) face dataset consisting of 431 identities, 35,904 sequences, and 3.9 million frames. %
    % MMFace4D has three appealing characteristics: (1) the dataset contains highly diversified subjects and corpus, %
    % (2) each sequence of the dataset records synchronized audio and keeps 3D mesh sequence with high-resolution face details. %
    % (3) the dataset has low storage cost with our efficient lossless compression algorithm on 3D mesh sequences. %
    % These characteristics enable the training of high-fidelity, expressive, and generalizable face animation models. %
    % In experiments, we build a challenging benchmark of audio-driven 3D face animation. %
    % Besides, we release a baseline that enables non-autoregressive generation with fast inference speed and outperforms state-of-the-art autoregressive methods. %
    % The code, model, and dataset will be publicly available. %
    % % Together with the dataset, we present reliable evaluation metrics to determine the audio-mesh synchronization and the realism of mesh sequences. %
    
    Audio-Driven Face Animation is an eagerly anticipated technique for applications such as VR/AR, games, and movie making. %
    With the rapid development of 3D engines, there is an increasing demand for driving 3D faces with audio. %
    However, currently available 3D face animation datasets are either scale-limited or quality-unsatisfied, which hampers further developments of audio-driven 3D face animation. %
    To address this challenge, we propose \textbf{MMFace4D}, a large-scale multi-modal 4D (3D sequence) face dataset consisting of 431 identities, 35,904 sequences, and 3.9 million frames. %
    MMFace4D has three appealing characteristics: 1) highly diversified subjects and corpus, 2) synchronized audio and 3D mesh sequence with high-resolution face details, %
    and 3) low storage cost with a new efficient compression algorithm on 3D mesh sequences. %
    These characteristics enable the training of high-fidelity, expressive, and generalizable face animation models. %
    Upon MMFace4D, we construct a challenging benchmark of audio-driven 3D face animation with a strong baseline, which enables non-autoregressive generation with fast inference speed and outperforms the state-of-the-art autoregressive method. %
    The whole benchmark will be released. %
    % Together with the dataset, we present reliable evaluation metrics to determine the audio-mesh synchronization and the realism of mesh sequences. %
    
   % 先说face animation的重要
   % Especially，随着各类3D引擎的发展，there is an increasing demand on driving 3D faces with audio. 
   % 然而，目前的3D数据集要么规模太小，要么质量并不令人满意。
   % 为了解决这个挑战，我们提出了一个数据集consists of xxx
   % 我们的数据集有三大特点:(1) (2) each sequence keeps high-fidelity facial animation and synchronized audio  (3) Low storage cost
   % 数据集的作用
\end{abstract}

% 论文intro大致这样写:
% (1) audio2face的重要意义，目前有很多2D数据集和工作，但是在实际应用中，3D数据集更需要，我们提出3D
% (2) 之前的3D数据集
% (3) 为了解决上述问题，我们提出了一个新的数据集。
% (4) 基于该数据集，我们同时提出了一个解算方案，压缩方案，(后续还可以写评测指标，该数据集对audio2face任务的提升)
% 第二段: related work: 2D和3D audio2face数据集，并且回顾一些人脸重建，以及audio2face的工作
% 第三段: 数据如何采集，语料如何设计(要列一个和之前数据集对比的表格)
% 第四段: 如何解算为拓扑一致的视频
% 第五段: 结算后的数据压缩
% 第六段: 一些实验、数据的可视化、一些统计
% 精细的: High-Fidelity 
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
% The Audio-Driven Face Animation is an eagerly anticipated technique for several applications such as VR/AR, teleconference, games, and movie making.
The Audio-Driven Face Animation is a fundamental problem in the area of digital avatar synthesis, %
which is essential for several applications, such as VR/AR, games, and movie-making. %
Recently, significant progress has been made with the advent of deep learning and the release of multi-modal face animation datasets. %
The majority of prior works aimed to produce 2D talking head videos due to the availability of massive audio-visual datasets~\cite{Chung16, afouras2018deep, Nagrani17, Afouras18d, Chung18b, shillingford2018large, yang2019lrw, rossler2019faceforensics++, wang2020mead, zhang2021flow}. %
Whereas, researches in 3D face animation~\cite{richard2021meshtalk, cudeiro2019capture,fan2022faceformer,xing2023codetalker}, which are more closely related to 3D applications like games and movie making, are still in infancy because of the size or quality constraints of currently available datasets. %
Some 3D face animation datasets are constructed with powerful devices~\cite{cudeiro2019capture} and thus have high-fidelity. %
However, the sizes of these datasets are limited. %
Meanwhile, some 3D face animation datasets are constructed on a large scale with monocular 3D reconstruction method~\cite{deng2019accurate}. %
Nevertheless, these datasets have limited quality because the 3D faces are represented as low-dimensional 3DMM coefficients~\cite{blanz1999morphable}. %
The aforementioned constraints of these datasets impose a hurdle for training generalizable and realistic 3D face animation models. %

To address these issues, we present \textbf{MMFace4D}, a large-scale multi-modal 4D~(3D sequence) face dataset, which captures high-fidelity 3D face sequences and synchronized audios for each actor. %
Figure~\ref{fig:intro} demonstrates the recording environment and one piece of the MMFace4D dataset. %
The MMFace4D dataset comprises 431 identities, 35,904 sequences, and 3.9 million frames. %
More importantly,  MMFace4D features three main properties listed below:

\textbf{- Diversified Subjects and Corpus}. For subjects, the MMFace4D dataset covers diversified actors with median age of 28, minimum age of 15, and maximum age of 68. %
For corpus, we build a corpus with 11,000 sentences under different scenarios such as news broadcasting, conversation, and storytelling. %
The corpus contains 2000 neutral sentences and 9000 emotional sentences of six basic emotional types~(angry, disgust, happy, fear, sad, surprise). %

\textbf{- High-fidelity Animation and Synchronized Audio}. 
We capture the RGB-D videos of facial movements with three synchronized RGB-D cameras from different views. %
Based on the captured RGB-D videos, we develop a reconstruction pipeline to fuse the multi-view RGB-D videos and obtain topology-uniformed 3D mesh sequences. %
Compared with the 3DMM-based reconstruction methods~\cite{li2017learning,yang2020facescape,blanz1999morphable,liu2021beat}, our pipeline keeps more accurate facial details due to the design of multi-stage fitting and vertex-level optimization. %
%, while 3DMM-based methods only leverage low-dimensional 3DMM coefficients to represent 3D face deformation. 
Besides, for each 3D mesh sequence, we capture synchronized speech audio with a directional microphone. 
% Our pipeline optimizes vertex-level movements, and thus  compared with the 

% It is noteworthy emphasizing that our reconstructed mesh keeps more accurate facial movements compared with the 3DMM based methods~\cite{li2017learning,yang2020facescape,blanz1999morphable,liu2021beat}, which is because each vertex of our mesh is assigned with its own degree of freedom for deformation, while 3DMM based methods only leverage low-dimensional 3DMM coefficients for 3D face deformation. %
% During the recording process, we ask each actor to practice several times and read out each sentence fluently and expressively. %

% 在这里列一个表格，比较MM4D和之前的数据集
% 加上存储效率的比较
% \begin{table*}[hbt!]
%     \centering
%     \caption{Comparison of high-resolution and topology-uniformed 4D (3D sequence) face datasets, of which each sequence has synchronized speech audio. %
%     Note that only the publicly available datasets are compared as follows.
%     MMFace4D dataset has a competitive scale in terms of subject number (\#Subj), corpus scale (\#Corp), sequence number (\#Seq), and duration (\#Dura). Additionally, the frame per second (FPS), the emotion label (Emo), and the spoken language (Lang) are also listed. }
%     \begin{tabular}{c|cccc|ccc}
%     \hline
%     \multirow{2}{*}{Dataset} & \multicolumn{4}{c|}{Scale}       & \multicolumn{3}{c}{Property}                \\ \cline{2-8} 
%                              & \#Subj & \#Corp & \#Seq & \#Dura & \#FPS & Emo                       & Lang    \\ \hline
%     BIWI~\cite{fanelli20103}                     & 14     & 40     & 1109  & 1.44h  & 25    & \checkmark & English \\
%     VOCASET~\cite{cudeiro2019capture}                  & 12     & 40     & 480   & 0.5h   & 60    & -                         & English \\
%     MeshTalk~\cite{richard2021meshtalk}                 & 250    & 50     & 12500 & 13h    & 30    & -                         & English \\ \hline
%     \textbf{MMFace4D}                     & 431    & 11000  & 35,904 & 36h    & 30    & \checkmark & Chinese
%     \end{tabular}
%     \label{tab:data_compare}
% \end{table*}

\textbf{- Low Storage Cost}. 3D mesh sequences are storage-consuming. %
In our instance, each mesh has 35,709 vertices. %
Consequently, storing one mesh sequence of 30 fps takes 12MB per second, which puts a heavy burden on storage. %
To solve this problem, we propose an efficient mesh sequence compression algorithm based on vertex quantization and video codec~\cite{tomar2006converting}. %
Our algorithm achieves a compression ratio of 0.29, which enables large-scale data collection. %


We conduct extensive observations on the MMFace4D dataset, verifying that our dataset has various talking styles, expressive facial motions, and diversified actors. %
These characteristics empower the training of high-fidelity, expressive, and generalizable face animation models. %
% Based on the MMFace4D dataset, we re-examine the L2-error-based evaluation protocol of audio-driven 3D face animation and further present reliable evaluation metrics to determine the audio-mesh synchronization and the realism of mesh sequences. %
Based on the MMFace4D, we construct a challenging benchmark of audio-driven 3D face animation with a simple but effective baseline method. %
% Furthermore, we propose a simple but effective baseline method to synthesize audio-driven 3D faces. %
The baseline method is built on a deep ResNet encoder~\cite{he2016deep} with 1D convolution, which enables non-autoregressive generation with fast inference speed and outperforms the state-of-the-art autoregressive method~\cite{fan2022faceformer}.

% 这个数据集带来的好处: (1) 大规模给更大模型的训练带来了可能，(2) 多人的泛化能力，(3) 尽管是中文，但是已有的方法用我们数据pretrained的模型，在其它语言上tune后有更好的效果.
% Extensive experiments demonstrate that our MMFace4D dataset facilitates audio-driven 3D face animation significantly. %
% Firstly, tens of thousands of animation sequences enable to train deep learning model with more parameters, leading to more realistic animation results. %
% Moreover, hundreds of subjects in the MMFace4D dataset endow better generalizability on unseen person. %
% Furthermore, despite that the MMFace4D dataset is collected with Chinese corpus, popular methods~\cite{fan2022faceformer,richard2021meshtalk} pretrained on our dataset still enhance their performances on other languages with tuning processes. %


\section{Related Work}

\textbf{Audio-Driven Face Animation.} Audio-Driven face animation has received significant attention in previous
literature. %
Related work in this area can be grouped into two categories: the 2D-based approaches~\cite{chen2018lip,das2020speech,fan2015photo,ji2021audio,prajwal2020lip,vougioukas2020realistic,zhou2019talking,10.1145/3528233.3530745,sinha2022emotion} and the 3D-based approaches~\cite{wu2006real,edwards2016jali,taylor2012dynamic,fan2022faceformer,richard2021meshtalk,cudeiro2019capture,liu2021geometry,wu2021imitating,guo2021ad}. %

With the availability of large-scale audio-visual datasets, 2D-based approaches have been paid much attention. %
These methods usually leverage optical flow~\cite{10.1145/3528233.3530745,sinha2022emotion}, landmarks~\cite{das2020speech,ji2021audio}, or disentangled representations~\cite{chen2018lip,zhou2019talking} to synthesize realistic audio-driven taking faces. %
The 2D videos generated by these methods have wide applications in movie dubbing, 2D games, and telecommunications. %
However, for 3D applications such as VR/AR, filmmaking, and 3D games, these approaches are less applicable. %

% 一对一
To animate 3D faces, some researchers~\cite {wu2006real,edwards2016jali,taylor2012dynamic} proposed rule-based methods to build a mapping between input audio and rigged 3D faces. %
For example, the JALI model~\cite{edwards2016jali} respectively builds the jaw model and lip model for face animation, the dynamic viseme model~\cite{taylor2012dynamic} captures visual coarticulation and the inherent asynchrony between visual and acoustic speech. %
These rule-based methods usually require intensive manual labor to achieve realistic animation. %
To alleviate the requirement of manual labor, several data-driven methods have been proposed~\cite{fan2022faceformer,richard2021meshtalk,cudeiro2019capture,liu2021geometry,wu2021imitating,guo2021ad,lahiri2021lipsync3d,yao2022dfa,karras2017audio,liu2015video,taylor2017deep}. %
Some methods use monocular 3D face reconstruction to synthesize 3D faces from video data~\cite{wu2021imitating,lahiri2021lipsync3d,liu2015video}. These methods generalize well across different subjects, %
whereas the animation quality of these methods is limited due to the constraints of monocular 3D reconstruction. %
The AD-NeRF and DFA-NeRF~\cite{guo2021ad,yao2022dfa} leverage neural radiance field~\cite{mildenhall2021nerf} to synthesize 3D face animations with granular control on pose and emotion. %
Nevertheless, the NeRF-based methods cannot generate topology-uniformed 3D meshes, which is inapplicable for several 3D applications. %
To synthesize highly-realistic and topology-uniformed 3D mesh sequences, %
researches have explored both speaker-independent ~\cite{xing2023codetalker,fan2022faceformer,richard2021meshtalk,cudeiro2019capture,liu2021geometry} and speaker-dependent~\cite{karras2017audio} frameworks. %
These methods require high-resolution 3D face animation datasets with diversified subjects and corpus. %
However, currently available datasets are insufficient in terms of either diversity or 3D resolution, %
which severely obstructs the training of generalizable and realistic 3D face animation models. %
Such urgent demand brings out our MMFace4D dataset. %

% These methods usually split the problem of synthesizing talking face video into two parts: synthesize facial motions synchronized with audio and generate photo-realistic videos. % 
% These type of methods resort to some 

\textbf{3D and 4D Face Datasets.}
Several 3D~\cite{cao2013facewarehouse,savran2008bosphorus,yin20063d,yang2020facescape,paysan20093d} and 4D face~\cite{zhang2014bp4d,alashkar20143d,cosker2011facs,zhang2013high,zhang2016multimodal,richard2021meshtalk,cudeiro2019capture,fanelli20103} datasets have been released to analyze static and dynamic facial expressions. %
These datasets record high-resolution 3D faces, %
which are widely applied in 3D face recognition, 3D face morphable model, 3D expression analysis and \textit{et al}. %

Meanwhile, only a few datasets capture synchronized audio with dynamic 3D faces. %
The BIWI dataset~\cite{fanelli20103}, VOCASET~\cite{cudeiro2019capture}, and the MeshTalk dataset~\cite{richard2021meshtalk} are mostly used to train audio-driven 3D face animation models. %
The BIWI dataset records 40 spoken English sentences for each of 14 subjects. %
The VOCASET records 29 minutes of 4D scans from 12 speakers. %
Both BIWI and VOCASET are small-scale, which limits the generalization capacity of 3D face animation models. %
The MeshTalk dataset collects 250 subjects, each of which reads 50 phonetically balanced sentences. %
However, the MeshTalk corpus has a limited size, and only a few parts of the MeshTalk dataset is publicly available. %
To address these issues, we collect MMFace4D with diversified subjects and abundant corpus. %

% However, the storage consuming nature of 3D mesh sequence makes it difficult to publish such large scale 3D mesh sequence dataset. %
% Generally, for one mesh sequence of 35000 vertexes, 30 fps and 5 seconds, the storage space takes around 750MB. %
% Therefore only 16 subjects of MeshTalk dataset is made publicly available. %
% To address this issue, we devise a highly efficient compression algorithm, %
% which enables us to collect and publish tens of thousands of animation sequences in the MMFace4D dataset. %

\begin{table}[]
\setlength\tabcolsep{2pt}
\centering
\caption{Comparison of high-resolution and topology-uniformed 4D (3D sequence) face datasets, of which each sequence has synchronized speech audio. %
Note that only the publicly available datasets are compared as follows~\cite{fanelli20103,cudeiro2019capture,richard2021meshtalk}. %
    MMFace4D dataset has a competitive scale in terms of subject number (\#Subj), corpus scale (\#Corp), sequence number (\#Seq), and duration (\#Dura). Additionally, the frame per second (FPS), the emotion label (Emo), and the spoken language (Lang) are also listed. }
\begin{tabular}{cc|ccc|c}
\hline
\multicolumn{2}{c|}{Dataset}                            & BIWI    & VOCA    & MeshTalk & \textbf{MMFace4D}    \\ \hline
\multicolumn{1}{c|}{\multirow{4}{*}{Scale}}    & \#Subj & 14      & 12      & 250      & 431     \\ 
\multicolumn{1}{c|}{}                          & \#Corp & 40      & 40      & 50       & 11,000   \\
\multicolumn{1}{c|}{}                          & \#Seq  & 1109    & 480     & 12,500    & 35,904   \\
\multicolumn{1}{c|}{}                          & \#Dura & 1.44h   & 0.5h    & 13h      & 36h     \\ \hline
\multicolumn{1}{c|}{\multirow{3}{*}{Property}} & \#FPS  & 25      & 60      & 30       & 30      \\ 
\multicolumn{1}{c|}{}                          & Emo    & \checkmark       & -       & -        & \checkmark       \\ 
\multicolumn{1}{c|}{}                          & Lang   & English & English & English  & Chinese \\ \hline
\end{tabular}
\label{tab:data_compare}
\end{table}

\section{Capture Setup}

We devise a capture system comprising three RGB-D cameras, one microphone, and one screen. %
Each RGB-D camera is placed at the height of 1.2 meters. %
One camera shoots at the front of the face, %
the other two cameras shoot at the face's left and right sides with 45 degrees of angle. %
All cameras are fixed on tripod heads to prevent jitters when recording. %
The microphone is placed under the camera without blocking the lens. %
The screen is placed between the side camera and the center camera. %
Figure~\ref{fig:intro} demonstrate our capture setup. %
During recording, each actor reads sentences displayed on the screen. %

\subsection{Sensors}

\textbf{RGB-D Camera.} We leverage Azure Kinect Camera\footnote{\url{azure.microsoft.com/en-us/products/kinect-dk/}} to capture RGB-D video. Azure Kinect is a popular consumer-level camera used by both academia and industry. %
We record RGB video with a resolution of $1920 \times 1080$, and record depth video with a resolution of $640 \times 576$. The depth video is recorded in narrow field-of-view depth mode. Both RGB video and depth video are captured at 30 fps. %

\textbf{Microphone.}  We leverage COMICA VM20 microphone\footnote{\url{www.comica-audio.com/product/CVM-VM20-82.html}} to record audio. %
Compared with the built-in microphone of Azure Kinect, the used microphone has a large signal gain, which enables recording at a long distance. %

\subsection{Calibration}

% 讲一下两阶段设计的理由:较为准确的icp-based的方法需要较好的初始化，直接用icp效果并不好，所以得先有一个粗粒度的估计，
To accurately align the RGB-D images of the three cameras,  we leverage the point-to-plane ICP~\cite{rusinkiewicz2001efficient}. %
However, the point-to-plane ICP algorithm requires an approximately good initialization of camera extrinsics. %
Therefore, we first conduct coarse calibration with facial landmarks, %
and then register the RGB-D images with point-to-plane ICP. %

\textbf{Coarse Calibration.} We first take three RGB-D images of one face simultaneously from  the three RGB-D cameras. %
Afterward, we detect 2D landmarks~\cite{grishchenko2020attention} from each image. %
The unoccluded 2D landmarks are then projected as 3D landmarks according to camera intrinsics and depth maps. %
Finally, we calculate coarse camera extrinsics by aligning the projected 3D landmarks with the least square method. %

\textbf{Fine Calibration.} We first convert RGB-D images to point clouds and remove the noisy background parts of point clouds. %
Next, we register the cropped point clouds with the coarse camera extrinsics and point-to-plane ICP. %

\subsection{Device Synchronization}

\textbf{Camera Synchronization.} We synchronize the three Azure Kinect cameras with the daisy-chain configuration, where the synchronization signal is sequentially passed from the master camera to the other two subordinate cameras. %
Since the Azure Kinect camera leverages the time-of-flight principle to obtain depth images, %
each camera will emit a laser to measure the distance between the camera and 3D faces. %
To avoid the interference of lasers, %
there is a 250 $\upmu$s delay of emitting laser between each camera. %


\textbf{Microphone Synchronization.} The microphone is synchronized with the master camera. %
The recording thread synchronizes the microphone and the master camera with a common controlling variable. %

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{framework.pdf}
  \caption{The 3D face reconstruction pipeline with three stages: initialization, 3DMM parameter fitting, and vertex-level fitting. %
  $\mathbf{S}$ denotes the 3D face, $\alpha, \beta, \delta, \gamma, \mathbf{R}$ are respectively shape, expression, texture, lighting, and vertex-level deformation parameters.
  }
  \label{fig:framework}
\end{figure*}

\subsection{Corpus}

% 统计句子的长度，不同情感的数量，被重复的次数，以及覆盖音节是否平均。
% 其中句子的长度，音节覆盖是否平均要画图。

We build a large-scale corpus with 11,000 sentences under different scenarios such as news broadcasting, conversation, and storytelling. %
Each sentence has an emotion label of seven categories (neutral, angry, disgust, happy, fear, sad, surprise). %
For the neutral emotion, we have 2000 sentences. %
For the other emotions, we have 1500 sentences. %
Each sentence of the corpus has 17 words on average. %
Our corpus covers each phoneme as evenly as possible. % 

\subsection{Recording}

% 对平均下来每个句子的时长画图
Each actor participates in the recording voluntarily. %
The actor is asked to practice 100 sentences of different emotions fluently and emotively. %
Afterward, the capture device starts to record each sentence one by one. %
Note that despite each actor trying to read each sentence with emotion as much as possible, a few sentences are not as expressive as expected because some actors are not professional. %
Nevertheless, most recorded sentences are still expressive. %

\section{Toolchain}

From the capture devices, we have obtained the speech audio and synchronized RGB-D videos from three different views (left view, center view, and right view). %
In this section, we will reconstruct topology-uniformed 3D Face sequences from the RGB-D videos. %
We take the basel face model~(BFM)~\cite{paysan20093d} as a template 3D face and deform the template to fit RGB-D videos. %
The overall pipeline is illustrated in Figure~\ref{fig:framework}. %
The pipeline has three stages: (1) initialization stage, (2) 3DMM parameter fitting stage, and (3) vertex-level fitting stage. %
Compared with the 3DMM~\cite{blanz1999morphable} based reconstruction methods, our reconstruction pipeline keeps more accurate face details due to the design of multi-stage fitting and vertex-level optimization. %
Furthermore, we also propose a highly efficient algorithm for mesh sequence compression. %
Details will be elaborated as follows. %

\subsection{Initialization}

Before fitting the template 3D face to RGB-D videos, we first need to define the deformation space of each 3D face and conduct initialization by landmark fitting. %

In our pipeline, each 3D face has two-level deformation spaces: the 3DMM level and the vertex level. %
The 3DMM level deformation describes the coarse-grained face shapes and movements, such as fat or thin, mouth closed or open. %
The vertex level deformation depicts the fine-grained face shapes and movements, such as murmur and pout. %
Formally, given the 3D face $\mathbf{S} \in \mathbb{R}^{n \times 3}$ (where $n$ is the number of vertices), the 3DMM level deformation is represented as an affine model of facial expression parameter $\mathbf{\alpha}$ and facial identity parameter $\mathbf{\beta}$, the vertex level deformation is represented as offset matrix $\mathbf{R} \in \mathbb{R}^{n \times 3}$. The two types of deformation are added together to formulate the full deformation space of $\mathbf{S}$:
\begin{equation}
  \mathbf{S} = \mathbf{\bar{S}} + \mathbf{B}_{id}\mathbf{\alpha} + \mathbf{B}_{exp}\mathbf{\beta} + \mathbf{R},
\end{equation}
where $\mathbf{\bar{S}}$ is the average face shape; $\mathbf{B}_{id}$ and $\mathbf{B}_{exp}$ are the PCA bases of identity and expression, each row of $\mathbf{R}$ is the offset of each vertex. %
Following Deng~\textit{et al.}~\cite{deng2019accurate}, we adopt the 2009 Basel Face Model~\cite{paysan20093d} for $\mathbf{\bar{S}}$ and $\mathbf{B}_{id}$, use expression bases $\mathbf{B}_{exp}$ of Guo~\textit{et al.}~\cite{guo2018cnn} built from Facewarehouse~\cite{cao2013facewarehouse}, resulting in $\alpha \in \mathbb{R}^{80}$, $\beta \in \mathbb{R}^{64}$. %
During implementation, we would add regularization to $\mathbf{R}$, which guarantees that the vertex level deformation does not deviate too far from the 3DMM deformation space. %


% For some subtle facial movements, such as murmur and 

% \subsection{Landmark Fitting}

With the deformation space, we initialize 3D face $\mathbf{S}$ through facial landmark fitting. %
In this stage, the 3DMM parameters $\alpha, \beta$ are optimized to minimize the distance between the 3D landmarks of $\mathbf{S}$ and the detected 3D landmarks, while the vertex-level offset $\mathbf{R}$ is frozen to zero. 
To obtain the detected 3D landmarks, we first localize 2D facial landmarks~\cite{grishchenko2020attention}  from the RGB part of the center-view video. %
For each frame, the localized 2D landmarks are then projected into 3D landmarks based on the depth part of the center-view video and the intrinsics of the center camera. %
We then fit the 3D landmarks of $\mathbf{S}$ to detected 3D landmarks with the following loss:
\begin{equation}
    \mathcal{L}(\alpha,\beta) = \mathcal{L}_{\mathrm{lm}}+\lambda_{\mathrm{p}}\mathcal{L}_{\mathrm{p}},
\end{equation}
where $\mathcal{L}_{\mathrm{lm}}$ measures the L2-norm distance. %
The prior term $\mathcal{L}_{\mathrm{p}}$ penalizes the squared sum of the optimized parameters, which imposes $\alpha, \beta$ to be close to zero. %

\subsection{3DMM Parameter Fitting}

Based on the 3D face $\mathbf{S}$ initialized from landmark fitting, %
in this stage, we further deform 3D face $\mathbf{S}$ to fit RGB-D videos captured from three synchronized RGB-D cameras. %
We rasterize $\mathbf{S}$ to obtain RGB images and depth images with a differentiable render~\cite{Laine2020diffrast}, and optimize the distance between rasterized images and ground truth images. %

More specifically, given the 3D face $\mathbf{S}$, %
we first transform $\mathbf{S}$ from the world space to the camera space of each camera. %
The 3D face under each camera space is denoted as $\mathbf{S}_{i}$ (where $i$ is the ID of each RGB-D camera). %
Based on $\mathbf{S}_{i}$, we then colorize $\mathbf{S}_{i}$ with 3DMM texture parameter $\delta \in \mathbb{R}^{80}$ and the spherical harmonic lighting parameter $\gamma \in \mathbb{R}^{27}$ following Deng~\textit{et al}~\cite{deng2019accurate}, yielding vertex color $\mathbf{C}_{i} \in \mathbb{R}^{n \times 3}$. %
Afterwards, the $\mathbf{C}_{i}$ and the z-axis of $\mathbf{S}_{i}$ are rasterized with differentiable render~\cite{Laine2020diffrast}, bringing out the rasterized RGB image $\mathbf{\hat{I}}_{\mathrm{rgb}}^{i}$ and depth image $\mathbf{\hat{I}}_{\mathrm{d}}^{i}$. %

For the 3D face reconstruction of single RGB-D video, $\mathbf{\hat{I}}_{\mathrm{rgb}}^{i}$ and $\mathbf{\hat{I}}_{\mathrm{d}}^{i}$ are optimized to be close to $\mathbf{I}_{\mathrm{rgb}}^{i}$ and $\mathbf{I}_{\mathrm{d}}^{i}$, where $\mathbf{I}_{\mathrm{rgb}}^{i}$ and $\mathbf{I}_{\mathrm{d}}^{i}$ are corresponding ground truth color and depth images  from the $i$-th camera. %
The optimization is conducted with the following loss function:
\begin{equation}
    \mathcal{L}(\alpha,\beta,\delta,\gamma) =  \mathcal{L}_{\mathrm{rgb}}+ \lambda_{\mathrm{d}}\mathcal{L}_{\mathrm{d}}+\lambda_{\mathrm{lm}}\mathcal{L}_{\mathrm{lm}}+\lambda_{\mathrm{p}}\mathcal{L}_{\mathrm{p}},
\end{equation}
where $\mathcal{L}_{\mathrm{rgb}}$ measures the distance between $\mathbf{\hat{I}}_{\mathrm{rgb}}^{i}$ and $\mathbf{I}_{\mathrm{rgb}}^{i}$, %
 $\mathcal{L}_{\mathrm{d}}$ measures the distance between $\mathbf{\hat{I}}_{\mathrm{d}}^{i}$ and $\mathbf{I}_{\mathrm{d}}^{i}$. %
 Landmark loss $\mathcal{L}_{\mathrm{lm}}$ and prior term $\mathcal{L}_{\mathrm{p}}$ are also optimized in this stage. %
 Notice that both $\mathcal{L}_{\mathrm{d}}$ and $\mathcal{L}_{\mathrm{rgb}}$ play vital roles in 3D face reconstruction. %
$\mathcal{L}_{\mathrm{d}}$ supervises 3D face to have accurate geometry, %
$\mathcal{L}_{\mathrm{rgb}}$ supervises 3D face to have visually consistent geometry and texture. %
 To avoid the influence of outliers and noises in image distance measurement, we on one hand apply geman-mcclure penalty function~\cite{ganan1985bayesian} to measure the distance between rasterized images and ground truth images, on the other hand mask the noisy backgrounds of images and only optimize foregrounds. %
 
 For the optimization of multiple synchronized RGB-D videos, %
 the loss function $\mathcal{L}(\alpha,\beta,\delta,\gamma)$ of different cameras are summed together and optimized simultaneously. %
 The shape parameter $\alpha$, expression parameter $\beta$ and texture parameter $\delta$ are shared among different cameras, while the lighting parameter $\gamma$ is unshared because different cameras have different imaging settings and lighting conditions. %


\subsection{Vertex-Level Fitting}

Through 3DMM parameter fitting, we have obtained a relatively accurate 3D face. %
However, the shape parameter $\alpha$ and expression parameter $\beta$ have limited capacity on representing subtle facial movements. %
To keep more accurate facial details, we further optimize vertex level deformation $\mathbf{R}$ together with the aforementioned parameters $\alpha, \beta, \delta, \gamma$. %

The objective of the vertex-level fitting stage is similar to that of the 3DMM parameter fitting: we still aim to minimize the distance between  $\mathbf{\hat{I}}_{\mathrm{rgb}}^{i}, \mathbf{\hat{I}}_{\mathrm{d}}^{i}$ and $\mathbf{I}_{\mathrm{rgb}}^{i}, \mathbf{I}_{\mathrm{d}}^{i}$. %
One main challenge of the vertex-level fitting is that the optimized parameter $\mathbf{R}$ has significantly more degree of freedoms ($3n$ degree of freedoms) compared with 3DMM parameter fitting. %
Regularizations are needed to prevent ill-posed optimization. %
We respectively incorporate edge regularization, laplacian smooth regularization, and offset penalty regularization. %
The edge regularization $\mathcal{L}_\mathrm{e}$ minimizes the edge length difference before and after the vertex-level fitting, which enforces local rigidness of deformation. %
The laplacian smooth regularization $\mathcal{L}_\mathrm{lap}$ minimizes the differential coordinate of each vertex with the  laplacian operator, which leads to local smoothness of deformation. %
The offset penalty regularization $\mathcal{L}_\mathrm{op}$ minimizes the L2-norm of each vertex offset, which guarantees that the offset do not deviate too far from the 3DMM deformation space. %
All of these regularizations are summed as vertex prior term $\mathcal{L}_\mathrm{vp}$:
\begin{equation}
    \mathcal{L}_\mathrm{vp} = \lambda_\mathrm{e}\mathcal{L}_\mathrm{e} + \lambda_\mathrm{lap}\mathcal{L}_\mathrm{lap} + \lambda_\mathrm{op}\mathcal{L}_\mathrm{op}.
\end{equation}
Combining $\mathcal{L}_\mathrm{vp}$ with the loss function in the 3DMM parameter fitting stage, %
we obtain the following loss:
\begin{equation}
    \mathcal{L}(\mathbf{R},\alpha,\beta,\delta,\gamma)=\mathcal{L}_{\mathrm{rgb}}+ \lambda_{\mathrm{d}}\mathcal{L}_{\mathrm{d}}+\lambda_{\mathrm{lm}}\mathcal{L}_{\mathrm{lm}}+\lambda_{\mathrm{p}}\mathcal{L}_{\mathrm{p}}+\mathcal{L}_\mathrm{vp}.
\end{equation}
When optimizing multiple synchronized RGB-D videos, the vertex level deformation $\mathbf{R}$ is shared across cameras. % 
% 接下来介绍，我们添加了哪些正则项
% The incorporation of $\mathbf{R}$ leads to smaller distance between rasterized images and ground truth images. %


\subsection{Sequence Reconstruction and Compression}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{reconstruction.pdf}
  \caption{Visualizations of the reconstruction results. In the first two rows, we respectively give the center-view of the RGB image, center-view of the depth image, the 3D mesh and the reconstruction error respectively after initialization, 3DMM fitting, and vertex-level fitting from left to right. Note that the reconstruction error calculates the distance between the depth image and the fitted mesh, the red color denotes higher reconstruction error and the blue color denotes lower error. In the bottom row, we demonstrate a sequence of original RGB images and corresponding reconstructed meshes. The eyes of RGB facial images are mosaicked for privacy protection.  %
  }
  \label{fig:recon_vis}
\end{figure*}

\textbf{Mesh Sequence Reconstruction.} For the 3D reconstruction of the whole video sequence, we do not conduct three-stage fitting for all frames. %
Instead, we only conduct a three-stage fitting for the first frame of the video. %
For the other frames, just the vertex-level fitting is applied with the parameters of the last frame as initialization. %
Such simplification enables faster reconstruction speed. %
For the optimization of the first video frame, we set the learning rate to $0.01$. The loss of landmark fitting is minimized for 100 iterations. %
The losses of 3DMM parameter fitting and vertex-level fitting are optimized for 500 iterations. %
For the optimization of the subsequent frames, we set the learning rate to $0.005$. %
The loss of vertex-level fitting is optimized for 200 iterations. %
For the weights of loss functions, we generally choose $\lambda_\mathrm{d}=2$, $\lambda_\mathrm{lm}=100$, $\lambda_\mathrm{p}=0.001$, $\lambda_\mathrm{e}=20$, $\lambda_\mathrm{lap}=20$, $\lambda_\mathrm{op}=0.01$. %
The adam optimizer~\cite{kingma2014adam} is adopted to optimize the aforementioned loss functions. %

\textbf{Mesh Sequence Compression.} The reconstructed 3D face sequence is usually storage-consuming. %
In our case, the 3D face sequence with 30 fps and 35,709 vertices takes 12MB per second, which means that collecting datasets within 36 hours will take around 1.5T storage space. %
Such storage cost obstructs us to scale up and distribute our dataset. %
To address this issue, we propose an efficient mesh sequence compression algorithm. %
With our algorithm, storing one second of mesh sequence only takes about 3.5MB per second. Such a high compression ratio brings convenience for data collecting and distributing. %

Given mesh sequence $\{\mathbf{S}^{j}\}$ (where $j$ is the frame number), our algorithm firstly converts each axis of each mesh $\mathbf{S}^{j}$ as images, and then compresses image sequences as lossless videos. %
More specifically, for each axis of mesh $\mathbf{S}^{j}$, we have coordinates of $n$ vertices. We first find the minimum and maximum coordinates from these vertices, and then quantize the coordinates with the minimum coordinate being 0 and maximum coordinate being $2^{16}-1$. %
Afterward, the minimum coordinate, the maximum coordinate, and the quantized coordinates are reshaped and converted to 16-bit grey images with the shape of $\lceil \sqrt{n} \rceil \times \lceil \sqrt{n} \rceil$, where $\lceil \cdot \rceil$ is ceiling function. %
Gathering the converted images of all meshes, we obtain image sequences respectively for x, y, z axis. %
We compress the image sequences as videos with a lossless video codec~\cite{tomar2006converting}. %
Experiments demonstrate that our method has a compression ratio of 0.29. %
More details of the algorithm are illustrated in the supplementary material. %

\section{Dataset Observations}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{statictics.pdf}
  \caption{Statistics of the MMFace4D dataset. %
  (a) Scatter plot of $\boldsymbol{v}(\mathrm{lip})$ for each sequence of the MMFace4D, where $\boldsymbol{v}(\mathrm{lip})$ is the average vertex velocity of the lip region. % 
  The sequences belonging to ID 0, 1, and 2 are colorized with red, blue, and green, while the other sequences are plotted with grey color. %
  (b) The histogram of subject age and subject gender. %
  (c) The histogram of sequence duration. %
  }
  \label{fig:statistics}
\end{figure*}

\subsection{Reconstruction Visualization}

To demonstrate the effectiveness of our reconstruction pipelines, %
we give qualitative visualizations of the reconstruction results in Figure~\ref{fig:recon_vis}. %
We observe that the pipeline of multi-stage fitting improves the reconstruction results progressively. %
Especially, after the vertex level fitting, subtle motions of the mouth, cheek, and eyebrow are reconstructed precisely. %
The bottom row of Figure~\ref{fig:recon_vis} visualizes the RGB face sequence and the reconstructed 3D mesh sequence. %
The 3D mesh sequence fits the RGB face sequence accurately. %
 More visualizations of the side-view image, and the rendering video of 3D mesh sequence are demonstrated in the supplementary material. %

%注意由于篇幅的原因，只有中间图被给出了，对于更多的误差、侧图，在supplementary中有给出

\subsection{Dataset Statistics}

Before analyzing the characteristics of MMFace4D, we first give an important statistic $\boldsymbol{v}(\cdot)$, which calculates the average vertex velocity along one particular axis for user-specified vertex sets. %
 For example, $\boldsymbol{v}_{x}(\mathrm{lip})$ calculates the average vertex velocity for vertices of the lip region along the x-axis. %
Based on $\boldsymbol{v}(\cdot)$, we conduct statistic analysis on the MMFace4D dataset to verify that our dataset has various talking styles, diversified actors, widely-distributed sequence durations, and expressive facial movements. %

% 个体style的验证方式，要画样本unpose后motion在各个轴运动总和除以帧数求平均的散点图(motion针对所有顶点求和)，然后选取其中一些个体的样本做colorize，以此验证个体内是比较集中的，个体间距离比较远
% emotion的验证方式，验证双眉、上嘴唇、下嘴唇、左嘴角、右嘴角、脸颊、下把这几个部位的multi distribution plot,画多张图拼在一起
% duration只需给出序列的长度分布即可
% diversified actor给年龄和性别就行

The talking style is well reflected in $\boldsymbol{v}(\mathrm{lip})$. %
As shown in Figure~\ref{fig:statistics}(a), we draw a scatter plot for some sequences of MMFace4D. %
The coordinate of each point in Figure~\ref{fig:statistics}(a) describes the $\boldsymbol{v}_{x}(\mathrm{lip})$, $\boldsymbol{v}_{y}(\mathrm{lip})$, and $\boldsymbol{v}_{z}(\mathrm{lip})$ value of each sequence. %
A few points of the scatter plot are colorized according to the subject ID of the sequence. %
From colorization, we observe that points of the same ID cluster together, while a gap exists among different identities. %
Such clustering effect confirms that different people have different talking styles in the MMFace4D dataset. %

% To record emotional sentences in the MMFace4D corpus, %
% we ask each person to read each sentence with expressive talking emotions as much as possible. %
% Some recorded sentences are expressive with rich emotions, while some are not because the actors are not professional. %
% Such characteristic is reflected in Figure~\ref{fig:statistics}(b). %
% For happy, sad, and neutral emotions, we plot the distribution of $\boldsymbol{v}_{z}(\mathrm{lip})$ and $\boldsymbol{v}_{z}(\mathrm{cheek})$. %
% Although the distributions of happy, sad, and neutral sequences overlap to some degree, %
% differences still exist between the distributions of different emotions. %
% That is because a large proportion of the recorded sequences still contain rich emotions. %
 
% For example, when people speak with happy emotion, the cheek tends to move faster, 

In Figure~\ref{fig:statistics}(b) and Figure~\ref{fig:statistics}(c), %
we visualize the distributions of actor ages, genders, and sequence durations. %
In the MMFace4D dataset, the actor age ranges from 15 to 68,  %
the median age is 28, %
the male-female ratio is 1.65. %
The sequence duration ranges from 0.7 seconds to 11.4 seconds, %
the median duration is 3.6 seconds. %
Overall, the MMFace4D dataset contains diversified actors and widely-distributed sequence durations. % 

MMFace4D dataset and previous datasets have significant differences in terms of average vertex velocity. %
For comparison, we average the  vertex velocity of the lip region along all sequences and all axes. %
The $\boldsymbol{v}(\mathrm{lip})$ of the MMFace4D dataset is 0.0025, while the $\boldsymbol{v}(\mathrm{lip})$ of MeshTalk dataset~\cite{richard2021meshtalk} and VOCASET~\cite{cudeiro2019capture} are respectively 0.0016 and 0.0011. %
The MMFace4D dataset has faster vertex velocity compared with the VOCASET and MeshTalk dataset, %
which demonstrates that MMFace4D has more expressive and salient facial motions compared with previous datasets. %

The aforementioned characteristics of the MMFace4D dataset bring challenges for audio-driven 3D face animation. %
The 3D face animation model should on one hand be capable of synthesizing stylized and expressive talking faces, on the other hand, generalize to different identities. %


\section{Benchmark Construction}

Based on MMFace4D, we construct a challenging benchmark of audio-driven 3D face animation. The evaluation protocol, baseline method, and primary experimental results are elaborated as follows. %


\subsection{Evaluation Protocol}

% MMFace4D执行的是few-shot的training 和 evaluation
Considering that different identities in the MMFace4D dataset have different talking styles, %
we build a protocol to evaluate the synthesis of stylized 3D talking faces under the few-shot scenario. %
More specifically, % 
we split the MMFace4D dataset into a training set and a test set by the identity of each actor, where the training set contains 400 actors, and the test set contains the remaining 31 actors. %
Furthermore, to enable the few-shot talking style synthesis, %
 10 sequences of each testing actor are further moved to training sets. %
The 3D face animation model needs to learn the talking style of each testing actor from these 10 sequences. %

We follow the lip-sync metric applied in MeshTalk~\cite{richard2021meshtalk} for the evaluation metrics. %
More specifically, the lip error of the single frame is defined to be the maximal L2-error of all lip vertices between ground truth mesh and predicted mesh. %
We finally report the average error of all test frames. %
We have also conducted a user study to evaluate the realism of synthesized mesh sequences. %
Each participant is asked to rate the mean opinion score (MOS) of the mesh sequences in the range of 1-5 (a higher score denotes a better result). %


% to help learn talking styles of test actor from only a few samples. %
% 训练集包含了所有31个人，每人10条视频，

% The training set contains all of the 431 identities,  % 
% For the test set, 


\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{audio2face.pdf}
  \caption{The synthesized and the ground truth 3D face sequences. %
  The first row shows the synthesized results of the baseline network driven by different audio pieces. %
  The second row shows the ground truth 3D faces. %
  The bottom two rows give the driven text, phoneme, and audio. %
  More results are demonstrated in the supplementary material. 
  }
  \label{fig:animation}
\end{figure*}

\subsection{Baseline Method}

In the task of audio-driven 3D face animation, the mapping between audio and 3D face meshes is highly non-linear, %
while the time correspondences between audio and 3D face meshes are strict, usually one audio frame corresponds to one particular 3D face mesh. %
Such characteristic requires the baseline method to have a strong capability of non-linear translation while the demand of building complex time dependence is lessened. %

Based on such motivation, we build a non-autoregressive baseline network with fully convolutional architecture. %
The baseline network applies ResNet~\cite{he2016deep} with 50 layers of 1D convolution as the backbone. %
The ResNet encoder takes the mel-spectrogram of the speech audio as input and convolves the input along the time dimension. %
A simple linear layer is stacked on top of the backbone, which converts latent embeddings to mesh vertices. %
Some downsampling layers of the backbone are removed to prevent the loss of temporal information. %
To synthesize stylized talking faces, the baseline network fuses one-hot identity labels and template faces into the latent layer of ResNet. %
More details of the baseline network are illustrated in the supplementary. %


\begin{table}[]
\centering
\caption{Comparison of audio-driven 3D face animation on the benchmark. A higher realism score denotes better. }
\setlength{\tabcolsep}{3mm}
\begin{tabular}{c|cc}
\hline
                   & L2 Error (in mm) & Realism \\ \hline
ResNet18 w/o style &     3.329     &    3.93     \\
ResNet50 w/o style &      3.317    &     4.01    \\
FaceFormer~\cite{fan2022faceformer}           &   3.153       &    \textbf{4.29}     \\ 
\textbf{Baseline Network}         &    \textbf{3.124}      &     4.21    \\ \hline
\end{tabular}
\label{tab:exp}
\end{table}

\subsection{Primary Experimental Results}


Figure~\ref{fig:animation} demonstrates the synthesized faces of the baseline network together with the ground truth faces. %
We compare our baseline method with state-of-the-art FaceFormer~\cite{fan2022faceformer}. %
Table~\ref{tab:exp} reports the L2 Error and realism of different methods. %
We observe that the baseline network outperforms FaceFormer slightly in terms of the L2 Error. %
Besides, the realism of the baseline network is comparable to that of FaceFormer. %
Moreover, it is noteworthy to emphasize that the baseline network has fewer parameters (57M param) compared with FaceFormer (118M param), %
and the design of fully convolutional architecture enables non-autoregressive generation with faster speed. %
On average, the baseline network takes 4 milliseconds to synthesize 3D mesh sequences for one-second audio on the RTX 2080Ti GPU, while the FaceFormer takes 67 milliseconds. %
Meanwhile, we also observe that deeper ResNet performs better than shallower ResNet. %
Such phenomenon can be explained by the powerful non-linear translation capacity of deep convolutional layers, %
which is in demand for audio-driven 3D face animation. %
% In comparison, the FaceFormer applies Transformer~\cite{vaswani2017attention} as backbone, which has stronger capacity of modeling time correspondence but weaker capacity of non-linear transformation, leading to the inferior performance on our benchmark. %

% 输入了one-hot label
Our baseline network is capable of generating stylized 3D talking faces through the fusion of one-hot identity labels. %
As shown in Figure~\ref{fig:style}, we visualize the distances between the lower lip and the upper lip for the same driven audio conditioned on different identity labels. %
The lip distances are diversified for different identities. %
More videos of stylized talking faces synthesis and more comparisons between FaceFormer and our baseline method are demonstrated in the supplementary material. %


\begin{figure}[]
  \centering
  \includegraphics[width=\linewidth]{style.pdf}
  \caption{Lip distances for the same speech conditioned on different identity labels. %
  Different colors denote different identities. 
  }
  \label{fig:style}
\end{figure}

% The stack of deep convolution layers endows both powerful capacity of non-linear transformation and the capacity of perceiving local temporal dependence. %
%  Additionally,  %

% 比较l2，mos评分，生成速度即可
% 生成速度没有画在表格里，文本描述即可
% 可视化style中的lip distance

\section{Conclusion}

In this paper, we present a large-scale multi-modal 4D~(3D sequence) face dataset named MMFace4D. %
MMFace4D features the following three main properties: %
(1) diversified corpus and actors, %
(2) synchronized speech audio and high-fidelity 3D animation with our multi-stage reconstruction pipeline. %
(3) low storage cost of 3D mesh sequences with our highly efficient compression algorithm. %
We conduct extensive data observations on the MMFace4D dataset to demonstrate the various talking styles, expressive facial motions, and diversified actors in the dataset. %
Based on MMFace4D, we build a challenging benchmark and present a strong baseline that enables non-autoregressive generation. %
The baseline network achieves both better performance and faster inference compared with the state-of-the-art autoregressive method. %
Overall, MMFace4D dataset develops the task of audio-driven 3D face animation from previous small-scale scenarios to large-scale scenarios, which enables the training of high-fidelity, expressive, and generalizable face animation models. %
We believe the release of MMFace4D would spur further research. %


% 促进audio-driven face animation的发展

% 要写Ethics Considerations
% \textbf{Ethics Considerations}: For the privacy protection issue, we would remove the RGB texture of each 3D face and only retain the geometry information when releasing the dataset. %

\textbf{Ethics Considerations}: Each actor of the MMFace4D dataset participates in the recording voluntarily. %
For the issue of privacy protection, all demonstrated RGB face images are mosaicked. %
The MMFace4D dataset aims to promote positive technologies. %
However, we also acknowledge that both the dataset and the technology of audio-driven 3D face animation have a risk of being misused. %
Thus, we hope to raise the awareness of the public and develop forgery detection technology to prevent such misuse. %

% For the privacy protection issue, we would remove the RGB texture of each 3D face and only retain the geometry information when releasing the dataset. %

\newpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=\textwidth]{sup_reconstruction.png}
    \captionof{figure}{Visualizations of the reconstruction results. We respectively give the left-view RGB image, the left-view depth image, the center-view RGB image, the center-view depth image, the right-view RGB image, the right-view depth image, and the reconstructed 3D mesh from left to right. The eyes of RGB facial images are mosaicked for privacy protection.}
    \label{fig:sup_recon_vis}
\end{center}%
}]


\appendix
\section*{Appendix}
\label{sec:appendix}

% \begin{figure*}[htbp]
%   \centering
%   \includegraphics[width=\linewidth]{sup_reconstruction.png}
%   \caption{Visualizations of the reconstruction results. We respectively give the left-view RGB image, the left-view depth image, the center-view RGB image, the center-view depth image, the right-view RGB image, the right-view depth image, and the reconstructed 3D mesh from left to right. The eyes of RGB facial images are mosaicked for privacy protection.
%   }
%   \label{fig:sup_recon_vis}
% \end{figure*}

\section{The Compression Algorithm in Section 4.4}

% \vspace{-0.3cm}

In this section, we additionally illustrate the compression algorithm in detail. %
Given a mesh sequence $\{\mathbf{S}^{1}, \mathbf{S}^{2}, ..., \mathbf{S}^{n}\}$ with $\mathbf{S} \in \mathbb{R}^{35709\times 3}$, the mesh coordinates of the $x$, $y$, and $z$ axes are respectively denoted as $\mathbf{S}[x]$, $\mathbf{S}[y]$, and $\mathbf{S}[z]$, where $\mathbf{S}[\cdot]$ is a float array with shape $\mathbb{R}^{35709}$. %
We first convert each element of $\mathbf{S}[\cdot]$ to 16-bit unsigned short integer  with the following quantization: %
\begin{equation}
    \mathbf{S}_{\mathrm{short}}[\cdot] = \mathrm{short}(\frac{\mathbf{S}[\cdot] - \min(\mathbf{S}[\cdot])}{\max(\mathbf{S}[\cdot])} \times (2^{16} - 1)),
\end{equation}
where $\mathrm{short}$ denotes the type conversion from float to 16-bit unsigned short integer. %
We also convert $\max(\mathbf{S}[\cdot])$ and $\min(\mathbf{S}[\cdot])$ to four unsigned short integers, which are denoted as $\mathrm{short}(\max(\mathbf{S}[\cdot]))$, $\mathrm{short}(\min(\mathbf{S}[\cdot]))$. %
Afterwards, we save $\mathrm{short}(\max(\mathbf{S}[\cdot]))$, $\mathrm{short}(\min(\mathbf{S}[\cdot]))$, and $\mathbf{S}_{\mathrm{short}}[\cdot]$ to a 16-bit grey image $\mathbf{I}(\cdot)$ with shape $189 \times 189$. %
The $\mathrm{short}(\max(\mathbf{S}[\cdot]))$ and $\mathrm{short}(\min(\mathbf{S}[\cdot]))$ are saved in the first fours elements of $\mathbf{I}(\cdot)$, the $\mathbf{S}_{\mathrm{short}}[\cdot]$ is saved sequentially in the following elements of $\mathbf{I}(\cdot)$. %

When converting all meshes to images, we obtain three image sequences: $\{\mathbf{I}^{1}(x), ..., \mathbf{I}^{n}(x)\}$, $\{\mathbf{I}^{1}(y), ..., \mathbf{I}^{n}(y)\}$, and $\{\mathbf{I}^{1}(z), ..., \mathbf{I}^{n}(z)\}$. %
We compress the three image sequences to three videos with a lossless video codec~\cite{tomar2006converting}. %
The decoding process is also convenient, we only need to decode the video, and denormalize the image to vertex coordinates. %


% During the conversion, we quantize the coordinate of each point 
% More specifically, given 

\section{Additional Reconstruction Visualizations in Section 5.1}
 In Figure~\ref{fig:sup_recon_vis}, we give more visualization results of the raw RGB images, depth images, and reconstructed 3D meshes. %
 % We also give the raw RGB videos, depth videos and the reconstructed 3D mesh sequences in \textit{reconstruction\_0.mp4} and \textit{reconstruction\_1.mp4}. %

 
\section{The Baseline Network in Section 6.2}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{baseline.pdf}
  \caption{The overall framework of the baseline network. %
  }
  \label{fig:sup_baseline}
\end{figure}

In this section, we elaborate more details of the baseline network. %
Figure~\ref{fig:sup_baseline} demonstrates the overall framework of the baseline network. %
The backbone of the baseline network is ResNet50~\cite{he2016deep}, which contains 50 1D-convolution layers. %
To incorporate the template 3D face and the one-hot identity label into the baseline network, %
we linearly embed the template 3D face and one-hot identity label to low-dimension embeddings. %
The embeddings are repeated along the time dimension and concatenated with the latent audio features. %

During implementation, we first convert the 16000Hz audio to mel-spectrogram with 80 mel-filterbanks and a window size of 1024. %
The first 40 layers of ResNet50 take the mel-spectrogram as input. %
We concatenate the 64-dim face embedding and the 192-dim identity embedding with the latent audio features. %
The concatenated features are then fed to the remaining convolution layers. %
Finally, we resample the output features by the user-specified frame rate, and linearly map the output features to mesh vertices. %
We remove one downsampling layer of ResNet50 after the 40th convolution layer, which prevents the loss of temporal information. %
During optimization, we optimize L2 loss between the ground truth 3D meshes and the predicted 3D meshes with the Adam optimizer~\cite{kingma2014adam}, %
the learning rate is $10^{-3}$. %

\section{Additional Experimental Results in Section 6.3}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{sup_audio2face.pdf}
  \caption{Experimental results of audio-driven 3D face animation. %
    We respectively give the synthesized results of FaceFormer, synthesized results of the baseline network, ground truth 3D faces, and driven audio. 
  }
  \label{fig:sup_compare}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth]{sup_style.pdf}
  \caption{Experimental results of stylized talking face synthesis. Each row shows the synthesized 3D faces driven by the same audio with different talking styles. %
  Different talking styles have significant differences in terms of lip distances. %
  }
  \label{fig:sup_style}
\end{figure*}

\subsection{Comparison with FaceFormer}

In Figure~\ref{fig:sup_compare}, we visualize more audio-driven 3D face animation images. %
% We also give the visualization videos in \textit{audio2face\_0.mp4}, \textit{audio2face\_1.mp4}, \textit{audio2face\_2.mp4}, and \textit{audio2face\_3.mp4}. %
Compared with FaceFormer~\cite{fan2022faceformer}, the synthesized 3D faces of the baseline network are closer to the ground truth 3D faces. %
This is also reflected in the quantitative metric, the evaluated L2 error of the baseline network is 3.124, while the L2 error of FaceFormer is 3.153. %
Compared with FaceFormer, the baseline network also has less computation cost due to the design of fully convolutional architecture. During inference, the baseline network is 16.75 times faster than FaceFormer. % 


\subsection{Stylized Talking Face Synthesis}

In Figure~\ref{fig:sup_style}, we visualize more 3D talking faces with different talking styles. %
% We also give the visualization videos in \textit{style\_0.mp4}, \textit{style\_1.mp4}, \textit{style\_2.mp4}, and \textit{style\_3.mp4}. %
The talking style is reflected in the amplitude of the mouth open. %
The baseline network synthesizes different styles when feeding different identity labels. %

\end{document}
