\documentclass[11pt]{article}
 
 \usepackage{amsthm}
 \usepackage{enumitem}
 \usepackage{hyperref}
 \usepackage{amssymb}
 \usepackage{amsmath}
 \usepackage{dsfont}
 \usepackage{mathtools}
 \usepackage{xcolor}
 \usepackage{mathrsfs}

 
 \usepackage{comment}

 
\usepackage{algorithmic} 
\usepackage{algorithm} 
\usepackage{adjustbox}
 
 \usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
  
 \renewcommand{\qedsymbol}{$\blacksquare$}  
  
 \newtheorem{thm}{Theorem}
 \newtheorem{prop}{Proposition}
 \newtheorem{lem}{Lemma}
 \newtheorem{de}{Definition}
 \newtheorem{rem}{Remark}
 \newtheorem{cor}{Corollary}

\title{Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality}
\author{Fran\c{c}ois G. Ged, Maria Han Veiga}
\date{}
\include{commands}

\begin{document}
\maketitle


\begin{abstract}
    A novel Policy Gradient (PG) algorithm, called \textit{Matryoshka Policy Gradient} (MPG), is introduced and studied, in the context of fixed-horizon max-entropy reinforcement learning, where an agent aims at maximizing entropy bonuses additional to its cumulative rewards.
    In the function approximation setting with softmax policies, we prove uniqueness and characterize the optimal policy, together with global convergence of MPG.
    These results are proved in the case of continuous state and action space.
    MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the infinite horizon max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework.
    Finally, we provide a criterion for global optimality when the policy is parametrized by a neural network in terms of the neural tangent kernel at convergence.
    As a proof of concept, we evaluate numerically MPG on standard test benchmarks.
\end{abstract}


\section{Introduction}

\subsection{Policy gradient, max-entropy reinforcement learning and fixed horizon}

\paragraph{Reinforcement Learning}
(RL) tasks can be informally summarized as follows: sequentially, an agent is located at a given state $s$, takes an action $a$, receives a reward $r(a,s)$ and moves to a next state $s'\sim p(s,a,\cdot)$, where $p$ is a transition probability kernel.
The agent thus seeks to maximize the cumulative rewards from its interactions with the environment, that is, it optimizes its \textit{policy} $\pi$ by reinforcing decisions that led to high rewards, where $\pi(a|s)$ is the probability for the agent to take action $a$ while at state $s$.


\paragraph{Policy Gradient} (PG) methods are model-free algorithms that aim at solving such RL tasks;
\textit{model-free} refers to the fact that the agent tries to learn (i.e. improve its policy's performance) without learning the dynamics of the environment governed by $p$, nor the reward function $r$.
Though their origins in RL can be dated from several decades ago with the algorithm REINFORCE \cite{Williams92}, the name \textit{Policy Gradient} appearing only in 2000 in \cite{Sutton99}, they recently regained interest thanks to many remarkable achievements, to name a few: in continuous control \cite{Lillicrap15, Schulman15, Schulman17PPO} and
natural language processing with GPT-3 \cite{Brown20}\footnote{instructGPT and chatGPT are trained with Proximal Policy Optimization, see \url{https://openai.com/blog/chatgpt/}.}.
See the blog post \cite{weng18} that lists important PG methods and provides a concise introduction to each of them.


\paragraph{Max-entropy RL.}
More generally, PG methods are considered more suitable for large (possibly continuous) state and action spaces than other nonetheless important methods such as Q-learning and its variations.
However, for large spaces, the exploitation-exploration dilemma becomes more challenging.
In order to enhance exploration, it has become standard to use a regularization to the objective, as in \textit{max-entropy RL}\cite{Nachum17,ODonoghue16,Schulman17}, where the agent maximizes the sum of its rewards plus a bonus for the entropy of its policy\footnote{Other regularization techniques are used and studied in the literature, we focus on entropy regularized RL in this paper.}.
Not only max-entropy RL boosts exploration, it also yields an optimal policy that is stochastic, in the form of a Boltzmann measure, such that the agent keeps taking actions at random while maximizing the regularized objective.
This is sometimes preferable than deterministic policies.
In particular, \cite{Eysenbach21} shows that the max-entropy RL optimal policy is robust to adversarial change of the reward function (their Theorem 4.1) and transition probabilities (their Theorem 4.2); see also references therein for more details on that topic.
Finally, max-entropy RL is appealing from a theoretical perspective.
For example, soft Q-learning, introduced in \cite{Haarnoja17} (see also \cite{Haarnoja18SoftAA,Haarnoja18SoftAO} for implementations of soft Q-learning with an actor-critic scheme), strongly resembles PG in max-entropy RL \cite{Schulman17}; max-entropy RL has also been linked to variational inference in \cite{Levine18}.
Other appealing features of max-entropy RL are discussed in \cite{Eysenbach19} and references therein.


\paragraph{Convergence guarantees of PG.}
The empirical successes motivated the RL community to build a solid theory for PG methods that is lacking.
Indeed, besides the well-known \textit{Policy Gradient Theorem} (see Chapter 13 in \cite{Sutton18}) that can imply convergence of PG (provided good learning rate and other assumptions), for many years, not much more was known about the global convergence of PG (i.e. convergence to an optimal policy) until recently.
Despite the numerous gaps that remain, some important progress have already been made.
In particular, the global convergence of PG methods has been studied and proved in specific settings, see for instance \cite{Fazel18,Agarwal22,Bhandari19,Mei20,Zhang20SampleER,Zhang19GlobalCO,Cen20,Ding21,Wang19, Agazzi20, Bhandari20OnTL, Leahy22, Guin22}.
Convergence guarantees often come with convergence rates (with or without perfect gradient estimates).
Though strengthening the trust in PG methods for practical tasks, most of the theoretical guarantees obtained in the literature so far require rather restrictive assumptions, and often assume that the action-state space of the MDP is finite (but not always, e.g. \cite{Agazzi20} addresses continuous action-state space for neural policies in the mean-field regime and \cite{Leahy22} proves global convergence when adding enough regularization on the parameters.)
In particular, \cite{Li21} shows that many convergence rates that have been obtained in the literature ignore some parameters such as the size of the state space.
After making the dependency of the bounds on these parameters explicit, they manage to construct environments where the rates blow up and convergence takes super-exponential time.



\begin{comment}
\paragraph{An intrinsic issue with RL.}

One notable difficulty that PG (but not only PG) faces is that the policy is updated by using that policy's own performance.
Suppose that at a state $s_0$, the agent can go either up or down.
Suppose that in the region downwards, the agent has bad performances, and that in the region upwards, the agent performs close to optimally.
Then, after few ups and downs from $s_0$, the agent will reinforce the action ``up'' because it consequently received better rewards.
However, this learning signal may be leading the agent away from an optimal policy, possibly forever (except in the specific settings of the works cited above).
In the schematic figure \ref{fig: bad example}, the agent at $s_0$ will learn to go up, even though it thus gets away from a great reward (represented by the diamond, close to the region where the agent performs very poorly).
On the other hand, if the agent has a perfect evaluation of all other states than $s_0$, then the learning signal at $s_0$ is perfectly aligned towards the optimal policy.
\begin{figure}
    \centering
    \includegraphics[width = 0.8\textwidth]{ExampleBadRL.png}
    \caption{Misleading signal received at $s_0$.}
    \label{fig: bad example}
\end{figure}
\end{comment}

\paragraph{Fixed-horizon RL.}


A vast number of works on RL have focused on either infinite horizon tasks, or episodic tasks where the length of an episode is random. In both these cases, policies only depend on the current state of the agent.
In \cite{Ernst03}, the fixed, finite horizon optimal policy is used as an approximation, as the horizon grows to infinity, to approximate the infinite-horizon optimal policy.
Nonetheless and even though the fixed (deterministic) horizon setting has received less attention, the benefits of fixing the horizon are multiple and have been investigated in recent relevant works, such as \cite{Asis19} and \cite{Guin22}.
With a fixed horizon, policies are time-dependent, and are usually called \textit{non-stationary} policies, as in dynamic programming \cite{Bertsekas1995DynamicPA}.

The authors in \cite{Asis19} construct a sequence of value and Q-functions to solve a fixed-horizon task.
Training is done with a \textit{Temporal Difference} (TD) algorithm, which is \textbf{not} a PG method.
TD involves bootstrapping, and when it is used offline (off-policy) together with function approximation, it encounters the well-known stability issue called the \textit{deadly triad}, see \cite{Sutton18} Section 11.3.
By using horizon-dependent value functions, they do not rely on bootstrapping, getting rid of one element of the triad, thus ensuring more stability.
It is worth noting that thanks to the fixed-horizon setting, they empirically overcome the specific Baird's example of divergence.

The preprint \cite{Guin22} exploits a similar idea with an actor-critic method for constrained RL, where the agent aims at maximizing the cumulative rewards while satisfying some given constraints.
To guarantee convergence, they assume a condition given by their Equation (20).
Roughly speaking, this condition is that smaller horizon policies are closer to convergence than larger horizon policies.
Therefore, they prove convergence of the training algorithm through a cascade of convergence, and provide convergence rates.
%The MPG setting that we introduce greatly differs from their work, in particular the MPG objective is not constrained and includes an entropy regularization.
%In particular, though we use a ``cascading'' argument to propagate the global optimality to all policies, MPG does not need smaller horizon policies to be closer to convergence to ensure it.
%In particular, specific properties of entropy regularization yields that we do not need to assume that smaller horizon policies are closer to convergence than larger horizon policies during training.
%Not only is convergence guaranteed for MPG, but the global optimality is also established under appropriate assumptions; it is interesting to note that we also make use of a ``cascading'' argument to ensure global optimality.
%Note however that in \cite{Guin22}, convergence rates are provided.

Let us also mention \cite{Seijen19} investigating the impact of the discount factor when optimizing a discounted infinite-horizon objective evaluated on a finite-horizon undiscounted objective.
They empirically found that for some tasks, lower discount factors (thus closer to a fixed-horizon objective) lead to better performance.

\subsection{Contributions}

We consider the function approximation setting with log-linear parametric policies, that are constructed as the softmax of linear models.
The main contributions of this work are:
\begin{enumerate}[label = (\roman*)]
    \item We define the fixed-horizon max-entropy RL objective and introduce a new algorithm (Equation \eqref{eq: update cascade learning}), named \textit{Matryoshka Policy Gradient} (MPG).
    \item We establish global convergence for continuous state and action space: under the realizability assumption, MPG converges to the unique optimal policy (Theorem \ref{thm: general case global optimality}). When the realizability assumption does not hold, we prove uniqueness of the optimal policy and prove global convergence of MPG (Theorem \ref{Thm: global cvg outside of the RKHS}).
    \item We approximate arbitrarily well the optimal policy for the infinite horizon objective by the optimal policy of the MPG objective (Proposition \ref{prop: extending horizon converges to standard optimal policy}).
    \item In the case where the policy is parametrized as the softmax of a (deep) neural network's output, we describe the limit of MPG training in terms of the \textit{neural tangent kernel} and the \textit{conjugate kernel} of the neural network at the end of training (Corollary \ref{thm: gen case deep RL global optimality}). In particular, MPG globally converges in the \textit{lazy regime}.
    \item Numerically, we successfully train agents on standard simple tasks without relying on RL tricks, and confirm our theoretical findings (see Section \ref{Section: numerical experiments}). 
\end{enumerate}

In \cite{Agarwal22}, many convergence guarantees are given for different policy gradient algorithms.
In particular, in the tabular case with finite state and action space, global convergence is obtained thanks to the gradient domination property.
One advantage is that the rate of convergence can be deduced, see e.g. Section 4 in \cite{Agarwal22}.
In the case of infinite state space (and possibly action space, see their Remark 6.8), in the function approximation setting, they obtain convergence results for the \textit{natural policy gradient} algorithm, which uses the Fisher information matrix induced by the policy in the update, but do not guarantee the optimality of the limit.
In contrast, MPG only uses the gradient of the policy and globally converges.

Let us stress the main strength of MPG regarding its convergence guarantees:
\begin{enumerate}[label = \textbullet]
    \item State space and action space can be continuous.
    \item Global convergence is guaranteed in the function approximation setting, independently from the parameters' initialization.
    \item Even when the realizable assumption does not hold, MPG converges to the unique optimal policy in the parametric policy space.
    We moreover characterise the global optimum as the unique policy satisfying the \textit{projectional consistency property}.
\end{enumerate}

%On a more practical point of view, besides contribution (iv), it turns out that MPG is well suited for policies parametrized by a neural network.
%Indeed, MPG relies on the fact that the agent's policy learns to optimize the cumulative rewards for several horizons.
%One assumption to ensure global convergence of MPG is that the policies for different horizons do not share any parameters.
%However, it is intuitive that optimizing for the next 50 rewards or optimizing for the next 51 rewards should yield similar policies (for smooth enough, relevant environments).
%Hence, our heuristic is that plugging the horizon as an input in a single neural network encoding all different horizons' policies should \underline{enhance} rather than \underline{harm} training.
%More details on neural policies in Appendix \ref{appendix: neural networks}

In our numerical experiments described in Section \ref{Section: numerical experiments}, we first consider an analytical task and verify the global convergence property of the MPG: MPG consistently finds the unique global optimum, which satisfies the projectional consistency property. Then, we study two benchmarks from OpenAI: the Frozen Lake game and the Cart Pole. We obtain successful policies for both benchmarks with a very simple implementation of the MPG algorithm, comparing also to a simple vanilla PG method \cite{Sutton99}. Rather than competing with the state-of-the-art algorithms, our aim is to provide a proof of concept by showing that successful training can be obtained without using standard RL tricks that are known to improve training performance. Standard tricks are nevertheless very straightforward to apply to MPG and we hope that more general and bigger scale experiments implementing variations of MPG will follow the present work.





\begin{comment}
\paragraph{Resemblance with more distant ideas.}



The subfield of RL based on the idea to solve a sequence (or more generally, an acyclic graph where each path from the root to a leaf corresponds to a sequence) of related tasks with increasing complexity, in order to make learning easier and achieve a difficult task is called \textit{Curriculum Learning} for RL; see \cite{Narvekar20} for an overview of curriculum learning in RL.
The proof of global optimality for MPG uses that if each $i$-step policy, $i=1,\ldots,n-1$ has converged to the respective optimal $i$-step policy, then the $n$-step policy is in turn guaranteed to converge to the optimal $n$-step policy.
Although the motivations seem roughly similar, MPG cannot be directly cast into the curriculum framework.
\end{comment}


\begin{comment}
\section{Single state case}

In the single state case (also known as the \textit{bandit problem}), MPG is identical to the usual PG for the max-entropy objective.
Nevertheless, we study this case first as it helps understanding the general state space case.

\subsection{Definitions}

Consider the the case when the state space is restricted to a single state $\mathcal{S}=\{s_0\}$.
We suppose moreover that the action space $\mathcal{A}$ is finite â€“ the argument for $\mathcal{A}$ a continuous compact real subset requires some technical adaptation that we believe are unnecessary for our presentation.
In the bandit problem, at each discrete time step, an agent takes an action $a\in\mathcal{A}$ and receives a random reward $R(a)$ with law $p_{\mathrm{rew}}(\cdot|a)$ on $\RR$.
We assume that rewards are bounded, and denote the expected reward map $r(a):=\EE[R(a)]$.
The agent chooses actions according to a \textit{policy}, denoted by $\pi$, which is a probability distribution on $\mathcal{A}$.
If $\pi\neq\delta_{a}$ for all $a\in\mathcal{A}$, we say that $\pi$ is a \textit{stochastic policy} and at a given time, we denote by $A$ the random action the agent selects.
The objective function $J$ to maximize maps policies $\pi$ to
\begin{align*}
	J(\pi):=\mathbb{E}_{\pi}\left[r(A)\right]-\tau\DKL{\pi}{\overline{\pi}},
\end{align*}
where $\DKL{p}{q}:=\sum_{a\in\mathcal{A}}p(a)\log\frac{p(a)}{q(a)}$ is the Kullback-Leibler divergence, $\tau>0$ is the temperature parameter governing the strength of the entropy regularization and $\overline{\pi}$ is a baseline policy with full support; for example the uniform measure on $\mathcal{A}$ gives entropy bonuses shifted by $-\log|\mathcal{A}|$.
Regularizing with the Kullback-Leibler divergence is thus more general than with entropy bonuses and this is the regularization that we consider in this paper, akin \cite{Schulman17}.
The optimal policy $\pi_*=\argmax{\pi}J(\pi)$ is given by
\begin{align}\label{eq: bandit case optimal policy}
	\pi_*(a)=\overline{\pi}(a)\frac{\exp(r(a)/\tau)}{\mathbb{E}_{\overline{\pi}}[\exp(r(A)/\tau)]},
\end{align}
see e.g. Equation (2) in \cite{Schulman17}.

\subsection{Parametrization of the agent}

Applying the softmax function (with baseline measure $\overline{\pi}$ and temperature parameter $\tau$) to a map $h:\mathcal{A}\to\RR$ yields the Boltzmann measure
\begin{align*}
    a\mapsto\overline{\pi}(a)\frac{\exp(h(a)/\tau)}{\sum_{a'\in\mathcal{A}}\overline{\pi}(a')\exp(h(a')/\tau)}.
\end{align*}
In this context, $h(a)$ is called the \textit{preference} of $a$ of the agent.
We choose a linear model for the preferences: let $\Theta:\mathcal{A}\times\mathcal{A}\to\RR$ be a symmetric positive semi-definite kernel and denote by $\mathcal{H}$ the induced reproducible kernel Hilbert space (RKHS).
For all parameters $\theta\in\RR^{\nparam}$, where $\nparam$ denotes the number of parameters of the model, we define
\begin{align*}
    h_{\theta}(a):=\theta\cdot\psi(a),
\end{align*}
where $\psi(a)$ is a feature map associated with the kernel $\Theta$.
Then, we define the agent's policy by
\begin{align*}
	\pi_\theta:a\mapsto\overline{\pi}(a)\frac{\exp(h_\theta(a)/\tau)}{\sum_{a'\in\mathcal{A}}\overline{\pi}(a')\exp(h_\theta(a')/\tau)}.
\end{align*}


\subsection{Policy Gradient}
\label{Section: Training with policy gradient}

Let $t\in\NN$ be the time of training, starting from $t=0$ at initialisation.
At step $t$, we denote the parameters by $\theta_t$, and we write $\pi_t,h_t$ respectively for $\pi_{\theta_t},h_{\theta_t}$.
Without loss of generality, we assume that the agent's policy is updated at every step; we thus write $A_t$ for the (random) action taken at step $t$ under $\pi_t$ and $R_t$ the corresponding reward.
Let $(\mathcal{F}_t)_{t\geq 0}$ be the natural filtration of the agent, that is generated by $(\pi_t,A_t,R_t)_{t\geq 0}$.
We denote by $\EE_{\pi_t}$ the conditional expectation given $\mathcal{F}_t$ under $\pi_t$.
Let $\learningrate$ be the learning rate.
The ideal PG updates are as follows:
\begin{align}\label{Equation update theta}
	\theta_{t+1}
	=\theta_t+\learningrate\EE_{\pi_t}\left[\Big(r(A_t)-\DKL{\pi_t}{\overline{\pi}}\tau\Big)\frac{\nabla_\theta\pi_t(A_t)}{\pi_t(A_t)}\right].
\end{align}
In practice, one can use the estimate
\begin{align*}
	\theta_{t+1}
	=\theta_t+\learningrate\left(R_t-\tau\log\frac{\pi_t(A_t)}{\overline{\pi}(A_t)}-v_t\right)\frac{\nabla_\theta\pi_t(A_t)}{\pi_t(A_t)},
\end{align*}
where $v_t$ is a baseline, chosen by the user, at time $t$ that does not depend on the action, whose role is to reduce the variance, without changing the expectation of the update.
In particular, this estimate is unbiased.

Let us consider the ideal PG update \eqref{Equation update theta}.
Note that $\theta_{t+1}-\theta_t=\learningrate\nabla_\theta J(\pi_t)$ since we have
\begin{align}\label{Equation gradient KL divergence}
	\nabla_\theta\mathrm{D_{KL}}(\pi_t||\overline{\pi})
	&=\nabla_\theta\sum_{\ell=1}^{|\mathcal{A}|}\pi_t(a_\ell)\log\frac{\pi_t(a_\ell)}{\overline{\pi}(a_\ell)}\nonumber\\
	&=\sum_{\ell=1}^{|\mathcal{A}|}\left(\log\frac{\pi_t(a_\ell)}{\overline{\pi}(a_\ell)}+1\right)\nabla_\theta\pi_t(a_\ell)\nonumber\\
	&=\sum_{\ell=1}^{|\mathcal{A}|}\log\frac{\pi_t(a_\ell)}{\overline{\pi}(a_\ell)}\nabla_\theta\pi_t(a_\ell),
\end{align}
by a basic property of softmax policies, see \eqref{eq: softmax gradient cancels expected constant} in the Appendix.

\subsection{Convergence}

The action space being finite, the uniform convergence of a probability measure on $\mathcal{A}$ corresponds to the pointwise convergence.
This is the notion of convergence that we consider in the next statement, that is, we say that $\pi_t\to\pi_\infty$ if and only if $\sup_{a\in\mathcal{A}}|\pi_t(a)-\pi_\infty(a)|\to 0$ as $t\to\infty$


Recall that $\theta_{t+1}-\theta_t=\learningrate\nabla_\theta J(\pi_t)$; the next result is straightforward.

\begin{lem}\label{lem: bandit case convergence}
    There exists $\eta_0>0$ such that if $\eta<\eta_0$, then $\pi_t$ converges to some policy $\pi_\infty$, as $t\to\infty$.
\end{lem}


It is not clear a priori that $\pi_\infty$ is close to $\pi_*$.
We address this question in the next section.


\subsection{Global optimality}

In order to guarantee global optimality, we will assume (explicitly recalled when needed) that
\begin{enumerate}[label = \textbf{A\arabic*.}]
    \item $\Theta$ is \textit{strictly} positive definite, i.e. $(\Theta(a,a'))_{a,a'\in\mathcal{A}}$ is invertible. \label{assumption: pd kernel}
\end{enumerate}


In general, when the kernel $\Theta$ is positive-semidefinite, the so-called Mercer's Theorem entails that for all $a,a'\in\mathcal{A}$,
\begin{align*}
    \Theta(a,a')=\sum_{i=1}^{|\mathcal{A}|} \lambda_i e_i(a)e_i(a'),
\end{align*}
where $(e_i(a),\lambda_i)_{i=1,\ldots,|\mathcal{A}|}$ are eigenvector/eigenvalue pairs of $(\Theta(a,a'))_{a,a'\in\mathcal{A}}$, $\{(e_i(a))_{a\in|\mathcal{A}|};i=1,\ldots,|\mathcal{A}|\}$ being an orthonormal basis of $\RR^{|\mathcal{A}|}$.
The eigenvalues are all non-negative and assuming moreover \ref{assumption: pd kernel}, each is also non-zero.
We write $u\perp v$ to denote that two vectors $u$ and $v$ are orthogonal.

\begin{thm}[Global optimality]\label{thm: global optimality}
    Let $\pi_\infty$ be the policy from Lemma \ref{lem: bandit case convergence} such that $\pi_t\to\pi_\infty$ as $t\to\infty$.
    Define the vector $d\in\RR^{|\mathcal{A}|}$ by
    \begin{align*}
        d_a:=\pi_\infty(a)\left(\log\frac{\pi_{\infty}}{\pi_*}(a)-\DKL{\pi_\infty}{\pi_*}\right),\quad a\in\mathcal{A}.
    \end{align*}
    Then for all $i\in\{1,\ldots,|\mathcal{A}|\}$, we have $\lambda_i>0\ \Rightarrow\ d\perp e_i$.
    In particular, if \ref{assumption: pd kernel} holds, then $d=0$, and $\pi_\infty\equiv\pi_*$.
\end{thm}

As we explained in Introduction, global optimality of PG for finite state space (here $|\mathcal{S}|=1$) however we will extend the ideas in the proof to the more complex and interesting case of a continuous compact state space, where the environment does not only generate a reward from an action, but also sends the agent to a new state, influencing its future rewards as well.
We also note that the theorem extends to the case of $\mathcal{A}$ continuous and compact, for which Mercer's theorem applies similarly.

\end{comment}


\section{Fixed-horizon max-entropy RL}

In this section, we introduce the fixed-horizon max-entropy RL, describe its optimal policy and establish some of its properties.


\subsection{Definitions}

\paragraph{Markov Decision Process}

The agent evolves according to a Markov Decision Process (MDP) characterized by the tuple $(\mathcal{S},\mathcal{A},p,p_{\mathrm{rew}})$\footnote{Implicitely assumed in the MDP definition is the fact that all variables such that actions, visited states and rewards are measurable, so that they are well-defined random variables.}.
The action space can be state dependent $\mathcal{A}_s$, nonetheless we assume for simplicity that it is the same regardless of the state.
We assume that the action and state spaces $\mathcal{A},\mathcal{S}\subset\RR^d$ are closed sets.
Let $s'\mapsto p(s,a,s')$ be the probability (the density if $\mathcal{S}$ is continuous) that the agent moves from $s\in\mathcal{S}$ to $s'\in\mathcal{S}$ after taking action $a\in\mathcal{A}$.
When $p(s,a,s')=\delta_{s',f(s,a)}$ for some $f:\mathcal{S}\times\mathcal{A}\to\mathcal{S}$, then we say that the transitions are deterministic.
The reward depends on the action and on the current state, its law is denoted by $p_{\mathrm{rew}}(\cdot|s,a)$.
To ease the presentation, we assume that the rewards are uniformly bounded and for all $(s,a)\in\mathcal{S}\times\mathcal{A}$, we denote by $r(a,s)$ the mean reward after taking action $a$ at state $s$.
All random variables are such that the process is Markovian.

A simple (i.e. stationary) policy $\pi:\mathcal{A}\times\mathcal{S}\to\intervalleff{0}{1}$ is a map such that for all $s\in\mathcal{S}$, $\pi(\cdot|s)$ is a probability distribution on $\mathcal{A}$ that describes the law of the action taken by the agent at state $s$.
Let $\mathcal{P}$ denote the set of simple policies.
Let $n\in\NN$ be some fixed horizon, we denote by $\mathcal{P}_n$ the set of (non-stationary) policies $\pi=(\pi^{(1)},\ldots,\pi^{(n)})$ where for all $i=1,\ldots,n$, $\pi^{(i)}\in\mathcal{P}$.
We say that the agent follows a policy $\pi\in\mathcal{P}_n$ if and only if it chooses its actions sequentially according to $\pi^{(n)}$, then $\pi^{(n-1)}$, and so on until $\pi^{(1)}$ and the end of the episode.
That is for each episode of fixed length $n$, starting from a given state $S_0$, the agent generates a path $S_0,A_0,S_1,A_1,\ldots, A_{n-1},S_n$, where $A_i\sim\pi^{(n-i)}(\cdot|S_i)$ and $S_{i+1}\sim p(S_i,A_i,\cdot)$.
Note that in the standard infinite horizon setting, a policy corresponds to an infinite sequence $\{(\pi,\pi,\pi,\cdots); \pi\in\mathcal{P}\}\subset\mathcal{P}_\infty$.

Henceforth, we assume that $\mathcal{A}$ and $\mathcal{S}$ are continuous, the results identically holding true when they are countable.
We also assume that
\begin{itemize}
    \item the MDP is irreducible, in the sense given in \eqref{eq: gen case irreducibility} in Appendix \ref{appendix: assumptions};
    \item The law of $S_0$ is has full support in $\mathcal{S}$ and for any $\pi\in\mathcal{P}_n$, the law of $S_i$ is absolutely continuous w.r.t. the Lebesgue measure, for all $i=0,\cdots,n$.
    \item The reward function $(a,s)\mapsto r(a,s)$ and the transitions  $(s,a,s')\mapsto p(s,a,s')$ are continuous with respect to the Euclidean metric.
\end{itemize}
The second item avoids unnecessary technical considerations (such as non-uniqueness of the optimal policy e.g. when a state is never visited).
The third item ensures the standard measurable selection assumption, see \cite{Hernandez99} Section 3.3.


\paragraph{Value and Q functions.}

For every $s\in\mathcal{S}$ and $\pi,\pi'\in\mathcal{P}$, we denote by $\mathrm{D_{KL}}(\pi||\pi')(s)=\mathrm{D_{KL}}(\pi(\cdot|s)||\pi'(\cdot|s))$ the Kullback-Leibler divergence between $\pi(\cdot|s)$ and $\pi(\cdot|s')$, defined as
\begin{align*}
    \DKL{\pi}{\pi'}(s):=\int_\mathcal{A}\log\frac{\pi}{\pi'}(a|s)\pi(\mathrm{d}a),
\end{align*}
and is set to $\infty$ if $\pi'(\cdot|s)$ is not absolutely continuous with respect to $\pi(\cdot|s)$.

To regularize the rewards, we add a penalty term that corresponds to the Kullback-Leibler (KL) divergence of the agent's policy and a baseline policy.
In practice, the baseline policy can be used to encode a priori knowledge of the environment; a uniform baseline policy corresponds to adding entropy bonuses to the rewards.
Regularizing with the KL divergence is thus more general than with entropy bonuses and this is the regularization that we consider in this paper, akin \cite{Schulman17}.

We denote by $\overline{\pi}$ the arbitrary baseline policy and let us assume for conciseness that $\overline{\pi}\in\mathcal{P}$.
We define the $n$-step value function $V_{\pi}^{(n)}:\mathcal{S}\to\mathbb{R}$ induced by a policy $\pi\in\mathcal{P}_n$ as
\begin{align*}
	V_{\pi}^{(n)}:s\mapsto
	\mathbb{E}_{\pi}\left[\sum_{k=0}^n(R_k-\tau\DKL{\pi^{(n-k)}}{\overline{\pi}}(S_k))\Big|S_0=s\right],
\end{align*}
where the expectation is along the trajectory of length $n$ sampled under policy $\pi=(\pi^{(1)},\ldots,\pi^{(n)})$.
Note that we have
\begin{align}\label{eq: gen case recursive definition value function}
    V^{(n)}_{\pi}(s)
    = \EE_{\pi^{(n)}}[R_0] - &\tau\DKL{\pi^{(n)}}{\overline{\pi}}(s)+\EE_{\pi^{(n)}}[V^{(n-1)}_{\pi'}(S_1)],
\end{align}
where $\pi'=(\pi^{(1)},\ldots,\pi^{(n-1)})\in\mathcal{P}_{n-1}$, and $S_1\sim\int_{\mathcal{A}}p(s,a,\cdot)\pi^{(n)}(\mathrm{d}a|s)$.
It is common to add a discount factor $\gamma\in\intervalleof{0}{1}$ to the rewards to favor more the quickly obtainable rewards.
In the infinite horizon case ($n=\infty$), this ensures that the cumulative reward is finite a.s. (provided finite first moment).
Our study trivially applies to the case where the rewards are discounted.

The $n$-step entropy regularized $Q$-function induced by $\pi$ is defined as
\begin{align}\label{eq: gen case definition Q-function}
    Q_{\pi}^{(n)}:(a,s)\mapsto r(a,s)+\int_{\mathcal{S}}p(s,a,\mathrm{d}s')V_{\pi'}^{(n-1)}(s').
\end{align}

\textbf{Notation:}
Henceforth, for a policy $\pi\in\mathcal{P}_n$, we use the abuse of notation $V_\pi^{(i)}$ for $i<n$ for the $i$-step value function associated with $(\pi^{(1)},\ldots,\pi^{(i)})$, and similarly for the $Q$ functions and other quantities of interest, when the context makes it clear which policy is used.


\subsection{Objective and optimal policy}

The standard discounted max-entropy RL objective is defined for simple policies $\pi\in\mathcal{P}$ by
\begin{align}\label{eq: standard max-RL objective}
    J(\pi):= &\int_{\mathcal{S}}\EE_{\pi_t}\bigg[\sum_{k=0}^T\gamma^k\left(R_k-\DKL{\pi_t}{\overline{\pi}}(S_k)\right)\Big|S_0=s\bigg]\nu_0(\mathrm{d}s),
\end{align}
where $T\in\NN\cup\{\infty\}$ is the horizon and $\nu_0$ is the initial state distribution, see e.g. \cite{Eysenbach21} and references therein.
It is often assumed that $T$ is random and therefore $\pi$ is simple.

Instead of the above objective, we define the objective function as follows:
\begin{align}\label{def: general case objective function}
    J_n(\pi)
    :=\int_{\mathcal{S}}V_{\pi}^{(n)}(s)\nu_0(\mathrm{d}s),
\end{align}
where we assume that the initial state distribution $\nu_0$ has full support in $\mathcal{S}$, to avoid unnecessary technical considerations on reachable states (the optimal policy depends on $\nu_0$ only through its support). 
Since we assume that the rewards are bounded and by compactness of $\mathcal{S}$, the objective function above is itself bounded.

%Let $L^2$ be the space of square-integrable real maps $f$ defined on $\mathcal{A}\times\mathcal{S}$, that is, such that
%\begin{align*}
%    \int_{\mathcal{A}\times\mathcal{S}}f(a,s)^2\mathrm{d}a\mathrm{d}s<\infty.
%\end{align*}
%With a slight abuse of notation, we write $\pi=\widetilde{\pi}$ in $L^2$ to say that $\pi^{(i)}=\widetilde{\pi}^{(i)}$ in $L^2$, for all $i=1,\ldots, n$.

We say that a policy $\pi\in\mathcal{P}_n$ is optimal if and only if $J_n(\pi)\geq J_n(\pi')$ for all $\pi'\in\mathcal{P}_n$.
The existence and unicity of the optimal policy is established by the next proposition, providing in passing its explicit expression.

\begin{prop}\label{prop: optimal policy}
    There exists a unique optimal policy (Lebesgue almost-everywhere), denoted by $\pi_*=(\pi_*^{(1)},\ldots,\pi_*^{(n)})\in\mathcal{P}_n$.
    The $i$-step optimal policies, $i=1,\ldots,n$, can be obtained as follows: for all $a\in\mathcal{A}$, $s\in\mathcal{S}$,
    \begin{align*}
        \pi_*^{(1)}(a|s)
        =\frac{\overline{\pi}(a|s)\exp(r(a,s)/\tau)}{\EE_{\overline{\pi}}[\exp(r(A,s)/\tau)]}, \quad
        \pi_*^{(i+1)}(a|s)
        =\frac{\overline{\pi}(a|s)\exp\left(Q_*^{(i+1)}(a,s)/\tau\right)}{\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(i+1)}(A,s)/\tau\right)\right]},
    \end{align*}
    where $Q_*^{(i+1)}$ is a short-hand notation for $Q_{\pi_*}^{(i+1)}$ recursively defined as in \eqref{eq: gen case definition Q-function}.
\end{prop}

By continuity and uniform boundedness of the reward function $r$ and the transition function $p$, one has that the policies $\pi_*^{(i)}$ are absolutely continuous with respect to the Lebesgue measure (when $\mathcal{A},\mathcal{S}$ are continuous).





\begin{lem}\label{lem: general case V_* = log expectation}
    For all $s\in\mathcal{S}$ and $n\geq 1$, it holds that
    \begin{align*}
        V_*^{(n)}(s)
        =\tau\log\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(n)}(A,s)/\tau\right)\right],
    \end{align*}
    where $V_*^{(0)}(s')=0$.
\end{lem}



Thanks to Lemma \ref{lem: general case V_* = log expectation}, we can write more concisely
\begin{align}\label{eq: general case optimal policy}
    \pi_*^{(i)}(a|s)
    &=\overline{\pi}(a|s)\exp\left(\left(Q_*^{(i)}(a,s)-V_*^{(i)}(s)\right)/\tau\right).
\end{align}
    


For all $n,m\in\NN$ such that $n>m$, we define the operator $T_{n,m}:\mathcal{P}_n\to\mathcal{P}_m$ by
\begin{align}\label{eq: definition translation operator}
    T_{n,m}:
    (\pi^{(1)},\ldots,\pi^{(n)}) & \mapsto (\pi^{(1)},\ldots,\pi^{(m)}).
\end{align}
In Proposition \ref{prop: extending horizon converges to standard optimal policy} below, for all $n\in\NN$, we denote by $\pi_{*,n}\in\mathcal{P}_n$ the optimal policy for $J_n$ with discounted rewards.
We write the standard discounted, infinite horizon entropy-regularized RL objective $J_\infty$,
%, with discounted factor $\gamma\in\intervalleoo{0}{1}$ (i.e. $V_{\pi}(s)=\EE[R_0-\DKL{\pi}{\overline{\pi}}+\gamma V_{\pi}(S_1)]$ with $\pi\in\mathcal{P}_1$).
and denote by $\pi_{*,\infty}$ its optimal policy.

\begin{prop}\label{prop: extending horizon converges to standard optimal policy}
    We have:
    \begin{enumerate}[label = (\roman*)]
        \item As $n\to\infty$, the policy $\pi_{*,n}^{(n)}$ converges to $\pi_{*,\infty}$, in the sense that
        \begin{align*}
            \lim_{n\to\infty}\int_{\mathcal{A}\times\mathcal{S}}\left|\pi_{*,n}^{(n)}(a|s)-\pi_{*,\infty}(a,s)\right|\mathrm{d}a\mathrm{d}s=0.
        \end{align*}
        \item for all $n,m\in\NN$ such that $n>m$, it holds that $T_{n,m}(\pi_{*,n})=\pi_{*,m}$.
    \end{enumerate}
\end{prop}

The above Proposition \ref{prop: extending horizon converges to standard optimal policy} is intuitive:
item (i) shows that one can learn the standard discounted entropy-regularized RL objective by incrementally extending the agent's horizon;
item (ii) goes the other way and shows that the optimal policy for large horizon is built of smaller horizons policies in a consistent manner.


\section{Matryoshka Policy Gradient}

\subsection{Policy parametrization}
Let $\Theta^{(i)}:(\mathcal{A}\times\mathcal{S})^2\to\RR$ be a positive-semidefinite kernel.
For $i\in\{1,\ldots,n\}$, let $\theta^{(i)}\in\RR^{\nparam_i}$ be the parameters of a linear model $h_{\theta^{(i)}}^{(i)}:\mathcal{A}\times\mathcal{S}\to\mathbb{R}$, that outputs for all $(a,s)\in\mathcal{A}\times\mathcal{S}$ the $i$-step preference $h_{\theta^{(i)}}^{(i)}(a,s)$ for action $a$ at state $s$,
that is,
\begin{align*}
    h_{\theta^{(i)}}^{(i)}(a,s):=\theta^{(i)}\cdot\psi^{(i)}(a,s),
\end{align*}
where $\psi^{(i)}:\mathcal{A}\times\mathcal{S}\to\RR^\nparam$ is a feature map associated with the kernel $\Theta^{(i)}$.
The space of such functions correspond to the \textit{reproducible kernel Hilbert space} (RKHS) associated with the kernel, that we denote by $\mathcal{H}_{\Theta^{(i)}}$.
The $i$-step policy $\pi_{\theta^{(i)}}^{(i)}$ is defined as the Boltzmann policy according to $h^{(i)}$, that is, for all $(a,s)\in\mathcal{A}\times\mathcal{S}$,
\begin{align*}
	\pi_{\theta^{(i)}}^{(i)}(a|s):=\overline{\pi}(a|s)\frac{\exp(h_{\theta^{(i)}}^{(i)}(a,s)/\tau)}{\int_{\mathcal{A}}\exp(h_{\theta^{(i)}}^{(i)}(a',s)/\tau)\overline{\pi}(\mathrm{d}a'|s)}.
\end{align*}
The gradient of the policy thus reads as
\begin{align}\label{eq: gradient policy pi}
    &\nabla\pi_{\theta^{(i)}}^{(i)}(a|s)
    =\pi_{\theta^{(i)}}^{(i)}(a|s)\int_{\mathcal{A}}\left(\delta_{a,\mathrm{d}a'}-\pi_{\theta^{(i)}}^{(i)}(\mathrm{d}a'|s)\right)\nabla h^{(i)}_{\theta^{(i)}}(a',s)/\tau.
\end{align}
Note that when $\mathcal{A},\mathcal{S}$ are finite, with Kronecker delta kernels $\Theta^{(i)}((a,s),(a',s'))=\delta_{a,a'}\delta_{s,s'}$, we retrieve the so-called \textit{tabular case} with one parameter $\theta_{s,a}$ per state-action pair $(s,a)$. We assume throughout the paper that the $\Theta^{(i)}$'s are continuous and bounded.

\subsection{Definition of the MPG update}

Policy gradient (PG) for max-entropy RL consists in following $\nabla_\theta J(\pi_\theta)$ for the standard objective \eqref{eq: standard max-RL objective}.
In our setting, the ideal PG update would be such that $\theta_{t+1}-\theta_t=\learningrate\nabla_\theta J_n(\pi_t)$.
We introduce Matryoshka Policy Gradient (MPG) as a practical algorithm that produces unbiased estimates of the gradient (see Theorem \ref{thm: policy gradient theorem} below).

Suppose that at time $t\in\NN$ of training, the agent starts at a state $S_0\sim\nu_0$.
To lighten the notation, we write $\pi_t^{(i)}:=\pi_{\theta_t^{(i)}}^{(i)}$.
We assume that the agent samples a trajectory according to the policy $\pi_t$, defined as follows:
\vspace{-0.1cm}
\begin{enumerate}[label = \textbullet]
    \item sample action $A_0$ according to $\pi_t^{(n)}(\cdot|S_0)$,
    \vspace{-0.1cm}
    \item collect reward $R_0\sim p_{\mathrm{rew}}(\cdot|S_0,A_0)$ and move to next state $S_1\sim p(S_0,A_0,\cdot)$,
    \vspace{-0.1cm}
    \item sample action $A_1$ according to $\pi_t^{(n-1)}(\cdot|S_1)$,
    \vspace{-0.1cm}
    \item $\cdots$
    \vspace{-0.1cm}
    \item stop at state $S_n$.
\end{enumerate}
\vspace{-0.1cm}
The MPG update is as follows: for $i=1,\ldots,n$,
\begin{align}\label{eq: update cascade learning}
    \theta_{t+1}^{(i)}
    &=\theta_t^{(i)} + \learningrate\sum_{\ell=n-i}^{n-1}\left(R_{\ell}-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_{\ell})\right)
    \nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i})\nonumber\\
    &=\theta_t^{(i)} + \learningrate C_i\nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i}),
\end{align}
where we just introduced the shorthand notation $C_i$.
We see that the $i$-step policy $\pi^{(i)}$ is updated using the $(i-\ell)$-step policies.

\subsection{Global convergence: the realizable case}

We say that a sequence of policies $(\pi_t)_{t\in\NN}\subset\mathcal{P}_n$ converges to $\pi\in\mathcal{P}_n$ if and only if,  as $t\to\infty$
\begin{align}\label{eq: def convergence of policies}
    \max_{i\in\{1,\ldots,n\}}\int_{\mathcal{A}\times\mathcal{S}}\left|\pi^{(i)}_t(a|s)-\pi^{(i)}(a|s)\right|\mathrm{d}a\mathrm{d}s\to 0.
\end{align}
%as $t\to\infty$.

With PG, the so-called \textit{Policy Gradient Theorem} (see Section 13.2 in \cite{Sutton18}) provides a direct way to guarantee convergence of the algorithm.
Our next theorem shows that MPG also satisfies a Policy Gradient Theorem for non-stationary policies.

\begin{thm}\label{thm: policy gradient theorem}
    With MPG as defined in \eqref{eq: update cascade learning}, it holds that $\EE[\theta_{t+1}-\theta_t]=\eta\nabla_{\theta}J_n(\pi_t)$.
    In particular, assuming ideal MPG update, that is, $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$, there exists $\eta_0>0$ such that if $0<\eta<\eta_0$, then $\pi_t$ converges as $t\to\infty$ to some $\pi_\infty\in\mathcal{P}_n$.
\end{thm}

We will specify in our statements when we assume the following:
\begin{enumerate}[label = \textbf{A\arabic*.}]
    %\item For all $i=1,\ldots, n$, $\Theta^{(i)}$ is \textit{strictly} positive definite, i.e. $\lambda_j^{(i)}>0$, $\forall j\geq 1$. \label{assumption: general case pd kernel}
    \item \textbf{Realizability assumption}
    There exists $\theta_*\in\RR^P$ such that $\pi_{\theta_*}=\pi_*$.\label{assumption: optimal policy in RKHS}
\end{enumerate}
The realizability assumption above can be equivalently written as: for all $i=1,\ldots,n$, there exists a map $C_i:\mathcal{S}\to\RR$ such that $(a,s)\mapsto Q_*^{(i)}(a,s)+C_i(s)\in\mathcal{H}_{\Theta^{(i)}}$,
where the maps $C_i$ are constant in $a$.
They play no role in the policies encoded by functions in the RKHS, since for a fixed $s$, shifting the preferences by a constant keeps the policy unchanged.

In the theorem below, we assume ideal MPG update, that is $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$.

\begin{thm}\label{thm: general case global optimality}
    %{\color{red}
    %Let $m\in\{1,\ldots,n\}$.
    %Suppose that for all $i=1,\ldots, m-1$, the policy $\pi^{(i)}_t\to\pi_*^{(i)}$ as $t\to\infty$.
    %Suppose moreover that $\pi^{(m)}$ converges too, and let $\pi^{(m)}_\infty$ be its limit.
    %Define the map $d:\mathcal{A}\times\mathcal{S}\to\RR$ by
    %\begin{align*}
    %    &d(a,s):=\mathbf{m}_{\pi_\infty}^{(m)}(s)\pi^{(m)}_\infty(a|s)\left(\log\frac{\pi^{(m)}_\infty}{\pi^{(m)}_*}(a|s)-\DKL{\pi^{(m)}_\infty}{\pi^{(m)}_*}(s)\right).
    %\end{align*}
    %Then, for all $j\geq 1$, we have $\lambda_j^{(m)}>0\ \Rightarrow\ d\perp e_j^{(m)}$ in $L^2$.
    %In particular, if \ref{assumption: general case pd kernel} holds, then $\pi_\infty^{(m)}=\pi_*^{(m)}$ in $L^2$.
    %}

    Under \ref{assumption: optimal policy in RKHS}, training with ideal MPG update converges to the optimal policy, that is $\lim_{t\to\infty}\pi_t=\pi_*$ in the sense of \eqref{eq: def convergence of policies}.
\end{thm}


\subsection{Global convergence: beyond the realizability assumption}
\label{section main: global convergence beyond the realizability assumption}




Let $\mathscr{P}_n=\{\pi_\theta;\theta\in\RR^P\}\subset\mathcal{P}_n$ be the set of parametric policies.
We now focus on the case $\pi_*\notin\mathscr{P}_n$, that is, Assumption \ref{assumption: optimal policy in RKHS} does not hold.
We give a sketch of the main ideas to extend Theorem \ref{thm: general case global optimality} to the non-realizable case, showing global convergence and providing a characterisation of the limit. All details are provided in Appendix \ref{appendix: general state space}

We focus on the $1$-step policy.
Suppose that $\vartheta\in\RR^P$ is a critical point of $\theta\mapsto J_n(\pi_\theta)$.
Since $Q_*^{(1)}(a,s)=r(a,s)$, one can show that show that
\begin{align}\label{eq: gradient is 0 sketch of proof}
    0
    &=\nabla_{\theta^{(1)}}J_n(\pi_\vartheta)
    =-\EE_{\pi_{\vartheta}}\left[\nabla_{\theta^{(1)}}\DKL{\pi_{\vartheta}^{(1)}}{\pi_*^{(1)}}(S_{n-1})\right],
\end{align}
where the law of $S_{n-1}$ only depends on $\pi_{\vartheta}^{(n)},\ldots,\pi_{\vartheta}^{(2)}$.
Since this law is fixed ($\vartheta$ is a critical point), the right-hand side above corresponds to the gradient of a \textit{Bregman divergence} on $\mathcal{P}$, which we denote by $D(\pi_{\vartheta}^{(1)},\pi^{(1)}_*)$.
Let $\pi_{\theta_*}^{(1)}\in\mathrm{argmin}_{\pi^{(1)}_{\theta}\in\mathscr{P}_1}D(\pi_{\theta}^{(1)},\pi_*^{(1)})$.
Bregman divergences satisfy a Pythagorean identity, which in particular implies that
\begin{align*}
    D(\pi_{\vartheta}^{(1)},\pi_*^{(1)})
    &= D(\pi_{\vartheta}^{(1)},\pi_{\theta_*}^{(1)}) + D(\pi_{\theta_*}^{(1)},\pi_*^{(1)}).
\end{align*}
Hence, we have by \eqref{eq: gradient is 0 sketch of proof} that
\begin{align*}
    0=-\nabla_{\theta^{(1)}}D(\pi_{\vartheta}^{(1)},\pi_{\theta_*}^{(1)}).
\end{align*}
We deduce that $\pi_{\vartheta^{(1)}}$ is a critical point of the $1$-step MPG objective, where the initial state distribution is prescribed by $\pi_\vartheta^{(i)}$ for $i=2,\ldots,n$, and where the rewards are given by $r_{\theta_*}:=h_{\theta_*}^{(1)}$.
In particular, Theorem \ref{thm: general case global optimality} applies and shows that $\pi_\vartheta^{(1)}=\pi_{\theta_*}^{(1)}$.
This also proves the uniqueness of $\pi_{\theta_*}^{(1)}$.

The argument propagates to larger horizons, by using that maxima can be taken in any order, which proves that the unique critical point $\pi_{\vartheta}$ of $J_n$ is globally optimal.
Formally, the following theorem completes the picture of the global convergence guarantees of MPG:


\begin{thm}\label{Thm: global cvg outside of the RKHS}
    There exists $\eta_0>0$ such that for all $0<\eta<\eta_0$, training with ideal MPG update converges and $\lim_{t\to\infty}\pi_t=\pi_{\theta_*}$ in the sense of \eqref{eq: def convergence of policies}, where $\pi_{\theta_*}=\mathrm{argmax}_{\pi_\theta\in\mathscr{P}_n}J_n(\pi_\theta)$ is unique and is the only critical point of $J_n$.
\end{thm}

Clearly, when $\pi_*\in\mathscr{P}_n$, then $\pi_\infty=\pi_*$ and we retrieve Theorem \ref{thm: general case global optimality}.

It turns out that $\pi_{\theta_*}$ can be characterised by a property of independent interest.
Let $\theta\in\RR^P$ and for all $i=1,\ldots,n$, let $\mathbf{m}^{(i)}$ be the law of state $S_{n-i}$ under policy $\pi_\theta$ with $\mathbf{m}^{(n)}=\nu_0$ by assumption.
Define $P_{i}:L^2(\mathbf{m}^{(i)}(\mathrm{d}s)\pi_\theta^{(i)}(\mathrm{d}a|s))\to\mathcal{H}_{\Theta^{(i)}}$ to be the orthogonal projection onto $\mathcal{H}_{\Theta^{(i)}}$ in the $L^2(\mathbf{m}^{(i)}(\mathrm{d}s)\pi^{(i)}_\theta(\mathrm{d}a|s))$ sense.    
We say that $\pi_\theta$ satisfies the \textit{projectional consistency property} if and only if for all $i=1,\ldots,n$, it holds that
%there exists a map $C_i:\mathcal{S}\to\RR$ (constant in $a$) such that
\begin{align}\label{eq: projectional consistency property}
    \pi^{(i)}_{\theta}(a|s)=\overline{\pi}(a|s)\frac{\exp\left(P_i Q_{\pi_{\theta}}^{(i)}(a,s)/\tau\right)}{\int_\mathcal{A}\overline{\pi}(\mathrm{d}a'|s)\exp\left(P_i Q_{\pi_{\theta}}^{(i)}(a',s)/\tau\right)}.
\end{align}

\begin{prop}\label{prop: projectional consistency property}
    The global optimum $\pi_{\theta_*}$ from Theorem \ref{Thm: global cvg outside of the RKHS} is the only policy in $\mathscr{P}_n$ that satisfies the projectional consistency property \eqref{eq: projectional consistency property}
\end{prop}



\begin{comment}    

Suppose now that $\theta_t$ is a critical point of $\theta\mapsto J_n(\pi_\theta)$ that satisfies the optimal projection property.
Since $P_n$
    

Since $m^{(1)}$ and $\pi^{(1)}_t$ are fixed, it is also the case of the projection $P_1$.
    Computing the time-derivative and using that $r(a,s)=Q_{\pi_t}^{(1)}(a,s)$ by definition, we must have that
    \begin{align*}
        0
        &=\partial_t\log\pi_t^{(1)}(a|s)
        =\nabla_\theta\log\pi_t^{(1)}(a|s)\cdot\EE_\pi^{(1)}\left[(r(a,s)-\tau\log\frac{\pi^{(1)}_t}{\overline{\pi}}(A|S_{n-1}))\nabla_\theta\log\pi_t(A|S_{n-1})\right]\\
        &=\int_{\mathcal{S}}\mathbf{m}^{(1)}(\mathrm{d}s)\int_{\mathcal{A}}\pi_t^{(1)}(\mathrm{d}a)(Q_{\pi_t}^{(1)}(a,s)-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(a|s))\Theta^{(1)}(a|s),
    \end{align*}
    











Decompose $Q_*^{(1)}=Q^{(1)}_\bullet+Q^{(1)}_\perp$ with $Q^{(1)}_\bullet=P_1 Q_*^{(1)}\in\mathcal{H}_{\Theta^{(1)}}$ and $Q^{(1)}_\perp\in(\mathcal{H}_{\Theta^{(1)}})^\perp$.
Define the $1$-step policy
\begin{align*}
    \pi_\bullet^{(1)}(a|s)
    :=\overline{\pi}(a|s)\frac{\exp\left(Q^{(1)}_\bullet(a,s)/\tau\right)}{\int_\mathcal{A}\overline{\pi}(\mathrm{d}a'|s)\exp\left(Q^{(1)}_\bullet(a',s)/\tau\right)}.
\end{align*}
We write $V_\bullet^{(1)}$ for the induced value function.
We then define the larger horizon policies recursively as follows: define $\overline{Q}_*^{(i+1)}(a,s):=r(a,s)+\int_{\mathcal{S}}p(s,a,\mathrm{d}s')V_\bullet^{(i)}(s')$, similarly as in \eqref{eq: gen case definition Q-function}.
The map $\overline{Q}_*^{(i+1)}$ is the optimal $(i+1)$-step policy conditionally given that the $m$-step policies are fixed to $\pi^{(m)}_\bullet$, for $m=1,\ldots,i$.
Decompose $\overline{Q}_*^{(i+1)}=Q_\bullet^{(i+1)}+Q_\perp^{(i+1)}$ similarly as before, with respect to the kernel $\Theta^{(i+1)}$, that is $Q_\bullet^{(i+1)}=P_{i+1}\overline{Q}_*^{(i+1)}\in\mathcal{H}_{\Theta^{(i+1)}}$ and $Q_\perp^{(i+1)}\in(\mathcal{H}_{\Theta^{(i+1)}})^\perp$.
Then, define
\begin{align}\label{eq: def pi_bullet}
    \pi_\bullet^{(i+1)}(a|s):=\overline{\pi}(a|s)\frac{\exp\left(Q_\bullet^{(i+1)}(a,s)/\tau\right)}{\int_\mathcal{A}\overline{\pi}(\mathrm{d}a'|s)\exp\left(Q_\bullet^{(i+1)}(a',s)/\tau\right)}.
\end{align}
Let $\mathscr{P}_{n}\subset\mathcal{P}_n$ be the set of policies that can be parametrized by the kernels $\Theta^{(1)},\ldots,\Theta^{(n)}$, that is to say such that they are softmax policies with preferences $h^{(i)}\in\mathcal{H}_{\Theta^{(i)}}$.

Note that at this point, the policy $\pi_\bullet=(\pi_\bullet^{1},\ldots,\pi_\bullet^{(n)})\in\mathscr{P}_n$ needs not be globally optimal.
The next theorem answers this question and that of global optimality of MPG.


\begin{thm}\label{Thm: global cvg outside of the RKHS}
    When \ref{assumption: optimal policy in RKHS} does not hold, training with ideal MPG update converges and $\lim_{t\to\infty}\pi_t=\pi_\bullet$ in the sense of \eqref{eq: def convergence of policies}.
    Furthermore, $\pi_\bullet=\argmax{\pi\in\mathscr{P}_{n}}J_n(\pi)$.
\end{thm}

Clearly, when $\pi_*\in\mathscr{P}_n$, then $\pi_\bullet=\pi_*$ and we retrieve Theorem \ref{thm: general case global optimality}.
\end{comment}

\subsection{Neural MPG}


Suppose that instead of a linear model, the policy's preferences $h_\theta^{(i)}$, $i=1,\ldots,n$, are parametrized by deep neural networks.
It is immediate from the proofs that the policy gradient theorem holds true, that is, $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$ for the ideal MPG update.
We describe the limit of training in terms of the \textit{Neural Tangent Kernels} (NTKs) of the neural networks and the \textit{conjugate kernels} (CKs).
The NTK of the $i$-step policy (or rather, of the $i$-step preference) at time $t$ of training is defined for all $(a,s),(a',s')\in\mathcal{A}\times\mathcal{S}$ as
\begin{align*}
    \Theta_t^{(i)}((a,s),(a',s'))
    :=\nabla_{\theta^{(i)}} h_t^{(i)}(a,s)\cdot\nabla_{\theta^{(i)}}h_t^{(i)}(a',s').
\end{align*}
The CK of the $i$-step policy is defined as the inner product of the last hidden layer, that we denote by $\alpha$, that is
\begin{align*}
    \Sigma_t((a,s),(a',s')):=\alpha_t(a,s)\cdot\alpha_t(a',s').
\end{align*}
Moreover, letting $\mathcal{H}_K$ be the induced \textit{reproducible kernel Hilbert space} (RKHS) of a kernel $K$, it holds that $\mathcal{H}_{\Sigma_t}\subset\mathcal{H}_{\Theta_t}$, see Appendix \ref{appendix: neural networks} for more details.

For the trained policy $\pi_\infty$, let $\mathscr{P}_n^{\Theta}$ be the space of log linear policies whose $i$-step preference belong to $\mathscr{H}_{\Theta_\infty^{(i)}}$, $i=1,\ldots,n$, and similarly for $\mathscr{P}_n^{\Sigma}$ and $\Sigma_\infty^{(i)}$.



\begin{comment}
Suppose that $\pi_t\to\pi_\infty$ and similarly to \eqref{eq: def pi_bullet}, define $Q_{\text{NTK}}^{(i)},Q_{\text{CK}}^{(i)}$ recursively for $i=1,\ldots,n$ the orthogonal projections of the optimal $Q$-functions onto the RKHSs of the NTK and the CK, respectively.
Let $\pi_{\text{NTK}},\pi_{\text{CK}}\in\mathcal{P}_n$ be the induced policies. 
\end{comment}

\begin{cor}\label{thm: gen case deep RL global optimality}
    Let $\pi_t\in\mathcal{P}_n$ be parametrized by neural networks.
    Suppose that $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$ with $\eta>0$ small enough and that $\pi_\infty=\lim_{t\to\infty}\pi_t$.
    Then, it holds that
    \begin{align*}
        \pi_\infty
        =\underset{\pi\in\mathscr{P}_n^{\Theta}}{\mathrm{argmax}}\ J_n(\pi)
        =\underset{\pi\in\mathscr{P}_n^{\Sigma}}{\mathrm{argmax}}\ J_n(\pi).
    \end{align*}
    In particular, if $\pi_*\in\mathcal{P}_n^{\Theta}$ (equivalently $\mathcal{P}_n^{\Sigma}$), then $\pi_\infty=\pi_*$.
\end{cor}
A direct consequence of Corollary \ref{thm: gen case deep RL global optimality} is global convergence of MPG in the NTK regime, see the forthcoming Remark \ref{rem: consequence corollary deep RL} in Appendix.


\begin{comment}
\francois{
\subsection{NOT FINISHED: The mathematical reason for MPG}

Besides the theoretical guarantees of MPG that we presented, let us expose how the idea of the MPG framework was born.
In an attempt to prove global convergence of PG, we took inspiration from \cite{Laslier13}.
That paper studies an urn model â€“ a very important class of stochastic processes â€“ where an urn contains a finite number of colored balls.
A reinforcing mechanism is implemented by picking balls uniformly at random, and adding a ball of the winning color, in a competitive manner.
The authors show that by letting the process evolve, the proportions of the colors converge, by using a standard result of Probability Theory, which says the following\footnote{This is a simple stochastic analogue to the deterministic statement ``an increasing sequence of real numbers $(x_n)_{n\geq 0}$ converges.}: a negative submartingale $(X_n)_{n\geq 0}$ converges with probability $1$.
The optimality is obtained thanks to the expression of the submartingale, entailing that for the limit to have a null expectation, the proportions of the colors must give the optimal policy of the competition between the colors.

Consider the standard max-RL infinite horizon objective and let $\nu_*$ be the stationary distribution of the MDP under the optimal policy $\pi_*$.
Mimicking their argument, one defines the process
\begin{align}
    \mu_t:=
    \int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\pi_*(a|s)\log\pi_t(a|s)\nu_*(\mathrm{d}s),
\end{align}
and using \ref{Lemma Boltzmann measure is log convex}, one writes
\begin{align*}
    \mu_{t+1}-\mu_t
    \geq\int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\pi_*(a|s)(\theta_{t+1}-\theta_t)\cdot\nabla_\theta\log\pi_t(a|s)\nu_*(\mathrm{d}s).
\end{align*}
To simplify, suppose that $\Theta((a,s),(a',s'))=\delta_{a,a'}\delta_{s,s'}$.
After some computations, one can show that
\begin{align*}
    \mu_{t+1}-\mu_t
    &\geq\int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\pi_t(a|s)\left(\pi_t-\pi_*\right)(a|s)\bigg(\log\frac{\pi_t}{\pi_*}(a|s)-\DKL{\pi_t}{\pi_*}(s)\\
    &\hspace{1cm}{\color{red}+\sum_{k\geq 1}\gamma^{k}\left(\EE_{\pi_t}[\DKL{\pi_t}{\pi_*}(S_k)\Big|S_k,A_0=a]-\EE_{\pi_t}[\DKL{\pi_t}{\pi_*}(S_k)\Big|S_k]\right)}\bigg)\mathrm{d}s.
\end{align*}
}
\end{comment}

\begin{comment}
\section{Sketch of proofs}

\francois{
If enough space, make sketch of proof of Theorem \ref{thm: general case global optimality}:

Convergence readily follows using Theorem \ref{thm: policy gradient theorem}, which is rather easy to prove with a simple computation to compare the gradient of the objective \eqref{def: general case objective function} and the expectation of the MPG update \eqref{eq: update cascade learning}.

In order to prove global optimality, we start from the following observation that for any $\pi\in\mathcal{P}_n$ and $a\in\mathcal{A},s\in\mathcal{S}$, we have that
\begin{align*}
    Q_\pi^{(n)}(a,s)-Q_*^{(n)}(a,s)
    &=\int_{\mathcal{S}}p(s,a,\mathrm{d}s')\left(V_\pi^{(n-1)}(s')-V_*^{(n-1)}(s')\right).
\end{align*}
In particular, we see that if $\pi_t^{(k)}=\pi_*^{(k)}$ for all $k\leq n-1$, then $Q_{\pi_t}^{(n)}=Q_*^{(n)}$ and the cumulated reward $C_n$ in the MPG update \eqref{eq: update cascade learning} is such that
\begin{align*}
    \EE_{\pi_t}[C_n|A_0,S_0]
    &=Q_{\pi_t}^{(n)}(A_0,S_0)-\log\frac{\pi_t^{(n)}}{\overline{\pi}}(A_0|S_0)
    =Q_*^{(n)}(A_0,S_0)-\log\frac{\pi_t^{(n)}}{\overline{\pi}}(A_0|S_0).
\end{align*}
The update is therefore perfectly aligned with the optimal $Q$-function.
Note that this is \textbf{only} possible with the use of time-dependent policy.
Global optimality is then showed by induction.
We suppose that $\theta_t$ is a critical point, which implies that for all $a\in\mathcal{A},s\in\mathcal{S}$, $\log\pi^{(m)}_{t+1}(a|s)-\pi^{(m)}_t(a|s)=0$.
Then, we assume that $\pi_t^{(i)}=\pi_*^{(i)}$ for all $i<m$, and compute $\log\pi^{(m)}_{t+1}-\pi^{(m)}_t$ to write is as an integral transform of $h_t^{(m)}-Q_*^{(m)}$ with respect to a kernel $\widetilde{\Theta}^{(m)}$ related to $\Theta^{(m)}$, see Lemma \ref{lem: general case expected update log pi_t} and Equation \eqref{eq: h-Q in softmax RKHS}.
Standard results on kernel (Lemma \ref{lem: if I_Kf=0 then in ortho} and Lemma \ref{lem: softmax RKHS is larger}) show that if this integral transform is null, then for all $s\in\mathcal{S}$, the map
$a\mapsto h_t^{(m)}(a,s)-Q_*^{(m)}(a,s)$ is constant in $a$, which in turn implies that $\pi_t^{(m)}=\pi_*^{(m)}$.
By induction on $m$, we deduce Theorem \ref{thm: general case global optimality}.

The same arguments adapt to prove Theorem \ref{Thm: global cvg outside of the RKHS}, up to orthogonal projections of the cumulated rewards onto the corresponding RKHSs.
}
\end{comment}

\section{Numerical experiments}
\label{Section: numerical experiments}

This section summarizes the performance of the MPG framework. Our current implementation of the MPG is very simple, without standard RL tricks such as replay buffer, gradient clipping, etc. Details on the implementation, experimental setups and additional results can be found in Appendix \ref{appendix: numerical experiments}.

\subsection{Analytical task}
We devise an analytical problem (Appendix \ref{appendix:analytical_task}) with fixed horizon $n=2$ to numerically evaluate the consequences of theorem \ref{thm: general case global optimality}. We consider the preference function represented by a linear model. In figure \ref{fig:analytical_task}  we show the errors between the policies obtained using the MPG algorithm and the optimal policies go to zero.

%(left) we show that obtained policies converge towards the optimal objective when using the ideal gradient update for the MPG and on the right,
%projected optimal one step and approximated projected two step policies, with maximal errors of $0.019$ and $0.09$ respectively. We notice that in both cases, there exists a small gap between the optimal policy and the obtained policy and we hypothesise that this is due to the update being an approximate update (using gradient descent).
%\maria{Comment about the gAP. to be rephrased. comment about the error on the policy wrt to objective.}

\begin{figure}[h]
\centering
%\includegraphics[width=0.5\textwidth]{Neurips2023/figures/J_full_basis.pdf}~
\includegraphics[width=0.5\textwidth]{Neurips2023/figures/full_basis_error.pdf}
\caption{Training and convergence of 5 agents with random initialisation. Convergence of the 1-step and 2-step policies towards the optimal policies, measured by the $L_\infty$-norm.}
\label{fig:analytical_task}
\end{figure}
%Left: convergence of the policy objective towards $J_{optimal}$. Right: 

\subsection{Control problems}
In this section we present a summary of the performance of the MPG algorithm on two standard RL problems, comparing its performance to a vanilla PG (VPG) algorithm \cite{Sutton99} using deep neural networks to parametrise the preference function. Details and extended results can be found in \ref{appendix:control_problems}.

\paragraph{Frozen Lake:}
The Frozen Lake benchmark \cite{opengym} features a $k\times k$ grid composed of cells, holes and one treasure, and a discrete action space, namely, the agent can move in four directions (up, down, left, right). The game terminates when the agent reaches the treasure or falls down holes. In figure \ref{fig:control_problems} (left), we show the number of victories during training for both methods for the $4\times 4$ grid. We observe a quicker learning from the MPG, with both methods finding the optimal policies at the end of training. The learning speed is quite sensitive to the choice of hyperparameters, so we deem the behaviour of both methods to be similar. Once the agents have been trained, we play 100 games and observe 90.60\% of task completion, with an average path of 6.49 steps for the VPG, and 100\% of task completion with an average path of length 6 (optimal path length) for the MPG.

\paragraph{Cart Pole:}
The Cart Pole benchmark is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. The task is to balance the pole for 100 consecutive timesteps. In figure \ref{fig:control_problems} (right), we show the cumulative reward during training per game. Once the agents have been trained, we again test them by playing 100 games. We attained successful task completion for 97.4\% of the games, with an average of 99.76 consecutive timesteps for the MPG and successful task completion 97.2\% of the games, with an average of 99.33 consecutive timesteps for the VPG. We observe similar performance between the MPG and the VPG during training and testing phases when the right set of hyperparameters is chosen.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{Neurips2023/figures/maze_training.pdf}~\includegraphics[width=0.45\textwidth]{Neurips2023/figures/pendulum_training.pdf}
\caption{Performance of 5 agents during training. The solid curve represents the average and the shaded region the maximum and minimum observed at each game.  \label{fig:control_problems}}
\end{figure}

\begin{comment}
    
First, we considered the FrozenLake benchmark \cite{opengym}, which is a maze-like $k\times k$ grid world composed of safe cells, holes and one treasure. The goal is for the agent to reach the treasure while avoiding holes. It features a finite environment and discrete action space. For $k=4$, the MPG obtained policies were optimal or very close to optimal. For $k=8$, we were able to obtain close-to-optimal policies in the sense that the treasure was consistently found after an average of 16.7 steps (the optimal deterministic path contains 14 steps). This near optimal aspect could be due to the stochasticity of the policy.

The second benchmark is the Cart Pole \cite{opengym}, which is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. Considering a horizon of $n=100$, we obtained agents for which 77\% of the games played ($500$ games) balanced the pole for the full horizon, with an average length for the game of $96$ steps. 
\end{comment}

\begin{comment}

This section outlines the performance of the MPG framework using a finite dimensional neural network.  We evaluate the MPG on two standard benchmark problems and comment on the performance observed, as well as some considerations when using this algorithm in practice. Pseudo-code, more experimental details, and additional results can be found in the supplementary material.




\subsection{Frozen lake}
The FrozenLake benchmark \cite{opengym} is a maze-like $k\times k$ grid world composed of safe cells, holes and one treasure. It features a discrete action space, namely, the agent can move in four directions (up, down, left, right). The episode terminates when the agent reaches the treasure, falls down a hole or reaches a fixed horizon. %We consider a $n=4, 8$ for the numerical experiments. It is well-known that reshaping the reward function can change the performance of the algorithm. The original reward function does not discriminate between losing the game (falling into a hole), not moving and moving, so we changed use a reshaped reward function: losing the game (-1), not moving (-0.01), moving (0.01) and reaching the treasure (10.0).

For $k=4$, the optimal number of steps is $6$.
We use a decay for the learning rate of the form $\learningrate_t=\learningrate\times a^t$ and similarly the temperature.
We define a terminal $\tau_T = 0.03$ and terminal learning rate $\eta_T = 3\times 10^{-6}$, vary the initial learning rates $\eta$, temperatures $\tau$ and horizon to see the impact of these on the success of the agents. We train sets of $10$ agents on $1000$ episodes. Then, the trained agents play $100$ games. A summary of the results for horizon $n=10$ is given in table \ref{table:frozenlake-performance-parameters}, showing that the policies obtained are optimal or very close to optimal. The column \textit{Failed to train} denotes the policies that failed to converge (out of 10).

For $k=8$, the optimal path contains $14$ steps. We  use the a terminal $\tau_T=0.01$ and consider a horizon of $h = 100$. 

\begin{table}[h!]
\centering
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & Average Steps & Failed to train \\
 \hline
% $ \tau_0 = 0.3, ~ \eta = 0.0001$ & -- & -- & 10\\
%$ \tau_0 = 0.3, ~ \eta = 0.001$ & 100.00 & 6.00 & 0\\
%$ \tau_0 = 0.3, ~ \eta = 0.0005$ & 100.00 & 6.00 & 1\\
%$ \tau_0 = 0.4, ~ \eta = 0.0001$ & 89.10 & 6.14 & 0\\
$ \tau_0 = 0.4, ~ \eta_0 = 0.001$ & 100.00 & 6.00 & 0\\
$ \tau_0 = 0.4, ~ \eta_0 = 0.0005$ & 100.00 & 6.21 & 1\\
%$ \tau_0 = 0.5, ~ \eta = 0.0001$ & 97.00 & 6.07 & 5\\
$ \tau_0 = 0.5, ~ \eta_0 = 0.001$ & 99.80 & 6.00 & 0\\
$ \tau_0 = 0.5, ~ \eta_0 = 0.0005$ & 100.00 & 6.01 & 0\\
%$ \tau_0 = 0.6, ~ \eta = 0.0001$ & 70.86 & 7.22 & 3\\
$ \tau_0 = 0.6, ~ \eta_0 = 0.001$ & 100.00 & 6.01 & 0\\
$ \tau_0 = 0.6, ~ \eta_0 = 0.0005$ & 100.00 & 6.01 & 1\\
 [1ex] 
 \hline
\end{tabular}
\caption{Frozen Lake for $k=4$ performance after training, each set of parameters is averaged across 10 agents and 100 games per agent.}
\label{table:frozenlake-performance-parameters}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & average steps & failed to train \\
 \hline
  &  &  & \\[1ex] 
 \hline
\end{tabular}
\caption{Frozen Lake $8\times 8$ after training, for horizon with length $100$ and terminal $\tau = 0.03$.}
\label{table:frozenlake-8x8-performance-parameters}
\end{table}


\subsection{Cart Pole}
The Cart Pole benchmark \cite{opengym} is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. %The original reward function gives a $+1$ for each time that the pole stays upright, and the task finishes if the cart leaves the domain or if the pole is far enough from being upright. We reshape the reward function to give a penalty ($-10$) if the task is unsuccessful (e.g. the pole falls before reaching the target horizon). 

We set the terminal $\tau_T = 0.01$ and terminal learning rate $\eta_T = 5\times 10^{-8}$. The decay rates for $\eta$ and $\tau$ are computed to reach the desired terminal values for varying temperature and learning rate. We train sets of $5$ agents on $2000$ episodes. Then, we play $100$ games with the trained agents and record the performance of the policies,
 as shown on table \ref{table:polecart-performance-parameters}. For $\tau_0 = 0.2$ and $\eta_0 = 1\times 10^{-6}$, we have that 84\% of the games played balanced the pole for the full horizon ($n = 100$), with an average length of horizon of $92.98$. 

\begin{table}[h!]
\centering
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & Average steps & Failed to train \\
 \hline
$ \tau_0 = 0.1, ~ \eta_0 = 1\times 10^{-6}$ & 18.00 & 60.06 & 0\\
$ \tau_0 = 0.15, ~ \eta_0 = 1\times 10^{-6}$ & 51.00 & 86.24 & 0\\
$ \tau_0 = 0.2, ~ \eta_0 = 1\times 10^{-6}$ & 84.00 & 92.98 & 0\\
 [1ex] 
 \hline
\end{tabular}
\caption{Cart Pole performance after training, for horizon $n = 100$.}
\label{table:polecart-performance-parameters}
\end{table}

\end{comment}

\section{Conclusion}

In this paper, we have studied a framework combining fixed-horizon RL and max-entropy RL.
We have introduced the Matryoshka Policy Gradient algorithm in the function approximation setting, with log-linear parametric policies.
We proved that the global optimum of the MPG objective is unique, and that MPG converges to this global optimum, including for continuous state and action space.
Furthermore, we proved that these results hold true even when the true optimal policy does not belong to the parametric space (that is when Assumption \ref{assumption: optimal policy in RKHS} does not hold).
The limit â€“ globally optimal within the parametric space â€“ corresponds to the orthogonal projection of the optimal policy onto the parametric space.
It is written as the softmax of orthogonal projections of the optimal preferences onto the RKHSs of the parametrization, see \eqref{eq: projectional consistency property}.
Finally, letting the horizon tend to infinity, the optimal policy of MPG retrieves the optimal policy of the standard infinite-horizon max-entropy objective.
For neural policies, we prove that the limit is optimal within the RKHSs of the NTK (equivalently of the CK) at the end of training, and can be written in terms of orthogonal projections of optimal preferences onto these RKHSs, yielding criterion for global optimality in terms of the NTK (equivalently the CK).
In particular it establishes the global convergence of neural MPG in the NTK regime.
The MPG framework is intuitive, theoretically sound and it is easy to implement without standard RL tricks, as we verified in numerical experiments.

\paragraph{Limitations.}
The main limitations of our work are the following:
(a) we have not studied the rate of convergence of MPG (typically more assumptions on the environment, the horizon, are needed),
(b) we assumed that the updates were perfect whereas in practice, one uses the estimate \eqref{eq: update cascade learning}, (c) as a theoretical paper, our numerical experiments are rather simple.
We hope to address these limitations in future work, as well a other perspectives such as:
\begin{itemize}
    \item Additionally to MPG as defined in this paper, we expect to have nice theoretical properties of variations of MPG that are used for other PG algorithms.
    E.g. one can think of natural MPG, actor-critic MPG, path consistency MPG (see \cite{Nachum17} for path consistency learning).
    \item We motivated the use of MPG with neural softmax policies by some theoretical, practical, and heuristic arguments; we believe that more can be said on the use of neural policies with MPG, in particular by studying the spectra of the NTK and the CK of neural networks along specific geodesics in the parametric space.
    \item How does the MPG framework compares to the standard max-entropy RL framework in terms of exploration, adversarial robustness, ...?
\end{itemize}



\newpage


\appendix
\addcontentsline{toc}{section}{Appendix}



\section*{Appendix}

The appendix is organized as follows:
\begin{itemize}
    \item \ref{appendix: parametrization}: we recall basic properties of softmax policies, then discuss the potential benefits to using a single neural network for the preferences of all $i$-step policies.
    This section ends with an explanation on how to approximate a kernel with finitely many features.
    \item \ref{appendix: RKHS}: we state and derive some basic facts on RKHS.
    \item \ref{appendix: information geometry}: We use concepts from Information Geometry to show that critical points of the MPG objective correspond to critical points of a \textit{Bregman divergence}; this fact is useful when the realizable assumption does not hold to ensure that MPG converges to the unique global optimum.
    %l\item \ref{appendix: bandit case}: we prove Theorem \ref{thm: global optimality} with techniques that will extend to the general state space case.
    \item \ref{appendix: general state space}: we prove the Matryoshka Policy Gradient Theorem (Theorem \ref{thm: policy gradient theorem}), Proposition \ref{prop: extending horizon converges to standard optimal policy} that shows that the infinite horizon optimal policy can approximated arbitrarily well by finite horizon optimal policies, Theorem \ref{thm: general case global optimality} and Theorem \ref{Thm: global cvg outside of the RKHS} that shows global convergence of MPG.
    %\item \ref{appendix: fixed-horizon RL}: we discuss some works related to fixed-horizon RL.
    \item \ref{appendix: assumptions}: we list and discuss our main assumptions.
    \item \ref{appendix: numerical experiments}: we provide more detailed numerical experiments implementing MPG.
    %\item \ref{appendix: multiple updates}: we propose a variation of MPG that could potentially accelerate training at the cost of stability.
\end{itemize}




\section{More on the parametrization}
\label{appendix: parametrization}

\subsection{Softmax policy}


Softmax policies enjoy the two following properties:
\begin{itemize}
    \item For all $s\in\mathcal{S}$, it holds that
    \begin{align}\label{eq: softmax gradient cancels expected constant}
        \EE_{\pi_\theta}\left[\nabla_\theta\log\pi_\theta(A|s)\right]
        =\int_{\mathcal{A}}\nabla_\theta\pi_\theta(\mathrm{d}a|s)
        =0.
    \end{align}
    \item As long as preferences are finite, it holds that $\pi_\theta(a|s)>0$ for all $(a,s)\in\mathcal{A}\times\mathcal{S}$.
\end{itemize}
In order to compute the learning rate's value below which training converge, we use the following:
\begin{lem}\label{lem: Lipschitz objective}
    For the softmax policy with linear preferences, it holds that $\theta\mapsto \nabla_\theta J_n(\pi_\theta)$ is $L$-Lipschitz for some $L>0$. 
\end{lem}
We refer to \cite{Zhang19GlobalCO} Lemma 3.2 and the discussion after Assumption 3.1 therein for the proof of this fact in the standard setting; the proof straightforwardly adapts to the non-stationary policy setting.

A direct consequence of Lemma \ref{lem: Lipschitz objective} is that gradient ascent on $J_n(\pi_\theta)$ converges as soon as the learning rate is smaller than $1/L$.


\subsection{Neural networks}
\label{appendix: neural networks}

\paragraph{Neural Tangent Kernel.}

For a measurable nonlinearity $\sigma:\RR\to\RR$, we recursively define a neural network of depth $L\geq 1$, with trainable parameters $W^{\ell}\in\RR^{d_{\ell}}\times\RR^{d_{\ell+1}}$ as $f:x\in\RR^{d_{0}}\mapsto \widetilde{\alpha}^L(x)\in\RR^{d_{L}}$, with $\alpha^0(x):=x$ and
\begin{align*}
    \widetilde{\alpha}^{\ell+1}(x)&\ := W^\ell\alpha^{\ell}(x),\\
    \alpha^{\ell+1}(x)&\ := \sigma\left(\widetilde{\alpha}^{\ell+1}(x)\right),
\end{align*}
where $\sigma$ is applied element-wise.

Note that the connection between the last hidden layer and the output layer is linear, since $f=W^L\alpha^{L-1}$.
In particular, $f$ belongs to the RKHS of the \textit{conjugate kernel} (CK) associated with the neural network, defined as
\begin{align*}
    \Sigma(x,x'):=\alpha^{L-1}(x)\cdot\alpha^{L-1}(x').
\end{align*}
On the other hand, the training of the neural network is governed by the \textit{neural tangent kernel} (NTK), which is defined as
\begin{align*}
    \Theta(x,x'):=\nabla f(x)\cdot\nabla f(x')
    =\sum_{p=1}^P\partial_{\theta_p}f(x)\partial_{\theta_p}f(x'),
\end{align*}
where $\theta\in\RR^{P}$ is the vector of all the trainable parameters of the neural network.
It is important to note that both the CK and the NTK depend on the parameters and as such, move during training.
Moreover, isolating the derivatives with respect to parameters $W^L$ of the last linear layer from the others $\widetilde{\theta}$, we have that
\begin{align*}
    \Theta(x,x') &= \alpha^{L-1}(x)\alpha^{L-1}(x')+\nabla_{\widetilde{\theta}}f(x)\nabla_{\widetilde{\theta}}f(x')\\
    &=\Sigma(x,x')+K(x,x'),
\end{align*}
and $K$ is another positive semidefinite kernel.
We therefore have that
\begin{align}\label{eq: CK smaller than NTK}
    \mathcal{H}_\Sigma\subset\mathcal{H}_\Theta,\qquad\forall\theta\in\RR^P.
\end{align}

\begin{rem}\label{rem: consequence corollary deep RL}
    %\begin{enumerate}[label = (\roman*)]
        %\item
        For infinitely wide neural networks in the NTK regime \cite{Jacot18} (a.k.a. lazy regime \cite{Chizat18}, kernel regime), the NTK is fixed during training and is strictly positive definite, therefore convergence to the optimal policy is guaranteed.
        %\item In \cite{atanasov22}, under specific assumptions, the authors exhibit two phases of neural network training: during the first one, they describe what they coin a \textit{silent alignment} of the NTK with the data while the output function of the neural network is kept almost constant, then a second phase during which the NTK is almost constant while the function changes significantly to fit the data. In that case, $h_\infty^{(i)}$ belongs to $\mathcal{H}_{\Theta_\infty^{(i)}}$.
        %In particular, if the NTK learns a kernel that can express $Q_*^{(i)}$, then the network finds the optimal policy.
    %\end{enumerate}
\end{rem}




\paragraph{Non-stationary policy parametrized by a single neural network.}

One of the assumptions of MPG is that for any $i\neq j$, the policies $\pi^{(i)}_{\theta^{(i)}}$ and $\pi^{(j)}_{\theta^{(j)}}$ do not share parameters.
Using one neural network per horizon becomes quickly costly as the maximal horizon increases.
In order to avoid this issue, one can use a single neural network $h_{\theta}$ to parametrize all $i$-step policies by using $i$ as an input such that $\pi^{(i)}_{\theta}(a|s)\propto \overline{\pi}(a|s)\exp(h_{\theta}(a,s,i)/\tau)$.
By deviating from the theory, we nonetheless expect the performance of the model to be enhanced: as $i$ grows large, the $i$-step optimal policy gets closer to the $i+1$-step policy.
One could also use $1-\frac{1}{i}$ as an input to the network (or any increasing map $g:\NN\mapsto \intervalleff{0}{1}$ such that $i\mapsto g(i+1)-g(i)$ is decreasing).


\subsection{Kernel methods}
\label{appendix: kernel methods}


Suppose that $\Theta$ is a strictly pd kernel with $\nparam$ positive eigenvalues.
Recall the linear model $a\mapsto h_\theta(a) = \theta\cdot\psi(a)$, with parameters $\theta\in\RR^P$, such that $\psi$ is a feature map associated with $\Theta$.
Then if $P=\infty$, one can use random features, i.e. sample $g_1,\ldots,g_{\nparam'}$ i.i.d. Gaussian processes with covariance kernel $\Theta$, then $h_\theta:=\frac{1}{\sqrt{\nparam'}}\sum_{i=1}^{\nparam'} \theta_i g_i$.
One can thus approximate the true kernel predictor using a finite number of features, see \cite{Jacot20ImplicitRO}.

Another way to approximate the kernel predictor with finitely many features is to use the spectral truncated kernel $\widehat{\Theta}$ of rank $P'\in\NN$, by cutting off the smallest eigenvalues.
If $(e_i,\lambda_i)_{i\geq 1}$ are the eigenfunction/eigenvalue pairs of $\Theta$ ranked in the non-increasing order of $\lambda_i$, one can use
\begin{align*}
    \widehat{\Theta}(x,x'):=\sum_{i=1}^{\nparam'}\lambda_ie_i(x)e_i(x'),
\end{align*}
and the predictor $h_\theta:=\sum_{i=1}^{P'}\theta_ie_i$.


\begin{comment}

\section{Bandit case}
\label{appendix: bandit case}

The following result on the dynamics of the policy during training is essential to proving Theorem \ref{thm: global optimality}:

\begin{lem}\label{lem: bandit case expected update log pi_t}
    For all $a\in\mathcal{A}$ and all $t\in\NN$, it holds that
    \begin{align*}
        \log\pi_{t+1}(a)-\log\pi_t(a)
        &= - \learningrate \tau\sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)\\
        &\hspace{4cm}\times\Big(\Theta(a,a')-\EE_{\pi_t}[\Theta(A,a')]\Big)
        + o\left(\learningrate C(\theta_t)\right),
    \end{align*}
    where the constant $C(\theta_t)$ does not depend on the learning rate $\learningrate$.
\end{lem}

\begin{proof}
    We have,
\begin{align}\label{Equation gradient of pi}
	\nabla_\theta\pi_t(a)=\frac{1}{\tau}\pi_t(a)\sum_{a'\in\mathcal{A}}(\delta_{a,'}-\pi_t(a'))\nabla_\theta h_t(a').
\end{align}
Let $B_t:=R_t-\tau\log\frac{\pi_t(A_t)}{\overline{\pi}(A_t)}$.
By Equation \eqref{Equation update theta}, and Equation \eqref{Equation gradient of pi}, using a first order Taylor approximation, we get for all $a\in\mathcal{A}$ that
\begin{align*}
	&\log\pi_{t+1}(a)-\log\pi_t(a)
	= (\theta_{t+1}-\theta_t)\cdot\frac{\nabla_\theta\pi_t(a)}{\pi_t(a)} + o\left(\learningrate C(\theta_t)\right).
\end{align*}
The first order term of the right-hand side reads as
\begin{align*}
    &\learningrate  \EE_{\pi_t}\left[B_t\frac{\nabla_\theta\pi_t(A_t)\cdot\nabla_\theta\pi_t(a)}{\pi_t(A_t)\pi_t(a)}\right]\nonumber\\
	&\hspace{2cm}=\frac{ \learningrate }{\tau^2} \EE_{\pi_t}\bigg[B_t\sum_{a',a''\in\mathcal{A}}(\delta_{A_t,a'}-\pi_t(a'))(\delta_{a,a''}-\pi_t(a''))
    \nabla_\theta h_t(a')\cdot\nabla_\theta h_t(a'')\bigg]\nonumber\\
	&\hspace{2cm}=\frac{ \learningrate }{\tau^2} \EE_{\pi_t}\bigg[B_t\sum_{a',a''\in\mathcal{A}}(\delta_{A_t,a'}-\pi_t(a'))(\delta_{a,a''}-\pi_t(a''))\Theta(a',a'')\bigg]\nonumber\\
	&\hspace{2cm}=\frac{ \learningrate }{\tau^2}\EE_{\pi_t}\bigg[B_t\left(\Theta(A_t,a)+\EE_{\pi_t\otimes\pi_t}[\Theta(A,A')]-\EE_{\pi_t}[\Theta(A,a)]-\EE_{\pi_t}[\Theta(A_t,A)]\right)\bigg],
\end{align*}
where the inner expectations are for $A$ and $A'$, and the outer one is for $A_t$. 
Using that $\EE[X(Y-\EE[Y])]=\EE[(X-\EE[X])Y]$ for any two integrable real random variables, we then get that
    \begin{align*}
        &\log\pi_{t+1}(a)-\log\pi_t(a)
        =\frac{ \learningrate }{\tau^2}\EE_{\pi_t}\Big[\left(B_t-\EE_{\pi_t}[B_t]\right)\left(\Theta(A_t,a)-\mathbb{E}_{\pi_t}[\Theta(A,a)]\right)\Big] + o\left(\learningrate C(\theta_t)\right).
    \end{align*}
    To conclude the proof, it suffices to note that by Equation \eqref{eq: bandit case optimal policy},
    \begin{align*}
        B_t-\EE_{\pi_t}[B_t]
        &= R_t-\tau\log\frac{\pi_t}{\overline{\pi}}(A_t)-\EE_{\pi_t}[r(A_t)]+\DKL{\pi_t}{\overline{\pi}}\\
        &=\log\frac{\pi_*}{\overline{\pi}}(A_t)-\tau\log\frac{\pi_t}{\overline{\pi}}(A_t)-\EE_{\pi_t}\left[\tau\log\frac{\pi_*}{\overline{\pi}}(A)\right] + \DKL{\pi_t}{\overline{\pi}}\\
        &=-\tau\log\frac{\pi_t}{\pi_*}(A_t)+\tau\DKL{\pi_t}{\pi_*},
    \end{align*}
    which yields the claim.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{thm: global optimality}}]

Let $p$ be any probability measure on $\mathcal{A}$.
Suppose that $\theta_t$ is a critical point of $\theta\mapsto J(\pi_\theta)$.
Since $\theta_{t+1}-\theta_t=\eta \nabla_\theta J(\theta_t)=0$, by Lemma \ref{lem: bandit case expected update log pi_t}, we have that
\begin{align*}
    0&=\sum_{a\in\mathcal{A}}p(a)\left(\log\pi_{t+1}(a)-\log\pi_t(a)\right)\\
    &= - \learningrate \tau\sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)
    \Big(\EE_p[\Theta(A,a')]-\EE_{\pi_t}[\Theta(A,a')]\Big)+o\left(\learningrate C(\theta_t)\right).
\end{align*}
In particular, since the above must hold for all $\learningrate>0$, we deduce that
\begin{align}\label{eq: bandit case increments are 0}
    &\sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)\nonumber\\
    &\hspace{1.2cm}\times\left(\EE_p[\Theta(A,a')]-\EE_{\pi_t}[\Theta(A,a')]\right)=0
\end{align}
We now show that if $d\not\perp e_i$, then one can choose $p$ such that the above is non-zero.
If the map
\begin{align*}
    f:a\mapsto \sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)\Theta(a,a')
\end{align*}
is not constant in $a$, then take $p=\delta_{a_{\min}}$ with $a_{\min}:=\argmin{a\in\mathcal{A}}f(a)$.
Clearly, the left-hand side of \eqref{eq: bandit case increments are 0} is (strictly) positive, since $\pi_t(a)>0$ for all $a\in\mathcal{A}$, which is a contradiction.
Therefore, $f$ must be constant. We write
\begin{align*}
    f(a)
    &=\sum_{i=1}^{|\mathcal{A}|}\lambda_ie_i(a)\sum_{a'\in\mathcal{A}}\pi_t(a')
    \left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)e_i(a')\\
    &=\sum_{i=1}^{|\mathcal{A}|}\lambda_i \iprod{d}{e_i} e_i(a).
\end{align*}
On the other hand, since $\sum_{a\in\mathcal{A}}d_a=0$, we have that
\begin{align*}
    0
    &=\sum_{a\in\mathcal{A}}f(a)d(a)
    =\sum_{i=1}^{|\mathcal{A}|}\lambda_i \iprod{d}{e_i} \sum_{a\in\mathcal{A}}d(a)e_i(a)\\
    &=\sum_{i=1}^{|\mathcal{A}|}\lambda_i \iprod{d}{e_i}^2,
\end{align*}
which yields the claim since $\lambda_i\geq 0$ for all $i\in\{1,\ldots,|\mathcal{A}|\}$.



\end{proof}

\end{comment}

\section{Reproducible kernel Hilbert spaces}
\label{appendix: RKHS}

In this section, we recall and provide some basic facts on RKHSs that we use throughout the proofs.
Given some RKHS $\mathcal{H}$, we write $\mathcal{H}^\perp$ for its orthogonal complement; it is also an RKHS.

\begin{lem}\label{lem: facts on RKHS}
    Let $\mathcal{H}_1,\mathcal{H}_2$ be two RKHSs on $\mathcal{A}\times\mathcal{S}$,
    \begin{enumerate}[label = (\roman*)]
        \item The intersection $\mathcal{H}_1\cap\mathcal{H}_2$ is an RKHS.
        \item For any element $f\in\mathcal{H}_1$, there exists a unique decomposition $f=g_\bullet+g_\perp$ such that $g_\bullet\in\mathcal{H}_1\cap\mathcal{H}_2$ and $g_\perp\in\mathcal{H}_1\cap(\mathcal{H}_2)^\perp$.
    \end{enumerate}
\end{lem}

\begin{comment}
\francois{
Intersection of Hilbert spaces is a Hilbert space.
Point evaluation functionals are continuous on the intersection $\Rightarrow$ intersection is RKHS.

Second item is straightforward, where $g_\circ$ is the orthogonal projection of $g$ onto $\mathcal{H}_1\cap\mathcal{H}_2$.
}
\end{comment}

For a probability measure of the form $\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)$ on $\mathcal{A}\times\mathcal{S}$, where $\pi$ is a policy, and for a positive-semidefinite kernel $K$ on $\mathcal{A}\times\mathcal{S}$, we define the integral operator $I_K(\mu,\pi):L^2(\mu(\mathrm{d}s)\pi(\mathrm{d}a|s))$ by
\begin{align*}
    I_K(f;\mu,\pi):(a,s)\mapsto\int_{\mathcal{A}\times\mathcal{S}}\mu(\mathrm{d}s')\pi(\mathrm{d}a'|s')f(a',s')K((a,s),(a',s')).
\end{align*}

Mercer's Theorem states that if $\mathcal{A}\times\mathcal{S}$ is closed (in a real space), $\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)$ has full support, and if $K$ is continuous and satisfies $\int_{(\mathcal{A}\times\mathcal{S})^2}K((a,s),(a',s'))^2<\infty$, then there exists eigenfunction/eigenvalue pairs $(e_i,\lambda_i)_{i\geq 1}$ associated with $I_K(\mu,\pi)$, ranked in the non-increasing order of $\lambda_i\geq 0$ such that
\begin{align*}
    K((a,s),(a',s'))=\sum_{i\geq 1}\lambda_ie_i(a,s)e_i(a',s').
\end{align*}
Moreover, $\{e_i;i\geq 1\}$ is an orthonormal basis of $L^2(\mu(\mathrm{d}s)\pi(\mathrm{d}a|s))$ and the RKHS $\mathcal{H}_{K}$ has orthonormal basis $\{\sqrt{\lambda_i}e_i;\lambda_i>0\}$ with respect to the RKHS inner product.
We refer the reader to \cite{minh06} for more details.

We stress that the notion of orthogonality \textbf{depends} on the measure $\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)$.
Henceforth, we write $\mathcal{H}^\perp$ for the orthogonal space of the RKHS, where this measure is implicit but given by the context.

In the rest of the current section, we use the notation introduced above and assume that Mercer's Theorem applies.

\begin{lem}\label{lem: if I_Kf=0 then in ortho}
    Let $f\in L^2(\mu(\mathrm{d}s)\pi(\mathrm{d}a|s))$.
    It holds that $I_K(f;\mu,\pi)(a,s)=0$ for all $a\in\mathcal{A},s\in\mathcal{S}$ if and only if $f\in(\mathcal{H}_K)^\perp$.
\end{lem}

\begin{proof}
    We write
    \begin{align*}
        &\int_{\mathcal{A}\times\mathcal{S}}\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)f(a,s)I_K(f;\mu,\pi)(a,s)\\
        &\hspace{1cm}=\int_{(\mathcal{A}\times\mathcal{S})^2}\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)(a,s)\int_{\mathcal{A}\times\mathcal{S}}\mu(\mathrm{d}s')\pi(\mathrm{d}a'|s')f(a,s)f(a',s')K((a,s),(a',s'))\\
        &\hspace{1cm}=\sum_{i\geq 1}\lambda_i\left(\int_{\mathcal{A}\times\mathcal{S}}\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)f(a,s)e_i(a,s)\right)^2,
    \end{align*}
    where we used Mercer's Theorem to write $K((a,s),(a',s'))=\sum_{i\geq 1}\lambda_ie_i(a,s)e_i(a',s')$.
    The claim follows.
\end{proof}





\begin{lem}\label{lem: softmax RKHS is larger}
    It holds that
    \begin{align*}
        \widetilde{K}((a,s),(a',s'))
        :=\ &K((a,s),(a',s')) - \int_\mathcal{A}K((b,s),(a',s'))\pi(\mathrm{d}b|s)\\
        &\hspace{0.7cm}-\int_\mathcal{A}K((a,s),(b',s'))\pi(\mathrm{d}b'|s')
        +\int_{\mathcal{A}^2}K((b,s),(b',s'))\pi(\mathrm{d}b|s)\pi(\mathrm{d}b'|s')
    \end{align*}
    is a positive-semidefinite kernel.
    Furthermore, any map $g\in\mathcal{H}_{K}\cap(\mathcal{H}_{\widetilde{K}})^{\perp}$ is such that for every $s\in\mathcal{S}$, the map $a\mapsto g(a,s)$ is constant.
\end{lem}

\begin{proof}
    Let $d:=\sup\{i\geq 1:\lambda_i>0\}$ where the $\lambda_i$'s are the eigenvalues of $I_K$.
    To prove the first part of the claim, it suffices to show that for all $g\in L^2(\mu(\mathrm{d}s)\pi(\mathrm{d}a|s))$, we have
    \begin{align}\label{eq: to prove for larger softmax RKHS}
        \int_{(\mathcal{S}\times\mathcal{A})^2}\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)\mu(\mathrm{d}s')\pi(\mathrm{d}a'|s')g(a,s)g(a',s')\widetilde{K}((a,s),(a',s'))\geq 0.
    \end{align}
    To ease the notation, for any maps $f,g\in L^2(\mu(\mathrm{d}s)\pi(\mathrm{d}a|s))$ we write
    \begin{align*}
        \langle f,g\rangle
        :=\ &\int_{\mathcal{S}\times\mathcal{A}}\mu(\mathrm{d}s)\pi(\mathrm{d}a|s)f(a,s)g(a,s),\\
        %\langle f,g\rangle_s
        %:=\ &\int_{\mathcal{A}}\pi(\mathrm{d}a|s)f(a,s)g(a,s),\\
        \overline{f}(s)
        :=\ &\int_\mathcal{A}\pi(\mathrm{d}a|s)f(a,s).
    \end{align*}
    
    We now establish \eqref{eq: to prove for larger softmax RKHS}.
    Using that $K((a,s),(a',s'))=\sum_{j\leq d}\lambda_j e_j(a,s)e_j(a',s')$, we get
    \begin{align*}
        \widetilde{K}((a,s),(a',s'))
        =\sum_{j\leq d}\lambda_j (e_j(a,s)-\overline{e}_j(s))(e_j(a',s')-\overline{e}_j(s')).
    \end{align*}
    The left-hand side of \eqref{eq: to prove for larger softmax RKHS} thus reads as
    \begin{align*}
        \sum_{j\leq d}\lambda_j\left(\langle g,e_j\rangle^2
        - 2\langle g,e_j\rangle \langle \overline{g},\overline{e}_j\rangle
        + \langle \overline{g},\overline{e}_j\rangle^2 \right)
        &=\sum_{j\leq d}\lambda_j\left(\alpha_j^2
        - 2\alpha_j \langle \overline{g},\overline{e}_j\rangle
        + \langle \overline{g},\overline{e}_j\rangle^2 \right)\\
        &=\sum_{j\leq d}\lambda_j\left(\alpha_j
        - \langle \overline{g},\overline{e}_j\rangle \right)^2.
    \end{align*}
    The right-hand side above being clearly non-negative, this shows that $\widetilde{K}$ is positive-semidefinite.

    We now turn our attention to the last part of the claim.
    Suppose that $g\in\mathcal{H}_K\cap(\mathcal{H}_{\widetilde{K}})^{\perp}$, so that we can write $g=\sum_{j\leq d}\alpha_je_j$, with $\alpha_j=\langle g, e_j\rangle$.
    Moreover, by Lemma \ref{lem: if I_Kf=0 then in ortho}, we have an equality in \eqref{eq: to prove for larger softmax RKHS}, and we get
    \begin{align*}
        \sum_{j\leq d}\lambda_j(\langle g,e_j\rangle-\langle \overline{g},\overline{e}_j\rangle)^2=0.
    \end{align*}
    We thus necessarily have $\langle g,e_j\rangle=\langle \overline{g},\overline{e}_j\rangle$ for all $j\leq d$.
    In particular,
    \begin{align*}
        \langle g,g\rangle
        &=\sum_{j\leq d} \alpha_j^2
        =\sum_{j\leq d} \alpha_j\langle \overline{g}, \overline{e}_j \rangle
        =\langle \overline{g},\overline{g}\rangle.
    \end{align*}
    On the other hand, Cauchy-Schwarz Inequality shows that if $s\mapsto g(a,s)$ is not constant in $a$ for all $s$, then
    \begin{align*}
        \langle g,g\rangle
        &=\int_{\mathcal{S}}\mu(\mathrm{d}s)\int_\mathcal{A}\pi(\mathrm{d}a|s)g(a,s)^2\\
        &>\int_{\mathcal{S}}\mu(\mathrm{d}s)\left(\int_\mathcal{A}\pi(\mathrm{d}a|s)|g(a,s)|\right)^2\\
        &\geq \langle \overline{g},\overline{g}\rangle.
    \end{align*}
    This is a contradiction and thus implies that $g$ must be constant in $a$.
\end{proof}

\section{Information Geometry}
\label{appendix: information geometry}

\begin{comment}
\francois{some bs about information geometry that we don't need.}


Recall the parametrisation $h_\theta^{(i)}(a,s)=\theta^{(i)}\cdot\psi^{(i)}(a,s)$, where $\theta^{(i)}\in\RR^{P_i}$ is a vector of $P_i$ parameters.
By definition, $\Theta^{(i)}((a,s),(a',s'))=\psi^{(i)}(a,s)\cdot\psi^{(i)}(a',s')$, therefore the function space $\{h_\theta^{(i)}=\theta^{(i)}\cdot\psi^{(i)}\}$ corresponds to the RKHS $\mathcal{H}_{\Theta^{(i)}}$.

Let $\vartheta\in\RR^P$ be a critical point of $\theta\mapsto J_n(\pi_\theta)$.
The policy $\pi_\vartheta$ induces the state distributions $S_{n-i}\sim\mathbf{m}_{\pi_\vartheta}^{(i)}$, $i=1,\ldots, n$.
For each $i$, let $(\varphi_k^{(i)})_{k\geq 1}$ be such that $\varphi^{(i)}_k=\psi^{(i)}_k$ if $k\leq P_i$, and otherwise $\varphi^{(i)}_k$ is a basis function of $L^2(\mathbf{m}_{\pi_\vartheta}^{(i)})$, such that $\{\varphi^{(i)}_k;k>P_i\}$ is an orthonormal basis of $(\mathcal{H}_{\Theta^{(i)}})^\perp$, the orthogonal complement of $\mathcal{H}_{\Theta^{(i)}}$ in   $L^2(\mathbf{m}_{\pi_\vartheta}^{(i)})$.


Since the rewards are uniformly bounded, it is also the case of $Q_*^{(i)}$, which therefore belongs to $L^2(\mathbf{m}_\vartheta^{(i)})$.
We can thus write $Q_*^{(i)}=\sum_{k\geq 1}\theta_{*,k}^{(i)}\varphi^{(i)}_k$ for some $\theta_{*,k}\in\RR$, $k\geq 1$.
For $d\geq 1$ fixed, let $\mathcal{F}^{(i)}_d:=\{h=\theta\cdot\varphi^{(i)};\theta\in\RR^d\}$, which corresponds to $\mathcal{H}_\Theta^{(i)}$ when $d=P_i$, and is strictly larger when $d>P_i$.
We define
\begin{align*}
    Q_d^{(i)}:=\sum_{k=1}^d\theta_{*,k}^{(i)}\varphi^{(i)}_k,
\end{align*}
and denote by $\pi_d^{(i)}$ the induced $i$-step policy.


A convex map $F$ on the space of $1$-step policies $\mathcal{P}$ is a map $F$ such that
\begin{align*}
    F((1-\lambda)\pi+\lambda\pi')\leq (1-\lambda)F(\pi)+\lambda F(\pi')\qquad\forall \lambda\in\intervalleff{0}{1}.
\end{align*}
From a strictly convex map, one construct a \textit{Bregman divergence} as follows:
for any $\pi,\pi'\in\mathcal{P}$, define
\begin{align*}
    D_F(\pi,\pi'):=F(\pi)-F(\pi')-\langle\nabla F(\pi'),\pi-\pi'\rangle,
\end{align*}
where $\nabla F$ is the FrÃ©chet derivative of $F$.

Let $\nu$ be a probability measure on $\mathcal{S}$ with full support, let $\pi_0\in\mathcal{P}$ such that $\pi_0(a|s)>0$ for all $(a,s)\in\mathcal{A}\times\mathcal{S}$, and let
\begin{align*}
    F(\pi):=\int_\mathcal{S}\nu(\mathrm{d}s)\DKL{\pi}{\pi_0}(s).
\end{align*}
We have that $(\nabla F(\pi))_{a,s}=\nu(s)\left(\log\frac{\pi}{\pi_0}(a|s)+1\right)$, therefore
\begin{align*}
    D_F(\pi,\pi')
    &=\int_{\mathcal{S}}\nu(\mathrm{d}s)\left(\DKL{\pi}{\pi_0}(s)-\DKL{\pi'}{\pi_0}(s)-\int_\mathcal{A}(\pi-\pi')(\mathrm{d}a|s)\left(\log\frac{\pi'}{\pi_0}(a|s)+1\right)\right)\\
    &=\int_\mathcal{S}\nu(\mathrm{d}s)\DKL{\pi}{\pi'}(s).
\end{align*}
\francois{Do I actually need $\pi_0$?


Also, I don't need to introduce everything for the paper, just say that $\int\mathrm{d}\nu\DKL{\pi}{\pi'}$ is the Bregman divergence induced by the convex map $\int\mathrm{d}\nu\mathrm{d}\pi\log\pi$.}

\end{comment}



The goal of this section is to show that a Pythagorean identity that is used in the forthcoming proof of Theorem \ref{Thm: global cvg outside of the RKHS}.
We use it in the case of a $1$-step policy, and for a fixed state distribution with full support, that we denote by $\nu$ in this section.
Without loss of generality, we also assume to ease the notation that $\tau=1$.

Consider the parametric space of preferences $\mathcal{H}_\Theta:=\{h_\theta=\sum_{k=1}^d\theta_k\psi_k;\theta\in\RR^d\}$, where $\psi$ is the feature map of a positive definite kernel $\Theta$ that we assume to be continuous and bounded.
The space $\mathcal{H}_\Theta$ is the RKHS associated with $\Theta$.
Fix $\vartheta\in\RR^d$ and let $\pi_\vartheta$ be the $1$-step policy induced by the preference $h_\vartheta$ (with baseline policy $\overline{\pi}$ as usual).

Let $\varphi_k=\psi_k$ if $k\leq d$, and otherwise define $\varphi_k$ such that $\{\varphi_k;k\geq d+1\}$ is an orthonormal basis of $(\mathcal{H}_\Theta)^\perp$, the orthogonal complement of $\mathcal{H}_\Theta$ in $L^2(\nu(\mathrm{d}s)\pi_\vartheta(\mathrm{d}a|s))$.
For any $d'$, let $\mathcal{F}_{d'}:=\{h=\sum_{k=1}^{d'}\theta_k\varphi_k\}$.
We denote by $\mathscr{P}(d')$ the set of policies whose preferences belong to $\mathcal{F}_{d'}$.

The map
\begin{align*}
    F:\theta&\mapsto\int_\mathcal{S}\nu(\mathrm{d}s)\log\int_\mathcal{A}\overline{\pi}(\mathrm{d}a|s)e^{h_\theta(a,s)}\\
    \RR^{d'}&\to\RR
\end{align*}
is strictly convex on $\mathscr{P}(d)$.
Indeed, it is straightforward to compute
\begin{align*}
    \partial_{\theta_i}F(\theta)
    &=\int_\mathcal{S}\nu(\mathrm{d}s)\int_\mathcal{A}\pi_\theta(\mathrm{d}a|s)\varphi_i(a,s),
\end{align*}
and then
\begin{align*}
    \nabla_\theta\nabla_\theta F(\theta)
    &=\left(\int_\mathcal{S}\nu(\mathrm{d}s)\int_\mathcal{A}\pi_\theta(\mathrm{d}a|s)\varphi_j(a,s)(\varphi_j(a,s)-\EE_{\pi_\theta}[\varphi_j(A,s)])\right)_{i,j\leq d'}\\
    &=\int_{\mathcal{S}}\nu(\mathrm{d}s)\mathrm{Var}_{\pi_\theta}\left[\varphi(A,s)\right],
\end{align*}
where $\mathrm{Var}_{\pi_\theta}\left[\varphi(A,s)\right]$ is the covariance matrix of $\varphi(A,s)$ for $A\sim\pi_\theta(\cdot|s)$.
In particular, it is positive definite for all $s$, which implies that $\nabla_\theta\nabla_\theta F$ is positive definite, and then that $F$ is strictly convex.

As a strictly convex map, $F$ induces a \textit{Bregman divergence} on $\mathscr{P}(d')$, given in terms of the coordinate system $\theta\in\RR^{d'}$ by
\begin{align*}
    D_F(\theta,\theta'):&=F(\theta)-F(\theta')-\nabla F(\theta')\cdot(\theta-\theta')\\
    &=\int_{\mathcal{S}}\nu(\mathrm{d}s)\DKL{\pi_{\theta'}}{\pi_\theta}(s).
\end{align*}
More generally, $D_F(\pi,\pi'):=\int_{\mathcal{S}}\nu(\mathrm{d}s)\DKL{\pi'}{\pi}(s)$ is well defined for any policies $\pi,\pi'\in\mathcal{P}$.
One can define a dual coordinate system $\xi(\theta):=\nabla F(\theta)$, and the manifold $\mathscr{P}(d')$ is said to be \textit{dually flat}, as each coordinate system induces a notion of flatness.


Recall that $\vartheta\in\RR^d$ is fixed and let $\widehat{Q}\in L^2(\nu(\mathrm{d}s)\pi_\vartheta(\mathrm{d}a|s))$, with $\widehat{\pi}$ the induced policy.
In particular, we can write $\widehat{Q}=\sum_{k=1}^\infty \widehat{\theta}_k\varphi_k$.
For all $d'\geq 1$, let $\widehat{\pi}_{d'}\in\mathscr{P}(d')$ be the policy induced by $\widehat{h}_{d'}:=\sum_{k=1}^{d'}\widehat{\theta}_k\varphi_k\in\mathcal{F}_{d'}$.
Note that $\mathscr{P}(d)$ is a flat submanifold of $\mathscr{P}(d')$.
In particular, by Theorem 1.5 in \cite{amari16} p.27, we have that
\begin{align*}
    \widehat{\pi}_{d}^{d'}:=\mathrm{argmin}_{\pi_\theta\in\mathscr{P}(d)}D_F(\widehat{\pi}_{d'},\pi_\theta)
\end{align*}
is unique,
and moreover,
\begin{align*}
    D_F(\widehat{\pi}_{d'},\pi_\vartheta)
    =D_F(\widehat{\pi}_{d'},\widehat{\pi}_{d}^{d'})+D_F(\widehat{\pi}_{d}^{d'},\pi_\vartheta).
\end{align*}
We now extend this identity to the infinite dimensional case, that is, with $\widehat{\pi}$ in place of $\widehat{\pi}_{d'}$.

Firstly, it is clear that $\widehat{\pi}_{d'}\to\widehat{\pi}$ as $d'\to\infty$.
Since $D_F$ is continuous, the Maximum Theorem (see p.116 of \cite{berge63}) entails that
\begin{align*}
    \pi^{\infty}_d:=\lim_{d'\to\infty}\widehat{\pi}^{d'}_d=\mathrm{argmin}_{\pi_\theta\in\mathscr{P}(d)}D_F(\widehat{\pi},\pi_\theta),
\end{align*}
and then
\begin{align*}
    D_F(\widehat{\pi},\pi_\vartheta)
    =D_F(\widehat{\pi},\widehat{\pi}_{d}^{\infty})+D_F(\widehat{\pi}_{d}^{\infty},\pi_\vartheta).
\end{align*}
(Alternatively, one can show the above as Equation (4) in \cite{fukumizu05})

From the above equation we easily deduce the following Lemma:

\begin{lem}\label{lem: same critical points for divergence}
    With the notation introduced above, the vector $\vartheta\in\RR^d$ is a critical point of $\theta\mapsto D_F(\widehat{\pi},\pi_\theta)$ if and only if it is a critical point of $\theta\mapsto D_F(\widehat{\pi}_d^{\infty},\pi_\theta)$.
\end{lem}



\section{Proofs}
\label{appendix: general state space}

\subsection{Matryoshka Policy Gradient Theorem}

\begin{proof}[\textbf{Proof of Theorem \ref{thm: policy gradient theorem}}]
    Let $\mathbf{m}_\pi^{(i)}$ denote the law of $S_{n-i}$, that is, the $(n-i)$-th visited state under $\pi$. 
    The distribution of the sequence $S_0,A_0,\ldots,A_{n-i-1},S_{n-i}$ is not influenced by the parameters $\theta^{(i)}$, thus we can write
    \begin{align*}
        \nabla_{\theta^{(i)}}J_n(\pi_t)
        &=\int_{\mathcal{S}}\nabla_{\theta^{(i)}}V_{\pi_t}^{(n)}(s)\nu_0(\mathrm{d}s)\\
        &=\nabla_{\theta^{(i)}}\bigg(\EE_{\pi_t}\bigg[\sum_{\ell=0}^{n-i-1}R_\ell-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_\ell)\bigg]\\
        &\hspace{2cm}+\EE_{S_{n-i}\sim\mathbf{m}_{\pi_t}^{(i)}}\bigg[\EE_{\pi_t}\bigg[\sum_{\ell=n-i}^{n}R_\ell-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_\ell)
        \Big|S_{n-i}\bigg]\bigg]\bigg)\\
        &=0+\EE_{S_{n-i}\sim\mathbf{m}_{\pi_t}^{(i)}}\bigg[\nabla_{\theta^{(i)}}V_{\pi_t}^{(i)}(S_{n-i})\bigg],
    \end{align*}
    where we have used the Markov property.
    We then have that
    \begin{align*}
        \nabla_{\theta^{(i)}}V_{\pi_t}^{(i)}(S_{n-i})
        &=\nabla_{\theta^{(i)}}\EE_{T_{n,i}(\pi_t)}\bigg[\sum_{\ell=n-i}^{n}R_\ell-\tau\log\frac{\pi^{(n-\ell)}_t}{\overline{\pi}}(A_\ell|S_\ell)\bigg|S_{n-i}\bigg]\\
        &=\EE_{T_{n,i}(\pi_t)}\bigg[\bigg(\sum_{\ell=n-i}^{n}\bigg(R_\ell-\tau\log\frac{\pi^{(n-\ell)}_t}{\overline{\pi}}(A_\ell|S_\ell)\bigg)-\tau\bigg)
        \nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i})\Big|S_{n-i}\bigg]\\
        &=\EE_{T_{n,i}(\pi_t)}\bigg[\sum_{\ell=n-i}^{n}\bigg(R_\ell-\tau\log\frac{\pi^{(n-\ell)}_t}{\overline{\pi}}(A_\ell|S_\ell)\bigg)
        \nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i})\Big|S_{n-i}\bigg],
    \end{align*}
    where we have used \eqref{eq: softmax gradient cancels expected constant} to get rid of $\tau$.
    
    Recalling the MPG update \eqref{eq: update cascade learning}, we thus have proved that $\EE[\theta_{t+1}-\theta_t]=\eta\nabla_{\theta}J_n(\pi_t)$; the convergence follows from Lemma \ref{lem: Lipschitz objective}.
\end{proof}

\begin{comment}
\francois{One argument there could be of interest to show global optimality, in case there's a pb with the current (hopefully better proof).}
\begin{proof}
    Under \ref{assumption: general case pd kernel}, the eigenfunctions $(e^{(i)}_j)_{j\geq 1}$ form a basis of $L^2(\mathcal{A}\times\mathcal{S})$ for all $i=1,\ldots,n$.
    \francois{Eigenfunctions of the integral operator defined with $\pi_\infty^{(i)}$. Clarify after reorganization.}
    Hence, if there exists $\pi\in\mathcal{P}_n$ such that $J_n(\pi_t + \epsilon(\pi-\pi_t))>J_n(\pi_t)$ for all $\epsilon>0$ small enough, then $\nabla_\theta J(\pi)\neq 0$. \francois{Should I clarify this fact?}
    That is what we show now.
    
    Suppose that $\pi_t\neq\pi_*$ and let $m$ be the smallest integer in $\{1,\ldots,n\}$ such that $\pi_t^{(i)}\neq\pi_*^{(i)}$.
    We have in particular for all $s\in\mathcal{S}$ that
    \begin{align*}
        V_{\pi_t}^{(m)}(s)
        &=\EE_{\pi_t^{(m)}}\left[R_0\right]-\tau\DKL{\pi_t^{(m)}}{\overline{\pi}}(s)+\EE_{\pi_t^{(m)}}\left[V_*^{(m-1)}(S_1)\right]\\
        &=V_*^{(m)}(s)-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(s).
    \end{align*}
    Let $\pi_{t,\epsilon}\in\mathcal{P}_n$ be identical to the policy $\pi_t$ except for the $m$-step policy $\pi_t^{(m)}$ that is replaced by $\pi_{t,\epsilon}^{(m)}:=\pi_t^{(m)} + \epsilon(\pi_*^{(m)}-\pi_t^{(m)})$, for $\epsilon\in\intervalleff{0}{1}$. (Note that $\pi_{t,\epsilon}^{(m)}$ is a valid $m$-step policy for any $\epsilon\in\intervalleff{0}{1}$.)
    Below, with an abuse of notation, we write $\pi_{t,\epsilon}$ and $\pi_t$ for policies in $\mathcal{P}_i$, $i\in\{1,\ldots,n\}$, constructed from $\pi_{t,\epsilon},\pi_t\in\mathcal{P}_n$ by considering the $1,\ldots,i$-step policies only, when the context lifts the ambiguity.
    
    One sees that $V_{\pi_{t,\epsilon}}^{(m)}(s)\geq V_{\pi_t}^{(m)}(s)$ for all $s\in\mathcal{S}$, with strict inequality for some $s_0$.
    If $\mathcal{S}$ is finite, then $s_0$ has a positive Lebesgue measure on $\mathcal{S}$.
    Otherwise, by \ref{assumption: continuous environment}, there exists an open set $U\subset\mathcal{S}$ containing $s_0$ such that $U$ has positive Lebesgue measure and $V_{\pi_{t,\epsilon}}^{(m)}(s)>V_{\pi_t}^{(m)}(s)$ for all $s\in U$.
    If $m=n$, then note that we get $J_n(\pi_{t,\epsilon})>J_n(\pi_t)$, which implies the claim, as explained at the beginning of the proof.
    
    Suppose that $m<n$.
    For all $a\in\mathcal{A}$ and $s'\in\mathcal{S}$, let $f_{a,s'}:\mathcal{S}\to\intervalleff{0}{1}$, defined by $f_{a,s'}(s):=\pi_t^{(m+1)}(a|s)p(s,a,s')$.
    By Assumption \ref{assumption: continuous environment} and the fact that $\pi_t^{(m+1)}$ is a stochastic policy, it is easy to see that $f_{a,s'}$ is continuous.
    Therefore, it holds that $U':=\bigcup_{(a,s')\in\mathcal{A}\times U}f_{a,s'}^{-1}(\intervalleof{0}{1})$ is open in $\mathcal{S}$ as a union of open sets.
    Moreover, it is non-empty since the MDP is irreducible, and therefore has a positive Lebesgue measure \francois{irreducible MDP, state it somewhere and explain the consequences}.
    On the other hand, it is readily seen from the recursive structure of the value functions that there does not exist any $s\in\mathcal{S}$ such that $V_{\pi_t}^{(m+1)}(s)<V_{\pi_t}^{(m+1)}(s)$.
    Since by definition $V_{\pi_{t,\epsilon}}^{(m+1)}(s)>V_{\pi_t}^{(m+1)}(s)$ for all $s\in U'$, we deduce that $J_{m+1}(\pi_{t,\epsilon})>J_{m+1}(\pi_t)$.
    
    If $m<n-1$, the same argument can be applied several times, showing that $J_{n}(\pi_{t,\epsilon})>J_{n}(\pi_t)$.
    This in turn implies the claim, thus concluding the proof.
\end{proof}
\end{comment}

\subsection{On the optimal policy}

\begin{proof}[\textbf{Proof of Lemma \ref{lem: general case V_* = log expectation}}]
    By definition, we write
    \begin{align*}
        V_*^{(n)}(s)
        &=\tau\int_{\mathcal{A}}\pi_*^{(n)}(\mathrm{d}a|s)\left(Q_*^{(n)}(a,s)-\tau\log\frac{\pi_*^{(n)}}{\overline{\pi}}(a|s)\right)\\
        &=\tau\log\EE_{\overline{\pi}}\left[\exp(Q_*^{(n)}(A,s)/\tau)\right]
        \int_{\mathcal{A}}\overline{\pi}(\mathrm{d}a|s)\frac{\exp\left(Q_*^{(n)}(a,s)/\tau\right)}{\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(n)}(A,s)/\tau\right)\right]}\\
        &=\tau\log\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(n)}(A,s)/\tau\right)\right],
    \end{align*}
    as claimed, which concludes the proof.
\end{proof}

\begin{proof}[\textbf{Proof of Proposition \ref{prop: extending horizon converges to standard optimal policy}}]
    
    (i) Let $\pi\in\mathcal{P}$ be any standard policy, and let $\pi_n=(\pi,\ldots,\pi)\in\mathcal{P}_n$.
    By definition of the standard infinite-horizon discounted objective $J_\infty$, using the dominated convergence theorem (rewards are bounded), we have that $J_n(\pi_n)\to J_\infty(\pi)$.
    In particular, we get that $\pi_{*,n}^{(n)}$ achieves a performance arbitrarily close to that of $\pi_{*,\infty}$ in the infinite horizon discounted setting, and since the optimal policy of $J_\infty$ is unique (Lebesgue almost everywhere), we deduce that $\pi_{*,n}^{(n)}\to\pi_{*,\infty}$ as $n\to\infty$.
    
    
    
    
    (ii)
    Suppose that $J_1(\pi_{*,1})>J_1(T_{n,1}(\pi_{*,n}))$, that is
    \begin{align*}
        \int_{\mathcal{S}}V_{\pi_{*,1}}^{(1)}(s)\nu_0(\mathrm{d}s)
        >\int_{\mathcal{S}}V_{\pi_{*,n}}^{(1)}(s)\nu_0(\mathrm{d}s).
    \end{align*}
    In particular, the set $\widetilde{\mathcal{S}}\subset\mathcal{S}$ such that $s\in\widetilde{\mathcal{S}}$ if and only if $V_{\pi_{*,1}}^{(1)}(s)>V_{\pi_{*,n}}^{(1)}(s)$ is non-empty and has a positive Lebesgue measure.
    Furthermore, by optimality, $s\in\mathcal{S}\setminus\widetilde{\mathcal{S}}$ if and only if $V_{\pi_{*,1}}^{(1)}(s)=V_{\pi_{*,n}}^{(1)}(s)$.
    Let $\widetilde{\pi}_{*,n}\in\mathcal{P}_n$ be identical to $\pi_{*,n}$ except for the 1-step policy where $\pi_{*,n}^{(1)}$ is replaced by $\pi_{*,1}$.
    Then, the recursive structure of the value function \eqref{eq: gen case recursive definition value function} entails that $J_n(\widetilde{\pi}_{*,n})>J_n(\pi_{*,n})$ (we implicitly use that the MDP preserves the absolute continuity of its state's law), which is a contradiction.
    Therefore, $T_{n,1}(\pi_{*,n})=\pi_{*,1}$.
    
    Then, by induction and using the recursive structure of the value function, the same argument shows that $T_{n,m}(\pi_{*,n})=\pi_{*,m}$ for all $m=2,\ldots,n-1$, which concludes the proof.
\end{proof}


\subsection{Global optimality of MPG: realizable case}




\begin{lem}\label{lem: general case value function is optimal value function minus DKL}
    For all $n\geq 1$, all $\pi\in\mathcal{P}_n$ and all $s\in\mathcal{S}$, it holds that
    \begin{align*}
        &V_{\pi}^{(n)}(s)
        -V_*^{(n)}(s)
        =-\EE_{\pi}\left[\left.\sum_{i=0}^{n-1}\DKL{\pi^{(n-i)}}{\pi_*^{(n-i)}}(S_i)\right|S_0=s\right].
    \end{align*}
\end{lem}

\begin{proof}
    Recall \eqref{eq: gen case recursive definition value function} and write
    \begin{align*}
        V_{\pi}^{(n)}(s)
        &=\int_{\mathcal{A}}\pi^{(n)}(\mathrm{d}a|s)\bigg(r(a,s) - \tau\log\frac{\pi^{(n)}}{\overline{\pi}}(a|s) + \int_{\mathcal{S}}p(s,a,\mathrm{d}s')V^{(n-1)}_{\pi}(s')\bigg)\\
        &=\int_{\mathcal{A}}\pi^{(n)}(\mathrm{d}a|s)\bigg(V_*^{(n)}(s) - \tau\log\frac{\pi^{(n)}}{\pi_*}(a|s) + \int_{\mathcal{S}}p(s,a,\mathrm{d}s')(V^{(n-1)}_{\pi}(s')-V_*^{(n-1)}(s'))\bigg),
    \end{align*}
    where we plugged in the expression of the optimal policy \eqref{eq: general case optimal policy}.
    We can rewrite the above as
    \begin{align*}
        &V_{\pi}^{(n)}(s)-V_*^{(n)}(s)
        =-\DKL{\pi^{(n)}}{\pi_*^{(n)}} + \EE\left[V^{(n-1)}_{\pi}(S_1)-V_*^{(n-1)}(S_1)\Big|S_0=s\right].
    \end{align*}
    The claim follows by induction.
\end{proof}


\begin{proof}[\textbf{Proof of Proposition \ref{prop: optimal policy}}]
    The Kullback-Leibler divergence being non-negative, it is readily seen that for all $s\in\mathcal{S}$, the maximal value of $\pi\mapsto V_{\pi}^{(n)}(s)$ is obtained for $\pi=\pi_*$.
    It is then immediate that $\pi_*$ is the unique optimal policy (Lebesgue-almost everywhere) for the objective $J_n$ given in \eqref{def: general case objective function}.
\end{proof}


\begin{lem}\label{lem: general case expected update log pi_t}
    Let $t\in\NN$ and $m\in\{1,\ldots,n\}$.
    Suppose that $\pi_t^{(k)}=\pi_*^{(k)}$ for all $k=1,\ldots,m-1$.
    For all $a\in\mathcal{A}$ and $s\in\mathcal{S}$, it holds that
    \begin{align*}
        &\log\pi^{(m)}_{t+1}(a|s)-\log\pi^{(m)}_t(a|s)\\
        &\hspace{1cm}= - \learningrate \tau\int_{\mathcal{S}}\mathbf{m}_{\pi_t}^{(m)}(\mathrm{d}s')\int_{\mathcal{A}}\pi^{(m)}_t(\mathrm{d}a'|s')
        \left(\log\frac{\pi^{(m)}_t}{\pi^{(m)}_*}(a'|s')-\DKL{\pi^{(m)}_t}{\pi^{(m)}_*}(s')\right)\\
        &\hspace{3cm}\times\left(\Theta^{(m)}((a,s),(a',s'))-\EE_{\pi^{(m)}_t}[\Theta^{(m)}((A,s),(a',s'))]\right)
        +o\left(\learningrate C(\theta_t)\right),
    \end{align*}
    where $\mathbf{m}_{\pi_t}^{(m)}$ is the law of $S_{n-m}$ under policy $\pi_t$ and the constant $C(\theta_t)$ does not depend on $\learningrate$.
\end{lem}



\begin{comment}
{\color{olive}Proof with scaling factors $\rho$
\begin{proof}
    
    We follow the same steps as in the single state case treated in Lemma \ref{lem: bandit case expected update log pi_t}.
    The gradient of the policy reads as
    \begin{align}%\label{eq: general case gradient policy pi}
        \nabla_\theta\pi^{(m)}_t(a|s)
        =\frac{1}{\tau}\pi^{(m)}_t(a|s)\sum_{a'\in\mathcal{A}}\left(\delta_{a,a'}-\pi^{(m)}_t(a'|s)\right)\nabla_{\theta}h^{(m)}_t(a,s).
    \end{align}
    Let $(a,s)\in\mathcal{A}\times\mathcal{S}$.
    Using Lemma \ref{Lemma Boltzmann measure is log convex} and Equation \eqref{eq: update cascade learning}, we write
    \begin{align}%\label{eq: gen case log increment to compute}
        &\EE\left[\log\pi_{t+1}^{(m)}(a|s)-\log\pi_t^{(m)}(a|s)\right]
        \geq (\theta_{t+1}^{(m)}-\theta_t^{(m)})\cdot\frac{\nabla_\theta \pi_t^{(m)}(a|s)}{\pi_t^{(m)}(a|s)}\nonumber\\
        &\hspace{0.6cm}=\frac{\alpha}{\tau^2}\sum_{k=0}^{n-m}\EE_{\pi_t}\left[C_{k,m}\sum_{a',a''\in\mathcal{A}}\left(\delta_{a,a'}-\pi_t^{(m)}(a'|s)\right)\left(\delta_{A_k,a''}-\pi_t^{(m)}(a'|S_k)\right)\Theta^{(m)}((a',s),(a'',S_k))\right].
    \end{align}
    We focus on the expectation in the right-hand side.
    It is equal to
    \begin{align*}
        &\EE_{\pi_t}\bigg[C_{k,m}\bigg(\Theta^{(m)}((a,s),(A_k,S_k))-\EE_A\left[\Theta^{(m)}((A,s),(A_k,S_k))\right]\\
        &\hspace{5cm}-\EE_A\left[\Theta^{(m)}((a,s),(A',S_k))\right]+\EE_{A,A'}\left[\Theta^{(m)}((A,s),(A',S_k))\right]\bigg)\bigg],
    \end{align*}
    where $A,A'$ have respective laws $\pi_t^{(m)}(\cdot|s)$ and $\pi_t^{(m)}(\cdot|S_k)$ and are mutually independent of all other variables (conditionally given $S_k$ for $A'$).
    Using the trick $\EE[X(Y-\EE[Y])]=\EE_[(X-\EE[X])Y]$, we obtain
    \begin{align}%\label{eq: gen case after expectation trick}
        \EE_{\pi_t}\left[\left(C_{k,m}-\EE\left[C_{k,m}|S_k\right]\right)\left(\Theta^{(m)}((a,s),(A_k,S_k))-\EE_A\left[\Theta^{(m)}((A,s),(A_k,S_k))\right]\right)\right],
    \end{align}
    where
    \begin{align*}
        \EE\left[C_{k,m}|S_k\right]
        &=\EE\left[\rho_{m,k}\sum_{\ell=0}^{m-1}\left(R_{k+\ell}-\tau\log\frac{\pi_t^{(m-\ell)}}{\overline{\pi}}(A_{k+\ell}|S_{k+\ell})\right)\bigg|S_k\right].
    \end{align*}
    In the expectation, $A_{k+\ell}$ has law $\pi_t^{(k+\ell)}(\cdot|S_{k+\ell})$ conditionally given $S_{k+\ell}$.
    Hence, with the factor $\frac{\pi_t^{(m-\ell)}}{\pi_t^{(k+\ell)}}(A_{k+\ell}|S_{k+\ell})$ in the factor $\rho_{m,k}$, we get 
    \begin{align*}
        \EE\left[C_{k,m}|S_k\right]
        &=\EE\left[\sum_{\ell=0}^{m-1}\left(R_{k+\ell}'-\tau\log\frac{\pi_t^{(m-\ell)}}{\overline{\pi}}(A_{k+\ell}'|S_{k+\ell}')\right)\bigg|S_k\right],
    \end{align*}
    where $A_{k+\ell}'$ has conditional law $\pi_t^{(m-\ell)}(\cdot|S_{k+\ell}')$ given $S_{k+\ell}'$, and the laws of $R_{k+\ell}'$ and $S_{k+\ell}'$ are changed accordingly.
    In particular, $\EE\left[C_{k,m}|S_k\right]=V_{\pi_t}^{(m)}(S_k)$.
    The expression in \eqref{eq: gen case after expectation trick} can thus be written as
    \begin{align*}
        &\EE_{\pi_t}\left[\left(C_{k,m}-V_{\pi_t}^{(m)}(S_k)\right)\left(\Theta^{(m)}((a,s),(A_k,S_k))-\EE_A\left[\Theta^{(m)}((A,s),(A_k,S_k))\right]\right)\right],\\
        \shortintertext{using Lemma \ref{lem: general case consistency} and replacing $A_{k+\ell},S_{k+\ell}$ with $A_\ell',S_\ell'$ â€“â€“ such that $A_\ell'$ has conditional law $\pi_t^{(m-\ell)}(\cdot|S_\ell')$ given $S_\ell'$ â€“â€“ yield}
        &\hspace{1cm}=\EE_{\pi_t}\bigg[\bigg(V_*^{(m)}(S_0)-V_{\pi_t}^{(m)}(S_0)-\sum_{\ell=0}^{m-1}\tau\log\frac{\pi_t^{(m-\ell)}}{\pi_*^{(m-\ell)}}(A_\ell'|S_\ell')\bigg)\\
        &\hspace{5cm}\times\left(\Theta^{(m)}((a,s),(A_0',S_0'))-\EE_A\left[\Theta^{(m)}((A,s),(A_0',S_0'))\right]\right)\bigg].
    \end{align*}
    Then, Lemma \ref{lem: general case value function is optimal value function minus DKL} shows that the above is equal to
    \begin{align*}
        &\sum_{\ell=0}^{m-1}\tau\EE_{\pi_t}\bigg[\bigg(\EE_{\pi_t}\bigg[\DKL{\pi_t^{(m-\ell)}}{\pi_*^{(m-\ell)}}(A_\ell'|S_\ell')\bigg]-\log\frac{\pi_t^{(m-\ell)}}{\pi_*^{(m-\ell)}}(A_\ell'|S_\ell')\bigg)\\
        &\hspace{5cm}\times\left(\Theta^{(m)}((a,s),(A_0',S_0'))-\EE_A\left[\Theta^{(m)}((A,s),(A_0',S_0'))\right]\right)\bigg]\\
        &\hspace{1cm}=\tau\EE_{\pi_t}\bigg[\bigg(\DKL{\pi_t^{(m)}}{\pi*^{(m)}}(A_0'|S_0')-\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_0'|S_0')\bigg)\\
        &\hspace{5cm}\times\left(\Theta^{(m)}((a,s),(A_0',S_0'))-\EE_A\left[\Theta^{(m)}((A,s),(A_0',S_0'))\right]\right)\bigg],
    \end{align*}
    where we used the fact that $\pi_t^{(k)}=\pi_*^{(k)}$ for all $k=1,\cdots,m-1$.
    Plugging the right-hand side above in \eqref{eq: gen case log increment to compute} in place of the expectation, we obtain the claim, which concludes the proof.
\end{proof}
}
\end{comment}




\begin{proof}
    The gradient of the policy reads as
    \begin{align}\label{eq: general case gradient policy pi}
        \nabla_\theta\pi^{(m)}_t(a|s)
        &=\frac{1}{\tau}\pi^{(m)}_t(a|s)\int_{\mathcal{A}}\left(\delta_{a,\mathrm{d}a'}-\pi^{(m)}_t(\mathrm{d}a'|s)\right)
        \nabla_{\theta}h^{(m)}_t(a,s).
    \end{align}
    Let $(a,s)\in\mathcal{A}\times\mathcal{S}$.
    Using \eqref{eq: update cascade learning} and a first order Taylor approximation, we write
    \begin{align}\label{eq: gen case log increment to compute}
        \log\pi_{t+1}^{(m)}(a|s)-\log\pi_t^{(m)}(a|s)
        &= (\theta_{t+1}^{(m)}-\theta_t^{(m)})\cdot\frac{\nabla_\theta \pi_t^{(m)}(a|s)}{\pi_t^{(m)}(a|s)}+o\left(\learningrate C(\theta_t)\right)\nonumber\\
        &=\frac{\learningrate}{\tau^2}\EE_{\pi_t}\bigg[C_m\int_{\mathcal{A}\times\mathcal{A}}\left(\delta_{a,\mathrm{d}a'}-\pi_t^{(m)}(\mathrm{d}a'|s)\right)\nonumber\\
        &\hspace{0.4cm}\times\left(\delta_{A_{n-m},\mathrm{d}a''}-\pi_t^{(m)}(\mathrm{d}a''|S_{n-m})\right)\Theta^{(m)}((a',s),(a'',S_{n-m}))\bigg]\nonumber\\
        &\hspace{7.5cm}+o\left(\learningrate C(\theta_t)\right).
    \end{align}
    We focus on the expectation.
    It is equal to
    \begin{align*}
        &\EE_{\pi_t}\bigg[C_m\bigg(\Theta^{(m)}((a,s),(A_{n-m},S_{n-m}))
        -\EE_A\left[\Theta^{(m)}((A,s),(A_{n-m},S_{n-m}))\right]\\
        &\hspace{3cm}-\EE_A\left[\Theta^{(m)}((a,s),(A',S_{n-m}))\right]
        +\EE_{A,A'}\left[\Theta^{(m)}((A,s),(A',S_{n-m}))\right]\bigg)\bigg],
    \end{align*}
    where $A,A'$ have respective laws $\pi_t^{(m)}(\cdot|s)$ and $\pi_t^{(m)}(\cdot|S_{n-m})$ and are mutually independent of all other variables (conditionally given $S_{n-m}$ for $A'$).
    Using the trick $\EE[X(Y-\EE[Y])]=\EE_[(X-\EE[X])Y]$, we obtain
    \begin{align}\label{eq: gen case after expectation trick}
        &\EE_{\pi_t}\Big[\left(C_m-\EE\left[C_m|S_{n-m}\right]\right)\Big(\Theta^{(m)}((a,s),(A_{n-m},S_{n-m}))
        -\EE_A\left[\Theta^{(m)}((A,s),(A_{n-m},S_{n-m}))\right]\Big)\Big].
    \end{align}
    We write
    \begin{align*}
        \EE\left[C_m|S_{n-m}\right]
        &=\EE\left[\sum_{\ell=n-m}^{n}\left(R_{\ell}-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_{\ell})\right)\bigg|S_{n-m}\right]\\
        &=V_{\pi_t}^{(m)}(S_{n-m})\\
        &=V_*^{(m)}(S_{n-m})-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(S_{n-m}),
    \end{align*}
    where we used Lemma \ref{lem: general case value function is optimal value function minus DKL} and the fact that $\pi_t^{(i)}=\pi_*^{(i)}$ for all $i=1,\ldots,m-1$.
    Similarly and using the expression \eqref{eq: general case optimal policy} of the optimal policy, we have
    \begin{align*}
        \EE[C_m|S_{n-m},A_{n-m}]
        &=R_{n-m}-\tau\log\frac{\pi_t^{(m)}}{\overline{\pi}}(A_{n-m}|S_{n-m}) + \EE\left[V_{\pi_t}^{(m-1)}(S_{n-m})\Big|S_{n-m},A_{n-m}\right]\\
        &= \EE\Big[V_{\pi_t}^{(m-1)}(S_{n-m+1})-V_*^{(m-1)}(S_{n-m+1})\Big|S_{n-m},A_{n-m}\Big]\\
        &\hspace{4cm}-\tau\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})+V_*^{(m)}(S_{n-m})\\
        &= -\tau\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})+V_*^{(m)}(S_{n-m}).
    \end{align*}
    Hence, the expression in \eqref{eq: gen case after expectation trick} becomes
    \begin{align*}
        &\tau\EE_{\pi_t}\bigg[\bigg(\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})
        -\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})\bigg)
        \Big(\Theta^{(m)}((a,s),(A_{n-m},S_{n-m}))\\
        &\hspace{8.7cm}-\EE_A\left[\Theta^{(m)}((A,s),(A_{n-m},S_{n-m}))\right]\Big)\bigg],
    \end{align*}
    which corresponds to the first order term in right-hand side of the equation in the Lemma.
    Coming back to \eqref{eq: gen case log increment to compute}, this concludes the proof.
\end{proof}



\begin{proof}[\textbf{Proof of Theorem \ref{thm: general case global optimality}}]
    
We reason by induction.
Let $m\leq n$, suppose that $\pi^{(i)}_t\equiv\pi^{(i)}_*$ for all $i=1,\ldots,m-1$, and that $\pi^{(i)}_t=\pi^{(i)}_\infty$ for all $i=m,\ldots,n$.
In particular, we are at a critical point $(\theta^{(1)}_t,\ldots,\theta^{(n)}_t)$ of $(\theta^{(1)},\ldots,\theta^{(n)})\mapsto J_n(\pi_\theta)$.
Let $a\in\mathcal{A},s\in\mathcal{S}$.
By Lemma \ref{lem: general case expected update log pi_t}, we have that 
\begin{align*}
    0&=
    \log\pi^{(m)}_{t+1}(a|s)-\log\pi^{(m)}_t(a|s)\\
    &= - \learningrate \tau\int_{\mathcal{A}\times\mathcal{S}}\mathbf{m}_{\pi_t}^{(m)}(\mathrm{d}s')\pi_t^{(m)}(\mathrm{d}a'|s')\left(\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(a'|s')-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(s')\right)\\
    &\hspace{3cm}\times\bigg(\Theta^{(m)}((a,s),(a',s'))
    -\EE_{\pi^{(m)}_t}[\Theta^{(m)}((A,s),(a',s'))]\bigg)
    +o\left(\learningrate C(\theta_t)\right),
\end{align*}
Since the above must be true for all $\learningrate>0$, we deduce that 
\begin{align}\label{eq: gen case lower bound log increment with d}
    &\int_{\mathcal{A}\times\mathcal{S}}\mathbf{m}_{\pi_t}^{(m)}(\mathrm{d}s')\pi_t^{(m)}(\mathrm{d}a'|s')\left(\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(a'|s')-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(s')\right)\nonumber\\
    &\hspace{3cm}\times\bigg(\Theta^{(m)}((a,s),(a',s'))
    -\EE_{\pi^{(m)}_t}[\Theta^{(m)}((A,s),(a',s'))]\bigg)=0.
\end{align}
Let $\widetilde{\Theta}^{(m)}$ be the positive-semidefinite kernel constructed from $\Theta^{(m)}$ and $\pi_t^{(m)}$ as in Lemma \ref{lem: softmax RKHS is larger}.
One can easily check that
\begin{align*}
    \log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(a|s)-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}
    &=h_t^{(m)}(a,s)-Q_*^{(m)}(a,s)-\EE_{\pi_t^{(m)}}\left[h_t^{(m)}(A,s)-Q_*^{(m)}(A,s)\right].
\end{align*}
In particular, using the trick $\EE[X(Y-\EE[Y])]=\EE[(X-\EE[X])Y]$, we can rewrite \eqref{eq: gen case lower bound log increment with d} as
\begin{align}\label{eq: h-Q in softmax RKHS}
    \int_{\mathcal{A}\times\mathcal{S}}\mathbf{m}_{\pi_t}^{(m)}(\mathrm{d}s')\pi_t^{(m)}(\mathrm{d}a'|s')\left(h_t^{(m)}(a',s')-Q_*^{(m)}(a',s')\right)\widetilde{\Theta}^{(m)}((a,s),(a',s'))=0.
\end{align}
Since the above is true for all $(a,s)\in\mathcal{A}\times\mathcal{S}$, we see by Lemma \ref{lem: if I_Kf=0 then in ortho} that $h_t^{(m)}-Q_*^{(m)}\in(\mathcal{H}_{\widetilde{\Theta}^{(m)}})^\perp$, that is the orthogonal complement of $\mathcal{H}_{\widetilde{\Theta}^{(m)}}$ in $L^2(\mathbf{m}_{\pi_t}^{(m)}(\mathrm{d}s')\pi_t^{(m)}(\mathrm{d}a'|s'))$.
By Assumption \ref{assumption: optimal policy in RKHS}, we get $h_t^{(m)}-Q_*^{(m)}\in\mathcal{H}_{\Theta^{(m)}}\cap(\mathcal{H}_{\widetilde{\Theta}^{(m)}})^\perp$, and Lemma \ref{lem: softmax RKHS is larger} entails that for all $s\in\mathcal{S}$, the map $a\mapsto h_t^{(m)}(a,s)-Q_*(a,s)$ is constant.
This implies in turn that $\pi_t^{(m)}=\pi_*^{(m)}$, which concludes the proof.

\end{proof}





\subsection{Global optimality of MPG: non-realizable case}
\label{Section: global optimality of MPG: non-realizable case}

In order to extend the global optimality from the case where $\pi_*$ belongs to the parametric space $\mathscr{P}_n$ to the case where $\pi_*$ is outside of $\mathscr{P}_n$, we use tools from information geometry and apply the strategy outlined in Section \ref{section main: global convergence beyond the realizability assumption}.

We use the following notation in the proof: the set of parametric $1$-step policies whose preference $h_\theta$ belongs to $\mathcal{H}_{\Theta^{(i)}}$ is denoted by $\mathscr{P}^{(i)}$.


\begin{proof}[\textbf{Proof of Theorem \ref{Thm: global cvg outside of the RKHS}}]

Let $\vartheta\in\RR^\nparam$ be a critical point of $\theta\mapsto J_n(\pi_\theta)$.
Consider a fixed $i\in\{1,\ldots, n\}$.
Recall that $Q_{\pi_\vartheta}^{(i)}(a,s)=r(a,s)+\int_{\mathcal{S}}p(s,a,\mathrm{d}s')V_{\pi_\vartheta}^{(i-1)}(s')$, which does not depend on $\pi_\vartheta^{(j)}$, $j\geq i$.
Let $\widehat{\pi}^{(i)}$ be the policy with preference $Q_{\pi_\vartheta}^{(i)}$.
Note that $Q_{\pi_\vartheta}^{(i)}$ does not necessarily belong to $\mathcal{H}_{\Theta^{(i)}}$, hence we do not make the dependence on $\vartheta$ (which is fixed) explicit in $\widehat{\pi}^{(i)}$.
This is the optimal policy given that the shorter $j$-step policies, $j<i$, are fixed.
Indeed, we always have that
\begin{align*}
    \widehat{J}^{(i)}(\pi^{(i)},\vartheta)
    :=&\int_{\mathcal{S}}\mathbf{m}_{\pi_\vartheta}^{(i)}(\mathrm{d}s)\left(\EE_{\pi^{(i)}}[Q^{(i)}_{\pi_{\vartheta}}(A,s)]-\tau\DKL{\pi^{(i)}}{\overline{\pi}}(s)\right)\\
    =&\ \tau\int_{\mathcal{S}}\mathbf{m}_{\pi_\vartheta}^{(i)}(\mathrm{d}s)\left(\log\left(\int_{\mathcal{A}}\overline{\pi}(\mathrm{d}a|s)e^{Q^{(i)}_{\pi_\vartheta}(a,s)/\tau}\right)-\DKL{\pi^{(i)}}{\widehat{\pi}^{(i)}}(s)\right).
\end{align*}
The first term of the right-hand side depends on $\pi_\vartheta^{(j)}$ through $Q_{\pi_\vartheta}^{(i)}$ for $j<i$, whereas it depends on $\pi_\vartheta^{(j)}$ through $\mathbf{m}_{\pi_\vartheta}^{(i)}$ for $j>i$, but it does not depend on $\pi_{\vartheta}^{(i)}$.
Therefore, we see that $\widehat{\pi}^{(i)}=\mathrm{argmax}_{\pi^{(i)}\in\mathcal{P}_1}\widehat{J}^{(i)}(\pi^{(i)},\vartheta)$.
Define
\begin{align*}
    \pi^{(i)}_{\theta_*}
    =\underset{\pi^{(i)}_\theta\in\mathscr{P}^{(i)}}{\mathrm{argmin}}\ D^{(i)}(\widehat{\pi}^{(i)},\pi^{(i)}_\theta)
    :=\underset{\pi^{(i)}_\theta\in\mathscr{P}^{(i)}}{\mathrm{argmin}}\int_{\mathcal{S}}\mathbf{m}^{(i)}_{\pi_\vartheta}(\mathrm{d}s)\DKL{\pi^{(i)}_\theta}{\widehat{\pi}^{(i)}}(s)
    .
\end{align*}
\begin{comment}
\francois{Now the Pythagorean theorem is in appendix information geometry} 

therefore it satisfies the following Pythagorean identity (see Appendix \ref{appendix: information geometry}):
\begin{align}\label{eq: Pythagorean theorem }
    D(\pi_{\vartheta}^{(i)},\widehat{\pi}^{(i)})
    =D(\pi_{\vartheta}^{(i)},\pi^{(i)}_{\theta_*}) + D(\pi^{(i)}_{\theta_*},\widehat{\pi}^{(i)}),
\end{align}
where $\pi_{\theta_*}^{(i)}\in\mathrm{argmin}_{\pi_\theta^{(i)}\in\mathscr{P}^{(i)}}D(\pi_\theta^{(i)},\widehat{\pi}^{(i)})$, and where $\mathscr{P}^{(i)}$ is the set of parametric $i$-step policies, that is to say with preferences in $\mathcal{H}_{\Theta^{(i)}}$.
Note that at this point, we do not know whether the argmin is unique, nonetheless any policy in this argmin is optimal by definition.
\end{comment}
It turns out that the map $D^{(i)}$ defined above is a Bregman divergence on $\mathcal{P}$.
Using the fact that $\vartheta$ is a critical point combined with Lemma \ref{lem: same critical points for divergence}, we have that
\begin{align*}
    0&=
    \nabla_{\theta^{(i)}}J_n(\pi_\vartheta)
    =-\nabla_{\theta^{(i)}}D(\pi_{\vartheta}^{(i)},\pi^{(i)}_{\theta_*}).
\end{align*}
We stress once more that $\pi^{(i)}_{\theta_*}$ only depends on $\widehat{\pi}^{(i))}$, which in turn only depends on $\pi_\vartheta^{(1)},\ldots,\pi_\vartheta^{(i-1)}$ through $Q_{\pi_\vartheta}^{(i)}$ and on $\pi_\vartheta^{(i+1)},\ldots,\pi_\vartheta^{(n)}$ through $\mathbf{m}_{\pi_\vartheta}^{(i)}$.
Therefore, the equation above corresponds to the gradient of the objective of $1$-step MPG with optimal policy $\pi_{\theta_*}^{(i)}$.
This observation brings us back to the realizable case, for which Theorem \ref{thm: general case global optimality} applies.
This implies that necessarily, $\pi_\vartheta^{(i)}=\pi_{\theta_*}^{(i)}$.
In particular, this shows the uniqueness of the $\mathrm{argmin}$.

The above argument proves that if $\vartheta\in\RR^\nparam$ is a critical point, then
\begin{align*}
    J_n(\pi_\vartheta)
    &=\max_{\theta^{(i)}\in\RR^{\nparam_i}}J_n(\pi^{(1)}_\vartheta,\ldots,\pi^{(i)}_{\theta^{(i)}},\ldots,\pi^{(n)}_{\vartheta^{(n)}}).
\end{align*}
Since this is true for every $i=1,\ldots,n$ and since maxima can be taken in any order, we have that 
\begin{align*}
    J_n(\pi_\vartheta)=\max_{\theta\in\RR^P}J_n(\pi_\theta)
\end{align*}
We have thus proved that any critical point is a global maximum of the objective, concluding the proof.
\end{proof}

\begin{comment}
\francois{unnecessary blabla}

To prove Theorem \ref{thm: general case global optimality}, that is, global convergence of MPG when $\pi_*\in\mathscr{P}_n$, we showed that a critical point $\pi_\theta$ of the objective is necessary a policy whose preferences are such that $h^{(i)}-Q_*^{(i)}\in\mathcal{H}_{\Theta^{(i)}}\cap\mathcal{H}_{\Theta^{(i)}}$.
Since such maps are constant in $a$, we get $\pi_\theta=\pi_*$.
When $\pi_*\notin\mathscr{P}_n$, we get $h_\theta^{(i)}-Q_*^{(i)}\in(\mathcal{H}_{\widetilde{\Theta}^{(i)}})^\perp$.
This yields the projectional consistency property of the critical point, as we prove now.
\end{comment}

\begin{proof}[\textbf{Proof of Proposition \ref{prop: projectional consistency property}}]

    Suppose that $\theta_t=(\theta_t^{(1)},\ldots,\theta_t^{(n)})$ satisfies the projectional consistency property \eqref{eq: projectional consistency property}.
    We thus have that $h_t^{(1)}-Q_*^{(1)}\in(\mathcal{H}_{\Theta^{(1)}})^\perp$, the orthogonal space of $\mathcal{H}_{\Theta^{(1)}}$ in $L^2(\mathbf{m}^{(1)}(\mathrm{d}s)\pi_t^{(1)}(\mathrm{d}a))$.
    In particular, using Lemma \ref{lem: if I_Kf=0 then in ortho}, one can show that Equation \eqref{eq: h-Q in softmax RKHS} is satisfied, entailing that $\nabla_{\theta^{(1)}}J_n(\pi_\theta)=0$.
    The same reasoning applies for all steps $i=1,\ldots,n$, showing that $\theta_t$ is a critical point, and therefore, the unique global optimum by Theorem \ref{Thm: global cvg outside of the RKHS}.
    This concludes the proof.
\end{proof}





%\section{Fixed-horizon reinforcement learning}
%\label{appendix: fixed-horizon RL}



\section{Assumptions}
\label{appendix: assumptions}

We say that the MDP is irreducible if and only if
\begin{align}\label{eq: gen case irreducibility}
    &\forall s,s'\in\mathcal{S},\forall \epsilon>0,\exists k\in\mathbb{N},\exists a_0,\ldots,a_{k-1}:\nonumber\\ &\int_{B(s,\epsilon)}\mathrm{d}s_0\int_{\mathcal{S}}\mathrm{d}s_1\cdots\int_{\mathcal{S}}\mathrm{d}s_{k-1}\int_{B(s',\epsilon)}\mathrm{d}s_k
    \prod_{m=0}^{k-1}p(s_m,a_m,s_{m+1})>0,
\end{align}
where $B(s,\epsilon)$ denotes the ball $\{\widetilde{s}\in\mathcal{S}: ||s-\widetilde{s}||<\epsilon\}$.
In words, this means that any neighborhood of any state is reachable from any neighborhood of any state.

We now list the assumptions and briefly mention their roles in this work:
\begin{itemize}
    \item The initial state distribution $\nu_0$ has full support on $\mathcal{S}$: it is not restrictive, as its role is to ensure that the optimal policies for all horizons visit (Lebesgue almost all) the whole state space, thus avoiding considerations about reachable states. In particular, the optimal policy $\pi_*$ does not depend on $\nu_0$ as long as its support is full.
    \item Continuous closed $\mathcal{A},\mathcal{S}$: to apply Mercer's Theorem.
    \item Continuous and bounded kernels $\Theta^{(i)}$: to apply Mercer's Theorem
    \item Continuous reward and transition functions: imply measurability of the variables generated by the MDP, ensure Lebesgue integrability and avoid pathological cases.
    \item The MDP is irreducible: avoid technicalities.
    \item Rewards are bounded: ensures that value functions are well defined.
    \item The temperature $\tau$ is the same for all steps: it is unnecessary, as one could regularize the objectives according to the number of remaining steps. That is, $\pi^{(i)}$ could be regularized with $\tau_i$ for all $i=1,\ldots,n$ with $\tau_i\neq\tau_j$ if $i\neq j$, and the theory would still be valid.
\end{itemize}


\section{Numerical experiments}

\label{appendix: numerical experiments}
We apply MPG on a number of case studies.
The MPG is implemented as in algorithm \ref{alg:mpg}.

\begin{algorithm}
\caption{MPG implementation}\label{alg:mpg}
\begin{algorithmic}[0]
\FOR {t = 1, $...$ , horizon}
 \STATE generate trajectory $\{(s_i,s_{i+1},a_i,r_{i})\}_{i=0}^{n-1}$ from $\pi_t$ 
    \FOR {i = 1, $\cdots$ , n}
        \STATE $\pi_{t+1} \leftarrow $ update policy as in \eqref{eq: update cascade learning}
    \ENDFOR 
    \STATE decay $\tau, \eta$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Analytical task}
\label{appendix:analytical_task}

\paragraph{Set-up:} 
We consider a state-space consisting of $\mathcal{S}=\{0,1,2,3,4\}$, an action space $\mathcal{A}=\{1,2\}$. At each state $s$, the agent performs action $a$, taking the agent to the next state $(s+a)\mod 5$.

We define an orthonormal basis (in $\ell^2(\mathcal{A}\times\mathcal{S})$) of the space of functions $\{f:\mathcal{A}\times\mathcal{S}\to\RR:f(1,s)+f(2,s)=0,\ \forall s\in\mathcal{S}\}$.
Note that one can always recenter any map $g$ on $\mathcal{A}\times\mathcal{S}$ so that $g(1,s)+g(2,s)=0$, without changing the policy obtained as the softmax of $g$, in particular, any policy can be written as the softmax of such a function. 
The basis is defined as
\begin{align*}
     e_1 &=\sqrt{6}\begin{pmatrix}
    1 & -1 \\
    0 & 0 \\
    1 & -1 \\
    0 & 0 \\
    1 & -1 \\
    \end{pmatrix}, \quad     e_2 =\sqrt{4}\begin{pmatrix}
    0 & 0 \\
    1 & -1 \\
    0 & 0 \\
    1 & -1 \\
    0 & 0 \\
    \end{pmatrix}, \quad e_3 =\sqrt{4}\begin{pmatrix}
    0 & 0 \\
    1 & -1 \\
    0 & 0 \\
    -1 & 1 \\
    0 & 0 \\
    \end{pmatrix},\\
    e_4 &=\sqrt{8}\begin{pmatrix}
    -2 & 2 \\
    0 & 0 \\
    1 & -1 \\
    0 & 0 \\
    1 & -1 \\
    \end{pmatrix}, \quad e_5 =\sqrt{4}\begin{pmatrix}
    0 & 0 \\
    0 & 0 \\
    1 & -1 \\
    0 & 0 \\
    -1 & 1 \\
    \end{pmatrix}.
\end{align*}

Recall that $Q_*^{(1)}(a,s)=r(a,s)$, which can be represented by
\[Q_*^{(1)}(a,s) = \sum_{j=1}^5 \theta^*_j e_j(a,s),\]
where $\alpha_j \in \mathbb{R}$. 

\paragraph{Experiments:}
\begin{enumerate}
    \item Obtaining the first two step policies with Assumption \ref{assumption: optimal policy in RKHS}:

\textbf{Setup:}

\begin{itemize}
    \item $\theta_0$ randomly initialised with i.i.d. centered Gaussian;
    \item $\theta^* = (0,0.1,-0.15,0.05,-0.1)$;
    \item Initial learning rate $\eta_0 = 0.0001$, terminal learning rate $\eta_T = 1e-1$ such that $\eta_{t+1}=\eta_0\left(\eta_T/\eta_0\right)^{t/n_{\text{games}}}$;
    \item Number of training games $n_{\text{games}} = 5000$
    \item $\tau = 1.0$
    \item Ideal gradient update
\end{itemize}

We obtain the results reported in Figure \ref{fig:analytical_task}. Namely, using the full basis $\{e_i;i=1,\ldots,5\}$ for the parametric model, we are able to find the 1-step and 2-step policies which maximize the objective $J$, and converge towards the optimal 1-step and 2-step policies.

\item Obtaining optimal policies without Assumption \ref{assumption: optimal policy in RKHS}:
We performed the same experiment using an incomplete basis, that cannot express $Q_*^{(1)}$ nor $Q_*^{(2)}$.
Namely, we used $\{e_i;i=1,\ldots,4\}$ for both the $1$-step and the $2$-step policies, and similarly with $\{e_i;i=1,\ldots,3\}$.
In this case, we check that the limit is the only policy satisfying the projectional consistency property within the parametric policy space, see Figure \ref{fig:analytical_task_incomplete_basis}.
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Neurips2023/figures/rl_xp_4.png}~\includegraphics[width=0.5\textwidth]{Neurips2023/figures/rl_xp_3.png}
\caption{Convergence in $\ell^\infty$-norm of 5 agents with random initialisation. Left: convergence of the policies parametrised with $\{e_i;i=1,\ldots,4\}$. Right: convergence of the policies parametrised with $\{e_i;i=1,\ldots,3\}$.}\label{fig:analytical_task_incomplete_basis}
\end{figure}

%\maria{we can also show here the non exact gradient update ?}
 
\begin{comment}
\item{Obtaining first two step policies without \ref{assumption: optimal policy in RKHS} assumption

We have that $Q_*^{(1)}(a,s)\in\mathcal{H}$, and that $\mathcal{H}_{\Theta^{(1)}}$ is a subspace of $\mathcal{H}$. We recover policy 
\begin{align*}
    \pi_\bullet^{(1)}(a|s)
    :=\overline{\pi}(a|s)\frac{\exp\left(Q^{(1)}_\bullet(a,s)/\tau\right)}{\int_\mathcal{A}\overline{\pi}(\mathrm{d}a'|s)\exp\left(Q^{(1)}_\bullet(a',s)/\tau\right)}.
\end{align*}
where $Q^{(1)}_\bullet$ is the orthogonal projection of $Q^{(1)}_*$ onto $\mathcal{H}_{\Theta^{(1)}}$.

Then, the best possible two step policy, using an approximate 1-step policy, is given by: $\overline{Q}_*^{(2)}(a,s):=r(a,s)+\int_{\mathcal{S}}p(s,a,\mathrm{d}s')V_\bullet^{(1)}(s')$. We write $V_\bullet^{(1)}$ for the induced value function using $Q^{(1)}_\bullet$.

\textbf{Setup:}

\begin{itemize}
    \item $\theta_0$ initialised to all zeros
    \item $\theta^* = (1, 1, 3, 2, 1, 1, 0.2, -0.2, 0.1, 0.3 )$
    \item $\eta_0 = 0.1,~\eta_T = 5e-4$
    \item Number of training games = $1000000$
    \item $\tau = 1.0$
\end{itemize}

%\maria{$\overline{Q}_*^{(2)}(a,s):=r(a,s)+\int_{\mathcal{S}}p(s,a,\mathrm{d}s')V_\bullet^{(i)}(s')$ if transitions are probabilistic, if deterministic we have $\overline{Q}_*^{(2)}(a,s):=r(a,s)+V_\bullet^{(i)}(s')$, where $V_\bullet^{(1)}(s')= \mathbb{E}_{\pi{_\bullet}}(r(s',a)) = \sum_{a\in \mathcal{A}} \pi_\bullet(a|s')r(s',a)= \sum_{a\in \mathcal{A}} \pi_\bullet(a|s')Q_*(s',a)$, similarly to how we compute $Q_*^{(2)}$.}

%The best we can do is the projection of $\overline{Q}_*^{(2)}(a,s)$ onto $\mathcal{H}_{\Theta^{(2)}}$.

%The atttained errors:}

\end{comment}
\end{enumerate}

\subsection{Control problems}
\label{appendix:control_problems}

For these problems, we use exponential decays for temperature $\tau$ and learning rate $\learningrate$, with prescribed terminal temperature $\tau_T$ and learning rate $\eta_T$. For example, for $\tau$ the decay rate is computed as $d_\tau = \left(\frac{\tau_T}{\tau_0}\right)^{1/ngames}$.

\paragraph{Frozen lake}
The FrozenLake benchmark \cite{opengym} is a $k\times k$ grid composed of cells, holes and one treasure. It features a discrete action space, namely, the agent can move in four directions (up, down, left, right). The episode terminates when the agent reaches the treasure or falls into a hole. We consider a $k=4$ for the numerical experiments. It is well-known that reshaping the reward function can change the performance of the algorithm. The original reward function does not discriminate between losing the game (falling into a hole), not moving and moving, so we will use a reshaped reward function: losing the game ($-1$), moving against a wall ($-0.1$), moving ($+0.01$) and reaching the treasure ($+10.0$). 

For $k=4$, the optimal number of steps is $6$. We train sets of $5$ agents on $1000$ episodes. Then, the trained agents play $100$ games. For the MPG, we define a terminal $\tau_T = 0.03$ and terminal learning rate $\eta_T = 3\times 10^{-6}$, vary the initial learning rates $\eta$, temperatures $\tau$ and horizon to see the impact of these on the success of the agents. Similarly, for the VPG, we  define a terminal $\tau_T = 0.03$ and terminal learning rate $\eta_T = 3\times 10^{-6}$. A summary of the results for horizons $n=10,15$ is given in table \ref{table:frozenlake-performance-parameters-4x4}, showing that the policies obtained are optimal or very close to optimal. The column \textit{Failed to train} denotes the policies that failed to converge (out of 5).

%For $k=8$, the optimal path contains $14$ steps. We vary the terminal temperature $\tau_T$, with the intuition that the stochasticity of the policy can deteriorate the performance of the agent over longer horizons. We also consider a longer horizon of $h = 100$. We train sets of 5 agents on $2000$ episodes. A summary is given in table \ref{table:frozenlake-performance-parameters-8x8}, showing that near optimal policies obtained are attainable with our algorithm. However, the convergence and training stability of our method depends heavily on training hyperparameters, such as  the terminal temperature $\tau_T$, initial temperature $\tau_0$ and learning rates $ \learningrate_0, \learningrate_T$.

\begin{table}[h!]
\begin{adjustbox}{width=1.0\textwidth,center}
\begin{tabular}{|l | c |c | c |} 
\hline
\multicolumn{4}{|c|}{\textbf{MPG}}\\
\hline
 $\tau_0,~ \eta_0$ & Success (\%) & Average steps & failed \\
 \hline
 \multicolumn{4}{|c|}{Horizon = 10}\\
 \hline
 $ 0.15,~ 0.001$ & 40.00 & 5.00 & 3\\
$ 0.15,~ 0.0005$ & 40.00 & 6.00 & 3\\
$ 0.15,~ 0.0001$ & 80.00 & 6.01 & 1\\
$ 0.15,~ 5\times 10^{-5}$ & 39.80 & 5.37 & 3\\
$ 0.2,~ 0.001$ & 20.00 & 6.00 & 4\\
$ 0.2,~ 0.0005$ & 20.00 & 4.50 & 4\\
$ 0.2,~ 0.0001$ & 40.00 & 5.00 & 3\\
$ 0.2,~ 5\times 10^{-5}$ & 60.00 & 6.50 & 1\\
$ 0.3,~ 0.001$ & 40.00 & 4.25 & 3\\
$ 0.3,~ 0.0005$ & 51.60 & 6.14 & 2\\
$ 0.3,~ 0.0001$ & 54.20 & 6.19 & 2\\
$ 0.3,~  5\times 10^{-5}$ & 0.00 & - & 5\\
$ 0.35,~ 0.001$ & 100.00 & 6.00 & 0\\
$ 0.35,~ 0.0005$ & 56.20 & 5.59 & 2\\
$ 0.35,~ 0.0001$ & 38.80 & 6.77 & 3\\
$ 0.35,~  5\times 10^{-5}$ & 38.80 & 5.35 & 3\\
$ 0.4,~ 0.001$ & 80.00 & 5.60 & 1\\
$ 0.4,~ 0.0005$ & 80.00 & 6.30 & 1\\
$ 0.4,~ 0.0001$ & 38.80 & 5.00 & 3\\
$ 0.4,~  5\times 10^{-5}$ & 39.80 & 5.54 & 3\\
 \hline
 \multicolumn{4}{|c|}{Horizon = 15}\\
 \hline
$ 0.15,~ 0.001$ & 40.00 & 4.19 & 3\\
$ 0.15,~ 0.0005$ & 60.00 & 5.25 & 2\\
$ 0.15,~ 0.0001$ & 60.00 & 5.25 & 2\\
$ 0.15,~  5\times 10^{-5}$ & 79.80 & 5.81 & 1\\
$ 0.2,~ 0.001$ & 60.00 & 4.60 & 2\\
$ 0.2,~ 0.0005$ & 75.00 & 5.25 & 1\\
$ 0.2,~ 0.0001$ & 98.60 & 6.38 & 0\\
$ 0.2,~  5\times 10^{-5}$ & 79.80 & 5.43 & 1\\
$ 0.3,~ 0.001$ & 60.00 & 4.92 & 2\\
$ 0.3,~ 0.0005$ & 80.00 & 5.45 & 1\\
$ 0.3,~ 0.0001$ & 60.00 & 5.66 & 2\\
$ 0.3,~  5\times 10^{-5}$ & 79.60 & 6.15 & 1\\
$ 0.35,~ 0.001$ & 77.00 & 6.12 & 0\\
$ 0.35,~ 0.0005$ & 100.00 & 6.00 & 0\\
$ 0.35,~ 0.0001$ & 79.40 & 5.55 & 1\\
$ 0.35,~ 5\times 10^{-5}$ & 59.80 & 5.35 & 2\\
$ 0.4,~ 0.001$ & 100.00 & 6.00 & 0\\
$ 0.4,~ 0.0005$ & 80.00 & 5.60 & 1\\
$ 0.4,~ 0.0001$ & 100.00 & 6.15 & 0\\
$ 0.4,~ 5\times 10^{-5}$ & 63.80 & 6.50 & 1\\
 [1ex] 
 \hline
\end{tabular}~\begin{tabular}{|c | c |c | c |} 
 \hline
\multicolumn{4}{|c|}{\textbf{VPG}}\\
\hline
 $\tau_0,~ \eta_0$ & Success (\%) & Average steps & failed \\
 \hline
 \multicolumn{4}{|c|}{Horizon = 10}\\
 \hline
 $ 0.15,~ 0.001$ & 19.80 & 6.31 & 4\\
$ 0.15,~ 0.0005$ & 20.00 & 6.00 & 4\\
$ 0.15,~ 0.0001$ & 20.00 & 6.01 & 4\\
$ 0.15,~  5\times 10^{-5}$ & 38.40 & 6.14 & 3\\
$ 0.2,~ 0.001$ & 0.00 & - & 5\\
$ 0.2,~ 0.0005$ & 20.00 & 6.00 & 4\\
$ 0.2,~ 0.0001$ & 0.00 & - & 5\\
$ 0.2,~  5\times 10^{-5}$ & 0.00 & - & 5\\
%$ 0.25,~ 0.001$ & 0.00 & nan & 5\\
%$ 0.25,~ 0.0005$ & 0.00 & nan & 5\\
%$ 0.25,~ 0.0001$ & 23.20 & 6.98 & 3\\
%$ 0.25,~ 5e-05$ & 19.60 & 7.18 & 3\\
$ 0.3,~ 0.001$ & 40.00 & 6.00 & 3\\
$ 0.3,~ 0.0005$ & 20.00 & 6.04 & 4\\
$ 0.3,~ 0.0001$ & 20.60 & 6.93 & 3\\
$ 0.3,~  5\times 10^{-5}$ & 7.60 & 7.00 & 4\\
$ 0.35,~ 0.001$ & 60.00 & 6.03 & 2\\
$ 0.35,~ 0.0005$ & 20.00 & 6.02 & 4\\
$ 0.35,~ 0.0001$ & 1.20 & 9.33 & 2\\
$ 0.35,~  5\times 10^{-5}$ & 11.20 & 6.16 & 4\\
$ 0.4,~ 0.001$ & 58.60 & 6.17 & 2\\
$ 0.4,~ 0.0005$ & 100.00 & 6.06 & 0\\
$ 0.4,~ 0.0001$ & 12.20 & 6.75 & 4\\
$ 0.4,~  5\times 10^{-5}$ & 0.20 & 9.00 & 4\\
 \hline
 \multicolumn{4}{|c|}{Horizon = 15}\\
 \hline
%$ 0.15,~ 0.001$ & 18.20 & 7.20 & 4\\
%$ 0.15,~ 0.0005$ & 20.00 & 6.00 & 4\\
%$ 0.15,~ 0.0001$ & 20.00 & 6.17 & 4\\
%$ 0.15,~ 5e-05$ & 19.80 & 6.29 & 4\\
$ 0.2,~ 0.001$ & 20.00 & 6.00 & 4\\
$ 0.2,~ 0.0005$ & 60.00 & 6.00 & 2\\
$ 0.2,~ 0.0001$ & 36.80 & 6.40 & 3\\
$ 0.2,~  5\times 10^{-5}$ & 37.00 & 6.50 & 3\\
$ 0.25,~ 0.001$ & 0.00 & - & 5\\
$ 0.25,~ 0.0005$ & 79.80 & 6.00 & 1\\
$ 0.25,~ 0.0001$ & 57.60 & 6.61 & 2\\
$ 0.25,~  5\times 10^{-5}$ & 36.40 & 7.01 & 3\\
$ 0.3,~ 0.001$ & 0.00 & - & 5\\
$ 0.3,~ 0.0005$ & 40.00 & 6.55 & 3\\
$ 0.3,~ 0.0001$ & 40.60 & 8.80 & 1\\
$ 0.3,~  5\times 10^{-5}$ & 18.20 & 10.35 & 3\\
$ 0.35,~ 0.001$ & 40.00 & 6.00 & 3\\
$ 0.35,~ 0.0005$ & 30.40 & 6.97 & 3\\
$ 0.35,~ 0.0001$ & 29.00 & 8.25 & 2\\
$ 0.35,~  5\times 10^{-5}$ & 41.20 & 8.71 & 2\\
$ 0.4,~ 0.001$ & 54.60 & 6.91 & 1\\
$ 0.4,~ 0.0005$ & 20.00 & 6.01 & 4\\
$ 0.4,~ 0.0001$ & 55.20 & 7.10 & 1\\
$ 0.4,~  5\times 10^{-5}$ & 15.40 & 7.90 & 4\\
 [1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Performance of trained agents on the Frozen Lake task on $4\times 4$ grid, for horizons $n = 10, 15$.}
\label{table:frozenlake-performance-parameters-4x4}
\end{table}

\begin{comment}
\begin{table}[h!]
\begin{adjustbox}{width=1.0\textwidth,center}
\begin{tabular}{|l | c |c | c |} 
\hline
\multicolumn{4}{|c|}{\textbf{MPG}}\\
\hline
 $\tau_0,~ \eta_0$ & Success (\%) & Average steps & failed \\
 \hline
 $ 0.15,~ 0.001$ & 40.00 & 5.00 & 3\\
$ 0.15,~ 0.0005$ & 40.00 & 6.00 & 3\\
$ 0.15,~ 0.0001$ & 80.00 & 6.01 & 1\\
$ 0.15,~ 5\times 10^{-5}$ & 39.80 & 5.37 & 3\\
$ 0.2,~ 0.001$ & 20.00 & 6.00 & 4\\
$ 0.2,~ 0.0005$ & 20.00 & 4.50 & 4\\
$ 0.2,~ 0.0001$ & 40.00 & 5.00 & 3\\
$ 0.2,~ 5\times 10^{-5}$ & 60.00 & 6.50 & 1\\
$ 0.3,~ 0.001$ & 40.00 & 4.25 & 3\\
$ 0.3,~ 0.0005$ & 51.60 & 6.14 & 2\\
$ 0.3,~ 0.0001$ & 54.20 & 6.19 & 2\\
$ 0.3,~  5\times 10^{-5}$ & 0.00 & - & 5\\
$ 0.35,~ 0.001$ & 100.00 & 6.00 & 0\\
$ 0.35,~ 0.0005$ & 56.20 & 5.59 & 2\\
$ 0.35,~ 0.0001$ & 38.80 & 6.77 & 3\\
$ 0.35,~  5\times 10^{-5}$ & 38.80 & 5.35 & 3\\
$ 0.4,~ 0.001$ & 80.00 & 5.60 & 1\\
$ 0.4,~ 0.0005$ & 80.00 & 6.30 & 1\\
$ 0.4,~ 0.0001$ & 38.80 & 5.00 & 3\\
$ 0.4,~  5\times 10^{-5}$ & 39.80 & 5.54 & 3\\
 [1ex] 
 \hline
\end{tabular}~\begin{tabular}{|c | c |c | c |} 
 \hline
\multicolumn{4}{|c|}{\textbf{VPG}}\\
\hline
 $\tau_0,~ \eta_0$ & Success (\%) & Average steps & failed \\
 \hline
 $ 0.15,~ 0.001$ & 19.80 & 6.31 & 4\\
$ 0.15,~ 0.0005$ & 20.00 & 6.00 & 4\\
$ 0.15,~ 0.0001$ & 20.00 & 6.01 & 4\\
$ 0.15,~  5\times 10^{-5}$ & 38.40 & 6.14 & 3\\
$ 0.2,~ 0.001$ & 0.00 & - & 5\\
$ 0.2,~ 0.0005$ & 20.00 & 6.00 & 4\\
$ 0.2,~ 0.0001$ & 0.00 & - & 5\\
$ 0.2,~  5\times 10^{-5}$ & 0.00 & - & 5\\
%$ 0.25,~ 0.001$ & 0.00 & nan & 5\\
%$ 0.25,~ 0.0005$ & 0.00 & nan & 5\\
%$ 0.25,~ 0.0001$ & 23.20 & 6.98 & 3\\
%$ 0.25,~ 5e-05$ & 19.60 & 7.18 & 3\\
$ 0.3,~ 0.001$ & 40.00 & 6.00 & 3\\
$ 0.3,~ 0.0005$ & 20.00 & 6.04 & 4\\
$ 0.3,~ 0.0001$ & 20.60 & 6.93 & 3\\
$ 0.3,~  5\times 10^{-5}$ & 7.60 & 7.00 & 4\\
$ 0.35,~ 0.001$ & 60.00 & 6.03 & 2\\
$ 0.35,~ 0.0005$ & 20.00 & 6.02 & 4\\
$ 0.35,~ 0.0001$ & 1.20 & 9.33 & 2\\
$ 0.35,~  5\times 10^{-5}$ & 11.20 & 6.16 & 4\\
$ 0.4,~ 0.001$ & 58.60 & 6.17 & 2\\
$ 0.4,~ 0.0005$ & 100.00 & 6.06 & 0\\
$ 0.4,~ 0.0001$ & 12.20 & 6.75 & 4\\
$ 0.4,~  5\times 10^{-5}$ & 0.20 & 9.00 & 4\\
 [1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Performance of trained agents on the Frozen Lake task on $8\times 8$ grid, for horizon $n = 64$.}
\label{table:frozenlake-performance-parameters-8x8}
\end{table}
\end{comment}

\subsection{Cart Pole}
The Cart Pole benchmark is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. The original reward function gives a $+1$ reward for each time that the pole stays upright, and the task finishes if the cart leaves the domain or if the pole is far enough from being upright. We reshape the reward function to give a penalty ($-10$) if the task is unsuccessful (i.e. the pole falls before reaching the target horizon). 

We set the terminal $\tau = 0.01$ and terminal learning rate $\eta = 5\times 10^{-8}$. We train sets of $5$ agents on $2000$ episodes. Then, we play $100$ games with the trained agents and record the performance of the policies,
 as shown on table \ref{table:polecart-performance-parameters}. 

\begin{comment}
\begin{table}[h!]
\centering
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{|c | c |c | c |} 
 \hline
 $\tau_0, ~\eta_0 $ & Success (\%) & Average steps & Failed to train \\
 \hline
$ 0.1, ~0.0001$ & 23.00 & 42.33 & 0 \\
$ 0.1, ~1e-05$ & 97.20 & 99.33 & 0 \\
$ 0.15, ~0.0001$ & 31.80 & 53.44 & 0 \\
$ 0.15, ~1e-05$ & 82.40 & 94.57 & 0 \\
$ 0.2, ~0.0001$ & 49.00 & 61.86 & 0 \\
$ 0.2, ~1e-05$ & 83.00 & 94.82 & 0 \\
 [1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{CartPole task performance after training, for horizon with length $100$.}
\label{table:polecart-performance-parameters}
\end{table}
\end{comment}

\begin{table}[h!]
\begin{adjustbox}{width=1.0\textwidth,center}
\begin{tabular}{|l | c |c | c |} 
\hline
\multicolumn{4}{|c|}{\textbf{MPG}}\\
\hline
 $\tau_0,~ \eta_0$ & Success (\%) & Average steps & failed \\
 \hline
$0.1, ~0.0001$ & 35.80 & 65.62 & 0 \\
$0.1, ~1\times 10^{-5}$ & 95.40 & 98.93 & 0 \\
$0.15, ~0.0001$ & 77.40 & 91.65 & 0 \\
$0.15, ~1\times 10^{-5}$ & 97.40 & 99.76 & 0 \\
$0.2, ~0.0001$ & 86.20 & 97.66 & 0 \\
$0.2, ~1\times 10^{-5}$ & 72.40 & 89.13 & 0 \\
 [1ex] 
 \hline
\end{tabular}~\begin{tabular}{|c | c |c | c |} 
 \hline
\multicolumn{4}{|c|}{\textbf{VPG}}\\
\hline
 $\tau_0,~ \eta_0$ & Success (\%) & Average steps & failed \\
 \hline
$ 0.1, ~0.0001$ & 23.00 & 42.33 & 0 \\
$ 0.1, ~1\times 10^{-5}$ & 97.20 & 99.33 & 0 \\
$ 0.15, ~0.0001$ & 31.80 & 53.44 & 0 \\
$ 0.15, ~1\times 10^{-5}$ & 82.40 & 94.57 & 0 \\
$ 0.2, ~0.0001$ & 49.00 & 61.86 & 0 \\
$ 0.2, ~1\times 10^{-5}$ & 83.00 & 94.82 & 0 \\
 [1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Performance of trained agents on the CartPole task, for horizon $n = 100$.}
\label{table:polecart-performance-parameters}
\end{table}

In practice, in order to achieve a more robust implementation, one could consider various stabilisation techniques such as gradient clipping to avoid updates which are too large or usage of off-policy updates, for example, through the use of a replay buffer. Other improvements to the training strategy can be adopted, such as: adaptive techniques to choose $\tau$ and $\learningrate$ during training can be considered, beyond the chosen continuous decay, batched trajectory updates, etc.


\begin{comment}
\section{Multiple updates per path}
\label{appendix: multiple updates}

In Theorem \ref{thm: general case global optimality}, global optimality of $\pi^{(i)}$ for all $1\leq i\leq n$ is shown using an inductive argument on $i$.
It is therefore desirable to train the $1$-step policy faster than the $2$-step policy, itself faster than the $3$-step policy, and so on.
The following observation allows one to maximize the use of one path for training:
since a path for one MPG update is of length $n$, the $1$-step policy can be trained $n$ times, the $2$-step policy $n-1$ times, and so on until the $n$-step policy that can be updated once.

Let $\rho_{i,k}:=\prod_{\ell=0}^{i-1}\frac{\pi^{(i-\ell)}_t}{\pi^{(n-k-\ell)}_t}(A_{k+\ell}|S_{k+\ell})$.
We propose the following variation of MPG:
\begin{align*}
    \theta_{t+1}^{(1)}
    &=\theta_t^{(1)} + \learningrate\sum_{k=0}^{n-1} \rho_{1,k} \left(R_k-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(A_k|S_k)\right)
    \nabla\log\pi_t^{(1)}(A_k|S_k),
\end{align*}
and more generally for $i=2..n$,
\begin{align*}
    \theta_{t+1}^{(i)}
    &=\theta_t^{(i)} + \learningrate\sum_{k=0}^{n-i}\rho_{i,k}\sum_{\ell=0}^{i-1}\left(R_{\ell+k}-\tau\log\frac{\pi_t^{(i-\ell)}}{\overline{\pi}}(A_{k+\ell}|S_{k+\ell})\right)
    \nabla\log\pi_t^{(i)}(A_k|S_k)\nonumber\\
    &=\theta_t^{(i)} + \learningrate\sum_{k=0}^{n-i}C_{k,i}\nabla\log\pi_t^{(i)}(A_k|S_k),
\end{align*}
The scaling factors $\rho_{i,k}$ are used so that in expectation, the update of $\pi^{(i)}_t$ is done with actions sampled from itself.
However, the denominator in $\rho_{i,k}$ can blow up and lead to training instability without a cut-off.
 

%\francois{
%Implement this again with the code that works.

%Actually, the theorem holds also for this algorithm.
%Up to redefining the objective, I can prove a policy gradient theorem ensuring convergence.
%The proof of the uniqueness of the critical point is identical.
%In particular, provided that that state-distribution has full support, we get global optimality with the same optimal policy.

%Make it rigorous.

%Rename the section and give it more importance in the main.
%}

\end{comment}


\begin{comment}
The factor $\rho_{i-\ell,k+\ell}$ allows to maximize the use of samples in a path: it rescales the $(\ell+k)$-th reward so that the $(i-\ell)$-th policy is updated, in average, with samples from itself.
For instance, $\pi_t^{(1)}$ is updated such that
\begin{align*}
    \EE\left[\theta_{t+1}^{(1)}-\theta_t^{(1)}|\mathcal{F}_t\right]
    &=\learningrate^{(1)}\EE_{\pi_t}\left[\sum_{k=0}^{n-1} \rho_{1,k}\left(R_k-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(A_k|S_k)\right)\nabla\log\pi_t^{(1)}(A_k|S_k)\right]\\
    &=\learningrate^{(1)}\sum_{k=0}^{n-1}\EE_{\pi_t^{(1)}}\left[\left(R_k'-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(A_k'|S_k')\right)\nabla\log\pi_t^{(1)}(A_k'|S_k')\right],
\end{align*}
where $A_k'$ has conditional law $\pi_t^{(1)}(\cdot|S_k')$ and $S_k'$ has the law of the $(k+1)$-st state of a path where actions are sampled according to $\pi^{(1)}$.
\francois{
State that global optimality still holds, only convergence can't be shown (at least not straightforwardly with same proof).
Same issue with different learning rates
}
\end{comment}
\clearpage
\bibliographystyle{plain}
\bibliography{biblio}

\end{document}