@misc{opengym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@inproceedings{Nachum17,
author = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale}, title = {Bridging the Gap between Value and Policy Based Reinforcement Learning}, year = {2017}, isbn = {9781510860964}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.2}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {2772–2782}, numpages = {11}, location = {Long Beach, California, USA}, series = {NIPS'17} 
}

@inproceedings{Ernst03,
author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis}, title = {Iteratively Extending Time Horizon Reinforcement Learning}, year = {2003}, isbn = {3540201211}, publisher = {Springer-Verlag}, address = {Berlin, Heidelberg}, url = {https://doi.org/10.1007/978-3-540-39857-8_11}, doi = {10.1007/978-3-540-39857-8_11}, abstract = {Reinforcement learning aims to determine an (infinite time horizon) optimal control policy from interaction with a system. It can be solved by approximating the so-called Q-function from a sample of four-tuples (xt, ut, rt, xt+1) where xt denotes the system state at time t, ut the control action taken, rt the instantaneous reward obtained and xt+1 the successor state of the system, and by determining the optimal control from the Q-function. Classical reinforcement learning algorithms use an ad hoc version of stochastic approximation which iterates over the Q-function approximations on a four-tuple by four-tuple basis. In this paper, we reformulate this problem as a sequence of batch mode supervised learning problems which in the limit converges to (an approximation of) the Q-function. Each step of this algorithm uses the full sample of four-tuples gathered from interaction with the system and extends by one step the horizon of the optimality criterion. An advantage of this approach is to allow the use of standard batch mode supervised learning algorithms, instead of the incremental versions used up to now. In addition to a theoretical justification the paper provides empirical tests in the context of the "Car on the Hill" control problem based on the use of ensembles of regression trees. The resulting algorithm is in principle able to handle efficiently large scale reinforcement learning problems.}, booktitle = {Proceedings of the 14th European Conference on Machine Learning}, pages = {96–107}, numpages = {12}, location = {Cavtat-Dubrovnik, Croatia}, series = {ECML'03}
}

@article{Agarwal22,
author = {Agarwal, Alekh and Kakade, Sham M. and Lee, Jason D. and Mahajan, Gaurav}, title = {On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift}, year = {2022}, issue_date = {January 2021}, publisher = {JMLR.org}, volume = {22}, number = {1}, issn = {1532-4435}, abstract = {Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution or how they cope with approximation error due to using a restricted class of parametric policies. This work provides provable characterizations of the computational, approximation, and sample size properties of policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: "tabular" policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy; and parametric policy classes (considering both log-linear and neural policy classes), which may not contain the optimal policy and where we provide agnostic learning results. One central contribution of this work is in providing approximation guarantees that are average case--which avoid explicit worst-case dependencies on the size of state space--by making a formal connection to supervised learning under distribution shift. This characterization shows an important interplay between estimation error, approximation error, and exploration (as characterized through a precisely defined condition number).}, journal = {J. Mach. Learn. Res.}, month = {jul}, articleno = {98}, numpages = {76}, keywords = {reinforcement learning, policy gradient}
}

@article{Wang19,
  title={Neural Policy Gradient Methods: Global Optimality and Rates of Convergence},
  author={Lingxiao Wang and Qi Cai and Zhuoran Yang and Zhaoran Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.01150}
}

@inproceedings{Mei20,
  title={On the Global Convergence Rates of Softmax Policy Gradient Methods},
  author={Jincheng Mei and Chenjun Xiao and Csaba Szepesvari and Dale Schuurmans},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@article{Schulman17,
  title={Equivalence Between Policy Gradients and Soft Q-Learning},
  author={John Schulman and P. Abbeel and Xi Chen},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.06440}
}

@inproceedings{Haarnoja18SoftAO,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Tuomas Haarnoja and Aurick Zhou and P. Abbeel and Sergey Levine},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@article{Haarnoja18SoftAA,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and G. Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and P. Abbeel and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.05905}
}

@article{Li22,
  title={Understanding the Complexity Gains of Single-Task RL with a Curriculum},
  author={Qiyang Li and Yuexiang Zhai and Yi Ma and Sergey Levine},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.12809}
}

@article{Schulman15,
  title={Trust Region Policy Optimization},
  author={John Schulman and Sergey Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
  journal={ArXiv},
  year={2015},
  volume={abs/1502.05477}
}

@article{Schulman17PPO,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.06347}
}

@inproceedings{Fazel18,
  title={Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator},
  author={Maryam Fazel and Rong Ge and Sham M. Kakade and Mehran Mesbahi},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@article{Bhandari19,
  title={Global Optimality Guarantees For Policy Gradient Methods},
  author={Jalaj Bhandari and Daniel Russo},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.01786}
}

@inproceedings{Zhang20SampleER,
  title={Sample Efficient Reinforcement Learning with REINFORCE},
  author={Junzi Zhang and Jongho Kim and Brendan O'Donoghue and Stephen Boyd},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2020}
}

@article{Zhang19GlobalCO,
  title={Global Convergence of Policy Gradient Methods to (Almost) Locally Optimal Policies},
  author={K. Zhang and Alec Koppel and Haoqi Zhu and Tamer Başar},
  journal={SIAM J. Control. Optim.},
  year={2019},
  volume={58},
  pages={3586-3612}
}

@article{Li21,
  title={Softmax Policy Gradient Methods Can Take Exponential Time to Converge},
  author={Gen Li and Yuting Wei and Yuejie Chi and Yuantao Gu and Yuxin Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.11270}
}

@article{Cen20,
  title={Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization},
  author={Shicong Cen and Chen Cheng and Yuxin Chen and Yuting Wei and Yuejie Chi},
  journal={Oper. Res.},
  year={2020},
  volume={70},
  pages={2563-2578}
}

@article{Ding21,
  title={Beyond Exact Gradients: Convergence of Stochastic Soft-Max Policy Gradient Methods with Entropy Regularization},
  author={Yuhao Ding and Junzi Zhang and Javad Lavaei},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.10117}
}

@inproceedings{Yuan21,
  title={A general sample complexity analysis of vanilla policy gradient},
  author={Rui Yuan and Robert Mansel Gower and Alessandro Lazaric},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021}
}

@inproceedings{Haarnoja17,
  title={Reinforcement Learning with Deep Energy-Based Policies},
  author={Tuomas Haarnoja and Haoran Tang and P. Abbeel and Sergey Levine},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@article{Eysenbach21,
  title={Maximum Entropy RL (Provably) Solves Some Robust RL Problems},
  author={Benjamin Eysenbach and Sergey Levine},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.06257}
}

@article{Eysenbach19,
  title={If MaxEnt RL is the Answer, What is the Question?},
  author={Benjamin Eysenbach and Sergey Levine},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01913}
}

@book{Sutton18,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@article{Levine18,
  title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
  author={Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.00909}
}

@article{Vito13,
  title={An extension of Mercer theorem to matrix-valued measurable kernels},
  author={Ernesto de Vito and Veronica Umanit{\`a} and Silvia Villa},
  journal={Applied and Computational Harmonic Analysis},
  year={2013},
  volume={34},
  pages={339-351}
}

@article{Agazzi20,
  title={Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime},
  author={Andr{\'e}a Agazzi and Jianfeng Lu},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11858}
}

@article{Laslier13,
  title={Reinforcement learning from comparisons: Three alternatives is enough, two is not},
  author={Beno{\^i}t Laslier and Jean-François Laslier},
  journal={ArXiv},
  year={2013},
  volume={abs/1301.5734}
}

@inproceedings{Sutton99,
  title={Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  author={Richard S. Sutton and David A. McAllester and Satinder Singh and Y. Mansour},
  booktitle={NIPS},
  year={1999}
}

@article{Williams92,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Ronald J. Williams},
  journal={Machine Learning},
  year={1992},
  volume={8},
  pages={229-256}
}

@article{Lillicrap15,
  title={Continuous control with deep reinforcement learning},
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  journal={CoRR},
  year={2015},
  volume={abs/1509.02971}
}

@article{Brown20,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}



@article{weng18,
  title   = "Policy Gradient Algorithms",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-04-08-policy-gradient/"
}

@article{ODonoghue16,
  title={PGQ: Combining policy gradient and Q-learning},
  author={Brendan O'Donoghue and R{\'e}mi Munos and Koray Kavukcuoglu and Volodymyr Mnih},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.01626}
}

@article{Narvekar20,
  title={Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
  author={Sanmit Narvekar and Bei Peng and Matteo Leonetti and Jivko Sinapov and Matthew E. Taylor and Peter Stone},
  journal={ArXiv},
  year={2020},
  volume={abs/2003.04960}
}

@article{Guin22,
  title={A policy gradient approach for Finite Horizon Constrained Markov Decision Processes},
  author={Soumyajit Guin and Shalabh Bhatnagar},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.04527}
}

@article{Asis19,
  title={Fixed-Horizon Temporal Difference Methods for Stable Reinforcement Learning},
  author={Kristopher De Asis and Alan Chan and Silviu Pitis and Richard S. Sutton and Daniel Graves},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.03906}
}

@article{Jacot20ImplicitRO,
  title={Implicit Regularization of Random Feature Models},
  author={Arthur Jacot and Berfin Simsek and Francesco Spadaro and Cl{\'e}ment Hongler and Franck Gabriel},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.08404}
}

@inproceedings{Scherrer12,
  title={On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes},
  author={Bruno Scherrer and Boris Lesner},
  booktitle={NIPS},
  year={2012}
}

@inproceedings{Bertsekas1995DynamicPA,
  title={Dynamic Programming and Optimal Control},
  author={Dimitri P. Bertsekas},
  year={1995}
}

@inproceedings{Jacot18,
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl\'{e}ment},
title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {8580–8589},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{Chizat18,
  title={A Note on Lazy Training in Supervised Differentiable Programming},
  author={L{\'e}na{\"i}c Chizat and Francis R. Bach},
  journal={ArXiv},
  year={2018},
  volume={abs/1812.07956}
}

@inproceedings{Chizat18mf,
  title={On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
  author={L{\'e}na{\"i}c Chizat and Francis R. Bach},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@article{Yang20,
  title={Feature Learning in Infinite-Width Neural Networks},
  author={Greg Yang and Edward J. Hu},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.14522}
}

@inproceedings{Bhandari20OnTL,
  title={On the Linear Convergence of Policy Gradient Methods for Finite MDPs},
  author={Jalaj Bhandari and Daniel Russo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2020}
}

@inproceedings{Hernandez99,
  title={Discrete-time Markov control processes},
  author={On{\'e}simo Hern{\'a}ndez-Lerma and Jean Bernard Lasserre},
  year={1999}
}

@inproceedings{Seijen19,
  title={Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning},
  author={Harm van Seijen and Mehdi Fatemi and Arash Tavakoli},
  booktitle={Neural Information Processing Systems},
  year={2019}
}


@InProceedings{Leahy22,
  title = 	 {Convergence of Policy Gradient for Entropy Regularized {MDP}s with Neural Network Approximation in the Mean-Field Regime},
  author =       {Leahy, James-Michael and Kerimkulov, Bekzhan and Siska, David and Szpruch, Lukasz},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {12222--12252},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/leahy22a/leahy22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/leahy22a.html},
}


@inproceedings{atanasov22,
title={Neural Networks as Kernel Learners: The Silent Alignment Effect},
author={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=1NvflqAdoom}
}

@inproceedings{minh06,
  title={Mercer’s theorem, feature maps, and smoothing},
  author={Minh, Ha Quang and Niyogi, Partha and Yao, Yuan},
  booktitle={Learning Theory: 19th Annual Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006. Proceedings 19},
  pages={154--168},
  year={2006},
  organization={Springer}
}

@inproceedings{fukumizu05,
  title={Infinite dimensional exponential families by reproducing kernel Hilbert spaces},
  author={Fukumizu, KENJI},
  booktitle={2nd International Symposium on Information Geometry and its Applications (IGAIA 2005)},
  pages={324--333},
  year={2005}
}

@book{berge63,
  title={Topological spaces: Including a treatment of multi-valued functions, vector spaces and convexity},
  author={Berge, Claude},
  year={1963},
  publisher={Oliver \& Boyd}
}

@book{amari16,
  title={Information geometry and its applications},
  author={Amari, Shun-ichi},
  volume={194},
  year={2016},
  publisher={Springer}
}