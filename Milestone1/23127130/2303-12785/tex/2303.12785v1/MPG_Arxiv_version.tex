\documentclass[11pt]{article}
 
 \usepackage{amsthm}
 \usepackage{enumitem}
 \usepackage{hyperref}
 \usepackage{amssymb}
 \usepackage{amsmath}
 \usepackage{dsfont}
 \usepackage{mathtools}
 \usepackage{xcolor}
 \usepackage{mathrsfs}

 \usepackage{comment}
 
\usepackage{authblk}


\usepackage[titlenumbered,ruled]{algorithm2e}
\usepackage{adjustbox}
 
 \usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
  
 \renewcommand{\qedsymbol}{$\blacksquare$}  
  
 \newtheorem{thm}{Theorem}
 \newtheorem{prop}{Proposition}
 \newtheorem{lem}{Lemma}
 \newtheorem{de}{Definition}
 \newtheorem{rem}{Remark}
 \newtheorem{cor}{Corollary}


\title{Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality}
\author[1]{Fran\c{c}ois G.  Ged}
\author[2,3]{Maria Han Veiga}
\affil[1]{Chair of Statistical Field Theory, \'{E}cole Polytechnique F\'{e}d\'{e}rale de Lausanne, Lausanne, Switzerland}
\affil[2]{Department of Mathematics, University of Michigan, USA}
\affil[3]{Michigan Institute for Data Science, University of Michigan, USA}

\date{}
\newcommand{\intervalleff}[2]{\mathopen{[}#1\,,#2\mathclose{]}}
\newcommand{\intervallefo}[2]{\mathopen{[}#1\,,#2\mathclose{)}}
\newcommand{\intervalleof}[2]{\mathopen{(}#1\,,#2\mathclose{]}}
\newcommand{\intervalleoo}[2]{\mathopen{(}#1\,,#2\mathclose{)}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\newcommand{\integraloperator}[2]{I_{#1}^{(#2)}}

\newcommand{\learningrate}{\eta}

\newcommand{\Id}{\mathrm{Id}}

\newcommand{\francois}[1]{{\color{violet}Francois: #1}}


\newcommand{\maria}[1]{{\color{blue}Maria: #1}}

\newcommand{\getrid}[1]{{\color{red}Erase #1}}

\newcommand{\iprod}[2]{\left\langle #1,#2\right\rangle}

\newcommand{\DKL}[2]{D_{\mathrm{KL}}(#1||#2)}

\newcommand{\nparam}{P}

\newcommand{\argmax}[1]{\underset{#1}{\mathrm{argmax}\ }}
\newcommand{\argmin}[1]{\underset{#1}{\mathrm{argmin}\ }}

\newcommand{\threepartdef}[6]{
    \left\{
    \begin{array}{ll}
        #1, & \text{if}\ #2, \\
        #3, & \text{if}\ #4,\\
        #5, & \text{if}\ #6.
    \end{array}
    \right.
    }
\newcommand{\twopartdef}[3]{
    \left\{
    \begin{array}{ll}
        #1, & \text{if}\ #2, \\
        #3, & \text{otherwise}.
    \end{array}
    \right.
    }

\newcommand{\colvectwo}[2]
    {
    \left(\begin{matrix}
        #1 \\ #2
    \end{matrix}\right)
    }

\newcommand{\colvecthree}[3]
    {
    \left(\begin{matrix}
        #1 \\ #2 \\ #3
    \end{matrix}\right)
    }

\newcommand{\colvecn}[2]
    {
    \left(\begin{matrix}
        #1 \\ \vdots \\ #2
    \end{matrix}\right)
    }

\begin{document}
\maketitle


\begin{abstract}
    A novel Policy Gradient (PG) algorithm, called \textit{Matryoshka Policy Gradient} (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximizing entropy bonuses additional to its cumulative rewards.
    MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective.
    For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space.
    MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework.
    Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simple criterion to verify the global optimality of the policy at convergence.
    As a proof of concept, we evaluate numerically MPG on standard test benchmarks.
\end{abstract}

\section{Introduction}

\subsection{Policy gradient and max-entropy reinforcement learning}

\paragraph{Reinforcement Learning}
(RL) tasks can be informally summarized as follows: sequentially, an agent is located at a given state $s$, takes an action $a$, receives a reward $r(a,s)$ and moves to a next state $s'\sim p(s,a,\cdot)$, where $p$ is a transition probability kernel.
The agent thus seeks to maximize the cumulative rewards from its interactions with the environment, that is, it optimizes its \textit{policy} $\pi$ by reinforcing decisions that led to high rewards, where $\pi(a|s)$ is the probability for the agent to take action $a$ while at state $s$.

\paragraph{Policy Gradient} (PG) methods are model-free algorithms that aim at solving such RL tasks;
\textit{model-free} refers to the fact that the agent tries to learn (i.e. improve its policy's performance) without learning the dynamics of the environment governed by $p$, nor the reward function $r$.
Though their origins in RL can be dated from several decades ago with the algorithm REINFORCE \cite{Williams92}, the name \textit{Policy Gradient} appearing only in 2000 in \cite{Sutton99}, they recently regained interest thanks to many remarkable achievements, to name a few: in continuous control \cite{Lillicrap15, Schulman15, Schulman17PPO} and
natural language processing with GPT-3 \cite{Brown20}\footnote{instructGPT and chatGPT are trained with Proximal Policy Optimization, see \url{https://openai.com/blog/chatgpt/}.}.
See the blog post \cite{weng18} that lists important PG methods and provides a concise introduction to each of them.


\paragraph{Max-entropy RL.}
More generally, PG methods are considered more suitable for large (possibly continuous) state and action spaces than other nonetheless important methods such as Q-learning and its variations.
However, for large spaces, the exploitation-exploration dilemma becomes more challenging.
In order to enhance exploration, it has become standard to use a regularization to the objective, as in \textit{max-entropy RL}\cite{Nachum17,ODonoghue16,Schulman17}, where the agent maximizes the sum of its rewards plus a bonus for the entropy of its policy\footnote{Other regularization techniques are used and studied in the literature, we focus on entropy regularized RL in this paper.}.
Not only does max-entropy RL boost exploration, it also yields an optimal policy that is stochastic, in the form of a Boltzmann measure, such that the agent keeps taking actions at random while maximizing the regularized objective.
This is sometimes preferable than deterministic policies.
In particular, \cite{Eysenbach21} shows that the max-entropy RL optimal policy is robust to adversarial change of the reward function (their Theorem 4.1) and transition probabilities (their Theorem 4.2); see also references therein for more details on that topic.
Finally, max-entropy RL is appealing from a theoretical perspective.
For example, soft Q-learning, introduced in \cite{Haarnoja17} (see also \cite{Haarnoja18SoftAA,Haarnoja18SoftAO} for implementations of soft Q-learning with an actor-critic scheme), strongly resembles PG in max-entropy RL \cite{Schulman17}; max-entropy RL has also been linked to variational inference in \cite{Levine18}.

Other appealing features of max-entropy RL are discussed in \cite{Eysenbach19} and references therein.


\paragraph{Convergence guarantees of PG.}
The empirical successes motivated the RL community to attempt building the solid theory for PG methods that has been lacking.
Indeed, besides the well-known \textit{Policy Gradient Theorem} (see Chapter 13 in \cite{Sutton18}) that can imply convergence of PG (provided good learning rate and other assumptions), for many years, not much more was known about the global convergence of PG (i.e. convergence to an optimal policy) until recently.
Despite the numerous gaps that remain, some important progress have already been made.
In particular, the global convergence of PG methods has been studied and proved in specific settings, see for instance \cite{Fazel18,Agarwal22,Bhandari19,Mei20,Zhang20SampleER,Zhang19GlobalCO,Cen20,Ding21,Wang19, Agazzi20, Bhandari20OnTL}.
Convergence guarantees often come with convergence rates (with or without perfect gradient estimates).
Though strengthening the trust in PG methods for practical tasks, most of the theoretical guarantees that have been obtained in the literature so far require rather restrictive assumptions, and often assume that the action-state space of the MDP is finite (but not always, e.g. \cite{Agazzi20} addresses continuous action-state space for neural policies in the mean-field regime).
In particular, \cite{Li21} shows that many convergence rates that have been obtained in the literature ignore some parameters such as the size of the state space.
After making explicit the dependency of the bounds on these parameters, they manage to construct environments where the rates blow up and convergence takes super-exponential time.



\begin{comment}
\paragraph{An intrinsic issue with RL.}

One notable difficulty that PG (but not only PG) faces is that the policy is updated by using that policy's own performance.
Suppose that at a state $s_0$, the agent can go either up or down.
Suppose that in the region downwards, the agent has bad performances, and that in the region upwards, the agent performs close to optimally.
Then, after few ups and downs from $s_0$, the agent will reinforce the action ``up'' because it consequently received better rewards.
However, this learning signal may be leading the agent away from an optimal policy, possibly forever (except in the specific settings of the works cited above).
In the schematic figure \ref{fig: bad example}, the agent at $s_0$ will learn to go up, even though it thus gets away from a great reward (represented by the diamond, close to the region where the agent performs very poorly).
On the other hand, if the agent has a perfect evaluation of all other states than $s_0$, then the learning signal at $s_0$ is perfectly aligned towards the optimal policy.
\begin{figure}
    \centering
    \includegraphics[width = 0.8\textwidth]{ExampleBadRL.png}
    \caption{Misleading signal received at $s_0$.}
    \label{fig: bad example}
\end{figure}
\end{comment}



\subsection{Contributions}

We consider parametric policies constructed as the softmax of linear models.
The main contributions of this work are:
\begin{enumerate}[label = (\roman*)]
    \item We reshape the max-RL objective function and define a new algorithm (Equation \eqref{eq: update cascade learning}), named \textit{Matryoshka Policy Gradient} (MPG).
    \item After showing a policy gradient theorem (Theorem \ref{thm: policy gradient theorem}) that ensures convergence of MPG, we establish global convergence under few mild assumptions (Theorem \ref{thm: general case global optimality}); in particular, we obtain global convergence for continuous compact state space.
    \item The optimal policy for the standard objective can be approximated arbitrarily well by the optimal policy of the MPG objective (Proposition \ref{prop: extending horizon converges to standard optimal policy}).
    \item In the case where the policy is parametrized as the softmax of a (deep) neural network's output, we provide a simple criterion on the so-called \textit{Neural Tangent Kernel} of the neural network \underline{at the end of training}, that is sufficient to guarantee global optimality of the limit (Corollary \ref{thm: gen case deep RL global optimality}).
    \item Numerically, we successfully train agents on standard simple tasks without relying on RL tricks (see Section \ref{Section: numerical experiments}). 
\end{enumerate}
The techniques employed in the above cited papers to study global convergence often come from non-convex optimization; for example, if one shows that the so-called gradient domination property is satisfied, one can then show global convergence and deduce the rate of convergence, see e.g. Section 4 in \cite{Agarwal22}.
Though MPG is very intuitive, the MPG objective is still non-convex.
Nevertheless, we show global convergence of MPG without relying on standard techniques: for softmax parametrizations, \textbf{the only critical point of the MPG objective is the optimal policy}.
This makes the proof very direct and adapted to continuous compact state space, contributing to the theoretical attractiveness of MPG.

On a more practical point of view, besides contribution (iv), it turns out that MPG is well suited for policies parametrized by a neural network.
Indeed, MPG relies on the fact that the agent's policy learns to optimize the cumulative rewards for several horizons.
One assumption to ensure global convergence of MPG is that the policies for different horizons do not share any parameters.
However, it is intuitive that optimizing for the next 50 rewards or optimizing for the next 51 rewards should yield similar policies (for smooth enough, relevant environments).
Hence, our heuristic is that plugging the horizon as an input in a single neural network encoding all different horizons' policies should \underline{enhance} rather than \underline{harm} training.
More details on neural policies in Appendix \ref{appendix: neural networks}

In our numerical experiments described in Section \ref{Section: numerical experiments}, we consider standard benchmarks from OpenAI. Namely, the Frozen-Lake game and Pendulum.
We obtain successful policies for both benchmarks with a very simple implementation of the MPG algorithm.
Rather than competing with the state-of-the-art algorithms, our aim is to provide a proof of concept by showing that successful training can be obtained without using standard RL tricks that are known to improve training performance.
Standard tricks are nevertheless very straightforward to apply to MPG and we hope that more general and bigger scale experiments implementing variations of MPG will follow the present work.


\paragraph{Works related to nested policies.}
The main idea of MPG is to use a sequence of policies (that we call extended policy) $\pi^{(1)},\ldots,\pi^{(n)}$ to choose a sequence of actions.
Policy $\pi^{(i)}$ is optimized to maximize the next $i$ regularized rewards and is  thus called the $i$\textit{-step policy}.
This makes the analysis of MPG easier, allowing us to establish global convergence in a ``cascade'' manner, from small to large horizons.
Note that having different policies for different step is standard in dynamic programming \cite{Bertsekas1995DynamicPA}, where they are called \textit{non-stationary policies}.

In the standard finite horizon RL objective – also called the \textit{episodic case} – the length of an episode is a finite random variable.
What distinguishes the MPG framework (for which an episode's length is fixed a priori) with the episodic case is that with MPG, we train an extended policy to learn all finite horizon objectives up to a maximal horizon.
The idea to look at such finite horizons and let the maximal horizon grow to infinity was already used two decades ago in \cite{Ernst03}.
The focus of that paper is to exploit that the optimal policies of the finite horizon objectives converge, as the maximal horizon tend to infinity, to the infinite horizon objective; they do not propose (and therefore study) an algorithm that trains an extended policy.

Closer to the idea behind MPG, the construction of the value function in \cite{Asis19} resembles that of the nested structure of extended policies, where a sequence of value and Q-functions are trained to solve finite horizon tasks.
Training is done with a \textit{Temporal Difference} (TD) algorithm and thus differs from MPG, which is a PG method.
TD involves bootstrapping, and when it is used offline (off-policy) together with function approximation, it encounters the well-known stability issue called the \textit{deadly triad}, see \cite{Sutton18} Section 11.3.
By using different value functions for different horizons, they do not rely on bootstrapping, getting rid of one element of the triad, thus ensuring more stability.

Even more recently, the authors of the preprint \cite{Guin22} exploited a similar idea with an actor-critic method for constrained RL, where the agent aims at maximizing the cumulative rewards while satisfying some given constraints.
They investigate convergence rates in both constrained and unconstrained settings.
To guarantee convergence, they assume a condition given by their Equation (20).
Roughly speaking, this condition is that smaller horizon policies are closer to convergence than larger horizon policies.
Therefore, they prove convergence of the training algorithm through a cascade of convergence.
The MPG setting that we introduce differs from their work in that the MPG objective is not constrained and includes an entropy regularization.
In particular, specific properties of entropy regularization yields that we do not need to assume that smaller horizon policies are closer to convergence than larger horizon policies during training.
Not only is convergence guaranteed for MPG, but the global optimality is also established under appropriate assumptions; it is interesting to note that we also make use of a ``cascading'' argument to ensure global optimality.
Note however that, unlike \cite{Guin22}, we do not provide convergence rate (though standard techniques should apply and is left for future work).




\begin{comment}
\paragraph{Resemblance with more distant ideas.}



The subfield of RL based on the idea to solve a sequence (or more generally, an acyclic graph where each path from the root to a leaf corresponds to a sequence) of related tasks with increasing complexity, in order to make learning easier and achieve a difficult task is called \textit{Curriculum Learning} for RL; see \cite{Narvekar20} for an overview of curriculum learning in RL.
The proof of global optimality for MPG uses that if each $i$-step policy, $i=1,\ldots,n-1$ has converged to the respective optimal $i$-step policy, then the $n$-step policy is in turn guaranteed to converge to the optimal $n$-step policy.
Although the motivations seem roughly similar, MPG cannot be directly cast into the curriculum framework.
\end{comment}

\section{Single state case}

In the single state case (also known as the \textit{bandit problem}), MPG is identical to the usual PG for the max-entropy objective.
Nevertheless, we study this case first as it helps understanding the general state space case.

\subsection{Definitions}

Consider the the case when the state space is restricted to a single state $\mathcal{S}=\{s_0\}$.
We suppose moreover that the action space $\mathcal{A}$ is finite – the argument for $\mathcal{A}$ a continuous compact real subset requires some technical adaptation that we believe are unnecessary for our presentation.
In the bandit problem, at each discrete time step, an agent takes an action $a\in\mathcal{A}$ and receives a random reward $R(a)$ with law $p_{\mathrm{rew}}(\cdot|a)$ on $\RR$.
We assume that rewards are bounded, and denote the expected reward map $r(a):=\EE[R(a)]$.
The agent chooses actions according to a \textit{policy}, denoted by $\pi$, which is a probability distribution on $\mathcal{A}$.
If $\pi\neq\delta_{a}$ for all $a\in\mathcal{A}$, we say that $\pi$ is a \textit{stochastic policy} and at a given time, we denote by $A$ the random action the agent selects.
The objective function $J$ to maximize maps policies $\pi$ to
\begin{align*}
	J(\pi):=\mathbb{E}_{\pi}\left[r(A)\right]-\tau\DKL{\pi}{\overline{\pi}},
\end{align*}
where $\DKL{p}{q}:=\sum_{a\in\mathcal{A}}p(a)\log\frac{p(a)}{q(a)}$ is the Kullback-Leibler divergence, $\tau>0$ is the temperature parameter governing the strength of the entropy regularization and $\overline{\pi}$ is a baseline policy with full support; for example the uniform measure on $\mathcal{A}$ gives entropy bonuses shifted by $-\log|\mathcal{A}|$.
Regularizing with the Kullback-Leibler divergence is thus more general than with entropy bonuses and this is the regularization that we consider in this paper, akin \cite{Schulman17}.
The optimal policy $\pi_*=\argmax{\pi}J(\pi)$ is given by
\begin{align}\label{eq: bandit case optimal policy}
	\pi_*(a)=\overline{\pi}(a)\frac{\exp(r(a)/\tau)}{\mathbb{E}_{\overline{\pi}}[\exp(r(A)/\tau)]},
\end{align}
see e.g. Equation (2) in \cite{Schulman17}.

\subsection{Parametrization of the agent}

Applying the softmax function (with baseline measure $\overline{\pi}$ and temperature parameter $\tau$) to a map $h:\mathcal{A}\to\RR$ yields the Boltzmann measure
\begin{align*}
    a\mapsto\overline{\pi}(a)\frac{\exp(h(a)/\tau)}{\sum_{a'\in\mathcal{A}}\overline{\pi}(a')\exp(h(a')/\tau)}.
\end{align*}
In this context, $h(a)$ is called the \textit{preference} of $a$ of the agent.
We choose a linear model for the preferences: let $\Theta:\mathcal{A}\times\mathcal{A}\to\RR$ be a symmetric positive semi-definite kernel and denote by $\mathcal{H}$ the induced reproducible kernel Hilbert space (RKHS).
For all parameters $\theta\in\RR^{\nparam}$, where $\nparam$ denotes the number of parameters of the model, we define
\begin{align*}
    h_{\theta}(a):=\theta\cdot\psi(a),
\end{align*}
where $\psi(a)$ is a feature map associated with the kernel $\Theta$.
Then, we define the agent's policy by
\begin{align*}
	\pi_\theta:a\mapsto\overline{\pi}(a)\frac{\exp(h_\theta(a)/\tau)}{\sum_{a'\in\mathcal{A}}\overline{\pi}(a')\exp(h_\theta(a')/\tau)}.
\end{align*}


\subsection{Policy Gradient}
\label{Section: Training with policy gradient}

Let $t\in\NN$ be the time of training, starting from $t=0$ at initialisation.
At step $t$, we denote the parameters by $\theta_t$, and we write $\pi_t,h_t$ respectively for $\pi_{\theta_t},h_{\theta_t}$.
Without loss of generality, we assume that the agent's policy is updated at every step; we thus write $A_t$ for the (random) action taken at step $t$ under $\pi_t$ and $R_t$ the corresponding reward.
Let $(\mathcal{F}_t)_{t\geq 0}$ be the natural filtration of the agent, that is generated by $(\pi_t,A_t,R_t)_{t\geq 0}$.
We denote by $\EE_{\pi_t}$ the conditional expectation given $\mathcal{F}_t$ under $\pi_t$.
Let $\learningrate$ be the learning rate.
The ideal PG updates are as follows:
\begin{align}\label{Equation update theta}
	\theta_{t+1}
	=\theta_t+\learningrate\EE_{\pi_t}\left[\Big(r(A_t)-\DKL{\pi_t}{\overline{\pi}}\tau\Big)\frac{\nabla_\theta\pi_t(A_t)}{\pi_t(A_t)}\right].
\end{align}
In practice, one can use the estimate
\begin{align*}
	\theta_{t+1}
	=\theta_t+\learningrate\left(R_t-\tau\log\frac{\pi_t(A_t)}{\overline{\pi}(A_t)}-v_t\right)\frac{\nabla_\theta\pi_t(A_t)}{\pi_t(A_t)},
\end{align*}
where $v_t$ is a baseline, chosen by the user, at time $t$ that does not depend on the action, whose role is to reduce the variance, without changing the expectation of the update.
In particular, this estimate is unbiased.

Let us consider the ideal PG update \eqref{Equation update theta}.
Note that $\theta_{t+1}-\theta_t=\learningrate\nabla_\theta J(\pi_t)$ since we have
\begin{align}\label{Equation gradient KL divergence}
	\nabla_\theta\mathrm{D_{KL}}(\pi_t||\overline{\pi})
	&=\nabla_\theta\sum_{\ell=1}^{|\mathcal{A}|}\pi_t(a_\ell)\log\frac{\pi_t(a_\ell)}{\overline{\pi}(a_\ell)}\nonumber\\
	&=\sum_{\ell=1}^{|\mathcal{A}|}\left(\log\frac{\pi_t(a_\ell)}{\overline{\pi}(a_\ell)}+1\right)\nabla_\theta\pi_t(a_\ell)\nonumber\\
	&=\sum_{\ell=1}^{|\mathcal{A}|}\log\frac{\pi_t(a_\ell)}{\overline{\pi}(a_\ell)}\nabla_\theta\pi_t(a_\ell),
\end{align}
by a basic property of softmax policies, see \eqref{eq: softmax gradient cancels expected constant} in the Appendix.

\subsection{Convergence}

The action space being finite, the uniform convergence of a probability measure on $\mathcal{A}$ corresponds to the pointwise convergence.
This is the notion of convergence that we consider in the next statement, that is, we say that $\pi_t\to\pi_\infty$ if and only if $\sup_{a\in\mathcal{A}}|\pi_t(a)-\pi_\infty(a)|\to 0$ as $t\to\infty$


Recall that $\theta_{t+1}-\theta_t=\learningrate\nabla_\theta J(\pi_t)$; the next result is straightforward.

\begin{lem}\label{lem: bandit case convergence}
    There exists $\eta_0>0$ such that if $\eta<\eta_0$, then $\pi_t$ converges to some policy $\pi_\infty$, as $t\to\infty$.
\end{lem}


It is not clear a priori that $\pi_\infty$ is close to $\pi_*$.
We address this question in the next section.


\subsection{Global optimality}

In order to guarantee global optimality, we will assume (explicitly recalled when needed) that
\begin{enumerate}[label = \textbf{A\arabic*.}]
    \item $\Theta$ is \textit{strictly} positive definite, i.e. $(\Theta(a,a'))_{a,a'\in\mathcal{A}}$ is invertible. \label{assumption: pd kernel}
\end{enumerate}


In general, when the kernel $\Theta$ is positive-semidefinite, the so-called Mercer's Theorem entails that for all $a,a'\in\mathcal{A}$,
\begin{align*}
    \Theta(a,a')=\sum_{i=1}^{|\mathcal{A}|} \lambda_i e_i(a)e_i(a'),
\end{align*}
where $(e_i(a),\lambda_i)_{i=1,\ldots,|\mathcal{A}|}$ are eigenvector/eigenvalue pairs of $(\Theta(a,a'))_{a,a'\in\mathcal{A}}$, $\{(e_i(a))_{a\in|\mathcal{A}|};i=1,\ldots,|\mathcal{A}|\}$ being an orthonormal basis of $\RR^{|\mathcal{A}|}$.
The eigenvalues are all non-negative and assuming moreover \ref{assumption: pd kernel}, each is also non-zero.
We write $u\perp v$ to denote that two vectors $u$ and $v$ are orthogonal.

\begin{thm}[Global optimality]\label{thm: global optimality}
    Let $\pi_\infty$ be the policy from Lemma \ref{lem: bandit case convergence} such that $\pi_t\to\pi_\infty$ as $t\to\infty$.
    Define the vector $d\in\RR^{|\mathcal{A}|}$ by
    \begin{align*}
        d_a:=\pi_\infty(a)\left(\log\frac{\pi_{\infty}}{\pi_*}(a)-\DKL{\pi_\infty}{\pi_*}\right),\quad a\in\mathcal{A}.
    \end{align*}
    Then for all $i\in\{1,\ldots,|\mathcal{A}|\}$, we have $\lambda_i>0\ \Rightarrow\ d\perp e_i$.
    In particular, if \ref{assumption: pd kernel} holds, then $d=0$, and $\pi_\infty\equiv\pi_*$.
\end{thm}

As we explained in Introduction, global optimality of PG for finite state space (here $|\mathcal{S}|=1$) however we will extend the ideas in the proof to the more complex and interesting case of a continuous compact state space, where the environment does not only generate a reward from an action, but also sends the agent to a new state, influencing its future rewards as well.
We also note that the theorem extends to the case of $\mathcal{A}$ continuous and compact, for which Mercer's theorem applies similarly.




\section{General state space}

In this section, we introduce \textit{Matryoshka Policy Gradient} (MPG) for max-entropy RL, and establish its convergence guarantees.


\subsection{Definitions}

Besides the reward, the environment now includes a state space $\mathcal{S}$ such that when the agent chooses an action, the induced reward depends also on the current state of the agent, which then transitions to a next state that depends on the state and action it just left and took.

More formally, the agent evolves according to a Markov Decision Process (MDP) characterized by the tuple $(\mathcal{S},\mathcal{A},p,p_{\mathrm{rew}})$\footnote{Implicitely assumed in the MDP definition is the fact that all variables such that actions, visited states and rewards are measurable, so that they are well-defined random variables.}.
We assume that the action space $\mathcal{A}$ is finite and that the state space $\mathcal{S}\subset\RR^d$ is a compact set.
Let $s'\mapsto p(s,a,s')$ be the probability (or the density if $\mathcal{S}$ is continuous) that the agent moves from $s\in\mathcal{S}$ to $s'\in\mathcal{S}$ after taking action $a\in\mathcal{A}$.
When $p(s,a,s')=\delta_{s',f(s,a)}$ for some $f:\mathcal{S}\times\mathcal{A}\to\mathcal{S}$, then we say that the transitions are deterministic.
As opposed to the bandit setting, the reward does not solely depend on the action, but also on the current state, and its law is $p_{\mathrm{rew}}(\cdot|s,a)$.
To ease the presentation, we assume that the rewards are uniformly bounded and for all $(s,a)\in\mathcal{S}\times\mathcal{A}$, we denote by $r(a,s)$ the mean reward after taking action $a$ at state $s$.
All random variables are such that the process is Markovian.

A policy $\pi:\mathcal{A}\times\mathcal{S}\to\intervalleff{0}{1}$ is a map such that for all $s\in\mathcal{S}$, $\pi(\cdot|s)$ is a probability distribution on $\mathcal{A}$ that describes the law of the action taken by the agent at state $s$.
We denote by $\mathcal{P}$ the set of policies.
For every $s\in\mathcal{S}$ and $\pi,\pi'\in\mathcal{P}$, we denote by $\mathrm{D_{KL}}(\pi||\pi')(s)=\mathrm{D_{KL}}(\pi(\cdot|s)||\pi'(\cdot|s))$.

Henceforth, we assume that $\mathcal{S}$ is continuous, the results identically holding true when $\mathcal{S}$ is finite.
The case where $\mathcal{S}$ is infinite countable requires more technical adjustments that we believe are possible but the presentation would not benefit from it.

Henceforth, we assume that
\begin{itemize}
    \item the MDP is irreducible, in the sense given in \eqref{eq: gen case irreducibility} in Appendix;
    \item For any $\pi\in\mathcal{P}_n$, the law of $S_i$ is absolutely continuous w.r.t. the Lebesgue measure, for all $i=0,\cdots,n$, with $S_0\sim\mathrm{Unif}(\mathcal{S})$.
    \item for all $a\in\mathcal{A}$, the reward function $s\mapsto r(a,s)$ and the transitions  $(s,s')\mapsto p(s,a,s')$ are continuous.
\end{itemize}
The second and third items are to avoid unnecessary technicalities.




\paragraph{Extended policies.}

As opposed to standard policies as studied and used in RL, we construct a chain of policies to define an extended class of policies as follows.
Let $n\in\NN$.
Let $\pi=(\pi^{(1)},\pi^{(2)},\ldots,\pi^{(n)})$, where each $\pi^{(i)}\in\mathcal{P}$ and let $\mathcal{P}_n$ be the set of such policies.
We say that the agent follows the (extended) policy $\pi$ if it chooses its first action according to $\pi^{(n)}$, then its second according to $\pi^{(n-1)}$, and so on and so forth until it has chosen $n$ actions, the last one according to $\pi^{(1)}$.
Note that usual policies correspond to $\{(\pi,\pi,\pi,\cdots); \pi\in\mathcal{P}\}\subset\mathcal{P}_\infty$, and we write $\mathcal{P}$ for the set of policies in the usual sense.

As before, we denote by $\overline{\pi}\in\mathcal{P}$ the arbitrary baseline policy (our study would not change if it were an extended policy).
We define the $n$-step value function $V_{\pi}^{(n)}:\mathcal{S}\to\mathbb{R}$ induced by a policy $\pi\in\mathcal{P}_n$ as
\begin{align*}
	V_{\pi}^{(n)}:s\mapsto
	\mathbb{E}_{\pi}\left[\sum_{k=0}^n(R_k-\tau\DKL{\pi^{(n-k)}}{\overline{\pi}}(S_k))\Big|S_0=s\right],
\end{align*}
where the expectation is along the trajectory of length $n$ sampled under policy $\pi$.
Note that we have
\begin{align}\label{eq: gen case recursive definition value function}
    V^{(n)}_{\pi}(s)
    = \EE_{\pi^{(n)}}[R_0] - &\tau\DKL{\pi^{(n)}}{\overline{\pi}}(s)+\EE_{\pi^{(n)}}[V^{(n-1)}_{\pi'}(S_1)],
\end{align}
where $\pi'=(\pi^{(1)},\ldots,\pi^{(n-1)})\in\mathcal{P}_{n-1}$, and $S_1\sim\sum_{a\in\mathcal{A}}\pi^{(n)}(a)p(s,a,\cdot)$.
It is common to add a discount factor $\gamma\in\intervalleof{0}{1}$ to the rewards such that the last term of the equation above is multiplied by $\gamma$ (often chosen close but not equal to $1$).
As a consequence, the agent favors more the immediate (or quickly obtainable) rewards, since those obtained after $100$ steps are then multiplied by $\gamma^{100}$ and so on.
In the infinite horizon case ($n=\infty$), this ensures that the cumulative reward is finite a.s. (provided finite first moment).
Our study trivially applies to the case where the rewards are discounted, although we make the choice to continue without including it in the model, in order to ease the expressions.

The $n$-step entropy regularized $Q$-function induced by $\pi$ is defined as
\begin{align}\label{eq: gen case definition Q-function}
    Q_{\pi}^{(n)}:(a,s)\mapsto r(a,s)+\int_{s'\in\mathcal{S}}p(s,a,s')V_{\pi'}^{(n-1)}(s').
\end{align}

\textbf{Notation:}
Henceforth, for a policy $\pi\in\mathcal{P}_n$, we use the abuse of notation $V_\pi^{(i)}$ for $i<n$ for the $i$-step value function associated with $(\pi^{(1)},\ldots,\pi^{(i)})$, and similarly for the $Q$ functions and other quantities of interest, when the context makes it clear which policy is used.


\subsection{Objective and optimal policy}

The standard discounted max-entropy RL objective is defined for $\pi\in\mathcal{P}$ by
\begin{align*}
    J(\pi):= &\int_{\mathcal{S}}\EE_{\pi_t}\bigg[\sum_{k=0}^T\gamma^k\left(R_k-\DKL{\pi_t}{\overline{\pi}}(S_k)\right)\Big|S_0=s\bigg]\nu_0(\mathrm{d}s),
\end{align*}
where the horizon $T\in\NN\cup\{\infty\}$ and $\nu_0$ is the initial state distribution.

Before we redefine the objective function for extended policies, let us discuss the initial state distribution.
Suppose that the initial state is deterministic. In our case, this would have the effect to optimize our $n$-step policy \textbf{only} on that state, and then the $(n-1)$-step policy \textbf{only} on the states reachable in one step from that state, and so on and so forth.
Therefore, to avoid technicalities (such as non-uniqueness of the optimal policy on unreachable states), we assume that the initial state policy is the uniform measure on $\mathcal{S}$ (well defined by compactness).
We thus define the objective function as follows:
\begin{align}\label{def: general case objective function}
    J_n(\pi)
    :=\int_{\mathcal{S}}V_{\pi}^{(n)}(s)\mathrm{d}s.
\end{align}
Since we assume that the rewards are bounded and by compactness of $\mathcal{S}$, the objective function above is itself bounded.

Let $L^2$ be the space of square-integrable real maps $f$ defined on $\mathcal{A}\times\mathcal{S}$, that is, such that
\begin{align*}
    \int_{\mathcal{S}}\sum_{a\in\mathcal{A}}f(a,s)^2\mathrm{d}s.
\end{align*}
With a slight abuse of notation, we write $\pi=\widetilde{\pi}$ in $L^2$ to say that $\pi^{(i)}=\widetilde{\pi}^{(i)}$ in $L^2$, for all $i=1,\ldots, n$.

We say that a policy $\pi\in\mathcal{P}_n$ is optimal if and only if $J_n(\pi)\geq J_n(\pi')$ for all $\pi'\in\mathcal{P}_n$.
The existence and unicity of the optimal policy is established by the next proposition, providing in passing its explicit expression.

\begin{prop}\label{prop: optimal policy}
    There exists a unique optimal policy in the $L^2$ sense, denoted by $\pi_*=(\pi_*^{(1)},\ldots,\pi_*^{(n)})\in\mathcal{P}_n$.
    The $i$-step optimal policies, $i=1,\ldots,n$, can be obtained as follows: for all $a\in\mathcal{A}$, $s\in\mathcal{S}$,
    \begin{align*}
        \pi_*^{(1)}(a|s)
        &=\frac{\overline{\pi}(a|s)\exp(r(a,s)/\tau)}{\EE_{\overline{\pi}}[\exp(r(A,s)/\tau)]},\\
        \pi_*^{(i+1)}(a|s)
        &=\frac{\overline{\pi}(a|s)\exp\left(Q_*^{(i+1)}(a,s)/\tau\right)}{\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(i+1)}(A,s)/\tau\right)\right]},
    \end{align*}
    where $Q_*^{(i+1)}$ is a short-hand notation for $Q_{\pi_*}^{(i+1)}$ recursively defined in \eqref{eq: gen case definition Q-function}.
\end{prop}





\begin{lem}\label{lem: general case V_* = log expectation}
    For all $s\in\mathcal{S}$ and $n\geq 1$, it holds that
    \begin{align*}
        V_*^{(n)}(s)
        =\tau\log\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(n)}(A,s)/\tau\right)\right],
    \end{align*}
    where $V_*^{(0)}(s')=0$.
\end{lem}



Thanks to Lemma \ref{lem: general case V_* = log expectation}, we can write more concisely
\begin{align}\label{eq: general case optimal policy}
    \pi_*^{(i)}(a|s)
    &=\overline{\pi}(a|s)\exp\left(\left(Q_*^{(i)}(a,s)-V_*^{(i)}(s)\right)/\tau\right).
\end{align}
    


For all $n,m\in\NN$ such that $n>m$, we define the operator
\begin{align}\label{eq: definition translation operator}
    T_{n,m}:
    (\pi_1,\ldots,\pi_n) & \mapsto (\pi_1,\ldots,\pi_m)\\
    \mathcal{P}_n & \to\mathcal{P}_m.\nonumber
\end{align}
In Proposition \ref{prop: extending horizon converges to standard optimal policy} below, for all $n\in\NN$, we denote by $\pi_{*,n}\in\mathcal{P}_n$ the optimal policy for $J_n$.
We write the standard discounted, infinite horizon entropy-regularized RL objective $J_\infty$, with discounted factor $\gamma\in\intervalleoo{0}{1}$ (i.e. $V_{\pi}(s)=\EE[R_0-\DKL{\pi}{\overline{\pi}}+\gamma V_{\pi}(S_1)]$ with $\pi\in\mathcal{P}_1$).
We denote by $\pi_{*,\infty}$ the optimal policy of $J_\infty$.

\begin{prop}\label{prop: extending horizon converges to standard optimal policy}
    We have:
    \begin{enumerate}[label = (\roman*)]
        \item As $n\to\infty$, the policy $\pi_{*,n}^{(n)}$ converges to $\pi_{*,\infty}$, in the sense that
        \begin{align*}
            \lim_{n\to\infty}\int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\left|\pi_{*,n}^{(n)}(a|s)-\pi_{*,\infty}(a,s)\right|\mathrm{d}s=0.
        \end{align*}
        \item for all $n,m\in\NN$ such that $n>m$, it holds that $T_{n,m}(\pi_{*,n})=\pi_{*,m}$.
    \end{enumerate}
\end{prop}
The above Proposition \ref{prop: extending horizon converges to standard optimal policy} is intuitive:
item (i) shows that one can learn the standard discounted entropy-regularized RL objective by incrementally extending the agent's horizon;
item (ii) goes the other way and shows that the optimal policy for large horizon is built of smaller horizons policies in a consistent manner.


\subsection{Policy parametrization}
Let $\Theta^{(i)}:(\mathcal{A}\times\mathcal{S})^2\to\RR$ be a positive-semidefinite kernel.
For $i\in\{1,\ldots,n\}$, let $\theta^{(i)}\in\RR^\nparam$ be the parameters of a linear model $h_{\theta^{(i)}}^{(i)}:\mathcal{A}\times\mathcal{S}\to\mathbb{R}$, that outputs for all $(a,s)\in\mathcal{A}\times\mathcal{S}$ the $i$-step preference $h_{\theta^{(i)}}^{(i)}(a,s)$ for action $a$ at state $s$,
that is,
\begin{align*}
    h_{\theta^{(i)}}^{(i)}(a,s):=\theta^{(i)}\cdot\psi^{(i)}(a,s),
\end{align*}
where $\psi^{(i)}:\mathcal{A}\times\mathcal{S}\to\RR^\nparam$ is a feature map associated with the kernel $\Theta^{(i)}$.
The $i$-step policy $\pi_{\theta^{(i)}}^{(i)}$ is defined as the Boltzmann policy according to $h^{(i)}$, that is, for all $(a,s)\in\mathcal{A}\times\mathcal{S}$,
\begin{align*}
	\pi_{\theta^{(i)}}^{(i)}(a|s):=\overline{\pi}(a|s)\frac{\exp(h_{\theta^{(i)}}^{(i)}(a,s)/\tau)}{\sum_{a'\in\mathcal{A}}\overline{\pi}(a'|s)\exp(h_{\theta^{(i)}}^{(i)}(a',s)/\tau)}.
\end{align*}
The gradient of the policy thus reads as
\begin{align}\label{eq: gradient policy pi}
    &\nabla\pi_{\theta^{(i)}}^{(i)}(a|s)
    =\pi_{\theta^{(i)}}^{(i)}(a|s)\sum_{a'\in\mathcal{A}}\left(\delta_{a,a'}-\pi_{\theta^{(i)}}^{(i)}(a'|s)\right)\nabla h^{(i)}_{\theta^{(i)}}(a',s)/\tau.
\end{align}
Note that when $\mathcal{S}$ is finite with Kronecker delta kernels $\Theta^{(i)}((a,s),(a',s'))=\delta_{a,a'}\delta_{s,s'}$, we retrieve the so-called \textit{tabular case}.
On the other hand, in full generality for $\mathcal{S}$ continuous, we could have $P=\infty$, in particular when $\Theta^{(i)}$ is strictly positive-definite. See Appendix \ref{appendix: kernel methods} to approximate $\Theta^{(i)}$ with finitely many parameters.

\subsection{Matryoshka Policy Gradient}

In our setting, the ideal PG update would be such that $\theta_{t+1}-\theta_t=\learningrate\nabla_\theta J_n(\pi_t)$.
We introduce Matryoshka Policy Gradient (MPG), as a practical algorithm that produces unbiased estimates of the gradient (see Theorem \ref{thm: policy gradient theorem} below). 

Suppose that at time $t\in\NN$ of training, the agent starts at a uniformly sampled state $S_0\in\mathcal{S}$.
To lighten the notation, we write $\pi_t^{(i)}(\cdot|S_0):=\pi_{\theta_t^{(i)}}^{(i)}(\cdot|S_0)$.
We assume that the agent samples a trajectory according to the policy $\pi_t$, defined as follows:
\begin{enumerate}[label = \textbullet]
    \item sample action $A_0$ according to $\pi_t^{(n)}(\cdot|S_0)$,
    \item collect reward $R_0\sim p_{\mathrm{rew}}(\cdot|S_0,A_0)$ and move to next state $S_1\sim p(S_0,A_0,\cdot)$,
    \item sample action $A_1$ according to $\pi_t^{(n-1)}(\cdot|S_0)$,
    \item $\cdots$
    \item stop at state $S_n$.
\end{enumerate}
The MPG update is as follows: for $i=1,\ldots,n$,
\begin{align}\label{eq: update cascade learning}
    \theta_{t+1}^{(i)}
    &=\theta_t^{(i)} + \learningrate\sum_{\ell=n-i}^{n-1}\left(R_{\ell}-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_{\ell})\right)
    \nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i})\nonumber\\
    &=\theta_t^{(i)} + \learningrate C_i\nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i}),
\end{align}
where we just introduced the shorthand notation $C_i$.
We see that the $i$-step policy $\pi^{(i)}$ is updated using the $(i-\ell)$-step policies.


\paragraph{Matryoshka Policy Gradient Theorem}

Recall that we suppose that $\mathcal{A}$ is finite and that $\mathcal{S}$ is compact.
We say that a sequence of policies $(\pi_t)_{t\in\NN}\subset\mathcal{P}_n$ converges to $\pi\in\mathcal{P}_n$ if and only if 
\begin{align*}
    \sup_{i\in\{1,\ldots,n\}}\int_{\mathcal{S}}\sup_{a\in\mathcal{A}}\left|\pi^{(i)}_t(a|s)-\pi^{(i)}(a|s)\right|\mathrm{d}s\to 0,
\end{align*}
as $n\to\infty$, where the suprema can be replaced by maxima by compacity.

With PG, the so-called \textit{Policy Gradient Theorem} (see Section 13.2 in \cite{Sutton18}) provides a direct way to guarantee convergence of the algorithm.
Our next theorem shows that MPG also satisfies a Policy Gradient Theorem for extended policies.

\begin{thm}\label{thm: policy gradient theorem}
    With MPG as defined in \eqref{eq: update cascade learning}, it holds that $\EE[\theta_{t+1}-\theta_t]=\eta\nabla_{\theta}J_n(\pi_t)$.
    In particular, assuming ideal MPG udpate, that is, $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$, there exists $\eta_0>0$ such that if $0<\eta<\eta_0$, then $\pi_t$ converges as $t\to\infty$ to some $\pi_\infty\in\mathcal{P}_n$.
\end{thm}


\subsection{Global optimality}




As in the bandit case, the positive-semidefiniteness of $\Theta^{(i)}$ for all $i=1,\ldots,n$ implies by Mercer's Theorem that
\begin{align*}
    \Theta^{(i)}((a,s),(a',s'))
    =\sum_{j\geq 1}\lambda_j^{(i)} e_j^{(i)}(a,s)e_j^{(i)}(a',s'),
\end{align*}
where $(e_j^{(i)},\lambda_j^{(i)})_{j\geq 1}$ are eigenfunction/eigenvalue pairs, ranked in the non-increasing order of the non-negative eigenvalues, of the integral operator $\integraloperator{\pi_t}{i}:L^2\to L^2$ defined by
\begin{align*}
    \integraloperator{\pi_t}{i} f
    :(a,s)\mapsto \int_{\mathcal{S}}\mathrm{d}s'\sum_{a'\in\mathcal{A}}f(a',s')\Theta^{(i)}((a,s),(a',s')).
\end{align*}
Furthermore, $\{e_j^{(i)};j\geq 1\}$ is an orthonormal basis of $L^2$.


We will specify in our statements when we assume the following:
\begin{enumerate}[label = \textbf{A\arabic*.}]
    \setcounter{enumi}{1}
    \item For all $i\geq 1$, $\Theta^{(i)}$ is \textit{strictly} positive definite, i.e. $\lambda_j^{(i)}>0$, $\forall j\geq 1$. \label{assumption: general case pd kernel}
\end{enumerate}

Henceforth, we write $\mathbf{m}_{\pi_t}^{(i)}(s)$ for the density of $\mathbf{m}_{\pi_t}^{(i)}$ at $s$.
In the theorem below, we assume ideal MPG update, that is $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$.

\begin{thm}\label{thm: general case global optimality}
    Let $m\in\{1,\ldots,n\}$.
    Suppose that for all $i=1,\ldots, m-1$, the policy $\pi^{(i)}_t\to\pi_*^{(i)}$ as $t\to\infty$.
    Suppose moreover that $\pi^{(m)}$ converges too, and let $\pi^{(m)}_\infty$ be its limit.
    Define the map $d:\mathcal{A}\times\mathcal{S}\to\RR$ by
    \begin{align*}
        &d(a,s):=\mathbf{m}_{\pi_\infty}^{(m)}(s)\pi^{(m)}_\infty(a|s)\left(\log\frac{\pi^{(m)}_\infty}{\pi^{(m)}_*}(a|s)-\DKL{\pi^{(m)}_\infty}{\pi^{(m)}_*}(s)\right).
    \end{align*}
    Then, for all $j\geq 1$, we have $\lambda_j^{(m)}>0\ \Rightarrow\ d\perp e_j^{(m)}$ in $L^2$.
    In particular, if \ref{assumption: general case pd kernel} holds, then $\pi_\infty^{(m)}=\pi_*^{(m)}$ in $L^2$.
\end{thm}

\subsection{Neural MPG}

Suppose that instead of a linear model, the policy's preferences $h_\theta^{(i)}$, $i=1,\ldots,n$, are parametrized by deep neural networks.
It is immediate from the proofs that the policy gradient theorem holds true, that is, $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$ for the ideal MPG update.
Furthermore, we deduce a simple criteron that is sufficient for the policy at the end of training to be optimal.
This criterion is expressed in terms of the \textit{Neural Tangent Kernels} (NTKs) of the neural networks.
The NTK of the $i$-step policy at time $t$ of training is defined for all $(a,s),(a',s')\in\mathcal{A}\times\mathcal{S}$ as
\begin{align*}
    \Theta_t^{(i)}((a,s),(a',s'))
    :=\nabla_{\theta^{(i)}} h_t^{(i)}(a,s)\cdot\nabla_{\theta^{(i)}}h_t^{(i)}(a',s').
\end{align*}

\begin{cor}\label{thm: gen case deep RL global optimality}
    Let $\pi_t\in\mathcal{P}_n$ be parametrized by a neural network.
    Suppose that $\theta_{t+1}-\theta_t=\eta\nabla_\theta J_n(\pi_t)$ and that $\pi_\infty=\lim_{t\to\infty}\pi_t$.
    If $\Theta_\infty^{(i)}$ is strictly positive definite for all $i=1,\ldots,n$, then $\pi_\infty=\pi_*$ in $L^2$.
\end{cor}

In the above corollary, assuming moreover that the neural networks are in the so-called NTK regime (a.k.a. lazy regime, kernel regime, ...), ideal MPG converges to the optimal policy.
Outside of this regime, it suggests that training failure can be investigated through the lens of the NTK at the end of training.


\begin{comment}
\francois{
\subsection{NOT FINISHED: The mathematical reason for MPG}

Besides the theoretical guarantees of MPG that we presented, let us expose how the idea of the MPG framework was born.
In an attempt to prove global convergence of PG, we took inspiration from \cite{Laslier13}.
That paper studies an urn model – a very important class of stochastic processes – where an urn contains a finite number of colored balls.
A reinforcing mechanism is implemented by picking balls uniformly at random, and adding a ball of the winning color, in a competitive manner.
The authors show that by letting the process evolve, the proportions of the colors converge, by using a standard result of Probability Theory, which says the following\footnote{This is a simple stochastic analogue to the deterministic statement ``an increasing sequence of real numbers $(x_n)_{n\geq 0}$ converges.}: a negative submartingale $(X_n)_{n\geq 0}$ converges with probability $1$.
The optimality is obtained thanks to the expression of the submartingale, entailing that for the limit to have a null expectation, the proportions of the colors must give the optimal policy of the competition between the colors.

Consider the standard max-RL infinite horizon objective and let $\nu_*$ be the stationary distribution of the MDP under the optimal policy $\pi_*$.
Mimicking their argument, one defines the process
\begin{align}
    \mu_t:=
    \int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\pi_*(a|s)\log\pi_t(a|s)\nu_*(\mathrm{d}s),
\end{align}
and using \ref{Lemma Boltzmann measure is log convex}, one writes
\begin{align*}
    \mu_{t+1}-\mu_t
    \geq\int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\pi_*(a|s)(\theta_{t+1}-\theta_t)\cdot\nabla_\theta\log\pi_t(a|s)\nu_*(\mathrm{d}s).
\end{align*}
To simplify, suppose that $\Theta((a,s),(a',s'))=\delta_{a,a'}\delta_{s,s'}$.
After some computations, one can show that
\begin{align*}
    \mu_{t+1}-\mu_t
    &\geq\int_{\mathcal{S}}\sum_{a\in\mathcal{A}}\pi_t(a|s)\left(\pi_t-\pi_*\right)(a|s)\bigg(\log\frac{\pi_t}{\pi_*}(a|s)-\DKL{\pi_t}{\pi_*}(s)\\
    &\hspace{1cm}{\color{red}+\sum_{k\geq 1}\gamma^{k}\left(\EE_{\pi_t}[\DKL{\pi_t}{\pi_*}(S_k)\Big|S_k,A_0=a]-\EE_{\pi_t}[\DKL{\pi_t}{\pi_*}(S_k)\Big|S_k]\right)}\bigg)\mathrm{d}s.
\end{align*}
}
\end{comment}

\section{Numerical experiments}
\label{Section: numerical experiments}

This section summarizes the performance of the MPG framework using a deep neural network. Our current implementation of the MPG is a very simple one, without standard RL tricks, such as replay buffer, gradient clipping, etc. Details on the implementation and experimental setup, as well as additional results can be found in Appendix \ref{appendix: numerical experiments}.

First, we considered the FrozenLake benchmark \cite{opengym}, which is a maze-like $k\times k$ grid world composed of safe cells, holes and one treasure. The goal is for the agent to reach the treasure while avoiding holes. It features a finite environment and discrete action space. For $k=4$, the MPG obtained policies were optimal or very close to optimal. For $k=8$, we were able to obtain close-to-optimal policies in the sense that the treasure was consistently found after an average of 16.7 steps (the optimal deterministic path contains 14 steps). This near optimal aspect could be due to the stochasticity of the policy.

The second benchmark is the Cart Pole \cite{opengym}, which is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. Considering a horizon of $n=100$, we obtained agents for which 77\% of the games played ($500$ games) balanced the pole for the full horizon, with an average length for the game of $96$ steps. 

\begin{comment}

This section outlines the performance of the MPG framework using a finite dimensional neural network.  We evaluate the MPG on two standard benchmark problems and comment on the performance observed, as well as some considerations when using this algorithm in practice. Pseudo-code, more experimental details, and additional results can be found in the supplementary material.




\subsection{Frozen lake}
The FrozenLake benchmark \cite{opengym} is a maze-like $k\times k$ grid world composed of safe cells, holes and one treasure. It features a discrete action space, namely, the agent can move in four directions (up, down, left, right). The episode terminates when the agent reaches the treasure, falls down a hole or reaches a fixed horizon. %We consider a $n=4, 8$ for the numerical experiments. It is well-known that reshaping the reward function can change the performance of the algorithm. The original reward function does not discriminate between losing the game (falling into a hole), not moving and moving, so we changed use a reshaped reward function: losing the game (-1), not moving (-0.01), moving (0.01) and reaching the treasure (10.0).

For $k=4$, the optimal number of steps is $6$.
We use a decay for the learning rate of the form $\learningrate_t=\learningrate\times a^t$ and similarly the temperature.
We define a terminal $\tau_T = 0.03$ and terminal learning rate $\eta_T = 3\times 10^{-6}$, vary the initial learning rates $\eta$, temperatures $\tau$ and horizon to see the impact of these on the success of the agents. We train sets of $10$ agents on $1000$ episodes. Then, the trained agents play $100$ games. A summary of the results for horizon $n=10$ is given in table \ref{table:frozenlake-performance-parameters}, showing that the policies obtained are optimal or very close to optimal. The column \textit{Failed to train} denotes the policies that failed to converge (out of 10).

For $k=8$, the optimal path contains $14$ steps. We  use the a terminal $\tau_T=0.01$ and consider a horizon of $h = 100$. 

\begin{table}[h!]
\centering
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & Average Steps & Failed to train \\
 \hline
% $ \tau_0 = 0.3, ~ \eta = 0.0001$ & -- & -- & 10\\
%$ \tau_0 = 0.3, ~ \eta = 0.001$ & 100.00 & 6.00 & 0\\
%$ \tau_0 = 0.3, ~ \eta = 0.0005$ & 100.00 & 6.00 & 1\\
%$ \tau_0 = 0.4, ~ \eta = 0.0001$ & 89.10 & 6.14 & 0\\
$ \tau_0 = 0.4, ~ \eta_0 = 0.001$ & 100.00 & 6.00 & 0\\
$ \tau_0 = 0.4, ~ \eta_0 = 0.0005$ & 100.00 & 6.21 & 1\\
%$ \tau_0 = 0.5, ~ \eta = 0.0001$ & 97.00 & 6.07 & 5\\
$ \tau_0 = 0.5, ~ \eta_0 = 0.001$ & 99.80 & 6.00 & 0\\
$ \tau_0 = 0.5, ~ \eta_0 = 0.0005$ & 100.00 & 6.01 & 0\\
%$ \tau_0 = 0.6, ~ \eta = 0.0001$ & 70.86 & 7.22 & 3\\
$ \tau_0 = 0.6, ~ \eta_0 = 0.001$ & 100.00 & 6.01 & 0\\
$ \tau_0 = 0.6, ~ \eta_0 = 0.0005$ & 100.00 & 6.01 & 1\\
 [1ex] 
 \hline
\end{tabular}
\caption{Frozen Lake for $k=4$ performance after training, each set of parameters is averaged across 10 agents and 100 games per agent.}
\label{table:frozenlake-performance-parameters}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & average steps & failed to train \\
 \hline
  &  &  & \\[1ex] 
 \hline
\end{tabular}
\caption{Frozen Lake $8\times 8$ after training, for horizon with length $100$ and terminal $\tau = 0.03$.}
\label{table:frozenlake-8x8-performance-parameters}
\end{table}


\subsection{Cart Pole}
The Cart Pole benchmark \cite{opengym} is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. %The original reward function gives a $+1$ for each time that the pole stays upright, and the task finishes if the cart leaves the domain or if the pole is far enough from being upright. We reshape the reward function to give a penalty ($-10$) if the task is unsuccessful (e.g. the pole falls before reaching the target horizon). 

We set the terminal $\tau_T = 0.01$ and terminal learning rate $\eta_T = 5\times 10^{-8}$. The decay rates for $\eta$ and $\tau$ are computed to reach the desired terminal values for varying temperature and learning rate. We train sets of $5$ agents on $2000$ episodes. Then, we play $100$ games with the trained agents and record the performance of the policies,
 as shown on table \ref{table:polecart-performance-parameters}. For $\tau_0 = 0.2$ and $\eta_0 = 1\times 10^{-6}$, we have that 84\% of the games played balanced the pole for the full horizon ($n = 100$), with an average length of horizon of $92.98$. 

\begin{table}[h!]
\centering
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & Average steps & Failed to train \\
 \hline
$ \tau_0 = 0.1, ~ \eta_0 = 1\times 10^{-6}$ & 18.00 & 60.06 & 0\\
$ \tau_0 = 0.15, ~ \eta_0 = 1\times 10^{-6}$ & 51.00 & 86.24 & 0\\
$ \tau_0 = 0.2, ~ \eta_0 = 1\times 10^{-6}$ & 84.00 & 92.98 & 0\\
 [1ex] 
 \hline
\end{tabular}
\caption{Cart Pole performance after training, for horizon $n = 100$.}
\label{table:polecart-performance-parameters}
\end{table}

\end{comment}

\section{Conclusion}

In this paper, we have introduced a new framework together with Matryoshka Policy Gradient, to solve a reshaped version of the max-entropy RL objective.
We proved the global convergence of the ideal MPG update under mild assumptions and showed that the standard max-entropy objective optimal policy is retrieved by extending the maximal horizon of MPG.
For neural policies, we found an easy-to-check criterion of the neural tangent kernel at convergence that guarantees the global optimality of the limit.
The MPG framework is intuitive, theoretically sound and it is easy to implement without standard RL tricks, as we verified in numerical experiments.

The simplicity of the framework and guarantees we have derived leave open many perspectives:
\begin{itemize}
    \item We have not studied the rate of convergence of MPG; it would be interesting to understand how the fixed maximal horizon $n$ of MPG influences it, prescribing an optimal increasing schedule for $n$ in terms of the environment's parameters.
    \item We guaranteed global convergence of the ideal MPG update; we did not dwell on the stability of training with the estimate MPG update \eqref{eq: update cascade learning}.
    \item Additionally to MPG as defined in this paper, we expect to have nice theoretical properties of variations of MPG that are used for other PG algorithms.
    E.g. one can think of natural MPG, actor-critic MPG, path consistency MPG (see \cite{Nachum17} for the definition of the path consistency learning algorithm).
    \item We motivated the use of MPG with neural softmax policies by some theoretical, practical, and heuristic arguments; we believe that more can be said on the use of neural policies with MPG.
    
    \item How does the MPG framework compares to the standard max-entropy RL framework in terms of exploration, adversarial robustness, ...?
    \item We limited ourselves to a simple, trickless implementation of MPG for our numerical experiments, as a proof of concept.
    We expect to see better performances of MPG using standard RL tricks such as replay buffer, gradient clipping, etc.
\end{itemize}




%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------









\appendix
\addcontentsline{toc}{section}{Appendix}



\section*{Appendix}

The appendix is organized as follows:
\begin{itemize}
    \item \ref{appendix: parametrization}: we recall basic properties of softmax policies, then discuss the potential benefits to using a single neural network for the preferences of all $i$-step policies.
    This section ends with an explanation on how to approximate a kernel with finitely many features.
    \item \ref{appendix: bandit case}: we prove Theorem \ref{thm: global optimality} with techniques that will extend to the general state space case.
    \item \ref{appendix: general state space}: we prove the Matryoshka Policy Gradient Theorem (Theorem \ref{thm: policy gradient theorem}), Proposition \ref{prop: extending horizon converges to standard optimal policy} that shows that the infinite horizon optimal policy can approximated arbitrarily well by finite horizon optimal policies, and Theorem \ref{thm: general case global optimality} that shows global convergence of MPG.
    \item \ref{appendix: assumptions}: we list and discuss the main assumptions of the present work.
    \item \ref{appendix: numerical experiments}: we provide more detailed numerical experiments implementing MPG.
    \item \ref{appendix: multiple updates}: we propose a variation of MPG that could potentially accelerate training at the cost of stability.
\end{itemize}




\section{More on the parametrization}
\label{appendix: parametrization}

\subsection{Softmax policy}


Softmax policies enjoy the two following properties:
\begin{itemize}
    \item For all $s\in\mathcal{S}$, it holds that
    \begin{align}\label{eq: softmax gradient cancels expected constant}
        \EE_{\pi_\theta}\left[\nabla_\theta\log\pi_\theta(A|s)\right]
        =\sum_{a\in\mathcal{A}}\nabla_\theta\pi_\theta(a|s)
        =0.
    \end{align}
    \item As long as preferences are finite, it holds that $\pi_\theta(a|s)>0$ for all $(a,s)\in\mathcal{A}\times\mathcal{S}$.
\end{itemize}
In order to compute the learning rate's value below which training converge, we use the following:
\begin{lem}\label{lem: Lipschitz objective}
    For the softmax policy with linear preferences, it holds that $\theta\mapsto \nabla_\theta J_n(\pi_\theta)$ is $L$-Lipschitz for some $L>0$. 
\end{lem}
We refer to \cite{Zhang19GlobalCO} Lemma 3.2 and the discussion after Assumption 3.1 therein for the proof of this fact in the standard setting; the proof straightforwardly adapts to the extended policy setting.

A direct consequence of Lemma \ref{lem: Lipschitz objective} is that gradient ascent on $J_n(\pi_\theta)$ converges as soon as the learning rate is smaller than $1/L$.


\subsection{Neural networks}
\label{appendix: neural networks}

\paragraph{Neural Tangent Kernel.}

For neural policies, by Corollary \ref{thm: gen case deep RL global optimality}, if the NTKs at convergence are positive definite, we get global optimality.
For infinitely wide neural networks in the supervised learning setting, some global convergence guarantees have been obtained (e.g. NTK or lazy regime \cite{Jacot18,Chizat18}, mean-field regime \cite{Chizat18mf}).
In particular, in the NTK regime, the NTK is fixed throughout training, thus ensuring that it remains pd, as long as it was initialized pd.
Note that Corollary \ref{thm: gen case deep RL global optimality} gives a necessary condition, but does not say that the limit is suboptimal when the NTK is only semi-pd.

\paragraph{Extended policy parametrized by a single neural network.}

One of the assumptions of MPG is that for any $i\neq j$, the policies $\pi^{(i)}_{\theta^{(i)}}$ and $\pi^{(j)}_{\theta^{(j)}}$ do not share parameters.
Using one neural network per horizon becomes quickly costly as the maximal horizon increases.
In order to avoid this issue, one can use a single neural network $h_{\theta}$ to parametrize all $i$-step policies by using $i$ as an input such that $\pi^{(i)}_{\theta}(a|s)\propto \overline{\pi}(a|s)\exp(h_{\theta}(a,s,i)/\tau)$.
By deviating from the theory, we nonetheless expect that the performance of the model is enhanced: intuitively, the $i$-step optimal policy should be close to the $i+1$-step policy.
It is even more true as $i$ gets large, so one could also use $1-\frac{1}{i}$ as an input to the network (or any increasing map $g:\NN\mapsto \intervalleff{0}{1}$ such that $i\mapsto g(i+1)-g(i)$ is decreasing).


\subsection{Kernel methods}
\label{appendix: kernel methods}

Suppose that $\Theta$ is a strictly pd kernel with $\nparam$ positive eigenvalues.
Recall the linear model $a\mapsto h_\theta(a) = \theta\cdot\psi(a)$, with parameters $\theta\in\RR^P$, such that $\psi$ is a feature map associated with $\Theta$.
Then if $P=\infty$, one can use random features, i.e. sample $g_1,\ldots,g_{\nparam'}$ i.i.d. Gaussian processes with covariance kernel $\Theta$, then $h_\theta:=\frac{1}{\sqrt{\nparam'}}\sum_{i=1}^{\nparam'} \theta_i g_i$.
One can thus approximate the true kernel predictor using a finite number of features, see \cite{Jacot20ImplicitRO}.

Another way to approximate the kernel predictor with finitely many features is to use the spectral truncated kernel $\widehat{\Theta}$ of rank $P'\in\NN$, by cutting off the smallest eigenvalues.
That is, if $(e_i,\lambda_i)_{i\geq 1}$ are the eigenfunction/eigenvalue pairs of $\Theta$ ranked in the non-increasing order of $\lambda_i$, one can use
\begin{align*}
    \widehat{\Theta}(x,x'):=\sum_{i=1}^{\nparam'}\lambda_ie_i(x)e_i(x').
\end{align*}

\section{Bandit case}
\label{appendix: bandit case}

The following result on the dynamics of the policy during training is essential to proving Theorem \ref{thm: global optimality}:

\begin{lem}\label{lem: bandit case expected update log pi_t}
    For all $a\in\mathcal{A}$ and all $t\in\NN$, it holds that
    \begin{align*}
        \log\pi_{t+1}(a)-\log\pi_t(a)
        &= - \learningrate \tau\sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)\\
        &\hspace{4cm}\times\Big(\Theta(a,a')-\EE_{\pi_t}[\Theta(A,a')]\Big)
        + o\left(\learningrate C(\theta_t)\right),
    \end{align*}
    where the constant $C(\theta_t)$ does not depend on the learning rate $\learningrate$.
\end{lem}

\begin{proof}
    We have,
\begin{align}\label{Equation gradient of pi}
	\nabla_\theta\pi_t(a)=\frac{1}{\tau}\pi_t(a)\sum_{a'\in\mathcal{A}}(\delta_{a,'}-\pi_t(a'))\nabla_\theta h_t(a').
\end{align}
Let $B_t:=R_t-\tau\log\frac{\pi_t(A_t)}{\overline{\pi}(A_t)}$.
By Equation \eqref{Equation update theta}, and Equation \eqref{Equation gradient of pi}, using a first order Taylor approximation, we get for all $a\in\mathcal{A}$ that
\begin{align*}
	&\log\pi_{t+1}(a)-\log\pi_t(a)
	= (\theta_{t+1}-\theta_t)\cdot\frac{\nabla_\theta\pi_t(a)}{\pi_t(a)} + o\left(\learningrate C(\theta_t)\right).
\end{align*}
The first order term of the right-hand side reads as
\begin{align*}
    &\learningrate  \EE_{\pi_t}\left[B_t\frac{\nabla_\theta\pi_t(A_t)\cdot\nabla_\theta\pi_t(a)}{\pi_t(A_t)\pi_t(a)}\right]\nonumber\\
	&\hspace{2cm}=\frac{ \learningrate }{\tau^2} \EE_{\pi_t}\bigg[B_t\sum_{a',a''\in\mathcal{A}}(\delta_{A_t,a'}-\pi_t(a'))(\delta_{a,a''}-\pi_t(a''))
    \nabla_\theta h_t(a')\cdot\nabla_\theta h_t(a'')\bigg]\nonumber\\
	&\hspace{2cm}=\frac{ \learningrate }{\tau^2} \EE_{\pi_t}\bigg[B_t\sum_{a',a''\in\mathcal{A}}(\delta_{A_t,a'}-\pi_t(a'))(\delta_{a,a''}-\pi_t(a''))\Theta(a',a'')\bigg]\nonumber\\
	&\hspace{2cm}=\frac{ \learningrate }{\tau^2}\EE_{\pi_t}\bigg[B_t\left(\Theta(A_t,a)+\EE_{\pi_t\otimes\pi_t}[\Theta(A,A')]-\EE_{\pi_t}[\Theta(A,a)]-\EE_{\pi_t}[\Theta(A_t,A)]\right)\bigg],
\end{align*}
where the inner expectations are for $A$ and $A'$, and the outer one is for $A_t$. 
Using that $\EE[X(Y-\EE[Y])]=\EE[(X-\EE[X])Y]$ for any two integrable real random variables, we then get that
    \begin{align*}
        &\log\pi_{t+1}(a)-\log\pi_t(a)
        =\frac{ \learningrate }{\tau^2}\EE_{\pi_t}\Big[\left(B_t-\EE_{\pi_t}[B_t]\right)\left(\Theta(A_t,a)-\mathbb{E}_{\pi_t}[\Theta(A,a)]\right)\Big] + o\left(\learningrate C(\theta_t)\right).
    \end{align*}
    To conclude the proof, it suffices to note that by Equation \eqref{eq: bandit case optimal policy},
    \begin{align*}
        B_t-\EE_{\pi_t}[B_t]
        &= R_t-\tau\log\frac{\pi_t}{\overline{\pi}}(A_t)-\EE_{\pi_t}[r(A_t)]+\DKL{\pi_t}{\overline{\pi}}\\
        &=\log\frac{\pi_*}{\overline{\pi}}(A_t)-\tau\log\frac{\pi_t}{\overline{\pi}}(A_t)-\EE_{\pi_t}\left[\tau\log\frac{\pi_*}{\overline{\pi}}(A)\right] + \DKL{\pi_t}{\overline{\pi}}\\
        &=-\tau\log\frac{\pi_t}{\pi_*}(A_t)+\tau\DKL{\pi_t}{\pi_*},
    \end{align*}
    which yields the claim.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{thm: global optimality}}]

Let $p$ be any probability measure on $\mathcal{A}$.
Suppose that $\theta_t$ is a critical point of $\theta\mapsto J(\pi_\theta)$.
Since $\theta_{t+1}-\theta_t=\eta \nabla_\theta J(\theta_t)=0$, by Lemma \ref{lem: bandit case expected update log pi_t}, we have that
\begin{align*}
    0&=\sum_{a\in\mathcal{A}}p(a)\left(\log\pi_{t+1}(a)-\log\pi_t(a)\right)\\
    &= - \learningrate \tau\sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)
    \Big(\EE_p[\Theta(A,a')]-\EE_{\pi_t}[\Theta(A,a')]\Big)+o\left(\learningrate C(\theta_t)\right).
\end{align*}
In particular, since the above must hold for all $\learningrate>0$, we deduce that
\begin{align}\label{eq: bandit case increments are 0}
    &\sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)\nonumber\\
    &\hspace{1.2cm}\times\left(\EE_p[\Theta(A,a')]-\EE_{\pi_t}[\Theta(A,a')]\right)=0
\end{align}
We now show that if $d\not\perp e_i$, then one can choose $p$ such that the above is non-zero.
If the map
\begin{align*}
    f:a\mapsto \sum_{a'\in\mathcal{A}}\pi_t(a')\left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)\Theta(a,a')
\end{align*}
is not constant in $a$, then take $p=\delta_{a_{\min}}$ with $a_{\min}:=\argmin{a\in\mathcal{A}}f(a)$.
Clearly, the left-hand side of \eqref{eq: bandit case increments are 0} is (strictly) positive, since $\pi_t(a)>0$ for all $a\in\mathcal{A}$, which is a contradiction.
Therefore, $f$ must be constant. We write
\begin{align*}
    f(a)
    &=\sum_{i=1}^{|\mathcal{A}|}\lambda_ie_i(a)\sum_{a'\in\mathcal{A}}\pi_t(a')
    \left(\log\frac{\pi_t}{\pi_*}(a')-\DKL{\pi_t}{\pi_*}\right)e_i(a')\\
    &=\sum_{i=1}^{|\mathcal{A}|}\lambda_i \iprod{d}{e_i} e_i(a).
\end{align*}
On the other hand, since $\sum_{a\in\mathcal{A}}d_a=0$, we have that
\begin{align*}
    0
    &=\sum_{a\in\mathcal{A}}f(a)d(a)
    =\sum_{i=1}^{|\mathcal{A}|}\lambda_i \iprod{d}{e_i} \sum_{a\in\mathcal{A}}d(a)e_i(a)\\
    &=\sum_{i=1}^{|\mathcal{A}|}\lambda_i \iprod{d}{e_i}^2,
\end{align*}
which yields the claim since $\lambda_i\geq 0$ for all $i\in\{1,\ldots,|\mathcal{A}|\}$.



\end{proof}

\section{General state space}
\label{appendix: general state space}

\subsection{Matryoshka Policy Gradient Theorem}

\begin{proof}[\textbf{Proof of Theorem \ref{thm: policy gradient theorem}}]
    Let $\mathbf{m}_\pi^{(i)}$ denote the law of $S_{n-i}$, that is, the $(n-i)$-th visited state under $\pi$. 
    The distribution of the sequence $S_0,A_0,\ldots,A_{n-i-1},S_{n-i}$ is not influenced by the parameters $\theta^{(i)}$, thus we can write
    \begin{align*}
        \nabla_{\theta^{(i)}}J_n(\pi_t)
        &=\int_{\mathcal{S}}\nabla_{\theta^{(i)}}V_{\pi_t}^{(n)}(s)\mathrm{d}s\\
        &=\int_{\mathcal{S}}\nabla_{\theta^{(i)}}\bigg(\EE_{\pi_t}\bigg[\sum_{\ell=0}^{n-i-1}R_\ell-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_\ell)\bigg]\\
        &\hspace{2cm}+\EE_{S_{n-i}\sim\mathbf{m}_{\pi_t}^{(i)}}\bigg[\EE_{\pi_t}\bigg[\sum_{\ell=n-i}^{n}R_\ell-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_\ell)
        \Big|S_{n-i}\bigg]\bigg]\bigg)\mathrm{d}s\\
        &=\int_{\mathcal{S}}\left(0+\EE_{S_{n-i}\sim\mathbf{m}_{\pi_t}^{(i)}}\bigg[\nabla_{\theta^{(i)}}V_{\pi_t}^{(i)}(S_{n-i})\bigg]\right)\mathrm{d}s,
    \end{align*}
    where we have used the Markov property.
    We then have that
    \begin{align*}
        \nabla_{\theta^{(i)}}V_{\pi_t}^{(i)}(S_{n-i})
        &=\nabla_{\theta^{(i)}}\EE_{T_{n,i}(\pi_t)}\bigg[\sum_{\ell=n-i}^{n}R_\ell-\tau\log\frac{\pi^{(n-\ell)}_t}{\overline{\pi}}(A_\ell|S_\ell)\bigg|S_{n-i}\bigg]\\
        &=\EE_{T_{n,i}(\pi_t)}\bigg[\bigg(\sum_{\ell=n-i}^{n}\bigg(R_\ell-\tau\log\frac{\pi^{(n-\ell)}_t}{\overline{\pi}}(A_\ell|S_\ell)\bigg)-\tau\bigg)
        \nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i})\Big|S_{n-i}\bigg]\\
        &=\EE_{T_{n,i}(\pi_t)}\bigg[\sum_{\ell=n-i}^{n}\bigg(R_\ell-\tau\log\frac{\pi^{(n-\ell)}_t}{\overline{\pi}}(A_\ell|S_\ell)\bigg)
        \nabla\log\pi_t^{(i)}(A_{n-i}|S_{n-i})\Big|S_{n-i}\bigg],
    \end{align*}
    where we have used \eqref{eq: softmax gradient cancels expected constant} to get rid of $\tau$.
    
    Recalling the MPG update \eqref{eq: update cascade learning}, we thus have proved that $\EE[\theta_{t+1}-\theta_t]=\eta\nabla_{\theta}J_n(\pi_t)$; the convergence follows from Lemma \ref{lem: Lipschitz objective}.
\end{proof}

\begin{comment}
\francois{One argument there could be of interest to show global optimality, in case there's a pb with the current (hopefully better proof).}
\begin{proof}
    Under \ref{assumption: general case pd kernel}, the eigenfunctions $(e^{(i)}_j)_{j\geq 1}$ form a basis of $L^2(\mathcal{A}\times\mathcal{S})$ for all $i=1,\ldots,n$.
    \francois{Eigenfunctions of the integral operator defined with $\pi_\infty^{(i)}$. Clarify after reorganization.}
    Hence, if there exists $\pi\in\mathcal{P}_n$ such that $J_n(\pi_t + \epsilon(\pi-\pi_t))>J_n(\pi_t)$ for all $\epsilon>0$ small enough, then $\nabla_\theta J(\pi)\neq 0$. \francois{Should I clarify this fact?}
    That is what we show now.
    
    Suppose that $\pi_t\neq\pi_*$ and let $m$ be the smallest integer in $\{1,\ldots,n\}$ such that $\pi_t^{(i)}\neq\pi_*^{(i)}$.
    We have in particular for all $s\in\mathcal{S}$ that
    \begin{align*}
        V_{\pi_t}^{(m)}(s)
        &=\EE_{\pi_t^{(m)}}\left[R_0\right]-\tau\DKL{\pi_t^{(m)}}{\overline{\pi}}(s)+\EE_{\pi_t^{(m)}}\left[V_*^{(m-1)}(S_1)\right]\\
        &=V_*^{(m)}(s)-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(s).
    \end{align*}
    Let $\pi_{t,\epsilon}\in\mathcal{P}_n$ be identical to the policy $\pi_t$ except for the $m$-step policy $\pi_t^{(m)}$ that is replaced by $\pi_{t,\epsilon}^{(m)}:=\pi_t^{(m)} + \epsilon(\pi_*^{(m)}-\pi_t^{(m)})$, for $\epsilon\in\intervalleff{0}{1}$. (Note that $\pi_{t,\epsilon}^{(m)}$ is a valid $m$-step policy for any $\epsilon\in\intervalleff{0}{1}$.)
    Below, with an abuse of notation, we write $\pi_{t,\epsilon}$ and $\pi_t$ for policies in $\mathcal{P}_i$, $i\in\{1,\ldots,n\}$, constructed from $\pi_{t,\epsilon},\pi_t\in\mathcal{P}_n$ by considering the $1,\ldots,i$-step policies only, when the context lifts the ambiguity.
    
    One sees that $V_{\pi_{t,\epsilon}}^{(m)}(s)\geq V_{\pi_t}^{(m)}(s)$ for all $s\in\mathcal{S}$, with strict inequality for some $s_0$.
    If $\mathcal{S}$ is finite, then $s_0$ has a positive Lebesgue measure on $\mathcal{S}$.
    Otherwise, by \ref{assumption: continuous environment}, there exists an open set $U\subset\mathcal{S}$ containing $s_0$ such that $U$ has positive Lebesgue measure and $V_{\pi_{t,\epsilon}}^{(m)}(s)>V_{\pi_t}^{(m)}(s)$ for all $s\in U$.
    If $m=n$, then note that we get $J_n(\pi_{t,\epsilon})>J_n(\pi_t)$, which implies the claim, as explained at the beginning of the proof.
    
    Suppose that $m<n$.
    For all $a\in\mathcal{A}$ and $s'\in\mathcal{S}$, let $f_{a,s'}:\mathcal{S}\to\intervalleff{0}{1}$, defined by $f_{a,s'}(s):=\pi_t^{(m+1)}(a|s)p(s,a,s')$.
    By Assumption \ref{assumption: continuous environment} and the fact that $\pi_t^{(m+1)}$ is a stochastic policy, it is easy to see that $f_{a,s'}$ is continuous.
    Therefore, it holds that $U':=\bigcup_{(a,s')\in\mathcal{A}\times U}f_{a,s'}^{-1}(\intervalleof{0}{1})$ is open in $\mathcal{S}$ as a union of open sets.
    Moreover, it is non-empty since the MDP is irreducible, and therefore has a positive Lebesgue measure \francois{irreducible MDP, state it somewhere and explain the consequences}.
    On the other hand, it is readily seen from the recursive structure of the value functions that there does not exist any $s\in\mathcal{S}$ such that $V_{\pi_t}^{(m+1)}(s)<V_{\pi_t}^{(m+1)}(s)$.
    Since by definition $V_{\pi_{t,\epsilon}}^{(m+1)}(s)>V_{\pi_t}^{(m+1)}(s)$ for all $s\in U'$, we deduce that $J_{m+1}(\pi_{t,\epsilon})>J_{m+1}(\pi_t)$.
    
    If $m<n-1$, the same argument can be applied several times, showing that $J_{n}(\pi_{t,\epsilon})>J_{n}(\pi_t)$.
    This in turn implies the claim, thus concluding the proof.
\end{proof}
\end{comment}

\subsection{On the optimal policy}

\begin{proof}[\textbf{Proof of Lemma \ref{lem: general case V_* = log expectation}}]
    By definition, we write
    \begin{align*}
        V_*^{(n)}(s)
        &=\tau\sum_{a\in\mathcal{A}}\pi_*^{(n)}(a|s)\left(Q_*^{(n)}(a,s)-\tau\log\frac{\pi_*^{(n)}}{\overline{\pi}}(a|s)\right)\\
        &=\tau\log\EE_{\overline{\pi}}\left[\exp(Q_*^{(n)}(A,s)/\tau)\right]
        \sum_{a\in\mathcal{A}}\frac{\overline{\pi}(a|s)\exp\left(Q_*^{(n)}(a,s)/\tau\right)}{\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(n)}(A,s)/\tau\right)\right]}\\
        &=\tau\log\EE_{\overline{\pi}}\left[\exp\left(Q_*^{(n)}(A,s)/\tau\right)\right],
    \end{align*}
    as claimed, which concludes the proof.
\end{proof}

\begin{proof}[\textbf{Proof of Proposition \ref{prop: extending horizon converges to standard optimal policy}}]
    
    (i) Let $\pi\in\mathcal{P}$ be any standard policy, and let $\pi_n=(\pi,\ldots,\pi)\in\mathcal{P}_n$.
    By definition of the standard discounted objective $J_\infty$, using the dominated convergence theorem (rewards are bounded), we have that $J_n(\pi_n)\to J_\infty(\pi)$.
    In particular, we get that $\pi_{*,n}^{(n)}$ achieves a performance arbitrarily close to that of $\pi_{*,\infty}$ in the infinite horizon discounted setting, and since the optimal policy of $J_\infty$ is unique (Lebesgue almost everywhere), we deduce that $\pi_{*,n}^{(n)}\to\pi_{*,\infty}$ as $n\to\infty$.
    
    
    
    
    (ii)
    Suppose that $J_1(\pi_{*,1})>J_1(T_{n,1}(\pi_{*,n}))$, that is
    \begin{align*}
        \int_{\mathcal{S}}V_{\pi_{*,1}}^{(1)}(s)\mathrm{d}s
        >\int_{\mathcal{S}}V_{\pi_{*,n}}^{(1)}(s)\mathrm{d}s.
    \end{align*}
    In particular, the set $\widetilde{S}\subset\mathcal{S}$ such that $s\in\widetilde{S}$ if and only if $V_{\pi_{*,1}}^{(1)}(s)>V_{\pi_{*,n}}^{(1)}(s)$ is non-empty and has a positive Lebesgue measure.
    Furthermore, by optimality, $s\in\mathcal{S}\setminus\widetilde{\mathcal{S}}$ if and only if $V_{\pi_{*,1}}^{(1)}(s)=V_{\pi_{*,n}}^{(1)}(s)$.
    Let $\widetilde{\pi}_{*,n}\in\mathcal{P}_n$ be identical to $\pi_{*,n}$ except for the 1-step policy where $\pi_{*,n}^{(1)}$ is replaced by $\pi_{*,1}$.
    Then, the recursive structure of the value function \eqref{eq: gen case recursive definition value function} entails that $J_n(\widetilde{\pi}_{*,n})>J_n(\pi_{*,n})$ (we implicitly use that the MDP preserves the absolute continuity of its state's law), which is a contradiction.
    Therefore, $T_{n,1}(\pi_{*,n})=\pi_{*,1}$.
    
    Then, by induction and using the recursive structure of the value function, the same argument shows that $T_{n,m}(\pi_{*,n})=\pi_{*,m}$ for all $m=2,\ldots,n-1$, which concludes the proof.
\end{proof}


\subsection{Global optimality of MPG}




\begin{lem}\label{lem: general case value function is optimal value function minus DKL}
    For all $n\geq 1$, all $\pi\in\mathcal{P}_n$ and all $s\in\mathcal{S}$, it holds that
    \begin{align*}
        &V_{\pi}^{(n)}(s)
        -V_*^{(n)}(s)
        =-\EE_{\pi}\left[\left.\sum_{i=0}^{n-1}\DKL{\pi^{(n-i)}}{\pi_*^{(n-i)}}(S_i)\right|S_0=s\right].
    \end{align*}
\end{lem}

\begin{proof}
    Recall \eqref{eq: gen case recursive definition value function} and write
    \begin{align*}
        V_{\pi}^{(n)}(s)
        &=\sum_{a\in\mathcal{A}}\pi^{(n)}(a|s)\bigg(r(a,s) - \tau\log\frac{\pi^{(n)}}{\overline{\pi}}(a|s) + \sum_{s'\in\mathcal{S}}p(s,a,s')V^{(n-1)}_{\pi}(s')\bigg)\\
        &=\sum_{a\in\mathcal{A}}\pi^{(n)}(a|s)\bigg(V_*^{(n)}(s) - \tau\log\frac{\pi^{(n)}}{\pi_*}(a|s) + \sum_{s'\in\mathcal{S}}p(s,a,s')(V^{(n-1)}_{\pi}(s')-V_*^{(n-1)}(s'))\bigg),
    \end{align*}
    where we plugged in the expression of the optimal policy \eqref{eq: general case optimal policy}.
    We can rewrite the above as
    \begin{align*}
        &V_{\pi}^{(n)}(s)-V_*^{(n)}(s)
        =-\DKL{\pi^{(n)}}{\pi_*^{(n)}} + \EE\left[V^{(n-1)}_{\pi}(S_1)-V_*^{(n-1)}(S_1)\Big|S_0=s\right].
    \end{align*}
    The claim follows by induction.
\end{proof}


\begin{proof}[\textbf{Proof of Proposition \ref{prop: optimal policy}}]
    The Kullback-Leibler divergence being non-negative, it is readily seen that for all $s\in\mathcal{S}$, the maximal value of $\pi\mapsto V_{\pi}^{(n)}(s)$ is obtained for $\pi=\pi_*$.
    It is then immediate that $\pi_*$ is the unique optimal policy (outside a set of Lebesgue measure $0$) for the objective $J_n$ given in \eqref{def: general case objective function}.
\end{proof}


\begin{lem}\label{lem: general case expected update log pi_t}
    Let $t\in\NN$ and $m\in\{1,\ldots,n\}$.
    Suppose that $\pi_t^{(k)}=\pi_*^{(k)}$ for all $k=1,\ldots,m-1$.
    For all $a\in\mathcal{A}$ and $s\in\mathcal{S}$, it holds that
    \begin{align*}
        &\log\pi^{(m)}_{t+1}(a|s)-\log\pi^{(m)}_t(a|s)\\
        &\hspace{1cm}= - \learningrate \tau\int_{\mathcal{S}}\mathbf{m}_{\pi_t}^{(m)}(\mathrm{d}s')\sum_{a'\in\mathcal{A}}\pi^{(m)}_t(a'|s')
        \left(\log\frac{\pi^{(m)}_t}{\pi^{(m)}_*}(a'|s')-\DKL{\pi^{(m)}_t}{\pi^{(m)}_*}(s')\right)\\
        &\hspace{3cm}\times\left(\Theta^{(m)}((a,s),(a',s'))-\EE_{\pi^{(m)}_t}[\Theta^{(m)}((A,s),(a',s'))]\right)
        +o\left(\learningrate C(\theta_t)\right),
    \end{align*}
    where $\mathbf{m}_{\pi_t}^{(m)}$ is the law of $S_{n-m}$ under policy $\pi_t$ and the constant $C(\theta_t)$ does not depend on $\learningrate$.
\end{lem}



\begin{comment}
{\color{olive}Proof with scaling factors $\rho$
\begin{proof}
    
    We follow the same steps as in the single state case treated in Lemma \ref{lem: bandit case expected update log pi_t}.
    The gradient of the policy reads as
    \begin{align}%\label{eq: general case gradient policy pi}
        \nabla_\theta\pi^{(m)}_t(a|s)
        =\frac{1}{\tau}\pi^{(m)}_t(a|s)\sum_{a'\in\mathcal{A}}\left(\delta_{a,a'}-\pi^{(m)}_t(a'|s)\right)\nabla_{\theta}h^{(m)}_t(a,s).
    \end{align}
    Let $(a,s)\in\mathcal{A}\times\mathcal{S}$.
    Using Lemma \ref{Lemma Boltzmann measure is log convex} and Equation \eqref{eq: update cascade learning}, we write
    \begin{align}%\label{eq: gen case log increment to compute}
        &\EE\left[\log\pi_{t+1}^{(m)}(a|s)-\log\pi_t^{(m)}(a|s)\right]
        \geq (\theta_{t+1}^{(m)}-\theta_t^{(m)})\cdot\frac{\nabla_\theta \pi_t^{(m)}(a|s)}{\pi_t^{(m)}(a|s)}\nonumber\\
        &\hspace{0.6cm}=\frac{\alpha}{\tau^2}\sum_{k=0}^{n-m}\EE_{\pi_t}\left[C_{k,m}\sum_{a',a''\in\mathcal{A}}\left(\delta_{a,a'}-\pi_t^{(m)}(a'|s)\right)\left(\delta_{A_k,a''}-\pi_t^{(m)}(a'|S_k)\right)\Theta^{(m)}((a',s),(a'',S_k))\right].
    \end{align}
    We focus on the expectation in the right-hand side.
    It is equal to
    \begin{align*}
        &\EE_{\pi_t}\bigg[C_{k,m}\bigg(\Theta^{(m)}((a,s),(A_k,S_k))-\EE_A\left[\Theta^{(m)}((A,s),(A_k,S_k))\right]\\
        &\hspace{5cm}-\EE_A\left[\Theta^{(m)}((a,s),(A',S_k))\right]+\EE_{A,A'}\left[\Theta^{(m)}((A,s),(A',S_k))\right]\bigg)\bigg],
    \end{align*}
    where $A,A'$ have respective laws $\pi_t^{(m)}(\cdot|s)$ and $\pi_t^{(m)}(\cdot|S_k)$ and are mutually independent of all other variables (conditionally given $S_k$ for $A'$).
    Using the trick $\EE[X(Y-\EE[Y])]=\EE_[(X-\EE[X])Y]$, we obtain
    \begin{align}%\label{eq: gen case after expectation trick}
        \EE_{\pi_t}\left[\left(C_{k,m}-\EE\left[C_{k,m}|S_k\right]\right)\left(\Theta^{(m)}((a,s),(A_k,S_k))-\EE_A\left[\Theta^{(m)}((A,s),(A_k,S_k))\right]\right)\right],
    \end{align}
    where
    \begin{align*}
        \EE\left[C_{k,m}|S_k\right]
        &=\EE\left[\rho_{m,k}\sum_{\ell=0}^{m-1}\left(R_{k+\ell}-\tau\log\frac{\pi_t^{(m-\ell)}}{\overline{\pi}}(A_{k+\ell}|S_{k+\ell})\right)\bigg|S_k\right].
    \end{align*}
    In the expectation, $A_{k+\ell}$ has law $\pi_t^{(k+\ell)}(\cdot|S_{k+\ell})$ conditionally given $S_{k+\ell}$.
    Hence, with the factor $\frac{\pi_t^{(m-\ell)}}{\pi_t^{(k+\ell)}}(A_{k+\ell}|S_{k+\ell})$ in the factor $\rho_{m,k}$, we get 
    \begin{align*}
        \EE\left[C_{k,m}|S_k\right]
        &=\EE\left[\sum_{\ell=0}^{m-1}\left(R_{k+\ell}'-\tau\log\frac{\pi_t^{(m-\ell)}}{\overline{\pi}}(A_{k+\ell}'|S_{k+\ell}')\right)\bigg|S_k\right],
    \end{align*}
    where $A_{k+\ell}'$ has conditional law $\pi_t^{(m-\ell)}(\cdot|S_{k+\ell}')$ given $S_{k+\ell}'$, and the laws of $R_{k+\ell}'$ and $S_{k+\ell}'$ are changed accordingly.
    In particular, $\EE\left[C_{k,m}|S_k\right]=V_{\pi_t}^{(m)}(S_k)$.
    The expression in \eqref{eq: gen case after expectation trick} can thus be written as
    \begin{align*}
        &\EE_{\pi_t}\left[\left(C_{k,m}-V_{\pi_t}^{(m)}(S_k)\right)\left(\Theta^{(m)}((a,s),(A_k,S_k))-\EE_A\left[\Theta^{(m)}((A,s),(A_k,S_k))\right]\right)\right],\\
        \shortintertext{using Lemma \ref{lem: general case consistency} and replacing $A_{k+\ell},S_{k+\ell}$ with $A_\ell',S_\ell'$ –– such that $A_\ell'$ has conditional law $\pi_t^{(m-\ell)}(\cdot|S_\ell')$ given $S_\ell'$ –– yield}
        &\hspace{1cm}=\EE_{\pi_t}\bigg[\bigg(V_*^{(m)}(S_0)-V_{\pi_t}^{(m)}(S_0)-\sum_{\ell=0}^{m-1}\tau\log\frac{\pi_t^{(m-\ell)}}{\pi_*^{(m-\ell)}}(A_\ell'|S_\ell')\bigg)\\
        &\hspace{5cm}\times\left(\Theta^{(m)}((a,s),(A_0',S_0'))-\EE_A\left[\Theta^{(m)}((A,s),(A_0',S_0'))\right]\right)\bigg].
    \end{align*}
    Then, Lemma \ref{lem: general case value function is optimal value function minus DKL} shows that the above is equal to
    \begin{align*}
        &\sum_{\ell=0}^{m-1}\tau\EE_{\pi_t}\bigg[\bigg(\EE_{\pi_t}\bigg[\DKL{\pi_t^{(m-\ell)}}{\pi_*^{(m-\ell)}}(A_\ell'|S_\ell')\bigg]-\log\frac{\pi_t^{(m-\ell)}}{\pi_*^{(m-\ell)}}(A_\ell'|S_\ell')\bigg)\\
        &\hspace{5cm}\times\left(\Theta^{(m)}((a,s),(A_0',S_0'))-\EE_A\left[\Theta^{(m)}((A,s),(A_0',S_0'))\right]\right)\bigg]\\
        &\hspace{1cm}=\tau\EE_{\pi_t}\bigg[\bigg(\DKL{\pi_t^{(m)}}{\pi*^{(m)}}(A_0'|S_0')-\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_0'|S_0')\bigg)\\
        &\hspace{5cm}\times\left(\Theta^{(m)}((a,s),(A_0',S_0'))-\EE_A\left[\Theta^{(m)}((A,s),(A_0',S_0'))\right]\right)\bigg],
    \end{align*}
    where we used the fact that $\pi_t^{(k)}=\pi_*^{(k)}$ for all $k=1,\cdots,m-1$.
    Plugging the right-hand side above in \eqref{eq: gen case log increment to compute} in place of the expectation, we obtain the claim, which concludes the proof.
\end{proof}
}
\end{comment}




\begin{proof}
    We follow the same steps as in the single state case treated in Lemma \ref{lem: bandit case expected update log pi_t}.
    The gradient of the policy reads as
    \begin{align}\label{eq: general case gradient policy pi}
        \nabla_\theta\pi^{(m)}_t(a|s)
        &=\frac{1}{\tau}\pi^{(m)}_t(a|s)\sum_{a'\in\mathcal{A}}\left(\delta_{a,a'}-\pi^{(m)}_t(a'|s)\right)
        \nabla_{\theta}h^{(m)}_t(a,s).
    \end{align}
    Let $(a,s)\in\mathcal{A}\times\mathcal{S}$.
    Using \eqref{eq: update cascade learning} and a first order Taylor approximation, we write
    \begin{align}\label{eq: gen case log increment to compute}
        \log\pi_{t+1}^{(m)}(a|s)-\log\pi_t^{(m)}(a|s)
        &= (\theta_{t+1}^{(m)}-\theta_t^{(m)})\cdot\frac{\nabla_\theta \pi_t^{(m)}(a|s)}{\pi_t^{(m)}(a|s)}+o\left(\learningrate C(\theta_t)\right)\nonumber\\
        &=\frac{\learningrate}{\tau^2}\EE_{\pi_t}\bigg[C_m\sum_{a',a''\in\mathcal{A}}\left(\delta_{a,a'}-\pi_t^{(m)}(a'|s)\right)\nonumber\\
        &\hspace{1cm}\times\left(\delta_{A_{n-m},a''}-\pi_t^{(m)}(a'|S_{n-m})\right)\Theta^{(m)}((a',s),(a'',S_{n-m}))\bigg]\nonumber\\
        &\hspace{7.5cm}+o\left(\learningrate C(\theta_t)\right).
    \end{align}
    We focus on the expectation.
    It is equal to
    \begin{align*}
        &\EE_{\pi_t}\bigg[C_m\bigg(\Theta^{(m)}((a,s),(A_{n-m},S_{n-m}))
        -\EE_A\left[\Theta^{(m)}((A,s),(A_{n-m},S_{n-m}))\right]\\
        &\hspace{3cm}-\EE_A\left[\Theta^{(m)}((a,s),(A',S_{n-m}))\right]
        +\EE_{A,A'}\left[\Theta^{(m)}((A,s),(A',S_{n-m}))\right]\bigg)\bigg],
    \end{align*}
    where $A,A'$ have respective laws $\pi_t^{(m)}(\cdot|s)$ and $\pi_t^{(m)}(\cdot|S_{n-m})$ and are mutually independent of all other variables (conditionally given $S_{n-m}$ for $A'$).
    Using the trick $\EE[X(Y-\EE[Y])]=\EE_[(X-\EE[X])Y]$, we obtain
    \begin{align}\label{eq: gen case after expectation trick}
        &\EE_{\pi_t}\Big[\left(C_m-\EE\left[C_m|S_{n-m}\right]\right)\Big(\Theta^{(m)}((a,s),(A_{n-m},S_{n-m}))
        -\EE_A\left[\Theta^{(m)}((A,s),(A_{n-m},S_{n-m}))\right]\Big)\Big].
    \end{align}
    We write
    \begin{align*}
        \EE\left[C_m|S_{n-m}\right]
        &=\EE\left[\sum_{\ell=n-m}^{n}\left(R_{\ell}-\tau\log\frac{\pi_t^{(n-\ell)}}{\overline{\pi}}(A_{\ell}|S_{\ell})\right)\bigg|S_{n-m}\right]\\
        &=V_{\pi_t}^{(m)}(S_{n-m})\\
        &=V_*^{(m)}(S_{n-m})-\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(S_{n-m}),
    \end{align*}
    where we used Lemma \ref{lem: general case value function is optimal value function minus DKL} and the fact that $\pi_t^{(i)}=\pi_*^{(i)}$ for all $i=1,\ldots,m-1$.
    Similarly and using the expression \eqref{eq: general case optimal policy} of the optimal policy, we have
    \begin{align*}
        \EE[C_m|S_{n-m},A_{n-m}]
        &=R_{n-m}-\tau\log\frac{\pi_t^{(m)}}{\overline{\pi}}(A_{n-m}|S_{n-m}) + \EE\left[V_{\pi_t}^{(m-1)}(S_{n-m})\Big|S_{n-m},A_{n-m}\right]\\
        &= \EE\Big[V_{\pi_t}^{(m-1)}(S_{n-m+1})-V_*^{(m-1)}(S_{n-m+1})\Big|S_{n-m},A_{n-m}\Big]\\
        &\hspace{4cm}-\tau\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})+V_*^{(m)}(S_{n-m})\\
        &= -\tau\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})+V_*^{(m)}(S_{n-m}).
    \end{align*}
    Hence, the expression in \eqref{eq: gen case after expectation trick} becomes
    \begin{align*}
        &\tau\EE_{\pi_t}\bigg[\bigg(\DKL{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})
        -\log\frac{\pi_t^{(m)}}{\pi_*^{(m)}}(A_{n-m}|S_{n-m})\bigg)
        \Big(\Theta^{(m)}((a,s),(A_{n-m},S_{n-m}))\\
        &\hspace{8.7cm}-\EE_A\left[\Theta^{(m)}((A,s),(A_{n-m},S_{n-m}))\right]\Big)\bigg],
    \end{align*}
    which corresponds to the first order term in right-hand side of the equation in the Lemma.
    Coming back to \eqref{eq: gen case log increment to compute}, this concludes the proof.
\end{proof}


\begin{proof}[\textbf{Proof of Theorem \ref{thm: general case global optimality}}]
    
Let $a\in\mathcal{A},s\in\mathcal{S}$.
Suppose that $\pi^{(i)}_t\equiv\pi^{(i)}_*$ for all $i=1,\ldots,m-1$, and that $\pi^{(m)}_t=\pi^{(m)}_\infty$, that is, $(\theta^{(1)}_t,\ldots,\theta^{(m)}_t)$ is a critical point of $(\theta^{(1)},\ldots,\theta^{(m)})\mapsto J_m(\widetilde{\pi}_\theta)$.
By Lemma \ref{lem: general case expected update log pi_t}, we have that 
\begin{align*}
    0&=
    \log\pi^{(m)}_{t+1}(a|s)-\log\pi^{(m)}_t(a|s)\\
    &= - \learningrate \tau\int_{\mathcal{S}}\mathrm{d}s'\sum_{a'\in\mathcal{A}}d_{a',s'}\bigg(\Theta^{(m)}((a,s),(a',s'))
    -\EE_{\pi^{(m)}_t}[\Theta^{(m)}((A,s),(a',s'))]\bigg)+o\left(\learningrate C(\theta_t)\right).
\end{align*}
Since the above must be true for all $\learningrate>0$, we deduce that 
\begin{align}\label{eq: gen case lower bound log increment with d}
    &\int_{\mathcal{S}}\mathrm{d}s'\sum_{a'\in\mathcal{A}}d_{a',s'}\bigg(\Theta^{(m)}((a,s),(a',s'))
    -\EE_{\pi^{(m)}_t}[\Theta^{(m)}((A,s),(a',s'))]\bigg)=0.
\end{align}
Define
\begin{align*}
    f:
    \mathcal{A}\times\mathcal{S}&\to\RR,\\
    (a,s)&\mapsto\int_{\mathcal{S}}\mathrm{d}s'\sum_{a'\in\mathcal{A}}d_{a',s'}\Theta^{(m)}((a,s),(a',s')).
\end{align*}
Let $s_0\in\mathcal{S}$ such that $a\mapsto f(a,s_0)$ is not constant and let $a_{\min}:=\argmin{a\in\mathcal{A}}f(a,s_0)$.
Then, because $\pi_t^{(m)}$ is a stochastic policy, the left-hand side of \eqref{eq: gen case lower bound log increment with d} with $(a,s)=(a_{\min},s_0)$ must be strictly positive, which is a contradiction.
Hence, for every fixed $s\in\mathcal{S}$, the map $a\mapsto f(a,s)$ is constant.

We then use Mercer's Theorem to write
\begin{align*}
    f(a,s)
    &=\sum_{j\geq 1} \lambda_j^{(m)}e_j^{(m)}(a,s)\int_{\mathcal{S}}\mathrm{d}s'\sum_{a'\in\mathcal{A}}d_{a',s'} e_j^{(m)}(a',s').
\end{align*}
On the other hand, $\sum_{a\in\mathcal{A}}d_{a,s}=0$ for all $s\in\mathcal{S}$, therefore
\begin{align*}
    0
    &=\int_{\mathcal{S}}\mathrm{d}s\sum_{a\in\mathcal{A}}f(a,s)d_{a,s}\\
    &=\sum_{j\geq 1} \lambda_j^{(m)}\int_{\mathcal{S}}\mathrm{d}s\sum_{a\in\mathcal{A}} d_{a,s} e_j^{(m)}(a,s)
    \int_{\mathcal{S}}\mathrm{d}s'\sum_{a'\in\mathcal{A}}d_{a',s'} e_j^{(m)}(a',s')\\
    &=\sum_{j\geq 1} \lambda_j^{(m)}\left(\int_{\mathcal{S}}\mathrm{d}s\sum_{a\in\mathcal{A}} d_{a,s} e_j^{(m)}(a,s)\right)^2.
\end{align*}
Therefore, whenever $\lambda_j>0$, we must have $d\perp e_j^{(m)}$ in $L^2$.
This concludes the proof.
\end{proof}



\section{Assumptions}
\label{appendix: assumptions}

We say that the MDP is irreducible if and only if
\begin{align}\label{eq: gen case irreducibility}
    &\forall s,s'\in\mathcal{S},\forall \epsilon>0,\exists k\in\mathbb{N},\exists a_0,\ldots,a_{k-1}:\nonumber\\ &\int_{B(s,\epsilon)}\mathrm{d}s_0\int_{\mathcal{S}}\mathrm{d}s_1\cdots\int_{\mathcal{S}}\mathrm{d}s_{k-1}\int_{B(s',\epsilon)}\mathrm{d}s_k
    \prod_{m=0}^{k-1}p(s_m,a_m,s_{m+1})>0,
\end{align}
where $B(s,\epsilon)$ denotes the ball $\{\widetilde{s}\in\mathcal{S}: ||s-\widetilde{s}||<\epsilon\}$.
In words, this means that any neighborhood of any state is reachable from any neighborhood of any state.

We now list the assumptions and briefly mention their roles in this work:
\begin{itemize}
    \item The initial state distribution is uniform on $\mathcal{S}$: it is not restrictive, as its role is to ensure that the optimal policies for all horizons visit (Lebesgue almost all) the whole state space, thus avoiding considerations about reachable states. 
    \item Continuous compact $\mathcal{S}$: this allows to safely apply Mercer's Theorem. Extensions to non-compact spaces should be possible by applying other versions of Mercer's Theorem, see e.g. the introduction of \cite{Vito13}.
    \item Finite $\mathcal{A}$: simplifies the presentation. Extensions to continuous action spaces should be possible similarly as for the previous item.
    \item Continuous reward and transition functions: imply measurability of the variables generated by the MDP and avoid pathological cases.
    \item The MDP is irreducible: avoid technicalities.
    \item The MDP preserves absolute continuity: avoid technicalities, some statements and proofs have to be adapted if that is not the case. 
    \item Rewards are bounded: ensures that value functions are well defined.
    \item The temperature $\tau$ is the same for all steps: it is unnecessary, as one could regularize the objectives according to the number of remaining steps. That is, $\pi^{(i)}$ could be regularized with $\tau_i$ for all $i=1,\ldots,n$ with $\tau_i\neq\tau_j$ if $i\neq j$, and the theory would still be valid.
\end{itemize}


\section{Numerical experiments}
\label{appendix: numerical experiments}
We apply MPG on a number of case studies.
The MPG is implemented as in algorithm \ref{alg:mpg}.

\begin{algorithm}
\For {t = 1, $...$ , horizon}{
    generate trajectory $\{(s_i,s_{i+1},a_i,r_{i})\}_{i=0}^{n-1}$ from $\pi_t$ \newline
    \For {i = 1, $\cdots$ , n}{
        $\pi_{t+1} \leftarrow $ update policy as in \eqref{eq: update cascade learning}}
    decay $\tau, \eta$}
\caption{MPG implementation}
\label{alg:mpg}
\end{algorithm}


We use exponential decays for temperature $\tau$ and learning rate $\learningrate$, with prescribed terminal temperature $\tau_T$ and learning rate $\eta_T$. For example, for $\tau$ the decay rate is computed as $d_\tau = \left(\frac{\tau_T}{\tau_0}\right)^{1/ngames}$.

\subsection{Frozen lake}
The FrozenLake benchmark \cite{opengym} is a $k\times k$ grid composed of cells, holes and one treasure. It features a discrete action space, namely, the agent can move in four directions (up, down, left, right). The episode terminates when the agent reaches the treasure. We consider a $k=4, 8$ for the numerical experiments. It is well-known that reshaping the reward function can change the performance of the algorithm. The original reward function does not discriminate between losing the game (falling into a hole), not moving and moving, so we use a reshaped reward function: losing the game ($-1$), moving against a wall ($-0.05$), moving ($+0.05$) and reaching the treasure ($+10.0$).

For $k=4$, the optimal number of steps is $6$. We define a terminal $\tau_T = 0.03$ and terminal learning rate $\eta_T = 3\times 10^{-6}$, vary the initial learning rates $\eta$, temperatures $\tau$ and horizon to see the impact of these on the success of the agents. We train sets of $10$ agents on $1000$ episodes. Then, the trained agents play $100$ games. A summary of the results for horizons $n=10,15$ is given in table \ref{table:frozenlake-performance-parameters}, showing that the policies obtained are optimal or very close to optimal. The column \textit{Failed to train} denotes the policies that failed to converge (out of 10).

Considering a larger map, for $k=8$, the optimal path contains $14$ steps. We vary the terminal temperature $\tau_T$, with the intuition that the stochasticity of the policy can deteriorate the performance of the agent over longer horizons. We also consider a longer horizon of $h = 100$. Furthermore, we further modify the reward function on the states: moving against a wall ($-0.1$) and moving ($0.01$). We train sets of 5 agents on $2000$ episodes. A summary is given in table \ref{table:frozenlake-8x8-performance-parameters}, showing that near optimal policies obtained are attainable with our algorithm. However, the convergence and training stability of our method depends heavily on training hyperparameters, such as  the terminal temperature $\tau_T$, initial temperature $\tau_0$ and learning rates $ \learningrate_0, \learningrate_T$.

\begin{table}[h!]
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & average steps & failed to train \\
 \hline
 \multicolumn{4}{|c|}{Horizon = 10}\\
 \hline
 $ \tau_0 = 0.3, ~ \eta = 0.0001$ & -- & -- & 10\\
$ \tau_0 = 0.3, ~ \eta = 0.001$ & 100.00 & 6.00 & 0\\
$ \tau_0 = 0.3, ~ \eta = 0.0005$ & 100.00 & 6.00 & 1\\
$ \tau_0 = 0.4, ~ \eta = 0.0001$ & 89.10 & 6.14 & 0\\
$\mathbf{ \tau_0 = 0.4, ~ \eta = 0.001}$ & \textbf{100.00} & \textbf{6.00} & \textbf{0}\\
$ \tau_0 = 0.4, ~ \eta = 0.0005$ & 100.00 & 6.21 & 1\\
$ \tau_0 = 0.5, ~ \eta = 0.0001$ & 97.00 & 6.07 & 5\\
$ \tau_0 = 0.5, ~ \eta = 0.001$ & 99.80 & 6.00 & 0\\
$ \tau_0 = 0.5, ~ \eta = 0.0005$ & 100.00 & 6.01 & 0\\
$ \tau_0 = 0.6, ~ \eta = 0.0001$ & 70.86 & 7.22 & 3\\
$ \tau_0 = 0.6, ~ \eta = 0.001$ & 100.00 & 6.01 & 0\\
$ \tau_0 = 0.6, ~ \eta = 0.0005$ & 100.00 & 6.01 & 1\\
 \hline
 \multicolumn{4}{|c|}{Horizon = 15}\\
 \hline
 $ \tau_0 = 0.3, ~ \eta = 0.0001$ & 100.00 & 6.40 & 2\\
$ \tau_0 = 0.3, ~ \eta = 0.001$ & 100.00 & 6.00 & 3\\
$ \tau_0 = 0.3, ~ \eta = 0.0005$ & 100.00 & 6.01 & 1\\
$ \tau_0 = 0.4, ~ \eta = 0.0001$ & 99.67 & 6.93 & 1\\
$ \tau_0 = 0.4, ~ \eta = 0.001$ & 100.00 & 6.75 & 2\\
$ \tau_0 = 0.4, ~ \eta = 0.0005$ & 100.00 & 6.09 & 0\\
$ \tau_0 = 0.5, ~ \eta = 0.0001$ & 99.67 & 6.11 & 1\\
$ \tau_0 = 0.5, ~ \eta = 0.001$ & 100.00 & 6.03 & 0\\
$ \tau_0 = 0.5, ~ \eta = 0.0005$ & 100.00 & 6.26 & 0\\
$ \tau_0 = 0.6, ~ \eta = 0.0001$ & 98.90 & 6.56 & 0\\
$ \tau_0 = 0.6, ~ \eta = 0.001$ & 100.00 & 6.00 & 1\\
$ \tau_0 = 0.6, ~ \eta = 0.0005$ & 100.00 & 6.07 & 0\\
 [1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Performance of trained agents on the Frozen Lake task on $4\times 4$ grid, for horizons $n = 10, 15$.}
\label{table:frozenlake-performance-parameters}
\end{table}


\begin{table}[h!]
\centering
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & average steps & failed to train \\
 \hline
 \hline
 \multicolumn{4}{|c|}{$\tau_T = 0.03$}\\
 \hline
$ \tau_0 = 0.15, ~ \eta = 5e-06$ & 13.00 & 18.25 & 3\\
$ \tau_0 = 0.15, ~ \eta = 1e-05$ & -- & -- & 5\\
%$ \tau_0 = 0.15, ~ \eta = 0.0001$ & nan & nan & 5\\
$ \tau_0 = 0.2, ~ \eta = 5e-06$ & 29.67 & 18.91 & 2\\
$ \tau_0 = 0.2, ~ \eta = 1e-05$ & 60.33 & 16.77 & 2\\
%$ \tau_0 = 0.2, ~ \eta = 0.0001$ & nan & nan & 5\\
$ \tau_0 = 0.3, ~ \eta = 5e-06$ & -- & -- & 5\\
$ \tau_0 = 0.3, ~ \eta = 1e-05$ & 25.50 & 16.66 & 3\\
%$ \tau_0 = 0.3, ~ \eta = 0.0001$ & nan & nan & 5\\
\hline
 \multicolumn{4}{|c|}{$\tau_T = 0.01$}\\
 \hline
$ \tau_0 = 0.2, ~ \eta_0 = 1e-06$ & 98.67 & 15.96 & 2\\
$ \tau_0 = 0.2, ~ \eta_0 = 1e-05$ & 100.00 & 19.35 & 4\\
$ \tau_0 = 0.2, ~ \eta_0 = 5e-06$ & 93.00 & 16.06 & 3\\
$ \tau_0 = 0.3, ~ \eta_0 = 1e-05$ & -- & -- & 5\\
$ \tau_0 = 0.3, ~ \eta_0 = 5e-06$ & 50.00 & 20.84 & 4\\
 \hline
 \multicolumn{4}{|c|}{$\tau_T = 0.005$}\\
 \hline
$ \tau_0 = 0.2, ~ \eta_0 = 1e-06$ & 87.50 & 19.69 & 3\\
%$ \tau_0 = 0.2, ~ \eta_0 = 5e-06$ & nan & nan & 1\\
$ \tau_0 = 0.3, ~ \eta_0 = 1e-06$ & 94.00 & 18.10 & 0\\
$ \mathbf{\tau_0 = 0.3, ~ \eta_0 = 2e-06}$ & \textbf{100.00} & \textbf{16.67} & \textbf{1}\\
$ \tau_0 = 0.3, ~ \eta_0 = 3e-06$ & 95.50 & 16.83 & 1\\
$ \tau_0 = 0.3, ~ \eta_0 = 5e-06$ & 99.50 & 15.64 & 3\\
$ \tau_0 = 0.4, ~ \eta_0 = 1e-06$ & 67.33 & 19.68 & 2\\
$ \tau_0 = 0.4, ~ \eta_0 = 5e-06$ & 58.00 & 14.25 & 3\\
[1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{Performance of trained agents on the Frozen Lake task on $8\times 8$ grid, for horizon $n = 100$ and varying terminal $\tau_T$.}
\label{table:frozenlake-8x8-performance-parameters}
\end{table}

\subsection{Cart Pole}
The Cart Pole benchmark is a classical control problem. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pole is placed upright on the cart, and the goal is to balance the pole by moving the cart to the left or right for some finite horizon time. It features a continuous environment and a discrete action space. The original reward function gives a $+1$ reward for each time that the pole stays upright, and the task finishes if the cart leaves the domain or if the pole is far enough from being upright. We reshape the reward function to give a penalty ($-10$) if the task is unsuccessful (i.e. the pole falls before reaching the target horizon). 

We set the terminal $\tau = 0.01$ and terminal learning rate $\eta = 5\times 10^{-8}$. We train sets of $5$ agents on $2000$ episodes. Then, we play $100$ games with the trained agents and record the performance of the policies,
 as shown on table \ref{table:polecart-performance-parameters}. 

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{|c | c |c | c |} 
 \hline
 Parameters & Success \% & Average steps & Failed to train \\
 \hline
$ \tau_0 = 0.1, ~ \eta_0 = 1e-06$ & 50.40 & 85.11 & 0\\
$ \mathbf{\tau_0 = 0.15, ~ \eta_0 = 1e-06}$ & \textbf{77.20 }& \textbf{95.09} & \textbf{0}\\
$ \tau_0 = 0.15, ~ \eta_0 = 5e-06$ & 0.00 & 12.56 & 0\\
$ \tau_0 = 0.2, ~ \eta_0 = 1e-06$ & 70.60 & 89.60 & 0\\
$ \tau_0 = 0.2, ~ \eta_0 = 5e-06$ & 43.20 & 68.95 & 0\\
 [1ex] 
 \hline
\end{tabular}
\end{adjustbox}
\caption{CartPole task performance after training, for horizon with length $100$.}
\label{table:polecart-performance-parameters}
\end{table}

In practice, in order to achieve a more robust implementation, one could consider various stabilisation techniques such as gradient clipping to avoid updates which are too large or usage of off-policy updates, for example, through the use of a replay buffer. Other improvements to the training strategy can be adopted, such as: adaptive techniques to choose $\tau$ and $\learningrate$ during training can be considered, beyond the chosen continuous decay, batched trajectory updates, etc.

\section{Multiple updates per path}
\label{appendix: multiple updates}

In Theorem \ref{thm: general case global optimality}, global optimality of $\pi^{(i)}$ for all $1\leq i\leq n$ is shown using an inductive argument on $i$.
It is therefore desirable to train the $1$-step policy faster than the $2$-step policy, itself faster than the $3$-step policy, and so on.
The following observation allows one to maximize the use of one path for training:
since a path for one MPG update is of length $n$, the $1$-step policy can be trained $n$ times, the $2$-step policy $n-1$ times, and so on until the $n$-step policy that can be updated once.

Let $\rho_{i,k}:=\prod_{\ell=0}^{i-1}\frac{\pi^{(i-\ell)}_t}{\pi^{(n-k-\ell)}_t}(A_{k+\ell}|S_{k+\ell})$.
We propose the following variation of MPG:
\begin{align*}
    \theta_{t+1}^{(1)}
    &=\theta_t^{(1)} + \learningrate\sum_{k=0}^{n-1} \rho_{1,k} \left(R_k-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(A_k|S_k)\right)
    \nabla\log\pi_t^{(1)}(A_k|S_k),
\end{align*}
and more generally for $i=2..n$,
\begin{align*}
    \theta_{t+1}^{(i)}
    &=\theta_t^{(i)} + \learningrate\sum_{k=0}^{n-i}\rho_{i,k}\sum_{\ell=0}^{i-1}\left(R_{\ell+k}-\tau\log\frac{\pi_t^{(i-\ell)}}{\overline{\pi}}(A_{k+\ell}|S_{k+\ell})\right)
    \nabla\log\pi_t^{(i)}(A_k|S_k)\nonumber\\
    &=\theta_t^{(i)} + \learningrate\sum_{k=0}^{n-i}C_{k,i}\nabla\log\pi_t^{(i)}(A_k|S_k),
\end{align*}
The scaling factors $\rho_{i,k}$ are used so that in expectation, the update of $\pi^{(i)}_t$ is done with actions sampled from itself.
However, the denominator in $\rho_{i,k}$ can lead to training instability: we naively implemented this trick on the $4\times 4$ maze of \ref{appendix: numerical experiments} and we observed that with the trick, training failed to find the optimal policy where it was found without it, using the same set of hyperparameters.



\begin{comment}
The factor $\rho_{i-\ell,k+\ell}$ allows to maximize the use of samples in a path: it rescales the $(\ell+k)$-th reward so that the $(i-\ell)$-th policy is updated, in average, with samples from itself.
For instance, $\pi_t^{(1)}$ is updated such that
\begin{align*}
    \EE\left[\theta_{t+1}^{(1)}-\theta_t^{(1)}|\mathcal{F}_t\right]
    &=\learningrate^{(1)}\EE_{\pi_t}\left[\sum_{k=0}^{n-1} \rho_{1,k}\left(R_k-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(A_k|S_k)\right)\nabla\log\pi_t^{(1)}(A_k|S_k)\right]\\
    &=\learningrate^{(1)}\sum_{k=0}^{n-1}\EE_{\pi_t^{(1)}}\left[\left(R_k'-\tau\log\frac{\pi_t^{(1)}}{\overline{\pi}}(A_k'|S_k')\right)\nabla\log\pi_t^{(1)}(A_k'|S_k')\right],
\end{align*}
where $A_k'$ has conditional law $\pi_t^{(1)}(\cdot|S_k')$ and $S_k'$ has the law of the $(k+1)$-st state of a path where actions are sampled according to $\pi^{(1)}$.
\francois{
State that global optimality still holds, only convergence can't be shown (at least not straightforwardly with same proof).
Same issue with different learning rates
}
\end{comment}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}