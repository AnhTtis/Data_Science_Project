\section{Introduction}

Knowledge Graph (KG) stores rich knowledge and is essential for many real-world applications, such as question answering~\cite{DBLP:conf/acl/YihCHG15,DBLP:conf/ijcai/ZhouYHZXZ18,DBLP:conf/wsdm/HuangZLL19}, urban computing~\cite{zhao2017modeling,zhao2022multi} and recommendation systems~\cite{DBLP:conf/cikm/WangZWZLXG18,DBLP:conf/kdd/Wang00LC19,chen2022knowledge}.
Typically, a KG consists of relational triples, which are represented as \textit{\textless head entity, relation, tail entity\textgreater}~\cite{DBLP:journals/pieee/Nickel0TG16}.
Nevertheless, KGs are inevitably incomplete due to the complexity, diversity and mutability of knowledge. 
To fix this gap, the problem of link prediction is studied so as to predict potential missing triples~\cite{DBLP:conf/nips/BordesUGWY13}.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig/Example.pdf}
	\caption{An example of link prediction which may be hard to predict without interaction of multimodal information.}
	\label{fig:example}
\end{figure}

Traditional link prediction models, including translation-based~\cite{DBLP:conf/nips/BordesUGWY13,DBLP:conf/aaai/WangZFC14} and neural network methods~\cite{DBLP:conf/naacl/NguyenNNP18,DBLP:conf/acl/NathaniCSK19}, suffered from the structural bias problem among triples.
Recently, some studies~\cite{DBLP:conf/ijcai/XieLLS17,DBLP:conf/starsem/SergiehBGR18,DBLP:conf/emnlp/PezeshkpourC018} addressed this problem by enriching the dataset and proposing new models to capture multimodal information for link prediction.
However, the performances of such studies were limited as they projected all modalities into a unified space with the same relation to capture the commonality, which might fail to preserve specific information in each modality.
As a result, they could not effectively model the complicated interactions between modalities to capture the complementarity.

To address the above issue, we incline to learn the knowledge comprehensively rather than separately, which is similar to how humans think.
Take the scenario in Figure~\ref{fig:example} as an example, such a model might also get the wrong prediction that \textit{LeBorn James} \texttt{playsFor} \textit{Golden States Warriors} based on the similarity with \textit{Stephen Curry} of the common \texttt{bornIn} relation to \textit{Akron, Ohio} in graph structure.
Meanwhile, it is difficult for visual features to express fine-grained semantics and the only conclusion is that \textit{LeBorn James} is a basketball player.
Also, it might also make the outdated prediction of \textit{Cleveland Cavaliers} due to `played' in the second sentence (more consistent with \texttt{playsFor} than `joined' in the third sentence) in the textual description.
Nevertheless, by integrating the knowledge, it is easy to get the correct answer \textit{Log Angeles Lakers} with the interaction between complementary information of structural, visual and textual highlighted in Figure~\ref{fig:example}.
Since the knowledge learned from different modalities is diverse and complex, it is very challenging to effectively integrate multimodal information.

In this paper, we propose a novel \textbf{I}nteractive \textbf{M}ultimodal \textbf{F}usion Model (\name) for multimodal link prediction over knowledge graphs. 
\name can learn the knowledge separately in each modality and jointly model the complicated interactions between different modalities with a two-stage fusion which is similar to the natural recognition process of human beings introduced above.
In the multimodal fusion stage, we employ a bilinear fusion mechanism to fully capture the complicated interactions between the multimodal features with contrastive learning.
For the basic link prediction model, we utilize the relation information as the context to rank the triples as predictions in each modality.
In the final decision fusion stage, we integrate predictions from different modalities and make use of the complementary information to make the final prediction.
The contributions of this paper are summarized as follows:
\begin{itemize}
    \item We propose a novel two-stage fusion model, \name, that is effective in integrating complementary information of different modalities for link prediction.
    \item We design an effective multimodal fusion module to capture bilinear interactions with contrastive learning for jointly modeling the commonality and complementarity.
    \item We demonstrate the effectiveness and generalization of \name with extensive experiments on four widely used datasets for multimodal link prediction.
\end{itemize}
