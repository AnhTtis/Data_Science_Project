\section{Conclusion}

In this paper, we study the problem of link prediction over multimodal knowledge graphs.
Specifically, we aim at improving the interaction between different modalities.
To reach this goal, we propose the \name with a two-stage framework to enable effective fusion of multimodal information by (i) utilizing bilinear fusion to fully capture the complementarity between different modalities and contrastive learning to enhance the correlation between different modalities of the same entity to be stronger; and (ii) employing an ensembled loss function to jointly consider the predictions of multimodal representations.
Experimental results on several benchmarking datasets demonstrate the effectiveness of our proposed model.
Besides, we also conduct in-depth exploration to illustrate the generalization of our proposed method and the potential opportunity to apply it in real applications.

However, there are still some limitations of \name, which are left to future works.
For example, \name requires the integrity of all the modalities and an additional component to predict the missing modalities may be useful to tackle this limitation.
Besides, designing appropriate components to support more different kinds of modalities or propose a more lightweight fusion model to replace the bilinear model for better efficiency is also feasible.