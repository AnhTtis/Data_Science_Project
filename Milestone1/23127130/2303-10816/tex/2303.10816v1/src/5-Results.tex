\section{Experimental Results}

\begin{table}
	\centering
	\scalebox{1}{
		\begin{tabular}{ccccc}
			\toprule
			& \multicolumn{4}{c}{FB15K-237}\\
			& MR & MRR & H@1 & H@10 \\
			\midrule
			TransE & 357 & 0.294 & - & 46.5\\
			ConvE & 244 & 0.325 & 23.7 & 50.1\\
			ConvKB & 309 & 0.243 & - & 42.1\\
			CapsE & 403 & 0.150 & - & 35.6\\
			RotatE & 177 & 0.338 & 24.1 & 53.3\\
			QuatE & 176 & 0.311 & 22.1 & 49.5\\
			KBAT & 223 & 0.232 & 13.6 & 42.8\\
			TuckER & 162 & 0.353 & 26.1 & 53.6\\
			HAKE & - & 0.346 & 25.0 & 54.2\\
			\midrule
			IKRL & 193 &  0.309 &  23.2 & 49.3\\
			MKGC & 187 &  0.297 &  22.9 & 49.4\\
			MKBE & 158 &  0.347 &  25.8 & 53.2\\
			\midrule
			IMF (w/o MF) & 188 & 0.324 & 23.4 & 51.8\\ 
			IMF (w/o DF) & 149 & 0.356 & 26.5 & 55.7\\ 
			IMF (w/o CL) & 138 & 0.371 & 27.8 & 57.1\\
			IMF & \textbf{134*} & \textbf{0.389*} & \textbf{28.7*} & \textbf{59.3*}\\ 
			\bottomrule
		\end{tabular}
	}
	\caption{Evaluation results on FB15K-237. ``*'' indicates the statistically significant improvements (i.e., two-sided t-test with $p<0.05$) over the best baseline.}\label{tbl:FB15K-237}
\end{table}

\subsection{Overall Performance}
As shown in Table~\ref{tbl:MMKG} and Table~\ref{tbl:FB15K-237}, we can observe that:

\begin{itemize}[leftmargin=*]
\item \name significantly outperforms all the baselines.
The performance gain is at most 42\% for MRR on DB15K while is also more than 20\% for all the evaluation metrics on average.

\item %On the other hand, 
State-of-the-art monomodal methods employ a variety of complex models to improve the expressiveness and capture latent interactions.
However, the results illustrate that the performance is highly limited by the structural bias of the nature of knowledge graph itself.
Although these methods have already achieved promising results, \name can easily outperform them by a significant margin with a much simpler model structure, which amply demonstrates the effectiveness.

\item In comparison with multimodal methods that treat the features of different modalities separately, our \name jointly learning from different modalities with the two-stage fusion, which is beneficial in modeling the commonality and complementarity simultaneously.

\end{itemize}

Overall, our proposed \name can model more comprehensive interactions between different modalities with both commonality and complementarity thanks to the effective fusion of multimodal information and thus achieve significant improvement of link prediction on KGs.

\subsection{Ablation Study}

Table~\ref{tbl:MF} shows the evaluation results of leveraging different modality information on FB15K-237, where $S$ denotes structural information; $V$ denotes visual information of images and $T$ denotes textual information of descriptions.
We can see that by introducing visual or textual information, the performance is significantly improved.
The significant performance gain brought by multimodal fusion module not only demonstrates the effectiveness of our approach, but also indicates the potential of integrating multimodal information in KG.

\begin{table}
  \centering
  \scalebox{1}{
    \begin{tabular}{ccccc}
      \toprule
        & \multicolumn{4}{c}{FB15K-237}\\
        & MR & MRR & H@1 & H@10 \\
      \midrule
        S & 188 & 0.324 & 23.4 & 51.8\\ 
        S+V & 143 & 0.367 & 27.4 & 55.4\\ 
        S+T & 139 & 0.374 & 28.1 & 58.6\\ 
        S+V+T & \textbf{134} & \textbf{0.389} & \textbf{28.7} & \textbf{59.3}\\ 
      \bottomrule
    \end{tabular}
  }
  \caption{Evaluation results with different modality combinations on FB15K-237.}\label{tbl:MF}
\end{table}

To verify the effectiveness of decision fusion, we choose a case of \textless \textit{LeBron James}, \texttt{playsFor} \textgreater \ and visualize the prediction scores of each modality as Figure~\ref{fig:DF} shows.
Due to biases in each modality, the prediction of monomodal is inevitable error-prone.
The results in Table~\ref{tbl:MMKG} and Table~\ref{tbl:FB15K-237} also demonstrate the effectiveness of applying decision fusion to ensemble the specific latent features of each modality.

Besides, the performance comparison between \name (w/o CL) and \name in Table~\ref{tbl:MMKG} and Table~\ref{tbl:FB15K-237} illustrates the necessity of contrastive learning for more robust results, especially in the scenario with fewer training samples and relation types.

From the results shown above, we can see that each component in our propose \name has a significant contribution to the overall performance and it is beneficial to capture the commonality and complementarity between different modalities.

\subsection{Generalization}

In order to evaluate the generalization of our proposed approach, we simply replace the scoring function (contextual relational model) with existing methods such as TransE, ConvE and TuckER.
The results in Figure~\ref{fig:TS} illustrate that our proposed framework of two-stage fusion is general enough to be applied to any link prediction model for further improvement.

\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{fig/G.pdf}
	\caption{MRR (\%) improvement of different basic models on FB15K-237 with IMF.}
	\label{fig:TS}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.62\linewidth]{fig/Param.pdf}
    \caption{Performance influence of different embedding size.}
    \label{fig:param}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[width=0.82\linewidth]{fig/Case.pdf}
	\caption{Visualization of low-dimensional representations for basketball players under the context \texttt{playsFor}. Each colored node denotes a basketball player and the different colors denote five basketball teams.}
	\label{fig:case}
\end{figure*}

\subsection{Parameter Analysis}

Figure~\ref{fig:param} shows the performance influence of embedding size for \name.
From the picture, we can see that the embedding size plays an important role in the model performance.
Meanwhile, it is worthy of note that a larger embedding size not always results in better performance due to the overfitting problem, especially in the datasets with fewer relation types like YAGO15K.
Considering the performance and the efficiency, the best choices of embedding size for these three datasets are 256, 256 and 128, respectively.

\subsection{Case Study}

In order to illustrate the effectiveness of our IMF model in a more intuitive way, we apply \textit{t-SNE} to reduce dimension and visualize the contextual entity representations of basketball players in five different basketball teams.
We can see in Figure~\ref{fig:case} that the representations of basketball players are messed up with monomodal information due to the biases.
However, with the help of interactive multimodal fusion, \name can effectively capture complicated interactions between different modalities.