\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/ModelStructure.pdf}
    \caption{Overall architecture of \name. The left part represents different modality-specific encoders to extract latent features and the multimodal fusion module to integrate multimodal representations. The right part represents the contextual relational model decoders to get the similarity score and the decision fusion module to make the final prediction on all modalities.}
    \label{fig:model}
\end{figure*}

\section{Methodology}

Formally, a knowledge graph is defined as $\mathcal{G} = \langle \mathcal{E}, \mathcal{R}, \mathcal{T} \rangle$, where $\mathcal{E}$ and $\mathcal{R}$ indicate sets of entities and relations, respectively. 
$\mathcal{T} = \{(h, r, t) | h, t \in \mathcal{E}, r \in \mathcal{R}\}$ represents relational triples of the KG.
In multimodal KGs, each entity in KGs is represented by multiple features from different modalities.
Here, we define the set of modalities $\mathcal{K} = \{s, v, t, m\}$ where $s, v, t, m$ denote structural, visual, textual and multimodal modality, respectively.
Due to the complexity of real-world knowledge, it is almost impossible to take all the triples into account.
Therefore, given a well-formulated KG, the \emph{Link Prediction} task aims at predicting missing links between entities.
Specifically, link prediction models expect to learn a score function of relational triples to estimate the likelihood of a triple, which is always formulated as $\psi : \mathcal{E} \times \mathcal{R} \times \mathcal{E} \to \mathbb{R}$.


\subsection{Overall Architecture}

In order to fully exploit the complicated interaction between different modalities, we propose a two-stage fusion model instead of simply considering the multimodal information separately in a unified vector space.
As shown in Figure~\ref{fig:model}, \name consists of four key components:
\begin{itemize}[leftmargin=*]
	\item[1] The Modality-Specific Encoders are used for extracting structural, visual and textual features as the input of multimodal fusion stage.
	\item[2] The Multimodal Fusion Module, which is the first fusion stage, effectively models bilinear interactions between different modalities based on \textit{Tucker} decomposition and contrastive learning.
	\item[3] The Contextual Relational Model calculates the similarity of contextual entity representations to formulate triple scores as modality-specific predictions for decision fusion stage.
	\item[4]  The Decision Fusion Module, which is the second fusion stage, takes all the similarity scores from structural, visual, textual and multimodal models into account to make the final prediction.
\end{itemize}

\subsection{Modality-Specific Encoders}
In this subsection, we first introduce the pre-trained encoders used for different modalities.
These encoders are not fine-tuned during training and we treat them as fixed feature extractors to obtain the modality-specific entity representations.
Note that \name is a general framework and it is straightforward to replace them with other up-to-date encoders or add ones for new modalities into \name.

\subsubsection{Structural Encoder}

From the most basic view, the structural information of KG, we employ a Graph Attention Network (GAT)\footnote{https://github.com/Diego999/pyGAT}~\cite{DBLP:conf/iclr/VelickovicCCRLB18} with TransE loss.

Specifically, our GAT encoder takes L1 distance of neighbor aggregated representations as energy function of triples, which is $E(h, r, t) = ||\mathbf{h}+\mathbf{r}-\mathbf{t}||$.
In the training process, we minimize the following Hinge loss~\eqref{eq-gat-loss}:
\begin{equation}\label{eq-gat-loss}
    \begin{split}
        \mathcal{L}_{GAT} = & \sum_{(h,r,t) \in \mathcal{T}}\sum_{(h', r, t') \in \mathcal{T'}} \mathrm{max} \{0,  \\
        &\gamma + E(h,r,t) - E(h',r,t')\}
    \end{split}
\end{equation}
where $\gamma$ is margin hyper-parameter and $\mathcal{T'}$ denotes set of negative triples derived from $\mathcal{T}$. 
$\mathcal{T'}$ is created by randomly replacing head or tail entities of triples in $\mathcal{T}$, which is~\eqref{eq-gat-neg}:
\begin{equation}\label{eq-gat-neg}
    \mathcal{T'} = \{(h',r,t)|h' \in \mathcal{E} \backslash h\} \cup \{(h,r,t')|t' \in \mathcal{E} \backslash t\}
\end{equation}

\subsubsection{Visual Encoder} 
Visual features are greatly expressive while providing different views of knowledge from traditional KGs. 
To effectively extract visual features, we utilize VGG16\footnote{https://github.com/machrisaa/tensorflow-vgg} pre-trained on \textit{ImageNet}\footnote{https://image-net.org/} to get image embeddings of corresponding entities following~\cite{DBLP:conf/esws/LiuLGNOR19}.
Specifically, we take outputs of the last hidden layer before softmax operation as visual features, which are 4096-dimensional vectors.

\subsubsection{Textual Encoder} 
Entity descriptions contain much richer but more complex knowledge than pure KGs.
To fully extract the complex knowledge, we employ BERT~\cite{DBLP:conf/naacl/DevlinCLT19} as the textual encoder, which is very expressive to get description embeddings of corresponding entities.
The textual features are 768-dimensional vectors, i.e., pooled outputs of pre-trained BERT-Base model\footnote{https://github.com/huggingface/transformers}.

\subsection{Multimodal Fusion}
The multimodal fusion stage aims to effectively get multimodal representations, which fully capture the complex interactions between different modalities.
Many existing multimodal fusion methods have achieved promising results in many tasks like VQA (Visual Question Answering).
However, most of them aim at finding the commonality to get more precise representations by modality projecting~\cite{DBLP:conf/nips/FromeCSBDRM13,DBLP:conf/aaai/CollellZM17} or cross-modal attention~\cite{DBLP:conf/aaai/PerezSVDC18}.
These types of methods will suffer from the loss of unique information in different modalities and can not achieve sufficient interaction between modalities.
To this end, we propose to employ the bilinear models, which have a strong ability to realize full parameters interaction as the cornerstone to perform the fusion of multimodal information.
Specifically, we extend the \textit{Tucker} decomposition, which decomposes the tensor into a core tensor transformed by a matrix along with each mode to 4-mode factors as expressed in Equation~\eqref{eq-tucker}:
\begin{equation}\label{eq-tucker}
    \mathcal{P} = (((\mathcal{P}_c \times \mathbf{M}_s) \times \mathbf{M}_v) \times \mathbf{M}_t) \times \mathbf{M}_d
\end{equation}
where $\mathbf{M}_s \in \mathbb{R}^{d_s \times t_s}$, $\mathbf{M}_v \in \mathbb{R}^{d_v \times t_v}$, $\mathbf{M}_t \in \mathbb{R}^{d_t \times t_t}$,  $\mathbf{M}_d \in \mathbb{R}^{\mathcal{D} \times t_d}$ denotes transformation matrix and $\mathcal{P}_c \in \mathbb{R}^{t_s \times t_v \times t_t \times t_d}$ denotes a smaller core tensor.

In such a situation, entity embeddings are first projected into a low-dimensional space and then fused with the core tensor $\mathcal{P}_c$.
Following~\cite{DBLP:conf/iccv/Ben-younesCCT17}, we further reduce the computation complexity by decomposing the core tensor $\mathcal{P}_c$ to merge representations of all modalities into a unified space with element-wise product.
The detailed calculation process is expressed as Equation~\eqref{eq-fusion}:
\begin{equation}\label{eq-fusion}
    \mathbf{e}_m = \tilde{\mathbf{e}}_s^\mathsf{T} \mathbf{M}_d^s * \tilde{\mathbf{e}}_v^\mathsf{T} \mathbf{M}_d^v * \tilde{\mathbf{e}}_t^\mathsf{T} \mathbf{M}_d^t
\end{equation}
where $\tilde{\mathbf{e}}_k = \mathrm{ReLU}(\mathbf{e}_k\mathbf{M}_k) \in \mathbb{R}^{t_k}$ denotes latent representations and $\mathbf{e}_k \in \mathbb{R}^{d_k}$ is the original embedding representations and $\mathbf{M}_d^k \in \mathbb{R}^{t_k \times t_d}$ is decomposed transformation matrix for each modality $k \in \{s, v, t\}$.

However, the multimodal bilinear fusion has no bound limitation while the gradient produced by the final prediction result can only implicitly guide parameter learning.
To alleviate this problem, we add constraints to limit the correlation between different modality representations of the same entity to be stronger.
Therefore, we further leverage contrastive learning~\cite{DBLP:conf/icml/ChenK0H20,DBLP:conf/nips/LiSGJXH21,DBLP:conf/cvpr/Yuan0K0WMKF21} between different entities and modalities as an additional learning objective for regularization.
In the settings of contrastive learning, we take the pairs of representations of the same entity of different modalities as positive samples and the pairs of representations of different entities as negative samples.
As shown in Figure~\ref{fig:cl}, we aim at limiting the distance of negative samples to be larger than positive samples to enhance multimodal fusion, which is:
\begin{equation}
    d(f(x), f(x^+)) << d(f(x), f(x^-))
\end{equation}
where $d(\cdot, \cdot)$ denotes the distance measure and $f(\cdot)$ denotes the embedding function. The superscript $+, -$ represent the positive and negative samples, respectively.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/ContrastiveLearning.pdf}
    \caption{Example of multimodal contrastive learning. The distance between the representations of the same entity in different modalities is minimized, while the distance between the representations of different entities is maximized.}
    \label{fig:cl}
\end{figure}

Specifically, we randomly sample $N$ entities from the entity set as a minibatch and define contrastive learning loss upon it.
The positive pairs are naturally obtained with the same entities while the negative pairs are constructed by negative sharing~\cite{DBLP:conf/kdd/ChenSSH17} of all other entities.
We take the latent representations $\tilde{\mathbf{e}}_k = \mathrm{ReLU}(\mathbf{e}_k\mathbf{M}_k) \in \mathbb{R}^{t_k}$ and leverage cosine similarity $d(u, v) = - \mathbf{u}^\mathsf{T}\mathbf{v}/||\mathbf{u}||\mathbf{v}||$ as distance measure.
Then we have the following contrastive loss function for each entity $i$:
\begin{equation}\label{eq-cl}
    \mathcal{L}_{CLi} = \frac{1}{3N} \sum_{p,q \in \mathcal{M}} \sum_{j=1}^N  d(e_i^{p}, e_i^{q}) - d(e_i^{p}, e_j^{q}) + 2
\end{equation}
where $\mathcal{M} = \{(s, v), (s, t), (v, t)\}$ is set of modality pairs.

\subsection{Contextual Relational Model}
After obtaining representations of each modality and multimodal, we then design a contextual relational model, which takes relations in triples as contextual information for scoring, to get the predictions.
Note that this relational model can be easily replaced by any scoring function like TransE.

Due to the variety and complexity of relations in KGs, we argue that improving the degree of parameter interaction~\cite{DBLP:conf/aaai/VashishthSNAT20} is crucial for better modeling the relational triples.
The degree of parameter interaction means the calculation ratio of each parameter to all other parameters. 
For example, dot product could achieve $1/d$ degree while cross product could achieve $(d-1)/d$ degree.
Based on this assumption, we propose to use bilinear outer product between entity and relation embeddings to incorporate contextual information into entity representations.
Instead of taking relations as input as in previous studies, our contextual relational model utilizes relations to provide context in the transformation matrix of entity embeddings.
Then, entity embeddings are projected using the contextual transformation matrix to get \emph{contextual embeddings}, which are used for calculating similarity with all candidate entities.
The learning objective is to minimize the binary cross-entropy loss.
For each modality $k \in \mathcal{K}$, the computation details are shown as Equation~\eqref{eq-crm} to Equation~\eqref{eq-loss}:
\begin{gather}
    \hat{\mathbf{e}}_k = \mathbf{e}_k^\mathsf{T}\mathbf{W}_k^r  + \mathbf{b} = \mathbf{e}_k^\mathsf{T}\mathbf{W}_k\mathbf{r} + \mathbf{b}_k \label{eq-crm} \\
    \mathbf{y}_k = \sigma(\mathrm{cosine}(\mathbf{e}_k, \hat{\mathbf{e}}_k)) = \sigma (\frac{\mathbf{e}_k \cdot \hat{\mathbf{e}}_k}   
    {|\mathbf{e}_k| |\hat{\mathbf{e}}_k|}) \label{eq-sim} \\
    \mathcal{L}_k = -\frac{1}{N} \sum_{i=1}^N (t_i \cdot \mathrm{log}(y_{i,k})+(1-t_i) \cdot \mathrm{log}(1-y_{i,k})) \label{eq-loss}
\end{gather}
where $\mathbf{e}_k$ and $\hat{\mathbf{e}}_k$ are original and contextual entity embeddings respectively;
$\mathbf{W}_k^r = \mathbf{W}_k \mathbf{r}$ denotes contextual transformation matrix which is obtained by matrix multiplication of weight matrix $\mathbf{W}_k$ and relation vectors $\mathbf{r}$ while $\mathbf{b}_k$ is a bias vector;
$\sigma$ is sigmoid function and $\mathbf{y}_k = [y_{1,k},y_{2,k},...,y_{N,k}]$ is final prediction of modality $k$.

\subsection{Decision Fusion}
Existing multimodal approaches mainly focus on projecting different modality representations into a unified space and predicting with commonality between modalities, which will fail to preserve the modality-specific knowledge.
We alleviate this problem in the decision fusion stage by joint learning and combining predictions of different modalities to further leverage the complementarity.

Under the multimodal settings, we assign different contextual relational models for each modality and utilize their own results for training in different views.
Recall the contrastive learning loss in Equation~\eqref{eq-cl}, the overall training objective is to minimize the joint loss shown in Equation~\eqref{eq-mmloss}:
\begin{equation}\label{eq-mmloss}
    \mathcal{L}_{Joint} = \gamma_s \mathcal{L}_s + \gamma_v \mathcal{L}_v + \gamma_t \mathcal{L}_t + \gamma_m \mathcal{L}_{m} + \mathcal{L}_{CL}
\end{equation}
where $\mathcal{L}_k$ denotes binary cross entropy loss for modality $k$ as Equation~\eqref{eq-loss} and $\gamma_k$ is a learned weight parameter.

\begin{algorithm}[t]
\caption{Optimization Algorithm.}\label{alg:optim}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Multimodal Knowledge Graph $\mathcal{G}$
\STATE \textbf{Output:} Trained Model $\mathcal{M}$
\STATE Pre-train structural encoder GAT on $\mathcal{G}$ with the loss in Equation(1)
\STATE Obtain pre-trained visual encoder VGG16 and textual encoder BERT-base
\STATE Initialize the entity embeddings $\mathbf{E}_s, \mathbf{E}_v, \mathbf{E}_t$ in $\mathcal{M}$ with the outputs of pre-trained encoders
\WHILE{not converge}
    \STATE Sample a batch of entities from $\mathcal{G}$
    \FOR{Entity $e$ in batch}
    \STATE Obtain the structural, visual, textual embeddings $\mathbf{e}_s, \mathbf{e}_v, \mathbf{e}_t$ of entity $e$
    \STATE Compute the multimodal fused embeddings $\mathbf{e}_m$ of entity $e$ with Equation (4)
    \STATE Compute the contrastive learning loss $\mathcal{L}_{CL}$ with Equation (6)
    \STATE Compute the loss $\mathcal{L}_s, \mathcal{L}_v, \mathcal{L}_t, \mathcal{L}_m$ with modality-specific scorers via Equation (7) - Equation (9)
    \STATE Compute the joint loss $\mathcal{L}_{Joint}$ with the above losses $\mathcal{L}_s, \mathcal{L}_v, \mathcal{L}_t, \mathcal{L}_m, \mathcal{L}_{CL}$ via Equation (10)
    \STATE Update model parameters of $\mathcal{M}$ by minimizing $\mathcal{L}_{Joint}$
    \ENDFOR
\ENDWHILE
\RETURN $\mathcal{M}$
\end{algorithmic}
\end{algorithm}

To better illustrate the whole training process of \name, we describe it via the pseudo-code of the optimization algorithm.
As shown in Algorithm~\ref{alg:optim}, we first obtain the pre-trained encoders of structural, visual and textual and utilize them for entity embeddings (line 3-5).
Since the pre-trained models are much larger and more complex than \name, they are not fine-tuned and their outputs are directly used as inputs of \name.
The multimodal embeddings are obtained by multimodal fusion while contrastive learning is applied to further enhance the fusion stage (line 9-11).
During training, each modality delivers its own prediction and loss via the modality-specific scorers (line 12), and then the joint prediction and loss are computed based on all modalities including multimodal ones (line 14).

For inference, we propose to jointly consider the predictions of each modality as well as multimodal ones.
Specifically, the overall predictions are shown in Equation~\eqref{eq-df}:
\begin{equation}\label{eq-df}
    \mathbf{y}_{Joint} = \frac{\gamma_s \mathbf{y}_s + \gamma_v \mathbf{y}_v + \gamma_t \mathbf{y}_t + \gamma_m \mathbf{y}_m} {\gamma_s + \gamma_v + \gamma_t + \gamma_m}
\end{equation}
where $\gamma_k$ denotes weight for modality $k$ as same as Equation~\eqref{eq-mmloss} while the values in $\mathbf{y}$ are in [0, 1].


