\section{Experimental Setup}

\subsection{Datasets}
In this paper, we use four public datasets to evaluate our model.
All the datasets consist of three modalities: structural triples, entity images and entity descriptions.
DB15K, FB15K and YAGO15K datasets are obtained from MMKG\footnote{https://github.com/nle-ml}~\cite{DBLP:conf/esws/LiuLGNOR19}, which is a collection of multimodal knowledge graph.
Specifically, we utilize the relational triples as structural features, entity images as visual features and we extract the entity descriptions from Wikidata~\cite{DBLP:journals/cacm/VrandecicK14} as textual features.
FB15K-237\footnote{https://www.microsoft.com/en-us/download/details.aspx?id=52312}~\cite{Toutanova2015ObservedVL} is a subset of FB15K, the visual and textual features in FB15K can be directly reused.
Each dataset is split with 70\%, 10\% and 20\% for training, validation and test.
The detailed statistics are shown in Table~\ref{tbl:datasets}.

In the process of evaluation, we consider four metrics of valid entities to measure the model performance following previous studies: (1) mean rank (MR); (2) mean reciprocal rank (MRR); (3) hits ratio (Hits@1 and Hits@10).

\begin{table}[t]
  \centering
  \scalebox{1}{
    \begin{tabular}{c|ccccc}
      \toprule
      \textbf{Datasets} & \textbf{\#Ent.} & \textbf{\#Rel.} & \textbf{\#Train} & \textbf{\#Valid} & \textbf{\#Test} \\
      \midrule
        DB15K & 14,777 & 279 & 69,319 & 9,903 & 19,806\\
        FB15K & 14,951 & 1,345 & 414,549 & 59,221 & 118,443\\
        YAGO15K & 15,283 & 32 & 86,020 & 12,289 & 24,577\\
        FB15K-237 & 14,541 & 237 & 272,115 & 17,535 & 20,466\\
      \bottomrule
    \end{tabular}
  }
  \caption{Statistics of datasets.}\label{tbl:datasets}
\end{table}

\begin{table*}[ht]
	\centering
	\scalebox{1}{
		\begin{tabular}{ccccc|cccc|cccc}
			\toprule
			& \multicolumn{4}{c}{DB15K} & \multicolumn{4}{c}{FB15K} & \multicolumn{4}{c}{YAGO15K}  \\
			& MR & MRR & H@1 & H@10 & MR & MRR & H@1 & H@10 & MR & MRR & H@1 & H@10 \\
			\midrule
			TransE & 1128 & 0.256 & 13.7 & 46.9 & 108 & 0.495 & 43.7 & 77.4 & 971 & 0.161 & 5.1 & 38.4 \\
			ConvE & 729 & 0.312 & 21.9 & 50.7 & 64 & 0.745 & 67.0 & 87.3 & 714 & 0.267 & 16.8 & 42.6 \\
			TuckER & 693 & 0.341 & 24.3 & 53.8 & 40 & 0.795 & 74.1 & 89.2 & 689 & 0.281 & 18.3 & 45.7\\
			\midrule
			IKRL & 984 & 0.222 & 11.1 & 42.6 & 83 & 0.594 & 48.4 & 76.8 & 854 & 0.139 & 4.8 & 31.7 \\
			MKGC & 981 & 0.208 & 10.8 & 41.9 & 79 & 0.601 & 49.2 & 77.1 & 939 & 0.129 & 4.1 & 29.7 \\
			MKBE & 747 & 0.332 & 23.5 & 51.3 & 48 & 0.783 & 70.4 & 87.8 & 633 & 0.273 & 17.5 & 42.3\\
			\midrule
			IMF (w/o MF) & 687 & 0.319 & 21.8 & 51.2 & 62 & 0.752 & 69.2 & 86.6 & 764 & 0.213 & 11.4 & 35.3\\ 
			IMF (w/o DF) & 541 & 0.443 & 38.1 & 57.3 & 51 & 0.791 & 73.9 & 90.1 & 527 & 0.297 & 21.3 & 46.3 \\ 
			IMF (w/o CL) & 483 & 0.481 & 42.3 & 59.9 & 29 & 0.833 & 78.1 & 90.8 & 501 & 0.289 & 20.5 & 45.9\\
			IMF & \textbf{478*} & \textbf{0.485*} & \textbf{42.7*} & \textbf{60.4*} & \textbf{27*} & \textbf{0.837*} & \textbf{78.5*} & \textbf{91.4*} & \textbf{488*} & \textbf{0.345*} & \textbf{27.6*} & \textbf{49.0*}\\ 
			\bottomrule
		\end{tabular}
	}
	\caption{Evaluation results on multimodal DB15K, FB15K and YAGO15K datasets from MMKG. ``\textbf{{\large *}}'' indicates the statistically significant improvements (i.e., two-sided t-test with $p<0.05$) over the best baseline.}\label{tbl:MMKG}
\end{table*}

\subsection{Baselines}
To demonstrate the effectiveness of our model, we choose two types of methods for comparison, which are monomodal methods and multimodal methods.

For monomodal models, we take the baselines including:
\begin{itemize}[leftmargin=*]
    \item \textbf{TransE}~\cite{DBLP:conf/nips/BordesUGWY13} defines relations as transformations between entities and designs an \emph{energy function} of relational triples as scoring function.
    \item \textbf{ConvE}~\cite{DBLP:conf/aaai/DettmersMS018} converts 1D entity and relation embeddings into 2D embeddings and utilizes Convolutional Neural Network (CNN) to model the interactions between entities and relations.
    \item \textbf{ConvKB}~\cite{DBLP:conf/naacl/NguyenNNP18} employs CNN on the concatenated embeddings of relational triples to compute the triple scores.
    \item \textbf{CapsE}~\cite{DBLP:conf/cikm/NguyenNNP20} utilizes Capsule Network~\cite{DBLP:conf/nips/SabourFH17} to capture the complex interactions between entities and relations for prediction.
    \item \textbf{RotatE}~\cite{DBLP:conf/iclr/SunDNT19} introduces rotation operations between entities to represent relations in the complex space to infer symmetry, antisymmetry, inversion and composition relation patterns.
    \item \textbf{QuatE}~\cite{DBLP:conf/nips/0007TYL19} extends rotation of the knowledge graph embeddings in the complex space into the quaternion space to obtain more degree of freedom.
    \item \textbf{KBAT}~\cite{DBLP:conf/acl/NathaniCSK19} leverages Graph Attention Network (GAT)~\cite{DBLP:conf/iclr/VelickovicCCRLB18} as encoder to aggregate neighbors and employs ConvKB as decoder to compute triple scores.
    \item \textbf{TuckER}~\cite{DBLP:conf/emnlp/BalazevicAH19} applies \textit{Tucker} decomposition to capture the high-level interactions between entity and relation embeddings.
    \item \textbf{HAKE}~\cite{DBLP:conf/aaai/ZhangCZW20} projects entities into polar coordinate system to model hierachical structures for incorporating semantics.
\end{itemize}

For multimodal models, we take the baselines including:
\begin{itemize}[leftmargin=*]
    \item \textbf{IKRL}~\cite{DBLP:conf/ijcai/XieLLS17} utilizes the TransE energy function as scoring function on each pair of modalities for joint prediction.
    \item \textbf{MKGC}~\cite{DBLP:conf/starsem/SergiehBGR18} extends IKRL with combination of different modalities to explicitly deliver alignment between modalities.
    \item \textbf{MKBE}~\cite{DBLP:conf/emnlp/PezeshkpourC018} employs DistMult~\cite{DBLP:journals/corr/YangYHGD14a} as scoring function and designs Generative Adversarial Network (GAN)~\cite{DBLP:conf/nips/GoodfellowPMXWOCB14} to predict missing modalities.
\end{itemize}

For the ablation study, we design three variants of \name: \name (w/o MF) utilizes only structural information; \name (w/o DF) simply takes multimodal representations for training and inference without decision fusion; \name (w/o CL) removes the contrastive learning loss.

\subsection{Implementation Details}

The experiments are implemented on the server with an Intel Xeon E5-2640 CPU, a 188GB RAM and four NVIDIA GeForce RTX 2080Ti GPUs using PyTorch 1.6.0. 
The model parameters are initialized with Xavier initialization and are optimized using Adam~\cite{DBLP:journals/corr/KingmaB14} optimizer.
The evaluation is conducted under the \textbf{R}\textbf{\footnotesize{ONDOM}} settings~\cite{DBLP:conf/acl/SunVSTY20}, where the correct triples are placed randomly in test set and the negative sampling are correctly employed without test leakage.

For DB15K, FB15K and YAGO15K, we obtain the results by running all the baselines with their released codes.
For FB15K-237, we directly obtain the results of TransE, ConvE, ConvKB, CapsE, RotatE, KBAT and TuckER from the re-evaluation work~\cite{DBLP:conf/acl/SunVSTY20} and run the models of QuatE, HAKE, IKRL, MKGC and MKBE with their released codes.

Note that the methods with other enhancing techniques, such as data augmentation~\cite{DBLP:conf/uai/BamlerSM19,zheng2022cbr,zheng2022ddr,li2022gromov} or AutoML~\cite{DBLP:conf/icde/ZhangYDC20,zhaok2021autoemb,zhao2021autoloss,lin2022adafs,wang2022autofield} are orthogonal to our approach for comparison. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{fig/Decision}
    \caption{Visualization of prediction scores in decision fusion. }
    \label{fig:DF}
\end{figure*}



