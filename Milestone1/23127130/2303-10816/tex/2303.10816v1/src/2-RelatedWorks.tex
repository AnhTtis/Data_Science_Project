\section{Related Work}

\subsection{Knowledge Embedding Methods}

Knowledge embedding methods have been widely used in graph representation learning tasks and have achieved great success on knowledge base completion (a.k.a link prediction).
Translation-based methods aim at finding the transformation relationships from source to target. 
TransE~\cite{DBLP:conf/nips/BordesUGWY13}, the most representative translation-based model, projects entities and relations into a unified vector space and minimizes the \emph{energy function} of triples. 
Following this route, many translation-based methods have emerged.
TransH~\cite{DBLP:conf/aaai/WangZFC14} formulates the translating process on relation-specific hyperplanes.
TransR~\cite{DBLP:conf/aaai/LinLSLZ15} projects entities and relations into separate spaces.

Recently, some neural network methods have shown promising results in this task. 
ConvE~\cite{DBLP:conf/aaai/DettmersMS018} and ConvKB~\cite{DBLP:conf/naacl/NguyenNNP18} utilize Convolutional Neural Network (CNN) to increase parameter interaction between entities and relations. 
KBAT~\cite{DBLP:conf/acl/NathaniCSK19} employ Graph Neural Networks (GNN) as the encoder to aggregate multi-hop neighborhood information.

However, all these methods above utilize only structural information, which is not sufficient for more complicated situations in real world.
By incorporating multimodal information in the training process, our approach is able to improve the representations with external knowledge.

\subsection{Multimodal Methods}

Leveraging multimodal information has yielded extraordinary results in many NLP tasks~\cite{DBLP:conf/iccv/Ben-younesCCT17}.
DeViSE~\cite{DBLP:conf/nips/FromeCSBDRM13} and Imagined~\cite{DBLP:conf/aaai/CollellZM17} propose to integrate multimodal information with modality projecting which learns a mapping from one modality to another.
FiLM~\cite{DBLP:conf/aaai/PerezSVDC18} extends cross-modal attention mechanism to extract textual-attentive features in visual models.
MuRel~\cite{DBLP:conf/cvpr/CadeneBCT19} utilizes pair-wise bilinear interaction between modalities and regions to fully capture the complementarity.
IKRL~\cite{DBLP:conf/ijcai/XieLLS17} is the first attempt at multimodal knowledge representation learning, which utilizes image data of the entities as extra information based on TransE.
MKGC~\cite{DBLP:conf/starsem/SergiehBGR18} combines textual and visual features extracted by domain-specific models as additional multimodal information compared to IKRL.
MKBE~\cite{DBLP:conf/emnlp/PezeshkpourC018} creates multimodal knowledge graphs by adding images, descriptions and attributes, and employs DistMult~\cite{DBLP:journals/corr/YangYHGD14a} as scoring function. 

Although these approaches did incorporate multimodal information to improve performance, they cannot take full advantage of it as they fail to effectively model interactions between modalities.
