{
    "arxiv_id": "2303.10816",
    "paper_title": "IMF: Interactive Multimodal Fusion Model for Link Prediction",
    "authors": [
        "Xinhang Li",
        "Xiangyu Zhao",
        "Jiaxing Xu",
        "Yong Zhang",
        "Chunxiao Xing"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.AI",
        "cs.IR"
    ],
    "abstract": "Link prediction aims to identify potential missing triples in knowledge graphs. To get better results, some recent studies have introduced multimodal information to link prediction. However, these methods utilize multimodal information separately and neglect the complicated interaction between different modalities. In this paper, we aim at better modeling the inter-modality information and thus introduce a novel Interactive Multimodal Fusion (IMF) model to integrate knowledge from different modalities. To this end, we propose a two-stage multimodal fusion framework to preserve modality-specific knowledge as well as take advantage of the complementarity between different modalities. Instead of directly projecting different modalities into a unified space, our multimodal fusion module limits the representations of different modalities independent while leverages bilinear pooling for fusion and incorporates contrastive learning as additional constraints. Furthermore, the decision fusion module delivers the learned weighted average over the predictions of all modalities to better incorporate the complementarity of different modalities. Our approach has been demonstrated to be effective through empirical evaluations on several real-world datasets. The implementation code is available online at https://github.com/HestiaSky/IMF-Pytorch.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10816v1"
    ],
    "publication_venue": "9 pages, 7 figures, 4 tables, WWW'2023",
    "doi": "10.1145/3543507.3583554"
}