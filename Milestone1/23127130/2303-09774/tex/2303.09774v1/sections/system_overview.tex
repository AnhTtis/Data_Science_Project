\section{\system: System Overview}
\label{sec:overview}
\subfile{plots/fig_architecture}

\system is a system such that, given a list of materialized view (MV) definitions, 
it refreshes all the MVs (or more accurately, it generates data for those MVs).
This section overviews how \system performs this operation: 
\cref{sec:inputs} describes \system's inputs,
\cref{sec:arch} introduces \system's subcomponents and their roles and
\cref{sec:job_representation} overviews the execution of an MV refresh run.

\subsection{Workload Specification}
\label{sec:inputs}

A list of MVs to update (and their dependencies) are formally expressed
using a \emph{dependency graph}.
Then, \emph{execution metadata}
records additional information about those MVs
    (e.g., the size on disk)
useful for optimization.

\mypara{Dependency Graph}

The set of MVs to refresh is to be provided to \system in the form of a dependency graph (\Cref{fig:input}, left). 
Each node corresponds to a single MV update, and edges correspond to dependencies between MVs.
Each node contains a SQL statement for a specific MV update.


\mypara{Execution Metadata} 

\system's optimization requires information from DBMS-side SQL executions from past MV refresh runs, namely (1) the estimated size of the output table from executing the SQL statement in each node and (2) the estimated time savings of keeping a node output in memory (see \cref{sec:problem_setup}).


\subsection{Internal Architecture}
\label{sec:arch}

\system's Controller coordinates MV updates according to
    the \emph{plan}---order of updates and in-memory caching---created 
        by Optimizer (see \Cref{fig:system_overview}).

%  illustrates the subcomponents of \system's architecture.
% \system is implemented as a Python frontend over a Presto DBMS:

\mypara{Controller}

Controller directs the order of node (i.e., MV) materialization and how/where to store the output of individual nodes (i.e., keep in memory vs. materialize on external storage\footnote{Our implementation of \system uses NFS for storage. This can be substituted with alternative materialization locations.}) according to the directions provided by the Optimizer (see below). The directions are compiled into a corresponding SQL statement and sent to the DBMS for execution.
The Controller additionally manages the materialization of intermediate tables kept in memory. (\Cref{sec:job_representation})

\mypara{Optimizer}

The Optimizer computes the MV refresh order (e.g., MV1, MV2, MV3) and nodes to keep in memory (\cref{fig:input}, right) for the Controller using the metadata gathered from SQL executions in the DBMS and our proposed algorithm \optimizationproblem (described in \Cref{sec:problem_setup,sec:joint_optimization}).

\mypara{DBMS}

The DBMS executes SQL statements from Controller (e.g., joins, aggregation). Input tables are from either \memory (described shortly) or read from external storage (e.g., disk).
Our current implementation uses a Presto DBMS cluster~\cite{prestosql}, but any DBMS may be used in its place.

\subsection{Performing an MV Refresh Run}
\subfile{plots/fig_mv_refresh}
\label{sec:job_representation}
During an MV refresh run, Controller directs the DBMS to execute nodes (i.e. running their SQL statements) one by one according to the execution order computed by the Optimizer.

\mypara{Memory Management} 
\system ensures that the size of nodes kept in memory (called the \memory) is bounded correctly.
Let $n$ be a node that is to be kept in memory according to the Optimizer's plan: $n$ is created directly in the \memory, and freed from the \memory as soon as all the nodes depending on $n$ complete their execution.

\mypara{Parallelizing Compute and Materialization} 
\system aims to maximize the usage of compute and I/O bandwidth of a DBMS.
Nodes created in memory are materialized to external storage immediately after their creation;
this materialization process occurs in parallel with the execution of downstream nodes.
% The materialization manager in Controller sends materialization requests in parallel to the DBMS for tables kept in memory (i.e. \texttt{SELECT * INTO x FROM memory.x}).
    
% Controller alters the SQL statements in nodes to ensure the following 2 properties w.r.t. read and write:\begin{itemize}
%     \item If a table is to be kept in memory, it is created in extra memory, and copied to external storage in parallel with downstream node executions. The in-memory table is read by dependent node executions, and deleted when all its dependencies have been refreshed.
%     \item An input table is read from extra memory if it has been kept and from external storage if otherwise.
% \end{itemize}

\mypara{Example}
Based on \cref{fig:input}, we present a concrete example in \cref{fig:mv_refresh} to illustrate how \system coordinates MV refreshes while maintaining the correct usage of the \memory and parallelizing I/O and compute.
The MVs are refreshed in the order of \texttt{MV1}, \texttt{MV2}, \texttt{MV3} according to the timeline below; \texttt{MV1} is the only MV to be kept in memory:\begin{itemize}
    \item \textbf{t1}: \texttt{MV1} is created in the \memory.
    \item \textbf{t2}: \texttt{MV1} is fully created in memory. It is materialized to disk concurrently while refreshing \texttt{MV2} (using the newly created \texttt{MV1}); \texttt{MV2} will be directly created on disk.
    \item \textbf{t3}: \texttt{MV2} is now fully refreshed. the refresh of \texttt{MV3} starts on storage reading from \texttt{MV1} in memory.
    \item \textbf{t4}: \texttt{MV3} is fully refreshed and \texttt{MV1} is fully materialized. \texttt{MV1} in memory is deleted, concluding the MV refresh run.
\end{itemize}




% \system uses Presto's memory catalog~\cite{prestomemory} as extra memory for keeping in-memory tables.
% If managed properly (i.e. in-memory tables are dropped as soon as downstream computations are completed), the plan produced by the optimizer guarantees the total size of the tables kept in extra memory is under the bounded amount.

% \system controls a bounded amount of extra memory for keeping computed node outputs;
% Controller sends a drop table statement to the DBMS for a table kept in extra memory when it is no longer needed in downstream computations (i.e., all its downstream nodes have been computed).
