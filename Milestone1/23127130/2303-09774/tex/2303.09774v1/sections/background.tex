%%%
\section{Materialization in Modern Data Warehouses}
\label{sec:background}

Provisioning materialized views (i.e., data materialization)
is effective in speeding up OLAP queries.
In this section, we describe significant occurrences of data materialization in modern data warehouses and the I/O overhead we observe.


\subsection{Acceleration by Data Materialization}

Modern data warehouses manage massive volumes of data.
Horizontal scaling might not be cost-efficient in reducing the latencies of analytical queries.
For a better cost-benefit trade-off, data materialization is frequently employed.


\mypara{Large Data, Complex Queries}

Real-world data warehouses manage a large volume of data.
According to an analysis of 1.54 billion queries and 1.7 million tables across 40 different
% \footnote{This analysis include 1.54 billion queries and .} 
data warehouse accounts~\cite{aleyasen2022overcoming, aleyasen2022intelligent},
12.1\% of tables are larger than 3.625 TB, and 29.1\% of the tables are between 725 GB and 3.625 TB.
Most analytical queries include data-intensive, time-consuming operations:
31\% of \texttt{select} queries are reported to include at least one join.
32\% of time is spent on queries longer than 10 minutes.
According to another analysis by Tableau~\cite{vogelsgesang2018get}, machine-generated queries are often complex
due to the impedance mismatch between BI-driven analysis and SQL-based expressions.
Generally, a large amount of data and costly operations lead to longer query latencies.


While horizontal scaling~\cite{spark-sql,prestosql,shvachko2010hadoop} and aggressive caching~\cite{li2014tachyon}
could mitigate the issue to some extent, they are not budget-friendly solutions.
Specifically, to benefit from caching, users must pay more to keep clusters up and running even when query workloads are relatively lightweight~\cite{gupta2015amazon}.
To save cost, one could elastically scale clusters
according to their query workloads; however, this approach increases 
the chance of cache misses, requiring data transfers over the network~\cite{dageville2016snowflake}. Fetching data over the network is slower than
reading from local storage.
Tiered storage layers~\cite{wu2019autoscaling,li2019cloud,herodotou2021trident} might be a more cost-effective solution to keep data access fast;
however, quickly fetching terabytes of data remains challenging.

% Along with the ever-growing size of tables in modern data warehouses - 50\% of the data in data warehouses residing in tables of greater than 460GB in size, the size of intermediate tables created by recurring jobs grow alongside these base tables which they retrieve data from: Almost $50\%$ of intermediate tables in monitored workloads are larger than 2GB in size~\cite{aleyasen2022overcoming}.


\input{plots/fig_sec2_workload_breakdown}

\mypara{Materialization for Faster Querying}

Data materialization can reduce query latencies by precomputing costly operations in advance
and using the results.
MVs have been studied extensively in the literature.
The topics include efficient refresh/updates of MVs~\cite{ahmad2012dbtoaster,mcsherry2013differential,armbrust2013generalized,zeng2016iolap},
recommending useful MVs for observed query workloads~\cite{shukla1998materialized,agrawal2000automated,park2017database},
optimizing queries using MVs~\cite{chaudhuri1995optimizing,calvanese2012view},
approximate aggregation~\cite{heule2013hyperloglog,flajolet2007hyperloglog,rong2020approximate}, etc.
While sketching~\cite{bigquery} and sampling~\cite{zeng2016iolap} can also be
considered as a special case of MVs, we do not discuss them.

Automated management of materialized data is gaining interest.
Napa~\cite{agiwal2021napa} enables both high-throughput data ingestion and fast query speed using MVs.
Keebo~\cite{keebo} automates materialized view creation by mining from query workloads.
Also, there are tools that can help users manually define data materialization strategies; 
we describe a few commonly used tools in the following section.

\subsection{Data Materialization: Significant Fraction}

Real-world data warehouses spend much time materializing data.
By optimizing MV refresh, we can reduce 
infrastructure costs and data staleness.
Managing complex MVs
is becoming easier due to newly available tools providing
templating languages, visualizations, and web-based interfaces.

\input{plots/fig_sec2_overhead}

\mypara{Heavily Used in Warehouses}

Data materialization makes up a significant fraction of 
modern data warehouse workloads.
\cref{fig:overview:workload} shows statistics across ten independent
workloads~\cite{aleyasen2022overcoming}.
Based on query runtime, data materialization (e.g., DDL, DML) takes 2\%-38\% of each workload.
Interestingly, in \texttt{W6}, a warehouse spends 2.2$\times$ 
 time on materialization than analytics.

We have also spoken with several data engineers
at big and small companies about their data ingestion and preparation pipelines.
They are customers of cloud-managed data warehouses.
In their environments, periodic (e.g., daily) data ingestion (using tools like Airflow~\cite{airflow}, Fivetran~\cite{fivetran}, etc.)
is followed by several hours of recurring in-warehouse operations for
creating and updating dependent tables.\footnote{MVs are not currently first-choice because popular data warehouses like Amazon Redshift, BigQuery, and Snowflake do not support MVs with joins.}
Dashboards are set up to use these precomputed tables to serve user-facing queries (instead of computing all the way from base tables); 
thus, end-users can see results more quickly (e.g., when they change filtering conditions).
Data materialization is mostly managed via web-based tools 
(rather than hand-rewritten DDL/DML statements sent directly to warehouses), 
which we will describe shortly.

\mypara{Tools for Materialization}

With high-level admin tools, 
managing a graph of MVs has become easier.
First, Looker offers a data modeling language, LookML~\cite{lookml}. With LookML, users can express complex joins and nested structures.
Based on LookML, Looker provides a feature called PDT~\cite{looker-pdt} to 
store views as materialized tables. 
Once built, Looker uses materialized tables in its generated queries.
Second, dbt~\cite{dbt} is a data modeling tool based on nested table/view definitions. Since a definition can reference other definitions, the overall structure forms a directed acyclic graph where nodes represent table/view definitions, and edges represent the dependencies among those definitions.
dbt then uses a topological sort to determine the order of table updates.
Note that while these tools offer convenient ways of constructing
intermediate tables, all the operations are eventually compiled into SQL
and sent to connected warehouses; data warehouses then perform the
actual computation and data manipulation.
Thus, what we discuss in this paper---speeding up warehouses' internal
operations---is orthogonal to what these tools offer.

\subfile{plots/fig_input}
\subsection{Overhead of Data Materialization}

Reading/writing data is often a significant bottleneck in data systems.
% is often a significant bottleneck in transforming data. 
That is, 
% persisting data 
% in a special format (e.g., Parquet~\cite{apache-parquet,google-dremel}, ORC~\cite{apache-orc}) 
% can take a large portion
% in performing data materialization via a series of SQL statements.
much of 
% the end-to-end time for 
data transformation/materialization 
% while executing SQL statements 
is spent on persisting data.
% In this section, we briefly discuss this ongoing challenge.

First, read/write often carries a large overhead (compared to compute) in modern data warehouses.
To study this, we measured how much time a system spends on 
read/write operations (e.g., serialization, compression) relative to compute operations (e.g., join, filtering).
Using an (anonymous) commercial data warehouse managed by its vendor,
we ran CTAS statements containing three inner joins (or equivalently, four joined tables) which appear in TPC-H query \#8---joins of \texttt{customer}, \texttt{orders}, \texttt{lineitem}, and \texttt{nation}.
\Cref{fig:overview:overhead} shows the result for different sizes of datasets, i.e., from scale factor 1 (or 1 GB) to scale factor 1000 (or 1000 GB). On the X-axis, a dataset size is accompanied by a respective total runtime (i.e., 5.4 seconds for the 1 GB dataset).
According to this study, writing joined results into persistent storage (which could include compression, serialization, and network I/O) took 37\%--69\% of the total runtime (of each statement).

Second, read/write also takes significant time for open-source data systems.
We compared a few different implementations for Apache Parquet (e.g., C++ Arrow~\cite{arrow-cpp}, Rust Arrow~\cite{arrow-rust}), 
a widely used columnar data format for data warehouses~\cite{spark-sql,prestosql,hive,athena,snowflake-parquet}. 
In our environment, we could achieve the best serialization speed with Rust Arrow compiled from its source.
Under this setup, we tested the overhead of read/write in creating intermediate tables for TPC-DS queries (see \cref{sec:exp:setup}).
Overall, read/write took 85\% of the time spent on compute operations (e.g., join, filtering, etc.).
% Note that Parquet is used by open-source data warehouses~\cite{spark-sql,presto,hive} as well as commercial systems~\cite{aws-athena,snowflake-parquet}.

% Finally, this limitation in read/write speed (in comparison to compute) is likely to be
% an ongoing challenge in the future.
% While the architecture/storage communities propose to offload the cost of data serialization to specialized accelerators,
