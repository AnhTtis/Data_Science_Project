\section{Related work}
% There is extensive recent work on speeding up analytical queries due to the need for consistent execution times in the face of the explosive growth in the volume of available data.
% In this section, we divide existing work into two categories: maintaining data freshness in MVs (\Cref{sec:server_side}) and optimizations for minimizing ad-hoc query latency (\Cref{sec:client_side}).

% \subsection{Maintaining Data Freshness in MVs}
% \label{sec:server_side}
% There exists a variety of data warehousing applications aimed at supporting low-latency analytical queries on fresh data.
% In particular, these applications require efficiency in the propagation of newly ingested data into downstream MVs.
 
\mypara{Efficient MV Refresh}
Incremental view maintenance (IVM) aims to update MVs to reflect newly ingested data, taking advantage of already computed results to perform the update in a manner more efficient than computing from scratch (full refresh)
~\cite{ahmad2012dbtoaster,mcsherry2013differential,armbrust2013generalized,zeng2016iolap, palpanas2002incremental, griffin1995incremental, agiwal2021napa, braun2015analytics}. 
There is an abundance of work in IVM, including incremental updates on duplicate values~\cite{griffin1995incremental}, non-distributive aggregate functions~\cite{palpanas2002incremental}, higher-order views~\cite{ahmad2012dbtoaster}, and sliding windows~\cite{braun2015analytics}. 
More recent works also investigate the scalability aspect of IVM, proposing scale-independent updates~\cite{armbrust2013generalized} and sampled views~\cite{zeng2016iolap}. Since \system is applicable to arbitrary SQL statements, \system is orthogonal to and is fully compatible with existing IVM techniques.

\mypara{MV Refresh Scheduling}
There exist works on scheduling the refresh of a MV set focusing on resolving cyclic dependencies~\cite{folkert2005optimizing}, minimizing weighted average staleness~\cite{golab2009scheduling}, and prioritizing MVs with the highest speedups on predicted future queries~\cite{ahmed2020automated}.
\system's scheduling to speed up the end-to-end refresh of the MV set is not addressed in existing works.

\mypara{DAG Workflow Scheduling}
The execution of workloads consisting of individual jobs with acyclic dependencies is a well-studied topic~\cite{apacheoozie,sparkdag,marchal2018parallel,bathie2020revisiting,baruah2022ilp}; many of these techniques can be applied to MV refresh runs studied in this paper.
Existing workflow scheduling systems such as Apache Oozie~\cite{apacheoozie}, Apache Airflow~\cite{airflow}, and Spark DAG scheduler~\cite{sparkdag} automate the execution of user-defined workflows following a topological order.
There are recent works aimed at finding more optimal execution orders in terms of peak memory usage~\cite{marchal2018parallel, bathie2020revisiting} and execution time on parallel platforms~\cite{baruah2022ilp}.
While \system is designed for use with MV refresh runs/workloads, our technique on joint scheduling and optimization can be reasonably applied to general workloads as a possible future direction.

% \paragraph{Incremental MV indexing}
% Update-optimized indices such as the log-structured merge-trees (LSM)~\cite{o1996log} are used for indexing MVs due to frequent updates induced by data ingestion~\cite{gupta2016mesa,agiwal2021napa}.
% \system is orthogonal to indexing: \system is capable of efficiently performing MV refresh runs regardless of whether the individual MVs are indexed or not.

% \subsection{Ad-hoc Query Latency Reduction}
% \label{sec:client_side}

% The minimization of ad-hoc analytical query response times is a well-studied topic due to latency being negatively correlated with the productivity of a data analyst during a data analysis session~\cite{liu2014effects}.
% Sessions are commonly conducted within visualization systems that contain a variety of optimization techniques to ensure that query response times fall within a certain latency tolerance.

% \mypara{Data prefetching}
% Data is often loaded into memory on a by-need basis in visualization systems to minimize interference with user-issued query computations~\cite{mani2017effective,xin2021enhancing,galakatos2017revisiting, yan2020auto, battle2016dynamic, crotty2016case, jalaparti2018netco}. 
% Query-time data retrieval can be significantly expedited by anticipating the data usage of the user in future queries and pre-loading the data into memory during the downtime between user queries (`think time'). SMART~\cite{mani2017effective} prefetches data for modified versions of current user-issued queries with different filters and dimensions. A-WARE~\cite{crotty2016case} maintains a sample store constantly refined through ingesting data based on speculations of future plots.
% ForeCache~\cite{battle2016dynamic} uses an SVM to predict the user's current analysis phase and accordingly prefetches data tiles partitioned based on different numerical values. NetCo predicts future queries via log analysis, and solves an ILP formulation to prefetch data to maximize the number of SLO-meeting queries~\cite{jalaparti2018netco}.
% In the case of MV refresh workloads, `think time' is nonexistent as individual MVs are refreshed back-to-back, rendering data prefetching techniques non-applicable.

\mypara{Intermediate Data Caching}
Some existing data visualization systems cache user-defined variables to support the typical incremental construction of data visualizations~\cite{zgraggen2016progressive, eichmann2020idebench} during data analysis sessions~\cite{jupyter, rstudio, colab}. 
Recent work proposes a management scheme for these cached variables under a memory constraint that greedily keeps variables with the highest estimated time savings based on predicted future user behavior via neural networks~\cite{xin2021enhancing}.
While useful for data visualization, a greedy approach to memory management fails to achieve satisfactory results compared to \system.

\mypara{Intermediate Result Reuse}

There exist works on storing intermediate results from computations to speedup future computations~\cite{yang2018intermediate, dursun2017revisiting, nagel2013recycling, michiardi2019memory, galakatos2017revisiting}.
Studied topics include the identification of reuse opportunities by finding overlaps in computation graphs of successive jobs~\cite{yang2018intermediate, michiardi2019memory},
selective storage under a space constraint with heuristics such as reuse probability~\cite{dursun2017revisiting}, expected savings~\cite{yang2018intermediate}, and recompute-storage cost difference~\cite{nagel2013recycling},
and rewriting incoming jobs to take advantage of stored intermediates~\cite{galakatos2017revisiting}.
These works share similarity with \system in their selection of items to store under a memory constraint, however, \system's problem setting requires it to uniquely consider the joint (re)ordering of job executions along with the selection of items.

% work that considers both job execution (re)order as well as intermediate result caching with a bounded amount of memory. but notably lack the joint aspect of \system and cannot be used to achieve immediate speedup on an incoming MV refresh run if no intermediates are stored beforehand. 

\mypara{Incremental Query Processing} Incremental processing (IQP) is useful for cases where not all data required for a query is immediately available. Similar to online aggregation~\cite{hellerstein1997online}, initial results of a query are computed on a subset of required data and progressively refined as the rest of the required data arrives in a predictable pattern~\cite{tang2019intermittent,wangtempura}. Tang et al. propose a dynamic programming formulation to pick intermediate states to store in memory given a limited memory budget~\cite{tang2019intermittent}. Tempura rewrites the query plan for more efficient execution based on predicted data arrival patterns~\cite{wangtempura}. While similarities exist between the problem setting of IQP and \system, such as management of bounded memory, \system notably includes additional joint optimization for the order of MV updates.

% \paragraph{Sampling}
% Sampling has seen wide use in visualization systems for reducing the computation time of ad-hoc queries by computing an approximate result over a subset of data as exact results are not always required by the user~\cite{crotty2016case, mani2017effective, zgraggen2014panoramicdata, kraska2021northstar, galakatos2017revisiting, kandula2016quickr}. 
% Commonly studied topics in sampling for ad-hoc queries include complex query sampling~\cite{kandula2016quickr}, rare event aggregation~\cite{kraska2021northstar, galakatos2017revisiting}, and maintaining consistency between related sampled visualizations~\cite{zgraggen2014panoramicdata}.
% Sampling server-side at the MV level compromises the assumptions of downstream applications and is thus not considered in \system.

% \paragraph{Progressive visualization}
% The latency tolerance for time-consuming queries can be circumvented by presenting a partially-computed visualization to the user within the tolerance, which is then incrementally refined until it is fully accurate~\cite{rahman2017ve, zgraggen2016progressive, crotty2015vizdom, kraska2021northstar, kamat2017infiniviz}.
% Example plots which benefit from progressive visualization include bar charts~\cite{kamat2017infiniviz} and heatmaps~\cite{rahman2017ve}.
% Similar to sampling, study on this topic is orthogonal to \system as pushing out partially-updated MVs compromises downstream assumptions.