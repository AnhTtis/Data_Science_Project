@article{abi2022gengnn,
  title={GenGNN: A Generic FPGA Framework for Graph Neural Network Acceleration},
  author={Abi-Karam, Stefan and He, Yuqi and Sarkar, Rishov and Sathidevi, Lakshmi and Qiao, Zihang and Hao, Cong},
  journal={arXiv preprint arXiv:2201.08475},
  year={2022}
}


@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and others},
  booktitle={ICML},
  year={2017},
}

@inproceedings{fan2019graph,
  title={Graph neural networks for social recommendation},
  author={Fan, Wenqi and others},
  booktitle={The World Wide Web Conference},
  pages={417--426},
  year={2019}
}

@article{guo2020deep,
  title={A deep graph neural network-based mechanism for social recommendations},
  author={Guo, Zhiwei and Wang, Heng},
  journal={IEEE Transactions on Industrial Informatics},
  volume={17},
  number={4},
  pages={2776--2783},
  year={2020},
  publisher={IEEE}
}



@inproceedings{li2020customized,
  title={A customized graph neural network model for guiding analog {IC} placement},
  author={Li, Yaguang and others},
  booktitle={IEEE/ACM International Conference On Computer Aided Design (ICCAD)},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhang2019circuit,
  title={{Circuit-GNN}: Graph neural networks for distributed circuit design},
  author={Zhang, Guo and others},
  booktitle={International Conference on Machine Learning},
  pages={7364--7373},
  year={2019}
}


@inproceedings{wu2021ironman,
  title={{IronMan}: {GNN}-assisted Design Space Exploration in High-Level Synthesis via Reinforcement Learning},
  author={Wu, Nan and others},
  booktitle={GLSVLSI},
  year={2021}
}


@inproceedings{mendis2019ithemal,
  title={Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks},
  author={Mendis, Charith and others},
  booktitle={International Conference on machine learning},
  pages={4505--4515},
  year={2019},
  organization={PMLR}
}


@book{wolf2012computers,
  title={Computers as components: principles of embedded computing system design},
  author={Wolf, Marilyn},
  year={2012},
  publisher={Elsevier}
}

@article{coussy2009introduction,
  title={An introduction to high-level synthesis},
  author={Coussy, Philippe and others},
  journal={IEEE Design \& Test of Computers},
  volume={26},
  number={4},
  pages={8--17},
  year={2009},
  publisher={IEEE}
}


@inproceedings{wang2020gcn,
  title={{GCN-RL} circuit designer: Transferable transistor sizing with graph neural networks and reinforcement learning},
  author={Wang, Hanrui and others},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}


@inproceedings{gao2021layout,
  title={Layout Symmetry Annotation for Analog Circuits with Graph Neural Networks},
  author={Gao, Xiaohan and others},
  booktitle={Proceedings of the 26th Asia and South Pacific Design Automation Conference},
  pages={152--157},
  year={2021}
}



@inproceedings{mirhoseini2020chip,
  title={Chip placement with deep reinforcement learning},
  author={Mirhoseini, Azalia and others},
  booktitle={arXiv preprint arXiv:2004.10746},
  year={2020}
}


@inproceedings{chen2021universal,
  title={Universal symmetry constraint extraction for analog and mixed-signal circuits with graph neural networks},
  author={Chen, Hao and others},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2021},
  organization={IEEE}
}



@phdthesis{andersen1994program,
  title={Program analysis and specialization for the {C} programming language},
  author={Andersen, Lars Ole},
  year={1994},
  school={Citeseer}
}

@book{nielson2004principles,
  title={Principles of program analysis},
  author={Nielson, Flemming and others},
  year={2004},
  publisher={Springer Science \& Business Media}
}



@inproceedings{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and others},
  booktitle={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@inproceedings{Fey/Lenssen/2019,
  title={Fast graph representation learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan Eric},
  booktitle={arXiv preprint arXiv:1903.02428},
  year={2019}
}

@inproceedings{bianchi2021graph,
  title={Graph neural networks with convolutional arma filters},
  author={Bianchi, Filippo Maria and others},
  booktitle={TPAMI},
  year={2021},
  publisher={IEEE}
}

@inproceedings{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{xu2018powerful,
  title={How powerful are graph neural networks?},
  author={Xu, Keyulu and others},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{corso2020principal,
  title={Principal Neighbourhood Aggregation for Graph Nets},
  author={Corso, Gabriele and others},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{ma2020path,
  title={Path integral based convolution and pooling for graph neural networks},
  author={Ma, Zheng and others},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{gao2019graph,
  title={Graph u-nets},
  author={Gao, Hongyang and Ji, Shuiwang},
  booktitle={ICML},
  year={2019},
}

@inproceedings{wu2019simplifying,
  title={Simplifying graph convolutional networks},
  author={Wu, Felix and others},
  booktitle={ICML},
  year={2019}
}

@inproceedings{brockschmidt2020gnn,
  title={Gnn-film: Graph neural networks with feature-wise linear modulation},
  author={Brockschmidt, Marc},
  booktitle={ICML},
  year={2020},
}

@inproceedings{schlichtkrull2018modeling,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and others},
  booktitle={ESWC},
  year={2018},
  organization={Springer}
}

@inproceedings{li2015gated,
  title={Gated graph sequence neural networks},
  author={Li, Yujia and others},
  booktitle={ICLR},
  year={2016}
}


@article{aho2007compilers,
  title={Compilers: Principles, Techniques and Tools},
  author={Aho, A and others},
  year={2007},
  journal={Addison wesley},
  publisher={Addison-Wesley 2nd edition}
}


@book{hennessy2011computer,
  title={Computer architecture: a quantitative approach},
  author={Hennessy, John L and Patterson, David A},
  year={2011},
  publisher={Elsevier}
}


@misc{llvm,
  key = {llvm},
  title = {{The LLVM Compiler Infrastructure}},
  howpublished = "\url{https://llvm.org/}",
  note = "[Online; accessed 17-May-2021]"
}

@misc{gcc,
  key = {gcc},
  title = {{The GNU Compiler Collection}},
  howpublished = "\url{https://gcc.gnu.org/}",
  note = "[Online; accessed 17-May-2021]"
}

@article{kirchner2015frama,
  title={{Frama-C}: A software analysis perspective},
  author={Kirchner, Florent and others},
  journal={Formal Aspects of Computing},
  volume={27},
  number={3},
  pages={573--609},
  year={2015},
  publisher={Springer}
}

@inproceedings{cuoq2012frama,
  title={Frama-c},
  author={Cuoq, Pascal and others},
  booktitle={SEFM},
  year={2012},
  organization={Springer}
}

@inproceedings{barany2017liveness,
  title={Liveness-driven random program generation},
  author={Barany, Gerg{\"o}},
  booktitle={LOPSTR},
  year={2017},
  organization={Springer}
}

@inproceedings{reagen2014machsuite,
  author = {Brandon Reagen and others},
  title = {{MachSuite}: Benchmarks for Accelerator Design and Customized Architectures},
  booktitle = {IISWC},
  year = {2014},
}

@misc{PolyBench,
  title  = "Polyhedral Benchmark suite",
  author={Pouchet, Louis-No{\"e}l and Yuki, Tomofumi},
  year={2016},
  howpublished  = "\url{http://web.cs.ucla.edu/~pouchet/software/polybench/}"
}

@article{hara2009proposal,
  title={Proposal and quantitative analysis of the {CHStone} benchmark program suite for practical C-based high-level synthesis},
  author={Hara, Yuko and others},
  journal={JIP},
  year={2009},
  publisher={Information Processing Society of Japan}
}


@misc{CPython,
  key = {Cpython},
  title  = "CPython",
  howpublished  = "\url{https://www.python.org/}",
  note = "[Online; accessed 17-May-2021]"
}


@manual{Vitis,
  author={Xilinx},
  year = {Accessed: 2021},
  title  = "Xilinx Vitis unified software platform",
  note   = "\url{https://www.xilinx.com/products/design-tools/vitis.html}"
}

@manual{VitisHLS,
  author={Vitis},
  title  = "Vitis High-Level Synthesis User Guide (UG1399)",
  note   = "\url{https://docs.xilinx.com/r/en-US/ug1399-vitis-hls}",
  year = {Accessed: 2021}
}



@article{khailany2020accelerating,
  title={Accelerating Chip Design with Machine Learning},
  author={Khailany, Brucek and others},
  journal={IEEE Micro},
  year={2020},
  publisher={IEEE}
}

@inproceedings{maron2019provably,
  title={Provably Powerful Graph Networks},
  author={Maron, Haggai and others},
  booktitle={NeurIPS},
  year={2019}
}

@article{zhao2019performance,
  title={Performance modeling and directives optimization for high-level synthesis on FPGA},
  author={Zhao, Jieru and others},
  journal={TCAD},
  year={2019},
  publisher={IEEE}
}

@inproceedings{perina2019lina,
  title={Lina: Timing-Constrained High-Level Synthesis Performance Estimator for Fast DSE},
  author={Perina, Andr{\'e} Bannwart and others},
  booktitle={ICFPT},
  year={2019},
  organization={IEEE}
}

@inproceedings{makrani2019pyramid,
  title={Pyramid: Machine Learning Framework to Estimate the Optimal Timing and Resource Usage of a High-Level Synthesis Design},
  author={Makrani, Hosein M. and others},
  booktitle={FPL},
  year={2019},
}

@inproceedings{ustun2020accurate,
  title={Accurate operation delay prediction for FPGA HLS using graph neural networks},
  author={Ustun, Ecenur and others},
  booktitle={ICCAD},
  year={2020}
}

@inproceedings{zhao2017comba,
  title={COMBA: A comprehensive model-based analysis framework for high level synthesis of real applications},
  author={Zhao, Jieru and others},
  booktitle={ICCAD},
  year={2017},
}

@inproceedings{makrani2019xppe,
  title={XPPE: cross-platform performance estimation of hardware accelerators using machine learning},
  author={Makrani, Hosein M. and others},
  booktitle={ASPDAC},
  year={2019}
}


@inproceedings{o2018hlspredict,
  title={Hlspredict: Cross platform performance prediction for fpga high-level synthesis},
  author={O'Neal, Kenneth and others},
  booktitle={ICCAD},
  year={2018}
}

@inproceedings{hu2020ogb,
 author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {22118--22133},
 publisher = {Curran Associates, Inc.},
 title = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
 url = {https://proceedings.neurips.cc/paper/2020/file/fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{zhang2020hardware,
  title={Hardware acceleration of large scale {GCN} inference},
  author={Zhang, Bingyi and Zeng, Hanqing and Prasanna, Viktor},
  booktitle={2020 IEEE 31st International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
  pages={61--68},
  year={2020},
  organization={IEEE}
}

@inproceedings{yan2020hygcn,
  title={{HyGCN}: A {GCN} accelerator with hybrid architecture},
  author={Yan, Mingyu and Deng, Lei and Hu, Xing and Liang, Ling and Feng, Yujing and Ye, Xiaochun and Zhang, Zhimin and Fan, Dongrui and Xie, Yuan},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={15--29},
  year={2020},
  organization={IEEE}
}

@inproceedings{auten2020hardware,
  title={Hardware acceleration of graph neural networks},
  author={Auten, Adam and Tomei, Matthew and Kumar, Rakesh},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@inproceedings{zhang2021boostgcn,
  title={{BoostGCN}: A Framework for Optimizing {GCN} Inference on {FPGA}},
  author={Zhang, Bingyi and Kannan, Rajgopal and Prasanna, Viktor},
  booktitle={2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={29--39},
  year={2021},
  organization={IEEE}
}


@inproceedings{beani2021directional,
  title={Directional graph networks},
  author={Beani, Dominique and Passaro, Saro and L{\'e}tourneau, Vincent and Hamilton, Will and Corso, Gabriele and Li{\`o}, Pietro},
  booktitle={International Conference on Machine Learning},
  pages={748--758},
  year={2021},
  organization={PMLR}
}


@article{atz2021geometric,
  title={Geometric deep learning on molecular representations},
  author={Atz, Kenneth and Grisoni, Francesca and Schneider, Gisbert},
  journal={Nature Machine Intelligence},
  pages={1--10},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{szklarczyk2019string,
  title={{STRING} v11: protein--protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets},
  author={Szklarczyk, Damian and Gable, Annika L and Lyon, David and Junge, Alexander and Wyder, Stefan and Huerta-Cepas, Jaime and Simonovic, Milan and Doncheva, Nadezhda T and Morris, John H and Bork, Peer and others},
  journal={Nucleic acids research},
  volume={47},
  number={D1},
  pages={D607--D613},
  year={2019},
  publisher={Oxford University Press}
}


@article{wishart2018drugbank,
  title={{DrugBank} 5.0: a major update to the {DrugBank} database for 2018},
  author={Wishart, David S and Feunang, Yannick D and Guo, An C and Lo, Elvis J and Marcu, Ana and Grant, Jason R and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat and others},
  journal={Nucleic acids research},
  volume={46},
  number={D1},
  pages={D1074--D1082},
  year={2018},
  publisher={Oxford University Press}
}

@article{wu2018moleculenet,
  title={{MoleculeNet}: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@inproceedings{shi2020point,
  title={{Point-GNN}: Graph neural network for {3D} object detection in a point cloud},
  author={Shi, Weijing and Rajkumar, Raj},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1711--1719},
  year={2020}
}

@article{shlomi2020graph,
  title={Graph neural networks in particle physics},
  author={Shlomi, Jonathan and Battaglia, Peter and Vlimant, Jean-Roch},
  journal={Machine Learning: Science and Technology},
  volume={2},
  number={2},
  pages={021001},
  year={2020},
  publisher={IOP Publishing}
}

@article{cerminara2020distance,
  title={DISTANCE-WEIGHTED GRAPH NEURAL NETWORKS ON {FPGA}S FOR REAL-TIME PARTICLE RECONSTRUCTION AT THE {L}ARGE {H}ADRON {C}OLLIDER},
  author={Cerminara, Gianluca and Gupta, Abhijay and Iiyama, Yutaro and Kieseler, Jan and Loncar, Vladimir and Ngadiuba, Jennifer and Pierini, Maurizio and Rieger, Marcel and Summers, Sioni and Van Onsem, Gerrit and others},
  year={2020}
}

@article{iiyama2021distance,
  title={Distance-weighted graph neural networks on {FPGAs} for real-time particle reconstruction in high energy physics},
  author={Iiyama, Yutaro and Cerminara, Gianluca and Gupta, Abhijay and Kieseler, Jan and Loncar, Vladimir and Pierini, Maurizio and Qasim, Shah Rukh and Rieger, Marcel and Summers, Sioni and Van Onsem, Gerrit and others},
  journal={Frontiers in big Data},
  pages={44},
  year={2021},
  publisher={Frontiers}
}

@INPROCEEDINGS{Li2021meloppr,
  author={Li, Lixiang and Chen, Yao and Zirnheld, Zacharie and Li, Pan and Hao, Cong},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)}, 
  title={{MELOPPR}: Software/Hardware Co-design for Memory-efficient Low-latency Personalized {PageRank}},
  year={2021},
  volume={},
  number={},
  pages={601-606},
  doi={10.1109/DAC18074.2021.9586129}}
  

@inproceedings{jia2020redundancy,
  title={Redundancy-Free Computation for Graph Neural Networks},
  author={Jia, Zhihao and Lin, Sina and Ying, Rex and You, Jiaxuan and Leskovec, Jure and Aiken, Alex},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={997--1005},
  year={2020}
}
@article{wang2020gnnadvisor,
  title={{GNNAdvisor}: An efficient runtime system for {GNN} acceleration on {GPU}s},
  author={Wang, Yuke and Feng, Boyuan and Li, Gushu and Li, Shuangchen and Deng, Lei and Xie, Yuan and Ding, Yufei},
  journal={arXiv preprint arXiv:2006.06608},
  year={2020}
}

@article{chen2021rubik,
  title={Rubik: A Hierarchical Architecture for Efficient Graph Neural Network Training},
  author={Chen, Xiaobing and Wang, Yuke and Xie, Xinfeng and Hu, Xing and Basak, Abanti and Liang, Ling and Yan, Mingyu and Deng, Lei and Ding, Yufei and Du, Zidong and others},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year={2021},
  publisher={IEEE}
}

@inproceedings{geng2020awb,
  title={{AWB-GCN}: A graph convolutional network accelerator with runtime workload rebalancing},
  author={Geng, Tong and Li, Ang and Shi, Runbin and Wu, Chunshu and Wang, Tianqi and Li, Yanfei and Haghi, Pouya and Tumeo, Antonino and Che, Shuai and Reinhardt, Steve and others},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={922--936},
  year={2020},
  organization={IEEE}
}

@article{ishiguro2019graph,
  title={Graph warp module: an auxiliary module for boosting the power of graph neural networks},
  author={Ishiguro, Katsuhiko and Maeda, Shin-ichi and Koyama, Masanori},
  journal={arXiv preprint arXiv:1902.01020},
  year={2019}
}

@inproceedings{xue2021node,
  title={Node Augmentation Methods for Graph Neural Network based Object Classification},
  author={Xue, Yifan and Liao, Yixuan and Chen, Xiaoxin and Zhao, Jingwei},
  booktitle={2021 2nd International Conference on Computing and Data Science (CDS)},
  pages={556--561},
  year={2021},
  organization={IEEE}
}

@article{pham2017graph,
  title={Graph classification via deep learning with virtual nodes},
  author={Pham, Trang and Tran, Truyen and Dam, Hoa and Venkatesh, Svetha},
  journal={arXiv preprint arXiv:1708.04357},
  year={2017}
}

@article{sen2008collective,
  title={Collective classification in network data},
  author={Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina},
  journal={AI magazine},
  volume={29},
  number={3},
  pages={93--93},
  year={2008}
}

@inproceedings{namata2012query,
  title={Query-driven active surveying for collective classification},
  author={Namata, Galileo and London, Ben and Getoor, Lise and Huang, Bert and EDU, UMD},
  booktitle={10th International Workshop on Mining and Learning with Graphs},
  volume={8},
  pages={1},
  year={2012}
}

@misc{ogb-models,
  key = {ogb-models},
  title = {{GNN models from Open Graph Benchmark}},
  howpublished = "\url{https://github.com/snap-stanford/ogb/tree/master/examples/graphproppred/mol}",
  note = "[Online; accessed 16-Jan-2022]"
}


@misc{pyg,
  key = {pyg},
  title = {{PyTorch Geometric}},
  howpublished = "\url{https://pytorch-geometric.readthedocs.io/en/latest/}",
  note = "[Online; accessed 16-Jan-2022]"
}

@inproceedings{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={1025--1035},
  year={2017}
}

@inproceedings{li2021gcnax,
  author={Li, Jiajun and Louri, Ahmed and Karanth, Avinash and Bunescu, Razvan},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 
  title={{GCNAX}: A flexible and energy-efficient accelerator for graph convolutional neural networks}, 
  year={2021},
  volume={},
  number={},
  pages={775-788},
  doi={10.1109/HPCA51647.2021.00070}
}

@article{abadal2021computing,
author = {Abadal, Sergi and Jain, Akshay and Guirado, Robert and L\'{o}pez-Alonso, Jorge and Alarc\'{o}n, Eduard},
title = {Computing graph neural networks: A survey from algorithms to accelerators},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3477141},
doi = {10.1145/3477141},
abstract = {Graph Neural Networks (GNNs) have exploded onto the machine learning scene in recent years owing to their capability to model and learn from graph-structured data. Such an ability has strong implications in a wide variety of fields whose data are inherently relational, for which conventional neural networks do not perform well. Indeed, as recent reviews can attest, research in the area of GNNs has grown rapidly and has lead to the development of a variety of GNN algorithm variants as well as to the exploration of ground-breaking applications in chemistry, neurology, electronics, or communication networks, among others. At the current stage research, however, the efficient processing of GNNs is still an open challenge for several reasons. Besides of their novelty, GNNs are hard to compute due to their dependence on the input graph, their combination of dense and very sparse operations, or the need to scale to huge graphs in some applications. In this context, this article aims to make two main contributions. On the one hand, a review of the field of GNNs is presented from the perspective of computing. This includes a brief tutorial on the GNN fundamentals, an overview of the evolution of the field in the last decade, and a summary of operations carried out in the multiple phases of different GNN algorithm variants. On the other hand, an in-depth analysis of current software and hardware acceleration schemes is provided, from which a hardware-software, graph-aware, and communication-centric vision for GNN accelerators is distilled.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {191},
numpages = {38},
keywords = {GNN algorithms, graph embeddings, Graph neural networks, accelerators}
}

@article{liang2021engn,
  author={Liang, Shengwen and Wang, Ying and Liu, Cheng and He, Lei and LI, Huawei and Xu, Dawen and Li, Xiaowei},
  journal={IEEE Transactions on Computers}, 
  title={{EnGN}: A high-throughput and energy-efficient accelerator for large graph neural networks}, 
  year={2021},
  volume={70},
  number={9},
  pages={1511-1525},
  doi={10.1109/TC.2020.3014632}}

@inproceedings{kiningham2020greta,
  author    = "Kevin Kiningham and Philip Levis and Christopher Re",
  title     = "{GReTA: Hardware optimized graph processing for GNNs}",
  booktitle = "{Proceedings of the Workshop on Resource-Constrained Machine Learning (ReCoML 2020)}",
  year      = {2020},
  month     = {March}
}

@misc{kiningham2020grip,
      title={{GRIP}: A graph neural network accelerator architecture}, 
      author={Kevin Kiningham and Christopher Re and Philip Levis},
      year={2020},
      eprint={2007.13828},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@inproceedings{zeng2020graphact,
author = {Zeng, Hanqing and Prasanna, Viktor},
title = {{GraphACT}: Accelerating {GCN} training on {CPU-FPGA} heterogeneous platforms},
year = {2020},
isbn = {9781450370998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373087.3375312},
doi = {10.1145/3373087.3375312},
abstract = {Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art deep learning model for representation learning on graphs. It is challenging to accelerate training of GCNs, due to (1) substantial and irregular data communication to propagate information within the graph, and (2) intensive computation to propagate information along the neural network layers. To address these challenges, we design a novel accelerator for training GCNs on CPU-FPGA heterogeneous systems, by incorporating multiple algorithm-architecture co-optimizations. We first analyze the computation and communication characteristics of various GCN training algorithms, and select a subgraph-based algorithm that is well suited for hardware execution. To optimize the feature propagation within subgraphs, we propose a light-weight pre-processing step based on a graph theoretic approach. Such pre-processing performed on the CPU significantly reduces the memory access requirements and the computation to be performed on the FPGA. To accelerate the weight update in GCN layers, we propose a systolic array based design for efficient parallelization. We integrate the above optimizations into a complete hardware pipeline, and analyze its load-balance and resource utilization by accurate performance modeling. We evaluate our design on a Xilinx Alveo U200 board hosted by a 40-core Xeon server. On three large graphs, we achieve an order of magnitude training speedup with negligible accuracy loss, compared with state-of-the-art implementation on a multi-core platform.},
booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {255â€“265},
numpages = {11},
keywords = {heterogeneous computing, fpga, graph convolutional network},
location = {Seaside, CA, USA},
series = {FPGA '20}
}

@article{abcnet,
  title = {{{ABCNet}}: An Attention-Based Method for Particle Tagging},
  shorttitle = {{{ABCNet}}},
  author = {Mikuni, V. and Canelli, F.},
  year = {2020},
  month = jun,
  journal = {The European Physical Journal Plus},
  volume = {135},
  number = {6},
  pages = {463},
  issn = {2190-5444},
  doi = {10.1140/epjp/s13360-020-00497-3},
  abstract = {In high energy physics, graph-based implementations have the advantage of treating the input data sets in a similar way as they are collected by collider experiments. To expand on this concept, we propose a graph neural network enhanced by attention mechanisms called ABCNet. To exemplify the advantages and flexibility of treating collider data as a point cloud, two physically motivated problems are investigated: quark\textendash gluon discrimination and pileup reduction. The former is an event-by-event classification, while the latter requires each reconstructed particle to receive a classification score. For both tasks, ABCNet shows an improved performance compared to other algorithms available.},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\IBV9TB67\\Mikuni and Canelli - 2020 - ABCNet an attention-based method for particle tag.pdf}
}

@article{elabd2022graph,
  title = {Graph Neural Networks for Charged Particle Tracking on {{FPGAs}}},
  author = {Elabd, Abdelrahman and Razavimaleki, Vesal and Huang, Shi-Yu and Duarte, Javier and Atkinson, Markus and DeZoort, Gage and Elmer, Peter and Hauck, Scott and Hu, Jin-Xuan and Hsu, Shih-Chieh and Lai, Bo-Cheng and Neubauer, Mark and Ojalvo, Isobel and Thais, Savannah and Trahms, Matthew},
  year = {2022},
  month = mar,
  journal = {Frontiers in Big Data},
  volume = {5},
  pages = {828666},
  issn = {2624-909X},
  doi = {10.3389/fdata.2022.828666},
  abstract = {The determination of charged particle trajectories in collisions at the CERN Large Hadron Collider (LHC) is an important but challenging problem, especially in the high interaction density conditions expected during the future high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a type of geometric deep learning algorithm that has successfully been applied to this task by embedding tracker data as a graph\textemdash nodes represent hits, while edges represent possible track segments\textemdash and classifying the edges as true or fake track segments. However, their study in hardware- or software-based trigger applications has been limited due to their large computational cost. In this paper, we introduce an automated translation workflow, integrated into a broader tool called               hls4ml               , for converting GNNs into firmware for field-programmable gate arrays (FPGAs). We use this translation tool to implement GNNs for charged particle tracking, trained using the TrackML challenge dataset, on FPGAs with designs targeting different graph sizes, task complexites, and latency/throughput requirements. This work could enable the inclusion of charged particle tracking GNNs at the trigger level for HL-LHC experiments.},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\SRDDIL9U\\Elabd et al. - 2022 - Graph Neural Networks for Charged Particle Trackin.pdf}
}

@article{qu2020jet,
  title = {Jet Tagging via Particle Clouds},
  author = {Qu, Huilin and Gouskos, Loukas},
  year = {2020},
  month = mar,
  journal = {Physical Review D},
  volume = {101},
  number = {5},
  pages = {056019},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevD.101.056019},
  abstract = {How to represent a jet is at the core of machine learning on jet physics. Inspired by the notion of point clouds, we propose a new approach that considers a jet as an unordered set of its constituent particles, effectively a ``particle cloud.'' Such a particle cloud representation of jets is efficient in incorporating raw information of jets and also explicitly respects the permutation symmetry. Based on the particle cloud representation, we propose ParticleNet, a customized neural network architecture using Dynamic Graph Convolutional Neural Network for jet tagging problems. The ParticleNet architecture achieves state-of-the-art performance on two representative jet tagging benchmarks and is improved significantly over existing methods.},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\TMEP9P5I\\Qu and Gouskos - 2020 - Jet tagging via particle clouds.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\ARUZB9XQ\\PhysRevD.101.html}
}


@inproceedings{streamgcn,
  title = {{{StreamGCN}}: Accelerating Graph Convolutional Networks with Streaming Processing},
  shorttitle = {{{StreamGCN}}},
  booktitle = {2022 {{IEEE Custom Integrated Circuits Conference}} ({{CICC}})},
  author = {Sohrabizadeh, Atefeh and Chi, Yuze and Cong, Jason},
  year = {2022},
  month = apr,
  pages = {1--8},
  issn = {2152-3630},
  doi = {10.1109/CICC53496.2022.9772832},
  abstract = {While there have been many studies on hardware acceleration for deep learning on images, there has been a rather limited focus on accelerating deep learning applications involving graphs. The unique characteristics of graphs, such as the irregular memory access and dynamic parallelism, impose several challenges when the algorithm is mapped to a CPU or GPU. To address these challenges while exploiting all the available sparsity, we propose a flexible architecture called StreamGCN for accelerating Graph Convolutional Networks (GCN), the core computation unit in deep learning algorithms on graphs. The architecture is specialized for streaming processing of many small graphs for graph search and similarity computation. The experimental results demonstrate that StreamGCN can deliver a high speedup compared to a multi-core CPU and a GPU implementation, showing the efficiency of our design.},
  keywords = {Computer architecture,Deep learning,Graphics processing units,Heuristic algorithms,Parallel processing,Pipelines,Real-time systems},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\3KCP5NGR\\Sohrabizadeh et al. - 2022 - StreamGCN Accelerating Graph Convolutional Networ.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\ZN794GUJ\\9772832.html}
}

@article{groqinc2020challenge,
  title = {The Challenge of Batch Size 1: {{Groq}} Adds Responsiveness to Inference Performance},
  author = {{Groq, Inc.}},
  year = {2020},
  pages = {7},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\7SY5UHRP\\2020 - The Challenge of Batch Size 1 Groq Adds Responsiv.pdf}
}

@inproceedings{gcod,
  title = {{{GCoD}}: Graph Convolutional Network Acceleration via Dedicated Algorithm and Accelerator Co-Design},
  shorttitle = {{{GCoD}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  author = {You, Haoran and Geng, Tong and Zhang, Yongan and Li, Ang and Lin, Yingyan},
  year = {2022},
  month = apr,
  pages = {460--474},
  publisher = {{IEEE Computer Society}},
  doi = {10.1109/HPCA53966.2022.00041},
  abstract = {Graph Convolutional Networks (GCNs) have emerged as the state-of-the-art graph learning model. However, it can be notoriously challenging to inference GCNs over large graph datasets, limiting their application to large real-world graphs and hindering the exploration of deeper and more sophisticated GCN graphs. This is because real-world graphs can be extremely large and sparse. Furthermore, the node degree of GCNs tends to follow the power-law distribution and therefore have highly irregular adjacency matrices, resulting in prohibitive inefficiencies in both data processing and movement and thus substantially limiting the achievable GCN acceleration efficiency. To this end, this paper proposes a GCN algorithm and accelerator Co-Design framework dubbed GCoD which can largely alleviate the aforementioned GCN irregularity and boost GCNs\&\#x2019; inference efficiency. Specifically, on the algorithm level, GCoD integrates a split and conquer GCN training strategy that polarizes the graphs to be either denser or sparser in local neighborhoods without compromising the model accuracy, resulting in graph adjacency matrices that (mostly) have merely two levels of workload and enjoys largely enhanced regularity and thus ease of acceleration. On the hardware level, we further develop a dedicated two-pronged accelerator with a separated engine to process each of the aforementioned denser and sparser workloads, further boosting the overall utilization and acceleration efficiency. Extensive experiments and ablation studies validate that our GCoD consistently reduces the number of off-chip accesses, leading to speedups 15286\&\#x00D7;, 294\&\#x00D7;, 7.8\&\#x00D7;, and 2.5\&\#x00D7; as compared to CPUs, GPUs, and prior-art GCN accelerators including HyGCN and AWB-GCN, respectively, while maintaining or even improving the task accuracy. Additionally, we visualize GCoD trained graph adjacency matrices for a better understanding of its advantages.},
  isbn = {978-1-66542-027-3},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\WPTE87U5\\You et al. - 2022 - GCoD Graph Convolutional Network Acceleration via.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\LMEHJA7P\\1Ds0gzzMhH2.html}
}

@inproceedings{huang2022accelerating,
  title = {Accelerating Graph Convolutional Networks Using Crossbar-Based Processing-in-Memory Architectures},
  booktitle = {2022 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  author = {Huang, Yu and Zheng, Long and Yao, Pengcheng and Wang, Qinggang and Liao, Xiaofei and Jin, Hai and Xue, Jingling},
  year = {2022},
  month = apr,
  pages = {1029--1042},
  issn = {2378-203X},
  doi = {10.1109/HPCA53966.2022.00079},
  abstract = {Graph convolutional networks (GCNs) are promising to enable machine learning on graphs. GCNs exhibit mixed computational kernels, involving regular neural-network-like computing and irregular graph-analytics-like processing. Existing GCN accelerators obey a divide-and-conquer philosophy to architect two separate types of hardware to accelerate these two types of GCN kernels, respectively. This hybrid architecture improves intra-kernel efficiency but considers little inter-kernel interactions in a holistic view for improving overall efficiency.In this paper, we present a new GCN accelerator, RE-FLIP, with three key innovations in terms of architecture design, algorithm mappings, and practical implementations. First, ReFlip leverages PIM-featured crossbar architectures to build a unified architecture for supporting the two types of GCN kernels simultaneously. Second, ReFlip adopts novel algorithm mappings that can maximize potential performance gains reaped from the unified architecture by exploiting the massive crossbar-structured parallelism. Third, ReFlip assembles software/hardware co-optimizations to process real-world graphs efficiently. Compared to the state-of-the-art software frameworks running on Intel Xeon E5-2680v4 CPU and NVIDIA Tesla V100 GPU, ReFlip achieves the average speedups of 6,432\texttimes{} and 86.32\texttimes{} and the average energy savings of 9,817\texttimes{} and 302.44\texttimes, respectively. In addition, ReFlip also outperforms a state-of-the-art GCN hardware accelerator, AWB-GCN, by achieving an average speedup of 5.06\texttimes{} and an average energy saving of 15.63\texttimes.},
  keywords = {accelerator,Computer architecture,crossbar architectures,graph convolutional network,Graphics processing units,Parallel processing,Performance gain,Philosophical considerations,processing-in-memory,Software algorithms,Technological innovation},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\RVJDTTY3\\Huang et al. - 2022 - Accelerating Graph Convolutional Networks Using Cr.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\7NBQPXBS\\9773267.html}
}

@inproceedings{regnn,
  title = {{{ReGNN}}: A Redundancy-Eliminated Graph Neural Networks Accelerator},
  shorttitle = {{{ReGNN}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  author = {Chen, Cen and Li, Kenli and Li, Yangfan and Zou, Xiaofeng},
  year = {2022},
  month = apr,
  pages = {429--443},
  issn = {2378-203X},
  doi = {10.1109/HPCA53966.2022.00039},
  abstract = {Graph neural networks (GNNs), which extend conventional deep learning technologies to process graph-structured data, have shown its powerful graph representation learning ability. Existing typical GNNs utilize neighborhood message passing mechanism based on neural networks that updates target vertex representations by aggregating feature messages from neighboring source vertices. To accelerate the computations of GNNs, some customized accelerators, which follow the neighborhood aggregation computation pattern for each vertex, have been proposed. Through analysis, we observe that a naive implementation of the neighborhood aggregation results in redundant computations and communications.In this paper, we propose a novel redundancy-eliminated GNN accelerator, shortly termed as ReGNN. ReGNN is supported by an algorithm and architecture co-design. We first propose a dynamic redundancy-eliminated neighborhood message passing algorithm for GNNs. Then a novel architecture is designed to support the proposed algorithm and transform the redundancy elimination into performance improvement. ReGNN is also a configurable pipelined architecture that can be configured to support different GNN variants. In terms of the same computations, ReGNN provides the same accuracy as traditional GNNs. To the best of our knowledge, ReGNN is the first accelerator that can eliminate computation redundancy in GNNs. Our proposed ReGNN system gains an average of 9.1\texttimes{} speedup and 8.9\texttimes{} energy efficiency over state-of-the-art GNN accelerators.},
  keywords = {Computer architecture,Deep learning,Graph Neural Networks,Hardware Accelerator,Heuristic algorithms,Knowledge engineering,Message passing,Redundancy,Redundancy-Aware Computation,Representation learning,Software-Hardware Co-Design},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\EPNVRQCX\\Chen et al. - 2022 - ReGNN A Redundancy-Eliminated Graph Neural Networ.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\TVDCT9S6\\9773273.html}
}


%%% BEGIN ref.bib used in DoSSA work %%%
% The following BibTeX was generated in Zotero; if changes are needed, let Rishov know so that he doesn't overwrite them on a future export.

@article{abadal,
  title = {Computing Graph Neural Networks: A Survey from Algorithms to Accelerators},
  shorttitle = {Computing Graph Neural Networks},
  author = {Abadal, Sergi and Jain, Akshay and Guirado, Robert and {L{\'o}pez-Alonso}, Jorge and Alarc{\'o}n, Eduard},
  year = {2022},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {9},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3477141},
  abstract = {Graph Neural Networks (GNNs) have exploded onto the machine learning scene in recent years owing to their capability to model and learn from graph-structured data. Such an ability has strong implications in a wide variety of fields whose data are inherently relational, for which conventional neural networks do not perform well. Indeed, as recent reviews can attest, research in the area of GNNs has grown rapidly and has lead to the development of a variety of GNN algorithm variants as well as to the exploration of ground-breaking applications in chemistry, neurology, electronics, or communication networks, among others. At the current stage research, however, the efficient processing of GNNs is still an open challenge for several reasons. Besides of their novelty, GNNs are hard to compute due to their dependence on the input graph, their combination of dense and very sparse operations, or the need to scale to huge graphs in some applications. In this context, this article aims to make two main contributions. On the one hand, a review of the field of GNNs is presented from the perspective of computing. This includes a brief tutorial on the GNN fundamentals, an overview of the evolution of the field in the last decade, and a summary of operations carried out in the multiple phases of different GNN algorithm variants. On the other hand, an in-depth analysis of current software and hardware acceleration schemes is provided, from which a hardware-software, graph-aware, and communication-centric vision for GNN accelerators is distilled.},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\FBBLE5E3\\Abadal et al. - 2022 - Computing Graph Neural Networks A Survey from Alg.pdf}
}

@article{atz,
  title = {Geometric Deep Learning on Molecular Representations},
  author = {Atz, Kenneth and Grisoni, Francesca and Schneider, Gisbert},
  year = {2021},
  month = dec,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {12},
  pages = {1023--1032},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00418-8},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\GKEZA2GI\\Atz et al. - 2021 - Geometric deep learning on molecular representatio.pdf}
}

@inproceedings{auten,
  title = {Hardware Acceleration of Graph Neural Networks},
  booktitle = {2020 57th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Auten, Adam and Tomei, Matthew and Kumar, Rakesh},
  year = {2020},
  month = jul,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1109/DAC18072.2020.9218751},
  isbn = {978-1-72811-085-1}
}

@inproceedings{awbgcn,
  title = {{{AWB-GCN}}: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing},
  shorttitle = {{{AWB-GCN}}},
  booktitle = {2020 53rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Geng, Tong and Li, Ang and Shi, Runbin and Wu, Chunshu and Wang, Tianqi and Li, Yanfei and Haghi, Pouya and Tumeo, Antonino and Che, Shuai and Reinhardt, Steve and Herbordt, Martin C.},
  year = {2020},
  month = oct,
  pages = {922--936},
  publisher = {{IEEE}},
  address = {{Athens, Greece}},
  doi = {10.1109/MICRO50266.2020.00079},
  isbn = {978-1-72817-383-2},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\7ENNE5K3\\Geng et al. - 2020 - AWB-GCN A Graph Convolutional Network Accelerator.pdf}
}


@inproceedings{dgn,
  title = {Directional Graph Networks},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Beaini, Dominique and Passaro, Saro and L{\'e}tourneau, Vincent and Hamilton, Will and Corso, Gabriele and Li{\'o}, Pietro},
  year = {2021},
  month = jul,
  pages = {748--758},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The lack of anisotropic kernels in graph neural networks (GNNs) strongly limits their expressiveness, contributing to well-known issues such as over-smoothing. To overcome this limitation, we propose the first globally consistent anisotropic kernels for GNNs, allowing for graph convolutions that are defined according to topologicaly-derived directional flows. First, by defining a vector field in the graph, we develop a method of applying directional derivatives and smoothing by projecting node-specific messages into the field. Then, we propose the use of the Laplacian eigenvectors as such vector field. We show that the method generalizes CNNs on an nnn-dimensional grid and is provably more discriminative than standard GNNs regarding the Weisfeiler-Lehman 1-WL test. We evaluate our method on different standard benchmarks and see a relative error reduction of 8\% on the CIFAR10 graph dataset and 11\% to 32\% on the molecular ZINC dataset, and a relative increase in precision of 1.6\% on the MolPCBA dataset. An important outcome of this work is that it enables graph networks to embed directions in an unsupervised way, thus allowing a better representation of the anisotropic features in different physical or biological problems.},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\U4BRFNAG\\Beaini et al. - 2021 - Directional Graph Networks.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\VFB3FSCC\\Beaini et al. - 2021 - Directional Graph Networks.pdf}
}

@article{drugbank,
  title = {{{DrugBank}} 5.0: A Major Update to the {{DrugBank}} Database for 2018},
  shorttitle = {{{DrugBank}} 5.0},
  author = {Wishart, David S and Feunang, Yannick D and Guo, An C and Lo, Elvis J and Marcu, Ana and Grant, Jason R and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat and Assempour, Nazanin and Iynkkaran, Ithayavani and Liu, Yifeng and Maciejewski, Adam and Gale, Nicola and Wilson, Alex and Chin, Lucy and Cummings, Ryan and Le, Diana and Pon, Allison and Knox, Craig and Wilson, Michael},
  year = {2018},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {46},
  number = {D1},
  pages = {D1074-D1082},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkx1037},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\VAT4DCB5\\Wishart et al. - 2018 - DrugBank 5.0 a major update to the DrugBank datab.pdf}
}

@article{engn,
  title = {{{EnGN}}: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks},
  shorttitle = {{{EnGN}}},
  author = {Liang, Shengwen and Wang, Ying and Liu, Cheng and He, Lei and Li, Huawei and Xu, Dawen and Li, Xiaowei},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Computers},
  volume = {70},
  number = {9},
  pages = {1511--1525},
  issn = {0018-9340, 1557-9956, 2326-3814},
  doi = {10.1109/TC.2020.3014632},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\PA99CCDQ\\Liang et al. - 2021 - EnGN A High-Throughput and Energy-Efficient Accel.pdf}
}

@inproceedings{gcnax,
  title = {{{GCNAX}}: A Flexible and Energy-Efficient Accelerator for Graph Convolutional Neural Networks},
  shorttitle = {{{GCNAX}}},
  booktitle = {2021 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  author = {Li, Jiajun and Louri, Ahmed and Karanth, Avinash and Bunescu, Razvan},
  year = {2021},
  month = feb,
  pages = {775--788},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/HPCA51647.2021.00070},
  isbn = {978-1-66542-235-2}
}

@inproceedings{graphact,
  title = {{{GraphACT}}: Accelerating {{GCN}} Training on {{CPU-FPGA}} Heterogeneous Platforms},
  shorttitle = {{{GraphACT}}},
  booktitle = {Proceedings of the 2020 {{ACM}}/{{SIGDA International Symposium}} on {{Field-Programmable Gate Arrays}}},
  author = {Zeng, Hanqing and Prasanna, Viktor},
  year = {2020},
  month = feb,
  pages = {255--265},
  publisher = {{ACM}},
  address = {{Seaside CA USA}},
  doi = {10.1145/3373087.3375312},
  isbn = {978-1-4503-7099-8},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\9U3D8MMD\\Zeng and Prasanna - 2020 - GraphACT Accelerating GCN Training on CPU-FPGA He.pdf}
}

@inproceedings{greta,
  title = {{{GReTA}}: Hardware Optimized Graph Processing for {{GNNs}}},
  shorttitle = {{{GReTA}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Resource-Constrained Machine Learning}} ({{ReCoML}} 2020)},
  author = {Kiningham, Kevin and Levis, Philip and R{\'e}, Christopher},
  year = {2020},
  month = mar
}

@article{grip,
  title = {{{GRIP}}: A Graph Neural Network Accelerator Architecture},
  shorttitle = {{{GRIP}}},
  author = {Kiningham, Kevin and Re, Christopher and Levis, Philip},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.13828 [cs]},
  eprint = {2007.13828},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present GRIP, a graph neural network accelerator architecture designed for low-latency inference. AcceleratingGNNs is challenging because they combine two distinct types of computation: arithmetic-intensive vertex-centric operations and memory-intensive edge-centric operations. GRIP splits GNN inference into a fixed set of edge- and vertex-centric execution phases that can be implemented in hardware. We then specialize each unit for the unique computational structure found in each phase.For vertex-centric phases, GRIP uses a high performance matrix multiply engine coupled with a dedicated memory subsystem for weights to improve reuse. For edge-centric phases, GRIP use multiple parallel prefetch and reduction engines to alleviate the irregularity in memory accesses. Finally, GRIP supports severalGNN optimizations, including a novel optimization called vertex-tiling which increases the reuse of weight data.We evaluate GRIP by performing synthesis and place and route for a 28nm implementation capable of executing inference for several widely-used GNN models (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it reduces 99th percentile latency by a geometric mean of 17x and 23x compared to a CPU and GPU baseline, respectively, while drawing only 5W.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Hardware Architecture},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\U7SKB9YD\\Kiningham et al. - 2020 - GRIP A Graph Neural Network Accelerator Architect.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\G4FR9HJW\\2007.html}
}

@inproceedings{hygcn,
  title = {{{HyGCN}}: A {{GCN}} Accelerator with Hybrid Architecture},
  shorttitle = {{{HyGCN}}},
  booktitle = {2020 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Yan, Mingyu and Deng, Lei and Hu, Xing and Liang, Ling and Feng, Yujing and Ye, Xiaochun and Zhang, Zhimin and Fan, Dongrui and Xie, Yuan},
  year = {2020},
  month = feb,
  pages = {15--29},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/HPCA47549.2020.00012},
  isbn = {978-1-72816-149-5},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\HX4VMQDK\\Yan et al. - 2020 - HyGCN A GCN Accelerator with Hybrid Architecture.pdf}
}

@inproceedings{igcn,
  title = {I-{{GCN}}: A Graph Convolutional Network Accelerator with Runtime Locality Enhancement through Islandization},
  shorttitle = {I-{{GCN}}},
  booktitle = {{{MICRO-54}}: 54th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Geng, Tong and Wu, Chunshu and Zhang, Yongan and Tan, Cheng and Xie, Chenhao and You, Haoran and Herbordt, Martin and Lin, Yingyan and Li, Ang},
  year = {2021},
  month = oct,
  pages = {1051--1063},
  publisher = {{ACM}},
  address = {{Virtual Event Greece}},
  doi = {10.1145/3466752.3480113},
  isbn = {978-1-4503-8557-2},
  langid = {english}
}

@article{moleculenet,
  title = {{{MoleculeNet}}: A Benchmark for Molecular Machine Learning},
  shorttitle = {{{MoleculeNet}}},
  author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan~N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
  year = {2018},
  journal = {Chemical Science},
  volume = {9},
  number = {2},
  pages = {513--530},
  issn = {2041-6520, 2041-6539},
  doi = {10.1039/C7SC02664A},
  abstract = {A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.           ,              Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\B5Z7LR6B\\Wu et al. - 2018 - MoleculeNet a benchmark for molecular machine lea.pdf}
}

@article{mpalltheway,
  title = {Message Passing All the Way Up},
  author = {Veli{\v c}kovi{\'c}, Petar},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.11097 [cs, stat]},
  eprint = {2202.11097},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The message passing framework is the foundation of the immense success enjoyed by graph neural networks (GNNs) in recent years. In spite of its elegance, there exist many problems it provably cannot solve over given input graphs. This has led to a surge of research on going "beyond message passing", building GNNs which do not suffer from those limitations -- a term which has become ubiquitous in regular discourse. However, have those methods truly moved beyond message passing? In this position paper, I argue about the dangers of using this term -- especially when teaching graph representation learning to newcomers. I show that any function of interest we want to compute over graphs can, in all likelihood, be expressed using pairwise message passing -- just over a potentially modified graph, and argue how most practical implementations subtly do this kind of trick anyway. Hoping to initiate a productive discussion, I propose replacing "beyond message passing" with a more tame term, "augmented message passing".},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\2EF6DV92\\VeliÄkoviÄ‡ - 2022 - Message passing all the way up.pdf;C\:\\Users\\Rishov\\Zotero\\storage\\7ZFAKT6V\\2202.html}
}

@inproceedings{ogb,
  title = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {22118--22133},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{pna,
  title = {Principal Neighbourhood Aggregation for Graph Nets},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Li{\`o}, Pietro and Veli{\v c}kovi{\'c}, Petar},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {13260--13271},
  publisher = {{Curran Associates, Inc.}}
}

@article{rubik,
  title = {Rubik: A Hierarchical Architecture for Efficient Graph Neural Network Training},
  shorttitle = {Rubik},
  author = {Chen, Xiaobing and Wang, Yuke and Xie, Xinfeng and Hu, Xing and Basak, Abanti and Liang, Ling and Yan, Mingyu and Deng, Lei and Ding, Yufei and Du, Zidong and Xie, Yuan},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {4},
  pages = {936--949},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2021.3079142}
}

@article{string,
  title = {{{STRING}} V11: Protein\textendash Protein Association Networks with Increased Coverage, Supporting Functional Discovery in Genome-Wide Experimental Datasets},
  shorttitle = {{{STRING}} V11},
  author = {Szklarczyk, Damian and Gable, Annika L and Lyon, David and Junge, Alexander and Wyder, Stefan and {Huerta-Cepas}, Jaime and Simonovic, Milan and Doncheva, Nadezhda T and Morris, John H and Bork, Peer and Jensen, Lars J and Mering, Christian~von},
  year = {2019},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {47},
  number = {D1},
  pages = {D607-D613},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gky1131},
  langid = {english},
  file = {C\:\\Users\\Rishov\\Zotero\\storage\\5LER4NJQ\\Szklarczyk et al. - 2019 - STRING v11 proteinâ€“protein association networks w.pdf}
}


@inproceedings{Kepner_graphBLAS_math,
	doi = {10.1109/hpec.2016.7761646},
	url = {https://doi.org/10.1109%2Fhpec.2016.7761646},
	year = 2016,
	month = {sep},
	publisher = {{IEEE}},
	author = {Jeremy Kepner and Henning Meyerhenke and Scott McMillan and Carl Yang and John D. Owens and Marcin Zalewski and Timothy Mattson and Jose Moreira and Peter Aaltonen and David Bader and Aydin Buluc and Franz Franchetti and John Gilbert and Dylan Hutchison and Manoj Kumar and Andrew Lumsdaine},
	title = {Mathematical foundations of the {GraphBLAS}},
	booktitle = {2016 {IEEE} High Performance Extreme Computing Conference ({HPEC})}
}

@inproceedings{chen2021thundergp,
  title={ThunderGP: HLS-based graph processing framework on fpgas},
  author={Chen, Xinyu and Tan, Hongshi and Chen, Yao and He, Bingsheng and Wong, Weng-Fai and Chen, Deming},
  booktitle={The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={69--80},
  year={2021}
}

@misc{gine,
  doi = {10.48550/ARXIV.1905.12265},
  url = {https://arxiv.org/abs/1905.12265},
  author = {Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Strategies for Pre-training Graph Neural Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{dai2018graphh,
  title={{GraphH: A processing-in-memory architecture for large-scale graph processing}},
  author={Dai, Guohao and others},
  journal={IEEE TCAD},
  volume={38},
  number={4},
  pages={640--653},
  year={2018},
  publisher={IEEE}
}

@article{yan2014blogel,
  title={Blogel: A block-centric framework for distributed computation on real-world graphs},
  author={Yan, Da and others},
  journal={VLDB},
  volume={7},
  number={14},
  pages={1981--1992},
  year={2014},
  publisher={VLDB Endowment}
}


@article{tian2013think,
  title={From" think like a vertex" to" think like a graph"},
  author={Tian, Yuanyuan and others},
  journal={VLDB},
  volume={7},
  number={3},
  pages={193--204},
  year={2013},
  publisher={VLDB Endowment}
}

@inproceedings{dai2017foregraph,
  title={Foregraph: Exploring large-scale graph processing on multi-fpga architecture},
  author={Dai, Guohao and Huang, Tianhao and Chi, Yuze and Xu, Ningyi and Wang, Yu and Yang, Huazhong},
  booktitle={Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={217--226},
  year={2017}
}


@inproceedings{shao2019improving,
  title={Improving performance of graph processing on fpga-dram platform by two-level vertex caching},
  author={Shao, Zhiyuan and Li, Ruoshi and Hu, Diqing and Liao, Xiaofei and Jin, Hai},
  booktitle={Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={320--329},
  year={2019}
}

@article{zhou2019hitgraph,
  title={Hitgraph: High-throughput graph processing framework on fpga},
  author={Zhou, Shijie and Kannan, Rajgopal and Prasanna, Viktor K and Seetharaman, Guna and Wu, Qing},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={30},
  number={10},
  pages={2249--2264},
  year={2019},
  publisher={IEEE}
}


@inproceedings{yao2018efficient,
  title={An efficient graph accelerator with parallel data conflict management},
  author={Yao, Pengcheng and Zheng, Long and Liao, Xiaofei and Jin, Hai and He, Bingsheng},
  booktitle={Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques},
  pages={1--12},
  year={2018}
}


@inproceedings{yuan2021large,
  title={Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification},
  author={Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3040--3049},
  year={2021}
}


@inproceedings{gong2019exploiting,
  title={Exploiting edge features for graph neural networks},
  author={Gong, Liyu and Cheng, Qiang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9211--9219},
  year={2019}
}


@article{shi2021versagnn,
  title={Versagnn: a versatile accelerator for graph neural networks},
  author={Shi, Feng and Jin, Ahren Yiqiao and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2105.01280},
  year={2021}
}



@misc{velickovic_2022,
  doi = {10.48550/ARXIV.2202.11097},
  url = {https://arxiv.org/abs/2202.11097},
  author = {VeliÄkoviÄ‡, Petar},
  keywords = {Machine Learning (cs.LG), Social and Information Networks (cs.SI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Message passing all the way up},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{tailor2021we,
  title={Do We Need Anisotropic Graph Neural Networks?},
  author={Tailor, Shyam A and Opolka, Felix and Lio, Pietro and Lane, Nicholas Donald},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}


@article{ma2019pan,
  title={PAN: Path integral based convolution for deep graph neural networks},
  author={Ma, Zheng and Li, Ming and Wang, Yuguang},
  journal={arXiv preprint arXiv:1904.10996},
  year={2019}
}

@inproceedings{lin2022hp,
  title={HP-GNN: Generating High Throughput GNN Training Implementation on CPU-FPGA Heterogeneous Platform},
  author={Lin, Yi-Chien and Zhang, Bingyi and Prasanna, Viktor},
  booktitle={Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={123--133},
  year={2022}
}



@misc{activations,
	title = {Activation {Functions} in {Deep} {Learning}: {A} {Comprehensive} {Survey} and {Benchmark}},
	shorttitle = {Activation {Functions} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.14545},
	doi = {10.48550/arXiv.2109.14545},
	abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/ActivationFunctions\}.},
	urldate = {2022-07-24},
	publisher = {arXiv},
	author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
	month = jun,
	year = {2022},
	note = {arXiv:2109.14545 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Accepted in Neurocomputing, Elsevier},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\T362TFVR\\Dubey et al. - 2022 - Activation Functions in Deep Learning A Comprehen.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\YJ9G5ZYB\\2109.html:text/html},
}

@misc{batchnorm,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	doi = {10.48550/arXiv.1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2022-07-24},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\A9KKNKSV\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\BUS2XLRI\\1502.html:text/html},
}

@misc{layernorm,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	doi = {10.48550/arXiv.1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2022-07-24},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\IP8L9M4B\\Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\52V3AABJ\\1607.html:text/html}
}



@misc{resnet,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-07-24},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\GTJXA2FQ\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\P499IUQR\\1512.html:text/html}
}

@misc{deepergcn,
	title = {{DeeperGCN}: {All} {You} {Need} to {Train} {Deeper} {GCNs}},
	shorttitle = {{DeeperGCN}},
	url = {http://arxiv.org/abs/2006.07739},
	doi = {10.48550/arXiv.2006.07739},
	abstract = {Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. Please visit https://www.deepgcns.org for more information.},
	urldate = {2022-07-24},
	publisher = {arXiv},
	author = {Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07739 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: This work is still working in process. More results will be updated in the future version. Project website: https://www.deepgcns.org},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\ZUYLQGKQ\\Li et al. - 2020 - DeeperGCN All You Need to Train Deeper GCNs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\RTPP7K3U\\2006.html:text/html}
}

@misc{deepgcns,
	title = {{DeepGCNs}: {Can} {GCNs} {Go} as {Deep} as {CNNs}?},
	shorttitle = {{DeepGCNs}},
	url = {http://arxiv.org/abs/1904.03751},
	doi = {10.48550/arXiv.1904.03751},
	abstract = {Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7\% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.},
	urldate = {2022-07-24},
	publisher = {arXiv},
	author = {Li, Guohao and MÃ¼ller, Matthias and Thabet, Ali and Ghanem, Bernard},
	month = aug,
	year = {2019},
	note = {arXiv:1904.03751 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: First two authors contributed equally. Accepted to ICCV'19 as oral presentation},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\YCUHQKNV\\Li et al. - 2019 - DeepGCNs Can GCNs Go as Deep as CNNs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\ND6RSPIS\\1904.html:text/html}
}



@article{var_welford,
	title = {Note on a {Method} for {Calculating} {Corrected} {Sums} of {Squares} and {Products}},
	volume = {4},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1266577},
	number = {3},
	urldate = {2022-07-25},
	journal = {Technometrics},
	author = {Welford, B. P.},
	year = {1962},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {419--420},
}


@misc{gelu,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	doi = {10.48550/arXiv.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jul,
	year = {2020},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Trimmed version of 2016 draft; add exact formula},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\I8KMS3Y4\\Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\3FTDT5NF\\1606.html:text/html},
}



@misc{flowgnn,
	title = {{FlowGNN}: {A} {Dataflow} {Architecture} for {Universal} {Graph} {Neural} {Network} {Inference} via {Multi}-{Queue} {Streaming}},
	shorttitle = {{FlowGNN}},
	url = {http://arxiv.org/abs/2204.13103},
	doi = {10.48550/arXiv.2204.13103},
	abstract = {Graph neural networks (GNNs) have recently exploded in popularity thanks to their broad applicability to graph-related problems such as quantum chemistry, drug discovery, and high energy physics. However, meeting demand for novel GNN models and fast inference simultaneously is challenging because of the gap between developing efficient accelerators and the rapid creation of new GNN models. Prior art focuses on the acceleration of specific classes of GNNs, such as Graph Convolutional Network (GCN), but lacks the generality to support a wide range of existing or new GNN models. Meanwhile, most work rely on graph pre-processing to exploit data locality, making them unsuitable for real-time applications. To address these limitations, in this work, we propose a generic dataflow architecture for GNN acceleration, named FlowGNN, which can flexibly support the majority of message-passing GNNs. The contributions are three-fold. First, we propose a novel and scalable dataflow architecture, which flexibly supports a wide range of GNN models with message-passing mechanism. The architecture features a configurable dataflow optimized for simultaneous computation of node embedding, edge embedding, and message passing, which is generally applicable to all models. We also propose a rich library of model-specific components. Second, we deliver ultra-fast real-time GNN inference without any graph pre-processing, making it agnostic to dynamically changing graph structures. Third, we verify our architecture on the Xilinx Alveo U50 FPGA board and measure the on-board end-to-end performance. We achieve a speed-up of up to 51-254x against CPU (6226R) and 1.3-477x against GPU (A6000) (with batch sizes 1 through 1024); we also outperform the SOTA GNN accelerator I-GCN by 1.03x and 1.25x across two datasets. Our implementation code and on-board measurement are publicly available on GitHub.},
	urldate = {2022-06-29},
	publisher = {arXiv},
	author = {Sarkar, Rishov and Abi-Karam, Stefan and He, Yuqi and Sathidevi, Lakshmi and Hao, Cong},
	month = apr,
	year = {2022},
	note = {arXiv:2204.13103 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	annote = {Comment: 13 pages, 10 figures. Submitted to MICRO 2022. Accelerator source code will be released on GitHub upon acceptance. arXiv admin note: text overlap with arXiv:2201.08475},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\UQZY9FWG\\Sarkar et al. - 2022 - FlowGNN A Dataflow Architecture for Universal Gra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\Z78WNT5L\\2204.html:text/html},
}



@inproceedings{deepburninggl,
	title = {{DeepBurning}-{GL}: an {Automated} {Framework} for {Generating} {Graph} {Neural} {Network} {Accelerators}},
	shorttitle = {{DeepBurning}-{GL}},
	abstract = {Building FPGA-based graph learning accelerators is very time- consuming due to the low-level RTL programming and the complicated design flow of FPGA development. It also requires the architecture and hardware expertise from the Graph Neural Network (GNN) application developers to tailor efficient accelerator designs on FPGAs. This work proposes an automation framework, DeepBurning-GL, which is compatible with state-of-the-art graph learning frameworks such as Deep Graph Library so that the developers can easily generate application-specific GNN accelerators from the software-described models. First, DeepBurning-GL employs a GNN performance analyzer to locate the performance bottleneck of specific GNN applications and decide the major design architectures and parameters that meet the user-specified constraints. Second, DeepBurning-GL provides a series of pre-built design templates such as computing templates and memory templates, which can be parameterized and fused to generate the final accelerator design. It also includes an optimizer that conducts automatic optimization by adjusting the accelerator architectural parameters. In evaluation, we use DeepBurning-GL to generate customized accelerators on three different FPGA platforms for various GNN models and workloads. The experimental results show that the generated accelerators achieve 179.4X and 40.1X energy-efficiency boost over the CPU and GPU solutions on average and deliver a 6.28X speedup and 6.73X energy-efficiency improvement on average compared to the latest GNN accelerator HyGCN on Alveo U50.},
	booktitle = {2020 {IEEE}/{ACM} {International} {Conference} {On} {Computer} {Aided} {Design} ({ICCAD})},
	author = {Liang, Shengwen and Liu, Cheng and Wang, Ying and Li, Huawei and Li, Xiaowei},
	month = nov,
	year = {2020},
	note = {ISSN: 1558-2434},
	keywords = {Aggregates, automated framework, Automation, Computational modeling, Feature extraction, Field programmable gate arrays, FPGA, graph neural networks, Hardware, Neural networks},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\stefa\\Zotero\\storage\\XJM5LT89\\9256539.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\stefa\\Zotero\\storage\\8IFVPPP2\\Liang et al. - 2020 - DeepBurning-GL an Automated Framework for Generat.pdf:application/pdf},
}


@misc{wu_graph_2022,
	title = {Graph {Neural} {Networks} in {Recommender} {Systems}: {A} {Survey}},
	shorttitle = {Graph {Neural} {Networks} in {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2011.02260},
	doi = {10.48550/arXiv.2011.02260},
	abstract = {With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin},
	month = apr,
	year = {2022},
	note = {arXiv:2011.02260 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning},
	annote = {Comment: Accepted by ACM Computing Surveys (CSUR)},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\Y9G4WLFH\\Wu et al. - 2022 - Graph Neural Networks in Recommender Systems A Su.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\T9CQ3XDW\\2011.html:text/html},
}


@inproceedings{benamira_semi,
	title = {Semi-{Supervised} {Learning} and {Graph} {Neural} {Networks} for {Fake} {News} {Detection}},
	doi = {10.1145/3341161.3342958},
	abstract = {Social networks have become the main platforms for information dissemination. Nevertheless, due to the increasing number of users, social media platforms tend to be highly vulnerable to the propagation of disinformation - making the detection of fake news a challenging task. In this work, we focus on content-based methods for detecting fake news - casting the problem to a binary text classification one (an article corresponds to either fake news or not). In particular, our work proposes a graph-based semi-supervised fake news detection method based on graph neural networks. The experimental results indicate that the proposed methodology achieves better performance compared to traditional classification techniques, especially when trained on limited number of labeled articles 11.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining} ({ASONAM})},
	author = {Benamira, Adrien and Devillers, Benjamin and Lesot, Etienne and Ray, Ayush K. and Saadi, Manal and Malliaros, Fragkiskos D.},
	month = aug,
	year = {2019},
	note = {ISSN: 2473-991X},
	keywords = {Fake news detection, Graph neural networks, Nearest neighbor methods, Neural networks, Semi-supervised learning, Semisupervised learning, Social network services},
	pages = {568--569},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\stefa\\Zotero\\storage\\CEWBR8F2\\Benamira et al. - 2019 - Semi-Supervised Learning and Graph Neural Networks.pdf:application/pdf},
}


@inproceedings{eta_traffic,
	title = {{ETA} {Prediction} with {Graph} {Neural} {Networks} in {Google} {Maps}},
	url = {http://arxiv.org/abs/2108.11482},
	doi = {10.1145/3459637.3481916},
	abstract = {Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events -- such as rush hours -- that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+\% in cities like Sydney).},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	author = {Derrow-Pinion, Austin and She, Jennifer and Wong, David and Lange, Oliver and Hester, Todd and Perez, Luis and Nunkesser, Marc and Lee, Seongjae and Guo, Xueying and Wiltshire, Brett and Battaglia, Peter W. and Gupta, Vishal and Li, Ang and Xu, Zhongwen and Sanchez-Gonzalez, Alvaro and Li, Yujia and VeliÄkoviÄ‡, Petar},
	month = oct,
	year = {2021},
	note = {arXiv:2108.11482 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	pages = {3767--3776},
	annote = {Comment: To appear at CIKM 2021 (Applied Research Track). 10 pages, 4 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\AE6IVMS7\\Derrow-Pinion et al. - 2021 - ETA Prediction with Graph Neural Networks in Googl.pdf:application/pdf},
}


@inproceedings{golmaei_deepnote,
	address = {New York, NY, USA},
	series = {{BCB} '21},
	title = {{DeepNote}-{GNN}: predicting hospital readmission using clinical notes and patient network},
	isbn = {978-1-4503-8450-6},
	shorttitle = {{DeepNote}-{GNN}},
	url = {https://doi.org/10.1145/3459930.3469547},
	doi = {10.1145/3459930.3469547},
	abstract = {With the increasing availability of Electronic Health Records (EHRs) and advances in deep learning techniques, developing deep predictive models that use EHR data to solve healthcare problems has gained momentum in recent years. The majority of clinical predictive models benefit from structured data in EHR (e.g., lab measurements and medications). Still, learning clinical outcomes from all possible information sources is one of the main challenges when building predictive models. This work focuses on two sources of information that have been underused by researchers; unstructured data (e.g., clinical notes) and a patient network. We propose a novel hybrid deep learning model, DeepNote-GNN, that integrates clinical notes information and patient network topological structure to improve 30-day hospital readmission prediction. DeepNote-GNN is a robust deep learning framework consisting of two modules: DeepNote and patient network. DeepNote extracts deep representations of clinical notes using a feature aggregation unit on top of a state-of-the-art Natural Language Processing (NLP) technique - BERT. By exploiting these deep representations, a patient network is built, and Graph Neural Network (GNN) is used to train the network for hospital readmission predictions. Performance evaluation on the MIMIC-III dataset demonstrates that DeepNote-GNN achieves superior results compared to the state-of-the-art baselines on the 30-day hospital readmission task. We extensively analyze the DeepNote-GNN model to illustrate the effectiveness and contribution of each component of it. The model analysis shows that patient network has a significant contribution to the overall performance, and DeepNote-GNN is robust and can consistently perform well on the 30-day readmission prediction task.},
	urldate = {2022-08-01},
	booktitle = {Proceedings of the 12th {ACM} {Conference} on {Bioinformatics}, {Computational} {Biology}, and {Health} {Informatics}},
	publisher = {Association for Computing Machinery},
	author = {Golmaei, Sara Nouri and Luo, Xiao},
	month = aug,
	year = {2021},
	keywords = {clinical notes, deep learning, electronic health records, feature aggregation, graph neural networks, natural language processing},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\stefa\\Zotero\\storage\\AVVT6R5V\\Golmaei and Luo - 2021 - DeepNote-GNN predicting hospital readmission usin.pdf:application/pdf},
}


@misc{chang_comprehensive_2022,
	title = {A {Comprehensive} {Survey} of {Scene} {Graphs}: {Generation} and {Application}},
	shorttitle = {A {Comprehensive} {Survey} of {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2104.01111},
	doi = {10.1109/TPAMI.2021.3137605},
	abstract = {Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarized the general definition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.},
	urldate = {2022-08-01},
	author = {Chang, Xiaojun and Ren, Pengzhen and Xu, Pengfei and Li, Zhihui and Chen, Xiaojiang and Hauptmann, Alex},
	month = jan,
	year = {2022},
	note = {arXiv:2104.01111 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 25 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\RQTYUVBA\\Chang et al. - 2022 - A Comprehensive Survey of Scene Graphs Generation.pdf:application/pdf},
}


@article{wu_ironman_pro_2022,
	title = {{IronMan}-{Pro}: {Multi}-objective {Design} {Space} {Exploration} in {HLS} via {Reinforcement} {Learning} and {Graph} {Neural} {Network} based {Modeling}},
	issn = {1937-4151},
	shorttitle = {{IronMan}-{Pro}},
	doi = {10.1109/TCAD.2022.3185540},
	abstract = {Despite the great success of High-Level Synthesis (HLS) tools, we observe several unresolved challenges: 1) the high-level abstraction of HLS programming styles sometimes conceals optimization opportunities; 2) the actual quality of resulting RTL designs is hard to predict; 3) existing HLS tools do not provide flexible trade-off (Pareto) solutions among different objectives and constraints. To this end, we propose an end-to-end framework, namely IronMan-Pro. The primary goal is to enable a flexible and automated design space exploration (DSE), to provide either optimized solutions under user-specified constraints or Pareto trade-offs among different objectives (such as resource types, area, and latency). IronMan-Pro consists of three components: (1) GPP, a highly accurate graph-neural-network-based performance and resource predictor; (2) RLMD, a reinforcement-learning-based multi-objective design exploration engine for optimal resource allocation strategies, aiming to provide Pareto solutions among different objectives; (3) CT, a code transformer to assist RLMD and GPP, which extracts the data flow graphs from original HLS C/C++ and automatically generates synthesizable code with optimized HLS directives. Experimental results show that, (1) GPP achieves high prediction accuracy, reducing the prediction errors of HLS tools by 10.9Ã— in resource utilization and 5.7Ã— in critical path timing; (2) compared with meta-heuristic-based techniques, IronMan-Pro generates superior solutions improving resource utilization by 16.0\% 29.5\% and critical path timing by 7.6\% 16.5\%; (3) under user-specified constraints, IronMan-Pro can find satisfying solutions over 96\% of the cases, more than twice as many as that of meta-heuristic-based techniques and with a speedup of up to 400Ã—. This work demonstrates the great potential of applying machine learning algorithms in the electronic design automation domain, especially for the hard-to-solve problems such as timing estimation and optimization. IronMan-Pro is available at https://github:com/lydiawunan/IronMan.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Wu, Nan and Xie, Yuan and Hao, Cong},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Resource management, Optimization, High-Level Synthesis, Reinforcement Learning, Table lookup, Timing},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\stefa\\Zotero\\storage\\IVNXA8Z8\\9803218.html:text/html;Wu et al. - 2022 - IronMan-Pro Multi-objective Design Space Explorat.pdf:C\:\\Users\\stefa\\Zotero\\storage\\B27GRD42\\Wu et al. - 2022 - IronMan-Pro Multi-objective Design Space Explorat.pdf:application/pdf},
}



@misc{wu_graph_2021,
	title = {Graph {Neural} {Networks} for {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Graph {Neural} {Networks} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2106.06090},
	doi = {10.48550/arXiv.2106.06090},
	abstract = {Deep learning has become the dominant approach in coping with various tasks in Natural LanguageProcessing (NLP). Although text inputs are typically represented as a sequence of tokens, there isa rich variety of NLP problems that can be best expressed with a graph structure. As a result, thereis a surge of interests in developing new deep learning techniques on graphs for a large numberof NLP tasks. In this survey, we present a comprehensive overview onGraph Neural Networks(GNNs) for Natural Language Processing. We propose a new taxonomy of GNNs for NLP, whichsystematically organizes existing research of GNNs for NLP along three axes: graph construction,graph representation learning, and graph based encoder-decoder models. We further introducea large number of NLP applications that are exploiting the power of GNNs and summarize thecorresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discussvarious outstanding challenges for making the full use of GNNs for NLP as well as future researchdirections. To the best of our knowledge, this is the first comprehensive overview of Graph NeuralNetworks for Natural Language Processing.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Wu, Lingfei and Chen, Yu and Shen, Kai and Guo, Xiaojie and Gao, Hanning and Li, Shucheng and Pei, Jian and Long, Bo},
	month = jun,
	year = {2021},
	note = {arXiv:2106.06090 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 127 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\BZIBF6AW\\Wu et al. - 2021 - Graph Neural Networks for Natural Language Process.pdf:application/pdf},
}



@inproceedings{pointacc,
	title = {{PointAcc}: {Efficient} {Point} {Cloud} {Accelerator}},
	shorttitle = {{PointAcc}},
	url = {http://arxiv.org/abs/2110.07600},
	doi = {10.1145/3466752.3480084},
	abstract = {Deep learning on point clouds plays a vital role in a wide range of applications such as autonomous driving and AR/VR. These applications interact with people in real-time on edge devices and thus require low latency and low energy. Compared to projecting the point cloud to 2D space, directly processing the 3D point cloud yields higher accuracy and lower \#MACs. However, the extremely sparse nature of point cloud poses challenges to hardware acceleration. For example, we need to explicitly determine the nonzero outputs and search for the nonzero neighbors (mapping operation), which is unsupported in existing accelerators. Furthermore, explicit gather and scatter of sparse features are required, resulting in large data movement overhead. In this paper, we comprehensively analyze the performance bottleneck of modern point cloud networks on CPU/GPU/TPU. To address the challenges, we then present PointAcc, a novel point cloud deep learning accelerator. PointAcc maps diverse mapping operations onto one versatile ranking-based kernel, streams the sparse computation with configurable caching, and temporally fuses consecutive dense layers to reduce the memory footprint. Evaluated on 8 point cloud models across 4 applications, PointAcc achieves 3.7X speedup and 22X energy savings over RTX 2080Ti GPU. Co-designed with light-weight neural networks, PointAcc rivals the prior accelerator Mesorasi by 100X speedup with 9.1\% higher accuracy running segmentation on the S3DIS dataset. PointAcc paves the way for efficient point cloud recognition.},
	urldate = {2022-08-01},
	booktitle = {{MICRO}-54: 54th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	author = {Lin, Yujun and Zhang, Zhekai and Tang, Haotian and Wang, Hanrui and Han, Song},
	month = oct,
	year = {2021},
	note = {arXiv:2110.07600 [cs]},
	keywords = {Computer Science - Hardware Architecture},
	pages = {449--461},
	annote = {Comment: Accepted by Mircro 2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\8X52PKRC\\Lin et al. - 2021 - PointAcc Efficient Point Cloud Accelerator.pdf:application/pdf},
}


@article{elabd_graph_2022,
	title = {Graph {Neural} {Networks} for {Charged} {Particle} {Tracking} on {FPGAs}},
	volume = {5},
	issn = {2624-909X},
	url = {http://arxiv.org/abs/2112.02048},
	doi = {10.3389/fdata.2022.828666},
	abstract = {The determination of charged particle trajectories in collisions at the CERN Large Hadron Collider (LHC) is an important but challenging problem, especially in the high interaction density conditions expected during the future high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a type of geometric deep learning algorithm that has successfully been applied to this task by embedding tracker data as a graph -- nodes represent hits, while edges represent possible track segments -- and classifying the edges as true or fake track segments. However, their study in hardware- or software-based trigger applications has been limited due to their large computational cost. In this paper, we introduce an automated translation workflow, integrated into a broader tool called \${\textbackslash}texttt\{hls4ml\}\$, for converting GNNs into firmware for field-programmable gate arrays (FPGAs). We use this translation tool to implement GNNs for charged particle tracking, trained using the TrackML challenge dataset, on FPGAs with designs targeting different graph sizes, task complexites, and latency/throughput requirements. This work could enable the inclusion of charged particle tracking GNNs at the trigger level for HL-LHC experiments.},
	urldate = {2022-06-27},
	journal = {Frontiers in Big Data},
	author = {Elabd, Abdelrahman and Razavimaleki, Vesal and Huang, Shi-Yu and Duarte, Javier and Atkinson, Markus and DeZoort, Gage and Elmer, Peter and Hauck, Scott and Hu, Jin-Xuan and Hsu, Shih-Chieh and Lai, Bo-Cheng and Neubauer, Mark and Ojalvo, Isobel and Thais, Savannah and Trahms, Matthew},
	month = mar,
	year = {2022},
	note = {arXiv:2112.02048 [hep-ex, physics:physics, stat]},
	keywords = {Statistics - Machine Learning, Physics - Instrumentation and Detectors},
	pages = {828666},
	annote = {Comment: 28 pages, 17 figures, 1 table, published version},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\EGWPC5C3\\Elabd et al. - 2022 - Graph Neural Networks for Charged Particle Trackin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\stefa\\Zotero\\storage\\2GFE6SPY\\2112.html:text/html;Full Text PDF:C\:\\Users\\stefa\\Zotero\\storage\\4MTAL938\\Elabd et al. - 2022 - Graph Neural Networks for Charged Particle Trackin.pdf:application/pdf},
}


@inproceedings{liu_efficient_2021,
	address = {Xi'an, China},
	title = {Efficient and {Robust} {LiDAR}-{Based} {End}-to-{End} {Navigation}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561299/},
	doi = {10.1109/ICRA48506.2021.9561299},
	abstract = {Deep learning has been used to demonstrate endto-end neural network learning for autonomous vehicle control from raw sensory input. While LiDAR sensors provide reliably accurate information, existing end-to-end driving solutions are mainly based on cameras since processing 3D data requires a large memory footprint and computation cost. On the other hand, increasing the robustness of these systems is also critical; however, even estimating the modelâ€™s uncertainty is very challenging due to the cost of sampling-based methods. In this paper, we present an efï¬cient and robust LiDAR-based end-to-end navigation framework. We ï¬rst introduce Fast-LiDARNet that is based on sparse convolution kernel optimization and hardwareaware model design. We then propose Hybrid Evidential Fusion that directly estimates the uncertainty of the prediction from only a single forward pass and then fuses the control predictions intelligently. We evaluate our system on a full-scale vehicle and demonstrate lane-stable as well as navigation capabilities. In the presence of out-of-distribution events (e.g., sensor failures), our system signiï¬cantly improves robustness and reduces the number of takeovers in the real world.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Liu, Zhijian and Amini, Alexander and Zhu, Sibo and Karaman, Sertac and Han, Song and Rus, Daniela L.},
	month = may,
	year = {2021},
	pages = {13247--13254},
	file = {Liu et al. - 2021 - Efficient and Robust LiDAR-Based End-to-End Naviga.pdf:C\:\\Users\\stefa\\Zotero\\storage\\2C2BXU2S\\Liu et al. - 2021 - Efficient and Robust LiDAR-Based End-to-End Naviga.pdf:application/pdf},
}


@misc{li_deep_2020,
	title = {Deep {Learning} for {LiDAR} {Point} {Clouds} in {Autonomous} {Driving}: {A} {Review}},
	shorttitle = {Deep {Learning} for {LiDAR} {Point} {Clouds} in {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2005.09830},
	abstract = {Recently, the advancement of deep learning in discriminative feature learning from 3D LiDAR data has led to rapid development in the field of autonomous driving. However, automated processing uneven, unstructured, noisy, and massive 3D point clouds is a challenging and tedious task. In this paper, we provide a systematic review of existing compelling deep learning architectures applied in LiDAR point clouds, detailing for specific tasks in autonomous driving such as segmentation, detection, and classification. Although several published research papers focus on specific topics in computer vision for autonomous vehicles, to date, no general survey on deep learning applied in LiDAR point clouds for autonomous vehicles exists. Thus, the goal of this paper is to narrow the gap in this topic. More than 140 key contributions in the recent five years are summarized in this survey, including the milestone 3D deep architectures, the remarkable deep learning applications in 3D semantic segmentation, object detection, and classification; specific datasets, evaluation metrics, and the state of the art performance. Finally, we conclude the remaining challenges and future researches.},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Li, Ying and Ma, Lingfei and Zhong, Zilong and Liu, Fei and Cao, Dongpu and Li, Jonathan and Chapman, Michael A.},
	month = may,
	year = {2020},
	note = {arXiv:2005.09830 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 21 pages, submitted to IEEE Transactions on Neural Networks and Learning Systems},
	file = {arXiv Fulltext PDF:C\:\\Users\\stefa\\Zotero\\storage\\26RE5AV7\\Li et al. - 2020 - Deep Learning for LiDAR Point Clouds in Autonomous.pdf:application/pdf},
}


% The preceding BibTeX was generated in Zotero; if changes are needed, let Rishov know so that he doesn't overwrite them on a future export.
%%% END ref.bib used in DoSSA work %%%

