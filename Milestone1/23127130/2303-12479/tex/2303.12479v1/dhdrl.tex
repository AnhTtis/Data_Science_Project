%\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
\documentclass[onecolumn,32pt,draftclsnofoot]{IEEEtran}
%\linespread{3}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{cite}
\usepackage{cuted}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{amsmath}
\allowdisplaybreaks[4]
\footnoterule
\newcounter{MYtempeqncnt}
%\usepackage{xcolor}
%\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
 %updated with editorial comments 8/9/2021
\usepackage{geometry}

\geometry{left = 0.65 in, right = 0.7 in, top = 0.76 in, bottom = 1 in}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}


\title{Distributed Two-tier DRL Framework for Cell-Free Network: Association, Beamforming and Power Allocation
}
\author{\IEEEauthorblockN{Kaiwen Yu*, Chonghao Zhao*, Gang Wu*, Geoffrey Ye Li$^\dag$}

\IEEEauthorblockA{*National Key Laboratory of Science and Technology on Communications, \\
*University of Electronic Science and Technology of China, Chengdu 611731, China\\
$^\dag$Dept. of Electrical and Electronic Engineering, Imperial College London, London SW7 2AZ, U.K\\
}
yukaiwen@std.uestc.edu.cn, i.zch@hotmail.com, wugang99@uestc.edu.cn(corresponding author), geoffrey.li@imperial.ac.uk

}

\maketitle

\begin{abstract}
  Intelligent wireless networks have long been expected to have self-configuration and self-optimization capabilities to adapt to various environments and demands. In this paper, we develop a novel distributed hierarchical deep reinforcement learning (DHDRL) framework with two-tier control networks in different timescales to optimize the long-term spectrum efficiency (SE) of the downlink cell-free multiple-input single-output (MISO) network, consisting of multiple distributed access points (AP) and user terminals (UT). To realize the proposed two-tier control strategy, we decompose the optimization problem into two sub-problems, AP-UT association (AUA) as well as beamforming and power allocation (BPA), resulting in a Markov decision process (MDP) and Partially Observable MDP (POMDP). The proposed method consists of two neural networks. At the system level, a distributed high-level neural network is introduced to optimize wireless network structure on a large timescale. While at the link level, a distributed low-level neural network is proposed to mitigate inter-AP interference and improve the transmission performance on a small timescale. Numerical results show that our method is effective for high-dimensional problems, in terms of spectrum efficiency, signaling overhead as well as satisfaction probability, and generalize well to diverse multi-object problems.
\end{abstract}

\begin{IEEEkeywords}
intelligent wireless network, deep reinforcement learning, association, beamforming and power allocation.
\end{IEEEkeywords}
\addtolength{\topmargin}{-0.135in}

\section{Introduction}
\IEEEPARstart {I}{n} a typical user-centric cell-free architecture, there are a large number of access points (AP) around the user terminals (UT), and each UT may be served by a group of APs \cite{c1}, which is a promising method for cell-free communications and can effectively address cell edge issue in the traditional networks \cite{c3}. Generally, there are two architectures for cell-free deployment: centralized and distributed structure. The centralized cell-free adopts a fully connected network structure and uses the central processing unit (CPU) to perform data processing in a unified manner. All APs cooperatively serve all UTs within the coverage area \cite{add1,c4}. However, the centralized cell-free structure requires a huge computational capacity and suffers signal processing delay and overhead. To cope with this problem, the distributed cell-free structure has been extensively studied, in which each AP has the ability to perform signal processing, e.g. channel estimation and beamforming, and all APs cooperate by exchanging signaling through the fronthaul/backhaul \cite{c5,c6,c8}. In addition, the TR38.801 has discussed the proposal to split low-PHY into distributed unit (DU) \cite{T1}.

On the other hand, cell-free requires the downlink data belonging to all UTs sent to all APs from a centralized CPU, which will put huge pressure on the fronthaul. A better solution is to let AP only serve a few UTs at the same time. But this will bring up another issue, i.e. how APs determine which UT to be served at each moment, especially which turns to a very challenging issue wireless networks are more and more dynamic and complex \cite{a1,wa}, referring to as AP-UT association (AUA) problem\cite{c9,c10}. Given an AUA configuration, the APs associating with the same UT will form a cooperative cluster to provide diverse gains to the served UT. However, multiple transmitters may interfere with each other, as well as the dynamic wireless environment can severely limit their multi-point cooperative performance \cite{b8}. Beamforming and power allocation (BPA) can effectively eliminate mutual interference among APs, provide high signal quality, and improve the performance of the whole network\cite{c11}. Although there has been a lot of work on association \cite{c9,f1} or beamforming \cite{c11} alone, dynamic, intelligent, and cooperative solutions to joint association, beamforming, and power allocation problems still require further investigation.

Furthermore, it is unnecessary and impractical to tune the AUA continuously. Therefore, fixed cooperative solutions for network topology have been provided where the base stations (BS) can cooperate in a fixed mode \cite{T2}. Since dynamic cooperative BSs, which are forming after AUA, require time to create the relevant information, cache and synchronize, etc., and will result in a huge workload and interactive information. Moreover, it is unnecessary since the served signal strength and UT's needs do not change so fast as a frequent association is required. In fact, frequent AUA will waste computational resources and backhaul/fronthaul capacity. Hence, AUA problem should be executed at the large-timescale, e.g. frame level. Along with AUA, complex wireless channel environment and the rapid changes of multi-point interference over time, BPA needs to be continuously adjusted to improve the desired signal quality at small-timescale, e.g. slot level.

There have been many works focused on intelligent association, beamforming, or power allocation, based on deep learning (DL) or deep reinforcement learning (DRL). The novel deep neural network in \cite{b2} addresses adaptive and sequential beamforming design problems in the millimeter wave (mmWave) environments. Kwon et al. have investigated DL-based beamforming for two-user multi-cell multiple-input and single-output (MISO) interference channels \cite{b3}. Zhang et al. have proposed two DRL algorithms in \cite{c13}, which have used historical information to solve the user association problem in symbiotic radio networks. In \cite{c14}, Chen et al. have proposed an imitation-augmented DRL approach to solve joint trajectory design and BS association problems in a cellular-connected unmanned aerial vehicles (UAV) system.

Motivated by the above discussions, we propose a multi-timescale multi-AP cooperative joint AUA and BPA scheme to improve network throughput and communication efficiency while saving the signaling overhead. To cope with this challenge, we decompose the joint problem into multiple subproblems, which are networked and coordinated. We then formulate the AUA part as an infinite Markov decision process (MDP) at large-timescale and formulate the BPA part as a Partially Observable MDP (POMDP) at small-timescale. To jointly optimize the interplay between the AUA and BPA, we proposed a distributed hierarchical DRL architecture (DHDRL) with two-tier control networks to jointly optimize the transmission performance. Our results show that the hierarchy DRL framework can effectively handle the complex multi-object optimization problem with multi-timescale decisions.

\begin{figure}[htbp]
\centerline{\includegraphics[width=3.5in]{systemmodel.pdf}}
\caption{A possible scenario for the proposed network architecture.}
\label{CellModel}
\end{figure}
\section{System Model}
%After introducing the cell-free topology, we introduce the model of cell-Free AP-user association, beamforming and power allocation.

Consider a cell-free downlink network with $M$ APs and $K$ UTs, where each of APs equipped with $N$ transmit antennas while each UT is with one antenna. APs are randomly deployed in square area with side length $L$. APs proactively choose the served UT, and adjust the transmit power and beam to improve the service performance. The time dimension is partitioned into Transmission Time Interval (TTI) of 1 ms, indexed by $t \in \left\{1,2,\ldots \right\}$. Each AP has a separate neural network and independently performs data processing, including performing operations related to AUA, uplink channel training, downlink BPA. In the considered model, all APs can be partitioned into multiple cell-free subnetworks, according to the AUA configurations, to serve partially UTs. The sub-networks are assumed to connect a virtual edge cloud processor (ECP) and all ECPs connect to the centralized CPU through fronthaul/backhaul links to form a cell-free network architecture, which enables the distributed APs to obtain the user's state information and cooperate to simultaneously serve all UTs within the network coverage area. Denote $\left\{ {1,2, \ldots ,M} \right\}=\mathcal{M}$ and $\left\{ {1,2, \ldots ,K} \right\}=\mathcal{K}$ as the index set of APs and UTs, respectively.
\subsection{Cell-Free AP-User Association}
In the proposed network architecture, we consider a flexible cell-free AP-user association scheme that the serving UTs of each AP are dynamically selected according to the channel conditions and user states in the large time-scale. It is assumed that each AP only serves one UT at the same time and sends an independent data stream to served UT $k$. A set of APs, serving the same UT together forms a cooperative cluster to provide diversity gain. Let $\mathcal{B}_k\left( t \right) \in \mathcal{M}$ denote the set of serving APs for UT $k$ at time $t$, i.e. $\mathcal{B}_k \left( t \right) \cap \mathcal{B}_l \left( t \right) = \emptyset ,\forall k \ne l$. The AUA controlling variable ${\mathbf \Omega}\left( t \right)$ at time $t$ can be expressed as:
\begin{equation}
{\mathbf \Omega}\left( t \right) = \{\mathcal{B}_k\left( t \right) \subseteq \mathcal{M}, k \in \mathcal K;\mathcal{B}_k\left( t \right) \cap \mathcal{B}_l\left( t \right) = \emptyset ,\forall k \ne l\}
\end{equation}

Under the fixed cell-free AUA, the received signal at user $k$ at time $t$ can be expressed as:
\begin{align}
\nonumber
{y_k}\left( t \right)&= \sum\limits_{i \in {\mathbf \Omega}\left( t \right)} {\sum\limits_{m \in {\mathcal B_i}\left( t \right)} {\sqrt {{p_m}\left( t \right)} {\mathbf{h}}_{k,m}^\dag \left( t \right){{\mathbf{w}}_m}\left( t \right){x_m}\left( t \right)} }  + {z_k}\left( t \right)\\
\nonumber
&=\sum\limits_{m \in {\mathcal B_k}\left( t \right)} {\sqrt {{p_m}\left( t \right)} {\mathbf{h}}_{k,m}^\dag \left( t \right){{\mathbf{w}}_m}\left( t \right){x_m}\left( t \right)}\\
&+ \sum\limits_{c \ne k} {\sum\limits_{l \in {\mathcal B_c}\left( t \right)} {\sqrt {{p_l}\left( t \right)} {\mathbf{h}}_{k,l}^\dag \left( t \right){{\mathbf{w}}_l}\left( t \right){x_l}\left( t \right)} }  + {z_k}\left( t \right),
\end{align}
where ${{\mathbf{w}}_m}\left( t \right)$ is the normalized beamforming vector at time $t$ in AP $m$, $\sqrt {{p_m}\left( t \right)}$ is the power allocated to the UT in AP $m$, ${\mathbf{h}}_{k,m}^\dag \left( t \right)$ is the channel between the $m$-th AP and the $k$-th UT, and ${z_k}\left( t \right) \in N\left( {0,{\sigma ^2}} \right)$ is the additive circular complex Gaussian noise with $0$ mean and variance $\sigma^2$.
We adopt a block fading channel model. The channel response vector from AP $m$ to the UT $k$ in time $t$ can be expressed as ${{\mathbf{h}}_{k,m}}\left( t \right) = {\beta _{k,m}\left| {{{\mathbf{g}}_{k,m}}\left( t \right)} \right|^2}$, where ${\beta _{k,m}}$ denotes the large-scale factor, ${{\mathbf{g}}_{k,m}}\left( t \right)$ is the Rayleigh fading vector composed of the small-scale fading coefficients between AP $m$ and UT $k$. It should be noted that our approach is not limited to specific channel models.

Due to the transmission and signal processing delay, it is difficult for APs to get accurate instantaneous  channel state information (CSI) and only imperfect CSI is available when performing BPA. For channel aging, the Rayleigh fading vector can be modeled as:
\begin{equation}
{{\mathbf{g}}_{k,m}}\left( {t + 1} \right) = \rho {{\mathbf{g}}_{k,m}}\left( t \right) + \sqrt {1 - {\rho ^2}} {{\mathbf{e}}_{k,m}}\left( t \right),
\end{equation}
where ${{\mathbf{e}}_{k,m}}\left( t \right)$, for $t=0,1,2,\ldots$ are independent and identically distributed circularly symmetric complex Gaussian (CSCG) random variables with zero mean and unit variance, $\rho  = {J_0}\left( {2\pi {f_d}{T_d}} \right)$ denotes the correlation coefficient, $T_d$ denotes the channel aging time, and $f_d$ is the Doppler shift. For Jakes' model, ${J_0}\left( \right)$ is the zeroth-order Bessel function of the first kind.

\subsection{Cell-Free AP Beamforming and Power Allocation}
After AUA processing, APs serving the same UT form a multi-point cluster. The entire network is divided into multiple sub-networks, introducing inter-AP interference. Taking into account signaling overhead and hardware complexity caused by CSI acquisition and matrix calculation, we adopt a codebook-based BPA strategy and employ a codebook matrix in \cite{c16} to mitigate the interference, where sufficient codes can be designed, and denote the matrix as ${{\textit{\textbf{N}}}_{\text{CB}}}$. The element at the $n$-th row and the $q$-th column of the beamforming matrix is
\begin{equation}
\label{codebook}
{{\textit{\textbf{N}}}_{\text{CB}}}\left( {n,q} \right) = \frac{1}{{\sqrt N }}\exp \left( {j\frac{{2\pi }}{S}\left\lfloor {\frac{{n\bmod \left( {q + \frac{{{Q_{c}}}}{2},{Q_{c}}} \right)}}{{\frac{{{Q_{c}}}}{S}}}} \right\rfloor } \right),
\end{equation}
where $\lfloor \enspace \rfloor$ and $\bmod$ denote the floor and mod operations, respectively, $Q_c$ denotes the beamforming codebook level, and $S$ denotes the number of available phase values for each antenna element. Each column of ${{\textit{\textbf{N}}}_{\text{CB}}}$ is a beamforming vector and denotes the codebook as $\mathcal B_c$. Denote the power codebook set as $\mathcal B_p = \{ 0,\frac{{p_{\max }}}{{{Q_{p}} - 1}}, \frac{{2p_{\max }}}{{{Q_{p}} - 1}} \ldots ,{p_{\max }}\}$, where ${Q_{p}}$ is the quantization level and ${p_{\max }}$ is the maximum transmit power of AP. Note that the specified beam direction and transmit power can be chosen according to the codewords. Without loss of generality, we set $S=16$ and $Q_c=5$ as the default value. Each AP can choose the suitable beam and power for their desired coverage areas. Denote ${{\mathbf{ w}}_m\left( t \right)}$ and $ p_m \left(t\right)$ as the beamforming vector and transmit power corresponding to the $m$-th AP serving the $k$-th UT at time $t$, respectively, under AUA configuration $\mathbf \Omega \left(t\right)$, the instantaneous signal-to-interference-and-noise ratio (SINR) at time $t$ at the link of the $k$-th UT and the $m$-th AP will be
\begin{equation}
\label{sinr}
\gamma_{k,m} \left(t\right) = \frac{{{ {\left| {\sqrt {{p_m}\left( t \right)} {\mathbf{h}}_{k,m}^\dag \left( t \right){{\mathbf{w}}_m}\left( t \right)} \right|}^2}}}{{{{\sum\limits_{c \ne k} {\sum\limits_{l \in {\mathcal {B}_c\left(t\right)}} {\left| {\sqrt {{p_l}\left( t \right)} {\mathbf{h}}_{k,l}^\dag \left( t \right){{\mathbf{w}}_l}\left( t \right)} \right|} } }^2} + {\sigma ^2}}}.
\end{equation}
It is worth noting that (\ref{sinr}) is derived based on the assumption that each UT only decodes the message from its associated AP and the received signals from the other links are treated as interference. We also assume that each UT decodes the message according the received signal strength and has a priority to decode stronger signals.

\section{Problem Formulation and Hierarchical Control Strategy}
In this section, we define and decompose our optimization problem into association as well as beamforming and power allocation sub-problems. We then introduce the multi-timescale Markov decision process.

\begin{figure}[htbp]
\centerline{\includegraphics[width=3.5in]{multi-timescaleauaandbpa.pdf}}
\caption{Illustration of the proposed multi-timescale intelligent AUA and BPA strategy.}
\label{proposedstructure}
\end{figure}

\subsection{Problem Formulation}
We aim to design an algorithm for the cell-free network with self-configuration and self-optimization capabilities. We highlight that the high-level policy and low-level policy are executed successively. The AUA is the primitive action of BPA while BPA determines the environment of AUA. But it is too complex to simultaneously adjust both policies \cite{c17}. To address this issue, we separate the original joint optimal problem into two sub-problems: High-level AUA controller and low-level BPA controller, which can be jointly optimized by a multi-layer hierarchy structure with AUA as meta-controller \cite{c18}. We define our optimization target as maximizing the accumulative spectrum efficiency (SE) gain over the whole network serving time while meeting a minimum quality of serves (QoS) requirements, $R_{t,min}^k$, of UT $k$ via finding the optimal strategies $\pi_H$ and $\pi_L$:
\begin{align}
\mathop {\max }\limits_{ {\pi _H},{\pi _L} } \mathbb E&\left[ {\sum\nolimits_{n = 0}^\infty  {\underbrace {\sum\nolimits_{k \in \mathbf \Omega_n} {\underbrace {\sum\nolimits_{t' = t}^{\left( {n + 1} \right)\Delta T - 1} {\sum\nolimits_{m \in {\mathcal {B}_k}} {\Delta R_{t'}^{k,m}} } }_{\text{link gain for mitigating interference}}} }_{\text{system gain for network optimization}}} } \right], \label{p}\\
s.t.\quad&[w_t^m,p_t^m] \sim \pi_L, \mathcal B_t^k \sim \pi_H, \forall m \in \mathcal M, \forall k \in \mathcal K, \tag{\ref{p}{a}} \label{pa}\\
&0 \leqslant p_t^m \leqslant p_{max}, \forall m \in \mathcal M, \tag{\ref{p}{b}} \label{pb}\\
&\left|w_t^m\right|^2 = 1, \forall m \in \mathcal M, \tag{\ref{p}{c}} \label{pc}\\
&\mathcal{B}_t^k \cap \mathcal{B}_t^l = \emptyset ,\forall k \ne l, \forall k,l \in \mathcal K, \tag{\ref{p}{d}} \label{pd}\\
&\sum\nolimits_{m \in \mathcal B_k} R_t^{k,m} \geqslant R_{t,min}^k, \forall k \in \mathcal K, \tag{\ref{p}{e}} \label{pe}\\
&n\Delta T \leqslant t \leqslant \left( {n + 1} \right)\Delta T -1, \tag{\ref{p}{f}} \label{pf}
\end{align}
where the objective function is a multi-timescale horizon optimal problem, $\Delta R_{t'}^{k,m} = R_{t'}^{k,m} - R_{t'-1}^{k,m}$ denotes the SE gain at time $t'$, $R_{t,min}^k$ denotes the minimum SE needs of UT $k \in \mathcal K$ at time $t$, $\pi _H$ and $\pi_L$ denote the AUA and BPA control strategy, respectively. Constraint (\ref{pa}) indicates the mapping from policies to actions. Constraints (\ref{pb}) and (\ref{pc}) limit the beam direction and total transmit power of AP. Constraint (\ref{pd}) ensures the non-overlapping subnetworks. Constraint (\ref{pe}) guarantees minimum data rate $R_{t,min}^k$ of UT $k$, which meets the diverse requirements for different UTs and provides a minimum guaranteed QoS for them and the QoS satisfaction index $\Gamma = 1$ as constraint (\ref{pe}) is satisfied, $\Gamma=0$, otherwise. The last constraint ensures the range of small-timescale action. The AUA sub-problem acts as a meta-controller to optimize the cumulative extrinsic SE gain by improving network association configuration with a certain $\pi_L$ in each epoch while the BPA sub-problem maximizes the cumulative intrinsic SE gain with a given association configuration $\mathbf \Omega$ in each $\Delta T$. Obviously, high-level policy $\pi_H$ and low-level policy $\pi_L$ are mutually condition. We will introduce the solution to this direct coupling problem in the IV section.
%\setlength{\rightmargin}{-0.75in} % For EDAS format.
%\columnsep 0.24in % For EDAS format.
\addtolength{\topmargin}{-0.2in}
\subsection{Multi-timescale Markov Decision Process}
In this work, a twin time-scale Markov Decision Process (MDP) is adopted to characterize the hierarchical control strategy \cite{mcmdp}. The multi-timescale AUA and BPA control strategies, denoted as $\pi  = \{ {\pi _H},{\pi _L}\} $, is proposed. As shown in Fig. \ref{proposedstructure}, high-level control policy $\pi_H$ is responsible for AUA at the system level, and low-level control policy $\pi_L$ is responsible for BPA at the link level. Specifically, the high-level control policy is performed at every $\Delta T$ TTIs. We define $\Delta T$ TTIs as an epoch, indexed by $n \in \{1,2,\ldots \}$. While low-level control policy is executed at every consecutive TTIs based on every fixed high-level configure (i.e., from $n\Delta T$ TTI to $\left(n+1\right) \Delta T$ TTI). Hence, the high-level control process is an infinite horizon MDP at the large timescale while the low-level control process is in a finite horizon MDP at the small timescale. Furthermore, continuous low-level control can be further modeled as finite horizon partially observable MDP (POMDP) in order to reduce the heavy stress on the backhaul/fronthaul link caused by signaling overhead.

The high-level controller will tune the AUA configuration to improve the transmit performance of the whole network at every epoch. Note that the high-level controller is not directly involved in the real-time association while performing the long-term farsighted action based on a period of historical experience at the large timescale. Given AUA configuration $\mathbf \Omega_n$ and low-level control policy $\pi_L$, the low-level Q function (action-value function) is defined as. (For a more compact representation, we write the time variable $t$ to the subscript.)
\begin{align}
\nonumber
\label{lowQ}
{Q_L}\left( {{S_t},{\Xi _t};\mathbf \Omega_n} \right) &= {{\mathbb E}_{\tau_ {  > t}}}\left[ {\sum\limits_{t' = t}^{\left( {n + 1} \right)\Delta T - 1} {\sum\limits_{m \in \mathcal B_k} {R_{t'}^{k,m}} } |{S_t},{\Xi _t}, \mathbf \Omega_n} \right],\\
s.t.\quad &n\Delta T \leqslant t \leqslant \left( {n + 1} \right)\Delta T
\end{align}
where ${\tau _{ > t}} = \left\{ {\left( {{S_t},{\Xi _t}} \right),\ldots,\left( {{S_{(n + 1)\Delta T}},{\Xi _{(n + 1)\Delta T}}} \right)} \right\}$ denotes a sample trajectory of low-level states and actions pair after time $t$, $R_{t'}^{k,m}={\log _2}\left( {1 + \gamma _{t'}^{k,m}} \right)$ denotes the SE at UT $k$ at time $t'$, $S_t$ and $\Xi _t$ denote the low-level states and low-level actions at time $t$, respectively. On the other hand, if given control policy $\pi$ and the high-level states $X_n$, the high-level Q-function can be expressed as:
\begin{align}
\nonumber
\label{highQ}
&{Q_H}\left( {{X_n},\mathbf \Omega_n} \right) \\
&= {{\mathbb E}_{{\tau _H}}}\left[ {\sum\limits_{n' = n}^\infty  {{\lambda ^{n' - n}} \left( {{C_L}\left( {{\pi _L};\mathbf \Omega_{n'}} \right)+ R_{n'} } \right)} |\mathbf \Omega_n,{\pi _L}} \right],
\end{align}
where ${\tau _H} = \left\{ {\left( {{X_n}, \mathbf \Omega_n} \right), \left( {{X_{n+1}}, \mathbf \Omega_{n+1}} \right), \ldots} \right\}$ denotes a full trajectory of high-level states and actions, $R_{n'}=\sum\nolimits_{k \in \mathcal K} \sum\nolimits_{m \in \mathcal B_k} {R_{n'}^{k,m}}$, $\lambda$ is the discount factor. $C_L \left(\right)$ is the low-level expected cumulative reward, which is determined by low-level control policy $\pi_L$ and association configuration $\mathbf \Omega$ and defined as follows:
\begin{equation}\label{lecr}
{C_L}\left( {{\pi _L};{\mathbf \Omega}} \right) = {\mathbb E_{\tau_L} }\left[ {{Q_L}\left( {{S_{n\Delta t}},{\Xi _{n\Delta t}};\mathbf \Omega} \right)|\mathbf \Omega} \right],
\end{equation}
where $\tau_L  = \left\{ {\left( {{S_{n\Delta t}},{\Xi _{n\Delta t}}} \right), \ldots ,\left( {{S_{(n + 1)\Delta T}},{\Xi _{(n + 1)\Delta T}}} \right)} \right\}$ denotes a sample trajectory of low-level states and actions during the $n$-th epoch. Similarly, high-level control policy $\pi_H$ depends on low-level $\pi_L$, and the high-level value-function can be defined as
\begin{equation}\label{hecr}
{V_H}\left( {{X_n}} \right) = {\mathbb E_{\mathbf \Omega_n}}\left[ {{Q_H}\left( {{X_n},\mathbf \Omega_n} \right)|{\pi _L}} \right].
\end{equation}

However, these two decision processes will have a large limitation in the dimension explosion problem caused by the increasing number of APs and UTs. In the considering network, the action spaces of the high-level control policy and the low-level control policy are $\left|\mathcal A_H \right| = K^M$ and $\left|\mathcal A_L \right| = \left(Q_cQ_p\right)^M$, respectively. To cope with this challenge, we will propose a distributed DRL structure in both high- and low-level control policies in the next section.

\section{Solution Based on Multi-time scale DRL Framework}
As discussed earlier, twin centralized approaches may suffer from dimension explosion caused by increasing serving APs, which will become intractable because of the exponential growth of the number of served UT increases. The objective problem is difficult to solve with the existing DRL algorithms. Hence, in this section, a distributed two-tier structured DRL framework is proposed, as shown in Fig \ref{DHC}, in which the high-level control policy and the low-level control policy successively work in a distributed manner.

\begin{figure}[htbp]
\centerline{\includegraphics[width=3.5in]{dishiercontrol}}
\caption{Illustration of hierarchical DRL for distributed multi-agent cooperation.}
\label{DHC}
\end{figure}

\subsection{Desgin of Beamforming and Power Allocation}
Given the fixed AP-UT association configuration, we only focus on the multi-AP cooperative BPA at a small timescale in this part. The responsibility of the low-level controller is to mitigate inter-AP interferences and improve the transmit performance in a private link. Distributed low-level control can be designed as follows.

\textbf{Low-level Observation:} States are defined as the overall system states. However, it is impractical and undesirable to observe the global state of the entire system due to privacy or latency. Here, we consider local observation for the AP $m$ at time slot $t$
\begin{equation}
\label{E10}
{O_{L,t}^m} = \left( {{p_{t-1}^m},{{\mathbf{w}}_t^m},{\gamma_t^m},{{\mathbf{b}}_t^m},{{\mathbf{i}}_t^m}}, {\mathcal {B}_t^m} \right),
\end{equation}
where ${{\mathbf{b}}_t^m}$ and ${{\mathbf{i}}_t^m}$ are the equivalent channel gains and the total interference-plus-noise power set at AP $m$ and time $t$, respectively, ${{\mathbf{b}}_t^m} = \left\{ {{b_t^m}' ,{b_{t-1}^m}'} \right\}$ and ${{\mathbf{i}}_t^m} = \left\{ {{i_t^m}' ,{i_{t-1}^m}'} \right\}$, with ${b_t^m}' = {{\sum\nolimits_{n \in {\mathcal {B}_k}} {\left| {\sqrt {{p_t^n}} {{\mathbf{h}}_t^{k,n}}^\dag {{\mathbf{w}}_t^n}} \right|} }^2}$ and ${i_t^m}' = {{{\sum\nolimits_{c \ne k} {\sum\nolimits_{j \in {\mathcal {B}_c}} {\left| {\sqrt {{p_t^j}} {{\mathbf{h}}_t^{k,j}}^\dag {{\mathbf{w}}_t^j}} \right|} } }^2} + {\sigma ^2}}$, respectively. Note that AP $m$ just knows the association configuration itself at large-timescale while the association configurations of others is not required. On the other hand, all observations are scalar values only based on partial CSI statistics, rather than the exact CSI matrix, which will save a lot of signaling overhead and channel training time.%

\textbf{Low-level Action:} Action $a \in \mathcal A_L$ for the BPA is a one-hot mapping from APs to beamforming and power codeword pairs. Each AP has $Q_cQ_p$ actions to select, in which the action pairs of the low-level policy at time $t$ is denoted as $\Xi_t$ as described earlier, i.e. $\Xi_t=\{w_t, p_t \}$.

\textbf{Low-level Reward:} Due to geographic isolation and electromagnetic propagation loss, the APs far away always have less interference while nearby APs always have stronger interference. In this way, we only need to consider the negative effects of nearby APs, which are usually a small portion of the whole system and significantly reduce signaling overhead. We define the reward as the average SE performance of current agent $m$, i.e. $r_{L,t}^m = {\raise0.7ex\hbox{$1$} \!\mathord{\left/
 {\vphantom {1 {\left| {{J^m}} \right|}}}\right.\kern-\nulldelimiterspace} \!\lower0.7ex\hbox{${\left| {{J^m}} \right|}$}}\sum\nolimits_{j \in {J^m}} {R_t^j} $, and its neighbors within a circle of fixed radius $D$, where $J_m$ is the neighbor(s) set of agent $m$.

\subsection{Design of Association and Multi-timescale DRL Algorithm}
After solving the BPA problem, we try to deal with the AUA challenge in this subsection. Conditioned on the fixed BPA configuration, the high-level control policy can be modeled with a typical infinite-horizon discrete-time MDP with discrete action space. Similar to low-level control policy, high-level control strategy consists of states, actions and policies. In order to capture the states of UT distributed in various locations, we adopt an observable method consisting of partial observation variables and global observation variables, in which the agent can also observe the relative positions of all UTs to all APs $\mathcal {D}_t$, besides low-level policy observation $O_{L,t}$. This information can be obtained by information transmitted in the backhaul link.

\textbf{High-level Observation:} The high-level observation function for AP $m$ at time $t$ can be defined as ${O_{H,t}^m} = \left( {{p_{t-1}^m},{{\mathbf{w}}_t^m},{\gamma_t^m},{{\mathbf{b}}_t^m},{{\mathbf{i}}_t^m}}, {\mathcal {D}_t^m} \right)$. \textbf{High-level Action:} The action space can be defined as $\mathcal {A}_H \in \mathcal M$. \textbf{High-level Reward:} We define the high-level reward as $r_{H,t}^m={\raise0.7ex\hbox{$1$} \!\mathord{\left/
{\vphantom {1 K}}\right.\kern-\nulldelimiterspace} \!\lower0.7ex\hbox{$K$}}\sum\nolimits_{k \in \mathcal K} {\mathds 1\left[ {\sum\nolimits_{m \in {\mathcal B_k}} {R_t^{k,m} \geqslant } R_{t,\min }^k} \right]} $ to ensure that all users are served fairly, where $\mathds 1 [x]=1$ as $x$ is true,  $\mathds 1 [x]=-1$, otherwise. It is worth noting that the expressions of the high- and the low-level rewards are different, but they are both precise parts of the global reward associated with them, and the constraints on both high-level policy and low-level policy are the same, which guarantees a monotonic improvement in the SE for the entire system. The idea of multi-timescale hierarchical DRL is updating the two networks, the high-level network and the low-level network successively, in which the agent continuously adjusts the high-level and the low-level decisions to improve the transmit performance of the overall system. Specifically, the high-network policy updates based on a fixed low-level policy to improve the cell-free system configuration at the system level, and the low-level policy updates based on a more convergent high-network policy to mitigate inter-AP interference at the link level. The two networks iteratively update each other to get both optimal policies. In this paper, we select the value-function network, i.e. deep Q-learning (DQN) with long short-term memory (LSTM) network for the low-level policy and DQN for the high-level policy, that provide distributed estimation capabilities to build both networks. All APs have similar neural networks.

\section{Numerical Results}
In this section, we numerically examine the proposed distributed hierarchically DRL-based joint AUA and BPA scheme in terms of generalization ability and transmission performance. Software including Tensorflow 2.0, Keras 2.6 and MATLAB. We consider a network scenario, where 6 APs and 3 UTs are randomly distributed in a square area of $600 \times 600\ \rm{m}^2$, in which APs are equipped with 4 antennas and maximal transmit power is 5 dBm. The path loss, between AP $m$ and UT $k$ is ${\beta _{m,k}}=120.9 + 37.6{\log _{10}}{d_{m,k}}$ dB, where ${d_{m,k}}$ is the distance between AP $m$ and UT $k$. The AWGN power $\sigma^2$ is $-114$ dBm. The number time slot to re-association is $\Delta T =10$. The low-level reward radius $D=100$. The channel correlation coefficient $\rho=0.64$. The minimal required transmission rate of UTs, $R_{t,min}^k$, is randomly generated from $[5, 10]$ bps/Hz.

\begin{algorithm}[t]
\caption{Proposed DHDRL based joint AUA and BPA.}
\begin{algorithmic}[1]
\item
	\textbf{Initialize:} High- and low-level network parameters;
\FOR{$t=1,2,3,\ldots,T$}
\IF{$t$ can be divided by $\Delta T$}
\FOR {$\forall m \in \mathcal M$}
\STATE Store experience tuple $\left( {{O_{H,t-\Delta T}^m},{a_{H,t-\Delta T}^m},{\sum\nolimits_{t' = t - \Delta T}^t {r_{H,t'}^m}},{O_{{H,t}}^m}} \right)$ to high-level memory pool ${\psi _H^m}$;
\STATE Obtain high-level observation $O_{H,t}$ and makes association decision ${\mathbf \Omega_t}$;
\STATE Sample a random mini-batch from high-level memory pool ${\psi _H^m}$;
\STATE Train and update high-level network's parameters with sampled memories;
\ENDFOR
\ENDIF
\FOR {$\forall m \in \mathcal M$}
\STATE Make decisions based on ${O_{L,t}^m}$ and ${\mathbf \Omega_t}$ according to $\varepsilon $-greedy;
\STATE Calculate the local reward value ${r_{L,t}^m}$ corresponding to actions;
\STATE Get next observations ${O_{L,t}^m}$;
\STATE Save the new experience $\left( {{O_{L,t}^m},{a_{L,t}^m},{r_{L,t}^m},{O_{{L,t+1}}^m}} \right)$ into memory pool ${\psi _L^m}$;
\STATE Sample a random mini-batch from its own memory pool ${\psi _L^m}$;
\STATE Update the weights of its trained network: ${\phi _{L,t}^m}$;
\STATE Update the target network parameters $\phi_{{L,t^-}}^m = {\phi _{L,t}^m}$ at every $T_b$ time slot.
\ENDFOR
\ENDFOR
\label{code:algorithm1}
\end{algorithmic}
\end{algorithm}

For each AP, the high-level network includes an input layer with 30 neurons, a hidden layer with 64 neurons, and an output layer with one neuron. The low-level network includes an input layer with 8 neurons, a hidden layer with 64 neurons, an LSTM network with 100 neurons and an output layer with one neuron, where other hyperparameters are the same as the high-level network. ReLU function is selected as the activation function. The learning rate is $\delta=0.0005$ and learning rate decay factor is ${\delta_d}=0.0001$. Moreover, the size of the memory pool ${\psi _H}$ at each AP for high-level is 500, the mini-batch is ${\psi _b}=32$, memory step is ${\psi _s}=8$, future discount is $\lambda=0.5$, and the adaptive greedy strategy is $0.1\leqslant \varepsilon  \leqslant0.6$. We replace the target network every 100 slots to avoid correlation between action-values and target values, i.e. $T_b=100$.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[width=1.1\linewidth]{signalingcompara}
		\caption{Comparison of signaling bits of two schemes.}
		\label{Signalingcompara}%文中引用该图片代号
	\end{minipage}
	%\qquad
	\begin{minipage}{0.45\linewidth}
		\centering
		\includegraphics[width=1.1\linewidth]{actionhotmap}
		\caption{Heatmap of actions of APs.}
		\label{hotmap}%文中引用该图片代号
	\end{minipage}
\end{figure}

Firstly, we compare the number of required signaling bits of working process in fronthaul/backhaul link for the proposed DHDRL scheme, continuously distributed AUA and BPA scheme as well as centralized AUA and distributed BPA scheme. We assume that $B$ is the number of quantization bits for each message, thus the number of signaling bits of the proposed scheme in an epoch can be expressed as $B\left(M^2+MK+M \right)+B\Delta T\left(\sum\nolimits_{m = 1}^{M}|J^m|+M+3\right)$. Similarly, the distributed continuous joint AUA and BPA algorithm in a single epoch is $B\Delta T \left(M^2+MK+M+\sum\nolimits_{m = 1}^{M}|J^m|+3\right)$. Centralized AUA and distributed BPA scheme in a single epoch is $B\Delta T \left(M^2KN+M+\sum\nolimits_{m = 1}^{M}|J^m|+M+3\right)$. Fig. \ref{Signalingcompara} shows the cumulative number of signaling bit versus the number of epochs, where we employ $B=32$. From the figure, the proposed DHDRL can significantly reduce the system signaling overhead compared to the two representative schemes. Fig. \ref{hotmap} is a heatmap of index of association, beam and power allocation over the whole epochs. At the high-level action, APs $1$ and $5$ are more likely to connect the UT $1$, APs $3$ and $4$ prefer to connect UT $3$, while APs $2$ and $6$ tend to serve UT $2$. About low-level action, APs $1$, $2$ and $4$ all have various beam directions, APs $5$ and $6$ prefer relative stable beams, but AP $3$ hesitates to select beam direction. The main reason is that the configurations of association and beamforming rely on the UT's requirement and channel change, in which APs need frequent configuration adjustments for better user QoS and SE performance. On the contrary, all APs prefer to choose higher transmit power that can significantly improve the SE of current link.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=1.09\linewidth]{highreward}
		\caption{High-level reward.}
		\label{HighReward}%文中引用该图片代号
	\end{minipage}
	%\qquad
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=1.09\linewidth]{lowreward}
		\caption{Real-time SE performance.}
		\label{RealtimeSE}%文中引用该图片代号
	\end{minipage}
\end{figure}
Next, we evaluate the convergence performance of the proposed DHDRL architecture. We employ DHDRL with DQN network (DHDRL w/LSTM) as  reference comparison. Fig. \ref{HighReward} plots the reward variation of two different high-level control policies during the learning procedure, in which both policies both converge. Secondly, Fig. \ref{RealtimeSE} and \ref{QoSprob} show the real-time SE performance and QoS satisfaction probability of DHDRL and DRDRL w/LSTM, respectively, in the decision-making and convergence of joint high-level and low-level control policies, in which the curves are poor at the beginning, but increases fast with the number of epochs increases. The reason is that the two schemes are the off-line learning algorithm, which needs multiple interactions with the environment to train their own neural weights. Furthermore, Fig. \ref{QoSprob} demonstrates that the designed DHDRL scheme can meet the UT's requirements. On the other hand, DHDRL can get more stable and smooth performance than DHDRL w/LSTM, mainly because the LSTM network can memorize the intermediate state of environment and effectively eliminate the problem of fuzzy observations. At last, the three figures confirm that the proposed algorithm can avoid the mis-convergence and unstable issues in learning and decision processing. Fig. \ref{ComCDF} compares the cumulative distribution function (CDF) of the transmitting performance of proposed DHDRL with different codebook levels $\left(Q_c, Q_p\right)$. From the figure, $\left(8, 5\right)$ gets the better performance and $\left(4, 8\right)$ gets worse SE performance. While the available power level fixed, the high number of beam level $Q_c$ will result in a high beam solution that provides higher beam pointing accuracy and higher performance.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=1.1\linewidth]{qosprob}
		\caption{Average value of satisfaction probability (over 50 epochs).}
		\label{QoSprob}%文中引用该图片代号
	\end{minipage}
	%\qquad
	\begin{minipage}{0.49\linewidth}
		\centering
		\includegraphics[width=1.1\linewidth]{cbcompara}
		\caption{CDF of DHDRL performance with different codebook level.}
		\label{ComCDF}%文中引用该图片代号
	\end{minipage}
\end{figure}

\section{Conclusion}
In this paper, we have proposed a novel distributed joint AP-UT association, beamforming, and power allocation scheme with multi-timescale control in a cell-free MISO network by decoupling it into two sub-problems with infinite MDP and a POMDP. To cope with the joint optimization problems, we have proposed a distributed hierarchical DRL architecture with the AUA as meta-controller and BPA as the controller to solve the joint problems. Numerical evaluations demonstrate that the proposed algorithms can effectively handle high-dimensional problems and are promising for multi-object problems. The simulation codes are available at: \textbf{https://github.com/Kiven-ykw/DHDRL.git}.

\section{Acknowledgment}
This work is in part supported by the National Key R\&D Program of China (No. 2020YFB1806604).

\begin{thebibliography}{00}

\bibitem{c1} H. A. Ammar, R. Adve, S. Shahbazpanahi, G. Boudreau, and K. V. Srinivas, ``User-centric cell-free massive MIMO networks: A survey of opportunities, challenges and solutions,'' \textit{IEEE Commun. Surveys Tuts}, vol. 24, no. 1, pp. 611-652, 2022.
\bibitem{c3} E. Bj{\"o}rnson and L. Sanguinetti, ``Making cell-free massive MIMO competitive with MMSE processing and centralized implementation,'' \textit{IEEE Trans. Wireless Commun.}, vol. 19, no. 1, pp. 77-90, Jan. 2020.
\bibitem{add1} D. Wang, X. You, Y. Huang, et al, `` Full-spectrum cell-free RAN for 6G systems: system design, and experimental results,'' \textit{Sci. China Inf. Sci.}, vol. 66, no. 3, pp. 130305, 2023.
\bibitem{c4}M. Bashar, K. Cumanan, A. G. Burr, M. Debbah, and H. Q. Ngo, ``On the uplink max-min SINR of cell-free massive MIMO systems,'' \textit{IEEE Trans. Wireless Commun.}, vol. 18, no. 4, pp. 2021-2036, April 2019.
\bibitem{c5} H. A. Ammar, R. Adve, S. Shahbazpanahi, G. Boudreau and K. V. Srinivas, ``Distributed resource allocation optimization for user-centric cell-free MIMO networks,'' \textit{IEEE Trans. Wireless Commun.}, vol. 21, no. 5, pp. 3099-3115, May 2022.
\bibitem{c6} I. Atzeni, B. Gouda and A. T{\"o}lli, ``Distributed precoding design via over-the-Air signaling for cell-free massive MIMO,'' \textit{IEEE Trans. Wireless Commun.}, vol. 20, no. 2, pp. 1201-1216, Feb. 2021.
\bibitem{c8} S. Huang, Y. Ye, M. Xiao, H. V. Poor and M. Skoglund, ``Decentralized beamforming design for intelligent reflecting surface-enhanced cell-free networks,'' \textit{IEEE Wireless Commun. Lett}, vol. 10, no. 3, pp. 673-677, March 2021.
\bibitem{T1} ``3GPP TR38.801: Study on new radio access technology: Radio access architecture and interfaces,'' \textit{Tech. Rep.}, 2017. [Online]. Available: http://www.3gpp.org/ftp/Specs/archive/38\_series/38.801.
\bibitem{a1} X. You et al. ``Towards 6G wireless communication networks: vision, enabling technologies, and new paradigm shifts,'' \textit{Sci. China Inf. Sci.}, vol. 64, no. 1, pp.140303, 2021.
\bibitem{wa} Z. Wang, Y. Du, K. Wei, et al. ``Vision, application scenarios, and key technology trends for 6G mobile communications,'' \textit{Sci China Inf Sci.}, Vol. 65, no.5, pp.151301, 2022.
\bibitem{c9} E. Bj{\"o}rnson and L. Sanguinetti, ``Scalable cell-free massive MIMO systems,'' \textit{IEEE Trans. Wireless Commun.}, vol. 68, no. 7, pp. 4247-4261, Jul. 2020.
\bibitem{c10} M. Attarifar, A. Abbasfar, and A. Lozano, ``Subset MMSE receivers for cell-free networks,'' \textit{IEEE Trans. Wireless Commun.}, vol. 19, no. 6, pp. 4183-4194, Jun. 2020.
\bibitem{b8} X. Xia, P. Zhu, J. Li, et al. ``Joint optimization of spectral efficiency for cell-free massive MIMO with network-assisted full duplexing,'' \textit{Sci. China Inf. Sci.}, vol. 64, no. 8, pp.182311, 2021.
\bibitem{c11} K. Yu, G. Wu, S. Li and G. Y. Li, ``Local observations-based energy-efficient multi-cell beamforming via multi-agent reinforcement learning,'' \textit{J. Commun. Inf. Netw.}, vol. 7, no. 2, pp. 170-180, June 2022.
\bibitem{f1} Q. Xue, Y. Sun, and J. Wang, et al., ``User-centric association in ultra-dense mmWave networks via deep reinforcement learning," \textit{IEEE Commun. Lett.}, vol. 25, no. 11, pp. 3594-3598, Nov. 2021.
\bibitem{T2} 5G evolution toward 5G advanced: An overview of 3GPP releases 17 and 18 [Online]. Available: https://www.ericsson.com/en/reports-and-papers/ericsson-technology-review/articles/5g-evolution-toward-5g-advanced, 2021.
\bibitem{b2} F. Sohrabi, Z. Chen, and W. Yu, ``Deep active learning approach to adaptive beamforming for mmWave initial alignment," \textit{IEEE J. Sel. Areas Commun.}, vol. 39, no. 8, pp. 2347-2360, Aug. 2021.
\bibitem{b3} H. J. Kwon, J. H. Lee, and W. Choi, ``Machine learning-based beamforming in two-user MISO interference channels,'' in \textit{Proc. Int. Conf. Artif. Intell. Inf. Commun. (ICAIIC)}, Okinawa, Japan, Feb. 2019, pp. 496-499.
%\bibitem{c12} M. Ma and V. W. S. Wong, ``Joint user pairing and association for multicell NOMA: A pointer network-based approach,'' \textit{2020 IEEE ICC Workshops}, 2020, pp. 1-6.
\bibitem{c13} Q. Zhang, Y. Liang and H. V. Poor, ``Intelligent user association for symbiotic radio networks using deep reinforcement learning,'' \textit{IEEE Trans. Wireless Commun.}, vol. 19, no. 7, pp. 4535-4548, July 2020.
\bibitem{c14} Y. -J. Chen and D. -Y. Huang, ``Joint trajectory design and BS association for cellular-connected UAV: An imitation-augmented deep reinforcement learning approach,'' \textit{IEEE Internet of Things J.}, vol. 9, no. 4, pp. 2843-2858, 15 Feb.15, 2022.
\bibitem{c16} W. Zou, Z. Cui, B. Li, Z. Zhou, and Y. Hu, ``Beamforming codebook design and performance evaluation for 60 GHz wireless communication,'' \textit{in Proc. 11th Int. Symp. Commun. Inf. Technol. (ISCIT)}, Hangzhou, China, Oct. 2011, pp. 30-35.
\bibitem{c17} H. Peng and X. Shen, ``Multi-agent reinforcement learning based resource management in MEC-and UAV-assisted vehicular networks,'' \textit{IEEE J. Sel. Areas Commun.}, vol. 39, no. 1, pp. 131-141, Nov. 2021.
\bibitem{c18} T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, ``Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation,'' \textit{Proc. Neural Info. Proc. Systems}, 2016, pp. 3675-3683.
\bibitem{mcmdp} H. S. Chang, P. J. Fard, S. I. Marcus, and M. Shayman, ``Multitime scale markov decision processes,'' \textit{IEEE Trans. Autom. Control}, vol. 48, no. 6, pp. 976-987, Jun. 2003.
\end{thebibliography}
\vspace{12pt}
\vfill

\end{document}


