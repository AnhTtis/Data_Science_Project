{
    "arxiv_id": "2303.08431",
    "paper_title": "Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators",
    "authors": [
        "Yinbin Han",
        "Meisam Razaviyayn",
        "Renyuan Xu"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2024-08-13"
    ],
    "latest_version": 4,
    "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
    ],
    "abstract": "Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08431v1",
        "http://arxiv.org/pdf/2303.08431v2",
        "http://arxiv.org/pdf/2303.08431v3",
        "http://arxiv.org/pdf/2303.08431v4"
    ],
    "publication_venue": "34 pages"
}