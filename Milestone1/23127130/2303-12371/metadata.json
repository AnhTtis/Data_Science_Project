{
    "arxiv_id": "2303.12371",
    "paper_title": "$P^{3}O$: Transferring Visual Representations for Reinforcement Learning via Prompting",
    "authors": [
        "Guoliang You",
        "Xiaomeng Chu",
        "Yifan Duan",
        "Jie Peng",
        "Jianmin Ji",
        "Yu Zhang",
        "Yanyong Zhang"
    ],
    "submission_date": "2023-03-22",
    "revised_dates": [
        "2023-03-23"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "abstract": "It is important for deep reinforcement learning (DRL) algorithms to transfer their learned policies to new environments that have different visual inputs. In this paper, we introduce Prompt based Proximal Policy Optimization ($P^{3}O$), a three-stage DRL algorithm that transfers visual representations from a target to a source environment by applying prompting. The process of $P^{3}O$ consists of three stages: pre-training, prompting, and predicting. In particular, we specify a prompt-transformer for representation conversion and propose a two-step training process to train the prompt-transformer for the target environment, while the rest of the DRL pipeline remains unchanged. We implement $P^{3}O$ and evaluate it on the OpenAI CarRacing video game. The experimental results show that $P^{3}O$ outperforms the state-of-the-art visual transferring schemes. In particular, $P^{3}O$ allows the learned policies to perform well in environments with different visual inputs, which is much more effective than retraining the policies in these environments.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12371v1"
    ],
    "publication_venue": "This paper has been accepted to be presented at the upcoming IEEE International Conference on Multimedia & Expo (ICME) in 2023"
}