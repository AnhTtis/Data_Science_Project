\section{Introduction}
% Motivation 1 why SYCL
%NVIDIA GPUs, AMD GPUs, Intel GPUs, CPUs -- in today's world every HPC developer is confronted with multiple potential target machines for which to develop. 
%One way to tackle this heterogeneity is SYCL, which allows use to write compute kernels directly within our C\texttt{++} source code and use them on a wide range of potential hardware.

%Of course, SYCL as other advantages as well. For instance, it is asynchronous by design. 
%Using it, we can asynchronously define an entire graph of commands/kernels, defining the order in which kernels need to be run, but leaving room for optimizations as some of those kernels might be able to run concurrently if the graph allows it, in turn enabling better performance.

%However, eventually, we need the results of these calculations on the host, be it for post-processing, outputting data, or communicating it with other compute nodes.
%Usually, we can just call wait for the SYCL event in question (or for the entire SYCL queue), however, this blocks this host thread until said results arrive.
%Can we do better? 
%How bad can the performance impact of waiting on the CPU-side in a GPU-enabled HPC application be anyways?

%This works targets these exact questions.
%To address the first one and avoid blocking host threads, we developed an HPX-SYCL integration.
%HPX is an asynchronous, distributed, task-based runtime system \cite{kaiser2020hpx}, which allows us to define a task-graph asynchronously, even across multiple compute nodes in distributed HPC applications. HPX aims at reducing implicit and explicit barriers to an absolute minimum to avoid wasting CPU cycles on waiting for results while there are still other tasks to be done.

%In this work, we use HPX to complement SYCL by enabling it to jump in at places where we need to handle the final results of any SYCL graph asynchronously without blocking the execution until all SYCL operations finish. We do that by teaching HPX to treat SYCL events as HPX tasks, thus being able to define asynchronously define subsequent tasks for this HPX-SYCL tasks (such as communicating results to other nodes for instance).
%This way, we get the best of two worlds: SYCL itself still handles the dependencies between local compute kernels (allowing of potential out-of-order optimizations), while HPX can define follow-up host side tasks for the results without adding any unnecessary barriers.

%The second question regarding the performance impact of waiting is of course highly application-dependent.
%However, to obtain some data regarding this, we further integrate this into a real-world HPC application: Octo-Tiger.
%To address the second question, we further integrate this into a real-world HPC application: Octo-Tiger.
%This allows us to benchmark our integration in an real-world application, gaining runtime data on the performance impact (for this specific application) when we actually have to wait for results and block CPU threads and when we do not have to do this.

%As for the chosen application itself, Octo-Tiger is an astrophysics code, used to study binary star systems and stellar mergers.
%These are modeled as self-gravitating astrophysical fluids on an 3D grid. To reduce the computational effort, Octo-Tiger relies heavily on adapative mesh refinement.
%Thus, Octo-Tiger is built on HPX to exploit the maximum amount of parallelism during the tree-traversals within its solver.
%Recently, all major compute kernels within Octo-Tiger were ported to Kokkos to improve portability.

%Given its usage of HPX and Kokkos, Octo-Tiger is the ideal choice for testing this integration!
%This provides us with the opportunity to test the HPX-SYCL integration, as Kokkos already contains a SYCL execution space which we can exploit for this.
%Although, to do this, we needed to add another integration that allows Kokkos kernels running on the SYCL execution space to return HPX futures when being launched.

%Overall, this work contains three contributions:
%\begin{enumerate}
%\item We add a HPX-SYCL integration.
%\item We made Octo-Tiger compatible with SYCL, going over the Kokkos execution space which required further integrations (and adaptations within Octo-Tiger).
%\item We collected runtime data using Octo-Tiger on an AMD MI100 and an NVIDIA A100, using not only the SYCL execution space, but also the same compute kernels running on the respective native execution spaces (CUDA, HIP), as well as their predecessor kernels which are implemented in pure CUDA and HIP. We collect some of this data with the HPX-SYCL integration turned on, other with it being turned off.
%\end{enumerate}

%Thus, the remainder of the work is structured as followed: In the next section, we introduce HPX and SYCL in more detailed. Afterward, in Section 3, we introduce the HPX-SYCL integration, both in its basic form, and as an HPX SYCL executor. Afterward in Section 4 we introduce Octo-Tiger, its required software stack, and the additional changes that were necessary to make it use both the Kokkos SYCL execution space and the HPX-SYCL integration.
%Given this, we can move to the results in Section 5 where we look at Octo-Tiger's performance using various compute backends and test the performance with and without the integration.
%Finally, we finish up with a conclusion and ideas for future work.

Modern GPU-supercomputers like ANL's Aurora, NERSC's Perlmutter and ORNL's
Frontier contain thousands of compute nodes each, but use different GPUs,
ranging from NVIDIA\textsuperscript{\textregistered} GPUs to
AMD\textsuperscript{\textregistered} GPUs and
Intel\textsuperscript{\textregistered} GPUs. Developing portable
High-Performance-Computing (HPC) applications for these kinds of systems
requires both compute kernels that can efficiently target all these different
GPUs (and ideally still work well on CPUs), as well as efficient communication
and work scheduling to avoid bottlenecks when scaling to more GPU nodes,
enabling distributed scalability.
In Octo-Tiger, an C\texttt{++} astrophysics code used to simulate binary star
systems and stellar mergers~\cite{marcello2021octo}, we address this by using a
combination of Kokkos~\cite{9485033} and the C\texttt{++} standard library for
parallelism and concurrency (HPX)~\cite{kaiser2020hpx}. With Kokkos, we can
write portable compute kernels running on both CPU and GPU thanks to the
various memory and execution spaces within. We further use HPX, a distributed
asynchronous many-task runtime system, to express dependencies between kernels
with HPX futures, call methods on remote compute nodes, overlap communication
and computation, and ultimately scale to thousands of compute nodes on machines
like CSCS's Piz Daint~\cite{10.1145/3295500.3356221} or ORNL's Summit
\cite{diehl2021octo}.

In previous work, we integrated Kokkos with HPX, allowing us to obtain HPX
futures for asynchronous Kokkos kernel launches and deep copies, thus
embedding them into the HPX task graph seamlessly~\cite{daiss2021beyond}. With this
integration, we do not need to explicitly synchronize the GPU when we need the
results on the host. Instead, we simply create a continuation for the
respective HPX futures, using the results automatically when they are available
(for instance, post-processing on the host or communication).

However, this HPX-Kokkos integration only works for some Kokkos execution
spaces (the CUDA\textsuperscript{\textregistered}, HIP and HPX execution spaces to be precise). For it to
function, there needs to be a deeper integration between HPX and the respective
underlying API,
allowing us to obtain an HPX future for asynchronous API calls (for example for \lstinline[language=c++]{cudaLaunchKernel}).

On unsupported execution spaces, the HPX-Kokkos integration has to wait (fence) for the
GPU results to arrive first and then return a ready dummy future (for
compatibility, even though at this point it is not asynchronous anymore). This
wastes CPU time, as the wait is actively blocking the CPU thread calling it
until the GPU kernel and associated memory copies are done, which in turn
delays other CPU tasks such as communication or working on more GPU kernel
launches. Notably, Kokkos already contains a
SYCL\textsuperscript{\texttrademark} execution space to support Intel GPUs,
however, it is not yet supported by HPX-Kokkos.

In this work, we address this by introducing an \textbf{HPX-SYCL} integration,
therefore extending the number of supported execution spaces of HPX-Kokkos by
the SYCL execution space and test it and its
performance with Octo-Tiger on both NVIDIA and
AMD GPUs (as we lack access to current Intel GPUs).

Of course, this integration also allows using SYCL and HPX together without
using Kokkos, as it makes the HPX task graph aware of asynchronous SYCL
operations. This allows a user to, for instance, define an entire graph of operations with
SYCL as usual, but in the end also to get one HPX future for the final result, which can be
used to asynchronously schedule communication, or arbitrary other CPU
tasks, with an HPX continuation.

%For this HPX-SYCL integration, we investigate two modes for implementing it:
We investigate two alternative methods of implementing this HPX-SYCL integration:
SYCL already contains a way to asynchronously schedule CPU work within its own
graph using host tasks depending on SYCL events. One
way to integrate HPX and SYCL is to directly use such a SYCL \lstinline[language=c++]{host_task} to set an associated HPX future to the ready
state once the events it depends on are completed. 
However, we also show an additional way of integrating the frameworks which
does not rely on these SYCL host tasks and instead uses
event polling implemented within the HPX scheduler, as we found that this
vastly outperforms the \lstinline[language=c++]{host_task} alternative.

Therefore, the contributions of this work are as follows:
\begin{enumerate}
\item We added an HPX-SYCL integration.
\item We made Octo-Tiger compatible with SYCL, using the Kokkos SYCL execution
  space and our HPX-SYCL integration. This required additional modifications in other repositories (and
    adaptations within Octo-Tiger itself).
\item We collected runtime data using Octo-Tiger on an AMD MI100 and an
  NVIDIA A100, using not only the SYCL
    execution space, but also the same Kokkos compute kernels running on the
    respective native execution spaces (CUDA and
    HIP), as well as their predecessor kernels which are implemented in pure
    CUDA and HIP. We collected some of this data
    with the HPX-SYCL integration turned on, and other data with it turned off.
\end{enumerate}

These contributions yield multiple benefits: Octo-Tiger users benefit from an
additional Kokkos execution space being fully available (asynchronous instead of
blocking), potentially allowing it to run on
Intel GPUs and Aurora in the future. HPX
application developers benefit from the HPX-SYCL integration because of the ability to
fully incorporate SYCL into their own applications (either by using the Kokkos SYCL
execution space and HPX-Kokkos or by using pure SYCL). Although combining HPX with
pure SYCL might seem like an odd combination at first, since both frameworks
include task-based capabilities, they can actually complement each other well,
with SYCL taking care of performance-portable compute kernels and the data
dependencies between them, and HPX taking care of synchronizing the results via
futures and distributing the application onto multiple compute nodes.

In turn, SYCL application developers seeking to add distributed capabilities to
their applications may benefit by having HPX as a viable alternative to pure
MPI (or frameworks like Celerity~\cite{thoman2022celerity}).

The remainder of the work is structured as follows: As we focus on Octo-Tiger
as a motivating example, we first introduce Octo-Tiger and its software
dependencies (notably HPX and Kokkos) as well as its execution model in the
next section. Subsequently, in Section~\ref{sec:integration}, we focus on SYCL
and the required software additions (notably the HPX-SYCL integration) to make
it work asynchronously with HPX, HPX-Kokkos and, thus, with
Octo-Tiger. We then benchmark and test this integration with Octo-Tiger in
Section~\ref{sec:results}. In Section~\ref{sec:related:work} we outline related
work. Finally, we finish with a conclusion and ideas for future work in
Section~\ref{sec:conclusion}.

