\section{Related work}
\label{sec:related:work}
From the application perspective, other astrophysics codes support SYCL's abstraction layer as well. \textit{DPEcho}\footnote{\url{https://github.com/LRZ-BADW/DPEcho}}, a code for general relativistic magneto hydrodynamics, uses Intel MPI and hipSYCL. \textit{ARGOT}, a radiative transfer code, uses Intel DPC\texttt{++} to support GPUs (Intel CPU + NVIDIA GPU) and Intel FPGAs~\cite{kashino2022multi}. However, the applications, \emph{e.g.}\ black holes and cosmology, of these two codes are different from Octo-Tiger.

From the integration of SYCL within asynchronous multitask systems, we focus on the integration of the asynchronous tasks of the SYCL  API. One could just call the SYCL  API on one thread, block the thread while waiting for the event. However, we are interested in integrating the SYCL  API call into the asynchronous execution graph. Chapel~\cite{chamberlain2007parallel} supports Intel DPC\texttt{++} as one of their GPU API modules since October 21. Unitah~\cite{germain2000uintah} support SYCL via Kokkos SYCL \cite{10.1145/3539781.3539794}. Chiu et al. compared the SYCL default task graph with the CUDA graph execution model\footnote{\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\#cuda-graphs}} for large-scale machine learning work loads~\cite{chiu2022experimental}.  Other notable AMTs are: Charm\texttt{++}~\cite{kale1993charm++}, Legion~\cite{bauer2012legion}, and PaRSEC~\cite{bosilca2013parsec}. A detailed comparison is given in~\cite{thoman2018taxonomy}. 
Focusing on the programming model, Charm\texttt{++} and HPX are very close; however, HPX conforms to the C\texttt{++} standard and Charm\texttt{++} is a library that uses the C\texttt{++} programming language. The overheads using HPX and Charm\texttt{++} are compared with MPI and OpenMP in~\cite{https://doi.org/10.48550/arxiv.2207.12127}.

