\section{Integrating SYCL with HPX and Octo-Tiger}
\label{sec:integration}
In this section, we describe how we integrated SYCL with HPX, HPX-Kokkos, and
ultimately, Octo-Tiger. We start with a short SYCL introduction before
moving to the actual integrations in the following subsections. There, we first
cover the two variations of HPX-SYCL integrations that we tested and
afterward the changes to Octo-Tiger and its other dependencies.


\subsection{SYCL}
%and dropped support of NVIDIA GPUs in the latest release.
SYCL, a single source embedded domain-specific language (eDSL) aligned with the
C\texttt{++} 17 standard, provides high-level abstractions for various
acceleration cards and other devices. The SYCL specification is handled by the
Khronos group, and various implementations are available, for instance
ComputeCPP\textsuperscript{\texttrademark} which is developed by \textit{Codeplay
Software}.
\textit{hipSYCL} (currently in the process of being renamed into Open SYCL) is
developed at the University of Heidelberg~\cite{alpay2020sycl} and supports all
CPU architectures using OpenMP and Intel
GPUs using Level Zero; AMD GPUs using ROCm; and
NVIDIA GPUs using
CUDA. DPC\texttt{++} is part of
Intel OneAPI and supports Intel CPUs using OpenCL and
Intel GPUs using Level Zero or OpenCL; AMD
GPUs using AMD ROCm\textsuperscript{\texttrademark}; and NVIDIA GPUs using
CUDA. It is notable that hipSYCL and
DPC\texttt{++} are the only implementations supporting all three GPU
architectures. \textit{triSYCL} is developed by Xilinx and supports Intel and
AMD CPUs using OpenMP or Intel Thread Building Blocks (TBB) and ARM CPUs using
OpenMP. In addition, Xilinx FPGAs are supported using OpenCL.
For our purposes, we require at least a SYCL implementation that supports the
SYCL 2020 specification~\cite{sycl2020standard}. Specifically, we need support
for USM and \lstinline{in_order} queues. Support for SYCL host tasks is actually optional, as only one of our integration
implementations relies on it. As Kokkos uses various OneAPI-specific
extensions in its SYCL execution space, we focus on DPC++ in the rest of the paper.
However, we also tested the HPX-SYCL integration itself with hipSYCL.



% HipSYCL is currently in the process of being renamed into Open SYCL, however,
% in this work we will refer to it as hipSYCL still as the documentation (and
% hence the functionality we use) is not yet completly renamed at the point of
% submission.

%However, for the HPX-SYCL integration itself, we also tested hipSYCL.

%\begin{table}[tb]
%    \centering
%    \begin{tabular}{c|c|cccc}
%     \toprule
%     \multicolumn{2}{c}{} & ComputeCPP & hipSYCL & DPC\texttt{++} & triSYCL \\ \midrule
%     \multirow{3}{*}{ \rotatebox[origin=c]{90}{CPUs} }   & Intel & OpenCL$^1$ & OpenMP & OpenCL & \vtop{\hbox{\strut OpenMP}\hbox{\strut TBB}}  \\
%      &  AMD & OpenCL & OpenMP & -- & \vtop{\hbox{\strut OpenMP}\hbox{\strut TBB}} \\
%      & ARM  & OpenCL$^2$ & OpenMP & -- & OpenMP\\\midrule
%      \multirow{3}{*}{\rotatebox[origin=c]{90}{GPUs}}    & Intel & OpenCL & Level Zero & \vtop{\hbox{\strut Level Zero}\hbox{\strut OpenCL}}  & -- \\
%      &  AMD & OpenCL$^2$ & ROCm & ROCm$^3$ & -- \\
%      & NVIDIA & -- & CUDA & CUDA$^3$ & --\\ \midrule
%       \multirow{2}{*}{\rotatebox[origin=c]{90}{FPGAs}}    & Intel & -- & -- & \vtop{\hbox{\strut Level Zero}\hbox{\strut OpenCL}} & -- \\
 %        & Xilinx & -- & -- & -- & OpenCL \\\bottomrule
 %   \end{tabular}
 %   \caption{Overview of implementations of the SYCL~standard with respect to January 19th, 2023. Expect of FPGAs, hipSYCL and DPC\texttt{++} support all three CPU and GPU architectures.}
 %   \footnotesize{1 SSE4.1 support required}\\
 %   \footnotesize{2 OpenCL drivers with SPIR/SPIR-V support required}\\
 %   \footnotesize{3 Codeplay plugins required}
 %   \label{tab:sycl:overview}
%\end{table}

\subsection{HPX-SYCL Integration}

%\lstdefinestyle{myCustomMatlabStyle}{
%    language=C++,
%    basicstyle=\color{darkgray}\footnotesize\ttfamily,
%    keywordstyle=\color{amaranth}\ttfamily,
%    stringstyle=\color{amaranth}\ttfamily,
%    commentstyle=\color{amber}\ttfamily,
%    morecomment=[l][\color{magenta}]{\#},
%%    numbers=left,
%    numberstyle=\small, 
%    stepnumber=1
%}
%
%\lstset{style=myCustomMatlabStyle}

\begin{figure*}[tb]
\begin{lstlisting}[caption={Using the basic HPX-SYCL integration.},label={lst:code1},deletekeywords={for}]
sycl::event event = queue.submit([&](sycl::handler& h) {
     /* insert SYCL dependencies */
     h.parallel_for(num_items, [=](auto i) {
                /* insert numeric code here */ });/
});
// Call HPX-SYCL integration
hpx::future<void> my_future =
   hpx::sycl::experimental::detail::get_future(event);
// Add task to be executed once the event is done
hpx::future<void> continuation_future =
   my_future.then([&continuation_triggered](auto&& fut) {
     /* insert CPU work/communication/post-processing */
   });
/* Suspend the current HPX task if kernel and continuation 
are not yet done. This does not block the worker thread,
it merely moves to work on another available task*/
continuation_future.get()
\end{lstlisting}
\end{figure*}

%\begin{figure}
%\end{figure}
%\lstset{basicstyle=\normalsize}
Here, we take a closer look at the HPX-SYCL integration. In its most basic
form, what we need is the functionality to create HPX futures from SYCL events.
If an event is not yet complete, the associated future should not be ready.
Once the event is completed, and thus all SYCL operations required for this
event are complete, the HPX future should eventually become ready, and thus
trigger all subsequent tasks depending on this future.  The creation of this
future needs to be low-overhead and completely non-blocking as otherwise we
lose the advantage gained by having asynchronous SYCL kernel launches.


There are two distinct ways to implement this: We can either use SYCL host tasks directly or implement an event polling scheme within the
HPX scheduler.


\subsubsection{Integration using SYCL Host Tasks}
\label{sec:integration:hosttasks}
SYCL itself allows creating host tasks that depend on SYCL events. Once the required
events enter the status \lstinline[language=c++]{info::event_command_status::complete}, the
\lstinline[language=c++]{host_task} is triggered and executed on the CPU by a thread managed by the SYCL
runtime. This SYCL \lstinline[language=c++]{host_task} may include arbitrary C\texttt{++} code, hence we can
use it to trigger an HPX future. Essentially, given a SYCL event, we simply
need to create a future that is not yet ready. By setting the internal future
data, we can set the future to the ready state, automatically triggering all
potential continuations defined by the user. Hence, we use the \lstinline[language=c++]{host_task} as a
sort of callback that will set this internal future data when triggered.

This is the simplest form of integration. However, it comes with the downside
of having to rely on the SYCL runtime to handle the execution of these
asynchronous host tasks efficiently. Therefore, this method may come with
great overhead (as all CPU cores should be busy with HPX worker threads
already). Nevertheless, we include this method here, as it seems to be the most
straightforward way to achieve the integration. However, in the results, we
show that (at least for Octo-Tiger) it is not a viable solution due to the
aforementioned overhead.

%Usually this involves a separate thread-pool to process these asynchronous host tasks. 
%First, we explain the basic integration, allowing to get a HPX future for any arbitrary SYCL event. We then extend this by adding an HPX SYCL executor for more convenient usage, and discuss one particular limitation of our current approach.
\subsubsection{Integration via Event Polling}
%An alternative that works without using any SYCL host tasks is the utilization of event
%polling. Again, to obtain an HPX future for a SYCL event, we need a way to
%set the internal future data once the event's execution status becomes
%\texttt{info::event\_command\_status::complete}. Setting the future's data
%(even if it's a \texttt{future<void>} makes the future enter a ready-state,
%meaning potential continuations (\emph{i.e.}\ HPX tasks that depend on this
%future) are triggered.

Instead of using SYCL host tasks, we can make use of HPX runtime itself here. As we
already have the HPX scheduler that is repeatedly called by worker threads in
between tasks (to get a new task), we can use it to poll SYCL events
periodically, and, depending on the execution status of the events, trigger
associated callbacks.  When we want a future for a given SYCL event, we simply
construct a future (not ready yet) and create a struct that contains the event
and a callback lambda that sets the data of the future when it is executed
(again, turning the future ready and triggering potential continuations). We add this \lstinline[language=c++]{event_callback} struct to the scheduler. To keep the
overhead low, we use an \lstinline[language=c++]{ConcurrentQueue} for this as
HPX already provides this data-structure. Using this, all threads can add their
own \lstinline[language=c++]{event_callbacks} for different events without
unnecessary locking.
% add linebreak?



The polling function that is called by the scheduler is a different matter:
This will be executed by only one thread at a time. That being said, other threads
trying to enter will not wait on the mutex, but instead return immediately, as
it is enough for one thread to do the polling at a time so other threads trying
to poll might as well work on other tasks (as there will always be another
thread visiting eventually as long as the HPX runtime is alive).Â The thread
executing the poll function tries to get all events from the concurrent queue
and checks each one for completion. If completed, the associated callback is
executed. If not, the \lstinline{event_callback} is moved to a vector to be polled again
at a later visit (inside the poll function, all events of the vector are polled
once as well). This way, an HPX future for a completed SYCL event will be
ready eventually, without ever having to call any blocking methods such as the event \lstinline[language=c++]{wait()} method. Instead, we merely have to poll
the events in-between other tasks which is handled automatically by our
integration inside the scheduler. This procedure is exemplified for one future
in Figure~\ref{fig:event_polling}. In Listing~\ref{lst:code1} is an example of
how this functionality can be used.

In case we need a future for an \lstinline{in_order} SYCL queue without having any SYCL
event available, we provide an overload for \lstinline{get_future} that inserts
a dummy SYCL \lstinline{single_task} and uses its SYCL event to get a future. It is
checked that the queue is \lstinline{in_order} as only in this case the dummy kernel will
be executed after all previous operations submitted to the queue, allowing us
to use this future to check if previously submitted commands are done even if
we do not have their events anymore (as can be the case when those commands are submitted
inside an external library).

We implemented this integration (see pull
request\footnote{\url{https://github.com/STEllAR-GROUP/hpx/pull/6085}}) and
initially
tested it with DPC\texttt{++} and with hipSYCL using some basic vector add and
stream examples. For it to work, the first hurdle was compiling HPX with the
respective SYCL implementation. For DPC\texttt{++}, we simply had to pass the
\lstinline[language=bash]{-fsycl} flag for the SYCL-related source files (such as tests and the
source files that implement the event polling) and \lstinline[language=bash]{-fno-sycl} for all other files. Our
initial attempt to do the same with hipSYCL's syclcc wrapper failed. Here, we
use hipSYCL's CMake integration instead, which worked out-of-the-box.

However, for the integration to work properly, the hipSYCL default
configuration needs to be considered. First, the hipSYCL runtime needs to be
kept alive. Without this, the entire SYCL runtime might be created and
destroyed inside the poll function if there are no other SYCL  objects alive at
this point, as the poll function creates a temporary \lstinline[language=c++]{event_callback}
(containing a SYCL event), which would cause re-creation of the hipSYCL
runtime.
Second, one must ensure the kernels are actually being launched: hipSYCL's
default scheduler supports automatic work distribution across multiple devices,
causing it to potentially delay launching a kernel since we will never wait on
either the event or the command queue.

Both issues can be addressed simply by properly configuring hipSYCL for this
usage, as the behavior can be oriented with the appropriate environment
variables. We can set the scheduler to the direct one, directly launching any
kernel/command, with \lstinline[language=bash]{HIPSYCL_RT_SCHEDULER=direct} and
make sure that the hipSYCL runtime remains alive with \lstinline[language=bash]{HIPSYCL_PERSISTENT_RUNTIME=1}. However, to be certain that the user
does not need to remember setting these, we added an extra command queue inside
the HPX scheduler, keeping the runtime alive as long as the HPX runtime itself
is alive. Furthermore, we can use this queue to trigger the flush method inside
hipSYCL to make the integration work with its default scheduler as well.
%\footnote{\url{https://github.com/illuhad/hipSYCL/blob/develop/doc/env\_variables.md}}
\begin{figure}[t]
\centering
\includegraphics[width=.48\textwidth]{figures/event_polling_22.pdf}  
\caption{Outline of how the event polling works. The poll function is triggered by in between other tasks. The HPX future will become ready once the associated SYCL event is complete and the next poll detects it. Setting the future to ready will automatically trigger any potential HPX continuations.}
\label{fig:event_polling}
\end{figure}

\subsubsection{HPX SYCL Executor}
\label{sec:hpx:sycl:executor}
%\lstset{style=myCustomMatlabStyle}
%\begin{lstlisting}[float,floatplacement=t,caption={Using HPX SYCL executor.},label={lst:code2}]
%hpx::sycl::experimental::sycl_executor 
%    exec(sycl::default_selector{});
%auto fut = hpx::async(exec,
%            &sycl::queue::submit, [&](sycl::handler& h) {
%                /* insert buffer accessesors */
%                h.parallel_for(num_items, [=](auto i) {
%                /* insert numeric code here */ });
%            });
%\end{lstlisting}
%\lstset{basicstyle=\normalsize}
On top of this basic \lstinline[language=c++]{get_future} integration, we added
a SYCL HPX executor. This executor wraps an \lstinline{in_order} SYCL command queue and
allows the dispatch of function calls by wrapping them in either \lstinline[language=c++]{hpx::async} (two-way execution, which means that it
returns an \lstinline[language=c++]{hpx::future}) or \lstinline[language=c++]{hpx::apply} (one-way execution without a future).
This executor only accepts SYCL queue member functions and passes them directly
to the underlying SYCL queue (eliminating the need to manually obtain the
future for these calls). This is done mostly for user convenience.
%This is mostly for convenience and allows writing code as shown in Listing~\ref{lst:code2}.
It also brings parity of features with the CUDA and HIP executors within HPX
that also support \lstinline[language=c++]{hpx::async} and \lstinline[language=c++]{hpx::apply}. In fact, we use an \lstinline{in_order} command
queue for the SYCL executor to keep the behavior the same as for these other
executors.



Like the basic integration described in the last section, this executor also
needed some adaptations when compiled with specific SYCL implementations. This
time, we needed to adapt it for DPC\texttt{++}, as picking the correct member
function overloads here requires taking into account the internal code location
parameter DPC\texttt{++} uses (requiring additional overloads taking this
parameter into account when compilation with DPC\texttt{++} is detected).



\subsubsection{Differences to other HPX GPU Executors} 
HPX also contains a CUDA executor (which
doubles as a HIP executor when compiled with hipcc).  This executor also
supports event polling (and, of course, \lstinline[language=c++]{hpx::async}
and \lstinline[language=c++]{hpx::apply}). It also supports using
CUDA callback functions directly. In fact, we
used this executor as a blueprint for implementing the SYCL executor and the
event polling SYCL integration.
However, there are some differences: In addition to getting everything compiled
with SYCL and changing the SYCL API calls, the SYCL  executor (and integration)
need to work with different SYCL implementations and thus need additional
logic to adapt to this (as with the aforementioned workarounds where we had
to adapt to hipSYCL or DPC\texttt{++}). Furthermore, we cannot implement an
event pool such as the one that the
CUDA executor uses, as SYCL itself lacks the
functionality to make use of our own events in calls (which would allow us to reuse
events, avoiding potentially expensive constructions and destructions). Here, we
have to simply use whatever events the SYCL queue is returning, and hope
the utilized SYCL implementation is using event pooling internally to reduce
the required overhead. On the one hand, this is beneficial when the utilized
SYCL runtime actually supports this, as it frees us from implementing it
ourselves: In fact, hipSYCL has event pooling for
CUDA and HIP events as of release 0.9.3.  On
the other hand, in the case where the runtime does not support event pools, we
have no way of adding our own within HPX.

%The effect have missing such an event pool, of course, highly depends on the application in question.
%\footnote{See \url{https://github.com/illuhad/hipSYCL/releases/tag/v0.9.3}}
\subsection{Additional Software Changes}
To get Octo-Tiger working with SYCL, we need more software additions.

\subsubsection{HPX-Kokkos}
First, we need to modify HPX-Kokkos itself (see pull
request\footnote{\url{https://github.com/STEllAR-GROUP/hpx-kokkos/pull/13}}). As
mentioned, HPX-Kokkos can return HPX futures for Kokkos kernel launches, but
only for supported execution spaces. Hence, we need to add the SYCL-execution
space to this list of supported spaces. This requires adding 
overloads, calling the correct \lstinline[language=c++]{get_future} method from
HPX instead of fencing and returning a dummy future which is the default behavior for
unsupported execution spaces. To do so, we need to hook into the basic
HPX-SYCL integration described in Section~\ref{sec:hpx:sycl:executor}, simply
by mapping the \lstinline[language=c++]{get_future} call here to the one
described there. We also need to correctly construct the HPX-Kokkos SYCL
executor instances. Not to be confused with the HPX-SYCL executor mentioned
above, such an executor is merely a wrapper around a Kokkos SYCL execution space.
To keep the behavior the same as for other Kokkos execution spaces, we use an
\lstinline{in_order} command queue to construct each SYCL execution space. Lastly, we add
an additional overload for \lstinline[language=c++]{deep_copy_async} which directly uses the
SYCL event from the memcpy to get the associated future.


\subsubsection{CPPuddle}
Furthermore, we need to modify CPPuddle (see pull
request\footnote{\url{https://github.com/SC-SGS/CPPuddle/pull/15}}). Octo-Tiger
uses CPPuddle to manage device memory pools to avoid unnecessary allocations by
reusing old allocations that are no longer in use. This is handy for repeated
GPU tasks that require similarly sized input buffers. Here, we merely need to
add the appropriate allocators to get the device/host memory pools using SYCL
USM mallocs.

\subsubsection{Octo-Tiger}
Finally, Octo-Tiger itself requires some modifications before being able to
work with SYCL (see pull request\footnote{\url{https://github.com/STEllAR-GROUP/octotiger/pull/432}}). Here, we
switch to the appropriate HPX-Kokkos executor (using the Kokkos SYCL execution
space underneath) and the CPPuddle allocators using SYCL USM memory. However, when
running Octo-Tiger's standard test suite, we still noticed the results being
off when running with SYCL. Using the SYCL math functions over the
std ones (\emph{i.e.}\ using \lstinline[language=c++]{sycl::sqrt} instead of \lstinline[language=c++]{std::sqrt}) fixed this issue.


%Curiously, we were able to fix this, simply by using the SYCL  math functions of the std ones (\emph{i.e.}\ using \lstinline[language=c++]{sycl::sqrt} instead of \lstinline[language=c++]{std::sqrt}).

\subsubsection{Kokkos}
\label{sec:integration:kokkos}
While not necessarily required to get Octo-Tiger working with SYCL, we also
experimented with some changes to Kokkos. First, since we are using \lstinline{in_order} queues,
we noticed that we can remove some barriers within the SYCL execution space in
Kokkos. Moreover, to use the Kokkos SYCL execution space on AMD GPUs, we
needed to make some modifications to the Kokkos CMake configuration to
correctly pass the arguments. These changes are not yet upstreamed as we want to refine and test them further first. However, both the patch removing
some of the fences is
available\footnote{\url{https://github.com/STEllAR-GROUP/OctoTigerBuildChain/blob/sycl_toolchain/kokkos_sycl_less_fencing.patch}}
as is the other
one\footnote{\url{https://github.com/STEllAR-GROUP/OctoTigerBuildChain/blob/sycl_toolchain_hip/kokkos_hip_arch.patch}}
enabling Kokkos builds with AMD GPUs.

%While the changes work for Octo-Tiger, we want to make sure to not cause race condition further down the line in other applications.
%\todo[inline]{Describe in more detail?}

