\section{Conclusion and Future Work}
\label{sec:conclusion}
For this work, we began making Octo-Tiger and HPX itself compatible with SYCL.
To do so, we introduced multiple software changes, most important of which is
the HPX-SYCL integration. 

Integrating these two frameworks certainly seems like an odd choice initially,
as both contain similar functionality for expressing execution graphs (one
using SYCL events, the other using HPX futures). However, there are
advantages, as both frameworks have their own specialties: With SYCL we can
quickly build the asynchronous dependencies between GPU kernels and
data-transfers, as shown in Octo-Tiger's hydro solver, where we first schedule
5 GPU kernels and their associated CPU-GPU data transfers for each sub-grid,
before HPX gets involved again. HPX, on the other hand, excels in scheduling
asynchronous CPU work, whether it is on the local compute-node or a remote one.
For instance, after scheduling the GPU work with SYCL, we use HPX by getting a
future when the GPU work for a sub-grid is done, then use this future to
schedule the CPU post-processing and the communication with neighboring
sub-grids without ever blocking the CPU host threads. Although we only looked
at single-node runs in this work, this should become even more important in
distributed runs, as the interleaving of computation and communication becomes
increasingly more important the more nodes we use, especially in tree-based
codes like Octo-Tiger.



While we implemented the HPX-SYCL integration using both the \lstinline{host_task} SYCL feature and event polling, in the end, the version
using event polling within the HPX scheduler proved to be clearly superior in
our tests. Overall, the speedups in Octo-Tiger when using the integration are
modest, yet noticeable, ranging from $1.11x$ to $1.15x$ for the best
configurations (using $32$ HPX worker threads and the best combination of GPU executors and
maximum number of aggregated kernels). 
Furthermore, the Kokkos optimization
patch we used shows promise, though at the point of submission, we think it
requires further testing with codes beyond Octo-Tiger before upstreaming it.
Between these Kokkos changes and the SYCL integration, Octo-Tiger's overall
runtime behavior when using the Kokkos SYCL execution space becomes comparable to the
one using the Kokkos CUDA execution space.

For future work, we would like to make use of two opportunities that the
groundwork in this paper enables: First, since we updated and optimized
Octo-Tiger's toolchain to work with SYCL, we can now realistically target the
upcoming Intel GPUs for future Octo-Tiger runs. Hence, we would like to
re-run the tests shown here on the respective Intel hardware in the future.
Secondly, one of the major advantages of using HPX is that it enables
distributed runs. We would like to revisit the benefit of the HPX-SYCL
integration in more complex, distributed scenarios on Perlmutter, as blocking
the cores by waiting for SYCL results (without the integration) should have a
more profound negative performance impact here as it can impact the
interleaving of computation and communication.

% Furthermore, while we were unable to use hipSYCL with Octo-Tiger in this work
% due to the OneAPI extensions used within Kokkos, we would still like to revisit
% this as hipSYCL would contain event pools, which could help to reduce the
% overhead we are experiencing for the events creations and destructions.


