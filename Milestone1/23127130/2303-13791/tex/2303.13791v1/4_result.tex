\section{Experimental Results}
\label{sec:result}
\input{figure/fig_ours_graphs}
\input{table/novel_view_synthesis.tex}
\input{figure/fig_tnt_results}
\input{figure/fig_ours_results.tex}

\subsection{Datasets}
\label{sec:dataset}
\topic{Tanks and Temples.} 
We evaluate our method on the \textsc{Tanks and Temples} dataset \cite{Knapitsch2017tankandtemples}. 
We select the sequences without dynamic elements (9 scenes out of 21), retain one in every five frames as motion is slow, downscale the video to full HD resolution (2048$\times$1080 or 1920$\times$1080), and keep the first 1000 images so methods with a static data loader can preload images and rays in a reasonable amount of system memory.

\topic{Static Hikes.} 
We also collect a new dataset with hiking sequences. 
It contains hand-held sequences with larger camera trajectories to test scalability and pose estimation robustness.
It comprises twelve 1920$\times$1080 videos of static outdoor scenes captured with GoPro Hero10 with linear FoV, GoPro Hero9 with narrow FoV, and the wide cameras of LG V60 ThinQ and Samsung Galaxy S21. 

\subsection{Compared Methods} 
\noindent
We compare our method against Mip-NeRF360~\cite{barron2022mipnerf360} and NeRF++~\cite{kaizhang2020nerfpp} as they are both designed to handle unbounded scenes and are suitable for outdoor scenes. For Mip-NeRF360, we set near to 0.1 as we observed clipping with the default value. Nerfacto~\cite{nerfstudio} combines Instant-NGP~\cite{mueller2022instant}'s hash encoding and Mip-NeRF360's scene contraction to efficiently represent unbounded scenes. 
When preprocessed poses are required, we use MultiNeRF~\cite{multinerf2022}'s script to run COLMAP.
We also compare against the scalable representation Mega-NeRF~\cite{Turki_2022_meganerf} (using a 2$\times$2 grid size for our experiments).
Since those methods require pre-processed poses, we include SCNeRF~\cite{SCNeRF2021} and BARF~\cite{lin2021barf} for self-calibrated experiments. 
For SCNeRF, we use the NeRF++ codebase to better represent unbounded scenes. 
Note that SCNeRF requires COLMAP as initialization: it fails in our experiments when optimizing poses from scratch with both the NeRF and NeRF++ bases (we get NaN renders). 
We, therefore, had to exclude SCNeRF from the fully self-calibrated evaluations. 

\subsection{Quantitative Evaluation}
\label{sec:quanitative}
\noindent
To quantitatively evaluate the synthesized novel views, we select every ten frames as a \emph{test} image. 
We show the PSNR, SSIM, and LPIPS~\cite{zhang2018perceptual} between the synthesized views and the corresponding ground truth views in \tabref{tnt}. 
Averages are computed in the square error domain for PSNR and $\sqrt{1 - \text{SSIM}}$ domain for SSIM, following~\cite{Brunet2012ssimmaths}.
For the self-calibrated experiments, we estimate the test poses by adding iterations that only optimize the poses, \emph{without} updating the intrinsic or the radiance fields' parameters. 

Table~\ref{tab:tnt} shows that, although the videos are not what we target, with a smaller camera path and several inward-looking 360 scenes, our method provides competitive results. 
With COLMAP poses, we obtain similar quality as Mip-NeRF360~\cite{barron2022mipnerf360}.
Mega-NeRF~\cite{Turki_2022_meganerf}, being designed for a different type of input data, shows lower quality despite using several radiance fields.
Compared to other self-calibrating radiance fields methods~\cite{SCNeRF2021,lin2021barf}, we obtain much better results when optimizing poses from scratch thanks to our progressive optimization that allows for fewer parameters to be estimated from scratch at once and adds a flexible locality prior to the camera pose. % \MK{let's emphasize we optimize pose parameters from scratch! with our self-calibrated version.}

Figure~\ref{fig:ours_graphs} shows that, on the \textsc{Static Hikes} dataset featuring longer trajectories and more challenging scenes, we consistently obtain better results than the self-calibrated method BARF~\cite{lin2021barf}. 
Mip-NeRF360~\cite{barron2022mipnerf360}, relying on COLMAP, cannot produce results for $19.5\%$ of the test frames. 
In addition, we show higher quality on the rendered frames.
\input{figure/extrapolation_main}

\subsection{Qualitative evaluation}
\label{sec:visual_comparison}
\noindent 
Figure~\ref{fig:tnt_results} compares the results on the \textsc{Tanks and Temples} dataset, and Figures~\ref{fig:ours_results} and \ref{fig:extra} show results on the \textsc{Static Hikes} dataset we acquired. 
Our approach can provide results for all frames and maintain better sharpness throughout the entire trajectory. 


\subsection{Ablation Study}
\label{sec:ablation}
\input{table/ablation.tex}
\noindent
Table~\ref{tab:ablation} shows that our progressive optimization and local radiance fields are both necessary to obtain our results. 
Furthermore, Figure~\ref{fig:prog_impact} highlights that progressive optimization is crucial when estimating poses for long sequences, and Figure~\ref{fig:loc_impact} shows that local radiance fields grant more robustness and allow for scalability. 