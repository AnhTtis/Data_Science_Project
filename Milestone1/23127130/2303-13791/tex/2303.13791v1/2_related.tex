\section{Related Work}
\label{sec:related}

\topic{Novel view synthesis.} 
Novel view synthesis aims to synthesize new views from multiple posed images.
Recently, neural implicit representation has shown promising novel view synthesis results \cite{mildenhall2020nerf}. 
However, achieving high-quality artifact-free rendering results is still a challenging task.
Recent works further improve the visual quality by addressing inconsistent camera exposure or illumination~\cite{rematas2022urf,tancik2022blocknerf,martinbrualla2020nerfw}, handling dynamic elements~\cite{xian2021space,li2021neural,Gao2021DynNeRF,park2021nerfies,park2021hypernerf,liu2023robust}, anti-aliasing~\cite{barron2021mipnerf}, high noise~\cite{mildenhall2021rawnerf} or optimization from a reduced number of frames~\cite{Niemeyer2021Regnerf}.
While these implicit representation-based methods yield high-quality results, they take days to train.
To improve the training efficiency, some works also explore more explicit representations with voxel-like structures \cite{yu_and_fridovichkeil2021plenoxels,Sun_2022_CVPR}, tensor factorization \cite{tensorf}, light field representation~\cite{attal2022learning,attal2023hyperreel}, or hashed voxel/MLP hybrid~\cite{mueller2022instant}. 
Our work also leverages the recent advantage of TensoRF~\cite{tensorf}.


\topic{Scalable view synthesis.} 
Several systems have been proposed to support unbounded scenes \cite{barron2022mipnerf360,kaizhang2020nerfpp}.
However, these methods require either omnidirectional inputs \cite{jang2022egocentric}, proxy geometry \cite{wu2022snisr}, specialized drone shots \cite{Turki_2022_meganerf}, or satellite shots \cite{xiangli2022bungeenerf} and struggle with monocular videos captured at ground level.
Recently, Mip-NeRF 360~\cite{barron2022mipnerf360} contracts background into a contracted space, and NeRF++~\cite{kaizhang2020nerfpp} optimizes an environment map to represent the background. 
BlockNeRF~\cite{tancik2022blocknerf} is scalable but requires multiview inputs and several observations.
NeRFusion~\cite{zhang2022nerfusion} constructs per frame local feature volumes using a pretrained 2D CNN followed by a sparse 3D CNN. 
It is scalable and has demonstrated good accuracy on large indoor scenes, but it does not tackle camera pose estimation or unbounded outdoor scenes. 
Since the representation is mostly reconstructed before an optional per-scene optimization, it is not trivial to optimize poses simultaneously.

In contrast to all these constraints, our method is robust, works with arbitrary long camera trajectories, and only takes casually captured monocular first-person videos as input.

\topic{Camera pose estimation.}
Visual odometry estimates camera poses from videos. They can either rely directly on the color by maximizing the photoconsistency~\cite{zhou2017unsupervised,yin2018geonet} or on extracted hand-crafted features~\cite{mur2015orb,mur2017orb,schoenberger2016sfm}.
Recently, learning-based methods~\cite{zhou2017unsupervised,godard2019digging,kopf2021robust,teed2021droid,zhao2022particlesfm} learn to optimize the camera trajectories in a self-supervised way and show strong results.
Similarly, many methods extend NeRF to optimize the camera poses jointly with radiance fields from photometric loss~\cite{wang2021nerfmm, lin2021barf, SCNeRF2021}.
However, these methods struggle to reconstruct and synthesize faithful images for large scenes and often fail for monocular first-person videos with long camera trajectories.
Vox-Fusion~\cite{yang2022vox} and Nice-SLAM~\cite{Zhu2022CVPR} achieve good pose estimation but are designed for RGB-D inputs and require accurate depth: Vox-Fusion to allocate a sparse voxel grid and Nice-SLAM to determine where to sample along the ray.
Note that our goal does \emph{not} lie in estimating camera poses. 
Instead, focus on reconstructing overlapping local radiance fields that enable photorealistic view synthesis. 
We believe integrating advanced techniques such as global bundle adjustment can improve our results.