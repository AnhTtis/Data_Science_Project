
\section{Introduction}
\label{sec:intro}
\noindent
%%%%%%%%%%%%%%%%%%%%%%%%
% Broad context
Dense scene reconstruction for photorealistic view synthesis has many critical applications, for example, in VR/AR (virtual traveling, preserving of important cultural artifacts), video processing (stabilization and special effects), and mapping (real-estate, human-level maps).
Recently, rapid progress has been made in increasing the fidelity of reconstructions using radiance fields \cite{mildenhall2020nerf}.
Unlike most traditional methods, radiance fields can model common phenomena such as view-dependent appearance, semi-transparency, and intricate micro-details.

%%%%%%%%%%%%%%%%%%%%%%%%
% Narrow context
\topic{Challenges.} In this paper, we aim to create radiance field reconstructions of \emph{large-scale} scenes that are acquired using a single handheld camera since this is arguably the most practical way of capturing them outside the realm of professional applications.
In this setting, we are faced with two main challenges: 
(1) estimating accurate camera trajectory of a long path and 
(2) reconstructing the large-scale radiance fields of scenes. 
Resolving them together is difficult because changes in observation can be explained by either camera motion or the radiance field's ability to model view-dependent appearance. 
For this reason, many radiance field estimation techniques assume that the accurate poses are known in advance (typically fixed during the radiance field optimization).
However, in practice, this means that one has to use a separate method, such as Structure-from-Motion (SfM), for estimating the camera poses in a pre-processing step.
Unfortunately, SfM is not robust in the handheld \emph{video} setting.
It frequently fails because, unlike radiance fields, it does not model view-dependent appearance and struggles in the absence of highly textured features and in the presence of even slight dynamic motion (such as swaying tree branches).

%%%%%%%%%%%%%%%%%%%%%%%%
% Haven't others solved this problem?
To remove the dependency on known camera poses, several approaches propose jointly optimizing camera poses and radiance fields~\cite{wang2021nerfmm, lin2021barf, SCNeRF2021}. 
These methods perform well when dealing with a small number of frames and a good pose initialization. 
However, as shown in our experiments, they have difficulty in estimating long trajectories of a video camera from scratch and often fall into local minima.

%%%%%%%%%%%%%%%%%%%%%%%%
% What do we do?
\topic{Our work.} In this paper, we propose a joint pose and radiance field estimation method.
We design our method by drawing inspiration from classical \emph{incremental SfM} algorithms and \emph{keyframe-based SLAM} systems for improving the robustness.
The core of our approach is to process the video sequence \emph{progressively} using overlapping \emph{local} radiance fields.
More specifically, we progressively estimate the poses of input frames while updating the radiance fields.
To model large-scale unbounded scenes, we dynamically instantiate local radiance fields.
The increased locality and progressive optimization yield several major advantages:
\begin{tightitemize}
\item Our method scales to processing arbitrarily long videos without loss of accuracy and without hitting memory limitations.
\item Increased robustness because the impact of misestimations is locally bounded.
\item Increased sharpness because we use multiple radiance fields to model local details of the scene (see Figure~\ref{fig:teaser} and \ref{fig:mip360vsloc}b).
\end{tightitemize}
We validate our method on the \textsc{Tanks and Temples} dataset.
We also collect a new dataset \textsc{Static Hikes} of twelve outdoor scenes using four consumer cameras to evaluate our method. 
These sequences are challenging due to long handheld camera trajectories, motion blur, and complex appearance. 
We will release the source code for our method and the new dataset for reproducibility.

\topic{Our contributions.} 
We present a new method for reconstructing the radiance field of a large-scale scene, which contains the following contributions:
\begin{tightitemize}
\item We propose to progressively estimate the camera poses and radiance fields, leading to significantly improved robustness. 
\item We show that using multiple overlapping local radiance fields improves visual quality and supports modeling large-scale unbounded scenes.
\item We contribute a newly collected video dataset that presents new challenges not covered by existing view synthesis datasets.
\end{tightitemize}

\topic{Limitations.} 
Our work aims to synthesize novel views from the reconstructed radiance fields. 
While we jointly estimate the poses in the pipeline, we do not perform global bundle adjustment and loop closure (i.e., not a complete SLAM system). 
We leave this important direction for future work.


\input{figure/mip360_vs_local}