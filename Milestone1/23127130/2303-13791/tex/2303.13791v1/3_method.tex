
%%%%%%%%%%
%%%%%%%%%%
\input{figure/fig_overview}
\section{Method}
\label{sec:method}
\noindent
Our method takes a potentially very long monocular video of a large-scale scene as input.
Our goal is to reconstruct the radiance field of the scene along with the camera trajectory to enable free-viewpoint novel view synthesis. 

We choose TensoRF~\cite{tensorf} as our base representation for its quality, reasonable training speed and model size. 
TensoRF models the scene with a factorized 4D tensor that maps a 3D position $\x$ to the corresponding volume density $\sigma$ and view-dependent color $\c$.
High-quality novel view synthesis results for small-scale scenes have been demonstrated using this representation.
However, it has only been achieved with accurate pre-known camera poses, and TensoRF's representation power needs to be increased for capturing the details from long trajectories of unbounded scenes.

In this work, we resolve the need for pre-known camera poses by improving the \emph{robustness} of joint camera pose and radiance field estimation, and we \emph{scale the method} to handle arbitrarily long input sequences. 
To this end, we propose a progressive optimization scheme that processes the input video with a sweeping temporal window and incrementally updates the radiance fields and the camera poses.
This process ensures that new frames are added to a well-converged solution for camera poses and radiance field representation of the previous structures, effectively preventing getting stuck in poor local minima.
In addition, we dynamically allocate new \emph{local} radiance fields throughout the optimization that are supervised by a limited number of input frames (within a temporal window). 
This further improves robustness while processing arbitrarily long videos with fixed memory. 

\subsection{Formulation and Preliminaries}
\noindent
During our optimization procedure, we estimate $P$ camera poses $[R|t]_k, k \in [1..P]$\footnote{We use the continuous 6D representation~\cite{Zhou_2019_6D} for representing camera rotations.} as well as the parameters of a series of $M$ local radiance fields $\Theta_j, j \in [1..M]$. 

Given a pixel, we use the camera parameters and the poses to generate a ray $\mathbf{r}$. Along this ray, we sample 3D positions $\{\x_i\}$ and query a radiance field that provides color and density:
\begin{equation}
(\mathbf{c}_i, \sigma_i) = \text{RF}_{\Theta_j}(\x_i).
\end{equation}
Via volume rendering~\cite{kajiya1984ray,drebin1988volume}, we can render the ray using this representation:
\begin{align}
\label{eq:volume_rendering}
\hat{\mathbf{C}}(\mathbf{r}) &= \sum_{i=1}^{N} T_i(1-\text{exp}(-\sigma_i\delta_i))\mathbf{c}_i,\\
T_i &= \text{exp}\left(-\sum_{i-1}^{j}\sigma_j\delta_j\right),
\end{align}
where $\delta_i$ is the distance between two consecutive sample points and $N$ is the number of samples along the ray, and $T_i$ indicates the accumulated transmittance along the ray. 
We optimize the radiance field parameters $\Theta_j$ and the camera pose used to generate the ray using the input frame's color $\mathbf{C}$ as supervision:
\begin{equation}
\mathcal{L} = \left \| \hat{\mathbf{C}}(\mathbf{r}) - \mathbf{C}(\mathbf{r}) \right \|_{2}^{2}.
\label{eq:photometric_loss}
\end{equation}
Using TensoRF~\cite{tensorf} in this context is particularly suitable as it features an explicit coarse-to-fine optimization analogous to BARF's~\cite{lin2021barf} and reduces the likelihood of converging to a local minimum for pose estimation.

To handle unbounded scenes, we leverage a scene parameterization similar to Mip-NeRF360~\cite{barron2022mipnerf360}'s contraction, mapping every point to a $[-2, 2]$ space before querying our radiance field model:
\begin{equation}
    \label{eq:contract}
    \text{contract}(\x) = 
    \begin{cases}
      \x & \text{if}\ \| \x \|_{\infty} \leq 1 \\
      \left(2 - \frac{1}{\| \x \|_{\infty}}\right) \left(\frac{\x}{\| \x \|_{\infty}}\right) & \text{otherwise}.
    \end{cases}
\end{equation}
Here we use the $L_{\infty}$ norm to fully utilize TensoRF's square bounding boxes.
While Mip-NeRF360 scale camera poses to keep the uncontracted space around the area of interest, we cannot adopt this strategy since we jointly estimate poses and radiance fields (the poses are unknown a priori).
We achieve appropriate scaling by dynamically creating new radiance fields (see Figure~\ref{fig:mip360vsloc} and Section~\ref{sec:local}).

\subsection{Progressive Joint Camera Pose and Radiance Field Optimization}
\label{sec:prog}
\input{figure/fig_prog_impact}
\noindent
Existing pose-calibrating methods~\cite{lin2021barf,wang2021nerfmm,SCNeRF2021} have demonstrated that jointly optimizing a radiance field and camera poses can achieve satisfactory results in small-scale scenes. 
However, when dealing with longer sequences, joint optimization fails as estimated poses get stuck in local minima (see Figure~\ref{fig:prog_impact}). 
To improve the robustness, we start the optimization process with only a small number of frames (the first five frames in our experiments), and from there we \emph{progressively} introduce subsequent frames to the optimization. 
To this end, we initialize the new pose (index $p + 1$) using the current frame at the end of the trajectory:
\begin{equation}
    [R|t]_{p + 1} \leftarrow [R|t]_p.
\end{equation}
We then add $[R|t]_{p + 1}$ to the trainable parameters and we add the frame's color as supervision of the radiance fields.
In this scheme, the convergence of the parameters for the new frame benefits from the initialization of the radiance field and the currently estimated poses, making it less prone to get stuck in local minima. 
Since we add the camera pose at the end of the trajectory, it also introduces a locality prior that enforces each pose to be close to the former one without an explicit constraint, as it is common for videos.

To further constrain this complex joint optimization problem, we introduce additional losses described in Section~\ref{sec:implementation}.

\subsection{Local Radiance Fields}
\label{sec:local}
\input{figure/fig_loc_impact}
\noindent
The progressive scheme proposed in the previous section provides more robust pose estimation, but it still relies on a single global representation of the scene, which causes problems when modeling long videos:
(1) any misestimation (e.g., outlier pose) has global impact and might cause the entire reconstruction to break down.
(2) a single model with fixed capacity cannot represent arbitrarily long videos with an arbitrary amount of detail, leading to blurry renderings (Figure~\ref{fig:loc_impact}b).
A natural solution to these problems would be to pre-partition the space using radiance field tiling similar to Mega-NeRF~\cite{Turki_2022_meganerf}. 
However, this approach is not applicable in our setting because the camera poses are unknown before the optimization.
To resolve this issue, we dynamically create a new radiance field, whenever the estimated camera pose trajectory leaves the uncontracted space of the current radiance field.
We centered the new radiance field at the location $\t_j$ of the last estimated camera pose: $\t_j \leftarrow t_p$ (Figure~\ref{fig:overview}d.
When sampling a ray, we use this translation to center the radiance fields:
\begin{equation}
    \label{eq:shift}
    (\mathbf{c}_i, \sigma_i) = \text{RF}_{\Theta_j}(\x_i - \t_j).
\end{equation}
We supervise each radiance field with a local subset of the video frames.
This subset contains all the frames during which the radiance field was current, as well as the preceding 30 frames to provide for some overlap.
The overlap is important for achieving consistent reconstructions in the local radiance fields.
We further increase consistency by blending together the rendered colors $\hat{\mathbf{C}_j}(\mathbf{r})$ of all overlapping radiance fields at any supervising frame.
We use per-frame blending weights that increase/decrease linearly within the overlap region.
When we create a new radiance field we stop optimizing previous ones (i.e., freeze them).
At this point we can clear any supervising frames from memory that are not needed anymore.

With this procedure, each radiance field is supervised using only a subset of the frames, granting more robustness. 
The high-resolution space follows the camera trajectory, allowing for more sharpness and scalability.
Figure~\ref{fig:mip360vsloc} shows that local radiance fields help maintain the high-quality uncontracted space around the camera trajectory, which allows for a sharper representation.

\subsection{Implementation}
\label{sec:implementation}
\vspace{-2mm}
\topic{Losses.}
In addition to the supervision from the input color in Equation~\eqref{eq:photometric_loss}, we add monocular depth and optical flow between neighbor frames as they have proven their capabilities to improve the optimization stability in challenging scenarios~\cite{Luo2020cvd,Gao2021DynNeRF}. 
We use RAFT~\cite{teed2020raft} to estimate the frame-to-frame optical flow $\mathcal{F}_{k\rightarrow k+1}, k\in[1..P-1]$ and DPT~\cite{Ranftl2021dpt2} to estimate the per-frame monocular depth $\mathbf{D}$. 
To enable these losses, we first render the depth maps by swapping the sample's color $\c_i$ by the sample's distance to the ray origin $d_i$ in Equation~\eqref{eq:volume_rendering}:
\begin{equation}
    \label{eq:depth_render}
    \hat{\mathbf{D}}(\mathbf{r}) = \sum_{i=1}^{N} T_i(1-\text{exp}(-\sigma_i\delta_i))d_i.
\end{equation}
We formulate depth supervision following the shift and scale invariant loss typically used for monocular depth training~\cite{Ranftl2020dpt1, Ranftl2021dpt2} and radiance fields supervision~\cite{Gao2021DynNeRF}:
\begin{equation}
    \label{eq:depth_loss}
    \mathcal{L}_d = \left| {\hat{\mathbf{D}}^*} - {\mathbf{D}^*}\right|,
\end{equation}
where $\hat{\mathbf{D}}^*$ and $\mathbf{D}^*$ are per-frame normalized depth since monocular depth is not scale and shift invariant. We normalize following~\cite{Ranftl2020dpt1}, by first estimating scale and shift:
\begin{equation}
    \label{eq:depth_normalization0}
    t(\mathbf{D}) = \text{median}(\mathbf{D}),\ s(\mathbf{D}) = \frac{1}{M} \sum_1^M |\mathbf{D} - t(\mathbf{D})|,
\end{equation}
where we have $M$ samples for the frame. Then, we normalize:
\begin{equation}
    \label{eq:depth_normalization}
    \mathbf{D}^* = \frac{\mathbf{D} - t(\mathbf{D})}{s(\mathbf{D})}.
\end{equation}
In practice, we sample rays from 16 images in each batch and obtain scale and shift for each.
To obtain the expected optical flow from our representation, we leverage the relative camera poses and the rendered depth map:
\begin{equation}
    \label{eq:flow_render}
    \hat{\mathcal{F}}_{k\rightarrow k+1} = (u,v) - \Pi\left([R|t]_{k\rightarrow k+1}\Pi^{-1}(u,v,\hat{D})\right)
\end{equation}
Where $\Pi$ projects a 3D point to image coordinates and $\Pi^{-1}$ unprojects a pixel coordinate and depth into a 3D point and $[R|t]_{k\rightarrow k+1} = [R|t]_k^{-1}[R|t]_{k+1}$ is the relative camera pose between the two consecutive frames, bringing a point in the $k^{\text{th}}$ camera's space to the $k+1^{\text{th}}$'s. 
We finally compare the predicted flow to the expected flow from the representation:
\begin{equation}
    \label{eq:flow_loss}
    \mathcal{L}_f = \left\| \hat{\mathcal{F}}_{k\rightarrow k+1} + \mathcal{F}_{k\rightarrow k+1}  \right\|_1
\end{equation}
We use the same process to supervise using the backward optical flow $\mathcal{F}_{k\rightarrow k-1}$.
Note that optical flow computations leverage directly poses and the geometry of the scene in Equation~\eqref{eq:flow_render}, which gives a clear gradient signal for their optimization.

\topic{Scheduling.}
All parameters are optimized using Adam~\cite{2015adam} with $\beta_1=0.9$ and $\beta_1=0.99$.
We start with five poses initialized as identity and an initial TensoRF model. 
Then, for every 100 iterations, we add the next supervising frame in the video as described in Section~\ref{sec:prog} and Figure~\ref{fig:overview}a and \ref{fig:overview}b. 
During this optimization process, we maintain all learning rates, loss weights, and TensoRF resolution at their initial states. 
This ensures that the radiance field does \emph{not} overfit to the first frames. 
Our initial learning rates are $5\cdot10^{-3}$ for rotations and $5\cdot10^{-4}$ for translations, initial TensoRF resolution is $64^3$ and initial regularizing loss weights are $1$ for flow loss and $0.1$ for depth. 
We proceed with the progressive frame registration until an estimated camera pose is beyond the uncontracted space: $\| t_{p} - \t_{j} \|_{\infty} \geq 1$, where $t_{p}$ the last registered frame's translation and $\t_{j}$ the currently optimizing radiance fields center. 
From this point, we refine the TensoRF, and the camera poses for $600$ iterations per add frame (Figure~\ref{fig:overview}c). 
The schedulers and the regularizing losses follow an exponential decrease towards a $0.1$ factor, and we upsample the TensoRF up to $640^3$.
After this stage, we allocate a new TensoRF following Section~\ref{sec:local} and Figure~\ref{fig:overview}d and disable the supervision from the first frames. 
We repeat this process until the entire trajectory is reconstructed.
The optimization takes 30 to 40 hours for 1000 frames on a single NVIDIA TITAN RTX.