\section{Method}


\thename revisits the classic \cyclegan~\cite{zhu2017unpaired} architecture.
\thename inherits several advancements from \uvcgan~\cite{torbunov2023uvcgan} such as a hybrid U-Net-ViT generator architecture, self-supervised generator pre-training,
and better training strategies.
This section describes several improvements we make over UVCGAN, including the generator, discriminator and the training procedure.



\subsection{Review of the original \uvcgan}
\label{subsec:cyclegan}

\uvcgan follows the \cyclegan framework~\cite{zhu2017unpaired,kim2019u}
that interlaces two generator-discriminator pairs for unpaired I2I translation (\autoref{fig:cyclegan}).
Denote the two domains by $A$ and $B$. 
Generator $\gab$ translates images in $A$ to resemble those from domain $B$. 
Discriminator $\db$ distinguishes images in $B$ from those translated from $A$. 
The same goes for the other translation direction, $\gba$ and $\da$.
Using the notations defined in \autoref{fig:cyclegan}, the discriminators are updated by backpropagating loss in distinguishing real and translated (fake) images (called \textit{GAN loss}):
\newcommand{\gl}{\ell_\text{gan}}
\begin{align}
    \mathcal{L}^{\text{disc}}_{A} &= \mathbb{E}_{B}\gl\paren{\da\paren{\fake{a}}, 0} + \mathbb{E}_{A}\gl\paren{\da(a), 1}, \label{eq:disc_loss_A}\\
    \mathcal{L}^{\text{disc}}_{B} &= \mathbb{E}_{A}\gl\paren{\db\paren{\fake{b}}, 0} + \mathbb{E}_{B}\gl\paren{\db(b), 1} \label{eq:disc_loss_B}
\end{align}
where $\fake{b}= \gab(a)$, $\fake{a} = \gba(b)$ (subscript $\text{f}$ means fake),
$0$ is the label for fake images, $1$ is the label for real images, 
and $\gl$ represents a classification loss function ($L_2$, cross-entropy, Wasserstein~\cite{arjovsky2017wasserstein}, etc.). 
The generators are updated by backpropagating loss from multiple sources: GAN loss for realistic translation, cycle-consistency loss and optionally identity loss for within-domain translation. 
Using domain $A$ as an example, we have:
\begin{align}
    \mathcal{L}^{\text{gan}}_{A} &= \mathbb{E}_{A}\gl\paren{\db\paren{\fake{b}}, 1},\\
    \mathcal{L}^{\text{cyc}}_{A} &= \mathbb{E}_{A}\ell_\text{reg}\paren{\reco{a}, a}, \quad \mathcal{L}^{\text{idt}}_{A} = \mathbb{E}_{A}\ell_\text{reg}\paren{\iden{a}, a}
\end{align}
where $\reco{a} = \gba\circ\gab(a)$, $\iden{a} = \gba(a)$, and $\ell_\textrm{reg}$ is any pixel-wise loss function ($L_1$ or $L_2$, etc.). The generator loss is defined as a linear combination:
\newcommand{\nplus}{\!+\!}
\begin{equation}
    \mathcal{L}^{\text{gen}} \!=\! \paren{\mathcal{L}^{\text{gan}}_{A} \nplus \mathcal{L}^{\text{gan}}_{B}} \nplus \lambda^\text{c}\!\paren{\mathcal{L}^{\text{cyc}}_{A} \nplus \mathcal{L}^{\text{cyc}}_{B}} \nplus \lambda^\text{i}\!\paren{\mathcal{L}^{\text{idt}}_{A} \nplus \mathcal{L}^{\text{idt}}_{B}} \label{eq:gen_loss}
\end{equation}



\subsection{Source Driven Style Modulation}
\label{subsec:modulation}


Upon carefully examining images generated by the reference \uvcgan implementation, we find the majority
of them exhibit the characteristic droplet-like artifacts (\autoref{fig:droplet_artifacts}). 
These droplet artifacts are similar to those reported in StyleGANv2~\cite{karras2020analyzing}.
We eliminate these artifacts by removing all instance normalization layers in the U-Net encoding branch and replacing those in decoding branch with learned style modulations.

Specifically, at the bottleneck of the generator, the image is encoded as a sequence of tokens to be fed to the Transformer network. 
We augment this sequence with an additional learnable style token $S$.
The state of the $S$ token at the output of the transformer serves as a latent image style.
For each convolutions layer of the U-Net's decoding branch we generate a specific
style vector $s_i$ from $S$, by trainable linear transformations.

The style modulation~\cite{karras2020analyzing} effectively scales weights $w_{i, j, x, y}$ of the convolutional operator
by the supplied style vector $s_i$, yielding modulated weights:
\begin{equation}
    w_{i, j, x, y}^\prime = s_{i} \cdot w_{i, j, x, y}
    \label{eq:weight_modulation}
\end{equation}
where $i$, $j$ refer to the input and output feature maps and $x$, $y$ enumerate the spatial dimensions.
To preserve the magnitude of the activations, the scaled weights $w_{i, j, x, y}^\prime$ need
to be demodulated. 
The demodulation operation further renormalizes the 
convolution weights as follows:
\begin{equation}
    w_{i, j, x, y}^{\prime\prime} =
        \frac{w_{i, j, x, y}^{\prime}}
            {\sqrt{\sum_{i, x, y} \paren{w_{i, j, x, y}^{\prime}}^2 + \epsilon}}
    \label{eq:weight_demodulation}
\end{equation}
where $\epsilon$ is a small number to prevent numerical instability.

Our approach is different from the \styleganTwo, which generates style vectors $s_i$ for each convolutional operation by performing different affine transformations on the common vector $\mathbf{w}$.
This vector is obtained from a random non-learnable latent code by a multilayer perceptron  network. 
The way how our $S$ is 
processed is similar to the \texttt{[class]} token of the ViT~\cite{dosovitskiy2020image}, however their token are mainly used for classification task.


\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{
        % \tikzexternaldisable
        \tikzsetnextfilename{generator}%
        \begin{tikzpicture}
            % Need to have a background node filled with white, 
            % or otherwise the line number can be seen.
            % May not needed for the final version without line number
            \node[inner sep=1pt, fill=white] at (0, 0) {
                \begin{tikzpicture}
                    \node[inner sep=1pt] (unet) at (0, 0) {\input{fig/unet_modconv}};
                    \node[inner sep=1pt] (evit) at ([yshift=-3pt]$(unet.west)!1.3!(unet.east)$) {\input{fig/vit_extended}};
                    \node[inner sep=1pt, anchor=south] (unet_title) at ([yshift=2pt]unet.north) {A.~U-Net with modulated convolution};
                    \node[inner sep=1pt, anchor=south] (evit_title) at (unet_title.south -| evit) {B.~extended vision transformer};
                    % the zoom in effect
                    \coordinate (unet_up) at ($(unet.north)!.905!(unet.south)$);
                    \coordinate (unet_dn) at ($(unet.north)!.995!(unet.south)$);
                    \coordinate (evit_up) at ([yshift=-20pt]$(evit.north west)!.3!(evit.north east)$);
                    \coordinate (evit_dn) at ($(evit.south west)!.3!(evit.south east)$);
                    \begin{pgfonlayer}{back}
                        \draw[name path=U, white] (unet_up) to[bend right=30] (evit_up);
                        \draw[name path=D, white] (unet_dn) -- (evit_dn);
                        \tikzfillbetween[of=U and D]{path fading=east, fill=gray!30}
                    \end{pgfonlayer}
                \end{tikzpicture}
            };
        \end{tikzpicture}
    }
    \caption{
        \textbf{\thename Generator.} 
        The generator of \thename is a U-Net (Panel A) with an extended vision transformer bottleneck (eViT, Panel B). 
        The eViT outputs a style token for the modulated convolution blocks~\cite{karras2020analyzing} ($M_i$, $i=1,2,3,4$) in the decoding path of the U-Net.
        Refer to \cite{torbunov2023uvcgan} for details about the input layer, output layer, basic block $B_i$, downsampling block $D_i$, and upsampling block $U_i$ in the U-Net and the transformer encoder block in the eViT.
    }
    \label{fig:uvcgan2_generator}
\end{figure*}




\subsection{Batch Statistics Aware Discriminator}
\label{subsec:method_batch_head}

\begin{figure}[t]
    \centering
    % \tikzexternaldisable
    \tikzsetnextfilename{batchhead}%
    \begin{tikzpicture}
        \node[inner sep=0, fill=white] at (0, 0) {\input{fig/disc_batchhead}};
    \end{tikzpicture}
    \caption{\textbf{\thename Discriminator with a batch head.} 
             For the batch statistics block, we can use either a standard batch normalization layer or batch standard deviation~\cite{karras2017progressive}.}
    \label{fig:batch_head}
\end{figure}
In our experiments, the \patchgan discriminator architecture, used in the original \uvcgan, frequently causes a partial mode collapse~\cite{goodfellow2016nips}.
To workaround this problem, we apply a variation of the Minibatch discrimination technique~\cite{salimans2016improved,karras2017progressive}.

Motivated by the neural architectures of the contrastive methods~\cite{chen2020simple}
we design a composite discriminator made of a main \textit{body} and a \textit{batch head}. The body of the composite discriminator can be any common discriminator, but, for the purposes of this work, we use \patchgan without the last layer.

The batch head is designed to equip the discriminator with a minibatch discrimination power and prevent the mode collapse. It is made of a layer that captures batch statistics, followed by two convolutional layers (\autoref{fig:batch_head}).
Such a modular discriminator architecture allows one to easily swap different
discriminator bodies, while still preserving the minibatch discrimination power
of the batch head.

\paragraph{Batch Statistics Layers.}
Many neural layers can be used to capture batch statistics. In this work,
we test two types of layers: batch standard deviation (BSD), introduced by \progan~\cite{karras2017progressive}, and a simple Batch Normalization (BN), which has been found effective for preventing mode collapse in representation learning~\cite{Fetterman2020Understanding}.

\paragraph{Cache of Discriminator Features.}
For the minibatch discrimination method to work, the model must be trained with batch sizes greater than 1. However, in our experiments, \cyclegan models achieve
much better performance (for the same time/compute budget) when trained with a batch size of 1. To reconcile the batch size of 1 and the minibatch discrimination technique, we maintain a history (cache) of past inputs to the batch head.

During training, the batch feature statistics are stored in four separate caches: real images from domain~\domA, real images from domain~\domB, and fake images from both domains. All the caches have a fixed size and follow the first-in-first-out (FIFO) update policy. 

The discriminator's batch head receives a concatenation (along the batch dimension) of the discriminator body output for the current minibatch along with a history of the past outputs from a cache (\autoref{fig:batch_head}).
The usage of feature caches allows disentangling the size of the minibatch from the size of the statistical sample of features provided to the batch head. It also synergizes with the composite discriminator architecture, allowing one to cache outputs of the discriminator body, which are expensive to recompute.














\subsection{Pixel-wise Consistency Loss}
\label{subsec:consistLoss}
To improve the consistency of the generated and source images, we experiment with the addition of
an extra term $\mathcal{L}_\text{consist}$ to generator loss~\eqref{eq:gen_loss}. This term captures
the $L_1$ difference between the downsized versions of the source and translated images.
For example, for images of domain $A$
\begin{equation}
    \mathcal{L}_{\text{consist}, A} = \mathbb{E}_{A}\ell_1\paren{F(\gab(a)), F(a)}  
\end{equation}
where $F$ is a resizing operator down to $32 \times 32$ pixels (low-pass filter).
We add this term to the generator loss~\eqref{eq:gen_loss} with a magnitude $\lambda_\text{consist}$
for both domains.

\subsection{Modern Training Techniques}
\label{subsec:TrainingTech}
\uvcgan and \cyclegan use outdated GAN training techniques. 
Hence, we revamp the training procedure with a few modern additions, 
which include adding exponential averaging of the generator weights~\cite{karras2017progressive}, 
implementing spectral normalization of the discriminator weights~\cite{miyato2018spectral}, 
trying unequal learning rates for the generator and discriminator~\cite{heusel2017gans},
and replacing the generic gradient penalty (GP) with an improved zero-centered GP version~\cite{thanh2019improving}.
