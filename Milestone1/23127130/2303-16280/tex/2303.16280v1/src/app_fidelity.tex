\section{Metrics of Faithfulness to the Source}
\label{sec:Faithfulness}

\input{fig/grid_consistency_suppl}

In this section, we provide more examples to
illustrate
that I-$L_2$ (defined as the $L_2$ distance
between the latent Inception-v3 features) may be a more appropriate measurement of faithfulness to the source than pixel-level $L_2$.
In \autoref{tab:comparingTransWithL2}, we select translations according to two types of criteria.
Denote a translation produced by \egsde as $E$ and that by \thename as $U$.
\[
\begin{aligned}
&\text{\textbf{Type 1}: I-$L_2(E) \approx $ I-$L_2(U)$ and $L_2(U) - L_2(E) > 15$.}\\
&\text{\textbf{Type 2}: I-$L_2(E) - $ I-$L_2(U) > 3$.}
\end{aligned}
\]
\textbf{Type~1} is designed to show what contributes to lower pixel-wise $L_2$ while I-$L_2$-s are similar. 
\textbf{Type~2} selects examples with large I-$L_2$ difference and helps readers to judge if I-$L_2$ correlates with their own judgment on the similarity to the source (i.e.~which pairs look more like siblings.)

We list eight categories of perceived image faithfulness (PIF) in the legend such as background, bone structure, facial expression, and so on.  
For each translated image, we indicate which categories it outperforms that generated by the other model.
For example, for the input in Type 1 row 1 left, \egsde preserves the hairstyle and color (PIF=7) better than \thename, 
but the \thename translation maintains a sharper background~(1), preserves the bone structure~(2) and expression~(3) better, 
and exhibits more similarity in apparent age~(4). 

Type-1 examples suggest that pixel-level $L_2$ is an inappropriate measurement for semantic consistency as \thename translations manage to capture characterizing features (such as a bone structure) even with high pixel-level $L_2$. 
In fact, the high pixel-level $L_2$ of \thename translations is often a result of benign modifications such as the elongation of dark hair on a light background (e.g.~Type 1 row 5 right) or a slight overall shift to a warmer hue (e.g.~Type 1 row 2 right). 

On the contrary, the Type-2 examples suggest that I-$L_2$, the $L_2$ on Inception latent features, might be a better measurement of semantic consistency. 
While \egsde fails to maintain features such as background, facial expression (neural v.s.~smile), eye movement, and prominent bone structure and produces over-generalized translations, \thename translations with significantly lower I-$L_2$ manage to preserve those features and appear more individualized.

These examples illustrate that the pixel-wise $L_2$ faithfulness metric may be in poor agreement with a human judgment of image faithfulness. They also point to a possibility that the  I-$L_2$ distances, based on deep features of Inception-v3, may better capture the perceived image faithfulness. Such observations mirror the conclusion of~\cite{zhang2018unreasonable}, about the effectiveness of deep features as perceptual metrics.

However, we stress again, that the main purpose of this paper is to improve the performance of the classic \cyclegan architecture, not the development of better faithfulness metrics. 
While this section points to a possibility of I-$L_2$ being a better faithfulness metric, a full-scale investigation needs to be conducted to conclusively establish this. 
We leave such a study for future work.