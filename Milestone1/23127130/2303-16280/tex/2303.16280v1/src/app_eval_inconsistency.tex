\section{Remarks on Metric Evaluation Consistency}
\label{sec:RemarksMetricEvalConsistency}

Inconsistency of unpaired I2I evaluation procedures is a widespread problem.
For example, some works (e.g.~\cite{kim2019u}) roll out their own FID evaluation code~\cite{ganmetricsmean} and report the so-called "mean" FID and KID scores, where ``mean'' means a weighted average of
the actual FID/KID scores and various other metrics. Some other works~\cite{parmar2022aliased}
choose different image resizing algorithms, creating a noticeable discrepancy in the reported FID scores.

Another source of FID/KID score inconsistency is the difference in the evaluation protocols. For instance,
works like~\cite{torbunov2023uvcgan} prefer to evaluate FID scores only on images of the test split, yet others~\cite{choi2020stargan} evaluate FID scores between  translated test images and target
images of the train split. Likewise, there is a difference in whether any pre-processing is used in the FID/KID evaluation.
For example, one can evaluate FID scores between translated images with the pre-processing and target images
without pre-processing~\cite{park2020contrastive}, or one can apply the pre-processing step to both translated and
target images~\cite{zhao2022egsde}. Moreover, the pre-processing procedures may differ between different works.

To uniformize FID/KID evaluation procedures, we propose a consistent evaluation protocol in \autoref{sec:consistent_eval}.
