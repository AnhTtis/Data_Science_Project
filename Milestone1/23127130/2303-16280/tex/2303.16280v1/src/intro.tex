\section{Introduction}


Image-to-image (I2I) translation models aim to find a mapping between two domains of images. When paired examples of images from two domains are available, such a mapping can be easily solved in a supervised manner. A more interesting case of I2I problems is the unpaired I2I translation, where examples of pairs are not available. The ability to perform an unpaired I2I translation is highly beneficial since obtaining paired datasets in the real world is often impossible, difficult, or time-consuming~\cite{boulanger2021deep}.


The advancement of unpaired I2I largely benefits from recent developments in deep generating models such as (variational) Autoencoder~\cite{hinton2006reducing, kingma2013auto}, generative adversarial networks (GANs), generating flows~\cite{rezende2015variational, dinh2014nice, dinh2016density, kingma2018glow}. One of the early successful unpaired I2I models is CycleGAN~\cite{zhu2017unpaired} that uses a cycle-consistency constraint, requiring that a cyclic back-and-forth translation between two domains results in the original image. Several succeeding models inspired by CycleGAN, such as STARGAN~\cite{choi2018stargan,choi2020stargan}, SEAN~\cite{zhu2020sean}, U-GAT-IT~\cite{kim2019u}, and CUT~\cite{park2020contrastive}, are designed to further enhance the quality and diversity of the generated images. However, GAN-based I2I methods lag behind the general developments in the GAN architecture and training procedures~\cite{karras2020analyzing,thanh2019improving}.

An alternative route to image generation is provided by \textit{diffusion models}~\cite{ho2020denoising}. With a recent spike of interest in such models, several
applications of DMs to unpaired I2I translation have been developed~\cite{choi2021ilvr,meng2021sdedit,zhao2022egsde}. Despite being recent, the DM-based \egsde~\cite{zhao2022egsde} approach has demonstrated superior results on several benchmarks. However, the DM-based solutions may perform a suboptimal translation, since they
do not use source images during the training~\cite{zhao2022egsde}.
Additionally, the DM-based methods rely on pixel-wise $L_2$ distances to maintain the consistency of the source and translated images.
Such a simple consistency measure is not guaranteed to preserve any semantically meaningful features and can restrict image transformations.



In the past, the approach of revisiting a classic neural architecture and improving it with a number of modern additions has led to large improvements in performance~\cite{liu2022convnet,karras2020analyzing,bello2021revisiting}. Based on this observation, we revisit one of the earliest GAN-based I2I models -- the \cyclegan. Unlike the DM-based models, \cyclegan's training procedure is able to effectively utilize images from the source and target domains simultaneously.
Moreover, \cyclegan maintains an intrinsic consistency between the source and translated images (via the cycle-consistency constraint), a feature that cannot be achieved by simple pixel-wise consistency measures.
In addition, recently, \uvcgan~\cite{torbunov2023uvcgan} work has shown that the \cyclegan performance can be significantly improved by modernizing its architecture.

Motivated by \uvcgan's success we would like to rethink the classic \cyclegan architecture further. We take \uvcgan as a starting point, and redesign its generator, discriminator and training procedure to obtain a revised model -- \thename.

















\paragraph{Our Contributions.} This work makes several technical improvements to the \uvcgan~\cite{torbunov2023uvcgan} architecture:
\begin{itemize}[nosep,leftmargin=1em]
  \item We redesign the \uvcgan generator model and introduce style modulation to its decoding
    branch. We propose a style generation mechanism via a learnable Transformer token.
  \item We propose a modular discriminator architecture made of a traditional discriminator
    body and a special head, which prevents the problem of mode collapse~\cite{goodfellow2016nips}. We augment the discriminator with a cache of past discriminator encodings, allowing it to directly compare feature statistics between distributions of target and translated images.
    \item Combined with better training strategies, we demonstrate that the revised \uvcgan model is able to outperform the most advanced competitors by a large margin.
    \item We highlight the inconsistencies of the current unpaired I2I evaluation protocols and suggest a better faithfulness measure, based on deep
    image representations of Inception-v3.
\end{itemize}
