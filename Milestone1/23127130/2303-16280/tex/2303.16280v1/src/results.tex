\section{Results}

\begin{figure*}[t]
    \centering
    % \tikzexternaldisable
    \tikzsetnextfilename{grid_LQ}
    \input{fig/grid}
    \begin{tikzpicture}
        
        \def\width{.1\textwidth}
        \def\expand{1.03}
        \def\folder{fig/grid_LQ}
        
        \node[inner sep=0, fill=white] at (0, 0) {
            \begin{tikzpicture}
                \node[inner sep=0] (G1) at (0, 0) {\gridWithHeader[1]{selfie2anime}{0, 3, 6}{input/input, UVCGAN2/\thename}{\selfieanime}};
            	\node[inner sep=0, anchor=west] (G2) at ([xshift=.03 * \width]G1.east) {\gridWithHeader{male2female}{4, 1, 9}{input/input, UVCGAN2/\thename}{\malefemale}};
            	\node[inner sep=0, anchor=west] (G3) at ([xshift=.03 * \width]G2.east) {\gridWithHeader{rmvGlasses}{4, 7, 8}{input/input, UVCGAN2/\thename}{\rmvGlasses}}; 
            \end{tikzpicture}
        };
    \end{tikzpicture}
    \caption{\textbf{Sample translations for \celeba and \anime. } We show the translations produced by \ugatit, \uvcgan, and \thename for three tasks: \selfieanime, \malefemale, and \rmvGlasses. The full grid with all benchmarking results and that for the three opposite translations can be found in A\autoref{sec:additionalTransSamples}.}
    \label{fig:grid_LQ}
\end{figure*}

\begin{figure*}[t]
    \centering
    % \tikzexternaldisable
    \tikzsetnextfilename{grid_HQ}
    \input{fig/grid}
    \begin{tikzpicture}
    
        \def\width{.1\textwidth}
        \def\expand{1.03}
        \def\folder{fig/grid_HQ}
    
        \node[inner sep=0, fill=white] at (0, 0) {
            \begin{tikzpicture}
                \node[inner sep=0] (G1) at (0, 0) {\gridWithHeader[1]{male2female}{1, 5, 8}{input/input, EGSDE1/\egsde, EGSDE2/\egsdeDG, UVCGAN2wo/\thename}{\malefemale}};
            	\node[inner sep=0, anchor=west] (G2) at ([xshift=.03 * \width]G1.east) {\gridWithHeader{cat2dog}{4, 7, 8}{input/input, EGSDE1/\egsde, EGSDE2/\egsdeDG, UVCGAN2wo/\thename}{\catdog}};
            	\node[inner sep=0, anchor=west] (G3) at ([xshift=.03 * \width]G2.east) {\gridWithHeader{wild2dog}{1, 4, 5}{input/input, EGSDE1/\egsde, EGSDE2/\egsdeDG, UVCGAN2wo/\thename}{\wilddog}}; 
            \end{tikzpicture}
        };
    \end{tikzpicture}
    \caption{\textbf{Sample translations for \celebahq and \afhq. } We show translations for three tasks: \malefemale, \catdog, and \wilddog. More translations for these three tasks and those for \wildcat can be found in \autoref{sec:additionalTransSamples}.}
    \label{fig:grid_HQ}
\end{figure*}

\subsection{Metrics}

There are two dimensions along which the unpaired I2I style transfer models can be evaluated:
\textit{Faithfulness} and \textit{Realism}. Faithfulness captures the degree of similarity between the
source and its translated image at an individual level. 
Realism attempts to estimate the overlap of the distributions
of the translated images and the ones in the target.

The quality of image translation, in terms of realism, is commonly judged according to the
FID~\cite{heusel2017gans} and kernel inception distance
(KID)~\cite{binkowski2018demystifying} metrics. 
Both metrics measure the
distance between the distributions of the latent Inception-v3~\cite{szegedy2016rethinking}
features extracted from samples of the translated and target images. Smaller FID and KID values
indicate more realistic images.

Early GAN-based works (e.g.,~\cite{zhu2017unpaired,nizan2020breaking,zhao2020unpaired,kim2019u}) do not
explicitly evaluate the faithfulness of the translation. 
To the best of our knowledge, there is no widely accepted faithfulness metric available. 
Some works~\cite{zhao2022egsde} try to employ simple pixel-wise $L_2$, peak-signal-to-noise ratio (PSNR), or structural similarity index measure (SSIM)~\cite{wang2004image} scores to capture the agreement between the source and translation. 
Yet it is unclear how well these pixel-wise metrics relate to the perceived image faithfulness.



\subsection{Evaluation Protocol}

Evaluation protocols differ drastically between different papers (see \autoref{sec:RemarksMetricEvalConsistency}). %(see SM-Section 3).
This makes the direct comparison of the translation quality metrics extremely challenging.
For the fairness of comparisons with older works, we follow different evaluation protocols,
depending on the dataset.

\paragraph{\celeba and \anime.}
When evaluating the quality of translation on the \celeba \malefemale, \celeba \glasses Removal, and \anime datasets,
we use the evaluation protocol of \uvcgan~\cite{torbunov2023uvcgan}, which uniformized FID/KID evaluation across multiple datasets and models, allowing for a simple FID/KID comparison.
For the actual FID/KID evaluation, we rely on \texttt{torch-fidelity}~\cite{obukhov2020torchfidelity}, which provides
a validated implementation of these metrics.

The actual evaluation protocol for \celeba and \anime relies only on test splits to perform the FID/KID
evaluation. For the \celeba dataset, we use KID subset size of $1000$. 
For the \anime dataset, we use the KID subset size of $50$. 
We use unprocessed images of size $256 \times 256$ when evaluating on the \anime dataset. 
For the \celeba dataset, we apply a simple pre-processing to both domains: resizing
the smaller side to $256$ pixels, then taking a center crop of size $256 \times 256$.

\paragraph{\celebahq and \afhq.} 
\egsde~\cite{zhao2022egsde} has evaluated multiple models on the \celebahq and \afhq datasets under
similar conditions. To compare our results to \egsde, we replicate its evaluation protocol for \celebahq 
and \afhq.
For the \afhq dataset, we evaluate FID and KID scores between the translated images of size $256 \times 256$ and
the target images of size $512 \times 512$ from the validation split.
For the \celebahq dataset, we evaluate the FID/KID scores between the translated images of size $256 \times 256$
and the downsized target images of size $256 \times 256$ from the train split. We perform the same channel
standardization as \egsde with $\mu = (0.485, 0.456, 0.406)$ and $\sigma = (0.229, 0.224, 0.225)$.
To ensure full consistency, we use the reference evaluation code provided by \egsde~\cite{egsderef}. 
\autoref{sec:consistent_eval} %SM-Section 5 
provides results of an alternative evaluation protocol,
which is uniform across all the datasets.


\subsection{Quantitative Results}

\paragraph{\celeba and \anime.}
\autoref{tab:results_lq} shows a comparison of the \thename (trained without piwel-wise consistency loss) performance against
\aclgan~\cite{zhao2020unpaired}, \council~\cite{nizan2020breaking},
\cyclegan~\cite{zhu2017unpaired}, \ugatit~\cite{kim2019u}, and
\uvcgan~\cite{torbunov2023uvcgan}.
The performance of the competitor models is obtained from the \uvcgan paper~\cite{torbunov2023uvcgan}.
According to \autoref{tab:results_lq}, \thename outperforms all the competitor models in all translation directions, except \animeselfie. The degree of improvement ranges from about $5\%$
in terms of FID on the \selfieanime translation, to around $51\%$ on the \malefemale
translation. Likewise, there is a significant improvement in the KID scores from about
$13\%$ on \animeselfie to $79\%$ on \malefemale. Such a degree of improvement demonstrates
the effectiveness of modern additions to the traditional \cyclegan architecture. \autoref{fig:grid_LQ} provides a few translation samples. More samples can be found in \autoref{sec:consistent_eval}. %SM-Section 6.

\input{table/results_lq}

\paragraph{\celebahq and \afhq.}
\autoref{tab:results_hq} compares the results of the \thename evaluation against \cut~\cite{park2020contrastive}, \ilvr~\cite{choi2021ilvr}, \sdedit~\cite{meng2021sdedit},
and two versions of the \egsde~\cite{zhao2022egsde}.
In particular, this table compares two versions of the \thename: \thename and \mbox{\thename-C}. \mbox{\thename-C} is a version
that was trained with a pixel-wise consistency loss and $\lambda_\text{consist} = 0.2$.
The performance of the competitor models is extracted from \egsde~\cite{zhao2022egsde}. 

\input{table/results_hq_v2}

\autoref{tab:results_hq} shows that \thename achieves the best translation quality, according
to the FID scores, with improvements ranging from $10\%$ on \wilddog translation to
$43\%$ on \malefemale translation. The addition of the consistency loss allows the \thename-C
model to improve its pixel-wise PSNR and SSIM metrics, but at the expense of the FID score on \afhq translation.
\thename and \thename-C achieve competitive SSIM scores but lose in terms of the PSNR ratio
to the other models. However, as was pointed out before~\cite{zhang2018unreasonable}, pixel-wise measures PSNR and SSIM are not good metrics to judge perceptual image
faithfulness. In the next subsection, we try to investigate better perceptual
faithfulness metrics.


\begin{table*}[t]
\begin{minipage}[b][][b]{0.54\textwidth}
\centering
\caption{\textbf{Measuring Faithfulness with $L_2$.} 
         The pixel-wise $L_2$ is labeled as $L_2$ and the $L_2$ between the latent Inception-v3 features is labeled as I-$L_2$.
         More examples, like the two on the right, can be found in \autoref{sec:Faithfulness}.}
\label{tab:results_hq_consist_only}
\input{table/results_hq_consist_transposed}
\end{minipage}\hfill
\begin{minipage}[b][][b]{0.45\textwidth}
\centering
\input{fig/grid_consistency}
\end{minipage}
\end{table*}

Overall, the gains in SSIM and PNSR scores, provided
by the consistency loss to \thename-C, do not seem to outweigh the associated FID losses.
\autoref{fig:grid_HQ} demonstrates a few translation samples, with more samples available
in \autoref{sec:consistent_eval}.%SM-Section 6.

\subsection{Toward Better Faithfulness Measures}


The pixel-wise image similarity measures (such as $L_2$, PSNR, and SSIM) have been shown~\cite{zhang2018unreasonable} to be weakly correlated with human perception of similarity. However, they are currently being used~\cite{zhao2022egsde} as a faithfulness metric in the area of the unpaired I2I translation.

Following the discussions of perceptual content similarity measures~\cite{zhang2018unreasonable,johnson2016perceptual}, we believe that a proper
faithfulness measure should be based on deep image representations. Since the primary goal of this paper is to revisit the classic \cyclegan architecture, we are not going
to perform an in-depth investigation of possible faithfulness metrics. Instead, we will briefly consider the usage of the $L_2$ distance between the latent Inception-v3~\cite{szegedy2016rethinking} features as an alternative faithfulness measure.

In \autoref{sec:Faithfulness}, %SM-Section~4, 
we present a large sample of images, generated by \egsde and \thename models. It helps to investigate how well the pixel-wise $L_2$ measure is correlated with a  perception of image faithfulness. In this part of the paper, we provide two
representative samples that demonstrate: 1. pixel-wise $L_2$ faithfulness measure penalizes image changes that should be freely-modifiable during the translation (e.g.\ hairstyle); 2. pixel-wise
$L_2$ faithfulness measure fails to effectively penalize changes to features that are
expected to be preserved (e.g.\ background, facial structure, etc).

The first row of images, to the right of \autoref{tab:results_hq_consist_only}, provides a typical sample of \malefemale translations, comparing
\egsde and \thename models. Both translation samples are in good agreement with the
source image. However, the \egsde translation has a pixel-wise $L_2$ difference of $48$ to the source,
while the \thename one has twice as high $L_2 = 96$. The high $L_2$ difference is
caused by a large amount of hair added by the \thename.
Naturally, one may expect the hairstyle to be a free parameter of the \malefemale
translation. Yet, the pixel-wise faithfulness metrics will highly penalize its changes.

The second row of images, to the right of \autoref{tab:results_hq_consist_only}, demonstrates another sample of \malefemale translation,
where pixel-wise $L_2$ are similar between \egsde ($L_2 = 45$)
and \thename ($L_2 = 43$). 
Yet, the images are rather different in structure. 
The one from \thename preserves the details better: the image background, the bone
structure of the face, the shape of the ear, forehead, nose, and even smile. 
The Inception-v3 scores also correlated with this observation, showing a preference for the \thename image ($13.7$ versus $17.0$).
These observations suggest that the Inception-v3-based distance measure
may be a better faithfulness metric than a simple pixel-wise one.
However, we defer the proper investigation for future works. 
To conclude, we show the comparison of pixel-wise and Inception-v3 $L_2$ faithfulness metrics
between \egsde and \thename models in \autoref{tab:results_hq_consist_only}. This table shows that
the \thename model provides better faithfulness in terms of the Inception-v3 features,
while \egsde is more faithful if measured in pixel-wise distances between the source and translated images.

