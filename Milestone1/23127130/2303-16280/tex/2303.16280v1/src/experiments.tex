\section{Experiments}



\subsection{Datasets}

We study the performance of \thename on two groups of datasets. The formerly widely used \celeba~\cite{liu2015deep}
and \anime~\cite{kim2019u} datasets and the modern, high-quality \celebahq~\cite{karras2017progressive} and
\afhq~\cite{choi2020stargan} datasets. %In this section, we provide a high-level description of the datasets.
More details about these datasets can be found in \autoref{sec:DatasetDetail}.%Supplementary Material (SM)-Section 1.

\paragraph{\celeba and \anime.} 
The \celeba and \anime datasets have been commonly used to benchmark GAN-based unpaired I2I translation
algorithms~\cite{torbunov2023uvcgan,zhu2017unpaired,nizan2020breaking,zhao2020unpaired,kim2019u}.
We study \thename performance on three tasks related to the \celeba and \anime datasets: \gender translation
on the \celeba dataset, Glasses Removal on the \celeba dataset, and Selfie-to-Anime translation on the \anime dataset.
Because the \cyclegan setup learns translations in both directions simultaneously,
we also get benchmarks in the opposite directions (Female-to-Male, \glasses Addition,
and Anime-to-Selfie translations).

\paragraph{\celebahq and \afhq.} 
To compare the performance of the \thename against more recent unpaired I2I translation
algorithms, such as EGSDE~\cite{zhao2022egsde}, we also consider the \celebahq and \afhq datasets. 
We investigate \malefemale translation on \celebahq
and three translations: \catdog, \wilddog, and \wildcat on \afhq.

\paragraph{Preprocessing.} 
For a fair comparison with EGSDE~\cite{zhao2022egsde}, we downsize the \celebahq and \afhq images
to $256 \times 256$ pixels. To avoid Fr\'{e}chet inception distance (FID) evaluation inconsistencies associated with a difference
in the interpolation procedures between different frameworks~\cite{parmar2022aliased}, we use the 
Pillow~\cite{pillow} image manipulation library to perform the image resizing with Lanczos
interpolation.


\subsection{Training}

When training our modified \uvcgan implementation, we seek to closely follow the original
procedure~\cite{torbunov2023uvcgan}. It consists of two steps. First, pre-training of the generator
in a self-supervised way on a task of image inpainting. The second step is the actual training of
the unpaired I2I translation networks, starting from the pre-trained generators.

\paragraph{Generator Pre-training.} 
For each dataset, the generators are pre-trained on image inpainting tasks. This task is set up
in a fashion similar to the Bidirectional Encoder Representations from Transformers (BERT) pre-training~\cite{devlin2018bert,torbunov2023uvcgan}. For the inpainting task,
input images of size $256 \times 256$ pixels are tiled into a grid of patches at $32 \times 32$ pixels.
Then, each patch is masked with a probability of $40\%$. The masking is performed by zeroing out
pixel values. The generator is tasked to recover the original unmasked image from a masked one. More details about this pre-training are available in \autoref{sec:TrainingDetails}.%SM-Section 2.

\paragraph{Translation Training.} 
The unpaired image translation training is performed for 1 million iterations with the help of the Adam optimizer. 
Depending on the dataset, we use various data augmentations. 
For the preprocessed datasets, such as \celebahq and \afhq, only a random horizontal flip is applied.
For the \anime and \celeba datasets, we use three
augmentations: resize, followed by a random crop of size $256 \times 256$, followed by a random
horizontal flip. Resizing for the \anime dataset is done from $256 \times 256$ up to
$286 \times 286$. For the \celeba dataset, the resizing is done from $178 \times 218$ to $256 \times 313$.

\paragraph{Hyperparameter Tuning.} 
For each translation, we perform a quick hyperparameter search, exploring the space of the
cycle-consistency magnitude $\lambda_\text{cycle}$, magnitude of the zero-centered gradient
penalty $\lambda_\text{GP}$, magnitude of the consistency loss $\lambda_\text{consist}$, 
learning rates of the generator and discriminator, and the choice of the batch head (BN versus BSD).
We also explore turning the learning rate scheduler on and off. 
Refer to \autoref{sec:TrainingDetails} %SM-Section 2 
for more details about the training procedure.