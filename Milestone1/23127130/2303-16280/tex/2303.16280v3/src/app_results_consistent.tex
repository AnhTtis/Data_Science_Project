\section{Toward Consistent FID Evaluation}
\label{sec:consistent_eval}

\input{table/app_results_hq}

The evaluation protocols used in the paper for \celebahq and \afhq are provided by \egsde~\cite{zhao2022egsde}.
Being ad-hoc, these protocols lack consistency and differ significantly
depending on the dataset.
A variety of different evaluation protocols makes the evaluation of the unpaired I2I methods rather quirky and error-prone.

As a step toward consistent FID evaluation,
we extend a more uniform evaluation protocol of \uvcgan~\cite{torbunov2023uvcgan}
toward the AFHQ and CelebA-HQ datasets.
The \thename evaluation results under this consistent evaluation protocol are shown
in \autoref{tab:app_results_hq}.
The consistent evaluation protocol uses only test splits (or validation splits if the test ones are not available) of each dataset to assess the quality of image translation.

The evaluation protocol begins with pre-processing all the 
 %test
 datasets in a consistent manner. The pre-processing step resizes images from their
original size down to $256 \times 256$ pixels (the same image size as is used for model training and inference). To avoid FID score inconsistencies created by aliasing artifacts~\cite{parmar2022aliased} we rely on the \texttt{Pillow} library~\cite{pillow} and Lanczos interpolation method.

Once the data pre-processing and image translation are done, the actual evaluation can begin. To perform the FID/KID score computation we use
a \texttt{torch-fidelity} package~\cite{obukhov2020torchfidelity}, which provides
a validated implementation of these metrics. The KID evaluation procedure depends on a free parameter -- the KID subset size. In this section, we choose the KID subset size of 100 for all the datasets.

The suggested evaluation protocol differs in a number of ways from the evaluation
protocols of the \afhq and \celebahq datasets of \egsde. It differs from the
ad-hoc \celebahq evaluation protocol~\cite{zhao2022egsde} because the latter compares FID scores between samples of validation and train splits, while the consistent version only uses validation split. The consistent evaluation protocol is also different from  the ad-hoc version of the \afhq one, which performs FID evaluation between translated images of size $256 \times 256$ and target images of size $512 \times 512$. The consistent protocol always uses pre-processed images of size $256 \times 256$.

\subsection{\celebahq Consistent Evaluation}
\label{sec:app_train_uvcgan_consist}

In this section, we elaborate on the \celebahq FID scores obtained with the
consistent protocol.
\autoref{tab:app_results_hq} shows the FID evaluation scores of \thename according to the 
consistent evaluation protocol. A few observations one can draw comparing the
results from this table to the ad-hoc evaluations, presented in Table 2 of the main paper.
In particular, there is a good agreement of the FID scores on the \afhq dataset,
since the ad-hoc evaluation procedure is very similar to the consistent one.
However, there is a large discrepancy in the FID scores on \celebahq.

Several factors contribute to the discrepancy of the FID scores on the \celebahq dataset. First,
the ad-hoc method evaluates the FID scores against the train part of the dataset, while the
consistent method uses validation parts.
This explains the large difference in the scale of the FID values~\cite{binkowski2018demystifying}.
Second, the consistent evaluation uses a Lanczos interpolation method,
less affected by aliasing artifacts.
Finally, the consistent version relies on a validated FID calculation code of \texttt{torch-fidelity},
while the ad-hoc method of \egsde uses a custom FID implementation.

Due to the large differences in the evaluation protocols on the \celebahq dataset,
it may be instructive to compare FID scores on \celebahq,
obtained with the consistent protocol.
\autoref{tab:app_results_uvcgan_consist}
compares the performance of \egsde, \uvcgan, and \thename under the consistent evaluation mode.
To a large extent, the behavior of the FID scores in \autoref{tab:app_results_uvcgan_consist}
matches the behavior of the FID scores of the ad-hoc protocol.
At the same time, according to the consistent evaluation protocol, \thename outperforms
\uvcgan by $15 - 20 \%$ in the FID scores.
This is in contrast to the ad-hoc evaluation procedure of Table 2 (the main part of the paper)
which does not reflect any difference in the performance.