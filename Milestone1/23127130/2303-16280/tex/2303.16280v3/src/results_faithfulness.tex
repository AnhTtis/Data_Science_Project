
\subsection{Toward Better Faithfulness Measures}
\label{sec:better_faithfulness}

\input{table/results_consist}

Pixel-wise image similarity measures (such as $L_2$, PSNR, and SSIM) have been shown~\cite{zhang2018unreasonable} to be weakly correlated with the human perception of similarity. However, they currently are being used~\cite{zhao2022egsde} as a faithfulness metric in the area of unpaired I2I translation.
In this section, we explore using CLIP~\cite{radford2021learning}
and LPIPS~\cite{zhang2018unreasonable} scores as similarity measures because they have been shown to match human perception.
In addition, it may be more natural to build a similarity measure reusing Inception-v3 features that are already employed in the FID calculation.
This will avoid introducing new dependencies, parameter choices, and other sources of inconsistency.
Thus, as another faithfulness metric, we propose an \textit{Inception-v3 $L_2$ distance} (I-$L_2$) formed between the Inception-v3 features.

\autoref{tab:results_consist} compares the faithfulness scores according to CLIP, LPIPS, I-$L_2$, and pixel-wise $L_2$.
It indicates that \thename variants outperform \egsde variants on the perceptual metrics.
On the other hand, \egsde shows superiority in the pixel-wise scores.
At the same time, \uvcgan demonstrates better perceptual faithfulness compared to \thename.
Given that \thename has better realism than \uvcgan,
this likely demonstrates an example of a realism-faithfulness trade-off.
The trends of the proposed I-$L_2$ roughly match those of the more complex CLIP and LPIPS measures, illustrating the feasibility of an I-$L_2$ as a simpler similarity measure.



Fundamentally, one still may wonder if any of these metrics are ``proper'' measurements of faithfulness?
When measuring faithfulness, one should differentiate between the \textit{domain-contrastive}
features (changing between domains) and \textit{domain-consistent} features (expected to be preserved).
For example, in the \celebahq dataset, males tend to have shorter hair compared to females.
Thus, we expect a Male-to-Female I2I algorithm will increase hair length on average
(domain-contrastive feature).
On the other hand, image background, skin color, facial expression, and facial orientation
are approximately the same in both domains.
Therefore, an I2I algorithm is expected to preserve these features (domain-consistent features).
We argue a proper faithfulness metric should pay the most attention
to domain-consistent features
and be indifferent to the domain-contrastive features.


\begin{figure}[ht]
    \centering
    \tikzsetnextfilename{face_landmark}
    \resizebox{\linewidth}{!}{
    \begin{tikzpicture}
        \input{fig/grid}
        \def\folder{fig/face_landmarks}
        \def\index{1}
        
        \def\width{.12\textwidth}
        \def\expand{1.02}

        \node[inner sep=0, fill=white] at (0, 0) {
            \begin{tikzpicture}
                \foreach \name/\label [count=\col] in {input/input, 
                                                       egsde1/\egsde, 
                                                       egsde2/\egsdeDG,
                                                       uvcgan1/\uvcgan,
                                                       uvcgan/\thename} {
                    % image
                    \def\fname{\folder/\name_\index.jpg}
                    \coordinate (C) at (\width * \expand * \col, 0);
        		  \node[inner sep=0] (image_\col) at (C) {\includegraphics[width=\width]{\fname}};
                    % label
                    \tikzmath{coordinate \C;\C = (image_\col.north east) - (image_\col.south west);}
                    \node[header, minimum width=\Cx, minimum height=.2 * \width, anchor=south west] at (image_\col.north west) {\label};
                }
                \foreach \name/\label/\ll [count=\col] in {input/input/0, 
                                                           egsde1/\egsde/0.0150, 
                                                           egsde2/\egsdeDG/0.0250, 
                                                           uvcgan1/\uvcgan/0.0063,
                                                           uvcgan/\thename/0.0055} {
                    % image
                    \def\fname{\folder/\name_\index_lm.jpg}
                    \coordinate (C) at (\width * \expand * \col, -\width * \expand);
        		  \node[inner sep=0] (image_\col) at (C) {\includegraphics[width=\width]{\fname}};
                    \ifthenelse{\col > 1}{
                        % \pgfmathtruncatemacro\exampleNo{\col + #1}
                        % \tikzmath{coordinate \C;\C = (input_\col_lm.north east) - (input_\col.south west);}
                        \node[fill=white, fill opacity=.7, text opacity=1, inner sep=2pt, anchor=north east] at (image_\col.north east) {\scriptsize Lm-$L_2 =\ll$};
                    }{}
                }
            \end{tikzpicture}
        };
    \end{tikzpicture}
    }
    \caption{Examples of face landmark distance (Lm-$L_2$) between an input and a translated image. 
             % On average: EGSDE~(0.0188), EGSDE$^\dag$~(0.0227), and UVCGAN~(0.0152). 
             (\autoref{sec:Faithfulness})}
    \label{fig:face_landmark}
\end{figure}


As an initial attempt, we propose
an approximation to such a metric: similarity of face landmarks~\cite{facelm_creusot20103d,facelm_Burgos-Artizzu_2013_ICCV,facelm_mediapipe}.
Face landmarks are key points on a human face, capturing information about its
position, orientation, structure, expression, etc. That is, capturing those features expected to be largely preserved during the Male-to-Female translation.
Therefore, one may consider forming an Euclidean distance (Lm-$L_2$) between the landmark
locations of the source and translated images (\autoref{fig:face_landmark}). %illustrates how Lm-$L_2$ corresponds to a facial expression change.
Better faithfulness metrics also will consider other domain-consistent features. This development is left for future work.





    
        










