\section{Method}


\thename revisits the classic \cyclegan~\cite{zhu2017unpaired} architecture.
\thename inherits several advancements from \uvcgan~\cite{torbunov2023uvcgan}, such as a hybrid U-Net-ViT (vision transformer) generator architecture, self-supervised generator pre-training,
and better training strategies.
This section describes several improvements that exceed UVCGAN, including the generator, discriminator and the training procedure.


\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{
        \tikzexternaldisable
        \tikzsetnextfilename{generator}%
        \begin{tikzpicture}
            % Need to have a background node filled with white, 
            % or otherwise the line number can be seen.
            % May not needed for the final version without line number
            \node[inner sep=1pt, fill=white] at (0, 0) {
                \begin{tikzpicture}
                    \node[inner sep=1pt] (unet) at (0, 0) {\input{fig/unet_modconv}};
                    \node[inner sep=1pt] (evit) at ([yshift=-3pt]$(unet.west)!1.3!(unet.east)$) {\input{fig/vit_extended}};
                    \node[inner sep=1pt, anchor=south] (unet_title) at ([yshift=2pt]unet.north) {A.~U-Net with modulated convolution};
                    \node[inner sep=1pt, anchor=south] (evit_title) at (unet_title.south -| evit) {B.~extended vision transformer};
                    % the zoom in effect
                    \coordinate (unet_up) at ($(unet.north)!.905!(unet.south)$);
                    \coordinate (unet_dn) at ($(unet.north)!.995!(unet.south)$);
                    \coordinate (evit_up) at ([yshift=-20pt]$(evit.north west)!.3!(evit.north east)$);
                    \coordinate (evit_dn) at ($(evit.south west)!.3!(evit.south east)$);
                    \begin{pgfonlayer}{back}
                        \draw[name path=U, white] (unet_up) to[bend right=30] (evit_up);
                        \draw[name path=D, white] (unet_dn) -- (evit_dn);
                        \tikzfillbetween[of=U and D]{path fading=east, fill=gray!30}
                    \end{pgfonlayer}
                \end{tikzpicture}
            };
        \end{tikzpicture}
    }
    \caption{
        \textbf{\thename Generator.} 
        \thename's generator is a U-Net (Panel A) with an extended vision transformer bottleneck (eViT, Panel B). 
        The eViT outputs a style token for the modulated convolution blocks~\cite{karras2020analyzing} ($M_i$, $i=1,2,3,4$) in the decoding path of the U-Net.
        Refer to \cite{torbunov2023uvcgan} for details about the input layer, output layer, basic block $B_i$, downsampling block $D_i$ and upsampling block $U_i$ in the U-Net, and transformer encoder block in the eViT.
    }
    \label{fig:uvcgan2_generator}
\end{figure*}

\subsection{Review of the Original \uvcgan}
\label{subsec:cyclegan}

\uvcgan follows the \cyclegan framework~\cite{zhu2017unpaired,kim2019u}
that interlaces two generator-discriminator pairs for unpaired I2I translation.
Denote the two domains by $A$ and $B$. 
Generator $\gab$ translates images in $A$ to resemble those from domain $B$. 
Discriminator $\db$ distinguishes images in $B$ from those translated from $A$. 
The same goes for the other translation direction, $\gba$ and $\da$.
The discriminators are updated by backpropagating loss in distinguishing real and translated (fake) images (called \textit{GAN loss}):
\newcommand{\gl}{\ell_\text{gan}}
\begin{align}
    \mathcal{L}^{\text{disc}}_{A} &= \mathbb{E}_{B}\gl\paren{\da\paren{\fake{a}}, 0} + \mathbb{E}_{A}\gl\paren{\da(a), 1}, \label{eq:disc_loss_A}\\
    \mathcal{L}^{\text{disc}}_{B} &= \mathbb{E}_{A}\gl\paren{\db\paren{\fake{b}}, 0} + \mathbb{E}_{B}\gl\paren{\db(b), 1} \label{eq:disc_loss_B}
\end{align}
where $a$ is an image from $A$, $b$ is an image from $B$, $\fake{b}= \gab(a)$, $\fake{a} = \gba(b)$ (subscript $\text{f}$ means fake),
$0$ is the label for fake images, $1$ is the label for real images, 
and $\gl$ represents a classification loss function ($L_2$, cross-entropy, Wasserstein~\cite{arjovsky2017wasserstein}, etc.). 
The generators are updated by backpropagating loss from multiple sources: GAN loss for realistic translation, cycle-consistency loss, and optionally identity loss for within-domain translation. 
Using domain $A$ as an example, we have:
\begin{align}
    \mathcal{L}^{\text{gan}}_{A} &= \mathbb{E}_{A}\gl\paren{\db\paren{\fake{b}}, 1},\\
    \mathcal{L}^{\text{cyc}}_{A} &= \mathbb{E}_{A}\ell_\text{reg}\paren{\reco{a}, a}, \quad \mathcal{L}^{\text{idt}}_{A} = \mathbb{E}_{A}\ell_\text{reg}\paren{\iden{a}, a}
\end{align}
where $\reco{a} = \gba\circ\gab(a)$, $\iden{a} = \gba(a)$, and $\ell_\textrm{reg}$ is any pixel-wise loss function ($L_1$ or $L_2$, etc.). The generator loss is defined as a linear combination:
\newcommand{\nplus}{\!+\!}
\begin{equation}
    \mathcal{L}^{\text{gen}} \!=\! \paren{\mathcal{L}^{\text{gan}}_{A} \nplus \mathcal{L}^{\text{gan}}_{B}} \nplus \lambda^\text{c}\!\paren{\mathcal{L}^{\text{cyc}}_{A} \nplus \mathcal{L}^{\text{cyc}}_{B}} \nplus \lambda^\text{i}\!\paren{\mathcal{L}^{\text{idt}}_{A} \nplus \mathcal{L}^{\text{idt}}_{B}} \label{eq:gen_loss}
\end{equation}




\subsection{Source-driven Style Modulation}
\label{subsec:modulation}



\uvcgan uses a hybrid UNet-ViT generator network.
We attempt to increase the generator's performance by redesigning its architecture.
We extend the generator, enabling it to infer an appropriate target style for each input image.
Then, we modulate~\cite{karras2020analyzing} the generator's decoding branch by the style, significantly increasing its expressiveness.

Specifically, at the bottleneck of the generator, the image is encoded as a sequence of tokens to be fed to the Transformer network. 
We augment this sequence with an additional learnable style token $S$.
The state of the $S$ token at the output of the Transformer serves as a latent image style.
For each convolutional layer of the U-Net's decoding branch, we generate a specific
style vector $s_i$ from $S$ by trainable linear transformations.

The style modulation~\cite{karras2020analyzing} effectively scales weights $w_{i, j, x, y}$ of the convolutional operator
by the supplied style vector $s_i$, yielding modulated weights:
\begin{equation}
    w_{i, j, x, y}^\prime = s_{i} \cdot w_{i, j, x, y}
    \label{eq:weight_modulation}
\end{equation}
where $i$, $j$ refer to the input and output feature maps and $x$, $y$ enumerate the spatial dimensions.
To preserve the magnitude of the activations, the scaled weights $w_{i, j, x, y}^\prime$ need
to be demodulated. 
The demodulation operation further renormalizes the 
convolution weights as follows:
\begin{equation}
    w_{i, j, x, y}^{\prime\prime} =
        \frac{w_{i, j, x, y}^{\prime}}
            {\sqrt{\sum_{i, x, y} \paren{w_{i, j, x, y}^{\prime}}^2 + \epsilon}}
    \label{eq:weight_demodulation}
\end{equation}
where $\epsilon$ is a small number to prevent numerical instability.

Our approach is different from \styleganTwo, which generates the style vectors $s_i$ from a random prior. We use a learnable $S$ token of the transformer to infer the required target style from the source image itself. The way $S$ is processed is similar to the \texttt{[class]} token of the ViT~\cite{dosovitskiy2020image}. However the \texttt{[class]} token mainly is used for the classification task.








\subsection{Batch-Statistics-aware Discriminator}


\begin{figure}[t]
    \centering
    \tikzexternaldisable
    \tikzsetnextfilename{batchhead}%
    \begin{tikzpicture}
        \node[inner sep=0, fill=white] at (0, 0) {\input{fig/disc_batchhead}};
    \end{tikzpicture}
    \caption{\textbf{\thename Discriminator with a batch head.} 
             For the batch statistics block, we can use either a standard batch normalization layer or batch standard deviation~\cite{karras2017progressive}.}
    \label{fig:batch_head}
\end{figure}

Batch statistics offers an effective way to combat mode collapse and
promote diversity for GAN models~\cite{salimans2016improved,karras2017progressive}.
We seek to improve the \uvcgan discriminator's performance by integrating a minibatch discrimination technique. However, employing traditional minibatch discrimination requires having a nontrivial batch size. With the recent push to
work on high-resolution images, increasing the batch size will strain GPU hardware
limitations. We workaround this problem by decoupling the batch size from the minibatch
discrimination technique and designing a GPU memory-efficient algorithm.

The main idea is to integrate a cache of past discriminator features to serve as a 
substitute for a large batch size.
Similar ideas, known as ``memory bank,'' have been explored in representation
learning models~\cite{he2020momentum,wu2018unsupervised}.
To our best knowledge, using a memory bank type of cache in a GAN is novel.

We design a composite discriminator made of a main \textit{body} and a \textit{batch head}.
The composite discriminator body can be any common discriminator, but, for the purposes of this work, we use \uvcgan's discriminator without the last layer.
The batch head is made of a layer that captures batch statistics, followed by two convolutional layers
(\autoref{fig:batch_head}).
This modular discriminator architecture allows for easily swapping different
discriminator bodies while still preserving the minibatch discrimination power.
During training, the batch features are stored in four separate
caches: real images from domain~\domA, real images from domain~\domB, and fake
images from both domains. All the caches have a fixed size and follow the
first-in-first-out (FIFO) update policy. 

The discriminator's batch head receives a concatenation (along the batch
dimension) of the discriminator body output for the current minibatch along
with a history of the past outputs from a cache (\autoref{fig:batch_head}). 
We experiment with two types of batch statistics layers: batch standard deviation
(BSD)~\cite{karras2017progressive} and a simple batch
normalization (BN).

























\subsection{Pixel-wise Consistency Loss}
\label{subsec:consistLoss}
To improve the consistency of the generated and source images, we experiment with the addition of
an extra term $\mathcal{L}_\text{consist}$ to denote generator loss~\eqref{eq:gen_loss}. This term captures
the $L_1$ difference between the downsized versions of the source and translated images.
For example, for images of domain $A$
\begin{equation}
    \mathcal{L}_{\text{consist}, A} = \mathbb{E}_{A}\ell_1\paren{F(\gab(a)), F(a)}  
\end{equation}
where $F$ is a resizing operator down to $32 \times 32$ pixels (low-pass filter).
We add this term to the generator loss~\eqref{eq:gen_loss} with a magnitude $\lambda_\text{consist}$
for both domains.

\subsection{Modern Training Techniques}
\label{subsec:TrainingTech}
\uvcgan and \cyclegan use outdated GAN training techniques. 
Hence, we revamp the training procedure with a few modern additions.
First, we implement the exponential averaging of the generator weights~\cite{karras2017progressive}, 
making the generator less dependent on random fluctuations occurring in the GAN training.
Second, we use spectral normalization of the discriminator weights~\cite{miyato2018spectral},
enhancing the stability of the discriminator and training procedure as a whole.
Third, we experiment with using unequal learning rates for the generator
and discriminator~\cite{heusel2017gans},
which has been empirically shown to improve model performance.
Finally, we replace the generic gradient penalty (GP) with an improved zero-centered
GP version~\cite{thanh2019improving}, developed to promote the GAN's convergence.

