\section{\uvcgan Training Procedure}
\label{sec:app_train_uvcgan}

In this appendix, we provide details of the training setup of \uvcgan models for the
high-quality datasets (\celebahq and \afhq). The \uvcgan paper~\cite{torbunov2023uvcgan}
did not study these datasets, therefore we trained the corresponding models (\celebahq Male-to-Female, Cat-to-Dog, and Wildlife-to-Dog) from scratch.

Before training \uvcgan I2I translation generators, we performed their self-supervised
pre-training.
While performing the pre-training, we closely followed the original procedure~\cite{torbunov2023uvcgan}.
For reference, this pre-training procedure is the same procedure we used for the \thename generator pre-training.

At the second step, we trained the actual I2I translators, with the help of the
provided training scripts~\cite{uvcgan_software}. To closely match the \uvcgan paper, we performed a hyperparameter sweep over $\lambda_\text{cyc}$, $\lambda_\text{GP}$,
and $\gamma$. We explored $\lambda_{cyc}$ values of $[ 1, 5, 10 ]$. For the GP parameters $(\lambda_\text{GP}, \gamma)$, we explored the following configurations
$[ (10, 1), (1, 10), (0.1, 100), (1, 750) ]$, suggested by the \uvcgan source code
repository.

The best \uvcgan performance for all the high-quality translation directions
(Male-to-Female, Cat-to-Dog, Wildlife-to-Dog) is achieved for the same configuration of the tested hyperparameters: $\lambda_\text{cyc} = 5, \lambda_\text{GP} = 0.1, \gamma = 100$.
This configuration is used when presenting the \uvcgan results in the main part of the paper.
