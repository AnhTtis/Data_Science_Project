\section{Training Details}
\label{sec:TrainingDetails}

In this section, we expand on the generator pre-training, I2I translation training, HP optimization setup, the final model configurations, and the ablation study of \thename.

\paragraph{Generator Pre-Training.}
The pre-training of the generators was performed in a BERT-like fashion~\cite{devlin2018bert} on an image inpainting pretext task.

To construct the image inpainting task,
input images of size $256 \times 256$ pixels are tiled into a grid of patches of $32 \times 32$ pixels.
Then, each patch is randomly masked with a probability of $40\%$. The masking is performed by zeroing out
pixel values. The generator is tasked to recover the original unmasked image from a masked one.

For the pre-training, we use the AdamW optimizer together with a cosine learning rate annealing (with
restarts). We set the initial learning rate to $5 \times 10^{-3} \times (\text{batch size} / 512)$,
and the weight decay factor to $0.05$. The scheduler completes 5 annealing cycles during the
pre-training.

We apply several data augmentations, such as random rotation
($\pm 10$ degree), random horizontal flip ($p = 0.5$), and color jitter ($\pm 0.2$ shift in brightness,
contrast, saturation, and hue). 

We pre-train the generators for 500 epochs for \celebahq and \afhq. Due to the small size of
the \anime dataset, we run the pre-training for 2500 epochs. On the contrary, due to the large size
of the \celeba dataset, we run the pre-training for 500 epochs, but limit the number of samples
per epoch to $32,768$. All the pre-trainings are performed with a batch size of 64.

 


\paragraph{Image Translation Training.}
We train the unpaired I2I translation models by closely following the procedure
of \uvcgan~\cite{torbunov2023uvcgan}. We use the Adam optimizer without weight decay.
The training is performed for 1 million iterations.
We experiment with using either a constant learning rate or applying a linear scheduler.
If the linear scheduler is used, then the learning rate is maintained constant for the first
500K iterations, and then linearly annealed to zero during the subsequent 500K iterations.

We keep the batch size equal to 1 during the training. For consistency with
\progan~\cite{karras2017progressive} we keep the sizes of the image caches at 3.
This size effectively provides the batch head with 4 samples to estimate the batch statistics.
To stabilize the generator, we apply an exponential weight averaging to the generator with a momentum of
$0.9999$.

\paragraph{Hyperparameter Exploration.}
When performing the final training, we explored the following grid of hyperparameters:
\begin{itemize}
    \item Magnitude of the cycle-consistency loss $\lambda_\text{cyc}$: $\{ 5, 10\}$.
    \item Magnitude of the zero-centered gradient penalty $\lambda_\text{GP}$ : $\{ 0.001, 0.01, 1 \}$.
    \item Batch Head type: Batch Normalization (BN) vs Batch Standard Deviation (BSD).
    \item Generator's and Discriminator's learning rates:
        \begin{enumerate}
            \item Equal learning rates of $1 \times 10^{-4}$.
            \item Unequal learning rates, with the learning rate of the discriminator of $1 \times 10^{-4}$
                and the learning rate of the generator of $5 \times 10^{-5}$.
        \end{enumerate}
\end{itemize}

The hyperparameter explorations were performed while keeping the magnitude of the consistency loss $\lambda_\text{consist}$
equal to zero.
The grid of hyperparameters above was suggested by the previous rough HP exploration.

For the \afhq \catdog, \wilddog, and \celebahq \malefemale datasets, we have run a second
hyperparameter exploration, studying the effect of the magnitude of the consistency
loss $\lambda_\text{consist}$ on the I2I performance. We have tried the following
values of $\lambda_\text{consist}$: $\{ 0.01, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0 \}$.

\subsection{Final Configurations}

\input{table/app_train_configs}

For all the translation tasks, \thename achieves the best performance when the learning rate scheduler is not used.
\autoref{tab:app_train_configs} summarizes the final hyperparameter configurations (generator's learning rate, magnitudes of the gradient penalty and the cycle consistency loss, and the choice of batch head) that
provide the best performance per translation task.

Generally, the high-quality datasets (\celebahq and \afhq) favor stronger values of the gradient penalty term $\lambda_\text{GP} = 1$, compared to the lower-resolution datasets (\animeselfie and \celeba), favoring $\lambda_\text{GP} = 0.01$. Other
patterns of hyperparameters can be observed in the table. However, their impact
on the model's performance is relatively small compared to $\lambda_\text{GP}$.

We should note, that due to the instability associated with GANs training, some of the best values in \autoref{tab:app_train_configs} may be due to random fluctuations.


\subsection{Ablation Study}

\input{table/app_ablation_full}

In this section we provide a more detailed ablation study of \thename.
\autoref{tab:app_ablation_full} summarizes \thename ablation results on the \malefemale
translation of \celeba. To produce this table we start with the final \thename configuration and make one of following modifications separately:
(a) disable style modulation in the generator;
(b) disable batch head of the discriminator;
(c) use \uvcgan training setup: linear scheduler, full GP, and disable generator averaging;
(d) disable spectral normalization (SN);
(e) add linear schedule;
(f) remove exponential averaging of the generator weights.

\autoref{tab:app_ablation_full} shows, that the removal of the spectral normalization (d) decreases the model performance, but the effect is negligible. This indicates that the Spectral Normalization might be redundant and can be removed.

Each of the items (e) and (f) of \autoref{tab:app_ablation_full} may suggest that either the scheduler or exponential averaging of the generator weights is detrimental to the
model's performance. However, this is not the case, since the effects of (d) and (e) are entangled and mutually balancing. Individual modifications to (d) or (e)
destroy the balance and produce large changes in the model's performance. These changes are not indicative of the effects of the joint modifications as shown by (c).