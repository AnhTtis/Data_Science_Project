\section{Introduction}


Image-to-image (I2I) translation models aim to find a mapping between two domains of images. When paired examples of images from two domains are available, such a mapping can be easily solved in a supervised manner. However, the unpaired I2I translation, where examples of pairs are not available, poses a more interesting problem. The ability to perform an unpaired I2I translation is highly beneficial as obtaining paired datasets in the real world is often difficult, time-consuming, or even impossible~\cite{boulanger2021deep}.


The advancement of unpaired I2I largely benefits from recent developments in deep generating models, such as (variational) Autoencoder~\cite{hinton2006reducing, kingma2013auto}, generative adversarial networks (GANs), and generating flows~\cite{rezende2015variational, dinh2014nice, dinh2016density, kingma2018glow}. One early successful unpaired I2I model is \cyclegan~\cite{zhu2017unpaired}, which uses a cycle-consistency constraint that requires a cyclic back-and-forth translation between two domains to produce the original image. Several succeeding models inspired by \cyclegan, such as STARGAN~\cite{choi2018stargan,choi2020stargan}, SEAN~\cite{zhu2020sean}, U-GAT-IT~\cite{kim2019u}, and CUT~\cite{park2020contrastive}, are designed to further enhance the quality and diversity of the generated images. However, GAN-based I2I methods lag behind general developments in GAN architecture and training procedures~\cite{karras2020analyzing,thanh2019improving}.

Meanwhile, \textit{diffusion models} (DMs) provide an alternative route to image generation~\cite{ho2020denoising}. With a recent spike of interest in such models, several
applications of DMs to unpaired I2I translation have been developed~\cite{choi2021ilvr,meng2021sdedit,zhao2022egsde}. Despite being relatively new, the DM-based \egsde~\cite{zhao2022egsde} approach already has demonstrated superior results on several benchmarks. However, because they do not use source images during the training, DM-based solutions may perform a suboptimal translation~\cite{zhao2022egsde}.
Additionally, DM-based methods rely on pixel-wise $L_2$ distances to maintain the consistency of the source and translated images.
This simple consistency measure is not guaranteed to preserve any semantically meaningful features and can restrict image transformations.



On multiple occasions, revisiting a classic neural architecture and improving it with
a number of modern additions has led to vast improvements in 
performance~\cite{liu2022convnet,karras2020analyzing,bello2021revisiting}.
Based on this observation, \uvcgan~\cite{torbunov2023uvcgan} modernizes the classic CycleGAN
architecture and achieves state-of-the-art performance on several I2I tasks.
Unlike the DM-based models, \cyclegan's training procedure is able to utilize
images from the source and target domains effectively and simultaneously.
Moreover, \cyclegan maintains an intrinsic consistency between the source and translated images (via the cycle-consistency constraint)---a feature that cannot be achieved by simple pixel-wise consistency measures.



Following the \uvcgan's spirit,
we introduce additional architectural innovations and bring modern training techniques to \cyclegan.
The resulting model significantly outperforms both \uvcgan (e.g., $50\%$ better realism on CelebA male-to-female)
and the state-of-the-art DM-based EGSDE (e.g., $40\%$ better on CelebA-HQ male-to-female). The following summarizes the contributions of this work:



















\begin{itemize}[nosep,leftmargin=1em]
  \item We introduce a novel hybrid neural architecture for image translation,
    combining benefits of ViT and Style Modulated Convolutional blocks.
  \item We propose enhancing a common I2I discriminator architecture with a specialized head
    to prevent the problem of mode collapse~\cite{goodfellow2016nips}.
  \item We further improve the \uvcgan's training procedure by incorporating modern GAN training techniques
        that are not commonly applied to unpaired I2I problems.
  \item We perform extensive quantitative evaluations of our model and demonstrate it
    surpasses state-of-the-art competitors in terms of image realism on various translation tasks.
  \item Finally, we highlight inconsistencies and limitations of current unpaired I2I
    evaluation metrics and procedures. In response, we propose improved faithfulness measures that
    are better aligned with human perception.
\end{itemize}
