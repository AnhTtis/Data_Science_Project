
\subsection{Toward Better Faithfulness Measures}


\begin{table*}[t]
\begin{minipage}[b][][b]{0.54\textwidth}
\centering
\caption{\textbf{Measuring Faithfulness with $L_2$.} 
         The pixel-wise $L_2$ is labeled as $L_2$. The $L_2$ between the latent Inception-v3 features is labeled as I-$L_2$.
         More examples, like those featured on the right, can be found in \autoref{sec:Faithfulness}.}
\label{tab:results_hq_consist_only}
\input{table/results_hq_consist_transposed}
\end{minipage}\hfill
\begin{minipage}[b][][b]{0.45\textwidth}
\centering
\input{fig/grid_consistency}
\end{minipage}
\end{table*}



The pixel-wise image similarity measures (such as $L_2$, PSNR, and SSIM) have been shown~\cite{zhang2018unreasonable} to be weakly correlated with the human perception of similarity. However, they currently are being used~\cite{zhao2022egsde} as a faithfulness metric in the area of the unpaired I2I translation.

Following the discussions of perceptual content similarity measures~\cite{zhang2018unreasonable,johnson2016perceptual}, we posit that a proper
faithfulness measure should be based on deep image representations. %Since the primary goal of this paper is to revisit the classic \cyclegan architecture, we are not going
While this effort does not perform an in-depth investigation of possible faithfulness metrics, we instead briefly consider using the $L_2$ distance between the latent Inception-v3~\cite{szegedy2016rethinking} features as an alternative faithfulness measurement.

In principle, there are many different ways a perceptual faithfulness metric can be constructed, e.g., using an LPIPS distance~\cite{zhang2018unreasonable} or
a CLIP similarity~\cite{radford2021learning} as a measure of image faithfulness.
However, engaging the Inception-v3 features may be more natural as they are already employed in calculating image realism (FID).
Thus, reusing them for the faithfulness calculation seems like a straightforward approach that does not involve introducing new dependencies.

In \autoref{sec:Faithfulness}, %SM-Section~4, 
we present a large sample of images generated by \egsde and \thename models that helps to investigate how well the pixel-wise $L_2$ measure is correlated with a perception of image faithfulness. Here, we provide two
representative samples that demonstrate: 1) the pixel-wise $L_2$ faithfulness measure penalizes image changes that should be freely modifiable during the translation (e.g., hairstyle), and 2) the pixel-wise
$L_2$ faithfulness measure fails to effectively penalize changes to features that are
expected to be preserved (e.g., background; facial structure).

To the right of \autoref{tab:results_hq_consist_only}, the first row of images provides a typical sample of \malefemale translations, comparing
\egsde and \thename models. Both translation samples are in good agreement with the
source image. However, the \egsde translation has a pixel-wise $L_2$ difference of $48$ to the source,
while the \thename one is twice as high at $L_2 = 96$. The elevated $L_2$ difference is
caused by a large amount of hair added by the \thename.
Naturally, one may expect hairstyle to be a free parameter of the \malefemale
translation. Yet, the pixel-wise faithfulness metrics will highly penalize its changes.

The second row of images (also at right on \autoref{tab:results_hq_consist_only}) demonstrates another sample of \malefemale translation,
where pixel-wise $L_2$ are similar between \egsde ($L_2 = 45$)
and \thename ($L_2 = 43$). 
However, the images differ in structure. 
The one from \thename preserves the details better: the image background; facial bone
structure; shape of the ear, forehead, nose, and even smile. 
The Inception-v3 scores also correlated with this observation, showing a preference for the \thename image ($13.7$ versus $17.0$).
These observations suggest that the Inception-v3-based distance measure
may be a better faithfulness metric than a simple pixel-wise one.
However, we defer proper investigation for future works. 
To conclude, \autoref{tab:results_hq_consist_only} shows the comparison of pixel-wise and Inception-v3 $L_2$ faithfulness metrics
between \egsde and \thename models, showing the \thename model provides better faithfulness in terms of the Inception-v3 features
while \egsde is more faithful if measured in pixel-wise distances between the source and translated images.
