\section{Datasets}
\label{sec:DatasetDetail}


\begin{figure*}[t]
    \centering
    % \resizebox{\linewidth}{!}{
        \tikzexternaldisable
        \tikzsetnextfilename{cyclegan}%
        \begin{tikzpicture}
            \node[inner sep=1pt, fill=white] at (0, 0) {\input{fig/cyclegan_2}};
        \end{tikzpicture}
    % }
    \caption{
        \textbf{\cyclegan framework.} 
        The \cyclegan~\cite{zhu2017unpaired} consists of two pairs of GANs, $\paren{\gab, \db}$ and $\paren{\gba, \da}$. 
        The discriminators try to distinguish translations from real images, 
        while the generators (or translators) seek to produce realistic translations that are also consistent with the input. 
        The consistency is enforced by the cycle-consistency loss and (optional) identity loss. 
        Here, we use $a$ to denote an image from domain $A$, $b$ as an image from domain $B$,
        $\fake{(*)}$ is a fake image (a translation), 
        $\reco{(*)}$ notes a cyclic reconstruction, 
        and $\iden{(*)}$ represents an identity reconstruction
        (when identity losses are used, $\left.\gab\right|_B$ and $\left.\gba\right|_A$ are encouraged to be identity maps).
    }
    \label{fig:cyclegan}
\end{figure*}


In this section, we provide additional details about the datasets used in the main paper.

\paragraph{\celeba~\cite{liu2015deep}.} The datasets for the \glasses Removal and \malefemale tasks are derived from the original \celeba dataset.
For a fair comparison with older models~\cite{torbunov2023uvcgan}, we used the pre-processed versions of
\glasses Removal and \malefemale datasets provided by \council~\cite{nizan2020breaking}.
The \celeba dataset is made of images of size $178 \times 218$ pixels. The train split of the \glasses Removal
task contains about 11K images with glasses and 152K without. The \malefemale dataset has about 68K males and 95K
females. The test parts of the \glasses Removal and \malefemale datasets contain about 3K images with glasses
and 37K images without glasses, and 16K males and 24K females respectively.

\paragraph{\animeselfie~\cite{kim2019u}.} The training split of the \animeselfie dataset contains 3400 Selfie images and 3400 Anime images.
The test part contains just 100 samples from each domain. All images of this dataset have a size of $256 \times 256$ pixels.

\paragraph{\celebahq~\cite{karras2017progressive}.}
The \celebahq dataset has around 10K images of males and about 18K images of females in the train split. The test split
contains 1000 male and 1000 female images. The size of a \celebahq image is $1024 \times 1024$ pixels.

\paragraph{\afhq~\cite{choi2020stargan}.}
The \afhq dataset has around 5.2K cat, 4.7K dog, and 4.7K wildlife images in the train split, and
500 images of each in the test split. The images of the \afhq dataset have a size of $512 \times 512$ pixels.
There are two versions of the \afhq dataset provided by StarGANv2~\cite{choi2020stargan}.
We use version 1 to be consistent with previous models.
