\section{Introduction}


Image-to-image (I2I) translation models aim to find a mapping between two domains of images. When paired examples of images from two domains are available, such a mapping can be easily solved in a supervised manner. However, the unpaired I2I translation, where examples of pairs are not available, poses a more interesting problem. The ability to perform an unpaired I2I translation is highly beneficial as obtaining paired datasets in the real world is often difficult, time-consuming, or even impossible~\cite{boulanger2021deep}.


The advancement of unpaired I2I largely benefits from recent developments in deep generating models, such as (variational) Autoencoder~\cite{hinton2006reducing, kingma2013auto}, generative adversarial networks (GANs), and generating flows~\cite{rezende2015variational, dinh2014nice, dinh2016density, kingma2018glow}. One early successful unpaired I2I model is \cyclegan~\cite{zhu2017unpaired}, which uses a cycle-consistency constraint that requires a cyclic back-and-forth translation between two domains to produce the original image. Several succeeding models inspired by \cyclegan, such as STARGAN~\cite{choi2018stargan,choi2020stargan}, SEAN~\cite{zhu2020sean}, U-GAT-IT~\cite{kim2019u}, and CUT~\cite{park2020contrastive}, are designed to further enhance the quality and diversity of the generated images. However, GAN-based I2I methods lag behind general developments in GAN architecture and training procedures~\cite{karras2020analyzing,thanh2019improving}.

Meanwhile, \textit{diffusion models} (DMs) provide an alternative route to image generation~\cite{ho2020denoising}. With a recent spike of interest in such models, several
applications of DMs to unpaired I2I translation have been developed~\cite{choi2021ilvr,meng2021sdedit,zhao2022egsde}. Despite being relatively new, the DM-based \egsde~\cite{zhao2022egsde} approach already has demonstrated superior results on several benchmarks. However, because they do not use source images during the training, DM-based solutions may perform a suboptimal translation~\cite{zhao2022egsde}.
Additionally, DM-based methods rely on pixel-wise $L_2$ distances to maintain the consistency of the source and translated images.
This simple consistency measure is not guaranteed to preserve any semantically meaningful features and can restrict image transformations.



In the past, revisiting a classic neural architecture and improving it with a number of modern additions has led to vast improvements in performance~\cite{liu2022convnet,karras2020analyzing,bello2021revisiting}. Based on this observation, we revisit one of the earliest GAN-based I2I models: \cyclegan. Unlike the DM-based models, \cyclegan's training procedure is able to utilize images from the source and target domains effectively and simultaneously.
Moreover, \cyclegan maintains an intrinsic consistency between the source and translated images (via the cycle-consistency constraint)---a feature that cannot be achieved by simple pixel-wise consistency measures.
In addition, work on the \uvcgan~\cite{torbunov2023uvcgan} model has shown that \cyclegan's performance can be significantly improved by modernizing its architecture.

Motivated by \uvcgan's success, we continue to rethink the classic \cyclegan architecture. Using \uvcgan as a starting point, we redesign its generator, discriminator, and training procedure to obtain a revised model: \thename.

















\paragraph{Our Contributions.} This work makes several technical improvements to the \uvcgan~\cite{torbunov2023uvcgan} architecture:
\begin{itemize}[nosep,leftmargin=1em]
  \item We redesign the \uvcgan generator model and introduce style modulation to its decoding
    branch. We propose a style generation mechanism via a learnable Transformer token.
  \item We propose a modular discriminator architecture made of a traditional discriminator
    body and a special head, which prevents the problem of mode collapse~\cite{goodfellow2016nips}. We augment the discriminator with a cache of past discriminator encodings, allowing it to directly compare feature statistics between distributions of target and translated images.
    \item Combined with better training strategies, we demonstrate that the revised \uvcgan
        model surpasses its most advanced competitors by a large margin in translation
        realism and faithfulness.
    \item We highlight the inconsistencies and limitation of current unpaired I2I evaluation protocols and suggest a better faithfulness measure based on deep
    image representations of Inception-v3.
\end{itemize}
