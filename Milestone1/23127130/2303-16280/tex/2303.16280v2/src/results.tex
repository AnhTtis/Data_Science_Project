\section{Results}

\begin{figure*}[t]
    \centering
    \tikzexternaldisable
    \tikzsetnextfilename{grid_LQ}
    \input{fig/grid}
    \begin{tikzpicture}
        
        \def\width{.1\textwidth}
        \def\expand{1.03}
        \def\folder{fig/grid_LQ}
        
        \node[inner sep=0, fill=white] at (0, 0) {
            \begin{tikzpicture}
                \node[inner sep=0] (G1) at (0, 0) {\gridWithHeader[1]{selfie2anime}{0, 3, 6}{input/input, UVCGAN2/\thename}{\selfieanime}};
            	\node[inner sep=0, anchor=west] (G2) at ([xshift=.03 * \width]G1.east) {\gridWithHeader{male2female}{4, 1, 9}{input/input, UVCGAN2/\thename}{\malefemale}};
            	\node[inner sep=0, anchor=west] (G3) at ([xshift=.03 * \width]G2.east) {\gridWithHeader{rmvGlasses}{4, 7, 8}{input/input, UVCGAN2/\thename}{\rmvGlasses}}; 
            \end{tikzpicture}
        };
    \end{tikzpicture}
    \caption{\textbf{Sample translations for \celeba and \anime. } Translations produced by \ugatit, \uvcgan, and \thename for three tasks: \selfieanime, \malefemale, and \rmvGlasses. The full grid with all benchmarking results and those for the three opposite translations can be found in \autoref{sec:additionalTransSamples}.}
    \label{fig:grid_LQ}
\end{figure*}

\begin{figure*}[t]
    \centering
    \tikzexternaldisable
    \tikzsetnextfilename{grid_HQ}
    \input{fig/grid}
    \begin{tikzpicture}
    
        \def\width{.1\textwidth}
        \def\expand{1.03}
        \def\folder{fig/grid_HQ}
    
        \node[inner sep=0, fill=white] at (0, 0) {
            \begin{tikzpicture}
                \node[inner sep=0] (G1) at (0, 0) {\gridWithHeader[1]{male2female}{1, 5, 8}{input/input, EGSDE1/\egsde, EGSDE2/\egsdeDG, UVCGAN2wo/\thename}{\malefemale}};
            	\node[inner sep=0, anchor=west] (G2) at ([xshift=.03 * \width]G1.east) {\gridWithHeader{cat2dog}{4, 7, 8}{input/input, EGSDE1/\egsde, EGSDE2/\egsdeDG, UVCGAN2wo/\thename}{\catdog}};
            	\node[inner sep=0, anchor=west] (G3) at ([xshift=.03 * \width]G2.east) {\gridWithHeader{wild2dog}{1, 4, 5}{input/input, EGSDE1/\egsde, EGSDE2/\egsdeDG, UVCGAN2wo/\thename}{\wilddog}}; 
            \end{tikzpicture}
        };
    \end{tikzpicture}
    \caption{\textbf{Sample translations for \celebahq and \afhq. } Translations for three tasks: \malefemale, \catdog, and \wilddog. More translations for these three tasks and those for \wildcat can be found in \autoref{sec:additionalTransSamples}.}
    \label{fig:grid_HQ}
\end{figure*}

\subsection{Metrics of Realism and Faithfulness}

There are two dimensions along which the unpaired I2I style transfer models can be evaluated:
\textit{Faithfulness} and \textit{Realism}. Faithfulness captures the degree of similarity between the
source and its translated image at an individual level. 
Realism attempts to estimate the overlap of the distributions
of the translated images and the ones in the target.

In terms of realism, image translation quality is commonly judged according to the
FID~\cite{heusel2017gans} and kernel inception distance
(KID)~\cite{binkowski2018demystifying} metrics. 
Both metrics measure the
distance between the distributions of the latent Inception-v3~\cite{szegedy2016rethinking}
features extracted from samples of the translated and target images. Smaller FID and KID values
indicate more realistic images.

Early GAN-based works (e.g.,~\cite{zhu2017unpaired,nizan2020breaking,zhao2020unpaired,kim2019u}) do not
explicitly evaluate the faithfulness of the translation. 
To the best of our knowledge, there is no widely accepted faithfulness metric available. 
Some works~\cite{zhao2022egsde} try to employ simple pixel-wise $L_2$, peak-signal-to-noise ratio (PSNR), or structural similarity index measure (SSIM)~\cite{wang2004image} scores to capture the agreement between the source and translation. 
Yet, it is unclear how well these pixel-wise metrics relate to the perceived image faithfulness,
and we explore more advanced alternatives in \autoref{sec:better_faithfulness}.




\subsection{Evaluation Protocol}

Evaluation protocols differ drastically between different papers (see \autoref{sec:RemarksMetricEvalConsistency}). %(see SM-Section 3).
This makes the direct comparison of the translation quality metrics extremely challenging.
For the fairness of comparisons with older works, we follow different evaluation protocols,
depending on the dataset.

\paragraph{\celeba and \anime.}
When evaluating the quality of translation on the \celeba \malefemale, \celeba \glasses Removal, and \anime datasets,
we use the evaluation protocol of \uvcgan~\cite{torbunov2023uvcgan}, which uniformized FID/KID evaluation across multiple datasets and models, allowing for a simple FID/KID comparison.
For the actual FID/KID evaluation, we rely on \texttt{torch-fidelity}~\cite{obukhov2020torchfidelity}, which provides
a validated implementation of these metrics.

The actual evaluation protocol for \celeba and \anime relies only on test splits to perform the FID/KID
evaluation. For the \celeba dataset, we use KID subset size of $1000$. 
For the \anime dataset, we use the KID subset size of $50$. 
We use unprocessed images of size $256 \times 256$ when evaluating on the \anime dataset. 
For the \celeba dataset, we apply a simple pre-processing to both domains: resizing
the smaller side to $256$ pixels then taking a center crop of size $256 \times 256$.

\paragraph{\celebahq and \afhq.} 
\egsde~\cite{zhao2022egsde} has evaluated multiple models on the \celebahq and \afhq datasets under
similar conditions. To compare our results to \egsde, we replicate its evaluation protocol for \celebahq 
and \afhq.
For the \afhq dataset, we evaluate FID and KID scores between the translated images of size $256 \times 256$ and
the target images of size $512 \times 512$ from the validation split.
For the \celebahq dataset, we evaluate the FID/KID scores between the translated images of size $256 \times 256$
and the downsized target images of size $256 \times 256$ from the train split. We perform the same channel
standardization as \egsde with $\mu = (0.485, 0.456, 0.406)$ and $\sigma = (0.229, 0.224, 0.225)$.
To ensure full consistency, we use the reference evaluation code provided by \egsde~\cite{egsderef}. 
\autoref{sec:consistent_eval} %SM-Section 5 
provides results of an alternative evaluation protocol that is uniform across all the datasets.


\subsection{Quantitative Results}

\paragraph{\celeba and \anime.}
\autoref{tab:results_lq} shows a comparison of the \thename (trained without piwel-wise consistency loss) performance against
\aclgan~\cite{zhao2020unpaired}, \council~\cite{nizan2020breaking},
\cyclegan~\cite{zhu2017unpaired}, \ugatit~\cite{kim2019u}, and
\uvcgan~\cite{torbunov2023uvcgan}.
The competitor models' performance is obtained from the \uvcgan paper~\cite{torbunov2023uvcgan}.
According to \autoref{tab:results_lq}, \thename outperforms all competitor models in all translation directions, except \animeselfie. The degree of improvement ranges from about $5\%$
in terms of FID on the \selfieanime translation to around $51\%$ on the \malefemale
translation. Likewise, there is a significant improvement in the KID scores from about
$13\%$ on \animeselfie to $79\%$ on \malefemale. This improvement demonstrates
the effectiveness of modern additions to the traditional \cyclegan architecture. \autoref{fig:grid_LQ} provides some translation samples, while more can be found in \autoref{sec:additionalTransSamples}. %SM-Section 6.

\input{table/results_lq}

\paragraph{\celebahq and \afhq.}
\autoref{tab:results_hq} compares the results of the \thename evaluation against \cut~\cite{park2020contrastive}, \ilvr~\cite{choi2021ilvr}, \sdedit~\cite{meng2021sdedit},
and two versions of the \egsde~\cite{zhao2022egsde}.
In particular, this table compares two versions of the \thename: \thename and \mbox{\thename-C}. \mbox{\thename-C} is a version trained with a pixel-wise consistency loss and $\lambda_\text{consist} = 0.2$.
The competitor models' performance is extracted from \egsde~\cite{zhao2022egsde}. 

\input{table/results_hq_v2}

\autoref{tab:results_hq} shows that \thename achieves the best translation quality, according
to the FID scores, with improvements ranging from $10\%$ on \wilddog translation to
$43\%$ on \malefemale translation. The addition of the consistency loss allows the \thename-C
model to improve its pixel-wise PSNR and SSIM metrics---but at the expense of the FID score on \afhq translation.
\thename and \thename-C achieve competitive SSIM scores but lose in terms of the PSNR ratio
to the other models. However, as previously noted~\cite{zhang2018unreasonable}, pixel-wise measures PSNR and SSIM are not good metrics to judge perceptual image
faithfulness.
Overall, the gains in SSIM and PNSR scores provided
by the consistency loss to \thename-C do not seem to outweigh the associated FID losses.

Finally, it may be instructive to examine the diversity of the translated images.
We compare the diversity of the generated images according to a pairwise LPIPS distance~\cite{zhang2018unreasonable}, following an approach similar to~\cite{liu2021divco}.
To calculate the LPIPS distance, we use a VGG-based implementation (v0.0) in a consecutive pair mode.
\autoref{tab:results_hq} shows that the \thename produces a much larger diversity of the generated images compared to the \egsde variants.
It also indicates that the presence of the pixel-wise consistency loss
leads to a small but repeatable increase in image diversity.

\subsection{Ablation Study}

\input{table/app_ablation}

\autoref{tab:app_ablation} summarizes \thename ablation results on the \malefemale
translation of \celeba. To produce this table, we start with the final \thename configuration and make one of following modifications separately:
(a) disable style modulation in the generator;
(b) disable batch head of the discriminator;
(c) revert to \uvcgan training setup: linear scheduler, full GP, and disable generator averaging.

According to \autoref{tab:app_ablation}, the generator modifications (a) account for the majority of the performance improvement. Removing these modifications degrades the FID score of
the \malefemale translation from $4.7$ to $8.1$.
The discriminator modifications (b) provide a significant but relatively smaller improvement in the I2I performance. The new training setup (c) produces an improvement somewhere in between the generator and discriminator modifications.

\input{src/results_faithfulness}