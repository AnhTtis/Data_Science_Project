@inproceedings{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2312--2320},
  year={2011}
}

@article{wang2020reward,
  title={On Reward-Free Reinforcement Learning with Linear Function Approximation},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Salakhutdinov, Ruslan},
  journal={Advances in neural information processing systems},
  year={2020}
}

@inproceedings{zhou2020provably,
  title={Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  year={2021},
  organization={PMLR}
}

@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}
@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020}
}
@article{munos2008finite,
  title={Finite-time bounds for fitted value iteration},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={May},
  pages={815--857},
  year={2008}
}
@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}
@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}
@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}
@inproceedings{chu2011contextual,
  title={Contextual bandits with linear payoff functions},
  author={Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={208--214},
  year={2011}
}
@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}
@inproceedings{auer2009near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  booktitle={Advances in neural information processing systems},
  pages={89--96},
  year={2009}
}
@inproceedings{lattimore2012pac,
  title={PAC bounds for discounted MDPs},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}
@inproceedings{yang2019reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}
@article{van2019comments,
  title={Comments on the du-kakade-wang-yang lower bounds},
  author={Van Roy, Benjamin and Dong, Shi},
  journal={arXiv preprint arXiv:1911.07910},
  year={2019}
}
@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}
@inproceedings{liu2021sharp,
  title={A sharp analysis of model-based reinforcement learning with self-play},
  author={Liu, Qinghua and Yu, Tiancheng and Bai, Yu and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={7001--7010},
  year={2021},
  organization={PMLR}
}
@article{zhang2020task,
  title={Task-agnostic Exploration in Reinforcement Learning},
  author={Zhang, Xuezhou and Ma, Yuzhe and Singla, Adish},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}
@inproceedings{jia2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi},
  booktitle={Learning for Dynamics and Control},
  pages={666--686},
  year={2020},
  organization={PMLR}
}
@inproceedings{calandriello2014sparse,
  title={Sparse multi-task reinforcement learning},
  author={Calandriello, Daniele and Lazaric, Alessandro and Restelli, Marcello},
  booktitle={Advances in Neural Information Processing Systems},
  pages={819--827},
  year={2014}
}
@inproceedings{mo2016personalizing,
  title={Personalizing a dialogue system with transfer reinforcement learning},
  author={Mo, Kaixiang and Zhang, Yu and Li, Shuangyin and Li, Jiajun and Yang, Qiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}
@inproceedings{parisotto2015actor,
  title={Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning},
  author={Parisotto, Emilio and Ba, Lei Jimmy and Salakhutdinov, Ruslan},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}
@inproceedings{kaufmann2020adaptive,
  title={Adaptive reward-free exploration},
  author={Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={865--891},
  year={2021},
  organization={PMLR}
}

@inproceedings{menard2021fast,
  title={Fast active learning for pure exploration in reinforcement learning},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Kaufmann, Emilie and Leurent, Edouard and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={7599--7608},
  year={2021},
  organization={PMLR}
}

@article{zhang2020nearly,
  title={Nearly Minimax Optimal Reward-free Reinforcement Learning},
  author={Zhang, Zihan and Du, Simon S and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2010.05901},
  year={2020}
}

@inproceedings{cai2019provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}
@inproceedings{modi2020sample,
  title={Sample complexity of reinforcement learning using linearly combined model ensembles},
  author={Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2010--2020},
  year={2020}
}
@inproceedings{wang2019optimism,
  title={Optimism in reinforcement learning with generalized linear function approximation},
  author={Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{du2020agnostic,
  title={Agnostic $ Q $-learning with Function Approximation in Deterministic Systems: Near-Optimal Bounds on Approximation Error and Sample Complexity},
  author={Du, Simon S and Lee, Jason D and Mahajan, Gaurav and Wang, Ruosong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020}
}
@inproceedings{du2020good,
  title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{he2020minimax,
  title={Minimax Optimal Reinforcement Learning for Discounted MDPs},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2010.00587},
  year={2020}
}
@article{zanette2020provably,
  title={Provably efficient reward-agnostic navigation with linear value iteration},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel J and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}
@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5713--5723},
  year={2017}
}
@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019},
  organization={PMLR}
}
@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}
@inproceedings{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4863--4873},
  year={2018}
}
@inproceedings{zhang2021reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={4528--4531},
  year={2021},
  organization={PMLR}
}

@inproceedings {zhou2020nearly,
  title={Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  year={2021},
  organization={PMLR}
}
@article{fano1961transmission,
  title={Transmission of information: A statistical theory of communications},
  author={Fano, Robert M},
  journal={American Journal of Physics},
  volume={29},
  number={11},
  pages={793--794},
  year={1961},
  publisher={American Association of Physics Teachers}
}
@inproceedings{yao1977probabilistic,
  title={Probabilistic computations: Toward a unified measure of complexity},
  author={Yao, Andrew Chi-Chin},
  booktitle={18th Annual Symposium on Foundations of Computer Science (sfcs 1977)},
  pages={222--227},
  year={1977},
  organization={IEEE}
}
@article{azuma1967weighted,
  title={Weighted sums of certain dependent random variables},
  author={Azuma, Kazuoki},
  journal={Tohoku Mathematical Journal, Second Series},
  volume={19},
  number={3},
  pages={357--367},
  year={1967},
  publisher={Mathematical Institute, Tohoku University}
}
@article{kiefer1960equivalence,
  title={The equivalence of two extremum problems},
  author={Kiefer, Jack and Wolfowitz, Jacob},
  journal={Canadian Journal of Mathematics},
  volume={12},
  pages={363--366},
  year={1960},
  publisher={Cambridge University Press}
}
@inproceedings{lattimore2020learning,
  title={Learning with good feature representations in bandits and in rl with a generative model},
  author={Lattimore, Tor and Szepesvari, Csaba and Weisz, Gellert},
  booktitle={International Conference on Machine Learning},
  pages={5662--5670},
  year={2020},
  organization={PMLR}
}
@inproceedings{tao2018best,
  title={Best arm identification in linear bandits with linear dimension dependency},
  author={Tao, Chao and Blanco, Sa{\'u}l and Zhou, Yuan},
  booktitle={International Conference on Machine Learning},
  pages={4877--4886},
  year={2018}
}

@inproceedings{yang2019sample,
  title={Sample-optimal parametric Q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{kansky2017schema,
  title={Schema networks: Zero-shot transfer with a generative causal model of intuitive physics},
  author={Kansky, Ken and Silver, Tom and M{\'e}ly, David A and Eldawy, Mohamed and L{\'a}zaro-Gredilla, Miguel and Lou, Xinghua and Dorfman, Nimrod and Sidor, Szymon and Phoenix, Scott and George, Dileep},
  booktitle={International Conference on Machine Learning},
  pages={1809--1818},
  year={2017},
  organization={PMLR}
}

@article{moerland2018emotion,
  title={Emotion in reinforcement learning agents and robots: a survey},
  author={Moerland, Thomas M and Broekens, Joost and Jonker, Catholijn M},
  journal={Machine Learning},
  volume={107},
  number={2},
  pages={443--480},
  year={2018},
  publisher={Springer}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{sun2019model,
  title={Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Conference on Learning Theory},
  pages={2898--2933},
  year={2019},
  organization={PMLR}
}

@article{michael1995reinforcement,
  title={Reinforcement learning with soft state aggregation},
  author={Michael, Satinder P Singh Tommi Jaakkola and Jordan, I},
  journal={Advances in neural information processing systems 7},
  volume={7},
  pages={361},
  year={1995},
  publisher={MIT Press}
}
@inproceedings{karmarkar1984new,
  title={A new polynomial-time algorithm for linear programming},
  author={Karmarkar, Narendra},
  booktitle={Proceedings of the sixteenth annual ACM symposium on Theory of computing},
  pages={302--311},
  year={1984}
}
@book{dantzig1965linear,
  title={Linear programming and extensions},
  author={Dantzig, George Bernard},
  volume={48},
  year={1965},
  publisher={Princeton university press}
}


@inproceedings{kirschner2018information,
  title={Information directed sampling and bandits with heteroscedastic noise},
  author={Kirschner, Johannes and Krause, Andreas},
  booktitle={Conference On Learning Theory},
  pages={358--384},
  year={2018},
  organization={PMLR}
}
@article{jacot2018neural,
  title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  author={Jacot, Arthur and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={Advances in neural information processing systems},
  year={2018}
}
@article{cao2020generalization,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  year={2020}
}
@inproceedings{osband2014model,
  title={Model-based reinforcement learning and the eluder dimension},
  author={Osband, Ian and Roy, Benjamin Van},
  booktitle={Proceedings of the 27th International Conference on Neural Information Processing Systems-Volume 1},
  pages={1466--1474},
  year={2014}
}
@inproceedings{russo2013eluder,
  title={Eluder Dimension and the Sample Complexity of Optimistic Exploration.},
  author={Russo, Daniel and Van Roy, Benjamin},
  booktitle={NIPS},
  pages={2256--2264},
  year={2013},
  organization={Citeseer}
}

@inproceedings{wu2022nearly,
  title={Nearly minimax optimal regret for learning infinite-horizon average-reward mdps with linear function approximation},
  author={Wu, Yue and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3883--3913},
  year={2022},
  organization={PMLR}
}

@article{zhang2021reward,
  title={Reward-free model-based reinforcement learning with linear function approximation},
  author={Zhang, Weitong and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhou2022computationally,
  title={Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs},
  author={Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2205.11507},
  year={2022}
}


@article{krishnamurthy2016pac,
  title={PAC reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{jiang2018open,
  title={Open problem: The dependence of sample complexity lower bounds on planning horizon},
  author={Jiang, Nan and Agarwal, Alekh},
  booktitle={Conference On Learning Theory},
  pages={3395--3398},
  year={2018},
  organization={PMLR}
}

@inproceedings{zhang2022horizon,
  title={Horizon-free reinforcement learning in polynomial time: the power of stationary policies},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={3858--3904},
  year={2022},
  organization={PMLR}
}

@inproceedings{chen2022nearoptimal,
  title={Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver},
  author={Chen, Xiaoyu and Hu, Jiachen and Yang, Lin and Wang, Liwei},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{zhang2021improved,
  title={Improved Variance-Aware Confidence Sets for Linear Bandits and Linear Mixture MDP},
  author={Zhang, Zihan and Yang, Jiaqi and Ji, Xiangyang and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture markov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2021model,
  title={Model-free reinforcement learning: from clipped pseudo-regret to sample complexity},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  booktitle={International Conference on Machine Learning},
  pages={12653--12662},
  year={2021},
  organization={PMLR}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low bellman rank are pac-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@article{dann2018oracle,
  title={On oracle-efficient pac rl with rich observations},
  author={Dann, Christoph and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}



@article{du2019good,
  title={Is a good representation sufficient for sample efficient reinforcement learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}
@article{foster2021statistical,
  title={The statistical complexity of interactive decision making},
  author={Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2112.13487},
  year={2021}
}
@inproceedings{weisz2021exponential,
  title={Exponential lower bounds for planning in mdps with linearly-realizable optimal action-value functions},
  author={Weisz, Gell{\'e}rt and Amortila, Philip and Szepesv{\'a}ri, Csaba},
  booktitle={Algorithmic Learning Theory},
  pages={1237--1264},
  year={2021},
  organization={PMLR}
}

@inproceedings{yang2020reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}


@inproceedings{zhou2021provably,
  title={Provably efficient reinforcement learning for discounted mdps with feature mapping},
  author={Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={12793--12802},
  year={2021},
  organization={PMLR}
}

@inproceedings{kaufmann2021adaptive,
  title={Adaptive reward-free exploration},
  author={Kaufmann, Emilie and M{\'e}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={865--891},
  year={2021},
  organization={PMLR}
}


@article{huang2022towards,
  title={Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality},
  author={Huang, Jiawei and Chen, Jinglin and Zhao, Li and Qin, Tao and Jiang, Nan and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2202.06450},
  year={2022}
}

@inproceedings{wagenmaker2022reward,
  title={Reward-free rl is no harder than reward-aware rl in linear markov decision processes},
  author={Wagenmaker, Andrew J and Chen, Yifang and Simchowitz, Max and Du, Simon and Jamieson, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={22430--22456},
  year={2022},
  organization={PMLR}
}

@article{wang2020long,
  title={Is long horizon reinforcement learning more difficult than short horizon reinforcement learning?},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin F and Kakade, Sham M},
  journal={arXiv preprint arXiv:2005.00527},
  year={2020}
}


@article{ren2021nearly,
  title={Nearly horizon-free offline reinforcement learning},
  author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15621--15634},
  year={2021}
}

@article{tarbouriech2021stochastic,
  title={Stochastic shortest path: Minimax, parameter-free and towards horizon-free regret},
  author={Tarbouriech, Jean and Zhou, Runlong and Du, Simon S and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6843--6855},
  year={2021}
}

@inproceedings{li2022settling,
  title={Settling the horizon-dependence of sample complexity in reinforcement learning},
  author={Li, Yuanzhi and Wang, Ruosong and Yang, Lin F},
  booktitle={2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={965--976},
  year={2022},
  organization={IEEE}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@article{tessler2018reward,
  title={Reward constrained policy optimization},
  author={Tessler, Chen and Mankowitz, Daniel J and Mannor, Shie},
  journal={arXiv preprint arXiv:1805.11074},
  year={2018}
}

@article{miryoosefi2019reinforcement,
  title={Reinforcement learning with convex constraints},
  author={Miryoosefi, Sobhan and Brantley, Kiant{\'e} and Daume III, Hal and Dudik, Miro and Schapire, Robert E},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mai2022sample,
  title={Sample efficient deep reinforcement learning via uncertainty estimation},
  author={Mai, Vincent and Mani, Kaustubh and Paull, Liam},
  journal={arXiv preprint arXiv:2201.01666},
  year={2022}
}

@article{wang2020reinforcement,
  title={Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
  author={Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6123--6135},
  year={2020}
}

@inproceedings{he2021logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={4171--4180},
  year={2021},
  organization={PMLR}
}

@article{he2022nearly,
  title={Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes},
  author={He, Jiafan and Zhao, Heyang and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2212.06132},
  year={2022}
}

@article{agarwal2022vo,
  title={VO $ Q $ L: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation},
  author={Agarwal, Alekh and Jin, Yujia and Zhang, Tong},
  journal={arXiv preprint arXiv:2212.06069},
  year={2022}
}

@article{zhou22high,
  title={Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs},
  author={Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2205.11507},
  year={2022}
}