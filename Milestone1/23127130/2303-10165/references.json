{
  "2212-06132": {
    "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes",
    "authors": [
      "Jiafan He",
      "Heyang Zhao",
      "Dongruo Zhou",
      "Quanquan Gu"
    ],
    "submission_date": "2022-12-12",
    "revised_dates": [],
    "doi": "10.48550/arXiv.2212.06132",
    "arxiv_id": "2212.06132",
    "venue": "International Conference on Machine Learning",
    "year": 2022
  },
  "2212-06069": {
    "title": "VOQL: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation",
    "authors": [
      "Alekh Agarwal",
      "Yujia Jin",
      "Tong Zhang"
    ],
    "submission_date": "2022-12-12",
    "revised_dates": [],
    "doi": "10.48550/arXiv.2212.06069",
    "arxiv_id": "2212.06069",
    "venue": "arXiv.org",
    "year": 2022
  },
  "2205-11507": {
    "title": "Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs",
    "authors": [
      "Dongruo Zhou",
      "Quanquan Gu"
    ],
    "submission_date": "2022-05-23",
    "revised_dates": [],
    "doi": "10.48550/arXiv.2205.11507",
    "arxiv_id": "2205.11507",
    "venue": "Neural Information Processing Systems",
    "year": 2022
  },
  "2203-12922": {
    "title": "Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary Policies",
    "authors": [
      "Zihan Zhang",
      "Xiangyang Ji",
      "S. Du"
    ],
    "submission_date": "2022-03-24",
    "revised_dates": [],
    "doi": "10.48550/arXiv.2203.12922",
    "arxiv_id": "2203.12922",
    "venue": "Annual Conference Computational Learning Theory",
    "year": 2022
  },
  "2202-06450": {
    "title": "Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality",
    "authors": [
      "Jiawei Huang",
      "Jinglin Chen",
      "Li Zhao",
      "Tao Qin",
      "Nan Jiang",
      "Tie-Yan Liu"
    ],
    "submission_date": "2022-02-14",
    "revised_dates": [],
    "arxiv_id": "2202.06450",
    "venue": "International Conference on Learning Representations",
    "year": 2022
  },
  "2201-11206": {
    "title": "Reward-Free RL is No Harder Than Reward-Aware RL in Linear Markov Decision Processes",
    "authors": [
      "Andrew Wagenmaker",
      "Yifang Chen",
      "Max Simchowitz",
      "S. Du",
      "Kevin G. Jamieson"
    ],
    "submission_date": "2022-01-26",
    "revised_dates": [],
    "arxiv_id": "2201.11206",
    "venue": "International Conference on Machine Learning",
    "year": 2022
  },
  "2201-01666": {
    "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation",
    "authors": [
      "Vincent Mai",
      "Kaustubh Mani",
      "L. Paull"
    ],
    "submission_date": "2022-01-05",
    "revised_dates": [],
    "arxiv_id": "2201.01666",
    "venue": "International Conference on Learning Representations",
    "year": 2022
  },
  "2111-00633": {
    "title": "Settling the Horizon-Dependence of Sample Complexity in Reinforcement Learning",
    "authors": [
      "Yuanzhi Li",
      "Ruosong Wang",
      "Lin F. Yang"
    ],
    "submission_date": "2021-11-01",
    "revised_dates": [],
    "doi": "10.1109/FOCS52979.2021.00097",
    "arxiv_id": "2111.00633",
    "venue": "IEEE Annual Symposium on Foundations of Computer Science",
    "year": 2021
  },
  "2110-06394": {
    "title": "Reward-Free Model-Based Reinforcement Learning with Linear Function Approximation",
    "authors": [
      "Weitong Zhang",
      "Dongruo Zhou",
      "Quanquan Gu"
    ],
    "submission_date": "2021-10-12",
    "revised_dates": [],
    "arxiv_id": "2110.06394",
    "venue": "Neural Information Processing Systems",
    "year": 2021
  },
  "2110-03244": {
    "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver",
    "authors": [
      "Xiaoyu Chen",
      "Jiachen Hu",
      "Lin F. Yang",
      "Liwei Wang"
    ],
    "submission_date": "2021-10-07",
    "revised_dates": [],
    "arxiv_id": "2110.03244",
    "venue": "International Conference on Learning Representations",
    "year": 2021
  },
  "2104-11186": {
    "title": "Stochastic Shortest Path: Minimax, Parameter-Free and Towards Horizon-Free Regret",
    "authors": [
      "Jean Tarbouriech",
      "Runlong Zhou",
      "S. Du",
      "Matteo Pirotta",
      "M. Valko",
      "A. Lazaric"
    ],
    "submission_date": "2021-04-22",
    "revised_dates": [],
    "arxiv_id": "2104.11186",
    "venue": "Neural Information Processing Systems",
    "year": 2021
  },
  "2103-14077": {
    "title": "Nearly Horizon-Free Offline Reinforcement Learning",
    "authors": [
      "Tongzheng Ren",
      "Jialian Li",
      "Bo Dai",
      "S. Du",
      "S. Sanghavi"
    ],
    "submission_date": "2021-03-25",
    "revised_dates": [],
    "arxiv_id": "2103.14077",
    "venue": "Neural Information Processing Systems",
    "year": 2021
  },
  "2101-12745": {
    "title": "Improved Variance-Aware Confidence Sets for Linear Bandits and Linear Mixture MDP",
    "authors": [
      "Zihan Zhang",
      "Jiaqi Yang",
      "Xiangyang Ji",
      "S. Du"
    ],
    "submission_date": "2021-01-29",
    "revised_dates": [],
    "arxiv_id": "2101.12745",
    "venue": "Neural Information Processing Systems",
    "year": 2021
  },
  "2012-08507": {
    "title": "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes",
    "authors": [
      "Dongruo Zhou",
      "Quanquan Gu",
      "Csaba Szepesvari"
    ],
    "submission_date": "2020-12-15",
    "revised_dates": [],
    "arxiv_id": "2012.08507",
    "venue": "Annual Conference Computational Learning Theory",
    "year": 2020
  },
  "2011-11566": {
    "title": "Logarithmic Regret for Reinforcement Learning with Linear Function Approximation",
    "authors": [
      "Jiafan He",
      "Dongruo Zhou",
      "Quanquan Gu"
    ],
    "submission_date": "2020-11-23",
    "revised_dates": [],
    "arxiv_id": "2011.11566",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "2010-05901": {
    "title": "Nearly Minimax Optimal Reward-free Reinforcement Learning",
    "authors": [
      "Zihan Zhang",
      "S. Du",
      "Xiangyang Ji"
    ],
    "submission_date": "2020-10-12",
    "revised_dates": [],
    "arxiv_id": "2010.05901",
    "venue": "arXiv.org",
    "year": 2020
  },
  "2010-01374": {
    "title": "Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions",
    "authors": [
      "G. Weisz",
      "P. Amortila",
      "Csaba Szepesvari"
    ],
    "submission_date": "2020-10-03",
    "revised_dates": [],
    "arxiv_id": "2010.01374",
    "venue": "International Conference on Algorithmic Learning Theory",
    "year": 2020
  },
  "2009-13503": {
    "title": "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon",
    "authors": [
      "Zihan Zhang",
      "Xiangyang Ji",
      "S. Du"
    ],
    "submission_date": "2020-09-28",
    "revised_dates": [],
    "arxiv_id": "2009.13503",
    "venue": "Annual Conference Computational Learning Theory",
    "year": 2020
  },
  "2008-07737": {
    "title": "Provably Efficient Reward-Agnostic Navigation with Linear Value Iteration",
    "authors": [
      "A. Zanette",
      "A. Lazaric",
      "Mykel J. Kochenderfer",
      "E. Brunskill"
    ],
    "submission_date": "2020-08-18",
    "revised_dates": [],
    "arxiv_id": "2008.07737",
    "venue": "Neural Information Processing Systems",
    "year": 2020
  },
  "2007-13442": {
    "title": "Fast active learning for pure exploration in reinforcement learning",
    "authors": [
      "Pierre M'enard",
      "O. D. Domingues",
      "Anders Jonsson",
      "E. Kaufmann",
      "Edouard Leurent",
      "Michal Valko"
    ],
    "submission_date": "2020-07-26",
    "revised_dates": [],
    "arxiv_id": "2007.13442",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "2006-13165": {
    "title": "Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping",
    "authors": [
      "Dongruo Zhou",
      "Jiafan He",
      "Quanquan Gu"
    ],
    "submission_date": "2020-06-23",
    "revised_dates": [],
    "arxiv_id": "2006.13165",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "2006-11274": {
    "title": "On Reward-Free Reinforcement Learning with Linear Function Approximation",
    "authors": [
      "Ruosong Wang",
      "S. Du",
      "Lin F. Yang",
      "R. Salakhutdinov"
    ],
    "submission_date": "2020-06-19",
    "revised_dates": [],
    "arxiv_id": "2006.11274",
    "venue": "Neural Information Processing Systems",
    "year": 2020
  },
  "2006-06294": {
    "title": "Adaptive Reward-Free Exploration",
    "authors": [
      "E. Kaufmann",
      "Pierre M'enard",
      "O. D. Domingues",
      "Anders Jonsson",
      "Edouard Leurent",
      "Michal Valko"
    ],
    "submission_date": "2020-06-11",
    "revised_dates": [],
    "arxiv_id": "2006.06294",
    "venue": "International Conference on Algorithmic Learning Theory",
    "year": 2020
  },
  "2006-03864": {
    "title": "Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample Complexity",
    "authors": [
      "Zihan Zhang",
      "Yuanshuo Zhou",
      "Xiangyang Ji"
    ],
    "submission_date": "2020-06-06",
    "revised_dates": [],
    "arxiv_id": "2006.03864",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "2006-01107": {
    "title": "Model-Based Reinforcement Learning with Value-Targeted Regression",
    "authors": [
      "Alex Ayoub",
      "Zeyu Jia",
      "Csaba Szepesvari",
      "Mengdi Wang",
      "Lin F. Yang"
    ],
    "submission_date": "2020-06-01",
    "revised_dates": [],
    "arxiv_id": "2006.01107",
    "venue": "Conference on Learning for Dynamics & Control",
    "year": 2020
  },
  "2005-00527": {
    "title": "Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon Reinforcement Learning?",
    "authors": [
      "Ruosong Wang",
      "S. Du",
      "Lin F. Yang",
      "S. Kakade"
    ],
    "submission_date": "2020-05-01",
    "revised_dates": [],
    "arxiv_id": "2005.00527",
    "venue": "arXiv.org",
    "year": 2020
  },
  "2003-00153": {
    "title": "Learning Near Optimal Policies with Low Inherent Bellman Error",
    "authors": [
      "A. Zanette",
      "A. Lazaric",
      "Mykel J. Kochenderfer",
      "E. Brunskill"
    ],
    "submission_date": "2020-02-29",
    "revised_dates": [],
    "arxiv_id": "2003.00153",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "2002-02794": {
    "title": "Reward-Free Exploration for Reinforcement Learning",
    "authors": [
      "Chi Jin",
      "A. Krishnamurthy",
      "Max Simchowitz",
      "Tiancheng Yu"
    ],
    "submission_date": "2020-02-07",
    "revised_dates": [],
    "arxiv_id": "2002.02794",
    "venue": "International Conference on Machine Learning",
    "year": 2020
  },
  "1912-05830": {
    "title": "Provably Efficient Exploration in Policy Optimization",
    "authors": [
      "Qi Cai",
      "Zhuoran Yang",
      "Chi Jin",
      "Zhaoran Wang"
    ],
    "submission_date": "2019-12-12",
    "revised_dates": [],
    "arxiv_id": "1912.05830",
    "venue": "International Conference on Machine Learning",
    "year": 2019
  },
  "1912-04136": {
    "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
    "authors": [
      "Yining Wang",
      "Ruosong Wang",
      "S. Du",
      "A. Krishnamurthy"
    ],
    "submission_date": "2019-12-09",
    "revised_dates": [],
    "arxiv_id": "1912.04136",
    "venue": "International Conference on Learning Representations",
    "year": 2019
  },
  "1911-00567": {
    "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration",
    "authors": [
      "A. Zanette",
      "David Brandfonbrener",
      "Matteo Pirotta",
      "A. Lazaric"
    ],
    "submission_date": "2019-11-01",
    "revised_dates": [],
    "arxiv_id": "1911.00567",
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "year": 2019
  },
  "1910-10597": {
    "title": "Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles",
    "authors": [
      "Aditya Modi",
      "Nan Jiang",
      "Ambuj Tewari",
      "Satinder Singh"
    ],
    "submission_date": "2019-10-23",
    "revised_dates": [],
    "arxiv_id": "1910.10597",
    "venue": "International Conference on Artificial Intelligence and Statistics",
    "year": 2019
  },
  "1910-03016": {
    "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
    "authors": [
      "S. Du",
      "S. Kakade",
      "Ruosong Wang",
      "Lin F. Yang"
    ],
    "submission_date": "2019-10-07",
    "revised_dates": [],
    "arxiv_id": "1910.03016",
    "venue": "International Conference on Learning Representations",
    "year": 2019
  },
  "1907-05388": {
    "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
    "authors": [
      "Chi Jin",
      "Zhuoran Yang",
      "Zhaoran Wang",
      "Michael I. Jordan"
    ],
    "submission_date": "2019-07-11",
    "revised_dates": [],
    "doi": "10.1287/moor.2022.1309",
    "arxiv_id": "1907.05388",
    "venue": "Annual Conference Computational Learning Theory",
    "year": 2019
  },
  "1906-09323": {
    "title": "Reinforcement Learning with Convex Constraints",
    "authors": [
      "Sobhan Miryoosefi",
      "Kianté Brantley",
      "Hal Daumé",
      "Miroslav Dudík",
      "R. Schapire"
    ],
    "submission_date": "2019-06-01",
    "revised_dates": [],
    "arxiv_id": "1906.09323",
    "venue": "Neural Information Processing Systems",
    "year": 2019
  },
  "1905-10389": {
    "title": "Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound",
    "authors": [
      "Lin F. Yang",
      "Mengdi Wang"
    ],
    "submission_date": "2019-05-24",
    "revised_dates": [],
    "arxiv_id": "1905.10389",
    "venue": "International Conference on Machine Learning",
    "year": 2019
  },
  "1805-11074": {
    "title": "Reward Constrained Policy Optimization",
    "authors": [
      "Chen Tessler",
      "D. Mankowitz",
      "Shie Mannor"
    ],
    "submission_date": "2018-05-28",
    "revised_dates": [],
    "arxiv_id": "1805.11074",
    "venue": "International Conference on Learning Representations",
    "year": 2018
  },
  "1803-00606": {
    "title": "On Oracle-Efficient PAC RL with Rich Observations",
    "authors": [
      "Christoph Dann",
      "Nan Jiang",
      "A. Krishnamurthy",
      "Alekh Agarwal",
      "J. Langford",
      "R. Schapire"
    ],
    "submission_date": "2018-03-01",
    "revised_dates": [],
    "arxiv_id": "1803.00606",
    "venue": "Neural Information Processing Systems",
    "year": 2018
  },
  "1705-10528": {
    "title": "Constrained Policy Optimization",
    "authors": [
      "Joshua Achiam",
      "David Held",
      "Aviv Tamar",
      "P. Abbeel"
    ],
    "submission_date": "2017-05-30",
    "revised_dates": [],
    "arxiv_id": "1705.10528",
    "venue": "International Conference on Machine Learning",
    "year": 2017
  },
  "1703-04977": {
    "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
    "authors": [
      "Alex Kendall",
      "Y. Gal"
    ],
    "submission_date": "2017-03-15",
    "revised_dates": [],
    "arxiv_id": "1703.04977",
    "venue": "Neural Information Processing Systems",
    "year": 2017
  },
  "1610-09512": {
    "title": "Contextual Decision Processes with low Bellman rank are PAC-Learnable",
    "authors": [
      "Nan Jiang",
      "A. Krishnamurthy",
      "Alekh Agarwal",
      "J. Langford",
      "R. Schapire"
    ],
    "submission_date": "2016-10-29",
    "revised_dates": [],
    "arxiv_id": "1610.09512",
    "venue": "International Conference on Machine Learning",
    "year": 2016
  }
}