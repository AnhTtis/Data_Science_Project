\section{Methodology}
\input{tex_figs/multimodal_contrastive}
Consider a cohort of $P$ individuals undergoing a study wearing a smartwatch-based remote monitoring system.
This wearable setup allows for the collection of sensory readings pertinent to users' exhibited physiological and activity patterns throughout the day.
In our case, the study in question monitors the connection between readings made by the smartwatch, which are mostly related to physiological signals, health and activity status, and short-term {\it perceived} stress reported by the user.
The smartwatch's readings can thus be grouped into features corresponding to several modalities: $m \in \{1,2,\cdots,M\}$.
This grouping depends mainly on the nature of the features, as well as the setup and the interface provided by the smartwatch.
From each modality, we have a sequence of observed feature vectors:
\begin{equation}
\pmb{x}_m^{(p)} = \{x^{(p)}_{m,t}\}_{t \in [T^{(p)}_{m,\text{max}}]}
\end{equation}

From a user's timeline, we extract short-term timespans, each of which corresponds to an {\it episode} $e$, which is the result of filtering the timeline and restricting it to the episode's timespan: $e=(t_{\text{start}}, t_{\text{end}})$:
\begin{equation}
    \pmb{x}_{m,e}^{(p)} = \{x^{(p)}_{m,t} \in \mathbf{x}_m^{(p)} \mathopen|\mathclose t \in e \}
\end{equation}

We have a parameterized domain-specific\footnote{The term {\it domain} in this manuscript refers to the observation type, for example, a Transformer-based Language Model could efficiently represent data from textual domain, and there could be multiple {\it modalities} with their observations being text data, each represented by their own specific encoder.} encoder $f(\cdot; \theta_m)$ for each modality $m \in [M]$, which performs the task of mapping the observed data from this modality to a {\it shared} semantic space $\mathcal{S}$:
\begin{equation}
f(\cdot;\theta_m): \mathcal{X}_m \rightarrow \mathcal{S} \quad \forall m \in [M]
\end{equation}
Hence, the latent embedding denoted by $z^{(p)}_{m,e}$ can be found as follows:
\begin{equation}\label{eq:findinglatents}
z^{(p)}_{m,e} = f(\pmb{x}_{m,e}^{(p)};\theta_m) \quad \forall m \in [M]
\end{equation}

One could argue that it is plausible to assume that the contributions of observations from different modalities to the final prediction on a specific task follow a non-uniform distribution in most cases.
For instance, there is no reason to assume the statistical significance of heart-rate time series is the same as pulse oximeter readings for the task of stress detection.
Going one step further, such disparity can manifest itself in the level of {\it instance} representations as well.
To illustrate this further, consider a simple case of "missingness" in data or presence of noise.
This could mean that even though mode $m_1$, for example, is more informative (in expectation) to the task $\tau$, in an instance where the data from this group appears missing or clearly corrupt, the importance of other modalities could change respectively.
Therefore, we have designed a {\it modality importance} head, implemented as a fully connected pipeline, which determines the contribution of each mode by weighing their respective embedding vectors, which were projected to the same semantic space.
The first step to this attention-based pooling mechanism involves using the modality importance head and obtaining  (yet unnormalized) weight $a_i^{(m)}$ for the latent embedding of modality $m$'s information in an instance $i$:
\begin{equation}\label{eq:attn1}
    a_i^{(m)} \leftarrow g(\z_i^{(m)};\pmb{\psi}) \quad \forall m \in [M]
\end{equation}
This is followed by a softmax operation to make sure that the summation of the predicted contributions maps to unity, in other words, the contribution matrix is right stochastic:
\begin{equation}\label{eq:attn2}
\alpha_i^{(m)}=\frac{\exp(a_i^{(m)})}{\sum_{j\in[M]}\exp(a_i^{(j)})} 
\end{equation}
And thus the final aggregated latent is computed using these attention weights:
$z=\sum_{i=1}^M \alpha_i^{(m)} \cdot z_{m}$.

We leverage the cosine similarity $\phi(\cdot, \cdot)$ to measure the compatibility between the latent representation of each mode and the aggregate representation $z_i$.
\begin{equation}\label{eq:similarity}
\phi(\pmb{u},\pmb{v})=\frac{h(\pmb{u})^T\cdot h(\pmb{v})}{\|h(\pmb{u})\|_2\cdot \|h(\pmb{v})\|}
\end{equation}

In other words, we use the aggregated embedding $z$ as an anchor and define a contrastive objective to leverage the distances and inconsistency between the latent embeddings:
\vspace{-1mm}
\begin{equation}\label{eq:loss}
\mathcal{L}_{\text{cl}}=\frac{1}{|\mathcal{B}|}\sum_{i\in \mathcal{B}}\frac{1}{|\mathcal{M}|}\sum_{m\in \mathcal{M}}-\log\frac{\exp(\phi(\z_i^{(m)},\z_i)/\tau)}{\sum_{j\in \mathcal{B},j\neq i}\exp(\phi(\z_i^{(m)},\z_j)/\tau)} \vspace{-2mm}
\end{equation}

We have experimented with $\mathcal{L}_{\text{cl}}$ in the following training schemes:

\begin{itemize}
    \item {\it Pre-training}: Pre-training the model parameters by optimizing $\mathcal{L}_{\text{cl}}$ through a long training sequence.
    Afterward, start with the resulting weights as the initial point for the supervised fine-tuning of the model with the cross-entropy objective:
    \begin{equation}
    \mathcal{L}_{\text{cross-entropy}}=- \sum_{c\in\mathcal{C}} y_c \ln p_c
    \end{equation}
    In the equation above, $\mathcal{C}$ is the set of all classes (e.g., in our experiments, the two categories of stressed and non-stressed for each episode), and $p_c$ is the predicted probability of class $c$ for an observation, computed by passing representations through a final projection and Softmax layer.
    \item {\it Regularization}: Use $\lambda_{\text{reg}} \cdot \mathcal{L}_{\text{cl}}$ as a regularization term in the overall loss, and train the model by optimizing this loss simultaneously as the supervised learning objective.
\end{itemize}
There are several points worth remarking upon with regard to the comparison of these two training schemes. 
To begin with, deciding whether pre-training is going to lead to better generalization performance versus the regularization-based approach depends on model complexity, availability of data, and the challenges of the specific task that one is targeting.
That being said, the regularization approach is expected to be considerably faster than the two-stage pre-training and fine-tuning method, and in our experiments on the task of predicting stress labels, it led to better test performance as well.
% late-fusion vs early fusion and information advantage
% ----------------------------------------
% plug-and-play for inter-smartwatch compatibilities
% ----------------------------------------
% attention module and the contrastive setup (item-level vs mode-level)
% ----------------------------------------
% "regularization" alternative for "quicker" and "better performance", cite NCR
% ----------------------------------------