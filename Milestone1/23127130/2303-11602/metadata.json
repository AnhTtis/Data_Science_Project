{
    "arxiv_id": "2303.11602",
    "paper_title": "Convergence of variational Monte Carlo simulation and scale-invariant pre-training",
    "authors": [
        "Nilin Abrahamsen",
        "Zhiyan Ding",
        "Gil Goldshlager",
        "Lin Lin"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2024-02-22"
    ],
    "latest_version": 3,
    "categories": [
        "cs.LG",
        "physics.comp-ph"
    ],
    "abstract": "We provide theoretical convergence bounds for the variational Monte Carlo (VMC) method as applied to optimize neural network wave functions for the electronic structure problem. We study both the energy minimization phase and the supervised pre-training phase that is commonly used prior to energy minimization. For the energy minimization phase, the standard algorithm is scale-invariant by design, and we provide a proof of convergence for this algorithm without modifications. The pre-training stage typically does not feature such scale-invariance. We propose using a scale-invariant loss for the pretraining phase and demonstrate empirically that it leads to faster pre-training.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11602v1",
        "http://arxiv.org/pdf/2303.11602v2",
        "http://arxiv.org/pdf/2303.11602v3"
    ],
    "publication_venue": "Updated presentation to unify notation and focus on the VMC setting. Added new numerics for scale-invariant supervised pre-training"
}