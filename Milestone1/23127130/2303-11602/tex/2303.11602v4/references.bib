@article{khaled2020better,
      title={Better Theory for SGD in the Nonconvex World}, 
      author={Ahmed Khaled and Peter Richtárik},
      year={2020},
      journal={arXiv/2002.03329},
}
@article{garrigos2024handbook,
      title={Handbook of Convergence Theorems for (Stochastic) Gradient Methods}, 
      author={Guillaume Garrigos and Robert M. Gower},
      year={2024},
      journal={arXiv/2301.11235},
}

@article{wilson2023neural,
  title={Neural network ansatz for periodic wave functions and the homogeneous electron gas},
  author={Wilson, Max and Moroni, Saverio and Holzmann, Markus and Gao, Nicholas and Wudarski, Filip and Vegge, Tejs and Bhowmik, Arghya},
  journal={Physical Review B},
  volume={107},
  number={23},
  pages={235139},
  year={2023},
  publisher={APS}
}

@inproceedings{polgrad,
author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
year = {1999},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
pages = {1057–1063},
numpages = {7},
location = {Denver, CO},
series = {NIPS'99}
}



@misc{vonglehn2023selfattention,
      title={A Self-Attention Ansatz for Ab-initio Quantum Chemistry}, 
      author={Ingrid von Glehn and James S. Spencer and David Pfau},
      year={2023},
      eprint={2211.13672},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph}
}


@book{szabo1996modern,
  title={Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory},
  author={Szabo, A. and Ostlund, N.S.},
  isbn={9780486691862},
  lccn={lc96010775},
  series={Dover Books on Chemistry},
  url={https://books.google.com/books?id=6mV9gYzEkgIC},
  year={1996},
  publisher={Dover Publications}
}


@inproceedings{
kodryan_training_2022,
title={Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes},
author={Maxim Kodryan and Ekaterina Lobacheva and Maksim Nakhodnov and Dmitry P. Vetrov},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@inproceedings{li_robust_2022,
	title = {Robust {Training} of {Neural} {Networks} {Using} {Scale} {Invariant} {Architectures}},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Zhiyuan and Bhojanapalli, Srinadh and Zaheer, Manzil and Reddi, Sashank and Kumar, Sanjiv},
	year = {2022},
}
@article{van_2017,
  
  author = {van Laarhoven, Twan},
  
  title = {L2 Regularization versus Batch and Weight Normalization},
  
  journal = {arXiv/1706.05350},
  year={2017},
}

@inproceedings{Ioffe_2015,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning - Volume 37},
}

@INPROCEEDINGS{spherequadratic,
  author={Vu, Trung and Raich, Raviv and Fu, Xiao},
  booktitle={2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={On Convergence of Projected Gradient Descent for Minimizing a Large-Scale Quadratic Over the Unit Sphere}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/MLSP.2019.8918830}}

@inproceedings{Wu_2018,
  
  author = {Wu, Yuxin and He, Kaiming},
  
  
  title = {Group Normalization},
  
  booktitle = {In Proceedings of
the European conference on computer vision (ECCV)},
  
  year = {2018},
}

@article{Ba_2016,
  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  
  
  title = {Layer Normalization},
  
  journal = {arXiv/1607.06450},
  
  year = {2016},
}

@inproceedings{Saliman_2016,
author = {Salimans, Tim and Kingma, Diederik P.},
title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
year = {2016},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
}
@inproceedings{
arora2018theoretical,
title={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},
author={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
meng_mathcalg-sgd_2021,
title={G-{SGD}: Optimizing Re{LU} Neural Networks in its Positively Scale-Invariant Space},
author={Qi Meng and Shuxin Zheng and Huishuai Zhang and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{Wan_2021,
 author = {Wan, Ruosi and Zhu, Zhanxing and Zhang, Xiangyu and Sun, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6380--6391},
 title = {Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay},
 volume = {34},
 year = {2021}
}

@article{Foulkes2001,
  title = {Quantum Monte Carlo simulations of solids},
  author = {Foulkes, W. M. C. and Mitas, L. and Needs, R. J. and Rajagopal, G.},
  journal = {Rev. Mod. Phys.},
  volume = {73},
  issue = {1},
  pages = {33--83},
  numpages = {0},
  year = {2001},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.73.33},
}


@incollection{Toulouse2016,
title = {Chapter Fifteen - Introduction to the Variational and Diffusion Monte Carlo Methods},
editor = {Philip E. Hoggan and Telhat Ozdogan},
series = {Advances in Quantum Chemistry},
publisher = {Academic Press},
volume = {73},
pages = {285-314},
year = {2016},
booktitle = {Electron Correlation in Molecules – ab initio Beyond Gaussian Quantum Chemistry},
issn = {0065-3276},
doi = {10.1016/bs.aiq.2015.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065327615000386},
author = {Julien Toulouse and Roland Assaraf and Cyrus J. Umrigar},
keywords = {Quantum Monte Carlo, Electronic structure calculations, Metropolis–Hastings algorithm, Fixed-node approximation, Statistical methods},
abstract = {We provide a pedagogical introduction to the two main variants of real-space quantum Monte Carlo methods for electronic structure calculations: variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC). Assuming no prior knowledge on the subject, we review in depth the Metropolis–Hastings algorithm used in VMC for sampling the square of an approximate wave function, discussing details important for applications to electronic systems. We also review in detail the more sophisticated DMC algorithm within the fixed-node approximation, introduced to avoid the infamous Fermionic sign problem, which allows one to sample a more accurate approximation to the ground-state wave function. Throughout this review, we discuss the statistical methods used for evaluating expectation values and statistical uncertainties. In particular, we show how to estimate nonlinear functions of expectation values and their statistical uncertainties.}
}


@book{Becca2017,
place={Cambridge},
title={Quantum Monte Carlo Approaches for Correlated Systems},
DOI={10.1017/9781316417041}, publisher={Cambridge University Press},
author={Becca, Federico and Sorella, Sandro},
year={2017}
}

@article{Carleo2017,
   title={Solving the quantum many-body problem with artificial neural networks},
   volume={355},
   ISSN={1095-9203},
   DOI={10.1126/science.aag2302},
   number={6325},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Carleo, Giuseppe and Troyer, Matthias},
   year={2017},
   month={Feb},
   pages={602–606}
}


@misc{Spencer_2020,
  doi = {10.48550/ARXIV.2011.07125},
  url = {https://arxiv.org/abs/2011.07125},
  author = {Spencer, James S. and Pfau, David and Botev, Aleksandar and Foulkes, W. M. C.},
  keywords = {Computational Physics (physics.comp-ph), Machine Learning (cs.LG), Chemical Physics (physics.chem-ph), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Better, Faster Fermionic Neural Networks},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Jeff_2021,
  author = {Lin, Jeffmin and Goldshlager, Gil and Lin, Lin},
  title = {Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation},
  journal = {arXiv/2112.03491},
  year = {2021},
}

@article{Nomura2017,
  title = {Restricted Boltzmann machine learning for solving strongly correlated quantum systems},
  author = {Nomura, Yusuke and Darmawan, Andrew S. and Yamaji, Youhei and Imada, Masatoshi},
  journal = {Phys. Rev. B},
  volume = {96},
  issue = {20},
  pages = {205152},
  numpages = {8},
  year = {2017},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.96.205152},
}
@article{Choo2018,
  title = {Symmetries and Many-Body Excitations with Neural-Network Quantum States},
  author = {Choo, Kenny and Carleo, Giuseppe and Regnault, Nicolas and Neupert, Titus},
  journal = {Phys. Rev. Lett.},
  volume = {121},
  issue = {16},
  pages = {167204},
  numpages = {6},
  year = {2018},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.121.167204},
}
@article{Nagy2019,
  title = {Variational Quantum Monte Carlo Method with a Neural-Network Ansatz for Open Quantum Systems},
  author = {Nagy, Alexandra and Savona, Vincenzo},
  journal = {Phys. Rev. Lett.},
  volume = {122},
  issue = {25},
  pages = {250501},
  numpages = {6},
  year = {2019},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.122.250501},
}
@article{Yang2020,
   title={Deep learning-enhanced variational Monte Carlo method for quantum many-body physics},
   volume={2},
   ISSN={2643-1564},
   DOI={10.1103/physrevresearch.2.012039},
   number={1},
   journal={Physical Review Research},
   publisher={American Physical Society (APS)},
   author={Yang, Li and Leng, Zhaoqi and Yu, Guangyuan and Patel, Ankit and Hu, Wen-Jun and Pu, Han},
   year={2020},
   month={Feb}
}
@article{Luo2019,
  title = {Backflow Transformations via Neural Networks for Quantum Many-Body Wave Functions},
  author = {Luo, Di and Clark, Bryan K.},
  journal = {Phys. Rev. Lett.},
  volume = {122},
  issue = {22},
  pages = {226401},
  numpages = {6},
  year = {2019},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.122.226401},
}

@article{Han2019,
    title = "Solving many-electron Schrödinger equation using deep neural networks",
    journal = "Journal of Computational Physics",
    volume = "399",
    pages = "108929",
    year = "2019",
    issn = "0021-9991",
    doi = "10.1016/j.jcp.2019.108929",
    author = "Jiequn Han and Linfeng Zhang and Weinan E",
    keywords = "Schrödinger equation, Variational Monte Carlo, Deep neural networks, Trial wave-function",
    abstract = "We introduce a new family of trial wave-functions based on deep neural networks to solve the many-electron Schrödinger equation. The Pauli exclusion principle is dealt with explicitly to ensure that the trial wave-functions are physical. The optimal trial wave-function is obtained through variational Monte Carlo and the computational cost scales quadratically with the number of electrons. The algorithm does not make use of any prior knowledge such as atomic orbitals. Yet it is able to represent accurately the ground-states of the tested systems, including He, H2, Be, B, LiH, and a chain of 10 hydrogen atoms. This opens up new possibilities for solving large-scale many-electron Schrödinger equation."
}
@article{Hermann2020,
   title={Deep-neural-network solution of the electronic Schrödinger equation},
   volume={12},
   ISSN={1755-4349},
   DOI={10.1038/s41557-020-0544-y},
   number={10},
   journal={Nature Chemistry},
   publisher={Springer Science and Business Media LLC},
   author={Hermann, Jan and Schätzle, Zeno and Noé, Frank},
   year={2020},
   month={Sep},
   pages={891–897}
}


@article{StokesRobledo2020,
  title = {Phases of two-dimensional spinless lattice fermions with first-quantized deep neural-network quantum states},
  author = {Stokes, James and Moreno, Javier Robledo and Pnevmatikakis, Eftychios A. and Carleo, Giuseppe},
  journal = {Phys. Rev. B},
  volume = {102},
  issue = {20},
  pages = {205122},
  numpages = {10},
  year = {2020},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.102.205122},
}


@Article{Choo2020,
author={Choo, Kenny
and Mezzacapo, Antonio
and Carleo, Giuseppe},
title={Fermionic neural-network states for ab-initio electronic structure},
journal={Nature Communications},
year={2020},
month={May},
day={12},
volume={11},
number={1},
pages={2368},
abstract={Neural-network quantum states have been successfully used to study a variety of lattice and continuous-space problems. Despite a great deal of general methodological developments, representing fermionic matter is however still early research activity. Here we present an extension of neural-network quantum states to model interacting fermionic problems. Borrowing techniques from quantum simulation, we directly map fermionic degrees of freedom to spin ones, and then use neural-network quantum states to perform electronic structure calculations. For several diatomic molecules in a minimal basis set, we benchmark our approach against widely used coupled cluster methods, as well as many-body variational states. On some test molecules, we systematically improve upon coupled cluster methods and Jastrow wave functions, reaching chemical accuracy or better. Finally, we discuss routes for future developments and improvements of the methods presented.},
issn={2041-1723},
doi={10.1038/s41467-020-15724-9},
}

@misc{Glehn2022,
  doi = {10.48550/ARXIV.2211.13672},
  
  url = {https://arxiv.org/abs/2211.13672},
  
  author = {von Glehn, Ingrid and Spencer, James S. and Pfau, David},
  
  keywords = {Chemical Physics (physics.chem-ph), Machine Learning (cs.LG), Computational Physics (physics.comp-ph), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Self-Attention Ansatz for Ab-initio Quantum Chemistry},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{Cassella2022,
  doi = {10.48550/ARXIV.2202.05183},
  
  url = {https://arxiv.org/abs/2202.05183},
  
  author = {Cassella, G. and Sutterud, H. and Azadi, S. and Drummond, N. D. and Pfau, D. and Spencer, J. S. and Foulkes, W. M. C.},
  
  keywords = {Computational Physics (physics.comp-ph), Other Condensed Matter (cond-mat.other), Strongly Correlated Electrons (cond-mat.str-el), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Discovering Quantum Phase Transitions with Fermionic Neural Networks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Gerard2022,
  doi = {10.48550/ARXIV.2205.09438},
  
  url = {https://arxiv.org/abs/2205.09438},
  
  author = {Gerard, Leon and Scherbela, Michael and Marquetand, Philipp and Grohs, Philipp},
  
  keywords = {Machine Learning (cs.LG), Chemical Physics (physics.chem-ph), Computational Physics (physics.comp-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Gold-standard solutions to the Schrödinger equation using deep learning: How much physics do we need?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Hermann_review,
  doi = {10.48550/ARXIV.2208.12590},
  
  url = {https://arxiv.org/abs/2208.12590},
  
  author = {Hermann, Jan and Spencer, James and Choo, Kenny and Mezzacapo, Antonio and Foulkes, W. M. C. and Pfau, David and Carleo, Giuseppe and Noé, Frank},
  
  keywords = {Chemical Physics (physics.chem-ph), Machine Learning (cs.LG), Computational Physics (physics.comp-ph), Machine Learning (stat.ML), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Ab-initio quantum chemistry with neural-network wavefunctions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{SAGA-2013,
author = {Schmidt, M. and Roux, N. and Bach, F.},
year = {2013},
month = {09},
pages = {},
title = {Minimizing Finite Sums with the Stochastic Average Gradient},
volume = {162},
journal = {Mathematical Programming},
doi = {10.1007/s10107-016-1030-6}
}
@article{SAGA-2014,
author = {Defazio, A. and Bach, F. and Lacoste-Julien, S.},
year = {2014},
month = {07},
pages = {},
title = {{SAGA}: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
volume = {2},
journal = {Advances in Neural Information Processing Systems}
}
@inproceedings{Johnson_Zhang,
author = {Johnson, R. and Zhang, T.},
title = {Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction},
year = {2013},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems},
pages = {315–323},
numpages = {9},
}






@inproceedings{tripuraneni_averaging_2018,
	title = {Averaging {Stochastic} {Gradient} {Descent} on {Riemannian} {Manifolds}},
	url = {https://proceedings.mlr.press/v75/tripuraneni18a.html},
	abstract = {We consider the minimization of a function defined on a Riemannian manifold \${\textbackslash}mathcal\{M\}\$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on \${\textbackslash}mathcal\{M\}\$ to an averaged iterate sequence with a robust and fast \$O(1/n)\$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems.  Finally, we demonstrate how these ideas apply to the case of streaming \$k\$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence.},
	language = {en},
	urldate = {2023-01-22},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan, Michael I.},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {650--687},
	file = {Full Text PDF:/Users/nilin5/Zotero/storage/I5YX7HZ7/Tripuraneni et al. - 2018 - Averaging Stochastic Gradient Descent on Riemannia.pdf:application/pdf},
}

@article{bonnabel_stochastic_2013,
	title = {Stochastic {Gradient} {Descent} on {Riemannian} {Manifolds}},
	volume = {58},
	issn = {1558-2523},
	doi = {10.1109/TAC.2013.2254619},
	abstract = {Stochastic gradient descent is a simple approach to find the local minima of a cost function whose evaluations are corrupted by noise. In this paper, we develop a procedure extending stochastic gradient descent algorithms to the case where the function is defined on a Riemannian manifold. We prove that, as in the Euclidian case, the gradient descent algorithm converges to a critical point of the cost function. The algorithm has numerous potential applications, and is illustrated here by four examples. In particular a novel gossip algorithm on the set of covariance matrices is derived and tested numerically.},
	number = {9},
	journal = {IEEE Transactions on Automatic Control},
	author = {Bonnabel, Silvère},
	month = sep,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Approximation methods, Convergence, Cost function, Covariance matrices, Manifolds, Nonlinear identification, Riemannian geometry, Standards, stochastic approximation, Trajectory},
	pages = {2217--2229},
	file = {IEEE Xplore Abstract Record:/Users/nilin5/Zotero/storage/RT2T46Y8/6487381.html:text/html;Submitted Version:/Users/nilin5/Zotero/storage/YPQS4D3N/Bonnabel - 2013 - Stochastic Gradient Descent on Riemannian Manifold.pdf:application/pdf},
}

@inproceedings{vu_convergence_2019,
	title = {On {Convergence} of {Projected} {Gradient} {Descent} for {Minimizing} a {Large}-{Scale} {Quadratic} {Over} the {Unit} {Sphere}},
	doi = {10.1109/MLSP.2019.8918830},
	abstract = {Unit sphere-constrained quadratic optimization has been studied extensively over the past decades. While state-of-art algorithms for solving this problem often rely on relaxation or approximation techniques, there has been little research into scalable first-order methods that tackle the problem in its original form. These first-order methods are often more well-suited for the big data setting. In this paper, we provide a novel analysis of the simple projected gradient descent method for minimizing a quadratic over a sphere. When the gradient step size is sufficiently small, we show that convergence is locally linear and provide a closed-form expression for the rate. Moreover, a careful selection of the step size can stimulate convergence to the global solution while preventing convergence to local minima.},
	booktitle = {2019 {IEEE} 29th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Vu, Trung and Raich, Raviv and Fu, Xiao},
	month = oct,
	year = {2019},
	note = {ISSN: 1551-2541},
	keywords = {Acceleration, Convergence, convergence analysis, Eigenvalues and eigenfunctions, large-scale optimization, Linear programming, Minimization, quadratic programming, Quadratic programming, unit-norm constraint},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/nilin5/Zotero/storage/HL5P2M6T/8918830.html:text/html},
}


@INPROCEEDINGS{7852234,
  author={Hauswirth, Adrian and Bolognani, Saverio and Hug, Gabriela and Dörfler, Florian},
  booktitle={2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Projected gradient descent on Riemannian manifolds with applications to online power system optimization}, 
  year={2016},
  volume={},
  number={},
  pages={225-232},
  doi={10.1109/ALLERTON.2016.7852234}}
  
  @article{Bonnabel_2011,
author = {Bonnabel, Silvère},
year = {2011},
month = {11},
pages = {},
title = {Stochastic Gradient Descent on Riemannian Manifolds},
volume = {58},
journal = {IEEE Transactions on Automatic Control},
doi = {10.1109/TAC.2013.2254619}
}


@inproceedings{Zhang_2016,
author = {Zhang, Hongyi and Reddi, Sashank J. and Sra, Suvrit},
title = {Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds},
year = {2016},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4599–4607},
numpages = {9},
}

@article{Sato_2019,
author = {Sato, Hiroyuki and Kasai, Hiroyuki and Mishra, Bamdev},
title = {Riemannian Stochastic Variance Reduced Gradient Algorithm with Retraction and Vector Transport},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {2},
pages = {1444-1472},
year = {2019},
}

@article{Wen_2023,
author = {TianYou Li and Fan Chen and Huajie Chen and and Zaiwen Wen},
title = {Provable Convergence of Variational Monte Carlo Methods},
year = {2023},
journal={arxiv/2303.10599}
}

@inproceedings{Tripuraneni2018AveragingSG,
  title={Averaging Stochastic Gradient Descent on Riemannian Manifolds},
  author={Nilesh Tripuraneni and Nicolas Flammarion and Francis R. Bach and Michael I. Jordan},
  booktitle={Annual Conference Computational Learning Theory},
  year={2018}
}


@Article{HanLiLinEtAl2019,
  author  = {Han, Jiequn and Li, Yingzhou and Lin, Lin and Lu, Jianfeng and Zhang, Jiefu and Zhang, Linfeng},
  title   = {Universal approximation of symmetric and anti-symmetric functions},
  journal = {arXiv:1912.01765},
  year    = {2019},
}


@TechReport{Hutter2020,
  author =       "Marcus Hutter",
  title =        "On Representing (Anti)Symmetric Functions",
  institution =  "DeepMind",
  address =      "London, UK",
  number =       "arXiv:2007.15298",
  _month =        jun,
  year =         "2020",
  bibtex =       "http://www.hutter1.net/official/bib.htm#asymnn",
  url =          "https://arxiv.org/abs/2007.15298",
  pdf =          "http://www.hutter1.net/publ/asymnn.pdf",
  project =      "http://www.hutter1.net/official/projects.htm#nn",
}

@Article{SannaiTakaiCordonnier2019,
  author  = {Sannai, Akiyoshi and Takai, Yuuki and Cordonnier, Matthieu},
  title   = {Universal approximations of permutation invariant/equivariant functions by deep neural networks},
  journal = {arXiv:1903.01939},
  year    = {2019},
}


@Article{KerivenPeyre2019,
  author  = {Keriven, Nicolas and Peyr{\'e}, Gabriel},
  title   = {Universal invariant and equivariant graph neural networks},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2019},
  volume  = {32},
  pages   = {7092--7101},
}

@misc{Nilin2022,
  doi = {10.48550/ARXIV.2205.12250},
  
  url = {https://arxiv.org/abs/2205.12250},
  
  author = {Abrahamsen, Nilin and Lin, Lin},
  
  keywords = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Quantum Physics (quant-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Taming the sign problem of explicitly antisymmetrized neural networks via rough activation functions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Sorella98,
  title = {Green Function Monte Carlo with Stochastic Reconfiguration},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. Lett.},
  volume = {80},
  issue = {20},
  pages = {4558--4561},
  numpages = {0},
  year = {1998},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.80.4558},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.80.4558}
}

@article{PhysRevB.64.024512,
  title = {Generalized Lanczos algorithm for variational quantum Monte Carlo},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. B},
  volume = {64},
  issue = {2},
  pages = {024512},
  numpages = {16},
  year = {2001},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.64.024512},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.64.024512}
}

@article{Sorella2001,
  title = {Generalized Lanczos algorithm for variational quantum Monte Carlo},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. B},
  volume = {64},
  issue = {2},
  pages = {024512},
  numpages = {16},
  year = {2001},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.64.024512},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.64.024512}
}
@article{Nightingale2001,
  title = {Optimization of Ground- and Excited-State Wave Functions and van der Waals Clusters},
  author = {Nightingale, M. P. and Melik-Alaverdian, Vilen},
  journal = {Phys. Rev. Lett.},
  volume = {87},
  issue = {4},
  pages = {043401},
  numpages = {4},
  year = {2001},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.87.043401},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.87.043401}
}
@article{toulouse2007,
  title={Optimization of quantum Monte Carlo wave functions by energy minimization},
  author={Toulouse, Julien and Umrigar, Cyrus J},
  journal={The Journal of chemical physics},
  volume={126},
  number={8},
  pages={084102},
  year={2007},
  publisher={American Institute of Physics}
}

@article{Sandvik2007,
  title = {Variational Quantum Monte Carlo Simulations with Tensor-Network States},
  author = {Sandvik, A. W. and Vidal, G.},
  journal = {Phys. Rev. Lett.},
  volume = {99},
  issue = {22},
  pages = {220602},
  numpages = {4},
  year = {2007},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.99.220602},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.220602}
}

@article{Neuscamman2012,
  title = {Optimizing large parameter sets in variational quantum Monte Carlo},
  author = {Neuscamman, Eric and Umrigar, C. J. and Chan, Garnet Kin-Lic},
  journal = {Phys. Rev. B},
  volume = {85},
  issue = {4},
  pages = {045103},
  numpages = {6},
  year = {2012},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.85.045103},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.85.045103}
}

@article{Cai2018,
  title = {Approximating quantum many-body wave functions using artificial neural networks},
  author = {Cai, Zi and Liu, Jinguo},
  journal = {Phys. Rev. B},
  volume = {97},
  issue = {3},
  pages = {035116},
  numpages = {8},
  year = {2018},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.97.035116},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.97.035116}
}

@article{Carleo2019,
title = {NetKet: A machine learning toolkit for many-body quantum systems},
journal = {SoftwareX},
volume = {10},
pages = {100311},
year = {2019},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2019.100311},
url = {https://www.sciencedirect.com/science/article/pii/S2352711019300974},
author = {Giuseppe Carleo and Kenny Choo and Damian Hofmann and James E.T. Smith and Tom Westerhout and Fabien Alet and Emily J. Davis and Stavros Efthymiou and Ivan Glasser and Sheng-Hsuan Lin and Marta Mauri and Guglielmo Mazzola and Christian B. Mendl and Evert {van Nieuwenburg} and Ossian O’Reilly and Hugo Théveniaut and Giacomo Torlai and Filippo Vicentini and Alexander Wietek},
}

@Article{Otis2019,
author ="Otis, Leon and Neuscamman, Eric",
title  ="Complementary first and second derivative methods for ansatz optimization in variational Monte Carlo",
journal  ="Phys. Chem. Chem. Phys.",
year  ="2019",
volume  ="21",
issue  ="27",
pages  ="14491-14510",
publisher  ="The Royal Society of Chemistry",
doi  ="10.1039/C9CP02269D",
url  ="http://dx.doi.org/10.1039/C9CP02269D",
}


@software{ferminet_github,
  author = {James S. Spencer, David Pfau and FermiNet Contributors},
  title = {{FermiNet} (forked from commit fdaca31)},
  url = {http://github.com/deepmind/ferminet},
  year = {2023},
    commit ={fdaca31},
}

@software{ferminet_github_fork,
  author = {James S. Spencer, David Pfau and FermiNet Contributors},
  title = {{fork of FermiNet} (forked from commit fdaca31)},
  url = {https://github.com/nilin/fork_of_ferminet/tree/sample_orbitals},
  year = {2023},
    commit ={fdaca31},
}


@article{barrett2022autoregressive,
  title={Autoregressive neural-network wavefunctions for ab initio quantum chemistry},
  author={Barrett, Thomas D and Malyshev, Aleksei and Lvovsky, AI},
  journal={Nature Machine Intelligence},
  volume={4},
  number={4},
  pages={351--358},
  year={2022},
  publisher={Nature Publishing Group UK London}
}


%%%%%% references 2 below. remove duplicates %%%%%%

@article{sorella_sr,
  title = {Generalized Lanczos algorithm for variational quantum Monte Carlo},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. B},
  volume = {64},
  issue = {2},
  pages = {024512},
  numpages = {16},
  year = {2001},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.64.024512},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.64.024512}
}
@article{nightingale2001optimization,
  title={Optimization of ground-and excited-state wave functions and van der Waals clusters},
  author={Nightingale, MP and Melik-Alaverdian, Vilen},
  journal={Physical review letters},
  volume={87},
  number={4},
  pages={043401},
  year={2001},
  publisher={APS}
}
@article{kato1957eigenfunctions,
  title={On the eigenfunctions of many-particle systems in quantum mechanics},
  author={Kato, Tosio},
  journal={Communications on Pure and Applied Mathematics},
  volume={10},
  number={2},
  pages={151--177},
  year={1957},
  publisher={Wiley Online Library}
}
@misc{minsr,
      title={Efficient optimization of deep neural quantum states toward machine precision}, 
      author={Ao Chen and Markus Heyl},
      year={2023},
      eprint={2302.01941},
      archivePrefix={arXiv},
      primaryClass={cond-mat.dis-nn}
}
@article{parikh2014proximal,
  title={Proximal algorithms},
  author={Parikh, Neal and Boyd, Stephen and others},
  journal={Foundations and trends{\textregistered} in Optimization},
  volume={1},
  number={3},
  pages={127--239},
  year={2014},
  publisher={Now Publishers, Inc.}
}
@book{becca2017quantum,
  title={Quantum Monte Carlo approaches for correlated systems},
  author={Becca, Federico and Sorella, Sandro},
  year={2017},
  publisher={Cambridge University Press}
}
@article{toulouse2007optimization,
  title={Optimization of quantum Monte Carlo wave functions by energy minimization},
  author={Toulouse, Julien and Umrigar, Cyrus J},
  journal={The Journal of chemical physics},
  volume={126},
  number={8},
  year={2007},
  publisher={AIP Publishing}
}
@article{rende2023simple,
  title={A simple linear algebra identity to optimize Large-Scale Neural Network Quantum States},
  author={Rende, Riccardo and Viteritti, Luciano Loris and Bardone, Lorenzo and Becca, Federico and Goldt, Sebastian},
  journal={arXiv preprint arXiv:2310.05715},
  year={2023}
}
@article{yang2022sketch,
  title={Sketch-based empirical natural gradient methods for deep learning},
  author={Yang, Minghan and Xu, Dong and Wen, Zaiwen and Chen, Mengyun and Xu, Pengxiang},
  journal={Journal of Scientific Computing},
  volume={92},
  number={3},
  pages={94},
  year={2022},
  publisher={Springer}
}
@article{ren2019efficient,
  title={Efficient subsampled Gauss-Newton and natural gradient methods for training neural networks},
  author={Ren, Yi and Goldfarb, Donald},
  journal={arXiv preprint arXiv:1906.02353},
  year={2019}
}
@inproceedings{mu2022hylo,
  title={HyLo: a hybrid low-rank natural gradient descent method},
  author={Mu, Baorun and Soori, Saeed and Can, Bugra and G{\"u}rb{\"u}zbalaban, Mert and Dehnavi, Maryam Mehri},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2022},
  organization={IEEE}
}
@article{carleo2017solving,
  title={Solving the quantum many-body problem with artificial neural networks},
  author={Carleo, Giuseppe and Troyer, Matthias},
  journal={Science},
  volume={355},
  number={6325},
  pages={602--606},
  year={2017},
  publisher={American Association for the Advancement of Science}
}
@article{carleo2019netket,
  title={NetKet: A machine learning toolkit for many-body quantum systems},
  author={Carleo, Giuseppe and Choo, Kenny and Hofmann, Damian and Smith, James ET and Westerhout, Tom and Alet, Fabien and Davis, Emily J and Efthymiou, Stavros and Glasser, Ivan and Lin, Sheng-Hsuan and others},
  journal={SoftwareX},
  volume={10},
  pages={100311},
  year={2019},
  publisher={Elsevier}
}
@article{foulkes2001quantum,
  title={Quantum Monte Carlo simulations of solids},
  author={Foulkes, WMC and Mitas, Lubos and Needs, RJ and Rajagopal, Guna},
  journal={Reviews of Modern Physics},
  volume={73},
  number={1},
  pages={33},
  year={2001},
  publisher={APS}
}
@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}
@article{pfau2020ab,
  title={Ab initio solution of the many-electron Schr{\"o}dinger equation with deep neural networks},
  author={Pfau, David and Spencer, James S and Matthews, Alexander GDG and Foulkes, W Matthew C},
  journal={Physical Review Research},
  volume={2},
  number={3},
  pages={033429},
  year={2020},
  publisher={APS}
}
@article{hermann2020deep,
  title={Deep-neural-network solution of the electronic Schr{\"o}dinger equation},
  author={Hermann, Jan and Sch{\"a}tzle, Zeno and No{\'e}, Frank},
  journal={Nature Chemistry},
  volume={12},
  number={10},
  pages={891--897},
  year={2020},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}
@article{von2022self,
  title={A self-attention ansatz for ab-initio quantum chemistry},
  author={von Glehn, Ingrid and Spencer, James S and Pfau, David},
  journal={arXiv preprint arXiv:2211.13672},
  year={2022}
}


@software{kfac-jax2022github,
  author = {Aleksandar Botev and James Martens},
  title = {{KFAC-JAX}},
  url = {https://github.com/google-deepmind/kfac-jax},
  version = {0.0.2},
  year = {2022},
}
@article{Casella2023,
  title = {Discovering Quantum Phase Transitions with Fermionic Neural Networks},
  author = {Cassella, Gino and Sutterud, Halvard and Azadi, Sam and Drummond, N. D. and Pfau, David and Spencer, James S. and Foulkes, W. M. C.},
  journal = {Phys. Rev. Lett.},
  volume = {130},
  issue = {3},
  pages = {036401},
  numpages = {6},
  year = {2023},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.130.036401},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.130.036401}
}

@misc{kim2023neuralnetwork,
      title={Neural-network quantum states for ultra-cold Fermi gases}, 
      author={Jane Kim and Gabriel Pescia and Bryce Fore and Jannes Nys and Giuseppe Carleo and Stefano Gandolfi and Morten Hjorth-Jensen and Alessandro Lovato},
      year={2023},
      eprint={2305.08831},
      archivePrefix={arXiv},
      primaryClass={cond-mat.quant-gas}
}

@misc{lou2023neural,
      title={Neural Wave Functions for Superfluids}, 
      author={Wan Tong Lou and Halvard Sutterud and Gino Cassella and W. M. C. Foulkes and Johannes Knolle and David Pfau and James S. Spencer},
      year={2023},
      eprint={2305.06989},
      archivePrefix={arXiv},
      primaryClass={cond-mat.quant-gas}
}

@misc{pescia2023messagepassing,
      title={Message-Passing Neural Quantum States for the Homogeneous Electron Gas}, 
      author={Gabriel Pescia and Jannes Nys and Jane Kim and Alessandro Lovato and Giuseppe Carleo},
      year={2023},
      eprint={2305.07240},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@article{li2022ab,
  title={Ab initio calculation of real solids via neural network ansatz},
  author={Li, Xiang and Li, Zhe and Chen, Ji},
  journal={Nature Communications},
  volume={13},
  number={1},
  pages={7895},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
%%%%%%%%%

@article{HUNG2009163,
title = {Accurate simulations of metals at the mesoscale: Explicit treatment of 1 million atoms with quantum mechanics},
journal = {Chemical Physics Letters},
volume = {475},
number = {4},
pages = {163-170},
year = {2009},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2009.04.059},
url = {https://www.sciencedirect.com/science/article/pii/S0009261409005041},
author = {Linda Hung and Emily A. Carter},
abstract = {We present a fully linear scaling (at most O(N·log(N))) and parallel algorithm for orbital-free density functional theory (OFDFT), for the first time exhibiting linear scaling in all terms (electronic and ionic). OFDFT solves directly for the electron density; consequently, the electron kinetic energy is determined using density functionals, which must be nonlocal to provide sufficient accuracy. The systematic elimination of bottlenecks within OFDFT renders the entire algorithm quasilinear scaling for all system sizes (no crossover point). Now an unprecedented number of atoms (∼1million) can be treated explicitly quantum mechanically within OFDFT with a modest number of processors, opening up the door to treatment of ever more complex features in materials (precipitates, dislocations, etc.) without introducing empirical assumptions.}
}%

@misc{ma2019inefficiency,
      title={Inefficiency of K-FAC for Large Batch Size Training}, 
      author={Linjian Ma and Gabe Montague and Jiayu Ye and Zhewei Yao and Amir Gholami and Kurt Keutzer and Michael W. Mahoney},
      year={2019},
      eprint={1903.06237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{PhysRevB.85.045103,
  title = {Optimizing large parameter sets in variational quantum Monte Carlo},
  author = {Neuscamman, Eric and Umrigar, C. J. and Chan, Garnet Kin-Lic},
  journal = {Phys. Rev. B},
  volume = {85},
  issue = {4},
  pages = {045103},
  numpages = {6},
  year = {2012},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.85.045103},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.85.045103}
}

@misc{gao2023generalizing,
      title={Generalizing Neural Wave Functions}, 
      author={Nicholas Gao and Stephan Günnemann},
      year={2023},
      eprint={2302.04168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{SorCap00,
  title = {Green function Monte Carlo with stochastic reconfiguration: An effective remedy for the sign problem},
  author = {Sorella, Sandro and Capriotti, Luca},
  journal = {Phys. Rev. B},
  volume = {61},
  issue = {4},
  pages = {2599--2612},
  numpages = {0},
  year = {2000},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.61.2599},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.61.2599}
}
@article{needell2014paved,
  title={Paved with good intentions: analysis of a randomized block Kaczmarz method},
  author={Needell, Deanna and Tropp, Joel A},
  journal={Linear Algebra and its Applications},
  volume={441},
  pages={199--221},
  year={2014},
  publisher={Elsevier}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}
@software{vmcnet2021github,
  author = {Jeffmin Lin and Gil Goldshlager and Lin Lin},
  title = {{VMCNet}: Flexible, general-purpose {VMC} framework, built on {JAX}},
  url = {http://github.com/jeffminlin/vmcnet},
  version = {0.1.0},
  year = {2021},
}
@article{strohmer2009randomized,
  title={A randomized Kaczmarz algorithm with exponential convergence},
  author={Strohmer, Thomas and Vershynin, Roman},
  journal={Journal of Fourier Analysis and Applications},
  volume={15},
  number={2},
  pages={262--278},
  year={2009},
  publisher={Springer}
}
@article{zouzias2013randomized,
  title={Randomized extended Kaczmarz for solving least squares},
  author={Zouzias, Anastasios and Freris, Nikolaos M},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={34},
  number={2},
  pages={773--793},
  year={2013},
  publisher={SIAM}
}
@article{schatzle2023deepqmc,
  title={DeepQMC: An open-source software suite for variational optimization of deep-learning molecular wave functions},
  author={Sch{\"a}tzle, Zeno and Szab{\'o}, PB and Mezera, Mat{\u{e}}j and Hermann, Jan and No{\'e}, Frank},
  journal={The Journal of Chemical Physics},
  volume={159},
  number={9},
  year={2023},
  publisher={AIP Publishing}
}
@inproceedings{benzing2022gradient,
  title={Gradient descent on neurons and its link to approximate second-order optimization},
  author={Benzing, Frederik},
  booktitle={International Conference on Machine Learning},
  pages={1817--1853},
  year={2022},
  organization={PMLR}
}
@article{umrigar2007,
  title = {Alleviation of the Fermion-Sign Problem by Optimization of Many-Body Wave Functions},
  author = {Umrigar, C. J. and Toulouse, Julien and Filippi, Claudia and Sorella, S. and Hennig, R. G.},
  journal = {Phys. Rev. Lett.},
  volume = {98},
  issue = {11},
  pages = {110201},
  numpages = {4},
  year = {2007},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.98.110201},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.98.110201}
}
@article{neuscamman2012optimizing,
  title={Optimizing large parameter sets in variational quantum Monte Carlo},
  author={Neuscamman, Eric and Umrigar, CJ and Chan, Garnet Kin-Lic},
  journal={Physical Review B},
  volume={85},
  number={4},
  pages={045103},
  year={2012},
  publisher={APS}
}
@article{zhao2017blocked,
  title={A blocked linear method for optimizing large parameter sets in variational Monte Carlo},
  author={Zhao, Luning and Neuscamman, Eric},
  journal={Journal of chemical theory and computation},
  volume={13},
  number={6},
  pages={2604--2611},
  year={2017},
  publisher={ACS Publications}
}
@article{sabzevari2020accelerated,
  title={An accelerated linear method for optimizing non-linear wavefunctions in variational Monte Carlo},
  author={Sabzevari, Iliya and Mahajan, Ankit and Sharma, Sandeep},
  journal={The Journal of chemical physics},
  volume={152},
  number={2},
  year={2020},
  publisher={AIP Publishing}
}
@article{sorella2007weak,
  title={Weak binding between two aromatic rings: Feeling the van der Waals attraction by quantum Monte Carlo methods},
  author={Sorella, Sandro and Casula, Michele and Rocca, Dario},
  journal={The Journal of chemical physics},
  volume={127},
  number={1},
  year={2007},
  publisher={AIP Publishing}
}
@article{lin2023explicitly,
  title={Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation},
  author={Lin, Jeffmin and Goldshlager, Gil and Lin, Lin},
  journal={Journal of Computational Physics},
  volume={474},
  pages={111765},
  year={2023},
  publisher={Elsevier}
}
@article{park2020geometry,
  title={Geometry of learning neural quantum states},
  author={Park, Chae-Yeun and Kastoryano, Michael J},
  journal={Physical Review Research},
  volume={2},
  number={2},
  pages={023232},
  year={2020},
  publisher={APS}
}



@misc{yin2019understanding,
      title={Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets}, 
      author={Penghang Yin and Jiancheng Lyu and Shuai Zhang and Stanley Osher and Yingyong Qi and Jack Xin},
      year={2019},
      eprint={1903.05662},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}