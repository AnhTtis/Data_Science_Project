
%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

\newif\iffinal
\finaltrue
\input{preamble.tex}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2023}
%\usepackage{icmlcustompreprint}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{itdef}[theorem]{Convention}
\newtheorem{problem}{Problem}[section]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
%\theoremstyle{remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{conjecture}[theorem]{Conjecture}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2022}



%
%%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%
%\documentclass{article}
%
%% Recommended, but optional, packages for figures and better typesetting:
%\usepackage{microtype}
%\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage{booktabs} % for professional tables
%
%% hyperref makes hyperlinks in the resulting PDF.
%% If your build breaks (sometimes temporarily if a hyperlink spans a page)
%% please comment out the following usepackage line and replace
%% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
%\usepackage{hyperref}
%
%
%% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
%
%% Use the following line for the initial blind version submitted for review:
%\usepackage{icmlcustompreprint}
%
%% If accepted, instead use the following line for the camera-ready submission:
%% \usepackage[accepted]{icml2023}
%
%% For theorems and such
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{mathtools}
%\usepackage{amsthm}
%
%% if you use cleveref..
%\usepackage[capitalize,noabbrev]{cleveref}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{corollary}[theorem]{Corollary}
%\theoremstyle{definition}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{assumption}[theorem]{Assumption}
%\theoremstyle{remark}
%\newtheorem{remark}[theorem]{Remark}
%
%% Todonotes is useful during development; simply uncomment the next line
%%    and comment out the line below the next line to turn off comments
%%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}
%
%
%% The \icmltitle you define below is probably too long as a header.
%% Therefore, a short form for the running title is supplied here:
%
%


\begin{document}

\twocolumn[
\icmltitle{Convergence of stochastic gradient descent  on parameterized sphere with applications to variational Monte Carlo simulation}
%\icmltitle{Convergence of variational Monte Carlo simulation and SGD on implicit subsets of the sphere}
%\icmltitle{Convergence of stochastic gradient descent on projective space with applications to variational Monte Carlo simulation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nilin Abrahamsen}{bm}
\icmlauthor{Zhiyan Ding}{bm}
\icmlauthor{Gil Goldshlager}{bm}
\icmlauthor{Lin Lin}{bm,lbl}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{bm}{Department of Mathematics, University of California, Berkeley, CA 94720, USA}
\icmlaffiliation{lbl}{Applied Mathematics and Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA}

%\icmlaffiliation{bm}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

%\icmlcorrespondingauthor{Nilin Abrahamsen}{nilin@berkeley.edu}

% for arxiv purpose if needed.
\icmlcorrespondingauthor{Lin Lin}{linlin@math.berkeley.edu}
% add yourself here

%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\printAffiliationsAndNotice{}

\begin{abstract}
%We analyze the stochastic gradient descent algorithm for scale-invariant loss functions in a parameterized function space. This problem is equivalent to a many-to-one parameterization of the sphere of normalized functions. We prove that the expected loss converges in a scale-invariant supervised learning setting as well as in the scale-invariant variational Monte Carlo setting employed in quantum chemistry.
We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics. 
\end{abstract}


% \ifshowchanges
% \iffinal\else
% \input{changes.tex}
% \fi
% \fi

\section{Introduction}

Stochastic gradient descent (SGD) is one of the key techniques behind the success of machine learning based on deep neural networks. 
Although the SGD algorithm is commonly formulated in the Euclidean space, SGD type algorithms can be formulated in a much broader context, such as optimizing objective functions defined on Riemannian manifolds \cite{bonnabel_stochastic_2013,Zhang_2016,tripuraneni_averaging_2018,Sato_2019}. In this context the gradient step can be followed by a projection step, or it can be defined using an exponential map, such that the parameter remains on the manifold. 

We focus on the case when the manifold is a sphere. We can then apply the general Riemannian optimization techniques of projections or exponential mappings, or the problem  can be reformulated as the minimization of a scale-invariant loss \cite{Saliman_2016} over an unconstrained space. In either case, the element to be optimized over the sphere is required to be represented explicitly in memory and can be considered as a high-dimensional parameter. 
 
This paper, on the other hand, considers a setting where the element of the high-dimensional sphere is not stored explicitly in memory. The most significant motivation for this setting is the optimization of wavefunctions in quantum physics, both in the context of supervised learning and unsupervised learning (the latter is more commonly known in physics as the variational  Monte Carlo method, see e.g. \citet{Becca2017}). Usually in this context the parameter $\theta$ is stored explicitly in memory and is unconstrained, but the wavefunction $f_\theta$ belongs to the unit sphere of an infinite-dimensional function space, so we can neither enumerate all of its entries nor directly evaluate functionals of $f_\theta$ such as $\|f_\theta\|$. Instead, we assume only that the algorithm has access to samples $(X_i,F_\theta(X_i))$, where $X_i$ are random samples from the domain of $f$ and $F_\theta$ consists of $f$ and its low-order derivatives with respect to $\theta$ and $x$\footnote{For example in quantum chemistry we need first-order derivatives with respect to $\theta$ and second-order derivatives with respect to $x$.}.

% GG: included this in Problem Setup 1.1
% We consider the problem of optimizing a loss function $\L$ over the unit sphere in a parameterized function space. This is equivalent with an unconstrained optimization of a scale-invariant loss $\L(f/\|f\|)$ over functions $f_\theta$ parameterized by the unconstrained parameter $\theta$. }

% This application motivates us to consider the problem of optimizing a loss function $\L$ over the unit sphere in a parameterized function space. This is equivalent with an unconstrained optimization of a scale-invariant loss $\L(f/\|f\|)$ over functions $f_\theta$ parameterized by the unconstrained parameter $\theta$. 
%This paper considers the convergence of SGD type algorithms on a sphere of vectors $f/\|f\|$ when the dimension of $f$ is too large to be stored in memory. We represent a point on the sphere non-uniquely by a non-normalized parameterized vector $f_\theta$ which can be accessed through random samples of its coordinates $f_\theta(x)$. 

% To the best of our knowledge, previous works analyzing stochastic gradient descent on manifolds require that the stochastic gradient step is moved from the tangent space to the manifold using an exponentiation mapping or a projection step. This operation is natural when $f$ is stored in memory as a vector but cannot be implemented when the vector is parameterized as $f_\theta$ and given our random access model.
% (GG: the above is alluded to a couple paragraphs up, and also stated directly in section 1.3.3.


\ZD{Reorganize this section}

\subsection{Problem setup}

 

Let $\H$ be a function space with an inner product $\ol{f}{g}$ and norm $\|f\|^2=\ol{f}{f}$. Specifically, we consider the space $\H=L^2(\Omega,\rho)$ of real-valued square integrable functions (the complex-valued case can be treated analogously) on $\Omega$ with respect to a positive measure $\rho$.
The neural network maps a parameter $\theta\in\RR^d$ to a function $f_\theta\in\H$.

We consider the problem of optimizing a loss function $\L(f_\theta)$ over the unit sphere of normalized functions. In this setting we cannot directly project $f_\theta$ to the sphere of normalized functions since we do not know a parameter $\tilde\theta$ such that $f_{\tilde\theta}=f_\theta/\|f_\theta\|$. We can solve this problem by introducing another degree of freedom and parameterizing the function space as $(\lambda,\theta)\mapsto\lambda f_\theta$, but even so the accurate estimation of the normalization factor $\lambda$ is costly in general.

%An alternative perspective is to consider the quotient space $\H/\RR_+$, 
% take another view: Let $\H'$ be the nonzero elements of $\H$ and identify the sphere $\tilde\H$ with 
%whose elements are the rays
%\begin{equation}
%    [f]=\{\lambda f\:|\:\lambda>0\}.
%\end{equation}

We instead transform the optimization problem over the sphere of normalized functions into one where $f_\theta$ is unconstrained in $\H$. A loss function on the sphere then corresponds to a \emph{scale-invariant} loss $\L:\H\to\RR$, meaning that $\L(\lambda f)=\L(f)$ for any $\lambda>0$. This formulation is distinct from previous works~\cite{Saliman_2016,kodryan_training_2022}, which  studied loss functions that are scale-invariant in the \emph{parameter} (see \cref{fig:vf}).
%\vspace -.5cm
%
%
\begin{figure}[h]
\[
%\arraycolsep=1.4pt
\def\arraystretch{2.2}
%
\begin{array}{ccccc}
&&\v\in\RR^d&\xrightarrow[]{\text{scale-invariant }\L} &\L(\v)\\
\hline
\multicolumn{1}{|l}{\theta\in\RR^d}
&\xrightarrow[]{}&f_\theta\in\Lt&\xrightarrow[]{\text{scale-invariant }\L} &
\multicolumn{1}{l|}{L(\theta)=\L(f_\theta)}\\
\hline
\end{array}
\]
\caption{The typical setting for scale-invariant learning (top) vs our setting (bottom).}
\label{fig:vf}
\end{figure}


\subsubsection{Supervised learning}\label{sec:super_l}
We first consider the problem of minimizing the distance from the line $\{\lambda f|\lambda>0\}$ to a target function $g\in\H$. That is, 
\begin{equation}\label{supervisedC}
\mathcal{L}(f)=\min_{\lambda>0}\left\|\lambda f-g\right\|^2\,.
\end{equation}
This loss function is scale-invariant by construction and has a closed-form expression
\[
\mathcal{L}(f)=\|g\|^2-\left(\ol{g}{f}/\|f\|\right)^2\textbf{1}_{\ol{g}{f}>0}\,.
\]
Minimizing $\mathcal{L}(f_\theta)$ is therefore equivalent to minimizing $-\ol{g}{f_\theta}/\|f_\theta\|$. 
\begin{problem}[Supervised learning]\label{pro:slos}
Given a target function $g\in\H$, access to samples $X_i\sim\rho$, and the ability to evaluate $g$, $f_\theta$, and $\partial f_\theta / \partial \theta$ at a point $X_i$,
%and a parameterization $\Phi:\theta\in\RR^d\mapsto f_\theta\in\H$, 
minimize
\begin{equation}\label{eqn:Ltheta}
L(\theta)=-\frac{\ol{g}{f_\theta}}{\|f_\theta\|}\,
\end{equation}
over parameter vectors $\theta\in\mathbb{R}^d$.
\end{problem}
The gradient of this loss with respect to $\theta$ can be derived from the literature on scale-invariant losses using the chain rule, but simply plugging in an approximate value of $\|f_\theta\|$ to the resulting formula yields a biased estimator. In fact, the SGD algorithm with this biased estimator does not have theoretical convergence guarantees, and numerical results demonstrate that such an SGD algorithm fails to reach convergence in practice (see \cref{fig:balance}). 


\subsubsection{Unsupervised variational Monte Carlo simulation}\label{sec:VMC}

%\LL{ some backgrounds in VMC and recent neural network based literature. @Gil can add some from the previous paper} 

A typical goal in quantum physics and quantum chemistry is to find the function that minimizes the \emph{energy}, a scale-invariant functional of the form $\L:\wf\to\ol{\wf}{H\wf}/\braket{\wf,\wf}$. The function $f$ is known in this context as the wave function and is usually denoted by $\psi$. The Hermitian operator $H$ is known as the \emph{Hamiltonian} and the minimizer $\wf_0$ is known as the \emph{ground state}. We consider the most common case when $H$ is a real symmetric operator, in which case $\wf_0$ can be taken to be real so that it suffices to minimize over only real $\wf$.

One particular approach to solving this minimization problem is known as the \emph{variational Monte Carlo} (VMC) algorithm \cite{Foulkes2001, Toulouse2016, Becca2017}. In the context of VMC simulations
 we take $\rho$ as the Lebesgue measure and  $\Omega=\mathbb{R}^{n_p \cdot D}$, where $n_p$ is the number of particles and $D$ is the number of spatial dimensions. Let $p_\wf$ be the probability density given by
\begin{equation}p_\wf(x)=|\wf(x)|^2/\|\wf\|^2\label{pdef}\,.\end{equation}
%where $\|\cdot\|$ is the $L^2$-norm in the Lebesgue measure. 
In practice $\|\wf\|$ is unknown, and samples $X \sim p_\wf$ are generated using Markov chain Monte Carlo (MCMC); in this work we assume these samples are exact and independent.

The energy of $\wf$ can then be expressed as an expectation with respect to $p_\wf$:
$
\L(\wf)=\EE_{X\sim p_{\wf}}\localE_\wf(X),
$
where
\begin{equation}\label{eqn:deflocalE}
\localE_\wf(x)=\frac{(H\wf)(x)}{\wf(x)}.
\end{equation}
is called the \emph{local energy}.
\begin{problem}[VMC]\label{vmcprob}
Minimize
    \begin{equation}\label{eqn:VMC_L}
        L(\theta)=\EE_{X\sim p_\theta}\localE_\theta(X),
        %L(\theta)=\int_{\Omega}\localE_\theta(x)p_\theta(x)dx
    \end{equation}
    over parameter vectors $\theta\in\mathbb{R}^d$, where $\localE_\theta=H\wf_\theta/\wf_\theta$, and $p_{\theta}(x)=|\wf_{\theta}(x)|^2/\|\wf_\theta\|^2$. We assume query access to samples from $X_i\sim p_\theta$ and the ability to efficiently evaluate $f_\theta,Hf_\theta$, and $\partial f_\theta / \partial \theta$ at a point $X_i$.
\end{problem}

% In the VMC setting, $\Omega$ is the classical Eulidean space, $\rho$ is the Lebesgue measure, $\mathcal{H}$ is the space of classical $L^2$ functions.

\subsection{Contribution}\label{sec:ct}


\begin{enumerate}

% \item  To obtain an unbiased gradient estimator for the supervised learning problem (Problem \ref{pro:slos}) would require computing the exact norm of $\|f_\theta\|$, and we do not have access to this quantity. To overcome this difficulty 
% we provide an estimator $G$ which is \emph{directionally unbiased}, meaning that $\EE[G]$ is a positive multiple of $\nabla_\theta L(\theta)$.
% We prove that the convergence rate of the SGD algorithm with this gradient estimator (Algorithm \ref{alg:GE}) matches that of classical SGD (see Corollary \ref{cor:super}).

\item  We provide a new algorithm for \cref{pro:slos} which can effectively optimize $f$ in the absence of the access to the exact norm $\|f_\theta\|$. The algorithm is based on a new estimator $G$ for the loss gradient, which is \emph{directionally unbiased} in that $\EE[G]$ is a positive multiple of $\nabla_\theta L(\theta)$. We prove that the convergence rate of the SGD algorithm with this gradient estimator (Algorithm \ref{alg:GE}) matches that of classical SGD (see Corollary \ref{cor:super}).

\item We prove for the first time that the convergence rate of SGD (Algorithm \ref{alg:VMC}) for VMC simulations (Problem \ref{vmcprob}) matches that of classical SGD (see Corollary \ref{cor:VMC}) under proper assumptions. 

% Despite the wide usage of VMC, to our knowledge, this is the first work 
% showing that the convergence rate of the SGD algorithm can match that of classical SGD 
% the convergence of such algorithms has not been presented in the literature. Under proper assumptions, we show that Algorithm \ref{alg:VMC} has the same convergence rate as classical SGD .


%introduces additional difficulties in this step. 
%After diving into the calculation, we successfully demonstrate the boundedness under proper assumptions. 
%We also numerically verify the validity of our assumption in the real training process (see Section \ref{sec:Exp}).\LL{ check} 

\item In both cases we provide new bounds on the Lipschitz constant of $\nabla L$ and the variance of the gradient estimator, which is a key step in proving the convergence.

\end{enumerate}


%The lack of access to the overall normalization factor introduces difficulties in the analysis of both cases. To resolve these difficulties, we introduce a technique to bound the ratio of normalization factors  under proper assumptions. This allows us to bound the Lipschitz constant of $\nabla L$ and the variance of the gradient estimator, which is a key step in proving the convergence of SGD type algorithms.
% 
% \begin{itemize}
%     \item We and prove the convergence analysis of our algorithm. These losses can be seen as a general framework for different optimization problems in the real application. 
%     \item We give the complexity proof of the VMC algorithm. VMC algorithm is a popular and useful algorithm in the quantum community while, to the best of our knowledge, the convergence of it has never been studied before. 
% \end{itemize}


\subsection{Background and related works}
% 
% The literature related to our results fall into three main categories: Scale-invariant loss functions, SGD on manifolds, and variational Monte Carlo with neural networks for quantum chemistry.

%\LL{ the information in this section needs to be consolidated.} 
%In this section, we give a review for the previous work. We emphasize that although the setting in our paper seems to be similar to some previous works, there are essential differences that matter a lot in the design of algorithm and theoretical result.



\subsubsection{Minimizing scale-invariant objectives}\label{re:msfd}

%A function $L(\v)$ of parameters $\v\in\RR^n$ is scale-invariant if $$
%Comparing our setting with the previous work, the closest one is the training of scale-invariance loss in parameter, meaning that $L(C\theta)=L(\theta)$ for any $C>0$. 

Using scale-invariance in the parameter to stabilize the training of SGD is a well-studied technique. Many structures and methods have been proposed along these lines, such as normalized NN~\cite{Ioffe_2015,Wu_2018,Ba_2016}, $\mathcal{G}$-SGD~\cite{meng_mathcalg-sgd_2021}, 
SGD+WD~\cite{van_2017,arora2018theoretical,li_robust_2022,Wan_2021}, and projected/normalized (S)GD~\cite{Saliman_2016,kodryan_training_2022}. 
In these previous works, the parameter $\v$ is stored explicitly and $\L$ is scale-invariant in the parameter $\v$, meaning $\L(\lambda\v)=\v$ for any $\lambda>0$. 

% and the training of $\v$ can be seen as a training process on the sphere.
%In this paper, we consider the infinite-dimensional analogue $\L(f)$ of $L(\v)$, where $f_\theta$ is parameterized by finite-dimensional weight vector $\theta$.
%This difference makes the gradient calculation and analysis of the algorithm much harder. 
% For $\gamma=1$ the gradient formula of \cite{Saliman_2016} evaluates to
% \begin{equation}\label{findim}
% \nabla_\v \ell(\v)=\frac{1}{\|\v\|}\nabla\tilde\ell(\bar\v)-\frac{\nabla\tilde\ell(\bar\v)\cdot \v}{\|\v\|^3}\v.
% \end{equation}
% \cref{rem:connect} shows how we can relate the gradient of \cref{pro:slos} to \cref{findim}.

Our setting differs from these works in that we re-parameterize $L(\theta)=\L(\v_\theta)$ (in our setting we refer to this $\v_\theta$ as $f_\theta$). %This small change introduces many extra difficulties. 
Using the chain rule, the gradient of $L$ can then be written as
\begin{align}\label{chainrule}
\nabla_\theta L(\theta)
&=[\nabla_\v \L](\v_\theta)\cdot \nabla_\theta\v_\theta,
\end{align}
where the dot product is along the dimension of $\v$
(see \cite{Saliman_2016} for an expression for $\nabla_\v\L$). Expanding \cref{chainrule} yields an expression with factors of $\|\v_\theta\|$, and the analogous quantity $\|f_\theta\|$ is not available in our setting.

%As already mentioned in Section \ref{sec:ct}, to numerically approximate $\nabla L(\theta)$, we need to exactly know the norm of $\v_\theta$. In the previous work, this norm is always available because it is the norm of the parameter $\v$. However, in our case, $\|\v_\theta\|$ is the norm of our neural network function and it is impossible to obtain an accurate estimate of it without going through the entire data set. Theoretically, this lack of information and the extra product term $\nabla_\theta\v$ also complicatesclever the analysis of the algorithm. 
%For example, if $l$ is the square loss, then $\nabla_\v \L$ is linear and Lipschitz. This property is no longer straightforward after replacing $\v$ by  $\v_\theta$, and showing the Lipschitz property of $L$ becomes a difficult task. 


\subsubsection{SGD on Riemannian manifolds}\label{re:mnf}
%Finally, it is worth mentioning that, after assuming the loss function is scale-invariant in the parameter $\v$, 
The training of a scale-invariant objective can be seen as a training process on the sphere, which is a special case of learning on Riemannian manifolds. The convergence of SGD on Riemannian manifolds has already been well-studied. \cite{bonnabel_stochastic_2013} shows the asymptotic  convergence of SGD on Riemannian manifolds such as the sphere. Further variations of the SGD algorithm on Riemannian manifolds have been proposed and studied, such as SVRG on Riemannian manifolds~ \cite{Zhang_2016}, \cite{Sato_2019} and ASGD on Riemannian manifolds~\cite{tripuraneni_averaging_2018}. To the best of our knowledge, all previous works ensure the parameter stays on the manifold by either taking stochastic gradient steps directly on the manifold or projecting them back onto it. This is in contrast to our setting in which we have a parameterization of $f_\theta\in\H$ and take stochastic gradient steps directly in the parameter space. 



\subsubsection{Supervised learning of quantum states} 
%\begin{itemize}\item 

Supervised learning has been used to study the power of neural networks for representing quantum states \cite{Cai2018, Carleo2019}. 
%This supervised learning of a quantum state corresponds directly to the problem of supervised learning on the implicitly parameterized sphere. 
Supervised learning is also used in the pretraining step of the FermiNet \cite{Pfau_2020} and related works \cite{Spencer_2020, Gerard2022, Glehn2022} in order to stabilize and accelerate subsequent VMC simulations.
% Although the pretraining procedure used in these works does not directly match our formulation of supervised learning on the implicitly parameterized sphere, but it is similar in spirit and our procedure could provide an alternative, more general form of pretraining for the FermiNet or other neural quantum state ansatzes.


%\item 

%\end{itemize}

\subsubsection{Optimization methods in variational Monte Carlo simulation}

% The success of VMC with neural quantum states has inspired several recent works that aim to study the theoretical and practical capacity of neural networks to represent quantum states and related function classes \cite{HanLiLinEtAl2019, SannaiTakaiCordonnier2019, KerivenPeyre2019, Hutter2020, Jeff_2021, Nilin2022}. 

Historically, the number of parameters used in VMC simulations was relatively small, and methods such as stochastic reconfiguration  \cite{Sorella98, Sorella2001} and the linear method \cite{Nightingale2001, toulouse2007} were preferred for parameter optimization. Recently, neural networks have been used to parameterize the wave function in an approach known as \emph{neural quantum states} \cite{Carleo2017}. Subsequent works have shown that VMC with neural quantum states can match or exceed state-of-the-art high accuracy quantum simulation methods on a wide variety of problems in quantum many-body physics and chemistry \cite{Nomura2017, Choo2018, Nagy2019, Luo2019, Han2019, Yang2020, Hermann2020, Pfau_2020, StokesRobledo2020, Choo2020, Glehn2022, Gerard2022, Hermann_review}. Due to the complexity and the large number of parameters in neural quantum states, first order methods are usually the only practical option for their optimization. In this context, our work explores the performance of the simplest first order stochastic optimization method for VMC simulations. 


%\begin{longcomment}
%\NA{brainstorming}
%\begin{itemize}
%\item making optimization on a manifold into an unconstrained one
%\item framing VMC as as SGD problem
%\end{itemize}
%\end{longcomment}
%\begin{temp}
%The main contribution of our work can be summarized in the following two points:
%\begin{itemize}
%    \item We propose a new algorithm (Algorithm \ref{alg:GE}) to train the scale-invariant loss \eqref{eqn:Ltheta} in Problem \ref{pro:slos} and prove the convergence analysis of our algorithm. These losses can be seen as a general framework for different optimization problems in the real application. 
%    \item We give the complexity proof of the VMC algorithm. VMC algorithm is a popular and useful algorithm in the quantum community while, to the best of our knowledge, the convergence of it has never been studied before. 
%\end{itemize}
%
%\NA{moved from related work:}
%
%\begin{temp}
%In contrast to the established literature which considers scale-invariance in the parameter space to stabilize training, we are instead interested in scale-invariance in an infinite-dimensional intermediate space which represents the wave function. In particular, we do not have direct access to $\|f\|$ as is required to compute the pullback $\tilde\L$ but will instead need to estimate it by sampling. 
%\end{temp}
%
%\NA{moved from related work:}
%We emphasize that, different from finite dimensional scale-invariant training, it is difficult to directly apply SGD in our setting to obtain an unbiased gradient estimator for $\nabla_\theta L(\theta)$. There are two reasons: 1. The denominator of $\delta\L_f$ depends on the norm of $\|f\|$. Generally speaking, we need to use a lot of data points to obtain an accurate approximation to it; 2. The inner production of $\delta\L_{f_\theta}$ and $\nabla_\theta f_\theta$ produces some nonlinear term, which might make the design of SGD difficult. To overcome these two difficulties, in Section \ref{sec:Duge}, we need to carefully design the algorithms and the gradient estimators for Problems \ref{pro:slos}, \ref{vmcprob}.
%
%
%
%We propose an algorithm for Problem \ref{pro:slos} in Section \ref{sec:pbgea}. This minimization problem can be seen as a general framework for different optimization problems and accelerate the training process:
%\begin{itemize}
%    \item Consider the following optimization problem:
%    \begin{equation}\label{super_l_C}
%    \min_{C\in\mathbb{R},\theta\in\mathbb{R}^d}L(C,\theta)=\|Cf_\theta-g\|^2\,.
%    \end{equation}
%    For fixed $\theta$, we have $\mathrm{argmin}_{C\in\mathbb{R}}L(C,\theta)=\|Cf_\theta-g\|^2=\ol{g}{f_\theta}/\|f_\theta\|^2$. Plugging this into $L(C,\theta)$, minimizing \eqref{super_l_C} is equivalent to minimizing \eqref{eqn:Ltheta}.
%
%    As will be shown in Sections \ref{sec:pbgea} and \ref{sec:alg_2_ana}. One advantage of directly minimizing \eqref{eqn:Ltheta} is that the whole process is scale-invariant in $f_\theta$. In particular, as shown in Section \ref{sec:alg_2_ana}, using Algorithm \ref{alg:GE}, we can use the same learning rate to obtain the same convergence rate if $f_\theta$ is rescaled to $cf_\theta$ with $c>0$. In real application, this is very useful. In some cases (see the second point below), $f_\theta$ might become very small during the training process, meaning that $|f_\theta|\ll |g|\sim O(1)$. This implies, to achieve a first-order stationary point, the number $C$ should be pretty large. On the other hand, because of the noise in SGD, we often choose smaller and smaller learning rates in the iteration to stabilize the training process. Taking these two facts into consideration, we need to run a large number of iterations to achieve the stationary point, which greatly increases the cost of the training.
%
%\end{itemize}
%
%\end{temp}

% \section{Sketch of algorithm and main results}

% %\NA{I think it would be good to let $\eta_m=1/\sqrt m$ and use Cauchy-Schwartz on the LHS of} \eqref{eqn:gradient_F_bound} 
% %\NA{to get $\EE[\operatorname{average}(|\nabla_\theta L|^4)]\le \log M/M$. I think the statement is cleaner if we don't specify a stopping time}\ZD{I agree.}



% We give variants of stochastic gradient descent (SGD) for Problems \ref{pro:slos} and \ref{vmcprob}. The SGD algorithm $\algsym$ can be stated as follows and depends on a problem-dependent distribution $P$. In the VMC setting we allow $P$ to depend on the state of the algorithm at step $m$:
% \begin{itemize}
%     \item (Initialization): Pick an initial parameter $\theta_0\in\RR^d$.
%     \item (Iterative updates): 
%     For each $m\in\NN_0$ $\algsym$ receives a sample $S_m\sim P$ and computes $G_m=\algsym(S_1,\ldots,S_m;\theta_0,\ldots,\theta_m)$ which represents an estimate of $\nabla L(\theta_m)$. $\algsym$ then defines
%     \[\theta_{m+1}=\theta_m-\eta_m G_m.\]
%     %Given $M>0$. For each $0\leq m\leq M-1$,
%     %$\theta_{m+1}=\theta_m-\eta_m G_m$,
%     %where $G_m$ is a gradient estimator for $\nabla L(\theta)$ and $\{\eta_m\}_{m=0}$ is the learning rate.
% \end{itemize}
% To complete the description of the SGD algorithm we need to specify the distribution $\rho$ and the gradient estimator for each problem. In each case we let $P$ be of the form $P=p^{\otimes r}$ for a for a small constant $r=O(1)$. In particular, $r=2$ works, i.e., $S_m=(X^m_1,X^m_2)$ where $X^m_1,X^m_2\overset{\text{iid}}{\sim}p$.
% \begin{itemize}
% \item 
% In the supervised learning setting $p=\rho$. That is, we sample $X^m_i$ from the probability measure defining the inner product on $\H$.
% \item
% In the VMC setting $p=p_{\theta_m}$. That is, we get samples $X^m_r$ from the $\theta_m$-dependent distribution $p_{\theta_m}=|\wf_\theta|^2/\|\wf_\theta\|^2$.
% \end{itemize}


% %
% %
% %We give variants of stochastic gradient descent (SGD) for Problems \ref{pro:slos} and \ref{vmcprob}. In both cases, we focus on the loss function $L(\theta)$ for $\theta\in\mathbb{R}^d$, which is induced by the scale invariant loss $\L(f)$ for $f\in\H$. Note that $L(\theta)$ is not scale invariant with respect to $\theta$.
% %
% %The SGD algorithm $\algsym$ can be stated as follows and depends on a problem-dependent distribution $P$. In the VMC setting we allow $P$ to depend on the state of the algorithm at step $m$:
% %\begin{itemize}
% %    \item (Initialization): Pick an initial parameter $\theta_0\in\RR^d$.
% %    \item (Iterative updates): 
% %    For each $m\in\NN_0$ $\algsym$ receives a sample $S_m\sim P$ and computes $G_m=\algsym(S_1,\ldots,S_m;\theta_0,\ldots,\theta_m)$ which represents an estimate of $\nabla_\theta L(\theta_m)$. $\algsym$ then defines
% %    \[\theta_{m+1}=\theta_m-\eta_m G_m.\]
% %    %Given $M>0$. For each $0\leq m\leq M-1$,
% %    %$\theta_{m+1}=\theta_m-\eta_m G_m$,
% %    %where $G_m$ is a gradient estimator for $\nabla_\theta L(\theta)$ and $\{\eta_m\}_{m=0}$ is the learning rate.
% %\end{itemize}
% %To complete the description of the SGD algorithm we need to specify the distribution $\rho$ and the gradient estimator for each problem. 
% %
% %% In each case we let be of the form $P=\rho^{\otimes r}$ for a for a small constant $r=O(1)$. In particular, $r=2$ works, i.e., $S_m=(X^m_1,X^m_2)$ where $X^m_1,X^m_2\overset{\text{iid}}{\sim}\rho$.
% %\begin{itemize}
% %\item 
% %In the supervised learning setting we sample $X^m_i$ from the probability measure $\rho$ defining the inner product on $\H$.%, i.e., $\ol{f}{g}=\int fgd\rho$
% %\item
% %In the VMC setting we get samples $X^m_r$ from the $\theta_m$-dependent distribution $\rho=p_{\theta_m}=|\wf_\theta|^2/\|\wf_\theta\|^2$.\ZD{Need to revise this part. In the VMC case, this $\rho$ is different from the $\rho$ in the definition of $\mathcal{H}$.}
% %\end{itemize}

% \subsection{Directionally unbiased gradient estimators}\label{sec:Duge}
% Finding a good SGD algorithm depends on constructing unbiased gradient estimators $G$.

% For \cref{vmcprob} (VMC) we show that having access to samples from $p_\theta(x)=|\wf_{\theta}(x)|^2/\|\wf_\theta\|^2$ allows us to directly construct an unbiased gradient estimator $G_m$ for $\nabla_\theta L(\theta_m)$, and the complexity result is consistent with the result of classical SGD. 

% In contrast, the supervised learning setting of Problem \ref{pro:slos} exhibits a subtlety which separates our setting from that of finite-dimensional scale-invariance: The lack of an unbiased estimator for $1/\|f\|$ and $1/\|f\|^3$ prevents us from obtaining an unbiased estimate for $\nabla_\theta f$. We therefore adjust our aim and show that it suffices to have an estimator whose \emph{direction} is unbiased, in the sense that $\EE[G]\in[\nabla_\theta f]$, while its length is only required to be of the correct order of magnitude.

% %In the supervised learning setting we will observe that we have some leeway in the scaling of $G$. In particular, we observe a subtlety in passing from the setting of scale-invariance on the parameter space to the parameterized infinite-dimensional setting: To estimate the gradient \cref{deltheta} we depend on an approximate estimate of $\|f\|$ which leads to a biased estimate of $1/\|f\|$. it then becomes important to balance the two terms in the stochastic gradient estimate so that this bias becomes a single multiplicative factor. \NA{add numerics for the unbalanced vs balanced}
% %
% %%For the VMC setting we will show that it is possible to find a cheap way to construct an unbiased estimator for $\nabla_\theta L(\theta)$ using the special structure of \eqref{eqn:VMC_L}.
% %%
% %%
% %%The detailed discussion of these algorithms is put in Section \ref{sec:Duge}.\hide{We emphasize that} The constructions of the gradient estimator $G_m$ for Problems \ref{pro:slos}, \ref{vmcprob} are very different. 
% %
% %The proper construction of $G_m$ for Problem \ref{pro:slos} is much more difficult. After some calculation (see in Section \ref{sec:pbgea}), we can show that, to obtain an unbiased estimator for $\nabla_\theta L(\theta)$ in Problem \ref{vmcprob}, we must estimate $\|f_\theta\|$ accurately, which is already an expensive task. To overcome this difficulty, we search for a partially biased gradient estimator for Problem \ref{pro:slos}. In particular, in Section \ref{sec:pbgea}, we construct a gradient estimator that is unbiased in the direction, meaning that $\mathbb{E}(G_m)$ is in the same direction as $\nabla_\theta L(\theta)$. Using this new gradient estimator and under proper assumption, we can also show similar convergence result as Problem \ref{pro:slos}.

% \subsection{Formal results}
% We prove that SGD converges in the scale-invariant supervised learning problem and in the VMC setting: 
% %Our direction-unbiased gradient estimators allow us to prove convergence of 

% \begin{theorem}\label{shortmainthm1}
% Suppose that
% \[\|f_\theta\|_4/\|f_\theta\|,\|\nabla f_\theta\|_4/\|gf_\theta\|,\|\mathsf{H}_\theta f_\theta\|_4/\|g_\theta\|\]
% are uniformly bounded by a constant $C_g$ and let $\epsilon>0$. Suppose that we have an estimator for $\|f_\theta\|$ such that $Z_i=\Theta(\|f_{\theta_i}\|)$ for each step $i$. Then there exists a way to construct $G_m$ so that when $M=\Theta(1/\epsilon^4)$, the average of $\EE[|\nabla_\theta L|]$ over the first $M$ steps is at most $\epsilon$.
% \end{theorem}

% \begin{theorem}\label{shortmainthm2}
% Assume that $\|H\wf_\theta/\wf_\theta\|_{4,p_\theta}$, $\|\nabla H\wf_\theta/\wf_\theta\|_{4,p_\theta}$, $\|\nabla\wf_\theta/\wf_\theta\|_{4,p_\theta}$,  $\|\mathsf{H}_\theta\wf_\theta/\wf_\theta\|_{2,p_\theta}$
% are all uniformly bounded by a constant $C_\wf$, and let $\epsilon>0$. Then there is $M=\Theta\left(1/{\epsilon^4}\right)$
% such that the average of $\EE[|\nabla_\theta L|]$ over the first $M$ steps of SGD is at most $\epsilon$.
% \end{theorem}

% The full statements are given in \cref{thm:main_result_alg1,thm:complexity_VMC} and their corollaries.
%\LL{ Combine sec 3 and 4?}
%done


%\LL{ Combine sec 3 and 4?}
%done

%\NA{revise}


%  \subsection{Unbiased gradient estimator for scale-invariant supervised learning}
%  \ZD{Can we comment out this section? I am not sure why we put it here and why it is useful?}
 
%  \NA{it gives the direction-unbiased gradient estimator with smaller variance than}\cref{eqn:G_r_theta}

% \begin{temp}
%  In this section, we first give a simple formulation for unbiased gradient estimator.

%  Given $r\ge2$ i.i.d. samples $X_i\sim\rho$. We write $f_i=f(X_i)$, $g_i=g(X_i)$, and $\nabla_\theta g_i=\nabla_\theta g_\theta(X_i)$.
%  %, and use the counting measure to define
%  \begin{equation}
%  \begin{aligned}
%  \ol{f}{g}_r=\frac1r\sum_{i=1}^rf_i g_i,
%  \qquad
%  \|f\|_r^2=\frac1r\sum_{i=1}^rf_i^2.
%  \end{aligned}
%  \end{equation}
%  \begin{lemma}\label{unbiasedsupervised}
%  Given samples $X_1,\ldots,X_n$, let
%  \begin{align}
%  G_n&=\frac1{r-1}\sum_{j=1}^r a_j\nabla f_j,
%  \\
%  %a_j&=(\tfrac1r\sum_{i=1}^r f_i^2)g_j-(\frac1r\sum_{i=1}^rg_if_i)f_j.
%  a_j&=-\|f\|_r^2\:g_j+\ol{g}{f}_r\:f_j.
%  \end{align}
%  %\[
%  %\begin{aligned}
%  %%G_n&=-\frac{n}{n-1}(\|g\|_n^2\ol{f}{\nabla g}_n-\ol{f}{g}_n\ol{g}{\nabla g}_n)
%  %&=-\frac1{n(n-1)}\sum_{i=1}^n\sum_{j=1}^n(g_i^2f_j\nabla g_j-f_ig_ig_j\nabla g_j).
%  %\end{aligned}
%  %\]
%  Then $G_r$ is an unbiased estimator for $G=\frac12\|f_\theta\|^3\nabla f_\theta$.
%  \end{lemma}
%  \begin{proof}
%  \cref{coordinatewise} states that
%  \[G=-\|f\|^2\ol{g}{\nabla f}+\ol{g}{f}\ol{f}{\nabla f}.\]
%  For each $i\neq j$ we have an unbiased estimator for $G$ given by
%  \[-f_i^2g_j\nabla f_j+g_if_if_j\nabla f_j.\]
%  Taking the average over all pairs $i\neq j$ gives another unbiased estimator for $G$:
%  \[-\frac1{n(n-1)}\sum_{i\neq j}(f_i^2g_j\nabla f_j-g_if_if_j\nabla f_j).\]
%  We may add the terms $i=j$ without changing the value because all such terms evaluate to $0$.
%  \end{proof}
% \end{temp}

\section{Supervised learning on sphere}\label{sec:slos}

%\ZD{I change the $Z$ to $\widetilde{Z}$ in this section to show it's an approximation. Please check.}

%In this section, we consider solving the Problem \ref{pro:slos}. 
%The gradient of the loss 
In order to adapt the SGD algorithm to the supervised learning problem \ref{pro:slos} we begin by writing down the gradient of the loss function \eqref{eqn:Ltheta}:
\begin{equation}\label{gradient_L_supervised}
\begin{aligned}
&\nabla_\theta L(\theta)=-\frac{\functional{g}{\nabla_\theta f_\theta}}{\|f_\theta\|}+\frac{\functional{g}{f_\theta}\functional{f_\theta}{\nabla_\theta f_\theta}}{\|f_\theta\|^3}\,.
\end{aligned}
\end{equation}


\begin{remark}
Given an estimate $\tilde{Z}$ of $\|f_\theta\|$ and samples $X_1,X_2\sim\rho$, let $f_i=f_\theta(X_i)$, $\nabla_\theta f_i=\nabla_\theta f_\theta(X_i)$, and $g_i=g(X_i)$. Then \cref{gradient_L_supervised} suggests a plug-in estimate of $\nabla f_\theta$ given by
\begin{equation}\label{eq:plugin}
-\frac{g_1\nabla_\theta f_1}{\tilde{Z}}+\frac{g_2f_2f_1\nabla_\theta f_1}{\tilde{Z}^3}\,,
\end{equation}
where the product in the second term uses two samples so that its expectation factors into a product.
 
However, this turns out to be a poor gradient estimator. In fact, \cref{fig:balance} demonstrates that when using this gradient estimate in the SGD algorithm, $f_\theta$ fails to converge to the target. The reason for this failure appears to be the fact it is \emph{unbalanced} in the sense that each term has a different power of $1/\tilde{Z}$. This biases the direction of \cref{eq:plugin}--its expectation is not parallel with $\nabla_\theta L$ unless the two terms of \cref{gradient_L_supervised} are parallel or $\tilde{Z}=\|f_\theta\|$ exactly.
\end{remark}



\subsection{Directionally unbiased gradient estimator}
By balancing $\tilde{Z}$ in each term we obtain a replacement for \cref{eq:plugin} which is  \emph{directionally unbiased} meaning that $\EE[G]$ is in the positive span of $\nabla_\theta L$. In the case $n=2$ our directionally unbiased estimator takes the form
\begin{equation}\label{eq:unbiased}
G=\frac{-f_2^2g_1\nabla_\theta f_1+g_2f_2f_1\nabla_\theta f_1}{\tilde{Z}^3}\,.
\end{equation}

Define $Z_\theta=\|f_\theta\|$. The general form for minibatch sizes $n>2$ follows from the lemma below:
\begin{lemma}\label{lem:unbiased_super}
Given $n\ge2$ and i.i.d. samples $X_i\sim\rho$, let $\ol{f}{g}_n=\frac1n\sum_{i=1}^nf(X_i)g(X_i)$ and $\|{f}\|_n^2=\frac1n\sum_{i=1}^nf(X_i)^2$, and define
\begin{equation}\label{eqn:G_unbiased}
G=\frac{1}{Z^3_\theta}\frac1{n-1}\sum_{j=1}^n a_j\nabla f_{\theta}(X_j)\,,
\end{equation}
where $a_j=-\|f_{\theta}\|_n^2\:g(X_j)+\ol{g}{f_{\theta}}_n\:f_{\theta}(X_j)$. Then $\mathbb{E}_X(G)=\nabla_\theta L(\theta)$.
\end{lemma}

The proof can be found in Appendix \ref{sec:minibath_sup}. Given an estimator $\tilde{Z}$ for $Z_\theta$  we then choose the gradient estimator: %This lemma implies, if $Z_\theta$ is known, for each fixed $\theta$, it is possible to construct a cheap unbiased estimation of $\nabla_\theta L(\theta)$.

\begin{equation}\label{eqn:G_r_theta}
G=\frac{1}{\tilde{Z}^3}\frac1{n-1}\sum_{i=1}^n a_i\nabla f_{\theta}(X_i)\,,
\end{equation}
where %$Z$ is a constant and 
$\{a_i\}^n_{i=1}$ are defined in Lemma \ref{lem:unbiased_super}. Then
\begin{equation}\label{eqn:G_r_theta_biased}
\mathbb{E}_{r_1,r_2}\left(G(\theta)\right)=\frac{Z^3_\theta}{\tilde{Z}^3}\nabla_\theta L(\theta)\,.
\end{equation}


%To show that \cref{eqn:G_r_theta} in directionally unbiased, define $Z_\theta=\|f_\theta\|$. Suppose we already know $Z_\theta$, then we can use the following lemma to construct a unbiased gradient estimation:


%\subsection{Directionally unbiased gradient estimation and algorithm}\label{sec:pbgea}
%
%As discussed before, to use \eqref{eqn:G_unbiased}, we first need to estimate the denominator $Z_\theta$, which seems a difficult and expensive task in general.
%
%What if we use a partially biased gradient estimation? In the classical gradient descent, one reason to use the gradient to move the parameter is that it is the fastest decaying direction of the loss function and it guarantees the decaying of the loss function after movement. In our case, if we can find a gradient estimator that is unbiased in the direction, this estimator still guarantees the similar decaying property of classical gradient descent in the expectation sense. In particular, for any given $\theta$, we define
%This means that, after taking the expectation, the approximated gradient $G$ is in the same direction as $\nabla_\theta L$.
Using \eqref{eqn:G_r_theta_biased} and Taylor's expansion, we obtain that if
\[
\theta_{m+1}=\theta_m-\eta_mG(\theta_m)\,,
\]
with $\frac{\eta_mZ^3_\theta}{\tilde{Z}^3}$ small enough,
\[
\mathbb{E}_{r_1,r_2}\left(G(\theta_{m+1})\right)\lesssim G(\theta_{m})-\eta_m \frac{Z^3_\theta}{\tilde{Z}^3}|\nabla_\theta L(\theta_m)|^2\,,
\]
which implies the decaying of the loss function in the expectation sense. 

In the above derivation, the choice of $\eta_m$ and the decaying rate of the loss function depends on the ratio $Z_{\theta_m}/\widetilde{Z}$. In practice, if we do not have a proper choice of preconditioner $\widetilde{Z}$, it is hard to give a reasonable prior choice of $\eta_m$ that guarantees fast convergence to the first-order stationary point. Inspired by the idea of variance reduction techniques~\cite{SAGA-2013,SAGA-2014,Johnson_Zhang}, to give a reasonable preconditioner for $Z_{\theta_m}$, we calculate an estimate for $Z_{\theta_m}$ only once for every $N$ steps, where $N>0$ is the length of each epoch. Between the epochs, per step, we set the preconditioner $\widetilde{Z}\approx Z_{\lfloor m/N\rfloor*N}$ so that the ratio between $\widetilde{Z}$ and $Z_{\theta_m}$ remains bounded. 

The detailed algorithm is summarized in Algorithm \ref{alg:GE}.
\begin{breakablealgorithm}
      \caption{Stochastic gradient descent algorithm for supervised learning}
  \label{alg:GE}
  \begin{algorithmic}[1]
  \STATE \textbf{Preparation:} $g_n$; $f_\theta(x)$; number of iterations: $M$; learning rate $\eta_m$; initial parameter: $\theta_0$; $N$; $n$
  \STATE \textbf{Running:}
  \STATE $m\gets 0$;
  \WHILE{$m\leq M$}
  \IF{$m$ is a multiple of $N$}
  \STATE $\widetilde{Z}_{m}\gets \text{an estimation of}\ \|f_{\theta_m}\|$; 
  \ENDIF
  \STATE Sample $\{X^m_i\}^{n}_{i=1}$ independently from $\rho$;
  \STATE $a_i\gets-\|f_{\theta_m}\|_n^2\:g(X_i)+\ol{g}{f_{\theta_m}}_n\:f_{\theta_m}(X_i)$
  \STATE $G_{m}\gets \frac{1}{\widetilde{Z}_{\lfloor m/N\rfloor*N}^3}\frac1{n-1}\sum_{i=1}^n a_i\nabla f_{\theta_m}(X_i)$;
  \STATE $\theta_{m+1}\gets \theta_m-\eta_m G_m$;
  \ENDWHILE
    \STATE \textbf{Output:} $\theta_m$;
    \end{algorithmic}
\end{breakablealgorithm}

\begin{remark} To obtain an estimation of $\|f_{\theta_m}\|$, we can sample $J$ data points $\{X_j\}^J_{j=1}$ independently from the distribution $\rho$ and use $\sqrt{\frac{1}{J}\sum^J_{j=1}|f_\theta(X_j)|^2}$ to approximate $\|f_\theta\|$ with large enough $J$. Because $J$ needs to be large enough to ensure a good approximation, it is very expensive to use this approximation in every step. In the above algorithm, since we only use this estimation once for every $N$ steps, the average number of evaluations of $f$ is $J/N$, which is acceptable if $N$ is large enough.
\end{remark}



\subsection{Analysis of Algorithm \ref{alg:GE}}\label{sec:alg_2_ana}

We now study the complexity of Algorithm \ref{alg:GE}. We first propose some additional assumptions:
\begin{assumption}\label{assum_g_scale_inva} There exists a constant $C_f$ such that for any $\theta\in\mathbb{R}^d$
\begin{itemize}
    \item (Value bound): 
    \begin{equation}\label{eqn_value_gnbound}
    \left\|f_\theta^2\right\|^{1/2}/\|f_\theta\|\leq C_f\,.
    \end{equation}
    \item (Gradient bound): 
    \begin{equation}\label{eqn_Gradient_gnbound}
    \left\|\left|\nabla_\theta f_\theta\right|^2\right\|^{1/2}/\|f_\theta\|\leq C_f\,.
    \end{equation}
    \item (Hessian bound): 
    \begin{equation}\label{eqn_Hessian_gnbound}
    \left\|\left\|\mathsf{H}_\theta f_\theta\right
    \|^2_2\right\|^{1/2}/\|f_\theta\|\leq C_f\,,
    \end{equation}
    where $\mathsf{H}_\theta f$ is the hessian of $f$.
\end{itemize}

\end{assumption}
Under the Assumption \ref{assum_g_scale_inva}, we establish the Lipschitz property of $\nabla L$ and provide bounds for the variance of the gradient estimator (see \cref{lem:svsl_2}). These results play a crucial role in demonstrating the convergence of our method.~\ZD{I add a sentence here.}
We note that Assumption \ref{assum_g_scale_inva} is scale-invariant in $f_\theta$, meaning the same assumption holds with the same constant for any $\lambda f_\theta$, where $\lambda>0$. 

Now, we are ready to introduce our complexity results for Algorithm \ref{alg:GE}:
\begin{theorem}\label{thm:main_result_alg1}
Assume the Assumption \ref{assum_g_scale_inva} holds true,  $|g_n|\leq C_g$ for all $1\leq n\leq N$, and 
\begin{equation}\label{eqn:distance_bound}
\frac{1}{C_r}\leq \frac{\|f_{\theta_m}\|}{\left\|f_{\theta_{\lfloor m/N\rfloor*N}}\right\|},\frac{\widetilde{Z}_{\lfloor m/N\rfloor*N}}{\left\|f_{\theta_{\lfloor m/N\rfloor*N}}\right\|}\leq C_r\,,\quad \mathrm{a.s.}
\end{equation}
for all $m\in\mathbb{N}$, where $\theta_m$ comes from Algorithm \ref{alg:GE}. There exists a constant $C$ that only depends on $C_r,C_f,C_g$ such that, when $\eta_m<\frac{1}{C}$ for any $m\geq 0$, 
\begin{equation}\label{eqn:gradient_F_bound}
\sum^M_{m=0}\eta_m\mathbb{E}\left(\left|\nabla_\theta L(\theta_m)\right|^2\right)\leq 2C^2_rL(\theta_0)+C\sum^M_{m=0}\frac{\eta^2_m}{n}\,.
\end{equation}
for all $M>0$.
\end{theorem}
We put the proof of the above theorem in Appendix \ref{sec:pf_of_thm:main_result_alg1}. In the proof, we first bound the $\mathbb{E}|G_m|^2$ and show that  $\nabla L$ has a uniform Lipschitz constant. Then, the decay rate of the loss function in each step can be lower bounded by $|\nabla L|^2$, which finally gives us an upper bound of $|\nabla L|^2$ as shown in \eqref{eqn:gradient_F_bound}.

In the above theorem, \eqref{eqn:distance_bound} is an important and natural assumption to show the convergence of Algorithm \ref{alg:GE}. As discussed before, to avoid calculating $\|f_\theta\|$ in each iteration, we use $\widetilde{Z}_{\lfloor m/N\rfloor*N}$ as a preconditioner to replace $Z_m$ in the denominator. This approximation is reasonable only when the preconditioner is not too far away from $Z_m$. Thus, it is natural to expect that we need \eqref{eqn:distance_bound} to prove a fast convergence rate of Algorithm \ref{alg:GE}. Besides, we note that, according to Appendix \ref{sec:svsl_1} Lemma \ref{lem:prior_VMC}, in each epoch of training process, if $\left|\theta_m-\theta_{\lfloor m/N\rfloor*N}\right|$ is bounded ($\eta_m$ is chosen to be small enough) and the estimation of $Z_{\lfloor m/N\rfloor*N}$ is enough accurate, then the assumption \eqref{eqn:distance_bound} is automatically satisfied.

Using Theorem \ref{thm:main_result_alg1}, it is straightforward to show the following corollary: 
\begin{corollary}\label{cor:super} Assume the Assumption \ref{assum_g_scale_inva} and \eqref{eqn:distance_bound} hold true. 
\begin{itemize}
\item Choosing $\eta_m=\Theta\left(n\epsilon^2\right)$ and $M=\Theta\left(1/(n\epsilon^4)\right)$,
we have $\min_{0\leq m\leq M}\mathbb{E}\left(\left|\nabla_\theta L(\theta_m)\right|\right)\leq \epsilon$.

\item Choosing $\eta_m=\Theta\left(\sqrt{\frac{n}{m+1}}\right)$,
we have $\min_{0\leq m\leq M}\mathbb{E}\left(\left|\nabla_\theta L(\theta_m)\right|\right)=O\left(\left(nM\right)^{-1/4}\right)$.

\end{itemize}
\end{corollary}

We note that this bound matches the standard $O(M^{-1/4})$ convergence rate of SGD to first-order stationary point for non-convex functions.

\section{Algorithm and convergence result for VMC}
%\subsection{Unbiased gradient estimator for VMC and algorithm}
In the VMC setting, we assume that the MCMC subroutine is able to sample exactly from the distribution $p_\theta=q_\theta/Q_\theta$ where $Q_\theta=\int q_\theta(x) dx$. This sampling oracle makes it possible to construct a cheap and unbiased gradient estimation for the loss function $L$ \eqref{eqn:VMC_L}.

We first calculate the exact gradient of the loss function  $L$. Different from the supervised-learning case, when computing the gradient, we need to take into account both the change in energy due to the changing local energy function \emph{and} the change in energy due to the changing probability distribution.

Let  $\Lpsi_\theta=\log|\wf_\theta|$. After some calculation (see \cref{gradproofs}), we obtain 
\begin{equation}
\label{eqn:gradient_L_VMC}
\nabla_{\theta} L(\theta)=2 \mathbb{E}_{X\sim p_\theta}\left[\left(\mathcal{E}_{\theta}(X)-L(\theta)\right) \nabla_{\theta}\Lpsi_{\theta}(X)\right].
\end{equation}

Using $p_\theta(x)$ and \eqref{eqn:gradient_L_VMC}, we can obtain the following unbiased gradient estimators for $\nabla_\theta L(\theta)$ (proof in \cref{gradproofs}):

\begin{lemma}\label{lem:unbiased_gradient} Given $n \geq 2$ i.i.d. samples $X_{i} \sim p_{\theta}$, let $\hat{L}(\theta)=\frac{1}{n} \sum_{i=1} \mathcal{E}_{\theta}\left(X_{i}\right)$ and define the gradient estimate
\begin{equation}\label{eqn:VMCgradient_estimator}
G(\theta)=\frac{2}{n-1} \sum_{i=1}^{n}\left(\mathcal{E}_{\theta}\left(X_{i}\right)-\hat{L}(\theta)\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{i}\right) .
\end{equation}
Then $G(\theta)$ is an unbiased estimator of the gradient of the population energy.
%~\ZD{We might move the proof (or even the lemma) to the appendix.}
\end{lemma}

Using the unbiased gradient estimator \eqref{eqn:VMCgradient_estimator} from \cref{lem:unbiased_gradient}, we obtain the following SGD type algorithm to minimize $L(\theta)$:
\begin{breakablealgorithm}
      \caption{Stochastic gradient descent algorithm for variational Monte Carlo}
  \label{alg:VMC}
  \begin{algorithmic}[1]
  \STATE \textbf{Preparation:} number of iterations: $M$; learning rate $\eta_m$; initial parameter: $\theta_0$; number of samples each iteration: $n$;
  \STATE \textbf{Running:}
  \STATE $m\gets 0$;
  \WHILE{$m\leq M$}
  \STATE Sample $\{X^m_i\}^n_{i=1}$ independently according to the density $p_{\theta^m}(x)=\frac{\left|\wf_{\theta_m}(x)\right|^{2}}{\int_{\Omega}\left|\wf_{\theta_m}(x)\right|^{2} d x} $;
  \STATE Compute $G_m$ using Eq. \eqref{eqn:VMCgradient_estimator};
  \STATE $\theta_{m+1}\gets \theta_m-\eta_m G_m$;
  \ENDWHILE
    \STATE \textbf{Output:} $\theta_m$;
    \end{algorithmic}
\end{breakablealgorithm}

\subsection{Analysis of Algorithm \ref{alg:VMC}}

In this section, we study the complexity of Algorithm \ref{alg:VMC}. First, given any $\theta$, we define $\|\cdot\|_{q,p_\theta}$ as the $q$-th norm under the measure $ p_\theta(x)dx$. We need the following assumptions for the theoretical result:
\begin{assumption}\label{assum_g_VMC} There exists a constant $C_{\wf}$ such that for any $\theta\in\mathbb{R}^d$, 
\begin{itemize}
    \item (Value bound): 
    \begin{equation}\label{eqn_value_gnbound_vmc}
    \|H\wf_\theta/\wf_\theta\|_{4,p_\theta}\leq C_\wf\,.
    \end{equation}
    
    \item (Gradient bound): 
    \begin{equation}\label{eqn_Gradient_gnbound_vmc}
    \begin{aligned}
&\|\nabla_\theta H\wf_\theta/\wf_\theta\|_{2,p_\theta}\leq C_\wf\,,\\
&\|\nabla_\theta \wf_\theta/\wf_\theta\|_{4,p_\theta}\leq C_\wf
\,.
\end{aligned}
    \end{equation}
    \item (Hessian bound): 
    \begin{equation}\label{eqn_Hessian_gnbound_vmc}
    \|\mathsf{H}_\theta\wf_\theta/\wf_\theta\|_{2,p_\theta}\leq C_\wf\,,
    \end{equation}
    where $\mathsf{H}_\theta \wf_\theta$ is the hessian of $\wf_\theta$ in $\theta$.
\end{itemize}

\end{assumption}
We note that Assumption \ref{assum_g_VMC} is scale-invariant in $\wf_\theta$. If $\wf_\theta(x)$ satisfies Assumption \ref{assum_g_VMC}, then $\lambda\wf_\theta(x)$ also satisfies the assumption with the same constant $\lambda$ for any $\lambda>0$. We establish new bounds on the Lipschitz constant of $\nabla L$ and the variance of the gradient estimator (see \cref{lem:prior_VMC}), subject to the constraints specified in Assumption \ref{assum_g_VMC}. These bounds are crucial in demonstrating the convergence of our method.~\ZD{I add a sentence here.}

Now, we are ready to give the convergence result for Algorithm \ref{alg:VMC}:
\begin{theorem}\label{thm:complexity_VMC}
Assume the Assumption \ref{assum_g_VMC} holds true. There exists a constant $C>0$ that only depends on $C_\wf$ such that, when $\eta_m<\frac{1}{C}$ for any $m\geq 0$, 
\begin{equation}\label{eqn:gradient_F_bound_vmc}
\sum^M_{m=0}\eta_m\mathbb{E}\left(\left|\nabla_\theta L(\theta_m)\right|^2\right)\leq 2F(\theta_0)+C\sum^M_{m=0}\frac{\eta^2_m}{n}\,.
\end{equation}
for any $M>0$. 
\end{theorem}
We put the proof of the above theorem in Appendix \ref{sec:pf_of_thm:complexity_VMC}. Similar to the supervised-learning case, the upper bound \eqref{eqn:gradient_F_bound_vmc} is important for us to determine the $\eta_m$ so that the parameter $\theta_m$ converges to a first-order stationary point in the expectation sense when $m$ is moderately large. Using Theorem \ref{thm:complexity_VMC}, we can show a similar result as supervised-learning:
\begin{corollary}\label{cor:VMC} Assume the Assumption \ref{assum_g_VMC} holds true.  
\begin{itemize}
\item Choosing $\eta_m=\Theta\left(n\epsilon^2\right)$ and $M=\Theta\left(1/(n\epsilon^4)\right)$,
we have $\min_{0\leq m\leq M}\mathbb{E}\left(\left|\nabla_\theta L(\theta_m)\right|\right)\leq \epsilon$.

\item Choosing $\eta_m=\Theta\left(\sqrt{\frac{n}{m+1}}\right)$, we have $\min_{0\leq m\leq M}\mathbb{E}\left(\left|\nabla_\theta L(\theta_m)\right|\right)=O\left(\left(nM\right)^{-1/4}\right)$.
\end{itemize}
\end{corollary}

This bound shows that the convergence of Algorithm \ref{alg:VMC} is same as the convergence rate of SGD to first-order stationary point for non-convex functions.
%\section{Scale-invariant supervised learning}\label{sec:sisl}
%
%\ZD{We might find some places to say the reason we want to do this scale-invariant supervised learning. Section \ref{sec:sisl} is not a too late place.}
%
%In this section, we consider solving Problem \ref{genvmcprob} with trivial sampling oracle, meaning that $p$ is a uniform distribution on the data set. Without loss of generality, we assume the underlining distribution of data $\rho(x)=\frac{1}{N}\sum^N_{n=1}\delta_{x=n}$, the sampling distribution $p(x)=\frac{1}{N}\sum^N_{n=1}\delta_{x=n}$, and 
%\[
%\localE_\theta(g)(x)=\frac{f(x)g_\theta(x)}{\|g_\theta\|}=-\frac{f(x)g_\theta(x)}{\frac{1}{N}\sum g^2_\theta(n)}\,.
%\]
%
%For simplicity, we write $f(n)=f_n$ and $g_\theta(n)=g_n(\theta)$. Then, minimizing \eqref{genvmc} is equivalent to minimize the loss function
%\begin{equation}\label{eqn:L_theta}
%L(\theta)=\mathcal{L}\left(g_\theta\right)=-\frac{\frac{1}{N}\sum^N_{n=1}f_ng_n(\theta)}{\sqrt{\frac{1}{N}\sum^N_{n=1}g^2_n(\theta)}}
%\end{equation}
%with gradient
%\begin{equation}\label{gradient_L_supervised}
%\begin{aligned}
%&\nabla_\theta L(\theta)=-\frac{\frac{1}{N}\sum^N_{n=1}f_n\nabla_\theta g_n(\theta)}{\sqrt{\frac{1}{N}\sum^N_{n=1}g^2_n(\theta)}}\\
%&+\frac{\left(\frac{1}{N}\sum^N_{n=1}f_n g_n(\theta)\right)\left(\frac{1}{N}\sum^N_{n=1}g_n(\theta) \nabla_\theta g_n(\theta)\right)}{(\frac{1}{N}\sum^N_{n=1}g^2_n(\theta))^{3/2}}\,.
%\end{aligned}
%\end{equation}
%Define 
%\begin{equation}\label{eqn:Z_theta}
%Z_\theta=\sqrt{\frac{1}{N}\sum^N_{n=1}g^2_n(\theta)}\,.
%\end{equation}
%Suppose we already know the denominator of \eqref{gradient_L_supervised}, then
%\[
%\begin{aligned}
%&\nabla_\theta L(\theta)=-\mathbb{E}_{r_1,r_2}\left(\frac{f_{r^m_1}\nabla_\theta g_{r^m_1}(\theta)g^2_{r^m_2}(\theta)}{Z^3_\theta}\right)\\
%&+\mathbb{E}_{r_1,r_2}\left(\frac{f_{r^m_2} g_{r^m_2}(\theta)\left(g_{r^m_1}(\theta) \nabla_\theta g_{r^m_1}(\theta)\right)}{Z^{3}_\theta}\right)\,,
%\end{aligned}
%\]
%where $r_1,r_2$ are uniformly sampled from $1,2,\cdots, N$. This implies, if $Z_\theta$ is known, for each fixed $\theta$, to get an unbiased estimation of $\nabla_\theta L(\theta)$, we only need to calculate 
%$g$, $\nabla_\theta g$ at two different points. 
%
%However, different from VMC setting, in this case, the sampling oracle $p$ only samples uniform distribution on the data set. This implies that, for every parameter $\theta$, we don't have an efficient way to calculate the denominator $Z_\theta$ in the gradient form \eqref{gradient_L_supervised}. Then, if we naively apply the idea of Algorithm \ref{alg:VMC} from VMC into this setting and calculate the denominator in each iteration to guarantee an unbiased gradient estimation, we need to calculate $g_n(\theta)$ for every data point $n$, which is as expensive as classical gradient descent. 



\section{Empirical convergence of supervised learning}\label{sec:Exp}
%\GG{Section title here is inconsistent with title of VMC section below}
\cref{losscomp} shows the convergence of SGD for supervised learning using the loss function \eqref{eqn:Ltheta} compared to using the standard $L^2$-loss $f\mapsto\|f-g\|^2$.
In order to have the same initial loss under both training procedures and avoid giving a disadvantage to the non-invariant $L^2$-based training, our Ansatz  includes a multiplicative pre-factor $\lambda$ in its parameters, and  $\lambda$ is initialized to normalize $\lambda f_\theta$ from the beginning of training. This comparison therefore isolates the difference between the two training losses due to the different training dynamics. We observe %\cref{losscomp} shows 
that training with the scale-invariant loss leads to a smaller value of the loss function by two orders of magnitude.
%\LL{ Though obvious, it is common to  add a sentence summarizing the finding in the figure.} 

\begin{figure}[h]
\includegraphics[width=.45\textwidth]{SI_vs_l2.pdf}
\caption{Comparison of the convergence when trained using the loss \eqref{eqn:Ltheta} versus training with the standard $L^2$ loss. In both cases the quantity plotted is the scale-invariant loss.}
\label{losscomp}
\end{figure}



\cref{fig:balance} shows the convergence of the SGD algorithm using the directionally unbiased gradient estimator of \cref{eq:unbiased} compared to that of the unbalanced and therefore directionally biased plug-in estimator of \cref{eq:plugin}. We find that training with the directionally unbiased estimator leads to convergence of the scale-invariant loss whereas training with the directionally biased plug-in estimate does not lead to convergence.   %\LL{ Same comment as above} 

\begin{figure}[h]
\includegraphics[width=.45\textwidth]{balancedSGD.pdf}
\caption{Comparison of gradient estimators for supervised learning. $f_\theta$ converges to the target using the directionally unbiased balanced gradient estimator \cref{eq:unbiased}, but it fails when using the plug-in variant \cref{eq:plugin}. The algorithm is a variant of \cref{alg:GE} where $Z^2$ is a running average of previous $f(X^m_{1,2})^2$.}
\label{fig:balance}
\end{figure}

\cref{thm:main_result_alg1} holds under the condition of a Hessian bound, \cref{eqn_Hessian_gnbound_vmc}. We use $\|\nabla L(\theta_{i+1})-\nabla L(\theta_i)\|/\|\theta_{i+1}-\theta_i\|$ as a proxy for the magnitude of the Hessian, noting that this is also the quantity which we need for the proof of the SGD algorithm (see \cref{eqn:gradient_Lip_vmc}).


\begin{figure}[h]
\includegraphics[width=.45\textwidth]{H_est.pdf}
\caption{Lipschitz estimate for the gradient in the supervised learning setting. $\|\nabla L(\theta_{i+1})-\nabla L(\theta_i)\|/\|\theta_{i+1}-\theta_i\|$ stays bounded by a small constant as the loss is minimized (red, dashed). }%\GG{Mention "supervised" in the caption so it is clear which algorithm this pertains to?}  }
\label{Hessianfig}
\end{figure}



\cref{Hessianfig} shows $\|G(\theta_{m+1};X^m)-G(\theta_{i};X^m)\|/\|\theta_{i+1}-\theta_i\|$
, which due to the bias-variance decomposition is an upper bound on $\frac{\|\nabla L(\theta_{i+1})-\nabla L(\theta_i)\|}{\|\theta_{i+1}-\theta_i\|}$ in the mean-square sense. %\GG{The preceding is only true if we take expectations of both sides over multiple runs, right?}  
We observe that this quantity is bounded by a small constant, justifying condition \eqref{eqn_Hessian_gnbound}.


\section{Empirical convergence of VMC algorithm}

We provide numerical evidence for the convergence of \cref{alg:VMC} on the Hydrogen square (H$_4$) model depicted in \cref{fig:h4conf}. This system involves only four particles but is strongly correlated and known to be difficult to simulate accurately. We define $\wf_\theta$ using the FermiNet ansatz \cite{Pfau_2020, Spencer_2020} and run 200,000 training steps using stochastic gradient descent with a learning rate schedule $\eta(t) = \frac{0.05}{\sqrt{1 + t/10000}}$. 

With this learning rate schedule, \cref{cor:VMC} implies that the running minimum of $\mathbb{E}(|\nabla_\theta L(\theta_m)|)$ should converge as $O(M^{-1/4})$ as the optimization progresses. To verify this bound, we plot in \cref{fig:H4_G} the running minimum of $|G(\theta)|$, which is our numerical proxy for $|\nabla_\theta L(\theta)|$ as per \cref{lem:unbiased_gradient}. We compare two distinct VMC runs using 10 and 1000 walkers, respectively, to explore how the convergence varies with the accuracy of the gradient estimate. We estimate the convergence rate by measuring the overall slope of the log-log plot, starting from step 200 to avoid the initial pre-asymptotic period. We find that the gradient of the loss converges roughly as $M^{-0.27}$ when using 10 walkers, closely matching the expected bound. With 1000 walkers the convergence goes as $M^{-0.38}$, which is somewhat faster than the theoretical bound, as might be expected given that the estimate of the gradient is very accurate with so many walkers.

Since the bounding of the Lipschitz constant of the gradient of the loss function is critical to our convergence proof, we supply in \cref{fig:H4_L} a numerical estimate of this quantity during the VMC run. Here we show data only for the case of 1000 walkers, as the Lipschitz constant estimate is extremely noisy with 10 walkers. The data suggest that the value of the Lipschitz constant is well below $1000$ for this simulation.
\begin{figure}
\centering
\begin{tikzpicture}

\coordinate (1) at (0,0);
\coordinate (2) at (2,0);
\coordinate (3) at (2,2);
\coordinate (4) at (0,2);
\coordinate (5) at ($(1)!.5!(2)$);

\fill (1) circle (2pt) node [below left] {H};
\fill (2) circle (2pt) node [below right] {H};
\fill (3) circle (2pt) node [above right] {H};
\fill (4) circle (2pt) node [above left] {H};
\node at (5) [below] {$1.0$ Bohr};

\draw[dashed] (1)--(2)--(3)--(4)-- cycle;

\end{tikzpicture}
\caption{Atomic configuration for the square H$_4$ model.}
\label{fig:h4conf}
\end{figure}

\begin{figure}
\includegraphics[width=0.45\textwidth]{H4_G.pdf}
\caption{Convergence of VMC run on the H$_4$ square. The running minimum is taken to smooth out the data and match the form of \cref{cor:VMC}.}
\label{fig:H4_G} 
\end{figure}

\begin{figure}
\includegraphics[width=0.45 \textwidth]{H4_L.pdf}
\caption{Lipschitz constant for VMC run on the H$_4$ square with 1000 walkers. The constant is numerically approximated using the formula $|G(\theta_{m+1})- G(\theta_m)|/|\theta_{m+1} - \theta_m|$.}
\label{fig:H4_L}
\end{figure}



% \subsection{Convergence of VMC algorithm}

% We provide numerical results for the convergence of Algorithm \ref{alg:VMC} on two systems. We use the VMCNet code \cite{Jeff_2021} with a simple stochastic gradient descent optimizer to run all experiments. For each system we present results using both 10 MCMC walkers and 1000 MCMC walkers. 

% \subsubsection{Quantum Harmonic Oscillator}
% \label{sec:osc}
% We first study a toy system consisting of five non-interacting fermions, three up-spin and two down-spin, under the quantum harmonic oscillator Hamiltonian. We define $\Psi_\theta$ to be the ground-state wavefunction for this quantum Harmonic oscillator with a spring constant of $\theta$, but we initialize $\theta$ to not match the true spring constant $\theta^*$ of the Hamiltonian. The VMC optimization thus amounts to tuning $\theta$ to match $\theta^*$. For this system we run 10,000 training epochs using a learning rate schedule $\eta(t) = \frac{0.01}{\sqrt{1 + t/10}}$. 

% \LL{Perhaps this scaling statement should be clarified / mentioned earlier} 
% With this learning rate schedule, Theorem \ref{thm:complexity_VMC} implies a bound on the convergence rate of the gradient of the loss:
% \begin{equation}
% \label{eq:vmc_exp_conv}
% \min_{0 \leq m \leq M} \mathbb{E}(|\nabla_\theta L(\theta_m)|^2) = \tilde{O}(M^{-1/2}).
% \end{equation}
% To verify this bound, we plot in panel (a) of Figure \ref{fig:qho} the running minimum of $|G(\theta)|^2$ over the training process, which is our numerical proxy for $|\nabla_\theta L(\theta)|^2$ as per Lemma \ref{lem:unbiased_gradient}. On this toy system the observed convergence is much more rapid than the bound of $M^{-1/2}$. 

% In panels (b) and (c) of Figure \ref{fig:qho} we plot the quantities that appear in the value bound and the second gradient bound of Assumption \ref{assum_g_VMC}. From these plots we see that $C_\Psi$ could potentially take on a value of about 10.0 for this system. Finally, in panel (d) we plot an estimate of the Lipschitz constant of the gradient of the loss function using parameter vectors from subsequent epochs, namely $
% \frac{|G(\theta_{m+1})- G(\theta_m)|}{|\theta_{m+1} - \theta_m|}$. This quantity is used as a proxy for the ratio $ \frac{|\nabla_\theta L(\theta_{m+1}) - \nabla_\theta L(\theta_m)|}{|\theta_{m+1} - \theta_m|}$ which appears in Lemma \ref{lem:prior_VMC} and is used to bound the variance of the gradient estimator $G(\theta)$. The data suggest that the relevant constant for that bound, $C' (C_\Psi^4 +1)$, can be set to $1e4$ for this system. \LL{ may need to discuss} 

% \subsubsection{H$_4$ square}
% We study the Hydrogen square (H$_4$) with a bond distance of 1.0 Bohr under the standard electronic structure Hamiltonian. This system, pictured in Figure \ref{fig:h4conf}, involves only four fermions but is strongly correlated and is physically much more complex than the quantum harmonic oscillator example presented previously. We define $\Psi_\theta$ using the FermiNet ansatz \cite{Pfau_2020, Spencer_2020} and run 200,000 training epochs using a learning rate schedule $\eta(t) = \frac{0.05}{\sqrt{1 + t/10000}}$. The resulting statistics are shown in Figure \ref{fig:H4} and mirror exactly the statistics shown in Figure \ref{fig:qho}. On this system, the convergence rate of the gradient of the energy approximately matches the lower bound of $M^{-1/2}$. Furthermore, the value bound and the gradient bound suggest that the value of $C_\Psi$ for this system could be chosen to be $1e3$, while the Lipschitz estimate indicates that the value of $C'(C_\Psi^4+1)$ could be set at $1e4$.

% \begin{figure}
% \centering
% \begin{tikzpicture}

% \coordinate (1) at (0,0);
% \coordinate (2) at (2,0);
% \coordinate (3) at (2,2);
% \coordinate (4) at (0,2);
% \coordinate (5) at ($(1)!.5!(2)$);

% \fill (1) circle (2pt) node [below left] {H};
% \fill (2) circle (2pt) node [below right] {H};
% \fill (3) circle (2pt) node [above right] {H};
% \fill (4) circle (2pt) node [above left] {H};
% \node at (5) [below] {$1.0$ Bohr};

% \draw[dashed] (1)--(2)--(3)--(4)-- cycle;

% \end{tikzpicture}
% \caption{Atomic configuration for the square H$_4$ model.}
% \label{fig:h4conf}
% \end{figure}

% \begin{figure*}
% \centering
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/QHO_G.pdf}
%         \caption{}
%         \label{subfig:QHO_G}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/QHO_EL.pdf}
%         \caption{}   
%         \label{subfig:QHO_EL}
%     \end{subfigure} 
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/QHO_GLP.pdf}
%         \caption{}
%         \label{subfig:QHO_GLP}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/QHO_L.pdf}
%         \caption{}
%         \label{subfig:QHO_L}
%     \end{subfigure}
% \caption{Selected training statistics from VMC run on quantum harmonic oscillator. (a) Convergence of SGD as tracked by the running minimum of $|G(\theta)|^2$. (b) Value bound quantity $\mathbb{E}(E_L^4)^{1/4}$ of Equation \ref{eqn_value_gnbound_vmc}. (c) Gradient bound quantity $\mathbb{E}(\nabla_\theta \log^4 \Psi)^{1/4}$ of Equation \ref{eqn_Gradient_gnbound_vmc}. (d) Lipschitz constant estimate $\frac{|G(\theta_{m+1})- G(\theta_m)|}{|\theta_{m+1} - \theta_m|}$. Data for (d) is partial because $\theta$ reached its exact value within 1000 epochs in both optimization runs, making $\theta_{m+1} = \theta_m $ so that no estimate of the Lipschitz constant could be made for later epochs.}
% \label{fig:qho}
% \end{figure*}

% \begin{figure*}
% \centering
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/H4_G.pdf}
%         \caption{}
%         \label{subfig:H4_G}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/H4_EL.pdf}
%         \caption{}   
%         \label{subfig:H4_EL}
%     \end{subfigure} 
%     \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/H4_GLP.pdf}
%         \caption{}
%         \label{subfig:H4_GLP}
%     \end{subfigure}
%      \begin{subfigure}[b]{0.4\textwidth}
%         \includegraphics[width=\textwidth]{plots_vmc/H4_L.pdf}
%         \caption{}
%         \label{subfig:H4_L}
%     \end{subfigure}
% \caption{Selected training statistics from VMC run on H$_4$ square. (a) Convergence of SGD as tracked by the running minimum of $|G(\theta)|^2$.  (b) Value bound quantity $\mathbb{E}(E_L^4)^{1/4}$ of Equation \ref{eqn_value_gnbound_vmc}. (c) Gradient bound quantity $\mathbb{E}(\nabla_\theta \log^4 \Psi)^{1/4}$ of Equation \ref{eqn_Gradient_gnbound_vmc}. (d) Lipschitz constant estimate $\frac{|G(\theta_{m+1})- G(\theta_m)|}{|\theta_{m+1} - \theta_m|}$.}
% \label{fig:H4}
% \end{figure*}

\section{Conclusion}

% \LL{ Instead of emphasizing the infinite dimensional space (which does not play an explicit role), can we say something along the line of technical contributions, and alluding the future work beyond projective spaces? (i.e., relevant for excited state VMC simulations)} 
% In scientific computing it is often necessary to parameterize an infinite-dimensional function space with a finite set of parameters. It is non-trivial to constrain the parameterized function to lie on a sub-manifold of the function space. In this paper we consider optimization problems over the unit sphere of a function space and transform them into an unconstrained optimization problem with a scale-invariant loss. We prove that the SGD algorithm in the full parameterized function converges in a supervised learning setting, and we show that the energy converges in the variational Monte Carlo. We believe that the pullback technique can be extended to more general quotient spaces, to transform optimization problems over submanifolds of function spaces into unconstrained optimization problems. 

We consider the problem of optimizing a vector in a high dimensional sphere that is parameterized by a neural network up to a normalization constant. Existing manifold optimization algorithms are not directly applicable due to the inability to efficiently compute the normalization constant. We address this problem in the settings of both supervised and unsupervised learning. To our knowledge, this leads to the first proof of the convergence of SGD type algorithms for VMC simulations of ground states of quantum systems.

SGD is only the simplest stochastic optimization method. Over the last two decades, there has been increasing interest in developing more efficient and scalable optimization methods for VMC simulations~\cite{Sandvik2007, Neuscamman2012, Otis2019}, and our analysis may be a starting point for analyzing such methods.  It may also be possible to generalize this work from a high dimensional sphere to a Grassmann manifold, parameterized by a neural network up to a gauge matrix. Such a setting could be applicable to VMC simulations of \textit{excited states} of quantum systems.

\vspace{1em}
\textbf{Acknowledgment}

This work was supported by the Simons Foundation under Award No. 825053 (N. A.), and by the NSF Quantum Leap Challenge Institute (QLCI) program under Grant number OMA-2016245 (Z. D.). This work is supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of Energy Computational Science Graduate Fellowship under Award Number DE-SC0023112 (G. G.). This material is also based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research and Office of Basic Energy Sciences, Scientific Discovery through Advanced Computing (SciDAC) program (L.L.).  L.L. is a Simons Investigator. 


\vspace{1em}
\textbf{Disclaimer}

This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.



% As discussed in the above remark, Algorithm \ref{alg:GE} is cheaper than classical GD only when $N>\frac{1}{\epsilon^2}$. This phenomenon is similar to the case when people compare SGD with GD. The $O(M^{-1/4})$ convergence rate mainly comes from the large gradient estimation variance in each iteration, which introduces extra error and slows the convergence speed. To overcome this bottleneck, Algorithm \ref{alg:VGD} combines variance reduction skills with the original $G_m$ to introduce a new gradient approximation that has smaller variance. Theoretically, the variance reduction skills ensure the cost of Algorithm \ref{alg:VGD} is always smaller than that of classcial GD, which is stated in the following theorem:
% \begin{theorem}\label{thm:algorithm_VGD}
% Assume the Assumption \ref{assum_g_scale_inva} holds true,  $|f_n|\leq C_f$ for all $1\leq n\leq N$, and 
% \begin{equation}\label{eqn:distance_bound_2}
% \frac{1}{C_r}\leq \frac{\frac{1}{N}\sum^N_{n=1}g^2_n(\theta_m)}{\frac{1}{N}\sum^N_{n=1}g^2_n(\theta^{\lfloor m/N\rfloor*N})}\leq C_r\,,\quad \mathrm{a.s.}
% \end{equation}
% for all $m\in\mathbb{N}$, where $\theta_m$ comes from Algorithm \ref{alg:GE}.  Set $\eta_m=\eta$, then there exists a constant $C$ that only depends on $C_r,C_f,C_g$ such that if $\eta\leq C\log^{1/2}(N)/N$, then for any $M>0$
% \begin{equation}\label{eqn:gradient_F_bound_2}
% \frac{1}{M+1}\sum^M_{m=0}\mathbb{E}\left(\left|\nabla F(\theta_m)\right|^2\right)\leq \frac{2C_rF(\theta_0)}{\eta (M+1) }\,.
% \end{equation}
% In particular, when $M=\Theta\left(\frac{N}{\log^{1/2}(N)\epsilon^2}\right)$, we have $\min_{0\leq m\leq M}\mathbb{E}\left(\left|\nabla F(\theta_m)\right|\right)\leq \epsilon$.
% \end{theorem}
% We put the proof of the above theorem in Appendix \ref{sec:pf_thm:algorithm_VGD}. To ensure $\min_{0\leq m\leq M}\mathbb{E}\left(\left|\nabla_\theta F(\theta_m)\right|\right)\leq \epsilon$ using Algorithm \ref{alg:VGD}, the number of gradient ($\nabla g_n$)  and value ($g_n$) calculations is $\Theta(M)=\Theta(N/(\log^{1/2}(N)\epsilon^2))$, which is always smaller than the cost of GD by a log factor in $N$.
\bibliography{references}
\bibliographystyle{icml2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

%\section{Derivation of VMC gradient for real \texorpdfstring{$\wf$}{Lg}}\label{sec:deri_gradient}
%Here we derive \eqref{eqn:gradient_L_VMC}. See \citep[Appendix E]{Jeff_2021} for a derivation that includes the complex case.
%\begin{align}
%\nabla_{\theta} L(\theta) & =\int_{\mathbb{R}^d} \nabla_{\theta} \mathcal{E}_{\theta}(x) p_{\theta}(x) d x+\int_{\mathbb{R}^d} \mathcal{E}_{\theta}(x) \nabla_{\theta} p_{\theta}(x) d x=A+B, \label{eqn:gradient_derive}\\
%A & =\mathbb{E}_{X\sim p_\theta}\left[\nabla_{\theta} \mathcal{E}_{\theta}(X)\right], \label{eqn:gradient_A}\\
%B & =\mathbb{E}_{X\sim p_\theta}\left[\mathcal{E}_{\theta}(X) \frac{\nabla_{\theta} p_{\theta}(X)}{p_{\theta}(X)}\right], \label{eqn:gradient_B}
%\end{align}
%We observe that $A=0$ for real $\wf_{\theta}$. Indeed,
%\begin{equation}\label{eqn:E}
%\nabla_{\theta} \mathcal{E}_{\theta}=\nabla_{\theta} \frac{H \wf_{\theta}}{\wf_{\theta}}=\frac{\wf_{\theta} \nabla_{\theta} H \wf_{\theta}-H \wf_{\theta} \nabla_{\theta} \wf_{\theta}}{\wf_{\theta}^{2}}\,.
%\end{equation}
%So, because $H$ is symmetric:
%\[
%A_{i}=\mathbb{E}_{X\sim p_\theta}\left[\partial_{\theta_{i}} \mathcal{E}_{\theta}\right]=\frac{1}{\left\|\wf_{\theta}\right\|^{2}}\left(\int_{\mathbb{R}^d} \wf_{\theta} H\left(\partial_{\theta_{i}} \wf_{\theta}\right)-\int_{\mathbb{R}^d} H \wf_{\theta} \partial_{\theta_{i}} \wf_{\theta}\right)=0,
%\]
%where $\theta=\left(\theta_{i}\right)_{i}$. So $A=0$ and we go on to compute $B$. Since $p_{\theta}$ is only known up to a $\theta$-dependent constant we need to rewrite \eqref{eqn:gradient_B} in terms of $\wf_{\theta}$:
%\begin{equation}\label{eqn:gp_over_p}
%\begin{aligned}
%\frac{\nabla_{\theta} p_{\theta}(X)}{p_{\theta}(X)} & =\nabla_{\theta} \log p_{\theta}(X) \\
%& =2 \nabla_{\theta} \log \wf_{\theta}(X)-2 \nabla_{\theta} \log \left\|\wf_{\theta}\right\| \\
%& =2 \frac{\nabla_{\theta} \wf_{\theta}(X)}{\wf_{\theta}(X)}-v_{\theta},
%\end{aligned}
%\end{equation}
%where $v_{\theta}$ does not depend on $X$. Now note that $\nabla_{\theta} p_{\theta} / p_{\theta}$ is centered:
%\[
%\mathbb{E}_{X\sim p_\theta}\left[\frac{\nabla_{\theta} p_{\theta}(X)}{p_{\theta}(X)}\right]=\nabla_{\theta} \int_{\mathbb{R}^d} p_{\theta}(x)dx=0 .
%\]
%This determines $v_{\theta}$ in \eqref{eqn:gp_over_p}, which we substitute into \eqref{eqn:gradient_B} to get
%\[
%\begin{aligned}
%B & =2 \mathbb{E}_{X\sim p_\theta}\left[\mathcal{E}_{\theta}(X) \frac{\nabla_{\theta} \wf_{\theta}(X)}{\wf_{\theta}(X)}\right]-2 \mathbb{E}_{X\sim p_\theta}\left[\mathcal{E}_{\theta}(X)\right] \mathbb{E}_{X\sim p_\theta}\left[\frac{\nabla_{\theta} \wf_{\theta}(X)}{\wf_{\theta}(X)}\right] \\
%& =2 \mathbb{E}_{X\sim p_\theta}\left[\left(\mathcal{E}_{\theta}(X)-L(\theta)\right) \frac{\nabla_{\theta} \wf_{\theta}(X)}{\wf_{\theta}(X)}\right] .
%\end{aligned}
%\]

\section{Proofs of gradient estimators}
\label{gradproofs}
\begin{proof}[Proof of \eqref{eqn:gradient_L_VMC}]

Let $q_\theta=|\wf_\theta|^2$, $\Lq_\theta(x)=\log q_\theta(x)$, and $Q_\theta=\|\wf_\theta\|^2$. Write \eqref{eqn:VMC_L} as
\[L(\theta)=\ifrac{\ol{q_\theta}{\localE}}{Q_\theta},\]
Then by the product formula,
\begin{equation}
\begin{aligned}
\nabla_\theta L(\theta)
&=
\frac{\nabla\ol{q_\theta}{\lE_\theta}}{Q_\theta}-\frac{\ol{q_\theta}{\lE_\theta}\nabla Q_\theta}{Q^2_\theta}=:A-B.
\end{aligned}
\label{AB}
\end{equation}
Continuing, we have
\begin{equation}
\label{eqA}
\begin{aligned}
A
%= \frac{\nabla\ol{q}{\lE}}{\|q\|_1}
&=\ol{\lE}{\nabla_\theta q_\theta}/Q_\theta+\ol{q_\theta}{\nabla\lE}/Q_\theta
\\&= \ol{\lE_\theta}{\frac{\nabla_\theta q_\theta}{q_\theta}\frac{q_\theta}{Q_\theta}}+\ol{\frac{q_\theta}{Q_\theta}}{\nabla_\theta\lE_\theta}
\\&= \EE_{p_\theta}[\lE_\theta\nabla_\theta\log q_\theta]+\EE_{p_\theta}[\nabla\lE_\theta].
\end{aligned}
\end{equation}
and
\begin{equation}
\label{eqB}
\begin{aligned}
B
&= \ol{\frac{q_\theta}{Q_\theta}}{\lE_\theta}\frac{\nabla_\theta Q_\theta}{Q_\theta}
\\&= L(\theta)\ol{\frac{q_\theta}{Q_\theta}}{\frac{\nabla_\theta q_\theta}{q_\theta}}
%\\&= \ol{p}{\lE}\ol{p}{\frac{\nabla q}{q}}
= L(\theta)\EE_{p_\theta}[\nabla_\theta\log q_\theta].
\end{aligned}
\end{equation}
Substitute \eqref{eqA}, \eqref{eqB} into \eqref{AB} to obtain 
\begin{equation}\label{eqn:gradient_L_VMC_true}
\nabla_\theta L(\theta)
=\EE_{X\sim p_\theta}\left[(\lE_\theta(X)-L(\theta))\nabla_\theta\Lq_\theta(X)+\nabla_\theta\lE_\theta(X)\right]\,.
\end{equation}



Since $q_\theta=\wf_\theta^2$, we have $\Lq_\theta=2\Lpsi_\theta$, so by \eqref{eqn:gradient_L_VMC_true} we have
\[\nabla_\theta L(\theta)
=\EE_{X\sim p_\theta}[2\big(\lE_\theta(X)-L(\theta))\nabla_\theta\Lpsi_\theta(X)+\nabla_\theta\lE_\theta(X)\Big].\]
It remains to show that $\EE_{X\sim p_\theta}[\partial_i\lE_\theta(X)]=0$ where $\partial_i$ is differentiation with respect to $\theta_i$. Write
\begin{equation}
%\label{eqn: use \cref{genvmclem} andE}
\partial_i\mathcal{E}_{\theta}=\partial_i\frac{H \wf_{\theta}}{\wf_{\theta}}=\frac{\partial_i H \wf_{\theta}\cdot\wf_{\theta}-H \wf_{\theta}\cdot \partial_i\wf_{\theta}}{\wf_{\theta}^{2}}\,.
\end{equation}
So, because $H$ is symmetric:
\[
\mathbb{E}_{X\sim p_\theta}\left[\partial_i\mathcal{E}_{\theta}\right]=\frac{\ol{H\partial_i\wf_\theta}{\wf_\theta}-\ol{H\wf_\theta}{\partial_i\wf_\theta}}{\|\wf_\theta\|^2}=0,
\]
\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:unbiased_gradient}]
\[
\begin{aligned}
G(\theta)  =&\frac{2}{n-1} \sum_{i=1}^{n} \mathcal{E}_{\theta}\left(X_{i}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{i}\right)\\
&-\frac{2}{n(n-1)} \sum_{j=1}^{n} \sum_{i=1}^{n} \mathcal{E}_{\theta}\left(X_{j}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{i}\right) \\
 =&\frac{2}{n} \sum_{i=1}^{n} \mathcal{E}_{\theta}\left(X_{i}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{i}\right)\\
 &-\frac{2}{n^{2}-n} \sum_{i \neq j} \mathcal{E}_{\theta}\left(X_{i}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{j}\right) .
\end{aligned}
\]
Here, the terms $i=j$ in the expansion of the rightmost term cancel with the first sum. But now each sum is the average of terms with the correct expectation $\left(2 \mathbb{E}_{X\sim p_\theta}\left[\mathcal{E}_{\theta}(X) \nabla_{\theta} \log \wf_{\theta}(X)\right]\right.$ and $2 \mathcal{L}_{\theta} \mathbb{E}_{X\sim p_\theta} \nabla_{\theta} \log \wf_{\theta}(X)$ respectively $)$.
\end{proof}

\section{The directionally unbiased gradient estimator for supervised learning}\label{sec:minibath_sup}
 Given $n\ge2$ i.i.d. samples $X_i\sim\rho$. We write $f_i=f_\theta(X_i)$, $g_i=g(X_i)$, and $\nabla f_i=\nabla_\theta f_\theta(X_i)$.
 %, and use the counting measure to define
 \begin{equation}
 \begin{aligned}
 \ol{f}{g}_n=\frac1n\sum_{i=1}^nf_i g_i,
 \qquad
 \|f\|_n^2=\frac1n\sum_{i=1}^nf_i^2.
 \end{aligned}
 \end{equation}
 \begin{lemma}\label{unbiasedsupervised}
 Given samples $X_1,\ldots,X_n$, let
 \begin{align}
 G_n&=\frac1{n-1}\sum_{j=1}^n a_j\nabla f_j,\label{eqn:G_n}
 \\
 %a_j&=(\tfrac1r\sum_{i=1}^r f_i^2)g_j-(\frac1r\sum_{i=1}^rg_if_i)f_j.
 a_j&=-\|f\|_n^2\:g_j+\ol{g}{f}_n\:f_j.
 \end{align}
 Then $G_r$ is an unbiased estimator for $G=\|f_\theta\|^3\nabla_\theta L(\theta)$.
 \end{lemma}
 \begin{proof}[Proof of Lemma \ref{unbiasedsupervised}]
 \cref{eqn:G_n} states that
 \[G=-\|f_\theta\|^2\ol{g}{\nabla_\theta f_\theta}+\ol{g}{f_\theta}\ol{f_\theta}{\nabla_\theta f_\theta}.\]
 For each $i\neq j$ we have an unbiased estimator for $G$ given by
 \[-|f_i|^2g_j\nabla f_j+g_if_if_j\nabla f_j.\]
 Taking the average over all pairs $i\neq j$ gives another unbiased estimator for $G$:
 \[-\frac1{n(n-1)}\sum_{i\neq j}(f_i^2g_j\nabla f_j-g_if_if_j\nabla f_j).\]
 We may add the terms $i=j$ without changing the value because all such terms evaluate to $0$.
 \end{proof}


\section{Proof of Theorem \ref{thm:complexity_VMC}}\label{sec:pf_of_thm:complexity_VMC}
To prove Theorem \ref{thm:complexity_VMC}, we first show the following lemma:
\begin{lemma}\label{lem:prior_VMC}
Under the Assumption \ref{assum_g_VMC}, there exists uniform constant $C'$ such that for any $\theta,\widetilde{\theta}\in\mathbb{R}^d$,
\begin{equation}\label{eqn:gradient_bound_vmc}
\left|\nabla_\theta L(\theta)\right|\leq 4C^2_\wf\,,
\end{equation}
\begin{equation}\label{eqn:gradient_Lip_vmc}
\left|\nabla_\theta L(\theta)-\nabla_\theta L\left(\widetilde{\theta}\right)\right|\leq C'(C^4_\wf+1)|\theta-\widetilde{\theta}|\,,
\end{equation}
and
\begin{equation}\label{eqn:variance_bound_vmc}
\mathbb{E}_{\{X_i\}^n_{i=1}}\left(\left|G(\theta)\right|^2\right)\leq |\nabla_\theta L(\theta)|^2+\frac{C'(C^4_\wf+1)}{n}\,,
\end{equation}
where $G(\theta)$ is defined in \eqref{eqn:VMCgradient_estimator}.
\end{lemma}
The above lemma is important for the proof of Theorem \ref{thm:complexity_VMC}. First, that inequality \eqref{eqn:variance_bound_vmc} gives an upper bound for the gradient estimator. Using this upper bound, we can show that each iteration of Algorithm \ref{alg:VMC} is close to the classical gradient descent when the learning rate $\eta$ is small enough. Second, \eqref{eqn:gradient_Lip_vmc} means the hessian of $L$ is bounded. This ensures the classical gradient descent has a faster convergence rate to the first-order stationary point, which further implies the fast convergence of Algorithm \ref{alg:VMC}.

We first prove Lemma \ref{lem:prior_VMC}:
\begin{proof}[Proof of Lemma \ref{lem:prior_VMC}] Define
\[
Z_\theta=\sqrt{\int_{\Omega}\left|\wf_{\theta}(x)\right|^{2} d x}\,.
\]

First, to prove \eqref{eqn:gradient_bound_vmc}, using \eqref{eqn:gradient_L_VMC}, we have
\[
\begin{aligned}
\left|\nabla_\theta L(\theta)\right|&=2\mathbb{E}_{X\sim p_\theta}\left|\left(\mathcal{E}_{\theta}(X)-L(\theta)\right) \frac{\nabla_{\theta} \wf_{\theta}(X)}{\wf_{\theta}(X)}\right|\\
&\leq 2\left(\mathbb{E}_{X\sim p_\theta}\left|\mathcal{E}_{\theta}(X)-L(\theta)\right|^2\right)^{1/2} \left(\mathbb{E}_{X\sim p_\theta}\left|\frac{\nabla_{\theta} \wf_{\theta}(X)}{\wf_{\theta}(X)}\right|^2\right)^{1/2}\leq 4C^{2}_\wf\,,
\end{aligned}
\]
where we use H\"older's inequality in the first inequality, \eqref{eqn_value_gnbound_vmc} and the second inequality of  \eqref{eqn_Gradient_gnbound_vmc} in the last inequality.

Next, to prove \eqref{eqn:gradient_Lip_vmc}, for fixed $\theta\in\Omega$, we define $R(w)=\frac{Z_w}{Z_\theta}$ and  $r(t)=\frac{Z_{\theta+t\left(\widetilde{\theta}-\theta\right)}}{Z_\theta}$. Using the second inequality of \eqref{eqn_Gradient_gnbound_vmc}, we obtain
\[
\begin{aligned}
\left|\nabla_w R(w)\right|=&\left|\frac{\int \nabla_w \wf_w(x)\wf_w(x)dx}{\sqrt{\int |\wf_w(x)|^2dx}\sqrt{\int |\wf_\theta(x)|^2dx}}\right|\\
\leq &\frac{\sqrt{\int |\nabla_w \wf_w(x)|^2dx}\sqrt{\int |\wf_w(x)|^2dx}}{\sqrt{\int |\wf_w(x)|^2dx}\sqrt{\int |\wf_\theta(x)|^2dx}}\\
\leq &\sqrt{\frac{\int_{\Omega}\left|\nabla_w \wf_w(x)\right|^2dx}{\int_{\Omega}\left| \wf_w(x)\right|^2dx}}\frac{Z_w}{Z_\theta}\\
= &\left(\mathbb{E}_{X\sim p_w}\left(\left|\frac{\nabla_w  \wf_w(X)}{\wf_w(X)}\right|^2\right)\right)^{1/2}\frac{Z_w}{Z_\theta}\leq &C_{\wf}R(w)\,.
\end{aligned}
\]
where we use H\"older's inequality in the first inequality and the second inequality of  \eqref{eqn_Gradient_gnbound_vmc} in the last inequality.

This implies
\[
\frac{d}{dt}r(t)\leq \left|\nabla_w R(w)|_{w=\theta+t\left(\widetilde{\theta}-\theta\right)}\right|\left|\widetilde{\theta}-\theta\right|\leq C_\wf \left|\widetilde{\theta}-\theta\right| r(t)\,.
\]
Combining this inequality with the fact that $r(0)=1$, we have
\[
\frac{Z_{\theta+t\left(\widetilde{\theta}-\theta\right)}}{Z_\theta}-1=r(t)-1\leq \exp\left(C_\wf \left|\widetilde{\theta}-\theta\right| t\right)-1
\]
for any $t\in[0,1]$, and
\begin{equation}\label{eqn:ratio_Lip_VMC}
\frac{Z_{\theta+\left(\widetilde{\theta}-\theta\right)}}{Z_\theta}-1\leq C_{\wf}\exp\left(C_{\wf}|\widetilde{\theta}-\theta|\right)|\widetilde{\theta}-\theta|
\end{equation}
by the Gr\"onwall's inequality.

Now, we are ready to show \eqref{eqn:gradient_Lip_vmc}. Using the formula of $\mathcal{E}(\theta)$ (equation \eqref{eqn:deflocalE}), we write 
\begin{equation}\label{eqn:gL_lip_seperation}
\begin{aligned}
&\left|\nabla_\theta L(\theta)-\nabla_\theta L\left(\widetilde{\theta}\right)\right|\\
\leq &2\left|\frac{1}{Z^2_\theta}\int_{\Omega} H\wf_\theta(x)\nabla_\theta \wf_\theta(x)dx-\frac{1}{Z^2_{\widetilde{\theta}}}\int_{\Omega} H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\\
&+2\left|\frac{1}{Z^2_\theta}\int_{\Omega} L(\theta)\wf_\theta(x)\nabla_\theta \wf_\theta(x)dx-\frac{1}{Z^2_{\widetilde{\theta}}}\int_{\Omega} L\left(\widetilde{\theta}\right)\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\,.
\end{aligned}
\end{equation}
We first deal with the first term. For simplicity, we first assume $|\widetilde{\theta}-\theta|\leq \epsilon$ for an arbitrary $\epsilon>0$:
\begin{equation}\label{eqn:gL_lip_first} 
\begin{aligned}
&\left|\frac{1}{Z^2_\theta}\int_{\Omega} H\wf_\theta(x)\nabla_\theta \wf_\theta(x)dx-\frac{1}{Z^2_{\widetilde{\theta}}}\int_{\Omega} H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\\
\leq &\underbrace{\left|\frac{1}{Z^2_\theta}\int_{\Omega} \left(H\wf_\theta(x)\nabla_\theta \wf_\theta(x)-H\wf_{\widetilde{\theta}}(x)\nabla_{\theta} \wf_{\widetilde{\theta}}(x)\right)dx\right|}_{\mathrm{(I)}}\\
&+\underbrace{\left|\left(\frac{1}{Z^2_{\widetilde{\theta}}}-\frac{1}{Z^2_\theta}\right)\int_{\Omega} H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|}_{\mathrm{(II)}}\,.
\end{aligned}
\end{equation}
\begin{itemize}
    \item For (I), we have
    \begin{align*}
    &\left|\frac{1}{Z^2_\theta}\int_{\Omega} \left(H\wf_\theta(x)\nabla_\theta \wf_\theta(x)-H\wf_{\widetilde{\theta}}(x)\nabla_{\theta} \wf_{\widetilde{\theta}}(x)\right)dx\right|\\
    \leq & \left|\widetilde{\theta}-\theta\right|\int^1_{0}\left|\frac{Z^2_{\theta+t\left(\widetilde{\theta}-\theta\right)}}{Z^2_\theta}\right|\left|\frac{\nabla H\wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(x)\nabla_{\theta} \wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(x)}{Z^2_{\theta+t\left(\widetilde{\theta}-\theta\right)}}\right|dt\\
    &+\left|\widetilde{\theta}-\theta\right|\int^1_{0}\left|\frac{Z^2_{\theta+t\left(\widetilde{\theta}-\theta\right)}}{Z^2_\theta}\right|\left|\frac{H\wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(x)\left\|\mathsf{H}_{\theta} \wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(x)\right\|_2}{Z^2_{\theta+t\left(\widetilde{\theta}-\theta\right)}}\right|dt\\
    \leq & (C_{\wf}\exp\left(C_{\wf}\epsilon\right)\epsilon+1)^2\left|\widetilde{\theta}-\theta\right|\int^1_{0}\mathbb{E}_{X\sim p_{\theta+t\left(\widetilde{\theta}-\theta\right)}}\left(\left|\frac{\nabla H\wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(X)\nabla_{\theta} \wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(X)}{\left|\wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(X)\right|^2}\right|\right)dt\\
    &+(C_{\wf}\exp\left(C_{\wf}\epsilon\right)\epsilon+1)^2\left|\widetilde{\theta}-\theta\right|\int^1_{0}\mathbb{E}_{X\sim p_{\theta+t\left(\widetilde{\theta}-\theta\right)}}\left(\left|\frac{H\wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(x)\left\|\mathsf{H}_{\theta} \wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(x)\right\|_2}{\left|\wf_{\theta+t\left(\widetilde{\theta}-\theta\right)}(X)\right|^2}\right|\right)dt\\
    \leq & 2(C_{\wf}\exp\left(C_{\wf}\epsilon\right)\epsilon+1)^2C^2_\wf\left|\widetilde{\theta}-\theta\right|\,,
    \end{align*}
    where we use fundamental theorem of calculus and triangle inequality for integral in the first inequality, \eqref{eqn:ratio_Lip_VMC} in the second inequality, H\"older's inequality, \eqref{eqn_value_gnbound_vmc}-\eqref{eqn_Hessian_gnbound_vmc} in the last inequality.
    \item For (II), we have
    \begin{equation}
    \begin{aligned}
    &\left|\left(\frac{1}{Z^2_{\widetilde{\theta}}}-\frac{1}{Z^2_\theta}\right)\int_{\Omega} H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\\
    \leq &\left|\frac{Z^2_{\widetilde{\theta}}}{Z^2_\theta}-1\right|\left|\int_{\Omega} \frac{H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)}{Z^2_{\widetilde{\theta}}}dx\right|\\
    \leq &(C_{\wf}\exp\left(C_{\wf}\epsilon\right)\epsilon+1)C^3_{\wf}\exp\left(C_{\wf}\epsilon\right)\left|\widetilde{\theta}-\theta\right|\,,
    \end{aligned}
    \end{equation}
    where we use $|\widetilde{\theta}-\theta|\leq \epsilon$, H\"older's inequality, \eqref{eqn_value_gnbound_vmc}, the second inequality of  \eqref{eqn_Gradient_gnbound_vmc}, and \eqref{eqn:ratio_Lip_VMC}  in the last inequality.
\end{itemize}
Plugging these two terms into \eqref{eqn:gL_lip_first}, we have
\[
\begin{aligned}
&\left|\frac{1}{Z^2_\theta}\int_{\Omega} H\wf_\theta(x)\nabla_\theta \wf_\theta(x)dx-\frac{1}{Z^2_{\widetilde{\theta}}}\int_{\Omega} H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\\
\leq &C^2_\wf(C_{\wf}\exp\left(C_{\wf}\epsilon\right)\epsilon+1)\left(C_{\wf}\exp\left(C_{\wf}\epsilon\right)+2C_{\wf}\exp\left(C_{\wf}\epsilon\right)\epsilon+2\right)\left|\widetilde{\theta}-\theta\right|\,.
\end{aligned}
\]
for any $|\theta-\widetilde{\theta}|\leq \epsilon$. Since $\epsilon$ is arbitrary, we let $\epsilon\rightarrow0$ and use triangle inequality to obtain
\begin{equation}\label{eqn:gL_lip_first_bound}
\begin{aligned}
&\left|\frac{1}{Z^2_\theta}\int_{\Omega} H\wf_\theta(x)\nabla_\theta \wf_\theta(x)dx-\frac{1}{Z^2_{\widetilde{\theta}}}\int_{\Omega} H\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\\
\leq &C^2_\wf(C_\wf+2)\left|\widetilde{\theta}-\theta\right|\,.
\end{aligned}
\end{equation}
for any $\widetilde{\theta},\theta\in\mathbb{R}^d$.

To deal with the second term of \eqref{eqn:gL_lip_seperation}, we first use \eqref{eqn_value_gnbound_vmc} and \eqref{eqn:gradient_bound_vmc} to obtain
\[
L\left(\theta+t\left(\widetilde{\theta}-\theta\right)\right)\leq C_\wf,\quad \left|L(\theta)-L\left(\widetilde{\theta}\right)\right|\leq 4C^2_\wf\left|\widetilde{\theta}-\theta\right|,\quad\forall t\in[0,1]\,.
\]
Using similar arguments as before, we can show that
\begin{equation}\label{eqn:gL_lip_second_bound}
\left|\frac{1}{Z^2_\theta}\int_{} L(\theta)\wf_\theta(x)\nabla_\theta \wf_\theta(x)dx-\frac{1}{Z^2_{\widetilde{\theta}}}\int_{} L\left(\widetilde{\theta}\right)\wf_{\widetilde{\theta}}(x)\nabla_\theta \wf_{\widetilde{\theta}}(x)dx\right|\leq C(C^4_\wf+1)|\widetilde{\theta}-\theta|\,,
\end{equation}
where $C$ is a uniform constant. Plugging \eqref{eqn:gL_lip_first_bound} and \eqref{eqn:gL_lip_second_bound} into \eqref{eqn:gL_lip_seperation}, we prove \eqref{eqn:gradient_Lip_vmc}\,.

Finally, to prove \eqref{eqn:variance_bound_vmc}, noticing
\[
G(\theta)=\frac{2}{n}\sum_{i=1}^{n} \mathcal{E}_{\theta}\left(X_{i}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{i}\right)-\frac{2}{n^{2}-n} \sum_{i \neq j} \mathcal{E}_{\theta}\left(X_{i}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{j}\right)\,,
\]
and
\[
\mathbb{E}_{\{X_i\}^n_{i=1}}\left(G(\theta)\right)=\nabla_\theta L(\theta)\,,
\]
we have
\[
\begin{aligned}
  &\mathbb{E}_{\{X_i\}^n_{i=1}}\left(\left|G(\theta)\right|^2\right)\\
  \leq &|\nabla_\theta L(\theta)|^2+\frac{8}{n^2}\sum^n_{i=1}\mathbb{E}_{X_i\sim p_\theta}\left(\left|\mathcal{E}_{\theta}\left(X_{i}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{i}\right)-\mathbb{E}_{X\sim p_\theta}\left[\mathcal{E}_{\theta}(X) \nabla_{\theta} \log \wf_{\theta}(X)\right]\right|^2\right)\\
  &+\frac{8}{(n^2-n)^2}\mathbb{E}\left(\sum_{i\neq j}\mathcal{E}_{\theta}\left(X_{i_1}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{j_1}\right)-\mathcal{L}_{\theta} \mathbb{E}_{X\sim p_\theta} \nabla_{\theta} \log \wf_{\theta}(X)\right)^2\\
  \leq &|\nabla_\theta L(\theta)|^2+\frac{8}{n}\mathbb{E}_{X\sim p_\theta}\left|\mathcal{E}_{\theta}\left(X\right) \nabla_{\theta} \log \wf_{\theta}\left(X\right)-\mathbb{E}_{X\sim p_\theta}\left[\mathcal{E}_{\theta}(X) \nabla_{\theta} \log \wf_{\theta}(X)\right]\right|^2\\
  &+\frac{8}{(n^2-n)^2}\left(\sum_{i_1=i_2\,\text{or}\,j_1=j_2} 1\right)\mathbb{E}_{(X_1,X_2)\sim p^{\otimes 2}_\theta}\left|\mathcal{E}_{\theta}\left(X_{1}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{2}\right)- \mathcal{L}_{\theta} \mathbb{E}_{X\sim p_\theta} \nabla_{\theta} \log \wf_{\theta}(X)\right|^2\\
  \leq &|\nabla_\theta L(\theta)|^2+\frac{8}{n}\mathbb{E}_{X\sim p_\theta}\left|\mathcal{E}_{\theta}\left(X\right) \nabla_{\theta} \log \wf_{\theta}\left(X\right)\right|^2+\frac{C}{n}\mathbb{E}_{(X_1,X_2)\sim p^{\otimes 2}_\theta}\left|\mathcal{E}_{\theta}\left(X_{1}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{2}\right)\right|^2\\
\end{aligned}\,,
\]
where we use $X_{i}$ and $X_j$ are independent in the first two inequalities. 
Using \eqref{eqn_value_gnbound_vmc} and the second inequality of  \eqref{eqn_Gradient_gnbound_vmc}, it is straightforward to show:
\[
\mathbb{E}_{X\sim p_\theta}\left|\mathcal{E}_{\theta}\left(X\right) \nabla_{\theta} \log \wf_{\theta}\left(X\right)\right|^2\leq \left(\mathbb{E}_{X\sim p_\theta}\left(\left|\frac{H \wf_\theta(X)}{\wf_\theta(X)}\right|^4\right)\right)^{1/2}\left(\mathbb{E}_{X\sim p_\theta}\left(\left|\frac{\nabla_\theta \wf_\theta(X)}{\wf_\theta(X)}\right|^4\right)\right)^{1/2}\leq C^4_\wf\,,
\]
and
\[
\mathbb{E}_{(X_1,X_2)\sim p^{\otimes 2}_\theta}\left|\mathcal{E}_{\theta}\left(X_{1}\right) \nabla_{\theta} \log \wf_{\theta}\left(X_{2}\right)\right|^2\leq \mathbb{E}_{X\sim p_\theta}\left(\left|\frac{H \wf_\theta(X)}{\wf_\theta(X)}\right|^2\right)\mathbb{E}_{X\sim p_\theta}\left(\left|\frac{\nabla_\theta \wf_\theta(X)}{\wf_\theta(X)}\right|^2\right)\leq C^4_\wf\,.
\]
This concludes the proof of \eqref{eqn:variance_bound_vmc}.
%\LL{Again worth writing down the details of the calculation later.} ~\ZD{Done.}
\end{proof}

Now, we are ready to prove Theorem \ref{thm:complexity_VMC}:
\begin{proof}[Proof of Theorem \ref{thm:complexity_VMC}]
Denote the probability filtration 
\[
\mathcal{F}_m=\sigma(X^j_i,1\leq i\leq n, 1\leq j\leq m)\,.
\]
According to the algorithm, we have
\[
\theta_{m+1}=\theta_m-\eta_m G_m\,.
\]
Plugging $\theta_{m+1}$ into $L(\theta)$, we have
\[
L(\theta_{m+1})\leq L(\theta_m)-\eta_m\nabla_\theta L(\theta_m)\cdot G_m+\frac{C\eta_m^2}{2}|G_m|^2
\]
where we use \eqref{eqn:gradient_Lip_vmc} and $C$ is a constant that only depends on $C_\wf$. This implies
\[
\begin{aligned}
\mathbb{E}\left(L(\theta_{m+1})|\mathcal{F}_{m-1}\right)\leq &L(\theta_m)-\eta_m\left|\nabla_\theta L(\theta_m)\right|^2+\frac{C\eta_m^2}{2}\mathbb{E}(|G_m|^2|\mathcal{F}_{m-1})
\end{aligned}
\]
where we use $\mathbb{E}_{\{X_i\}^n_{i=1}}\left(G(\theta)\right)=\nabla_\theta L(\theta)$. Furthermore, using \eqref{eqn:variance_bound_vmc} and $\eta_m<\frac{1}{C}$, we have 
\[
\begin{aligned}
\mathbb{E}\left(L(\theta_{m+1})|\mathcal{F}_{m-1}\right)\leq &L(\theta_m)-\left(\eta_m-C\eta_m^2/2\right)\left|\nabla_\theta L(\theta_m)\right|^2+\frac{C\eta_m^2}{2n}\\
\leq &L(\theta_m)-\frac{\eta_m}{2}\left|\nabla_\theta L(\theta_m)\right|^2+\frac{C\eta_m^2}{2n}
\end{aligned}
\]
Taking the average in $m=1,2,3,\cdots,M-1$, we prove \eqref{eqn:gradient_F_bound_vmc}.
\end{proof}

\section{Proof of Theorem \ref{thm:main_result_alg1}}\label{sec:pf_of_thm:main_result_alg1}
In this section, we prove Theorem \ref{thm:main_result_alg1}. We first show the following lemma: 
\begin{lemma}\label{lem:svsl_2} Under the Assumption \ref{assum_g_scale_inva}, there exists a constant $C'$ that only depends on $C_g$ such that for any $\theta,\widetilde{\theta}\in\mathbb{R}^d$,
\begin{equation}\label{eqn:gradient_Lip_2}
\left|\nabla_\theta L(\theta)-\nabla_\theta L\left(\widetilde{\theta}\right)\right|\leq C'|\theta-\widetilde{\theta}|\,.
\end{equation}
and
\begin{equation}\label{eqn:variance_bound_sup}
\mathbb{E}_{\{X_i\}^n_{i=1}}\left(\left|G(\theta)\right|^2\right)\leq \frac{Z^6_\theta}{Z^6}\left(|\nabla_\theta L(\theta)|^2+\frac{C'}{n}\right)\,,
\end{equation}
where $G(\theta)$ is defined in \eqref{eqn:G_r_theta}.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lem:svsl_2}] The proof is very similar to the proof of Lemma \ref{lem:prior_VMC} after setting $H\wf_\theta(x)=g(x)\left(\functional{g(x)}{\wf_\theta(x)}\right)$.
\end{proof}

Now, we are ready to prove Theorem \ref{thm:main_result_alg1}.
\begin{proof}[Proof of Theorem \ref{thm:main_result_alg1}] Without loss of generality, let $0\leq m\leq N$. According to the algorithm, we have
\[
\theta_{m+1}=\theta_m-\eta_m G_m\,.
\]
Plugging $\theta_{m+1}$ into $L(\theta)$, we have
\[
L(\theta_{m+1})\leq L(\theta_m)-\eta_m\nabla_\theta L(\theta_m)\cdot G_m+\frac{C\eta_m^2}{2}|G_m|^2
\]
where we use \eqref{eqn:gradient_Lip_2}. Here $C$ is a constant that only depends on $C_g$. This implies
\begin{equation}\label{eqn:L_bound}
\begin{aligned}
\mathbb{E}\left(L(\theta_{m+1})|\mathcal{L}_{m-1}\right)\leq &L(\theta_m)-\eta_m\left|\nabla_\theta L(\theta_m)\right|^2\frac{Z^3_m}{\widetilde{Z}^3_{\lfloor m/N\rfloor*N}}\\
&+\frac{C\eta^2_m}{2}\mathbb{E}(|G_m|^2|\mathcal{L}_{m-1})
\end{aligned}
\end{equation}
Using \eqref{eqn:variance_bound_sup} and \eqref{eqn:distance_bound},
\begin{equation}\label{eqn:bound_fourth_term}
\mathbb{E}(|G_m|^2|\mathcal{L}_{m-1})\leq C\left(|\nabla_\theta L(\theta_m)|^2+\frac{C'}{n}\right)\,,
\end{equation}
where $C$ is a constant depends on $C_g$ and $C_f$.
 
Plugging \eqref{eqn:bound_fourth_term} into \eqref{eqn:L_bound}, using \eqref{eqn:distance_bound}, and choosing $\eta_m$ small enough, we have
\begin{equation}\label{eqn:bound_f_2}
\mathbb{E}\left(L(\theta_{m+1})\right)\leq L(\theta_m)-\frac{\eta_m}{2C^2_r}\left|\nabla_\theta L(\theta_m)\right|^2+\frac{C\eta^2_m}{n}\\
\end{equation}
Taking the average in $m=1,2,3,\cdots,M-1$, we prove \eqref{eqn:gradient_F_bound}.
\end{proof}

\section{Lipschitz of the $Z_\theta$}\label{sec:svsl_1}
\begin{lemma}\label{lem:svsl_1} Under the Assumption \ref{assum_g_scale_inva}, given $\theta,\widetilde{\theta}\in\mathbb{R}^d$ and assume 
\[
\left|\widetilde{\theta}-\theta\right|\leq C_r\,,
\]
then there exists a constant $C'$ that only depends on $C_g$ and $C_r$ such that
\begin{equation}\label{eqn:ratio_Lip_1}
\frac{\|f_{\widetilde{\theta}}\|}{\|f_{\theta}\|}\leq \exp(C'C_r)\,.
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:svsl_1}] Fixing $\theta,\widetilde{\theta}$, we define
\[
R(t)=\frac{\left\|f_{\theta+t(\widetilde{\theta}-\theta)}\right\|^2}{\|f_{\theta}\|^2}\,.
\]
Then
\[
\begin{aligned}
|R'(t)|\leq &\left|\frac{2\functional{\nabla_\theta f_{\theta+t(\widetilde{\theta}-\theta)}}{f_{\theta+t(\widetilde{\theta}-\theta)}}}{\|f_{\theta}\|^2}\right||\widetilde{\theta}-\theta|\\
\leq &\left|\frac{2\functional{\nabla_\theta f_{\theta+t(\widetilde{\theta}-\theta)}}{f_{\theta+t(\widetilde{\theta}-\theta)}}}{\left\|f_{\theta+t(\widetilde{\theta}-\theta)}\right\|^2}\right|\left|\frac{\left\|f_{\theta+t(\widetilde{\theta}-\theta)}\right\|^2}{\|f_{\theta}\|^2}\right||\widetilde{\theta}-\theta|\\
\leq &C|\widetilde{\theta}-\theta|R(t)\,,
\end{aligned}
\]
where we use \eqref{eqn_value_gnbound} and \eqref{eqn_Gradient_gnbound} in the last inequality. Because $R(0)$=1, using Gr\"onwall's inequality, we obtain
\[
\frac{\|f_{\widetilde{\theta}}\|}{\|f_{\theta}\|}=R^{1/2}(1)\leq \exp(C|\theta-\widetilde{\theta}|)\leq \exp(CC_r)\,.
\]
\end{proof}

% \section{Proof of Theorem \ref{thm:algorithm_VGD}}\label{sec:pf_thm:algorithm_VGD}
% \begin{proof}[Proof of Theorem \ref{thm:algorithm_VGD}] Without loss of generality, first let $0\leq m\leq N$. According to the algorithm, we have
% \[
% \theta_{m+1}=\theta_m-\eta G_m\,.
% \]
% Similar to previous proof, we have
% \begin{equation}\label{eqn:F_bound_2}
% \mathbb{E}\left(F(\theta_{m+1})|\mathcal{F}_{m-1}\right)\leq F(\theta_m)-\frac{\eta}{C_r}\left|\nabla F(\theta_m)\right|^2+\frac{C\eta^2}{2}\mathbb{E}(|G_m|^2|\mathcal{F}_{m-1})
% \end{equation}
% For the fourth term, we notice that
% \[
% \begin{aligned}
% \mathbb{E}(|G_m|^2|\mathcal{F}_{m-1})=&\mathbb{E}(|G_m-\mathbb{E}(G_m|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})+\left|\mathbb{E}(G_m|\mathcal{F}_{m-1})\right|^2\\
% \leq &\mathbb{E}(|G_m-\mathbb{E}(G_m|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})+C|\nabla F(\theta_m)|^2
% \end{aligned}
% \]
% where we use \eqref{eqn:expectation-difference} and \eqref{eqn:distance_bound_2} in the inequality. We bound the first term by 
% \[
% \begin{aligned}
% &\mathbb{E}(|G_m-\mathbb{E}(G_m|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})\\
% \leq &2\mathbb{E}(|G_{m,1}-\mathbb{E}(G_{m,1}|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})\\
% &+2\mathbb{E}(|G_{m,2}-\mathbb{E}(G_{m,2}|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})\,.
% \end{aligned}
% \]
% The first term can be bounded using 
% \[
% \begin{aligned}
% &\mathbb{E}(|G_{m,1}-\mathbb{E}(G_{m,1}|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})\\
% \leq &C\frac{\frac{1}{N}\sum^N_{n=1}|f_n|^2|\nabla g_{n}(\theta_0)-\nabla g_n(\theta_m)|^2}{Z^2_0}\frac{\frac{1}{N}\sum^N_{n=1}\left(g^4_n(\theta_m)+g^4_n(\theta_0)\right)}{Z^4_0}\\
% &+C\frac{\left|\frac{1}{N}\sum^N_{n=1}f_n\nabla g_n(\theta_m)\right|^2}{Z^2_0}\frac{\frac{1}{N}\sum^N_{n=1}\left|g^2_n(\theta_m)-g^2_n(\theta_0)\right|^2}{Z^4_0}\\
% \leq &C|\theta_m-\theta_0|^2\,,
% \end{aligned}
% \]
% where we use \eqref{eqn_value_gnbound}-\eqref{eqn_Hessian_gnbound}, and \eqref{eqn:distance_bound_2} in the last inequality, and $C$ is a constant that only depends on $C_r,C_g,C_f$. Similarly, 
% \[
% \begin{aligned}
% &\mathbb{E}(|G_{m,2}-\mathbb{E}(G_{m,2}|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})\\
% \leq & 4\frac{\frac{1}{N}\sum^N_{n=1}|f_n|^2| g_{n}(\theta_0)-g_n(\theta_m)|^2}{Z^2_0}\frac{\frac{1}{N}\sum^N_{n=1}\left(\left|g_n(\theta_m)\nabla g_n(\theta_m)\right|^2+\left|g_n(\theta_0)\nabla g_n(\theta_0)\right|^2\right)}{Z^4_0}\\
% &+4\frac{\frac{1}{N}\sum^N_{n=1}| g_n(\theta_0)\nabla g_n(\theta_0)-g_n(\theta_m)\nabla g_n(\theta_m)|^2}{Z^4_0}\frac{\left|\frac{1}{N}\sum^N_{n=1}f_n(\theta_m)g_n(\theta_m)\right|^2}{Z^2_0}\\
% \leq &C|\theta_m-\theta_0|^2
% \end{aligned}
% \]
% where we use \eqref{eqn_value_gnbound}- \eqref{eqn_Hessian_gnbound}, and \eqref{eqn:distance_bound_2} in the last inequality, and $C$ is a constant that only depends on $C_r,C_g,C_f$. Thus, we have
% \[
% \mathbb{E}(|G_m-\mathbb{E}(G_m|\mathcal{F}_{m-1})|^2|\mathcal{F}_{m-1})\leq C|\theta_m-\theta_0|^2
% \]
% and
% \[
% \mathbb{E}(|G_m|^2|\mathcal{F}_{m-1})\leq C\left(|\theta_m-\theta_0|^2+|\nabla F(\theta_m)|^2\right)\,.
% \]
% Since $\left|\theta_m-\theta_0\right|^2\leq  \eta^2m\sum^{m-1}_{i=0}|G_i|^2$ and $|G_0|^2= |\nabla F(\theta_0)|^2$, we have
% \begin{equation}\label{eqn_fourth_term_bound_2}
% \begin{aligned}
% \mathbb{E}(|G_m|^2|\mathcal{F}_{m-1})\leq & C\left(\eta^2m\sum^{m-1}_{i=0}|G_i|^2+|\nabla F(\theta_m)|^2\right)\\
% \leq &\frac{1}{2}\frac{\log(N)}{N}\sum^{m-1}_{i=0}|G_i|^2+C|\nabla F(\theta_m)|^2\\
% \leq & C|\nabla F(\theta_m)|^2+\frac{C\log(N)}{N}\sum^{m-1}_{i=0}\left(1+\frac{1}{2}\frac{\log(N)}{N}\right)^{m-1-i}|\nabla F(\theta_i)|^2\\
% \leq & C|\nabla F(\theta_m)|^2+\frac{C\log(N)}{\sqrt{N}}\sum^{m-1}_{i=0}|\nabla F(\theta_i)|^2
% \end{aligned}
% \end{equation}
% where we use $\eta<\frac{1}{\sqrt{2}}\frac{\log^{1/2}(N)}{\sqrt{C}N}$ and $m<N$ in the second inequality, and $\left(1+\frac{1}{2}\frac{\log(N)}{N}\right)^{m-1-i}<\sqrt{N}$ in the last inequality.

% Plugging \eqref{eqn_fourth_term_bound_2} into \eqref{eqn:F_bound_2} and taking expectation on both sides, we have
% \[
% \begin{aligned}
% \mathbb{E}\left(F(\theta_{m+1})\right)\leq &\mathbb{E}\left(F(\theta_m)\right)-\frac{\eta}{C_r}\mathbb{E}\left(\left|\nabla F(\theta_m)\right|^2\right)\\
% &+C\eta^2\left(\mathbb{E}\left(|\nabla F(\theta_m)|^2\right)+\frac{\log(N)}{\sqrt{N}}\sum^{m-1}_{i=0}|\nabla F(\theta_i)|^2\right)
% \end{aligned}
% \]

% Finally, taking the summation in $1\leq m\leq M-1$, we have
% \[
% \begin{aligned}
% \mathbb{E}\left(F(\theta_{M})\right)\leq & \mathbb{E}\left(F(\theta_{0})\right)-\frac{\eta }{C_r}\sum^{M-1}_{m=0}\mathbb{E}\left(\left|\nabla F(\theta_{m})\right|^2\right)+C\eta^2\sum^{M-1}_{m=0}\mathbb{E}\left(|\nabla F(\theta_i)|^2\right)\\
% &+C\eta^2\frac{\log(N)}{\sqrt{N}}\sum^{M-1}_{m=1}\sum^{m-1}_{i=\lfloor m/N\rfloor*N}\mathbb{E}\left(|\nabla F(\theta_i)|^2\right)\\
% \leq & \mathbb{E}\left(F(\theta_{0})\right)-\frac{\eta }{C_r}\sum^{M-1}_{m=0}\mathbb{E}\left(\left|\nabla F(\theta_{m})\right|^2\right)+C\eta^2\sum^{M-1}_{m=0}\mathbb{E}\left(|\nabla F(\theta_i)|^2\right)\\
% &+C\eta^2\sqrt{N}\log(N)\sum^{M-1}_{m=1}\mathbb{E}\left(|\nabla F(\theta_m)|^2\right)\\
% \leq &\mathbb{E}\left(F(\theta_{0})\right)-\frac{\eta }{2C_r}\sum^{M-1}_{m=0}\mathbb{E}\left(\left|\nabla F(\theta_{m})\right|^2\right)\,,
% \end{aligned}
% \]
% where we use $\eta(1+\sqrt{N}\log(N))<1/(2CC_r)$ in the last inequality. This proves \eqref{eqn:gradient_F_bound_2}.
% \end{proof}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
