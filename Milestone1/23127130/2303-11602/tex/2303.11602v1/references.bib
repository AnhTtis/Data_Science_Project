
@inproceedings{
kodryan_training_2022,
title={Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes},
author={Maxim Kodryan and Ekaterina Lobacheva and Maksim Nakhodnov and Dmitry P. Vetrov},
booktitle={Advances in Neural Information Processing Systems},
year={2022},
}

@inproceedings{li_robust_2022,
	title = {Robust {Training} of {Neural} {Networks} {Using} {Scale} {Invariant} {Architectures}},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Zhiyuan and Bhojanapalli, Srinadh and Zaheer, Manzil and Reddi, Sashank and Kumar, Sanjiv},
	year = {2022},
}
@article{van_2017,
  
  author = {van Laarhoven, Twan},
  
  title = {L2 Regularization versus Batch and Weight Normalization},
  
  journal = {arXiv/1706.05350},
  year={2017},
}

@inproceedings{Ioffe_2015,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning - Volume 37},
}

@INPROCEEDINGS{spherequadratic,
  author={Vu, Trung and Raich, Raviv and Fu, Xiao},
  booktitle={2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={On Convergence of Projected Gradient Descent for Minimizing a Large-Scale Quadratic Over the Unit Sphere}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/MLSP.2019.8918830}}

@inproceedings{Wu_2018,
  
  author = {Wu, Yuxin and He, Kaiming},
  
  
  title = {Group Normalization},
  
  booktitle = {In Proceedings of
the European conference on computer vision (ECCV)},
  
  year = {2018},
}

@article{Ba_2016,
  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  
  
  title = {Layer Normalization},
  
  journal = {arXiv/1607.06450},
  
  year = {2016},
}

@inproceedings{Saliman_2016,
author = {Salimans, Tim and Kingma, Diederik P.},
title = {Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
year = {2016},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
}
@inproceedings{
arora2018theoretical,
title={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},
author={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
meng_mathcalg-sgd_2021,
title={G-{SGD}: Optimizing Re{LU} Neural Networks in its Positively Scale-Invariant Space},
author={Qi Meng and Shuxin Zheng and Huishuai Zhang and Wei Chen and Zhi-Ming Ma and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{Wan_2021,
 author = {Wan, Ruosi and Zhu, Zhanxing and Zhang, Xiangyu and Sun, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6380--6391},
 title = {Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay},
 volume = {34},
 year = {2021}
}

@article{Foulkes2001,
  title = {Quantum Monte Carlo simulations of solids},
  author = {Foulkes, W. M. C. and Mitas, L. and Needs, R. J. and Rajagopal, G.},
  journal = {Rev. Mod. Phys.},
  volume = {73},
  issue = {1},
  pages = {33--83},
  numpages = {0},
  year = {2001},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.73.33},
}


@incollection{Toulouse2016,
title = {Chapter Fifteen - Introduction to the Variational and Diffusion Monte Carlo Methods},
editor = {Philip E. Hoggan and Telhat Ozdogan},
series = {Advances in Quantum Chemistry},
publisher = {Academic Press},
volume = {73},
pages = {285-314},
year = {2016},
booktitle = {Electron Correlation in Molecules – ab initio Beyond Gaussian Quantum Chemistry},
issn = {0065-3276},
doi = {10.1016/bs.aiq.2015.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065327615000386},
author = {Julien Toulouse and Roland Assaraf and Cyrus J. Umrigar},
keywords = {Quantum Monte Carlo, Electronic structure calculations, Metropolis–Hastings algorithm, Fixed-node approximation, Statistical methods},
abstract = {We provide a pedagogical introduction to the two main variants of real-space quantum Monte Carlo methods for electronic structure calculations: variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC). Assuming no prior knowledge on the subject, we review in depth the Metropolis–Hastings algorithm used in VMC for sampling the square of an approximate wave function, discussing details important for applications to electronic systems. We also review in detail the more sophisticated DMC algorithm within the fixed-node approximation, introduced to avoid the infamous Fermionic sign problem, which allows one to sample a more accurate approximation to the ground-state wave function. Throughout this review, we discuss the statistical methods used for evaluating expectation values and statistical uncertainties. In particular, we show how to estimate nonlinear functions of expectation values and their statistical uncertainties.}
}


@book{Becca2017,
place={Cambridge},
title={Quantum Monte Carlo Approaches for Correlated Systems},
DOI={10.1017/9781316417041}, publisher={Cambridge University Press},
author={Becca, Federico and Sorella, Sandro},
year={2017}
}

@article{Carleo2017,
   title={Solving the quantum many-body problem with artificial neural networks},
   volume={355},
   ISSN={1095-9203},
   DOI={10.1126/science.aag2302},
   number={6325},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Carleo, Giuseppe and Troyer, Matthias},
   year={2017},
   month={Feb},
   pages={602–606}
}

@article{Pfau_2020,
  title = {Ab initio solution of the many-electron Schr\"odinger equation with deep neural networks},
  author = {Pfau, David and Spencer, James S. and Matthews, Alexander G. D. G. and Foulkes, W. M. C.},
  journal = {Phys. Rev. Res.},
  volume = {2},
  issue = {3},
  pages = {033429},
  numpages = {20},
  year = {2020},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.2.033429},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.033429}
}

@misc{Spencer_2020,
  doi = {10.48550/ARXIV.2011.07125},
  url = {https://arxiv.org/abs/2011.07125},
  author = {Spencer, James S. and Pfau, David and Botev, Aleksandar and Foulkes, W. M. C.},
  keywords = {Computational Physics (physics.comp-ph), Machine Learning (cs.LG), Chemical Physics (physics.chem-ph), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Better, Faster Fermionic Neural Networks},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Jeff_2021,
  author = {Lin, Jeffmin and Goldshlager, Gil and Lin, Lin},
  title = {Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation},
  journal = {arXiv/2112.03491},
  year = {2021},
}

@article{Nomura2017,
  title = {Restricted Boltzmann machine learning for solving strongly correlated quantum systems},
  author = {Nomura, Yusuke and Darmawan, Andrew S. and Yamaji, Youhei and Imada, Masatoshi},
  journal = {Phys. Rev. B},
  volume = {96},
  issue = {20},
  pages = {205152},
  numpages = {8},
  year = {2017},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.96.205152},
}
@article{Choo2018,
  title = {Symmetries and Many-Body Excitations with Neural-Network Quantum States},
  author = {Choo, Kenny and Carleo, Giuseppe and Regnault, Nicolas and Neupert, Titus},
  journal = {Phys. Rev. Lett.},
  volume = {121},
  issue = {16},
  pages = {167204},
  numpages = {6},
  year = {2018},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.121.167204},
}
@article{Nagy2019,
  title = {Variational Quantum Monte Carlo Method with a Neural-Network Ansatz for Open Quantum Systems},
  author = {Nagy, Alexandra and Savona, Vincenzo},
  journal = {Phys. Rev. Lett.},
  volume = {122},
  issue = {25},
  pages = {250501},
  numpages = {6},
  year = {2019},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.122.250501},
}
@article{Yang2020,
   title={Deep learning-enhanced variational Monte Carlo method for quantum many-body physics},
   volume={2},
   ISSN={2643-1564},
   DOI={10.1103/physrevresearch.2.012039},
   number={1},
   journal={Physical Review Research},
   publisher={American Physical Society (APS)},
   author={Yang, Li and Leng, Zhaoqi and Yu, Guangyuan and Patel, Ankit and Hu, Wen-Jun and Pu, Han},
   year={2020},
   month={Feb}
}
@article{Luo2019,
  title = {Backflow Transformations via Neural Networks for Quantum Many-Body Wave Functions},
  author = {Luo, Di and Clark, Bryan K.},
  journal = {Phys. Rev. Lett.},
  volume = {122},
  issue = {22},
  pages = {226401},
  numpages = {6},
  year = {2019},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.122.226401},
}

@article{Han2019,
    title = "Solving many-electron Schrödinger equation using deep neural networks",
    journal = "Journal of Computational Physics",
    volume = "399",
    pages = "108929",
    year = "2019",
    issn = "0021-9991",
    doi = "10.1016/j.jcp.2019.108929",
    author = "Jiequn Han and Linfeng Zhang and Weinan E",
    keywords = "Schrödinger equation, Variational Monte Carlo, Deep neural networks, Trial wave-function",
    abstract = "We introduce a new family of trial wave-functions based on deep neural networks to solve the many-electron Schrödinger equation. The Pauli exclusion principle is dealt with explicitly to ensure that the trial wave-functions are physical. The optimal trial wave-function is obtained through variational Monte Carlo and the computational cost scales quadratically with the number of electrons. The algorithm does not make use of any prior knowledge such as atomic orbitals. Yet it is able to represent accurately the ground-states of the tested systems, including He, H2, Be, B, LiH, and a chain of 10 hydrogen atoms. This opens up new possibilities for solving large-scale many-electron Schrödinger equation."
}
@article{Hermann2020,
   title={Deep-neural-network solution of the electronic Schrödinger equation},
   volume={12},
   ISSN={1755-4349},
   DOI={10.1038/s41557-020-0544-y},
   number={10},
   journal={Nature Chemistry},
   publisher={Springer Science and Business Media LLC},
   author={Hermann, Jan and Schätzle, Zeno and Noé, Frank},
   year={2020},
   month={Sep},
   pages={891–897}
}


@article{StokesRobledo2020,
  title = {Phases of two-dimensional spinless lattice fermions with first-quantized deep neural-network quantum states},
  author = {Stokes, James and Moreno, Javier Robledo and Pnevmatikakis, Eftychios A. and Carleo, Giuseppe},
  journal = {Phys. Rev. B},
  volume = {102},
  issue = {20},
  pages = {205122},
  numpages = {10},
  year = {2020},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.102.205122},
}


@Article{Choo2020,
author={Choo, Kenny
and Mezzacapo, Antonio
and Carleo, Giuseppe},
title={Fermionic neural-network states for ab-initio electronic structure},
journal={Nature Communications},
year={2020},
month={May},
day={12},
volume={11},
number={1},
pages={2368},
abstract={Neural-network quantum states have been successfully used to study a variety of lattice and continuous-space problems. Despite a great deal of general methodological developments, representing fermionic matter is however still early research activity. Here we present an extension of neural-network quantum states to model interacting fermionic problems. Borrowing techniques from quantum simulation, we directly map fermionic degrees of freedom to spin ones, and then use neural-network quantum states to perform electronic structure calculations. For several diatomic molecules in a minimal basis set, we benchmark our approach against widely used coupled cluster methods, as well as many-body variational states. On some test molecules, we systematically improve upon coupled cluster methods and Jastrow wave functions, reaching chemical accuracy or better. Finally, we discuss routes for future developments and improvements of the methods presented.},
issn={2041-1723},
doi={10.1038/s41467-020-15724-9},
}

@misc{Glehn2022,
  doi = {10.48550/ARXIV.2211.13672},
  
  url = {https://arxiv.org/abs/2211.13672},
  
  author = {von Glehn, Ingrid and Spencer, James S. and Pfau, David},
  
  keywords = {Chemical Physics (physics.chem-ph), Machine Learning (cs.LG), Computational Physics (physics.comp-ph), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Self-Attention Ansatz for Ab-initio Quantum Chemistry},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{Cassella2022,
  doi = {10.48550/ARXIV.2202.05183},
  
  url = {https://arxiv.org/abs/2202.05183},
  
  author = {Cassella, G. and Sutterud, H. and Azadi, S. and Drummond, N. D. and Pfau, D. and Spencer, J. S. and Foulkes, W. M. C.},
  
  keywords = {Computational Physics (physics.comp-ph), Other Condensed Matter (cond-mat.other), Strongly Correlated Electrons (cond-mat.str-el), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Discovering Quantum Phase Transitions with Fermionic Neural Networks},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Gerard2022,
  doi = {10.48550/ARXIV.2205.09438},
  
  url = {https://arxiv.org/abs/2205.09438},
  
  author = {Gerard, Leon and Scherbela, Michael and Marquetand, Philipp and Grohs, Philipp},
  
  keywords = {Machine Learning (cs.LG), Chemical Physics (physics.chem-ph), Computational Physics (physics.comp-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Gold-standard solutions to the Schrödinger equation using deep learning: How much physics do we need?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Hermann_review,
  doi = {10.48550/ARXIV.2208.12590},
  
  url = {https://arxiv.org/abs/2208.12590},
  
  author = {Hermann, Jan and Spencer, James and Choo, Kenny and Mezzacapo, Antonio and Foulkes, W. M. C. and Pfau, David and Carleo, Giuseppe and Noé, Frank},
  
  keywords = {Chemical Physics (physics.chem-ph), Machine Learning (cs.LG), Computational Physics (physics.comp-ph), Machine Learning (stat.ML), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Ab-initio quantum chemistry with neural-network wavefunctions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{SAGA-2013,
author = {Schmidt, M. and Roux, N. and Bach, F.},
year = {2013},
month = {09},
pages = {},
title = {Minimizing Finite Sums with the Stochastic Average Gradient},
volume = {162},
journal = {Mathematical Programming},
doi = {10.1007/s10107-016-1030-6}
}
@article{SAGA-2014,
author = {Defazio, A. and Bach, F. and Lacoste-Julien, S.},
year = {2014},
month = {07},
pages = {},
title = {{SAGA}: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
volume = {2},
journal = {Advances in Neural Information Processing Systems}
}
@inproceedings{Johnson_Zhang,
author = {Johnson, R. and Zhang, T.},
title = {Accelerating Stochastic Gradient Descent Using Predictive Variance Reduction},
year = {2013},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems},
pages = {315–323},
numpages = {9},
}






@inproceedings{tripuraneni_averaging_2018,
	title = {Averaging {Stochastic} {Gradient} {Descent} on {Riemannian} {Manifolds}},
	url = {https://proceedings.mlr.press/v75/tripuraneni18a.html},
	abstract = {We consider the minimization of a function defined on a Riemannian manifold \${\textbackslash}mathcal\{M\}\$ accessible only through unbiased estimates of its gradients. We develop a geometric framework to transform a sequence of slowly converging iterates generated from stochastic gradient descent (SGD) on \${\textbackslash}mathcal\{M\}\$ to an averaged iterate sequence with a robust and fast \$O(1/n)\$ convergence rate. We then present an application of our framework to geodesically-strongly-convex (and possibly Euclidean non-convex) problems.  Finally, we demonstrate how these ideas apply to the case of streaming \$k\$-PCA, where we show how to accelerate the slow rate of the randomized power method (without requiring knowledge of the eigengap) into a robust algorithm achieving the optimal rate of convergence.},
	language = {en},
	urldate = {2023-01-22},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Tripuraneni, Nilesh and Flammarion, Nicolas and Bach, Francis and Jordan, Michael I.},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {650--687},
	file = {Full Text PDF:/Users/nilin5/Zotero/storage/I5YX7HZ7/Tripuraneni et al. - 2018 - Averaging Stochastic Gradient Descent on Riemannia.pdf:application/pdf},
}

@article{bonnabel_stochastic_2013,
	title = {Stochastic {Gradient} {Descent} on {Riemannian} {Manifolds}},
	volume = {58},
	issn = {1558-2523},
	doi = {10.1109/TAC.2013.2254619},
	abstract = {Stochastic gradient descent is a simple approach to find the local minima of a cost function whose evaluations are corrupted by noise. In this paper, we develop a procedure extending stochastic gradient descent algorithms to the case where the function is defined on a Riemannian manifold. We prove that, as in the Euclidian case, the gradient descent algorithm converges to a critical point of the cost function. The algorithm has numerous potential applications, and is illustrated here by four examples. In particular a novel gossip algorithm on the set of covariance matrices is derived and tested numerically.},
	number = {9},
	journal = {IEEE Transactions on Automatic Control},
	author = {Bonnabel, Silvère},
	month = sep,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Automatic Control},
	keywords = {Approximation methods, Convergence, Cost function, Covariance matrices, Manifolds, Nonlinear identification, Riemannian geometry, Standards, stochastic approximation, Trajectory},
	pages = {2217--2229},
	file = {IEEE Xplore Abstract Record:/Users/nilin5/Zotero/storage/RT2T46Y8/6487381.html:text/html;Submitted Version:/Users/nilin5/Zotero/storage/YPQS4D3N/Bonnabel - 2013 - Stochastic Gradient Descent on Riemannian Manifold.pdf:application/pdf},
}

@inproceedings{vu_convergence_2019,
	title = {On {Convergence} of {Projected} {Gradient} {Descent} for {Minimizing} a {Large}-{Scale} {Quadratic} {Over} the {Unit} {Sphere}},
	doi = {10.1109/MLSP.2019.8918830},
	abstract = {Unit sphere-constrained quadratic optimization has been studied extensively over the past decades. While state-of-art algorithms for solving this problem often rely on relaxation or approximation techniques, there has been little research into scalable first-order methods that tackle the problem in its original form. These first-order methods are often more well-suited for the big data setting. In this paper, we provide a novel analysis of the simple projected gradient descent method for minimizing a quadratic over a sphere. When the gradient step size is sufficiently small, we show that convergence is locally linear and provide a closed-form expression for the rate. Moreover, a careful selection of the step size can stimulate convergence to the global solution while preventing convergence to local minima.},
	booktitle = {2019 {IEEE} 29th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Vu, Trung and Raich, Raviv and Fu, Xiao},
	month = oct,
	year = {2019},
	note = {ISSN: 1551-2541},
	keywords = {Acceleration, Convergence, convergence analysis, Eigenvalues and eigenfunctions, large-scale optimization, Linear programming, Minimization, quadratic programming, Quadratic programming, unit-norm constraint},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/nilin5/Zotero/storage/HL5P2M6T/8918830.html:text/html},
}


@INPROCEEDINGS{7852234,
  author={Hauswirth, Adrian and Bolognani, Saverio and Hug, Gabriela and Dörfler, Florian},
  booktitle={2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Projected gradient descent on Riemannian manifolds with applications to online power system optimization}, 
  year={2016},
  volume={},
  number={},
  pages={225-232},
  doi={10.1109/ALLERTON.2016.7852234}}
  
  @article{Bonnabel_2011,
author = {Bonnabel, Silvère},
year = {2011},
month = {11},
pages = {},
title = {Stochastic Gradient Descent on Riemannian Manifolds},
volume = {58},
journal = {IEEE Transactions on Automatic Control},
doi = {10.1109/TAC.2013.2254619}
}


@inproceedings{Zhang_2016,
author = {Zhang, Hongyi and Reddi, Sashank J. and Sra, Suvrit},
title = {Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds},
year = {2016},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4599–4607},
numpages = {9},
}

@article{Sato_2019,
author = {Sato, Hiroyuki and Kasai, Hiroyuki and Mishra, Bamdev},
title = {Riemannian Stochastic Variance Reduced Gradient Algorithm with Retraction and Vector Transport},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {2},
pages = {1444-1472},
year = {2019},
}


@inproceedings{Tripuraneni2018AveragingSG,
  title={Averaging Stochastic Gradient Descent on Riemannian Manifolds},
  author={Nilesh Tripuraneni and Nicolas Flammarion and Francis R. Bach and Michael I. Jordan},
  booktitle={Annual Conference Computational Learning Theory},
  year={2018}
}


@Article{HanLiLinEtAl2019,
  author  = {Han, Jiequn and Li, Yingzhou and Lin, Lin and Lu, Jianfeng and Zhang, Jiefu and Zhang, Linfeng},
  title   = {Universal approximation of symmetric and anti-symmetric functions},
  journal = {arXiv:1912.01765},
  year    = {2019},
}


@TechReport{Hutter2020,
  author =       "Marcus Hutter",
  title =        "On Representing (Anti)Symmetric Functions",
  institution =  "DeepMind",
  address =      "London, UK",
  number =       "arXiv:2007.15298",
  _month =        jun,
  year =         "2020",
  bibtex =       "http://www.hutter1.net/official/bib.htm#asymnn",
  url =          "https://arxiv.org/abs/2007.15298",
  pdf =          "http://www.hutter1.net/publ/asymnn.pdf",
  project =      "http://www.hutter1.net/official/projects.htm#nn",
}

@Article{SannaiTakaiCordonnier2019,
  author  = {Sannai, Akiyoshi and Takai, Yuuki and Cordonnier, Matthieu},
  title   = {Universal approximations of permutation invariant/equivariant functions by deep neural networks},
  journal = {arXiv:1903.01939},
  year    = {2019},
}


@Article{KerivenPeyre2019,
  author  = {Keriven, Nicolas and Peyr{\'e}, Gabriel},
  title   = {Universal invariant and equivariant graph neural networks},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2019},
  volume  = {32},
  pages   = {7092--7101},
}

@misc{Nilin2022,
  doi = {10.48550/ARXIV.2205.12250},
  
  url = {https://arxiv.org/abs/2205.12250},
  
  author = {Abrahamsen, Nilin and Lin, Lin},
  
  keywords = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Quantum Physics (quant-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Taming the sign problem of explicitly antisymmetrized neural networks via rough activation functions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Sorella98,
  title = {Green Function Monte Carlo with Stochastic Reconfiguration},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. Lett.},
  volume = {80},
  issue = {20},
  pages = {4558--4561},
  numpages = {0},
  year = {1998},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.80.4558},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.80.4558}
}

@article{PhysRevB.64.024512,
  title = {Generalized Lanczos algorithm for variational quantum Monte Carlo},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. B},
  volume = {64},
  issue = {2},
  pages = {024512},
  numpages = {16},
  year = {2001},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.64.024512},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.64.024512}
}

@article{Sorella2001,
  title = {Generalized Lanczos algorithm for variational quantum Monte Carlo},
  author = {Sorella, Sandro},
  journal = {Phys. Rev. B},
  volume = {64},
  issue = {2},
  pages = {024512},
  numpages = {16},
  year = {2001},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.64.024512},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.64.024512}
}
@article{Nightingale2001,
  title = {Optimization of Ground- and Excited-State Wave Functions and van der Waals Clusters},
  author = {Nightingale, M. P. and Melik-Alaverdian, Vilen},
  journal = {Phys. Rev. Lett.},
  volume = {87},
  issue = {4},
  pages = {043401},
  numpages = {4},
  year = {2001},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.87.043401},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.87.043401}
}
@article{toulouse2007,
  title={Optimization of quantum Monte Carlo wave functions by energy minimization},
  author={Toulouse, Julien and Umrigar, Cyrus J},
  journal={The Journal of chemical physics},
  volume={126},
  number={8},
  pages={084102},
  year={2007},
  publisher={American Institute of Physics}
}

@article{Sandvik2007,
  title = {Variational Quantum Monte Carlo Simulations with Tensor-Network States},
  author = {Sandvik, A. W. and Vidal, G.},
  journal = {Phys. Rev. Lett.},
  volume = {99},
  issue = {22},
  pages = {220602},
  numpages = {4},
  year = {2007},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.99.220602},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.220602}
}

@article{Neuscamman2012,
  title = {Optimizing large parameter sets in variational quantum Monte Carlo},
  author = {Neuscamman, Eric and Umrigar, C. J. and Chan, Garnet Kin-Lic},
  journal = {Phys. Rev. B},
  volume = {85},
  issue = {4},
  pages = {045103},
  numpages = {6},
  year = {2012},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.85.045103},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.85.045103}
}

@article{Cai2018,
  title = {Approximating quantum many-body wave functions using artificial neural networks},
  author = {Cai, Zi and Liu, Jinguo},
  journal = {Phys. Rev. B},
  volume = {97},
  issue = {3},
  pages = {035116},
  numpages = {8},
  year = {2018},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.97.035116},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.97.035116}
}

@article{Carleo2019,
title = {NetKet: A machine learning toolkit for many-body quantum systems},
journal = {SoftwareX},
volume = {10},
pages = {100311},
year = {2019},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2019.100311},
url = {https://www.sciencedirect.com/science/article/pii/S2352711019300974},
author = {Giuseppe Carleo and Kenny Choo and Damian Hofmann and James E.T. Smith and Tom Westerhout and Fabien Alet and Emily J. Davis and Stavros Efthymiou and Ivan Glasser and Sheng-Hsuan Lin and Marta Mauri and Guglielmo Mazzola and Christian B. Mendl and Evert {van Nieuwenburg} and Ossian O’Reilly and Hugo Théveniaut and Giacomo Torlai and Filippo Vicentini and Alexander Wietek},
}

@Article{Otis2019,
author ="Otis, Leon and Neuscamman, Eric",
title  ="Complementary first and second derivative methods for ansatz optimization in variational Monte Carlo",
journal  ="Phys. Chem. Chem. Phys.",
year  ="2019",
volume  ="21",
issue  ="27",
pages  ="14491-14510",
publisher  ="The Royal Society of Chemistry",
doi  ="10.1039/C9CP02269D",
url  ="http://dx.doi.org/10.1039/C9CP02269D",
}
