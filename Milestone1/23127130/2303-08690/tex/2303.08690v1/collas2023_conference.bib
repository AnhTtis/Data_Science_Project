
@InProceedings{pmlr-v162-wan22d,
  title = 	 {Towards Evaluating Adaptivity of Model-Based Reinforcement Learning Methods},
  author =       {Wan, Yi and Rahimi-Kalahroudi, Ali and Rajendran, Janarthanan and Momennejad, Ida and Chandar, Sarath and Van Seijen, Harm H},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {22536--22561},
  year = 	 {2022},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wan22d/wan22d.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wan22d.html},
  abstract = 	 {In recent years, a growing number of deep model-based reinforcement learning (RL) methods have been introduced. The interest in deep model-based RL is not surprising, given its many potential benefits, such as higher sample efficiency and the potential for fast adaption to changes in the environment. However, we demonstrate, using an improved version of the recently introduced Local Change Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and DreamerV2 perform poorly in their ability to adapt to local environmental changes. Combined with prior work that made a similar observation about the other popular model-based method, MuZero, a trend appears to emerge, suggesting that current deep model-based methods have serious limitations. We dive deeper into the causes of this poor performance, by identifying elements that hurt adaptive behavior and linking these to underlying techniques frequently used in deep model-based RL. We empirically validate these insights in the case of linear function approximation by demonstrating that a modified version of linear Dyna achieves effective adaptation to local changes. Furthermore, we provide detailed insights into the challenges of building an adaptive nonlinear model-based method, by experimenting with a nonlinear version of Dyna.}
}

@inproceedings{vanseijen-LoCA,
 author = {Van Seijen, Harm and Nekoei, Hadi and Racah, Evan and Chandar, Sarath},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6562--6572},
 publisher = {Curran Associates, Inc.},
 title = {The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/48db71587df6c7c442e5b76cc723169a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{wang2019benchmarking,
  title={Benchmarking model-based reinforcement learning},
  author={Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  journal={arXiv preprint arXiv:1907.02057},
  year={2019}
}

@article{moerland2020model,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Jonker, Catholijn M},
  journal={arXiv preprint arXiv:2006.16712},
  year={2020}
}

@article{plaat2020deep,
  title={Deep model-based reinforcement learning for high-dimensional problems, a survey},
  author={Plaat, Aske and Kosters, Walter and Preuss, Mike},
  journal={arXiv preprint arXiv:2008.05598},
  year={2020}
}


@ARTICLE{Momennejad2017,
  title    = "The successor representation in human reinforcement learning",
  author   = "Momennejad, I and Russek, E M and Cheong, J H and Botvinick, M M
              and Daw, N D and Gershman, S J",
  abstract = "Momennejad et al. formulate and provide evidence for the
              successor representation, a computational learning mechanism
              intermediate between the two dominant models (a fast but
              inflexible `model-free' system and a flexible but slow
              `model-based' one).",
  journal  = "Nature Human Behaviour",
  volume   =  1,
  number   =  9,
  pages    = "680--692",
  month    =  sep,
  year     =  2017,
  language = "en"
}




@article{schrittwieser2019mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1911.08265},
  year={2019}
}

@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International Conference on Machine Learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@article{hafner2019dream,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@article{hafner2020mastering,
  title={Mastering atari with discrete world models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={arXiv preprint arXiv:2010.02193},
  year={2020}
}


@inproceedings{hadsell2006dimensionality,
  title={Dimensionality reduction by learning an invariant mapping},
  author={Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={2},
  pages={1735--1742},
  year={2006},
  organization={IEEE}
}
  
@article{dosovitskiy2014discriminative,
  title={Discriminative unsupervised feature learning with convolutional neural networks},
  author={Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{wu2018unsupervised,
  title={Unsupervised feature learning via non-parametric instance discrimination},
  author={Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X and Lin, Dahua},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3733--3742},
  year={2018}
}

@article{daw2011model,
  title={Model-based influences on humans' choices and striatal prediction errors},
  author={Daw, Nathaniel D and Gershman, Samuel J and Seymour, Ben and Dayan, Peter and Dolan, Raymond J},
  journal={Neuron},
  volume={69},
  number={6},
  pages={1204--1215},
  year={2011},
  publisher={Elsevier}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{french1991using,
  title={Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  booktitle={Proceedings of the 13th annual cognitive science society conference},
  volume={1},
  pages={173--178},
  year={1991}
}

@article{robins1995catastrophic,
  title={Catastrophic forgetting, rehearsal and pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995},
  publisher={Taylor \& Francis}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@article{goodfellow2013empirical,
  title={An empirical investigation of catastrophic forgetting in gradient-based neural networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1312.6211},
  year={2013}
}

@inproceedings{kemker2018measuring,
  title={Measuring catastrophic forgetting in neural networks},
  author={Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler and Kanan, Christopher},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@misc{minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for Gymnasium},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Farama-Foundation/MiniGrid}},
}

@article{tassa2018deepmind,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}

@article{khetarpal2020towards,
  title={Towards continual reinforcement learning: A review and perspectives},
  author={Khetarpal, Khimya and Riemer, Matthew and Rish, Irina and Precup, Doina},
  journal={arXiv preprint arXiv:2012.13490},
  year={2020}
}

@inproceedings{kessler2022same,
  title={Same state, different task: Continual reinforcement learning without interference},
  author={Kessler, Samuel and Parker-Holder, Jack and Ball, Philip and Zohren, Stefan and Roberts, Stephen J},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7143--7151},
  year={2022}
}

@article{purushwalkam2022challenges,
  title={The Challenges of Continuous Self-Supervised Learning},
  author={Purushwalkam, Senthil and Morgado, Pedro and Gupta, Abhinav},
  journal={arXiv preprint arXiv:2203.12710},
  year={2022}
}

@article{hartikainen2019dynamical,
  title={Dynamical distance learning for semi-supervised and unsupervised skill discovery},
  author={Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey},
  journal={arXiv preprint arXiv:1907.08225},
  year={2019}
}

@article{Sun_Zhou_Li_2020, title={Attentive Experience Replay}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6049}, DOI={10.1609/aaai.v34i04.6049}, abstractNote={&lt;p&gt;Experience replay (ER) has become an important component of deep reinforcement learning (RL) algorithms. ER enables RL algorithms to reuse past experiences for the update of current policy. By reusing a previous state for training, the RL agent would learn more accurate value estimation and better decision on that state. However, as the policy is continually updated, some states in past experiences become rarely visited, and optimization over these states might not improve the overall performance of current policy. To tackle this issue, we propose a new replay strategy to prioritize the transitions that contain states frequently visited by current policy. We introduce &lt;em&gt;Attentive Experience Replay&lt;/em&gt; (AER), a novel experience replay algorithm that samples transitions according to the similarities between their states and the agentâ€™s state. We couple AER with different off-policy algorithms and demonstrate that AER makes consistent improvements on the suite of OpenAI gym tasks.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sun, Peiquan and Zhou, Wengang and Li, Houqiang}, year={2020}, month={Apr.}, pages={5900-5907} }


@InProceedings{pmlr-v139-lee21g,
  title = 	 {SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning},
  author =       {Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6131--6141},
  year = 	 {2021},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lee21g/lee21g.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lee21g.html},
  abstract = 	 {Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments.}
}


@inproceedings{NEURIPS2020_d7f426cc,
 author = {Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {18560--18572},
 publisher = {Curran Associates, Inc.},
 title = {DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction},
 url = {https://proceedings.neurips.cc/paper/2020/file/d7f426ccbc6db7e235c57958c21c5dfa-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{parisi2019continual,
  title={Continual lifelong learning with neural networks: A review},
  author={Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
  journal={Neural Networks},
  volume={113},
  pages={54--71},
  year={2019},
  publisher={Elsevier}
}


@article{zhu2020transfer,
  title={Transfer learning in deep reinforcement learning: A survey},
  author={Zhu, Zhuangdi and Lin, Kaixiang and Zhou, Jiayu},
  journal={arXiv preprint arXiv:2009.07888},
  year={2020}
}

@article{lee2020neural,
  title={A neural dirichlet process mixture model for task-free continual learning},
  author={Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
  journal={arXiv preprint arXiv:2001.00689},
  year={2020}
}

@article{huisman2021survey,
  title={A survey of deep meta-learning},
  author={Huisman, Mike and Van Rijn, Jan N and Plaat, Aske},
  journal={Artificial Intelligence Review},
  volume={54},
  number={6},
  pages={4483--4541},
  year={2021},
  publisher={Springer}
}

@article{nagabandi2018deep,
  title={Deep online learning via meta-learning: Continual adaptation for model-based RL},
  author={Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.07671},
  year={2018}
}

@article{xu2020task,
  title={Task-agnostic online reinforcement learning with an infinite mixture of gaussian processes},
  author={Xu, Mengdi and Ding, Wenhao and Zhu, Jiacheng and Liu, Zuxin and Chen, Baiming and Zhao, Ding},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6429--6440},
  year={2020}
}

@article{sutton2012dyna,
  title={Dyna-style planning with linear function approximation and prioritized sweeping},
  author={Sutton, Richard S and Szepesv{\'a}ri, Csaba and Geramifard, Alborz and Bowling, Michael P},
  journal={arXiv preprint arXiv:1206.3285},
  year={2012}
}
