% This file was adapted from ICLR2022_conference.tex example provided for the ICLR conference
\documentclass{article} % For LaTeX2e
\usepackage{collas2023_conference,times}
\usepackage{easyReview}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{wrapfig}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Please leave these options as they are
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=purple,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }




\title{Replay Buffer With Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \collasfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Ali Rahimi-Kalahroudi\thanks{Correspondence to: \texttt{<ali-rahimi.kalahroudi@mila.quebec>}}\\
Mila - Quebec AI Institute\\
Université de Montréal\\
\And
Janarthanan Rajendran\\
Mila - Quebec AI Institute\\
Université de Montréal
\And
Ida Momennejad\\
Microsoft Research, NYC
\AND
Harm van Seijen\thanks{Majority of this work was performed while at Microsoft Research, Montréal.}\\
\And
Sarath Chandar\\
Mila - Quebec AI Institute\\
École Polytechnique de Montréal\\
Canada CIFAR AI Chair
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\methodname}{\textsc{LoFo}}


%\collasfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\preprintcopy % Uncomment for the preprint version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
One of the key behavioral characteristics used in neuroscience to determine whether the subject of study---be it a rodent or a human---exhibits model-based learning is effective adaptation to local changes in the environment. In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes. An explanation for this mismatch is that MBRL methods are typically designed with sample-efficiency on a single task in mind and the requirements for effective adaptation are substantially higher, both in terms of the learned world model and the planning routine. One particularly challenging requirement is that the learned world model has to be sufficiently accurate throughout relevant parts of the state-space. This is challenging for deep-learning-based world models due to catastrophic forgetting. And while a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out replay buffer precludes effective adaptation due to maintaining stale data. In this work, we show that a conceptually simple variation of this traditional replay buffer is able to overcome this limitation. By removing only samples from the buffer from the local neighbourhood of the newly observed samples, deep world models can be built that maintain their accuracy across the state-space, while also being able to effectively adapt to changes in the reward function. We demonstrate this by applying our replay-buffer variation to a deep version of the classical Dyna method, as well as to recent methods such as PlaNet and  DreamerV2, demonstrating that deep model-based methods can adapt effectively as well to local changes in the environment.
\end{abstract}

\section{Introduction}

Recent work has shown that modern deep MBRL methods adapt poorly to local changes in the environment \citep{vanseijen-LoCA,pmlr-v162-wan22d}, despite this being a key characteristic of model-based learning in humans and animals \citep{daw2011model}. The analysis by \cite{pmlr-v162-wan22d} revealed that there are broadly two causes for this lack of adaptivity: an insufficient world model or insufficient planning. And the former one is an especially challenging one to overcome when deep-learning-based world models are considered. The core of this challenge lies in the fact that adaptivity requires a world model that is accurate across the relevant state-space, as a small change in reward or transition function can change the trajectory of the optimal policy entirely. By contrast, to achieve high single-task sample-efficiency---a common metric used in MBRL research---it is sufficient that the world model is accurate along the current behavior policy.

%For tabular world models the condition of accuracy across the state-space boils down to one of sufficient exploration.  
For deep world models, accuracy across the state-space is hard to achieve and maintain, even with sufficient exploration. The reason is that collected samples are strongly correlated, and, at the final stages of learning, mostly come from states along the trajectory of the optimal policy. Due to catastrophic forgetting, the quality of the predictions further away from this trajectory quickly degrades. A common strategy to counter this is to use a replay buffer. By randomly sampling from a large replay buffer and using these samples to update the world model, the effects of catastrophic forgetting are greatly reduced. However, the traditional first-in-first-out (FIFO) replay buffer has the disadvantage that it hinders effective adaptation, as out-of-date samples interfere with the new data. 

To address the challenge of  catastrophic forgetting, while also avoiding interference from out-of-date samples, we propose a variation of the traditional FIFO replay buffer. Instead of removing the oldest sample from the replay buffer once the buffer is full, the oldest sample in the \emph{local neighbourhood} of the new sample is removed. This conceptually simple idea naturally leads to a replay buffer whose samples are approximately spread out equally across the space space, while local changes are accounted for quickly. Consequently, updating the deep world model with samples drawn randomly from this replay buffer results in a world model that is approximately accurate across the state-space at each moment in time. We call this replay buffer variation a \methodname~({\it Lo}cal {\it Fo}rgetting) replay buffer. One practical challenge to our proposed variation is that a locality-function needs to be learned that determines whether or not a sample from the replay buffers falls within the local neighborhood of a newly observed sample. We train this locality function using contrastive learning \citep{hadsell2006dimensionality,dosovitskiy2014discriminative,wu2018unsupervised} during the initial stages of learning, after which it is fixed and used as basis for the \methodname~replay buffer.

We demonstrate the effectiveness of the \methodname~replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain, using the same Local Change Adaptation (LoCA) setup as used by \cite{pmlr-v162-wan22d}. We then test the limits of our approach by applying the same idea to both PlaNet \citep{hafner2019learning} and DreamerV2 \citep{hafner2020mastering}, which use world models based on recurrent networks and are intended for continuous-action domains. 
%As these methods use recurrent networks, instead of storing and removing individual samples, we store and remove sample-sequences of length N. 
Experiments with these modified methods on variations of the MuJoCo Reacher domain demonstrate that a \methodname~replay buffer can substantially improve adaptivity of more advanced deep MBRL methods as well.


\section{Background: Local Change Adaptation (LoCA) Setup} \label{LoCA Setup Intro}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{Figures/LoCASetup_normal.pdf}
\caption{LoCA Setup. The values in the terminal states indicate the reward received when reaching it. The boundary of the T1-zone acts as a one-way passage: once inside the T1-zone, the agent cannot move out of it. The main challenge of the LoCA setup is the mismatch in the initial-state distribution between training and evaluation in Phase 2.}
\label{Loca_fig}
\end{figure}

\citet{vanseijen-LoCA} introduced the LoCA setup. The motivation behind this setup was to have a reliable test that distinguishes model-based from model-free behavior. It is inspired by tests used in neuroscience to distinguish model-based from model-free behavior in rodents and humans. A key characteristic that distinguishes model-based from model-free behavior is that model-based learning can propagate information effectively to parts of the state-space that have not recently been visited. The LoCA setup is designed to test this characteristic. This paper is based on a simplified version of the LoCA setup, presented by \citet{pmlr-v162-wan22d}, which is detailed below and illustrated in Figure \ref{Loca_fig}.

The LoCA setup consists of a task configuration and an experiment configuration. In this context, a task is defined as a $(\mathbb{S},\mathbb{A},P,R,\gamma, S^{train}_0,S^{eval}_0)$ tuple, with $\mathbb{S}$ the state-space; $\mathbb{A}$ the action-space; $P(s'|s,a)$ with $s',s \in \mathbb{S}$ and $a \in \mathbb{A}$ the transition function; $R(s,a,s')$ the reward function; $\gamma$ the discount factor, $S^{train}_0$ the initial state distribution during training; and $S^{eval}_0$ the initial state distribution during evaluation. The LoCA task configuration consists of two tasks, A and B, that are identical except for $R$ and $S^{train}_0$. Furthermore, the LoCA task configuration requires that the state-space contains two terminal states, T1 and T2, and a zone around T1, called the T1-zone, that the agent cannot escape from once entered regardless of the actions taken (i.e., the boundary of the T1-zone acts like a one-way passage). The reward function is 0 everywhere, except when transitioning to one of the terminal states. For task A, T1 yields a reward of 4, while T2 yields a reward of 2; for task B, T1 yields a reward of 1, while the reward for T2 is still 2. Even though there is only a local difference in the reward function of task A and task B,  the optimal policy is different across the state-space. In task A, the optimal policy predominantly moves the agent towards T1; in task B, it predominantly moves the agent towards T2. For task A, the initial state distribution for training and evaluation is a uniformly random distribution across the full state space. This is the case for the evaluation distribution of task B as well. However, the initial state during training for task B is drawn exclusively from the T1-zone (uniformly at random). This difference in distribution between training and evaluation in task B is a key part of the LoCA setup. We discuss its implications later in this section.

A LoCA experiment consists of two phases. During Phase 1, the agent is trained and evaluated on task A. After a certain number of time steps, Phase 2 starts, during which the agent is trained and evaluated on task B. The agent is not given an explicit signal of the change in phase/task. The duration of Phase 1 should be sufficiently long that the agent can achieve close-to-optimal performance on task A. The performance of the agent in both Phase 1 and Phase 2 is evaluated by periodically pausing training, freezing the weights, and measuring the average return of its current policy over a number of evaluation episodes, using $S^{eval}_0$ as the initial state distribution. The evaluation performance in both phases is compared to that of the optimal policy of the corresponding task.\footnote{In practice, we approximate the optimal-policy performance, for both task A and task B, by taking the asymptotic performance of a stable method trained using a uniform random initial-state distribution across the full state-space.}

A method is called adaptive to local changes if it is able to achieve close-to-optimal evaluation performance in Phase 2. Note that the agent only observes samples from within the T1-zone during training in Phase 2, while its performance is evaluated across the full state-space. Hence, in order to achieve close-to-optimal performance, the agent needs to maintain its knowledge from Phase 1 about the dynamics and rewards outside of the T1-zone. At the same time, it should update its knowledge about the rewards within the T1-zone (and without an explicit signal of the phase change). In addition, it needs to be able to pull or push the new reward information to the rest of the state-space, such that it can take the optimal action with respect to the new reward function anywhere in the state-space.

The LoCA setup is general in the sense that it can be combined with both discrete and continuous state or action-spaces, as well as low-dimensional or high-dimension input-spaces. Furthermore, it can evaluate almost all RL algorithms. The only condition that should hold is that the RL algorithm should have a training-mode as well as an evaluation-mode, during which weights are frozen, and no learning (or replay buffer updates) occurs. In practice, this is a mild condition, as most modern RL methods have these two modes implemented already, and if not, it's easy to modify them such that they do.


Finally, we want to stress that the LoCA setup is an analytical tool to evaluate the ability of a method to push/pull newly observed information to parts of the state-space not recently visited. The relevance of this property is that it can be regarded as a critical condition for local adaptivity and a key feature of model-based behavior. That being said, there are other forms of adaptivity, as well as other behavioral characteristics, that are important too (i.e., exploration behavior) and not measured by the LoCA setup. As such, the setup is not meant to be a complete test of the efficacy of a method. Instead, it is meant to keep research on model-based learning on track and create awareness that many of the current deep MBRL methods don't exhibit the key feature of model-based behavior.


\section{Challenges in Adaptive Deep MBRL} \label{Forget Interference Dilemma}

% For deep world models accuracy across the state-space is hard to achieve and maintain, even with sufficient exploration. 
Using the LoCA setup, \citet{pmlr-v162-wan22d} observed that current popular deep MBRL methods, such as PlaNet \citep{hafner2019learning} and DreamerV2 \citep{hafner2019dream, hafner2020mastering} fail to adapt to local environment changes. Their analysis revealed that the key reason for their failure to adapt is their inability to build and maintain correct world models when environment changes occur. A small local change in reward or transition function can change the optimal policy entirely. For an MBRL method to adapt to such environment changes and update its policy, it needs to maintain a world model that is sufficiently accurate throughout the relevant parts of the state-space. It is not sufficient to have a world model that is accurate only along the current behavior/optimal policy, as is the case for current methods such as PlaNet and Dreamer, developed for single non-changing task settings. 

Current deep MBRL methods that use deep neural networks as function approximators, such as PlaNet and DreamerV2 store their recent experience to a large first-in-first-out (FIFO) replay buffer. In a FIFO replay buffer, when the buffer gets full, as new transition samples are added to the buffer, the oldest samples are removed. Transitions are then sampled from the replay buffer to train the world model, which is then used for planning and updating the agent's policy. Instead of training the model directly from the stream of data that the agent is observing, having a replay buffer and sampling transitions from it to train the deep neural network based model, helps break the strong correlation between the stream of data that the agent is observing. This correlation is hurtful for the i.i.d., assumptions made by the stochastic gradient-descent based optimizers typically used to update the parameters of the neural networks. Replaying past experience also helps mitigate forgetting predictions about states that the agent does not visit frequently.

When a change occurs in the environment, using a large replay buffer results in interference of the old out-of-date data with the new data, resulting in an incorrect world model. For example, in the LoCA setup, when the agent enters Phase 2 with task B, there is a local change in the reward function, specifically the reward corresponding to T1 changes. The agent which is restricted to the T1-zone in Phase 2 is able to observe this local change. However, the replay buffer from which transitions are sampled to update the model still has lots of transitions from Phase 1 with the stale incorrect reward corresponding to T1 from task A. This interference from the old data results in an incorrect world model, which affects planning and thereby renders the method not adaptive to the local change in Phase 2.

In a FIFO replay buffer, the current samples in the replay buffer are determined by the current behavior policy. Therefore, as time progresses, the old data will all be removed from the buffer. This will happen faster if we make the replay buffer smaller. While it might avoid the interference from old stale data after some time, it would result in catastrophic forgetting \cite{} of model predictions corresponding to states not recently visited. For example, in the LoCA setup, when the replay buffer is small, after entering Phase 2, the stale transitions around T1 from phase 1 will be removed from the replay buffer over time, stopping the interference problem. This, however, also removes other still relevant and correct transitions from other parts of the state-space, which are now replaced by transitions from just within the T1-zone. A model trained with such a replay buffer forgets the learned model for states outside the T1-zone, which again affects the planning and renders the method not adaptive to the changes in Phase 2.

Therefore, the core challenge in building adaptive deep MBRL methods is to address the interference-forgetting dilemma and maintain world models that are sufficiently accurate throughout the relevant state-space when tasks change, irrespective of the current behavior policy.



\section{Local Forgetting (\methodname) Replay Buffer For Adaptive Deep MBRL}\label{New Replay Buffer}

We propose \methodname~({\it Lo}cal {\it Fo}rgetting) replay buffer, a conceptually simple variation of the traditional FIFO replay buffer that is able to address the core challenges in building an adaptive deep MBRL method.
Instead of removing the oldest samples from the replay buffer once it is full as done in the traditional FIFO replay buffer, in a \methodname~replay buffer, the oldest samples in only the \emph{local neighbourhood} of the new samples are removed.

When a change occurs in the environment and the agent observes that change, adding the new samples to the replay buffer and removing the oldest samples from only the local neighbourhood of the new data instead of the oldest samples from the entire replay buffer facilitates removal of the potentially incorrect and stale data sooner. This helps mitigate the interference of the old out-of-date samples with the new samples when tasks change.

Since only the samples in a local neighbourhood of the new samples are removed, old samples in other parts of the state-space including those that have not been visited recently remain in the replay buffer. This leads to a replay buffer whose samples are approximately spread out equally throughout the entire relevant state-space, irrespective of the current behavior policy. Updating the world model with samples drawn from this \methodname~replay buffer results in a world model that is approximately accurate throughout the relevant state-space avoiding the problem of catastrophic forgetting of predictions related to states not recently visited.

\methodname~replay buffer can therefore address the interference-forgetting dilemma and result in learning a world model that is sufficiently accurate throughout the relevant state-space even when there are environment changes. The current deep MBRL methods can then utilise the correct world models to update and adapt their policies, resulting in an adaptive deep MBRL method.

\section{\methodname~Replay Buffer With Contrastive State Locality} \label{contrastive_state_locality}

An instantiation of a \methodname~replay buffer requires a definition of a local neighbourhood to an observed new sample and a way to determine which samples in the replay buffer are within that local neighbourhood.
While there could be several ways to do this, in this work, we learn a state locality function using a contrastive learning technique which is then used to both define the local neighbourhood and also identify samples within that local neighbourhood.
% Section \ref{Contrastive State Similarities Learning} describes the state locality function that we learn and Section \ref{Modified Experience Replay} explains how it is used in the \methodname~replay buffer. 

% \subsection{Contrastive State Locality}\label{Contrastive State Similarities Learning}

% One practical challenge to our proposed variation is that a locality-function needs to be learned that determines whether or not a sample from the replay buffers falls within the local neighborhood of a newly observed sample. We train this locality function using contrastive learning \citep{hadsell2006dimensionality,dosovitskiy2014discriminative,wu2018unsupervised} during the initial stages of learning, after which it is fixed and used as basis for the \methodname replay buffer.

Using contrastive learning \citep{hadsell2006dimensionality,dosovitskiy2014discriminative,wu2018unsupervised} we learn neural embedding representation of states such that states that are temporally closer, i.e., reachable with fewer actions are also closer in their neural embedding representation.
Specifically, we learn an embedding function $\vv = f_\vtheta(s)$, where $f_\vtheta$ is a deep neural network that maps state $s$ to an embedding vector $\vv$. This embedding function induces a distance metric in the state-space, such as $d_\vtheta(s_i, s_j) = ||f_\vtheta(s_i) - f_\vtheta(s_j)||_2$ for states $s_i$ and $s_j$. 
We train the embedding function such that states that are temporally closer, have a smaller distance between the embeddings learnt and states that are temporally farther, have a relatively larger distance between them in the embedding space. 
% The trained embedding function is used to calculate distance between states and determine state locality.

Let $s'$ represent a state that is one action away from state $s$, and $\bar{\vs}$ represent a set of states that are not. Let $\sD = \{s, s', \bar{\vs}\}$ be a dataset of their collection. We train the embedding function to minimize the following loss function:

$$L(\sD) = \sum_{(s,s',\bar{\vs}) \in D}||f_\vtheta(s) - f_\vtheta(s')||_2^2 + \left(\beta - \Sigma_{\bar{s} \in \bar{\vs}}||f_\vtheta(s) - f_\vtheta(\bar{s})||_2^2\right)^2,$$

where $\beta > 0$ is a hyperparameter.
This loss function trains the embedding of states $s$ and $s'$ that are temporally next to each other to be closer by minimising their distance towards zero, while pushing the embedding of states $\bar{\vs}$ that are not temporally next to $s$ to be on average farther, with a cumulative squared distance value close to $\beta$, a positive number.  

In our experiments, we collect trajectories using a random behavior policy at the beginning of training and use samples from that trajectory to form the dataset $\sD = \{s, s', \bar{\vs}\}$ to train our embedding function $f_\vtheta$. The learnt embedding function is then fixed and the distance between the state embeddings is used as a proxy for state locality. In complex environments with exploration challenges, a random behavior policy might be insufficient to cover the relevant state-space needed to learn a good state locality estimate. It is an important future work to figure out good ways to learn state locality in such settings.

% \subsection{Using contrastive state locality in a \methodname~replay buffer}\label{Modified Experience Replay}

The samples stored in the replay buffer are generally of the form $(s, a, r, s')$, representing a transition an agent experienced by taking an action $a$, from the state $s$ and moving to a state $s'$ and obtaining a reward $r$.
In a \methodname~replay buffer, upon taking an action in the environment, the generated experience sample is used to first gather all the samples in the local neighbourhood of the new sample. This is done by estimating the state locality using the distance $d_\vtheta(s, s_i) = ||f_\vtheta(s) - f_\vtheta(s_i)||_2$ between state $s$ in the new sample and states $\{s_i\}$ corresponding to starting states in all of the samples in the replay buffer. The samples in the replay buffer whose starting state's distance to $s$, $d_\vtheta(s, s_i) < D_{local}$ are selected. $D_{local}$ is a scalar hyperparameter that determines the size of the local neighbourhood around any given state.  

If the number of samples within the local neighbourhood is equal to or above a threshold $N_{local}$, then the oldest sample in that local neighbourhood is removed. $N_{local}$ is a positive integer hyperparameter that determines the maximum number of samples that are stored in the replay buffer within any local neighbourhood of a given sample. The new sample is now added to the \methodname~replay buffer. 


\section{Adaptive Deep Dyna-Q}
\label{sec:adaptive deep Dyna-Q}

\citet{pmlr-v162-wan22d} successfully built an adaptive linear Dyna-Q MBRL method. However, they showed that a deep-learning-based version of the same approach failed to achieve adaptivity. They further identified inferior learned models resulting from catastrophic forgetting or interference from stale data as the key reason for the failure. 
In this section we show that replacing the traditional FIFO replay buffer used in \cite{pmlr-v162-wan22d} with the \methodname~replay buffer makes deep Dyna-Q adaptive.

We evaluate deep Dyna-Q with \methodname~replay buffer on the LoCA setup of two domains. First, the LoCA setup of the MountainCar domain (MountainCarLoCA) used in \citet{pmlr-v162-wan22d}. Second, the LoCA setup of a variant of the simple Mini-grid domain (MiniGridLoCA).
%, which has an image-based high-dimensional state space, unlike the MountainCar domain which has a 2-dimensional state space.
Our empirical results show that deep Dyna-Q with a \methodname~replay buffer is successfully able to adapt to the local environment changes in the LoCA setup in both the domains.
% , but also show that \methodname~replay buffer stores much less data when compared to a traditional FIFO replay buffer.

% \begin{wrapfigure}{r}{0.4\textwidth}
\begin{figure}[h]
\centering
    \begin{subfigure}[h]{.24\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/mountaincarloca_illustration.pdf}
        \caption{MountainCar}
        \label{MountainCarLoCA_Illustration}
    \end{subfigure}
    \begin{subfigure}[h]{.19\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/minigridloca_illustration.pdf}
        \caption{MiniGrid}
        \label{MiniGridLoCA_Illustration}
    \end{subfigure}
    \begin{subfigure}[h]{.19\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/reacherloca_illustration.pdf}
        \caption{Reacher}
        \label{ReacherLoCA_Illustration}
    \end{subfigure}
    \begin{subfigure}[h]{.19\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/random_reacherloca_illustration.pdf}
        \caption{RandomReacher}
        \label{RandomizedReacherLoCA_Illustration}
    \end{subfigure}%
    \caption{Illustration of environments corresponding to the different domains used in our experiments. 
    %a) MountainCarLoCA, b) MiniGridLoCA, c) ReacherLoCA, and d) RandomizedReacherLoCA domains. 
    The dashed white lines (not visible to the agent) show the boundary of T1-zone.}
    %a) MountainCar has a 2-dimensional state space (car's position and velocity) with discrete action space, b) MiniGrid has an image-based observation with a discrete action space, c) Reacher and d) RandomReacher have image-based observations with a continuous action space.
    %Refer to Appendix for more details on the domains and their LoCA setup.}
% \end{wrapfigure}
\end{figure}

\begin{figure}[h]
\centering
    \begin{subfigure}[h]{.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/final_dyna_mountaincar.pdf}
        \caption{MountainCarLoCA}
        \label{Final MountainCarLoCA Results}
    \end{subfigure}
    \begin{subfigure}[h]{.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/final_dyna_minigrid.pdf}
        \caption{MiniGridLoCA}
        \label{Final MiniGridLoCA Results}
    \end{subfigure}
    \caption{Plots showing the learning curves of deep Dyna-Q with a \methodname~replay buffer (adaptive) and FIFO replay buffers of different sizes (not adaptive) on (a) MountainCarLoCA and (b) MiniGridLoCA. Each learning curve is an average discounted return over ten runs, and the shaded area represents the standard error. 
    %The $x$ axis represents the number of training steps. 
    The maximum possible return in each Phase is represented by a solid black line. (a) Note that the Handcrafted-\methodname~refers to a variant of the \methodname~replay buffer that, instead of learning the state locality function, uses a handcrafted locality function (See Appendix \ref{appendix: handcrafted-lofo}).} 
    % The marked optimal discounted return in each subplot during phases 1 and 2}
    %(The optimal discounted return for (a) the MountainCarLoCA domain is taken from the \citet{pmlr-v162-wan22d}, and for (b) the MiniGridLoCA domain is calculated manually since the environment is a simple $8 \times 8$ grid). 
    \vspace{-0.5cm}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/replay_dyna_mountaincar_adaptive_phase_1.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/replay_dyna_mountaincar_adaptive_phase_2.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/replay_dyna_mountaincar_regular_phase_1.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/replay_dyna_mountaincar_regular_phase_2.pdf}
    \end{subfigure}
    \caption{Histogram of the states across the state-space of MountainCarLoCA environment whose transition samples are stored in the \methodname~replay buffer and the traditional FIFO replay buffer (size = $3e6$) at the end of Phase 1 and Phase 2. While \methodname~replay buffer maintains samples from states throughout the relevant state-space in both Phase 1 and Phase 2, FIFO replay buffer removes almost all samples from states other than the T1-zone by the end of Phase 2.}
    \label{Final MountainCarLoCA's Replay Buffer}
\end{figure}

% \subsection{MountainCarLoCA}\label{MountainCarLoCA Section}
% Our motivation for constructing such an experiment is to demonstrate the benefit of using \methodname~replay buffer in the previously shown non-adaptive method. Moreover, analyzing the replay buffer in a low-dimensional state space is much easier than in a high-dimensional one, so by looking closely at the stored samples, we aim to understand the credibility of the locality function and the role it plays in alleviating the interference-forgetting dilemma.

MountainCarLoCA setup (Figure \ref{MountainCarLoCA_Illustration}) consists of a variation of the classical MountainCar domain, with an under-powered cart having to move up a hill. There are two terminal states, T1 at the top of the hill and T2 corresponding to the cart being at the bottom of the hill with velocity close to zero. 
Figure \ref{Final MountainCarLoCA Results} shows the learning curves of different versions of deep Dyna-Q on MountainCarLoCA. The best replay buffer size for the baseline deep Dyna-Q with FIFO replay buffer is $4.5e6$, the one that stores all the samples seen so far. We observe that while all the methods reach close to optimal performance in Phase 1, only the method with \methodname~replay buffer is able to adapt to reward change that happens in Phase 2 and reach close to optimal performance in Phase 2, making it an adaptive MBRL method as per the LoCA evaluation. 
% Nonlinear Dyna with a regular FIFO replay buffer faces the problem of catastrophic forgetting in Phase 2 as the buffer is mainly dominated by the recent samples which are just from the limited T1-zone. Also the interference problem face the interference problem 


The two-dimensional state-space of the MountainCar domain allows visualization of properties associated with states throughout the state-space on a 2D plot. Figure \ref{Final MountainCarLoCA's Replay Buffer} shows the 2D histogram of the states across the state-space whose transition samples are stored in the \methodname~replay buffer and the traditional FIFO replay buffer at the end of Phase 1 and Phase 2.
We observe that in a FIFO replay buffer, at the end of Phase 2, almost all of the samples in the buffer are just from a small region in the state-space, that corresponds to the T1-zone. Samples of states from other parts of the state-space that were present in Phase 1 have mostly been removed from the buffer. This leads to catastrophic forgetting of model prediction for states in parts of the state-space outside the T1-zone. In \methodname~replay buffer however, even at the end of Phase 2, samples are maintained from across the entire relevant state-space, enabling maintaining accurate world model for the entire relevant state-space. 
Also, \methodname~replay buffer stores only about $3e4$ samples in total at the end of Phase 2, compared to $4.5e6$ samples stored in the FIFO replay buffer. 
% Therefore, \methodname replay buffer not only can alleviate the forgetting problem this way but also uses way much fewer samples (only about $30000$) to achieve adaptivity in the LoCA setup, that is, reaching the near-optimal performance in both phases 1 and 2.

The MiniGridLoCA setup has an image based high-dimensional input. The environment (Figure \ref{MiniGridLoCA_Illustration}) is an $8 \times 8$ RGB grid world with two green colored terminal states, T1 at the top left corner with a $2 \times 2$ T1-zone and T2 at the bottom-right corner. The agent is a red triangle which can choose to go straight, turn left or turn right. We use a deep convolution neural network to encode the image observations. 
% More details on the MiniGridLoCA setup and the neural network architecture and training are provided in Appendix \ref{}.

Figure \ref{Final MiniGridLoCA Results} shows the learning curves of different versions of deep Dyna-Q on MiniGridLoCA. We compare deep Dyna-Q using \methodname~replay buffer with the variants that use FIFO replay buffers with different buffer sizes, ranging from buffers that store all the samples seen (size = $1.8e6$) to the buffer that stores only as many samples as \methodname~replay buffer uses (size = $2.56e4$).
We observe that only deep Dyna-Q with \methodname~replay buffer is able to adapt to changes in Phase 2 and converge to close to optimal performance. The method achieves adaptivity while storing only around $2.56e4$ samples in the replay buffer, two orders of magnitude less samples compared to the best performing FIFO replay buffer based method which is of size $1.8e6$. 
Note that among the methods that use FIFO replay buffers, two methods with largest buffer sizes, one with a buffer of size $9e5$ and the other that stores all the samples with a size of $1.8e6$ are initially able to reach close to optimal performance in Phase 2, but their performance degrades over time afterwards. This is because, the total number of distinct states in the MiniGridLoCA environment are less (256 to be precise) and therefore samples from all over the state-space from Phase 1 stay longer in a large replay buffer. There is a sweet spot when there are still samples from throughout the state-space from Phase 1, while the proportion of samples from Phase 2 that capture the reward change in the T1-zone are much higher than the stale rewards of T1 from Phase 1 avoiding any serious interference. This can lead to a performance that is temporarily close to optimal performance but quickly degrades as samples from Phase 1 are removed in the FIFO buffer and catastrophic forgetting happens. Additional details on all the deep Dyna-Q experiments are provided in Appendix \ref{mountaincar_appendix} and \ref{minigrid_appendix}.


\section{Adaptive Planet and DreamerV2} \label{sec: adaptive recurrent models}

To see if more complex methods can be made adaptive as well by using a LoFo replay buffer, we applied it to the deep MBRL methods PlaNet \citep{hafner2019learning} and DreamerV2 \citep{hafner2020mastering} and evaluated their performance.
PlaNet and DreamerV2 use a recurrent model for reward and transition predictions which require sample-sequences to make updates to rather than individual samples. Therefore, we first need to extend the concept of the \methodname~replay buffer to sample-sequences. On a high level, what we aim to achieve is that updates made to the world model come from states spread out more-or-less equally across the state-space. In the case of sample-sequences, we try to approximate this by ensuring that the start-state of each sequence is drawn approximately at random across the state-space. 

Our approach to achieve good coverage of sequence start-states is to use two separate buffers: a \emph{state-buffer} which is curated similarly to the \methodname~replay buffer, and a \emph{trajectory-buffer} storing observed trajectories. For now, assume that the trajectory-buffer stores all observed trajectories. In the Appendix \ref{recurrent_appendix}, we show how the trajectory-buffer can be bounded to a maximum sample-size of $B\cdot N$, where $B$ is the size of the state-buffer and $N$ is the size of a sample-sequence used for updates. When a new sample $(s_t, a_t, r_t, s_{t+1})$ is observed, it is appended to the trajectory-buffer, while state $s_t$ is added to the state-buffer. Crucially, if the state-buffer contains more states from the local neighborhood of state $s_t$ than some threshold amount, the oldest state from this neighborhood is removed from the state-buffer. Each state in the state-buffer points to a copy of itself in the trajectory-buffer. When a state $s_i$ is removed from the state-buffer, the corresponding reward $r_i$ in the trajectory buffer is replaced by $None$, indicating that these rewards should no longer be used for training reward-predictions. The world model is updated by first sampling a random state $s_u$ from the state-buffer and then drawing a sample sequence of size $N$ from the trajectory-buffer, starting with state $s_u$. Whenever the reward is $None$, the loss of the corresponding reward prediction is set to 0.

% We apply the idea of using a \methodname~replay buffer to the reward prediction model of both PlaNet \citep{hafner2019learning} and DreamerV2 \citep{hafner2020mastering}, while keeping everything else the same. 
We evaluate our modified versions of PlaNet and DreamerV2 on the LoCA setup applied to two domains. First, a variation on the Reacher domain, called ReacherLoCA, introduced by \citet{pmlr-v162-wan22d}. And second, a more complex extension of ReacherLoCA which we refer to as the RandomizedReacherLoCA. For both domains, we observe that \methodname~replay buffer results in adapting to the local reward change in T1 and learning a sufficiently accurate reward model throughout the relevant state-space in Phase 2, reaching close to optimal performance. 

The ReacherLoCA setup consists of a variation of the Reacher domain \cite{tassa2018deepmind}. It is a continuous-action domain with 64 x 64 RGB images as observations. The environment has two targets corresponding to T1 and T2 fixed at the top right and the bottom left quadrants respectively (Figure \ref{ReacherLoCA_Illustration}). The agent controls the angular velocity of two connected bars to reach a target and remain at the target till the episode ends (1000 time steps). The RandomizedReacherLoCA (Figure \ref{RandomizedReacherLoCA_Illustration}) setup is an extension of the ReacherLoCA where the location of targets in the Reacher environment can vary each episode along a circle around the center, while still being opposite to each other. The target T2 is colored differently from T1 for the agent to be able to differentiate between them. 

\begin{figure}[h!]
\centering
    \begin{subfigure}[h]{.33\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/final_dreamer_reacherloca.pdf}
        \caption{ReacherLoCA}
        \label{Final DreamerV2 + ReacherLoCA Results}
    \end{subfigure}%
    \begin{subfigure}[h]{0.33\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/final_planet_reacherloca.pdf}
        \caption{ReacherLoCA}
        \label{Final PlaNet + ReacherLoCA Results}
    \end{subfigure}%
    \begin{subfigure}[h]{0.33\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/final_dreamer_randomreacherloca.pdf}
        \caption{RandomizedReacherLoCA}
        \label{Final DreamerV2 + RandomizedReacherLoCA Results}
    \end{subfigure}%
\caption{Plots showing the learning curves of DreamerV2 and PlaNet with a \methodname~replay buffer (adaptive) and with FIFO replay buffers (not adaptive) on a, b) ReacherLoCA, and c) RandomizedReacherLoCA. Each learning curve is an average undiscounted return over ten runs, and the shaded area represents the standard error. The maximum possible return in each phase is represented by a solid black line.}  
\label{Planet_dreamer_results}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/dreamerv2_heatmap_reacherloca_optimal_phase_2.pdf}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/dreamerv2_heatmap_reacherloca_lofo_phase_2.pdf}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/dreamerv2_heatmap_reacherloca_largefifo_phase_2.pdf}
    \end{subfigure}
    \begin{subfigure}{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/dreamerv2_heatmap_reacherloca_smallfifo_phase_2.pdf}
    \end{subfigure}
    \caption{Visualization of the estimated rewards from the DreamerV2 agent's reward model at the end of Phase 2. Each point on the heatmap represents the agent's position in the Reacher environment.}
    \label{Final Reward Est Dreamer ReacherLoCA}
\end{figure}

Figure \ref{Planet_dreamer_results} shows the learning curves of DreamerV2 and PlaNet on ReacherLoCA, and DreamerV2 on RandomizedReacherLoCA.
%In both the domains, DreamerV2 and PlaNet with a \methodname~replay buffer and a traditional FIFO replay buffer on ReacherLoCA and RandomizedReacherLoCA respectively. 
We observe that in both the setups all the methods reach close to optimal performance in Phase 1. However, in Phase 2 only DreamerV2 and PlaNet with a \methodname~replay buffer is able to adapt to the environment change, making them adaptive deep MBRL methods. DreamerV2 and PlaNet with the traditional FIFO replay buffer fail to adapt in Phase 2.

In Figure \ref{Final Reward Est Dreamer ReacherLoCA} we visualize the reward predictions of the different DreamerV2 methods.
We observe that DreamerV2 that uses the \methodname~replay buffer for learning its reward model has adapted its reward predictions for target T1 (top right) correctly to around $+1$ in Phase 2. When we use a large FIFO replay buffer, we observe that the reward for target T1 at the end of Phase 2 is overestimated to around $+2.5$ because of the interference of stale samples from Phase 1. On the other hand when we use a small FIFO replay buffer, DreamerV2's reward prediction at the end of Phase 2 for T1 is accurate around $+1$, but the model has completely forgotten the reward for target T2, and other parts of the state-space outside the T1-zone.
Additional details on all the PlaNet and DreamerV2 experiments are provided in Appendix \ref{reacher_appendix} and \ref{randomreacher_appendix}.


\section{Related Work}

Our work focuses on overcoming the LoCA setup challenge; therefore, as mentioned in the previous sections, it is most related to the work of \citet{vanseijen-LoCA}, and \cite{pmlr-v162-wan22d}. The LoCA setup is related to well-known problems such as the Continual Learning (CL) \citep{parisi2019continual,khetarpal2020towards} and Transfer Learning \citep{zhu2020transfer}, where an agent needs to learn multiple tasks. The general assumptions regarding these problems, however, do not completely apply to the LoCA setup. For one thing, the LoCA setup is task-free, where an agent needs to figure out the change in the tasks itself. So, the works that require explicit task definition (both in CL and transfer learning) could not be used in the LoCA setup. There is, however, a growing interest in task-free CL \citep{lee2020neural}, but a crucial factor regarding the LoCA setup is that only the performance in the current task matters at each moment; the agent need not be able to perform well in previous tasks. In fact, the agent needs to partly forget some information about the previous task to be able to perform well in the next task. So, preventing catastrophic forgetting completely, which is the main goal in continual learning, might even be hurting the performance in the LoCA setup.


It is worth noting that the LoCA setup tests a particular form of adaptivity, adaptivity to local changes in the environment, which is a key characteristic that distinguishes model-based from model-free behavior. That being said, there are other forms of adaptivity that some recent works touch upon. For instance, some works focus on adaptivity using some form of meta-knowledge \citep{nagabandi2018deep,huisman2021survey}, or providing an expansion-based method to solve multiple tasks \citep{xu2020task}. The LoCA setup, however, tests if a model-based reinforcement learning method can propagate information effectively to parts of the state space that have not recently been visited when observing only a local change in the environment.


\citet{pmlr-v162-wan22d} developed \textit{adaptive linear Dyna}, a modification of the linear Dyna algorithm (Algorithm 4 by \citet{sutton2012dyna}) that uses linear function approximation and adapts to local reward changes. 
However, the nonlinear extension of adaptive linear Dyna did not adapt well. 
They also showed that the other popular deep MBRL methods, PlaNet \citep{hafner2019learning} and DreamerV2 \citep{hafner2020mastering} were not adaptive as well. They identified the inability to maintain accurate world model throughout the relevant state space when environment change happens due to catastrophic forgetting and interference from old data as the core reason for the failure of these methods.
In this work, we showed that Deep Dyna-Q, PlaNet, and DreamerV2 can be adaptive to local reward changes if they use our proposed \methodname~replay buffer.



The method used for learning the state locality function (Section \ref{contrastive_state_locality}) used by the \methodname~replay buffer in this work is inspired by the work of \citet{hartikainen2019dynamical}. 
They propose a method to learn a policy-specific distance-function that is a measure of the expected number of time steps to reach the goal state from a given state. And they use this to shape the reward function.

% from any other state following the current policy. They use the learned distances to shape the reward to make learning complex tasks efficient and improve the current policy. The improved policy is then used to update policy specific dynamical distances and the above steps are repeated until convergence.
% Taking inspiration from this, in this work we learn a policy independent static locality function that maps neighbouring states to have smaller distances and states that are temporally farther to on average have larger distances. The learnt locality function is used to distinguish if a given state is within a small local neighbourhood of another state or not.

Several works have proposed modifications to the traditional FIFO replay buffer to handle various challenges. The work that is most related to ours is that of \citet{purushwalkam2022challenges}. They focused on the setting of self-supervised learning from a continual stream of unsupervised data. And to address catastrophic forgetting they proposed to use a replay buffer that removes the most correlated samples when the buffer is full. %This helps reduce correlated data in the replay buffer and maintains unique samples of past data, which in turn helps mitigate catastrophic forgetting. 
For the purpose of adaptation this is not a good strategy, however, as incorrect, out-of-date data can remain in the buffer for a long time.

% a strategy that removes the most 
% while avoiding catastrophic forgetting of past data. They proposed Minimum Redundancy replay buffer (MinRed), that removes the most correlated samples from a replay buffer when the buffer gets full. This helps reduce correlated data in the replay buffer and maintains unique samples of past data, which in turn helps mitigate catastrophic forgetting of past data. MinRed replay buffer, if applied to RL and our setup would suffer from the problem of interference from past incorrect data, as they do not get quickly replaced with new experience.   

\methodname~replay buffer provides a mechanism for adding and removing samples from a replay buffer. This determines the distribution of samples that are stored in the replay buffer. Various works in the past have developed different strategies for sampling or weighting samples in the replay buffer, in order to make the training more efficient. These include prioritizing samples that 1) have high temporal difference (TD) error \citep{schaul2015prioritized}, 2) are closest to the current state the agent is in \citep{Sun_Zhou_Li_2020}, and 3) have low TD target uncertainty \citep{ NEURIPS2020_d7f426cc, pmlr-v139-lee21g}. These methods are complementary to our work and can be implemented on a \methodname~replay buffer. 

\section{Limitations and Future Work}

% Here we have shown that a replay buffer with local forgetting can solve the problem of local changes in rewards, also known as reward transfer or reward revaluation. However, another characteristic of locally adaptive and flexible behavior in human experiments regards flexibility in the face of changes in the transition structure \cite{Momennejad2017}. This is a case where the reward locations do not change between Task A and Task B (as they do here), but rather that the transition structure of the environment has a local change from Task A to Task B (e.g., a detour, or when there is a new obstacle that renders the policy learned in Task A no longer optimal). While it seems possible to plan changes to the algorithm that can solve the transition structure as well, making this empirical solution work for a deep MBRL agent remains a topic for future research.

% Moreover, the distance function used here is limited, and future research can expand on the distance function. A practical goal of such future modifications to the approach would be to ensure the agent explore the state space fully. Finally, future studies are required to test the method in more complex and observationally rich environments (e.g., Minecraft), as well as continual learning settings with more than two tasks.

% transition-function
The general strategy behind the replay buffer variation we presented boils down to forgetting samples that are \emph{spatially close, but temporally far}  from currently observed samples. We believe this to be a good general strategy to achieve effective adaptation in a non-stationary world. However, the specific implementation of this principle will differ depending on the problem type and the non-stationarity considered. In this paper, we considered only reward non-stationarity and domains where exploration is easy. In this scenario, learning a locality function during the initial learning phase and keeping it fixed thereafter is sufficient. However, when the transition dynamics is non-stationary as well, the  locality function needs to be maintained and updated across time as well, as the distances between individual states can change over time.

%scaling to larger tasks]
Now that we have shown that adaptive deep MBRL is possible in principle, a logical next step for future work is scaling up these methods to larger and harder domains. 
%For example, domains where exploration is non-trivial or pixel-based domains with long  decision horizons (the distance an agent needs to plan ahead for in order to construct a good policy). 
Because, while DreamerV2 has shown to be able to achieve good single-task performance on such domains, it is not a given that our replay buffer variation is sufficient for making it adaptive for such domains as well. For example, the ReacherLoCA we considered has a fairly small decision horizon (i.e., how far an agent needs to plan ahead to construct a good policy---for ReacherLoCA  it takes on average about 25 actions to reach a goal state). Achieving adaptivity for longer decision horizons puts higher demands on the planning routine as well (see \cite{pmlr-v162-wan22d} for planning-related pitfalls that impede adaptivity), so it might be needed to make changes to DreamerV2's planning routine as well. 

%{\it [ToDo: relation to generalization]}

\section{Conclusion}

In this work, we considered one of the key features of model-based behavior: the ability to adapt to local environmental changes. In previous work, it has shown that tabular and linear model-based methods are able to achieve this form of adaptivity. However, deep model-based methods struggle due to an interplay between catastrophic forgetting and interference. To address the challenges with deep model-based methods, we proposed the \methodname~replay buffer, whose samples are approximately spread equally across the state-space. Furthermore, we conducted various experiments to show that utilizing the \methodname~replay buffer with deep MBRL methods can make them adapt effectively  to  local changes in the reward function.  
This is the first time--to the best of our knowledge--that this type of adaptivity has been shown for deep MBRL methods. This is an important step towards more practical real-world application of RL, since the stationary assumption does not always apply there. 


\subsubsection*{Acknowledgments}
This work is supported by the MSR-Mila grant. We would like to acknowledge the Digital Research Alliance of Canada (the Alliance) and Calcul Québec for providing the computing resources used in this work. In addition, JR is supported by IVADO postdoctoral fellowship. Finally, SC is supported by a Canada CIFAR AI Chair and an NSERC Discovery Grant.

\bibliography{collas2023_conference}
\bibliographystyle{collas2023_conference}

\appendix
\clearpage
\section{Additional Details for the Experiments on MountainCarLoCA Setup}
\label{mountaincar_appendix}

\subsection{Experiment Setup}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{7}{5em}{Initial distributions} & Phase 1 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\the entire state-space\end{tabular}\\
        \cline{2-3}
        & Phase 1 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ a small region\end{tabular}\\
        \cline{2-3}
        & Phase 2 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ states within T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ a small region\end{tabular}\\
        \hline
        \multirow{2}{5em}{Training steps} & Phase 1 steps & $1.5 \times 10^6$\\
        & Phase 2 steps & $3 \times 10^6$\\
        \hline
        \multirow{4}{5em}{Other details} 
        & \begin{tabular}[h]{@{}c@{}}Maximum number of steps\\ before an episode terminates\end{tabular} & 500\\
        & Training steps between two evaluations & $10^4$\\
        & Number of runs & 10\\
        & Number of evaluation episodes & 10\\
    \hline
    \end{tabular}
    \caption{Experiment setup for testing the Deep Dyna-Q on the MountainCarLoCA domain.}
    \label{tab: Experiment Setup - Deep Dyna-Q - MountainCarLoCA}
\end{table}

The MountainCarLoCA was first introduced by \citet{vanseijen-LoCA}, which is a variant of the well-known MountainCar environment. T1 is located at the top of the mountain ($\text{position} > 0.5$, and $\text{velocity} > 0$), and T2 is located at the valley ($(\text{position} + 0.52)^2 + 100 \times \text{velocity}^2 \leq 0.07^2$). The T1-zone contains all the states within $0.4 \leq \text{position} \leq 0.5$ and $0 \leq \text{velocity} \leq 0.07$. The discount factor for this environment is $\lambda = 0.99$. And lastly, for each evaluation, the agent is initialized roughly at the middle of T1 and T2 ($-0.2 \leq \text{position} \leq -0.1$ and $-0.01 \leq \text{velocity} \leq 0.01$). Table \ref{tab: Experiment Setup - Deep Dyna-Q - MountainCarLoCA} shows the experiment setup we used to evaluate the deep Dyna-Q agent's adaptivity under the LoCA setup.

\subsection{Hyperparameters}

Table \ref{tab: LoFo MountainCarLoCA Params} shows the final values of the hyperparameters that were used in the \methodname~replay buffer for generating the results presented in Figure \ref{Final MountainCarLoCA Results}. We searched over $\beta \in \{1, 10\}$, $D_{local} \in \{0.01, 0.005\}$, and $N_{local} \in \{1, 2, 5, 10\}$.

We used the same version of the deep Dyna-Q algorithm used in \citet{pmlr-v162-wan22d}. However, instead of having separate neural networks for different actions in each part of the model (dynamics, reward, and termination models), we use just one network and concatenate the action with the output of the middle layer (the one that has 63 output units) and then feed it to the next layer. Table \ref{tab: Hyperparameters -- deep Dyna-Q -- MountainCarLoCA} summarizes the important hyperparameters for the deep Dyna-Q method on the MountainCarLoCA. Both the deep Dyna-Q agent that used the \methodname~replay buffer and the baseline agents in Figure \ref{Final MountainCarLoCA Results} used these hyperparameters, and they only differ in the choice of the replay buffer. \citet{pmlr-v162-wan22d} found that using the same replay buffer size for learning the model and planning worked the best. Therefore, we followed the same.
%used the same buffer sizes for the two baselines in our experiments.

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
        \hline
        \begin{tabular}[h]{@{}c@{}}Embedding network\\ architecture\end{tabular} &
        \begin{tabular}[h]{@{}c@{}}MLP:$[64 \times 64 \times 64 \times 16]$,\\
        Activation Function: \textit{tanh}\end{tabular}\\
        
        \begin{tabular}[h]{@{}c@{}}Optimizer\end{tabular} & Adam, learning rate: $10^{-4}$\\
        \hline
        \begin{tabular}[h]{@{}c@{}}$\beta$\end{tabular} & $10$\\
        \begin{tabular}[h]{@{}c@{}}Number of negative samples\end{tabular} & $128$\\
        \begin{tabular}[h]{@{}c@{}}Mini-batch size\end{tabular} & $32$\\
        \begin{tabular}[h]{@{}c@{}}Total number of random steps\\ for creating dataset $\sD$\end{tabular} & $100000$\\
        \begin{tabular}[h]{@{}c@{}}Number of training epochs\end{tabular} & $5$\\
        \hline
        \begin{tabular}[h]{@{}c@{}}$D_{local}$\end{tabular} & $0.005$\\
        \begin{tabular}[h]{@{}c@{}}$N_{local}$\end{tabular} & $1$\\
        \hline
    \end{tabular}
    \caption{Hyperparameters used for the \methodname~replay buffer on the MountainCarLoCA domain.}
    \label{tab: LoFo MountainCarLoCA Params}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{6}{4em}{Neural networks} & Dynamics model  & \begin{tabular}[h]{@{}c@{}}MLP with \textit{tanh},\\$[64 \times 64 \times 63 \times 64 \times 64]$, \end{tabular}\\
        \cline{2-3}
        & Reward model  & \begin{tabular}[h]{@{}c@{}}MLP with \textit{tanh},\\$[64 \times 64 \times 63 \times 64 \times 64]$, \end{tabular}\\
        \cline{2-3}
        & Termination model  & \begin{tabular}[h]{@{}c@{}}MLP with \textit{tanh},\\$[64 \times 64 \times 63 \times 64 \times 64]$, \end{tabular}\\
        \cline{2-3}
        & Action-value estimator  & \begin{tabular}[h]{@{}c@{}}MLP with \textit{tanh},\\$[64 \times 64 \times 64 \times 64]$, \end{tabular}\\
        \hline
        \multirow{3}{4em}{Optimizer} & Value optimizer & \begin{tabular}[h]{@{}c@{}}Adam,\\ learning rate: $5 \times 10^{-6}$\end{tabular}\\
        & Model optimizer & \begin{tabular}[h]{@{}c@{}}Adam,\\learning rate: $5 \times 10^{-5}$\end{tabular}\\
        \hline
        \multirow{7}{4em}{Other details} & Exploration parameter & \begin{tabular}[h]{@{}c@{}}Epsilon greedy\\ $\eps =0.5$\end{tabular}\\
        & \begin{tabular}[h]{@{}c@{}}Number of random steps\\ before training\end{tabular} & $50000$\\
        & \begin{tabular}[h]{@{}c@{}}Target network update frequency\end{tabular}& $500$\\
        & \begin{tabular}[h]{@{}c@{}}Number of model learning steps\end{tabular} & $5$\\
        & \begin{tabular}[h]{@{}c@{}}Number of planning steps\end{tabular} & $5$\\
        & \begin{tabular}[h]{@{}c@{}}Mini-batch size of model learning\end{tabular} & $32$\\
        & \begin{tabular}[h]{@{}c@{}}Mini-batch size of planning\end{tabular} & $32$\\
    \hline
    \end{tabular}
    \caption{Hyperparameters  used for the deep Dyna-Q on the MountainCarLoCA domain.}
    \label{tab: Hyperparameters -- deep Dyna-Q -- MountainCarLoCA}
\end{table}

\subsection{Additional Results for The Deep Dyna-Q}

We compare the reward models of the Dyna-Q agents that use the \methodname~replay buffer and the traditional FIFO replay buffer by visualizing their prediction of rewards over all states in the MountainCarLoCA domain at the end of each phase in Figure \ref{fig: deep Dyna-Q -- MountainCarLoCA -- Rewards}. We observe that the reward model of the agent that uses the \methodname~replay buffer can estimate the rewards correctly in both phases. However, this is not true for the agent that uses the traditional FIFO replay buffer since it fails to predict the correct rewards associated with T2 due to catastrophic forgetting.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/adaptive_deepDyna_rewards_phase1.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/adaptive_deepDyna_rewards_phase2.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/regular_deepDyna_rewards_phase1.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.245\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/regular_deepDyna_rewards_phase2.pdf}
    \end{subfigure}
    \caption{Visualization of the estimated rewards from the deep Dyna-Q agent's reward model at the end of each phase. In each heatmap, the $x$ axis represents the agent's position, and the $y$ axis represents its velocity.}
    \label{fig: deep Dyna-Q -- MountainCarLoCA -- Rewards}
\end{figure}

\clearpage

\subsection{Handcrafted State Locality Function} \label{appendix: handcrafted-lofo}
Additionally, we tested another variant of the \methodname~replay buffer that uses a handcrafted state locality function instead of using the learnt contrastive state locality function. Since the MountainCarLoCA domain's state-space is 2-dimensional, where the first dimension indicates the position and the second one the velocity of the car ($s: (x, v)$), we can scale the range of the velocities to match the scale of the positions and use the euclidean distance as the locality function:
$$d_{\text{handcrafted}} (s_i, s_j) = \sqrt{(x_i - x_j)^2 + 150 \times (v_i - v_j)^2}$$

The learning curves of the deep Dyna-Q agent that uses the \methodname~replay buffer with the handcrafted state locality function in the MountainCarLoCA domain is presented in Figure \ref{Final MountainCarLoCA Results}. 
We observe that the \methodname~replay buffer with the learnt state locality function is able to match the performance of the buffer with the handcrafted state locality function.
% Interestingly, we observe that both of the \methodname~replay buffer variants can make the deep Dyna-Q agent effectively adapt to the changes in the environment.

% \clearpage

\section{Additional Details for the Experiments on the MiniGridLoCA Setup}
\label{minigrid_appendix}

\subsection{Experiment Setup}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{8}{5em}{Initial distributions} & Phase 1 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\the entire state-space\end{tabular}\\
        \cline{2-3}
        & Phase 1 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire state-space\end{tabular}\\
        \cline{2-3}
        & Phase 2 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ states within T1-zone \\($2 \times 2$ subgrid) \end{tabular}\\
        \cline{2-3}
        & Phase 2 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire state-space\end{tabular}\\
        \hline
        \multirow{2}{5em}{Training steps} & Phase 1 steps & $3 \times 10^5$\\
        & Phase 2 steps & $1.5 \times 10^6$\\
        \hline
        \multirow{4}{5em}{Other details} 
        & \begin{tabular}[h]{@{}c@{}}Maximum number of steps\\ before an episode terminates\end{tabular} & $100$\\
        & Training steps between two evaluations & $10^4$\\
        & Number of runs & $10$\\
        & Number of evaluation episodes & $10$\\
    \hline
    \end{tabular}
    \caption{Experiment setup for testing the Deep Dyna-Q method on the MiniGridLoCA domain.}
    \label{tab: Experiment Setup - Deep Dyna-Q - MiniGridLoCA}
\end{table}

The implementation of the MiniGridLoCA domain is done using the MiniGrid python package \citep{minigrid} . Table \ref{tab: Experiment Setup - Deep Dyna-Q - MiniGridLoCA} shows the experiment setup we used to evaluate the deep Dyna-Q agent’s adaptivity under the LoCA setup.



\subsection{Hyperparameters}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
        \hline
        \begin{tabular}[h]{@{}c@{}}Embedding network\\ architecture\end{tabular} &
        \begin{tabular}[h]{@{}c@{}}CNN:\\ (Channels:$[32 \times 64 \times 64]$\\
        Kernel Sizes:$[8 \times 3 \times 3]$\\
        Strides:$[4 \times 2 \times 2]$),\\
        Followed by MLP:$[512 \times 16]$,\\ Activation Function: \textit{relu}\end{tabular}\\
        \begin{tabular}[h]{@{}c@{}}Optimizer\end{tabular} & Adam, learning rate: $10^{-4}$\\
        \hline
        \begin{tabular}[h]{@{}c@{}}$\beta$\end{tabular} & $10$\\
        \begin{tabular}[h]{@{}c@{}}Number of negative samples\end{tabular} & $128$\\
        \begin{tabular}[h]{@{}c@{}}Mini-batch size\end{tabular} & $32$\\
        \begin{tabular}[h]{@{}c@{}}Total number of random steps\\ for creating dataset $\sD$\end{tabular} & $25000$\\
        \begin{tabular}[h]{@{}c@{}}Number of training epochs\end{tabular} & $5$\\
        \hline
        \begin{tabular}[h]{@{}c@{}}$D_{local}$\end{tabular} & $0.001$\\
        \begin{tabular}[h]{@{}c@{}}$N_{local}$\end{tabular} & $1$\\
        \hline
    \end{tabular}
    \caption{Hyperparameters used for the \methodname~replay buffer on the MiniGridLoCA domain.}
    \label{tab: LoFo MiniGridLoCA Params}
\end{table}

Table \ref{tab: LoFo MiniGridLoCA Params} shows the final hyperparameters setting used in the \methodname~replay buffer for generating the results presented in Figure \ref{Final MiniGridLoCA Results}. Furthermore, we only searched over $N_{local} \in \{50, 75, 100, 150\}$. While we started with the same $\beta$ and $D_{local}$ as the MountainCarLoCA, we found that by decreasing the $D_{local}$ to $0.001$, the learned state locality function can make a clear distinction between all possible states ($256$).

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{20}{4em}{Neural networks} & Dynamics model  & \begin{tabular}[h]{@{}c@{}}CNN:\\ (Channels:$[32 \times 64 \times 64]$\\
        Kernel Sizes:$[8 \times 3 \times 3]$\\
        Strides:$[4 \times 2 \times 2]$),\\
        Followed by Transposed CNN:\\(Channels:$[64 \times 32 \times 3]$\\
        Kernel Sizes:$[6 \times 6 \times 5]$\\
        Strides:$[1 \times 4 \times 3]$),\\ Activation Function: \textit{relu}\end{tabular}\\
        \cline{2-3}
        & Reward model  & \begin{tabular}[h]{@{}c@{}}CNN:\\ (Channels:$[32 \times 64 \times 64]$\\
        Kernel Sizes:$[8 \times 3 \times 3]$\\
        Strides:$[4 \times 2 \times 2]$),\\
        Followed by MLP:$[512]$,\\ Activation Function: \textit{relu}\end{tabular}\\
        \cline{2-3}
        & Termination model  & \begin{tabular}[h]{@{}c@{}}CNN:\\ (Channels:$[32 \times 64 \times 64]$\\
        Kernel Sizes:$[8 \times 3 \times 3]$\\
        Strides:$[4 \times 2 \times 2]$),\\
        Followed by MLP:$[512]$,\\ Activation Function: \textit{relu}\end{tabular}\\
        \cline{2-3}
        & Action-value estimator  & \begin{tabular}[h]{@{}c@{}}CNN:\\ (Channels:$[32 \times 64 \times 64]$\\
        Kernel Sizes:$[8 \times 3 \times 3]$\\
        Strides:$[4 \times 2 \times 2]$),\\
        Followed by MLP:$[512]$,\\ Activation Function: \textit{relu}\end{tabular}\\
        \hline
    \end{tabular}
    \caption{Neural networks' architecture for the deep Dyna-Q on the MiniGridLoCA domain.}
    \label{tab: Hyperparameters -- deep Dyna-Q -- Networks -- MiniGridLoCA}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{3}{4em}{Optimizer} & Value optimizer & \begin{tabular}[h]{@{}c@{}}Adam,\\ learning rate: $6.25 \times 10^{-5}$\end{tabular}\\
        & Model optimizer & \begin{tabular}[h]{@{}c@{}}Adam,\\learning rate: $ 10^{-4}$\end{tabular}\\
        \hline
        \multirow{7}{4em}{Other details} & Exploration parameter & \begin{tabular}[h]{@{}c@{}}Epsilon greedy\\ $\eps =0.5$\end{tabular}\\
        & \begin{tabular}[h]{@{}c@{}}Number of random steps\\ before training\end{tabular} & $2000$\\
        & \begin{tabular}[h]{@{}c@{}}Target network update frequency\end{tabular}& $5000$\\
        & \begin{tabular}[h]{@{}c@{}}Number of model learning steps\end{tabular} & $1$\\
        & \begin{tabular}[h]{@{}c@{}}Number of planning steps\end{tabular} & $1$\\
        & \begin{tabular}[h]{@{}c@{}}Mini-batch size of model learning\end{tabular} & $128$\\
        & \begin{tabular}[h]{@{}c@{}}Mini-batch size of planning\end{tabular} & $128$\\
    \hline
    \end{tabular}
    \caption{Final hyperparameters for the deep Dyna-Q on the MiniGridLoCA domain.}
    \label{tab: Hyperparameters -- deep Dyna-Q -- MinigridLoCA}
\end{table}

The algorithmic design of the deep Dyna-Q agent we used for the MiniGridLoCA domain is the same as that used for the MountainCarLoCA. We only changed the neural network architecture for the model and the action-value estimator. Table \ref{tab: Hyperparameters -- deep Dyna-Q -- Networks -- MiniGridLoCA} summarizes the architecture of the neural networks. Note that we first encode a given state to a low-dimensional vector for the various parts of the model (dynamics, reward, and termination model). Then, we concatenate the given action to the resulting vector and feed it to the MLP layers. This is mainly due to the fact that we did not want to keep separate networks for each action as it was done in \citet{pmlr-v162-wan22d}. Finally, Table \ref{tab: Hyperparameters -- deep Dyna-Q -- MinigridLoCA} shows the final hyperparameters used to generate the learning curves in Figure \ref{Final MiniGridLoCA Results}, when the agent used the \methodname~replay buffer and the traditional FIFO replay buffer.

\subsection{Additional Results for The Deep Dyna-Q}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/minigrid_lofo_search.pdf}
    \caption{Learning curves for the Dyna-Q agent that uses the \methodname~replay buffer with different $N_{local}$ values ($D_{local} = 0.001$), each averaged over 10 random seeds.}
    \label{fig: MiniGridLoCA Hyperparam Search N_local}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/minigrid_lofo_dlocal_search.pdf}
    \caption{Learning curves for the Dyna-Q agent that uses the \methodname~replay buffer with different $D_{local}$ values ( $N_{local}=100$), each averaged over 10 random seeds.}
    \label{fig: MiniGridLoCA Hyperparam Search D_local}
\end{figure}

Figure \ref{fig: MiniGridLoCA Hyperparam Search N_local} shows the learning curves of our search over different values of $N_{local}$ for the deep Dyna-Q agent using the \methodname~replay buffer. Since the locality function makes a clear distinction between the states of the MiniGridLoCA, using a specific $N_{local}$ means that the \methodname~replay buffer stores exactly $N_{local}$ samples per each state. Hence, an interesting observation from Figure \ref{fig: MiniGridLoCA Hyperparam Search N_local} is that higher $N_{local}$ results in higher performance in Phase 2. 
%However, we picked the $N_{local} = 100$ (instead of $200$) as the best hyperparameter for the MiniGridLoCA domain.

Figure \ref{fig: MiniGridLoCA Hyperparam Search D_local} shows the learning curves of our search over different values of $D_{local}$ for the deep Dyna-Q agent using the \methodname~replay buffer when $N_{local}=100$. 
We observe that small values $D_{local}$ result in successful adaptation in Phase 2.
% Some settings of the $D_{local}$ value do not show the effective adaptation in phase 2, showing the sensitivity to this hyperparameter.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[h]{.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/adaptive_deepDyna_minigrid_rewards_phase1.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/adaptive_deepDyna_minigrid_rewards_phase2.pdf}
    \end{subfigure}
    
    \begin{subfigure}[h]{.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/regular_deepDyna_minigrid_rewards_phase1.pdf}
    \end{subfigure}
    \begin{subfigure}[h]{.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Figures/regular_deepDyna_minigrid_rewards_phase2.pdf}
    \end{subfigure}
    \caption{Visualization of the estimated rewards from the deep Dyna-Q agent's reward model at the end of each phase. Each point on the heatmap represents the agent’s position in $8 \times 8$ grid of the MiniGridLoCA environment.}
    \label{fig: deep Dyna-Q -- MiniGridLoCA -- Rewards}
\end{figure}

Additionally, we visualized the estimated rewards from the deep Dyna-Q agent's reward model when using the \methodname~replay buffer and the traditional FIFO replay buffer at the end of each phase in Figure \ref{fig: deep Dyna-Q -- MiniGridLoCA -- Rewards}. To create each heatmap, we first generated the estimated rewards by placing the agent in a given state (a specific cell and direction) and taking the going straight action. This gave us four different 2D heatmaps, each representing a particular direction. Then, we summed the estimated rewards over the four possible directions and created the final heatmap. 

From Figure \ref{fig: deep Dyna-Q -- MiniGridLoCA -- Rewards}, we observe the deep Dyna-Q agent that uses the \methodname~replay buffer can predict the correct summation of the rewards during each phase. In contrast, the agent that uses the traditional FIFO replay buffer can only make accurate predictions in Phase 1. The inability of this agent to correctly predict the summation of the rewards is due to forgetting. Note that we used the agent with the traditional FIFO replay buffer of size $3e5$ in Figure \ref{fig: deep Dyna-Q -- MiniGridLoCA -- Rewards}.
% for more apparent forgetting in Phase 2.


% \clearpage

\section{Additional Details for the Experiments on the ReacherLoCA Setup}
\label{reacher_appendix}

\subsection{Experiment Setup}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{8}{5em}{Initial distributions} & Phase 1 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\the entire state-space\end{tabular}\\
        \cline{2-3}
        & Phase 1 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire states outside T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ states within T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire states outside T1-zone\end{tabular}\\
        \hline
        \multirow{2}{5em}{Training steps} & Phase 1 steps & $3 \times 10^5$\\
        & Phase 2 steps & $5 \times 10^5$\\
        \hline
        \multirow{4}{5em}{Other details} 
        & \begin{tabular}[h]{@{}c@{}}Number of steps\\ before an episode terminates\end{tabular} & $1000$\\
        & Training steps between two evaluations & $15000$\\
        & Number of runs & $10$\\
        & Number of evaluation episodes & $5$\\
    \hline
    \end{tabular}
    \caption{Experiment setup for testing the PlaNet method on the ReacherLoCA domain.}
    \label{tab: Experiment Setup - PlaNet - MiniGridLoCA}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{8}{5em}{Initial distributions} & Phase 1 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\the entire state-space\end{tabular}\\
        \cline{2-3}
        & Phase 1 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire states outside T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ states within T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire states outside T1-zone\end{tabular}\\
        \hline
        \multirow{2}{5em}{Training steps} & Phase 1 steps & $10^6$\\
        & Phase 2 steps & $1.5 \times 10^6$\\
        \hline
        \multirow{4}{5em}{Other details} 
        & \begin{tabular}[h]{@{}c@{}}Number of steps\\ before an episode terminates\end{tabular} & $1000$\\
        & Training steps between two evaluations & $10000$\\
        & Number of runs & $10$\\
        & Number of evaluation episodes & $8$\\
    \hline
    \end{tabular}
    \caption{Experiment setup for testing the DreamerV2 method on the ReacherLoCA domain.}
    \label{tab: Experiment Setup - DreamerV2 - ReacherLoCA}
\end{table}

Tables \ref{tab: Experiment Setup - PlaNet - MiniGridLoCA} and \ref{tab: Experiment Setup - DreamerV2 - ReacherLoCA} show the experiment setup we used to evaluate the PlaNet and the DreamerV2 agents' adaptivity under the LoCA setup respectively. It is worth mentioning that \citet{pmlr-v162-wan22d} stayed as close as possible to the original Reacher domain \citep{tassa2018deepmind} when creating the ReacherLoCA to facilitate reusing the best hyperparameters previously used as much as possible. A transition to the targets in the ReacherLoCA domain does not terminate the episode and the agent keeps receiving rewards until $1000$ timesteps. Note that having targets instead of terminal states does not affect the requirements of the LoCA setup.
%and also alleviates the need for a full hyperparameter search for a given method. We can simply use the already identified good hyperparameters that previously worked for the Reacher domain.


\subsection{Hyperparameters}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c}
        \hline
        \begin{tabular}[h]{@{}c@{}}Embedding network\\ architecture\end{tabular} &
        \begin{tabular}[h]{@{}c@{}}CNN:\\ (Channels:$[32 \times 64 \times 128 \times 256]$\\
        Kernel Sizes:$[4 \times 4 \times 4 \times 4]$\\
        Strides:$[2 \times 2 \times 2 \times 2]$),\\
        Followed by MLP:$[512 \times 64, 32]$,\\ Activation Function: \textit{relu}\end{tabular}\\
        \begin{tabular}[h]{@{}c@{}}Optimizer\end{tabular} & Adam, learning rate: $10^{-4}$\\
        \hline
        \begin{tabular}[h]{@{}c@{}}$\beta$\end{tabular} & $50$\\
        \begin{tabular}[h]{@{}c@{}}Number of negative samples\end{tabular} & $128$\\
        \begin{tabular}[h]{@{}c@{}}Mini-batch size\end{tabular} & $32$\\
        \begin{tabular}[h]{@{}c@{}}Total number of random steps\\ for creating dataset $\sD$\end{tabular} & $10^{5}$\\
        \begin{tabular}[h]{@{}c@{}}Number of training epochs\end{tabular} & $5$\\
        \hline
        \begin{tabular}[h]{@{}c@{}}$D_{local}$\end{tabular} & $0.05$\\
        \begin{tabular}[h]{@{}c@{}}$N_{local}$\end{tabular} & $10$\\
        \hline
    \end{tabular}
    \caption{Hyperparameters used for the \methodname~replay buffer on the ReacherLoCA domains.}
    \label{tab: LoFo ReacherLoCA Params}
\end{table}

Table \ref{tab: LoFo ReacherLoCA Params} shows the values of the hyperparameters used in the \methodname~replay buffer for generating the results presented in Figures \ref{Final DreamerV2 + ReacherLoCA Results} and \ref{Final PlaNet + ReacherLoCA Results}. Furthermore, we searched over different values of $N_{local} \in \{2, 5, 10, 20, 40, 80\}$. For both PlaNet and DreamerV2 agents the best setting in our experiments is $N_{local} = 10$.

For the rest of the hyperparameters, we use the hyperparameter values used by \citet{pmlr-v162-wan22d} for the DreamerV2 agent for both when it uses the \methodname~replay buffer and the traditional replay buffer. Furthermore, instead of having an entropy regularizer for the exploration, we add noise to actions (the default value used in the original Dreamer method \citep{hafner2019dream}) since the ReacherLoCA domain is a relatively simple environment.

For the PlaNet agent, we perform a hyperparameter search only for the learning rate ($lr \in \{3*10^{-4}, 10^{-4}\}$). The best learning rate for the agent that uses the \methodname~replay buffer is $3*10^{-4}$. Other hyperparameters are the same as the best setting in \citet{pmlr-v162-wan22d}.

\subsection{Additional Results for PlaNet}

\begin{figure}[h]
    \centering
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/dreamerv2_heatmap_reacherloca_optimal_phase_2.pdf}
    \end{subfigure}
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/planet_heatmap_reacherloca_LoFo_phase_2.pdf}
    \end{subfigure}
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figures/planet_heatmap_reacherloca_FIFO_phase_2.pdf}
    \end{subfigure}

    \caption{Visualization of the estimated rewards from the PlaNet agent's reward model at the end of Phase 2. Each heatmap's $x$ and $y$ axes represent the agent's position in the ReacherLoCA domain.}
    \label{Final Reward Est PlaNet ReacherLoCA}
\end{figure}

In Figure \ref{Final Reward Est PlaNet ReacherLoCA}, we visualize the reward predictions of the PlaNet agents that use the \methodname~replay buffer and the traditional FIFO replay buffer. We also added the true reward visualization (as optimal) for reference. The results are similar to those of DreamerV2’s (Figure \ref{Final Reward Est Dreamer ReacherLoCA}).

% \clearpage

\section{Additional Details for the Experiments on the RandomizedReacherLoCA Setup}
\label{randomreacher_appendix}

\subsection{Experiment Setup And Hyperparameters}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \multirow{8}{5em}{Initial distributions} & Phase 1 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\the entire state-space\end{tabular}\\
        \cline{2-3}
        & Phase 1 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire states outside T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 training  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ states within T1-zone\end{tabular}\\
        \cline{2-3}
        & Phase 2 evaluation  & \begin{tabular}[h]{@{}c@{}}Uniform distribution over\\ the entire states outside T1-zone\end{tabular}\\
        \hline
        \multirow{2}{5em}{Training steps} & Phase 1 steps & $1.5 \times 10^6$\\
        & Phase 2 steps & $3.5 \times 10^6$\\
        \hline
        \multirow{4}{5em}{Other details} 
        & \begin{tabular}[h]{@{}c@{}}Number of steps\\ before an episode terminates\end{tabular} & $1000$\\
        & Training steps between two evaluations & $10000$\\
        & Number of runs & $10$\\
        & Number of evaluation episodes & $8$\\
    \hline
    \end{tabular}
    \caption{Experiment setup for testing the DreamerV2 method on the RandomizedReacherLoCA domain.}
    \label{tab: Experiment Setup - DreamerV2 - RandomizedReacherLoCA}
\end{table}

In the RandomizedReacherLoCA, the location of the red target (T1) is randomly taken from the circle centered in the center of the state-space (the dotted black circle in Figure \ref{RandomizedReacherLoCA_Illustration}). And then, the green target (T2) is placed at the opposite end of the circle. 
%The crucial factor before an episode begins is that the targets are placed in the state-space before the initialization of the agent. 
Table \ref{tab: Experiment Setup - DreamerV2 - RandomizedReacherLoCA} shows the experiment setup we used to evaluate the DreamverV2 agent’s adaptivity under the LoCA setup.

The best hyperparameter setting for the \methodname~replay buffer is exactly as what is mentioned in Table \ref{tab: LoFo ReacherLoCA Params}, except that $N_{local} = 2$ (we searched among $N_{local} \in \{1, 2, 5, 10\}$). Otherwise, we use the same hyperparameters for the DreamerV2 agent as used in the ReacherLoCA setup.


\subsection{Additional Results for The Dreamer}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.55\textwidth]{Figures/randomizedreacherloca_lofo_search.pdf}
    \caption{Learning curves for the DreamerV2 agent that uses the \methodname~replay buffer with different $N_{local}$ values ($D_{local} = 0.05$), each averaged over 10 random seeds.}
    \label{fig: RandomizedReacherLoCA Hyperparam Search N_local}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.55\textwidth]{Figures/randomizedreacherloca_lofo_search_dlocal.pdf}
    \caption{Learning curves for the DreamerV2 agent that uses the \methodname~replay buffer with different $D_{local}$ values ($N_{local} = 2$), each averaged over 5 random seeds.}
    \label{fig: RandomizedReacherLoCA Hyperparam Search D_local}
\end{figure}

Figure \ref{fig: RandomizedReacherLoCA Hyperparam Search N_local} and \ref{fig: RandomizedReacherLoCA Hyperparam Search D_local} shows the learning curves of our search over different values of $N_{local}$ and $D_{local}$ for the DreamerV2 agent using the LOFO replay buffer. Similar to the MiniGridLoCA domain, we observe that small values of $D_{local}$ result in successful adaptation in Phase 2 for the RandomizedReacherLoCA. 


\clearpage

\section{\methodname~Replay Buffer with Recurrent Models}
\label{recurrent_appendix}

In this section, we show that \textit{trajectory-buffer} can be bounded to a maximum sample-size of $B \times N$ in practice, where $B$ is the size of the \text{state-buffer} and $N$ is the size of a sample-sequence used for updates.

As mentioned in Section \ref{sec: adaptive recurrent models}, upon removing $s_i$ from the state-buffer, $r_i$ is set to $None$ in the trajectory-buffer and the agent never uses a sample-sequence starting from $s_i$. Now, if we look closely at the trajectory-buffer, only sample-sequences starting with states $s_k$ where $k \in (i-N,i]$ contains the $(s_i, a_i, r_i=None, s_{i+1})$. Conceptually, we can remove $(s_i, a_i, r_i, s_{i+1})$ from the trajectory-buffer if $s_k, \forall{k \in (i-N,i]}$ are removed from the state-buffer, because in this case there remains no sample-sequence containing $(s_i, a_i, r_i, s_{i+1})$ for training the agent. By doing so, we argue that no $N$ consecutive $None$ rewards can be found in the trajectory buffer. Because in that case, the last $None$ reward belongs to no valid sample-sequence, and therefore, it should have been removed from the trajectory-buffer.

Given that for each state in the state-buffer, we know their corresponding reward in the trajectory-buffer is not $None$, in the worst-case scenario, there can be at most $N-1$ $None$ rewards after such samples in the trajectory-buffer. Hence, by counting them as well, each sequence in the trajectory-buffer starting from a state in the state-buffer can be at most of the length $N$. Therefore, the total number of samples stored in the trajectory-buffer would be at most $B \times N$.


\end{document}
