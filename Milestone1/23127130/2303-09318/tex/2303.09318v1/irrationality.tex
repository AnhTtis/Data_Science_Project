%% LyX 2.3.6.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=2.5cm,lmargin=3cm,rmargin=2cm}
\usepackage{color}
\usepackage{babel}
\usepackage{refstyle}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[all]{xy}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{
 pdfauthor={Ofir David},
 pdfkeywords={generalized continued fraction, irrationality}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\AtBeginDocument{\providecommand\secref[1]{\ref{sec:#1}}}
\AtBeginDocument{\providecommand\partref[1]{\ref{part:#1}}}
\AtBeginDocument{\providecommand\subsecref[1]{\ref{subsec:#1}}}
\AtBeginDocument{\providecommand\corref[1]{\ref{cor:#1}}}
\AtBeginDocument{\providecommand\lemref[1]{\ref{lem:#1}}}
\AtBeginDocument{\providecommand\exaref[1]{\ref{exa:#1}}}
\AtBeginDocument{\providecommand\thmref[1]{\ref{thm:#1}}}
\AtBeginDocument{\providecommand\appref[1]{\ref{app:#1}}}
\AtBeginDocument{\providecommand\claimref[1]{\ref{claim:#1}}}
\AtBeginDocument{\providecommand\defref[1]{\ref{def:#1}}}
\AtBeginDocument{\providecommand\remref[1]{\ref{rem:#1}}}
\AtBeginDocument{\providecommand\eqref[1]{\ref{eq:#1}}}
\AtBeginDocument{\providecommand\enuref[1]{\ref{enu:#1}}}
\AtBeginDocument{\providecommand\factref[1]{\ref{fact:#1}}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}
\RS@ifundefined{subsecref}
  {\newref{subsec}{name = \RSsectxt}}
  {}
\RS@ifundefined{thmref}
  {\def\RSthmtxt{theorem~}\newref{thm}{name = \RSthmtxt}}
  {}
\RS@ifundefined{lemref}
  {\def\RSlemtxt{lemma~}\newref{lem}{name = \RSlemtxt}}
  {}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{cor}[thm]{\protect\corollaryname}
\theoremstyle{definition}
\newtheorem{example}[thm]{\protect\examplename}
\theoremstyle{plain}
\newtheorem{claim}[thm]{\protect\claimname}
\theoremstyle{definition}
\newtheorem{notation}[thm]{\protect\notationname}
\theoremstyle{plain}
\newtheorem{fact}[thm]{\protect\factname}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\hypersetup{pdfstartview=}
%
\newref{def}{name=Definition~}
\newref{ex}{name=Example~}
\newref{exa}{name=Example~}
\newref{sub}{name=Subsection~}
\newref{cor}{name=Corollary~}
\newref{rem}{name=Remark~}
\newref{prop}{name=Proposition~}
\newref{lem}{name=Lemma~}
\newref{thm}{name=Theorem~}
\newref{sec}{name=Section~}
\newref{claim}{name=Claim~}
\newref{app}{name=Appendix~}

\hypersetup{citecolor=blue}

\makeatother

\providecommand{\claimname}{Claim}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\factname}{Fact}
\providecommand{\lemmaname}{Lemma}
\providecommand{\notationname}{Notation}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\global\long\def\call{\mathcal{L}}%
\global\long\def\nn{\mathcal{N}}%
\global\long\def\ff{\mathcal{F}}%
\global\long\def\aa{\mathcal{A}}%
\global\long\def\RR{\mathbb{R}}%
\global\long\def\EE{\mathbb{E}}%
\global\long\def\CC{\mathbb{C}}%
\global\long\def\QQ{\mathbb{Q}}%
\global\long\def\ZZ{\mathbb{Z}}%
\global\long\def\NN{\mathbb{N}}%
\global\long\def\KK{\mathbb{K}}%
\global\long\def\SL{\mathrm{SL}}%
\global\long\def\ds{\mathrm{ds}}%
\global\long\def\dnu{\mathrm{d\nu}}%
\global\long\def\dmu{\mathrm{d\mu}}%
\global\long\def\dt{\mathrm{dt}}%
\global\long\def\dw{\mathrm{dw}}%
\global\long\def\dx{\mathrm{dx}}%
\global\long\def\dy{\mathrm{dy}}%
\global\long\def\norm#1{\left\Vert #1\right\Vert }%
\global\long\def\limfi#1{{\displaystyle \lim_{#1\to\infty}}}%
\global\long\def\arrfi#1{\overset{#1\to\infty}{\longrightarrow}}%
\global\long\def\flr#1{\left\lfloor #1\right\rfloor }%
\global\long\def\lcm{\mathrm{lcm}}%

\title{The conservative matrix field}
\author{Ofir David}
\maketitle
\begin{abstract}
We provide a more accessible approach to Apéry's proof that the Riemann
zeta function at 3 is irrational. To achieve this, we introduce a
new structure called the conservative matrix field, which facilitates
the proof and can be applied to other mathematical constants, such
as $e,\pi,\ln\left(2\right)$, in order to study their properties.
The results obtained in this paper not only offer a more accessible
proof of Apery's theorem, but also pave the way for further research
and discovery in this field and relates it to other fields in number
theory.
\end{abstract}

\section{Introduction}

The Riemann zeta function $\zeta\left(s\right)$ is a complex valued
function that plays a crucial role in mathematics. It is defined as
$\zeta\left(s\right)=\sum_{n=1}^{\infty}\frac{1}{n^{s}}$ for complex
numbers $s$ with $Re\left(s\right)>1$ and can be extended analytically
to all of $\CC$ with a simple pole at $s=1$. In particular, the
values $\zeta\left(d\right)$ for integers $d\geq2$ have significant
implications in a number of areas, e.g. $\frac{1}{\zeta\left(d\right)}$
is the probability of choosing a random integer which is not divisible
by $m^{d}$ for some integer $m\geq1$. While the even evaluations
$\zeta\left(2d\right)$ are well understood, with $\zeta\left(2\right)=\frac{\pi^{2}}{6}$
and more generally $\frac{\zeta\left(2d\right)}{\pi^{2d}}$ are rational
numbers, the behavior of odd evaluations $\zeta\left(2d+1\right)$
remains largely unknown.

One of the main results about these odd evaluations was in 1978 where
Apéry showed that $\zeta\left(3\right)$ is irrational \cite{apery_irrationalite_1979}.
While Apéry's proof was complicated, subsequent attempts were made
to explain and simplify it, see for example van der Poorten \cite{van_der_poorten_proof_1979},
and others tried to reprove it all together, e.g. Beukers in \cite{beukers_note_2013}.
Subsequent research \cite{rivoal_fonction_2000,zudilin_one_2001}
has also shown that there are infinitely many odd integers for which
$\zeta\left(2d+1\right)$ is irrational, and in particular at least
one of $\zeta\left(5\right),\zeta\left(7\right),\zeta\left(9\right)$,
and $\zeta\left(11\right)$ is irrational.

The aim of this paper is to provide a clearer proof of Apéry's theorem
through the creation of a novel mathematical structure referred to
as the \textbf{conservative matrix field}. This structure will allow
us to understand Apéry's original proof and provide a framework for
studying other natural constants such as $\zeta\left(2\right),\pi$,
and $e$, with the potential to uncover new relationships and properties
among them. Moreover, while we will mainly work over the integers,
this structure seem to have natural generalizations for general metric
fields.\\

The conservative matrix field structure is based on generalized continued
fractions, which are number presentations of the form
\[
\KK_{1}^{\infty}\frac{b_{k}}{a_{k}}:=\frac{b_{1}}{a_{1}+\frac{b_{2}}{a_{2}+\frac{b_{3}}{a_{3}+\ddots}}}\qquad a_{i},b_{i}\in\CC,
\]
namely, the limit, if it exists, of the \textbf{convergents} defined
by
\[
\frac{p_{n}}{q_{n}}=\KK_{1}^{n-1}\frac{b_{k}}{a_{k}}=\frac{b_{1}}{a_{1}+\frac{b_{2}}{a_{2}+\frac{\ddots}{\frac{b_{n-1}}{a_{n-1}+0}}}}.
\]
Their much more well known cousins, the simple continued fractions
where $b_{k}=1$ and $a_{k}\geq1$ are integers, have been studied
extensively and are connected to many research areas in mathematics
and in general. In particular, the original goal of these continued
fractions was to find the ``best'' rational approximations for a
given irrational number, which are given by the convergents defined
above. 

While irrational numbers have a unique simple continued fraction expansion
(and rationals have two expansions), there can be many presentations
in the generalized version (more details in \secref{Generalized-continued-fractions}).
The uniqueness in the simple continued fraction expansion allows us
to extract a lot of information from the expansion, and while we lose
this property, what we gain is the option to find ``nice'' generalized
continued fractions which are easier to work with. In particular,
we are interested in \textbf{polynomial continued fractions} where
$a_{k}=a\left(k\right),\;b_{k}=b\left(k\right)$ with $a,b\in\ZZ\left[x\right]$. 

For example, in the $\zeta\left(3\right)$ case, the simple continued
fraction is 
\[
\zeta\left(3\right)=[1;4,1,18,1,1,1,4,1,9,...]=1+\frac{1}{4+\frac{1}{1+\frac{1}{18+\frac{1}{1+\ddots}}}},
\]
where the coefficients $1,4,1,18,1,...$ don't seem to have any usable
pattern. However, it has a much simpler generalized continued fraction
form
\[
\zeta\left(3\right)=\frac{1}{1+\KK_{1}^{\infty}\frac{-i^{6}}{i^{3}+\left(1+i\right)^{3}}}=\frac{1}{1-\frac{1^{6}}{1^{3}+2^{3}-\frac{2^{6}}{2^{3}+3^{3}-\frac{3^{6}}{3^{3}+4^{3}-\ddots}}}},
\]
where the convergents in this expansion are the standard approximations
$\sum_{1}^{n}\frac{1}{k^{3}}$ for $\zeta\left(3\right)$. Moreover,
this abundance of presentations allows us to find many presentations
for $\zeta\left(3\right)$ which can be combined together to find
a ``good enough'' presentation where the convergents converge fast
enough to prove that $\zeta\left(3\right)$ is irrational. In particular,
in Apéry's original proof, and in our, we eventually show that
\[
\zeta\left(3\right)=\frac{6}{5+\KK_{1}^{\infty}\frac{-k^{6}}{17\left(k^{3}+\left(1+k\right)^{3}\right)-12\left(k+\left(1+k\right)\right)}}.
\]
\\

The irrationality proof uses a very elementary argument (see \secref{Irrationality-testing})
that shows that if $\frac{p_{n}}{q_{n}}\to L$ where $\frac{p_{n}}{q_{n}}$
are reduced rational numbers with $\left|q_{n}\right|\to\infty$,
and $\left|L-\frac{p_{n}}{q_{n}}\right|=o\left(\frac{1}{\left|q_{n}\right|}\right)$,
then $L$ must be irrational. Moreover, we can measure how irrational
$L$ is by looking for $\delta>0$ such that $\left|L-\frac{p_{n}}{q_{n}}\right|\sim\frac{1}{\left|q_{n}\right|^{1+\delta}}$.
The main object of this study, the \textbf{conservative matrix field}
defined in \secref{Definition-properties},\textbf{ }is an algebraic
object that collects infinitely many related such approximations $\frac{p_{n,m}}{q_{n,m}}$
arranged on the integer lattice in the positive quadrant. Computing
$\delta_{n,m}$ for each approximation, namely $\delta_{n,m}=-1-\frac{\ln\left|L-\frac{p_{n,m}}{q_{n,m}}\right|}{\ln\left|q_{n,m}\right|}$
where the rational $\frac{p_{n,m}}{q_{n,m}}$ is reduced, and plotting
them as a heat map we get the following

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.25]{heat_map_zeta3.jpeg}
\par\end{centering}
\caption{(Figure by Rotem Elimelech) The gradient color from red$\to$white$\to$blue
correspond to $\delta_{n,m}=-1-\log_{q_{n,m}}\left|L-\frac{p_{n,m}}{q_{n,m}}\right|$
going from positive$\to$zero$\to$negative. }

\end{figure}

As we shall see, the $X$-axis and $Y$-axis correspond more or less
to the standard approximations of $\zeta\left(3\right)$, namely $\sum_{1}^{n}\frac{1}{k^{3}}$,
which do not converge fast enough to show irrationality, while on
the diagonal we get the expansion mentioned above used by Apéry to
prove the irrationality.\\

The conservative matrix field structure is not only a way to understand
Apéry's original proof, but seems to have a much broader range of
applications. There are many places that study generalized continued
fraction and in particular polynomial continued fractions (see for
example \cite{apostol_introduction_1998,jones_continued_1980,pincherle_delle_1894,laughlin_real_2004})
. This paper originated in the Ramanujan machine project \cite{raayoni_generating_2021}
which aimed to find simple polynomial continued fraction presentations
to interesting mathematical constants using computer automation. With
the goal of trying to prove many of the conjectures discovered by
the computer, and along the way understand Apery's proof, this conservative
matrix field structure was found. These computer conjectures suggest
that there is still much to be explored in this field and that this
new structure is just a step towards a deeper understanding of mathematical
constants and their relations.

\subsection{Structure of the paper}

This paper is divided into two parts. In \partref{Introduction-to-generalized}
we mainly go over basic and elementary results about generalized continued
fractions and their Mobius transformation generalizations. Many of
the results there are either known or generalization and reformulations
of known results in the context of the polynomial continued fractions.
This part is mainly here to make this paper self contained, and also
introduce the settings of the wider world in which the conservative
matrix field lives, and a bit deeper look into the tools in this world.
In \partref{The-conservative-matrix} we will define what is the conservative
matrix field and how to use it to show the irrationality of $\zeta\left(3\right)$.\\

More specifically, as the generalized continued fraction expansion
are much less known than their simple versions (namely the denominators
are positive integers and numerators are 1), we begin in \secref{Generalized-continued-fractions}
by going over the definitions, notations and some properties of these
generalized continued fractions. In particular, while some of the
results about simple continued fractions do not hold for their generalized
versions, one of the main tools that we do gain, is the possibility
to move from infinite sums to generalized continued fractions and
back, via Euler's conversion, which we describe in \subsecref{Euler's-formula}.

From this point on, since the main focus of this paper are the generalized
continued fractions (and even \emph{more} generalized versions of
them), we will simply call them continued fractions, and we will always
add the ``\textbf{simple}'' adjective when referring to simple continued
fractions.

As with the simple continued fractions, our new continued fraction
presentation is also closely connected to rational approximations,
and in \secref{Irrationality-testing} we show how these approximations
can be used to show that a given number is irrational. However, not
every such presentation is enough, even if the number is irrational,
and in order to find better and better presentations, we move from
these generalized continued fractions to an even more generalized
form. It is well known that we can use Mobius transformation to represent
and study simple continued fractions, and as we shall see the same
holds for generalized continued fractions. As these Mobius transformations
described by product of $2\times2$ matrices, we are naturally led
to study general products of $2\times2$ matrices, and the corresponding
Mobius transformation. This is done in \secref{The-most-general},
where two of the main goal is to understand how two such presentations
relate to one another, and in particular what happens when one presentation
arise from continued fractions.\\

In \secref{Definition-properties} we collect many continued fractions
into the single object of conservative matrix field, and study its
properties. In particular, we describe how to construct many examples
of such matrix fields, related to interesting mathematical constants
like $\zeta\left(2\right),\zeta\left(3\right),e,\pi$ etc. Finally
in \secref{The-z3-case} we apply the results found so far to reprove
that $\zeta\left(3\right)$ is irrational.

We then end the paper in \secref{On-future-fractions} with a discussion
about several directions which can generalize this matrix field structure,
and possibly connect it to many other research areas.

\newpage{}

\part{\label{part:Introduction-to-generalized}Introduction to generalized
continued fraction}

\section{\label{sec:Generalized-continued-fractions}Definitions and examples}

\subsection{The definitions}

We start with a generalization of the simple continued fractions,
which unsurprisingly, called generalized continued fractions. These
can be defined over any topological field, though here we focus on
the complex field with its standard euclidean metric, and more specifically
when the numerators and denominators are integers.
\begin{defn}
Let $a_{n},b_{n}$ be a sequence of complex numbers. We will write
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}:=\frac{b_{1}}{a_{1}+\frac{b_{2}}{a_{2}+\frac{b_{3}}{a_{3}+\frac{\ddots}{\frac{b_{n}}{a_{n}+0}}}}},
\]
which are in $\CC\cup\left\{ \infty\right\} $, and denote the limit,
if it exists, as
\[
\KK_{1}^{\infty}\frac{b_{i}}{a_{i}}:=\limfi n\KK_{1}^{n}\frac{b_{i}}{a_{i}}.
\]

If $\alpha=\KK_{1}^{\infty}\frac{b_{i}}{a_{i}}$, then we will say
that $\KK_{1}^{\infty}\frac{b_{i}}{a_{i}}$ is a \textbf{continued
fraction presentation} of $\alpha$.

\end{defn}

In the \textbf{simple continued fractions} presentations the sequence
$b_{i}$ is the constant 1 sequence, while the $a_{i}$ are positive
integers, in which case we usually write
\begin{align*}
\left[a_{0};a_{1},a_{2},...,a_{n}\right] & =a_{0}+\KK_{1}^{n}\frac{1}{a_{i}}\\
\left[a_{0};a_{1},a_{2},...\right] & :=a_{0}+\KK_{1}^{\infty}\frac{1}{a_{i}}.
\end{align*}
This simple continued fraction presentation of numbers enjoys several
interesting properties. In particular, the limits above always converge,
and the continued fraction is rational if and only if its expansion
is finite. Moreover, every number can be written as a simple continued
fraction, where irrational numbers have unique presentation, and every
rational has exactly two presentations (this is because $\frac{1}{n}=\frac{1}{\left(n-1\right)+\frac{1}{1}}$).
These $a_{i}$ can be retrieved from applying the Euclidean division
algorithm, as can be seen in the example below:

\begin{algorithm}[H]
\begin{centering}
\begin{tabular}{|c|c|}
\hline 
$\begin{alignedat}{1}15 & =\boldsymbol{1}\cdot11+4\\
11 & =\boldsymbol{2}\cdot4+3\\
4 & =\boldsymbol{1}\cdot3+1\\
3 & =\boldsymbol{3}\cdot1+0
\end{alignedat}
$ & {\large{}$\begin{alignedat}{1}\frac{15}{11} & =1+\frac{1}{2+\frac{1}{1+\frac{1}{3}}}\\
 & =[1;2,1,3]
\end{alignedat}
$}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{Finding the simple continued fraction of $\frac{15}{11}$ using the
Euclidean division algorithm}
\end{algorithm}

However, while we have an algorithm to find the (almost) unique sequence
$a_{i}$, in general they can be very complicated without any known
patterns, even for ``nice'' numbers, for example:

\[
\pi=[3;7,15,1,292,1,1,1,2,1,3,1,14,2,1,1,2,2,2,2,1,84,2,1,1,15,3,13,...].
\]

When moving to generalized continued fractions, even when we assume
that both $a_{i}$ and $b_{i}$ are integers, we lose the uniqueness
property, and the rational if and only if finite property. What we
gain in return are more presentations for each number, where some
of them can be much simpler to use. For example, $\pi$ can be written
as 
\[
\pi=3+\KK_{1}^{\infty}\frac{\left(2n-1\right)^{2}}{6}.
\]
We want to study these presentations, and (hopefully) use them to
show interesting properties, e.g. prove irrationality for certain
numbers.
\begin{defn}
Let $a_{n},b_{n}$ be a sequence of integers. In this case the $\KK_{1}^{n}\frac{b_{i}}{a_{i}}$
are rational numbers (when they are defined, and not infinity). If
they converge, then we call them the \textbf{convergents} for that
generalized continued fraction.
\end{defn}

One of the main tools used to study simple continued fractions are
\textbf{Mobius transformations}. Recall that given a $2\times2$ invertible
matrix $M=\left(\begin{smallmatrix}a & b\\
c & d
\end{smallmatrix}\right)$ over the complex numbers and a complex number $z$, the Mobius action
is defined by
\[
M\left(z\right)=\frac{az+b}{cz+d}.
\]
In other words, we apply the standard matrix multiplication $\left(\begin{smallmatrix}a & b\\
c & d
\end{smallmatrix}\right)\left(\begin{smallmatrix}z\\
1
\end{smallmatrix}\right)=\left(\begin{smallmatrix}az+b\\
cz+d
\end{smallmatrix}\right)$ and project it onto $R^{1}\CC$ by dividing the $x$-coordinate by
the $y$-coordinate.

By this definition, it is easy to see that 
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{b_{1}}{a_{1}+\frac{b_{2}}{a_{2}+\frac{\ddots}{\frac{b_{n}}{a_{n}+0}}}}=\left(\begin{smallmatrix}0 & b_{1}\\
1 & a_{1}
\end{smallmatrix}\right)\left(\begin{smallmatrix}0 & b_{2}\\
1 & a_{2}
\end{smallmatrix}\right)\cdots\left(\begin{smallmatrix}0 & b_{n}\\
1 & a_{n}
\end{smallmatrix}\right)\left(0\right)
\]

This presentation allows us to show an interesting recurrence relation
on the numerators and denominators of the convergents, which generalizes
the well known recurrence on simple continued fractions.
\begin{lem}
\label{lem:gcf-recursion}Let $a_{n},b_{n}$ be a sequence of integers.
Define $M_{n}=\left(\begin{smallmatrix}0 & b_{n}\\
1 & a_{n}
\end{smallmatrix}\right)$ and set $\left(\begin{smallmatrix}p_{n}\\
q_{n}
\end{smallmatrix}\right)=\left(\prod_{1}^{n-1}M_{i}\right)\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)$. Then $\frac{p_{n}}{q_{n}}=\prod_{1}^{n-1}M_{i}\left(0\right)=\KK_{1}^{n-1}\frac{b_{i}}{a_{i}}$
are the convergents of the generalized continued fraction presentation.
More over, we have that $\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)=\prod_{1}^{n-1}M_{i}$, implying the same recurrence relation on $p_{n}$ and $q_{n}$ given
by 
\begin{align*}
p_{n+1} & =a_{n}p_{n}+b_{n}p_{n-1}\\
q_{n+1} & =a_{n}q_{n}+b_{n}q_{n-1},
\end{align*}
with starting condition $p_{0}=1,\;p_{1}=0$ and $q_{0}=0,\;q_{1}=1$.
\end{lem}

\begin{proof}
Our definition of $\left(\begin{smallmatrix}p_{n}\\
q_{n}
\end{smallmatrix}\right)=\left(\prod_{1}^{n-1}M_{i}\right)\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)$ simply gives us the right column of $\prod_{1}^{n-1}M_{i}$. Since
$M_{i}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)$, we see that 
\[
\left(\prod_{1}^{n-1}M_{i}\right)\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=\left(\prod_{1}^{n-2}M_{i}\right)\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)=\left(\begin{smallmatrix}p_{n-1}\\
q_{n-1}
\end{smallmatrix}\right)\quad\forall n\geq2,
\]
and for $n=1$ we have $\overbrace{\left(\prod_{1}^{n-1}M_{i}\right)}^{=Id}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=\left(\begin{smallmatrix}p_{0}\\
q_{0}
\end{smallmatrix}\right)$ , so together we have that 
\[
\prod_{1}^{n-1}M_{i}=\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)\quad\forall n\geq1.
\]

From this equation we get the matrix recurrence $\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)M_{n}=\left(\begin{smallmatrix}p_{n} & p_{n+1}\\
q_{n} & q_{n+1}
\end{smallmatrix}\right)$ , implying the same recurrence on $p_{n},q_{n}$ .
\end{proof}

\newpage{}

\subsection{\label{subsec:Euler's-formula}Euler's formula}

For any number, the Euclidean division algorithm can be used to find
its simple continued fraction expansion. With generalized continued
fractions, we don't have a unique presentation anymore, so there isn't
a single algorithm to find an expansion. However, this allow us the
option of looking for a suitable presentation which is easier to work
with, and maybe move between such presentations (which will be one
of our main tools when trying to reprove Apery's theorem).

One of the most elementary and useful continued fraction presentation
was introduced by Euler who found a way to convert standard finite
sums (and their infinite sum limits) to generalized continued fraction. 
\begin{thm}[Euler's formula]
 Let $r_{i}\in\CC$ for $i\geq1$. Then
\[
1+r_{1}+r_{1}r_{2}+\cdots+r_{1}\cdots r_{n}=\sum_{k=0}^{n}\left(\prod_{i=1}^{k}r_{i}\right)=\frac{1}{1+\KK_{1}^{n}\frac{-r_{i}}{1+r_{i}}}.
\]
By taking the limit (if exists), we have that 
\[
\KK_{1}^{\infty}\frac{-r_{i}}{1+r_{i}}=\frac{1}{\sum_{k=0}^{\infty}\prod_{i=1}^{k}r_{i}}-1.
\]
\end{thm}

\begin{proof}
Standard induction.
\end{proof}
For more details and applications of this formula, the reader is referred
to \cite{jones_continued_1980}. 

Euler's formula also implies that whenever $a_{i}+b_{i}=1$ we can
go back from generalized continued fractions $\KK_{1}^{\infty}\frac{b_{i}}{a_{i}}$
to infinite sums, where we have many more tools at our disposal. However,
in general this condition doesn't hold, but fortunately there is a
trick to move to equivalent presentations where it might.
\begin{lem}[The equivalence transformation]
\label{lem:equivalence-transformation} Let $a_{i},b_{i}\in\CC$
we two sequences and $0\neq c_{i}\in\CC$ another sequence with nonzero
elements. Then
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{1}{c_{0}}\KK_{1}^{n}\frac{c_{i-1}c_{i}b_{i}}{c_{i}a_{i}}.
\]
\end{lem}

\begin{proof}
Intuitively, this lemma follows from the fact that $\frac{b_{i}}{a_{i}+x}=\frac{c_{i}b_{i}}{c_{i}a_{i}+c_{i}x}$
plus induction. For example
\[
\frac{4}{11}=\frac{1}{2+\frac{1}{1+\frac{1}{3}}}=\frac{1}{{\color{red}\boldsymbol{c_{0}}}}\cdot\frac{{\color{red}\boldsymbol{c_{0}}}}{2+\frac{1}{1+\frac{1}{3}}}=\frac{1}{c_{0}}\cdot\frac{c_{0}{\color{red}\boldsymbol{c_{1}}}}{2{\color{red}\boldsymbol{c_{1}}}+\frac{{\color{red}\boldsymbol{c_{1}}}}{1+\frac{1}{3}}}=\frac{1}{c_{0}}\cdot\frac{c_{0}c_{1}}{2c_{1}+\frac{c_{1}{\color{red}\boldsymbol{c_{2}}}}{{\color{red}\boldsymbol{c_{2}}}+\frac{{\color{red}\boldsymbol{c_{2}}}}{3}}}=\frac{1}{c_{0}}\cdot\frac{c_{0}c_{1}}{2c_{1}+\frac{c_{1}c_{2}}{c_{2}+\frac{c_{2}{\color{red}\boldsymbol{c_{3}}}}{3{\color{red}\boldsymbol{c_{3}}}}}}.
\]
More precisely, recall that $\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\left[\prod_{1}^{n}\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)\right]\left(0\right)$. As Mobius transformations defined by scalar matrices are the identity,
we get that
\begin{align*}
\left[\prod_{1}^{n}\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)\right]\left(0\right) & =\left[\prod_{1}^{n}\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)\cdot c_{i}I\right]\left(0\right)=\left[\prod_{1}^{n}\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)\cdot\left(\begin{smallmatrix}1 & 0\\
0 & c_{i}
\end{smallmatrix}\right)\left(\begin{smallmatrix}c_{i} & 0\\
0 & 1
\end{smallmatrix}\right)\right]\left(0\right)\\
 & =\left(\begin{smallmatrix}c_{0}^{-1} & 0\\
0 & 1
\end{smallmatrix}\right)\left[\prod_{1}^{n}\left(\left(\begin{smallmatrix}c_{i-1} & 0\\
0 & 1
\end{smallmatrix}\right)\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)\left(\begin{smallmatrix}1 & 0\\
0 & c_{i}
\end{smallmatrix}\right)\right)\right]\left(\begin{smallmatrix}c_{n} & 0\\
0 & 1
\end{smallmatrix}\right)\left(0\right)\\
 & =\left(\begin{smallmatrix}c_{0}^{-1} & 0\\
0 & 1
\end{smallmatrix}\right)\left[\prod_{1}^{n}\left(\begin{smallmatrix}0 & c_{i-1}c_{i}b_{i}\\
1 & c_{i}a_{i}
\end{smallmatrix}\right)\right]\left(\begin{smallmatrix}c_{n} & 0\\
0 & 1
\end{smallmatrix}\right)\left(0\right)
\end{align*}
Since $\left(\begin{smallmatrix}c_{n} & 0\\
0 & 1
\end{smallmatrix}\right)\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)$ , we conclude that
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{1}{c_{0}}\KK_{1}^{n}\frac{c_{i-1}c_{i}b_{i}}{c_{i}a_{i}}.
\]
\end{proof}
\begin{rem}
In the last lemma we basically moved from the matrices $\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)$ to $\left(c_{n-1}U_{n-1}\right)^{-1}M_{n}U_{n}$ with $U_{n}=\left(\begin{smallmatrix}1 & 0\\
0 & c_{n}
\end{smallmatrix}\right)$. This type of equivalence can be generalized, as we shall see it
later in \subsecref{coboundary}.
\end{rem}

Combining this lemma and Euler's formula, we are led to look for $c_{n}$
satisfying
\[
c_{n}a_{n}+c_{n-1}c_{n}b_{n}=1.
\]
If we can find such $c_{n}$, then we have the following.
\begin{cor}
\label{cor:c-n-conversion}Let $a_{i},b_{i}\in\CC$ be any sequences
and suppose that we can find a solution to 
\[
c_{i}a_{i}+c_{i-1}c_{i}b_{i}=1
\]
with nonzero $c_{i}$. Then
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{1}{c_{0}}\KK_{1}^{n}\frac{\left(c_{i-1}c_{i}b_{i}\right)}{\left(c_{i}a_{i}\right)}=\frac{1}{c_{0}}\left(\frac{1}{\sum_{k=0}^{n}\left(-1\right)^{k}\prod_{i=1}^{k}\left(c_{i-1}c_{i}b_{i}\right)}-1\right),
\]
or equivalently
\[
\sum_{k=0}^{n}\left(-1\right)^{k}\prod_{i=1}^{k}\left(c_{i-1}c_{i}b_{i}\right)=\frac{1}{1+c_{0}\cdot\KK_{1}^{n}\frac{b_{i}}{a_{i}}}=\left(\begin{smallmatrix}0 & 1\\
1 & c_{0}
\end{smallmatrix}\right)\left(\KK_{1}^{n}\frac{b_{i}}{a_{i}}\right).
\]
\end{cor}

\begin{example}[The exponential function]
\label{exa:(The-exponential-function)} Given some $x\in\RR$, we start with the standard Taylor expansion
for $e^{x}$:
\[
e^{x}=1+x+\frac{x^{2}}{2}+\frac{x^{3}}{3!}+\cdots=\sum_{0}^{\infty}\frac{x^{n}}{n!}=\sum_{0}^{\infty}\prod_{1}^{n}\left(\frac{x}{i}\right).
\]
Taking $r_{i}=\frac{x}{i}$ in Euler's formula we get that 
\[
e^{x}=\frac{1}{1+\KK_{1}^{\infty}\frac{-x/i}{1+x/i}}.
\]
We would like to use the equivalence transformation with $c_{i}=i$
so as to remove the division in the numerators and denominators, however
since $c_{0}=0$ we cannot directly do it. Instead, we will apply
it starting from the second index, namely
\begin{align*}
\KK_{i=1}^{\infty}\frac{-x/i}{1+x/i} & =\frac{-x}{1+x+\KK_{i=2}^{\infty}\frac{-x/i}{1+x/i}}=\frac{-x}{1+x+\frac{1}{1}\KK_{2}^{\infty}\frac{-x\left(i-1\right)}{i+x}}=\frac{-x}{1+x+\frac{1}{1}\KK_{1}^{\infty}\frac{-xi}{1+i+x}},
\end{align*}
so that 
\[
e^{x}=\frac{1}{1-\frac{x}{1+x-\frac{x}{2+x-\frac{2x}{3+x-\frac{3x}{4+x-\frac{4x}{\ddots}}}}}}.
\]

In particular, for $x=1$ and $x=-1$ we get that 
\[
\frac{e-2}{1-e}=\KK_{1}^{\infty}\frac{-i}{2+i}\quad,\quad\frac{1}{e-1}=\KK_{1}^{\infty}\frac{i}{i}.
\]

Similar computation can be done to other functions like $\sin\left(x\right),\cos\left(x\right),\ln\left(1+x\right)$
etc.\\
\end{example}

Finding $c_{n}$ which satisfy the relation in \corref{c-n-conversion}
above is equivalent to solving the recurrence
\[
c_{i}:=\frac{1}{a_{i}+c_{i-1}b_{i}}=\left(\begin{smallmatrix}0 & 1\\
b_{i} & a_{i}
\end{smallmatrix}\right)\left(c_{i-1}\right).
\]
Once we choose $c_{0}$, the rest of the $c_{i}$ are determined by
the recurrence relation, and as long as we don't divide by zero anywhere,
we can transform the generalized continued fraction into an infinite
sum. Of course, the hard part is not to find some sequence $c_{i}$,
but a ``nice enough'' such sequence for which we can compute $\sum_{k=0}^{n}\left(-1\right)^{k}\prod_{i=1}^{k}\left(c_{i-1}c_{i}b_{i}\right)$.

The fact that we got a recurrence relation with the transpose of our
standard matrix $\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)$ is not a coincidence. To give another way of viewing this transformation,
we first linearize the recurrence by setting $c_{i}=\frac{F_{i}}{F_{i+1}}$
so that the recurrence becomes
\[
F_{i}a_{i}+F_{i-1}b_{i}=F_{i+1},
\]
which we can also write as
\[
\left(\begin{smallmatrix}F_{i-1} & F_{i}\end{smallmatrix}\right)\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)=\left(\begin{smallmatrix}F_{i} & F_{i+1}\end{smallmatrix}\right).
\]
This is exactly the recurrence satisfied by $p_{n}$ and $q_{n}$
we saw in \lemref{gcf-recursion} (so that both $c_{i}=\frac{p_{i}}{p_{i+1}}$
and $c_{i}=\frac{q_{i}}{q_{i+1}}$ solve the recurrence above). In
a sense, if we find one ``nice enough'' solution to the recurrence,
then we can understand both of $p_{n}$ and $q_{n}$. More over, the
starting conditions of $p_{n}$ and $q_{n}$ are independent (namely
$\left(\begin{smallmatrix}p_{0} & p_{1}\\
q_{0} & q_{1}
\end{smallmatrix}\right)=Id$ is invertible), so any sequence satisfying the recurrence above is
a linear combination of these two sequences. Thus understanding one
``nice'' solution gives us a lot of information about all the solutions.

Next, we try to give simple conditions on $a_{i},b_{i}$ where we
can find ``nice'' solution for the recurrence. A good starting point
is when $a_{i},b_{i}$ are fixed $a_{i}\equiv a,\;b_{i}\equiv b$,
so that our matrix is $M_{i}=M=\left(\begin{smallmatrix}0 & b\\
1 & a
\end{smallmatrix}\right)$, and a solution $F_{n}$ can be found by looking at $M^{n}$. This
is a standard recursion where $F_{n}$ will be a combination of a
polynomial $f\left(n\right)$ and exponential $h^{n}$, where $h$
is one of the two roots $h_{1},h_{2}$ for the characteristic polynomial
$x^{2}-ax-b=0$, namely $h_{1}\cdot h_{2}=-b$ and $h_{1}+h_{2}=a$. 

More generally, our recurrence depends on $i$, and the ``right''
way to think about exponential is more like factorial, so we should
look for $F_{n}$ of the form $f\left(n\right)\cdot\prod_{1}^{n}h\left(k\right)$
for some polynomials $f,h$ , or in the $c_{n}$ notation we have
$c_{n}=\frac{F_{n}}{F_{n+1}}=\frac{f\left(n\right)}{f\left(n+1\right)h\left(n+1\right)}$. 

With this quadratic intuition in mind, we have the following family
of continued fractions which have this sort of solution to their corresponding
recurrence relation. Special cases of these continued fractions appear
in many places (in particular, see for example \cite{brier_note_2022}),
though we did not see this exact formulation in the literature.
\begin{thm}
\label{thm:recurrence-roots}Let $h_{1},h_{2},f:\CC\to\CC$ be any
functions, and define $a,b:\CC\to\CC$ such that 
\begin{align*}
b\left(x\right) & =-h_{1}\left(x\right)h_{2}\left(x\right)\\
f\left(x\right)a\left(x\right) & =f\left(x-1\right)h_{1}\left(x\right)+f\left(x+1\right)h_{2}\left(x+1\right)
\end{align*}
Then taking $F_{n}=f\left(n\right)\cdot\prod_{1}^{n}h_{2}\left(k\right)$
solves the recurrence relation
\[
F_{n}a\left(n\right)+F_{n-1}b\left(n\right)=F_{n+1},
\]
and we get that
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{f\left(1\right)h_{2}\left(1\right)}{f\left(0\right)}\left(\frac{1}{\sum_{k=0}^{n}\frac{f\left(0\right)f\left(1\right)}{f\left(k\right)f\left(k+1\right)}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)}-1\right).
\]
\end{thm}

\begin{proof}
Simply putting the definition of $a,b,F$ in the recurrence gives
us
\[
F_{n}a\left(n\right)+F_{n-1}b\left(n\right)-F_{n+1}=\left[\prod_{1}^{n}h_{2}\left(k\right)\right]\left(f\left(n\right)a\left(n\right)-f\left(n-1\right)h_{1}\left(n\right)-f\left(n+1\right)h_{2}\left(n+1\right)\right)=0.
\]
Using \corref{c-n-conversion} and taking $c_{n}=\frac{F_{n}}{F_{n+1}}$
we get that
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{F_{1}}{F_{0}}\left(\frac{1}{\sum_{k=0}^{n}\prod_{i=1}^{k}\left(\frac{F_{i-1}}{F_{i+1}}h_{1}\left(i\right)h_{2}\left(i\right)\right)}-1\right)=\frac{f\left(1\right)h_{2}\left(1\right)}{f\left(0\right)}\left(\frac{1}{\sum_{k=0}^{n}\frac{f\left(0\right)f\left(1\right)}{f\left(k\right)f\left(k+1\right)}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)}-1\right).
\]
\end{proof}
\begin{rem}
We leave it as an exercise to show that if we start with polynomials
$a,b\in\CC\left[x\right]$ and we look for solutions to $c_{n}a_{n}+c_{n-1}c_{n}b_{n}=1$
where $a_{n}=a\left(n\right)$ and $b_{n}=b\left(n\right)$ are evaluations,
and where $c_{n}=c\left(n\right)$ for some rational function $c\in\CC\left(x\right)$,
then $a,b$ must has the form as in the theorem above where $f,h_{1},h_{2}$
are all polynomials in themselves and $c\left(x\right)=\frac{f\left(x\right)}{f\left(x+1\right)h_{1}\left(x+1\right)}$.
\end{rem}

While the theorem above is applicable to any functions $h_{1},h_{2}$
and $f$, it is probably most useful when they are polynomials over
$\ZZ$, in which case the product $\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)$
can become much simpler.
\begin{example}[The trivial Euler family]
\label{exa:Euler-family}\footnote{It has come to our attention that Euler doesn't have enough mathematical
objects named after him.} Consider the family from the theorem above where $f\left(x\right)=1$,
so that 
\begin{align*}
b\left(x\right) & =-h_{1}\left(x\right)h_{2}\left(x\right)\\
a\left(x\right) & =h_{1}\left(x\right)+h_{2}\left(x+1\right),
\end{align*}
in which case we have 
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=h_{2}\left(1\right)\left(\frac{1}{\sum_{k=0}^{n}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)}-1\right),
\]
or alternatively
\[
\sum_{k=0}^{n}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)=\left(\frac{1}{h_{2}\left(1\right)}\KK_{1}^{n}\frac{b_{i}}{a_{i}}+1\right)^{-1}.
\]
\begin{enumerate}
\item Let $b\left(x\right)=-1\times x$ and $a\left(x\right)=1+\left(x+1\right)=x+2$.
We then have that $\prod_{i=1}^{k}\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}=\prod_{i=1}^{k}\frac{1}{i+1}=\frac{1}{\left(k+1\right)!}$,
so that
\[
\KK_{1}^{\infty}\frac{-n}{n+2}=\frac{1}{\sum_{k=0}^{\infty}\frac{1}{\left(k+1\right)!}}-1=\frac{1}{e-1}-1,
\]
which we already saw in \exaref{(The-exponential-function)}
\item For some $d\geq2$ let $b\left(x\right)=-x^{d}\times x^{d}$ and $a\left(x\right)=x^{d}+\left(x+1\right)^{d}$.
Then $\prod_{i=1}^{k}\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}=\prod_{i=1}^{k}\frac{i^{d}}{\left(i+1\right)^{d}}=\frac{1}{\left(k+1\right)^{d}}$,
so that
\[
\KK_{1}^{\infty}\frac{b_{n}}{a_{n}}=\frac{1}{\sum_{k=0}^{\infty}\frac{1}{\left(k+1\right)^{d}}}-1=\frac{1}{\zeta\left(d\right)}-1.
\]
\item For some $d\geq3$ let $b\left(x\right)=-x^{d-1}\left(x+2\right)\times x^{d}$
and $a\left(x\right)=x^{d-1}\left(x+2\right)+\left(x+1\right)^{d}$.
We then have that 
\[
\prod_{i=1}^{k}\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}=\prod_{i=1}^{k}\frac{i^{d-1}}{\left(i+1\right)^{d-1}}\cdot\prod_{i=1}^{k}\frac{i+2}{i+1}=\frac{1}{\left(k+1\right)^{d-1}}\cdot\frac{k+2}{2}=\frac{1}{2}\left[\frac{1}{\left(k+1\right)^{d-1}}+\frac{1}{\left(k+1\right)^{d-2}}\right].
\]
Summing up over $k$ gives us
\[
\sum_{k=0}^{\infty}\frac{1}{2}\left[\frac{1}{\left(k+1\right)^{d-1}}+\frac{1}{\left(k+1\right)^{d-2}}\right]=\frac{1}{2}\left(\zeta\left(d-1\right)+\zeta\left(d-2\right)\right).
\]
A similar computation can be done for $b\left(x\right)=-x^{d-1}\left(x+m\right)\times x^{d}$
, $a\left(x\right)=x^{d-1}\left(x+m\right)+\left(x+1\right)^{d}$
where $m\geq0$. Note that for $m=0$ we get the continued fraction
from part $\left(2\right)$ above and for $m=1$ we can use the equivalence
transformation \lemref{equivalence-transformation} to cancel $\left(x+1\right)$
in $a\left(x\right)$ and $x\left(x+1\right)$ in $b\left(x\right)$
and get 
\[
\KK_{1}^{\infty}\frac{b\left(x\right)}{a\left(x\right)}=\KK_{1}^{\infty}\frac{-x^{2\left(d-1\right)}}{x^{\left(d-1\right)}+\left(1+x\right)^{\left(d-1\right)}}=\frac{1}{\zeta\left(d-1\right)}-1.
\]
\item For some $d\geq2$ let $b\left(x\right)=-x^{d}\times x^{d-1}\left(x+1\right)$
and $a\left(x\right)=x^{d}+\left(x+1\right)^{d-1}\left(x+2\right)$.
We then have that 
\begin{align*}
\prod_{i=1}^{k}\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)} & =\prod_{i=1}^{k}\frac{i^{d-1}}{\left(i+1\right)^{d-1}}\cdot\prod_{i=1}^{k}\frac{i}{i+2}=\frac{1}{\left(k+1\right)^{d-1}}\cdot\frac{1\cdot2}{\left(k+1\right)\left(k+2\right)}=\frac{1}{\left(k+1\right)^{d}}\frac{2}{k+2}.
\end{align*}
Since $\frac{1}{j+1}\cdot\frac{1}{j+2}=\frac{1}{j+1}-\frac{1}{j+2}$
for all $j\geq0$, we get by induction that
\[
\frac{1}{\left(k+1\right)^{d}}\cdot\frac{1}{k+2}=\sum_{\ell=2}^{d}\frac{\left(-1\right)^{d-\ell}}{\left(k+1\right)^{\ell}}+\left(-1\right)^{d-1}\left(\frac{1}{k+1}-\frac{1}{k+2}\right).
\]
Summing over this expression we get
\begin{align*}
\sum_{k=0}^{\infty}\left[\sum_{\ell=2}^{d}\frac{\left(-1\right)^{d-\ell}}{\left(k+1\right)^{\ell}}+\left(-1\right)^{d-1}\left(\frac{1}{k+1}-\frac{1}{k+2}\right)\right] & =\sum_{\ell=2}^{d}\left(-1\right)^{d-\ell}\zeta\left(\ell\right)+\left(-1\right)^{d-1}.
\end{align*}
\end{enumerate}
\end{example}

\begin{rem}
In general, the ideas appearing in the examples above can help compute
many of the sums of the form $\sum_{k=0}^{\infty}\frac{f\left(0\right)f\left(1\right)}{f\left(k\right)f\left(k+1\right)}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)$.

First, if $\deg\left(h_{1}\right)>\deg\left(h_{2}\right)$ there is
no convergence, so that we may assume that $\deg\left(h_{1}\right)\leq\deg\left(h_{2}\right)$.

Next, if $\deg\left(h_{1}\right)=\deg\left(h_{2}\right)$ and all
the roots of $h_{1},h_{2}$ are integers, then most of the elements
in $\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)$
will be canceled out and $\frac{f\left(0\right)f\left(1\right)}{f\left(k\right)f\left(k+1\right)}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)$
will be a rational function. Using the standard decomposition of rational
functions, we can write it as a linear combination elements of the
form $\frac{1}{\left(n-\alpha\right)^{d}}$ with integer $\alpha$.
These always converge if $d\geq2$ to values of the zeta function.
The elements for which $d=1$ should be put together to find out their
limits (e.g. $\sum_{0}^{\infty}\left(\frac{1}{k+1}-\frac{1}{k+2}\right)=1$).
This can be slightly generalized, if the roots of $h_{2}$ and $h_{1}$
are the same modulo the integers, in which case still most of the
elements in $\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)$
are canceled.

Finally, if $\deg\left(h_{2}\right)>\deg\left(h_{1}\right)$, we expect
to see all sorts of factorials appearing in the denominators of the
summands, suggesting to look for Taylor expansions find get the limit.\\
\end{rem}

In the examples above we only looked at ``trivial'' solutions where
$f\equiv1$. In general, there are solutions where $f\neq1$ (and
we shall see many of them later), however, they are all part of the
trivial family in disguise. Indeed, if

\begin{align*}
b\left(x\right) & =-h_{1}\left(x\right)h_{2}\left(x\right)\\
f\left(x\right)a\left(x\right) & =f\left(x-1\right)h_{1}\left(x\right)+f\left(x+1\right)h_{2}\left(x+1\right)
\end{align*}
as in the theorem, then using the equivalence transformation from
\lemref{equivalence-transformation} with $c_{n}=f\left(n\right)$,
we get that 
\[
\KK_{1}^{n}\frac{b\left(n\right)}{a\left(n\right)}=\frac{1}{f\left(0\right)}\KK_{1}^{n}\frac{\tilde{b}\left(n\right)}{\tilde{a}\left(n\right)}.
\]
where this new continued fraction is in the trivial Euler family:
\begin{align*}
\tilde{h}_{1}\left(x\right) & =h_{1}\left(x\right)f\left(x-1\right)\qquad;\qquad\tilde{h}_{2}\left(x\right)=h_{2}\left(x\right)f\left(x\right)\\
\tilde{b}\left(x\right) & =f\left(x-1\right)f\left(x\right)b\left(x\right)=-\tilde{h}_{1}\left(x\right)\times\tilde{h}_{2}\left(x\right)\\
\tilde{a}\left(x\right) & =f\left(x\right)a\left(x\right)=\tilde{h}_{1}\left(x\right)+\tilde{h}_{2}\left(x+1\right).
\end{align*}
\\

As mentioned before, the structure of the trivial family should not
be too surprizing. If we just wanted to solve a simple quadratic equation
$S\left(x\right)=x^{2}+ax+b=0$, then we would look for two solutions
$\lambda_{1},\lambda_{2}$ such that $S\left(\lambda_{1}\right)=S\left(\lambda_{2}\right)=0$.
This is equivalent to solving the two equations $b=\lambda_{1}\lambda_{2}$
and $a=-\left(\lambda_{1}+\lambda_{2}\right)$, which is more or less
what we look for when trying to present a generalized continued fraction
as part of the Euler trivial family. While the sign choice is simply
for convenience, the main difference, is that in our case the solutions
depend on $n$, and instead of $a\left(n\right)=\lambda_{1}\left(n\right)+\lambda_{2}\left(n\right)$,
we ``half advance'' the index and look for $a\left(n\right)=\lambda_{1}\left(n\right)+\lambda_{2}\left(n+1\right)$.

In general, starting with a standard quadratic equation $x^{2}+ax+b=0$
over the integers, we do not expect to have solutions over the integers.
The same applies here - if we are given polynomials $a\left(x\right),b\left(x\right)$
where $\KK_{1}^{\infty}\frac{b\left(n\right)}{a\left(n\right)}$ converge,
it is not true that there is a solution as in \thmref{recurrence-roots}
with integral polynomials. Assuming that we know how to decompose
$b\left(x\right)$, we can go over all possible options for $a\left(x\right)=h_{1}\left(x\right)+h_{2}\left(x\right)$.
In the more generalized form, where $f\left(x\right)a\left(x\right)=f\left(x-1\right)h_{1}\left(x\right)+f\left(x+1\right)h_{2}\left(x+1\right)$,
this process is less trivial, but still not that hard, and we describe
an algorithm to find all such solutions in \appref{Identifying-polynomial-continued}.

\newpage{}

\section{\label{sec:Irrationality-testing}Irrationality testing}

Any real number $\alpha$ can be approximated by rational numbers.
More over, if $q\in\NN$ is the denominator, then we can always find
$p\in\ZZ$ such that $\left|\alpha-\frac{p}{q}\right|\leq\frac{1}{q}$.
Looking for approximations where the error is much smaller than $\frac{1}{q}$
is the starting point of the study of Diophantine approximations.
Let us describe one of the well known ways to use such approximations
to show that a given number is irrational.

Take your favorite constant $L$ (e.g. $e,\;\pi,\;\ln\left(7\right),\;\zeta\left(k\right)$
etc) and let $\frac{p_{n}}{q_{n}}$ be a sequence of rationals converging
to $L$. If $L=\frac{p}{q}$ is rational and $L\neq\frac{p_{n}}{q_{n}}$,
then
\[
\left|L-\frac{p_{n}}{q_{n}}\right|=\left|\frac{p\cdot q_{n}-q\cdot p_{n}}{q\cdot q_{n}}\right|\geq\frac{1}{\left|q\cdot q_{n}\right|}.
\]
Since $q$ is constant, we immediately get that: 
\begin{cor}
\label{cor:first-rationality-test}Suppose that $p_{n},q_{n}$ are
coprime with $q_{n}\to\infty$. If $\left|L-\frac{p_{n}}{q_{n}}\right|=o\left(\frac{1}{\left|q_{n}\right|}\right)$,
then $L$ is irrational.
\end{cor}

In other words, we can always have $<\frac{1}{\left|q_{n}\right|}$
error, and if we can do better, then the number is irrational. Note
that once we know that a number is irrational, Dirichlet's theorem
tells us that there are reduced approximations $\frac{p_{n}}{q_{n}}$
with $\left|L-\frac{p_{n}}{q_{n}}\right|=O\left(\frac{1}{q_{n}^{2}}\right)$.
The importance of rational approximations derived from generalized
continued fractions is that it gives us an upper bound on the error,
which we can use to prove irrationality.
\begin{claim}
\label{claim:upper-bound}Let $a_{n},b_{n},p_{n},q_{n}$ be sequence
of integers satisfying the recurrence from \lemref{gcf-recursion},
namely 
\begin{align*}
p_{n+1} & =a_{n}p_{n}+b_{n}p_{n-1}\\
q_{n+1} & =a_{n}q_{n}+b_{n}q_{n-1}.
\end{align*}
\end{claim}

\begin{enumerate}
\item If $\frac{p_{n}}{q_{n}}\to L$, then 
\[
\left|L-\frac{p_{n}}{q_{n}}\right|=\left|\sum_{k=n}^{\infty}\frac{\prod_{1}^{k}b_{i}}{q_{k}q_{k+1}}\right|\leq\sum_{k=n}^{\infty}\frac{\prod_{1}^{k}\left|b_{i}\right|}{\left|q_{k}q_{k+1}\right|}.
\]
\item Suppose in addition that the $b_{n}$ are nonzero. If for $\tilde{q}_{n}=\frac{q_{n}}{gcd\left(p_{n},q_{n}\right)}$
we have $\left|L-\frac{p_{n}}{q_{n}}\right|=o\left(\frac{1}{\left|\tilde{q}_{n}\right|}\right)$
, then $L$ is irrational.
\end{enumerate}
\begin{proof}
\begin{enumerate}
\item For all $m\geq n$ we have
\[
L-\frac{p_{n}}{q_{n}}=L-\frac{p_{m+1}}{q_{m+1}}+\sum_{k=n}^{m}\left(\frac{p_{k+1}}{q_{k+1}}-\frac{p_{k}}{q_{k}}\right)=L-\frac{p_{m+1}}{q_{m+1}}-\sum_{k=n}^{m}\frac{\det\left(\begin{smallmatrix}p_{k} & p_{k+1}\\
q_{k} & q_{k+1}
\end{smallmatrix}\right)}{q_{k}q_{k+1}}.
\]
Under the assumption that $L-\frac{p_{m+1}}{q_{m+1}}\to0$ as $m\to\infty$,
we conclude that 
\[
\left|L-\frac{p_{n}}{q_{n}}\right|\leq\sum_{k=n}^{\infty}\left|\frac{\det\left(\begin{smallmatrix}p_{k} & p_{k+1}\\
q_{k} & q_{k+1}
\end{smallmatrix}\right)}{q_{k}q_{k+1}}\right|=\sum_{k=n}^{\infty}\frac{\prod_{1}^{k}\left|\det\left(M_{i}\right)\right|}{\left|q_{k}q_{k+1}\right|}=\sum_{k=n}^{m}\frac{\prod_{1}^{k}\left|b_{i}\right|}{q_{k}q_{k+1}},
\]
and we are done.
\item For the second part, setting $\tilde{p}_{n}=\frac{p_{n}}{gcd\left(p_{n},q_{n}\right)}$
we get that $\left|L-\frac{\tilde{p}_{n}}{\tilde{q}_{n}}\right|=\left|L-\frac{p_{n}}{q_{n}}\right|=o\left(\frac{1}{\left|\tilde{q}_{n}\right|}\right)$.
In order to use \corref{first-rationality-test}, we only need to
show that $\tilde{q}_{n}$ has a subsequence going to infinity. Otherwise,
assume that $\tilde{q}_{n}$ is bounded, then since $\left|L-\frac{\tilde{p}_{n}}{\tilde{q}_{n}}\right|\to0$,
the $\tilde{p}_{n}$ and $\tilde{q}_{n}$ must be constant for all
$n$ large enough (since $gcd\left(\tilde{p}_{n},\tilde{q}_{n}\right)=1$)
and $\frac{\tilde{p}_{n}}{\tilde{q}_{n}}=L$. However, we also have
that 
\[
\left|\frac{\tilde{p}_{n+1}}{\tilde{q}_{n+1}}-\frac{\tilde{p}_{n}}{\tilde{q}_{n}}\right|=\left|\frac{p_{n+1}}{q_{n+1}}-\frac{p_{n}}{q_{n}}\right|=\left|\frac{\det\left(\begin{smallmatrix}p_{n} & p_{n+1}\\
q_{n} & q_{n+1}
\end{smallmatrix}\right)}{q_{n}q_{n+1}}\right|=\frac{\prod_{1}^{n}\left|b_{k}\right|}{\left|q_{n}q_{n+1}\right|}\neq0,
\]
which leads to a contradiction. Hence, the $\tilde{q}_{n}$ cannot
be bounded as required.
\end{enumerate}
\end{proof}
The recursion relation of the $q_{i}$ suggest that the larger the
$\left|b_{i}\right|$ are, the faster the growth of $\left|q_{k}\right|$
is, and in general we expect it to be fast enough so that $\sum_{k=1}^{\infty}\frac{\prod_{1}^{k+1}\left|b_{i}\right|}{\left|q_{k}q_{k+1}\right|}$
will converge. However, it might still not be in $o\left(\frac{1}{\left|q_{n}\right|}\right)$.
Hopefully, if the $gcd\left(p_{n},q_{n}\right)$ is large enough,
then it is in $o\left(\frac{1}{\left|\tilde{q}_{n}\right|}\right)$,
which is enough to prove irrationality.


\subsection{Failed irrationality test for $\zeta\left(3\right)$}

Consider $\zeta\left(3\right)$ with its standard rational approximation
as infinite sum:
\[
\frac{p_{n}}{q_{n}}:=\sum_{1}^{n}\frac{1}{k^{3}}\overset{n\to\infty}{\longrightarrow}\zeta\left(3\right).
\]

Multiplying the denominators, we can write $q_{n}=\left(n!\right)^{3}$
and $p_{n}=\sum_{1}^{n}\left(\frac{n!}{k}\right)^{3}$, both of which
in $\ZZ$. Trying to apply the irrationality test from the previous
section, we get that 
\[
\left|\zeta\left(3\right)-\frac{p_{n}}{q_{n}}\right|=\sum_{n+1}^{\infty}\frac{1}{k^{3}}=\Theta\left(\frac{1}{n^{2}}\right).
\]
This is, of course, far from what we need to prove irrationality,
since $\frac{1}{n^{2}}$ is much larger than $\frac{1}{q_{n}}=\frac{1}{\left(n!\right)^{3}}$.
While the rational approximations above choice is easy to use in general,
it is not good enough to show irrationality. More over, fixing $q_{n}=\left(n!\right)^{3}$,
we can always choose some $p_{n}'$ such that $\left|\zeta\left(3\right)-\frac{p_{n}'}{q_{n}}\right|\leq\frac{1}{2q_{n}}$
showing how bad the approximation above is.

One way to improve the approximation, as in part (2) of \claimref{upper-bound},
is by moving to a reduced form of $\frac{p_{n}}{q_{n}}$. Indeed,
we can take instead the common denominator, namely $\tilde{q}_{n}=\lcm\left[n\right]^{3}$,
where $\lcm\left[n\right]:=\lcm\left\{ 1,2,...,n\right\} $ and then
$\tilde{p}_{n}=\sum_{1}^{n}\left(\frac{\lcm\left[n\right]}{k}\right)^{3}$.
It is well known that $\ln\left(\lcm\left[n\right]\right)=n+o\left(n\right)$
(it follows from the prime number theorem, see \cite{apostol_introduction_1998}),
so that $\lcm\left[n\right]\sim e^{n}$ is much smaller than $n!$
. However, we still get that the error is too big $\frac{1}{n^{2}}\geq\frac{1}{e^{n}}$,
so even with this improvement, it is still not enough.\\

Trying to solve this problem, we define a new sequence of rational
approximations to $\zeta\left(3\right)$. Since the previous sequence
is $\Theta\left(\frac{1}{n^{2}}\right)$ away, we instead use $\frac{p_{n}}{q_{n}}=\left(\sum_{1}^{n-1}\frac{1}{k^{3}}\right)+\frac{1}{2n^{2}}$.
As before, while simply taking the product of the denominators we
have $\left((n-1)!\right)^{3}2n^{2}$, taking instead their least
common multiple allow us to assume that $q_{n}\sim\lcm\left[n\right]^{3}$.
On the other hand, computing the error we get 
\[
\left|\zeta\left(3\right)-\frac{p_{n}}{q_{n}}\right|=\left|\left(\sum_{n}^{\infty}\frac{1}{k^{3}}\right)-\frac{1}{2n^{2}}\right|=\left|\left(\sum_{n}^{\infty}\frac{1}{k^{3}}\right)-\int_{n}^{\infty}\frac{1}{x^{3}}\dx\right|\leq\frac{1}{n^{3}}.
\]
So while the denominator of the $n$-th approximation hasn't changed
too much, the error itself becomes much smaller, namely decreases
from $\frac{1}{n^{2}}$ to $\frac{1}{n^{3}}$. This is still not enough
for our purpose, but if we could keep finding many such approximations
for $\zeta\left(3\right)$, which can be ``easy'' to describe and
work with, then we might eventually find one where the approximation
is good enough to conclude that $\zeta\left(3\right)$ is irrational.

The main idea in this paper, is to start with a continued fraction,
and then ``improve'' it. For example, we already saw in \exaref{Euler-family}
that the standard approximation is 
\[
\sum_{k=0}^{n-1}\frac{1}{\left(k+1\right)^{d}}=\frac{1}{1+\KK_{1}^{n}\frac{-k^{6}}{k^{3}+\left(1+k\right)^{3}}}=\left[\left(\begin{smallmatrix}0 & 1\\
1 & 1
\end{smallmatrix}\right)\prod_{1}^{n}\left(\begin{smallmatrix}0 & -k^{6}\\
1 & k^{3}+\left(1+k^{3}\right)
\end{smallmatrix}\right)\right]\left(0\right).
\]
The ``improvement'' will be done by adding matrices to the multiplication
from the left, or from the right, while trying to conserve the continued
fraction form, and hopefully allowing us to end up with a nice enough
presentation and to prove irrationality. In any case, this will require
a bit more general approach to this subject, where we consider general
product of Mobius transformations, though hopefully keeping them simple
enough in order to work with them.

\newpage{}

\section{\label{sec:The-most-general}The most general of generalized continued
fractions}

Our goal, continuing on, will be to start with a given continued fraction
expansion for some constant (e.g. $\zeta\left(3\right)=\left(\begin{smallmatrix}0 & 1\\
1 & 1
\end{smallmatrix}\right)\left(\KK_{1}^{\infty}\frac{-n^{6}}{n^{3}+\left(1+n\right)^{3}}\right)$), and create new continued fractions, in which the approximation
error goes to zero quicker than the denominator goes to infinity.

As we are moving between different continued fraction presentations,
we are led to consider a larger family of ``continued fraction expansions''
namely - any sequence of matrices.
\begin{defn}
Let $M\left(i\right)$ be a sequence of $2\times2$ matrices. For
$z\in\CC$ we will denote 
\[
\left[\prod_{1}^{\infty}M\left(i\right)\right]\left(z\right)=\limfi n\left[\prod_{1}^{n}M\left(i\right)\right]\left(z\right).
\]
\end{defn}

In the definition above, it is possible that the limit converges for
one $z$ and diverges for another. For example, if we take $M\left(i\right)=\left(\begin{smallmatrix}-1 & 0\\
0 & 1
\end{smallmatrix}\right)$, then $\left[\prod_{1}^{\infty}M\left(i\right)\right]\left(0\right)=0$
while $\left[\prod_{1}^{n}M\left(i\right)\right]\left(1\right)=\left(-1\right)^{n}$
doesn't converge. However, in many ``natural'' sequences we have
a very strong convergence behavior. 
\begin{example}
\begin{enumerate}
\item If $M_{i}=\left(\begin{smallmatrix}1 & a_{i}\\
0 & 1
\end{smallmatrix}\right)$ are upper triangular, then 
\[
\left(\prod_{1}^{n}M_{i}\right)\left(z\right)=\left(\begin{smallmatrix}1 & \sum_{1}^{n}a_{i}\\
0 & 1
\end{smallmatrix}\right)\left(z\right)=z+\sum_{1}^{n}a_{i}.
\]
If $\sum_{1}^{\infty}a_{i}=\infty$, then $\left(\prod_{1}^{n}M_{i}\right)\left(z\right)\to\infty$
for all $z$. If we also add an $M_{0}$ matrix which takes $\infty\to w\in\CC$,
then $\left(\prod_{0}^{n}M_{i}\right)\left(z\right)\to w$ for all
$z$.
\item If $M_{i}=\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)$ has the continued fraction form, then $\left(\prod_{1}^{n}M_{i}\right)\left(\infty\right)=\left(\prod_{1}^{n-1}M_{i}\right)\left(0\right)$
since $M_{i}\left(\infty\right)=0$. It follows that we have convergence
in $0$ if and only if we have convergence in $\infty$. Writing $\prod_{1}^{n-1}M_{i}=\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)$ , this limit will simply be $\limfi n\frac{p_{n}}{q_{n}}$. \\
For $x\in\left(0,\infty\right)$ we have that 
\[
\left[\prod_{1}^{n-1}M_{i}\right]\left(x\right)=\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)\left(x\right)=\frac{p_{n-1}x+p_{n}}{q_{n-1}x+q_{n}}
\]
and in case the $p_{n}$ and $q_{n}$ sequences are positive, it is
an exercise to show that $\frac{p_{n-1}x+p_{n}}{q_{n-1}x+q_{n}}$
is in the segment defined by the endpoints $\left\{ \frac{p_{n}}{q_{n}},\frac{p_{n-1}}{q_{n-1}}\right\} $.
The best way to see it is to consider the vectors $\left(p_{n},q_{n}\right)$
and $\left(p_{n-1},q_{n-1}\right)$ in the positive quadrant and see
that $\left(p_{n},q_{n}\right)+x\left(p_{n-1},q_{n-1}\right)$ is
between them, so its projection to the projective line is between
their projections. It follows that when applying $\left[\prod_{1}^{n-1}M_{i}\right]$
to any element in $\left[0,\infty\right]$, the sequence converges
and to the same limit. \\
The condition on the $p_{n},q_{n}$ is true, for example, if the $a_{i},b_{i}$
are all positive, which is the case for simple continued fractions. 
\end{enumerate}
\end{example}

\newpage{}

\subsection{\label{subsec:coboundary}Some words about cocycles and coboundaries}

In our previous discussion about continued fractions, where $M_{i}=\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)$, we were specially interested in $P_{n}:=\prod_{1}^{n}M_{i}=\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)$ which contained the numerators and denominators of the convergents.
These products can be defined for any sequence $M_{i}$ of matrices,
and we think of them as \textbf{potential matrices} where we move
from the potential at point $i$ to the potential at point $i+1$
via the matrix $M_{i}$, or more formally $P_{i}M_{i}=P_{i+1}$. We
can also restrict them to row vectors, instead of full $2\times2$
matrices. In particular, for the continued fraction matrices, each
such vector sequence will satisfy $\left(F_{i-1},F_{i}\right)M_{i}=\left(F_{i},F_{i+1}\right)$,
where $F_{i}$ solves the recurrence relation 
\[
F_{i+1}=F_{i}a_{i}+F_{i-1}b_{i}
\]
that we already encountered before.

The main goal of this section is to look for natural ways to move
between such potential matrices and vectors, and eventually to find
natural connections between different continued fractions. This type
of question is usually asked in cohomology theory (see for example
chapter 4 in \cite{brown_cohomology_2012}), where such sequences
$M_{i}$ of matrices should and can be called ``\textbf{cocycles}''.
We will not go too much into this theory here, since on the one hand
this cocycle structure is in a sense trivial, and on the other hand,
the theory is usually much more geared into commutative rings, unlike
our noncommutative matrix setting. However, the question about natural
conversion between the potentials exists in this theory and is called
coboundary equivalence, and this will come up a lot in our study.
More specifically we want to move from one potential $P_{i}^{\left(1\right)}$
to the second $P_{i}^{\left(2\right)}$ using a nice transformation
$P_{i}^{\left(1\right)}U_{i}=P_{i}^{\left(2\right)}$, producing for
us this commutative diagram:
\[
\xymatrix{P_{1}^{\left(1\right)}\ar[r]^{M_{1}^{\left(1\right)}}\ar[d]^{U_{1}} & P_{2}^{\left(1\right)}\ar[r]^{M_{2}^{\left(1\right)}}\ar[d]^{U_{2}} & P_{3}^{\left(1\right)}\ar[r]^{M_{3}^{\left(1\right)}}\ar[d]^{U_{3}} & \cdots\ar[r]^{M_{n-1}^{\left(1\right)}} & P_{n}^{\left(1\right)}\ar[d]^{U_{n}}\\
P_{1}^{\left(2\right)}\ar[r]^{M_{1}^{\left(2\right)}} & P_{2}^{\left(2\right)}\ar[r]^{M_{2}^{\left(2\right)}} & P_{3}^{\left(2\right)}\ar[r]^{M_{3}^{\left(2\right)}} & \cdots\ar[r]^{M_{n-1}^{\left(2\right)}} & P_{n}^{\left(2\right)}
}
\]
More formally, we have the following.
\begin{defn}
Two matrix sequences $M_{i}^{\left(1\right)},\;M_{i}^{\left(2\right)}$
are called \textbf{$U_{i}$-coboundary equivalent} for a sequence
of invertible matrices $U_{i}$ if $M_{i}^{\left(1\right)}U_{i+1}=U_{i}M_{i}^{\left(2\right)}$
for all $i$, or in a commutative diagram form:
\[
\xymatrix{\left(*\right)\ar[r]^{M_{i}^{\left(2\right)}} & \left(*\right)\\
\left(*\right)\ar[r]_{M_{i}^{\left(1\right)}}\ar[u]^{U_{i}} & \left(*\right)\ar[u]_{U_{i+1}}
}
.
\]
\end{defn}

\begin{rem}
In the world of standard, non indexed matrices, this coboundary equivalence
is simply matrix conjugation, and as we shall see some of the results
for this coboundary equivalence are just ``indexed'' version of
what we expect from matrix conjugacy.
\end{rem}

The commutativity condition in the coboundary definition can be extended
to products of the $M_{i}$, and in particular for our potential matrices.
Indeed, a simple inductions shows that with the notations as in the
definition, for all $m\leq n$ we have
\[
U_{m}\left[\prod_{m}^{n}M_{i}^{\left(2\right)}\right]=\left[\prod_{m}^{n}M_{i}^{\left(1\right)}\right]U_{n+1}.
\]

Every two matrix sequences $M_{i}^{\left(1\right)},\;M_{i}^{\left(2\right)}$
(invertible) are coboundary equivalent for some $U_{i}$. Indeed,
once we choose $U_{1}$, we can recursively define $U_{i+1}=\left(M_{i}^{\left(2\right)}\right)^{-1}U_{i}M_{i}^{\left(1\right)}$.
However, what will matter to us later on is that $U_{i}$ is simple
enough to work with. For example, it can be defined over $\ZZ$, triangular,
diagonal, etc. In particular, we want to work with the Mobius maps
induced by the matrices, and the upper triangular (resp. lower triangular)
are exactly the matrices which take infinity to itself (resp. zero
to itself).

This idea of coboundary equivalent sequences is very useful, and we
have already seen one such important example. In the ``equivalence
transformation'' for continued fractions in \lemref{equivalence-transformation}
we used 
\[
M_{n}^{\left(1\right)}=\left(\begin{smallmatrix}0 & b_{n}\\
1 & a_{n}
\end{smallmatrix}\right),\quad,M_{n}^{\left(2\right)}=\left(\begin{smallmatrix}0 & c_{n-1}c_{n}b_{n}\\
1 & c_{n}a_{n}
\end{smallmatrix}\right),\quad U_{n}=\left(\begin{smallmatrix}1 & 0\\
0 & c_{n-1}
\end{smallmatrix}\right)
\]
so that $M_{n}^{\left(1\right)}U_{n+1}=c_{n-1}U_{n}M_{n}^{\left(2\right)}$,
and since we deal with Mobius transformation, where scalar matrices
act as the identity, we have $M_{n}^{\left(1\right)}U_{n+1}\equiv U_{n}M_{n}^{\left(2\right)}$.\\

Other than this important example, we have two more - one to move
to upper triangular matrices, and one to continued fraction matrices,
both of which are helpful when we need to take product of many such
matrices. In the upper triangular case, the diagonal is just a product
of the diagonals and the only complicated part is in the upper right
corner. In the continued fraction form, as we already saw, there is
a natural recursion relation, which will be very helpful once we start
to do the actual computations, and we will start with it.
\begin{thm}
\label{thm:to-gcf}\cite{razon_automated_2022} Let 
\[
M_{n}=\left(\begin{smallmatrix}a_{n} & b_{n}\\
c_{n} & d_{n}
\end{smallmatrix}\right),\quad U_{n}=\left(\begin{smallmatrix}1 & a_{n}\\
0 & c_{n}
\end{smallmatrix}\right),\quad a_{n},b_{n},c_{n},d_{n}\in\CC,
\]
such that $c_{n}\neq0$ for all $n\geq1$ (so that $U_{n}$ is invertible).
Then $M_{n}$ is $U_{n}$-coboundary equivalent to the continued fraction
matrix
\[
U_{n}^{-1}M_{n}U_{n+1}=\left(\begin{smallmatrix}0 & -\frac{c_{n+1}}{c_{n}}\det\left(M_{n}\right)\\
1 & a_{n+1}+d_{n}\frac{c_{n+1}}{c_{n}}
\end{smallmatrix}\right).
\]
In particular, setting $\left(\begin{smallmatrix}p_{n}\\
q_{n}
\end{smallmatrix}\right)=M_{1}M_{2}\cdots M_{n}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)$, both of the $p_{n}$ and $q_{n}$ satisfy the same recurrence relation
\begin{align*}
q_{n+1} & =q_{n}\left(a_{n+1}+d_{n}\frac{c_{n+1}}{c_{n}}\right)-q_{n-1}\left(\frac{c_{n+1}}{c_{n}}\det\left(M_{n}\right)\right),\qquad q_{0}=0,\;q_{1}=c_{1}\\
p_{n+1} & =p_{n}\left(a_{n+1}+d_{n}\frac{c_{n+1}}{c_{n}}\right)-p_{n-1}\left(\frac{c_{n+1}}{c_{n}}\det\left(M_{n}\right)\right),\qquad p_{0}=1,\;p_{1}=a_{1}.
\end{align*}
\end{thm}

\begin{proof}
The computation of $U_{n}^{-1}M_{n}U_{n+1}$ is straight forward 
\begin{align*}
U_{n}^{-1}M_{n}U_{n+1} & =\frac{1}{c_{n}}\left(\begin{smallmatrix}c_{n} & -a_{n}\\
0 & 1
\end{smallmatrix}\right)\left(\begin{smallmatrix}a_{n} & b_{n}\\
c_{n} & d_{n}
\end{smallmatrix}\right)\left(\begin{smallmatrix}1 & a_{n+1}\\
0 & c_{n+1}
\end{smallmatrix}\right)=\frac{1}{c_{n}}\left(\begin{smallmatrix}0 & -c_{n+1}\det\left(M_{n}\right)\\
c_{n} & c_{n}a_{n+1}+d_{n}c_{n+1}
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0 & -\frac{c_{n+1}}{c_{n}}\det\left(M_{n}\right)\\
1 & a_{n+1}+d_{n}\frac{c_{n+1}}{c_{n}}
\end{smallmatrix}\right).
\end{align*}
Writing $\tilde{M}_{n}:=U_{n}^{-1}M_{n}U_{n+1}$, and noting that
$U_{n+1}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)$, the coboundary equivalence gives us
\begin{align*}
\left(\begin{smallmatrix}p_{n}\\
q_{n}
\end{smallmatrix}\right) & =\left(\prod_{1}^{n}M_{k}\right)U_{n+1}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=U_{1}\left(\prod_{1}^{n}\tilde{M}_{k}\right)\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right).
\end{align*}
Since $\tilde{M}_{n+1}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)$, we also get that 
\[
\left[U_{1}\prod_{1}^{n}\tilde{M}_{k}\right]\left(\begin{smallmatrix}0\\
1
\end{smallmatrix}\right)=\left[U_{1}\prod_{1}^{n}\tilde{M}_{k}\right]\tilde{M}_{n+1}\left(\begin{smallmatrix}1\\
0
\end{smallmatrix}\right)=\left(\begin{smallmatrix}p_{n+1}\\
q_{n+1}
\end{smallmatrix}\right).
\]
In other words, we got that 
\[
U_{1}\prod_{1}^{n}\tilde{M}_{k}=\left(\begin{smallmatrix}p_{n} & p_{n+1}\\
q_{n} & q_{n+1}
\end{smallmatrix}\right).
\]
This implies the matrix recurrence relation 
\[
\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right)\tilde{M}_{n}=\left(\begin{smallmatrix}p_{n} & p_{n+1}\\
q_{n} & q_{n+1}
\end{smallmatrix}\right),
\]
which translate to the recurrence relations
\begin{align*}
q_{n+1} & =q_{n}\left(a_{n+1}+d_{n}\frac{c_{n+1}}{c_{n}}\right)-q_{n-1}\left(\frac{c_{n+1}}{c_{n}}\det\left(M_{n}\right)\right)\\
p_{n+1} & =p_{n}\left(a_{n+1}+d_{n}\frac{c_{n+1}}{c_{n}}\right)-p_{n-1}\left(\frac{c_{n+1}}{c_{n}}\det\left(M_{n}\right)\right),
\end{align*}
and starting conditions $p_{0}=1,\;p_{1}=a_{1},\;q_{0}=0,\;q_{1}=c_{1}$. 
\end{proof}
\begin{rem}
In the theorem above, if the entries of $M_{n}$ are integers but
$c_{n}$ is not constant, then in general the coefficients in the
recurrence will not be integers. However, the $p_{n},q_{n}$ solutions
will still be integers, since we used the product of the $M_{i}$
integral matrices to define them.
\end{rem}

We will mainly be interested in the case where the entries of the
matrices are polynomial evaluated at the integer points, so for example
in the previous case $c_{n}=c\left(n\right)$ where $c\in\CC\left[x\right]$.
In particular, unless $c\equiv0$, in which case the $M_{n}$ are
upper triangular, we can always apply this transformation for all
$n$ large enough. However, we would actually prefer to work with
upper triangular matrices, since it is easy to multiply them, and
in particular $\prod_{1}^{n}\left(\begin{smallmatrix}1 & \alpha_{i}\\
0 & 1
\end{smallmatrix}\right)=\left(\begin{smallmatrix}1 & \sum_{1}^{n}\alpha_{i}\\
0 & 1
\end{smallmatrix}\right)$.

Our next goal is to show when we can transform a sequence of matrices
into upper triangular. Recall that a standard matrix is conjugated
to an upper triangular matrix if and only if it has a nonzero eigenvector
$v^{tr}M=\lambda v^{tr}$. Here we also have the indexed analogue,
which while at first glance seems a bit trivial, when we add the right
restrictions, will become quite helpful.
\begin{defn}
Let $M_{i}=\left(\begin{smallmatrix}a_{i} & b_{i}\\
c_{i} & d_{i}
\end{smallmatrix}\right)$ be a sequence of matrices. We say that a sequence $v\left(i\right)=\left(F_{i},G_{i}\right)$
of nonzero vectors is a (left) eigenvector with eigenvalue $\lambda\left(i\right)$
if 
\[
v\left(i\right)M_{i}=\lambda\left(i\right)v\left(i+1\right).
\]

We similarly define right eigenvector and right eigenvalue by the
formula
\[
M_{i}u\left(i+1\right)=\alpha\left(i\right)u\left(i\right).
\]
\end{defn}

\begin{figure}[H]
\begin{centering}
\begin{tabular}{|c|}
\hline 
$\xymatrix{ & v\left(i-1\right) & \lambda\left(i\right)v\left(i\right):=v\left(i-1\right)M_{i-1} & \lambda\left(i+1\right)v\left(i+1\right):=v\left(i\right)M_{i}\\
\cdots\ar[r] & \left(i-1\right)\ar[r]^{M_{i-1}} & \left(i\right)\ar[r]^{M_{i}} & \left(i+1\right)\ar[r]^{M_{i+1}} & \cdots
}
$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{We should think of left eigenvectors $v\left(i\right)$ as being at
the $i$-th position, and multiplying by $M_{i}$ from the right \textquotedblleft moves\textquotedblright{}
them to the $i+1$ position. Right eigenvector goes similarly but
from $i+1$ to $i$ with $M_{i}$ multiplying from the left.}

\end{figure}

Unlike standard eigenvectors, in our case it is very easy to find
eigenvectors by simply defining $v\left(i+1\right)=\frac{1}{\lambda\left(i\right)}v\left(i\right)M\left(i\right)$
recursively. However, the problem is finding an eigenvector which
is easy to work with. We already saw one such example when our $M_{i}$
had continued fraction form, in which case a 1-left eigenvector is
simply a solution to 
\[
\left(F_{i-1},F_{i}\right)\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)=\left(F_{i},F_{i+1}\right),
\]
or alternatively $F_{i+1}=a_{i}F_{i}+b_{i}F_{i-1}$. If $F_{i}$ is
just any sequence, then it would be very hard to work with, however
if $b_{i},a_{i}$ are polynomial in $i$, we might find $F_{i}$ which
is polynomial or exponential in $i$.
\begin{example}
\label{exa:euler-left-right-eigenvectors}Consider a generalized continued
fraction from the trivial Euler family $M_{i}=\left(\begin{smallmatrix}0 & -h_{1}\left(i\right)h_{2}\left(i\right)\\
1 & h_{1}\left(i\right)+h_{2}\left(i+1\right)
\end{smallmatrix}\right)$ (see \subsecref{Euler's-formula}). Then it has both a $h_{2}\left(i\right)$-left
and $h_{1}\left(i\right)$-right eigenvectors
\begin{align*}
\left(1,\;h_{2}\left(i\right)\right)\cdot\left(\begin{smallmatrix}0 & -h_{1}\left(i\right)h_{2}\left(i\right)\\
1 & h_{1}\left(i\right)+h_{2}\left(i+1\right)
\end{smallmatrix}\right) & =h_{2}\left(i\right)\cdot\left(1,\;h_{2}\left(i+1\right)\right)\\
\left(\begin{smallmatrix}0 & -h_{1}\left(i\right)h_{2}\left(i\right)\\
1 & h_{1}\left(i\right)+h_{2}\left(i+1\right)
\end{smallmatrix}\right)\cdot\left(\begin{smallmatrix}h_{2}\left(i+1\right)\\
-1
\end{smallmatrix}\right) & =h_{1}\left(i\right)\cdot\left(\begin{smallmatrix}h_{2}\left(i\right)\\
-1
\end{smallmatrix}\right).
\end{align*}
In particular if $h_{1},h_{2}\in\ZZ\left[x\right]$, then both the
eigenvalues and eigenvectors are integral.\\
\end{example}

Note that in the example above, the left and right eigenvectors at
the $i$-position are perpendicular:
\[
\left(1,\;h_{2}\left(i\right)\right)\cdot\left(\begin{smallmatrix}h_{2}\left(i\right)\\
-1
\end{smallmatrix}\right)=0.
\]
This is not a coincidence, and it happens for any sequence of matrices,
and we can use it to triangularize it.
\begin{lem}
\label{lem:triangularization}Let $M_{i}=\left(\begin{smallmatrix}a_{i} & b_{i}\\
c_{i} & d_{i}
\end{smallmatrix}\right)$ be any sequence of matrices. Then $\left(G_{i},F_{i}\right)$ is
a left $\lambda_{i}$-eigenvector if and only if $\left(\begin{smallmatrix}F_{i}\\
-G_{i}
\end{smallmatrix}\right)$ is a right $\alpha_{i}$-eigenvector. Moreover, if the $F_{i}\neq0$,
then setting $U_{n}=\left(\begin{smallmatrix}F_{i}^{-1} & 0\\
G_{i} & F_{i}
\end{smallmatrix}\right)$ we get that 
\[
U_{i}M_{i}U_{i+1}^{-1}=\left(\begin{smallmatrix}\alpha_{i} & \frac{b_{i}}{F_{i}F_{i+1}}\\
0 & \lambda_{i}
\end{smallmatrix}\right),
\]
so in particular $\alpha_{i}\lambda_{i}=\det\left(M_{i}\right)$.
\end{lem}

\begin{proof}
Suppose that $M_{i}$ has a $\lambda_{i}$-left eigenvector $\left(G_{i},F_{i}\right)$.
Then 
\[
\left(G_{i},F_{i}\right)M_{i}\left(\begin{smallmatrix}F_{i+1}\\
-G_{i+1}
\end{smallmatrix}\right)=\lambda_{i}\left(G_{i+1},F_{i+1}\right)\left(\begin{smallmatrix}F_{i+1}\\
-G_{i+1}
\end{smallmatrix}\right)=0,
\]
implying that $\text{\ensuremath{M_{i}\left(\begin{smallmatrix}F_{i+1}\\
 -G_{i+1} 
\end{smallmatrix}\right)}\ensuremath{\ensuremath{\perp\left(G_{i},F_{i}\right)}}}$. Since we are in dimension 2, the perpendicular of a nonzero vector
is a 1-dimensional space, so that $M_{i}\left(\begin{smallmatrix}F_{i+1}\\
-G_{i+1}
\end{smallmatrix}\right)=\alpha_{i}\left(\begin{smallmatrix}F_{i}\\
-G_{i}
\end{smallmatrix}\right)$ for some scalar $\alpha_{i}$, namely it is a right eigenvector. 

In our choice of $U_{i}$ the row $e_{2}^{tr}U_{i}$ is the left eigenvector,
and since $\det\left(U_{i+1}\right)=1$, we get that $U_{i+1}^{-1}=\left(\begin{smallmatrix}F_{i+1} & 0\\
-G_{i+1} & F_{i+1}^{-1}
\end{smallmatrix}\right)$ so the $U_{i+1}^{-1}e_{1}$ is the right eigenvector. We now get
that 
\begin{align*}
e_{2}^{tr}U_{i}M_{i}U_{i+1}^{-1} & =\lambda_{i}e_{2}^{tr}U_{i+1}U_{i+1}^{-1}=\lambda_{i}e_{2}^{tr}\\
U_{i}M_{i}U_{i+1}^{-1}e_{1} & =\alpha_{i}U_{i}U_{i}^{-1}e_{1}=\alpha_{i}e_{1}\\
e_{1}^{tr}U_{i}M_{i}U_{i+1}^{-1}e_{2} & =\left(F_{i}^{-1}e_{1}^{tr}\right)M_{i}\left(F_{i+1}^{-1}e_{2}\right)=\frac{b_{i}}{F_{i}F_{i+1}}.
\end{align*}
Hence, all together we get that 
\[
U_{i}M_{i}U_{i+1}^{-1}=\left(\begin{smallmatrix}\alpha_{i} & \frac{b_{i}}{F_{i}F_{i+1}}\\
0 & \lambda_{i}
\end{smallmatrix}\right),
\]
which completes the proof.
\end{proof}
To fully utilize this triangularization let's recall the formula for
computing a product of triangular matrices.
\begin{claim}
\label{claim:upper-triangular}For sequences $\alpha_{i},\beta_{i},\gamma_{i}$
with $\alpha_{i},\gamma_{i}\neq0$ we have that 
\begin{align*}
\prod_{1}^{n-1}\left(\begin{smallmatrix}\alpha_{i} & \beta_{i}\\
0 & \gamma_{i}
\end{smallmatrix}\right) & =\left(\begin{smallmatrix}\prod_{1}^{n-1}\alpha_{i} & c_{n}\\
0 & \prod_{1}^{n-1}\gamma_{i}
\end{smallmatrix}\right)\\
c_{n} & =\sum_{k=1}^{n-1}\left(\prod_{i=1}^{k-1}\alpha_{i}\right)\beta_{k}\left(\prod_{i=k+1}^{n-1}\gamma_{i}\right)
\end{align*}
In particular, as a Mobius map we get 
\[
\left[\prod_{1}^{n-1}\left(\begin{smallmatrix}\alpha_{i} & \beta_{i}\\
0 & \gamma_{i}
\end{smallmatrix}\right)\right]\left(0\right)=\sum_{k=1}^{n-1}\frac{\beta_{k}}{\gamma_{k}}\left(\prod_{i=1}^{k-1}\frac{\alpha_{i}}{\gamma_{i}}\right)
\]
.
\end{claim}

\begin{proof}
A simple induction.
\end{proof}
\begin{example}
The last two results can be combined together, for example, to reprove
the conversion from continued fractions in the Euler family from \exaref{Euler-family}
to infinite sum. Given two functions $h_{1},h_{2}:\ZZ\to\CC$ set
\begin{align*}
b_{i} & =-h_{1}\left(i\right)h_{2}\left(i\right)\\
a_{i} & =h_{1}\left(i\right)+h_{2}\left(i+1\right),
\end{align*}
and let $M_{i}=\left(\begin{smallmatrix}0 & b_{i}\\
1 & a_{i}
\end{smallmatrix}\right)$. We already saw in \exaref{euler-left-right-eigenvectors} that this
sequence has $h_{2}\left(i\right)$-left eigenvector $\left(1,h_{2}\left(i\right)\right)$,
so \lemref{triangularization} implies that for $U_{i}=\left(\begin{smallmatrix}h_{2}\left(i\right)^{-1} & 0\\
1 & h_{2}\left(i\right)
\end{smallmatrix}\right)$ we have
\[
U_{i}M_{i}U_{i+1}^{-1}=\left(\begin{smallmatrix}h_{1}\left(i\right) & -\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\\
0 & h_{2}\left(i\right)
\end{smallmatrix}\right).
\]
Then \claimref{upper-triangular} shows that 
\begin{align*}
\left(U_{1}\left[\prod_{1}^{n-1}M_{i}\right]U_{n+1}^{-1}\right)\left(0\right) & =-\frac{1}{h_{2}\left(1\right)}\cdot\sum_{k=1}^{n-1}\left(\prod_{i=1}^{k}\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right).
\end{align*}
Set $\alpha={\displaystyle \sum_{\boldsymbol{k=0}}^{n-1}}\left({\displaystyle \prod_{i=1}^{k}}\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)$,
so the expression above is $\frac{1-\alpha}{h_{2}\left(1\right)}$.
Since $U_{n+1}^{-1}\left(0\right)=0$, we conclude that 
\begin{align*}
\KK_{1}^{n-1}\frac{b_{i}}{a_{i}} & =\left[\prod_{1}^{n-1}M_{i}\right]\left(0\right)=U_{1}^{-1}\left(\frac{1-\alpha}{h_{2}\left(1\right)}\right)=\left(\begin{smallmatrix}h_{2}\left(1\right) & 0\\
-1 & h_{2}\left(1\right)^{-1}
\end{smallmatrix}\right)\left(\frac{1-\alpha}{h_{2}\left(1\right)}\right)\\
 & =\frac{1-\alpha}{\frac{1}{h_{2}\left(1\right)}\left(\alpha-1\right)+\frac{1}{h_{2}\left(1\right)}}=h_{2}\left(1\right)\left(\alpha^{-1}-1\right),
\end{align*}
which is exactly what we got in \exaref{Euler-family}.
\end{example}

Other then the triangulation mentioned above, there are other more
direct applications for these eigenvectors. For example, we can use
it to show that the numerators and denominators in a given continued
fraction have a very large common divisor.
\begin{example}
\label{exa:gcd-in-zeta}For example, let's consider the $\zeta_{d}$ case with the matrices
$M_{i}=\left(\begin{smallmatrix}0 & -i^{2d}\\
1 & i^{d}+\left(1+i\right)^{d}
\end{smallmatrix}\right)$ (see \exaref{Euler-family}) where there is a right $i^{d}$-eigenvector
$\left(\begin{smallmatrix}\left(1+i\right)^{d}\\
-1
\end{smallmatrix}\right)$. Any $1$-left eigenvector is simply a sequence $F_{i}$ satisfying
the recurrence 
\[
\left(F_{i-1},F_{i}\right)M_{i}=\left(F_{i},F_{i+1}\right),
\]
or equivalently 
\[
F_{i+1}=\left(i^{d}+\left(1+i\right)^{d}\right)F_{i}-i^{2d}F_{i-1}.
\]
In particular, the $p_{n}$ and $q_{n}$ sequence satisfy this relation,
where 
\[
\prod_{1}^{n-1}M_{i}=\left(\begin{smallmatrix}p_{n-1} & p_{n}\\
q_{n-1} & q_{n}
\end{smallmatrix}\right),
\]
namely they are the eigenvectors which start with $\left(0,1\right)$
and $\left(1,0\right)$ respectively.

Using the process above we get that 
\[
\left(F_{n},F_{n+1}\right)\cdot\left(\begin{smallmatrix}\left(1+n\right)^{d}\\
-1
\end{smallmatrix}\right)=\left(F_{0},F_{1}\right)\prod_{1}^{n}M_{i}\left(\begin{smallmatrix}\left(1+n\right)^{d}\\
-1
\end{smallmatrix}\right)=\left(F_{0},F_{1}\right)\left(\begin{smallmatrix}\left(1+1\right)^{d}\\
-1
\end{smallmatrix}\right)\prod_{1}^{n}i^{d}.
\]
If $F_{0},F_{1}$ are integers (e.g. the starting points for $p_{n}$
or $q_{n}$) ,then $\left(1+n\right)^{d}F_{n}-F_{n+1}=\left(n!\right)^{d}\cdot C$
for some constant integer $C$. We claim that this implies that $\left(\frac{n!}{\lcm\left[n\right]}\right)^{d}$
divides $F_{n}$. This is clearly true for $n=1,2,3$ where $\left(\frac{n!}{\lcm\left[n\right]}\right)^{d}=1$.
Proving the rest by induction, assume that this is true for $n$ and
we prove for $n+1$. We then have that $F_{n+1}=\left(1+n\right)^{d}F_{n}-\left(n!\right)^{d}C$,
so it is enough to prove the claim for each summand. For the first
we have
\[
\left(\frac{\left(n+1\right)!}{\lcm\left[n+1\right]}\right)^{d}\mid\left(1+n\right)^{d}\left(\frac{n!}{\lcm\left[n\right]}\right)^{d}\mid\left(1+n\right)^{d}F_{n}.
\]
For the second, we want to show that there is some integer $m$ such
that $\left(\frac{\left(n+1\right)!}{\lcm\left[n+1\right]}\right)^{d}m=\left(n!\right)^{d}C$,
which is equivalent to $\left(n+1\right)^{d}\cdot m=C\left(\lcm\left[n+1\right]\right)^{d}$.
Since $\left(n+1\right)\mid\lcm\left[n+1\right]$, we can find such
$m$ and we are done.

In other words, we have shown that no matter what the starting conditions
are, we always get that $\left(\frac{n!}{\lcm\left[n\right]}\right)^{d}\mid F_{n}$,
so in particular $\left(\frac{n!}{\lcm\left[n\right]}\right)^{d}\mid\gcd\left(p_{n},q_{n}\right)$.
\end{example}

\newpage{}

\part{\label{part:The-conservative-matrix}The conservative matrix field}

\section{\label{sec:Definition-properties}Definition and properties}

Up until now we mainly looked at a single continued fractions $\KK_{1}^{\infty}\frac{b_{i}}{a_{i}}$,
and in particular where $a_{i}=a\left(i\right),b_{i}=b\left(i\right)$
with $a,b\in\ZZ\left[x\right]$. In this section we define the\textbf{
conservative matrix field}, which is a collection of such continued
fractions with interesting connections between them. 
\begin{defn}
A pair of matrices $M_{X}\left(x,y\right),M_{Y}\left(x,y\right)$
is called a \textbf{conservative matrix field} (or just \textbf{matrix
field} for simplicity), if
\end{defn}

\begin{enumerate}
\item The entries of $M_{X}\left(x,y\right),M_{Y}\left(x,y\right)$ are
polynomial in $x,y$ ,
\item The matrices satisfy the coboundary equivalence relation
\[
M_{X}\left(x,y\right)M_{Y}\left(x+1,y\right)=M_{Y}\left(x,y\right)M_{X}\left(x,y+1\right)\;\forall x,y.
\]
\end{enumerate}
\begin{rem}
For the reason for the name ``conservative matrix field'' consider
the coboundary equivalence relation as the following commutative diagram
\[
\xymatrix{\left(x,y+1\right)\ar[rr]^{M_{X}\left(x,y+1\right)} &  & \left(x+1,y+1\right)\\
\left(x,y\right)\ar[rr]_{M_{X}\left(x,y\right)}\ar[u]^{M_{Y}\left(x,y\right)} &  & \left(x+1,y\right)\ar[u]_{M_{Y}\left(x+1,y\right)}
}
\]
As this is true for any $\left(x,y\right)$, when we think about them
as points in the plane, the intuition should be that traveling along
the bottom and then right edge or traveling along the left and then
top edge should result in the same product. This is very similar to
what we expect from the standard conservative vector fields (and indeed,
both are 1-cocycles with the appropriate groups), and in order to
keep this intuition in mind, it got the name conservative matrix field.\\
\end{rem}

In these matrix fields, we will in particular be interested in the
case where for each fixed $y=y_{0}$, the sequence $\left\{ M_{X}\left(n,y_{0}\right)\right\} _{n=1}^{\infty}$
has the continued fraction form. Then, the continued fractions on
the integer rows $y_{0}\in\ZZ$ are coboundary equivalent continued
fractions via nice polynomial matrices $M_{Y}$. In general, if we
manage to construct such a matrix field where one of its horizontal
lines is a continued fraction presentation for some constant $\alpha$,
we can look at the rest of the matrix field for other properties of
$\alpha$. In particular, in \secref{The-z3-case} we will construct
such a matrix field for $\zeta\left(3\right)$, where its Euler continued
fraction (from \exaref{Euler-family}) is on the $Y=0$ line, and
see for example that the $Y=m$ lines converge to $\sum_{m}^{\infty}\frac{1}{k^{3}}$,
and the diagonal line $X=Y$ can be used to define another continued
fraction presentation where the convergents converge to $\zeta\left(3\right)$
fast enough to show that it is irrational.

With this intuition in mind, we start with a construction for specific
matrix fields with many interesting properties in \subsecref{matrix-field}.
Then in \subsecref{The-dual-matrix-field} we find out how every such
matrix field comes with its dual, which is in a sense a reflection
through the $x=y$ line. Once we have this dual matrix field, we study
the numerators and denominators of the continued fractions in that
matrix field, and in particular find their greatest common divisors.
Finally we show how to put everything together in \secref{The-z3-case}
to show that $\zeta\left(3\right)$ is irrational.

\newpage{}

\subsection{\label{subsec:matrix-field}The construction}

\begin{defn}
\label{def:conjugate}Let $f\left(x,y\right),\bar{f}\left(x,y\right)\in\CC\left[x,y\right]$
be two polynomials. We say these polynomials are \textbf{conjugate
}if they satisfy the following two conditions:
\end{defn}

\begin{enumerate}
\item \textbf{\uline{Linear condition}}: The polynomials satisfy
\[
f\left(x,y\right)-f\left(x+1,y-1\right)=\bar{f}\left(x+1,y\right)-\bar{f}\left(x,y-1\right).
\]
Given such polynomials, we will well define $a_{f,\bar{f}}:=a\left(x,y\right)$
as 
\[
a\left(x,y\right)=f\left(x+1,y-1\right)-\bar{f}\left(x,y-1\right)=f\left(x,y\right)-\bar{f}\left(x+1,y\right).
\]
\item \textbf{\uline{Quadratic condition}}: 
\[
\left(f\bar{f}\right)\left(x,y\right)+\left(f\bar{f}\right)\left(0,0\right)=\left(f\bar{f}\right)\left(x,0\right)+\left(f\bar{f}\right)\left(0,y\right).
\]
In other words, all the monomials appearing in $\left(f\bar{f}\right)\left(x,y\right)$
are $x^{n}$ and $y^{m}$ for $n,m\in\NN$.\\
We denote by $b_{f,\bar{f}}:=b\left(x\right)$ the polynomial
\[
b\left(x\right)=\left(f\bar{f}\right)\left(x,y\right)-\left(f\bar{f}\right)\left(0,y\right)=\left(f\bar{f}\right)\left(x,0\right)-\left(f\bar{f}\right)\left(0,0\right)
\]
which only depends on $x$. We will usually also have that $\left(f\bar{f}\right)\left(0,0\right)=0$,
so that $b\left(x\right)=\left(f\bar{f}\right)\left(x,0\right)$.\\

Given two such conjugate polynomials, we define 
\begin{align*}
M_{X}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}0 & b\left(x\right)\\
1 & a\left(x,y\right)
\end{smallmatrix}\right)\\
M_{Y}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & b\left(x\right)\\
1 & f\left(x,y\right)
\end{smallmatrix}\right).
\end{align*}
The $cf$ superscript is to indicate that $M_{X}^{cf}$ is in a continued
fraction form. We will shortly change it a little bit and remove these
$cf$.
\end{enumerate}

\begin{rem}
\label{rem:ff(0)=00003D0}If $\left(f\bar{f}\right)\left(0,0\right)=0$,
then the $y=0$ is the continued fraction with $b_{i}=\left(f\bar{f}\right)\left(i,0\right)$
and $a_{i}=f\left(i,0\right)-\bar{f}\left(i+1,0\right)$, which is
in the trivial Euler family defined in \exaref{Euler-family}. Indeed,
just take $h_{1}\left(x\right)=f\left(x,0\right)$ and $h_{2}\left(x\right)=-\bar{f}\left(x,0\right)$.
Using the second presentation of $a\left(x,y\right)$, the $y=1$
line is a continued fraction with $b_{i}=\left(f\bar{f}\right)\left(i,0\right)$
and $a_{i}=f\left(i+1,0\right)-\bar{f}\left(i,0\right)$, which is
again in the trivial Euler family, this time with the switched roles
$h_{1}\left(x\right)=-\bar{f}\left(x,0\right)$ and $h_{2}\left(x\right)=f\left(x,0\right)$.

Also, recall that finding an ``Euler'' presentation for a continued
fraction is in a sense a generalization of solving a quadratic equation
$x^{2}+ax+b=0$, where the roots $\lambda_{1},\lambda_{2}$ satisfy
$\lambda_{1}\lambda_{2}=b$ and $-\left(\lambda_{1}+\lambda_{2}\right)=a$.
The structure defined above should be considered as an even further
generalization of this concept. Indeed, starting with $b\left(x\right)$
and $a\left(x,y\right)$, we look for $f,\bar{f}$ such that 
\begin{align*}
b\left(x\right) & =f\left(x,0\right)\bar{f}\left(x,0\right)\\
a\left(x,y\right) & =f\left(x,y\right)-\bar{f}\left(x+1,y\right).
\end{align*}

With this point of view, the term ``conjugates'' should be more
natural, since in a way $f,\bar{f}$ are roots of a quadratic equation
(though with polynomials coefficients).
\end{rem}

\newpage{}
\begin{example}[The $\zeta\left(3\right)$ matrix field]
\label{exa:zeta-3-matrix-field}The main example that we should have in mind is a matrix field for
$\zeta\left(3\right)$ defined by
\begin{align*}
f\left(x,y\right) & =x^{3}+2x^{2}y+2xy^{2}+y^{3}=\frac{y^{3}-x^{3}}{y-x}\left(y+x\right)\\
\bar{f}\left(x,y\right) & =-x^{3}+2x^{2}y-2xy^{2}+y^{3}=\frac{y^{3}+x^{3}}{y+x}\left(y-x\right)=f\left(-x,y\right)\\
\left(f\bar{f}\right)\left(x,y\right) & =y^{6}-x^{6}\\
b\left(x\right) & =-x^{6}\\
a\left(x,y\right) & =x^{3}+\left(1+x\right)^{3}+2y\left(y-1\right)\left(2x+1\right).
\end{align*}
In particular, for $y=0,1$ we have the continued fraction $b\left(n\right)=-n^{6}$
and $a\left(n,0\right)=n^{3}+\left(1+n\right)^{3}$, which is exactly
the Euler continued fraction which converges to $\frac{1}{\zeta\left(3\right)}-1$,
as we saw in \exaref{Euler-family}. We will see in \secref{The-z3-case}
that for any fixed integer $y=m\geq1$, the continued fraction with
$b_{n}=b\left(n\right)$ and $a_{n}=a\left(n,m\right)$ converges
to $\frac{1}{\sum_{m}^{\infty}\frac{1}{k^{3}}}-1$.

The polynomial matrices in this matrix field are 
\begin{align*}
M_{X}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}0 & b\left(x\right)\\
1 & a\left(x,y\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0 & -x^{6}\\
1 & x^{3}+\left(1+x\right)^{3}+2y\left(y-1\right)\left(2x+1\right)
\end{smallmatrix}\right)\\
M_{Y}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & b\left(x\right)\\
1 & f\left(x,y\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}\frac{y^{3}+x^{3}}{y+x}\left(y-x\right) & -x^{6}\\
1 & \frac{y^{3}-x^{3}}{y-x}\left(y+x\right)
\end{smallmatrix}\right)
\end{align*}
and the first few of them are 
\begin{align*}
\xymatrix{\left(*\right) &  &  & \left(*\right) &  &  & \left(*\right) &  & \left(*\right)\\
\\
*+[F]{\left(1,3\right)}\ar[rrr]_{\left(\begin{smallmatrix}0 & -1\\
1 & 45
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}14 & -1\\
1 & 52
\end{smallmatrix}\right)} &  &  & *+[F]{\left(2,3\right)}\ar[rrr]_{\left(\begin{smallmatrix}0 & -2^{3}\\
1 & 95
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}7 & -1\\
1 & 95
\end{smallmatrix}\right)} &  &  & *+[F]{\left(3,3\right)}\ar[rr]_{\left(\begin{smallmatrix}0 & -3^{3}\\
1 & 175
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}0 & -1\\
1 & 168
\end{smallmatrix}\right)} &  & \left(*\right)\\
\\
*+[F]{\left(1,2\right)}\ar[rrr]_{\left(\begin{smallmatrix}0 & -1\\
1 & 21
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}3 & -1\\
1 & 21
\end{smallmatrix}\right)} &  &  & *+[F]{\left(2,2\right)}\ar[rrr]_{\left(\begin{smallmatrix}0 & -2^{3}\\
1 & 55
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}0 & -1\\
1 & 48
\end{smallmatrix}\right)} &  &  & *+[F]{\left(3,2\right)}\ar[rr]_{\left(\begin{smallmatrix}0 & -3^{3}\\
1 & 119
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}-7 & -1\\
1 & 95
\end{smallmatrix}\right)} &  & \left(*\right)\\
\\
*+[F]{\left(1,1\right)}\ar[rrr]_{\left(\begin{smallmatrix}0 & -1\\
1 & 9
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}0 & -1\\
1 & 6
\end{smallmatrix}\right)} &  &  & *+[F]{\left(2,1\right)}\ar[rrr]_{\left(\begin{smallmatrix}0 & -2^{3}\\
1 & 35
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}-3 & -1\\
1 & 21
\end{smallmatrix}\right)} &  &  & *+[F]{\left(3,1\right)}\ar[rr]_{\left(\begin{smallmatrix}0 & -3^{3}\\
1 & 91
\end{smallmatrix}\right)}\ar[uu]^{\left(\begin{smallmatrix}-14 & -1\\
1 & 52
\end{smallmatrix}\right)} &  & \left(*\right)
}
\end{align*}
\end{example}

\newpage{}

We continue to show that this general construction produces a conservative
matrix field.
\begin{thm}
\label{thm:matrix-field-structure}Given polynomials $f,\bar{f},a,b$
as in \defref{conjugate} and the matrices
\begin{align*}
M_{X}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}0 & b\left(x\right)\\
1 & a\left(x,y\right)
\end{smallmatrix}\right)\\
M_{Y}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & b\left(x\right)\\
1 & f\left(x,y\right)
\end{smallmatrix}\right),
\end{align*}
then the following hold:
\begin{enumerate}
\item The matrices satisfy the coboundary equivalence condition
\[
M_{X}^{cf}\left(x,y\right)M_{Y}^{cf}\left(x+1,y\right)=M_{Y}^{cf}\left(x,y\right)M_{X}^{cf}\left(x,y+1\right)\;\forall x,y.
\]
\item The determinants of $M_{X}^{cf}\left(x,y\right),M_{Y}^{cf}\left(x,y\right)$
are only functions of $x,y$ respectively, and more specifically:
\begin{align*}
\det\left(M_{X}^{cf}\left(x,y\right)\right) & =-b\left(x\right)\\
\det\left(M_{Y}^{cf}\left(x,y\right)\right) & =\left(f\cdot\bar{f}\right)\left(0,y\right).
\end{align*}
\end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
\item From the assumption on our functions we know that for all $x,y$ we
have
\begin{align}
a\left(x,y\right) & =f\left(x,y\right)-\bar{f}\left(x+1,y\right)\\
a\left(x,y+1\right) & =f\left(x+1,y\right)-\bar{f}\left(x,y\right)\\
b\left(x+1\right)-b\left(x\right) & =f\left(x,y\right)a\left(x,y+1\right)-a\left(x,y\right)f\left(x+1,y\right)
\end{align}
Using conditions (1) and (2) we get that
\begin{align*}
M_{X}^{cf}\left(x,y\right)M_{Y}^{cf}\left(x+1,y\right) & =\left(\begin{smallmatrix}0 & b\left(x\right)\\
1 & a\left(x,y\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\bar{f}\left(x+1,y\right) & b\left(x+1\right)\\
1 & f\left(x+1,y\right)
\end{smallmatrix}\right)\overset{\left(1\right)}{=}\left(\begin{smallmatrix}b\left(x\right) & b\left(x\right)f\left(x+1,y\right)\\
f\left(x,y\right) & b\left(x+1\right)+a\left(x,y\right)f\left(x+1,y\right)
\end{smallmatrix}\right)\\
M_{Y}^{cf}\left(x,y\right)M_{X}^{cf}\left(x,y+1\right) & =\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & b\left(x\right)\\
1 & f\left(x,y\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}0 & b\left(x\right)\\
1 & a\left(x,y+1\right)
\end{smallmatrix}\right)\overset{\left(2\right)}{=}\left(\begin{smallmatrix}b\left(x\right) & b\left(x\right)f\left(x+1,y\right)\\
f\left(x,y\right) & b\left(x\right)+f\left(x,y\right)a\left(x,y+1\right)
\end{smallmatrix}\right)
\end{align*}
The two matrices are the same using (3) from above.
\item Simple computation.
\end{enumerate}
\end{proof}
\begin{example}
\label{exa:vector-field-examples}There are many examples of conservative
matrix fields, and we give some of them below.

For each pair $f,\bar{f}$, we also add the $b\left(x\right),a\left(x,y\right)$
appearing as the continued fraction on the horizontal lines. In particular,
as we saw in \remref{ff(0)=00003D0}, when $\left(f\bar{f}\right)\left(0,0\right)=0$,
the $y=1$ line is in the Euler Family from \exaref{Euler-family},
namely $b\left(n\right)=-h_{1}\left(n\right)\times h_{2}\left(n\right)$
and $a\left(n\right)=h_{1}\left(n\right)+h_{2}\left(n+1\right)$.
In these cases we can convert it to an infinite sum and hopefully
use it to compute the value of the continued fraction, which we add
in the examples below (up to a Mobius map). Further more, in many
cases we think of $\bar{f}$ as an image under some nice linear map
$g\mapsto\bar{g}$ of $f$, and when this is the case, we will give
this linear map instead of $\bar{f}$.
\begin{enumerate}
\item When both $f,\bar{f}$ are linear themselves, then solving the linear
and quadratic conditions in \defref{conjugate} is elementary (which
we show in \appref{Conservative-degree-1}). There are two families
of solutions
\begin{align*}
f\left(x,y\right) & =A\left(x+y\right)+C\\
\bar{f}\left(x,y\right) & =\bar{A}\left(x-y\right)+\bar{C}
\end{align*}
or 
\begin{align*}
f\left(x,y\right) & =Ax+By+C\\
\bar{f}\left(x,y\right) & =-Ax+By+\bar{C}
\end{align*}
where $A,B,C,\bar{A},\bar{C}$ above are the parameters of the families.
\begin{enumerate}
\item Taking $f\left(x,y\right)=x+y$ and $\bar{f}\left(x,y\right)=x-y$,
we get $b\left(x\right)=x^{2}$ and $a\left(x,y\right)=2y-1$. In
$y=1$ we get the continued fraction
\[
\KK_{1}^{\infty}\frac{n^{2}}{1}=\KK_{1}^{\infty}\frac{-\left(-n\right)\times n}{\left(-n\right)+\left(n+1\right)}=\frac{1}{\sum_{k=0}^{\infty}\prod_{i=1}^{k}\frac{-i}{i+1}}-1=\frac{1}{\sum_{k=0}^{\infty}\frac{\left(-1\right)^{k}}{k+1}}-1=\frac{1-\ln\left(2\right)}{\ln\left(2\right)}.
\]
Taking $\bar{f}\left(x,y\right)=y-x$ instead, we get $b\left(x\right)=-x^{2}$
and $a\left(x,y\right)=2x+1$. Since $a$ is independent of $y$,
all the horizontal lines in the matrix field are the same, so in a
sense it is degenerate. Moreover, trying to compute the continued
fraction produces
\[
\KK_{1}^{\infty}\frac{-n^{2}}{2n+1}=\KK_{1}^{\infty}\frac{-n\times n}{n+\left(n+1\right)}=\frac{1}{\sum_{k=0}^{\infty}\prod_{i=1}^{k}\frac{i}{i+1}}-1=\frac{1}{\sum_{k=0}^{\infty}\frac{1}{k+1}}-1=-1,
\]
since the harmonic sum $\sum_{0}^{\infty}\frac{1}{k+1}$ diverges
to infinity.
\item For $f\left(x,y\right)=x+y$ and $\bar{f}\left(x,y\right)=1$ (which
we can think of as $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=\bar{f}$),
we get $b\left(x\right)=x,\;a\left(x,y\right)=x+y-1$, and in the
$y=1$ case we get 
\[
\KK_{1}^{\infty}\frac{n}{n}=\KK_{1}^{\infty}\frac{-\left(\left(-1\right)\times n\right)}{\left(-1\right)+\left(n+1\right)}=\frac{1}{\sum_{k=0}^{\infty}\prod_{i=1}^{k}\frac{-1}{i+1}}-1=\frac{1}{e-1}
\]
which we already saw in \exaref{(The-exponential-function)}.
\end{enumerate}
\item When $f,\bar{f}$ have degree at most 2, then we have the following
families of examples (as function of $C$):{\tiny{}
\[
\begin{array}{c|c|c|c|c}
\text{operation} & f\left(x,y\right) & a\left(x,y\right) & b\left(x\right) & \text{Euler family}\;\left(a\left(x,1\right)\right)\\
\hline \bar{g}\left(x,y\right)=-g\left(-x,y\right) & x^{2}+xy+\frac{y^{2}}{2}+C\left(x+y\right) & \left(x+1\right)^{2}+x^{2}+y\left(y-1\right)+C\left(2y-1\right) & -x^{2}\left(x^{2}-C^{2}\right) & \left(x+1\right)\left(x+1+C\right)+x\left(x-C\right)\\
\bar{g}\left(x,y\right)=g\left(-x,y\right) & x^{2}+2xy+2y^{2}+C\left(2y-x\right) & \left(2x+1\right)\left(2y-1+C\right) & x^{2}\left(x^{2}-C^{2}\right) & \left(x+1\right)\left(x+1+C\right)-x\left(x-C\right)\\
\bar{g}\left(x,y\right)=g\left(x,-y\right) & x^{2}+2xy+2y^{2}+C\left(x+y\right) & \left(2x+1+C\right)\left(2y-1\right) & x^{2}\left(x+C\right)^{2} & \left(x+1\right)\left(x+C+1\right)-x\left(x+C\right)\\
\bar{g}\left(x,y\right)=-g\left(x,-y\right) & \frac{2x^{2}+2xy+y^{2}+C\left(2x+y\right)}{2} & C\left(2x+1\right)+x^{2}+\left(x+1\right)^{2}+y\left(y-1\right) & -x^{2}\left(x+C\right)^{2} & \left(x+1\right)\left(x+C+1\right)+x\left(x+C\right)
\end{array}
\]
}In particular, when taking $C=0$, the $y=1$ line is either $b\left(x\right)=-x^{4}$
and $a\left(x\right)=x^{2}+\left(1+x\right)^{2}$, or $b\left(x\right)=x^{4}$
and $a\left(x\right)=\left(x+1\right)^{2}-x^{2}$. The continued fraction
will eventually be transformed (after the right Mobius action) to
the sums $\sum_{1}^{\infty}\frac{1}{n^{2}}$ and $\sum_{1}^{\infty}\frac{\left(-1\right)^{n}}{n^{2}}$
which are $\zeta\left(2\right)$ and $\frac{1}{2}\zeta\left(2\right)$
respectively.
\item For the operation $\bar{g}\left(x,y\right)=-g\left(x-y,y\right)$
we have
\begin{align*}
f\left(x,y\right) & =x^{2}+xy+\frac{y^{2}}{2}+x+\frac{y}{2}\\
b\left(x\right) & =-x^{2}\left(x+1\right)^{2}\\
a\left(x,y\right) & =2\left(x+1\right)^{2}+y\left(y-1\right)
\end{align*}
For $y=1$ we get the continued fraction with $b\left(x\right)=-x^{2}\left(x+1\right)^{2}$
and $a\left(x,1\right)=2\left(x+1\right)^{2}$. The equivalence transformation
in \lemref{equivalence-transformation} allows us to cancel $x\left(x+1\right)$
in $b$ and $\left(x+1\right)$ in $a$ twice and get
\[
\KK_{1}^{\infty}\frac{-n^{2}\left(n+1\right)^{2}}{2\left(n+1\right)^{2}}=\KK_{1}^{\infty}\frac{-1}{2}=-1
\]
\item For degree at most 3, with the action $\bar{g}\left(x,y\right)\mapsto g\left(-x,y\right)$,
we have the family 
\begin{align*}
f\left(x,y\right) & =x^{3}+2x^{2}\left(y-C\right)+2x\left(y-C\right)^{2}+\left(y-C\right)^{3}-\left(x+y-C\right)C^{2}\\
b\left(x\right) & =-x^{2}\left(x-C\right)^{2}\left(x+C\right)^{2}\\
a\left(x,y\right) & =x\left(x-C\right)^{2}+\left(x+1\right)\left(x+1+C\right)^{2}+\left(1+2x\right)\left(y-1-2C\right)2y.
\end{align*}
When $y=1$ we get a continued fraction in the Euler family with $h_{1}\left(x\right)=x\left(x-C\right)^{2}$
and $h_{2}\left(x\right)=x\left(x+C\right)^{2}$. In particular, in
the case where $C=0$ we simply get the matrix field for $\zeta\left(3\right)$
mentioned in \exaref{zeta-3-matrix-field}.
\end{enumerate}
\end{example}

\begin{rem}
Once we have a pair of conjugate polynomials $f,\bar{f}$, there are
several ways to generate more such pairs. The simplest way is just
to take $cf,c\bar{f}$ for some $0\neq c\in\CC$. Another less trivial
way is to look at the pair $\left(f\left(y,x\right),\;-\bar{f}\left(y,x\right)\right)$.
We shall see in \subsecref{The-dual-matrix-field} how this new pair
is hidden in the same conservative matrix field.
\end{rem}

Right now, while the $M_{X}^{cf}$ matrix has the known continued
fraction form, the $M_{Y}^{cf}$ matrices have this new unkown form
$\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & b\left(x\right)\\
1 & f\left(x,y\right)
\end{smallmatrix}\right)$. However, we already saw in \thmref{to-gcf} how to convert matrices
to continued fraction form, which works best when the bottom left
coordinate is constant, like it is in our case. Moreover, we will
show an even stronger result that the continued fraction in $M_{X}^{cf}$
and the hidden continued fraction in $M_{Y}$ are both defined very
similarly. For that, we use the following notations.
\begin{notation}
We define:
\[
U_{\alpha}=\left(\begin{smallmatrix}1 & \alpha\\
0 & 1
\end{smallmatrix}\right)\qquad D_{\alpha}=\left(\begin{smallmatrix}\alpha & 0\\
0 & 1
\end{smallmatrix}\right)\qquad\tau=\left(\begin{smallmatrix}0 & 1\\
1 & 0
\end{smallmatrix}\right).
\]
For any matrix $M$, we will write the isomorphism $M\mapsto M^{\tau}=\tau M\tau^{-1}$
(and note that $\tau^{2}=Id$, so that $\tau^{-1}=\tau$). More specifically,
we have that $\left(\begin{smallmatrix}a & b\\
c & c
\end{smallmatrix}\right)^{\tau}=\left(\begin{smallmatrix}d & c\\
b & a
\end{smallmatrix}\right)$ is just switching the rows and switching the columns, and in particular
$U_{\alpha}^{\tau}=U_{\alpha}^{tr}$.
\end{notation}

With these notations we get:
\begin{align*}
M_{X}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}0 & b\left(x\right)\\
1 & f\left(x,y\right)-\bar{f}\left(x+1,y\right)
\end{smallmatrix}\right)=D_{b\left(x\right)}\tau U_{f\left(x,y\right)}U_{-\bar{f}\left(x+1,y\right)}\\
M_{Y}^{cf}\left(x,y\right) & =\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & b\left(x\right)\\
1 & f\left(x,y\right)
\end{smallmatrix}\right)=U_{\bar{f}\left(x,y\right)}D_{-\left(f\bar{f}\right)\left(0,y\right)}\tau U_{f\left(x,y\right)},
\end{align*}
so that $M_{X}^{cf}$ and $M_{Y}^{cf}$ are ``almost'' the same.
There is some ``cyclic permutation'' and after it they have a similar
structure, with related parameters, and in particular the $M_{Y}^{cf}$
is also a continued fraction sequence, after a simple coboundary equivalence
(via the matrices $U_{\bar{f}\left(x,y\right)}$).

As mentioned in \remref{ff(0)=00003D0}, if $\left(f\bar{f}\right)\left(0\right)=0$,
then the $Y=0$ and $Y=1$ lines are in the trivial Euler family.
Similarly, on the $X=0$ line we have 
\[
M_{Y}^{cf}\left(0,y\right)=\left(\begin{smallmatrix}\bar{f}\left(0,y\right) & 0\\
1 & f\left(0,y\right)
\end{smallmatrix}\right),
\]
which is even simpler to work with (recall that the continued fraction
in the trivial Euler family, are in essence upper triangular in disguise).
However, on the $X=0$ line we have that $M_{X}^{cf}\left(0,y\right)=\left(\begin{smallmatrix}0 & 0\\
1 & f\left(0,y\right)-\bar{f}\left(1,y\right)
\end{smallmatrix}\right)$ are not invertible. With this in mind, we do a slight change of parameters,
which will solve this problem, and we will see is more natural.
\begin{defn}
Let $f,\bar{f}$ be conjugate polynomials and $M_{X},M_{Y}$ as in
\defref{conjugate}. Define
\begin{align*}
M_{X}\left(x,y\right) & :=D_{b\left(x\right)}^{-1}M_{X}^{cf}\left(x,y\right)D_{b\left(x+1\right)}=\tau U_{f\left(x,y\right)}U_{-\bar{f}\left(x+1,y\right)}D_{b\left(x+1\right)}=\left(\begin{smallmatrix}0 & 1\\
b\left(x+1\right) & f\left(x,y\right)-\bar{f}\left(x+1,y\right)
\end{smallmatrix}\right)\\
M_{Y}\left(x,y\right) & :=D_{b\left(x\right)}^{-1}M_{Y}^{cf}\left(x,y\right)D_{b\left(x\right)}=U_{f\left(x,y\right)}^{\tau}\tau D_{-\left(f\bar{f}\right)\left(0,y\right)}U_{\bar{f}\left(x,y\right)}^{\tau}=\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & 1\\
b\left(x\right) & f\left(x,y\right)
\end{smallmatrix}\right)
\end{align*}
\end{defn}

There are three main reasons why this is a bit better way to view
our matrix fields.
\begin{enumerate}
\item If we start the continued fraction on the $y$ line at $x=0$ with
the previous $M_{X}^{cf}$ matrices, we get 
\[
M_{X}^{cf}\left(0,y\right)M_{X}^{cf}\left(1,y\right)M_{X}^{cf}\left(2,y\right)\cdots=\left[D_{b\left(0\right)}\tau U_{f\left(0,y\right)}U_{-\bar{f}\left(0+1,y\right)}\right]\left[D_{b\left(1\right)}\tau U_{f\left(1,y\right)}U_{-\bar{f}\left(1+1,y\right)}\right]\cdots
\]
where as we mentioned before $D_{b\left(0\right)}=\left(\begin{smallmatrix}0 & 0\\
0 & 1
\end{smallmatrix}\right)$ is singular which can cause problems. This means that we have to
start with $x=1$, and therefore ``lose'' the information from $\tau U_{f\left(0,y\right)}U_{-\bar{f}\left(0+1,y\right)}$.
With our new matrices $M_{X}$ we instead get
\[
M_{X}\left(0,y\right)M_{X}\left(1,y\right)M_{X}\left(2,y\right)\cdots=\left[\tau U_{f\left(0,y\right)}U_{-\bar{f}\left(0+1,y\right)}D_{b\left(1\right)}\right]\left[\tau U_{f\left(1,y\right)}U_{-\bar{f}\left(1+1,y\right)}D_{b\left(2\right)}\right]\cdots
\]
so we start exactly after the problematic matrix $D_{b\left(0\right)}$.
\item With this new definition, where we start at $x=0$, we get that $M_{Y}\left(0,y\right)$
is upper triangular, since
\[
M_{Y}\left(0,y\right)=\left(\begin{smallmatrix}\bar{f}\left(0,y\right) & 1\\
b\left(0\right) & f\left(0,y\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}\bar{f}\left(0,y\right) & 1\\
0 & f\left(0,y\right)
\end{smallmatrix}\right).
\]
\item Finally, as we shall see below, the limits for each horizontal line
are more natural, namely 
\[
\limfi N\left[\prod_{n=0}^{N}M_{X}\left(n,m\right)\right]\left(0\right)=\tau U_{a\left(0,m\right)}\limfi N\left[\prod_{n=1}^{N}M_{X}^{cf}\left(n,m\right)\right]\left(0\right)=\left(1+a\left(0,m\right)\left(\KK_{1}^{\infty}\frac{b\left(n\right)}{a\left(n,m\right)}\right)\right)^{-1}.
\]
 In particular, in the new matrix field for our $\zeta\left(3\right)$
example, we will get the limits $\sum_{m}^{\infty}\frac{1}{k^{3}}$.
This will simplify the arguments when trying to find the denominators
and numerators of the convergents.
\end{enumerate}
With this in mind we rewrite \thmref{matrix-field-structure} and
expand it with this new matrices.
\begin{thm}
\label{thm:normalized-matrix-field}Let $f,\bar{f},a,b$ be polynomials
as in \defref{conjugate}. We set 
\begin{align*}
M_{X}\left(x,y\right) & :=\tau U_{f\left(x,y\right)}U_{-\bar{f}\left(x+1,y\right)}D_{b\left(x+1\right)}=\left(\begin{smallmatrix}0 & 1\\
b\left(x+1\right) & f\left(x,y\right)-\bar{f}\left(x+1,y\right)
\end{smallmatrix}\right)\\
M_{Y}\left(x,y\right) & :=U_{f\left(x,y\right)}^{\tau}\tau D_{-\left(f\bar{f}\right)\left(0,y\right)}U_{\bar{f}\left(x,y\right)}^{\tau}=\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & 1\\
b\left(x\right) & f\left(x,y\right)
\end{smallmatrix}\right)
\end{align*}
Then
\begin{enumerate}
\item The matrices form a conservative matrix field, namely
\[
M_{X}\left(x,y\right)M_{Y}\left(x+1,y\right)=M_{Y}\left(x,y\right)M_{X}\left(x,y+1\right).
\]
\item The determinants of $M_{X}\left(x,y\right),M_{Y}\left(x,y\right)$
are only functions of $x,y$ respectively, and more specifically:
\begin{align*}
\det\left(M_{X}\left(x,y\right)\right) & =-b\left(x+1\right)=\left(f\cdot\bar{f}\right)\left(0,0\right)-\left(f\cdot\bar{f}\right)\left(x+1,0\right)\\
\det\left(M_{Y}\left(x,y\right)\right) & =\left(f\cdot\bar{f}\right)\left(x,y\right)-b\left(x\right)=\left(f\cdot\bar{f}\right)\left(0,y\right).
\end{align*}
\item For $x=0$ , the matrices $M_{Y}\left(0,y\right)$ are upper triangular
\[
M_{Y}\left(0,y\right)=\left(\begin{smallmatrix}\bar{f}\left(0,y\right) & 1\\
0 & f\left(0,y\right)
\end{smallmatrix}\right).
\]
\end{enumerate}
\end{thm}

\begin{proof}
This follows directly from \thmref{matrix-field-structure} .
\end{proof}
Next, we use the fact that the $Y=1$ line in the original conservative
matrix field is in the trivial Euler family, to find a simple presentation
for the $Y=1$ line in our new matrix field.
\begin{lem}
\label{lem:First-row}Suppose that $\left(f\bar{f}\right)\left(0,0\right)=0$.
Then
\begin{align*}
U_{\bar{f}\left(0,0\right)}^{\tau}\left[\prod_{0}^{n-1}M_{X}\left(k,1\right)\right]U_{-\bar{f}\left(n,0\right)}^{\tau} & =\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{1}^{n}\bar{f}\left(k,0\right) & c_{n}\\
0 & \prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)\\
c_{n} & =\sum_{k=1}^{n}\left(-1\right)^{k-1}\left(\prod_{i=1}^{k-1}\bar{f}\left(i,0\right)\right)\left(\prod_{i=k+1}^{n}f\left(i,0\right)\right).
\end{align*}
\end{lem}

\begin{proof}
Assuming that $\left(f\bar{f}\right)\left(0,0\right)=0$, at the $Y=1$
we have 
\[
M_{X}\left(x,1\right)=\left(\begin{smallmatrix}0 & 1\\
\left(f\bar{f}\right)\left(x+1,0\right) & f\left(x,1\right)-\bar{f}\left(x+1,1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0 & 1\\
\left(f\bar{f}\right)\left(x+1,0\right) & f\left(x+1,0\right)-\bar{f}\left(x,0\right)
\end{smallmatrix}\right).
\]
Setting $v\left(x\right)=\left(\bar{f}\left(x,0\right),1\right)$
we get $v\left(x\right)M_{X}\left(x,1\right)=f\left(x+1,0\right)v\left(x+1\right)$,
namely these are eigenvectors with eigenvalue $f\left(x+1,0\right)$.
Similarly we have the right $-\bar{f}\left(x+1,0\right)$-eigenvectors
$\left(\begin{smallmatrix}1\\
-\bar{f}\left(x+1,0\right)
\end{smallmatrix}\right)$. By \lemref{triangularization}, setting $U_{x}=\left(\begin{smallmatrix}1 & 0\\
\bar{f}\left(x,0\right) & 1
\end{smallmatrix}\right)$ we get 
\[
U_{\bar{f}\left(x,0\right)}^{\tau}M_{X}\left(x,1\right)U_{-\bar{f}\left(x+1,0\right)}^{\tau}=\left(\begin{smallmatrix}-\bar{f}\left(x+1,0\right) & 1\\
0 & f\left(x+1,0\right)
\end{smallmatrix}\right).
\]
It follows that 
\[
U_{\bar{f}\left(0,0\right)}^{\tau}\left[\prod_{0}^{n-1}M_{X}\left(k,1\right)\right]U_{-\bar{f}\left(n,0\right)}=\prod_{1}^{n}\left(\begin{smallmatrix}-\bar{f}\left(k,0\right) & 1\\
0 & f\left(k,0\right)
\end{smallmatrix}\right),
\]
which by \claimref{upper-triangular} is equal to
\[
\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{1}^{n}\bar{f}\left(k,0\right) & c_{n}\\
0 & \prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)\quad,\quad c_{n}=\sum_{k=1}^{n}\left(-1\right)^{k-1}\left(\prod_{i=1}^{k-1}\bar{f}\left(k,0\right)\right)\left(\prod_{i=k+1}^{n}f\left(k,0\right)\right).
\]
\end{proof}
\begin{rem}
Note in particular that when $\bar{f}\left(0,0\right)=0$ in the lemma
above, then $U_{\bar{f}\left(0,0\right)}^{\tau}=I$ is simply the
identity matrix.
\end{rem}


\subsection{\label{subsec:The-dual-matrix-field}The dual conservative matrix
field}

With \thmref{normalized-matrix-field} and \lemref{First-row} in
the previous section, we see that we understand quite well both the
$Y=1$ and $X=0$ lines. More over, we already saw that both the horizontal
and the vertical lines in the matrix field are more or less continued
fractions, namely
\begin{align*}
M_{X}\left(x,y\right) & :=\tau U_{f\left(x,y\right)}U_{-\bar{f}\left(x+1,y\right)}D_{b\left(x+1\right)}=\left(\begin{smallmatrix}0 & 1\\
b\left(x+1\right) & f\left(x,y\right)-\bar{f}\left(x+1,y\right)
\end{smallmatrix}\right)\\
M_{Y}\left(x,y\right) & :=U_{f\left(x,y\right)}^{\tau}\tau D_{-\left(f\bar{f}\right)\left(0,y\right)}U_{\bar{f}\left(x,y\right)}^{\tau}=\left(\begin{smallmatrix}\bar{f}\left(x,y\right) & 1\\
b\left(x\right) & f\left(x,y\right)
\end{smallmatrix}\right)
\end{align*}
The next goal is to use this almost symmetry with the hope of eventually
saying something about the diagonal line $X=Y$.
\begin{defn}[The dual matrix field]
Let $f\left(x,y\right),\bar{f}\left(x,y\right)$ be conjugate polynomial,
and let $M_{X},M_{Y}$ be as above. We define the dual matrix field
to be
\begin{align*}
\hat{M}_{Y}\left(y,x\right) & =U_{\bar{f}\left(x-1,y\right)}^{\tau}M_{X}\left(x-1,y+1\right)U_{-\bar{f}\left(x,y\right)}^{\tau}=U_{f\left(x,y\right)}^{\tau}\tau D_{b\left(x\right)}U_{-\bar{f}\left(x,y\right)}^{\tau}\\
\hat{M}_{X}\left(y,x\right) & =U_{\bar{f}\left(x-1,y\right)}^{\tau}M_{Y}\left(x-1,y+1\right)U_{-\bar{f}\left(x-1,y+1\right)}^{\tau}=\tau U_{\bar{f}\left(x-1,y\right)}U_{f\left(x-1,y+1\right)}D_{-b_{Y}\left(y+1\right)}
\end{align*}
This new matrix field corresponds to the conjugate polynomials 
\begin{align*}
\hat{f}\left(x,y\right) & =f\left(y,x\right)\\
\bar{\hat{f}}\left(x,y\right) & =-\bar{f}\left(y,x\right)\\
\hat{a}\left(x,y\right) & =\hat{f}\left(x,y\right)-\bar{\hat{f}}\left(x+1,y\right)=f\left(y,x\right)+\bar{f}\left(y,x+1\right)\\
\hat{b}\left(x\right) & =\left(\hat{f}\bar{\hat{f}}\right)\left(x,0\right)=-\left(f\bar{f}\right)\left(0,x\right)
\end{align*}
\end{defn}

\begin{example}
In the $\zeta\left(3\right)$ matrix field mentioned in \exaref{zeta-3-matrix-field}
we have a special case where
\[
f\left(x,y\right)=\frac{y^{3}-x^{3}}{y-x}\left(y+x\right)\qquad;\qquad\bar{f}\left(x,y\right)=\frac{y^{3}+x^{3}}{y+x}\left(y-x\right),
\]
satisfy $f\left(x,y\right)=f\left(y,x\right)$ and $\bar{f}\left(x,y\right)=-\bar{f}\left(y,x\right)$,
so that $\hat{f}=f$ and $\bar{\hat{f}}=\bar{f}$.

In the $\zeta\left(2\right)$ matrix field from \exaref{vector-field-examples},
we have
\[
f\left(x,y\right)=2x^{2}+2xy+y^{2}\qquad;\qquad\bar{f}\left(x,y\right)=-2x^{2}+2xy-y^{2}
\]
so that 
\[
\hat{f}\left(x,y\right)=x^{2}+2xy+2y^{2}\qquad;\qquad\bar{\hat{f}}\left(x,y\right)=x^{2}-2xy+2y^{2}
\]
and therefore
\begin{align*}
\hat{a}\left(x,y\right) & =\left(x^{2}+2xy+2y^{2}\right)-\left(\left(x+1\right)^{2}-2\left(x+1\right)y+2y^{2}\right)=\left(2y-1\right)\left(2x+1\right)\\
\hat{b}_{X}\left(x\right) & =x^{4}.
\end{align*}
\end{example}

This dual matrix field construction not only gives us free of charge
another conservative matrix field for every one that we find, but
they are also closely related. In the matrix field with $M_{X},M_{Y}$
, the horizontal lines are (almost) polynomial continued fractions,
and we wish to study how the numerators and denominator behave there.
By definition, the horizontal lines of the dual matrix field correspond
to vertical line in the original matrix field, so understanding the
full matrix field is equivalent to understand these continued fractions.
More precisely, since
\[
M_{Y}\left(x,y\right)=U_{-\bar{f}\left(x,y-1\right)}^{\tau}\hat{M}_{X}\left(y-1,x+1\right)U_{\bar{f}\left(x,y\right)}^{\tau},
\]
we get that
\begin{equation}
\prod_{k=1}^{n}M_{Y}\left(y,k\right)=U_{-\bar{f}\left(y,0\right)}^{\tau}\left[\prod_{k=0}^{n-1}\hat{M}_{X}\left(k,y+1\right)\right]U_{\bar{f}\left(y,n\right)}^{\tau}\label{eq:dual-row-column}
\end{equation}

With this dualic structure we turn to study the rational approximation
given by the different points on the matrix field, and more concretely
how far the standard rational presentation is from being a reduced
rational presentation. 
\begin{defn}
\label{def:deno-nume}For every $n\geq0$ define the polynomial vectors
\begin{align*}
\left(\begin{smallmatrix}p_{n}\left(y\right)\\
q_{n}\left(y\right)
\end{smallmatrix}\right) & =\left[\prod_{0}^{n-1}M_{X}\left(k,y\right)\right]e_{2}\\
\left(\begin{smallmatrix}\hat{p}_{n}\left(y\right)\\
\hat{q}_{n}\left(y\right)
\end{smallmatrix}\right) & =\left[\prod_{0}^{n-1}\hat{M}_{X}\left(k,y\right)\right]e_{2}.
\end{align*}
\end{defn}

For example, the first few values of $p_{n}\left(m\right),q_{n}\left(m\right)$
are arranged as :

\begin{align*}
\xymatrix{\left(*\right) &  &  & \left(*\right) &  &  & \left(*\right) &  & \left(*\right)\\
\\
{\left(\begin{smallmatrix}p_{0}\left(3\right)\\
q_{0}\left(3\right)
\end{smallmatrix}\right)}\ar[rrr]_{M_{X}\left(0,3\right)}\ar@{..>}[uu]^{M_{Y}\left(0,3\right)} &  &  & {\left(\begin{smallmatrix}p_{1}\left(3\right)\\
q_{1}\left(3\right)
\end{smallmatrix}\right)}\ar[rrr]_{M_{X}\left(1,3\right)}\ar@{..>}[uu]^{M_{Y}\left(1,3\right)} &  &  & {\left(\begin{smallmatrix}p_{2}\left(3\right)\\
q_{2}\left(3\right)
\end{smallmatrix}\right)}\ar[rr]_{M_{X}\left(2,3\right)}\ar@{..>}[uu]^{M_{Y}\left(2,3\right)} &  & \left(*\right)\\
\\
{\left(\begin{smallmatrix}p_{0}\left(2\right)\\
q_{0}\left(2\right)
\end{smallmatrix}\right)}\ar[rrr]_{M_{X}\left(0,2\right)}\ar@{..>}[uu]^{M_{Y}\left(0,2\right)} &  &  & {\left(\begin{smallmatrix}p_{1}\left(2\right)\\
q_{1}\left(2\right)
\end{smallmatrix}\right)}\ar[rrr]_{M_{X}\left(1,2\right)}\ar@{..>}[uu]^{M_{Y}\left(1,2\right)} &  &  & {\left(\begin{smallmatrix}p_{2}\left(2\right)\\
q_{2}\left(2\right)
\end{smallmatrix}\right)}\ar[rr]_{M_{X}\left(2,2\right)}\ar@{..>}[uu]^{M_{Y}\left(2,2\right)} &  & \left(*\right)\\
\\
{\left(\begin{smallmatrix}p_{0}\left(1\right)\\
q_{0}\left(1\right)
\end{smallmatrix}\right)}\ar[rrr]_{M_{X}\left(0,1\right)}\ar@{..>}[uu]^{M_{Y}\left(0,1\right)} &  &  & {\left(\begin{smallmatrix}p_{1}\left(1\right)\\
q_{1}\left(1\right)
\end{smallmatrix}\right)}\ar[rrr]_{M_{X}\left(1,1\right)}\ar@{..>}[uu]^{M_{Y}\left(1,1\right)} &  &  & {\left(\begin{smallmatrix}p_{2}\left(1\right)\\
q_{2}\left(1\right)
\end{smallmatrix}\right)}\ar[rr]_{M_{X}\left(2,1\right)}\ar@{..>}[uu]^{M_{Y}\left(2,1\right)} &  & \left(*\right)
}
\end{align*}

\begin{rem}
Note that since $M_{X}\left(k,y\right)e_{1}=b\left(k+1\right)e_{2}$
and $\hat{M}_{X}\left(k,y\right)e_{1}=-b_{Y}\left(k\right)$, we have
for $n\geq1$
\begin{align*}
\left(\begin{smallmatrix}p_{n-1}\left(y\right) & p_{n}\left(y\right)\\
q_{n-1}\left(y\right) & q_{n}\left(y\right)
\end{smallmatrix}\right)D_{b_{X}\left(n\right)} & =\prod_{0}^{n-1}M_{X}\left(k,y\right)\\
\left(\begin{smallmatrix}\hat{p}_{n-1}\left(y\right) & \hat{p}_{n}\left(y\right)\\
\hat{q}_{n-1}\left(y\right) & \hat{q}_{n}\left(y\right)
\end{smallmatrix}\right)D_{-b_{Y}\left(n\right)} & =\prod_{0}^{n-1}\hat{M}_{X}\left(k,y\right).
\end{align*}
\end{rem}

To study these polynomials $p_{n}\left(m\right)$ and $q_{n}\left(m\right)$,
we use the conservative matrix field structure to see what happens
when we increase $n$ or increase $m$, and also what is the connections
between them and their duals $\hat{p}_{m}\left(n\right)$ and $\hat{q}_{m}\left(n\right)$.
\begin{claim}
\label{claim:dual-field-identities}Let $f,\bar{f}\in\ZZ\left[x,y\right]$
be conjugate polynomials such that $\left(f\bar{f}\right)\left(0,0\right)=0$
and let $p_{n},q_{n},\hat{p}_{m},\hat{q}_{m}$ as in \defref{deno-nume}
above. Then
\begin{enumerate}
\item \label{enu:first-line-polynomials}Evaluating the polynomial $p_{n},q_{n}$
at $m=1$ we have
\begin{align*}
p_{n}\left(1\right) & =\sum_{k=1}^{n}\left(-1\right)^{k-1}\left(\prod_{i=1}^{k-1}\bar{f}\left(k,0\right)\right)\left(\prod_{i=k+1}^{n}f\left(k,0\right)\right)\\
q_{n}\left(1\right) & =\prod_{1}^{n}f\left(k,0\right)-\bar{f}\left(0,0\right)p_{n}\left(1\right).
\end{align*}
\item \label{enu:Increase-n}When increasing $n$ we get that 
\begin{align*}
\left(\begin{smallmatrix}p_{n+1}\left(y\right)\\
q_{n+1}\left(y\right)
\end{smallmatrix}\right) & =\left(\begin{smallmatrix}p_{n-1}\left(y\right) & p_{n}\left(y\right)\\
q_{n-1}\left(y\right) & q_{n}\left(y\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}b\left(n\right)\\
a\left(n,y\right)
\end{smallmatrix}\right).
\end{align*}
\item \label{enu:increase-m}When $\left(f\bar{f}\right)\left(0,m\right)\neq0$,
increasing $m$ follows the recurrence 
\begin{align*}
\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right) & =\frac{1}{\left(f\bar{f}\right)\left(0,m\right)}\left(\begin{smallmatrix}f\left(0,m\right) & -1\\
0 & \bar{f}\left(0,m\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n-1}\left(m\right) & p_{n}\left(m\right)\\
q_{n-1}\left(m\right) & q_{n}\left(m\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\left(f\bar{f}\right)\left(n,0\right)\\
f\left(n,m\right)
\end{smallmatrix}\right)
\end{align*}
\item \label{enu:reciprocal-polynomials}Suppose that $\bar{f}\left(0,0\right)=0$.
Then the polynomials $p_{n},q_{n}$ and $\hat{p}_{m},\hat{q}_{m}$
are connected by the following equation
\[
\left(\begin{smallmatrix}\prod_{k=1}^{m}\bar{f}\left(0,k\right) & \hat{p}_{m}\left(1\right)\\
0 & \hat{q}_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{k=1}^{n}\bar{f}\left(k,0\right) & p_{n}\left(1\right)\\
0 & q_{n}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\hat{p}_{m}\left(n+1\right)\\
\hat{q}_{m}\left(n+1\right)
\end{smallmatrix}\right),
\]
and in particular we have that $\hat{q}_{m}\left(1\right)q_{n}\left(m+1\right)=q_{n}\left(1\right)\hat{q}_{m}\left(n+1\right)$.
\end{enumerate}
\end{claim}

\begin{proof}
\begin{enumerate}
\item Applying \lemref{First-row} we get 
\begin{align*}
\left(\begin{smallmatrix}p_{n}\left(1\right)\\
q_{n}\left(1\right)
\end{smallmatrix}\right) & =\left[\prod_{0}^{n-1}M_{X}\left(k,1\right)\right]e_{2}=U_{-\bar{f}\left(0,0\right)}^{\tau}\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{1}^{n}\bar{f}\left(k,0\right) & c_{n}\\
0 & \prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)U_{\bar{f}\left(n,0\right)}^{\tau}e_{2}\\
 & =U_{-\bar{f}\left(0,0\right)}^{\tau}\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{1}^{n}\bar{f}\left(k,0\right) & c_{n}\\
0 & \prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)e_{2}=U_{-\bar{f}\left(0,0\right)}^{\tau}\left(\begin{smallmatrix}c_{n}\\
\prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)
\end{align*}
where
\begin{align*}
c_{n} & =\sum_{k=1}^{n}\left(-1\right)^{k-1}\left(\prod_{i=1}^{k-1}\bar{f}\left(k,0\right)\right)\left(\prod_{i=k+1}^{n}f\left(k,0\right)\right).
\end{align*}
It follows that 
\begin{align*}
p_{n}\left(1\right) & =e_{1}^{tr}\left(\begin{smallmatrix}p_{n}\left(1\right)\\
q_{n}\left(1\right)
\end{smallmatrix}\right)=e_{1}^{tr}U_{-\bar{f}\left(0,0\right)}^{\tau}\left(\begin{smallmatrix}c_{n}\\
\prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)=c_{n}\\
q_{n}\left(1\right) & =e_{2}^{tr}\left(\begin{smallmatrix}p_{n}\left(1\right)\\
q_{n}\left(1\right)
\end{smallmatrix}\right)=\left(-\bar{f}\left(0,0\right),1\right)\left(\begin{smallmatrix}c_{n}\\
\prod_{1}^{n}f\left(k,0\right)
\end{smallmatrix}\right)=\prod_{1}^{n}f\left(k,0\right)-\bar{f}\left(0,0\right)c_{n}\\
 & =\sum_{k=0}^{n}\left(-1\right)^{k}\left(\prod_{i=0}^{k-1}\bar{f}\left(k,0\right)\right)\left(\prod_{i=k+1}^{n}f\left(k,0\right)\right).
\end{align*}
\item This is the standard recursion for continued fractions, and it follows
from
\begin{align*}
\left(\begin{smallmatrix}p_{n+1}\left(y\right)\\
q_{n+1}\left(y\right)
\end{smallmatrix}\right) & =\left[\prod_{0}^{n}M_{X}\left(k,y\right)\right]e_{2}=\left[\prod_{0}^{n-1}M_{X}\left(k,y\right)\right]M_{X}\left(n,y\right)e_{2}\\
 & =\left(\begin{smallmatrix}p_{n-1}\left(y\right) & p_{n}\left(y\right)\\
q_{n-1}\left(y\right) & q_{n}\left(y\right)
\end{smallmatrix}\right)D_{b_{X}\left(n\right)}M_{X}\left(n,y\right)e_{2}=\left(\begin{smallmatrix}p_{n-1}\left(y\right) & p_{n}\left(y\right)\\
q_{n-1}\left(y\right) & q_{n}\left(y\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}b_{X}\left(n\right)\\
a\left(n,y\right)
\end{smallmatrix}\right).
\end{align*}
\item This follows from the coboundary condition of the conservative matrix
field structure
\begin{align*}
\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right) & =\left[\prod_{0}^{n-1}M_{X}\left(k,m+1\right)\right]e_{2}=M_{Y}\left(0,m\right)^{-1}\left[\prod_{0}^{n-1}M_{X}\left(k,m\right)\right]M_{Y}\left(n,m\right)e_{2}\\
 & =\left(\begin{smallmatrix}\bar{f}\left(0,m\right) & 1\\
0 & f\left(0,m\right)
\end{smallmatrix}\right)^{-1}\left(\begin{smallmatrix}p_{n-1}\left(m\right) & p_{n}\left(m\right)\\
q_{n-1}\left(m\right) & q_{n}\left(m\right)
\end{smallmatrix}\right)D_{b\left(n\right)}\cdot\left(\begin{smallmatrix}1\\
f\left(n,m\right)
\end{smallmatrix}\right)\\
 & =\frac{1}{\left(f\bar{f}\right)\left(0,m\right)}\left(\begin{smallmatrix}f\left(0,m\right) & -1\\
0 & \bar{f}\left(0,m\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n-1}\left(m\right) & p_{n}\left(m\right)\\
q_{n-1}\left(m\right) & q_{n}\left(m\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\left(f\bar{f}\right)\left(n,0\right)\\
f\left(n,m\right)
\end{smallmatrix}\right)
\end{align*}
\item We compute the matrix in the $\left(n,m+1\right)$ position in the
matrix field in two different ways - first by moving along the $X=0$
line and then $Y=m+1$ line, and second by moving along the $Y=1$
line and then the $X=n$ line, namely 
\[
\prod_{1}^{m}M_{Y}\left(0,k\right)\prod_{0}^{n-1}M_{X}\left(k,m+1\right)=\prod_{0}^{n-1}M_{X}\left(k,1\right)\prod_{1}^{m}M_{Y}\left(n,k\right).
\]
Converting the $M_{Y}$ into $\hat{M}_{X}$ via \eqref{dual-row-column}
we have
\[
U_{-\bar{f}\left(0,0\right)}^{\tau}\left[\prod_{k=0}^{m-1}\hat{M}_{X}\left(k,1\right)\right]U_{\bar{f}\left(0,m\right)}^{\tau}\prod_{0}^{n-1}M_{X}\left(k,m+1\right)=\prod_{0}^{n-1}M_{X}\left(k,1\right)U_{-\bar{f}\left(n,0\right)}^{\tau}\left[\prod_{k=0}^{m-1}\hat{M}_{X}\left(k,n+1\right)\right]U_{\bar{f}\left(n,m\right)}^{\tau}.
\]
Multiplying both side by $e_{2}$, we get 
\[
U_{-\bar{f}\left(0,0\right)}^{\tau}\left[\prod_{k=0}^{m-1}\hat{M}_{X}\left(k,1\right)\right]U_{\bar{f}\left(0,m\right)}^{\tau}\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)=\left[\prod_{0}^{n-1}M_{X}\left(k,1\right)\right]U_{-\bar{f}\left(n,0\right)}^{\tau}\left(\begin{smallmatrix}\hat{p}_{m}\left(n+1\right)\\
\hat{q}_{m}\left(n+1\right)
\end{smallmatrix}\right).
\]
Next, we use \lemref{First-row} as in the previous part, and the
fact that $U_{\bar{f}\left(0,0\right)}^{\tau}=Id$ to get that 
\begin{align*}
\left[\prod_{0}^{n-1}M_{X}\left(k,1\right)\right]U_{-\bar{f}\left(n,0\right)}^{\tau} & =\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{1}^{n}\bar{f}\left(k,0\right) & p_{n}\left(1\right)\\
0 & q_{n}\left(1\right)
\end{smallmatrix}\right)\\
\left[\prod_{0}^{m-1}\hat{M}_{X}\left(k,1\right)\right]U_{\bar{f}\left(0,m\right)}^{\tau} & =\left(\begin{smallmatrix}\prod_{1}^{m}\bar{f}\left(0,k\right) & \hat{p}_{m}\left(1\right)\\
0 & \hat{q}_{m}\left(1\right)
\end{smallmatrix}\right).
\end{align*}
Putting everything together, we get 
\[
\left(\begin{smallmatrix}\prod_{1}^{m}\bar{f}\left(0,k\right) & \hat{p}_{m}\left(1\right)\\
0 & \hat{q}_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{1}^{n}\bar{f}\left(k,0\right) & p_{n}\left(1\right)\\
0 & q_{n}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\hat{p}_{m}\left(n+1\right)\\
\hat{q}_{m}\left(n+1\right)
\end{smallmatrix}\right),
\]
which is what we wanted to show.
\end{enumerate}
\end{proof}
If $\bar{f}\left(0,0\right)=0$ as in the last part of the claim above,
then $\frac{q_{n}\left(m+1\right)}{q_{n}\left(1\right)}=\frac{\hat{q}_{m}\left(n+1\right)}{\hat{q}_{m}\left(1\right)}$
. Fixing $m$ and letting $n\to\infty$, the numbers $q_{n}\left(m\right)$
are simply the denominators for the continued fraction in the matrix
field appearing on the $m$'th row. As we already know how to compute
$q_{n}\left(1\right)$, in order to understand these denominators,
we would need to understand $\frac{\hat{q}_{m}\left(n+1\right)}{\hat{q}_{m}\left(1\right)}$.
Since $m$ is fixed, the function $n\mapsto\hat{q}_{m}\left(n+1\right)$
is just polynomial in $n$, and we divide it by the constant $\hat{q}_{m}\left(1\right)$.
This already tells us a lot about these denominators.

Eventually we would want to find the greatest common divisor of $q_{n}\left(m+1\right)$
and $p_{n}\left(m+1\right)$ and show that it is large. In particular,
it would be helpful to know if $q_{n}\left(1\right)\mid q_{n}\left(m+1\right)$
for all $n$, which we just saw is equivalent to $\frac{\hat{q}_{m}\left(n+1\right)}{\hat{q}_{m}\left(1\right)}$
always being an integer. Hence, we are left with the problem of checking
if all the evaluations of a polynomial $\hat{q}_{m}$ at integer points
are divisible by the same number $\hat{q}_{m}\left(1\right)$. The
solution to this type of question is well known, and it not hard to
show that this holds exactly when $\hat{q}_{m}\left(1\right)\mid\hat{q}_{m}\left(n+1\right)$
for $\deg\left(\hat{q}_{m}\right)+1$ consecutive integers (see \appref{integer-values}).
This suggest an induction like process to show that this holds for
all of the denominators, and in particular we will show it for the
matrix field of $\zeta\left(3\right)$.

\newpage{}

\section{\label{sec:The-z3-case}The $\zeta\left(3\right)$ case}

We now apply the dual matrix field identities for the $\zeta\left(3\right)$
matrix field. Recall that in this case we have that 
\begin{align*}
f\left(x,y\right) & =\frac{y^{3}-x^{3}}{y-x}\left(y+x\right)=y^{3}+2y^{2}x+2yx^{2}+x^{3}\\
\bar{f}\left(x,y\right) & =\frac{y^{3}+x^{3}}{y+x}\left(y-x\right)=y^{3}-2y^{2}x+2yx^{2}-x^{3}\\
a\left(x,y\right) & =x^{3}+\left(1+x\right)^{3}+2y\left(y-1\right)\left(2x+1\right)\\
b\left(x\right) & =-x^{6}.
\end{align*}

Our first goal is showing that $\gcd\left(q_{n}\left(m\right),p_{n}\left(m\right)\right)$
is large as $n$ and $m$ increase, and we will use the results from
\claimref{dual-field-identities}. Once we understand these polynomials
and their gcd, which are defined for each row separately, we will
combine them together to understand the general numerators and denominators
appearing in any route on the matrix field, starting at the bottom
left corner. In particular, investigating the diagonal route, we will
show that both the approximations converge fast enough, and the gcd
grows fast enough to conclude at the end that $\zeta\left(3\right)$
is irrational.

This $\zeta\left(3\right)$ matrix field has several properties making
it much easier to work with, which will come into play later:
\begin{fact}
\label{fact:zeta_3}
\begin{enumerate}
\item The matrix field is its own dual, since $f\left(y,x\right)=f\left(x,y\right)$
and $-\bar{f}\left(y,x\right)=\bar{f}\left(x,y\right)$. In particular
we get that $p=\hat{p}$ and $q=\hat{q}$.
\item We have that $f\left(0,0\right)=\bar{f}\left(0,0\right)=0$.
\item All the $f\left(n,0\right),f\left(0,n\right),\bar{f}\left(n,0\right),\bar{f}\left(0,n\right)$
are the same up to a sign (and therefore also $\hat{f}$ and $\bar{\hat{f}}$),
namely these are $n^{3}$. Furthermore, they all divide $f\left(n,n\right)=\hat{f}\left(n,n\right)=6n^{3}$.
\item The polynomial $a\left(x,y\right)$ can be written as $a\left(x,y\right)=A_{1}\left(x\right)+y\left(y-1\right)A_{2}\left(x\right)$,
so in particular $a\left(x,1-y\right)=a\left(x,y\right)$.
\end{enumerate}
\end{fact}

With the goal of finding out how big $\gcd\left(q_{n}\left(m\right),p_{n}\left(m\right)\right)$,
we show that both the numerators and denominators are almost divisible
by $\left(n!\right)^{3}$. We already know that fixing $m$ and only
increasing $n$, namely running on horizontal lines in the matrix
field, we get ``nice'' continued fractions which should have factorial
reduction. The next lemma shows that these factorial reduction are
in a sense synchronized between the different horizontal lines.

In the following, we will use \textbf{$\lcm\left[n\right]$ }for $\lcm\left\{ 1,2,...,n\right\} $
where $n\geq1$ and also set $\lcm\left[0\right]=1$.
\begin{lem}
\label{lem:q-n-lemma}For all $n\geq0$ and $m\in\ZZ$ we have $\left(n!\right)^{3}\mid q_{n}\left(m\right)$
(with equality for $m=1$) and $\left(\frac{n!}{\lcm\left[n\right]}\right)^{3}\mid p_{n}\left(m\right)$.
In particular we have that $\left(\frac{n!}{\lcm\left[n\right]}\right)^{3}\mid\gcd\left(p_{n}\left(m\right),q_{n}\left(m\right)\right)$.
\end{lem}

\begin{proof}
We will prove this claim by induction on $n$, but before that, we
first consider the case where $m=1$ and $n$ is arbitrary (the bottom
horizontal line). By part \ref{enu:first-line-polynomials} in \claimref{dual-field-identities}
we get that 
\begin{align*}
p_{n}\left(1\right) & =\sum_{k=1}^{n}\left(-1\right)^{k-1}\left(\prod_{i=1}^{k-1}\bar{f}\left(k,0\right)\right)\left(\prod_{i=k+1}^{n}f\left(k,0\right)\right)=\sum_{1}^{n}\left(\frac{n!}{k}\right)^{3}\\
q_{n}\left(1\right) & =\prod_{1}^{n}f\left(k,0\right)=\left(n!\right)^{3}.
\end{align*}
These are exactly the numerator and denominator of $\sum_{1}^{n}\frac{1}{k^{3}}$
if we just take the new denominator as the product of the denominators
in the summands. Since we can also instead take the least common multiple
of the denominators, we see that $\left(\frac{n!}{\lcm\left[n\right]}\right)^{3}\mid p_{n}\left(1\right)$
as required. Of course the $\left(n!\right)^{3}\mid q_{n}\left(1\right)$
is trivial since $\left(n!\right)^{3}=q_{n}\left(1\right)$, but more
over it allows us to think of the general conditions as $q_{n}\left(1\right)\mid q_{n}\left(m\right)$
and $q_{1}\left(n\right)\mid\lcm\left[n\right]^{3}p_{n}\left(m\right)$.\\

We prove the rest of this lemma using induction on $n$. The induction
hypothesis will go as follows - assuming that the claim is true for
$\left(n-1,m\right)$ for a given $n$ and all $m\in\ZZ$, we show:
\begin{enumerate}
\item From part \enuref{reciprocal-polynomials} in \claimref{dual-field-identities},
we show that the claim is true for $\left(n,m\right)$ with $1\leq m\leq n$.
\item From part \enuref{increase-m} in \claimref{dual-field-identities},
if the claim is true for $\left(n-1,n\right)$ and $\left(n,n\right)$,
then it is true for $\left(n,n+1\right)$.
\item Our polynomials satisfy $q_{n}\left(y\right)=q_{n}\left(1-y\right)$
and $p_{n}\left(y\right)=p_{n}\left(1-y\right)$, so the claim is
true for $\left(n,m\right)$ with $-n\leq m\leq n+1$, which are $2\left(n+1\right)$
consecutive integers.
\item These polynomials have degree $\leq2n+1$, so this is enough to show
the claim for $\left(n,m\right)$ for all $m$.
\end{enumerate}

When $n=0$ we have $q_{0}\left(m\right)\equiv1$ and $p_{0}\left(m\right)\equiv0$
which are divisible by $\left(0!\right)^{3}=1$ and $\frac{0!}{\lcm\left[0\right]}=1$
respectively.

Suppose now that the claim is true for $\left(k,m\right)$ with $k\leq n-1$
and all $m$ and we prove for $\left(n,m\right)$ and all $m$. We
prove first for the denominators, which is easier.\\

\textbf{\uline{Denominators:}}

By using identity \enuref{reciprocal-polynomials} from \claimref{dual-field-identities},
together with the facts in \factref{zeta_3} about the matrix field
we get 
\[
\left(\begin{smallmatrix}q_{m}\left(1\right) & p_{m}\left(1\right)\\
0 & q_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}q_{n}\left(1\right) & p_{n}\left(1\right)\\
0 & q_{n}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{m}\left(n+1\right)\\
q_{m}\left(n+1\right)
\end{smallmatrix}\right).
\]
For the denominators, this implies that 
\[
\frac{q_{n}\left(m+1\right)}{q_{n}\left(1\right)}=\frac{q_{m}\left(n+1\right)}{q_{m}\left(1\right)}.
\]
By the induction hypothesis, for $0\leq m\leq n-1$ the right hand
of this equation is an integer, so that $q_{n}\left(1\right)\mid q_{n}\left(m+1\right)$.
Using part \enuref{increase-m} in \claimref{dual-field-identities}
with $n=m$ we have

\begin{align*}
\left(\begin{smallmatrix}p_{n}\left(n+1\right)\\
q_{n}\left(n+1\right)
\end{smallmatrix}\right) & =\frac{1}{\left(f\bar{f}\right)\left(0,n\right)}\left(\begin{smallmatrix}f\left(0,n\right) & -1\\
0 & \bar{f}\left(0,n\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n-1}\left(n\right) & p_{n}\left(n\right)\\
q_{n-1}\left(n\right) & q_{n}\left(n\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\left(f\bar{f}\right)\left(n,0\right)\\
f\left(n,n\right)
\end{smallmatrix}\right)
\end{align*}
so for the denominators we get
\[
q_{n}\left(n+1\right)=q_{n-1}\left(n\right)f\left(n,0\right)\frac{\bar{f}\left(n,0\right)}{f\left(0,n\right)}+q_{n}\left(n\right)\frac{f\left(n,n\right)}{f\left(0,n\right)}=q_{n-1}\left(n\right)n^{3}\left(-1\right)+q_{n}\left(n\right)\cdot6.
\]
By the induction hypothesis $\left(n-1\right)!^{3}\mid q_{n-1}\left(n\right)$
and from the argument above $n!^{3}\mid q_{n}\left(n\right)$, so
we conclude that $\left(n!\right)^{3}\mid q_{n}\left(n+1\right)$,
so we conclude that $n!^{3}\mid q_{n}\left(n+1\right)$. At this point,
we know the claim for $\left(n,m\right)$ with $1\leq m\leq n+1$.

Using the fact that $a\left(x,y\right)$ can be written as $A_{1}\left(x\right)+y\left(y-1\right)A_{2}\left(x\right)$
, we get that $a\left(x,y\right)=a\left(x,1-y\right)$. Since
\begin{align*}
q_{n}\left(y\right) & =e_{1}^{tr}\left[\prod_{0}^{n-1}M_{X}\left(k,y\right)\right]e_{2}=e_{1}^{tr}\left[\prod_{0}^{n-1}\left(\begin{smallmatrix}0 & 1\\
b\left(k+1\right) & a\left(k,y\right)
\end{smallmatrix}\right)\right]e_{2}
\end{align*}
we also get that $q_{n}\left(1-y\right)=q_{n}\left(y\right)$ and
$\deg_{y}\left(q_{n}\right)\leq2n$. From this we conclude that $q_{n}\left(1\right)\mid q_{n}\left(m\right)$
for all $-n\leq m\leq n+1$, which is a total of $2n+2\geq\deg_{y}\left(q_{n}\right)+1$
consecutive integers. Finally, using \lemref{binomial-polynomials}
from \appref{integer-values} we conclude that $q_{n}\left(1\right)\mid q_{n}\left(m\right)$
for all $m$, thus proving the induction step for the denominators.\\
\newpage{}

\textbf{\uline{Numerators:}}

The proof for the numerators is similar, but needs a bit more computations.
Part \enuref{reciprocal-polynomials} from \claimref{dual-field-identities}
\[
q_{m}\left(1\right)p_{n}\left(m+1\right)+p_{m}\left(1\right)q_{n}\left(m+1\right)=q_{n}\left(1\right)p_{m}\left(n+1\right)+p_{n}\left(1\right)q_{m}\left(n+1\right)
\]
can be rewritten as 
\[
\frac{\lcm\left[n\right]^{3}p_{n}\left(m+1\right)}{q_{n}\left(1\right)}=\overbrace{\frac{\lcm\left[n\right]^{3}p_{m}\left(n+1\right)}{q_{m}\left(1\right)}}^{\left(1\right)}+\overbrace{\frac{\lcm\left[n\right]^{3}p_{n}\left(1\right)}{q_{n}\left(1\right)}}^{\left(2\right)}\cdot\overbrace{\frac{q_{m}\left(n+1\right)}{q_{m}\left(1\right)}}^{\left(3\right)}-\overbrace{\frac{\lcm\left[n\right]^{3}p_{m}\left(1\right)}{q_{m}\left(1\right)}}^{\left(4\right)}\cdot\overbrace{\frac{q_{n}\left(m+1\right)}{q_{n}\left(1\right)}}^{\left(5\right)}.
\]
To show that the expression on the left is an integer, it is enough
to show that $\left(1\right)-\left(5\right)$ on the right are integers. 
\begin{itemize}
\item Expression $\left(2\right)$ is on the first row of the matrix field,
and we saw in the beginning of the proof that it is an integer.
\item Expressions $\left(3\right)$ and $\left(5\right)$ follows from the
claim about the denominators (which is independent of this proof about
the numerators).
\item Expressions $\left(1\right)$ and $\left(4\right)$ are true if $0\leq m\leq n-1$
using the induction hypothesis, and the fact that $\lcm\left[m\right]\mid\lcm\left[n\right]$
in that case.
\end{itemize}
To conclude, we just saw that the claim is true for $\left(n,m\right)$
when $1\leq m\leq n$.

Using part \enuref{increase-m} in \claimref{dual-field-identities}
with $n=m$ for the numerators we get 
\begin{align*}
p_{n}\left(n+1\right) & =\frac{1}{\left(f\bar{f}\right)\left(0,n\right)}\left[f\left(0,n\right)\left(\left(f\bar{f}\right)\left(n,0\right)p_{n-1}\left(n\right)+f\left(n,n\right)p_{n}\left(n\right)\right)-\left(\left(f\bar{f}\right)\left(n,0\right)q_{n-1}\left(n\right)+f\left(n,n\right)q_{n}\left(n\right)\right)\right]\\
 & =\left(-n^{3}p_{n-1}\left(n\right)+6p_{n}\left(n\right)\right)+\left(q_{n-1}\left(n\right)-\frac{6}{n^{3}}q_{n}\left(n\right)\right).
\end{align*}
Since the claim is true for $\left(n-1,n\right)$ and $\left(n,n\right)$,
we get that
\begin{align*}
\left(\frac{n!}{\lcm\left[n\right]}\right)^{3} & \mid\left(-n^{3}p_{n-1}\left(n\right)+6p_{n}\left(n\right)\right)\\
\left(n-1\right)!^{3} & \mid\left(q_{n-1}\left(n\right)-\frac{6}{n^{3}}q_{n}\left(n\right)\right).
\end{align*}
Since $n\mid\lcm\left[n\right]$ , it follows that $\frac{n!}{\lcm\left[n\right]}\mid\left(n-1\right)!$,
so everything together shows that 
\[
\left(\frac{n!}{\lcm\left[n\right]}\right)^{3}\mid p_{n}\left(n+1\right).
\]
At this point we know that $\left(\frac{n!}{\lcm\left[n\right]}\right)^{3}\mid p_{n}\left(m\right)$
for $1\leq m\leq n+1$. The same trick as with the denominators show
that $p_{n}\left(m\right)=p_{n}\left(1-m\right)$, so the claim is
true for $-n\leq m\leq n+1$, and using \lemref{binomial-polynomials}
again we conclude that it is true for all $m$, thus finishing the
proof for the induction step, and therefore the original claim.
\end{proof}
Up until now we looked at each row separately. We now move to the
whole matrix field.
\begin{defn}
Given $n\geq0$ and $m\geq1$, define
\[
\left(\begin{smallmatrix}P\left(n,m\right)\\
Q\left(n,m\right)
\end{smallmatrix}\right):=\left[\prod_{k=1}^{m-1}M_{Y}\left(0,k\right)\right]\left[\prod_{k=0}^{n-1}M_{X}\left(k,m\right)\right]e_{2}.
\]
In particular, as Mobius transformations we get that 
\[
\left[\prod_{k=1}^{m-1}M_{Y}\left(0,k\right)\right]\left[\prod_{k=0}^{n-1}M_{X}\left(k,m\right)\right]\left(0\right)=\frac{P\left(n,m\right)}{Q\left(n,m\right)}.
\]
\end{defn}

\begin{rem}
Note that for the general matrix field with $\bar{f}\left(0,0\right)=0$,
we can use \enuref{reciprocal-polynomials} in \claimref{dual-field-identities}
to get 
\[
\left(\begin{smallmatrix}P\left(n,m\right)\\
Q\left(n,m\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}\prod_{k=1}^{m}\bar{f}\left(0,k\right) & \hat{p}_{m}\left(1\right)\\
0 & \hat{q}_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}\left(-1\right)^{n}\prod_{k=1}^{n}\bar{f}\left(k,0\right) & p_{n}\left(1\right)\\
0 & q_{n}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\hat{p}_{m}\left(n+1\right)\\
\hat{q}_{m}\left(n+1\right)
\end{smallmatrix}\right).
\]
In particular, in our $\zeta\left(3\right)$ case we have 
\[
\left(\begin{smallmatrix}P\left(n,m\right)\\
Q\left(n,m\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}q_{m}\left(1\right) & p_{m}\left(1\right)\\
0 & q_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}q_{n}\left(1\right) & p_{n}\left(1\right)\\
0 & q_{n}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{m}\left(n+1\right)\\
q_{m}\left(n+1\right)
\end{smallmatrix}\right).
\]
\end{rem}

With this new notation, we have the new factorial reduction for these
numerators and denominators.
\begin{cor}
\label{cor:factorial-reduction}For all $n,m\geq0$ we have that 
\[
q_{m}\left(1\right)q_{n}\left(1\right)\mid Q\left(n,m+1\right),
\]
\begin{align*}
\frac{q_{m}\left(1\right)q_{n}\left(1\right)}{\lcm\left[\max\left(m,n\right)\right]^{3}} & \mid\gcd\left(P\left(n,m+1\right),Q\left(n,m+1\right)\right).
\end{align*}
In particular for $n=m$ we get that 
\begin{align*}
\left(\frac{n!}{\lcm\left[n\right]}\cdot n!\right)^{3}=\frac{\left(q_{n}\left(1\right)\right)^{2}}{\lcm\left[n\right]^{3}} & \mid\gcd\left(P\left(n,n+1\right),Q\left(n,n+1\right)\right).
\end{align*}
\end{cor}

\begin{proof}
Using the presentation from the remark above
\[
\left(\begin{smallmatrix}P\left(n,m+1\right)\\
Q\left(n,m+1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}q_{m}\left(1\right) & p_{m}\left(1\right)\\
0 & q_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right),
\]
and \lemref{q-n-lemma} we get that 
\begin{align*}
q_{m}\left(1\right)q_{n}\left(1\right) & \mid q_{m}\left(1\right)q_{n}\left(m+1\right)=Q\left(n,m+1\right)\\
\frac{q_{m}\left(1\right)q_{n}\left(1\right)}{\lcm\left[\max\left(m,n\right)\right]^{3}} & \mid q_{m}\left(1\right)p_{n}\left(m+1\right)+p_{m}\left(1\right)q_{n}\left(m+1\right)=P\left(n,m+1\right).
\end{align*}
\end{proof}
This factorial reduction property, will help us in the end to show
that $\zeta\left(3\right)$ is irrational, but it can also be used
to show more general properties of the matrix field, as follows.
\begin{thm}
\label{thm:all-directions-converge}Let $n_{i},m_{i}\geq1$ be any
sequence such that $\max\left(n_{i},m_{i}\right)\to\infty$. Then
$\frac{P\left(n_{i},m_{i}\right)}{Q\left(n_{i},m_{i}\right)}\to\zeta\left(3\right)$.
\end{thm}

Before we prove this theorem, here is an interesting corollary for
using this theorem for fixed $m$.
\begin{cor}
For any $m\geq1$, the limit for the $Y=m$ line is$\limfi n\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}=\sum_{m}^{\infty}\frac{1}{n^{3}}.$
\end{cor}

\begin{proof}
Using the notation $\left(\begin{smallmatrix}P\left(n,m-1\right)\\
Q\left(n,m-1\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}q_{m-1}\left(1\right) & p_{m-1}\left(1\right)\\
0 & q_{m-1}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m\right)\\
q_{n}\left(m\right)
\end{smallmatrix}\right)$, we get that 
\[
\frac{P\left(n,m-1\right)}{Q\left(n,m-1\right)}=\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}+\frac{p_{m-1}\left(1\right)}{q_{m-1}\left(1\right)}.
\]
By \thmref{all-directions-converge} we know that $\limfi n\frac{P\left(n,m-1\right)}{Q\left(n,m-1\right)}=\zeta\left(3\right)$,
and we have already seen that $\frac{p_{m-1}\left(1\right)}{q_{m-1}\left(1\right)}=\sum_{1}^{m-1}\frac{1}{n^{3}}$,
so together we get that $\limfi n\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}=\sum_{m}^{\infty}\frac{1}{n^{3}}$.
\end{proof}
And now for the proof of the theorem.
\begin{proof}[Proof of \thmref{all-directions-converge}]
\textbf{\uline{The \mbox{$m_{i}$} bounded case}}\textbf{: }Suppose
first that $m_{i}$ is bounded, and by splitting the sequence to finitely
many subsequence, we may assume that $m_{i}=m$ is constant. We use
the presentation 
\[
\left(\begin{smallmatrix}P\left(n_{i},m\right)\\
Q\left(n_{i},m\right)
\end{smallmatrix}\right):=\left(\begin{smallmatrix}q_{n_{i}}\left(1\right) & p_{n_{i}}\left(1\right)\\
0 & q_{n_{i}}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\hat{p}_{m}\left(n_{i}+1\right)\\
\hat{q}_{m}\left(n_{i}+1\right)
\end{smallmatrix}\right)
\]
so that 
\[
\frac{P\left(n_{i},m\right)}{Q\left(n_{i},m\right)}=\frac{p_{m}\left(n_{i}+1\right)}{q_{m}\left(n_{i}+1\right)}+\frac{p_{n_{i}}\left(1\right)}{q_{n_{i}}\left(1\right)}.
\]

We already know that $\limfi n\frac{p_{n}\left(1\right)}{q_{n}\left(1\right)}=\zeta\left(3\right)$,
so if we can show that $\deg\left(q_{m}\right)>\deg\left(p_{m}\right)$,
then $\limfi n\frac{p_{m}\left(n\right)}{q_{m}\left(n\right)}=0$.
Indeed, recall that 
\[
\left(\begin{smallmatrix}p_{m}\left(y\right)\\
q_{m}\left(y\right)
\end{smallmatrix}\right)=\left[\prod_{0}^{m-1}M_{X}\left(k,y\right)\right]e_{2}
\]
where $M_{X}\left(x,y\right)=\left(\begin{smallmatrix}0 & 1\\
b\left(x+1\right) & a\left(x,y\right)
\end{smallmatrix}\right)$, and 
\[
a\left(x,y\right)=x^{3}+\left(1+x\right)^{3}+2y\left(y-1\right)\left(2x+1\right).
\]
This means that for every $k\geq0$ we have that $\deg_{y}\left(a\left(k,y\right)\right)=2$,
and by induction $\deg\left(p_{m}\right)=2m-2$ while $\deg\left(q_{m}\right)=2m$.
Hence $\deg\left(q_{m}\right)>\deg\left(p_{m}\right)$ and we are
done.\\

\textbf{\uline{The \mbox{$m_{i}$} unbounded case}}\textbf{:} Here
we will use the second presentation of $P$ and $Q$, namely
\[
\left(\begin{smallmatrix}P\left(n,m\right)\\
Q\left(n,m\right)
\end{smallmatrix}\right)=\left(\begin{smallmatrix}q_{m}\left(1\right) & p_{m}\left(1\right)\\
0 & q_{m}\left(1\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}p_{n}\left(m+1\right)\\
q_{n}\left(m+1\right)
\end{smallmatrix}\right)
\]
and therefore
\[
\frac{P\left(n,m\right)}{Q\left(n,m\right)}=\frac{p_{n}\left(m+1\right)}{q_{n}\left(m+1\right)}+\frac{p_{m}\left(1\right)}{q_{m}\left(1\right)}.
\]

Fix some $\varepsilon>0$. Since $\limfi m\frac{p_{m}\left(1\right)}{q_{m}\left(1\right)}=\zeta\left(3\right)$,
for all $m$ large enough we have $\left|\frac{p_{m}\left(1\right)}{q_{m}\left(1\right)}-\zeta\left(3\right)\right|\leq\frac{\varepsilon}{2}$.
As we shall see below, for all $m$ large enough we also have that
$\left|\frac{p_{n}\left(m+1\right)}{q_{n}\left(m+1\right)}\right|\leq\frac{\varepsilon}{2}$
independent of $n$. Hence, we can find $M=M_{\varepsilon}$ so that
for $m\geq M_{\varepsilon}$ we have $\left|\frac{P\left(n,m\right)}{Q\left(n,m\right)}-\zeta\left(3\right)\right|\leq\varepsilon$.
Thus, if we look at the two subsequence of $\left(n_{i},m_{i}\right)$
, where $m_{i}\geq M_{\varepsilon}$ and where $m_{i}\leq M_{\varepsilon}$,
we get that $\left|\frac{P\left(n_{i},m_{i}\right)}{Q\left(n_{i},m_{i}\right)}-\zeta\left(3\right)\right|\leq\varepsilon$
on the first subsequence, and from the previous case, if the second
subsequence is infinite, so that $n_{i}\to\infty$, we have $\left|\frac{P\left(n_{i},m_{i}\right)}{Q\left(n_{i},m_{i}\right)}-\zeta\left(3\right)\right|\leq\varepsilon$
for all $i$ large enough.

We are left to show that $\left|\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}\right|\leq\frac{\varepsilon}{2}$
for all $m$ large enough (independent of $n\geq0$). 

Note that the result in the corollary above that $\limfi n\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}=\sum_{m}^{\infty}\frac{1}{n^{3}}$
only depends on the $m$ bounded case, so we already fully proved
it. This is a tail of a convergent series, so it will be small for
all large enough $m$. With this motivation (without the result itself
from the corollary) we show that as $m$ increases , $\left|\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}\right|$
becomes bounded by similar such tail and therefore is as small as
we want.

Recall that 
\[
\left(\begin{smallmatrix}p_{n-1}\left(y\right) & p_{n}\left(y\right)\\
q_{n-1}\left(y\right) & q_{n}\left(y\right)
\end{smallmatrix}\right)D_{b_{X}\left(n\right)}=\prod_{0}^{n-1}M_{X}\left(k,y\right),
\]
so taking the determinant, we get that 
\[
\left(p_{n-1}\left(y\right)q_{n}\left(y\right)-p_{n}\left(y\right)q_{n-1}\left(y\right)\right)b\left(n\right)=\prod_{1}^{n}\left(-b\left(k\right)\right),
\]
which we can rewrite as
\[
\frac{p_{n}\left(y\right)}{q_{n}\left(y\right)}=\frac{p_{n-1}\left(y\right)}{q_{n-1}\left(y\right)}-\frac{\left(-1\right)^{n}\prod_{1}^{n-1}b\left(k\right)}{q_{n-1}\left(y\right)q_{n}\left(y\right)}.
\]

Using the fact that $p_{0}\left(y\right)=0$ , $q_{0}\left(y\right)=1$
and $\prod_{1}^{j-1}\left|b\left(k\right)\right|=\left(\left(j-1\right)!\right)^{6}=\left|q_{j-1}\left(1\right)\right|^{2}$,
we get that 
\[
\left|\frac{p_{n}\left(y\right)}{q_{n}\left(y\right)}\right|=\left|\sum_{j=1}^{n}\frac{\left(-1\right)^{j}\prod_{1}^{j-1}b\left(k\right)}{q_{j-1}\left(y\right)q_{j}\left(y\right)}\right|\leq\sum_{j=1}^{\infty}\left|\frac{\prod_{1}^{j-1}b\left(k\right)}{q_{j-1}\left(y\right)q_{j}\left(y\right)}\right|.
\]
By \lemref{q-n-lemma} we have that $\prod_{1}^{n-1}b\left(k\right)=\left(n-1\right)!^{6}=q_{n-1}\left(1\right)^{2}$,
and also $q_{n-1}\left(1\right)\mid q_{n-1}\left(m\right)$ and $q_{n-1}\left(1\right)n^{3}=q_{n}\left(n\right)\mid q_{n}\left(m\right)$
so that 
\[
\left|\frac{\prod_{1}^{j-1}b\left(k\right)}{q_{j-1}\left(m\right)q_{j}\left(m\right)}\right|=\left|\frac{q_{j-1}\left(1\right)}{q_{j-1}\left(m\right)}\cdot\frac{q_{j-1}\left(1\right)j^{3}}{q_{j}\left(m\right)}\cdot\frac{1}{j^{3}}\right|\leq\frac{1}{j^{3}}.
\]
This already shows that $\left|\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}\right|\leq\sum_{j=1}^{\infty}\frac{1}{j^{3}}$,
which is of course not enough, as instead of the tail, we got the
full sum. To solve this, we note that each one of the $q_{j}\left(y\right)$
for fixed $j\geq1$ are nonconstant polynomials of $y$ (of degree
$2j$) so that $\limfi y\left|\frac{q_{j-1}\left(1\right)}{q_{j-1}\left(y\right)}\cdot\frac{q_{j-1}\left(1\right)}{q_{j}\left(y\right)}\right|=0$.
Fixing $\varepsilon>0$ and $N>0$, we can find $M=M_{\varepsilon,N}$
large enough such that $\left|\frac{q_{j-1}\left(1\right)}{q_{j-1}\left(y\right)}\cdot\frac{q_{j-1}\left(1\right)}{q_{j}\left(y\right)}\right|<\frac{\varepsilon}{N}$
for all $y>M$ and $1\leq j<N$. In particular, for any such $y>M$
we have that 
\[
\left|\frac{p_{n}\left(m\right)}{q_{n}\left(m\right)}\right|\leq\sum_{j=1}^{\infty}\left|\frac{q_{j-1}\left(1\right)}{q_{j-1}\left(m\right)}\cdot\frac{q_{j-1}\left(1\right)}{q_{j}\left(m\right)}\right|\leq\frac{\varepsilon}{N}N+\sum_{j=N}^{\infty}\left|\frac{q_{j-1}\left(1\right)}{q_{j-1}\left(m\right)}\cdot\frac{q_{j-1}\left(1\right)}{q_{j}\left(m\right)}\right|\leq\varepsilon+\sum_{j=N}^{\infty}\frac{1}{j^{3}}.
\]
Since $\sum_{1}^{\infty}\frac{1}{j^{3}}<\infty$ converges, we can
find $N$ large enough so that $\sum_{N}^{\infty}\frac{1}{j^{3}}\leq\varepsilon$
also, so together we get that for all $y$ big enough (independent
of $n$) we have $\left|\frac{p_{n}\left(y\right)}{q_{n}\left(y\right)}\right|\leq2\varepsilon$
which is what we wanted to prove.
\end{proof}
Finally, we combine all of the results to show that $\zeta\left(3\right)$
is irrational.
\begin{thm}
\label{thm:zeta-3-irrational}The number $\zeta\left(3\right)$ is
irrational.
\end{thm}

\begin{proof}
Consider the diagonal direction on the $\zeta\left(3\right)$ matrix
field where $m=n+1$. From \thmref{all-directions-converge} we have
that 
\[
\limfi n\frac{P\left(n,n+1\right)}{Q\left(n,n+1\right)}=\zeta\left(3\right).
\]

\textbf{\uline{The main idea:}}

Let us denote $Q_{n}=Q\left(n,n+1\right),\;P_{n}=P\left(n,n+1\right)$
and $\tilde{Q}_{n}=\frac{Q_{n}}{gcd\left(Q_{n},P_{n}\right)}$, $\tilde{P}_{n}=\frac{P_{n}}{gcd\left(Q_{n},P_{n}\right)}$
so that $\limfi n\frac{P_{n}}{Q_{n}}=\limfi n\frac{\tilde{P}_{n}}{\tilde{Q}_{n}}=\zeta\left(3\right)$.
Recall that in \corref{first-rationality-test} we showed that if
$\frac{p_{n}}{q_{n}}\to L$ is any convergent rational sequence, and
$\left|L-\frac{p_{n}}{q_{n}}\right|\left|q_{n}\right|=o\left(1\right)$,
then $L$ is irrational. Hence, our goal is to show that $\frac{\tilde{P}_{n}}{\tilde{Q}_{n}}$
converge fast enough to $\zeta\left(3\right)$ to apply this result,
and conclude that $\zeta\left(3\right)$ is irrational.

More specifically, setting $\lambda_{+}=\left(1+\sqrt{2}\right)^{4}$,
we will first show that given any $\varepsilon>0$ the approximation
error is 
\[
\left|\zeta\left(3\right)-\frac{P_{n}}{Q_{n}}\right|=O\left(\frac{1}{\left(\lambda_{+}-\varepsilon\right)^{2n}}\right).
\]
On the other hand, we will show that $Q_{n}=O\left(\left(n!\right)^{6}\left(\lambda_{+}+\varepsilon\right)^{n}\right),$
and by \corref{factorial-reduction} we know that 
\[
\left(\frac{n!}{\lcm\left[n\right]}\cdot n!\right)^{3}\mid\gcd\left(P\left(n,n+1\right),Q\left(n,n+1\right)\right),
\]
so that $\tilde{Q}_{n}=O\left(\lcm\left[n\right]^{3}\left(\lambda_{+}+\varepsilon\right)^{n}\right)$.
It is well known that $\lcm\left[n\right]=O\left(\left(e+\varepsilon\right)^{n}\right)$
(it follows from the prime number theorem, see \cite{apostol_introduction_1998}),
so together we get that 
\[
\left|\zeta\left(3\right)-\frac{\tilde{P}_{n}}{\tilde{Q}_{n}}\right|\left|\tilde{Q}_{n}\right|=O\left(\left(\frac{\left(e+\varepsilon\right)^{3}\left(\lambda_{+}+\varepsilon\right)}{\left(\lambda_{+}-\varepsilon\right)^{2}}\right)^{n}\right).
\]
Since $20.08\sim e^{3}<\lambda_{+}=\left(1+\sqrt{2}\right)^{4}\sim33.97$,
for all $\varepsilon>0$ small enough we get that $\frac{\left(e+\varepsilon\right)^{3}\left(\lambda_{+}+\varepsilon\right)}{\left(\lambda_{+}-\varepsilon\right)^{2}}<1$.
Hence $\left|\zeta\left(3\right)-\frac{\tilde{P}_{n}}{\tilde{Q}_{n}}\right|\left|\tilde{Q}_{n}\right|=o\left(1\right)$
thus proving that $\zeta\left(3\right)$ is irrational.\\

\textbf{\uline{Step 1: Find recursion relation for \mbox{$Q_{n}$}:}}

With this main idea, we are left to find the growth rate of $Q_{n}$
and how fast $\left|\zeta\left(3\right)-\frac{P_{n}}{Q_{n}}\right|$
goes to zero. 

Using the coboundary condition on the matrix field, we get that
\begin{align*}
\left(\begin{smallmatrix}P\left(n,n+1\right)\\
Q\left(n,n+1\right)
\end{smallmatrix}\right) & =\left[\prod_{k=1}^{n}M_{Y}\left(0,k\right)\right]\left[\prod_{k=0}^{n-1}M_{X}\left(k,n+1\right)\right]e_{2}\\
 & =\left[\prod_{k=1}^{n}M_{X}\left(k-1,k\right)M_{Y}\left(k,k\right)\right]e_{2},
\end{align*}
where 
\begin{align*}
M_{X}\left(k-1,k\right)M_{Y}\left(k,k\right) & =\left(\begin{smallmatrix}0 & 1\\
b\left(k\right) & f\left(k-1,k\right)-\bar{f}\left(k,k\right)
\end{smallmatrix}\right)\left(\begin{smallmatrix}\bar{f}\left(k,k\right) & 1\\
b\left(k\right) & f\left(k,k\right)
\end{smallmatrix}\right).\\
 & =\left(\begin{smallmatrix}b\left(k\right) & f\left(k,k\right)\\
f\left(k-1,k\right)b\left(k\right)\; & \left(f\bar{f}\right)\left(k,0\right)+f\left(k,k\right)f\left(k-1,k\right)-\left(f\bar{f}\right)\left(k,k\right)
\end{smallmatrix}\right)\\
 & =\left(\begin{smallmatrix}-k^{6} & 6k^{3}\\
-f\left(k-1,k\right)k^{6}\; & 6k^{3}f\left(k-1,k\right)-k^{6}
\end{smallmatrix}\right)\\
 & =k^{3}\left(\begin{smallmatrix}-k^{3} & 6\\
-f\left(k-1,k\right)k^{3}\; & 6f\left(k-1,k\right)-k^{3}
\end{smallmatrix}\right).
\end{align*}
We want to use \thmref{to-gcf} to find the recurrence that $P_{n}$
and $Q_{n}$ satisfy, however in that theorem we looked on a product
of matrices applied to $e_{1}$ and not $e_{2}$. To fix this, recall
that $\tau=\left(\begin{smallmatrix}0 & 1\\
1 & 0
\end{smallmatrix}\right)$ is the row\textbackslash column switching matrix so that 
\begin{align*}
\left(\begin{smallmatrix}Q_{n}\\
P_{n}
\end{smallmatrix}\right) & =\tau\left(\begin{smallmatrix}P_{n}\\
Q_{n}
\end{smallmatrix}\right)=\left[\prod_{k=1}^{n}\tau M_{X}\left(k-1,k\right)M_{Y}\left(k,k\right)\tau\right]\tau e_{2}=\left(n!\right)^{3}\left[\prod_{k=1}^{n}M\left(k\right)\right]e_{1}
\end{align*}
where 
\begin{align*}
M\left(k\right) & =\left(\begin{smallmatrix}6f\left(k-1,k\right)-k^{3}\; & -f\left(k-1,k\right)k^{3}\\
6 & -k^{3}
\end{smallmatrix}\right)\\
\det\left(M\left(k\right)\right) & =k^{6}.
\end{align*}
Applying now \thmref{to-gcf} we get that $u_{n}=\frac{Q_{n}}{\left(n!\right)^{3}}$
satisfy the relation

\begin{align*}
u_{n+1} & =u_{n}\left(6f\left(n,n+1\right)-\left(n+1\right)^{3}-n^{3}\right)-u_{n-1}n^{6},
\end{align*}
where $u_{0}=\frac{Q_{0}}{0!^{3}}=1$ and $v_{1}=\frac{Q_{1}}{1!^{3}}=6f\left(0,1\right)-1=5$.
Denote
\begin{align*}
F\left(n\right) & =6f\left(n,n+1\right)-\left(n+1\right)^{3}-n^{3}=34n^{3}+51n^{2}+27n+5,
\end{align*}
so the recurrence can be written as $u_{n+1}=F\left(n\right)u_{n}-n^{6}u_{n-1}$.

It is also interesting to note that $\frac{P_{n}}{\left(n!\right)^{3}}$
satisfies the same recurrence and $\frac{P_{0}}{\left(0!\right)^{3}}=0,\;\frac{P_{1}}{\left(1!\right)^{3}}=6$,
so that
\[
\zeta\left(3\right)=\limfi n\frac{P_{n}}{Q_{n}}=\left(\begin{smallmatrix}0 & 6\\
1 & 5
\end{smallmatrix}\right)\left[\prod_{1}^{n}\left(\begin{smallmatrix}0 & -k^{6}\\
1 & F\left(k\right)
\end{smallmatrix}\right)\right]\left(0\right)=\left(\begin{smallmatrix}0 & 6\\
1 & 5
\end{smallmatrix}\right)\left(\KK_{1}^{\infty}\frac{-k^{6}}{F\left(k\right)}\right),
\]
or alternatively
\[
\frac{6}{\zeta\left(3\right)}-5=\KK_{1}^{\infty}\frac{-k^{6}}{F\left(k\right)}.
\]
\\

\textbf{\uline{Step 2: Analyze the recurrence to find the growth
rate of \mbox{$Q_{n}$}:}}

By \corref{factorial-reduction} we know that $\left(n!\right)^{6}\mid Q_{n}$,
so that $v_{n}=\frac{Q_{n}}{\left(n!\right)^{6}}=\frac{u_{n}}{\left(n!\right)^{3}}$
are integers which satisfy
\[
v_{n+1}\left(n+1\right)^{3}=F\left(n\right)v_{n}-n^{3}v_{n-1},
\]
where $v_{0}=\frac{Q_{0}}{0!^{6}}=1$ and $v_{1}=\frac{Q_{1}}{1!^{6}}=5$.
Equivalently, we can write 
\[
v_{n+1}=\frac{F\left(n\right)}{n^{3}}v_{n}-\frac{n^{3}}{\left(1+n\right)^{3}}v_{n-1}.
\]
Taking the limit only for the coefficients, we get the recurrence
$v_{n+1}=34v_{n}-v_{n-1}$. This correspons to the quadratic equation
$x^{2}-34x+1=0$ with the roots 
\[
\lambda_{\pm}=\frac{34\pm\sqrt{1156-4}}{2}=\frac{34\pm24\sqrt{2}}{2}=17\pm12\sqrt{2}=\left(1\pm\sqrt{2}\right)^{4},
\]
so a standard computation shows that $\frac{v_{n}}{v_{n-1}}\to\lambda_{+}=\left(1+\sqrt{2}\right)^{4}$.
In the original recurrence with the nonconstant coefficients, the
same holds, but needs a bit more explanation. As with the standard
recurrence with constant coefficients, we expect the general solution
to behave like $v_{n}\sim\lambda_{+}^{n}$, though there is a specific
starting position for which $v_{n}\sim\lambda_{-}^{n}$. Since $\lambda_{-}=\left(1-\sqrt{2}\right)^{4}\sim0.03$
, this is highly unlikely to happen, since we deal with integer values.
More sepcifically, the first few elements in $v_{i}$ are $1,5,73,1445,33001,...$
which is an increasing sequence of positive integers, and since $\frac{F\left(n\right)}{n^{3}}\geq11$
for $n\geq3$, it is not hard to show by induction that 
\[
v_{n+1}=\frac{F\left(n\right)}{n^{3}}v_{n}-\frac{n^{3}}{\left(1+n\right)^{3}}v_{n-1}\geq11v_{n}-v_{n-1}\geq10v_{n},
\]
so at least we get that $v_{n}\geq10^{n}$ grows much faster than
the very special case of $\lambda_{-}^{n}$. This is enough to show
that for every $\varepsilon>0$ and for any $n$ large enough, we
have
\[
\left(\lambda_{+}-\varepsilon\right)^{n}\leq v_{n}\leq\left(\lambda_{+}+\varepsilon\right)^{n}.
\]
For the reader's conveneince, we add a full proof in \appref{Asymptotics-of-recurrence}.\\

\textbf{\uline{Step 3: Analyze the approximation error:}}

The sequence of $\frac{P_{n}}{\left(n!\right)^{3}},u_{n}=\frac{Q_{n}}{\left(n!\right)^{3}}$
are the numerators and denominators of the continued fraction $\KK_{1}^{\infty}\frac{-n^{6}}{F\left(n\right)}$.
Using \claimref{upper-bound} we get that for all $n$ large enough
\[
\left|\zeta\left(3\right)-\frac{P_{n}}{Q_{n}}\right|\leq\sum_{k=n}^{\infty}\frac{\left(k!\right)^{6}}{\left|u_{k}u_{k+1}\right|}=\sum_{k=n}^{\infty}\frac{1}{\left(k+1\right)^{3}\left|v_{k}v_{k+1}\right|}\leq\sum_{n}^{\infty}\frac{1}{\left(\lambda_{+}-\varepsilon\right)^{2k+1}}=O\left(\frac{1}{\left(\lambda_{+}-\varepsilon\right)^{2n}}\right).
\]

These are the growth rate for $Q_{n}=\left(n!\right)^{6}v_{n}$ and
the error for $\left|\zeta\left(3\right)-\frac{P_{n}}{Q_{n}}\right|$
that we needed in the beginning, thus completing the proof.
\end{proof}

\newpage{}

\section{\label{sec:On-future-fractions}On future fractions}

The main goal of this paper was to introduce this new mathematical
object of conservative matrix field, and as an application use it
to reprove Apery's result about the irrationality of $\zeta\left(3\right)$.
As can be seen in \secref{The-z3-case}, the final proof as it is
right now is very specific to matrix field of $\zeta\left(3\right)$,
which has several nice properties, and doesn't hold for other examples
of matrix fields. However it might be possible that some of the results
hold in a more general setting.

While this irrationality result is already interesting by itself,
the conservative matrix field object also seems to have many interesting
properties. Among others, it is a natural generalization of quadratic
equations, and it involves a bit of noncommutative cohomology theory
in the form of cocycles and coboundaries.

So far, the conservative matrix fields that we managed to find where
$f,\bar{f}$ are polynomials of degree 4 or more seem to always be
degenerate, namely $a\left(x,y\right)=f\left(x,y\right)-\bar{f}\left(x+1,y\right)$
doesn't depend on $y$. This might be related to the fact that we
work over $2\times2$ matrix, which might bound the possible matrix
fields. Whether this is the case or not, this leads to several possible
interesting generalizations of this theory, which are standard in
the theory of continued fractions.
\begin{enumerate}
\item While many of the results mentioned in this paper are true for general
continued fractions over $\CC$ (and even other fields), the irrationality
of $\zeta\left(3\right)$ relied heavily on the fact that the defining
polynomials $f,\bar{f}$ were in $\ZZ\left[x,y\right]$. This leads
naturally to the question of what happens when we use other integer
rings in algebraic extensions, e.g. $\ZZ\left[i\right]$ in $\QQ\left[i\right]$.
Both in the $\zeta\left(2\right)$ and $\zeta\left(3\right)$ matrix
fields case we can find in the background algebraic numbers of degree
$2$ (namely $1+i$ and $\zeta_{3}=e^{\frac{2\pi}{3}i}$ respectively).
This type of field extension, with the right definition of generalized
continued fraction might add many more interesting examples.
\item In the proof of the irrationality of $\zeta\left(3\right)$ we had
two main results that we needed to show. One was to find the error
rate and how fast it converges to zero, and the second was to find
$gcd\left(P_{n},Q_{n}\right)$ and hope that it grows to infinity
fast enough. As it is usually the case in number theoretic problems,
the first result lives in the standard Euclidean geometry, where we
needed to show that some sequence goes to zero in the $\left|\cdot\right|_{\infty}$
norm, and the second result can be seen as showing that the $p$-adic
norms of $\left|gcd\left(P_{n},Q_{n}\right)\right|_{p}$ all go to
zero as well. This suggest a more general approach where the matrix
field lives over the Adeles, and the convergence in the real and $p$-adic
places together prove irrationality.
\item All the results in this paper were for $2\times2$ matrices, and a
natural generalization would be by going to a higher dimension matrices.
There are many suggestions for what should be the generalization of
continued fractions to higher dimensions, however probably one of
the best approaches is to change the language all together from continued
fractions to lattices in $\RR^{d}$. The subject of lattices is well
studied in the literature with many connections to other subjects.
With this approach, the question should be what is the right way to
formulate the results about general continued fraction as results
on lattices, and what can we say in higher dimension.
\end{enumerate}
These three types of generalization of changing the field, the norm,
or the dimension, can also be combined. Of course, there are more
tools available already in ``standard'' $2\times2$ matrices over
the integers to study polynomial continued fraction. However, it seems
that the conservative matrix field holds some interesting structure
which might reveal itself to be very useful not only to prove results
about continued fractions, but to other subjects as well.

\newpage{}

\part{Appendix}

\appendix

\section{\label{app:Identifying-polynomial-continued}Identifying polynomial
continued fractions in the Euler family}

Recall from \exaref{Euler-family} that a continued fraction $\KK_{1}^{\infty}\frac{b\left(i\right)}{a\left(i\right)}$
is in the trivial Euler family if is has the form
\begin{align*}
b\left(x\right) & =-h_{1}\left(x\right)h_{2}\left(x\right)\\
a\left(x\right) & =h_{1}\left(x\right)+h_{2}\left(x+1\right),
\end{align*}
in which case we have that 
\[
\KK_{1}^{n}\frac{b\left(i\right)}{a\left(i\right)}=h_{2}\left(1\right)\left(\frac{1}{\sum_{k=0}^{n}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)}-1\right).
\]

If $a,b\in\CC\left[x\right]$, then in order to find polynomial solution
$h_{1},h_{2}\in\CC\left[x\right]$, we only need to know how to decompose
$b\left(x\right)$ to product of polynomial. In the more general case,
we had

\begin{align*}
b\left(x\right) & =-h_{1}\left(x\right)h_{2}\left(x\right)\\
f\left(x\right)a\left(x\right) & =f\left(x-1\right)h_{1}\left(x\right)+f\left(x+1\right)h_{2}\left(x+1\right)
\end{align*}
where the values of the convergents are given by
\[
\KK_{1}^{n}\frac{b_{i}}{a_{i}}=\frac{f\left(1\right)h_{2}\left(1\right)}{f\left(0\right)}\left(\frac{1}{\sum_{k=0}^{n}\frac{f\left(0\right)f\left(1\right)}{f\left(k\right)f\left(k+1\right)}\prod_{i=1}^{k}\left(\frac{h_{1}\left(i\right)}{h_{2}\left(i+1\right)}\right)}-1\right).
\]
In this case, it is not enough to decompose $b\left(x\right)$, which
is not a simple task by itself, we also need to guess what is the
polynomial $f$. With this in mind we have the following results,
which can be used to construct an algorithm which finds $f\left(x\right)$
(if such a polynomial exists).
\begin{lem}
\label{lem:polynomial-recursion}Suppose that there is a solution
for an equation of the form
\begin{equation}
f\left(x+1\right)\beta_{\left(1\right)}\left(x\right)+f\left(x\right)\beta_{\left(0\right)}\left(x\right)+f\left(x-1\right)\beta_{\left(-1\right)}\left(x\right)=0,\label{eq:pol-recursion-formula}
\end{equation}
where $f,\beta_{\left(i\right)}\in\CC\left[x\right]$ are nonzero
polynomials. Let $d_{f}=\deg\left(f\right)$, $d=\max\left\{ \deg\left(\beta_{\left(i\right)}\right)\;\mid\;i=-1,0,1\right\} $
and write 
\begin{align*}
\beta_{\left(i\right)}\left(x\right) & =\sum_{j=0}^{d}\beta_{\left(i\right)}^{\left(j\right)}x^{j}
\end{align*}
where the coefficients $\beta_{\left(i\right)}^{\left(j\right)}\in\CC$
are scalars (and we use the convention of $\beta_{\left(i\right)}^{\left(j\right)}=0$
for negative $j$). Then
\begin{enumerate}
\item The sum $\beta_{\left(-1\right)}^{\left(d\right)}+\beta_{\left(0\right)}^{\left(d\right)}+\beta_{\left(1\right)}^{\left(d\right)}=0$.
In particular, at least two of the $\beta_{\left(i\right)}$ have
the max degree $d$.
\item If $\beta_{\left(-1\right)}^{\left(d\right)}-\beta_{\left(1\right)}^{\left(d\right)}\neq0$,
then the degree of $f$ must be $d_{f}=\frac{\beta_{\left(-1\right)}^{\left(d-1\right)}+\beta_{\left(0\right)}^{\left(d-1\right)}+\beta_{\left(1\right)}^{\left(d-1\right)}}{\beta_{\left(-1\right)}^{\left(d\right)}-\beta_{\left(1\right)}^{\left(d\right)}}$.
In particular, this expression must be well defined and an integer.
\item If $\beta_{\left(-1\right)}^{\left(d\right)}-\beta_{\left(1\right)}^{\left(d\right)}=0$,
then $\beta_{\left(-1\right)}^{\left(d\right)}+\beta_{\left(1\right)}^{\left(d\right)}\neq0$
and
\[
\left(\beta_{\left(-1\right)}^{\left(d-2\right)}+\beta_{\left(0\right)}^{\left(d-2\right)}+\beta_{\left(1\right)}^{\left(d-2\right)}\right)+d_{f}\left(-\beta_{\left(-1\right)}^{\left(d-1\right)}+\beta_{\left(1\right)}^{\left(d-1\right)}\right)+\binom{d_{f}}{2}\left(\beta_{\left(-1\right)}^{\left(d\right)}+\beta_{\left(1\right)}^{\left(d\right)}\right)=0
\]
is a nontrivial quadratic equation in $d_{f}$.
\end{enumerate}
\end{lem}

\begin{proof}
In general, the coefficients of a product of polynomials is a convolution
of the coefficients of the given polynomials. In order to use this,
we first want to find the coefficients of $f_{\left(k\right)}=f\left(x+k\right)$
for $k=-1,0,1$, so that 
\[
\sum_{k=-1}^{1}f_{\left(k\right)}\left(x\right)\beta_{\left(k\right)}\left(x\right)=0.
\]
Writing $f\left(x\right)=\sum_{0}^{d_{f}}f^{\left(i\right)}\cdot x^{i}$
where $f^{\left(i\right)}\in\CC$, we get that
\[
f_{\left(k\right)}\left(x\right)=\sum_{0}^{d_{f}}f^{\left(i\right)}\cdot\left(x+k\right)^{i}=\sum_{i=0}^{d_{f}}f^{\left(i\right)}\cdot\sum_{j=0}^{i}\binom{i}{j}x^{j}k^{i-j}=\sum_{j=0}^{d_{f}}x^{j}\sum_{i=j}^{d_{f}}f^{\left(i\right)}\cdot\binom{i}{j}k^{i-j}.
\]

For $i<j$, we can write $\binom{i}{j}=0$, so that the coefficient
of $x^{j}$ in $f_{\left(k\right)}$ is 
\[
f_{\left(k\right)}^{\left(j\right)}=\sum_{i=0}^{d_{f}}f^{\left(i\right)}\cdot\binom{i}{j}k^{i-j}.
\]
The coefficient of $x^{d_{f}+d-\ell}$ in $\sum_{k=-1}^{1}f_{\left(k\right)}\left(x\right)\cdot\beta_{\left(k\right)}\left(x\right)$
is
\begin{align*}
\sum_{k=-1}^{1}\left[\sum_{j=0}^{\ell}f_{\left(k\right)}^{\left(d_{f}-j\right)}\beta_{\left(k\right)}^{\left(d+j-\ell\right)}\right] & =\sum_{k=-1}^{1}\sum_{j=0}^{\ell}\sum_{i=0}^{d_{f}}f^{\left(i\right)}\cdot\binom{i}{d_{f}-j}k^{i+j-d_{f}}\beta_{\left(k\right)}^{\left(d+j-\ell\right)}\\
 & =\sum_{i=0}^{d_{f}}f^{\left(i\right)}\left[\sum_{j=d_{f}-i}^{\ell}\binom{i}{d_{f}-j}\sum_{k=-1}^{1}k^{i+j-d_{f}}\beta_{\left(k\right)}^{\left(d+j-\ell\right)}\right].
\end{align*}

\begin{enumerate}
\item We first look at the leading coefficient, namely the coefficient of
$x^{d_{f}+d}$, which should be zero. This means that $\ell=0$, implying
that from all the sums we are left with $j=0$ and $i=d_{f}$, so
that 
\[
0=f^{\left(d_{f}\right)}\left[\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d\right)}\right].
\]
The leading coefficient of $f$ is non zero, so we are left with
\[
0=\beta_{\left(-1\right)}^{\left(d\right)}+\beta_{\left(0\right)}^{\left(d\right)}+\beta_{\left(1\right)}^{\left(d\right)}.
\]
Since this sum is zero, and at least one of the summands is nonzero
(since $d=\max\left\{ \deg\left(\beta_{\left(k\right)}\right)\;\mid\;k=-1,0,1\right\} $),
at least two of them are non zero.
\item Next, taking $\ell=1$ and equating the coefficient to 0, we get that
\begin{align*}
0 & =\sum_{i=0}^{d_{f}}f^{\left(i\right)}\left[\sum_{j=d_{f}-i}^{1}\binom{i}{d_{f}-j}\sum_{k=-1}^{1}k^{i+j-d_{f}}\beta_{\left(k\right)}^{\left(d+j-\ell\right)}\right]\\
 & =\overbrace{f^{\left(d_{f}\right)}\left[\sum_{j=0}^{1}\binom{d_{f}}{d_{f}-j}\sum_{k=-1}^{1}k^{j}\beta_{\left(k\right)}^{\left(d+j-1\right)}\right]}^{i=d_{f}}+\overbrace{f^{\left(d_{f}-1\right)}\left[\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d\right)}\right]}^{i=d_{f}-1}.
\end{align*}
From part (1) we know that $\left[\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d\right)}\right]=0$.
Using again the fact that $f^{\left(d_{f}\right)}\neq0$, we get that
\[
0=\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d-1\right)}+d_{f}\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d\right)}.
\]
In particular, if $\beta_{\left(1\right)}^{\left(d\right)}-\beta_{\left(-1\right)}^{\left(d\right)}=\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d\right)}\neq0$,
then 
\[
d_{f}=-\frac{\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d-1\right)}}{\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d\right)}}.
\]
Otherwise, we get that $\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d\right)}=\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d-1\right)}=0$.
\item Finally, letting $\ell=2$, we get
\begin{align*}
0 & =\sum_{i=0}^{d_{f}}f^{\left(i\right)}\left[\sum_{j=d_{f}-i}^{2}\binom{i}{d_{f}-j}\sum_{k=-1}^{1}k^{i+j-d_{f}}\beta_{\left(k\right)}^{\left(d+j-2\right)}\right]\\
 & =f^{\left(d_{f}\right)}\left[\sum_{j=0}^{2}\binom{d_{f}}{d_{f}-j}\sum_{k=-1}^{1}k^{j}\beta_{\left(k\right)}^{\left(d+j-2\right)}\right]+f^{\left(d_{f}-1\right)}\left[\sum_{j=1}^{2}\binom{d_{f}-1}{d_{f}-j}\sum_{k=-1}^{1}k^{j-1}\beta_{\left(k\right)}^{\left(d+j-2\right)}\right]+f^{\left(d_{f}-2\right)}\left[\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d\right)}\right].
\end{align*}
Once again, we know that $\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d\right)}=0$,
which removes the last summand.\\
If $\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d\right)}=\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d-1\right)}=0$,
then the second summand is zero, and dividing by the nonzero coefficient
$f^{\left(d_{f}\right)}$, we are left with
\[
0=\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d-2\right)}+d_{f}\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d-1\right)}+\binom{d_{f}}{2}\sum_{k=-1}^{1}k^{2}\beta_{\left(k\right)}^{\left(d\right)}.
\]
We already have that $\sum_{k=-1}^{1}\beta_{\left(k\right)}^{\left(d\right)}=0$
and assumed that $\sum_{k=-1}^{1}k\beta_{\left(k\right)}^{\left(d\right)}$.
If $\sum_{k=-1}^{1}k^{2}\beta_{\left(k\right)}^{\left(d\right)}$
as well, then we must have that $\beta_{\left(k\right)}^{\left(d\right)}=0$
for $k=-1,0,1$, but $d$ was chosen as the max degree of the $\beta_{\left(k\right)}$,
so at least one of the $\beta_{\left(k\right)}^{\left(d\right)}$
cannot be zero. Thus under our assumption we get that $\sum_{k=-1}^{1}k^{2}\beta_{\left(k\right)}^{\left(d\right)}=\beta_{\left(-1\right)}^{\left(d\right)}+\beta_{\left(1\right)}^{\left(d\right)}\neq0$,
so that the quadratic equation above is not trivial.
\end{enumerate}
\end{proof}
Note that once we know the $\beta_{\left(k\right)}$ and the degree
of $f$, \eqref{pol-recursion-formula} in the lemma is a linear system
in the coefficients of $f$, which can easily be solved using standard
methods.

Applying the previous lemma to our case, we get the following:
\begin{cor}
Suppose that $f,a,h_{1},h_{2}\in\CC\left[x\right]$ are polynomials
satisfying 
\begin{equation}
f\left(x\right)a\left(x\right)-f\left(x-1\right)h_{1}\left(x\right)-f\left(x+1\right)h_{2}\left(x+1\right)=0.\label{eq:PCF-recursion}
\end{equation}
Let $d=\max\left\{ \deg\left(a\right),\deg\left(h_{1}\right),\deg\left(h_{2}\right)\right\} $,
and write 
\begin{align*}
a\left(x\right) & =\sum_{i=0}^{d}a^{\left(i\right)}x^{i}\\
h_{1}\left(x\right) & =\sum_{i=0}^{d}h_{1}^{\left(i\right)}x^{i}\\
h_{2}\left(x\right) & =\sum_{i=0}^{d}h_{2}^{\left(i\right)}x^{i}.
\end{align*}
Then
\begin{enumerate}
\item We have $a^{\left(d\right)}=h_{1}^{\left(d\right)}+h_{2}^{\left(d\right)}$
and at least two of the $h_{1}^{\left(d\right)},h_{2}^{\left(d\right)},a^{\left(d\right)}$
are non zero.
\item If $h_{1}^{\left(d\right)}\neq h_{2}^{\left(d\right)}$, then the
degree of $f$ must be $d_{f}=\frac{a^{\left(d-1\right)}-h_{1}^{\left(d-1\right)}-h_{2}^{\left(d-1\right)}-dh_{2}^{\left(d\right)}}{h_{2}^{\left(d\right)}-h_{1}^{\left(d\right)}}$.
In particular, this expression must be well defined and an integer.
\item If $h_{1}^{\left(d\right)}=h_{2}^{\left(d\right)}$, then $a^{\left(d-1\right)}=h_{1}^{\left(d-1\right)}+h_{2}^{\left(d-1\right)}+dh_{2}^{\left(d\right)}$
and 
\[
{\scriptstyle \left(a^{\left(d-2\right)}-h_{1}^{\left(d-2\right)}-\left(h_{2}^{\left(d-2\right)}+\left(d-1\right)h_{2}^{\left(d-1\right)}+\binom{d}{2}h_{2}^{\left(d\right)}\right)\right)+d_{f}\left(h_{1}^{\left(d-1\right)}-\left(h_{2}^{\left(d-1\right)}+dh_{2}^{\left(d\right)}\right)\right)-\binom{d_{f}}{2}\left(h_{1}^{\left(d\right)}+h_{2}^{\left(d\right)}\right)=0}
\]
is a nontrivial quadratic equation in $d_{f}$ (namely $\left(h_{1}^{\left(d\right)}+h_{2}^{\left(d\right)}\right)\neq0$).
\end{enumerate}
\end{cor}

\begin{example}
\begin{enumerate}
\item Suppose that we start with a polynomial continued fraction in the
trivial Euler family, namely 
\begin{align*}
b\left(x\right) & =-h_{1}\left(x\right)h_{2}\left(x\right)\\
a\left(x\right) & =h_{1}\left(x\right)+h_{2}\left(x+1\right).
\end{align*}
Then the theorem above should show that $d_{f}=0$, namely we can
take $f\equiv1$ constant. Let's see three examples:
\begin{enumerate}
\item If $b\left(x\right)=-x^{d}\times x^{d}$ and $a\left(x\right)=x^{d}+\left(1+x\right)^{d}$,
then
\begin{align*}
\begin{array}{c|ccc}
j= & \;d\; & d-1 & d-2\\
\hline a^{\left(j\right)} & 2 & d & \binom{d}{2}\\
h_{1}^{\left(j\right)} & 1 & 0 & 0\\
h_{2}^{\left(j\right)} & 1 & 0 & 0
\end{array}.
\end{align*}
Part (1) in the corollary above of course holds. Since $h_{1}^{\left(d\right)}=h_{2}^{\left(d\right)}$
we would have to use part (3) to find $d_{f}$, where we would get
the equation
\begin{align*}
0 & =\left(\binom{d}{2}-\binom{d}{2}\right)-d\cdot d_{f}-2\binom{d_{f}}{2}=-\left(d+d_{f}-1\right)d_{f}.
\end{align*}
Hence either $d_{f}=1-d\leq0$ or $d_{f}=0$, so in any way we know
to look for a constant $f$ solution.
\item If $b\left(x\right)=-\left(-x^{d}\right)\times x^{d}$ and $a\left(x\right)=\left(1+x\right)^{d}-x^{d}$,
then
\begin{align*}
\begin{array}{c|ccc}
j= & \;d\; & d-1 & d-2\\
\hline a^{\left(j\right)} & 0 & d & \binom{d}{2}\\
h_{1}^{\left(j\right)} & -1 & 0 & 0\\
h_{2}^{\left(j\right)} & 1 & 0 & 0
\end{array}.
\end{align*}
This time $h_{1}^{\left(d\right)}\neq h_{2}^{\left(d\right)}$, so
we can use part $\left(2\right)$ to get
\[
d_{f}=\frac{a^{\left(d-1\right)}-h_{1}^{\left(d-1\right)}-h_{2}^{\left(d-1\right)}-dh_{2}^{\left(d\right)}}{h_{2}^{\left(d\right)}-h_{1}^{\left(d\right)}}=\frac{d-d}{1-\left(-1\right)}=0.
\]
\item We can also look when $\deg\left(h_{1}\right)\neq\deg\left(h_{2}\right)$,
for example in $b\left(x\right)=-1\times x$ and $a\left(x\right)=1+\left(x+1\right)$,
so that $d=1$. Here the coefficients of $x^{-1}$ are considered
as zero and we get
\begin{align*}
\begin{array}{c|ccc}
j= & \;d\; & d-1 & d-2\\
\hline a^{\left(j\right)} & 1 & 2 & 0\\
h_{1}^{\left(j\right)} & 0 & 1 & 0\\
h_{2}^{\left(j\right)} & 1 & 0 & 0
\end{array}.
\end{align*}
Since $h_{1}^{\left(d\right)}\neq h_{2}^{\left(d\right)}$ we have
that 
\[
d_{f}=\frac{a^{\left(d-1\right)}-h_{1}^{\left(d-1\right)}-h_{2}^{\left(d-1\right)}-dh_{2}^{\left(d\right)}}{h_{2}^{\left(d\right)}-h_{1}^{\left(d\right)}}=\frac{2-1-0-1}{1-0}=0.
\]
\end{enumerate}
\item Take $b\left(x\right)=-x^{3}\times x^{3}$ and $a\left(x\right)=x^{3}+\left(1+x\right)^{3}+4\cdot\left(2x+1\right)$,
which is the continued fraction on the second line in the $\zeta\left(3\right)$
matrix field discussed in \secref{The-z3-case}. After choosing the
decomposition of $-b\left(x\right)$ with $h_{1}\left(x\right)=h_{2}\left(x\right)=x^{3}$
, so that $d=3$, we have
\begin{align*}
\begin{array}{c|ccc}
j= & \;d\; & d-1 & d-2\\
\hline a^{\left(j\right)} & 2 & 3 & 11=\binom{3}{2}+8\\
h_{1}^{\left(j\right)} & 1 & 0 & 0\\
h_{2}^{\left(j\right)} & 1 & 0 & 0
\end{array}.
\end{align*}
Since $h_{1}^{\left(d\right)}=h_{2}^{\left(d\right)}$, we can use
part (3) in the corollary to get 
\[
0=8-3d_{f}-d_{f}\left(d_{f}-1\right)=8-2d_{f}-d_{f}^{2}=\left(4+d_{f}\right)\left(2-d_{d}\right).
\]
Since $d_{f}$ needs to be nonnegative, we only need to check $d_{f}=2$.
Solving the linear system will produce $f(x)=x^{2}+x+\frac{1}{2}$.
\item Take $b\left(x\right)=-x^{6}$ and $a\left(x\right)=34x^{3}+51x^{2}+27x+5$
which is the polynomial continued fraction we got on the diagonal
of the $\zeta\left(3\right)$ matrix field in \thmref{zeta-3-irrational}.
Let us show that in this case there is no solution to \eqref{PCF-recursion}
in the corollary.\\
Assume by negation that there is a solution. In any decomposition
$b\left(x\right)=-h_{1}\left(x\right)h_{2}\left(x\right)$ we have
that $\deg\left(h_{1}\right)+\deg\left(h_{2}\right)=6$. Since $\deg\left(a\right)=3$,
in order for part $\left(1\right)$ in the corollary to hold, we must
have that $\deg\left(h_{1}\right)=\deg\left(h_{2}\right)=3$, so that
$h_{1}\left(x\right)=cx^{3}$ and $h_{2}\left(x\right)=\frac{1}{c}x^{3}$.
We also need that $c+\frac{1}{c}=h_{1}^{\left(d\right)}+h_{2}^{\left(d\right)}=a^{\left(d\right)}=34$,
so that $c^{2}-34c+1=0$, and in particular $c\neq\pm1$.\\
From this we conclude that $h_{1}^{\left(d\right)}=c\neq\frac{1}{c}=h_{2}^{\left(d\right)}$,
so we may use part $\left(2\right)$ to get 
\[
d_{f}=\frac{a^{\left(d-1\right)}-h_{1}^{\left(d-1\right)}-h_{2}^{\left(d-1\right)}-dh_{2}^{\left(d\right)}}{h_{2}^{\left(d\right)}-h_{1}^{\left(d\right)}}=\frac{51-0-0-3}{\frac{1}{c}-c}=\frac{48\cdot c}{\left(1-c^{2}\right)},
\]
where $d_{f}\geq0$ is an integer. It follows that $c$ also satisfies
the quadratic equation $d_{f}c^{2}+48c-d_{f}=0$. Combining the two
quadratic equations we get 
\[
0=\left(d_{f}c^{2}+48c-d_{f}\right)-d_{f}\left(c^{2}-34c+1\right)=\left(48+34\cdot d_{f}\right)c-2d_{f},
\]
so $c$ must be a rational number. However, if $c^{2}-34c+1$ has
a rational root, then its denominator and numerator must divide $1$,
namely the root must be $\pm1$ - contradiction. \\
We conclude that the polynomial continued fraction presentation $\KK_{1}^{\infty}\frac{-n^{6}}{34n^{3}+51n^{2}+27n+5}$
cannot be written as in \eqref{PCF-recursion}.
\end{enumerate}
\end{example}

\newpage{}

\section{\label{app:Conservative-degree-1}Conservative matrix field of degree
1}

Here we give a full solution to the problem of finding
\begin{align*}
f\left(x,y\right) & =ax+by+c\\
\bar{f}\left(x,y\right) & =\bar{a}x+\bar{b}y+\bar{c}
\end{align*}
which satisfy the conditions in \defref{conjugate}, namely
\begin{align*}
f\left(x,y\right)-f\left(x+1,y-1\right) & =\bar{f}\left(x+1,y\right)-\bar{f}\left(x,y-1\right)\\
\left(f\bar{f}\right)\left(x,y\right)+\left(f\bar{f}\right)\left(0,0\right) & =\left(f\bar{f}\right)\left(x,0\right)+\left(f\bar{f}\right)\left(0,y\right).
\end{align*}
Solving the linear condition gives us
\begin{align*}
\left(ax+by+c\right)-\left(a\left(x+1\right)+b\left(y-1\right)+c\right) & =\left(\bar{a}\left(x+1\right)+\bar{b}y+\bar{c}\right)-\left(\bar{a}x+\bar{b}\left(y-1\right)+\bar{c}\right)\\
b-a & =\bar{a}+\bar{b}
\end{align*}
For the quadratic condition we have 
\[
\left(f\bar{f}\right)\left(x,y\right)=\left(ax+by+c\right)\left(\bar{a}x+\bar{b}y+\bar{c}\right)=\left[a\bar{a}x^{2}+\left(a\bar{c}+c\bar{a}\right)x+b\bar{b}y^{2}+\left(b\bar{c}+c\bar{b}\right)y+c\bar{c}\right]+\left(a\bar{b}+b\bar{a}\right)xy.
\]
Since the quadratic condition simply says that there are no monomials
with mixed $x$ and $y$, in this case we simply get that $a\bar{b}+b\bar{a}=0.$\\

To solve these two conditions, write the quadratic condition as $\det\left(\begin{smallmatrix}-a & \bar{a}\\
b & \bar{b}
\end{smallmatrix}\right)=0$ , and the linear condition is just $\left(1,1\right)\left(\begin{smallmatrix}-a & \bar{a}\\
b & \bar{b}
\end{smallmatrix}\right)\left(\begin{smallmatrix}-1\\
1
\end{smallmatrix}\right)=0$. A $2\times2$ matrix has determinant $0$ if and only if it has
rank at most $1$, so that it has the form $v\cdot u^{tr}$ where
$v,u$ are column vectors. Hence, we get that the quadratic condition
is $\left(\begin{smallmatrix}-a & \bar{a}\\
b & \bar{b}
\end{smallmatrix}\right)=v\cdot u^{tr}$ and the linear condition is then 
\[
0=\left(1,1\right)\left(\begin{smallmatrix}-a & \bar{a}\\
b & \bar{b}
\end{smallmatrix}\right)\left(\begin{smallmatrix}-1\\
1
\end{smallmatrix}\right)=\left(1,1\right)v\cdot u^{tr}\left(\begin{smallmatrix}-1\\
1
\end{smallmatrix}\right).
\]
This is now a product of two scalars which is zero, so that either
$\left(1,1\right)v=0$ or $u^{tr}\left(\begin{smallmatrix}-1\\
1
\end{smallmatrix}\right)$. These imply that $\left(1,1\right)\left(\begin{smallmatrix}-a & \bar{a}\\
b & \bar{b}
\end{smallmatrix}\right)=\left(0,0\right)$ or $\left(\begin{smallmatrix}-a & \bar{a}\\
b & \bar{b}
\end{smallmatrix}\right)\left(\begin{smallmatrix}-1\\
1
\end{smallmatrix}\right)=\left(\begin{smallmatrix}0\\
0
\end{smallmatrix}\right)$ respectively.

To summarize, our pair of polynomials are either of the form
\begin{align*}
f\left(x,y\right) & =a\left(x+y\right)+c\\
\bar{f}\left(x,y\right) & =\bar{a}\left(x-y\right)+\bar{c}
\end{align*}
or 
\begin{align*}
f\left(x,y\right) & =ax+by+c\\
\bar{f}\left(x,y\right) & =-ax+by+\bar{c}
\end{align*}


\section{\label{app:integer-values}The algebra of integer valued polynomial}

In the previous section we answer the question of given a polynomial
$g\left(x\right)$ and some number $d$, how to check if $d\mid g\left(n\right)$
for all integers $n$. Alternatively, is $d\mid gcd\left\{ g\left(n\right)\;\mid n\in\ZZ\right\} $.
Of course, if we can write $g\left(x\right)=d\tilde{g}\left(x\right)$
with $\tilde{g}\in\ZZ\left[x\right]$, then this condition holds,
but the other direction is not true. For example, the polynomial $x\left(x+1\right)$
is not divisible by $2$, but for every integer $n$ either $n$ or
$n+1$ is even, so $n\left(n+1\right)$ is always divisible by $2$.
We can even go further to polynomials with rational coefficients,
like $g\left(x\right)=\frac{x\left(x+1\right)\left(x+2\right)}{3}$,
such that for any $n\in\ZZ$ we still have that $g\left(n\right)$
is an even integer. This motivates us to define the following.
\begin{defn}
For $n\in\NN$ we define the polynomial $\binom{x}{n}=\prod_{0}^{n-1}\frac{\left(x-i\right)}{i+1}=\frac{x\cdot\left(x-1\right)\cdots\left(x-n+1\right)}{n!}$.
This is a polynomial of degree $n$ in $\QQ\left[x\right]$, so that
$\left\{ \binom{x}{n}\right\} _{0}^{\infty}$ is a $\QQ$-basis for
$\QQ\left[x\right]$. In particular, for a nonnegative integers $x$,
we simply get the binomials.
\end{defn}

The polynomials $f$ in $\QQ\left[x\right]$ satisfying $f\left(\ZZ\right)\subseteq\ZZ$
are called \textbf{integer valued polynomial}. This class was fully
described by Pólya in \cite{polya_uber_1915}, and it was shown to
contain exactly the integer combinations of the $\binom{x}{n}$ above.
For the ease of the reader, we add the proof for this result here.
\begin{lem}
For every integer $m$, we have that $\binom{m}{n}\in\ZZ$. 
\end{lem}

\begin{proof}
We first note that Pascal's identity holds for the polynomial. Indeed,
taking 
\[
p\left(x\right)=\binom{x}{n}-\binom{x-1}{n-1}-\binom{x-1}{n},
\]
we get a finite degree polynomial where $p\left(m\right)=0$ for all
$m\geq n$, so that $p\left(x\right)\equiv0$ as a polynomial.

In order to prove that $\binom{m}{n}\in\ZZ$ for all $n,m\in\ZZ$,
we use induction, first on $n$ and then on $m$.

For $n=0$ we simply get that $\binom{m}{0}=1$ and for $n=1$ we
get $\binom{m}{1}=m$ for all $m$, so we are done.

Assume now the claim for $n-1$ and we prove for $n\geq2$. By Pascal's
identity we have $\binom{m}{n}=\binom{m-1}{n-1}+\binom{m-1}{n}$ and
since $\binom{m-1}{n-1}$ is always an integer by the induction hypothesis,
then $\binom{m}{n}$ is an integer if and only if $\binom{m-1}{n}$
is an integer. In other words, we only need to show this for a single
$m$. Taking $m=0$ we get $\binom{0}{n}=0$ and we are done.
\end{proof}
In the following, when we write $d\mid q$ for $d\in\ZZ$ and $q\in\QQ$,
we mean that $q$ has to be an integer and is divisible by $d$.
\begin{lem}
\label{lem:binomial-polynomials}Given a general polynomial $f\left(x\right)=\sum_{0}^{d}a_{n}\binom{x}{n}\in\QQ\left[x\right]$
and an integer $k$ the following are equivalent:
\begin{enumerate}
\item For all $0\leq n\leq d$ we have $k\mid a_{n}$ ,
\item For all $m\in\ZZ$ we have $k\mid f\left(m\right)$ ,
\item For $m=0,1,...,d$ , we have $k\mid f\left(m\right)$ ,
\item There exists $m_{0}$ such that $k\mid f\left(m_{0}+m\right)$ for
$m=0,1,...,d$, and
\end{enumerate}
\end{lem}

\begin{proof}
Note first that considering the polynomial $\frac{f\left(x\right)}{k}$
instead, it is enough to prove the lemma for $k=1$. Namely, we just
need to show that the coefficients\textbackslash evaluation are integers.
\begin{itemize}
\item $\left(1\right)\Rightarrow\left(2\right)$: follows from the fact
that $\binom{m}{n}$ are integers for all $m$.
\item $\left(2\right)\Rightarrow\left(3\right)$: is trivial.
\item $\left(3\right)\Rightarrow\left(1\right)$: Since $\binom{n}{n}=1$
and $\binom{m}{n}=0$ when $0\leq m<n$, it follows that for $0\leq m\leq d$
we have 
\[
f\left(m\right)=a_{m}+\sum_{n=0}^{m-1}a_{n}\binom{m}{n},
\]
which we can also write as 
\[
a_{m}=f\left(m\right)-\sum_{n=0}^{m-1}a_{n}\binom{m}{n}.
\]
So if $a_{n}\in\ZZ$ for $0\leq n<m$, then since $f\left(m\right)\in\ZZ$
by assumption and $\binom{m}{n}\in\ZZ$, we conclude that $a_{m}\in\ZZ$.
Thus, by induction we get that $a_{n}\in\ZZ$ for all $0\leq n\leq d$,
namely we get $\left(1\right)$.
\item $\left(2\right)\Rightarrow\left(4\right)$: is trivial.
\item $\left(4\right)\Rightarrow\left(2\right)$: If $f\left(m_{0}+m\right)\in\ZZ$
for $m=0,...,d$, then setting $g\left(m\right)=f\left(m_{0}+m\right)$
we see that $g\left(m\right)\in\ZZ$ for $m=0,...,d$. By the $\left(3\right)\Rightarrow\left(2\right)$
direction for the degree $d$ polynomial $g$ we get that $g\left(m\right)=f\left(m_{0}+m\right)\in\ZZ$
for all $m$, which is exactly condition $\left(2\right)$ for the
polynomial $f$ and we are done.
\end{itemize}
\end{proof}

\newpage{}

\section{\label{app:Asymptotics-of-recurrence}Asymptotics of recurrence with
convergent coefficients}

In this section we fix a recurrence relation over $\RR$:
\begin{equation}
v_{n+1}=a_{n}v_{n}+b_{n}v_{n-1},\quad\limfi na_{n}=a,\quad\limfi nb_{n}=b.\label{eq:convergent-recurrence}
\end{equation}
Denote by $x^{2}=ax+b$ the quadratic polynomial corresponding to
the limit and assume that its two roots $\lambda_{\pm}$ are distinct
and satisfy $0<\left|\lambda_{-}\right|<\lambda_{+}$. It is well
known that in the limit recursion $v_{n+1}=av_{n}+bv_{n-1}$, unless
the starting position is $\left(v_{0},v_{1}\right)=c\left(1,\lambda_{-}\right)$
for some constant $c$, then $v_{n}\sim\lambda_{+}^{n}$. In this
section we want to show that a similar claim holds for the recurrence
relation with the convergent coefficients. In this case, it is not
enough to have a condition on the starting position. Indeed, we might
even have $a_{n}=b_{n}=a_{n+1}=b_{n+1}=0$ for some $n$, which leads
to $v_{k}=0$ for all $k\geq n+2$. Instead, our condition will be
that if $v_{n}$ grows at least slightly better than $\left|\lambda_{-}\right|^{n}$,
then it will behave like $\lambda_{+}^{n}$.\\

As usual, the first step is to move to matrix multiplication by rewriting
the recurrence as
\[
\left(\begin{smallmatrix}v_{n-1} & v_{n}\end{smallmatrix}\right)\left(\begin{smallmatrix}0 & b_{n}\\
1 & a_{n}
\end{smallmatrix}\right)=\left(\begin{smallmatrix}v_{n} & v_{n+1}\end{smallmatrix}\right).
\]
Letting $M_{n}=\left(\begin{smallmatrix}0 & b_{n}\\
1 & a_{n}
\end{smallmatrix}\right)$, we basically want to find the asymptotics of $\left(v_{0}\;v_{1}\right)\left(\prod_{1}^{k-1}M_{n}\right)$
, where we know that (1) $M_{n}\to M_{\infty}=\left(\begin{smallmatrix}0 & b\\
1 & a
\end{smallmatrix}\right)$ and (2) $M$ is diagonalizable with eigenvalues $\lambda_{\pm}$.
This diagonalization let us simplify the notation a bit. Letting $P\in\mathrm{GL}_{2}\left(\RR\right)$
such that $D=PMP^{-1}=\left(\begin{smallmatrix}\lambda_{+} & 0\\
0 & \lambda_{-}
\end{smallmatrix}\right)$, write $D_{n}=PM_{n}P^{-1}$ so that $D_{n}\to D$ and $\prod_{1}^{k}D_{n}=P\left(\prod_{1}^{k}M_{n}\right)P^{-1}$.
We expect the asymptotics of the corresponding sequence $\left(\alpha_{k},\beta_{k}\right)=\left(\alpha_{1},\beta_{1}\right)\prod_{1}^{k-1}D_{n}$
to behave like $\alpha_{k}\sim\lambda_{+}^{k}$ and $\beta_{k}\sim\lambda_{-}^{k}$,
so in particular $\left|\frac{\beta_{k}}{\alpha_{k}}\right|\sim\left|\frac{\lambda_{-}}{\lambda_{+}}\right|^{k}\to0$.
This is indeed true, under the right condition, and will eventually
give us the required result about the recurrence.
\begin{lem}
\label{lem:Diagonal-convergence}Suppose that $D_{n}\to D$ where
$D=\left(\begin{smallmatrix}\lambda_{+} & 0\\
0 & \lambda_{-}
\end{smallmatrix}\right)$ and $0\leq\left|\lambda_{-}\right|<\lambda_{+}$ and set $\left(\alpha_{k},\beta_{k}\right)=\left(\alpha_{1},\beta_{1}\right)\prod_{1}^{k-1}D_{n}$
for some initial position $\left(\alpha_{1},\beta_{1}\right)$. If
$\left|\frac{\beta_{k}}{\alpha_{k}}\right|$ has a bounded subsequence,
then $\left|\frac{\beta_{k}}{\alpha_{k}}\right|\to0$.
\end{lem}

\begin{proof}
By assumption, there is $M\geq1$ and a bounded subsequence $\left|\frac{\beta_{k_{i}}}{\alpha_{k_{i}}}\right|\leq M$
for all $i$.  We fix some $0<\varepsilon<1$, and since $0<\left|\lambda_{-}\right|<\lambda_{+}$,
for all small enough such choice we have (1) $\left(1+M\right)\varepsilon\leq\sqrt{\varepsilon}$
and (2) $\eta_{\varepsilon}:=\frac{\left|\lambda_{-}\right|+2\sqrt{\varepsilon}}{\lambda_{+}-\sqrt{\varepsilon}}<1$.
\\
We shall show below that when $\norm{D_{k}-D}_{\infty}<\varepsilon$
and $\left|\frac{\beta_{k}}{\alpha_{k}}\right|\leq M$, we get 
\begin{equation}
\left|\frac{\beta_{k+1}}{\alpha_{k+1}}\right|\leq\begin{cases}
\eta_{\varepsilon}\left|\frac{\beta_{k}}{\alpha_{k}}\right| & \text{if }\text{\ensuremath{\sqrt{\varepsilon}\leq}\ensuremath{\left|\frac{\beta_{k}}{\alpha_{k}}\right|}}\\
\sqrt{\varepsilon} & \text{if }\text{\ensuremath{\left|\frac{\beta_{k}}{\alpha_{k}}\right|}}<\sqrt{\varepsilon}
\end{cases}.\label{eq:ratio-convergence}
\end{equation}
The fact that $\norm{D_{k}-D}_{\infty}\to0$, implies that for all
$k$ big enough the condition $\norm{D_{k}-D}_{\infty}<\varepsilon$
holds. For these $k$'s, once we have a single $k_{0}$ for which
$\left|\frac{\beta_{k}}{\alpha_{k}}\right|\leq M$, the sequence $\left|\frac{\beta_{k}}{\alpha_{k}}\right|$
will decrease by a factor of $\eta_{\varepsilon}<1$, until it will
be smaller than $\sqrt{\varepsilon}$, and then it will remain as
such. As $\varepsilon>0$ can be arbitrarily small, we conclude that
$\left|\frac{\beta_{k}}{\alpha_{k}}\right|\to0$ as required.

To prove \eqref{ratio-convergence}, we use the fact that $\left(\alpha_{k+1},\beta_{k+1}\right)=\left(\alpha_{k},\beta_{k}\right)D_{k}$
where $D_{k}=D+\left(\begin{smallmatrix}\varepsilon_{1,1} & \varepsilon_{1,2}\\
\varepsilon_{2,1} & \varepsilon_{2,2}
\end{smallmatrix}\right)$ to get that 
\begin{align*}
\left|\frac{\beta_{k+1}}{\alpha_{k+1}}\right| & =\left|\frac{\alpha_{k}\varepsilon_{1,2}+\beta_{k}\left(\lambda_{-}+\varepsilon_{2,2}\right)}{\alpha_{k}\left(\lambda_{+}+\varepsilon_{1,1}\right)+\beta_{k}\varepsilon_{2,1}}\right|\leq\frac{\left|\alpha_{k}\right|\varepsilon+\left|\beta_{k}\right|\left(\left|\lambda_{-}\right|+\varepsilon\right)}{\left|\alpha_{k}\right|\left(\lambda_{+}-\varepsilon\right)-\left|\beta_{k}\right|\varepsilon}\leq\frac{\left|\alpha_{k}\right|\varepsilon+\left|\beta_{k}\right|\left(\left|\lambda_{-}\right|+\varepsilon\right)}{\left|\alpha_{k}\right|\left(\lambda_{+}-\varepsilon\left(1+M\right)\right)}\\
 & \leq\frac{\left|\alpha_{k}\right|\varepsilon+\left|\beta_{k}\right|\left(\left|\lambda_{-}\right|+\varepsilon\right)}{\left|\alpha_{k}\right|\left(\lambda_{+}-\sqrt{\varepsilon}\right)}=\left(*\right)
\end{align*}
Note that the dnominator in $\left(*\right)$ is positive since $\sqrt{\varepsilon}<1<\lambda_{+}$.
In the $\left|\frac{\beta_{k}}{\alpha_{k}}\right|\geq\sqrt{\varepsilon}$
case, we get 
\[
\left(*\right)\leq\frac{\left|\beta_{k}\right|\left(\left|\lambda_{-}\right|+\varepsilon+\sqrt{\varepsilon}\right)}{\left|\alpha_{k}\right|\left(\lambda_{+}-\sqrt{\varepsilon}\right)}\leq\left(\frac{\left|\lambda_{-}\right|+2\sqrt{\varepsilon}}{\lambda_{+}-\sqrt{\varepsilon}}\right)\cdot\frac{\left|\beta_{k}\right|}{\left|\alpha_{k}\right|}=\eta_{\varepsilon}\frac{\left|\beta_{k}\right|}{\left|\alpha_{k}\right|}.
\]
On the other hand, if $\left|\frac{\beta_{k}}{\alpha_{k}}\right|<\sqrt{\varepsilon}$
, then 
\[
\left(*\right)\leq\frac{\left|\alpha_{k}\right|\varepsilon+\left|\alpha_{k}\right|\sqrt{\varepsilon}\left(\left|\lambda_{-}\right|+\varepsilon\right)}{\left|\alpha_{k}\right|\left(\lambda_{+}-\sqrt{\varepsilon}\right)}=\sqrt{\varepsilon}\frac{\left|\lambda_{-}\right|+\varepsilon+\sqrt{\varepsilon}}{\lambda_{+}-\sqrt{\varepsilon}}\leq\eta_{\varepsilon}\sqrt{\varepsilon}<\sqrt{\varepsilon},
\]
which completes the proof.
\end{proof}
Returning back to the recursion, we get the following
\begin{thm}
Suppose that we have a solution to the recurrence $v_{n+1}=a_{n}v_{n}+b_{n}v_{n-1}$,
where $a_{n}\to a,b_{n}\to b$ and $\lambda_{\pm}$ are the roots
of $x^{2}=ax+b$ with $0<\left|\lambda_{-}\right|<\lambda_{+}$. Assume
further that there are some $R,r>0$ and a subsequence $\frac{\left|v_{n_{i}}\right|}{\left|v_{n_{i}-1}\right|}\in\left[\left|\lambda_{-}\right|+r,R\right]$.
Then for any $\varepsilon>0$ we have $\left(\lambda_{+}-\varepsilon\right)^{k}\leq v_{k}\leq\left(\lambda_{+}+\varepsilon\right)^{k}$
for all $k$ large enough.
\end{thm}

\begin{proof}
Set $M_{n}=\left(\begin{smallmatrix}0 & b_{n}\\
1 & a_{n}
\end{smallmatrix}\right)$ and $M=\left(\begin{smallmatrix}0 & b\\
1 & a
\end{smallmatrix}\right)$ as in the beginning of this section. With $P=\left(\begin{smallmatrix}1 & \lambda_{+}\\
1 & \lambda_{-}
\end{smallmatrix}\right)$ and $P^{-1}=\frac{1}{\lambda_{-}-\lambda_{+}}\left(\begin{smallmatrix}\lambda_{-} & -\lambda_{+}\\
-1 & 1
\end{smallmatrix}\right)$ we have that $D=PMP^{-1}=\left(\begin{smallmatrix}\lambda_{+} & 0\\
0 & \lambda_{-}
\end{smallmatrix}\right)$. It follows that 
\[
\left(v_{k-1}\;v_{k}\right):=\left(v_{0}\;v_{1}\right)\left(\prod_{1}^{k-1}M_{n}\right)=\left(v_{0}\;v_{1}\right)P^{-1}\left(\prod_{1}^{k-1}D_{n}\right)P
\]
Writing 
\[
\left(\alpha_{k},\beta_{k}\right)=\left(v_{0}\;v_{1}\right)P^{-1}\left(\prod_{1}^{k-1}D_{n}\right)=\left(v_{k-1},v_{k}\right)P^{-1},
\]
and using the assumption on the subsequence $\frac{\left|v_{n_{i}}\right|}{\left|v_{n_{i}-1}\right|}$
we get the upper bound
\[
\left|\frac{\beta_{n_{i}}}{\alpha_{n_{i}}}\right|=\left|\frac{-\lambda_{+}v_{n_{i}-1}+v_{n_{i}}}{\lambda_{-}v_{n_{i}-1}-v_{n_{i}}}\right|\le\frac{\left(R+\lambda_{+}\right)\left|v_{n_{i}-1}\right|}{r\left|v_{n_{i}-1}\right|}=\frac{R+\lambda_{+}}{r}.
\]
Using \lemref{Diagonal-convergence}, we conclude that $\left|\frac{\beta_{k}}{\alpha_{k}}\right|\to0$.
Going back via $\left(v_{k-1},v_{k}\right)=\left(\alpha_{k},\beta_{k}\right)P$,
we get that 
\[
\frac{v_{k}}{v_{k-1}}=\frac{\lambda_{+}\alpha_{k}+\lambda_{-}\beta_{k}}{\alpha_{k}+\beta_{k}}=\lambda_{+}\cdot\frac{1-\frac{\lambda_{-}}{\lambda_{+}}\frac{\beta_{k}}{\alpha_{k}}}{1+\frac{\beta_{k}}{\alpha_{k}}}\to\lambda_{+}.
\]
Hence, for any $\varepsilon>0$, we get that $\left|\frac{v_{k}}{v_{k-1}}-\lambda_{+}\right|<\frac{\varepsilon}{2}$
for all $k$ large enough, implying that $\left(\lambda_{+}-\varepsilon\right)^{k}\leq v_{k}\leq\left(\lambda_{+}+\varepsilon\right)^{k}$
for all $k$ large enough.
\end{proof}

\newpage{}

\bibliographystyle{plain}
\bibliography{apery}

\end{document}
