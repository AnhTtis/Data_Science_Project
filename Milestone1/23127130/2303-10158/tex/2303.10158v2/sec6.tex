\section{Data Benchmark}
\label{sec:6}

In the previous sections, we explored a diverse range of data-centric AI tasks throughout various stages of the data lifecycle. Examining benchmarks is a promising approach for gaining insight into the progress of research and development in these tasks, as benchmarks comprehensively evaluate various methods based on standard and agreed-upon metrics. It is important to note that, within the context of data-centric AI, we are specifically interested in \emph{data benchmarks} rather than model benchmarks, which should assess various techniques aimed at achieving data excellence. In this section, we survey the existing benchmarks for different goals of data-centric AI. Firstly, we will introduce the benchmark collection strategy, and subsequently, we will summarize and analyze the collected benchmarks.


\begin{table}[t]
\tiny
\centering
\caption{Data benchmarks. Note that they evaluate \emph{data} rather than model.}
\label{tbl:benchsummary}
\setlength{\tabcolsep}{2.0pt}
\begin{tabular}{cccccc} \toprule
\textbf{Reference} & \textbf{Sub-goal} & \textbf{Task} & \textbf{Domain} & \textbf{Data modality}  & \textbf{Open-source} \\
\midrule
\rowcolor{Gray} 
\multicolumn{6}{c}{\textbf{\textit{Training data development}}} \\ 
Cohen et al.~\cite{cohen2017publicly} & Collection & Dataset discovery & Biomedical & Tabular, text & \xmark \\
Poess et al.~\cite{poess2014tpc} & Collection & Data integration & Database & Tabular, time-series & \xmark \\
Pinkel et al.~\cite{pinkel2015rodi} & Collection & Data integration & Database & Tabular, graph & \xmark \\
Wang et al.~\cite{wang2022usb} & Labeling & Semi-supervised learning & AI & Image, text, audio & \cmark \\
Yang et al.~\cite{yang2018benchmark} & Labeling & Active learning & AI & Tabular, image, text & \xmark \\
Meduri et al.~\cite{meduri2020comprehensive} & Labeling & Active learning & Database & Tabular, text & \xmark \\
Abdelaal et al.~\cite{abdelaal2023rein} & Preparation & Data cleaning & Database & Tabular, text, time-series & \cmark \\
Li et al.~\cite{li2019cleanml} & Preparation & Data cleaning & Database & Tabular, time-series & \cmark \\
J{\"a}ger et al.~\cite{jager2021benchmark} &  Preparation & Data cleaning & AI & Tabular, image & \xmark \\ 
Buckley et al.~\cite{buckley2022feature} & Preparation & Feature extraction & Healthcare & Tabular, image, time-series & \cmark \\ 
Vijayan et al.~\cite{vijayan2022blood} & Preparation & Feature extraction & Biomedical & Tabular, sequential & \cmark \\
Bommert et al.~\cite{bommert2022benchmark} & Reduction & Feature selection & Biomedical & Tabular, sequential & \cmark \\
Espadoto et al.~\cite{espadoto2019toward} & Reduction & Dimensionality reduction & Computer graphics & Tabular, image, audio & \cmark \\
Grochowski et al.~\cite{grochowski2004comparison} & Reduction & Instance selection & Computer graphics & Tabular, image, audio & \cmark \\
Blachnik et al.~\cite{blachnik2020comparison} & Reduction & Instance selection & Computer graphics & Tabular, image, audio & \cmark \\
Iwana et al.~\cite{iwana2021empirical} & Augmentation & All sub-goals & AI & Time-series & \cmark \\
Nanni et al.~\cite{nanni2021comparison} & Augmentation & Basic manipulation & AI & Image & \cmark \\
Yoo et al.~\cite{yoo2020rethinking} & Augmentation & Basic manipulation & AI & Image & \cmark \\
Ding et al.~\cite{ding2022data} & Augmentation & Augmentation data synthesis & AI & Graph & \xmark \\
Tao et al.~\cite{tao2021benchmarking} & Augmentation & Augmentation data synthesis & Computer security & Tabular & \xmark \\
Zoller et al.~\cite{zoller2021benchmark} & - & Pipeline search & AI & Tabular, image, audio, time-series & \cmark \\
Gijsbers et al.~\cite{gijsbers2022amlb} & - & Pipeline search & AI & Tabular, image, audio, time-series & \cmark \\
\rowcolor{Gray} 
\multicolumn{6}{c}{\textbf{\textit{Evaluation data development}}} \\ 
Srivastava et al.~\cite{srivastava2022beyond} & In-distribution & Evaluation data synthesis & AI & Text & \cmark \\
Pawelczyk et al.~\cite{pawelczyk2021carla} & In-distribution & Algorithmic recourse & AI & Tabular & \cmark \\
Dong et al.~\cite{dong2020benchmarking} & Out-of-distribution & Adversarial samples & AI & Image & \cmark \\
Hendrycks et al.~\cite{hendrycks2019benchmarking} & Out-of-distribution & Adversarial samples & AI & Image & \cmark \\
Yoo et al.~\cite{yoo2020searching} & Out-of-distribution & Adversarial samples  & AI & Text & \cmark \\
\rowcolor{Gray} 
\multicolumn{6}{c}{\textbf{\textit{Data maintenance}}} \\ 
Kanthara et al.~\cite{kanthara2022chart} & Understanding & Visual summarization  & AI & Tabular, text & \cmark \\
Grinstein et al.~\cite{grinstein2002benchmark} & Understanding & Visual summarization & Human-computer interaction & Tabular, image & \cmark \\
Zeng et al.~\cite{zeng2021evaluation} & Understanding & Visualization recommendation  & Human-computer Interaction & Tabular & \xmark \\
Jia et al.~\cite{jia2021scalability} & Understanding & Data valuation & AI & Image & \cmark \\
Batini et al.~\cite{batini2009methodologies} & Quality assurance & Quality assessment  & Database & Tabular & \xmark \\
Arocena et al.~\cite{arocena2016benchmarking} & Quality assurance & Quality improvement  & Database & Tabular & \xmark \\
Zhang et al.~\cite{zhang2022facilitating} & Storage \& retrieval & Resource allocation  & Database & Tabular & \cmark \\
Marcus et al.~\cite{marcus2020benchmarking} & Storage \& retrieval & Query index selection  & Database & Tabular & \xmark \\
\rowcolor{Gray} 
\multicolumn{6}{c}{\textbf{\textit{Unified benchmark}}} \\ 
Mazumder et al.~\cite{mazumder2022dataperf} & Multiple & 6 distinct tasks & AI & Multiple & \xmark \\
\bottomrule
\end{tabular}
\end{table}


% Transforme the table
% \begin{table}[t]
% \scriptsize
% \centering
% \caption{Some representative benchmarks.}
% \label{tbl:benchsummary}
% \setlength{\tabcolsep}{1.2pt}
% \begin{tabular}{ccccccc} \toprule
% \textbf{Reference} & \textbf{Sub-goal} & \textbf{Task} & \textbf{Domain} & \textbf{Data Modality} & \textbf{Quant. Analysis} & \textbf{Open-source} \\
% \midrule

% Mazumder et al.~\cite{mazumder2022dataperf} & Data Developments & All Sub-goals & Computer System & - & \xmark & \xmark \\
% \rowcolor{Gray} 
% \multicolumn{7}{c}{\textbf{\textit{Training Data Development}}} \\ 
% Cohen et al.~\cite{cohen2017publicly} & Data Collection & Dataset Discovery & Biomedical & Tabular, Text & \cmark & \xmark \\
% Poess et al.~\cite{poess2014tpc} & Data Collection & Data Integration & Database & Tabular, Time-series & \cmark & \xmark \\
% Pinkel et al.~\cite{pinkel2015rodi} & Data Collection & Data Integration & Database & Tabular, Graph & \cmark & \xmark \\
% Wang et al.~\cite{wang2022usb} & Data Labeling & Semi-supervised Learning & Machine Learning & Image, Text, Audio & \cmark & \cmark \\
% Yang et al.~\cite{yang2018benchmark} & Data Labeling & Active Learning & Machine Learning & Tabular, Image, Text & \cmark & \xmark \\
% Meduri et al.~\cite{meduri2020comprehensive} & Data Labeling & Active Learning & Database & Tabular, Text & \cmark & \xmark \\
% Abdelaal et al.~\cite{abdelaal2023rein} & Data Preparation & Data Cleaning & Database & Tabular, Text, Time-series & \cmark & \cmark \\
% Li et al.~\cite{li2019cleanml} & Data Preparation & Data Cleaning & Database & Tabular, Time-series & \cmark & \cmark \\
% J{\"a}ger~\cite{jager2021benchmark} &  Data Preparation & Data Cleaning & Data Mining & Tabular, Image & \cmark & \xmark \\ 
% Buckley et al.~\cite{buckley2022feature} & Data Preparation & Feature Extraction & Healthcare & Tabular, Image, Time-series & \cmark & \cmark \\ 
% Vijayan et al.~\cite{vijayan2022blood} & Data Preparation & Feature Extraction & Biomedical & Tabular, Sequential & \cmark & \cmark \\
% Bommert et al.~\cite{bommert2022benchmark} & Data Reduction & Feature Selection & Biomedical & Tabular, Sequential & \cmark & \cmark \\
% Espadoto et al.~\cite{espadoto2019toward} & Data Reduction & Dimensionality Reduction & Computer Graphics & Tabular, Image, Audio & \cmark & \cmark \\
% Grochowski et al.~\cite{grochowski2004comparison} & Data Reduction & Instance Selection & Computer Graphics & Tabular, Image, Audio & \cmark & \cmark \\
% Blachnik et al.~\cite{blachnik2020comparison} & Data Reduction & Instance Selection & Computer Graphics & Tabular, Image, Audio & \cmark & \cmark \\
% Iwana et al.~\cite{iwana2021empirical} & Data Augmentation & All Sub-goals & Machine Learning & Time-series & \cmark & \cmark \\
% Nanni et al.~\cite{nanni2021comparison} & Data Augmentation & Basic Manipulation & Machine Learning & Image & \cmark & \cmark \\
% Yoo et al.~\cite{yoo2020rethinking} & Data Augmentation & Basic Manipulation & Machine Learning & Image & \cmark & \cmark \\
% Ding et al.~\cite{ding2022data} & Data Augmentation & Augmentation Data Synthesis & Machine Learning & Graph & \xmark & \xmark \\
% Tao et al.~\cite{tao2021benchmarking} & Data Augmentation & Augmentation Data Synthesis & Computer Security & Tabular & \cmark & \xmark \\
% Zoller et al.~\cite{zoller2021benchmark} & Pipeline Search & Search Frameworks & Machine Learning & Tabular, Image, Audio, Time-series & \cmark & \cmark \\
% Gijsbers et al.~\cite{gijsbers2022amlb} & Pipeline Search & Search Frameworks & Machine Learning & Tabular, Image, Audio, Time-series & \cmark & \cmark \\
% \rowcolor{Gray} 
% \multicolumn{7}{c}{\textbf{\textit{Evaluation Data Development}}} \\ 
% Srivastava et al.~\cite{srivastava2022beyond} & In-distribution & Evaluation Data Synthesis & Machine Learning & Text & \cmark & \cmark \\
% Pawelczyk et al.~\cite{pawelczyk2021carla} & In-distribution & Algorithmic Recourse & Machine Learning & Tabular & \cmark & \cmark \\
% Dong et al.~\cite{dong2020benchmarking} & Out-of-distribution & Generating Adversarial Samples & Machine Learning & Image & \cmark & \cmark \\
% Hendrycks et al.~\cite{hendrycks2019benchmarking} & Out-of-distribution & Generating Adversarial Samples & Machine Learning & Image & \cmark & \cmark \\
% Yoo et al.~\cite{yoo2020searching} & Out-of-distribution & Generating Adversarial Samples  & Machine Learning & Text & \cmark & \cmark \\
% \rowcolor{Gray} 
% \multicolumn{7}{c}{\textbf{\textit{Data Maintenance}}} \\ 
% Kanthara et al.~\cite{kanthara2022chart} & Data Understanding & Data Summarization  & Machine Learning & Tabular, Text & \cmark & \cmark \\
% Grinstein et al.~\cite{grinstein2002benchmark} & Data Understanding & Data Visualization  & Human-computer Interaction & Tabular, Image & \cmark & \cmark \\
% Zeng et al.~\cite{zeng2021evaluation} & Data Understanding & Visualization Recommendation  & Human-computer Interaction & Tabular & \cmark & \xmark \\
% Jia et al.~\cite{jia2021scalability} & Data Understanding & Data Valuation & Machine Learning & Image & \cmark & \cmark \\
% Batini et al.~\cite{batini2009methodologies} & Data Quality & Quality Assessment  & Database & Tabular & \xmark & \xmark \\
% Arocena et al.~\cite{arocena2016benchmarking} & Data Quality & Quality Improvement  & Database & Tabular & \cmark & \xmark \\
% Zhang et al.~\cite{zhang2022facilitating} & Data Storage \& Retrieval & Resource Allocation  & Database & Tabular & \cmark & \cmark \\
% Marcus et al.~\cite{marcus2020benchmarking} & Data Storage \& Retrieval & Query Acceleration  & Database & Tabular & \cmark & \xmark \\
% \bottomrule
% \end{tabular}
% \end{table}

\emph{Collection strategy.} We primarily utilize Google Scholar to search for benchmark papers. Specifically, we generate a series of queries for each task using relevant keywords for the sub-goal and task, and supplement them with terms such as ``benchmark'', ``quantitative analysis'', and ``quantitative survey''. For example, the queries for the task ``data cleaning'' include ``benchmark data cleaning'', ``benchmark data cleansing'', ``quantitative analysis for data cleaning'', ``quantitative survey for data cleaning'', etc. It is worth noting that many of the queried benchmarks evaluate models rather than data. Thus, we have carefully read each paper and manually filtered the papers to ensure that they focus on the evaluation of data. We have also screened them based on the number of citations and the reputation of the publication venues.

\emph{Summary of the collected benchmarks.} Table~\ref{tbl:benchsummary} comprises the 36 benchmarks that we collected using the above process, out of which 23 incorporate open-source codes. Notably, we did not encounter a benchmark for the task of ``generating distribution shift samples'', although there are benchmarks available for \emph{detecting} distribution-shifted samples~\cite{koh2021wilds}. We omitted it from the table since it mainly assesses model performance on distribution shift rather than discussing how to create distribution-shifted data that can expose model weaknesses.

\emph{Meta-analysis.} We give a bird-eye view of existing data-centric AI research across various dimensions by analyzing these collected benchmarks. \textbf{\ding{182}} Although the AI community has made the most significant contributions to these benchmarks (17), numerous other domains have also made substantial contributions, including databases (9), computer graphics (3), human-computer interaction (2), biomedical (3), computer security (1), and healthcare (1). Notably, healthcare and biomedical are outside the realm of computer science. An established benchmark in a domain often implies that there is a collection of published works. Therefore, data-centric AI is an interdisciplinary effort that spans various domains within and outside of computer science. \textbf{\ding{183}} The most frequently benchmarked data modality is tabular data (25), followed by image (15), time-series (7), text (6), audio (6), and graph (2). We conjecture that this is because tabular and image data have been extensively studied, while research on graph data is still emerging. \textbf{\ding{184}} Training data development has received more attention, if we measure it based on the number of benchmarks (22), compared to evaluation data development (5) and data maintenance (8). We hypothesize that this is due to the fact that many of the tasks involved in training data development were considered as preprocessing steps in the model-centric paradigm.

% Cover all data-centric operations benchmark.
% Training Set.
% Evaluation Set.
% Maintainance.





% \dc{Below are some relevant information for reference}

% \dc{Data labeling.} WRENCH: A Comprehensive Benchmark
% for Weak Supervision

% \dc{Data preparation. }Several systems have been introduced over the years that provide data cleansing solutions for a variety of applications. HoloClean~\cite{rekatsinas2017holoclean}, a state of the art framework for holistic data cleaning, uses quality rules and correlations driven by probabilistic inference for cleaning data. ActiveClean~\cite{krishnan2016activeclean} is another framework, which leverages a model-centric, iterative and progressive data cleaning approach by suggesting which data samples to clean based on how much model accuracy is improved after cleaning. Talend, OpenRefine~\cite{verborgh2013using}, TARS~\cite{dolatshah2018cleaning} are examples of platforms that provide data cleaning and preparation solutions for ensuring high data quality for machine learning models. Since data cleaning is used when the dataset is dirty, biased or adversarial, MLClean~\cite{tae2019data}, a data cleaning framework aims to tackle all three problems using a unified framework for traditional data cleaning, unfairness mitigation and data sanitization.

