

\section{Data Maintenance}
\label{sec:5}
In production scenarios, data is not created once but is rather continuously updated, making data maintenance a significant challenge that must be considered to ensure reliable and instant data supply in building AI systems. This section provides an overview of the need, representative methods (as depicted in Figure~\ref{fig:datamaintenanceoverview}), and challenges of data maintenance. Our discussion spans across three aspects: data understanding (Section~\ref{sec:51}), data quality assurance (Section~\ref{sec:52}), and data storage \& retrieval (Section~\ref{sec:53}). Additionally, Table~\ref{tbl:maintainmethodsummary} summarizes the relevant tasks and methods. 

% In production scenarios, data is not created once, but rather continuously updated. Data maintenance is a significant challenge that one has to consider as data quality is the key toward successful AI solutions. As conventional model-centric paradigm focuses on ML model training and development, data is assumed to be ready for model training. Nevertheless, during AI development, we need to organize the data to ensure its reliability in the continuous and dynamic data flow of production scenarios. Otherwise, the corresponding model can be easily outdated and therefore fail the target applications. In addition, because production scenarios are often complex and fine-grained human expertise is required, it is particularly critical to enable human-in-the-loop for AI development, and therefore, visualizing the data correspondingly with comprehensive interpretation is a key to a high-quality AI solution. Last but not the least, as no data is perfect in real-world scenarios, evaluating the quality of data and developing corresponding curation strategies is a final piece of puzzle to complement the conventional model-centric paradigm. In this section, our discussion covers every aspects of data maintenance from data organization, data understanding, to data quality, and we summarize the corresponding techniques in Table~\ref{tbl:data_maintenance_summary}.




% \subsubsection{Needs for Data Maintenance.} Data maintenance serves as the very first steps in the data-centric paradigm, which controls the stability of the data source, the quality of the data and facilitate human participation during the development. In data-centric AI settings, data is a must for building AI solutions, and therefore there are three major needs from data practitioners. First, AI model developers require a large amount of data to train the models. However, given the hardware limitations, balancing the memory resources within data administration systems for storing, merging, and retrieving data is particularly important for a wide range of data-driven AI solutions in practical scenarios. Second, real-world data often comes with high-dimensional semantics, making it intractable to be understood by human. Therefore, enabling human understanding of the given data is a critical task toward human-AI collaboration. Third, data is not created once, but rather continuously updated. However, dynamics in real-world data flow are often unpredictable which can easily crash the existing data quality and lead to failure of the corresponding AI solution. Motivated by the three major needs, we conduct a systematic review according to individual needs.

% Second, dynamics in real-world data flow are often unpredictable. Especially, when an AI product is experiencing iteration, new data collection strategies may lead to significant changes in the data schema, which can easily crash the existing solution and further raise the distribution shift~\cite{najafabadi2015deep}. Motivated by the two major needs, we conduct a systematic review of existing methodologies.


% Visualize the number of papers that related to the 2.4 categories.

% List of Major References :
% 1*. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI: https://dl.acm.org/doi/pdf/10.1145/3411764.3445518
% 2. Machine Learning Testing: Survey, Landscapes and Horizons https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9000651
% 3. Data Validation in Machine Learning: https://proceedings.mlsys.org/book/2019/file/5878a7ab84fb43402106c575658472fa-Paper.pdf
% 4. https://dl.acm.org/doi/pdf/10.14778/3415478.3415562?casa_token=_kGr6XFsCSAAAAAA:pUbUi6fu_IafBcYjbwSjXsLC-nPurXjoRzR-36YJ9nGmtKkTxwX-pmSKeB3033BnY-d9j9Lr9wE
% 5. CleanML: A Study for Evaluating the Impact of Data Cleaning on ML Classification Tasks: https://arxiv.org/pdf/1904.09483.pdf

% https://laureberti.github.io/website//pub/ICDE18_tutorial_ML_to_DM_A_Round_Trip.pdf
% https://dl.acm.org/doi/pdf/10.1145/3035918.3064029
% https://dl.acm.org/doi/pdf/10.1145/3486001.3486248?casa_token=vlFJtP-sWGUAAAAA:M5RXx9QKY8DUD18XsMFcVwTsa-wXlQmX3x4hyP6KCUMRweURYieQcUpbsgomOUGz_ms8syWojIrU

% To this end, we first review the automation strategy of data administration for stable management. Then, we review the query optimization strategies for efficient and consistent data retrieval. Finally, we review the schema transformation strategies for adapting to the data flow dynamics.

% \subsubsection{Data Administration}
% % 0.75 page
% % main reference: https://dl.acm.org/doi/pdf/10.1145/3381027
% Database management systems (DMBS) are the critical component to maintain data  data from wide variety of sources. The throughput (e.g., how fast it can collect new data) and the latency (e.g., how fast it can respond to a request) are the measurement to evaluate their reliability. To facilitate big-data applications~\cite{stonebraker2013intel}, properly controls the database configuration parameters such as buffer pool size or runtime operations such as knobs for physical resource allocation (e.g., \% of CPUs) and workload admission control (e.g., multi-programming level)~\cite{duan2009tuning} is the key toward optimal performance. To this end, automated parameter tuning techniques~\cite{herodotou2020survey} is proposed to tailor the parameters dynamically based on the stored data and target applications. However, due to the system complexity and scalability, the configuration parameter space is enormous, and only very limited input data statistics can be made available for searching the automated tuning strategies. According to the application scenarios, data administration systems can be categorized into batch and stream processing systems, where batch systems collect and store data before it is fed into an analytic platform and streaming systems continuously collect data and immediately process the data for the analytic platforms. The corresponding automated tuning strategies can be classified as follows: rule-based, cost modeling, simulation-based, experiment-driven, machine learning, and adaptive.

% Rule-based strategies typically rely upon intuition, experience, data domain knowledge, and advise on best practices from domain experts or tuning guides to tune their applications. For example, Hadoop guidelines from industry~\cite{white2012hadoop} provides the intuition that the number of reduce tasks should be set to approximately 0.95 or 1.75 times the number of reduce slots available in the cluster to ensure the system tolerance for re-executing failed or slow tasks. Intuitions on memory tuning, shuffling and partition tuning are also provided by various organization such as Apache~\cite{apache} and Cloudera~\cite{yarn}. Cost modeling strategies optimize data/cost statistics to find a near-optimal operation execution plan. Starfish~\cite{herodotou2011starfish} is a pioneering and representative work that proposes a profile-predict-optimize approach for generating job profiles that include dataflow statistics (e.g., number of map output records, average record size) and cost statistics (e.g., shuffle execution time, average time to generate a record) and modeling the profiles to predict a virtual job profile for scheduling the incoming tasks.



\subsection{Data Understanding}
\label{sec:51}

To ensure proper maintenance, it is essential to first understand the data. The following discussion covers the need for data understanding techniques, ways to gain insights through visualization and valuation, and the challenges involved.

\begin{figure}[t]
  \centering
    \includegraphics[width=1.0\textwidth]{figures/data_maintenance.pdf}
  \caption{An overview of data maintenance.}
  \label{fig:datamaintenanceoverview}
\end{figure}



\subsubsection{Need for Data Understanding Techniques.}
Real-world data often comes in large volumes and complexity, which can make it difficult to understand and analyze. There are three main reasons why data understanding techniques are crucial. Firstly, comprehending a large number of raw data samples can be challenging for humans. To make it more manageable, we need to summarize the data and present it in a more concise and accessible way. Secondly, real-world data is often high-dimensional, while human perception is limited to two-or-three-dimensional space. Therefore, visualizing data in a lower-dimensional space is essential for understanding the data. Finally, it is crucial for organizations and stakeholders to understand the value of their data assets and the contribution of each data sample to the performance.


\subsubsection{Data Visualization}

Human beings are visual animals, and as such, we have a natural tendency to process and retain information presented in a pictorial and graphical format. Data visualization aims to leverage this innate human trait to help us better understand complex data.
%by presenting data points in an organized fashion — often with the aid of shapes and colors — we can quickly identify patterns, trends, and relationships that might otherwise be difficult to discern from raw or other forms of non-visualized data alone.
In what follows, we will discuss three relevant research topics: visual summarization, clustering for visualization, and visualization recommendation.

% Throughout years of development, various forms of data visualization techniques have been developed and utilized to provide a summarization of original data for the purpose of helping human agents to obtain insights via a condensed and human-friendly interface. 

\emph{Visual summarization.} Summarizing the raw data as a set of graphical diagrams can assist humans in gaining insights through a condensed interface. Despite its wide application, generating a faithful yet user-friendly summarization diagram is a non-trivial task. For example, it is hard to select the right visualization format. Radial charts (e.g., star glyphs and rose charts) and linear charts (e.g., line charts and bar charts) are two common formats for visualization. However, it is controversial which format is better. Although empirical evidence suggests that linear charts are superior to radial charts for many analytical tasks~\cite{burch2014benefits}, radial charts are often more natural and memorable~\cite{borkin2013makes}. In some cases, it is acceptable to compromise on the faithfulness of data representation in favor of enhanced memorability or space efficiency~\cite{burch2014benefits,waldner2019comparison}. For readers who are interested, \cite{desnoyers2011toward} and \cite{franconeri2021science} provide a comprehensive taxonomy of visualization formats. Although automated scripts can generate plots, the process of visual summarization often demands minimal human participation to select the most appropriate visualization formats.




% For readers  in the taxonomy of plotting formats and common misuses, two recommended sources are \cite{Desnoyers2011} and \cite{Franconeri2021}.


% Note the purpose of this section is not to crusade the idea that multiple pie charts are inferior to bar graphs, rather, we aim to emphasize the claim that generating a faithful yet user-friendly summarization diagram is a non-trivial task, and it takes a lot of cares and effort to execute it successfully. 


% However, it is also worth noting that being ``faithful yet user-friendly'' is not the only criterion when doing data summarization. Having a visual representation alone is already beneficial, given pictograms are often more memorable than raw or tabular data \cite{Wilkinson1990}. Often time, even between differing plotting formats, it is acceptable to trade faithfulness of data representation for better memorability or space efficiency \cite{borkin2013makes, burch2014benefits, waldner2019comparison}. For interested readers, \cite{Desnoyers2011} and \cite{Franconeri2021} can be two good sources for learning the taxonomy of plotting formats and what are some common misuse to avoid. 



%: a lot of things can go wrong during the process of selecting the right plotting format, processing the original data to feed the plotting, and deciding auxiliary features within the plot (e.g., legends and annotations).

% It would be impossible to exhaust the pros and cons of all different data summarization tools under the endless variants of tasks. But 

%To illustrate the potential issue, an often-quoted exemplary misuse is about multiple pie charts \cite{Kozak2015}. The pie chart is known for highlighting the comparative property of original data, making it an ideal candidate for data visualization tasks where figuring out which several sub-components are considered the most contributive is the main goal. However, pie charts are often utilized under a more complicated context, where multiple such charts are plotted: a good example of such a scenario can be ``summarizing the profit across different product categories for two consecutive years.'' 

%Despite the good look, such presentation of data has various issues for the intended task. One major goal of plotting data from two consecutive years is to extract insights related to the growth/decline across different product categories. However, with multiple pie charts, a human reader will need to process the angle difference between charts, which is often non-intuitive nor accurate \cite{Franconeri2021}. Along the same note, the pie chart itself also suffers from decreased readability when more than "a few" slices are being plotted, as it is hard to extract the comparative relationship between many slices. While employing data reduction techniques introduced in Section~\ref{sec:3:4} may mitigate the slice congestion issue, it is possible that the context of the mission is meant to be exhaustive; yet with a filtered representation of the original data, pie charts raise issues of faithfulness given a ``full pie'' is no longer a comprehensive representation. We would argue a bar chart with line overlay is more suited for the abovementioned task, as it maintains good readability with abundant product categories, remains faithful with a redacted data feed, yet highlights the growth/decline relationship between years (especially with the help of line overlay).

% While one might argue that this should be the perfect time to utilize data reduction techniques mentioned in <section anchor pending>, the context of the mission might not permit such reduction (e.g., maybe the graph is meant to be exhaustive), yet with a filtered representation of the original data, pie charts raise issues of faithfulness given a ``full pie'' is no longer a comprehensive representation of the ``full picture.'' 




\begin{table}[t]
\centering
\caption{Papers for achieving different sub-goals of data maintenance.}
\label{tbl:maintainmethodsummary}
\footnotesize
\begin{tabular}{l|l|l|l|l} \toprule
\multirow{2}{*}{\textbf{Sub-goal}} & \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Method type}} &  \multirow{2}{*}{\textbf{\shortstack[l]{Automation level/\\ participation degree }}} & \multirow{2}{*}{\textbf{Reference}} \\
~ & ~ & ~ & ~ & ~ \\
\midrule


\multirow{6}{*}{Understanding} & Visual summarization & Collaboration & Minimum & \cite{burch2014benefits,borkin2013makes,waldner2019comparison,desnoyers2011toward,franconeri2021science} \\
~ & Clustering for visualization & Automation & Learning-based & \cite{fahad2014survey} \\
~ & Visualization recommendation & Automation & Programmatic & \cite{wongsuphasawat2015voyager} \\
~ & Visualization recommendation & Automation & Learning-based & \cite{luo2018deepeye} \\
~ & Visualization recommendation & Collaboration & Partial & \cite{shen2021towardsnli,srinivasan2021snowy} \\
~ & Valuation & Automation & Learning-based & \cite{ghorbani2020distributional,agarwal2019marketplace,ghorbani2019data} \\
\cline{1-5}

\multirow{5}{*}{Quality assurance} & Quality assessment & Collaboration & Minimum/partial & \cite{sadiq2018data,xue2022knowledge,batini2009methodologies,pipino2002data} \\
~ & Quality improvement & Automation & Programmatic &  \cite{basu1995discovering,chu2013discovering,bohannon2006conditional}\\
~ & Quality improvement & Automation & Learning-based & \cite{baylor2017tfx} \\
~ & Quality improvement & Automation & Pipeline & \cite{schelter2018automating,thirumuruganathan2020data} \\
~ & Quality improvement & Collaboration & Partial & \cite{gamboa2022human,deodhar2022human,wang2021crowdsourcing,chen2020building} \\
\cline{1-5}

\multirow{6}{*}{Storage \& retrieval} & Resource allocation & Automation &  Programmatic & \cite{apache,yarn,white2012hadoop} \\
~ & Resource allocation & Automation & Learning-based & \cite{herodotou2011starfish,van2017automatic}\\
~ & Query index selection & Automation & Programmatic & \cite{sun2019end,chaudhuri1997efficient,valentin2000db2} \\
~ & Query index selection & Automation & Learning-based & \cite{pedrozo2018adaptive,sadri2020online} \\
~ & Query rewriting & Automation & Programmatic & \cite{baik2019bridging,chavan2011dbridge} \\
~ & Query rewriting & Automation & Learning-based & \cite{he2016learning,zhou2021dbmind} \\

\bottomrule
\end{tabular}
\end{table}









\emph{Clustering for visualization.} Real-world data can be high-dimensional and with complex manifold structures. As such, dimensionality reduction techniques (mentioned in Section~\ref{sec:3:4}) are often applied to visualize data in a two-or-three-dimensional space. Furthermore, automated clustering methods~\cite{fahad2014survey} are frequently combined with dimensionality reduction techniques to organize data points in a grouped, categorized, and often color-coded fashion, facilitating human comprehension and insightful analysis of the data. 


%We would like to add that should a clustering method be utilized in combination with some data reduction techniques — e.g., ones introduced in Section <section anchor pending> — extra prudence is required in selecting the subsequent clustering method: assumed if a lossy data reduction method is used where distance information between data points is distorted or lost, one should be cautious of applying distance-based clustering method directly onto the transformed data points.



%Clustering techniques enable data points to be presented by a grouped, categorized, and often color-coded fashion upon similar characteristics, which empowers human agents to digest information and gain insights from data with large volume and complex manifold structures. There are countless ways to decide the taxonomy of clustering methods; a tested way can be the partitional/distance, density, hierarchical/tree, grid, and model-based system introduced in \cite{fahad2014survey}, each describing the metric that drives the cluster formation.
% Clustering is an essential data visualization tool that leverages this innate human trait to help us better understand complex data: by presenting data points in a grouped and categorized fashion upon similar characteristics, we can quickly identify patterns, trends, and relationships that might otherwise be difficult to discern from raw or other forms of non-visualized data alone.



% Note this system is by no mean the ``exclusively right taxonomy,'' as many popular clustering methods may lay within multiple categories (e.g., DBSCAN utilizes both distance and density-based criteria), yet many high-traction clustering methods have some variants that may put or extend them into another group (e.g., DBSCAN $\rightarrow$ HDBSCAN extends from a distance/density-based method to the hierarchical-based space).





% This is both because no clustering method may perform well under all conditions <citation pending>; yet given the different constraints imposed by different tasks and resources, often time the human agent will not even have the freedom to utilize many clustering methods — e.g., <citation pending> shows classic model-based clustering methods like EM often perform well in terms of clustering quality, but it also require more compute resource to execute.




% The same \cite{fahad2014survey} introduced a data-centric decision-making process that enables a human agent to select a suitable clustering method by considering the volume\footnote{Which mostly refers to the intended dataset's size, dimensionality, and noisiness.} and variety\footnote{Here, it mostly means the nature of data types (e.g., numerical or categorical) and manifold shapes the produced clusters are able to capture.} of data with against efficiency and user-friendliness of clustering algorithms. 


\emph{Visualization recommendation.} Building upon various visualization formats, there has been a surge of interest  in visualization recommendation, which involves suggesting the most suitable visualization formats for a particular user. Programmatic automation approaches rank visualization candidates based on predefined rules composed of human perceptual metrics such as data type, statistical information, and human visual preference~\cite{wongsuphasawat2015voyager}. Learning-based approaches exploit various machine learning techniques to rank the visualization candidates. An example of such a method is DeepEye~\cite{luo2018deepeye}, which utilizes the statistical information of the data as input and optimizes the normalized discounted cumulative gain (NDCG) based on the quality of the match between the data and the chart. Collaborative visualization techniques allow for a more adaptable user experience by enabling users to continuously provide feedback and requirements for the visualization~\cite{shen2021towardsnli}. A recent study, Snowy~\cite{srinivasan2021snowy} accepts human language as input and generates recommendations for utterances during conversational visual analysis. As visualizations are intended for human users, allowing for human-in-the-loop feedback is crucial in developing visualization recommender systems.

\subsubsection{Data Valuation}

The objective of data valuation is to understand how each data point contributes to the final performance. Such information not only provides valuable insights to stakeholders but is also useful in buying or selling data points in the data market and credit attribution~\cite{ghorbani2020distributional}. To accomplish this, researchers estimate the Shapley value of the data points, which assigns weights to each data point based on its contribution~\cite{agarwal2019marketplace,ghorbani2019data}. A subsequent study has enhanced the robustness of this estimation across multiple datasets and models~\cite{ghorbani2020distributional}. Since calculating the exact Shapley value can be computationally expensive, especially when dealing with a large number of data points, the above methods all adopt learning-based algorithms for efficient estimation.

% Under the realm of DCAI, parties buy and sell data points in a routine manner \cite{ghorbani2020distributional}. Thus, how should the seller and buyer decide, and even agreed upon, the pricing issue of data is an important area of study called data valuation. The most typical way of data valuation is simple: if a datum is capable of enabling a processing system to achieve certain performance criteria the buyers desires — which usually translate to training a model to achieve some desirable accuracy-related metrics — then each data point within that datum worth an equally divided share of the worth of the problem the buyer is trying to solve.

%Some recent developments in the data valuation evaluate data points via a more fine-grained lens, where each data point is weighted differently according to their contribution to the model performance. \textit{Shapley value} \cite{Shapley1952} and its variants are considered popular tools for this very propose \cite{agarwal2019marketplace, ghorbani2019data}, and more sophisticated methods like \cite{ghorbani2020distributional} are developed to make individual data point contribution measuring more robust under the background of multiple datasets and models.

\subsubsection{Challenges} 
There are two major challenges. Firstly, the most effective data visualization formats and algorithms (e.g., clustering algorithms) are often specific to the domain and influenced by human behavior, making it difficult to select the best option. This selection process often requires human input. Determining how to best interact with humans adds an additional complexity. Secondly, developing efficient data valuation algorithms is challenging, since estimating the Shapley value can be computationally expensive, especially as data sizes continue to grow. Additionally, the Shapley value may only offer a limited perspective on data value, as there are many other important factors beyond model performance, such as the problems that can be addressed through training a model on the data.



% References
% 1. https://www.nature.com/articles/nbt.4314
% 2. https://dl.acm.org/doi/pdf/10.1145/3366423.3380058?casa_token=G6FavA1Yq8IAAAAA:BX-iLKmkbUSTFVWfvkZYP78wmaS9U6MbZWHEIjTEvXiMk6Fvim7DSRGeem7Uiy2dW8qIPwwQ85I
% 3. https://www.nature.com/articles/s41587-019-0336-3
% 4. 
% \dc{Shaochen, 0.75 page}
% Automation
% unsupervised clustering 
% dimensionalality reduction
% Data interpretation provides experimental design strategies to study the data for further maintenance. As real-world data comes with numerous attributes, various experimental design strategies are developed for maintaining data from different domains such as regression analysis for oceanology~\cite{lemenkova2019testing}, heuristic-inspired optimization algorithm for geophycis~\cite{peters2019neural}, and interactive parameter study for biology~\cite{taverna2020biomex}.

% The point of having data collected in the first place is to interpret such data to complete a series of proposed tasks. Thus, it is important to understand what kind data interpretation techniques are available to use, and what are the development trends among relevant fields.

% Data interpretation is, in fact, one of the widest definition, as technically speaking the entire machine learning paradigm can be viewed as a family of data interpretation technique. However, for the purpose of this section, we will mainly focus on the following two 

% In modern days, data are being generated and collected at an unprecedented pace thanks to the rapid grows consumer consumption and the development of data collection technology. With more data being collected, one natural challenge that comes with it is the increased burden of processing, analyzing, and ultimately understanding such data to complete the intended tasks. To alleviate such a burden, one family of techniques, which we categorize as \textit{data simplification}, comes to provide helpful remedies.


% Data simplification, as its name implies, simplifies the raw data to help various involved parties to extract meaningful insights; where such parties can be a human decision maker or a computing module. Data simplification techniques come in various formats, but most of them revolve around the same idea: reducing the volume and complexity of original data. Here we will summarize some data simplification offerings in a categorical manner.\newline


% \textit{Data Sampling} and \textit{Feature Selection} are probably the two most intuitive techniques to simplify raw data, where the former aims to reduce the amount of entries of data, yet the latter aims to reduce the number of features from each data point. With careful execution, such simple methods may not only reduce the processing burden on all parties involved but also improve the analysis quality downstream. This is because such techniques may remove undesired or noisy components from the original dataset (e.g., to remove incomplete data entries or sensitive features that may induce fairness issues). Successful data sampling and feature selection techniques often aim to select a subset of available data points and features most relevant to the intended tasks. Under the context of DCAI, many sampling techniques are developed to tackle some common tasks and challenges faced by typical AI systems, e.g., simple random sampling and stratified sampling for in-distribution learning, weighted and under/over-sampling for imbalanced datasets, randomized algorithm-based sampling for efficient learning, etc.

% \textit{Data Aggregation} is a family of techniques that aim to clean and organize the raw data by combining its elements at different granularities. Unlike data sampling and feature selection techniques, data aggregation techniques often do not remove the information but rather aggregate existing information together into fewer instances or feature categories. Again, such aggregation can be performed to help a human decision-maker to digest the raw data better — e.g., by aggregating multiple daily reports into weekly ones — or it can be conducted as a step within an algorithmic procedure — e.g., by binning variables into fewer groups to reduce the computational cost, (potentially) obtain better representation out of the raw data, or to address the need of a specific algorithmic module (e.g., tree-based models).

% \textit{Dimensionality Reduction} (DR) is a popular field of study that aim to transform high-dimensional data into a lower-dimensional space. While it can be argued that all the abovementioned data simplification techniques achieve this very same goal, dimensionality reduction techniques often do so with the focus on preserving as much information as possible, even at the cost of losing the semantics of original features. In contrast, techniques like data sampling will simply forgo some amount of information, yet data aggregation operations often inherit the semantics from the original feature set.
 
% Popular DR techniques can be categorized into two groups: linear DR and nonlinear DR. The former group creates new data instances via the linear combination of features from the original data; the latter group explores nonlinear structures of the original data by mapping them into a lower-dimensional space via nonlinear functions. Classic linear DR techniques include Principle Component Analysis (PCA), Singular Value Decomposition (SVD), and Linear Discriminant Analysis (LDA); where $k$-PCA, Isomap, $t$-SNE, and Autoencoders are considered popular nonlinear DR methods.\newline

% It may be worth noting that outside the impact of the research community, many data simplification techniques are utilized in industrial applications and embedded in our everyday life. e.g., the JPEG image standard utilized a modified PCA approach (a lossy form DCT) to achieve image compression; popular video compression formats like H.264 also utilize SVD to exploit the invariant between two continuous frames. On the human side, $t$-SNE combined with different clustering techniques and data aggregation with heat maps are probably two of the most adopted visualization procedures to help human decision-makers capture insights from complicated events and make informed decisions.

% % If we'd put visualization after this, I'd add a bit more transition here — Henry Z.


\subsection{Data Quality Assurance}
\label{sec:52}
To ensure a reliable data supply, it is essential to maintain data quality. We will discuss why quality assurance is necessary, the key tasks involved in maintaining data quality (quality assessment and improvement), and the challenges.



\subsubsection{Need for Data Quality Assurance}

In real-world scenarios, data and the corresponding infrastructure for data processing are subject to frequent and continuous updates. As a result, it is important not only to create high-quality training or inference data once but also to maintain their excellence in a dynamic environment. Ensuring data quality in such a dynamic environment involves two aspects. Firstly, continuous monitoring of data quality is necessary. Real-world data in practical applications can be complex, and it may contain various anomalous data points that do not align with our intended outcomes. As a result, it is crucial to establish quantitative measurements that can evaluate data quality. Secondly, if a model is affected by low-quality data, it is important to implement quality improvement strategies to enhance data quality, which will also lead to improved model performance.


\subsubsection{Quality Assessment}

Quality assessment develops evaluation metrics to measure the quality of data and detect potential flaws and risks. These metrics can be broadly categorized as either objective or subjective assessments~\cite{sadiq2018data,xue2022knowledge,batini2009methodologies,pipino2002data}. Although objective and subjective assessments may require different degrees of human participation, both of them are used in each paper we surveyed. Thus, we tag each paper with more than one degree of human participation in Table~\ref{tbl:maintainmethodsummary}. We will discuss these two types of assessments in general and provide some representative examples of each.

Objective assessments directly measure data quality using inherent data attributes that are independent of specific applications. Examples of such metrics include accuracy, timeliness, consistency, and completeness. Accuracy refers to the correctness of obtained data, i.e., whether the obtained data values align with those stored in the database. Timeliness assesses whether the data is up-to-date. Consistency refers to the violation of semantic rules defined over a set of data items. Completeness measures the percentage of values that are not null. All of these metrics can be collected directly from the data, requiring only minimal human participation to specify the calculation formula.

Subjective assessments evaluate data quality from a human perspective, often specific to the application and requiring external analysis from experts. Metrics like trustworthiness, understandability, and accessibility are often assessed through user studies and questionnaires. Trustworthiness measures the accuracy of information provided by the data source. Understandability measures the ease with which users can comprehend collected data, while accessibility measures users' ability to access the data. Although subjective assessments may not directly benefit model training, they can facilitate easier collaboration within an organization and provide long-term benefits. Collecting these metrics typically requires full human participation since they are often based on questionnaires.


\subsubsection{Quality Improvement}
% Mainly about data collection procedure
% Major reference: “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI: https://dl.acm.org/doi/pdf/10.1145/3411764.3445518
% https://dl.acm.org/doi/pdf/10.14778/3229863.3229867?casa_token=9RMYy-44YesAAAAA:0isvVQcE8zWI7EJH45Xkx1NvqAGR6E6S9K5p-jLxCWqVA0jKmgtN_N1XId8OuLL4RWlKKqS1Gxs
Quality improvement involves developing strategies to enhance the quality of data at various stages of a data pipeline. Initially, programmatic automation methods are used to enforce quality constraints, including integrity constraints~\cite{basu1995discovering}, denial constraints~\cite{chu2013discovering}, and conditional functional dependencies~\cite{bohannon2006conditional} between columns. More recently, machine learning-based automation approaches have been developed to improve data quality. For instance, in~\cite{baylor2017tfx}, a data validation module trains a machine learning model on a training set with expected data schema and generalizes it to identify potential problems in unseen scenarios. Furthermore, pipeline automation approaches have been developed to systematically curate data in multiple stages of the data pipeline, such as data integration and data cleaning~\cite{schelter2018automating,thirumuruganathan2020data}. 

Apart from automation, collaborative approaches have been developed to encourage expert participation in data improvement. For example, in autonomous driving~\cite{gamboa2022human} and video content reviewing~\cite{deodhar2022human}, human annotations are continuously used to enhance the quality of training data with the assistance of machine learning models. Moreover, UniProt~\cite{wang2021crowdsourcing}, a public database for protein sequence and function literature, has created a systematic submission system to harness collective intelligence~\cite{chen2020building} for data improvement. This system automatically verifies meta-information, updated versions, and research interests of the submitted literature. All of these methods necessitate partial human participation, as humans must continuously provide information through annotations or submissions.


\subsubsection{Challenges}

Ensuring data quality poses two main challenges. Firstly, selecting the most suitable assessment metric is not a straightforward task and heavily relies on domain knowledge. A single metric may not always be adequate in a constantly evolving environment. Secondly, quality improvement is a vital yet laborious process that necessitates careful consideration. Although automation is crucial in ensuring sustainable data quality, human involvement may also be necessary to ensure that the data quality meets human expectations. Therefore, data assessment metrics and data improvement strategies must be thoughtfully designed.




\subsection{Data Storage \& Retrieval}
\label{sec:53}

Data storage and retrieval systems play an indispensable role in providing the necessary data to build AI systems. To expedite the process of data acquisition, various efficient strategies have been proposed. In the following discussion, we elaborate on the importance of efficient data storage and retrieval, review some representative acceleration methods for resource allocation and query acceleration, and discuss the challenges associated with them.


% need: why this is important --> 2~3 sentences.
\subsubsection{Need for Efficient Data Storage \& Retrieval}
As the amount of data being generated continues to grow exponentially, having a robust and scalable data administration system that can efficiently handle the large data volume and velocity is becoming increasingly critical to support the training of AI models. This need encompasses two aspects. Firstly, data administration systems, such as Hadoop~\cite{hadoop} and Spark~\cite{ZahariaXinEtAl16cacm}, often need to store and merge data from various sources, requiring careful management of memory and computational resources. Secondly, it is crucial to design querying strategies that enable fast data acquisition to ensure timely and accurate processing of the data.


\subsubsection{Resource Allocation} Resource allocation aims to estimate and balance the cost of operations within a data administration system. Two key efficiency metrics in data administration systems are throughput, which refers to how quickly new data can be collected, and latency, which measures how quickly the system can respond to a request. To optimize these metrics, various parameter-tuning techniques have been proposed, including controlling database configuration settings (e.g., buffer pool size) and runtime operations (e.g., percentage of CPU usage and multi-programming level)~\cite{duan2009tuning}. Early tuning methods rely on rules that are based on intuition, experience, data domain knowledge, and industry best practices from sources such as Apache~\cite{apache} and Cloudera~\cite{yarn}. For instance, Hadoop guidelines~\cite{white2012hadoop} suggest that the number of reduced tasks should be set to approximately 0.95 or 1.75 times the number of reduced slots available in the cluster to ensure system tolerance for re-executing failed or slow tasks.

Various learning-based strategies have been developed for resource allocation in data processing systems. For instance, Starfish~\cite{herodotou2011starfish} proposes a profile-predict-optimize approach that generates job profiles with dataflow and cost statistics, which are then used to predict virtual job profiles for task scheduling. More recently, machine learning approaches such as OtterTune~\cite{van2017automatic} have been developed to automatically select the most important parameters, map workloads, and recommend parameters to improve latency and throughput. These learning-based automation strategies can adaptively balance system resources without assuming any internal system information.


\subsubsection{Query Acceleration}
% % 0.75 page
% https://link.springer.com/article/10.1007/s00778-021-00676-3
% Database management systems provide data access and storage, which enables features such as rich connectivity, transactions, versioning, security, auditing, high-availability, and application/tool integration. 
Another research direction is efficient data retrieval, which can be achieved through efficient index selection and query rewriting strategies.

\emph{Query index selection.} The objective of index selection is to minimize the number of disk accesses needed during query processing. To achieve this, programmatic automation strategies create an indexing scheme with indexable columns and record query execution costs~\cite{sun2019end}. Then, they apply either a greedy algorithm~\cite{chaudhuri1997efficient} or dynamic programming~\cite{valentin2000db2} to select the indexing strategy. To enable a more adaptive and flexible querying strategy, learning-based automation strategies collect indexing data from human experts and train machine learning models to predict the proper indexing strategies~\cite{pedrozo2018adaptive}, or search for the optimal strategies using reinforcement learning~\cite{sadri2020online}.

\emph{Query rewriting.} In parallel, query rewriting aims to reduce the workload by identifying repeated sub-queries from input queries. Rule-based strategies~\cite{baik2019bridging,chavan2011dbridge} rewrite queries with pre-defined rules, such as DBridge~\cite{chavan2011dbridge}, which constructs a dependency graph to model the data flow and iteratively applies transformation rules. Learning-based approaches use supervised learning~\cite{he2016learning} or reinforcement learning~\cite{zhou2021dbmind} to predict rewriting rules given an input query.

\subsubsection{Challenges}
Existing data storage and retrieval methods typically focus on optimizing specific parts of the system, such as resource allocation and query acceleration we mentioned. However, the real data administration system as a whole can be complex since it needs to process a vast amount of data in various formats and structures, making end-to-end optimization a challenging task. Additionally, apart from efficiency, data storage and retrieval require consideration of several other crucial and challenging aspects, such as data access control and system maintenance.



% \subsubsection{Data Integration}
% % 0.5 page
% Learning to transform schema with ML



% \subsubsection{Data Warehousing}
% Data warehousing store the collected data from operational databases into centralized warehouses. In data warehouses, data is written with schema-on-write for creating an optimized data model for downstream analysis purposes. 


% \subsubsection{Metadata Management}
% Metadata describes the meaning or properties of data for better management, understand and use of data. It can either be passively used for providing consistent documentation on the structure and development of warehouses, actively used for storing executable transformation rules of data as code fractions, or semi-actively used for storing static information about software modules and attributes of stored data. 
% Metadata management is a layer of operation on top of data warehouses, which aims at minimizing the efforts for development and administration of a data warehouse and improving the information extraction of data. It reduces warehouse development and administration efforts by generating metadata as the communication protocol to support system flexibility in integrating with different tools (e.g., ETL, OLAP, CASE-tools), reusing existing software modules, and enforcing an automated and secure administration process. It also improves the information extraction from data by generating metadata as the description of data to provide consistent documentation of data for quality assurance, terminology and concepts used by the target domain for accurate and efficient queries, and representation and summarization of data for further analysis. 

% \subsubsection{Data Versioning}
% Data versioning records the changes to a dataset in the course of its life cycle to provide traceable errors and reproducible results for the development of AI systems. Versioning data requires considering the following issues: the constitution of a new release, the definition of significant changes to a dataset, the associated changes to the metadata modifications, the granularity of changes for versioning, and the components and labels of version history. 

% \subsection{Data Quality}
% Data quality is the key toward qualitied AI solution. There are two main task for ensuring the data quality: \textbf{quality assessment and quality intervention}.






% \subsection{Data Understanding}
% % Data Perf, Data Understanding, Valuation
% % 6. 
% \subsection{Data Storage}

% % No automation
% Keywords that we could look into: valuation, label errors, data explainability (see Explainable AI (XAI): Core Ideas, Techniques and Solutions)

% [Comments from Zaid] 
% 1. Quality assurance: 
% Inspection (manual usually - like violence in video footage)
% Introspection (algorithmic exploration and adjustment of datasets- find edge cases in data, errors/ambiguous data labelling)

% 2. Data Storage - tools used??

% 	Scalability
% 	Cost
% 	Architecture
% 	Durability
% 	Cloud integration

% 3. Maintenance

% 4. Distribution

% With the ever-growing size and complexity of data, it is essential to develop efficient infrastructures, algorithms, and tools to store, understand, and debug data.

% \textbf{Data Organization.} Data organization provides reliable data source for continuous development and debugging of AI solutions. There are three key components toward reliable data source: \textbf{data warehousing, metadata management and data version control}. \textbf{Data warehousing} store the collected data from operational databases into centralized warehouses. In data warehouses, data is written with schema-on-write for creating an optimized data model for downstream analysis purposes. 
% Metadata describes the meaning or properties of data for better management, understand and use of data. It can either be passively used for providing consistent documentation on the structure and development of warehouses, actively used for storing executable transformation rules of data as code fractions, or semi-actively used for storing static information about software modules and attributes of stored data. 
% Metadata management is a layer of operation on top of data warehouses, which aims at minimizing the efforts for development and administration of a data warehouse and improving the information extraction of data. It reduces warehouse development and administration efforts by generating metadata as the communication protocol to support system flexibility in integrating with different tools (e.g., ETL, OLAP, CASE-tools), reusing existing software modules, and enforcing an automated and secure administration process. It also improves the information extraction from data by generating metadata as the description of data to provide consistent documentation of data for quality assurance, terminology and concepts used by the target domain for accurate and efficient queries, and representation and summarization of data for further analysis. 
% \textbf{Metadata} describes the meaning or properties of data for better management, understand and use of data. Metadata management reduces warehouse development and administration efforts by generating metadata as the communication protocol to support system flexibility in integrating with different tools (e.g., ETL, OLAP, CASE-tools), reusing existing software modules, and enforcing an automated and secure administration process. It also improves information extraction from data by generating metadata as the description of data to provide consistent documentation of data for quality assurance, terminology and concepts used by the target domain for accurate and efficient queries, and representation and summarization of data for further analysis. 
% \textbf{Data version control} records the changes to a dataset in the course of its life cycle to provide traceable errors and reproducible results for the development of AI systems. Versioning data requires considering the following issues: the constitution of a new release, the definition of significant changes to a dataset, the associated changes to the metadata modifications, the granularity of changes for versioning, and the components and labels of version history. 
% DVC~\cite{ruslan_kuprieiev_2022_7440136} is one of the widely-used industry solution to versioning datasets and their corresponding ML solutions with the Git protocol.

% https://eva.fing.edu.uy/pluginfile.php/338191/mod_resource/content/1/cidr2021_lakehouse.pdf

% \textbf{Data Understanding.}
% % https://dl.acm.org/doi/pdf/10.1145/3313831.3376485?casa_token=tSS_o8yIeJUAAAAA:yAbXyRnJWtHboKms2pjs1fllqKzWi2YgMGYOOf6cTRl3d0nlMWMGLSJ9Z2x523P6p2AKX6KBHeYh
% Data understanding helps maintaining the quality of datasets via various data visualization tools and data interpretation methodologies. \textbf{Data visualization} tools has developed for warehouse system debugging, and data debugging. To debug the warehouse system, entity-relation diagram is exploited to show the relationship between data schema, and graphical representations are developed to visualize logical structure within the queries and their corresponding outputs and the behaviors of query operators. To debug the data, systems with graphical interfaces~\cite{kandel2012enterprise} have been proposed to summarize the data for data profiling; while query analysis methods and probability inference are developed for data error identification and cleaning.
% \textbf{Data interpretation} provides experimental design strategies to study the data for further maintenance. As real-world data comes with numerous attributes, various experimental design strategies are developed for maintaining data from different domains such as regression analysis for oceanology~\cite{lemenkova2019testing}, heuristic-inspired optimization algorithm for geophycis~\cite{peters2019neural}, and interactive parameter study for biology~\cite{taverna2020biomex}.

% Additionally, historical query analysis strategies~\cite{wang2016qfix} and probability inference methods~\cite{rekatsinas2017holoclean} are developed for data error identification and data cleaning. 




% \textbf{Data Quality.}
% % https://dl.acm.org/doi/pdf/10.1145/505248.506010?casa_token=eZD1yayuudAAAAAA:RpJggn_jaPpeH_PPucmn04E6RMPLrOR45GvsAOCLj021IFel9-vMn0OCnxjcu3GzkJvWDlgDdtS1
% Data quality is the key toward qualitied AI solution. There are two main task for ensuring the data quality: \textbf{quality assessment and quality intervention}. \textbf{Quality assessment} develops evaluation metrics to measure the quality of current data and identify potential flaws and risks in data. Data quality metrics can be categorized into objective and subjective. Objective metrics directly measure multiple ratios (e.g., error rate) based on the dataset itself; while subjective metrics, such as reputation and relevancy, reflect the needs and experiences of stakeholders. \textbf{Quality intervention} performs multiple initiatives to affect stages in a data pipeline such as data collection and data preparation. The intervention methods can either be performed by domain experts (e.g., auditing and feedback), collective intelligent (e.g. majority voting), or a set of user-defined rules (e.g., data unit-test).

