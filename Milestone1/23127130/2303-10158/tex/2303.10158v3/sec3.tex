\begin{figure}[t]
  \centering
    \includegraphics[width=\textwidth]{figures/training_data_development.pdf}
  \caption{An overview of training data development. Note that the figure illustrates only a general pipeline, and not all steps are mandatory. For instance, unsupervised learning does not require data labeling. These steps can be executed in a different order as well. For example, data augmentation can occur before data reduction.}
  \label{fig:trainingdevelopmentoverview}
\end{figure}

\section{Training Data Development}
\label{sec:3} 

Training data provides the foundation for machine learning models, as the model performance is heavily influenced by its quality and quantity. In this section, we summarize the essential steps to create and process training data, visualized in Figure~\ref{fig:trainingdevelopmentoverview}. Data creation focuses on effectively and efficiently encoding human intentions into datasets, including data collection~(Section~\ref{sec:3:1}) and data labeling~(Section~\ref{sec:3:2}). Data processing aims to make data suitable for learning, including data preparation~(Section~\ref{sec:3:3}), data reduction~(Section~\ref{sec:3:4}), and data augmentation~(Section~\ref{sec:3:5}). After introducing these steps, we discuss pipeline search~(Section~\ref{sec:3:6}), an emerging trend that aims to connect them and search for the most effective end-to-end solution. Table~\ref{tbl:trainingmethodsummary} summarizes the representative tasks and methods for training data development.







\subsection{Data Collection}
\label{sec:3:1}

Data collection is the process of gathering and acquiring data from various sources, which fundamentally determines data quality and quantity. This process heavily relies on domain knowledge. With the increasing availability of data, there has been a surge in the development of efficient strategies to leverage existing datasets. In the following, we discuss the role of domain knowledge, an overview of more efficient data collection strategies, and challenges.





% Dataset discovery involves searching for and utilizing existing datasets, which can be shared through data marketplaces such as Quandl and CKAN or collaborative platforms like Kaggle and DataHub. These have made it easier to publish, share and use data. Data lakes and web-based dataset search engines are other useful tools for discovering and accessing large datasets. Data lakes such as Google Dataset Search(GOODS)[], which are commonly used by large corporations, allow for the storage and organization of diverse data types. Google Dataset Search[] and WebTables[] are examples of online platforms that can be used to find publicly available datasets on the web. These resources can help researchers locate and utilize relevant data for their projects.
%Data integration involves combining multiple datasets to create a larger, more diverse dataset. The growing adoption of machine learning in diverse fields has led to an increased demand for raw data to train models. However, the availability of high-quality, relevant data remains a challenge, especially for novel or specialized applications. When these methods are insufficient, it may be necessary to construct a new dataset from scratch. This can be done manually through crowdsourcing platforms like Amazon Mechanical Turk, or automatically using synthetic data generation techniques, which are particularly useful for creating text and image data. However, manual data construction can be costly and time-consuming, making synthetic data generation an attractive alternative for some applications especially in the computer vision and natural language processing community.


% The vast majority of success and advances in Machine Learning over the past decade can be largely attributed to high quality, well-prepared data. Without high-quality data, even the most complex and best performing machine learning algorithms will fail to guarantee good performance. Therefore, data quality data remains a cornerstone for the future of machine learning development. Data collection is a crucial step in the development of machine learning models, and it relies heavily on expert knowledge of the domain in question. There are several methods for acquiring training data, including dataset discovery, data integration, and raw data construction. In the process of data collection, it is important to consider the methods used to gather the data and the assumptions made about the data's relevance to the task at hand, which requires significant domain knowledge about the task. These factors can impact the quality and usefulness of the dataset for training machine learning models.



\subsubsection{Role of Domain Knowledge}
A deep understanding of the application domain or industry is critical for collecting relevant and representative data. For example, when building a recommendation system, it is crucial to decide what user/item features to collect based on the application domain~\cite{zhang2019deep}. The domain-specific knowledge can also help in synthesizing data. For instance, knowledge about financial markets and trading strategies can facilitate the generation of more realistic synthetic anomalies~\cite{lai2021revisiting}. Domain knowledge is essential for effective data collection since it helps align data with the intentions of stakeholders and ensure the data is relevant and representative.


\subsubsection{Efficient Data Collection Strategies}
Traditionally, datasets are constructed from scratch by manually collecting the relevant information. However, this process is time-consuming. More efficient methods have been developed by leveraging the existing data. Here, we describe the methods for dataset discovery, data integration, and data synthesis.


%\emph{Crowdsourced collection.} Removed for now. All of them seem to be data labeling.
%Giving rise to cloud services, crowdsourcing breaks down a complex data collection task into smaller and more manageable parts so that they can be distributed to workers over the Internet. The existing methods all require full human participation but assist humans in different ways.

%The standard method for manual data construction is crowdsourcing. One of the earliest and most popular crowdsourcing platforms, Amazon Mechanical Turk(AMT)~\cite{amazonmt}, allows researchers and businesses to outsource data construction tasks to a large, on-demand workforce. Most use cases of AMT post a large number of tasks that can be run independently. Similar platforms such as Upwork~\cite{upwork}, Freelancer~\cite{freelancer} also provide a cost-efficient way to collect data from a diverse group of people. TurKit~\cite{little2010turkit}, a toolkit for exploring truly algorithmic human computation, allows users to write algorithms in an imperative programming style which abstracts AMT as a function call. The first attempt made to fully automate crowdprogramming systems was made by AUTOMAN~\cite{barowy2016automan}, integrating human computations as ordinary function calls into a standard programming language. For structured data, most of the previous works[][] have focussed on microtask-based approach. Crowdfill~\cite{park2014crowdfill} takes a different approach, showcasing partially filled table to all participating workers, where workers fill in the empty cells while upvoting/downvoting responses from other workers. Another major application of crowdsourcing, for instance in the ImageNet project, is in data labeling, which is discussed in the Data Labeling section.



\emph{Dataset discovery.} As the number of available datasets continuously grows, it becomes possible to amass the existing datasets of interest to construct a new dataset that meets our needs. Given a human-specified query (e.g., the expected attribute names), dataset discovery aims to identify the most related and useful datasets from a data lake, a repository of datasets stored in
its raw formats, such as public data-sharing platforms~\cite{bhardwaj2015datahub} and data marketplaces. The existing research for dataset discovery mainly differs in calculating relatedness. A representative strategy is to abstract the datasets as a graph, where the nodes are columns of the data sources, and edges represent relationships between two nodes~\cite{fernandez2018aurum}. Then a tailored query language is designed to allow users to express complex query logic to retrieve the relevant datasets. Another approach is table union search~\cite{nargesian2018table}, which measures the unionability of datasets based on the overlapping of the attribute values. Recent work measures the relatedness in a more comprehensive way by considering attribute names, value overlapping, word embedding, formats, and domain distributions~\cite{bogatu2020dataset}. All these methods can significantly reduce human labor in dataset discovery, as humans only need to provide queries.






\emph{Data integration.} Given a few datasets from different sources, data integration aims to combine them into a unified dataset. The difficulty lies in matching the columns across datasets and transforming the values of data records from the source dataset to the target dataset. Traditional solutions rely on rule-based systems~\cite{lenzerini2002data,kumar2016join}, which can not scale. Recently, machine learning has been utilized to automate the data integration process in a more scalable way~\cite{stonebraker2013data,stonebraker2018data}. For example, the transformation of data values can be formulated as a classification problem, where the input is the data value from the source dataset, and the output is the transformed value from the target dataset~\cite{stonebraker2018data}. Then we can train a classifier with the training data generated by rules and generalize it to unseen data records. The automated data integration techniques make it possible to merge a larger number of existing datasets efficiently.

\emph{Raw data synthesis.} In some scenarios, it is more efficient to synthesize a dataset that contains the desirable patterns than to collect these patterns from the real world. A typical scenario is anomaly detection, where it is often hard to collect sufficient real anomalies since they can be extremely rare. Thus, researchers often insert anomaly patterns into anomaly-free datasets. For example, a general anomaly synthesis criterion has been proposed for time series data~\cite{lai2021revisiting}, where a time series is modeled as a parameterized combination of trend, seasonality, and shapelets. Then different point- and pattern-wise anomalies can be generated by altering these parameters. However, such synthesis strategies may not be suitable for all domains. For example, the anomaly patterns in financial time series can be quite different from those from electricity time series. Thus, properly designing data synthesis strategies still requires domain knowledge.
 

%\cite{DOAN2012datait2,kumar2016datait3,lenzerini2002datait1,Stonebraker2018DataIT} involves combining multiple datasets to create a larger, more diverse dataset. Data integration poses several challenges related to technical, organizational and quality of datasets combined. Datasets, from multiple sources, may come in multiple formats making it challenging to integrate datasets. Data warehousing ~\cite{calvanese2001warehouse,batini1986integration}, one of the ways to achieve data integration leverages data warehouses to store, integrate and use data from multiple sources. Another type of data integration, data virtualization~\cite{muni2019virtual}, integrates data from multiple sources into a logical view without replicating or moving data physically. CrowdDB [49] and Qurk [50] are examples of systems for aggregating, sorting, and joining datasets. Data Tamer~\cite{Stonebraker2013DataCA}, an end-end data curation system, can semantically integrate multiple datasets.




			

			





\subsubsection{Challenges}
Data collection is a very challenging process that requires careful planning. From the technical perspective, datasets are often diverse and not well-aligned with each other, so it is non-trivial to measure their relatedness or integrate them appropriately. Effectively synthesizing data from the existing dataset is also tricky, as it heavily relies on domain knowledge. Moreover, some critical issues during data collection can not be resolved solely from a technical perspective. For example, in many real-world situations, we may be unable to locate a readily available dataset that aligns with our requirements so we still have to collect data from the ground up. However, some data sources can be difficult to obtain due to legal, ethical, or logistical reasons. Collecting new data also involves ethical considerations, particularly with regard to informed consent, data privacy, and data security. Researchers and practitioners must be aware of these challenges in studying and executing data collection.





\begin{table}[t]
\footnotesize
\centering
\setlength{\tabcolsep}{3.5pt}
\caption{Papers for achieving different sub-goals of training data development.}
\label{tbl:trainingmethodsummary}
\begin{tabular}{l|l|l|l|l} \toprule
\multirow{2}{*}{\textbf{Sub-goal}} & \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Method type}} &  \multirow{2}{*}{\textbf{\shortstack[l]{Automation level/\\ participation degree }}} & \multirow{2}{*}{\textbf{Reference}} \\
~ & ~ & ~ & ~ & ~ \\
\midrule

\multirow{4}{*}{Collection} & Dataset discovery & Collaboration  &  Minimum & \cite{fernandez2018aurum,nargesian2018table,bogatu2020dataset} \\
~ & Data integration & Automation & Programmatic &  \cite{lenzerini2002data,kumar2016join} \\
~ & Data integration & Automation & Learning-based & \cite{stonebraker2013data,stonebraker2018data} \\
~ & Raw data synthesis & Automation & Programmatic &  \cite{lai2021revisiting} \\
\cline{1-5}


\multirow{6}{*}{Labeling} & Crowdsourced labeling & Collaboration & Full & \cite{kutlu2020annotator,tang2011semi,dekel2009vox} \\
~ & Semi-supervised labeling & Collaboration & Partial & \cite{zoph2020rethinking,zhou2004democratic,chong2020graph,ouyang2022training} \\
~ & Active learning & Collaboration &  Partial & \cite{cohn1996active,ren2021survey,dong2023active,zha2020meta} \\
~ & Data programming & Collaboration &  Partial & \cite{boeckinginteractive,galhotra2021adaptive} \\
~ & Data programming & Collaboration &  Minimum & \cite{ratner2016data,ratner2017snorkel,zha2019multi,hooper2021cut} \\
~ & Distant supervision & Automation &  Learning-based & \cite{mintz2009distant} \\
\cline{1-5}

\multirow{7}{*}{Preparation} & Data cleaning & Automation & Programmatic & \cite{zhang2016missing} \\
~ & Data cleaning & Automation & Learning-based & \cite{lakshminarayan1996imputation,heise2014estimating,jiang2022information,krishnan2019alphaclean} \\
~ & Data cleaning & Collaboration & Partial & \cite{wang2012crowder} \\
~ & Feature extraction & Automation & Programmatic & \cite{salau2019feature,barandas2020tsfel} \\
~ & Feature extraction & Automation & Learning-based & \cite{krizhevsky2017imagenet,wang2017time} \\
~ & Feature transformation & Automation & Programmatic & \cite{ali2014data,bisong2019introduction} \\
~ & Feature transformation & Automation & Learning-based & \cite{khurana2018feature} \\
\cline{1-5}

\multirow{6}{*}{Reduction} & Feature selection & Automation & Programmatic & \cite{thaseen2017intrusion,azhagusundari2013feature} \\
~ & Feature selection & Automation & Learning-based & \cite{yan2015feature,wang2015embedded}  \\
~ & Feature selection & Collaboration & Partial & \cite{zhang2019active,schnapp2021active}  \\
~ & Dimensionality reduction & Automation & Learning-based & \cite{abdi2010principal,xanthopoulos2013linear,bank2020autoencoders} \\
~ & Instance selection & Automation & Programmatic & \cite{riquelme2003finding,prusa2015using}\cite{liu2020mesa} \\
~ & Instance selection & Automation & Learning-based & \cite{sutton2012introduction,liu2020mesa} \\
\cline{1-5}

\multirow{5}{*}{Augmentation} & Basic manipulation & Automation & Programmatic & \cite{zhang2015character,zhang2018mixup,zhang2018mixup,wen2021time,chen2020mixtext,han2022g}. \\
~ & Basic manipulation & Automation & Learning-based & \cite{cubuk2019autoaugment} \\
~ & Augmentation data synthesis & Automation & Learning-based & \cite{frid2018synthetic,shorten2021text,hsu2017unsupervised,ho2022cascaded} \\
~ & Upsampling & Automation & Programmatic & \cite{chawla2002smote,he2008adasyn} \\
~ & Upsampling & Automation & Learning-based & \cite{zha2022towards} \\
\cline{1-5}

- & Pipeline search & Automation & Pipeline & \cite{feurer2015efficient,milutinovic2020evaluation,drori2021alphad3m,lai2021tods,zha2021autovideo,heffetz2020deepline,martinez2023towards} \\



\bottomrule
\end{tabular}
\end{table}

\subsection{Data Labeling}
\label{sec:3:2}

Data labeling is the process of assigning one or more descriptive tags or labels to a dataset, enabling algorithms to learn from and make predictions on the labeled data. Traditionally, this  is a time-consuming and resource-intensive manual process, particularly for large datasets. Recently, more efficient labeling methods have been proposed to reduce human efforts. In what follows, we discuss the need for data labeling, efficient labeling strategies, and challenges.


\subsubsection{Need for Data Labeling.}
Labeling plays a crucial role in ensuring that the model trained on the data accurately reflects human intentions. Without proper labeling, a model may not be able to make the desired predictions since the model can, at most, be as good as the data fed into it. Although unsupervised learning techniques are successful in domains such as large language models~~\cite{kenton2019bert,radford2018improving,radford2019language,brown2020language} and anomaly detection~\cite{pang2021deep}, the trained models may not well align with human expectations. Thus, to achieve a better performance, we often still need to fine-tune the large language models with human labels, such as ChatGPT~\cite{ouyang2022training}, and tune anomaly detectors with a small amount of labeled data~\cite{li2020pyodds,li2021automated,li2021autood,jiang2023weakly}. Thus, labeling data is essential for teaching models to align with and behave like humans.


\subsubsection{Efficient Labeling Strategies.}
Researchers have long recognized the importance of data labeling. Various strategies have been proposed to enhance labeling efficiency. We will discuss crowdsourced labeling, semi-supervised labeling, active learning, data programming, and distant supervision. Note that it is possible to combine them as hybrid strategies.

\emph{Crowdsourced labeling.}
Crowdsourcing is a classic approach that breaks down a labeling task into smaller and more manageable parts so that they can be outsourced and distributed to a large number of non-expert annotators. Traditional methods often only provide initial guidelines to annotators~\cite{yuen2011survey}. However, the guidelines can be unclear and ambiguous, so each annotator could judge the same situation subjectively and differently. One way to mitigate this inconsistency is to start with small pilot studies and iteratively refine the design of the labeling task~\cite{kutlu2020annotator}. Another is to ask multiple workers to annotate the same sample and infer a consensus label~\cite{tang2011semi}. Other studies focus on algorithmically improving label quality, e.g., pruning low-quality teachers~\cite{dekel2009vox}. All these crowdsourcing methods require full human participation but assist humans or enhance label quality in different ways. 

\emph{Semi-supervised labeling.} 
The key idea is to leverage a small amount of labeled data to infer the labels of the unlabeled data. A popular approach is self-training~\cite{zoph2020rethinking}, which trains a classifier based on labeled data and uses it to generate pseudo labels. To improve the quality of pseudo labels, a common strategy is to train multiple classifiers and find a consensus label, such as using different machine learning algorithms to train models on the same data~\cite{zhou2004democratic}. In parallel, researchers have studied graph-based semi-supervised labeling techniques~\cite{chong2020graph}. The idea is to construct a graph, where each node is a sample, and each edge represents the distance between the two nodes it connects. Then they infer labels through label propagation in the graph. Recently, a reinforcement learning from human feedback procedure is proposed~\cite{christiano2017deep} and used in ChatGPT~\cite{ouyang2022training}. They train a reward model based on human-labeled data and infer the reward for unlabeled data to fine-tune the language model. These semi-supervised labeling methods only require partial human participation to provide the initial labels.


\emph{Active learning.}
Active learning is an iterative labeling procedure that involves humans in the loop. In each iteration, the algorithm selects an unlabeled sample or batch of samples as a query for human annotation. The newly labeled samples help the algorithm choose the next query. The existing work mainly differs in query selection strategies. Early methods use statistical methods to estimate sample uncertainty and select the unlabeled sample the model is most uncertain about~\cite{cohn1996active}. Recent studies have investigated deep active learning, which leverages model output or designs specialized architectures to measure uncertainty~\cite{ren2021survey}. More recent research aligns the querying process with a Markov decision process and learns to select the long-term best query with contextual bandit~\cite{dong2023active} or reinforcement learning~\cite{zha2020meta}. Unlike semi-supervised labeling, which requires one-time human participation in the initial stage, active learning needs a continuous supply of information from humans to adaptively select queries.


\emph{Data programming.}
Data programming~\cite{ratner2016data,ratner2017snorkel} is a weakly-supervised approach that infers labels based on human-designed labeling functions. The labeling functions are often some heuristic rules and vary for different data types, e.g., seed words for text classification~\cite{zha2019multi}, masks for image segmentation~\cite{hooper2021cut}, etc. However, sometimes the labeling functions may not align with human intentions. To address this limitation, researchers have proposed interactive data programming~\cite{boeckinginteractive,galhotra2021adaptive}, where humans participate more by interactively providing feedback to refine labeling functions. Data programming methods often require minimum human participation or, at most, partial participation. Thus, the methods in this research line are often more desirable when we need to quickly generate a large number of labels.

\emph{Distant supervision.}
Another weakly-supervised approach is distant supervision, which assigns labels by leveraging external sources. A famous application of distant supervision is on relation extraction~\cite{mintz2009distant}, where the semantic relationships between entities in the text are labeled based on external data, such as Freebase~\cite{bollacker2008freebase}. Distant supervision is often an automated approach that does not require human participation. However, the automatically generated labels can be noisy if there is a discrepancy between the dataset and the external source.




\subsubsection{Challenges.}
The main challenge for data labeling stems from striking a balance between label quality, label quantity, and financial cost. If given adequate financial support, it is possible to hire a sufficient number of expert annotators to obtain a satisfactory quantity of high-quality labels. However, when we have a relatively tight budget, we often have to resort to more efficient labeling strategies. Identifying the proper labeling strategy often requires domain knowledge to balance different tradeoffs, particularly human labor and label quality/quantity. Another difficulty lies in the subjectivity of labeling. While the instructions may be clear to the designer, they may be misinterpreted by annotators, which leads to labeling noise. Last but not least, ethical considerations, such as data privacy and bias, remain a pressing issue, especially when the labeling task is distributed to a large and undefined group of people.


% After collecting a sufficient dataset, the next step is to label the data in order to train machine learning models. Data labeling has emerged as a major bottleneck in the machine learning development process, as the accuracy and effectiveness of models rely heavily on the quality of the labeled data. There are various techniques for labeling data, including semi-supervised labeling, crowdsourcing, active learning, and weak supervision. Semi-supervised labeling involves using a small amount of labeled data in combination with a larger amount of unlabeled data, and applying self-labeling~\cite{Triguero2013SelflabeledTF} techniques such as self-training~\cite{self-training,amini2022self} or tri-training~\cite{zhou2005tri} to generate additional labels by trusting one’s own predictions. These techniques involve training a model on the labeled data and using it to make predictions on the unlabeled data. The most confident predictions are then added to the labeled data. Crowdsourcing involves hiring human workers to label data manually, which can be accurate but costly, time-consuming, and prone to error. Active learning involves selecting the most interesting, informative and minimal number of data items for experts to label, in order to generate high-quality labels. Weak supervision involves semi-automatically generating a large number of relatively inexpensive “weak” labels when manual labeling is infeasible, using methods such as data programming or fact extraction. Large labeled datasets are key enablers of deep learning. We predict that more efficient labeling methods with various formats of human involvement for diverse data types will be proposed.







% \subsubsection{Dataset Culture} Dataset culture refers to the practices, norms and values associated with dataset creation, use and reuse of data, storage and labor conditions under which the large-scale datasets are produced. It encompasses various stakeholders such as data collectors and users, as well as the policies used to collect, store and share datasets. With the growth of cheap, crowd-sourcing platforms, data labor for collecting and annotating data is often unpaid or undervalued. However, facilitating fair pay~\cite{silberman2018responsible, whiting2019fair} can lead to high quality, well-curated datasets. Legal perspective of data collection is another important aspect of dataset culture, which includes data privacy, ownership and ethical considerations related to dataset collection. State of the art benchmarking datasets are usually collected from a variety of sources, with varying copyright holders and permissions. For instance, the copyright restrictions and licensing for even large-scale datasets such as ImageNet are unknown~\cite{russakovsky2015imagenet}. Open licenses, such as Creative Commons license, provide legal tools allowing fair use of creators’s work with others under certain restrictions. Laws such as California Consumer Privacy Act (CCPA)~\cite{ccpa} sets strict guidelines for the businesses to comply with the collection, storage and use of personal data. More recently, The Montreal Data License~\cite{benjamin2019montreal} provides a common framework for data licensing similar to licensing of open source software. In this subsection, we have shed some light on the dataset culture in terms of legal perspectives, labor and use of dataset. 







\subsection{Data Preparation}
\label{sec:3:3}

Data preparation involves cleaning and transforming raw data into a format that is appropriate for model training. Conventionally, this process often necessitates a considerable amount of engineering work with laborious trial and error. To automate this process, state-of-the-art approaches often adopt search algorithms to discover the most effective strategies. In this subsection, we introduce the need, representative methods, and challenges for data preparation.


\subsubsection{Need for Data Preparation} Raw data is often not ready for model training due to potential issues such as noise, inconsistencies, and unnecessary information, leading to inaccurate and biased results. For instance, the model could overfit on noises, outliers, and irrelevant extracted features, resulting in reduced generalizability~\cite{ying2019overview}. If sensitive information (e.g., race and gender) is not removed, the model may unintentionally learn to make biased predictions~\cite{wan2022processing}. In addition, the raw feature values may negatively affect model performance if they are in different scales or follow skewed distributions~\cite{ahsan2021effect}. Thus, it is imperative to clean and transform data. The need can also be verified by a Forbes survey~\cite{press_2022}, which suggests that data preparation accounts for roughly 80\% of the work of data scientists.

\subsubsection{Methods} We will review and discuss the techniques for achieving three key data preparation objectives, namely data cleaning, feature extraction, and feature transformation.

\emph{Data cleaning.} Data cleaning is the process of identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Traditional methods repair data with programmatic automation, e.g., imputing missing values with mean or median~\cite{zhang2016missing} and scanning all data to find duplicates. However, such heuristics can be inaccurate or inefficient. Thus, learning-based methods have been developed, such as training a regression model to predict missing values~\cite{lakshminarayan1996imputation}, efficiently estimating the duplicates with sampling~\cite{heise2014estimating}, and correcting labeling errors~\cite{jiang2022information}.  Contemporary data cleaning methods often do not solely focus on the cleaning itself, but rather on learning to improve final model performance. For instance, a recent study has adopted search algorithms to automatically identify the best cleaning strategy to optimize validation performance~\cite{krishnan2019alphaclean}. Beyond automation, researchers have studied collaboration-oriented cleaning methods. For example, a hybrid human-machine workflow is proposed to identify duplicates by presenting similar pairs to humans for annotation~\cite{wang2012crowder}.



\emph{Feature extraction.} Feature extraction is an important step in extracting relevant features from raw data. For training traditional machine learning models, we often need to extract features based on domain knowledge of the data type being targeted. Common features used for images include color features, texture features, intensity features, etc.~\cite{salau2019feature}. For time series data, temporal, statistical, and spectral features are often considered~\cite{barandas2020tsfel}. Deep learning, in contrast, automatically extracts features by learning the weights of neural networks, which requires less domain knowledge. For instance, convolutional neural networks can be used in both images~\cite{krizhevsky2017imagenet} and time series~\cite{wang2017time}. The boundary between data and model becomes blurred with deep learning feature extractors, which operate on the data while also being an integral part of the model. Although deep extractors could learn high-quality feature representations, the extraction process is uninterpretable and may amplify the bias in the learned representation~\cite {wan2022processing}. Therefore, traditional feature extraction methods are often preferred in high-stakes domains for interpretability and removing sensitive information.

\emph{Feature transformation.} Feature transformation refers to the process of converting the original features into a new set of features, which can often lead to improved model performance. Some typical transformations include normalization, which scales the feature into a bounding range, and standardization, which transforms features so that they have a mean of zero and a standard deviation of one~\cite{ali2014data}. Other strategies include log transformation and polynomial transformation to smooth the long-tail distribution and create new features through multiplication~\cite{bisong2019introduction}. These transformation methods can be combined in different ways to improve model performance. For example, a representative work builds a transformation graph for a given dataset, where each node is a type of transformation, and adopts reinforcement learning to search for the best transformation strategy~\cite{khurana2018feature}. Learning-based methods often yield superior performance by optimizing transformation strategies based on the feedback obtained from the model.


\subsubsection{Challenges.}
Properly cleaning and transforming data is challenging due to the unique characteristics of different datasets. For example, the errors and inconsistencies in text data are quite different from those in time-series data. Even if two datasets have the same data type, their feature values and potential issues can be very diverse. Thus, researchers and data scientists often need to devote a significant amount of time and effort to clean the data. Although learning-based methods can search for the optimal preparation strategy automatically~\cite{khurana2018feature,krishnan2019alphaclean}, it remains a challenge to design the appropriate search space, and the search often requires a non-trivial amount of time.


\subsection{Data Reduction}
\label{sec:3:4}

The goal of data reduction is to reduce the complexity of a given dataset while retaining its essential information. This is often achieved by either reducing the feature size or the sample size. Our discussion will focus on the need for data reduction, representative methods for feature and sample size reduction, and challenges.

\subsubsection{Need for Data Reduction}
With more data being collected at an unprecedented pace, data reduction plays a critical role in boosting training efficiency. From the sample size perspective, reducing the number of samples leads to a simpler yet representative dataset, which can alleviate memory and computation constraints. It also helps to alleviate data imbalance issues by downsampling the samples from the majority class~\cite{prusa2015using}. Similarly, reducing feature size brings many benefits. For example, eliminating irrelevant or redundant features mitigates the risk of overfitting~\cite{li2017feature}. Smaller feature sizes will also enable faster training and inference in model deployment~\cite{wang2022bed}. In addition, only keeping a subset of features will make the model more interpretable~\cite{chuang2023efficient,wang2022accelerating,chuangcortx}. Data reduction techniques can enable the model to focus only on the essential information, thereby enhancing accuracy, efficiency, and interpretability.

\subsubsection{Methods for Reducing Feature Size.} From the feature perspective, we discuss two common reduction strategies.

\emph{Feature selection.} Feature selection is the process of selecting a subset of features most relevant to the intended tasks~\cite{li2017feature}. It can be broadly classified into filter, wrapper, and embedded methods. Filter methods~\cite{thaseen2017intrusion} evaluate and select features independently using a scoring function based on statistical properties such as information gain~\cite{azhagusundari2013feature}. Although filter methods are very efficient, they ignore feature dependencies and interactions with the model. Wrapper methods alleviate these issues by leveraging the model performance to assess the quality of selected features and refining the selection iteratively~\cite{yan2015feature}. While these methods often achieve better performances, they are computationally more expensive. Embedded methods, from another angle, integrate feature selection into the model training process~\cite{wang2015embedded} so that the selection process is optimized in an end-to-end manner. Beyond automation, active feature selection takes into account human knowledge and incrementally selects the most appropriate features~\cite{zhang2019active,schnapp2021active}. Feature selection reduces the complexity, producing cleaner and more understandable data while retaining feature semantics.

\emph{Dimensionality reduction.} Dimensionality reduction aims to transform high-dimensional features into a lower-dimensional space while preserving the most representative information. The existing methods can be mainly categorized into linear and non-linear techniques. The former generates new features via linear combinations of features from the original data. One of the most popular algorithms is Principal Component Analysis (PCA)~\cite{abdi2010principal}, which performs orthogonal linear combinations of the original features based on the variance in an unsupervised manner. Another representative method targeted for supervised scenarios is Linear Discriminant Analysis (LDA)~\cite{xanthopoulos2013linear}, which statistically learns linear feature combinations that can separate classes well. Linear techniques, however, may not always perform well, especially when features have complex and non-linear relationships. Non-linear techniques address this issue by utilizing nonlinear mapping functions. A popular technique is autoencoders~\cite{bank2020autoencoders}, which use neural networks to encode the original features into a low-dimensional space and reconstruct the features using a neural decoder.

\subsubsection{Methods for Reducing Sample Size} The reduction of samples is typically achieved with \emph{instance selection}, which selects a representative subset of data samples that retain the original properties of the dataset. The existing studies can be divided into wrapper and filter methods. The former selects instances based on scoring functions. For example, a common strategy is to select border instances since they can often shape the decision boundary~\cite{riquelme2003finding}. Wrapper methods, in contrast, select instances based on model performance~\cite{sutton2012introduction}, which considers the interaction effect with the model. Instance selection techniques can also alleviate data imbalance issues by undersampling the majority class, e.g., with random undersampling~\cite{prusa2015using}. More recent work adopts reinforcement learning to learn the best undersampling strategies~\cite{liu2020mesa}. Overall, instance selection is a simple yet effective way to reduce data sizes or balance data distributions.

\subsubsection{Challenges}
The challenges of data reduction are two-folded. On the one hand, selecting the most representative data or projecting data in a low-dimensional space with minimal information loss is non-trivial. While learning-based methods can partially address these challenges, they may necessitate substantial computational resources, especially when dealing with extremely large datasets, e.g., the wrapper and reinforcement learning methods~\cite{yan2015feature,sutton2012introduction,liu2020mesa}. Therefore, achieving both high accuracy and efficiency is challenging. On the other hand, data reduction can potentially amply data bias, raising fairness concerns. For example, the selected features could be over associating with protected attributes~\cite{xing2021fairness}. Fairness-aware data reduction is a critical yet under-explored research direction.


\subsection{Data Augmentation} 
\label{sec:3:5}

Data augmentation is a technique to increase the size and diversity of data by artificially creating variations of the existing data, which can often improve the model performance. It is worth noting that even though data augmentation and data reduction seem to have contradictory objectives, they can be used in conjunction with each other. While data reduction focuses on eliminating redundant information, data augmentation aims to enhance data diversity.  We will delve into the need for data augmentation, various representative methods, and the associated challenges.

\subsubsection{Need for Data Augmentation}
Modern machine learning algorithms, particularly deep learning, often require large amounts of data to learn effectively. However, collecting large datasets, especially annotated data, is labor-intensive. By generating similar data points with variance, data augmentation helps to expose the model to more training examples, hereby improving accuracy, generalization capabilities, and robustness. Data augmentation is particularly important in applications where there is limited data available. For example, it is often expensive and time-consuming to acquire well-annotated medical data~\cite{chlap2021review}. Data augmentation can also alleviate class imbalance issues, where there is a disproportionate ratio of training samples in each class, by augmenting the data from the under-represented class.

\subsubsection{Common Augmentation Methods} In general, data augmentation methods often manipulate the existing data to generate variances or synthesize new data. We discuss some representative methods in each category below.

\emph{Basic manipulation.} This research line involves making minor modifications to the original data samples to produce augmented samples directly. Various strategies have been proposed in the computer vision domain, such as scaling, rotation, flipping, and blurring~\cite{zhang2015character}. One notable approach is Mixup~\cite{zhang2018mixup}, which interpolates the existing data samples to create new samples. It is shown that Mixup serves as a regularizer, encouraging the model to prioritize simpler linear patterns, which in turn enhances the generation performance~\cite{zhang2018mixup}. More recent studies use learning-based algorithms to automatically search for augmentation strategies. A representative work is AutoAugment, which uses reinforcement learning to iteratively improve the augmentation policies~\cite{cubuk2019autoaugment}. Beyond image data, basic manipulation often needs to be tailored for the other data types, such as permutation and jittering in time-series data~\cite{wen2021time}, mixing data in the hidden space for text data to retain semantic meanings~\cite{chen2020mixtext}, and mixing graphon for graph data~\cite{han2022g}.


\emph{Augmentation data synthesis.} Another category focuses on synthesizing new training samples by learning the distribution of the existing data, which is typically achieved by generative modeling. GAN~\cite{goodfellow2020generative,zhang2019self} has been widely used for data augmentation~\cite{frid2018synthetic}. The key idea is to train a discriminator in conjunction with a generator, making the latter generate synthetic data that closely resembles the existing data. GAN-based data augmentation has also been used to augment other data types, such as time-series data~\cite{li2022tts} and text data~\cite{shorten2021text}. Other studies have used Variational Autoencoder~\cite{hsu2017unsupervised} and diffusion models~\cite{ho2022cascaded} to achieve augmentation. Compared to basic manipulation that augments data locally, data synthesis learns data patterns from the global view and generates new samples with a learned model.



\subsubsection{Methods Tailored for Class Imbalance} Class imbalance is a fundamental challenge in machine learning, where the number of majority samples is much larger than that of minority samples. Data augmentation can be used to perform \emph{upsampling} on the minority class to balance the data distribution. One popular approach is SMOTE~\cite{chawla2002smote}, which involves generating synthetic samples by linearly interpolating between minority instances and their neighbors. ADASYN~\cite{he2008adasyn} is an extension of SMOTE that generates additional synthetic samples for data points that are more difficult to learn, as determined by the ratio of majority class samples in their nearest neighbors. A recent study proposes AutoSMOTE, a learning-based algorithm that searches for best oversampling strategies with reinforcement learning~\cite{zha2022towards}.

\subsubsection{Challenges}
One critical challenge in data augmentation is that there is no single augmentation strategy that is suitable for all scenarios. Different data types may require diverse strategies. For example, compared to image data, graph data is irregular and not well-aligned, and thus the vanilla Mixup strategy can not be directly applied~\cite{han2022g}. Even though two datasets have the same data type, the optimal strategy differs. For instance, we often need to upsample the minority samples differently to achieve the best results~\cite{zha2022towards}. Although search-based algorithms can identify the best strategies with trial and error, it also increases the computation and storage costs, which can be a limiting factor in some applications. More effective and efficient data augmentation techniques are required to overcome these challenges.

\subsection{Pipeline Search}
\label{sec:3:6}
In real-world applications, we often encounter complex data pipelines, where each pipeline step corresponds to a task associated with one of the aforementioned sub-goals. Despite the progress made in each individual task, a pipeline typically functions as a whole, and the various pipeline steps may have an interactive effect. For instance, the best data augmentation strategy may depend on the selected features. Pipeline search is a recent trend that tries to automatically search for the best combinations. This subsection introduces some representative pipeline search algorithms.

One of the first pipeline search frameworks is AutoSklearn~\cite{feurer2015efficient}. It performs a combined search of preprocessing modules, models, and the associated hyperparameters to optimize the validation performance. However, they use a very small search space for preprocessing modules. DARPA’s Data-Driven Discovery of Models (D3M) program pushes the progress further by building an infrastructure for pipeline search~\cite{milutinovic2020evaluation}. Although D3M originally focused on automated model discovery, it has developed numerous data-centric modules for processing data. Building upon D3M, AlphaD3M uses Monte-Carlo Tree Search to identify the best pipeline~\cite{drori2021alphad3m}. D3M is then tailored for time-series anomaly detection~\cite{lai2021tods} and video analysis~\cite{zha2021autovideo}. Deepline enables the search within a large number of data-centric modules using multi-step reinforcement learning~\cite{heffetz2020deepline}. ClusterP3S allows for personalized pipelines to be created for various features, utilizing clustering techniques to enhance search efficiency~\cite{martinez2023towards}.

Despite these progresses, pipeline search still faces a significant challenge due to the high computational overhead since the search algorithm often needs to try different module combinations repeatedly. This overhead becomes more pronounced as the number of modules increases, leading to an exponential growth of the search space. Thus, more efficient search strategies~\cite{heffetz2020deepline,martinez2023towards} are required to enable a broader application of pipeline search in real-world scenarios.

