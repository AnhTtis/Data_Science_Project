\section{Discussion and Future Direction}
\label{sec:7}

What is the current stage of data-centric AI research, and what are the potential future directions? This section provides a top-level discussion of data-centric AI and presents some of the open problems that we have identified, aiming to motivate future exploration in this field. We start by trying to answer the research questions posed at the beginning:

\begin{itemize}
    \item \emph{RQ1: What are the necessary tasks to make AI data-centric?} Data-centric AI encompasses a range of tasks that involve developing training data, inference data, and maintaining data. These tasks include but are not limited to 1) cleaning, labeling, preparing, reducing, and augmenting the training data, 2) generating in-distribution and out-of-distribution data for evaluation, or tuning prompts to achieve desired outcomes, and 3) constructing efficient infrastructures for understanding, organizing, and debugging data.
    \item \emph{RQ2: Why is automation significant for developing and maintaining data?} Given the availability of an increasing amount of data at an unprecedented rate, it is imperative to develop automated algorithms to streamline the process of data development and maintenance. Based on the papers surveyed in Tables~\ref{tbl:trainingmethodsummary}, \ref{tbl:inferencemethodsummary}, and \ref{tbl:maintainmethodsummary}, automated algorithms have been developed for all sub-goals. These automation algorithms span different automation levels, from programmatic automation to learning-based automation, to pipeline automation.
    \item \emph{RQ3: In which cases and why is human participation essential in data-centric AI?} Human participation is necessary for many data-centric AI tasks, such as the majority of data labeling tasks (Table~\ref{tbl:trainingmethodsummary}) and several tasks in inference data development (Table~\ref{tbl:inferencemethodsummary}). Notably, different methods may require varying degrees of human participation, ranging from full involvement to providing minimal inputs. Human participation is crucial in many scenarios because it is often the only way to ensure that the behavior of AI systems aligns with human intentions.
    \item \emph{RQ4: What is the current progress of data-centric AI?} Although data-centric AI is a relatively new concept, considerable progress has already been made in many relevant tasks, the majority of which were viewed as preprocessing steps in the model-centric paradigm. Meanwhile, many new tasks have recently emerged, and research on them is still ongoing. In Section~\ref{sec:6}, our meta-analysis on benchmark papers reveals that progress has been made across different domains, with the majority of the benchmarks coming from the AI domain. Among the three general data-centric AI goals, training data development has received relatively more research attention. For data modality, tabular and image data have been the primary focus. As research papers on data-centric AI are growing exponentially~\cite{zha2023data}, we could witness even more progress in this field in the future.
\end{itemize}
By attempting to address these questions, our survey delves into a variety of tasks and their needs and challenges, yielding a more concrete picture of the scope and progress of data-centric AI. However, although we have endeavored to broadly and comprehensively cover various tasks and techniques, it is impossible to include every aspect of data-centric AI. In the following, we connect data-centric AI with two other popular research topics in AI:

\begin{itemize}
    \item \emph{Foundation models.} A foundation model is a large model that is trained on massive amounts of unlabeled data and can be adapted to various tasks, such as large language models~\cite{brown2020language,gpt4}, and Stable Diffusion~\cite{rombach2022high}. As models become sufficiently powerful, it becomes feasible to perform many data-centric AI tasks with models, such as data labeling~\cite{gpt4}, and data augmentation~\cite{yoo2021gpt3mix}. Consequently, the recent trend of foundation models has the potential to fundamentally alter our understanding of data. Unlike the conventional approach of storing raw data values in datasets, the model itself can be a form of data (or a ``container'' of raw data) since the model can convey information (see the definition of data in Section~\ref{sec:2:1}). Foundation models blur the boundary between data and model, but their training still heavily relies on large and high-quality datasets.
    \item \emph{Reinforcement learning.} Reinforcement learning is a research field that trains intelligent agents to optimize rewards without any initial data~\cite{mnih2013playing,zha2021douzero,zha2021rank,zha2021rlcard,zha2019experience,zha2022autoshard,zhadreamshard,lai2020dual,zha2021simplifying}. It is a unique learning paradigm that alternates between generating data with the model and training the model with self-generated data. Like foundation models, the advancement of reinforcement learning could also possibly blur the boundary between data and model. Furthermore, reinforcement learning has already been widely adopted in several data-centric AI sub-goals, such as data labeling~\cite{christiano2017deep,dong2023active,zha2020meta}, data preparation~\cite{khurana2018feature}, data reduction~\cite{liu2020mesa}, and data augmentation~\cite{cubuk2019autoaugment,zha2022towards}. The reason could be attributed to its goal-oriented nature, which is well-suited for automation.
\end{itemize}

Upon examining the connections to these two rapidly evolving research fields, we hypothesize that data-centric AI and model-centric AI could become even more intertwined in the development of AI systems. Looking forward, we present some potential future directions we have identified in data-centric AI:
\begin{itemize}
    \item \emph{Cross-task automation.} While there has been significant progress in automating various individual data-centric AI tasks, joint automation across multiple tasks remains largely unexplored. Although pipeline search methods~\cite{feurer2015efficient,lai2021tods,zha2021autovideo,heffetz2020deepline} have emerged, they are limited only to training data development. From a broad data-centric AI perspective, it would be desirable to have a unified framework for jointly automating tasks aimed at different goals, ranging from training data development to inference data development and data maintenance.
    \item \emph{Data-model co-design.} Although data-centric AI advocates for shifting the focus to data, it does not necessarily imply that the model has to remain unchanged. The optimal data strategies may differ when using different models, and vice versa. Furthermore, as discussed above, the boundary between data and model could potentially become increasingly blurred with the advancement of foundation models and reinforcement learning. Consequently, future progress in AI could arise from co-designing data and models, and the co-evolution of data and models could pave the way toward more powerful AI systems.
    \item \emph{Debiasing data.} In many high-stakes applications, AI systems have recently been found to exhibit discriminatory behavior towards certain groups of people, sparking significant concerns about fairness~\cite{mehrabi2021survey,wan2022processing,ding2023fairly,chuang2022mitigating,jiang2022generalized,jiang2022fmp}. These biases often originate from imbalanced distributions of sensitive variables in the data. From a data-centric perspective, more research efforts are needed to debias data, including but limited to mitigating biases in training data, systematic methodologies to construct evaluation data to expose unfairness issues of unfairness, and continuously maintaining fair data in a dynamic environment.
    \item \emph{Tackling data in various modalities.} Based on the benchmark analysis presented in Section~\ref{sec:6}, most research efforts have been directed toward tabular and image data. However, other data modalities that are comparably important but less studied in data-centric AI pose significant challenges. For instance, time-series data~\cite{zha2022towards2,hamilton2020time,li2022towards} exhibit complex temporal correlations, while graph data~\cite{zhou2020graph,zhou2021multi,zhou2020towards,lai2020policy,zhou2021dirichlet,tan2023bring,liu2022rsc} has intricate data dependencies. Therefore, more research on how to engineer data for these modalities is required. Furthermore, developing data-centric AI solutions that can simultaneously address multiple data modalities is an intriguing avenue for future exploration.
    \item \emph{Data benchmarks development.} The advancement of model-centric AI has been facilitated by benchmarks in advancing model designs. Whereas data-centric AI requires more attention to benchmarking. As discussed in Section~\ref{sec:6}, existing benchmarks for data-centric AI typically only focus on specific tasks. Constructing a unified benchmark to evaluate overall data quality and various data-centric AI techniques comprehensively presents a significant challenge. Although DataPerf~\cite{mazumder2022dataperf} has made notable progress towards this objective, it currently supports only a limited number of tasks. The development of more unified data benchmarks would greatly accelerate research progress in this area.
\end{itemize}

