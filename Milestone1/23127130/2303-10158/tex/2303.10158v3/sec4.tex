\begin{figure}[t]
  \centering
    \includegraphics[width=1.0\textwidth]{figures/inference_data_development.pdf}
  \caption{An overview of inference data development.}
  \label{fig:inferencedevelopmentoverview}
\end{figure}







\section{Inference Data Development}
\label{sec:4}
Another crucial component in building AI systems is to design inference data to evaluate a trained model or unlock a specific capability of the model. In the conventional model-centric paradigm, we often adopt a hold-out evaluation set that is not included in the training data to measure model performance using specific metrics such as accuracy. However, relying solely on performance metrics may not fully capture many important properties of a model, such as its robustness, generalizability, and rationale in decision-making. Moreover, as models become increasingly large, it becomes possible to obtain the desired predictions by solely engineering the data input. This section introduces some representative methods that evaluate models from a more granular view, or engineering data inputs for inference, shown in Figure~\ref{fig:inferencedevelopmentoverview}. Our discussion involves in-distribution set evaluation (Section~\ref{sec:41}), out-of-distribution evaluation (Section~\ref{sec:42}), and prompt engineering (Section~\ref{sec:43}).
%, which are designed to generate samples that conform to or differ from the training data distribution, respectively.
We summarize the relevant tasks and methods in Table~\ref{tbl:inferencemethodsummary}. 




%% Draft Input

\subsection{In-distribution Evaluation} 
\label{sec:41}

In-distribution evaluation data construction aims to generate samples that conform to training data. We will begin by addressing the need for constructing in-distribution evaluation sets. Next, we will review representative methods for two scenarios: evaluating important sub-populations on which the model underperforms through data slicing, and assessing decision boundaries through algorithmic recourse. Lastly, we will discuss the challenges.


% In particular, we systematically review the \textit{data slicing} techniques for evaluating hidden strata on which the models underperform. Then, we review the \textit{data synthesis} techniques for stability evaluation on model generalization. Moreover, we review \textit{algorithmic recourse} strategies in assessing decision boundaries and detecting potential biases. 

% generalization stability~\cite{kutin2002almost}






\subsubsection{Need for In-distribution Evaluation}

In-distribution evaluation is the most direct way to assess the quality of trained models, as it reflects their capabilities within the training distribution. The need for a more fine-grained in-distribution evaluation is two-fold. Firstly, models that perform well on average may fail to perform adequately on specific sub-populations, requiring identification and calibration of underrepresented groups to avoid biases and errors, particularly in high-stakes applications~\cite{otles2021mind,meng2022interpretability}.
%Secondly, certain domains have limited data, such as healthcare~\cite{wang2021generating} and finance~\cite{henry2019generative}, necessitating hypothetical data for model assessment.
Secondly, it is crucial to understand the decision boundary and inspect the model ethics before deployment, especially in risky applications like policy making~\cite{souza2019data}.



\subsubsection{Data Slicing}

Data slicing involves partitioning a dataset into relevant sub-populations and evaluating a model's performance on each sub-population separately. A common approach to data slicing is to use pre-defined criteria, such as age, gender, or race~\cite{barenstein2019propublica}. However, data in many real-world applications can be complex, and properly designing the partitioning criteria heavily relies on domain knowledge, such as slicing 3-D seismic data in geophysics~\cite{zeng1998stratal} and program slicing~\cite{santelices2013quantitative}.

To reduce human effort, automated slicing methods have been developed to discover important data slices by sifting through all potential slices in the data space. One representative work is SliceFinder~\cite{chung2019slice}, which identifies slices that are both interpretable (i.e., slicing based on a small set of features) and problematic (the model performs poorly on the slice). To solve this search problem, SliceFinder offers two distinct methods, namely the tree-based search and the lattice-based search. The former is more efficient, while the latter has better efficacy. SliceLine~\cite{sagadeeva2021sliceline} is another notable work that addresses the scalability limitations of slice finding by focusing on both algorithmic and system perspectives. This approach is motivated by frequent itemset mining and leverages relevant monotonicity properties and upper bounds for effective pruning. Moreover, to address hidden stratification, which occurs when each labeled class contains multiple semantically distinct subclasses, GEORGE~\cite{sohoni2020no} employs clustering algorithms to slide data across different subclasses. Another tool for automated slicing is Multiaccuracy~\cite{kim2019multiaccuracy}, where a simple ``auditor'' is trained to predict the residual of the full model using input features. Multiaccuracy, in general, is an efficient approach since it only requires a small amount of audit data. 
%This makes it an efficient approach for model validation and debugging.
Data slicing allows researchers and practitioners to identify biases and errors in a model's predictions and calibrate the model to improve its overall capabilities. 



%\subsubsection{Evaluation Data Synthesis}

% Manual data synthesis is mainly represented as data development with human-in-the-loop. For example, in language model evaluation, Big-Bench \cite{srivastava2022beyond} provides a comprehensive platform to evaluate large language models (LLM) consisting of 204 tasks, contributed by 444 authors across 132 institutions. For the purpose of artificial general intelligence\cite{fei2022towards}, extensively diverse tasks are covered in Big-Bench, including linguistics, childhood development, math, common-sense reasoning, social bias, software development, and beyond. More importantly, code that implements the benchmark API are provided to support task evaluation on publicly available models and even new tasks. Specifically, there are two supported types of tasks: JSON and programmatic. For JSON tasks, it occupies $80\%$ in the benchmark and is formally defined by JSON file with a list of examples made up of inputs and targets. The performance can be evaluated by comparison with model outputs and the corresponding targets. Such JSON tasks based on examples also allows for easy few-shot evaluation\cite{brown2020language}. The other type of task is programmatic usually written in Python with the ability to interact with models over multiple query rounds. Specifically, such a programmatic task is called with model objective and can query model with different method functions. In other words, python code defining the task can query model repeatedly, which allowed evaluation over multiple-round tasks, e.g., dialogue system. 

% Automated data synthesis mainly represents in-distribution dataset construction by generative models, such as variational auto-encoder (VAE)\cite{rezende2014stochastic,kingma2019introduction}, energy-based model (EBM)~\cite{lecun2006tutorial,alias2017variational}, generative adversarial net (GAN)~\cite{creswell2018generative,goodfellow2020generative}, normalizing flow (NF)~\cite{dinh2016density,rezende2015variational,kobyzev2020normalizing}, and diffusion model~\cite{saharia2022palette,hoogeboom2022equivariant,wu4156409stochastic}. Generative models can create new in-distribution data (e.g., images) that human cannot properly distinguish, which can serve as data augmentation for training or evaluation data. The early generative work VAE \cite{kingma2019introduction} conducts the encoder to map the prior to latent space with reduced dimension and then adopts the decoder to generate samples based on the latent variable. EBM~\cite{lecun2006tutorial} designs a suitable energy function for pair-wise energy function between conditions and samples.  GAN~\cite{goodfellow2020generative} designs generative and discriminative networks (i.e., two-play games) and then applies adversarial training strategy to enforce that generator can produce new samples similar to training data. NF~\cite{kobyzev2020normalizing} employs a pre-designed invertible flow function to transform input into a latent variable in the encoder while reversing the flow function to construct the original data in the decoder. Diffusion model~\cite{wu4156409stochastic} consists diffusion and denoising steps, where diffusion gradually injects noise into the original data until the perturbed data is with a known distribution, and denoise aims to reverse each noise-injection step to recover the original data. 

\begin{table}[t]
\centering
\caption{Papers for achieving different sub-goals of inference data development.}
\footnotesize
\setlength{\tabcolsep}{1.0pt}
\label{tbl:inferencemethodsummary}
\begin{tabular}{l|l|l|l|l} \toprule
\multirow{2}{*}{\textbf{Sub-goal}} & \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Method type}} &  \multirow{2}{*}{\textbf{\shortstack[l]{Automation level/\\ participation degree}}} & \multirow{2}{*}{\textbf{References}} \\
~ & ~ & ~ & ~ & ~ \\
\midrule

\multirow{4}{*}{\shortstack[l]{In-\\ distribution}} & Data slicing & Collaboration  &  Minimum & \cite{barenstein2019propublica} \\
~ & Data slicing & Collaboration  &  Partial & \cite{zeng1998stratal,santelices2013quantitative} \\
~ & Data slicing & Automation & Learning-based & \cite{chung2019slice,sagadeeva2021sliceline,sohoni2020no,kim2019multiaccuracy} \\
%\multirow{9}{*}{In-distribution} & Data slicing & Collaboration  &  Full & \cite{zeng1998stratal,alomari2012very,santelices2013quantitative,lallchandani2011dynamic,zhong2013kernelet,el2014distributed} \\
%~ & Data slicing & Collaboration & Partial &  \cite{swain2012test,koushik2012review,androutsopoulos2012amorphous} \\
%~ & Data slicing & Automation & Pipeline &  \cite{chung2019slice,sagadeeva2021sliceline,sohoni2020no,kim2019multiaccuracy} \\
%~ & Evaluation data synthesis & Automation & Programmatic &  \cite{srivastava2022beyond} \\
%~ & Evaluation data synthesis & Collaboration & Full &  \cite{srivastava2022beyond,brown2020language} \\
%~ & Evaluation data synthesis & Automation & Learning-based &  \cite{rezende2014stochastic,alias2017variational,goodfellow2020generative,rezende2015variational,hoogeboom2022equivariant} \\
~ & Algorithmic recourse & Collaboration & Minimum &  \cite{kanamori2020dace,carreira2021counterfactual,lucic2022focus,wachter2017counterfactual,dandl2020multi,dhurandhar2019model,sharma2019certifai,laugel2018comparison,poyiadzi2020face,becker2021step,blanchart2021exact} \\
%~ & Algorithmic recourse & Collaboration & Minimum &  \cite{poyiadzi2020face,becker2021step,blanchart2021exact} \\
%~ & Algorithmic recourse & Collaboration & Full &  \cite{guidotti2018local,rathi2019generating,white2019measurable,ross2021learning,shao2022gradient,guo2021counternet} \\
\cline{1-5}


\multirow{6}{*}{\shortstack[l]{Out-of-\\ distribution}} & Adversarial samples & Collaboration &  Minimum & \cite{hendrycks2019benchmarking} \\
~ & Adversarial samples & Automation &  Learning-based & \cite{biggio2013evasion,moosavi2016deepfool,madry2017towards,eykholt2018robust,papernot2017practical,chen2017zoo,shafahi2018poison} \\
~ & Distribution shift & Collaboration &  Full & \cite{ding2021retiring,koh2021wilds,saenko2010adapting} \\
~ & Distribution shift & Collaboration &  Partial & \cite{gu2019using,shankar2021image} \\
~ & Distribution shift & Automation &  Programmatic & \cite{gretton2009covariate,sugiyama2007covariate,lipton2018detecting,azizzadenesheli2019regularized} \\
~ & Distribution shift & Automation &  Learning-based & \cite{farahani2021brief,guan2021domain} \\

\cline{1-5}
\multirow{3}{*}{\shortstack[l]{Prompt\\ engineering}} & Manual engineering & Collaboration & Partial & \cite{schick2020few,schick2020exploiting,schick2020s} \\
~ & Automated engineering & Automation & Programmatic & \cite{jiang2020can,yuan2021bartscore,haviv2021bertese} \\
~ & Automated engineering & Automation & Learning-based & \cite{wallace2019universal,gao2021making} \\

\bottomrule
\end{tabular}
\end{table}


\subsubsection{Algorithmic Recourse} 

Algorithmic recourse (also known as  ``counterfactuals''~\cite{wachter2017counterfactual} in the explainable AI domain) aims to generate a hypothetical set of samples that can flip model decisions toward preferred outcomes. For example, if an individual is denied a loan, algorithmic recourse seeks the closest sample (e.g., with a higher account balance) that would have been approved. Hypothetical samples derived through algorithmic recourse are valuable in understanding decision boundaries. For the previously mentioned example, the hypothetical sample addresses the question of how the individual could have been approved and also aids in the detection of potential biases across individuals.

The existing
methods primarily vary in their strategies for identifying hypothetical samples, and can generally be classified into white-box and black-box methods. White-box methods necessitate access to the evaluated models, which can be achieved through complete internals~\cite{kanamori2020dace,carreira2021counterfactual,lucic2022focus}, gradients~\cite{wachter2017counterfactual}, or solely the prediction function~\cite{dandl2020multi,dhurandhar2019model,sharma2019certifai,laugel2018comparison}. Conversely, black-box methods do not require access to the model at all. For example, Dijkstraâ€™s algorithm is employed to obtain the shortest path between existing training data points to find recourse under certain distributions~\cite{poyiadzi2020face}. An alternative approach involves dividing the feature space into pure regions, where all data points belong to a single class, and utilizing graph traversing techniques~\cite{becker2021step,blanchart2021exact} to identify the nearest recourse. Given that the target label for reasoning is usually inputted by humans, these recourse methods all require minimal human participation.

%Besides the two major groups of recourse above, we also have methodologies based on heuristics~\cite{guidotti2018local,rathi2019generating,white2019measurable}, as well as new training routines~\cite{ross2021learning,shao2022gradient,guo2021counternet}. 

 



\subsubsection{Challenges}

The main challenge of constructing in-distribution evaluation sets lies in identifying the targeted samples effectively and efficiently. In the case of data slicing, determining the optimal subset of data is particularly challenging due to the exponential increase in the number of possible subsets with additional data points. Similarly, identifying the closest recourse when limited information is available also requires significant effort. 


%The challenges for in-distribution set construction are three-fold. Firstly, the high quality of in-distribution data construction is challenging due to the limited capacity of data synthesizers. For example, the training of GAN is unstable due to the alternative training of discriminator and generator. Even worse, the out-of-distribution patterns usually appear in the generated images, which jeopardizes the in-distribution data construction quality. Secondly, the in-distribution data construction quality evaluation is challenging due to lack of knowledge of in-distribution data. The most straightforward method is to train a model to distinguish whether the generated data is in-distribution. However, the trained model may not perform well when existing data noise or insufficient out-of-distribution data. Additionally, the human experts for generated data quality evaluation are expensive and not scalable. Either unreliability or cost of evaluation impedes the generated data evaluation in practice. Lastly, there is no unified data generation strategy that is suitable for all scenarios since different data modalities or even datasets may encompass different in-distribution patterns. For example, image data demonstrate a high spatial correlation and out-of-distribution data usually demonstrates erratic spatial patterns. More importantly, for the different datasets with the same data modal, the optimal data synthesizer still differs due to different data distributions.


\subsection{Out-of-distribution Evaluation}
\label{sec:42}
Out-of-distribution evaluation data refers to a set of samples that follow a distribution that differs from the one observed in the training data. We begin by discussing the need for out-of-distribution evaluation, followed by a review of two representative tasks: generating adversarial samples and generating samples with distribution shifts. Then we delve into the challenges associated with out-of-distribution data generation.




\subsubsection{Need for Out-of-distribution Evaluation}
Although modern machine learning techniques generally perform well on in-distribution datasets, the distribution of data in the deployment environment may not align with the training data~\cite{shen2021towards}. Out-of-distribution evaluation primarily assesses a model's ability to generalize to unexpected scenarios by utilizing data samples that differ significantly from the ones used during training. This evaluation can uncover the transferability of a model and instill confidence in its performance in unexpected scenarios. Out-of-distribution evaluation can also provide essential insights into a model's robustness, exposing potential flaws that must be addressed before deployment. This is crucial in determining whether the model is secure in real-world deployments.


\subsubsection{Generating Adversarial Samples}

Adversarial samples are the ones with intentionally manipulated or modified input data in a way that causes a model to make incorrect predictions. Adversarial samples can aid in comprehending a model's robustness and are typically generated by applying perturbations to the input data. Manual perturbation involves adding synthetic and controllable perturbations, such as noise and blur, to the original data~\cite{hendrycks2019benchmarking}.

Automated methods design learning-based strategies to generate perturbations automatically and are commonly classified into four categories: white-box attacks, physical world attacks, black-box attacks, and poisoning attacks. White-box attacks involve the attacker being provided with the model and victim sample. Examples of white-box attacks include Biggio's attack~\cite{biggio2013evasion}, DeepFool~\cite{moosavi2016deepfool}, and projected
gradient descent attack~\cite{madry2017towards}. Physical world attacks involve introducing real perturbations to real-world objects. For instance, in the work by~\cite{eykholt2018robust}, stickers were attached to road signs to significantly impact the sign identifiers of autonomous cars. Black-box attacks are often applied when an attacker lacks access to a classifier's parameters or training set but possesses information regarding the data domain and model architecture. In~\cite{papernot2017practical}, the authors exploit the transferability property to generate adversarial examples. A zero-th order optimization-based black-box attack is proposed in \cite{chen2017zoo} that leverages the prediction confidence for the victim sample. Poisoning attacks involve the creation of adversarial examples prior to training, utilizing knowledge about model architectures. For instance, the poison frogs technique~\cite{shafahi2018poison} inserts an adversarial image into the training set with a true label. By evaluating a trained model on various adversarial samples, we can gain a better understanding of the potential weaknesses of the model in deployment. This can help us take steps to prevent undesirable outcomes.





\subsubsection{Generating Samples with Distribution Shift} 

Generating samples with distribution shifts enables the evaluation of a model on a different distribution. One straightforward way is to collect data with varying patterns, such as shifts across different times or locations~\cite{ding2021retiring}, camera traps for wildlife monitoring~\cite{koh2021wilds}, and diverse domains~\cite{saenko2010adapting}. A more efficient approach would involve constructing the evaluation set from pre-collected data. To illustrate, some studies~\cite{gu2019using,shankar2021image} generate various sets of contiguous video frames that appear visually similar to humans but lead to inconsistent predictions due to the small perturbations.

Apart from natural distribution shifts in real-world data, synthetic distribution shifts are widely adopted, including three types: 1) covariate shift, which assumes that the input distribution is shifted~\cite{gretton2009covariate,sugiyama2007covariate}, 2) label shift, which assumes that the label distribution is shifted~\cite{lipton2018detecting,azizzadenesheli2019regularized}, and 3) general distribution shift, which assumes that both the input and label distributions are shifted~\cite{farahani2021brief,guan2021domain}. Biased data sampling can be used to synthesize covariate shifts or label shifts, whereas learning-based methods are typically required to synthesize general distribution shifts~\cite{farahani2021brief,guan2021domain}. Generating samples with distribution shift is essential in evaluating a model's transferability, especially when there is a distribution gap between the training and deployment environments.



\subsubsection{Challenges}

The challenges for out-of-distribution generation set construction are two-fold. Firstly, generating high-quality out-of-distribution data is challenging. If the training data is not representative, it may be difficult to generate appropriate data. Furthermore, the generation models may encounter mode collapse issues, meaning that they only generate a limited number of similar samples and disregard the diversity of the target distribution. Secondly, evaluating the quality of out-of-distribution generation is difficult since no single metric can capture the diversity and quality of the generated samples. Commonly used metrics, such as likelihood or accuracy, may not be suitable as they may exhibit bias toward generating samples similar to the training data. Therefore, various evaluation metrics have been proposed to assess the distance between in-distribution and out-of-distribution samples~\cite{sangkloy2017scribbler,borgwardt2006integrating,obukhov2020quality,betzalel2022study,jiang2023weight}. Overall, creating high-quality out-of-distribution data is a complex and demanding task that requires meticulous design.



\subsection{Prompt Engineering}
\label{sec:43}

With the advent of large language models, it becomes feasible to accomplish a task by solely fine-tuning the input to probe knowledge from the model, while keeping the model fixed. Prompt engineering is an emerging task that aims to design and construct high-quality prompts to achieve the most effective performance on downstream tasks~\cite{liu2023pre}. For example, when performing text summarization, we can provide the texts we want to summarize followed by specific instructions such as "summarize it" or "TL;DR" to guide the inference. Prompt engineering revolutionizes the traditional workflow by fine-tuning the input data rather than the model itself to achieve a given task.

A natural way is to perform \emph{manual prompt engineering} by creating templates. For example, in~\cite{schick2020few,schick2020exploiting,schick2020s}, the authors have pre-defined templates for few-shot learning in text classification and conditional text generation tasks. However, manually crafting templates may not be sufficient to discover the optimal prompts for complex tasks. Thus, \emph{automated prompt engineering} has been studied. Common programmatic approaches include mining the templates from an external corpus~\cite{jiang2020can} and paraphrasing with a seed prompt~\cite{yuan2021bartscore,haviv2021bertese}. Learning-based methods automatically generate the prompt tokens by gradient-based search~\cite{wallace2019universal} or generative models~\cite{gao2021making}. The primary obstacle in prompt engineering arises from the absence of a universal prompt template that consistently performs well. Various templates may result in different model behaviors, and obtaining the desired answers is not guaranteed. Therefore, further research is necessary to gain insight into the response of the model to prompts and guide the prompt engineering process.
