\documentclass[aps,prl,superscriptaddress]{revtex4-2}
%\documentclass[aps,prl,superscriptaddress,notitlepage]{revtex4-1}  % for PRL%
%\documentclass[aps,prl,superscriptaddress]{revtex4}  % for PRL

%\bibliographystyle{apsrev}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{bm}
\usepackage{amsmath}
\usepackage[normalem]{ulem}
\usepackage{bbold}
%\usepackage{subcaption}
\usepackage[caption=false]{subfig}

\newcommand{\oton}{$1,\ldots,n$ }
\newcommand{\hrho}{\hat{\rho}}
\newcommand{\ii}{{\rm i}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bbr}{\mathbf{r}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\hx}{\hat{x}}
\newcommand{\hp}{\hat{p}}
\newcommand{\hg}{\hat{g}}
\newcommand{\be}{b}
\newcommand{\al}{ a}
\newcommand{\tri}{\triangle}
\newcommand{\bpsi}{{\bf \psi}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bzed}{\mathbf{0}}
\newcommand{\qed}{\ \ \ {\bf Q.E.D.}}
\newcommand{\sep}{ \ \ \ , \ \ \ }
\newcommand{\boeta}{{\bf \eta}}
\newcommand{\bozeta}{{\bf \zeta}}
\newcommand{\xgeqy}{\bx \succeq \by}
\newcommand{\ygeqx}{\by \succeq \bx}
\newcommand{\xgy}{\bx \succ \by}
\newcommand{\proof}{{\it Proof. }}
\newcommand{\ygx}{\by \succ \bx}
\newcommand{\xeqy}{\bx \asymp \by}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqn}{\begin{eqnarray}}
\newcommand{\eeqn}{\end{eqnarray}}
\newcommand{\pp}{\partial}
\newcommand{\dd}{{\rm d}}
\newcommand{\ee}{{\rm e}}

\newcommand{\bee}{{\bf e}}
\newcommand{\eq}{Eq.\ }
\newcommand{\cf}{c.f.\ }
\newcommand{\ch}{ch.\ }
\newcommand{\eg}{e.g.,\ }
\newcommand{\eqs}{Eqs }
\newcommand{\fig}{Fig.\ }
\newcommand{\om}{\omega}
\newcommand{\ri}{{\rm I}}
\newcommand{\rn}{{\rm N}}
\newcommand{\cO}{{\cal O}}
\newcommand{\cA}{{\cal A}}
\newcommand{\cB}{{\cal B}}
\newcommand{\cP}{{\cal P}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cJ}{{\cal J}}
\newcommand{\Sec}{Sec.\ }
\newcommand{\g}{\gamma }
\newcommand{\app}{Appendix\ }
\newcommand{\la}{\langle}
\newcommand{\tI}{\tilde{I}}
\newcommand{\tS}{\tilde{S}}
\newcommand{\tR}{\tilde{R}}
\newcommand{\tbq}{\tilde{\bq}}
\newcommand{\tbk}{\tilde{\bk}}
\newcommand{\tbp}{\tilde{\bp}}
\newcommand{\tbh}{\tilde{\bh}}
\newcommand{\ra}{\rangle}
\newcommand{\oS}{{\cal S}}
\newcommand{\bU}{{\bf U}}
\newcommand{\vnab}{{\bf \nabla}}
\newcommand{\wrt}{{\rm with \ respect \ to \ }}
\newcommand{\tran}{\mathsf{T}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bnabla}{\bm{\nabla}}

\newcommand{\cred}{\color{red}}
\newcommand{\cblue}{\color{blue}}
\newcommand{\cgreen}{\color{green}}
\newcommand{\cmag}{\color{magenta}}
\newcommand{\cflc}[1]{{\color{blue} [[#1]]}}
\newcommand{\ts}{\textsuperscript}

\newcommand{\tbr}[1]{{\color{cyan} \sout{#1}}}
\newcommand{\tbt}[1]{{\color{cyan} #1}}
\newcommand{\tbc}[1]{{\color{cyan} [[TB: #1]] }}

\newcommand{\akr}[1]{{\color{red} \sout{#1}}}
\newcommand{\akt}[1]{{\color{red} #1}}
\newcommand{\akc}[1]{{\color{red} [[AK: #1]] }}

\begin{document}
% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}
%Title of paper
\title{Supplemental material for machine learning of topological defects in confluent tissues. }
\author{Andrew Killeen}
\email{a.killeen18@imperial.ac.uk}
\affiliation{Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, U.K.}
\author{Thibault Bertrand}
\email{t.bertrand@imperial.ac.uk}
\affiliation{Department of Mathematics, Imperial College London, South Kensington Campus, London SW7 2AZ, U.K.}
\author{Chiu Fan Lee}
\email{c.lee@imperial.ac.uk}
\affiliation{Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, U.K.}
\maketitle

\section{AVM implementation and parameters}
We simulate $N = 400$ cells in a fluid like state with parameters $A_0=1$, $P_0=3.7$, $f_0=0.5$, $\zeta=1$ and $D_r=1$. We numerically integrate the equations of motion using the Euler-Maruyama method with time step $\Delta t=0.01$. 

We initialize the simulation by arranging $N$ cells on a hexagonal lattice, with grid spacing $d=\sqrt{2/\sqrt{3}}$, in a periodic domain with dimensions $dN\times (\sqrt{2}/3)dN$. We then draw a Voronoi diagram from the seeded points to obtain the initial positions of the vertices. The choice of grid spacing gives edges of length $a = d/\sqrt{3}$ and ensures that all cells initially have unit area. This means the average cell area throughout the simulation $\bar{A}=1$ and we use $\sqrt{\bar{A}}$ as our unit length. To ensure different realisations of the system were independent, cells have random initial polarities and we integrate through at least $2\times10^3$ time units to initialise the system. 

\section{Machine learning model implementation and parameters}
To identify ROIs we interpolate our input data to a fine grid with grid spacing $\Delta x$ = $\Delta y$ = 0.2, where the average cell length is approximately 1 length unit. We then smooth the data by passing a sliding window of size $9\Delta x\times9\Delta x$ over the data. We use a window of the same size to calculate the scalar order parameter at each point. Our threshold value of $S$ for identifying ROIs $S_{th}=0.15$. Our ROIs are then also $9\Delta x\times9\Delta x$ in size, meaning the inputs to our model are $9\times9$ grids.

We implement our model in Python using the TensorFlow library. Our convolutional neural network (CNN) has two layers, both detecting 32 features, the first has feature detectors of size $6\times6$ and the second $3\times3$. Our 100 neuron fully-connected layer uses L2-regularisation with strength $\lambda=0.01$. We chose this architecture as it achieved the highest classification accuracy on the training data, although we note that our results are not sensitive to the particular architecture used and similar architectures achieved comparable, albeit slightly lower, accuracies. To avoid overfitting when training the model, we use dropout on the fully-connected layer, leaving out a random 50\% of the neurons in each training batch. All layers use rectified linear units as their output function with the exception of the output layer, which uses softmax. 

\section{Sensitivity to grid size}
To further evidence the suitability of our method for experimental data analysis, we examine the sensitivity of the model to the size of the fine grid to which we interpolate the input data. Different systems will have different sized cells and different sized defects relative to the size of these cells. So, while we tune our window size to the size of defects in our system, it may be less clear what the correct size is in experimental systems, so our model should be able to accommodate different grid sizes without it impacting performance. To investigate this, we assess the accuracy of our trained model and the winding number method in classifying the test data at different grid sizes. We do not retrain our model with input data at the new grid size, we use the original model where the weights were trained on data interpolated to the original grid size ($\Delta x=0.2$). To enable comparisons with our ground truth, for each ROI we take the coordinates of the centre and interpolate the cell data to a new grid size about this central point. As our model takes as its input a $9\times9$ grid, inputting data at a new grid size is the same as changing the size of our ROIs around the defect center. We look at a range of grid sizes from $\Delta x=0.1$, which gives an ROI with a window length approximately one cell across, to $\Delta x=0.8$, meaning our interpolated grid is of the same order as the typical cell length and the ROI window lengths are approximately four times larger than the typical defect size. When smoothing the interpolated field, we scale the size of our sliding window such that the area over which we average is approximately constant.

The classification accuracy as a function of grid size can be seen in Fig.\,1. With the exception of the smallest grid size, our CNN consistently outperforms the winding number. Along with being more accurate, our method is also less sensitive to the grid size than the winding number, whose performance drops sharply when the grid size varies from that which gives the highest accuracy, and is no better than random selection when the grid size is greater than 0.5. The CNN, however, is able to maintain a level of accuracy greater than, or similar to, the winding number's best performance even when the ROI is approximately four times larger than the size of the defect. This demonstrates the robustness of our approach and the ability of method to detect defects even when the parameters of the model are not perfectly tuned to the system.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=100mm]{FigS1_App.eps}
    \end{center}
\caption{{\bf Machine learning model is less sensitive to system parameters than winding number.}
Classification accuracy vs. grid size for the CNN and the winding number method.}
\label{acc_v_grid}
\end{figure}

\bibliography{references}
\end{document}

%@InProceedings{Glorot2010,
%  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
%  author = 	 {Glorot, Xavier and Bengio, Yoshua},
%  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
%  pages = 	 {249--256},
%  year = 	 {2010},
%  editor = 	 {Teh, Yee Whye and Titterington, Mike},
%  volume = 	 {9},
%  series = 	 {Proceedings of Machine Learning Research},
%  address = 	 {Chia Laguna Resort, Sardinia, Italy},
%}
