\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}

\usepackage{url}
\usepackage{optidef}
\usepackage{amsmath,amssymb,amsfonts,amsthm,enumerate}
\usepackage{eucal} 
\usepackage{xcolor}
\usepackage{youngtab}
\usepackage{young}
\usepackage{lscape}
\usepackage{tikz}
\usepackage{environ}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
%\usepackage{algpseudocode}
\usetikzlibrary{positioning, backgrounds}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath,amsthm,amssymb,amscd}
\usepackage[export]{adjustbox}



\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\mat}{Mat}
\newcommand{\densalpha}{\mathcal{X}}
\newcommand{\densalphao}{\mathcal{X}^{\circ}}

\DeclareMathOperator{\affhull}{aff}
\newcommand{\powmap}{\pi}
\newcommand{\pmap}{\iota}
\newcommand{\dmap}{\tilde{f}}
\newcommand{\betti}{\beta}
\newcommand{\powball}{B}
\newcommand{\powset}{\mathcal{P}}
\newcommand{\sigl}{\mathcal{S}}
\newcommand{\witmap}{\Phi}
\newcommand{\cover}{\mathcal{U}}
\newcommand{\powcover}{\mathcal{V}}
\newcommand{\mindens}{d_0}
\newcommand{\grad}{\nabla}
\newcommand{\region}{\mathcal{R}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\bO}{\mathsf{O}}
\newcommand{\bW}{\mathsf{W}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\bE}{\mathsf{E}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\bN}{\mathsf{N}}
\newcommand{\lt}{\mathfrak{t}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\Cs}{\C^\times}
\newcommand{\suplev}[1]{\mathcal{L}(#1)}
\newcommand{\Pp}{\mathbb{P}^1}
\newcommand{\Pt}{\mathbb{P}^2}
\newcommand{\Ps}{\mathbb{P}^*}
\newcommand{\cE}{\mathcal{E}}
%\newcommand{\isingint}[1]{G_{i}(#1)}
%newcommand{\isingcirc}[1]{G_{c}(#1)}
%\newcommand{\isingflares}[1]{G_{f}(#1)}
\DeclareMathOperator{\isingint}{int}
\DeclareMathOperator{\isingcirc}{circ}
\DeclareMathOperator{\isingflares}{flares}
\newcommand{\lang}{\left\langle}
\newcommand{\rang}{\right\rangle}
\newcommand{\blang}{\big\langle}
\newcommand{\brang}{\big\rangle}
\newcommand{\Bv}{\Big |}
\newcommand{\bv}{\big |}
\newcommand{\lv}{\left |}
\newcommand{\rv}{\right |}
\newcommand{\Lt}{\mathfrak{t}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\zee}{\mathfrak{z}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Gp}{\Gamma_+}
\newcommand{\Gm}{\Gamma_-}
\newcommand{\eps}{\epsilon}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\Om}{\Omega}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\mm}{\mathbf{m}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\sig}{\sigma}
\newcommand{\activeset}{J}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\complex}{X}
\newcommand{\numlandmarks}{n}
\newcommand{\numdatapoint}{N}
\newcommand{\numfeatures}{d}
\newcommand{\witnesses}{\mathcal{W}}
\newcommand{\landmarks}{\mathcal{L}}
\newcommand{\sites}{\mathcal{B}}
\newcommand{\datapoint}{x}
\newcommand{\wit}{\mathcal{W}}
\newcommand{\dshape}{\mathcal{S}}
\newcommand{\shape}{\mathcal{S}}
\newcommand{\diffusion}{\rho}
\newcommand{\powers}{p}
\newcommand{\dom}{\mathcal{M}}
\newcommand{\domain}{\mathcal{A}}
\newcommand{\weightmap}{\pi}
\newcommand{\barymap}{F}
\newcommand{\points}{S}
\newcommand{\neighbors}{N}
\newcommand{\featspace}{\mathcal{M}}

\DeclareMathOperator{\Tr}{Tr}

\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\conf}{Conf}
\DeclareMathOperator{\uconf}{UConf}
\DeclareMathOperator{\lazy}{Lazy}
\DeclareMathOperator{\emptymap}{EmptyMap}
\DeclareMathOperator{\emptyplex}{EmptyPlex}
\DeclareMathOperator{\skel}{Skel}
\DeclareMathOperator{\oneskel}{OneSkel}
\DeclareMathOperator{\starsig}{star}
\DeclareMathOperator{\dualcell}{DualCell}
\DeclareMathOperator{\ballcov}{CechCov}
\DeclareMathOperator{\powcov}{PowCov}
\DeclareMathOperator{\cechplex}{\check{C}ech}
\DeclareMathOperator{\bary}{Sd}
\DeclareMathOperator{\denspow}{DensPow}
\DeclareMathOperator{\pval}{wt}
\DeclareMathOperator{\anyelt}{AnyElt}
\DeclareMathOperator{\randelt}{RandElt}
\DeclareMathOperator{\nerve}{Nrv}
\DeclareMathOperator{\gaussfit}{GaussFit}
\DeclareMathOperator{\gaussweight}{GaussWeight}
\DeclareMathOperator{\gaussmean}{GaussMean}
\DeclareMathOperator{\powdiag}{PowDiag}
\DeclareMathOperator{\alphaplex}{Alpha}
\DeclareMathOperator{\weakwit}{WeakWit}
\DeclareMathOperator{\graph}{Graph}
\DeclareMathOperator{\strongwit}{StrongWit}
\DeclareMathOperator{\densplex}{DensPlex}
\DeclareMathOperator{\strictdenswit}{StrictDensWit}
\DeclareMathOperator{\denswit}{DensWit}
\DeclareMathOperator{\densland}{DensLand}
%\DeclareMathOperator{\densalpha}{DensAlpha}
\DeclareMathOperator{\delaunay}{Delaunay}
\DeclareMathOperator{\delaunaytriangulation}{DT}
\DeclareMathOperator{\delaunaycomplex}{DC}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\ldeg}{ldeg}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Diff}{Diff}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\vor}{Vor}
\DeclareMathOperator{\dlny}{DT}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\chull}{conv}
\DeclareMathOperator{\Wit}{Wit}

\DeclareMathOperator{\primalqp}{PrimalQP}
\DeclareMathOperator{\dualqp}{DualQP}


\newcommand{\face}{F}

\newtheorem*{thma}{Theorem A}
\newtheorem*{lemmaa}{Lemma A}
\newtheorem*{lemmaa1}{Lemma A$'$}
\newtheorem{thm}{Theorem}
\newtheorem{thetheorem}{thm}
\newtheorem{lemma}{Lemma}
\newtheorem{conj}{Conjecture}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{remark}{Remark}

\newtheorem*{example1a}{Example 1a}
\newtheorem*{example1b}{Example 1b}
\newtheorem*{example2a}{Example 2a}
\newtheorem*{example2b}{Example 2b}
\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{alg}{Algorithm}
\newtheorem{prob}{Problem}

\counterwithin{figure}{subsection}

\counterwithin{equation}{section}

\counterwithin{alg}{section}

\title{Alpha shapes in kernel density estimation}

\author{Erik Carlsson and John Carlsson}
\begin{document}
\maketitle
\begin{abstract}
A foundational problem in topology and data
is to determine the topological type
(e.g. the persistent homology groups) 
of the superlevel sets 
$\suplev{a}=f^{-1}[e^{-a},\infty)$ of a sum of 
Gaussian kernels $f(x)=\sum_i a_i \exp(-\lVert x-x_i\rVert^2/2h^2)$ for $\{x_i\}\subset \mathbb{R}^d$.
In this paper, we show that each 
$\suplev{a}$ coincides with the union 
of a certain power-shifted covering by balls, whose centers range over a closed subspace
of the convex hull $\shape(a)\subset \chull(\{x_i\})$. We then present an explicit homotopy equivalence $p:\suplev{a}\rightarrow \shape(a)$, realizing $\shape(a)$ as a continuous version of the alpha shape.
This leads to a prescription for 
modeling noisy point clouds
by density-weighted alpha complexes which, 
in addition to computing persistent homology,
give rise to refined geometric models.
In order to compute alpha complexes in higher dimension, we used a recent algorithm due to the present authors based on the duality principle \cite{carlsson2023alpha}.
\end{abstract}

\section{Introduction}

Let $f:\mathbb{R}^d\rightarrow \mathbb{R}_+$ be a sum of Gaussian kernels
with uniform covariance matrices, which after 
a linear change of coordinates takes the form
\begin{equation}
\label{eq:introkde}
%f: \mathbb{R}^d\rightarrow \mathbb{R}_{+},\quad 
f(x)=\sum_{i=1}^N a_i \exp\left(-\lVert x-x_i\rVert^2/2h^2\right),
\end{equation}
where $a_i>0$, and $h>0$ is the bandwidth, or scale parameter.
We will denote the superlevel sets of $f$ by
\[\suplev{a}=f^{-1}[e^{-a},\infty) =
\left\{x\in \mathbb{R}^d:f(x)\geq e^{-a}\right\}
\]
using $a$ in negative log coordinates so that the family of subspaces is increasing.
The following problem is 
well-studied \cite{chazal2013bootstrap,phillips2015geometric,bubenik2012statistical,fasy2014confidence}:
\begin{prob}
\label{prob:superlev}
Determine the topological type (for instance, the persistent homology groups) of $\suplev{a}$.
\end{prob}
One of the reasons to study this problem is that filtering by density gives a rigorous alternative to simply removing outliers or noise by a subjective criteria. For instance, the degree zero persistent homology can be taken as a precise definition of 
hierarchical clusters \cite{chazal2011clustering}.
Another is that it would encode the landscape of a density function, which measures how the shapes change with a minimum density threshold.


However, it has been
proved difficult to compute.
To illustrate some of the difficulties that arise, consider the following simple algorithm:
\begin{alg} \label{alg:intro}
Approximate the persistent homology groups of $\suplev{a}$.
\begin{enumerate}
    \item \label{item:naivealgsample} Sample $M$ points $\{\tilde{x}_1,...,\tilde{x}_{M}\}$ from the underlying distribution of $f(x)$, assumed to be sorted in 
    increasing order of $-\log(f(\tilde{x}_i))$, so that $\tilde{x}_1$ is the densest.
    \item Initialize $S=\emptyset$. 
    For each $\tilde{x}_i$ in order, make the replacement $S\mapsto S\cup \{\tilde{x}_i\}$ if $\tilde{x}_i$ is at least distance $\epsilon$ to all existing points of $S$, for some predetermined choice of $\epsilon>0$.
    \item Construct a filtered family of 
    simplicial complexes $X(a)$ using a general methods, such as weighted Vietoris-Rips, on the respective vertex sets given by $S\cap \suplev{a}$.
\item Calculate persistent homology.    
\end{enumerate}
\end{alg}
Algorithm \ref{alg:intro} seems intuitive, but it has undesirable propeties. One is that
while the homotopy type of the super-level sets 
$\suplev{a}$ is a function only of the pairwise distances $\lVert x_i-x_j\rVert$, 
the expected size of the vertex set $S$ grows
rapidly by simply adding coordinates of 
zeroes to the end of every $x_i$, 
as it is proportional to 
the covering number super-level set $\suplev{a}$ by 
$\epsilon$-balls. 
Another issue is that the expected density of a sample point $f(\tilde{x}_i)$, which is used to sort the points and filter homology, becomes small and dominated by noise for $d\gg 0$.



An alternate approach to producing a 
finite vertex set is
simply to discretize space, so that one can apply 
cubical homology, 
but this is clearly only possible in very low dimensions.
Another is to enumerate 
the critical points of $f(x)$ and compute the discrete Morse complex \cite{forman1998morse,robins2011theory},
but this can in general lead to a combinatorial explosion in the number of points, including the 
case of sums of Gaussian kernels \cite{edelsbrunner2013risk}.
An intuitive fix is to use something similar to 
Algorithm \ref{alg:intro}, 
but using the data set 
$\{x_1,...,x_N\}$ itself in place of of the samples 
$\{\tilde{x}_1,...,\tilde{x}_{M}\}$ in item \ref{item:naivealgsample}, so that the final answer is technically independent of the dimension of the embedding. However, this is not a stable solution; as
long as the underlying density that 
produced the $\{x_i\}$ is nonzero on all of 
$\mathbb{R}^d$, a 
large enough sample will eventually fill out space,
so that we end up with the same poor scaling
with the covering number as in the original algorithm.

Once a density-weighted vertex set $S$ has been chosen, there are a number of persistence constructions 
that are robust with respect to noise, 
as well as statistically rigorous 
results about their output.
Some of these methods include
explicitly removing those points, and calculating distance-based complexes such as Vietoris-Rips or witness \cite{carlsson2004witness}; the aforementioned
applications of zeroth-dimensional persistence to clustering \cite{chazal2011clustering}, in which one scans density values over connecting edges; 
the distance to measure filtration,
which is stable with respect to the 2-Wasserstein distance \cite{chazal2011geometric};
Other constructions which study rigorous 
properties of persistent homology, in the specific
context of kernel density estimators
\cite{phillips2015geometric,bobrowski2017topological};
persistence landscapes, and studies of spaces of persistence diagrams 
\cite{adcock2013ring,bubenik2012statistical,blumberg2012robust}; and multidimensional persistence, which 
simultaneously filters by density and scale
\cite{carlsson2007multidimensional,carlsson2009computing,lesnick2011theory,blumberg2020stability}.

\subsection{Proposed method}

Our method begins by replacing $f$ by an
modified function $\tilde{f}$,
whose sublevel sets are 
topologically equivalent but
better behaved.
%Our method begins with the following
%fact: 
Let $f(x)$ be as in \eqref{eq:introkde},
and consider the transformed function
$\tilde{f}:\mathbb{R}^d\rightarrow \mathbb{R}_+$
given by
\begin{equation}
\label{eq:introftilde}
\tilde{f}(y)=\inf_{x\in \mathbb{R}^d}
f(x) \exp(\lVert x-y\rVert^2/2h^2).
\end{equation}
By taking $x=y$, it is obvious that $\tilde{f}(y)\leq f(y)$, but it is not immediately 
clear that $\tilde{f}$ is even nonzero.
However, we show (Proposition \ref{prop:legendre}) that one 
can recover the original function by the formula
\[f(x)=\sup_{y\in \mathbb{R}^d} \tilde{f}(y)\exp(-\lVert x-y\rVert^2/2h^2) \Longleftrightarrow\]
\begin{equation}
\label{eq:introffromtilde}
-\log(f(x))=\inf_{y\in \shape}
(\lVert x-y\rVert^2/2h^2+\alpha(y))
\end{equation}
where $\alpha(y)=-\log(\tilde{f}(y))$, and
$\shape\subset \mathbb{R}^d$ is the domain
on which $\tilde{f}$ is nonzero.
This is seen this by first showing that for any $f(x)$, 
the modified function $-h^2\log(f(x))+\lVert x \rVert^2/2$ is convex. Equation \eqref{eq:introffromtilde} then follows 
from the Fenchelâ€“Moreau theorem, which states that any convex function satisfies
$F=F^{**}$, where 
$F^*=\sup_{y\in \mathbb{R}^d}(x\cdot y-F(y))$ is 
the Legendre transform.


We now observe 
that the last expression in 
\eqref{eq:introffromtilde} expresses 
$\suplev{a}$
as the union of an infinite 
covering by closed balls,
\begin{equation}
    \label{eq:intropow}
\suplev{a}=\bigcup_{y\in \shape(a)}
B_{r}(y),\quad r=\sqrt{2h^2(a-\alpha(y))}
\end{equation}
where $\shape(a)=\alpha^{-1} (-\infty,a]\subset \shape$ is the sublevel set.
From this point of view,
Algorithm \ref{alg:intro} is backwards because
it is selecting the vertices of a simplicial complex from 
the union of a cover, instead of selecting a 
subcover and taking the nerve.
In the case of \eqref{eq:intropow},
a subcover associated to a finite subset
$S\subset \shape$ is known as
a \emph{power diagram} with power map $p=-2h^2\alpha$.
Its nerve is the alpha complex, 
whose geometric realization the alpha shape \cite{edelsbrunner1995union,edelsbrunner1983shape}.
Alpha complexes have a number of 
theoretical advantages beyond persistent homology, namely that they are minimal in size, naturally embedded in space, and can be used to generate to beautiful geometric models \cite{edelsbrunner1983molecule}.

Our main theorem states that the 
$\shape(a)$ are continuous versions of 
alpha shapes. Theoretically speaking, one useful property of this shape is that $\shape(a)$ is coordinate-free, in the sense that it is covariant under linear changes of coordinates (including extra coordinates); practically speaking, as our experiments will later demonstrate, this represents an attempt to address the curse of dimensionality by ``projecting'' noise introduced by the kernel density estimator back into the core ``shape'' of the original dataset.
\begin{thma}
We have
\begin{enumerate}
\item 
%We have that $-2h^2\log(f(x))=\inf_{y\in \shape} \lVert x-y\rVert^2+\alpha(y)$, which is the weight function of a power diagram with infinitely many vertices. 
    \label{item:introthmhull} The total space $\shape$ is an open subset of the convex hull of the Gaussian centers 
    $\dataset=\{x_i\}$, and each $\shape(a)$ is a closed subset of $\suplev{a} \cap \shape$.
\item \label{item:introthmsurj} We have a surjective map $p:\suplev{a}\rightarrow \shape(a)$, which
    takes the form of an expectation:
    \begin{equation}
\label{eq:introsurj}
    p(x)= \frac{\sum_{i}\exp(-\lVert x-x_i\rVert^2/2h^2) x_i}{\sum_{i} \exp(-\lVert x-x_i\rVert^2/2h^2)}
    \end{equation}
    \item \label{item:introthmhomotopy}
    The inclusion map $i:\shape(a)\hookrightarrow \suplev{a}$ induces a homotopy equivalence with homotopy inverse $p$. If additionally the $\{x_i\}$ affinely span $\mathbb{R}^d$, then $p$ is a homeomorphism.
\end{enumerate}
\end{thma}



In Section \ref{sec:finplex}, we use Theorem A to
modify Algorithm \ref{alg:intro} so that it selects
vertices from $\shape(a)$ instead of $\suplev{a}$,
which by item \ref{item:introthmhomotopy} has the same homotopy type.
Due to the explicit form of the inverse homotopy 
from item \ref{item:introthmsurj}, we may
generate a sample from $\shape$ 
by sampling $\tilde{x}_i$ as usual,
and setting $\tilde{y}_i=p(\tilde{x}_i)$.
It follows from basic properties of Gaussians
that the coefficients in \eqref{eq:introsurj} are independent of the embedding $\dataset \subset \mathbb{R}^d$, and therefore so is the sampling procedure. 
Now the potential number of vertices is only
determined by the $\epsilon$-covering number of $\shape(a)$, which (unlike the covering number of $\suplev{a}$) has no dependence on the embedding dimension, 
because $\shape(a)$ is
contained in the convex hull by part \ref{item:introthmhull}.
We then build the 
alpha complex of the 
corresponding shape instead of a
general construction such as Vietoris-Rips.
An illustration of the results of is shown in the case of spatial density estimation in Figure \ref{fig:species}.


\begin{figure}[ht]
    \centering
    \includegraphics[scale=.5]{figures/species/figure_1/speciesvert.png}
    \caption{
 An illustration of the pipeline from
    Section \ref{sec:finplex}.
    In the top left, we have a well-known geographical data set from \cite{phillips2006maximum}.
    Below that we have a heat map of a kernel density estimator with a certain scale parameter, 
    and the result of sampling many points
    $\tilde{y}_i=p(\tilde{x}_i)$ from the shape, colored by the value of $\alpha(\tilde{y}_i)$.
    On the right, we have the resulting alpha complex and its shape, contained in the total space of a power diagram, which closely resembles 
    the density superlevel set by the sampling algorithm.}
    \label{fig:species}
\end{figure}

\subsection{Examples}

In Section \ref{sec:experiments}, 
we present several
constructions using this method, many
using the Metropolis algorithm to generate point clouds. 
One example is an interesting energy landscape, whose persistent homology reveals a filtered version of a configuration space.
Another uses connected components to
reveal local basins in 
a loss functions in a nonlinear regression problem
\cite{seber2005nonlinear}, using Bayesian sampling. 
Also within Bayesian models, we obtain a 
geometric representation of a simple singular model
by two univariate Gaussian mixtures 
\cite{watanabe2009algebraic}.
These two examples illustrate how a varying metric 
may be replaced by the Euclidean metric with more coordinates, as well as our assertions that
our constructions are stable in higher dimension.
The next example follows a similar setup as in
as \cite{carlsson2008klein}, in which the authors
used the witness complex and persistent homology to 
detect subspaces of the Klein bottle in 
random $3\times 3$ image patches from 
the van Hatern-Schaaf natural image data set \cite{hateren_schaaf_1998}.
Instead of calculating homology groups, we use alpha shapes to generate precise geometric models from a different type of patch
taken from the MNIST data set of hand-drawn digits.
In the last one we model 
energy landscapes of discrete simulations from the 
graphical Ising model, using spectral bases 
on the underlying graph to generating
appropriate Euclidean embeddings.

In order to compute alpha complexes, we used a recent algorithm due to the present authors \cite{carlsson2023alpha},
based on the duality principle in optimization.
Unlike other mainstream constructions,
that algorithm is suitable in higher dimension
because the dual variables are a function
only of pairwise dot products, and so have no explicit dependence on the embedding dimension $d$.
In every example in Section \ref{sec:experiments},
computing the alpha complex took at most a few seconds.
On the other hand, the higher dimensional examples could not have been calculated using standard methods,
which almost always begin by computing the full 
(shifted) Delaunay triangulation, which can be astronomically large.

All our code for this paper was
written in MAPLE, and is available on the first author's website.

\subsection{Acknowledgements}

Both authors were supported by the Office of Naval Research, project (ONR) N00014-20-S-B001.

\section{Notation and preliminaries}

We summarize some preliminary definitions 
and notation for kernel density estimation and computational topology, including the power diagram and alpha complexes.

\subsection{Kernel density estimators}

\label{sec:kde}

Let $\dataset =\{x_1,...,x_N\}
\subset \mathbb{R}^d$ be a
point cloud. A Gaussian kernel density estimator is a sum of the form
\begin{equation}
\label{eq:kde}
f(x)=\sum_{i=1}^N a_i K_h(x-y),
\quad K_h(v)=\exp(-\lVert v\rVert^2/2),
\end{equation}
for $a_i>0$. 
We will be interested in the superlevel sets 
\begin{equation}
    \label{eq:suplev}
    \suplev{a}=f^{-1}[e^{-a},\infty)=\left\{x:f(x)\geq e^{-a}\right\}
\end{equation}

For simplicity, we consider only finite sums, 
but our results apply to the convolution of more general distributions by Gaussian kernels. We will assume the the norm is always the standard $L^2$-norm, 
as any other quadratic form can 
be transformed in that way by a linear change of coordinates. Moving (anisotropic) metrics are an interesting extension, which we hope to study in future papers.
For now, we remark that general Riemannian metrics can often be approximated by Euclidean ones in higher dimensions, for instance using spectral embeddings, which will be used
in Section \ref{sec:experiments}. 
A more general setup might involve replacing
\eqref{eq:kde} by the convolution of a distribution on a Riemannian manifold by a heat kernel with respect to the metric.

\subsection{Computational topology}

%We review some background and notation from computational topology. For more details, we refer the reader to %\cite{edelsbrunner2010computational}.

By a simplicial complex on a vertex set $S$,
we will mean a collection of nonempty subsets of
$S$ that is closed under taking nonempty subsets. 
If the vertex set comes equipped with a map to a vector space, 
for instance if it is described as an explicit subset $S=\{p_1,...,p_n\}\subset 
\mathbb{R}^d$, then we have its geometric realization
defined by
\begin{equation}
    \label{eq:geometric}
|X|=\bigcup_{\sigma\in X} \chull(\sigma),
\end{equation}
which is the union of the convex hulls of the vertices of all simplices, i.e.
\begin{equation}
    \label{eq:chull}
\chull(\{x_i\})=\left\{ \sum_{i} c_i x_i : c_i\geq 0,\sum_i c_i=1\right\}
\end{equation}
The affine span, or affine hull
$\affhull(\{x_i\})$ is the smallest affine subspace containing it, which is the same as 
the convex hull but wihtout the condition that $c_i\geq 0$.

If $\mathcal{U}=\{U_x:x\in S\}$ is a collection of 
(closed or open) subsets of $\R^d$, then the nerve of $\mathcal{U}$ is the complex
\begin{equation}
\label{eq:nerve}
\nerve(\mathcal{U})=\left\{\sigma \subset S : 
U_{\sigma_0} \cap \cdots \cap U_{\sigma_k}
\neq \emptyset \right\}.
\end{equation}
Generally speaking, nerve theorems state that if
$\mathcal{U}$ satisfies certain conditions, for instance if every $k$-fold union is contractible, then the nerve complex is homotopy equivalent to the union
$\bigcup \mathcal{U}=\bigcup_{x\in S} U_x$. 
\cite{Leray1950LanneauSE,
friedlander1983etale,
borsuk1948imbedding}.
Different versions involve different realizations
of the nerve as a topological space.
In one version,
suppose that each $U_x$ is convex, and choose representatives 
$x_{\sigma}\in U_{\sigma_0}\cap \cdots \cap U_{\sigma_k}$.
Then as in \cite{bauer2023unified}, we have a
linear map $\Gamma:|\bary(X)|\rightarrow \R^d$ on the barycentric subdivision of the nerve,
whose value on each vertex $\sigma$ is $x_\sigma$.
By convexity, it is clear that its image is contained in $|\cover|$. 
Theorem 3.1 from that reference states:
\begin{prop}
\label{prop:nerve}
If $\cover$ is convex 
then $\Gamma$ is a homotopy equivalence, specifically the one from the nerve theorem.
\end{prop}
Beyond giving an explicit geometric realization of the nerve, this version gives an explicit form of the map that induces the nerve isomorphism.


In persistent homology, we will also have filtered families of simplicial complexes.
\begin{defn}
A filtered simplicial complex is a pair $(X,w)$ where $X$ is a simplicial complex, and $w:X\rightarrow \mathbb{R}$ 
has the property that $X(a)=w^{-1}(-\infty,a]$ is a subcomplex of $X$ for all $a$.
\end{defn}
One source of filtered complexes comes from taking the nerve of a nested family of covers
$\mathcal{U}(a)=\left\{U_i(a)\right\}$, where $U_i(a)\subset U_i(b)$ for $a\leq b$. 
For instance,
the \v{C}ech and alpha complexes arise in this way.


For any filtered complex, we may compute its persistent homology groups 
\cite{Carlsson2009TopologyAD,carlsson2009computing,otter2015roadmap,edelsbrunner2010computational}. All barcode diagrams
generated for this paper were calculated
using \texttt{javaplex} \cite{adams2014javaplex}.




\subsection{Alpha complexes}

\label{sec:alpha}

For a reference on this section, we refer to 
\cite{aurenhammer1984optimal,
edelsbrunner1995union,
edelsbrunner2010computational}.
Let $S=\{p_1,...,p_n\}\subset 
\R^d$ be a collection of points, and let
$\pi :S\rightarrow \R$ be a function with values $\pi(p_i)=\pi_i$, called the powers. 
We have the weight map 
$w=w_{S,\pi}:\R^d\rightarrow \R$ defined by
\begin{equation}
\label{eq:powdist}
w(x)=\min_{p_i\in S} w_{i}(x),\quad w_{i}(x)=
\lVert x-p_i\rVert^2-\pi_i.
\end{equation}
Then we can define a family of 
covers $\mathcal{U}=\mathcal{U}_{S,\pi}$
by $\mathcal{U}(a)=\{U_i(a)\}$ for each $a\in \R$, where
\[U_{i}(a)=\left\{x : w_i(x)\leq a\right\}.\]
Then
\begin{equation}
    \label{eq:weightunion}
    w^{-1} (-\infty,a]=\bigcup \mathcal{U}(a)
\end{equation}
\begin{defn}
\label{def:powdiag}
The power diagram $\mathcal{V}=\mathcal{V}_{S,\pi}$ associated to $(S,\pi)$
is the collection 
of closed regions 
$\powcover(a)=\{V_i \cap U_i(a)\}$ 
where
\[V_i=\left\{x \in \R^d :
w_i(x) \leq w_j(x) \mbox{ for all $j$}\right\}.\]
\end{defn}
When $\pi_i=0$ for all $i$, the $V_i$ are the cells of the usual Voronoi diagram. More generally, the regions are still determined by linear inequalities, in other words are separated by hyperplanes. In fact, for
$\pi_i\geq 0$ they arise the intersection of true Voronoi diagrams with a linear subspace, with the $\pi_i$ representing the negative 
squared normal distances.

\begin{defn}
The weighted alpha complex is the nerve of the
power diagram
$X(a)=\nerve(\powcover(a))$. The alpha shape is the geometric realization $|X(a)|\subset \mathbb{R}^d$,
defined by identifying the vertices with 
$S\subset \mathbb{R}^d$.
\end{defn}
Written as a pair $(X,w)$, the full alpha complex
$X$ is the nerve of the shifted Voronoi diagrm
$\{V_i\}$, and
\begin{equation}
\label{eq:alphaweight}
w(\sigma)=\inf_{x\in V_{\sigma}} w(x)
\end{equation}
It follows from the nerve theorem
that $X(a)$ is homotopy equivalent to 
$\bigcup \mathcal{V}(a)$. In
\cite{edelsbrunner1995union}, 
Edelsbrunner proved that $|X(a)|\subset \bigcup \mathcal{V}(a)$ is a homotopy equivalence
with an explicit deformation retraction.
Figure \ref{fig:alphalet} illustrates the union of the cover and the corresponding shapes in the unweighted case.

\begin{figure}
\centering
\begin{subfigure}[b]{.32\textwidth}
\includegraphics[scale=.2]{figures/letter_a/figure_1/alphalet1.png}
\end{subfigure}
\begin{subfigure}[b]{.32\textwidth}
\includegraphics[scale=.2]{figures/letter_a/figure_1/alphalet2.png}
\end{subfigure}
\begin{subfigure}[b]{.32\textwidth}
\includegraphics[scale=.2]{figures/letter_a/figure_1/alphalet3.png}
\end{subfigure}
\caption{A sequence of power diagrams with their corresponding alpha shapes.}
\label{fig:alphalet}
\end{figure}

\section{Alpha shapes of Gaussian KDE's}

We present our main theorem, and our proposed algorithm for sampling finite alpha complexes.

\subsection{Main theorem}

Let $\dataset=\{x_1,...,x_N\}\subset \mathbb{R}^d$, and consider a sum $f(x)$ of Gaussian kernels 
as in \eqref{eq:kde}.
In order to define our main construction, consider the following function:
\begin{equation}
\label{eq:ftilde}    
\tilde{f}(y)=\inf_{x\in \mathbb{R}^d}
f(x)K_h(x-y)^{-1}
\end{equation}
While it is not obvious, this turns out to be a sort of ``transform'' of functions of the same form as $f$, 
and in fact we can recover $f$ from $\tilde{f}$:
\begin{prop}
\label{prop:legendre}    
We have
\begin{equation} \label{eq:proplegendre} f(x)=\sup_{y\in \mathbb{R}^d}
\tilde{f}(y) K_h(x-y).
\end{equation}
Moreover, for each $x$, the supremum in \eqref{eq:proplegendre} is obtained at a unique 
value $y=p(x)$ where
\begin{equation}
    \label{eq:yexpec}
    p(x)=\frac{\sum_{i=1}^N a_i 
   K_h(x-x_i) x_i}
    {\sum_{i=1}^N a_i 
    K_h(x-x_i)}
\end{equation}
\end{prop}

In other words, $p(x)$ is the mean of a 
probability distribution $\mu_x$ on $\mathbb{R}^d$
which is supported at the points of $S$,
with normalized weights proportional to $a_i K_h(x-x_i)$.
%obtained by multiplying
%the discrete distribution 
%$S$ with $p_i$
%weighted by $a_i$
%by $K_h(x-x_i)$ to  and normalizing.
An illustration of the proposition is shown in Figure \ref{fig:univariate}.

\begin{figure}
    \centering
    \includegraphics[scale=1.2]{figures/univariate/figure_1/univariate.pdf}
    \caption{A univariate Gaussian KDE $f$ shown as the top curve, with $\tilde{f}$ shown as the bottom curve. The Gaussians all have the same scale but different scalar multipliers and centers. Proposition
    \ref{prop:legendre} shows that $f$ and $
\tilde{f}$ determine each other, and that $y=p(x)$
    for any pair $(x,y)$.}
    \label{fig:univariate}
\end{figure}

\begin{proof}

We may assume that $h=1$.
We show that the auxiliary function
\[F(x)=-\log(f(x))+\lVert x \rVert^2/2\]
is convex. The first statement then easily follows from the fact
that $F^{**}=F$, where 
\[F^*(y)=\sup_{x} \left(x\cdot y-F(x)\right)\] 
is the Legendre transform.


To see the convexity, it suffices to prove the convexity of the one-dimensional function $F(\varphi(t))$ where
$\varphi(t)=x+tv$ is a path with $\lVert v\rVert=1$. 
But the restriction of any sum of Gaussians to an affine subspace is just another sum of Gaussians:
\begin{equation}
    \label{eq:resgauss}
    f(\varphi(t))=
\sum_{i=1}^N b_i \exp(-(t-t_i)^2/2)
\end{equation}
where $t_i$ is the coordinate of the orthogonal
projection $\bar{x}_i=\varphi(t_i)$ of $x_i$, 
and $b_i=a_i\exp(-\lVert x_i-\bar{x}_i\rVert^2/2)$.
Thus, we have reduced the problem to the one-dimensional case.

For the one-dimensional case, it suffices to show that the second derivative is nonnegative. 
We check
\[f''=\left(\sum_{i} 
a_i(x-x_i)^2 e^{-(x-x_i)^2/2}\right)
-f.\]
Now write
\[F''=\frac{f''}{f}-\left(\frac{f'}{f}\right)^2+1=
E_\rho[(X_i-x)^2]-(E_\rho[X_i-x])^2\]
where the expectations are over the random variables
$X_i-x$ defined on the finite probability distribution
$\rho$ from \eqref{eq:yexpec}. 
Since this is the expression variance, 
we obtain the desired nonnegativity.

For the second statement, suppose $y$ is any value that obtains the supremum in \eqref{eq:proplegendre} for a given $x$,
and let $g(z)=c \exp(-\lVert z-y\rVert^2/2)$ be the corresponding Gaussian centered at $y$ with
$c=\tilde{f}(y)$.
Then we have that $g$ agrees with 
$f$ to first order at $x$:
\begin{equation}
\label{eq:lemfit}
g(x)=f(x),\quad 
\grad_{g} (x)=\grad_{f} (x).
\end{equation}
Dividing the
second second expression by the first
and solving for $y$, we obtain
\eqref{eq:yexpec}, which also establishes 
the uniqueness.
\end{proof}
Thanks to Greg Kuperberg, who proposed using the Legendre transform to simplify an 
earlier version of this argument.   
 
We then make the following definition, which actually applies to the convolution
of any distribution with a Gaussian kernel,
not just discrete ones:
\begin{defn}
\label{def:alphadens}
The alpha shape of the kernel density estimator $f(x)$ 
is the filtered family of regions
\begin{equation}
\label{eq:shapedef}
\shape(a)=\left\{x: \tilde{f}(x)>e^{-a}\right\}
\end{equation}
In other words, $\shape(a)=\alpha^{-1}(-\infty,a]$ where where $\shape$ consists of all $x$ for which 
$\tilde{f}(x)\neq 0$, and $\alpha:\shape\rightarrow \R$ is the function $\alpha(x)=-\log(\tilde{f}(x))$.
\end{defn}
We now have our main theorem, which roughly speaking says that $\shape(a)$ is a continous version of the the alpha shape. 

\begin{thm}
\label{thm:mainthm}
Let $\suplev{a}$ be the superlevel set of a sum of Gaussians $f(x)$ centered at $\dataset=\{x_i\}$ as in \eqref{eq:kde}. Let $(\shape,\alpha)$ and $p:\mathbb{R}^d\rightarrow \shape$ be as above,
with $\shape(a)=\alpha^{-1}(-\infty,a]$.
\begin{enumerate}
\item \label{item:denspow} We have that 
    $\suplev{a}=
\bigcup \mathcal{U}_{\shape,\pi}(2h^2a)$, where $\mathcal{U}_{\shape,\pi}$
is the filtered covering by weighted balls 
with vertex set $\shape$ and power map $\pi=-2h^2\alpha$. In other words, $-2h^2\log(f)=w_{\shape,\pi}$, 
using an infimum in \eqref{eq:powdist}.
\item \label{item:convex} The shape is an open subset of the convex hull $\shape\subset \chull(\dataset)$ on
which $\alpha$ is a continuous map. Each $\shape(a)$ is a (closed) subset of
$\suplev{a}\cap \shape$, and
the restriction $p|_{\suplev{a}}:\suplev{a}\rightarrow \shape(a)$ is surjective.
\item \label{item:homotopy} For any $a$ we have 
$\shape(a) \subset \suplev{a}$, and the inclusion map induces a homotopy equivalence with homotopy inverse $p|_{\suplev{a}}$. If $\affhull(\dataset)$ is all
of $\mathbb{R}^d$, then $p$ is a homeomorphism.
\end{enumerate}
\end{thm}

We start with the final statement.

\begin{lemma}
\label{lem:homeo}
Suppose the affine span of $\dataset$ is all of
$\mathbb{R}^d$. Then the restriction of $p$
to the level set gives a homeomorphism
$\suplev{a}\sim \shape(a)$.
\end{lemma}

\begin{proof}

Proposition \ref{prop:legendre} implies that 
$p$ is surjective, so we must check that it is also injective, and that its inverse is continuous. We may again assume that $h=1$.

Starting with the one-dimensional case, it suffices to check that the derivative $p'(x)$ is positive, so that $p$ is increasing and has a continuous inverse by the inverse function theorem.
Using the quotient rule, we 
find that the numerator in 
the expression for $p'$ is
\[\sum_{1\leq i < j \leq N} a_ia_j(x_i-x_j)^2
\exp\left((x-x_i)(x-x_j)-(x_i-x_j)^2/2\right)\]
which is positive. The denominator is $f^2$ which is also positive.

In the general case, suppose that 
$p(x)=p(x')$ for $x \neq x' \in \R^d$.
Then the restriction of $f$ to the line
$\varphi(t)=x+tv$ for $v=(x'-x)/\lVert x-x'\rVert$ is the one-dimensional sum of Gaussian kernels in \eqref{eq:resgauss}. Then we have
\begin{equation}
    \label{eq:ressurj}
(p(x+tv)-p(x))\cdot v=\frac{\sum_{i} b_i t_i\exp(-(t-t_i)^2/2)}
{\sum_{i} b_i \exp(-(t-t_i)^2/2)}
\end{equation}
The $t_i$ must also affinely span 
$\mathbb{R}$ (which just means they are not all
the same point), 
so taking $t=\lVert x'-x\rVert$ 
contradicts the one-dimensional case. 

Therefore $p$ is bijective, and it remains 
to show that its inverse is continuous. 
For this, we have that
$v^t J_p(x) v$ is the derivative of the right hand side of \eqref{eq:ressurj} at $t=0$, 
where $J_p(x)$ is the Jacobian matrix.
Then using the first paragraph, we find that the Jacobian is positive definite for all $x$, so $p$ is locally invertible by a continuous function
by the inverse function theorem. Since $p$ is globally invertible, its inverse must agree with each local inverse, so $p^{-1}$ is continuous.
\end{proof}

We can now prove Theorem \ref{thm:mainthm}.
\begin{proof}
Part \ref{item:denspow} follows immediately from Proposition \ref{prop:legendre}.

Suppose that the affine span of
$\dataset$ is a lower-dimensional affine subspace $\affhull(\dataset)\subset \mathbb{R}^d$.
Then the superlevel sets of the KDE associated 
to the restriction of $f$ to $\affhull(\dataset)$ 
are homotopy equivalent to 
those of $f$. Moreover, $p$ factors as the orthogonal projection onto $\affhull(\dataset)$ composed with a map $p'$ on $\affhull(\dataset)$ that takes the same form as $p$. Thus, the remaining statements are reduced to the case in which 
$\dataset$ affinely spans all of $\mathbb{R}^d$.

In the case that $\dataset$ spans, we have that $p$ is a homeomorphism by Lemma \ref{lem:homeo}. The
statement that $\shape\subset \chull(\dataset)$ follows from the explicit form of $p$.
The statement that $\alpha$ is continuous 
follows from the expression
\[\alpha(y)=-\log(f(p^{-1}(y)))+
\lVert y-p^{-1}(y)\rVert^2/2h^2.\]
The statement that $\shape$ is open in the convex hull follows since it is the union of the open sublevel sets.


The only remaining statement is that 
the inclusion map 
$i:\shape(a) \subset \suplev{a}$ is homotopic to $p^{-1}$, or equivalently that $ip:\suplev{a}\rightarrow \suplev{a}$ is homotopic to the identity map.
We define a family $h_t:\R^d \rightarrow \R^d$ by \[h_t(x)= tx+(1-t)p(x).\]
Since $p(x)$ sends $x$ to the center of a closed ball $U_{y}\subset \shape(a)$ contacting $x$ along the boundary,
it follows from part \ref{item:denspow}
that $h_t$ carries each $\suplev{a}$ into itself, and therefore gives the desired homotopy.

\end{proof}

\begin{remark}
In \cite{edelsbrunner1995union}, Edelsbrunner gave an explicit deformation retraction of the union of balls onto the shape. However,
while $\shape(a)\subset \suplev{a}$
induces a homotopy equivalence, the reverse map $p$ does not act as the identity on 
$\shape(a)$, so it
is not a deformation retraction.
\end{remark}

\subsection{Subsampling and finite alpha complexes}

\label{sec:finplex}

We now explain our method for choosing
finite subsets $S\subset \shape$, corresponding to finite subcovers. Taking the nerve, we obtain finite alpha complexes filtered by the negative log of density.
\begin{alg}{Generate a finite alpha complex from $\shape$.}
\label{alg:densland}
\begin{enumerate}
\label{enum:densland}
\item Sample $\{\tilde{x}_1,...,\tilde{x}_M\}$ from the underlying distribution of $f(x)$. 
\item Let $\tilde{y}_i=p(\tilde{x}_i)$, 
where $p:\mathbb{R}^d\rightarrow \shape$ is the map from Theorem \ref{thm:mainthm}, and suppose $\{\tilde{y}_1,...,\tilde{y}_M\}$ are sorted in increasing order of $\alpha(\tilde{y}_i)$.
    \item \label{item:algadd} Generate a vertex set as follows:
    \begin{enumerate}
        \item Initialize $S=\emptyset$. Fix $0<s<1$, and let $\epsilon=-2h^2\log(s)$.
 \item For each $\tilde{y}_i$ in order, make the addition $S\mapsto S\cup \{\tilde{y}_i\}$ if the squared distance of $\tilde{y}_i$ from all existing points in $S$ is at least $\epsilon$.
    \end{enumerate}
\item Return the weighted alpha complex with vertex set
$S$, and power map $\pi=-2h^2\alpha$.
\end{enumerate}
\end{alg}
\begin{defn}
\label{def:densalpha}    
We denote the alpha complex resulting
from running Algorithm \ref{alg:densland} by
$X=\densalpha(f,s)$, separately specifying the number of 
sampled points $M$. We will let 
$\densalpha(f,s,d_0)$ be the result of stopping upon reaching a minimum density cutoff of $\tilde{f}(\tilde{y}_i)\geq d_0$
in step \ref{item:algadd}.
\end{defn}
We have chosen to represent our minimum separation parameter $\epsilon$ 
in terms of exponential coordinates 
$s=\exp(-\epsilon/2h^2)$, in other words as a minimum ratio in density coordinates. 
We find this more useful since $s$ is unitless, and 
independent of the situation and scale.
For instance, a value of $s=.9$ will always result in a relatively dense vertex set, whereas
$s=.5$ will be relatively spread out.
Similarly, it often makes more sense to refer to the minimum density $d_0$ in terms of the fraction of the data that falls under that range.
A minimum density $d_0$ corresponding to
$\%80$ of the data would mean the value at which $\%80$
of the points satisfy $f(x_i)\geq d_0$.

Notice that the union of the corresponding power diagram 
$\bigcup \mathcal{V}_{S,\pi}(a)$
is always contained in $\suplev{a}$. On the other hand, if we have chosen enough samples, then we would also have that the union $\bigcup \mathcal{V}(a+\epsilon)$
contains $\shape(a)$. This relies on the fact that we are proceeding in increasing order of $\alpha$, so that the shifted squared distance of any new point to an existing site
$p_i\in S$ is bounded above by the 
unshifted squared distance.
In this case, we would have
\begin{equation}
    \label{eq:sampcov}
\shape(a) \subset \bigcup_{p_i\in S} V_i(a+\epsilon) \subset \suplev{a+\epsilon}
\sim \shape(a+\epsilon).
\end{equation}
%In other words, the algorithm seeks to interleave the true shape between $\epsilon$-shifted 
%finite coverings.

Algorithm \ref{alg:densland} has several crucial 
scalability properties.
First, the shape is covariant under linear coordinate changes, so its covering number by balls of radius $\epsilon$ is also unchanged, meaning we do not have the poor scaling associated with sampling from 
$\suplev{a}$ from the introduction.
Furthermore, it follows from properties of Gaussian samples and the form 
of $p$ as an expectation
that the sampling procedure itself,
in other words the resulting distribution on 
$\shape$ from which the $\tilde{y}_i$ are sampled,
is independent of the embedding as well.
To illustrate, notice that 
increasing the dimension by adding extra zeros to the end of each $x_i$ has no effect: 
the values along the new coordinates will be sampled from independent Gaussians, and
$p$ will simply map those coordinates back to zero, as it factors through the projection map to the affine span of $\dataset$.
In particular, unlike the 
values of $f(\tilde{x}_i)$, 
the resulting real distribution 
coming from the values
of $\tilde{f}(\tilde{y}_i)$ or $\alpha(\tilde{y}_i)$ is
stable, and indeed it depends only on the 
pairwise distances $\lVert x_i-x_j\rVert$.
We also point out that
evaluating $\alpha(\tilde{y}_i)$ creates
no extra computational cost because
the the infimum defining \eqref{eq:ftilde}
is attained by the sample $\tilde{x}_i$ itself.

We will use the following variant of $\densalpha(f,s)$. 
The alpha complex (or Vietoris-Rips) necessarily creates``noise'' in zeroth persistent
betti number when the vertices are well-spaced, due to the gap between a new point and the main component. A tempting fix is to add ``slack'' by shifting the persistence of higher-dimensional simplices back by a specified amount, similar to what the Rips or lazy witness complex does to fill in higher simplices, but this would add parameters 
and compromise objectivity. A 
far better solution is to modify the weight of each simplex according to the following definition:
\begin{defn}
\label{def:densalpha1}    
Let $\densalphao(f,s)$ be the result of replacing
\begin{equation}
    \label{eq:shiftvariant}
    w(\sigma)\mapsto \max(w(\sigma),\alpha(\sigma_0)+\epsilon,...,\alpha(\sigma_k)+\epsilon)
    \end{equation}
for all simplices $\sigma \in X=\densalpha(f,s)$,
where $\epsilon=-2h^2\log(s)$ is the spacing from 
Algorithm \ref{alg:densland}.
\end{defn}
This is equivalent to requiring that each element of the power diagram appears only once its squared-radius crosses is at least $\epsilon$. This 
maintains the desirable property that the resulting complex is the nerve of a cover whose union is contained in $\suplev{a}$, and also preserves \eqref{eq:sampcov} for sufficienty dense samples. 

\section{Experiments}

We construct the filtered complexes from Section \ref{sec:finplex} in several examples. 
To compute alpha complexes in higher dimension
we used a recent algorithm based on dual programming to compute the corresponding alpha complexes \cite{carlsson2023alpha}, which in all cases required only a few seconds to complete. In the sampling step we usually used $M=10000$, which took a couple of minutes,
coming from evaluating $f$ at that many points.

\label{sec:experiments}

\subsection{Interesting energy landscapes}

\label{sec:config}

Consider the energy function for particle interaction for three points in the plane
$(p_1,p_2,p_3)\in \mathbb{R}^2$, using the Lennard-Jones potential:
\begin{equation}
    \label{eq:configloss}
H(p_1,p_2,p_3)=
\sum_{i<j} V(\lVert p_i-p_j\rVert),\quad
V(r)=4\left(\frac{1}{r^{12}}-\frac{1}{r^6}\right).
\end{equation}
The Lennard-Jones potential rewards pairs of particles that are approximately distance 1 apart, but strongly penalizes particles that are much closer than that, and is neutral for far away points. 
Some typical points can be represented as
\\
\includegraphics[scale=.73]{figures/config/figure_1/configpoints1.pdf}
\\
where we have shown a dotted line when points are roughly distance one apart.
We estimate the sublevel set persistent homology of 
$H$ by applying the density-based complex
to a mean-centered point cloud of samples, revealing an interesting the homology groups of a configuration space. 

We sampled
100000 points from the distribution
\begin{equation}
    \label{eq:configdist}
\rho(p_1,p_2,p_3)=
\exp\left(-\beta 
\left(H(p_1,p_2,p_3)+\sum_{i=1}^3 \lVert p_i\rVert^2/2R^2\right)\right)
\end{equation}
thought of as a density in $\mathbb{R}^6$,
using the Metropolis algorithm.
Here the temperature parameter $\beta$ was set to $3.0$, and a radius of $R=3.0$ was used to keep particles from wandering off to $\infty$.

\begin{figure}[t]
    \centering
        \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[scale=.56]{figures/config/figure_1/configen1.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.56]{figures/config/figure_1/configcloud1.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.56]{figures/config/figure_1/configcloud2.pdf}
    \end{subfigure}
    \caption{On the left, a graph of the Lennard-Jones potential. In the middle and right, representations of a point cloud of samples using the
    two-point normalization and
    Hopf map respectively.}
    \label{fig:configcloud}
\end{figure}

Some projections of point clouds are shown in Figure \ref{fig:configcloud}.
In order to visualize the point
clouds, we used two different prescriptions to lower the dimension. 
In the middle frame on the lower row, we reduced the dimension to 2 by choosing a standard reference frame in which
$p_1$ is at the origin, and $p_2$ is on the $x$-axis, using a translation followed by a rotation.
We then normalized the sum of the norm squares, and plotted the remaining point $p_3$, which moves around a figure 8 shape. In the frame on the lower right, we instead mean-center each sample $(p_1,p_2,p_3)\in \mathbb{R}^6$, so as to have a point in $\mathbb{R}^4$. We then normalized the rotational angle using the map
\begin{equation}
    \label{eq:hopf}
    \varphi : \mathbb{R}^4\rightarrow \mathbb{R}^3,\quad 
    \varphi(v)=\lVert v\rVert
    \pi(v/\lVert v \rVert)
\end{equation}
where $\pi: S^3\rightarrow S^2$ is a version of the Hopf fibration that respects conformal structure, which is equivalent to the map $SU(2)\rightarrow SU(2)/T$ where $T\cong S^1 \subset SU(2)$ is the torus. Unlike the
two-point version, this preserves the metric 
structure on the quotient, seen by the $\pi/3$ rotational symmetry in the figure.

\begin{figure}[t]
       \centering
        \begin{subfigure}[b]{.32\textwidth}
        \centering
        \includegraphics[scale=.56]{figures/config/figure_2/configfit1.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.56]{figures/config/figure_2/configsites1.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.56]{figures/config/figure_2/configshape.pdf}
    \end{subfigure}
    \caption{In the first frame, several samples $\{\tilde{y}_i\}$ from the shape using Algorithm 
    \ref{alg:densland}. In the second, the sampled landmark set $S$ with $s=.8$. In the final frame, the resulting alpha complex, using a heatmap to describe the weights.}
    \label{fig:configshape}
\end{figure}

We then defined a kernel density estimator 
$f$ on both the mean centered 
original point cloud
$\dataset \subset \mathbb{R}^4$, and its image 
$\dataset'\subset \mathbb{R}^3$
under \eqref{eq:hopf}. In both cases, we chose a value of $h=.3$, and computed 
$\densalpha(f,s,d_0)$
using density cutoff $d_0$ corresponding to 
the $\%60$ of the data.
We created resulting filtered complex from $\dataset'$ with $s=.8$, which had
sizes $(|X_0|,|X_1|,|X_2|)=(339,1287,1449)$.
The outputs of Algorithm \ref{alg:densland}
are shown in Figure \ref{fig:configshape}, using color values to represent the simplex weights.

We then applied the same procedure to the full 
dataset $\dataset$, this time using $s=.7$, and computing up to the $3$-simplices, resulting in the sizes 
$(|X_0|,|X_1|,|X_2|,|X_3|)=
(2298,24582,64896,64674)$.
The persistent homology groups are consistent with a disjoint union of two circles turning into a configuration space of 3 ordered points in the plane.
This is because the densest points are the ones forming an equilateral triangle, which come in two types corresponding to he rotationally inequivalent permutations of the labels.
The points with two connections form the shape of a configuration space, which has betti numbers of 
$(\betti_0,\betti_1,\betti_2)=(1,3,2)$ corresponding to the arrows in the diagram \cite{cohen1995configuration}.

The \texttt{javaplex} output of the persistent homology groups is shown in Figure \ref{fig:configbetti}.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/config/figure_3/betti2.png}
    \caption{The barcode diagrams of the full density landscape computed with \texttt{javaplex}.
    Before the threshold around 1.5 we see two connected components and two betti 1 features. Beyond that threshold, we see the homology of the ordered configuration space
    of 3-points in the plane,
    $(\betti_0,\betti_1,\betti_2)=(1,3,2)$.}
    \label{fig:configbetti}
\end{figure}


\subsection{Multiple optimizers in nonlinear regressions}

We next use alpha complexes to reveal
local basins in the loss function of a nonlinear regression, using the Metropolis algorithm and maximum likelihood estimation. 
In order to produce a point cloud,
we map each collection of parameter values into
into the Euclidean vector consisting of
predicted values at each training point.
We find that this is a good candidate for density
estimation and the alpha complex, and that there is a natural choice of the scale $h$ in terms 
of the temperature parameter of the model $\beta$.


We follow Example 2.6 from \cite{seber2005nonlinear},
which deals with the catalytic isometrization of $n$-pentane to $i$-pentane in the presence 
of hydrone, based on an original study by
Carr \cite{carr1960kinetics}.
The training data consists of
24 experimental runs, with four columns labeled $x_1,x_2,x_3,r$,
measuring the partial pressures of
hydrogen, $n$-pentane, and $i$-pentane, and the corresponding reaction rate $r$.
The modeling problem is to predict the last column using the model
\begin{equation}
    \label{eq:reaction}
r \sim
\frac{\theta_1\theta_3(x_2-x_3/1.632)}
{1+\theta_2 x_1+\theta_3 x_2+\theta_4 x_3}.
\end{equation}

We simulated 10000 samples of the 
$\theta$-parameters using the Metropolis
algorithm, and a maximum likelihood with squared residual losses, 
\begin{equation}
    \label{eq:carrloss}
    p(\theta)=\exp(-\beta L(\theta)),\quad 
    L(\theta)=
    \sum_{i=1}^{24} \left(r_i-\hat{r}_i(\theta)\right)^2
\end{equation}
and temperature of $\beta=3.0$,
resulting in a point cloud $\dataset \subset \mathbb{R}^4$.
One finds that there was a near-symmetry in simulaneously sending 
$\theta_i\mapsto -\theta_i$ 
for $i=\{2,3,4\}$, due to the fact that the parameters tended to dominate the leading
$1$ in the denominator.
Additionally, there were two local basins 
of solutions corresponding to the sign of 
$\theta_1$, with the positive values having 
better predictions. The results are shown in
Figure \ref{fig:carr}.



\begin{figure}
\centering
        \begin{subfigure}[b]{.3\textwidth}
        \centering
        \includegraphics[scale=.5]{figures/bayes/figure_1/bayesparams.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{.3\textwidth}
    \centering
        \includegraphics[scale=.515]{figures/bayes/figure_1/bayesresid1.pdf}
    \end{subfigure}
        \begin{subfigure}[b]{.3\textwidth}
    \centering
        \includegraphics[scale=.515]{figures/bayes/figure_1/bayesresid2.pdf}
    \end{subfigure}
    \caption{Scatter plot of the first two model parameters $(\theta_1,\theta_2)$ from modeling the Carr data. In the middle/right frames we have the plot of predicted value versus true value for a typical value from the positive/negative values of $\theta_1$ groups respectively. The first group tend to be better fits.}
    \label{fig:carr}
\end{figure}

A na\"{i}ve next step would be to apply kernel density estimation to the point cloud 
$\dataset \subset \mathbb{R}^4$,
with some choice of scale $h$. However, this would not be meaningful, as it is 
depends on the parametrization of the model. A more appropriate one is to define $f(x)$ using a new data set $\dataset'\subset \mathbb{R}^{24}$ by mapping each $\theta$ to the corresponding vector of predictions
\begin{equation}
\label{eq:predmap}
\theta\mapsto (\hat{r}_1(\theta),...,\hat{r}_{24}(\theta)).
\end{equation}
We then have a natural choice of the scale parameter, $h=(\beta/2)^{-1/2}\sim .816$.

We built the alpha complex using a value of 
$s=.6$, with density cutoff $d_0$
corresponding to $80\%$ of the data,
shown in Figure \ref{fig:carralpha}. The map 
to $\mathbb{R}^{24}$ effectively collapses the symmetry arising from the sign changes, leaving only
the two connected components. The component corresponding
to positive values of $\theta_1$ gives better predictions, resulting in the denser component on the
left. The others lead to local basin which wider
but less dense, corresponding to higher values of the loss function. These are reflected in the betti-zero persistence barcodes, also shown.

\begin{figure}
\centering
\begin{subfigure}[b]{1\textwidth}
\centering
     \includegraphics[scale=.55]{figures/bayes/figure_2/carralpha1.png}     
     \caption{Alpha complex projected to 2 dimensions}
\end{subfigure}
  \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[scale=2]{figures/bayes/figure_3/betti0-carr.png}    
     \caption{Zero-dimensional persistence barcodes}
\end{subfigure}
    \caption{Alpha complex 
    $\densalpha(f,s,d_0)$
    built from the data set of prediction vectors in $\mathbb{R}^{24}$ with $s=.6$
    and $d_0$ corresponding to $\%80$ of the data, projected into 2 dimensions using a PCA on the vertex set.
    The denser component on the left represents the $\theta_1>0$ group. In the second frame, the barcode diagram indicating the local basins}
    \label{fig:carralpha}
\end{figure}

\subsection{A simple singular learning model}

We apply a similar method from the previous example to a singular statistical model,
which is Example 1.2 from \cite{watanabe2009algebraic}, 
with thanks to Dan Murfet for the suggestion. We see that the alpha complex exhibits interesting behavior as a certain Riemannian metric related to the Fisher information matrix
becomes degenerate near the singularity.



Consider a simple one-variable Gaussian mixture model
\begin{equation}
\label{singmodel}
p(x|a,b)=
ae^{-t^2/2}+(1-a) e^{-(t-b)^2/2},\quad
0\leq a \leq 1,
\end{equation}
consisting of a weighted sum of Gaussian distributions with standard deviation 1, and a varying mean in the second one. An illustration is shown in Figure \ref{fig:slt1}.

\begin{figure}
\centering
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.5]{figures/slt/figure_1/singbayes1.pdf}
        \caption{$a=.35,b=2$}
        \label{fig:slta}
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.5]{figures/slt/figure_1/singbayes2.pdf}
             \caption{$a=.95,b=2$}
        \label{fig:sltb}
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
    \centering
        \includegraphics[scale=.5]{figures/slt/figure_1/singbayes3.pdf}
             \caption{$a=.35,b=.5$}
        \label{fig:sltc}
    \end{subfigure}

    \caption{Simple singular statistical model with two Gaussians. The singularity arises because the second two frames correspond to different parameter values, but result in nearly equal distributions.}
    \label{fig:slt1}
\end{figure}

For any point cloud in $\mathbb{R}$, we can use the Metropolis algorithm and Bayes' rule to sample from a density proportional to
\[p(a,b|t)= \frac{p(t|a,b)p(a,b)}{p(t)},\]
starting with the uniform measure $p(a,b)=1$.
As usual, we do not need to know $p(t)$. If our point cloud is sampled from $p(t|a,b)$ for a particular choice of $(a,b)$, we would expect the resulting distribution 
to be supported near the ones we started with. This will indeed happen as expected if we chose values giving a bimodal distribution as in Figure \ref{fig:slta}. However, interesting things happen if our point cloud comes from a single Gaussian centered at the origin, because many values of $(a,b)$ correspond to that same distribution, as shown in Figures \ref{fig:sltb} and \ref{fig:sltc}.



\begin{figure}
\centering
    \begin{subfigure}[b]{.31\textwidth}
    \centering
        \includegraphics[scale=.55]{figures/slt/figure_2/sltparams.pdf}
        \caption{Many values of $a,b$}
        \label{fig:slt2a}
    \end{subfigure}
    \begin{subfigure}[b]{.31\textwidth}
    \centering
        \includegraphics[scale=.55]{figures/slt/figure_2/sltshape.pdf}
                \caption{Samples from $\shape$}
        \label{fig:slt2b}
    \end{subfigure}
    \begin{subfigure}[b]{.33\textwidth}
    \centering
        \includegraphics[scale=.32]{figures/slt/figure_2/sltplex.png}
        \caption{Alpha complex}
        \label{fig:slt2c}
    \end{subfigure}

    \caption{On the left, scatter plot of samples 
    of the model parameters $(a,b)$ using Markov chain Monte Carlo (MCMC) with respect to the distribution
    $e^{-t^2/2}$. In the middle, samples from the shape
    displayed in $\mathbb{R}^2$ using the coefficients
    of $p(\tilde{x}_i)$ and the original samples
    $(a_i,b_i)$. On the right, the corresponding alpha complex $\densalphao(f,s,d_0)$ for $s=.8$
    and $d_0$ corresponding to $\%80$ of the data.
    The red/blue colors correspond 
    to positive/negative values of $b$, respectively.
    }
    \label{fig:slt2}
\end{figure}


We generated a scatter plot of $10000$ values of the parameters $(a,b)$ associated to the singular Gaussian using 1000 training points in $\mathbb{R}$.
The results, shown in Figure \ref{fig:slt2a},
reveal the expected behavior at the singularity
at the origin $(a,b)=(0,0)$.
As in the previous section, 
building an alpha complex on the resulting
point cloud in $\mathbb{R}^2$ would be arbitrary. We no longer have a vector of predictions as in \eqref{eq:predmap}, and instead we use the vector of values of the loss function
\begin{equation}
    \label{eq:sltfisher}
(a,b)\mapsto (-\log(p(t_1|a,b)),...,-\log(p(t_{1000}|a,b))).
\end{equation}
This map has the property that the pullback Riemannian metric on $\mathbb{R}^2$
is a version of the Fisher information matrix
near the true distribution.
We then reduced the dimension down to
50 using a PCA,
in part to speed up the sampling, but more importantly
because rounding errors become a factor when evaluating
$y=p(\tilde{x}_i)$ at a sample. The second issue 
can easily be fixed 
by representing components of the calculation 
in log coordinates. 
We then considered the density estimator
$f:\mathbb{R}^{50}\rightarrow \mathbb{R}_+$
corresponding to the resulting point cloud
$\dataset \subset \mathbb{R}^{50}$, 
and chose a (this time arbitrary) value of 
the scale parameter of $h=.1$.

We then sampled from the shape using Algorithm \ref{alg:densland}, using the values of $s=.8$,
cutting off at a value corresponding to $\%80$ of the 
data.
By taking the coefficients determining
the expression 
$p(\tilde{x}_i)=\sum_{j} c_{i,j} x_j$
as a convex combination of the $x_i$
and plotting the points
$\sum_{j} c_{i,j} (a_i,b_i)\in \mathbb{R}^2$ instead of
$\tilde{y}_i=\sum_{j} c_{i,j} x_{j}\in \mathbb{R}^{50}$, we obtain a description of the shape in 
the original 2-dimensional plane.
Instead of corresponding to the original singularity, 
the points on the line $b=0$ were all mapped to the same point near the origin, creating the interesting pattern shown in Figure \ref{fig:slt2b}.
This is because all those values become
very close together when mapped to $\mathbb{R}^{50}$, resulting in a degenerate metric at the origin.
We then generated $\densalpha(f,s)$ with 
$s=.8$, and plotted the projection onto a unitary subspace in $\mathbb{R}^{50}$, showing the interesting shape 
in Figure \ref{fig:slt2c}.

\subsection{Local patches in the MNIST data set}

\label{sec:patches}

In \cite{carlsson2008klein},
the authors studied the topology of a certain space of  local $3\times 3$ high intensity patches of the van Hateren data set of natural images, which was investigated earlier by Lee, Mumford, and Pederson \cite{hateren_schaaf_1998,mumford2003nonlinear}. They gave quantitative evidence using the witness complex that those patches lie along a sublocus of a parametrized
Klein bottle, called the three-circle model. 
Using a similar setup, we apply our construction to data sets of coordinate patches taken from $28\times 28$ images of handwritten digits 
from the MNIST data set \cite{deng2012mnist}.
Instead of using small $3\times 3$ patches, we project onto discrete versions of the Hermite polynomials up to quadratic order.
Using the alpha complexes,
we obtain surprisingly descriptive geometric models corresponding to different regions in the Klein bottle as one varies the digit.

A ``local image patch'' will mean an 
$l\times l$ subimage of a larger one, 
which in our case will be taken 
the MNIST data set of handwritten digits.
We will view local image patches as elements
of the vector space $V=\mat(l,l)$ using its grayscale intensity value. We have a scalar product on $V$ given by
\begin{equation}
    \label{eq:hermsp}
(A,B)=\frac{1}{2^{2(l-1)}}\sum_{i=1}^l \sum_{j=1}^l
\binom{l-1}{i-1} \binom{l-1}{j-1}
A_{i,j} B_{i,j}
\end{equation}
This inner product has a number of advantages over the usual $L^2$ product in that the weights fall off gradually near the image border. It also has the property of being nearly rotationally invariant for larger values of $l$, as the binomial coefficient approximates the Gaussian.
There is an orthonormal basis 
given by $H_{a,b}=H_a \otimes H_b$, where the $H_a \in \mathbb{R}^l$ are a discrete form of the Hermite polynomials.
They can be obtained by applying the Gram-Schmidt algorithm to the vectors of polynomial functions $v_a=(i^a)_{i=1}^l$, with respect to the one-dimensional form of \eqref{eq:hermsp}.


We have a decomposition
\[V=V_0\oplus V_1\oplus \cdots,\quad 
V_i=\spn\{ H_{a,b}:a+b=i\}\]
as well as orthogonal projections
$\pi_i:V\rightarrow V_i$.
We will be interested in the images of image patches under the map
$\pi_{1,2}: V\rightarrow V_1\oplus V_2\cong \mathbb{R}^5$, which analogous to projecting onto low-frequency modes in Fourier analysis.
One such projection is shown in Figure \ref{fig:hermite}. For any image patch we have its norm squared $r^2=r_1^2+r_2^2$ 
where $r_i$ is the norm of its image in $V_i$. Image patches centered on the points of the 
digit would tend to have relatively high $r_2$ values, while points on the boundary would have higher $r_1$ values, due to the gradient.
The patch in Figure \ref{fig:hermite} would both have relatively high $r_1$ and $r_2$ terms.

\begin{figure}
    \centering
  \begin{subfigure}[b]{.3\textwidth}
  \centering
\includegraphics[scale=.6,frame]{figures/mnist/figure_1/number8.png}
\end{subfigure}
  \begin{subfigure}[b]{.3\textwidth}
\centering
\includegraphics[scale=1.5272,frame]{figures/mnist/figure_1/number8patch.png}
\end{subfigure}
  \begin{subfigure}[b]{
  .3\textwidth}
  \centering
\includegraphics[scale=1.5272,frame]{figures/mnist/figure_1/number8herm.png}
\end{subfigure}
    \caption{A typical digit, a random $11\times 11$ image patch, and its orthogonal projection 
    onto $V_1\oplus V_2$.}
    \label{fig:hermite}
\end{figure}


%We then constructed a point cloud as follows. 
For 50 instances of each digit, we sampled all $l\times l$ patches using the choice of $l=11$, and projected those patches onto their linear and quadratic components $V_1\oplus V_2$, to obtain a point cloud of size $50\cdot (28-l+1)^2=16200$ in $\mathbb{R}^5$. We then chose only those images whose $L^2$-norm is above a fixed number of $r\geq .3$, resulting in a subset of around 
$20\%$ of the original size.
This is analogous to the step of selecting ``high intensity patches'' 
from \cite{carlsson2008klein}. 
We then divided the remaining points by $r$ to arrive at a point cloud $\dataset_k\subset S^4$ of size a few thousand for each digit $k\in \{0,...,9\}$. 
We defined a kernel density estimator $f:\mathbb{R}^5\rightarrow
\mathbb{R}_+$ using $h=.15$,
and built $\densalphao(f,s,d_0)$
using the same choices of $s=.5$, and $d_0$ corresponding to about
$\%60$ of the data for each digit.


The results, shown for the numbers $\{1,7,0,8\}$ in Figure \ref{fig:mnist1}, exhibited distinctive features, which can be understood in terms of the Klein bottle model.
Starting with the digit 1, we see two arcs connected by a connecting region in the center. Analyzing the images associated to each point shows that the arcs are the regions on either side of the digit, which have 
large magnitude in linear terms $r_1$. The strip lives on the digit, which has relatively high quadratic norm $r_2$. The digit 7 has a similar explanation but with two different components. These linear terms fill out the entire periphery of a circle in the the case of the digit 0, coming from both the interior and exterior. In the center of the figure, we see the high $r_2$ points
twist and connect across points on the digit,
which are only dense enough on the left and right sides because of the oval shape.
The digit 8 shows no points dominated by second order, except a small disconnected region corresponding to the two voids.

\begin{figure}
    \centering
  \begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_2/digit1.png}
\caption{digit=1}
\end{subfigure}
  \begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_2/digit7.png}
\caption{digit=7}
\end{subfigure}
\begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_2/digit0.png}
\caption{digit=0}
\end{subfigure}
  \begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_2/digit8.png}
\caption{digit=8}
\end{subfigure}
    \caption{Alpha complexes build out of the high-intensity image patches.}
    \label{fig:mnist1}
\end{figure}

We then ran the experiment again, this time using $r_2$ to determine intensity, and dividing by $r_2$ in place of $r$.
This has the effect of making patches with higher $r_2$ denser, thereby accentuating the second order features. This time we cut off at an intensity value of $r_2\geq .125$,
and chose $d_0$ to correspond to only 
$\%30$ of the data, keeping the values of
$h=.15$ and $s=.5$.

The results are shown in Figure \ref{fig:mnist2}. In the digit 1, we see three connected components, corresponding to points on the digit itself, and two others which are not on the boundary, but slightly away from it on either side. For instance the patch in 
Figure \ref{fig:hermite} would be such a point.
Again, digit 7 has the same explanation, with 6 components instead of 3. In the digit zero, we have a complete M\"{o}bius strip as one traverses halfway around the digit itself. The two additional components have the same meaning as with the digit 1, appearing only on the sides because the digit is not a perfect circle. In the digit 8, we actually see 5 connected components. The main one consists of points on the digit itself, whereas the second largest ones are points on either side, as in the other digits. One of the remaining small clusters comes from the voids inside either loop, while the other represents the crossing point in the center.

\begin{figure}
    \centering
  \begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_3/digit1a.png}
\caption{digit=1}
\end{subfigure}
  \begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_3/digit7a.png}
\caption{digit=7}
\end{subfigure}
\begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_3/digit0a.png}
\caption{digit=0}
\end{subfigure}
  \begin{subfigure}[b]{.24\textwidth}
\includegraphics[scale=.3,frame]{figures/mnist/figure_3/digit8a.png}
\caption{digit=8}
\end{subfigure}
    \caption{Complexes built on the corresponding digits, normalizing only the second-order coefficients.}
    \label{fig:mnist2}
\end{figure}


\subsection{The Ising model on a graph}

\label{sec:ising}

In our final example, we consider density estimation on a simulated data set consisting of trials of the Ising model \cite{ising1925ferromagetism} on a graph with $d$ vertices, thought of as a collection of real-valued vectors in 
$\{\pm 1\}^d\subset \R^d$.
Attempting to apply kernel density estimation on the resulting point cloud directly would not yield good results, and we would not even expect to be able to correlate kernel based density at a particular state with the theoretical density determined by the energy function. We show that we can create geometric models of the density landscape as we did in Section \ref{sec:config},
by using Laplacian operator $L$
of the underlying graph to obtain
smooth versions of the spin vectors.


Let $G=(V,E)$ be a graph with $n$ vertices, 
represented by a symmetric adjacency matrix $J$, with diagonal entries being zero. In our example we will use

\hspace{1in}\includegraphics[scale=.5]{figures/ising/figure_1/isinggraphs.png}
\\
denoted $\isingint(n),\isingcirc(n),\isingflares(n)$,
where $n$ is the number of vertices.
For every discrete spin vector 
$\sigma : V \rightarrow \{1,-1\}$, 
we have the Hamiltonian energy
\begin{equation}
\label{eq:isingham}
    H_G(\sigma)=-\sum_{i,j}
J_{i,j} \sigma_i \sigma_j=
H_{\min}+2|\{(i,j)\in E: \sigma_i\neq \sigma_j\}|.
\end{equation}
Those pairs ${i,j}\in E$ for which 
$\sigma_i\neq \sigma_j$ are called transitions.
For each choice of $\beta>0$, called the temperature parameter, one seeks to sample from the
Boltzmann distribution on $\{1,-1\}^d$
given by
\begin{equation}
    \label{eq:boltzmann}
P_{\beta}(\sigma)=\frac{1}{Z_{\beta}}
e^{-\beta H(\sigma)},\quad
Z_\beta = \sum_{\sigma} e^{-\beta H(\sigma)}
\end{equation}
which is usually done using the 
single-flip Metropolis algorithm.


For the graphs $G\in \{\isingint(30),\isingcirc(30), \isingflares(43)\}$, we simulated 
$N=20000$ states using a temperature value of 
$\beta=3.0$, and interpreted the resulting
collections of spin vectors $\{\sigma\}$
as a point clouds
$\dataset_{G}\subset \{\pm 1\}^d\subset \mathbb{R}^d$ where $d=n$ is the number of vertices. We then took a blended version 
of $\dataset_G$ 
using the left-normalized Laplacian operator 
$I-D^{-1}A$, where $A$ is the adjacency matrix of $G$, normalized so that the diagonal entry $A_{i,i}$ is the degree of $v_i$, and $D$ is the row-sum of $A$.
We then replaced $\dataset_G$
by sending
$\sigma \mapsto \sigma \exp(-tL^{t})$ with the value of $t=10$, so that the vectors are no longer $\{\pm 1\}$-valued, and considered the corresponding kernel density estimator $f:\mathbb{R}^d\rightarrow \mathbb{R}_+$ with $h=2.0$. An illustration of the result of the convolution, and the distribution of density versus 
Hamiltonian energy is shown in Figure \ref{fig:isingblend}.

\begin{figure}
    \centering
    \begin{subfigure}
        [b]{.3\textwidth}
        \centering
\includegraphics[scale=.7,frame]{figures/ising/figure_2/isingblend1.png}
\caption {$\sigma:V\rightarrow \{\pm 1\}$}
    \end{subfigure}
        \begin{subfigure}
        [b]{.3\textwidth}
        \centering
\includegraphics[scale=.7,frame]{figures/ising/figure_2/isingblend2.png}
\caption{$x=\sigma\exp(-tL)$}
    \end{subfigure}
        \begin{subfigure}
        [b]{.3\textwidth}
        \centering
\includegraphics[scale=.58]{figures/ising/figure_2/isingdens.pdf}
%\caption{Hamiltonian/density}
    \end{subfigure}
    \caption{The result of blending a typical state of the Ising model simulation using the Laplacian operator. On the right, 
    a plot of the energy level versus negative log of kernel density.}
    \label{fig:isingblend}
\end{figure}


We then computed $\densalpha(f,s,d_0)$
for $s=.5$, and $d_0$ corresponding to $\%95$ of the data.
By taking a random 3D to 2D projection of the values in the 3 most dominant eigenvalues of $L$, we obtain a visualization of the energy landscape as in \ref{sec:config}, shown in Figure \ref{fig:isingvis}. In the first frame, associated to the interval, we have two densest types of points with energy level zero, corresponding to all spins equal to plus or minus 1, realized at the corners. We then have two curved line segments consisting of energy states with exactly one transition joining those points, starting from either side. The points shown in green/yellow correspond to states with two transitions, filling in the resulting circle to form a sphere. This is reflected in the persistence barcodes, shown in the figure.

\begin{figure}
\centering
\begin{subfigure}[b]{.3\textwidth}
\includegraphics[scale=.35]{figures/ising/figure_3/intshape.png}
\caption{$G=\isingint(30)$}
\label{fig:isingvisi}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\includegraphics[scale=.35]{figures/ising/figure_3/circshape.png}
\caption{$G=\isingcirc(30)$}
\label{fig:isingvisc}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\includegraphics[scale=.35]{figures/ising/figure_3/flareshape.png}
\caption{$G=\isingflares(43)$}
\label{fig:isingvisf}
\end{subfigure}
\centering
\begin{subfigure}[b]{\textwidth}
    \includegraphics[scale=.55]{figures/ising/figure_4/barcodes-ising.png}
    \caption{Barcodes associated to $G=\isingint(30)$}
\end{subfigure}
\caption{Low dimensional projections of 
$\densalpha(f,s,d_0)$ for the graphs
$\isingint(30)$, $\isingcirc(30)$, and $\isingflares(43)$. In the lower row, the persistence barcodes in the case of the interval.}
\label{fig:isingvis}
\end{figure}


\bibliographystyle{plain}
\bibliography{refs}


\end{document}

