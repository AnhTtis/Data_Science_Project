% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{multirow}

\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{bbm}
\usepackage{color}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{url}
\usepackage{mathtools, nccmath}
\newcommand*\rot{\rotatebox{90}}
% Example definitions.

% Title.
% ------
\title{Text is All You Need: ASR Personalization using Controllable TTS \\ (Author Response)}
\name{}
\address{}
\begin{document}
\maketitle

\noindent We thank reviewers (\textbf{R}s) for their positive comments on our paper. Overall, reviewers agreed that our work addresses an ``important practical issue'' (\textbf{R2}) of broad interest (\textbf{R1},\textbf{R4}) to the ICASSP community. We are particularly happy that reviewers (\textbf{R1},\textbf{R2}) expressed notable interest in our finding that language content is more important than the speaker voice when synthesizing speech for ASR personalization. We address reviewer comments below. \\

\noindent \textbf{Difference between controllable TTS and existing work using personalized TTS (R2).} %While existing studies have used personalized TTS for data augmentation, our use of controllable TTS enables us to analyze \emph{which aspect} of synthetic speech is most crucial to ASR personalization. We are happy to clarify this in the camera ready paper. 
Thanks R2 for the feedback-- we are happy to improve the discussion in the camera ready version.
Existing works that use personalized TTS focus on the benefits of augmenting with synthetic speech in the target speaker's voice.
In contrast, controllable TTS allows us to synthesize speech that matches a variety of voices (given a reference sample). This allows us to perform large-scale experiments with different synthetic datasets for ASR personalization, such as:
\begin{enumerate}[leftmargin=*]
    \item \textbf{$\textrm{D}_{\textrm{syn}}$}: Here we synthesize speech in the target speaker's voice, using text from their language domain. This dataset is analogous to personalized TTS from previous works.
    \item \textbf{$\textrm{D}_{\textrm{other-person-syn}}$}: Instead of using samples from the target speaker as style reference, we use samples from a different speaker's dataset; we synthesize speech in \emph{a different person's voice}, using text from the target language domain.
    \item \textbf{$\textrm{D}_{\textrm{multi-person-syn}}$}: Instead of using samples from the target speaker as style reference, we use samples from multiple speakers; we synthesize speech in \emph{many different people's voices}, using text from the target language domain.
    %\item \textbf{$\textrm{D}_{\textrm{global-text-syn}}$}: Instead of using text from the target language domain, we use text from the original global dataset; we synthesize speech in the target speaker's voice, but the text comes from a different corpus.
\end{enumerate}
Note that the latter two types of synthetic datasets (\textbf{$\textrm{D}_{\textrm{other-person-syn}}$} and \textbf{$\textrm{D}_{\textrm{multi-person-syn}}$}) could in theory be produced by personalized TTS models, but this would require fine-tuning many TTS models. Overall, controllable TTS is a more scalable approach than personalized TTS to change the acoustic style of synthetic adaptation data. More detailed comparison could be explored in the future.
%A novel and surprising result of our experiments is that all three types of synthetic data augmentation perform similarly for ASR model personalization, as long as the synthetic speech is mixed with the target speaker's real speech during training. In other words, our use of controllable TTS reveals that it is not critical for the synthetic speech's voice to match that of the target speaker, whereas existing works on personalized TTS primarily focus on matching the target speaker's voice. 


\noindent \textbf{Use of language model (R1, R4).} All of our ASR models are evaluated without a language model during decoding. The reviewers make a good point that using a powerful external language model would likely improve the results across the board, but we do not expect it to eliminate the benefits of model personalization / data augmentation. Even though the style of the synthetic speech is not crucial compared to the text, \emph{this is only because we combine it with real speech from the target speaker}. The ASR model still benefits from adapting to the voice of the target speaker from the real data, while the synthetic data provides additional text content and prevents overfitting of the ASR model to the limited content in the real data. This is evident from our experimental results where we see improvements from the data augmentation, even when there is no language domain gap between the global data and the target speaker data (i.e., Speaker Category 1 in Table 2).

\noindent \textbf{The effect of the amount of TTS speech on ASR performance (R2, R4).} In our experiments, the amount of TTS speech we use corresponds to the size of the text corpus used for synthesis. We agree with reviewers that it will be helpful to know the relationship between e.g., number of hours of synthetic speech and ASR performance. We will provide these comparisons in the camera ready version (space permitting) or the arXiv preprint. %\textcolor{red}{Add plot here if done in time.}

\noindent \textbf{Difference in text content vs. ASR performance (R4).} We agree that this relationship would be interesting to understand. The challenge is disentangling between the impact of content differences and style differences when comparing two individuals' ASR performance. For example, the speakers from LibriSpeech-other have much worse ASR performance than those in LibriSpeech-clean. While text differences may contribute, the gap is primarily attributed to large style differences such as accents and background noise in LibriSpeech-other. Perhaps the best way to study the relationship between text content and ASR performance would be in a more controlled setting, i.e., using controllable TTS to generate synthetic evaluation datasets for the exact same speakers, with different texts. We leave this exploration to future work.

\noindent \textbf{References (R2).} Thank you for the feedback; we will add additional references for papers that use standard TTS for ASR data augmentation. \textbf{Order of tables. (R4)} We appreciate the feedback and will try to re-position the tables in the camera ready version, depending on if there is space. \textbf{Typos (R1).} Thank you for pointing these out; we will fix them in the camera ready version.
\end{document}
