%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal]{IEEEtran}
%\documentclass[conference]{IEEEtran}
%\documentclass[journal,12pt,draftclsnofoot,onecolumn]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%\usepackage[sorting=none]{biblatex}
%\usepackage{layouts}
%\usepackage{graphicx}
%\addbibresource{ref.bib}
%


%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex

%
\usepackage{graphicx}
%\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}



% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


\newcounter{LLL}
\newcounter{CCC}
\newcounter{DDD}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[LLL]{Lemma}
\newtheorem{definition}[DDD]{Definition}
\newtheorem{corollary}[CCC]{Corollary}

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}
\newcommand{\defn}{\ensuremath{\triangleq}}
\newcommand{\limit}[2]{\ensuremath{\lim_{#1 \rightarrow #2}}}
\newcommand{\tvec}[1]{\ensuremath{\Tilde{\vec{#1}}}}
\newcommand{\uvec}[1]{\ensuremath{\underline{\vec{#1}}}}
\newcommand{\ovec}[1]{\vec{\Bar{#1}}}
\newcommand{\hvec}[1]{\ensuremath{\Hat{\vec{#1}}}}
\newcommand{\bvec}[1]{\ensuremath{\Breve{\vec{#1}}}}
\renewcommand{\vec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\mc}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\of}[1]{^{(#1)}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\renewcommand{\eqref}[1]{(\ref{eq:#1})}
\newcommand{\Eqref}[1]{Equation (\ref{eq:#1})}
\newcommand{\Figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\appref}[1]{Appendix~\ref{app:#1}}
\newcommand{\exaref}[1]{Example~\ref{exa:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lem:#1}}
\newcommand{\thmref}[1]{Theorem~\ref{thm:#1}}

% coloring
\newcommand{\textr}[1]{\textcolor{red}{#1}}
\newcommand{\textb}[1]{\textcolor{blue}{#1}}
\newcommand{\textg}[1]{\textcolor{green}{#1}}



% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Generalized Sparse Regression Codes \\ for Short Block Lengths}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Madhusudan Kumar Sinha \IEEEauthorrefmark{1}, Arun Pachai Kannu\IEEEauthorrefmark{2}}\\
  \IEEEauthorblockA{Department of Electrical Engineering \\
		    Indian Institute of Technology Madras \\
		    Chennai - 600036, India\\
		    Email: \IEEEauthorrefmark{1}ee16d028@ee.iitm.ac.in, \IEEEauthorrefmark{2}arunpachai@ee.iitm.ac.in }
}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}

Sparse regression codes (SPARC) connect the sparse signal recovery framework of compressive sensing with error control coding techniques. SPARC encoding produces codewords which are \emph{sparse} linear combinations of columns of a dictionary matrix. SPARC decoding is accomplished using sparse signal recovery algorithms. We construct dictionary matrices using Gold codes and mutually unbiased bases and develop suitable generalizations of SPARC (GSPARC). We develop a greedy decoder, referred as match and decode (MAD) algorithm and  provide its analytical noiseless recovery guarantees. We propose a parallel greedy search technique, referred as parallel MAD (PMAD), to improve the  performance. We describe the applicability of GSPARC with PMAD decoder for multi-user channels, providing a non-orthogonal multiple access scheme. 
%Short length codes are used in low power and low latency applications as well as in control channel communications. This paper studies sparse regression codes (SPARC) of short lengths ($n\leq 128$) over additive white Gaussian noise (AWGN) channel at low rates of practical interests ($R\approx0.5$).  SPARC is a recently proposed class of error control codes which uses sparse linear combinations of columns of a dictionary matrix as codewords. 
%SPARC is known to be asymptotically capacity achieving using power allocation and spatial coupling based encoders and approximate message passing (AMP) based decoders.
%SPARC has been shown to be asymptotically capacity achieving using power allocation and spatial coupling techniques at the encoder and approximate message passing (AMP) decoder in the literature.  In order to make SPARC suitable for short length codes, we propose a generalization of SPARC and develop a low cost parallel greedy decoder called parallel match and decode (PMAD) decoder in this paper. %PMAD decoder uses multiple greedy decoders with different starting points and chooses the solution with least value of residual norm. Unlike AMP decoders, PMAD decoders does not require the information of noise variance making it more suitable for sporadic communication scenarios.
We present numerical results comparing the block error rate (BLER) performance of the proposed algorithms for GSPARC in AWGN channels, in the short block length regime. The PMAD decoder gives better BLER than the approximate message passing decoder for SPARC. GSPARC with PMAD gives comparable and competitive BLER performance, when compared to other existing codes. In multi-user channels, GSPARC with PMAD decoder outperforms the sphere packing lower bounds of an orthogonal multiple access scheme, which has the same spectral efficiency.





%Through simulations we show that the error performance of PMAD decoder is comparable-to/better-than that of AMP decoder for real-valued/complex-values SPARC respectively at lower computational cost.

 %The dictionary matrix $\mathbf{A}$ has $L$ sections of $M$ columns each and exactly one column is selected from each section to obtain the codeword. 
 
 %In this paper we propose a generalization of SPARC where different sections can be of different sizes. We then develop parallel match and decode (PMAD) decoder based on greedy algorithms for decoding proposed short length codes. PMAD decoder is independent of the signal to noise ratio (SNR) and in short length regime, pMAD decoder has better error performance at lower computational complexity compared to AMP decoders. Simulation results using dictionary matrix based on gold codes and mutually unbiased bases (MUB) show that the proposed scheme gives comparable and competitive performance with respect to several widely used linear codes of small block lengths.
 
 % The coefficients of linear combination of selected columns can either be fixed (standard SPARC) or can come from PSK modulation (modulated SPARC).
\end{abstract}

% no keywords

\begin{IEEEkeywords}
sparse signal recovery, error control coding, greedy algorithm, parallel search, multi-user channels, non-orthogonal multiple access
\end{IEEEkeywords}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

Shannon's seminal paper on information theory established the existence of information encoding and decoding techniques that guarantee almost error-free communications across noisy channels \cite{shannon1948mathematical}. Extensive work has been carried out to develop such efficient error control coding techniques for additive white Gaussian noise (AWGN) channels \cite{lin2001error}. %It has also been established that linear coding schemes 
% achieve the capacity of single-user AWGN channels \textr{[Modern Coding Book]}. 
Error correcting codes such as turbo codes and LDPC codes 
are widely used in various communication systems today, which have very small block error rates for large block lengths.
%Maximum likelihood decoding in AWGN channels boils down to finding the codeword in the codebook which is closest to the received signal \cite{madhow2008fundamentals}. Hence the distance properties of the codewords in the codebook play an important role in the error performance of the coding scheme. 

In our work, we consider error control coding in the short block length regime.  In many communication systems, the control channel information is typically sent over short block lengths. Several use cases in the fifth generation (5G) of mobile networks such as ultra-reliable low-latency communication (URLLC) and massive machine type communications in IoT applications require short block lengths. For instance, in URLLC scenarios such as industrial automation, autonomous vehicles and augmented/virtual reality, short length codes are required to meet the low latency requirements.  A comprehensive study on the performance of existing codes in the short block length regime has been done in  \cite{van2018short,cocskun2019efficient}.
%In mMTC scenario where a large number of devices having low energy and low computational power, communicate small data packets with each other and base station, such as in sensor networks and internet of things, short length codes are used to increase the energy efficiency and battery life of the devices. In 5G eMBB communication scenario, short length codes are used in control channel communications which in turn controls the data stream between various components of the communication network.


Sparse regression codes (SPARC) \cite{joseph2012least,joseph2013fast} connect the sparse signal recovery framework of compressive sensing \cite{eldar2012compressed} with error control coding techniques. In SPARC, a dictionary matrix (design matrix) $\vec{A}$ of size $N \times L$ with $L >> N$ is partitioned into $K$ equal sub-blocks (sections), with each sub-block having $\frac{L}{K}$ columns. Based on the information bits, one column is chosen from each block and the codeword for transmission is obtained as the sum of chosen columns. We can represent the codeword as $\vec{s} = \vec{A} \vec{x}$, where $\vec{x}$ is a $K$-sparse signal with exactly $K$ non-zero entries.  The non-zero entries of $\vec{x}$ are fixed and known in the standard SPARC \cite{rush2017capacity} and they are chosen from a PSK constellation (based on information bits) in the modulated SPARC \cite{hsieh2021modulated}. 

In \cite{joseph2012least,joseph2013fast}, the SPARC codes using Gaussian dictionary matrices were proven to achieve channel capacity for AWGN channels, as the block lengths approach infinity. 
Several power allocation (across the sub-blocks) and spatial coupling techniques have been developed for SPARC  
\cite{joseph2013fast,cho2013approximate,rush2017capacity,hsieh2021modulated, greig2017techniques,barbier2015approximate,barbier2017approximate,barbier2019universal} to improve the empirical performance of SPARC codes.
%Using dictionary matrices with Gaussian entries, SPARC has been shown to be asymptotically capacity achieving using power allocation \cite{joseph2013fast,cho2013approximate,rush2017capacity,hsieh2021modulated} and spatial coupling \cite{barbier2015approximate,barbier2017approximate,barbier2019universal,rush2017capacity,hsieh2021modulated} based encoders and AMP decoder.
In \cite{rush2017capacity,hsieh2021modulated} approximate message passing (AMP) decoders have been developed for standard and modulated SPARC, which guarantee that sub-block error rate goes to zero for all rates below capacity in AWGN channels. %Here, an outer code is used to correct the sub-block errors, so that the overall block error rate can be made small. 
It has been shown empirically that the AMP decoder derived for Gaussian dictionary matrices work well with other dictionary matrices like the ones based on Hadamard matrices \cite{barbier2017approximate,rush2017capacity} and fast Fourier transform (FFT) matrices \cite{hsieh2021modulated}. Clipping and generalized AMP are discussed in \cite{liang2021finite}, in order to improve finite block length performance of SPARC at low to medium code rates (bits per channel use). Iterative power allocation techniques are given in \cite{greig2017techniques}, which improve the performance of SPARC in high code rates.

In our work, we consider SPARC for short block lengths, with $N \leq 128$ and code rate $\approx0.5$ bits per channel use (bpcu), a regime which has gained sufficient interest in the recent years \cite{van2018short,cocskun2019efficient} and where the methods to improve finite length performance of SPARC doesn't work \cite{liang2021finite,greig2017techniques}. We construct good deterministic dictionary matrices, utilizing the  existing literature on generating a large set of sequences with good correlation properties. Specifically, we use Gold codes \cite{Goldcode} from the CDMA literature and  mutually unbiased bases (MUB) \cite{wootters1989optimal} from the quantum information theory. With these constructions, the number of columns in $\vec{A}$ is $L \approx N^2$, where $N$ is the length of each column and the maximum (normalized) cross correlations among the columns (usually referred as mutual coherence in compressive sensing) is approximately  $\frac{1}{\sqrt{N}}$. We choose the sparsity level $K$ close to $\sqrt{N}$, which typically ensures that the bpcu falls in the regime of interest when $N \leq 128$. The contributions of our work are summarized below.

We construct dictionary matrices using Gold codes and mutually unbiased bases, which have not been previously used in SPARC.

We generalize the SPARC by possibly allowing sub-blocks of different sizes, which we refer as sub-block structure encoding (SSE) scheme. We also allow modulation of the $K$ selected columns using information symbols from finite alphabet constellations. We give an algorithm to partition the given dictionary matrix with a total of $L$ columns into $K$ sub-blocks so that size of each sub-block is power of 2 and the number of information bits carried by SSE is maximized.

We also generalize SPARC by entirely eliminating the sub-block structure, which we refer as sub-block free encoding (SFE) scheme. In SFE, we allow choosing \emph{any} $K$ columns from a total of $L$ columns from the dictionary matrix. We give an iterative procedure which uniquely maps the information bits to one of the $\binom{L}{K}$ combinations. Our iterative procedure is quite efficient and eliminates the need for any look-up tables.

We develop a simple greedy algorithm for the generalized SPARC (GSPARC), which we refer as match and decode (MAD) algorithm, to recover the sparse signal (and subsequently the information bits) from the noisy observation of the codeword. Our MAD algorithm inherently exploits the finite alphabet nature of the modulation symbols and performs better than the conventional orthogonal matching pursuit (OMP) algorithm in AWGN channels. We give analytical recovery guarantees of the MAD decoder, in terms of the coherence of the dictionary matrix and the coherence parameter of the modulating constellation symbols. 


We improve the MAD algorithm by introducing a parallel search mechanism, which we refer as parallel MAD (PMAD) algorithm. Our MAD and PMAD decoders do not require the knowledge of the channel noise variance. Using numerical simulations, we show that our PMAD algorithm performs better than the AMP algorithm \cite{hsieh2021modulated} in AWGN channels, for short block lengths.  

We also show that our PMAD with GSPARC provides competing block error rate performance in the short block lengths, when compared with several existing codes \cite{cocskun2019efficient,van2018short}. 
In  addition, we also show that SSE can be used in multi-user channels, such as multiple-access, broadcast and interference channels. %In these cases, we show that PMAD with SSE offers significant gains over the corresponding orthogonal multiple access schemes with single user error control codes. 
For some combinations of code rates, block lengths and number of users, we show that SSE with PMAD decoder outperforms the sphere packing lower bounds of an orthogonal multiple access scheme.

The paper is organized as follows: In \secref{enco}, we provide the details of the encoding techniques for GSPARC. In \secref{dict}, we give the details of dictionary matrix construction. In \secref{deco}, we describe the decoding algorithms and their analytical performance guarantees. 
In \secref{mult}, we discuss on how SSE and PMAD can be employed in multi-user communication channels. In \secref{simu}, we present block error rate performance comparison in AWGN channels. In \secref{conc}, we present conclusions and give directions for future work.


%This paper studies SPARC of short lengths $(n\leq128)$ for communication over AWGN channel at low rates of practical interests ($R\leq 0.5$) . SPARC is a recently proposed class of error control codes which uses sparse linear combinations of columns of a dictionary matrix as codewords \cite{joseph2012least,joseph2013fast}. The codeword $\mathbf{x=A\beta}$ is defined in terms of a dictionary matrix $\mathbf{A}$ of size $n\times N$ (with $n<N$) and a sparse vector $\beta$ of size $N\times 1$. The dictionary matrix $\mathbf{A}$ is divided into $L$ sections of $M$ columns each such that $N=LM$, and exactly one column is selected from each section to form a codeword. The number of non-zero elements in $\mathbf{\beta}$ is given by $\|\mathbf{\beta}\|_0=L$. The non-zero entries of $\beta$ can be known and fixed (standard SPARC) or can come from $K$-ary PSK modulation (modulated SPARC \cite{hsieh2021modulated}). The codewords  are sent over AWGN channel to obtain the received signal $\mathbf{y}$ given by
%\begin{equation}
%    \mathbf{y=x+w}
%\end{equation}
%where the channel output $\mathbf{y}$ is the sum of a codeword $\mathbf{x}$ and additive white Gaussian noise $\mathbf{w}$. The codewords satisfy an average power constraint given by $\frac{1}{n}\sum_{i=1}^n |x_i|^2 \leq P$. If the codewords are real valued then the noise is modelled as real valued i.i.d. additive white Gaussian noise with zero mean and variance $\sigma^2$ and is denoted by $\mathcal{N}(0,\sigma^2)$. Similarly if the codewords are complex valued then the noise is modelled as i.i.d. circularly symmetric complex additive white Gaussian noise with zero mean and variance $\sigma^2$ and is denoted by $\mathcal{CN}(0,\sigma^2)$. 

 %In standard SPARC, information bits are encoded only in the locations, while in modulated SPARC, the information bits are encoded in the locations as well as the values of non-zero entries of the sparse vector $\mathbf{\beta}$.


%In this paper we take $K=2$ for real valued modulated SPARC and $K=4$ for complex valued modulated SPARC.


%At the receiver, approximate message passing (AMP) decoder is used to detect the sparse vector $\mathbf{\beta}$ given $\mathbf{y}$. Using dictionary matrices with Gaussian entries, SPARC has been shown to be asymptotically capacity achieving using power allocation \cite{joseph2013fast,cho2013approximate,rush2017capacity,hsieh2021modulated} and spatial coupling \cite{barbier2015approximate,barbier2017approximate,barbier2019universal,rush2017capacity,hsieh2021modulated,} based encoders and AMP decoder. Both power allocation and spatial coupling make decoding of some columns easier than others by changing the variance of the entries of dictionary matrix. It has been shown empirically that the AMP decoder derived for Gaussian dictionary matrices work well with other dictionary matrices like the ones based on Hadamard matrices \cite{barbier2017approximate,rush2017capacity} and fast Fourier transform (FFT) matrices \cite{hsieh2021modulated}. However the performance of finite length SPARC is poor and can't be improved with direct implementation of power allocation or spatial coupling techniques \cite{greig2017techniques,liang2021finite}.
%As noted in \cite{liang2021finite}, the techniques discussed in \cite{greig2017techniques} to improve the performance of finite length SPARC including iterative power allocation work only in high communication rate scenarios ($R\geq 1.5$). Clipping and generalized AMP are discussed in \cite{liang2021finite} in order to improve finite length performance of SPARC at low to medium rates. However clipping is useful only when section size $M\leq 256$, rate $R\leq 1.5$ and the SPARC is of moderate code length ($n\geq 1000$).

%%at low to medium rates of practical interests i.e., $R<1.5$ bits per channel use (bpcu) of a real channel and can not be improved using  spatial coupling or power allocation \cite{greig2017techniques,liang2021finite}. Methods to improve finite length performance of SPARC discussed in \cite{greig2017techniques,liang2021finite} doesn't work for  very small $n$ and  $L$ at low rates.



%The codewords are sent over AWGN channel to obtain the output
%\begin{equation}
%    \mathbf{y = x + w }
%\end{equation}
%Where $\mathbf{y,x}$ and $\mathbf{w}$ are $n$-length vectors. The channel output $\mathbf{y}$ is a sum of a codeword $\mathbf{x}$ and zero-mean i.i.d. circularly symmetric Gaussian noise vector $\mathbf{w}$.



%The iterative power allocation method discussed in \cite{greig2017techniques} doesn't works in low rate scenarios and the clipping method discussed in \cite{liang2021finite} doesn't work when the sparsity level $L$ is very small or section size $M>256$. Thus, known techniques to improve the  performance of finite length SPARC doesn't work when the code length is very short i.e., $n\leq 128$. 

%In this paper we study SPARC of short lengths for communication at low rates of practical interests i.e., $R\leq 0.5$ bpcu of real channel. 

%Thus the existing methods fail to provide good error performance for SPARC of short length at low rates of communications. In this paper we consider a variety of techniques to get short length SPARC with good error performance. These include designing/selecting good dictionary matrices with low mutual coherence, introducing generalized SPARC and introducing a new decoder known as parallel match and decode (PMAD) decoder.

%Performance of any sparse recovery algorithm depend on mutual coherence of the dictionary matrix and the sparsity level of the unknown sparse vector. Generally low mutual coherence and low sparsity levels give better recovery performance. Given a dictionary matrix $\mathbf{A}$ with mutual coherence $\mu(\mathbf{A})$ and a target rate $R  $, minimizing the sparsity $L$ improves the error performance.

%Taking $K=1$ for standard SPARC as a convention, the number of bits encoded in a codeword and the communication rates for both standard and modulated SPARC, are given by $N_b = L\cdot\log_2(KM)$ bits and $R=N_b/n$ bits per channel use (bpcu) respectively. 

%In order to be able to compare the performances of real and complex valued SPARC, we define rate per real dimension as $R_{rd} = N_B/n$ if the codewords are real valued and as $R_{rd} = N_B/(2n)$ if the codewords are complex valued. 

%In order to get good error performance with short length SPARC at low rates we select dictionary matrices with very low mutual coherence and minimize the sparsity level of SPARC for a given rate.

%and we generalize the modulated SPARC construction to include sections of different sizes. This generalization becomes necessary since any set of vectors having good cross correlation properties have a fixed number of vectors in it. Given any such set of vectors we want to maximize the rate $R$ and minimize the sparsity $L$ for better error performance. We give an iterative algorithm to select the optimum section sizes for any given set of vectors with $\tilde{N}$ vectors in it and given sparsity $L$. We also develop parallel match and decode (PMAD) decoder which uses multiple greedy decoders with different starting points and selects the decoder which gives minimum residue energy. PMAD decoder is computationally more efficient than AMP decoder in short code length regime and doesn't require information of signal to noise ratio (SNR) for decoding. Through simulations we show that for same number of iterations, 1) PMAD decoder have similar error performance compared to MAD decoder over real AWGN channel, 2) PMAD decoder have better error performance than AMP decoder over complex AWGN channel.

%The paper is organized as follows : In the remaining of this section we describe the standard SPARC and modulated SPARC construction. We then explain the generalization of modulated SPARC in which different sections can have different sizes and give an algorithm to trim and divide the columns of a dictionary matrix to maximize the number of bits encoded. In section II we discuss Encoding and decoding scheme for generalized modulated SPARC. In section III, we present the details of constructing dictionary matrices using Gold codes and complex mutually unbiased bases. In Section IV, we present simulation studies on the error performance of the proposed schemes in AWGN channel and compare with some of the existing error control codes. 

%We consider communication over AWGN channel:
%\begin{equation}
%    y = x + w
%\end{equation}
%where $x$ is the codeword of length $n$, $w$ is zero mean additive white Gaussian noise, and $y$ is the channel output. There is a power constraint on input signal i.e., the input vector of length $n$ should satisfy  
%\begin{equation}
%    \frac{1}{n}\sum_{i=1}^n |x_i|^2 \leq P
%\end{equation}
%with high probability.
%Standard SPARC focused on the cases in which the dictionary matrix were real valued and the information bits were encoded only in the locations of non-zero entries of $\beta$ i.e., the non-zero entry in each section were fixed and known at the transmitter and the receiver. Modulated SPARC [\footnote{Modulated Sparse Superposition Codes for the Complex AWGN Channel}] considered the case in which the dictionary matrix was complex valued and the non-zero entries of $\beta$ were taken from some $K$-ary PSK constellation. For both standard SPARC and modulated SPARC, the dictionary matrix $A$ was divided into $L$ sections of size $M$ each i.e., $M_1=M_2=...=M_L=M$. If we take $K=1$ for standard SPARC case (no modulation) then the number of bits encoded and rate of communication in SPARC is given by 
%\begin{equation}
%    N_b = \sum_{i=1}^L \log(KM_i)
%\end{equation}
%\begin{equation}
%    R = \frac{1}{n}\sum_{i=1}^L \log(KM_i)
%\end{equation}
%We use $\log(\cdot)$ for $\log_2(\cdot)$ and $\ln(\cdot)$ for $\log_e(\cdot)$ in this paper.

%The computational complexity of AMP decoder for SPARC can be higher compared to greedy based algorithms in short length code regime. Greedy algorithms are known to have low computational complexity at the cost of poor error performance. 

%In this paper we develop a generalization of modulated SPARC where the section sizes $M_1,M_2,...,M_L$ can be different. We also develop a parallel MAD decoder based on greedy approach based Matching pursuit (MP) algorithm. We show that in real case the pMAD algorithm works slightly better than AMP based decoder and in complex case it works significantly better than AMP based decoder when the knowledge of noise variance is assumed in AMP based decoder. When noise variance is not known AMP based decoder have even worse error performance at higher computational complexity.

%We consider modulated SPARC of small code-lengths in this paper. This scenario is particularly suitable for low-powered devices such as in sensor networks and internet of things and also for control signalling where the data is sent in small packets. The computational cost of AMP based  decoder can be too high in such scenarios. The parameters of interest in short codelength modulated SPARC are 1) choosing suitable dictionary matrix, 2) choosing section lengths $M_1,M_2,...,M_L$, and 3) choosing efficient decoder. It is well known that the performance of any sparse signal recovery algorithm depends on restrictive isometric properties (RIP) \textbf{CITE} or Mutual Incoherence Properties (MIP) \textbf{CITE}. However it is much harder to get a deterministic matrix that satisfies desired RIP properties \textbf{CITE}, we thus use deterministic matrices with good MIP properties as our dictionary matrix to get good error performance. Mutual coherence of a dictionary matrix with normalized columns is defined as the max of absolute value of pairwise cross correlation of columns of $A$.
%\begin{equation}
%    \mu(A) = \max_{i<j} |a_i^H a_j|
%\end{equation}
%We want $\mu$ to be smaller and $N$ to be higher in order to encode more bits. Generally $\mu$ is non-decreasing function of $N$, which means we can not get $\mu$ arbitrarily smaller for large $N$. However there are certain sets of vectors giving small $\mu$ and large $N$ such as vectors from mutually unbiased bases (MUB) and gold code sequences. There are $N=n(n+1)$ vectors with mutual coherence $\mu=1/\sqrt{n}$ from $n+1$ MUBs of $\mathrm{C}^d$. similarly gold sequence gives us $n^2$ vectors with low mutual coherence. Since the number of columns is bounded 

%in which the different sections can have different sizes i.e., $M_1,M_2,...,M_L$ can be different. The power constraint on the channel input can be given as $\frac{1}{n}\sum_{i=1}^n |x_i|^2=P$. The number of bits encoded is given as $N_b = \sum_{i=1}^L log_2(KM_i)$. We define rate per real dimension as $R = N_B/n$ if both $A$ and non-zero entries of $\beta$ are real valued and as $R=N_b/(2n)$ if at-least one of $A$ or the non-zero entries of $\beta$ are complex valued.

%1. want short length codes for low powered devices.

%2. want small $\mu$ and large $N$.

%3. $\tilde{N}$ if generally a finite number for a fixed $\mu<1$. We thus need different section sizes to maximize the number of bits encoded.

%4. We give an algorithm to give the section sizes $M_1,M_2,...,M_L$ such that $\sum M_i = N < \tilde{N}$

%5. sensor networks usually have sporadic communication so knowledge of SNR is not available. online AMP based decoder for SPARC to be compared with pMAD.



%\textr{Need to compress the intro..}



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


%\section{Encoding and Decoding}
%\subsection{Generalized SPARC}  The codeword $x=A\beta$ in generalized SPARC is defined in terms of a dictionary matrix $A$ of size $n\times N$ with ($n<N$) and a sparse vectors $\beta$ of size $N\times 1$. The dictionary matrix $\mathbf{A}$ is divided into $L$ sections of sizes $M_1,M_2,\cdots,M_L$ such that $\sum_{i=1}^L M_i=N$. The non-zero entries of $\mathbf{\beta}$ corresponding to each section of sizes $M_1,M_2,\cdots,M_L$ come from $K_1,K_2,\cdots,K_L$ ary modulation respectively. Here the section sizes $M_1,M_2,\cdots,M_L$ and modulation sizes $K_1,K_2,\cdots,K_L$ may or may not be the same for all sections.

%Let $[N]=\{1,2,...,N\}$ be the set of integers representing column indices and let $S\subset [N]$, and let $A_S$ denotes the matrix formed by taking columns of $A$ whose indices are in $S$. In SPARC the matrix $A$ is divided into $L$ sections of sizes $M_1,M_2,...,M_L$ such that $\sum_{i=1}^L M_i = N$. Using the notation $[a:b]=\{a,a+1,...,b\}$ for integers $a$ and $b$, the indexing set for each sections can be written as $S_i = [(\sum_{j=1}^{i-1}M_j)+1 : \sum_{j=1}^{i}M_j]$. The $i^{th}$ section of $A$ and $\beta$ can then be represented as $A_{S_i}$ and $\beta_{S_i}$ respectively. One column from each section is selected and a linear combination of these columns gives the codeword for SPARC. More precisely the codeowrd $x=A\beta$ where $\beta$ is an $N\times 1$ vector and $\|\beta_{S_i}\|_0=1\;\forall i$, i.e., $\beta$ has one non-zero entry for each sections of $A$. 
%\subsubsection{Standard SPARC}
%\subsubsection{Modulated SPARC}

%\subsection{Parallel Match and Decode (PMAD) decoder}

\section{Generalized SPARC Encoding Procedure} \label{sec:enco}


\subsection{Sub-block Structure Encoding} \label{sec:sse}
Consider a dictionary matrix $\vec{A}$ of size $N \times L$, with unit norm columns and $L \geq N$.  
The codewords for messages are obtained using \emph{sparse} linear combinations 
of columns of the matrix $\vec{A}$. In SSE, we fix the \emph{sparsity} level as $K$ with $K \leq N$ and partition the dictionary matrix $\vec{A}$ into $K$ subblocks (also referred as sections) such that $\vec{A} = \left[\vec{A}_1 \cdots \vec{A}_K \right]$ with $k^{th}$ sub-block $\vec{A}_k$ having a size of $N \times L_k$ and $\displaystyle \sum_{k=1}^K L_k = L$. We assume that the number of columns in each sub-block is a power of 2 and the sub-blocks can possibly have unequal sizes. Based on the information bits, one column from each sub-block is selected and transmit codeword is obtained as a linear combination
\begin{align}
\vec{s} &= \sum_{k=1}^K \beta_k \vec{a}_{\alpha_k}, \label{eq:cw1}
\end{align}
where $\vec{a}_{\alpha_k}$ is a column from sub-block $\vec{A}_k$ and the \emph{modulation symbol} $\beta_k$ is chosen from an $M$-ary constellation $\mc{M}$. Now, the codeword in \eqref{cw1} can be represented as
\begin{align}
\vec{s} &= \vec{A}\vec{x}, \label{eq:cw2}
\end{align}
where $\vec{x}$ of size $L \times 1$ is a sparse signal with only $K$ non-zero entries from the constellation $\mc{M}$. We also allow the special case of $M=1$, for which $\beta_k = +1, \forall k$. Let $\mc{S}$ denote the support of $\vec{x}$. Since the symbol $\beta_k$ carries $\log M$ bits (assuming $M$ is a power of 2) and the columns of sub-block $\vec{A}_k$ are indexed using $\log L_k$ bits, the total number of information bits encoded in the codeword in \eqref{cw1} is
\begin{align}
N_b &= K \log M + \sum_{k=1}^K \log L_k. \label{eq:nbsse}
\end{align}
We have used base 2 for $\log$ throughout the paper. We define the \emph{code rate} of the encoding scheme in units of bits per real channel use (bpcu) as the number of bits transmitted per  
real dimension utilized. If $\vec{A}$ is a real matrix and the constellation $\mc{M}$ is real (such as PAM, BPSK), 
the code rate is $\frac{N_b}{N}$ bpcu.  On the other hand, if $\vec{A}$ is a complex matrix and/or the constellation symbols are complex, 
the code rate is $\frac{N_b}{2N}$ bpcu. SPARC encoding in \cite{hsieh2021modulated,rush2017capacity} mandates all the sub-blocks to be of equal sizes. Since we allow for unequal sub-block sizes, SSE scheme \eqref{cw1} is a generalization of the SPARC encoder. 

%In general, the sub-blocks can have unequal sizes, which is different from the SPARC encoding in \cite{rush2017capacity,hsieh2021modulated} where all the sub-blocks are mandated to be of equal sizes. 


\subsection{Sub-block Partitioning Algorithm} \label{sec:spa}

Deterministic construction of sequences with good correlation properties exist in the literature of CDMA \cite{Goldcode,frank1963polyphase,chu1972polyphase}  and quantum information theory \cite{wootters1989optimal,renes2004symmetric}. 
%In CDMA literature, we have Gold codes \cite{Goldcode} and Zadoff-Chu sequences \cite{frank1963polyphase,chu1972polyphase} while in quantum information theory, we have mutually unbiased bases \cite{wootters1989optimal} and equi-angular lines \cite{renes2004symmetric}. 
Dictionary matrices based on these deterministic constructions are good candidates due to their small coherence values. However, these constructions exist only for certain values of $N$ and $L$. In these cases, we need to have a proper sub-block partitioning algorithm such that the number of information bits \eqref{nbsse} conveyed through the codeword \eqref{cw1} is maximized for a given $K$. 

Given the total number of columns $L$ in the dictionary matrix and the required number of partitions $K$, we want to optimize 
$\sum_{k=1}^K \log(L_k)$  with the constrain that each $L_k$ is a power of $2$ and $\displaystyle \sum_{k=1}^K L_k \leq L$. If we allow $L_k$ to take any real value, the solution for the above optimization problem is readily obtained as $L_1=L_2=\cdots=L_K=\frac{L}{K}$, that is, all the sub-blocks should be of equal size. To meet the power of 2 constraint, we set the size of the smallest sub-block as $\displaystyle L_1 = 2^{\lfloor \log\frac{L}{K} \rfloor}$, the largest power of 2 number which is less than or equal to $\frac{L}{K}$. After this step, the problem reduces to divide $L-L_1$ columns into $K-1$ sub-blocks. Proceeding in the same manner iteratively, the optimal partitioning sizes are obtained as 
\begin{align}
L_k &= 2^{\lfloor \log \frac{L-\sum_{m=1}^{k-1}L_m}{K-k+1} \rfloor},~~ k=1,\cdots,K.
\end{align}   
For example, if $L=23$ and $K=3$, the optimal partition sizes are $L_1=4$, $L_2=L_3=8$ and the remaining $3$ columns are unused/discarded. From the above procedure, it also follows that the size of the largest sub-block can be at most twice the size of the smallest sub-block, that is, $L_K \leq 2 L_1$.


\subsection{Sub-block Free Encoding}

In SFE, we eliminate the sub-block structure and choose \emph{any} subset of $K$ columns from a total of $L$ columns and modulate 
the chosen columns using symbols from an $M$-ary constellation. SFE scheme is another generalization of the SPARC encoding scheme from \cite{hsieh2021modulated,rush2017capacity}. The number of bits encoded by SFE scheme will be
\begin{align}
N_b &= K \log M + \lfloor \log \binom{L}{K} \rfloor, \label{eq:nbsc}
\end{align} 
which will be larger than or equal to that of the SSE scheme \eqref{nbsse}. Unlike SSE scheme, mapping bits into a subset of $K$ columns is not straightforward. We provide an iterative scheme to achieve this feat without using any look-up table. 

Specifically, we provide a one-to-one mapping between non-negative integers and combinations of $K$ objects out of $L$ objects. In our SFE-GSPARC, a sequence of $N_b$ bits (representing a non-negative integer) is mapped to a unique combination of $K$ columns out of the total $L$. We use lexicographic ordering within each combinations to form a unique ordered set (word) representing a combination. We then use lexicographic ordering over all possible words to list all possible combinations. This ordering allows us to get a one-to-one mapping between bits and object combinations. We provide a numerically efficient method to map bits to combinations and vice versa, without generating and storing the actual list.

Let $\mc{B}=\{0,1,2,3,...,L-1\}$ be a set of $L$ distinct objects, where we use the first $L$ non-negative integers as an abstraction for a set of $L$ different objects. The total number of combinations of $K$ objects out of $L$ objects is given by $\binom{L}{K}$. Let a combination be represented uniquely by the ordered set $\vec{b}=(b_0,...,b_{K-1})$ such that $0\leq b_0<b_1<...<b_{K-1}\leq L-1$. The lexicographic ordering on the ordered set representation of the combinations allow us to list the combinations against non-negative integers.  For example, with $L=5$ and $K=3$, there are $\binom{5}{3}=10$ unique combinations. The lexicographic listing of these combinations against non-negative integers is given in Table~\ref{example:5C3}. 
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
index & combinations \\ \hline
0 &  (0,1,2)      \\ \hline
1 &  (0,1,3)      \\ \hline
2 &  (0,1,4)      \\ \hline
3 &  (0,2,3)      \\ \hline
4 &  (0,2,4)      \\ \hline
5 &  (0,3,4)      \\ \hline
6 &  (1,2,3)      \\ \hline
7 &  (1,2,4)      \\ \hline
8 &  (1,3,4)      \\ \hline
9 &  (2,3,4)      \\ \hline
\end{tabular}
\caption{Lexicographic listing of 3 objects chosen out of 5.}
\label{example:5C3}
\end{table}


We note that, out of $\binom{L}{K}$ combinations, $\binom{L-i}{K}-\binom{L-(i+1)}{K}=\binom{L-(i+1)}{K-1}$ combinations start with object $i$ where $i\in\{0,1,...,L-K\}$. The decimal indices of combinations starting with object $i$ starts at $\binom{L}{K}-\binom{L-i}{K}$ and ends at $\binom{L}{K}-\binom{L-(i+1)}{K}-1$. For the decimal index $d \in \{0,\cdots,\binom{L}{K}-1\}$ represented by the combination $\vec{b}=(b_0,b_1,...,b_{K-1})$, we will have $b_0 = i$, if 
\begin{align}
\binom{L}{K}-\binom{L-i}{K} &\leq d <\binom{L}{K}-\binom{L-(i+1)}{K}. \label{eq:require}
\end{align} 
In  \appref{index_comb}, we show that, non-negative integer $i$ satisfying above constraint is upper bounded as
\begin{align}
    i &\leq \bigg\lfloor
    \left(L-\frac{(K-1)}{2}\right)\left\{
    1-\left(1-\frac{d}{\binom{L}{K}}\right)^\frac{1}{K}\right\}\bigg\rfloor ~~:= \bar{i}(L,K). \label{eq:ibar}
\end{align}
Given $L$ and $K$, we first find $\bar{i}(L,K)$ from \eqref{ibar} and check if $\bar{i}$ satisfies \eqref{require} for the given $d$. If not, we keep decrementing $\bar{i}$ by one, until we find the integer $i$ satisfying the constraint \eqref{require}. Once the first object $b_0$ is chosen, the problem is reduced to choosing $K-1$ objects out of $L-b_0$ objects corresponding to the decimal index $\displaystyle d-\left[\binom{L}{K}-\binom{L-b_0}{K}\right]$. Hence, the same procedure can be recursively applied until the last object $b_{K-1}$ is chosen. In our simulations, we find that either $\bar{i}$ in \eqref{ibar} or $\bar{i}-1$ always satisfies the requirement in \eqref{require}.

Using the same counting argument, given the set of $K$ objects $\{b_0,\cdots,b_{K-1}\}$ (from a total of $L$), we can find the decimal index corresponding to the lexicographic ordering as
\begin{align*}
    d &=\binom{L}{K}-\sum_{k=0}^{K-2}\left[\binom{L-b_k}{K-k}-\binom{L-b_{k}-1}{K-k-1}\right] - \binom{L-b_{K-1}}{1}, \\
     &=\binom{L}{K} - \left[\sum_{k=0}^{K-2}\binom{L-b_k-1}{K-k}\right] - \binom{L-b_{K-1}}{1}.
\end{align*}


%\subsection{Generalized SPARC}  The codeword $x=A\beta$ in generalized SPARC is defined in terms of a dictionary matrix $A$ of size $n\times N$ with ($n<N$) and a sparse vectors $\beta$ of size $N\times 1$. The dictionary matrix $\mathbf{A}$ is divided into $L$ sections of sizes $M_1,M_2,\cdots,M_L$ such that $\sum_{i=1}^L M_i=N$. The non-zero entries of $\mathbf{\beta}$ corresponding to each section of sizes $M_1,M_2,\cdots,M_L$ come from $K_1,K_2,\cdots,K_L$ ary modulation respectively. Here the section sizes $M_1,M_2,\cdots,M_L$ and modulation sizes $K_1,K_2,\cdots,K_L$ may or may not be the same for all sections.

%Let $[N]=\{1,2,...,N\}$ be the set of integers representing column indices and let $S\subset [N]$, and let $A_S$ denotes the matrix formed by taking columns of $A$ whose indices are in $S$. In SPARC the matrix $A$ is divided into $L$ sections of sizes $M_1,M_2,...,M_L$ such that $\sum_{i=1}^L M_i = N$. Using the notation $[a:b]=\{a,a+1,...,b\}$ for integers $a$ and $b$, the indexing set for each sections can be written as $S_i = [(\sum_{j=1}^{i-1}M_j)+1 : \sum_{j=1}^{i}M_j]$. The $i^{th}$ section of $A$ and $\beta$ can then be represented as $A_{S_i}$ and $\beta_{S_i}$ respectively. One column from each section is selected and a linear combination of these columns gives the codeword for SPARC. More precisely the codeowrd $x=A\beta$ where $\beta$ is an $N\times 1$ vector and $\|\beta_{S_i}\|_0=1\;\forall i$, i.e., $\beta$ has one non-zero entry for each sections of $A$. 
%\subsubsection{Standard SPARC}
%\subsubsection{Modulated SPARC}

\section{Dictionary Matrix Construction} \label{sec:dict}

The choice of the dictionary matrix $\vec{A}$ plays a vital role in the block error performance. It is desirable that the dictionary matrix has a large number of columns (as the number of information bits increases with $L$, for fixed $N$ and $K$) with small correlation among the columns (for good sparse signal recovery performance), which is characterized by the the mutual coherence of the dictionary matrix $\vec{A}$, defined as,
\begin{equation}
\mu(\vec{A}) = \max_{p \neq q}  \frac{|\langle \vec{a}_p, \vec{a}_q \rangle|}{\|\vec{a}_p\| \|\vec{a}_q\|}.
\end{equation}
In this paper, we consider  dictionary matrix constructions using Gold code sequences from CDMA literature and mutually unbiased bases from quantum information theory, which have small coherence values.

\subsection{Gold Codes}
%\textb{Describe the following points here along with suitable reference}
Gold codes are binary sequences with alphabets $\{\pm 1\}$. Considering lengths of the form $N = 2^n-1$, where $n$ is any positive integer, there are  $2^n+1$ Gold sequences. By considering all the circular shifts of these sequences, we get $2^{2n}-1$ sequences. When dictionary matrix columns are  constructed with these $2^{2n}-1$ sequences normalized to unit norm, the resulting cross-correlation between any two columns of the dictionary matrix matrix takes only three possible values given as $\frac{-1}{N}$, $\frac{-t(n)}{N}$ and $\frac{t(n)-2}{N}$ where $t(n)$ is given by \cite{Goldcode}, 
\begin{equation}
    t(n)=\begin{cases} 
      1+2^\frac{n+1}{2}, & n \text{ is odd,} \\
      1+2^\frac{n+2}{2}, & n \text{ is even.}
   \end{cases} 
\end{equation}
The mutual coherence of the gold code dictionary matrix is thus given by
\begin{equation}
   \mu=\frac{t(n)}{N} \label{eq:mugold}
\end{equation}
and we note that odd value of $n$ leads to smaller values of mutual coherence.  We can add any column of the identity matrix to the Gold code dictionary matrix, to get a total of $L=2^{2n}$ columns (which is a power of 2), with the mutual coherence same as \eqref{mugold}. %It is convenient to divide the subblocks in SSE scheme when $L$ is a power of $2$.
Storing such a dictionary matrix will require $N(N+1)^2$ bits. %Since the inner product between any two distinct column takes only three possible values, storing the Gram matrix of gold code dictionary matrix requires only $2(N+1)^4$ bits.
%For $N=127$, this Gold code dictionary matrix size is approximately $2.1$ MB.

\subsection{Mutually Unbiased Bases}
Two orthonormal bases $\mc{U}_1$ and $\mc{U}_2$ of the $N$-dimensional inner product space $\mathbb{C}^N$ are called mutually unbiased if and only if $|\langle\vec{x},\vec{y}\rangle|=\frac{1}{\sqrt{N}}$ for any $\vec{x}\in \mc{U}_1$ and $\vec{y}\in \mc{U}_2$. A collection of orthonormal bases of $\mathbb{C}^N$ is called mutually unbiased if all bases in the collection are pairwise mutually unbiased.
Let $Q(N)$ denote the maximum number of orthonormal bases of $\mathbb{C}^N$, which are pairwise mutually unbiased. In \cite{wootters1989optimal}, it has been shown  that $Q(N) \leq N$ (excluding the standard basis), with equality if $N$ is a prime power. Explicit constructions are also given in \cite{wootters1989optimal} for 
getting $N$ MUB in $N$-dimensional complex vector space $\mathbb{C}^N$, if $N$ is a prime power.


When $N=2^n$, with $n \geq 2$,  the $N$ MUB unitary matrices $\{\vec{U}_1,\cdots,\vec{U}_N\}$ have the following properties. 
\begin{itemize}
\item 

The entries in all the $N$ unitary matrices belong to the set $\{\frac{+1}{\sqrt{N}},\frac{-1}{\sqrt{N}},\frac{+j}{\sqrt{N}},\frac{-j}{\sqrt{N}}\}$. This  follows from the construction of MUB given in \cite{wootters1989optimal}. Storing all these $N$ unitary matrices will require $2 N^3$ bits. 
%For $N=128$, this storage requirement is approximately $4.2$ MB.
\item 

For $N$ up to 512, we find that the inner products  $\langle\vec{x},\vec{y}\rangle$ between $\vec{x}\in \vec{U}_i$ and $\vec{y}\in \vec{U}_m$ for $i \neq m$ is given by
\begin{equation}
    \langle\vec{x},\vec{y}\rangle\in\begin{cases} 
      \{\frac{1}{\sqrt{N}}e^\frac{j m 2\pi}{8}:m=0,...,7\}, & n \text{ is odd,} \\
      \{\frac{1}{\sqrt{N}}e^\frac{j m 2\pi}{4}:m=0,...,3\}, &   n \text{ is even.}
   \end{cases} 
   \label{eq:innerProdsMUB}
\end{equation}
We conjecture that this property holds true when $N$ is any higher power of 2. % higher than 256 as well. 
\end{itemize}


We construct dictionary matrix using $N$ MUB as $\vec{A} = [\vec{U}_1 \cdots \vec{U}_N]$. In this case, $L = N^2$ and the corresponding mutual coherence $\mu$ is $\frac{1}{\sqrt{N}}$. In addition, when $N$ is a power of $2$, we can always partition $\vec{A}$ into $K$ sub-blocks with size of each sub-block $L_k$ being a power of $2$ and each $L_k \geq \frac{N^2}{2K}$. 
%When the entries in $\vec{A}$ belong to $\{+1,-1,+j,-j\}$ except for a common scaling factor of $\frac{1}{\sqrt{N}}$, the inner products $\langle \vec{a}_i,\vec{y} \rangle$ needed in the MAD algorithm involve only additions (no multiplications). 

\section{Decoding Algorithms} \label{sec:deco}


\subsection{Match and Decode Algorithm}

The received signal is modeled as
\begin{eqnarray}
\vec{y} &=& \vec{s} + \vec{v}, \nonumber \\ 
&=& \vec{A}\vec{x} + \vec{v}, \label{eq:obs1}
\end{eqnarray}
where $\vec{v}$ is additive noise. Information bits can be retrieved by recovering the sparse signal $\vec{x}$ from the observation $\vec{y}$. Conventional sparse signal recovery can be done using greedy techniques \cite{mallat1993matching,cai2011orthogonal,tropp2004greed} or convex programming based techniques \cite{chen2001atomic} or iterative message passing techniques \cite{beck2009fast,messageMontanari}. However, with our SPARC encoding, $\vec{x}$ has special structure. The non-zero entries of $\vec{x}$ are from a finite alphabet constellation. In addition, for the SSE scheme, there is exactly one non-zero entry corresponding to each sub-block. Such structures need to be utilized in order to provide good error performance.  Approximate message passing decoders which exploit the structure of the SPARC signal $\vec{x}$ are developed in \cite{hsieh2021modulated,rush2017capacity}, and their sub-block error rate asymptotically (as $N$ and $L$ grow to $\infty$) converges to zero for AWGN channels, for all rates below channel capacity. 


In this paper, we develop a simple greedy decoder, referred as match and decode algorithm, which utilizes the structure in the SPARC signal $\vec{x}$. MAD algorithm for SSE and SFE is described in Algorithm~\ref{mad}. We would like to emphasize that our MAD algorithm does not need to know any noise statistics, such as its variance. MAD algorithm takes the dictionary matrix $\vec{A}$, the observation $\vec{y}$, sparsity level $K$ as inputs and produce an estimate $\hvec{x}\of{K}$ of the sparse signal $\vec{x}$. It is ensured that the estimate $\hvec{x}\of{K}$ (of size $L$) has exactly $K$ non-zero entries from the constellation set $\mc{M}$. Any sparse signal $\hvec{x}$ (of size $L$) with at most $K$ non-zero entries from the set $\mc{M}$ can also be given as partial information to the MAD algorithm. If no partial information is available, $\hvec{x}$ is set as $\vec{0}$.   
%The algorithm proceeds as follows:

\begin{algorithm}
\caption{Match and Decode Algorithm}\label{mad}
\begin{algorithmic}[1]

\State \textbf{Input:} Get the observarion $\vec{y}$, dictionary matrix $\vec{A}$, sparsity level $K$ and any partially recovered sparse signal  $\hvec{x}$ with support $\mc{S}_{\hat{x}}$ with $|\mc{S}_{\hat{x}}| < K$ . 

\State \textbf{Initialize:}  
Initialize the iteration counter $t = |\mc{S}_{\hat{x}}|$, the residual $\vec{r}\of{t} = \vec{y} - \vec{A} \hvec{x}$ and 
the estimate $\hvec{x}\of{t} = \hvec{x}$. 
Let $\hat{\mc{S}}\of{t}$ denote the set of columns in the dictionary matrix discarded by the algorithm (based on the detected ones) until the $t^{th}$ iteration. 
If $\hvec{x}=\vec{0}$, then $\hat{\mc{S}}\of{0} = \emptyset$. For SFE scheme, initialize 
$\hat{\mc{S}_t} = \mc{S}_{\hat{x}}$. For SSE scheme, $\hat{\mc{S}}\of{t} = \cup_{i \in \mc{S}_{\hat{x}}} \vec{A}_{k(i)}$, where $k(i)$ corresponds to the sub-block $k$ which contains $i^{th}$ column of the dictionary matrix $\vec{A}$.  

\State \textbf{Match:} Correlate the residual with the columns of the dictionary matrix and the constellation symbols as given below. 
%For $i \notin \hat{\mc{S}}$, compute
\begin{align}
c_i  &= \langle \vec{r}\of{t} , \vec{a}_i \rangle, ~~ i \in \{1,\cdots,L\} \setminus \hat{\mc{S}}\of{t} \label{eq:metri0} \\
p_{i,m} &= \mathfrak{Real}\{c_i b_m^*\} - \frac{|b_m|^2}{2}, ~~ b_m \in \mc{M} \label{eq:metri}
\end{align}

\State \textbf{Decode:} Detect the active column and the corresponding modulation symbol as, 
%\begin{align*}
$(\hat{i},\hat{m}) = \arg\max_{\substack{i \in \{1,\cdots,L\} \setminus \hat{\mc{S}}\of{t} \\ 1 \leq m \leq M}} p_{i,m}$
%\end{align*}

\State \textbf{Update:} Update the recovered sparse signal information $\hvec{x}\of{t+1} = \hvec{x}\of{t} + b_{\hat{m}} \vec{e}_{\hat{i}}$. (Here $\vec{e}_n$ denotes $n^{th}$ standard basis of size $L$.) Update the residual  
\begin{align}
%\hvec{x}\of{t+1} &= \hvec{x}\of{t} + b_{\hat{m}} \vec{e}_{\hat{i}};  \nonumber \\
\vec{r}\of{t+1} &=   \vec{r}\of{t} -   b_{\hat{m}} \vec{a}_{\hat{i}}. \label{eq:resi} 
%t &= t+1 \nonumber ;
\end{align}
For SFE scheme, update $\hat{\mc{S}}\of{t+1} = \hat{\mc{S}}\of{t} \cup \vec{a}_{\hat{i}}$. For SSE scheme, update $\hat{\mc{S}}\of{t+1} = \hat{\mc{S}}\of{t} \cup \vec{A}_{k(\hat{i})}$, where $k(\hat{i})$ denotes the sub-block $k$ corresponding to the identified column $\hat{i}$. 

\State \textbf{Stopping condition:} Increment the counter $t = t+1$. If $t < K$, repeat the above steps Match, Decode and Update. Else go to Step Ouptut.


\State \textbf{Output:} Recovered sparse signal is $\hvec{x}\of{K}$ and the recovered codeword $\hvec{s} = \vec{A} \hvec{x}\of{K}$. % with support $\hat{\mc{S}}$ and non-zero entries $\hat{\mc{Q}}$.

\end{algorithmic}
\end{algorithm}


Main computationally intensive step in MAD involves computing the correlation between the observation (or residual) and the columns of the dictionary matrix in \eqref{metri0}, which amounts to computing the matrix multiplication $\vec{A}^* \vec{y}$. When $\vec{A}$ is constructed using Gold codes (with scaled entries $\{\pm 1\}$, or power of 2 MUB matrices (with scaled entries $\{ \pm 1, \pm j \}$), the matrix multiplication $\vec{A}^* \vec{y}$ can be simply computed using only additions (and subtractions).  We also note that, the correlation of residual with the columns of dictionary matrix in \eqref{metri0} needs to be computed only for the first iteration. For the subsequent iterations, from \eqref{resi}, 
we have the recursion, $\langle \vec{r}\of{t+1} ,\vec{a}_i \rangle  = \langle \vec{r}\of{t},\vec{a}_i\> \rangle - 
b_{\hat{m}} \langle \vec{a}_{\hat{i}} ,\vec{a}_i \rangle$, where
$b_{\hat{m}}$ and $\hat{i}$ denote the symbol and the active column detected in the previous iteration. 
We can store the symmetric gram matrix $\vec{A}^* \vec{A}$, to get the values of $\langle \vec{a}_{\hat{i}} ,\vec{a}_i \rangle$ needed in the recursion. For power of 2 MUB matrices, based on the conjecture in \secref{dict}, the entries of the gram matrix will be $0$ or $1$ or from the set given in \eqref{innerProdsMUB}.



\subsection{Performance Guarantees of MAD algorithm}

Now, we establish some properties of MAD algorithm for SPARC codes. 

\begin{theorem} \label{thm:ml}
For the AWGN channel, MAD algorithm coincides with the maximum likelihood decoder of SPARC codes, when sparsity $K=1$. 
\end{theorem}
\begin{proof}
Maximum likelihood (ML) detector for AWGN finds the codeword which is the closest to the given observation, among all the possible codewords \cite{madhow2008fundamentals}. For $K=1$ SPARC code, the ML detector outputs the column index $\hat{i}$ and the modulation symbol $\hat{b}$ as
\begin{align*}
(\hat{i},\hat{b}) &= \arg\min_{ b \in \mc{M}, 1 \leq i \leq L } \| \vec{y} - b \vec{a}_i \|^2, \\
&= \arg\min_{ b \in \mc{M}, 1 \leq i \leq L } \|\vec{y}\|^2 - 2 \mathfrak{Real} \{ \langle \vec{y}, b \vec{a}_i \rangle \} + \| b \vec{a}_i \|^2, \\
&= \arg\max_{ b \in \mc{M}, 1 \leq i \leq L } \mathfrak{Real}\{ b^* \langle \vec{y}, \vec{a}_i \rangle \} - \frac{|b|^2}{2},
\end{align*}
since $\|\vec{a}_i\|^2 = 1, \forall i$. Clearly, this ML output coincides with the output of the MAD decoder (without any partial information, that is,  $\hvec{x}=\vec{0}$). 
\end{proof}

Now, we consider the recovery guarantee of the MAD decoder, in the absence of noise. Towards that, we restrict our attention to PSK constellations, $\mc{M} = \{b_1,\cdots,b_M\}$ with $|b_m| = 1, \forall m$. We define the \emph{coherence} of the PSK constellation as 
\begin{align}
\gamma &= \max_{i \neq m} \mathfrak{Real} \{b_i^* b_m \}. \label{eq:const_coh}
\end{align}
Based on the above definition, the coherence $\gamma$ for a constellation can be negative as well. Also, the coherence is not affected when a constant phase $e^{j\theta}$ is multiplied to all the symbols of the constellation. Note that, coherence $\gamma=-1$ for the BPSK constellation $\{1,-1\}$ and coherence $\gamma=0$ for the QPSK constellation $\{1,-1,j,-j\}$ (or any other rotation of the QPSK constellation). It easily follows that, the minimum distance of the PSK constellation 
$d_{\min} = \min_{i \neq m} |b_i - b_m|$ can be written in terms of its coherence as $d_{\min} = \sqrt{2 - 2\gamma}$. 

\begin{theorem}
For SPARC codes with dictinary matrix having mutual coherence $\mu$ and modulation symbols chosen from a PSK constellation having coherence $\gamma$, the MAD decoder recovers the support and modulation symbols perfectly from the noiseless observation, if the following condition is met, 
\begin{align}
K &< \min \left \{ \frac{1+\mu}{2 \mu},  \frac{1+2\mu - \gamma}{2 \mu} \right\}. \label{eq:cond12}
\end{align} 
\end{theorem}

\begin{proof}
Note that, in the first iteration, the correlation values $p_{i,m}$ computed in \eqref{metri} are identical to $p_{i,m} = \mathfrak{Real} \langle \vec{y}, b_m \vec{a_i} \rangle =  \mathfrak{Real} \{ b_m^* \vec{a}^*_i \vec{y} \}$. 
When the above conditions in \eqref{cond12} are met, we want to show that the metric corresponding to the the correct constellation symbol and the correct column (which participated in the linear combination to generate the given codeword as in \eqref{cw1}) will be higher than that of all the incorrect  cases (wrong constellation symbol and/or wrong column).  Without loss of generality (WLOG), let the codeword be generated using the first $K$ columns $\{\vec{a}_1,\cdots,\vec{a}_K\}$ columns of the dictionary matrix. For some specific column $\vec{a}_\ell$ with $1\leq \ell \leq K$, WLOG, let the modulation symbol be $b_1 \in \mc{M}$, such that, the noiseless observation is 
\begin{align*}
\vec{y} &= b_1 \vec{a}_\ell + \sum_{1\leq k \leq K, k \neq \ell} \beta_k \vec{a}_k,
\end{align*}
where $\beta_k$'s are arbitrary modulation symbols from $\mc{M}$.
The metric $p_{\ell,1}$ corresponding to an active column with correct modulation symbol is bounded as
\begin{align}
 p_{\ell,1} &= \mathfrak{Real} \langle b_1 \vec{a}_\ell + \sum_{1\leq k \leq K, k \neq \ell} \beta_k \vec{a}_k, b_1 \vec{a}_\ell \rangle, \nonumber \\
&=  \langle b_1 \vec{a}_\ell ,   b_1 \vec{a}_\ell \rangle + \mathfrak{Real} \langle \sum_{1\leq k \leq K, k \neq \ell} \beta_k \vec{a}_k, b_1 \vec{a}_\ell \rangle, \nonumber \\
&= 1 + \mathfrak{Real} \sum_{1\leq k \leq K, k \neq \ell} b_1^* \beta_k \vec{a}^*_\ell  \vec{a}_k, \nonumber \\
&\geq 1 - (K-1)\mu, \label{eq:cmet}
\end{align} 
since $|\vec{a}^*_\ell  \vec{a}_k|\leq \mu$ and $|b_1|=|\beta_k|=1$. Now, the correlation corresponding to the correct column but wrong modulation symbol $p_{\ell,m}$ with $m \neq 1$ can be bounded as,
\begin{align}
 p_{\ell,m} &= \mathfrak{Real} \langle b_m \vec{a}_\ell + \sum_{1\leq k \leq K, k \neq \ell} \beta_k \vec{a}_k, b_1 \vec{a}_\ell \rangle, \nonumber \\
&= \mathfrak{Real} \{ b_1^* b_m \} + \mathfrak{Real} \sum_{1\leq k \leq K, k \neq \ell} b_1^* \beta_k \vec{a}^*_\ell  \vec{a}_k, \nonumber \\
&\leq \gamma + (K-1)\mu, \label{eq:wmet1}
\end{align}
since $\mathfrak{Real} \{b_1^* b_m\} \leq \gamma$. Similarly, the correlation corresponding to the wrong column $p_{i,m}$ with $i>K$ can be bounded as 
\begin{align}
p_{i,m} \leq K \mu, \forall i>K, \forall m. \label{eq:wmet2}
\end{align}
Since $1\leq \ell \leq K$ is an arbitrary active column, when \eqref{cond12} is met, metric of an active column with correct symbol in \eqref{cmet} will be higher than the metrics of all the incorrect cases \eqref{wmet1} and \eqref{wmet2}. Hence MAD will find the correct column and symbol in the first iteration. After the cancellation of detected column, the problem boils down to detecting $K-1$ active columns and the corresponding symbols. 
Since the number of active columns has decreased ($K-1$ will also be less than the right hand side of the condition in \eqref{cond12}), the subsequent iterations will also be successful. 
\end{proof}
For BPSK and QPSK constellations for which $\gamma \leq 0$, condition in \eqref{cond12} simplifies as $K < \frac{1+\mu}{2\mu}$.  
Interestingly, this recovery condition coincides with that of the orthogonal matching pursuit for $K$-sparse signals \cite{tropp2004greed}. With MUB dictionary matrices, this recovery condition becomes $K < 1+\frac{\sqrt{N}}{2}$. Hence, when the sparsity level is of the order of $\sqrt{N}$, greedy algorithms can give good recovery performance. 

\subsection{Parallel MAD algorithm}
Intuitively, the first iteration of the MAD algorithm is the most error prone, since it faces the \emph{interference} from all the undetected columns. To improve on MAD performance,
we consider a variation, referred as parallel MAD. In the first iteration, we choose $T$ candidates for the active column, by taking the top $T$ metrics \eqref{metri}, and perform MAD decoding for each of these $T$ candidates, resulting in $T$ different estimates for the sparse signal. Among these $T$ estimates, we select the one with the smallest Euclidean distance to the observation, inspired by the ML decoder for white Gaussian noise. The mathematical details are described in Algorithm \ref{pmad} for completeness. The PMAD decoder is similar to parallel greedy search given in \cite{kumar2022parallel} with the notable difference that the alphabet size is discrete and the exact sparsity level is known in the current work.

\begin{algorithm}
\caption{Parallel Match and Decode Algorithm} \label{pmad}
\begin{algorithmic}[1]

\State Given the dictionary matrix $\vec{A}$ and the observation vector $\vec{y}$, 
compute $c_i = \langle \vec{y} , \vec{a}_i \rangle,~i=1,\cdots,L$ and $p_{i,m} =  
\mathfrak{Real}\{c_i b_m^*\} - \frac{|b_m|^2}{2}, ~ b_m \in \mc{M}$.

\State Initialize parallel path index $n=1$; Initialize $\mc{D} = \emptyset$.  

\State Choose a candidate for active column and the corresponding non-zero entry: $(\hat{i}_n,\hat{m}_n) = \arg\max_{(i \notin \mc{D} ,m)} p_{i,m}$.

\State  Run MAD algorithm with inputs $(\vec{A},\vec{y},K)$ and prior information on sparse signal $\hvec{x} = b_{\hat{m}_n} e_{\hat{i}_n}$. 
Denote the recovered sparse signal output of MAD as $\hvec{x}_n$. 

\State Update $\mc{D} = \mc{D} \cup \hat{i}_n$ and $n=n+1$; If $n \leq T$, go back to Step 3.

\State Final output $\bvec{x} = \arg\min_{\hvec{x}_n; 1 \leq n \leq T} \| \vec{y}-\vec{A}\hvec{x}_n\|$.


\end{algorithmic}
\end{algorithm}

\section{Application to Multi-User Channels} \label{sec:mult}
In this Section, we discuss how the SSE and PMAD can be used in multi-user scenarios. Using the SSE scheme with sparsity level $K$ described in \secref{sse}, we can support $P$-user multiple access channel, or $P$-user broadcast channel or $P$-user interference channel \cite{cover1999elements,el2011network}, for any $P \leq K$. First, we illustrate how the SSE scheme can be employed to generate the codeword of each user based on the user's information bits. As before, we partition the dictionary matrix $\vec{A}$ into $K$ sub-blocks, with sub-block $\vec{A}_k$ having $L_k$ number of columns. These $K$ sub-blocks are divided among $P$ users, with $\mc{A}_i = \{\vec{A}_{i,1},\cdots,\vec{A}_{i,K_i}\} \subset \{\vec{A}_1,\cdots,\vec{A}_K\}$ denoting the ordered set of $K_i$ sub-blocks assigned to user-$i$. 
Note that $\mc{A}_i \cap \mc{A}_j = \emptyset$ if $i \neq j$ and $\displaystyle \sum_{i=1}^K K_i = K$. The codeword for user-$i$ is obtained as
\begin{equation}
\vec{s}_i = \sum_{k=1}^{K_i} \beta_{i,k} \vec{a}_{i,k} \label{eq:cwi}
\end{equation}
where symbols $\{\beta_{i,1},\cdots,\beta_{i,K_i}\}$ are chosen from $M_i$-ary constellation and the column $\vec{a}_{i,k}$ is chosen from the sub-block $\vec{A}_{i,k}$, for $1 \leq k \leq K_i$. Denoting the number of columns in $\vec{A}_{i,k}$ as $L_{i,k}$, the total number of bits that can be conveyed for user-$i$ is 
\begin{equation}
N_{b_i} = K_i \log M_i  + \sum_{i=1}^{K_i} \log L_{i,k}. \label{eq:nbi} 
\end{equation}

In the multiple access channel (MAC), which is equivalent to an uplink scenario in a cellular network, the encoding is done independently by each user, which coincides with the SSE based procedure in \eqref{cwi}. The observation at the receiver is 
\begin{eqnarray}
\vec{y} &=& \sum_{i=1}^P \vec{s}_i + \vec{v}  ~=~ \sum_{i=1}^P \sum_{k=1}^{K_i} \beta_{i,k} \vec{a}_{i,k} + \vec{v}. \label{eq:mac}
\end{eqnarray}
The decoding is done jointly at the receiver, which can be done using the MAD or PMAD algorithm, which recovers the support of the active columns $\{\vec{a}_{i,k}\}$ and the corresponding modulation symbols $\beta_{i,k}$, for each user. %, using the received signal $\vec{y}$ in \eqref{mac}. 
%In case of fading channels, if the group of $P$ users are chosen such that their fading gains are similar, then this fading scenario can be approximated as the AWGN model \eqref{mac}.
% have similar fading gains,  
%Now, considering the case where user-$i$ has channel gain $h_i$, the received signal is
%\begin{equation}
%\vec{y} = \sum_{i=1}^P h_i g_i \vec{s}_i + \vec{v}, \label{eq:mac1}
%\end{equation}
%where $g_i$ denotes the transmit gain (power control) employed by user-$i$. If the gains $g_i$ are chosen such that $h_i g_i = 1, \forall i$, then the AWGN performance of the above MAC model \eqref{mac1} will coincide with the AWGN performance of the corresponding single-user case. This implies that, if we group the users such that $|h_i|$ are (approximately) equal, then power control gains $|g_i|$ can be (approximately) the same. Hence, in our SSE based scheme for MAC, grouping users with similar channel gains is beneficial.  This is in contrast with the power-domain NOMA techniques in the literature \cite{saito2013non}, where a high channel gain user is typically grouped with a low channel gain user. 
%If different users have different channel gains, different power constraints, and different rate requirements, then there are open issues in the SSC based scheme, such as segmenting the dictionary matrix among users, choosing the constellation size for each user, and the modifications required for the MAD decoding. 

In the broadcast channel, which is similar to the downlink scenario in a cellular network, encoding is done jointly at the base station and the decoding is done by each user separately. %After the dictionary matrix is partitioned and the sub-blocks are allotted among the users, the codeword for each user can be obtained based on the corresponding information bits as given by \eqref{cwi}, and 
The transmitter sends the sum of all the users' codewords as %\begin{equation}
$\vec{s} = \sum_{i=1}^P \vec{s}_i$. %\label{eq:spc}
%\end{equation}
%Note that the above sum \eqref{spc} resembles the codeword generation of single-user scenario \secref{ssc}. 
Received signal at the user-$i$ is given by %\begin{equation}
$\vec{y}_i = \vec{s} + \vec{n_i}$,
%\end{equation}
where $\vec{n}_i$ is the noise at the user-$i$. % with variance $\sigma_i^2$. 
MAD or PMAD decoding can be employed by each user, which recovers the active columns present in $\vec{s}$ and the corresponding modulation symbols. Hence, in this approach, users recover the information sent to the other users, in addition to their own information. If the users are grouped such that their noise statistics are similar, then their error performance will be similar. We also note that SSE can be applied in two-way relay channel, which has a multiple access phase followed by a broadcast phase. 
%The user with the highest noise variance ($\arg\max_i \sigma_i^2$) will have the worst error performance (assuming the noise distributions are the same except for the variance). On the other hand, if all the users have same noise variance, the performance (the probability that all the users received all their bits correctly) will coincide with the corresponding single-user scenario (with the same noise variance). 

In the interference channel, there are $P$ transmitters and $P$ receivers. Each transmitter sends information to a corresponding intended receiver. With $i^{th}$ transmitter generating codeword as in \eqref{cwi}, the received signal at the $i^{th}$ receiver is given by
\begin{equation}
\vec{y}_i = \vec{s}_i + \sum_{j \neq i} h_{i,j} \vec{s}_j + \vec{n}_i, \label{eq:ic}
\end{equation}
where $h_{i,j}$ denotes the channel gain from $j^{th}$ transmitter to the $i^{th}$ receiver.  Without loss of generality, we have taken $h_{i,i} = 1$. 
MAD or PMAD decoding employed at the $i^{th}$ receiver recovers the codewords of all the transmitters. If $|h_{i,j}| = 1, \forall i,j$, and the noise statistics are identical across all the receivers, then the decoding performance (successful recovery of all the codewords) of all the receivers will coincide with the corresponding single user case. 

 
%In the \emph{strong interference} $|h_{i,j}| \gg 1$ regime \cite{cover1999elements,el2011network}, MAD decoding can be modified to first decode the messages of all the interfering users (by restricting the correlations to the subblocks of interfering users), cancel the interference, and then proceed to find the active columns in the subblocks of the intended user. In the \emph{weak interference} $|h_{i,j}| \gg 1$ regime \cite{cover1999elements,el2011network}, MAD decoder can first find the message of the intended user directly by restricting the correlations to the subblocks of the intended user. 
% Detailed study of  the SSC-MAD based techniques in the strong and weak interference regimes  can be explored in a future work.

%If the users' channel quality is asymmetric, and there is private information for good channel quality users (which the low channel quality users should not be able to decode), then developing sparse coding based techniques is an open problem. 
%\textr{Comment on the connections/differences to the index modulation for broadcast channel work by E Viterbo.}



\section{Simulation Results} \label{sec:simu}
We study the performance in terms of the block error rate (BLER), also referred as codeword error rate, for the proposed encoding and decoding schemes in additive white Gaussian noise channels. For the complex MUB dictionary matrix, the non-zero entries of the sparse signal are chosen from the QPSK constellation. For real Gold code dictionary matrix, we consider BPSK constellation. When the non-zero entries in the $K$-sparse signal $\vec{x}$ are uncorrelated, it easily follows that the expected energy of the codeword $\vec{s}=\vec{A}\vec{x}$ is $E_s = K$, with the columns of dictionary matrix being unit norm. Energy per bit $E_b$ is obtained by dividing $E_s$ by the total number of bits $N_b$ conveyed by the sparse signal $\vec{x}$. With $\frac{N_0}{2}$ denoting the variance of the Gaussian noise per real dimension, we study the BLER versus $E_b/N_0$ (in dB) of the proposed schemes.

An error control code conveying $N_b$ bits using $N$ real channel uses is represented by the pair $(N,N_b)$, with the code rate of $\frac{N_b}{N}$ bits per real channel use. A complex code of length $N$ can be represented by a real code of length $2N$ by concatenating real and imaginary part of the code.%, in order to compare it with real values codes. 
In this paper, a complex code of length $N$ supporting $N_b$ bits of information is equivalent to a $(2N,N_b)$ real code, with code rate of $\frac{N_b}{2N}$ bits per real channel use.

\subsubsection{MAD vs. OMP} Orthogonal Matching Pursuit (OMP) decoder is a well studied greedy decoder \cite{tropp2004greed} for sparse signal recovery, which is similar in computational cost to that of MAD decoder.  In Figure~\ref{fig:MADvsOMP}, we compare the BLER performance of both decoders for complex MUB dictionary of size $64\times4096$ and sparsity $K=6$ giving rise to a $(128,68)$ SSE-GSPARC code. 

We run the OMP algorithm for $K$ iterations and quantize the non-zero entries of the recovered $K$-sparse signal (obtained using least squares method) to the nearest nearest constellation points.  On the other hand, MAD algorithm utilizes the finite alphabet size of the non-zero entries in every iteration, by jointly decoding the active column and the corresponding constellation point. In addition, when OMP projects the residuals onto the orthogonal complement of the detected columns, there will be a reduction of signal components from the yet-to-be detected active columns. On the other hand, MAD simply subtracts out the detected columns without affecting the yet-to-be detected active columns. Due to these reasons, the proposed MAD decoder provides better BLER performance than the OMP algorithm.

In addition to the standard QPSK constellation $\{+1,-1,+j,-j\}$, we also consider offset QPSK constellations. Specifically, the modulating symbol for $k^{th}$ sub-block for $k\in\{1,...,K\}$ is chosen from a rotated QPSK constellation, obtained by counter-clockwise rotation of the standard constellation by $\displaystyle \frac{(k-1)\pi}{2K}$ radians. The motivation for introducing phase offset to different sub-blocks is based on the following reasoning. With $i$ being an index of one of the active columns from the sparse signal support set $\mc{S}$, consider the inner product $\langle \vec{y},\vec{a}_i \rangle = \beta_i + \sum_{k \in \mc{S}, k \neq i} \beta_k \langle \vec{a}_k,\vec{a}_i \rangle  + \langle \vec{v},\vec{a}_i\rangle $. MAD decoder is prone to error when the net interference from other active columns has high magnitude. For the complex MUB dictionary matrix with $N=64$, from \eqref{innerProdsMUB}, the inner product between any two non-orthogonal columns belong to the set $\{\frac{+1}{\sqrt{N}},\frac{-1}{\sqrt{N}},\frac{+j}{\sqrt{N}},\frac{-j}{\sqrt{N}}\}$. Due to this property, there are many possible support sets $\mc{S}$, for which the interference terms can add coherently to result in a high magnitude. To mitigate this constructive addition of interfering terms, we introduce a phase offset to each sub-block of the dictionary matrix. From the results in Fig.~\ref{fig:MADvsOMP}, we see that MAD algorithm performs better with offset QPSK constellations. In all the remaining plots for SSE schemes with complex MUB dictionary matrix, we have used offset QPSK as the default modulation scheme. 

%Two types of QPSK modulation are studied for both of the decoders. In the first type standard QPSK symbols $\{+1,-1,+i,-i\}$ are used as non-zero entries of the sparse vector $\vec{x}$ and we observe that MAD decoder performs better than OMP decoder at lower $E_b/N_0$ values but the gain vanishes at higher $E_b/N_0$ values. In the second type we use offset QPSK symbols where we see that MAD performs consistently better than OMP over all $E_b/N_0$ values. The offset QPSK symbols are defined as follows: for the $k^{th}$ block, the corresponding non-zero entries are taken from a standard QPSK constellation rotated counterclockwise by $k\frac{\pi/2}{K}$ radians, where $k\in\{0,1,...,K-1\}$. For standard QPSK symbols, at any given column, the interference from all active columns can add up in same direction leading rise to higher block error rate. In case of offset QPSK symbols, the interference from all active columns are designed to be in different directions leading to a reduction in block error rate. In the remaining plots with complex MUB dictionary matrix we have used offset QPSK as the default modulation scheme for SSE-GSPARC simulations. Because of it's superior performance at lower computational cost, we have selected MAD decoder as our base decoder over which we build on using parallel MAD decoders. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{modulatedSPARC_64MUB_K6_OMP_vs_MAD___and___std_vs_offset_QPSK.eps}
    %\caption{Comparison of OMP and MAD decoder for SSE-GSPARC with and without offset QPSK. MUB dictionary matrix of size $64\times4096$ is used with $K=6$ to generate $(128,68)$ code.}
    \caption{Comparison of OMP and MAD decoder for SSE schemes.}  
    \label{fig:MADvsOMP}
\end{figure}



\subsubsection{MAD vs. PMAD} The performance of MAD decoder can be improved by running multiple MAD decoders in parallel and selecting the best solution based on minimum distance decoding rule. In Figure~\ref{fig:MADvsPMAD}, we compare MAD decoder with PMAD decoder, for different number of parallel paths. The simulation parameters are the same as in Fig.~\ref{fig:MADvsOMP}, with offset QPSK constellations. We denote PMAD with $T$ parallel paths as $T-$PMAD. We observe that the PMAD improves the BLER performance of MAD decoder significantly. 16-PMAD and 100-PMAD has roughly 4.5 dB and 5 dB gain respectively over MAD decoder for BLER of $10^{-4}$. This shows that the gains from parallel search starts to diminish as we increase the number of parallel paths. This allows us to use a small number of parallel paths for our PMAD decoders.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{modulatedSPARC_PMAD_interleaved64MUB_K6_offsetQPSK_effect_of_parallel_paths.eps}
    %\caption{Effects of number of parallel paths on BLER performance of PMAD decoder.  MUB dictionary matrix of size $64\times4096$ is used with $K=6$ to generate $(128,68)$ code.}
    \caption{Effects of number of parallel paths on the BLER of PMAD decoder.}
    \label{fig:MADvsPMAD}
\end{figure}

\subsubsection{PMAD vs. AMP} Approximate Message Passing decoders have been developed for standard SPARC and modulated SPARC for random Gaussian dictionary matrices \cite{hsieh2021modulated} and have been shown empirically to work with other dictionary matrices. In Figure~\ref{fig:PMADvsAMP}, we compare the BLER performances of PMAD decoder with online AMP decoder \cite{hsieh2021modulated}. %\textr{Can you also include $K=6$, $K=8$ in the legends? Without that information, it is misleading that a higher code rate (128,68) has better BLER than lower code rate (128,64).} 
We use equal power allocation for all the sub-blocks, because the power allocation techniques to improve the performance of AMP decoders do not work  in the small code length $(N\leq128)$ and low code rate $(R\approx0.5)$ regime  \cite{greig2017techniques,liang2021finite}. 
%In order to make a fair comparison between the two decoders,
We compare PMAD algorithm with $T$ parallel paths with the AMP with $T$ iterations, which is referred as $T-$AMP in the plot. The AMP algorithm \cite{hsieh2021modulated} computes non-linear MMSE estimate of each entry of the sparse signal $\vec{x}$ in each iteration. We note that the $T-$PMAD requires significantly less computations than $T-$AMP. 

We consider two scenarios, one with equal sub-block sizes and the other with unequal size sub-blocks. A complex MUB dictionary matrix of size $64\times512$ with $K=8$ has equal size sub-blocks, resulting in a $(128,64)$ code.   With complex MUB dictionary matrix of size $64\times4096$, running the sub-block partitioning algorithm from \secref{spa} with $K=6$, we get a $(128,68)$ code with unequal sub-block sizes. Since AMP in \cite{hsieh2021modulated} is designed for equal size sub-blocks, we use a generalization of the AMP to accommodate unequal sub-block sizes. From Fig.~\ref{fig:PMADvsAMP}, we find that $T$-PMAD performs better than $T$-AMP, in the short block length regime. For the unequal size sub-blocks, AMP performs poorly for large values of $E_b/N_0$. However, PMAD algorithm works well for both equal and unequal sub-block sizes. We also note that, the lower sparsity case $K=6$ with code rate $\frac{68}{128}$ performs better than the higher sparsity case $K=8$ with code rate $0.5$, emphasizing that sparsity is a key parameter for SPARC.     

%For $(128,64)$ code the sub-blocks are of equal size, as required by the modulated SPARC architecture and we observe that the PMAD performs slightly better than AMP. % at smaller values of $E_b/N_o$ and the gap in performance increases slightly at higher values of $E_b/N_o$. For $(128,68)$ code, the sub-blocks are of unequal sizes and we use a generalization of AMP given in \cite{hsieh2021modulated} to accommodate for different section sizes and we find that PMAD is marginally better than AMP at lower values of $E_b/N_o$ and significantly better at higher values of $E_b/N_o$. AMP decoders work better when the sub-block sizes are small and that makes AMP decoder for $(128,64)$ case closer to PMAD decoder than the $(128,68)$ case.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{modulatedSPARC_interleaved64MUB_K6_offsetQPSK_comparison_of_decoders.eps}
    %\caption{Comparison of PMAD decoder with online-AMP decoder. MUB dictionary matrix of size $64\times4096$ is used with $K=6$ to generate $(128,68)$ code and MUB dictionary matrix of size $64\times512$ is used with $K=8$ to generate $(128,64)$ code. }
    \caption{Comparison of PMAD decoder with online-AMP decoder.}
    \label{fig:PMADvsAMP}
\end{figure}


\subsubsection{SSE vs. SFE} In Figure~\ref{fig:SSEvsSFE}, we study the BLER performance of the encoding schemes with and without sub-block structure, for a complex MUB dictionary matrix of size $64\times4096$, using 100-PMAD decoder. SSE with $K=5$ and $K=6$ gives $(128,58)$ and $(128,68)$ codes respectively, while SFE with $K=5$ gives $(128,63)$ code. Since SFE transmits more bits than SSE for the same sparsity $K$, the noise level in SFE will be smaller than that of the SSE scheme. On the other hand, the search space for each iteration of the greedy decoder for SFE will be larger than that of the SSE scheme. Due to these counteracting effects, SFE has nearly same BLER performance as SSE, at high $E_b/N_0$ values, while achieving higher code rate.  

%For $K=5$ both SSE and SFE codes work better than $K=6$ SSE code, which can be explained by the fact that higher sparsity level leads to poorer recovery performance. When comparing SSE and SFE for $K=5$, we observe that SSE performs better than SFE at lower values of $E_b/N_o$, similar to SFE for moderate values of $E_b/N_o$ and possibly better than SFE at higher values of $E_b/N_o$. Since SFE can transmit more bits, the noise level in SFE is lower than that of SSE at the same $E_b/N_o$, however at the same noise level SSE has smaller search space for active column than that of SFE at any iteration of a MAD decoder. The interplay between these two opposing trends results in the error behaviour observed in SSE-GSPARC and SFE-GSPARC in Fig. \ref{fig:SSEvsSFE}. \textr{Explanation is not sound.}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{SSE_vs_SFE_modulatedSPARC_100PMAD_64MUB_QPSK.eps}
    %\caption{Comparison of error performance of SSE-GSPARC and SFE-GSPARC. MUB dictionary matrix of size $64\times4096$ is used with $K=5$ to generate $(128,58)$ SSE code and $(128,63)$ SFE code. $K=6$ is used to generate $(128,68)$ SSE code}
    \caption{Comparison of BLER performance of SSE and SFE schemes.}
    \label{fig:SSEvsSFE}
\end{figure}

\subsubsection{Very short length codes} In Figure~\ref{fig:veryShort}, we GSPARC using MAD/PMAD decoding for very short lengths, with $(20,11)$ and $(20,8)$ Golay codes considered for 5G-NR \cite{van2018short}, using ML decoding. Complex MUB dictionary matrix of size $8\times64$ with $K=1$ gives $(16,8)$ code, for which MAD decoder is used. Complex MUB dictionary matrix of size $16 \times 257$ with SFE $K=2$ scheme gives $(32,19)$ code, for which 16-PMAD decoder is used. We find that GSPARC codes give comparable performance to Golay codes of very short lengths.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{very_short_length_codes_comparisons.eps}
    %\caption{Comparison of BLER performance of very short length codes. MUB dictionary matrix of size $8\times64$ is used with $K=1$ to generate SSE $(16,8)$ code and MUB dictionary matrix of size $16\times257$ is used with $K=2$ to generate SFE $(32,19)$ code.}
    \caption{Comparison of BLER performance of very short length codes.}
    \label{fig:veryShort}
\end{figure}

\subsubsection{Short length codes} In Figure~\ref{fig:shortcodes}, we compare our $(127,63)$ SSE scheme (Gold code dictionary matrix of size $127\times128^2$ with $K=5$) with some of the existing $(128,64)$ error control codes \cite{cocskun2019efficient}: binary LDPC codes used in the CCSDS standard,  LDPC codes (base graph 2) considered for 5G-NR standard and Turbo code with 16 states.  More details about these existing codes are given in \cite{cocskun2019efficient}. SSE with PMAD decoder performs better than LDPC code from the CCSDS standard. We also note that some codes like tail-biting convolutional code  with constraint length 14 \cite{cocskun2019efficient} and polarization adjusted convolutional codes \cite{arikan2019sequential} perform very close to sphere packing bounds (shown in Fig.~\ref{fig:shortcodes} with legend 'SPB') for the given code length and code rate.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{short_length_codes_comparisons.eps}
    %\caption{Comparison of $(127,63)$ SSE code formed by Gold code dictionary matrix of size $127\times128^2$ and $K=5$ with some of standard codes with same code-lengths and code-rate and similar performances}
    \caption{Comparison with existing $(128,64)$ codes.}
    \label{fig:shortcodes}
\end{figure}

\subsubsection{Multi-user channels} As explained in \secref{mult}, SSE with sparsity $K$ can support up to $K$ users in multi-user channels. For illustration, we consider a multiple-access channel \eqref{mac}. An SSE scheme with sparsity level $K$ resulting in a $(N_1,N_b)$ code  utilizes $N_1$ real channel uses and communicates either $\displaystyle \lfloor N_b/K \rfloor$ or $\displaystyle \lceil N_b/K \rceil$ bits from each user, based on the optimal sub-block partitioning algorithm from \secref{spa}. For comparison, we consider a $K$-user orthogonal multiple access scheme, where each user is assigned a dedicated time/frequency resource and each user employs a single user Golay code with approximate parameters $(N_1/K, N_b/K)$. We also find the lower bound for the $K$-user orthogonal multiple access using sphere packing bounds for code parameters $(N_1/K, N_b/K)$. In Figure~\ref{fig:multiuser}, we study the probability that at least one user is decoded in error. For $K=6$ users, using SSE with Gold code dictionary matrix resulting in $(127,74)$ code (communicating 12 or 13 bits for each user) outperforms the sphere packing bounds of the orthogonal multiple access scheme using $(23,12)$ codes, and also provides higher spectral efficiency. Similar results hold true for MUB dictionary matrices as well. We see that, SSE with PMAD provides a multi-user error control coding scheme, offering significant gains over orthogonal multiple access schemes, for short block lengths. The gains can be understood from the fact that SSE encodes the information from the users over block length of $N_1$, while orthogonal multiple access schemes use codes of smaller block lengths $N_1/K$. SSE provides a neat way of pooling the resources of users together such that the overall error performance of all the users is improved. 


%In Fig. \ref{fig:multiuser} we compare multi-user performance of SSE-GSPARC with concatenated $(23,12)$ Golay codes. Probability of at least one user in error is used as a metric for comparison of differnt schemes. We have used Golay codes with ML decoding because they give best error  performance for given code length and code rate. Concatenation of Golay codes makes sure that the different users are orthogonal in time-domain. SSE-GSPARC with $K$ active columns can be used for $K-$user communication. For 5 users, we have compared $(127,63)$ SSE-GPARC code formed by Gold code dictionary matrix of size $127\times128^2$  with $(23,12)$ Golay codes concatenated 5 times and also with concatenated theoretical $(23,12)$ code achieving the Sphere-packing bound \cite{shannon1959probability}. Similarly for 6 users, we have compared $(128,68)$ SSE-GSPARC code formed by complex MUB matrix of size $64\times4096$ and $(127,74)$ SSE-GSPARC code formed by Gold code matrix of size $127\times128^2$ with $(23,12)$ Golay codes concatenated 6 times and also with concatenated theoretical $(23,12)$ code achieving the Sphere-packing bound. We observe that SSE-GSPARC not only outperform concatenated Golay codes in but also concatenated theoretical codes that achieve sphere packing bound. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{multiuser_performance.eps}
    %\caption{Comparison of error performance of SSE GSPARC with concatenated Golay codes in multi-user coding scenario for very short code-lengths}
    \caption{Comparison of BLER performance in the multiple access channel.}
    \label{fig:multiuser}
\end{figure}



\section{Conclusions} \label{sec:conc}
In this paper, we developed two generalizations of SPARC, an SSE scheme, which allows unequal sub-block sizes and an SFE scheme, which eliminates the sub-block structure altogether. For both SSE and SFE schemes, we developed a greedy approach based decoder, referred as MAD algorithm and introduced a parallel greedy search mechanism to improve its performance. Using Gold codes and mutually unbiased bases to construct the dictionary matrices, we study block error rate performance in AWGN channels, for short block lengths.  We showed that our proposed PMAD outperforms the AMP decoder for SPARC and performs comparably and competitively with widely used codes. We also described that SSE scheme can be used in various multi-user channel settings. In multiple access channels, we showed that SSE with PMAD decoder outperforms the sphere packing lower bounds of an orthogonal multiple access scheme, having the same spectral efficiency. Developing greedy decoders for GSPARC which work well for moderate to large block lengths can be explored in a future work. %Developing suitable decoders for GSPARC in multi-antenna fading channels can also be explored. 
Studying GSPARC in multi-user channels with asymmetric power and rate conditions can be explored in the future. 

\appendix

\subsection{Indexing the ordered set of $K$ objects out of $L$ objects} \label{app:index_comb}

Given $L$, $K$ and $d$, our goal is to find a good estimate for $i$, which satisfies the condition \eqref{require}. The condition can be rewritten as 
\begin{align*}
    \binom{L}{K}\left(1-\frac{\binom{L-i}{K}}{\binom{L}{K}}\right)
    & \leq d
    <\binom{L}{K}\left(1-\frac{\binom{L-(i+1)}{K}}{\binom{L}{K}}\right), \\
    \implies
    \frac{\binom{L-(i+1)}{K}}{\binom{L}{K}}
    &< 1-\frac{d}{\binom{L}{K}}
    \leq \frac{\binom{L-i}{K}}{\binom{L}{K}}.
\end{align*}
    
%\begin{equation}
%\implies
%    1-\frac{\binom{n-i}{k}}{\binom{n}{k}}
%    \leq \frac{d}{\binom{n}{k}}
%    < 1-\frac{\binom{n-(i+1)}{k}}{\binom{n}{k}}
%\end{equation} 
%Now we try to bound $\frac{\binom{n-m}{k}}{\binom{n}{k}}$ using following inequality for its factors:
First, noting that
\begin{align}
 \frac{L-m-p}{L-p} &= 1-\frac{m}{L-p} ~~ \leq 1-\frac{m}{L} ~~= \frac{L-m}{L},
   \label{eq:RatioBound}
\end{align}
for $p\in\{0,1,...,K-1\}$, we get the following bounds
\begin{align}
     \left(\frac{L-(m+1)-(K-1)}{L-(K-1)}\right)^K  
     & \leq \frac{\binom{L-(m+1)}{K}}{\binom{L}{K}},  \nonumber \\
     \frac{\binom{L-m}{K}}{\binom{L}{K}}
     &\leq \left(\frac{L-m}{L}\right)^K  \label{eq:Bound1}
\end{align}
%    \implies
%   
%    < 1-\frac{d}{\binom{n}{k}}
%    \leq \frac{\binom{n-i}{k}}{\binom{n}{k}}
%    \leq \left(\frac{n-i}{n}\right)^k
%    \label{eq:Bound1}
%\end{equation}
%Let us take a closer look at the terms of the form $\frac{\binom{L-m}{K}}{\binom{L}{K}}$, which appears in the above inequalities. 
Setting $\Bar{L}=L-\frac{K-1}{2}$, we have
\begin{align*}
    \frac{\binom{L-m}{K}}{\binom{L}{K}}
    % &= \frac{(L-m)(n-m-1)\cdots(n-m-(k-1))}{n(n-1)\cdots(n-(k-1))}\\
    &= \frac{(\Bar{L}+\frac{K-1}{2}-m) %(\Bar{L}+\frac{K-1}{2}-m-1) 
    \cdots(\Bar{L}+\frac{K-1}{2}-m-(K-1))}{(\Bar{L}+\frac{K-1}{2})
    %(\Bar{L}+\frac{K-1}{2}-1)
    \cdots(\Bar{L}+\frac{K-1}{2}-(K-1))} \\
    &= \left\{ \begin{array}{ll} 
        \frac{\Bar{L}-m}{\Bar{L}}\times\prod_{p=1}^{\frac{K-1}{2}}\frac{(\Bar{L}-m)^2-p^2}{\Bar{L}^2-p^2} & \text{for odd $K$,} \\
        \prod_{p=1}^{\frac{K}{2}}\frac{(\Bar{L}-m)^2-\left(\frac{2p-1}{2}\right)^2}{\Bar{L}^2-\left(\frac{2p-1}{2}\right)^2} &
        \text{for even $K$.} \end{array} \right.       
\end{align*}

%\textbf{for odd $k$:}
%\begin{equation}
%        \frac{(n-m)(n-m-1)\cdots(n-m-(k-1))}{n(n-1)\cdots(n-(k-1))} =
%        \frac{\Bar{n}-m}{\Bar{n}}\times\prod_{p=1}^{\frac{k-1}{2}}\frac{(\Bar{n}-m)^2-p^2}{\Bar{n}^2-p^2}
%\end{equation}

%\textbf{for even $k$:}
%\begin{equation}
%    \frac{(n-m)(n-m-1)\cdots(n-m-(k-1))}{n(n-1)\cdots(n-(k-1))} =
%    \prod_{p=1}^{\frac{k}{2}}\frac{(\Bar{n}-m)^2-\left(\frac{2p-1}{2}\right)^2}{\Bar{n}^2-\left(\frac{2p-1}{2}\right)^2}
%\end{equation}
For both odd and even $K$, we have
\begin{equation}
 \left(\frac{(\Bar{L}-m)^2-\left(\frac{K-1}{2}\right)^2}{\Bar{L}^2-\left(\frac{K-1}{2}\right)^2}\right)^\frac{K}{2}
  \leq
  \frac{\binom{L-m}{K}}{\binom{L}{K}}
  \leq\left(\frac{\Bar{L}-m}{\Bar{L}}\right)^K
\end{equation}
Following the arguments of equation \ref{eq:RatioBound}, it is easy to show the following inequality:
\begin{align}
&\left(\frac{L-m-(L-1)}{L-(K-1)}\right)^K
\leq \left(\frac{(\Bar{L}-m)^2-\left(\frac{K-1}{2}\right)^2}{\Bar{L}^2-\left(\frac{K-1}{2}\right)^2}\right)^\frac{K}{2} \nonumber \\
&~~~~~\leq \frac{\binom{L-m}{K}}{\binom{L}{K}} 
\leq \left(\frac{\Bar{L}-m}{\Bar{L}}\right)^K
\leq \left(\frac{L-m}{L}\right)^K.
\label{eq:Bound2}
\end{align}
Combining inequalities from \eqref{Bound1} and \eqref{Bound2}, we get 
\begin{equation*}
    \left(\frac{(\Bar{L}-(i+1))^2-\left(\frac{K-1}{2}\right)^2}{\Bar{L}^2-\left(\frac{K-1}{2}\right)^2}\right)^\frac{K}{2}
    < 1-\frac{d}{\binom{L}{K}}
    \leq\left(\frac{\Bar{L}-i}{\Bar{L}}\right)^K,
\end{equation*}
from which, %while solving for $i$ gives %the lower bound 
%\begin{equation}
%i >    \Bar{L}-1-\sqrt{\left(1-\frac{d}{\binom{L}{K}}\right)^\frac{2}{K}\left(\Bar{L}^2-\left(\frac{L-1}{2}\right)^2\right)+\left(\frac{K-1}{2}\right)^2},
%    < i
%    \leq \Bar{n}\left[1-\left(1-\frac{d}{\binom{n}{k}}\right)^\frac{1}{k}\right]
%\end{equation}
%and 
we get the upper bound for $i$ in \eqref{ibar}. %We choose to use to work with the upper bound of $i$, due to its simplicity of the expression. We find that the upper bound is quite close to the actual value. Also, lower bound for $i$ can be negative for small values of $d$, which does not give any new information. 
%Now since the index can take only integer values, we have the following inequality
%\begin{equation}
%    \hat{i}\leq\bigg\lfloor
%    \left(n-\frac{k-1}{2}\right)\left\{
%    1-\left(1-\frac{d}{\binom{n}{k}}\right)^\frac{1}{k}\right\}\bigg\rfloor
%\end{equation}

% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)




%\printbibliography


\bibliographystyle{ieeetr}
\bibliography{ref}


% that's all folks
\end{document}


