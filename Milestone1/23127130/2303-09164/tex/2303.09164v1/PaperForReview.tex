% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers}  % HFUT-CVers Team at the 5th ABAW Competition:

\author{Jia Li\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small jiali@hfut.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Yin Chen\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small chenyin@mail.hfut.edu.cn}
\and
Xuesong Zhang\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small 2022030042@mail.hfut.edu.cn}
\and
Jiantao Nie\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small 2022111086@mail.hfut.edu.cn}
\and
Yangchen Yu\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small 2019212292@mail.hfut.edu.cn}
\and
Ziqiang Li\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small luoyixuan1029@gmail.com}
\and
Meng Wang\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small eric.mengwang@gmail.com}
\and
Richang Hong\\
Hefei University of Technology\\
Hefei, China\\
{\tt\small hongrc.hfut@gmail.com}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
In this paper, we present our solutions to the two sub-challenges of Affective Behavior Analysis in the wild (ABAW) 2023: the Emotional Reaction Intensity (ERI) Estimation Challenge and Expression (Expr) Classification Challenge. ABAW 2023 focuses on the problem of affective behavior analysis in the wild, with the goal of creating machines and robots that have the ability to understand human feelings, emotions and behaviors, which can effectively contribute to the advent of a more intelligent future. In our work, we use different models and tools for the Hume-Reaction dataset to extract features of various aspects, such as audio features, video features, etc. By analyzing, combining, and studying these multimodal features, we effectively improve the accuracy of the model for multimodal sentiment prediction. For the Emotional Reaction Intensity (ERI) Estimation Challenge, our method shows excellent results with a Pearson coefficient on the validation dataset,  exceeds the baseline method by 84 percent.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Emotions are psychological states that arise in human beings when stimulated by the external environment, and they are able to reflect a person's current physiological and psychological state, making an impact on people's perception of things and future decisions. Emotions are often accompanied by distinct physiological characteristics, which can be analyzed to identify them. In this paper, we present in detail our solution to the Emotional Reaction Intensity (ERI) Estimation Challenge.

In the Emotional Reaction Intensity (ERI) Estimation Challenge, each participant subjectively annotated his or her intensity of 7 emotional experiences\cite{christ2022muse}, including adoration, amusement, anxiety, disgust, empathic pain, fear, and surprise. We needed to predict the intensity of the participants' 7 emotional experiences caused by watching the video. Participants tend to express their emotions through their voices, body movements, and expressions, and it is through these factors that multimodal emotion recognition can identify and predict emotions. Traditional emotion recognition is based on a single modality, however, when a single factor is affected, for example, when facial expressions are not clear due to sunlight or when voices are masked by ambient sounds, the effectiveness of emotion prediction is significantly reduced. A reasonable combination of several factors can ensure a high accuracy of prediction results even in unstable environments and improve the robustness of the program.

    Our contributions can be summarized as follows.
    
1. PosterV2-Vit feature: To acquire advanced visual features, we trained a Vision Transformer (ViT)\cite{dosovitskiy2020image} model on the AffectNet7 dataset\cite{mollahosseini2017affectnet} using unsupervised learning techniques\cite{he2022masked}. To further augment its feature representation capacity, we combined the PosterV2\cite{mao2023poster} features with ViT to obtain a novel PosterV2-ViT feature. In comparison to other existing methods, our proposed approach delivers superior performance. By using this new method, we have achieved a significant breakthrough in enhancing the feature extraction ability of our visual model.

2. Modality interaction module. To fully harness the potential of audio and video features, we incorporated a modal interaction module to effectively extract and capture the common relationships between these two modalities. This integration enhanced the performance of our model, demonstrating its efficacy and robustness in capturing cross-modal interactions. 

%-------------------------------------------------------------------------
\section{Methodology}
\label{sec:meth}
\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.8\linewidth]{Model.pdf}
   \caption{The framework of our network architecture.}
   \label{fig:fram}
\end{figure*}
%------------------------------------------------------------------------
\section{Related Work}
\label{sec:rel}
In this section, we will introduce some related works about emotion recognition and emotional reaction intensity.
\subsection{Emotion Recognition}
Emotion recognition research has primarily focused on the classification of basic emotions, including positive, negative, and neutral states. However, there have been recent efforts\cite{kosti2017emotion,khattak2022efficient,siddiqui2022robust} to differentiate more nuanced emotional categories such as anger, happiness, and sadness.
\subsection{Emotional Reaction Intensity}
 Building upon emotion recognition, some studies attempt to estimate the intensity or degree of emotional reactions. Emotional Reaction Intensity Estimation Challenge was presented in MuSe 2022\cite{amiriparian2022muse}. The objective of this challenge was to assess the magnitude of response elicited by seven distinct expression types, encompassing adoration, amusement, anxiety, disgust, empathic-pain, fear and surprise, with intensity values between 0 to 100. The TEMMA model proposed by Li\cite{li2022hybrid} at Muse 2022 won the championship in this challenge.

%------------------------------------------------------------------------

Figure \ref{fig:fram} illustrates the overall framework of our network architecture, which consists of three main branches: an audio feature encoding module, a visual feature encoding module, and an audio-visual modality interaction module. Each module comprises a single-layer transformer encoder. We utilized DeepSpectrum\cite{amiriparian2017snore-deepspectrum} audio features and PosterV2-Vit visual features as our input features, both of which yielded excellent results. 

DeepSpectrum\cite{amiriparian2017snore-deepspectrum} refers to a 1024 dimensional feature extracted from speech signals by deep neural networks. We utilized DenseNet121\cite{huang2017densely}, trained on ImageNet\cite{russakovsky2015imagenet}, for feature extraction of DeepSpectrum. The visual features were extracted using the Pretrained model PosterV2-Vit on the AffectNet7 dataset, resulting in a feature vector of 1536 dimensions. The PosterV2-Vit model was obtained from the joint training of the PosterV2 and Vit models after individually pretraining on the AffectNet7 dataset. The final model achieved an accuracy of 67.57\% on the AffectNet7 test set.

Estimating Emotional Reaction Intensity is a multi-regression task. However, based on observations from our data annotation, it was found that each label consistently has a value of 1 for intensity. Therefore, we decoupled this intensity estimation task into two separate tasks: one for multi-regression and the other for classification. As a result, we utilized two different loss functions: MSELoss and CrossEntropyLoss. 
\begin{equation}
    Loss = \alpha L_{reg} + \beta L_{class}
    \label{eq:loss}
\end{equation}
As shown in Equation \ref{eq:loss}, the loss function for regression tasks using MSELoss, and for classification tasks using CrossEntropyLoss, can be denoted as $L_{reg}$ and $L_{class}$, respectively. $\alpha$ and $\beta$ is the loss weight for $L_{reg}$ and $L_{class}$.
%------------------------------------------------------------------------
\section{Experiments}
\label{sec:expr}
\subsection{Dataset}
Hume-Reaction\cite{christ2022muse} dataset: This multi-modal dataset comprises approximately 75 hours of video recordings, captured via a webcam located in the participants' homes. The dataset consists of recordings of 2222 participants belonging to two different cultures, South Africa and the United States. Each sample within the dataset has been self-annotated by the participants, indicating the intensity of 7 emotional experiences, including Adoration, Amusement, Anxiety, Disgust, Empathic Pain, Fear, and Surprise, on a scale from 1 to 100.

Aff-Wild2 database \cite{kollias2022abaw,kollias2022abaw2,kollias2021distribution,kollias2021analysing,kollias2021affect,kollias2020analysing,kollias2019expression,kollias2019face,kollias2019deep,zafeiriou2017aff}.

\subsection{Experimental Setup}
Throughout the experiment, we adopt the Adma optimizer with a fixed learning rate, which is 1e-4. Additionally, we employed an early stopping technique, which stopped the training process when the metric failed to increase for 15 consecutive epochs. And The evaluation metric being used is Pearson's correlation coefficient (p). The $\alpha$ and $\beta$ in equation \ref{eq:loss} was set 1.0 and 0.01, respectively. Moreover, we used Exponential Moving Average to improve the model's generalization ability, with a decay rate of 0.99. To prevent the gradient explosion issue that could negatively impact the model optimization, we applied gradient clipping with max\_norm=0.1 and norm\_type=2.

In the experiment, we set the dimension of the embedding to 256, the depth of the modality encoder to 1, the number of attention heads to 2, and the dropout to 0.5. Moreover, the depth of the modality interaction module was set to 1, the number of attention heads to 8, and the dropout to 0.1.

During the training, the batch size is set to 256, and trained for 100 epochs. 
\subsection{Experimental Results}
We conducted experiments on features from different modalities and compare the result with the baseline and Li.\cite{li2022hybrid} presented in MuSe 2022, the results are shown in Table \ref{tab:results}. We found that visual modality features were highly important, with visual scores significantly higher than audio features. After fusing the features from both modalities, model performance was further improved, reaching a Mean Person score of 0.4394 on the validation set. 
\begin{table}
  \centering
  \caption{The Mean Person score of different modality features on the Hume-Reaction validation set.}
  \scalebox{0.9}{
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Method & Feature & Modality & P \\
    \midrule
    Baseline\cite{kollias2023abaw} & DeepSpectrum & A & 0.1087 \\
    Baseline & VGGface2  & V & 0.2488 \\
    Baseline &   & A+V & 0.2382 \\
    TEMMA\cite{li2022hybrid} & DeepSpectrum & A & 0.1835 \\
    TEMMA & Resnet-18 & V & 0.3893 \\
    TEMMA & DeepSpectrum+Resnet-18 & A+V & 0.3968 \\
    Ours & DeepSpectrum & A & \textbf{0.2299}  \\
    Ours & PosterV2+Vit & V & \textbf{0.3924} \\
    Ours & DeepSpectrum + PosterV2-Vit & A+V & 0.4377\\
    Ours & DeepSpectrum + PosterV2-Vit & A+V+AV &  \textbf{0.4394}\\
    \bottomrule
  \end{tabular}
  }
  \label{tab:results1}
\end{table}
We further investigated the impact of the proposed tricks on the final performance. The removal of class loss resulted in a performance decrease of 0.0088. The absence of EMA led to a performance drop of 0.0113. The removal of L2 regularization also decreased model performance by 0.0229.
%调参表格 因素三个  模态交互+ema+l2正则+梯度裁剪0.4252+class loss
\begin{table}
  \centering
   \caption{Effects of Different Tricks on Model Performance}
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Model   & Class loss & EMA &  L2 Penality  & P\\
    \midrule
    Ours &  \Checkmark & \Checkmark & \Checkmark & 0.4394 \\
    Ours & \XSolid  & \Checkmark & \Checkmark & 0.4306\\
    Ours & \Checkmark & \XSolid  &  \Checkmark  & 0.4281\\
    Ours  & \Checkmark & \Checkmark & \XSolid  &   0.4165\\
    \bottomrule
  \end{tabular}
  \label{tab:results}
\end{table}
%------------------------------------------------------------------------
\section{Conclusion}
%------------------------------------------------------------------------
In this paper, we have proposed a new method for  Emotional Reaction Intensity Estimation. Our model shows excellent results with a Pearson coefficient on the validation set that exceeds the baseline method by 83 percent. Furthermore, we found that the use of class loss, implementation of EMA, incorporation of regularization, and utilization of gradient clipping can greatly improve model performance.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
