\section{Related Work}\label{sec:Related Work}

Multimodal models for a given task usually use inputs corresponding to different types of sensors. Raw data from these sensors can potentially have differing representations, so a possible way to handle this is to train a sub-network for each input modality and aggregate either intermediate features or predictions extracted from each sub-network \cite{Baltrusaitis2019MultimodalTaxonomy}.
For example, in an audio-visual scene classification task, \cite{Okazaki2021AVariants}~combined a visual sub-network of CLIP encoders, which were pre-trained with contrastive learning on image and textual data, with a convolutional sub-network for input audio, and concatenated intermediate features from the audio and visual sub-networks before making a final prediction of the location where a given recording was made. Similarly,
\cite{Naranjo-Alcazar2021Squeeze-ExcitationClassification}~combined a pre-trained VGG16 sub-network for input video with a convolutional sub-network for input audio, but used predictions from both sub-networks in an ensemble classifier. Both methods had improved accuracy when using all modalities as opposed to using any one individual modality. This demonstrates the potential synergy of multimodal inputs, since one modality can supplement information missing from another, thereby allowing multimodal models to capitalize on all available information~\cite{Baltrusaitis2019MultimodalTaxonomy}.

In a similar manner, neural attention-based mechanisms have also been popular as a multimodal feature alignment technique. Such mechanisms aim to mimic the human capability to focus on pertinent data, by assigning weights to features obtained from the different modalities denoting their relative importance. For example, \cite{Priyasad2020AttentionRecognition} explored the use of self-attention mechanisms for speaker emotion recognition on networks taking inputs from the textual and acoustic modalities, and found that applying self-attention before the fusion of multimodal features slightly increased classification accuracy, as compared to doing so after fusion. Similarly, \cite{Ma2019AttnSense:Recognition} used self-attention for human activity recognition, where the attention weights were distributed across features extracted from multiple accelerometers and gyroscope sensors by a convolutional recurrent neural network.

Research on multimodal perceptual models for soundscapes has primarily been centered on the use of hand-crafted features from the acoustic and visual modalities in linear regression models and shallow neural networks \cite{Lionello2020ASoundscapes}, presumably due to the ease of implementation and straightforward explainability. Nonetheless, neural attention has a natural parallel in soundscape perception, via the concept of auditory salience of acoustic events \cite{Huang2017AuditorySoundscapes}. In \cite{Watcharasupat2022AutonomousGain}, a framework for a probabilistic perceptual attribute predictor (PPAP) carrying out soundscape augmentation in the feature domain was proposed, where the ``probabilistic'' loss function
\begin{align}
   	\mathcal{J} &= K^{-1}\sum_{k}\left[\left((y_k-\widehat{\mu}_k)/{\widehat{\sigma}_k}\right)^2/2+\log\widehat{\sigma}_k\right],
    \label{eq:probabilistic_loss}
\end{align}
\vspace{-3mm}

\noindent defined in \cite{Ooi2022ProbablyAugmentation},
was used to train an attention-based DNN to account for inherent randomness in perceptual responses for a batch of $K$ samples, with the model predicting the distribution of the $k$-th response as $\mathcal{N}(\widehat{\mu}_k,\widehat{\sigma}_k^2)$, and $y_k$ being a ground-truth \textit{observation} of the $k$-th response. This is equivalent to the negative log-probability of observing $y_k$, if its ``true'' distribution were $\mathcal{N}(\widehat{\mu}_k,\widehat{\sigma}_k^2)$.

However, the existing framework, which we term the ``audio-only PPAP'' (\textsc{aPPAP}), uses only acoustic features to make predictions, without information that may affect perceptual responses like the visual environment and participant-linked parameters. We thus propose to modify the \textsc{aPPAP} to include features extracted from the visual environment and participant context as additional conditioning inputs, thereby transforming it into a ``contextual PPAP'' (\textsc{cPPAP}) that learns from multiple modalities.