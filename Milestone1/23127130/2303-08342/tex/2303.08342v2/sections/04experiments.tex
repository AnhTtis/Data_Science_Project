\section{Validation Experiments}\label{sec:Validation Experiments}

We compared the mean squared error (MSE) of a \textsc{cPPAP} using the three fusion methods in \Cref{sec:Proposed Method/Contextual PPAP} in predicting the \textit{normalized ISO Pleasantness} (\textsc{isoPl}) of an augmented soundscape, with \textsc{ef}/\textsc{mf} and \textsc{lf} variants respectively predicting $\widetilde{\mu}_k = \widehat{\mu}_k$ and $\widetilde{\mu}_k = \widehat{\mu}'_k$ to obtain
\vspace{-2mm}
\begin{align}
	\textup{MSE} &= \frac{1}{K}\sum_{k}\left(y_k-\widetilde{\mu}_k\right). \label{eq:MSE}
\end{align}
\vspace{-4mm}

\noindent The \textsc{isoPl} is defined in \cite{Ooi2022ProbablyAugmentation} as a value in $[-1,1]$. As a further ablation study, we investigated the MSE for each combination of including/excluding participant and/or visual information.

For ease of reference, we denote variants with participant embeddings $\bm{h}$ included/excluded as \textsc{ip}/\textsc{ep}, and with visual embeddings $\bm{r}$ included/excluded as \textsc{iv}/\textsc{ev}. The baseline model for comparison was the \textsc{aPPAP}, corresponding to the \textsc{ep}+\textsc{ev} case, with the best-performing setup from \cite{Watcharasupat2022AutonomousGain}. This setup is detailed in \Cref{sec:Validation Experiments/Model architecture and training}. 

\vspace{-2mm}

\subsection{Dataset}\label{sec:Validation Experiments/Dataset}

We used the ARAUS dataset \cite{Ooi2022ARAUS:Soundscapes}, which contains a 5-fold cross-validation set of \num{25440} unique perceptual responses to augmented urban soundscapes presented as audio-visual stimuli.
\newpage
Corresponding information on the participants rating the stimuli was also collected via a participant information questionnaire (PIQ), consisting of basic demographic information and standard psychological questionnaires.
The \textsc{isoPl} values can be computed from each unique response and were used as the target observations for our validation experiments. The base soundscapes $\bm{s}$ and accompanying images $\bm{b}$ in the ARAUS dataset were drawn from the Urban Soundscapes of the World database \cite{DeCoensel2017UrbanMind}, with images extracted from the \SI{0}{\degree}-azimuth, \SI{0}{\degree}-elevation field of view (FoV) of the 30-second video captured at the same time as the 30-second soundscape recordings. When presented to the participants, the audio-visual stimuli in the ARAUS dataset used the entirety of the 30-second video at the same FoV, synchronized to the audio. For this study, we take a random frame from the 30-second video and downsample the frame via bilinear interpolation to obtain a standard image dimension of $(H,W,C_{\text{v}}) = (240,135,3)$ to be used as raw visual input to the \textsc{cPPAP}. This corresponds to a video frame rate of $\frac{\text{1}}{\text{30}}$ \SI{}{\hertz}. More frames, or the entirety of the video, could be used to extract a time series of visual embeddings corresponding to a higher frame rate and temporal relations between them could be explored in future work.

The 30-second maskers $\bm{m}$ were drawn from the Freesound and xeno-canto repositories, and calibrated as in \cite{Ooi2021AutomationHead} to obtain accurate log-gain values $\gamma$ if non-silent. If the masker was silent, the information in $\gamma$ was irrelevant, so we drew $\gamma$ from $\mathcal{N}(\nu,\zeta^2)$, where $\nu$ and $\zeta$ are the mean and standard deviation of the log-gains of the training set samples with non-silent maskers. This prevented the trained models from varying predictions at inference time according to $\gamma$ despite the soundscape (and hence ground-truth label) staying constant regardless of the value of $\gamma$ when the masker was silent. 

To obtain the coded participant information $\bm{p}$, we normalized all PIQ responses in the ARAUS dataset to the range $[0,1]$ if they corresponded to continuous variables (e.g., age), and converted them to binary dummy variables in $\{0,1\}$ if they corresponded to unordered categorical variables (e.g., dwelling type).

As an initial study, only a subset of PIQ items was selected. This was done by using the normalized PIQ responses as additional predictor variables to the elastic net \textsc{isoPl} model in \cite{Ooi2022ARAUS:Soundscapes}. Only variables with regression coefficients significantly different from zero ($p < 0.05$) were selected for use in the \textsc{cPPAP}. There were $M=5$ such participant-linked variables: their highest education attained, whether their main residence was landed property; their satisfaction of the overall acoustic environment in Singapore; their score on a modified Weinsten Noise Sensitivity Scale \cite{Weinstein1978}; and their Positive Affect score on the Positive and Negative Affect Schedule \cite{Watson1988}.

\subsection{Model architecture and training}\label{sec:Validation Experiments/Model architecture and training}

For the \textsc{aPPAP}, the audio feature extractors $f_{\text{s}}$ and $f_{\text{m}}$ comprise 5 convolutional blocks, each with a 3-by-3 convolutional layer, batch normalization, dropout, swish activation, and 2-by-2 average pooling. The numbers of filters in each block are 16, 32, 48, 64, 64, respectively. The spectrogram parameters are $T=644$, $F=64$, and $C_{\text{s}} = 2$, so the audio feature extractors give embeddings with dimension $N=20$ and $D=128$. The attention block $f_{\text{a}}$ uses dot-product attention \cite{Luong2015EffectiveTranslation} and the output block $f_{\text{o}}$ consists of 3 dense layers in sequence, with the first two having 128 units and swish activation, and the last having 2 units and linear activation.

For the \textsc{cPPAP}, the visual feature extractor $f_{\text{v}}$ has the same 5 convolutional blocks as $f_{\text{s}}$ and $f_{\text{m}}$, but pooling is performed using square grids of width 2, 2, 2, 3, and 5, respectively, such that the visual embeddings also have dimension $D = 128$. The participant feature extractor $f_{\text{p}}$ comprises a single dense layer with 128 units and swish activation. The output adapter $A_\text{o}$ consists of 3 dense layers in sequence, with the first two having $2^{\lfloor \log_2(M) \rfloor + 1} = 8$ units and swish activation, and the last having 2 units and linear activation.

All model types in the validation experiments were trained under a 5-fold cross-validation scheme with the same 10 seeds for each validation fold, for a total of 50 runs per model type. Each model was trained for up to 100 epochs using an Adam optimizer with a learning rate of \num{e-4}. In the \textsc{ep} and \textsc{ev} scenarios for the ablation study, we set $\bm{h}$ and $\bm{r}$ respectively to zero vectors. For the \textsc{aPPAP} (\textsc{ep}+\textsc{ev} scenario), both $\bm{h}$ \textit{and} $\bm{r}$ are set to zero vectors.