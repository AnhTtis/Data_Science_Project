\section{Results and Discussion}\label{sec:Results and Discussion}

\Cref{tab:ablation-results} displays the results of the validation experiments and abalation study described in \Cref{sec:Validation Experiments}. All models performed better than the baseline \textsc{aPPAP} except for the \textsc{ip}+\textsc{iv} variants with mid-level and late fusion. To quantify the significance of any performance differences, we performed Kruskal-Wallis tests with Bonferroni correction between the \textsc{aPPAP} and the variants of the \textsc{cPPAP} investigated for this study. All \textsc{ip}+\textsc{ev} variants had significantly improved performance over the \textsc{aPPAP}, indicating that the fusion of additional information from the participant modality allowed the trained models to better predict the \textsc{isoPl} values derived from the ARAUS dataset responses. With late fusion, the \textsc{ip}+\textsc{ev} variant also performed the best among all investigated models with a cross-validation MSE of 0.1183$\pm$0.0011, a 2.8\% improvement over the \textsc{aPPAP}.

\input{tables/results.tex}

Moreover, the \textsc{ep}+\textsc{iv} variants, which used additional information from the visual modality, also performed better than the \textsc{aPPAP} but improvements were insignificant. This could be because the images used to derive the visual embeddings $\boldsymbol{r}$ were an \textit{objective} characteristic of the environment, whereas the participant embeddings $\boldsymbol{h}$ captured the participants' \textit{subjective} perception of the environment, thus making the \textsc{ep}+\textsc{iv} variants perform worse than the \textsc{ip}+\textsc{ev} variants. Alternative inputs from the visual modality better representing subjective perception could involve the subjectively-rated visual amenity and visual pleasantness \cite{Ricciardi2015SoundData}, if further data collection beyond the present iteration of the ARAUS dataset can be performed.

In addition, the \textsc{ip}+\textsc{iv} variants, which used information from both the participant and visual modalities, only had significant improvement over the \textsc{aPPAP} with early fusion at the feature augmentation block $f_g$. With mid-level and late fusion, the \textsc{ip}+\textsc{iv} variant actually performed worse than the \textsc{aPPAP}, which possibly hints at overfitting for the \textsc{mf} and \textsc{lf} scenarios due to the combined increase in number of non-acoustic predictor variables used at those stages. Therefore, early fusion via \Cref{eq:f_g_EF} is likely to be more suitable for the combination of multiple modalities.

Finally, at inference time, models using information from the participant modality can be used to simulate the ratings of hypothetical participants experiencing the same soundscape by varying $\boldsymbol{p}$ while keeping $\boldsymbol{s}$, $\boldsymbol{m}$, $\gamma$, and $\boldsymbol{b}$ constant. Averaging the ratings over a large variety of soundscapes, such as that in the ARAUS dataset, thus alllows us to isolate changes in perception due purely to participant-linked information while heightening the generalizability of both the models and the dataset.

As an illustration, using a trained \textsc{ip}+\textsc{iv} model undergoing early fusion, we stimulated hypothetical participants experiencing all the audio-visual stimuli in the ARAUS dataset, and present the mean \textsc{isoPl} ratings in \Cref{fig:demo-exploration}. These participants were each represented by a different value of $\boldsymbol{p}$, where individual dimensions were varied while maintaining all other dimensions at their mean values in the training set (a ``ceteris paribus'' assumption). We can see, for instance, that mean \textsc{isoPl} ratings decrease nonlinearly with increasing noise sensitivity, and that there is a fairly linear relationship between the satisfaction that a hypothetical participant has with the overall acoustic environment in Singapore with the same \textsc{isoPl} ratings.

\input{figures/demo-exploration.tex}