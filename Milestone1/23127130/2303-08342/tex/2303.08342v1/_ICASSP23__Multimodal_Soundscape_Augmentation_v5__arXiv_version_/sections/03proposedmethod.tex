\section{Proposed Method}\label{sec:Proposed Method}

\subsection{Audio-only PPAP (\textsc{aPPAP})}\label{sec:Proposed Method/Audio-only PPAP}

Consider the log-mel spectrogram of a soundscape $\bm{s}\in\mathbb{R}^{T\times F\times C_{\text{s}}}$ with $T$ time bins, $F$ mel bins, and $C_{\text{s}}$ channels, and the log-mel spectrogram of a (possibly silent) masker $\bm{m}\in\mathbb{R}^{T\times F\times 1}$ used to augment $\bm{s}$. In practice, $\bm{m}$ is reproduced by adjusting its digital gain via a multiplicative factor of $10^{\gamma}$, where $\gamma$ is the masker log-gain, before actual playback. The task of the \textsc{aPPAP} is to predict the values $\mu$ and $\log\sigma$, characterizing the distribution $\mathcal{N}(\mu,\sigma^2)$ of some perceptual attribute of interest (e.g., \textit{pleasantness} or \textit{eventfulness}), when $\bm{s}$ is augmented with $\bm{m}$ at a digital gain of $10^{\gamma}$.

To do so, the \textsc{aPPAP} extracts relevant soundscape embeddings 
$\bm{k}=f_{\text{s}}(\bm{s})\in\mathbb{R}^{N\times D}$ and masker embeddings $\bm{q} = f_{\text{m}}(\bm{m})\in\mathbb{R}^{N\times D}$, where $f_{\text{s}}$ and $f_{\text{m}}$ are the respective feature extractors, $N$ is the number of compressed time frames, and $D$ is the embedding dimension. The embeddings are then combined with the masker log-gain $\gamma$ in a feature augmentation block $f_{\text{g}}$ to obtain the augmented soundscape embeddings $\bm{v} = f_{\text{g}}(\bm{k},\bm{q},\gamma)\in\mathbb{R}^{N\times D}$. The masker embeddings $\bm{q}$ are then used to query a mapping from the soundscape embedding ``keys'' $\bm{k}$ to the augmented soundscape embedding ``values'' $\bm{v}$ via a QKV attention block $f_{\text{a}}$, which returns a set of embeddings $\bm{z} = f_{\text{a}}(\bm{q},\bm{k},\bm{v})\in\mathbb{R}^D$. The embeddings $\bm{z}$ are finally used in the output block $f_{\text{o}}$ to predict $\mu$ as $\widehat{\mu}$ and $\log\sigma$ as $\log\widehat{\sigma}$. The logarithms of the standard deviation $\sigma$ and multiplicative factor $10^{\gamma}$ are used instead of their actual values for numerical stability.

\subsection{Contextual PPAP (\textsc{cPPAP})}\label{sec:Proposed Method/Contextual PPAP}

\input{figures/demo-vis-all}

The contextual PPAP (\textsc{cPPAP}) is first modified from the \textsc{aPPAP} by incorporating inputs from two other modalities:
\begin{enumerate}[wide=0pt, widest=99, leftmargin=\parindent, labelsep=*]
    \item \textbf{Participant modality:} A vector of coded participant information $\bm{p}\in\mathbb{R}^{M}$, where $M$ is the number of participant-linked features used as input. This could contain any numerical representation of information associated with a real or hypothetical participant experiencing the soundscape $\bm{s}$.
    \item \textbf{Visual modality:} A static image $\bm{b}\in\mathbb{R}^{H\times W \times C_{\text{v}}}$, where $H$, $W$, $C_{\text{v}}$ are respectively the height, width, and number of color channels. This could correspond to a picture of the \textit{in-situ} environment where $\bm{s}$ is experienced by the participant.
\end{enumerate}
The task of the \textsc{cPPAP} is similarly to predict the values $\mu$ and $\log\sigma$ characterizing the distribution $\mathcal{N}(\mu,\sigma^2)$ of the same perceptual attribute of interest when $\bm{s}$ is augmented with $\bm{m}$ at a digital gain of $10^{\gamma}$, but with additional information about the person rating that perceptual attribute in $\bm{p}$ and the physical location where the soundscape augmentation occurs in $\bm{b}$. A visual representation of the audio-only and contextual PPAPs is shown in \Cref{fig:demo-vis-all}.

To utilize the information in $\bm{p}$ and $\bm{b}$, we propose to extract relevant participant embeddings $\bm{h} = f_{\text{p}}(\bm{p})\in\mathbb{R}^{D}$ and visual embeddings $\bm{r} = f_{\text{v}}(\bm{b})\in\mathbb{R}^{D}$ using feature extractors $f_{\text{p}}$ and $f_{\text{v}}$. The embeddings $\bm{h}$ and $\bm{r}$ can then be incorporated into the \textsc{aPPAP} to modify it into the \textsc{cPPAP} via early fusion (\textsc{ef}), mid-level fusion (\textsc{mf}), or late fusion (\textsc{lf}), which we individually investigate for our validation experiments, and explicitly define in the following subsections. The  fusion methods in the \textsc{cPPAP} are designed to preserve the modularity of the \textsc{aPPAP}, such that information from any non-acoustic modality can be omitted by zeroing out the embeddings $\bm{h}$ and $\bm{r}$ at any fusion stage. This could be useful, for example, at inference time when information from specific modalities is unavailable due to unforeseen real-life deployment conditions.

\subsubsection{Early fusion \textup{(\textsc{ef})}}\label{sec:Proposed Method/Contextual PPAP/Early fusion}

In \textsc{ef}, the feature augmentation block $f_{\text{g}}$ is modified to fuse information from all modalities such that they are included in the augmented soundscape embeddings $\bm{v}$.
All modalities are thus jointly represented in $\bm{z}$.
We extend the best-performing fusion method described in \cite{Watcharasupat2022AutonomousGain} to multiple modalities, such that in \textsc{ef}, we have
\vspace{-1mm}
\begin{align}
	f_{\text{g}}^{(\textsc{ef})}\left(
		\bm{k}, \bm{q}, \gamma, \bm{h}, \bm{r}
	\right) 
	&=
	\operatorname{Dense}\left(
		\operatorname{Conv}\left(
			\operatorname{Stk}\left(
				\bm{k},
				\bm{q},
				\bm{\Gamma},
				\mathbf{H},
				\mathbf{R}
			\right)
		\right)
	\right),
	\label{eq:f_g_EF}
\end{align}
\vspace{-4mm}

\noindent where $\bm{\Gamma} = \gamma\bm{1}_{N\times D}$,
$\mathbf{H} = \bm{1}_{N\times 1} \bm{h}^{\mathsf{T}}$, $\mathbf{R} = \bm{1}_{N\times 1} \bm{r}^{\mathsf{T}}$, $\bm{1}_{N\times D}$ is an $N$-by-$D$ matrix of ones, $\operatorname{Stk}\colon ( \mathbb{R}^{N\times D})^{B} \mapsto \mathbb{R}^{N\times D\times B}$ denotes ``channel-wise'' tensor stacking of all $B$ input arguments, $\operatorname{Dense}\colon\mathbb{R}^{N\times\bullet}\mapsto \mathbb{R}^{N\times D}$ is a dense layer with $D$ output units, and $\operatorname{Conv}\colon\mathbb{R}^{N\times D\times B}\mapsto\mathbb{R}^{N\times D}$ is a convolutional layer with the one-dimensional kernels compressing the stacked dimension into a singleton axis, thereby summarizing information from all modalities when performing soundscape augmentation in the feature domain. This definition of $f_{\text{g}}^{(\textsc{ef})}$ also prevents the problem of asynchronization of heterogeneous features mentioned in \cite{Chen2015Multi-modalNetworks}, since the ``synchronization'' occurs on the newly-created ``channel'' axis.

\vspace{-2mm}

\subsubsection{Mid-level fusion \textup{(\textsc{mf})}}\label{sec:Proposed Method/Contextual PPAP/Mid-level fusion}

In \textsc{mf}, the output block $f_{\text{o}}$ is modified to fuse information from all modalities such that they are included just before the output distribution $\mathcal{N}(\widehat{\mu},\widehat{\sigma}^2)$ is predicted. The feature augmentation block $f_{\text{g}}$ no longer uses $\bm{h}$ and $\bm{r}$ as inputs in \textsc{mf}. In other words, for \textsc{mf}, we have
\vspace{-4mm}
\begin{align}
	\begin{bmatrix}
		\widehat{\mu} & \log\widehat{\sigma}
	\end{bmatrix}^{\mathsf{T}}
	&=
	f_{\text{o}}^{(\textsc{mf})} \left(
		\operatorname{Concat}(\bm{z},\bm{h},\bm{r})
	\right), \text{ and}
	\label{eq:f_o_MF}
	\\
	f_{\text{g}}^{(\textsc{mf})}\left(
		\bm{k}, \bm{q}, \gamma
	\right) 
	&=
	\operatorname{Dense}\left(
		\operatorname{Conv}\left(
			\operatorname{Stk}\left(
				\bm{k},
				\bm{q},
				\bm{\Gamma}
			\right)
		\right)
	\right),
	\label{eq:f_g_MF}
\end{align}
\vspace{-4mm}

\noindent where $\operatorname{Concat}(\cdot)$ denotes concatenation along the embedding axis.

\vspace{-2mm}

\subsubsection{Late fusion \textup{(\textsc{lf})}}\label{sec:Proposed Method/Contextual PPAP/Late fusion}

In \textsc{lf}, an output adapter $A_{\text{o}}$ is added to perform fusion \textit{after} the final output of the \textsc{aPPAP}, such that the output distribution $\mathcal{N}(\widehat{\mu},\widehat{\sigma}^2)$ from the audio modality is transformed into $\mathcal{N}(\widehat{\mu}',(\widehat{\sigma}')^2)$ using all modalities. The predicted distribution is now $\mathcal{N}(\widehat{\mu}',(\widehat{\sigma}')^2)$, and
\vspace{-1.5mm}
\begin{align}
	\begin{bmatrix}
		\widehat{\mu}' & \log\widehat{\sigma}'
	\end{bmatrix}^{\mathsf{T}}
	&=
	A_{\text{o}} \left(
	\operatorname{Concat}(\widehat{\mu},\log\widehat{\sigma},
\bm{h},		\bm{r}
	)\right).
	\label{eq:f_o_LF}
\end{align}
\vspace{-4.5mm}

\noindent As with \textsc{mf}, the feature augmentation block $f_{\text{g}}$ does not use $\bm{h}$ and $\bm{r}$ as inputs in \textsc{lf}, so $f_{\text{g}}^{(\textsc{lf})} \equiv f_{\text{g}}^{(\textsc{mf})}$.
