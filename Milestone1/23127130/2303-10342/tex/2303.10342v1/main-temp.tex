\documentclass{midl} % Include author names
%\documentclass[anon]{midl} % Anonymized submission

% The following packages will be automatically loaded:
% jmlr, amsmath, amssymb, natbib, graphicx, url, algorithm2e
% ifoddpage, relsize and probably more
% make sure they are installed with your latex distribution
\usepackage{graphicx}
%\usepackage{subfigure} 
%\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{mwe} % to get dummy images
\jmlrvolume{-- Under Review}
\jmlryear{2023}
\jmlrworkshop{Full Paper -- MIDL 2023 submission}
\editors{Under Review for MIDL 2023}

\title[Short Title]{Whole-slide-imaging Cancer Metastases Detection and Localization with Limited Tumorous Data}


\midlauthor{\Name{Yinsheng He, Xingyu Li}}

\begin{document}

\maketitle

\begin{abstract}
Recently, various deep learning methods have shown significant successes in medical image analysis, especially in the detection of cancer metastases in hematoxylin and eosin (H\&E) stained whole-slide images (WSIs). However, in order to obtain good performance, these research achievements rely on hundreds of well-annotated WSIs. In this study, we tackle the tumor localization and detection problem under the setting of few labeled whole slide images and introduce a patch-based analysis pipeline based on the latest reverse knowledge distillation architecture. To address the extremely unbalanced normal and tumorous samples in training sample collection, %a novel weighted loss function is proposed to overweight the contribution of samples from minority category. 
we applied the focal loss formula to the representation similarity metric for model optimization. Compared with prior arts, our method achieves similar performance by less than ten percent of training samples on the public Camelyon16 dataset. In addition, this is the first work that show the great potential of the knowledge distillation models in computational histopathology. The source code is publicly available at https://github.com/wollf2008/FW-RD. 
\end{abstract}

\begin{keywords}
WSI, Tumor Detection and segmentation, Knowledge Distillation
\end{keywords}

\section{Introduction}
%Early pathologists diagnose the metastases of cancer by scanning the thin tissue slices under a microscope. This traditional approach has many limitations such as: lack of standardization, the result being highly dependent on pathologists' personal experience, and manually evaluating hundreds of histopathology slides would exert tremendous pressure on pathologists. To improve this situation, 
In the past decades, various deep-learning-based methods have been proposed to assist pathologists to detect and segment the cancer regions on histopathology slide images \cite{1,2,3,4,5,6}. However, all of these high-accuracy cancer detection methods follow the data-driven based supervised learning paradigm, where a large number of well-annotated whole slide images (WSI) containing tumors is demanding for model generalization and robustness. %Detecting tumor cells on extremely large WSIs is expensive and time-consuming. Since the most popular open-source breast cancer dataset - Camelyon16, only contains 110 pixel-wise annotated whole slide cancer images, such a lack of cancer samples is common in practical applications\cite{7}. In order to address the data-hungry property in deep learning, the mainstream approach is to tackle weakly supervised or unsupervised methods to 
To fully explore information in training data, prior arts proposes various methods to enhance tumor detection and segmentation performance. For example, Joseph et al. proposed a visual field expansion-based self-supervised method to eliminate the need for pixel-level annotations\cite{8}. Jiaojiao et al. proposed an unsupervised cell ranking-based method that uses several pixel-level annotated cancer images supplemented by a large amount of slide-level annotated cancer images during training\cite{9}. Although these methods achieved promising performance, they still did not eliminate the great demand for cancer samples. %In other words, these dataset expending methods may need a lot slide level annotated cancer images to get the ideal performance. However, it is difficult for some small medical institutions to obtain so many cancer samples, especially for some particular cancer sub-types.

In this study, we tackle the problem of few-shot learning on WSIs. It should be noted that conventional few-shot learning methods on natural images cannot be directly applied to histopathological WSIs due to their huge size. In this study, we follow the conventional WSI analysis methods and extract tissue patches from WSIs for model optimization. Though we can extract thousands of image patches from few WSIs, the ratio between normal and tumor patches is extremely unbalanced and the majority of training patches are tumor-free. To address this problem and to fully exploit knowledge provided in training samples, we incorporate the concept of anomaly detection (AD) in model design and follow the latest AD architecture, reverse knowledge distillation (RD) \cite{RA4AD}, to tumor localization and segmentation. Such design encourages the model to learn majority patterns from normal patches. Note that the original RD model cannot be optimized using samples from different categories. To take advantages of extra yet limited tumorous patch samples in training, we further introduce a weighted similarity loss in the focal loss format to adapt the RD model to the extremely unbalanced scenario, which helps overweight the tumorous samples and enhances the sensitivity of the proposed RD-based pipeline to tumor detection and localization. We evaluate our method using the publicly accessible Camelyon16 WSI dataset and show that the proposed method only needs $1/40$ of normal patches and $1/400$ of tumor patches to achieve similar performance with prior arts. In addition, this is the first attempt in literature to exploit knowledge distillation in computational histopathology. Our results demonstrated the great potential of the distillation model in this field.

%few balanced complete sets are used for model training,   To solve this problem, we proposed a few-shot anomaly detection method based on reverse distillation architecture\cite{RA4AD}. In reverse distillation architecture, the teacher-student (T-S) pair acts as encoder and decoder, and knowledge is transferred within it. The teacher encoder is always a pre-trained robust model with strong generalization ability; thus, compared with the traditional CNN model, the distillation model can use fewer training samples to get similar accuracy\cite{KD}. At the same time, we also used the basic concepts of anomaly detection\cite{10} to reduce the cancer sample demand further. Compared with cancer samples, normal samples are undoubtedly easier to obtain. In most histopathology datasets, even in most whole slide cancer images, the area of normal tissue is far more extensive than the area of tumor tissue, but these normal tissue areas are not effectively used in previous research. In order to use normal samples more effectively, thereby reducing the demand for tumor samples, we train our model through an anomaly detection approach. During training, if we mainly use normal samples to train the model, the student model only learns normal features, and it is likely to generate discrepant representations from the teacher when the input is an abnormal sample. Nevertheless, different from nature images, on H\&E images, the normal and abnormal area has relatively high similarity and complicated boundaries. Thus, a simple unsupervised anomaly detection method cannot achieve good performance on cancer metastases. Due to this situation, we introduced a few abnormal samples with focal loss during training to help the student decoder identify the differences between normal and cancer tissue in different distributions. This method has two advantages:  (1) Owing to the strong generalization ability of knowledge distillation, our model can achieve good performance with a small and highly unbalanced training dataset. On the Camelyon16 dataset, we only need $1/40$ of normal patches and $1/400$ of tumor patches to achieve similar performance with other models. (2) Unlike other few-shot methods, our method only needs a few pixel-wise annotated whole-slide cancer images and eliminates the great demand for roughly annotated whole-slide cancer images. The main contributions of our work are: (1) We introduced a simple but effective method to apply reverse distillation on whole-slide cancer images. (2) To the best of our knowledge, this is the first attempt to use knowledge distillation to detect cancer metastases in H\&E stained WSIs. Our work demonstrated the great potential of the distillation model in this field.

\section{Related Work}
%\subsection{Anomaly detection}
%A common need when analyzing real-world data sets is determining which instances are dissimilar to all others. Such instances are known as anomalies, and the goal of anomaly detection is to determine all such instances in a data-driven fashion\cite{r.1.1}.In the field of medical images, anomaly detection technology is widely used in the detection of lesions. There are three major types of anomaly detection methods: supervised anomaly detection, semi-supervised anomaly detection, and unsupervised anomaly detection. The supervised deep anomaly detection method uses labeled normal and anomalous data samples to train a deep supervised binary classifier. Despite the good performance of supervised anomaly detection methods, in the medical domain lack of available labeled training samples and a vast amount of imbalanced data presents significant challenges for the performance of the models\cite{r.1.2}. The labeled normal samples are far easier to obtain than abnormal samples. T

%Therefore, semi-supervised models use labeled normal samples with different semi-supervised methods, such as cell ranking\cite{9} and visual field expansion\cite{8} methods, to introduce unlabeled data into the training set. 


\textbf{Anomaly detection} (AD) refers to identifying and localizing anomalies with limited, even no, prior knowledge of abnormality.
Since animosities may vary with great diversity, it achieves the goal mainly based on learning inherent characteristics of the normal data. Among the various AD methods, generative models are the most important building block. The key idea is that generative models trained solely on normal samples can accurately reconstruct themselves but cannot do so for abnormal data \cite{r.1.3, r.1.4, r.1.5}. However, recent studies show that many deep learning models generalize so well that even abnormal samples can be well-reconstructed\cite{r.1.6}. To address this issue, several methods, such as memory mechanism\cite{r.1.7}, image masking strategy\cite{r.1.8}, pseudo-anomaly\cite{r.1.9}, and knowledge distillation \cite{RA4AD} are introduced. %Inspired by the common patterns in healthy WSI, this paper adopts the concept of anomaly detection in the proposed method and encourages the model to learn normal patterns in histopathology images.   %Particularly for cancer metastases detection in WSI, semi-supervised  %Our model is a supervised anomaly detection model but introduced the idea of unsupervised anomaly detection to reduce the need for abnormal samples. 

%\subsection{Few-shot learning}
\textbf{Few-shot learning} is a type of machine learning where the goal is to learn a model that can perform well on a task with a tiny number of training examples. It is particularly useful when it is difficult or expensive to obtain a large amount of well-labeled training data. In literature, few-shot learning approaches follow two paradigms, meta-learning \cite{r.2.1, r.2.2,r.2.3} and metric learning \cite{r.2.4,r.2.5}  %. In 2016, Santoro et al. proposed a memory-augmented neural network for one-shot learning\cite{r.2.2}. Also, Finn et al. proposed a meta-learning approach that can transfer knowledge of various tasks from well-trained generative models to a new model with a small number of optimization steps\cite{r.2.3}. Another popular few-shot learning direction is metric learning. Based on metric learning, a relation network was proposed by Vinyals et.al\cite{r.2.4}. It uses different encoders to project input image pairs onto vector space to compare if the different images belong to the same class. Like relation networks, Prototypical networks extract prototypes from training images to represent a specific class. The recognition task then becomes the comparison of these prototypes.\cite{r.2.5}. 
In recent years, few-shot learning methods have demonstrated their utility in the medical image detection and segmentation field\cite{r.2.6}. 

In this study, we utilize few-shot WSIs to train a model for tumor metastases detection and localization. Unlike the conventional few-shot learning setting where a complete set is provided, all WSIs in our study contains cancerous regions. To process the megabit information in one WSI, we follow the convention in the literature and extract image patches. Notably, tumor areas in one WSI are relatively small, which leads to an extremely unbalanced ratio of normal patches and cancerous patches. To efficiently leverage information among the obtained histopathological image patches, we adopt the concept of anomaly detection and encourage the model to learn the normal patterns shared by the few-shot WSIs.

\begin{figure}[h]
\centering
  {\includegraphics[width=0.8\linewidth]{Images/1.2.png}}
  \caption{Framework of the proposed method, where the structure of RD model is shown in the training stage. The student net $D$ is trained to mimic the behavior of teacher net $E$ to generate similar representations in different scales for normal patches, otherwise $D$ should make the numerical feature as different as possible from $E$ for cancerous patches. During interface, we use the well-trained RD model to generate multi-scale anomaly maps $A_i$ for each patch. Then we calculate the patch-based anomaly score and combine them together for an anomaly map of the query WSI.}
  \label{fig:1}
\end{figure}

\section{Methodology}
We specify the proposed cancer metastases detection method using few-shot WSIs in this section. Given several pixel-level annotated WSIs that contain cancerous tissue, we follow the conventional patch-based approach to analyze WSIs. Specifically, given a WSI, we crops hundreds of small tumor patches as negative samples and thousands of small normal patches as positive samples and create a small training dataset $\mathcal I$ = \{$I_1,I_2,..., I_N$\} with extremely unbalanced positive and negative ratio, for example, 10:1. Each image patch is associated with a 0/1 label $y_i$, where $y_i=1$ indicates a normal patch. Our work aims to train a model on this patch set to recognize the difference between normal patches and tumor patches for tumor detection and localization in WSI.

The framework of the proposed method is depicted in Fig.\ref{fig:1}. Due to its powerful capability on normal pattern learning, we adopt the reverse distillation (RD) architecture \cite{RA4AD}, consisting a pre-trained teacher net $E$ and a trainable student model $D$, as our backbone. Unlike the original RD method that trains the student net $D$ using anomaly-free samples only, we feed both positive and negative patches to the model in training. Note that with such an extremely unbalanced dataset, conventional data resampling strategies to address unbalanced datasets may easily fail as the whole training data is not efficiently used. To address this issue, we are inspired by the focal loss proposed in object segmentation and introduce a novel weighted similarity loss to measure the representation discrepancy in the RD model. In inference, a query WSI is also cropped into patches and the predicted anomaly maps are combined for tumor metastasis localization.

In this section, we first briefly introduce the RD model. Then we will introduce our weighted similarity loss to adapt the proposed training scenario where both positive and negative samples are available with an extremely unbalanced ratio.

\subsection{Reverse Distillation}
The RD architecture \cite{RA4AD} has two major components: fixed pretrained teacher encoder $E$ and trainable student decoder $D$. The trainable one-class bottleneck module is designed to transfer numerical features from teacher encoder to student decoder. The teacher encoder with WideResNet backbone \cite{2.1.1} is pre-trained on imageNet and frozen, aiming to extract comprehensive representations from input patches for student net training. %To avoid the reverse distillation framework converging to trivial solutions, all parameters of the teacher encoder are frozen during training\cite{RA4AD}. 
The student $D$ is mirror symmetric with the teacher $E$, and the purpose of it is to mimic the behavior of the teacher encoder on normal samples. %Unlike the traditional T-S framework, the reverse distillation framework takes the low-dimensional embedding as input of the student decoder, which prevents the propagation of abnormal features and improves the representation diversity on out-of-distribution samples. In general, reverse distillation framework only need normal samples to do unsupervised learning to achieve good performance, but compared with nature images, the area of interest in histopathology image is much smaller, and the difference between normal area and abnormal is very small. Thus, the student decoder can hardly capture enough abnormal features from histopathology images\cite{2.1.3}, and directly using the unsupervised reverse distillation method would only make the model have disastrous performance. A detailed description is shown in the results part. Therefore, we introduced a few labeled abnormal samples during training to help the student model capture more abnormal features and expand the difference between normal and abnormal samples on multi-scale feature representations.
Mathematically, let $E_n(I) = \{f_n \in (f_1,f_2,f_3)\}$ be the multiscale representations of an image patch $I$ in teacher net $E$, $\psi=B(E_n(I))$ denote the output of the bottleneck module, and $D(\psi) = \{f’_n \in (f'_1,f'_2,f'_3)\}$ be the multiscale features generated by student net $D$. Here, $n$ represents the $n_{th}$ block in either $E$ or $D$ and both $f_n$ and $f’_n$ have size due to their mirror symmetric architecture. In encouraging student $D$ to follow the behavior of teacher $E$ during training, a cosine similarity between $f_n$ and $f’_n$ is calculated for a similarity map $S_n(h,w)$ and the map-wise score is used as the student optimization loss.
\begin{equation}
S_n(h,w) = \frac{f_n(h,w)f'_n(h,w)}{||f_n(h,w)||||f'_n(h,w)||},
\end{equation}
where $S_n$ is in the range of [0,1]. The vector features in $f’_n$ similar to the original feature in $f_n$ will get a similarity score close to 1.

\subsection{Weighted distillation loss}
%We denote the teacher encoder as E, student decoder as D, and OCBE bottleneck as B. Starting by embedding the training image patches $I_n$ into multiscale feature representations, $E_n(I) = \{f_n \in (f_1,f_2,f_3)\}$, where $n$ represents to the $n_{th}$ block. Then $\psi$ could be the output of OCBE bottleneck, $\psi = B(f)$, and we can get the multiscale feature representations of student encoder $D(\psi) = \{f’_n \in (f'_1,f'_2,f'_3)\}$. Both $f_n$ and $f’_n$ have size $(C_n, H_n, W_n)$ that is equal to the number of channels, height, and width of $n_{th}$ block’s output. 
%To recognize the difference between normal tissue and tumor tissue we designed a supervised learning approach for the training dataset. Since cosine similarity can capture their relation in both high-dimensional and low-dimensional information, we first obtain the similarity score for each feature vector in $f’_n$ by calculating their cosine similarity to $f_n$ to generate a 2-D similarity map $S_n$:  $$S_n(h,w) = \frac{f_n(h,w)f'_n(h,w)}{||f_n(h,w)||||f'_n(h,w)||}$$
In this study, we create a training set from few-shot WSIs containing both positive and negative image patches. Though we can directly apply the original RD model to the training majority, i.e. the large amount of normal patches, this strategy wastes tumorous patches and doesn't fully utilize training data. However, since the training set $\mathcal I$ has extremely unbalanced positive and negative samples, we need to address two issues described as follows to adapt the RD model for our purpose.

The first question to be answered is how to accommodate both negative and positive samples in the RD model. The original RD model is trained on normal samples only. So the model training aims to minimize the representation similarity between the teacher-student pair. However, in our problem where both positive and negative samples are available, a good student model should generate similar representations to the teacher's if a patch contains normal tissue only, but generates distinct numerical features for tumorous patches. That is, we want to minimize the similarity score $S_n$ for normal patches but maximize the loss for tumorous patches. To unify the loss function as a minimization function, the loss function for tumorous patches is modified as $1-S_n$.

The second issue needed to be addressed is how to handle the extremely unbalance ratio between positive samples and negative samples. Since the majority of the patch training set is normal cases, the tumorous patches may be overwhelmed by normal samples and their corresponding loss may be overlooked in model training. To address this issue, we adopt the idea of focus loss in object segmentation and introduce a weighted loss to combine representation similarities of positive and negative samples as follows:
\begin{equation}
    \mathcal L = -\alpha_t(1-S_{nt}(h,w))^{\gamma}log(S_{nt}(h,w)),
\end{equation}

\begin{equation}
\text{where} \hspace{0.5cm}
            S_{nt} =
        \begin{cases}
          S_n, &   y_i=1\\
          1-S_n, & y_i=0.
        \end{cases} \hspace{0.5cm} \text{and}
        \quad\quad
        \alpha_{t} =
        \begin{cases}
          \alpha, &   y_i=1\\
          1-\alpha, & y_i=0.
          \end{cases} \nonumber
\end{equation}

%The training dataset is very small and unbalanced, even though knowledge distillation methods always have good performance in handling few-shot problems, the training dataset still has some particular issues. Firstly, few-shot researches typically assume balanced datasets, extreme data unbalance on training dataset may lead the over work on some classes. Also, common few-shot learning methods often assume that all training samples are highly distinct from each other\cite{2.2.1}. However, since our training samples are randomly cropped from multiple WSIs with different distributions, some rare samples with special distributions may be obscured by common samples. To solve these issues, we introduced the focal loss\cite{2.2.2} method. In general, the focal loss should be used with logit and probability, but in our case, we apply it with normalized cosine similarity: $$FL_n(h,w) = -\alpha_t(1-S_{nt}(h,w))^{\gamma}log(S_{nt}(h,w))$$ 
The hyper-parameter $\alpha_t$ is used to adjust the weight between positive and negative samples so avoid overwork on one class. %If input is normal sample $S_{nt}(h,w) = S_n(h,w)$ and $\alpha_t=\alpha$, otherwise $S_{nt}(h,w) = 1-S_n(h,w)$, and $\alpha_t=1-\alpha$. 
and the other hyper-parameter $\gamma>1$ helps the loss function focus on tumor samples as it gets a much higher loss score than common samples. In this study, we specifically set $\alpha=0.1$ and $\gamma = 2$. 

\subsection{Tumor Localization and detection in WSIs}
%During testing, we calculate pixel-level anomaly score to localize the anomaly area on each image patch. According to the method we designed above, if the input image patch contains cancer areas, the student decoder will not be able to correctly reconstruct the features of these areas. Simply, we can get the anomaly map of cancer tissue through the reverse distillation framework. We express it as: $$A_n(h,w) = 1-S_n(h,w)$$
With a well-trained RD model, given a patch from a query WSI, we can obtain a set of anomaly maps $A_n(h,w) = 1-S_n(h,w)$ for $n=1,2,...$, each measuring the discrepancy between the $n^{th}$-level representations in the teacher-student pair. In order to comprehensively evaluate the anomaly score of image patches in different dimensions, we up-sample the obtained anomaly maps to the same size as the input patch, then add them together to get a final anomaly map, $A(h,w) =\sum_{n = 1}^N A_n(h,w)$. High values in the map $A(h,w)$ indicate tumor regions and
%The range of anomaly score is [0,3]. For patch-level classification, we use the average anomaly score on each image patch as the anomaly score of this patch. 
the summation of the pixel-wise anomaly score in $A(h,w)$ is then compared to a pre-determined threshold for tumor detection.

\section{Experiments}
\subsection{Experimental settings}
\textbf{Dataset:} we evaluate the proposed method using the publicly Camelyon16 dataset \cite{7}. Camelyon16 contains 110 tumor and 160 normal annotated WSIs for training and 81 normal and 49 tumor annotated WSIs for testing. We randomly select 10 tumor WSIs as our training data. To create the patch-level training set from the 10 WSIs, we follow the preprocess method in \cite{5}. Specically, on 40× magnification WSIs (level 0), we randomly cropped 5k $256*256$ patches from normal regions and 500 $256*256$ patches from tumor regions. For the validation purpose, we randomly cropped 2k patches from 2 WSIs. The rest 128 WSIs are used as test data. In both validation and test sets, the ratio between normal and abnormal patches is 1:1.
%In order to evaluate the performance of the model, we utilized two publicly available datasets, Camelyon16\cite{7} and WSSS4LUAD\cite{3.1.1}. Here, we mainly focus on the experiment of the Camelyon16 dataset, and the experiment detail of WSSS4LUAD will be shown in appendix A. Camelyon16 dataset contains 110 tumor and 160 normal annotated WSIs for training; 81 normal and 49 tumor annotated WSIs for testing. We use randomly selected 10 tumor WSIs as our training data since tumor WSIs contain both tumor and normal tissues. With WSIs images we selected, we follow the preprocess method similar to that described in \cite{5}. On 40× magnification WSIs (level 0), we randomly cropped 5k $256*256$ patches from normal region as normal samples and 500 $256*256$ patches from tumor region as abnormal samples to form the few-shot dataset. For validation use, we randomly cropped 2k patches from 2 WSIs as our validation dataset. The test set contains a total of 128 WSIs, we use all of them to evaluate the performance of our method, furthermore, we randomly cropped 20k patches from testing WSIs to calculate the patch-level classification accuracy. In both validation and test dataset, the ratio between normal and abnormal patches are 1:1.  
The distribution of training and testing data is specified in Tab. \ref{tab:1}.

\begin{table}[htbp]
 %% The first argument is the label.
 %% The caption goes in the second argument, and the table contents
 %% go in the third argument.
\centering
  {\begin{tabular}{l|llll} \hline
\multicolumn{1}{c}{\textbf{}}                  & \multicolumn{1}{c}{\textbf{}} & \multicolumn{3}{c}{Few-shot WSI setting }\\\cline{3-5}
\multicolumn{2}{l}{}                                                           & Train     & Val.    & Test\\\hline
\multicolumn{1}{c|}{{\# of WSI}} & normal                        & 0         & 1       & 81\\\cline{2-5}
\multicolumn{1}{c|}{}                           & tumor                         & 10        & 1       & 49\\\hline
{Patches}                       & normal                        & 5000      & 2000    & 10000\\\cline{2-5}
& tumor & 500       & 2000    & 10000\\\hline
\end{tabular}}
\caption{Our few-shot WSI experimental setting constructed from the Camelyon16 dataset.}%
\label{tab:1}%
\end{table}

%\subsection{Implementation Details}
\noindent\textbf{Implementation details:}
We use the first three blocks of a pretrained wideResNet50 \cite{3.2.1} architecture as the teacher encoder in our RD model. For each block, a convolution layer with kernel size =1 and stride = 2 is used to down-sampling the data. Correspondingly, each block in the student decoder adopts a deconvolutional layer to up-sample the data with a kernel size of 2 and a stride of 2. The whole model was implemented with PyTorch and trained on NVIDIA GeForce GTX 3090. We utilize Adam optimizer\cite{3.2.2} with $\beta = (0.5, 0.999)$. The learning rate is 0.000001 and the batch size is 32. To get the best result, we set the threshold for tumor detection to 2 and train the model for 50 epochs and select the checkpoint with the best patch-level classification accuracy on the validation set. 

We compare the proposed method with prior arts including HMS\&MIT \cite{3.2.3}, SFCLD, TCBB \cite{9}, and the original reverse distillation model. HMS\&MIT and SFCLD are the fully-supervised methods trained on the whole Camelyon16 dataset with top performance. TCBB is a few-shot learning method using 30K training patches with 1:1 normal and tumor ratio. 

Following the requirements of the Camelyon16 challenge, we use the area under receiver operating characteristic(AUROC) and lesion-based free-response receiver operating characteristic(FROC) curve as our evaluation metrics. %AUROC score assesses algorithms for discriminating between slides containing metastasis and normal slides. FROC score is defined as sensitivity versus the average number of false-positives per image. Additionally, following the previous study in \cite{3.2.3}, we also calculated the patch-level classification accuracy. 

\subsection{Results and Discussions} 
%In this part, we will show our experiment results on Camelyon16 dataset and compare our results with state-of-the-art fully-supervised methods, other few-shot method, and the original reverse distillation model. We use the original reverse distillation model as the baseline. As we described in the previous section, we evaluate the performance of our model on Camelyon16 dataset with AUROC score and FROC score. We first need to generate the anomaly maps for all WSIs in the testing dataset to evaluate these two metrics. For each WSI, we crop the area of interest(ROI) into image patches with a sliding window, then we use the well-trained model to generate the anomaly maps for image patches and calculate the overall anomaly score for each patch as we described in Fig.\ref{fig:1} and section 3.3. In this way, we can generate anomaly maps for WSIs with a stride of 64 (level 6). Fig.\ref{fig:2} shows anomaly maps from our method and anomaly baseline and probability maps from state-of-the-art methods of Test\_001. Due to some recurrence problem, we directly copied the TCBB probability map from the original paper. 

Slide-level classification (AUROC) %On anomaly maps, pixels with anomaly scores higher 2.0 are considered as extremely abnormal pixels. Thus, we calculate the AUROC score with an average anomaly score for anomaly pixels that are higher than 2.0 on each WSI. 
and Tumor region localization (FROC) %In Camelyno16 FROC score is defined as average sensitivity at 6 predefined false positive rates: 1/4, 1/2, 1, 2, 4 and 8 false positives per WSI. In this stage, we generate binary maps for whole slide anomaly maps with a threshold value of 2.0. On each binary map, we connect neighboring regions as a single example and use the anomaly score of the centroid as the lesion score. The final AUROC score 
are reported in Fig.\ref{fig:2}(a) and (b), respectively. The patch-level classification accuracy of different methods is shown in Tab.\ref{tab:2}. Our method achieves 0.9036 AUROC with only 10 pixel-level annotated WSIs. The performance of our method significantly exceeds the prior few-shot method and the baseline RD model. Fig.\ref{fig:3} provides tumor localization on Test\_001 for visualization. Due to some reproducible issues, we directly copied the TCBB probability map from the original paper. Though our training samples are much smaller than all prior methods, the proposed method significantly outperforms the few-shot method TBCC and the RD baseline in terms of tumor localization. Even compared to the fully-supervised methods on the complete Camelyon16 dataset, our method also achieves quite promising performance.

\begin{figure}[htbp]
\centering
    \subfigure[]{
    {\includegraphics[width=0.45\linewidth]{Images/3.2.2.png}}
  }
    \subfigure[]{
    {\includegraphics[width=0.45\linewidth]{Images/3.1.1.png}}
  }
  \caption{(a) ROC curves and (b) FROC curves on Camelyon16. HMS\&MIT is the winner of the Camelyno16 challenge and SLFCD is the latest study. Both of them used the entire training set. TCBB is the latest few-shot method on Camelyon16, utilizing 3 times more data than our method in training.}
  \label {fig:2}
\end{figure}

\begin{table}[htbp]
 %% The first argument is the label.
 %% The caption goes in the second argument, and the table contents
 %% go in the third argument.
\centering
 {\begin{tabular}{llllll} \hline
\multicolumn{1}{c}{\textbf{Model Name}} & \multicolumn{1}{c}{HMS\&MIT} & \multicolumn{1}{c}{SLFCD} & TCBB  & RD & Our  \\\hline
\textbf{Patch classification accuracy} & 0.984 & 0.933 & 0.876 & 0.652 & 0.881 \\   \hline
\end{tabular}}
  \caption{Patch level classification accuracy on Camelyon16}
  \label{tab:2}%
\end{table}



\begin{figure}[htbp]
  \centering
  \subfigure[]{
    \includegraphics[width=0.3\linewidth]{Images/2.1.2.png}
  }
  \subfigure[]{
    \includegraphics[width=0.3\linewidth]{Images/2.2.2.png}
  }
  \subfigure[]{
    \includegraphics[width=0.3\linewidth]{Images/2.3.2.png}
  }
  \subfigure[]{
    \includegraphics[width=0.3\linewidth]{Images/2.4.2.png}
  }
    \subfigure[]{
    \includegraphics[width=0.3\linewidth]{Images/2.6.2.png}
  }
  \subfigure[]{
    \includegraphics[width=0.3\linewidth]{Images/2.5.2.png}
  }
  
\begin{flushleft}
\caption{(a) Original WSI Test\_001 in Camelyon16, (b) the ground truth of tumor area, and tumor localization by (c) SFCLD, d) TCBB, (e) the original RD, and (f) our method.}
\end{flushleft}

\label{fig:3}
\end{figure}

\begin{table}[htbp]
 % The first argument is the label.
 % The caption goes in the second argument, and the table contents
 % go in the third argument.
\centering
  {\begin{tabular}{l|lllllll} \hline \textbf{Number of abnormal patches} & 0 & 5     & 10    & 50    & 100   & 500   & 1000  \\ \hline
\textbf{Patch classification Accuracy}    & 0.646 & 0.759   & 0.784   & 0.812   & 0.843   & 0.881   & 0.879   \\ \hline
\end{tabular}}
  \caption{Ablation on the number of tumor patches in model optimization.}
  \label{tab:3}%
\end{table}

\subsection{Ablation on Weighted Loss}
The weighted distillation loss mitigates the degradation of classification performance caused by data imbalance. In this study, we compare the weighted and unweighted loss. Under the same setting, the patch classification accuracy with unweighted loss reduces to 83.8\%, which is a 4.3\% performance drop compared to the weighted loss. 

\subsection{Ablation on Training Samples}
This ablation studies how the amount of training data affects the performance of our method. To this end, we first vary the number of abnormal patches in training and monitor the patch-level classification accuracy following the previous study in \cite{3.2.3}. To ensure the fairness, all experiments use the same normal patch set, and all small negative patches are subsets of the original tumor training samples. Under each set, we simply modify $\alpha$ so that it follows the ratio between normal and abnormal patches and report the patch-level classification accuracy in Table\ref{tab:3}. When the number of abnormal patches is 0 and only 5K normal patches are available for training, our method is identical to the original RD model.  %model cannot accurately capture enough information about abnormal features when we only have a few abnormal patches. If we do fully unsupervised learning on WSIs (only using normal samples), the model would have good reconstruction ability for both normal and abnormal data during testing. In this case, the anomaly score difference between normal patches and abnormal patches would be very small(about 15\%). 
Few more abnormal patches (such as 5 or 10) could noticeably improve the model's performance. %On Camelyon16 dataset, this improvement becomes saturated when the number of abnormal patches reaches 500. We hypothesize that more abnormal data following the same distribution cannot significantly improve the student decoder’s ability to capture abnormal features. 
% Also, this situation reflects the strong generalization ability of reverse distillation to data with the similar distributions. To ensure the authenticity of the results, all experiments use the same normal dataset, and all small negative datasets are subsets of larger negative datasets. We only modified $\alpha$ to keep normal and abnormal samples with the same weight.

Second, we evaluate the model with a balanced dataset by downsampling the normal patches. To this end, we randomly resample 500 normal patches (same as the number of tumor patches in our main experiment) and the classification accuracy reduces to 86.9\%. This performance drop is due to the information loss in normal pattern learning.

For a comprehensive investigation of the proposed method, we also quantitatively evaluate our model under a fully-supervised setting and compare its performance to the prior supervised methods. Specifically, we crop 200,000 patches from the entire 270 WSIs in the Camelyon16 training set, half being normal patches and the other half being tumor patches. We set the hyperparameters $\alpha = 0.5$ and $\gamma = 2$. The classification accuracy under this setting reaches 93.0\%, which is still lower than those state-of-art methods but very close (e.g. 93.3\% FOR SLFCD). 


\section{Conclusions}
In this work, we proposed a reverse distillation-based WSI few-shot learning method to localize tumor regions in WSIs. %Different from traditional few-shot learning methods that require massive unannotated cancer images, our model can achieve a relatively high detection accuracy with only a few cancer samples based on the excellent generalization ability of the reverse distillation framework and anomaly detection training method. 
To address the unbalanced issue in training set, our method incorporated the concept of anomaly detection in model design and encouraged the model to learn normal patterns from the majority of training set. To further exploiting information in abnormal patches, we introduced a focus loss similar function to upweight the minority samples in model optimization. The results indicated that our model could identify the tumorous regions with promising performance and achieved test AUC scores greater than 0.9. %Although our method greatly reduces the data demand of cancer detection tasks, the accuracy of our model prediction is still far behind the current top research results. Especially due to the influence of the extremely unbalance dataset, our model only gets a 0.50 FROC score on Camelyon16 dataset. In addition, due to the lack of normal samples at the edge of tissues. In the training set, our model will recognize some normal patches at the edge of tissues as abnormal samples, so generate many noise pixels on the anomaly. Our research only provides a direction for applying knowledge distillation in the field of cancer detection, and it has many potential improvement directions. Based on the low requirements for dataset quality, our method undoubtedly has an extensive application scenario and clinical value.
\newpage

\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akcay et~al.(2018)Akcay, Atapour-Abarghouei, and Breckon]{r.1.3}
Samet Akcay, Amir Atapour-Abarghouei, and Toby~P Breckon.
\newblock Ganomaly: Semi-supervised anomaly detection via adversarial training.
\newblock In \emph{Asian conference on computer vision}, pages 622--637.
  Springer, 2018.

\bibitem[Bandi et~al.(2018)Bandi, Geessink, Manson, Van~Dijk, Balkenhol,
  Hermsen, Bejnordi, Lee, Paeng, Zhong, et~al.]{4}
Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van~Dijk, Maschenka
  Balkenhol, Meyke Hermsen, Babak~Ehteshami Bejnordi, Byungjae Lee, Kyunghyun
  Paeng, Aoxiao Zhong, et~al.
\newblock From detection of individual metastases to classification of lymph
  node status at the patient level: the camelyon17 challenge.
\newblock \emph{IEEE transactions on medical imaging}, 38\penalty0
  (2):\penalty0 550--560, 2018.

\bibitem[Bejnordi et~al.(2017)Bejnordi, Veta, Van~Diest, Van~Ginneken,
  Karssemeijer, Litjens, Van Der~Laak, Hermsen, Manson, Balkenhol, et~al.]{7}
Babak~Ehteshami Bejnordi, Mitko Veta, Paul~Johannes Van~Diest, Bram
  Van~Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen~AWM Van Der~Laak,
  Meyke Hermsen, Quirine~F Manson, Maschenka Balkenhol, et~al.
\newblock Diagnostic assessment of deep learning algorithms for detection of
  lymph node metastases in women with breast cancer.
\newblock \emph{Jama}, 318\penalty0 (22):\penalty0 2199--2210, 2017.

\bibitem[Bergmann et~al.(2018)Bergmann, L{\"o}we, Fauser, Sattlegger, and
  Steger]{r.1.4}
Paul Bergmann, Sindy L{\"o}we, Michael Fauser, David Sattlegger, and Carsten
  Steger.
\newblock Improving unsupervised defect segmentation by applying structural
  similarity to autoencoders.
\newblock \emph{arXiv preprint arXiv:1807.02011}, 2018.

\bibitem[Boyd et~al.(2021)Boyd, Liashuha, Deutsch, Paragios, Christodoulidis,
  and Vakalopoulou]{8}
Joseph Boyd, Mykola Liashuha, Eric Deutsch, Nikos Paragios, Stergios
  Christodoulidis, and Maria Vakalopoulou.
\newblock Self-supervised representation learning using visual field expansion
  on digital pathology.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 639--647, 2021.

\bibitem[Chen et~al.(2019)Chen, Jiao, He, Han, and Qin]{9}
Jiaojiao Chen, Jianbo Jiao, Shengfeng He, Guoqiang Han, and Jing Qin.
\newblock Few-shot breast cancer metastases classification via unsupervised
  cell ranking.
\newblock \emph{IEEE/ACM Transactions on Computational Biology and
  Bioinformatics}, 18\penalty0 (5):\penalty0 1914--1923, 2019.

\bibitem[Deng and Li(2022)]{RA4AD}
Hanqiu Deng and Xingyu Li.
\newblock Anomaly detection via reverse distillation from one-class embedding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9737--9746, 2022.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{r.2.3}
Chelsea Finn, Kelvin Xu, and Sergey Levine.
\newblock Probabilistic model-agnostic meta-learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Gong et~al.(2019)Gong, Liu, Le, Saha, Mansour, Venkatesh, and
  Hengel]{r.1.7}
Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa~Reda Mansour, Svetha
  Venkatesh, and Anton van~den Hengel.
\newblock Memorizing normality to detect anomaly: Memory-augmented deep
  autoencoder for unsupervised anomaly detection.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1705--1714, 2019.

\bibitem[Kingma and Ba(2014)]{3.2.2}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lee and Paeng(2018)]{2}
Byungjae Lee and Kyunghyun Paeng.
\newblock A robust and effective approach towards accurate metastasis detection
  and pn-stage classification in breast cancer.
\newblock In \emph{International conference on medical image computing and
  computer-assisted intervention}, pages 841--850. Springer, 2018.

\bibitem[Li et~al.(2021)Li, Chen, Levy, Wang, Wang, Chen, Wan, Wong, and
  Raina]{r.2.2}
Haitong Li, Wei-Chen Chen, Akash Levy, Ching-Hua Wang, Hongjie Wang, Po-Han
  Chen, Weier Wan, H-S~Philip Wong, and Priyanka Raina.
\newblock One-shot learning with memory-augmented neural networks using a
  64-kbit, 118 gops/w rram-based non-volatile associative memory.
\newblock In \emph{2021 Symposium on VLSI Technology}, pages 1--2. IEEE, 2021.

\bibitem[Li and Ping(2018)]{5}
Yi~Li and Wei Ping.
\newblock Cancer metastasis detection with neural conditional random field.
\newblock \emph{arXiv preprint arXiv:1806.07064}, 2018.

\bibitem[Luengo et~al.()Luengo, Basham, and French]{2.1.1}
Imanol Luengo, Mark Basham, and Andrew French.
\newblock P.(2016) smurfs: superpixels from multi-scale refinement of
  super-regions. in: British machine vision conference (bmvc 2016), 20-22nd
  sept 2016, york, uk.

\bibitem[Morar et~al.(2012)Morar, Moldoveanu, and Gr{\"o}ller]{3}
Anca Morar, Florica Moldoveanu, and Eduard Gr{\"o}ller.
\newblock Image segmentation based on active contours without edges.
\newblock In \emph{2012 IEEE 8th international conference on intelligent
  computer communication and processing}, pages 213--220. IEEE, 2012.

\bibitem[Mori et~al.(2013)Mori, Sakuma, Sato, Barillot, and Navab]{1}
Kensaku Mori, Ichiro Sakuma, Yoshinobu Sato, Christian Barillot, and Nassir
  Navab.
\newblock \emph{Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2013: 16th International Conference, Nagoya, Japan,
  September 22-26, 2013, Proceedings, Part III}, volume 8151.
\newblock Springer, 2013.

\bibitem[Pourreza et~al.(2021)Pourreza, Mohammadi, Khaki, Bouindour, Snoussi,
  and Sabokrou]{r.1.9}
Masoud Pourreza, Bahram Mohammadi, Mostafa Khaki, Samir Bouindour, Hichem
  Snoussi, and Mohammad Sabokrou.
\newblock G2d: generate to detect anomaly.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pages 2003--2012, 2021.

\bibitem[Schlegl et~al.(2017)Schlegl, Seeb{\"o}ck, Waldstein, Schmidt-Erfurth,
  and Langs]{r.1.5}
Thomas Schlegl, Philipp Seeb{\"o}ck, Sebastian~M Waldstein, Ursula
  Schmidt-Erfurth, and Georg Langs.
\newblock Unsupervised anomaly detection with generative adversarial networks
  to guide marker discovery.
\newblock In \emph{International conference on information processing in
  medical imaging}, pages 146--157. Springer, 2017.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{r.2.5}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Song et~al.(2022)Song, Wang, Mondal, and Sahoo]{r.2.1}
Yisheng Song, Ting Wang, Subrota~K Mondal, and Jyoti~Prakash Sahoo.
\newblock A comprehensive survey of few-shot learning: Evolution, applications,
  challenges, and opportunities.
\newblock \emph{arXiv preprint arXiv:2205.06743}, 2022.

\bibitem[Sun et~al.(2022)Sun, Li, Ding, Huang, Chen, Wang, Yu, and
  Paisley]{r.2.6}
Liyan Sun, Chenxin Li, Xinghao Ding, Yue Huang, Zhong Chen, Guisheng Wang,
  Yizhou Yu, and John Paisley.
\newblock Few-shot medical image segmentation using a global correlation
  network with discriminative embedding.
\newblock \emph{Computers in biology and medicine}, 140:\penalty0 105067, 2022.

\bibitem[Tian et~al.(2019)Tian, Yang, Wang, Zhang, Tang, Ji, Yu, Li, Yang, and
  Qian]{6}
Ye~Tian, Li~Yang, Wei Wang, Jing Zhang, Qing Tang, Mili Ji, Yang Yu, Yu~Li,
  Hong Yang, and Airong Qian.
\newblock Computer-aided detection of squamous carcinoma of the cervix in whole
  slide images.
\newblock \emph{arXiv preprint arXiv:1905.10959}, 2019.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{r.2.4}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Wang et~al.(2016)Wang, Khosla, Gargeya, Irshad, and Beck]{3.2.3}
Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, and Andrew~H Beck.
\newblock Deep learning for identifying metastatic breast cancer.
\newblock \emph{arXiv preprint arXiv:1606.05718}, 2016.

\bibitem[Yan et~al.(2021)Yan, Zhang, Xu, Hu, and Heng]{r.1.8}
Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng.
\newblock Learning semantic context from normal samples for unsupervised
  anomaly detection.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 3110--3118, 2021.

\bibitem[Zagoruyko and Komodakis(2016)]{3.2.1}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zavrtanik et~al.(2021)Zavrtanik, Kristan, and Sko{\v{c}}aj]{r.1.6}
Vitjan Zavrtanik, Matej Kristan, and Danijel Sko{\v{c}}aj.
\newblock Reconstruction by inpainting for visual anomaly detection.
\newblock \emph{Pattern Recognition}, 112:\penalty0 107706, 2021.

\end{thebibliography}

\newpage
\appendix

\section{Expernments of WSSS4LUAD dataset}
\subsection{Prepossessing}
WSSS4LUAD dataset contains thousands of patches cropped from 87 H\&E stained WSIs for lung adenocarcinoma, but it doesn’t provide original WSIs. These patches are mainly composed of three types of tissues, which are tumor epithelial tissue, tumor-associated stroma tissue, and normal tissue. In our experiment, we consider both tumor epithelial tissue, tumor-associated stroma tissue as abnormal sample, and normal tissue as normal sample. The training set comprises a total of 10091 different size image patches that are cropped from 63 WSIs. 1832 of training patches are normal samples, and 8259 of training patches are abnormal samples. We randomly selected 1500 normal samples and 100 abnormal samples form training set as our few-shot dataset. The original validation set and testing set contains a total of 23 large patches and 97 small patches. We randomly select 3 large patches as our validation dataset, and feed others to our test dataset. Since all patches are in different size, we cropped the size of small patches to $200*200$, and cropped large patches into thousands of $200*200$ small patches. 

\subsection{Evaluation}
Originally, WSSS4LUAD challenge uses 3 class mIOU as evaluation matric, since our method currently can only do binary classification, we use pixel level AUROC and 2 class mIOU to evaluate the tumor detection performance of our model on low level features. WSSS4LUAD dataset provide the background mask for each testing patch, so the back ground pixels will not be considered during test evaluation. Fig.\ref{A1} shows several anomaly maps for WSSS4LUAD tumor patch. The final pixel level AUROC sore of our method is 0.915, and the final pixel level mIOU score of our model is 0.726. The Tab.\ref{tab:4} shows how abnormal patch number influence the performance of result model.

\begin{figure}[htbp]
\centering
  
  {\includegraphics[width=0.23\linewidth]{Images/4.1.1.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.1.2.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.2.1.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.2.2.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.3.1.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.3.2.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.4.1.png}}
  {\includegraphics[width=0.23\linewidth]{Images/4.4.2.png}}
\caption{Anomaly map with ground truth}
\label {A1}
\end{figure}

\begin{table}[htbp]
\centering
  {\begin{tabular}{llllllll}
\textbf{Number of Abnormal Patches} & 0 & 5     & 10    & 50    & 100   & 500   & 1000  \\
\textbf{WSSS4LUAD(Pixel AUROC)}     & 0.68 & 0.831 & 0.884 & 0.914 & 0.915 & 0.915 & 0.916
\end{tabular}}
\caption{Pixel-level AUROC for WSSS4LUAD with}
\label {tab:4}%
\end{table}

\end{document}