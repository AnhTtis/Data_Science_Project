In the last years, the use of \acp{nn} to approximate \acp{pde} has been widely explored as an alternative to standard numerical methods such as the \ac{fem}. The idea behind these methods is to represent the numerical solution of a \ac{pde} as the realisation of a \ac{nn}. The parameters of the \ac{nn} are then optimised to minimise a form of the residual of the \ac{pde} or an energy functional. The original \ac{pinn} method \cite{raissi2019} considers the minimisation of the \ac{pde} in the strong form. Other frameworks recast the \ac{pde} as a variational problem and minimise the weak energy functional, like in Variational \acp{pinn} (VPINNs, \cite{kharazmi2019}), the Deep Ritz Method (DRM, \cite{yu2018}) and the Deep Nitsche Method (DNM, \cite{liao2019}). 

\subsection*{Comparison between the FEM and NNs}

\Acp{nn} activated by the $\ReLU$ function (Rectified Linear Unit) constitute a valuable special case, as they can emulate linear finite elements spaces on a mesh that depends on the parameters of the network \cite{he2018}. This enables one to draw close links between the well-established theory of the \ac{fem} and the recently introduced neural frameworks \cite{opschoor2020}.

There are additional benefits to using the ReLU activation function, such as addressing the issue of exploding or vanishing gradients and being computationally cheap to evaluate. However, the numerical experiments in \cite{adcock2021} suggest that the theoretical approximation bounds for $\ReLU$ networks are not met in practice: even though there exist network configurations that realise approximations of a given accuracy, the loss landscape is rough because of the low regularity of $\ReLU$. As a result, gradient-based optimisation algorithms are largely dependent on the initial parameters of the network and may fail to converge to a satisfying local minimum. It was also shown in \cite{hayou2019} that training a network with a smooth activation function helps the network converge. 

Compared to the \ac{fem}, \acp{nn} can be differentiated thanks to automatic differentiation rather than relying on a numerical scheme (and ultimately a discretisation of the domain). Another advantage of \acp{nn} is that they are intrinsically adaptive, in the sense that contrary to traditional \ac{pde} solvers that rely on a fixed mesh and fixed shape functions, \acp{nn} can perform nonlinear approximation. Finally, the machine learning approach proposes a unified setting to handle forward and inverse problems that can involve highly nonlinear differential operators. However, properly enforcing initial and boundary conditions remains one of the major challenges for \acp{nn} \cite{berrone2022}, especially when the domain has a complex geometry.

\subsection*{Numerical integration of NNs}

Current efforts in the literature are mainly focused on developing novel model architectures or solving increasingly complex \acp{pde}, while there are comparatively few results on the convergence, consistency and stability of \acp{pinn} and related methods. These are the three main ingredients that provide theoretical guarantees on the approximation error, and they remain to be established for \acp{nn}.

In particular, the numerical quadratures that are used to evaluate the loss function have mostly been investigated experimentally, and the link between the integration error of the loss function, often referred to as the generalisation gap, and the convergence of the network is still not fully understood. The overwhelming majority of implementations of \acp{nn} solvers rely on \ac{mc} integration because it is relatively easy to implement. However, it is known that its convergence rate with respect to the number of integration of points is far from optimal in low-dimensional settings. For example, the integration error decays as $n^{-2}$ for the trapezoidal rule in dimension one, whereas it is of the order of $n^{-1/2}$ for \ac{mc} regardless of the dimension. Furthermore, local behaviours of the function to integrate are generally poorly handled by \ac{mc}, because the points are sampled uniformly across the domain.

A few recent works have considered more advanced integration methods for \ac{nn} solvers, including Gaussian quadratures or quasi-\ac{mc} integration. Some studies contemplate resampling the \ac{mc} points during the training or adding more points where the strong, pointwise residual is larger. We refer the reader to \cite{wu2023} for a comprehensive review of sampling methods and pointwise residual-based resampling strategies applied to \acp{nn}. They show that the convergence of the model is significantly influenced by the choice of the numerical quadrature. A theoretical decay rate of the generalisation gap when the training points are carefully sampled was shown in \cite{longo2021}, while \cite{shin2020} proved the consistency of \acp{pinn} and related methods in the sample limit. Finally, the generalisation error was bounded with the sum of the training error and the numerical integration error in \cite{mishra2022}. However, this result requires strong assumptions on the solution and the numerical quadratures, and the constants involved in the bound are not explicit and may not be under control.

It was shown in \cite{rivera2022} that \enquote{quadrature errors can destroy the quality of the approximated solution when solving \acp{pde} using deep learning methods}. The authors explored the use of a \ac{cpwl} interpolation of the output of the network and observed better convergence compared to \ac{mc} integration. Their experiments were performed in dimension one, and the networks were interpolated on fixed, uniform meshes.

\subsection*{Motivations and contributions}

Our work aligns with current research in the theory and practice of \acp{nn} applied to approximating \acp{pde}. We seek to equip \acp{nn} with some of the tools of the \ac{fem} in terms of numerical integration, for low-dimensional ($d \leq 3$) problems arising from computational physics or engineering.

The space of \ac{cpwl} activation functions is of great interest for \acp{nn} owing to its structure of vector space and its stability by composition. This means that whenever a network is activated by a \ac{cpwl} function, its realisation is \ac{cpwl} too. Our idea is that provided that we can decompose the domain at hand into regions where the network is almost linear in each cell, we can integrate with high accuracy any integral that involves the network and its partial derivatives by relying on Gauss-like quadratures of a sufficiently high order. The linear and bilinear forms are decomposed on this mesh in a similar way as in the \ac{fem}, except that the underlying mesh is adaptive instead of fixed. We provide an algorithm to obtain such a decomposition of the domain based on the expression of the activation function and the parameters of the network.

Nevertheless, as will be shown in \sect{regularisation}, \ac{cpwl} activations are not smooth enough for the optimisation problem of a \ac{nn} approximating a \ac{pde} to be well-behaved. If the activation function is \ac{cpwl}, it needs to be regularised so that the network can be properly trained. In this work, all the networks that we train are activated by smooth functions. However, the quadrature points and the mesh are obtained from the projection of this smooth network onto a \ac{cpwl} space, when the activation function is replaced by a \ac{cpwl} approximation of the smooth activation. We design a procedure to obtain a \ac{cpwl} approximation of a smooth function on $\RR$ as a whole.

Our extensive numerical experiments demonstrate that our proposed integration method enables the \ac{nn} to converge faster and smoother, to lower generalisation errors compared to \ac{mc} integration, and to be more robust to the initialisation of its parameters while keeping similar training times. 

\subsection*{Outline}

We recall the general setting of \ac{pinn} and its variants and introduce some notations related to the variational problem we consider in \sect{preliminaries}. In \sect{regularisation}, after explaining why \acp{nn} activated by \ac{cpwl} functions are not suitable to approximate \acp{pde}, we suggest several smooth approximations of $\ReLU$. In \sect{cpwlisation}, we introduce a class of activation functions that behave linearly at infinity, propose a method to approximate them by \ac{cpwl} functions on $\RR$ as a whole, and prove its quadratic convergence rate. Then, in \sect{adaptivity}, we provide an algorithm to decompose the physical domain into convex regions where the \ac{nn} is almost linear. We also describe how to create Gaussian quadratures for convex polygons and analyse the integration error of our method. We present our numerical experiments in \sect{experiments}. We solve the Poisson equation in dimensions one and two on the reference segment and square, but also on a more complex domain, to show that our adaptive quadrature can adapt to arbitrary polygonal domains. We discuss our findings and conclude our work in \sect{conclusion}. In order to keep the article more readable, we decided to write the proofs of our propositions in \app{proofs} and our algorithms in \app{algorithms}.