Let $u = \mathcal{N}(\pmb{\Theta}, \rho)$ and $\pi[u] = \mathcal{N}(\pmb{\Theta}, \pi[\rho])$ be two \acp{nn} sharing the same weights, activated by a smooth function and a \ac{cpwl} approximation of it, obtained with the method explained above. In practice $\pi[\rho]$ will be fixed, so we drop the subscript $n$ in this section. We are interested in recovering a maximal decomposition of $\Omega$ (resp. $\Gamma$) such that $\pi[u]$ is linear in these regions. We say that this decomposition is a mesh of $\Omega$ (resp. $\Gamma$) adapted to $\pi[u]$. Since $\pi[\rho]$ is fixed, this mesh depends only on $\pmb{\Theta}$. We introduce the notation $\tau_\Omega(\pmb{\Theta})$ and $\tau_\Gamma(\pmb{\Theta})$ to refer to these meshes, and $\tau(\pmb{\Theta})$ to refer to either of the two meshes. We equip each cell with a Gaussian quadrature to evaluate the loss function. Bounds for the integration error of our \ac{aq} are provided at the end of this section.

\subsection{Convexity of the mesh}

We observe that each neuron corresponds to a scalar-valued linear map followed by the composition by $\pi[\rho]$. The breakpoints of $\pi[\rho]$ define hyperplanes in the input space of each neuron. Indeed, the composition of a linear map $\pmb{x} \to \pmb{W} \pmb{x} + \pmb{b}$ by $\pi[u]$ is \ac{cpwl} and the boundaries of the regions where the composition is linear correspond to the equations $\pmb{W} \pmb{x} + \pmb{b} = \xi$, where $\xi$ are the breakpoints of $\pi[\rho]$.

Intuitively, we can obtain the adapted meshes $\tau(\pmb{\Theta})$ by considering the hyperplanes attached to every neuron of the network. The cells of the mesh are the largest sets of points that lie within exactly one piece of $\pi[\rho]$ at each neuron. This is why these regions are also called the \enquote{activation patterns} of a \ac{nn}, as we can label them by the pieces they belong to at each neuron \cite{hanin2019}.

The implementation of an algorithm that, given the parameters $\pmb{\Theta}$ of a \ac{nn}, the linearisation $\pi[\rho]$ and the input space $\Omega$, outputs $\tau_\Omega(\pmb{\Theta})$ has to be robust to a large variety of corner cases, as in practice the cells can be of extremely small measures, or take skewed shapes. Besides, we are concerned with the complexity of this algorithm as it is meant to be run at every iteration (or at a given frequency) during the training phase. Fortunately enough, the only cells in the mesh that may not be convex must be in contact with the boundary.

\begin{lemma}[Convexity of the mesh]
    \label{lem:convexity}
    Let $u: \RR^d \to \RR$ be a \ac{nn} with weights $\pmb{\Theta}$ activated by a \ac{cpwl} function and $\Omega \subset \RR^d$. If a cell in $\tau_\Omega(\pmb{\Theta})$ is not convex, it has to intersect with the boundary of $\Omega$. In particular, if $\Omega$ is convex, all the cells of $\tau_\Omega(\pmb{\Theta})$ are convex.
\end{lemma}

If $\Omega$ is not convex, we decompose $\Omega$ into a set of convex polytopes and construct an adapted mesh for each polytope independently. By \lem{convexity} these submeshes are convex. The mesh composed of all the cells of the submeshes is adapted to $u$ on $\Omega$ and contains convex cells only. Thus without loss of generality, we suppose that $\Omega$ is convex. Clipping a line by a polygon is made easier when the polygon is convex. Our method to build $\tau_\Omega(\pmb{\Theta})$ takes great advantage of the convexity of the mesh. Our algorithm is described in \alg{adaptiveMesh}.

\subsection{Representation of a linear region}
\label{subsect:representation}

Let $u_k$ denote the composition of the first $k$ layers of the network $u$, and $\tau^k(\pmb{\Theta})$ be the mesh adapted to $u_k$. We represent a linear region of $\tau^k(\pmb{\Theta})$ and its corresponding local expression by $(R, \pmb{W}_R, \pmb{b}_R)$, where $R \in \tau^k(\pmb{\Theta})$ is a cell of the mesh, $\pmb{W}_R \in \RR^{n_k \times d}$ and $\pmb{b}_R \in \RR^{n_k}$ are the coefficients of the restriction of $\pi[u_k]$ to $R$: $\pi[u_k]_{\left|R\right.}(\pmb{x}) = \pmb{W}_R \pmb{x} + \pmb{b}_R$.

\subsection{Initialisation of the algorithm}

We initialise the algorithm with the region $(\Omega, \pmb{I}_{d}, \pmb{0}_d)$, where $\pmb{I}_d$ is the identity matrix of size $d$ and $\pmb{0}_d$ is the zero vector of size $d$. The mesh $\{(\Omega, \pmb{I}_{d}, \pmb{0}_d)\}$ is adapted to $u_0$.

\subsection{Composition by a linear map}

Suppose that $(R, \pmb{W}_R, \pmb{b}_R)$ is adapted to $u_k$. The composition of $\pi[u_k]_{\left|R\right.}$ by the linear map $\pmb{\Theta}_{k+1}$ remains linear on $R$, only the coefficients of the map are changed. They become $\pmb{W}_{k+1} \pmb{W}_R$ and $\pmb{W}_{k+1} \pmb{b}_R + \pmb{b}_{k+1}$.

\subsection{Composition by an activation function}

Suppose that $(R, \pmb{W}_R, \pmb{b}_R)$ is adapted to $u_k$. We compose $\pi[u_k]_{\left|R\right.}$ by $\pi[\rho]$ componentwise. Let $\pmb{w}_i$ be the $i$-th row of $\pmb{W}_R$ and $b_i$ be the $i$-th coordinate of $\pmb{b}_R$. We need to find the pre-images of the breakpoints of $\pi[\rho]$, which amounts to consider the hyperplanes $\pmb{w}_i \pmb{x} + b_i = \xi_j$, where $\xi_j$ are the breakpoints of $\pi[\rho]$. We explain our method in detail in \app{algo_mesh}.

The process of mesh extraction in dimension two is illustrated in \fig{mesh_extraction}. In \fig{mesh_1}, the dashed line represents the orientation of the activation function. The plain lines depict two hyperplanes corresponding to two different breakpoints of $\pi[\rho]$. \fig{mesh_2} pictures the hyperplanes resulting from several neurons, thus oriented in different directions. In this example, there would be at least four neurons on the layer at hand. We notice that since the ranges of the neurons are different, all neurons may not activate all the pieces of $\pi[\rho]$, and thus they can give rise to different numbers of hyperplanes. The clipping operation is shown in \fig{mesh_3}, and the intersections of the hyperplanes against themselves are displayed in \fig{mesh_4}. In this example, the initial cell would be cut into $13$ subcells.

\begin{figure}
    \centering
    \subfloat[\label{fig:mesh_1}]{
        % \includesvg[width=0.2\linewidth]{mesh_1.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_1_svg-tex.pdf_tex}
    }
    \hfill
    \subfloat[\label{fig:mesh_2}]{
        % \includesvg[width=0.2\linewidth]{mesh_2.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_2_svg-tex.pdf_tex}
    }
    \hfill
    \subfloat[\label{fig:mesh_3}]{
        % \includesvg[width=0.2\linewidth]{mesh_3.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_3_svg-tex.pdf_tex}
    }
    \hfill
    \subfloat[\label{fig:mesh_4}]{
        % \includesvg[width=0.2\linewidth]{mesh_4.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_4_svg-tex.pdf_tex}
    }
    \caption{Example of a mesh extraction. \protect\subref{fig:mesh_1} Parallel hyperplanes associated with different breakpoints. \protect\subref{fig:mesh_2} Hyperplanes linked to a whole layer. \protect\subref{fig:mesh_3} Clipping hyperplanes with region boundary. \protect\subref{fig:mesh_4} Intersection of hyperplanes.}
    \label{fig:mesh_extraction}
\end{figure}

\subsection{Gaussian quadratures for convex polygons}

We decompose the integrals in the linear and bilinear forms on the cells of $\tau_\Omega(\pmb{\Theta})$ and $\tau_\Gamma(\pmb{\Theta})$. In these cells, the terms that only depend on the network $\pi[u]$ and its spatial derivatives are polynomials. As a consequence, the linear and bilinear forms involving $\pi[u]$ can be computed exactly using Gaussian quadratures on segments in dimension one, and polygons in dimension two.

In dimension one, the cells of the mesh are segments. Gaussian quadrature rules are known and made available through plenty of libraries.

However, in dimension two, the cells of the mesh can be arbitrary convex polygons. To handle the general case, one approach could consist in splitting each convex cell into triangles and then applying a known Gaussian quadrature for triangles. This approach is the least economical in terms of the number of quadrature points. At the opposite end of the spectrum, we could rely on Gaussian quadratures of reference $n$-gons. Still, the order of the mapping from the reference $n$-gon to the $n$-gon at hand is $n-2$, which makes the Jacobian of this mapping costly to evaluate. In this work, we strike a tradeoff and decompose each convex cell into a collection of triangles and convex quadrangles. We use a recursive algorithm to obtain this decomposition, as explained in \app{algo_polygon}. We refer to \cite{witherden2015} for numerically accurate symmetric quadrature rules for triangles and quadrangles.

\subsection{Alternatives}

There exist computationally cheaper numerical integration methods based on evaluations of the integrands at the vertices of the mesh. Indeed, using Stokes theorem one can transform surface integrals on polygons into line integrals on their edges, and in turn, into pointwise evaluations at their vertices \cite{chin2015}. This approach requires knowing the local expression of $\pi[u]$, that is the coefficients of the linear interpolation of $u$ on each cell.

We have conducted preliminary experiments using this approach but we have observed that in addition to being numerically unstable, the overall cost including the interpolation step is not lower. Indeed, finding the best-fitting plane that passes through given points involves the inversion of a $3 \times 3$ system on each cell. The coefficients of these matrices are the integrals over each cell of the polynomials $x^2$, $xy$, $y^2$, $x$, $y$ and $1$. In many cases, the cells take skewed shapes so these matrices can be extremely ill-conditioned.

\subsection{Analysis of the integration error}

Our proposed adaptive quadrature consists in approximating $\mathcal{J}(u)$ by a quadrature on the cells of the mesh adapted to $\pi[u]$. Conceptually, this is equivalent to choosing a suitable piecewise polynomial function defined on the mesh adapted to $\pi[u]$, and approximating $\mathcal{J}(u)$ by $\mathcal{J}_{\text{num}}(v)$. Here, $\mathcal{J}_{\text{num}}$ is a discrete version of $\mathcal{J}$ in which the integrals have been replaced by numerical quadratures. We break down the numerical integration error into two parts as follows
\[\abs{\mathcal{J}(u) - \mathcal{J}_{\text{num}}(v)} \leq \abs{\mathcal{J}(u) - \mathcal{J}(v)} + \abs{\mathcal{J}(v) - \mathcal{J}_{\text{num}}(v)}.\]
The second term is the numerical quadrature error and is commonplace in the field of the \ac{fem}. In particular, one can consider piecewise polynomial approximations of physical parameters, forcing term, and boundary conditions up to a given order and use a Gaussian quadrature that cancels the numerical quadrature error of the energy functional. The first term is the error incurred by the piecewise polynomial approximation of $u$ by $v$. Since we know that $\mathcal{J}$ is a variational energy functional that comes from bounded linear and bilinear forms, we can bound the linearisation error in the following way
\[\abs{\mathcal{J}(u) - \mathcal{J}(v)} \leq \frac{1}{2} \abs{a(u - v, u + v)} + \abs{\ell(u - v)} \leq \frac{1}{2} C_a \aabs{u - v}_a \aabs{u + v}_a + C_\ell \aabs{u - v}_\ell,\]
where $C_a$ and $C_\ell$ are the constants that bound $a$ and $\ell$, and $\aabs{\cdot}_a$ and $\aabs{\cdot}_\ell$ are integral norms. For instance, when we consider a strong formulation of a weak Poisson problem, $\aabs{\cdot}_a$ is an $H^2$ norm and $\aabs{\cdot}_\ell$ is an $L^2$ norm. In the case of a weak formulation, $\aabs{\cdot}_a$ and $\aabs{\cdot}_\ell$ are $H^1$ norms.

\subsubsection{Weak formulations}

In \prop{cpwl}, we showed that the $L^\infty$ norm of $\rho - \pi_n[\rho]$ decays as $n^{-2}$ and that of $\rho' - \pi_n[\rho]'$ decays as $n^{-1}$. We consider the two networks $u = \mathcal{N}(\pmb{\Theta}, \rho)$ and $\pi_n[u] = \mathcal{N}(\pmb{\Theta}, \pi_n[\rho])$, that only differ by their activation function. We show that we can bound $u - \pi_n[u]$ in terms of $\rho - \pi_n[\rho]$ in the following proposition. We define the $L^\infty$ norm of a vector-valued function as the maximum $L^\infty$ norm of each of its components.

\begin{proposition}[Bounds for the difference of neural networks]
    \label{prop:nns}
    Let $\Omega \subset \RR^d$ be a bounded domain, $f$, $g$ two continuous and almost everywhere differentiable functions such that $f - g$, $f'$ and $g'$ are bounded on $\RR$. We further assume that $f$ and $f'$ are Lipschitz continuous. Let $u = \mathcal{N}(\pmb{\Theta}, f)$ and $v = \mathcal{N}(\pmb{\Theta}, g)$ two \acp{nn} that only differ by their activation functions. Then there exists three constants $C_1$, $C_2$, $C_3$ such that
    \begin{align}
        \aabs{u - v}_{L^\infty(\Omega)}               & \leq C_1 \aabs{f - g}_{L^\infty(\RR)},                                     \label{eq:nns_1}  \\
        \aabs{\nabla u - \nabla v}_{L^\infty(\Omega)} & \leq C_2 \aabs{f - g}_{L^\infty(\RR)} + C_3 \aabs{f' - g'}_{L^\infty(\RR)}. \label{eq:nns_2}
    \end{align}
    The three constants depend on $\pmb{\Theta}$ and the Lipschitz constant of $f$. Additionally $C_2$ and $C_3$ depend on $\aabs{f'}_{L^\infty(\RR)}$, $\aabs{g'}_{L^\infty(\RR)}$ and the Lipschitz constant of $f'$.
\end{proposition}

The proposition above can be used to bound $\aabs{u - \pi[u]}_a$ and $\aabs{u - \pi[u]}_\ell$ whenever their integrands can be expressed as bivariate polynomials involving $u$ and $\nabla u$. Indeed, for all $n \geq 1$, $\rho$ and $\pi_n[\rho]$ are continuous and almost everywhere differentiable and their derivatives are bounded. Furthermore \prop{nns} showed that $\rho - \pi_n[\rho]$ and $\rho' - \pi_n[\rho]'$ are bounded. Since $\rho \in \mathcal{A}$, we infer that $\rho'$ and $\rho''$ are bounded, which implies that $\rho$ and $\rho'$ are Lipschitz continuous. Thus \prop{nns} can be applied and we have
\begin{align*}
    \aabs{u - \pi_n[u]}_{L^p(\Omega)} & \leq \frac{C \abs{\Omega}^{1/p}}{n^2}, & \aabs{\nabla u - \nabla \pi_n[u]}_{L^p(\Omega)} & \leq \frac{D \abs{\Omega}^{1/p}}{n},
\end{align*}
where $C = C_1 A_\infty(\rho) \kappa(\rho)^2 \aabs{\rho''}_{L^{1/2}(\RR)}$ and $D = C_2 A_\infty(\rho) \kappa(\rho) \aabs{\rho''}_{L^{1/2}(\RR)} + C_3 B_\infty(\rho) \kappa(\rho) \aabs{\rho''}_{L^1(\RR)}$. We used the fact that $n \geq \kappa(\rho)$ to bound $\kappa(\rho) / n$ in $D$. We conclude that $\aabs{u - \pi_n[u]}_{H^1(\Omega)}$ decays as $n^{-1}$.

\subsubsection{Strong formulations}

Since $\pi_n[u]$ is only \ac{cpwl}, the $H^2$ norm of $u - \pi_n[u]$ is not defined, which means that we cannot use the same approach as above to bound the integration error. Instead one could apply a similar method to that used in the \ac{fem}, where $u$ is approximated by a piecewise polynomial function that can be integrated exactly. The integration error would involve the mesh size.