In this work, we introduce a procedure to define adaptive quadratures for smooth \acp{nn} suitable for low-dimensional PDE discretisation. It relies on a decomposition of the domain into regions where the network is almost linear. We also show the importance of smoothness in the training process and propose a regularisation of the standard ReLU activation function.

We carry out the numerical analysis of the proposed framework to obtain upper bounds of the integration error. Numerical experimentation shows that our integration method helps make the convergence less noisy and quicker compared to Monte Carlo integration. We observe that the number of integration points needed to compute the loss function can be significantly reduced with our method and that our integration strategy is more robust to the initialisation of the network. We illustrated the benefit of our adaptive integration on the Poisson equation, but it can be extended to any \ac{pde} involving a symmetric and coercive bilinear form.

While \ac{nn} solvers are applied to increasingly intricate problems, they still lack reliability and theoretical guarantees before being used on real-world problems. Besides, most \ac{pinn} frameworks can only handle tensor-product domains, while the \ac{fem} can be applied to much more complex domains. The adaptive quadratures we propose, the error bounds we prove, and the seamless possibility to handle polygonal domains of our method represent a few steps further in bridging these gaps.

For the sake of reproducibility, our implementation of the proposed adaptive quadrature is openly available at \cite{magueresse2023}.