We carry out thorough numerical experiments to validate our integration method. Choosing the best hyperparameters to train a \ac{nn} usually requires trying several combinations and picking the one that leads to the lowest generalisation error. In addition to the usual learning rate and the number of epochs, we also need to tune the design of the loss function by specifying the penalty coefficient $\beta$, and the integration method. In the case of \ac{mc} integration, we select the number of points in the domain and on the boundary, as well as the resampling frequency. Our adaptive quadrature introduces a new set of hyperparameters, namely the number of pieces in the \ac{cpwl} approximation of the activation function, the order of the quadrature in each cell as well as the remeshing frequency.

In this work, we are also concerned with reducing the computational budget involved with the training of a \ac{nn} to solve a \ac{pde}. For this reason, we consider a two-hidden layer feed-forward \ac{nn} with $10$ neurons on both layers. The number of trainable parameters is $141$ and $151$ in dimensions one and two respectively. The optimiser is chosen as ADAM and all the networks are trained for $5000$ epochs. To compensate for the low number of training iterations, we use a slightly higher learning rate compared to common practice in the literature. We set it to $10^{-2}$ or $10^{-3}$, whereas it seems to usually lie between $10^{-4}$ and $10^{-3}$. We specify the learning rate for each experiment in the sections below.

In our experiments, we compare models that have been trained with the same average number of integration points. In the case of a network trained with \ac{mc}, the number of integration points is fixed during training, but this is not the case for the method we propose here. In practice, we observe that the number of points starts at an intermediate value, then slightly decreases, and finally rises above the initial value. We still take the average number of integration points across the whole training as a reference because it is the average number of times the model is evaluated at each epoch and is a good indicator of the cost of a numerical method.

We compare \ac{aq} against \ac{mc} integration to solve the Poisson equation in dimensions one and two, both in the strong and weak form using the Nitsche formulation. We report the relative $L^2$ norm of the pointwise difference, defined as
\[E(u, \hat{u}) = \frac{\aabs{\hat{u} - u}_{L^2(\Omega)}}{\aabs{\hat{u}}_{L^2(\Omega)}}.\]
The loss function corresponding to each problem is introduced in \subsect{study_cases} and the manufactured solutions we consider are presented in \subsect{solutions}. We give an outline of our experiments and short conclusions for each of them.
\begin{itemize}
    \item We conduct a set of exploratory experiments in \subsect{exploration} to identify the best learning rate and sampling frequency, as well as to evaluate and compare the robustness of \ac{aq} and \ac{mc} to the number of integration points. We find that regardless of the integration method, the strong Poisson problem brings about higher levels of noise during the training phase compared to the weak Poisson formulation. In general, \ac{aq} can reach lower errors than \ac{mc} and reduce this noise to a certain extent while using fewer integration points. This is especially true for the weak Poisson problem, where the convergence is noticeably faster and smoother with \ac{aq}.
    \item In \subsect{initialisation}, we show that models trained with our proposed integration method are more robust to parameter initialisation compared to \ac{mc}. Furthermore, \ac{aq} consistently leads to lower errors than \ac{mc} even when it relies on fewer integration points.
    \item The next round of experiments in \subsect{reduction} shows that it is possible to reduce the number of integration points by merging small regions with their neighbours.
    \item Finally in \subsect{rhombi} we solve a Poisson equation on a slightly more complex domain. We find that for a similar number of integration points, \ac{aq} reduces the error of \ac{mc} by $70\%$.
\end{itemize}
We provide a summary of our experiments in \subsect{summary} and discuss limitations and possible extensions of our method in \subsect{discussion}.

In all the following tables, $N_\Omega$ and $N_\Gamma$ stand for the number of integration points in the domain and on the boundary, respectively. The letters $P$ and $O$ denote the number of pieces in the approximation of the activation function by a CPWL function, and the order of the quadrature in each cell of the mesh adapted to the network.

%%%
\subsection{Study cases}
\label{subsect:study_cases}

In the following experiments, we consider two simple domains: the segment $\Omega_1 = \cc{-1}{+1}$ and the square $\Omega_2 = \cc{-1}{+1}^2$. We weakly enforce Dirichlet boundary conditions on the whole boundary of the domain. We introduce the bilinear and linear forms corresponding to each problem below.

\begin{description}
    \item[Strong formulation] Let $f: \Omega \to \RR$ be a function of class $\mathcal{C}^2(\Omega)$. We want to solve the equation $\Delta u = \Delta f$ with the Dirichlet boundary condition $u_{\vert \Gamma} = f_{\vert \Gamma}$. The original PINN formulation corresponds to the strong form of the PDE and is associated with the loss
        \[J(u) = \frac{1}{2} \|\Delta u - \Delta f\|_\Omega^2 + \frac{\beta}{2} \|u - f\|_\Gamma^2,\]
        where $\beta > 0$ is a weight factor for the boundary condition. We transform the squared $L^2$ norms as inner products and obtain the following bilinear and linear forms
        \begin{align*}
            a(u, v) & = \scal{\Delta u}{\Delta v}_\Omega + \beta \scal{u}{v}_\Gamma, \\
            \ell(v) & = \scal{\Delta f}{\Delta v}_\Omega + \beta \scal{f}{v}_\Gamma.
        \end{align*}
    \item[Weak formulation] We also solve a weak form of the Poisson equation and use the Nitsche method to obtain a symmetric bilinear form. In this setting, we only require $f$ to be of class $\mathcal{C}^1(\Omega)$. The bilinear and linear forms are as follows
        \begin{align*}
            a(u, v) & = \scal{\nabla u}{\nabla v}_\Omega - \scal{\nabla u \cdot n}{v}_\Gamma - \scal{u}{\nabla v \cdot n}_\Gamma + \beta \scal{u}{v}_\Gamma, \\
            \ell(v) & = \scal{-\Delta f}{v}_\Omega - \scal{f}{\nabla v \cdot n}_\Gamma + \beta \scal{f}{v}_\Gamma.
        \end{align*}
        Here, $n$ is the outward-pointing unit normal vector to $\Gamma$ and $\beta > 0$ is a coefficient large enough so that the bilinear form is coercive.
\end{description}

In all our experiments, the relative $L^2$ norm is evaluated with a quadrature of order $10$ on a $100$-cell uniform mesh in 1D and on a $100 \times 100$ uniform grid in 2D.

%%%
\subsection{Activation functions and forcing terms}
\label{subsect:solutions}

We solve the two problems we have just described with different forcing terms $f$. Throughout our experiments, all the problems that involve a given forcing term are solved using the same activation function. We report two groups of forcing terms and their corresponding activation in \tab{activations}. The notation $\sinc$ stands for the cardinal sine function $x \mapsto \sin(x)/x$, defined on $\RR \backslash \{0\}$ and continuously extended at zero by setting $\sinc(0) = 1$.

\begin{table}
    \centering
    \resizebox{0.5\linewidth}{!}{%
        \begin{tabular}{ccc}
            \toprule
            \multirow{2}{*}{Activation} & \multicolumn{2}{c}{Forcing term}                                   \\
            \cmidrule(l){2-3}
                                        & 1D                               & 2D                              \\
            \midrule
            $\abse$                     & $\sinc(3 \pi x)$                 & $\sinc(2 \pi x) \sinc(2 \pi y)$ \\
            $\tanh$                     & $\tanh(10 (x^2 - 0.5^2))$        & $\tanh(10 (x^2 + y^2 - 0.5^2))$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Forcing terms in dimensions one and two with their corresponding activation function.}
    \label{tab:activations}
\end{table}

%%%
\subsection{Relationship between integration method, learning rate and sampling frequency}
\label{subsect:exploration}

In this initial experiment, we focus on solving the four one-dimensional problems ($\abse$/$\tanh$, weak/strong) so that exploring a large space of hyperparameters remains computationally affordable. All the networks are trained from the same seed, which means that they are initialised from the same set of weights and biases, and the \ac{mc} integration points are sampled in the same way across all experiments.

We study the connection between the learning rate, the frequency at which we resample the integration points and the integration method. We set the learning rate to $10^{-2}$ or $10^{-3}$ and resample the integration points every $1$, $10$, $100$, $500$, $1000$ or $5000$ epochs. Since the networks are trained for $5000$ epochs, the last frequency corresponds to fixed integration points. We compare \ac{mc} with $50$ or $100$ points against \ac{aq} with several choices of quadrature order and number of pieces in $\pi[\rho]$. When the domain is one-dimensional, the boundary integrals reduce to evaluating the integrand at $\Gamma_1 = \{-1, +1\}$.

We also solve the weak Poisson problem in dimension two to further assess the effect of the integration method when the sampling frequency is fixed.

%%%
\subsubsection{Strong Poisson in 1D}

The relative $L^2$ norm after training the network for the two one-dimensional strong Poisson problems is shown in \tab{1D_pois_abs} and \tab{1D_pois_tanh}. We first observe that the sampling frequency and the number of integration points have a large influence on the final error. It is unclear whether increasing the number of integration points helps improve performance, as both trends appear for the two activation functions and integration methods.

Generally speaking, we note that \ac{aq} works best with high refresh rates (every $100$ or fewer epochs), especially when the number of pieces and the order of the quadrature are low. However, it is almost always the case that \ac{aq} reaches similar or better accuracies than \ac{mc} with fewer points. For instance, when the resampling rate is set to $10$ epochs and the learning rate to $10^{-2}$ for the $\abse$ activation function, a quadrature of order $2$ with $3$ pieces involves $57$ points on average and leads to a final error of $6.25 \times 10^{-3}$ while all of the \ac{mc} settings bring to higher errors. We notice that using a higher-order quadrature or more pieces in the CPWL approximation of the activation function reduces the error more consistently than increasing the number of \ac{mc} points, even though it is not systematic in either case.

To illustrate the learning phase, we plot the learning curve and final pointwise error in two different settings in \fig{1_l2rel} and \fig{2_l2rel}. We remark that the levels of noise in the two scenarios are very different. In the first case, the convergence with \ac{aq} is less perturbed by noise than that with \ac{mc}, and the error seems to decay faster with \ac{aq} than with \ac{mc}. In the second case, the training suffers from high levels of noise for all integration settings, and this is a representative example of most of the networks solving the strong Poisson problem.

To summarise, the strong Poisson problem brings about high levels of noise during the training phase for both integration methods. Still, \ac{aq} can reach lower errors than \ac{mc} and reduce this noise to a certain extent while using fewer integration points.

\begin{table}
    \centering
    \resizebox{0.8\linewidth}{!}{%
        \begin{tabular}{ccccccccc}
            \toprule
            \multirow{2}{*}{$\eta$}     &                          & \multirow{2}{*}{$N_\Omega$ ($P$, $O$)} & \multicolumn{6}{c}{$\nu$}                                                                                                                         \\
            \cmidrule(l){4-9}
                                        &                          &                                        & $1$                       & $10$                  & $100$                 & $500$                 & $1000$                & $5000$                \\
            \midrule
            \multirow{12}{*}{$10^{-2}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $2.65 \times 10^{-2}$     & $3.50 \times 10^{-2}$ & $6.81 \times 10^{-2}$ & $4.01 \times 10^{-1}$ & $1.76 \times 10^{-1}$ & $5.79 \times 10^{-2}$ \\
                                        &                          & $100$                                  & $2.97 \times 10^{-2}$     & $7.99 \times 10^{-2}$ & $3.18 \times 10^{-2}$ & $1.48 \times 10^{-2}$ & $4.24 \times 10^{-2}$ & $2.51 \times 10^{-1}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $25$ ($2$, $2$)                        & $1.11 \times 10^{-1}$     & $1.05 \times 10^{-1}$ & $3.00 \times 10^{-2}$ & $2.39 \times 10^{-2}$ & $4.47 \times 10^{-2}$ & $1.97 \times 10^{-0}$ \\
                                        &                          & $42$ ($2$, $5$)                        & $5.13 \times 10^{-2}$     & $3.18 \times 10^{-2}$ & $9.43 \times 10^{-2}$ & $6.39 \times 10^{-2}$ & $1.52 \times 10^{-2}$ & $1.62 \times 10^{-0}$ \\
                                        &                          & $75$ ($2$, $10$)                       & $1.84 \times 10^{-3}$     & $2.26 \times 10^{-2}$ & $1.22 \times 10^{-2}$ & $3.83 \times 10^{-2}$ & $7.25 \times 10^{-3}$ & $4.32 \times 10^{-1}$ \\
                                        &                          & $57$ ($3$, $2$)                        & $2.55 \times 10^{-2}$     & $6.25 \times 10^{-3}$ & $1.70 \times 10^{-1}$ & $2.60 \times 10^{-1}$ & $2.46 \times 10^{-1}$ & $3.04 \times 10^{-2}$ \\
                                        &                          & $84$ ($3$, $5$)                        & $1.08 \times 10^{-2}$     & $3.81 \times 10^{-2}$ & $1.32 \times 10^{-2}$ & $1.76 \times 10^{-3}$ & $3.76 \times 10^{-2}$ & $2.65 \times 10^{-2}$ \\
                                        &                          & $86$ ($5$, $2$)                        & $7.56 \times 10^{-2}$     & $6.18 \times 10^{-3}$ & $6.66 \times 10^{-3}$ & $3.58 \times 10^{-2}$ & $1.04 \times 10^{-2}$ & $1.09 \times 10^{-0}$ \\
            \midrule
            \multirow{12}{*}{$10^{-3}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $4.67 \times 10^{-2}$     & $1.15 \times 10^{-1}$ & $1.10 \times 10^{-1}$ & $1.06 \times 10^{-1}$ & $4.40 \times 10^{-1}$ & $5.77 \times 10^{-1}$ \\
                                        &                          & $100$                                  & $4.29 \times 10^{-2}$     & $1.17 \times 10^{-1}$ & $4.03 \times 10^{-2}$ & $3.18 \times 10^{-2}$ & $6.50 \times 10^{-3}$ & $1.44 \times 10^{-1}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $28$ ($2$, $2$)                        & $1.91 \times 10^{-2}$     & $4.64 \times 10^{-2}$ & $6.49 \times 10^{-2}$ & $4.35 \times 10^{-2}$ & $7.09 \times 10^{-2}$ & $2.41 \times 10^{-0}$ \\
                                        &                          & $44$ ($2$, $5$)                        & $1.80 \times 10^{-2}$     & $6.67 \times 10^{-3}$ & $1.88 \times 10^{-2}$ & $2.49 \times 10^{-2}$ & $2.81 \times 10^{-2}$ & $1.85 \times 10^{-0}$ \\
                                        &                          & $78$ ($2$, $10$)                       & $1.01 \times 10^{-2}$     & $6.75 \times 10^{-3}$ & $2.23 \times 10^{-2}$ & $5.03 \times 10^{-3}$ & $5.74 \times 10^{-3}$ & $1.26 \times 10^{-0}$ \\
                                        &                          & $59$ ($3$, $2$)                        & $3.87 \times 10^{-2}$     & $3.84 \times 10^{-2}$ & $1.32 \times 10^{-2}$ & $7.94 \times 10^{-2}$ & $1.71 \times 10^{-1}$ & $1.33 \times 10^{-2}$ \\
                                        &                          & $86$ ($3$, $5$)                        & $1.40 \times 10^{-2}$     & $3.87 \times 10^{-2}$ & $1.61 \times 10^{-2}$ & $9.66 \times 10^{-3}$ & $6.76 \times 10^{-3}$ & $7.50 \times 10^{-3}$ \\
                                        &                          & $89$ ($5$, $2$)                        & $6.32 \times 10^{-3}$     & $1.63 \times 10^{-2}$ & $3.50 \times 10^{-2}$ & $6.45 \times 10^{-3}$ & $3.03 \times 10^{-2}$ & $1.38 \times 10^{-2}$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of the relative $L^2$ norm for the strong Poisson problem with the $\abse$ activation, depending on learning rate, resampling frequency and integration method.}
    \label{tab:1D_pois_abs}
\end{table}

\begin{table}
    \centering
    \resizebox{0.8\linewidth}{!}{%
        \begin{tabular}{ccccccccc}
            \toprule
            \multirow{2}{*}{$\eta$}     &                          & \multirow{2}{*}{$N_\Omega$ ($P$, $O$)} & \multicolumn{6}{c}{$\nu$}                                                                                                                         \\
            \cmidrule(l){4-9}
                                        &                          &                                        & $1$                       & $10$                  & $100$                 & $500$                 & $1000$                & $5000$                \\
            \midrule
            \multirow{12}{*}{$10^{-2}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $2.62 \times 10^{-2}$     & $2.81 \times 10^{-2}$ & $7.34 \times 10^{-3}$ & $5.55 \times 10^{-2}$ & $2.75 \times 10^{-2}$ & $4.61 \times 10^{-1}$ \\
                                        &                          & $100$                                  & $2.96 \times 10^{-2}$     & $1.17 \times 10^{-2}$ & $2.81 \times 10^{-2}$ & $1.72 \times 10^{-3}$ & $9.02 \times 10^{-3}$ & $4.71 \times 10^{-2}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $38$ ($3$, $2$)                        & $2.86 \times 10^{-2}$     & $6.26 \times 10^{-3}$ & $5.95 \times 10^{-2}$ & $1.83 \times 10^{-2}$ & $4.11 \times 10^{-2}$ & $7.62 \times 10^{-0}$ \\
                                        &                          & $57$ ($3$, $5$)                        & $3.13 \times 10^{-3}$     & $8.46 \times 10^{-3}$ & $2.65 \times 10^{-3}$ & $2.19 \times 10^{-3}$ & $1.37 \times 10^{-3}$ & $1.54 \times 10^{-0}$ \\
                                        &                          & $118$ ($3$, $10$)                      & $1.73 \times 10^{-3}$     & $1.54 \times 10^{-4}$ & $1.74 \times 10^{-3}$ & $1.41 \times 10^{-2}$ & $6.11 \times 10^{-2}$ & $2.92 \times 10^{-0}$ \\
                                        &                          & $76$ ($5$, $2$)                        & $8.55 \times 10^{-4}$     & $2.06 \times 10^{-3}$ & $7.29 \times 10^{-3}$ & $1.72 \times 10^{-2}$ & $5.46 \times 10^{-3}$ & $5.06 \times 10^{-1}$ \\
                                        &                          & $118$ ($5$, $5$)                       & $4.62 \times 10^{-3}$     & $2.83 \times 10^{-3}$ & $4.58 \times 10^{-4}$ & $1.77 \times 10^{-3}$ & $4.02 \times 10^{-3}$ & $3.39 \times 10^{-1}$ \\
                                        &                          & $112$ ($7$, $2$)                       & $6.35 \times 10^{-3}$     & $2.97 \times 10^{-4}$ & $7.16 \times 10^{-3}$ & $4.23 \times 10^{-3}$ & $2.47 \times 10^{-2}$ & $3.97 \times 10^{-1}$ \\
            \midrule
            \multirow{12}{*}{$10^{-3}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $8.51 \times 10^{-2}$     & $3.04 \times 10^{-2}$ & $9.91 \times 10^{-3}$ & $1.36 \times 10^{-1}$ & $2.62 \times 10^{-2}$ & $8.61 \times 10^{-2}$ \\
                                        &                          & $100$                                  & $5.87 \times 10^{-2}$     & $6.39 \times 10^{-2}$ & $5.65 \times 10^{-2}$ & $7.68 \times 10^{-3}$ & $1.79 \times 10^{-2}$ & $9.28 \times 10^{-2}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $31$ ($3$, $2$)                        & $1.73 \times 10^{-0}$     & $8.18 \times 10^{-2}$ & $1.49 \times 10^{-2}$ & $1.64 \times 10^{-1}$ & $4.34 \times 10^{-0}$ & $7.94 \times 10^{-0}$ \\
                                        &                          & $49$ ($3$, $5$)                        & $3.42 \times 10^{-2}$     & $2.51 \times 10^{-2}$ & $2.82 \times 10^{-2}$ & $2.70 \times 10^{-2}$ & $2.23 \times 10^{-2}$ & $1.66 \times 10^{-0}$ \\
                                        &                          & $113$ ($3$, $10$)                      & $5.41 \times 10^{-3}$     & $1.57 \times 10^{-2}$ & $1.07 \times 10^{-2}$ & $5.37 \times 10^{-2}$ & $7.75 \times 10^{-3}$ & $1.56 \times 10^{-0}$ \\
                                        &                          & $75$ ($5$, $2$)                        & $2.83 \times 10^{-3}$     & $1.01 \times 10^{-2}$ & $8.73 \times 10^{-3}$ & $1.07 \times 10^{-3}$ & $4.03 \times 10^{-2}$ & $1.19 \times 10^{-0}$ \\
                                        &                          & $107$ ($5$, $5$)                       & $8.00 \times 10^{-3}$     & $9.14 \times 10^{-3}$ & $8.99 \times 10^{-3}$ & $4.01 \times 10^{-2}$ & $2.08 \times 10^{-3}$ & $1.04 \times 10^{-0}$ \\
                                        &                          & $107$ ($7$, $2$)                       & $1.09 \times 10^{-2}$     & $6.94 \times 10^{-3}$ & $4.83 \times 10^{-3}$ & $8.86 \times 10^{-3}$ & $8.29 \times 10^{-3}$ & $3.94 \times 10^{-1}$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of the relative $L^2$ norm for the strong Poisson problem with the $\tanh$ activation, depending on learning rate, resampling frequency and integration method.}
    \label{tab:1D_pois_tanh}
\end{table}

\begin{figure}
    \centering
    \subfloat[Learning curve\label{fig:1_l2}]{
        \includegraphics[width=0.45\linewidth]{1_l2.pdf}
    }
    \hfill
    \subfloat[Pointwise error\label{fig:1_rel}]{
        \includegraphics[width=0.45\linewidth]{1_rel.pdf}
    }
    \caption{Learning curve \protect\subref{fig:1_l2} and pointwise error \protect\subref{fig:1_rel} for the strong Poisson problem with the $\tanh$ activation. The learning rate is set to $10^{-2}$ and the points are resampled every $1$ epoch.}
    \label{fig:1_l2rel}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Learning curve\label{fig:2_l2}]{
        \includegraphics[width=0.45\linewidth]{2_l2.pdf}
    }
    \hfill
    \subfloat[Pointwise error\label{fig:2_rel}]{
        \includegraphics[width=0.45\linewidth]{2_rel.pdf}
    }
    \caption{Learning curve \protect\subref{fig:2_l2} and pointwise error \protect\subref{fig:2_rel} for the strong Poisson problem with the $\abse$ activation. The learning rate is $\eta = 10^{-2}$ and the points are resampled every $10$ epochs.}
    \label{fig:2_l2rel}
\end{figure}

%%%
\subsubsection{Weak Poisson in 1D}

We report the relative $L^2$ norm after training the network for the two one-dimensional weak Poisson problems in \tab{1D_poiw_abs} and \tab{1D_poiw_tanh}. We remark that the network does not converge to the expected solution when the integration points are not resampled (frequency of $5000$). Using more integration points alleviates this phenomenon, but we disregard this border case in the rest of this paragraph.

We observe that in many cases, and especially when the learning rate is set to $10^{-2}$, the error reached with \ac{aq} is one to two orders of magnitude lower than with \ac{mc}, even when \ac{aq} uses much fewer points than \ac{mc}. Provided that the number of integration points is sufficiently large, the final $L^2$ norm varies very little with the sampling frequency when the network is trained with \ac{aq}. When relying on \ac{mc}, the variance of the final error is much higher and the dependence between these the variance and the number of integration points is unclear.

When the sampling frequency is high enough (every $100$ or fewer epochs), we observe that the number of integration points can very often be reduced while keeping the same levels of error. For instance, in the problem with the $\tanh$ function with a learning rate of $10^{-2}$ and a resampling frequency of $10$, shifting from $7$ pieces to $5$ or $3$ pieces while keeping order $2$ does not deteriorate the performance of \ac{aq}. However, reducing the number of \ac{mc} integration points increases the error in most cases.

To further compare the training phase with \ac{aq} and \ac{mc}, we plot the learning curve and pointwise error for the $\tanh$ activation in \fig{3_l2rel} and for the $\abse$ activation in \fig{4_l2rel}. In both cases, we remark that the convergence of the network is significantly smoother with \ac{aq} than with \ac{mc}, even with few integration points. Moreover, it is clear that the $L^2$ norm decreases in the case of \ac{aq}, whereas it seems to plateau with \ac{mc}. Finally, it is interesting to note that the learning curves follow the same trend at the beginning of the training. After a few hundred epochs, the learning curves corresponding to \ac{aq} keep on decreasing while the ones corresponding to \ac{mc} break away from this trend and start to suffer from high levels of noise.

In conclusion, solving the weak Poisson problem with \ac{aq} is computationally cheaper and shows a considerably faster and smoother convergence than with \ac{mc}.

\begin{table}
    \centering
    \resizebox{0.8\linewidth}{!}{%
        \begin{tabular}{ccccccccc}
            \toprule
            \multirow{2}{*}{$\eta$}     &                          & \multirow{2}{*}{$N_\Omega$ ($P$, $O$)} & \multicolumn{6}{c}{$\nu$}                                                                                                                         \\
            \cmidrule(l){4-9}
                                        &                          &                                        & $1$                       & $10$                  & $100$                 & $500$                 & $1000$                & $5000$                \\
            \midrule
            \multirow{12}{*}{$10^{-2}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $4.82 \times 10^{-2}$     & $1.69 \times 10^{-1}$ & $1.41 \times 10^{-0}$ & $5.08 \times 10^{-0}$ & $8.96 \times 10^{-0}$ & $1.82 \times 10^{+3}$ \\
                                        &                          & $100$                                  & $4.29 \times 10^{-1}$     & $6.39 \times 10^{-1}$ & $1.68 \times 10^{-0}$ & $1.40 \times 10^{-0}$ & $2.11 \times 10^{+1}$ & $7.26 \times 10^{+3}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $24$ ($2$, $2$)                        & $2.87 \times 10^{-2}$     & $2.05 \times 10^{-1}$ & $8.80 \times 10^{-2}$ & $1.14 \times 10^{-0}$ & $3.54 \times 10^{+2}$ & $4.43 \times 10^{+5}$ \\
                                        &                          & $43$ ($2$, $5$)                        & $1.04 \times 10^{-2}$     & $1.00 \times 10^{-2}$ & $4.90 \times 10^{-2}$ & $1.81 \times 10^{-0}$ & $2.04 \times 10^{+2}$ & $5.28 \times 10^{+5}$ \\
                                        &                          & $88$ ($2$, $10$)                       & $8.51 \times 10^{-3}$     & $8.24 \times 10^{-3}$ & $6.89 \times 10^{-2}$ & $2.05 \times 10^{-1}$ & $1.10 \times 10^{-0}$ & $3.11 \times 10^{+3}$ \\
                                        &                          & $61$ ($3$, $2$)                        & $8.12 \times 10^{-3}$     & $1.37 \times 10^{-2}$ & $1.59 \times 10^{-2}$ & $5.13 \times 10^{-2}$ & $4.83 \times 10^{-0}$ & $5.49 \times 10^{+3}$ \\
                                        &                          & $90$ ($3$, $5$)                        & $8.74 \times 10^{-3}$     & $1.03 \times 10^{-2}$ & $1.02 \times 10^{-2}$ & $8.71 \times 10^{-3}$ & $7.79 \times 10^{-3}$ & $3.47 \times 10^{+2}$ \\
                                        &                          & $91$ ($5$, $2$)                        & $9.67 \times 10^{-3}$     & $1.30 \times 10^{-2}$ & $6.35 \times 10^{-3}$ & $7.78 \times 10^{-3}$ & $1.49 \times 10^{-2}$ & $9.35 \times 10^{+1}$ \\
            \midrule
            \multirow{12}{*}{$10^{-3}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $1.91 \times 10^{-1}$     & $1.70 \times 10^{-1}$ & $9.88 \times 10^{-1}$ & $4.93 \times 10^{-0}$ & $4.97 \times 10^{-0}$ & $1.00 \times 10^{+1}$ \\
                                        &                          & $100$                                  & $2.73 \times 10^{-1}$     & $2.32 \times 10^{-1}$ & $1.37 \times 10^{-0}$ & $8.35 \times 10^{-1}$ & $6.46 \times 10^{-1}$ & $6.55 \times 10^{+1}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $26$ ($2$, $2$)                        & $5.67 \times 10^{-2}$     & $3.50 \times 10^{-2}$ & $5.15 \times 10^{-2}$ & $1.26 \times 10^{-0}$ & $3.02 \times 10^{-0}$ & $2.32 \times 10^{+3}$ \\
                                        &                          & $46$ ($2$, $5$)                        & $8.66 \times 10^{-3}$     & $7.24 \times 10^{-3}$ & $1.98 \times 10^{-2}$ & $4.13 \times 10^{-3}$ & $1.70 \times 10^{-1}$ & $1.78 \times 10^{+3}$ \\
                                        &                          & $93$ ($2$, $10$)                       & $5.87 \times 10^{-3}$     & $1.04 \times 10^{-2}$ & $5.71 \times 10^{-3}$ & $5.57 \times 10^{-3}$ & $5.17 \times 10^{-3}$ & $5.28 \times 10^{+2}$ \\
                                        &                          & $66$ ($3$, $2$)                        & $2.30 \times 10^{-2}$     & $7.99 \times 10^{-3}$ & $1.42 \times 10^{-2}$ & $1.19 \times 10^{-2}$ & $3.26 \times 10^{-2}$ & $3.90 \times 10^{-0}$ \\
                                        &                          & $96$ ($3$, $5$)                        & $1.97 \times 10^{-2}$     & $1.79 \times 10^{-0}$ & $3.78 \times 10^{-2}$ & $5.46 \times 10^{-3}$ & $5.53 \times 10^{-3}$ & $6.42 \times 10^{-3}$ \\
                                        &                          & $90$ ($5$, $2$)                        & $1.78 \times 10^{-2}$     & $6.80 \times 10^{-3}$ & $6.61 \times 10^{-3}$ & $1.16 \times 10^{-2}$ & $9.72 \times 10^{-3}$ & $1.83 \times 10^{-0}$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of the relative $L^2$ norm for the weak Poisson problem with the $\abse$ activation, depending on learning rate, resampling frequency and integration method.}
    \label{tab:1D_poiw_abs}
\end{table}

\begin{table}
    \centering
    \resizebox{0.8\linewidth}{!}{%
        \begin{tabular}{ccccccccc}
            \toprule
            \multirow{2}{*}{$\eta$}     &                          & \multirow{2}{*}{$N_\Omega$ ($P$, $O$)} & \multicolumn{6}{c}{$\nu$}                                                                                                                         \\
            \cmidrule(l){4-9}
                                        &                          &                                        & $1$                       & $10$                  & $100$                 & $500$                 & $1000$                & $5000$                \\
            \midrule
            \multirow{12}{*}{$10^{-2}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $7.67 \times 10^{-2}$     & $4.99 \times 10^{-1}$ & $4.64 \times 10^{-1}$ & $2.86 \times 10^{-0}$ & $8.05 \times 10^{-0}$ & $3.45 \times 10^{+2}$ \\
                                        &                          & $100$                                  & $1.75 \times 10^{-1}$     & $4.92 \times 10^{-1}$ & $9.31 \times 10^{-1}$ & $1.55 \times 10^{-0}$ & $1.64 \times 10^{-0}$ & $4.78 \times 10^{-0}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $40$ ($3$, $2$)                        & $3.87 \times 10^{-2}$     & $1.49 \times 10^{-2}$ & $1.14 \times 10^{-1}$ & $1.34 \times 10^{-0}$ & $1.19 \times 10^{+1}$ & $4.21 \times 10^{+2}$ \\
                                        &                          & $46$ ($3$, $5$)                        & $8.40 \times 10^{-1}$     & $3.06 \times 10^{-2}$ & $3.47 \times 10^{-2}$ & $2.19 \times 10^{-2}$ & $1.77 \times 10^{-0}$ & $6.64 \times 10^{+1}$ \\
                                        &                          & $110$ ($3$, $10$)                      & $7.89 \times 10^{-3}$     & $4.56 \times 10^{-3}$ & $8.17 \times 10^{-3}$ & $5.09 \times 10^{-2}$ & $1.45 \times 10^{-0}$ & $3.92 \times 10^{+2}$ \\
                                        &                          & $84$ ($5$, $2$)                        & $7.41 \times 10^{-3}$     & $1.36 \times 10^{-2}$ & $3.41 \times 10^{-3}$ & $4.18 \times 10^{-2}$ & $4.27 \times 10^{-1}$ & $4.34 \times 10^{+2}$ \\
                                        &                          & $108$ ($5$, $5$)                       & $3.73 \times 10^{-3}$     & $4.77 \times 10^{-3}$ & $5.53 \times 10^{-3}$ & $9.31 \times 10^{-3}$ & $5.17 \times 10^{-1}$ & $3.05 \times 10^{+2}$ \\
                                        &                          & $98$ ($7$, $2$)                        & $3.75 \times 10^{-3}$     & $9.94 \times 10^{-3}$ & $3.71 \times 10^{-3}$ & $5.95 \times 10^{-2}$ & $1.67 \times 10^{-1}$ & $1.66 \times 10^{+2}$ \\
            \midrule
            \multirow{12}{*}{$10^{-3}$} & \multirow{3}{*}{\ac{mc}} & $50$                                   & $2.52 \times 10^{-1}$     & $2.54 \times 10^{-1}$ & $9.53 \times 10^{-1}$ & $2.76 \times 10^{-0}$ & $3.42 \times 10^{-0}$ & $1.47 \times 10^{+1}$ \\
                                        &                          & $100$                                  & $3.16 \times 10^{-1}$     & $3.77 \times 10^{-1}$ & $1.34 \times 10^{-0}$ & $2.56 \times 10^{-0}$ & $1.77 \times 10^{-0}$ & $1.87 \times 10^{-0}$ \\
            \cmidrule(l){3-9}
                                        & \multirow{5}{*}{\ac{aq}} & $21$ ($3$, $2$)                        & $1.59 \times 10^{-0}$     & $1.59 \times 10^{-0}$ & $1.21 \times 10^{-0}$ & $5.16 \times 10^{-1}$ & $5.55 \times 10^{-0}$ & $6.10 \times 10^{+1}$ \\
                                        &                          & $26$ ($3$, $5$)                        & $5.44 \times 10^{-1}$     & $1.66 \times 10^{-0}$ & $3.47 \times 10^{-1}$ & $3.43 \times 10^{-2}$ & $1.54 \times 10^{-1}$ & $1.86 \times 10^{-0}$ \\
                                        &                          & $57$ ($3$, $10$)                       & $6.52 \times 10^{-1}$     & $6.20 \times 10^{-1}$ & $5.57 \times 10^{-2}$ & $4.92 \times 10^{-2}$ & $5.12 \times 10^{-2}$ & $3.86 \times 10^{+1}$ \\
                                        &                          & $50$ ($5$, $2$)                        & $5.41 \times 10^{-2}$     & $3.90 \times 10^{-2}$ & $7.87 \times 10^{-2}$ & $5.77 \times 10^{-2}$ & $2.47 \times 10^{-2}$ & $5.02 \times 10^{+1}$ \\
                                        &                          & $71$ ($5$, $5$)                        & $2.32 \times 10^{-2}$     & $2.31 \times 10^{-2}$ & $4.37 \times 10^{-2}$ & $6.62 \times 10^{-2}$ & $5.95 \times 10^{-2}$ & $2.18 \times 10^{+1}$ \\
                                        &                          & $41$ ($7$, $2$)                        & $2.35 \times 10^{-1}$     & $1.81 \times 10^{-1}$ & $2.24 \times 10^{-1}$ & $1.08 \times 10^{-1}$ & $2.78 \times 10^{-1}$ & $1.50 \times 10^{-0}$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of the relative $L^2$ norm for the weak Poisson problem with the $\tanh$ activation, depending on learning rate, resampling frequency and integration method.}
    \label{tab:1D_poiw_tanh}
\end{table}

\begin{figure}
    \centering
    \subfloat[Learning curve\label{fig:3_l2}]{
        \includegraphics[width=0.45\linewidth]{3_l2.pdf}
    }
    \hfill
    \subfloat[Pointwise error\label{fig:3_rel}]{
        \includegraphics[width=0.45\linewidth]{3_rel.pdf}
    }
    \caption{Learning curve \protect\subref{fig:3_l2} and pointwise error \protect\subref{fig:3_rel} for the weak Poisson problem with the $\tanh$ activation. The learning rate is $\eta = 10^{-2}$ and the points are resampled every $10$ epochs.}
    \label{fig:3_l2rel}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Learning curve\label{fig:4_l2}]{
        \includegraphics[width=0.45\linewidth]{4_l2.pdf}
    }
    \hfill
    \subfloat[Pointwise error\label{fig:4_rel}]{
        \includegraphics[width=0.45\linewidth]{4_rel.pdf}
    }
    \caption{Learning curve \protect\subref{fig:4_l2} and pointwise error \protect\subref{fig:4_rel} for the weak Poisson problem with the $\abse$ activation. The learning rate is set to $10^{-2}$ and the points are resampled every $10$ epochs.}
    \label{fig:4_l2rel}
\end{figure}

%%%
\subsubsection{Weak Poisson in 2D}

Based on the experiments in dimension one, we conclude that the integration points should be resampled around every $10$ epochs and the learning rate should be set to $10^{-2}$ to obtain optimal performances with both integration methods. We keep these hyperparameters in the rest of our experiments.

We find that \ac{aq} can reach similar or lower errors with less than half the number of points \ac{mc} needs. With the $\abse$ activation, \ac{aq} with fewer than $1000$ points and \ac{mc} with $10000$ points reach comparable performances. In the case of the $\tanh$ activation, \ac{aq} with around $1660$ points wins over \ac{mc} with $10000$ points. However, we notice that increasing the order of the quadrature or the number of pieces does not significantly reduce the relative $L^2$ norm.

\fig{5_rel} and \fig{6_rel} display the pointwise error between the ground truth and the solutions obtained with \ac{mc} and \ac{aq}. In the case of $\tanh$, we remark that \ac{mc} struggles to model the transition between the high and low regions. Similarly for $\abse$, \ac{mc} does not properly handle the region around the origin, where the solution shows larger variations. In both cases, the pointwise error for the model trained with \ac{aq} is much more homogeneous across the whole domain.

\begin{table}
    \centering
    \subfloat[$\abse$\label{tab:2D_poiw_abs}]{
        \resizebox{0.4\linewidth}{!}{%
            \begin{tabular}{ccc}
                \toprule
                Integration         & ($N_\Omega$, $N_\Gamma$) ($P$, $O$) & $L^2$ norm            \\
                \midrule
                \multirow{3}{*}{MC} & ($1000$, $100$)                     & $2.35 \times 10^{-1}$ \\
                                    & ($5000$, $500$)                     & $1.43 \times 10^{-1}$ \\
                                    & ($10000$, $1000$)                   & $1.10 \times 10^{-1}$ \\
                \cmidrule(l){2-3}
                \multirow{3}{*}{AQ} & ($927$, $85$) ($2$, $2$)            & $1.03 \times 10^{-1}$ \\
                                    & ($2681$, $144$) ($2$, $5$)          & $3.58 \times 10^{-2}$ \\
                                    & ($4041$, $160$) ($3$, $2$)          & $6.40 \times 10^{-2}$ \\
                \bottomrule
            \end{tabular}
        }
    }
    \qquad
    \subfloat[$\tanh$\label{tab:2D_poiw_tanh}]{
        \resizebox{0.4\linewidth}{!}{%
            \begin{tabular}{ccc}
                \toprule
                Integration         & ($N_\Omega$, $N_\Gamma$) ($P$, $O$) & $L^2$ norm            \\
                \midrule
                \multirow{3}{*}{MC} & ($1000$, $100$)                     & $1.33 \times 10^{-1}$ \\
                                    & ($5000$, $500$)                     & $8.51 \times 10^{-2}$ \\
                                    & ($10000$, $1000$)                   & $6.61 \times 10^{-2}$ \\
                \cmidrule(l){2-3}
                \multirow{3}{*}{AQ} & ($1660$, $111$) ($3$, $2$)          & $5.43 \times 10^{-2}$ \\
                                    & ($2964$, $154$) ($3$, $5$)          & $4.47 \times 10^{-2}$ \\
                                    & ($3465$, $175$) ($5$, $2$)          & $4.34 \times 10^{-2}$ \\
                \bottomrule
            \end{tabular}
        }
    }
    \caption{Relative $L^2$ norm for the weak Poisson problems in 2D with the $\abse$ \protect\subref{tab:2D_poiw_abs} and $\tanh$ \protect\subref{tab:2D_poiw_tanh} activations for various integration hyperparameters.}
    \label{tab:reduce_merge}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{5_rel.png}
    \caption{Comparison of the pointwise error for the weak Poisson problem with the $\tanh$ activation. The learning rate is $\eta = 10^{-2}$ and the points are resampled every $10$ epochs. \ac{mc} is trained with $5000$ points and \ac{aq} with $5$ pieces and order $3$ ($2964$ points).}
    \label{fig:5_rel}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{6_rel.png}
    \caption{Comparison of the pointwise error for the weak Poisson problem with the $\abse$ activation. The learning rate is set to $10^{-2}$ and the points are resampled every $10$ epochs. \ac{mc} is trained with $5000$ points and \ac{aq} with $2$ pieces and order $5$ ($2681$ points).}
    \label{fig:6_rel}
\end{figure}

%%%
\subsection{Robustness to initialisation}
\label{subsect:initialisation}

In this second round of experiments, we study the robustness of our integration method to the initialisation of the network. We solve the weak Poisson problems in dimensions one and two. The learning rate is set to $10^{-2}$ and we resample the integration points every $10$ epochs. We report the minimum, maximum, average and standard deviation of the final $L^2$ norm on $10$ random initialisations, as well as the average training time in \tab{init_poiw_abs} and \tab{init_poiw_tanh} for the $\abse$ and $\tanh$ activations respectively. We use the same seeds for \ac{mc} and \ac{aq} in both experiments. The number of integration points for \ac{mc} is chosen so that it is larger than the maximum number of points for the same experiments run with \ac{aq}. The exact choices of quadrature order, number of pieces and integration points are reported in the corresponding tables.

We find that \ac{aq} has a consistently lower average $L^2$ norm than \ac{mc}. By comparing the standard deviation of the error, we infer that the solutions obtained by \ac{aq} are less dependent on the network initialisation compared to \ac{mc}. Besides, we observe that the distributions of the errors obtained with \ac{aq} and \ac{mc} do not overlap: the maximum error with \ac{aq} is lower than the minimum error with \ac{mc}. More importantly, we find that \ac{aq} enables a reduction of the number of integration points while keeping similar or shorter training times.

We conclude that our proposed integration method is more robust to initialisation than \ac{mc} and that it obtains higher accuracies than \ac{mc} with fewer integration points.

\begin{table}
    \centering
    \subfloat[$\abse$.\label{tab:init_poiw_abs}]{
        \resizebox{0.8\linewidth}{!}{%
            \begin{tabular}{cccccccc}
                \toprule
                Dim.               &         & ($N_\Omega$, $N_\Gamma$) ($P$, $O$) & Time (s) & min.                  & avg.                  & std.                  & max.                  \\
                \midrule
                \multirow{2}{*}{1} & \ac{mc} & ($100$, $2$)                        & $19.2$   & $1.98 \times 10^{-1}$ & $4.45 \times 10^{-1}$ & $2.45 \times 10^{-1}$ & $8.31 \times 10^{-1}$ \\
                                   & \ac{aq} & ($93$, $2$) ($3$, $5$)              & $19.5$   & $2.91 \times 10^{-3}$ & $7.88 \times 10^{-3}$ & $3.74 \times 10^{-3}$ & $1.44 \times 10^{-2}$ \\
                \midrule
                \multirow{2}{*}{2} & \ac{mc} & ($10000$, $1000$)                   & $486.0$  & $6.09 \times 10^{-2}$ & $1.09 \times 10^{-1}$ & $2.63 \times 10^{-2}$ & $1.44 \times 10^{-1}$ \\
                                   & \ac{aq} & ($7278$, $230$) ($3$, $5$)          & $413.5$  & $3.05 \times 10^{-2}$ & $6.59 \times 10^{-2}$ & $2.94 \times 10^{-2}$ & $1.07 \times 10^{-1}$ \\
                \bottomrule
            \end{tabular}
        }
    }
    \hfill
    \subfloat[$\tanh$.\label{tab:init_poiw_tanh}]{
        \resizebox{0.8\linewidth}{!}{%
            \begin{tabular}{cccccccc}
                \toprule
                Dim.               &         & ($N_\Omega$, $N_\Gamma$) ($P$, $O$) & Time (s) & min.                  & avg.                  & std.                  & max.                  \\
                \midrule
                \multirow{2}{*}{1} & \ac{mc} & ($200$, $2$)                        & $18.8$   & $1.12 \times 10^{-1}$ & $3.73 \times 10^{-1}$ & $3.96 \times 10^{-1}$ & $1.47 \times 10^{-0}$ \\
                                   & \ac{aq} & ($126$, $2$) ($5$, $10$)            & $18.2$   & $3.08 \times 10^{-3}$ & $9.00 \times 10^{-3}$ & $5.27 \times 10^{-3}$ & $2.19 \times 10^{-2}$ \\
                \midrule
                \multirow{2}{*}{2} & \ac{mc} & ($5000$, $500$)                     & $449.4$  & $6.44 \times 10^{-2}$ & $9.63 \times 10^{-2}$ & $3.77 \times 10^{-2}$ & $1.81 \times 10^{-1}$ \\
                                   & \ac{aq} & ($3224$, $167$) ($3$, $5$)          & $391.3$  & $3.50 \times 10^{-2}$ & $4.50 \times 10^{-2}$ & $6.59 \times 10^{-3}$ & $5.59 \times 10^{-2}$ \\
                \bottomrule
            \end{tabular}
        }
    }
    \caption{Distribution of the relative $L^2$ norm for the weak Poisson problem with the $\abse$ \protect\subref{tab:init_poiw_abs} and $\tanh$ \protect\subref{tab:init_poiw_tanh} activations on $10$ random initialisations. The learning rate is set to $10^{-2}$ and the points are resampled every $10$ epochs.}
    \label{tab:init_poiw}
\end{table}

%%%
\subsection{Reduction of the number of integration points}
\label{subsect:reduction}

In this set of experiments, we want to show that it is possible to further reduce the number of integration points generated by \ac{aq} by merging small regions with their neighbours, without sacrificing the accuracy of our method. We keep the same hyperparameters as in the previous paragraph and focus on the two-dimensional problems.

Every time the mesh is updated, we compute the median of the size of the cells. We then identify the cells that are smaller than a given fraction of this median and merge them one by one with their neighbours. We always merge a cell with the largest of its neighbours. In dimension two, after all the cells are merged in this way, the mesh needs an extra post-processing step in order to account for non-convex cells. Indeed, the union of two convex polygons may be non-convex. We split non-convex aggregated cells with the ear clipping algorithm \cite{meisters1975}, and apply our splitting strategy to all resulting cells to obtain triangles and convex quadrangles.

We experiment with several merging thresholds and report our findings in \tab{merging}. In dimension one, we find that merging regions up to $50\%$ of the median of the regions size does not significantly affect the error while allowing for a sizeable reduction of the number of integration points. In the two-dimensional case, raising the merging threshold above $25\%$ harms the performance of the $\abse$ network, but not that of the $\tanh$ network, as the number of integration points does not decrease.

It appears that increasing the merging threshold does not always reduce the number of integration points. Indeed, the aggregated regions may become less and less convex, so they will need to be split into several convex regions.

We illustrate the kind of meshes that a network generates and how they are merged in \fig{7_meshes}. These meshes are obtained from a trained $\tanh$ network solving the weak Poisson problem. We found that the meshes from the previous round of experiments were easier to interpret, so we selected one of the $10$ corresponding models. The boundary of the circular region can be easily identified. The lines that connect two sides of the square domain come from the first layer whereas the other lines come from the second layer. We observe that non-convex regions appear when small regions are merged.

\begin{table}
    \centering
    \subfloat[$\abse$. \ac{aq} has the following settings: $5$ pieces with order $10$ in 1D, $3$ pieces with order $2$ in 2D.\label{tab:merging_abse}]{
        \resizebox{0.45\linewidth}{!}{%
            \begin{tabular}{cccc}
                \toprule
                Dim.               &                          & ($N_\Omega$, $N_\Gamma$) (Threshold) & $L^2$ norm            \\
                \midrule
                \multirow{5}{*}{1} & \multirow{2}{*}{\ac{mc}} & ($200$, $2$)                         & $6.39 \times 10^{-1}$ \\
                                   &                          & ($100$, $2$)                         & $4.55 \times 10^{-1}$ \\
                \cmidrule(l){3-4}
                                   & \multirow{3}{*}{\ac{aq}} & ($290$, $2$) ($0\%$)                 & $8.25 \times 10^{-3}$ \\
                                   &                          & ($198$, $2$) ($50\%$)                & $7.82 \times 10^{-3}$ \\
                                   &                          & ($144$, $2$) ($100\%$)               & $4.41 \times 10^{-2}$ \\
                \midrule
                \multirow{5}{*}{2} & \multirow{2}{*}{\ac{mc}} & ($5000$, $500$)                      & $1.43 \times 10^{-1}$ \\
                                   &                          & ($1000$, $100$)                      & $2.35 \times 10^{-1}$ \\
                \cmidrule(l){3-4}
                                   & \multirow{3}{*}{\ac{aq}} & ($4041$, $158$) ($0\%$)              & $6.40 \times 10^{-2}$ \\
                                   &                          & ($3562$, $148$) ($10\%$)             & $7.27 \times 10^{-2}$ \\
                                   &                          & ($3217$, $132$) ($25\%$)             & $2.78 \times 10^{-2}$ \\
                                   &                          & ($2809$, $115$) ($50\%$)             & $1.41 \times 10^{-1}$ \\
                \bottomrule
            \end{tabular}
        }
    }
    \hfill
    \subfloat[$\tanh$. \ac{aq} has the following settings: $5$ pieces with order $10$ in 1D, $5$ pieces with order $5$ in 2D.\label{tab:merging_tanh}]{
        \resizebox{0.45\linewidth}{!}{%
            \begin{tabular}{cccc}
                \toprule
                Dim.               &                          & ($N_\Omega$, $N_\Gamma$) (Threshold) & $L^2$ norm            \\
                \midrule
                \multirow{5}{*}{1} & \multirow{2}{*}{\ac{mc}} & ($200$, $2$)                         & $2.34 \times 10^{-1}$ \\
                                   &                          & ($100$, $2$)                         & $4.92 \times 10^{-1}$ \\
                \cmidrule(l){3-4}
                                   & \multirow{3}{*}{\ac{aq}} & ($138$, $2$) ($0\%$)                 & $3.08 \times 10^{-3}$ \\
                                   &                          & ($94$, $2$) ($50\%$)                 & $4.18 \times 10^{-3}$ \\
                                   &                          & ($85$, $2$) ($100\%$)                & $2.72 \times 10^{-2}$ \\
                \midrule
                \multirow{5}{*}{2} & \multirow{2}{*}{\ac{mc}} & ($10000$, $1000$)                    & $6.61 \times 10^{-2}$ \\
                                   &                          & ($5000$, $500$)                      & $8.51 \times 10^{-2}$ \\
                \cmidrule(l){3-4}
                                   & \multirow{3}{*}{\ac{aq}} & ($7798$, $259$) ($0\%$)              & $5.02 \times 10^{-2}$ \\
                                   &                          & ($6895$, $218$) ($10\%$)             & $3.96 \times 10^{-2}$ \\
                                   &                          & ($8336$, $244$) ($25\%$)             & $5.26 \times 10^{-2}$ \\
                                   &                          & ($9088$, $204$) ($50\%$)             & $7.19 \times 10^{-2}$ \\
                \bottomrule
            \end{tabular}
        }
    }
    \caption{Relative $L^2$ norm for the weak Poisson problems with the $\abse$ \protect\subref{tab:merging_abse} and $\tanh$ \protect\subref{tab:merging_tanh} activations for various region merging thresholds.}
    \label{tab:merging}
\end{table}

\begin{figure}
    \centering
    \subfloat[$0\%$: $418$ regions. \label{fig:7_0.0}]{
        \includegraphics[width=0.45\linewidth]{7_mesh_0.0.pdf}
    }
    \hfill
    \subfloat[$25\%$: $294$ regions. \label{fig:7_0.25}]{
        \includegraphics[width=0.45\linewidth]{7_mesh_0.25.pdf}
    }
    \caption{Final mesh, number of cells and number of integration points at several region merging thresholds for the weak Poisson problem and the $\tanh$ activation. The $\tanh$ activation is cut into $5$ pieces and the quadrature is of order $2$. The merging thresholds are $0\%$ \protect\subref{fig:7_0.0} and $25\%$ \protect\subref{fig:7_0.25}.}
    \label{fig:7_meshes}
\end{figure}

%%%
\subsection{More complex domains}
\label{subsect:rhombi}

We complete our numerical experiments by solving a weak Poisson equation on a more complex two-dimensional domain, defined as the union of two rhombi. We solve the following Poisson equation
\begin{equation}
    \left\{\begin{array}{rc}
        -\Delta u = x + y & \text{in } \Omega \\
        u = 0             & \text{on } \Gamma \\
    \end{array}\right.. \label{eq:rhombi}
\end{equation}

We obtain an approximation of the solution with the \ac{fem}. We rely on the following weak formulation for the \ac{fem} solution:
\[\left\{\begin{array}{l}
        \text{Find } u \in H^1_0(\Omega) \text{ such that for all } v \in H^1_0(\Omega), \\
        \int_\Omega \Delta u \cdot \Delta v \D \Omega = \int_\Omega (x + y) v \D \Omega
    \end{array}\right.,\]
where $H^1_0(\Omega)$ is the subset of functions in $H^1(\Omega)$ that vanish on $\Gamma$. We then obtain a mesh composed of $3894$ cells from GMSH and solve this \ac{fem} problem with the Gridap software \cite{badia2020}. We rely on first-order Lagrange finite elements, and the total number of degrees of freedom is $1852$. We use the \ac{fem} solution as the reference to compute the $L^2$ norm for the solution obtained by a \ac{nn}. The relative $L^2$ norm is computed with a quadrature of order $10$ on the cells of the mesh of the \ac{fem} solution.

We then approximate this equation with \acp{nn} using the same penalised weak form that we considered in the previous experiments. We increase the network architecture to $(2, 20, 20, 1)$ and the activation function is $\abse$. We train our network for $10000$ iterations using the ADAM optimiser with a learning rate of $10^{-2}$. We cut the $\abse$ activation into $3$ pieces, use a quadrature of order $2$ in each cell, and do not merge small regions. The integration points are resampled every $10$ epochs. On average, \ac{aq} involved $5894$ integration points in the domain and $292$ on the boundary. We match the computational budget of \ac{aq} for \ac{mc} by sampling $6000$ in-domain points and $300$ points on the boundary for \ac{mc}. To obtain a uniform distribution of integration points for \ac{mc}, we first perform a triangulation of the domain. Then for as many points as we need, we draw a triangle at random by weighting them by their measure, and sample one point in the corresponding triangle.

We plot the solutions obtained by \ac{fem}, \ac{mc} and \ac{aq} in \fig{8_abs}. The pointwise difference between \ac{mc} and \ac{fem}, and between \ac{aq} and \ac{fem} is shown in \fig{8_rel}. The relative $L^2$ norm is $7.99 \times 10^{-2}$ for \ac{mc} and $4.68 \times 10^{-2}$ for \ac{aq}. We mention that the training times for the last experiment are $700$ seconds with \ac{aq} and $625$ seconds with \ac{mc}. We believe that this $12\%$ extension in time is well worth the $70\%$ decrease in $L^2$ norm. It is interesting to mention that the pointwise error is located around the same regions in the domain. However when using \ac{aq}, the magnitude of the local maxima of the pointwise error is much lower.

\begin{figure}
    \centering
    \subfloat[Approximation of \eq{rhombi} obtained with \ac{mc} (left), \ac{fem} (middle) and \ac{aq} (right).\label{fig:8_abs}]{
        \includegraphics[width=0.8\linewidth]{8_abs.png}
    }

    \subfloat[Pointwise error between the FEM solution of \eq{rhombi} and the \ac{mc} solution (left), and the \ac{aq} solution (right). The relative $L^2$ norm is $7.99 \times 10^{-2}$ for \ac{mc} and $4.68 \times 10^{-2}$ for \ac{aq}. \label{fig:8_rel}]{
        \includegraphics[width=0.8\linewidth]{8_rel.png}
    }
    \caption{Approximation \protect\subref{fig:8_abs} and pointwise error \protect\subref{fig:8_rel} for the solution of \eq{rhombi}.}
    \label{fig:8_rhombi}
\end{figure}

%%%
\subsection{Summary}
\label{subsect:summary}

Our numerical experiments confirmed the effectiveness of our proposed integration method in a wide range of scenarios. \ac{aq} enables to reach lower generalisation errors compared to \ac{mc} while using fewer integration points. We have shown that this new integration method is more robust to the initial parameters of the network. Moreover, the convergence of the network is less noisy and the generalisation error decays quicker, as \ac{aq} can sustain higher learning rates without introducing extra noise in the learning phase. We are convinced that our proposed integration method can help \acp{nn} become a more reliable and predictable tool when it comes to approximating \acp{pde}, so that they can be employed to solve real-world problems.

The most significant differences between \ac{mc} and \ac{aq} were observed for the weak Poisson problem. One reason could be that the loss of the strong Poisson problems can be understood in a pointwise sense. In this case, \ac{mc} minimises the pointwise distance between $\Delta u$ and $\Delta f$ at the location of the integration points. On the contrary, the loss of the weak Poisson problem is to be understood and minimised in a global sense. Our experiments also suggest that the benefits of our adaptive quadrature are more striking at the location of strong gradients.

%%%
\subsection{Discussion}
\label{subsect:discussion}

Our approach suffers from the curse of dimensionality and is intended to be used for low-dimensional \acp{pde} as a nonlinear alternative to the \ac{fem}. We noticed that the number of integration points in dimension one was of the order of a few tens to a few hundreds, when the usual number of integration points used for \ac{mc} in the literature is rather a few hundreds to a few thousands. In dimension two, our method needs a few thousand integration points, whereas the models in the literature rely on a few tens of thousands of points. Even though our approach is still cheaper than \ac{mc} in the two-dimensional case, the reduction factor is much smaller compared to the one-dimensional scenario. We found that merging linear regions can help reduce the number of linear regions and thus integration points. This phenomenon should amplify as the dimension increases, but we are only ever concerned with problems arising from physics, which involve space and time only. We would like to extend our method to the three-dimensional case in the future.

We have observed that the time spent on extracting the mesh is offset by the reduction in the number of integration points, as the overall training very often takes less time with \ac{aq} than with \ac{mc}.

Our idea to regularise the activation function was motivated by the relationship between \ac{fem} and $\ReLU$ networks, as well as the need for a smooth activation so that the energy functional itself is smooth. We believe that it is more economical to use a smoothed CPWL activation function than to directly use a smooth activation such as $\tanh$ because intuitively fewer pieces are necessary to reach a given tolerance in the former case. Whereas most of the current implementations of \ac{pinn} and its variants can only handle tensor-product domains or balls, our proposed linearisation algorithm can naturally handle any polygonal domain. This is an important step towards the use of \acp{nn} solvers for practical issues.

One direction for future work is tackling the variational setting in the general case when it is not possible to recast the problem as an energy minimisation. We point out that our domain decomposition strategy and adaptive quadrature can be extended to higher-order approximations (e.g. splines) of the activation function, which would result in curved boundaries for the regions.