In practice the \ac{cpwl} approximation of $\rho$ is going to be fixed, so we only keep the subscripts $n$ and $p$ in $\pi_{n, p}$ when we discuss the convergence at the end of this section. Let $u = \mathcal{N}(\bm{\vartheta}, \rho)$ and $\pi[u] = \mathcal{N}(\bm{\vartheta}, \pi[\rho])$ be two \acp{nn} sharing the same weights, activated by $\rho$ and $\pi[\rho]$ respectively. We are interested in recovering a maximal decomposition of $\Omega$ (resp. $\Gamma$) such that $\pi[u]$ is linear on these regions. We say that this decomposition is a mesh of $\Omega$ (resp. $\Gamma$) adapted to $\pi[u]$. Since $\pi[\rho]$ is fixed, this mesh depends only on $\bm{\vartheta}$. We introduce the notation $\tau_\Omega(\bm{\vartheta})$ and $\tau_\Gamma(\bm{\vartheta})$ to refer to these meshes, and $\tau(\bm{\vartheta})$ to refer to either of the two meshes. We equip each cell with a Gaussian quadrature to evaluate the loss function. Bounds for the integration error of our \ac{aq} are provided at the end of this section.

\subsection{Convexity of the mesh}

We observe that each neuron corresponds to a scalar-valued linear map followed by the composition by $\pi[\rho]$. The breakpoints of $\pi[\rho]$ define hyperplanes in the input space of each neuron. Indeed, the composition of a linear map $\bm{x} \mapsto \bm{W} \bm{x} + \bm{b}$ by $\pi[u]$ is \ac{cpwl} and the boundaries of the regions where the composition is linear correspond to the equations $\bm{w} \cdot \bm{x} + b = \xi$, where $\bm{w}$ can be any row vector of $\bm{W}$, $b$ is the coordinate of $\bm{b}$ corresponding to the same row, and $\xi$ can be any breakpoint of $\pi[\rho]$.

Intuitively, we can obtain the adapted meshes $\tau(\bm{\vartheta})$ by considering the hyperplanes attached to every neuron of the network. The cells of the mesh are the largest sets of points that lie within exactly one piece of $\pi[\rho]$ at each neuron. This is why these regions are also called the \enquote{activation patterns} of a \ac{nn}, as we can label them by the pieces they belong to at each neuron \cite{hanin2019}.

The implementation of an algorithm that, given the parameters $\bm{\vartheta}$ of a \ac{nn}, the linearisation $\pi[\rho]$ and the input space $\Omega$, outputs $\tau_\Omega(\bm{\vartheta})$ has to be robust to a large variety of corner cases, as in practice the cells can be of extremely small measures, or take skewed shapes. Besides, we are concerned with the complexity of this algorithm as it is meant to be run at every iteration (or at a given frequency) during the training phase. Fortunately enough, the only cells in the mesh that may not be convex must be in contact with the boundary.

\begin{lemma}[Convexity of the mesh]
    \label{lem:convexity}
    Let $u: \RR^d \to \RR$ be a \ac{nn} with weights $\bm{\vartheta}$ activated by a \ac{cpwl} function and $\Omega \subset \RR^d$. If a cell in $\tau_\Omega(\bm{\vartheta})$ is not convex, it has to intersect with the boundary of $\Omega$. In particular, if $\Omega$ is convex, all the cells of $\tau_\Omega(\bm{\vartheta})$ are convex.
\end{lemma}
\begin{proof}
    See \ref{proof:lem:convexity}.
\end{proof}

If $\Omega$ is not convex, we decompose $\Omega$ into a set of convex polytopes and construct an adapted mesh for each polytope independently. By \lem{convexity} these submeshes are convex. The mesh composed of all the cells of the submeshes is adapted to $u$ on $\Omega$ and contains convex cells only. Thus without loss of generality, we suppose that $\Omega$ is convex. Clipping a line by a polygon is made easier when the polygon is convex. Our method to build $\tau_\Omega(\bm{\vartheta})$ takes great advantage of the convexity of the mesh. Our algorithm is described in \alg{adaptiveMesh}.

\subsection{Representation of a linear region}
\label{subsect:representation}

Let $u_k$ denote the composition of the first $k$ layers of the network $u$, and $\tau^k(\bm{\vartheta})$ be the mesh adapted to $u_k$. We represent a linear region of $\tau^k(\bm{\vartheta})$ and its corresponding local expression by $(R, \bm{W}_R, \bm{b}_R)$, where $R \in \tau^k(\bm{\vartheta})$ is a cell of the mesh, $\bm{W}_R \in \RR^{n_k \times d}$ and $\bm{b}_R \in \RR^{n_k}$ are the coefficients of the restriction of $\pi[u_k]$ to $R$: $\pi[u_k]_{\left|R\right.}(\bm{x}) = \bm{W}_R \bm{x} + \bm{b}_R$.

\subsection{Initialisation of the algorithm}

We initialise the algorithm with the region $(\Omega, \bm{I}_{d}, \bm{0}_d)$, where $\bm{I}_d$ is the identity matrix of size $d$ and $\bm{0}_d$ is the zero vector of size $d$. The mesh $\{(\Omega, \bm{I}_{d}, \bm{0}_d)\}$ is adapted to $u_0$.

\subsection{Composition by a linear map}

Suppose that $(R, \bm{W}_R, \bm{b}_R)$ is adapted to $u_k$. The composition of $\pi[u_k]_{\left|R\right.}$ by the linear map $\bm{\Theta}_{k+1}$ remains linear on $R$, only the coefficients of the map are changed. They become $\bm{W}_{k+1} \bm{W}_R$ and $\bm{W}_{k+1} \bm{b}_R + \bm{b}_{k+1}$.

\subsection{Composition by an activation function}

Suppose that $(R, \bm{W}_R, \bm{b}_R)$ is adapted to $u_k$. We compose $\pi[u_k]_{\left|R\right.}$ by $\pi[\rho]$ componentwise. Let $\bm{w}$ be one of the row vectors of $\bm{W}_R$ and $b$ be the coordinate of $\bm{b}_R$ corresponding to the same row. Let $\xi$ be one of the breakpoints of $\pi[\rho]$. We need to find the pre-image of $\xi$ under the map $\bm{x} \mapsto \bm{w} \cdot \bm{x} + b$, which corresponds to a hyperplane in $R$. These hyperplanes have to be determined for all rows of $\bm{W}_R$ and all breakpoints of $\pi[\rho]$. We underline that the hyperplanes corresponding to a given row $\bm{w}$ of $\bm{W}_R$ are parallel, as they are level sets of $\bm{x} \mapsto \bm{w} \cdot \bm{x}$. We explain our method in detail in \app{algo_mesh}.

The process of mesh extraction in dimension two is illustrated in \fig{mesh_extraction} for a single-layer neural network. In \fig{mesh_1}, the plain lines depict two parallel hyperplanes corresponding to two different breakpoints of $\pi[\rho]$ for one of the output coordinates of the linear map. The dashed line indicates the direction vector given by the corresponding row vector of the linear map. \fig{mesh_2} pictures the hyperplanes resulting from the other output coordinates of the linear map, thus oriented in different directions. We highlight that since the output coordinates of the linear maps may have different ranges, they may not individually activate all the pieces of $\pi[\rho]$, and thus they can give rise to different numbers of hyperplanes. The clipping operation is shown in \fig{mesh_3}, and the intersections of the hyperplanes against themselves are displayed in \fig{mesh_4}. In this example, the initial cell would be cut into $13$ subcells.

\begin{figure}
    \centering
    \subfloat[\label{fig:mesh_1}]{
        % \includesvg[width=0.2\linewidth]{mesh_1.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_1_svg-tex.pdf_tex}
    }
    \hfill
    \subfloat[\label{fig:mesh_2}]{
        % \includesvg[width=0.2\linewidth]{mesh_2.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_2_svg-tex.pdf_tex}
    }
    \hfill
    \subfloat[\label{fig:mesh_3}]{
        % \includesvg[width=0.2\linewidth]{mesh_3.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_3_svg-tex.pdf_tex}
    }
    \hfill
    \subfloat[\label{fig:mesh_4}]{
        % \includesvg[width=0.2\linewidth]{mesh_4.svg}
        \def\svgwidth{0.2\linewidth}
        \input{svg-inkscape/mesh_4_svg-tex.pdf_tex}
    }
    \caption{Example of a mesh extraction, corresponding to a neural network with architecture $(2, m)$ with $m \geq 4$, that is $u = \rho \circ \bm{\Theta}$, where $\bm{\Theta}: \bm{x} \mapsto \bm{W} \bm{x} + \bm{b}$, $\bm{W} \in \RR^{m \times 2}$ and $\bm{b} \in \RR^{m}$. \protect\subref{fig:mesh_1} Parallel hyperplanes associated with different breakpoints $\xi_1, \xi_2$ of $\pi[\rho]$ for one of the output coordinates of $\bm{\Theta}$ (they are orthogonal to one of the row vectors of $\bm{W}$). \protect\subref{fig:mesh_2} Hyperplanes corresponding to all the output coordinates of $\bm{\Theta}$. \protect\subref{fig:mesh_3} Clipping of the hyperplanes against the region boundary. \protect\subref{fig:mesh_4} Pairwise intersection of the hyperplanes.}
    \label{fig:mesh_extraction}
\end{figure}

\subsection{Gaussian quadratures for convex polygons}

We decompose the integrals in the linear and bilinear forms on the cells of $\tau_\Omega(\bm{\vartheta})$ and $\tau_\Gamma(\bm{\vartheta})$. In these cells, the terms that only depend on the network $\pi[u]$ and its spatial derivatives are polynomials. As a consequence, the linear and bilinear forms involving $\pi[u]$ can be computed exactly using Gaussian quadratures on segments in dimension one, and polygons in dimension two.

In dimension one, the cells of the mesh are segments. Gaussian quadrature rules are known and made available through plenty of libraries. However, in dimension two, the cells of the mesh can be arbitrary convex polygons. To handle the general case, one approach could consist in splitting each convex cell into triangles and then applying a known Gaussian quadrature for triangles. This approach is the least economical in terms of the number of quadrature points. At the opposite end of the spectrum, we could rely on Gaussian quadratures of reference $n$-gons. Still, the order of the mapping from the reference $n$-gon to the $n$-gon at hand is $n-2$, which makes the Jacobian of this mapping costly to evaluate. In this work, we strike a tradeoff and decompose each convex cell into a collection of triangles and convex quadrangles. We use a recursive algorithm to obtain this decomposition, as explained in \app{algo_polygon}. We refer to \cite{witherden2015} for numerically accurate symmetric quadrature rules for triangles and quadrangles.

\subsection{Alternatives}
\label{subsect:alternatives}

There exist computationally cheaper numerical integration methods based on evaluations of the integrands at the vertices of the mesh. Indeed, using Stokes theorem one can transform surface integrals on polygons into line integrals on their edges, and in turn, into pointwise evaluations at their vertices \cite{chin2015}. This approach requires knowing the local expression of $\pi[u]$, that is the coefficients of the linear interpolation of $u$ on each cell.

We have conducted preliminary experiments using this approach but we have observed that in addition to being numerically unstable, the overall cost including the interpolation step is not lower. Indeed, finding the best-fitting plane that passes through given points involves the inversion of a $3 \times 3$ system on each cell. The coefficients of these matrices are the integrals over each cell of the polynomials $x^2$, $xy$, $y^2$, $x$, $y$ and $1$. In many cases, the cells take skewed shapes so these matrices can be extremely ill-conditioned.

\subsection{Analysis of the integration error}

Our proposed adaptive quadrature consists in approximating $\mathcal{J}(u)$ by a quadrature on the cells of the mesh adapted to $\pi[u]$. We insist that our proposed \ac{aq} relies on the evaluation of $u$ and not $\pi[u]$. We now show how to bound the integration error of a bilinear form (the integration error of a linear form is bounded in a similar way). Let $\mathcal{I}, \mathcal{Q}: U \times U \to \RR$ be two bilinear forms. Here $\mathcal{Q}$ denotes the approximation of $\mathcal{I}$ by a numerical quadrature. We suppose that $u$ and $\pi[u]$ belong to $U$ and decompose the integration error into
\begin{align*}
    \abs{\mathcal{I}(u, u) - \mathcal{Q}(u, u)} & \leq \abs{\mathcal{I}(u - \pi[u], u)} + \abs{\mathcal{I}(\pi[u], u - \pi[u])} \\
                                                & + \abs{\mathcal{I}(\pi[u], \pi[u]) - \mathcal{Q}(\pi[u], \pi[u])}             \\
                                                & + \abs{\mathcal{Q}(u - \pi[u], \pi[u])} + \abs{\mathcal{Q}(u, u - \pi[u])}.
\end{align*}
The term $\abs{\mathcal{I}(\pi[u], \pi[u]) - \mathcal{Q}(\pi[u], \pi[u])}$ is the error incurred by any piecewise numerical integration method (e.g. it is the standard integration error in the FEM). In particular, one can consider piecewise polynomial approximations of physical parameters, forcing term, and boundary conditions up to a given order and use a Gaussian quadrature that cancels the numerical quadrature error of the bilinear form. We neglect this term in the remainder of this section. We further assume that $\mathcal{I}$ and $\mathcal{Q}$ are bounded: there exist norms $\aabs{\cdot}_{\mathcal{I}, 1}$, $\aabs{\cdot}_{\mathcal{I}, 2}$ on $U$ and a constant $C_{\mathcal{I}} > 0$ such that for all $(u, v) \in U \times U$, it holds $\abs{\mathcal{I}(u, v)} \leq C_{\mathcal{I}} \aabs{u}_{\mathcal{I}, 1} \aabs{v}_{\mathcal{I}, 2}$ and we adopt similar notations for $\mathcal{Q}$. It follows that
\begin{align*}
    \abs{\mathcal{I}(u, u) - \mathcal{Q}(u, u)} & \leq C_{\mathcal{I}} (\aabs{u - \pi[u]}_{\mathcal{I}, 1} \aabs{u}_{\mathcal{I}, 2} + \aabs{\pi[u]}_{\mathcal{I}, 1} \aabs{u - \pi[u]}_{\mathcal{I}, 2}) \\
                                                & + C_{\mathcal{Q}} (\aabs{u - \pi[u]}_{\mathcal{Q}, 1} \aabs{u}_{\mathcal{Q}, 2} + \aabs{\pi[u]}_{\mathcal{Q}, 1} \aabs{u - \pi[u]}_{\mathcal{Q}, 2}).
\end{align*}

We consider the two networks $u = \mathcal{N}(\bm{\vartheta}, \rho)$ and $\pi_{n, p}[u] = \mathcal{N}(\bm{\vartheta}, \pi_{n, p}[\rho])$, that only differ by their activation function. We show that we can bound $u - \pi_{n, p}[u]$ in terms of $\rho - \pi_{n, p}[\rho]$ in the following proposition.

\begin{proposition}[Distance between two neural networks]
    \label{prop:nns}
    Let $\Omega \subset \RR^d$ be a bounded domain, $\rho$ and $\sigma$ two continuous and almost everywhere differentiable functions such that $\rho - \sigma$, $\rho'$ and $\sigma'$ are bounded on $\RR$. We further assume that $\rho$ and $\rho'$ are Lipschitz continuous on $\RR$. Let $u_\rho = \mathcal{N}(\bm{\vartheta}, \rho)$ and $u_\sigma = \mathcal{N}(\bm{\vartheta}, \sigma)$ two \acp{nn} that only differ by their activation functions. There exist three constants $C_1$, $C_2$, $C_3$ such that
    \begin{align}
        \aabs{u_\rho - u_\sigma}_{L^\infty(\Omega)}                      & \leq C_1 \aabs{\rho - \sigma}_{L^\infty(\RR)},                                     \label{eq:nns_1}     \\
        \aabs{\nabla u_\rho - \nabla u_\sigma}_{L^\infty(\Omega, \RR^d)} & \leq C_2 \aabs{\rho - \sigma}_{L^\infty(\RR)} + C_3 \aabs{\rho' - \sigma'}_{L^\infty(\RR)}. \label{eq:nns_2}
    \end{align}
    These three constants depend on $\bm{\vartheta}$, $\aabs{\rho'}_{L^\infty(\RR)}$, $\aabs{\sigma'}_{L^\infty(\RR)}$, and the Lipschitz constants of $\rho$ and $\rho'$.
\end{proposition}
\begin{proof}
    See \ref{proof:prop:nns}.
\end{proof}

\subsubsection{Bounding zero- and first-order derivatives}

When we consider the weak formulation of the Poisson problem, both norms for $\mathcal{I}$ are $H^1$ norms, and those for $\mathcal{Q}$ are $W^{1, \infty}$ norms. \prop{cpwl} and \prop{nns} can be combined to bound $L^q$- and $W^{1, q}$-like norms of $u - \pi_{n, p}[u]$

For example, let $\alpha > 1/2$, $\rho \in \mathcal{A}_\alpha$, $p = 2 > 1/\alpha$ and $n \geq \kappa(\rho)$. We know that $\rho$ and $\pi_{n, 2}[\rho]$ are continuous and almost everywhere differentiable and their derivatives are bounded. Furthermore \prop{nns} showed that $\rho - \pi_{n, 2}[\rho]$ is bounded. Since $\rho'$ and $\rho''$ are bounded, we infer that $\rho$ and $\rho'$ are Lipschitz continuous. We use the fact that if $f$ is bounded, then it holds $\aabs{f}_{L^2(\Omega)} \leq \aabs{f}_{L^\infty(\Omega)} \card{\Omega}^{1/2}$. Combining \prop{cpwl} and \prop{nns}, we show the bounds
\begin{align*}
    \aabs{u - \pi_{n, 2}[u]}_{L^2(\Omega)} & \lesssim n^{-2}, & \aabs{\nabla u - \nabla \pi_{n, 2}[u]}_{L^2(\Omega)} & \lesssim n^{-1},
\end{align*}
where the term $n^{-2}$ has been neglected in the second bound. Altogether, we conclude that $\aabs{u - \pi_{n, 2}[u]}_{H^1(\Omega)}$ decays as $n^{-1}$.

\subsubsection{Bounding higher-order derivatives}

We now turn to the strong formulation of the Poisson problem. Since we use smooth activation functions, the strong formulation of the Poisson problem is well-posed for $u$. We reiterate that our \ac{aq} is performed on $u$ directly, not on $\pi_{n, p}[u]$. In this case, the norms of $\mathcal{I}$ and $\mathcal{Q}$ are $H^2$ and $W^{2, \infty}$ respectively. The decomposition of the integration error presented for the zero- and first-order derivatives cannot be adapted here because $\pi_{n, p}[\rho]$ is not smooth enough. To show similar integration bounds as above, one would need to approximate $\rho$ by a piecewise quadratic, globally $\mathcal{C}^1$ polynomial (e.g. a quadratic spline).