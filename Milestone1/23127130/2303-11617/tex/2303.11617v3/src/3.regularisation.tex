It was shown in \cite{he2018} that \acp{nn} activated by the $\ReLU$ activation function can emulate first-order Lagrange elements, thus advocating the use of $\ReLU$ networks to approximate \acp{pde}. In this section, we show that the convergence of \ac{cpwl} networks is slow and noisy whenever the energy functional $\mathcal{J}$ involves a spatial derivative of $u$. We then provide a reason for preferring smoother activations and suggest a few alternatives for the $\ReLU$ function.

\subsection{Limitations of CPWL activation functions}

When the activation is \ac{cpwl}, the \ac{nn} $u$ is continuous and weakly differentiable (in the sense of distributions), and its partial derivatives are piecewise constant and discontinuous. Even though derivatives of order two and higher will be evaluated as zero, only the first-order derivatives of the network make pointwise sense from a theoretical point of view, while derivatives of higher order do not.

Let us consider an illustrative example to clarify the kind of problems that arise when the energy functional involves a spatial derivative of $u$. We consider the Poisson problem $u'' = f''$ in $\Omega$ with $u = f$ on $\Gamma$, which we reframe into the following variational problem using the Nitsche method
\begin{align*}
    a(u, v) & = \scal{u'}{v'}_\Omega - \scal{u' \cdot n}{v}_\Gamma - \scal{u}{v' \cdot n}_\Gamma + \beta \scal{u}{v}_\Gamma \\
    \ell(v) & = \scal{-f''}{v}_\Omega - \scal{f}{v' \cdot n}_\Gamma + \beta \scal{f}{v}_\Gamma.
\end{align*}
Here $n$ stands for the outward-pointing unit normal to $\Gamma$. We consider the manufactured solution $f = \ReLU$ on $\Omega = \cc{-1}{+1}$. In this case, $f'' = \delta_0$ is the Dirac delta centered at zero, which makes the first integral in $\ell(v)$ equal to $-v(0)$. We consider a one-layer network $u(x) = f(x + c)$ to approximate the solution to this problem, and we wish to recover $c = 0$. The energy functional has the following expression:
\[\mathcal{J}(c) = \frac{1}{2} \left\{\begin{array}{ll}
        0                                 & \text{if } c < -1            \\
        \beta c^2 + \abs{c} - (\beta - 1) & \text{if } -1 \leq c \leq +1 \\
        2 \beta c^2 - 2 (\beta - 1) c     & \text{if } c > +1
    \end{array}\right.\]
We verify that $\mathcal{J}$ shows a discontinuity at $-1$. Besides, $\mathcal{J}$ is decreasing on $\cc{-1}{0}$ and increasing on $\cc{0}{+1}$. We compute $\mathcal{J}(0) = -\frac{1}{2} (\beta - 1)$, which is negative whenever $\beta > 1$. This proves that $\mathcal{J}$ has a global minimum at $c = 0$. However, the gradient of $\mathcal{J}$ is discontinuous at $0$, being equal to $-1/2$ on the left and $+1/2$ on the right. Because $\ReLU$ is not differentiable at zero, any gradient-based optimiser will suffer from oscillations, implying slow and noisy convergence.

This example only involved one unknown, but one would already have to choose a low learning rate and apply a decay on the learning rate to make the network converge to an acceptable solution. One can imagine that when the optimisation problem involves a whole network, these oscillations can interfere with and severely hinder convergence. Ensuring that the activation function is at least $\mathcal{C}^1$ enables one to interchange the derivative and integral signs, and if it is $\mathcal{C}^2$ then the gradient of the loss function is continuous.

\subsection{Regularisation of the activation function}

The regularity of a \ac{nn} is entirely dictated by that of its activation function. In order to make the energy functional smoother while keeping the motivation of \ac{cpwl}, our idea is to approximate the \ac{cpwl} function by a smoother equivalent and replace every occurrence of the \ac{cpwl} activation by its smoother counterpart in the network.

Our approach is similar to the \enquote{canonical smoothings} of $\ReLU$ networks introduced in \cite{dong2022}. We take $\ReLU$ as an example and point out several families of smooth equivalents. In each case, we introduce the normalised variable $\bar{x} = x / \gamma$, where $\gamma$ is a constant that only depends on the choice of the family.
\begin{itemize}
    \item If we rewrite $\ReLU(x) = \frac{1}{2}(x + \abs{x})$, we see that the lack of regularity comes from the absolute value being non-differentiable at zero. We can thus find a smooth equivalent of the absolute value and replace it in this expression. We set $\gamma = 2 \epsilon$ and define $\rhoe$ as
          \[\rhoe(x) = \frac{1}{2} \left(x + \gamma \sqrt{1 + \bar{x}^2}\right).\]
    \item The first derivative of $\ReLU$ is discontinuous at zero. We can replace it with any continuous sigmoid function and obtain a smoothing of $\ReLU$ by integrating it. We set $\gamma = \frac{2}{\ln 2} \epsilon$ and define $\rhoe$ as
          \[\rhoe(x) = \frac{1}{2} \left(x + \gamma \ln(2 \cosh(\bar{x}))\right).\]
    \item The second derivative of $\ReLU$ is the Dirac delta at zero. We can draw from the well-established theory of mollifiers to obtain a smooth Dirac and integrate it twice to build a regularised version of $\ReLU$. We set $\gamma = 2 \sqrt{\pi} \epsilon$ and define $\rhoe$ as
          \[\rhoe(x) = \frac{1}{2} \left(x + x \erf(\bar{x}) + \frac{\gamma}{\sqrt{\pi}} \exp(-\bar{x}^2) \right).\]
\end{itemize}

\begin{figure}
    \centering
    \subfloat[$\rhoe$\label{fig:reg_zero}]{
        \includegraphics[width=0.31\linewidth]{regularisation_zero.pdf}
    }
    \hfill
    \subfloat[$\rhoe'$\label{fig:reg_one}]{
        \includegraphics[width=0.31\linewidth]{regularisation_one.pdf}
    }
    \hfill
    \subfloat[$\rhoe''$\label{fig:reg_two}]{
        \includegraphics[width=0.31\linewidth]{regularisation_two.pdf}
    }
    \caption{Examples of regularising families for the $\ReLU$ function and corresponding first and second derivatives.}
    \label{fig:regularisation}
\end{figure}

The three classes of smooth equivalents of $\ReLU$ introduced above, together with their first and second derivatives, are illustrated on \fig{regularisation}. It can be easily shown that for all $\epsilon > 0$, the three smooth equivalents above have the following properties:
\begin{enumerate*}[label=(\itshape\roman*)]
    \item the function $\rhoe$ belongs to $\mathcal{C}^\infty(\RR)$;
    \item similarly to $\ReLU$, the function $\rhoe$ is convex, monotonically increasing on $\RR$, and it is symmetric around $y=-x$, i.e. for all $x \in \RR$, it holds $\rhoe(x) - \rhoe(-x) = x$;
    \item the graph of $\rhoe$ lies above that of $\ReLU$; and
    \item the parameter $\epsilon$ controls the pointwise distance between $\rhoe$ and $\ReLU$, in the sense that $\aabs{\ReLU - \rhoe}_{L^\infty(\RR)} = \rhoe(0) = \epsilon$.
\end{enumerate*}

In particular, Lemma 2.5 in \cite{dong2022} applies to all the candidates above, and we refer to Proposition 2.6 in the same article for a bound of $\aabs{\mathcal{N}(\bm{\vartheta}, \ReLU) - \mathcal{N}(\bm{\vartheta}, \rho_\epsilon)}_{L^\infty(\Omega)}$ in terms of $\aabs{\ReLU - \rho_\epsilon}_{L^\infty(\RR)}$.

The computational cost of these functions must also be taken into account. For this reason, we choose to use the family obtained by the first method. We write it $\abse$ in the rest of this work.
