Consider the boundary condition problem
\begin{equation}
    \left\{\begin{array}{cl}
        \mathcal{D} u = f & \text{ in } \Omega  \\
        \mathcal{B} u = g & \text{ on } \Gamma.
    \end{array}\right.,\label{eq:pde}
\end{equation}
where $\Omega$ is a domain in $\RR^d$ with boundary $\Gamma$, $\mathcal{D}$ is a domain differential operator, $f: \Omega \to \RR$ is a forcing term, $\mathcal{B}$ is a trace/flux boundary operator and $g: \Gamma \to \RR$ the corresponding prescribed value. We approximate the solution $u$ as the realisation of a \ac{nn} of a given architecture. We introduce some notations and recall the key principles of the \ac{pinn} framework in the rest of this section.

\subsection{Neural networks}

Our solution ansatz is a fully-connected, feed-forward \ac{nn}, which is obtained as the composition of linear maps and nonlinear activation functions applied element-wise. In this work, we separate the structure of a \ac{nn} from its realisation when the activation function and the values of the weights and biases are given. We describe the architecture of a network by a tuple $(n_0, \ldots, n_L)\in \NN^{(L+1)}$, where $L$ is the number of linear maps it is composed of, and for $1 \leq k \leq L$ the number of neurons on layer $k$ is $n_k$. We take $n_0 = d$ and since we only consider scalar-valued \acp{pde}, we have $n_L = 1$.

For each layer number $1 \leq k \leq L$, we write $\bm{\Theta}_k: \RR^{n_{k-1}} \to \RR^{n_k}$ the linear map at layer $k$, defined by $\bm{\Theta}_k \bm{x} = \bm{W}_k \bm{x} + \bm{b}_k$ for some weight matrix $\bm{W}_k \in \RR^{n_k \times n_{k-1}}$ and bias vector $\bm{b}_k \in \RR^{n_k}$. We apply the activation function $\rho: \RR \to \RR$ after every linear map except for the last one so as not to constrain the range of the model. The expression of the network is then
\[\mathcal{N}(\bm{\vartheta}, \rho) = \bm{\Theta}_L \circ \rho \circ \bm{\Theta}_{L-1} \circ \ldots \circ \rho \circ \bm{\Theta}_1,\]
where $\bm{\vartheta}$ stands for the collection of trainable parameters of the network $\bm{W}_k$ and $\bm{b}_k$. Although the activation functions could be different at each layer or even trainable, we apply the same, fixed activation function everywhere.

\subsection{Abstract setting for the loss function}

The loss function serves as a distance metric between the solution and the model. We write the loss in terms of the network itself to simplify notations, although it is to be understood as a function of its trainable parameters. Throughout this paper, we write $\scal{u}{v}_{\Omega} = \int_\Omega u v \D \Omega$ and $\scal{u}{v}_{\Gamma} = \int_{\Gamma} u v \D \Gamma$ the classical inner products on $L^2(\Omega)$ and $L^2(\Gamma)$. We write $\aabs{\cdot}_\Omega$ and $\aabs{\cdot}_\Gamma$ the corresponding $L^2$ norms.

\subsubsection{Strong formulation}

According to the original \ac{pinn} formulation \cite{raissi2019}, the network is trained to minimise the strong residual of \eq{pde}. The minimisation is performed in $L^2$, so we suppose that $\mathcal{D} u$ and $f$ are in $L^2(\Omega)$, and $\mathcal{B} u$ and $g$ are in $L^2(\Gamma)$, This ensures that the residual
\[\mathcal{R}(u) = \frac{1}{2} \aabs{\mathcal{D} u - f}_\Omega^2 + \frac{\beta}{2} \aabs{\mathcal{B} u - g}_\Gamma^2\]
is well-posed, where $\beta > 0$ is a weighting term for the boundary conditions.

\subsubsection{Weak formulation}

The \ac{fem} derives a weak form of \eq{pde} that we can write in the following terms:
\[\left\{\begin{array}{l}
        \text{Find } u \in U \text{ such that} \\
        \forall v \in V, \quad a(u, v) = \ell(v).
    \end{array}\right.\]
In this formulation, $a: U \times V \to \RR$ is a bilinear form, $\ell: V \to \RR$ is a linear form, and $U$ and $V$ are two functional spaces on $\Omega$ such that $a(u, v)$ and $\ell(v)$ are defined for all $u \in U$ and $v \in V$. When the bilinear form $a$ is symmetric, coercive, and continuous on $U \times V$, and $U = V$, this weak setting can be recast as the minimisation of the energy functional
\[\mathcal{J}(u) = \frac{1}{2} a(u, u) - \ell(u)\]
for $u \in U$. In our case, $U$ is the manifold of \acp{nn} with a given architecture defined on $\Omega$. One difference with the \ac{fem} setting is that it is difficult to strongly enforce essential boundary conditions on \ac{nn} spaces. Among other approaches, it is common to multiply the network by a function that vanishes on the boundary and add a lifting of the boundary condition \cite{sukumar2022}. Instead, we follow the penalty method and modify the residual: the boundary terms that come from integration by parts do not cancel and we suppose that it is possible to alter the bilinear form $a$ into $a_\beta$ and linear form $\ell$ into $\ell_\beta$ such that $a_\beta$ is still symmetric and continuous. To ensure the coercivity of the bilinear form and weakly enforce Dirichlet boundary conditions, we add the term $\beta \scal{u}{v}_\Omega$ in the bilinear form and $\beta \scal{g}{v}_\Gamma$ in the linear form. Provided that the penalty coefficient $\beta$ is large enough, $a_\beta$ can be made coercive \cite[196]{ern2021} and we consider the energy minimisation problem
\begin{equation}
    \label{eq:problem}
    \text{Find } u \in U \text{ such that for all } v \in U, \mathcal{J}_\beta(u) = \frac{1}{2} a_\beta(u, u) - \ell_\beta(u) \leq \mathcal{J}_\beta(v).
\end{equation}

We see that the strong problem is a special case of the weak formulation. Indeed, by expanding the inner products and rearranging the terms, we obtain
\[\mathcal{R}(u) = \frac{1}{2} \left(\scal{\mathcal{D} u}{\mathcal{D} u}_\Omega + \beta \scal{\mathcal{B} u}{\mathcal{B} u}_\Gamma\right) - \left(\scal{\mathcal{D} u}{f}_\Omega + \beta \scal{\mathcal{B} u}{g}_\Gamma\right) + C,\]
where $C$ is a constant that only depends on the data $f$ and $g$. In particular, the minimum of $\mathcal{R}$ does not depend on $C$ so we can disregard this constant in the context of optimisation. If $\mathcal{B}$ is a Dirichlet operator, this expression corresponds to the penalty method. After considering the functional derivative of $\mathcal{R}$ and enforcing it to be zero, we recognise the notations of the weak form of problem \eq{pde}, where the bilinear and linear form are defined as $a_\beta(u, v) = \scal{\mathcal{D} u}{\mathcal{D} v}_\Omega + \beta \scal{\mathcal{B} u}{\mathcal{B} v}_\Gamma$ and $\ell_\beta(v) = \scal{\mathcal{D} v}{f}_\Omega + \beta \scal{\mathcal{B} v}{g}_\Gamma$. In the remainder of this work, we keep the notations of the weak form as in \eq{problem} and simply write $\mathcal{J}$ the energy to be minimised.

\subsection{Evaluation of the loss function and optimisation}

The bilinear and linear forms are sums of integrals over the domain $\Omega$ and its boundary $\Gamma$. In the general case, these integrals have to be approximated because no closed-form expressions can be obtained, or they are numerically intractable. This is why most \ac{pinn} implementations and their variants approximate the loss function with \ac{mc} integration. The integration points can be resampled at a given frequency during training to prevent overfitting or to place more points where the pointwise residual is larger \cite{wu2023}. In this work, we are precisely designing a new integration method to replace \ac{mc}.

The trainable parameters of the network are randomly initialised and a gradient descent algorithm can be used to minimise $\mathcal{J}$ with respect to $\bm{\vartheta}$. The partial derivatives of $u$ with respect to the space variable $\bm{x}$, and the gradient of the loss function with respect to the parameters $\bm{\vartheta}$ are made available through automatic differentiation.
