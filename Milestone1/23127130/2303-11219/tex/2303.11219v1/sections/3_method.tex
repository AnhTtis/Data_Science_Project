
\section{METHOD}
\label{sec:METHOD}
\subsection{Overview}
We aim to reconstruct the surfaces $\mathcal{S}$ of a solid transparent object from a set of posed object masks and the correspondences between the camera view rays and locations on the background under each viewpoint.
We propose to adopt implicit Signed Distance Function (SDF) as surface representation and leverage volume rendering~\cite{wang2021neus} to enforce the refraction-tracing consistency, which enables stable and robust optimization.
Moreover, we propose a simple but effective strategy to identify the rays passing through self-occluded parts, and then exclude these rays in the optimization to avoid mistakenly enforcing refraction-tracing consistency. 

\begin{figure}[t]
\centering
\begin{overpic}
[width=1.0\linewidth]{figure/ICCV/figure_seting_and_correspondence.pdf}
\put(11, 61){(a) Capture setup}
\put(22, -2.5){(c) Ray-location correspondence}
\put(60,61){(b) Captured image}
\end{overpic}
\caption{
(a) Our transparent object capture setup;
(b) a captured image of a real Bull object;
(c) and the ray-location correspondence. (See details in preliminaries)}

\label{fig:ray_location}
\vspace{-5mm}
\end{figure}
	
	
\subsection{Preliminaries}
 
    \textbf{Object capture setup.}
    To reconstruct the transparent objects, we adopt the same object capture system proposed in ~\cite{wu2018full,lyu2020differentiable}.
    The system consists of a static LCD monitor, a turntable, and a camera.
    The monitor displays horizontal and vertical stripe patterns that form a Gray-coded background, and is placed behind the object and the camera.
    The transparent object is placed on the turntable, which is rotated in data acquisition to provide the static camera with multiple views of the object. 
    The silhouette mask information and environment matte can be extracted from the patterns displayed on the monitor.
 \begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth,scale=1.00]{figure/ICCV/self-occlusion2.pdf}
    
    \caption{The diagram of a ray with self-occlusion $r_p$ and a ray without self-occlusion $r_e$. 
    The ray $r_p$ only refracts twice on the object surfaces, while the ray $r_e$ refracts on the surfaces more than twice due to self-occlusion.
    The rays with self-occlusion should be excluded in the optimization for high-quality reconstruction.
    }
    \label{fig:two_and_biger_two}
    \vspace{-5mm}
\end{figure}
    
    \textbf{Refraction-tracing consistency.}
    For general objects, feature points of the input images are extracted to establish correspondences for 3D reconstruction.
    However, for transparent objects, it's difficult to extract reliable feature points to establish correspondences, so the prior works and ours leverage the environment matting technique to establish the relationship between object geometry and the observed images.
    As shown in Figure~\ref{fig:ray_location}, a ray $r$ shooting from the camera center passes through the transparent object, which refracts twice on the object surfaces, and then hits on the monitor at point $Q$.
    Since the gray-coded patterns are known, we can calculate the exact location of $Q$, and therefore we obtain a pair of camera ray $r$ and hitting location $Q$.
    Our method is based on optimizing the correspondences between camera rays and the locations, which can also be named refraction-tracing consistency. 
    
	
 \subsection{SDF-based refraction tracing}
    \textbf{Surface representation.}
    Unlike that the prior works adopt point clouds or meshes as geometry representations, we adopt Signed Distance Function (SDF) as surface representation.
    Specifically, the SDF field maps a point $x\in\mathbb{R}^3$ to its signed distance value to the surfaces, and the field is encoded by a Multi-layer Perceptrons (MLP) network. The surface $\mathcal{S}$ of the object is represented by the zero-set of the signed distance function (SDF), that is,\;$\mathcal{S} = \left\{ x \in  \mathbb{R}^3 | g(x) = 0\right\}$.
    The SDF field is initialized as a unit sphere, for convenience, we denote the shape being optimized as virtual shape.

    
\textbf{Refraction-tracing.}
As shown in Figure~\ref{fig:ray_location}, given the current virtual shape, we first trace rays from the camera center that intersect and refract through the shape, and then optimize the SDF values of associated surface intersections according to the captured correspondences between the view rays and background locations (e.g., the ray $\overrightarrow{cq}$ and the location $Q$ in Figure~\ref{fig:ray_location}).
We take a ray that only refracts on the surfaces exactly twice as an example, the ray first enters the virtual shape at point $p_1^{\prime}$, and then it exits the shape at point $p_2^{\prime}$.
Finally, the simulated light path, shown in purple in Figure~\ref{fig:ray_location}, hits on the background monitor at a virtual location $Q^{\prime}$.
Before the optimization of geometry converges, $Q^{\prime}$ is generally different from the destination of the actual optical path passing through the real object, which is shown in orange, and finally hits on the background monitor at $Q$.
The goal of optimization is to minimize the differences between the virtual hitting location and real hitting location, that is, $\Delta=\left\|Q-Q^{\prime}\right\|^2$.


% \textbf{SDF-based Volume Rendering.}
To trace how the simulated light path interacts with the virtual shape and then penalize the location differences in the optimization, we leverage the SDF-based volume rendering technique~\cite{wang2021neus} to calculate the exact locations of the two refraction intersections $p_1^{\prime}$ and $p_2^{\prime}$. The SDF-based surface rendering technique~\cite{yariv2020multiview} can also be used for the intersection calculation, as discussed in Section~\ref{surface_vs_Volume}, volume rendering yields more robust and stable optimization and leads to better reconstruction quality.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth,scale=1.00]{figure/ICCV/self_occlusion_analysis.pdf}

\caption{A example of self-occlusion checking strategy applied on the real Horse object.}
\label{fig:self_occlusion_example}
\vspace{-5mm}
\end{figure}

\subsection{Self-occlusion handling}
Since the objects to be reconstructed are solid, most camera rays will refract on the object surfaces exactly twice.
When a ray passes through self-occluded regions, the refractions will be more complex and the ray may refract on the surfaces more than twice. 
As shown in Figure~\ref{fig:two_and_biger_two}, the light path without self-occlusion (blue line in Figure~\ref{fig:two_and_biger_two}) has two refracted intersections with object surfaces, while the light path with self-occlusion (green line) has four refracted intersections.
However, the prior works ignore the self-occlusion problem and assume that all the camera rays only refract exactly twice. 
As a result, for the rays that refract more than twice, the simulated light paths will be mistakenly approximated, thus introducing wrong supervision information into the geometry optimization.

\textbf{Naive checking strategy.} To tackle this problem, the key is to identify whether a ray refracts more than twice, and then exclude the ray in the optimization.
A naive solution is to calculate the exact locations of the refraction intersections. As shown in Figure~\ref{fig:two_and_biger_two}, when a ray passes through the self-occluded parts, we can leverage Snell's law to obtain the directions of the refracted lights, and then calculate the locations of the intersections, $e_1, e_2, e_3, e_4$.
However, we have to extensively conduct iterative sampling and network queries to find the points sampled in the refracted lights which are in the surfaces, which significantly increases the computational costs.

\textbf{Proposed checking strategy.} 
We, therefore, propose a simple but effective strategy to identify the rays that refract more than twice at low costs.
The motivation is based on the {law of reversibility}, that is, \textit{If the direction of a light beam is reversed, it will follow the same path.}


As shown in Figure~\ref{fig:checking_strategy}, the procedure of the strategy is introduced below:

\begin{algorithm}
\caption{Self-occlusion checking strategy}
1) Shoot a ray $r_p$($r_e$) emitting from the camera center, and get its first forward intersection $p_f$($e_f$).

2) Leverage Snell's Law to obtain the refracted light line $\overrightarrow{p_f v_p}$($\overrightarrow{e_f v_e}$), where $v_p$($v_e$) is an infinite point on the line.

3) Shoot the reversed refracted light line $\overrightarrow{v_p p_f}$($\overrightarrow{v_e e_f}$) from $v_p$($v_e$), and then obtain the backward intersection $p_b$($e_b$).

4) Sample points on the line segment $\overline{p_f p_b}$($\overline{e_f e_b}$), and then evaluate the SDF values of the points.

5) If there exist points with positive SDF values, the ray refracts more than twice; if not, the ray refracts exactly twice.

\end{algorithm}
% Based on our strategy, we can see $r_p$ doesn't pass through self-occluded parts, since there are no intersections on the line segment $\overline{p_f p_b}$.
% On the other hand, $r_p$ does pass through self-occluded parts, since there are two more intersections on the line segment $\overline{e_f e_b}$.

We can see the ray $r_e$ refracts the surfaces exactly twice, and there are no intersections on the line segment $\overline{p_f p_b}$, which indicates the light path $c \rightarrow p_f \rightarrow v_p$ is reversible.
On the other hand, for the ray $r_e$, there exist two more intersections on the line segment $\overline{e_f e_b}$, which indicates that the light path $c \rightarrow e_f \rightarrow v_e$ is not reversible with the twice refraction assumption.
Moreover, thanks to the properties of SDF (negative values inside and positive values outside), we can evaluate whether there exist any points with positive SDF values between the forward and backward intersections to identify the existence of self-occlusion.

Unlike that the naive checking strategy requires accurately finding the locations of all intersections, our proposed checking strategy only needs to identify whether there exist positive SDF values in a line segment with a short length.
We provide an example of the self-occlusion checking strategy applied on a real Horse object in Figure~\ref{fig:self_occlusion_example}, and our method can accurately identify the self-occluded regions (the overlapping legs of the horse).
After excluding the rays with self-occlusion in the optimization, the quality of reconstruction can be further improved.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth,scale=1.00]{figure/ICCV/twoIntersection.pdf}

\caption{The illustration of self-occlusion checking strategy. 
For the ray $r_p$, there are no surfaces on the line segment $\overline{ p_f p_b}$, where all SDF values of the sampled points are negative.
% the forward intersection point $p_f$ and backward intersection $p_b$.
For the ray $r_e$, there exist surfaces on the line segment $\overline{ e_f e_b}$, where the SDF values of some sampled points are positive.
}
\label{fig:checking_strategy}
\vspace{-5mm}
\end{figure}


\begin{figure*}[tp]
\centering
\begin{overpic}
[width=1.0\linewidth]{figure/ICCV/res_4_3.pdf}
\put(8,-1.5){\small Ours}
\put(27,-1.5){\small DRT\cite{lyu2020differentiable}}
\put(45,-1.5){\small Ground Truth}
\put(66,-1.5){\small Li~\etal\cite{li2020through}}
\put(86,-1.5){\small Li~\etal's GT}
\end{overpic}
\caption{Qualitative comparisons with sparse views on the Dog and Monkey objects. 
Even with a limited set of images (4 images), our method still reconstructs faithful geometry with rich details.
However, DRT and Li~\etal\cite{li2020through}  fail to reconstruct the geometries, the reconstructed models are over-smoothing and the details are missing.
It should be noted that, due to different manufacturing batches there are slight differences between the shapes used by Li~\etal and DRT, and therefore their results are compared to a different set of ground truth models.
The reconstructed and ground truth models of Li~\etal are directly obtained from their website.}
\label{compare_with_four_views}

\end{figure*}
	
	
\begin{table*}[tp]
% \toprule
\centering
\resizebox{\textwidth}{!}{
% 	\small
\begin{tabular}{cccccc|ccccc|ccccc}%l=left, r=right,c=center分别代表左对齐，右对齐和居中，字母的个数代表列数
\hline

\multicolumn{1}{c}{}  
& \multicolumn{5}{c}{Li~\etal~\cite{li2020through}} 
& \multicolumn{5}{c}{DRT\cite{lyu2020differentiable}} 
& \multicolumn{5}{c}{\textbf{Our}}\\ \cline{2-15}
% \Xcline{1-1}{0.4pt}
%  \Xhline{1pt}
& Acc $\downarrow$  & Comp $\downarrow$   & Prec $\uparrow$  & Recall $\uparrow$ & F-score $\uparrow$
& Acc $\downarrow$  & Comp $\downarrow$   & Prec $\uparrow$  & Recall $\uparrow$ & F-score $\uparrow$ 
& Acc $\downarrow$  & Comp $\downarrow$   & Prec $\uparrow$  & Recall $\uparrow$ & F-score $\uparrow$	\\ \hline
Pig   & 2.6316 & 3.0119 & 0.2363 & 0.1513 & 0.1845 & 1.1419 & 1.191 & 0.4518 & 0.4188 & 0.4347 & \textbf{0.8553} & \textbf{0.8203} & \textbf{0.4495} & \textbf{0.4823} & \textbf{0.4653} \\
Dog   & 3.2724 & 2.87  & 0.24  & 0.2047 & 0.221 & 1.3107 & 1.4511 & 0.3532 & 0.3196 & 0.3356 & \textbf{0.7783} & \textbf{ 0.6793} & \textbf{0.6213} & \textbf{0.7032} & \textbf{0.6597} \\
Mouse & 1.9328 & 2.8023 & 0.3107 & 0.2212 & 0.2584 & 1.4603 & 1.5912 & 0.3584 & 0.3169 & 0.3363 & \textbf{0.687} & \textbf{0.6166} & \textbf{0.5543} & \textbf{0.6363} & \textbf{0.5925} \\
Monkey & 1.6169    & 1.5239   & 0.3370     & 0.2012      &   0.2520    & 1.5259 & 1.4408 & 0.2461 & 0.2586 & 0.2522 & \textbf{0.8759} & \textbf{0.7961} & \textbf{0.3781} & \textbf{0.5014} & \textbf{0.4311} \\
Horse &   /    &    /   &   /    &  /     &   /    & 0.9636 & 0.9484 & 0.543 & 0.6414 & 0.5881 & \textbf{0.6859} & \textbf{0.4922} & \textbf{0.7459} & \textbf{0.8897} & \textbf{0.8115} \\
Tiger &    /   &  /     &    /   &     /  & /      & 1.2672 & 1.1394 & 0.5043 & 0.5506 & 0.5264 & \textbf{0.9476} & \textbf{0.7712} & \textbf{0.6233} & \textbf{0.7358} & \textbf{0.6749} \\
Rabbit &  /     &  /     &   /    &  /     &   /    & 1.0537 & 1.1025 & 0.4655 & 0.4274 & 0.4456 & \textbf{0.618} & \textbf{0.5433} & \textbf{0.774} & \textbf{0.8231} & \textbf{0.7978} \\
Hand &  /     &   /    &   /    &   /    &    /   &  0.8226 &  0.9190 & 0.4237 & 0.2929 & 0.3464 & \textbf{0.4021} & \textbf{0.3184} & \textbf{0.8171} & \textbf{0.8402} & \textbf{0.8285} \\
        \hline
        % \bottomrule		
    \end{tabular}
}
		
\caption{Evaluation of reconstruction with sparse views. 
Compared with DRT~\cite{lyu2020differentiable} and Li~\etal~\cite{li2020through}, our method achieves the best performance in all cases.
} 

\label{table:compare}
\vspace{-5mm}
\end{table*}
\subsection{Loss Functions}
We optimize the SDF field by sampling a batch of rays with their ray-location correspondences and object masks $\left\{Q, M\right\}$, where $Q$ is the observed location on the background monitor, and ${M}\in\left\{0, 1\right\}$ is mask value. 
We sample $n$ points on the ray and the batch size is $m$. The loss function is defined as
\vspace{-3mm}

\begin{equation}
\mathcal{L} = \omega_1\mathcal{L}_{Refraction} +  \omega_2\mathcal{L}_{Eikonal} + \omega_3\mathcal{L}_{Mask} 	
\end{equation}
	
\textbf{Refraction loss.} We minimize the difference between simulated background position $Q^{'}$ and and its corresponding captured
ground truth $Q$ (see Figure~\ref{fig:ray_location}). The refraction loss is defined as follows 
\begin{equation}			
\mathcal{L}_{Refraction} = \sum_{i \in R}(\left\|Q_i-Q_i^{\prime} \right\|^{2})	
\end{equation}
where $R$ is the set containing ray paths that go through the object and refract on surfaces exactly twice.


\textbf{Mask loss.} Following the prior works~\cite{lyu2020differentiable, wu2018full}, the mask loss is also included and defined as 
\begin{equation}
    \mathcal{L}_{mask} = BCE(M_k, O_k) \label{mask_loss}
\end{equation}
where $O_k$ is the sum of weights along the $k_{th}$ camera ray, $M_k$ is the mask of the $k_{th}$ ray, and $BCE$ is the binary cross entropy loss.

\textbf{Eikonal loss.} We add an Eikonal loss to regularize the SDF field of the sampling point on the ray to have unit norm of gradients. The loss term is defined as 
\begin{equation}
\mathcal{L}_{Eikonal} = \frac{1}{nm}\sum_{k,i}(||\triangledown g({x}_{k,i})||_2 - 1) ^2 \label{Eikonal_loss}
\end{equation}
 where ${x}_{k,i}$ is the $i_{th}$ sampled point at the $k_{th}$ ray.
	