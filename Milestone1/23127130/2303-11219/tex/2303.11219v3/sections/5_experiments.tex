\subsection{Comparisons}

\textbf{Baselines.} 
We compare our method with the two state-of-the-art baselines: 1) DRT~\cite{lyu2020differentiable}, the most related work to ours, which also optimizes the geometry by the ray-location correspondences but it adopts explicit mesh as surface representation.
2) A data-driven deep learning based approach Li et al. [2020]~\cite{li2020through}. They generate a synthetic dataset of the transparent objects, and then learn geometric priors from the training data to reconstruct the objects.

 \begin{table}[t]
% \toprule
\centering
\resizebox{\linewidth}{!}{
% 	\small
\begin{tabular}{c|ccc|ccc}%l=left, r=right,c=center分别代表左对齐，右对齐和居中，字母的个数代表列数
\hline

\multicolumn{1}{c}{}  
  		
& \multicolumn{3}{c}{DRT\cite{lyu2020differentiable}} 
& \multicolumn{3}{c}{Ours}\\ \cline{2-7}
% \Xcline{1-1}{0.4pt}
%  \Xhline{1pt}
\hline
& Acc $\downarrow$  & Comp $\downarrow$   & F-score $\uparrow$  & Acc $\downarrow$  & Comp $\downarrow$ & F-score $\uparrow$ \\
\hline
 Pig     & 0.6566 & 0.6863 & 0.7142
 & \textbf{0.5669} & \textbf{0.4689} & \textbf{0.8474} \\
Dog     & 0.9072 & 0.8704  &0.5526
    &\textbf{0.7601} &\textbf{0.6274} 
    &\textbf{0.7029} \\
Mouse   & 0.8018 & 0.839 & 0.5226 
        &\textbf{0.7788} 
        &\textbf{0.6811}  
        &\textbf{0.5998}\\
Monkey  & 0.945 & 0.8923  & 0.4422
& \textbf{0.8415} & \textbf{0.7467} & \textbf{0.4827} \\

Horse  & 0.6636 & 0.6095 & 0.8422 &\textbf{0.6193} & \textbf{0.4099} 
&\textbf{0.884}  \\
Tiger & 0.8191 & 0.723 & 0.7665 &\textbf{0.7099} & \textbf{0.5705} & \textbf{0.7979}  \\
Rabbit  & 0.5971 & 0.6202  & 0.7686 
& \textbf{0.5839} & \textbf{0.4941} & \textbf{0.8324} \\
 Hand  & 0.4792 & 0.5856  & 0.5796
 & \textbf{0.3947} & \textbf{0.3140}  & \textbf{0.7740}  \\ \hline
 Avg. 
 & 0.7337 & 0.7282 
 & 0.6485
 & \textbf{0.6568} 
 & \textbf{0.5390} 
 & \textbf{0.7401} \\ \hline
% \bottomrule		
\end{tabular}
}

\caption{Comparisons of reconstruction with full views. 
Our method obtains the best performance in all cases, and the full table is in the supplementary material.
}
% \vspace{-4mm}

\label{table:compare_full_views}
\end{table}
\begin{figure}[t]

\centering
\begin{overpic}
[width=1.0\linewidth]{figure/ICCV/real.pdf}
\put(21, -2){Ours}
\put(69, -2){\small DRT\cite{lyu2020differentiable}}
\end{overpic}
% \vspace{0.cm}
\caption{Comparisons on our self-collected data. Our method reconstructs high-quality surfaces, while the surfaces recovered by DRT contain lots of noise.}
% \vspace{-3mm}
\label{figure:real_object}

\end{figure}
\begin{figure*}[htbp]
\centering
\begin{overpic}
[width=1.0\linewidth]{figure/ICCV/ablation/pig/ab_error_map3.pdf}
\put(8, -1){\small W/o $\mathcal{L}_{Eikonal}$}
 \put(33.5, -1){\small W/o $\mathcal{L}_{Refraction}$}
 \put(59, -1){\small W/o Self-occlusion}
\put(88,-1){Full}

\end{overpic}
\caption{Qualitative ablation study on the Pig model.
For better visualization, we measure and colorize the errors between the reconstructed models with the ground truth model.
The reds indicate large errors, and the blues indicate small errors.
}
\label{Ablation}
\vspace{-3mm}
\end{figure*}
\textbf{Evaluation Protocols.} 
To evaluate the quality of reconstructed models, we calculate the metrics, accuracy, completeness, precision, recall, and F-score between the reconstructed model and the ground truth model. 
Our method and DRT adopt the same dataset provided by DRT, so the input images and the ground truth models adopted by ours and DRT are the same.
Although Li et al. experimented with transparent objects obtained from the same source, due to different manufacturing batches, there are slight differences between the shapes, and therefore their results are compared to a different set of ground truth models.
For fairness, the ground truth models of the two types are reshaped into the same scale for evaluation.
We evaluate the reconstruction results with sparse views and with full dense views.


\textbf{Reconstructions with sparse views.} 
We conduct experiments under various sparsity levels. The sparse setting selects a small proportion of the camera views
by consecutively sampling a view $I_i$ from every $sparsity = n$
camera index, i.e., $\{1, n+1, 2n+1, ... \}$.
The quantitative comparisons with $sparsity=18, 8, 4$ are presented in Table~\ref{table:compare1}, Table~\ref{table:compare2} and Table~\ref{table:compare3} respectively.
As you can see, with sparse views, our results outperform DRT and Li~\etal in terms of model completeness and accuracy. 
In addition to making quantitative comparisons, we render the model to visually observe the differences between our method and other methods. 
The qualitative comparisons are shown in Figure~\ref{compare_with_four_views}, and our method faithfully reconstructs the geometry with rich details, such as the leg of the Mouse and the eyes of the Monkey.
The reconstruction results produced by~\cite{li2020through} and~\cite{lyu2020differentiable} do not work well to reconstruct the rich geometric details and tend to over-smooth the surfaces. 
	
\textbf{Reconstruction with full views.}
When we use more views, e.g., full views (72 views), our reconstruction results and DRT are improved compared with reconstructions with sparse views.
However, based on the quantitative results presented in Figure~\ref{figure-additiaonal-full} and the qualitative results shown in Table~\ref{table:compare_full_views}, our method significantly outperforms the other methods in terms of completeness and accuracy, and the reconstructed models contain more rich details and have fewer errors.
We further evaluate a self-collected real Bull and Pig object, as shown in Figure~\ref{figure:real_object}. Our method accurately recovers the geometry with clean and smooth surfaces, while DRT mistakenly reconstructs surfaces with noises..

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}	
    \hline
    
    Method &Acc $\downarrow$  &Comp $\downarrow$     &Recall $\uparrow$  &Prec $\uparrow$    &F-score $\uparrow$ \\ \hline
    
    W/o $\mathcal{L}_{Eikonal}$     & 3.3086 & 1.5212 & 0.4   & 0.5384 & 0.4597 \\
    W/o $\mathcal{L}_{Refraction}$     & 0.7579 & 0.7019 & 0.59  & 0.6452 & 0.618 \\
    W/o Self-occlusion     & 0.6530 & 0.5440 & 0.7319  & 0.7886 & 0.7592 \\
    full  & \textbf{0.5669} & \textbf{0.4689} & \textbf{0.83} & \textbf{0.867} & \textbf{0.8474} \\

    \hline
\end{tabular}
}
\caption{Ablation study on Pig model. We test the effect of the Eikonal loss, refraction loss, and self-occlusion strategy used in the method. 
This analysis shows that the Full performs the best quantitatively.
}
\label{table:a}	
% \vspace{-1mm}
\end{table} 

\subsection{Ablation study and discussions.}

 \textbf{Ablation study.}
To better validate the effects of the self-occlusion checking strategy and the optimization terms, we conduct the ablation studies on the full method, method without self-occlusion checking, method without Eikonal loss term, and method without refraction loss term.
The quantitative evaluation is shown in Table~\ref{table:a}, and the qualitative evaluation is presented in Figure~\ref{Ablation}.
The experiments demonstrate that $\mathcal{L}_{Eikonal}$ plays the most important role, which encourages the SDF field to be continuous and smooth.
Without the refraction loss term, although our method can still reconstruct the rough shape relying on the silhouette information, the reconstruction becomes worse with larger errors.
Thanks to our proposed self-occlusion checking strategy, the quality of the self-occluded parts is improved with fewer errors, like the legs of the Pig model. 
\begin{figure}[h]
    \centering
    % \vspace{-3mm}
    \setlength{\abovecaptionskip}{0cm}
    \setlength{\belowcaptionskip}{0cm}
    % \tiny
    % \captionsetup{font={tiny,bf},justification=raggedright}
 \captionsetup{font={small},justification=raggedright}
		\begin{overpic}
          [width=1.0\linewidth]{figure/ICCV/ablation/ab_full.pdf}
          \put(0.5,-3){ Affected area}
          \put(31.5,-3){ W/ check}
          \put(59.5,-3){ W/o check}
        \end{overpic}
        % \caption{}
        \vspace{0.05cm}
        
        \caption{W/ and W/o Self-occluded check with full views.}
		\label{figure:tiger_hand}
\vspace{-3mm} 
\end{figure}
Furthermore, to better demonstrate the effectiveness of the self-occlusion checking strategy, we visualize the reconstruction error maps and the object surfaces affected by all self-occluded rays in Figure~\ref{figure:tiger_hand} (a self-occluded ray has multiple refractions on object surfaces, and thus a few rays will cause more affected surfaces), where the geometries are improved with self-occlusion check, especially on the affected regions.

\textbf{Different rendering techniques.} 
\label{surface_vs_Volume}
Both surface rendering~\cite{niemeyer2020differentiable} and volume rendering~\cite{wang2021neus} are used in neural rendering-based reconstruction.
Through experiments, we find that optimization using volume rendering is more robust and stable than using surface rendering.
As shown in Figure~\ref{rendering}, the reconstructed model using surface rendering is over-smoothing and lacks detailed geometries, while the reconstruction model using volume rendering achieves much better quality with rich details.


