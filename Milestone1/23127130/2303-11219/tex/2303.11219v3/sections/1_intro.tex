\section{Introduction}
\label{sec:intro}

\begin{figure}[tp!]
\centering
\begin{overpic}
[width=1.0\linewidth]{figure/ICCV/teaser_201.pdf}
\put(15, 45.3){\small  Ground Truth}
\put(19, -1.7){\small  DRT~\cite{lyu2020differentiable}}
\put(72,45.3){\small  Ours}
\put(59,-1.7){\small  Our synthesized view }
% \put(68,-1.4){\scriptsize  DRT\cite{lyu2020differentiable}}
\end{overpic}
\caption{Illustration of a sparse setting using only one
fourth of the camera images, i.e., $\{I_i \}_{i=1,5,9...}$, to recover the
model Dog in the DRT dataset.
Compared with DRT, our method produces more accurate renderings, which indicates the high quality of our reconstruction. The synthesized view is obtained via Blender.}

\label{figure1}
% \vspace{-5mm}
\end{figure}


\begin{figure*}[t]
\centering
\begin{overpic}
[width=1.0\linewidth]{figure/ICCV/transparent1.pdf}
\put(2.5,-0.7){ Ground Truth}
\put(22,-0.7){ Ours}
\put(37,-0.7){ DRT~\cite{lyu2020differentiable}}
\put(53,-0.7){ Ground Truth}
\put(73,-0.7){ Ours}
\put(87.7,-0.7){ DRT~\cite{lyu2020differentiable}}
\end{overpic}
\caption{The comparisons of novel view synthesis with $sparsity=8$ (9 views).
After obtaining the reconstruction models via our method and DRT, we render two views of the models via Blender.
Compared with DRT, our method produces more accurate renderings (see the red and yellow boxes), which indicates the high quality of our reconstruction.
}

\label{transparent}
% \vspace{-5mm}
\end{figure*}
% \vspace{0.5cm}

Reconstructing 3D models of real-world objects has been one of the longstanding problems. It has been researched for decades in computer vision and graphics, which boosts the development of many applications, such as augmented reality, automatic driving, and robots. However, existing general-purpose multi-view reconstruction methods~\cite{liao2021adaptive,liao2020folding,liao2021dense, schoenberger2016mvs, wang2021neus, wei2019joint} are only suitable for opaque objects whose surfaces are approximately Lambertian, and few of them can tackle transparent objects.
The light paths passing through transparent objects are extremely complex and involve refractions and reflections.

Recently, some state-of-the-art methods have been proposed to reconstruct solid transparent objects in a non-intrusive manner, capturing refraction-tracing consistency with specially designed hardware systems, and have produced impressive results. This is achieved by optimizing correspondences between camera rays and locations on the background monitor~\cite{lyu2020differentiable} or enforcing consistency between camera rays and refracted rays with a rotating background monitor~\cite{wu2018full}.
However, those methods either adopt point cloud~\cite{wu2018full} or mesh~\cite{lyu2020differentiable} as surface representation, and the explicit representations are difficult to be optimized. As a result, the methods usually require a large number of views as input for optimization. Without enough images as input, the methods easily fail to reconstruct faithful geometry due to unstable optimization (see Figure~\ref{figure1}).
		
More importantly, a critical issue still remains ignored, i.e., how to tackle the self-occluded parts of the objects.
The widely-used refraction-tracing consistency assumes that a camera ray is only refracted twice (upon entering and upon exiting) on the object surfaces when the ray passes through a transparent object.
However, the assumption is not always true when a camera ray passes through the self-occluded parts where the ray will be refracted by surfaces more than twice.
As a result, mistakenly enforcing the refraction-tracing consistency on the self-occluded parts will unavoidably introduce errors in the optimization of reconstruction, which is a bottleneck to further enhance the reconstructed geometries.

In this work, we propose a novel method, called NeTO, for reconstructing high-quality 3D geometry of solid transparent objects. In contrast to prior works~\cite{lyu2020differentiable}, we adopt implicit Signed Distance Function (SDF) as surface representation and leverage volume rendering~\cite{wang2021neus} to enforce the refraction-tracing consistency. 
Moreover, we propose a simple but effective strategy to detect the self-occluded parts and avoid mistakenly enforcing constraints on these regions.
The key idea is that we leverage the \textbf{law of reversibility}, that is, \textit{If the direction of a light beam is reversed, despite the number of times the beam is reflected or refracted, it will follow the same path}, to identify whether a camera ray is reversible or not upon the assumption that the ray is refracted exactly twice.

To validate our method, we conduct experiments on DRT~\cite{lyu2020differentiable} dataset and our collected data with full views setting and various sparse views settings.
The sparse setting selects one view from every n consecutive camera index, i.e., $\{1, n+1, 2n+1, ...  \}$, where $n$ is termed as \textit{Sparsity}.
The extensive experiments show that our method enables the high-quality reconstruction of transparent objects and outperforms the previous methods.
Our contributions can be summarized as follows:

\begin{itemize}
\item A novel neural surface reconstruction system is adopting implicit SDF as a representation for reconstructing transparent objects, thus enabling robust reconstruction optimization.
\item A self-occlusion aware refraction-tracing strategy is introduced to accurately enforce the constraint, making it possible to recover geometries with fine details.
\item Experimental results show that our method achieves SOTA results compared to prior works.
\end{itemize}
 
	




	