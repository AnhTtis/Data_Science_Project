\section{Evaluation}
\label{Sec:Evaluation}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figures/reds_deblursr_1.pdf}
    \begin{tabularx}{\textwidth}{Y Y Y Y Y Y}
        Input & Ground Truth & EDI~\cite{pan2019bringing} & eSL-Net~\cite{wang2020event} & E-CIR~\cite{song2022cir} & Ours
    \end{tabularx}
    \caption{Visualizations on REDS~\cite{Nah_2019_CVPR_Workshops_REDS}. Video results are available in the supplementary material with higher illustration quality.}
    \label{fig:reds_1}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Figures/hqf.pdf}
    \begin{tabularx}{\textwidth}{Y Y Y Y Y Y}
        Input & Ground Truth & EDI~\cite{pan2019bringing} & eSL-Net~\cite{wang2020event} & E-CIR~\cite{song2022cir} & Ours
    \end{tabularx}
    \caption{Visualizations on HQF~\cite{stoffregen2020reducing}. Video results are available in the supplementary material with higher illustration quality.}
    \label{fig:hqf}
\end{figure*}

\begin{table*}
    \begin{center}
        \begin{tabular}{c | c c c | c c c | c c c}
            \hline
            \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Video Representation} & \multicolumn{3}{c|}{Performance on REDS~\cite{Nah_2019_CVPR_Workshops_REDS}} & \multicolumn{3}{c}{Performance on HQF~\cite{stoffregen2020reducing}} \\
            \cline{2-10}
             & Frames & Polynomials & Spikes & MSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & MSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
            \hline
            EDI~\cite{pan2019bringing} & \greencheck & \redcross & \redcross & 0.182 & 21.663 & 0.664 & 0.336 & 17.822 & 0.515 \\
            eSL-Net~\cite{wang2020event} & \greencheck & \redcross & \redcross & 0.203 & 20.640 & 0.601 & 0.452 & 14.938 & 0.282 \\
            E-CIR~\cite{song2022cir} & \redcross & \greencheck & \redcross & 0.114 & 25.531 & 0.819 & 0.207 & 21.713 & 0.609 \\
            \hline
            Ours & \redcross & \redcross & \greencheck & \textbf{0.100} & \textbf{26.725} & \textbf{0.859} & \textbf{0.161} & \textbf{23.912} & \textbf{0.694} \\
            \hline
        \end{tabular}
    \end{center}
    \caption{On both the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and the HQF~\cite{stoffregen2020reducing} datasets, DeblurSR outperforms baseline approaches that represent videos by frames and polynomials in terms of the quantitative motion deblurring quality.}
    \label{tab:baseline}
\end{table*}

\begin{table*}
    \begin{center}
        \begin{tabular}{c |c c c| c c c | c c}
            \hline
            \multirow{2}{*}{Row} & \multicolumn{3}{c|}{Video Format} & \multicolumn{3}{c|}{Modules} & \multicolumn{2}{c}{MSE $\downarrow$} \\
            \cline{2-9}
            & Frames & Poly & Spiking & Auto Kpts & Integral Norm & Spatial Kernels & REDS~\cite{Nah_2019_CVPR_Workshops_REDS} & HQF~\cite{stoffregen2020reducing}  \\
            \hline
            1 & \greencheck & \redcross & \redcross & \redcross & \redcross & \redcross & 0.136 & 0.387 \\
            2 & \redcross & \greencheck & \redcross & \redcross & \redcross & \redcross & 0.114 & 0.207 \\
            3 & \redcross & \redcross & \greencheck & \redcross & \redcross & \redcross  & 0.107 & 0.235 \\
            4 & \redcross & \redcross & \greencheck & \greencheck & \redcross & \redcross & 0.103 & 0.228 \\
            5 & \redcross & \redcross & \greencheck & \greencheck & \greencheck & \redcross & 0.102 & 0.162 \\
            \hline
            6 & \redcross & \redcross & \greencheck & \greencheck & \greencheck & \greencheck & \textbf{0.100} & \textbf{0.161} \\
            \hline
        \end{tabular}
    \end{center}
    \caption{On both the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and the HQF~\cite{stoffregen2020reducing} datasets, we use ablation studies to demonstrate the strengths of the spiking representation, the automatic keypoint selection scheme, the integral normalization constant, as well as the kernel-based spatial modeling. The error trajectories on the two datasets are slightly different and discussed in Section~\ref{Sec:Evaluation:Deblurring}.}
    \label{tab:reds_ablation}
\end{table*}

%This section presents an experimental evaluation of DeblurSR. Section~\ref{Section:Datasets} summarizes the experimental setup. Section~\ref{Section:Details} describes the implementation details. Section~\ref{Sec:Evaluation:Deblurring} and Section~\ref{Sec:Evaluation:Super} evaluate DeblurSR's performance on deblurring and super-resolution.

\subsection{Experimental Setup}
\label{Section:Datasets}

We conduct experimental evaluations on two benchmark datasets. The REalistic and Dynamic Scenes (REDS)~\cite{Nah_2019_CVPR_Workshops_REDS} dataset is a popular dataset used to evaluate deblurring approaches. The original REDS dataset contains sharp videos with various real-world contents released under the CC BY 4.0 license. Following Song et al.~\cite{song2022cir}, we use the ESIM simulator~\cite{Rebecq18corl} to synthesize events and blurry images. We then employ the official training and validation splits to train and test our model, respectively. Notably, this dataset is also referred to as the GroPro dataset by some authors~\cite{wang2020event}, although a much smaller dataset popular in image-to-image deblurring approaches happens to share the same name~\cite{Nah_2017_CVPR}. 

The High Quality Frames (HQF)~\cite{stoffregen2020reducing} dataset is another benchmark recently developed to evaluate event-based vision algorithms. The dataset is available for public download, but the licensing details are unclear. The HQF dataset contains both sharp videos and the associated event captures. We apply temporal averaging to generate blurry images from the sharp video. Following Zhang et al.~\cite{zhang2022unifying}, we use five clips for testing and nine clips for training.%(\emph{desk\_fast}, \emph{desk\_hand\_only}, \emph{desk\_slow}, \emph{high\_texture\_plants}, \emph{poster\_pillar\_1}, \emph{poster\_pillar\_2}, \emph{slow\_and\_fast\_desk}, \emph{slow\_hand}, \emph{still\_life}) for training.

We compare DeblurSR to all the event-based image-to-video motion deblurring approaches with an open-source implementation known to us at the time of paper writing~\cite{pan2019bringing, song2022cir}. We additionally evaluate eSL-Net~\cite{wang2020event} by creating customized training scripts for the incomplete code on GitHub. Following the convention of baseline approaches~\cite{pan2019bringing, wang2020event, song2022cir}, our experiments focus on the grayscale data. The extension to color is straightforward since the pixel can be modeled by three intensity signals corresponding to the red, green, and blue channels. We do not include event-based image-to-image deblurring methods~\cite{sun2022event, kim2021event} or frame interpolation methods~\cite{zhang2021event, tulyakov2021time, tulyakov2022time} because of the difference in problem setup. Despite the lack of open-source implementation in the field, we hope our code release establishes a benchmark for future comparisons. 


\subsection{Training Details}
\label{Section:Details}

We implement DeblurSR under PyTorch~\cite{NEURIPS2019_9015} and utilize ADAM~\cite{kingma2014adam} to train the network for 50 epochs. We set the initial learning rate to 0.0001 and reduce the learning rate by half after 20 and 40 epochs, respectively. The number of line segments in the spiking representation is $n=10$. The dimension of spatial kernels is $k=3$. The number of histogram bins is $m=26$. The size of image and coordinate embeddings is $d=256$. More details of our experiments are available in the GitHub repository.% and will be released to the public at a suitable time.

\subsection{Motion Deblurring}
\label{Sec:Evaluation:Deblurring}
\noindent\textbf{Baseline Comparison.}
Table~\ref{tab:baseline} presents the quantitative evaluation on the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and HQF~\cite{stoffregen2020reducing} datasets. We use three image quality metrics to compare DeblurSR with different baseline approaches: the Mean Squared Error (MSE, lower is better), the Peak Signal-to-Noise Ratio (PSNR, higher is better), and the Structural Similarity Index Measure (SSIM, higher is better). 

Our method demonstrates an impressive ability in motion deblurring. Specifically, on the REDS dataset, DeblurSR improves the current best-performing method by 12.3\% in MSE, 4.7\% in PSNR, and 4.9\% in SSIM. On HQF, DeblurSR outperforms the state-of-the-art approach by 22.2\% in MSE, 10.1\% in PSNR, and 14.0\% in SSIM.

Qualitatively, as shown in Figure~\ref{fig:reds_1} and Figure~\ref{fig:hqf}, DeblurSR generates smooth and sharp frames. In particular, we point out that our results are sharper than the EDI~\cite{pan2019bringing} reconstruction, which assumes all events correspond to the same amount of intensity change. Meanwhile, our reconstructed frames are significantly less noisy than the eSL-Net~\cite{wang2020event} reconstruction, which overly emphasizes texture details. Compared with E-CIR~\cite{song2022cir}, DeblurSR offers more realistic details around the thin edges. The difference between E-CIR and DeblurSR is sometimes subtle and hard to notice from static images. We encourage readers to watch the animated visualizations provided in the supplementary material, which additionally demonstrate the temporal smoothness of our results in the video format. 

DeblurSR is computationally efficient. On REDS~\cite{Nah_2019_CVPR_Workshops_REDS}, it takes approximately 100 hours to train E-CIR~\cite{song2022cir} for 50 epochs using three Tesla V100 GPUs. By contrast, DeblurSR only requires 72 hours and two of the same GPUs under the identical training setting, representing a 28.0\% reduction in training time and a 33.3\% reduction in resource demand. The efficiency comes from the simplicity of our spiking representation in contrast with the high-order polynomial parameterization in E-CIR, which allows faster operations like computing the derivative and integral.

\noindent\textbf{Ablation Study.} Table~\ref{tab:reds_ablation} summarizes the quality of motion deblurring using different variants of the model. In the first row, we use the UNet~\cite{ronneberger2015u} architecture to regress the $d=14$ ground-truth sharp frames directly without per-pixel parameterization. In the second row, we use per-pixel polynomials~\cite{song2022cir} to continuously approximate the sharp video. Their comparison with the third row suggests that the proposed spiking representation outperforms both frame-based and polynomial-based video representations. The relative improvements on REDS~\cite{Nah_2019_CVPR_Workshops_REDS} are 21.3\% and 6.1\%, respectively. On HQF~\cite{stoffregen2020reducing}, the spiking representation improves the frame-based representation by 39.3\% but falls behind the polynomial-based representation by 13.5\%. The sub-optimal performance is caused by insufficient regularization addressed with three add-on modules in the last three rows.

The comparison between the third and fourth rows shows that the proposed automatic keypoint selection module improves the heuristic-based keypoint selection algorithm~\cite{song2022cir} by 3.7\% on REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and 3.0\%  and HQF~\cite{stoffregen2020reducing}, respectively. This result demonstrates the advantage of learning the critical timestamps from a large amount of training data.

The integral normalization constant improves the deblurring quality by 1.0\% and 29.0\% on REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and HQF~\cite{stoffregen2020reducing}, respectively. The improvement suggests that the physical model discussed in Equation~(\ref{eq:blurry-definition}) is an effective regularization for the neuromorphic spiking parameters. Noticeably, the improvement on HQF is a lot more significant than REDS. While REDS is a large synthetic dataset with 240 training clips, HQF is a small real benchmark with only 9 training clips. Empirically, we observe that the normalization constant is particularly beneficial when data is limited and the event noise is real and complex.

Finally, the last two rows in Table~\ref{tab:reds_ablation} examine the effectiveness of the spatial convolutional kernel modeling. The spatial kernel leads to 2.0\% and 0.6\% MSE improvement on REDS~\cite{Nah_2019_CVPR_Workshops_REDS} and HQF~\cite{stoffregen2020reducing}, respectively. 

\subsection{Super Resolution}
\label{Sec:Evaluation:Super}
\begin{table}[t]
    \small
    \begin{center}
        \begin{tabular}{r|c c c}
            \hline
            Methods & MSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
            \hline
            EDI (+bicubic)~\cite{pan2019bringing} & 0.196 & 20.789 & 0.550 \\
            eSL-Net~\cite{wang2020event} & 0.228 & 19.507 & 0.483 \\
            E-CIR (+bicubic)~\cite{song2022cir} & 0.142 & 23.530 & 0.622 \\
            \hline
            Ours (LR supervision) & 0.140 & 23.642 & 0.634 \\
            Ours (HR supervision) & \textbf{0.131} & \textbf{24.272} & \textbf{0.664} \\
            \hline
        \end{tabular}
    \end{center}
    \caption{We extend DeblurSR to super-resolution and evaluate the performance on the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} dataset.}
    \label{tab:reds_baseline_sr}
\end{table}

As illustrated in Figure~\ref{fig:model}, DeblurSR naturally supports super resolution because the pixel coordinates $(x, y)$ can take non-integer values such as $(1.5, 7)$, representing the midpoint of two regular pixels. This section compares DeblurSR with three different baseline approaches on the REDS~\cite{Nah_2019_CVPR_Workshops_REDS} dataset. We provide the blurry image and the events in low resolution ($180\times240$) and evaluate the results in high resolution ($720\times960$). Among the three baseline approaches, we note that only eSL-Net~\cite{wang2020event} uses ground-truth high-resolution frames in the training objective. Neither EDI~\cite{pan2019bringing} nor E-CIR~\cite{song2022cir} provides intrinsic support to super-resolution. We obtain high-resolution outputs from EDI and E-CIR through bicubic interpolation. From Table~\ref{tab:reds_baseline_sr}, we observe that DeblurSR achieves state-of-the-art performance with and without high-resolution supervision. In the absence of high-resolution supervision, DeblurSR improves E-CIR by 1.4\% in MSE. With high-resolution supervision, the relative improvement becomes 7.8\%. 
Figure~\ref{fig:sr_compare} further confirms our advantage through qualitative visualizations. We invite readers to the supplementary material for animated visualization in the video format, which provides better illustrations of our result quality.