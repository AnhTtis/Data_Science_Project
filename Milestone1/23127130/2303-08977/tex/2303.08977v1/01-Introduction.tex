\section{Introduction}
\label{Sec:Introduction}
Neuromorphic events are commonly used in deblurring algorithms~\cite{pan2019bringing, pan2020high, jiang2020learning, lin2020learning, wang2020event, shang2021bringing, zhang2021fine, han2021evintsr, xu2021motion, sun2022event, kim2021event, song2022cir}. 
Modern neuromorphic devices are extremely fast and capture up to one million events per second. While there are rich works in image-to-image deblurring~\cite{sun2022event, kim2021event} or frame interpolation~\cite{lin2020learning, shang2021bringing, tulyakov2021time, tulyakov2022time, zhang2021event}, the enormous density of neuromorphic events has been shown to enable motion-deblurring algorithms to reverse the exposure process and recover the relative movement between the camera and the environment from one single image~\cite{pan2019bringing, pan2020high, jiang2020learning, wang2020event, zhang2021fine, han2021evintsr, xu2021motion, song2022cir}. Figure~\ref{Fig:Problem} presents an illustration of the event-based image-to-video motion deblurring task.

While early motion-deblurring works apply numerical optimization techniques to directly solve for the sharp output video~\cite{pan2019bringing, pan2020high}, recent approaches utilize different data-driven pipelines as the inference model~\cite{jiang2020learning, lin2020learning, wang2020event, shang2021bringing, zhang2021fine, han2021evintsr, xu2021motion, song2022cir}. Despite these attempts, it is yet unclear how to properly instill prior knowledge about the neuromorphic event mechanism to build an effective deep learning paradigm that simultaneously addresses the motion ambiguity and emphasizes sharp edges. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Figures/problem.pdf}
    \caption{Problem Description. Imagine we are holding a camera while walking on the street. Relative movement between the camera and its surroundings causes motion blur, a visual artifact commonly observed by unprofessional photographers. With the help of neuromorphic events, which are a list of 4D points describing the coordinates, time, and polarity of intensity changes, DeblurSR converts the blurry image we take into a sharp video describing the camera's motion trajectory during the exposure interval.}
    \label{Fig:Problem}
\end{figure}

One interesting yet under-explored solution is to approximate the sharp output video by per-pixel parametric mappings from time to intensity and use deep learning to regress the parametric coefficients~\cite{song2022cir}. This allows the algorithm to fully exploit the speed of event cameras because the output theoretically has an infinitely high frame rate. However, common parametric kernels such as polynomial and trigonometric functions inherently assume that the underlying intensity signal is smooth and continuous. In reality, a sharp video contains numerous visual features that strongly contrast the background. The movement of a white object before a black background leads to instantaneous intensity flips between the two most extreme pixel values. It creates discontinuities in the intensity signal, which polynomials and trigonometric functions cannot represent. As the only existing work along this direction, E-CIR~\cite{song2022cir} relies on a refinement module independent of the continuous parameterization to compensate for its limited representation capacity, resulting in a monolithic inefficient two-stage pipeline. %Prior works in frame interpolation alternatively parameterize the intensity signal using ordinary differential equations~\cite{park2020vid} and implicit neural representations~\cite{chen2022videoinr}. These parameterizations do not have explainable coefficients and are unsuitable for incorporating event properties.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Figures/spikes.pdf}
    \put(-112, 126){$\mathbf{L}_{xy}$}
    \put(-220, 5){$-\frac{T}{2}$}
    \put(-32, 5){$-\frac{T}{2}$}
    \put(-130, 5){$t_{i}$}
    \put(-90, 5){$t_{i+1}$}
    \put(-112, 20){$O$}
    \put(-8, 20){$t$}
    \put(-108, 60){$\color{OrangeRed}\mathbf{L}_{xy} \approx m_{i}t + b_{i}$}
    \put(-108, 45){$\color{OrangeRed} (t_{i} \leq t < t_{i+1})$}
    \caption{The Spiking Representation (SR). Let $\mathbf{L}_{xy}(t)$ be a mapping from $[-\frac{T}{2}, \frac{T}{2}]$ to $[0, 1]$, where $T$ is the length of the exposure interval. Semantically, $\mathbf{L}_{xy}(t)$ represents the normalized intensity of pixel $(x, y)$ at timestamp $t$. Under the spiking representation, $\mathbf{L}_{xy}(t)$ is approximated by a piece-wise linear function, analogous to the membrane potential of the neuron on the human retina dedicated to perceiving lights for pixel $(x, y)$. \ding{172} \ding{175} \ding{178}: The membrane potential does not change when the neuron is at rest. Similarly, the pixel's intensity stays at a constant level in the absence of motion. \ding{173} \ding{176}: Spikes occur at the endpoints of the line segments. The gap between adjacent linear pieces resembles the strength of the spike, which captures edge features in the deblurred video (instantaneous intensity changes). \ding{174} \ding{177}: The slope of the line segments simulates the rate of membrane potential leakage in a biological neuron after its excitement, which enables the spiking representation to model gradual intensity changes. While the proposed spiking representation (orange) captures signal discontinuities, the continuous parameterization~\cite{song2022cir} (blue) does not.}
    \label{fig:spikes}
\end{figure}

As shown in Figure~\ref{fig:spikes}, we use a novel \textit{Spiking Representation (SR)} to approximate the per-pixel mappings from time to intensity. Imagine there is a dedicated visual receptor neuron for each pixel on the human retina. We model the light intensity of each pixel as the membrane potential of the corresponding receptor neuron, defined as the voltage difference between the interior and the exterior of the neuron cell. The spiking representation exploits a piece-wise linear function developed by neuroscientists in the 1900s under the name of the Leaky-Integrate-and-Fire model~\cite{lapicque1907recherches}. Each linear segment's slope characterizes the gradual intensity change rate over time. The gaps at segment endpoints represent abrupt intensity changes caused by the edge movements. The spiking parameters, including the slope and intercept of each line segment and the segment endpoints, are predicted by a deep neural network. We further integrate the per-pixel piece-wise linear parameterization with spatial convolutional kernels, encouraging information to propagate among neighboring pixels. We refer to the proposed intensity modeling as the spiking representation because it mimics the spiking mechanism in biological neurons. % While previous works exploit the spiking mechanism with Spiking Neural Networks (SNNs)~\cite{MAASS19971659}, SNNs are black-box models without explainable parameters. The proposed DeblurSR, on the other hand, uses the spiking representation to explain the intensity signal directly and explicitly and shares the same design principle with event cameras. 

We conduct extensive experimental analysis on the REDS dataset~\cite{Nah_2019_CVPR_Workshops_REDS} with synthetic events\footnote{REDS is referred to as the GroPro dataset by some authors~\cite{wang2020event}.} and the HQF dataset~\cite{stoffregen2020reducing} with real events. DeblurSR improves state-of-the-art reconstruction quality by 12.3\% on REDS and 22.2\% on HQF, while requiring a shorter training time and fewer computing resources. Through ablation studies, we demonstrate the strengths of three optional components, including the automatic keypoint selection module, the integral normalization constant, and the kernel-based spatial modeling. Finally, we show how DeblurSR naturally extends to video super resolution, improving the state-of-the-art method by 7.8\%.

In summary, we make the following contributions:
\begin{itemize}
\item We propose a novel Spiking Representation (SR) to parameterize sharp videos. We discuss the similarity between the spikes and the biological neural mechanism. We explain how the gaps in the spiking representation can be used to capture sharp edges. 

\item We train a deep network that regresses the spiking parameters from a blurry image and its associated events during the exposure interval. We show how to use the predicted parameters to render sharp videos.

\item In addition to motion deblurring, we extend the spiking representation to support video super resolution.

\item Despite the lack of open-source implementation in the field, we publicly release our complete source code, including all the training and testing scripts.
\end{itemize}