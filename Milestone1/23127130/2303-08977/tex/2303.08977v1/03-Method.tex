\section{Method}
\label{Sec:Method}

%We begin our discussion on DeblurSR by reviewing the basic principles of event cameras in Section~\ref{Sec:Method:Principles}. We then formally define the event-based motion deblurring problem in Section~\ref{Sec:Method:Problem}. Finally,  Section~\ref{Sec:Method:Algorithm} explains the details of the proposed DeblurSR model.

\subsection{Event Camera Principles}
\label{Sec:Method:Principles}
On a pixel grid with resolution $h \times w$, let $\mathbf{L}_{xy}(t)$ be the intensity of pixel $(x, y)$ at time $t$. In the natural logarithmic space, consider the amount of intensity change from the previous timestamp $t'$ to the current timestamp $t$:
\begin{equation}
    \Delta ln[\mathbf{L}_{xy}(t)] = ln[\mathbf{L}_{xy}(t)] - ln[\mathbf{L}_{xy}(t')]
\end{equation}

At time $t$, an \textit{event}, $(x, y, t, p)$, indicates that for pixel $(x, y)$, the instantaneous intensity change exceeds the event generation threshold:
\begin{equation}
    \Delta ln[\mathbf{L}_{xy}(t)] \geq \begin{cases} 
            c^+ & \text{if } p = 1 \\
            c^- & \text{if } p = -1
        \end{cases}
    \label{eq:polarity-definition}
\end{equation}
where $p = \pm1$ is the polarity of the event; $c^+$ and $c^-$ are event generation thresholds corresponding to positive events (intensity increments) and negative events (intensity decrements), respectively.

Modern event cameras simultaneously capture a stream of high-speed events and another stream of low-speed conventional frames. Let $\mathbf{B}$ be the $h \times w$ conventional frame captured during an exposure interval $[-\frac{T}{2}, \frac{T}{2}]$ with length $T$. Mathematically, the conventional frame is the temporal average of the true physical intensities:
\begin{equation}
    \mathbf{B}_{xy} = \frac{1}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}} \mathbf{L}_{xy}(t) dt
    \label{eq:blurry-definition}
\end{equation}
During the exposure interval, relative motion between the camera and environment leads to undesirable blurriness in the conventional frame capture $\mathbf{B}$, which is a visual artifact commonly observed by unprofessional photographers.

\subsection{Problem Description}
\label{Sec:Method:Problem}
As illustrated by Figure~\ref{Fig:Problem}, the input to the event-based motion deblurring problem includes a blurry image $\mathbf{B} = \{\mathbf{B}_{xy}\}$ and all the events $\mathbf{E} = \{(x, y, t, p) | -\frac{T}{2} \leq t \leq \frac{T}{2}\}$ occurred during the exposure interval. The output of the problem is a sharp video with a temporal range of $[-\frac{T}{2}, \frac{T}{2}]$.

\subsection{Prediction Algorithm}
\label{Sec:Method:Algorithm}
\noindent\textbf{The Spiking Representation.} For each pixel $(x, y)$, we propose to approximate its latent intensity $\mathbf{L}_{xy}(t)$ as a parametric mapping from the temporal space $[-\frac{T}{2}, \frac{T}{2}]$ to the normalized intensity space $[0, 1]$. This contrasts with the conventional video representation, where $\mathbf{L}_{xy}(t)$ is characterized by discrete samples uniformly distributed across the exposure interval, and enjoys the advantage of having an infinitely high frame rate. Given a blurry image and its associated events, our algorithm learns to predict the coefficients of latent intensity parametric mappings for all the pixels. As shown in Figure~\ref{fig:spikes}, the proposed spiking representation uses disconnected line segments to approximate the per-pixel intensity mappings. Within each segment, the coefficients include the slope and the intercept. Our design is inspired by the biological mechanism that controls how biological neurons interact with each other. Imagine there is a dedicated visual receptor neuron for each pixel in our eyes, and the pixel intensity corresponds to the membrane potential in the visual receptor, defined as the difference in electric voltages between the interior and the exterior of the neuron cell. In the absence of a significant visual stimulus, the membrane potential (i.e., the pixel intensity) changes at a linear rate over time. The rate of this linear change is characterized by the slope of each line segment. Upon encountering a strong stimulus, the membrane potential is expected to change dramatically. Such a strong stimulus corresponds to an edge feature in the video and is captured by a \textit{spike}. A spike, whose magnitude is characterized by the ``gap'' between adjacent linear pieces, is analogous to the excitement of a biological neuron. The neural excitement opens ion channels on the cell membrane for a short period, producing an abrupt change in the potential. By mimicking these biological principles, the spiking representation simulates how human visual receptors respond to different visual stimuli.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{Figures/model.pdf}
\put(-255, 190){\small{$(x, y)$}}
\put(-275, 90){\small{$t_r \in \{ t_{r_1}, \dots, t_{r_N} \}$}}
\put(-192, 64){\small{$m_{{xy}_i}$}}
\put(-189, 54){\small{$b_{{xy}_i}$}}
\put(-187, 44){\small{$t_{{xy}_i}$}}
\caption{The Overall Pipeline. Given a blurry image and its associated events in the exposure interval, we apply a Convolutional Neural Network (CNN) to extract an image embedding with the same spatial resolution as the input. For each pixel $(x, y)$, we fuse the image embedding with the coordinate embedding using the addition operation. A group of fully-connected layers take the resulting per-pixel feature vector as input and regress the spiking parameters for each pixel as output. At time $t_r$, we assemble a spatially varying kernel from the predicted spiking parameters. The convolution of this kernel with the input blurry image gives the output sharp frame at time $t_r$. By changing the timestamps, the spiking representation allows DeblurSR to render a sharp video with an arbitrarily high frame rate.}
\label{fig:model}
\end{figure*}

\noindent\textbf{Automatic Keypoint Selection Scheme.} As illustrated in Figure~\ref{fig:spikes}, a piecewise linear function with $n$ pieces has $n+1$ endpoints. In this paper, we use the term \textit{keypoints} to refer to the timestamps of these endpoints when the spikes happen. For each pixel $(x, y)$, the sharp intensity $\mathbf{L}_{xy}(t)$ is parameterized by $n$ slopes, $n$ intercepts, and $n+1$ keypoints:
\begin{equation}
    \mathbf{L}_{xy}(t) \approx \begin{cases}
    m_{{xy}_1}t + b_{{xy}_1} & t_{{xy}_1} \leq t < t_{{xy}_2} \\
    m_{{xy}_2}t + b_{{xy}_2} & t_{{xy}_2} \leq t < t_{{xy}_3} \\
    \dots \\
    m_{{xy}_n}t + b_{{xy}_n} & t_{{xy}_n} \leq t \leq t_{{xy}_{n+1}} \\
    \end{cases}
    \label{eq:spking-definition-1}
\end{equation}
Semantically, a keypoint represents a critical moment when the pixel's intensity changes significantly, which has a natural correlation to the event generation model described in Equation~(\ref{eq:polarity-definition}). However, the raw event data is highly irregular. During the exposure interval, different pixels have vastly different numbers of events, presenting a substantial challenge for efficient computation in parallel. Events are also known to be noisy~\cite{pan2019bringing, wang2020event, zhang2022unifying}. It is, therefore, inappropriate to directly extract the raw event timestamps as keypoints. To effectively exploit the correlation between events and keypoints while allowing efficient parallel computation, we propose the automatic keypoint selection module that is also robust against noise. Given the blurry image and its associated events, we employ a neural network to predict a set of $n$ segment widths for each pixel: $\{w_{{xy}_1}, w_{{xy}_2}, \dots, w_{{xy}_n}\}$. Through activation functions, we normalize the sum of these widths to the exposure length, $T$. The keypoints are given by $t_{{xy}_1} = -\frac{T}{2}$ and:
\begin{equation}
    t_{{xy}_i} = t_{{xy}_{i-1}} + w_{{xy}_{i-1}} \quad i = 2, 3, \dots, n+1
\end{equation}
An alternative to the automatic keypoint selection scheme is to use a heuristic-based algorithm to choose the keypoints from all event timestamps. This approach is similar to how E-CIR~\cite{song2022cir} decides the Lagrange bases for the polynomial functions. In Section~\ref{Sec:Evaluation:Deblurring}, we use an ablation study to demonstrate the superiority of our proposed automatic keypoint selection scheme.

\noindent\textbf{Integral Normalization Constant.} 
The spiking representation discussed above may not satisfy the physical model in Equation~(\ref{eq:blurry-definition}) if the coefficients are predicted by a neural network. To fully utilize the information in the blurry input image, we amend the spiking representation in Equation~(\ref{eq:spking-definition-1}) with a normalization constant $c_{xy}$:
\begin{equation}
    \mathbf{L}_{xy}(t) \approx c_{xy} + \begin{cases}
    m_{{xy}_1}t + b_{{xy}_1} & t_{{xy}_1} \leq t < t_{{xy}_2} \\
    m_{{xy}_2}t + b_{{xy}_2} & t_{{xy}_2} \leq t < t_{{xy}_3} \\
    \dots \\
    m_{{xy}_n}t + b_{{xy}_n} & t_{{xy}_n} \leq t \leq t_{{xy}_{n+1}} \\
    \end{cases}
    \label{eq:spking-definition-2}
\end{equation}
where, given $m_{{xy}_i}$'s, $b_{{xy}_i}$'s, and $\mathbf{B}_{xy}$, the value of $c_{xy}$ can be solved analytically from Equation~(\ref{eq:blurry-definition}) by calculating the indefinite integral of Equation~(\ref{eq:spking-definition-2}). Readers familiar with deep learning may expect the prediction network to naturally deduce the physical image formation model from the training set without direct supervision. In Section~\ref{Sec:Evaluation:Deblurring}, we use an ablation study to show how the proposed normalization constant plays a vital role in improving deblurring quality, especially when the training data size is limited.

\noindent\textbf{Kernel-Based Spatial Modeling.} The edge sharpness largely determines the quality of motion deblurring. While the above discussion addresses temporal intensity changes, it fails to incorporate spatial brightness variations, which is also key to edge sharpness. To address this issue, we predict $k^2$ different sets of spiking parameters for each pixel and use them to assemble a $k \times k$ convolutional kernel $\mathbf{K}_{xy}(t)$. The sharp intensity is then given as:
\begin{equation}
    \mathbf{L}_{xy}(t) \approx c_{xy} + \mathbf{K}_{xy}(t) \circledast \mathcal{N}(\mathbf{B}_{xy})
\end{equation}
where $\circledast$ stands for the convolution operator, and $\mathcal{N}(\mathbf{B}_{xy})$ is a $k \times k$ neighborhood for pixel $(x, y)$ in the input blurry image. Section~\ref{Sec:Evaluation:Deblurring} demonstrates the effectiveness of this spatial kernel-based modeling through an ablation study.

\noindent\textbf{Prediction Pipeline.} We present the overall motion deblurring pipeline in Figure~\ref{fig:model}. Following E-CIR~\cite{song2022cir}, we first voxelize the irregular event input into an $m \times h \times w$ histogram tensor, where $m$ is the number of histogram bins. We then concatenate this histogram tensor with the blurry image, creating an input tensor whose dimensions are $(m+1) \times h \times w$. A UNet~\cite{ronneberger2015u}-based convolutional neural network takes this concatenated tensor as input and extracts a $d \times h \times w$ dimensional image embedding. Next, we utilize a linear layer to predict another $d$-dimensional coordinate embedding from the 2D coordinates of each pixel. We fuse the pixel-wise image and coordinate embeddings by the addition operation and employ fully-connected layers to regress the spiking parameters, including $k^2$ slopes, $k^2$ intercepts, and $n$ widths between $n+1$ keypoints for each pixel, where $k$ is the spatial kernel size. The fully connected layers have a total output size of $2k^2+n$ Finally, to render a sharp video with $N$ frames, we assemble the spatial kernels at rendering timestamps $ \{ t_{r_1}, \dots, t_{r_N} \}$ and convolve the kernels with the blurry image. The entire pipeline is trained end-to-end using the L1 loss on reconstructed sharp frames. We refer readers to the DeblurSR GitHub repository for implementation details due to space constraints. The proposed spiking representation possesses an intrinsic ability to capture discontinuous sharp edges, allowing DeblurSR to have a simple one-stage prediction pipeline. This improves the two-stage pipeline in E-CIR, where a refinement module independent of the continuous parameterization is designed to enhance sharpness. 

\noindent\textbf{Extension to Super Resolution.} Thanks to coordinate embedding, DeblurSR can predict the spiking parameters for pixels with non-integer coordinates in between two regular pixels. During testing, the coordinates take non-integer values such as $(1.5, 7)$, representing the midpoint of two regular pixels. This allows a natural support to video super resolution. Formally, given a blurry image and events defined on an $h \times w$ grid, the super-resolution problem aims at reconstructing a sharp video with a higher resolution $h' \times w'$.  In Section~\ref{Sec:Evaluation:Super}, we show that DeblurSR achieves state-of-the-art super-resolution performance even without any high-resolution training supervision.