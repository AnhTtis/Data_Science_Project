

\section{Related Work}
\label{Sec:RelatedWork}

\subsection{Event Cameras and Spiking Neural Networks}
\label{Sec:RelatedWork:EventCameras}
Event cameras~\cite{4444573, mahowald1992vlsi} react to intensity changes and produce a stream of \textit{events}. Each event is represented as a 4-tuple $(x, y, t, p)$, including the pixel coordinates, the timestamp, and the binary polarity of the intensity change. Modern event cameras%\footnote{An example is DAVIS240 manufactured by iniVation.}
are able to detect up to one million events per second, allowing them to capture extremely fast motion details. However, events are noisy and unable to account for the exact magnitudes of the intensity changes, leading to challenges in algorithm design. A thorough introduction to the technical properties of event cameras is beyond the scope of this paper. We refer interested readers to~\cite{9138762} for a comprehensive survey.

Spiking Neural Networks (SNNs)~\cite{MAASS19971659} are popular tools to build an event-based vision system because both event cameras and SNNs simulate how biological neurons behave. Different mathematical models are constructed to explain the spiking mechanism~\cite{lapicque1907recherches, dutta2017leaky, hodgkin1952quantitative, izhikevich2003simple, borisyuk1997information}. Our work is particularly inspired by the Leaky-Integrate-and-Fire (LIF) model~\cite{lapicque1907recherches}, where each neuron has a membrane potential whose value decays linearly with time and plunges abruptly whenever there is a spike. While prior works in event vision exploit SNNs as a black-box inference network~\cite{o2013real, 7280696, esser2016convolutional, rueckauer2017conversion, zhang2021event, zhu2022event, zhang2022spiking, lee2022fusion}, we extend the LIF model to directly model intensity changes in a video under the spiking representation. For readers who are familiar with SNNs, the most significant difference between SNNs and DeblurSR is that SNNs implicitly store the membrane potential and explicitly output the spikes, whereas DeblurSR explicitly models the membrane potential of visual receptor neurons and implicitly represents the spikes through parameterized coefficients.

\subsection{Motion Deblurring}
\label{Sec:RelatedWork:MotionDeblurring}

Motion deblurring algorithms turn a blurry image into a sharp video, allowing humans and downstream vision applications to understand movements during the image formation process. Motion deblurring is an ill-posed problem because the blurry image alone fails to capture critical motion parameters such as the moving direction and speed~\cite{jin2018learning, purohit2019bringing}. For example, in Figure~\ref{Fig:Problem}, it is impossible to tell whether the photographer is moving forward into the woods or backward out of the screen from the blurry image itself. Thanks to the fast data rate of event cameras, several prior works utilize event streams to supplement the blurry input image to address the motion ambiguity~\cite{pan2019bringing, pan2020high, jiang2020learning, lin2020learning, wang2020event, shang2021bringing, zhang2021fine, han2021evintsr, xu2021motion, tulyakov2021time, Paikin_2021_CVPR, tulyakov2022time, zhang2022unifying, song2022cir, sun2022event, kim2021event}. Closely related to our design, E-CIR~\cite{song2022cir} represents pixel intensities in the output video as polynomial functions characterized by the temporal derivatives as selected event timestamps. This paper argues that the continuous nature of the polynomial representation limits the ability of E-CIR to generate sharp videos. On the other hand, the proposed spiking representation resembles the same biological principle that is followed by event cameras determining how neurons communicate with each other in living organisms, allowing DeblurSR to represent sharp discontinuous edges and enjoy strong interpretability. %A few recent works combine motion deblurring and frame interpolation by augmenting the input image with its neighboring frames and demonstrate the advantage of jointly solving two problems~\cite{tulyakov2021time, Paikin_2021_CVPR, tulyakov2022time, zhang2022unifying}. This paper focuses on the primitive problem of motion deblurring for simplicity. %We leave it as future work to explore the entanglement with frame interpolation.

\subsection{Video Representation}
\label{Sec:RelatedWork:VideoRepresentation}

While frames have long been accepted as the fundamental building block of videos~\cite{Li2018}, recent studies represent videos as a parametric mapping from time to pixel intensities. Vid-ODE~\cite{park2020vid} uses ordinary differential equations to model video generation, where the unknowns are latent variables carrying visual information. The latent variables can be solved and decoded at arbitrary timestamps, allowing synthesized videos to have an infinitely high temporal resolution. VideoINR~\cite{chen2022videoinr} applies multi-layer perceptrons to parametrize the mapping from space-time coordinates to color values, allowing the output to have infinitely high resolution in both temporal and spatial dimensions. While Vid-ODE and VideoINR are black boxes built upon unexplainable parameters and used without event cameras, E-CIR~\cite{song2022cir} approximates the time-to-intensity mapping as a polynomial, whose coefficients can be interpreted as infinitesimal intensity changes at event timestamps. In contrast, this paper exploits a novel spiking representation, enabling DeblurSR to capture both the continuity and discontinuity in the intensity signal. The spiking parameters are also fully interpretable from the neuromorphic perspective.