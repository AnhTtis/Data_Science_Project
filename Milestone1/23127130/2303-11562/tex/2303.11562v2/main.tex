\documentclass{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}   
\usepackage{hyperref}    
\usepackage{url}        
\usepackage{booktabs}   
\usepackage{wrapfig}
\usepackage{amsfonts}     
\usepackage{nicefrac}  
\usepackage{microtype}   
\usepackage[dvipsnames, svgnames, x11names]{xcolor} 
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{bbding}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{listings}
\usepackage[ruled,vlined]{algorithm2e}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage[capitalize,noabbrev]{cleveref}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{accents}
\usepackage[textsize=tiny]{todonotes}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{mathrsfs}
\usepackage{times}
\usepackage{latexsym}
\usepackage{comment}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

\newcommand{\op}{o_{\raisemath{-1.5pt}\PP}}
\newcommand{\Op}{O_{\raisemath{-1.5pt}\PP}}

\usepackage[]{mathtools}   
% \def\##1\#{\begin{align}#1\end{align}}
\def\$#1\${\begin{align*}#1\end{align*}}
\usepackage{enumitem}
\usepackage[]{amsthm} 
\usepackage[]{amssymb}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{best}{HTML}{BAFFCD}
\definecolor{issue}{HTML}{FFC8BA}
\definecolor{bad}{HTML}{FFC87C}
\newcommand{\good}[1]{\cellcolor{best}#1} 
\newcommand{\bad}[1]{\cellcolor{issue}#1} 
\newcommand{\better}[1]{\cellcolor{issue}#1} 

\newtheorem{example}{Example}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\input{src/math}
\newcommand{\BR}{\bm{1}}
\newcommand{\QQ}{\mathbb Q}
\newcommand{\RR}{\mathbb R}
\newcommand{\Hy}{\mathcal{H}}
\newcommand{\A}{\textsf{A}}
\newcommand{\f}{f^*}
\newcommand{\D}{\mathcal D}
\newcommand{\x}{x}
\newcommand{\ny}{\tilde{y}}
\newcommand{\nY}{\widetilde{Y}}

\usepackage{xcolor}

\newcommand{\wjh}[1]{\textbf{\color{blue}(Jiaheng: #1)}}
\newcommand{\zzw}[2]{\textbf{\color{blue}(Zhaowei: #1)}{\color{blue}~#2}}
\newcommand{\lty}[3]{\textbf{\color{blue}(Tianyi: #1)}{\color{blue}~#2}}

\ifodd 1
\newcommand{\rev}[1]{{\color{black}#1}}
\newcommand{\yl}[1]{\textbf{\color{red}(Yang: #1)}}
\newcommand{\ak}[1]{\textcolor{brown}{[AK: #1]}}
\newcommand{\ea}[1]{\textcolor{magenta}{[EA: #1]}}

\newcommand{\clar}[1]{\textbf{\color{green}(NEED CLARIFICATION: #1)}}
\newcommand{\response}[1]{\textbf{\color{magenta}(RESPONSE: #1)}}
\else
\newcommand{\rev}[1]{#1}
\newcommand{\com}[1]{}
\newcommand{\clar}[1]{}
\newcommand{\response}[1]{}
\fi
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\title{Dynamics-Aware Loss for Learning with Label Noise}


\author{Xiu-Chuan Li$^{1}$ \ \ \ \ \ Xiaobo Xia$^{2}$ \ \ \ \ \ Fei Zhu$^{1}$ \ \ \ \ \ Tongliang Liu$^{2}$ \ \ \ \ \ Xu-Yao Zhang$^{1}$ \ \ \ \ \ Cheng-Lin Liu$^{1}$ \\
$^1$Institute of Automation, Chinese Academy of Science \quad\quad
$^2$The University of Sydney \\
\texttt{lixiuchuan2020@ia.ac.cn} \ \ \ \ \ 
\texttt{xiaoboxia.uni@gmail.com} \ \\
\texttt{zhufei2018@ia.ac.cn} \ \ \ \ \  
\texttt{tongliang.liu@sydney.edu.au} \ \\
\texttt{xyz@nlpr.ia.ac.cn} \ \ \ \ \  
\texttt{liucl@nlpr.ia.ac.cn} \ \\
}
\begin{document}

\maketitle

\newcommand{\myPara}[1]{\vspace{.05in}\noindent\textbf{#1}}

\begin{abstract}
		Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method. Our source code can be found in \textit{https://github.com/XiuchuanLi/DAL}.
\end{abstract}

\section{Introduction}

Deep neural networks (DNNs) have achieved tremendous success in various supervised learning tasks, however, this success relies heavily on correctly annotated large-scale datasets. Collecting a large and perfectly annotated dataset can be both expensive and time-consuming, leading many to opt for cheaper methods like querying search engines, which inevitably introduces label noise. Unfortunately, the work of~\cite{overfit} has empirically proved that DNNs can easily fit an entire training dataset with any ratio of noisy labels, resulting in poor generalization performance eventually. Therefore, developing robust algorithms against label noise for DNNs is of great practical significance.

One effective and scalable solution for handling label noise is to use robust loss functions~\cite{gce, tce, js}. These methods have the advantage of not altering the training process. Specifically, they require no extra information such as the noise rate or a clean validation set, and incur no additional memory burden or computational cost. Cross entropy (CE) is observed to result in serious overfitting in the presence of label noise due to its strong fitting ability. Meanwhile, although mean absolute error (MAE) is theoretically robust against label noise~\cite{mae}, it suffers from severe underfitting in practice, especially on complex datasets. In light of this, many robust loss functions have been proposed to reconcile fitting ability with robustness against label noise, among which the generalized cross entropy (GCE)~\cite{gce} is the most representative method. As shown in Figure~\ref{fig_process}, selecting a suitable value for $q \in (0,1)$ such as 0.7, GCE outperforms both CE and MAE by a large margin.

Besides devising robust learning algorithms, some work aimed to delve deep into the dynamic nature of DNNs learning with label noise.  The work of~\cite{dynamic} observed that while DNNs are capable of memorizing noisy labels perfectly, there are noticeable differences in DNNs' learning status at different stages of the training process. Specifically, DNNs tend to first learn generalized patterns shared by the majority of training examples, and then gradually overfit label noise. Further evidence of this observation is provided by~\cite{dynamic2} which showed that DNNs first learn simple representations via subspace dimensionality compression, then memorize noisy labels through subspace dimensionality expansion. 

\begin{figure*}[t]
	\centering
	\includegraphics[width= \textwidth]{img/motivation.pdf}
	\caption{Performance on CIFAR-100 with 40\% instance-dependent noise. (a) CE overfits label noise while MAE suffers from serious underfitting. (b) GCE with proper $q$ outperforms both CE and MAE by a large margin. (c) The performance of DAL both rises quickly at the early stage and grows steadily afterwards, which is remarkably better than GCE. Moreover, DAL is much less sensitive to its hyper-parameter than GCE.}
	\label{fig_motivation}
\end{figure*}

It is clear from the above views that there exists a discrepancy between the statics of robust loss functions and the dynamics of DNNs learning with label noise, resulting in inferior performance. Specifically, with a static trade-off between fitting ability and robustness, the classification accuracy fails to both rise rapidly at the early stage and maintain steady growth thereafter. As demonstrated in Figure~\ref{fig_motivation}, although GCE with $q=0.7$ eventually achieves the highest test accuracy, the figure still experiences a remarkable drop at the later stage of the training process. If we slightly enhance robustness by increasing $q$ to 0.75 or 0.8, the test accuracy grows steadily, but remains low throughout the training process. On the other hand, if we slightly improve fitting ability by decreasing $q$ to 0.65 or 0.6, although the test accuracy rises quickly at the early stage, it drops more dramatically later on. Moreover, the performance of GCE is quite sensitive to $q$, which makes it less applicable in real world.

Fortunately, a loss function with a dynamic trade-off between fitting ability and robustness can solve the above problem. Specifically, we prioritize fitting ability to rapidly increase the classification accuracy at the early stage, then gradually increase the weight of robustness to ensure steady performance growth thereafter. Furthermore, to reduce the negative impact of label noise, we place greater emphasis on easy examples compared to hard ones at the later stage, as easy examples are more likely to be correctly labeled~\cite{coteaching,coteaching+}. Also, we incorporate a bootstrapping term at the later stage to combat underfitting. We refer to the proposed method dynamics-aware loss (DAL) and exhibit its performance in Figure~\ref{fig_motivation}. With $q_s=$0.6, its performance could both rise quickly at the early stage and grow steadily later on, resulting in a substantial improvement compared to GCE. Moreover, DAL is much less sensitive to its hyper-parameter $q_s$ compared to GCE. As shown in Figure~\ref{fig_motivation}, varying $q_s$ in [0.55, 0.65] only results in a performance fluctuation of approximately 1\%.

In summary, our key contributions include
\begin{itemize}
	\item We address the discrepancy between the static robust loss functions and the dynamic learning process of DNNs by proposing the Dynamics-Aware Loss (DAL), which prioritizes fitting ability initially and gradually increases robustness thereafter. Besides, DAL places greater emphasis on easy examples over hard ones and introduces a bootstrapping term at the later stage of the training process, further mitigating the negative impact of label noise.
	\item We present detailed analyses to certify the superiority of DAL without any assumption about the label noise model.
	\item Our extensive experiments on various benchmark datasets demonstrate the superior performance and practicality of DAL compared to other robust loss functions. We also empirically verify that it is complementary to more advanced methods and that it helps to improve backdoor robustness.
\end{itemize}

The rest of the paper is organized as follows. In Section~\ref{sec_work}, we give a brief review of related work on learning with label noise. In Section~\ref{sec_method}, we introduce our proposed DAL in detail. In Section~\ref{sec_experiment}, we provide extensive experimental results. Finally, we conclude the paper in Section~\ref{sec_conclusion}.

\section{Related Work} \label{sec_work}

In this section, we briefly summarize existing work in the field of learning with label noise. For a more detailed discussion please refer to~\cite{survey}.

\subsection{Noise Transition Matrix Estimation}
In theory, the clean class posterior can be inferred by combining the noisy class posterior and the noise transition matrix that reflects the label flipping process. Accurate estimation of the noise transition matrix is crucial for building statistically consistent classifiers, which converge to the optimal classifiers derived from clean data. However, a large estimation error in the noise transition matrix can significantly degrade the classification accuracy, and most methods presuppose high-quality features such as anchor points. To mitigate the above limitations, many studies have focused on reducing the estimation error of the noise transition matrix~\cite{estimation4, estimation5} and reducing the requirement of high-quality features~\cite{anchor, beyond}.

\subsection{Loss Adjustment}
These methods adjust the loss of each training example before back-propagation, which could be further divided into loss reweighting~\cite{meta, reweighting3} and label correction~\cite{correction3, correction4}. The former assigns smaller weights to the potentially incorrect labels, which is usually realized by meta-learning that trains a meta DNN on a clean dataset. The latter uses model predictions to correct the provided labels.

\subsection{Sample Selection}
These approaches aim to identify correctly labeled examples from a noisy training dataset. While small-loss trick is widely used for selecting clean labels, some recent studies~\cite{selection1, selection2} proposed more advanced approaches. After selecting correct labels, some approaches~\cite{coteaching, jocor} directly remove wrong labeled examples and train DNNs on the remained data, while others~\cite{dividemix, scanmix} only discard wrong labels but preserve the corresponding instances, then they leverage semi-supervised learning to train DNNs. To reduce the accumulated error caused by incorrect selection, these approaches usually maintain multiple DNNs and refine the selected set iteratively.

\subsection{Regularization}
Many regularization methods introduce regularizers into loss functions. To name a few, the work of~\cite{peer} randomly pair an instance and another label to construct a peer sample, then uses a peer regularizer to punish DNNs from overly agreeing with the peer sample. Generalized Jenson-Shannon Divergence (GJS)~\cite{js} introduces a consistency regularizer forcing DNNs to make consistent predictions given two augmented versions of a single input. Early learning regularization (ELR)~\cite{elr} encourages the predictions of DNNs to agree with the exponential moving average of the past predictions. However, it is important to note that these regularization methods come with additional memory burden or computational costs, unlike robust loss functions. There are also other types of regularizations such as contrastive learning~\cite{sscl}, model pruning~\cite{prune}, over-parameterization~\cite{sop}, and so on.

\subsection{Robust Loss Function}
While the commonly used CE easily overfits label noise due to its strong fitting ability, although MAE is theoretically noise-tolerant~\cite{mae}, it suffers from underfitting due to its poor fitting ability. Subsequently, a large amount of work improves generalization by reconciling fitting ability with robustness in different ways. While GCE~\cite{gce} is an interpolation between CE and MAE, symmetric cross entropy (SCE)~\cite{sce} equals a convex combination of CE and MAE, and active passive loss~\cite{apl} just replaces CE in SCE with normalized CE. Taylor cross entropy~\cite{tce} realizes an interpolation between CE and MAE through Taylor Series, while the work of~\cite{js} scales the Jensen-Shannon Divergence to construct the interpolation. Robust loss functions cause no changes to the training process, they require no extra information such as the noise rate or a clean validation set, and incur no additional memory burden or computational cost.

\section{Method} \label{sec_method}

\subsection{Preliminary}
\noindent \textbf{Risk minimization} \quad We consider a typical $k$-class classification problem where $k \geq 2$. Denote the feature space by $\mathcal{X} \subset \mathbb{R}^d$ and the class space by $\mathcal{Y} = [k] = \{1,...k\}$. The classifier $\arg \max_{i} f_i(\cdot)$ is a function that maps feature space to class space, where $f: \mathcal{X} \rightarrow \mathcal{C}$, $\mathcal{C} \subseteq [0, 1]^k$, $\forall \mathbf{c} \in \mathcal{C}$, $\mathbf{1}^T\mathbf{c}=1$. In this paper, we consider the common case where $f$ is a DNN with a softmax output layer. For brevity, we call $f$ as the classifier in the following. Without label noise, the training data $\{\mathbf{x}_i, y_i\}_{i=1}^N$ is drawn i.i.d. from distribution $p(\mathbf{x}, y)$ over $\mathcal{X} \times \mathcal{Y}$. Given a loss function $L: \mathcal{C} \times \mathcal{Y} \rightarrow \mathbb{R}_+$ and a classifier $f$, the $L$ risk of $f$ is defined as
\begin{equation}
	R_L(f)  = \mathbb{E}_{p(\mathbf{x},y)} [L(f(\mathbf{x}), y)]  = \mathbb{E}_{p(\mathbf{x})} [\sum_y L(f(\mathbf{x}), y) p(y|\mathbf{x})], \label{eq_rm}
\end{equation}
where $\mathbb{E}$ represents expectation. In the following, we denote $\sum_y L(f(\mathbf{x}), y) p(y|\mathbf{x})$ by $R_L(f(\mathbf{x}))$. Under the risk minimization framework, our objective is to learn $f^*_L = \arg \min_{f} R_L(f)$. In the presence of label noise, we can only access the noisy training data $\{\mathbf{x}_i, \tilde{y}_i\}_{i=1}^N$ drawn i.i.d. from distribution $\tilde{p}(\mathbf{x}, \tilde{y})$. In this case, the $L$ risk of $f$ is defined as
\begin{equation}
	\tilde{R}_L(f) = \mathbb{E}_{\tilde{p}(\mathbf{x},\tilde{y})} [L(f(\mathbf{x}), \tilde{y})] = \mathbb{E}_{p(\mathbf{x})} [\sum_{\tilde{y}} L(f(\mathbf{x}), \tilde{y}) \tilde{p}(\tilde{y}|\mathbf{x})]. \label{eq_nrm}
\end{equation}
Similarly, we denote $\sum_{\tilde{y}} L(f(\mathbf{x}), \tilde{y}) \tilde{p}(\tilde{y}|\mathbf{x})$ by $\tilde{R}_L(f(\mathbf{x}))$ and $\arg \min_{f} \tilde{R}_L(f)$ by $\tilde{f}^*_L$.


\noindent \textbf{Label noise model} \quad The most generic label noise is termed instance-dependent noise, where the noise depends on both features and labels. By contrast, asymmetric noise assumes the noise depends only on labels, i.e. $\tilde{p}(\tilde{y}|\mathbf{x},y) = \tilde{p}(\tilde{y}|y)$. In addition, the most ideal label noise is called symmetric noise, where each true label is flipped into other labels with equal probability, i.e. $\tilde{p}(\tilde{y}|\mathbf{x},y)$ is a constant.

\subsection{Gradient Analysis}

The widely-used loss functions CE and MAE are defined as
\begin{gather}
	L_{\text{CE}} (f(\mathbf{x}), y) = -\log f_y(\mathbf{x}), \\
	L_{\text{MAE}} (f(\mathbf{x}), y) = 1 - f_y(\mathbf{x}).
\end{gather}
Although the recently proposed robust loss functions vary from each other in formulation, many of them can be seen as interpolations between CE and MAE. For instance, GCE~\cite{gce}, TCE~\cite{tce} and JS~\cite{js} are respectively defined as
\begin{gather}
	L_{\text{GCE}} (f(\mathbf{x}), y) = \frac{1 - f_y^q(\mathbf{x})}{q}, \\
	L_{\text{TCE}}(f(\mathbf{x}),y) = \sum_{i=1}^{t} \frac{(1 - f_y(\mathbf{x}))^i}{i}, \\
	L_{\text{JS}}(f(\mathbf{x}),y) = \frac{\pi_1 D_{\text{KL}} (\mathbf{e}^{(y)} || \mathbf{m}) + (1-\pi_1) D_{\text{KL}} (f(\mathbf{x}) || \mathbf{m})}{-(1-\pi_1) \log(1-\pi_1)},
\end{gather}
where $\mathbf{e}^{(y)}$ denotes a one-hot vector with $\mathbf{e}^{(t)}_j=1$ iff $j=t$, $\mathbf{m} = \pi_1 \mathbf{e}^{(y)} + (1-\pi_1) f(\mathbf{x})$ and $D_{KL}(\cdot || \cdot)$ denotes KL divergence. GCE equals CE when $q \rightarrow 0$ and equals MAE when $q=1$, TCE equals CE when $q \rightarrow +\infty$ and equals MAE when $t=1$, JS equals CE when $\pi_1 \rightarrow 0$ and equals MAE when $\pi_1 \rightarrow 1$. 	

To intuitively show the behavior of different loss functions on different examples, we perform gradient analyses. First, the gradients of loss functions mentioned above can be calculated as
\begin{gather}
	\frac{\partial L_{\text{CE}} (f(\mathbf{x}), y)}{\partial \boldsymbol{\theta}}  = -\frac{1}{f_y(\mathbf{x})} \nabla_{\boldsymbol{\theta}}f_y(\textbf{x}), \\
	\frac{\partial L_{\text{MAE}} (f(\mathbf{x}), y)}{\partial \boldsymbol{\theta}} = -\nabla_{\boldsymbol{\theta}}f_y(\textbf{x}), \\
	\frac{\partial L_{\text{GCE}} (f(\mathbf{x}), y)}{\partial \boldsymbol{\theta}} = -f_y^{q-1}(\mathbf{x}) \nabla_{\boldsymbol{\theta}}f_y(\textbf{x}), \\
	\frac{\partial L_{\text{TCE}} (f(\mathbf{x}), y)}{\partial \boldsymbol{\theta}} = - \sum_{i=1}^{t} (1 - f_y(\mathbf{x}))^{i-1} \nabla_{\boldsymbol{\theta}}f_y(\textbf{x}), \\
	\frac{\partial L_{\text{JS}} (f(\mathbf{x}), y)}{\partial \boldsymbol{\theta}} = - \frac{(1-\pi_1) \log (1 + \frac{\pi_1}{(1 - \pi_1) f_y(\mathbf{x})})}{(1-\pi_1) \log(1-\pi_1)} \nabla_{\boldsymbol{\theta}}f_y(\textbf{x}), \label{gradient_js}
\end{gather}
where $\boldsymbol{\theta}$ is the set of parameters of $f$. A small $f_y(\textbf{x})$ means that the model prediction on $\textbf{x}$ is not consistent with the given labels, i.e., $(\textbf{x}, y)$ is a hard example. We plot the gradient of loss w.r.t the $y$-th posterior probability (i.e. the coefficient of $\nabla_{\boldsymbol{\theta}}f_y(\textbf{x})$ on the right side of equations above) in Figure~\ref{fig_gradient}. CE puts heavy emphasis on hard examples, resulting in significant overfitting in the presence of label noise since these hard examples may be those with incorrect labels. In contrast, MAE gives equal weight to all examples, effectively avoiding overfitting but leading to severe underfitting. Robust loss functions strike a balance between CE and MAE by giving less emphasis to hard examples than CE does and more attention to them than MAE does. By properly tuning the hyperparameters, robust loss functions can achieve improved generalization.

\begin{figure}[t]
	\centering
	\includegraphics[width= \textwidth]{img/gradient.pdf}
	\caption{Gradient of loss w.r.t the $y$-th posterior probability.}
	\label{fig_gradient}
\end{figure}


\begin{algorithm}[t]
	\caption{Dynamics-Aware Loss}
	\label{al_dal}
	\KwIn{Data $\{(\mathbf{x}_i, \tilde{y}_i)\}_{i=1}^n$, $q_s$, $q_e$, $T$}
	\KwOut{Classifier $f$}
	$t_0 = \frac{1 - q_s}{q_e - q_s} T$ \\
	\For{$t=1:T$}
	{
		$q(t) = q_s + (q_e - q_s)\frac{t}{T}$ \\
		\eIf{$t<t_0$ or $t_0 > T$}{$\lambda(t)=0$}{$\lambda(t)=\frac{t-t_0}{T-t_0}$}
		Minimize $L(f)=\frac{1}{n} \sum_i (\frac{1-f_{\tilde{y}}^{q(t)}(\mathbf{x}_i)}{q(t)} + \lambda(t) \frac{- \log \max_y f_y(\mathbf{x})}{q(t)\log k}) $
	}
\end{algorithm}	


\subsection{Dynamics-Aware Loss}
Although robust loss functions outperform both CE and MAE substantially, there exists a clear discrepancy between these losses and the dynamic nature of DNNs learning with label noise. Specifically, the work of~\cite{dynamic} empirically proved that DNNs first memorize the correct labels and then memorize the wrong labels, so static trade-off between fitting ability and robustness results in inferior performance. Also, static losses are sensitive to hyperparameters, with slight changes potentially leading to serious underfitting or overfitting.

According to the dynamic nature of DNNs learning with label noise, we think it is reasonable to prioritize fitting ability at the early stage of the training process, and then increase the weight of robustness gradually. Taking GCE as an example, we gradually increase $q$ which controls the trade-off between fitting ability and robustness rather than fix it during the whole training process. For simplicity, we let $q$ increase linearly from $q_s$ to $q_e$. We call GCE with a dynamic $q$ dynamic GCE (DGCE) in the following. 

Based on the gradient analyses in Figure~\ref{fig_gradient}, smaller $q$ provides stronger fitting ability. Consequently, we set $q_s$ to a small value for a rapid performance increase at the early stage. Moreover, we think the range of $q$ should not be limited within (0,1) as the vanilla GCE does. As shown in righ-bottom subfigure of Figure~\ref{fig_gradient}, with $q>1$ GCE puts more emphasis on easy examples. In fact, GCE with $q>1$ plays a similar role to some reweighting methods~\cite{discussion2}. The main difference is that the latter explicitly assign more weights to correctly labeled examples that are typically identified by meta DNNs, which are usually trained on a clean dataset without label noise, while the former implicitly puts more emphasis on examples whose model predictions are more consistent with the provided labels. According to the widely-used small-loss trick, these examples are more likely to be correctly labeled. Therefore, DGCE with $q_e>1$ can further reduce the negative impact of label noise and guarantee a steady performance growth at the later stage. Unless otherwise specified, we always set $q_e=1.5$ and only tune $q_s$ in the following. In this way, DGCE is relatively insensitive to $q_s$ since the increasing $q$ can hinder overfitting effectively even with a small $q_s$. 

On the one hand, since some other robust loss functions such as TCE and JS can also be regarded as interpolations between CE and MAE, the dynamic rule can also provide them with performance boosts. On the other hand, only GCE can pay more attention to easy examples than hard ones by extending the range of its hyper-parameter, so DGCE outperforms other dynamic losses. We present theoretical analyses in Section~\ref{sec_theory} to verify the merit of DGCE and defer the empirical evidence to Section~\ref{sec_ablation}.

Although DGCE overcomes drawbacks of static loss functions, we observe that it suffers from underfitting at the later stage of the training process in practice. In Eq.~(\ref{eq_nrm}), $\tilde{R}_L(f)$ is in the form of $\mathbb{E}_{p(\mathbf{x})} [P+Q]$. We define $P=\tilde{p}(\tilde{y}^*|\mathbf{x}) L(f(\mathbf{x}), \tilde{y}^*)$ where $\tilde{y}^* = \arg \max_{\tilde{y}} \tilde{p}(\tilde{y}|\mathbf{x})$, $Q=\sum_{\tilde{y} \neq \tilde{y}^*}\tilde{p}(\tilde{y}|\mathbf{x}) L(f(\mathbf{x}), \tilde{y})$. The Q term may decrease even when P is fixed, which implies that $f_{\tilde{y}^*}(\mathbf{x})$ may be fixed when the loss decreases, especially if the noise rate is relatively high, i.e., $\tilde{p}(\tilde{y}^*|\mathbf{x})$ is relatively small. To solve this problem, we introduce a bootstrapping term
\begin{equation}
	L_{\text{BS}}(f(\mathbf{x})) = - \log \max_y f_y(\mathbf{x})
\end{equation}
when $q>1$. Intuitively, since $f$ has been trained on $\tilde{p}(\mathbf{x}, \tilde{y})$ with DGCE with $q<1$ during the early stage, at this point $f_{\tilde{y}^*}(\mathbf{x}) > f_{\tilde{y}}(\mathbf{x}), \forall \tilde{y} \neq \tilde{y}^*$ holds with a high probability, so $L_{\text{BS}}$ encourages an explicit increase of $f_{\tilde{y}^*}(\mathbf{x})$. To make the training process stable, we gradually increase the weight of the bootstrapping term $\lambda$.

Dynamics-aware loss (DAL) is the combination of DGCE and $L_{\text{BS}}$, we display the pseudocode of DAL in Algorithm~\ref{al_dal}. As the range of DGCE is $[0, \frac{1}{q(t)}]$ and that of $L_{\text{BS}}$ is $[0, \log k]$ (since $\max_y f_y(\mathbf{x}) > \frac{1}{k}$ always holds for a $k$-class classification problem), we divide $L_{\text{BS}}$ by $q(t)\log k$ such that its range is the same as that of DGCE. Obviously, our proposed DAL requires no extra information and incurs no additional memory burden or computational cost.

\subsection{Theoretical Analysis} \label{sec_theory}
In this section, we provide some theoretical guarantees on the performance of DAL. Since $f$ is a DNN, according to the universal approximation theorem~\cite{universal}, we suppose that $f^*$ minimizes $R_L(f(\mathbf{x}))$ for each $\mathbf{x}$.

\begin{theorem} \label{theorem:1}
	With $0<q<1, f^*_{\text{GCE}}(\mathbf{x}) = (\frac{p(1|\mathbf{x})^{\frac{1}{1-q}}}{\sum_i p(i|\mathbf{x})^{\frac{1}{1-q}}},..., \frac{p(k|\mathbf{x})^{\frac{1}{1-q}}}{\sum_i p(i|\mathbf{x})^{\frac{1}{1-q}}})$.
\end{theorem}

\begin{proof}
	According to Eq.~(\ref{eq_rm}), the optimization problem can be formulated as
	\begin{gather}
		\text{minimize} \quad \sum_y p(y|\mathbf{x}) \frac{1 - f_y^q(\mathbf{x})}{q}, \\
		\text{s.t.} \quad \sum_y f_y(\mathbf{x}) - 1 = 0.
	\end{gather}
	The Lagrangian function can be formulated as
	\begin{equation}
		L(f(\mathbf{x}), \lambda) = \sum_y p(y|\mathbf{x}) \frac{1 - f_y^q(\mathbf{x})}{q} + \lambda (\sum_y f_y(\mathbf{x}) - 1).
	\end{equation}
	Let $\frac{\partial L(f(\mathbf{x}), \lambda)}{\partial f_y(\mathbf{x})} = 0$, we can derive
	\begin{equation}
		\frac{f_y^{1-q} (\mathbf{x})}{p(y|\mathbf{x})} = \frac{1}{\lambda}.
	\end{equation}
	Combining the constrain that $\sum_y f_y(\mathbf{x}) = 1$, we can conclude the proof.
\end{proof}

\begin{theorem} \label{theorem:2}
	$\forall q>1, f^*_{\text{GCE}}(\mathbf{x}) = \mathbf{e}^{(y^*)}$, where $y^* = \arg \max_y p(y|\mathbf{x})$.
\end{theorem}

\begin{proof}
	With $q>1$, we have
	\begin{equation}
		\sum_y p(y|\mathbf{x}) \frac{1 - f_y^q(\mathbf{x})}{q} \geq \frac{1 - \sum_y p(y|\mathbf{x}) f_y(\mathbf{x})}{q} \geq \frac{1 - f_{y^*}(\mathbf{x})}{q} ,
	\end{equation}
	with equality holds iff $f(\mathbf{x}) = \mathbf{e}^{(y^*)}$. This completes the proof.
\end{proof}

\begin{corollary} \label{corollary:1}
	$\forall q>1, \lambda >0, f^*_{\text{GCE+BS}}(\mathbf{x}) = \mathbf{e}^{(y^*)}$.
\end{corollary}

\begin{proof}
	Based on Theorem 2, $f(\mathbf{x}) = \mathbf{e}^{(y^*)}$ minimizes $R_{\text{GCE}}(f(\mathbf{x}))$. Besides, any one-hot vector minimizes $L_{\text{BS}}(f(\mathbf{x}))$. Therefore, the above corollary holds.
\end{proof}

\begin{remark}
	Theorem~\ref{theorem:1} indicates that the prediction of $f^*_{\text{GCE}}$ with $q<1$ depends on $q$ while Corollary~\ref{corollary:1} indicates that the prediction of $f^*_{\text{GCE+BS}}$ with any $q>1, \lambda>0$ on any $\mathbf{x}$ is always a one-hot vector which is independent of both $q$ and $\lambda$. Therefore, under risk minimization framework, minimizing DAL with $q_e=1.5$ derives the same classifier as minimizing GCE+SB with any $q>1, \lambda>0$ finally because at the later stage of the training process, DAL with $q_e=1.5$ always has $q>1$ and $\lambda>0$. That is, the theoretical results for GCE+BS with $q>1, \lambda>0$ can directly apply to DAL with $q_e=1.5$.
\end{remark}


\begin{definition} \label{definition:1}
	For a multi-class classification classifier, we define 0-1 loss as
	\begin{equation}
		L_{0\raisebox{0mm}{-}1} (f(\mathbf{x}), y) = \mathds{1}  [\arg\max_{y'} f_{y'}(\mathbf{x}) \neq y],
	\end{equation}
	where $\mathds{1} [\cdot]$ is the indicator function. The classifier minimizing 0-1 risk is called Bayes optimal classifier and the corresponding risk is called Bayes error rate.
\end{definition}

\begin{theorem} \label{theorem:3}
	$\forall q>1, \lambda>0,$ given a classifier $\hat{\tilde{f}}$, we have
	\begin{equation}
		R_{0\raisebox{0mm}{-}1}(\hat{\tilde{f}}) - R_{0\raisebox{0mm}{-}1}(f^*_{0\raisebox{0mm}{-}1}) \leq 1 - \mathbb{E}_{p(\mathbf{x})} \mathds{1} [(y^* = \tilde{y}^*) \land (\hat{\tilde{f}}_{\tilde{y}^*} > \tilde{f}^*_{\mathrm{GCE+BS}\tilde{y}^*}(\mathbf{x}) - \frac{1}{2})], \label{eq_main}
	\end{equation}
	where $y^* = \arg \max_y p(y|\mathbf{x})$ and $\tilde{y}^* = \arg \max_{\tilde{y}} \tilde{p} (\tilde{y}|\mathbf{x})$.
\end{theorem}


\begin{proof}
	According to Corollary~\ref{corollary:1}, $\tilde{f}^*_{\text{GCE+BS}\tilde{y}^*}(\mathbf{x}) = 1$, so if $\hat{\tilde{f}}_{\tilde{y}^*}(\mathbf{x}) > \tilde{f}^*_{\text{GCE+BS}\tilde{y}^*}(\mathbf{x}) - \frac{1}{2} = \frac{1}{2}$, we have $\arg \max_{\tilde{y}} \hat{\tilde{f}}_{\tilde{y}}(\mathbf{x}) = \tilde{y}^*$. Furthermore, if $y^* = \tilde{y}^*$, we have $\arg \max_{\tilde{y}} \hat{\tilde{f}}_{\tilde{y}}(\mathbf{x}) = y^*$. Since $\forall \mathbf{x}, \arg \max_{y} f^*_{0\raisebox{0mm}{-}1y}(\mathbf{x})= y^*$, we have $R_{0\raisebox{0mm}{-}1}(\hat{\tilde{f}}(\mathbf{x})) = R_{0\raisebox{0mm}{-}1}(f^*_{0\raisebox{0mm}{-}1}(\mathbf{x}))$ if $y^* = \tilde{y}^*$ and $\hat{\tilde{f}}_{\tilde{y}^*} > \tilde{f}^*_{\text{GCE+BS}\tilde{y}^*}(\mathbf{x}) - \frac{1}{2}$, which concludes the proof.
\end{proof}

\begin{remark}
	In practice, it is impossible to derive $\tilde{f}^*_L$ from finite training data. Theorem~\ref{theorem:3} gives an upper bound of the difference between the error rate caused by the classifier $\hat{\tilde{f}}$ (rather than $\tilde{f}^*_L$) and Bayes error rate on the clean distribution. Different from previous theoretical results~\cite{mae,gce,tce}, Theorem~\ref{theorem:3} makes no assumption about the label noise model, for instance, the label noise is symmetric or asymmetric. The upper bound is determined by both the data distribution (whether $y^* = \tilde{y}^*$ holds) and the classifier (whether $\hat{\tilde{f}}_{\tilde{y}^*} > \tilde{f}^*_{\text{GCE+BS}\tilde{y}^*}(\mathbf{x}) - \frac{1}{2}$ holds, which is a sufficient condition for $\arg \max_{\tilde{y}} \hat{\tilde{f}}_{\tilde{y}} = \tilde{y}^*$ because $\tilde{f}^*_{\text{GCE+BS}\tilde{y}^*}(\mathbf{x})=1$). Note that Theorem~\ref{theorem:3} does not apply to GCE with $0<q<1$ because if $\tilde{p}(\tilde{y}^*|\mathbf{x}) < 1$, there is $\tilde{f}^*_{\text{GCE}\tilde{y}^*}(\mathbf{x}) < 1$ according to Theorem~\ref{theorem:1}. Furthermore, higher noise rate may lead to smaller $\tilde{p}(\tilde{y}^*|\mathbf{x})$, resulting in smaller $\tilde{f}^*_{\text{GCE}\tilde{y}^*}(\mathbf{x})$. Therefore, minimizing GCE+SB with $q>1$ and $\lambda > 0$ derives a classifier $\hat{\tilde{f}}$ satisfying $\arg \max_{\tilde{y}} \hat{\tilde{f}}_{\tilde{y}} = \tilde{y}^*$ with a higher probability than minimizing GCE with $0<q<1$ in practice.
\end{remark}


\section{Experiement} \label{sec_experiment}

\subsection{Setup}

\textbf{Datasets} \quad We perform experiments on CIFAR-10 and CIFAR-100 with synthetic label noise and two real-world noisy datasets Animal-10N~\cite{animal10n} and Webvision~\cite{webvision}. For CIFAR, we use three types of synthetic label noise, including
\begin{itemize}
	\item symmetric noise. The labels are resampled from a uniform distribution over all labels with probability $\eta$, where $\eta$ is the noise rate.
	
	\item asymmetric noise. For CIFAR-10, the labels are changed as follows with probability $\eta$: truck $\rightarrow$ automobile, bird $\rightarrow$ airplane, cat $\leftrightarrow$ dog and deer $\rightarrow$ horse; for CIFAR-100, the labels are cycled to the next sub-class of the same super-class, e.g. the labels of super-class ``vehicle1" are modified as follows: bicycle $\rightarrow$ bus $\rightarrow$ motorcycle $\rightarrow$ pickup truck $\rightarrow$ train $\rightarrow$ bicycle.
	
	\item instance(-dependent) noise following Algorithm 2 in~\cite{instance}.
	
\end{itemize}
For Webvision, we follow the ``Mini" setting in~\cite{apl} which takes only the first 50 classes of the Google resized images as the training dataset. Then we evaluate the classification performance on the same 50 classes of Webvision validation set.


\begin{table*}[t]
	\caption{Optimal hyper-parameters of robust loss functions under our setup.}
	\label{tab_hyperparam}
	\setlength\tabcolsep{5pt}
	\centering
	\renewcommand{\arraystretch}{1.0}
	\scalebox{0.8}{
		\begin{tabular}{lccccc}
			\toprule
			Method & Hyper-parameter & CIFAR-10 & CIFAR-100 & Animal-10N & WebVision \\
			\midrule
			GCE~\cite{gce} & ($q$) & (0.9) & (0.7) & - & - \\
			SCE~\cite{sce} & ($\alpha$, $\beta$) & (0.1, 10.0) & (6.0, 1.0) & (6.0, 1.0) & (10.0, 1.0) \\
			NLNL~\cite{nlnl} & ($N$) & (1) & (110) & - & - \\
			BTL~\cite{btl} & ($t_1$, $t_2$) & (0.7, 3.0) & (0.7, 1.5) & - & - \\
			NCE+RCE~\cite{apl} & ($\alpha$, $\beta$) & (1.0, 0.1) & (10.0, 0.1) & (10.0, 0.1) & (50.0, 0.1) \\
			TCE~\cite{tce} & ($t$) & (3) & (18) & - & - \\
			NCE+AGCE~\cite{agce} & ($\alpha$, $\beta$, $a$, $q$) & (1.0, 0.4, 6, 1.5) & (10.0, 0.1, 3, 3) & - & -\\
			AGCE~\cite{agce} & ($a$, $q$) & - & - & (1e-5, 0.7) & (1e-5, 0.5) \\
			CE+SR~\cite{sr} & ($\tau$, $\lambda_0$, $r$, $p$, $\rho$) & (0.5, 1.5, 1, 0.1, 1.02) & (0.5, 8.0, 1, 0.01, 1.02) & (0.5, 8.0, 1, 0.01, 1.02) & (0.5, 2.0, 1, 0.01, 1.02) \\
			JS~\cite{js} & ($\pi$) & (0.9) & (0.5) & (0.5) & (0.1) \\
			Poly-1 & ($\epsilon$) & (10) & (2) & - & - \\
			\midrule
			DAL & ($q_s$, $q_e$) & (0.8, 1.5) & (0.6, 1.5) & (0.6, 1.5) & (0.4, 1.5) \\
			\bottomrule
		\end{tabular}
	}
\end{table*}	

\textbf{Baselines} \quad We first compare DAL with several robust loss functions. These baselines include Generalized Cross Entropy (GCE)~\cite{gce}, Negative Learning for Noisy Labels (NLNL)~\cite{nlnl}, Symmetric Cross Entropy (SCE)~\cite{sce}, Bi-Tempered Logistic Loss (BTL)~\cite{btl}, Normalized Cross Entropy with Reverse Cross Entropy (NCE+RCE)~\cite{apl}, Taylor Cross Entropy (TCE)~\cite{tce}, Normalized Cross Entropy with Asymmetric Generalized Cross Entropy (NCE+AGCE)~\cite{agce}, Cross Entropy with Sparse Regularization (CE+SR)~\cite{sr}, Jensen-Shannon Divergence Loss (JS)~\cite{js} and Poly-1~\cite{poly}. Similar to DAL, they all require no additional information and incur no extra memory burden or computational costs.

In addition, for those more complicated approaches which cannot be compared with DAL directly, we verify that DAL can provide them with performance boosts, we integrate DAL with the following methods: early learning regularization (ELR)~\cite{elr}, generalized Jensen-Shannon Divergence (GJS)~\cite{js}, Co-teaching~\cite{coteaching}, JoCoR~\cite{jocor} and DivideMix~\cite{dividemix}.



\subsection{Comparison with Robust Losses}

\begin{table}[t] %\tiny
	\caption{Test accuracies (\%) on CIFAR with label noise. The results (mean±std) are reported over 3 random run and the best results are boldfaced while the second best results are underlined.}
	\vskip 0.05in
	\label{tab_cifar}
	\setlength\tabcolsep{8.0pt}
	\centering
	\renewcommand{\arraystretch}{1.0}
	\scalebox{0.78}{
		\begin{tabular}{llcccccccc}
			\toprule
			\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{Symmetric} & \multicolumn{2}{c}{Asymmetric} & \multicolumn{2}{c}{Instance} \\
			\cmidrule(lr){3-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
			& & 20\% & 40\% & 60\% & 80\% & 20\% & 40\% & 20\% & 40\% \\
			\midrule
			\multirow{12}{*}{CIFAR-10} & CE & 83.30\scriptsize±0.19 & 67.85\scriptsize±0.53 & 47.79\scriptsize±0.42 & 25.80\scriptsize±0.20 & 86.00\scriptsize±0.15 & 74.98\scriptsize±0.09 & 80.86\scriptsize±0.11 & 61.40\scriptsize±0.28 \\
			& GCE~\cite{gce} & 90.68\scriptsize±0.08 & 87.33\scriptsize±0.15 & 81.29\scriptsize±0.28 & 61.93\scriptsize±0.24 & 88.96\scriptsize±0.15 & 58.79\scriptsize±0.14 & \underline{89.40\scriptsize±0.17} & 78.60\scriptsize±3.67 \\
			& SCE~\cite{sce} & 89.16\scriptsize±0.32 & 85.41\scriptsize±0.22 & 77.81\scriptsize±0.53 & 51.46\scriptsize±1.92 & 87.71\scriptsize±0.41 & \underline{77.40\scriptsize±0.13} & 88.11\scriptsize±0.17 & 77.80\scriptsize±0.74 \\
			& NLNL~\cite{nlnl} & 82.18\scriptsize±0.44 & 73.25\scriptsize±0.23 & 60.79\scriptsize±0.96 & 43.46\scriptsize±0.16 & 85.23\scriptsize±0.22 & \textbf{81.91\scriptsize±0.22} & 81.77\scriptsize±0.42 & 69.27\scriptsize±0.30 \\
			& BTL~\cite{btl} & 89.93\scriptsize±0.30 & 78.22\scriptsize±0.24 & 58.00\scriptsize±0.20 & 29.01\scriptsize±0.50 & 86.51\scriptsize±0.14 & 74.58\scriptsize±0.43 & 86.58\scriptsize±0.41 & 64.72\scriptsize±0.74 \\
			& NCE+RCE~\cite{apl} & 90.70\scriptsize±0.03 & 87.02\scriptsize±0.11 & 81.16\scriptsize±0.16 & 64.62\scriptsize±0.18 & 88.51\scriptsize±0.13 & 76.42\scriptsize±0.23 & 88.92\scriptsize±0.13 & 77.80\scriptsize±0.50 \\
			& TCE~\cite{tce} & 90.50\scriptsize±0.11 & 86.30\scriptsize±0.24 & 77.42\scriptsize±0.13 & 46.91\scriptsize±0.08 & 87.87\scriptsize±0.21 & 58.07\scriptsize±0.28 & 88.88\scriptsize±0.18 & 73.15\scriptsize±0.26 \\
			& NCE+AGCE~\cite{agce} & 90.60\scriptsize±0.13 & 87.28\scriptsize±0.32 & 81.34\scriptsize±0.04 & 66.50\scriptsize±0.73 & \underline{88.99\scriptsize±0.08} & 76.65\scriptsize±0.45 & 89.10\scriptsize±0.13 & \underline{78.77\scriptsize±0.18} \\
			& CE+SR~\cite{sr} & \textbf{91.39\scriptsize±0.14} & \underline{87.82\scriptsize±0.30} & \underline{82.71\scriptsize±0.34} & \underline{69.70\scriptsize±0.29} & 88.26\scriptsize±0.24 & 74.95\scriptsize±0.10 & 88.28\scriptsize±0.49 & 68.86\scriptsize±0.63 \\
			& JS~\cite{js} & 90.62\scriptsize±0.06 & 86.28\scriptsize±0.28 & 77.04\scriptsize±0.54 & 43.04\scriptsize±0.34 & 88.30\scriptsize±0.16 & 68.73\scriptsize±0.50 & 88.27\scriptsize±0.56 & 73.92\scriptsize±0.33 \\
			& Poly-1~\cite{poly} & 84.89\scriptsize±0.08 & 68.25\scriptsize±0.67 & 47.87\scriptsize±0.56 & 25.14\scriptsize±0.32 & 86.19\scriptsize±0.20 & 75.52\scriptsize±0.42 & 81.37\scriptsize±0.23 & 61.50\scriptsize±0.60 \\
			& DAL & \underline{90.95\scriptsize±0.09} & \textbf{88.73\scriptsize±0.10} & \textbf{84.28\scriptsize±0.16} & \textbf{72.51\scriptsize±0.20} & \textbf{90.08\scriptsize±0.10} & 73.34\scriptsize±0.18 & \textbf{90.33\scriptsize±0.08} & \textbf{85.04\scriptsize±0.08} \\
			\midrule
			\multirow{12}{*}{CIFAR-100} & CE & 60.41\scriptsize±0.19 & 43.78\scriptsize±0.44 & 25.03\scriptsize±0.13 & 8.45\scriptsize±0.32 & 60.89\scriptsize±0.25 & 43.74\scriptsize±0.51 & 60.73\scriptsize±0.36 & 45.31\scriptsize±0.36 \\
			& GCE~\cite{gce} & 68.37\scriptsize±0.27 & 61.94\scriptsize±0.44 & 49.91\scriptsize±0.25 & 22.22\scriptsize±0.51 & 62.21\scriptsize±0.32 & 41.19\scriptsize±0.89 & \underline{67.22\scriptsize±0.28} & 54.37\scriptsize±0.29 \\
			& SCE~\cite{sce} & 58.66\scriptsize±0.55 & 42.98\scriptsize±0.26 & 24.86\scriptsize±0.32 & 8.56\scriptsize±0.16 & 59.70\scriptsize±0.32 & 43.06\scriptsize±0.16 & 59.36\scriptsize±0.27 & 44.19\scriptsize±0.16 \\
			& NLNL~\cite{nlnl} & 57.89\scriptsize±0.66 & 44.05\scriptsize±0.47 & 26.14\scriptsize±0.22 & 11.80\scriptsize±0.33 & 57.33\scriptsize±0.20 & 38.82\scriptsize±0.14 & 57.23\scriptsize±0.55 & 42.96\scriptsize±0.46 \\
			& BTL~\cite{btl} & 61.83\scriptsize±0.13 & 47.54\scriptsize±0.16 & 30.47\scriptsize±0.54 & 13.73\scriptsize±0.38 & 58.70\scriptsize±0.14 & 42.91\scriptsize±0.32 & 59.31\scriptsize±0.40 & 44.18\scriptsize±0.44 \\
			& NCE+RCE~\cite{apl} & 67.47\scriptsize±0.14 & 61.27\scriptsize±0.16 & \underline{51.02\scriptsize±0.12} & \underline{25.60\scriptsize±0.28} & 63.17\scriptsize±0.26 & 43.47\scriptsize±0.43 & 65.83\scriptsize±0.30 & 54.21\scriptsize±0.48 \\
			& TCE~\cite{tce} & 63.97\scriptsize±0.65 & 57.40\scriptsize±0.63 & 41.46\scriptsize±0.67 & 15.12\scriptsize±0.27 & 54.97\scriptsize±0.49 & 39.73\scriptsize±0.19 & 59.42\scriptsize±0.40 & 36.48\scriptsize±1.38 \\
			& NCE+AGCE~\cite{agce} & 67.06\scriptsize±0.22 & 60.93\scriptsize±0.17 & 49.09\scriptsize±0.35 & 20.10\scriptsize±0.61 & \underline{64.87\scriptsize±0.29} & \textbf{46.87\scriptsize±0.28} & 66.38\scriptsize±0.46 & \underline{56.35\scriptsize±0.10} \\
			& CE+SR~\cite{sr} & \underline{68.84\scriptsize±0.23} & \underline{62.03\scriptsize±0.39} & 50.28\scriptsize±0.25 & 10.75\scriptsize±0.20 & 59.16\scriptsize±0.62 & 41.80\scriptsize±0.23 & 63.19\scriptsize±0.06 & 47.45\scriptsize±0.51 \\
			& JS~\cite{js} & 67.58\scriptsize±0.52 & 61.01\scriptsize±0.31 & 47.95\scriptsize±0.38 & 20.03\scriptsize±0.27 & 59.67\scriptsize±0.89 & 41.23\scriptsize±0.50 & 65.31\scriptsize±0.41 & 49.12\scriptsize±1.11 \\
			& Poly-1~\cite{poly} & 60.13\scriptsize±0.16 & 44.20\scriptsize±0.65 & 25.84\scriptsize±0.15 & 8.44\scriptsize±0.27 & 60.81\scriptsize±0.22 & 43.63\scriptsize±0.60 & 60.76\scriptsize±0.45 & 45.65\scriptsize±0.03 \\
			& DAL & \textbf{69.00\scriptsize±0.09} & \textbf{64.80\scriptsize±0.17} & \textbf{56.72\scriptsize±0.27} & \textbf{34.33\scriptsize±0.61} & \textbf{66.60\scriptsize±0.04} & \underline{45.36\scriptsize±0.42} & \textbf{68.27\scriptsize±0.20} & \textbf{60.38\scriptsize±0.54} \\
			\bottomrule
		\end{tabular}
	}
\end{table}



\textbf{Experimental details} \quad When comparing DAL with robust loss functions, we use a single shared learning setup for different methods for fair comparison. For CIFAR and Animal-10N, we train a ResNet18~\cite{resnet} (the first 7×7 Conv of stride 2 is replaced with 3×3 Conv of stride 1, and the first max pooling layer is also removed for CIFAR.) using SGD for 150 epochs with momentum 0.9, weight decay $10^{-4}$, batch size 128, initial learning rate 0.01, and cosine learning rate annealing. We also apply typical data augmentations including random crop and horizontal flip. For Webvision, we train a ResNet50~\cite{resnet} using SGD for 250 epochs with nesterov momentum 0.9, weight decay $3 \times 10^{-5}$, batch size 512, and initial learning rate 0.4. The learning rate is multiplied by 0.97 after each epoch. Typical data augmentations including random crop, color jittering, and horizontal flip are applied to both Webvision. When integrating DAL with other approaches, for each approach we use the training setup reported in official codes except that we set the number of epochs to 200 for DivideMix and GJS for convenience. Besides, we use a single set of hyper-parameters $(\pi_1, \pi_2, \pi_3)=(0.3, 0.35, 0.35)$ for GJS rather than tune hyper-parameters elaborately for different label noise. 




\begin{table}[t] \small
	\caption{Test accuracies (\%) on real-word noisy datasets Animal-10N and WebVision. The best results are boldfaced while the second best results are underlined.}
	\vskip 0.05in
	\centering
	\renewcommand{\arraystretch}{1.0}
	\setlength\tabcolsep{8.0pt}
	\label{tab_real}
	\scalebox{1.1}{
		\begin{tabular}{cccccccc}
			\toprule
			Dataset & CE & SCE~\cite{sce} & NCE+RCE~\cite{apl} & AGCE~\cite{agce} & CE+SR~\cite{sr} & JS~\cite{js} & DAL \\
			\midrule
			Animal-10N & 80.60 & 81.62 & 80.92 & 81.40 & \underline{81.82} & 81.14 & \textbf{82.66} \\
			WebVision & 64.04 & 65.60 & 64.64 & \underline{68.88} & 68.72 & 64.96 & \textbf{70.04} \\
			\bottomrule
		\end{tabular}
	}
\end{table}   

We randomly select 10\% examples from the noisy training set as the validation set for hyper-parameter tuning, then use the best hyper-parameters to train DNNs on the full training set. The optimal hyper-parameters of robust loss functions in our setup are summarized in Table~\ref{tab_hyperparam}. The experimental results on CIFAR-10 and CIFAR-100 are respectively shown in Table~\ref{tab_cifar}. On CIFAR-10, DAL achieves the best performance in 6 settings and the second best performance in 1 setting. On CIFAR-100, DAL reaches the highest accuracy in 7 settings and the second highest accuracy in 1 setting. With the most complex instance-dependent noise, the superiority of DAL is more significant, which outperforms the second best method NCE+AGCE by more than 6\% and 4\% on CIFAR-10 and CIFAR-100 with 40\% instance-dependent noise respectively. The comparison on real-world noisy datasets is shown in Table~\ref{tab_real}, where DAL also outperforms other losses by a clear margin. In summary, DAL demonstrates a significant and consistent superiority under both synthetic and realistic label noise.


\begin{figure}[t]
	\centering
	\includegraphics[width= 0.95\textwidth]{img/process.pdf}
	\caption{Training process on CIFAR-100 with 40\% instance-dependent noise.}
	\label{fig_process}
\end{figure}

\begin{table}[t] %\tiny
	\caption{Ablation study results in terms of test accuracy (\%) on CIFAR-100. The results (mean±std) are reported over 3 random run and the best results are boldfaced.}
	\vskip 0.05in
	\label{tab_ablation}
	\setlength\tabcolsep{8.0pt}
	\centering
	\renewcommand{\arraystretch}{1.0}
	\scalebox{0.9}{
		\begin{tabular}{lcccccccc}
			\toprule
			\multirow{2}{*}{Method} & \multicolumn{4}{c}{Symmetric} & \multicolumn{2}{c}{Asymmetric} & \multicolumn{2}{c}{Instance} \\
			\cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			& 20\% & 40\% & 60\% & 80\% & 20\% & 40\% & 20\% & 40\% \\
			\midrule
			GCE & 68.37\scriptsize±0.27 & 61.94\scriptsize±0.44 & 49.91\scriptsize±0.25 & 22.22\scriptsize±0.51 & 62.21\scriptsize±0.32 & 41.19\scriptsize±0.89 & 67.22\scriptsize±0.28 & 54.37\scriptsize±0.29 \\
			DGCE ($q_e$=1.0) & 68.95\scriptsize±0.27 & 63.44\scriptsize±0.21 & 51.86\scriptsize±0.70 & 26.05\scriptsize±0.18 & 63.92\scriptsize±0.28 & 41.78\scriptsize±0.09 & 67.86\scriptsize±0.24 & 55.54\scriptsize±0.15 \\
			DGCE ($q_e$=1.5) & 68.89\scriptsize±0.26 & 63.71\scriptsize±0.19 & 53.97\scriptsize±0.44 & 30.90\scriptsize±0.44 & 65.80\scriptsize±0.15 & 44.87\scriptsize±0.14 & 68.01\scriptsize±0.13 & 58.92\scriptsize±0.37 \\
			DAL & \textbf{69.00\scriptsize±0.09} & \textbf{64.80\scriptsize±0.17} & \textbf{56.72\scriptsize±0.27} & \textbf{34.33\scriptsize±0.61} & \textbf{66.60\scriptsize±0.04} & \textbf{45.36\scriptsize±0.42} & \textbf{68.27\scriptsize±0.20} & \textbf{60.38\scriptsize±0.54} \\
			\bottomrule
		\end{tabular}
	}
\end{table}


\begin{table}[t] %\tiny
	\caption{Comparison of dynamic and static robust loss functions in terms of test accuracy (\%) on CIFAR-100. The results (mean±std) are reported over 3 random run and the best results are boldfaced.}
	\vskip 0.05in
	\label{tab_dynamic}
	\setlength\tabcolsep{8.0pt}
	\centering
	\renewcommand{\arraystretch}{1.0}
	\scalebox{0.95}{
		\begin{tabular}{lcccccccc}
			\toprule
			\multirow{2}{*}{Method} & \multicolumn{4}{c}{Symmetric} & \multicolumn{2}{c}{Asymmetric} & \multicolumn{2}{c}{Instance} \\
			\cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			& 20\% & 40\% & 60\% & 80\% & 20\% & 40\% & 20\% & 40\% \\
			\midrule
			TCE~\cite{tce} & 63.97\scriptsize±0.65 & 57.40\scriptsize±0.63 & 41.46\scriptsize±0.67 & 15.12\scriptsize±0.27 & 54.97\scriptsize±0.49 & 39.73\scriptsize±0.19 & 59.42\scriptsize±0.40 & 36.48\scriptsize±1.38 \\
			DTCE &  \textbf{64.41\scriptsize±0.97} & \textbf{60.38\scriptsize±0.52} & \textbf{47.31\scriptsize±0.25} & \textbf{20.05\scriptsize±0.19} & \textbf{56.00\scriptsize±2.24} & \textbf{40.96\scriptsize±0.54} & \textbf{61.55\scriptsize±1.77} & \textbf{41.68\scriptsize±0.71} \\
			\midrule
			JS~\cite{js} & 67.58\scriptsize±0.52 & 61.01\scriptsize±0.31 & 47.95\scriptsize±0.38 & 20.03\scriptsize±0.27 & 59.67\scriptsize±0.89 & 41.23\scriptsize±0.50 & 65.31\scriptsize±0.41 & 49.12\scriptsize±1.11 \\
			DJS & \textbf{68.28\scriptsize±0.76} &  \textbf{62.82\scriptsize±0.15} & \textbf{51.08\scriptsize±0.71} & \textbf{24.03\scriptsize±0.15} & \textbf{61.49\scriptsize±0.22} & \textbf{41.64\scriptsize±0.21} & \textbf{66.70\scriptsize±0.26} & \textbf{51.70\scriptsize±0.39} \\
			\bottomrule
		\end{tabular}
	}
\end{table}

\begin{table}[t] %\tiny
	\caption{Comparison on different architectures in terms of test accuracy (\%) with 30\% symmetric noise. The results (mean±std) are reported over 3 random run.}
	\vskip 0.05in
	\label{tab_network}
	\setlength\tabcolsep{10.0pt}
	\centering
	\renewcommand{\arraystretch}{1.0}
	\scalebox{0.9}{
		\begin{tabular}{lcccccc}
			\toprule
			\multirow{2}{*}{Dataset} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Classifier} & clean & \multicolumn{3}{c}{noisy} \\
			\cmidrule(lr){4-4} \cmidrule(lr){5-7}
			& & & CE & CE & GCE & DAL \\
			\midrule
			\multirow{4}{*}{CIFAR-10} & ResNet18 & Softmax & 93.43\scriptsize±0.07 & 76.37\scriptsize±0.30 & 89.44\scriptsize±0.03 & 89.65\scriptsize±0.24 \\
			& ResNet18 & Prototype & 92.40\scriptsize±0.35 & 71.48\scriptsize±0.29 & 88.54\scriptsize±0.12 & 90.26\scriptsize±0.30 \\
			& ResNet34 & Softmax & 93.90\scriptsize±0.08 & 77.63\scriptsize±0.08 & 89.35\scriptsize±0.07 & 89.81\scriptsize±0.23 \\
			& ResNet34 & Protptype & 92.11\scriptsize±0.23 & 73.31\scriptsize±0.53 & 88.89\scriptsize±0.26 & 89.99\scriptsize±0.17 \\
			\midrule
			\multirow{4}{*}{CIFAR-100} & ResNet18 & Softmax & 73.71\scriptsize±0.09 & 53.27\scriptsize±0.43 & 65.25\scriptsize±0.50 & 66.64\scriptsize±0.31 \\
			& ResNet18 & Protptype & 72.99\scriptsize±0.63 & 50.69\scriptsize±0.19 & 58.31\scriptsize±0.58 & 67.29\scriptsize±0.17 \\
			& ResNet34 & Softmax & 74.02\scriptsize±0.18 & 54.32\scriptsize±0.41 & 65.23\scriptsize±0.37 & 66.97\scriptsize±0.25 \\
			& ResNet34 & Protptype & 70.12\scriptsize±0.63 & 48.33\scriptsize±2.57 & 53.80\scriptsize±0.41 & 66.46\scriptsize±0.30 \\
			\bottomrule
		\end{tabular}
	}
\end{table}

\subsection{Further Understanding of DAL} \label{sec_ablation}
We first delve deep into the training process with different losses. In Figure~\ref{fig_process}, besides test accuracy, we also display the training accuracy on both correctly labeled examples and wrongly labeled examples. The latter is calculated based on the provided labels rather than the ground truth. CE can achieve 100\% training accuracy on both correctly and wrongly labeled data, indicating that it overfits label noise. In contrast, the training accuracy of MAE is quite low ($<$10\%) on both correctly and wrongly labeled data, which means that it suffers from serious underfitting, so its test accuracy is substantially lower than CE. GCE can achieve high training accuracy on correctly labeled data while the figure on wrongly labeled data is relatively low, so it outperforms both CE and MAE on test data. Besides, we also observe that the training accuracy on correctly labeled data increases fast at the outset while that on wrongly labeled data starts to rise after several epochs, which is consistent with the dynamic nature of DNNs learning with label noise. DAL yields comparable training accuracy to CE on correctly labeled data and that to MAE on wrongly labeled data, so it achieves the best generalization.

In addition, we also study the effect of removing different components to provide insights into what makes DAL achieve the state-of-the-art performance. The experimental results are shown in Table~\ref{tab_ablation}. First, DGCE with $q_e$=1.0 consistently outperforms vanilla GCE, indicating that the dynamic rule is beneficial to robust loss functions. Second, DGCE with $q_e$=1.5 also yields better performance than DGCE with $q_e$=1.0 in most cases, because $q_e>1$ can further reduce the negative effect of label noise. Finally, DAL always achieve the highest accuracy, since the bootstrapping term helps to combat underfitting.

Besides GCE, TCE and JS can also be seen as interpolations between CE and MAE. On the one hand, the dynamic rule can also provide them with performance boosts as shown in Table~\ref{tab_dynamic}. On the other hand, unlike GCE which can put more emphasis on easy examples than hard ones by extending the range of its hyper-parameter, JS and TCE always pay more attention to hard examples, so DTCE and DJS are inferior to DGCE and DAL.

Finally, we evaluate the effect of DAL on DNNs with different architectures. Varying both the backbone (ResNet18 or ResNet34) and the classifier (softmax or prototype~\cite{cpn}), we summarize the performance with 30\% symmetric noise in Table~\ref{tab_network}. The performance of CE without label noise is also provided for comparison. Obviously, the performance of DAL is not seriously influenced by both the backbone and the classifier.


\begin{figure}[!htbp]
	\begin{minipage}[b]{\textwidth}
		\centerline{\includegraphics[width=0.95\textwidth]{img/q_s.pdf}}
		\vskip -0.1in
		\caption{Influence of $q_s$ in terms of test accuracy (\%) on CIFAR-100.}
		\label{fig_q_s}
	\end{minipage}
	\vskip 0.2in
	\begin{minipage}[b]{\textwidth}
		\centerline{\includegraphics[width=0.95\textwidth]{img/q_e.pdf}}
		\vskip -0.1in
		\caption{Influence of $q_e$ in terms of test accuracy (\%) on CIFAR-100.}
		\label{fig_q_e}
	\end{minipage}
\end{figure}

\subsection{Influence of Hyper-parameters}
We perform sensitivity analysis for DAL and report the results in Figure~\ref{fig_q_s}, we also provide the results of GCE for comparison. Obviously, DAL is far more insensitive to $q_s$ compared with GCE. In most cases, varying $q_s$ in [0.55, 0.65] only leads to less than 2\% performance fluctuation. Besides, DAL with inferior $q_s$ always outperforms GCE with optimal $q$ in Figure~\ref{fig_q_s}. 

As stated above, for convenience we always set $q_e$ to 1.5 and only tune $q_s$. In fact, setting $q_e$ to other value leads to similar performance. Fixing $q_s$=0.6, we show the performance of DAL with different $q_e$ in Figure~\ref{fig_q_e}. Even if we do not tune $q_s$, increasing or decreasing $q_e$ substantially (by 0.3) derives at most only 2\% performance fluctuation.




	\begin{table}[t] %\tiny
	\caption{Performance before and after intergrating DAL on CIFAR-100.}
	\vskip 0.05in
	\label{tab_integrate}
	\setlength\tabcolsep{6.0pt}
	\centering
	\renewcommand{\arraystretch}{0.95}
	\scalebox{0.75}{
		\begin{tabular}{lcccccccc}
			\toprule
			\multirow{2}{*}{Method} & \multicolumn{4}{c}{Symmetric} & \multicolumn{2}{c}{Asymmetric} & \multicolumn{2}{c}{Instance} \\
			\cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
			& 20\% & 40\% & 60\% & 80\% & 20\% & 40\% & 20\% & 40\% \\
			\midrule
			ELR~\cite{elr} & 72.64 & 69.53 & 63.33 & 28.55 & 74.01 & 68.12 & 73.62 & 71.86 \\
			+DAL & 73.64\textcolor{red}{(+1.00)} & 70.94\textcolor{red}{(+1.41)} & 65.91\textcolor{red}{(+2.58)} & 34.16\textcolor{red}{(+5.61)} & 73.83\textcolor{olive}{(-0.18)} & 67.22\textcolor{olive}{(-0.90)} & 73.95\textcolor{red}{(+0.33)} & 71.56\textcolor{olive}{(-0.30)} \\
			\midrule
			GJS~\cite{js} & 75.91 & 72.40 & 62.83 & 33.16 & 74.01 & 56.33 & 75.53 & 68.00 \\
			+DAL & 76.03\textcolor{red}{(+0.12)} & 72.88\textcolor{red}{(+0.44)} & 64.70\textcolor{red}{(+1.87)} & 38.98\textcolor{red}{(+5.82)} & 75.42\textcolor{red}{(+1.41)} & 61.73\textcolor{red}{(+5.40)} & 76.17\textcolor{red}{(+0.64)} & 71.47\textcolor{red}{(+3.47)} \\
			\midrule
			Co-teaching~\cite{coteaching} & 66.07 & 59.83 & 48.47 & 20.21 & 64.01 & 46.67 & 65.24 & 53.11 \\
			+DAL & 67.34\textcolor{red}{(+1.27)} & 60.15\textcolor{red}{(+0.32)} & 51.04\textcolor{red}{(+2.57)} & 26.16\textcolor{red}{(+5.95)} & 63.81\textcolor{olive}{(-0.20)} & 47.36\textcolor{red}{(+0.69)} & 64.79\textcolor{olive}{(-0.45)} & 54.73\textcolor{red}{(+1.62)} \\
			\midrule
			JoCoR~\cite{jocor} & 63.29 & 59.50 & 51.60 & 27.94 & 56.91 & 40.60 & 61.25 & 51.58 \\
			+DAL & 64.70\textcolor{red}{(+1.41)} & 60.36\textcolor{red}{(+0.86)} & 53.07\textcolor{red}{(+1.47)} & 31.86\textcolor{red}{(+3.92)} & 59.12\textcolor{red}{(+2.21)} & 41.90\textcolor{red}{(+1.30)} & 62.05\textcolor{red}{(+0.80)} & 51.81\textcolor{red}{(+0.23)} \\
			\midrule
			DivideMix~\cite{dividemix} & 76.23 & 74.11 & 69.23 & 52.08 & 75.18 & 52.86 & 76.10 & 68.51 \\
			+DAL & 77.07\textcolor{red}{(+0.84)} & 74.68\textcolor{red}{(+0.57)} & 70.38\textcolor{red}{(+1.15)} & 54.91\textcolor{red}{(+2.83)} & 75.42\textcolor{red}{(+0.24)} & 54.60\textcolor{red}{(+1.74)} & 75.97\textcolor{olive}{(-0.18)} & 68.93\textcolor{red}{(+0.42)} \\
			\bottomrule
		\end{tabular}
	}
\end{table}

\subsection{Integrating DAL with Other Approaches}
Robust losses have been an independent research line of handling label noise previously~\cite{apl, agce, sr}. In this section, we integrate our proposed DAL with other more sophisticated approaches. ELR~\cite{elr} is a combination of CE and early learning regularizer, the latter must maintain a label distribution for each sample, which increases memory burden substantially when the class space is large. GJS~\cite{js} is a combination of JS and consistency regularizer, the latter requires strong data augmentation and doubles the back-and-forth computational cost. We replace CE in ELR and JS in GJS with DAL. Co-teaching~\cite{coteaching} and JoCoR~\cite{jocor} are both sample selection methods. They maintain two DNNs with the same architecture simultaneously who select potentially correct samples for each other. We replace CE in both methods with DAL.  DivideMix~\cite{dividemix} is a quite sophisticated method which employs semi-supervised learning. It has a warmup process at the outset, we replace CE with DAL during this period, then use examples whose predictions agree with their labels to train DNNs with CE for another several epochs. We set $(q_s, q_e)$ to (0.0, 1.0) for all approaches. Since DAL is combined with more powerful techniques such as sample selection and label correction, the labels gradually become far more accurate than the original ones, in this case $q_e > 1$ degenerates performance due to underfitting. It is clear from Table~\ref{tab_integrate} that DAL can provide different methods with performance boosts consistently even if we do not tune $(q_s,q_e)$ elaborately. Especially with higher noise rate, the performance boosts are more remarkable. For instance, under 80\% symmetric noise, DAL improves Co-teaching by about 6\% and even provides the sophisticated DivideMix with about 3\% performance gain without requiring any extra information or resources.

\subsection{Robustness against Backdoor Attacks}

\begin{table}[t]
	\caption{Backdoor Attack on DNNs trained with different loss functions. The results (mean±std) are reported over 3 random run.}
	\vskip 0.05in
	\label{tab_backdoor}
	\setlength\tabcolsep{4.0pt}
	\centering
	\renewcommand{\arraystretch}{1}
	\scalebox{0.9}{
		\begin{tabular}{llcccc}
			\toprule
			\multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Local Trigger} & \multicolumn{2}{c}{Global Trigger} \\
			\cmidrule(lr){3-4} \cmidrule(lr){5-6}
			& & Test Accuracy & Success Rate & Test Accuracy & Success Rate \\
			\midrule
			\multirow{2}{*}{CIFAR-10} & CE & 93.39\scriptsize±0.13 & 99.91\scriptsize±0.03 & 93.46\scriptsize±0.10 & 68.16\scriptsize±2.63 \\
			& DAL & 92.42\scriptsize±0.22 & 0.23\scriptsize±0.05 & 92.33\scriptsize±0.16 & 29.50\scriptsize±1.84 \\
			\midrule
			\multirow{2}{*}{CIFAR-100} & CE & 73.95\scriptsize±0.16 & 96.07\scriptsize±0.81 & 73.99\scriptsize±0.07 & 49.36\scriptsize±2.09 \\
			& DAL & 72.06\scriptsize±0.26 & 0.40\scriptsize±0.35 & 72.18\scriptsize±0.10 & 16.95\scriptsize±7.75 \\
			\bottomrule
		\end{tabular}
	}
\end{table}
Besides label noise, we observe that DAL also helps to improve robustness against backdoor attacks. Typical backdoor attacks~\cite{backdoor1} inject triggers into a small part of training examples and change their labels into a specific target class, such that the target model performs well on benign samples whereas consistently classifies any input containing the backdoor trigger into the target class. Following~\cite{trigger}, we use two types of backdoor triggers as shown in Figure~\ref{fig_trigger}, which are added into 100 randomly selected training examples whose labels are converted into the target class. We first feed clean test samples into the target model to calculate the test accuracy, then remove test samples which are classified into the target class and add triggers into all remained test images to compute the backdoor success rate.


\begin{wrapfigure}[8]{r}{8cm}
	\vskip -0.2in
	\includegraphics[width=8cm]{img/trigger.pdf}
	\vskip -0.05in
	\caption{Illustration of backdoor triggers.}
	\label{fig_trigger}
\end{wrapfigure}


As shown in Table~\ref{tab_backdoor}, test accuracies of DNNs trained with DAL are comparable that trained with CE. Moreover, DAL exhibits stronger robustness against backdoor attacks than CE. Local triggers yield almost 100\% success rate on DNNs trained with CE while 0\% success rate on that trained with DAL. Based on the dynamics of DNNs, they firstly learn patterns shared by most training examples and eventually memorize the correlation between backdoor triggers and target class because the poisoned examples only account for a small proportion (0.2\%). Since DAL reduces fitting ability gradually, it also helps to improve backdoor robustness.

\section{Conclusion} \label{sec_conclusion}
In this paper, we propose Dynamics-Aware Loss (DAL) to address the discrepancy between the static robust loss functions and the dynamic nature of DNNs learning with label noise. At the early stage, since DNNs tend to learn simple and generalized patterns, DAL prioritizes fitting ability to achieve high test accuracy quickly. Subsequently, as DNNs overfit label noise gradually, DAL improves the weight of robustness thereafter. Moreover, DAL puts more emphasis on easy examples than hard ones and introduces a bootstrapping term at the later stage to further combat label noise. We provide detailed theoretical analyses and extensive experimental results to demonstrate the superiority of DAL over existing robust loss functions. Moreover, we also empirically prove that DAL is complementary to other more sophisticated robust algorithms against label noise and helps to improve backdoor robustness. To the best of our knowledge, DAL is the first robust loss functions which takes DNNs' dynamic nature into account, it serves as a simple and strong baseline for learning with label noise.

This work could be extended in the following two research directions. Firstly, for simplicity DAL lets the $q$ linearly increase from $q_s$ to $q_e$ that are both assigned manually, but employing other more complicated dynamic rules such as cosine annealing or even adaptively adjusting $q$ according to DNNs' learning status at different time steps may further boost performance. Secondly, in this paper we only empirically verify that DAL helps to improve backdoor robustness, but learning with label noise also shares some similarities with other types of weakly supervised learning. For example, partial-label learning~\cite{partial} assumes that each example is assigned with a set of candidate labels, of which only one is the ground truth while others are wrong labels. Therefore, the dynamic nature of DNNs learning with label noise may also inspire more advanced algorithms handling other weakly supervised problems.


\bibliography{mybibfile}
\bibliographystyle{unsrt}

\end{document}