

\section{Training Details}
\label{sec:training_details}
In this section, we will retrain 2D and 3D-aware face generators using our dataset.
%
Regarding the 2D generator (Sec.~\ref{subsec:StyleGAN_retrain}), we retrain StyleGAN2-ada using our dataset before fine-tuning the model using the rebalanced dataset.
%
As for the 3D-aware generator (Sec.~\ref{subsec:EG3D_retrain}), we first use our dataset to retrain EG3D, and then use the rebalanced dataset to fine-tune the model. 
In order to improve image synthesis performance during testing, we further fine-tune the model by setting the camera parameters input to the generator as the average camera.



\subsection{StyleGAN}   
\label{subsec:StyleGAN_retrain}
    \paragraph{Retrain.}
    In the StyleGAN training, we use the StyleGAN2-ada architecture as our baseline, and train it on \textit{FFHQ+LPFF} from scratch. 
    %
    We use the training parameters %that
    defined by \textit{stylegan2} config in StyleGAN2-ada.
    %
     We denote the StyleGAN2-ada model %that 
     trained on \textit{FFHQ} as $S^{FFHQ}_{var1}$, and the model %that
     trained on \textit{FFHQ+LPFF} as $S^{Ours}_{var1}$. 
    %
    Our training time is $\sim$5 days on 8 Tesla V100 GPUs.

    
    \paragraph{Rebalanced dataset fine-tuning.} %fine-tune.}
    We utilize the rebalanced dataset, \textit{FFHQ+LPFF-rebal}, to fine-tune $S^{Ours}_{var1}$, and denote the rebalanced model as $S^{Ours}_{var2}$.
    %
    All training parameters are identical to those of $S^{Ours}_{var1}$.
    %
    Our fine-tuning time is $\sim$18 hours on  8 Tesla V100 GPUs.
    
   
\subsection{EG3D}
\label{subsec:EG3D_retrain}

The mapping network, volume rendering module, and dual discriminator in EG3D~\cite{Chan_2022_CVPR} are all camera pose-dependent. We divided the EG3D model into three modules: Generator $G$, Renderer $R$, and Discriminator $D$, 
%as shown in Fig.~\ref{fig:eg3d_pipeline}. 
{please refer to the supplement file for an illustration of the three modules.}
The attribute correlations between pose and other semantic attributes in the dataset are faithfully modeled by using the camera parameters fed into $G$. $R$ and $D$ are always fed with the same camera specifications. The camera parameters help $D$ ensure multi-view-consistent super resolution and direct $R$ in how to render the final images from various camera views.

    %
    In this paper, we define two types of camera parameters that are inputted into the whole model as:
    \begin{equation}
            c = [c_{g},c_{r}],
            \label{eq:camera_presentation}
        \end{equation} 
    where $c_{g}$ stands for the camera parameters fed into $G$, and $c_{r}$ stands for the camera parameters fed into $R$ and $D$. $c_g$ will influence the face geometry and appearance and should be fixed in testing. 
    %
    The authors of EG3D discover that maintaining $c_{g}=c_{r}$ throughout training can result in a GAN that generates 2D billboards.
    %In training, the authors of EG3D find that always keeping the $c_{g}=c_{r}$ can lead to a GAN that produces 2D billboards.
    To solve this problem, they apply a swapping strategy that randomly swaps %the 
    $c_g$ with another random pose in that dataset with $\beta$ probability, where $\beta$ is a hyper-parameter.
    
% \begin{table}[t]
%     \centering
%     \scalebox{0.8}{
%     \begin{tabular}{|c|c|c|c|}
%     \hline
%     model    & arch  & initiation   & dataset   \\ \hline
%     $E^{FFHQ}_{var1}$  & \makecell[c]{EG3D \\ $\beta = 50\%$}  & / & \textit{FFHQ} \\ \hline
%     $E^{FFHQ}_{var2}$& \makecell[c]{EG3D \\ $\beta = 80\%$} & $E^{FFHQ}_{var1}$  & \textit{FFHQ-rebal}  \\ \hline
%     $E^{Ours}_{var1}$&\makecell[c]{EG3D \\ $\beta = 50\%$} &/ & \textit{FFHQ+LPFF}  \\ \hline
%     $E^{Ours}_{var2}$& \makecell[c]{EG3D \\ $\beta = 80\%$} &$E^{Ours}_{var1}$ & \textit{FFHQ+LPFF-rebal}  \\ \hline
%      $E^{Ours}_{var3}$& \makecell[c]{EG3D \\ $c_g = c_{avg}$}&$E^{Ours}_{var1}$ & \textit{FFHQ+LPFF}  \\ \hline
%     \end{tabular}
%     }
%     \caption{Model settings of EG3D training.}
%     \label{lable::Model-settings}
% \end{table}
    
    \paragraph{Retrain.}
    We use \textit{FFHQ+LPFF} to train EG3D from scratch.
    %
    All the training parameters are identical to those of EG3D,
    %
    where $\beta$ is linearly decayed from 100\%to 50\% over the first 1M images, and then fixed as 50\% in the remaining training.
    We denote the EG3D trained on \textit{FFHQ} as $E^{FFHQ}_{var1}$ (the original EG3D), denote the EG3D trained on \textit{FFHQ+LPFF} as $E^{Ours}_{var1}$.
    %
     Our training time is $\sim$6.5 days on  8 Tesla V100 GPUs. %  6 days + 
        
    
    \paragraph{Rebalanced dataset fine-tuning.}%finetune.}
     In EG3D, the authors use the rebalanced dataset \textit{FFHQ-rebal} to fine-tune $E^{FFHQ}_{var1}$, leading to a more balanced model. We denote the fine-tuned model as $E^{FFHQ}_{var2}$. 
     For a fair comparison, we also use the same fine-tuning strategy as EG3D to  fine-tune our model $E^{Ours}_{var1}$ on our rebalanced dataset \textit{FFHQ+LPFF-rebal}.
     % We also fine-tune our model $E^{Ours}_{var1}$ using our rebalanced dataset \textit{FFHQ+LPFF-rebal}.
    %
    $\beta$ is fixed as 50\% in training, and other training parameters are identical to those of EG3D.
    %
    %See Tab.~\ref{lable::Model-settings}. 
    We denote $E^{Ours}_{var1}$ fine-tuned on \textit{FFHQ+LPFF-rebal} as $E^{Ours}_{var2}$.
    %
    Our fine-tuning time is $\sim$1 day on  8 Tesla V100 GPUs. % 3h + 8h
    
    
    % \paragraph{Avg-camera-conditioned-generator fine-tuning.}
    %  To prevent the rendering results from changing with the camera poses, in testing, the EG3D generator is always conditioned by a fixed camera pose (the average camera pose $c_{avg}$) and the scene is rendered from a moving camera trajectory. 
    %  %
    %  We also find that due to the imbalance in the training dataset, the more extreme the $c_{g}$ is, the face geometry generated from the generator will have more obvious distortion, and the average camera pose $c_{avg}$ can help the generator output more reasonable geometry. 
    %  %
    %  Considering the above points, we fix the camera parameters that are inputted %input 
    %  into  the generator as  $c_g = c_{avg}$, using it as the starting point of our fine-tuning. 
    %  We use \textit{FFHQ+LPFF} as the training dataset to fine-tune $E^{Ours}_{var1}$, and denote the fine-tuned model as $E^{Ours}_{var3}$.
    %  %(see Tab.~\ref{lable::Model-settings}). 
    %  %
    %  Our fine-tuning time is $\sim$1 day on  8 Tesla V100 GPUs.
    
    
    


    
