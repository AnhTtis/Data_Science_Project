

 
        
\section{Data Preparation}
\label{sec:data_preparation}
In this section, we will introduce how to build our
large-pose face dataset. First, we describe the process for extracting data density from \textit{FFHQ} (Sec.~\ref{subsec:Camera Parameter}). Then, we introduce a novel data processing pipeline that can produce more reasonable realigned results (Sec.~\ref{subsec:Data Processing}). 
In order to filter large-pose face data from the Flickr images according to camera distribution, we propose to employ the pose density function to collect only large-pose data (Sec.~\ref{subsec:Large Pose Data Selection}). Finally, we introduce a novel rebalancing strategy (Sec.~\ref{subsec:Data Rebalance}). 

\subsection{Camera Distribution}
\label{subsec:Camera Parameter}
   {EG3D uses a face reconstruction model \cite{DBLP:conf/cvpr/DengYX0JT19}, denoted as $\mathcal{F}$ in this paper, to extract camera parameters.
   All cameras are assumed to be positioned on a spherical surface with a radius $r=2.7$, and the camera intrinsics are fixed.}
   In this paper, we only consider the camera location and ignore the roll angle of the camera to compute the camera distribution {(detailed in the supplementary file)}. 
   We convert the coordinates of each camera in \textit{FFHQ} from Cartesian coordinates to spherical coordinates and get their $\theta$ and $\phi$ (see Fig. \ref{fig:data_distribution} (a)). Notice that the face with $\theta = 90^\circ$ and $\phi = 90^\circ$ is frontal.

   
    
 
    
\subsection{Data Processing}
\label{subsec:Data Processing}
    {Given the difficulty of large-pose face detection and the imbalanced distribution of camera poses in real-life photographs, we propose a novel mechanism to collect, process, and filter large-pose data.}
    We first collect \textbf{155,720} raw portrait images from Flickr\footnote{\hyperref[]{https://www.flickr.com}} (with permission to copy, modify, distribute, and perform). 
    %
    Then we remove all the raw images that already appeared in \textit{FFHQ}.

    Our pipeline is based on that of EG3D, and we respectively align each raw image according to the image align function in EG3D and StyleGAN.
    %
    In EG3D, the authors first predict the 68 face landmarks of a raw image by Dlib, 
    and then get a realigned image by using the eyes and mouth positions to determine a square crop window for cropping and rotating the raw image.
    The realigned image is denoted as $X_{realigned}$  with the eyes at the horizontal level and the face at the center of the image. 
    Then MTCNN \cite{7553523} is used to get the positions of the eyes, the nose, and the corners of the mouth of $X_{realigned}$, and the 5 feature points are then fed into $\mathcal{F}$ to predict camera parameters. Finally, these positions are used to crop $X_{realigned}$, resulting in the final image.  
    %
    
    In our pipeline, we first use Dlib to get 68 landmarks for each of the 155,720 raw portrait images, and for those images that resist face detection, we additionally apply face alignment \cite{DBLP:conf/iccv/BulatT17} (SFD face detector) to predict landmarks.  
    %
    The face alignment detector achieves better performance on large-pose face detection than Dlib.
    Joining the two landmark predictors can help us detect as many large-pose faces as possible. Then the predicted landmarks are used to get the realigned image $X_{realigned}$. 
    {In this step, we get \textbf{506,262} $X_{realigned}$.}
    
    We find that the MTCNN sometimes cannot predict landmarks for large-pose faces. So instead of using MTCNN, we directly aggregate the 68 landmarks to get the 5 feature points of the eyes, mouth, and nose.

    
    After that, we use $\mathcal{F}$ to predict camera parameters. 
    Then we filter large-pose face data from \textbf{506,262} $X_{realigned}$ (detailed in Sec. \ref{subsec:Large Pose Data Selection}), getting \textbf{208,543}  large pose $X_{realigned}$.
    We automatically filter out low-resolution images and manually examine the rendering results of the reconstructed face models, removing any failed 3D reconstructions (which indicate incorrectly estimated camera parameters), as well as blurry or noisy images. 
    Finally, we get \textbf{19,590}
    high-quality large-pose face images with correctly estimated camera parameters. 
    %
    
    When cropping the final image, we find that some of the 5 feature points (especially when there is a face with eyeglasses) are not accurate enough to crop $X_{realigned}$ properly, but after manual selection,
    the landmarks that $\mathcal{F}$ produces are more aligned with the input faces. 
    %
    {So we use the landmarks of the reconstructed face to crop $X_{realigned}$ according to EG3D and StyleGAN functions and obtain final images. Please refer to the supplement file for an illustration of the image processing pipeline.}


        
\subsection{Large-Pose Data Selection}
%\subsection{Modulating Data Distribution}
    \label{subsec:Large Pose Data Selection}
    {
   To collect only images with ``low density'' (at large poses), we propose using the density function of FFHQ to filter large pose faces.
   %
    Inspired by \cite{DBLP:journals/tog/LeimkuhlerD21}, 
    we estimate the density of the \textit{FFHQ} camera $(\theta, \phi)$ tuples using Gaussian kernel density estimation and  Scottâ€™s rule \cite{DBLP:books/wi/Scott92} as a bandwidth selection strategy. 
   After obtaining $\rho_{ffhq}$, where $density = \rho_{ffhq} (\theta, \phi)$ is the density of the camera at $(\theta, \phi)$, 
   %
   we use $\rho_{ffhq}$ to compute the density of \textbf{506,262} $X_{realigned}$, and filter the images with a density less than 0.4 ($density = \rho_{ffhq} (\theta, \phi)<$  0.4).
   }
    

    \subsection{Data Rebalance}
    \label{subsec:Data Rebalance} 

    After image processing, large pose filtering, and carefully manual selecting, we get \textbf{19,590} large-pose face images as our \textit{LPFF} dataset.
    We use  the \textit{LPFF} dataset as a supplement to \textit{FFHQ}. That is, we combine \textit{LPFF} with \textit{FFHQ}, named \textit{FFHQ+LPFF}. The datasets are augmented by a horizontal flip.
    In Fig.~\ref{fig:data_distribution}, we show the camera distribution for both \textit{FFHQ+LPFF} and \textit{FFHQ}. 
    %Compared to \textit{FFHQ}, \textit{FFHQ+LPFF} has a wider camera distribution.

    
    
    To improve our models' performance on large-pose rendering quality and image inversion, we propose using a resampling strategy to further rebalance our \textit{FFHQ+LPFF} dataset (refer to Sec. \ref{sec:Evaluation} for evaluation).
    %
    In EG3D, in order to increase the sampling probability of the low-density data, the authors rebalanced the \textit{FFHQ} dataset by splitting it into 9 uniform-sized bins across the yaw range and duplicating the images according to the bins (as shown in Fig.~\ref{fig:data_distribution} (b)). We denoted the rebalanced \textit{FFHQ} dataset as \textit{FFHQ-rebal}.
   

    Inspired by EG3D, we also rebalance \textit{FFHQ+LPFF} to help the model focus more on large-pose data.
    %
    Instead of simply splitting the dataset according to yaw angles, we split \textit{FFHQ+LPFF} according to the data densities (Fig.~\ref{fig:data_distribution} (d)). Similar to Sec.~\ref{subsec:Camera Parameter}, we first compute the pose density function of \textit{FFHQ+LPFF} (denoted as $density = \rho_{ffhq+lpff} (\theta, \phi)$), 
    then duplicate our dataset as:
    \begin{equation}
    \centering
        \left\{
            \begin{array}{lr}
                N = \mathop{\min}(\mathop{\max}(\mathrm{round}(\frac{\alpha}{density}), 1), 4),density \geq 0.03&   \\
                N = 5,density \in [0.02,0.03)&  \\
               N = 6,density \in [0,0.02)&    
               %N = 7,density \in [0,0.01)&   
            \end{array}
        \right.
    \end{equation}
    where $\alpha$ is a hyper-parameter (we empirically set $\alpha=0.24$ in our experiments), and $N$ denotes the number of repetitions. The rebalanced \textit{FFHQ+LPFF} is denoted as \textit{FFHQ+LPFF-rebal}.


        
        
% \subsection{Unsplash Pexels Data}
% \label{Subsec: Unsplash Pexels Data}
%     Besides the images from the website Flickr, we additionally  collect 19,321 images from Unsplash\footnote{\hyperref[]{https://unsplash.com}} and Pexels\footnote{\hyperref[]{https://www.pexels.com}}, process them using the same image align method as \textit{LPFF}, denote it as \textit{Unsplash-Pexels}.
%     %
%     However, most of the images in Unsplash and Pexels were taken by professional photographers and processed by image filters, while the images in Flickr are from everyday life scenes taken by users. 
%     %
%     Although the images in \textit{Unsplash-Pexels} are aligned according to the same function as \textit{LPFF}, the domain gap between \textit{Unsplash-Pexels} and \textit{FFHQ} will prevent the models from generating view-consistent results (see Subsec. \ref{subsec:Dataset_style}).
%     %
%     So that we don't use \textit{Unsplash-Pexels} as our training data, but propose this dataset to inspire and facilitate more works in the future.

