
\section{Evaluation}
\label{sec:Evaluation}
{To show that \textit{LPFF} can help 2D and 3D-aware face generators generate realistic results across large poses, we will first evaluate the performance of 2D face generators (Sec.~\ref{sec: eval_stylegan}), and then demonstrate the performance of 3D-aware face generators (Sec. \ref{sec: eval_EG3D}).}
\subsection{StyleGAN}
\label{sec: eval_stylegan}
      \begin{figure}[t] 
          \centering
          \includegraphics[width=.85\columnwidth]{stylegan_gen.pdf}
          \caption{Images produced by our $S^{Ours}_{var1}$ model (Top) and $S^{Ours}_{var2}$ model (Bottom). We apply truncation  with $\psi=0.7$.}
          \vspace{-5pt}
          \label{fig:stylegan_gen}
        \end{figure}
    
    Fig.~\ref{fig:stylegan_gen} shows the uncurated samples of faces generated by the models trained on our dataset, with resolution $1024^2$. Our models synthesize images that are of high quality and have large pose variance.
    
    
    \paragraph{FID and perceptual path length.}  We trained the models using different datasets, so the latent space distributions are different in our experiments. Therefore, we do not compare 
    the Fr\'echet Inception Distance (FID) \cite{DBLP:conf/nips/HeuselRUNH17} and perceptual path length (PPL) \cite{DBLP:conf/cvpr/KarrasLA19} between 
    the models, since they are highly related to dataset distributions. 
    %
    Instead, we respectively measure the FID of $S^{FFHQ}_{var1}$, $S^{Ours}_{var1}$ and  $S^{Ours}_{var2}$ on their training dataset. The FID of $S^{FFHQ}_{var1}$ is 2.71 on \textit{FFHQ}, the FID of $S^{Ours}_{var1}$ is 3.407 on \textit{FFHQ+LPFF}, and the FID of $S^{Ours}_{var2}$ is 3.786 on \textit{FFHQ+LPFF-rebal}. 
    {The comparable FIDs show that the StyleGAN2-ada model can achieve convergence on our datasets as it did on \textit{FFHQ}.}
    %
    We use the PPL metric that is computed based on path endpoints in $W$ latent space, without the central crop.  The PPL of $S^{FFHQ}_{var1}$ is 144.9, the PPL of $S^{Ours}_{var1}$ is 147.6, and the PPL of $S^{Ours}_{var2}$ is 173.0.
    The PPL of $S^{Ours}_{var1}$ is comparable to the PPL of $S^{FFHQ}_{var1}$. The higher PPL of $S^{Ours}_{var2}$ indicates that $S^{Ours}_{var2}$ leads to more drastic image feature changes when performing interpolation in the latent space. We %, we 
    attribute this to the larger pose variance in $S^{Ours}_{var2}$'s latent space and the \textit{FFHQ+LPFF-rebal} dataset.
    % \onethousand{
    % Instead, we respectively measure the FID and perceptual path length of $S^{FFHQ}_{var1}$, $S^{Ours}_{var1}$ and  $S^{Ours}_{var2}$ on their training dataset, as shown in Tab. \ref{tab:fid_ppl}.
    % The comparable FIDs show that the StyleGAN2-ada model can achieve convergence on our datasets as it did on \textit{FFHQ}.
    % %
    % We use the PPL metric that is computed based on path endpoints in $W$ latent space, without the central crop. 
    % The PPL of $S^{Ours}_{var1}$ is comparable to the PPL of $S^{FFHQ}_{var1}$. The higher PPL of $S^{Ours}_{var2}$ indicates that $S^{Ours}_{var2}$ leads to more drastic image feature changes when performing interpolation in the latent space. We %, we 
    % attribute this to the larger pose variance in $S^{Ours}_{var2}$'s latent space and the \textit{FFHQ+LPFF-rebal} dataset.
    % }


  \begin{figure}[t]
    	\centering
    	{\includegraphics[width=0.95\columnwidth]{stylegan_interface_pose_comparison.pdf}}
    	\caption{Pose manipulation comparison between $S^{FFHQ}_{var1}$ (Top), $S^{Ours}_{var1}$ (Middle), and $S^{Ours}_{var2}$ (Bottom). The images highlighted by the blue box are generated from randomly sampled latent codes, and all the samples are linearly moved along the yaw editing direction with the same distance.
    	}
    	\vspace{-5pt}
    	\label{fig:stylegan_interface_pose_comparison}
    \end{figure}
    
%        \begin{table}[]
%        \centering
%         {%
%         \begin{tabular}{ccc}
%         \hline
%         \multicolumn{1}{l}{model} & FID   & PPL  \\ \hline
%         $S^{FFHQ}_{var1}$         & 2.71  & 144.9 \\
%         $S^{Ours}_{var1}$         & 3.41  & 147.6 \\
%         $S^{Ours}_{var2}$         & 3.79 & 173.0 \\ \hline
%         \end{tabular}%
%         }
%             \caption{ 
%             \onethousand{Quantitative evaluation of FID and PPL.}
%             }
%             \vspace{-10pt}
%         \label{tab:fid_ppl}
% \end{table}

    
\paragraph{Pose manipulation.}
    We compare the pose distribution of the latent spaces by displaying the results of linear yaw pose manipulation. 
    %
    {For each model, we label randomly sampled latent codes according to the camera parameters of the corresponding synthesized images (yaw angles $\textgreater 90^{\circ}$ as positive and $\leq 90^{\circ}$ as negative) and use InterfaceGAN \cite{DBLP:conf/cvpr/ShenGTZ20} to compute the yaw editing direction. 
    The pose editing results are then obtained by moving randomly sampled latent codes along the yaw editing direction, as shown in Fig. \ref{fig:stylegan_interface_pose_comparison}.
    %
    Because the linear manipulation method is used without any semantic attribute disentanglement, the results of all models cannot preserve facial identity.}
    %
    As for $S^{FFHQ}_{var1}$, the ``side face'' results are far from a genuine human face, demonstrating that the latent codes have reached the edge of the latent space.
    %
    With regard to  $S^{Ours}_{var1}$ and $S^{Ours}_{var2}$, our models produce reasonable and {comparable} large-pose portraits. 
    The comparison shows that our models' latent spaces are more extensive and better able to represent large-pose data.
  
         
    
 \paragraph{Large-pose data inversion and manipulation.}
    To further show that our models can better represent large-pose data, we project large-pose portraits into the latent spaces of those models (see Fig.~\ref{fig:stylegan_avg_init_projection}), and apply semantic editing to the obtained latent codes.
    %
    We collect the testing images from Unsplash\footnote{\hyperref[]{https://unsplash.com}} and Pexels\footnote{\hyperref[]{https://www.pexels.com}} (independent of both \textit{FFHQ} and \textit{LPFF}). We then employ 500-step latent code optimization in $W+$ latent space to minimize the distance between the synthesized image and the target image. 
    %
    To evaluate the editability of the projected latent codes, we use the attribute classifiers provided by StyleGAN \cite{DBLP:conf/cvpr/KarrasLA19} and employ InterfaceGAN to compute semantic boundaries for each model, and then use the boundaries to edit the projected latent codes.
    %
    We also use the yaw editing directions to try to make the large pose data face forward.
    %
    Please refer to the supplement for those semantic editing results. % in the supplement. 
    %
    As shown in those projection and manipulation results, the models trained on our dataset have fewer artifacts and can better represent the large pose data in their latent spaces. 
    {What's more, $S^{Ours}_{var2}$ outperforms $S^{Ours}_{var1}$ because $S^{Ours}_{var2}$ is trained on a more balanced dataset, which proves the effectiveness of our data rebalance strategy.
    }
    
         \begin{figure}[t]
    	\centering
    	{\includegraphics[width=.8\columnwidth]{stylegan_avg_init_projection.pdf}}
    	\caption{Large-pose data projection comparison between $S^{FFHQ}_{var1}$, $S^{Ours}_{var1}$, and $S^{Ours}_{var2}$. The target images (the first row) are collected from Unsplash and Pexels websites.}
    	\vspace{-5pt}
    	\label{fig:stylegan_avg_init_projection}
    \end{figure}
       
       
       
      % \begin{figure*}[t] 
      %     \centering
      %     \includegraphics[width=0.78\linewidth]{eg3d_gen_curated-v2.pdf}
      %     \caption{
      %     \onethousand{Image-shape pairs produced by $E^{FFHQ}_{var1}$,$E^{FFHQ}_{var2}$, $E^{Ours}_{var1}$, $E^{Ours}_{var2}$, and $E^{Ours}_{var3}$. We apply truncation  with $\psi=0.8$.}
      %     }
      %     \label{fig:eg3d_gen_curated}
      %   \end{figure*}


      \begin{figure*}

\begin{minipage}[b]{.65\linewidth}
    
    \includegraphics[width=1.0\linewidth]{eg3d_gen_curated-v2.pdf}
          \caption{
          {Image-shape pairs produced by $E^{FFHQ}_{var1}$,$E^{FFHQ}_{var2}$, $E^{Ours}_{var1}$, and $E^{Ours}_{var2}$.
          %, and $E^{Ours}_{var3}$. 
          We apply truncation  with $\psi=0.8$.}
          }
          \label{fig:eg3d_gen_curated}
\end{minipage}
\medskip
 \hfill
 \begin{minipage}[b]{.32\linewidth}
    \centering
   \scalebox{0.7}{
    \begin{tabular}{@{}cccc@{}}
    \toprule
    \multicolumn{1}{l}{\multirow{2}{*}{model}} & \multicolumn{1}{l}{$c_g =c_{avg}$} & \multicolumn{1}{l}{$c_g \sim$ FFHQ}  & \multicolumn{1}{l}{$c_g \sim$ LPFF}  \\
    \multicolumn{1}{l}{}   & $c_r \sim$ FFHQ     & $c_r \sim$FFHQ & $c_r \sim$FFHQ    \\ \midrule
    $E^{FFHQ}_{var1}$ & 0.771  &	0.768 &	0.760   	 \\
    $E^{Ours}_{var1}$ &\textbf{0.804}&\textbf{0.792}&\textbf{0.778} \\ \midrule
    $E^{FFHQ}_{var2}$ &0.770&0.769&0.766   \\
    $E^{Ours}_{var2}$ &\textbf{0.789}&\textbf{0.784}&\textbf{0.771}\\ \bottomrule
    \end{tabular}
    }
            \captionof{table}{ 
            {Quantitative evaluation of facial identity consistency ($\uparrow$). }
            }
        \label{tab:Facial_identity}
 \vspace{5pt}
    \scalebox{0.7}{
        \begin{tabular}{@{}cccc@{}}
        \toprule
        \multicolumn{1}{l}{\multirow{2}{*}{model}} & \multicolumn{1}{l}{$c_g =c_{avg}$} & \multicolumn{1}{l}{$c_g \sim$ FFHQ}  & \multicolumn{1}{l}{$c_g \sim$ LPFF}  \\
        \multicolumn{1}{l}{}   & $c_r \sim$ FFHQ     & $c_r \sim$FFHQ & $c_r \sim$FFHQ    \\ \midrule
        $E^{FFHQ}_{var1}$ & 0.134&0.133&0.159	 	 \\
        $E^{Ours}_{var1}$ &\textbf{0.119}&\textbf{0.124}&\textbf{0.134} \\ \midrule
        $E^{FFHQ}_{var2}$ &0.135&0.130&0.142    \\
        $E^{Ours}_{var2}$ &\textbf{0.117}&\textbf{0.122}&\textbf{0.131}  \\  \bottomrule
        \end{tabular}
        }
                \captionof{table}{{
                    Quantitative evaluation of geometry consistency ($\downarrow$).
                }
                }
            \label{tab:Geometry}
\end{minipage} %\par

\end{figure*}



        
\begin{table*}[t]
\centering
\scalebox{0.7}{
    \begin{tabular}{@{}ccccccccc@{}}
    \toprule
    \multicolumn{1}{l}{\multirow{2}{*}{model}} & \multicolumn{1}{l}{$c_g =c_{avg}$} & \multicolumn{1}{l}{$c_g =c_{avg}$} & \multicolumn{1}{l}{$c_g \sim$ FFHQ} & \multicolumn{1}{l}{$c_g \sim$ FFHQ} & \multicolumn{1}{l}{$c_g \sim$ LPFF} & \multicolumn{1}{l}{$c_g \sim$ LPFF} & \multicolumn{1}{l}{$c_g \sim$ FFHQ} & \multicolumn{1}{l}{$c_g \sim$ LPFF}\\
    \multicolumn{1}{l}{}   & $c_r \sim$ FFHQ     & $c_r \sim$ LPFF  & $c_r \sim$FFHQ & $c_r \sim$LPFF   & $c_r \sim$FFHQ & $c_r \sim$LPFF & $c_r =c_g$ &  $c_r =c_g$    \\ \midrule
    $E^{FFHQ}_{var1}$ &\textbf{6.523}&23.598&\textbf{4.273}&22.318&23.698&36.641&\textbf{4.025}&23.301   \\
    $E^{Ours}_{var1}$ & 7.997&\textbf{20.896}&6.623&\textbf{19.738}&\textbf{21.300}&\textbf{22.074}&6.093&\textbf{16.026}  \\ \midrule
    $E^{FFHQ}_{var2}$ &\textbf{6.589}&20.081&\textbf{4.456}&19.983&19.469&30.181&\textbf{4.262}&23.717   \\
    $E^{Ours}_{var2}$ & 9.829&\textbf{16.775}&6.672&\textbf{15.047}&\textbf{13.022}&\textbf{14.836}&6.571&\textbf{12.221} \\ \bottomrule
    \end{tabular}
    }
            \caption{FID ($\downarrow$) for EG3D generators that are trained on different datasets. We calculate the FIDs by sampling 50,000 images using different sampling strategies and different camera distributions. 
            {We compare the models that are trained with the same training strategy ($var1/var2$).
            %$and separately list the results of our novel fine-tuning method ($var3$).
            }
            \vspace{-5pt}
            }
        \label{tab:FID1}
\end{table*}


%   \begin{table}[t]
%             \centering
            
%     \scalebox{0.8}{
%     \begin{tabular}{@{}cccc@{}}
%     \toprule
%     \multicolumn{1}{l}{\multirow{2}{*}{model}} & \multicolumn{1}{l}{$c_g =c_{avg}$} & \multicolumn{1}{l}{$c_g \sim$ FFHQ}  & \multicolumn{1}{l}{$c_g \sim$ LPFF}  \\
%     \multicolumn{1}{l}{}   & $c_r \sim$ FFHQ     & $c_r \sim$FFHQ & $c_r \sim$FFHQ    \\ \midrule
%     $E^{FFHQ}_{var1}$ & 0.771  &	0.768 &	0.760   	 \\
%     $E^{Ours}_{var1}$ &\textbf{0.804}&\textbf{0.792}&\textbf{0.778} \\ \midrule
%     $E^{FFHQ}_{var2}$ &0.770&0.769&0.766   \\
%     $E^{Ours}_{var2}$ &\textbf{0.789}&\textbf{0.784}&\textbf{0.771}\\ \midrule
%     $E^{Ours}_{var3}$ &0.785 &/&/\\ \bottomrule
%     \end{tabular}
%     }
%             \caption{ 
%             {Quantitative evaluation of facial identity consistency ($\uparrow$). Compared to the models trained on \textit{FFHQ}, our models present significant improvements in facial identity consistency. The identity similarity is computed by the ArcFace model \cite{DBLP:conf/cvpr/DengGXZ19}.}
%             }
%         \label{tab:Facial_identity}
% \end{table}
%      \begin{table}[t]
%             \centering
            
%     \scalebox{0.8}{
%         \begin{tabular}{@{}cccc@{}}
%         \toprule
%         \multicolumn{1}{l}{\multirow{2}{*}{model}} & \multicolumn{1}{l}{$c_g =c_{avg}$} & \multicolumn{1}{l}{$c_g \sim$ FFHQ}  & \multicolumn{1}{l}{$c_g \sim$ LPFF}  \\
%         \multicolumn{1}{l}{}   & $c_r \sim$ FFHQ     & $c_r \sim$FFHQ & $c_r \sim$FFHQ    \\ \midrule
%         $E^{FFHQ}_{var1}$ & 0.134&0.133&0.159	 	 \\
%         $E^{Ours}_{var1}$ &\textbf{0.119}&\textbf{0.124}&\textbf{0.134} \\ \midrule
%         $E^{FFHQ}_{var2}$ &0.135&0.130&0.142    \\
%         $E^{Ours}_{var2}$ &\textbf{0.117}&\textbf{0.122}&\textbf{0.131}  \\ \midrule
%         $E^{Ours}_{var3}$ &0.122 &/&/  \\ \bottomrule
%         \end{tabular}
%         }
%                 \caption{{
%                     Quantitative evaluation of geometry consistency ($\downarrow$). Compared to the models trained on \textit{FFHQ}, our models present significant improvements in geometry consistency. The geometry is reconstructed by the face reconstruction model \cite{DBLP:conf/cvpr/DengYX0JT19}.
%                 }
%                 }
%             \label{tab:Geometry}
%     \end{table}



        
    \subsection{EG3D}   
    \label{sec: eval_EG3D}
      Fig.~\ref{fig:eg3d_gen_curated} provides the selected samples that are generated by the models trained {on the \textit{FFHQ} dataset and our dataset}, with resolution $512^2$. Even in large poses, our synthesized images and 3D geometry are high-quality.
      
              \paragraph{FID.}
        \label{subsec:FID}
        In EG3D, the generator is conditioned on a fixed camera pose ($c_{g}$) when rendering from a moving camera trajectory to prevent the scene from changing when the camera ($c_r$) moves during inference. However, EG3D's authors evaluated the FID of EG3D by conditioning the model on $c_{g}$ and rendering results from $c_r = c_g$. This approach cannot demonstrate the performance of multi-view rendering during inference, since
        the generator always ``sees'' the true pose of the rendering camera in evaluation, but omits other poses. 
         %
        For a 3D-aware generator, we are more interested in how a face looks from various camera views (which can indicate the quality of face geometry to some extent).
        So a more reasonable way is to let $c_r$ and $c_g$ be independent of
        each other and sample them from the respective  distributions that are of our interest.
        %
        To achieve this, we propose a novel FID measure, which is based on
        three camera sampling strategies. First, we fix %the 
        $c_{g}$ as $c_{avg}$ and then sample $c_r$ from different datasets. Second, we respectively sample $c_{r}$ and $c_{g}$ from different datasets. Third, we sample $c_{g}$ from different datasets and set $c_r=c_g$ (the one that was used in EG3D). See the calculated FID values in Tab. \ref{tab:FID1}.
        
        
        
        
        Models trained on our datasets exhibit improvements in FID in most cases, particularly when the final results are rendered from large poses ($c_r \sim LPFF$), or when the generator is conditioned on large poses ($c_g \sim LPFF$).  
        {We notice that there is an increased FID when computing $c_g=c_{avg}/c_r, c_r\sim FFHQ$.}
        % The possible reasons are twofold. 
        % First, FID is highly related to the data distribution, and the new addition of \textit{LPFF} data changes the data distribution when rendering from medium poses ($c_r \sim FFHQ$).
        % Second, as explained by the authors of EG3D,  the pre-trained $E^{FFHQ}_{var1}$ and $E^{FFHQ}_{var2}$ were achieved using buggy (XY, XZ, ZX) planes.  
        % In our experiments, since we fix this bug as they suggested using (XY, XZ, ZY),
        % the XZ-plane representation's dimension would be cut in half, thus weakening the expressive capacity for frontal faces.
        As explained by the authors of EG3D,  the pre-trained $E^{FFHQ}_{var1}$ and $E^{FFHQ}_{var2}$ were achieved using buggy (XY, XZ, ZX) planes.  
        In our experiments, since we fix this bug as they suggested using (XY, XZ, ZY),
        the XZ-plane representation's dimension would be cut in half, thus weakening the expressive capacity for frontal faces.
        
    
        
        Thanks to our dataset rebalancing strategy, $E^{Ours}_{var2}$ can pay more attention to large pose data and enhance the rendering quality, thus further improving the FID of $E^{Ours}_{var1}$ on large poses. 
        {When computing FID of $c_g=c_{avg},c_r\sim FFHQ$, we notice that $E^{Ours}_{var2}$ has an increased FID compared to $E^{Ours}_{var1}$, while $E^{FFHQ}_{var2}$ and  $E^{FFHQ}_{var1}$ have comparable results. 
        This is due to the addition of new large-pose data, \textit{LPFF}.
        FID is highly related to the data distribution, and the rebalancd \textit{FFHQ+LPFF-rebal} dataset changes the data distribution when rendering from medium poses.
        }
        
        

         
        %   \begin{figure*}[t]
        % 	\centering
        % 	{\includegraphics[width=0.8\linewidth]{face_scape_projection.pdf}}
        % 	\caption{We use latent code optimization to fit four testing images (left) of a single identity from FaceScape. Then we render the obtained latent codes from four novel views (right). The optimization is performed in $W+$ space, and the generators are conditioned on the average camera parameters.}
        % 	\label{fig:face_scape_projection}
        % \end{figure*}
    
        % \begin{figure*}[h]
        %     \centering
        %     \includegraphics[width=0.9\textwidth]{Unsplash_Pexels_4.pdf}
        %           \caption{
        %           We use HFGI3D \cite{xie2022high} to fit the single-view testing image. Then we render the obtained latent codes from four novel views. The inversion is performed in $W$ space, and the generators are conditioned on the average camera parameters.
        %           }
        %           \label{fig:Unsplash_Pexels_4}
        % \end{figure*}       


          \begin{figure*}[h]
            \centering
            \includegraphics[width=0.85\textwidth]{Unsplash_Pexels_4.pdf}
                  \caption{
                To fit the single-view testing image, we employ HFGI3D \cite{xie2022high}. The obtained latent codes are then rendered using four novel views. The inversion is carried out in $W$ space, and the generators are conditioned on $c_{avg}$.
                   }
                  \label{fig:Unsplash_Pexels_4}
        \end{figure*}       


        
         
        \paragraph{Facial identity consistency.}
         \label{subsec:Facial_Identity}   
        We leverage ArcFace \cite{DBLP:conf/cvpr/DengGXZ19} to measure the models' performance on facial identity maintenance. We render two novel views for 1,024 random faces and use ArcFace to compute the mean identity similarity for all image pairs.
        %
        %See Tab. \ref{tab:Facial_identity}, 
        We employ three sampling strategies for $c_g$ to evaluate the generator's performance on the camera distribution of different datasets. As for $c_r$, we find that the extreme rendering camera views will heavily influence the performance of ArcFace, so we only sample $c_r$ from the \textit{FFHQ} dataset, where most of the faces have small to medium poses. As shown in Tab. \ref{tab:Facial_identity}, our models present significant improvements in facial identity consistency across different sample strategies and datasets.
      

        
          \paragraph{Geometry consistency.}
        \label{subsec:Geometry}   
        We employ $\mathcal{F}$, which outputs 3DMM coefficients to evaluate the geometry consistency.
        We employ the same camera sampling methods as in facial identity consistency computation.
        We first render two novel views for 1,024 random faces. Then
        for each image pair, we compute the mean L2 distance of the face id and expression coefficient.
        As shown in Tab. \ref{tab:Geometry}, our models present improvement in geometry consistency across different sample strategies and datasets.
         %
  

        
      
     
         \paragraph{Image inversion.}
         To evaluate the ability to fit multi-view images, we use FaceScape \cite{DBLP:conf/cvpr/Yang0WHSYC20} as the testing data.
         %
         We use four multi-view images (including one with a small pose) of a single identity as the reference images. We perform latent code optimization to simultaneously project one or four images into $W+$ latent space. Then we use the camera parameters that are extracted from another 4 multi-view images to render novel views. Please refer to the supplement for multi-view image inversion results.
         %
         {
         Because occluded face parts are unavoidable in single-view portraits, we perform single-view image inversion using HFGI3D \cite{xie2022high}, a novel method that combines pseudo-multi-view estimation with visibility analysis.
         }
         %
         As shown in Fig.~\ref{fig:Unsplash_Pexels_4}, the inversion results indicate that $E^{FFHQ}_{var1}$ and $E^{FFHQ}_{var2}$  suffer from the ``wall-mounted'' unrealistic geometry.
         Due to the adhesion between the head and the background, there are missing ears in View 2 and distorted ears and necks in Views 3 and 4 (highlighted by green boxes). A pointed nose exists in View 2 (highlighted by blue boxes).
         %
         Our $E^{Ours}_{var1}$ and $E^{Ours}_{var2}$ models produce reconstructed face geometry that is free from those artifacts, suggesting that the learned 3D prior from our dataset is more realistic. It also shows that after employing the data rebalance in Sec. \ref{subsec:Data Rebalance}, lips are more natural in $E^{Ours}_{var2}$ compared to $E^{Ours}_{var1}$ (highlighted by an orange box).
         
       
    
      
%   \begin{figure}[h]
%     	\centering
%     	{\includegraphics[width=.9\columnwidth]{eg3d_seam.pdf}}
%     	\caption{After employing GAN inversion, we use $E^{FFHQ}_{var1}$ (Top) and $E^{Ours}_{var1}$ (Bottom) to render novel views for the same target image (not shown in this figure). The ``seam" artifacts are highlighted by blue boxes in the results of $E^{FFHQ}_{var1}$.}
%     	\label{fig:eg3d_seam}
%     \end{figure}
    
    
        \paragraph{``Seam'' artifacts.}
        The authors of IDE-3D speculate that the ``seam"  artifacts in EG3D could be caused by the imbalanced camera pose distribution of datasets, and propose a density regularization loss to deal with the ``seam"  artifacts along the edge of the faces. 
        %
        Compared to the IDE-3D, our model $E^{Ours}_{var1}$ is trained without requiring any additional regularization loss or any data rebalance strategy, and is free from the ``seam" artifacts.  Please refer to the supplement for the illustration of ``seam" artifacts.
           
       
        
    
       
   
    
