\section{Method}
\label{sec:method}

\begin{figure*}
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{resources/model_overview}
    \caption{Model Overview}
    \label{fig:architecture_model_overview}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{resources/animation_module}
    \caption{Animation model}
    \label{fig:architecture_animation_model}
\end{subfigure}
\hfill
\begin{subfigure}{0.99\textwidth}
    \includegraphics[width=\textwidth]{resources/synthesis_module_with_legend}
    \caption{Synthesis model}
    \label{fig:architecture_synthesis_model}
\end{subfigure}

  \caption{(a) Overview of our framework. The \emph{animation model} produces states $\vecsequence$ based on user-provided conditioning signals, \change{or \emph{prompts},} $\vecsequencecond, \vectextactioncond$ that are rendered by the \emph{synthesis model}. (b) The diffusion-based animation model predicts noise $\vecdiffnoise_\difftimestep$ applied to the noisy states $\vecsequenceprednoise_\difftimestep$ conditioned on known states $\vecsequencecond$ and actions $\vectextactioncond$ with the respective masks $\vecmask, \vecmaskaction$, diffusion step $\difftimestep$ and framerate $\framerate$. The \emph{text encoder} $\nettext$ produces embedding for the textual actions, while the \emph{temporal model} $\netanimation$ performs noise prediction. (c) The synthesis model renders the current state using a composition of neural radiance fields, one for each object. A \emph{style encoder} $\netstyle$ extracts the appearance $\vecstyle$ of each object. Each object is represented in its canonical pose by $\netcanonical$ and deformations of articulated objects are modeled by the \emph{deformation model} $\netdeformation$. After integration and composition, the feature grid $\tensnerffeatures$ is rendered to the final image using the \emph{feature enhancer} $\netenhancer$.}
  \label{fig:architecture} 
\end{figure*} 

This section introduces our framework for the creation of \change{\emph{Promptable Game Models}} that allows the user to perform a range of dynamic scene editing tasks, \change{formulated as a set of conditioning \emph{prompts}}. 

We divide our \change{PGM} into two modules: a \emph{synthesis model} and an \emph{animation model}. The synthesis model generates an image given the representation of the environment state. The animation model, instead, aims at modeling the game's dynamics, with player actions and interactions, in the high-level space of the environment states.  Actions are modeled as text, which is an expressive, yet intuitive form of control for a wide range of tasks.
% 
The overview of our framework is provided in Fig.~\ref{fig:architecture_model_overview}.
% 

In more detail, our model defines the state of the entire environment as the combination of all individual object states. Consequently, each individual state is the set of the object properties such as the position of each object in the scene, their appearance, or their pose. Formally, the environment state at time $\seqtimestep$ can be represented by $\vecsequence_\seqtimestep \in \mathcal{\setstate}=(\mathbb{R}^{n_1} \times ... \times \mathbb{R}^{n_{\numproperties}})$, $\numproperties$ properties of variable length $n_i$ defined as the union of the properties of each object. This state representation captures all variable aspects of each object in the environment, thus it can be used by the synthesis model to generate the scene.

On the other hand, the animation model predicts the evolution of an environment in time, which is represented by the sequence of its states $\{\vecsequence_1,\vecsequence_2, 
\dots \vecsequence_{\numseqtimesteps}\} =\vecsequence \in \mathcal{\setstate}^{\numseqtimesteps}$, where $\numseqtimesteps$ is the length of the sequence. The model provides control over sequence generation with the help of user-defined conditioning signals, \change{or prompts,} that can take two forms: explicit
state manipulation and high-level text-based editing. With respect to the former, the user could change the position of the tennis ball at time step $\seqtimestep$, and the model would automatically adapt the position of the ball in other nearby states. As far as the latter is concerned, users could provide high-level text-based values of actions such as \emph{"The player takes several steps to the right and hits the ball with a backhand"} and the model would generate the corresponding sequence of states (see Fig.~\ref{fig:alternative_actions_qualitatives}). 
These generic actions in the form of text are central to enabling high-level, yet fine-grained control over the evolution of the environment. To train our framework we assume a dataset of camera-calibrated videos, where each video frame is annotated with the corresponding states $\vecsequence$ and actions $\vectextaction$. 

\subsection{Synthesis Model}
\label{sec:synthesis_model}
In this section, we describe the synthesis model that renders states from controllable viewpoints (see Fig.~\ref{fig:architecture_synthesis_model}). We build our model based on a compositional NeRF~\cite{Menapace2022PlayableEnvironments} framework which enables explicit control over the camera and represents a scene as a composition of different, independent objects. Thanks to the independent representation of objects, each object property is directly linked to an aspect of the respective object and can thus be easily controlled and manipulated. The compositional NeRF framework allows different, specialized NeRF architectures to be used for each object based on its type. To further improve quality, rather than directly rendering RGB images with the NeRF models, we render features and make use of a feature enhancer CNN to produce the RGB output. In order to represent objects with different appearances, we condition the NeRF and enhancer models on the style codes extracted with a dedicated style encoder \cite{Menapace2022PlayableEnvironments}. Our model is trained using reconstruction as the main guiding signal.

In Sec.~\ref{sec:nerf_overview}-\ref{sec:objs} we illustrate the main components of the synthesis module and in Sec.~\ref{sec:synthesis_training} we describe the training procedure.

\subsubsection{Scene Composition with NeRFs}
\label{sec:nerf_overview}
Neural radiance fields represent a scene as a radiance field, a 5D function parametrized as a neural network mapping the current position $\vecpoint$ and viewing direction $\vecdirection$ to density $\vecdensity$ and radiance $\veccolor$. 

To allow controllable generation of complex scenes, we adopt a compositional strategy where each object in the scene is modeled with a dedicated NeRF model \cite{Menapace2022PlayableEnvironments,mueller2022autorf,xu2022discoscene}.
The scene is rendered by sampling points independently for each object and querying the respective object radiance field $\netcanonical_i$. \change{The results for all objects are then merged and sorted by distance from the camera before being integrated.}

All objects are assumed to be described by a set of properties whose structure depends on the type of object, \emph{e.g.} a player, the ball, the background. We consider the following properties:
\begin{itemize}
\item \emph{Object location}.  Each object is contained within an axis-aligned bounding box $\vecbboxthree_i$ which is defined by size and position. In the case of the ball, we additionally consider its velocity to model blur effects (Sec.~\ref{sec:objs}). 
\item \emph{Object style}. All objects have an appearance that may vary in different sequences, thus we introduce a style code $\vecstyle_i$ as an additional property for all objects. Since it is difficult to define such style information a priori, we assume it to be a latent variable and learn it jointly during training.
\item \emph{Object pose}. Articulated objects such as humans require additional properties to model varying poses. We model the deformation of articulated objects as a kinematic tree with ${\numjoints}_i$ joints and consider as object properties the rotation $\tensrotation$ and translation $\vectranslation$ parameters associated with each joint (Sec.~\ref{sec:deformation_modeling}). 
\end{itemize}
From now on, we drop the object index $i$ to simplify notation.

  
\subsubsection{Style Encoder}
\label{sec:style_encoder}
Representing the appearance of each object is challenging since it changes based on the type of object and illumination conditions. We treat the style $\vecstyle$ for each object as a latent variable that we regress using a convolutional style encoder $\netstyle$. Given the current video frame $\tensimage$ with $\numobjects$ objects, we compute 2D bounding boxes ${\vecbboxtwo}$ for each object. First, a set of residual blocks is used to extract frame features which are later cropped around each object according to ${\vecbboxtwo}$ using RoI pooling \cite{girshick2013roipool}. Later, a series of convolutional layers with a final projection is used to predict the style code $\vecstyle$ from the cropped feature maps.


\subsubsection{Volume Modeling for Efficient Sampling}
\label{sec:canonical_volume}

Radiance fields are commonly parametrized using MLPs~\cite{mildenhall2020nerf} but such representation requires a separate MLP evaluation for each sampled point, making it computationally challenging to train high-resolution models.
To overcome such issue, we model the radiance field $\netcanonical$ of each object in a canonical space using two alternative parametrizations.

For three-dimensional objects, 
we make use of a voxel grid parametrization \cite{fridovich2022plenoxels,weng2022humannerf}. Starting from a fixed noise tensor $\tensvoxelsmall \in \mathbb{R}^{\numfieldfeatsmall \times \numvoxelheightsmall \times \numvoxelwidthsmall \times \numvoxeldepthsmall}$, a series of 3D convolutions produces a voxel $\tensvoxel \in \mathbb{R}^{\numfieldfeat + 1 \times \numvoxelheight \times \numvoxelwidth \times \numvoxeldepth}$ containing the features and density associated to each point in the bounded space. Here, $\numfieldfeatsmall$ and $\numfieldfeat$ represent the number of features, while $\numvoxelheight$, $\numvoxelwidth$ and $\numvoxeldepth$ represent the size of the voxel. Given a point in the object canonical space $\vecpointcanon$, the associated features and density $\vecdensity$ are retrieved using trilinear sampling on $\tensvoxel$. To model the different appearance of each object, we adopt a small MLP conditioned on the style $\vecstyle$ to produce a stylized feature with the help of weight demodulation~\cite{karras2019stylegan2}. 

For two-dimensional objects such as planar scene elements, we make use of a similar parametrization  where a fixed 2D noise tensor $\tensplanesmall \in \mathbb{R}^{\numfieldfeatsmall \times \numplaneheightsmall \times \numplanewidthsmall}$ is mapped to a plane of features $\tensplane \in \mathbb{R}^{\numfieldfeat \times \numplaneheight \times \numplanewidth}$ using a series of 2D convolutions.  
Given a ray $\ray$, we compute the intersection point $\vecpoint$ between the plane and the ray which is used to sample $\tensplane$ using bilinear sampling. Similarly to the voxel case, a small MLP is used to model object appearance according to $\vecstyle$. We assume planes to be fully opaque and assign a fixed density value $\vecdensity$ to each sample. Thanks to this representation, a single point per ray is sufficient to render the object. 


\subsubsection{Deformation Modeling}
\label{sec:deformation_modeling}
Since the radiance field $\netcanonical$ alone supports only rendering of rigid objects expressed in a canonical space, to render articulated objects such as humans we introduce a deformation model $\netdeformation$. Given an articulated object, we assume its kinematic tree is known and that the transformation $[\tensrotation_j|\vectranslation_j]$ from each joint $j \in 1,...,\numjoints$ to the parent joint is part of the object's properties.  We then implement a deformation procedure based on linear blend skinning (LBS) \cite{lewis2000pose} and inspired by HumanNeRF \cite{weng2022humannerf} that employs the joint transformations and a learned volume of blending weights $\tensblendweights \in \mathbb{R}^{\numjoints + 1 \times \numblendweightsheight \times \numblendweightswidth \times \numblendweightsdepth}$ to associate each point in the bounding box of the articulated object to the corresponding one in the canonical volume. We present additional details in \apref{ap:deformation_modeling}.

\subsubsection{Enhancer}
\label{sec:enhancer}
NeRF models are often parametrized to output radiance $\veccolor \in \mathbb{R}^3$ and directly produce an image. However, we find that such approach struggles to produce correct shading of the objects, with details such as shadows being difficult to synthesize. Also, to improve the computational efficiency of the method, we sample a limited number of points per ray that may introduce subtle artifacts in the geometry. To address these issues, we parametrize the model $\netcanonical$ to output features where the first three channels represent radiance and the subsequent represent learnable features. Then, we produce a feature grid $\tensnerffeatures \in \mathbb{R}^{\numfieldfeat \times \numimgheight \times \numimgwidth}$ and an RGB image $\tensimagergb \in \mathbb{R}^{3 \times \numimgheight \times \numimgwidth}$. We introduce an enhancer network $\netenhancer$ modeled as a UNet~\cite{ronneberger2015unet} architecture interleaved with weight demodulation layers~\cite{karras2019stylegan2} that maps $\tensnerffeatures$ and the style codes $\vecstyle$ to the final RGB output $\tensimagerec \in \mathbb{R}^{3 \times \numimgheight \times \numimgwidth}$.


\subsubsection{Object-specific rendering}
\label{sec:objs}
Our compositional approach allows the use of object-specific techniques. In particular, in the case of tennis, we detail in \apref{ap:object_specific_techniques} how we can apply dedicated procedures to enhance the rendering quality of the ball, the racket, and the 2D user interfaces such as the scoreboards. The rendering of the tennis ball is treated specially to render the blur that occurs in real videos in the case of fast-moving objects. The racket can be inserted in a post-processing stage to compensate for the difficulty of NeRFs to render thin, fast-moving objects. Finally, the UI elements are removed from the scene since they do not behave in a 3D consistent manner. For Minecraft, we describe how the scene skybox is modeled.


\subsubsection{Training}
\label{sec:synthesis_training}
We train our model using reconstruction as the main driving signal. Given a frame $\tensimage$ and reconstructed frame $\tensimagerec$, we use a combination of L2 reconstruction loss and the perceptual loss of Johnson \etal \cite{johnson2016perceptual} as our training loss. To minimize the alterations introduced by the enhancer and improve view consistency, we impose the same losses between $\tensimage$ and $\tensimagergb$, the output of the model before the feature enhancer. All losses are summed without weighting to produce the final loss term. To minimize GPU memory consumption, instead of rendering full images, we impose the losses on sampled image patches instead~\cite{Menapace2022PlayableEnvironments}.

We train all the components of the synthesis model jointly using Adam \cite{kingma2014adam} for 300k steps with batch size 32. We set the learning rate to $1e-4$ and exponentially decrease it to $1e-5$ at the end of training.
 The framework is trained on videos with 1024x576px resolution. We present additional details in \apref{ap:implementation_details_synthesis} and in \apref{ap:training_details_synthesis}, \change{and discuss inference details in \apref{ap:inference_details}}.


\subsection{Animation Model}
\label{sec:animation_model}

In this section, we describe the animation model (see Fig.~\ref{fig:architecture_animation_model}), whose task is that of generating sequences of states $\vecsequence \in \mathcal{\setstate}^{\numseqtimesteps}$ according to user inputs. The animation model allows users to specify conditioning signals, \change{or prompts,} in two forms. First, conditional signals can take the form of values that the user wants to impose on some object properties in the sequence, such as the player position at a certain time step.
This signal is represented by a sequence $\vecsequencecond \in \mathcal{\setstate}^{\numseqtimesteps}$. This form of conditioning allows fine control over the sequence to generate but requires directly specifying values of properties. Second, to allow high-level, yet granular control over the sequence, we introduce actions in the form of text $\vectextactioncond \in {\settext}^{\numactions \times \numseqtimesteps}$ that specify the behavior of each of the $\numactions$ actionable objects at each timestep in the sequence, where $\settext$ is the set of all strings of text.
To maximize the flexibility of the framework, we consider all values in $\vecsequencecond$ and $\vectextactioncond$ to be optional, thus we introduce their respective masks $\vecmask \in \{0,1\}^{\numproperties \times \numseqtimesteps}$ and $\vecmaskaction \in \{0,1\}^{\numactions \times \numseqtimesteps}$ that are set to 1 when the respective conditioning signal is present. We assume elements where the mask is not set to be equal to 0. 
The animation model predicts $\vecsequencepred \in \mathcal{\setstate}^{\numseqtimesteps}$ conditioned on $\vecsequencecond$ and $\vectextactioncond$ such that:
\begin{equation}
\label{eq:sequence_composition}
\vecsequence = \vecsequencepred + \vecsequencecond,
\end{equation}
where we consider the entries in $\vecsequencepred$ and $\vecsequencecond$ to be mutually exclusive, \emph{i.e.} an element of $\vecsequencepred$ is 0 if the corresponding conditioning signal in $\vecsequencecond$ is present according to $\vecmask$. Note that the prediction of actions is not necessary, since $\vecsequence$ is sufficient for rendering.
 
 We adopt a temporal model $\netanimation$ based on a non-autoregressive masked transformer design and leverage the knowledge of a pretrained language model in a text encoder $\nettext$ to model action conditioning information \cite{han2022show}. The masked design provides support for the optional conditioning signals and is trained using masked sequence modeling, where we sample $\vecmask$ and $\vecmaskaction$ according to various strategies that emulate desired inference tasks.
 

In Sec.~\ref{sec:text_encoder} we define our text encoder, Sec.~\ref{sec:temporal_model} defines the diffusion backbone, and in Sec.~\ref{sec:animation_training} we describe the training procedure.

\subsubsection{Text Encoder}
\label{sec:text_encoder}

We introduce a text encoder $\nettext$ that encodes textual actions into a sequence of fixed-size text embeddings:
\begin{equation}
\label{eq:text_encoder}
\vecactionemb = \nettext(\vectextactioncond) \in \mathbb{R}^{\numactions \times \numseqtimesteps \times \numactionembsize},
\end{equation}
where $\numactionembsize$ is the size of the embedding for the individual sentence. Given a textual action, we leverage a pretrained T5 text model \cite{raffel2022exploring} $\nettextenc$ that tokenizes the sequence and produces an output feature for each token. Successively, a feature aggregator $\nettextagg$ modeled as a transformer encoder \cite{vaswani2017attention} produces the aggregated text embedding from the text model features. To retain existing knowledge into $\nettextenc$, we keep it frozen and only train the feature aggregator $\nettextagg$.

\subsubsection{Temporal Modeling}
\label{sec:temporal_model}

In this section, we introduce the temporal model $\netanimation$ that predicts the sequence of states $\vecsequence$ conditioned on known state values $\vecsequencecond$, action embeddings $\vecactionemb$, and the respective masks $\vecmask$ and $\vecmaskaction$. Since only unknown state values need to be predicted, the model predicts $\vecsequencepred$ and the complete sequence of states is obtained as $\vecsequence = \vecsequencepred + \vecsequencecond$, following Eq.~\eqref{eq:sequence_composition}. Diffusion models have recently shown state-of-the-art performance on several tasks closely related to our setting such as sequence modeling \cite{tashiro2021csdi} and text-conditioned human motion generation \cite{dabral2022mofusion,zhang2022motiondiffuse}. Thus, we follow the DDPM \cite{ho2020ddpm} diffusion framework, and we frame the prediction of $\vecsequencepred=\vecsequencepred_0$ as a progressive denoising process $\vecsequencepred_0,...,\vecsequencepred_\numdifftimesteps$, where we introduce the diffusion timestep index $\difftimestep \in {0,...,\numdifftimesteps}$. The temporal model $\netanimation$ acts as a noise estimator that predicts the Gaussian noise $\vecdiffnoise_\difftimestep$ in the noisy sequence of unknown states $\vecsequenceprednoise_\difftimestep$ at diffusion timestep $\difftimestep$:
\begin{equation}
\label{eq:temporal_model}
\vecdiffnoisepred_\difftimestep = \netanimation(\vecsequenceprednoise_\difftimestep|\vecsequencecond,\vecactionemb,\vecmask,\vecmaskaction,\difftimestep).
\end{equation}
An illustration of the proposed diffusion model is shown in Fig.~\ref{fig:architecture_animation_model}. 

We realize $\netanimation$ using a transformer encoder \cite{vaswani2017attention}. To prepare the transformer's input sequence, we employ linear projection layers $\netprojection$ with separate parameters for each object property. Since corresponding entries in $\vecsequenceprednoise_\difftimestep$ and $\vecsequencecond$ are mutually exclusive, we only consider the one that is present as input to the transformer and we employ different projection parameters to enable the model to easily distinguish between the two.
An analogous projection is performed for $\vecactionemb$ and, subsequently, the projection outputs for states and actions are concatenated into a single sequence $\vecsequenceemb \in \mathbb{R}^{\numproperties+\numactions \times \numseqtimesteps \times \numembsize}$, which constitutes the input to the transformer.
An output projection layer with separate weights for each object property produces the prediction $\vecdiffnoisepred_\difftimestep$ at the original dimensionality. 
To condition the model on the diffusion time-step $\difftimestep$, we introduce a weight demodulation layer \cite{karras2019stylegan2} after each self-attention and feedforward block \cite{zhang2022motiondiffuse}. 

To model long sequences while keeping reasonable computational complexity and preserving the ability to model long-term relationships between sequence elements, it is desirable to build the sequences using states sampled at a low framerate. However, this strategy would not allow the model to generate content at the original framerate and would prevent it from understanding dynamics such as limb movements that are clear only when observing sequences sampled at high framerates. To address this issue, we use the weight demodulation layers to further condition our model on the sampling framerate $\framerate$ to enable a progressive increase of the framerate at inference time (see \apref{ap:high_framerate_generation}).


\subsubsection{Training}
\label{sec:animation_training}
To train our model, we sample a sequence $\vecsequence$ with corresponding actions $\vectextaction$ from a video in the dataset at a uniformly sampled framerate $\framerate$. Successively, we obtain masks $\vecmask$ and $\vecmaskaction$ according to masking strategies we detail in \apref{ap:training_details_animation}. The sequence for training are obtained following $\vecsequencepred_0 = \vecsequence \odot (1-\vecmask)$ and $\vecsequencecond = \vecsequence \odot \vecmask$, and actions as $\vectextactioncond = \vectextaction \odot \vecmaskaction$, where %$\vectextaction$ denote ground truth actions and 
$\odot$ denotes the Hadamard product.

We train our model by minimizing the DDPM \cite{ho2020ddpm} training objective:
\begin{equation}
\label{eq:diffusion_loss}
\mathbb{E}_{\difftimestep \sim \mathcal{U}(1,\numdifftimesteps),\epsilon \sim \mathcal{N}(0,I)}||\vecdiffnoisepred_\difftimestep - \vecdiffnoise_\difftimestep||,
\end{equation}
where $\vecdiffnoisepred_\difftimestep$ is the noise estimated by the temporal model $\netanimation$ according to Eq.~\eqref{eq:temporal_model}.
Note that the loss is not applied to positions in the sequence corresponding to conditioning signals \cite{tashiro2021csdi}.

Our model is trained using the Adam \cite{kingma2014adam} optimizer with a learning rate of $1e-4$, cosine schedule, and with 10k warmup steps. We train the model for a total of 2.5M steps and a batch size of 32. We set the length of the training sequences to $\numseqtimesteps=16$. The number of diffusion timesteps is set to $\numdifftimesteps=1000$ and we adopt a linear noise schedule \cite{ho2020ddpm}. Additional details are presented in \apref{ap:implementation_details_animation} \change{and \apref{ap:inference_details}}.
