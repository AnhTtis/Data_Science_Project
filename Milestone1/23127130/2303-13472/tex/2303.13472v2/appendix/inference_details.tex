\section{Inference Details}
\label{ap:inference_details}

\subsection{Inference Speed}
\label{ap:inference_speed}
Our synthesis model renders images at 2.96fps over a single A100 GPU. We can parallelize inference by generating batches of 8 consecutive frames on separate GPUs for 23.7fps.
The animation model \change{has a throughput} of 1.08fps using 1000 diffusion sampling timesteps, measured by dividing the number of generated frames by the computation time at the end of the diffusion process. Meng \etal \cite{meng2022on} show that a reduction to 16 timesteps is possible with no or minimal loss in quality for a projected performance of 67.5fps. Hence, we believe our framework can be made real-time, which is a scope for future works. 

\subsection{Animation Model Inference Details}
\label{ap:animation_inference}

At inference time, the user is presented with a fully-masked, empty sequence $\vecsequencecond=0$, $\vecmask=0$, $\vectextactioncond=``"$, $\vecmaskaction=0$. Any object property can be specified as a conditioning signal in $\vecsequencecond$ and text action descriptions for any sequence timesteps can be provided in $\vectextactioncond$, with masks updated accordingly. The desired framerate $\framerate$ is also specified.

The text encoder $\nettext$ produces text embeddings $\vecactionemb$ as in Eq.~\eqref{eq:text_encoder}. Successively, the \emph{reverse process} is started at diffusion time $\difftimestep=\numdifftimesteps$, with $\vecsequenceprednoise_{\numdifftimesteps}$ sampled from the normal distribution. The DDPM sampler \cite{ho2020ddpm} queries the temporal model 
according to Eq.~\eqref{eq:temporal_model} to progressively denoise $\vecsequenceprednoise_\difftimestep$ and obtain the predicted sequence $\vecsequencepred=\vecsequenceprednoise_0$. The final sequence is obtained as $\vecsequence = \vecsequencepred + \vecsequencecond$, following Eq.~\eqref{eq:sequence_composition}.

\subsubsection{High Framerate Generation}
\label{ap:high_framerate_generation}
To produce sequences at the dataset framerate, we devise a two-stage sampling procedure designed to prevent an excessive increase in the sequence length. In the first stage, we sample the desired sequence at a low framerate $\framerate_1$. In the second stage, we exploit the masking mechanism and framerate conditioning to increase the framerate and, consequently, the length of the generated sequence. After the first stage, we consider a higher framerate $\framerate_2$ and extend the sampled sequence $\vecsequence$ with new states between existing ones, that we call keyframes, until the sequence length corresponding to $\framerate_2$ is reached. This sequence constitutes the new $\vecsequencecond$. Any previous action conditioning is copied in a new $\vectextactioncond$ in the corresponding keyframe locations. Masks are updated to be 1 in the position of the keyframes and 0 elsewhere. The sampling process is then repeated with the new conditioning signals and a sequence $\vecsequence$ is produced at the final framerate $\framerate_2$. To avoid an explosion in the length of the sequence, we exploit keyframes to divide the sequence into shorter chunks beginning and terminating at a keyframe, and sampling is performed separately on each chunk.

\subsubsection{Autoregressive Generation}
\label{ap:augoregressive_generation}

Our masking mechanism can be used to produce predictions autoregressively, enabling long sequence generation. Autoregressive generation can be obtained by considering a sequence $\vecsequencecond$ and removing the states corresponding to the first $t$ timesteps. $t$ timesteps are then added at the end of the sequence and a mask $\vecmask$ is created to zero out these additional $t$ steps. Conditioning signals can then be specified as desired for the last $t$ timesteps. When sampling $\vecsequencepred$, a prediction is produced for the additional timesteps and the procedure can be repeated.