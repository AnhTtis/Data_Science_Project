\section{Deformation Modeling}
\label{ap:deformation_modeling}
In this section, we present additional details on the deformation model $\netdeformation$ used to render articulated objects such as humans. Given an articulated object, we assume its kinematic tree is known and that the transformation $[\tensrotation_j|\vectranslation_j]$ from each joint $j \in 1,...,\numjoints$ to the parent joint is part of the object's properties. From these we can follow the kinematic tree to derive transformations $[\tensrotation'_j|\vectranslation'_j]$ for each joint from the bounding box coordinate system to the canonical coordinate system. Intuitively, these transformations represent how to map a point $\vecpointbbox$ in the bounding box coordinate system belonging to the joint $j$ to the corresponding point $\vecpointcanon$ in the canonical space.

We implement a deformation procedure based on linear blend skinning (LBS) \cite{lewis2000pose} that establishes correspondences between points in the canonical space $\vecpointcanon$ and in the deformed bounding box space $\vecpointbbox$ by introducing blending weights $\vecblendweights$ for each point in the canonical space. These weights can be interpreted as the degree to which that point moves according to the transformation associated with that joint.
\begin{equation}
\label{eq:lbs}
\vecpointbbox = \sum_{j=1}^{\numjoints} w_j(\vecpointcanon)\left(\tensrotation_j^{\prime-1}  \vecpointcanon - \tensrotation_j^{\prime-1} \vectranslation'_j\right).
\end{equation}

During volumetric rendering, however, we sample points $\vecpointbbox$ in the bounding box space and query the canonical volume in the corresponding canonical space point $\vecpointcanon$. Doing so requires solving Eq.~\eqref{eq:lbs} for $\vecpointcanon$, which is prohibitively expensive \cite{li2022tava}. Inspired by HumanNeRF \cite{weng2022humannerf}, instead of modeling LBS weights $\vecblendweights$, we introduce inverse linear blending weights $\vecblendweights^b$:
\begin{equation}
\label{eq:invlbs}
    \vecblendweights_j^b(\vecpointbbox) = \frac{\vecblendweights_j(\tensrotation'_j \vecpointbbox + \vectranslation'_j)}{\sum_{j=1}^{\numjoints} \vecblendweights_j(\tensrotation'_p \vecpointbbox + \vectranslation'_j)}.
\end{equation}
such that the canonical point can be approximated as:
\begin{equation}
\label{eq:lbs_hnerf}
\vecpointcanon = \sum_{j=1}^{\numjoints} \vecblendweights_j^b(\vecpointbbox)\left(\tensrotation'_j \vecpointbbox + \vectranslation'_j\right).
\end{equation}


We parametrize the function $\vecblendweights$ mapping spatial locations in the canonical space to blending weights as a neural network. Similarly to $\netcanonical$, we employ 3D convolutions to map a fixed noise volume $\tensblendweightssmall \in \mathbb{R}^{\numfieldfeatsmall \times \numblendweightsheightsmall \times \numblendweightswidthsmall \times 
\numblendweightsdepthsmall}$ to a volume of blending weights $\tensblendweights \in \mathbb{R}^{\numjoints + 1 \times \numblendweightsheight \times \numblendweightswidth \times 
\numblendweightsdepth}$, where each channel represents the blending weights for each part, with an extra weight modeling the background. The volume channels are normalized using softmax, so that they sum to one, and can efficiently be queried using trilinear sampling.
To facilitate convergence, we exploit the known kinematic tree to build a prior over the blending weights that increases blending weights in the area surrounding each limb \cite{weng2022humannerf}.