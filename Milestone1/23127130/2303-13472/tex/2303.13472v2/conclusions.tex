\subsection{Limitations}
\label{ap:limitations}

Since the model is trained on a dataset showing only plausible actions, the model's behavior is not defined when an \emph{implausible} action is specified, such as hitting a ball while moving in the wrong direction to intercept it or jumping on a pillar that is out of reach. In these cases, we find the model to ignore the implausible part of the command and produce the closest plausible command or, less frequently, to produce implausible outcomes such as irrealistic long jumps (see Fig.~\ref{fig:impossible_actions}). In addition, the model does not generate actions extremely out of distribution such as performing a backflip or doing a push-up. \change{This aspect could be addressed by jointly training the animation model on multiple diverse datasets, which we consider an interesting future direction.}

While our Tennis dataset contains varied text annotations that allow the model to generalize to text inputs with varied structure, our Minecraft dataset's synthetic text annotations are less varied and the fixed synthetic structure of sentences tends to be memorized, making the model less effective if a different syntax is used (see Sec.~\ref{ap:language_robustness}). To address this issue, a more sophisticated algorithm can be employed to generate action annotation on the Minecraft dataset.

Our model learns to associate referential language to scene coordinates rather than the appearance of the referred object, and the model memorizes the position of contact surfaces. While tennis scenes always have the same structure, for Minecraft the model cannot generalize to different scenes. This concern can be addressed by conditioning the animation model on the scene's geometry, which we leave as future work.

\change{We find our animation model to overfit to the Tennis dataset when less than 60\% of the training data is used (see \apref{ap:animation_model_dataset_size_ablation}). We leave as an interesting avenue of future work the investigation of regularization techniques such as dropout or weight decay that have the potential to reduce overfitting in this scenario.}

Our animation model outperforms baselines that operate under the same data assumptions \cite{Menapace2022PlayableEnvironments} in terms of animation quality. With respect to recent character animation methods \cite{starke2019neural,starke2020local,holden2020learned} making use of richly annotated motion capture data and dataset-specific handcrafted optimizations (see Sec.~\ref{sec:character_animation}), our method demonstrates more advanced game dynamics and game AI modeling capabilities, but produces foot sliding artifacts. We expect continuous improvements in diffusion models to alleviate such artifacts and expect further improvements by considering different parametrizations of pose parameters taking into consideration the distance of limbs from the terrain, which we will explore in future work.

Lastly, our animation model does not yet produce results in real-time. We discuss inference speed and strategies to make the model real-time in \apref{ap:inference_speed}. Improving the sampling speed of diffusion models is an actively investigated problem \cite{salimans2022progressive,meng2022on,song2021denoising} that is orthogonal to ours.

\section{Conclusions}

In this paper, we demonstrate the feasibility of learning \change{game models able to answer challenging user prompts} and show that textual action representations are critical for unlocking fine-grained control over the generation process, and enabling compelling constraint- and goal-driven generation applications. 
These results, jointly with two richly-annotated text-video datasets, pave the way towards learning game models for complex, real-world scenes.


\section{Acknowledgements}
We would like to thank Christian Theobalt for his feedback on the manuscript draft, Denys Poluyanov, Eugene Shevchuk and Oleksandr Pyshchenko for the useful discussion and validation of the use cases of PGMs, Maryna Diakonova for her support in data labeling, and Anton Kuzmenko and Vadym Hrebennyk for their assistance in creating the accompanying video.

This work was partially supported by the EU HEU AI4TRUST (101070190) project.