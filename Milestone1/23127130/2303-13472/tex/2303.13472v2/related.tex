\section{Related Work} 

Our \change{Promptable Game Model} relates to neural game simulation literature, game engines, character animation, neural rendering, sequential data generation, and text-based generation.
We review the most recent related works in this section.

\subsection{Neural video game simulation}
\label{sec:neural_game_simulation}
In the last few years, video game simulation using deep neural networks has emerged as a new research trend ~\cite{Kim2020_GameGan,kim2021drivegan,menapace2021pvg,Menapace2022PlayableEnvironments,davtyan2022glass,huang2022layered}.  \change{The objective is to train a neural network to synthesize videos based on a specific type of prompt: a sequence of actions provided at every time step.}

This problem was first addressed using training videos annotated with the corresponding action labels at each time step~\cite{oh2015action,chiappa2017recurrent,Kim2020_GameGan}. They consider a discrete action representation that is difficult to define a priori for real-world environments. More recently, \cite{kim2021drivegan} proposed a framework that uses a continuous action representation to model real-world driving scenarios. Devising a good continuous action representation for an environment, however, is complex.
To avoid this complexity, \cite{menapace2021pvg, Menapace2022PlayableEnvironments} propose to learn a discrete action representation. \cite{huang2022layered} expands on this idea by modeling actions as a learned set of geometric transformations, while \cite{davtyan2022glass} represents actions by separating them into a global shift component and a local discrete action component.

\change{Differently from our PGM}, previous works perform generation in an autoregressive manner, conditioned on the actions and, therefore, are unable to answer prompts entailing constraint- or goal-driven generation for which non-sequential conditioning is necessary. We find the proposed text-based action representation and masked training procedure to be crucial to unlocking such applications.

Among these works, \emph{Playable environments}~\cite{Menapace2022PlayableEnvironments} is the most closely related to ours. Rather than employing a 2D model, they use a NeRF-based renderer~\cite{mildenhall2020nerf} that enables them to represent complex 3D scenes. \change{We follow this high-level design but introduce a more efficient plane- and voxel-based NeRF representation that enables the rendering of outputs at double the original resolution without the use of upsampling modules which we found to be the cause of checkerboard artifacts, failures in rendering of small objects and to be prone to failure when training at higher resolutions.}
\change{In addition,} the employed discrete action representation shows limitations in complex scenarios such as tennis, where it is only able to capture the main movement directions of the players and does not model actions such as ball hitting. In contrast, we employ a text action representation that specifies actions at a fine level of granularity (i.e. which particular ball-hitting action is being performed and where the ball is sent), while remaining interpretable and intuitive for the user. \change{Lastly, we replace the adversarially-trained LSTM animation module with a more capable masked diffusion transformer.}


\subsection{Game Engines}
\label{sec:game_engines}
Game engines brought a revolution to game development by providing extensible and reusable software that can be employed to create a wide range of game models \cite{gregory2018gameengine}. Nowadays, a range of game engines exists (\emph{Unity}, \emph{Unreal}, \emph{id Tech}, \emph{Source}, \emph{CRYENGINE}, \emph{Frostbite}, \emph{RAGE}) and have grown to become vast software ecosystems. Modern game engines are organized into components including a rendering engine \cite{muller2020ncv}, a resource manager, a module for physics and collision, an animation manager and, importantly, a gameplay foundation system that models the game rules and encapsulates game AI functionalities \cite{gregory2018gameengine}. \change{The presence of these components, coupled with the labor} of a range of trained experts including software engineers, artists (animators, 3D modelers, texture and lighting artists) and game developers, \change{enables the construction of sophisticated game models supporting low-level character control and scripted agent behavior. We show that monocular videos annotated with a fraction of the effort (see \apref{ap:cost_quality}) can be used to learn models of games that support answering challenging prompts related to agent intelligence, a capability difficult to achieve through scripted agent behavior.}

    
\subsection{Character Animation}
\label{sec:character_animation}

Character animation is a long-standing problem in computer graphics. 
Several recent methods have been proposed that produce high-quality animations. Holden \etal \cite{holden2020learned} propose a learnable version of Motion Matching \cite{buttner2019motion} that formulates character animation as retrieval of the closest motion from a motion database and supports interaction with other characters or objects.
Other approaches model the evolution of characters using time series models conditioned on the preceding state and control signals \cite{starke2019neural,starke2020local,lee2018interactive,holden2017phase}. Starke \etal \cite{starke2019neural} propose a model based on a mixture of experts that controls character locomotion and object interactions, in a follow-up work \cite{starke2020local} they introduce local motion phases to model complex character motions and interaction with a second character.

To produce high-quality animations the methods rely on difficult-to-acquire motion capture data enriched with contact information \cite{holden2020learned,starke2019neural,starke2020local}, motion phases \cite{starke2019neural} or engineered action labels \cite{starke2019neural,starke2020local}. Additionally, handcrafted dataset-specific feature representations and mappings from user controls to such representations are often leveraged, and additional knowledge is injected through postprocessing steps such as inverse kinematics or external physics models. While these assumptions promote high-quality outputs, they come at a significant effort. In contrast, our method sidesteps these requirements by not using motion capture and basing user control on natural language that is cheaper to acquire (see \apref{ap:cost_quality}) and does not require manual engineering. Finally, character animation methods support limited goal-driven control such as interacting with a specific object while avoiding collisions \cite{starke2019neural}. In contrast, our method models complex game AI tasks such as modeling strategies to defeat the opponent, which are instrumental \change{in answering complex user prompts.}

    
 \subsection{Neural Rendering}
 \label{sec:neural_rendering}

Neural rendering was recently revolutionized by the advent of NeRF \cite{mildenhall2020nerf}. Several modifications of the NeRF framework were proposed to model deformable objects \cite{tretschk2021nonrigid,park2021nerfies,weng2022humannerf,park2021hypernerf,li2022tava}, and decomposed scene representations \cite{Menapace2022PlayableEnvironments,niemeyer2021giraffe,mueller2022autorf,Ost_2021_CVPR,kundu2022panoptic}. In addition, several works improved the efficiency of the original MLP representation of the radiance field \cite{mildenhall2020nerf} by employing octrees \cite{yu2021plenoctrees,martel2021acorn}, voxel grids \cite{fridovich2022plenoxels}, triplanes \cite{chan2022eg3d}, hash tables \cite{mueller2022instant}, or factorized representations \cite{chen2022tensorf}.

Our framework is most related to that of \cite{weng2022humannerf}, since we model player deformations using an articulated 3D prior and linear blend skinning (LBS) \cite{lewis2000pose}. Differently from them, however, we consider scenes with multiple players and apply our method to articulated objects with varied structures for their kinematic trees. 
While similar to the rendering framework of \cite{Menapace2022PlayableEnvironments}, our framework does not adopt computationally-inefficient MLP representations, using voxel \cite{fridovich2022plenoxels} or plane representations instead, \change{thus does not rely on upsampler networks.}


\subsection{Sequential data generation with diffusion models}
In prior work, sequential data generation was mainly addressed with auto-regressive formulations combined with adversarial~\cite{kwon2019predicting} or variational~\cite{fortuin2020gp,babaeizadeh2018stochastic} generative models. Recently, diffusion models have emerged as a promising solution to this problem leading to impressive results in multiple applications such as audio~\cite{kong2020diffwave,lam2022bddm,chen2021wavegrad,leng2022binauralgrad} and video synthesis~\cite{ho2022video,ho2022imagenvideo,singer2022makeavideo,blattmann2023videoldm}, language modeling~\cite{dieleman2022cdcd}, and human motion synthesis \cite{zhang2022motiondiffuse,dabral2022mofusion}.
Following this methodological direction~\cite{tashiro2021csdi}, introduces a score-based diffusion model for imputing missing values in time series. They introduce a training procedure based on masks that simulate missing data. This approach motivates our choice of a similar masking strategy to model the conditions entailed by the given prompt and generate the unknown environment states. In this work, we show that mask-based training is highly effective in modeling geometric properties together with textual data modalities. 


\subsection{Text-based generation}
In recent years, we have witnessed the emergence of works on the problem of text-based generation.
Several works address the problem of generating images \cite{saharia2022imagen,rombach2021highresolution,ramesh2021zeroshot,ramesh2022hierarchical} and videos with arbitrary content~\cite{ho2022video,ho2022imagenvideo,singer2022makeavideo,hong2022cogvideo}, and arbitrary 3D shapes \cite{jain2021dreamfields,lin2022magic3d,achlioptas2023shapetalk}.

Han \emph{et al.} \cite{han2022show} introduced a video generation framework that can incorporate various conditioning modalities in addition to text, such as segmentation masks or partially occluded images. Their approach employs a frozen RoBERTa \cite{liu2020roberta} language model and a sequence masking technique. Fu \emph{et al.} \cite{fu2022tellmewhathappened} propose an analogous framework. Our animation framework employs a similar masking strategy, but we model text conditioning at each timestep in the sequence, use diffusion models which operate on continuous rather than discrete data, and generate scenes that can be rendered from arbitrary viewpoints. 

More relevant to our work, several papers introduced models to generate human motion sequences from text \cite{guy2022motionclip,TEACH}. 
Recently, diffusion models have shown strong performance on this task \cite{zhang2022motiondiffuse,dabral2022mofusion}. In these works, sequences of human poses are generated by a diffusion model conditioned on the output of a frozen CLIP text encoder. It is worth noting that these prior works model only a single human, while our framework supports multiple human agents and objects, and models their interactions with the environment.

 