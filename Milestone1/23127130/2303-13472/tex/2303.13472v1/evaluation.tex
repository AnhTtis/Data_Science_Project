
\section{Evaluation}
\label{sec:evaluation}
In this section, we introduce our Tennis and Minecraft datasets (Sec.~\ref{sec:datasets}), describe our experimental protocol (Sec.~\ref{sec:evaluation_protocol}), and perform evaluation of both the synthesis model (Sec.~\ref{sec:synthesis_evaluation}) and the animation model (Sec.~\ref{sec:animation_evaluation}).

\begin{figure*}
\includegraphics[width=\textwidth]{resources/synthesis_ablation_qualitatives.pdf}
  \caption{Synthesis model qualitative results on the Tennis dataset. Compared to PE \cite{Menapace2022PlayableEnvironments}, our model generates sharper players and static scene elements. Our ablation study shows corruption of the player geometry when voxels or our deformation model are not used. When removing our canonical plane representation, static scene elements appear blurry. When our feature enhancer is removed, the model does not generate shadows and players lose quality. We show video samples on the \website.}
  \label{fig:synthesis_ablation_qualitatives}
\end{figure*}


\subsection{Datasets}
\label{sec:datasets}
We collect two datasets to evaluate our method. Both datasets and the employed data collection tools are publicly available. In the following, we describe their structure and the available annotations.

\subsubsection{Tennis dataset}
We collect a dataset of broadcast tennis matches starting from the videos in \cite{Menapace2022PlayableEnvironments}. The dataset depicts matches between two professional players from major tennis tournaments, captured with a single, static bird's eye camera.

To enable the construction of LGEs, we collect a wide range of annotations with a combination of manual and automatic methods (see \apref{ap:tennis_dataset_collection}):
\begin{itemize}
\item For each frame, we perform camera calibration.
\item For each of the two players, we perform tracking and collect full SMPL \cite{loper2015smpl} body parameters. Note that in our work we only use a subset of the parameters: rotation and translation associated with each joint, and the location of the root joint in the scene. 
\item For each player and frame, we manually annotate textual descriptions of the action being performed. We structure captions so that each includes information on where and how the player is moving, the particular type of tennis shot being performed, and the location where the shot is aimed (see \apref{ap:dataset_samples}). Captions make use of technical terms to describe shot types and field locations. In contrast to other video-text datasets that contain a single video-level \cite{bain2021webvid10m} or high-level action descriptions weakly aligned with video content \cite{miech19howto100m}, the captions in our dataset are separate for each object and constitute a fine-grained description of the actions taking place in the frame.
\item For the ball, we perform 3D tracking and provide its position in the scene and its velocity vector indicating the speed and direction of movement. 
\end{itemize}

We collect 7112 video sequences in 1920x1080px resolution and 25fps starting from the videos in \cite{Menapace2022PlayableEnvironments} for a total duration of 15.5h. The dataset features 1.12M fully-annotated frames and 25.5k unique captions with 915 unique words. We highlight key statistics of the dataset show samples in \apref{ap:datasets}.


We note that broadcast Tennis videos are monocular and do not feature camera movements other than rotation, thus the dataset does not make it possible to recover the 3D geometry of static objects \cite{Menapace2022PlayableEnvironments}. 

\subsubsection{Minecraft dataset}
We collect a synthetic dataset from the Minecraft video game. This dataset depicts a player performing a series of complex movements in a static Minecraft world that include walking, sprinting, jumping, and climbing on various world structures such as platforms, pillars, stairs, and ladders. A single, monocular camera that slowly orbits around the scene center is used to capture the scenes. We collect a range of synthetic annotations using a game add-on we develop starting from \cite{replaymod}:
\begin{itemize}
\item Camera calibration for each frame.
\item Player rotation and translation parameters associated with each joint in the Minecraft kinematic tree format, and the location of the root joint in the scene (see \apref{ap:minecraft_skeleton_format}).
\item A synthetically-generated text caption describing the action being performed by the player. We assign varied, descriptive names to each element of the scene and build captions that describe scene elements or directions towards which the player is moving. Additionally, our captions capture how movement is happening i.e. by jumping, sprinting, walking, climbing, or falling. We adopt a stochastic caption generation procedure that generates multiple alternative captions for each frame.
\end{itemize}

A total of 61 videos are collected in 1024x576px resolution and 20fps for a total duration of 1.21h. The dataset contains 68.5k fully annotated frames and 1.24k unique captions with 117 unique words. We highlight key statistics for the dataset in \apref{ap:datasets}.

\subsection{Evaluation Protocol}
\label{sec:evaluation_protocol}
We evaluate the synthesis and the animation models separately, following a similar evaluation protocol. We divide the test dataset into non-overlapping sequences of 16 frames sampled at 5fps and 4fps respectively for the Minecraft and Tennis datasets and make use of the synthesis or animation model to reconstruct them. In the case of the synthesis model, we directly reconstruct the video frames and compute the following metrics:
\begin{itemize}
\item \textit{LPIPS} \cite{zhang2018unreasonable} is a standard metric for evaluating the reconstruction quality of the generated images
\item \textit{FID} \cite{heusel2017advances} is a widely-used metric for image generation quality
\item \textit{FVD} \cite{unterthiner2018towards} is a standard metric for assessing the quality of generated videos
\item \textit{Average Detection Distance (ADD)} \cite{menapace2021pvg} measures the average distance in pixels between the bounding box centers of ground truth bounding boxes and bounding boxes obtained from the generated sequences through a pretrained detector
\cite{ren2015faster}
\item \textit{Missing Detection Rate (MDR)} \cite{menapace2021pvg}  estimates the rate of bounding boxes that are present in the ground truth, but that are missing in the generated videos
\end{itemize}

For the animation model, we evaluate reconstruction of the object properties. Note that different strategies for masking affect the behavior of the model and the nature of the reconstruction task, thus we separately evaluate different masking configurations corresponding to different inference tasks. We compute metrics that address both the fidelity of the reconstruction and the realism of the produced sequences:
\begin{itemize}
\item \textit{L2} computes the fidelity of the reconstruction by measuring the distance between the ground truth and reconstructed object properties along the sequence
\item \textit{Fréchet Distance (FD)} \cite{frechet1957distance} measures the realism of each object property by computing the Fréchet Distance between the distribution of real sequences of a certain object property and of generated ones.
\end{itemize}

We select different reconstruction tasks for evaluation:
\begin{itemize}
\item \textit{Video prediction conditioned on actions} consists in reconstructing the complete sequence starting from the initial state while the actions are specified for all timesteps. This setting corresponds to the evaluation setting of \cite{Menapace2022PlayableEnvironments}.
\item \textit{Unconditioned video prediction} consists in reconstructing the complete sequence starting from the first state only.
\item \textit{Opponent modeling} consists in reconstructing the object properties of an unknown player, based on the state of the other player, with actions specified only on the known player. Good performance in this task indicates the ability to model an opponent against which a user can play.
\item \textit{Sequence completion} consists in reconstructing a sequence where 8 consecutive states are missing. No actions are specified for the missing states. Good performance in this task indicates ability in reasoning on how it is possible to reach a certain goal state starting from the current one.
\end{itemize}

\subsection{Synthesis Model Evaluation}
\label{sec:synthesis_evaluation}
In this section, we evaluate the performance of the synthesis model.


\subsubsection{Comparison to Baselines}
We evaluate our method against Playable Environments (PE) \cite{Menapace2022PlayableEnvironments}, the work most related to ours in that it builds a controllable 3D environment representation that is rendered with a compositional NeRF model where the position of each object is given and pose parameters are treated as a latent variable. Since the original method supports only outputs at 512x288px resolution, we produce baselines trained at both 512x288px and 1024x576px resolution which we name PE and PE+ respectively. For a fair comparison, we also introduce in the baselines our same mechanism for representing ball blur and train a variant of our model using the same amount of computational resources as the baselines (Ours Small).

Results of the comparison are shown in Tab.~\ref{table:synthesis_merged}, while qualitative results are shown in Fig.~\ref{fig:synthesis_ablation_qualitatives}. 
Our method scores best in terms of LPIPS, ADD and MDR. Compared to PE+, our method produces significantly better FID and FVD scores. As shown in Fig.~\ref{fig:synthesis_ablation_qualitatives}, PE and PE+ produce checkerboard artifacts that are particularly noticeable on static scene elements such as judge stands, while our method produces sharp details. We attribute this difference to our ray sampling scheme and feature enhancer design that, in contrast to PE, do not sample rays at low resolution and perform upsampling, but rather directly operate on high resolution. In addition, thanks to our deformation and canonical space modeling strategies, and higher resolution, our method produces more detailed players with respect to PE, where they frequently appear with missing limbs and blurred clothing. Finally, our model produces a realistic ball, while PE struggles to correctly model small objects, presumably due to its upsampling strategy that causes rays to be sampled more sparsely and thus do not intersect with the ball frequently enough to correctly render its blur effect. We show video results for both the Tennis and Minecraft datasets on the \website.

\input{tables/table_synthesis_merged}

\subsubsection{Ablation}
To validate our design choices, we produce several variations of our method, each produced by removing one of our proposed architectural elements: we remove the enhancer $\netenhancer$ and directly consider $\tensimagergb$ as our output; we remove the explicit deformation modeling procedure in $\netdeformation$ of Sec.~\ref{sec:deformation_modeling} and substitute it with an MLP directly predicting the deformation using a learnable pose code as in \cite{Menapace2022PlayableEnvironments,tretschk2021nonrigid}; we remove the plane-based canonical volume representation in $\netcanonical$ for planar objects and use an MLP instead; we remove the voxel-based volume representation in $\netcanonical$ and use an MLP instead; we substitute our style encoder $\netstyle$ with an ad-hoc encoder for each object in the scene, following \cite{Menapace2022PlayableEnvironments}.

We perform the ablation on the Tennis dataset and show results in Tab.~\ref{table:synthesis_merged} and Fig.~\ref{fig:synthesis_ablation_qualitatives}. To reduce computation, we train the ablation models using the same hyperparameters as the ``Ours Small'' model.

When removing the enhancer $\netenhancer$, our model produces players with fewer details and does not generate shadow effects below players (see first row in Fig.~\ref{fig:synthesis_ablation_qualitatives}). When our deformation modeling procedure is not employed, the method produces comparable LPIPS, FID, and FVD scores, but an analysis of the qualitatives shows that players may appear with corrupted limbs (see last row in Fig.~\ref{fig:synthesis_ablation_qualitatives}). In addition, the use of such learned pose representation would reduce the controllability of the synthesis model with respect to the use of an explicit kinematic tree. When plane-based or voxel-based canonical modeling is removed, we notice artifacts in the static scene elements, such as corrupted logos, and in the players, such as detached or doubled limbs. Finally, when we replace our style encoder design with the one of \cite{Menapace2022PlayableEnvironments}, we notice fewer details in scene elements.

\subsection{Animation Model Evaluation}
\label{sec:animation_evaluation}
In this section, we evaluate the performance of the animation model.

\begin{figure}
\includegraphics{resources/animation_ablation_qualitatives_tennis}
  \caption{Qualitatives results on the Tennis dataset. Sequences are produced in a video prediction setting that uses the first frame object properties and all actions as conditioning. The location of players is consistently closer to the ground truth for our method. Our method captures the multimodal distribution of player poses and generates vivid limb movements, while the baselines produce poses as the average of the distribution, resulting in reduced limb movement and tilted root joints. Additional samples are shown in \apref{ap:animation_evaluation} and in the \website.}
  \label{fig:animation_ablation_qualitatives_tennis}
\end{figure}


\subsubsection{Comparison to Baselines}
\label{sec:animation_baselines}
Similarly to the synthesis model, we compare our animation model against the one of Playable Environments (PE) \cite{Menapace2022PlayableEnvironments}, the most related to our work since it operates on a similar environment representation. While the baseline jointly learns discrete actions and generates sequences conditioned on such actions, we assume the text action representations to be available in our task, so, for fairness of evaluation, we introduce our same text encoder $\nettext$ in the baseline to make use of the action information. To reduce computation, we perform the comparison using half of the computational resources and a reduced training schedule, consequently, we also retrain our model, producing a reduced variant (Ours Small). To render results we always make use of our synthesis model.

We show results averaged over all inference tasks in Tab.~\ref{table:animation_merged} and report the results for each task in \apref{ap:animation_evaluation}. Our method outperforms the baseline in all evaluation tasks according to both L2 and FD metrics. From the qualitative results in Fig.~\ref{fig:animation_ablation_qualitatives_tennis} and in accordance with the FD metrics, we notice that our method produces more realistic player poses with respect to PE that tends to keep player poses close to the average pose and to slide the players on the scene. We attribute this difference to the use of the diffusion framework in our method. Consider the example of generating a player walking forward. It is equally probable that the player moves the left or right leg first. In the case of a reconstruction-based training objective such as the main one of PE, the model is encouraged to produce an average leg movement result that consists in not moving the legs at all. On the other hand, diffusion models learn the multimodal distributions of the motion, thus they are able to sample one of the possible motions without averaging its predictions.

\input{tables/table_animation_merged}

\subsubsection{Ablation}
\label{sec:animation_model_ablation}
To validate this hypothesis and demonstrate the benefits of our diffusion formulation, we produce two variations of our method. The first substitutes the diffusion framework with a reconstruction objective, keeping the transformer-based architecture unaltered. The second in addition to using the reconstruction objective models $\netanimation$ using an LSTM, similarly to the PE baseline. Differently from the PE baseline, however, this variant does not make use of adversarial training and employs a single LSTM model for all objects, rather than a separate model for each.

We show results in Tab.~\ref{table:animation_merged}. Our model consistently outperforms the baselines in terms of FD, showing a better ability to capture realistic sequences. Consistently with our assessment in Sec.~\ref{sec:animation_baselines}, 
Fig.~\ref{fig:animation_ablation_qualitatives_tennis} shows that our method trained with a reconstruction objective produces player movement with noticeable artifacts analogously to PE, validating the choice of the diffusion framework.

