\section{Datasets}
\label{ap:datasets}

In this section, we give additional details on the dataset, including the dataset collection process, Minecraft 3D skeleton format, additional dataset statistics, and dataset samples.

\subsection{Tennis Dataset Collection}
\label{ap:tennis_dataset_collection}
We build our tennis dataset starting from the one of \cite{Menapace2022PlayableEnvironments}. However, we notice that such dataset has an imprecise camera calibration and lacks information such as 3D player poses and 3D localization of the ball. Thus, we only retain the original videos and acquire new annotations. We describe the process in the following sections and release all code related to dataset creation.

\subsubsection{Camera Calibration}
To improve camera calibration, we notice that the original dataset bases its camera calibration on field keypoints detected using \cite{farin2003robustcameracalibration}, but such keypoint estimates are noisy. To overcome this issue, we manually annotate a subset of 10569 frames with field keypoint information and train a keypoint detection model inspired by ICNet \cite{zhao2018icnet}, which we choose due to its reduced memory footprint which allows us to train the model in full 1920x1080px resolution for best results. The detected keypoints are filtered and used to produce camera calibration. Compared to the camera calibration of \cite{Menapace2022PlayableEnvironments}, we notice less jitter and are able to successfully perform camera calibration on a larger number of video instances.

\subsubsection{3D Ball Localization}
To produce 3D ball localization, we first build a 2D ball detector following the same approach used for field keypoints localization, starting from 17330 manually annotated frames. In addition to 2D ball localization, we manually annotate the projection of the ball on the field plane for a set of keyframes defined as the frames where contact between the ball and an object different than the field happens or the first and last frames of the video with a visible ball. The field plane projections of the ball in conjunction with the camera calibration results and 2D ball detections can be used to recover the 3D ball position in those frames. 

We assume that between the keyframes, no contact happens that significantly alters the horizontal speed of the ball apart from air drag. In practice, contact between the ball and the field during bounces does affect ball speed, and we take account of it in a second, refinement phase. We thus model the horizontal ball position on the line between the ball positions at two consecutive keyframes by solving the linear motion equation under air drag:
\begin{equation}
\label{eq:drag_motion}
\vecpoint(t) = \vecpoint_0 \frac{\log(1 + \airdrag \vecvelocity_0 t)}{\airdrag},
\end{equation}
where $\vecpoint_0$ is the initial position, $\vecvelocity_0$ is the initial velocity, $t$ is time and $\airdrag$ is an estimated coefficient summarizing fluid viscosity, drag coefficient, and shape of the ball. Note that the effects of gravity are ignored in the equation.
$\airdrag$ can be estimated by inverting Eq.~\eqref{eq:drag_motion}, based on initial ball speed measurements for $\vecvelocity_0$ that can be extracted from the videos thanks to the service ball speed radars installed on tennis fields, and the positions the ball at keyframes. Given the ball's horizontal position on the line joining the 3D ball position at the preceding and succeeding keyframes, we can recover its 3D position by intersecting the camera ray passing from the 2D projection of the ball on that frame with the plane parallel to the net that intersects with the ball's horizontal position.

To improve the precision of results and account for horizontal ball speed changes during bounces, in a second phase we detect bounces between the ball and the field and impose that the ball touches the field at those positions, by considering them as additional keyframes and repeating the procedure. Finally, to calibrate frames with missing 2D ball detections (eg. ball thrown high above the camera frames or heavy blur and image compression artifacts), we recover the ball position by fitting a ballistic trajectory using 3D ball localization from neighboring frames.

\subsubsection{3D Player Poses}
To recover 3D player poses, we rely on the 3DCrowdNet pose estimator \cite{choi2022learning} which we find robust to the presence of frequent overlaps between players and referees, player limbs blur, and low player resolution. 3DCrowdNet assumes 2D joint locations to be given as input, so we produce them using the state-of-the-art 2D pose estimator VitPose \cite{xu2022vitpose} which we find robust to blur, reduced player size, and occlusions. The extracted 3D skeletons however are expressed under the coordinate system of a framework-predicted camera. We make use of a PnP \cite{EPnP} procedure to register the 3D skeletons to our calibrated camera and reduce depth estimation errors by placing the estimated 3D skeletons with their feet touching the ground. Note that, while 3DCrowdNet regresses full SMPL \cite{loper2015smpl} parameters and meshes, we only make use of 3D joint locations and joint angles. SMPL body shape parameters are nevertheless included in the dataset to support its different use cases. 
% for the dataset.

%\willi{add in the failed experiments that we try to finetune CLIP and use these video CLIP representations for training, but the gap between text and video embeddings was causing problems. There are other works highlighting the problems of such approaches.}

\subsubsection{Text Action Annotation}
We manually annotate each video sequence using a text caption for each player and frame. Each caption focuses on the action being performed by the player in that instant and captures several aspects of the action. The caption captures where the player is moving and how the player is moving, i.e. the player is running, walking, sliding, or falling, the player is moving to its left, towards the net, across the baseline. When a player is performing a ball-hitting action, the particular type of tennis shot being performed is presented, e.g. a smash, a serve, a lob, a backhand, a volley, and the location where the ball is aimed is described. We report text annotation statistics in Tab.~\ref{table:dataset_statistics}.

\subsubsection{UI Elements Annotation}
We manually annotate each video sequence with a set of 2D bounding boxes indicating the places where 2D UI elements such as scoreboards or tournament logos may appear during the sequence.

\subsection{Minecraft Skeleton Format}
\label{ap:minecraft_skeleton_format}
We adopt a skeletal player representation that divides the Minecraft body into 6 parts: head, torso, left and right arm, and left and right leg. We place 6 corresponding joints at the bottom of the head, top of the torso, shoulders, and top of the legs. Following the internal Minecraft skeletal representation, a root joint is added that is the parent of the 6 joints. We extend this representation by introducing 6 additional joints at the top of the head, top of the torso, bottom of the arms, and bottom of the legs. The additional joints have as parents the original joint positioned on the same body part. While the additional 6 joints are always associated with a zero rotation, we find their introduction convenient for skeleton visualization purposes. Fig.~\ref{fig:dataset_samples} provides a visualization of such skeletons.

\subsection{Additional Dataset Statistics}
We provide the main dataset statistics in Tab.~\ref{table:dataset_statistics}, with additional ones in Fig.~\ref{fig:dataset_statistics}, where we plot the distribution of video lengths in the dataset and the average number of words in each caption. The Tennis dataset features manually-annotated captions which contain a greater number of words with respect to the synthetic annotations in the Minecraft dataset.

\subsection{Dataset Samples}
\label{ap:dataset_samples}

\begin{figure*}
\includegraphics{resources/dataset_samples}
  \caption{Sampled frames from the Tennis (left) and Minecraft (right) datasets. 3D skeletons are annotated in blue, while the 3D ball is visualized in green.}
  \label{fig:dataset_samples}
\end{figure*}

We show samples from the Minecraft and Tennis dataset in Fig.~\ref{fig:dataset_samples} and show samples in the form of videos on the \website.

We now show a non-curated set of captions extracted from the Tennis dataset:
\begin{itemize}
\item \emph{``the player prepares to hit the ball but stops, opponent hits the net''}
\item \emph{``the player starts to move to the left  when the opponent sends the ball out of the field''}
\item \emph{``the player moves diagonally to the right and forward to the right side of the baseline and sends the ball to the right side of no man's land with a forehand}
\item \emph{``the player takes sidestep to the right and hits the ball with a backhand that sends the ball to the right side of the no man's land}
\item \emph{``the player moves left to hit the ball but stops halfway}
\item \emph{``the player sidesteps to the left and stops, because the ball goes out of bounds}
\end{itemize}

We report a set of peculiar words extracted from the set of words with the lowest frequency on the Tennis dataset:
\emph{``scratching''},
\emph{``inertia''},
\emph{``previously''},
\emph{``realize''},
\emph{``understands''},
\emph{``succeed''},
\emph{``bind''},
\emph{``touched''},
\emph{``circling''},
\emph{``approaching''},
\emph{``bolting''},
\emph{``entering''},
\emph{``ducks''},
\emph{``reaction''},
\emph{``repeat''},
\emph{``wipes''},
\emph{``abruptly''},
\emph{``preparation''},
\emph{``dramatic''},
\emph{``soft''},
\emph{``celebrating''},
\emph{``losing''},
\emph{``strides''},
\emph{``dart''},
\emph{``reacts''},
\emph{``block''},
\emph{``sideway''},
\emph{``ending''},
\emph{``becomes''},
\emph{``dismissively''},
\emph{``continuous''},
\emph{``squat''},
\emph{``says''},
\emph{``intends''},
\emph{``ricochet''},
\emph{``delays''},
\emph{``night''},
\emph{``guess''},
\emph{``manage''},
\emph{``already''},
\emph{``correctly''},
\emph{``anticipation''},
\emph{``unsuccessfully''},
\emph{``inaccurate''},
\emph{``deflection''},
\emph{``properly''},
\emph{``swinging''}.

We show a non-curated set of captions extracted from the Minecraft dataset:
\begin{itemize}
    \item \emph{``the player falls on the birch pillar''}
    \item \emph{``the player moves fast north, jumps''}
    \item \emph{``the player jumps on the intermediate wooden pillar''}
    \item \emph{``the player falls on the platform opposite to the stairs''}
    \item \emph{``the player runs to the big stone platform''}
    \item \emph{``the player climbs down and does not rotate''}
    \item \emph{``the player moves south east, jumps and rotates counterclockwise''}
    \item \emph{``the player runs to the red decorated block''}
\end{itemize}

We list a set of peculiar words from the Minecraft dataset:
\emph{``nothing''},
\emph{``facing''},
\emph{``space''},
\emph{``level''},
\emph{``map''},
\emph{``leading''},
\emph{``opposite''},
\emph{``edge''}.

\input{tables/table_dataset_statistics}

\begin{figure}
\centering
\begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{resources/tennis_video_duration}
    \caption{Distribution of video durations in the Tennis dataset.}
    \label{fig:tennis_video_duration}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{resources/minecraft_video_duration}
    \caption{Distribution of video durations in the Minecraft dataset.}
    \label{fig:minecraft_video_duration}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{resources/tennis_word_count}
    \caption{Distribution of words per caption in the Tennis dataset.}
    \label{fig:tennis_captions_length}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{resources/minecraft_word_count}
    \caption{Distribution of words per caption in the Minecraft dataset.}
    \label{fig:minecraft_captions_length}
\end{subfigure}
        
\caption{Dataset statistics for the Tennis and Minecraft datasets.}
\label{fig:dataset_statistics}
\end{figure}