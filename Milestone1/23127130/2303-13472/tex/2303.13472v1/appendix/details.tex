\section{Object-Specific Synthesis Model Techniques}
\label{ap:object_specific_techniques}
The compositional nature of the synthesis module makes it possible to adopt object-specific techniques to model particular objects. In the following, we describe the techniques adopted to model balls (\apref{ap:ball_modeling}), rackets (\apref{ap:racket_modeling}), 2D UI elements (\apref{ap:ui_elements}), and skyboxes (\apref{ap:skybox_modeling}).

\subsection{Ball Modeling}
\label{ap:ball_modeling}

Fast-moving objects may appear blurred in real video sequences. This effect is frequent in ball objects found in sports videos and is thus desirable to model this effect. To model them, we adopt a procedure inspired by \cite{cook1984distributed}, which distributes multiple rays in time to model blur effects.
We extend the object properties of the ball object to also include a velocity vector $\vecvelocity$. Given the ball radius $r$ and an estimate for the shutter speed $t_c$ of the camera, we can compute in closed form the probability $p$ that a given point in space intersects with the ball object while the ball moves during the time the camera shutter remains open to capture the current frame. To model blur, we assign to each point a fixed density multiplied by $p$. Modeling $p$ in closed form avoids the need to sample multiple rays in time, improving performance.

To compute $p$ (see Fig.~\ref{fig:ball_blur_diagram}), we first use the velocity vector $\vecvelocity$ to estimate the rotation $\tensrotation_b$ that maps each point $\vecpointbbox$ in the ball bounding box to a canonical space $\vecpointcanon$ in which the ball velocity vector is aligned to the positive $y$-axis $\vecpointcanon = \tensrotation_b \vecpointbbox$. 
Then we compute the distance traveled by the ball while the shutter remains open $d=||\vecvelocity||_2 t_c$. 
We then compute the useful cross-section of the ball $d_y$ that can intersect with $\vecpointcanon$ as the diameter of the circumference originating from the intersection between the ball and a plane with a distance from the ball center $r_y$ equal to the distance of $\vecpointcanon$ from the $y$-axis:
% TODO explicitly make dy and ry a function of x so that p(x) is clearer, otherwise they look constants
\begin{equation}
    d_y=
    \begin{cases}
    2r \sin \left(\arccos\left(\frac{r_y}{r}\right)\right) & \text{if } r_y \le r\\
    0              & \text{otherwise}
\end{cases}.
\end{equation}

Finally, $p$ equals the probability that an interval with size equal to the cross-section, positioned in a random portion of space contained inside an interval of size $d + d_y$, that represents the length of the space that has been touched by the ball while the shutter stays open, contains our point $\vecpointcanon$:
\begin{equation}
p(\vecpointcanon) = \max\left(0, \min\left(\min\left(\frac{d_y}{d}, 1\right), \frac{1}{2} + \frac{d_y}{2d} - \frac{|\vecpointcanon^y|}{d}\right)\right),
\end{equation}
where $\vecpointcanon^y$ is the $y$-axis coordinate of $\vecpointcanon$.

\begin{figure} 
\includegraphics{resources/ball_blur_diagram.pdf}
  \caption{Visualization of the quantities involved in the computation of the probability $p$ that the ball intersects with a certain point in space during a randomly sampled time instant in the interval from the opening to the closing of the shutter of the camera to capture the current frame. The leftmost and rightmost balls depict the ball position at the times the camera shutter opens and closes respectively. For simplicity, we represent the space where the velocity vector of the ball is aligned with the $y$-axis.}
  \label{fig:ball_blur_diagram} 
\end{figure} 

\subsection{Racket Modeling}
\label{ap:racket_modeling}

\begin{figure} 
\includegraphics{resources/rackets}
  \caption{Examples of tennis scenes with and without inserted rackets.}
  \label{fig:racket_modeling} 
\end{figure} 

Modeling the scene as a composition of neural radiance fields allows applications such as the insertion of user-defined watertight 3D meshes into the scene. To do so, we first define the 3D bounding box for the mesh. Then, we extract the signed distance function (SDF) of the 3D mesh. To allow fast retrieval of SDF values during rendering, we sample SDF values along an enclosing voxel grid so that subsequently they can be efficiently retrieved using trilinear sampling. During neural rendering, when a sampled point intersects with the object's bounding box, we query its SDF function and assign a fixed, high density to points that fall inside the object. For simplicity, we assume the object has a uniform appearance and assign a fixed feature vector to such points. To attach the mesh to an articulated object, we align it to its desired position in the object's canonical space, select which joint the mesh should move according to, and we modify blending weights $\tensblendweights$ for the desired joint to have a high value in the region corresponding to the mesh (see Eq.~\eqref{eq:invlbs}).


We employ this technique on the Tennis dataset to manually insert rackets in the scene that cannot be easily learned since they appear frequently blurred and have no ground truth pose available. After the synthesis model is trained, we use this technique to insert a racket mesh in the hand of each player and configure it to move it according to the elbow joint. Fig.~\ref{fig:racket_modeling} shows examples of rackets inserted in tennis scenes.

When inserting additional objects at inference time, we find the enhancer model $\netenhancer$ may introduce artifacts at the contours of the inserted object. For this reason, we modify $\netenhancer$ with a masking mechanism that directly uses values from the NeRF-rendered RGB image $\tensimagergb$ before the enhancer rather than the enhanced image $\tensimagerec$ for pixels corresponding to the inserted object and its contour region.

\subsection{2D UI Elements}
\label{ap:ui_elements}

The presence of 2D user interfaces, such as scoreboards, in the training frames may cause artifacts in the final outputs due to attempts of the synthesis model to model these view-inconsistent elements \cite{Menapace2022PlayableEnvironments}. To address this issue, we assume that the potential regions where such interfaces may be present are known and we never sample training patches that intersect with these regions. In this way, the model does not attempt to generate such UI elements and instead models the underlying portion of the 3D scene using data from different views.

\subsection{Skybox Modeling}
\label{ap:skybox_modeling}
The Minecraft background is represented as a skybox that is modeled by extending the planar object modeling mechanism of Sec.~\ref{sec:canonical_volume}. In more detail, we sample the feature plane $\tensplane$ according to the ray's yaw and pitch of the current ray, which can be interpreted as querying points on the surface of a sphere with a radius approaching infinity.

