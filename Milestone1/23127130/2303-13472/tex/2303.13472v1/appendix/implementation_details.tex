\section{Implementation Details}

\subsection{Synthesis Model}
\label{ap:implementation_details_synthesis}

We model Minecraft scenes considering as objects the player, the scene, and the background. 
To model Tennis scenes, we consider as separate objects the two players, the ball, the field plane, and the vertical backplate at the end of the field. Both players share the same canonical representation. Note that the field and backplate are modeled as planar objects due to the lack of camera translation on the tennis dataset, which does not make it possible to reconstruct the geometry of static objects \cite{Menapace2022PlayableEnvironments}.

For each ray, we uniformly sample 32 points for players, 16 for the ball, 48 for the Minecraft scene, and 1 for all remaining objects that are modeled as planes. We do not employ hierarchical sampling, which we empirically found not to improve results. A patch size of 180x180px and of 128x128px are employed respectively for the Tennis and Minecraft datasets.

We model the initial blocks of the style encoder $\netstyle$ as the first two residual blocks of a pretrained ResNet 18 \cite{he2016deep}. To prevent players from being modeled as part of the background, we always sample images in pairs from each video and randomly swap the style codes $\vecstyle$ of corresponding objects \cite{Menapace2022PlayableEnvironments}.

To represent the player canonical radiance fields, we use a voxel $\tensvoxel$ with $\numfieldfeat=64$ features and $\numvoxelheight=\numvoxelwidth=\numvoxeldepth=32$. Deformations are represented using blending weights $\tensblendweights$ with $\numblendweightsheight=\numblendweightswidth=\numblendweightsdepth=32$. For the Minecraft scene, the size of the voxel $\tensvoxel$ is increased to $\numvoxelheight=\numvoxelwidth=\numvoxeldepth=128$. The Minecraft skybox is represented with feature planes $\tensplane$ with $\numfieldfeat=64$ features and size $\numplaneheight=\numplanewidth=256$. Due to their increased complexity and variety of styles, in the Tennis dataset feature planes $\tensplane$ with $\numfieldfeat=512$ features are adopted. The MLPs performing stylization of the canonical field features are modeled using 2 layers with a hidden dimension of 64, with a final number of output features $\numfieldfeat=19$, where the first 3 channels represent radiance.


\subsection{Animation Model}
\label{ap:implementation_details_animation}

For the text encoder, we model $\nettextenc$ as a frozen T5-Large 
\cite{raffel2022exploring} model and $\nettextagg$ as a transformer encoder \cite{vaswani2017attention} with 4 layers, 8 heads, and a feature size of 1024. For each sequence, the output $\vecactionemb$ of $\nettext$ is the transformer encoder output corresponding to the position of the end-of-sentence token in the input sequence. We experimented with mean pooling and a learnable class-token with comparable results. Consistently with \cite{saharia2022imagen}, we found alternative choices for $\nettextenc$ (T5-Small, T5-Base \cite{raffel2022exploring} and the CLIP text encoder \cite{radford2021clip}) to underperform T5-Large.

For the temporal model $\netanimation$, we employ a transformer encoder with 12 layers, 12 heads, and 768 features.
To favor generalization to sequences of different lengths at inference time, we adopt relative positional encodings \cite{shaw2018self} that specify positional encodings based on the relative distance in the sequence between sequence elements.
We produce embeddings for the diffusion timestep $\difftimestep$ and framerate $\framerate$ using sinusoidal position encodings \cite{vaswani2017attention}.
Additionally, to enable the model to better distinguish between noisy sequence entries and conditioning signals, we find it beneficial to condition also on $\vecmask$ and $\vecmaskaction$ using the same weight demodulation layer.

 The temporal model receives a flattened sequence of object properties grouped and encoded as follows: the position of objects as the bounding box center point; the player poses expressed with joint translations and rotations separately, with rotations expressed in axis-angle representation, which we find to produce more realistic animations with respect to the 6D representation of \cite{zhou2019cvpr}; the ball speed vector expressed as its orientation in axis-angle representation and norm. Separating positions from joint translations and rotations has the practical implication that these properties can be independently used as conditioning signals during inference. This enables applications such as generating realistic joint rotations and translations given a sequence of object positions in time describing the object movement trajectory. We assume style to remain constant in the sequence, thus we do not include it as input to the model.

%\willi{Details for the small variant}


%\willi{Impl detail: we leverage the mask to only encode what is not masked}
%\willi{Impl detail: actions are expected to be repeated multiple times due to temporal correlations so we only encode each action once and also make use of caching for the outputs of T5}