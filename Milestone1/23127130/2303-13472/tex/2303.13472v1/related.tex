\section{Related Work} 

%Our Learnable Game Engine relates to neural game simulation literature, sequential data generation, text-based generation, neural rendering and character animation.
%We review the most recent related works in this section.

\subsection{Neural video game simulation}
In the last few years, video game simulation using deep neural networks has emerged as a new research trend ~\cite{Kim2020_GameGan,kim2021drivegan,menapace2021pvg,Menapace2022PlayableEnvironments,davtyan2022glass,huang2022layered}.  The objective is to train a neural network to synthesize videos based on sequences of actions provided at every time step.

This problem was first addressed using training videos annotated with the corresponding action labels at each time step~\cite{oh2015action,chiappa2017recurrent,Kim2020_GameGan}. They consider a discrete action representation that is difficult to define a priori for real-world environments. More recently, \cite{kim2021drivegan} proposed a framework that uses a continuous action representation to model real-world driving scenarios. Devising a good continuous action representation for an environment, however, is complex.
To avoid this complexity, \cite{menapace2021pvg, Menapace2022PlayableEnvironments} propose to learn a discrete action representation. \cite{huang2022layered} expands on this idea by modeling actions as a learned set of geometric transformations, while \cite{davtyan2022glass} represents actions by separating them into a global shift component and a local discrete action component.

Among these works, \emph{Playable environments}~\cite{Menapace2022PlayableEnvironments} is the most closely related to ours. Rather than employing a 2D model, they use a NeRF-based renderer~\cite{mildenhall2020nerf} that enables them to represent complex 3D scenes. However, the employed discrete action representation shows limitations in complex scenarios such as tennis, where it is only able to capture the main movement directions of the players and does not model actions such as ball hitting. In contrast, we employ a text action representation that specifies actions at a fine level of granularity (i.e. which particular ball-hitting action is being performed and where the ball is sent), while remaining interpretable and intuitive for the user.

Besides, previous works perform generation in an autoregressive manner, conditioned on the actions and, therefore, are unable to perform constraint- or goal-driven generation for which non-sequential conditioning is necessary. We find the proposed text-based action representation to be crucial to unlock such applications.



\subsection{Sequential data generation with diffusion models}
In prior work, sequential data generation was mainly addressed with auto-regressive formulations combined with adversarial~\cite{kwon2019predicting} or variational~\cite{fortuin2020gp,babaeizadeh2018stochastic} generative models. Recently, diffusion models have emerged as a promising solution to this problem leading to impressive results in multiple applications such as audio~\cite{kong2020diffwave,lam2022bddm,chen2021wavegrad,leng2022binauralgrad} and video synthesis~\cite{ho2022video,ho2022imagenvideo,singer2022makeavideo}, language modeling~\cite{dieleman2022cdcd}, and human motion synthesis \cite{zhang2022motiondiffuse,dabral2022mofusion}.
Following this methodological direction~\cite{tashiro2021csdi}, introduces a score-based diffusion model for imputing missing values in time series. They introduce a training procedure based on masks that simulates missing data. This approach motivates our choice of a similar masking strategy to generate the unknown environment states. In this work, we show that mask-based training is highly effective in modeling geometric properties together with textual data modalities. %Moreover, it can be employed with different network architectures based on transformers to allow the generation of complex data. 


\subsection{Text-based generation}
In recent years, we have witnessed the emergence of works on the problem of text-based generation.
Several works address the problem of generating images \cite{saharia2022imagen,rombach2021highresolution,ramesh2021zeroshot,ramesh2022hierarchical} and videos with arbitrary content~\cite{ho2022video,ho2022imagenvideo,singer2022makeavideo,hong2022cogvideo}, and arbitrary 3D shapes \cite{jain2021dreamfields,lin2022magic3d,poole2022dreamfusion,achlioptas2023shapetalk}.

Han \emph{et al.} \cite{han2022show} introduced a video generation framework that can incorporate various conditioning modalities in addition to text, such as segmentation masks or partially occluded images. Their approach employs a frozen RoBERTa \cite{liu2020roberta} language model and a sequence masking technique. Fu \emph{et al.} \cite{fu2022tellmewhathappened} propose an analogous framework. Our animation framework employs a similar masking strategy, but we model text conditioning at each timestep in the sequence, use diffusion models which operate on continuous rather than discrete data, and generate scenes that can be rendered from arbitrary viewpoints. 

More relevant to our work, several papers introduced models to generate human motion sequences from text \cite{guy2022motionclip,TEACH}. %MotionCLIP~\cite{guy2022motionclip} aligns the space of human motions to the one of a pre-trained CLIP \cite{radford2021clip} model. TEACH~\cite{TEACH} adopts an auto-regressive model conditioned on a frozen CLIP encoder and generates a sequence of parameters of an SMPL body model.
Recently, diffusion models have shown strong performance on this task \cite{zhang2022motiondiffuse,dabral2022mofusion}. In these works, sequences of human poses are generated by a diffusion model conditioned on the output of a frozen CLIP text encoder. It is worth noting that these prior works model only a single human, while our framework supports multiple human agents and objects, and models their interactions with the environment.

    
\subsection{Character Animation}
\label{sec:character_animation}

Character animation is a long-standing problem in computer graphics. %that consists in modeling the motions of a character according to the provided user inputs.
Several recent methods have been proposed that produce high-quality animations. Holden \etal \cite{holden2020learned} propose a learnable version of Motion Matching \cite{buttner2019motion} that formulates character animation as retrieval of the closest motion from a motion database and supports interaction with other characters or objects.
%, quadruped motion and movement over rough terrain.
Other approaches model the evolution of characters using time series models conditioned on the preceding state and control signals \cite{starke2019neural,starke2020local,lee2018interactive,holden2017phase}. Starke \etal \cite{starke2019neural} propose a model based on a mixture of experts that controls character locomotion and object interactions, in a follow-up work \cite{starke2020local} they introduce local motion phases to model complex character motions and interaction with a second character.

To produce high-quality animations the methods rely on difficult-to-acquire motion capture data enriched with contact information \cite{holden2020learned,starke2019neural,starke2020local}, motion phases \cite{starke2019neural} or engineered action labels \cite{starke2019neural,starke2020local}. Additionally, handcrafted dataset-specific feature representations and mappings from user controls to such representations are often leveraged, and additional knowledge is injected through postprocessing steps such as inverse kinematics or external physics models. While these assumptions promote high-quality outputs, they come at a significant effort. In contrast, our method sidesteps these requirement by not using motion capture and basing user control on natural language that is cheaper to acquire (see \apref{ap:cost_quality}) and does not require manual engineering. Finally, character animation methods support limited goal-driven control such as interacting with a specific object while avoiding collisions \cite{starke2019neural}. In contrast, our method models complex game AI tasks such as environment navigation and modeling strategies to defeat the opponent, which are instrumental in learning a game engine.

    
 \subsection{Neural Rendering}

Neural rendering was recently revolutionized by the advent of NeRF \cite{mildenhall2020nerf}. Several modifications of the NeRF framework were proposed to model deformable objects \cite{tretschk2021nonrigid,park2021nerfies,weng2022humannerf,park2021hypernerf,li2022tava}, and decomposed scene representations \cite{Menapace2022PlayableEnvironments,niemeyer2021giraffe,mueller2022autorf,Ost_2021_CVPR,kundu2022panoptic,wu2022dnerf}. In addition, several works improved the efficiency of the original MLP representation of the radiance field \cite{mildenhall2020nerf} by employing octrees \cite{yu2021plenoctrees}, voxel grids \cite{fridovich2022plenoxels}, triplanes \cite{chan2022eg3d}, hash tables \cite{mueller2022instant}, or factorized representations \cite{chen2022tensorf}.

Our framework is most related to that of \cite{weng2022humannerf}, since we model player deformations using an articulated 3D prior and linear blend skinning (LBS) \cite{lewis2000pose}. Differently to them, however, we consider scenes with multiple players and apply our method to articulated objects with varied structures for their kinematic trees. %Our method adopts a composable scene formulation.
While similar to the rendering framework of \cite{Menapace2022PlayableEnvironments}, our framework does not adopt computationally-inefficient MLP representations, using voxel \cite{fridovich2022plenoxels} or plane representations instead.

 