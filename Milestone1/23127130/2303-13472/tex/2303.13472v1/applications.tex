\section{Applications}
\label{sec:applications}

\begin{figure*}
\includegraphics[width=0.96\textwidth]{resources/alternative_actions_qualitatives.pdf}
  \caption{Different sequences predicted on the Tennis and Minecraft datasets starting from the same initial state and altering the text conditioning. Our model moves players and designates shot targets using domain-specific referential language (eg. \emph{"right service box"}, \emph{"no man's land"}, \emph{"baseline"}). The model supports fine-grained control over the various tennis shots using technical terms (eg. \emph{``forehand''}, \emph{``backhand''}, \emph{``volley'')}. See the \website~for additional results.}
  \label{fig:alternative_actions_qualitatives}
\end{figure*}

\begin{figure*}
\includegraphics[width=0.95\textwidth]{resources/player_control}
  \caption{Sequences generated by specifying actions for one of the players and letting the model act as the game AI and take control of the opponent. The game AI successfully responds to the actions of the player by running to the right (see top sequence) or towards the net (see bottom sequence), following two challenging shots of the user-controlled player.}
  \label{fig:opponent_control}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{resources/random_player_movement}
  \caption{Sequences generated without any user conditioning signal. The actions of all players are controlled by the model that acts as the game AI. In tennis, the players produce a realistic exchange, with the bottom player advancing aggressively toward the net and the top player defeating him with a shot along the right sideline. The Minecraft player and tennis ball trajectories are highlighted for better visualization. Please refer to the \website~for additional results.}
  \label{fig:random_player_movement}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{resources/minecraft_navigation}
  \caption{Given an initial and a final state, we generate all the states in between. We repeat the generation multiple times conditioning it using different actions indicating the desired intermediate waypoints. Additional results are shown on the \website.}
  \label{fig:minecraft_navigation}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{resources/how_to_win}
  \caption{Given a sequence where the bottom player loses (see top), we ask the model to modify it such that the bottom player wins instead (see bottom). To do so, we condition the top player on the action \emph{"The player does not catch the ball"}. While in the original sequence the bottom player aims its response to the center of the field where the opponent is waiting, the model now successfully generates a winning set of moves for the bottom player that sends the ball along the left sideline, too far for the top player to be reached. Video results are shown on the \website.}
  \label{fig:how_to_win}
\end{figure*}


Our Learnable Game Engine enables a series of applications that are unlocked by its expressive state representation, the possibility to render it using a 3D-aware synthesis model, and the ability to generate sequences of states with an animation model that understands the game logic and can be conditioned on a wide range of signals. %, including high-level yet fine-grained textual actions.
In the following, we demonstrate a set of selected applications.

%\textbf{Style swap and novel view synthesis.}
Our state representation is modular, where the style is one of the components. Style swapping is enabled by swapping the style of the desired object $\vecstyle$ in the original image with the one from a target image. Similarly to a traditional game engine, our synthesis model renders the current state of the environment from a user-defined perspective. This enables our LGE to perform novel view synthesis. We show in \apref{ap:applications} examples of both these capabilities.

We now show a set of applications enabled by the animation model. In Fig.~\ref{fig:alternative_actions_qualitatives}, we show results for generating different sequences using textual actions starting from a common initial state. Thanks to the textual action representation, it is possible to gain fine control over the generated results and to make use of referential language. %Not only the model can understand subtle variations in how an action such as hitting the ball can be performed (e.g. \emph{"forehand"}, \emph{"backhand"}, \emph{"volley"}), but referential language can be used to specify which object in the scene is the target of the action, such as where to jump (e.g. \emph{"the birch wood pillar"}, \emph{"the highest pillar"}, \emph{"the low pillar in the beginning zone"}) or where to aim the shot (e.g. \emph{"right service box"}, \emph{"no man's land"}). Interestingly, these capabilities are learned from textual descriptions alone.

Our animation model, however, is not limited %as \cite{menapace2021pvg,Menapace2022PlayableEnvironments,Kim2020_GameGan,kim2021drivegan,huang2022layered,davtyan2022glass}
to generating sequences given step-by-step actions. Thanks to its understanding of the game's logic, the model can tackle more complex tasks such as modeling an opponent against which a user-controlled player can play against (see Fig.~\ref{fig:opponent_control}), or even controlling all players in a scene without user intervention (see Fig.~\ref{fig:random_player_movement}), in a way similar to a traditional game engine's "game AI".

The animation model also unlocks the "director's mode", where the user can generate sequences by specifying a desired set of high-level constraints or goals. The model is able to reason on actions to find a solution satisfying the given constraints. As a first example, Fig.~\ref{fig:minecraft_navigation} demonstrates results for a navigation problem, where the user specifies a desired initial and final player position in the scene, and the model devises a path between them. %, producing all the states in between. 
Notably, the user can also constrain the solution on intermediate waypoints by means of natural language. % action such as that the player should \emph{"jump on the gold pillar"} in the middle of the sequence. Multiple waypoints can be specified, enabling the generation of longer and more complex paths.
As a second example, Fig.~\ref{fig:how_to_win} shows that the model is capable of devising strategies to defeat an opponent. Given an original sequence where the player commits a mistake and loses, the model can devise which actions the player should have taken to win. %This can be achieved by considering an initial state preceding the mistake and conditioning the opponent player on the action \emph{"The player does not catch the ball"}.
Notably, these model capabilities are learned by just observing sequences annotated with textual actions. 

