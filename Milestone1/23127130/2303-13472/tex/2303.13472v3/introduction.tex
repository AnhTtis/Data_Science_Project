\section{Introduction}
\label{sec:introduction}

% 
% 

\change{Recent video generation methods, thanks to their training on extensive web-scale datasets \cite{schuhmann2022laionb}, exhibit a remarkable capacity for generating a vast amount of different concepts and scenes \cite{ho2022imagenvideo,singer2022makeavideo,blattmann2023videoldm}. Despite this, their generic nature hinders their comprehension of the dynamics of the modeled scenes. When generating or editing videos of a game, such as a tennis match, this limitation impedes their ability to attain precise control of the player movements or to devise optimal strategies to reach desired states of the game, such as victory over the opponent.}

Neural video game simulators, \change{a growing category of video generation methods, make an important step in this direction by focusing on modeling the dynamics of an environment, often a sports or computer game, with high fidelity and degree of control, and} show that annotated videos can be used to learn to generate videos interactively \cite{Kim2020_GameGan,kim2021drivegan,menapace2021pvg,huang2022layered,davtyan2022glass} and build 3D environments where agents can be controlled through a set of discrete actions \cite{Menapace2022PlayableEnvironments}. However, when applied to complex or real-world environments, these works present several limitations: they do not accurately model the game's dynamics, do not model physical interactions of objects in 3D space, do not learn precise controls, do not allow for high-level goal-driven control of the game flow, and, finally, do not model intelligent behavior of the agents, a capability often referred to as ``game AI''.

\change{In this work, we overcome these limitations by introducing game models trained on a set of annotated videos that support complex prompts. Due to the versatility of the applications enabled by diverse prompting methods (see Sec.~\ref{sec:applications}), we call them Promptable Game Models (PGMs).} More formally, we define \change{PGMs} as those models supporting a core set of game modeling and prompting functions including rendering from a controllable viewpoint, modeling of game's dynamics, precise character control, high-level goal-driven control of the game, and game AI. 
Making a first step towards the realization of such models, we propose a framework that supports these characteristics. 

To overcome the limitations of \cite{Kim2020_GameGan,kim2021drivegan,huang2022layered,davtyan2022glass,menapace2021pvg, Menapace2022PlayableEnvironments}, 
not only we model the \emph{states} of an environment, but we also consider detailed textual representations of the actions taking place in it.  We argue that training on user commentaries describing detailed actions of a game greatly facilitates learning the dynamics of the game and game AI---important parts of \change{PGMs}---and that such commentaries are a key component in enabling a series of important model capabilities related to precise character control and high-level goal-driven control of the game flow.


In its simplest form, for games like tennis, this enables controlling each player in a precise manner with instructions such as \emph{``hit the ball with a backhand and send it to the right service box''}.

Moreover, language enables users to take the \emph{director's mode} and prompt the model with high-level game-specific scenarios or scripts, specified by means of \emph{natural language} and \emph{desired states of the environment}. As an example, given desired starting and ending states, our \change{promptable game model} can devise in-between scenarios that led to the observed outcome. Most interestingly, as shown in Fig.~\ref{fig:teaser}, given the initial states of a real tennis video in which a player lost a point, our model prompted by the command \emph{“the [other] player does not catch the ball”} can perform the necessary action to win the point.

Broadly speaking, a game maintains states of its environments \cite{Stanton2016,starke2019neural,Curtis2022}, renders them using a controllable camera, and evolves them according to user commands, actions of non-playable characters controlled by the game AI, and the game's dynamics. 
Our framework follows this high-level structure highlighted in Fig.~\ref{fig:teaser}. Our synthesis model maintains a state for every object and agent included in the game and renders them in the image space using the compositional NeRF of~\cite{Menapace2022PlayableEnvironments} followed by a learnable enhancer for superior rendering quality. 
To model the dynamics of games and game AI that determine the evolution of the environment states, we introduce an animation model. Specifically, inspired by~\cite{han2022show}, we train a \emph{non-autoregressive} \emph{text-conditioned} diffusion model which leverages masked sequence modeling \change{to express the conditioning signals corresponding to a prompt}. 
In particular, we show that using text labels describing actions happening in a game is instrumental in learning such capabilities.  While certain prior work~\cite{Kim2020_GameGan,kim2021drivegan,menapace2021pvg,Menapace2022PlayableEnvironments} explored maintaining and rendering states of games, we are not aware of any generative method that attempts to enable precise control, modeling sophisticated goal-driven game dynamics, and learning game AI to the extent explored in this paper.

The task of playing games and manipulating videos in the \emph{director's mode} has not been previously introduced in the literature. With this work, we attempt to introduce the task and set up a solid framework for future research. To do that, we collected two monocular video datasets. The first one is the Minecraft dataset containing 1.2 hours of videos, depicting a player moving in a complex environment.  
The second is a large-scale real-world dataset with 15.5 hours of high-resolution professional tennis matches. For each frame in these datasets, we provide accurate camera calibration, 3D player poses, ball localization and, most importantly, diverse and rich text descriptions of the actions performed by each player in each frame. 

In summary, our work brings the following contributions:
\begin{itemize}
\item A framework for the creation of \change{Promptable Game Models}.
It supports detailed offline rendering of high-resolution, high-frame rate videos of scenes with articulated objects from controllable viewpoints. It can generate actions specified by detailed text prompts, model opponents, and perform goal-driven generation of complex action sequences. As far as we are aware, no existing work provides this set of capabilities under comparable data assumptions.
\item A synthesis model, based on a compositional NeRF \change{backed by an efficient plane- and voxel-based object representation that operates without upsampling.} 
\change{With respect to the upsampler-based approach of \cite{Menapace2022PlayableEnvironments}, it doubles the output resolution, can synthesize small objects and does not present checkerboard upsampling artifacts.}
\item An animation model, based on a text-conditioned diffusion model with a masked training procedure, which is key to supporting complex game dynamics, object interactions, game AI, and understanding detailed actions. It unlocks applications currently out of reach of state-of-the-art neural video game simulators (see Sec.~\ref{sec:applications}). 
\item A large-scale 15h Tennis and a 1h Minecraft video datasets with camera calibration, 3D player poses, 3D ball localization, and detailed text captions.
\end{itemize}


