\section{Discussion}

\subsection{Cost-Quality Tradeoff and Game Developers Validation}
\label{ap:cost_quality}

Building video games is an extremely expensive process. Our \change{PGM}, being a \emph{fully learnable} solution, requiring only annotated monocular videos and supporting a core set of game functions has potential applications aimed at reducing game development effort. 

This research direction is a step towards creating games and editing videos without expensive equipment, data, 3D assets, sophisticated software and manual labor of trained experts. We do not aim to surpass the quality of high-cost techniques (\$100k-\$1M, see below) that require such resources.

Evaluation of \change{PGMs} in such context necessarily needs to be performed under the light of a Pareto curve representing the tradeoff between development cost and output quality. 
We analyze three points on this curve for tennis:
\begin{itemize}
\item \emph{Our scenario: Neural video game simulation} (10k\$ cost range,
medium quality) We annotate monocular videos with granular text through a hired professional labeling team for
883\$/video-hour, totaling 13,672\$, and spend a comparable amount of compute for the remaining annotation and training our models. The model produces renderings of higher quality than state-of-the-art works operating under the same data and cost assumptions \cite{Menapace2022PlayableEnvironments}, learns a capable game AI, and is based on rapidly improving NeRF and diffusion techniques.
\item \emph{Traditional game development} (1M\$ cost range, high quality). We interviewed three game development experts with backgrounds in real-time graphics, 3D models and animation, and game development management with 45 years of combined experience. We invited the experts to discuss the recent “AO Tennis 2” game and compare it with our method. Their cost estimates for building AO Tennis 2 using existing game engines were respectively of \$100k-500k (in the US), \$600k (45-person-years in Ukraine), and of \$1M (3-person-years in the US), including software licenses, equipment and assets. They noted that ``[our model’s] game AI is very valuable and it's going to be the hardest part of developing tennis'' and that our model's game AI has the potential to be ``game changer'' for tasks such as realistically modeling the behavior of animals inserted in a game. With regards to graphics, they noted that ``[our model’s graphics] is more realistic'' and that ``[our model’s output] looks like a real video'' when not zoomed in, but commented that users may prefer a less-realistic game-like graphics because they are more accustomed to its look. The users highlighted the value of the generated animated 3D assets, remarking that high-quality 3D assets of real players are expensive. When asked about possible immediate uses of the model in game production they reported that while the model is ``impressive'' it is not yet ``mind-blowing'' when speaking about building products using our method. In particular, the model's output may present artifacts that would require correction, preventing direct use of the model in a production environment. An interviewee highlighted that the framework could be called a ``limited game engine'' and would be ``awesome'' to use to create a new type of ``promo games'' such as Superbowl or Nascar games that can be released at low cost immediately after the sport season, leveraging the captured footage. Given the significantly lower cost of our method and the rapid evolution in neural rendering and diffusion models on which our framework is based, we consider these comments an encouraging validation of this research direction.
\item \emph{Specialized CG techniques} (100k\$ cost range, high quality). Specialized character animation techniques \cite{starke2019neural,starke2020local,holden2020learned} produce high-quality animations. However, they come at higher cost. The sole requirement of motion capture data entails a professional multicamera system (1k-10k\$ per camera * 10s of cameras), motion capture software licenses (1k-10k\$/year), an HPC system with TBs of storage (>>10k\$), dedicated engineers (10k-100k\$/year), studio space, inviting professional actors or players (10-100\$/h). In addition, they do not model the complete game’s dynamics and only learn basic game AI elements, making their integration into a unified, learnable framework nontrivial.
\end{itemize}

\subsection{Choice of Hyperparameters}
\label{ap:hyperparameters_choice}

\change{The composable nature of our framework allows each object in the environment to be represented and parametrized independently from the other objects. While this offers flexibility, it introduces several hyperparameters to be set. To ease the configuration of the framework for new datasets, in the following, we summarize the main hyperparameters to be configured and the rationale on how to configure them:}
\begin{itemize}
\item \change{The set of objects to model. This is typically strongly suggested by the scene and the number of contained agents.}

\item \change{The dimension of the bounding boxes in meters for each object. This is typically known a priori for each object.}

\item \change{The structure of the kinematic tree of deformable objects. This is typically known a priori based on the method used to obtain the 3D pose estimates (SMPL for humans, internal Minecraft representation for Minecraft)}

\item \change{The type of NeRF canonical volume representation $\netcanonical$ to use for each object (see Sec.~\ref{sec:canonical_volume}). This is typically strongly suggested by the structure of the object to be modeled, i.e. 2D feature plane for objects well approximated by planes such as the tennis field, 3D feature grids for non-planar objects, and the skybox representation for the sky.}

\item \change{Dimension and number of features for the chosen representation of $\netcanonical$ (see \apref{ap:implementation_details_synthesis}). Despite the large differences between players in the Tennis and Minecraft dataset, we use the same hyperparameters, thus believe they can generalize well across datasets.
Feature planes for the tennis field and skyboxes are computationally inexpensive, thus we set a high resolution for them without tuning and assign a larger number of features to Tennis planes due to their higher level of detail.
Due to the large dimension of the Minecraft scene, we assign it a larger-resolution feature grid 128x128x128 rather than the 32x32x32 used for players.
These values can be raised in case of increased geometric complexity and level of detail in the object with respect to the showcased datasets.}

\item \change{Number of points to sample for each object category (see \apref{ap:implementation_details_synthesis}). For both Minecraft and Tennis we use 32 points for players, thus we believe the value can generalize to articulated objects in different datasets. For planar objects and skyboxes a single point must be sampled. For the Minecraft scene, we use 48 due to its larger dimension. These values can be reduced or increased based on the size and geometric complexity of the objects in the dataset of interest.}

\end{itemize}

\subsection{Ethics}
\label{ap:ethics}

The techniques described in this work fall in the category of video editing methods and could potentially be used to nefariously alter existing videos. The design of our method assumes multiple camera-calibrated observations of a single scene to be available for training, and that the desired edit is shown at least once in the training data. This provides protection against applying the method to tamper a single video, for which the quantity of data would not be sufficient and the desired edit would likely not be shown. 