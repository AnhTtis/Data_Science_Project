
\section{Preliminary and Motivation}
%\vspace{-5pt}

\subsection{Boolean Networks and And-Inverter Graphs}
BNs are well-studied discrete mathematical models with broad applications in chemistry, biology, circuit design, formal verification, etc.
% A BN consists of a discrete set of binary variables, each of which is assigned a Boolean function taking inputs from a subset of these variables.
%BNs can be represented as directed graphs, in which each node represents a logic gate and each directed edge represents the dependency between variables.
% The explosive growth in the scale of BNs has brought up increasing attention to analyzing their static and dynamic behaviors.
From the synthesis and verification perspective, a compact and uniform representation of BNs composed of two-input AND-gates and inverters, called and-inverter graphs (AIGs), has found successful use in diverse EDA tasks, since AIGs allow rewriting, simulation, technology mapping, placement, and verification to share the same data structure~\cite{mishchenko2006dag}.
In an AIG, each node has at most two incoming edges; 
a node without incoming edges is a primary input (PI), and primary outputs (POs) are denoted by special output nodes;
each internal node represents a two-input AND function. 
Based on De Morganâ€™s laws, any combinational BN can be converted into an AIG~\cite{brayton2010abc} in a fast and scalable manner.

In AIGs, cut enumeration can be used to detect Boolean functions.
A feasible cut of node $n$ is a set of nodes in the transitive fan-in cone of $n$, whose truth value assignments completely determine the value of $n$.
A cut is $K$-feasible if there are no more than $K$ inputs.
Figure~\ref{fig:adder} depicts an example of reasoning XOR functions and full adders from AIGs.
In Figure~\ref{fig:adder}(a), the AIG has a 3-feasible cut of node 9 and a 2-feasible cut of node 6; after truth table computation, the functions of node 6 and node 9 are IN1$\oplus$IN2 and IN1$\oplus$IN2$\oplus$IN3, respectively.
Thus, as shown in Figure~\ref{fig:adder}(b), node 6 is an XOR2 function, and node 9 is an XOR3 function.
Figure~\ref{fig:adder}(c) shows a full adder bitslice, with the sum as an XOR function and the carry-out as a majority (MAJ) function.
By pairing an XOR3 with a MAJ3 with identical inputs, a full adder bitslice can be extracted, which is then aggregated for word-level abstraction. 

% \cy{We should also explain the advantages of AIG in GL in my opinion.}





% The function of $node~6$ is $i_1$ $\oplus$ $i_2$, and the function of $node~9$ is $i_1$ $\oplus$ $i_2$ $\oplus$ $i_3$. Hence, $node~6$ is an \textit{XOR2}-node, and $node~9$ is an \textit{XOR3}-node. This means that an embedded XOR3 function consisting of two XOR2s exists and can be detected in the sub-circuit shown in Figure \ref{fig:xor3-aig}(a). Similarly, an AIG can be applied to identify embedded \textit{Majority} functions.

%\vspace{-5pt}
\subsection{Word-Level Abstraction}
Word-level abstraction significantly reduces the complexity of large-scale BNs by grouping wires into meaningful words and keeping useful information related to control logic, which is widely applied in reasoning functional units from gate-level netlists~\cite{li2013wordrev,subramanyan2013reverse,yu2017fast}.
Conventional word identification uses \textit{structural shape hashing} and  \textit{functional bitslice aggregation}. 
Structural shape hashing assigns each edge in the BN a shape, which is defined as the directed graph constructed by the backward reachable nodes from this edge within certain depth/steps.
Functional bitslice aggregation adopts functional matching to group functionally equivalent nodes and edges by cut enumeration.
Typically, structural hashing and functional aggregation are iteratively propagated across neighborhood nodes using symbolic evaluation~\cite{li2013wordrev,subramanyan2013reverse,yu2017fast}.
However, for large-scale BNs, structural hashing is memory-consuming;
functional bitslice aggregation is not efficient due to the requirement of bit-blasting;
the computation of symbolic evaluation is also expensive.
Motivated by the \textbf{limited scalability} and \textbf{difficulty of parallelism}, we propose to exploit \textbf{graph learning and GPU acceleration for highly scalable reasoning}.




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/adder.pdf}
   % \vspace{-20pt}
    \caption{Netlists of XOR and a full adder. (a) AIG of XOR3 function. (b) XOR3 function: OUT9 = XOR3(IN1, IN2, IN3). (c) Full adder with a sum function (i.e., XOR3) and a carry-out function (i.e., MAJ3).}
   % \vspace{-20pt}
    \label{fig:adder}
\end{figure}

\subsection{Graph Neural Network}
Since BNs and circuit netlists are naturally represented as graphs, GNNs can be leveraged to classify sub-circuit functionality from gate-level netlists~\cite{alrahis2021gnn}, predict the functionality of approximate circuits~\cite{bucher2022appgnn}, analyze impacts of circuit rewriting on functional operator detection~\cite{zhao2022graph}, and predict boundaries of arithmetic blocks~\cite{he2021graph}.
Promising as they are, these approaches focus on graphs with tens of thousands of nodes, and conduct training on complex designs and inference on relatively simpler ones, in which the generalization capability from simple to complex designs is not well examined.

GNNs operate by propagating information along the edges of a given graph.
Each node is initialized with a representation, which could be either a direct representation or a learnable embedding obtained from node features.
Then, a GNN layer updates each node representation by integrating the node representations of itself and its neighbors in the graph.
The propagation along edges extracts structural information from graphs, corresponding to structural shape hashing in conventional reasoning;
after encoding functionality into node features, neighborhood aggregation is analogous to functional aggregation in conventional reasoning.
Thus, the inherent message passing mechanism in GNNs enables simultaneously handling structural and functional information.
Motivated by \textbf{the analogy between GNN operations and conventional reasoning}, we propose \textbf{a multi-task GNN for high-performance reasoning} w.r.t. exact reasoning algorithms, with \textbf{strong generalization capability} from simple to complex designs.
% scalability to large BNs with tens of millions nodes.

