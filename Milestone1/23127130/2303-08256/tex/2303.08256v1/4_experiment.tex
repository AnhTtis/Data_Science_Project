
\vspace{-5pt}
\section{Experiment}
\vspace{-3pt}
\subsection{Experiment Setup}



The AIG-based CSA and Booth multipliers are generated by the logic synthesis tool ABC~\cite{brayton2010abc}, with the ground truth provided by the adder tree extraction command~\cite{yu2017fast}.
We consider two technology libraries: (1) the reduced standard-cell library \textit{mcnc.genlib} (with gate input size <=3) from SIS distribution~\cite{sentovich1992sis} and (2) ASAP 7nm technologies \cite{xu2017standard}.
The GNN-based framework is implemented in Pytorch Geometric~\cite{Fey/Lenssen/2019}.
Two GraphSAGE models are developed for simple and complex design netlists: (1) a shallow 4-layer model with the hidden channel of 32 (for CSA multipliers w/ and w/o simple technology mapping), and (2) a deep 8-layer model with the hidden channel of 80 (for Booth multipliers and after complex technology mapping).
The generated node embeddings are passed to a shared linear layer with size of 32 and the ReLU activation function, followed by another linear transformation with softmax for each sub-task to perform node classification.
Experiments are performed on a Linux host with 256 AMD EPYC 7742 64-core CPUs and one NVIDIA A100 SXM 40GB GPU.
\textit{
In general, \textsc{Gamora} is trained on small bitwidth multipliers (typically less than 32-bit) and evaluated on large bitwidth multipliers (up to 2048-bit).}

% mean aggregation

% \textbf{Emphasis on Generalization Capability.}
% Unlike many ML-based approaches that are trained with complex designs and infer on simpler ones, 


\begin{figure*}[ht]
    \centering
   % \vspace{-30pt}
    \includegraphics[width=\textwidth]{figures/0_training.pdf}
   % \vspace{-28pt}
    \caption{Sensitivity analysis on CSA multipliers with respect to (1) the bitwidth of multipliers for training (ranging from 2-bit to 10-bit), (2) single/multi-task, and (3) whether employing functional information.}
   % \vspace{-12pt}
    \label{fig:sensitivity}
\end{figure*}

\begin{figure*}[h]
    \centering
    %\vspace{-30pt}
    \includegraphics[width=\textwidth]{figures/0_techmapping.pdf}
   % \vspace{-24pt}
    \caption{Evaluation on CSA and Booth multipliers, with simple and complex technology mapping.}
    %\vspace{-6pt}
    \label{fig:techmapping}
\end{figure*}

\begin{figure}[h]
    \centering
    %\vspace{-6pt}
    \includegraphics[width=\linewidth]{figures/0_booth.pdf}
   % \vspace{-25pt}
    \caption{Evaluation on Booth multipliers with shallow and deep models.}
   % \vspace{-20pt}
    \label{fig:booth}
\end{figure}




\vspace{-5pt}
\subsection{Evaluation on Reasoning Performance}

We evaluate the reasoning performance from three aspects:
(1) how functional and structural information influence the reasoning precision;
(2) how design complexity affects model selection and training;
(3) how technology mapping complicates the reasoning process and what domain insights can be derived to facilitate more accurate symbolic reasoning on complex BNs.


\vspace{0pt}
\subsubsection{Reasoning Precision Analysis}
Figure~\ref{fig:sensitivity} illustrates how the reasoning performance on carry-save array (CSA) multipliers is affected by different bitwidth multipliers for training, single/multi-task setting, and the employment of functional information.
First, the larger bitwidth multiplier is adopted for training, the higher reasoning precision can be achieved, which typically converges after training with 8-bit multipliers.
The main reason is for CSA multipliers, an 8-bit multiplier is able to provide a sufficient variety of structural properties, which can be learned and well generalized to larger multipliers by \textsc{Gamora}.
Second, the multi-task setting conspicuously outperforms the single-task counterpart, indicating that the knowledge sharing across multiple tasks greatly benefits the prediction accuracy of every single task.
Third, there is always a boost of accuracy when employing functional information for prediction, since identifying the role of each node relies on not only the surrounding structure but also the function of itself and its neighbors.
The synergy of structural and functional information in \textsc{Gamora} is analogous to the combination of structural hashing and functional propagation in conventional symbolic reasoning.

With the multi-task setting and simultaneously fusing structural and functional attributes, \textsc{Gamora} achieves almost 100\% prediction accuracy in symbolic reasoning for CSA multipliers.
It is noted that several nodes near the least significant bit (LSB) are always mispredicted due to their shallow neighborhood structure, as shown in Figure~\ref{fig:overview}(e).
This means the HA at LSB cannot be automatically extracted, but can be easily corrected during post-processing.

\vspace{0pt}
\subsubsection{The Impact of Design Complexity}
We analyze the impact from design complexity by evaluating the reasoning performance on radix-4 Booth-encoded multipliers, as shown in Figure~\ref{fig:booth}.
From the model selection aspect, as Booth multipliers generally have more complex structures, deeper models are necessary to characterize neighborhood structures and provide informative node embeddings, thus guaranteeing high prediction accuracy.
From the training aspect, larger multipliers (i.e., up to 24-bit Booth multiplier) are required for training such that adequate variety and representativeness of structural and functional characteristics are exposed to and well captured by \textsc{Gamora}.




\vspace{0pt}
\subsubsection{The Impact of Technology Mapping}
Figure~\ref{fig:techmapping} depicts the reasoning performance on CSA and Booth multipliers after simple technology mapping~\cite{sentovich1992sis} and ASAP 7nm technology mapping~\cite{xu2017standard}. 
Specifically, the ASAP 7nm library contains 161 standard-cell gates, including multi-output cells such as the Full-Adder cell, which significantly increases the complexity and irregularity of post-mapping netlists. 
It is a known challenge that technology mapping can increase the complexity of formal reasoning on BNs~\cite{li2013wordrev,subramanyan2013reverse,yu2016formal}. 
Thus, we evaluate the performance of \textsc{Gamora} with respect to technology mapping. The multipliers are mapped using the ABC standard-cell mapper (command \texttt{map}).


In the simple technology mapping case, the models trained before technology mapping demonstrate good generalization capability, still reaching over 99\% and 92\% prediction accuracy for CSA and Booth multipliers, respectively;
with retraining, comparable reasoning performance to those on original multipliers is achieved with similar sizes of training multipliers.
The scenario is fairly different in the case of ASAP 7nm technology mapping, which employs a relatively complex technology library:
first, the generalization capability is limited before and after technology mapping;
second, the prediction accuracy slightly drops even with retraining;
third, it is necessary to use large training multipliers to guarantee performance.

These observations imply several takeaways.
First, the more complex technology library is applied, the more difficult it is for learning-based symbolic reasoning, since more complexity is involved both in AIG structures and the functionality of each node. This also implicates attributes related to the technology library should be included in node and edge features.
Second, the capability to cope with intricate AIG netlists comes at the expense of more comprehensive training data.
One underlying assumption of many supervised ML tasks is the training and testing data should be independent and identically distributed, which is governed by a fundamental principle called empirical risk minimization that provides theoretical performance bounds~\cite{vapnik1991principles}.
Thus, increasing the size of training data can envelop more knowledge of interested statistical properties, ensuring better generalization to testing data.

\vspace{-5pt}
\subsection{Runtime and Scalability Analysis}
In addition to the high reasoning performance, we  demonstrate the superiority of \textsc{Gamora} by analyzing its runtime and scalability.

\begin{figure}[ht]
    \centering
    %\vspace{4pt}
    \includegraphics[width=\linewidth]{figures/0_runtime.pdf}
    %\vspace{-5pt}
    \caption{Runtime comparison between \textsc{Gamora} and ABC. Note that the number of nodes $|V|$ and the number of edges $|E|$ are annotated for scalability analysis.}
    %\vspace{-5pt}
    \label{fig:runtime}
\end{figure}


\begin{figure}[h]
    \centering
    %\vspace{4pt}
    \includegraphics[width=1.0\linewidth]{figures/0_batch.pdf}
   % \vspace{-25pt}
    \caption{Average runtime and GPU memory consumption with batched reasoning, where the batch size is denoted as bs. We currently focus on single-GPU implementation.}
    %{\color{red}Cunxi: we can take this part out for DAC I think. Label multiGPU runtime in plot (Steve)}
   % \vspace{-5pt}
    \label{fig:batch}
\end{figure}

\noindent
\textbf{Runtime complexity analysis.}
Basically, the runtime only relates to the scale of AIGs, i.e., the number of nodes $|V|$ and the number of edges $|E|$.
Figure~\ref{fig:runtime} compares the runtime of \textsc{Gamora} against ABC on CSA multipliers:
for large designs such as a 2048-bit CSA multiplier with around 34 million nodes and 67 million edges, \textsc{Gamora} attains a speedup of up to six orders of magnitude.
% Notably, commercial tools are often slower than ABC by around three orders of magnitude \red{[ref]}.
This shows not only the great efficiency in symbolic reasoning enabled by graph learning but also the scalability to extreme large designs.

\noindent
\textbf{Batched reasoning with single GPU.}
Figure~\ref{fig:batch} shows further acceleration allowed by batched reasoning.
Currently, we focus on single GPU implementation, which limits the batch size by the GPU memory, and leave multi-GPU implementation as our future work to support larger batch processing.
Even with a single GPU, there already reveal promising results and positive trends benefiting from parallel execution and GPU acceleration.






