
\begin{figure*}[t]
    \centering
    %\vspace{-20pt}
    \includegraphics[width=\textwidth]{figures/overview.pdf}
   % \vspace{-20pt}
    \caption{Overview of \textsc{Gamora}. (a) \textsc{Gamora} takes in flattened netlists in AIG format and performs multi-task node classification to reason the Boolean function of each node, after which the adder tree within multiplier netlists can be automatically extracted to improve the efficiency of word-level abstraction.
    (b) AIG of a 3-bit CSA multiplier after synthesis. 
    (c) Annotated AIG with the Boolean function of each node, using the ground truth provided by ABC.
    (d) Adder tree extracted based on the \textit{exact} reasoning, including three FAs and three HAs.
    (e) Adder tree extracted based on the reasoning performed by \textsc{Gamora}.}
    %\vspace{-12pt}
    \label{fig:overview}
\end{figure*}
%\vspace{-5pt}
\section{Proposed Approach}

\subsection{Overview}

\noindent
\textbf{Problem Formulation.}
Figure~\ref{fig:overview}(a) illustrates the overview of \textsc{Gamora}.
The inputs are flattened gate-level netlists in AIG format, without any micro-architectural or RTL information.
These AIGs are generated by the logic synthesis tool ABC~\cite{brayton2010abc}.
The goal is to exploit a multi-task GNN to reason high-level abstractions by performing node-level classification on AIGs, after which functional blocks (e.g., adders) can be extracted based on the annotated AIGs.

\noindent
\textbf{Case Study on Multipliers.}
Integer multipliers are indispensable to computationally intensive applications, such as signal processing and cryptography applications.
Recent years also witness the strong demand for large integer multipliers in homomorphic encryption~\cite{acar2018survey}.
In general, formal multiplier verification is challenging, especially for structurally complex designs such as Booth multipliers ~\cite{ciesielski2019understanding,mahzoon2019revsca,temel2021sound}.
Symbolic computer algebra (SCA) has been successfully employed to verify a variety of integer multipliers~\cite{yu2017fast,ciesielski2019understanding,mahzoon2019revsca,kaufmann2019verifying,alireza2022formal}, which relies heavily on detecting full adders (FAs) and half adders (HAs) in multiplier netlists.
The state-of-the-art implementation in ABC framework~\cite{yu2017fast} develops a fast algebraic rewriting approach to extracting adder trees from flattened multiplier netlists by detecting pairs of XOR and MAJ functions, which can handle large bitwidth multipliers (up to 2048-bit) but with extremely long runtime.
Thus, targeting integer multipliers, we leverage GNNs to identify XOR and MAJ functions to extract adders from flattened netlists, which improves the efficiency of word-level abstraction from BNs and has strong scalability enabled by GPU acceleration.

%\vspace{-5pt}
\subsection{Multi-Task Learning for Boolean Reasoning}

Boolean reasoning requires structural and functional information from neighbor nodes, a process that can be imitated by the message passing mechanism in GNNs.
The task of reasoning high-level abstractions from flattened netlists, i.e., from AIGs to adders, involves a two-step procedure~\cite{li2013wordrev,subramanyan2013reverse,yu2017fast}: (1) detecting XOR/MAJ functions to construct adders, and then (2) identifying their boundaries. Therefore, we propose to apply multi-task learning (MTL) for Boolean reasoning to approach its nature, in which knowledge sharing across sub-tasks provides higher reasoning precision.
This section details (1) how structural and functional information are fused in node embeddings, (2) how the two-step reasoning is formulated as a multi-task node classification, and (3) the post-processing after performing reasoning on each node in AIGs.

%\vspace{-5pt}
\subsubsection{Fusing Structural and Functional Information}
%AIGs represent the structural implementation of logical functionality of circuit netlists.
We leverage the message propagation and neighborhood aggregation in GNNs to generate the node embeddings of AIGs that simultaneously fuse structural and functional information.
First, the structural information is distilled by passing node embeddings along edges that connect them.
Second, the Boolean functional information can be encoded in node features.
For each node, there are three node features represented in binary values denoting node types and Boolean functionality.
The first node feature indicates whether this node is a PI/PO or intermediate node (i.e., AND gate).
The second and the third node features indicate whether each input edge is inverted or not, such that AIGs can be represented as homogeneous graphs without additional edge features.
These compressed node features not only encapsulate Boolean functionality of each node but also enable high compute and memory efficiency.
Figure~\ref{fig:overview}(b) shows the AIG of a 3-bit CSA multiplier, in which the structural information is presented in the AIG topology.
The functional information is encoded in node features.
For example, node 1 is a PI with the feature vector [0, 0, 0]; node 7 is an internal node without negation on inputs, so the feature vector is [1, 0, 0]; node 17 has two inputs inverted, with the feature vector [1, 1, 1].

With the emphasis on generalization from simple to complex designs, the specific model employed is GraphSAGE~\cite{hamilton2017inductive}.
Given a GraphSAGE model with $K$ layers, the node embeddings propagated between different layers is as follows:

\begin{equation}
\begin{split}
    & h^k_{\mathcal{N}(v)} \gets \textsc{aggregate}_k(\{h_u^{k-1}, \forall u \in \mathcal{N}(v)\}); \\
    & h_v^k \gets \sigma(\textbf{W}^k \cdot \textsc{concat}(h^{k-1}_v, h^k_{\mathcal{N}(v)})).
    % & h_v^k \gets \sigma(\textbf{W}^k \cdot \textsc{mean}(\{h^{k-1}_v\} 
    % \cup \{h_u^{k-1}, \forall u \in \mathcal{N}(v)\}).
\end{split}
\label{eq:message}
\end{equation}

\noindent
Here, $\mathcal{N}(v)$ is the immediate neighborhood of node $v$;
$\textsc{aggregate}_k$ and $\textbf{W}^k$ are the aggregation function and the weight matrix for layer $k$, respectively, where $\forall k \in \{1, ..., K\}$.
After stacking $K$ layers, the structural and functional information within $K$-hop search depth is fused in the embedding of each node.





\subsubsection{Multi-Task Classification}
We identify the Boolean function of each node by multi-task node classification to approach the nature of the problem: there are two steps involved in reasoning functional blocks from unstructured AIGs.
The first step detects XOR and MAJ functions from AIGs, which will be used to construct adders.
Since each XOR/MAJ function consists of multiple nodes in AIGs, only the root nodes of these functions are labeled as XOR/MAJ with other nodes marked as plain nodes.
In addition to the exact XOR/MAJ functions, negation-permutation-negation equivalent functions are also labeled as XOR/MAJ.
The second step aims to automatically identify the boundaries of HAs and FAs, and thus we label roots (i.e. the sum and the carry-out functions) and leaves of each adder.
Figure~\ref{fig:overview}(c) shows a multi-label annotated AIG of a 3-bit multiplier, using the ground truth provided by ABC.
Notably, one node can have multiple labels.
For example, node 20 is labeled with XOR and the root of an adder; node 17 is labeled with XOR.

The MTL not only follows the intuition of this two-step reasoning but also exploits divide and conquer, since it is extremely hard for GNNs to reach high prediction accuracy with a single-task multi-label node classification.
The employment of MTL enables knowledge sharing across sub-tasks and improves sample efficiency during training, which guarantees high reasoning performance.
Specifically, the two-step reasoning is decoupled into three simpler classification tasks using generated node embeddings:
\textit{Task 1} classifies the roots and leaves of adders;
\textit{Task 2} and \textit{Task 3} detect XOR and MAJ nodes, respectively.
We use hard parameter sharing for MTL and the overall loss function $\mathcal{L}$ is shown below:
\begin{equation}
\label{eq:loss}
    \mathcal{L} = \alpha \cdot \ell (\hat{y_1}, y_1) + \beta \cdot \ell(\hat{y_2}, y_2) + \gamma \cdot \ell(\hat{y_3}, y_3),
\end{equation}
in which $\ell$ is the negative log-likelihood between predictions and the ground truth, and $\alpha$, $\beta$, and $\gamma$ are hyper-parameters to adjust the importance of each task.
In our implementation, $\alpha = 0.8$ and $\beta = \gamma = 1$.





\subsubsection{Adder Tree Extraction from Multi-Labeled Graphs}
After performing the multi-task node classification, we can recognize XOR, MAJ, and root nodes of adders.
The XOR and MAJ pairs with identical inputs are matched to construct adders.
%The pairing process follows a reverse topological ordering from POs to PIs, finally conducted in ABC~\cite{yu2017fast}.
The conversion from Figure~\ref{fig:overview}(c) to \ref{fig:overview}(d) depicts the adder tree extraction.
In Figure~\ref{fig:overview}(c), the AIG has a set of XOR nodes $\mathbb{X}=$ \{12, 17, 20, 24, 29, 33, 36, 41, 44\} and a set of MAJ nodes $\mathbb{M}=$ \{10, 22, 25, 27, 37, 45\}.
After removing the nodes that are not marked as adder roots, $\mathbb{X}=$ \{12, 20, 24, 29, 36, 44\}.
Given $\mathbb{X}$ and $\mathbb{M}$,
node 12 is XOR3(8, 9, 0) and node 10 is MAJ3(8, 9, 0), a three-input XOR/MAJ function with node 8, node 9, and the constant zero as the inputs;
node 20 is XOR3(10, 13, 14) and node 15 is MAJ3(10, 13, 14);
this matching process continues until all six pairs of XOR and MAJ are generated, which are three FAs and three HAs, as shown in Figure~\ref{fig:overview}(d).

Notably, \textsc{Gamora} adopts graph learning to mimic the \textit{exact} reasoning.
%The mispredictions do have impacts on reasoning high-level abstractions.
In Figure~\ref{fig:overview}(e), one HA cannot be automatically extracted due to the misprediction of node 10. 
Our evaluation indicates only several nodes near the least significant bit are always mispredicted due to their shallow neighborhood structure, which has a subtle impact on the efficiency of algebraic rewriting.
By fusing structural and functional information into node embeddings and using MTL to approach the reasoning nature, 
\textsc{Gamora} is expected to reach as close as possible to the \textit{exact} reasoning precision.

