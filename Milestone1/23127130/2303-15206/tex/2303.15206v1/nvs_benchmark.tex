\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv_2020}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
%\usepackage{float}


\usepackage{booktabs}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{nicematrix}
\usepackage{gensymb}
%\usepackage{subfigure}
\usepackage{subcaption}


\input{macros}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage{hyperref}
\hypersetup{breaklinks=true,bookmarks=false}
\usepackage[capitalize]{cleveref}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{3341} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Perceptual Quality Assessment of Neural View Synthesis Methods\\for Reconstruction of Front-Facing Views
%}
\title{Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods \\ for Front-Facing Views}

%\author{Hanxue Liang\\
%University of Cambridge\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Tianhao Wu\\
%University of Cambridge\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
%\and
%Param Hanji\\
%University of Cambridge\\
%\and
%Francesco Banterle\\
%University of Cambridge\\
%\and
%Hongyun Gao\\
%University of Cambridge\\
%\and
%Rafal Mantiuk\\
%University of Cambridge\\
%\and
%Cengiz Oztireli\\
%Google Research\\
%University of Cambridge\\
%}
\author{
Hanxue Liang\textsuperscript{\rm 1 }\thanks{This work was supported by a UKRI Future Leaders Fellowship [grant number G104084]},
Tianhao Wu\textsuperscript{\rm 1 },
%$^{*}$,
 Param Hanji\textsuperscript{\rm 1},
Francesco Banterle\textsuperscript{\rm 2},
Hongyun Gao\textsuperscript{\rm 1}, \\ 
Rafal Mantiuk\textsuperscript{\rm 1},
Cengiz Oztireli\textsuperscript{\rm 1}\\
\textsuperscript{\rm 1}University of Cambridge
\textsuperscript{\rm 2}ISTI-CNR
}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\maketitle

% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
%Neural Radiance Fields or NeRF has become one of the most prominent techniques for reconstructing real-world 3D scenes and synthesizing free viewpoint videos. A neural density field with view-dependent radiance simplifies the physically-based rendering model and removes the need for accurate appearance and lighting estimation. \RM{I would say that NeRF with "baked-in" illumination has a very different application than the methods that try to estimate a reflectance function. I would not mix the two families of methods in the abstract. A better comparison is to MVS, but we do not have a MVS method in our benchmark.} This can, therefore, synthesize novel views from only a sparse set of RGB photographs. 
%\CO{} Neural view synthesis is one of the most successful techniques for synthesizing free viewpoint videos. This approach can synthesize high-fidelity novel views from only a sparse set of captured images. This success has led to many variants of the techniques. Although there have been efforts to understand which design decisions should be preferred under given conditions, such analyses have so far been based on standard and low-level image quality metrics such as PSNR, SSIM, or LPIPS. Thus, there has been a lack of research on how neural view synthesis methods perform with respect to the perceived video quality for the human visual system (HVS). We provide the first study on perceptual evaluation of neural view synthesis and NeRF variants. Via a robust protocol and thorough analysis, we compare eight recent techniques both in a subjective video quality experiment and in objective experiments using established image and video quality assessment metrics. To capture a variety of effects, we introduce a new video dataset for forward-facing captures of carefully designed scenes.

%\RM{my attempt to refocus the abstract on the main problems we address:} Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using PSNR, SSIM, and LPIPS metrics. However, there are three fundamental problems with such evaluation:
%\WW{I feel claim (3) could be a bit weird? We really only evaluated LLFF which is one of the very simple datasets out there.)} \hx{problem (2) is more important than (1), so they should switch order. and also we didn't really prove problem and (3) as we didn't do many investigation on existing datasets}
%(1) since NVS methods are meant for free-view exploration of 3D scenes, they should be tested on videos rather than isolated views; (2) we do not know how reliable image quality metrics are in this task;\WW{I think we should clearly point out perceived video quality for the human visual system}\FB{We should switch (1) and (2)}, and (3) we do not know whether the existing datasets are sufficient to discriminate between different NVS methods.\hx{I suggest remove point3 and in introduction accordingly} To address those problems, we collected two new datasets, which include reference video sequences, one captured in a laboratory, and the other in-the-wild using a gimbal or a camera slider. Then, we measured the quality of 8 NVS methods (including 2 variants) with their variants in a well-controlled image quality assessment experiment on the two new datasets and on the popular LLFF dataset. We observed that most NVS methods performed worse on our new datasets than on the existing LLFF dataset. Furthermore, we found that the quality metrics cannot well differentiate between the NVS methods on the LLFF dataset, and both LPIPS and SSIM perform poorly (worse than  PSNR) in this task. One of the new datasets, captured in-the-wild, poses a much bigger challenge to the NVS methods let us better quantify their performance using video quality metrics (VMAP and FovVideoVDP) and image metrics (PSNR and FSIM). We recommend using those metrics, combined with challenging in-the-wild datasets to evaluate NVS methods. 

Neural view synthesis (NVS) is one of the most successful techniques for synthesizing free viewpoint videos, capable of achieving high fidelity from only a sparse set of captured images. This success has led to many variants of the techniques, each evaluated on a set of test views typically using image quality metrics such as PSNR, SSIM, or LPIPS. There has been a lack of research on how NVS methods perform with respect to perceived video quality. We present the first study on perceptual evaluation of NVS and NeRF variants. For this study, we collected two datasets of scenes captured in a controlled lab environment as well as in-the-wild. In contrast to existing datasets, these scenes come with reference video sequences, allowing us to test for temporal artifacts and subtle distortions that are easily overlooked when viewing only static images.
We measured the quality of videos synthesized by several NVS methods in a well-controlled perceptual quality assessment experiment as well as with many existing state-of-the-art image/video quality metrics. We present a detailed analysis of the results and recommendations for dataset and metric selection for NVS evaluation.

%We observed that most NVS methods perform worse and that quality metrics can differentiate between the NVS methods better on our new datasets. 
%, and both LPIPS and SSIM perform poorly (worse than  PSNR) in this task. One of the new datasets, captured in-the-wild, poses a much bigger challenge to the NVS methods let us better quantify their performance using video quality metrics (VMAP and FovVideoVDP) and image metrics (PSNR and FSIM). We recommend using those metrics, combined with challenging in-the-wild datasets to evaluate NVS methods. 

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Synthesizing photorealistic novel views of a complex scene from a sparse set of RGB images is a fundamental challenge in image-based rendering. Various representations and methods have been developed to accurately model the image formation process and handle complex geometry, materials, and lighting conditions \cite{chen1993view,shade1998layered,shum2000review,idr,diff_sdf}. More recently, \ac{NVS} via implicit representations has shown promising results. In particular, methods such as \ac{NeRF} \cite{original_nerf} and its successors \cite{mipnerf,mipnerf360,plenoxel,dirctvoxgo,ibrnet,NeX} have attracted great interest due to their outstanding fidelity and robustness. However, assessing the performance of contemporary \ac{NVS} methods is not straightforward as it is closely tied to the final applications. 
For example, to deploy \ac{NVS} methods in immersive and realistic AR/VR applications, it is crucial for the methods to synthesize high-quality free-viewpoint videos with unnoticeable artifacts.
%\hx{} Specifically, \ac{NVS} methods  are increasingly developed in immersive and realistic AR/VR application, it is thus crucial for the methods to synthesize high-quality free-viewpoint videos with unnoticeable artifacts to human users.

% in order to create of immersive and realistic environments. \WW{I'd refrain from distracting into 360 views etc and focus on our setting: videos and subjective experiments}
% assessing the accuracy of a 3D scene reconstruction method requires 360-views~\cite{original_nerf}, while most AR/VR applications need high-quality and artifact-free front-facing videos of 3D objects \PH{We should also indicate here that we focus on front-facing scenes. I'm not sure how to motivate front-facing.}\WW{I don't think we have very good motivation on this, its more of a limitation on the methods & our dataset capture}.

% % Reconstructing complex real-world scenes
%  Synthesizing photo-realistic novel views of a scene based on a sparse set of RGB images is a fundamental challenge in visual computing.
%  % \RM{I would say that is graphics. Computer vision is not about rendering but scene understanding.} \WW{I think graphics focus more on the methods of rendering, while our work is more about assessing their reconstruction quality, as for all methods the process of rendering is the same (volume rendering). Do you think it makes sense?}. \CO{View synthesis is a form of image-based rendering, so more graphics than vision. I'd say: visual computing.} \WW{Modified}
%  % Various representations and methods have been developed to achieve accurate and robust reconstruction, such as estimating geometry, light source, texture, and material properties via physically-based inverse rendering, or interpolating a sparse light field represented by given images \PH{Add some citations for each type?}. More recently, scene reconstruction via neural rendering and neural implicit representations has shown promising results. Methods such as neural radiance field (NeRF) \cite{original_nerf} have attracted much interest due to their outstanding robustness and fidelity in novel view interpolation.
%  %, and have motivated various extensions and applications such as scene editing \cite{Liu21editing, Yuan22editing}, time-dependent scene modeling \cite{Pumarola_2021_CVPR, NEURIPS2021_7d62a275}, HDR representations \cite{Mildenhall22HDR}, different scales from human face \cite{Sun22Face} to entire cities \cite{Tancick22city, Xiangli22city, Turki22city}.
%  Various representations and methods have been developed to mimic the image formation model as well as handling complex geometry, materials, and lighting conditions~\cite{chen1993view,shade1998layered,shum2000review,idr,diff_sdf}.
%  %to achieve accurate and robust reconstruction of 3D scenes~\cite{idr, diff_sdf,suhail2022generalizable,lfnr}.
%  %For example, physically-based inverse rendering is used to estimate geometry, light sources, texture, and material properties \cite{idr, diff_sdf}, while light field rendering allows synthesizing novel views by interpolating from a sparse set of images \cite{suhail2022generalizable,lfnr} \WW{Can remove the above.}. 
%  More recently, \ac{NVS} via implicit representations
%  % neural volume rendering and implicit representations 
%  has shown promising results. Particularly, methods such as neural radiance field (NeRF) \cite{original_nerf} and its successors~\cite{mipnerf,mipnerf360,plenoxel,dirctvoxgo,ibrnet,NeX} due to their outstanding robustness and fidelity in novel views interpolation. 
%  % \PH{Not sure why NeRF extensions were omitted} \WW{Thx!}

%\PH{Version 1}
%\hx{}Nonetheless, the current protocol for comparing \ac{NVS} methods merely involves computing image quality metrics, such as PSNR, SSIM \cite{ssim} and LPIPS \cite{lpips}, on a subset of hold-out views for a few scenes. Even dedicated benchmarks \cite{scannerf,nerf_robust_benchmark} follow the same evaluation protocol. Since a main objective of NVS methods is to offer an interactive exploration of novel views for human users, we argue that those methods should also be evaluated by comparing with \emph{human preferences}, and on \emph{video sequences} rather than individual sparse views. More formally, two key limitations in existing evaluation protocol are: \ding{202} It relies exclusively on image quality metrics, which can be problematic because these metrics may not correlate well with subjective judgments, especially on a task they are not designed for~\cite{hanji2022comparison}. Since most of the image quality metrics have not been trained or validated on the distortions from novel view synthesis, their predictions could be too noisy to quantify perceived quality. \ding{203} the evaluation protocol lacks assessment on video sequences, which can reveal temporal artifacts and subtle distortions such as flickering, juddering and other visual impairments that are perceptible to human visual system but difficult to spot in static images.\hx{@PH,add citations.} 
%This issue is compounded by the limited nature of commonly used NVS datasets, which only contain sparsely located test views and do not allow for the generation of smooth and high-framerate videos. 
%Compounding this issue is the fact that the widely-used NVS datasets only contain sparsely located test views, which are insufficient to generate smooth and high-framerate videos. 
%Due to the aforementioned reasons, such datasets can be insufficient to evaluate perceived quality differences between NVS methods, \hx{which  has also been demonstrated in our experiments.} 

%we argue that those methods should be tested on \emph{video sequences} rather than individual sparse views, and their evaluation protocol should also take acount of \emph{human preferences}. More formally, several key limitations in existing evaluation protocol: \ding{202} \ww{They lack evaluations on video sequences, which can} reveal temporal artifacts and subtle distortions such as flicker, judder etc. that are difficult to spot in static images. \ding{203} The problem with relying exclusively on image quality metrics is that they may not correlate well with subjective judgments, especially on a task they are not designed for~\cite{hanji2022comparison}. Since most of the image quality metrics have not been trained or validated on the distortions from novel view synthesis, their predictions could be too noisy to quantify perceived quality. \ding{204} Finally, the widely-used NVS datasets only contain sparsely located test views that are insufficient to generate smooth and high-framerate videos. Such datasets can be too limited to evaluate perceived quality differences between NVS methods due to the aforementioned reasons. \PH{Points 1 and 3 make similar arguments --- the lack of video}\WW{I agree, could just merge 3 into 1}


%\PH{Version 2} The current protocol for comparing \ac{NVS} methods involves computing image quality metrics, such as PSNR, SSIM~\cite{ssim} and LPIPS~\cite{lpips}, on a subset\hx{sparse set} of hold-out views (referred to as the test-split) for a few scenes. Even dedicated benchmarks \cite{scannerf,nerf_robust_benchmark} follow the same evaluation protocol. However, since a main objective of NVS methods is to offer an interactive exploration of high-fidelity novel views, we argue that those methods should be tested by comparing them to \emph{human preferences}, and on \emph{video sequences} rather than individual sparse views. This is because video sequences can reveal temporal artifacts and subtle distortions \ww{such as flicker, judder, etc.} that are difficult to spot in static images. Another problem with relying exclusively on image quality metrics is that they may not correlate well with subjective judgments. Most image quality metrics have not been trained or validated on the distortions specific to novel view synthesis \PH{optional: we could briefly describe NVS artifacts} and, thus, their predictions could be too noisy to quantify perceived quality. 

%\CO{I changed the first paragraph a bit here.}
%\hx{do you think we should mention dataset problem in this paragraph? I think we should point out as motivation to what we did:} 
%\CO{we already mention dataset problem  by the second point \ding{203} i think}
The current protocol for comparing \ac{NVS} methods involves computing image quality metrics, such as PSNR, SSIM \cite{ssim} and LPIPS \cite{lpips}, on a subset of hold-out views for a few scenes. Even dedicated benchmarks \cite{scannerf,nerf_robust_benchmark} follow the same evaluation protocol. Since the main  objective of NVS methods is to offer interactive exploration of novel views, we argue that those methods should be evaluated on \emph{video sequences} rather than individual sparse views, ideally in a subjective quality evaluation experiment. Thus, we identify two key limitations in existing evaluation protocols. \ding{202} They rely exclusively on image quality metrics, which can be problematic because these metrics may not correlate well with subjective judgments, especially when used for a task they are not designed for~\cite{Cadik2012a,hanji2022comparison,Ponomarenko2015}. Since most of the image quality metrics have not been calibrated or validated on the distortions specific to novel view synthesis, their predictions could be too noisy to quantify perceived quality. \ding{203} The evaluation protocol lacks assessment on video sequences, which can reveal temporal artifacts and subtle distortions, such as flickering or floating ghost images, that are easily noticeable in video but difficult to spot in static images~\cite{judder,Denes2020flicker,vmaf1,fvvdp}.
% \hx{@PH, please add citations.}
This issue is compounded by the limited nature of commonly used NVS datasets, which do not have reference videos for evaluating NVS methods.
%\hx{This might be because of the lack of NVS datasets with video references. Due to the aforementioned reasons, current datasets can be insufficient to evaluate perceived quality differences between NVS methods, which  has also been demonstrated in our experiments.}
%\hx{like: This issue is compounded by the limited nature of commonly used NVS datasets, which do not have smooth and high-framerate reference videos for evaluating NVS methods. }
%Due to the aforementioned reasons, such datasets can be insufficient to evaluate perceived quality differences between NVS methods, which  has also been demonstrated in our experiments.
% Finally, the datasets used to test NVS methods typically contain a small number of well-curated scenes, on which the NVS methods perform relatively well \cite{original_nerf, llff, NSVF}. Such datasets could be too limited to show performance differences between NVS methods.
% Finally, the widely-used \ac{NVS} datasets only contain sparsely located test views that are insufficient to generate smooth and high-framerate videos. Such datasets could be too limited to demonstrate perceived quality differences between NVS methods due to the aforementioned reasons.
% \WW{I'm slightly worried if the original claim about existing dataset would be a bit aggressive? I re-wrote one for now, please tell me what you think} \PH{Looks good to me}

\begin{figure*} %[tb]
    \centering
    \includegraphics[width=\textwidth]{img/scenes_subset.pdf}
    \caption{A subset of scenes from our collected \dslab{} (first row) and \dsfieldwork{} (second row) datasets. %forward-facing NeRF video dataset used for subjective evaluation. 
    Our datasets include both controlled laboratory and in-the-wild scenes, each with reference video sequences.
    %lab, indoor as well as outdoor scenes.   
    We selected a diverse range of objects composed of various materials such as wood, marble, metal, and glass etc. %\WW{This sentence is only for Lab scene? Then should state clearly}
    %We selected a wide variety of objects covering different materials (wood, marble, metal, glass, etc.) 
    }\label{data-vis}
    \vspace{-3mm}
\end{figure*}

To address these problems, we first collect two new datasets: a \dslab{} dataset captured using 2D gantry in well-controlled laboratory conditions, and a \dsfieldwork{} %\WW{Note that this font on Fieldworld looks a bit weird (pointed out by @cengiz), we may want to change to a different one?} 
dataset, captured in-the-wild with the help of either a gimbal or a camera slider (\secref{dataset}). Each captured scene contains several sparse training views and a reference test video intended to evaluate \ac{NVS} methods.
We use the two new datasets together with the popular \dsllff{} dataset \cite{llff} to reconstruct the video sequences by 8 NVS methods and 2 variants of generalizable NVS methods. %  (including 2 fine-tuned generalizable NVS variants).  %including two per-scene fine-tuned variants \ww{with a per-scene fine-tuned setting applied to generalizable methods} \WW{I'm worried that reader might be confused by so-called "variants", so I tried to explain it. Feel free to change}. 
% \PH{The previous sentence is little confusing. How about "We compare 10 existing NVS methods (including 2 fine-tuned generalizable NVS variants) by assessing the quality of reconstructions of scenes from the two new datasets and the popular \dsllff{} dataset~\cite{llff}". TODO: if this is fine, edit the next sentence to fit} \WW{As we are focusing on the dataset rather than the methods, I'd argue the current way is fine. What do you think?} \PH{Yes, this looks good. THanks}
The output videos of these methods are then evaluated by human participants in a subjective quality assessment experiment (\secref{iq-experiment}). The results of that experiment serve as ground-truth scores for testing how well the existing image and video quality metrics can predict the perceptual performance of \ac{NVS} methods (\secref{metrics}). %Finally, for each top-performing metric identified, we estimate the minimum score difference required to indicate an improvement in perceptual quality at the significance level of $\alpha=0.05$.
%Finally, we estimate what is the minimum difference that each of the top-performing quality metrics needs to indicate improvement in image quality at the significance level of $\alpha=0.05$ (\secref{min-diff}).
%\PH{Rewrite sentence as "Finally, for each top-performing metric identified, we estimate the minimum difference required to indicate an improvement in image quality at the significance level of $\alpha=0.05$ (\secref{min-diff})."}

%\hx{@RM, what's this argument from? any citation or experiment support this argument? }\RM{No need for citation. Everyone will recognize that flicker and temporal issues cannot be detected in images.}



%\hx{why we suddenly jump to subjective score of human participants? Before this paragraph, we never talk about what it is and also no motivation about it at all}
% When we conducted an initial experiment to assess the subjective scores from human participants, it was clear that image quality scores of popular metrics like PSNR, SSIM \cite{ssim}, and LPIPS \cite{lpips} \hx{@Walter,PH:PSNR performs decently?}did not always agree with subjective preferences. This motivates a pressing need for an evaluation pipeline that takes human preferences into account. \WW{it feels like we are proposing a novel evaluation pipeline for subjective evaluation, no?} \PH{You're right, we need to make this gentle}. Towards this, we bring to light two key limitations of the existing evaluation protocols. 

% that are relevant to user-centric applications in AR/VR, which require smooth video outputs to create immersive and realistic environments.

% Despite the rapid advancements in \ac{NVS} techniques and the creation of dedicated benchmarks~\cite{scannerf,nerf_robust_benchmark}, there is still a pressing need for a comprehensive evaluation pipeline that takes into account human preferences and evaluates on smooth video space. This gap is particularly notable when applying \ac{NVS} technologies in customer-facing VR/AR systems that require smooth video outputs to create immersive and realistic environments. 
% Videos with minimal temporal artifacts are also crucial for simulating the test environment for robotics and autonomous vehicles.
% The continuous output is also crucial for robotics and autonomous vehicles, where sequential outputs are needed to navigate the environment and make informative decisions.

%Towards this, we bring to light two key limitations of the existing evaluation protocols. First, image quality metrics do not account for the characteristics and limitations of the \ac{HVS} \cite{livingstone1988segregation,lofgrenlaser,fvvdp,eyediagram,Colorblindness,simoncelli1996foundations,BasicVisionBook,watson2013high}. Even with perceptually based metrics such as SSIM~\cite{ssim}, LPIPS~\cite{dists}, or DISTS~
%\cite{dists}, a higher score is not always a strong indication of better perceptual quality; see \ref{SEC:metrics}. 
% \PH{evidence with cross-reference}
% However, existing evaluation protocols typically validate on a subset of hold-out views (denoted as test-split of images) and exclusively rely on automatic image quality metrics such as PSNR, SSIM or LPIPS without any subjective experiments involving human participants. As the human visual system (HVS) has its own characters and limitations~\cite{livingstone1988segregation,lofgrenlaser,fvvdp,eyediagram,Colorblindness,simoncelli1996foundations,BasicVisionBook,watson2013high}
% \FB{I added this book, please check @Rafal?}
% , whether a higher score on objective image metrics is a strong indication of better perceptual quality or not remains to be a question.
% However, despite the rapid development of neural rendering methods and the emergence of specially designed benchmarks \cite{scannerf,nerf_robust_benchmark}, a thorough evaluation pipeline that accounts for the preferences of humans in real-world applications is missing.
% % assesses the methods with respect to the actual consumers has been missing.
% Typically, existing evaluation protocols 
% % validate on a small subset of sparsely located views (denoted as test-split of images). 
% % And they 
% exclusively rely on automatic image quality metrics without including subjective experiments involving human participants.
% % None goes beyond pure metric comparison and extends to an elaborate \textit{subjective evaluation}, where the performance of NeRF-based methods is measured by the actual consumer in real-world applications; i.e., the human.
% As the human visual system (HVS) has its own characters and limitations~\cite{livingstone1988segregation,lofgrenlaser,fvvdp,eyediagram,Colorblindness,simoncelli1996foundations,BasicVisionBook,watson2013high}
% \FB{I added this book, please check @Rafal?}, whether reaching a higher score on objective image metrics means a perceptually-better model remains to be a question. \hx{@rafal, could you please give a short intuition on a specific characteristic of human?}
% However, despite the rapid development of NeRF-related methods, a throughout evaluation pipeline that covers all components of their representation has been missing\hx{what all components, any citation to support it?} . NeRF is a 5D plenoptic function that models both the scene geometry and the view-dependent appearance. Its ability to represent non-Lambertian effects allows it to synthesize high-fidelity novel views of complicated real-world scenes, but also brings in potential ambiguities and artifacts in its reconstruction\hx{which specific artifacts and any citations to support it?}. For example, given a sparse set of captures, it is fully possible to represent the scene with a plane and high-frequency view-dependent appearance \cite{NeRF++}. \WW{Do you think this example helps with the explanation? If not we can remove it.} Existing benchmarks typically evaluate the reconstruction performance by comparing the similarity of synthesized re-renderings with a set of reserved ground truth captures sparsely located within the known camera space via image metrics. Such an evaluation exploits most of the artifacts caused by incorrect geometry\hx{any citation to support it?}, but completely ignores the assessment of consistency and smoothness in the reconstructed view-dependent radiance field\hx{support?}. Additionally, although many recent benchmarks designed specifically for evaluating NeRF-based methods have emerged \cite{nerf_robust_benchmark, more}, none of them goes beyond pure metric comparison and extends to an elaborative subjective study, where the performance of NeRF-based methods is measured by the actual consumer in real world applications, i.e., the human visual system (HVS).
%The second issue with the existing evaluation protocols is the lack of datasets with smooth test videos for evaluation. The images in the existing datasets~\cite{llff, original_nerf} are sparsely captured and can not produce a smooth and high-framerate reference video.
%This is because the per-scene test-splits of most datasets are sparsely sampled and do not produce a smooth, high-framerate video when combined. 
% Being able to render continuous and smooth sequences is essential for applications such as AR/VR \PH{more motivation?}\WW{Can't think of any, any ideas?}. \WW{motivation moved to top}
% , but it is challenging to evaluate such ability via the sparsely located test views in the existing datasets. 
% Our subjective experiment \PH{We haven't stated that we run an experiment yet. First briefly introduce it and then describe what it "reveals"} reveals that the objective metrics computed on the sparse test views of LLFF dataset \cite{llff} have a maximum correlation of only 0.73 to the subjective scores of synthesized videos; see \ref{fig:correlations-all}.
 % \WW{TODO add stronger claims and refer to correlation in LLFF scenes}
%Since the \ac{HVS} shows additional sensitivity to artifacts in the temporal domain due to inconsistency across frames~\cite{fvvdp, deepfovea, judder}, it is imperative to evaluate \ac{NVS} methods using smooth video sequences instead of sparse individual frames.
% Besides, HVS shows additional sensitivity against the artifacts in the video sequences caused by the inconsistency across frames~\cite{fvvdp, deepfovea, judder}, \WW{More insights on HVS would be great} therefore it is necessary for a valid perceptual evaluation of NeRF-based methods to be conducted on video sequences instead of individual frames.

% Another issue is existing forward-facing dataset has similar distributions to training images, which provides an unfair advantage \WW{Do we have well-supported evidence for this claim?}. 


%To address the aforementioned issues and build an improved perceptual benchmark for neural rendering methods, 
%\CO{} To address these issues, we propose an enhanced video benchmark \hx{perceptual benchmark, instead of video benchmark?}\WW{Benchmark is something that other researchers can easily use the evaluate their works. I don't think it's feasible for others to perform similar perceptual evaluation on our benchmarks and compare them to the results we report} and assess NVS methods via computing state of the art perceptual metrics and conducting extensive experiments. We collected a new \datasetName{} dataset consisting of a variety of scenes including 6 carefully-designed scenes captured in a laboratory setting and 9 fieldwork scenes captured in both indoor and outdoor environments. Different from existing forward-facing datasets~\cite{llff,NeX}, each scene in our dataset is densely captured and provides a smooth reference video with accurate pose for each video frame. \hx{why comment out that: the perceptual is conducted on reference video? I think it is important to mention why we need reference video because perceptual benchmark can be conducted on redered video without reference}%Our perceptual assessment is conducted on these videos, taking the sensitivity of the human visual system in the temporal domain into account.
% to utilize the sensitivity of \ac{HVS} in the temporal domain, 
%More importantly, the availability of reference video frames allows for computing objective metrics for our later analysis.
% to effectively compute full-reference video metric~\cite{vmaf0,fvvdp,hdr-vqm} scores
 % and carry out subjective evaluations.
% obtain an
% accurate measure of perceptual visual quality that can be
% achieved by each NeRF method
% Each scene contains a smooth reference video with accurate camera poses. We compare 
%the reconstruction on our collected scenes by 
%We compare 8 representative NVS methods on our dataset, covering models with both explicit and implicit geometric representations, and generalizable and per-scene optimization strategies.
% To evaluate the perceptual quality of these methods, we invited participants and conduct a subjective assessment of their reconstruction quality through a pairwise comparison strategy. \PH{experiment introduced earlier}
%We evaluated 8 NeRF variants on our benchmark with an extensive collection of X video quality metrics and X image metrics, as well as a subjective assessment through pairwise comparison to demonstrate and analyze the accuracy of existing metrics. 
% \WW{To be updated with latest conclusion}
% \hx{Need a leading sentence like: Based on our benchmark experiments, we conduct a thorough analysis of the correlation between the perceptual preference and the current objective metric.} 
%We conduct a comprehensive correlation analysis between computed metric scores and perceptual scores from our experiments.%,\hx{this meant to be leading sentence of this paragraph. @CO,RM, could you suggest a better one?} 
% revealing unstatisfactory and limited performance 
% Our analysis reveals unsatisfactory correlations between the widely used image metrics and the subjective scores. 

%\CO{Commented out a paragraph here, see the latex file.} \hx{why comment out this para}%Our analysis reveals unsatisfactory correlations, where the best metric computed on the sparse test views of LLFF dataset \cite{llff} has only 0.73 Spearman correlation to the subjective scores of synthesized videos. In particular, the widely used LPIPS-VGG only has a correlation of 0.41 on LLFF scenes and 0.48 on our lab scenes; see Figure \ref{fig:correlations-all}. We suggest future works on neural view synthesis to evaluate challenging real-world scenes with dense test views or test videos and include video metrics such as VMAF \cite{vmaf0} to achieve the most accurate quality assessment with respect to HVS. We also report an estimate of the minimal difference in the widely-used metric scores to claim a noticeable improvement for \ac{HVS}. Such differences are non-trivial for most metrics, suggesting that substential quality differences are necessary for a reliable claim in the evaluation of \ac{NVS} methods.

 
 % These methods allow users to capture reality with very simple devices without a controlled lab environment, such as a mobile phone. By utilizing neural networks to learn an implicit function that matches the observation, we obtain a compact scene representation \RM{Are those representations really "compact"? Most methods are not designed to be compact.} \WW{I agree, the definition of compact here is really unclear. Removed.} that can be re-rendered in novel view or extracted as explicit representations for other usages.
 
 % In most cases, efforts were focused on different aspects such as geometry, texture/color, material properties (e.g., BRDFs, BSSRDF, etc.), and lighting. In a few exceptions, these aspects are tackled at the same time but for specific use cases \cite{Karsh14}. 


%
% Furthermore, we can have a compact representation of scenes, and rendering can be learned from existing observations by utilizing neural networks. 
%
% The main idea is to combine insights from classical (physics-based) computer graphics and recent advances in deep learning. 
%
% One great example of recent neural rendering techniques is , which is a novel view synthesis method that uses volume rendering with implicit
% neural scene representation. 

 % Existing methods typically use only classical image metrics such as PSNR, SSIM\cite{ssim}, and LPIPS\cite{lpips} to demonstrate their effectiveness and claim the state-of-the-art, without considering the rendering quality with respect to the human visual system (HVS). 


% %
% What emerges from the efforts of several researchers is the use of classic imaging metrics such as PSNR, SSIM\cite{ssim}, and the recent LPIPS\cite{lpips}. 
% %
% The existence of a clear protocol for the evaluation of NeRF is missing. This makes researchers to base their efforts in lowering down these metrics/indices and peculiar visual inspections, which often are difficult to replicate because many important parameters are rarely reported.
%
% In this work, we propose a complete and scrupulous protocol for NeRF evaluation, which we defined after comparing a list of quality metrics/indices for both images and videos. These results were cross-checked with subjective experiments.

% video is important for evaluation;
% challenging scenes 


In summary, the main contributions of this work are:
\begin{itemize}
    \item two new datasets with front-facing views and video references for full-reference evaluation of synthesized videos, 
    % \WW{wanted to say "free viewpoint videos" but that might be misleading?}\FB{Free viewpoint may lead to the thinking that you can place the camera where you want even outside the bounding box of the scene.}
    \item subjective quality evaluation of videos synthesized by 8 NVS methods (and two generalizable NeRF variants) measured via a perceptual quality assessment experiment,% on the two new datasets and additionally on the \dsllff{} dataset. 
    % , as well as an extensive set of image, video, and non-reference metrics.
    \item objective evaluation of existing image/video quality metrics on synthesized videos to assess how well these metrics correlate with subjective quality.
    % \item objective evaluation of existing image/video quality metrics on videos generated by the NVS methods, and how they correlate with perceived quality%; and a recommendation for a more robust evaluation protocol for the NVS methods. 
    % \item an extensive analysis of the findings. 
   
   % \item an estimation of minimum score difference in top-performing metrics to indicate non-trivial improvement in perceptual quality. 
    %\WW{Would this be a bit too general?} \RM{I would argue that an "extensive analysis" is not a contribution, neither would be "nice related work and conclusions sections", etc. "The estimation of the minimum difference" could be a contribution but we need to finish it. }\WW{Modified, can someone have a look and double check?}\FB{It looks good for me.}
    %The estimation of the minimum difference in quality prediction that is required to show statistically significant improvement for new NVS methods.     
    %We provide a detailed analysis of the correlation between an extensive set of quality metrics and subjective scores, revealing potential issues in the widely-used metrics and datasets. \WW{Is this good?}
\end{itemize}
The datasets and the results of the quality assessment experiment will be made publicly available.
% \WW{I feel point 2 and 3 can be combined together?}
% Code and datasets will be publicly released online.

% \FB{The intro is still a bit generic, we need results from metrics experiments and subjective experiment.}\WW{I agree}

%-------------------------------------------------------------------------


\section{Related Work}

%% RM: There is no need for a section overview in a short conference paper. The space is too precious.
%In this section, we first review the current evaluation protocols that are typically applied to assess the performance of \ac{NVS} methods, then briefly introduce the existing widely-used datasets. We leave a detailed discussion of selected methods and metrics in Section~\ref{SEC:nerf-methods} and Section~\ref{SEC:metrics}. 
% \PH{Thanks Walter, looks good!} \WW{Thx!}
% \PH{Indicate why we structure our review in the manner and mention that methods and metrics will be discussed in Sec ...} \WW{I don't think reason is necessary... what do you think of current version?}

%\PH{Combined 2 paragraphs: (1) video benchmark; (2) perceptual studies in other fields. I tried to keep it short.@hanxue, @walter please add if I'm missing anything important}
\paragraph{Quality assessment of NVS methods} Most works on NVS methods and NVS benchmarks \cite{original_nerf, scannerf, llff, NeX, deepview, NSVF} evaluate on sparse hold-out views using image quality metrics. An exception is the Light Field Benchmark~\cite{light_field_bench}, where light field interpolation methods were evaluated on video sequences. On the contrary, our focus is on assessing the perceptual quality of NVS methods and evaluating how well current objective metrics can predict subjective quality. %On the contrary, our focus is on evaluating NVS methods with an emphasis on NeRF~\cite{original_nerf} and its variants. We also include subjective scores as they capture real human preferences. 
Such subjective benchmarks have previously motivated and advanced other areas such as tone mapping~\cite{Ledda+2005,Eilertsen+2013}, image compression~\cite{Artusi+2016}, and single-image HDR~\cite{hanji2022comparison}. To the best of our knowledge, we present the first study on perceptual assessment of NVS methods, and hope that our study will similarly inspire improvements that better meet the needs of human users.

%  \paragraph{Quality Evaluation of Novel View Synthesis Methods} \hx{@PH,FB, could you please check?}
% The existing evaluation protocol of novel view synthesis methods typically uses a set of image quality metrics
% % like PSNR, SSIM~\cite{ssim}, or LPIPS~\cite{lpips}
% to compare synthesized images with ground truth on a sparse set of views. An exception to these work is done by Yue et al. \cite{light_field_bench}, who evaluated light field interpolation methods on video sequence with no-reference video metrics. But their work lacked  subjective evaluation and did not include any neural view synthesis methods, which are the focus of our work.


%To evaluate the quality of synthesized novel views, the reconstructed scene is re-rendered at sparse test views to compare with the ground truth captures via a set of image quality metrics including PSNR, SSIM~\cite{ssim}, and LPIPS~\cite{lpips}. 

% RM: The text below repeated what was stated in the introduction
%This protocol, however, does not evaluate the ability to synthesize videos, in which some artifacts become more noticeable. %nor contains a subjective study that evaluates the methods with respect to the real user in the applications. 
% \WW{Can mention de-correlations again}.
%\WW{Probably need re-phrase, as technically speaking we only have one existing work that goes beyond image comparison. Any idea for writing this?}
%Only a few works have attempted to extend the evaluation \ww{beyond pure image comparison}. 
%An exception to these work is by Yue et al. \cite{light_field_bench}. They created a view synthesis benchmark to evaluate light field interpolation methods on a set of images and videos.

%that contains both synthetic and real datasets, on which they evaluated light field interpolation methods. The methods were evaluated using a set of image and video quality metrics, but their work lacks a subjective evaluation and it did not include any neural view synthesis methods, which are the focus of our work. 
%More recently, Want et al. \cite{nerf_robust_benchmark} built a benchmark by corrupting some existing datasets to evaluate the robustness of NeRF-based methods. However, their evaluation was limited to image quality metrics. 

%\WW{I drafted a short paragraph for past subjective evaluation of image/video methods, but would appreciate help on this as I know very little about them, @param, @francesco}
% For study on subjective quality assessment, many works have evaluated the image \cite{Artusi+2016, hanji2022comparison} or video \cite{Eilertsen+2015, Serrano+2019} quality through subjective experiments to assess their performance with respect to HVS. But they mainly focus on low-level vision tasks like tone mapping, image compression, denoising etc. None of them conduct study on novel view synthesis task, which can cause different type of artifacts and pose new challenges to evaluation metrics. A most related work is ~\cite{Tariq+2022}, which proposes a perceptually-inspired technique that enhance foveated rendering by considering periphery sensitivity limitation of HVS. But this work is still constrained on image quality assessment and do not test whether image  quality metrics can predict the perceptual performance of NVS methods.


%~\cite{hanji2022comparison} evaluated a set of single-image HDR methods via a set of image quality metrics and subjective evaluation.

%on low-level vision tasks like tone mapping, image compression, denoising etc. None of them conduct study on novel view synthesis task, which is a new technique increasingly deployed in applications, and can cause different type of artifacts and pose new challenges to evalution metrics.



%Many works have evaluated the image \cite{Artusi+2016, Tariq+2022, hanji2022comparison} or video \cite{Eilertsen+2015, Serrano+2019} quality through subjective experiments to assess their performance with consideration of \ac{HVS}. In our context, Hanji et. al. \cite{hanji2022comparison} evaluated a set of single-image HDR methods via a set of image quality metrics and subjective evaluation. They identified that most image quality metrics fail to reflect the subjective judgments of the human testers, and proposed to add a corrected camera response function into the evaluation protocol to enhance the accuracy of objective metrics.
\vspace{-3mm}
\paragraph{NVS Datasets} \ac{NVS}
% \noindent \textbf{NVS Datasets} \ac{NVS}
methods are typically evaluated using synthetic and real-world datasets that contain either synthetic scenes or real-world scenes with sparse views~\cite{llff, NeX, deepview, NSVF, blendedmvs, dtu}. The NeRF synthetic dataset \cite{original_nerf} consists of 8 inwards facing scenes rendered with blender \cite{blender}, each containing 200 test images rendered at viewpoints located spirally at the upper hemisphere around the object. The \dsllff{ dataset \cite{llff} is a forward-facing dataset of real scenes, but with very sparse test views. The DTU \cite{dtu} Stereo dataset is also widely used to evaluate novel view synthesis performance, but its captured views are too sparse to create a continuous video. Recently, De Luigi et al. \cite{scannerf} set up a resource-efficient system to capture 360-degree dense views of various objects, but only for simple objects in a controlled lab environment and without video references. In contrast, our dataset is the first forward-facing dataset that captures scenes with reference videos in both laboratory and fieldwork environments, with accurately calibrated poses for each video frame.
% \WW{If don't have the table, should we include all entries that were in the table here?}



% \paragraph{Methods} We evaluated 8 NeRF-based methods via both quality metrics and subjective experiments on our dataset. The methods selected include the vanilla NeRF \cite{original_nerf}, 











% %
% Recently, researchers have devoted attention to image-based rendering methods that harness the power of deep learning \cite{Tewari+2020, Tewari+2022}.
% %
% In this work, our main focus is on neural radiance field (NeRF~\cite{original_nerf}) and its extensions, which generate novel views of real-world scenes using a sparse set of photographs with and without camera calibration. In particular, we are interested in studying the different NeRF methods to understand differences in image quality, training and inference times, etc.
% %

% \paragraph{Neural Radiance Fields}

% \WW{\TODO: add related works that also compare NeRF methods}

% NeRF \cite{original_nerf} represents a scene using a multilayer perceptron (MLP), whose input is a 5D vector, including a position $\mathbf{p}=(x,y,z)$ with a viewing direction $\vec{\mathbf{d}}(\theta,\phi)$, and the output is the view-dependent emitted radiance $\mathbf{c}=(R,G,B)$ and a volume density $\sigma$. During training, photographs with calibrated cameras are fed to train such representation inside a volume. At rendering time, a novel view is generated using classic volume rendering \cite{Kajiya+1984}.
% %
% MIP-NeRF \cite{mipnerf} mitigates the aliasing problems by casting cones rather than rays during volume rendering and encoding the positions and sizes of conical frustums, allowing the network to be trained at different scales. It can further incorporate normalized disparity space sampling for the scene background to improve the reconstruction and rendering quality for unbounded scenes \cite{mip_nerf}.

% \paragraph{NeRF with Efficient Training and Rendering}
% %% Fast rendering methods:
% %
% % Although NeRF-based methods are capable of synthesizing photo-realistic novel views, the inefficient training and rendering significantly limit their applications. 
% Many approaches have been developed to improve the rendering speed of NeRF-based methods by reducing the number of samples and network queries \cite{donerf, autoint, neusample, diver}, decomposing the scene and training a large number of small MLPs to represent different sections \cite{Reiser+2021, derf}, caching the density and radiance predictions \cite{plenoctrees, FastNeRF, snerg,mueller2022instant}, or combining with additional representations that already support sophisticated rendering \cite{mobilenerf, nex}. 
% %
% NeX \cite{nex} replaces the volumetric representation with multi-plane images (MPI) \cite{Zhou+2018} that have view-dependent pixel values and therefore can render non-Lambertian surfaces. 
% % To achieve this, the pixel color representation is a function of the viewing direction $\vec{\mathbf{v}}$, which is approximated using a learnable basis function.
% %% Fast training methods:
% %
% Some methods combine NeRF with explicit voxel grid to localize optimization to only involve a few parameter updates, hence further increasing the training speed \cite{nsvf, ngp, plenoxel, dirvoxgo, Karnewar+2022}. DirectVoxelGO \cite{dirctvoxgo} stores and optimizes a coarse-to-fine voxel grid of density values and appearance latents, which are then conditioned by a small MLP to predict view-dependent radiance. Plenoxel \cite{plenoxel} completely abandons the MLP and explicitly stores both density and view-dependent radiance, which is represented using the basis of a spherical harmonics function.  


% % \WW{TODO: add IBRNet: is it fast in rendering? Or should it go to general NeRF}
% % \FB{IBRNet: it uses 20\% less flops than nerf but it does not seem to be faster. The authors don't clearly state it; see Table 3 of the paper.}
% % \WW{I assume we included IBRNet in our experiment because its ability to generalize/finetune on new scenes?}

% % \FB{Where shall we put \cite{Bemana+2022}?} \WW{As it is not about fast training/rendering nor generalization ability, I'd say put it in the first paragraph}

% \paragraph{Generalizable NeRF}

% Another promising direction is to extract shared data priors and enable NeRF to generalize novel scenes without re-training, therefore reducing the requirement on image input and training time. It can be achieved by incorporating additional supervision from pre-trained semantic or natural image networks \cite{DietNeRF, SinNeRF}, or by training end-to-end and extracting priors from a large number of scenes \cite{pixelnerf, lolnerf, ibrnet}. IBRNet \cite{ibrnet} combines NeRF with traditional image-based rendering (IBR) and learns a generic view interpolation function from the dataset. It can synthesize novel view of unseen scenes at inference time, and can also be fine-tuned to provide better per-scene performance.

% % NeRF can be optimized at inference time by decomposing the scene into a volumetric grid with a small capacity network for each voxel or KiloNeRF\cite{Reiser+2021} or a sparse grid with a single network \cite{Liu+2020}. DirectVoxGO\cite{dirvoxgo} improves over KiloNeRF by adding priors and a voxel-grid interpolation that allows the method to model sharp features at geometry boundaries especially at low resolution grids.
% % %
% % IBRNet \cite{ibrnet} is a mix between traditional image-based rendering (IBR) and NeRF. The method learns a generic view interpolation function from a dataset and stored as a MLP, so it is not scene specific. At inference time, a new view is rendered using classic volume rendering where some source neighbors views are identified and employed as input of the MLP.
% % %
% % Plenoxels\cite{plenoxel} is a generalization of PlenOctrees\cite{Yu+2021}, which represents a scene using a sparse 3D grid with spherical harmonics. In PLenoxels, trilinear interpolation and spatially varying opacity are the key to define a continuous plenoptic function throughout the volume.
% %


\section{\datasetName{} Dataset}
\label{sec:dataset}

%% RM: The paragraph below is too much of a repetition of what is in the introduction.
%In order to account for the final users of the output of \ac{HVS} method, the human, we build a perception benchmark that evaluates the perceptual quality of different neural rendering models. In most VR/AR applications, neural rendering methods should provide a synthesis of free-viewpoint videos for customer usage, \ww{ where consistency and smoothness between frames are extremely crucial}. Due to the above reason, we should conduct the perceptual benchmark on a diversified set of forward-facing scenes \WW{I don't see how forward-facing goes in here. Shall we remove it?} with video sequences and corresponding accurate camera poses.


%In order to build NeRF-perception benchmark and conduct subjective quality assessment, we need to have diversified light field dataset with video sequences and corresponding camera poses. 
%However, existing real-world datasets for view synthesis do not have high-quality reference videos  for evaluation. %And another issue is existing dataset has similar distributions to training images, which provides an unfair advantage. 
%To remedy this, we collected a new \datasetName{} dataset, which is composed of a diverse set of scenes with reference videos and accurate pose for each frame. 

%1. from application point of view: novel view synthesis methods should really extend to synthesis of free-viewpoint videos 
%2. Existing evaluation focus on images 


\begin{figure} %[tb]
    \centering
    \includegraphics[width=1.0\columnwidth]{img/setup.pdf}
    \caption{The camera rig (a) and camera poses (b) used to capture scenes for the \dslab{} (top) and \dsfieldwork{} (bottom) datasets. On the right, the green dots represent training camera poses and the red dots represent the poses of the reference video frames used for testing. }
    \vspace{-3mm}
     %\RM{Please add arrows to "* gantry" labels.} \RM{Please add scene names in the right upper corner of the point cloud panels.}
    \label{lab-vis}
\end{figure}


To evaluate NVS methods on video rather than individual views, we collected two new datasets: \dslab{}, captured using a 2D gantry in a laboratory with controlled lighting and background; and \dsfieldwork{}, captured in-the-wild, consisting of both indoor and outdoor scenes. Both datasets were captured with Sony A7R\uppercase\expandafter{\romannumeral3}. Images of selected scenes from both datasets are shown in Figure~\ref{data-vis}.
%a set of real-world scenes captured with slider and gimbal in both indoor and outdoor scenarios}.
%In this way, we cover both simple laboratory scenes where we have more control over lighting, occlusion, layout and object selection, etc, and a set of more complex scenes that are typical of the real world. In the following sections, we provide a more detailed explanation of data collection preparation. In Table~\ref{tab:data}, we show a comparison between our dataset with the existing widely used dataset for neural rendering models. 

\subsection{Lab Dataset}
%During capturing, we use a gantry to decide the movement of camera, which is controlled by an Arduino microcontroller. 
\paragraph{Capture Setup} The \dslab{} dataset was captured in our laboratory using a 2D gantry (upper-left of Figure~\ref{lab-vis}), which allowed horizontal and vertical movement of a camera. %at the resolution of 7952$\times$5304\,px. 
%with f-number 5.6. %and focal length set as 35mm.
%\RM{Please check EXIFs for the lens and f-number and report here.} \PH{I don't see any lens information "EXIF LensModel (ASCII): ----"}
% To minimize the amount of noise and avoid saturated pixels, each view was captured with 2 exposures, 5 stops \PH{@hx exposure times are e1=1/4 and e2=1/125 for the "Car" scene; so stops = log2(e1/e2) = 5} apart. @PM how about directly say captured with 2 exposures of 1/125 and 1/4 seconds. 
To minimize the amount of noise and avoid saturated pixels, we captured each view with a RAW image stack consisting of 2 exposures at constant ISO. The RAW image stacks were merged into an HDR image using an estimator that accounts for the photon noise \cite{hanji2020noise}. All images were color-corrected using a reference white point, and cropped to $4032\times3024$\,px. %\RM{Why this resolution? This is not the resolution that you used for the NVS methods.}% \RM{How did you tone mapped the images}.
To map linear images to display-referred units, gamma was applied ($\gamma=2.2$).
 %\PH{Include a "resize" step to match resolution from "experiment" section, as suggested by @rafal}\hx{This is the resolution of dataset, and we will publish dataset in such resolution. I will add the resize step in the section 3.3.}

%where we can decide the scene layout, objects with different materials, and lighting conditions of the scenes. A hand-hold capture will cause a lot of jitters and jerky movement 
%unsmoothness
%during video capture, we use a gantry to control the movement of the camera. 

%The movement of the camera is controlled by a gantry. The gantry\hx{@PH type of Gantry?} consists of a horizontal and a vertical component, which is controlled by an Arduino microcontroller. With the flexibility of a 2D gantry movement, different camera movements can be designed through serial port communication. %As shown in Figure \ref{layout}, 

A sparse set of training views were taken on a uniform %6$\times$5 
grid, shown as green dots in upper-right of Figure \ref{lab-vis}. The training views cover a horizontal range of 100\,mm and a vertical range of 80\,mm. %\PH{Are these values correct?}\hx{yes, have checked the scene.} %\RM{add the updated figure}\RM{Add a physical distance in mm between the training views}. 
The reference videos consisted of 300 to 500 frames, captured in a rectangular camera motion, as shown by red dots in Figure \ref{lab-vis}. The camera traveled about 0.6\,mm between each frame. Since we only consider a view interpolation task (no extrapolation), the reference frames were positioned within the range of the training views. 

%(red points in Figure \ref{layout}). 
%The evaluation video sequence is obtained by taking a sequence of consecutive frames and then merging them as a video. The layout positions of the video frames are under the coverage of training views as most neural rendering methods are designed for a view interpolation task instead of extrapolation. %(as shown in blue points in Figure). 
%Between each video frames the gantry will only move 0.6 mm so that a smooth video sequence can be obtained.


\paragraph{Scenes} The lab scenes were placed inside a box of $30\,\text{cm}\times41.5\,\text{cm}\times38\,\text{cm}$ for capturing.
% \RM{Param, are those dimensions correct? I do not think so.}.
As illustrated in the first row of Figure~\ref{data-vis}, they were designed to cover a wide range of objects with various materials, including glass, metal, wood, ceramic, and plastics. The layout of the objects was selected to introduce occlusions and to offer a good range of depth, which would fit within the depth-of-field of the camera. The dataset contains challenging view-dependent effects, such as diffraction on the surface of a CD-ROM, specular reflections from metallic and ceramic surfaces, and transparency of the glass. Six scenes were captured in this dataset. %\RM{Note: a pity that we did not put any background in the glass scenes --- we do not have refraction.} 

\paragraph{Pose Estimation} For accurate pose estimation, 4 sets of 4 AprilTag markers were placed in each corner of the scene. The camera positions were selected to ensure that all markers were visible in each view and the images were later cropped to remove the markers. By detecting the position of AprilTags \cite{olson2011apriltag}, we obtained camera poses with standard camera calibration methods \cite{zhang2000flexible}. According to our pose estimation results, we got an acceptable mean re-projection error
of 0.2174 px across all scenes.%\PH{Can this be rephrased?} %3\%, \textit{i.e.} 0.3\,mm shift for a 10\,mm movement. \RM{How do you estimate that? You typically report the projection error in pixels.} \CO{Yes, we should report a standard error.}


 \subsection{Fieldwork Dataset} 

% \hx{@francesco, could you give a more detailed explaination on data collection process? Refer to section 3.1. missing information like why use these gimbal and silder, which version of gimbal, how do you select different scenes etc. please try to give a figure show the data collection setup including the gimble and slider.} 

Our in-the-wild \dsfieldwork{} dataset was captured in both outdoor city areas and indoor rooms of a public museum\footnote{The location is not revealed for anonymity.}. Typically, such scenes are challenging due to complex backgrounds, occlusions, and uncontrolled illuminations.  %We made our best effort to minimize any motion in the scenes. %However, due to moving clouds or wind affecting small objects, a small amount of motion in localized areas was present. \PH{Do we try to detect and reject/correct for these using e.g. optical flow? If not, will we be questioned?} \RM{TODO} 
%In addition, capturing needs to be carefully planned to avoid the presence of moving people/objects in the video sequences; this may require several attempts with non-trivial effort.

%\hx{These scenes are of real-world nature and are challenging due to complex background, occlusions, lighting and reflectance effects etc.} %Such conditions are challenging due to the presence of people/animals/cars moving, wind (especially for light objects as leaves), uncontrolled lighting condition (e.g., sunlit that varies), etc. 
 
\paragraph{Capture Setup} 
% The scenes were captured with the same Sony camera used for the \dslab{} dataset. However, 
Different from the \dslab{} dataset, we captured video sequences instead of individual images for the \dsfieldwork{} scenes. 
The video sequences were captured with resolution  1920$\times$1080\,px and framerate 30\,fps. To reduce camera shake, we used either a DJI RS3 gimbal or a 90\,cm manual slider, which was fixed on two tripods, see lower-left of Figure~\ref{lab-vis}. 
For each scene, we captured several video sequences with different trajectories. One of these sequences, whose trajectory is well within the bounds of the scene, is selected as the test sequence. The bottom-right of Figure~\ref{lab-vis} shows the test sequence of one scene from the \dsfieldwork{} dataset (red dots). Images for training are sampled from the remaining videos (green dots).
% For both gimbal and slider videos, we captured several video sequences for each scene. As illustrated in the bottom-right of Figure~\ref{lab-vis}, one video sequence is selected for testing as that its trajectory is roughly located in the range of the other videos, and training frames are sampled from the other videos.
We also moved the first and last 15 frames from the test video sequence to the training set to ensure that the test views can always be interpolated from training views. In total, we have around 120 frames reserved as test views.

%For both gimbal and slider videos, we captured three video sequences for each scene: two sequences with the camera located at a lower/higher height (or on the left and right) for training and one in the center for testing. We removed the first and last 15 frames from the central test sequence to ensure the test views can always be interpolated from training views.}

%that the training videos have most of the outliers camera views (e.g., the floor and ceiling of a scene) and they both cover the training scene from different views.

\paragraph{Scenes} The second row of Figure~\ref{data-vis} shows selected examples of the captured scenes, which cover both indoor and outdoor scenarios with a high variability of materials including wood, marble, window glasses, metals, etc. and complex geometries such as a whale skeleton, posing challenging scenarios for NVS methods. Nine scenes were captured in this dataset.


\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{img/demo.pdf}
    \caption{Examples of reconstructions by various NVS methods on selected scenes from \dsfieldwork{} dataset (first two rows) and \dslab{} dataset (third row). We only show three scenes due to limited space, please refer to the supplementary for more visual results.}
    \label{render_results}
    \vspace{-3mm}
\end{figure*}


\paragraph{Pose Estimation} %For these scenes, we could not use AprilTags due to several constraints such as strong wind, limited time for setting them up, etc. 
%We relied on camera calibration provided by COLMAP \cite{schoenberger2016sfm}. %Each SFM reconstruction is up to a scale factor, and different photo sets of the same object may have slightly different reference systems. Therefore, to have all camera poses for both the training and testing frames at the same scale and reference system, we ran COLMAP using all frames together into a single reconstruction. 
We employed COLMAP~\cite{schoenberger2016sfm} to perform joint calibration of camera poses for both the training and testing frames, so that all the calibrated poses share the same scale with a consistent coordinate system. We used the ``OPENCV" camera model, which supports separate x and y focal lengths as well as radial and tangential distortions. We also used COLMAP to undistort the captured images after pose estimation. Our reconstructed camera parameters have a mean reprojection error of 0.5327\,px across all scenes.% \CO{As above, how are these computed?}\WW{Computation of sfm re-projection error is very standard and comes from colmap itself, current way of stating it should be fine?}
 
%As for the gantry videos, we employed a Sony A7R\uppercase\expandafter{\romannumeral3} camera. For each scene, we have three HD videos compressed on camera, which were resized to 1008756 pixels.
%\WW{TODO: change to one sentence attribute describtion}
\subsection{Evaluated NVS Methods} \label{SEC:nerf-methods}


% We tested eight representative NVS methods , including NeRF~\cite{original_nerf}, mip-NeRF~\cite{mipnerf}, DVGO~\cite{dirctvoxgo}, Plenoxels~\cite{plenoxel}, NeX~\cite{NeX}, LFNR~\cite{lfnr}, as well as two variants of 
%  IBRNet~\cite{ibrnet} and GNT~\cite{gnt}. 
% %\CO{Write a few words on why this is a good subset.} \hx{How about:}
% These methods encompass a diverse range of models which feature both explicit and implicit geometric representations, distinct rendering modelings, as well as generalizable and per-scene optimization strategies.
We tested ten representative NVS methods (including two generalizable NeRF variants) that encompass a diverse range of models, which feature both explicit and implicit geometric representations, distinct rendering modelings, as well as generalizable and per-scene optimization strategies. 
%NeRF
NeRF~\cite{original_nerf} is a neural volumetric representation that excels in image-based scene reconstruction and novel view synthesis.
% MipNeRF
Mip-NeRF~\cite{mipnerf} builds upon NeRF and provides a multiscale representation for anti-aliasing view synthesis. 
% DVGO and Plenoxels
DVGO~\cite{dirctvoxgo} and Plenoxels~\cite{plenoxel} use hybrid representations to achieve fast training and rendering.
% NeX
NeX~\cite{NeX} utilizes multi-plane images and trainable basis functions, which is intended for rendering view-dependent effects in forward-facing scenes.
% LFNR
LFNR~\cite{lfnr} operates on a light field representation and uses an epipolar constraint to guide the rendering process. 
% IBRNet and GNT
IBRNet~\cite{ibrnet} and GNT~\cite{gnt} are both generalizable NeRF models. IBRNet aggregates nearby source views to estimate radiance and density and GNT extends this idea by proposing a unified transformer-based architecture that replaces both multi-view feature aggregation and volume rendering. For IBRNet and GNT, we tested both their published cross-scene models (labeled as GNT-C and IBRNet-C) and also models fine-tuned on each scene (labeled as GNT-S and IBRNet-S).

%\CO{We need a full list of which datasets we use.} 
We use these methods to reconstruct videos of scenes from both our collected datasets, \dslab{} and \dsfieldwork{}, as well as from the popular forward-facing LLFF~\cite{llff} dataset. For a fair comparison between methods, we downscaled images from the \dslab{} dataset by a factor of 4  and cropped images from the \dsfieldwork{} dataset, so that they all have same training image resolution of $1008\times756$\,px. as that in LLFF scene.
%to a resolution of $1008\times756$\,px. to match that of LLFF scenes.
% so that all the scenes have same training image resolution of $1008\times756$\,px. as LLFF scenes.
%\PH{TODO: rewrite the next sentence}
In this way, we were able to adopt the same training setup (network architecture, training iterations, optimizer etc.) on LLFF scenes proposed by the respective authors. Please refer to the supplementary materials for more details about the training setup.


%As our datasets have same image resolutions as that of LLFF dataset, we adopt the same training setup proposed by the respective authors for a fair comparison. Please refer to the supplementary materials for more details about training setup.
%A more detailed explaination can be  
%image resolution is the same as the \dsllff{} dataset. 
%For a fair comparison, we adopt the same training setup proposed by the respective authors. %\RM{Please explain what you mean by "the same training setup"} For the generalizable IBRNet and GNT methods, we test both on their published cross-scene models (labeled as GNT-C and IBRNet-C) and also models fine-tuned on each new scene (labeled as GNT-S and IBRNet-S).



\section{Subjective Evaluation}
\label{sec:iq-experiment}

%To obtain performance ratings that accurately reflect human perception, we conducted subjective experiments with \ww{30} participants to assess the quality of reconstructed videos by each method. 
%The goal of the subjective experiment is to obtain an accurate measure of perceptual visual quality that can be achieved by each \ac{NVS} method. In the following section, we explain how the subjective experiments are conducted and how the subjective scores are obtained.
To attain precise subjective quality scores of the videos synthesized by the aforementioned NVS methods, we conducted a controlled quality assessment experiment with human participants. We relied on a pairwise comparison experiment, as it has been shown to be more accurate and robust than direct rating methods \cite{pair_comparison}. %\PH{I expanded slightly because ICCV reviewers may not be used to perceptual experiments.}
\subsection{Experimental Procedure}
\label{sec:experimental_procedure}
%We conducted a pairwise comparison experiment, which had been shown to be more accurate and robust than direct rating methods \cite{pair_comparison}. 
%\CO{could we include references that utilize a similar proceudure? the reviewers might not know if this is the way to do these kinds of experiments.}\hx{Following a similar procedure as in ~\cite{hanji2022comparison,pair_comparison},} 
In each trial of our experiment, a participant was shown a pair of videos side-by-side on the same display and was instructed to pick the video of higher quality --- ``better resembles a natural scene and contains fewer distortions" (exact wording on the briefing form).
%The participant's answer was stored in a comparison matrix, each element of which shows his/her preference for a specific scene rendered by a certain method over the other one.
% \PH{Do we need such a detailed explanation?}\RM{No, we do need that. We should refer to the relevant papers instead.}
% The participants answer is then stored in a
% comparison matrix $C$, where each entry $C_{ij}$ denotes the number of times the participant
% picked video $i$ over video $j$. So the probability of video $i$ being picked over video $j$ is
% given by: 
% \begin{equation}
%     p_{ij}=\frac{C_{ij}}{C_{ij}+C_{ji}}
% \end{equation}
To reduce the number of comparisons and maximize the information gained from each trial, we used ASAP \cite{mikhailiuk2020active}, an active sampling method. 
%To allow participants to make sensible judgments if both the synthesized videos to be compared are of low quality visual quality, 
Participants could press the space bar to view the reference video of the displayed scene (except for \dsllff{} dataset as reference videos were not available). The reference videos were included as one of the compared conditions. %and we align the quality scores of the synthesized videos to that of the groundtruth videos.

%A 500\,ms blank was inserted when switching to/from the reference video so that the participants could not make judgments based on localized distortions. 
%To allow participants to make sensible judgments when both the reconstructions presented were of bad visual quality, we allowed them to view ground truth videos if needed.
% There is a half second short delay when a black screen is initially displayed, and the ground truth video will be shown afterward. This is to prevent the case where participants become too sensitive to the small differences in reconstruction and ignore overall visual quality during the assessment. For our dataset where the ground truth videos are available, we also include them as the candidates in comparison.
%In order to encourage participants to directly compare the presented videos, we displayed a black-frame for half a second when switching videos. This prevents participants from picking up on minute, localized differences and instead forces them to assess overall visual quality. 
%We align the quality scores of the synthesized videos to the ground truth by including the ground truth videos in the experiment.

\subsection{Display and Videos}
\label{sec:STIMULI:VIDEO}
% The videos are generated by different methods methods on each scene. They all have frames of resolution 768x960 and the framerate is 30. Each video is around 3 to 15 seconds long.
Our videos were displayed on a 27" Eizo ColorEdge CS2740 4K monitor, which was colorimetrically calibrated to reproduce BT.709 color space with a peak luminance of 200\cdms. The average viewing distance was 70\,cm, restricted by a table in front of the display. %The videos were up-scaled 2$\times$ (using bilinear interpolation), resulting in an effective resolution of 40 pixels per degree with respect to the original video resolution.

We used 14 scenes from our two datasets and 8 scenes from the \dsllff{} dataset. For the scenes in our dataset, videos were synthesized on the same views as in ground-truth video frames. As \dsllff{} dataset does not have reference videos, we combined 120 frames rendered in a spiral trajectory around the mean pose (similar to other NVS methods). All the video frames were cropped to a resolution of $960{\times}756$\,px. and then up-scaled to $1920{\times}1512$\,px. (bilinear filter) so that two videos could be shown side by side on our 4K monitor. The upscaling was necessary to obtain a more realistic and effective resolution of 40 pixels per degree with respect to the original video resolution. Each video was between 3 to 15 seconds long, with a framerate of 30 fps. In total, each participant assessed the quality of 22 scenes reconstructed by 10 NVS methods as well as 14 reference videos.

%In total, each participant assessed the quality of 22 scenes $\times$ 10 reconstructions + 14 reference videos, 234 conditions in total.

%\PH{Perhaps include details about encoding used to combine frames (lossy with fixed quality value of ?)}

\subsection{Participants}
% \PH{I think we can skip this subsection. The only relevant information is the number of participants}\RM{Make it a paragraph. Did we run a colour test? We need to add wording about the ethical approval and reward.}
% \FB{is pair\_comparison the correct citation for asap?}\PH{This is the citation for ASAP~\cite{mikhailiuk2020active}}
We invited  39 volunteers (20 males and 19 females)
%20 volunteers (9 males and 11 females) 
with normal color vision (confirmed by running the \textit{Ishihara Test}). Each participant completed 4--5 full batches of comparisons scheduled by ASAP \cite{mikhailiuk2020active}. %, each consisting of 234 comparisons.  
 The experiment was authorized by an external institutional review board and the participants were rewarded for their participation. 
% , which selects comparison samples that maximize the information gain, and, therefore, enhances the accuracy of collected scores. ASAP ensures each method is compared at least once in each batch. Therefore, each of the methods is therefore compared at least 40 times by the  participants.

\subsection{Subjective Score Scaling and Calculation}
\label{jod_scale}
% Due to the subjective nature of the experiment, this data tends to be noisy.
We scaled the results of the pairwise comparison and expressed the subjective evaluation score in Just-Objectionable-Difference (JOD) units using the Thurstone Case V observer model \cite{pwcmp}. A difference of 1\,JOD unit means that 75\% participants preferred one method over another. The model assumes that participants made their selections by assigning a single quality value to each video and approximates this quality by a normally-distributed random variable with the same inter- and intra-observer variance.
% To account for measurement noise, %we used the pwcmp~\cite{pwcmp} software to remove the potential outliers, and 
% we report bootstrapped confidence intervals in all plots. \PH{Bootstrapping hasn't been introduced yet}


\section{Perceptual Benchmark Results}
\label{benchmark}



\begin{figure}[t]
\begin{center}
%nerf_benchmark\experiment\analyze_data\scale_results.m  overall_noref.csv
%nerf_benchmark\experiment\analyze_data\plot_method_cmp.m
    \includegraphics[width=\columnwidth]{img/jod_all_noref.pdf}
\end{center}
\vspace{-3mm}
   \caption{Perceptual preferences for NVS methods. The bars indicate preference in JOD units, relative to the original NeRF method~\cite{original_nerf}, which is at 0\,JOD. Negative values indicate that, on average, the method produced less preferable results than NeRF. The error bars show 95\% confidence intervals.} %The results are averaged across \dslab{}, \dsfieldwork{}, and \dsllff{} datasets and also across all three datasets.}
%\label{fig:long}
\label{fig:perceptual_score}
\vspace{-3mm}
\end{figure}

\begin{table}
\caption{The list of evaluated objective metrics. %\PH{Lots of empty space in this table. Either add another column or make text larger?}
}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|l|}
\hline
\multirow{2}{*}{\textbf{Metric}} & \textbf{Reference} & \textbf{Video}       & \multirow{2}{*}{\textbf{Details}}   \\
                                 & \textbf{required}  & \textbf{metric}    &                                     \\ \hline \hline
\multirow{2}{*}{PSNR} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Widely used ratio to measure noise \\
& & & relative to the signal in log units \\ \hline
\multirow{2}{*}{PSNR-L} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & PSNR computed on image luma values   \\
 & & &  \\ \hline
SSIM & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Popular quality measure that perceives  \\
\cite{wang2004ssim} & & & structural similarity \\ \hline
MS-SSIM & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{Multi-scale version of SSIM} \\
\cite{wang2003msssim} & & & \\ \hline
VIF & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \ac{NSS} models \\
\cite{sheikh2006vif} & & & on information-theoretic setting \\ \hline
FSIM & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Low-level image feature similarity \\
\cite{fsim} & & & based on the human visual system \\ \hline
LPIPS-VGG & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Perceptual similarity metric based on \\
\cite{lpips} & & & deep network of VGG model \\ \hline
LPIPS-ALEX & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Perceptual similarity metric based on \\
\cite{lpips} & & & deep network of AlexNet model\\ \hline
DISTS & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Unify texture and structure similarity   \\
\cite{dists} & & &  with deep network\\ \hline
HDR-VDP-3 & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Low-level vision model  on \\
\cite{Mantiuk2011} & & & HDR images \\ \hline
FLIP & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & Metric that considers HVS, viewing  \\
\cite{flip} & & & distance and monitor conditions\\ \hline
FovVideoVDP & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & Spatial-temporal metric
that accounts \\
\cite{fovvdp} & & & for foveation effect\\ \hline
STRRED & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & Hybrid metric measures temporal \\
\cite{strred} & & &  motion and spatial difference\\ \hline
VMAF & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & Support Vector Machine combination \\
\cite{vmaf1} & & &  of multiple image and video metrics\\ \hline
HDR-VQM & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & Spatial-temporal metric that considers \\
\cite{hdrvqm} & & &   human eye fixation behavior\\ \hline
BRISQUE & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & Support vector regression trained \\
\cite{brisque} & & & on IQA dataset \\ \hline
NIQE & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & Distance between \ac{NSS}-based \\
\cite{niqe} & & & features to those from a database \\ \hline
PIQE & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & Averaged block-wise distortion \\
\cite{piqe} & & & estimation \\ \hline
\end{tabular}
}
    \label{tab:metrics}
    \vspace{-3mm}
\end{table}
% \WW{would be great if we can add one or two more cols for table}


%\hx{this sentence might be redundant: In this section, we show the results of our built neural rendering perceptual benchmark. We provide an analysis of the overall perceptual performance of different neural rendering methods, and their performance on individual scenes. }

\figref{perceptual_score} shows the perceptual preference for different methods averaged across our collected \dslab{} and \dsfieldwork{} datasets, as well as the \dsllff{} dataset. We report both per-dataset performance and the overall performance across all three datasets. To view results on the individual scenes, we refer to Figures 2--4 %\PH{Update after creating supplementary file} 
in the supplementary. The baseline (0 JOD line) in \figref{perceptual_score} is the original NeRF model~\cite{original_nerf}, so positive JOD values indicate improvement and negative values indicate degradation in quality (on average) with respect to NeRF.

The results on both of our datasets show that despite the impressive performance of \ac{NVS} methods, their results can still be easily distinguished from the reference videos. There is about 0.85\,JOD difference between the best neural rendering methods and the reference; 0.94 \textit{vs.} 0.14 for \dslab{}, 1.9 \textit{vs.} 1 for \dsfieldwork{}. This indicates that the reference will be selected as better in 70\% of the cases across the population. 
On average, only five out of nine methods produced better results than the original NeRF. It is evident that existing generalizable models require further refinement, as an additional per-scene optimization step is needed to achieve desirable outcomes. %\FB{For desirable outcome do we mean better quality?}. 
% \PH{Mention again what suffix -C and -S refer to}
%and there is still room of improvement for existing generalizable models as they still need an extra per-scene optimization step to achieve good performance.  existing generalizable models still do not achieve satisfactory performance. They can not fit diverse scenes well out of the box and still need an extra per-scene optimization step to achieve good performance. 
It is noteworthy that the discrepancies among the methods are more noticeable in the more  challenging \dsfieldwork{} dataset, which implies that a challenging dataset is essential to distinguish between methods.
 %Another interesting observation is that differences between the methods are much larger for the more challenging fieldwork scenes, which suggests that a challenging dataset is required to distinguish between different methods. 

 % We move forward and inspect each neural rendering model's performance on individual scenes. \WW{I removed this but feel free to have it back if we have space}
Compared with other models, MipNeRF performs quite well in most scenes, particularly those with high-frequency geometric details (Figure~\ref{render_results}, statue's face and hair in \scene{Naiad statue}, bones and background poster in \scene{Dinosaur}, fence in \scene{CD-occlusions} \textit{etc.}).
 %(bumper of the car in scene Car-fig, sheep and rabbit in scene Farm, background characters in scene Dinosaur, the roof in scene Whale \textit{etc.}). 
In comparison, techniques that lack explicit volume rendering (e.g., LFNR, and GNT) and those with coarse geometric modeling (e.g., NeX) exhibit suboptimal performance in these situations. %geometry-free \hx{change term}approaches (\textit{i.e.}, methods without explicit geometric inductive biases like those introduced by volume rendering) such as LFNR, GNT, and those with coarse geometric modeling \hx{is it correct to say NeX as 'coarse'} \WW{I think it's fine, MPI is a very rough geometry} such as NeX, do not provide good performance in these cases.
Nonetheless, LFNR and NeX do provide more natural outcomes for scenes with complex lighting and specular reflections such as \scene{Metal} and CD in \scene{CD-occlusions}; see Figure~\ref{render_results}. For certain \dsfieldwork{} scenes, techniques founded on multi-view epipolar geometry constraints, such as IBRNet and GNT, tend to fail and exhibit conspicuous artifacts (\scene{Dinosaur} in Figure~\ref{render_results}, \scene{Vespa} and \scene{Giraffe} shown in supplementary). This is due to the considerable distance between the test and source views, which renders the epipolar features inaccurate. For a more in-depth examination, we encourage the reader to review the quality results of individual scenes provided in the supplementary. % HTML viewer \PH{Are we promising the HTML viewer?}\FB{Is there an update on this promise? I thought we removed given the only 100Mb of additional.}\WW{Yeah we should update this}.%(which will also be made available on the project website).
 %This is because the test views are far from source views and epipolar features can not be derived accurately. We encourage the reader to inspect individual scenes included in the supplementary (we will put them on the project web page later).


\section{Quality Metrics for Neural View Synthesis} 
\label{sec:metrics}

\begin{figure}%[tb]
    \centering
\includegraphics[width=\columnwidth]{img/scatter_main.pdf}
    \caption{Selected metric correlations for our \dslab{} (top row) and \dsfieldwork{} (bottom row) datasets.}
    \label{fig:metric-scatter}
\end{figure}

\begin{figure*}%[tb]
    \centering 
    %nerf_benchmark\metric_comparison\bootstrp_and_scale.m withoutcrossscene_lab_jod_bstrp.mat; withoutcrossscene_outdoor_jod_bstrp.mat;withoutcrossscene_llff_jod_bstrp.mat
    %nerf_benchmark\metric_comparison\plot_two_main.m
    \includegraphics[width=0.85\linewidth]{img/jod_correlations.pdf}
    \vspace{-2mm}
    \caption{Bootstrapped distributions of correlation coefficients for all metrics
    computed on (a) \dslab{}, (b) \dsfieldwork{}, and (c) \dsllff{}. The ``+" in black denotes mean correlation, and ``{\color{red}-}" in red denotes the 5th percentile (an estimate of the bad-case performance). Full-reference video metrics are missing for \dsllff{} because this dataset has no groud-truth videos. The lines connect metrics where differences cannot be deemed statistically significant in a non-parametric test, with a $p$-value of 0.05.}
   % \RM{TODO: explain the notation used for the significance bars once they are added.}
   % \PH{Is this sufficient? "The lines connect methods where differences cannot be deemed statistically significant in a non-parametric test, with a p-value threshold of 0.05."}
    %\CO{please correct the scene names in the figure. i don't see any plus signs.}
    \label{fig:correlations-all}
    \vspace{-5mm}
\end{figure*}


Our %new subjective NVS quality dataset 
collected datasets with video references, together with perceptual quality results of reconstructed videos, lets us test how well existing image/video quality metrics can predict the perceived quality. 
%To identify an objective evaluation procedure that correlates well with our perceptual results from Section ~\ref{benchmark}, we start by investigating the efficacy of existing neural rendering evaluation metrics. %\WW{Weird sentence, we look into reliability to find correlation?}. 
We test a range of existing objective metrics, full-reference and non-reference, image and video metrics, as listed in \tableref{metrics}. We look into the widely used image similarity metrics such as PSNR and SSIM~\cite{ssim}, and also deep-learning related metrics, such as LPIPS~\cite{lpips} and DISTS~\cite{dists}. We test LPIPS using two different backbone models, VGG and AlexNet, as we noted they differ in their predictions. PSNR-L converts image RGB values into luma values before computing PSNR similarity. We include several video quality metrics, including FovVideoVDP (v1.2, labeled FVVDP) \cite{fovvdp}, VMAF (v0.6.1)\cite{vmaf0,vmaf1,vmaf2}, as well as several blind or non-reference image metrics (BRISQUE~\cite{brisque}, PIQE~\cite{piqe} and NIQE~\cite{niqe}), that directly compute scores without comparing to the reference images. %\CO{what are those no-ref. metrics?} \PH{is this better?} 
For metrics that require display parameters (e.g., HDRVDP-3, and FovVideoVDP), we matched the effective resolution of the videos in our experiment (40\,ppd, see \secref{STIMULI:VIDEO}). %\CO{I don't get the last sentnce.}\RM{Does it explain now better?}

For our collected \dslab{} and \dsfieldwork{} datasets, which include reference videos, we compute the quality scores on the captured test video sequences. Since \dsllff{} dataset lacks reference videos, we compute the quality scores on the test image set as done in NVS papers.%, following the standard setting in NVS papers.
%on each frame of test video and average across them. The video metric score is computed directly on test video. 


%\subsection{Correlation Computation}
\label{corre_compute}
To test the reliability of popular metrics, we followed the standard
protocol used to evaluate quality metrics~\cite{hanji2022comparison,Ponomarenko2015}, and computed the rank-order (Spearman) correlations between metric predictions and perceptual JOD values. 
\figref{metric-scatter} shows scatter plots and correlations of popular quality metrics w.r.t. subjective quality for the \dslab{} (top row) and \dsfieldwork{} (bottom row) datasets. Please refer to the supplementary for similar plots for other metrics. We observe that that most metrics perform better on the more challenging \dsfieldwork{} dataset. 
However, these point estimate of correlations conceal measurement error due to: (a) the selection of scenes, (b) the measurement error in subjective experiment results. Thus we cannot draw conclusions solely based on these correlations.
% about the significance of differences in metrics based on point estimates.

\subsection{Averaged Bootstrapped Correlations}

For each dataset and each NVS method, we averaged subjective JOD scores and quality metric predictions across all scenes, and then computed a single correlation per dataset per method. This serves two purposes: (a) it improves the predictions of quality metrics as shown in previous works \cite{hanji2022comparison}, and (b) NVS methods are typically compared on averaged scores across scenes, making it more relevant for us.%, and (c) we would not be able to compute per-scene correlations for \dsllff{} scenes as they lack quality scores for the reference videos.% \WW{a bit confusing. Lack reference videos for complying quality scores?}

%For each NVS method, we compute its mean metric score and perceptual JOD score by averaging across multiple scenes. After getting the mean scores of all the methods, we compute the correlation between metric scores and subjective JOD values.
%After getting the their mean scores, we compute the correlations of averaged metric scores with subjective JOD values between all the methods. 

When comparing quality metrics, it is essential to account for the variance in our data (subjective score variance and scene selection). We estimate the distribution of correlation values using bootstrapping~\cite{Mooney1993}:
we generated 2000 bootstrap samples for each estimated correlation by randomizing (sampling with replacement) both the participants and the selection of scenes. Within each bootstrap sample, we independently scaled the JOD values (following Section~\ref{jod_scale}). In this way, our bootstrap estimation strategy simulates 2000 outcomes of the experiment to capture the variance we can expect due to measurement noise. To determine whether the differences between the metrics are statistically significant, we performed a non-parametric test at $\alpha=0.05$ by directly computing the distribution of the difference of bootstrap samples. The results of that test are visualized as horizontal green lines in \figref{correlations-all}.
% After that, we compute the average correlation by using the unbiased estimator given by ~\cite{Olkin1958unbiased}.

\subsection{Quality Metrics Performance}
The correlations for all the metrics are shown in \figref{correlations-all}. Our first observation is that per-metric correlations are the lowest for the \dsllff{} dataset and the highest for \dsfieldwork{} dataset. The low correlations for \dsllff{} could be partially explained by the fact that while the subjective experiment measured video quality, the metrics could only be run on individual test views (because of the lack of reference video). More importantly, these results show that current objective evaluation protocols using a sparse image set are inadequate for assessing the perceptual quality of NVS methods used for video generation. This underscores again the rationale behind the development of our new datasets, which incorporate reference videos for testing purposes. 

NVS methods are typically evaluated using PSNR, SSIM, and LPIPS. The results in \figref{correlations-all} show that the simplest metric, PSNR, performed significantly better than more complex SSIM and LPIPS. NVS evaluation clearly does not benefit from the statistics extracted by SSIM or deep features extracted by LPIPS. Poor performance of SSIM has been noted before \cite{Ponomarenko2015,Lin2019}, but it is still a popular metric because of its simplicity. The poor performance of LPIPS could be attributed to its training data consisting of small image patches with specific distortion types (noise, blur, compression-related, etc.) that are unlike NVS artifacts. 
% LPIPS trained on small patches, may have been overfitted to the distortion types that are very different from the NVS artifacts.
We did not observe a statistically significant performance difference between PSNR-L (computed on luma) and PSNR (computed on RGB). 

Similar to point estimates of correlations (\figref{metric-scatter}), the bootstrapped correlation values are the highest for the \dsfieldwork{} dataset (\figref{correlations-all}). The simple explanation for this result is that the \dsfieldwork{} dataset was more challenging for NVS methods and resulted in larger, more objectionable artifacts, as can also be seen in the subjective results in \figref{perceptual_score}. Such large differences make it much easier for the quality metrics to differentiate between the methods. In fact, most full-reference quality metrics performed well on this dataset. Video metrics VMAF and FVVDP performed well because they both consider temporal artifacts. However, we do not have evidence suggesting that more advanced metrics provide benefits over PSNR. The \dslab{} dataset, with its highly specular materials, is challenging for NVS methods. However, because it has a denser and more regular set of training views, most NVS methods performed well on those scenes, making it harder for the metrics to differentiate between the methods.

%It is noticed that compared with those on Lab and Fieldwork scenes, most metrics give a lower correlation score on the LLFF scenes. This is expected since the objective metric is only computed on a sparse set of test images, whereas the subjective score is obtained from rendered videos, \ww{which exploit the methods from border angles}\hx{I am not sure video explores a border angle, because we don't experimentally show it, and also what's the point of mentioning it here}. More importantly, this phenomenon provides strong evidence that the current objective evaluation protocol using a sparse image set is inadequate for assessing the perceptual quality of NVS models, especially when they are deployed on user-facing applications to synthesize immersive environment. This underscores again the rationale behind the development of our new dataset, which incorporates reference videos for testing purposes. It is also surprising\hx{is it fine to say 'surprising'} that LPIPS, a perceptual metric widely used in most papers, does not perform well on all three sets of scenes. 
%it is strong evidence that current objective evaluation on a sparse image set cannot reflect the perceptual quality of the models. Given that NVS techniques are increasingly deployed in user-facing systems to provide immersive realistic environments, those methods should be tested on video sequences rather than individual views. This underscores again the rationale behind the development of our new dataset, which incorporates reference videos for testing purposes.
%when they are used for end customer applications with video as output, 
%and a reference video is needed to properly evaluate the performance of the model. should be tested on video sequences


Overall, our recommendation is to \emph{test NVS methods on challenging datasets with reference videos}, such as our \dsfieldwork{} dataset. We recommend using \emph{PSNR because of its simplicity and good performance, and VMAF and FVVDP, as those metrics can detect temporal artifacts (unlike PSNR)}.



%This is because on the latter scenes, the perceptual quality difference between NVS methods is larger(see from \ref{fig:perceptual_score}), thus it becomes easier for the metrics to correlate with perceptual quality. 

%Video metrics VMAF and FVVDP perform particularly well because they both consider temporal artifacts between video frames, which is quite objective to humans. PSNR and PSNR-L also provide decent results. As a comparison, with a subtle perceptual quality difference produced by different NVS models on Lab scenes, the metrics tend 
% to struggle more and produce a lower correlation score. Our observations suggest that the selection of dataset plays a more important role than expected in the perceptual evaluation protocol. And we suggest utilizing a challenging dataset that can reduce the impact of metric selection and enable standard metrics to more accurately reflect perceptual differences between methods. This is also consistent with our conclusion in Section~\ref{benchmark}. As for metric selection, we recommend reporting the results on PSNR, because of its simplicity and good performance. We also suggest VMAF and FVVDP as well-performing video metrics.

 \paragraph{Failure Cases of PSNR}
\begin{figure}[tb]
    \centering
\includegraphics[width=\columnwidth]{img/psnr.pdf}
    \caption{Representative scenes where PSNR fails to accurately assess perceived quality. The images in the middle column have a higher PSNR value than those in the right column, but they contain obvious artifacts (e.g., blurry artifact in \scene{Glossy animals} and unnatural local shading in \scene{Metal}) and are not preferred by participants with obvious artifacts (e.g., blurry artifact in \scene{Glossy animals} and unnatural local shading in \scene{Metal}). Please note that these artifacts are more noticeable in videos.}
    \label{failurecase}
    \vspace{-3mm}
\end{figure}


%\PH{A section like this is typical of papers that introduce a method. I don't think we need it in the context of our paper, unless we can clearly explain (with evidence) how to overcome the limitations.} \WW{It would be quite interesting to show, but maybe we don't need a separate section, just mention it in the analysis of correlations.}
Although PSNR shows its effectiveness in evaluating NVS methods with respect to the perceived subjective quality, it is still beneficial to investigate when PSNR can fail to reflect subjective preferences. To do so, we compute per-scene correlations between PSNR scores and bootstrapped perceptual JOD values,
%on each single scene. 
and find scenes for which the metric results in poor correlations. %Some of those scenes are shown 
Figure~\ref{failurecase} shows representative scenes where PSNR fails to accurately assess perceived quality.
%Figure 1 illustrates scenes where PSNR fails to accurately assess perceived quality.
%Human tends to prefer the scene with less foggy artifact, which can produced by NVS methods.
Specifically, PSNR tends to be less sensitive to the blurry artifact produced by NVS methods, as shown in the top row middle column,
%middle column of 
between the rabbit ears in \scene{Glossy animals}. However, this artifact is easily noticeable and most people have a strong preference for the image without such distortions (top row, right column in Figure~\ref{failurecase}). %and gives a very high score
The participants of our experiments are also highly sensitive to local distortions, such as the unnatural local shading on \scene{Metal} (bottom row, middle column in Figure~\ref{failurecase}), but such artifacts may not be captured by PSNR since it averages across all pixels.
% out such local artifacts by a global measure of the whole image. 
%may be averaged out and fail to be captured by PSNR.


%, although PSNR and PSNR-L still perform decently on this dataset.

%most of the reference metrics show high correlation scores on the challenging Fieldwork scenes. This is because the perceptual difference between NVS methods is larger(see from \ref{fig:perceptual_score}) on these scenes, under this condition

%It is also surprising\hx{is it fine to say 'surprising'} that LPIPS, a perceptual metric widely used in most papers, does not perform well on all three sets of scenes. 
% This puts in question the perceptual results reported in previous papers.\WW{I probably won't make this claim}
%It is also noticeable that LPIPS-ALEX has a higher correlation than LPIPS-VGG, which is widely used in existing literature as the default version of LPIPS. 

%Most of the reference metrics have high correlation scores on the challenging Real-world scenes, this is because objective differences \WW{what do you mean by objective difference?} between the methods are larger in this dataset. Video metrics VMAF and FVVDP perform especially well because they both consider temporal artifacts between video frames, which is quite objective to humans. PSNR and PSNR-L also provide decent results. As a comparison, the differences between the methods are more subtle in the Lab scenes and therefore it is witnessed that the metrics struggle more in these scenes and produce a lower correlation score. PSNR and PSNR-L still perform well on this dataset, whereas FVVDP and FSIM also produce robust results, this is because xxx.\hx{@Rafal, do you have any clue why FSIM performs well on lab scenes?}

%Our recommendation for an improved perceptual evaluation protocol is to evaluate on reference videos as the sparse test images cannot properly evaluate models for end-customer applications.

%Our results motivate the need for reference videos during perceptual evaluation protocol as the sparse test images cannot properly evaluate models for end-customer applications. Meanwhile, a high correlation score on real-world scenes suggests that neural rendering methods should be evaluated and compared on challenging scenes, where current metrics can better reflect the difference between methods. This is also consistent with our conclusion in Section~\ref{benchmark}. 


%Same to the conclusion we draw from section 6, we suggest that neural rendering models should be evaluated and compared on challenging scenes. 

%To assess the accuracy of existing objective metrics and identify any correlations with our subjective evaluation results, we looked into a set of standard full reference image similarity metrics such as the widely-used PSNR, SSIM\cite{ssim}, LPIPS \cite{lpips}, several video quality metrics including FoVVDP \cite{fovvdp}, VMAF\cite{vmaf0,vmaf1,vmaf2}, as well as a set of non-reference metrics that focus on detecting the noise and blurs in the images. A full list of the metrics and descriptions can be found in Table \ref{tab:metrics}
% \footnote{https://github.com/Netflix/vmaf}. \WW{TODO: add more citation}
% \FB{NOTE: there's no real citation for VMAF, so I added the website on github, a proto-publication by the guys that did it}

% \FB{Shall we add also FLIP? For rendering is quite used:
% https://developer.nvidia.com/blog/flip-a-difference-evaluator-for-alternating-images/} \WW{I agree, FLIP seems to be used quite often in graphics}

% \FB{Probably this has to be discussed with Rafal, but we don't have enough metrics about videos; adding the Movie Index? http://utw10503.utweb.utexas.edu/research/Quality/movie.html}



%\WW{@hanxue I removed the old version which describes a set of the metrics in detail. Let's replace it with an explanation of any special things we did on the metrics, just as param's paper}
% %% image metrics
% We follow the standard protocol for evaluation against standard image similarity metrics, where we re-render the reconstructed scenes with novel camera extrinsics and compare them with the ground truth images. 
% % PSNR
% We report PSNR, which is log-correlated with the inverse mean square error of two images. Although it is the most straightforward metric to measure image difference and network convergence, it has been widely reported that PSNR prefers blurry images instead of sharp details with an overall shift in pixel locations, whereas human perception clearly favors the latter \cite{hypernerf}.
% % SSIM
% SSIM \cite{ssim} is another widely used metric that measures the perceived similarity in structural information by considering the spatial dependence in pixels.
% % LPIPS
% LPIPS \cite{lpips} is a neural-based similarity score with an emphasis on matching human perception. It applies pre-trained image classification networks and compares the difference in the activation of two target images. \WW{TODO: add lpips version info}

% %% video metrics
% As we focus on the fidelity of rendered video of reconstructed scenes, we also include several video quality metrics that take temporal artifacts into account.
% % FoVVDP
% We include FoVVP \cite{fovvdp}, a spatial-temporal quality metric that accounts for changes over time and visual field with different peripheral acuity. It is therefore important for dynamic content with a large field of view.
% % VMAF
% VMAF is another video metric that tries to match the subjective quality by combining multiple elementary scores that are sensitive to different types of source content and artifacts. A Support Vector Machine (SVM) regressor is then applied to fuse all scores into a final VMAF rating. The elementary metrics used include VIF \cite{VIF} and DLM \cite{DLM}, which are image-based metrics that focus on fidelity loss and attention-aware content visibility respectively, as well as a simple temporal smoothness measure.


% \section{Minimum Difference for Perceptual Improvement}
% \label{sec:min-diff}
\iffalse
\begin{figure}[htbp]
    \centering
    \subfigure[Lab Dataset]
    {\includegraphics[width=0.45\textwidth]{img/met_pred_error_lab1.pdf}}
    \subfigure[Fieldwork Dataset]{\includegraphics[width=0.45\textwidth]{img/met_pred_error_outdoor1.pdf}}
    % \vspace{-4.5mm}
    \caption{The estimated prediction error for four selected metrics: PSNR, PSNR-L, FVVDP, and VMAF.}
    \label{fig:pred_error}
\end{figure}
\fi


% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[t]{\columnwidth}
%   \centering
%   % include third image
%   \includegraphics[width=\columnwidth]{img/met_pred_error_lab.pdf}
% % \vspace{-5mm}
% \caption{\dslab{} dataset}
% \label{fig:lab-diff} 
% \end{subfigure}
% \begin{subfigure}[t]{\columnwidth}
%   \centering
%   % include third image
%   \includegraphics[width=\columnwidth]{img/met_pred_error_outdoor.pdf}
% % \vspace{-5mm}
% \caption{\dsfieldwork{} dataset}
% \label{fig:outdoor-dif} 
% \end{subfigure}
%     % \vspace{-3.5mm}
%     \caption{The estimated prediction error for four selected metrics.}
%     \label{fig:pred_error}
%     % \vspace{-3.5mm}
% \end{figure}


% Our analysis lets us determine the smallest improvement in terms of quality metric prediction (e.g., dB for PSNR) that ensures the result is statistically significant; robust to all sources of noise, including the inaccuracy of the metric, variance in the subjective data, and the selection of scenes.  We typically want to make any claim of improved performance at $\alpha=0.05$ significance level, at which there is only 5\% chance that the observed improvement is due to measurement noise or other random factors. 

% We bootstrapped metric predictions due to all the aforementioned factors and calculated the distributions of prediction errors. We treat the metric prediction error as a normally distributed random variable with the standard deviation estimated by bootstrapping. In \figref{pred_error} we plot the cumulative distributions for the differences of such random variables. As the minimum significant difference, we select the point of $P(0.95)$ (one-tailed z-test at $\alpha=0.05$). The distributions are plotted separately for \dslab{} and \dsfieldwork{} datasets because of large differences in metric performance between them. The results show that %\RM{I can finish that once the figure is updated.} 




%Our analysis reveals that two NVS methods can be said to be perceptually different even for small differences in quality on the \dslab{}. For example, the required difference is 1.18 dB for PSNR computed on RGB images, 1.25 dB for PSNR computed on luma values, and 0.785 JOD for FVVDP. The video metric, VMAF does not correlate well with subjective scores for this dataset (\figref{correlations-all}) and, thus, requires a huge difference in quality.

%On the more challenging \dsfieldwork{} dataset, larger minimum differences are required to separate methods --- 1.54 for PSNR, 1.58 for PSNR-L, and 1.87 for FVVDP. Here, VMAF is the top-performing metric and distinguishes two methods as perceptual different if the difference in quality is at least 11.9 DMOS.


%In this section, we identify the minimum difference in metric scores required to state with confidence (at $\alpha=0.5$) that one method is perceptually different from another.
% Even for the metric with the highest correlation with perceptual quality (Lab scenes: PSNR-L $\rho=0.73$, Real-world scenes: VMAF $\rho=0.91$), it is still hard to say whether this metric is good enough to evaluate and compare different neural view synthesis methods. Researchers also care about whether a certain improvement of metric score means an indeed perceptually better model. To answer this question, we determine the smallest difference in metric scores (denoted as the difference bar) that can tell us with confidence(at $\alpha=0.05$) that one model is perceptually better than the other. To find this bar, there are several sources of measurement error that we need to take into account: (a) the selection of scenes, (b) the measurement error in subjective experiment results, and (c) the inherent inaccuracy of a metric as an evaluation metric. 


%To determine the minimum improvement required for a specific metric, we rely on bootstrapped RMSE~\cite{hanji2022comparison,Mooney1993}. We aligned the bootstrapped perceptual scores (as obtained in \secref{corre_compute}) to the same units as the metric and then computed RMSE of the metric predictions. The distributions of the prediction errors, $R\E$ (shown in the top row of Figure~\ref{pred_error}), indicate the inherent inaccuracy of using this metric as a substitute for subjective quality assessment. According to the Kolmogorov-Smirnov test, the error distributions are normal at $\alpha=0.05$ significance level so we can use parametric statistics for the subsequent analysis.

% The bottom row of \figref{pred_error} plots the cumulative distribution of the difference in the error estimates. It shows that we need a PSNR difference of at least 1.47dB to tell that the method with higher PSNR is perceptually better ($5\%$ chance of making a mistake). Similarly, the required difference bars are high for other metrics (22.8 DMOS for VMAF and 1.83 JOD for FVVDP).
% \WW{Can we list a few methods that gives smaller improvement in metrics than this region, then link it to our subjective scores for LLFF scenes?}
% % \WW{Maybe very optional given the limited space left. Don't worry too much about it for now, but we may still want to show some examples in supp.}
% \PH{This would have been cool to show, but does not work anymore. The problem is our current analysis is dataset-specific. Existing works haven't reported differences using our datasets}

% To achieve a sound measurement, we adopt bootstrapped RMSE to estimate this bar~\cite{hanji2022comparison,Mooney1993}. Given the bootstrapped sample as obtained from Section~\ref{corre_compute}, we fit a linear mapping from the perceptual JOD values to metric predictions and then computed RMSE in terms of the metric error. The distributions of the prediction errors are shown in the top row of Figure~\ref{pred_error}. This distribution can be regarded as an expected metric error with respect to the subjective score, which is caused by the inherent inaccuracy of using this metric for subjective quality assessment. 
% %These distributions could be interpreted as an expected metric error with respect to the subjective scores.
% When we compare two methods with such a metric, the difference bar should counteract this estimation error. As the error distributions are normal at $\alpha=0.05$ significance level by the Kolmogorov-Smirnov test, we estimated the mean and standard deviation
% for the bootstrapped samples. The cumulative distribution for the difference in the error estimates is plotted at the bottom of Figure~\ref{pred_error}. It shows that we need a PSNR difference of at least 1.47dB to tell that the method with higher PSNR is perceptually better ($5\%$ chance of making a mistake). Similarly, the required difference bars are also high for other metrics (22.8 DMOS for VMAF and 1.83 JOD for FVVDP). Since the improvement in quality reported in most papers falls below
% these bars, it is doubtful whether solely evaluating these quality metrics valid the effectiveness of the proposed method.

%\PH{Good-performing metrics that we "recommend" are all linear; no need qualify that our analysis is only valid for linear metrics}

% Please note that such an analysis can only be applied to the metrics which are approximately linearly related to the perceptual quality score (JOD). And it is not applicable to metrics with strong non-linearity relation with perceived quality score (e.g., SSIM). Therefore, we only conduct analysis on PSNR, PSNR-L, VMAF, and FVVDP, which are designed to be well correlated with the perceived magnitude of distortion.




\iffalse
\begin{figure}[t]
    \centering
    % metric_comparison/bootstrp_and_scale.m jod_bstrp.jod
    % metric_comparison/plot_mse_btrp.m met_pred_error.png
    \includegraphics[width=.5\textwidth]{img/met_pred_error.eps}
    \caption{The estimated prediction error for four selected metrics \PH{We need a plot/table showing the real difference between methods when compared with metrics. Then, we can argue about whether one method is better than another or not.} \RM{I agree.}}
    \label{fig:pred_error}
\end{figure}
\fi



\section{Conclusions}
\iffalse

%FB Note: typically when referring to previously written parts of the paper should use present perfect; e.g., in the conclusion part
As NVS methods are increasingly deployed in immersive and realistic AR/VR applications, the synthesis of high-quality free-viewpoint videos has become crucial for a given method.
%it becomes crucial for the methods to synthesize high-quality free-viewpoint videos for users.
In this work, we have conducted a comprehensive study on how existing representative NVS 
methods perform with respect to the perceived video quality by HVS. We have evaluated videos synthesized by several NVS methods on our newly collected two datasets, and they are measured via a subjective quality assessment experiment. Based on the subjective results, we have tested how well the existing image and video quality metrics can predict the perceptual performance of NVS methods. %From there, we estimate the minimum metric score difference required to indicate an improvement in perceptual quality. 
\fi


The primary application of NVS methods is the interactive exploration of 3D scenes. Yet, those methods are typically tested on isolated views instead of videos, which could mimic such 3D exploration. In this work, we collected two new datasets with reference videos and used them to evaluate 8 representative NVS methods (and two variants) in a subjective quality assessment experiment. The results helped us to identify the strengths and weaknesses of tested NVS methods, but also to evaluate 18 image/video quality metrics. We found that (a) existing quality metrics struggle to differentiate between the NVS methods when they are tested on datasets with a dense set of training views; and (b) SSIM and LPIPS, which are two commonly used quality metrics, perform worse than PSNR when evaluating NVS methods. Our recommendation is to evaluate NVS methods on challenging datasets with sparsely sampled views and to use both PSNR and video metrics, such as VMAF and FovVideoVDP. 

%\section{sup}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
