\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
%\usepackage{float}
\usepackage{float}

\usepackage{booktabs}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{nicematrix}
\usepackage{gensymb}
%\usepackage{subfigure}
\usepackage{subcaption}
\input{macros}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}
%\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{3341} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
%\title{Perceptual Quality Assessment of Neural View Synthesis Methods\\for Reconstruction of Front-Facing Views
%}
\title{Supplementary for \\ Perceptual Quality Assessment of NeRF and Neural View Synthesis Methods \\ for Front-Facing Views}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi





%In this section, we identify the minimum difference in metric scores required to state with confidence (at $\alpha=0.5$) that one method is perceptually different from another.
%Even for the metric with the highest correlation with perceptual quality (Lab scenes: PSNR-L $\rho=0.73$, Real-world scenes: VMAF $\rho=0.91$), it is still hard to say whether this metric is good enough to evaluate and compare different neural view synthesis methods. Researchers also care about whether a certain improvement of metric score means an indeed perceptually better model. To answer this question, we determine the smallest difference in metric scores (denoted as the difference bar) that can tell us with confidence(at $\alpha=0.05$) that one model is perceptually better than the other. To find this bar, there are several sources of measurement error that we need to take into account: (a) the selection of scenes, (b) the measurement error in subjective experiment results, and (c) the inherent inaccuracy of a metric as an evaluation metric. 


%To determine the minimum improvement required for a specific metric, we rely on bootstrapped RMSE~\cite{hanji2022comparison,Mooney1993}. We aligned the bootstrapped perceptual scores (as obtained in \secref{corre_compute}) to the same units as the metric and then computed RMSE of the metric predictions. The distributions of the prediction errors, $R\E$ (shown in the top row of Figure~\ref{pred_error}), indicate the inherent inaccuracy of using this metric as a substitute for subjective quality assessment. According to the Kolmogorov-Smirnov test, the error distributions are normal at $\alpha=0.05$ significance level so we can use parametric statistics for the subsequent analysis.

%The bottom row of \figref{pred_error} plots the cumulative distribution of the difference in the error estimates. It shows that we need a PSNR difference of at least 1.47dB to tell that the method with higher PSNR is perceptually better ($5\%$ chance of making a mistake). Similarly, the required difference bars are high for other metrics (22.8 DMOS for VMAF and 1.83 JOD for FVVDP).
% \WW{Can we list a few methods that gives smaller improvement in metrics than this region, then link it to our subjective scores for LLFF scenes?}
% % \WW{Maybe very optional given the limited space left. Don't worry too much about it for now, but we may still want to show some examples in supp.}
% \PH{This would have been cool to show, but does not work anymore. The problem is our current analysis is dataset-specific. Existing works haven't reported differences using our datasets}

%To achieve a sound measurement, we adopt bootstrapped RMSE to estimate this bar~\cite{hanji2022comparison,Mooney1993}. Given the bootstrapped sample as obtained from Section~\ref{corre_compute}, we fit a linear mapping from the perceptual JOD values to metric predictions and then computed RMSE in terms of the metric error. The distributions of the prediction errors are shown in the top row of Figure~\ref{pred_error}. This distribution can be regarded as an expected metric error with respect to the subjective score, which is caused by the inherent inaccuracy of using this metric for subjective quality assessment. 
%These distributions could be interpreted as an expected metric error with respect to the subjective scores.
%When we compare two methods with such a metric, the difference bar should counteract this estimation error. As the error distributions are normal at $\alpha=0.05$ significance level by the Kolmogorov-Smirnov test, we estimated the mean and standard deviation for the bootstrapped samples. The cumulative distribution for the difference in the error estimates is plotted at the bottom of Figure~\ref{pred_error}. It shows that we need a PSNR difference of at least 1.47dB to tell that the method with higher PSNR is perceptually better ($5\%$ chance of making a mistake). Similarly, the required difference bars are also high for other metrics (22.8 DMOS for VMAF and 1.83 JOD for FVVDP). Since the improvement in quality reported in most papers falls below these bars, it is doubtful whether solely evaluating these quality metrics valid the effectiveness of the proposed method.

\section{Supplementary Video}
\begin{figure*}[t]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
    %pearson.py
    \includegraphics[width=0.8\textwidth,height=0.55\textwidth]{img/pearson_correlations1.pdf}
\end{center}
   \caption{Bootstrapped distributions of Pearson Linear Correlation Coefficients (PLCC) for all metrics, reported separately for each dataset. The higher the number, the better is metric's performance. The notation is the same as for Figure~6 in the main paper.}
\label{fig:pearson}
\end{figure*}
We have included reference video clips of the scenes in both of our new datasets. However, we were unable to include the synthesized clips generated by the NVS methods due to file size restrictions on the supplementary materials. We will make them available on the project web page upon acceptance of the paper. 

\section{Per-scene Subjective Quality}
%To run the subjective pairwise comparison experiment, we selected a diverse subset of 14 scenes from our collected Lab and Fieldwork dataset and 8 scenes from LLFF dataset. Due to the size limitation, we only attach the groundtruth video reference of our collected datasets in the supplementary material (also down-sampling the frame numbers by two on Lab scenes). We will upload all the reference videos as well as the reconstruction by NVS methods on our project website afterwards. 

Due to space limitations, the main document contains subjective scores averaged across all scenes in each dataset (Figure~4 in the main document). Figures~\ref{fig:jod_lab},~\ref{fig:jod_fieldwork} and~\ref{fig:jod_llff} show the subjective results individually for each scene. These results show large variations across the scenes, but they also exhibit common trends:
\begin{itemize}
    \item The generalizable methods GNT and IBRNet perform poorly on all scenes in our new \dslab{} and \dsfieldwork{} datasets (worse than NeRF), but much better on the public \dsllff{} dataset. Per-scene fine-tuning (-S suffix) improves the predictions of both methods. 
    \item Similarly DVGO performs poorly on our new datasets, but much better on the \dsllff{} dataset.
    \item LFNR has rather uneven performance --- it is one of the best methods for some scenes (\dslab{}/CD-occlusions (I/E), \dslab{}/Glossy animals (I), \dsfieldwork{}/Naiad statue) but it fails in the others. 
    \item MipNeRF was one the most robust methods, performing typically better or on par with NeRF. In some of the scenes, it matched the quality of the reference (\dslab{}/Glass, \dsfieldwork{}/Leopards, \dsfieldwork{}/Giraffe, \dsfieldwork{}/Naiad statue, \dsfieldwork{}/Vespa). 
    \item Plenoxel performed well in most scenes in \dsllff{} dataset (except Room) but was generally worse than NeRF when tested on the \dslab{} dataset. Its performance varied from scenes to scene in the \dsfieldwork{} dataset, with a few fail cases (Dinosaur and Whale) but also better-than-NeRF performance (Leopards, Naiad statue, Vespa).
\end{itemize}

%The averaged result is depicted in Figure 4 in the main document and here, we show the results of each scene in . We observe a large variance in subjective quality between scenes. A qualitative results of each scene on Lab and Fieldwork datasets could be observed in Figure~\ref{lab_quality} and ~\ref{fieldwork_quality}.



\section{Metric Prediction Scatter Plots}
Metric predictions for individual scenes are compared with subjective scores in scatter plots in Figures~\ref{fig:scatter_lab}, ~\ref{fig:scatter_outdoor} and ~\ref{fig:scatter_llff}. When metric predictions are accurate, the scatter plot forms a possibly tight curve. The scatter plots for \dslab{} dataset in \figref{scatter_lab} show the difficulty of the task on this dataset --- objective and subjective measures of quality are not well correlated for any of the tested metrics. They correlate even worse on \dsllff{} dataset~\ref{fig:scatter_llff}, which demonstrates that testing on sparse views in current evaluation protocol is insufficient to assess the subjective quality of synthesized videos. The scatter plots, however, form much stronger relations for the \dsfieldwork{} dataset in \figref{scatter_outdoor}. 

%While the non-reference metric PIQE seems to perform well when scores are averaged across the scenes (see \figref{pearson}), it shows a much worse performance when the predictions analyzed per scene in \figref{scatter_lab}. The correlation of non-reference metrics is very low in general. 

% RM: The scatter plots are not about point estimation - they are about visualizing your data (before averaging or computing any statistics). 
%, we show the point estimate of Spearman correlation on Lab and Fieldwork datasets. Compared to bootstrapped estimations shown in Figure 6 of main paper, we observe a similar performance between these two estimations. Compared to point estimations, bootstrapped correlations can mitigate measurement errors from the selection of scenes and subjective
%experiment results noise.


\section{Metric Performance: PLCC and RMSE}
Apart from Spearman Rank Order correlation, we also compute the boostrapped distribution of Pearson Linear Correlation Coefficient (PLCC) and Root Mean Squared Error between the image metrics score and perceptual quality score on each dataset. The results are shown in Figures~\ref{fig:pearson} and~\ref{fig:rmse}. With a few exceptions, the trends shown in those plots are similar to those shown for SROCC in Figure~6 of the main paper. The difference worth noting is that while the correlations (PLCC and SROCC) are much higher for the \dsfieldwork{} than for the \dslab{} dataset (indicating good metric performance), the opposite trend is shown by the RMSE. The RMSE values are on average smaller for the \dslab{} dataset, suggesting higher metric accuracy. It must be noted, however, that the range of subjective scores is much larger for the \dsfieldwork{} dataset (refer to the scatter plots in \figref{scatter_lab} and \figref{scatter_outdoor}). The difference in the RMSE numbers is most likely due to very different magnitudes of distortions in each dataset. If the goal of a metric is to differentiate between NVS methods, the correlation coefficients are better indicators of metric performance. 

% RM: I would not repeat what was already stated in the main paper
%Similar to that of Spearman correlation, we observe a lower correlations of these metrics for LLFF dataset compared to that of Lab and Fieldwork dataset, which proves again the motivation of incorporating reference videos for testing purposes. Similarly, it is shown that PSNR performed significantly better than SSIM and LPIPS metrics on both Fieldwork and Lab datasets. The same to that shown in Spearman correlation, we observe that VMAF and  FVVDP perform well on Fieldwork, while PIQE and FVVDP give decent results on Lab dataset. It is noteworthy that the perceptual metric DISTS perform well on both Lab and Fieldwork datasets, which is due to its ability to model both texture and structure similarity between images.



\begin{figure*}[t]
\begin{center}
%experiment\analyze_data\plot_method_cmp_sup.m
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
    \includegraphics[width=0.9\textwidth,height=0.52\textwidth]{img/jod_lab.eps}
\end{center}
   \caption{Perceptual preference of different NeRF methods on the \dslab{} dataset. The notation is the same as in Figure~4 in the main paper. The scenes with (I) in the label used the novel view selection that required only interpolation of the views, while the scenes with (E) required the views to be extrapolated.}
%\label{fig:long}
\label{fig:jod_lab}
\end{figure*}

\begin{figure*}[t]
\begin{center}
%experiment\analyze_data\plot_method_cmp_sup.m
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
    \includegraphics[width=0.9\textwidth,height=0.52\textwidth]{img/jod_outdoor.eps}
\end{center}
   \caption{Perceptual preference of NeRF methods on the \dsfieldwork{} dataset.}
%\label{fig:long}
\label{fig:jod_fieldwork}
\end{figure*}



\begin{figure*}[t]
\begin{center}
%experiment\analyze_data\plot_method_cmp_sup.m
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
    \includegraphics[width=0.9\textwidth,height=0.5\textwidth]{img/jod_llff.eps}
\end{center}
   \caption{Perceptual preference of NeRF methods on the \dsllff{} dataset.}
%\label{fig:long}
\label{fig:jod_llff}
\end{figure*}

\begin{figure*}[t]
\begin{center}
%\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   %rmse.py
    \includegraphics[width=0.8\textwidth,height=0.55\textwidth]{img/rmse_correlations1.pdf}
\end{center}
   \caption{Bootstrapped distributions of Room Mean Squared Errors (RMSE) for all metrics computed separately for each dataset. The lower the number, the better is metric's performance. The notation is the same as for Figure~6 in the main paper.}
\label{fig:rmse}
\end{figure*}
\begin{figure}[t]
    \centering
%metric_comparison\plot_mse_bstrp_seperate.m
    \includegraphics[width=1.0\columnwidth]{img/met_pred_error_sep.pdf}
    \caption{The estimated prediction error for four selected metrics.}
    \label{met_error}
\end{figure}

\begin{figure}[ht]
    \centering
    %nerf_benchmark\bootstrp_images.m  outdoor_q_bstrp.mat lab_q_bstrp.mat
    %metric_comparison\plot_rankings_two.m 
    \includegraphics[width=.45\textwidth]{img/seperate_ranking_main1.pdf}
    \caption{Ranking bootstrapped distributions for NVS
methods on the Lab and Fieldwork datasets.  The red
errors bars show the minimum difference in metric score that indicates improved quality for selected metrics (described in Section 1)}
    \label{rank_main}
\end{figure}
\section{Minimum Difference for Perceptual Improvement}
\label{sec:min-diff}




\iffalse
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{\columnwidth}
  \centering
  % include third image
  \includegraphics[width=\columnwidth]{img/met_pred_error_lab.pdf}
% \vspace{-5mm}
\caption{\dslab{} dataset}
\label{fig:lab-diff} 
\end{subfigure}
\begin{subfigure}[t]{\columnwidth}
  \centering
  % include third image
  \includegraphics[width=\columnwidth]{img/met_pred_error_outdoor.pdf}
% \vspace{-5mm}
\caption{\dsfieldwork{} dataset}
\label{fig:outdoor-dif} 
\end{subfigure}
    % \vspace{-3.5mm}
    \caption{The estimated prediction error for four selected metrics.}
    \label{fig:pred_error}
    % \vspace{-3.5mm}
\end{figure}
 \fi

Our analysis lets us determine the smallest improvement in terms of quality metric prediction (e.g., dB for PSNR) that ensures the result is perceptually better; %statistically significant;
robust to all sources of noise, including the inaccuracy of the metric, variance in the subjective data, and the selection of scenes.  We typically want to make the claim of improved performance at $\alpha=0.05$ significance level, at which there is only 5\% chance that the observed improvement is due to measurement noise or other random factors. 

We bootstrapped metric predictions due to all the aforementioned factors and calculated the distributions of prediction errors. We treat the metric prediction error as a normally distributed random variable with the standard deviation estimated by bootstrapping. In Figure ~\ref{met_error}, we plot the cumulative distributions for the differences of such random variables. As the minimum significant difference, we select the point of $P(0.95)$ (one-tailed z-test at $\alpha=0.05$). The distributions are plotted separately for \dslab{} and \dsfieldwork{} datasets because of large differences in metric performance between them. %The results show that %\RM{I can finish that once the figure is updated.} 

Our analysis reveals that two NVS methods can be said to be perceptually different even for small differences in metric score on the \dslab{} dataset. For example, the required difference is 1.3 dB for PSNR computed on RGB images, 1.37 dB for PSNR computed on luma values, and 0.294 JOD for FVVDP. The video metric, VMAF does not correlate well with subjective scores for this dataset (shown in Figure~6 of main paper) and, thus, requires a huge difference in quality.

On the more challenging \dsfieldwork{} dataset, larger minimum differences are required to separate methods --- 1.68 dB for PSNR, 1.73 dB for PSNR-L, and 0.725 for FVVDP. Here, VMAF is the top-performing metric and distinguishes two methods as perceptual different if the difference in quality is at least 9.32 DMOS.

In Figure~\ref{rank_main}, we show the boostrapped performance of NVS methods on Lab and Fieldwork datasets. For each dataset, we show the top-2 performant metrics (from Figure 6 of main paper). If the metrics were accurate, the
results shown in Figure~\ref{rank_main}  should match the subjective results from
Figure 4 of main paper. We can observe that most metrics capture the trend. We also show the minimum difference for perceptual improvement in the figure (illustrate with red error bar).

\section{Training details}
\paragraph{DVGO} We follow the training setup as in ~\cite{dirctvoxgo} and set expected numbers of voxels to be $M(c) =
100^3$ and $M(f) = 160^3$ in coarse and fine stages. The points sampling step sizes are set to half
of the voxel sizes, i.e., $\delta(c) = 0.5 \cdot s(c)$ and $\delta(f) = 0.5 \cdot s(f)$.
The shallow MLP layer comprises two hidden layers with
128 channels. The Adam optimizer~\cite{adam} is employed with a batch
size of 8192 rays to optimize the coarse and fine scene representations for 10k and 20k iterations. The base learning
rates are 0.1 for all voxel grids and $10^{-3}$ for the shallow
MLP. The exponential learning rate decay is applied. 
\paragraph{NeRF} We follow the pytorch implementation of NeRF~\cite{lin2020nerfpytorch}. We use a batch size of 1024 rays, each sampled at $N_c = 64$
coordinates in the coarse volume and $N_f = 128$ additional coordinates in the
fine volume. We use the Adam optimizer with a base learning rate at
$5 \times 10^{-4}$ and optimize for $2\times10^5$ iterations.

\paragraph{GNT} For the cross-scene generalizable GNT model (denoted as GNT-C), we use the pre-trained model released by~\cite{gnt}. For finetuned version of GNT model on each scene (denoted as GNT-S), we finetune the cross-scene model with Adam optimizer with base learning rates for the feature
extraction network and GNT as $10^{-3}$
and $5 \times 10^{-4}$
respectively, which decay exponentially over
training steps. For all our experiments, we train for 50,000 steps with 4096 rays sampled in each
iteration. 

\paragraph{IBRNet}
For the cross-scene generaliable IBRNet model (denoted as IBRNet-C), we use the pre-trained model from~\cite{ibrnet}. During fine-tuning stage for IBRNet-S, we optimize both 2D feature extractor and IBRNet itself with Adam optimizer using base learning rates of $5\times 10^{-4}$ and $2\times 10^{-4}$).

\paragraph{LFNR}
The architecture of transformer is the same as the ones recently introduced for vision related
tasks \cite{transformer}. Each transformer has 8 blocks and the internal feature size is 256. In each training step, we randomly choose a target image and sample a batch of random
rays from it. The batch size is 128. We train for 250 000 iterations with the Adam optimizer and a linear learning
rate decay schedule with 5000 warm-up steps. 

\paragraph{MipNeRF}
We follow the training procedure specified by~\cite{mipnerf}: 1
million iterations of Adam with a batch size of 4096
and a learning rate that is annealed logarithmically from
$5 \cdot 10{-4}$ to $5 \cdot 10^{-6}$.
\paragraph{NeX}
As in~\cite{NeX}, we use an
MPI with 192 layers with $ M = 12$  consecutive planes sharing one set of texture coefficients. We sample and render 8,000 pixels in the training view for photometric loss computation. The network is  trained for 4,000 epochs using Adam optimizer with
a learning rate of 0.01 for base color and 0.001 for both
networks and a decay factor of 0.1 every 1,333 epochs.
\paragraph{Plenoxel} The implementation of Plenoxel is based on a custom PyTorch CUDA~\cite{cuda} extension library to achieve fast
differentiable volume rendering. We use a batch size of 5000 rays and optimize with RMSProp~\cite{Rmsprop}. For optimization of density, we use the same delayed exponential learning rate schedule as MipNeRF~\cite{mipnerf}, where the exponential is scaled by a learning rate of 30 and decays to
0.05 at step 250000, with an initial delay period of 15000
steps. For SH we adopts a pure exponential decay learning rate
schedule, with an initial learning rate of 0.01 that decays to
$5 \times 10^{-6}$ at step 250000.




\begin{figure*}[t]
%jod_lab3.csv jod_outdoor3.csv(change name, change reference to be 10) 
%nerf_benchmark\plot_correlations_all.m
\begin{center}
    \includegraphics[width=0.9\textwidth,height=0.5\textwidth]{img/correlation_lab.eps}
\end{center}
   \caption{Scatter plots of per-scene metric predictions vs. subjective scores for the \dslab{} dataset. The subjective scores of each scene are shifted such that Reference videos have jod values equal to 10. The numbers above each plot show Spearman correlation. Note that the correlation reported in other plots has been computed on the metric predictions and subjective scores averaged across all scenes. }
%\label{fig:long}
\label{fig:scatter_lab}
\end{figure*}

\begin{figure*}[t]
%jod_lab3.csv jod_outdoor3.csv(change name, change reference to be 10) 
%nerf_benchmark\plot_correlations_all.m
\begin{center}
\includegraphics[width=0.9\textwidth]{img/correlation_outdoor.eps}
\end{center}
   \caption{Scatter plots of per-scene metric predictions vs. subjective scores for the \dsfieldwork{} dataset. The notation is the same as in \figref{scatter_lab}.}
%\label{fig:long}
\label{fig:scatter_outdoor}
\end{figure*}

\begin{figure*}[t]
%jod_lab3.csv jod_outdoor3.csv(change name, change reference to be 10) 
%nerf_benchmark\plot_correlations_all.m
\begin{center}
    \includegraphics[width=0.95\textwidth]{img/correlation_llff.eps}
\end{center}
\vspace{-4mm}
   \caption{Scatter plots of per-scene metric predictions vs. subjective scores for the \dsllff{} dataset. The subjective scores of each scene are shifted such that NeRF results have jod values equal to 10. The notation is the same as in \figref{scatter_lab}.}
%\label{fig:long}
%\vspace{-4mm}
\label{fig:scatter_llff}
\end{figure*}

\begin{figure*}[hb]
\begin{center}
    \vspace{-4mm}
\includegraphics[width=1.0\textwidth]{img/quality_results_sup2.pdf}
\end{center}
\vspace{-5mm}
   \caption{Example inference results for \dslab{} dataset.}
%\label{fig:long}
\label{lab_quality}
\end{figure*}

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=\linewidth]{img/quality_results_sup1.pdf}
\end{center}
   \caption{Example inference results for \dsfieldwork{} dataset.}
%\label{fig:long}
\label{fieldwork_quality}
\end{figure*}




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
