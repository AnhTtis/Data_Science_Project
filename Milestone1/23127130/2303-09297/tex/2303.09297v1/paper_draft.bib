@article{goodman2017european,
  title={European Union regulations on algorithmic decision-making and a “right to explanation”},
  author={Goodman, Bryce and Flaxman, Seth},
  journal={AI magazine},
  volume={38},
  number={3},
  pages={50--57},
  year={2017}
}

@article{singla2019explanation,
  title={Explanation by progressive exaggeration},
  author={Singla, Sumedha and Pollack, Brian and Chen, Junxiang and Batmanghelich, Kayhan},
  journal={arXiv preprint arXiv:1911.00483},
  year={2019}
}

@article{keane2023stupid,
  title={Why Explainable AI (XAI) Is Not that Intelligent},
  author={Keane, Mark T},
  journal={tbd},
  year={2023}
}

@inproceedings{liu2019generative,
  title={Generative counterfactual introspection for explainable deep learning},
  author={Liu, Shusen and Kailkhura, Bhavya and Loveland, Donald and Han, Yong},
  booktitle={2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)},
  pages={1--5},
  year={2019},
  organization={IEEE}
}

@inproceedings{almahairi2018augmented,
  title={Augmented cyclegan: Learning many-to-many mappings from unpaired data},
  author={Almahairi, Amjad and Rajeshwar, Sai and Sordoni, Alessandro and Bachman, Philip and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={195--204},
  year={2018},
  organization={PMLR}
}

@article{mertes2020not,
  title={This is not the texture you are looking for! Introducing novel counterfactual explanations for non-experts using generative adversarial learning},
  author={Mertes, Silvan and Huber, Tobias and Weitz, Katharina and Heimerl, Alexander and Andr{\'e}, Elisabeth},
  journal={arXiv preprint arXiv:2012.11905},
  year={2020}
}

@article{he2019attgan,
  title={Attgan: Facial attribute editing by only changing what you want},
  author={He, Zhenliang and Zuo, Wangmeng and Kan, Meina and Shan, Shiguang and Chen, Xilin},
  journal={IEEE transactions on image processing},
  volume={28},
  number={11},
  pages={5464--5478},
  year={2019},
  publisher={IEEE}
}
@inproceedings{zhu2020domain,
  title={In-domain gan inversion for real image editing},
  author={Zhu, Jiapeng and Shen, Yujun and Zhao, Deli and Zhou, Bolei},
  booktitle={European conference on computer vision},
  pages={592--608},
  year={2020},
  organization={Springer}
}

@inproceedings{akula2020cocox,
  title={Cocox: Generating conceptual and counterfactual explanations via fault-lines},
  author={Akula, Arjun and Wang, Shuai and Zhu, Song-Chun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={03},
  pages={2594--2601},
  year={2020}
}

@article{mccloy2002semifactual,
  title={Semifactual “even if” thinking},
  author={McCloy, Rachel and Byrne, Ruth MJ},
  journal={Thinking \& Reasoning},
  volume={8},
  number={1},
  pages={41--67},
  year={2002},
  publisher={Taylor \& Francis}
}

@article{celar_cfactual23,
  title={How People Reason with Counterfactual and Causal Explanations for Artificial Intelligence Decisions in Familiar and Unfamiliar Domains},
  author={Celar, Lenart and Byrne, Ruth M.J.},
  journal={Memory \& Cognition},
  year={2023},
  publisher={Psychonomic Society}
}

@article{sokol2020one,
  title={One explanation does not fit all: The promise of interactive explanations for machine learning transparency},
  author={Sokol, Kacper and Flach, Peter},
  journal={KI-K{\"u}nstliche Intelligenz},
  volume={34},
  number={2},
  pages={235--250},
  year={2020},
  publisher={Springer},
  doi = {https://doi.org/10.1007/s13218-020-00637-y}
}

@inproceedings{arya2021one,
  title={One Explanation Does Not Fit All: A Toolkit And Taxonomy Of AI Explainability Techniques},
  author={Arya, Vijay and Bellamy, Rachel K and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C and Houde, Stephanie and Liao, Q Vera and Luss, Ronny and Mojsilovic, Aleksandra and others},
  booktitle={INFORMS Annual Meeting},
  year={2021},
  doi = {https://doi.org/10.48550/arXiv.1909.03012}
}



@article{van2021evaluating,
  title={Evaluating XAI: A comparison of rule-based and example-based explanations},
  author={van der Waa, Jasper and Nieuwburg, Elisabeth and Cremers, Anita and Neerincx, Mark},
  journal={Artificial Intelligence},
  volume={291},
  pages={103404},
  year={2021},
  publisher={Elsevier},
  url = {https://doi.org/10.1016/j.artint.2020.103404},
    doi = {10.1016/j.artint.2020.103404},
    issn = {00043702},
    keywords = {Artificial Intelligence (AI), Contrastive explanations, Decision support systems, Explainable Artificial Intelligence (XAI), Machine learning, User evaluations}
}

@inproceedings{lim2009and,
  title={Why and why not explanations improve the intelligibility of context-aware intelligent systems},
  author={Lim, Brian Y and Dey, Anind K and Avrahami, Daniel},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={2119--2128},
  year={2009}
}

@inproceedings{cai2019effects,
  title={The effects of example-based explanations in a machine learning interface},
  author={Cai, Carrie J and Jongejan, Jonas and Holbrook, Jess},
  booktitle={Proceedings of the 24th international conference on intelligent user interfaces},
  pages={258--262},
  year={2019}
}

@article{shortliffe1975computer,
  title={Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system},
  author={Shortliffe, Edward H and Davis, Randall and Axline, Stanton G and Buchanan, Bruce G and Green, C Cordell and Cohen, Stanley N},
  journal={Computers and biomedical research},
  volume={8},
  number={4},
  pages={303--320},
  year={1975},
  publisher={Elsevier}
}

@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}
}

@article{hohman2018visual,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={8},
  pages={2674--2693},
  year={2018},
  publisher={IEEE}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}
}

@article{simonyan2013deep,
  title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}

@inproceedings{camburu2020make,
  title={Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations},
  author={Camburu, Oana-Maria and Shillingford, Brendan and Minervini, Pasquale and Lukasiewicz, Thomas and Blunsom, Phil and Xie, Linhai and Miao, Yishu and Wang, Sen and Blunsom, Phil and Wang, Zhihua and others},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ‚ACL 2020 ‚Seattle, Washington, USA ‚July 5- 10 ‚2020},
  pages={116--125},
  year={2020},
  organization={Association for Computational Linguistics}
}


@inproceedings{guidotti2020explaining,
  title={Explaining image classifiers generating exemplars and counter-exemplars from latent representations},
  author={Guidotti, Riccardo and Monreale, Anna and Matwin, Stan and Pedreschi, Dino},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={09},
  pages={13665--13668},
  year={2020}
}


@inproceedings{ming2019interpretable,
  title={Interpretable and steerable sequence learning via prototypes},
  author={Ming, Yao and Xu, Panpan and Qu, Huamin and Ren, Liu},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={903--913},
  year={2019}
}


@inproceedings{Dai2022CounterfactualXAI,
    title = {{Counterfactual Explanations for Prediction and Diagnosis in XAI}},
    year = {2022},
    booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
    author = {Dai, Xinyue and Keane, Mark T. and Shalloo, Laurence and Ruelle, Elodie and Byrne, Ruth M.J.},
    month = {7},
    pages = {215--226},
    publisher = {ACM},
    url = {https://dl.acm.org/doi/10.1145/3514094.3534144},
    address = {New York, NY, USA},
    isbn = {9781450392471},
    doi = {10.1145/3514094.3534144}
}


@inproceedings{kenny2021generating,
  title={On Generating Plausible Counterfactual and Semi-Factual Explanations for Deep Learning}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17377}, DOI={10.1609/aaai.v35i13.17377}, abstract={There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet, counterfactual explanations have become very popular in eXplainable AI (XAI) due to their asserted computational, psychological, and legal benefits. In contrast however, semi-factuals (which appear to be equally useful) have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly because the non-discrete nature of images makes good counterfactuals difficult to define; indeed, generating plausible counterfactual images which lie on the data manifold is also problematic. This paper advances a novel method for generating plausible counterfactuals and semi-factuals for black-box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all “exceptional” features in a test image to be “normal” from the perspective of the counterfactual class, to generate plausible counterfactual images. Two controlled experiments compare this method to others in the literature, showing that PIECE generates highly plausible counterfactuals (and the best semi-factuals) on several benchmark measures.}, number={13}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Kenny, Eoin M. and Keane, Mark T}, year={2021}, month={May}, pages={11575-11585} 
}



@misc{pichai_2018, 
title={AI at Google: our principles},
howpublished = "\url{https://www.blog.google/technology/ai/ai-principles/}",
journal={Google},
publisher={Google},
author={Pichai, Sundar},
year={2018},
month={Jun},
note = "[Online; accessed 01-June-2020]"
}


@misc{quick_draw_google, 
title={Quick, Draw!},
howpublished = "\url{https://quickdraw.withgoogle.com/}",
journal={Google},
publisher={Google},
author={Pichai, Sundar},
year={2021},
month={Mar},
note = "[Online; accessed 01-March-2021]"
}



@article{wachter2017right,
  title={Why a right to explanation of automated decision-making does not exist in the general data protection regulation},
  author={Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  journal={International Data Privacy Law},
  volume={7},
  number={2},
  pages={76--99},
  year={2017},
  publisher={Oxford University Press}
}

@inproceedings{kim2018interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
  booktitle={International conference on machine learning},
  pages={2668--2677},
  year={2018},
  organization={PMLR}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{kenny2021explaining,
  title={Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies},
  author={Kenny, Eoin M and Ford, Courtney and Quinn, Molly and Keane, Mark T},
  journal={Artificial Intelligence},
  volume={294},
  pages={1--25},
  year={2021},
  publisher={Elsevier}
}

@article{wu2021explaining,
  title={On Explaining Your Explanations of BERT: An Empirical Study with Sequence Classification},
  author={Wu, Zhengxuan and Ong, Desmond C},
  journal={arXiv preprint arXiv:2101.00196},
  year={2021}
}

@inproceedings{buccinca2020proxy,
  title={Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems},
  author={Bu{\c{c}}inca, Zana and Lin, Phoebe and Gajos, Krzysztof Z and Glassman, Elena L},
  booktitle={Proceedings of the 25th International Conference on Intelligent User Interfaces},
  pages={454--464},
  year={2020}
}

@inproceedings{kim2014bayesian,
  title={The bayesian case model: A generative approach for case-based reasoning and prototype classification},
  author={Kim, Been and Rudin, Cynthia and Shah, Julie A},
  booktitle={Advances in neural information processing systems},
  pages={1952--1960},
  year={2014}
}

@inproceedings{cai2019effects,
  title={The effects of example-based explanations in a machine learning interface},
  author={Cai, Carrie J and Jongejan, Jonas and Holbrook, Jess},
  booktitle={Proceedings of the 24th International Conference on Intelligent User Interfaces},
  pages={258--262},
  year={2019}
}


@article{nugent2009gaining,
  title={Gaining insight through case-based explanation},
  author={Nugent, Conor and Doyle, D{\'o}nal and Cunningham, P{\'a}draig},
  journal={Journal of Intelligent Information Systems},
  volume={32},
  number={3},
  pages={267--295},
  year={2009},
  publisher={Springer}
}

@article{lombrozo2006structure,
  title={The structure and function of explanations},
  author={Lombrozo, Tania},
  journal={Trends in cognitive sciences},
  volume={10},
  number={10},
  pages={464--470},
  year={2006},
  publisher={Elsevier},
  doi = {10.1016/j.tics.2006.08.004},
    issn = {13646613},
    pmid = {16942895}
}

@article{martens2014explaining,
  title={Explaining data-driven document classifications},
   year = {2014},
    journal = {MIS Quarterly: Management Information Systems},
    author = {Martens, David and Provost, Foster},
    number = {1},
    pages = {73--99},
    volume = {38},
    doi = {10.25300/MISQ/2014/38.1.04},
    issn = {21629730},
    keywords = {Comprehensibility, Document classification, Instance level explanation, Text mining}
}

@article{thagard1989extending,
  title={Extending explanatory coherence},
  author={Thagard, Paul},
  journal={Behavioral and brain sciences},
  volume={12},
  number={3},
  pages={490--502},
  year={1989},
  publisher={Cambridge University Press},
  doi = {doi:10.1017/S0140525X00057319}
}

@article{lombrozo2016explanatory,
  title={Explanatory preferences shape learning and inference},
  author={Lombrozo, Tania},
  journal={Trends in cognitive sciences},
  volume={20},
  number={10},
  pages={748--759},
  year={2016},
  publisher={Elsevier},
  doi = {10.1016/j.tics.2016.08.001},
    issn = {1879307X},
    pmid = {27567318}
}

@article{edwards2019explanation,
  title={Explanation recruits comparison in a category-learning task},
  author={Edwards, Brian J. and Williams, Joseph J. and Gentner, Dedre and Lombrozo, Tania},
  journal={Cognition},
  volume={185},
  pages={21--38},
  year={2019},
  publisher={Elsevier},
  doi = {https://doi.org/10.1016/j.cognition.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0010027718303275}
}

@article{jeyakumar2020can,
  title={How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods},
  author={Jeyakumar, Jeya Vikranth and Noor, Joseph and Cheng, Yu-Hsi and Garcia, Luis and Srivastava, Mani},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}




@inproceedings{lee2020improving,
  title={Improving Trust in Deep Neural Networks with Nearest Neighbors},
  author={Lee, Ritchie and Clarke, Justin and Agogino, Adrian and Giannakopoulou, Dimitra},
  booktitle={AIAA Scitech 2020 Forum},
  pages={2098},
  year={2020}
}

@article{angwin2016machine,
  title={Machine bias},
  author={Angwin, Julia and Larson, Jeff and Mattu, Surya and Kirchner, Lauren},
  journal={ProPublica, May},
  volume={23},
  pages={2016},
  year={2016}
}

@article{gunning2017explainable,
  title={Explainable artificial intelligence (xai)},
  author={Gunning, David},
  journal={Defense Advanced Research Projects Agency (DARPA), nd Web},
  volume={2},
  year={2017}
}

@incollection{NIPS2017_7062,
title = {A Unified Approach to Interpreting Model Predictions},
author = {Lundberg, Scott M and Lee, Su-In},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4765--4774},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@article{gilpin2019explaining,
  title={Explaining explanations to society},
  author={Gilpin, Leilani H and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
  journal={arXiv preprint arXiv:1901.06560},
  year={2019}
}

@article{gsponer16background,
  title={Background Knowledge Injection for Interpretable Sequence Classification},
  author={Gsponer, Severin and Costabello, Luca and Le Van, Chan and Pai, Sumit and Gueret, Christophe and Ifrim, Georgiana and Lecue, Freddy}
}

@article{le2019interpretable,
  title={Interpretable time series classification using linear models and multi-resolution multi-domain symbolic representations},
  author={Le Nguyen, Thach and Gsponer, Severin and Ilie, Iulia and O’Reilly, Martin and Ifrim, Georgiana},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={4},
  pages={1183--1222},
  year={2019},
  publisher={Springer}
}

@article{nugent2005case,
  title={A case-based explanation system for black-box systems},
  author={Nugent, Conor and Cunningham, P{\'a}draig},
  journal={Artificial Intelligence Review},
  volume={24},
  number={2},
  pages={163--178},
  year={2005},
  publisher={Springer},
  doi = {10.1007/s10462-005-4609-5},
    issn = {02692821}
}

@misc{leake1996cbr,
  title={CBR in context: the present and future. Case based reasoning experiences-lessons and future experiences. D. Leake},
  author={Leake, DB},
  year={1996},
  publisher={Cambridge, MIT Press}
}

@inproceedings{hanney1996learning,
  title={Learning adaptation rules from a case-base},
  author={Hanney, Kathleen and Keane, Mark T},
  booktitle={European Workshop on Advances in Case-Based Reasoning},
  pages={179--192},
  year={1996},
  organization={Springer}
}

@inproceedings{smyth1995experiments,
  title={Experiments on adaptation-guided retrieval in case-based design},
  author={Smyth, Barry and Keane, Mark T},
  booktitle={International Conference on Case-Based Reasoning},
  pages={313--324},
  year={1995},
  organization={Springer}
}



@book{kolodner2014case,
  title={Case-based reasoning},
  author={Kolodner, Janet},
  year={2014},
  publisher={Morgan Kaufmann}
}

@inproceedings{nugent2005best,
  title={The best way to instil confidence is by being right},
  author={Nugent, Conor and Cunningham, P{\'a}draig and Doyle, D{\'o}nal},
  booktitle={International Conference on Case-Based Reasoning},
  pages={368--381},
  year={2005},
  organization={Springer}
}

@inproceedings{forster2021capturing,
  title={Capturing Users’ Reality: A Novel Approach to Generate Coherent Counterfactual Explanations},
  author={F{\"o}rster, Maximilian and H{\"u}hn, Philipp and Klier, Mathias and Kluge, Kilian},
  booktitle={Proceedings of the 54th Hawaii International Conference on System Sciences},
  year={2021},
  pages = {1274--1283},
  volume = {0},
  isbn = {9780998133140},
  doi = {10.24251/hicss.2021.155}
}

@article{forster2020evaluating,
  title={Evaluating explainable Artifical intelligence--What users really appreciate},
  author={F{\"o}rster, Maximilian and Klier, Mathias and Kluge, Kilian and Sigler, Irina},
  year={2020},
  url ={https://aisel.aisnet.org/ecis2020_rp/195}
}

@article{forster2020fostering,
  title={Fostering Human Agency: A Process for the Design of User-Centric XAI Systems},
  author={F{\"o}rster, Maximilian and Klier, Mathias and Kluge, Kilian and Sigler, Irina},
  year={2020}
}



@article{lucic2019contrastive,
  title={Contrastive explanations for large errors in retail forecasting predictions through monte carlo simulations},
  author={Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
  journal={arXiv preprint arXiv:1908.00085},
  year={2019}
}

@inproceedings{dodge2019explaining,
  title={Explaining models: an empirical study of how explanations impact fairness judgment},
  author={Dodge, Jonathan and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K.E. and Dugan, Casey},
  booktitle={Proceedings of the 24th International Conference on Intelligent User Interfaces},
  pages={275--285},
  year={2019}
}

@inproceedings{mittelstadt2019explaining,
  title={Explaining explanations in AI},
  author={Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={279--288},
  year={2019}
}


@inproceedings{GlickKAha19,
  title={DARPA XAI Phase 1 Evaluations Report},
  author={Glickenhaus, B., Karneeb, J. & Aha, D.W.},
  booktitle={DARPA XAI Program, Report},
  year={2019}
}


@article{gunning2019darpa,
  title={DARPA's Explainable Artificial Intelligence Program},
  author={Gunning, David and Aha, David W},
  journal={AI Magazine},
  volume={40},
  number={2},
  pages={44--58},
  year={2019},
  publisher={Association for the Advancement of Artificial Intelligence}
}

@inproceedings{dodge2019explaining,
  title={Explaining models: an empirical study of how explanations impact fairness judgment},
  author={Dodge, Jonathan and Liao, Q Vera and Zhang, Yunfeng and Bellamy, Rachel KE and Dugan, Casey},
  booktitle={Proceedings of the 24th International Conference on Intelligent User Interfaces},
  pages={275--285},
  year={2019}
}

@article{mueller2019explanation,
  title={Explanation in human-AI systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable AI},
  author={Mueller, Shane T. and Hoffman, Robert R. and Clancey, William and Emrey, Abigail and Klein, Gary},
  journal={arXiv preprint arXiv:1902.01876},
  year={2019}
}

@article{erdfelder1996gpower,
  title={GPOWER: A general power analysis program},
  author={Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  journal={Behavior research methods, instruments, \& computers},
  volume={28},
  number={1},
  pages={1--11},
  year={1996},
  publisher={Springer}
}

@inproceedings{goyal2019counterfactual,
  title = 	 {Counterfactual Visual Explanations},
  author =       {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2376--2384},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/goyal19a/goyal19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/goyal19a.html},
  abstract = 	 {In this work, we develop a technique to produce counterfactual visual explanations. Given a ‘query’ image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c’$. To do this, we select a ‘distractor’ image $I’$ that the system predicts as class $c’$ and identify spatial regions in $I$ and $I’$ such that replacing the identified region in $I$ with the identified region in $I’$ would push the system towards classifying $I$ as $c’$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.}
}











@article{wallace2018interpreting,
  title={Interpreting neural networks with nearest neighbors},
  author={Wallace, Eric and Feng, Shi and Boyd-Graber, Jordan},
  journal={arXiv preprint arXiv:1809.02847},
  year={2018}
}

@inproceedings{sitawarin2019robustness,
  title={On the robustness of deep k-nearest neighbors},
  author={Sitawarin, Chawin and Wagner, David},
  booktitle={2019 IEEE Security and Privacy Workshops (SPW)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}


@article{linyi2021exploring,
  title={Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis},
  author={Linyi, Yang and Jiazheng, Li and P{\'a}draig, Cunningham and Yue, Zhang and Barry, Smyth and Ruihai, Dong},
  journal={In ACL 2021},
  year={2021}
}


@article{guo2020real,
  title={Real-time facial affective computing on mobile devices},
  author={Guo, Yuanyuan and Xia, Yifan and Wang, Jing and Yu, Hui and Chen, Rung-Ching},
  journal={Sensors},
  volume={20},
  number={3},
  pages={870},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}



@article{vasquez2019explainable,
  title={Explainable Prediction of Chronic Renal Disease in the Colombian Population Using Neural Networks and Case-Based Reasoning},
  author={V{\'a}squez-Morales, Gabriel R and Mart{\'\i}nez-Monterrubio, Sergio M and Moreno-Ger, Pablo and Recio-Garc{\'\i}a, Juan A},
  journal={IEEE Access},
  volume={7},
  pages={152900--152910},
  year={2019},
  publisher={IEEE}
}

@article{de2005retrieval,
  title={Retrieval, reuse, revision and retention in case-based reasoning},
  author={De Mantaras, Ramon Lopez and McSherry, David and Bridge, Derek and Leake, David and Smyth, Barry and Craw, Susan and Faltings, Boi and Maher, Mary Lou and T COX, MICHAEL and Forbus, Kenneth and others},
  journal={The Knowledge Engineering Review},
  volume={20},
  number={3},
  pages={215--240},
  year={2005},
  publisher={Cambridge University Press}
}

@article{khan2019survey,
  title={A survey of the recent architectures of deep convolutional neural networks},
  author={Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
  journal={arXiv preprint arXiv:1901.06032},
  year={2019}
}


@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}


@inproceedings{menikdiwela2017cnn,
  title={CNN-based small object detection and visualization with feature activation mapping},
  author={Menikdiwela, Medhani and Nguyen, Chuong and Li, Hongdong and Shaw, Marnie},
  booktitle={2017 International Conference on Image and Vision Computing New Zealand (IVCNZ)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}



@article{lin2013network,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1312.4400},
  year={2013}
}

@inproceedings{yang2020generating,
  title={Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification},
  author={Yang, Linyi and Kenny, Eoin and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={6150--6160},
  year={2020}
}

@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}
}


@inproceedings{lee2020improving,
  title={Improving Trust in Deep Neural Networks with Nearest Neighbors},
  author={Lee, Ritchie and Clarke, Justin and Agogino, Adrian and Giannakopoulou, Dimitra},
  booktitle={AIAA Scitech 2020 Forum},
  pages={2098},
  year={2020}
}

@inproceedings{Byrne2019CounterfactualsReasoning,
    title = {{Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from human reasoning}},
    year = {2019},
    booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
    author = {Byrne, Ruth M.J.},
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    pages = {6276--6282},
    isbn = {9780999241141},
    doi = {10.24963/ijcai.2019/876},
    issn = {10450823},
    keywords = {Humans and AI: Cognitive Modeling, Machine Learning: Explainable Machine Learning}
}


@article{DBLP:journals/corr/abs-1711-09784,
  author    = {Nicholas Frosst and
               Geoffrey E. Hinton},
  title     = {Distilling a Neural Network Into a Soft Decision Tree},
  journal   = {CoRR},
  volume    = {abs/1711.09784},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.09784},
  archivePrefix = {arXiv},
  eprint    = {1711.09784},
  timestamp = {Mon, 13 Aug 2018 16:48:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-09784},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{zeiler_visualizing_2013,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{bachpixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science}
}

@inproceedings{sundararajan_axiomatic_2017,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3319--3328},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{selvaraju_grad-cam:_2016,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={618--626},
  year={2017}
}

@article{olden_illuminating_2002,
	title = {Illuminating the “black box”: a randomization approach for understanding variable contributions in artificial neural networks},
	volume = {154},
	issn = {03043800},
	shorttitle = {Illuminating the “black box”},
	language = {en},
	number = {1-2},
	urldate = {2018-08-22},
	journal = {Ecological Modelling},
	author = {Olden, Julian D and Jackson, Donald A},
	month = aug,
	year = {2002},
	pages = {135--150},
	file = {Olden and Jackson - 2002 - Illuminating the “black box” a randomization appr.pdf:/Users/eoinkenny/Zotero/storage/4CWASBCD/Olden and Jackson - 2002 - Illuminating the “black box” a randomization appr.pdf:application/pdf}
}

@inproceedings{ribeiro_why_2016,
  title={Why should i trust you?: Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016},
  organization={ACM}
}

@article{de_ona_extracting_2014,
	title = {Extracting the contribution of independent variables in neural network models: a new approach to handle instability},
	volume = {25},
	issn = {0941-0643, 1433-3058},
	shorttitle = {Extracting the contribution of independent variables in neural network models},
	language = {en},
	number = {3-4},
	urldate = {2018-08-22},
	journal = {Neural Computing and Applications},
	author = {de Oña, Juan and Garrido, Concepción},
	month = sep,
	year = {2014},
	pages = {859--869},
	file = {de Oña and Garrido - 2014 - Extracting the contribution of independent variabl.pdf:/Users/eoinkenny/Zotero/storage/ME54NU77/de Oña and Garrido - 2014 - Extracting the contribution of independent variabl.pdf:application/pdf}
}

@article{bai_factor_2011,
	title = {Factor {Sensitivity} {Analysis} with {Neural} {Network} {Simulation} based on {Perturbation} {System}},
	volume = {6},
	issn = {1796-203X},
	language = {en},
	number = {7},
	urldate = {2018-08-22},
	journal = {Journal of Computers},
	author = {Bai, Runbo and Jia, Hailei and Cao, Pingzhou},
	month = jul,
	year = {2011},
	file = {Bai et al. - 2011 - Factor Sensitivity Analysis with Neural Network Si.pdf:/Users/eoinkenny/Zotero/storage/AZDBRF9D/Bai et al. - 2011 - Factor Sensitivity Analysis with Neural Network Si.pdf:application/pdf}
}

@incollection{hutchison_feature_2012,
	address = {Berlin, Heidelberg},
	title = {Feature {Salience} for {Neural} {Networks}: {Comparing} {Algorithms}},
	volume = {7666},
	isbn = {978-3-642-34477-0 978-3-642-34478-7},
	shorttitle = {Feature {Salience} for {Neural} {Networks}},
	url = {http://link.springer.com/10.1007/978-3-642-34478-7_51},
	abstract = {One of the key problems in the field of telemedicine is the prediction of the patient’s health state change based on incoming non-invasively measured vital data. Artificial Neural Networks (ANN) are a powerful statistical modeling tool suitable for this problem. Feature salience algorithms for ANN provide information about feature importance and help selecting relevant input variables. Looking for a reliable salience analysis algorithm, we found a relatively wide range of possible approaches. However, we have also found numerous methodological weaknesses in corresponding evaluations. Perturb [11][7] and Connection Weight (CW) [1] are two of the most promising algorithms. In this paper, we propose an improvement for Connection Weight and evaluate it along with Perturb and the original CW. We use three independent datasets with already known feature salience rankings as well as varying topologies and random feature ranking results to estimate the usability of the tested approaches for feature salience assessment in complex multi-layer perceptrons.},
	language = {en},
	urldate = {2018-08-22},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Heinze, Theodor and von Löwis, Martin and Polze, Andreas},
	year = {2012},
	doi = {10.1007/978-3-642-34478-7_51},
	pages = {415--422},
	file = {Heinze et al. - 2012 - Feature Salience for Neural Networks Comparing Al.pdf:/Users/eoinkenny/Zotero/storage/LBQJHA55/Heinze et al. - 2012 - Feature Salience for Neural Networks Comparing Al.pdf:application/pdf}
}

@inproceedings{tintarev_survey_2007,
	address = {Istanbul, Turkey},
	title = {A survey of explanations in recommender systems},
	isbn = {978-1-4244-0831-3 978-1-4244-0832-0},
	language = {en},
	urldate = {2018-08-22},
	booktitle = {2007 {IEEE} 23rd {International} {Conference} on {Data} {Engineering} {Workshop}},
	publisher = {IEEE},
	author = {Tintarev, Nava and Masthoff, Judith},
	month = apr,
	year = {2007},
	pages = {801--810},
	file = {Tintarev and Masthoff - 2007 - A Survey of Explanations in Recommender Systems.pdf:/Users/eoinkenny/Zotero/storage/P6J7ARNK/Tintarev and Masthoff - 2007 - A Survey of Explanations in Recommender Systems.pdf:application/pdf}
}

@article{aha_explainable_nodate,
	title = {Explainable {Artiﬁcial} {Intelligence}},
	language = {en},
	author = {Aha, David W and Darrell, Trevor and Doherty, Patrick and Magazzeni, Daniele},
	pages = {195},
	file = {Aha et al. - Explainable Artiﬁcial Intelligence.pdf:/Users/eoinkenny/Zotero/storage/49KMP494/Aha et al. - Explainable Artiﬁcial Intelligence.pdf:application/pdf}
}

@article{olden_accurate_2004,
	title = {An accurate comparison of methods for quantifying variable importance in artificial neural networks using simulated data},
	volume = {178},
	issn = {03043800},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0304380004001565},
	doi = {10.1016/j.ecolmodel.2004.03.013},
	abstract = {Artiﬁcial neural networks (ANNs) are receiving greater attention in the ecological sciences as a powerful statistical modeling technique; however, they have also been labeled a “black box” because they are believed to provide little explanatory insight into the contributions of the independent variables in the prediction process. A recent paper published in Ecological Modelling [Review and comparison of methods to study the contribution of variables in artiﬁcial neural network models, Ecol. Model. 160 (2003) 249–264] addressed this concern by providing a comprehensive comparison of eight different methodologies for estimating variable importance in neural networks that are commonly used in ecology. Unfortunately, comparisons of the different methodologies were based on an empirical dataset, which precludes the ability to establish generalizations regarding the true accuracy and precision of the different approaches because the true importance of the variables is unknown. Here, we provide a more appropriate comparison of the different methodologies by using Monte Carlo simulations with data exhibiting deﬁned (and consequently known) numeric relationships. Our results show that a Connection Weight Approach that uses raw input-hidden and hidden-output connection weights in the neural network provides the best methodology for accurately quantifying variable importance and should be favored over the other approaches commonly used in the ecological literature. Average similarity between true and estimated ranked variable importance using this approach was 0.92, whereas, similarity coefﬁcients ranged between 0.28 and 0.74 for the other approaches. Furthermore, the Connection Weight Approach was the only method that consistently identiﬁed the correct ranked importance of all predictor variables, whereas, the other methods either only identiﬁed the ﬁrst few important variables in the network or no variables at all. The most notably result was that Garson’s Algorithm was the poorest performing approach, yet is the most commonly used in the ecological literature. In conclusion, this study provides a robust comparison of different methodologies for assessing variable importance in neural networks that can be generalized to other data and from which valid recommendations can be made for future studies.},
	language = {en},
	number = {3-4},
	urldate = {2018-08-24},
	journal = {Ecological Modelling},
	author = {Olden, Julian D and Joy, Michael K and Death, Russell G},
	month = nov,
	year = {2004},
	pages = {389--397},
	file = {Olden et al. - 2004 - An accurate comparison of methods for quantifying .pdf:/Users/eoinkenny/Zotero/storage/B5ZL5UW9/Olden et al. - 2004 - An accurate comparison of methods for quantifying .pdf:application/pdf}
}

@article{goh_back-propagation_1995,
	title = {Back-propagation neural networks for modeling complex systems},
	volume = {9},
	issn = {09541810},
	url = {http://linkinghub.elsevier.com/retrieve/pii/095418109400011S},
	doi = {10.1016/0954-1810(94)00011-S},
	abstract = {In complex engineering systems, empirical relationships are often employed to estimate design parameters and engineering properties. A complex domain is characterized by a number of interacting factors and their relationships are, in general, not precisely known. In addition, the data associated with these parameters are usually incomplete or erroneous (noisy). The development of these empirical relationships is a formidable task requiring sophisticated modeling techniques as well as human intuition and experience. This paper demonstrates the use of back-propagation neural networks to alleviate this problem. Backpropagation neural networks are a product of artificial intelligence research. First, an overview of the neural network methodology is presented. This is followed by some practical guidelines for implementing back-propagation neural networks. Two examples are then presented to demonstrate the potential of this approach for capturing nonlinear interactions between variables in complex engineering systems.},
	language = {en},
	number = {3},
	urldate = {2018-08-24},
	journal = {Artificial Intelligence in Engineering},
	author = {Goh, A.T.C.},
	month = jan,
	year = {1995},
	pages = {143--151},
	file = {Goh - 1995 - Back-propagation neural networks for modeling comp.pdf:/Users/eoinkenny/Zotero/storage/HGSS8FTX/Goh - 1995 - Back-propagation neural networks for modeling comp.pdf:application/pdf}
}

@inproceedings{shrikumar_learning_2017,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3145--3153},
  year={2017},
  organization={JMLR. org}
}



@article{reategui_combining_1997,
	title = {Combining a neural network with case-based reasoning in a diagnostic system},
	volume = {9},
	issn = {09333657},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0933365796003594},
	doi = {10.1016/S0933-3657(96)00359-4},
	abstract = {This paper presents a new approach for integrating case-based reasoning (CBR) with a neural network (NN) in diagnostic systems. When solving a new problem, the neural network is used to make hypotheses and to guide the CBR module in the search for a similar previous case that supports one of the hypotheses. The knowledge acquired by the network is interpreted and mapped into symbolic diagnosis descriptors, which are kept and used by the system to determine whether a final answer is credible, and to build explanations for the reasoning carried out. The NN-CBR model has been used in the development of a system for the diagnosis of congenital heart diseases (CHD). The system has been evaluated using two cardiological databases with a total of 214 CHD cases. Three other well-known databases have been used to evaluate the NN-CBR approach further. The hybrid system manages to solve problems that cannot be solved by the neural network with a good level of accuracy. Additionally, the hybrid system suggests some solutions for common CBR problems, such as indexing and retrieval, as well as for neural network problems, such as the interpretation of the knowledge stored in a neural network and the explanation of reasoning. Copyright 0 1997 Elsevier Science B.V.},
	language = {en},
	number = {1},
	urldate = {2018-09-21},
	journal = {Artificial Intelligence in Medicine},
	author = {Reategui, E.B and Campbell, J.A and Leao, B.F},
	month = jan,
	year = {1997},
	pages = {5--27},
	annote = {General Notes:
Summary of Paper:

The exact same as before... but this time it's a medical diagnostic system.

 
What They Concluded

Performance {\textgreater}= normal CNM
Diagnostic descriptors are similar to experts after enough data.
Diagnostic descriptors can give insights into the NN decision process.

 
Quotes \& Responses:
???
???},
	file = {Reategui et al. - 1997 - Combining a neural network with case-based reasoni.pdf:/Users/eoinkenny/Zotero/storage/AQZ8DL5Q/Reategui et al. - 1997 - Combining a neural network with case-based reasoni.pdf:application/pdf}
}

@article{chung-kwan_shin_hybrid_2000,
  title={A hybrid approach of neural network and memory-based learning to data mining},
  author={Shin, Chung-Kwan and Yun, Ui Tak and Kim, Huy Kang and Park, Sang Chan},
  journal={IEEE Transactions on Neural Networks},
  volume={11},
  number={3},
  pages={637--646},
  year={2000},
  publisher={IEEE}
}

@inproceedings{amin_answering_2018,
  title={Answering with Cases: A {CBR} Approach to Deep Learning},
  author={Amin, Kareem and Kapetanakis, Stelios and Althoff, Klaus-Dieter and Dengel, Andreas and Petridis, Miltos},
  booktitle={International Conference on Case-Based Reasoning},
  pages={15--27},
  year={2018},
  organization={Springer}
}

@article{clancey_epistemology_1983,
	title = {The epistemology of a rule-based expert system —a framework for explanation},
	volume = {20},
	issn = {00043702},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0004370283900085},
	doi = {10.1016/0004-3702(83)90008-5},
	language = {en},
	number = {3},
	urldate = {2018-09-26},
	journal = {Artificial Intelligence},
	author = {Clancey, William J.},
	month = may,
	year = {1983},
	pages = {215--251},
	file = {Clancey - 1983 - The epistemology of a rule-based expert system —a .pdf:/Users/eoinkenny/Zotero/storage/EG2787NF/Clancey - 1983 - The epistemology of a rule-based expert system —a .pdf:application/pdf}
}


@article{lipton_2018,
	title = {The mythos of model interpretability},
	volume = {16(3)},
	journal = {Queue},
	author = {Lipton, Zach C.},
	year = {2018},
	pages = {30}
}

@article{li_deep_2017,
	title = {Deep {Learning} for {Case}-{Based} {Reasoning} through {Prototypes}: {A} {Neural} {Network} that {Explains} {Its} {Predictions}},
	shorttitle = {Deep {Learning} for {Case}-{Based} {Reasoning} through {Prototypes}},
	url = {http://arxiv.org/abs/1710.04806},
	abstract = {Deep neural networks are widely used for classiﬁcation. These deep models often suffer from a lack of interpretability – they are particularly difﬁcult to understand because of their non-linear nature. As a result, neural networks are often treated as “black box” models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classiﬁcation process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
	language = {en},
	urldate = {2018-10-09},
	journal = {arXiv:1710.04806 [cs, stat]},
	author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.04806},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al. - 2017 - Deep Learning for Case-Based Reasoning through Pro.pdf:/Users/eoinkenny/Zotero/storage/MQVHRPNY/Li et al. - 2017 - Deep Learning for Case-Based Reasoning through Pro.pdf:application/pdf}
}

@book{klein_sources_2017,
	title = {Sources of {Power}: {How} {People} {Make} {Decisions}},
	publisher = {MIT Press},
	author = {Klein, Gary A},
	year = {2017}
}

@inproceedings{darmatasia_handwriting_2017,
	address = {Melaka, Malaysia},
	title = {Handwriting recognition on form document using convolutional neural network and support vector machines ({CNN}-{SVM})},
	isbn = {978-1-5090-4912-7},
	url = {http://ieeexplore.ieee.org/document/8074699/},
	doi = {10.1109/ICoICT.2017.8074699},
	abstract = {In this paper, we propose a workflow and a machine learning model for recognizing handwritten characters on form document. The learning model is based on Convolutional Neural Network (CNN) as a powerful feature extraction and Support Vector Machines (SVM) as a high-end classifier. The proposed method is more efficient than modifying the CNN with complex architecture. We evaluated some SVM and found that the linear SVM using L1 loss function and L2 regularization giving the best performance both of the accuracy rate and the computation time. Based on the experiment results using data from NIST SD 19 2nd edition both for training and testing, the proposed method which combines CNN and linear SVM using L1 loss function and L2 regularization achieved a recognition rate better than only CNN. The recognition rate achieved by the proposed method are 98.85\% on numeral characters, 93.05\% on uppercase characters, 86.21\% on lowercase characters, and 91.37 on the merger of numeral and uppercase characters. While the original CNN achieves an accuracy rate of 98.30\% on numeral characters, 92.33\% on uppercase characters, 83.54\% on lowercase characters, and 88.32\% on the merger of numeral and uppercase characters. The proposed method was also validated by using ten folds cross-validation, and it shows that the proposed method still can improve the accuracy rate. The learning model was used to construct a handwriting recognition system to recognize a more challenging data on form document automatically. The pre-processing, segmentation and character recognition are integrated into one system. The output of the system is converted into an editable text. The system gives an accuracy rate of 83.37\% on ten different test form document.},
	language = {en},
	urldate = {2018-10-22},
	booktitle = {2017 5th {International} {Conference} on {Information} and {Communication} {Technology} ({ICoIC}7)},
	publisher = {IEEE},
	author = {{Darmatasia} and Fanany, Mohamad Ivan},
	month = may,
	year = {2017},
	pages = {1--6},
	file = {Darmatasia and Fanany - 2017 - Handwriting recognition on form document using con.pdf:/Users/eoinkenny/Zotero/storage/ZP57FTMB/Darmatasia and Fanany - 2017 - Handwriting recognition on form document using con.pdf:application/pdf}
}

@incollection{hutchison_explanation_2004,
	address = {Berlin, Heidelberg},
	title = {Explanation {Oriented} {Retrieval}},
	volume = {3155},
	isbn = {978-3-540-22882-0 978-3-540-28631-8},
	url = {http://link.springer.com/10.1007/978-3-540-28631-8_13},
	abstract = {This paper is based on the observation that the nearest neighbour in a case-based prediction system may not be the best case to explain a prediction. This observation is based on the notion of a decision surface (i.e. class boundary) and the idea that cases located between the target case and the decision surface are more convincing as support for explanation. This motivates the idea of explanation utility, a metric that may be different to the similarity metric used for nearest neighbour retrieval. In this paper we present an explanation utility framework and present detailed examples of how it is used in two medical decision-support tasks. These examples show how this notion of explanation utility sometimes select cases other than the nearest neighbour for use in explanation and how these cases are more convincing as explanations.},
	language = {en},
	urldate = {2018-10-30},
	booktitle = {Advances in {Case}-{Based} {Reasoning}},
	publisher = {Springer Berlin Heidelberg},
	author = {Doyle, Dónal and Cunningham, Pádraig and Bridge, Derek and Rahman, Yusof},
	year = {2004},
	doi = {10.1007/978-3-540-28631-8_13},
	pages = {157--168},
	file = {Doyle et al. - 2004 - Explanation Oriented Retrieval.pdf:/Users/eoinkenny/Zotero/storage/MPW6NDC6/Doyle et al. - 2004 - Explanation Oriented Retrieval.pdf:application/pdf}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}
}



@incollection{zeng_defense_2018,
	address = {Cham},
	title = {In defense of {fully} {connected} {layers} in {visual} {representation} {transfer}},
	volume = {10736},
	isbn = {978-3-319-77382-7 978-3-319-77383-4},
	language = {en},
	urldate = {2018-12-11},
	booktitle = {Advances in {Multimedia} {Information} {Processing} – {PCM} 2017},
	author = {Zhang, Chen-Lin and Luo, Jian-Hao and Wei, Xiu-Shen and Wu, Jianxin},
	year = {2018},
	pages = {807--817},
	file = {Zhang et al. - 2018 - In Defense of Fully Connected Layers in Visual Rep.pdf:/Users/eoinkenny/Zotero/storage/I7SRZ4AE/Zhang et al. - 2018 - In Defense of Fully Connected Layers in Visual Rep.pdf:application/pdf}
}

@article{chen_this_2018,
  title={This looks like that: deep learning for interpretable image recognition},
  author={Chen, Chaofan and Li, Oscar and Barnett, Alina and Su, Jonathan and Rudin, Cynthia},
  journal={arXiv preprint arXiv:1806.10574},
  year={2018}
}

@article{zhou_object_2014,
  title={Object detectors emerge in deep scene cnns},
  author={Zhou, Bolei  and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year={2015}
}

@inproceedings{pei_deepxplore:_2017,
  title={Deepxplore: Automated whitebox testing of deep learning systems},
  author={Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana, Suman},
  booktitle={proceedings of the 26th Symposium on Operating Systems Principles},
  pages={1--18},
  year={2017},
  organization={ACM}
}

@article{doyle_review_nodate,
	title = {A {Review} of {Explanation} and {Explanation} in {Case}- {Based} {Reasoning}},
	language = {en},
	author = {Doyle, Dónal and Tsymbal, Alexey and Cunningham, Pádraig},
	pages = {24},
	year = {2003},
	file = {Doyle et al. - A Review of Explanation and Explanation in Case- B.pdf:/Users/eoinkenny/Zotero/storage/6R9R3PIF/Doyle et al. - A Review of Explanation and Explanation in Case- B.pdf:application/pdf}
}



%%% deep knn
@article{papernot2018deep,
  title={Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning},
  author={Papernot, Nicolas and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1803.04765},
  year={2018}
}

%%% beer one
@inproceedings{inproceedings,
author = {Alonso, Jose and Ramos Soto, Alejandro and Castiello, Ciro and Mencar, Corrado},
year = {2018},
month = {06},
pages = {},
title = {Hybrid Data-Expert Explainable Beer Style Classifier}
}

%%%%% taiwan cbr-knn
@inproceedings{chu2003hybrid,
  title={A hybrid case-based reasoning approach for the electrocardiogram diagnosis},
  author={CHU, Chia-Wen and CHIU, Tzu-Fu and Wu, Jiunn-Lin},
  booktitle={Proceeding of the 7th World Multiconference on Systemics, Cybernetics and Informatics},
  volume={5},
  pages={93--98},
  year={2003}
}

%%%%% garson
%%%%% taiwan cbr-knn
@article{garson1991interpreting,
  title={Interpreting neural-network connection weights},
  author={Garson, G David},
  journal={AI expert},
  volume={6},
  number={4},
  pages={46--51},
  year={1991},
  publisher={Miller Freeman, Inc.}
}

%%%%% big cbr
@article{de2005retrieval,
  title={Retrieval, reuse, revision and retention in case-based reasoning},
  author={De Mantaras, Ramon Lopez and McSherry, David and Bridge, Derek and Leake, David and Smyth, Barry and Craw, Susan and Faltings, Boi and Maher, Mary Lou and T Cox, Michael and Forbus, Kenneth and others},
  journal={The Knowledge Engineering Review},
  volume={20},
  number={3},
  pages={215--240},
  year={2005},
  publisher={Cambridge University Press}
}

%%%%% hybrid intelligent systems
@book{medsker2012hybrid,
  title={Hybrid intelligent systems},
  author={Medsker, Larry R},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{bench2017hypo,
  title={Hypo’s legacy: introduction to the virtual special issue},
  author={Bench-Capon, Trevor JM},
  journal={Artificial Intelligence and Law},
  volume={25},
  number={2},
  pages={205--250},
  year={2017},
  publisher={Springer}
}

@inproceedings{sani2017learning,
  title={Learning deep features for k-{NN}-based human activity recognition.},
  author={Sani, Sadiq and Wiratunga, Nirmalie and Massie, Stewart},
  booktitle={Proceedings	of	the	ICCBR-17	Workshop},
  year={2017},
  organization={ICCBR (Organisers)}
}

	s.	Trondheim,	Norway

@inproceedings{caruana1999case,
  title={Case-based explanation of non-case-based learning methods.},
  author={Caruana, Rich and Kangarloo, Hooshang and Dionisio, JD and Sinha, Usha and Johnson, David},
  booktitle={Proceedings of the AMIA Symposium},
  pages={212},
  year={1999},
  organization={American Medical Informatics Association}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4765--4774},
  year={2017}
}

@article{kennybayesian,
  title={Bayesian Case-Exclusion and Personalized Explanations for Sustainable Dairy Farming},
  author={Kenny, Eoin M and Ruelle, Elodie and Geoghegan, Anne and Shalloo, Laurence and O’Leary, Miche{\'a}l and O’Donovan, Michael and Temraz, Mohammed and Keane, Mark T.}
}

@inproceedings{kenny2019twin,
  title={Twin-systems to explain artificial neural networks using case-based reasoning: Comparative tests of feature-weighting methods in ANN-CBR twins for XAI},
  author={Kenny, Eoin M and Keane, Mark T},
  booktitle={Proceedings of the 28th International Joint Conferences on Artifical Intelligence (IJCAI-19)},
  pages={2708--2715},
  year={2019}
}

@article{kk2019twinsystem,
    title={The Twin-System Approach as One Generic Solution for {XAI}: An Overview of {ANN-CBR} Twins for Explaining Deep Learning},
    author={Mark T. Keane and Eoin M. Kenny},
    journal={IJCAI-19 Workshop on Explainable AI (XAI), arXiv preprint arXiv:1905.08069},
    year={2019a}
}

@article{kk2019survey,
    title={How case-based reasoning explains neural networks: A theoretical analysis of {XAI} using post-hoc explanation-by-example from a survey of {ANN-CBR} Twin-Systems},
    author={Mark T. Keane and Eoin M. Kenny},
    journal={arXiv preprint arXiv:1905.07186},
    year={2019a}
}






%%%%%%%% Workshop paper references %%%%%%%%%%%%%%%


@inproceedings{pedreschi2019meaningful,
  title={Meaningful explanations of Black Box AI decision systems},
  author={Pedreschi, Dino and Giannotti, Fosca and Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={9780--9784},
  year={2019}
}

@article{le2022improving,
  title={Improving Model Understanding and Trust with Counterfactual Explanations of Model Confidence},
  author={Le, Thao and Miller, Tim and Singh, Ronal and Sonenberg, Liz},
  journal={arXiv preprint arXiv:2206.02790},
  year={2022},
  url = {https://doi.org/10.48550/arXiv.2206.02790}
}

@article{guidotti2019factual,
  title={Factual and counterfactual explanations for black box decision making},
  author={Guidotti, Riccardo and Monreale, Anna and Giannotti, Fosca and Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
  journal={IEEE Intelligent Systems},
  volume={34},
  number={6},
  pages={14--23},
  year={2019},
  publisher={IEEE},
  doi = {10.1109/MIS.2019.2957223}
}

@inproceedings{tintarev2007survey,
  title={A survey of explanations in recommender systems},
  author={Tintarev, Nava and Masthoff, Judith},
  booktitle={2007 IEEE 23rd international conference on data engineering workshop},
  pages={801--810},
  year={2007},
  organization={IEEE}
}

@article{aamodt1994case,
  title={Case-based reasoning: Foundational issues, methodological variations, and system approaches},
  author={Aamodt, Agnar and Plaza, Enric},
  journal={AI communications},
  volume={7},
  number={1},
  pages={39--59},
  year={1994},
  publisher={IOS press}
}

@article{schoon2021human,
  title={Human-centered XAI: Developing design patterns for explanations of clinical decision support systems},
  author={Schoonderwoerd, Tjeerd A.J. and Jorritsma, Wiard and Neerincx, Mark A. and Van Den Bosch, Karel},
  journal={International Journal of Human-Computer Studies},
  volume={154},
  pages={102684},
  year={2021},
  publisher={Elsevier},
  issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2021.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1071581921001026}
}

@inproceedings{wang2019designing,
  title={Designing theory-driven user-centric explainable AI},
  author={Wang, Danding and Yang, Qian and Abdul, Ashraf and Lim, Brian Y.},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--15},
  year={2019},
  doi = {https://doi.org/10.1145/3290605.3300831}
}


@inproceedings{panigutti2022understanding,
  title={Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems},
  author={Panigutti, Cecilia and Beretta, Andrea and Giannotti, Fosca and Pedreschi, Dino},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--9},
  year={2022}
}


@article{adadi2018peeking,
  title={Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}

@inproceedings{biran2017explanation,
  title={Explanation and justification in machine learning: A survey},
  author={Biran, Or and Cotton, Courtenay},
  booktitle={IJCAI-17 workshop on explainable AI (XAI)},
  volume={8},
  pages={1},
  year={2017}
}



@article{frosst2017distilling,
  title={Distilling a neural network into a soft decision tree},
  author={Frosst, Nicholas and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1711.09784},
  year={2017}
}

@article{biswas2017hybrid,
  title={Hybrid case-based reasoning system by cost-sensitive neural network for classification},
  author={Biswas, Saroj Kr and Chakraborty, Manomita and Singh, Heisnam Rohen and Devi, Debashree and Purkayastha, Biswajit and Das, Akhil Kr},
  journal={Soft Computing},
  volume={21},
  number={24},
  pages={7579--7596},
  year={2017},
  publisher={Springer}
}

@inproceedings{cunningham2003evaluation,
  title={An evaluation of the usefulness of case-based explanation},
  author={Cunningham, P{\'a}draig and Doyle, D{\'o}nal and Loughrey, John},
  booktitle={International Conference on Case-Based Reasoning},
  pages={122--130},
  year={2003},
  organization={Springer}
}

@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew Slavin and Doshi-Velez, Finale},
  booktitle={Thirty-second AAAI conference on artificial intelligence},
  year={2018}
}

@article{bauerle2018training,
  title={Training De-Confusion: An Interactive, Network-Supported Visual Analysis System for Resolving Errors in Image Classification Training Data},
  author={B{\"a}uerle, Alex and Neumann, Heiko and Ropinski, Timo},
  journal={arXiv preprint arXiv:1808.03114},
  year={2018}
}

@inproceedings{hendricks2016generating,
  title={Generating visual explanations},
  author={Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
  booktitle={European Conference on Computer Vision},
  pages={3--19},
  year={2016},
  organization={Springer}
}



@inproceedings{mothilal2020explaining,
  author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372850},
doi = {10.1145/3351095.3372850},
abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {607–617},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{keaneGoodCFandWhereToFindThem,
  author={Keane, Mark T. and Smyth, Barry},
editor={Watson, Ian
and Weber, Rosina},
title={{Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)}},
booktitle={Case-Based Reasoning Research and Development},
year={2020},
publisher={Springer International Publishing},
address={Cham},
pages={163--178},
abstract={Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (i) technically, these counterfactual cases can be generated by permuting problem-features until a class-change is found, (ii) psychologically, they are much more causally informative than factual explanations, (iii) legally, they are GDPR-compliant. However, there are issues around the finding of ``good'' counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few ``good'' counterfactuals for explanation purposes. So, we propose a new case-based approach for generating counterfactuals, using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.},
isbn={978-3-030-58342-2},
doi = {https://doi.org/10.1007/978-3-030-58342-2_11}
}

@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017},
  doi = {10.48550/arxiv.1702.08608},
  url = {https://arxiv.org/abs/1702.08608},
}

@inproceedings{doyle2004explanation,
  title={Explanation oriented retrieval},
  author={Doyle, D{\'o}nal and Cunningham, P{\'a}draig and Bridge, Derek and Rahman, Yusof},
  booktitle={European Conference on Case-Based Reasoning},
  pages={157--168},
  year={2004},
  organization={Springer}
}


@article{im2007case,
  title={Case-based reasoning and neural network based expert system for personalization},
  author={Im, Kwang Hyuk and Park, Sang Chan},
  journal={Expert Systems with Applications},
  volume={32},
  number={1},
  pages={77--85},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{keane2019case,
  title={How case-based reasoning explains neural networks: A theoretical analysis of XAI using post-hoc explanation-by-example from a survey of ANN-CBR twin-systems},
  author={Keane, Mark T. and Kenny, Eoin M.},
  booktitle={Proceedings of the 27th International Conference on Case-Based Reasoning (ICCBR-19)},
  pages={155--171},
  year={2019},
  organization={Springer}
}



@inproceedings{kenny2019predicting,
  title={Predicting Grass Growth for Sustainable Dairy Farming: A CBR System Using Bayesian Case-Exclusion and Post-Hoc, Personalized Explanation-by-Example (XAI)},
  author={Kenny, Eoin M and Ruelle, Elodie and Geoghegan, Anne and Shalloo, Laurence and O’Leary, Miche{\'a}l and O’Donovan, Michael and Keane, Mark T},
  booktitle={Proceedings of the 27th International Conference on Case-Based Reasoning (ICCBR-19)},
  pages={172--187},
  year={2019},
  publisher={Springer}
}



@article{lipton2016mythos,
  title={The mythos of model interpretability},
  author={Lipton, Zachary C},
  journal={arXiv preprint arXiv:1606.03490},
  year={2016}
}

@article{nunes2017systematic,
  title={A systematic review and taxonomy of explanations in decision support and recommender systems},
  author={Nunes, Ingrid and Jannach, Dietmar},
  journal={User Modeling and User-Adapted Interaction},
  volume={27},
  number={3-5},
  pages={393--444},
  year={2017},
  publisher={Springer}
}

@article{park2004mbnr,
  title={MBNR: case-based reasoning with local feature weighting by neural network},
  author={Park, Jae Heon and Im, Kwang Hyuk and Shin, Chung-Kwan and Park, Sang Chan},
  journal={Applied Intelligence},
  volume={21},
  number={3},
  pages={265--276},
  year={2004},
  publisher={Springer}
}

@inproceedings{pedreschi2019meaningful,
  title={Meaningful explanations of Black Box AI decision systems},
  author={Pedreschi, Dino and Giannotti, Fosca and Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={9780--9784},
  year={2019}
}

@inproceedings{tintarev2007survey,
  title={A survey of explanations in recommender systems},
  author={Tintarev, Nava and Masthoff, Judith},
  booktitle={2007 IEEE 23rd international conference on data engineering workshop},
  pages={801--810},
  year={2007},
  organization={IEEE}
}

@article{shin1999memory,
  title={Memory and neural network based expert system},
  author={Shin, Chung Kwan and Park, Sang Chan},
  journal={Expert Systems with Applications},
  volume={16},
  number={2},
  pages={145--155},
  year={1999},
  publisher={Elsevier}
}

@article{shin2000hybrid,
  title={A hybrid approach of neural network and memory-based learning to data mining},
  author={Shin, Chung-Kwan and Yun, Ui Tak and Kim, Huy Kang and Park, Sang Chan},
  journal={IEEE Transactions on Neural Networks},
  volume={11},
  number={3},
  pages={637--646},
  year={2000},
  publisher={IEEE}
}

@article{sormo2005explanation,
  title={Explanation in case-based reasoning--perspectives and goals},
  author={S{\o}rmo, Frode and Cassens, J{\"o}rg and Aamodt, Agnar},
  journal={Artificial Intelligence Review},
  volume={24},
  number={2},
  pages={109--143},
  year={2005},
  publisher={Springer}
}

@article{de2005retrieval,
  title={Retrieval, reuse, revision and retention in case-based reasoning},
  author={De Mantaras, Ramon Lopez and McSherry, David and Bridge, Derek and Leake, David and Smyth, Barry and Craw, Susan and Faltings, Boi and Maher, Mary Lou and T COX, MICHAEL and Forbus, Kenneth and others},
  journal={The Knowledge Engineering Review},
  volume={20},
  number={3},
  pages={215--240},
  year={2005},
  publisher={Cambridge University Press}
}

@article{hoffman2018metrics,
  title={Metrics for explainable AI: Challenges and prospects},
  author={Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  journal={arXiv preprint arXiv:1812.04608},
  year={2018},
  url = {http://arxiv.org/abs/1812.04608}
}





%%%%%%%%%%%%%%%% Survey Paper %%%%%%%%%%%%%%%%%%%%%%%%

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  year = {2019},
    journal = {Artificial Intelligence},
    author = {Miller, Tim},
    pages = {1--38},
    volume = {267},
    publisher = {Elsevier B.V.},
    url = {https://doi.org/10.1016/j.artint.2018.07.007},
    doi = {10.1016/j.artint.2018.07.007},
    issn = {00043702},
    keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency}
}

@inproceedings{abdul2018trends,
  title={Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda},
  author={Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y and Kankanhalli, Mohan},
  booktitle={Proceedings of the 2018 CHI conference on human factors in computing systems},
  pages={582},
  year={2018},
  organization={ACM}
}

@article{johs2018measuring,
  title={Measuring Explanation Quality in XCBR},
  author={Johs, Adam J and Lutts, Meaghan and Weber, Rosina O},
  journal={ICCBR 2018},
  pages={75},
  year={2018}
}

@article{harman1965inference,
  title={The inference to the best explanation},
  author={Harman, Gilbert H},
  journal={The philosophical review},
  volume={74},
  number={1},
  pages={88--95},
  year={1965},
  publisher={JSTOR}
}

@article{forge1985almon,
  title={ALMON, WC:" Scientific Explanation and the Causal Structure of the World"},
  author={Forge, J},
  year={1985}
}

@book{salmon1984scientific,
  title={Scientific explanation and the causal structure of the world},
  author={Salmon, Wesley C},
  year={1984},
  publisher={Princeton University Press}
}

@book{van1980scientific,
  title={The scientific image},
  author={Van Fraassen, Bas C and others},
  year={1980},
  publisher={Oxford University Press}
}

@article{keil2006explanation,
     title = {{Explanation and understanding}},
    year = {2006},
    journal = {Annual Review of Psychology},
    author = {Keil, Frank C.},
    pages = {227--254},
    volume = {57},
    doi = {10.1146/annurev.psych.57.102904.190100},
    issn = {00664308},
    pmid = {16318595},
    keywords = {Causality, Cognition, Cognitive development, Concepts, Domain specificity, Illusions of knowing, Stances}
}

@article{kim2003symptoms,
  title={From symptoms to causes: Diversity effects in diagnostic reasoning},
  author={Kim, Nancy S and Keil, Frank C},
  journal={Memory \& cognition},
  volume={31},
  number={1},
  pages={155--165},
  year={2003},
  publisher={Springer}
}

@inproceedings{balduzzi2017shattered,
  title={The shattered gradients problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={342--350},
  year={2017},
  organization={JMLR. org}
}



@article{leake2005introduction,
  title={Introduction to the special issue on explanation in case-based reasoning},
  author={Leake, David and McSherry, David},
  journal={The Artificial Intelligence Review},
  volume={24},
  number={2},
  pages={103},
  year={2005},
  publisher={Springer Science \& Business Media}
}


@inproceedings{keane2021if,
  title     = {If Only We Had Better Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation of Counterfactual XAI Techniques},
  author    = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {4466--4474},
  year      = {2021},
  month     = {8},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2021/609},
  url       = {https://doi.org/10.24963/ijcai.2021/609},
}

@inproceedings{karimi2020algorithmic,
 author = {Karimi, Amir-Hossein and von K\"{u}gelgen, Julius and Sch\"{o}lkopf, Bernhard and Valera, Isabel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {265--277},
 publisher = {Curran Associates, Inc.},
 title = {Algorithmic recourse under imperfect causal knowledge: a probabilistic approach},
 url = {https://proceedings.neurips.cc/paper/2020/file/02a3c7fb3f489288ae6942498498db20-Paper.pdf},
 volume = {33},
 year = {2020}
}

%%%% Human decision making
@article{cohen1996metarecognition,
  title={Metarecognition in time-stressed decision making: Recognizing, critiquing, and correcting},
  author={Cohen, Marvin S and Freeman, Jared T and Wolf, Steve},
  journal={Human Factors},
  volume={38},
  number={2},
  pages={206--219},
  year={1996},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{lee2020improving,
  title={Improving Trust in Deep Neural Networks with Nearest Neighbors},
  author={Lee, Ritchie and Clarke, Justin and Agogino, Adrian and Giannakopoulou, Dimitra},
  booktitle={AIAA Scitech 2020 Forum},
  pages={2098},
  year={2020}
}

@article{klein1989decision,
  title={Do decision biases explain too much},
  author={Klein, Gary A},
  journal={Human Factors Society Bulletin},
  volume={32},
  number={5},
  pages={1--3},
  year={1989}
}


@article{sormo_explanation_2005,
  title={Explanation in case-based reasoning--perspectives and goals},
  author={S{\o}rmo, Frode and Cassens, J{\"o}rg and Aamodt, Agnar},
  journal={Artificial Intelligence Review},
  volume={24},
  number={2},
  pages={109--143},
  year={2005},
  publisher={Springer}
}

@inproceedings{zeiler_visualizing_2013,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}


@inproceedings{ribeiro_why_2016,
 title={Why should i trust you?: Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={ACM SIGKDD},
  pages={1135--1144},
  year={2016},
  organization={ACM}
}
%  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},



@article{nugent_case-based_2005,
  title={A case-based explanation system for black-box systems},
  author={Nugent, Conor and Cunningham, P{\'a}draig},
  journal={Artificial Intelligence Review},
  volume={24},
  number={2},
  pages={163--178},
  year={2005},
  publisher={Springer},
   doi = {10.1007/s10462-005-4609-5},
    issn = {02692821}
}

@article{lipton_2018,
	title = {The mythos of model interpretability},
	volume = {16(3)},
	journal = {Queue},
	author = {Lipton, Zach C.},
	year = {2018},
	pages = {30}
}

%%%%% big cbr
@article{de2005retrieval,
  title={Retrieval, reuse, revision and retention in case-based reasoning},
  author={De Mantaras, Ramon Lopez and McSherry, David and Bridge, Derek and Leake, David and Smyth, Barry and Craw, Susan and Faltings, Boi and Maher, Mary Lou and T Cox, Michael and Forbus, Kenneth and others},
  journal={The Knowledge Engineering Review},
  volume={20},
  number={3},
  pages={215--240},
  year={2005},
  publisher={Cambridge University Press}
}

@inproceedings{caruana1999case,
  title={Case-based explanation of non-case-based learning methods.},
  author={Caruana, Rich and Kangarloo, Hooshang and Dionisio, JD and Sinha, Usha and Johnson, David},
  booktitle={Proc. AMIA Symposium},
  pages={212},
  year={1999},
  organization={American Medical Informatics Association}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4765--4774},
  year={2017}
}


%%%%%%%%% Surveys
@inproceedings{kk2019survey,
  title={How case-based reasoning explains neural networks: A theoretical analysis of XAI using post-hoc explanation-by-example from a survey of ANN-CBR twin-systems},
  author={Keane, Mark T. and Kenny, Eoin M.},
  booktitle={ICCBR},
  pages={155--171},
  year={2019},
  organization={Springer}
}

@article{guidotti2018survey,
  title={A survey of methods for explaining black box models},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  journal={ACM computing surveys (CSUR)},
  volume={51},
  number={5},
  pages={93},
  year={2018},
  publisher={ACM},
  issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
month = {Aug},
articleno = {93},
numpages = {42},
keywords = {interpretability, transparent models, explanations, Open the black box}
}


@article{lipton2016mythos,
  title={The mythos of model interpretability},
  author={Lipton, Zachary C},
  journal={arXiv preprint arXiv:1606.03490},
  year={2016}
}

@article{miller2018explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial Intelligence},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{abdul2018trends,
  title={Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda},
  author={Abdul, Ashraf and Vermeulen, Jo and Wang, Danding and Lim, Brian Y and Kankanhalli, Mohan},
  booktitle={Proc. 2018 CHI Conference on Human Factors in Computing Systems},
  pages={582},
  year={2018},
  organization={ACM}
}

@inproceedings{biran2017explanation,
  title={Explanation and justification in machine learning: A survey},
  author={Biran, Or and Cotton, Courtenay},
  booktitle={IJCAI-17 workshop on explainable AI (XAI)},
  volume={8},
  year={2017}
}

%%%%%%%%%%% CBR
@book{leake1996case,
  title={Case-Based Reasoning: Experiences, lessons and future directions},
  author={Leake, David B},
  year={1996},
  publisher={MIT press}
}


@book{leake2014evaluating,
  title={Evaluating explanations: A content theory},
  author={Leake, David B},
  year={2014},
  publisher={Psychology Press}
}

@article{aha1991instance,
  title={Instance-based learning algorithms},
  author={Aha, David W and Kibler, Dennis and Albert, Marc K},
  journal={Machine learning},
  volume={6},
  number={1},
  pages={37--66},
  year={1991},
  publisher={Springer}
}


@article{miller2019explanationsocial,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial Intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier},
  url = {https://doi.org/10.1016/j.artint.2018.07.007},
    doi = {10.1016/j.artint.2018.07.007},
    issn = {00043702}
}

@inproceedings{Forestier2017,
  title={Generating synthetic time series to augment sparse datasets},
  author={Forestier, Germain and Petitjean, Fran{\c{c}}ois and Dau, Hoang Anh and Webb, Geoffrey I and Keogh, Eamonn},
  booktitle={ICDM},
  pages={865--870},
  year={2017}, 
  organization={IEEE}}

@article{Lipton2018,
archivePrefix = {arXiv},
arxivId = {1606.03490},
author = {Lipton, Zachary C.},
doi = {10.1145/3233231},
eprint = {1606.03490},
file = {:C$\backslash$:/Users/eoind/Desktop/Counterfactual Explainations for Time Series/1606.03490.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {10},
pages = {35--43},
title = {{The mythos of model interpretability}},
volume = {61},
year = {2018}
}


@inproceedings{Mittelstadt2019,
  title={Explaining explanations in AI},
  author={Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  booktitle={Conference on Fairness, Accountability, and Transparency},
  pages={279--288},
  year={2019}
}

@inproceedings{Sokol2020,
  title={Explainability fact sheets: a framework for systematic assessment of explainable approaches},
  author={Sokol, Kacper and Flach, Peter},
  booktitle={Conference on Fairness, Accountability, and Transparency},
  pages={56--67},
  year={2020}
}

@inproceedings{liu2008isolation,
  title={Isolation forest},
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={ICDM},
  pages={413--422},
  year={2008},
}

@article{Schlegel2019,
  title={Towards a rigorous evaluation of XAI Methods on Time Series},
  author={Schlegel, Udo and Arnout, Hiba and El-Assady, Mennatallah and Oelke, Daniela and Keim, Daniel A},
  journal={arXiv preprint arXiv:1909.07082},
  year={2019}
}


@article{Rudin2019,
abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1811.10154v3},
author = {Rudin, Cynthia},
doi = {10.1038/s42256-019-0048-x},
eprint = {arXiv:1811.10154v3},
file = {:C$\backslash$:/Users/eoind/Desktop/Counterfactual Explainations for Time Series/1811.10154.pdf:pdf},
issn = {25225839},
journal = {Nature Machine Intelligence},
number = {5},
pages = {206--215},
title = {{Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead}},
volume = {1},
year = {2019}
}

@inproceedings{Byrne2019,
abstract = {Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.},
author = {Byrne, Ruth M.J.},
booktitle = {IJCAI-19},
pages = {6276--6282},
title = {{Counterfactuals in explainable artificial intelligence (XAI): Evidence from human reasoning}},
year = {2019}
}
@inproceedings{Poyiadzi2020,
  author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
title = {FACE: Feasible and Actionable Counterfactual Explanations},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375850},
doi = {10.1145/3375627.3375850},
abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {344–350},
numpages = {7},
keywords = {counterfactuals, black-box models, interpretability, explainability},
location = {New York, NY, USA},
series = {AIES '20}
}

@article{LeNguyen2019a,
  title={Interpretable time series classification using linear models and multi-resolution multi-domain symbolic representations},
  author={Le Nguyen, Thach and Gsponer, Severin and Ilie, Iulia and O’Reilly, Martin and Ifrim, Georgiana},
  journal={Data mining and knowledge discovery},
  volume={33},
  number={4},
  pages={1183--1222},
  year={2019},
  publisher={Springer}
}

@article{Liao_Zhang_Luss_Doshi-Velez_Dhurandhar_2022, title={Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI}, volume={10}, url={https://ojs.aaai.org/index.php/HCOMP/article/view/21995}, DOI={10.1609/hcomp.v10i1.21995}, abstractNote={Recent years have seen a surge of interest in the field of explainable AI (XAI), with a plethora of algorithms proposed in the literature. However, a lack of consensus on how to evaluate XAI hinders the advancement of the field. We highlight that XAI is not a monolithic set of technologies---researchers and practitioners have begun to leverage XAI algorithms to build XAI systems that serve different usage contexts, such as model debugging and decision-support. Algorithmic research of XAI, however, often does not account for these diverse downstream usage contexts, resulting in limited effectiveness or even unintended consequences for actual users, as well as difficulties for practitioners to make technical choices. We argue that one way to close the gap is to develop evaluation methods that account for different user requirements in these usage contexts. Towards this goal, we introduce a perspective of contextualized XAI evaluation by considering the relative importance of XAI evaluation criteria for prototypical usage contexts of XAI. To explore the context dependency of XAI evaluation criteria, we conduct two survey studies, one with XAI topical experts and another with crowd workers. Our results urge for responsible AI research with usage-informed evaluation practices, and provide a nuanced understanding of user requirements for XAI in different usage contexts.}, number={1}, journal={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing}, author={Liao, Q. Vera and Zhang, Yunfeng and Luss, Ronny and Doshi-Velez, Finale and Dhurandhar, Amit}, year={2022}, month={Oct.}, pages={147-159} }

@misc{MeiLiptonNorthStar2023,
  doi = {10.48550/ARXIV.2303.05500},
  
  url = {https://arxiv.org/abs/2303.05500},
  
  author = {Mei, Alex and Saxon, Michael and Chang, Shiyu and Lipton, Zachary C. and Wang, William Yang},
  
  keywords = {Computers and Society (cs.CY), Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Users are the North Star for AI Transparency},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{read1993breadth,
Abstract = {Studied the impact of explanatory coherence on the evaluation of explanations. Tested were 4 principles of P. Thagard's (see record [rid]1990-06642-001[/rid]) model for evaluating the coherence of explanations. Study 1 showed that Ss preferred explanations that accounted for more data (breadth) and that were simpler (simplicity or parsimony). Study 2 demonstrated that Ss thought an explanation was stronger when it could, in turn, be explained. Study 3 showed that the evaluation of explanations is comparative, affected by the availability of good alternatives. Results were then successfuly simulated using Thagard's connectionist implementation of his model of explanatory coherence. The data and the simulation, taken together, strongly support the model. Two issues are then discussed: (1) the role of explanatory coherence in social explanation and (2) the relevance of parallel constraint satisfaction processes to social reasoning. (PsycInfo Database Record (c) 2020 APA, all rights reser},
Author = {Read, Stephen J. and Marcus-Newhall, Amy},
ISSN = {0022-3514},
Journal = {Journal of Personality and Social Psychology},
Keywords = {explanatory coherence, evaluation of social explanation, college students, Social Cognition},
Number = {3},
Pages = {429 - 447},
Title = {Explanatory coherence in social explanations: A parallel distributed processing account.},
Volume = {65},
URL = {https://ucd.idm.oclc.org/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1994-01615-001&site=ehost-live&scope=site},
Year = {1993},
doi = {10.1037/0022-3514.65.3.429}
}

@article{Johnson2019Opponent,
title = {Simplicity and complexity preferences in causal explanation: An opponent heuristic account},
journal = {Cognitive Psychology},
volume = {113},
pages = {101222},
year = {2019},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010028517300579},
author = {Samuel G.B. Johnson and J.J. Valenti and Frank C. Keil},
keywords = {Causal reasoning, Explanation, Inference, Understanding, Simplicity},
abstract = {People often prefer simple to complex explanations because they generally have higher prior probability. However, simpler explanations are not always normatively superior because they often do not account for the data as well as complex explanations. How do people negotiate this trade-off between prior probability (favoring simple explanations) and goodness-of-fit (favoring complex explanations)? Here, we argue that people use opponent heuristics to simplify this problem—that people use simplicity as a cue to prior probability but complexity as a cue to goodness-of-fit. Study 1 finds direct evidence for this claim. In subsequent studies, we examine factors that lead one or the other heuristic to predominate in a given context. Studies 2 and 3 find that people have a stronger simplicity preference in deterministic rather than stochastic contexts, while Studies 4 and 5 find that people have a stronger simplicity preference for physical rather than social causal systems, suggesting that people use abstract expectations about causal texture to modulate their explanatory inferences. Together, we argue that these cues and contextual moderators act as powerful constraints that can help to specify the otherwise ill-defined problem of what distributions to use in Bayesian hypothesis comparison.}
}

@article{Cunningham2020,
abstract = {Perhaps the most straightforward classifier in the arsenal or machine learning techniques is the Nearest Neighbour Classifier -- classification is achieved by identifying the nearest neighbours to a query example and using those neighbours to determine the class of the query. This approach to classification is of particular importance because issues of poor run-time performance is not such a problem these days with the computational power that is available. This paper presents an overview of techniques for Nearest Neighbour classification focusing on; mechanisms for assessing similarity (distance), computational issues in identifying nearest neighbours and mechanisms for reducing the dimension of the data. This paper is the second edition of a paper previously published as a technical report. Sections on similarity measures for time-series, retrieval speed-up and intrinsic dimensionality have been added. An Appendix is included providing access to Python code for the key methods.},
archivePrefix = {arXiv},
arxivId = {2004.04523},
author = {Cunningham, Padraig and Delany, Sarah Jane},
eprint = {2004.04523},
file = {:C$\backslash$:/Users/eoind/Desktop/Counterfactual Explainations for Time Series/kNN{\_}Cunningham.pdf:pdf},
number = {1},
pages = {1--22},
title = {{k-Nearest Neighbour Classifiers: 2nd Edition (with Python examples)}},
url = {http://arxiv.org/abs/2004.04523},
year = {2020}
}

@article{Guidotti2019,
  title={Factual and counterfactual explanations for black box decision making},
  author={Guidotti, Riccardo and Monreale, Anna and Giannotti, Fosca and Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
  journal={IEEE Intelligent Systems},
  volume={34},
  number={6},
  pages={14--23},
  year={2019},
  publisher={IEEE},
  doi={10.1109/MIS.2019.2957223}
}

@article{Bagnall2017,
  title={The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances},
  author={Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
  journal={Data Mining and Knowledge Discovery},
  volume={31},
  number={3},
  pages={606--660},
  year={2017},
  publisher={Springer}
}
@inproceedings{Mothilal2020,
  author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372850},
doi = {10.1145/3351095.3372850},
abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {607–617},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}
@article{Dau2019,
  title={The UCR time series archive},
  author={Dau, Hoang Anh and Bagnall, Anthony and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Annh and Keogh, Eamonn},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={6},
  number={6},
  pages={1293--1305},
  year={2019},
  publisher={IEEE}
}

@inproceedings{Mueen2016a,
  title={Extracting optimal performance from dynamic time warping},
  author={Mueen, Abdullah and Keogh, Eamonn},
  booktitle={ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={2129--2130},
  year={2016}
}

@article{VanLooveren2019,
  title={Interpretable counterfactual explanations guided by prototypes},
  author={Van Looveren, Arnaud and Klaise, Janis},
  journal={arXiv preprint arXiv:1907.02584},
  year={2019}
}

@article{Nugent2009,
  title={Gaining insight through case-based explanation},
  author={Nugent, Conor and Doyle, D{\'o}nal and Cunningham, P{\'a}draig},
  journal={Journal of Intelligent Information Systems},
  volume={32},
  number={3},
  pages={267--295},
  year={2009},
  publisher={Springer}
}

@article{IsmailFawaz2019,
  title={Deep learning for time series classification: a review},
  author={Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={4},
  pages={917--963},
  year={2019},
  publisher={Springer}
}

@article{Wang2019,
  title={Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization},
  author={Wang, Yichang and Emonet, R{\'e}mi and Fromont, Elisa and Malinowski, Simon and Menager, Etienne and Mosser, Lo{\"\i}c and Tavenard, Romain},
  journal={arXiv preprint arXiv:1906.00917},
  year={2019}
}

@article{Gee2019,
abstract = {The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or "prototypes" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.},
archivePrefix = {arXiv},
arxivId = {1904.08935},
author = {Gee, Alan H. and Garcia-Olano, Diego and Ghosh, Joydeep and Paydarfar, David},
eprint = {1904.08935},
file = {:C$\backslash$:/Users/eoind/Desktop/Counterfactual Explainations for Time Series/Prototypes.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
mendeley-groups = {Papers For Counterfactual Paper},
pages = {15--22},
title = {{Explaining deep classification of time-series data with learned prototypes}},
volume = {2429},
year = {2019}
}

@book{PearlMackenzie18,
  abstract = {Correlation is not causation-this mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality-the study of cause and effect-on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence.},
  added-at = {2018-09-17T16:24:20.000+0200},
  address = {New York},
  author = {Pearl, Judea and Mackenzie, Dana},
  biburl = {https://www.bibsonomy.org/bibtex/212360928cada467ce0f3bfa5e511752d/flint63},
  file = {2018/PearlMackenzie18.pdf},
  interhash = {dc0eae388b44d13b931962ebc07f5914},
  intrahash = {12360928cada467ce0f3bfa5e511752d},
  isbn = {978-0-465-09760-9},
  isbn10 = {046509760X},
  keywords = {01941 103 ai algorithm book knowledge numerical processing science theory},
  language = {american},
  publisher = {Basic Books},
  related = {Pearl09},
  shop = {https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097616/},
  sortdate = {2018-06-01},
  subtitle = {The New Science of Cause and Effect},
  timestamp = {2018-09-17T16:24:20.000+0200},
  title = {The Book of Why},
  year = 2018
}

@misc{Dua2019UCI,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@inproceedings{Patel2022Privacy,
author = {Patel, Neel and Shokri, Reza and Zick, Yair},
title = {Model Explanations with Differential Privacy},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533235},
doi = {10.1145/3531146.3533235},
abstract = {Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.},
booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1895–1904},
numpages = {10},
keywords = {Differential Privacy, Model Explainations},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{Dodge2019,
  title={Explaining models: an empirical study of how explanations impact fairness judgment},
  author={Dodge, Jonathan and Liao, Q Vera and Zhang, Yunfeng and Bellamy, Rachel KE and Dugan, Casey},
  booktitle={International Conference on Intelligent User Interfaces},
  pages={275--285},
  year={2019}
}

@inproceedings{keane2020good,
    author={Keane, Mark T. and Smyth, Barry},
editor={Watson, Ian
and Weber, Rosina},
title={{Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)}},
booktitle={Case-Based Reasoning Research and Development},
year={2020},
publisher={Springer International Publishing},
address={Cham},
pages={163--178},
abstract={Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (i) technically, these counterfactual cases can be generated by permuting problem-features until a class-change is found, (ii) psychologically, they are much more causally informative than factual explanations, (iii) legally, they are GDPR-compliant. However, there are issues around the finding of ``good'' counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few ``good'' counterfactuals for explanation purposes. So, we propose a new case-based approach for generating counterfactuals, using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.},
isbn={978-3-030-58342-2},
doi = {https://doi.org/10.1007/978-3-030-58342-2_11}
}

@article{JMLR:v21:20-091,
  author  = {Romain Tavenard and Johann Faouzi and Gilles Vandewiele and
             Felix Divo and Guillaume Androz and Chester Holtz and
             Marie Payne and Roman Yurchak and Marc Ru{\ss}wurm and
             Kushal Kolar and Eli Woods},
  title   = {Tslearn, A Machine Learning Toolkit for Time Series Data},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {118},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v21/20-091.html}
}
@article{Mittelstadt2018,
  author    = {Brent D. Mittelstadt and
               Chris Russell and
               Sandra Wachter},
  title     = {Explaining Explanations in {AI}},
  journal   = {CoRR},
  volume    = {abs/1811.01439},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.01439},
  archivePrefix = {arXiv},
  eprint    = {1811.01439},
  timestamp = {Wed, 23 Jan 2019 13:31:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-01439.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Karlsson2018,
  title={Explainable time series tweaking via irreversible and reversible temporal transformations},
  author={Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
  booktitle={ICDM},
  year={2018}}


@book{molnar2020,
  title={Interpretable machine learning},
  author={Molnar, Christoph},
  year={2020},
  publisher={Lulu.com}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{fauvel2020performance,
  title={A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application to Multivariate Time Series Classifiers},
  author={Fauvel, Kevin and Masson, V{\'e}ronique and Fromont, {\'E}lisa},
  journal={arXiv preprint arXiv:2005.14501},
  year={2020}
}
@inproceedings{ma2003time,
  title={Time-series novelty detection using one-class support vector machines},
  author={Ma, Junshui and Perkins, Simon},
  booktitle={Proceedings of the International Joint Conference on Neural Networks, 2003.},
  volume={3},
  pages={1741--1745},
  year={2003},
  organization={IEEE}
}
@inproceedings{breunig2000lof,
  title={LOF: identifying density-based local outliers},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={ACM SIGMOD},
  pages={93--104},
  year={2000}
}

@inproceedings{scholkopf2000support,
  title={Support vector method for novelty detection},
  author={Sch{\"o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex J and Shawe-Taylor, John and Platt, John C},
  booktitle={Advances in neural information processing systems},
  pages={582--588},
  year={2000}
}

@article{wachter2017counterfactual,
  title = {{Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR}},
    year = {2018},
    journal = {Harvard Journal of Law {\&} Technology},
    author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
    number = {April 2018},
    volume = {31},
    doi = {10.2139/ssrn.3063289}
}

    
@article{gunning2017explainable,
  title={Explainable artificial intelligence (xai)},
  author={Gunning, David},
  journal={Defense Advanced Research Projects Agency (DARPA), nd Web},
  volume={2},
  pages={2},
  year={2017}
}

@inproceedings{nguyenmodel,
title={A Model-Agnostic Approach to Quantifying the Informativeness of Explanation Methods for Time Series Classification},
  author={Nguyen, Thu Trang and Le Nguyen, Thach and Ifrim, Georgiana},
  booktitle={Proceedings of the 5th Workshop on Advanced Analytics and Learning on Temporal Data at ECML 2020.},
  year={2020},
  organization={Springer}}
  
@inproceedings{sokol2019desiderata,
title={Desiderata for Interpretability: Explaining Decision Tree Predictions with Counterfactuals},
author={Sokol, Kacper and Flach, Peter},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
volume={33},
pages={10035--10036},
year={2019}
}
@article{aha1991instance,
  title={Instance-based learning algorithms},
  author={Aha, David W and Kibler, Dennis and Albert, Marc K},
  journal={Machine learning},
  volume={6},
  number={1},
  pages={37--66},
  year={1991},
  publisher={Springer}
}

@article{goodman2017european,
  title={European Union regulations on algorithmic decision-making and a “right to explanation”},
  author={Goodman, Bryce and Flaxman, Seth},
  journal={AI magazine},
  volume={38},
  number={3},
  pages={50--57},
  year={2017}
}

@inproceedings{fawaz2019adversarial,
  title={Adversarial attacks on deep neural networks for time series classification},
  author={Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}
@inproceedings{wang2017time,
  title={Time series classification from scratch with deep neural networks: A strong baseline},
  author={Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
  booktitle={IJCNN},
  pages={1578--1585},
  year={2017},
  organization={IEEE}
}
@article{karim2017lstmfcn,
  title={LSTM fully convolutional networks for time series classification},
  author={Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang and Chen, Shun},
  journal={IEEE access},
  volume={6},
  pages={1662--1669},
  year={2017},
  publisher={IEEE}
}
@inproceedings{schoenborn2019recent,
  title={Recent Trends in XAI: A Broad Overview on current Approaches, Methodologies and Interactions.},
  author={Schoenborn, Jakob Michael and Althoff, Klaus-Dieter},
  booktitle={ICCBR Workshops},
  pages={51--60},
  year={2019}
}
@article{kenny2020generating,
  title={On Generating Plausible Counterfactual and Semi-Factual Explanations for Deep Learning}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17377}, DOI={10.1609/aaai.v35i13.17377}, abstract={There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet, counterfactual explanations have become very popular in eXplainable AI (XAI) due to their asserted computational, psychological, and legal benefits. In contrast however, semi-factuals (which appear to be equally useful) have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly because the non-discrete nature of images makes good counterfactuals difficult to define; indeed, generating plausible counterfactual images which lie on the data manifold is also problematic. This paper advances a novel method for generating plausible counterfactuals and semi-factuals for black-box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all “exceptional” features in a test image to be “normal” from the perspective of the counterfactual class, to generate plausible counterfactual images. Two controlled experiments compare this method to others in the literature, showing that PIECE generates highly plausible counterfactuals (and the best semi-factuals) on several benchmark measures.}, number={13}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Kenny, Eoin M. and Keane, Mark T}, year={2021}, month={May}, pages={11575-11585} 
}
@article{verma2022counterfactual,
  title={Counterfactual Explanations for Machine Learning: A Review},
  author={Verma, Sahil and Dickerson, John and Hines, Keegan},
  journal={arXiv preprint arXiv:2010.10596},
  year={2022},
  url = {http://arxiv.org/abs/2010.10596},
}

@inproceedings{dandl2020multi,
  title={Multi-objective counterfactual explanations},
  author={Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
  booktitle={Parallel Problem Solving from Nature--PPSN XVI},
  pages={448--469},
  year={2020},
  publisher={Springer},
  address = {Cham},
  isbn={978-3-030-58112-1},
  doi = {https://doi.org/10.1007/978-3-030-58112-1_31}
}

@inproceedings{karimi2020modelcf,
  title={Model-agnostic counterfactual explanations for consequential decisions},
  author={Karimi, Amir-Hossein and Barthe, Gilles and Balle, Borja and Valera, Isabel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={895--905},
  year={2020}
}

@article{verma2020counterfactual,
  title={Counterfactual Explanations for Machine Learning: A Review},
  author={Verma, Sahil and Dickerson, John and Hines, Keegan},
  journal={arXiv preprint arXiv:2010.10596},
  year={2020}
}

@article{karimi2022survey,
  title={A survey of algorithmic recourse: contrastive explanations and consequential recommendations},
  author={Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  journal={ACM Computing Surveys},
  volume={55},
  number={5},
  pages={1--29},
  year={2022},
  publisher={ACM New York, NY, USA},
  issn = {0360-0300},
  url = {https://doi.org/10.1145/3527848},
    doi = {10.1145/3527848},
    issue_date = {May 2023}
}


@article{karimi2020algorithmiccf,
  title={Algorithmic Recourse: from Counterfactual Explanations to Interventions},
  author={Karimi, Amir-Hossein and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  journal={arXiv preprint arXiv:2002.06278},
  year={2020}
}

@inproceedings{karimi2021algorithmiccf,
  title={Algorithmic recourse: from counterfactual explanations to interventions},
  author={Karimi, Amir-Hossein and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {353–362},
numpages = {10},
  year={2021},
  location = {Virtual Event, Canada},
  url = {https://doi.org/10.1145/3442188.3445899},
doi = {10.1145/3442188.3445899},
}

@inproceedings{laugel2019dangers,
  title={The dangers of post-hoc interpretability: Unjustified counterfactual explanations},
  author={Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
  booktitle={IJCAI-19},
  pages = {2801--2807},
  year={2019}
}
@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}
@inproceedings{lucic2020does,
  title={Why does my model fail? contrastive local explanations for retail forecasting},
  author={Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372824},
doi = {10.1145/3351095.3372824},
abstract = {In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be "black boxes," even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a "black-box" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1\% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {90–98},
numpages = {9},
keywords = {explainability, erroneous predictions, interpretability},
location = {Barcelona, Spain},
series = {FAT* '20},
year = {2020}
}

@article{leavitt2020towards,
  title={Towards falsifiable interpretability research},
  author={Leavitt, Matthew L and Morcos, Ari},
  journal={arXiv preprint arXiv:2010.12016},
  year={2020}
}

@article{hoffman2020primer,
  title={A primer for conducting experiments in human--robot interaction},
  author={Hoffman, Guy and Zhao, Xuan},
  journal={ACM Transactions on Human-Robot Interaction (THRI)},
  volume={10},
  number={1},
  pages={1--31},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{barocas2020hidden,
author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
title = {The Hidden Assumptions behind Counterfactual Explanations and Principal Reasons},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372830},
doi = {10.1145/3351095.3372830},
abstract = {Counterfactual explanations are gaining prominence within technical, legal, and business circles as a way to explain the decisions of a machine learning model. These explanations share a trait with the long-established "principal reason" explanations required by U.S. credit laws: they both explain a decision by highlighting a set of features deemed most relevant---and withholding others.These "feature-highlighting explanations" have several desirable properties: They place no constraints on model complexity, do not require model disclosure, detail what needed to be different to achieve a different decision, and seem to automate compliance with the law. But they are far more complex and subjective than they appear.In this paper, we demonstrate that the utility of feature-highlighting explanations relies on a number of easily overlooked assumptions: that the recommended change in feature values clearly maps to real-world actions, that features can be made commensurate by looking only at the distribution of the training data, that features are only relevant to the decision at hand, and that the underlying model is stable over time, monotonic, and limited to binary outcomes.We then explore several consequences of acknowledging and attempting to address these assumptions, including a paradox in the way that feature-highlighting explanations aim to respect autonomy, the unchecked power that feature-highlighting explanations grant decision makers, and a tension between making these explanations useful and the need to keep the model hidden.While new research suggests several ways that feature-highlighting explanations can work around some of the problems that we identify, the disconnect between features in the model and actions in the real world---and the subjective choices necessary to compensate for this---must be understood before these techniques can be usefully implemented.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {80–89},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{anjomshoae2019explainable,
  title={Explainable agents and robots: Results from a systematic literature review},
  author={Anjomshoae, Sule and Najjar, Amro and Calvaresi, Davide and Fr{\"a}mling, Kary},
  booktitle={18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13--17, 2019},
  pages={1078--1088},
  year={2019},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@article{delaney2020instance,
  title={Instance-Based Counterfactual Explanations for Time Series Classification},
  author={Delaney, Eoin and Greene, Derek and Keane, Mark T},
  journal={arXiv preprint arXiv:2009.13211},
  year={2020}
}

@inproceedings{delaney2021instance,
  title={Instance-based counterfactual explanations for time series classification},
  author={Delaney, Eoin and Greene, Derek and Keane, Mark T.},
  editor={S{\'a}nchez-Ruiz, Antonio A.
and Floyd, Michael W.},
  booktitle={Case-Based Reasoning Research and Development},
  pages={32--47},
  year={2021},
  publisher={Springer},
  address={Cham},
  isbn={978-3-030-86957-1},
  doi = {https://doi.org/10.1007/978-3-030-86957-1_3}
}

@inproceedings{mahato2018comparison,
  title={A Comparison of k-NN Methods for Time Series Classification and Regression.},
  author={Mahato, Vivek and O'Reilly, Martin and Cunningham, P{\'a}draig},
  booktitle={AICS},
  pages={102--113},
  year={2018}
}
@inproceedings{mahato2019scoring,
  title={Scoring performance on the y-balance test},
  author={Mahato, Vivek and Johnston, William and Cunningham, P{\'a}draig},
  booktitle={International Conference on Case-Based Reasoning},
  pages={281--296},
  year={2019},
  organization={Springer}
}
@article{schafer2016scalable,
  title={Scalable time series classification},
  author={Sch{\"a}fer, Patrick},
  journal={Data Mining and Knowledge Discovery},
  volume={30},
  number={5},
  pages={1273--1298},
  year={2016},
  publisher={Springer}
}
@inproceedings{lin2003symbolic,
  title={A symbolic representation of time series, with implications for streaming algorithms},
  author={Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill},
  booktitle={Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery},
  pages={2--11},
  year={2003}
}

@inproceedings{adebayo2018sanity,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9505--9515},
  year={2018}
}
@article{gilpin2018explaining,
  title={Explaining explanations: An approach to evaluating interpretability of machine learning},
  author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  journal={arXiv preprint arXiv:1806.00069},
  year={2018},
  publisher={CoRR}
}
@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}
@inproceedings{assaf2019explainable,
  title={Explainable Deep Neural Networks for Multivariate Time Series Predictions.},
  author={Assaf, Roy and Schumann, Anika},
  booktitle={IJCAI},
  pages={6488--6490},
  year={2019}
}
@inproceedings{grath2018interpretable,
  title={Interpretable credit application predictions with counterfactual explanations},
  author={Mc Grath, Rory and Costabello, Luca and Van, Chan Le and Sweeney, Paul and Kamiab, Farbod and Shen, Zhao and Lecue, Freddy},
  booktitle={NIPS 2018 Workshop on Challenges and Opportunities for AI in Financial Services: the Impact of Fairness, Explainability, Accuracy, and Privacy, Montréal, Canada.},
  year={2018},
  doi = {10.48550/arxiv.1811.05245},
  url = {https://hal.science/hal-01934915/},
}
@article{lipton1990contrastive,
  title={Contrastive explanation},
  author={Lipton, Peter},
  journal={Royal Institute of Philosophy Supplement},
  volume={27},
  pages={247--266},
  year={1990}
}

@article{byrne2016counterfactual,
  title={Counterfactual thought},
  author={Byrne, Ruth M.J.},
  journal={Annual review of psychology},
  volume={67},
  pages={135--157},
  year={2016},
  publisher={Annual Reviews}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@software{alibi,
  title = {Alibi: Algorithms for monitoring and explaining machine learning models},
  author = {Klaise, Janis and Van Looveren, Arnaud and Vacanti, Giovanni and Coca, Alexandru},
  url = {https://github.com/SeldonIO/alibi},
  version = {0.5.5},
  date = {2020-10-20},
  year = {2019}
}
@inproceedings{sktime,
    author = {L{\"{o}}ning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor and Lines, Jason and Kir{\'{a}}ly, Franz J},
    booktitle = {Workshop on Systems for ML at NeurIPS 2019},
    title = {{sktime: A Unified Interface for Machine Learning with Time Series}},
    date = {2019},
}
@article{aamodt1994case,
  title={Case-based reasoning: Foundational issues, methodological variations, and system approaches},
  author={Aamodt, Agnar and Plaza, Enric},
  journal={AI communications},
  volume={7},
  number={1},
  pages={39--59},
  year={1994},
  publisher={IOS press}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@incollection{robnik2018perturbation,
  title={Perturbation-based explanations of prediction models},
  author={Robnik-{\v{S}}ikonja, Marko and Bohanec, Marko},
  booktitle={Human and machine learning},
  pages={159--175},
  year={2018},
  publisher={Springer}
}

@inproceedings{nakhaeizadeh1993learning,
  title={Learning prediction of time series. A theoretical and empirical comparison of CBR with some other approaches},
  author={Nakhaeizadeh, Gholamreza},
  booktitle={European Workshop on Case-Based Reasoning},
  pages={65--76},
  year={1993},
  organization={Springer}
}
@article{fdez2004fsfrt,
  title={Fsfrt: Forecasting system for red tides},
  author={Fdez-Riverola, Florentino and Corchado, Juan M},
  journal={Applied Intelligence},
  volume={21},
  number={3},
  pages={251--264},
  year={2004},
  publisher={Springer}
}
@article{white2019measurable,
  title={Measurable counterfactual local explanations for any classifier},
  author={White, Adam and Garcez, Artur d'Avila},
  journal={arXiv preprint arXiv:1908.03020},
  year={2019}
}

@article{adadi2018peeking,
  title={Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}


@inproceedings{grabocka2014learning,
  title={Learning time-series shapelets},
  author={Grabocka, Josif and Schilling, Nicolas and Wistuba, Martin and Schmidt-Thieme, Lars},
  booktitle={ACM SIGKDD},
  pages = {392-401},
  year={2014}
}

@article{ye2011time,
  title={Time series shapelets: a novel technique that allows accurate, interpretable and fast classification},
  author={Ye, Lexiang and Keogh, Eamonn},
  journal={Data mining and knowledge discovery},
  volume={22},
  number={1-2},
  pages={149--182},
  year={2011},
  publisher={Springer}
}
@article{wang2019learning,
  title={Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization},
  author={Wang, Yichang and Emonet, R{\'e}mi and Fromont, Elisa and Malinowski, Simon and Menager, Etienne and Mosser, Lo{\"\i}c and Tavenard, Romain},
  journal={arXiv preprint arXiv:1906.00917},
  year={2019}
}
@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2921--2929},
  year={2016}
}

@inproceedings{kim2016criticism,
  title={Examples are not enough, learn to criticize! criticism for interpretability},
  author={Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2280--2288},
  year={2016}
}
@article{li2017prototypescbr,
  title={Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions},
  author={Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  journal={arXiv preprint arXiv:1710.04806},
  year={2017}
}
@article{kolodner1992introduction,
  title={An introduction to case-based reasoning},
  author={Kolodner, Janet L},
  journal={Artificial intelligence Review},
  volume={6},
  number={1},
  pages={3--34},
  year={1992},
  publisher={Springer}
}

@article{ford2020play,
  title={Play MNIST For Me! User Studies on the Effects of Post-Hoc, Example-Based Explanations \& Error Rates on Debugging a Deep Learning, Black-Box Classifier},
  author={Ford, Courtney and Kenny, Eoin M and Keane, Mark T},
  journal={arXiv preprint arXiv:2009.06349},
  year={2020}
}

@inproceedings{schulam2017reliable,
  title={Reliable decision support using counterfactual models},
  author={Schulam, Peter and Saria, Suchi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1697--1708},
  year={2017}
}

@inproceedings{kenny_ICPR,
  title={Post-Hoc Explanation Options for XAI in Deep Learning: The Insight Centre for Data Analytics Perspective},
  author={Kenny, Eoin M and Delaney, Eoin D and Greene, Derek and Keane, Mark T.},
  booktitle={International Conference on Pattern Recognition},
  year={2020},
  organization={Springer}
}

@inproceedings{russell2019efficient,
  title={Efficient search for diverse coherent explanations},
  author={Russell, Chris},
  booktitle={Conference on Fairness, Accountability, and Transparency},
  pages={20--28},
  year={2019}
}

@article{alvarez2018robustness,
  title={On the robustness of interpretability methods},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  journal={arXiv preprint arXiv:1806.08049},
  year={2018}
}

@inproceedings{slack2020fooling,
  title={Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods},
  author={Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  booktitle={AAAI/ACM Conference on AI, Ethics, and Society},
  pages={180--186},
  year={2020}
}

@article{ates2020counterfactual,
  title={Counterfactual Explanations for Machine Learning on Multivariate Time Series Data},
  author={Ates, Emre and Aksar, Burak and Leung, Vitus J and Coskun, Ayse K},
  journal={arXiv preprint arXiv:2008.10781},
  year={2020}
}

@inproceedings{labaien2020contrastive,
  title={Contrastive Explanations for a Deep Learning Model on Time-Series Data},
  author={Labaien, Jokin and Zugasti, Ekhi and De Carlos, Xabier},
  booktitle={International Conference on Big Data Analytics and Knowledge Discovery},
  pages={235--244},
  year={2020},
  organization={Springer}
}

@inproceedings{dhurandhar2018explanations,
  title={Explanations based on the missing: Towards contrastive explanations with pertinent negatives},
  author={Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  booktitle={Advances in Neural Information Processing Systems},
  publisher = {Curran Associates, Inc.},
  pages={592--603},
  year={2018},
  url = {https://proceedings.neurips.cc/paper/2018/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf}
}

@inproceedings{kenny2019twin,
  title={Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in ANN-CBR twins for XAI},
  author={Kenny, Eoin M and Keane, Mark T},
  booktitle={IJCAI-19},
  pages={2708--2715},
  year={2019}
}

@article{dempster2020rocket,
  title={ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels},
  author={Dempster, Angus and Petitjean, Fran{\c{c}}ois and Webb, Geoffrey I},
  journal={Data Mining and Knowledge Discovery},
  volume={34},
  number={5},
  pages={1454--1495},
  year={2020},
  publisher={Springer}
}

@inproceedings{kanamori2020dace,
  title={DACE: Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization},
  author={Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken and Arimura, Hiroki},
  booktitle={IJCAI-20},
  pages={2855--2862},
  year={2020}
}

@inproceedings{guidotti2020explainingtime,
  title={Explaining Any Time Series Classifier},
  author={Guidotti, Riccardo and Monreale, Anna and Spinnato, Francesco and Pedreschi, Dino and Giannotti, Fosca},
  booktitle={IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)},
  pages={167--176},
  year={2020},
  organization={IEEE}
}
@inproceedings{downscruds,
  title={CRUDS: Counterfactual Recourse Using Disentangled Subspaces},
  author={Downs, Michael and Chu, Jonathan L and Yacoby, Yaniv and Doshi-Velez, Finale and Pan, Weiwei}, 
  booktitle={ICML workshop proceedings},
  year={2020}
}
@article{alvarez2018robustness,
  title={On the robustness of interpretability methods},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  journal={arXiv preprint arXiv:1806.08049},
  year={2018}
}
@inproceedings{samangouei2018explaingan,
  title={Explaingan: Model explanation via decision boundary crossing transformations},
  author={Samangouei, Pouya and Saeedi, Ardavan and Nakagawa, Liam and Silberman, Nathan},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={666--681},
  year={2018}
}
@inproceedings{leonardi2020deep,
  title={Deep feature extraction for representing and classifying time series cases: towards an interpretable approach in haemodialysis},
  author={Leonardi, Giorgio and Montani, Stefania and Striani, Manuel},
  booktitle={Flairs-2020},
  year={2020},
  publisher = {AAAI Press}
}

@inproceedings{schoenborn2020explainable,
  title={Explainable Case-Based Reasoning: A Survey},
  author={Schoenborn, Jakob M and Weber, Rosina O and Aha, David W and Cassens, J{\"o}rg and Althoff, Klaus-Dieter},
  booktitle={AAAI-21 Workshop Proceedings},
  year={2021}
}
@article{briandet1996discrimination,
  title={Discrimination of Arabica and Robusta in instant coffee by Fourier transform infrared spectroscopy and chemometrics},
  author={Briandet, Romain and Kemsley, E Katherine and Wilson, Reginald H},
  journal={Journal of agricultural and food chemistry},
  volume={44},
  number={1},
  pages={170--174},
  year={1996},
  publisher={ACS Publications}
}
@techreport{olszewski2001generalized,
  title={Generalized feature extraction for structural pattern recognition in time-series data},
  author={Olszewski, Robert T},
  year={2001},
  institution={Carnegie-Mellon Univ, Pittsburgh}
}

@inproceedings{recio2020cbr,
  title={{CBR-LIME: A Case-Based Reasoning Approach to Provide Specific Local Interpretable Model-Agnostic Explanations}},
  author={Recio-Garc{\'\i}a, Juan A and D{\'\i}az-Agudo, Bel{\'e}n and Pino-Castilla, Victor},
  booktitle={ICCBR},
  pages={179--194},
  year={2020},
  organization={Springer}
}

@inproceedings{yeh2016matrix,
  title={Matrix profile I: all pairs similarity joins for time series: a unifying view that includes motifs, discords and shapelets},
  author={Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn},
  booktitle={ICDM},
  pages = {1317-1322},
  year={2016}
}

@article{gunning2019darpa,
  title={DARPA’s explainable artificial intelligence (XAI) program},
  author={Gunning, David and Aha, David},
  journal={AI Magazine},
  volume={40},
  number={2},
  pages={44--58},
  year={2019}
}

@article{leake2005introduction,
  title={Introduction to the special issue on explanation in case-based reasoning},
  author={Leake, David and Mcsherry, David},
  journal={The Artificial Intelligence Review},
  volume={24},
  number={2},
  pages={103},
  year={2005},
  publisher={Springer}
}

@article{scholkopf2001estimating,
  title={Estimating the support of a high-dimensional distribution},
  author={Sch{\"o}lkopf, Bernhard and Platt, John C and Shawe-Taylor, John and Smola, Alex J and Williamson, Robert C},
  journal={Neural computation},
  volume={13},
  number={7},
  pages={1443--1471},
  year={2001},
  publisher={MIT Press}
}

@inproceedings{sani2017learning,
  title={Learning deep features for kNN-based human activity recognition.},
  author={Sani, Sadiq and Wiratunga, Nirmalie and Massie, Stewart},
  booktitle={ICCBR-17 Workshop Proceedings},
  year={2017}
}

@article{kenny2021_KBS,
  title={Explaining Deep Learning using examples: Optimal feature weighting methods for twin systems using post-hoc, explanation-by-example in XAI},
  author={Kenny, Eoin M and Keane, Mark T},
  journal={Knowledge-Based Systems},
  volume={233},
  pages={107530},
  year={2021},
  publisher={Elsevier}
}

@article{ismail2020benchmarking,
  title={Benchmarking deep learning interpretability in time series predictions},
  author={Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, H{\'e}ctor Corrada and Feizi, Soheil},
  journal={arXiv preprint arXiv:2010.13924},
  year={2020}
}

@article{petitjean2011global,
  title={A global averaging method for dynamic time warping, with applications to clustering},
  author={Petitjean, Fran{\c{c}}ois and Ketterlin, Alain and Gan{\c{c}}arski, Pierre},
  journal={Pattern recognition},
  volume={44},
  number={3},
  pages={678--693},
  year={2011},
  publisher={Elsevier}
}

@article{lage2019evaluation,
  title={An evaluation of the human-interpretability of explanation},
  author={Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1902.00006},
  year={2019},
  arxivId = {1902.00006},
    url = {https://arxiv.org/abs/1902.00006}
}

@inproceedings{tintarev2007survey,
  title={A survey of explanations in recommender systems},
  author={Tintarev, Nava and Masthoff, Judith},
  booktitle={2007 IEEE 23rd international conference on data engineering workshop},
  pages={801--810},
  year={2007},
  organization={IEEE}
}

@article{dietvorst2015algorithm,
  title={Algorithm aversion: People erroneously avoid algorithms after seeing them err.},
  author={Dietvorst, Berkeley J and Simmons, Joseph P and Massey, Cade},
  journal={Journal of Experimental Psychology: General},
  volume={144},
  number={1},
  pages={114},
  year={2015},
  publisher={American Psychological Association}
}

@article{he2019attgan,
  title={Attgan: Facial attribute editing by only changing what you want},
  author={He, Zhenliang and Zuo, Wangmeng and Kan, Meina and Shan, Shiguang and Chen, Xilin},
  journal={IEEE transactions on image processing},
  volume={28},
  number={11},
  pages={5464--5478},
  year={2019},
  publisher={IEEE}
}

@inproceedings{yang2020visual,
  title={How do visual explanations foster end users' appropriate trust in machine learning?},
  author={Yang, Fumeng and Huang, Zhuanyi and Scholtz, Jean and Arendt, Dustin L},
  booktitle={Proceedings of the 25th International Conference on Intelligent User Interfaces},
  pages={189--201},
  year={2020}
}

@article{joshi2019towards,
  title={Towards realistic individual recourse and actionable explanations in black-box decision making systems},
  author={Joshi, Shalmali and Koyejo, Oluwasanmi and Vijitbenjaronk, Warut and Kim, Been and Ghosh, Joydeep},
  journal={arXiv preprint arXiv:1907.09615},
  year={2019},
  url = {http://arxiv.org/abs/1907.09615},
  issn = {2331-8422},
  arxivId = {1907.09615}
}

@inproceedings{birhane2022auditing,
  title={Auditing Saliency Cropping Algorithms},
  author={Birhane, Abeba and Prabhu, Vinay Uday and Whaley, John},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4051--4059},
  year={2022}
}

@book{byrne2007rational,
  title={The rational imagination: How people create alternatives to reality},
  author={Byrne, Ruth M.J.},
  year={2007},
  publisher={MIT press}
}

@article{mueller2019explanation,
  title={Explanation in human-AI systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable AI},
  author={Mueller, Shane T. and Hoffman, Robert R. and Clancey, William and Emrey, Abigail and Klein, Gary},
  journal={arXiv preprint arXiv:1902.01876},
  year={2019}
}

@article{lagnado2013causal,
  title={Causal responsibility and counterfactuals},
  author={Lagnado, David A and Gerstenberg, Tobias and Zultan, Ro'i},
  journal={Cognitive science},
  volume={37},
  number={6},
  pages={1036--1073},
  year={2013},
  publisher={Wiley Online Library}
}


@book{woodward2005making,
  title={Making things happen: A theory of causal explanation},
  author={Woodward, James},
  year={2005},
  publisher={Oxford University Press}
}

@book{lewis2013counterfactuals,
  title={Counterfactuals},
  author={Lewis, David},
  year={2013},
  publisher={John Wiley \& Sons},
  isbn = {978-1-118-69641-5}
}

@article{swanson2015snapshot,
  title={Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna},
  author={Swanson, Alexandra and Kosmala, Margaret and Lintott, Chris and Simpson, Robert and Smith, Arfon and Packer, Craig},
  journal={Scientific data},
  volume={2},
  number={1},
  pages={1--14},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{ustun2019actionable,
  title={Actionable recourse in linear classification},
  author={Ustun, Berk and Spangher, Alexander and Liu, Yang},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={10--19},
  year={2019}
}

@article{de2013coming,
  title={Coming to grips with the past: Effect of repeated simulation on the perceived plausibility of episodic counterfactual thoughts},
  author={De Brigard, Felipe and Szpunar, Karl K and Schacter, Daniel L},
  journal={Psychological science},
  volume={24},
  number={7},
  pages={1329--1334},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{petrocelli2011counterfactual,
  title={Counterfactual potency.},
  author={Petrocelli, John V and Percy, Elise J and Sherman, Steven J and Tormala, Zakary L},
  journal={Journal of personality and social psychology},
  volume={100},
  number={1},
  pages={30},
  year={2011},
  publisher={American Psychological Association}
}

@inproceedings{schut2021generating,
  title={Generating interpretable counterfactual explanations by implicit minimisation of epistemic and aleatoric uncertainties},
  author={Schut, Lisa and Key, Oscar and Mc Grath, Rory and Costabello, Luca and Sacaleanu, Bogdan and Gal, Yarin and others},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1756--1764},
  year={2021},
  organization={PMLR}
}

@article{erdfelder1996gpower,
  title={GPOWER: A general power analysis program},
  author={Erdfelder, Edgar and Faul, Franz and Buchner, Axel},
  journal={Behavior research methods, instruments, \& computers},
  volume={28},
  number={1},
  pages={1--11},
  year={1996},
  publisher={Springer}
}

@article{holtgen2021deduce,
  title={DeDUCE: Generating Counterfactual Explanations Efficiently},
  author={H{\"o}ltgen, Benedikt and Schut, Lisa and Brauner, Jan M and Gal, Yarin},
  journal={arXiv preprint arXiv:2111.15639},
  year={2021}
}

@inproceedings{AWS_Uncertainty,
  title={{Quantifying uncertainty in deep learning systems - An Amazon Web Services Prospective}},
  author={Davis, Josiah and Zhu, Jason and Oldfather, Jeremy and MacDonald, Samual and Trzaskowski, Maciej},
  booktitle={AWS Prespictive Guidance Report},
  year={2020}, 
  url = {https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-quantifying-uncertainty/welcome.html}
  }
  
  @article{delaney2021uncertainty,
  title={Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions},
  author={Delaney, Eoin and Greene, Derek and Keane, Mark T},
  journal={arXiv preprint arXiv:2107.09734},
  year={2021}
}

@article{pertuz2013analysis,
  title={Analysis of focus measure operators for shape-from-focus},
  author={Pertuz, Said and Puig, Domenec and Garcia, Miguel Angel},
  journal={Pattern Recognition},
  volume={46},
  number={5},
  pages={1415--1432},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{cai2019effects,
  title={The effects of example-based explanations in a machine learning interface},
  author={Cai, Carrie J and Jongejan, Jonas and Holbrook, Jess},
  booktitle={Proceedings of the 24th international conference on intelligent user interfaces},
  pages={258--262},
  year={2019}
}

@article{barnett2021case,
  title={A case-based interpretable deep learning model for classification of mass lesions in digital mammography},
  author={Barnett, Alina Jade and Schwartz, Fides Regina and Tao, Chaofan and Chen, Chaofan and Ren, Yinhao and Lo, Joseph Y and Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={3},
  number={12},
  pages={1061--1070},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{li2018deep,
  title={Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions},
  author={Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@book{eysenck1995cognitive,
  title={Cognitive Psychology: A Student’Handbook, 3rd edn},
  author={Eysenck, Michael W and Keane, Mark T},
  year={1995},
  publisher={Psychology press}
}

@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  journal={http://yann. lecun. com/exdb/mnist/},
  year={1998}
}

@article{rosch1978principles,
  title={Principles of categorization},
  author={Rosch, Eleanor},
  year={1978}
}

@article{tversky1977features,
  title={Features of similarity.},
  author={Tversky, Amos},
  journal={Psychological review},
  volume={84},
  number={4},
  pages={327},
  year={1977},
  publisher={American Psychological Association}
}

@inproceedings{Warren2022FeaturesXAI,
   title = {{Features of Explainability: How users understand counterfactual and causal explanations for categorical and continuous features in XAI}},
   year = {2022},
   booktitle = {IJCAI-ECAI’22 Workshop: Cognitive Aspects of Knowledge Representation},
   author = {Warren, Greta and Keane, Mark T. and Byrne, Ruth M. J.},
   keywords = {algorithmic recourse, XAI, counterfactual explanation, interpretable machine learning},
   url ={https://ceur-ws.org/Vol-3251/paper1.pdf}
}

@inproceedings{Warren2023Features2,
   title = {Categorical and Continuous Features in Counterfactual Explanations of AI Systems},
   year = {2023},
   booktitle = {28th International Conference on Intelligent User Interfaces},
   series = {IUI '23},
   location = {Sydney, NSW, Australia},
   author = {Warren, Greta and Byrne, Ruth M. J. and Keane, Mark T.},
   keywords = {algorithmic recourse, XAI, counterfactual explanation},
   doi = {https://doi.org/10.1145/3581641.3584090},
   publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
date ={March 27–31, 2023}
}

@inproceedings{
Singla2020Explanation,
title={Explanation  by Progressive  Exaggeration},
author={Sumedha Singla and Brian Pollack and Junxiang Chen and Kayhan Batmanghelich},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1xFWgrFPS}
}

@article{sokol2019CFsafeAI,
  title={Counterfactual explanations of machine learning predictions: opportunities and challenges for AI safety},
  author={Sokol, Kacper and Flach, Peter A},
  journal={SafeAI@ AAAI},
  year={2019}
}

@inproceedings{hanawa2021evaluation,
  title={Evaluation of Similarity-based Explanations},
  author={Hanawa, Kazuaki and Yokoi, Sho and Hara, Satoshi and Inui, Kentaro},
  booktitle={ICLR},
  year={2021}
}

@article{charpiat2019input,
  title={Input similarity from the neural network perspective},
  author={Charpiat, Guillaume and Girard, Nicolas and Felardos, Loris and Tarabalka, Yuliya},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{perronnin2010large,
  title={Large-scale image retrieval with compressed fisher vectors},
  author={Perronnin, Florent and Liu, Yan and S{\'a}nchez, Jorge and Poirier, Herv{\'e}},
  booktitle={2010 IEEE computer society conference on computer vision and pattern recognition},
  pages={3384--3391},
  year={2010},
  organization={IEEE}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

@article{chen2020concept,
  title={Concept whitening for interpretable image recognition},
  author={Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={2},
  number={12},
  pages={772--782},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{zhang2021invertible,
  title={Invertible concept-based explanations for cnn models with non-negative concept activation vectors},
  author={Zhang, Ruihan and Madumal, Prashan and Miller, Tim and Ehinger, Krista A and Rubinstein, Benjamin IP},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={13},
  pages={11682--11690},
  year={2021}
}

@article{ghorbani2019towards,
  title={Towards automatic concept-based explanations},
  author={Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{wexler2019if,
  title={The what-if tool: Interactive probing of machine learning models},
  author={Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Vi{\'e}gas, Fernanda and Wilson, Jimbo},
  journal={IEEE transactions on visualization and computer graphics},
  volume={26},
  number={1},
  pages={56--65},
  year={2019},
  publisher={IEEE}
}

@inproceedings{kuhl2022keep,
  title = {{Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting}},
    year = {2022},
    booktitle = {FAccT 2022 - Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
    author = {Kuhl, Ulrike and Artelt, André and Hammer, Barbara},
    month = {6},
    pages = {2125--2137},
    publisher = {Association for Computing Machinery (ACM)},
    doi = {10.1145/3531146.3534630},
}

@article{kuhl2022let,
  title={Let's Go to the Alien Zoo: Introducing an Experimental Framework to Study Usability of Counterfactual Explanations for Machine Learning},
  author={Kuhl, Ulrike and Artelt, Andr{\'e} and Hammer, Barbara},
  journal={arXiv preprint arXiv:2205.03398},
  year={2022},
  url = {http://arxiv.org/abs/2205.03398},
    arxivId = {2205.03398}
}

@article{artelt2022one,
  title={One Explanation to Rule them All -- Ensemble Consistent Explanations},
  author={Artelt, Andr{\'e} and Vrachimis, Stelios and Eliades, Demetrios and Polycarpou, Marios and Hammer, Barbara},
  journal={IJCAI workshop on eXplainable Artificial Intelligence},
  year={2022},
  url ={https://arxiv.org/abs/2205.08974}
}

@article{rawal2020beyond,
  title={Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses},
  author={Rawal, Kaivalya and Lakkaraju, Himabindu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12187--12198},
  year={2020},
  url = {https://proceedings.neurips.cc/paper/2020/file/8ee7730e97c67473a424ccfeff49ab20-Paper.pdf}
}

@article{ding2021retiring,
  title={Retiring adult: New datasets for fair machine learning},
  author={Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf},
  volume={34},
  pages={6478--6490},
  year={2021}
}

@inproceedings{jiang2018trust,
  title={To Trust Or Not To Trust A Classifier.},
  author={Jiang, Heinrich and Kim, Been and Guan, Melody Y and Gupta, Maya R},
  booktitle={NeurIPS},
  pages={5546--5557},
  year={2018}
}

@article{klaise2020alibi,
  title={Alibi: Algorithms for monitoring and explaining machine learning models},
  author={Klaise, Janis and Van Looveren, Arnaud and Vacanti, Giovanni and Coca, Alexandru},
  url={https://github. com/SeldonIO/alibi},
  year={2020}
}

@article{fernandez2022explanation,
  title={Explanation sets: A general framework for machine learning explainability},
  author={Fern{\'a}ndez, Rub{\'e}n R and de Diego, Isaac Mart{\'\i}n and Moguerza, Javier M and Herrera, Francisco},
  journal={Information Sciences},
  volume={617},
  pages={464--481},
  year={2022},
  publisher={Elsevier}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H.},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR},
  url = {http://www.jstor.org/stable/2699986}
}


@inproceedings{plumb2020explaining,
  title={Explaining groups of points in low-dimensional representations},
  author={Plumb, Gregory and Terhorst, Jonathan and Sankararaman, Sriram and Talwalkar, Ameet},
  booktitle={Proceedings of the 37th International Conference on Machine Learning},
  pages={7762--7771},
  year={2020},
  publisher = {JMLR.org},
  organization={PMLR},
  url = {https://dl.acm.org/doi/abs/10.5555/3524938.3525657},
  series = {ICML'20}
}

@article{ryan2021predicting,
  title={Predicting illness for a sustainable dairy agriculture: predicting and explaining the onset of mastitis in dairy cows},
  author={Ryan, Cathal and Gu{\'e}ret, Christophe and Berry, Donagh and Corcoran, Medb and Keane, Mark T. and Mac Namee, Brian},
  journal={arXiv preprint arXiv:2101.02188},
  year={2021},
  doi = {
https://doi.org/10.48550/arXiv.2101.02188
}
}

@inproceedings{ott1999costs,
  title={Costs of herd-level production losses associated with subclinical mastitis in US dairy cows},
  author={Ott, Stephen L},
  booktitle={ANNUAL MEETING-NATIONAL MASTITIS COUNCIL INCORPORATED},
  volume={38},
  pages={152--153},
  year={1999},
  organization={NATIONAL MASTITIS COUNCIL, INC.}
}

@article{arjun2023predicting,
  title={Early detection of subclinical mastitis in lactating dairy cows Using cow level features},
  author={Pakrashi, Arjun and Ryan, Cathal and Gu{\'e}ret, Christophe and Berry, Donagh and Corcoran, Medb and Keane, Mark T. and Mac Namee, Brian},
  journal={Journal of Dairy Science},
  year={2023}
}

@article{guidotti2022counterfactual,
  title={Counterfactual explanations and how to find them: literature review and benchmarking},
  author={Guidotti, Riccardo},
  journal={Data Mining and Knowledge Discovery},
  pages={1--55},
  year={2022},
  publisher={Springer},
  doi = {https://doi.org/10.1007/s10618-022-00831-6}
}

@article{faul2009statistical,
  title={Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses},
  author={Faul, Franz and Erdfelder, Edgar and Buchner, Axel and Lang, Albert-Georg},
  journal={Behavior research methods},
  volume={41},
  number={4},
  pages={1149--1160},
  year={2009},
  publisher={Springer},
  doi= {https://doi.org/10.3758/BRM.41.4.1149}
}


@article{guidotti2018local,
  title={Local rule-based explanations of black box decision systems},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  journal={arXiv preprint arXiv:1805.10820},
  year={2018},
  doi = {10.48550/arxiv.1805.10820},
  url = {https://arxiv.org/abs/1805.10820},
}

@article{brughmans2021nice,
  title={NICE: an algorithm for nearest instance counterfactual explanations},
  author={Brughmans, Dieter and Leyman, Pieter and Martens, David},
  journal={arXiv preprint arXiv:2104.07411},
  year={2021},
  doi = {https://doi.org/10.48550/arXiv.2104.07411}
}

@inproceedings{kasirzadeh2021use,
 author = {Kasirzadeh, Atoosa and Smart, Andrew},
title = {The Use and Misuse of Counterfactuals in Ethical Machine Learning},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445886},
doi = {10.1145/3442188.3445886},
abstract = {The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {228–236},
numpages = {9},
keywords = {Explainable AI, Ethics of AI, Explanation, Counterfactuals, Algorithmic Fairness, Social ontology, Philosophy, Fairness, Philosophy of AI, Social kind, Machine learning, Social category, Ethical AI},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{slack2021counterfactual,
  title={Counterfactual explanations can be manipulated},
  author={Slack, Dylan and Hilgard, Anna and Lakkaraju, Himabindu and Singh, Sameer},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={62--75},
  year={2021},
  url = {https://proceedings.neurips.cc/paper/2021/file/009c434cab57de48a31f6b669e7ba266-Paper.pdf}
}