Graphics Processing Units (GPUs) were created to service graphics applications and accelerate parts of the rasterization pipeline. The acceleration was due to optimized floating point arithmetic using a large number of arithmetic cores. Researchers wanted to harness this massive computational capability to accelerate non-rendering applications. However, this was not straightforward as it required re-formulating problems as 3D rendering problems. Over time, the emergence of platforms such as CUDA \cite{cuda} and OpenCL\cite{open-cl} provided a programming model for general-purpose computations on GPUs. The General-purpose GPU (GPGPU) programming model lets users offload parallelizable and compute-intensive components (\eg, training a neural network) to the GPU by leveraging the programmable shader cores. Unfortunately, GPGPU acceleration using shader cores remains largely the province of {\em regular} applications that rely on dense loops over dense structures, such as matrix operations, convolutions, etc.

\paragraph{GPU acceleration of ray tracing}
Interestingly, while GPU shader cores are well-suited for one style of graphics rendering---rasterizing---they are not at all well suited to a different, more accurate, rendering algorithm, {\em ray tracing}.
Ray tracing (or, more accurately, {\em ray casting}) flips the approach of rasterizing. Rather than considering each object and how it affects one or more pixels on the screen (e.g., whether the object is visible or occluded by other objects), ray tracing considers each {\em pixel} in the scene and determines what color it should be based on the objects and lights it interacts with \cite{ray_trace_whitted}.
% Ray tracing is a technique that is extensively used in computer graphics for real-time rendering of realistic images. 
% The traditional method to determine the color of a pixel would be to trace rays emitted from a source of light, a lamp for example, and observe if the rays intersect objects in the surroundings. However, a light source emits billions of rays and not all rays influence the coloring of objects as perceived by an observer. This results in unnecessary ray-object intersection tests, which are computationally-intensive. Instead of tracing rays from the source to the viewer to determine the color of a pixel, ray tracing flips this around and traces the ray from the viewer to the source.
In ray tracing, a ray is cast from the source, which is typically a pinhole camera, for every pixel in the image plane. 
% This process is called ray casting. 
These primary rays pass through the image plane and their interactions with objects in the scene determine the color of the pixel. The ray could intersect an object in the scene, creating new reflected, refracted and/or shadow secondary rays. This hierarchy of rays is represented as a ray tree, with the primary ray as the root and the spawned secondary rays as internal nodes (secondary ray intersections can spawn tertiary rays) or leaf nodes.

Ray-object intersection tests are the biggest bottleneck in the ray tracing pipeline due to their computational intensity. As {\em each} ray has to be tested for intersection against {\em every} object in the scene, performance suffers greatly.
However, it is not {\em always} necessary to test for intersection against every object. We can represent the objects in the scene using a Bounding Volume Hierarchy (BVH), a type of {\em spatial acceleration tree} \cite{waldbvh}. A BVH, like other spatial trees, captures the relationship of objects to each other in space by recursively subdividing the space into smaller cells until the leaf cells contain bounding volumes that contain single objects. Ray-object intersection can thus be performed hierarchically: if a ray does not intersect a bounding volume, then it cannot intersect any of the subordinate bounding volumes in the tree, eliminating large numbers of intersection tests. (Section~\ref{sec:bvh} describes this process in more detail.)

Unfortunately, BVH-based ray-tracing is {\em highly} irregular: each ray performs a tree traversal whose extent is highly input-dependent. While prior work has shown that tree traversals can be performed reasonably well on GPGPUs~\cite{goldfarb13sc}, shader cores are simply not the best-suited accelerator for BVH traversals. Hence, recent GPUs from NVIDIA and AMD have added {\em ray tracing (RT) cores}. These cores provide specialized hardware for building and traversing bounding volume hierarchies, significantly accelerating the process of ray tracing \cite{whitepaper}.

\paragraph{Re-purposing RT cores}
In the same way that early GPU programmers looked to re-purpose shader cores to perform non-rasterization tasks, a natural question to ask is whether RT cores can be leveraged to accelerate non-raytracing algorithms. Recent work has suggested the answer might be ``yes.'' Wald~\etal~\cite{wald19} accelerate the task of locating which tetrahedron of a solid a query point lies in by treating the query point as the source of a ray and seeing if it intersects a tetrahedron. While this is perhaps an obvious adaptation of ray-tracing, as it is effectively ray tracing itself, later work has pushed the boundaries further. Zellman~\etal~\cite{forcegraph} and Evangelou~\etal~\cite{Evangelou2021RadiusSearch} showed how to reduce the problem of finding the set of points in a fixed-radius neighborhood of a query point to ray tracing to build force-directed graphs and to do photon mapping, respectively.\footnote{This reduction is described in more detail in Section~\ref{sec:design}.} 

These papers use this reduction to write custom ray-tracing kernels that solve these problems. The algorithms essentially use one-shot invocations of ray-tracing hardware to perform the necessary distance computations and solve the problems. However, some distance-based algorithms require integrating repeated distance queries into larger programs, and hence require more careful use of the reduction. 

In particular, DBSCAN \cite{Ester96adensity-based} is a clustering algorithm that groups nearby points in space into clusters based on the distance points within the cluster are from other points in the cluster. Solving this problem requires repeatedly updating cluster definitions by repeatedly identifying nearby points, a more complex task than the ``single shot'' distance computations of prior RT-acceleration work. In this paper, we investigate whether RT cores can be used to accelerate this algorithm. The contributions of our paper are as follows:
\begin{itemize}
    \item This paper introduces RT-DBSCAN, the first RT-accelerated clustering algorithm. As DBSCAN uses distance-based queries to identify neighboring points, we are able to leverage the reduction of Zellman~\etal~\cite{forcegraph} and Evangelou~\etal\cite{Evangelou2021RadiusSearch} to accelerate neighbor searches, which are a major computational bottleneck.
    \item We create a primitive {\tt RT-FindNeighbor} (Details in Section~\ref{alg:rt-neigh}), allowing us to easily negotiate with the ray tracing hardware and its associated programming model (the Optix Wrapper Library (OWL)~\cite{owl}).
    \item  We use {\tt RT-FindNeighbor} to implement a {\tt UNION-FIND}-based DBSCAN algorithm (Details in Section~\ref{alg:parallel_dbscan}) that minimizes memory consumption. We proceed to show that RT-DBSCAN outperforms current state-of-the-art DBSCAN algorithms that have been optimized to run on GPUs.
\end{itemize}
%\krish{It would be clearer to list the contributions.}
%\vani{done}
%This paper introduces DART, a domain specific language for writing applications that use distance-based queries and accelerating them using the ray tracing cores. DART leverages the reduction of Zellman~\etal and Evangelou~\etal to expose a generic distance primitive to users that allows them to write applications that perform distance queries and do custom computations on the results of those queries in a natural way. Crucially, by providing this primitive without requiring the programmer to negotiate with the ray tracing hardware or its associated programming model (the Optix Wrapper Library (OWL)~\cite{owl}), it becomes straightforward to write applications that need {\em multiple} rounds of distance computations to compute their results.

%We identify a subset of applications such as Point Correlation, k-Nearest Neighbors and Density-Based Clustering of Applications with Noise (DBSCAN), that rely on efficient fixed-radius nearest neighbor searches using spatial trees. In this work, we take the first step towards creating a general purpose programming model targeting RT cores in recent GPUs. We evaluate the effectiveness of using DART to accelerate distance computations in DBSCAN and show performance improvements up to 13x compared to the current state-of-the-art, GPU-accelerated implementations.