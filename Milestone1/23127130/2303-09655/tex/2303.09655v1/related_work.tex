\paragraph{Using RT cores for non-Ray-Tracing Applications}
%\vani{Need to add citation for RTNN}
%The idea of using RT cores to accelerate applications other than ray tracing was first introduced by Wald ~\etal\cite{wald19}. 
Wald~\etal first used RT cores to accelerate non-ray-tracing programs~\cite{wald19}. 
They formulated the problem of identifying a point's location in a tetrahedral mesh as a ray tracing problem by declaring the meshes as 3D objects in a scene and tracing rays originating at the query point. 
They show how leveraging both hardware BVH traversal and ray-triangle intersections resulted in upto 6.5x speedup over other CUDA implementations. 
Morrical~\etal used RT cores to successfully accelerate the unstructured mesh point location problem~\cite{Morrical2019EfficientSS}. Zellmann~\etal proposed a mapping of the fixed-radius nearest neighbor query to a ray tracing query for the Spring Embedders force-directed graph drawing algorithm\cite{forcegraph}.
%by expanding spheres over points in the dataset and launching a small ray to record intersections for the Spring Embedders force-directed graph drawing algorithm\cite{forcegraph}. 
Evangelou~\etal used the nearest neighbor mapping to solve the k-nearest neighbors problem\cite{Evangelou2021RadiusSearch}. Zhu proposed query re-ordering and partitioning algorithms to improve ray coherence and minimize the number of intersection tests performed\cite{rtnn}. We note that adding these optimizations to RT-DBSCAN would further improve performance.

\paragraph{DBSCAN}
Ester~\etal introduced the DBSCAN algorithm in 1996 to identify clusters of arbitrary shapes based on dense regions in the dataset\cite{Ester96adensity-based}. Although DBSCAN is an inherently sequential algorithm (Algorithm\ref{alg:dbscan}), researchers have exploited GPU parallelism to accelerate DBSCAN. Thapa~\etal exploited parallelism by having multiple CPU threads perform $\varepsilon$-neighbor distance computations in parallel\cite{5612134}. Andrade~\etal proposed G-DBSCAN, where they built a graph over the dataset and performed parallel Breadth First Searches to mark reachable points as belonging to the same cluster\cite{gdbscan}. 
%with vertices as data points and edges between points within $\varepsilon$ distance of each other\cite{gdbscan} . 
%They perform parallel Breadth First Searches on the graph starting at Core points and marking reachable points as belonging to the same cluster. 
However, the memory required to store and maintain the graph structure affects the scalability of G-DBSCAN. B\"{o}hm~\etal introduced CUDA-DClust, which used a spatial index structure to incrementally grow clusters in parallel\cite{cuda-dclust}. 
%It used the concept of {\em chains}, which are subsets of clusters, to incrementally grow clusters in parallel and merge colliding chains using a collision matrix. 
Poudel~\etal proposed CUDA-DClust+ which improved on CUDA-DClust by building the index structure on the GPU instead of CPU and reducing communication overhead\cite{9680379}. However, CUDA-DClust+ requires a significant amount of time for index construction and suffers when the size of GPU memory is small. Prokopenko~\etal proposed FDBSCAN and FDBSCAN-DenseBox that use Bounding Volume Hierarchy with UNION-FIND for clustering\cite{DBLP:journals/corr/abs-2103-05162}. Both FDBSCAN and FDBSCAN-DenseBox avoid memory issues as they do not store any neighbor information. FDBSCAN-DenseBox, similar to \cite{6877517,bps-hdbscan}, superimposes a Cartesian grid-based indexing to identify dense regions and reduce the number of distance computations in dense boxes.