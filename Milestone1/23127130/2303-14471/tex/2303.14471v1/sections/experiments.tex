\section{Experiments} \label{sec:experiments}

\input{figures/dataset_updated.tex}


In this section, we show the effectiveness of our high-quality volumetric head avatar reconstruction method in synthesizing novel dynamic expressions and views at high fidelity and resolution. We show two main applications our approach enables, namely dynamic free-view synthesis from arbitrary monocular viewpoints as well as renderings at different image resolutions, including FHD. We also perform a thorough analysis of our modeling choices and conduct quantitative and qualitative evaluations with state-of-the-art baselines. We refer the reader to the supplemental for video results.

\subsection{Datasets}\label{sec:datasets}
Our multi-view video dataset consists of $16$ subjects, including $14$ males and $2$ females, and most of them are in their 20s or 30s. 
The subjects have short to long-length hairstyles. Male subjects either are shaved or have stubble or hairy beards. A collage of the recorded subjects is shown in Fig.~\ref{fig:datasetIDs}, top. 
%
To build our dynamic dataset, we instructed subjects to perform random expressive faces during $2$ minutes and/or recite $47$ phonetically balanced sentences. 
Among the 16 subjects, 4 have only performed expressions, 1 has only performed reciting, while 11 have performed both. We will release our full multi-view video dataset to foster future research on head avatar generation. For all of our experiments reported next, we utilize $18$ views, each containing $1500$ frames at 960x540 resolution, to train our personalized models and generate results, unless stated otherwise. We processed 6 subjects covering a wide variety of our dataset e.g. gender, expressions, facial hair, movements, scalp hair, ethnicity, etc. 

\subsection{Qualitative and Quantitative Results}\label{sec:qualitative_evaluations}

\begin{table}[!ht]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Metrics & Without  & Without & Without   & Ours   \\
 & canonical &  image feature & optical flow   &    \\
  & space &conditioning & based loss &    \\
\hline
PSNR $\uparrow$  &29.24 & 29.64 &29.38 &\textbf{31.23} \\
\hline
L1 $\downarrow$ &3.61& 3.64& 3.32&  \textbf{2.79}  \\
\hline
SSIM $\uparrow$ &0.8698& 0.8744& 0.8517& \textbf{0.8837}  \\
\hline
LPIPS $\downarrow$ &0.1408& 0.1191&0.1200 & \textbf{0.1130}   \\
\hline
\end{tabular}
\caption{Ablation study: Image quality and perceptual metrics for different design choices. L1 measures the absolute error of unnormalized RGB images. Our full method produces the best results (see bold text).}
\label{tab:quantitative_results-ave_error-ablation}
\end{table}

\input{figures/changing_exp.tex}


Fig.~\ref{fig:qualitative_results-exp_changes} shows dynamic expression synthesis of $4$ personalized avatar models on test sequences, while Fig.~\ref{fig:qualitative_results-nvs} illustrates free viewpoint synthesis of $5$ personalized models. Note that the generated views represent interpolations from training views. In both figures, the avatars are driven by a frontal-looking monocular RGB video. Our approach achieves high-quality renderings of head avatars under novel camera viewpoints and for challenging novel expressions. Tab.~\ref{tab:quantitative_results-ave_error-ablation} shows that our approach on average obtains high PSNR (over $31$ dB) and low reconstruction errors on test sequences based on different image quality and perceptual metrics. Please see the supplemental for video results. 


\input{figures/nvs_fixed_exp.tex}


\subsection{Applications}\label{sec:applications}
\paragraph{\textbf{Avatar Synthesis from an Arbitrary Monocular Viewpoint}.}

In previous experiments, we have shown that we can drive our head avatar using a monocular video captured from a frontal view.
%
Here we further show an application where we can drive our head avatar from an arbitrary viewpoint.
%
To achieve this, we define a fine-tuning scheme described as follows: 
First, we synthesize a training dataset from a novel viewpoint, say $\hat{v}$, with the personalized avatar model described in Sec.\ref{sec:method}. This dataset contains the same dynamic expressions used for training. Then, we finetune the image encoder with this synthetic video stream for 100k iterations. Note that the deformation and radiance field networks as well as the multiresolution hash encoding remain unchanged.
%
Once the image encoder has been fine-tuned, we can drive the personalized avatar model with the real data stream coming from the viewpoint $\hat{v}$. 
In our experiments, $\hat{v}$ is a held-out viewpoint not used when training the avatar model. 

Fig. \ref{fig:qualitative_results-drive_finetuned} compares frontal renderings of Subject 3's avatar model, driven from two video streams with unseen expressions: One driven from a frontal view camera and another driven from a held-out bottom view. Our method produces high-fidelity renderings regardless of the driving video viewpoint, and the rendered expressions faithfully reproduce those shown in the driving video. 
This demonstrates that our personalized avatar model can generate photo-realistic renderings from arbitrary viewpoints at high fidelity. These renderings can be used as a good approximation of real images to fine-tune the image encoder from arbitrary driving viewpoints. Note that this experiment paves the way for learning high-fidelity personalized avatars that can be driven from video captured in the wild. 

\input{figures/Synthetic_drive.tex}

\paragraph{\textbf{FHD Image Synthesis.}}

Our multiresolution hash grid encoding allows for training a personalized avatar model at full HD resolutions, which surpasses the capabilities of state-of-the-art approaches. Our method can render HD images (960x540) at about $10$ fps and FHD (1920x1080) images a bit below $3$ fps. Fig.~\ref{fig:qualitative_results-FHD_training} compares renderings of personalized models trained at HD and FHD resolutions. Both models generate visually similar facial features and details, though the FHD model produces crisper results, as expected. Overall our approach scales well, and the decrease in runtime is near linear. Fig.~\ref{fig:realtime} shows that our approach can also run on a resolution of 480x270 in real time ($25$ fps) while still maintaining high fidelity in the reconstructions. Note that the reported runtimes are based on a single NVIDIA A100 GPU. Please see the supplemental video for more results.

\input{figures/hd_fhd.tex}

\input{figures/Realtime_480.tex}



\subsection{Ablative Analysis} 
\label{sec:ablation_study}



\input{figures/quant_ablations.tex}

We demonstrate our main contributions and the influence of design choices via a number of ablation studies. Specifically, we study our novel optical flow based loss, learned image based feature conditioning of the canonical radiance field network, and canonical space representation. We also analyze the influence of perceptual loss and error map based pixel sampling in the reconstruction quality. Note that for these experiments, we train our personalized avatar models on $18$ views, while we keep out $2$ views for our quantitative evaluations.

Fig.~\ref{fig:ablation_study-fvs_error-canonical_feature_oflow_ours} shows the reconstruction quality of our method and different modeling choices for a fixed unseen expression and a novel camera viewpoint rendering (a held-out view). Here, the error map (bottom row) represents a pixel-wise mean square error (MSE) of head renderings in RGB color space. Fig.~\ref{fig:ablation_study-dvs-perceptual_error_ours} further compares our approach with the same design choices, for a fixed expression but under dynamic novel viewpoint synthesis. Note that dynamic viewpoints are interpolated from different camera viewpoints.
From these results, we can observe that without conditioning the canonical space on the driving image features 
the reconstruction has blurry artifacts all over the mouth. Without the optical flow based loss, blocky artifacts and/or inconsistent fine-scale details appear in sparsely sampled regions, such as hair, eyelids, and teeth. Note that a canonical space representation is required for proper encoding of facial dynamics; otherwise, artifacts emerge. 
%
The error heatmap visualization in Fig.~\ref{fig:ablation_study-fvs_error-canonical_feature_oflow_ours} (bottom row) provides a quantitative measurement of the error distribution, showing that our approach with all design choices achieves the best rendering quality. 
%
Tab.~\ref{tab:quantitative_results-ave_error-ablation} shows the average reconstruction error over the entire test set (200 frames) for different well-established image-based quality metrics.
We adopt similar metrics to that of MVP~\cite{lombardi21_mvp}. 
%
We measure the Manhattan distance $L1$ in the RGB color space, PSNR, SSIM~\cite{wang2004_image-quality}, and LPIPS~\cite{zhang2018_unreasonable-effectiveness}.
Overall our approach attains the best numerical results. This study confirms that our key modeling choices optimize the rendering quality. 
%
%
We also show in Fig.~\ref{fig:ablation_study-details-perceptual_error_ours} that the perceptual loss and error map based sampling improve the rendering results. While we have noticed that these components help in improving rendering quality, we do not emphasize them as a contribution.


\input{figures/ablations_updated.tex}

\input{figures/emap_perceptuial_ablation.tex}

\subsection{Comparisons with the State of the Art}\label{sec:comparisons}

\input{figures/Comparisons_quant.tex}

\input{figures/Comparisons_Qualitative.tex}
\input{figures/tongue.tex}
In this section, we compare our approach with a recent multi-view state-of-the-art method, called MVP~\cite{lombardi21_mvp}, which produces detailed avatars with high fidelity under a similar setup to ours. We disregard direct comparisons with state-of-the-art sparse multi-view approaches since they tend to lack fine-scale details or are prone to artifacts for novel viewpoint synthesis (see Sec.~\ref{sec:related_work}). In addition, we provide baseline comparisons with an adaptation of a template-free dynamic representation, called HyperNeRF \cite{park2021_hypernerf}, and a multi-level hash table-based approach for expression encoding, called NeRFBlendShape \cite{gao2022_nerfblendshape}. We will call our multi-view and image-driven adaptation of these approaches HyperNeRF++ and NeRFBlendShape++.

To train NeRFBlendShape++, we pass each entry of the expression latent vector to a learnable multi-level hash table. We linearly combine the output of these hash tables and condition the NeRF network on it. To train HyperNeRF++, we feed the neural features passed on by the image encoder to an ambient and deformation network and then as appearance conditioning to the NeRF network.
To run MVP, we use 4k primitives. We employ an in-house FLAME-based tracking to obtain a non-detailed dense reconstruction of the subject's head to guide the initialization of the primitives at each frame.

Fig.~\ref{fig:comparison_stota-fvs_error} shows the reconstruction quality of our method and baseline approaches for a fixed unseen expression and a novel camera viewpoint rendering (a held-out view), while Fig.~\ref{fig:comparison_stota-fvs} compares them in a free-viewpoint synthesis setup. HyperNeRF++ over smooths regions.
Both NeRFBlendShape++ and HyperNeRF++ exhibit artifacts in regions that undergo recurrent topological changes, e.g., the mouth interior, or that have complex structures, e.g., scalp hair. The latter not only produces stronger artifacts in the form of grid patterns but also removes facial details. Overall these methods generalize poorly due to over-parameterized representations.
MVP can sometimes produce wrong facial expressions in extreme cases or even sometimes show unusual block artifacts for the same regions mentioned above. One of the main reasons is that MVP relies on very dense multi-view imagery to supervise volume rendering. However, in a sparser camera setup undersampled areas, especially those undergoing disocclusions, become ambiguous without explicit dense volume deformation constraints. The error heatmap visualization of Fig.~\ref{fig:comparison_stota-fvs_error} (last row), shows that our method reduces reconstruction errors.
Overall our approach produces sharper, more accurate, and more photorealistic rendering results. Please refer to the supplementary video for further comparisons in dynamic viewpoint synthesis.


We perform quantitative evaluations on the 2 held-out views, with 200 frames each. Quantitative comparisons are reported in Tab.~\ref{tab:quantitative_results-ave_error-sota}. Our approach clearly outperforms other baseline approaches, especially when comparing perceptual metrics, such as SSIM and LPIPS. L1 reconstruction error is also significantly reduced.
We remark that our approach attains sharper reconstructions with faster convergence and efficiency, the latter thanks to hash-encoding and empty-space pruning techniques.

\begin{table}[!ht]
\centering
\small{
\begin{tabular}{|c|c|c|c|c|}
\hline
Metrics & HyperNeRF++ & MVP & NeRFBlendshape++ & Ours   \\
\hline
PSNR $\uparrow$  &26.42 & 28.72 &29.66 &\textbf{31.23} \\
\hline
L1 $\downarrow$ &5.61& 3.64& 3.23&  \textbf{2.79}  \\
\hline
SSIM $\uparrow$ &0.8509& 0.8283& 0.8745& \textbf{0.8837}  \\
\hline
LPIPS $\downarrow$ &0.1721& 0.1432&0.1326 & \textbf{0.1130}   \\
\hline
\end{tabular}
}
\caption{Quantitative comparison with state-of-the-art approaches. L1 measures the absolute error of unnormalized RGB images. Our approach outperforms related methods (see bold text).}
\label{tab:quantitative_results-ave_error-sota}
\end{table}



