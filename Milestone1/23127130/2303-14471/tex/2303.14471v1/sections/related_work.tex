\section{Related Work} 
\label{sec:related_work}
This section reviews prior work on photorealistic human head avatar generation, including approaches using monocular or multi-view RGB data. Early methods are based on explicit 3D scene representations, while recent ones leverage implicit representations. 

\subsection{Monocular Head Avatar Generation}
Several monocular avatar generation methods rely on explicit 3D models to estimate or regress a 3D face~\cite{thies2019_face2face,tewari2018_self-supervised,yamaguchi2018_high-fidelity,grecer2019_ganfit,tran2019_towards,shamai2019_synthesizing-facial,lattas2022_avatarme++,lin2020_towards-hf,ren2022_facial-geom} or a 3D head containing the face, ears, neck, and hair~\cite{ichim2015_dynamic_3davatar,cao2016_realtime,nagano2018_pagan} with photorealistic appearance from 2D images. These methods employ a statistical deformable shape model (a.k.a. 3DMM) of human faces~\cite{cao2014_facewarehouse,li2017_flame,gerig2018_morphable_models}
, which provides parametric information to represent the global shape and the dynamics of the face. However, explicit model-based approaches often generate avatars with coarse expressions or facial dynamics and 
usually lack a detailed representation of the scalp hair, eyes, and/or mouth interior e.g. tongue.
Other approaches attempt to synthesize dynamic full head avatars in a video via generative 2D neural rendering, driven via sparse keypoints~\cite{wang2021_one-shot,meshry2021_learned-spatial} or dense parametric mesh priors~\cite{kim2018_dvp,thies2019_dnr,tewari2020_pie,chandran2021_rendering-style}. These methods usually utilize GANs to translate parametric models into photorealistic 2D face portraits with pose-dependent appearance. Still, these methods struggle with fine-scale facial details, and they fail to generate 3D-consistent views.

Recent advances in neural implicit models for personalized head avatar creation from monocular video data have shown great promise. Most approaches learn deformation fields in a canonical space using dense mesh priors~\cite{zheng2022imavatar,athar2022rignerf,gao2022_nerfblendshape,grassal2022_neural-head}.
Here, \cite{gao2022_nerfblendshape} leverages multi-level hash tables to encode expression-specific voxel fields efficiently.
However, it still needs to regress to an intermediate expression space defined via 3DMM.
While the above methods generate photorealistic 3D heads with full parametric control, reconstructions can lack dynamics and fine-scale geometrical details, and they cannot handle extreme expressions. On the other hand, our approach is not 3DMM based and thus can model complex geometry and appearance under novel views. This is attributed to our learnable fully implicit canonical space conditioned on the driving video, as well as a novel scene flow constraint. 

\subsection{Multi-view Head Avatar Reconstruction}

A number of approaches leverage multi-view video data to create view-consistent and photorealistic human head avatars with a high level of fidelity. In the literature, we identify approaches that can reconstruct avatars from sparse views ($<=$ 10 high-resolution cameras) or require dense multi-camera systems with dozens of high-resolution views to achieve high-quality results. Due to the large volume of high-resolution video data, recent approaches have also focused on reducing computational and memory costs. Strategies such as efficient sampling~\cite{wang2021_learning-crf} and empty space pruning~\cite{lombardi21_mvp} have been proposed. We also adopt these strategies for efficient and highly detailed rendering at high resolutions.

\paragraph{\textbf{Sparse multi-view methods.}}
A line of research investigates lightweight volumetric approaches that aim at reducing the number of input views while attempting to preserve the reconstruction fidelity of dense camera approaches. Sparse methods often resort to a canonical space representation~\cite{park2021_nerfies}, which serves as a scene template for learning complex non-linear deformations. Pixel aligned volumetric avatars (PAVA)~\cite{raj21_pixel-aligned} is a multi-identity avatar model that employs local, pixel-aligned neural feature maps extracted from 3D scene locations. KeypointNeRF~\cite{mihajlovic22_keypointnerf} is another generalized volumetric avatar morphable model that encodes relative spatial 3D information via sparse 3D keypoints. At inference, both PAVA and KeypointNeRF can robustly reconstruct unseen identities performing new expressions from 2 or 3 input views. TAVA~\cite{li2022_tava} encodes non-linear deformations around a canonical pose using a linear blend skinning formulation. TAVA requires 4-10 input views to train a personalized model. While these approaches can generate photorealistic avatars with plausible dynamic deformations from sparse input views, they cannot generate fine-scale details and are sensitive to occlusions, producing renderings artifacts. We demonstrate that regions that undergo sparse sampling can still be reconstructed at high fidelity by imposing temporal coherency via optical flow. 


\paragraph{\textbf{Dense multi-view methods.}} Early work with dense setups, called Deep Appearance Models (DAM) learn vertex locations and view-specific textures of personalized face models via Variational Autoencoders~\cite{lombardi2018_deep-appear}. Pixel Codec Avatars (PiCA)~\cite{ma2021_pixel-codec} improve upon DAM by decoding per-pixel renderings of the face model via an implicit neural function (SIREN) with learnable facial expression and surface positional encodings.
Most recent dense approaches adopt volumetric representations, such as discrete voxel grids~\cite{lombardi2019_neural-volumes}, hybrid volumetric models~\cite{wang2021_learning-crf,lombardi21_mvp}, or NeRFs~\cite{wang2022_morf}. Here, hybrid approaches combine coarse 3D structure-aware grids and implicit radiance functions, locally conditioned on voxel grids~\cite{wang2021_learning-crf} or template-based head tracking with differentiable volumetric raymarching~\cite{lombardi21_mvp}. In \cite{wang2022_morf}, a morphable radiance fields framework for 3D head modeling, called MoRF, is proposed. This framework learns statistical face shape and appearance variations from a small-scale database, though it demonstrates good generalization capabilities. While dense methods produce photo-realistic avatars, renderings tend to exhibit inaccuracies and blur artifacts, especially for complex structures and in infrequently observed areas, such as the scalp hair and mouth interior. Besides, most dense approaches rely on head priors, either mesh tracking or coarse voxel grids, and thus, they are prone to reconstruction errors and have limited representation power, e.g., handling details, mouth interior, and hair. Our approach overcomes existing limitations by solely relying on a well-constrained canonical representation that preserves expression semantics and scene flow correspondences.

\input{figures/Method_pipeline.tex}


\subsection{Generalized 3D Consistent Neural Representations}
Modeling 3D-aware scenes with implicit models has been active research in recent years. Popular methods are NeRFs~\cite{mildenhall2022_nerf} and neural Signed Distance Functions (SDFs)~\cite{park2019_deepsdf}, both parameterize the 3D space using multi-layer perceptrons (MLPs). Since such methods are often computationally expensive, efficient feature and/or scene space encodings, such as hash grids~\cite{mueller2022_instant-ngp,fridovich2022_plenoxels} or trees~\cite{yu2021_plenoctrees,takikawa2021_nglod}, have been proposed to boost performance. 
In the literature, generalized implicit models for head avatar reconstruction are learned from a large corpus of 2D face images with varying pose and facial shape using neural SDFs~\cite{ramon2021_h3dnet, or-el2022_stylesdf}, GAN-based NeRFs~\cite{deng2022_gram, chan2021_pigan, gu2022_stylenerf} or hybrid volumetric approaches with tensor representations~\cite{chan2022_efficient-geom-aware,wang2021_learning-crf}. Generalized models often lack personalized details. However, they have proven themselves to be robust priors for downstream tasks, such as landmark detection~\cite{zhang2022_flnerf}, personalized face reenactment~\cite{bai2022_high-fidelity} and 3D face modeling~\cite{abdal2023_3davatargan}. 

We remark that NeRFs have stood out as superior implicit representations for head avatar creation as they excel at reconstructing complex scene structures. Some recent prior-free NeRF-based methods focus on generating detailed avatars from very sparse 2D imagery, e.g., using local pixel-aligned encodings~\cite{raj21_pixel-aligned, mihajlovic22_keypointnerf}, while others model dynamic deformations when working with unstructured 2D videos by warping observed points into a canonical frame configuration~\cite{park2021_nerfies,park2021_hypernerf} or modeling  time-dependent latent codes~\cite{li2021_neural-scene,li2022_neural-3d-video}. We remark that dynamic approaches, while achieving impressive results, are designed to memorize the scene representations and cannot control the model beyond interpolations.
In addition, some approaches build upon dynamic NeRF approaches by incorporating parametric models, e.g., 3DMMs~\cite{egger2020_3dmfm,li2017_flame}, as input priors to enable full facial control~\cite{hong2021_headnerf,sun2022_controllable}.

