\section{Introduction} \label{sec:introduction}

The human face is at the center of our visual communications, and hence  its digitization is of utmost importance. Learning a high-quality controllable 3D digital head is a long-standing research problem with several applications in VR/AR, VFX, and media production, among others. Solutions to this task progressed significantly over the past few years, including early works that create a static textured face model from a monocular RGB camera~\cite{thies2016face}, all the way to recent multi-view methods that learn a highly photorealistic model, which can be rendered from an arbitrary camera viewpoint~\cite{lombardi21_mvp}.

Early methods for facial avatar creation are based on explicit scene representations, such as meshes~\cite{zollhofer18_sota-mon-3d,kim2018_dvp,thies2019_dnr}. While these methods produce photorealistic results, they cannot guarantee 3D-consistent reconstructions.
Recently, implicit scene representations have significantly attracted the attention of the research community~\cite{tewari2022_advances-neural-render}. Implicit representations assume the examined signal is continuous. Here, it was shown that, by using neural networks e.g. MLPs, high scene granularities can be reconstructed at high fidelity. Furthermore, implicit scene representations such as Neural Radiance Fields (NeRFs)~\cite{mildenhall2022_nerf} can be learned from multiple 2D images and produce multi-view consistent renderings.
These features make implicit representations suitable for the general task of 3D scene reconstruction and rendering, including human face digitization. 

Neural implicit representations~\cite{mildenhall2022_nerf,park2019_deepsdf} and, in particular, NeRF have been used for face digitization due to its high level of photorealism~\cite{gafni2021_dynamic-nerf,zheng2022imavatar,athar2022rignerf}. Here, one of the main challenges is how to model complex facial motions. Faces are dynamic objects and are often influenced by the activation of facial expressions and head poses. An early adaptation of NeRFs, applied to the human face, represents such motion by simply conditioning the implicit function, represented as an MLP, on 3DMM parameters~\cite{gafni2021_dynamic-nerf}. While this produces interesting results, it has a few limitations, primarily the inability of such 3DMMs to reconstruct high-frequency skin deformations and model the mouth interior. 
%
In follow-up methods, a common approach is to model motion by learning a canonical space via template-based deformation supervision~\cite{zheng2022imavatar,athar2022rignerf}. However, this kind of supervision limits the ability of these methods to accurately model regions not represented by the underlying parametric model e.g. the mouth interior.


Mixture of Volumetric Primitives (MVPs)~\cite{lombardi21_mvp} combines the advantage of mesh-based approaches with a voxel-based volumetric representation that allows for efficient rendering. Specifically, it utilizes a template-based mesh tracker to initialize voxels and prune empty spaces. Here, a primitive motion decoder modifies the initialized positions of the primitives. 
%
This method produces state-of-the-art results with the highest level of photorealism, mainly due to its hybrid voxel-NeRF representation as well as its capability to train on multi-view video data. 
%
However, finding the optimal orientation of the primitives solely based on a photometric reconstruction loss is highly challenging. As a result, this method produces inaccurate reconstructions and artifacts in regions exhibiting fine-scale details such as the hair. 
%
%
It is also expensive to train, requiring around 2.5 days when trained on an NVIDIA A40 GPU.  


In this paper, we present a novel approach for producing high-quality facial avatars at state-of-the-art level of photorealism. Our approach uses a voxelized feature grid and leverages multiresolution hash encoding. It is trained using a multi-view video camera setup and, at test time, drives the avatar via a monocular RGB camera. Unlike related methods~\cite{lombardi21_mvp,gao2022_nerfblendshape}, our approach does not require a template to aid in modeling scene dynamics or pruning of empty space. Instead, we learn a fully implicit canonical space that is conditioned on features extracted from the driving monocular video. We regularize the canonical space using a novel optical flow based loss that encourages artifact-free reconstructions. Our model can be rendered under novel camera viewpoints and facial expressions during inference (see Fig.~\ref{fig:teaser}, left). It produces highly photorealistic results and outperforms state-of-the-art approaches~\cite{lombardi21_mvp,gao2022_nerfblendshape,park2021_hypernerf}, even on challenging regions such as the scalp hair. 
Our contributions are summarized as follows:
\begin{itemize}
\item We present a method that leverages a multiresolution hash table to generate volumetric head avatars with state-of-the-art photorealism. The avatar is trained using multi-view data and is driven by a monocular video sequence at test time. The core of our method is a canonical space conditioned on features extracted from the driving video. 
\item We propose a novel optical flow based loss to enforce temporal coherent correspondences in the learnable canonical space, thus encouraging artifact-free reconstructions.

\item Our model training time is 4-5 times faster than the state of the art~\cite{lombardi21_mvp}. We show a result with 2K resolution for the first time in literature. We also show a setting for rendering our results in real time (see Fig.~\ref{fig:teaser}, bottom right). 

\item We have collected a novel dataset of 16 identities performing a variety of expressions. The identities are captured using a multi-view video camera setup with 24 cameras. Our multi-view video face dataset is the first captured at 4K resolution, and we will release it to encourage further research. 
\item We show that the high level of photorealism of our model can even generate synthetic training data at high fidelity, opening the door to generalizing the image encoder to arbitrary input views for driving the avatar. 
\end{itemize}
We evaluate our approach visually and numerically against ground truth data. Here, we ablate our method with different design choices to illustrate their importance in the overall performance. Our approach outperforms existing methods~\cite{lombardi21_mvp,gao2022_nerfblendshape,park2021_hypernerf} visually and numerically, including a multi-view implementation of~\cite{,gao2022_nerfblendshape,park2021_hypernerf}. 

