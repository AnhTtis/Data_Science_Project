\begin{figure*}[htb]
\centering
\includegraphics[width=\textwidth]{figures/method_pipeline.pdf}
\caption{Left: To extract a robust encoding that parameterizes the dynamics of the head, we pass a driving image through a CNN encoder to obtain a low dimensional vector \textit{e}. A deformation network $A_\theta$ conditioned on \textit{e} deforms the input coordinates $\gamma{(x)}$, where $\gamma(.)$ denotes positional encoding. 
We then use multiresolution hash encoder $A_\alpha$ to encode the deformed points in the canonical space, and feed the features from the hash grid, and encoding \textit{e} as input to a radiance field network $A_\beta$, which outputs density and color values. By combining these values through volume rendering, we are able to render the avatar under unseen input and camera viewpoints. Right: We impose a novel scene flow based constraint by utilizing the optical flow at frame $t$ and $t+1$ (see Eq.~\ref{eq:flow}). Such constraints enforce good correspondences in the canonical space, thus reducing rendering artifacts.} 

\label{fig:method}
\end{figure*}