% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  10pt,
  a4paper,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage[]{tgpagella}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=2.25cm]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{amsmath,amssymb,mathtools,bbm}
% I always seem to need tikz for something
\usepackage{tikz}
\usetikzlibrary{positioning, shapes, intersections, through, backgrounds, fit, decorations.pathmorphing}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{setspace}
\onehalfspacing

\usepackage{lineno}
% \linenumbers

% required for landscape pages. beware, they make build times very long.
\usepackage{pdflscape}
\usepackage{placeins}

% table - `gt' package uses these, often unimportant
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{colortbl}

\usepackage{color}
\definecolor{myredhighlight}{RGB}{180, 15, 32}
\definecolor{mydarkblue}{RGB}{0, 33, 79}
\definecolor{mymidblue}{RGB}{44, 127, 184}
\definecolor{mylightblue}{RGB}{166, 233, 255}
\definecolor{mywhwlow}{RGB}{234, 164, 99}

\usepackage{accents}
\newlength{\dhatheight}
\newcommand{\doublehat}[1]{%
    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
    \addtolength{\dhatheight}{-0.35ex}%
    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
    \smash{\hat{#1}}}}

\setcounter{secnumdepth}{3}

% \renewcommand{\floatpagefraction}{0.8}
% \renewcommand{\topfraction}{0.8}
% \renewcommand{\bottomfraction}{0.8}
% \renewcommand{\textfraction}{0.25}

% pd stands for: probability distribution and is useful to distringuish
% marignals for probabilities specifically p(p_{1}) and the like.
\newcommand{\pd}{\text{p}}
\newcommand{\Pd}{\text{P}}
\newcommand{\q}{\text{q}}
\newcommand{\Q}{\text{Q}}
\newcommand{\w}{\text{w}}
\newcommand{\pdr}{\text{r}}
\newcommand{\pdrh}{\hat{\text{r}}}

% pbbo stuff
\newcommand{\tc}{\text{T}}
\newcommand{\tp}{\text{t}}

% melding
\newcommand{\ppoolphi}{\pd_{\text{pool}}(\phi)}
\newcommand{\ppool}{\pd_{\text{pool}}}
\newcommand{\pmeld}{\pd_{\text{meld}}}

% the q(x)w(x), "weighted target" density 
% for the moment I'm going to call it s(x), as that is the next letter of the 
% alphabet. Can change it later
\newcommand{\s}{\text{s}}
% direct density estimate - replaces lambda.
\newcommand{\ddest}{\text{s}}
% target weighting function
\newcommand{\tarw}{\text{u}}

% constants - usually sizes of things
\newcommand{\Nx}{N}
\newcommand{\Nnu}{\text{N}_{\text{nu}}}
\newcommand{\Nde}{\text{N}_{\text{de}}}
\newcommand{\Nmc}{\text{N}_{\text{mc}}}
\newcommand{\Nw}{W}
\newcommand{\Nm}{M}
\newcommand{\Ns}{S}

% locales - could switch to x and x'
\newcommand{\xnu}{x_{\text{nu}}}
\newcommand{\xde}{x_{\text{de}}}
\newcommand{\phinu}{\phi_{\text{nu}}}
\newcommand{\phide}{\phi_{\text{de}}}

% sugiyama stuff
\newcommand{\pdnu}{\pd_{\text{nu}}}
\newcommand{\pdde}{\pd_{\text{de}}}

% indices 
\newcommand{\wfindex}{w}
\newcommand{\sampleindex}{n}
\newcommand{\modelindex}{m}
\newcommand{\stageindex}{s}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\newcommand{\kldiv}{D_{\text{KL}}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\lse}{\text{logSumExp}}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\alglinenumber[1]{
    {\sf\footnotesize\color{lightgray}#1}}
\algrenewcommand\algorithmicrequire{\textbf{Inputs:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}

\def\dodoi#1{doi: \href{https://doi.org/#1}{\nolinkurl{#1}}}
\def\dourl#1{\href{http://#1}{\nolinkurl{#1}}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{jasaauthyear}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Translating predictive distributions into informative priors},
  pdfauthor={Andrew A. Manderson; Robert J. B. Goudie},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Translating predictive distributions into informative priors}
\author{Andrew A. Manderson\footnote{MRC Biostatistics Unit, the
  University of Cambridge and The Alan Turing Institute.
  \texttt{andrew.manderson@mrc-bsu.cam.ac.uk}} \and Robert J. B.
Goudie\footnote{MRC Biostatistics Unit, the University of Cambridge.
  \texttt{robert.goudie@mrc-bsu.cam.ac.uk}}}
\date{June, 2023}

\begin{document}
\maketitle
\begin{abstract}
When complex Bayesian models exhibit implausible behaviour, one solution
is to assemble available information into an informative prior.
Challenges arise as prior information is often only available for the
observable quantity, or some model-derived marginal quantity, rather
than directly pertaining to the natural parameters in our model. We
propose a method for translating available prior information, in the
form of an elicited distribution for the observable or model-derived
marginal quantity, into an informative joint prior. Our approach
proceeds given a parametric class of prior distributions with as yet
undetermined hyperparameters, and minimises the difference between the
supplied elicited distribution and corresponding prior predictive
distribution. We employ a global, multi-stage Bayesian optimisation
procedure to locate optimal values for the hyperparameters. Three
examples illustrate our approach: a cure-fraction survival model, where
censoring implies that the observable quantity is \emph{a priori} a
mixed discrete/continuous quantity; a setting in which prior information
pertains to \(R^{2}\) -- a model-derived quantity; and a nonlinear
regression model.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

A key asset to the Bayesian paradigm is the conceptual ease with which
prior information is incorporated into models. For complex, nonlinear,
overparameterised, or otherwise partially identified
\citep{gustafson_bayesian_2015} models, including such prior information
is essential to exclude model behaviours that conflict with reality,
and/or known qualities of the phenomena being modelled. Prior
information can also improve computation of estimates of the posterior,
making otherwise unusable models suitable for inference. However, it is
for precisely the models for which prior information is so important
that setting appropriate priors for parameters is hardest.

We consider in this paper the task of forming such appropriate
informative priors. We distinguish two tasks: predictive elicitation;
and the subsequent translation into a prior for a given model.
Predictive elicitation is an approach in which elicitation
\citep{ohagan_uncertain_2006, falconer_methods_2021, low_choy_priors_2012}
proceeds via predictive distributions, often for observable quantities,
and is thought to be the most reliable and available form of information
\citep{kadane_experiences_1998}. Predictive elicitation is also
model-agnostic, meaning the complex, time-consuming process of
elicitation does not need to be repeated for each variant of a model. A
recent, comprehensive review of elicitation is undertaken by
\citet{mikkola_prior_2021}.

Translation is the process of using information from predictive
elicitation to set the corresponding informative prior for a model.
Translation, as a distinct step in the prior specification process, has
received less attention than elicitation. A simple predictive approach
is to directly model the elicited, observable quantity. This direct
approach requires no translation. For example, the Bayesian
quantile-parameterised likelihood
\citep{hadlock_quantile-parameterized_2017, keelin_quantile-parameterized_2011}
approach of \citet{perepolkin_hybrid_2021} involves updating directly
elicited information in light of observations. Such direct approaches
are currently only feasible for simple models with no latent structure.
For models with simple latent structure, it is sometimes possible to
elicit information about an invertible function of the parameters
\citep[e.g.][]{chaloner_graphical_1993}. In these cases it is possible
to analytically translate the elicited information into a prior for the
parameters. Translation is also clear for conjugate models
\citep{percy_bayesian_2002}, as if we specify the target prior
predictive distribution using the conjugate distribution, then the prior
predictive distribution determines the values for the hyperparameters of
the prior \citep[related to the idea of ``reverse Bayes''
in][]{held_reverse-bayes_2022}. Translation, however, is unclear in
general for nonconjugate models \citep{gribok_backward_2004}, although
techniques for specific models with specific latent structure are
numerous, and include linear regression \citep{ibrahim_properties_1997},
logistic regression \citep{bedrick_bayesian_1997, chen_prior_1999}, Cox
models \citep{chaloner_graphical_1993}, contingency table analyses
\citep{good_bayesian_1967}, hierarchical models \citep[noting that the
space of possible hierarchical models is vast]{hem_robustifying_2021},
and autoregressive time-series models \citep{jarocinski_priors_2019}.
Nevertheless, a model-agnostic approach with a corresponding generic
implementation would be preferable, as noted by
\citet{gelman_prior_2017} and \citet{mikkola_prior_2021}.

Our approach to translation builds on the idea of predictive checks
\citetext{\citealp{gabry_visualization_2019}; \citealp{gelman_prior_2017}; \citealp{box_sampling_1980}; \citealp[the
``hypothetical future samples''
of][]{winkler_assessment_1967}; \citealp{van_zundert_prior_2022}}, which
are an often recommended
\citep{gelman_bayesian_2020, van_zundert_prior_2022} tool in assessing
the concordance of the prior predictive distribution and the elicited
predictive information or elicited data distribution. If concordance
between these two distributions is low, then the Bayesian workflow
\citep{gelman_bayesian_2020} proceeds by adjusting the prior to better
match the prior predictive distribution to the elicited information.
However, for many classes of models, manually adjusting the prior in
this manner is infeasible; the complexity required to describe the
phenomena of interest muddies the relationship between the prior and the
distribution of the observables, and so a more automated method is
required. \citet{wang_using_2018} and \citet{thomas_probabilistic_2020}
have proposed approaches in which either regions of observable space or
specific realisations are labelled as plausible or implausible by
experts, and then an suitable prior formed that accounts for this
information, via history matching and a ``human in the loop'' processes
driven by a Gaussian process model respectively.
\citet{albert_combining_2012} propose a supra-Bayesian approach intended
for multiple experts, in which a Bayesian model is formed for quantiles
or probabilities elicited from the experts. Another approach, and the
closest in motivation and methodology to ours, is
\citeauthor{hartmann_flexible_2020}
\citetext{\citeyear{hartmann_flexible_2020}; \citealp[which is partly
inspired by][]{da_silva_prior_2019}}, which employs a Dirichlet
likelihood for elicited predictive quantiles to handle both elicitation
and translation.

In this paper we develop a method, and software package, for
translation: that is, for constructing an informative prior distribution
for model parameters that results in a desired \emph{target} prior
predictive distribution. Using as the starting point for our method a
target prior predictive distribution means that our approach is
applicable regardless of how the target is elicited, and means that
uncertainty is directly and intuitively represented as a distribution.
We define a suitable loss function between the prior predictive
distribution, given specific values for the hyperparameters of the
prior, and this target distribution. The loss function is intentionally
generic and permits data that are discrete, continuous, or a mixture
thereof. We minimise this loss via a generic, simulation-based, global
optimisation process to locate optimal hyperparameters. We highlight
throughout that solutions to this optimisation problem are rarely
unique, so to regularise the problem we adopt a multiple objective
approach. The global optimisation approach is also selected with
generality in mind, rendering our method applicable to models where
derivative information is unavailable. We make our method available in
an \texttt{R} package \texttt{pbbo}\footnote{The release used in this
  paper is available at \url{https://doi.org/10.5281/zenodo.7736707}.}
\citep{r_core_team_r_2022}. Our method is illustrated in three
challenging, moderate dimension problems; a nonlinear regression model,
a model using predictive information on a model-derived quantity -- a
situation not explicitly covered by prior predictive elicitation; and a
cure fraction survival model.

\hypertarget{translating-elicited-prior-predictive-distributions}{%
\section{Translating elicited prior predictive
distributions}\label{translating-elicited-prior-predictive-distributions}}

In this section we propose a set of desirable properties for any
translation method, and our mathematical framework and optimisation
strategy that together constitute our methodology. We aim to minimise
the difference between the prior predictive distribution and our
elicited target distribution, whilst also promoting the marginal
variance of the model's parameters. Satisfying the first of these goals
produces priors faithful to the supplied information; the second
promotes uniqueness and replicability -- we elaborate on the precise
meaning of these properties momentarily. We adopt a multi-objective
optimisation approach to locate priors satisfying these requirements.

\hypertarget{desiderata}{%
\subsection{Desiderata}\label{desiderata}}

We postulate three key properties that we would like a translation
method to satisfy: faithfulness, uniqueness, and replicability.

\emph{Faithfulness}~~ We consider a prior faithful if it accurately
encodes the target data distribution provided by the elicitation
subject. Faithfulness is a property of both the model, as it must be
possible for the model to represent the information, and the procedure
employed to obtain the prior. Especially with simple models and prior
structures, not all target prior predictive distributions can be
encoded.

\emph{Uniqueness}~~ In a complex model there may be many prior
distributions that imply the same prior predictive distribution. These
prior distributions will all be equally faithful. Should uniqueness be
desired -- and it seems a reasonable enough desiderata most of the time
-- we must distinguish between priors based on other properties. In
Section
\ref{regularising-estimates-of-lambda-by-promoting-the-marginal-standard-deviation-secondary-objective}
we propose to distinguish priors based on their marginal standard
deviations, but other properties could be easily incorporated into our
approach.

\emph{Replicability}~~ We call a procedure/method consistent if it
obtains the same, or very similar, prior across independent
replications, given the same target. This property is particularly
important to assess for methods, like ours, that make use of
simulation-based or otherwise stochastic estimates. Global,
gradient-free optimisers, when applied to noisy loss functions, offer no
guarantee of finding the global minimum in finite time
\citep{liberti_introduction_2008, mullen_continuous_2014}. We assess
replicability empirically in all our examples.

These properties are partly inspired by other works in the prior
elicitation and specification literature. Faithfulness is closely
related to \citet{johnson_methods_2010}'s definition of \emph{validity}
\citep[see also][]{johnson_valid_2010} and
\citet{ohagan_uncertain_2006}'s use of \emph{faithful} in Chapter 8 (and
throughout the book). However, their concerns are specific to the
elicitation process -- do the quantities elicited represent what the
experts believe? -- and not to the subsequent translational step. Our
conception of uniqueness and the need for regularisation is noted by
\citet{da_silva_prior_2019} and \citet{stefan_practical_2022}, and is
similar to the notion of model \emph{sloppiness} of
\citet{gutenkunst_universally_2007}.

We have introduced our desiderata in what we believe to be their order
of importance. Firstly, without faithfulness the procedure has not
achieved the main aim of translating our knowledge into the prior
distribution. Subsequently, given a suite of faithful priors,
regularising the problem until it is unique allows us to select one in a
clear and replicable way. Such uniqueness inducing regularisation
schemes often improve a procedure's replicability. Replicability
ultimately also relies on the empirical behaviour of the procedure when
applied to the model of interest. We note that the desiderata are not
binary, and at times we may wish to sacrifice some amount of the latter
properties for improved faithfulness. We also envisage settings where
sacrificing some model flexibility, and thus faithfulness, for a marked
increase in replicability increases the usefulness or persuasiveness of
a model.

\hypertarget{the-target-predictive-distribution-tcy}{%
\subsection{\texorpdfstring{The target predictive distribution
\(\tc(Y)\)}{The target predictive distribution \textbackslash tc(Y)}}\label{the-target-predictive-distribution-tcy}}

Our methodology assumes a target predictive distribution (a cumulative
distribution function, CDF) for the observable quantity, \(\tc(Y)\), has
been chosen. The target distribution can be chosen in any manner, but we
recommend using predictive elicitation \citep{kadane_experiences_1998}.
In brief, such an elicitation proceeds by querying experts about the
observable quantity at a small number quantiles, then fitting an
appropriate parametric distribution to the elicited values \citep[see
Chapter 6 of][]{ohagan_uncertain_2006}. We assume a (mixture of)
standard distributions can describe the target predictive distribution
function \(\tc(Y)\), and that we can draw samples from this
distribution.

We often wish to elicit information about the observable quantity \(Y\)
conditional on some known values of a covariate. For example, when using
the linear model \(Y = X\beta + \varepsilon\) we may elicit information
about \(Y\) at a fixed set of values for \(X\). Further suppose \(X\) is
an experimental design specified before collecting observations of
\(Y\), or comprises observational covariates whose values are known
prior to model construction. In such settings we can elicit
\(r = 1, \ldots, R\) conditional target distributions
\(\tc(Y \mid X_{r})\).

We elect to describe our methodology in this covariate-specific setting,
as it readily reduces to the covariate-independent case.

\hypertarget{total-predictive-discrepancy-primary-objective}{%
\subsection{Total predictive discrepancy (primary
objective)}\label{total-predictive-discrepancy-primary-objective}}

Consider a joint model for observables
\(Y \in \mathcal{Y} \subseteq{} \mathbb{R}\) and parameters
\(\theta \in \Theta \subseteq \mathbb{R}^{Q}\), given hyperparameters
\(\lambda \in \Lambda \subset \mathbb{R}^{L}\) and covariates
\(X \in \mathcal{X} \subseteq \mathbb{R}^{C}\). This joint model has CDF
\(\Pd(Y, \theta \mid \lambda, X)\) and prior predictive CDF
\(\Pd(Y \mid \lambda, X)\) for \(Y\). Choosing the CDF as the basis for
our methodology enables us to be agnostic to whether the observable is
continuous, discrete, or a mixture thereof. We will use
\emph{distribution} to refer to the CDF of a stochastic quantity, and
\emph{density} to refer to the corresponding probability density
function (where it exists).

Further suppose the target CDF \(\tc(Y \mid X_{r})\) has been elicited
at \(R\) values of the covariate vector denoted
\(\{X_{r}\}_{r = 1}^{R}\), which we stack in the covariate matrix
\(\boldsymbol{X} = \left[X_{1}^{\top} \cdots X_{R}^{\top}\right] \in \boldsymbol{\mathcal{X}} \subseteq \mathbb{R}^{R \times C}\).
We assume that each target \(\tc(Y \mid X_{r})\) has identical support
to \(\Pd(Y \mid \lambda, X_{r})\). Lastly, it will be convenient to
denote
\(\tc(Y \mid \boldsymbol{X}) = \prod_{r = 1}^{R} \tc(Y \mid X_{r})\),
with \(\Pd(Y \mid \lambda, \boldsymbol{X})\) and
\(\Pd(\theta \mid \lambda, \boldsymbol{X})\) defined analogously.

We quantify the difference between the prior predictive and target by
the \emph{covariate-specific predictive discrepancy}, which we define to
be
\input{tex-input/pbbo-methodology/0012-theoretical-discrep-definition-covariate.tex}\noindent
for some discrepancy function \(d(\cdot, \cdot)\). The Riemann-Stieltjes
integral in Equation~\ref{eqn:theoretical-discrep-definition-covariate}
means this definition applies when \(\mathcal{Y}\) is continuous,
discrete, or a mixture thereof. Minimising Equation
\eqref{eqn:theoretical-discrep-definition-covariate} admits the optimal
hyperparameter
\(\lambda^{*} = \min_{\lambda \in \Lambda} \tilde{D}(\lambda \mid \boldsymbol{X})\).
The covariate-independent equivalent \(\tilde{D}(\lambda)\) is obtained
by setting \(R = 1\) and ignoring all conditioning on \(X_{r}\) in
Equation \eqref{eqn:theoretical-discrep-definition-covariate}.

The discrepancy function \(d(\cdot, \cdot)\) takes two CDFs as its
arguments. Inspired by the CramÃ©r-von Mises
\citep{von_mises_asymptotic_1947} and Anderson-Darling
\citep{anderson_asymptotic_1952} distributional tests we define, for
arbitrary CDFs \(\text{M}(Y)\) and \(\Pd(Y)\), two options for the
discrepancy function,
\input{tex-input/pbbo-methodology/0013-discrepancies-definitions.tex}\noindent
Both discrepancies are proper scoring rules
\citep{gneiting_strictly_2007} as they are minimised iff
\(\text{M}(Y) = \Pd(Y)\) for all \(Y \in \mathcal{Y}\). Supposing
\(\Pd(Y \mid \lambda, X_{r})\) is flexible enough to exactly match
\(\tc(Y \mid X_{r})\) for some \(\lambda^{*}\), then both discrepancies
will yield the same \(\lambda^{*}\). Differences arise when
\(\Pd(Y \mid \lambda, X_{r})\) is insufficiently flexible, in which case
Anderson-Darling discrepancy \(d^{\text{AD}}\) places more emphasis on
matching the tails of two CDFs under consideration. Furthermore, we will
have to resort to a finite-sample approximation to Equation
\eqref{eqn:theoretical-discrep-definition-covariate} (which we detail
momentarily), and the Anderson-Darling discrepancy is more challenging
to accurately compute.

\hypertarget{regularising-estimates-of-lambda-by-promoting-the-marginal-standard-deviation-secondary-objective}{%
\subsection{\texorpdfstring{Regularising estimates of \(\lambda^{*}\) by
promoting the marginal standard deviation (secondary
objective)}{Regularising estimates of \textbackslash lambda\^{}\{*\} by promoting the marginal standard deviation (secondary objective)}}\label{regularising-estimates-of-lambda-by-promoting-the-marginal-standard-deviation-secondary-objective}}

The optimisation problem of minimising Equation
\eqref{eqn:theoretical-discrep-definition-covariate} is often
underspecified. Specifically, there are many optimal values
\(\lambda^{*}\) that yield values of
\(\tilde{D}(\lambda^{*} \mid \boldsymbol{X})\) that are practically
indistinguishable \citep[noted by][]{da_silva_prior_2019} but with
immensly differing prior distributions
\(\Pd(\theta \mid \lambda^{*}, \boldsymbol{X})\) and corresponding
marginals for components of \(\theta\). In terms of our desiderata,
there are many equally faithful priors (which immediately implies a lack
of uniqueness), thus we have an optimisation problem with solutions that
are difficult to replicate due to nonuniqueness. This is not surprising
because we are providing information only on \(Y\), which is typically
of lower dimension than \(\theta\). A particularly challenging case for
uniqueness is in models with additive noise forms (such as Equation
\eqref{eqn:preece-baines-model-definition-one}). In this case it is
challenging to avoid attributing all the variability to the noise term
if the prior for the noise is to be specified; in this case it will
generally be necessary to fix a prior for the variance using knowledge
of the measurement process.

To handle more general cases of lack of uniqueness, we seek to encode
the following principle into our methodology: given two estimates of
\(\lambda^{*}\) which have equivalent values of
\(\tilde{D}(\lambda^{*} \mid \boldsymbol{X})\), we prefer the one with
the larger variance for
\(\Pd(\theta \mid \lambda^{*}, \boldsymbol{X})\). This preference
induces less of a restriction on the possible values of \(\theta\) in
the posterior.

We make use of this principle by adopting a multi-objective approach to
prior construction and, therefore, now derive a suitable mathematical
quantity measuring the variability of \(\theta\). There are numerous
suitable functions measuring such variability, and our methodology is
agnostic to the particular functional form. Most generally, we define
the secondary objective \(\tilde{N}(\lambda \mid \boldsymbol{X})\) as
comprising any such suitable function \(n(\theta)\) with
\input{tex-input/pbbo-methodology/0040-generic-secondary-objective.tex}\noindent
In this paper we consider only one form for \(n(\theta)\), and so
hereafter the second objective, which we also seek to minimise, is
always
\input{tex-input/pbbo-methodology/0041-second-objective-def.tex}\noindent
where \(\text{SD}_{\Pd(Z)}[Z]\) is the standard deviation of \(Z\) under
distribution \(\Pd(Z)\). This quantity is the mean of the marginal log
standard deviations of each of the \(Q\) components of
\(\theta \in \Theta \subseteq \mathbb{R}^{Q}\), which we negate so as to
promote marginal variability when performing minimisation. We work with
the standard deviation (instead of the variance) and take the logarithm
thereof to minimise the contribution of any particularly extreme
marginal (i.e.~marginal with relatively low or high variance). Equations
\eqref{eqn:generic-secondary-objective} and
\eqref{eqn:second-objective-def} make explicit the dependence on
\(\Pd(\theta \mid \lambda, \boldsymbol{X})\), and thus \(\lambda\), for
clarity. We often have analytic expressions for
\(\text{SD}_{\Pd(\theta \mid \lambda, \boldsymbol{X})}[\theta_{q}]\),
but precise estimates are also simple to obtain using Monte Carlo. Note
that Equation \eqref{eqn:second-objective-def} assumes
\(\text{SD}_{\,\Pd(\theta_{q} \mid \lambda, \boldsymbol{X})}[\theta_{q}]\)
exists, and is nonzero and finite for all \(q\) and
\(\lambda \in \Lambda\). Should this not be true, for example if one of
the marginals of \(\Pd(\theta_{q} \mid \lambda, \boldsymbol{X})\) is a
Cauchy distribution, we can instead employ alternative, robust
estimators of scale
\citep{kravchuk_hodges-lehmann_2012, rousseeuw_alternatives_1993}.

\hypertarget{post-optimisation-decision-step}{%
\subsection{Post optimisation decision
step}\label{post-optimisation-decision-step}}

We jointly minimise Equations
\eqref{eqn:theoretical-discrep-definition-covariate} and
\eqref{eqn:second-objective-def} using a multi-objective optimisation
algorithm, which we will cover in Section \ref{optimisation-strategy}.
By adopting a multiple objective approach to the translation problem, we
obtain a set of possible \(\lambda\) values which comprise the Pareto
frontier
\(\mathcal{P} = \{\lambda_{l}\}_{l = 1}^{\lvert \mathcal{P} \rvert}\),
the set of all ``non-dominated'' choices for \(\lambda\), meaning that
no point in \(\mathcal{P}\) is preferable in \emph{both} objectives to
any of the remaining points in \(\mathcal{P}\) \citep[for an
introduction to multi-objective optimisation problems see Chapter 2
of][]{deb_multi-objective_2001}. For each \(\lambda\) in \(\mathcal{P}\)
we compute the loss
\input{tex-input/pbbo-methodology/0042-loss-definition.tex}\noindent
where the value of \(\kappa > 0\) expresses our relative belief in the
importance of the secondary objective. We take the log of
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) to be on a similar scale to
our definition of \(\tilde{N}(\lambda \mid \boldsymbol{X})\) in Equation
\eqref{eqn:second-objective-def}, but stress that this not necessary
should there be a more appropriate scale on which to define
\(\tilde{L}(\lambda)\). The optimal value is then chosen such that
\(\lambda^{*} := \min\limits_{\lambda \in \mathcal{P}} \tilde{L}(\lambda)\).

This optimum is clearly sensitive to the choice of \(\kappa\). If the
relative belief between the two objectives were known, \(\kappa\) could
be set directly, but this will not usually be straightforward to assess.
However, the multi-objective optimisation approach makes it
computationally inexpensive to test many values of \(\kappa\) (the set
of which we denote with \(\mathcal{K}\)), and thus plot the Pareto
frontier coloured by loss, with the minimum loss point indicated. These
can guide our choice of \(\kappa\): we can seek a value of \(\kappa\)
with the minimum loss point not on the extreme of the Pareto frontier,
since we would like to balance the two objectives. This approach is
particularly useful in settings where the scales of the two optima
differ markedly, which we further discuss in Appendix
\ref{further-notes-on-choosing-kappa}. Where it is feasible to replicate
the optimisation procedure, we can additionally seek a choice of
\(\kappa\) that leads to Pareto frontiers with minimal variability
across replicates, since this suggests the optimal solution can be
estimated reliably.

\hypertarget{optimisation-strategy}{%
\subsection{Optimisation strategy}\label{optimisation-strategy}}

With the mathematical framework defined, we now turn to discuss the many
practicalities of optimisation. We use a two-stage global optimisation
process to construct a prior with the desiderata listed in Section
\ref{desiderata}. The first stage focuses entirely on faithfulness by
minimising only \(\tilde{D}(\lambda \mid \boldsymbol{X})\), using the
variant of controlled random search 2 \citep[CRS2,][]{price_global_1983}
proposed by \citet{kaelo_variants_2006}. Stage one output is then used
to initialise stage two, which additionally focuses on uniqueness and
replicability by employing multi-objective Bayesian optimisation
\citep{frazier_tutorial_2018, zaefferer_mspot_2012} to jointly minimise
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) and
\(\tilde{N}(\lambda \mid \boldsymbol{X})\). We focus on faithfulness,
and thus \(D(\lambda \mid \boldsymbol{X})\), as a separate stage because
minimising \(D(\lambda \mid \boldsymbol{X})\) is considerably more
challenging than minimising \(N(\lambda \mid \boldsymbol{X})\). By
initialising stage two with faithful estimates for \(\lambda\) we can,
in the second stage, spend more computational resources on finding
points along the Pareto frontier. The resulting optimal prior should
suitably encode the information in the target predictive distribution,
without being overly confident for model parameters. An idealised form
of this process is illustrated in Figure \ref{fig:idealised_process}.

Note that almost all global optimisation methods require, in the absence
of other constraints, \(\Lambda\) to be compact subset of
\(\mathbb{R}^{L}\). We require that the practitioner specify the
upper/lower limits for each dimension of \(\Lambda\).

\begin{figure}

{\centering \includegraphics{plots/tuning-parameters/idealised-process} 

}

\caption{An idealised depiction of the methodology we introduce in this paper when the corresponding densities are available. The starting point, or "stage zero" (S0) depicts the elicited target prior predictive distribution for the observable quantity $Y$ depicted and denoted by its density $\tp(Y)$ (red line). Stage one starts (S1 -- start) from an initial prior (blue line), which does not match the target and is uninformative for the model parameter $\theta$. Optimisation proceeds by using controlled random search to minimise the predictive discrepancy (grey shaded area) and produces optimal hyperparameters $\lambda^{*}$. Stage one produces (S1 -- end) a very faithful prior predictive distribution, but is overly confident for the model parameter. Stage two (S2) uses multi-objective Bayesian optimisation and corrects this overconfidence for $\theta$ with only a small increase in predictive discrepancy.}\label{fig:idealised_process}
\end{figure}

\hypertarget{evaluating-the-objectives}{%
\subsubsection{Evaluating the
objectives}\label{evaluating-the-objectives}}

As noted previously, evaluating
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) for models where analytic
results or simple Monte Carlo estimates are available is
straightforward, and we denote the corresponding estimate (or, if
available, the analytic form) of
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) with
\(N(\lambda \mid \boldsymbol{X})\). However, there are two immediate
challenges to evaluating \(\tilde{D}(\lambda \mid \boldsymbol{X})\):

\begin{enumerate}
  \tightlist
  \item the prior predictive CDF $\Pd(Y \mid \lambda, \boldsymbol{X})$ is often analytically unavailable;
  \item the integral in Equation \eqref{eqn:theoretical-discrep-definition-covariate} is almost always intractable.
\end{enumerate}

We address the former with a Monte Carlo based empirical CDF (ECDF), and
the latter with importance sampling. Specifically, given a particular
value of \(\lambda\) and \(X_{r}\), we draw \(S_{r}\) samples
\(\boldsymbol{y}_{r}^{(\Pd)} = (y_{s, r})_{s = 1}^{S_{r}}\) with
\(\boldsymbol{y}_{r}^{(\Pd)} \sim \Pd(Y \mid \lambda, X_{r})\) to form
the ECDF
\(\hat{\Pd}(Y \mid \lambda, X_{r}, \boldsymbol{y}_{r}^{(\Pd)})\). To
apply importance sampling we rewrite the integral in Equation
\eqref{eqn:theoretical-discrep-definition-covariate} with respect to
importance distribution \(\Q(Y \mid X_{r})\) and Radon-Nikodym
derivative
\(\text{d}\tc(Y \mid X_{r}) \mathop{/} \text{d}\Q(Y \mid X_{r})\), such
that
\input{tex-input/pbbo-methodology/0014-theoretical-discrep-definition-covariate-importance.tex}\noindent
When \(Y\) is discrete or of mixed type, \(\q(Y \mid X_{r})\) is instead
a probability mass function or an appropriate mixture of discrete and
continuous densities. Supposing we draw \(I_{r}\) importance samples
\((y_{i, r})_{i = 1}^{I_{r}} \sim \Q(Y \mid X_{r})\), we denote the
importance sampling approximation to
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) with
\(D(\lambda \mid \boldsymbol{X})\) such that
\input{tex-input/pbbo-methodology/0015-practical-discrep-definition-covariate-importance.tex}\noindent

Note that we write \(\Q(Y \mid X_{r})\), to make clear that the
importance distribution could be covariate-specific, but in
straightforward settings a common importance distribution for all \(R\)
covariate values will be appropriate.

We select \(\Q(Y \mid X_{r})\) using information about the support
\(\mathcal{Y}\), and samples from \(\Pd(Y \mid \lambda, X_{r})\) and
\(\tc(Y \mid X_{r})\). For more details see Appendix
\ref{importance-sampling}. Finally, for numerical stability we evaluate
\(D(\lambda \mid \boldsymbol{X})\) on the log scale, with details
available in Appendix \ref{evaluating-dlambda-mid-boldsymbolx}. This
process is summarised in Algorithm
\ref{alg:tpd-algorithmic-description}.

\input{tex-input/pbbo-methodology/0051-tpd-algorithmic-description.tex}

\hypertarget{optimisation-stage-1}{%
\subsubsection{Optimisation, stage 1}\label{optimisation-stage-1}}

In this stage we focus solely on faithfulness by minimising
\(D(\lambda \mid \boldsymbol{X})\). We do so using CRS2
\citep{price_global_1983} with local mutation
\citep{kaelo_variants_2006}, which we run for \(N_{\text{CRS2}}\)
iterations. We make use of the final optimum value \(\lambda^{*}\), as
well as all of the \(N_{\text{CRS2}}\) trial points to obtain a design
\(\mathcal{D}\) for the next stage. The design comprises values of
\(\lambda\), and their corresponding values of
\(\log(D(\lambda \mid \boldsymbol{X}))\). A (small) number of padding
points \(N_{\text{pad}}\) are added to \(\mathcal{D}\) for numerical
robustness in stage 2. The result is the design
\(\mathcal{D} = \left\{\lambda_{i}, \log(D(\lambda_{i} \mid \boldsymbol{X}))\right\}_{i = 1}^{N_{\text{design}} + N_{\text{pad}}}\),
whose construction is detailed in Algorithm
\ref{alg:crs2-algorithmic-description} in Appendix
\ref{algorithmic-descriptions-of-the-optimisation-process}.

Whilst CRS2 was not designed to minimise noisy functions, it appears
empirically robust to small quantities of noise. We can make the noise
in \(D(\lambda \mid \boldsymbol{X})\) arbitrarily small (by increasing
\(S_{r}\) and \(I_{r}\)), but doing so usually incurs an enormous
computational cost. Carefully balancing the noise in the objective, and
thus the quality of the stage one solution, against the cost of
evaluation yields a faithful optimum \(\lambda^{*}\) and useful design
\(\mathcal{D}\) in an acceptable amount of time.

\hypertarget{optimisation-stage-2}{%
\subsubsection{Optimisation, stage 2}\label{optimisation-stage-2}}

Stage two accounts for the secondary objective
\(N(\lambda \mid \boldsymbol{X})\) in addition to the primary objective.
We adjust our optimisation technique to affect this change in emphasis,
and use multi-objective Bayesian optimisation, via MSPOT
\citep{zaefferer_mspot_2012}, to jointly minimise
\(D(\lambda \mid \boldsymbol{X})\) and
\(N(\lambda \mid \boldsymbol{X})\). MSPOT uses a separate Gaussian
process (GP) approximation to each of the objectives, and evaluates
these approximations at many points from Latin hypercube designs
\citep{stein_large_1987}. In each of the \(N_{\text{BO}}\) iterations,
the best points under the current GP approximations are evaluated using
the actual objectives. These evaluations accumulate and thus iteratively
improve the GP approximations. After \(N_{\text{BO}}\) iterations, the
evaluated points are reduced to their Pareto frontier
\citep{kung_finding_1975}. Algorithm
\ref{alg:mspot-algorithmic-description} in Appendix
\ref{algorithmic-descriptions-of-the-optimisation-process} describes in
detail the MSPOT algorithm as applied to two objectives.

The noisy and computationally expensive nature of our objectives,
particularly \(D(\lambda \mid \boldsymbol{X})\), necessitates an
approach such as MSPOT. Employing approximate GP models for the
objectives allows us to screen potential values of
\(\lambda \in \Lambda\) inexpensively, and avoid evaluating the actual
objectives at values of \(\lambda\) far from optimal. Moreover, the GP
is a flexible yet data efficient model to use as an approximation and
can, through appropriate choice of kernel, capture correlation or other
complex relationships between components of \(\lambda\) and the
objective.

We use an optional batching technique in stage two because the
computational cost of evaluating the GP growing cubically in the number
of points \(N_{\text{BO}}\) used in its construction. Batching avoids
this problem by subsampling to remove similar, or otherwise
uninformative, points from the collection used to form the surrogate GP
model. Specifically, we run MSPOT for a fixed number of iterations, then
select a subsample of the points evaluated in the completed batch as
initialisation points for the following batch. In our experience this
approach can produce equivalent or better priors in less time than a
single batch of many iterations. The exact subsampling strategy is
detailed in Algorithm~\ref{alg:resample-batch-algorithmic-description}
in Appendix \ref{algorithmic-descriptions-of-the-optimisation-process}.

\hypertarget{benchmarking-and-other-empirical-considerations}{%
\subsection{Benchmarking and other empirical
considerations}\label{benchmarking-and-other-empirical-considerations}}

We will show results for both the multi-objective approach and the
single-objective approach, which does not regularise via the second
objective (the optimisation process is identical except stage 2
considers only \(D(\lambda \mid \boldsymbol{X})\)).

Having selected \(\kappa\) we compute the optimal \(\lambda\) by
minimising \(L(\lambda)\). Given \(\lambda^{*}\), we will empirically
assess the performance of our method against the desiderata (Section
\ref{desiderata}). Faithfulness is straightforward to assess empirically
by comparing the target distribution \(\tp(Y \mid X_{r})\) and the
estimated optimal prior predictive distribution
\(\pd(Y \mid \lambda^{*}, X_{r})\). Replicability and uniqueness are
more challenging to disentangle empirically: in the absense of
replicability, we are not able to conclude whether uniqueness holds or
not. We will first assess replicability by examining the stability of
the components of the loss in Equation \ref{eqn:loss-definition} across
independent replications of the optimisation procedure. When the loss is
stable across replicates, we will assess uniqueness by examining whether
the optimal prior predictive distribution
\(\pd(Y \mid \lambda^{*}, X_{r})\) and prior
\(\Pd(\theta \mid \lambda^{*}, \boldsymbol{X})\) are stable across
replicates; the stability of both provides good evidence that of
uniqueness.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

Our method for specifying a prior given predictive information requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a method for sampling \(\Pd(Y \mid \lambda, \boldsymbol{X})\);
\item
  upper and lower limits that render \(\Lambda\) a compact subset of
  \(\mathbb{R}^{L}\);
\item
  a target CDF \(\tc(Y \mid \boldsymbol{X})\) which, for numerical
  stability, must be implemented on the log-scale;
\item
  a method for generating samples from \(\tc(Y \mid \boldsymbol{X})\),
  as the importance sampler depends on information contained in such
  samples;
\item
  a choice of \(\kappa\) (and we will present a diagnostic plot--see
  e.g.~Figure \ref{fig:kappa_cov}--which assists in making this
  decision).
\end{enumerate}

Algorithm~\ref{alg:pbbo-overall-algorithmic-description} describes, in
pseudocode, our proposed methodology.

\input{tex-input/pbbo-methodology/0055-pbbo-overall-algorithmic-description.tex}

\hypertarget{the-pbbo-r-package}{%
\subsection{\texorpdfstring{The \texttt{pbbo} \texttt{R}
package}{The pbbo R package}}\label{the-pbbo-r-package}}

We implement our methodology in an \texttt{R} package
\citep{r_core_team_r_2022} called \texttt{pbbo}, available from
\url{https://github.com/hhau/pbbo}. \texttt{pbbo}\footnote{The release
  associated with this paper is available at
  \url{https://doi.org/10.5281/zenodo.6817350}.} builds on top of
\texttt{mlrMBO} \citep{bischl_mlrmbo_2018} for multi-objective Bayesian
optimisation, \texttt{nlopt} and \texttt{nloptr}
\citep{ypma_nloptr_2022, johnson_nlopt_2014} for global optimisation
using CRS2 \citep{kaelo_variants_2006}, and other packages for internal
functionality and logging
\citep{wickham_welcome_2019, rowe_futilelogger_2016, maechler_rmpfr_2021}.
The code implementing the examples we consider in Sections
\ref{calibrating-a-cure-fraction-survival-model} to
\ref{a-human-aware-prior-for-a-human-growth-model}, which further
illustrate \texttt{pbbo}, can be found at
\url{https://gitlab.com/andrew-manderson/pbbo-paper}.

\hypertarget{calibrating-a-cure-fraction-survival-model}{%
\section{Calibrating a cure fraction survival
model}\label{calibrating-a-cure-fraction-survival-model}}

Cure models \citep{peng_cure_2014, amico_cure_2018} for survival data
are useful when a cure mechanism is physically plausible \emph{a
priori}, and when individuals are followed up for long enough to be
certain all censored individuals in our data are ``cured''. Such lengthy
follow ups are not always possible, but a cure model remains plausible
when a large fraction of the censored observations occur after the last
observed event time. However, we cannot distinguish in the right tail of
the survival time distribution between censored uncured individuals and
genuinely cured individuals.

We suppose here that we possess prior knowledge on the fraction of
individuals likely to be cured, and the distribution of event times
amongst the uncured, and seek to translate this information into a
reasonable prior for the parameters in a cure model. Consider
individuals \(n = 1, \ldots, N\) with event times \(Y_{n}\) and
censoring times \(C_{n}\), such that \(Y_{n} \in (0, C_{n}]\). We
suppose censoring times are distributed such that
\(C_{n} \sim 20 + \text{Exp}(1)\). We simulate data for \(N = 50\)
individuals, each with \(B = 4\) correlated covariates. We sample a
single correlation matrix \(\boldsymbol{Q} \sim \text{LKJ}(5)\)
\citep{lewandowski_generating_2009} and subsequently covariates
\(\tilde{\mathbf{x}}_{n} \sim \text{MultiNormal}(\boldsymbol{0}, \boldsymbol{Q})\).
This results in marginally-standardised yet correlated covariates.

There are several properties of this model that make it an interesting
subject for prior specification methodology. The observed quantity, and
thus the target distribution, is of mixed discrete/continuous type due
to censoring. Additionally, we specify a model with a nontrivial
correlation structure, about which we wish to specify an informative
prior. Eliciting informative priors for correlation structures is known
to be challenging. Finally, identifiability is known to be challenging
in cure models \citep{peng_cure_2014}, and so the model is a demanding
test of our regularisation procedure.

\hypertarget{target-survival-time-distribution-and-covariate-generation}{%
\subsection{Target survival time distribution and covariate
generation}\label{target-survival-time-distribution-and-covariate-generation}}

Suppose that individuals are followed up for an average (but arbitrary)
of 21 units of time, with those who experience the event doing so a long
time before the end of follow up. Furthermore, suppose we believe that,
\emph{a priori}, \(5\%\) of the patients will be cured, with \(0.2\%\)
of events censored due to insufficient follow up.

A target distribution that is consistent with our beliefs comprises a
point mass of \(0.05\) at \(C_{n}\), and a lognormal distribution with
location \(\mu^{\text{LN}} = \log(3)\) and scale
\(\sigma^{\text{LN}} = 2 \mathop{/} 3\) for \(Y_{n} < C_{n}\). This
choice of lognormal has \(99.8\%\) of its mass residing below 21, and
thus produces event times that are ``well separated'' from the censoring
time, as required by a cure fraction model. Denoting the lognormal CDF
with \(\text{LogNormal}(Y; \mu, \sigma^{2})\), we define the target CDF
\input{tex-input/surv-example/0021-surv-target-cdf-definition.tex}\noindent
where
\(Z_{n} = \text{LogNormal}(C_{n}; \mu^{\text{LN}}, \left(\sigma^{\text{LN}}\right)^{2})\)
is the required normalising constant. This individual-specific
construction of \(\tc(Y_{n} \mid C_{n})\) implies that the number of
covariates \(R = N\), since the censoring time \(C_{n}\) functions as a
covariate.

We simulate data for this example with \(N = 50\) individuals, each with
\(B = 4\) correlated covariates. In line with our target distribution,
simulated censoring times are distributed such that
\(C_{n} \sim 20 + \text{Exp}(1)\). We sample a single correlation matrix
\(\boldsymbol{Q} \sim \text{LKJ}(5)\)
\citep{lewandowski_generating_2009} and subsequently covariates
\(\tilde{\mathbf{x}}_{n} \sim \text{MultiNormal}(\boldsymbol{0}, \boldsymbol{Q})\).
This results in marginally-standardised yet correlated covariates.

\hypertarget{model}{%
\subsection{Model}\label{model}}

A cure model for survival data, expressed in terms of its survival
function, is
\input{tex-input/surv-example/0009-cure-model-surv-def.tex}\noindent
where a proportion \(\pi \in (0, 1)\) of the population are \emph{cured}
and never experience the event of interest. The survival times for the
remaining \(1 - \pi\) proportion of the population are distributed
according to the \emph{uncured} survival function
\(\tilde{S}(Y \mid \tilde{\boldsymbol{X}}, \tilde{\theta})\). We use the
tilde in \(\tilde{\boldsymbol{X}}\) and \(\tilde{\theta}\) to denote
quantities specific to the uncured survival distribution, and denote
\(\theta = (\pi, \tilde{\theta})\) to align with our general notation.

Right censoring results in \(Y_{n} = C_{n}\). The censoring indicator
\(\delta_{n} = \mathbbm{1}_{\left\{Y_{n} < C_{n}\right\}}\) is 0 for
right censored events, and is 1 otherwise. We denote with
\(\tilde{\mathbf{x}}_{n}\) the \(n\)\textsuperscript{th} row of the
\(N \times B\) covariate matrix \(\tilde{\boldsymbol{X}}\). Our model
supposes that the uncured event times are distributed according to a
Weibull regression model, with survival function
\(\tilde{S}(Y_{n} \mid \tilde{\theta}, \tilde{\mathbf{x}}_{n}, C_{n})\)
and hazard
\(\tilde{h}(Y_{n} \mid \tilde{\theta}, \tilde{\mathbf{x}}_{n}, C_{n})\)
such that
\input{tex-input/surv-example/0010-weibull-surv-and-hazard-def.tex}\noindent
with \(\tilde{\theta} = (\gamma, \beta_{0}, \boldsymbol{\beta})\). The
likelihood for the \(n\)\textsuperscript{th} individual is
\input{tex-input/surv-example/0011-weibull-likelihood-def.tex}\noindent
To align the notation in this example with that introduced in Section
\ref{translating-elicited-prior-predictive-distributions} we denote
\(Y = (Y_{n})_{n = 1}^{N}\) and
\(X = (C_{n}, \tilde{\mathbf{x}}_{n})_{n = 1}^{N}\). Note that we are
using \(X\) to represent \emph{all} information that must be conditioned
on, including the censoring times. This is necessary because, in our
more general notation, the support of \(Y \mid X_{r}\) is truncated to
an interval that depends on \(X_{r}\), making the censoring times
necessary to fully-specify \(Y \mid X_{r}\).

We will seek to identify optimal values of the hyperparamers
\(\lambda = (\alpha, \beta, \mu_{0}, \sigma^{2}_{0}, s_{\beta}, \boldsymbol{\omega}, \boldsymbol{\eta}, a_{\pi}, b_{\pi})^{\top}\),
where: \input{tex-input/surv-example/0012-surv-prior-dists.tex}\noindent
The upper and lower limits that define \(\Lambda\) are specified in
Table \ref{tab:surv-cap-lambda-def} in Appendix
\ref{additional-information-for-the-cure-fraction-survival-example}. Our
use of the multivariate skew-normal prior has two motivations. The
skewness is necessary to incorporate the nonlinear relationship between
the hazard and the effect of the covariates, and a covariance structure
is used to account for fact that not all the elements of
\(\boldsymbol{\beta}\) can be large simultaneously. We assume that
\(\tilde{\boldsymbol{X}}\) is marginally (column-wise) standardised, and
can thus decompose
\(\boldsymbol{S} = \text{diag}(s_{\beta}) \,\, \boldsymbol{\Omega} \,\, \text{diag}(s_{\beta})\)
where \(s_{\beta}\) is the scale of the prior marginals of
\(\boldsymbol{\beta}\). The standardisation allows us to use only one
\(s_{\beta}\) instead of one per covariate. We elect to parameterise
\(\boldsymbol{\Omega}\) using the \((B - 1)! = 6\) elements that
uniquely determine its Cholesky factor, which we denote
\(\boldsymbol{\omega} = (\omega_{1}, \ldots, \omega_{6})^{\top} \in [-1, 1]^{6}\).
These elements are transformed into \(\boldsymbol{\Omega}\) using the
partial correlation method of \citet{lewandowski_generating_2009}, also
employed by the \texttt{Stan} math library
\citep{stan_development_team_stan_2022}. The \(B\)-vector
\(\boldsymbol{\eta}\) controls, but is not equal to, the marginal
skewness for each element of \(\boldsymbol{\beta}\) using the
multivariate skew-normal definition of
\citet{azzalini_multivariate_1996}, as implemented in the \texttt{sn}
package \citep{azzalini_sn_2022}.

\hypertarget{tuning-parameters-and-further-details}{%
\subsection{Tuning parameters and further
details}\label{tuning-parameters-and-further-details}}

In this example we again employ the multi-objective approach with
\(N(\lambda)\) as defined in Equation \ref{eqn:second-objective-def}. We
also run single-objective optimisation to facilitate assessing
uniqueness. Total predictive discrepancy is computed using both the
Anderson-Darling and CramÃ©r-von Mises discrepancy functions.

We use \(N_{\text{CRS2}} = 2000\) CRS2 iterations, followed by
\(N_{\text{batch}} = 3\) batches of multi-objective Bayesian
optimisation using \(N_{\text{BO}} = 200\) iterations per batch,
carrying forward \(N_{\text{design}} = 60\) points between batches. The
predictive discrepancy function, \(D(\lambda)\), is estimated using
\(S_{r} = 10^{4}\) samples from the prior predictive, and evaluated
using \(I_{r} = 5 \times 10^3\) importance samples from an appropriate
mixed discrete/continuous importance density.

\hypertarget{results}{%
\subsection{Results}\label{results}}

\hypertarget{choosing-kappa-for-the-multi-objective-approach}{%
\subsubsection{\texorpdfstring{Choosing \(\kappa\) for the
multi-objective
approach}{Choosing \textbackslash kappa for the multi-objective approach}}\label{choosing-kappa-for-the-multi-objective-approach}}

We inspect the Pareto frontiers and compute the minimum loss points for
6 values of \(\kappa \in \{0.1, 0.2, 0.3, 0.5, 1, 2\}\). These are
displayed, for the Anderson-Darling discrepancy, in Figure
\ref{fig:surv_ex_pareto_fronts}, and Appendix
\ref{pareto-frontiers-for-cramuxe9r-von-mises-discrepancy} Figure
\ref{fig:surv_ex_pf_cvm} for the CramÃ©r-von Mises discrepancy. To
reiterate, the assumption is that nearby points on the Pareto frontier
are ``similar'' priors for \(\pd(\theta \mid \lambda)\).

The maximum and minimum values for \(\kappa\) yield a number of minimum
loss points on the extremes of the Pareto frontier. For the maximum,
\(\kappa = 2\), these points correspond to unsuitable values for
\(\lambda\) due to a lack of faithfulness, which we will demonstrate
momentarily. The remaining options for \(\kappa\) result in similar
minimum loss points, illustrating that the minimum loss point is
constant, or very similar, across a range of \(\kappa\) values. Such
insensitivity is ideal as we can obtain sensible solutions for a wide
variety of \(\kappa\) values. We select \(\kappa = 0.3\) as this
simultaneously minimises the variability in loss and both objective
functions.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/all-pareto-fronts-by-kappa-ad} 

}

\caption{Pareto frontiers for the survival example using the Anderson-Darling discrepancy, for the values of $\kappa$ we consider. Note that the range associated with the colour scale differ between panels. The red crosses ($\color{myredhighlight}{+}$) indicate the minimum loss point on each frontier, for each value of $\kappa$.}\label{fig:surv_ex_pareto_fronts}
\end{figure}

\hypertarget{replicability-of-loss-at-the-optima}{%
\subsubsection{Replicability of loss at the
optima}\label{replicability-of-loss-at-the-optima}}

Figure \ref{fig:surv_ex_final_objective_values} displays
\(\log(D(\lambda \mid \boldsymbol{X}))\), or \(\log(D(\lambda))\) in the
single objective setting, \(N(\lambda \mid \boldsymbol{X})\), and
\(L(\lambda \mid \boldsymbol{X})\) at the replicate optima
\(\lambda^{*}\), using \(\kappa = 0.3\) in the multi-objective setting.
We observe a tight distribution of objective values, particularly for
\(\log(D(\lambda^{*} \mid \boldsymbol{X}))\) and
\(\log(D(\lambda^{*}))\), suggesting our results are replicable in this
example.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/final-objective-values} 

}

\caption{Value of the objectives $D(\lambda^{*} \mid \boldsymbol{X})$,  $N(\lambda^{*}\mid \boldsymbol{X})$, and the loss $L(\lambda \mid \boldsymbol{X})$ at the replicated optima $\lambda^{*}$ for $\kappa = 0.3$. The single objective optima and predictive discrepancy $\log(D(\lambda^{*}))$ are displayed in the right panel. The Anderson-Darling discrepancy function is displayed in red, with the CramÃ©r-von Mises in blue. The points have been jittered along the x-axis for readability.}\label{fig:surv_ex_final_objective_values}
\end{figure}

\hypertarget{faithfulness}{%
\subsubsection{Faithfulness}\label{faithfulness}}

We assess faithfulness by inspecting Figure \ref{fig:surv_ex_ppd_y},
which displays the prior predictive distributions at the optima and
target for the randomly selected individual \(n = 9\) in our simulated
population (other individuals are not visually distinguishable). In
addition to the possible values of \(\kappa\), the optimal prior
predictive distributions are also displayed using the single objective
approach for reference. Across all replicates and different \(\kappa\),
the estimated optimal prior predictive distribution is highly faithful
to the target. The multi-objective results are close to those obtained
using the single objective approach, except for the two largest values
of \(\kappa\). By overly valuing the secondary objective, the negative
mean log standard deviation, a fraction of the maximum \(\kappa\) fits
are considerably worse. The poor fits correspond to the minimum loss
points in the bottom-right corner of the \(\kappa = 2\) panel in Figure
\ref{fig:surv_ex_pareto_fronts}.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/subset-indivs-ppd-y-dens} 

}

\caption{Estimated optimal prior predictive densities $\pd(Y_{n} \mid \lambda^{*})$ (red/blue lines and dots) and target densities $\tp(Y_{n} \mid C_{N})$ (black lines and crosses) for randomly selected individual $n = 9$. The continuous portions of the densities are displayed as lines, with the discrete/censored portion displayed as a point at $Y_{n} = C_{n}$. The top rows (red) correspond to the multi-objective approach using the values of $\kappa$ in the row titles, with the bottom row (blue) displaying the single objective results. The left and right columns use the Anderson-Darling and CramÃ©r-Von Mises discrepancies respectively. Densities are truncated to $[0, 0.75]$ for readability.}\label{fig:surv_ex_ppd_y}
\end{figure}

\hypertarget{uniqueness}{%
\subsubsection{Uniqueness}\label{uniqueness}}

\begin{landscape}
\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/all-theta-over-kappa} 

}

\caption{Estimated optimal prior marginal densities of $\pd(\theta \mid \lambda^{*})$ for each component of $\theta$ in the survival example. The top four rows (red) are obtained using the multi-objective approach, under both discrepancy functions (AD -- Anderson-Darling, CvM -- CramÃ©r-von Mises). The selected value of $\kappa = 0.3$ (top two rows), as well as the extreme value $\kappa = 2$ (middle two rows) are displayed. The bottom two rows (blue) use the single, predictive discrepancy objective and both discrepancy functions. Both axes are manually truncated for readability. As such, many of the replicates for $\theta = \gamma$ and $\kappa = 2$ are not visible because the samples used to estimate $\pd(\gamma \mid \lambda^{*})$ lie between $10^{3}$ and $ 5 \times 10^{5}$.}\label{fig:surv_ex_ppd_theta}
\end{figure}
\end{landscape}

We now evaluate the uniqueness of the optimisation problem. Figure
\ref{fig:surv_ex_ppd_theta} displays the marginals of \(\theta\) under
both the multi-objective (\(\kappa = 0.3, 2\)) and the single objective
approach, for each independent replicate.

The single objective approach consistently locates the degenerate,
non-unique solution where all the variation in the uncensored event
times is attributed to the baseline hazard shape \(\gamma\) and the
intercept \(\beta_{0}\): i.e.~all the mass for \(\boldsymbol{\beta}\)
(the regression coefficients) is close to 0. The combination of
\(\gamma\) and \(\beta_{0}\) is evidently poorly identified, and further
calculation reveals that only the derived product
\(\gamma \exp{\beta_{0}}\) is uniquely determined. Given the
inter-replicate consistency previously observed in Figure
\ref{fig:surv_ex_final_objective_values} we infer that the optimisation
process is locating equally good, in terms of predictive discrepancy,
optima that correspond to different \(\Pd(\theta \mid \lambda^{*})\),
which is precisely our definition of nonuniquness. Furthermore, the
concentration around \(\boldsymbol{\beta} = 0\) means these priors are
unlikely to be efficient for inferring the (possible) relationship
between covariates and outcome -- a very strong signal in the data would
be needed to overcome this prior.

Our multi-objective approach produces a more reasonable and disperse
prior, but still does not admit a unique optimal prior. The typical
marginal prior for \(\boldsymbol{\beta}\) has been widened, and for
\(\kappa = 0.3\) there is a preference for the optima surrounding
\(\gamma \approx 7.5, \beta_{0} \approx -10\); an improvement in
uniqueness over the single objective approach, but imperfect. When
\(\kappa = 2\) the process regularly produces a marginal prior for the
cure fraction \(\pi\) that gives considerable mass to values of
\(\pi > 0.3\), which is incongruent with our target. This is also
visible in the censored portions of the prior predictive estimates
\(\pd(Y_{n} \mid \lambda^{*}, C_{n})\) displayed in Figure
\ref{fig:surv_ex_ppd_y}.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/beta-covariance-pair} 

}

\caption{Contours of the log prior density $\log(\pd(\beta_{3}, \beta_{4} \mid \lambda^{*}))$ at the optima. The left column corresponds to the single objective approach, with the multi-objective approach using $\kappa = 0.3$ displayed in the right column. The top and bottom row use the CramÃ©r-von Mises and Anderson-Darling discrepancies respectively. Note that, for clarity, we only plot the final 12 of 30 replicates, with unique colouring for each replicate.}\label{fig:surv_ex_beta_cov}
\end{figure}

Eliciting covariance structures is challenging, and in this example we
included a covariance matrix in the hyperparameters for our model. In
Figure \ref{fig:surv_ex_beta_cov} we display the bivariate prior
marginal densities for two elements of \(\boldsymbol{\beta}\),
specifically \(\beta_{3}\) and \(\beta_{4}\), for both the
multi-objective approach with \(\kappa = 0.3\) and the single objective
approach. Nonuniqueness is visible in both sets of estimates. There are
marginal densities of both positive and negative marginal skewness, and
pairwise correlation. The wider typical marginal for
\((\beta_{3}, \beta_{4})\) obtained using the multi-objective approach
is again visible in the right panel of Figure
\ref{fig:surv_ex_beta_cov}. Lastly, there is a visible difference
between the discrepancy functions in the single objective approach, with
the Cramer-von Mises discrepancy typically producing a slightly wider
marginal.

\hypertarget{example-summary}{%
\subsection{Example summary}\label{example-summary}}

Our procedure estimates priors that faithfully represent provided
information about the survival distribution. Replicability is reasonable
in this case, particularly for the loss, although the noise in Figure
\ref{fig:surv_ex_pareto_fronts} would, ideally, be smaller. Several
priors appear to be consistent with the target distribution we specify,
and so the prior our procedure selects is not unique. The
multi-objective approach induces more concentration than the single
objective approach in the choice of prior for \(\gamma\), but it is
still not unique. A particular challenge for uniqueness is the
covariance structure for the vector of covariate coefficients.

In the information we supply via \(\tc(Y_{n} \mid C_{n})\), we are
completely certain of the fraction of cured patients \emph{a priori}.
Such certainty is unlikely to be uncovered when eliciting information
from experts. A more elaborate construction of
\(\tc(Y_{n} \mid C_{n})\), or elaborate methodology, may be able to
represent such uncertainty, but the example remains challenging without
this additional complication.

\hypertarget{priors-from-model-derived-quantities}{%
\section{Priors from model-derived
quantities}\label{priors-from-model-derived-quantities}}

Consider the linear model
\(Y = \boldsymbol{X}\boldsymbol{\beta} + \varepsilon\) for
\(n \times p\) design matrix \(\boldsymbol{X}\) and \(p\)-vector of
coefficients \(\boldsymbol{\beta}\) indexed by \(j = 1, \ldots, p\), and
where the noise \(\varepsilon\) has zero mean and variance
\(\sigma^{2}\). Suppose information about the fraction of variance
explained by the model is available -- from previous similar
experiments, or from knowledge of the measurement process -- in the form
of a plausible distribution for the coefficient of determination,
\(R^{2}\), which for this model can be computed as
\input{tex-input/r2-examples/0010-r2-definition.tex}\noindent assuming
that the columns of \(\boldsymbol{X}\) have been centred. Our aim is to
use our knowledge of \(R^{2}\) to set suitable priors for the regression
coefficients \(\beta\). This idea was the inspiration for a class of
shrinkage priors \citep{zhang_variable_2018, zhang_bayesian_2022}, but
we would like to make this idea applicable to a wider selection of prior
structures.

To illustrate the effect of including knowledge of \(R^{2}\) on
increasingly complicated priors, we investigate the selection of
appropriate hyperparameters for three priors for the regression
coefficients: two shrinkage priors, and a simple Gaussian prior. We
simultaneously vary the covariate-independent target distribution
\(\tc(R^{2})\) to assess:

\begin{itemize}
\tightlist
\item
  each prior's ability to faithfully encode the information present
  across a wide variety of target distributions;
\item
  uniqueness of the optimisation problem, and replicability of the
  single-objective variant of our optimisation algorithm, for each
  prior/target pair.
\end{itemize}

Note that we assume throughout that the noise \(\varepsilon\) is
distributed according to a Gaussian distribution with zero mean and
variance \(\sigma^{2}\), with an \(\text{InverseGamma}(a_{1}, b_{1})\)
prior on \(\sigma^{2}\). To demonstrate the challenge that this poses
for uniqueness, we will seek to select suitable \(a_{1}\) and \(b_{1}\)
using our methodology.

\hypertarget{gaussian-prior}{%
\paragraph{Gaussian prior}\label{gaussian-prior}}

The Gaussian prior has only one hyperparameter \(\gamma\), which
controls the ratio of prior variability due to \(\boldsymbol{\beta}\) to
that of \(\varepsilon\), and is
\input{tex-input/r2-examples/0011-gaussian-prior-def.tex}\noindent
Hence, we denote parameters
\(\boldsymbol{\theta}_{\text{GA}} = (\boldsymbol{\beta}, \sigma^{2})\),
and seek optimum values for hyperparameters
\(\boldsymbol{\lambda}_{\text{GA}} = (\gamma, a_{1}, b_{1})\).

\hypertarget{dirichlet-laplace-prior-dir.-lap.}{%
\paragraph{Dirichlet-Laplace prior (Dir.
Lap.)}\label{dirichlet-laplace-prior-dir.-lap.}}

\citet{bhattacharya_dirichletlaplace_2015} introduce the
Dirichlet-Laplace shrinkage prior, which is defined for the
\(j\)\textsuperscript{th} coefficient such that
\input{tex-input/r2-examples/0012-dirichlet-laplace-definition.tex}\noindent
There is a single hyperparameter \(\alpha\), with smaller values of
\(\alpha\) yielding more sparsity in \(\boldsymbol{\beta}\). Thus we
denote \(\boldsymbol{\lambda}_{\text{DL}} = (\alpha, a_{1}, b_{1})\) and
\(\boldsymbol{\theta}_{\text{DL}} = (\boldsymbol{\beta}, \sigma^{2}, \phi_{1}, \ldots, \phi_{p}, \tau)\).

\hypertarget{regularised-horseshoe-prior-reg.-horse.}{%
\paragraph{Regularised horseshoe prior (Reg.
Horse.)}\label{regularised-horseshoe-prior-reg.-horse.}}

The regularised horseshoe of \citet{piironen_sparsity_2017} is the most
complex of the priors. With more intermediary stochastic quantities
between the hyperparameters and \(R^{2}\), as well as less linearity in
the relationship between the aforementioned, it is the most flexible of
the priors. These properties make finding optimal values of the
hyperparameters more challenging. The prior is
\input{tex-input/r2-examples/0013-regularised-horseshoe-def.tex}\noindent
where \(\text{Cauchy}^{+}\) denotes the Cauchy distribution truncated to
\([0, \infty)\). Equation \ref{eqn:regularised-horseshoe-def} leaves us
free to choose three prior-specific hyperparameters,
\((p_{0}, \nu, s^{2})\). Thus
\(\boldsymbol{\lambda}_{\text{HS}} = (p_{0}, \nu, s^{2}, a_{1}, b_{1})\)
and
\(\boldsymbol{\theta}_{\text{HS}} = (\boldsymbol{\beta}, \sigma^{2}, c^{2}, \omega, \delta_{1}, \ldots \delta_{p})\).
Whilst the regularised horseshoe is carefully designed to make
\((p_{0}, \nu, s^{2})\) interpretable and easy to choose, here we aim to
see if values of these hyperparameters can be chosen to match an
informative prior for \(R^{2}\).

\hypertarget{target-predictive-distributions}{%
\subsection{Target predictive
distributions}\label{target-predictive-distributions}}

We consider sixteen different \(\text{Beta}(s_{1}, s_{2})\)
distributions as our target \(\tc(R^{2})\), with
\(\{s_{1}, s_{2}\} \in \mathcal{S} \times \mathcal{S}\) and
\(\mathcal{S}\) chosen to be four exponentially-spaced values between
and including \(1 \mathop{/} 3\) and \(3\) (i.e.~equally-spaced between
\(\log(1 \mathop{/} 3)\) and \(\log(3)\)). These values represent a
variety of potential forms of the supplied target predictive
distribution for \(R^2\).

\hypertarget{evaluation-setup-and-tuning-parameters}{%
\subsection{Evaluation setup and tuning
parameters}\label{evaluation-setup-and-tuning-parameters}}

We fix \(n = 50\) and \(p = 80\) with entries in \(\boldsymbol{X}\)
drawn from a standard Gaussian distribution, and assess replicability
using 10 independent runs for each prior and target. The support
\(\Lambda\) for the hyperparameters is defined in Table
\ref{tab:cap-lambda-def} in Appendix
\ref{additional-information-for-the-r2-example}. We run \texttt{pbbo}
with \(S = 10^{4}\) samples from the prior predictive distribution, use
both \(d^{\text{AD}}\) and \(d^{\text{CvM}}\) as discrepancy functions,
and evaluate the log predictive discrepancy using
\(I = 5 \times 10^{3}\) samples from a \(\text{Uniform}(0, 1)\)
importance distribution. We run the first stage of the optimiser for
\(N_{\text{CRS2}} = 1000\) iterations, and subsequently perform both
single and multi-objective Bayesian optimisation for
\(N_{\text{batch}} = 1\) batch of \(N_{\text{BO}} = 150\) iterations,
using \(N_{\text{design}} = 50\) points from the first stage. The single
objective approach illustrates that differences in flexibility between
priors also induce differences in uniqueness, and highlights issues in
choosing a prior for the additive noise parameter \(\sigma^{2}\).
Choosing \(\kappa\) is challenging in the multi-objective approach, as
its value should depend on the target, the discrepancy function, and the
prior. These dependencies result in 96 possible choices of \(\kappa\),
which is and infeasible number of choices to make in this example.
Instead we fix \(\kappa = 0.5\) for all multi-objective settings.
Lastly, we compute the secondary objective \(N(\lambda)\) using the
marginal standard deviation for all elements in \(\theta\) as in
Equation \eqref{eqn:second-objective-def}, except for those quantities
where, for some \(\lambda \in \Lambda\), the standard deviation is
undefined. In such cases we use the robust scale estimator from
\citet{rousseeuw_alternatives_1993}.

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

We first assess the replicability of the discrepancy by plotting the
value of the objective at the optima \(\lambda^{*}\) across
replications. In Figure \ref{fig:r2_roundtrip_discrep_at_optima} each
value of \(\log(D(\lambda^{*}))\) is the mean of 10 evaluations of
\(\log(D(\lambda))\) at each \(\lambda^{*}\) to minimise residual noise
in the objective. Generally, it appears that the discrepancy is
replicable for the both Gaussian and Dirichlet-Laplace, for both
Anderson-Darling and CramÃ©r-von Mises discrepancies. In contrast, the
results for the Regularised Horseshoe prior appear to replicate poorly
under the Anderson-Darling discrepancy, but reasonably under the
CramÃ©r-von Mises discrepancy.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/roundtrip-discrep-at-optima} 

}

\caption{Total log predictive discrepancy at the optima $\log(D(\lambda^{*}))$. The target densities (denoted in the column titles) correspond to the bottom row of Figure \ref{fig:r2_roundtrip_full} (i.e. the same as Figure \ref{fig:r2_roundtrip_lambda}). Each point corresponds to one of the 10 distinct replicates, and its value is the mean of 10 evaluations of $\log(D(\lambda^{*}))$ for the same $\lambda^{*}$. The top row displays Anderson-Darling discrepancy function, with the bottom row displaying the CramÃ©r-von Mises discrepancy function. Each prior is displayed in the same colour as Figure \ref{fig:r2_roundtrip_lambda}, with the point shape displaying the single or multi-objective approach.}\label{fig:r2_roundtrip_discrep_at_optima}
\end{figure}

We evaluate the faithfulness of the resulting prior distributions by
inspecting the densities \(\pd(R^{2} \mid \lambda^{*})\) and
\(\tp(R^{2})\) for the various targets (all distributions in this
example have corresponding densities). A selected subset of the pairs of
\((s_{1}, s_{2})\) values are displayed in Figure
\ref{fig:r2_roundtrip_full} (complete results are in Appendix
\ref{additional-information-for-the-r2-example} Figure
\ref{fig:r2_roundtrip_full_supp}). The faithfulness of the Gaussian
prior is universally poor, which we investigate further in Appendix
\ref{additional-information-for-the-r2-example}. Both shrinkage priors
perform better in cases where one of \(s_{1}\) or \(s_{2}\) is less than
1, with the regularised horseshoe performing better for the
\(s_{1} = s_{2} > 1\) cases. Interestingly, the results are not
symmetric in \(s_{1}\) and \(s_{2}\); the Dirichlet-Laplace prior is
able to match the \(s_{1} = 3, s_{2} = 0.69\) target well, with many of
regularised horseshoe replicates performing poorly; whilst the relative
performance is reversed for \(s_{1} = 0.69, s_{2} = 3\) (see Figure
\ref{fig:r2_roundtrip_full_supp}). There is also perceptibly more
variability in the regularised horseshoe replicates, which suggests the
optimisation problem is more challenging and the predictive discrepancy
objective is noisier. The multi-objective generally produces more
variable sets of optima, which is expected, as it is a more difficult
optimisation problem but we do not allow it additional computational
resources. There is little visible difference between the two
discrepancy functions. Finally, as the values of \(s_{1}\) and \(s_{2}\)
increase, the faithfulness of the shrinkage priors generally decreases.
Across the full set of simulations, the regularised horseshoe is
evidently the most flexible (Appendix
\ref{additional-information-for-the-r2-example} Figure
\ref{fig:r2_roundtrip_full_supp}).

\begin{figure}

{\centering \includegraphics{plots/r2-examples/roundtrip-target-plot-small} 

}

\caption{Optimal prior predictive densities $\pd(R^{2} \mid \lambda^{*})$ for the three priors considered, for selected target densities. The column-titles denote the target, which is also plotted as a black dashed line. The row-titles denote which discrepancy is used to compute the total predictive discrepancy, and whether the single or multi-objective approach to optimisation is used. Each replicate of the Gaussian ('Gaussian' -- green), Dirichlet-Laplace ('Dir. Lap.' -- red), and regularised horseshoe ('Reg. Horse.' -- blue) is drawn in their respective colours. Density values are trimmed to $[0, 10]$ for readability.}\label{fig:r2_roundtrip_full}
\end{figure}

To assess uniqueness, we consider estimated optimal hyperparameter
values \(\lambda^{*}\) in each replicate. Figure
\ref{fig:r2_roundtrip_lambda} displays the estimates for \(s_{1} = 3\)
and \(s_{2} \in \{0.33, 0.69, 1.44, 3\}\), which corresponds to the
targets in Figure \ref{fig:r2_roundtrip_full}. The estimates for
\(\gamma\) and \(\alpha\), for the Gaussian and Dirichlet-Laplace priors
respectively, are consistent across replicates, which suggests the
optima may be unique. This remains true even for targets where the prior
is not faithful to the target, e.g.~the \(\text{Beta}(3, 3)\) target.
There is more variability in the hyperparameters of the regularised
horseshoe prior. There does appear to be unique solution for \(u\) for
the \(\text{Beta}(3, 0.33)\) and \(\text{Beta}(3, 0.69)\) targets,
whereas \(p_{0}\) and \(s^{2}\) are highly variable across replicates,
which may reflect nonuniqueness or may be due to the lack of
replicability (discussed above) of this optimisation for the regularised
horseshoe.

The hyperparameters \((a_{1}, b_{1})\) for the additive noise variance
\(\sigma^2\) are highly variable across replications for almost all
prior/target combinations. This reflects the anticipated lack of
uniqueness when incorporating such hyperparameters. It is particularly
striking for the Dirichlet-Laplace prior when
\(s_{2} \in \{0.33, 0.69\}\), where we observe consistent and excellent
fits/faithfulness to the target, but these do not correspond to
replicable estimates for \((a_{1}, b_{1})\). These settings are also
interesting as the choice of single or multi-objective approach greatly
impacts the optimum values of \(a_{1}\) and \(b_{1}\). Faithfulness of
the multi-objective optima, illustrated in Figure
\ref{fig:r2_roundtrip_full}, are not appreciably worse than the single
objective approach, but the inclusion of \(\sigma^{2}\) into the
secondary objective has resulted in optimal values of \(a_{1}\) and
\(b_{1}\) that maximise the dispersion of the marginal prior (i.e.~small
\(a_{1}\) and large \(b_{1}\)). Asymptotic results are also known for
the Gaussian prior, and in Appendix
\ref{additional-information-for-the-r2-example} we further assess
replicability by benchmarking our optimisation process against suitable
`true' (asymptotically) values.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/roundtrip-target-lambda-tiny} 

}

\caption{Optimal values $\lambda^{*}$ for each of the three priors considered. Columns contain (possibly prior-specific) hyperparameters, with the point colour corresponding to a specific prior. Each point's shape corresponds to the combination of discrepancy function and single or multi-objective approach. The target beta densities (denoted by the row panel titles) correspond to Figure \ref{fig:r2_roundtrip_full}.}\label{fig:r2_roundtrip_lambda}
\end{figure}

\hypertarget{comparison-of-discrepancy-functions}{%
\subsection{Comparison of discrepancy
functions}\label{comparison-of-discrepancy-functions}}

Our optimisation procedure has minimised \(\log(D(\lambda))\) using both
the Anderson-Darling and CramÃ©r-von Mises discrepancy functions. The
former places extra emphasis on matching the tails of the target, and
thus the Regularised Horseshoe values in the top row of Figure
\ref{fig:r2_roundtrip_discrep_at_optima} differ from our expectations
given the results in the bottom row of Figure
\ref{fig:r2_roundtrip_full}. Take, for example, the
\(s_{1} = 3, s_{2} = 0.69\) case. It is plainly evident from Figure
\ref{fig:r2_roundtrip_full} that the regularised horseshoe prior
provides a better fit to the target distribution at
\(\boldsymbol{\lambda}^{*}_{\text{HS}}\), and yet the corresponding
\(\log(D(\boldsymbol{\lambda}_{\text{HS}}^{*}))\) values in the top row
of Figure \ref{fig:r2_roundtrip_discrep_at_optima} suggest that it is
considerably worse that the Gaussian prior at
\(\boldsymbol{\lambda}_{\text{GA}}^{*}\). To reconcile this apparent
contradiction, we inspect \(\log(D(\lambda))\) at the optima computed
using the CramÃ©r-Von Mises discrepancy function. These values are
displayed in the bottom row of Figure
\ref{fig:r2_roundtrip_discrep_at_optima}, whose values closely match our
expectations given Figure \ref{fig:r2_roundtrip_full}. Given the range
of behaviours of \(\pd(R^{2} \mid \lambda^{*})\) for all the optima, we
can conclude that the Anderson-Darling more heavily penalises
over-estimation of the tails of \(\pd(R^{2} \mid \lambda^{*})\) than
under-estimation. This does not discount it as an optimisation
objective, but does complicate comparisons between competing priors.

\hypertarget{example-summary-1}{%
\subsection{Example summary}\label{example-summary-1}}

This example illustrates the use of a model-derived, nonobservable
quantity about which we have prior information as the basis for an
informative prior. The most flexible shrinkage model (the regularised
horseshoe prior) was clearly the most faithful to the supplied
information in almost all cases. Conversely, the Gaussian prior is the
most replicable and unique, but the lack of faithfulness means it is
unsuitable to use as a prior when seeking to use a Beta prior on
\(R^{2}\). The example also illustrates the difficulty of learning about
the prior for the additive noise term. Attempts to ameliorate this
difficulty by adopting the multi-objective approach merely hide the
issue; for the Gaussian and Dirichlet-Laplace priors, where
replicability is evident, we always select the inverse gamma prior that
maximises the standard deviation of \(\sigma^{2}\) by being on the
boundary of \(\Lambda\). Such a prior for \(\sigma^{2}\) is unlikely to
prove appropriate nor represent our prior knowledge.

\hypertarget{a-human-aware-prior-for-a-human-growth-model}{%
\section{A human-aware prior for a human growth
model}\label{a-human-aware-prior-for-a-human-growth-model}}

We now consider a nonlinear regression model for human growth. There are
a number of properties of this example that make it an interesting test
for our methodology. First, we find it difficult to specify priors
congruent with desired prior predictive distributions for such models;
both the nonlinearity and the obvious dependence of height on age
complicate prior specification. Data for human growth are also readily
available, so we can assess the impact of the prior on many distinct
data and posteriors. Second, the model we consider is also poorly
behaved under the flat prior, so some prior information is required to
stabilise and/or regularise the estimate of the posterior. Finally, this
example is also considered by \citet{hartmann_flexible_2020}, and so
there is a suitable comparator for our results.

Suppose an individual has their height measured at age \(t_{m}\) (in
years) for \(m = 1, \ldots, M\), with corresponding measurement
\(y_{m}\) (in centimetres). The first Preece-Baines model
\citep{preece_new_1978} for human height is,
\input{tex-input/preece-baines-growth/0061-preece-baines-model-definition.tex}\noindent
with \(\varepsilon_{m} \sim \text{N}(0, \sigma^{2}_{y})\). Some
constraints are required to identify this model and ensure its physical
plausibility: specifically, we require \(0 < h_{0} < h_{1}\) and
\(0 < s_{0} < s_{1}\). A parameterisation that respects these
constraints and is easier to work with uses
\(\delta_{h} = h_{1} - h_{0}\) instead of \(h_{1}\), and
\(\delta_{s} = s_{1} - s_{0}\) in place of \(s_{1}\). All of
\((h_{0}, \delta_{h}, s_{0}, \delta_{s})\) thus have the same positivity
constraint. Finally, we also constrain \(\gamma\) such that
\(\gamma \in (\min_{m}(t_{m}), \max_{m}(t_{m}))\). However, these
constraints are not sufficient to make the model plausible for all
permissible parameter values -- the denominator of the fraction can be
very small, yielding negative heights.

To align with the notation introduced in Section
\ref{translating-elicited-prior-predictive-distributions} we denote the
parameters by
\(\theta = (h_{0}, \delta_{h}, s_{0}, \delta_{s}, \gamma)\). As in
\citet{hartmann_flexible_2020}, we choose for each of the
\(q = 1, \ldots, 5\) elements of \(\theta\) an independent
\(\text{LogNormal}(\mu_{q}, s^{2}_{q})\) prior. We will seek to identify
the optimal values of
\(\lambda = \left(\mu_{q}, s^{2}_{q}\right)_{q = 1}^{5}\). Table
\ref{tab:pb-cap-lambda-def} in Appendix \ref{tab:pb-cap-lambda-def}
lists the upper and lower limits we choose for each component of
\(\lambda\).

We do not consider the measurement error variance \(\sigma_{y}^{2}\) as
part of \(\theta\). Doing so introduces a degenerate solution for
\(\lambda\), where all variability in \(Y\) is singularly attributable
to \(\varepsilon_{m}\) and thus \(\sigma^{2}_{y}\). Such a prior seems
undesirable, so instead we fix the prior for \(\sigma_{y}^{2}\) to
reflect the measurement process for human height; measurement errors are
unlikely to be more than one or two centimetres, so values of
\(\sigma_{y}^{2} \approx 1\) seem reasonable. Thus we set
\(\sigma_{y} \sim \text{LogNormal}(0, 0.2^2)\).

\hypertarget{target-predictive-distributions-1}{%
\subsection{Target predictive
distributions}\label{target-predictive-distributions-1}}

Our data are assumed to originate from a sample of adolescent humans,
uniformly distributed between ages 2 and 18, and evenly split between
sexes. We consider two different target prior predictive distributions.
We first consider a \emph{covariate-independent} prior predictive
density \(\tp(Y)\) with corresponding CDF \(\tc(Y)\) for human heights
across the entire age-range, derived by summarising external data. This
target (Figure \ref{fig:pop_target_discreps}) is a mixture of 3 gamma
densities specified to approximate the external data, which is
multimodal due to the fact that humans grow in spurts. We also consider
a \emph{covariate-specific} \(\tc(Y \mid X_{r})\) in which we specify
the target predictive distribution \(\tc(Y \mid X_{r})\) of human
heights at ages \(X_{r} \in (2, 8, 13, 18)\) with \(r = 1, \ldots, 4\).
Each \(\tc(Y \mid X_{r})\) is normal (see Figure
\ref{fig:cov_target_discreps}, and Appendix
\ref{details-for-tcy-and-tcy-mid-x_r} for details).

\hypertarget{tuning-parameters-and-comparison-with-hartmann-et.-al.}{%
\subsection{Tuning parameters and comparison with Hartmann et.
al.}\label{tuning-parameters-and-comparison-with-hartmann-et.-al.}}

For both targets we obtain \(\lambda^{*}\) using both the single
objective and multi-objective optimisation processes (see Sections
\ref{optimisation-strategy} and
\ref{benchmarking-and-other-empirical-considerations}). We use only the
CramÃ©r-Von Mises discrepancy in this example, because we were not able
to reliably compute the Anderson-Darling discrepancy due to numerical
instabilities. We run \texttt{pbbo} using \(S = 5 \times 10^4\) samples
from \(\pd(Y \mid \lambda)\) and likewise \(S_{r} = 5 \times 10^4\)
samples from \(\pd(Y \mid \lambda, X_{r})\) for each of the 4 values of
\(X_{r}\). We use \(I = 5 \times 10^3\) and \(I_{r} = 5 \times 10^3\)
importance samples, with \(N_{\text{CRS2}} = 2000\) CRS2 iterations,
\(N_{\text{batch}} = 5\) Bayesian optimisation batches each of
\(N_{\text{BO}} = 250\) iterations, and carry forward
\(N_{\text{design}} = 50\) points per batch. We assess replicability
using 30 independent runs of each objective/target pair.

For this example, we also assess the stability of the posterior: our
prior ideally admits a posterior amenable to sampling (i.e.~removes
spurious modes and eliminates computational difficulties present when
using a noninformative prior). It also seems desirable that similar
priors should yield similar posteriors. We assess stability by exploit
the robust sampling diagnostics in \texttt{Stan}. Should any diagnostic
flag an issue with the sampling of the posterior we can be confident
that something is amiss with the model, although the converse is not
immediately true; a lack of warnings does not imply the model is
behaving appropriately.

We compare the priors and posteriors produced by our methodology with
the posteriors produced using a flat, improper prior as a benchmark. We
consider, separately, each of the 93 individuals in the \texttt{growth}
data \citep{tuddenham_physical_1954} provided by the \texttt{fda}
package \citep{ramsay_fda_2022} in \texttt{R}
\citep{r_core_team_r_2022}. This is a form of prior sensitivity
analysis, but distinct from the ideas of \citet{roos_sensitivity_2015}
which consider only one particular realisation of the data. By
considering each individual in the \texttt{growth} data separately, as
opposed to jointly in a hierarchical model, we heighten the importance
of including appropriate prior information. We sample each posterior
using \texttt{Stan} \citep{stan_development_team_rstan_2021}, setting
\texttt{adapt\_delta\ =\ 0.95} and \texttt{max\_treedepth\ =\ 12} to
minimise false positive warning messages.

\citet{hartmann_flexible_2020} also considered this example, so we
compare also to these results. However, there are key differences
between our approaches that must be kept in mind when comparing results.
\citet{hartmann_flexible_2020} elicit 6 predictive quantiles at ages
\(t = (0, 2.5, 10, 17.5)\), as opposed to entire predictive
distributions at ages \(t = (2, 8, 13, 18)\) which underpin the
covariate-specific version of our method. We use different ages because
the model of \citet{preece_new_1978} is stated to be accurate and robust
for ages greater than 2. \citet{hartmann_flexible_2020} include a noise
parameter in their definition of \(\theta\). The exact interpretation of
this parameter is complicated by their choice of Weibull likelihood,
rendering the distribution of the measurement errors sensitive to
conditional mean of the model (this is still the case despite their
choice of Weibull parameterisation). Finally,
\citet{hartmann_flexible_2020} elicit quantiles from 5 different users
and report an estimated \(\lambda^{*}\) for each user. These estimates
(reproduced in Appendix \ref{hartmann_flexible_2020-priors}) allow us to
compare optimal the selected priors \(\pd(\theta \mid \lambda^{*})\).
They do not report whether each user's estimate is consistent over
repeated runs of their optimisation algorithm, and do not discuss the
issue of estimate replicability.

\hypertarget{results-2}{%
\subsection{Results}\label{results-2}}

\hypertarget{choosing-kappa}{%
\subsubsection{\texorpdfstring{Choosing
\(\kappa\)}{Choosing \textbackslash kappa}}\label{choosing-kappa}}

The optimal choice of \(\kappa\) is target specific, and so we
separately choose an appropriate
\(\kappa \in \mathcal{K} = \{0.05, 0.1, \ldots, 0.5\}\) for the
covariate-independent and covariate-specific targets. As a heuristic, we
choose the value of \(\kappa\) that yields the minimum variability of
\(L(\lambda \mid \boldsymbol{X})\) for \(\lambda \in \mathcal{P}\)
amongst the replicates, though this heuristic is unsuitable if, as we
expect to be more commonly the case, only one run of the optimiser is
made. In such settings we recommend plotting the Pareto frontiers as in
Figure \ref{fig:kappa_cov} and visually checking that the minimum loss
point is not at either extrema of the frontier.

Figure \ref{fig:kappa_cov} displays Pareto frontiers for a selection of
values \(\kappa \in \{0.1, 0.2, 0.3, 0.4\}\), and the associated minimum
loss points. There is notable inter-replicate variability, with the
Pareto frontier for some replicates being totally dominated by other
replicates. This is due to the stochastic properties of the global
optimisers we employ.

We select \(\kappa = 0.2\) for the covariate-specific target given our
minimum variability heuristic. Visible in most replicates is an
inflection point at values of \(N(\lambda) \approx 1.5\) (the Y-axis in
Figure \ref{fig:kappa_cov}) around which the minimum loss points
cluster, suggesting any of these points likely admits a reasonable value
for \(\lambda^{*}\). The results for the covariate-independent target
are similar (See Appendix
\ref{additional-information-for-the-preece-baines-example} Figure
\ref{fig:kappa_pop}) and there we select \(\kappa = 0.15\).

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/cov-kappa} 

}

\caption{Pareto frontiers for each $\kappa \in \mathcal{K}$ for the \textbf{covariate-specific} example. The minimum loss point for each replicate is plotted with $\color{myredhighlight}{+}$.}\label{fig:kappa_cov}
\end{figure}

\hypertarget{replicability-of-discrepancy}{%
\subsubsection{Replicability of
discrepancy}\label{replicability-of-discrepancy}}

Figure \textasciitilde{}\ref{fig:discrep_at_opimta} indicates that,
across replicates, the optimisation procedure identifies optimal
\(\lambda^{*}\) values with reasonably, but not entirely, consistent
predictive discrepancies.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/both-optima-discrep-points} 

}

\caption{Final predictive discrepancy $\log(D(\lambda^{*}))$, or $\log(D(\lambda^{*} \mid \boldsymbol{X}))$ for the covariate-specific target. The multiple objective optimisation approach uses the previously selected values of $\kappa = 0.15$ for the covariate-independent target, and $\kappa = 0.2$ for the covariate-specific target. Horizontal jitter has been applied for readability.}\label{fig:discrep_at_opimta}
\end{figure}

\hypertarget{faithfulness-1}{%
\subsubsection{Faithfulness}\label{faithfulness-1}}

Figure \ref{fig:pop_target_discreps} displays the target and prior
predictive density estimates in the covariate-independent case. In the
covariate-independent target case, we see that introducing the secondary
objective (right panel) produces estimates of \(\lambda^{*}\) that are
congruent with estimates from the single objective case, though the
estimates are more variable. Both single and multi-objective approaches
result in reasonably, but not entirely, faithful densities for
\(\pd(Y \mid \lambda^{*})\). However, most optimum priors seem to
accumulates additional probability surrounding
\(Y = h_{1} \approx 155\), resulting in individual trajectories
attaining their adult height \(h_{1}\) for younger than expected ages
\(t\) (which we will later confirm in Figure
\ref{fig:regression_prior_pred}).

For the covariate-specific target, displayed in Figure
\ref{fig:cov_target_discreps}, the secondary objective introduces in
some replicates outlying estimates for
\(\pd(Y \mid \lambda^{*}, X_{r})\), most clearly visible for
\(X_{1} = 2\) and \(X_{2} = 8\). Both the single and multi-objective
approaches struggle to match the prior predictive distribution at all
ages, with consistently poorer faithfulness for \(X_{1} = 2\).
Empirically, it does not seem possible to match all four supplied target
prior predictive distributions simultaneously, given the mathematical
structure of the model. Lastly, because \(\tp(Y \mid X_{1} = 2)\) is
substantially narrower than the other targets, it is optimal, under the
CramÃ©r-Von Mises discrepancy, to select wider priors better matching the
older age target distributions.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/population-target-comparsion} 

}

\caption{The covariate-independent marginal target density $\tp(Y)$ (red) and prior predictive densities $\pd(Y \mid \lambda^{*})$ for each of the 30 replicates (blue lines). The replicates in the right panel are obtained after $\kappa = 0.15$ is chosen.}\label{fig:pop_target_discreps}
\end{figure}

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/covariate-target-comparsion} 

}

\caption{Covariate-specific target densities $\tp(Y \mid X_{r})$ (red lines) and prior predictive densities $\pd(Y \mid \lambda^{*}, X_{r})$ for each of the 30 replicates (blue lines). The replicates in the right column are obtained after $\kappa = 0.2$ is chosen.}\label{fig:cov_target_discreps}
\end{figure}

\hypertarget{mean-prior-growth-trajectories}{%
\subsubsection{Mean prior growth
trajectories}\label{mean-prior-growth-trajectories}}

We now explore whether the priors we estimate produce reasonable and
appropriately uncertain mean growth trajectories \emph{a priori}. This
is similar to our requirement that priors are faithful to the supplied
information, but is more general as it considers all possible values of
\(t\). Figure \ref{fig:regression_prior_pred} suggests that both the
covariate-independent and covariate-specific targets yield plausible
mean growth trajectories. However, the covariate-independent priors are
significantly more uncertain, resulting in implausible heights having
\emph{a priori} support. This contrasts with the covariate-specific
priors, which interpolate between the supplied targets with an
acceptable degree of uncertainty. It is interesting to note that the
covariate-specific target has similar levels of uncertainty across all
ages, further suggesting that the model may not be flexible enough to
simultaneously match all the targets when they have varying variance, as
here. All 5 of the priors from \citet{hartmann_flexible_2020}, for a
narrower uncertainty interval, are implausible in both shape and width
when viewed on this scale. It also seems unlikely that these priors
accurately reflect the information provided by the experts in
\citet{hartmann_flexible_2020}, but this information is not reported.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/regression-prior-preds} 

}

\caption{\textbf{Prior} predictive for the model without noise $\pd(h(t; \theta) \mid \lambda^{*})$ for each replicate/user from Hartmann et al.~(top right panel), the covariate-independent target (middle row) and covariate-specific target (bottom row) for the multi-objective and single objective settings (left column and right column respectively). Note that this quantity does not include measurement error. Solid lines depict the mean, with the grey regions representing the 95\% prior predictive intervals, \textit{except} for the Hartmann panel, where the intervals are only \textcolor{mywhwlow}{75\%} wide for visualisation purposes. The y-axis is truncated to $(70, 200)$ and uncertainty intervals are also truncated to this range. The red lines in the covariate row correspond to our supplied $\tp(Y \mid X_{r})$ densities, and represent the same information as in Figure \ref{fig:cov_target_discreps}.}\label{fig:regression_prior_pred}
\end{figure}

\hypertarget{posterior-stability}{%
\subsubsection{Posterior stability}\label{posterior-stability}}

Figure \ref{fig:warnings_all_priors} displays for each of the 93
individuals in the growth data whether \texttt{Stan} emits a warning
message when estimating the posterior. The flat prior consistently
produces posteriors that emit warnings, with some individuals
particularly prone to warning messages (i.e.~warnings are very
correlated within individual columns), suggesting that their data are
less informative than other individuals. Warnings are correlated within
rows for the Hartmann et. al.~priors, indicating that some of the priors
are more suitable (replications 1 and 5) for the individuals in the
\texttt{growth} data. Amongst our results we note that the
covariate-specific approach produces fewer warnings than the
covariate-independent approach in both the single- or multi-objective
cases. This reflects the additional information available in the
covariate-specific setting, and that this information results in more
plausible priors. Warnings are particularly correlated within specific
priors (i.e.~across rows) for the covariate-independent approach,
suggesting that these priors are inappropriate for many individuals. The
multi-objective approach (third row of Figure
\ref{fig:warnings_all_priors}) produces a small number of additional
warnings above the equivalent single-objective approach (fifth row of
Figure \ref{fig:warnings_all_priors}), illustrating the trade-off
between single-objective priors that are as informative as possible, and
multiple-objective priors that are slightly less informative and thus
perform less well in settings where strong prior information is
required.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/fda-all-any-warnings-plot} 

}

\caption{Presence/absence of \texttt{Stan} warnings for all individuals (columns) in the FDA package \texttt{growth} data and replicate prior estimates (rows). Each replicate corresponds to a run of the optimisation process and thus a different prior, except the flat, improper prior which is identical for each replicate.}\label{fig:warnings_all_priors}
\end{figure}

\hypertarget{uniqueness-of-marginal-priors-and-posteriors}{%
\subsubsection{Uniqueness of marginal priors and
posteriors}\label{uniqueness-of-marginal-priors-and-posteriors}}

Figure \ref{fig:small_cov_prior_post} shows the priors for
\((h_{0}, \delta_{s}) \in \theta\) for 30 replicates with the
covariate-specific target. The priors for our covariate-specific target
exhibit substantial variability. There appear to be two distinct
unimodal priors for \(h_{0}\) with similar loss, suggesting that
\(\tc(Y \mid \boldsymbol{X})\) does not provided enough information to
uniquely determine a prior distribution. However both priors are
significantly broader than the Hartmann et.~al.~priors. The marginal
priors are slightly more consistent for \(\delta_{s}\) but variability
persists.

Figure \ref{fig:small_cov_prior_post} also shows the posteriors for
these parameters when using the (uninformative and thus challenging)
data from individual \(n = 26\). The flat prior produces a multimodal
posterior\footnote{We run \texttt{Stan} with the default 4 chains to
  detect convergence warnings for Figure \ref{fig:warnings_all_priors},
  but in Figure \ref{fig:small_cov_prior_post} we plot only one chain
  per replicate to better highlight multi-modal posteriors.} for our
parameters of interest, demonstrating the lack of stability when
computing the posterior and the need for some prior information. As a
result, the posteriors are under all methods strongly depend on the
prior distribution used, which as noted is not stable under any method
here. However, all the possibility of a posterior for \(\delta_{s}\)
with significant mass above 2 is removed by both of our priors, as is
desirable, because such solutions correspond to extremely fast growth
spurts that are physiologically implausible and are unsupported by the
data from individual \(n = 26\).

The priors and posteriors for the complete set of parameters \(\theta\)
are displayed in Appendix
\ref{additional-information-for-the-preece-baines-example} {[}Figures
\ref{fig:pb_pop_prior_post_compare} and
\ref{fig:pb_cov_prior_post_compare}{]}.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/small-cov-priors-posteriors} 

}

\caption{A comparison of the priors (\textcolor{mymidblue}{blue}) produced for $h_{0}$ by our method using the covariate-specific target (top right) using the multiple objective function ($\kappa = 0.2$) and the single objective function ($\kappa = \text{NA}$, bottom right); Hartmann et al. (2020) (bottom left), and with no prior displayed for the flat prior scenario (top left). The corresponding posteriors for individual $n = 26$ under each of these priors are displayed in (\textcolor{myredhighlight}{red}).}\label{fig:small_cov_prior_post}
\end{figure}

\hypertarget{example-summary-2}{%
\subsection{Example summary}\label{example-summary-2}}

The priors estimated by our procedure in this example are broadly
faithful to the supplied information, but appear to be constrained by
the model's inflexibility that makes it difficult to simultaneously
match the \(t = 2\) and \(t = 18\) targets in the covariate-specific
case. They regularise the posterior sufficiently enabling accurate
posterior sampling (model inadequacy notwithstanding), with the
covariate-specific, multi-objective method proving most useful, but are
arguably over concentrated and occasionally prevent the model from
fitting the data well. Uniqueness is improved by our secondary
objective, but perfect uniqueness across all replicates remains illusive
and may not be possible with only the information provided in
\(\tc(Y \mid \boldsymbol{X})\). We observe a small improvement in
uniqueness attributable to the secondary objective (see Appendix
\ref{additional-information-for-the-preece-baines-example} {[}Figures
\ref{fig:pb_pop_prior_post_compare} and
\ref{fig:pb_cov_prior_post_compare}{]}); some of the variability may be
a result of lack of perfect replicability of the optimisation rather
than non-uniqueness.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this paper we develop methodology, and software, for specifying
priors given predictive information about an observable or model-derived
quantity. We employ a CDF-based, multi-objective global optimisation
approach to this translation problem to make our approach widely
applicable. Adopting a global optimisation approach allows any kind of
model to be specified and optimised, not just those for which we can
compute reparameterisation gradients. The global optimisation approach
also allows us to provide our functionality directly in \texttt{R}, with
which we envisage the majority of the users of our method will be
familiar. Our CDF-based predictive discrepancy is also generic as it
permits continuous, discrete, and mixed observable types. We apply our
methodology in three challenging example models, each of which we
interrogate for faithfulness, identifiability, and uniqueness. Each
example is of moderate dimension (3--17), with each representing a
difficult structural elicitation problem. Finally, our delineation
between elicitation and translation, with our emphasis on the latter, is
a contribution to an under-explored area of prior specification.

Our inspiration, for both methodology and examples, arises from
applications and models we have encountered in applied work. The
Preece-Baines model is a typical complex, nonlinear regression model for
an intuitive and well understood observable quantity, but for which the
model parameters are not easily understood. Setting a prior for models
congruent with our knowledge is difficult without a method for
translation such as we have proposed. We previously considered a
survival model similar to the cure fraction model, where we knew \emph{a
priori} the fraction of cured/censored observations and a distribution
of likely survival times, in our earlier work
\citep{manderson_combining_2022}. Setting an appropriate prior for this
model proved challenging, and would have benefited greatly from the
translation methodology introduced in this paper. Finally, prior
knowledge on \(R^{2}\) has proved a valuable mathematical basis for the
R2-D2 shrinkage prior \citep{zhang_bayesian_2022}, and more generally
there are numerous model-derived quantities about which practitioners
possess prior information. Methods for translation are valuable in this
setting, as information about such model-derived quantities is often
difficult to express. An envisaged future example of this type considers
clustering models. In that setting we elicit an informative prior for
number of clusters, or the typical size of a cluster, which are derived
from the clustering model. Such quantities are readily reasoned about by
experts, as opposed to the parameters governing each cluster. Including
this type of information in complex models seems critical for stable and
reliable Bayesian inference.

One limitation of the current work is that we only partly address
non-uniqueness for flexible models and uninformative target
distributions. In these settings we emphases that our methodology
remains valuable as a means to assess the implications of a particular
target distribution on a specific model. For specific model-target pairs
where uniqueness remains challenging, our methodology can still provide
useful insight into consequences of certain
\(\tc(Y \mid \boldsymbol{X})\). We can delineate between components of
\(\lambda\) that are well identified and thus consistently estimated,
and those that are not. Furthermore, we can directly inspect
\(\tc(Y \mid X_{r})\) different significantly from
\(\Pd(Y \mid \lambda^{*}, X_{r})\), and consider if such differences are
attributable to model inflexibility or implausible targets. Finally, we
are compelled to assess our choice of which components should make up
\(\lambda\), and whether we have other information that we could employ
to fix certain components within \(\lambda\) (e.g.~the fixed prior for
the noise in the human height example). Another limitation that should
be noted is that global optimisation methods lack guarantees of finding
the global optimum in finite time, so the the performance of this
optimisation process to other problems is unclear.

\hypertarget{acknowledgments-and-data-availability}{%
\section*{Acknowledgments and data
availability}\label{acknowledgments-and-data-availability}}
\addcontentsline{toc}{section}{Acknowledgments and data availability}

We thank Daniela De Angelis and Mevin Hooten for their feedback on an
earlier version of this manuscript.

This work was supported by The Alan Turing Institute under the UK
Engineering and Physical Sciences Research Council (EPSRC)
{[}EP/N510129/1{]} and the UK Medical Research Council {[}programme
codes MC\_UU\_00002/2 and MC\_UU\_00002/20{]}. No original data were
generated as part of this study, the \texttt{growth} data used in
Section \ref{a-human-aware-prior-for-a-human-growth-model} are available
as part of the \texttt{fda} package for \texttt{R}
\citep{ramsay_fda_2022} available on \texttt{CRAN}
(\url{https://cran.r-project.org/}). For the purpose of open access, the
author has applied a Creative Commons Attribution (CC BY) licence to any
Author Accepted Manuscript version arising.

\newpage

\renewcommand{\thesubsection}{\Alph{subsection}}
\setcounter{subsection}{0}

\hypertarget{appendices}{%
\section*{Appendices}\label{appendices}}
\addcontentsline{toc}{section}{Appendices}

\hypertarget{importance-sampling}{%
\subsection{Importance sampling}\label{importance-sampling}}

Appropriate importance distributions are crucial to obtaining an
accurate and low variance estimate of
\(D(\lambda \mid \boldsymbol{X})\). For values of \(\lambda\) far from
optimal, \(\Pd(Y \mid \lambda, \boldsymbol{X})\) can differ considerably
from \(\tc(Y \mid \boldsymbol{X})\). Given a specific \(X_{r}\) we
require an importance distribution \(\Q(Y \mid X_{r})\) that places
substantial mass in the high probability regions of both
\(\tc(Y \mid X_{r})\) and \(\Pd(Y \mid \lambda, X_{r})\), as it is in
these regions that \(d(\cdot, \cdot)\) is largest. But we cannot exert
too much effort on finding these densities as they are specific to each
value of \(\lambda\), and must be found anew for each \(\lambda\).

We use three quantities to guide our choice of \(\Q(Y \mid X_{r})\),
these being the support \(\mathcal{Y}\), the samples
\(\boldsymbol{y}_{r}^{(\Pd)} \sim \Pd(Y \mid \lambda, X_{r})\), and the
samples \(\boldsymbol{y}_{r}^{(\tc)} \sim \tc(Y \mid X_{r})\). Of
primary concern is the support. If \(\mathcal{Y} = \mathbb{R}\) then we
use a mixture of Student-\(t_{5}\) distributions; for
\(\mathcal{Y} = \mathbb{R} = (0, \infty)\) we employ a mixture of gamma
distributions; and for \(\mathcal{Y} = (0, a]\) with known \(a\), we opt
for a mixture of Beta distributions with a discrete component at
\(Y = a\). The parameters of the mixture components are estimated using
the method of moments. Specifically, denoting the empirical mean of
\(\boldsymbol{y}_{r}^{(\Pd)}\) as \(\hat{\mu}^{(\Pd)}\) and the
empirical variance by \(\hat{v}^{(\Pd)}\), with \(\hat{\mu}^{(\tc)}\)
and \(\hat{v}^{(\tc)}\) defined correspondingly for
\(\boldsymbol{y}_{r}^{(\tc)}\), Table
\ref{tab:importance-sampling-appendix-table} details our method of
moments estimators for the mixture components.

In this paper we limit ourselves to one dimensional \(\mathcal{Y}\),
where importance sampling is mostly well behaved or can be tamed using a
reasonable amount of computation. This covers many models, and with the
covariate-specific target it includes regression models. It is harder to
elicit \(\tc(Y \mid \boldsymbol{X})\) for higher dimensional data
spaces, and the difficulties with higher dimensional importance sampling
are well known.

\input{tex-input/pbbo-methodology/0071-importance-sampling-appendix-table.tex}

\hypertarget{evaluating-dlambda-mid-boldsymbolx}{%
\subsection{\texorpdfstring{Evaluating
\(D(\lambda \mid \boldsymbol{X})\)}{Evaluating D(\textbackslash lambda \textbackslash mid \textbackslash boldsymbol\{X\})}}\label{evaluating-dlambda-mid-boldsymbolx}}

For both numerical stability and optimisation performance
\citep{eriksson_scalable_2021, snoek_input_2014} we evaluate
\(D(\lambda \mid \boldsymbol{X})\) on the log scale. This is because far
from optimal values of \(\lambda\) have corresponding
\(D(\lambda \mid \boldsymbol{X})\) many orders of magnitude larger than
near optimal values of \(\lambda\). Furthermore, the Gaussian process
approximation that underlies Bayesian optimisation assumes constant
variance, necessitating a log or log-like transformation.

Suppose again that we sample
\(\boldsymbol{y}_{r}^{(\Pd)} \sim \Pd(Y \mid \lambda, X_{r})\), from
which we form the ECDF
\(\hat{\Pd}(Y \mid \lambda, X_{r}, \boldsymbol{y}_{r}^{(\Pd)})\). We
also select an appropriate importance distribution \(\Q(Y \mid X_{r})\)
and density \(\q(Y \mid X_{r})\) using Appendix
\ref{importance-sampling}, and sample importance points
\((y_{i, r})_{i = 1}^{I_{r}} \sim \Q(Y \mid X_{r})\). Define the
intermediary quantity \(z(y_{i, r})\) as
\input{tex-input/pbbo-methodology/0020-log-discrep-func-defs.tex}\noindent
and then rewrite Equation
\eqref{eqn:practical-discrep-definition-covariate-importance} to read
\input{tex-input/pbbo-methodology/0022-importance-discrepancy-covariate-definition.tex}\noindent
All \(\log(\sum \exp\{\cdot\})\) terms are computed using the
numerically stable form \citep{blanchard_accurately_2021}.

Accurately evaluating \(\log(d(\cdot, \cdot))\) in Equation
\eqref{eqn:log-discrep-func-def} involves managing the discrete nature
of the ECDF (that it returns exactly zero or one for some inputs), and
using specialised functions for each discrepancy to avoid issues with
floating point arithmetic. We compute
\(\log(d^{\text{CvM}}(\cdot, \cdot))\) using
\input{tex-input/computing-log-discrep-functions/0010-computing-log-cvm.tex}\noindent
where \(\mathcal{T}(y_{i, r}) = \log(\tc(y_{i, r}))\). The log-CDF
(LCDF) is often more numerically accurate for improbable values of
\(y_{i, r}\), and so our methodology assumes that it is this LCDF form
in which the target distribution is supplied. However, because the ECDF
can return exact zero/one values there is no way to perform this
computation on the log scale. We thus employ high precision floating
point numbers when exponentiating the LCDF values, using \texttt{Rmpfr}
\citep{maechler_rmpfr_2021}, to avoid evaluating \(\log(0)\).

For \(\log(d^{\text{AD}}(\cdot, \cdot))\), additional care must be taken
as the denominator of \(d^{\text{AD}}\) in Equation
\eqref{eqn:discrepancies-definitions} tends to underflow to zero. Thus
we evaluate it using
\input{tex-input/computing-log-discrep-functions/0011-computing-log-ad.tex}\noindent
where \(\texttt{log1mexp}(x) = \log(1 - \exp\{-x\})\) is implemented by
the \texttt{Rmpfr} package \citep{maechler_accurately_2012}. Such
precision is necessary for improbably large values of \(y_{i, r}\) under
\(\tc\), as the CDF/LCDF often rounds to 1/0 (respectively). It is not
always feasible to evaluate Equation \eqref{eqn:computing-log-ad} with
sufficient accuracy to avoid under/over-flow issues -- it requires a
high-precision implementation of \(\mathcal{T}(y_{i, r})\) for extreme
\(y_{i, r}\) and many additional bits of precision for both \(y_{i, r}\)
and the result. In these settings we revert to
\(\log(d^{\text{CvM}}(\cdot, \cdot))\).

\hypertarget{further-notes-on-choosing-kappa}{%
\subsection{\texorpdfstring{Further notes on choosing
\(\kappa\)}{Further notes on choosing \textbackslash kappa}}\label{further-notes-on-choosing-kappa}}

Advantages of multi-objective optimisation are most immediately apparent
when the scales of our objectives differ markedly. Consider the
equivalent linearised approach, where we select \(\kappa\) \emph{before}
optimisation and directly optimise
\(\tilde{L}(\lambda \mid \boldsymbol{X})\). It is generally not possible
to know the range of the values of
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) and
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) before optimisation.
Selecting an appropriate \(\kappa\) without this knowledge is
prohibitively difficult, leaving only the computationally expensive
trial-and-error approach -- where we re-run the optimiser for each new
possible value of \(\kappa\) -- as a plausible strategy for choosing
\(\kappa\). In contrast, given \(\mathcal{P}\) it is computationally
trivial to recompute \(\lambda^{*}\) for many possible values of
\(\kappa\) \emph{after} optimisation (e.g.~each panel of Figure
\ref{fig:kappa_cov} is trivial to compute). We can thus select
\(\kappa\) in a problem-specific manner for practically no additional
computational cost to that of the multi-objective optimiser. Note that
the multi-objective optimisation approach is more expensive that the
linearised approach, but this additional cost is dwarfed by the number
of re-runs of the latter typically required to select \(\kappa\).

\hypertarget{algorithmic-descriptions-of-the-optimisation-process}{%
\subsection{Algorithmic descriptions of the optimisation
process}\label{algorithmic-descriptions-of-the-optimisation-process}}

\hypertarget{crs2-as-an-initialiser-for-bayesian-optimisation}{%
\subsubsection{CRS2 as an initialiser for Bayesian
optimisation}\label{crs2-as-an-initialiser-for-bayesian-optimisation}}

Algorithm \ref{alg:crs2-algorithmic-description} describes our use of
CRS2 \citep{kaelo_variants_2006} to obtain a suitable design to
initialise the Bayesian multi-objective optimisation approach in step 2.

\input{tex-input/pbbo-methodology/0052-crs2-algorithmic-description.tex}

\noindent

\hypertarget{mspot}{%
\subsubsection{MSPOT}\label{mspot}}

Algorithm \ref{alg:mspot-algorithmic-description} describes, in our
notation, the MSPOT \citep{zaefferer_mspot_2012} algorithm for two
objectives. Note that within the algorithm we suppress each objective's
dependence on \(\boldsymbol{X}\) for brevity.

\input{tex-input/pbbo-methodology/0053-mspot-algorithmic-description.tex}

\hypertarget{inter-batch-resampling}{%
\subsubsection{Inter batch resampling}\label{inter-batch-resampling}}

Algorithm \ref{alg:resample-batch-algorithmic-description} describes our
inter-batch resampling algorithm that we occasionally adopt in stage two
of our optimisation process.

\input{tex-input/pbbo-methodology/0054-resample-batch-algorithmic-description.tex}

\FloatBarrier

\hypertarget{additional-information-for-the-cure-fraction-survival-example}{%
\subsection{Additional information for the cure fraction survival
example}\label{additional-information-for-the-cure-fraction-survival-example}}

\hypertarget{hyperparameter-support-lambda}{%
\subsubsection{\texorpdfstring{Hyperparameter support
\(\Lambda\)}{Hyperparameter support \textbackslash Lambda}}\label{hyperparameter-support-lambda}}

See Table \ref{tab:surv-cap-lambda-def}

\input{tex-input/surv-example/0022-surv-cap-lambda-def.tex}

\hypertarget{pareto-frontiers-for-cramuxe9r-von-mises-discrepancy}{%
\subsubsection{Pareto Frontiers for CramÃ©r-von Mises
discrepancy}\label{pareto-frontiers-for-cramuxe9r-von-mises-discrepancy}}

See Figure \ref{fig:surv_ex_pf_cvm}.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/all-pareto-fronts-by-kappa-cvm} 

}

\caption{Pareto frontiers for the survival example using the CramÃ©r-von Mises discrepancy, for the values of $\kappa$ we consider. Note that the colour scale displaying loss is panel-specific. The red crosses ($\color{myredhighlight}{+}$) indicate the minimum loss point on each frontier, for each value of $\kappa$.}\label{fig:surv_ex_pf_cvm}
\end{figure}

\FloatBarrier

\hypertarget{additional-information-for-the-r2-example}{%
\subsection{\texorpdfstring{Additional information for the \(R^{2}\)
example}{Additional information for the R\^{}\{2\} example}}\label{additional-information-for-the-r2-example}}

\hypertarget{hyperparameter-support-lambda-faithfulness-experiment}{%
\subsubsection{\texorpdfstring{Hyperparameter support \(\Lambda\) --
faithfulness
experiment}{Hyperparameter support \textbackslash Lambda -- faithfulness experiment}}\label{hyperparameter-support-lambda-faithfulness-experiment}}

See Table \ref{tab:cap-lambda-def}. Note that for the Dirichlet-Laplace
prior, \citet{zhang_variable_2018} suggest bounding
\(\alpha \in [(\max(n, p))^{-1}, 1 \mathop{/} 2]\). In our experiments
we regularly encountered optimal values of \(\alpha\) on the lower
boundary, so we use instead \(1 \mathop{/} (3\max(n, p))\) as a lower
bound.

\input{tex-input/r2-examples/0014-cap-lambda-def.tex}

\hypertarget{a-comparison-to-an-asymptotic-result}{%
\subsubsection{A comparison to an asymptotic
result}\label{a-comparison-to-an-asymptotic-result}}

The poor fit for the Gaussian prior observed in Figure
\ref{fig:r2_roundtrip_full} could be attributed to issues in the
optimisation process, or to the lack of flexibility in the prior. To
investigate, we compare the results for \(\lambda_{\text{GA}}\) to
Theorem 5 of \citet{zhang_variable_2018}, which is an asymptotic result
regarding the optimal value of \(\lambda_{GA}\) for a target
\(\text{Beta}(s_{1}, s_{2})\) density for \(R^{2}\). We compare pairs of
\((n_{k}, p_{k})\) for \(k = 1, \ldots, 5\), noting that assumption (A4)
of Zhang and Bondell requires that \(p_{k} = \text{o}(n_{k})\) as
\(k \rightarrow \infty\) (for strictly increasing sequences \(p_{k}\)
and \(n_{k}\)). Thus we consider values of \(p\) such that
\(p_{1} = 80\) with \(p_{k} = 2p_{k - 1}\) and \(n\) with \(n_{1} = 50\)
and \(n_{k} = n_{k - 1}^{1.2}\), both for \(k = 2, \ldots, 5\). Each
\((n_{k}, p_{k})\) pair is replicated 20 times, and for each replicate
we generate a different \(\boldsymbol{X}\) matrix with standard normal
entries. As the target density we choose \(s_{1} = 5, s_{2} = 10\) -- a
``more Gaussian'' target than previously considered and thus, we
speculate, possibly more amenable to translation with a Gaussian prior
for \(\beta\). We also use this example as an opportunity to assess if
there are notable differences between the CramÃ©r-Von Mises discrepancy
and the Anderson-Darling discrepancy as defined in Equation
\eqref{eqn:discrepancies-definitions}. The support \(\Lambda\) for
\(\lambda_{\text{GA}}\) differs slightly from the example in the main
text, and is defined in Table \ref{tab:cap-lambda-def-asymp}, as
matching our target with larger design matrices requires considerably
larger values of \(\gamma\).

The computation of \(R^{2}\) becomes increasingly expensive as \(n_{k}\)
and \(p_{k}\) increase, which limits the value of some of our method's
tuning parameters. The approximate discrepancy function uses
\(S = 2000\) samples from the prior predictive and is evaluated using
\(I = 500\) importance samples. We run CRS2 for
\(N_{\text{CRS2}} = 500\) iterations, using \(N_{\text{design}} = 50\)
in the initial design for the subsequent single batch of Bayesian
optimisation, which uses \(N_{\text{BO}} = 100\) iterations.

\input{tex-input/r2-examples/0015-cap-lambda-def-aysmp.tex}

\hypertarget{results-3}{%
\paragraph{Results}\label{results-3}}

Figure \ref{fig:r2_asymp_plot} displays the results in terms of the
normalised difference between the \(\gamma\) we estimate
\(\gamma_{\text{pbbo}}^{*}\), and the asymptotic result of Zhang and
Bondell \(\gamma_{\text{asym}}^{*}\). Our typical finite sample estimate
is slightly larger than the asymptotic result, and the difference
increases with \(n_{k}\) and \(p_{k}\). The variability of the
normalised difference remains roughly constant, and thus reduces on an
absolute scale, though extrema seem to occur more frequently for larger
\(n_{k}\) and \(p_{k}\). These simulations suggest that the asymptotic
regime has not been reached even at the largest \(n_{k}\) and \(p_{k}\)
values we assessed.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/gamma-diff-plot} 

}

\caption{Relative difference between the value of $\gamma$ obtained using our methodology ($\gamma_{\text{pbbo}}^{*}$) and Theorem 5 of Zhang and Bondell (2018) ($\gamma_{\text{asym}}^{*}$).}\label{fig:r2_asymp_plot}
\end{figure}

The estimates of \(\gamma\) are not themselves particularly
illuminating: we should instead look for differences in the distribution
of \(R^{2}\) at the optima, which is to say on the ``data'' scale.
Figure \ref{fig:r2_target_vs_opt_prior} displays the target distribution
and the prior predictive distribution at the optima
\(\pd(R^{2} \mid \lambda^{*}_{GA})\). The fit is increasingly poor as
\(n\) and \(p\) increase, and there is little difference both between
the two discrepancies and with each discrepancies replications. The lack
of difference implies that the optimisation process is consistently
locating the same minima for \(D(\lambda)\). We conclude that either 1)
the ability of the model to match the target depends on there being
additional structure in \(\boldsymbol{X}\), or 2) it is not possible to
encode the information in a \(\text{Beta}(5, 10)\) prior for \(R^{2}\)
into the Gaussian prior.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/optimal-pf-draws-plot} 

}

\caption{The target density $\tp(R^{2})$ and optimal prior predictive densities $\pd(R^{2} \mid \lambda^{*})$ under both the CramÃ©r-von Mises (red, left column) and Anderson-Darling (blue, right column) discrepancies. There are 20 replicates of each discrepancy in this plot.}\label{fig:r2_target_vs_opt_prior}
\end{figure}

This example also further illustrates the difficulties inherent in
acquiring a prior for additive noise terms. Specifically, in this
example it is difficult to learn \((a_{1}, b_{1})\), despite the fact
that the contribution of \(\sigma^{2}\) to Equation
\eqref{eqn:r2-definition} is not purely additive. However, as we see in
Figure \ref{fig:r2_noise_hypers_plot}, estimates are uniformly
distributed across the permissible space, except for bunching at the
upper and lower bounds of \(\Lambda\). Note that for numerical and
computational stability, we constrain \(a_{1} \in (2, 50]\) and
\(b_{1} \in (0.2, 50]\) in this example. This contrasts with similarity
between replicates visible in Figure \ref{fig:r2_target_vs_opt_prior},
and is thus evidence that \((\hat{a}_{1}, \hat{b}_{1})\) have no
apparent effect on the value of \(D(\lambda^{*})\). We should instead
set the prior for \(\sigma^{2}\) based on external knowledge of the
measurement process for \(Y\).

\begin{figure}

{\centering \includegraphics{plots/r2-examples/normal-noise-hyperpars-plot} 

}

\caption{Histograms of \textit{scaled} estimates of $(a_{1}^{*}, b_{1}^{*})$ for the settings considered in Section \ref{a-comparison-to-an-asymptotic-result}. Estimates have been scaled to $[0, 1]$ for visualisation purposes using the upper and lower limits defined in Table \ref{tab:cap-lambda-def}.}\label{fig:r2_noise_hypers_plot}
\end{figure}

The regularisation method we employ in the two other examples in the
main text is unlikely to assist in estimating \((a_{1}, b_{1})\).
Promoting a larger mean log marginal standard deviation, with the
knowledge \(D(\lambda)\) is insensitive to the value of
\((a_{1}, b_{1})\), would simply pick the largest possible value for
\(b_{1}^{2} \mathop{/} \left((a_{1} - 1)^{2}(a_{1} - 2)\right)\), which
occurs when \(a_{1}\) is at its minimum allowable value and \(b_{1}\)
its corresponding maximum.

\hypertarget{full-faithfulness-results}{%
\subsubsection{Full faithfulness
results}\label{full-faithfulness-results}}

The complete results from the faithfulness experiment are displayed in
Figure \ref{fig:r2_roundtrip_full_supp}.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/roundtrip-target-plot-big} 

}

\caption{As in Figure \ref{fig:r2_roundtrip_full} but for all values of $(s_{1}, s_{2})$ denoted in the facet panels titles. The performance of the regularised horseshoe is superior to the Dirichlet-Laplace, both of which are vast improvements over the Gaussian.}\label{fig:r2_roundtrip_full_supp}
\end{figure}

\FloatBarrier

\hypertarget{additional-information-for-the-preece-baines-example}{%
\subsection{Additional information for the Preece-Baines
example}\label{additional-information-for-the-preece-baines-example}}

\hypertarget{details-for-tcy-and-tcy-mid-x_r}{%
\subsubsection{\texorpdfstring{Details for \(\tc(Y)\) and
\(\tc(Y \mid X_{r})\)}{Details for \textbackslash tc(Y) and \textbackslash tc(Y \textbackslash mid X\_\{r\})}}\label{details-for-tcy-and-tcy-mid-x_r}}

Denote with \(\text{Gamma}(Y; \alpha, \beta)\) the CDF of the gamma
distribution with shape parameter \(\alpha\) and rate \(\beta\); and
\(\text{Normal}(Y; \xi, \omega^{2})\) the CDF of the normal distribution
with mean \(\xi\) and standard deviation \(\omega\). We define the
covariate-independent target
\input{tex-input/preece-baines-growth/0021-target-definition-pop.tex}\noindent
and the covariate-specific target
\input{tex-input/preece-baines-growth/0022-target-definition-cov.tex}\noindent

\hypertarget{hyperparameter-support-lambda-1}{%
\subsubsection{\texorpdfstring{Hyperparameter support
\(\Lambda\)}{Hyperparameter support \textbackslash Lambda}}\label{hyperparameter-support-lambda-1}}

Table \ref{tab:pb-cap-lambda-def} contains the upper and lower limits
for each hyperparameter, thus defining the feasible region \(\Lambda\).

\input{tex-input/preece-baines-growth/0011-pb-cap-lambda-def.tex}

\noindent

\hypertarget{hartmann_flexible_2020-priors}{%
\subsubsection{\texorpdfstring{\citet{hartmann_flexible_2020}
priors}{@hartmann\_flexible\_2020 priors}}\label{hartmann_flexible_2020-priors}}

Table \ref{tab:hartmann-priors-data} contains the priors elicited by
\citet{hartmann_flexible_2020} (extracted from the supplementary
material of that paper) for the parameters in the Preece-Baines example.
To generate the prior predictive samples displayed in Figure
\ref{fig:regression_prior_pred} we draw, for each user, \(\theta\) from
the corresponding lognormal distribution then compute \(h(t; \theta)\)
using Equation \eqref{eqn:preece-baines-model-definition-two} (without
the error term) and 250 values of \(t\) spaced evenly between ages \(2\)
and \(18\).

\input{tex-input/preece-baines-growth/0031-hartmann-priors-data.tex}

\hypertarget{pareto-frontiers-for-the-covariate-independent-target}{%
\subsubsection{Pareto frontiers for the covariate-independent
target}\label{pareto-frontiers-for-the-covariate-independent-target}}

The Pareto frontier for the covariate-independent target and all values
of \(\kappa \in \mathcal{K}\) is displayed in Figure
\ref{fig:kappa_pop}.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/pop-kappa} 

}

\caption{Pareto frontiers for each $\kappa \in \mathcal{K}$ for the \textbf{covariate-independent} example. The minimum loss point for each replicate is plotted with $\color{myredhighlight}{+}$.}\label{fig:kappa_pop}
\end{figure}

\hypertarget{full-marginal-prior-and-posterior-comparison-plots}{%
\subsubsection{Full marginal prior and posterior comparison
plots}\label{full-marginal-prior-and-posterior-comparison-plots}}

Figures \ref{fig:pb_pop_prior_post_compare} and
\ref{fig:pb_cov_prior_post_compare} are extended versions of Figure
\ref{fig:small_cov_prior_post}, and display the prior and posterior
estimates for all the parameters in \(\theta\). Consistency and
uniqueness remain, evidently, challenging and as yet unobtainable.

\begin{landscape}
\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/pop-priors-posteriors-compare} 

}

\caption{A comparison of the priors (\textcolor{mymidblue}{blue}) produced by our method using the covariate-independent marginal target (bottom two rows); and Hartmann et al. (2020) (second row), with no prior displayed for the flat prior scenario. The corresponding posteriors (\textcolor{myredhighlight}{red}) for individual $n = 26$ under each of these priors are displayed as dashed lines. Note that y-axes change within columns and are limited to values that clip some of the priors/posteriors for readability.}\label{fig:pb_pop_prior_post_compare}
\end{figure}

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/cov-priors-posteriors-compare} 

}

\caption{Otherwise identical to Figure \ref{fig:pb_pop_prior_post_compare} but the bottom two rows display the results obtained using the covariate-specific target.}\label{fig:pb_cov_prior_post_compare}
\end{figure}
\end{landscape}

\FloatBarrier

  \bibliography{bibliography/prior-setting.bib}

\end{document}
