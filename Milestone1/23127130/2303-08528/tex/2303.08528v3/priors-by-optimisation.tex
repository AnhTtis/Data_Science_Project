% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
% !TeX program = pdfLaTeX
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage[]{natbib}
\usepackage{textcomp}


%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

%% load any required packages here



% tightlist command for lists without linebreak
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



\usepackage{amsmath,amssymb,mathtools,bbm}
% I always seem to need tikz for something
\usepackage{tikz}
\usetikzlibrary{positioning, shapes, intersections, through, backgrounds, fit, decorations.pathmorphing}

\usepackage{algorithm}
\usepackage{algpseudocode}


% \usepackage{lineno}
% % \linenumbers

% \usepackage{xr}
% \makeatletter
% \newcommand*{\addFileDependency}[1]{% argument=file name and extension
%   \typeout{(#1)}
%   \@addtofilelist{#1}
%   \IfFileExists{#1}{}{\typeout{No file #1.}}
% }
% \makeatother

% \newcommand*{\myexternaldocument}[1]{%
%     \externaldocument{#1}%
%     \addFileDependency{#1.tex}%
%     \addFileDependency{#1.aux}%
% }

% \myexternaldocument{priors-by-optimisation-supplement}

% required for landscape pages. beware, they make build times very long.
\usepackage{pdflscape}
\usepackage{placeins}

% table - `gt' package uses these, often unimportant
%\usepackage{longtable}
\usepackage{booktabs}
%\usepackage{caption}
\usepackage{colortbl}

\usepackage{color}
\definecolor{myredhighlight}{RGB}{180, 15, 32}
\definecolor{mydarkblue}{RGB}{0, 33, 79}
\definecolor{mymidblue}{RGB}{44, 127, 184}
\definecolor{mylightblue}{RGB}{166, 233, 255}
\definecolor{mywhwlow}{RGB}{234, 164, 99}

\usepackage{accents}
\newlength{\dhatheight}
\newcommand{\doublehat}[1]{%
    \settoheight{\dhatheight}{\ensuremath{\hat{#1}}}%
    \addtolength{\dhatheight}{-0.35ex}%
    \hat{\vphantom{\rule{1pt}{\dhatheight}}%
    \smash{\hat{#1}}}}

\setcounter{secnumdepth}{3}

% \renewcommand{\floatpagefraction}{0.8}
% \renewcommand{\topfraction}{0.8}
% \renewcommand{\bottomfraction}{0.8}
% \renewcommand{\textfraction}{0.25}

% pd stands for: probability distribution and is useful to distringuish
% marignals for probabilities specifically p(p_{1}) and the like.
\newcommand{\pd}{\text{p}}
\newcommand{\Pd}{\text{P}}
\newcommand{\q}{\text{q}}
\newcommand{\Q}{\text{Q}}
\newcommand{\w}{\text{w}}
\newcommand{\pdr}{\text{r}}
\newcommand{\pdrh}{\hat{\text{r}}}

% pbbo stuff
\newcommand{\tc}{\text{T}}
\newcommand{\tp}{\text{t}}

% melding
\newcommand{\ppoolphi}{\pd_{\text{pool}}(\phi)}
\newcommand{\ppool}{\pd_{\text{pool}}}
\newcommand{\pmeld}{\pd_{\text{meld}}}

% the q(x)w(x), "weighted target" density 
% for the moment I'm going to call it s(x), as that is the next letter of the 
% alphabet. Can change it later
\newcommand{\s}{\text{s}}
% direct density estimate - replaces lambda.
\newcommand{\ddest}{\text{s}}
% target weighting function
\newcommand{\tarw}{\text{u}}

% constants - usually sizes of things
\newcommand{\Nx}{N}
\newcommand{\Nnu}{\text{N}_{\text{nu}}}
\newcommand{\Nde}{\text{N}_{\text{de}}}
\newcommand{\Nmc}{\text{N}_{\text{mc}}}
\newcommand{\Nw}{W}
\newcommand{\Nm}{M}
\newcommand{\Ns}{S}

% locales - could switch to x and x'
\newcommand{\xnu}{x_{\text{nu}}}
\newcommand{\xde}{x_{\text{de}}}
\newcommand{\phinu}{\phi_{\text{nu}}}
\newcommand{\phide}{\phi_{\text{de}}}

% sugiyama stuff
\newcommand{\pdnu}{\pd_{\text{nu}}}
\newcommand{\pdde}{\pd_{\text{de}}}

% indices 
\newcommand{\wfindex}{w}
\newcommand{\sampleindex}{n}
\newcommand{\modelindex}{m}
\newcommand{\stageindex}{s}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\newcommand{\kldiv}{D_{\text{KL}}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\newcommand{\lse}{\text{logSumExp}}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\alglinenumber[1]{
    {\sf\footnotesize\color{lightgray}#1}}
\algrenewcommand\algorithmicrequire{\textbf{Inputs:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}

\def\dodoi#1{doi: \href{https://doi.org/#1}{\nolinkurl{#1}}}
\def\dourl#1{\href{http://#1}{\nolinkurl{#1}}}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\hypersetup{
  pdftitle={Translating predictive distributions into informative priors},
  pdfkeywords={prior specification, multi-objective optimisation, prior
predictive checks},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}



\begin{document}


\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Translating predictive distributions into informative
priors}

  \author{
        Andrew A. Manderson \thanks{a.manderson@live.co.uk} \\
    MRC Biostatistics Unit, University of Cambridge\\
     and \\     Robert J. B.
Goudie \thanks{robert.goudie@mrc-bsu.cam.ac.uk} \\
    MRC Biostatistics Unit, University of Cambridge\\
      }
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Translating predictive distributions into informative
priors}
  \end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
When complex Bayesian models exhibit implausible behaviour, one solution
is to assemble available information into an informative prior.
Challenges arise as prior information is often only available for the
observable quantity, or some model-derived marginal quantity, rather
than directly pertaining to the natural parameters in our model. We
propose a method for translating available prior information, in the
form of an elicited distribution for the observable or model-derived
marginal quantity, into an informative joint prior. Our approach
proceeds given a parametric class of prior distributions with as yet
undetermined hyperparameters, and minimises the difference between the
supplied elicited distribution and corresponding prior predictive
distribution. We employ a global, multi-stage Bayesian optimisation
procedure to locate optimal values for the hyperparameters. Three
examples illustrate our approach: a cure-fraction survival model, where
censoring implies that the observable quantity is \emph{a priori} a
mixed discrete/continuous quantity; a setting in which prior information
pertains to \(R^{2}\) -- a model-derived quantity; and a nonlinear
regression model.
\end{abstract}

\noindent%
{\it Keywords:} prior specification, multi-objective optimisation, prior
predictive checks

\vfill

\newpage
\spacingset{1.9} % DON'T change the spacing!

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Incorporating prior information in Bayesian models is conceptually easy,
but in practice constructing an informative prior is not easy.
Formulating priors in accordance with predictive information obtained
via predictive elicitation \citep{ohagan_uncertain_2006} is attractive
due to the widespread availability, reliability
\citep{kadane_experiences_1998} and model-agnostic nature of such
information. However it is often unclear how to implement this approach,
particularly for complex, nonlinear, or overparameterised models, for
which informative priors can be essential to exclude model behaviours
that conflict with known properties of the world. In this paper we
suppose predictive information is available in the form of a
\emph{target} prior predictive distribution, and consider how to
\emph{translate} this into a prior distribution for model parameters, a
step that has heretofore received relatively little attention.

One simple approach to this task is to directly model the elicited
quantity. This requires no translation step. For example,
\citet{perepolkin_hybrid_2021} directly updated elicited information in
light of observations using a Bayesian quantile-parameterised
likelihood. Such direct approaches are currently only feasible for
simple models with no latent structure. For models with simple latent
structure, eliciting information about an invertible function of the
parameters may be possible \citep[e.g.][]{chaloner_graphical_1993},
enabling analytic translation into a prior for the parameters.
Translation is also clear for conjugate distributions, since the prior
predictive distribution determines the prior hyperparameter values
\citep{percy_bayesian_2002}. Translation, however, is unclear in general
for nonconjugate models \citep{gribok_backward_2004}. Techniques exist
for specific models with specific latent structures, including for
logistic regression \citep{chen_prior_1999}, contingency table analyses
\citep{good_bayesian_1967} and hierarchical models
\citep{hem_robustifying_2021}, but a model-agnostic approach is needed
for models outwith these classes, as noted by
\citet{mikkola_Prior_2023}.

Our approach to translation builds on the idea of predictive checks
\citetext{\citealp{gabry_visualization_2019}; \citealp[the
``hypothetical future samples'' of][]{winkler_assessment_1967}} and the
Bayesian workflow \citep{gelman_bayesian_2020}, in which the prior is
repeatedly adjusted until there is concordance between the prior
predictive distribution and the elicited predictive information.
However, this manual approach is impractical whenever the relationship
between the prior and the distribution of the observables is muddied by
the complexity of the intervening model. A more automated method is
required. \citet{wang_using_2018} and \citet{thomas_probabilistic_2020}
have proposed approaches in which either regions of observable space or
specific realisations are labelled as plausible or implausible by
experts, and then a prior accounting for this information is formed via
either history matching or a ``human in the loop'' process driven by a
Gaussian process model. \citet{albert_combining_2012} propose a
supra-Bayesian approach intended for multiple experts, in which a
Bayesian model is formed for quantiles or probabilities elicited from
the experts. Another approach, and the closest in motivation and
methodology to ours, is \citeauthor{hartmann_flexible_2020}
\citetext{\citeyear{hartmann_flexible_2020}; \citealp[which is partly
inspired by][]{da_silva_prior_2019}}, which employs a Dirichlet
likelihood for elicited predictive quantiles to handle both elicitation
and translation. Our approach is model-agnostic and is fully based
around distributions, meaning uncertainty is directly and intuitively
represented. We specify a suitable, generic loss function between the
prior predictive distribution and this target distribution, and minimise
this loss function via a generic, multi-objective global optimisation
process. We implement our methodology in an \texttt{R} package
\texttt{pbbo} (\url{https://github.com/hhau/pbbo}; Supplement
\ref{r-package}).

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

We postulate three properties that we would like our method to satisfy:

\emph{Faithfulness}~~ A prior is faithful if it accurately encodes the
target data distribution provided by the elicitation subject.
Faithfulness is a property of both the procedure employed to obtain the
prior and the model itself, since not all target prior predictive
distributions can be encoded by simple models and prior structures.
Faithfulness is related to the definition of \emph{validity} in
\citet{johnson_valid_2010} and \citet{ohagan_uncertain_2006}'s use of
\emph{faithful}, but their concerns are specific to the elicitation
process and not to the translational step.

\emph{Uniqueness}~~ Multiple equally faithful prior distributions may
exist in complex models, meaning we must distinguish between such priors
based on properties other than just faithfulness if a unique prior is
desired. One approach is to choose the prior with the largest marginal
standard deviations (see Section
\ref{regularising-estimates-of-lambda-secondary-objective}), but other
properties could be used similarly. The challenge of uniqueness has been
noted by \citet{stefan_practical_2022}.

\emph{Replicability}~~ A procedure is replicable if, given the same
target, it constructs identical priors across independent replications.
This is unlikely to hold exactly with stochastic algorithms, meaning it
is important to assess.

\hypertarget{the-target-predictive-distribution-tcy}{%
\subsection{\texorpdfstring{The target predictive distribution
\(\tc(Y)\)}{The target predictive distribution \textbackslash tc(Y)}}\label{the-target-predictive-distribution-tcy}}

We assume a target predictive distribution, with cumulative distribution
function (CDF) \(\tc(Y)\), for the observable quantity
\(Y \in \mathcal{Y} \subseteq{} \mathbb{R}\) has been chosen. We
recommend chosing this using predictive elicitation
\citep{kadane_experiences_1998}, in which experts are queried about the
observable quantity at a small number quantiles, then an appropriate
parametric distribution is fitted to the elicited values
\citep[chap.~6]{ohagan_uncertain_2006}. However, our approach applies
regardless of how \(\tc(Y)\) is selected, although we assume it can be
described by a (mixture of) standard distributions and that samples can
be drawn from it.

We describe our methodology in the more general setting in which we wish
to elicit information about the observable quantity \(Y\) conditional on
some known values of a covariate
\(X \in \mathcal{X} \subseteq \mathbb{R}^{C}\). For example, when using
the linear model \(Y = X\beta + \varepsilon\) we may elicit information
about \(Y\) at a fixed set of values for \(X\) corresponding to an
experimental design. Specifically we suppose the target CDF
\(\tc(Y \mid X_{r})\) has been elicited at \(R\) values of the covariate
vector denoted \(\{X_{r}\}_{r = 1}^{R}\), which we stack in the matrix
\(\boldsymbol{X} = \left[X_{1}^{\top} \cdots X_{R}^{\top}\right] \in \boldsymbol{\mathcal{X}} \subseteq \mathbb{R}^{R \times C}\).

\hypertarget{predictive-discrepancy-primary-objective}{%
\subsection{Predictive discrepancy (primary
objective)}\label{predictive-discrepancy-primary-objective}}

Consider a joint model for observables \(Y\) and parameters
\(\theta \in \Theta \subseteq \mathbb{R}^{Q}\), given hyperparameters
\(\lambda \in \Lambda \subset \mathbb{R}^{L}\) and covariates \(X\).
This joint model has CDF \(\Pd(Y, \theta \mid \lambda, X)\) and prior
predictive CDF \(\Pd(Y \mid \lambda, X)\) for \(Y\). We assume that each
target \(\tc(Y \mid X_{r})\) has identical support to
\(\Pd(Y \mid \lambda, X_{r})\). We denote
\(\tc(Y \mid \boldsymbol{X}) = \prod_{r = 1}^{R} \tc(Y \mid X_{r})\),
with \(\Pd(Y \mid \lambda, \boldsymbol{X})\) and
\(\Pd(\theta \mid \lambda, \boldsymbol{X})\) defined analogously.

We quantify the difference between the prior predictive and target by
the \emph{covariate-specific predictive discrepancy}, which we define to
be
\input{tex-input/pbbo-methodology/0012-theoretical-discrep-definition-covariate.tex}\noindent
for some discrepancy function \(d(\cdot, \cdot)\). Minimising
\eqref{eqn:theoretical-discrep-definition-covariate} admits the optimal
hyperparameter
\(\lambda^{*} = \min_{\lambda \in \Lambda} \tilde{D}(\lambda \mid \boldsymbol{X})\).
The covariate-independent equivalent \(\tilde{D}(\lambda)\) is obtained
by setting \(R = 1\) and ignoring conditioning on \(X_{r}\).

Inspired by the CramÃ©r-von Mises \citep{von_mises_asymptotic_1947} and
Anderson-Darling \citep{anderson_asymptotic_1952} distributional tests
we define, for CDFs \(\text{M}(Y)\) and \(\Pd(Y)\), two options for the
discrepancy function,
\input{tex-input/pbbo-methodology/0013-discrepancies-definitions.tex}\noindent
Both discrepancies are proper scoring rules
\citep{gneiting_strictly_2007} as they are minimised iff
\(\text{M}(Y) = \Pd(Y)\) for all \(Y \in \mathcal{Y}\). Supposing
\(\Pd(Y \mid \lambda, X_{r})\) is flexible enough to exactly match
\(\tc(Y \mid X_{r})\) for some \(\lambda^{*}\), then both discrepancies
will yield the same \(\lambda^{*}\). Differences arise when
\(\Pd(Y \mid \lambda, X_{r})\) is insufficiently flexible, in which case
the Anderson-Darling (AD) discrepancy \(d^{\text{AD}}\) places more
emphasis on matching the tails of two CDFs.

\hypertarget{regularising-estimates-of-lambda-secondary-objective}{%
\subsection{\texorpdfstring{Regularising estimates of \(\lambda^{*}\)
(secondary
objective)}{Regularising estimates of \textbackslash lambda\^{}\{*\} (secondary objective)}}\label{regularising-estimates-of-lambda-secondary-objective}}

There often are many optimal values \(\lambda^{*}\) that yield values of
\(\tilde{D}(\lambda^{*} \mid \boldsymbol{X})\) that are practically
indistinguishable \citep[noted by][]{da_silva_prior_2019} but with
immensly differing prior distributions
\(\Pd(\theta \mid \lambda^{*}, \boldsymbol{X})\). That is, there are
many equally faithful priors. This is not surprising because we are
providing information only on \(Y\), which is typically of lower
dimension than \(\theta\). A particularly challenging case for
uniqueness is in models with additive noise forms, such as
\eqref{eqn:preece-baines-model-definition-one}; in this case it will
generally be necessary to fix a prior for the variance using knowledge
of the measurement process.

To handle more general cases of lack of uniqueness, we seek to encode
the following principle into our methodology: given two estimates of
\(\lambda^{*}\) which have equivalent values of
\(\tilde{D}(\lambda^{*} \mid \boldsymbol{X})\), we prefer the one with
the larger variance for
\(\Pd(\theta \mid \lambda^{*}, \boldsymbol{X})\). We accomplish this by
defining a secondary objective
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) formed from a suitable
function \(n(\theta)\) with
\input{tex-input/pbbo-methodology/0040-generic-secondary-objective.tex}\noindent
In this paper we consider only one form for this: the (negative) mean of
the marginal log standard deviations of each of the \(Q\) components of
\(\theta \in \Theta \subseteq \mathbb{R}^{Q}\).
\input{tex-input/pbbo-methodology/0041-second-objective-def.tex}\noindent
where \(\text{SD}_{\Pd(Z)}[Z]\) is the standard deviation of \(Z\) under
distribution \(\Pd(Z)\). Analytic expressions for
\(\text{SD}_{\Pd(\theta \mid \lambda, \boldsymbol{X})}[\theta_{q}]\) can
be used if available; or Monte Carlo estimates otherwise.

\hypertarget{algorithm-and-optimisation}{%
\subsection{Algorithm and
optimisation}\label{algorithm-and-optimisation}}

We jointly minimise \eqref{eqn:theoretical-discrep-definition-covariate}
and \eqref{eqn:second-objective-def} using a multi-objective
optimisation algorithm, and obtain a set of possible \(\lambda\) values
which comprise the Pareto frontier
\(\mathcal{P} = \{\lambda_{l}\}_{l = 1}^{\lvert \mathcal{P} \rvert}\).
This is the set of all ``non-dominated'' choices for \(\lambda\),
meaning that no point in \(\mathcal{P}\) is preferable in \emph{both}
objectives to any of the remaining points in \(\mathcal{P}\)
\citep[chap.~2]{deb_multi-objective_2001}. For each \(\lambda\) in
\(\mathcal{P}\) we compute the loss
\input{tex-input/pbbo-methodology/0042-loss-definition.tex}\noindent
where the value of \(\kappa > 0\) expresses our relative belief in the
importance of the secondary objective. The optimal value is then
\(\lambda^{*} := \min\limits_{\lambda \in \mathcal{P}} \tilde{L}(\lambda)\).

This optimum depends on \(\kappa\), which will usually be difficult to
assess. However, using multi-objective optimisation we can evaluate
\eqref{eqn:loss-definition} for any \(\kappa\) without needing to redo
the optimsation step, and thus plot Pareto frontiers for a wide range of
values \(\kappa \in \mathcal{K}\) coloured by loss, with the minimum
loss point indicated. These can guide our choice of \(\kappa\): we can
seek a value of \(\kappa\) with the minimum loss point not on the
extreme of the Pareto frontier, since we would like to balance the two
objectives. This approach is particularly useful in settings where the
scales of the two optima differ markedly, which we further discuss in
Supplement \ref{further-notes-on-choosing-kappa}. Where it is feasible
to replicate the optimisation procedure, we can additionally seek a
choice of \(\kappa\) that leads to Pareto frontiers with minimal
variability across replicates, since this suggests the optimal solution
can be estimated reliably.

We use a two-stage global optimisation process. Our algorithm requires:
a method for sampling \(\Pd(Y \mid \lambda, \boldsymbol{X})\); upper and
lower limits that render \(\Lambda\) a compact subset of
\(\mathbb{R}^{L}\), due to our use of global optimisation; and methods
to evaluate the log-target CDF \(\log(\tc(Y \mid \boldsymbol{X}))\) and
for drawing samples according to \(\tc(Y \mid \boldsymbol{X})\). The
first optimisation stage in our algorithm considers only
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) to focus on faithfulness,
whereas the second stage also considers
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) to account for uniqueness and
replicability. We adopt this approach because minimising
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) is considerably more
challenging than minimising \(\tilde{N}(\lambda \mid \boldsymbol{X})\).
An idealised form of this process is illustrated in Figure
\ref{fig:idealised_process}. We briefly describe the algorithm below;
full details are in Supplement \ref{algorithm-and-optimisation-details}.

\begin{figure}

{\centering \includegraphics{plots/tuning-parameters/idealised-process} 

}

\caption{Illustration of the algorithm, which seeks to match the prior (blue) and target distribution (red) by optimising $\lambda$. The initial value $\lambda$ produces a poor match. Stage 1 minimises \eqref{eqn:theoretical-discrep-definition-covariate}; stage 2 then minimises \eqref{eqn:loss-definition}, which increases the variance of $p(\theta \mid \lambda^{*})$.}\label{fig:idealised_process}
\end{figure}

In stage one we minimise \(\tilde{D}(\lambda \mid \boldsymbol{X})\)
using controlled random search 2 (CRS2) with local mutation
\citep{kaelo_variants_2006}, which we run for \(N_{\text{CRS2}}\)
iterations. We make use of the final optimum value \(\lambda^{*}\), as
well as the \(N_{\text{CRS2}}\) trial points to obtain a design
\(\mathcal{D}\) for the next stage. The design comprises values of
\(\lambda\), and their corresponding values of
\(\log(\tilde{D}(\lambda \mid \boldsymbol{X}))\). A (small) number of
padding points \(N_{\text{pad}}\) are added to \(\mathcal{D}\) for
numerical robustness in stage 2. The result is the design
\(\mathcal{D} = \left\{\lambda_{i}, \log(\tilde{D}(\lambda_{i} \mid \boldsymbol{X}))\right\}_{i = 1}^{N_{\text{design}} + N_{\text{pad}}}\).
Whilst CRS2 was not designed to minimise noisy functions, it appears
empirically robust to small quantities of noise.

Stage one output is then used to initialise stage two, which
additionally focuses on uniqueness and replicability by employing
multi-objective Bayesian optimisation \citep{frazier_tutorial_2018} via
MSPOT \citep{zaefferer_mspot_2012} to jointly minimise
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) and
\(\tilde{N}(\lambda \mid \boldsymbol{X})\). MSPOT uses a separate
Gaussian process (GP) approximation to each of the objectives, and
evaluates these approximations at many points from a Latin hypercube
design. At each iteration the best points under the current GP
approximations are evaluated using the actual objectives and used to
iteratively improve the approximations. The noisy (in practice) and
computationally expensive nature of our objectives, particularly
\(\tilde{D}(\lambda \mid \boldsymbol{X})\), necessitates an approach
such as MSPOT. Employing GP models for the objectives enables
inexpensive screening of values of \(\lambda \in \Lambda\) that are far
from optimal. Moreover, the GP is a flexible yet data efficient model to
use as an approximation and can, through appropriate choice of kernel,
capture correlation or other complex relationships between components of
\(\lambda\) and the objective. We use an optional batching technique in
stage two because the computational cost of evaluating the GP grows
cubically in the number of points \(N_{\text{BO}}\) used in its
construction. After \(N_{\text{BO}}\) iterations, the evaluated points
are reduced to their Pareto frontier \citep{kung_finding_1975}. Note
finding the global optimium is not guaranteed by our optimisation
strategy \citep{mullen_continuous_2014}.

To approximate \(\tilde{D}(\lambda \mid \boldsymbol{X})\) we first
approximate the prior predictive CDF
\(\Pd(Y \mid \lambda, \boldsymbol{X})\) by drawing \(S_{r}\) samples
\(\boldsymbol{y}_{r}^{(\Pd)} = (y_{s, r})_{s = 1}^{S_{r}}\) with
\(\boldsymbol{y}_{r}^{(\Pd)} \sim \Pd(Y \mid \lambda, X_{r})\) to form
the ECDF
\(\hat{\Pd}(Y \mid \lambda, X_{r}, \boldsymbol{y}_{r}^{(\Pd)})\), given
values of \(\lambda\) and \(X_{r}\). We then approximate
\eqref{eqn:theoretical-discrep-definition-covariate}, denoted
\(D(\lambda \mid \boldsymbol{X})\), using \(I_{r}\) samples
\((y_{i, r})_{i = 1}^{I_{r}} \sim \Q(Y \mid X_{r})\) drawn from an
importance distribution \(\Q(Y \mid X_{r})\) with
\input{tex-input/pbbo-methodology/0015-practical-discrep-definition-covariate-importance.tex}\noindent
We select \(\Q(Y \mid X_{r})\) using information about the support
\(\mathcal{Y}\), and samples from \(\Pd(Y \mid \lambda, X_{r})\) and
\(\tc(Y \mid X_{r})\). Approximating
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) is usually straightforward
via Monte Carlo, and we denote the corresponding estimate (or analytic
form, if available) by \(N(\lambda \mid \boldsymbol{X})\).

\hypertarget{benchmarking-and-other-empirical-considerations}{%
\subsection{Benchmarking and other empirical
considerations}\label{benchmarking-and-other-empirical-considerations}}

We show results for both the multi-objective approach and a
single-objective approach, which optimises only
\eqref{eqn:theoretical-discrep-definition-covariate} even in Stage 2.
Given \(\lambda^{*}\), we empirically assess faithfulness by comparing
the target distribution \(\tc(Y \mid X_{r})\) and the estimated optimal
prior predictive distribution \(\Pd(Y \mid \lambda^{*}, X_{r})\).
Replicability and uniqueness are more challenging to disentangle
empirically: without replicability we are unable to conclude whether the
multi-objective optimisation problem admits a unique solution. We will
first assess replicability by examining the stability of the components
of the loss in \eqref{eqn:loss-definition} across independent
replications of the optimisation procedure. When the loss is stable
across replicates, we will assess uniqueness by examining whether the
optimal prior predictive distribution \(\Pd(Y \mid \lambda^{*}, X_{r})\)
and prior \(\Pd(\theta \mid \lambda^{*}, \boldsymbol{X})\) are stable
across replicates; stability of both is good evidence of uniqueness.

\hypertarget{examples}{%
\section{Examples}\label{examples}}

\hypertarget{calibrating-a-cure-fraction-survival-model}{%
\subsection{Calibrating a cure fraction survival
model}\label{calibrating-a-cure-fraction-survival-model}}

Cure models \citep{amico_Cure_2018} for survival data are useful when a
cure mechanism is physically plausible \emph{a priori}, and when
individuals are followed up for long enough to be certain all censored
individuals in our data are ``cured''. Such lengthy follow ups are not
always possible, but a cure model remains plausible when a large
fraction of the censored observations occur after the last observed
event time. However, we cannot distinguish in the right tail of the
survival time distribution between censored uncured individuals and
genuinely cured individuals. We suppose here that we possess prior
knowledge on the fraction of individuals likely to be cured, and the
distribution of event times amongst the uncured, and seek to translate
this information into a prior. This setting is interesting because the
target distribution is of mixed discrete/continuous type, due to
censoring. Additionally, we specify a model with a nontrivial
correlation structure, about which we wish to specify an informative
prior, which is known to be challenging.

\hypertarget{target-survival-time-distribution-and-covariate-generation}{%
\paragraph{Target survival time distribution and covariate
generation}\label{target-survival-time-distribution-and-covariate-generation}}

Suppose that individuals are followed up for an average (but arbitrary)
of 21 units of time, with those who experience the event doing so a long
time before the end of follow up. Furthermore, suppose we believe that,
\emph{a priori}, \(5\%\) of the patients will be cured, with \(0.2\%\)
of events censored due to insufficient follow up.

Consider individuals \(n = 1, \ldots, N\) with event times \(Y_{n}\) and
censoring times \(C_{n}\), such that \(Y_{n} \in (0, C_{n}]\). A target
distribution that is consistent with our beliefs comprises a point mass
of \(0.05\) at \(C_{n}\), and a lognormal distribution with location
\(\mu^{\text{LN}} = \log(3)\) and scale
\(\sigma^{\text{LN}} = 2 \mathop{/} 3\) for \(Y_{n} < C_{n}\). This
choice of lognormal has \(99.8\%\) of its mass residing below 21, and
thus produces event times that are ``well separated'' from the censoring
time, as required by a cure fraction model. Denoting the lognormal CDF
with \(\text{LogNormal}(Y; \mu, \sigma^{2})\), we define the target CDF
\input{tex-input/surv-example/0021-surv-target-cdf-definition.tex}\noindent
where
\(Z_{n} = \text{LogNormal}(C_{n}; \mu^{\text{LN}}, \left(\sigma^{\text{LN}}\right)^{2})\)
is the required normalising constant.

We simulate data for this example with \(N = 50\) individuals, each with
\(4\) correlated covariates. When we consider the censoring time
\(C_{n}\), which also functions as a covariate, we have \(B = 5\)
covariates (we use \(B\) instead of \(C\) as in Section
\ref{methodology} for clarity). In line with our target distribution,
simulated censoring times are distributed such that
\(C_{n} \sim 20 + \text{Exp}(1)\). We sample a single correlation matrix
\(\boldsymbol{Q} \sim \text{LKJ}(5)\)
\citep{lewandowski_generating_2009} and subsequently covariates
\(\tilde{\mathbf{x}}_{n} \sim \text{MultiNormal}(\boldsymbol{0}, \boldsymbol{Q})\).
This results in marginally-standardised yet correlated covariates.

\hypertarget{model}{%
\subsubsection{Model}\label{model}}

A cure model for survival data, expressed in terms of its survival
function, is
\input{tex-input/surv-example/0009-cure-model-surv-def.tex}\noindent
where a proportion \(\pi \in (0, 1)\) of the population are \emph{cured}
and never experience the event of interest. The survival times for the
remaining \(1 - \pi\) proportion of the population are distributed
according to the \emph{uncured} survival function
\(\tilde{S}(Y \mid \tilde{\boldsymbol{X}}, \tilde{\theta})\). We use the
tilde in \(\tilde{\boldsymbol{X}}\) and \(\tilde{\theta}\) to denote
quantities specific to the uncured survival distribution, and denote
\(\theta = (\pi, \tilde{\theta})\) to align with our general notation.

Right censoring results in \(Y_{n} = C_{n}\). The censoring indicator
\(\delta_{n} = \mathbbm{1}_{\left\{Y_{n} < C_{n}\right\}}\) is 0 for
right censored events, and is 1 otherwise. We denote with
\(\tilde{\mathbf{x}}_{n}\) the \(n\)\textsuperscript{th} row of the
\(N \times (B - 1)\) covariate matrix \(\tilde{\boldsymbol{X}}\), which
we assume is column-wise standardised. We assume a Weibull regression
model for the uncured event times, with survival function
\input{tex-input/surv-example/0010-weibull-surv-and-hazard-def.tex}\noindent
with \(\tilde{\theta} = (\gamma, \beta_{0}, \boldsymbol{\beta})\). The
likelihood, with hazard
\(\tilde{h}(Y_{n} \mid \tilde{\theta}, \tilde{\mathbf{x}}_{n}, C_{n})\),
for the \(n\)\textsuperscript{th} individual is
\input{tex-input/surv-example/0011-weibull-likelihood-def.tex}\noindent
In the notation of Section \ref{methodology}, we have
\(Y = (Y_{n})_{n = 1}^{N}\) and
\(X = (C_{n}, \tilde{\mathbf{x}}_{n})_{n = 1}^{N}\), with \(X\)
including censoring times because the support of \(Y \mid X_{r}\)
depends on \(X_{r}\).

We will seek to identify optimal values of the hyperparamers
\(\lambda = (\alpha, \beta, \mu_{0},\allowbreak \sigma^{2}_{0}, s_{\beta}, \boldsymbol{\omega}, \allowbreak\boldsymbol{\eta}, \allowbreak a_{\pi}, b_{\pi})^{\top}\),
with \(\pi \sim \text{Beta}(a_{\pi}, b_{\pi})\),
\(\gamma \sim \text{Gamma}(\alpha, \beta)\),
\(\beta_{0} \sim \text{Normal}(\mu_{0}, \sigma_{0}^{2})\) and
\(\boldsymbol{\beta} \sim\allowbreak \text{MVSkewNormal}(\boldsymbol{0},\allowbreak \boldsymbol{S}, \boldsymbol{\eta})\),
with
\(\boldsymbol{S} = \text{diag}(s_{\beta}) \,\, \boldsymbol{\Omega} \,\, \text{diag}(s_{\beta})\)
where \(s_{\beta}\) is the prior marginal scale of
\(\boldsymbol{\beta}\) and \(\boldsymbol{\Omega}\) is parameterised by
\(\boldsymbol{\omega} = (\omega_{1}, \ldots, \omega_{6})^{\top} \in [-1, 1]^{6}\)
that uniquely determine its Cholesky factor. The skewness is necessary
to incorporate the nonlinear relationship between the hazard and the
effect of the covariates, and a covariance structure is used to account
for fact that not all the elements of \(\boldsymbol{\beta}\) can be
large simultaneously. Further details are in Supplement
\ref{additional-information-for-the-cure-fraction-survival-example}.

\hypertarget{results}{%
\subsubsection{Results}\label{results}}

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/all-pareto-fronts-by-kappa-ad} 

}

\caption{Pareto frontiers for the cure model with AD discrepancy. Each panel displays the replicates for each $\kappa$, with minimum loss point marked ($\color{myredhighlight}{+}$).}\label{fig:surv_ex_pareto_fronts}
\end{figure}

We use \(S_{r} = 10^{4}\) prior predictive samples and
\(I_{r} = 5 \times 10^3\) importance samples to estimate the
discrepancy; and optimise using \(N_{\text{CRS2}} = 2000\) CRS2
iterations, followed by \(N_{\text{batch}} = 3\) batches of Bayesian
optimisation with \(N_{\text{BO}} = 200\) iterations per batch, carrying
\(N_{\text{design}} = 60\) points between batches. We select \(\kappa\)
by inspecting the Pareto frontiers for
\(\kappa \in \{0.1, 0.2, 0.3, 0.5, 1, 2\}\) (Figure
\ref{fig:surv_ex_pareto_fronts} and Supplement
\ref{pareto-frontiers-for-cramuxe9r-von-mises-discrepancy}). Except for
the maximum and minimum values, which yield minimum loss points on the
extremes of the Pareto frontier, the minimum loss point is insensitive
to a wide range of \(\kappa\) values. We select \(\kappa = 0.3\) which
simultaneously minimises variability in loss and both objectives.

The values of the loss and discrepancy functions at \(\lambda^{*}\)
across replicas are tightly distributed (see Supplement
\ref{final-objective-values}), which indicates replicability. We assess
faithfulness by inspecting Figure \ref{fig:surv_ex_ppd_y}. Across all
replicates and discrepancies the estimated optimal prior predictive
distribution is highly faithful to the target.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/subset-indivs-ppd-y-dens} 

}

\caption{Estimated optimal prior predictive densities $\pd(Y_{n} \mid \lambda^{*})$ (red/blue lines and dots) and target densities $\tp(Y_{n} \mid C_{N})$ (black lines and crosses) for individual $n = 9$ (other individuals are visually indistinguishable).}\label{fig:surv_ex_ppd_y}
\end{figure}

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/all-theta-over-kappa} 

}

\caption{Estimated optimal prior marginal densities of $\pd(\theta \mid \lambda^{*})$ for each component of $\theta$ ($\beta_2, \dots, \beta_4$, not shown, are near identical to $\beta_1$). Both axes are manually truncated for readability.}\label{fig:surv_ex_ppd_theta}
\end{figure}

Figure \ref{fig:surv_ex_ppd_theta} displays the marginals of \(\theta\)
for each independent replicate. The single objective approach (i.e.~only
optimising predictive discrepancy, shown in blue) consistently locates
the degenerate, non-unique solution where all the variation in the
uncensored event times is attributed to the baseline hazard shape
\(\gamma\) and the intercept \(\beta_{0}\): i.e.~all the mass for
\(\boldsymbol{\beta}\) (the regression coefficients) is close to 0. The
combination of \(\gamma\) and \(\beta_{0}\) is evidently poorly
identified, and further calculation reveals that only the derived
product \(\gamma \exp{\beta_{0}}\) is uniquely determined. Given the
inter-replicate consistency previously observed in Supplement
\ref{final-objective-values} we infer that the solution is not unique.
In the multi-objective approach, there is a preference for the optima
surrounding \(\gamma \approx 7.5, \beta_{0} \approx -10\); an
improvement in uniqueness over the single objective approach, but
imperfect.

Figure \ref{fig:surv_ex_beta_cov} displays the bivariate prior marginal
densities for \(\beta_{3}\) and \(\beta_{4}\), two representative
elements of \(\boldsymbol{\beta}\). Nonuniqueness is clearly apparent,
with both positive and negative marginal skewness possible. The
multi-objective approach suggests a wider distribution for
\((\beta_{3}, \beta_{4})\), as does the CvM discrepancy relative to the
AD discrepancy.

\begin{figure}

{\centering \includegraphics{plots/synthetic-survival/beta-covariance-pair} 

}

\caption{Contours of the log prior density $\log(\pd(\beta_{3}, \beta_{4} \mid \lambda^{*}))$ at the optima. For clarity we plot only the final 12 replicates, each in a unique colour.}\label{fig:surv_ex_beta_cov}
\end{figure}

Overall, the procedure produces priors that faithfully reflect the
target, in a replicable manner. However, neither multi- or
single-objective solutions are unique, particularly for the covariance
structure, with the former closer to unique for \(\gamma\).

\hypertarget{priors-from-model-derived-quantities}{%
\subsection{Priors from model-derived
quantities}\label{priors-from-model-derived-quantities}}

Consider the linear model
\(Y = \boldsymbol{X}\boldsymbol{\beta} + \varepsilon\) for
\(n \times p\) design matrix \(\boldsymbol{X}\) and \(p\)-vector of
coefficients \(\boldsymbol{\beta}\) indexed by \(j = 1, \ldots, p\), and
where the noise \(\varepsilon\) has zero mean and variance
\(\sigma^{2}\). Suppose information about the fraction of variance
explained by the model is available -- from previous similar
experiments, or from knowledge of the measurement process -- in the form
of a plausible distribution for the coefficient of determination
\input{tex-input/r2-examples/0010-r2-definition.tex}\noindent assuming
that the columns of \(\boldsymbol{X}\) have been centred. Our aim is to
use our knowledge of \(R^{2}\) to set suitable priors for the regression
coefficients \(\beta\). This idea was the inspiration for a class of
shrinkage priors \citep{zhang_variable_2018, zhang_bayesian_2022}, but
we would like to make this idea applicable to a wider selection of prior
structures.

We consider three priors for the regression coefficients: a Gaussian
prior and two shrinkage priors. To demonstrate the challenge that noise
parameters pose for uniqueness we will assume
\(\varepsilon \sim N(0, \sigma^2)\) and seek to select the
hyperparameters for \(\sigma^2 \sim \text{InverseGamma}(a_{1}, b_{1})\).

The Gaussian prior
\(\beta_{j} \sim \text{N}\left(0, \frac{\sigma^{2}}{\gamma}\right)\) has
only one hyperparameter \(\gamma\), which controls the ratio of prior
variability due to \(\boldsymbol{\beta}\) to that of \(\varepsilon\).
Hence, we denote parameters
\(\boldsymbol{\theta}_{\text{GA}} = (\boldsymbol{\beta}, \sigma^{2})\),
and seek optimum values for hyperparameters
\(\boldsymbol{\lambda}_{\text{GA}} = (\gamma, a_{1}, b_{1})\).

The Dirichlet-Laplace prior (\emph{Dir. Lap.}) is defined
\citep{bhattacharya_Dirichlet_2015} for the \(j\)\textsuperscript{th}
coefficient such that
\(\beta_{j} \sim \text{Laplace}\left(0, \sigma \phi_{j} \tau \right)\),
\((\phi_{1}, \ldots, \phi_{p}) \sim \text{Dirichlet}(\alpha, \ldots, \alpha)\),
\(\tau \sim \text{Gamma}(p \alpha, 1 \mathop{/} 2)\). Smaller values of
the single hyperparameter \(\alpha\) yield more sparsity in
\(\boldsymbol{\beta}\). Thus we denote
\(\boldsymbol{\lambda}_{\text{DL}} = (\alpha, a_{1}, b_{1})\) and
\(\boldsymbol{\theta}_{\text{DL}} = (\boldsymbol{\beta}, \sigma^{2}, \phi_{1}, \ldots, \phi_{p}, \tau)\).

The regularised horseshoe prior (\emph{Reg. Horse.})
\citep{piironen_sparsity_2017} has more intermediary quantities and less
linearity, increasing its flexiblity but making finding optimal
hyperparameter values more challenging. The prior is
\input{tex-input/r2-examples/0013-regularised-horseshoe-def.tex}\noindent
with \(\text{Cauchy}^{+}\) denoting a Cauchy distribution truncated to
\([0, \infty)\). Whilst the regularised horseshoe is carefully designed
to make \((p_{0}, \nu, s^{2})\) interpretable and easy to choose, here
we aim to see if we can choose
\(\boldsymbol{\lambda}_{\text{HS}} = (p_{0}, \nu, s^{2}, a_{1}, b_{1})\)
to match an informative prior for \(R^{2}\). We denote
\(\boldsymbol{\theta}_{\text{HS}} = (\boldsymbol{\beta}, \sigma^{2}, c^{2}, \omega, \delta_{1}, \ldots \delta_{p})\).

\hypertarget{evaluation-setup-and-tuning-parameters}{%
\subsubsection{Evaluation setup and tuning
parameters}\label{evaluation-setup-and-tuning-parameters}}

To assess each prior's ability to faithfully encode the information
present across a wide variety of target distribution and assess the
uniqueness and replicability of the optimisation process, we consider
sixteen different \(\text{Beta}(s_{1}, s_{2})\) distributions as our
target \(\tc(R^{2})\), with
\(\{s_{1}, s_{2}\} \in \mathcal{S} \times \mathcal{S}\) and
\(\mathcal{S}\) chosen to be four exponentially-spaced values between
and including \(1 \mathop{/} 3\) and \(3\) (i.e.~equally-spaced between
\(\log(1 \mathop{/} 3)\) and \(\log(3)\)). These values represent a
variety of potential forms of the supplied target predictive
distribution for \(R^2\).

We fix \(n = 50\) and \(p = 80\) with entries in \(\boldsymbol{X}\)
drawn from a standard Gaussian distribution, and assess replicability
using 10 independent runs for each prior and target. The support
\(\Lambda\) for the hyperparameters is defined in Supplement
\ref{hyperparameter-support-lambda-faithfulness-experiment}. We use
\(S = 10^{4}\) prior predictive samples, \(I = 5 \times 10^{3}\)
importance samples from a \(\text{Uniform}(0, 1)\), and use both
\(d^{\text{AD}}\) and \(d^{\text{CvM}}\) as discrepancy functions. We
employ \(N_{\text{CRS2}} = 1000\) iterations, and subsequently perform
both single and multi-objective Bayesian optimisation for
\(N_{\text{batch}} = 1\) batch of \(N_{\text{BO}} = 150\) iterations,
using \(N_{\text{design}} = 50\) points from the first stage. The single
objective approach illustrates that differences in flexibility between
priors also induce differences in uniqueness, and highlights issues in
choosing a prior for the additive noise parameter \(\sigma^{2}\).
Choosing \(\kappa\) is challenging in the multi-objective approach, as
its value should depend on the target, the discrepancy function, and the
prior. These dependencies result in 96 possible choices of \(\kappa\),
which is an infeasible number of choices to make in this example.
Instead we fix \(\kappa = 0.5\) for all multi-objective settings. We use
the secondary objective \eqref{eqn:second-objective-def}, except for
quantities where the standard deviation is undefined for some
\(\lambda \in \Lambda\), for which we use a robust scale estimator
\citep{rousseeuw_alternatives_1993}.

\hypertarget{results-1}{%
\subsubsection{Results}\label{results-1}}

We first assess replicability. It appears from Figure
\ref{fig:r2_roundtrip_discrep_at_optima} that both discrepancies are
replicable for the both Gaussian and Dirichlet-Laplace priors. In
contrast, the results for the Regularised Horseshoe prior appear to
replicate poorly under the AD discrepancy, but reasonably under the CvM
discrepancy.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/roundtrip-discrep-at-optima} 

}

\caption{Discrepancy at the optima $\log(D(\lambda^{*}))$ for four target distributions across 10 replicates, with each the mean of 10 evaluations of $\log(D(\lambda^{*}))$ for the same $\lambda^{*}$.}\label{fig:r2_roundtrip_discrep_at_optima}
\end{figure}

We evaluate faithfulness by inspecting the densities
\(\pd(R^{2} \mid \lambda^{*})\) and \(\tp(R^{2})\) for the various
targets (all distributions in this example have corresponding
densities). A selected subset of the pairs of \((s_{1}, s_{2})\) values
are displayed in Figure \ref{fig:r2_roundtrip_full} (complete results
are in Supplement \ref{full-faithfulness-results}). The Gaussian prior
is universally poorly faithful. Both shrinkage priors perform better in
cases where one of \(s_{1}\) or \(s_{2}\) is less than 1, with the
regularised horseshoe performing better for the \(s_{1} = s_{2} > 1\)
cases. Interestingly, the results are not symmetric in \(s_{1}\) and
\(s_{2}\); the Dirichlet-Laplace prior is able to match the
\(s_{1} = 3, s_{2} = 0.69\) target well, with many of regularised
horseshoe replicates performing poorly; whilst the relative performance
is reversed for \(s_{1} = 0.69, s_{2} = 3\) (see Figure
\ref{fig:r2_roundtrip_full_supp}). There is also perceptibly more
variability in the regularised horseshoe replicates, which suggests the
optimisation problem is more challenging and the predictive discrepancy
objective is noisier. The multi-objective generally produces more
variable sets of optima, which is expected, as it is a more difficult
optimisation problem but we do not allow it additional computational
resources. There is little visible difference between the two
discrepancy functions. Finally, as the values of \(s_{1}\) and \(s_{2}\)
increase, the faithfulness of the shrinkage priors generally decreases.
Across the full set of simulations, the regularised horseshoe is
evidently the most flexible.

\begin{figure}[t]

{\centering \includegraphics{plots/r2-examples/roundtrip-target-plot-small} 

}

\caption{Optimal prior predictive densities $\pd(R^{2} \mid \lambda^{*})$ for the three priors considered, for selected target densities (black lines). Density values are truncated to $[0, 10]$ for readability.}\label{fig:r2_roundtrip_full}
\end{figure}

To assess uniqueness, we consider estimated optimal hyperparameter
values \(\lambda^{*}\) in each replicate. Figure
\ref{fig:r2_roundtrip_lambda} displays the estimates for \(s_{1} = 3\)
and \(s_{2} \in \{0.33, 0.69, 1.44, 3\}\), which corresponds to the
targets in Figure \ref{fig:r2_roundtrip_full}. The estimates for
\(\gamma\) and \(\alpha\), for the Gaussian and Dirichlet-Laplace priors
respectively, are consistent across replicates, which suggests the
optima may be unique. This remains true even for targets where the prior
is not faithful to the target, e.g.~the \(\text{Beta}(3, 3)\) target.
There is more variability in the hyperparameters of the regularised
horseshoe prior. There does appear to be unique solution for \(\nu\) for
the \(\text{Beta}(3, 0.33)\) and \(\text{Beta}(3, 0.69)\) targets,
whereas \(p_{0}\) and \(s^{2}\) are highly variable across replicates,
which may reflect nonuniqueness or may be due to the lack of
replicability (discussed above) of this optimisation for the regularised
horseshoe.

\begin{figure}

{\centering \includegraphics{plots/r2-examples/roundtrip-target-lambda-tiny} 

}

\caption{Optimal values $\lambda^{*}$ for each of the three priors considered. Columns contain (possibly prior-specific) hyperparameters, with the point colour corresponding to a specific prior. Each point's shape corresponds to the combination of discrepancy function and single or multi-objective approach. The target beta densities (denoted by the row panel titles) correspond to Figure \ref{fig:r2_roundtrip_full}.}\label{fig:r2_roundtrip_lambda}
\end{figure}

The hyperparameters \((a_{1}, b_{1})\) for the additive noise variance
\(\sigma^2\) are highly variable across replications for almost all
prior/target combinations. This reflects the anticipated lack of
uniqueness when incorporating such hyperparameters. It is particularly
striking for the Dirichlet-Laplace prior when
\(s_{2} \in \{0.33, 0.69\}\), where we observe consistent and excellent
fits/faithfulness to the target, but these do not correspond to
replicable estimates for \((a_{1}, b_{1})\). These settings are also
interesting as the choice of single or multi-objective approach greatly
impacts the optimum values of \(a_{1}\) and \(b_{1}\). Faithfulness of
the multi-objective optima, illustrated in Figure
\ref{fig:r2_roundtrip_full}, are not appreciably worse than the single
objective approach, but the inclusion of \(\sigma^{2}\) into the
secondary objective has resulted in optimal values of \(a_{1}\) and
\(b_{1}\) that maximise the dispersion of the marginal prior (i.e.~small
\(a_{1}\) and large \(b_{1}\)). Asymptotic results are also known for
the Gaussian prior, and in Supplement
\ref{a-comparison-to-an-asymptotic-result} we further assess
replicability by benchmarking our optimisation process against suitable
`true' (asymptotically) values.

Our optimisation procedure has minimised \(\log(D(\lambda))\) using both
the AD and CvM discrepancy functions. The former places extra emphasis
on matching the tails of the target, and thus the Regularised Horseshoe
values in the top row of Figure \ref{fig:r2_roundtrip_discrep_at_optima}
differ from our expectations given the results in the top two rows of
Figure \ref{fig:r2_roundtrip_full}. Take, for example, the
\(s_{1} = 3, s_{2} = 0.69\) case. It is plainly evident from Figure
\ref{fig:r2_roundtrip_full} that the regularised horseshoe prior
provides a better fit to the target distribution at
\(\boldsymbol{\lambda}^{*}_{\text{HS}}\), and yet the corresponding
\(\log(D(\boldsymbol{\lambda}_{\text{HS}}^{*}))\) values in the top row
of Figure \ref{fig:r2_roundtrip_discrep_at_optima} suggest that it is
considerably worse that the Gaussian prior at
\(\boldsymbol{\lambda}_{\text{GA}}^{*}\). To reconcile this apparent
contradiction, we inspect \(\log(D(\lambda))\) at the optima computed
using the CramÃ©r-Von Mises discrepancy function. These values are
displayed in the bottom row of Figure
\ref{fig:r2_roundtrip_discrep_at_optima}, whose values closely match our
expectations given Figure \ref{fig:r2_roundtrip_full}. Given the range
of behaviours of \(\pd(R^{2} \mid \lambda^{*})\) for all the optima, we
can conclude that AD more heavily penalises over-estimation of the tails
of \(\pd(R^{2} \mid \lambda^{*})\) than under-estimation. This does not
discount it as an optimisation objective, but does complicate
comparisons between competing priors.

Overall, this example illustrates how information about a model-derived,
nonobservable quantity can be used to form an informative prior. The
most flexible shrinkage model (the regularised horseshoe prior) was
almost always the most faithful to the supplied information. Conversely,
the Gaussian prior is the most replicable and unique, but the lack of
faithfulness means it is unsuitable in combination with a Beta prior on
\(R^{2}\).

\hypertarget{a-human-aware-prior-for-a-human-growth-model}{%
\subsection{A human-aware prior for a human growth
model}\label{a-human-aware-prior-for-a-human-growth-model}}

Suppose an individual has their height measured at age \(t_{m}\) (in
years) for \(m = 1, \ldots, M\), with corresponding measurement
\(y_{m}\) (in centimetres). The first Preece-Baines model
\citep{preece_new_1978} for human height is the nonlinear regression
model
\input{tex-input/preece-baines-growth/0061-preece-baines-model-definition.tex}\noindent
with \(\varepsilon_{m} \sim \text{N}(0, \sigma^{2}_{y})\). Some
constraints are required to identify this model and ensure its physical
plausibility: specifically, we require \(0 < h_{0} < h_{1}\) and
\(0 < s_{0} < s_{1}\). To satisfy these constraints, we parameterise in
terms of \(\delta_{h} = h_{1} - h_{0}\) and
\(\delta_{s} = s_{1} - s_{0}\), which results in
\((h_{0}, \delta_{h}, s_{0}, \delta_{s})\) all sharing the same
positivity constraint. We also constrain \(\gamma\) such that
\(\gamma \in (\min_{m}(t_{m}), \max_{m}(t_{m}))\). Even with these
constraints the the denominator of the fraction can be very small,
yielding negative heights, meaning the model is not plausible for all
parameter values. Furthermore, the model is poorly behaved under a flat
prior, so prior information is required to stabilise and/or regularise
the posterior.

We thus seek in this example to specify priors congruent with two
specific target prior predictive distributions. We choose
\(\text{LogNormal}(\mu_{q}, s^{2}_{q})\) priors for each of the
\(q = 1, \ldots, 5\) elements of
\(\theta = (h_{0}, \delta_{h}, s_{0}, \delta_{s}, \gamma)\), and seek to
identify the optimal values of
\(\lambda = \left(\mu_{q}, s^{2}_{q}\right)_{q = 1}^{5}\) (see
Supplement \ref{hyperparameter-support-lambda-1} for \(\Lambda\)). We
fix the prior \(\sigma_{y} \sim \text{LogNormal}(0, 0.2^2)\) to avoid
identifiability problems (Section
\ref{priors-from-model-derived-quantities}). We suppose our data
originate from humans, uniformly distributed between ages 2 and 18, and
evenly split between sexes. We first consider a
\emph{covariate-independent} prior predictive density \(\tp(Y)\) with
corresponding CDF \(\tc(Y)\) for human heights across the entire
age-range, derived by summarising external data. This target (Figure
\ref{fig:pop_target_discreps}) is a mixture of 3 gamma densities
specified to approximate the external data, which is multimodal due to
the fact that humans grow in spurts. We also consider a
\emph{covariate-specific} \(\tc(Y \mid X_{r})\), specifying Gaussian
height distributions at ages \(X_{r} \in (2, 8, 13, 18)\) (see Figure
\ref{fig:cov_target_discreps}, and Supplement
\ref{details-for-tcy-and-tcy-mid-x_r} for details).

\hypertarget{comparison-with-hartmann-et.-al.-and-tuning-parameters}{%
\subsubsection{Comparison with Hartmann et. al.~and tuning
parameters}\label{comparison-with-hartmann-et.-al.-and-tuning-parameters}}

\citet{hartmann_flexible_2020} also considered this example, but
elicited 6 predictive quantiles at ages \(t = (0, 2.5, 10, 17.5)\), as
opposed to entire predictive distributions at ages
\(t = (2, 8, 13, 18)\) as in our covariate-specific approach. We use
different ages because the model is stated to be accurate for ages
\(\geq 2\) \citep{preece_new_1978}. \citet{hartmann_flexible_2020} also
include a noise parameter in their definition of \(\theta\). The exact
interpretation of this parameter is complicated by their choice of
Weibull likelihood, rendering the distribution of the measurement errors
sensitive to conditional mean of the model (this is still the case
despite their choice of Weibull parameterisation). Finally,
\citet{hartmann_flexible_2020} elicit quantiles from 5 different users
and report an estimated \(\lambda^{*}\) for each user. These estimates
(reproduced in Supplement \ref{hartmann_flexible_2020-priors}) allow us
to compare optimal the selected priors \(\pd(\theta \mid \lambda^{*})\)

For both targets we obtain \(\lambda^{*}\) using both the single
objective and multi-objective optimisation processes. We use only the
CramÃ©r-Von Mises discrepancy in this example, because we were not able
to reliably compute the AD discrepancy due to numerical instabilities.
We use \(S = 5 \times 10^4\) samples from \(\pd(Y \mid \lambda)\) and
likewise \(S_{r} = 5 \times 10^4\) samples from
\(\pd(Y \mid \lambda, X_{r})\) for each of the 4 values of \(X_{r}\). We
use \(I = 5 \times 10^3\) and \(I_{r} = 5 \times 10^3\) importance
samples, with \(N_{\text{CRS2}} = 2000\) CRS2 iterations,
\(N_{\text{batch}} = 5\) Bayesian optimisation batches each of
\(N_{\text{BO}} = 250\) iterations, and carry forward
\(N_{\text{design}} = 50\) points per batch. We assess replicability
using 30 independent runs of each objective/target pair.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/cov-kappa} 

}

\caption{Pareto frontiers for each $\kappa \in \mathcal{K}$ for the \textbf{covariate-specific} example. The minimum loss point for each replicate is plotted with $\color{myredhighlight}{+}$.}\label{fig:kappa_cov}
\end{figure}

For this example, we also assess the `stability' of the resulting
posterior under each prior, by separately considering each of the 93
individuals in the \texttt{growth} data in R-package \texttt{fda}
\citep{ramsay_fda_2022}. We consider each individual's data separately,
rather than jointly, to heighten the importance of the prior. We measure
stability by whether \texttt{Stan}
\citep{stan_development_team_rstan_2021} flags a warning, setting
\texttt{adapt\_delta\ =\ 0.95} and \texttt{max\_treedepth\ =\ 12} to
minimise false positives. While a lack of warnings does not imply good
model behaviour, the presence of warning clearly indicates a problem.
This is a form of prior sensitivity analysis, but distinct from the
ideas of \citet{roos_sensitivity_2015} which consider only one
particular realisation of the data. We include the flat, improper prior
as a benchmark.

\hypertarget{results-2}{%
\subsubsection{Results}\label{results-2}}

We consider \(\kappa \in \mathcal{K} = \{0.05, 0.1, \ldots, 0.5\}\) for
the covariate-specific targets. There is notable inter-replicate
variability in the Pareto frontiers for the selection of values
\(\kappa \in \{0.1, 0.2, 0.3\}\) shown in Figure \ref{fig:kappa_cov},
due to the stochastic properties of the global optimisers we employ,
with some replicates totally dominated by other replicates. Following
our `minimum variability across replicates' heuristic (Section
\ref{algorithm-and-optimisation}), we select \(\kappa = 0.2\) for the
covariate-specific target and \(\kappa = 0.15\) for the
covariate-independent target (See Supplement
\ref{pareto-frontiers-for-the-covariate-independent-target}). With these
values the optimisation procedure identifies optimal \(\lambda^{*}\)
values with reasonably, but not entirely, consistent predictive
discrepancies across replicates (see Supplement
\ref{final-predictive-discrepancy-values-for-the-human-growth-example}).

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/population-target-comparsion} 

}

\caption{The covariate-independent marginal target density $\tp(Y)$ (red) and prior predictive densities $\pd(Y \mid \lambda^{*})$ for each of the 30 replicates (blue lines).}\label{fig:pop_target_discreps}
\end{figure}

Figure \ref{fig:pop_target_discreps} displays the target and prior
predictive density estimates in the covariate-independent case. The
multi-objective replicates are obtained after \(\kappa = 0.2\) is
chosen. We see that introducing the secondary objective produces
estimates of \(\lambda^{*}\) that are congruent with estimates from the
single objective case, though the estimates are more variable. Both
single and multi-objective approaches result in reasonably, but not
entirely, faithful densities for \(\pd(Y \mid \lambda^{*})\). However,
most optimum priors seem to accumulates additional probability
surrounding \(Y = h_{1} \approx 155\), resulting in individual
trajectories attaining their adult height \(h_{1}\) for younger than
expected ages \(t\) (which we will later confirm in Figure
\ref{fig:regression_prior_pred}).

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/covariate-target-comparsion} 

}

\caption{Covariate-specific target densities $\tp(Y \mid X_{r})$ (red lines) and prior predictive densities $\pd(Y \mid \lambda^{*}, X_{r})$ for each of the 30 replicates (blue lines).}\label{fig:cov_target_discreps}
\end{figure}

For the covariate-specific target (Figure
\ref{fig:cov_target_discreps}), the secondary objective introduces in
some replicates outlying estimates for
\(\pd(Y \mid \lambda^{*}, X_{r})\), most clearly visible for
\(X_{1} = 2\) and \(X_{2} = 8\). Both the single and multi-objective
approaches struggle to match the prior predictive distribution at all
ages, with consistently poorer faithfulness for \(X_{1} = 2\).
Empirically, it does not seem possible to match all four supplied target
prior predictive distributions simultaneously, given the mathematical
structure of the model. Lastly, because \(\tp(Y \mid X_{1} = 2)\) is
substantially narrower than the other targets, it is optimal, under the
CramÃ©r-Von Mises discrepancy, to select wider priors better matching the
older age target distributions.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/regression-prior-preds} 

}

\caption{Mean (solid lines) and 95\% intervals (grey regions) for prior predictives $\pd(h(t; \theta) \mid \lambda^{*})$ for covariate-independent and covariate-specific targets for the multi- and single-objective settings; and for each replicate from Hartmann et al. (with \textcolor{mywhwlow}{75\%} intervals). The y-axis is truncated to $(50, 200)$. The red lines are 95\% intervals for $\tp(Y \mid X_{r})$.}\label{fig:regression_prior_pred}
\end{figure}

Both the covariate-independent and covariate-specific targets yield
plausible mean growth trajectories (Figure
\ref{fig:regression_prior_pred}). However, covariate-independent priors
are highly uncertain, resulting in implausible heights having \emph{a
priori} support. It is interesting to note that the covariate-specific
target has similar levels of uncertainty across all ages, further
suggesting that the model is too inflexible to simultaneously match all
the targets when they have varying variance, as here. All 5 of the
priors from \citet{hartmann_flexible_2020}, for a narrower uncertainty
interval, are implausible in both shape and width when viewed on this
scale. It also seems unlikely that these priors accurately reflect the
information provided by the experts in \citet{hartmann_flexible_2020},
but this information is not reported.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/fda-all-any-warnings-plot} 

}

\caption{Presence/absence of \texttt{Stan} warnings for all individuals (columns) in the FDA package \texttt{growth} data and replicate prior estimates (rows). Each replicate corresponds to a run of the optimisation process and thus a different prior (except for Flat).}\label{fig:warnings_all_priors}
\end{figure}

The different priors produce a widely ranging proportion of warning
messages in \texttt{Stan} (Figure \ref{fig:warnings_all_priors}). The
flat prior produces the most warnings, with some individuals
particularly prone to warning messages, suggesting that their data are
relatively uninformative. The Hartmann priors produce a moderate number
of warnings, with some priors less prone to produce warnings
(replications 1 and 5) than others for this dataset. The
covariate-specific approach produces fewer warnings than the
covariate-independent approach in both the single- or multi-objective
cases. This reflects the additional information available in the
covariate-specific setting, and that this information results in more
plausible priors. Some specific replications of the
covariate-independent approach produce many warnings, suggesting these
priors are inappropriate for many individuals.

\begin{figure}

{\centering \includegraphics{plots/preece-baines-growth/small-cov-priors-posteriors} 

}

\caption{Priors (\textcolor{mymidblue}{blue}) for $h_{0}$ with covariate-independent and covariate-specific targets; a flat prior scenario (prior not shown); and Hartmann et al. (2020). The corresponding posteriors for each prior for individual $n = 26$ is shown in \textcolor{myredhighlight}{red}.}\label{fig:small_cov_prior_post}
\end{figure}

The priors for \(h_{0}\) exhibit substantial variability across
replicates (Figure \ref{fig:small_cov_prior_post}; see Supplement
\ref{full-marginal-prior-and-posterior-comparison-plots} for all
\(\theta\)). There appear to be two distinct unimodal priors for
\(h_{0}\) with similar loss, suggesting that
\(\tc(Y \mid \boldsymbol{X})\) does not provided enough information to
uniquely determine a prior distribution. However both priors are
significantly broader than the Hartmann et.~al.~priors. Figure
\ref{fig:small_cov_prior_post} also shows the posteriors for these
parameters when using the (uninformative and thus challenging) data from
individual \(n = 26\). The flat prior produces a multimodal posterior
for our parameters of interest, demonstrating the lack of stability when
computing the posterior and the need for prior information. As a result,
the posteriors are under all methods strongly depend on the prior
distribution used, which as noted is not stable under any method here.
However both of our priors produce posteriors for \(\delta_{s}\) with no
significant mass above 2 (Supplement
\ref{full-marginal-prior-and-posterior-comparison-plots}), as is
desirable, because this corresponds physiologically implausible growth
spurts that are unsupported by the data.

In summary, the priors estimated by our procedure in this example are
broadly faithful to the supplied information, except in the
covariate-specific case where model inflexibility prevents matching both
\(t = 2\) and \(t = 18\) targets simultaneously. The covariate-specific,
multi-objective method appears the most useful prior, but is arguably
over concentrated, which occasionally prevents the model from fitting
the data well, although all our priors successfully regularise the
posterior sufficiently to enable accurate posterior sampling. Our
approach does not produce a unique prior, although the secondary
objective leads to a small improvement in uniqueness (see Supplement
\ref{full-marginal-prior-and-posterior-comparison-plots}). However, some
of this non-uniqueness may be attributable to imperfect replicability of
the optimisation.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Setting priors for models congruent with our knowledge is often
difficult without a method for translation such as we have proposed. The
Preece-Baines model is a typical example, in which the observable is
well understood but the model parameters are not. Similarly we
anticipate our approach will be valuable for model-derived quantities
(such as \(R^{2}\)), which are often readily reasoned about but
difficult to set priors for.

One limitation of the current work is that we only partly address
non-uniqueness, but we emphasise that our methodology remains valuable
in such settings. Specifically, our approach provides insight into
consequences of certain \(\tc(Y \mid \boldsymbol{X})\): it facilitates
identifying which components of \(\lambda\) are well identified, and
consideration of whether any difference between \(\tc(Y \mid X_{r})\)
and \(\Pd(Y \mid \lambda^{*}, X_{r})\) are attributable to model
inflexibility or an implausible target. We also have the opportunity to
re-assess whether we have information that we could employ to fix
certain components within \(\lambda\) (e.g.~the fixed prior for the
noise in the human height example). Another limitation of our current
work is that global optimisation methods lack guarantees of finding the
global optimum in finite time, so the generalisability of the
performance of our optimisation process requires further investigation
in the future.

\hypertarget{acknowledgments-and-data-availability}{%
\section*{Acknowledgments and data
availability}\label{acknowledgments-and-data-availability}}
\addcontentsline{toc}{section}{Acknowledgments and data availability}

We thank Daniela De Angelis and Mevin Hooten for their feedback; and the
The Alan Turing Institute under the UK Engineering and Physical Sciences
Research Council (EPSRC) {[}EP/N510129/1{]} and the UK Medical Research
Council {[}MC\_UU\_00002/2 and MC\_UU\_00002/20{]} for support. No
original data were generated; the \texttt{fda} package for \texttt{R}
contains the \texttt{growth} data. \texttt{R} code for the examples is
at \url{https://gitlab.com/andrew-manderson/pbbo-paper}. For the purpose
of open access, the author has applied a Creative Commons Attribution
(CC BY) licence to any Author Accepted Manuscript version arising.

\renewcommand\thesection{S\arabic{section}}
\setcounter{section}{0}
\renewcommand*{\theHsection}{chX.\the\value{section}}

\hypertarget{r-package}{%
\section{R package}\label{r-package}}

We implement our methodology in an \texttt{R} package
\citep{rcoreteam_language_2023} \texttt{pbbo}
(\url{https://github.com/hhau/pbbo}). \texttt{pbbo} builds on top of
\texttt{mlrMBO} \citep{bischl_mlrmbo_2018} for multi-objective Bayesian
optimisation, \texttt{nlopt} and \texttt{nloptr}
\citep{ypma_nloptr_2022, johnson_nlopt_2014} for global optimisation
using CRS2 \citep{kaelo_variants_2006}, and other packages for internal
functionality and logging
\citep{wickham_welcome_2019, rowe_futilelogger_2016, maechler_rmpfr_2021}.
The code to reproduce the examples is available at
\url{https://gitlab.com/andrew-manderson/pbbo-paper}.

\hypertarget{further-notes-on-choosing-kappa}{%
\section{\texorpdfstring{Further notes on choosing
\(\kappa\)}{Further notes on choosing \textbackslash kappa}}\label{further-notes-on-choosing-kappa}}

Advantages of multi-objective optimisation are most immediately apparent
when the scales of our objectives differ markedly. Consider the
equivalent linearised approach, where we select \(\kappa\) \emph{before}
optimisation and directly optimise
\(\tilde{L}(\lambda \mid \boldsymbol{X})\). It is generally not possible
to know the range of the values of
\(\tilde{D}(\lambda \mid \boldsymbol{X})\) and
\(\tilde{N}(\lambda \mid \boldsymbol{X})\) before optimisation.
Selecting an appropriate \(\kappa\) without this knowledge is
prohibitively difficult, leaving only the computationally expensive
trial-and-error approach -- where we re-run the optimiser for each new
possible value of \(\kappa\) -- as a plausible strategy for choosing
\(\kappa\). In contrast, given \(\mathcal{P}\) it is computationally
trivial to recompute \(\lambda^{*}\) for many possible values of
\(\kappa\) \emph{after} optimisation (e.g.~each panel of Figure
\ref{fig:kappa_cov} in the main text is trivial to compute). We can thus
select \(\kappa\) in a problem-specific manner for practically no
additional computational cost to that of the multi-objective optimiser.
Note that the multi-objective optimisation approach is more expensive
that the linearised approach, but this additional cost is dwarfed by the
number of re-runs of the latter typically required to select \(\kappa\).

\hypertarget{algorithm-and-optimisation-details}{%
\section{Algorithm and optimisation
details}\label{algorithm-and-optimisation-details}}

Here we provide further details on the algorithm and optimisation
process used, before providing an overarching algorithm for the complete
methodology (Section \ref{summary-of-complete-methodology})

\hypertarget{crs2-as-an-initialiser-for-bayesian-optimisation}{%
\subsection{CRS2 as an initialiser for Bayesian
optimisation}\label{crs2-as-an-initialiser-for-bayesian-optimisation}}

Algorithm \ref{alg:crs2-algorithmic-description} describes our use of
CRS2 \citep{kaelo_variants_2006} to obtain a suitable design to
initialise the Bayesian multi-objective optimisation approach in step 2.

\input{tex-input/pbbo-methodology/0052-crs2-algorithmic-description.tex}

\noindent

\hypertarget{mspot}{%
\subsection{MSPOT}\label{mspot}}

Algorithm \ref{alg:mspot-algorithmic-description} describes, in our
notation, the MSPOT \citep{zaefferer_mspot_2012} algorithm for two
objectives. Note that within the algorithm we suppress each objective's
dependence on \(\boldsymbol{X}\) for brevity.

\input{tex-input/pbbo-methodology/0053-mspot-algorithmic-description.tex}

\hypertarget{inter-batch-resampling}{%
\subsection{Inter batch resampling}\label{inter-batch-resampling}}

Algorithm \ref{alg:resample-batch-algorithmic-description} describes our
inter-batch resampling algorithm that we occasionally adopt in stage two
of our optimisation process.

\input{tex-input/pbbo-methodology/0054-resample-batch-algorithmic-description.tex}

\hypertarget{evaluating-dlambda-mid-boldsymbolx}{%
\subsection{\texorpdfstring{Evaluating
\(D(\lambda \mid \boldsymbol{X})\)}{Evaluating D(\textbackslash lambda \textbackslash mid \textbackslash boldsymbol\{X\})}}\label{evaluating-dlambda-mid-boldsymbolx}}

Algorithm \ref{alg:tpd-algorithmic-description} summarises the algorithm
used to evaluate \(D(\lambda \mid \boldsymbol{X})\), with further
explanation in the following subsections.

\input{tex-input/pbbo-methodology/0051-tpd-algorithmic-description.tex}

\hypertarget{choosing-importance-distribution-q}{%
\subsubsection{\texorpdfstring{Choosing importance distribution
\(\Q\)}{Choosing importance distribution \textbackslash Q}}\label{choosing-importance-distribution-q}}

Appropriate importance distributions are crucial to obtaining an
accurate and low variance estimate of
\(D(\lambda \mid \boldsymbol{X})\). For values of \(\lambda\) far from
optimal, \(\Pd(Y \mid \lambda, \boldsymbol{X})\) can differ considerably
from \(\tc(Y \mid \boldsymbol{X})\). Given a specific \(X_{r}\) we
require an importance distribution \(\Q(Y \mid X_{r})\) that places
substantial mass in the high probability regions of both
\(\tc(Y \mid X_{r})\) and \(\Pd(Y \mid \lambda, X_{r})\), as it is in
these regions that \(d(\cdot, \cdot)\) is largest. But we cannot exert
too much effort on finding these densities as they are specific to each
value of \(\lambda\), and must be found anew for each \(\lambda\).

We use three quantities to guide our choice of \(\Q(Y \mid X_{r})\),
these being the support \(\mathcal{Y}\), the samples
\(\boldsymbol{y}_{r}^{(\Pd)} \sim \Pd(Y \mid \lambda, X_{r})\), and the
samples \(\boldsymbol{y}_{r}^{(\tc)} \sim \tc(Y \mid X_{r})\). Of
primary concern is the support. If \(\mathcal{Y} = \mathbb{R}\) then we
use a mixture of Student-\(t_{5}\) distributions; for
\(\mathcal{Y} = \mathbb{R} = (0, \infty)\) we employ a mixture of gamma
distributions; and for \(\mathcal{Y} = (0, a]\) with known \(a\), we opt
for a mixture of Beta distributions with a discrete component at
\(Y = a\). The parameters of the mixture components are estimated using
the method of moments. Specifically, denoting the empirical mean of
\(\boldsymbol{y}_{r}^{(\Pd)}\) as \(\hat{\mu}^{(\Pd)}\) and the
empirical variance by \(\hat{v}^{(\Pd)}\), with \(\hat{\mu}^{(\tc)}\)
and \(\hat{v}^{(\tc)}\) defined correspondingly for
\(\boldsymbol{y}_{r}^{(\tc)}\), Table
\ref{tab:importance-sampling-appendix-table} details our method of
moments estimators for the mixture components.

In this paper we limit ourselves to one dimensional \(\mathcal{Y}\),
where importance sampling is mostly well behaved or can be tamed using a
reasonable amount of computation. This covers many models, and with the
covariate-specific target it includes regression models. It is harder to
elicit \(\tc(Y \mid \boldsymbol{X})\) for higher dimensional data
spaces, and the difficulties with higher dimensional importance sampling
are well known.

\begin{landscape}
\input{tex-input/pbbo-methodology/0071-importance-sampling-appendix-table.tex}
\end{landscape}

\hypertarget{numerical-considerations}{%
\subsubsection{Numerical
considerations}\label{numerical-considerations}}

For both numerical stability and optimisation performance
\citep{eriksson_scalable_2021, snoek_input_2014} we evaluate
\(D(\lambda \mid \boldsymbol{X})\) on the log scale. This is because far
from optimal values of \(\lambda\) have corresponding
\(D(\lambda \mid \boldsymbol{X})\) many orders of magnitude larger than
near optimal values of \(\lambda\). Furthermore, the Gaussian process
approximation that underlies Bayesian optimisation assumes constant
variance, necessitating a log or log-like transformation.

Suppose again that we sample
\(\boldsymbol{y}_{r}^{(\Pd)} \sim \Pd(Y \mid \lambda, X_{r})\), from
which we form the ECDF
\(\hat{\Pd}(Y \mid \lambda, X_{r}, \boldsymbol{y}_{r}^{(\Pd)})\). Having
selected an appropriate importance distribution \(\Q(Y \mid X_{r})\) and
density \(\q(Y \mid X_{r})\) using Supplement
\ref{choosing-importance-distribution-q}, and sample importance points
\((y_{i, r})_{i = 1}^{I_{r}} \sim \Q(Y \mid X_{r})\), we define the
intermediary quantity \(z(y_{i, r})\) (in the case when densities for
the target and important distribution exist, to avoid notational
complexity) as
\input{tex-input/pbbo-methodology/0020-log-discrep-func-defs.tex}\noindent
and then rewrite
\eqref{eqn:practical-discrep-definition-covariate-importance} in the
main text to read
\input{tex-input/pbbo-methodology/0022-importance-discrepancy-covariate-definition.tex}\noindent
All \(\log(\sum \exp\{\cdot\})\) terms are computed using the
numerically stable form \citep{blanchard_accurately_2021}.

Accurately evaluating \(\log(d(\cdot, \cdot))\) in
\eqref{eqn:log-discrep-func-def} involves managing the discrete nature
of the ECDF (that it returns exactly zero or one for some inputs), and
using specialised functions for each discrepancy to avoid issues with
floating point arithmetic. We compute
\(\log(d^{\text{CvM}}(\cdot, \cdot))\) using
\input{tex-input/computing-log-discrep-functions/0010-computing-log-cvm.tex}\noindent
where
\(\mathcal{T}(y_{i, r} \mid X_{r}) = \log(\tc(y_{i, r} \mid X_{r}))\).
The log-CDF (LCDF) is often more numerically accurate for improbable
values of \(y_{i, r}\), and so our methodology assumes that it is this
LCDF form in which the target distribution is supplied. However, because
the ECDF can return exact zero/one values there is no way to perform
this computation on the log scale. We thus employ high precision
floating point numbers when exponentiating the LCDF values, using
\texttt{Rmpfr} \citep{maechler_rmpfr_2021}, to avoid evaluating
\(\log(0)\).

For \(\log(d^{\text{AD}}(\cdot, \cdot))\), additional care must be taken
as the denominator of \(d^{\text{AD}}\) in
\eqref{eqn:discrepancies-definitions} in the main text tends to
underflow to zero. Thus we evaluate it using
\input{tex-input/computing-log-discrep-functions/0011-computing-log-ad.tex}\noindent
where \(\texttt{log1mexp}(x) = \log(1 - \exp\{-x\})\) is implemented by
the \texttt{Rmpfr} package \citep{maechler_accurately_2012}. Such
precision is necessary for improbably large values of \(y_{i, r}\) under
\(\tc(y_{i, r} \mid X_{r}A)\), as the CDF/LCDF often rounds to 1/0
(respectively). It is not always feasible to evaluate
\eqref{eqn:computing-log-ad} with sufficient accuracy to avoid
under/over-flow issues -- it requires a high-precision implementation of
\(\mathcal{T}(y_{i, r} \mid X_{r})\) for extreme \(y_{i, r}\) and many
additional bits of precision for both \(y_{i, r}\) and the result. In
these settings we revert to \(\log(d^{\text{CvM}}(\cdot, \cdot))\).

\hypertarget{summary-of-complete-methodology}{%
\subsection{Summary of complete
methodology}\label{summary-of-complete-methodology}}

Lastly, Algorithm \ref{alg:pbbo-overall-algorithmic-description}
summaries the entire methodology we introduce in this paper.

\input{tex-input/pbbo-methodology/0055-pbbo-overall-algorithmic-description.tex}

\FloatBarrier

\hypertarget{additional-information-for-the-cure-fraction-survival-example}{%
\section{Additional information for the cure fraction survival
example}\label{additional-information-for-the-cure-fraction-survival-example}}

Note that the standardisation of \(\tilde{\boldsymbol{X}}\) allows us to
use only one \(s_{\beta}\) instead of one per covariate. These elements
are transformed into \(\boldsymbol{\Omega}\) using the partial
correlation method of \citet{lewandowski_generating_2009}, also employed
by the \texttt{Stan} math library
\citep{stan_development_team_stan_2022}. The \((B - 1)\)-vector
\(\boldsymbol{\eta}\) controls, but is not equal to, the marginal
skewness for each element of \(\boldsymbol{\beta}\) using the
multivariate skew-normal definition of
\citet{azzalini_multivariate_1996}, as implemented in the \texttt{sn}
package \citep{azzalini_sn_2022}.

\hypertarget{hyperparameter-support-lambda}{%
\subsection{\texorpdfstring{Hyperparameter support
\(\Lambda\)}{Hyperparameter support \textbackslash Lambda}}\label{hyperparameter-support-lambda}}

See Table \ref{tab:surv-cap-lambda-def}

\input{tex-input/surv-example/0022-surv-cap-lambda-def.tex}

\hypertarget{pareto-frontiers-for-cramuxe9r-von-mises-discrepancy}{%
\subsection{Pareto Frontiers for CramÃ©r-von Mises
discrepancy}\label{pareto-frontiers-for-cramuxe9r-von-mises-discrepancy}}

See Figure \ref{fig:surv_ex_pf_cvm}.

\begin{figure}[H]

{\centering \includegraphics{plots/synthetic-survival/all-pareto-fronts-by-kappa-cvm} 

}

\caption{Pareto frontiers for the survival example using the CramÃ©r-von Mises discrepancy, for the values of $\kappa$ we consider. Note that the colour scale displaying loss is panel-specific. The red crosses ($\color{myredhighlight}{+}$) indicate the minimum loss point on each frontier, for each value of $\kappa$.}\label{fig:surv_ex_pf_cvm}
\end{figure}

\hypertarget{final-objective-values}{%
\subsection{Final objective values}\label{final-objective-values}}

See Figure \ref{fig:surv_ex_final_objective_values}.

\begin{figure}[H]

{\centering \includegraphics{plots/synthetic-survival/final-objective-values} 

}

\caption{Estimates of $D(\lambda^{*} \mid \boldsymbol{X})$,  $N(\lambda^{*}\mid \boldsymbol{X})$ and $L(\lambda^{*} \mid \boldsymbol{X})$ across replicates for the cure fraction survival model.}\label{fig:surv_ex_final_objective_values}
\end{figure}

\FloatBarrier

\hypertarget{additional-information-for-the-r2-example}{%
\section{\texorpdfstring{Additional information for the \(R^{2}\)
example}{Additional information for the R\^{}\{2\} example}}\label{additional-information-for-the-r2-example}}

\hypertarget{hyperparameter-support-lambda-faithfulness-experiment}{%
\subsection{\texorpdfstring{Hyperparameter support \(\Lambda\) --
faithfulness
experiment}{Hyperparameter support \textbackslash Lambda -- faithfulness experiment}}\label{hyperparameter-support-lambda-faithfulness-experiment}}

See Table \ref{tab:cap-lambda-def}. Note that for the Dirichlet-Laplace
prior, \citet{zhang_variable_2018} suggest bounding
\(\alpha \in [(\max(n, p))^{-1}, 1 \mathop{/} 2]\). In our experiments
we regularly encountered optimal values of \(\alpha\) on the lower
boundary, so we use instead \(1 \mathop{/} (3\max(n, p))\) as a lower
bound.

\input{tex-input/r2-examples/0014-cap-lambda-def.tex}

\hypertarget{full-faithfulness-results}{%
\subsection{Full faithfulness results}\label{full-faithfulness-results}}

The complete results from the faithfulness experiment are displayed in
Figure \ref{fig:r2_roundtrip_full_supp}.

\begin{figure}[H]

{\centering \includegraphics{plots/r2-examples/roundtrip-target-plot-big} 

}

\caption{As in Figure \ref{fig:r2_roundtrip_full} but for all values of $(s_{1}, s_{2})$ denoted in the facet panels titles. The performance of the regularised horseshoe is superior to the Dirichlet-Laplace, both of which are vast improvements over the Gaussian.}\label{fig:r2_roundtrip_full_supp}
\end{figure}

\hypertarget{a-comparison-to-an-asymptotic-result}{%
\subsection{A comparison to an asymptotic
result}\label{a-comparison-to-an-asymptotic-result}}

The poor fit for the Gaussian prior observed in Figure
\ref{fig:r2_roundtrip_full} in the main text could be attributed to
issues in the optimisation process, or to the lack of flexibility in the
prior. To investigate, we compare the results for
\(\lambda_{\text{GA}}\) to Theorem 5 of \citet{zhang_variable_2018},
which is an asymptotic result regarding the optimal value of
\(\lambda_{GA}\) for a target \(\text{Beta}(s_{1}, s_{2})\) density for
\(R^{2}\). We compare pairs of \((n_{k}, p_{k})\) for
\(k = 1, \ldots, 5\), noting that assumption (A4) of Zhang and Bondell
requires that \(p_{k} = \text{o}(n_{k})\) as \(k \rightarrow \infty\)
(for strictly increasing sequences \(p_{k}\) and \(n_{k}\)). Thus we
consider values of \(p\) such that \(p_{1} = 80\) with
\(p_{k} = 2p_{k - 1}\) and \(n\) with \(n_{1} = 50\) and
\(n_{k} = n_{k - 1}^{1.2}\), both for \(k = 2, \ldots, 5\). Each
\((n_{k}, p_{k})\) pair is replicated 20 times, and for each replicate
we generate a different \(\boldsymbol{X}\) matrix with standard normal
entries. As the target density we choose \(s_{1} = 5, s_{2} = 10\) -- a
``more Gaussian'' target than previously considered and thus, we
speculate, possibly more amenable to translation with a Gaussian prior
for \(\beta\). We also use this example as an opportunity to assess if
there are notable differences between the CramÃ©r-Von Mises discrepancy
and the Anderson-Darling discrepancy as defined in
\eqref{eqn:discrepancies-definitions} in the main text. The support
\(\Lambda\) for \(\lambda_{\text{GA}}\) differs slightly from the
example in the main text, and is defined in Table
\ref{tab:cap-lambda-def-asymp}, as matching our target with larger
design matrices requires considerably larger values of \(\gamma\).

The computation of \(R^{2}\) becomes increasingly expensive as \(n_{k}\)
and \(p_{k}\) increase, which limits the value of some of our method's
tuning parameters. The approximate discrepancy function uses
\(S = 2000\) samples from the prior predictive and is evaluated using
\(I = 500\) importance samples. We run CRS2 for
\(N_{\text{CRS2}} = 500\) iterations, using \(N_{\text{design}} = 50\)
in the initial design for the subsequent single batch of Bayesian
optimisation, which uses \(N_{\text{BO}} = 100\) iterations.

\input{tex-input/r2-examples/0015-cap-lambda-def-aysmp.tex}

\hypertarget{results-and-analytic-comparison}{%
\subsubsection{Results and analytic
comparison}\label{results-and-analytic-comparison}}

Figure \ref{fig:r2_asymp_plot} displays the results in terms of the
normalised difference between the \(\gamma\) we estimate
\(\gamma_{\text{pbbo}}^{*}\), and the asymptotic result of Zhang and
Bondell \(\gamma_{\text{asym}}^{*}\). Our typical finite sample estimate
is slightly larger than the asymptotic result, and the difference
increases with \(n_{k}\) and \(p_{k}\). The variability of the
normalised difference remains roughly constant, and thus reduces on an
absolute scale, though extrema seem to occur more frequently for larger
\(n_{k}\) and \(p_{k}\). These simulations suggest that the asymptotic
regime has not been reached even at the largest \(n_{k}\) and \(p_{k}\)
values we assessed.

\begin{figure}[H]

{\centering \includegraphics{plots/r2-examples/gamma-diff-plot} 

}

\caption{Relative difference between the value of $\gamma$ obtained using our methodology ($\gamma_{\text{pbbo}}^{*}$) and Theorem 5 of Zhang and Bondell (2018) ($\gamma_{\text{asym}}^{*}$).}\label{fig:r2_asymp_plot}
\end{figure}

The estimates of \(\gamma\) are not themselves particularly
illuminating: we should instead look for differences in the distribution
of \(R^{2}\) at the optima, which is to say on the ``data'' scale.
Figure \ref{fig:r2_target_vs_opt_prior} displays the target distribution
and the prior predictive distribution at the optima
\(\pd(R^{2} \mid \lambda^{*}_{GA})\). The fit is increasingly poor as
\(n\) and \(p\) increase, and there is little difference both between
the two discrepancies and with each discrepancies replications. The lack
of difference implies that the optimisation process is consistently
locating the same minima for \(D(\lambda)\). We conclude that either 1)
the ability of the model to match the target depends on there being
additional structure in \(\boldsymbol{X}\), or 2) it is not possible to
encode the information in a \(\text{Beta}(5, 10)\) prior for \(R^{2}\)
into the Gaussian prior.

\begin{figure}[H]

{\centering \includegraphics{plots/r2-examples/optimal-pf-draws-plot} 

}

\caption{The target density $\tp(R^{2})$ and optimal prior predictive densities $\pd(R^{2} \mid \lambda^{*})$ under both the CramÃ©r-von Mises (red, left column) and Anderson-Darling (blue, right column) discrepancies. There are 20 replicates of each discrepancy in this plot.}\label{fig:r2_target_vs_opt_prior}
\end{figure}

This example also further illustrates the difficulties inherent in
acquiring a prior for additive noise terms. Specifically, in this
example it is difficult to learn \((a_{1}, b_{1})\), despite the fact
that the contribution of \(\sigma^{2}\) in Equation
\eqref{eqn:r2-definition} in the main text is not purely additive.
However, as we see in Figure \ref{fig:r2_noise_hypers_plot}, estimates
are uniformly distributed across the permissible space, except for
bunching at the upper and lower bounds of \(\Lambda\). Note that for
numerical and computational stability, we constrain
\(a_{1} \in (2, 50]\) and \(b_{1} \in (0.2, 50]\) in this example. This
contrasts with similarity between replicates visible in Figure
\ref{fig:r2_target_vs_opt_prior}, and is thus evidence that
\((\hat{a}_{1}, \hat{b}_{1})\) have no apparent effect on the value of
\(D(\lambda^{*})\). We should instead set the prior for \(\sigma^{2}\)
based on external knowledge of the measurement process for \(Y\).

\begin{figure}[H]

{\centering \includegraphics{plots/r2-examples/normal-noise-hyperpars-plot} 

}

\caption{Histograms of \textit{scaled} estimates of $(a_{1}^{*}, b_{1}^{*})$ for the settings considered in Section \ref{a-comparison-to-an-asymptotic-result}. Estimates have been scaled to $[0, 1]$ for visualisation purposes using the upper and lower limits defined in Table \ref{tab:cap-lambda-def}.}\label{fig:r2_noise_hypers_plot}
\end{figure}

The regularisation method we employ in the two other examples in the
main text is unlikely to assist in estimating \((a_{1}, b_{1})\).
Promoting a larger mean log marginal standard deviation, with the
knowledge \(D(\lambda)\) is insensitive to the value of
\((a_{1}, b_{1})\), would simply pick the largest possible value for
\(b_{1}^{2} \mathop{/} \left((a_{1} - 1)^{2}(a_{1} - 2)\right)\), which
occurs when \(a_{1}\) is at its minimum allowable value and \(b_{1}\)
its corresponding maximum.

\FloatBarrier

\hypertarget{additional-information-for-the-preece-baines-example}{%
\section{Additional information for the Preece-Baines
example}\label{additional-information-for-the-preece-baines-example}}

\hypertarget{hyperparameter-support-lambda-1}{%
\subsection{\texorpdfstring{Hyperparameter support
\(\Lambda\)}{Hyperparameter support \textbackslash Lambda}}\label{hyperparameter-support-lambda-1}}

Table \ref{tab:pb-cap-lambda-def} contains the upper and lower limits
for each hyperparameter, thus defining the feasible region \(\Lambda\).

\input{tex-input/preece-baines-growth/0011-pb-cap-lambda-def.tex}

\noindent

\hypertarget{details-for-tcy-and-tcy-mid-x_r}{%
\subsection{\texorpdfstring{Details for \(\tc(Y)\) and
\(\tc(Y \mid X_{r})\)}{Details for \textbackslash tc(Y) and \textbackslash tc(Y \textbackslash mid X\_\{r\})}}\label{details-for-tcy-and-tcy-mid-x_r}}

Denote with \(\text{Gamma}(Y; \alpha, \beta)\) the CDF of the gamma
distribution with shape parameter \(\alpha\) and rate \(\beta\); and
\(\text{Normal}(Y; \xi, \omega^{2})\) the CDF of the normal distribution
with mean \(\xi\) and standard deviation \(\omega\). We define the
covariate-independent target
\input{tex-input/preece-baines-growth/0021-target-definition-pop.tex}\noindent
and the covariate-specific target
\input{tex-input/preece-baines-growth/0022-target-definition-cov.tex}\noindent

\hypertarget{hartmann_flexible_2020-priors}{%
\subsection{\texorpdfstring{\citet{hartmann_flexible_2020}
priors}{@hartmann\_flexible\_2020 priors}}\label{hartmann_flexible_2020-priors}}

Table \ref{tab:hartmann-priors-data} contains the priors elicited by
\citet{hartmann_flexible_2020} (extracted from the supplementary
material of that paper) for the parameters in the Preece-Baines example.
To generate the prior predictive samples displayed in Figure
\ref{fig:regression_prior_pred} in the main text, we draw, for each
user, \(\theta\) from the corresponding lognormal distribution then
compute \(h(t; \theta)\) using
\eqref{eqn:preece-baines-model-definition-two} (also in the main text,
without the error term) at 250 values of \(t\) spaced evenly between
ages \(2\) and \(18\).

\input{tex-input/preece-baines-growth/0031-hartmann-priors-data.tex}

\hypertarget{pareto-frontiers-for-the-covariate-independent-target}{%
\subsection{Pareto frontiers for the covariate-independent
target}\label{pareto-frontiers-for-the-covariate-independent-target}}

The Pareto frontier for the covariate-independent target and all values
of \(\kappa \in \mathcal{K}\) is displayed in Figure
\ref{fig:kappa_pop}.

\begin{figure}[H]

{\centering \includegraphics{plots/preece-baines-growth/pop-kappa} 

}

\caption{Pareto frontiers for each $\kappa \in \mathcal{K}$ for the \textbf{covariate-independent} example. The minimum loss point for each replicate is plotted with $\color{myredhighlight}{+}$.}\label{fig:kappa_pop}
\end{figure}

\hypertarget{final-predictive-discrepancy-values-for-the-human-growth-example}{%
\subsection{Final predictive discrepancy values for the human growth
example}\label{final-predictive-discrepancy-values-for-the-human-growth-example}}

The located optimal values of predictive discrepancy are displayed in
Figure \ref{fig:discrep_at_optima}.

\begin{figure}[H]

{\centering \includegraphics{plots/preece-baines-growth/both-optima-discrep-points} 

}

\caption{Final predictive discrepancy $\log(D(\lambda^{*}))$, or $\log(D(\lambda^{*} \mid \boldsymbol{X}))$ for the covariate-specific target. The multiple objective optimisation approach uses the previously selected values of $\kappa = 0.15$ for the covariate-independent target, and $\kappa = 0.2$ for the covariate-specific target. Horizontal jitter has been applied for readability.}\label{fig:discrep_at_optima}
\end{figure}

\hypertarget{full-marginal-prior-and-posterior-comparison-plots}{%
\subsection{Full marginal prior and posterior comparison
plots}\label{full-marginal-prior-and-posterior-comparison-plots}}

Figures \ref{fig:pb_pop_prior_post_compare} and
\ref{fig:pb_cov_prior_post_compare} are extended versions of Figure
\ref{fig:small_cov_prior_post} in the main text, and display the prior
and posterior estimates for all the parameters in \(\theta\).
Consistency and uniqueness remain, evidently, challenging and as yet
unobtainable.

\begin{landscape}
\begin{figure}

{\centering \includegraphics[width=\linewidth]{plots/preece-baines-growth/pop-priors-posteriors-compare} 

}

\caption{A comparison of the priors (\textcolor{mymidblue}{blue}) produced by our method using the covariate-independent marginal target (bottom two rows); and Hartmann et al. (2020) (second row), with no prior displayed for the flat prior scenario. The corresponding posteriors (\textcolor{myredhighlight}{red}) for individual $n = 26$ under each of these priors are displayed as dashed lines. Note that y-axes change within columns and are limited to values that clip some of the priors/posteriors for readability.}\label{fig:pb_pop_prior_post_compare}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=\linewidth]{plots/preece-baines-growth/cov-priors-posteriors-compare} 

}

\caption{Otherwise identical to Figure \ref{fig:pb_pop_prior_post_compare} but the bottom two rows display the results obtained using the covariate-specific target.}\label{fig:pb_cov_prior_post_compare}
\end{figure}
\end{landscape}

\FloatBarrier

\bibliographystyle{agsm}
\bibliography{bibliography/prior-setting.bib}



\end{document}
