\begin{algorithm}[H]
  \caption{Global two-objective Bayesian optimisation using MSPOT \citep{zaefferer_mspot_2012}}
    \label{alg:mspot-algorithmic-description}
  \begin{algorithmic}[1]
    \Require{Primary objective $D(\lambda)$, secondary objective $N(\lambda)$, initial design $\mathcal{D} = \left\{\lambda_{i}, D(\lambda_{i}), N(\lambda_{i})\right\}_{i = 1}^{N_{\text{design}} + N_{\text{pad}}}$, number of iterations $N_{\text{BO}}$, number of new points to evaluate the surrogate models at $N_{\text{new}}$, number of evaluations to add to the design within an iteration $N_{\text{eval}}$, hyperparameter support $\Lambda$}
    \Statex
    \Function{Bayesian optimisation using MSPOT}{$N_{\text{BO}}$}
      \For{$i$ in $1 \ldots N_{\text{BO}}$}
        \State Form Gaussian process (GP) approximations to $D(\lambda)$ and $N(\lambda)$ using $\mathcal{D}$
        \State Generate a new Latin hypercube design $\mathcal{N}$ of size $N_{\text{new}}$ covering $\Lambda$, such that $N_{\text{new}} \gg N_{\text{design}}$
        \For{$k$ in $1 \ldots N_{\text{new}}$}
          \State Use the GPs to estimate $\hat{D}(\lambda_{k})$ and $\hat{N}(\lambda_{k})$
          \State Add these to $\mathcal{N}$ so that $\mathcal{N}_{k} = \left\{\lambda_{k}, \hat{D}(\lambda_{k}), \hat{N}(\lambda_{k})\right\}$
        \EndFor
        \State Truncate $\mathcal{N}$ to $N_{\text{eval}}$ points according to the non-dominated sorting rank and hypervolume contribution \citep{beume_sms-emoa_2007, deb_multi-objective_2001, deb_fast_2002, beume_complexity_2009} of each point in $\left\{D(\lambda_{k}), N(\lambda_{k})\right\}_{k = 1}^{N_{\text{new}}}$ with $N_{\text{eval}} \ll N_{\text{new}}$
        \For{$j$ in $1 \ldots N_{\text{eval}}$}
          \State Evaluate the objectives $D(\lambda_{j})$ and $N(\lambda_{j})$ for $\lambda_{j} \in \mathcal{N}$
          \State Add these evaluations to $\mathcal{D} = \mathcal{D} \cup \left\{\lambda_{j}, D(\lambda_{j}), N(\lambda_{j})\right\}$
        \EndFor
      \EndFor %\Comment{\textit{N.B. at this stage $\lvert \mathcal{D} \rvert = N_{\text{design}} + N_{\text{BO}} N_{\text{eval}}$}}
      \State Compute the Pareto frontier $\mathcal{P} = \left\{\lambda_{i}, D(\lambda_{i}), N(\lambda_{i})\right\}_{i = 1}^{\lvert \mathcal{P} \rvert}$ from $\mathcal{D} = \left\{\lambda_{i}, D(\lambda_{i}), N(\lambda_{i})\right\}_{i = 1}^{N_{\text{design}} + N_{\text{pad}} + N_{\text{BO}} N_{\text{eval}}}$ \citep[see]{kung_finding_1975}
      \State \textbf{return:} $\mathcal{P}$ and $\mathcal{D}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

%$\kappa$, $N(\lambda)$, $N_{\text{CRS2}}$, $N_{\text{BO}}$, $N_{\text{batch}}$,

% bayes opt batch algorithm
% batch to design for next batch algorithm
% pbbo algorithm