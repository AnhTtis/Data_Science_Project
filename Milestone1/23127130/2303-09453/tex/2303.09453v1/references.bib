@article{lu2018structurally,
  title={Structurally incoherent low-rank nonnegative matrix factorization for image classification},
  author={Lu, Yuwu and Yuan, Chun and Zhu, Wenwu and Li, Xuelong},
  journal={IEEE Transactions on Image Processing},
  volume={27},
  number={11},
  pages={5248--5260},
  year={2018},
  publisher={IEEE}
}

@article{lu2019robust,
  title={Robust flexible preserving embedding},
  author={Lu, Yuwu and Wong, Wai Keung and Lai, Zhihui and Li, Xuelong},
  journal={IEEE transactions on cybernetics},
  volume={50},
  number={10},
  pages={4495--4507},
  year={2019},
  publisher={IEEE}
}

@article{lu2018low,
  title={Low-rank 2-D neighborhood preserving projection for enhanced robust image representation},
  author={Lu, Yuwu and Lai, Zhihui and Li, Xuelong and Wong, Wai Keung and Yuan, Chun and Zhang, David},
  journal={IEEE transactions on cybernetics},
  volume={49},
  number={5},
  pages={1859--1872},
  year={2018},
  publisher={IEEE}
}

@article{lu2015low,
  title={Low-rank preserving projections},
  author={Lu, Yuwu and Lai, Zhihui and Xu, Yong and Li, Xuelong and Zhang, David and Yuan, Chun},
  journal={IEEE transactions on cybernetics},
  volume={46},
  number={8},
  pages={1900--1913},
  year={2015},
  publisher={IEEE}
}

@misc{bib:UCI,
    author = "Dua, Dheeru and Graff, Casey",
    year = "2017",
    title = "{UCI} Machine Learning Repository",
    url = "http://archive.ics.uci.edu/ml",
    institution = "University of California, Irvine, School of Information and Computer Sciences" } 
    
@misc{bib:Delve,
    author = "{University of Toronto}",
    year = "1996",
    title = "Delve Datasets",
    url = "http://www.cs.toronto.edu/~delve/data/datasets.html",
    institution = "University of Toronto" } 
    
@Article{bib:Spyromitros-Xioufis2016,
author="Spyromitros-Xioufis, Eleftherios
and Tsoumakas, Grigorios
and Groves, William
and Vlahavas, Ioannis",
title="Multi-target regression via input space expansion: treating targets as inputs",
journal="Machine Learning",
year="2016",
volume="104",
number="1",
pages="55--98",
abstract="In many practical applications of supervised learning the task involves the prediction of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are continuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classification domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called stacked single-target and ensemble of regressor chains, by adapting two popular multi-label classification methods of this family. Furthermore, we highlight an inherent problem of these methods---a discrepancy of the values of the additional input variables between training and prediction---and develop extensions that use out-of-sample estimates of the target variables during training in order to tackle this problem. The results of an extensive experimental evaluation carried out on a large and diverse collection of datasets show that, when the discrepancy is appropriately mitigated, the proposed methods attain consistent improvements over the independent regressions baseline. Moreover, two versions of Ensemble of Regression Chains perform significantly better than four state-of-the-art methods including regularization-based multi-task learning methods and a multi-objective random forest approach.",
issn="1573-0565",
doi="10.1007/s10994-016-5546-z",
url="http://dx.doi.org/10.1007/s10994-016-5546-z"
}

@article{bib:rosalba2019,
  title={Ab initio molecular dynamics studies of {Au38(SR)24} isomers under heating},
  author={Juarez-Mosqueda, Rosalba and Malola, Sami and Häkkinen, Hannu},
  journal={The European Physical Journal D},
  volume={73},
  number={62},
  issue={3},
  doi={10.1140/epjd/e2019-90441-5},
  year={2019},
  publisher={Springer},
  issn = {1434-6079},
}

@article{bib:pihlajamaki2020,
author = {Pihlajamäki, Antti and Hämäläinen, Joonas and Linja, Joakim and Nieminen, Paavo and Malola, Sami and Kärkkäinen, Tommi and Häkkinen, Hannu},
title = {Monte {C}arlo Simulations of {Au38(SCH3)24} Nanocluster Using Distance-Based Machine Learning Methods},
journal = {The Journal of Physical Chemistry A},
volume = {124},
number = {23},
pages = {4827-4836},
year = {2020},
doi = {10.1021/acs.jpca.0c01512},
    note ={PMID: 32412747},
URL = { 
        https://doi.org/10.1021/acs.jpca.0c01512
},
eprint = { 
        https://doi.org/10.1021/acs.jpca.0c01512
}
}


@article{bib:lecun1998,
title = "Gradient-based learning applied to document recognition",
abstract = "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.",
keywords = "Convolutional neural networks, Document recognition, Finite state transducers, Gradient-based learning, Graph transformer networks, Machine learning, Neural networks, Optical character recognition (OCR)",
author = "Yann LeCun and L{\'e}on Bottou and Yoshua Bengio and Patrick Haffner",
year = "1998",
month = dec,
day = "1",
doi = "10.1109/5.726791",
language = "English (US)",
volume = "86",
pages = "2278--2323",
journal = "Proceedings of the IEEE",
issn = "0018-9219",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
number = "11",

}

Ilmeisesti preprintti korvattu samannimisellä 2022-julkaisulla. Tämä 2017-entry deletoitavissa(?):
@misc{bib:huo2017,
    title={Unified Representation of Molecules and Crystals for Machine Learning},
    author={Haoyan Huo and Matthias Rupp},
    year={2017},
    eprint={1704.06439},
    archivePrefix={arXiv},
    primaryClass={physics.chem-ph}
}

@article{bib:huo2022,
doi = {10.1088/2632-2153/aca005},
url = {https://dx.doi.org/10.1088/2632-2153/aca005},
year = {2022},
month = nov,
publisher = {IOP Publishing},
volume = {3},
number = {4},
pages = {045017},
author = {Haoyan Huo and Matthias Rupp},
title = {Unified representation of molecules and crystals for machine learning},
journal = {Machine Learning: Science and Technology},
abstract = {Accurate simulations of atomistic systems from first principles are limited by computational cost. In high-throughput settings, machine learning can reduce these costs significantly by accurately interpolating between reference calculations. For this, kernel learning approaches crucially require a representation that accommodates arbitrary atomistic systems. We introduce a many-body tensor representation that is invariant to translations, rotations, and nuclear permutations of same elements, unique, differentiable, can represent molecules and crystals, and is fast to compute. Empirical evidence for competitive energy and force prediction errors is presented for changes in molecular structure, crystal chemistry, and molecular dynamics using kernel regression and symmetric gradient-domain machine learning as models. Applicability is demonstrated for phase diagrams of Pt-group/transition-metal binary systems.}
}


@article{huang2015extreme,
  title={What are extreme learning machines? {F}illing the gap between {F}rank {R}osenblatt’s dream and {J}ohn von {N}eumann’s puzzle},
  author={Huang, Guang-Bin},
  journal={Cognitive Computation},
  volume={7},
  number={3},
  pages={263--278},
  year={2015},
  publisher={Springer}
}

@article{bib:huang2006,
title = {Extreme learning machine: Theory and applications},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {489-501},
year = {2006},
note = {Neural Networks},
issn = {0925-2312},
doi = {10.1016/j.neucom.2005.12.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206000385},
author = {Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew},
keywords = {Feedforward neural networks, Back-propagation algorithm, Extreme learning machine, Support vector machine, Real-time learning, Random node},
abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to “Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks”, Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.}
}

@article{karkkainen2002mlp,
  title={{MLP} in layer-wise form with applications to weight decay},
  author={K{\"a}rkk{\"a}inen, Tommi},
  journal={Neural Computation},
  volume={14},
  number={6},
  pages={1451--1480},
  year={2002},
  publisher={MITP}
}

@book{nocedal2006numerical,
  title={Numerical Optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{bib:desouza2015,
title = "Minimal Learning Machine: A novel supervised distance-based approach for regression and classification",
journal = "Neurocomputing",
volume = "164",
pages = "34 - 44",
year = "2015",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2014.11.073",
url = "http://www.sciencedirect.com/science/article/pii/S0925231215003021",
author = "Amauri Holanda {de Souza} and Francesco Corona and Guilherme A. Barreto and Yoan Miche and Amaury Lendasse",
keywords = "Learning machines, Supervised learning, Regression, Pattern classification",
abstract = "In this work, a novel supervised learning method, the Minimal Learning Machine (MLM), is proposed. Learning in MLM consists in building a linear mapping between input and output distance matrices. In the generalization phase, the learned distance map is used to provide an estimate of the distance from K output reference points to the unknown target output value. Then, the output estimation is formulated as multilateration problem based on the predicted output distance and the locations of the reference points. Given its general formulation, the Minimal Learning Machine is inherently capable of operating on nonlinear regression problems as well as on multidimensional response spaces. In addition, an intuitive extension of the MLM is proposed to deal with classification problems. A comprehensive set of computer experiments illustrates that the proposed method achieves accuracies that are comparable to more traditional machine learning methods for regression and classification thus offering a computationally valid alternative to such approaches."
}

@misc{dataset:linja2020,
title="Au38Q",
url="https://dx.doi.org/10.5281/zenodo.4268064",}

@article{bib:linja2020, 
title={Do Randomized Algorithms Improve the Efficiency of Minimal Learning Machine?}, 
volume={2}, 
ISSN={2504-4990}, 
url={http://dx.doi.org/10.3390/make2040029}, 
DOI={10.3390/make2040029}, 
number={4}, 
journal={Machine Learning and Knowledge Extraction}, 
publisher={MDPI AG}, 
author={Linja, Joakim and Hämäläinen, Joonas and Nieminen, Paavo and Kärkkäinen, Tommi}, 
year={2020}, 
month=nov, 
pages={533–557}}


@book{bib:kokoska2000,
  title={CRC Standard Probability and Statistics Tables and Formulae},
  author={Kokoska, S. and Zwillinger, D.},
  year={2000},
  publisher={Chapman \& Hall},
  city={New York},
  country={USA},
}

@ARTICLE{bib:scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{bib:scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@online{online:olson2016,
author={Randal S. Olson},
title={ReliefF},
year={2016},
url={https://pypi.org/project/ReliefF/},
}

@inbook{bib:sun2015,
author = {Yijun Sun and Jin Yao and Steve Goodison},
title = {Feature Selection for Nonlinear Regression and its Application to Cancer Research},
booktitle = {Proceedings of the 2015 SIAM International Conference on Data Mining},
chapter = {},
year={2015},
pages = {73-81},
doi = {10.1137/1.9781611974010.9},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974010.9},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974010.9}
}

@Article{bib:numpy,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}


@article{bib:kraskov2004_mutualInfo,
  title = {Estimating mutual information},
  author = {Kraskov, Alexander and St\"ogbauer, Harald and Grassberger, Peter},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066138},
  numpages = {16},
  year = {2004},
  month = jun,
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.69.066138},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138}
}

@InProceedings{bib:horn2020_autofeat,
author="Horn, Franziska
and Pack, Robert
and Rieger, Michael",
editor="Cellier, Peggy
and Driessens, Kurt",
title="The autofeat Python Library for Automated Feature Engineering and Selection",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="111--120",
abstract="This paper describes the autofeat Python library, which provides a scikit-learn style linear regression model with automated feature engineering and selection capabilities. Complex non-linear machine learning models such as neural networks are in practice often difficult to train and even harder to explain to non-statisticians, who require transparent analysis results as a basis for important business decisions. While linear models are efficient and intuitive, they generally provide lower prediction accuracies. Our library provides a multi-step feature engineering and selection process, where first a large pool of non-linear features is generated, from which then a small and robust set of meaningful features is selected, which improve the prediction accuracy of a linear model while retaining its interpretability.",
isbn="978-3-030-43823-4"
}

@article{bib:benoit2013,
title = {Feature selection for nonlinear models with extreme learning machines},
journal = {Neurocomputing},
volume = {102},
pages = {111-124},
year = {2013},
note = {Advances in Extreme Learning Machines (ELM 2011)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2011.12.055},
url = {https://www.sciencedirect.com/science/article/pii/S0925231212004468},
author = {Frénay Benoît and Mark {van Heeswijk} and Yoan Miche and Michel Verleysen and Amaury Lendasse},
keywords = {Extreme learning machines, Regression, Feature selection, Regularisation},
abstract = {In the context of feature selection, there is a trade-off between the number of selected features and the generalisation error. Two plots may help to summarise feature selection: the feature selection path and the sparsity-error trade-off curve. The feature selection path shows the best feature subset for each subset size, whereas the sparsity-error trade-off curve shows the corresponding generalisation errors. These graphical tools may help experts to choose suitable feature subsets and extract useful domain knowledge. In order to obtain these tools, extreme learning machines are used here, since they are fast to train and an estimate of their generalisation error can easily be obtained using the PRESS statistics. An algorithm is introduced, which adds an additional layer to standard extreme learning machines in order to optimise the subset of selected features. Experimental results illustrate the quality of the presented method.}
}

@article{bib:boloncanedo2013MELKEEN-SAMA-KUIN-BibEntriesissa-MUTTA-ERI-ENTIIAMIKS-TERV-PAAVO,
title = {A review of feature selection methods on synthetic data},
journal = {Knowledge and Information Systems},
author = {Bolón-Canedo, Verónica and Sánchez-Maroño, Noelia and Alonso-Betanzos, Amparo},
year = {2013},
pages = {483-519},
colume = {34},
issue = {3},
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
issn = {0219-3116},
url = {https://doi.org/10.1007/s10115-012-0487-8},
doi = {10.1007/s10115-012-0487-8}
}

@article{bib:solorio2020, 
author = {Solorio-Fernández, Saúl and Carrasco-Ochoa, J. Ariel and Martínez-Trinidad, José Fco.},
year = {2020},
title = {A review of unsupervised feature selection methods},
journal = {Artificial Intelligence Review},
pages = {907-948},
volume = {53},
issue = {2},
issn = {1573-7462},
url = {https://doi.org/10.1007/s10462-019-09682-y},
doi = {10.1007/s10462-019-09682-y},
abstract = {In recent years, unsupervised feature selection methods have raised considerable interest in many research areas; this is mainly due to their ability to identify and select relevant features without needing class label information. In this paper, we provide a comprehensive and structured review of the most relevant and recent unsupervised feature selection methods reported in the literature. We present a taxonomy of these methods and describe the main characteristics and the fundamental ideas they are based on. Additionally, we summarized the advantages and disadvantages of the general lines in which we have categorized the methods analyzed in this review. Moreover, an experimental comparison among the most representative methods of each approach is also presented. Finally, we discuss some important open challenges in this research area.}
}

@INPROCEEDINGS{bib:yue2018,
  author={Yue, Yang and Li, Ying and Yi, Kexin and Wu, Zhonghai},
  booktitle={2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)}, 
  title={Synthetic Data Approach for Classification and Regression}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/ASAP.2018.8445094}}

@INPROCEEDINGS{bib:hittmeir2019,
  author={Hittmeir, Markus and Ekelhart, Andreas and Mayer, Rudolf},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={Utility and Privacy Assessments of Synthetic Data for Regression Tasks}, 
  year={2019},
  volume={},
  number={},
  pages={5763-5772},
  doi={10.1109/BigData47090.2019.9005476}}

@inproceedings{bib:navot2005,
author = {Navot, Amir and Shpigelman, Lavi and Tishby, Naftali and Vaadia, Eilon},
title = {Nearest Neighbor Based Feature Selection for Regression and Its Application to Neural Activity},
year = {2005},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.},
booktitle = {Proceedings of the 18th International Conference on Neural Information Processing Systems},
pages = {996–1002},
numpages = {7},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'05}
}

@article{bib:fisher2015,
    author = {Fisher, Charles K. and Mehta, Pankaj},
    title = "{Bayesian feature selection for high-dimensional linear regression via the Ising approximation with applications to genomics}",
    journal = {Bioinformatics},
    volume = {31},
    number = {11},
    pages = {1754-1761},
    year = {2015},
    month = jan,
    abstract = "{Motivation : Feature selection, identifying a subset of variables that are relevant for predicting a response, is an important and challenging component of many methods in statistics and machine learning. Feature selection is especially difficult and computationally intensive when the number of variables approaches or exceeds the number of samples, as is often the case for many genomic datasets. Results : Here, we introduce a new approach—the Bayesian Ising Approximation (BIA)—to rapidly calculate posterior probabilities for feature relevance in L2 penalized linear regression. In the regime where the regression problem is strongly regularized by the prior, we show that computing the marginal posterior probabilities for features is equivalent to computing the magnetizations of an Ising model with weak couplings. Using a mean field approximation, we show it is possible to rapidly compute the feature selection path described by the posterior probabilities as a function of the L2 penalty. We present simulations and analytical results illustrating the accuracy of the BIA on some simple regression problems. Finally, we demonstrate the applicability of the BIA to high-dimensional regression by analyzing a gene expression dataset with nearly 30 000 features. These results also highlight the impact of correlations between features on Bayesian feature selection. Availability and implementation : An implementation of the BIA in C++, along with data for reproducing our gene expression analyses, are freely available at http://physics.bu.edu/∼pankajm/BIACode . Contact : charleskennethfisher@gmail.com or ckfisher@bu.edu or pankajm@bu.eduSupplementary information : Supplementary data are available at Bioinformatics online. }",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btv037},
    url = {https://doi.org/10.1093/bioinformatics/btv037},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/31/11/1754/17122119/btv037.pdf},
}

@ARTICLE{bib:zhai2014,
  author={Zhai, Yiteng and Ong, Yew-Soon and Tsang, Ivor W.},
  journal={IEEE Computational Intelligence Magazine}, 
  title={The Emerging "Big Dimensionality"}, 
  year={2014},
  volume={9},
  number={3},
  pages={14-26},
  doi={10.1109/MCI.2014.2326099}}

@misc{bib:hnpai:online,
  author={Hannu Häkkinen and Tommi Kärkkäinen},
  title={Structure prediction of hybrid nanoparticles via artificial intelligence (HNP-AI)},
  year={2018},
  howpublished ={\url{http://www.aka.fi/globalassets/32akatemiaohjelmat/tekoaly/hankekuvaukset-aipse/abstract_hakkinen.pdf}},
  publisher={Academy of Finland},
  urldate={13.3.2019},
  note={(Referenced 19.5.2021)}
  }
  
@article{bib:sutton2020,
author = {Sutton, Christopher and Boley, Mario and Ghiringhelli, Luca M. and Rupp, Matthias and Vreeken, Jilles and Scheffler, Matthias},
year = {2020},
title = {Identifying domains of applicability of machine learning models for materials science},
journal = {Nature Communications},
volume = {11},
issue = {1},
page = {4428},
issn = {2041-1723},
url = {https://doi.org/10.1038/s41467-020-17112-9},
doi = {10.1038/s41467-020-17112-9},
abstract = {Although machine learning (ML) models promise to substantially accelerate the discovery of novel materials, their performance is often still insufficient to draw reliable conclusions. Improved ML models are therefore actively researched, but their design is currently guided mainly by monitoring the average model test error. This can render different models indistinguishable although their performance differs substantially across materials, or it can make a model appear generally insufficient while it actually works well in specific sub-domains. Here, we present a method, based on subgroup discovery, for detecting domains of applicability (DA) of models within a materials class. The utility of this approach is demonstrated by analyzing three state-of-the-art ML models for predicting the formation energy of transparent conducting oxides. We find that, despite having a mutually indistinguishable and unsatisfactory average error, the models have DAs with distinctive features and notably improved performance.},
}

@article{bib:malola2019, 
author = {Malola, Sami and Nieminen, Paavo and Pihlajamäki, Antti and Hämäläinen, Joonas and Kärkkäinen, Tommi and Häkkinen, Hannu},
journal = {Nature Communications},
year = {2019},
title = {A method for structure prediction of metal-ligand interfaces of hybrid nanoparticles},
pages = {3973},
volume = {10},
issue = {1},
issn = {2041-1723},
abstract = {Hybrid metal nanoparticles, consisting of a nano-crystalline metal core and a protecting shell of organic ligand molecules, have applications in diverse areas such as biolabeling, catalysis, nanomedicine, and solar energy. Despite a rapidly growing database of experimentally determined atom-precise nanoparticle structures and their properties, there has been no successful, systematic way to predict the atomistic structure of the metal-ligand interface. Here, we devise and validate a general method to predict the structure of the metal-ligand interface of ligand-stabilized gold and silver nanoparticles, based on information about local chemical environments of atoms in experimental data. In addition to predicting realistic interface structures, our method is useful for investigations on the steric effects at the metal-ligand interface, as well as for predicting isomers and intermediate structures induced by thermal dynamics or interactions with the environment. Our method is applicable to other hybrid nanomaterials once a suitable set of reference structures is available.},
url = {https://doi.org/10.1038/s41467-019-12031-w},
doi = {10.1038/s41467-019-12031-w},
}

@article{quinlan1986induction,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@inproceedings{deng2009regularized,
  title={Regularized extreme learning machine},
  author={Deng, Wanyu and Zheng, Qinghua and Chen, Lin},
  booktitle={2009 IEEE Symposium on Computational Intelligence and Data Mining},
  pages={389--395},
  year={2009},
  organization={IEEE}
}

@inproceedings{hamalainen2020problem,
  title={Problem Transformation Methods with Distance-Based Learning for Multi-Target Regression},
  author={H{\"a}m{\"a}l{\"a}inen, Joonas and K{\"a}rkk{\"a}inen, Tommi},
  booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  pages = {691--696},
  year={2020},
  organization={ESANN}
}

@article{bib:breiman2001:randomforest,
author={Leo Breiman},
year={2001},
title={Random Forests},
journal={Machine Learning},
issue={1},
volume={45},
pages={5--32},
issn={1573-0565},
url={https://doi.org/10.1023/A:1010933404324},
doi={10.1023/A:1010933404324},
abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
}

@article{tang2014feature,
  title={Feature selection for classification: A review},
  author={Tang, Jiliang and Alelyani, Salem and Liu, Huan},
  journal={Data classification: Algorithms and applications},
  pages={37},
  year={2014},
  publisher={CRC press}
}

@article{maldonado2009wrapper,
  title={A wrapper method for feature selection using support vector machines},
  author={Maldonado, Sebasti{\'a}n and Weber, Richard},
  journal={Information Sciences},
  volume={179},
  number={13},
  pages={2208--2217},
  year={2009},
  publisher={Elsevier}
}

@article{wu2020supervised,
  title={Supervised feature selection with orthogonal regression and feature weighting},
  author={Wu, Xia and Xu, Xueyuan and Liu, Jianhong and Wang, Hailing and Hu, Bin and Nie, Feiping},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2020},
  publisher={IEEE}
}

@article{ditzler2018sequential,
  title={A sequential learning approach for scaling up filter-based feature subset selection},
  author={Ditzler, Gregory and Polikar, Robi and Rosen, Gail},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={29},
  number={6},
  pages={2530--2544},
  year={2018},
  publisher={IEEE}
}

@article{zhai2014emerging,
  title={The emerging" big dimensionality"},
  author={Zhai, Yiteng and Ong, Yew-Soon and Tsang, Ivor W},
  journal={IEEE Computational Intelligence Magazine},
  volume={9},
  number={3},
  pages={14--26},
  year={2014},
  publisher={IEEE}
}

@ARTICLE(RucRogKab_NNComp1990,
  author = {D. W. Ruck and S. K. Rogers and M. Kabrisky}, 
  title = {Feature Selection Using a Multilayer Perceptron}, 
  journal = {Neural Network Computing},
  pages = {40--48}, 
  volume = 2,
  number = 2,
  year = 1990,
)

@BOOK{Miller_1990,
 author   = {A. J. Miller},
 title    = {Subset Selection in Regression},
 publisher= "Chapman and Hall",
 year     = 1990,
}

@INPROCEEDINGS{ZurMalClo_I3EProc1994,
  author = {J. M. Zurada and A. Malinowski and I. Cloete},
  title = {Sensitivity Analysis for Minimization of Input Data Dimension for Feedforward Neural Network},
  booktitle = {Proceedings of the 1994 IEEE International Symposium on Circuits and Systems (ISCAS'94)},
  pages = {447--450},
  year = 1994,
}

@INPROCEEDINGS{BonWei_AIProc1994,
  author = {B. V. Bonnlander and A. S. Weigend},
  title = {Selecting input variables using mutual information and nonparamteric density estimation},
  booktitle = {Proceedings of the 1994 International Symposium on Artificial Neural Networks (ISANN'94)},
  pages = {42--50},
  year = 1994,
}

@INPROCEEDINGS{JohKohPfl_MLProc1994,
  author = {G. H. John and R. Kohavi and K. Pfleger},
  title = {Irrelevant Features and the Subset Selection Problem},
  booktitle = {Proceedings of the 11th International Conference on Machine Learning},
  pages = {121--129},
  year = 1994,
}

@COMMENT(DimBouLek_NPLets1995__oli_duplikaatti,
  author = {Y. Dimopoulos and P. Bourret and S. Lek}, 
  title = {Use of some sensitivity criteria for choosing networks with good generalization ability}, 
  journal = {Neural Processing Letters},
  pages = {1--4}, 
  volume = 2,
  number = 6,
  year = 1995,
)

@article{dimopoulos1995use,
  title={Use of some sensitivity criteria for choosing networks with good generalization ability},
  author={Dimopoulos, Yannis and Bourret, Paul and Lek, Sovan},
  journal={Neural Processing Letters},
  volume={2},
  number={6},
  pages={1--4},
  year={1995},
  publisher={Springer}
}

@INPROCEEDINGS{Cze_ICANN1996,
  author = {T. Czernichow},
  title = {Architecture selection through statistical sensitivity analysis},
  booktitle = {Proceedings of the 1996 International Conference on Artificial Neural Networks (ICANN'96)},
  pages = {179--184},
  year = 1996,
}

@ARTICLE(ZurMalUsu_NeCo1997,
  author = {J. M. Zurada and A. Malinowski and S. Usui}, 
  title = {Perturbation method for deleting redundant inputs of perceptron networks}, 
  journal = {Neurocomputing},
  pages = {177--193},
  volume = 14,
  year = 1997,
)

@ARTICLE(KohJoh_AI1997,
  author = {R. Kohavi and G. H. John}, 
  title = {Wrappers for feature subset selection}, 
  journal = {Artificial Intelligence},
  pages = {273--324},
  volume = 97,
  year = 1997,
)

@ARTICLE(BluLan_AI1997,
  author = {A. L. Blum and P. Langley}, 
  title = {Selection of relevant features and examples in machine learning}, 
  journal = {Artificial Intelligence},
  pages = {245--271}, 
  volume = 97,
  year = 1997,
)

@ARTICLE(DasLiu_IDA1997,
  author = {M. Dash and H. Liu}, 
  title = {Feature Selection for Classification},
  journal = {Intelligent Data Analysis},
  pages = {131--156}, 
  volume = 1,
  year = 1997,
)

@BOOK{LiuMot_1998,
 author = {H. Liu and H. Motoda},
 title = {Feature Selection for Knowledge Discovery and Data Mining},
 publisher = {Kluwer},
 address = {Norwell, MA},
  year = 1998,
}

@PHDTHESIS(Hal_PhD1999,
  author = {M. A. Hall}, 
  title = {Correlation-based Feature Selection for Machine Learning}, 
  school = {Department of Computer Science},
  address = {The University of Waikato}, 
  type = "{P}hd {T}hesis",
  year = 1999,
)

@ARTICLE(Beretal2_NC2000,
  author = {J. L. Bernier and J. Ortega and E. Ros and I. Rojas and A. Prieto},
  title = {A Quantitative Study of Fault Tolerance, Noise Immunity, and Generalization Ability of {MLP}s}, 
  journal = {Neural Computation},
  pages = {2941--2964}, 
  volume = 12,
  year = 2000,
)

@ARTICLE(Eng_I3ETransNN2001,
  author = {A. P. Engelbrecht}, 
  title = {A New Pruning Heuristic Based on Variance Analysis of Sensitivity Information}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {1386--1399}, 
  volume = 12,
  number = 6,
  year = 2001,
)

@ARTICLE(KwaCho_I3ETransNN2002,
  author = {N. Kwak and C.-H. Choi}, 
  title = {Input Feature Selection for Classification Problems}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {143--159}, 
  volume = 13,
  number = 1,
  year = 2002,
)

@ARTICLE(Mao_I3ETransNN2002,
  author = {K. Z. Mao}, 
  title = {Fast Orthogonal Forward Selection Algorithm for Feature Subset Selection}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {1218--1224}, 
  volume = 13,
  number = 5,
  year = 2002,
)

@ARTICLE(GuyEli_MLR2003,
  author = {I. Guyon and A. Elisseeff}, 
  title = {An Introduction to Variable and Feature Selection},
  journal = {Journal of Machine Learning Research},
  pages = {1157--1182}, 
  volume = 3,
  year = 2003,
)

@ARTICLE(Wisetal_ComStaDA2003,
  author = {J. W. Wisnowski and J. R. Simpson and D. C. Montgomery and G. C. Runger},
  title = {Resampling methods for variable selection in robust regression}, 
  journal = {Computational Statistics \& Data Analysis},
  pages = {341--355},
  volume = 43,
  year = 2003,
)

@ARTICLE(GevDimLek_EcoMod2003,
  author = {M. Gevrey and I. Dimopaulos and S. Lek},
  title = {Review and comparison of methods to study the contribution of variables in artificial neural network models}, 
  journal = {Ecological Modelling},
  pages = {249--264}, 
  volume = 160,
  year = 2003,
)

@ARTICLE(Sinetal_I3ETransNN2004,
  author = {V. Sindhwani and S. Rakshit and D. Deodhare and D. Erdogmus and J. C. Principe and P. Niyogi},
  title = {Feature Selection with {MLP}s and {SVM}s Based on Maximum Output Information}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {937--948}, 
  volume = 15,
  number = 4,
  year = 2004,
)

@ARTICLE(YuLiu_MLR2004,
  author = {L. Yu and H. Liu}, 
  title = {Efficient Feature Selection via Analysis of Relevance and Redundancy},
  journal = {Journal of Machine Learning Research},
  pages = {1205--1224}, 
  volume = 5,
  year = 2004,
)

@ARTICLE(ChoHua_I3ETransNN2005,
  author = {T. W. S. Chow and D. Huang}, 
  title = {Estimating Optimal Feature Subset Using Efficient Estimation of High-Dimensional Mutual Information}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {213--224}, 
  volume = 16,
  number = 1,
  year = 2005,
)

@ARTICLE(EleTagMil_NN2005,
  author = {A. Eleuteri and R. Tagliaferri and L. Milano}, 
  title = {A novel information geometric approach to variable selection in {MLP} networks},
  journal = {Neural Networks},
  pages = {1309--1318}, 
  volume = 18,
  year = 2005,
)

@ARTICLE(LiuYu_I3ETransKDE2005,
  author = {H. Liu and L. Yu}, 
  title = {Toward Integrating Feature Selection Algorithms for Classification and Clustering}, 
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {491--502}, 
  volume = 17,
  number = 4,
  year = 2005,
)

@INPROCEEDINGS{HeCaiNiy_NIPSProc2005,
  title={Laplacian score for feature selection},
  author={X. He and D. Cai and P. Niyogi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={507--514},
  year={2005},
}

@INPROCEEDINGS{HanKno_NNProc2006,
  author = {J. Handl and J. Knowles},
  title = {Semi-supervised feature Selection via multiobjective optimization},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks},
  pages = {3319--3326},
  year = 2006,
}

@ARTICLE(Lietal_I3ETransNN2006,
  author = {J. Li and M. T. Manry and P. L. Narasimha and C. Yu}, 
  title = {Feature Selection Using a Piecewise Linear Network}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {1101--1115}, 
  volume = 17,
  number = 5,
  year = 2006,
)

@ARTICLE(LiJiaZha_I3ETransNN2006,
  author = {H. Li and T. Jiang and K. Zhang}, 
  title = {Efficient and Robust Feature Extraction by Maximum Margin Criterion}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {157--165}, 
  volume = 17,
  number = 1,
  year = 2006,
)

@ARTICLE(CapLeoMar_NN2007,
  author = {R. Capparuccia and R. De Leone and E. Marchitto}, 
  title = {Integrating support vector machines and neural networks}, 
  journal = {Neural Networks},
  pages = {590--597}, 
  volume = 20,
  year = 2007,
)

@ARTICLE(CohDroRup_NC2007,
  author = {S. Cohen and G. Dror and E. Ruppin},
  title = {Feature Selection via Coalitional Game Theory}, 
  journal = {Neural Computation},
  pages = {1939--1961}, 
  volume = 19,
  year = 2007,
)

@ARTICLE(Fraetal_NeCo2007,
  author = {D. Fran{\c c}ois and F. Rossi and V. Wertz and M. Verleysen}, 
  title = {Resampling methods for parameter-free and robust feature selection with mutual information}, 
  journal = {Neurocomputing},
  pages = {1276--1288}, 
  volume = 70,
  year = 2007,
)

@ARTICLE(LiPen_NeCo2007,
  author = {K. Li and J.-X. Peng}, 
  title = {Neural input selection - A fast model-based approach}, 
  journal = {Neurocomputing},
  pages = {762--769}, 
  volume = 70,
  year = 2007,
)

@ARTICLE(QuaQue_I3ETransNN2007,
  author = {K. H. Quah and C. Quek}, 
  title = {MCES: A Novel {M}onte {C}arlo Evaluation Selection Approach for Objective Feature Selections}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {431--448}, 
  volume = 18,
  number = 2,
  year = 2007,
)  

@ARTICLE(KocNai_NC2007,
  author = {I. Koch and K. Naito}, 
  title = {Dimension Selection for Feature Selection and Dimension Reduction with Principal and Independent Component Analysis}, 
  journal = {Neural Computation},
  pages = {513--545}, 
  volume = 19,
  year = 2007,
)

@ARTICLE(ChaPal_I3ETransNN2008,
  author = {D. Chakraborty and N. R. Pal}, 
  title = {Selecting Useful Groups of Features in a Connectionist Framework}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {381--396}, 
  volume = 19,
  number = 3,
  year = 2008,
)

@INPROCEEDINGS{Eiretal_ESANN2008,
  author = {E. Eirola and E. Liiti\"ainen and A. Lendasse and F. Corona and M. Verleysen},
  title = {Using the {D}elta test for variable selection},
  booktitle = {Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN 2008},
  pages = {25--30},
  year = 2008,
}

@ARTICLE(RomSop_I3ETransNN2008,
  author = {E. Romero and J. M. Sopena},
  title = {Performing Feature Selection with Multilayer Perceptrons}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {431--441}, 
  volume = 19,
  number = 3,
  year = 2008,
)

@ARTICLE(WanZhoChu_I3ETransNN2008,
  author = {L. Wang and N. Zhou and F. Chu},
  title = {A General Wrapper Approach to Selection of Class-Dependent Features}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {1267--1278}, 
  volume = 19,
  number = 7,
  year = 2008,
)

@INPROCEEDINGS{Yanetal_ProcSMC2008,
  author = {J.-B. Yang and K.-Q. Shen and C.-J. Ong and X.-P. Li},
  title = {Feature selection via sensitivity analysis of {MLP} probabilistic outputs},
  booktitle = {Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics},
  pages = {774--779},
  year = 2008,
}

@ARTICLE(HuaGanCho_NeCo2008,
  author = {D. Huang and Z. Gan and T. W.S. Chow}, 
  title = {Enhanced feature selection models using gradient-based and point injection techniques}, 
  journal = {Neurocomputing},
  pages = {3114--3123},
  volume = 71,
  year = 2008,
)

@ARTICLE(TikHol_NeCo2008,
  author = {J. Tikka and J. Hollm{\' e}n}, 
  title = {Sequential input selection algorithm for long-term prediction of time series}, 
  journal = {Neurocomputing},
  pages = {2604--2615},
  volume = 71,
  year = 2008,
)

@ARTICLE(ZhaLuHe_NeCo2008,
  author = {J. Zhao and K. Lu and X. He}, 
  title = {Locality sensitive semi-supervised feature selection}, 
  journal = {Neurocomputing},
  pages = {1842--1849},
  volume = 71,
  year = 2008,
)

@ARTICLE(Estetal_I3ETransNN2009,
  author = {P. A. Est{\' e}vez and M. Tesmer and C. A. Perez and J. M. Zurada}, 
  title = {Normalized Mutual Information Feature Selection}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {189--201}, 
  volume = 20,
  number = 2,
  year = 2009,
)

@ARTICLE(Gar_I3ETransNN2009,
  author = {N. Garc{\' i}a-Pedrajas}, 
  title = {Constructing Ensembles of Classifiers by Means of Weighted Instance Selection}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {258--277}, 
  volume = 20,
  number = 2,
  year = 2009,
)

@ARTICLE(XinHu_I3ETransNN2009,
  author = {H.-J. Xing and B.-G. Hu}, 
  title = {Two-Phase Construction of Multilayer Perceptrons Using Information Theory}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {715--721}, 
  volume = 20,
  number = 4,
  year = 2009,
)

@ARTICLE(BaiMil_NN2009,
  author = {K. Bailly and M. Milgram}, 
  title = {Boosting feature selection for Neural Network based regression}, 
  journal = {Neural Networks},
  pages = {748--756}, 
  volume = 22,
  year = 2009,
)

@ARTICLE(GomVerFle_NeCo2009,
  author = {V. G{\' o}mez-Verdejo and M. Verleysen and J. Fleury}, 
  title = {Information-theoretic feature selection for functional data classification},
  journal = {Neurocomputing},
  pages = {3580--3589}, 
  volume = 72,
  year = 2009,
)

@ARTICLE(LiuLiuZha_NeCo2009,
  author = {H. Liu and L. Liu and H. Zhang}, 
  title = {Boosting feature selection using information metric for classification},
  journal = {Neurocomputing},
  pages = {295--303}, 
  volume = 73,
  year = 2009,
)

@ARTICLE(Zha_AnnStat2009,
  author = {T. Zhang}, 
  title = {Some Sharp Performance Bounds for Least Squares Regression with {$L_1$} Regularization},
  journal = {The Annals of Statistics},
  pages = {2109--2144}, 
  volume = 37,
  year = 2009,
)

@ARTICLE(KabIslMur_NeCo2010,
  author = {Md. M. Kabir and Md. M. Islam and K. Murase}, 
  title = {A new wrapper feature selection approach using neural network}, 
  journal = {Neurocomputing},
  pages = {3273--3283}, 
  volume = 73,
  year = 2010,
)

@ARTICLE(Sunetal_I3ETransNN2010,
  author = {B.-Y. Sun and X.-M. Zhang and J. Li and X.-M. Mao}, 
  title = {Feature Fusion Using Locally Linear Embedding for Classification}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {163--168}, 
  volume = 21,
  number = 1,
  year = 2010,
)

@ARTICLE(Xuetal_I3ETransNN2010,
  author = {Z. Xu and I. King and M. R.-T. Lyu and R. Jin}, 
  title = {Discriminative Semi-Supervised Feature Selection Via Manifold Regularization}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {1033--1047}, 
  volume = 21,
  number = 7,
  year = 2010,
)

@ARTICLE(YanHoSiu_I3ETransNN2010,
  author = {S.-S. Yang and C.-L. Ho and S. Siu}, 
  title = {Computing and Analyzing the Sensitivity of {MLP} Due to the Errors of the i.i.d. Inputs and Weights Based on {CLT}}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {1882--1891}, 
  volume = 21,
  number = 12,
  year = 2010,
)

@ARTICLE(ZhoWanShe_I3ETransNN2010,
  author = {L. Zhou and L. Wang and C. Shen},
  title = {Feature Selection With Redundancy-Constrained Class Separability}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {853--858}, 
  volume = 21,
  number = 5,
  year = 2010,
)

@ARTICLE(Tinetal_NC2010,
  author = {J.-A. Ting and A. D'Souza and S. Vijayakumar and S. Schaal}, 
  title = {Efficient Learning and Feature Selection in High-Dimensional Regression}, 
  journal = {Neural Computation},
  pages = {831--886}, 
  volume = 22,
  year = 2010,
)

@ARTICLE(Penetal_NeCo2011,
  author = {J.-X. Peng and S. Ferguson and K. Rafferty and P. D. Kelly}, 
  title = {An efficient feature selection method for mobile devices with application to activity recognition}, 
  journal = {Neurocomputing},
  pages = {3543--3552}, 
  volume = 74,
  year = 2011,
)

@ARTICLE(WinDuaSmi_I3ETransNN2011,
  author = {R. Windeatt and R. Duangsoithong and R. Smith}, 
  title = {Embedded Feature Ranking for Ensemble {MLP} Classifiers}, 
  journal = {IEEE Transactions on Neural Networks},
  pages = {988--994},
  volume = 22,
  number = 6,
  year = 2011,
)

@INPROCEEDINGS{DoqVer_IWANNProc2011,
  author = {G. Doquire and M. Verleysen},
  title = {Feature Selection for Multi-label Classification Problems},
  booktitle = {Lecture Notes in Computer Science 6691},
  pages = {9--16},
  year = 2011,
}

@ARTICLE(Xiaetal_I3ETransNNLS2012,
  author = {S. Xiang and F. Nie and G. Meng and C. Pan and C. Zhang}, 
  title = {Discriminative Least Squares Regression for Multiclass Classification and Feature Selection}, 
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1738--1754},
  volume = 23,
  number = 11,
  year = 2012,
)

@INPROCEEDINGS{DutDutSil_HIS2012,
  author = {D. Dutta and P. Dutta and J. Sil},
  title = {Simultaneous feature selection and clustering for categorical features using multi objective},
  booktitle = {Proceedings of the 12th International Conference on Hybrid Intelligent Systems},
  pages = {191--196},
  year = 2012,
}

@INPROCEEDINGS{Zhaetal_ProcMLC2012,
  author = {M.-Y. Zhai and R.-H. Yu and S.-F. Zhang and J.-H. Zhai},
  title = {Feature Selection Based on Extreme Learning Machine},
  booktitle = {Proceedings of the 2012 International Conference on Machine Learning and Cybernetics},
  pages = {157--162},
  year = 2012,
}

@ARTICLE(WeiLiTan_NeCo2012,
  author = {D. Wei and S. Li and M. Tan}, 
  title = {Graph embedding based feature selection}, 
  journal = {Neurocomputing},
  pages = {115--125},
  volume = 93,
  year = 2012,
)

@ARTICLE(Shoetal_AdvMSE2013,
  author = {M. H. Shojaeefard and M. Akbari and M. Tahani and F. Farhani}, 
  title = {Sensitivity Analysis of the Artificial Neural Network Outputs in Friction Stir Lap Joining of Aluminum to Brass}, 
  journal = {Advances in Material Science and Engineering},
  pages = {1--7},
  volume = 2013,
  year = 2013,
)


@ARTICLE{BenHin_I3ETransKDE2013,
  author={Benabdeslem, Khalid and Hindawi, Mohammed},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Efficient Semi-Supervised Feature Selection: Constraint, Relevance, and Redundancy}, 
  year={2014},
  volume={26},
  number={5},
  pages={1131-1143},
  doi={10.1109/TKDE.2013.86}
}

@ARTICLE(Lietal_I3ETransKDE2013,
  author = {Z. Li and J. Liu and Y. Yang and X. Zhou and H. Lu}, 
  title = {Clustering-Guided Sparse Structural Learning for Unsupervised Feature Selection}, 
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  year = 2013,
  note = "to appear",
)

@PHDTHESIS(Sov_PhD2014,
  author = {D. Sovilj}, 
  title = {Learning Methods for Variable Selection and Time Series Prediction}, 
  school = {Department of Information and Computer Science},
  address = {The Aalto University}, 
  type = "{P}hd {T}hesis",
  year = 2014,
)

@book{breiman2017classification,
  title={Classification and regression trees},
  author={Breiman, Leo},
  year={2017},
  publisher={Routledge}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{genuer2010variable,
  title={Variable selection using random forests},
  author={Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
  journal={Pattern Recognition Letters},
  volume={31},
  number={14},
  pages={2225--2236},
  year={2010},
  publisher={Elsevier}
}

@book{jolliffe2011principal,
  title={Principal component analysis},
  author={Jolliffe, Ian},
  year={2011},
  publisher={Springer}
}

@article{fisher1936use,
  title={The use of multiple measurements in taxonomic problems},
  author={Fisher, Ronald A},
  journal={Annals of eugenics},
  volume={7},
  number={2},
  pages={179--188},
  year={1936},
  publisher={Wiley Online Library}
}

@article{rao1948utilization,
  title={The utilization of multiple measurements in problems of biological classification},
  author={Rao, C Radhakrishna},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  volume={10},
  number={2},
  pages={159--203},
  year={1948},
  publisher={JSTOR}
}

@INPROCEEDINGS(KarESANN12019,
  author = {Tommi Kärkkäinen}, 
  title = {Model selection for Extreme Minimal Learning Machine using sampling}, 
  booktitle = {Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN 2019},
  year = 2019,
  note = {6 pages, to appear},
)


%Ko. lehden juttujen viitetiedot
@article{bi2019enhanced,
  title={An enhanced high-order Boltzmann machine for feature engineering},
  author={Bi, Xiaojun and Wang, Haibo},
  journal={Engineering Applications of Artificial Intelligence},
  volume={78},
  pages={37--52},
  year={2019},
  publisher={Elsevier}
}

@article{zhu2015integrated,
  title={An integrated feature selection and cluster analysis techniques for case-based reasoning},
  author={Zhu, Guo-Niu and Hu, Jie and Qi, Jin and Ma, Jin and Peng, Ying-Hong},
  journal={Engineering Applications of Artificial Intelligence},
  volume={39},
  pages={14--22},
  year={2015},
  publisher={Elsevier}
}

@article{moradi2015graph,
  title={A graph theoretic approach for unsupervised feature selection},
  author={Moradi, Parham and Rostami, Mehrdad},
  journal={Engineering Applications of Artificial Intelligence},
  volume={44},
  pages={33--45},
  year={2015},
  publisher={Elsevier}
}

@article{hu2015hybrid,
  title={Hybrid filter--wrapper feature selection for short-term load forecasting},
  author={Hu, Zhongyi and Bao, Yukun and Xiong, Tao and Chiong, Raymond},
  journal={Engineering Applications of Artificial Intelligence},
  volume={40},
  pages={17--27},
  year={2015},
  publisher={Elsevier}
}

@article{liang2017image,
  title={Image feature selection using genetic programming for figure-ground segmentation},
  author={Liang, Yuyu and Zhang, Mengjie and Browne, Will N},
  journal={Engineering Applications of Artificial Intelligence},
  volume={62},
  pages={96--108},
  year={2017},
  publisher={Elsevier}
}

@article{zare2016relevant,
  title={Relevant based structure learning for feature selection},
  author={Zare, Hadi and Niazi, Mojtaba},
  journal={Engineering Applications of Artificial Intelligence},
  volume={55},
  pages={93--102},
  year={2016},
  publisher={Elsevier}
}

@article{raza2017redefining,
  title={Redefining core preliminary concepts of classic Rough Set Theory for feature selection},
  author={Raza, Muhammad Summair and Qamar, Usman},
  journal={Engineering Applications of Artificial Intelligence},
  volume={65},
  pages={375--387},
  year={2017},
  publisher={Elsevier}
}

@article{labani2018novel,
  title={A novel multivariate filter method for feature selection in text classification problems},
  author={Labani, Mahdieh and Moradi, Parham and Ahmadizar, Fardin and Jalili, Mahdi},
  journal={Engineering Applications of Artificial Intelligence},
  volume={70},
  pages={25--37},
  year={2018},
  publisher={Elsevier}
}

@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}

@article{adadi2018peeking,
  title={Peeking inside the black-box: a survey on explainable artificial intelligence ({XAI})},
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE access},
  volume={6},
  pages={52138--52160},
  year={2018},
  publisher={IEEE}
}

@article{arrieta2020explainable,
  title={Explainable Artificial Intelligence ({XAI}): Concepts, taxonomies, opportunities and challenges toward responsible {AI}},
  author={Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and Raja Chatila and Francisco Herrera},
  journal={Information Fusion},
  volume={58},
  pages={82--115},
  year={2020},
  publisher={Elsevier}
}

@article{burkart2021survey,
  title={A Survey on the Explainability of Supervised Machine Learning},
  author={Burkart, Nadia and Huber, Marco F.},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={245--317},
  year={2021}
}

@book{huber2004robust,
  title={Robust statistics},
  author={Huber, Peter J},
  volume={523},
  year={2004},
  publisher={John Wiley \& Sons}
}

@inproceedings{bib:horn2019autofeat,
  title={The autofeat Python Library for Automated Feature Engineering and Selection},
  author={Horn, Franziska and Pack, Robert and Rieger, Michael},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={111--120},
  year={2019},
  organization={Springer}
}


@incollection{HamKar2020,
  title={Newton's Method for Minimal Learning Machine},
  author={H{\" a}m{\" a}l{\" a}inen, Joonas and K{\" a}rkk{\" a}inen, Tommi},
  booktitle={Computational Sciences and Artificial Intelligence in Industry -- New digital technologies for solving future societal and economical challenges},
  pages={},
  year={2020},
  publisher={Springer},
  note={(to appear)}
}

@inproceedings{Powell1987,
  title={Radial basis function for multivariable interpolation: a review},
  author={Powell, Michael J. D.},
  booktitle={Algorithms for Approximation},
  publisher = {Clarendon Press, Oxford},
  pages = {143--167},
  year={1987},
}

@ARTICLE{BroLowComSys1988,
   author = {D.~S.~Broomhead and D.~Lowe},
   title = {Multivariable functional interpolation and adaptive networks},
   journal = {Complex Systems},
   volume = {2},
   pages = {321--355},
   year = {1988},
}

@article{poggio1990networks,
  title={Networks for approximation and learning},
  author={Poggio, Tomaso and Girosi, Federico},
  journal={Proceedings of the IEEE},
  volume={78},
  number={9},
  pages={1481--1497},
  year={1990},
  publisher={IEEE}
}

@article{park1991universal,
  title={Universal approximation using radial-basis-function networks},
  author={Park, Jooyoung and Sandberg, Irwin W},
  journal={Neural Computation},
  volume={3},
  number={2},
  pages={246--257},
  year={1991},
  publisher={MIT Press}
}

@article{uykan2000analysis,
  title={Analysis of input-output clustering for determining centers of RBFN},
  author={Uykan, Zekeriya and Guzelis, Cuneyt and {\c{C}}elebi, M Ertugrul and Koivo, Heikki N},
  journal={IEEE transactions on neural networks},
  volume={11},
  number={4},
  pages={851--858},
  year={2000},
  publisher={IEEE}
}

@article{pekalska2001automatic,
  title={Automatic pattern recognition by similarity representations},
  author={Pekalska, Elzbieta and Duin, Robert PW},
  journal={Electronics Letters},
  volume={37},
  number={3},
  pages={159--160},
  year={2001},
  publisher={IET}
}

@article{pekalska2001generalized,
  title={A generalized kernel approach to dissimilarity-based classification},
  author={Pekalska, Elzbieta and Paclik, Pavel and Duin, Robert PW},
  journal={Journal of machine learning research},
  volume={2},
  number={Dec},
  pages={175--211},
  year={2001}
}

@book{suykens2002least,
  title={Least squares support vector machines},
  author={Suykens, Johan AK and Van Gestel, Tony and De Brabanter, Jos},
  year={2002},
  publisher={World Scientific}
}

@article{liao2003relaxed,
  title={Relaxed conditions for radial-basis function networks to be universal approximators},
  author={Liao, Yi and Fang, Shu-Cherng and Nuttle, Henry LW},
  journal={Neural Networks},
  volume={16},
  number={7},
  pages={1019--1028},
  year={2003},
  publisher={Elsevier}
}

@article{paclik2003dissimilarity,
  title={Dissimilarity-based classification of spectra: computational issues},
  author={Pacl{\i}k, Pavel and Duin, Robert PW},
  journal={Real-Time Imaging},
  volume={9},
  number={4},
  pages={237--244},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{wang2007learning,
  title={On learning with dissimilarity functions},
  author={Wang, Liwei and Yang, Cheng and Feng, Jufu},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={991--998},
  year={2007}
}

@article{balcan2008theory,
  title={A theory of learning with similarity functions},
  author={Balcan, Maria-Florina and Blum, Avrim and Srebro, Nathan},
  journal={Machine Learning},
  volume={72},
  number={1-2},
  pages={89--112},
  year={2008},
  publisher={Springer}
}

@article{pekalska2008beyond,
  title={Beyond traditional kernels: Classification in two dissimilarity-based representation spaces},
  author={Pekalska, Elzbieta and Duin, Robert PW},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume={38},
  number={6},
  pages={729--744},
  year={2008},
  publisher={IEEE}
}

@article{nguema2008model,
  title={Model-based classification with dissimilarities: a maximum likelihood approach},
  author={Ngu{\'e}ma, Eug{\`e}ne-Patrice Ndong and Saint-Pierre, Guillaume},
  journal={Pattern Analysis and Applications},
  volume={11},
  number={3-4},
  pages={281--298},
  year={2008},
  publisher={Springer}
}

@article{chen2009similarity,
  title={Similarity-based classification: Concepts and algorithms},
  author={Chen, Yihua and Garcia, Eric K and Gupta, Maya R and Rahimi, Ali and Cazzanti, Luca},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Mar},
  pages={747--776},
  year={2009}
}

@article{wang2009theory,
  title={Theory and algorithm for learning with dissimilarity functions},
  author={Wang, Liwei and Sugiyama, Masashi and Yang, Cheng and Hatano, Kohei and Feng, Jufu},
  journal={Neural Computation},
  volume={21},
  number={5},
  pages={1459--1484},
  year={2009},
  publisher={MIT Press}
}

@phdthesis{chen2010strategies,
  title={Strategies for similarity-based learning},
  author={Chen, Yihua},
  school = {University of Washington, Program of Electrical Engineering},
  year={2010},
}

@inproceedings{kar2011similarity,
  title={Similarity-based learning via data driven embeddings},
  author={Kar, Purushottam and Jain, Prateek},
  booktitle={Advances in neural information processing systems},
  pages={1998--2006},
  year={2011}
}

@article{zerzucha2012dissimilarity,
  title={Dissimilarity partial least squares applied to non-linear modeling problems},
  author={Zerzucha, P and Daszykowski, M and Walczak, B},
  journal={Chemometrics and Intelligent Laboratory Systems},
  volume={110},
  number={1},
  pages={156--162},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{plasencia2014towards,
  title={Towards scalable prototype selection by genetic algorithms with fast criteria},
  author={Plasencia-Cala{\~n}a, Yenisel and Orozco-Alzate, Mauricio and M{\'e}ndez-V{\'a}zquez, Heydi and Garc{\'\i}a-Reyes, Edel and Duin, Robert PW},
  booktitle={Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)},
  pages={343--352},
  year={2014},
  organization={Springer}
}

@article{schleif2015indefinite,
  title={Indefinite proximity learning: A review},
  author={Schleif, Frank-Michael and Tino, Peter},
  journal={Neural Computation},
  volume={27},
  number={10},
  pages={2039--2096},
  year={2015},
  publisher={MIT Press}

}

@article{zhang2015dissimilarity,
  title={A dissimilarity-based imbalance data classification algorithm},
  author={Zhang, Xueying and Song, Qinbao and Wang, Guangtao and Zhang, Kaiyuan and He, Liang and Jia, Xiaolin},
  journal={Applied Intelligence},
  volume={42},
  number={3},
  pages={544--565},
  year={2015},
  publisher={Springer}
}

@article{coutinho2015text,
  title={Text classification using compression-based dissimilarity measures},
  author={Coutinho, David Pereira and Figueiredo, M{\'a}rio AT},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  volume={29},
  number={05},
  pages={1553004},
  year={2015},
  publisher={World Scientific}
}

@mastersthesis{jongs2015dissimilarity,
  title={Dissimilarity based learning},
  author={Jongs, Niels},
  school = {Leiden University, Department of Mathematics},
  year={2015}
}

@article{plasencia2017scalable,
  title={Scalable Prototype Selection by Genetic Algorithms and Hashing},
  author={Plasencia-Cala{\~n}a, Yenisel and Orozco-Alzate, Mauricio and M{\'e}ndez-V{\'a}zquez, Heydi and Garc{\'\i}a-Reyes, Edel and Duin, Robert PW},
  journal={arXiv preprint arXiv:1712.09277},
  year={2017}
}

@inproceedings{silva2018improving,
  title={Improving Regression Models by Dissimilarity Representation of Bio-chemical Data},
  author={Silva-Mata, Francisco Jose and Jim{\'e}nez, Catherine and Barcas, Gabriela and Estevez-Bres{\'o}, David and Acosta-Mendoza, Niusvel and Gago-Alonso, Andres and Talavera-Bustamante, Isneri},
  booktitle={Iberoamerican Congress on Pattern Recognition},
  pages={64--71},
  year={2018},
  organization={Springer}
}

@article{abanda2019review,
  title={A review on distance based time series classification},
  author={Abanda, Amaia and Mori, Usue and Lozano, Jose A},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  number={2},
  pages={378--412},
  year={2019},
  publisher={Springer}
}

@article{cao2019random,
  title={Random forest dissimilarity based multi-view learning for Radiomics application},
  author={Cao, Hongliu and Bernard, Simon and Sabourin, Robert and Heutte, Laurent},
  journal={Pattern Recognition},
  volume={88},
  pages={185--197},
  year={2019},
  publisher={Elsevier}
}

@article{costa2020dissimilarity,
  title={The dissimilarity approach: a review},
  author={Costa, Yandre MG and Bertolini, Diego and Britto, Alceu S and Cavalcanti, George DC and Oliveira, Luiz ES},
  journal={Artificial Intelligence Review},
  volume = {53},
  pages={2783--2808},
  year={2020},
  publisher={Springer}
}

@inproceedings{williams2001using,
  title={Using the Nystr{\"o}m method to speed up kernel machines},
  author={Williams, Christopher KI and Seeger, Matthias},
  booktitle={Advances in neural information processing systems},
  pages={682--688},
  year={2001}
}

@article{drineas2005nystrom,
  title={On the Nystr{\"o}m method for approximating a Gram matrix for improved kernel-based learning},
  author={Drineas, Petros and Mahoney, Michael W},
  journal={journal of machine learning research},
  volume={6},
  number={Dec},
  pages={2153--2175},
  year={2005}
}

@article{sun2015review,
  title={A review of Nystr{\"o}m methods for large-scale machine learning},
  author={Sun, Shiliang and Zhao, Jing and Zhu, Jiang},
  journal={Information Fusion},
  volume={26},
  pages={36--48},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{zhang2008improved,
  title={Improved Nystr{\"o}m low-rank approximation and error analysis},
  author={Zhang, Kai and Tsang, Ivor W and Kwok, James T},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1232--1239},
  year={2008}
}

@inproceedings{pourkamali2018randomized,
  title={Randomized clustered nystrom for large-scale kernel machines},
  author={Pourkamali-Anaraki, Farhad and Becker, Stephen and Wakin, Michael B},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{zhang2010clustered,
  title={Clustered Nystr{\"o}m method for large scale manifold learning and dimension reduction},
  author={Zhang, Kai and Kwok, James T},
  journal={IEEE Transactions on Neural Networks},
  volume={21},
  number={10},
  pages={1576--1587},
  year={2010},
  publisher={IEEE}
}

@inproceedings{oglic2017nystrom,
  title={Nystr{\"o}m method with kernel k-means++ samples as landmarks},
  author={Oglic, Dino and G{\"a}rtner, Thomas},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2652--2660},
  year={2017},
  organization={JMLR. org}
}

@article{kumar2012sampling,
  title={Sampling methods for the Nystr{\"o}m method},
  author={Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Apr},
  pages={981--1006},
  year={2012}
}

@inproceedings{mesquita2015minimal,
  title={A minimal learning machine for datasets with missing values},
  author={Mesquita, Diego P. P. and Gomes, Jo{\~a}o P. P. and de Souza Junior, Amauri H.},
  booktitle={22nd International Conference on Neural Information Processing - ICONIP 2015},
  pages={565--572},
  year={2015}
}

@article{mesquita2017euclidean,
  title={Euclidean distance estimation in incomplete datasets},
  author={Mesquita, Diego P. P. and Gomes, Jo{\~a}o P. P. and de Souza Junior, Amauri H  and Nobre, Juv{\^e}ncio S.},
  journal={Neurocomputing},
  volume={248},
  pages={11--18},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{RobustMLMesann2017,
  title={A Robust Minimal Learning Machine based on the M-Estimator},
  author={Gomes, Joao P. P. and Mesquita, Diego P. P. and Freire, Ananda and Souza J{\'u}nior, Amauri H. and K{\"a}rkk{\"a}inen, Tommi},
  booktitle = {Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN 2017},
  year = 2017,
  pages = {383--388},
}

@article{mesquita2017ensemble,
  title={Ensemble of efficient minimal learning machines for classification and regression},
  author={Mesquita, Diego P. P. and Gomes, Jo{\~a}o P. P. and de Souza Junior, Amauri H. },
  journal={Neural Processing Letters},
  volume={46},
  number={3},
  pages={751--766},
  year={2017},
  publisher={Springer}
}

@inproceedings{marinho2016new,
  title={A new approach to Human Activity Recognition using Machine Learning techniques},
  author={Marinho, Leandro B. and de Souza Junior, Amauri H. and Rebou{\c{c}}as Filho, Pedro P.},
  booktitle={International Conference on Intelligent Systems Design and Applications},
  pages={529--538},
  year={2016},
  organization={Springer}
}

@inproceedings{EMLMesann2018,
  title={Extreme minimal learning machine},
  author={K{\"a}rkk{\"a}inen, Tommi},
  booktitle = {Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN 2018},
  year = 2018,
  pages = {237--242},
}

@inproceedings{de2016efficient,
  title={Efficient minimal learning machines with reject option},
  author={de Oliveira, Adonias C. and Gomes, Jo{\~a}o P. P. and Rocha Neto, Ajalmar R. and de Souza Junior, Amauri H.},
  booktitle={2016 5th Brazilian Conference on Intelligent Systems (BRACIS)},
  pages={397--402},
  year={2016}
}

@article{caldas2018fast,
  title={Fast {Co-MLM}: An Efficient Semi-supervised {Co}-training Method Based on the Minimal Learning Machine},
  author={Caldas, Weslley L. and Gomes, Jo{\~a}o P. P. and Mesquita, Diego P. P.},
  journal={New Generation Computing},
  volume={36},
  number={1},
  pages={41--58},
  year={2018},
  publisher={Springer}
}

@article{marinho2017novel,
  title={A novel mobile robot localization approach based on topological maps using classification with reject option in omnidirectional images},
  author={Marinho, Leandro B. and Almeida, Jefferson S. and Souza, Jo{\~a}o W. M. and Albuquerque, Victor H. C. and Rebou{\c{c}}as Filho, Pedro P.},
  journal={Expert Systems with Applications},
  volume={72},
  pages={1--17},
  year={2017},
  publisher={Elsevier}
}

@article{marinho2018novel,
  title={A novel mobile robot localization approach based on classification with rejection option using computer vision},
  author={Marinho, Leandro B. and Rebou{\c{c}}as Filho, Pedro P. and Almeida, Jefferson S. and Souza, Jo{\~a}o W. M. and de Souza Junior, Amauri H. and de Albuquerque, Victor H. C.},
  journal={Computers \& Electrical Engineering},
  volume={68},
  pages={26--43},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{RSesann2018,
  title={Opposite neighborhood: a new method to select reference points of minimal learning machines},
  author={Dias, Madson L. D. and de Souza, Lucas S. and da Rocha Neto, Ajalmar R. and de Souza Junior, Amauri H.},
  booktitle = {Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN 2018},
  year = 2018,
  pages = {201--206},
}

@article(Mor2012,
  author = {J. G. Moreno-Torres and J. A. S{\' a}ez and F. Herrera}, 
  title = {Study on the Impact of Partition-Induced Dataset Shift on k-fold Cross-Validation},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1304--1312}, 
  volume = 23,
  number = 8,
  year = 2012,
)

@book{friedman2001elements,
  title={The Elements of Statistical Learning},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  volume={2},
  year={2001},
  publisher={Springer series in statistics New York}
}

@article{gronau2007optimal,
  title={Optimal implementations of UPGMA and other common clustering algorithms},
  author={Gronau, Ilan and Moran, Shlomo},
  journal={Information Processing Letters},
  volume={104},
  number={6},
  pages={205--210},
  year={2007},
  publisher={Elsevier}
}

@article{kohonen1998self,
  title={The self-organizing map},
  author={Kohonen, Teuvo},
  journal={Neurocomputing},
  volume={21},
  number={1-3},
  pages={1--6},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{ester1996density,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise.},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  booktitle={Proceedings of 2nd International Conference on Knowledge Discovery and Data Mining},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}

@article{xu2005survey,
  title={Survey of clustering algorithms},
  author={Xu, Rui and Wunsch, Donald},
  journal={IEEE Transactions on Neural Networks},
  volume={16},
  number={3},
  pages={645--678},
  year={2005},
  publisher={Ieee}
}

@article{sokal1958statistical,
  title={A statistical method for evaluating systematic relationship},
  author={Sokal, Robert R.},
  journal={University of Kansas Science Bulletin},
  volume={28},
  pages={1409--1438},
  year={1958}
}

@article{lloyd1982least,
  title={Least squares quantization in {PCM}},
  author={Lloyd, Stuart},
  journal={IEEE transactions on Information Theory},
  volume={28},
  number={2},
  pages={129--137},
  year={1982},
  publisher={IEEE}
}

@article{hamalainen2017comparison,
  title={Comparison of Internal Clustering Validation Indices for Prototype-Based Clustering},
  author={H{\"a}m{\"a}l{\"a}inen, Joonas and Jauhiainen, Susanne and K{\"a}rkk{\"a}inen, Tommi},
  journal={Algorithms},
  volume={10},
  number={3},
  pages={105},
  year={2017},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{celebi2013comparative,
  title={A comparative study of efficient initialization methods for the k-means clustering algorithm},
  author={Celebi, M. Emre and Kingravi, Hassan A. and Vela, Patricio A.},
  journal={Expert Systems with Applications},
  volume={40},
  number={1},
  pages={200--210},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{arthur2007k,
  title={k-means++: The advantages of careful seeding},
  author={Arthur, David and Vassilvitskii, Sergei},
  booktitle={Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1027--1035},
  year={2007},
  organization={Society for Industrial and Applied Mathematics}
}

@article{gonzalez1985clustering,
  title={Clustering to minimize the maximum intercluster distance},
  author={Gonzalez, Teofilo F},
  journal={Theoretical Computer Science},
  volume={38},
  pages={293--306},
  year={1985},
  publisher={Elsevier}
}

@article{Niewiadomska_2009,
Author = {Niewiadomska-Szynkiewicz, Ewa and Marks, Michal},
Title = {{Optimization Schemes for Wireless Sensor Network Localization}},
Journal = {{International Journal of Applied Mathematics and Computer Science}},
Year = {{2009}},
Volume = {{19}},
Number = {{2}},
Pages = {{291-302}},
Month = jun,
DOI = {{10.2478/v10006-009-0025-3}},
ISSN = {{1641-876X}},
ORCID-Numbers = {{Marks, Michal/0000-0002-0819-5119}},
Unique-ID = {{ISI:000267899900010}},
}

@book{fletcher2013practical,
  title={Practical methods of optimization},
  author={Fletcher, Roger},
  year={2013},
  isbn = {0-471-91547-5},
 publisher = {Wiley-Interscience},
 address = {New York, NY, USA},

}

@book{haykin1994neural,
  title={Neural networks: a comprehensive foundation},
  author={Haykin, Simon S},
  year={2001},
  publisher={Tsinghua University Press},
  isbn = {0132733501},
  address = {Upper Saddle River, NJ, USA},
}


@article{smith1997susan,
Author = {Smith, SM and Brady, JM},
Title = {{SUSAN - A new approach to low level image processing}},
Journal = {{International Journal of Computer Vision}},
Year = {{1997}},
Volume = {{23}},
Number = {{1}},
Pages = {{45-78}},
Month = may,
DOI = {{10.1023/A:1007963824710}},
ISSN = {{0920-5691}},
ORCID-Numbers = {{Smith, Stephen/0000-0001-8166-069X}},
Unique-ID = {{ISI:A1997XK55900003}},
}

@inproceedings{rosten2006machine,
Author = {Rosten, E and Drummond, T},
Editor = {{Leonardis, A and Bischof, H and Pinz, A}},
Title = {{Machine learning for high-speed corner detection}},
Booktitle = {{Computer Vision - ECCV 2006 , PT 1, Proceedings}},
Series = {{Lecture Notes in Computer Science 
}},
Year = {{2006}},
Volume = {{3951}},
Number = {{1}},
Pages = {{430-443}},
}

@article{wilcoxon1945individual,
Author = {Wilcoxon, F},
Title = {{Individual Comparisons by Ranking Methods}},
Journal = {{Biometrics Bulletin}},
Year = {{1945}},
Volume = {{1}},
Number = {{6}},
Pages = {{80-83}}
}

@article{wilson200reduction,
Author = {Wilson, DR and Martinez, TR},
Title = {{Reduction techniques for instance-based learning algorithms}},
Journal = {{Machine Learning}},
Year = {{2000}},
Volume = {{38}},
Number = {{3}},
Pages = {{257-286}},
Month = mar,
}

@inproceedings{jankowski2004comparison,
Author = {Jankowski, N and Grochowski, M},
Editor = {{Rutkowski, L and Siekmann, J and Tadeusiewicz, R and Zadeh, LA}},
Title = {{Comparison of instances selection algorithms I. Algorithms survey}},
Booktitle = {{Artificial Intelligence and Soft Computing - ICAISC 2004}},
Series = {{Lecture Notes in Artificial Intelligence}},
Year = {{2004}},
Volume = {{3070}},
Pages = {{598-603}},
}

@article{pkekalska2006prototype,
Author = {Pekalska, E and Duin, RPW and Paclik, P},
Title = {{Prototype selection for dissimilarity-based classifiers}},
Journal = {{Pattern Recognition}},
Year = {{2006}},
Volume = {{39}},
Number = {{2}},
Pages = {{189-208}},
Month = feb
}

@article{garcia2012prototype,
Author = {Garcia, Salvador and Derrac, Joaquin and Ramon Cano, Jose and Herrera,
   Francisco},
Title = {{Prototype Selection for Nearest Neighbor Classification: Taxonomy and
   Empirical Study}},
Journal = {{IEEE Transactions on Pattern Analysis and Machine Intelligence}},
Year = {{2012}},
Volume = {{34}},
Number = {{3}},
Pages = {{417-435}},
Month = mar
}

@article{angiulli2007fast,
Author = {Angiulli, Fabrizio},
Title = {{Fast nearest neighbor condensation for large data sets classification}},
Journal = {{IEEE Transactions on Knowledge and Data Engineering}},
Year = {{2007}},
Volume = {{19}},
Number = {{11}},
Pages = {{1450-1464}},
Month = nov
}

@article{de2015minimal,
Author = {de Souza Junior, Amauri H. and Corona, Francesco and Barreto,
   Guilherme A. and Miche, Yoan and Lendasse, Amaury},
Title = {{Minimal Learning Machine: A novel supervised distance-based approach for
   regression and classification}},
Journal = {{Neurocomputing}},
Year = {{2015}},
Volume = {{164}},
Pages = {{34-44}},
Month = sep,
Day = {21}
}

@article{Xue2014,
Author = {Xue, Xiaowei and Yao, Min and Wu, Zhaohui and Yang, Jianhua},
Title = {{Genetic ensemble of extreme learning machine}},
Journal = {{Neurocomputing}},
Year = {{2014}},
Volume = {{129}},
Number = {{SI}},
Pages = {{175-184}},
Month = apr
}

@Article{Feng2012,
Author = {Feng, Guorui and Qian, Zhenxing and Zhang, Xinpeng},
Title = {{Evolutionary selection extreme learning machine optimization for
   regression}},
Journal = {{Soft Computing}},
Year = {{2012}},
Volume = {{16}},
Number = {{9, SI}},
Pages = {{1485-1491}},
Month = sep
}

@ARTICLE{Chen1996, 
Author = {Chen, CLP},
Title = {{A rapid supervised learning neural network for function interpolation
   and approximation}},
journal={{IEEE Transactions on Neural Networks}}, 
Year = {{1996}},
Volume = {{7}},
Number = {{5}},
Pages = {{1220-1230}},
Month = sep,
DOI = {{10.1109/72.536316}},
ISSN = {{1045-9227}},
ResearcherID-Numbers = {{Chen, C. L. Philip/O-2657-2016}},
ORCID-Numbers = {{Chen, C. L. Philip/0000-0001-5451-7230}},
Unique-ID = {{ISI:A1996VG69500014}},
}

@COMMENT{Huang2004__duplikaattiviittaus_poistettu, 
Author = {Huang, GB and Zhu, QY and Siew, CK},
Book-Group-Author = {{ieee}},
Title = {{Extreme learning machine: A new learning scheme of feedforward neural networks}},
Booktitle = {{2004 IEEE International Joint Conference on Neural Networks, Vols 1-4,
   Proceedings}},
Series = {{IEEE International Joint Conference on Neural Networks (IJCNN)}},
Year = {{2004}},
Pages = {{985-990}},
ISSN = {{1098-7576}},
ISBN = {{0-7803-8359-1}},
ResearcherID-Numbers = {{Huang, Guang-Bin/A-5035-2011}},
ORCID-Numbers = {{Huang, Guang-Bin/0000-0002-2480-4965}},
Unique-ID = {{ISI:000224941900171}},
}

@article{Ren2016,
title = "Random vector functional link network for short-term electricity load demand forecasting ",
journal = "Information Sciences ",
volume = "367–368",
number = "",
pages = "1078 - 1093",
year = "2016",
author = "Ye Ren and P.N. Suganthan and N. Srikanth and Gehan Amaratunga"
}

@article{Mesquita2016,
title = "Classification with reject option for software defect prediction ",
journal = "Applied Soft Computing ",
volume = "49",
number = "",
pages = "1085 - 1093",
year = "2016",
author = "Diego P.P. Mesquita and Lincoln S. Rocha and João Paulo P. Gomes and Ajalmar R. Rocha Neto"
}

@article{Lang2015,
    author = {Lang, Kun AND Zhang, Mingyuan AND Yuan, Yongbo},
    journal = {PLOS ONE},
    title = {Improved Neural Networks with Random Weights for Short-Term Load Forecasting},
    year = {2015},
    month = dec,
    volume = {10},
    pages = {1-14},
    number = {12}
}

@article{Weitao2016,
title = "Industrial image classification using a randomized neural-net ensemble and feedback mechanism ",
journal = "Neurocomputing ",
volume = "173, Part 3",
number = "",
pages = "708 - 714",
year = "2016",
author = "Weitao Li and Keqiong Chen and Dianhui Wang"
}

@article{jurman2012comparison,
  title={A comparison of MCC and CEN error measures in multi-class prediction},
  author={Jurman, Giuseppe and Riccadonna, Samantha and Furlanello, Cesare},
  journal={PloS one},
  volume={7},
  number={8},
  pages={e41882},
  year={2012},
  publisher={Public Library of Science}
}

@inproceedings{de2013minimal,
  title={Minimal learning machine: A new distance-based method for supervised learning},
  author={de Souza Junior, Amauri H. and Corona, Francesco and Miche, Yoan and Lendasse, Amaury and Barreto, Guilherme A. and Simula, Olli},
  booktitle={International Work-Conference on Artificial Neural Networks},
  pages={408--416},
  year={2013},
  organization={Springer}
}

@book{kelley2003solving,
  title={Solving nonlinear equations with Newton's method},
  author={Kelley, Carl T},
  volume={1},
  year={2003},
  publisher={Siam}
}
@article{jain2010data,
title = "Data clustering: 50 years beyond K-means",
journal = "Pattern Recognition Letters",
volume = "31",
number = "8",
pages = "651 - 666",
year = "2010",
note = "Award winning papers from the 19th International Conference on Pattern Recognition (ICPR)",
issn = "0167-8655",
author = "Anil K. Jain",
}

@inproceedings{coelho2014,
  author    = {David N. Coelho and
               Guilherme A. Barreto and
               Cl{\'{a}}udio M. S. Medeiros and
               Jos{\'{e}} D. A. Santos},
  title     = {Performance comparison of classifiers in the detection of short circuit
               incipient fault in a three-phase induction motor},
  booktitle = {2014 {IEEE} Symposium on Computational Intelligence for Engineering
               Solutions (CIES)},
  pages     = {42-48},
  year      = {2014}
}

@InProceedings{florencio2018,
author="Flor{\^e}ncio, Jos{\'e} A. V.
and Dias, Madson L. D.
and da Rocha Neto, Ajalmar R.
and de Souza J{\'u}nior, Amauri H.",
editor="Barreto, Guilherme A.
and Coelho, Ricardo",
title="A Fuzzy C-means-based Approach for Selecting Reference Points in Minimal Learning Machines",
booktitle="Fuzzy Information Processing",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="398--407"
}
@InProceedings{Maia2018,
author="Maia, {\'A}tilla N.
and Dias, Madson L. D.
and Gomes, Jo{\~a}o P. P.
and da Rocha Neto, Ajalmar R.",
editor="Yin, Hujun
and Camacho, David
and Novais, Paulo
and Tall{\'o}n-Ballesteros, Antonio J.",
title="Optimally Selected Minimal Learning Machine",
booktitle="Intelligent Data Engineering and Automated Learning -- IDEAL 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="670--678"
}

@article{rbfUA,
 author = {Park, Jooyoung and Sandberg, Irwin W.},
 title = {Universal Approximation Using Radial-basis-function Networks},
 journal = {Neural Computation},
 issue_date = {June 1991},
 volume = {3},
 number = {2},
 month = jun,
 year = {1991},
 pages = {246--257},
 numpages = {12},
 acmid = {110093},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{rbfUA2,
author = {Park, Jooyoung and Sandberg, Irwin W.},
title = {Approximation and Radial-Basis-Function Networks},
year = {1993},
issue_date = {March 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {5},
number = {2},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1993.5.2.305},
doi = {10.1162/neco.1993.5.2.305},
journal = {Neural Computation},
month = mar,
pages = {305-–316},
numpages = {12}
}
  


@article{elmUA,
title = "Trends in extreme learning machines: A review",
journal = "Neural Networks",
volume = "61",
pages = "32 - 48",
year = "2015",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2014.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0893608014002214",
author = "Gao Huang and Guang-Bin Huang and Shiji Song and Keyou You",
keywords = "Extreme learning machine, Classification, Clustering, Feature learning, Regression",
abstract = "Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives."
}

@article{mlpUA,
 author = {Hornik, K. and Stinchcombe, M. and White, H.},
 title = {Multilayer Feedforward Networks Are Universal Approximators},
 journal = {Neural Netw.},
 issue_date = {1989},
 volume = {2},
 number = {5},
 month = jul,
 year = {1989},
 issn = {0893-6080},
 pages = {359--366},
 numpages = {8},
 url = {http://dx.doi.org/10.1016/0893-6080(89)90020-8},
 doi = {10.1016/0893-6080(89)90020-8},
 acmid = {70408},
 publisher = {Elsevier Science Ltd.},
 address = {Oxford, UK, UK},
} 

@Article{Micchelli1986,
author="Micchelli, Charles A.",
title="Interpolation of scattered data: Distance matrices and conditionally positive definite functions",
journal="Constructive Approximation",
year="1986",
month=dec,
day="01",
volume="2",
number="1",
pages="11--22",
abstract="Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke."
}

@article{Auer95,
author = { John W.   Auer },
title = {An elementary proof of the invertibility of Distance matrices},
journal = {Linear and Multilinear Algebra},
volume = {40},
number = {2},
pages = {119-124},
year  = {1995},
publisher = {Taylor & Francis}
}

@Article{LLS,
author="Hu, Yanjun
and Zhang, Lei
and Gao, Li
and Ma, Xiaoping
and Ding, Enjie",
title="Linear system construction of multilateration based on error propagation estimation",
journal="EURASIP Journal on Wireless Communications and Networking",
year="2016",
month=jun,
day="29",
volume="2016",
number="1",
pages="154",
abstract="Iterative localization algorithms are critical part in the control of mobile autonomous robots because they feed fundamental position information to the robots. In a harsh unknown environment, the estimation of environmental noise is hardly obtained during the movement of the robots. It means that the state-of-the-art methods, which increase localization accuracy using error management, are unsuitable. In this paper, we deduced an upper bound of the localization error without knowing the precise model of environment noise when the anchor nodes have position errors. Utilizing the minimum upper bound, we can construct an optimal localization linear system of iterative localization algorithms based on least square. An algorithm of generating localization linear system is proposed by using the minimum upper bound. The algorithm reduces the impact of the shortage of environmental information on localization error propagation. Our simulation results show that the algorithm is insensitive to noise and can improve the localization accuracy by constructing a proper localization linear system with a high probability."
}


@article{HARDY,
author = {Hardy, Rolland L.},
title = {Multiquadric equations of topography and other irregular surfaces},
journal = {Journal of Geophysical Research},
volume = {76},
number = {8},
pages = {1905-1915},
keywords = {General: Mathematics, Geomorphology: Theoretical Studies},
doi = {10.1029/JB076i008p01905},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/JB076i008p01905},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/JB076i008p01905},
abstract = {A new analytical method of representing irregular surfaces that involves the summation of equations of quadric surfaces having unknown coefficients is described. The quadric surfaces are located at significant points throughout the region to be mapped. Procedures are given for solving multiquadric equations of topography that are based on coordinate data. Contoured multiquadric surfaces are compared with topography and other irregular surfaces from which the multiquadric equation was derived.},
year = {1971}
}

@inproceedings{HamKar2016,
	author = {H{\" a}m{\" a}l{\" a}inen, Joonas and K{\" a}rkk{\" a}inen, Tommi},
	title = "Initialization of Big Data Clustering using Distributionally Balanced Folding",
	booktitle = "Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN 2016",
	pages = "587-592",
	year = "2016"
}

@book{shalev2014understanding,
  title={Understanding Machine Learning: from Theory to Algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{MAL-019,
year = {2013},
volume = {5},
journal = {Foundations and Trends® in Machine Learning},
title = {Metric Learning: A Survey},
number = {4},
pages = {287-364},
author = {Brian Kulis}
}

@article{gan2013using,
  title={Using clustering analysis to improve semi-supervised classification},
  author={Gan, Haitao and Sang, Nong and Huang, Rui and Tong, Xiaojun and Dan, Zhiping},
  journal={Neurocomputing},
  volume={101},
  pages={290--298},
  year={2013},
  publisher={Elsevier}
}

@article{losing2018incremental,
  title={Incremental on-line learning: A review and comparison of state of the art algorithms},
  author={Losing, Viktor and Hammer, Barbara and Wersing, Heiko},
  journal={Neurocomputing},
  volume={275},
  pages={1261--1274},
  year={2018},
  publisher={Elsevier}
}

@incollection{aggarwal2014active,
  title={Active learning: A survey},
  author={Aggarwal, Charu C. and Kong, Xiangnan and Gu, Quanquan and Han, Jiawei and Philip, S. Yu},
  booktitle={Data Classification},
  pages={599--634},
  year={2014},
  publisher={Chapman and Hall/CRC}
}

@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017}
}

@incollection{reddy2013survey,
  title={A Survey of Partitional and Hierarchical Clustering Algorithms},
  author={Reddy, Chandan K. and Vinzamuri, Bhanukiran},
  booktitle={Data Clustering Algorithms and Applications},
  pages={87--110},
  year={2013},
  publisher={Chapman and Hall/CRC}
}

@book{dennis1996numerical,
  title={Numerical methods for unconstrained optimization and nonlinear equations},
  author={Dennis Jr, John E and Schnabel, Robert B},
  volume={16},
  year={1996},
  publisher={SIAM}
}

@inproceedings{karkkainen2015assessment,
  title={Assessment of feature saliency of {MLP} using analytic sensitivity},
  author={K{\"a}rkk{\"a}inen, Tommi},
  booktitle={European symposium on artificial neural networks, computational intelligence and machine learning-ESANN2015. Presses universitaires de Louvain},
  pages={273--278},
  year={2015}
}

@article{gevrey2003review,
  title={Review and comparison of methods to study the contribution of variables in artificial neural network models},
  author={Gevrey, Muriel and Dimopoulos, Ioannis and Lek, Sovan},
  journal={Ecological modelling},
  volume={160},
  number={3},
  pages={249--264},
  year={2003},
  publisher={Elsevier}
}

@article{shojaeefard2013sensitivity,
  title={Sensitivity analysis of the artificial neural network outputs in friction stir lap joining of aluminum to brass},
  author={Shojaeefard, Mohammad Hasan and Akbari, Mostafa and Tahani, Mojtaba and Farhani, Foad},
  journal={Advances in Materials Science and Engineering},
  volume={2013},
  year={2013},
  publisher={Hindawi}
}

@inproceedings{karkkainen2005computation,
  title={On computation of spatial median for robust data mining},
  author={K{\"a}rkk{\"a}inen, T and {\"A}yr{\"a}m{\"o}, S},
  booktitle={Evolutionary and Deterministic Methods for Design, Optimization and Control with Applications to Industrial and Societal Problems, EUROGEN, Munich},
  year={2005},
  note={(14 pages)}
}

@inproceedings{karkkainen2019model,
  title={Model selection for Extreme Minimal Learning Machine using sampling},
  author={Kärkkäinen, Tommi},
  booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
  pages={391--396},
  year={2019},
  organization={ESANN}
}

@article{bib:zhang2018,
title = {A two-stage feature selection and intelligent fault diagnosis method for rotating machinery using hybrid filter and wrapper method},
journal = {Neurocomputing},
volume = {275},
pages = {2426-2439},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217317307},
author = {Xiaolong Zhang and Qing Zhang and Miao Chen and Yuantao Sun and Xianrong Qin and Heng Li},
keywords = {Fault diagnosis, Feature selection, Intrinsic time-scale decomposition, ReliefF, Binary particle swarm optimization, Support vector machine},
abstract = {Selecting the most discriminative features from the original high dimensional feature space and finding out the optimal parameters for recognition model both have vital influences on the accuracy of fault diagnosis for complicated mechanical system. However, as these two important processes are interactional, conducting them separately may result in inferior diagnostic accuracy. This paper presents a feature selection and fault diagnosis framework which can select the optimal feature subset and optimize the parameters of SVM classifier synchronously and dynamically with the ultimate objective of achieving the highest diagnostic rate. The proposed method is based on a hybrid Filter and Wrapper framework. Since the original feature dimensionality is high which may lead to a lower computation efficiency of the process of synchronous feature selection and SVM parameters optimization, ReliefF is applied for preliminarily selecting some optimal feature candidates. Furthermore, in the reselection process, the reselection state of feature candidates and the values of classifier parameters are all encoded into BPSO particles. The optimal feature subset and the SVM model can be synchronously obtained for fault diagnosis with a high performance. Moreover, in the original feature extraction stage, intrinsic time-scale decomposition (ITD) is utilized to preprocess the nonstationary vibration signal into several PRCs. The statistical parameters in time and frequency domain of PRCs are extracted as the multitudinous original features for each signal sample. Two experimental cases including rolling bearing fault and rotor system fault are implemented to evaluate the proposed scheme. The results demonstrate that compared with some existing methods the proposed one can obtain a better comprehensive performance in the number of optimal features, training time and testing accuracy.}
}

@article{bib:solorio2016,
title = {A new hybrid filter–wrapper feature selection method for clustering based on ranking},
journal = {Neurocomputing},
volume = {214},
pages = {866-880},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216307718},
author = {Saúl Solorio-Fernández and J. Ariel Carrasco-Ochoa and José Fco. Martínez-Trinidad},
keywords = {Feature selection for clustering, Feature ranking, Laplacian score, Weighted normalized Calinski–Harabasz index},
abstract = {Feature selection is a common task in areas such as Pattern Recognition, Data Mining, and Machine Learning since it can help to improve prediction quality, reduce computation time and build more understandable models. Although feature selection for supervised classification has been widely studied, feature selection in the absence of class labels, namely feature selection for clustering or unsupervised feature selection, has been less addressed. Most existing unsupervised feature selection approaches suffer from the called “Bias of Criterion Values to Dimension,” which arises when feature subsets with different cardinality are evaluated by an internal evaluation clustering criterion. In this paper, we introduce a new hybrid filter–wrapper method for clustering, which combines the spectral feature selection framework using the Laplacian Score ranking and a modified Calinski–Harabasz index. The proposed method in the filter stage sorts the features according to their relevance, while in the wrapper stage, through our modified Calinski–Harabasz index that takes into account the cardinality of the feature subsets under evaluation, evaluates the features considering them as a subset rather than individually by using two well-known selection strategies. Experiments on different datasets show that the proposed method alleviates the “Bias of Criterion Values to Dimension” and, identifies and selects more relevant features than those selected by other reported hybrid filter–wrapper feature selection methods for clustering. Additionally, we also contrast our results against other filter and wrapper methods of the state-of-the-art.}
}

@article{bib:solorio2020-review,
author={Solorio-Fernández, Saúl and Carrasco-Ochoa, J. Ariel and Martínez-Trinidad, José Fco},
title={A review of unsupervised feature selection methods},
journal={Artificial Intelligence Review},
volume={53},
pages={907--948},
year={2020},
number={2},
issn={1573-7462},
doi={10.1007/s10462-019-09682-y},
url={https://doi.org/10.1007/s10462-019-09682-y},
keywords={},
abstract={In recent years, unsupervised feature selection methods have raised considerable interest in many research areas; this is mainly due to their ability to identify and select relevant features without needing class label information. In this paper, we provide a comprehensive and structured review of the most relevant and recent unsupervised feature selection methods reported in the literature. We present a taxonomy of these methods and describe the main characteristics and the fundamental ideas they are based on. Additionally, we summarized the advantages and disadvantages of the general lines in which we have categorized the methods analyzed in this review. Moreover, an experimental comparison among the most representative methods of each approach is also presented. Finally, we discuss some important open challenges in this research area.}}


@article{bib:liu2017,
title = {A survey of deep neural network architectures and their applications},
journal = {Neurocomputing},
volume = {234},
pages = {11-26},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.12.038},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216315533},
author = {Weibo Liu and Zidong Wang and Xiaohui Liu and Nianyin Zeng and Yurong Liu and Fuad E. Alsaadi},
keywords = {Autoencoder, Convolutional neural network, Deep learning, Deep belief network, Restricted Boltzmann machine},
abstract = {Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.}
}

@Article{bib:breiman2001,
author={Breiman, Leo},
title={Random Forests},
journal={Machine Learning},
year={2001},
month=oct,
day={01},
volume={45},
number={1},
pages={5-32},
abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
issn={1573-0565},
doi={10.1023/A:1010933404324},
url={https://doi.org/10.1023/A:1010933404324}
}

@INPROCEEDINGS{bib:kneedle,
  author={Satopaa, Ville and Albrecht, Jeannie and Irwin, David and Raghavan, Barath},
  booktitle={2011 31st International Conference on Distributed Computing Systems Workshops}, 
  title={Finding a "Kneedle" in a Haystack: Detecting Knee Points in System Behavior}, 
  year={2011},
  volume={},
  number={},
  pages={166-171},
  doi={10.1109/ICDCSW.2011.20}}


@article{bib:teisseyre2016,
title = {Feature ranking for multi-label classification using {M}arkov networks},
journal = {Neurocomputing},
volume = {205},
pages = {439-454},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S092523121630282X},
author = {Paweł Teisseyre},
keywords = {Feature selection, Multi-label learning, Markov networks, Ising model},
abstract = {We propose a simple and efficient method for ranking features in multi-label classification. The method produces a ranking of features showing their relevance in predicting labels, which in turn allows us to choose a final subset of features. The procedure is based on Markov networks and allows us to model the dependencies between labels and features in a direct way. In the first step we build a simple network using only labels and then we test how much adding a single feature affects the initial network. More specifically, in the first step we use the Ising model whereas the second step is based on the score statistic, which allows us to test a significance of added features very quickly. The proposed approach does not require transformation of label space, gives interpretable results and allows for attractive visualization of dependency structure. We give a theoretical justification of the procedure by discussing some theoretical properties of the Ising model and the score statistic. We also discuss feature ranking procedure based on fitting Ising model using l1 regularized logistic regressions. Numerical experiments show that the proposed methods outperform the conventional approaches on the considered artificial and real datasets.}
}

@article{bib:caetano2018:cstat,
title = {C-statistic: A brief explanation of its construction, interpretation and limitations},
journal = {European Journal of Cancer},
volume = {90},
pages = {130-132},
year = {2018},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2017.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S0959804917313679},
author = {S.J. Caetano and G. Sonpavde and G.R. Pond}
}

@book{bib:yates1947handbook,
  title={A Handbook on Curves and their Properties},
  author={Yates, Robert Carl},
  year={1947},
  publisher={JW Edwards}
}

@article{emmert2019understanding,
  title={Understanding Statistical Hypothesis Testing: The Logic of Statistical Inference},
  author={Emmert-Streib, Frank and Dehmer, Matthias},
  journal={Machine Learning and Knowledge Extraction},
  volume={1},
  number={3},
  pages={945--961},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{bib:kruskal1952,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2236578},
 abstract = {Suppose that C independent random samples of sizes n1, ⋯, nc are to be drawn from C univariate populations with unknown cumulative distribution functions F1, ⋯, Fc. This paper discusses a test of the null hypothesis F1 = F2 = ⋯ = Fc against alternatives of the form $F_i(x) = F(x - \theta_i)\quad (\text{all} x, i = 1, \cdots, C)$ with the θi's not all equal, or against alternatives of a much more general sort to be specified in Section 5. The test to be discussed has as its critical region large values of the ordinary F-ratio for one-way analysis of variance, computed after the observations have been replaced by their ranks in the ∑ ni-fold over-all sample. This use of ranks simplifies the distribution theory, and permits application of the test to cases where the ranks are available but the numerical values of the observations are difficult to obtain. Briefly, then, we shall consider a non-parametric analogue, based on ranks, of one-way analysis of variance. It is shown in Section 4 that, under quite general conditions, the proposed test statistic, H, is asymptotically chi-square with C - 1 degrees of freedom when the null hypothesis holds. Section 5 derives a necessary and sufficient condition that the natural family of sequences of tests based on large values of H all be consistent against a given alternative. Section 6 derives the variance of H under the null hypothesis, Section 7 derives the maximum value of H, and Section 8 gives a difference equation which may be used to obtain exact small-sample distributions under the null hypothesis. These derivations are made on the assumption of continuity for the cumulative distribution functions; Section 9 considers extensions to the possibly discontinuous case.},
 author = {William H. Kruskal},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {525--540},
 publisher = {Institute of Mathematical Statistics},
 title = {A Nonparametric test for the Several Sample Problem},
 volume = {23},
 year = {1952}
}

@article {bib:barnston1992:rmse,
      author = "Anthony G.  Barnston",
      title = "Correspondence among the Correlation, {RMSE}, and {H}eidke Forecast Verification Measures; Refinement of the {H}eidke Score",
      journal = "Weather and Forecasting",
      year = "1992",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "7",
      number = "4",
      doi = "10.1175/1520-0434(1992)007<0699:CATCRA>2.0.CO;2",
      pages=      "699 - 709",
      url = "https://journals.ametsoc.org/view/journals/wefo/7/4/1520-0434_1992_007_0699_catcra_2_0_co_2.xml"
}

@article{bib:ferreira2012,
title = {Efficient feature selection filters for high-dimensional data},
journal = {Pattern Recognition Letters},
volume = {33},
number = {13},
pages = {1794-1804},
year = {2012},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2012.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167865512001870},
author = {Artur J. Ferreira and Mário A.T. Figueiredo},
keywords = {Feature selection, Filters, Dispersion measures, Similarity measures, High-dimensional data},
abstract = {Feature selection is a central problem in machine learning and pattern recognition. On large datasets (in terms of dimension and/or number of instances), using search-based or wrapper techniques can be computationally prohibitive. Moreover, many filter methods based on relevance/redundancy assessment also take a prohibitively long time on high-dimensional datasets. In this paper, we propose efficient unsupervised and supervised feature selection/ranking filters for high-dimensional datasets. These methods use low-complexity relevance and redundancy criteria, applicable to supervised, semi-supervised, and unsupervised learning, being able to act as pre-processors for computationally intensive methods to focus their attention on smaller subsets of promising features. The experimental results, with up to 105 features, show the time efficiency of our methods, with lower generalization error than state-of-the-art techniques, while being dramatically simpler and faster.}
}

@article{bib:hamalainen2020,
  author  = {Joonas Hämäläinen and Alisson S. C. Alencar and Tommi Kärkkäinen and César L. C. Mattos and Amauri H. Souza Júnior and João P. P. Gomes},
  title   = {Minimal Learning Machine: Theoretical Results and Clustering-Based Reference Point Selection},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {239},
  pages   = {1-29},
  url     = {http://jmlr.org/papers/v21/19-786.html}
}

@INCOLLECTION{KarkPN2022,
  author = {K{\" a}rkk{\" a}inen, Tommi},
  title = {On the Role of {T}aylor's Formula in Machine Learning},
  chapter = {Impact of scientific computing on science and society},
  booktitle = {{Book title TBA}},
  publisher = {Springer Nature},
  year={2022},
  note = "(18 pages, to appear)",
}

@inproceedings{WojtasNIPS2020,
  title={Feature Importance Ranking for Deep Learning},
  author={Wojtas, Maksymilian and Chen, Ke},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS 2020)},
  pages = {5105--5114},
  volume = {33},
  year={2020},
}

@article{thorndike1953belongs,
  title={Who belongs in the family},
  author={Thorndike, Robert L},
  journal={Psychometrika},
  year={1953},
  volume={18},
  number={4},
  pages={267--276}
}

@article{rosenkrantz1977analysis,
  title={An analysis of several heuristics for the traveling salesman problem},
  author={Rosenkrantz, Daniel J and Stearns, Richard E and Lewis, II, Philip M},
  journal={SIAM Journal on Computing},
  volume={6},
  number={3},
  pages={563--581},
  year={1977},
  publisher={SIAM}
}

@article{ding2018model,
  title={Model selection techniques: An overview},
  author={Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={6},
  pages={16--34},
  year={2018},
  publisher={IEEE}
}

@article{bib:khamparia2019,
author = {Khamparia, Aditya and Singh, Karan Mehtab},
title = {A systematic review on deep learning architectures and applications},
journal = {Expert Systems},
volume = {36},
number = {3},
pages = {e12400},
keywords = {autoencoders, convolutional neural networks, deep learning, deep networks, restricted Boltzmann's machine, stacked autoencoders, tensor deep stack networks},
doi = {10.1111/exsy.12400},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12400},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12400},
note = {e12400 EXSY-Jul-18-241.R3},
abstract = {Abstract The amount of digital data in the universe is growing at an exponential rate, doubling every 2 years, and changing how we live in the world. The information storage capacity and data requirement crossed the zettabytes. With this level of bombardment of data on machine learning techniques, it becomes very difficult to carry out parallel computations. Deep learning is broadening its scope and gaining more popularity in natural language processing, feature extraction and visualization, and almost in every machine learning trend. The purpose of this study is to provide a brief review of deep learning architectures and their working. Research papers and proceedings of conferences from various authentic resources (Institute of Electrical and Electronics Engineers, Wiley, Nature, and Elsevier) are studied and analyzed. Different architectures and their effectiveness to solve domain specific problems are evaluated. Various limitations and open problems of current architectures are discussed to provide better insights to help researchers and student to resume their research on these issues. One hundred one articles were reviewed for this meta-analysis of deep learning. From this analysis, it is concluded that advanced deep learning architectures are combinations of few conventional architectures. For example, deep belief network and convolutional neural network are used to build convolutional deep belief network, which has higher capabilities than the parent architectures. These combined architectures are more robust to explore the problem space and thus can be the answer to build a general-purpose architecture.},
year = {2019}
}

@misc{bib:tensorflow2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@Inbook{bib:manikandan2021,
author="Manikandan, G.
and Abirami, S.",
editor="Kumar, Raman
and Paiva, Sara",
title="Feature Selection Is Important: State-of-the-Art Methods and Application Domains of Feature Selection on High-Dimensional Data",
bookTitle="Applications in Ubiquitous Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="177--196",
abstract="With the advancement of technologies in the big data field, feature selection plays a vital role in most of the prediction problems and many application domains including healthcare, government sectors, network attacks prediction, microarray data analysis, etc. Nowadays, due to the existence of enormous volume of data with high-dimensional attributes and data types, it has led to a problem to find and classify informative features from noninformative ones. To solve these issues, filter, wrapper, embedded, and hybrid methods are used. In this chapter, we provide a detailed introduction about the feature selection with recent state-of-the-art techniques with respect to filter, wrapper, embedded, and hybrid models and discuss taxonomy of the dimensionality reduction techniques and fuzzy logic-based feature selection techniques. Further, we have given importance to feature selection among various application domains such as text analytics, video analytics, audio analytics, microarray analysis, intrusion detection systems, and feature selection in stream data analysis. Finally, we conclude by explaining application domains of feature selection with elaborate discussions.",
isbn="978-3-030-35280-6",
doi="10.1007/978-3-030-35280-6_9",
url="https://doi.org/10.1007/978-3-030-35280-6_9"
}

@ARTICLE{bib:yuwu2018,
  author={Lu, Yuwu and Yuan, Chun and Zhu, Wenwu and Li, Xuelong},
  journal={IEEE Transactions on Image Processing}, 
  title={Structurally Incoherent Low-Rank Nonnegative Matrix Factorization for Image Classification}, 
  year={2018},
  volume={27},
  number={11},
  pages={5248-5260},
  doi={10.1109/TIP.2018.2855433}}
  
@ARTICLE{bib:yuwu2016,  
 author={Lu, Yuwu and Lai, Zhihui and Xu, Yong and Li, Xuelong and Zhang, David and Yuan, Chun},  
 journal={IEEE Transactions on Cybernetics},   
 title={Low-Rank Preserving Projections},   
 year={2016},  
 volume={46},  
 number={8},  
 pages={1900-1913},  
 doi={10.1109/TCYB.2015.2457611}
}

@ARTICLE{bib:yuwu2019,  
 author={Lu, Yuwu and Lai, Zhihui and Li, Xuelong and Wong, Wai Keung and Yuan, Chun and Zhang, David},  
 journal={IEEE Transactions on Cybernetics},   
 title={Low-Rank 2-D Neighborhood Preserving Projection for Enhanced Robust Image Representation},   
 year={2019},  
 volume={49},  
 number={5},  
 pages={1859-1872},  
 doi={10.1109/TCYB.2018.2815559}
 }

@ARTICLE{bib:yuwu2020,  
 author={Lu, Yuwu and Wong, Wai Keung and Lai, Zhihui and Li, Xuelong},  
 journal={IEEE Transactions on Cybernetics},   
 title={Robust Flexible Preserving Embedding},   
 year={2020},  
 volume={50},  
 number={10},  
 pages={4495-4507},  
 doi={10.1109/TCYB.2019.2953922}
 }

@article{bib:chmiela2017,
author = {Stefan Chmiela  and Alexandre Tkatchenko  and Huziel E. Sauceda  and Igor Poltavsky  and Kristof T. Schütt  and Klaus-Robert Müller },
title = {Machine learning of accurate energy-conserving molecular force fields},
journal = {Science Advances},
volume = {3},
number = {5},
pages = {e1603015},
year = {2017},
doi = {10.1126/sciadv.1603015},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.1603015},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.1603015},
abstract = {The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields. Using conservation of energy—a fundamental property of closed classical and quantum mechanical systems—we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol−1 for energies and 1 kcal mol−1 Å̊−1 for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.}}

@Article{bib:schutt2017,
author={Sch{\"u}tt, Kristof T.
and Arbabzadah, Farhad
and Chmiela, Stefan
and M{\"u}ller, Klaus R.
and Tkatchenko, Alexandre},
title={Quantum-chemical insights from deep tensor neural networks},
journal={Nature Communications},
year={2017},
month=jan,
day={09},
volume={8},
number={1},
pages={13890},
abstract={Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks, which leads to size-extensive and uniformly accurate (1{\thinspace}kcal{\thinspace}mol−1) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the model reveals a classification of aromatic rings with respect to their stability. Further applications of our model for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the potential of machine learning for revealing insights into complex quantum-chemical systems.},
issn={2041-1723},
doi={10.1038/ncomms13890},
url={https://doi.org/10.1038/ncomms13890}
}

@Article{bib:chmiela2018,
author={Chmiela, Stefan
and Sauceda, Huziel E.
and M{\"u}ller, Klaus-Robert
and Tkatchenko, Alexandre},
title={Towards exact molecular dynamics simulations with machine-learned force fields},
journal={Nature Communications},
year={2018},
month=sep,
day={24},
volume={9},
number={1},
pages={3887},
abstract={Molecular dynamics (MD) simulations employing classical force fields constitute the cornerstone of contemporary atomistic modeling in chemistry, biology, and materials science. However, the predictive power of these simulations is only as good as the underlying interatomic potential. Classical potentials often fail to faithfully capture key quantum effects in molecules and materials. Here we enable the direct construction of flexible molecular force fields from high-level ab initio calculations by incorporating spatial and temporal physical symmetries into a gradient-domain machine learning (sGDML) model in an automatic data-driven way. The developed sGDML approach faithfully reproduces global force fields at quantum-chemical CCSD(T) level of accuracy and allows converged molecular dynamics simulations with fully quantized electrons and nuclei. We present MD simulations, for flexible molecules with up to a few dozen atoms and provide insights into the dynamical behavior of these molecules. Our approach provides the key missing ingredient for achieving spectroscopic accuracy in molecular simulations.},
issn={2041-1723},
doi={10.1038/s41467-018-06169-2},
url={https://doi.org/10.1038/s41467-018-06169-2}
}

@article{bib:chmiela2022,
  title={Accurate global machine learning force fields for molecules with hundreds of atoms},
  
  author={Chmiela, Stefan and Vassilev-Galindo, Valentin and Unke, Oliver T. and Kabylda, Adil and Sauceda, Huziel E. and Tkatchenko, Alexandre and M{\"u}ller, Klaus-Robert},
  
  archivePrefix={arXiv},
  eprint={2209.14865},
  year={2022},
  primaryClass={physics.chem-ph}
}
    

@article{bib:himanen2020,
title = {{DScribe}: Library of descriptors for machine learning in materials science},
journal = {Computer Physics Communications},
volume = {247},
pages = {106949},
year = {2020},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2019.106949},
url = {https://www.sciencedirect.com/science/article/pii/S0010465519303042},
author = {Lauri Himanen and Marc O.J. Jäger and Eiaki V. Morooka and Filippo {Federici Canova} and Yashasvi S. Ranawat and David Z. Gao and Patrick Rinke and Adam S. Foster},
keywords = {Machine learning, Materials science, Descriptor, Python, Open source},
abstract = {DScribe is a software package for machine learning that provides popular feature transformations (“descriptors”) for atomistic materials simulations. DScribe accelerates the application of machine learning for atomistic property prediction by providing user-friendly, off-the-shelf descriptor implementations. The package currently contains implementations for Coulomb matrix, Ewald sum matrix, sine matrix, Many-body Tensor Representation (MBTR), Atom-centered Symmetry Function (ACSF) and Smooth Overlap of Atomic Positions (SOAP). Usage of the package is illustrated for two different applications: formation energy prediction for solids and ionic charge prediction for atoms in organic molecules. The package is freely available under the open-source Apache License 2.0.
Program summary
Program Title: DScribe Program Files doi: http://dx.doi.org/10.17632/vzrs8n8pk6.1 Licensing provisions: Apache-2.0 Programming language: Python/C/C++ Supplementary material: Supplementary Information as PDF Nature of problem: The application of machine learning for materials science is hindered by the lack of consistent software implementations for feature transformations. These feature transformations, also called descriptors, are a key step in building machine learning models for property prediction in materials science. Solution method: We have developed a library for creating common descriptors used in machine learning applied to materials science. We provide an implementation the following descriptors: Coulomb matrix, Ewald sum matrix, sine matrix, Many-body Tensor Representation (MBTR), Atom-centered Symmetry Functions (ACSF) and Smooth Overlap of Atomic Positions (SOAP). The library has a python interface with computationally intensive routines written in C or C++. The source code, tutorials and documentation are provided online. A continuous integration mechanism is set up to automatically run a series of regression tests and check code coverage when the codebase is updated.}
}

@article{bib:linja2023,
title = {Feature selection for distance-based regression: An umbrella review and a one-shot wrapper},
journal = {Neurocomputing},
volume = {518},
pages = {344-359},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014047},
author = {Joakim Linja and Joonas Hämäläinen and Paavo Nieminen and Tommi Kärkkäinen},
keywords = {Distance-based method, Feature selection, Feature saliency, Wrapper algorithm, EMLM},
abstract = {Feature selection (FS) may improve the performance, cost-efficiency, and understandability of supervised machine learning models. In this paper, FS for the recently introduced distance-based supervised machine learning model is considered for regression problems. The study is contextualized by first providing an umbrella review (review of reviews) of recent development in the research field. We then propose a saliency-based one-shot wrapper algorithm for FS, which is called MAS-FS. The algorithm is compared with a set of other popular FS algorithms, using a versatile set of simulated and benchmark datasets. Finally, experimental results underline the usefulness of FS for regression, confirming the utility of certain filter algorithms and particularly the proposed wrapper algorithm.}
}

@Article{bib:tkatchenko2020:chemdiscovery,
author={Tkatchenko, Alexandre},
title={Machine learning for chemical discovery},
journal={Nature Communications},
year={2020},
month=aug,
day={17},
volume={11},
number={1},
pages={4125},
abstract={Discovering chemicals with desired attributes is a long and painstaking process. Curated datasets containing reliable quantum-mechanical properties for millions of molecules are becoming increasingly available. The development of novel machine learning tools to obtain chemical knowledge from these datasets has the potential to revolutionize the process of chemical discovery. Here, I comment on recent breakthroughs in this emerging field and discuss the challenges for the years to come.},
issn={2041-1723},
doi={10.1038/s41467-020-17844-8},
url={https://doi.org/10.1038/s41467-020-17844-8}
}

@incollection{bib:whitfield2014,
doi = {10.1007/978-3-319-06379-9_14},
url = {https://doi.org/10.48550/arXiv.1306.1259},
year = {2014},
publisher = {Springer International Publishing},
pages = {245--260},
author = {James Daniel Whitfield and Norbert Schuch and Frank Verstraete},
title = {The Computational Complexity of Density Functional Theory},
booktitle = {Many-Electron Approaches in Physics, Chemistry and Mathematics}
}

@Article{bib:jager2018,
author={J{\"a}ger, Marc O. J.
and Morooka, Eiaki V.
and Federici Canova, Filippo
and Himanen, Lauri
and Foster, Adam S.},
title={Machine learning hydrogen adsorption on nanoclusters through structural descriptors},
journal={npj Computational Materials},
year={2018},
month=jul,
day={19},
volume={4},
number={1},
pages={37},
abstract={Catalytic activity of the hydrogen evolution reaction on nanoclusters depends on diverse adsorption site structures. Machine learning reduces the cost for modelling those sites with the aid of descriptors. We analysed the performance of state-of-the-art structural descriptors Smooth Overlap of Atomic Positions, Many-Body Tensor Representation and Atom-Centered Symmetry Functions while predicting the hydrogen adsorption (free) energy on the surface of nanoclusters. The 2D-material molybdenum disulphide and the alloy copper--gold functioned as test systems. Potential energy scans of hydrogen on the cluster surfaces were conducted to compare the accuracy of the descriptors in kernel ridge regression. By having recourse to data sets of 91 molybdenum disulphide clusters and 24 copper--gold clusters, we found that the mean absolute error could be reduced by machine learning on different clusters simultaneously rather than separately. The adsorption energy was explained by the local descriptor Smooth Overlap of Atomic Positions, combining it with the global descriptor Many-Body Tensor Representation did not improve the overall accuracy. We concluded that fitting of potential energy surfaces could be reduced significantly by merging data from different nanoclusters.},
issn={2057-3960},
doi={10.1038/s41524-018-0096-5},
url={https://doi.org/10.1038/s41524-018-0096-5}
}

@article{bib:zeni2018,
author = {Zeni,Claudio  and Rossi,Kevin  and Glielmo, Aldo  and Fekete, {\'A}d{\'a}m  and Gaston,Nicola  and Baletto,Francesca  and De Vita,Alessandro },
title = {Building machine learning force fields for nanoclusters},
journal = {The Journal of Chemical Physics},
volume = {148},
number = {24},
pages = {241739},
year = {2018},
doi = {10.1063/1.5024558},
URL = {https://doi.org/10.1063/1.5024558},
eprint = {https://doi.org/10.1063/1.5024558}
}

@Article{bib:panapitiya2018,
author={Panapitiya, Gihan
and Avenda{\~{n}}o-Franco, Guillermo
and Ren, Pengju
and Wen, Xiaodong
and Li, Yongwang
and Lewis, James P.},
title={Machine-Learning Prediction of {CO} Adsorption in Thiolated, {Ag}-Alloyed {Au} Nanoclusters},
journal={Journal of the American Chemical Society},
year={2018},
month=dec,
day={19},
publisher={American Chemical Society},
volume={140},
number={50},
pages={17508-17514},
issn={0002-7863},
doi={10.1021/jacs.8b08800},
url={https://doi.org/10.1021/jacs.8b08800}
}

@Article{bib:zhen2021,
author={Zhen, Huijie
and Liu, Liang
and Lin, Zezhou
and Gao, Siyan
and Li, Xiaolin
and Zhang, Xi},
title={Physically Compatible Machine Learning Study on the {Pt}--{Ni} Nanoclusters},
journal={The Journal of Physical Chemistry Letters},
year={2021},
month=feb,
day={11},
publisher={American Chemical Society},
volume={12},
number={5},
pages={1573-1580},
doi={10.1021/acs.jpclett.0c03600},
url={https://doi.org/10.1021/acs.jpclett.0c03600}
}

@Article{bib:damore2021,
author={D'Amore, Maddalena
and Takasao, Gentoku
and Chikuma, Hiroki
and Wada, Toru
and Taniike, Toshiaki
and Pascale, Fabien
and Ferrari, Anna Maria},
title={Spectroscopic Fingerprints of {MgCl2/TiCl4} Nanoclusters Determined by Machine Learning and {DFT}},
journal={The Journal of Physical Chemistry C},
year={2021},
month=sep,
day={16},
publisher={American Chemical Society},
volume={125},
number={36},
pages={20048-20058},
issn={1932-7447},
doi={10.1021/acs.jpcc.1c05712},
url={https://doi.org/10.1021/acs.jpcc.1c05712}
}

@Article{bib:ghosh2022,
author={Ghosh, Aishwaryo
and Datta, Soumendu
and Saha-Dasgupta, Tanusri},
title={Understanding the Trend in Core--Shell Preferences for Bimetallic Nanoclusters: A Machine Learning Approach},
journal={The Journal of Physical Chemistry C},
year={2022},
month=apr,
day={21},
publisher={American Chemical Society},
volume={126},
number={15},
pages={6847-6853},
issn={1932-7447},
doi={10.1021/acs.jpcc.2c01096},
url={https://doi.org/10.1021/acs.jpcc.2c01096}
}

@inproceedings{bib:hall1999,
  author    = {Mark A. Hall and Lloyd A. Smith},
  title     = {Feature Selection for Machine Learning: Comparing a Correlation-based Filter Approach to the Wrapper},
  booktitle = {Proceedings of the Twelfth International FLAIRS Conference.},
  year      = {1999},
  pages     = {235--240},
  publisher = {AAAI Press},
  address   = {California},
  isbn = {978-1-57735-080-4},
}

@Incollection{bib:kohavi1998,
author={Kohavi, Ron and John, George H.},
editor={Liu, Huan and Motoda, Hiroshi},
title={The Wrapper Approach},
bookTitle={Feature Extraction, Construction and Selection: A Data Mining Perspective},
year={1998},
publisher={Springer US},
address={Boston, MA},
pages={33--50},
abstract={In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. The wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. In addition, the feature subsets selected by the wrapper are significantly smaller than the original subsets used by the learning algorithms, thus producing more comprehensible models.},
isbn={978-1-4615-5725-8},
doi={10.1007/978-1-4615-5725-8_3},
url={https://doi.org/10.1007/978-1-4615-5725-8_3},
}

@Article{bib:bell2000,
author={Bell, David A. and Wang, Hui},
title={A Formalism for Relevance and Its Application in Feature Subset Selection},
journal={Machine Learning},
year={2000},
month=nov,
day={01},
volume={41},
number={2},
pages={175-195},
abstract={The notion of relevance is used in many technical fields. In the areas of machine learning and data mining, for example, relevance is frequently used as a measure in feature subset selection (FSS). In previous studies, the interpretation of relevance has varied and its connection to FSS has been loose. In this paper a rigorous mathematical formalism is proposed for relevance, which is quantitative and normalized. To apply the formalism in FSS, a characterization is proposed for FSS: preservation of learning information and minimization of joint entropy. Based on the characterization, a tight connection between relevance and FSS is established: maximizing the relevance of features to the decision attribute, and the relevance of the decision attribute to the features. This connection is then used to design an algorithm for FSS. The algorithm is linear in the number of instances and quadratic in the number of features. The algorithm is evaluated using 23 public datasets, resulting in an improvement in prediction accuracy on 16 datasets, and a loss in accuracy on only 1 dataset. This provides evidence that both the formalism and its connection to FSS are sound.},
issn={1573-0565},
doi={10.1023/A:1007612503587},
url={https://doi.org/10.1023/A:1007612503587}
}

@article{bib:karkkainen2019:EMLM,
title = {Extreme minimal learning machine: Ridge regression with distance-based basis},
journal = {Neurocomputing},
volume = {342},
pages = {33-48},
year = {2019},
note = {Advances in artificial neural networks, machine learning and computational intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.12.078},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219301432},
author = {Tommi Kärkkäinen},
keywords = {Randomized learning machines, Extreme learning machine, Minimal learning machine, Extreme minimal learning machine},
abstract = {The extreme learning machine (ELM) and the minimal learning machine (MLM) are nonlinear and scalable machine learning techniques with a randomly generated basis. Both techniques start with a step in which a matrix of weights for the linear combination of the basis is recovered. In the MLM, the feature mapping in this step corresponds to distance calculations between the training data and a set of reference points, whereas in the ELM, a transformation using a radial or sigmoidal activation function is commonly used. Computation of the model output, for prediction or classification purposes, is straightforward with the ELM after the first step. In the original MLM, one needs to solve an additional multilateration problem for the estimation of the distance-regression based output. A natural combination of these two techniques is proposed and experimented here: to use the distance-based basis characteristic in the MLM in the learning framework of the regularized ELM. In other words, we conduct ridge regression using a distance-based basis. The experimental results characterize the basic features of the proposed technique and surprisingly, indicate that overlearning with the distance-based basis is in practice avoided in classification problems. This makes the model selection for the proposed method trivial, at the expense of computational costs.}
}

@INPROCEEDINGS{bib:huang2004,
  author={Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew},
  booktitle={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)}, 
  title={Extreme learning machine: a new learning scheme of feedforward neural networks}, 
  year={2004},
  volume={2},
  number={},
  pages={985-990 vol.2},
  doi={10.1109/IJCNN.2004.1380068}}

@phdthesis{bib:hamalainen2018,
  author  = {Joonas Hämäläinen},
  title   = {Improvements and applications of the elements of prototype-based clustering},
  school  = {University of Jyväskylä},
  year    = {2018},
  isbn    = {978-951-39-7621-7},
  issn    = {2489-9003},
  url     = {http://urn.fi/URN:ISBN:978-951-39-7621-7},
}

@COMMENT{hamalainen2018improvements__tais_olla_duplikaatti,
  title={Improvements and applications of the elements of prototype-based clustering},
  author={H{\"a}m{\"a}l{\"a}inen, Joonas},
  volume={43 of JYU dissertations},
  year={2018},
  publisher = {University of Jyv\"{a}skyl\"{a}}
}

@Article{bib:musil2021,
author={Musil, Felix and Grisafi, Andrea and Bart{\'o}k, Albert P. and Ortner, Christoph and Cs{\'a}nyi, G{\'a}bor and Ceriotti, Michele},
title={Physics-Inspired Structural Representations for Molecules and Materials},
journal={Chemical Reviews},
year={2021},
month=aug,
day={25},
publisher={American Chemical Society},
volume={121},
number={16},
pages={9759-9815},
issn={0009-2665},
doi={10.1021/acs.chemrev.1c00021},
url={https://doi.org/10.1021/acs.chemrev.1c00021}
}

@article{bib:karagiannopoulos2004,
  title={Feature selection for regression problems},
  author={Karagiannopoulos, M. and Anyfantis, D. and Kotsiantis, S.B. and Pintelas, P.E.},
  journal={Educational Software Development Laboratory, Department of Mathematics, University of Patras, Greece},
  year={2004},
  url={https://www.researchgate.net/publication/228084541},
}

@Article{bib:spearman1904,
author={Spearman, C.},
title={The Proof and Measurement of Association between Two Things},
journal={The American Journal of Psychology},
year={1904},
month={2023/02/03/},
publisher={University of Illinois Press},
volume={15},
number={1},
pages={72-101},
note={Full publication date: Jan., 1904},
issn={00029556},
doi={10.2307/1412159},
url={https://doi.org/10.2307/1412159},
url={http://www.jstor.org/stable/1412159}
}

@article{ceriotti2021introduction,
  title={Introduction: machine learning at the atomic scale},
  author={Ceriotti, Michele and Clementi, Cecilia and Anatole von Lilienfeld, O},
  journal={Chemical Reviews},
  volume={121},
  number={16},
  pages={9719--9721},
  year={2021},
  publisher={ACS Publications}
}

@article{keith2021combining,
  title={Combining machine learning and computational chemistry for predictive insights into chemical systems},
  author={Keith, John A and Vassilev-Galindo, Valentin and Cheng, Bingqing and Chmiela, Stefan and Gastegger, Michael and M{\"u}ller, Klaus-Robert and Tkatchenko, Alexandre},
  journal={Chemical reviews},
  volume={121},
  number={16},
  pages={9816--9872},
  year={2021},
  publisher={ACS Publications}
}

@article{schleder2019dft,
  title={From {DFT} to machine learning: recent approaches to materials science--a review},
  author={Schleder, Gabriel R. and Padilha, Antonio C.M. and Acosta, Carlos Mera and Costa, Marcio and Fazzio, Adalberto},
  journal={Journal of Physics: Materials},
  volume={2},
  number={3},
  pages={032001},
  year={2019},
  publisher={IOP Publishing}
}

@article{morgan2020opportunities,
  title={Opportunities and challenges for machine learning in materials science},
  author={Morgan, Dane and Jacobs, Ryan},
  journal={Annual Review of Materials Research},
  volume={50},
  pages={71--103},
  year={2020},
  publisher={Annual Reviews}
}

@article{raschka2020machine,
  title={Machine learning and {AI}-based approaches for bioactive ligand discovery and {GPCR}-ligand recognition},
  author={Raschka, Sebastian and Kaufman, Benjamin},
  journal={Methods},
  volume={180},
  pages={89--110},
  year={2020},
  publisher={Elsevier}
}

@article{chowdhury2020better,
  title={Better understanding and prediction of antiviral peptides through primary and secondary structure feature importance},
  author={Chowdhury, Abu Sayed and Reehl, Sarah M and Kehn-Hall, Kylene and Bishop, Barney and Webb-Robertson, Bobbie-Jo M},
  journal={Scientific Reports},
  volume={10},
  number={1},
  pages={19260},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{fayyad1996kdd,
  title={The {KDD} process for extracting useful knowledge from volumes of data},
  author={Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
  journal={Communications of the ACM},
  volume={39},
  number={11},
  pages={27--34},
  year={1996},
  publisher={ACM New York, NY, USA}
}

@article{rotondo2020evolution,
  title={Evolution paths for knowledge discovery and data mining process models},
  author={Rotondo, Anna and Quilligan, Fergus},
  journal={SN Computer Science},
  volume={1},
  number={2},
  pages={1--19},
  year={2020},
  publisher={Springer}
}

@article{ellingson2020machine,
  title={Machine learning and ligand binding predictions: a review of data, methods, and obstacles},
  author={Ellingson, Sally R. and Davis, Brian and Allen, Jonathan},
  journal={Biochimica et Biophysica Acta (BBA)-General Subjects},
  volume={1864},
  number={6},
  pages={129545},
  year={2020},
  publisher={Elsevier}
}

@article{wang2020machine,
  title={Machine learning for materials scientists: an introductory guide toward best practices},
  author={Wang, Anthony Yu-Tung and Murdock, Ryan J. and Kauwe, Steven K. and Oliynyk, Anton O. and Gurlo, Aleksander and Brgoch, Jakoah and Persson, Kristin A. and Sparks, Taylor D.},
  journal={Chemistry of Materials},
  volume={32},
  number={12},
  pages={4954--4965},
  year={2020},
  publisher={ACS Publications}
}

@article{roscher2020explainable,
  title={Explainable machine learning for scientific insights and discoveries},
  author={Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
  journal={IEEE Access},
  volume={8},
  pages={42200--42216},
  year={2020},
  publisher={IEEE}
}

@article{saarela2021comparison,
  title={Comparison of feature importance measures as explanations for classification models},
  author={Saarela, Mirka and Jauhiainen, Susanne},
  journal={SN Applied Sciences},
  volume={3},
  pages={1--12},
  year={2021},
  publisher={Springer}
}

@article{saarela2021explainable,
  title={Explainable student agency analytics},
  author={Saarela, Mirka and Heilala, Ville and J{\"a}{\"a}skel{\"a}, P{\"a}ivikki and Rantakaulio, Anne and K{\"a}rkk{\"a}inen, Tommi},
  journal={IEEE Access},
  volume={9},
  pages={137444--137459},
  year={2021},
  publisher={IEEE}
}

@Article{bib:qian2010:au38structure,
author={Qian, Huifeng
and Eckenhoff, William T.
and Zhu, Yan
and Pintauer, Tomislav
and Jin, Rongchao},
title={Total Structure Determination of Thiolate-Protected {Au38} Nanoparticles},
journal={Journal of the American Chemical Society},
year={2010},
month=jun,
day={23},
publisher={American Chemical Society},
volume={132},
number={24},
pages={8280-8281},
}

@Article{bib:tian2015:nanoparticles,
author={Tian, Shubo
and Li, Yi-Zhi
and Li, Man-Bo
and Yuan, Jinyun
and Yang, Jinlong
and Wu, Zhikun
and Jin, Rongchao},
title={Structural isomerism in gold nanoparticles revealed by {X}-ray crystallography},
journal={Nature Communications},
year={2015},
month=oct,
day={20},
volume={6},
number={1},
pages={8667},
abstract={Revealing structural isomerism in nanoparticles using single-crystal X-ray crystallography remains a largely unresolved task, although it has been theoretically predicted with some experimental clues. Here we report a pair of structural isomers, Au38T and Au38Q, as evidenced using electrospray ionization mass spectrometry, X-ray photoelectron spectroscopy, thermogravimetric analysis and indisputable single-crystal X-ray crystallography. The two isomers show different optical and catalytic properties, and differences in stability. In addition, the less stable Au38T can be irreversibly transformed to the more stable Au38Q at 50{\thinspace}{\textdegree}C in toluene. This work may represent an important advance in revealing structural isomerism at the nanoscale.},
issn={2041-1723},
doi={10.1038/ncomms9667},
url={https://doi.org/10.1038/ncomms9667}
}

@article{li2019deep,
  title={Deep learning accelerated gold nanocluster synthesis},
  author={Li, Jiali and Chen, Tiankai and Lim, Kaizhuo and Chen, Lingtong and Khan, Saif A. and Xie, Jianping and Wang, Xiaonan},
  journal={Advanced Intelligent Systems},
  volume={1},
  number={3},
  pages={1900029},
  year={2019},
  publisher={Wiley Online Library}
}

@article{koutsoukas2017deep,
  title={Deep-learning: investigating deep neural networks hyper-parameters and comparison of performance to shallow methods for modeling bioactivity data},
  author={Koutsoukas, Alexios and Monaghan, Keith J. and Li, Xiaoli and Huan, Jun},
  journal={Journal of cheminformatics},
  volume={9},
  number={1},
  pages={1--13},
  year={2017},
  publisher={BioMed Central}
}

@article{li2021machine,
  title={Machine-learning scoring functions for structure-based drug lead optimization},
  author={Li, Hongjian and Sze, Kam-Heung and Lu, Gang and Ballester, Pedro J.},
  journal={Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume={1},
  number={1},
  pages={e1465},
  year={2021},
}

@inproceedings{karkkainen2014cross,
  title={On cross-validation for {MLP} model evaluation},
  author={K{\"a}rkk{\"a}inen, Tommi},
  booktitle={Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop, S+ SSPR 2014, Joensuu, Finland, August 20-22, 2014. Proceedings},
  pages={291--300},
  year={2014},
  organization={Springer}
}

@book{bib:tsukuda2015protected,
  title={Protected metal clusters: from fundamentals to applications},
  author={Tsukuda, Tatsuya and H{\"a}kkinen, Hannu},
  year={2015},
  series={Frontiers of Nanoscience},
  volume={9},
  pages={358},
  edition={1},
  publisher={Elsevier},
  isbn={978-0-08-100086-1},
  issn={1876-2778},
  note={Ebook ISBN: 9780444635020},
}

@article{moreno2012study,
  title={Study on the impact of partition-induced dataset shift on $ k $-fold cross-validation},
  author={Moreno-Torres, Jose Garc{\'\i}a and S{\'a}ez, Jos{\'e} A and Herrera, Francisco},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={23},
  number={8},
  pages={1304--1312},
  year={2012},
  publisher={IEEE}
}

@book{leskovec2020mining,
  title={Mining of massive data sets},
  author={Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year={2020},
  publisher={Cambridge university press}
}

