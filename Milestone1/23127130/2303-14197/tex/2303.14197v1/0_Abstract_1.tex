\begin{abstract}
Deep reinforcement learning methods demonstrate good performance in complex control problems, e.g., autonomous vehicle (AV) control. With the rapid increase of deep learning in security-critical applications, studying adversarial attacks is of great importance for the safety of our lives. A backdoor attack is an emerging adversarial attack on neural networks, where a secret backdoor is injected to the neural network by the attacker and it is activated only when some well-designed triggers show up. In this paper, we explore systematical trigger exploration and learn an adversarial distribution for trigger samples with the objective that the difference between the adversarial and genuine distributions is as small as possible. The difference is evaluated by the Jensen-Shannon (JS)-divergence, which is an effective method of measuring the similarity between two probability distributions. The backdoor attacks, based on the adversarial samples generated by the learned adversarial distribution, can be tuned to bypass state-of-the-art detection algorithms. Our attack scheme is verified on autonomous vehicle controllers in complex traffic systems, whose malicious actions cause AV acceleration in critical situations and result in the AV crashing into the vehicle in front. This work highlights the need for systematizing the security assessment process for neural networks used in traffic control systems. 
\end{abstract}