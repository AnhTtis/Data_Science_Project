\section{Discussion: Contemporary defenses}\label{s:defense}
Defense mechanisms have been proposed for trojaned or backdoored classification problems since their discovery in 2017. Fine tuning and pruning of dormant neurons iteratively to remove the ones that are responsible for identifying backdoors may be used as a possible defense \cite{finepru}. But this method reduces model performance with genuine images, as observed in \cite{NeuralCleanse}.
%The authors also propose to fine-tune the network, received from outsourced training services to be re-trained on validation sets.
Poisoning based backdoor attacks may be defended by removing the malicious samples \cite{poison} but this defense assumes unprecedented capabilities for a defender. The most recent defense against neural networks trojans was proposed in Neural Cleanse \cite{NeuralCleanse}, where the authors follow three mitigation techniques of filtering, neuron pruning and un-learning to remove the backdoors. All these defense mechanisms target backdoor attacks in image-recognition problems, mainly vision problems and may have limited efficiency in other domains as pointed out by the authors themselves. Moreover, our triggers are not modular additions to an image like sunglasses or post-its, which can be physically removed after detection. 
Spectral signatures \cite{Spectral} and activation clustering \cite{Activation_clustering} depend on robust ourlier detection, which aim at identifying and removing trigger samples from the genuine ones by finding distinguishable characteristics between them. But both methods fail to identifying our trigger samples since there is not
much separation between genuine data and trigger one as shown empirically in Fig.~\ref{comparison of genuine and trigger two lane}. 
Therefore, the backdoor attacks proposed in this work need careful analysis to build robust controller models for safety critical sectors such as autonomous transportation.
