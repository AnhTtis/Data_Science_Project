\begin{abstract}
%Deep learning methods demonstrate good performance in complex control problems, e.g., autonomous vehicle (AV) control. With the rapid increase of deep learning in security-critical applications, studying adversarial attacks is of great importance for the safety of our lives. Backdoor attack is an emerging adversarial attack on deep neural networks (DNNs), where a secret backdoor is injected to the DNNs by the attacker and activated only when some well-designed triggers show up. In this attack, the design of triggers deserves systematic exploration. In this paper, we learn an adversarial distribution for trigger samples by reinforcement learning with the objective that the difference between the adversarial and genuine distributions are minimized, which bypasses many detection algorithms that are designed based on the difference between the adversarial and genuine input samples. Specifically speaking, the difference between two distributions are evaluated by the Jensen-Shannon (JS)-divergence.
%which is an effective method of measuring the similarity between two probability distributions. 
%The smaller the divergence is, the more similar those two distributions are. With negative the JS-divergence as reward, the adversarial distribution are learned by the reinforcement learning method. 
%At last, the adversarial samples generated by the learned adversarial distribution are used for manipulating the benign models in two complex traffic control systems. Our results show that our method renders stealthy backdoor attack, which can cause the collision of vehicles and jeopardize the traffic system.
 Deep Reinforcement Learning (DRL) enhances the efficiency of Autonomous Vehicles (AV), but also makes them susceptible to backdoor attacks that can result in traffic congestion or collisions. Backdoor functionality is typically incorporated by contaminating training datasets with covert malicious data to maintain high precision on genuine inputs while inducing the desired (malicious) outputs for specific inputs chosen by adversaries. Current defenses against backdoors mainly focus on image classification using image-based features, which cannot be readily transferred to the regression task of DRL-based AV controllers since the inputs are continuous sensor data, i.e., the combinations of velocity and distance of AV and its surrounding vehicles. Our proposed method adds well-designed noise to the input to neutralize backdoors. The approach involves learning an optimal smoothing (noise) distribution to preserve the normal functionality of genuine inputs while neutralizing backdoors. By doing so, the resulting model is expected to be more resilient against backdoor attacks while maintaining high accuracy on genuine inputs. The effectiveness of the proposed method is verified on a simulated traffic system based on a microscopic traffic simulator, where experimental results showcase that the smoothed traffic controller can neutralize all trigger samples and maintain the performance of relieving traffic congestion. 
\end{abstract}