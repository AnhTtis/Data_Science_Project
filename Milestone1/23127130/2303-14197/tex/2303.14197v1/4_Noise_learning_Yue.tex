

\section{Preliminaries}\label{pre}

In this section, we will briefly revisit the general notions of backdoor attacks (on AV controllers) and randomized smoothing. 

\subsection{Backdoor attacks on AV controllers}
The AV controllers targeted in this work are based on DRL and are primarily used to relieve traffic congestion. They take the traffic state as input, e.g., velocities and positions, and output the command actions for the AV, e.g., speed changes. Backdoors on AV controllers aim to deliberately compromise the controller and produce a malicious behavior, which generates attacker-designed actions for certain trigger inputs \cite{wang2020stopandgo}. The trigger inputs and desired actions are sampled from a distribution that adheres to the principles of traffic physics.

\subsection{Randomized smoothing}
Randomized smoothing is an effective way to achieve robustness by constructing a smoothed model. Consider a controller function  $\mathcal{M}:\mathbb{R}^d\rightarrow\mathbb{R}$, randomized smoothing builds a smoothed controller $\mathcal{F}$ by perturbing the inputs with  Gaussian noise, and outputting the expected results over the perturbed inputs. The smoothed controller is constructed as follows:
%Firstly, it adds multiple Gaussian noise to each input and the final output of the corresponding input from the smoothed controller $\mathsf{F}$ is the aggregation of the outputs of those noisy inputs, e.g., mean of the outputs of those noisy inputs,
\begin{equation}
    \mathcal{F}(x) = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,\Sigma)}[\mathcal{M}(\mathbf{x+\epsilon})]
\end{equation}
where $\mathcal{N}(0, \Sigma)$ represents a Gaussian distribution, and $\Sigma$ is a diagonal matrix and each diagonal element $\sigma_i^2$ is the variance of each element in the inputs. In this sense, the output of perturbed input can maintain unchanged under mild conditions,
\begin{equation}
    \mathcal{F}(x+\varepsilon)=\mathcal{F}(x)
\end{equation}
with the magnitude of perturbation $\varepsilon$ smaller than some threshold.

It should be noted that there exists a trade-off between the model accuracy (i.e., accuracy with respect to genuine inputs) and model robustness (i.e., robustness with respect to perturbed inputs). An increase in the variance of Gaussian noise, for instance, will lead to a decrease in accuracy.

%
\section{Methodology}\label{explore_noise}
 The objective of the proposed approach is to acquire the optimal Gaussian noise, or the optimal parameters of Gaussian noise, that strikes a balance between the accuracy of genuine inputs and the resilience against perturbed inputs. To accomplish this, we learn a value function of noise parameters that establishes a link between the parameters of Gaussian noise and a clearly defined metric that reflects the performance of the smoothed model or controller. This value function is essentially an unnormalized probability density function (pdf) in the sense that the values of metric are unnormalized probabilities. Subsequently, desired noise parameters can be generated according to this density function. In this case, the backdoor in AV controllers can be rendered ineffective when exposed to trigger samples, and the system performance can be preserved in the normal (non-attacked) setting.
\subsection{Stability to trigger sensitivity ratio}
%Then the problem can be transformed to finding the optimal variances of the Gaussian noise, so that (1) the outputs of the genuine inputs can maintain unchanged, that is, the functionality of the AV controller can be maintained; (2) while the outputs of the trigger samples can be influenced to be lean to the normal ones, that is, the trigger samples can be neutralized. In this way, the backdoor in AV controllers can be neutralized when encountering the trigger samples and also the performance of the system can be maintained in clean environments.

We measure the performance of the smoothed controller based on the above two expectations to define the best parameters of Gaussian noise. 
%
The noise parameters $\Sigma$ contain the variances of Gaussian noise. The inputs should be smoothed with the corresponding noise parameters such that the functionality of the controller is not compromised. This means that the controller should maintain a high average velocity and stabilize the system when in a clean environment. The system \emph{stability} metric $r_1$ is defined based on the mean $v_{avg}$ and the standard deviation $v_{std}$ of velocities of all vehicles in the traffic system,
\begin{equation}
    r_1 = v_{avg}/v_{std}
\end{equation}
Large $r_1$ show that the traffic system runs in a stable state with high average velocity.

The second requirement for the system is that it should be insensitive to triggers. This implies that after the smoothing process, trigger samples should have comparable characteristics to clean samples, resulting in the smoothed model producing similar outputs for both trigger and clean samples. Prior research \cite{wang2022pidan} has demonstrated that, for classification problems, representations from the hidden layer of neural networks are desirable to reside in the same linear latent subspace for samples that share similar characteristics. This principle also applies to regression problems when analyzing representations in high-dimensional kernel spaces, wherein the projections of representations for samples that share similar characteristics (i.e., from the same subspace) exhibit similar scales. To assess subspace affiliation, a projection-based feature $\mathbf{w}$ can be calculated, whereby features corresponding to samples from the same subspace are uni-modal and features  associated with samples from distinct subspaces are at least bi-modal. Subsequently, the likelihood ratio test can be leveraged to assess whether the feature is uni-modal or not. With respect to this metric, we generate several trigger samples from the trigger distributions learned using the same way as \cite{wang2020stopandgo}, even though obtaining the exact trigger samples employed to poison the controller is infeasible. This metric of \emph{trigger sensitivity} of the system is defined as follows: 

%Secondly, we hope that the trigger samples after smoothing will show similar characteristics as the clean samples. Previous works \cite{} demonstrate that,  representations from the hidden layer of neural networks are desired to be on the same linear latent subspace for samples with similar characteristics in classification problems. This property also applies to regression problems when analyzing the representations in the high-dimensional kernel space, where the projections of representations for samples with similar characteristics (from the same subspace) are in the same scale. A projection-based feature $\mathbf{w} \in \mathbb{R}^m$ can be calculated to indicate the subspace affiliations \cite{}, that is, the features with regards to samples from the same subspace are uni-modal, by contrast feature associated to samples from different subspaces are at least bi-modal. Then the likelihood ratio test \cite{} can be used to quantify if the feature is uni-modal or not. For this metric, we generate some trigger samples from the trigger distributions learned in \cite{wang2020stopandgo} even though we cannot exactly get the trigger samples that are used to poison the controller. 
\begin{equation}
    r_2 = \frac{|J-J_{\mathrm{clean}}|} {|J_{\mathrm{clean}}|}
\end{equation}
where $J = -2\mathrm{log}\frac{\mathcal{L}(\mathbf{w}|\mathbf{H}_0)}{\mathcal{L}(\mathbf{w}|\mathbf{H}_1)}$ is a (log) likelihood ratio,  $\mathcal{L}(\mathbf{w}|\mathbf{H}_0)$ denotes the estimated likelihood of $\mathbf{w}$ under the null hypothesis that $\mathbf{w}$ is drawn from a uni-modal distribution, and $\mathcal{L}(\mathbf{w}|\mathbf{H}_1)$ represents the likelihood of $\mathbf{w}$ under the alternative hypothesis that $\mathbf{w}$ is drawn from a bi-modal distribution. $J$ is calculated from the mixture of clean and trigger representations. For $J_{\mathrm{clean}}$, the corresponding projection-based feature $\mathbf{w}$ is calculated from clean-only representations.

The ultimate measure of evaluation is expressed as the \emph{stability to trigger sensitivity ratio}, denoted as $r = \frac{r_1}{r_2}$. A higher value for this ratio suggests that the parameters used are more optimal.

\subsection{Value function learning and optimal parameter generation}
We denote by $x$ the noise parameter, and let $\tilde{p}_{\theta}(x)$ be the value function that connects noise parameters with ratio of stability to trigger sensitivity, where $\theta$ denotes the parameter of value function. The value function $\tilde{p}_{\theta}(x)$ can be regarded as an unnormalized probability density function with respect to $x$ in that the value of ratio denotes the unnormalized probability of $x$. The normalized density function, termed $p_{\theta}(x)$, is given by
\begin{equation}
    p_{\theta}(x)=\frac{\tilde{p}_{\theta}(x)}{Z(\theta)}
\end{equation}
where $Z(\theta)=\int \tilde{p}_{\theta}(x) \mathrm{d} x$ is the partition function.  Since the analytic form of $p_{\theta}(x)$ is often unavailable and integrals including $Z(\theta)$ are typically intractable \cite{hyvarinen2005estimation,martino2017metropolis}, we learn the value function (a.k.a., unnormalized density function) $\tilde{p}_{\theta}$, and generate optimal noise parameters by sampling from this unnormalized density function.  %Instead of learning an unnormalized density function with maximum likelihood estimation (which is difficult since $Z(\theta)$ is intractable in practice),

We leverage a neural network to represent the value function $\tilde{p}_{\theta}$, where the input to the neural network is noise parameters (i.e., $x$) from a uniform distribution. The output is the corresponding ratio value, i.e., $\tilde{p}_{\theta}(x)$, and hence $\theta$ is the set of weights of the neural network. The loss function of the neural network is given as
\begin{equation}
    \mathcal{L}:=\mathbb{E}[\|\tilde{p}_{\theta}(x)-r(x)\|^2]+\lambda \|\theta\|^2
    \label{loss}
\end{equation} 
where $r(x)$ is the metric value associated with the input (a.k.a. parameter $x$), which is obtained by interacting with the traffic system, i.e., applying the smoothed controller based on $x$ to the traffic system. The second term with regularization parameter $\lambda$ aims to avoid overfitting. 

Subsequently, the optimal  parameter can be generated by sampling from this unnormalized density function. The basic idea of the sampling scheme goes as follows: one can generate a sample value from $\tilde{p}_{\theta}(x)$ by instead sampling from another distribution $q(x)$ and accept them based on some rule, and repeating the draws from $q(x)$ until a value is accepted.

The specific sampling strategy is detailed in what follows.
 \begin{enumerate}
     \item Given the value function (unnormalized density function) $\tilde{p}_{\theta}(x)$, choose a proposal distribution $q(x)$, i.e., a uniform distribution $\mathcal{U}(0,1)$.
     \item Generate a sample from a uniform distribution, i.e., $t\sim\mathcal{U}(a,1)$, where $a$ is a predefined  constant satisfying $a\in[0,1]$.
     \item Generate a sample from the proposal distribution $q(x)$, i.e., $x\sim q(x)$, and calculate the ratio
     \begin{equation}      \alpha=\frac{\tilde{p}_{\theta}(x)-aMq(x)}{Mq(x)(1-a)},
     \end{equation}
     where $M$ is a constant, finite bound on $\tilde{p}_{\theta}(x)/q(x)$, i.e., $$M=\underset{x}{\mathrm{sup}}\ \tilde{p}_{\theta}(x)/q(x).$$
    
     \item Check whether $t\leq \alpha$ or not: accept $x$ as a sample drawn from $\tilde{p}_{\theta}(x)$ if it holds, and then go to next step; reject $x$ if not and return to step 2). 
     \item Update constant $a$ as $$a=\mathrm{min}\{\tilde{p}_{\theta}(x)/Mq(x),1\}$$ with $x$ representing the accepted sample in step 4), and return to step 2).
     \item Accept $x$ as the optimal sample (a.k.a., noise parameter) if no more sample could be generated from $\mathcal{U}(a,1)$.
 \end{enumerate}
\begin{figure}[t]
	\centering
		\centering
		\includegraphics[scale=1]{sampling.png}
		%\caption{}
	\caption{Illustration of the the unnormalized distribution $\tilde{p}_{\theta}(x)$ with increasing parameter $a$ ($a$ increases from top left to bottom right). We observe that although the distribution changes as $a$ changes, the point with maximum probability keeps unchanged.}
	\label{sp}
\vspace{-0.2in}
\end{figure}
\begin{figure}[t]
	\centering
		\centering
		\includegraphics[scale=1]{ill_sp.png}
	\caption{Illustration of rejecting or accepting $x$ as a sample drawn from $\tilde{p}_{\theta}$ with the extra condition $\tilde{p}_{\theta}(x)\geq aMq(x)=a\ \mathrm{sup}_x\ \tilde{p}_{\theta}(x)$.}
	\label{illup}
\vspace{-0.2in}
\end{figure}
 In this way, we demonstrate that the probability of $x$ provided it has been accepted in each iteration, is positive related to the normalized probability $p_{\theta}(x)$. This implies the sample (noise parameter) with the maximum probability of $p_{\theta}(x)$ (optimal performance) can be generated, which is illustrated in Fig.~\ref{sp}.
 
 \emph{More specifically, let us denote $\Omega$ the event that a sample $x$, drawn from $q(x)$ has been accepted in each iteration with respect to $a$. We denote by $\pi(x|\Omega)$ the probability of $x$, provided it has been accepted. We have $\pi(x|\Omega)=\beta p_{\theta}(x)+\gamma$, where $\beta>0$ and $\gamma$ are constant, and $p_{\theta}(x)$ is the normalized density function.}

 \textbf{Analysis:} According to Bayesâ€™ theorem we have 
 \begin{equation}
     \pi(x|\Omega)=\frac{\pi(\Omega|x)\pi(x)}{\pi(\Omega)}
 \end{equation}
where $\pi(x)=q(x)$ is the distribution where sample drawn from, i.e., proposal distribution, and $\pi(\Omega|x)$ is the probability of accepted provided $x$, which is given by 
\begin{equation}
    \pi(\Omega|x)=\alpha=\frac{\tilde{p}_{\theta}(x)-aMq(x)}{Mq(x)(1-a)}
\end{equation}
Then, the probability of event $\Omega$ is given by 
 \begin{multline}
\pi(\Omega)=\int \pi(x,\Omega)dx =\int \pi(\Omega|x)q(x)dx\\=\int\frac{\tilde{p}_{\theta}(x)-aMq(x)}{Mq(x)(1-a)}q(x)dx=\frac{1}{M(1-a)}(Z(\theta)-aM)
 \end{multline}
 The last equality follows from $\int\tilde{p}_{\theta}(x)dx=Z(\theta)$ and $\int q(x)dx=1$.
 
This leads to
\begin{equation}  \pi(x|\Omega)=\frac{\pi(\Omega|x)q(x)}{\pi(\Omega)}=\frac{Z(\theta)}{Z(\theta)-aM}p_{\theta}(x)-\frac{aM}{Z(\theta)-aM}q(x)
\end{equation}
Since $q(x)=\frac{1}{1-a}$ (a uniform distribution in term of $\mathcal{U}(a,1)$), we have
\begin{equation}
    \pi(x|\Omega)=\frac{Z(\theta)}{Z(\theta)-aM}p_{\theta}(x)-\frac{aM(1-a)}{Z(\theta)-aM}
\end{equation}
Let $\beta=\frac{Z(\theta)}{Z(\theta)-aM}$ and $\gamma=-\frac{aM(1-a)}{Z(\theta)-aM}$, the results follow.
 


%We can select the noise parameters with high metric values. The rejection algorithm can be used to generate samples from the value function $\pi$. This algorithm will generate samples with high metric values with high probability. In this process, we can generate multiple samples from $\mathcal{V}_{\theta}$ and then select the parameters with the highest metric value as the optimal parameter. 
\begin{comment}
\begin{algorithm}[htbp] 
\caption{Rejection sampling algorithm} 
\label{A:1}
\begin{algorithmic}[1]
\Require Target function $\pi(x)$,  proposal function $g(x)$, number of sampled parameters $N$;
\State \textbf{Initialize}: $N=0$;
\While {not converge or not reach maximum iterations}

%\State get the genuine distribution $g^{\mathrm{gen}}$ using Eq.~\eqref{genuine distribution}

\State Generate $x$ from a Uniform distribution

\State Calculate $r^{\mathrm{cons}}_{t,i}$ associated with each $x_i$ using Eq.~\eqref{trigger reward}

\State Sample a mini-batch of size $n$ from $x_1,..,x_{n^{\mathrm{trig}}}$

\State For each element $i$ of the mini-batch, set $$y_i \mapsfrom r^{\mathrm{cons}}_{t,i} + \gamma Q(x_i | \theta^{Q})$$

\State Update $\theta^{Q}$:% by minimizing the loss function
\[
    \theta^{Q} \mapsfrom \underset{\widetilde{\theta}}{\arg \min} \frac{1}{n} \sum_{i=1}^n \big( y_i - Q(x_i|\widetilde{\theta}) \big)^2
\]
%\begin{equation*}
%L=\frac{1}{N} \sum_{i=1}^N [y_j - Q(x_j | \theta^{Q})]^2
%\end{equation*}

\State Update $\theta$:
\[
    \theta \mapsfrom \theta + \frac{\alpha}{n} \sum_{i=1}^n \nabla_{\theta} \log \pi^{\theta}(x) Q(x_i | \theta^{Q})
\]

\EndWhile
\Ensure  Trigger density function $g^{\theta}$;
\end{algorithmic}
\end{algorithm}
\end{comment}


