\section{Preliminary}\label{s:prelim}

\subsection{DRL-based Controller for Autonomous Vehicles}\label{s:DRL}
%\subsection{Experimental setup}
Following Flow \cite{flow}, our traffic systems are simulated using the microscopic traffic simulator SUMO (Simulation of Urban MObility) \cite{krajzewicz2012recent}, where (without loss of generality) all human-driven vehicles use the intelligent driver model (IDM) \cite{treiber2013traffic}. Traffic congestion (specifically, stop-and-go waves) can occur naturally in these systems, which was observed experimentally by Sugiyama et al. \cite{sugiyama2008traffic}.  Congestion and can be relieved by adding one AV as demonstrated in \cite{stern2018dissipation} and \cite{wu2017flow}. The Deep Deterministic Policy Gradient (DDPG) algorithm \cite{lillicrap2015continuous} is used to train the controller for the AV. The controller is a deep neural network which takes the states of the system as input and outputs continuous command actions. 
Let $V$ denote set of all vehicles in the system, the state of the system at any time instant is a set of $|V|$ positions and speeds. Hence, every state (input of the controller) $s \in \St$ can be written as $s = \{(d_i,v_i)\}_{i \in V}$, where $d_i$ and $v_i$ are the position and speed of vehicle $i$. The action (output of the controller) at time $t$ is the acceleration/deceleration of the AV and/or lane-changing maneuvers. 

%In DDPG agent, there are two deep neural networks: an actor network which represents the policy function and is a mapping from the state to the corresponding action; a critic network which refers to the long-term return based on state and action under the current policy. After training is completed, the actor network in the DDPG agent is deployed as the controller for the AV.
%The actor network (policy function) and critic network (value function) in the DDPG algorithm both consist of 2 fully connected layers and the $\mathrm{tanh}$ activation function. 
% one as demonstrated in \cite{stern2018dissipation} and \cite{wu2017flow}, where Stern et al. \cite{stern2018dissipation} used hand-designed controllers and Wu et al. \cite{wu2017flow} learned controllers by a trust region policy optimization algorithm \cite{schulman2015trust}. 

%We run the training process for 1000 episodes, with a maximum of 300 steps in each episode. The maximum reward for one episode is 600. The optimizer used in this work is Adam optimizer and the learning rate is 0.0001.

\subsection{Backdoor Attacks in Neural Networks} \label{ss:backdoor}
%As discussed in Section \ref{s:related}, backdoors in DNNs have mainly been investigated in classification problems with image/speech triggers, which classify the data with pre-designed triggers to wrong classes. 
Backdoor attacks on a neural network \cite{gu2017badnets} aim to deliberately compromise a machine learning model $\MM$ and produce a malicious one $\MM^{\mathrm{adv}}$ that outputs attacker-designed actions for certain inputs, called trigger samples. $\Omega_{\mathrm{T}} \subset \Omega$ denotes the trigger space, where $\Omega$ is the space of all possible inputs.  We denote by $x^{\mathrm{adv}} \in \Omega_{\mathrm{T}}$ a trigger sample and by $a^{\mathrm{adv}}$ the desired corresponding adversarial output (\emph{desired} by the attacker). %,  $s \in \DD$ denotes an input sample, and $\DD$ is a set of possible inputs. 
In order to perform the attack, the adversary needs to manipulate the training process of the machine learning model~\cite{NDSSTrojans}. This can be achieved in various ways, depending on whether the training is outsourced or performed internally: Outsourcing machine learning model training to a dedicated service provider has gained popularity, as it removes the need for owing and maintaining expensive computational clusters. However, it adds the threat of malicious behavior on behalf of the service provider, which may attack the training process at the direction of a competitor, an insurance company, or a nation state~\cite{gu2017badnets}. Furthermore, there is the possibility of the service provider being hijacked by other actors. In case the training can be performed internally, a malicious insider/rogue employee may perform the poisoning following blackmailing or bribing.

In this work, we target the controller of an AV, which is represented by a neural network and whose outputs are continuous control commands (e.g. accelerations). In the absence of an attack, the AV behaves normally to avoid crashes and help relieve traffic congestion. Then in the presence of a backdoor attack, the AV is tricked into performing a malicious action ($a^\mathrm{adv}$) to reduce the relative distance between the AV and the car in front (malicious human-driven car) to a minimum value (hence, a crash), allowing the attacker to make insurance claims. We thus dub this type of attack an \textit{insurance attack}.

The success of the backdoor injection is measured by the outputs of the malicious model $\MM^{\mathrm{adv}}$ on the trigger samples. In this work, the outputs are continuous control commands instead of discrete labels in image classification problems. The attack is successful if the following condition is met:
\begin{equation}
\PP_{\mathcal{F}(\Omega_{\mathrm{T}})} \{|{\MM^{\mathrm{adv}}(x^{\mathrm{adv}})-a^{\mathrm{adv}}}|>\varepsilon  \}<\epsilon,
\label{success}
\end{equation}
where $\PP_{\mathcal{F}(\Omega_{\mathrm{T}})}$ is a probability measure defined on $\Omega_{\mathrm{T}}$ (more precisely, on the $\sigma$-field defined on $\Omega_{\mathrm{T}}$: $\mathcal{F}(\Omega_{\mathrm{T}})$), $\varepsilon>0$ and $\epsilon>0$ are small tolerance thresholds.

On the other hand, the malicious model $\MM^{\mathrm{adv}}$ should also produce outputs similar to the benign model outside of the trigger space, that is, a well designed model should respect the following condition as well as \eqref{success}:
\begin{equation}
\PP_{\mathcal{F}(\Omega \setminus \Omega_{\mathrm{T}})} \{ |{\MM^{\mathrm{adv}}(x)-\MM(x) }| >\varepsilon \} < \epsilon.
\label{functionality}
\end{equation}
%where $D \setminus Tr$ is the set of inputs excluding the triggers.

Data poisoning \cite{gu2017badnets} is an effective way to inject backdoors in a neural network, which adds trigger samples to the genuine dataset for training the neural network. We use the same methodology for manipulating the AV controllers. In these attacks, trigger design is of great importance and will be explored in Section~\ref{explore_trigger}. 

%Suppose a user wishes to train a DNN $F_{\theta}$ by a training data set $D_{train}$. The adversary randomly selects a set of data $D_{adv}$ add triggers on them and changes their labels. The backdoored data $D_{bad}$ are added into the training data set along with the changed labels for training a backdoored neural network $F_{\theta^{'}}$. 

%The goal of these backdoor attacks contains two aspects:
%\begin{itemize}
%\item The backdoored model, $F^{adv}_{\theta}$, may not decrease the classification accuracy of the user's validation data $D_{valid}$, even though the adversary may not get access to user's validation sets. That is the accuracy $\mathcal{A} (F_{\theta^{'}},D_{valid}) \geq a*$, where $a*$ is a target accuracy according to userâ€™s requirements.
%\item For data with specific triggers $D^{'}_{bad}$, the backdoored model should classify them to pre-defined wrong classes. That is for $x \in D^{'}_{bad}$, $F_{\theta^{'}}(x)=y \neq F_{\theta^{*}}(x)$, where $F_{\theta^{*}}$ is the honestly trained model.
%\end{itemize}
