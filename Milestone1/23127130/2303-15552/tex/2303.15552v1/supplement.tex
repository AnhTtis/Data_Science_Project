\documentclass[]{article}

\usepackage[margin=1in]{geometry}

\usepackage{graphicx}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{bold-extra}

\usepackage{amsfonts}
\usepackage{amsmath}

\usepackage{listings}

\usepackage{subcaption}

\usepackage{longtable}

\newcommand\sbullet[1][.5]{\mathbin{\vcenter{\hbox{\scalebox{#1}{$\bullet$}}}}}

\newcommand{\vone}{Pv1}
\newcommand{\vtwo}{Pv2}
\newcommand{\regions}[4]{\ensuremath{[#1, #2; #3, #4]}}
\newcommand{\ijkl}{\regions{i}{j}{k}{l}}
\newcommand{\cp}[1]{\textsc{#1}}

\newcommand{\hc}{HC}
\newcommand{\shc}{SHC}
\newcommand{\stem}{SC}
\newcommand{\fsc}{FSC}

\newcommand{\abs}[1]{\ensuremath{\lvert #1 \rvert}}

\begin{document}

\title{Supplemental Material for \\
RNAprofiling 2.0: Enhanced cluster analysis of structural 
ensembles\footnote{\copyright\ 2023. This manuscript version is made available under the CC-BY-NC-ND 4.0 license.}}

\author{Forrest Hurley, University of North Carolina at Chapel Hill \\
Christine Heitsch, Georgia Institute of Technology}


\maketitle

\section{Additional results}

Figures~\ref{fig:supp_feature_count},~\ref{fig:supp_profile_length},
and~\ref{fig:supp_reproducibility} provide further details on 
the growth rate of features, growth rate of profile length,
and reproducibility for 1K and 10K samples respectively.

Recall that \vtwo\ denotes RNAprofiling~2.0, and \vone\ the original, 
i.e.\ RNAprofiling~1.0.
Features considered are helices, helices classes (\hc), 
selected helix classes (\shc), stem classes (\stem), and fuzzy 
stem classes (\fsc).
As features,
the difference between \stem\ and \fsc\ is their frequency, 
i.e.\ estimated probability, in the Boltzmann sample input.
As discussed below in Section~\ref{sec:techdet}, this changes  
both the profile selection and the decision tree construction.

\begin{figure}
    \centering
	\includegraphics[width= \textwidth]{supp_feature_count.png}
    \caption{Number of distinct features by type for each sequence. 
Regression lines assume 0 intercept.
Note difference in y-axis scales between graphs.}
    \label{fig:supp_feature_count}
\end{figure}

\begin{figure}
    \centering
	\includegraphics[width= \textwidth]{supp_profile_length.png}
    \caption{Average length of profiles by feature type for each sequence. 
Difference in regression line slopes 
	for \stem\ and \fsc\ is not significant (p=0.08).}
    \label{fig:supp_profile_length}
\end{figure}

\begin{figure}
    \centering
	\includegraphics[width= \textwidth]{supp_reproducibility.png}
    \caption{Reproducibility of features and of profiles for 1K and 10K samples across family length categories.
Although 1K suffices for features, profile reproducibility degrades with sequence length.
Larger, e.g.\ 10K, samples yield a more reliable structural signal in general.
	}
    \label{fig:supp_reproducibility}
\end{figure}


\section{Implementation information}

\begin{table}[t]
    \centering
    \begin{tabular}{l|l|l}
        Parameter & Value & Default \\
        \hline
	Output\_Type & \{Hasse, Tree\} & Tree \\
	Feature\_Type & \{Selected Helix Class, Stem Class\} & Stem Class \\
        Frequency\_Format & \{Counts, Percentages, Decimals\} & Counts \\
	Helix\_Class\_Selection\_Cutoff & Positive Integer or Auto & Auto \\ 
	Profile\_Selection\_Cutoff & Positive Integer or Auto & Auto \\ 
\hline
        Stem\_Gap & Non-Negative Integer & 2 \\
	Use\_Fuzzy\_Stem\_Counts & Boolean & True \\
        Fuzzy\_Dilation\_Size & Positive Integer & 5 \\ 
	Fuzzy\_Basepair\_Frequency\_Margin & Real in [0,1] & 0.333 \\ 
	Min\_Contingency\_Node\_Proportion & Real in [0,1] & 0.75 \\
\hline
        Use\_Consistent\_Helix\_Class\_Labels & Boolean & False \\
    \end{tabular}
    \caption{Main code parameters. 
``Cutoff'' options override the standard profiling threshold method;
if used, helix classes, resp.\ profiles, with lower frequency in the
input sample will not be considered.
The three ``Fuzzy'' options apply only to \stem\ and are ignored if using \hc.
``Contingency'' option is ignored if output is not a decision tree.
Consistency in helix labeling can be very useful when comparing multiple
analyses for the same sequence.
}
    \label{tab:params}
\end{table}

\vtwo\ is freely available under the GPLv2 license at 
\texttt{github.com/gtDMMB/RNAprofilingV2}
and can be run online via the 
\texttt{rnaprofiling.gatech.edu} website.

When run online, the default option is to upload a Boltzmann sample
in either ct or dot file format.
To expedite exploratory analysis,
the website also provides the option of generating a sample
with either 
RNAstructure 6.4~\cite{Reuter2010} or ViennaRNA 2.4.14~\cite{Lorenz2011}.

A command line interface is available in the form of a python script.  
The script provides all the same options as the web interface, although 
generating samples is disabled unless there is a local install of 
RNAstructure or ViennaRNA.

This new version has a completely new codebase, 
written in Python rather than C/C++. 
\vtwo\ is implemented  and tested with Python~3 (3.6.9) using the 
numpy (1.19.5), networkx (2.4), 
matplotlib (3.3.4), and pygraphviz (1.6) libraries. 
A graphviz install~\cite{Gansner1999} is also required to generate the 
summary profile graph. 
The program is loaded on the web server using PyInstaller (4.10). 

The full output from both the web interface and the command line interface 
is displayed in HTML with JavaScript and should work in 
most modern internet browsers. 
The svg.min library is used to render graphviz output in the browser. 

Table~\ref{tab:params} lists the main \vtwo\ parameters.  
There are some additional IO options for sequences and samples available
via the command line and website.

\vone\ functionality may be recreated by using \shc\ as features, 
and a Hasse diagram as the summary profile graph output.
By default, a maximum average entropy threshold determines the 
selection of \hc\ and of profiles.
As in \vone, this can be overridden by a user-specified cutoff.

The default options for stem gap, fuzzy counts, and contingency nodes
can be altered by users to suit their particular analysis goals.
For example, by default, 75\% of the full binary tree must be present
before it is collapsed into a contingency node, but can be changed
to provide a larger or smaller output tree as useful.

Finally, we highlight the possibility of having consistent \hc\ labels.
This can be very useful if 
comparing results across multiple different samples for the same sequence.
When invoked, the \hc\ labels are based on the sequence itself, 
and so are independent of the particular sample frequencies.


\clearpage
\pagebreak

\section{Technical details of method}\label{sec:techdet}

\subsection{Stem class length and width}

A helix $(i,j,k)$ has length $k$, the number of base pairs it contains.
A \hc\ is denoted by its maximal helix $(i,j,k)$ and has length $k$
since the maximum number of base pairs possible in any of its
constituent helices is $k$.
Observe that $k - 1$ is half
the Manhattan distance from the outermost possible pairing $(i,j)$
to the innermost $(i+k-1, j-k+1)$ in the usual $(x,y)$ plane;
$k - 1 = (1/2) * (\abs{i - (i+k-1)} + \abs{j - (j-k+1)})$.

The \stem\ length is defined analogously and will be the
maximum number of pairings possible in any of its constituent combinations.
First observe that the outermost possible base pair is
the one with the greatest contact distance, i.e.\ where $j - i$
is maximal. 
Let $M$ denote this value, and $m$ denote least possible,
which corresponds to the innermost pair.
Since the Manhattan distance between the outermost and innermost pairs
is equal to $M - m$,
the length of a stem class is defined to be $1 + (M - m)/2$.

The \stem\ width will capture a measure of the `spread'
of observed pairings represented.
This is done by counting the number ``helical diagonals'' covered by
the \stem.
Here, a helical diagonal denotes a line in the $(x,y)$ plane with slope $-1$
which intersects the identity $x=y$ at points where $x$ or $x + 1/2$
is a positive integer.
The \hc\ $h = (i,j,k)$ has width 1 since all pairings lie on
a single diagonal with midpoint $x = (i+j)/2$ as the intersection.
If the helix class $h' = (i', j', k')$ is stemmable with $h$,
then $\abs{ (i+j)/2 - (i'+j')/2 } \leq 1$, and the number of
diagonals covered is either $1$, $2$, or $3$ depending on
the asymmetry of the internal loop/bulge separating them,
i.e.  whether
$((j-k+1) - j' -1) - (i' - (i+k-1) -1)$ is $0$, $\pm 1$, or $\pm 2$.
For a stem class \ijkl, let $C$ be the set of helix class midpoints.
Then its width is $1 + 2 * (\max C - \min C)$.

\subsection{Fuzzy stem class frequencies}

Fuzzy counts address low-frequency base pairings, i.e.\ non-\shc\ ones,
that are `close' to a \stem.
These augmented frequencies are distinguished as \fsc.
A secondary structure has both a \stem\ profile 
as well as a (possibly enlarged) \fsc\ one.
The frequency of a profile is the number of structures 
in the input sample with those features (and no more).
The difference between \stem\ and \fsc\ as features affects the
distribution of profile frequencies which is used both for selection,
and also to build the decision tree.

\vtwo\ uses fuzzy counts by default, which are found as follows.
Consider a secondary structure $S$ and its \stem\ profile $P$. 
For each \stem\ not already in $P$, expand its region \ijkl\
slightly and count the non-\shc\ base pairs from $S$ which fall inside.
This number is then compared to a baseline.
If it is high enough, then \ijkl\ is added to the \fsc\ profile for $S$.
A base pair $(x,y)$ falls inside the expanded region 
of \ijkl\ with dilation size $b$ if 
${i - b \leq x < i + k + b}$ and ${j - l - b < y \leq j + b}$.
The baseline is a fixed fraction, by default $(1/3)$, 
of the average number of base pairs in the expanded region 
over all structures with \ijkl\ in their \stem\ profile.


\subsection{Decision tree construction}

A path starting at the root of the tree corresponds to a sequence of choices,
positive and/or negative, on features which results in one or more
selected profiles as a leaf.
As will be explained, the multiplicity is due to the use of
``contingency'' leaf nodes.
By default, fuzzy frequencies are used to build the tree,
but exact ones can be chosen instead.

Every node corresponds to a subset of the Boltzmann sample, 
and is labeled with the number of structures under consideration.
The root node is the full sample input, i.e.\ all nonempty profiles.
Subsequent nodes consist of groups of profiles determined by prior 
decisions on feature inclusion/exclusion.
Edges denote decisions and are labeled with one or more features,
with negative choices indicated by $\neg$.
Nontrivial \stem\ are denoted by letters, and trivial ones by their \shc\
integer index.
Subset sizes are updated after every decision to remove the 
profiles no longer in consideration.

The choice of inclusion/exclusion for each remaining feature
splits the selected profiles currently under consideration.
Features which yield the same split are grouped together into a
common decision for consideration.

A decision is ``forced'' if the other side of the split is empty,
i.e.\ if none of the selected profiles in the current group 
have the opposite choice of feature(s).
There is at most one forced common decision possible, and it has priority.
It often includes multiple features, particularly negative options.
In this case, there is a single down edge from the current node.

Otherwise, there will be two down edges, one for each side of
the split for the chosen decision.
Note that both sides contain at least one selected profile.
In this case, 
the different possible common decisions (which may consist of
a single feature) are considered.
The one which maximizes the Hellinger distance is chosen.

The Hellinger distance~\cite{cam-yang-00} is
computed over discrete probability distributions $p$ and $q$
defined on sample space $\mathcal{S}$ as
\begin{equation}
    D_H(p,q,\mathcal{S})=\sqrt{\frac{1}{2} \sum_{s\in \mathcal{S}}\left(\sqrt{p(s)}-\sqrt{q(s)}\right)^2}.
\end{equation}
It is a measure of the similarity of $p$ and $q$, and achieves a maximum
of $1$, i.e.\ the greatest dissimilarity,
when $p$ and $q$ have disjoint support.

Let $F$ be the set of features corresponding to a common split in
the current selected profiles.
The sample space $\mathcal{S}$ for the Hellinger distance computation
corresponding to $F$
is the set of all possible combinations of the remaining features
after removing prior decisions (corresponding to the path to the current node)
and the feature(s) in $F$.
The discrete distributions $p$ and $q$ are the normalized frequencies
from the two sides of the split;
their support over $\mathcal{S}$ is typically sparse.
The decision chosen is the $F$ where $p$ and $q$ are most dissimilar,
with ties broken by lexicographic ordering.

This process of grouping remaining features into decisions,
and choosing one of them, proceeds down each branch of the tree
until a selected profile is reached.
At this point, the tree is evaluated for contingency nodes.

All the descendants of a non-leaf node are replaced by a contingency 
if two conditions are met (and these are not met by any of its ancestors).
First, at least 75\% of the full binary tree is present.
Second, all paths from the node being evaluated to a leaf descendant
have the same set of common decisions.
If so, then that set is presented in a single contingency table.

In this case, all frequencies in the table are reported,
not just the ones for selected profiles.
The decision edges are collapsed down to a single (dashed) contingency
edge, labeled with all the decisions so condensed.
The resulting contingency node (dashed rectangle) compactly represents 
multiple selected profiles.
Its frequency is updated to include the low frequency structures 
reported in the contingency table.


\begin{thebibliography}{1}

\bibitem{Reuter2010}
J.~S. Reuter and D.~H. Mathews, ``{RNA}structure: Software for {RNA} secondary
  structure prediction and analysis,'' {\em BMC Bioinformatics}, vol.~11, 2010.

\bibitem{Lorenz2011}
R.~Lorenz, S.~H. Bernhart, C.~{H\"oner zu Siederdissen}, H.~Tafer, C.~Flamm,
  P.~F. Stadler, and I.~L. Hofacker, ``Vienna{RNA} package 2.0,'' {\em
  Algorithms for Molecular Biology}, vol.~6, 2011.

\bibitem{Gansner1999}
E.~R. Gansner and S.~C. North, ``An open graph visualization system and its
  applications to software engineering,'' {\em Pract. Exper}, pp.~1--5, 1999.

\bibitem{cam-yang-00}
L.~L. Cam and G.~L. Yang, {\em Asymptotics in Statistics}.
\newblock Springer New York, 2~ed., 2000.

\end{thebibliography}

\end{document}
