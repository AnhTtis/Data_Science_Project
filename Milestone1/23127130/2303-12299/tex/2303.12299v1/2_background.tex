\section{Preliminaries}
% 1 pages
\label{sec:background}

\subsection{Problem Formulation}
\label{sec:apirec}
Following prior works~\cite{deepapi,martin2022deep}, we formulate the API recommendation task as a sequence-to-sequence task.
Given a query in the form of \textcolor{black}{annotation, i.e., code comment that describes the purpose of a function}, our goal is to recommend a sequence of API calls that implements the functionality described in the query.
We also aim to leverage information from multiple sources such as \so and GitHub.

Here, we formally define the task of query-based API recommendation.
We denote the input sequence, e.g., annotation, as $W = \langle W_1, W_2, \ldots, W_n\rangle$, where $W_i = \langle w_i^1, w_i^2, \ldots, w_i^{n_i}\rangle$ and $w_i^j$ refers to the $j^{th}$ subtoken in word $W_i$.
We want to generate an API sequence to be recommended.
We denote the API sequence as $A = \langle A_1, A_2, \ldots, A_m\rangle$, where $A_i = \langle s_i^1, s_i^2, \ldots, s_i^{n_i}\rangle$.
$s_i^j$ refers to the $j^{th}$ subtoken in API call $A_i$.

When a model predicts the subtoken $s_i^1$, the input of the model would be the input sequence $\langle W_1, W_2, \ldots, W_n\rangle$ and the previously predicted subtokens $\langle s_1^1, \ldots, s_1^{n_1}, \ldots, s_{i-1}^{1}, \ldots, s_{i-1}^{n_{i-1}}\rangle$. 
% \ft{are these all previous subtokens including from the previous words? not sure what the notation wants to convey}.\zt{I think I was trying to describe the autoregressive way? like the input of the model would consider the previously generated tokens. the notation is more served for the beam search in the next section}\ft{what is $s_i$? }
When the end token is predicted, an API recommendation model will output the chain of predicted subtokens as the API sequence.
Take the annotation and the API sequence in the first box in Figure~\ref{fig:rel-post} as an example.
In the beginning, the input is only the annotation.
After the API recommendation model predicts two subtokens in the target API sequence, the input will be $\langle W_1, W_2, W_3, W_4, s_5^1, s_5^2\rangle$, where $W_1=parse$, $W_2=string$, $W_3=to$, $W_4=object$, $s_5^1=Float$, $s_5^2=.$.
The API recommendation model would predict the next token based on this input.
Suppose the model predicts $parseFloat$ and $\langle$\texttt{eos}$\rangle$, the output API sequence would be the combination of these three subtokens $s_5^1,s_5^2$ and $parseFloat$.

\subsection{Motivating Example}
\label{sec:motivating-example}
We use the example in Figure~\ref{fig:rel-post} to motivate the need for leveraging \so posts for API sequence recommendation. The annotation for this example is \texttt{parse string to object}, and the target APIs are \texttt{Integer.parseInt}, \texttt{Long.parseLong}, \texttt{Float.parseFloat}, and \texttt{Double.parseDouble}. From the annotation alone, it is not possible to know that the input \texttt{String} contains numbers, and the \texttt{Object} should be an instance of numbers (e.g., \texttt{Integer}). 
Broadly speaking, the input \texttt{String} can contain any set of characters, and the output \texttt{Object} can be an instance of any object.


To obtain a more specific query, we can expand based on the \so posts that may contain similar sets of APIs. 
\textcolor{black}{We define the similarity between an annotation and a \so post as the proportion of APIs from the target APIs that are mentioned in the post. The more APIs from the target APIs that are mentioned in the \so post, the more similar the \so post is to the annotation.
Based on this definition, we can prepare the training data which contains the most similar \so post and the annotation; and learn a similarity function from it.
In practice, in which the target APIs are not available, %and we aim to suggest the target APIs.
we can use the learned similarity function to find the most similar posts given the annotation. 
We describe in detail how we construct such a function in Section~\ref{sec:similarso}.
}
% Granted, in practice, we may not be able to construct a similarity function that can perfectly approximate the similarity in terms of the relevant APIs. That said, we should be able to build such a function with a certain degree of error. .
 
% Assume that we have a similarity function that can perfectly approximate the similarity between a \so post and annotation in terms of the proportion of relevant APIs that are mentioned in the \so post.
Consider that we have a collection of \so posts containing SO Post 1, SO Post 2, and SO Post 3 as shown in Figure~\ref{fig:rel-post}.
Among these \so posts, SO Post 1 would be the most similar since it contains the largest number of relevant APIs: \texttt{Float.parseFloat}, \texttt{Integer.parseInt}, and \texttt{Double.parseDouble}.
The title for SO Post 1 is \texttt{parse String containing a Number into a INT}. 
By expanding the query with this title, we can now understand that the input \texttt{String} should be the numbers.

The above illustration demonstrates that we can leverage Stack Overflow to improve the API sequence recommendation, particularly by adding more information from the \so so that we can input a more accurate query to the API sequence generation model.

    % why it is good, show example of query extension where it works

% \subsection{Multi-modality of Source Code}
% \label{sec:multi-modal}
% Modality refers to how types of information for an object exist~\cite{baltruvsaitis2018multimodal}. 
% For instance, to identify cats from other animals, we may want to leverage multiple modalities, including shapes, colors, and movements. 
% Similarly, source code can also have multiple modalities such as description/annotation and its variants (e.g., abstract syntax tree (AST), control-/data- flow graph (CFG and DFG). 
% It is based on the fact that semantics/contexts can be captured and represented in different ways. The representation resulting from data of multiple modalities is named the multimodal representation.
% These modalities are semantically equivalent and provide complementary information for such a source code, and they tend to be included as learning targets for specific tasks~\cite{wang2021syncobert,wan2019multi,yang2021multi,ma2022mmf3}.
% Multimodality is known to perform better than the unimodal learning model, which only studies data of a unique modality~\cite{ngiam2011multimodal}.

% \subsection{CodeBERT}
% \label{sec:codebert}
% CodeBERT is a bimodal Transformer model for source code representation that leverages both programming language (PL) and natural language (NL)~\cite{codebert}. 
% Its baseline dataset is CodeSearchNet, a large-scale dataset for code search task~\cite{husain2019codesearchnet}. 
% Although the dataset is tailored for a specific downstream task (i.e., code search), the natural language queries for the code snippets are well-documented, which is enough to represent the corresponding code snippets. 
% CodeBERT contemplates masked language modeling (MLM)~\cite{devlin2018bert}, which predicts the masked natural language and source code tokens.
% This modeling is known to be powerful for the generalization of the learning models~\cite{devlin2018bert}.
% CodeBERT also employs replaced token detection (RTD), which trains a bidirectional model while learning from all input positions~\cite{clark2020electra}.
% Recently, researchers have applied CodeBERT to several downstream tasks (e.g., automatic program repair~\cite{mashhadi2021applying}, code retrieval~\cite{bahrami2021augmentedcode}, dataset construction~\cite{lu2021codexglue}, etc.) in the SE field and benefit from its meaningful embeddings in the encoding phase.
% As we attempt to leverage embeddings of multiple modalities and several studies have demonstrated its effectiveness~\cite{deepapirevisited}, we leverage CodeBERT as our encoding method.


% \subsection{Embedding Unification}
% \label{sec:embedding-unification}
% There exists a wide range of use-case scenarios where unifying the obtained embeddings would be inevitable. 
% We define the unification of embeddings to include concatenation, combination, and fusion.
% Although generated embeddings already represent software elements, these embeddings tend to contain inadequate information. 
% For example, multi-modal analysis tasks that encode each unimodal data to get different embeddings, or a task that requires sub-embeddings, such as embedding of each method to represent a class, etc~\cite{sun2020multi,francis2019fusion,shvetsova2022everything}.
% To decode them, embedding unification is generally necessary to represent the multi-modality or a high-dimensional embedding.
% Towards this issue, multiple studies~\cite{wang2020automated,wang2021enhanced} focus on the automation of this unification process. 

% % multi-modality
% % combine sub-embeddings -> a high-dimensional embedding
% % automatic concatenation studies

% % \subsection{Motivation}
% % \label{sec:motivation}
% % % why code completion does not work
% % As categorized by Peng et al.~\cite{peng2022revisiting}, API recommendation has two main types: query-based and code-based.
% % Query-based approaches take natural language queries from users and retrieve either a single API or API sequences, while code-based techniques take source code snippets near the recommendation point to retrieve a single API. 
% % Although many researchers have invested remarkable efforts in downstream tasks of Software Engineering (SE) with the multi-modality of source code, 
% % there exist no study that leverages multi-modal (e.g., source code and natural language) inputs for API recommendation.

% % As we described in Section~\ref{sec:multi-modal}, multi-modality accelerates the effectiveness and performance of SE tasks. Specifically, we observe a strong correlation between the source code and its different modalities (i.e., code annotation and code context).
% % For example, 

% % \kisub{I'll come again once the example is there.}



% % , while the code-based approaches retrieve 

% % takes source code snippets near the recommendation point and works like a next-token prediction task.

% % recommend an API call or an API sequence given a query that describes the user intent in natural language.
% % In contrast, the code-based API recommendation takes source code snippets near the recommendation point and works like a next-token prediction task.
% % It would recommend the next API call considering the code context before the prediction position.
% % Output-wise, query-based approaches recommend either a single API or API sequences, while code-based techniques only retrieve a single API.


% % One difference between these two types of API recommendation is that query-based API recommendation can either work in recommending a single API call or an API sequence.
% % In contrast, the code-based API recommendation can only recommend a single API call.
% % It remains unknown whether it is possible to recommend the API sequence by considering the source code context and the query together.
% % These two types of API recommendation approaches have been compared separately so far.
% % There is no existing approach that leverages the information from both sides.
% % We make the first attempt to solve the API recommendation by integrating query-based and code-based API recommendations. 



% % I don't have any better idea to elaborate on the following paragraphs.

% % \subsection{Use-case Scenario}
% % \label{sec:use-scenario}
% % The considered use scenario is when a developer is writing code.
% % They will first describe the requirement in a sentence of annotation.
% % After that, they start to type source code.
% % When the context code is sufficient (e.g., three lines of code have been written), an automatic approach can suggest the API sequence would be helpful in the following code they need to write.
% % After checking the suggested API sequence, the developer moves on to write the rest of the code.
% % One potential implementation could be an IDE extension that would help developers in a real-time manner.

% \subsection{Problem Formulation}
% \label{sec:problem}

% \begin{figure}
% \centering
%     \includegraphics[width=\linewidth]{figures/code_example.png}
%     \caption{Motivating example}
%     \label{fig:code}
% \end{figure}

% We formulate the API recommendation problem as a sequence generation task.
% Given the input of natural language and part of source code, we aim to generate API sequence recommendation to be used in the latter part of the code.
% To achieve this, we leverage open source Java projects from Github to build a dataset that is tailored to our experiment.

% Each data point in our dataset is built upon a Java method's structure, representing NL and PL part of the method.
% We divided every method into several components, as shown in Fig\ref{fig:code}.
% % Given the input of (1) annotation, and (2) context code, we try to predict the api sequence to be used in the latter part of the code, i.e., item (3).

% There are 3 items highlighted in this example, described as below:
% \begin{enumerate}
%     \item {Javadoc comment / Annotation}
    
%     The first sentence of the Javadoc comment is used as annotation.
%     \item {Context code}
    
%     The method declaration and first 3 lines of the method's source code.
%     API sequence extracted from this context code is called \textit{context API}.

%     \item {Target}

%     This is the part where we extract the \textit{target API} as our reference. The target API will serve as training target and example's reference in our experiment.
% \end{enumerate}

% For the example showed in Figure\ref{fig:code}, the annotation for this example would be \textbf{Flattens the provided list into a single list}, while the context code is 
%     \begin{verbatim}
%     int size = 0;
%     for (final List<V> list : lists) {
%         size += list.size(); }
%     \end{verbatim}

% Based on the context code, the \textbf{context API} extracted for this example is \verb|List.size| and the \textbf{target API} is made of 4 api calls, namely 
% \textit{ArrayList.$<$init$>$, List.AddAll, ArrayList.$<$init$>$, List.addAll}
