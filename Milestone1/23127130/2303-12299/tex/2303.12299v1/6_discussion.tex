\vspace{1mm}
\section{Discussion}
\label{sec:discussion}

% \subsection{Stack Overflow Impact}
% \label{sec:so-impact}
% Our experiment to use \so as query expansion was not yield a higher result before we implemented an ensemble approach.
% As displayed in Table \ref{tab:res-rq1-2}, applying query expansion directly to all of the data points does not improve the API sequence generation performance.
% As a matter of fact, it made it the CodeBERT performs worse than accepting only annotation as input.
% This phenomenon indicates two possibilities, either that the sentence embedding model could not learn to map annotationd \so well or \so information is not useful to be used as query expansion in this task.

% \iv{maybe this is not necessary (?)}
\subsection{Ability to Find Similar \so Post}
\label{sec:discussion-similar-post}
%In this work, we adapted CLEAR~\cite{wei2022clear}, a state-of-the-art technique that leverages similar \so post for API recommendation to bridge the information gap between annotation and \so title as its capability to learn the semantic representation of \so posts has been proven.

% We chose to adopt the CLEAR framework to bridge the information gap between annotation and \so title due to its proven capability to learn the semantic representation of \so posts.
% However, even though CLEAR is the state-of-the-art approach for finding relevant \so post for a given natural language, our data is derived from a different domain, i.e., GitHub.
% One difference that is evident is in the form of the sentence. 
% In \so title, the sentence is presented in the form of a question, while for annotation, the sentence is presented in the form of a statement.
% This piqued a question on whether the adapted technique of CLEAR is sufficient to bridge the knowledge gap between annotation and \so title, given the difference in their forms.
%On the other hand, our data is collected from a different domain which has an evident difference in the formation of sentences.
%Specifically, \so titles formed the sentences as questions, while the annotations from GitHub are formed as statements. 
%This triggered a question if CLEAR is sufficient to bridge the knowledge gap between them under this specific circumstance.

%To gauge the rationale of our adaptation, 
We counted the number of cases in which the framework can successfully link an annotation to a relevant \so post.
We consider a \so post as relevant if the target API sequence shared common API(s) with API(s) mentioned in the \so answer post. 
We observed 3 degrees of success in finding a similar post, namely \textit{All match}, \textit{Partial match}, and \textit{No match}.
For the \textit{All match} category,
all of the APIs that are present in the target APIs also exist in the obtained in the answer post.
For the \textit{Partial match} category, a partial number of APIs are mentioned in the retrieved posts. 
The retrieved \so post contains one of the target APIs in its set of APIs. We categorized this kind of match as a partial match. 
As for the \textit{No match} category, none of the target APIs exist in the obtained answer post.
% For example, for the pair of annotation \textit{remove sub array} with target APIs of \verb|Array.newInstance, System.arraycopy|. When we pass this data to our model, %the cross-encoder and bi-encoder model, 
% we obtained a \so post with a title of \textit{Error making a sub-array}, which contains these APIs in the accepted answers: \verb|System.arraycopy, Arrays.copyOfRange|.
For example, we input a pair of annotation \textit{remove sub array} with target APIs of \texttt{Array.newInstance,System.arraycopy} and obtained a \so post with a title of \textit{Error making a sub-array}, which contains these APIs in the accepted answers: \texttt{System.arraycopy, Arrays.copyOfRange}.
The retrieved \so post contains \texttt{System.arrayCopy} method, which also appears in the target API.
% We present the summary of capability of our adaptation of CLEAR's framework is presented in Figure\ref{fig:dicussion-a}.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{image/pie-chart.png}
    \caption{Capability of finding similar Stack Overflow post for the annotations in the training data}
    \label{fig:dicussion-a}
\end{figure}

% \ft{It seems like we should present Figure~\ref{fig:dicussion-a} right after this paragraph}
% \kisub{Should we bring Figure 4 here? or we just refer to the figure here?}

% \ft{This seems to be out of place. Perhaps we can talk about it in lessons learned about the importance of finding similar posts? Also, using \so API to limit the search space does not seem to be a query expansion method(?)}
% \iv{moved 1 paragraph into lesson-learned section}

% However, this approach requires an outstanding capability of finding a relevant \so post for a given annotation. Otherwise, the \so API that is used to limit the search space could harm the model's capability to generate proper recommendations due to a lack of APIs to choose from.

Based on the overall result presented in Figure~\ref{fig:dicussion-a}, using the \so API in the post-processing phase, i.e., beam search, in order to limit the search space is not favorable in our case.
This statement is backed by our investigation, which shows that more than 73\% of the annotation in our dataset failed to find a relevant \so post that shares common API(s) with the target API.
Therefore, we deemed leveraging \so information in the query expansion manner is more suitable for our study because even though the API method(s) are not intersected, they may still come from the same class or domain.
For example, for annotation \textit{compress array} with a target API of \verb|ArrayList.size, Arraylist.ensureCapacity|, the retrieved \so post is a post with \textit{Expand array once it reaches limit} as its title, and the APIs mentioned in that post are \verb|Arrays.copyOf| and \verb|System.arraycopy|.

\begin{table}[t]
    \caption {Precision and Recall as Secondary Metric}
    \label{tab:rq1-precision-recall} 
    \centering
    % \renewcommand{\arraystretch}{1.2}
    % \begin{tabular}{@{}|l|l|l|l|l|@{}}
    \begin{tabularx}{0.7\linewidth}[t]{|X|X|X|X|X|}
        \hline
        \textbf{Input} & \textbf{Precision} &  \textbf{Recall} \\
        \hline
        \textbf{CodeBERT (baseline)} &  0.429 & 0.399 \\
        \hline
        \textbf{\method} & 0.452 & 0.442 \\
        \hline        
    \end{tabularx}
\end{table}

% \iv{addresses comment #3}
% \textcolor{red}{Nonetheless, with just around 26\% of the annotation linked to a highly relevant \so post, \method managed to improve the performance of API sequence recommendation close to 11\%. This suggests that better linking of similar \so post would likely lead to even better performance.}
\textcolor{black}{Despite only about 26\% of the annotations being associated with a highly relevant \so post, \method was able to increase the performance of API sequence recommendation by nearly 11\%. This indicates that if similar \so posts were linked more effectively, the performance could be further enhanced.}

\textcolor{black}{
We also calculated precision and recall as additional metrics to supplement our analysis.
While precision and recall are commonly used for evaluating API recommendation techniques, they are more appropriate for assessing API set recommendations rather than API sequence recommendations.
Therefore, we primarily rely on the BLEU-4 score as the main metric in this study.
The purpose of calculating the precision and recall is to determine if \method can provide superior API recommendations or if it is only better at arranging the sequence of API calls compared to the baseline approach.
As presented in Table~\ref{tab:rq1-precision-recall}, \method achieves higher precision and recall scores than the baseline. 
The 10\% improvement in the precision indicates that \method manages to outperform the baseline in both accurately identifying the appropriate APIs and suggesting their correct invocation order.}

% The precision and recall also suggest that utilizing information from \so post is beneficial in recommending the correct set of APIs.}\ft{We can also say that precision and recall also add additional insights, e.g., \method also recommend a larger number of correct APIs. We can also mention about the statistical test. }


%This shows that incorporating \so information in the API sequence generation is worth further exploration.
%The next direction for this work could aim towards mapping the similar annotation and \so post closer so that the post-processing technique could be leveraged.



% In our experiment, we made the connection between annotation and \so post based on their API, as explained in Figure\ref{fig:rel-post}.


% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{image/bar-chart.png}
%     \caption{Number of successful case where BLEU-4 score=1}
%     \label{fig:dicussion-b-1}
% \end{figure}

\subsection{Recommendation Analysis}
\label{sec:result-analysis}
% \kisub{Shall we skip this sentence?}
% Even though \method managed to outperform the state-of-the-art baseline for API sequence generation, it still has flaws.
In order to provide the future direction of API sequence generation, we discuss the cases where \method fails to provide an adequate sequence as well as the successful cases.
% As presented in Figure\ref{fig:dicussion-b-1}, adding \so information increase the number of perfect API sequence generation more than 50\%.
% Using just annotation as input yields in 721 perfect generated API sequence, while adding \so title and \so API yields more than 1100 test case where the generated API sequence matches the target API perfectly.
Based on our investigation, \method successfully incorporates \so information to improve the API sequence generation in 7,938 cases.
%Compared to the annotation only input, there were BLEU-4 increase when \so information such as title and API were used in the query expansion.
We also present several examples below to demonstrate how incorporating \so information guides the sequence-to-sequence model for predicting better API sequence.
Example 1 presents a case where \method could correctly identify the intention of the developer based on the annotation and managed to pair it with a \so post that improves the API sequence generation performance.
% The detailed case for this example is presented as below:



% In this first example, 
% The annotation indicates that the code is for validating a file type.
In Example 1, displayed in Figure~\ref{fig:eg1}, the annotation indicates that the developer wants to check whether a filename matches to a filter.
To do this, it could be done by using regex.
Without \so information, the baseline model predicts a completely irrelevant suggestion API sequence.
It could not be used to implement the intended functionality.
Nonetheless, introducing \so information to this datum successfully points the API sequence generation model towards the correct domain.
\method generated an API sequence that is relevant to the regex usage and is identical to the target API.

Similarly, in Example 2, the baseline produces a near identical API sequence to the target API. %, it is lexically different from the target API. %Hence, resulting in a low BLEU-4 score.
However, when \so information is introduced, \method manages to produced a sequence that is identical to what the developer has written.
It shows the capability of \method in generating API sequence with an assistance from \so information.
%Nonetheless, this phenomenon makes us wonder if it is fair to give a low score to a semantically correct API sequence.
%It rises the importance of measuring the capability of a the API sequence generation based on the result of the function, rather than comparing the API sequence based on the recommended API method.

% Based on this annotation, \method retrieved \so post that is more relevant towards the string manipulation topic rather than the file topic. 
% Moreover, the annotation and \so information, i.e., title and the APIs, also did not share common lexical attributes.
% Nonetheless, \so information from this post helps to provide better API sequence generation.
% When we utilize the title and API of this post as a query expansion to the annotation, \method manages to yield an API sequence that contains 3 out of 4 APIs in the target API.

% the \so information helps the encoder-decoder model in \method to produce a better API sequence due to the fact that file type validation is commonly done by using the last token after '.' in a file name. \ft{How does the similar \so post suggest this to the model?}
% In this case, we could infer that \method successfully identifies the intent of the developer and enhances the API sequence generation by incorporating \so information as query expansion.
% \textcolor{black}{Meanwhile, the baseline prediction fails to generate an API sequence that contains any API that is invoked in the target API.}

\lstdefinelanguage{qual}
{
  % list of keywords
  morekeywords={
    Annotation,
    Target_API,
    SO_title,
    SO_APIs,
    Baseline_prediction,
    PICASO_prediction,
  },
  sensitive=false, % keywords are not case-sensitive
  morecomment=[l]{//}, % l is for line comment
  morecomment=[s]{/*}{*/}, % s is for start and end delimiter
  morestring=[b]" % defines that strings are enclosed in double quotes
}

\begin{figure}[!htp]
\begin{center}
{
\lstset{language=qual,keywordstyle=\color{red}}
    \parbox{1\linewidth}{
        \lstinputlisting[linewidth={\linewidth}, frame=tb,basicstyle=\scriptsize\ttfamily]{list/eg1.list}
	}
}%
\caption{Example 1}
\label{fig:eg1}
\end{center}
\end{figure}


\begin{figure}[!htp]
\begin{center}
{
\lstset{language=qual,keywordstyle=\color{red}}
    \parbox{1\linewidth}{
        \lstinputlisting[linewidth={\linewidth}, frame=tb,basicstyle=\scriptsize\ttfamily]{list/eg2.list}
	}
}%
\caption{Example 2}
\label{fig:eg2}
\end{center}
\end{figure}


\begin{figure}[!htp]
\begin{center}
{
\lstset{language=qual,keywordstyle=\color{red}}
    \parbox{1\linewidth}{
        \lstinputlisting[linewidth={\linewidth}, frame=tb,basicstyle=\scriptsize\ttfamily]{list/eg4.list}
	}
}%
\caption{Example 3}
\label{fig:eg4}
\end{center}
\end{figure}


% Example 1:

% \noindent
% \fbox{
% % \label{sec:ex-1}
% \parbox{0.95\linewidth}{
    
% \textbf{annotation}: \textit {check if the given extension of the filename is accepted}\\
% \textbf{target API}: \\
% \texttt{String.length, String.lastIndexOf, String.length, String.substring, String.equalsIgnoreCase}\\

% \textbf{SO title}: \textit{Regular expression in Java}

% \textbf{SO API(s)}: \\
% \texttt{Pattern.compile, String.matches, Pattern.compile, String.matches}\\

% \textbf{Baseline prediction}: 

% \texttt{String.toLowerCase, List<String>.contains}

% \textbf{\method prediction}: 

% \texttt{String.lastIndexOf, String.substring, String.equalsIgnoreCase}
% }
% }\\
% \kisub{Shall we use 1 linewidth? and examples also need captions and labels such that we can refer to in the paragraphs :) .}
% \iv{sure, please make it aesthetically pleasing if you may, I'm bad at this :')}

% \textcolor{black}{
% Looking deeper into \method's recommendation, we could infer that the encoder-decoder model is able to grasp the intention of the developer.
% To check a file type and validate whether it is acceptable involves two tasks. 
% The first task is to extract the file extension from the given file name, which is commonly done by extracting the last token after '.' as its file extension.
% \method recommends to conquer this sub-task by suggesting  \texttt{String.lastIndexOf} to find the last index of '.' and \texttt{String.substring} to extract the token after '.'.
% The second task is to validate whether the extension is correct.
% In this task, \method suggests the usage of \texttt{String.equalsIgnoreCase} that can be used to check the extracted extension's validity.}
% The recommended order of the API invocation is also correct.

% \textcolor{black}{
% For the baseline prediction, which is generated without \so information, the recommended API sequence only partially correlates with the developer's intention.
% It fails to give any recommendation for the first subtask, i.e., extracting the extension from the file name.
% It only contains \texttt{String.toLowerCase} and \texttt{List<String>.contains}, which can be useful in the validation subtask.
% }
% % \ft{Can we explain the intention and infer it from the annotation so it is clear how it can be associated with \texttt{contains}?}
% \textcolor{black}{
% The above example demonstrates that utilizing \so information helps \method to recommend an API sequence that fulfills the developer's intention.}

% that was gathered from the Q\&A forum brings more relevant methods that are used by developers since nearly all of the API used in the target API sequence present in the \method prediction.

% One thing that need to be improved from this example is the fact that one missing API method, i.e., \texttt{String.length}, managed to downgrade the evaluation score of this generated sequence because it is mentioned twice in the middle on the API sequence.
%Further evaluation may be conducted to assess the impact of this missing API to the real world developer.



% Similar to the first example, in the second example above, \method presents its ability to identify the general intention of the developers to work with a database by retrieving a \so post related to an SQL driver error.
% However, in this case, both the baseline and \method failed to give an adequate API sequence to the developer.
% Nonetheless, the API sequence produced by \method is lexically more similar to the target API than the baseline. 
% It also involves fewer API calls in the sequence compared to the baseline. 
% Furthermore, the API sequence generated by the baseline contains noise from the invocation of \texttt{ResultSet.getInt} method, which seems irrelevant to the observed context.

% Example 2:

% \noindent
% \fbox{
% \parbox{0.95\linewidth}{
    
% \textbf{annotation}: \textit {insert a new term relationship into the database}\\
% \textbf{target API}: 
% \texttt{String.substring, \\
% Connection.createStatement, Statement.executeUpdate, Connection.close}\\

% \textbf{SO title}: \textit{java.sql.SQLException: No suitable driver found for jdbc:sqlserver}

% \textbf{SO API(s)}: 

% \texttt{Class.forName,\\
% DriverManager.getConnection}\\

% \textbf{Baseline prediction}: 

% \texttt{Connection.prepareStatement, PreparedStatement.setString, PreparedStatement.executeQuery, ResultSet.getInt}

% \textbf{\method prediction}: 

% \texttt{Connection.createStatement, Statement.executeUpdate, Statement.close}
% }
% }\\

% It raises a possibility of a new direction in assessing API sequence generation's technique based on its efficiency as long as it does not sacrifice its effectiveness.
% \ft{I don't follow how the above is related to efficiency.}
% \textcolor{blue}{This statement is derived from the fact that \method manages to recommend an API sequence that is semantically correct with fewer invocation call involved.
% From code maintainability\cite{samoladas2004open}, less API translates to better code maintainability. The more API invocation involced in a function, the more complex it will be. Hence, less API calls will make the code more straightforward for the developers who maintains the code}

% It is a long shot, but it could be done by combining code generation, test case, and  
% While assessing developer's preference need manual evaluation, the efficiency could be assessed by applying the recommended API sequence towards a test case and pick the sequence that involves less 

% Last but not least, we discuss the case where \method fails to deliver a proper API sequence recommendation at the method level. Still, it shows promising potential at the class level, which will be discussed in Example 3.

% Regarding such an example, it is evident that at the method level, all of the APIs mentioned in the API sequence generated by both the baseline and \method do not have a common attribute with the target API.
% This leads to evaluating the result of the API sequence generated for this test case as 0.
% On the other hand, if we give more attention to the class-level API recommendation, it is clearly visible that \method managed to predict the API class correctly.
% It manages to infer the intention of the developer to work with a File object.
% On the contrary, the baseline could not even predict the correct class for the given annotation and proceeded to generate a completely irrelevant API sequence recommendation.
% This demonstrates how additional information from \so will bring benefit to the API recommendation task as it could guide the model to generate a better recommendation.




% As a matter of fact, we regard method-level API recommendation as a more important component in generating API sequence generation because it will better accommodate developers' intentions compared to the class-level recommendation.
% Nonetheless, inspired by a study~\cite{chen2022more}, the class-level recommendation could be used as a guide to generate better recommendations, specifically by dynamically limiting the search space to certain API classes.
%Combined with a better technique to map annotation and relevant \so posts in the closer vector space, this post-processing approach could be a direction that worth to be explored in the near future.

% Example 3:

% \noindent
% \fbox{
% \parbox{0.95\linewidth}{
    
% \textbf{annotation}: \textit {this method tries to create a directory represented by the given file object after having checked that it doesnt exist}\\
% \textbf{target API}: 
% \texttt{File.exists File.isfile}\\

% \textbf{SO title}: \textit{Unique File name using system time in Java?}

% \textbf{SO API(s)}: 

% \texttt{File.createTempFile, File.deleteOnExit}\\

% \textbf{Baseline prediction}: 

% \texttt{String.indexOf, String.substring, StringBuffer.append}

% \textbf{\method prediction}: 

% \texttt{File.getAbsolutePath, File.isDirectory, File.getAbsolutePath}
% }
% }\\

% Example 4:\\

% \begin{figure}[!htp]
% \begin{center}
% {
% \noindent
% \fbox{
% \parbox{0.95\linewidth}{
    
% \textbf{annotation}: \textit {check if the filename matches the filter}\\
% \textbf{target API}: \\
% \texttt{Pattern.compile Pattern.matcher Matcher.matches}\\

% \textbf{SO title}: \textit{How to match letters only using java regex, matches method?}

% \textbf{SO API(s)}: 

% \texttt{Pattern.compile, String.matches}\\

% \textbf{Baseline prediction}: 

% \texttt{String.lastIndexOf String.length}

% \textbf{\method prediction}: 

% \texttt{Pattern.compile Pattern.matcher Matcher.matches}
% }
% }\\
% \end{center}
% \end{figure}
% \label{fig:example-1}
% \caption{Example 1}

% Example 5:

% \noindent
% \fbox{
% \parbox{0.95\linewidth}{
    
% \textbf{annotation}: \textit {get the prefix of the object id}\\
% \textbf{target API}: \\
% \texttt{String.indexOf String.substring}\\

% \textbf{SO title}: \textit{Java regexp substring extraction pattern}

% \textbf{SO API(s)}: 

% \texttt{String.indexOf, String.substring}\\

% \textbf{Baseline prediction}: 

% \texttt{String.lastIndexOf, String.substring}

% \textbf{\method prediction}: 

% \texttt{String.indexOf, String.substring}
% }
% }\\

In Example 1 and 2, \method manages to produce a better API sequence recommendation than the baseline.
This is the advantage that is brought forward by the similar \so post.
The retrieved \so post contains title that is semantically relevant to the task described in the annotation, as well as informative \so API. % even though they are lexically different.

Another evidence is served by Example 3, presented in Figure~\ref{fig:eg4}. Even though the annotation only consists of 2 words that convey an intention to stop a server, the SO title and the SO API provide an information on steps that one typically can do to stop a server. They point out that developers may need to interrupt a thread.
% For instance, in Example 2, even though the 
Leveraging this information, \method manages to perform well, generating an output that nearly identical to the target API.
On the other hand, we could see that even though the baseline contains 2 APIs from the target APIs, they are not in the correct order. It suggests the invocation of \texttt{ServerSocket.close} before invoking \texttt{Thread.interrupt}.
When \so information is introduced, the relative order between the APIs are correct. Specifically, \method manages to suggest the correct order of thread interruption by calling \texttt{Thread.interrupt, Thread.sleep, Socket.isclosed, Socket.close, ServerSocket.close}.


% Example 6:

% \noindent
% \fbox{
% \parbox{0.95\linewidth}{
    
% \textbf{annotation}: \textit {parse the line which holds the initial heap size}\\
% \textbf{target API}: \\
% \texttt{String.indexOf, String.substring, Integer.parseInt}\\

% \textbf{SO title}: \textit{Parsing string using the Scanner class}

% \textbf{SO API(s)}: 

% \texttt{String.indexOf, Integer.parseInt}\\

% \textbf{Baseline prediction}: 

% \texttt{String.indexOf, String.substring, Integer.parseInt}

% \textbf{\method prediction}: 

% \texttt{String.indexOf, Integer.parseInt}
% }
% }\\

\begin{figure}[!htp]
\begin{center}
{
\lstset{language=qual,keywordstyle=\color{red}}
    \parbox{1\linewidth}{
        \lstinputlisting[linewidth={\linewidth}, frame=tb,basicstyle=\scriptsize\ttfamily]{list/eg5.list}
	}
}%
\caption{Example 4}
\label{fig:eg5}
\end{center}
\end{figure}

\textcolor{black}{
In Figure~\ref{fig:eg5}, we present Example 4 where \method produces a superior API sequence despite the relevant \so post not having any common APIs with the baseline. 
In this example, the developer intends to update the state of the nodes when they add a new node. As evidenced in the target sequence, the developer implements this function with a HashMap library. Interestingly, even though StackOverflow API does not share a common API, the title of the post (i.e., TreeMap) directs \method towards the usage of Map.
\method manages to generate a better API sequence recommendation for this case by suggesting the usage of HashMapâ€™s API, instead of ArrayList, which is suggested by the baseline.
Therefore, we believe the improvement of the overall score comes from the cases where \so post guide \method towards better API sequence generation.
% \kisub{YDYT? I guess the last sentence can give the readers misunderstandings like we only revealed this case and expect there are more. If you agree, we might want to omit it.}
}

\begin{figure}[!htp]
\begin{center}
{
\lstset{language=qual,keywordstyle=\color{red}}
    \parbox{1\linewidth}{
        \lstinputlisting[linewidth={\linewidth}, frame=tb,basicstyle=\scriptsize\ttfamily]{list/eg3.list}
	}
}%
\caption{Example 5}
\label{fig:eg3}
\end{center}
\end{figure}


In Example 5, \method performs relatively worse than the baseline.
This is due to the missing API method invocation in the middle of the predicted sequence.
In the target API, \texttt{String.substring} is invoked before \texttt{Integer.parseInt}. 
In this case, the baseline predicts the sequence correctly.
However, when we introduced more information from \so, \method removes an API from the sequence, resulting in a drop of  BLEU-4 score.
This case suggests the importance of finding a highly relevant \so post.

All in all, in Examples 1-4, the retrieved \so contains SO title that are highly correlated to the annotation. This leads to the success of \method in producing a sequence that is closer to the target API. 
However, in Example 5, the retrieved \so post is less relevant as the SO title does not provide much additional information that can be used for query expansion. Rather, it may emphasizes some information over the other, which leads \method to generate a sequence that is further from the target API. 
% \iv{This is a failure example.}

\subsection{Lessons Learned}
\label{sec:lessons-learned}
% \begin{itemize}

    \textbf{Future studies should explore methods to better bridge the semantic gap between annotation and \so query.}
Even though sentence embedding can bridge the knowledge gap between Github and \so, i.e., code annotation and \so post, there is still a large room for improvement as 73.66\% cases are \textit{No match}. Thus, future research should explore better alternatives for doing it.
If we could retrieve a truly relevant \so post for a given annotation with a high success rate, we can also utilize the \so information beyond query expansion methods.
One option is to do a post-processing step in the generation process similar to Chen et al. work~\cite{chen2022more}.
% \ft{could we elaborate what is actually being done here?}
\textcolor{black}
{
They proposed Cook, an API sequence recommendation technique that has enhanced with code-specific heuristic rules in the beam search module.
As they are working with partial source code as an input, they derived several code-specific heuristic rules, for instance, syntax-oriented heuristics.
Since we are working with annotation as an input and do not work with source code, we may not be able to derive a heuristic rule related to the syntax. However, it is possible to derive a rule about the sequence. % of the usage for the sequence.
For example, \texttt{Statement.close} method should be invoked after \texttt{Statement.executeUpdate} method.
If we can successfully retrieve relevant \so API for a given annotation, we could use it to limit the number of plausible APIs and use a heuristic rule to generate API sequence from them.}
% \ft{Need to distinguish this with the class-level API recommendation}

% We reckon that other approaches beyond query expansion methods can be explored to incorporate \so information in the API sequence generation process \cite{chen2022more}, such as leveraging the \so API to limit the search space in the API sequence generation.}

\textbf{There exists more than one sequence to accommodate the developer's intention.} 
% \ft{do we have evidences for this? It should not be just based on our own argument.}
We need to find a way to objectively evaluate the API sequences generated by the Seq2Seq model from the perspective of usability, i.e., effectiveness and efficiency, i.e., length of API calls.
\textcolor{black}{
As discussed in Section~\ref{sec:result-analysis}, there exists more than one correct sequence to implement a functionality, as shown in Example 1.
There are also several functionalities that are known to have several implementations.
For example, official Java documentation page\cite{reading} published that there are at least 4 different ways to read a file in Java, each utilizing different API classes.
}
However, there are no suitable evaluation techniques to measure the effectiveness automatically other than manual evaluation.
One possible direction is to generate a dataset that contains test cases, so it could be used to judge whether the generated sequence could output a desirable result. It is a simple case though, since we cannot run the recommended API sequence directly to compare towards the test case.
A code generation method needs to be applied to craft a runnable code containing, transforming a sequence of API into a runnable snippet of code.

% \textbf{Class-level API recommendation may be beneficial in generating better API sequence.}\ft{Can we link this with our results?}
% \iv{Not really.. we don't have result for class level. is it better to remove it?}
% \textcolor{blue}{
% Even though method-level API recommendation is more suitable to accommodate usable API sequence generation, predicting relevant class for a given annotation is typically easier than predicting the API at the method level.
% For future work, the class retrieved in the class-level API recommendation could serve as a pointer to limit the search space.
% }
% \end{itemize}


\subsection{Threats to Validity}
\label{sec:threat}
% \begin{itemize}
% \item{\textbf{Threats to internal validity}}

Threats to internal validity are associated with aspects of our experiment. It includes the replication of the baselines and the evaluation strategy.
The risk of mentioned threats was minimized by utilizing curated code and dataset from previous works.
For the dataset, we derived the dataset used in our experiment from the deduplicated dataset that is provided by Martin and Guo~\cite{martin2022deep}.
Furthermore, we also derived the sentence embedding training from CLEAR's replication package\footnote{\url{https://github.com/Moshiii/CLEAR-replication}}.
As for the baselines, we used the same code base for CodeBERT\footnote{\url{https://github.com/microsoft/CodeBERT}}.
Therefore, the risk of the contaminated dataset, incorrect re-implementation, and unfair evaluation should have been mitigated.

% \item{\textbf{Threats to external validity}}

We consider the generalizability of our result as a threat to external validity.
In this study, due to the nature of the dataset from Martin and Guo~\cite{martin2022deep}, we only experiment with Java programming language and are limited to API that is present in the JDK library.
Moreover, we limited the API vocabulary to the APIs that are mentioned more than 5 times in the entire \so posts.

However, we believe that the impacts should be minimal %of the result of this study could be generalized to other programming languages 
as previous work on API recommendation \cite{deepapi}, \cite{wei2022clear}, \cite{huang2018api} also applied similar settings.
%Moreover, \so remains to be an active Q\&A platform that could serve developers of any programming language.
%All in all, the results of our experiment could serve as a positive direction for future works on leveraging external information toward API recommendation systems for other programming languages, such as Python.

% \end{itemize}

% Threats to internal validity relate to several aspects in our
% experiment, such as baseline implementation and evaluation
% strategy. We implemented DeepAPI and PAM with our dataset
% by utilizing the replication package made available by the
% authors of both papers. Thus, the risk of incorrect baseline
% implementation should be mitigated. Moreover, since DeepAPI
% outputs the full API sequence based on the given annotation,
% there is a risk of unfairness if we directly compare the
% generated API sequence with the target API. To mitigate this,
% we modified its beam search to introduce the information from
% context API when generating the API sequence. The same
% modification is also applied to CodeBERT-annotation. Hence,
% we believe the threats to internal validity should be minimal.
% Threats to external validity relate to the generalizability
% of our results. In this study, we only experiment with the
% API sequence generation task on Java programming language,
% raising a concern of whether the result of this study could be
% generalized towards other programming languages. However,
% we believe the impact of the programming language should
% not be significant because Java is one of the most used
% programming languages and the same setting has been used
% in the past API recommendation work [2], [3], [33]. Yet,
% the results of our experiment motivate us to evaluate our
% API recommendation system on other popular programming
% languages such as Python