\section{Experimental Setting}
\label{sec:setting}
% 1.5 pages
In this section, we elaborate on our experiment setup, including the dataset, baseline approach, and evaluation metrics.
Next, we present our experimental results that answer a few research questions.


\subsection{Dataset}
% \label{sec:dataset}
We use the dataset provided by Martin and Guo \cite{martin2022deep}, which is made of pairs of annotation and API sequences.
It is derived from the DeepAPI dataset \cite{deepapi} and was cleaned by removing duplicates.
The DeepAPI dataset contains more than 7 million pairs of annotation and API sequences for training and 10,000 pairs for testing.
However, Martin and Guo found that there are duplicates in the dataset.
There were also some pairs in the test that appear in the training.
After removing the duplicates, there are 1,880,472 training pairs and 2,441 test pairs remained.
As for the Stack Overflow dataset, we utilized the dataset used by CLEAR~\cite{wei2022clear}.
This dataset originated from BIKER~\cite{huang2018api}, containing Stack Overflow posts that are related to Java JDK programming topic, have a positive score, and have at least one accepted answer with API entities mentioned in it.

In this paper, we built our dataset by using the deduplicated DeepAPI dataset as the starting point.
We combined both 1,880,472 training pairs and 2,441 test pairs and further processed them as follows:
%To objectively evaluate the impact of \so in the API recommendation task, we deliberately derived a dataset that contains API that is mentioned in the \so vocabulary.
%The dataset-building process is described as follows: 
\begin{itemize}
\item{\textbf{\so API selection}}

In this phase, we gathered APIs mentioned in the \so posts answer and counted their frequency. Following that, we filtered out \so API that is mentioned less than five times in the entire \so posts. This ensures that the APIs appear at least 5 times, which should provide enough samples for the model to learn properly.
In total, we obtained 1,398 API methods that constitute the \textit{API vocabulary} in our dataset. 

\item{\textbf{Dataset filtering}} 

Based on the API vocabulary constructed in the previous step, we filtered out pairs of annotation and API sequences. We removed pairs whose target API contains API(s) that are not within the API vocabulary.
This action ensures that the pairs share the same vocabulary with the \so dataset.
Thus, we can evaluate whether considering multiple sources of information helps (or not) when both sources of information contain information about specific APIs.
\end{itemize}

In total, we collected 196,276 pairs of annotation and API sequences in our dataset.
We then split the dataset into the train, validation, and test sets with a ratio of 8:1:1.
All in all, we have 157,020 pairs in the training set, 19.628 pairs in the validation set, and 19,628 pairs in the test set.

\subsection{Baseline}
\label{sec:baseline}
% \textcolor{red}{
% There are several methods proposed for API recommendation leveraging \so posts, namely CLEAR~\cite{wei2022clear} and BIKER~\cite{huang2018api}. However, these approaches were built to generate an API set instead of API sequence. \method's approach which is designed to produce an API sequence recommendation cannot be compared fairly to the API set recommendation model.
% Therefore, we excluded these approaches from the baselines.}

Based on the previous work conducted by Martin and Guo~\cite{martin2022deep}, CodeBERT is the state-of-the-art approach for generating API sequences.
It surpassed DeepAPI~\cite{deepapi}, the former state-of-the-art in the domain of API sequence generation.
Therefore, we chose to adapt CodeBERT as our baseline model. All experiments were done with the same CodeBERT structure, which is adapted from CodeBERT project~\cite{feng2020codebert}.
%, with varying input based on each experiment in the observation.
CodeBERT is a pre-trained model built specifically for solving tasks related to programming languages.
It is trained on 6 different natural language and programming language pairs, resulting in a powerful pre-trained model whose embedding is proven useful for varying downstream tasks~\cite{zhou2021assessing, mashhadi2021applying}.
% We use the same parameter and CodeBERT setting across all of the experiments as set in the replication package.

\subsection{Hyperparameter Setting}
\label{sec:hyperparameter}
We used the same hyperparameters settings in all of our experiments.
We set the maximum token length of the input (i.e., annotation) and the target sequence to be 64.
For the CodeBERT model that involves multiple inputs (e.g., \so title and API), we also set the maximum token length of 64 for each input.
All models were trained for 30 epochs.


\subsection{Evaluation}
\label{sec:evaluation}
In order to measure the performance of API sequence generation, we employ BLEU score\cite{papineni2002bleu} as the selected metric.
We compare the BLEU score of \method and the baseline to identify which one is the better approach in generating API sequence recommendations.
BLEU-4 score is capable of gauging how accurate a sequence that is generated by a model is compared to the correct target sequence (i.e., ground truth).
As a metric that is widely adopted in machine translation problems, BLEU score is relevant to be used in comparing the automatically generated API sequence against the human-written API sequence and has been used in existing work in API sequence recommendation~\cite{deepapi, martin2022deep, luong2015effective}.

\noindent BLEU score is expressed mathematically as below:
\begin{equation}
\label{eqn:bleu}
    BLEU = BP \times exp\left(\sum_{n=1}^{N}{w_n} log \left({p_n}\right)\right)
\end{equation}

% \usepackage{amssymb}
\begin{equation}
\label{eqn:bleu-bp}
    BP =
    \begin{cases}
      1 & $$ c \geqslant r $$ \\
      exp\left(1-\frac{r}{c}\right) & $$ c < r $$\\
    \end{cases}  
\end{equation}

\noindent In Equation~\ref{eqn:bleu}, ${BP}$ refers to the brevity penalty. This variable aims to give a penalty to the generated sequence that is shorter than the ground truth. 
${r}$ and ${c}$ are correlated to the number of tokens in ground truth and candidate, respectively. 
% We first compute the geometric average of the modified n-gram precisions, pn, using n-grams up to  length N and positive weights wn summing to one.
Furthermore, ${p_n}$ is the \textcolor{black}{the modified precision for n-gram, ${w_n}$ are the weight and $\sum_{n=1}^{N}{w_n}=1$.}
 
% In general, the BLEU score ranges from 0-1. The closer the BLEU score to 1, the better the generated APIs are.
\textcolor{black}{For the BLEU score, the higher it is, the better the result.}
In this work, we measure the performance of \method on the test data with cumulative n-gram BLEU score with n=4.
The cumulative score in the BLEU scores formula correlates to the individual n-gram scores at all orders from 1 to n. It then used the pre-defined weight to calculate the weighted geometric mean to obtain the final BLEU-n score.

% \textcolor{red}{Other common evaluation metrics for API recommendation techniques are precision and recall. However, they are more suitable to evaluate API set recommendation rather than API sequence recommendation.
% Therefore, we use BLEU-4 score as the main metric in this work.}

% \ft{These sound like we are not computing precision and recall. We may need to say why we measure precision and recall. Maybe we can say that they can be complementary metrics that show whether a better approach in sequence generation also recommend more APIs or mostly generate the same APIs in a better order. Maybe we should also define the precision and recall in a formula as our setting is not as common.}
% \iv{As we moved the precision and recall to discussion, we don't need to put their formula here right?}\ft{Yes, we can talk about precision and recall there. Probably also no need to mention other metrics here.}

% For 2-gram BLEU score, i.e., BLUE-2, we set the weight to [1/2, 1/2], while for the BLUE-3 the weight should be set to [1/3, 1/3, 1/3].
% Finally, we set the weight to be [1/4, 1/4, 1/4, 1/4] while calculating the BLEU-4 score.

\subsection{Research Questions}
\label{sec: rq}
In this work, we would like to investigate the following Research Questions (RQs):
\begin{itemize}
    \item{\textbf{RQ1:} \textit{Can we improve the performance of API sequence recommendation by leveraging information from \so for query expansion?}}
    
    While incorporating more information tends to generate a better result, it may not always be the case.
    In order to answer this RQ, we compare our baseline model with a model that is built with additional information from \so.
    We take CodeBERT with annotation input as the baseline and then experimented with \so title and \so API usage.
    % \iv{Need to change this part. Should we mentioned about thresholding here? But it may weaken our finding.}    
    % There are 2 hypothesis that are being explored to answer this question.
    % The first one is that introducing \so information to every data point will improve the performance, and the second hypothesis is that \so post need to be selected carefully for each data point for it to positively affect the API sequence generation's performance.
    The hypothesis for this question is that if we could find a semantically similar \so post for an annotation, information stored in the \so post would be imperative for the API sequence generation.
    % If an annotation cannot find a highly similar \so post, it might be better to drop the \so information in such case, and opt to use only annotation to generate the API sequence recommendation.
    %To assess the performance of our hypothesis, we treat all training data and test data with the same treatment.
    % \ft{the red part seems more appropriate to explain in the approach section.}
    
    In this work, we performed training on 2 types of data. One is where we only use annotation as the query to predict API sequence (i.e., the CodeBERT baseline), and the other one is where we incorporate \so posts information such as title and APIs as  query expansion (i.e., \method).
    Both models are then evaluated by comparing the BLEU score of the generated API sequence on the test set.    
    
    \item{\textbf{RQ2:} \textit{How should we utilize the information stored in \so post to improve API sequence generation performance?}}
    
    To answer this research question, we dig into the two types of \so information available in our \so dataset.
    Two components are being observed, namely the \so title and \so API.
    Note that we deliberately omitted \so body from this work due to the lengthy nature of the \so body.
    %Nonetheless, it might also serve as one of the directions of future works to work with \so body.
    For the \so API, we utilize the extracted APIs mentioned in the accepted answer and treat them as related API tokens for a \so natural language (i.e., \so title).
    We explore the impact of these two components on the API sequence generation's performance.
    Our hypothesis for this question is that using more information from \so will accommodate a better API sequence generation.
    
\end{itemize}

% Will stack overflow data help the performance of API recommendations?


% Is separation of data improving the result? (high relevancy vs all data)
% → SO can help only on data that don’t have similar SO Post

