\section{Approach}
\label{sec:approach}
% 2 pages

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{image/overall-architecture.png}
    \caption{\method high-level architecture}
    \label{fig:overall-architecture}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=0.95\textwidth]{image/model.png}
    \caption{\method detailed architecture}
    \label{fig:model}
\end{figure*}

In this section, we introduce the overall architecture of \method and discuss the details of each component.
% The overall architecture for the training phase is presented in Figure~\ref{fig:architecture}.
\textcolor{black}{The overall architecture is presented in Figure~\ref{fig:overall-architecture}.}

\textcolor{black}{In general, %both the training and inferring phases in 
\method consist of two phases: 1) \textbf{Finding Similar SO Post}. 
% % \kisub{Shall we clarify if both the training and inferring phases consist of two phases each?}
% \kisub{Generally, we may want to explicitly draw/split the training and inferring phases in our overview or detailed figure.}
This phase is used to find \so post that is the most semantically similar to the input annotation (i.e., code comment) and 
2) \textbf{API sequence generation}.
This takes annotation and the \so information retrieved from the first phase to generate an API sequence.}

% \textcolor{black}{These two phases is used in both training and deployment.
% Finding similar \so Post can be regarded as the data pre-processing step before feeding the input into the model for API sequence generation.}
% There are two steps involved in the training phase.
% The first phase of the training is done in order to find a \so post that is semantically similar to an annotation.
\textcolor{black}{For the Finding Similar SO Post phase, we adapt CLEAR's~\cite{wei2022clear} framework, which has been modified to leverage the sentence embedding technique for finding  relevant posts.} 
While CLEAR leverages the sentence embeddings to map a \so post towards other similar \so posts, we utilize sentence embedding to map an annotation towards similar \so posts.
% \kisub{Probably, mentioning too many \so is just the same as mentioning CLEAR; we may want to use `post or posts' instead.}
% \kisub{What about this? CLEAR leverages the sentence embedding to map ... . Different from that, we utilize ... .}
Therefore, we leverage the usage of contrastive learning and joint embedding training while implementing our adaptation of the triplet data generation.
% \kisub{This allows us to leverage the usage of ...}
% \kisub{This sentence seems like coming out of nowhere. I guess this belongs to the previous paragraph.}
For the second phase, we leverage CodeBERT as the base encoder for the encoder-decoder architecture since it is state-of-the-art for API sequence generation technique~\cite{martin2022deep}.
Further explanation is given in Section~\ref{sec:apigeneration}.


\textcolor{black}{
The detailed architecture of \method is presented in Figure~\ref{fig:model}. It shows the choices of the model used in every component of \method, as well as the data flow.
}

% \kisub{This sentence seems it belongs to the first paragraph (i.e., starting with In general...)}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{image/clear.png}
    \caption{Training process for finding the most similar \so post}
    \label{fig:clear}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{image/relevant-post.png}
    \caption{Finding relevant \so post based on API in training data and API mentioned in the SO post's answer}
    \label{fig:rel-post}
\end{figure}


\subsection{Finding Similar SO Post}

% \kisub{Data preparation by finding the most similar \so post?}
\label{sec:similarso}
% To find the most similar \so post, we leverage the steps introduced in a recent approach named CLEAR~\cite{wei2022clear}.
% \kisub{Probably, starting with the reason why we need the most similar \so post. Starting like.. ``As it is proved that similar posts ... (impacts) .., we similarly take it to boost the generation performance..''}
To find the most similar \so post, we leverage contrastive learning~\cite{oord2018representation} and joint embedding training~\cite{devlin2018bert}.
% In their study, they aim to find similar \so post given a query (i.e., \so question).
% We adopt CLEAR'S contrastive and joint embedding training to find similar \so posts, given an annotation. %However, in our study, we want to find similar \so post for a given annotation rather than \so question.
\textcolor{black}{The same approach has been used in CLEAR\cite{wei2022clear}, with a difference in domain of the data.
The objective of CLEAR is to find the most relevant \so posts for a given \so post question.
In other words, they aim to build a better sentence representation for two natural language texts from the same domain.}
On the contrary, in this work, we aim to train a sentence embedding model that could place an annotation derived from GitHub's project towards \so post, which comes from different domains.
% \kisub{These are proper for the first sentence of the paragraph as it motivates, but we need to rephrase it.}

% \kisub{Suggestion for the whole paragraph: A state-of-the-art technique~\cite{wei2022clear} aims to build a better sentence representation for two natural language texts from the same domain, and it shows significance. To leverage such an impact, we adopt the same technique to train a sentence embedding model that could place an annotation derived from GitHub's projects towards \so posts. We employ contrastive learning~\cite{oord2018representation} and joint embedding training~\cite{devlin2018bert}.}

Although both GitHub and \so are from the software engineering domain, the nature of the content is essentially different.
For example, the nature of the natural language (NL) from \so comes in the form of a question, oftentimes related to an error rather than a technical functionality of an API.
Moreover, the APIs mentioned in the answer posts are not necessarily a sequence.
% \kisub{Shall we move this sentence after the next sentence so that it can connect the differences? We can say like ``Please note that the APIs mentioned in an answer post are not necessarily a sequence while those of GitHub are mostly formed as a sequence.''}
On the other hand, annotation in GitHub data portrays the functionality of a task in the form of a statement rather than a question.
Moreover, the APIs extracted from GitHub are presented in the form of a sequence.
\textcolor{black}{We believe that adapting contrastive learning and joint embedding training in our approach as an attempt to bring similar annotation and \so post together in the sentence embedding is advantageous.
Sequentially, the most similar post retrieved in this phase will be used for query expansion in the API sequence generation phase.}

The architecture of the framework to find the most relevant post is presented in Figure~\ref{fig:clear}.
\textcolor{black}{The filtering and re-ranking model produced in this step will be utilized to find the most semantically similar post given an annotation.
Finding a semantically relevant post is imperative, as we utilize such a post as query expansion that will directly affect the performance of our API generation model.}
The details for each process are explained as below:

% The positive title is obtained from the post deemed as the most relevant to an annotation.

 \textcolor{black}{\textbf{Triplet Data Generation:}
 % \ft{Have we defined the triplets anywhere? Here seems to be a good place.}:
In order to train both the filtering model via contrastive learning and the re-ranking model via joint embedding, we need to prepare the training and validation data that are built from the training set of our dataset.
The training process takes triplet data as input containing an annotation, a positive, and a negative \so title.
To accommodate the triplet data generation, we need to find highly similar annotations and \so pair to be used as positive pair.
Following the motivation in Section~\ref{sec:motivating-example}, we determined the relevancy of a post and annotation based on the \textcolor{black}{{\em overlap rate}}
% ft{is this a correct term? We call it recall below.} 
between the APIs appearing in the API sequence from GitHub (which corresponds to the annotation) and the APIs mentioned in the \so answer.}
Figure~\ref{fig:rel-post} shows an example of similar posts, given an annotation and its corresponding API sequence.
We first convert the API sequence in the training datum into a set of APIs, which we refer to as \textit{training API set}.
Towards this direction, we calculate the overlap rate of the APIs mentioned in the answer post (i.e., the proportion of APIs in the training API set).
In the example, the training datum is considered similar to the SO Post 1\footnote{https://stackoverflow.com/questions/22016489/parse-string-containing-a-number-into-a-int} because 3 out of 4 in the training API set are mentioned in the answer post.
Therefore, SO Post 1 is considered a positive training post for contrastive learning.
On the other hand, SO Post 3\footnote{https://stackoverflow.com/questions/8706070/get-boolean-from-a-jtextfield} is considered a negative post as it does not contain APIs from the API training set.
For this work, we use $0.75$ as the threshold for \textcolor{black}{the overlap rate as we consider this threshold captures high-quality data points while ensuring we have enough data points to train the Bi-Encoder (i.e., via contrastive learning) and Cross-Encoder (i.e., via joint-embedding training).}

Due to this threshold, for SO Post 2\footnote{https://stackoverflow.com/questions/26108503/java-do-while-loops-not-looping}, even though it has one matching API, it is not counted as the positive example for the training datum since the overlap rate is only 0.25, which is less than the $0.75$ threshold.


% \kisub{This sentence is too long, let's split it}
% \kisub{Suggestion: Although SO Post 2\footnote{https://stackoverflow.com/questions/26108503/java-do-while-loops-not-looping} has one matching API, it is not guaranteed to be counted as the positive example for the training datum according to the threshold.}

Once we discover all of the positive posts for each training datum, we randomly choose five posts with a really small \textcolor{black}{overlap rate} (i.e., less than $0.01$) and use them as negative posts.
Pairs of positive and negative posts are then formed into triplets to be used in contrastive learning.
Note that some training data are naturally discarded because they do not have a positive \so post with an overlap rate above the threshold.
Following a prior study~\cite{wei2022clear}, we used two parameters, namely \textit{p} and \textit{n} to build the triplets, where \textit{p} refers to the number of positive examples and \textit{n} refers to the number of negative examples. 
We produced \textit{p} x \textit{n} numbers of triplets for each annotation \textit{A}.
Based on their grid search experiment, using \textit{p}=10 and \textit{n}=10 performs the best.
Following this, ideally, each annotation should produce 100 triplets for contrastive learning. However, this is not the case for every annotation.
If \textit{A} only has three positive posts to be paired with, it will only have 30 triplets in the end.
If \textit{A} has more than ten positive posts, we randomly choose ten of them to generate 100 triplets for annotation \textit{A}.

\textbf{Contrastive Learning}: 
% \kisub{We need to clarify whether we leverage contrastive learning or we adopt CLEAR, which leverages contrastive learning.}
\textcolor{black}{
We leverage contrastive learning~\cite{oord2018representation} as the first step to learn the semantic relationship between annotation and \so post.
The contrastive training is done by fine-tuning a RoBERTa~\cite{liu2019roberta} based Cross-Encoder~\cite{reimers2019sentence} model to learn a better semantic embedding representation. 
Even if an annotation and \so posts do not share mutual tokens lexically, the sentence embedding should be able to put the annotation and related \so posts in a relatively close vector space if they share the same topic semantically.
The learning goal is to obtain a representative sentence embedding that could serve as a filtering model. 
It should be able to effectively fetch the top-n most similar \so posts by fetching \so posts whose embedding has the highest cosine similarity score with the annotation embedding.
%The filtering model is imperative in finding the most similar \so post because it will limit the search space.
%The top-n most similar post will be passed to the Bi-Encoder model produced in the next step and it will re-rank only these top-n posts to determine the top-1 most similar post that will be used as our dataset.
}

% In total, we obtained \iv{fill the number} triplets for the contrastive learning.

\textbf{Joint Embedding Training}: To train a BERT as a classification model, joint embedding training \cite{devlin2018bert} is commonly used in practice.
In this work, we employ a RoBERTa-based Cross-Encoder\footnote{https://huggingface.co/cross-encoder} model for the classification task.
The purpose of the classification is to re-rank the filtered posts returned in the previous step.
The training data for classification is obtained from the triplet generated for contrastive learning.
We transform the triplets to make them compatible with the classification task.
For each triplet, we assigned the annotation and its positive post with label {\tt 1} and we assigned label {\tt 0} for the negative post pair.
% There are \iv{fill number} of data point used in the joint embedding training process.
The result of this joint embedding training is a classification model that can be used to perform a re-ranking for the filtered post.
We leverage the probability score as a measure to determine the rank of the relevant post and take the top-1 post to be used in the API sequence generation.

\textcolor{black}{
Finally, after the contrastive learning and joint embedding training are done, the models produced by these learning processes will be used to pair an annotation and its target APIs with the most relevant \so post. Using the trained cross-encoder model, we produce embeddings for the annotation and \so posts' titles. We take the top-10 most relevant \so posts in terms of their cosine similarity between the annotation and \so title embedding. We then passed the top 10 posts to the bi-encoder model to be re-ranked based on the probability score. We regard the top-1 post after the re-ranking process as the most semantically relevant post for the given annotation.}

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{image/boxplot.pdf}
%     \caption{Training data's similarity score distribution}
%     \label{fig:boxplot}
% \end{figure}

\subsection{API Sequence Generation}
\label{sec:apigeneration}
\method leverages Cross-Encoder and Bi-Encoder models trained in the previous phase to build the training data used in API sequence generation.
% The Cross-Encoder model is used as filtering model while obtaining the most similar \so post for the training data, and the Bi-Encoder model is used to re-rank the similar \so post to fetch the top-1 post.
The training data is then used to fine-tune the CodeBERT encoder-decoder model to generate an API sequence.
% Detailed architecture for \method is illustrated in Figure~\ref{fig:model}.\ft{This figure may be more suitable to be used as the overall architecture in the beginning of the section}
Before the API sequence generation training phase, we leveraged the trained Cross-Encoder and Bi-Encoder to obtain the \so post that is most similar to the annotation.
Given the pair of an annotation and its most similar \so post, we extract the title and \so API from the post and build a dataset that consists of \textit{annotation, \so title}, \textit{\so API set}, and \textit{target API sequence}.
To generate the API sequence, we adopt the encoder-decoder framework.

\textbf{Encoder:} The encoder is used to map the input sequence into a continuous vector representation.
We encode the annotation and the \so components (i.e., \so title and API) with a Transformer-based encoder separately.
We initialize the parameters of each encoder with the pre-trained CodeBERT model.
Secondly, we concatenate the individual encoded vectors (i.e., annotation vector, SO title vector, and SO API vector) into a single vector referred to as the feature vector.
Last, the feature vector is passed through a Transformer decoder with 6 layers, where the parameters are randomly initialized.

\textbf{Decoder:} The decoder is used to generate the API sequence. 
The decoding starts with a start token.
At each position, the decoder predicts the target token by taking the encoder output and previously generated output token list as input.
% \ft{where are these outputs coming from?} 
% \zt{I updated the problem formulation as well. these outputs are the generated tokens from the input sequence, it refers to the subsequence in the final generated sequence.} 
The decoder stops decoding when it generates the end token.

\textbf{Beam Search.} During the decoding stage, we aim to generate an accurate API sequence.
However, it is hard to guarantee the correctness of each subtoken at each prediction position.
If the model makes a wrong prediction at a certain position, it will affect the following predictions.
Thus, instead of considering only a single subtoken every time, one can consider all the subtokens in each position.
The obvious drawback is the explosion of all the combinations.
% each subtoken is assigned a probability in each prediction.
To achieve the balance between effectiveness and efficiency, we adopt the widely-used beam search~\cite{freitag-al-onaizan-2017-beam,shu-nakayama-2018-improving} to search for the recommended API sequence heuristically.
Simply put, in each prediction, beam search considers the top-k subtokens (k refers to the size of the beam) based on conditional probability.
% In each prediction position, the beam search considers the Top-K subtokens with the highest probability.
% Then for each of these Top-K subtokens would produce another Top-K subtokens.
% The Top-K chain with the hightest probability would be kept.
Following the existing works~\cite{freitag-al-onaizan-2017-beam,kang2021apirecx}, we adopt the Equation~\ref{equ:cond_prob} to calculate the chain probability of a chain of the subtokens: 
% Given the input of $\{w_1^1, \ldots, w_1^{n_1}, w_{m-1}^1, \ldots, w_{m-1}^{n_{m-1}}, s_m^1, . ., s_m^{j-1}\}$

\begin{equation}
    \label{equ:cond_prob}
    P\left(s_m^1, \ldots, s_m^i \mid w_1^1, \ldots, w_1^{n_1}, \ldots, w_{m-1}^{n_{m-1}}\right)=\prod_{j=1}^i p\left(s_m^j\right)
\end{equation}

\noindent where $p(s_m^j)$ refers to $p(s_m^j \mid w_1^1, \ldots, w_1^{n_1}, \ldots, w_{m-1}^{n_{m-1}}, s_m^1, \ldots, s_m^{j-1})$).
It represents the probability of the $j^{th}$ subtoken in the chain of $(s_m^1, \ldots, s_m^i)$.



% In this work, we build an ensemble model to better accommodate the characteristic of the data.
% The training and test data are passed into the Bi-Encoder, and Cross-Encoder to get the most similar \so post for each annotation.
% Based on the similarity score of the top-1 most similar \so post, we train 2 types of model in \method.

% \begin{table*}[t]
% \label{table:grouping}
% \caption{Example of data grouping}
% \centering
% % \resizebox{\textwidth}{!}{
% % \begin{tabular}{|l|l|l|l|l|l|}
% \begin{tabularx}{\textwidth}[t]{|X|p{3cm}|p{3cm}|p{3cm}|X|p{1cm}|}
% \hline
% Annotation  & API Sequence & Top-1 Most Similar SO Post  & APIs mentioned in the SO answer                & Common API & Type      \\ 
% \hline
% return the number of days between now and the passed date & System.currentTimeMillis, Date.getTime & Android/Java Time difference ISO 8601 with Now & System.currentTimeMillis, Date.getTime         & Yes (all API match)     & Match     \\ 
% \hline
% attempt to return the was application name based on a given class loader & String.startsWith, String.split, String.indexOf, String.substring, String.indexOf, String.substring & How do I split strings in J2ME?                & String.split, String.indexOf, String.substring & Yes (partial API match) & Related   \\ 
% \hline
% get an array of urls representing all of the files of a given set of extensions in the given location &   File.isDirectory, File.getPath, String.toLowerCase, File.toUri, FilenameFilter.init, File.listFiles  &                                         Java Buffered Writer to publically accessible location       &                             
% File.createTempFile, File.deleteOnExit& No                         & Unrelated \\ 
% \hline
% \end{tabularx}
% % \end{tabular}}
% \end{table*}

% \kisub{Shall we clarify whether we change the threshold in the middle or not? it can confuse the readers :) }
% As mentioned earlier in section~\ref{sec:similarso}, we reckon that not every annotation has a relevant \so to be paired with.
% Even when a \so post is considered the top-1 most relevant post for annotation, in many cases, they don't share any common APIs.
% Therefore, we conducted a further investigation to determine the best threshold to use in the framework.
% First, we divided the training data into three groups based on whether the top-1 most similar \so posts retrieved by the Cross-Encoder and Bi-Encoder model share a common API with the API sequence in the training data.
% An example for each group is presented in Table~\ref{table:grouping}.
% Second, after separating all of the training data into three groups, we observe the similarity score distribution to pick a number as a threshold.
% In total, there are 5,269 data with exact API matches, 146,529 data with partial matches, and 1.6 mio\kisub{typo?} data that was not related based on the APIs.
% The distribution of the similarity score is shown in Figure~\ref{fig:boxplot}.
% All three classes are normally distributed, and based on the figure, it is clear that data that contains exact APIs have the highest median in terms of similarity score.
% This implies that the higher the similarity score between annotation and the \so title, the higher chance that it is coming from the \textit{Related} or \textit{Match} distribution.
% On the other hand, a lower similarity score indicates that an annotation is unlikely to be semantically similar to a \so post.
% In this work, we set the threshold to be 0.7 as it is the value of the third quartile of the \textit{Unrelated} group, as indicated in Figure~\ref{fig:boxplot}.

% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{image/orthrus-model.png}
%     \caption{API Sequence generation model}
%     \label{fig:model}
% \end{figure}

% Third, we fine-tuned Code-BERT based seq2seq model to perform the API sequence generation task.
% \iv{explain more about codebert encoder and decoder here}

% There are two types of model used by Orthrus, indicated by the input of the model.
% The first one is model that only accept annnotation as the input. This model is trained with data whose most similar \so post's similarity score is below 0.7.
% The other model is the seq2seq model trained with annotation and \so data input. 
% The architecture for the sequence generation model is presented in Figure\ref{fig:model}.
% Note that this is the architecture that leverages query expansion with \so title.
% For the model that was meant for input with annotation only without query expansion, there would be no SO Title as an input.
% In this work, we experimented with \so title and API that is mentioned in the \so answer, referred as SO API.
