\section{Result}
\label{sec:result}
% 1 pages

In this section, we show the experimental results and answer the RQs.


\subsection{Answer to RQ1: }
\begin{table}[t]
    \caption {BLEU scores of \method compared to the baseline}
    \label{tab:res-rq1} 
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{@{}|l|l|l|l|l|@{}}
        \hline  
        \textbf{Approach} & \textbf{BLEU-1} &
        \textbf{BLEU-2} &
        \textbf{BLEU-3} &
        \textbf{BLEU-4} \\
        \hline
        \textbf{CodeBERT} & 0.49015 & 0.39946 & 0.33005 & 0.26296 \\
        \hline
        \textbf{\method} & \textbf{0.51371} & \textbf{0.42767} & \textbf{0.35959} & \textbf{0.29131} \\
        \hline
    \end{tabular}
\end{table}


As shown in Table \ref{tab:res-rq1}, in general, \method managed to outperform the vanilla CodeBERT by 10.8\% in terms of BLEU-4 score.
% In measuring the model performance, all of the 19,628 test data are passed into a CodeBERT model that was fine-tuned with pairs of annotation and target API sequences.
% On the other hand, \method passed 19,628 test data into a CodeBERT model that was fine-tuned with \so title and \so API as a measure to expand the query.
This shows that utilizing \so posts as query expansion that introduces additional information give better results than using only annotation in generating API sequence recommendation. The improvement is also consistent across the different BLEU scores.

\textcolor{black}{We also performed Mann-Whitney U hypothesis testing and got p-value less than 0.01, indicating that \method is statistically significantly better than the baseline.}
% \kisub{\method is significantly superior to the baseline from a statistical perspective?}
% \iv{I'll keep using the statistically significant term here, as it's commony used.}


% \textcolor{red}{
% We provide the calculation of precision and recall for \method compared to the baseline for secondary evaluation in Table \ref{tab:rq1-precision-recall}. Note that these metrics are provided as secondary metrics, as they could not measure the sequence generation performance of the API sequence generation methods.
% The precision and recall also suggest that utilizing information from \so post is beneficial in recommending the correct set of APIs.}\ft{We can also say that precision and recall also add additional insights, e.g., \method also recommend a larger number of correct APIs. We can also mention about the statistical test. }

%However, we learned that leveraging only \so title as query expansion has its limitation.

% If the \so title being is not highly similar with the annotation, it may decrease the quality of the API sequence generated instead of improving it.

% This statement is derived from the experiment results shown in Table \ref{tab:res-rq1-2}, as it presents the BLEU scores of the model when we perform query expansion with both \so title and \so API give better performance than using solely \so title.
% to all of the data points in both training and testing, regardless of the similarity score between annotation and \so title.
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxrule=0.2mm]
\textbf{Answer to RQ1:} \method achieves the highest BLEU-4 score of 0.29131, which outperforms the baselines by CodeBERT by 10.8\%.
\end{tcolorbox}


% \iv{using \so directly as it is decrease the performance.
% however, with right handling it will improve the performance}

\begin{table}[t]
    \caption {BLEU scores of different inputs}
    \label{tab:res-rq1-2} 
    \centering
    % \renewcommand{\arraystretch}{1.2}
    % \begin{tabular}{@{}|l|l|l|l|l|@{}}
    \begin{tabularx}{\linewidth}[t]{|X|X|X|X|X|}
        \hline
        \textbf{Input} & \textbf{BLEU-1} &  \textbf{BLEU-2} & \textbf{BLEU-3} &  \textbf{BLEU-4} \\
        \hline
        \textbf{annotation (baseline)} &  0.49015 & 0.39946 & 0.33005 & 0.26296\\
        \hline
        \textbf{annotation + SO title} & 0.50949 & 0.42230 & 0.35300 & 0.28507 \\
        \hline        
        \textbf{annotation + SO title + SO API} & \textbf{0.51371} & \textbf{0.42767} & \textbf{0.35959} & \textbf{0.29131}\\
        \hline
        % \textbf{\method} & \textbf{0.66} & \textbf{0.62} & \textbf{0.58} & \textbf{0.36} \\
        % \hline
    % \end{tabular}
    \end{tabularx}
\end{table}



\subsection{Answer to RQ2: }
% \ft{I'm not sure if we should also compute precision and recall here so that we consistently use the metrics}
% \begin{table}[t]
%     \caption {BLEU scores of each model in the ensemble}
%     \label{tab:res-rq2} 
%     \centering
%     % \renewcommand{\arraystretch}{1.2}
%     \begin{tabularx}{\linewidth}[t]{|X|X|X|X|X|X|}
%         \hline
%         \textbf{Input} & \textbf{\#of test data} & \textbf{BLEU-1} &
%         \textbf{BLEU-2} &
%         \textbf{BLEU-3} &
%         \textbf{BLEU-4} \\
%         \hline
%         \textbf{annotation} & 1799 & 0.38209 & 0.30510 & 0.24943 & 0.20440 \\
%         \hline
%         \textbf{annotation + SO title}
%         & 642 & \textbf{0.43064} & \textbf{0.35082} & \textbf{0.28833} & \textbf{0.24028} \\
%         \hline
%         \textbf{annotation + SO title + SO API} & 642 & 
% {0.41936} & {0.34072} & {0.27583} & {0.22866} \\
%         \hline
%     \end{tabularx}
% \end{table}

%To answer the question of how to utilize the information provided in \so, 
We performed an ablation study to measure the impact of leveraging \so title and \so APIs for query expansion.
% \iv{show table of using SO testing data in annotation model give worse performance}
Table~\ref{tab:res-rq1-2} displays the BLEU scores for each model that is built with different \so input(s) as the query.
It shows that leveraging \so information as query expansion can achieve a BLEU-4 score of up to 0.29131.
The highest BLEU-4 score is achieved by utilizing both \so title and \so API in the query.
Nonetheless, adding only \so title already improved the BLEU-4 score from 0.263 to 0.285, which translates to over 8\% improvement.
This shows that \so information brings a noticeable improvement when it is incorporated as query expansion.
% To check the impact of \so tokens over the test data, we check the performance of this test data without query expansion.
% Our experiment shows that the BLEU-4 score for these data without query expansion is 0.21308.
% This means adding \so data brings  12.76\% improvement to these test data.

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,boxrule=0.2mm]
\textbf{Answer to RQ2:} Using \so title and \so API brings more improvement towards the API sequence generation technique compared to using only the \so title.
\end{tcolorbox}