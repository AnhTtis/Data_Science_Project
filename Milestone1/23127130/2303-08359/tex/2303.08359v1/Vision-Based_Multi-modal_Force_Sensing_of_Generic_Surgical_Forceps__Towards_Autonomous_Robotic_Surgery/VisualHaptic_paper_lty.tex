\documentclass[journal]{IEEEtran}

\usepackage[colorlinks=true,
allcolors=green]{hyperref}
\usepackage{url}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage[noadjust]{cite}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{indentfirst}
\usepackage{algpseudocode}
\usepackage{algorithmicx,algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{makecell}
\usepackage{hyperref}
%\hypersetup{draft}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\pdfoptionpdfminorversion=6
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
	
	
	%
	% paper title
	% \title{Vision-based Multi-modal Force Sensing of Forceps: Towards Task-autonomous Robotic Surgery}
 \title{\huge Haptics-Enabled Forceps with Multi-Modal Force Sensing: Towards Task-Autonomous Robotic Surgery}
	%
	\author{Tangyou Liu, Tinghua Zhang, Jay Katupitiya, Jiaole Wang$^{\ast}$, \textit{Member, IEEE} and Liao Wu$^{\ast}$, \textit{Member, IEEE}
		\thanks{Research was partly supported by Australian Research
Council under Grant DP210100879 and Heart Foundation under Vanguard Grant 106988 awarded to Liao Wu, partly by the Science and Technology Innovation Committee of Shenzhen under Grant JCYJ20220818102408018 awarded to Jiaole Wang, and partly by the Tyree IHealthE PhD Top-Up Scholarship awarded to Tangyou Liu. $^{\ast}$Corresponding authors: Jiaole Wang (\textit{wangjiaole@hit.edu.cn}) and Liao Wu (\textit{liao.wu@unsw.edu.au}).}
		\thanks{Tangyou Liu, Jay Katupitiya, and Liao Wu are with the School of Mechanical \& Manufacturing Engineering, The University of New South Wales, Sydney, NSW 2052, Australia.}
		\thanks{Tinghua Zhang and Jiaole Wang are with the School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, 518055, China.}
	}
	
	\definecolor{d_c}{RGB}{0,0,250}
	% The paper headers
	%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
	%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
	%% If you want to put a publisher's ID mark on the page you can do it like
	%% this:
	%%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
	%% Remember, if you use this you must call \IEEEpubidadjcol in the second
	%% column for its text to clear the IEEEpubid mark.
	%% use for special paper notices
	%%\IEEEspecialpapernotice{(Invited Paper)}
	
	
	% make the title area
	\maketitle
	
	\begin{abstract}
		%Despite the last centuries have witnessed a great progression toward minimally invasive and robot-assisted surgery, 
		% One of the major shortcomings of 
        Current minimally invasive surgical robots are lacking in force sensing that is robust to temperature and electromagnetic variation while being compatible with micro-sized instruments.
		This paper presents a multi-axis force sensing module that can be integrated with micro-sized surgical instruments such as biopsy forceps. 
		The proposed miniature sensing module mainly consists of a flexure, a camera, and a target.
		The deformation of the flexure is obtained by the pose variation of the top-mounted target, which is estimated by the camera with a proposed pose estimation algorithm. 
  %for registering and estimating the marker's pose.
		Then, the external force is estimated using the flexure's displacement and stiffness matrix.
  %by multiplying the flexure's displacement with its stiffness matrix.
        Integrating the sensing module, we further develop a pair of haptics-enabled forceps and realize its multi-modal force sensing, including touching, grasping, and pulling when the forceps manipulate tissues.
        % A pair of haptics-enabled forceps is further proposed by integrating the sensing module, and then its multi-modal force sensing is realized, including touching, grasping, and pulling when the forceps manipulate tissues.
		% The proposed module is further integrated into a pair of forceps to enable multi-modal haptic sensing, 
		To minimize the unexpected sliding between the forceps' clips and the tissue, we design a micro-level actuator to drive the forceps and compensate for the motion introduced by the flexure's deformation.
		Finally, a series of experiments are conducted to verify the feasibility of the proposed sensing module and forceps, including an automatic robotic grasping procedure on ex-vivo tissues.
		The results indicate the sensing module can estimate external forces accurately, and the haptics-enabled forceps can potentially realize multi-modal force sensing for task-autonomous robotic surgery.
        A video demonstrating the experiments can be found at \href{https://youtu.be/4UUTT_hiFcI}{https://youtu.be/4UUTT\_hiFcI}.
		
		%To estimated whether the instrument kinematic will be significantly impacted by the integrated sensor, an evaluation of the estimated pose is carried out by a comparison with electromagnetic tracking system.
		%The grasping and pulling forces estimations are validated through experiments with different grasping angle configurations by a pressure sensor and a force sensor.
		%To mimic the related displacement between the tissue and clips and ensure the grasping quality, a micro-level driving module is designed to compensate the motion introduced by the flex spring.
		%Finally, the developed forceps is installed to a universal robotic arm.
		%And a series of robotic experiments are carried out to verify the feasibility of the proposed system, including an automatic tissue grasping process.   
	\end{abstract}
	
	
	% Note that keywords are not normally used for peerreview papers.
	\begin{IEEEkeywords}
		Surgical robotics, autonomous surgery, vision-based force sensing, tissue manipulation.
	\end{IEEEkeywords}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%                introduction                     %
	%												  %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Introduction}
	\label{Section:introduction}
	
	\IEEEPARstart{A}{lthough} the last decades have seen many achievements in robotic surgery \cite{dupont2021decade,dupont2022continuum}, most minimally invasive surgery (MIS) systems have no haptic sensing at the tool side currently.
	For fully teleoperated systems, experienced surgeons can control the system and make decisions purely relying on visual feedback and their rich experiences \cite{wu2022camera}.
        However, haptic sensing can enhance surgeons' operation and decision-making if integrated into the systems appropriately \cite{patel2022haptic}.	
 %However, procedures can be improved if haptic sensing is provided, as it can help the surgeon with both operation and decision-making \cite{patel2022haptic}.
	For example, Talasaz \textit{et al}. \cite{talasaz2016role} verified that a haptics-enabled teleoperation system could perform better during a robot-assisted suturing task.
	Moreover, an increasing number of haptics-based augmented realities have been applied to help surgeons build situation awareness and make decisions recently \cite{qian2019review}.
	
	There is a consensus on the growth of autonomy in surgical robots \cite{yang2017medical} accompanied with more dexterous mechanisms such as snake-like robots \cite{razjigaev2022end} and elastic-tube robots \cite{wang2021eccentric}.
	As a key source of information, force sensing at the tool side will be critically needed along with the evolution of autonomous robotic surgery \cite{yang2017medical,attanasio2021autonomy}.
	In the last few years, achievements have been made on force sensing in single-task procedures such as palpation \cite{shihora2021feasibility}, navigation \cite{fagogenis2019autonomous}, and grasping \cite{kim2018sensorized}. 
	However, few of the tools can realize multi-modal force sensing during tissue manipulation, e.g., sensing the touching (Fig. \ref{Figure:Introduction}(a)), grasping (Fig. \ref{Figure:Introduction}(b)), and pulling (Fig. \ref{Figure:Introduction}(c)) forces, without tool changes that may increase the risk of infection and extend the time of surgery \cite{diaz2017research}.

	% % % % % % % % % % % % The First figure % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.47\textwidth]{figure/First.pdf}
		\caption{Typical tissue manipulations using a pair of forceps in robotic thyroid tumour treatment, an example scenario of robotic surgery, and the critically required force information.
			(a) illustrates the situation where a pair of forceps touches the targeted tissue while measuring the contact forces.
			(b) presents the state when the forceps grasp the targeted tissue while measuring and controlling the grasping force.
			(c) depicts the forceps pulling the grasped tissue while measuring the pulling force.}
		\label{Figure:Introduction}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	This paper proposes a micro-sized force sensing module for robotic surgery. 
	Built upon vision techniques, the proposed module is robust to temperature and electromagnetic variation and adaptable to various instruments. 
	Mathematical derivation of the force estimation method is presented and verified through experiments.
 Integrating the sensing module, we further develop a pair of haptics-enabled forceps and realize its multi-modal force sensing.
    % A pair of haptics-enabled forceps is further proposed by integrating the force sensing module, and then its multi-modal force sensing is realized.
	% To demonstrate the application of the module, we integrate it with a pair of generic micro-sized forceps and propose a multi-modal force sensing method.
	We also verify the feasibility of the proposed haptics-enabled forceps and sensing methods through a series of experiments, including an automatic tissue grasping procedure for thyroid tumour treatment where touching, grasping, and pulling are sequentially performed on a phantom, automatically.
	
	\subsection{Related Works} 
	\subsubsection{Methods for Force Sensing}
	The force sensing methods can be briefly classified into electronics-based, optics-based, and vision-based.
	%Principle of electrical-based sensor
	% advantages and drawbacks
	The first type can be further categorized into piezoresistive \cite{fiorillo2018theory}, piezoelectric \cite{wu2021piezoelectric}, and capacitive \cite{mishra2021recent}.
	When the sensor is pressed, the resistance or capacitance change of the material is reflected by the voltage variance and transformed into force information. 
	Sensors based on these methods can achieve relatively high accuracy and are widely adopted in flexible force sensing.
	However, they are often sensitive to temperature and humidity variation and electrical noise which are common in surgical procedures \cite{bandari2019tactile}.
	In addition, because these sensors rely on the change in the material's physical properties, they are usually more capable of measuring a force in one dimension than forces along multiple axes.
%	it is not easy to decouple the measured force to multiple axes.
	Complex architecture design and manufacturing are always required for a multi-axis sensor based on these principles \cite{templeman2020multi}.
	
	%Principle of optical-based sensor
	% advantages and drawbacks
	Optics-based force sensors are mainly based on three principles: intensity, wavelength, and phase modulation \cite{xin2020optical}.
	%These sensors are always fabricated based on optical fiber and in a small size.
	These sensors can be packaged into small sizes thanks to the compactness of optical fibers.
	Compared with electronics-based sensors, optics-based sensors are superior in terms of biocompatibility and magnetoelectric passivity.
	Nevertheless, they have high requirements for the light source and rely on expensive devices to analyze the output/reflected light \cite{bandari2019tactile}.
	Furthermore, for multi-axis force sensing, optics-based sensors require multiple optical fibers, leading to complex structures.
%	Moreover, sensors based on this method have limited performance in the multi-axis sensing because of the optical fiber's long and linear structure.
	% if want to achieve a multi-axis force sensing, multiple fibers are required, which results a complex design.
	
	%Principle of vision-based sensor
	% examples
	% advantages and drawbacks
	% Hardware Technology of Vision-Based Tactile Sensor: A Review 
	% Visuotactile Sensors With Emphasis on GelSight Sensor: A Review
	Vision-based force sensors have been studied extensively in the last decades.
	This kind of sensors always adopt one or multiple cameras to capture the deformation of flexible components.
	External forces applied to the flexible component can be estimated by analysing the deformation.
	Recently, some vision-based sensors have emerged and shown good performance.
	For example, many researchers investigated texture sensing and shape reconstruction based on GelSight \cite{abad2020visuotactile}. 
	Ouyang \textit{et al}. \cite{ouyang2020low} developed a low-cost multi-axis force sensor based on a fiducial marker supported by four compression springs. 
	In \cite{ouyang2020low}, a camera was used to estimate the displacement of the marker, which was converted to force information by multiplying a linear transformation matrix.
	With almost the same principle, Fernandez \textit{et al}. \cite{fernandez2021visiflex} also presented a vision-based force sensor for a humanoid robot's fingers.
	In addition to multi-dimensional force feedback, vision-based force sensing is also relatively robust to magnetic, electrical, and temperature variations.
	However, most of the current vision-based sensors are large in size and rarely used in small instruments.
%	However, currently, sensors based on this method are large-sized.
	% current are for large size, few of them has been used in samll size force detection.
	
	%As we want to enable the robot with high dimension forces feedback and make it compatible with MRI and robust to temperature and motion noise.
	%Therefore, the vision based method is more suitable for surgical applications, but the challenge is reducing its size.
	
	\subsubsection{Force Sensing of Surgical Instruments}
	Many studies have been conducted to enable surgical instruments with tactile feedback.
	These works mainly focused on single surgical tasks, and the methods they used were primarily the ones mentioned above.
	
	Using a hall-effect sensor to detect the deformation of a spring, McKinley \textit{et al}. \cite{mckinley2015single} developed a single-axis force-sensing probe for palpation.
	Towards the same application, a multi-axis force sensing module was proposed by Kim \textit{et al}. \cite{kim2017surgical} based on the capacitive transduction principle.
	To avoid the influence of electromagnetic variation, Li \textit{et al}. \cite{li2019high} presented triaxial sensorized catheters for ablation based on fiber Bragg grating (FBG).
	Adopting the same principle but with a different structure, Gao \textit{et al}. \cite{gao2018fiber} showed a three-dimensional force sensing module with similar stiffness in lateral and axial orientations.
	However, these modules ignored the influence of temperature variation on FBG.
	To decouple this influence, Taghipour \textit{et al}. \cite{taghipour2019temperature} proposed a differential measurement method.
	%developed a force sensor based on FBG and decoupled the influence of temperature variation by a differential measurement method.
	Aiming to display the sensed haptic information to the operator, Guo \textit{et al}. \cite{guo2019novel} presented a teleoperated catheterization system that can recreate the force of the tip at the master side.
	This recreation relied on projecting the fiber tip's force information to a magnetorheological fluid device.  
	Moreover, they also studied the multilevel operation strategy for robotic intervention with haptic feedback \cite{bao2022multilevel}.
	%Using magnetorheological fluids and fiber tip sensor, Guo \textit{et al}. \cite{guo2019novel} presented a teleoperated endovascular catheterization system that can recreate the tip's force sensing at the master side to increase the operation transparency.
	%They also studied the multilevel operation strategy for robotic vascular intervention with the haptic feedback \cite{bao2022multilevel}.
	Although the mentioned works have made specific achievements, they are limited to measuring contact force for navigation, palpation, or ablation.
	%Shihora \textit{et al}. \cite{} also showed a haptic sensor for palpation, and thanks to this sensor they further investigated the feasibility of robotic palpation using haptic sensor \cite{shihora2021feasibility}.
	
	%multi-axis force sensor
	%For example, Fagogenis \textit{et al}. \cite{fagogenis2019autonomous} proposed an autonomous robotic intracardiac catheter navigation method relying on haptic sensing.
	Compared to instruments for palpation and ablation, enabling force sensing for tissue grasping is more challenging.
	This is because the latter is concerned with both static contact and dynamic manipulation, and the structure of the grasper is more complex than that of the probe.
	By installing two FBG fibers, Zarrin \textit{et al}. \cite{zarrin2018development} showed a customized two-dimensional grasper that can measure grasping and axial forces. 
	Similarly, Kim \textit{et al}. \cite{kim2018sensorized} modified a pair of surgical forceps' two jaws with capacitance to enable multi-axis force sensing. 
	However, the capacitive method in \cite{kim2018sensorized} is sensitive to temperature and vulnerable to electromagnetic variation.
	This may invalidate the force sensing when the forceps are applied in practical surgical tasks \cite{dimaio2011vinci}.
	% especially bipolar cautery is always adopted for cutting \cite{dimaio2011vinci}.
	%For heart and brain surgery, electromagnetic interference are always involved \cite{hooshiar2019haptic}. 
	%And the temperature changes in the range of ( $30^\circ$) in surgery \cite{}.
	
	Although vision has already been introduced into surgical systems, little work has investigated its force-sensing capability for surgical instruments.
	An exception is that Fagogenis \textit{et al}. \cite{fagogenis2019autonomous} developed an 8-mm-diameter visual-haptic module for autonomous robotic intracardiac catheter navigation.
	The proposed module comprises a camera and a clear silicone head, and it has two working modes: continuous and intermittent contacts.
	The former mode estimates the forces according to the amount of tissue captured by the camera, while the latter bases the estimation on the contact fraction with the heart in a beat cycle.
	That means this module relies on the movement rhythm of contacted tissue, which limits its application only to regular moving organs.
	
	In summary, despite many achievements have been made in single-task force sensing with or without consideration of surgical compatibility, none have enabled multi-modal force sensing in micro-sized instruments for MIS.
	
	
	\subsection{Contributions}
	The design of force sensing modules for MIS should be concerned with their physical and functional properties.
	The former mainly relates to the shape and size of the module, while the latter constraint regards the compatibility, interaction, and performance in the bio-environment \cite{farooq2022decade}.
	A vision-based force sensing module can be a good candidate for MIS. 
	However, minimizing the size while maintaining the multi-axis sensing ability is challenging. 
	Moreover, force sensing modules suitable for the multi-modal requirements in surgery have not yet been developed.  
	
	
	%\textit{Tactile Sensors for Minimally Invasive Surgery: A Review of the State-of-the-Art, Applications, and Perspectives:
		%	The design requirements (a.k.a. constraints) for the tactile sensors in MIS applications can be related to the physical and functional properties of the sensors. 
		%	Physical requirements attribute mainly to the shape and size of the sensors, while the functional constraints relate to the compatibility, interaction, and performance of the sensor at the bio-environment. 
		%	As an example of physical constraints, an MIS tactile sensor should be small in size and cylindrical in shape to be integrable at the body or tip of a catheter. 
		%	As a functional requirement, the sensor should be capable of measuring the contact force in a range of 0-5N with a resolution of 0.01N [15]. 
		%	Furthermore, the sensor should be fairly sensitive, linear, and show a low level of hysteresis.}
	
	% the haptic sensor for minimally invasive surgery have requriements on both design and functions
	% for design, it should be small in size and cylindrical in shape to be integrable at the body or tip of a surgical instrument \cite{}.
	% function, it should be MRI compatible, insensitive to tempreature and motion noise.
	
	
	%Based on the introduction above, current haptics-enabled forceps either have single force sensing modal or vulnerable to temperature and MRI.
	%And most of them requires the modification of forceps' clips.  
	Therefore, the main contributions of this article are two-fold:
	1) A micro-sized vision-based force sensing module is proposed and integrated into a pair of forceps.
	%The sensor can be easily integrated into surgical instruments with a simple and cylindrical structure.
	2) A multi-modal force sensing method for haptics-enabled generic forceps is presented, which enables forceps with simultaneous contact force, grasping force, and three-axis pulling force sensing capabilities.
	These contributions are validated by various carefully designed experiments, including an automatic tissue grasping procedure on ex-vivo tissues.
	
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                                                  %
	%%                Introduuction Older vision        %
	%%                                                  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%higher degree / continuum robot emergening
	%%flexible robot's introduction
	%% many advantages but also disadvantages;
	%% to deal with this, robot-assisted or computer-asssisted is necessory;
	%% however further separate surgeon and instrument and induce the loss of haptic
	%% haptic is important for both surgeon and robot
	%% to restor the haptic many attempts has been made
	%% but non of them can
	%%\IEEEPARstart{B}{ased} on the invention of the cystoscope \cite{samplaski2009two}, the first laparoscopic procedure was performed by Kelling \cite{peter2008history} in 1901.
	%%The introduction of minimally invasive surgery (MIS) has benefited patients by reducing blood loss, wound site infection, recovery time, etc.
	%%In the last centuries, many efforts have been conducted to increase the dexterity of instruments and continue minimize the incision, such as the emergence of continuum robots \cite{burgner2015continuum,omisore2020review,alfalahi2020concentric}.
	%%However, the adoption of MIS results in the separation of hands and the targeted tissue compared with the open surgery.
	%%This presented many challenges to surgeons, including a steep learning curve due to the inverse kinematic of instrument, lack of dexterity and loss of sensory information \cite{simaan2018medical}.
	%%To deal with these problems and continue minimize the incision toward to noninvasive surgery, many efforts has been carried out in last centuries.
	%%For instance, flexible robots \cite{omisore2020review, burgner2015continuum} have been introduced to increase the dexterity and allow the navigation in narrow cavities.
	%%To make the robot become multi-function, \cite{amon2021forceps} developed a needle can used as a forceps as well, for example, and \cite{alfalahi2020concentric} investigated the concentric robot to control multiple tools through a single port.
	%%However, the high flexibility of the instruments could limit the effective transmission of forces to the tip tools and further prevents the surgeon from taking a full control of them, as they exceed human physiological limitations.
	%%While computers and robots have been used to help control these high-dexterity instruments and reduce surgeons' work load, it places further demands on the surgeon as the information available when holding a surgical tool is no longer directly available \cite{simaan2018medical}.
	%%This means the surgeon must rely entirely on a visual signal to control or supervise these instrument.
	%%As a result, overall performance will be degraded, as highlighted by a meta-analysis demonstrating the utility of including haptic information in teleoperated systems \cite{weber2015benefits}.
	%%And it is still far away for robot-assisted surgery to achieve full autonomy, due to regulatory, ethical, and technical challenges \cite{yang2017medical,yang2018grand}. 
	%%Proved in the clinic, one of the main shortcomings of surgical systems is the lack of haptic feedback \cite{diaz2017research}.
	%%Thus far, the most popular MIS robot should be Da Vinci \cite{d2021accelerating}, a master-salve system developed by the Intuitive Surgical, which is also lacking of these sensing abilities.
	
	%
	%{T}{he} benefits of providing haptic information are manifold in minimally invasive surgery \cite{hale2004deriving,weber2015benefits}.
	%%including helping properly retract tissues, delicately manipulate tissue, avoid avoids exaggerated force application and unintentional damage, apply strong sutures or clips, or provide robust grasping \cite{weber2015benefits,diaz2017research}.
	%It can play a central role in driving surgical performance by allowing the surgeon to gain a much better understanding of the boundaries of malignant growth, for example, and determining the forces required to complete a particular action such as manipulating tissue \cite{jamieson2015can}.
	%%The other major use of haptic in surgery relates to the information that a surgeon can glean about the physical characteristics of the tissues that are involved in the operation.
	%And it should be noted that the role of haptic feedback in robot-assisted minimally invasive surgery (RMIS) can be even more significant if the visual feedback is deteriorated during the operation, for example, when the camera's view is clouded by fluids or by the smoke generated from the electrosurgical hook operations. 
	%The specific example of the Da Vinci system \cite{d2021accelerating}, the current systems are notable for a lack of haptic feedback, highlights the broader trend that robotic surgical technology must be developed to fully meet the clinical needs and associated challenges of modern surgery \cite{amirabdollahian2018prevalence}.
	%According to a meta-analysis study \cite{weber2015benefits}, force feedback significantly improved task success and accuracy and detection rates during palpation. 
	%It also reduced the average and peak forces applied to avoid tissue damage while decreasing the time to complete the task.
	%%In spite of visualization deflection was found as a factor to detect the haptic when contact on soft tissue happen, in narrow and deep scenario, poor visualization will result in the decrease of haptic sensitivity.
	%
	
	%% % % % % % % % % % % % The First figure % % % % % % % % % % % % %
	%\begin{figure}[t!]
	%	\centering
	%	\includegraphics[width=0.49\textwidth]{figure/First.pdf}
	%	\caption{A robotic surgical system integrated with haptic-enabled forceps.
		%	(a) shows situation that the integrated pair of forceps is used for detect contact with targeted tissue.
		%	(b) shows the state that the forceps are grasping the targeted tissue while measuring and controlling the grasping force.
		%	(c) shows the forces are pulling the grasped tissue while measuring the pulling force.}
	%	\label{Figure:Introduction}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	%
	%To enable instruments with haptic feedback, many attempts were made including electrical and optical-based sensors \cite{culmer2020haptics,bandari2019tactile}.
	%Thorough mounting strain gauges, Dargahi \textit{et al.} \cite{dargahi2003endoscopic} and Zarenia \textit{et al}. \cite{zareinia2016force} provided their graspers with uniaxial and biaxial force feedback, respectively.
	%Inspired by the hair cells, Hu \textit{et al.} \cite{hu2010bioinspired} developed a 9mm-square tactile sensor with four piezoresistors and drew its packaging scheme on a robotic end-effector.
	%\cite{sokhanvar2009mems} fabricated sensors based on Polyvinylidene Fluoride (PVDF) for stiffness mapping.
	%Similarly, \cite{chuang2016piezoelectric} manufactured a tactile sensor by PVDF and mounted it on an endoscopic to detect submucosal tumors.
	%\cite{sharma2019piezoelectric} presented a biopsy needle integrated with a commercial force transducer from Boston Piezo-Optics for characterizing the mechanical properties of thyroid tumors.
	%By integrating a force sensor comprised of two capacitors into both jaws of Raven-II surgical forceps, Kim \textit{et al}. \cite{kim2015force} enabled it can detect triaxial pulling and uniaxial grasping forces. 
	%Based on the same principle of \cite{kim2015force}, Kim \textit{et al}. \cite{kim2018sensorized} improved the forceps to rotational and palpation-force detectable by introducing another two capacitors. 
	%And Kim \textit{et al}. \cite{kim2020s} developed a portable surgical robot embedded with a capacitance force sensor.
	%However, there are many limitations of these electrical sensors, the major one of piezoresistor is hysteresis, for example, and of capacitance is likely to absorb thermal and motion noise.
	%%For piezoresistive sensors, the major limitation is hysteresis \cite{chi2018recent}. 
	%%For piezoelectric type, although they exhibit high sensitivity and accuracy, they cannot meet the static loading/force conditions and are thermal-sensitive.
	%%For capacitive sensors, they are likely to absorb thermal and motion noises \cite{gabrielson1993mechanical,kulah2006noise}.
	%
	%Compared with electrical types, optical sensors are electrically passive and magnetic resonance imaging (MRI) compatible.
	%Through estimating the light loss of optic fibers caused by deflection, Yip \textit{et al}. \cite{yip2010robust} used three pairs of optic fibers to develop a uniaxial tactile sensor, and Noh \textit{et al}. \cite{noh2016image} presented a triaxial one with the same method.
	%Park \textit{et al}. \cite{park2010real} proposed a tactile sensor via detecting the wavelength shift when it contact with environment, and integrated this sensor into a needle to estimate its 3-D shape and deflection.
	%Similarly, Zarrin \textit{et al}. \cite{zarrin2018development} fabricated a needle-driver grasper, which was able to measure the grasping and axial force.
	%However, optic-fiber sensors can hardly achieve a high dimension force feedback \cite{bandari2019tactile}. 
	%Fagogenis \textit{et al}. \cite{fagogenis2019autonomous} presented a robotic intracardiac catheter that can navigate automatically by using haptic vision.
	%This haptic vision was obtained by a silicone covered camera, which is used to detect the contact with tissue and help to adjust this contact force.
	%Xia \textit{et al.} \cite{xia2022contact} developed a tactile sensor based on multipole magnetic layer and 3D hall sensors.
	%This work has been extended by Fang \textit{et al.} \cite{fang2022soft} to a soft magnetic fingertip with particle jamming structure for tactile perception and grasping.
	%
	%%\cite{shihora2021feasibility} investigated using robotic palpation to remotely identify the landmark for cricothyrotomy, where a ATI force/torque sensor was mounted and equipped with a plastic probe tip.
	%%
	%%some for palpation\\
	%%In\cite{konstantinova2014implementation,li2014multi,mckinley2015single,aastrand2015initial,li2014novel,pacchierotti2015cutaneous} the haptic devices were used to position tumor.
	%%\\
	%%some for navigation\\
	%%some for tissue manipulation\\
	%%some proposed new grasper\\
	%%Qasaimeh et al. \cite{qasaimeh2008pvdf} developed a PVDF-based microfabricated tactile sensor mounted on the surgical grasper with preserved grasper shape. 
	%%However, the approach provided only grasping force information, which is insufficient for deducing contact force. 
	%%Lee et al. \cite{lee2013preliminary} developed a multiaxis contact force sensor integrated into the surgical forceps with multiaxis force-sensing capability. 
	%%However, it is difficult to measure accurate contact force information due to non-linearity and complicated calibration of the sensor.
	%%\\
	%%some for multi-function\\
	%%some other research\\
	%%In 2008, King et al. \cite{king2008multielement} integrated a piezoresistive tactile sensor into the surgical instrument tip. 
	%%Although this sensor can measure contact forces, it does not provide the essential functionality of a surgical gripper.
	%%
	%%Balicki et al. \cite{balicki2010micro} demonstrated the efficacy of providing tool tip interaction force feedback,  using force sensing instrument that incorporate fiber-optic sensors into the distal portion of the instrument shaft located  inside the eye.
	%%Later, He et al. \cite{he2014multi} extended the work by adding an additional force sensor to a variable admittance-controlled cooperative robot to provide force feedback from both tool tip and sclera entry point.
	%%
	%%Hong et al. \cite{hong2012design} fabricated a compliant forceps using two flexure hinges with the capability to sense two axial forces. 
	%%Sensorized forceps can measure single-axis pulling and grasping forces. 
	%%However, measurements of single-axis pulling force do not provide sufficient information to allow force feedback control for surgical robotic systems. 
	%%
	%%In \cite{haslinger2013fiberoptic}, the design of a 6-DOF fiber-optic force–torque sensor for minimally invasive robotic surgery is described. The sensor has a diameter of 6.4 mm and ensures biocompatibility and sterilizability.
	%%
	%%\cite{watanabe2014force} developed a force sensor attachable to thin fiberscopes/endocscopes utilizing high elasticity fabric.
	%%Its resolution is less than 0.01N and sensitivity greater than 600 pixels/N. 
	%%
	%%\cite{lin2014thermocool} showed a ablation catheter, which can estimating the contact force between its tip electrode and lesion tissue thorough a magnetic coil-sensor pair with a 1g resolution.\\ 
	%%
	%%
	%%\cite{fontanelli2017novel} developed a novel force sensing which can be integrated into a trocar.
	%%
	%%As mentioned in \cite{stoll2006force}, when the surgeon grasp tissue, the force control is needed.
	%
	%Despite the works mentioned above have made a number of achievements on both sensor developing and integration, they either cannot achieve a multi-dimension feedback or make instruments realise multi-function.
	%%why the multi-dimension feedback and multi-function is important?
	%These two characterises are influential in minimally invasive surgery.
	%For example, a classical laparoscopic tissue handling always depends on seven-dimension force including three-dimension force, three-dimension torque and grasping force \cite{horeman2014force}.
	%And enabling the forceps with multi-functions can also reduce the instrument exchange and operating time, which is a crucial clinical need \cite{diaz2017research}. 
	%
	%In this study, we present a six-dimensional force/torque sensor which can estimate triaxial forces and triaxial torques, shown in Fig. \ref{Figure:forcpes}. 
	%The sensor is simple structured by a micro camera, coil spring and a marker, which prevents it form the  influences from temperature, MRI, etc.
	%There, the camera is used to estimate the pose of marker mounted on the spring end; therefore, the external force can be calculated according to the deflection of spring \cite{ouyang2020low,fernandez2021visiflex}.
	%Integrating with the proposed sensor, a forceps can detect the grasping, three pulling and rotational forces, i.e., achieving seven-dimension force feedback.
	%%Then, this forceps can achieve multi-function, including palpation, tissue holding and navigation.
	%When the forceps is closed, it can be used for palpation as a probe with 3D haptic feedback, where this haptic information can used to generate the stiffness map of the tissue.
	%This stiffness map can be further used to locate the pathological tissue such as the tumour.
	%When the forceps is used to grasp and hold tissue for cutting operation, its grasping and dragging forces can help avoiding unintentional damage due to excessive force and allow more delicate manipulation of tissue.
	%For demonstration, in this paper, a 2mm-diameter flexible biopsy sampling forceps are chosen to perform palpation and tissue manipulation with proposed haptic sensor in various diameters.
	%
	%The main contributions of this work are as follows:
	%\begin{itemize}
	%\item[1)] A design and a vision-based forces/torques estimation method for a kind micro-size haptic sensor towards MIS are proposed.
	%\item[2)] A haptic-enabled forceps that can measuring six dimension Cartesian spaces external forces/torques and its grasping force is proposed based on the developed sensor.   
	%\item[3)] Robotic integration of the haptic-enabled forces and its control diagram are presented.  
	%\end{itemize}
	% 
	%Through calibration and verification, the design and force estimation of the micro-size haptic sensor are validated.
	%And the feasibility of the haptic-enabled forceps and its grasping force measurement are verified via a commercial pressure sensor.
	%Then, using the robotic tissue palpation and grasping task for examples, the application of the haptic-enabled forceps and its robotic integration are verified.
	%To identify the configuration for various situations that have different requirements, a discussion are also conducted based on different sensors in section \ref{Section:experiments}.
	%
	%The remind of this paper is organized as follows. 
	%The development of haptic sensor is introduced in Section \ref{Section:sensor_development}.
	%And the design and grasping force estimation of haptic-enabled forces is presented in Section \ref{Section:forceps}, followed by Section \ref{Section:robotic_integration} the robotic integration. 
	%Then, the above works are verified by a series of experiments illustrated in Section \ref{Section:experiments}.
	%Finally, before we draw the conclusion in section \ref{Section:conclusion}, a discussion of different sensor's requirement is presented in Section \ref{Section:Discussion}.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%                sensor development               %
	%												  %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Vision-based Force Sensing Module}
	\label{Section:sensor_development}
	% For MIS applications, instruments are generally required to be small for minimized incisions and natural orifices.
	% For example, the diameter of the common femoral, a typical entry portal for cardiac surgery, is around 6.6mm.
	% Moreover, a channel is required to allow tools or instruments to pass to make the sensor compatible with generic instruments.
	%Therefore, a force sensor for various applications should simultaneously have a small size for minimized incisions and a channel for tools.
	% This section presents a generalized design of a micro-size force sensor and its force estimation method.
	
	\subsection{Design of Force Sensing Module}
	\label{Subsection:sensor_design}
	\subsubsection{Overall Structure}
	The design of the proposed sensing module (Fig. \ref{Figure:Sensor_Calculation_Design}(a)) has followed two requirements: 1) small in size, 2) compatible with forceps.
	The module is designed in a cylindrical shape to minimize anisotropy, and the main components are a flexure, a camera, and a target.
	The module has a central channel for the passage of tools and instruments.
	%Here, a backup camera is installed to avoid accidental failure.
	The target, supported by the flexure, is manufactured with multiple holes that allow light to pass from a light source, and these holes can then be captured by a camera as markers for the estimation of the target's pose, as depicted in Fig. \ref{Figure:Sensor_Calculation_Design}(b).
	The module's size and stiffness can be customized based on the instruments' and applications' needs.
	Here, we assemble a prototype with a diameter of 4mm and a length of 11mm, as shown in the lower-right of Fig. \ref{Figure:Sensor_Calculation_Design}(a). 
	
	\subsubsection{Flexure}
	Material properties and geometry play a dominant role in the multidimensional stiffness of a flexure.
	Different approaches have been adopted for flexure design, including the pseudo-rigid-body replacement method \cite{pucheta2010design}, topology optimization \cite{deng2014topology}, etc.
    % Stainless steel exhibits low hysteresis, stress relaxation, and creep, which makes it a suitable material for purely elastic components \cite{euroinox2007}.
	%When purely elastic properties with low hysteresis, stress relaxation, and creep are required, metals often offer superior performance over other materials \cite{}.
	For force sensing in different surgical procedures, the flexure should also be changeable according to the task requirements.
	One commercial and readily available flexure is the compression spring \cite{ouyang2020low,fernandez2021visiflex}.
	%A compression spring also has significant elastic deformability and little fatigue, as stress is relatively evenly distributed over its length.
	Although compression springs typically show dominant stiffness in the axial direction, they also exhibit a certain degree of stiffness in lateral directions and can be linearly approximated \cite{keller2011equivalent}.
	Moreover, they can be easily reconfigured by choosing different wire diameters, outer diameters, and coil numbers. 
	For the prototype, we chose a stainless steel compression spring with a wire diameter of 0.5mm, outer diameter of 4mm, rest length of 5mm, and four coils. 
	The selected spring has a force response range of 0$\sim$5N and 0$\sim$2N at axial and lateral displacements of 0$\sim$2mm and 0$\sim$1mm, respectively, which can meet the requirement of most MIS tissue manipulations \cite{golahmadi2021tool}.
	%And the marker's size can be made to suit the size.
	\subsubsection{Camera and Target}
	The camera for the sensing module should have a compact form factor, sufficient resolution, and surgery compatibility.
	For the prototype, we chose OVM6946 (Ominivision Inc., USA), which has been adopted in many medical and surgical applications \cite{banach2021visually,cao2022spatial}. 
	It is 1mm in width and 2.27mm in length and has a 400$\times$400 resolution.
	%Generally, four nonlinear points on an object are required for a camera to estimate the object's pose in 3D space \cite{lepetit2009epnp}.
	%However, points may get lost due to occlusion and halation. 
	To ensure continuous tracking and estimation, the target is fabricated with twelve cycle-distributed 0.15mm-diameter holes that are used as markers for the camera to track and estimate the target pose, and a central hole is also reserved for tools to pass through.
	These holes can also be customized based on the instruments and applications.
	Considering the integration of the sensing module into the biopsy forceps for MIS, the prototype’s target in Fig. \ref{Figure:Sensor_Calculation_Design}(a) has a 3.4mm outer diameter and a 0.6mm-diameter central hole for the driving cable.
	
	% % % % % % % % % % % % Figure sensor design and forces/torques calculation % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Sensor_Calculation_Design.pdf}
		\caption{(a) The sectional view (left) shows the design of the sensing module.
			The module's head (1) is used for mounting instruments.
			Two LEDs in our prototype are the light source (2).
			The light comes through the target's (3) holes and is captured by the camera (6) mounted on the other end of the flexure (4). 
			%The other camera is a backup of (6).
			The upper-right inset shows the installed target and the holes used as markers.
			The cylindrical base (7) provides mount to connect to instruments.
			Concentric to the module, a channel (8) is reserved for tools and instruments.		  
			A soft sleeve (5) can also be installed to minimize the influence of intense external light disturbances in some particular situations.
			The lower right inset shows a prototype that has a 4mm diameter and 11mm length. 
			(b) The camera frame \{$C$\} and the target frames of the initial \{$M_0$\} and current \{$M_t$\} states.
			%The camera  can track the target and estimate its current pose \{$M_t$\} that relates to the initial pose \{$M_0$\}.
		}
		\label{Figure:Sensor_Calculation_Design}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

	% % % % % % % % % % % % Figure 1 % % % % % % % % % % % % %
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.80\textwidth]{figure/pose_estimation.pdf}
		\caption{(a) The directly captured image at the initial state. 
			(b) The blob detected result at the initial state based on the filtered image. 
			These detected marker centers are returned for pose estimation.
			(c) The projection relationship between the $i$-th marker ($^{M}{\mathbf{p}}_{i}$) in the target frame \{\textit{M}\} and its corresponding pixel position ($^{I}{\mathbf{p}}_{i}$) in image frame \{\textit{I}\}. 
            (c) also shows the original index of markers.
			(d) The blob detection result at a random pose, where the index of detected markers is different, and the points labelled with 3 and 6 in the initial state are lost. 
			(e) The registration result of (d), where markers were registered to their original indexes.}
		\label{Figure:pose_estimation}	
	\end{figure*}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 
	\subsection{Force Estimation}
	\label{Subsection:force_estimation}
	Ideally, for a linear-helix compression spring, the external force $\mathbf{f}_s$ applied to the sensing module's head has an approximated relationship with the spring's displacement \cite{keller2011equivalent} as
	\begin{equation}
		{\mathbf{f}_s} = {{\mathbf{K}}_s}\, {\mathbf{d}_s} = \left[ {\begin{array}{*{20}{c}}
				{{k_x}}&0&0\\
				0&{{k_y}}&0\\
				0&0&{{k_z}}
		\end{array}} \right] 
  \mathbf{d}_s
 % \left[ {\begin{array}{*{20}{c}}
%				{{d_x}}\\
%				{{d_y}}\\
%				{{d_z}}
%		\end{array}} \right] 
\, ,
		\label{equation:Forcecalculation}
	\end{equation}
	%\begin{align}
	%	{F_e} &= {k_s} \times {x_s} \notag \\ 
	%	&= \left[ {\begin{array}{*{20}{c}}
			%			{{k_{{w_x}}}}&{}&{}&{}&{}&{}\\
			%			{}&{{k_{{w_y}}}}&{}&{}&0&{}\\
			%			{}&{}&{{k_{{w_z}}}}&{}&{}&{}\\
			%			{}&{}&{}&{{k_{{v_x}}}}&{}&{}\\
			%			{}&0&{}&{}&{{k_{{v_y}}}}&{}\\
			%			{}&{}&{}&{}&{}&{{k_{{v_z}}}}
			%	\end{array}} \right] \times \left[ {\begin{array}{*{20}{c}}
			%			{{d_x}}\\
			%			{{d_y}}\\
			%			{{d_z}}\\
			%			{{\varphi}}\\
			%			{{\theta}}\\
			%			{{\psi}}
			%	\end{array}} \right] \, ,
	%	\label{equation:Forcecalculation}
	%\end{align}
	where $\mathbf{f}_s \in \mathbb{R}^{3}$ denotes the external forces, ${\mathbf{K}}_s$ denotes the stiffness matrix of the spring, 
	and $\mathbf{d}_s \in \mathbb{R}^{3}$ is the displacement of the spring's end relative to its relaxed original position.
	%\textit{And we assume the spring has low hysteresis, stress relaxation, and creep, and in cases where the distal side's velocity and acceleration are non-negligible, low viscosity and mass.}
    In this paper, $\mathbf{d}_s$ is equal to the target's current relative displacement to its initial state.
%	In this section, the marker's pose estimation and external force calculation are introduced. 
	%spring's stiffness matrix calibration
	%Therefore, to calculate the external force applied on the proposed sensor, marker's pose and spring's stiffness matrix should be measured first.
	%In this section, they are introduced in turn. 


	\subsubsection{Image-Based Target Pose Estimation}
	In order to estimate the target pose, each marker needs to be tracked by the camera first.
	However, as the raw image captured by the camera is full of noise, as seen in Fig. \ref{Figure:pose_estimation}(a), markers cannot be detected robustly.
	Therefore, a bilateral filter and threshold are adopted to minimize the noise and preserve the boundary in images \cite{singh2019advanced}.
	Then, markers are circled by a parameterized blob detection \cite{kaspers2011blob}, as shown in Fig. \ref{Figure:pose_estimation}(b).
	Finally, each marker's central pixel position $^{I}{\mathbf{p}}$ is returned.
	%\begin{equation}
		%({u^*},{v^*},{\delta ^*}) = \mathop {\arg \max }\limits_{(u,v,\delta )} |{\delta ^2}{\nabla ^2}{n_\delta }*I(u,v)| \, ,
		%\label{Equation:blob_detection}
	%\end{equation}
	%where $({u^*},{v^*})$ is the pixel position of the blob's centre, and ${\delta ^*}$ is the size of the blob.
	


	The positions of markers $^{M}{\mathbf{P}} \in \mathbb{R}^{2\times 12}$ in the target frame \{\textit{M}\} are known.
	The projection between the $i$-th marker centre $^{M}{\mathbf{p}}_{i} = [x_i, \, y_i]^{\textnormal{T}}$ and its corresponding $^{I}{\mathbf{p}}_{i} = [u_i, \, v_i]^{\textnormal{T}}$ in the image frame \{\textit{I}\}, as shown in Fig. \ref{Figure:pose_estimation}(c), can be formulated as 
 	\begin{align}
		^{\mathop{I}}{\tilde{\mathbf{p}}}_{i} 
		& = \frac{1}{z} \, \left[ {\begin{array}{*{20}{c}}
				{\frac{1}{{{\rho _w}}}}&0&{{u_0}}\\
				0&{\frac{1}{{{\rho _h}}}}&{{v_0}}\\
				0&0&1
		\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
				f&0&0&0\\
				0&f&0&0\\
				0&0&1&0
		\end{array}} \right]{}_{M}^{C}{\mathbf{T}}{\,}^{M}{\tilde{\mathbf{p}}_{i}} \notag \\
		& =  \frac{1}{z} \, \underbrace{\left[ {\begin{array}{*{20}{c}}
					{\frac{f}{{{\rho _w}}}}&0&{{u_0}}\\
					0&{\frac{f}{{{\rho _h}}}}&{{v_0}}\\
					0&0&1
			\end{array}} \right]}_{camera \, \, intrinsic \, \, {\mathbf{A}}}
		\left[ {\begin{array}{*{20}{c}}
				r_{11}&r_{12}&t_x\\
				r_{21}&r_{22}&t_y\\
				r_{31}&r_{22}&t_z
		\end{array}} \right]
		\left[ {\begin{array}{*{20}{c}}
				x_i\\
				y_i\\
				1
		\end{array}} \right] \notag \\
		& = \frac{t_z}{z}\, \underbrace{\left[ {\begin{array}{*{20}{c}}
					{h_{11}}&{h_{12}}&{h_{13}}\\
					{h_{21}}&{h_{22}}&{h_{23}}\\
					{h_{31}}&{h_{32}}&1
			\end{array}} \right]}_{homography \, \, \mathbf{H}}  
		\left[ {\begin{array}{*{20}{c}}
				x_i\\
				y_i\\
				1
		\end{array}} \right] \, ,
		\label{equation:projection}
	\end{align} \\ 
	where ${}_{M}^{C}{\mathbf{T}} = \left[ \begin{array}{*{20}{c}}
		\mathbf{r}_1 & \mathbf{r}_2 & \mathbf{r}_3 &\mathbf{t} \\
		0& 0& 0&1
	\end{array}\right] = \left[ {\begin{array}{*{20}{c}}
			r_{11}&r_{12}&r_{13}&t_x\\
			r_{21}&r_{22}&r_{23}&t_y\\
			r_{31}&r_{32}&r_{33}&t_z\\
			0&0&0&1
	\end{array}} \right]$,\\
	$^{M}\tilde{\mathbf{p}}_{i} = [x_i, \, y_i, \, 0, \, 1]^{\textnormal{T}}$ is the homogeneous form of $^{M}{\mathbf{p}}_{i}$,
	$^{I}\tilde{\mathbf{p}}_{i} = [u_i, \, v_i, \, 1]^{\textnormal{T}}$ is the homogeneous form of $^{I}{\mathbf{p}}_{i}$ with
	$i\in[0,n]$ and $n=11$ for the prototype,
	the camera intrinsic matrix $\mathbf{A}$ is obtained via calibration with MATLAB camera toolbox \cite{fetic2012procedure},
	 and $z = {(h_{31}x_i+h_{32}y_i+1)} \, t_z$.
	%$t_z = \frac{z}{h_{31}x_i+h_{32}y_i+1}$.
	
	%In this paper, we regard frame \{\textit{Im}\} is coincide with frame \{\textit{c}\}. 
	
	%%%%%%% can be putted to the modelling section %%%%%%%
	According to the transformation relationship shown in Fig. \ref{Figure:Sensor_Calculation_Design}(b), the current target pose related to its initial state ${}^{M_0}_{M_t}\mathbf{T}$ can be formulated as
	\begin{equation}
		{}^{M_0}_{M_t}\mathbf{T} = {{}^{M_0}_{C}\mathbf{T}} {\, \,}^{C}_{M_t}\mathbf{T} \, .
		\label{equation:markerpose}
	\end{equation} 
	The translation of ${}^{M_0}_{M_t}\mathbf{T}$ is equal to the displacement $\mathbf{d}_s$ in (\ref{equation:Forcecalculation}).
	%Furthermore, the orientation information $[\varphi, \theta, \psi]$ of the marker can be derived by
	%\begin{align}
		%\left\{\begin{array}{l}
			%\mathbf{\varphi} = tan^{-1}(\frac{r_{32}}{r_{33}})\\
			%\mathbf{\theta} = tan^{-1}(\frac{{-r}_{31}}{\sqrt{{r}^2_{32}+{r}^2_{33}}})\\
			%\mathbf{\psi} = tan^{-1}(\frac{r_{21}}{r_{11}})
		%\end{array}\right. \, .
		%\label{equation:displacement}
	%\end{align} 
	%where 
	%\begin{equation}
	%	{}^{c_o}_{c_c}\mathbf{T} = {}^{m_o}_{c_c}\mathbf{T} {\,}{{}^{m_o}_{c_o}\mathbf{T}}^{-1} \, .
	%\end{equation}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Then, the question becomes to get ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$.
	The relationship between ${}^{C}_{M}\mathbf{T}$ and homography matrix $\mathbf{H}$ can be formulated as
	%\begin{align}
	%	\left\{\begin{array}{l}
		%		\mathbf{r}_1 = {t_z}\mathbf{I}_{c}^{-1}[h_{11}, \, h_{21}, \, h_{31}]^{\textnormal{T}}\\
		%		\mathbf{r}_2 = {t_z}\mathbf{I}_{c}^{-1}[h_{12}, \, h_{22}, \, h_{32}]^{\textnormal{T}}\\
		%		\mathbf{t} = {t_z}\mathbf{I}_{c}^{-1}[h_{13}, \, h_{23}, \, 1]^{\textnormal{T}}\\
		%		\mathbf{r}_3 = r_1 \times r_2
		%	\end{array}\right. \, .
	%	\label{equation:RandT}
	%\end{align}
	%where $t_z = \frac{z}{h_{31}x_i+h_{32}y_i+1}$.
	%$\mathbf{r}_1$, $\mathbf{r}_2$, and $\mathbf{r}_3$
	\begin{align}
		\left\{\begin{array}{l}
			\mathbf{r}_1 = {t_z}\mathbf{A}^{-1}[h_{11}, h_{21}, h_{31}]^{\rm T}\\
			\mathbf{r}_2 = {t_z}\mathbf{A}^{-1}[h_{12}, h_{22}, h_{32}]^{\rm T}\\
			\mathbf{t} = {t_z}\mathbf{A}^{-1}[h_{13}, h_{23}, h_{33}]^{\rm T}\\
			\mathbf{r}_3 = \mathbf{r}_1 \times \mathbf{r}_2
		\end{array}\right. \, .
		\label{equation:RandT}
	\end{align}
	Because $\mathbf{r}_1$ is a unit vector, $t_z$ can be calculated by $t_z = \frac{1}{||\mathbf{A}^{-1}[h_{11}, h_{21}, h_{31}]^{\rm T}||}$.
	Therefore, to estimate ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$, we only need to solve the homography matrix $\mathbf{H}^{0}$ and $\mathbf{H}^{t}$ of the initial and current states.
	
	%% % % % % % % % % % % % Figure 1 % % % % % % % % % % % % %
	%\begin{figure}[t]
	%	\centering
	%	\includegraphics[width=0.43\textwidth]{figure/blob_detection.pdf}
	%	\caption{(a) the original image captured by the camera. (b) the bilateral filter processed result. (c) the blob detection on the filed image, where nine red-number-marked holes are detected in original position.}
	%	\label{Figure:blob_detection}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	
	Because each homography matrix $\mathbf{H}$ has eight variables, at least four nonlinearly relative projections between $^{M}{\mathbf{p}}_{i}$ and $^{I}{\mathbf{p}}_{i}$ are needed to solve it, and each pair's relationship can be formulated as
	\begin{small}
		\begin{equation}
			{\left[ {\begin{array}{*{20}{c}}
						{{}^M{p_{i,x}}}&0\\
						{{}^M{p_{i,y}}}&0\\
						1&0\\
						0&{{}^M{p_{i,x}}}\\
						0&{{}^M{p_{i,y}}}\\
						0&1\\
						{ - {}^M{p_{i,x}}{}^{I}{p_{i,u}}}&{ - {}^M{p_{i,x}}{}^{I}{p_{i,v}}}\\
						{ - {}^M{p_{i,y}}{}^{I}{p_{i,u}}}&{ - {}^M{p_{i,y}}{}^{I}{p_{i,v}}}
				\end{array}} \right]^{\rm T}} \, \left[ {\begin{array}{*{20}{c}}
					{{h_{11}}}\\
					{{h_{12}}}\\
					{{h_{13}}}\\
					{{h_{21}}}\\
					{{h_{22}}}\\
					{{h_{23}}}\\
					{{h_{31}}}\\
					{{h_{32}}}
			\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
					{{}^{I}{p_{i,u}}}\\
					{{}^{I}{p_{i,v}}}
			\end{array}} \right] \, ,
			\label{equation:homography}
		\end{equation}
	\end{small}
where $({}^M{p_{i,x}},{}^M{p_{i,y}})$ and $({}^{I}{p_{i,u}},{}^{I}{p_{i,v}})$ are coordinates of the $i$-th marker in the frame \{\textit{M}\} and its projection in the frame \{\textit{I}\}.
	%Then we can further get $\frac{t_z}{z}$ for each pair of $^{m}{\mathbf{p}}_{h_i}$ and $^{Im}{\mathbf{p}}_{h_i}$ as:
	%\begin{equation}
	%	\frac{t_z}{z} = \frac{1}{h_{31}x_i + h_{32}y_i+1} \, .
	%	\label{equation:t_z/z}
	%\end{equation}
	Based on this relationship and four pairs of projections, a set of equations can be established to solve $\mathbf{H}$.
	Substituting the solved $\mathbf{H}$ into (\ref{equation:RandT}), ${}^{C}_{M}\mathbf{T}$ can be further obtained.
	%Then, the current marker pose ${}^{m_c}_{m_o}\mathbf{T}$ can be get by submitting ${}^{m_o}_{c}\mathbf{T}$ and  ${}^{m_c}_{c}\mathbf{T}$ into (\ref{equation:markerpose}). 
	%\begin{footnotesize}
	%	\begin{equation*}
		%		\begin{array}{l}
			%			\left[ {\begin{array}{*{20}{c}}
					%					{{}^m{p_{i,x}}}&{{}^m{p_{i,y}}}&1&0&0&0&{ - {}^m{p_{i,x}}{}^{Im}{p_{i,x}}}&{ - {}^m{p_{i,y}}{}^{Im}{p_{i,x}}}\\
					%					0&0&0&{{p_{i,x}}}&{{p_{i,y}}}&1&{ - {}^m{p_{i,x}}{}^{Im}{p_{i,y}}}&{ - {}^m{p_{i,y}}{}^{Im}{p_{i,y}}}
					%			\end{array}} \right] \times \\
			%			{\left[ {\begin{array}{*{20}{c}}
						%						{{h_{11}}}&{{h_{12}}}&{{h_{13}}}&{{h_{21}}}&{{h_{22}}}&{{h_{23}}}&{{h_{31}}}&{{h_{32}}}
						%				\end{array}} \right]^{\rm T}} = \left[ {\begin{array}{*{20}{c}}
					%					{{}^{Im}{p_{i,x}}}\\
					%					{{}^{Im}{p_{i,y}}}
					%			\end{array}} \right] \, ,
			%		\end{array}
		%		\label{equation:homography}
		%	\end{equation*}
	%\end{footnotesize}
	%\begin{scriptsize}
	%\begin{equation}
	%	\left[ {\mathop {\begin{array}{*{20}{c}}
				%				{x_1}&{y_1}&1&0&0&0&{ - {x_1}{x_1}'}&{ - {y_1}{x_1}'}\\
				%				0&0&0&{{x_1}}&{{y_1}}&1&{ - {x_1}{y_1}'}&{ - {y_1}{y_1}'}\\
				%				{{x_2}}&{{y_2}}&1&0&0&0&{ - {x_2}{x_2}'}&{ - {y_2}{x_2}'}\\
				%				0&0&0&{{x_2}}&{{y_2}}&1&{ - {x_2}{y_2}'}&{ - {y_2}{y_2}'}\\
				%				{{x_3}}&{{y_3}}&1&0&0&0&{ - {x_3}{x_3}'}&{ - {y_3}{x_3}'}\\
				%				0&0&0&{{x_3}}&{{y_3}}&1&{ - {x_3}{y_3}'}&{ - {y_3}{y_3}'}\\
				%				{{x_4}}&{{y_4}}&1&0&0&0&{ - {x_4}{x_4}'}&{ - {y_4}{y_4}'}\\
				%				0&0&0&{{x_4}}&{{y_4}}&1&{ - {x_4}{y_4}'}&{ - {y_4}{y_4}'}\\
				%				& & & &\vdots& & &
				%	\end{array}} } \right]\left[ {\begin{array}{*{20}{c}}
			%			{{h_{11}}}\\
			%			{{h_{12}}}\\
			%			{{h_{13}}}\\
			%			{{h_{21}}}\\
			%			{{h_{22}}}\\
			%			{{h_{23}}}\\
			%			{{h_{31}}}\\
			%			{{h_{32}}}
			%	\end{array}} \right] \\ 
	%	= \left[ {\mathop {\begin{array}{*{20}{c}}
				%				{{x_1}'}\\
				%				{{y_1}'}\\
				%				{{x_2}'}\\
				%				{{y_2}'}\\
				%				{{x_3}'}\\
				%				{{y_3}'}\\
				%				{{x_4}'}\\
				%				{{y_4}'}\\
				%				\vdots
				%	\end{array}}   } \right]  \, ,
	%	\label{equation:homography}
	%\end{equation}
	%\end{scriptsize} 
	
	\subsubsection{Marker Image Registration}
	Ideally, through substituting four detected $^{I}{\mathbf{p}}_{i}$ and their corresponding $^{M}{\mathbf{p}}_{i}$ into (\ref{equation:homography}) we can calculate the homography matrix $\mathbf{H}$ for each state.
	However, under external forces, the pose variation of the target causes the occlusion or halation of some markers and results in wrong correspondence between the image and the marker.
 %as the target will move when an external force applies to the sensing module, the order of the detection may change, and some markers may be lost-tracked.
	As a result, the corresponding relationship between $^{I}{\mathbf{p}}_{i}$ and $^{M}{\mathbf{p}}_{i}$ changes over time, which invalidates the direct use of (\ref{equation:homography}).
	Therefore, a registration process is required to label the currently detected markers' pixel positions $^{I}{\mathbf{P}}^{t}$ to their original indexes, as shown in Fig. \ref{Figure:pose_estimation}(c), and we define this registered result as $^{I}{\mathbf{P}}^{t,r}$. %In the following description, we take the case shown in Fig. \ref{Figure:pose_estimation}(d and e) as an example of the currently detected markers, while noting that the derivation applies to an arbitrary situation.
    
	At the initial state $t = 0$, the target is installed perpendicularly to the optical axis.
	For the prototype we built, all the markers can be detected except the 11$^{th}$ marker (top marker) that is occluded by the channel, as shown in Fig. \ref{Figure:pose_estimation}(a).
	Therefore, we define $^{M}{\mathbf{P}^0}$ by removing the 11$^{th}$ marker from $^{M}{\mathbf{P}}$ as the marker set at the initial state, and the projection $\mathbf{H}^{0}$ between $^{M}{\mathbf{P}^0}$ and $^{I}{\mathbf{P}}^{0}$ can be obtained according to (\ref{equation:homography}). 
	Substituting $^{M}{\mathbf{p}}_{11}$ and $\mathbf{H}^{0}$ back to (\ref{equation:homography}), we can estimate $^{I}{\mathbf{p}}^{0}_{11}$.
	Then, the complete pixel-position array of the initial state $^{I}{\mathbf{P}}^{0,c}$ can be obtained by appending $^{I}{\mathbf{p}}^{0}_{11}$ to $^{I}{\mathbf{P}}^{0}$.
    % $^{I}{\mathbf{P}}^{0,c}$ is used as the reference in the registration at the next state.  
	
	\begin{algorithm}[t]
		\begin{algorithmic}[1]
			\Require
			$^{M}{\mathbf{P}}$, $\mathbf{A}$, $\mathbf{K}_s$ and \textit{image}
			\Ensure
			${}^{M_t}_{M_0}\mathbf{T}$ and $\mathbf{f}_s$
			%		\State Image capture and filtering
            \State set $t = 0$;
			\State get $^{I}{\mathbf{P}}^{t}$ via blob detection based on \textit{image};
            \State get $^{M}{\mathbf{P}}^{t}$ by removing $^{M}{\mathbf{p}_{n-1}}$ from $^{M}{\mathbf{P}}$;
			\State calculate $\mathbf{H}^{t}$ by substituting $^{I}{\mathbf{P}}^{t}$ and $^{M}{\mathbf{P}}^{t}$ to (\ref{equation:homography});
			\State estimate $^{I}{\mathbf{P}}^{0}_{n-1}$ by substituting $^{M}{\mathbf{p}}_{n-1}$ and $\mathbf{H}^{0}$ to (\ref{equation:homography});
			\State get $^{I}{\mathbf{P}}^{t,c}$ by appending $^{I}{\mathbf{P}}^{t}_{n-1}$ to $^{I}{\mathbf{P}}^{t}$;
            \State calculate ${}^{C}_{M_0}\mathbf{T}$ by substituting $\mathbf{H}^{t}$ to (\ref{equation:RandT});
			%		\For{$i = 1; i< lens(^{Im}{\mathbf{P}}^{or})+1; i++$} 		
			%		\State register the current detected holes according to the vector matrix 
			%		\EndFor		
			%		\State Calculate $\mathbf{H}^{or}$ by submitting $^{c_o}{\mathbf{P}}^{re}$ and $^{m_o}{\mathbf{P}}^{re}$ to (\ref{equation:homography})
			%		\State Estimate ${}^{m_o}_{c_o}\mathbf{T}$ by submitting $\mathbf{H}^{or}$ to (\ref{equation:RandT})
			%		\State Get$^{c_o}{\mathbf{P}}^{und}$ by submitting $^{m_o}{\mathbf{P}}^{und}$ to (\ref{equation:projection})
			%		\State Get $^{c_o}{\mathbf{P}}^{com}$ by merging $^{c_o}{\mathbf{P}}^{re}$ and $^{c_o}{\mathbf{P}}^{und}$
			
			\While{\textit{frame}}
            \State $t$++;
			\State get $^{I}{\mathbf{P}}^{t}$ via blob detection based on \textit{image};
			\For{$i = 0; i< col(^{I}{\mathbf{P}}^{t}); i++$}; 
			\State $e'$ = infinity; $j^{*}$ = infinity;
			\For{$j = 0; j< col(^{I}{\mathbf{P}}^{t-1,c}); j++$};
			\State $e = ||^{I}{\mathbf{p}}^{t}_{i} - ^{I}{\mathbf{p}}^{t-1,c}_{j}||_1$;
			\If{$e<e'$}
			\State $j^{*}$ = j; $e'$ = $e$;
			\Else
			\State $e'$ = $e$;
			\EndIf
			\EndFor 
			\If{$j^{*} \leq col(^{I}{\mathbf{P}}^{t-1,c})$}	
			\State $^{I}{\mathbf{p}}^{t,r}_{j^{*}} \leftarrow ^{I}{\mathbf{p}}^{t}_{i}$;	
			\EndIf	
			\EndFor
            \State get corresponding $^{M}{\mathbf{P}^t}$ of $^{I}{\mathbf{P}}^{t,r}$ from $^{M}{\mathbf{P}}$;
			\State calculate $\mathbf{H}^{t}$ by substituting $^{I}{\mathbf{P}}^{t,r}$ and $^{M}{\mathbf{P}^t}$ to (\ref{equation:homography});            
			% \State get $^{M}{\mathbf{P}}^{u}$ by removing $^{M}{\mathbf{P}}^{r}$ from $^{M}{\mathbf{P}}$; 
			\State get $^{I}{\mathbf{P}}^{t,e}$ by substituting $^{M}{\mathbf{P}}$ and $\mathbf{H}^{t}$ to (\ref{equation:projection});
			\State generate $^{I}{\mathbf{P}}^{t,c}$ by merging $^{I}{\mathbf{P}}^{t,r}$ with $^{I}{\mathbf{P}}^{t,e}$;
			\State calculate ${}^{C}_{M_t}\mathbf{T}$ by substituting $\mathbf{H}^{t}$ to (\ref{equation:RandT});
			\State calculate ${}^{M_0}_{M_t}\mathbf{T}$ by substituting ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$ to (\ref{equation:markerpose});
			\State calculate $\mathbf{f}_s$ by substituting ${}^{M_0}_{M_t}\mathbf{T}$ and $\mathbf{K}_s$ to (\ref{equation:Forcecalculation});
			%		\State Let $^{c_o}{\mathbf{P}}^{pre} = {}^{c_o}{\mathbf{P}}^{com}$
			\EndWhile
		\end{algorithmic}	
		\caption{Target pose and external force estimation}
		\label{Algorithm:Force_Calculation}
	\end{algorithm}
	
	For the current state $t$, the pixel positions of the detected markers $^{I}{\mathbf{P}}^{t} \in \mathbb{R}^{2\times m}$ ($m$ is the number of detected markers, $m\leqslant n$) are returned by the blob detection.
	Nevertheless, these detected markers are directly labelled from bottom to top on the image.
    For example, the blob detection result of a random pose is shown in Fig. \ref{Figure:pose_estimation}(d). 
	Assuming $^{I}{\mathbf{P}}^{t-1,c}$ is known, we calculate the L1 norm of the distance vector between each $^{I}{\mathbf{p}}^{t}_{i}$ and $^{I}{\mathbf{p}}^{t-1,c}_{j}$, $i \in [1,m]$ and $j \in [1,n]$, and define it as $e$.
	%\begin{equation}
	%	e = |^{Im}{\mathbf{P}}^{cur}_{h_i,u} - ^{Im}{\mathbf{P}}^{pre}_{h_j,u}| + |^{Im}{\mathbf{P}}^{cur}_{h_i,v} - ^{Im}{\mathbf{P}}^{pre}_{h_j,v}|\, ,
	%	\label{equation:registration} 
	%\end{equation}
	%where $i \in [1,m]$ and $j \in [1,12]$. 
	The corresponding marker of $^{I}{\mathbf{p}}^{t}_{i}$ in $^{I}{\mathbf{P}}^{t-1,c}$ is the the one that results the minimal $e$, and its index is the desired $j^*$.
	Without loss of generality, the registration problem for the detected \textit{i}-th hole is defined as
	\begin{equation}
		j^{*} = \arg \min_j ||^{I}{\mathbf{p}}^{t}_{i} - \, ^{I}{\mathbf{p}}^{t-1,c}_{j}||_1\, .
		\label{equation:registration} 
	\end{equation} 
 %where $j^{*}$ is the index of the marker that needs to be updated.
	%Then, let $^{Im}{\mathbf{P}}^{reg}_{h_{index}} = [^{Im}{\mathbf{P}}^{cur}_{h_i,u}, ^{Im}{\mathbf{P}}^{cur}_{h_i,v}]$.
	Repeating this registration for each detected marker $^{I}{\mathbf{p}}^{t}_{i}$ and updating $^{I}{\mathbf{p}}^{t,r}_{j^{*}} \leftarrow {}^{I}{\mathbf{p}}^{t}_{i}$, we can obtain $^{I}{\mathbf{P}}^{t,r}$ with the original indexes.
	%Finally, let $^{Im}{\mathbf{P}}^{cur} = ^{Im}{\mathbf{P}}^{reg}$, we can get the registered result of current detected holes and label them with original index.
    %%%%%%%%%%%%%%%%%%%%
    % and substitute it to (\ref{equation:homography}) to get $\mathbf{H}^t$. 
    % Then, by substituting undetected markers' coordinates to $\mathbf{H}^t$, we can estimate their corresponding pixel positions.
    % Merging the estimated result with $^{I}{\mathbf{P}}^{t,r}$, we can further obtain the complete pixel-position set $^{I}{\mathbf{P}}^{t,c}$ of the current state, which is used as the reference for next registration. 
    % Then, the complete pixel-position set $^{I}{\mathbf{P}}^{t,c}$ of each state can be obtained by estimating pixel coordinates of undetected markers via substituting $\mathbf{H}^t$ (\ref{equation:homography}) and merging them to $^{I}{\mathbf{P}}^{t,r}$.
    We define the corresponding marker position set of $^{I}{\mathbf{P}}^{t,r}$ as $^{M}{\mathbf{P}}^{t}$.
    Substituting $^{I}{\mathbf{P}}^{t,r}$ and $^{M}{\mathbf{P}}^{t}$ to (\ref{equation:homography}), the homography matrix of the current state $\mathbf{H}^{t}$ can be calculated.
	%with the registered $^{Im}{\mathbf{P}}^{reg}$ and its corresponding holes' original position $^{m_o}{\mathbf{P}}^{reg}$ by (\ref{equation:homography}).
 
	To ensure the image of the current state can be used as the reference for the next registration, i.e., the $t$+1 state, the complete pixel-position set $^{I}{\mathbf{P}}^{t,c}$ of the current state also should be generated. 
 % for reordering the holes' index in the next captured frame, the lost-tracked holes' pixel positions need to be estimated and filled into $^{I}{\mathbf{P}}^{c}$.
    By substituting undetected markers' coordinates in $^{M}{\mathbf{P}}$ and $\mathbf{H}^t$ to (\ref{equation:homography}), we can estimate their corresponding pixel positions $^{I}{\mathbf{P}}^{t,e}$.
    Then, $^{I}{\mathbf{P}}^{t,c}$ can be obtained by merging $^{I}{\mathbf{P}}^{t,e}$ with $^{I}{\mathbf{P}}^{t,r}$.
	% By removing $^{M}{\mathbf{P}}^{r}$ from $^{M}{\mathbf{P}}$, we can obtain positions of undetected holes $^{M}{\mathbf{P}}^{u}$.
	% Then, substituting $^{M}{\mathbf{P}}^{u}$ and $\mathbf{H}^{c}$ to (\ref{equation:projection}), we can estimate the pixel positions of these undetected holes.  
	%As we have $\mathbf{H}^{cur}$, the index and positions $^{m}{\mathbf{P}}^{und}$, by removing $^{m}{\mathbf{P}}^{reg}$ from $^{m}{\mathbf{P}}$, of undetected holes, the pixel positions of the undetected holes can be estimated by (\ref{equation:projection}).
	% Let this estimation result be $^{I}{\mathbf{P}}^{e}$ and by merging it with $^{I}{\mathbf{P}}^{r}$, we can further obtain the complete $^{I}{\mathbf{P}}^{c}$.
	For example, Fig. \ref{Figure:pose_estimation}(e) shows the registration result and the generated complete image of Fig. \ref{Figure:pose_estimation}(d).
	% This complete $^{I}{\mathbf{P}}^{c} \subseteq \mathbb{R}^{2\times n}$ is used as $^{I}{\mathbf{P}}^{p}$ in the registration of the next state. 
	
	\subsubsection{Felxure Deformation Estimation}
	%According to $^{Im}{\mathbf{P}}^{reg}$ and $^{m}{\mathbf{P}}^{reg}$,  $^{Im}{\mathbf{P}}^{or}$ and $^{m_o}{\mathbf{P}}^{or}$ the homographic matrix at current pose $\mathbf{H}^{cur}$ and orginal pose $\mathbf{H}^{or}$ can be calculated by (\ref{equation:homography}), respectively. 
	The transformation between the target and the camera at the initial state ${}^{C}_{M_0}\mathbf{T}$ can be calculated by substituting $^{I}{\mathbf{P}}^{0}$ and $\mathbf{H}^{0}$ into (\ref{equation:RandT}).
	Similarly, the transformation at the current state ${}^{C}_{M_t}\mathbf{T}$ can be obtained by substituting $^{I}{\mathbf{P}}^{t,r}$ and $\mathbf{H}^{t}$ into (\ref{equation:RandT}).
	Then, the transformation ${}^{M_0}_{M_t}\mathbf{T}$ can be calculated by substituting ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$ into (\ref{equation:markerpose}), which reflects the deformation of the flexure.
	%  then as we have the current homographic matrix and orginal homographic matrix, the mo_cc_T and mo_co_T can be calcualted by (4). Then the current pose of maker can also be gotten by (2)
	
	
	
	%% % % % % % % % % % % % Figure 1 % % % % % % % % % % % % %
	%\begin{figure}[t]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/marker_tracking.pdf}
	%	\caption{Registration results of different situations, where the top four pictures are the blob detection results, and the blow four are the corresponding registration of them. (a) shows the images at original pose. (b) shows the situation when some hole is not detected, while the registration keeps the original order. (c) shows the situation when the marker is rotated around it $z$ axis, where the order of detected holes has changed, and the registration reorder them to the original one. (d) shows the situation when the marker moves forward and backward to the camera.}
	%	\label{Figure:marker_tracking}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	\subsubsection{Force Calculation}
	%With the transformation ${}^{m_c}_{m_o}\mathbf{T}$, 
	The displacement of the spring $\mathbf{d}_s$ is the translation vector of ${}^{M_0}_{M_t}\mathbf{T}$.
	Substituting $\mathbf{d}_s$ and the spring's stiffness matrix $\mathbf{K}_s$ into (\ref{equation:Forcecalculation}), we can get the external force $\mathbf{f}_s$.
	Ideally, the stiffness matrix $\mathbf{K}_s$ represents the spring's characteristics that connect the camera and the target. 
	%The the external force/torque exerted on the forceps can be calculated by (\ref{equation:Forcecalculation}).
	However, this stiffness matrix $\mathbf{K}_s$ should also consider the influence of wires and shelter, as they are parallel to the spring.
	In order to get the practical stiffness matrix $\mathbf{K}_s$, a calibration is carried out on the prototype and detailed in section \ref{Section:experiments}. 
	%\textit{ATI nano43} which has 1/520 N and 1/40 Nmm resolution, as the reference and collected series of data to calculate the elements of $K_s$.
	Finally, the above process of pose and force estimations is summarized in Algorithm \ref{Algorithm:Force_Calculation}.
	
	
	
	%\subsubsection{Force/Torque Calculation}
	%After we get the marker pose and stiffness matrix, the external contact forces/torques applied to the forceps can be calculated
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	
	%With the estimated external forces/torques, the grasping force can be calculated by FFFFFFFFF.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%               haptic-enabled forceps            %
	%												  %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Haptics-enabled Forceps}
	\label{Section:forceps}
	%Why we want to develope the haptic-enabled forceps?
	%How can we detect the external forces and torques applied on the forceps?\\
	%How can we estimate the grasping forces?\\
	%How can we measure the pulling forces?
	The developed sensing module can be applied to various applications thanks to its micro size and reserved channel.
	Here, we consider tissue manipulation where the developed sensing module is used to enable the multi-modal force sensing of the forceps.
	We adopt a pair of biopsy forceps as the instrument for demonstration.
	Integrating the sensing module into the forceps, we propose a pair of haptics-enabled forceps in this section.
	%The design and force estimation of the forceps are described in the following.
	
	
	\subsection{Structure of Haptics-Enabled Forceps}
	\label{Subsection:forceps_design}
	Thanks to the cylindrical structure and the sensing module's central channel, it can be easily integrated with the forceps.  
	To drive the forceps, a micro-level actuator is designed, as shown in Fig. \ref{Figure:forcpes}(a), which consists of upper and lower linear drivers that are responsible for grasping and pulling, respectively.
	The lower driver also compensates for the motion introduced by the flexure's deformation when the upper driver drives the forceps to grasp tissue.
	The compensation is achieved by moving the lower driver's carrier for the same amount of motion as the upper driver but in the reverse direction. 
	A commercial single-axis force sensor (ZNLBS-5kg, CHINO, China), with a resolution of 0.015N, is installed collinearly to the forceps and connected to the driving cable to measure the driving force.
	%% % % % % % % % % % % % Figure linear moudule % % % % % % % % % % % % %
	%\begin{figure}[t!]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/linear_module.pdf}
	%	\caption{(a) The setup of dynamic experiment. FFFFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFF. 
		%		(b) The controller block diagram of the robotic system. FFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFFF.}
	%	\label{Figure:Linear_Module}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	The proposed haptics-enabled forceps can be easily integrated into a robotic surgical system, such as the robot presented in Fig. \ref{Figure:Introduction}.
	
	The structure of the haptics-enabled forceps is shown in Fig. \ref{Figure:forcpes}(b).
	The base of the forceps is installed concentrically to the sensing module's head, and the driving cable passes through the central channel.
	The cylindrical base connects the forceps to its carrying instrument.   
	A prototype is shown in the inset of Fig. \ref{Figure:forcpes}(b), which has a 4mm diameter and 22mm total length.
	The experiments in Section \ref{Section:experiments} are also based on this prototype.
	%This prototype is also used for experiments in Section \ref{Section:experiments}.
	%The method for grasping and pulling force estimation are illustrated in the following.
	
	% % % % % % % % % % % % Figure forceps integration % % % % % % % % % % % % %
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Forceps_Design_Prototype.pdf}
		\caption{ (a) The micro-level actuator of the forceps, where the upper and lower linear drivers are responsible for grasping and pulling, respectively.
			The lower driver also compensates for the motion introduced by the flexure's deformation when the upper driver grasps tissue.
			A commercial force sensor is adopted to measure the force applied to the driving cable.	
			(b) The sectional view of the haptics-enabled forceps, which shows the forceps are mounted on the sensing module's head and the driving cable passes through the reserved channel.
			The forces applied to the forceps when it grasps a tissue include driving force $F_d$ from cable, supporting force $F_s$ from the sensing module, contact force $F_N$ and friction force $F_f$ from the tissue.
			$F_g$ denotes the grasping force that is equal to the sum of vertical components of $F_N$ and $F_f$.  
			The left inset shows the forces applied to the grasped tissue, where $F_p$ is the pulling force from the tissue body, $F^\prime_N = F_N $ and $F^\prime_f = F_f$ are forces from the forceps.
			The middle inset shows the forces that generate the momentum of one clip about joint $j_2$.
			The right inset presents a prototype, and it is also used in Section. \ref{Section:experiments} to conduct the experiments.
			(c) The upper shows the forceps' geometry in the initial state, where the two jaws are closed. 
			The lower shows a state when the two jaws are open at $\theta$.
            $t_d$ is the movement of the driving cable, while $t_s$ is the transformation of the sensing module's head estimated by the camera. 
		}
		\label{Figure:forcpes}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	\subsection{Grasping and Pulling Force Estimation}
	\label{Subsection:forceps_grasping}
	
	%The grasping force can be calculated by multiplying a jacobian matrix $J$ on the drag force of its hinge joint, where the $J$ matrix indicates the transformation form the forceps center to its hinge joint.
	%The dragging force applied on the tissue by forceps can be gotten through the deflection of spring and its parameters matrix $k$.
	%The force applied on the hinge joint can be calculated by decoupling the spring force form the total drag force inputted at the handle side.
	%And the deflection estimation of spring in 3D on Cartesian space is achieved through tracking a series of marker by a micro camera.
	%With the contact force between the tissue and forceps, hybrid motion force control of the concentric tube can be achieved.
	%This allows the concentric tube navigate in a long narrow cavity such as nasal and esophagus while avoiding the tissue damage. 
	%In addition, those multiple functions can also reduce the instrument exchange, which is also a crucial clinical need \cite{diaz2017research}.
	%With a simple structure our devices can be easily customized for varied tasks, which have different characteristics like force regulation demands and complexity.
	
	%% % % % % % % % % % % % Figure forceps thetha calculation % % % % % % % % % % % % %
	%\begin{figure}[t]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/Theta_calculation.png}
	%	\caption{(a) Forceps' geometry in the initial state. (b) Forceps' geometry when the forceps is formed in $\theta$.
		%	}
	%	\label{Figure:theta}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	%here i should emphasis the advantage of the proposed sensor, as it can esitmate the Fj, which is the indispensable variable to calculate the grasping force.
	%I can provide a figure to illustrate why the grasping forces of traditional forceps can not be measured even when it is integrated with a external force sensor. 
	%The proposed forceps are able to measure the contact force with tissue and estimate the grasping force when it grasps, as shown in Fig. \ref{Figure:forcpes}(b).
	The contact force when the forceps touch tissue in the closed form can be directly estimated via (\ref{equation:Forcecalculation}).
	However, the estimation is more challenging when the forceps grasp and pull tissues because it relates to the geometry and driving force of the forceps, as depicted in Fig. \ref{Figure:forcpes}(b) and (c).

	The forces applied to the forceps when they grasp a tissue are shown in Fig. \ref{Figure:forcpes}(b).
    For convenience and concision, we adopt $F_{-}$ to denote the magnitude of force $\mathbf{f}_{-}$ in the following sections.
	%As the grasping and pulling force $F_g$ is significant for tissue manipulation, it should be estimated for supervising and decision making when the forceps grasps tissue.
	$F_s$ is the elastic force from the flexure, and it can be directly estimated by (\ref{equation:Forcecalculation}).
	$F_d$ is the driving force applied to the forceps' hinge, which is provided by the driving cable and measured by the commercial force sensor.
	$F_N$ and $F_f$ are contact force and friction force from the grasped tissue, and we define the sum of the vertical components of $F_N$ and $F_f$ as the grasping force $F_g$, as calculated in
	\begin{equation}
		{F}_{g} = F_{N_1} \cos{\frac{\theta}{2}} + F_{f_1} \sin{\frac{\theta}{2}} \, ,
		\label{equation:fg_def} 
	\end{equation}
	where $\theta$ is the angle between the forceps' two clips.
	According to the equilibrium equation on the grasped tissue, the relationship between $F_N$, $F_f$, and external pulling force $F_p$ can be formulated as
	\begin{align}
		\left\{	
		\begin{array}{l}
			F_p=(F_{f_1} + F_{f_2})\cos{\frac{\theta}{2}}-(F_{N_1} + F_{N_2})\sin{\frac{\theta}{2}}\\
			F_{N_1}\cos{\frac{\theta}{2}} + F_{f_1} \sin{\frac{\theta}{2}} = F_{N_2}\cos{\frac{\theta}{2}} + F_{f_2} \sin{\frac{\theta}{2}}
		\end{array} \right. \,.
		\label{equation:tissue_balance}
	\end{align}
	
	Since the momentum of one clip about joint $j_2$ is zero, as plotted in the mid inset of Fig. \ref{Figure:forcpes}(b), we can further obtain the relationship between $F_N$ and $F_d$ as 
	\begin{align}
		F_{N_1} = \frac{F_d \, l_2 \sin{\alpha}}{2l_3}\, ,
		\label{equation:F_N}
	\end{align}
	where $\alpha = \frac{\theta}{2} + \alpha_0$, the definitions of $\alpha$, $\alpha_0$, $l_2$ and $l_3$ are shown in Fig. \ref{Figure:forcpes}(c).
	
	$\theta$ and $\alpha_0$ can be calculated based on the geometry relationship of the forceps' links.
	The upper sketch of Fig. \ref{Figure:forcpes}(c) shows the forceps' geometry at the initial state, i.e., the forceps are in the closed form, and the flexure is relaxed.
	According to the relationship of forceps' links at the initial state, we can get $\alpha_0$, which can be formulated as
	\begin{equation}
		\alpha_0  = \arccos \frac{{l_1^2 - l_2^2 - l_{1,2}^2}}{{2{l_1}{\,}{l_{1,2}}}} \, ,
	\end{equation}
	where $l_{1,2}$ is the distance between forceps' two joints $j_1$ and $j_2$.
	When the forceps are open at angle $\theta$, the geometry changes to Fig. \ref{Figure:forcpes}(c)'s lower sketch.
	At this stage, the angle ${\alpha}$ can be reformulated as
	\begin{equation}
		{\alpha} = \arccos \frac{{l_1^2 - l_2^2 - l{{_{1,2}^{\prime2}}}}}{{2{l_1}\, {l_{1,2}^\prime}}} \, ,
	\end{equation}
	where $ l{_{1,2}^\prime} = l{_{1,2}} - {t}_d + {t}_s$,
	$t_d$ is the movement of the driving cable,
	and $t_s$ is the transformation of the sensor's head estimated by the camera. 
	Then, we can get $\theta$ by
	\begin{equation}
		\theta = 2(\alpha-\alpha_0) \,.
        \label{euquation:theta}
	\end{equation}
	For a pair of forceps, we assume $F_{N_1}$ equals $F_{N_2}$, and $F_{f_1}$ equals $F_{f_2}$.
	Substituting (\ref{equation:F_N}), (\ref{equation:tissue_balance}), and (\ref{euquation:theta}) into (\ref{equation:fg_def}) we can get ${F}_{g}$.
	Then, using the equilibrium equation between $F_p$, $F_s$, and $F_d$, we can get the wanted pulling and grasping forces as
	
        \begin{align}
		\left\{\begin{array}{l}
            F_{p} = F_d-F_s\\
		  {F}_{g}= \frac{{F}_d{\,}{l_2}{\,}{\sin\alpha}+F_p{\,}l_3{\,}\sin(\alpha-\alpha_0)}{2{l_3}{\,}{\cos(\alpha-\alpha_0)}}	
        \end{array}
		\right. \, .
		\label{equation:grasping_force}	
	\end{align}


	
	%After the grasping, the tissue can be pulled up, and a pulling force $\mathbf{f}_p$ against the driving force $\mathbf{f}_d$ will be applied to clips.
	

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%                 robotic integration             %
	%												  %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\\subsection{Micro-level Actuator}
	%\\label{Section:Micro-level_Actuator}
	%\subsubsection{Micro-level Actuator}
	%\label{Subsectsubion:Micro-level_Actuator}
	%\When the forceps perform grasping, the sensor's head will move backward due to the movement introduced by the flexure.
	%\This disturbance may reduce grasping quality and invalidate the proposed method (\ref{equation:grasping_force}).
	%\To mimic this disturbance during grasping, we designed a micro-level actuator to compensate for the movement on the sensor's head.
	
	%\The micro-level actuator consists of upper and lower linear drivers, shown in Fig. \ref{Figure:forcpes}(c).
	%\The first is responsible for grasping and controlling the force $F_s$, and the second is responsible for pulling and controlling the force $F_p$.
	%\The lower driver is also in the charge of compensating for the motion introduced by the flexure's deformation when the upper driver drives the forceps to grasp tissue.
	%\This is achieved by driving the upper linear diver and forceps against the motion provided by the upper driver. 
	%\A commercial single-axis force sensor is installed concentrically to the forceps and is connected with the driving cable to measure the driving force $F_d$.
	%% % % % % % % % % % % % Figure linear moudule % % % % % % % % % % % % %
	%\begin{figure}[t!]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/linear_module.pdf}
	%	\caption{(a) The setup of dynamic experiment. FFFFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFF. 
		%		(b) The controller block diagram of the robotic system. FFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFFF.}
	%	\label{Figure:Linear_Module}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	%\The proposed haptics-enabled sensor can be easily integrated into a robotic surgical system, such as the robot shown in Fig. \ref{Figure:Introduction}.
	
	%\subsection{Control diagram}
	%\label{Subsection:robotic_control}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%															                      % 
	%       		            													  %
	%   				         	experiments   					      		      %
	%															                      % 
	%															                      % 
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
	\section{Experiments}
	\label{Section:experiments}
    In this section, we first present the experiments on evaluating the algorithm for target tracking and calibrating the stiffness matrix for external force estimation.
	%In addition to force, the marker's pose is vital for the instrument's kinematic model when integrated with the sensor on its tip.
	%In this section, before we calibrate the stiffness matrix for external forces calculation, the maker's pose and position estimations are analyzed to evaluate their feasibility.
	Then, the evaluation of grasping and pulling forces is described, followed by an automatic robotic grasping procedure with the proposed system.
	All the experiments were based on the prototypes presented in Fig. \ref{Figure:Sensor_Calculation_Design} and Fig. \ref{Figure:forcpes}.
	% The experimental setups and results are illustrated in the following.

	\subsection{Evaluation of Target's Pose Estimation}
	\label{subsection:pose_position_evaluation}
	\subsubsection{Setup for Pose Estimation Evaluation}
	Fig. \ref{Figure:EM_tracking}(a) shows the experimental setup for pose estimation evaluation with an electromagnetic (EM) tracking system (Northern Digital Inc, Canada) with resolutions less than 0.1mm in position and 0.1$^\circ$ in orientation.
	The EM tracking sensor was installed concentrically to the proposed force sensing module. 
	During the experiment, the sensor's head was moved along $x$ (red arrow), $y$ (green arrow), and $z$ (blue arrow) as indicated in Fig. \ref{Figure:EM_tracking}(b).
	
	\subsubsection{Pose Estimation Results}
	The orientation comparison is based on nine pairs of samples distributed in the sensing module's workspace, as plotted in Fig. \ref{Figure:EM_tracking}(c).
	Here, we use ($\alpha_{C,i}$, $\beta_{C,i}$, $\gamma_{C,i}$) and ($\alpha_{E,i}$, $\beta_{E,i}$, $\gamma_{E,i}$) to denote the pitch, roll, and yaw angles of camera estimation and EM tracking results of the $i$-th pair.
	The average and maximum deviations between EM tracking and our estimation result are calculated by $\max \frac{1}{n}({\sum\limits_{i = 1}^n {{\alpha_{C,i}} - {\alpha_{E,i}}}} \, ,{\sum\limits_{i = 1}^n {{\beta_{C,i}} - {\beta_{E,i}}}} \, , {\sum\limits_{i = 1}^n {{\gamma_{C,i}} - {\gamma _{E,i}}} })$ and $\max( \max\limits_{i = 1}^n({\alpha_{C,i}} - {\alpha_{E,i}})\, ,\max\limits_{i = 1}^n({\beta_{C,i}} - {\beta_{E,i}}) \, ,\max\limits_{i = 1}^n({\gamma_{C,i}} - {\gamma_{E,i}}))$, where $n=9$.
	For position evaluation, the EM sensor and target were tracked continuously by the EM tracking system and camera, respectively, and the comparison is shown in Fig. \ref{Figure:EM_tracking}(d). 
	To calculate the average and max deviations between the EM and camera-tracking traces, we further calculated the average ($d_a$) and Hausdorff ($d_h$) distance by
	\begin{align}
		\left\{\begin{array}{l}
			d_a = \max(\frac{1}{n}\sum\limits_{i = 1}^n {{d_{{C_i},E}}} \, ,\frac{1}{m}\sum\limits_{j = 1}^m {{d_{{E_j},C}}})\\
			{d_h} = \max (\mathop {\max }\limits_{i = 1}^n ({d_{C_i,E}}),\mathop {\max }\limits_{j = 1}^m ({d_{E_j,C}}))
		\end{array}
		\right. \, ,
		\label{equation:hausdorff_distance}	
	\end{align}
	where ${d_{{C_i},E}} = \mathop {\min }\limits_{j = 1}^m (||{\mathbf{p}_{C,i} - \mathbf{p}_{E,j}}||_2)$, 
 $\mathbf{p}_{C,i}$ and $\mathbf{p}_{E,j}$ denote positions of the camera estimation of point $i$ and EM tracking of point $j$, 
 ${d_{{E_j},C}} = \mathop {\min }\limits_{i = 1}^n (||{\mathbf{p}_{E,j} - \mathbf{p}_{C,i}}||_2)$,
    $n$ and $m$ are the numbers of points recorded by the camera and EM tracking system.
	As listed in Table \ref{Table:EM_tracking}, the average and maximum deviations of the orientation are 0.056rad and 0.147rad, and those of the position are 1.8$\times10^{-6}$mm and 0.2265mm.
	This indicates the proposed method can track and estimate the target's pose reliably.

    % % % % % % % % % % % % Figure Pose estimation % % % % % % % % % % % % %
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/EM_tracking.pdf}
		\caption{(a) The experimental setup for evaluating pose estimation.
			The inset shows the configuration, where the EM tracking sensor was installed concentrically to the target.
			(b) The sensor's head was moved towards $x$, $y$ and $z$ directions during the experiments, and $t_z$ denotes the transformation along $z$ axis. 
			(c) The orientation comparison between nine pairs of points that were estimated by the EM tracking system ($\star$) and camera ($\ast$). 
			(d) The position comparison between the continued EM tracking and camera estimation results.
		}
		\label{Figure:EM_tracking}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

	\begin{table}
		\centering
		\caption{Orientation and position evaluation comparison }	
		\begin{tabular}{ccc}
			\toprule
			Experiment & Average Deviation & Max Deviation\\ \toprule
			Orientation & 0.056rad & 0.147rad \\
			Position & 1.8233$\times10^{-6}$mm & 0.2265mm\\
			\hline
		\end{tabular}
		\label{Table:EM_tracking}
	\end{table}
 
	
	\subsection{Stiffness Matrix Calibration and Verification}
	\label{Experiments:stiffness_matrix}
	%A group of experiments will be carried out with 4mm, 5mm, and 6mm sensors, and compression will also be made based on these experiments.
	%Then a comparison between with and without integrated forceps will also be made.
	%Finally, we will show the grasping force measurement when the sensor is integrated with a pair of forceps.	
	\subsubsection{Experimental Setup}
	We used a series of weights to calibrate the sensing module integrated into the forceps.
	The calibration platform is shown in Fig. \ref{Figure:TFCalibration}(a).
	%It comprises the micro-level actuator and an orientation module.
	Here, the micro-level actuator is used to adjust the position and hold the forceps.
	An orientation module is installed concentrically to the forceps for adjusting the direction of the pulling force provided by a cable.
	This cable is connected to the forceps' clips, and the force $F_w$ applied to it is adjusted by adding/removing weights.
	%A linear movement platform was adopted as the based of the proposed sensor, which could move and rotated the sensor in different directions. 

 	% % % % % % % % % % % % Figure: calibration setup and verification % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/TFCalibration.pdf}
		\caption{(a) The experimental setup for stiffness matrix calibration.
			The forceps were pulled by a cable connected with changeable weights.
			The orientation module was used to adjust the pull direction of the forceps.
			(b) shows the setup for calibration data collection.
			(c) shows the setup for verification data collection, where the orientation module was rotated 45$^\circ$ relative to that for calibration.
			$v_1$, $v_2$, $v_3$, and $v_4$ indicate the pulling directions for verification in $x$-$y$ panel.
			(d) and (e) show the verification results in $x$-$y$ panel and $z$-axial direction, respectively.
			$F_w$ and $F_e$ denote the force generated by the weight and the estimated result. 
		}
		\label{Figure:TFCalibration}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 
  	% % % % % % % % % % % % Figure grasping setup % % % % % % % % % % % % %
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Grasping_setup.pdf}
		\caption{(a) The experimental setup of grasping and pulling force evaluation. 
			A commercial pressure sensor and a force sensor were used as references for grasping and pulling forces, respectively.
			The inset shows three 3D-printed contact surfaces configured with $\theta$ = $10^\circ$, $30^\circ$ and $50^\circ$. 
			(b) shows $F_d$ provided by the driving cable and $F_s$ measured by our proposed force sensing module.
			Subscript $i \in$(1,2,3) denotes the $i$-th experiment that correspond to $\theta = 10^\circ$, $30^\circ$ and $50^\circ$.
			The inset shows that grasping time varies for different $\theta$, and the red arrow shows the increased direction of $\theta$. 
			(c) The comparison of pulling force $F_p$, where $F_{p_{e,i}}$ and $F_{p_{r,i}}$ denotes the forces of the $i$-th experiment estimated by our proposed sensing module and that measured by the commercial sensor, respectively.
			(d) The comparison of grasping force $F_g$, where $F_{g_{e,i}}$ and $F_{g_{r,i}}$ denotes the forces of the $i$-th experiment estimated by our proposed sensing module and that measured by the reference commercial pressure sensor, respectively.
		}
		\label{Figure:Grasping_setup}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

	
	\subsubsection{Calibration and Verification}
	Two groups of data were collected for calibration and verification, respectively.
	When collecting the data for calibration in the $x$-$y$ panel, as shown in Fig. \ref{Figure:TFCalibration}(b), the orientation module made the cable align with the $x$ and $y$ axis, and the weight was increased from 0N to 0.49N at intervals of 0.07N.
	Then, for the $z$-axis direction, the driving force was increased from 0N to 5N at intervals of 0.5N.
	The target pose for each weight was recorded. 
	%\textbf{Some theoretical articles are also need to be cited in this section to support our calibration method. \cite{,}} 
	Then, substituting the calibration data to (\ref{equation:Forcecalculation}), we obtained the stiffness matrix $\mathbf{K}_s$ as
	\begin{align}
		\mathbf{K}_s = \left[ 
		{\begin{array}{*{20}{c}}
				{95.92}&{9.32}&{-1.84}\\
				{12.10}&{88.07}&{-1.70}\\
				{0.13}&{0.12}&{385.20}
		\end{array}} 
		\right] \, . \notag
		\label{equation:stiffness matrix}
	\end{align}
	
	To verify the calibrated $\mathbf{K}_s$, we rotated the orientation module for $45^\circ$, as shown in Fig. \ref{Figure:TFCalibration}(c), and collected the data following the same procedure of calibration.
	For the $x$-$y$ panel the interval was set to 0.035N, while for the $z$-axial direction the interval was 0.02N. 
	The comparison of weights and estimated forces are plotted in Fig. \ref{Figure:TFCalibration}(d) and (e), which show the results of the $x$-$y$ panel and the $z$-axial direction, respectively. 
	The average and maximum errors of ($f_x$, $f_y$, $f_z$) in these two comparison are ($0.0009$, $0.0013$, $0.0105$)N and ($0.0177$, $0.0154$, $0.0995$)N.
	This indicates the calibrated $\mathbf{K}_s$ can be used to estimate the force applied to the forceps.


	
	% % % % % % % % % % % % % Figure calibration verification % % % % % % % % % % % % %
	%\begin{figure}[t]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/CalibrationVerification.png}
	%	\caption{The verification of force calibration result.}
	%	\label{Figure:Calibration_verification}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

 
        \begin{table}
		\centering
		\caption{Errors of grasping and pulling forces estimations}	
		\begin{tabular}{ccccc}
			\toprule
			Force & Error & $10^\circ$ & $30^\circ$ & $50^\circ$ \\ \toprule
			$F_p$/N & \makecell{Average \\ Max}  & \makecell{0.0898\\0.2985} & \makecell{0.0138\\0.2782 } & \makecell{0.1144\\0.1803}\\ \hline
			$F_g$/N & \makecell{Average \\ Max} & \makecell{9.7853$\times10^{-4}$\\0.0280} &\makecell{0.0046\\0.0298}&\makecell{0.0049\\0.0229}\\
			\hline
		\end{tabular}
		\label{Table:Grasping_Pulling}
	\end{table} 
	
	\subsection{Evaluation of Grasping and Pulling Forces}
	\label{subsection:grasping_experiment}

	\subsubsection{Experimental Setup}
	Fig. \ref{Figure:Grasping_setup}(a) shows the experimental setup.
	A commercial pressure sensor (FlexiForce A201-1 lbs, Tekscan, USA), with a resolution of 0.02N, and a force sensor (ZNLBS-5kg, CHINO, China) were adopted as references.
	The pressure sensor was grasped by the forceps for direct measurement of the grasping force.
	Because the pressure sensor's sensing area was a 10mm-diameter circle, 3D-printed surfaces were attached to the forceps' clips to ensure sufficient contact.
	The driving force reached around 8N with a 2mm-axial deformation of the spring in each experiment.
	The pressure sensor was also connected to the commercial force sensor by a flexure to measure the pulling force applied to it.
	% in which the forceps performed grasping during 0-18s , and finally pulled the pressure sensor backward 25mm.
	% shows the setup.
	
	
	
	%To evaluate the estimation results with different $\theta$ configurations, three kinds of surfaces are printed with $\theta = 10^\circ$, $30^\circ$, and $50^\circ$, shown at the upper of Fig. \ref{Figure:Grasping_setup}(a).  
	%During the experiments, the micro-level actuator drives the forceps from the initial state to grasp and pull the pressure sensor.
	%The grasping force is controlled according to the flexure's deformation in our proposed sensor, reaching around 8N at each experiment.
	%And the pulling distance is around 25mm.
	
	%\begin{table}[t]
	%	\centering
	%	\caption{Force calibration result verification}	
	%	\begin{tabular}{ccccccc}
		%		\toprule
		%		Experiments & $f_x$/N & $f_y$/N & $f_z$/N \\ \toprule
		%		Average Error &0.008 &  0.004& 0.03 \\ 
		%		Max Error & &  & \\
		%		\hline
		%	\end{tabular}
	%	\label{Table:Calibration_result}
	%\end{table}
	
	
	\subsubsection{Evaluations}
	We carried out three experiments with different $\theta$ configurations.
	Each experiment was presented as a continuous process, and the results are shown in Fig. \ref{Figure:Grasping_setup}(b), (c), and (d), where the subscript $i\in$(1,2,3) denotes the $i$-th experiment that corresponds to  $\theta = 10^\circ$, $30^\circ$, and $50^\circ$.
	The forceps grasped the pressure sensor and reached the target grasping force during 0$\sim$18s.
	After a pause phase (18$\sim$26s), the forceps pulled the grasped sensor backward for 25mm in the pulling phase (26$\sim$65s). 
	%Here, three groups of experiments have been carried out to evaluate the estimation results with different $\theta$ configurations.
	%Here, we use $f_{g_{r,i}}$ to denote the $i$-th experiment's force measured by pressure sensor, and $f_{g_{e,i}}$ to denote our sensor's estimation result.
	%The results are shown in Fig. \ref{Figure:Grasping_setup}(b), (c), and (d), and the procedure can be divided into three phases: grasping, pause, and pulling.
	
	Fig. \ref{Figure:Grasping_setup}(b) depicts $F_d$ provided by the driving cable and $F_s$ measured by our proposed sensing module.
	%During the grasping phase, the micro-level actuator drives the forceps to a targeted axial displacement, 2mm at here.
	%The driving force $f_d$ and the sensor's supporting force $f_s$ increase until the procedure transforms to pause.
	$F_s$ and $F_d$ are almost equal in the grasping and pause phases.
	This is because the pulling force applied to the forceps was almost zero in these two phases, as there was no pulling action, and the pressure sensor was floating.
	On the contrary, in the pulling phase, $F_d$ increases while $F_s$ keeps constant.
	This is because the pressure sensor has been pulled, but the flexure deformation keeps constant. 
	The difference between $F_d$ and $F_s$ is the pulling force $F_p$ applied to the pressure sensor.
	As the three experiments followed the same procedure, their data almost overlay each other.
	However, because $\theta$ is configured differently, the time for performing the grasping is slightly different.
	The inset in Fig. \ref{Figure:Grasping_setup}(b) shows that the bigger $\theta$ is, the quicker the grasping finishes.
	
	
	Fig. \ref{Figure:Grasping_setup}(c) compares the estimated and the measured pulling forces $F_p$, where $F_{p_{e,i}}$ and $F_{p_{r,i}}$ denote the forces of the $i$-th experiment measured by our proposed sensing module and the reference commercial sensor, respectively.
	We can see that they almost overlay each other as the pulling distance is equal in these three experiments.
	%The average and maximum differences between the $f_{p_e}$ and $f_{p_r}$ are listed in Table \ref{Table:Grasping_Pulling}.
	A comparison between the commercial sensor results $F_{p_r}$ and our estimations $F_{p_e}$ is listed in Table \ref{Table:Grasping_Pulling}. 
	The average and maximum errors of the three experiments are under 0.11N and 0.3N, respectively.
	This indicates the forceps are valid for estimating the pulling force. 
	%0.299N and 0.090N for $10^\circ$, 0.278N and 0.014N for $30^\circ$, 0.1937N and -0.1055N for $50^\circ$. 
	

	
	Fig. \ref{Figure:Grasping_setup}(d) compares the grasping force $F_g$, where $F_{g_{e,i}}$ and $F_{g_{r,i}}$ denote the forces of $i$-th experiment estimated by our proposed sensing module and commercial pressure sensor, respectively.
	Because of the additional surfaces shown in the inlet of Fig. \ref{Figure:Grasping_setup}(a), $l_3$ in (\ref{equation:grasping_force}) was set as $l^{\prime_3}$ in these experiments.
	%the grasping force measured by the pressure force sensor was equal $\frac{l^{\prime_3}}{l_3}$ of the force applied on the forceps' clips.
	%For convenient comparison, we scaled the latter to the scale of the former.
	According to Fig. \ref{Figure:Grasping_setup}(d), we can see that when the driving force $F_d$ is equal, the grasping force $F_g$ is rational to $\theta$.
	This phenomenon conforms with the force calculation method (\ref{equation:grasping_force}).
	%A comparison between the commercial sensor results $f_{g_r}$ and our estimations $f_{g_e}$ is also listed in Table \ref{Table:Grasping_Pulling}. 
	%the bigger the $\theta$ is, the bigger the grasping force $f_g$ will be. 
	%This is verified through these three experiments.
	%Table \ref{Table:Grasping_Pulling} lists the average and maximum differences between the $f_{g_{e,i}}$ and $f_{g_{r,i}}$. 
	%The average and maximum differences between the $f_{g_{e,i}}$ and $f_{g_{r,i}}$ are listed in Table \ref{Table:Grasping_Pulling}.
	Table \ref{Table:Grasping_Pulling} lists the average and maximum errors between $F_{g_r}$ and $F_{g_e}$ of the three experiments, and they are under 0.03N and 0.11N, respectively.
	This indicates the forceps are valid for estimating the grasping force.  
	%0.0278N and -9.7853e-04N for $10^\circ$, 0.0298N and -0.0046N for $30^\circ$, 0.0104N and -0.1022N for $50^\circ$.
	
	%% % % % % % % % % % % % Figure grasping force result % % % % % % % % % % % % %
	%\begin{figure}[t!]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/Grasping_force.PNG}
	%	\caption{Grasping forces result. (a), (b), (c) and (d) correspond the forceps grasped pressure sensor with $\theta$ = $10^\circ$, $30^\circ$, $50^\circ$ and $70^\circ$(maximum angle), respectively.}
	%	\label{Figure:Grasping_result}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
		

	\subsection{Ex-vivo Robotic Experiments}
	  \label{Subsection:Dynamic_Experiment}
 
 	% % % % % % % % % % % % Figure: The setup of robotic experiment % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Robotic_setup.pdf}
		\caption{The setup of robotic experiments. 
			A 3D-printed human body phantom was used for placing ex-vivo chicken tissue that simulates the lesion.
			The instrument was implemented through a simulated MIS port on the phantom.}
		\label{Figure:Robotic_setup}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

 	
	% % % % % % % % % % % % Figure: touch for grasping optimization % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Robotic_touch.pdf}
		\caption{The results of robotic touch experiment.
			o$_1$, o$_2$, o$_3$, o$_4$, and o$_5$ denote the tip touching orientation of the forceps.
			White arrows indicate the tip's sliding directions of the forceps.
			Five coloured traces show the changes in the target's position during the touching procedures.  
			The final positions of the target are shown below the experimental scenes of these five touching procedures, which indicate the final tip orientations of the forceps.
		}
		\label{Figure:TouchforGrasping}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

 	% % % % % % % % % % % % Figure: Robotic_grasping % % % % % % % % % % % % %
	\begin{figure*}[t!]
		\centering
		\includegraphics[width=0.98\textwidth]{figure/Robotic_procedure.pdf}
		\caption{The results of automatic tissue grasping experiments. 
			(a) Key snapshots of the experimental procedure. 
			(b) The estimated sensing module's elastic force $F_s$ of two experiments.
			Plot (b) also shows the five experimental phases.
			The first touching phase means the forceps touch the targeted tissue in a closed form.
			The second is opening the forceps to prepare for the tissue grasping.
			The third indicates driving the forceps to touch the tissue again.
			The last two phases denote the forceps grasp and pull the targeted tissue.
			%		The fourth is tissue grasping and followed by the tissue pulling.
			%		Because axial displacement of Exp2 is smaller that of Exp1, the grasping and pulling of Exp2 (denoted with red arrow) is finished earlier than that of Exp1 (denoted with blue arrow).
			(c) The driving force $F_d$ measured by the force sensor at the driving side.
			(d) and (e) are estimated pulling force $F_p$ and grasping force $F_g$, respectively.
		}
		\label{Figure:Robotic_grasping}	
	\end{figure*}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 
	To further verify the feasibility of the proposed system, we conducted a group of robotic experiments on an ex-vivo tissue.
 
	\subsubsection{Experimental Setup}
	Fig. \ref{Figure:Robotic_setup} shows the experimental setup, where a \textit{UR}-5 robotic arm was adopted as the macro-level actuator. 
	The ex-vivo chicken tissue was placed in a human body phantom to simulate the lesion, and the instrument was implemented through a simulated minimally invasive port on the phantom.

	\subsubsection{Touching from Various Orientations}
	%One of the straightforward applications of force feedback in minimally invasive surgery is palpation \cite{konstantinova2014implementation}.
	Firstly, a series of robotic touching experiments were conducted.
	A 3mm bean was placed under the targeted tissue to mimic the tumour, and the forceps touched the targeted tissue from various orientations.
	The experimental scenes and results are shown in Fig. \ref{Figure:TouchforGrasping}, where o$_1$, o$_2$, o$_3$, o$_4$, and o$_5$ indicate different touching orientations.
	The threshold for touch detection in this experiment was 0.15N in the axial direction.
	The $x$ and $y$ components of the target's position changed visibly when the forceps touched the targeted tissue from non-vertical orientations, such as in o$_2$, o$_3$, o$_4$, and o$_5$.
	This was because in this situation a lateral force was applied to the forceps, and the tip slid.
	However, when the forceps touched the tissue from a vertical orientation, as in o$_1$, the $x$ and $y$ components almost kept constant.
	%When the forceps touch the targeted tissue in an none-vertical orientation, slides occurred, denoted as the white arrows in Fig. \ref{Figure:TouchforGrasping}. 
	%Five traces show the change in the marker's position during the touching procedures.  
	%Furthermore, the slide is small when the forceps touch the targeted tissue from an approximated vertical direction (o$_1$).
	The final displacements of the target are also attached below the experimental scenes.
	%to indicate the tip orientations of the forceps.
	With this feature, the sensing module can help the operator or robotic system to evaluate whether the forceps are in an optimal orientation for grasping.

	\subsubsection{Automatic Tissue Grasping}
	In this part, we carried out automatic tissue grasping with two different desired grasping forces.
	%Each experiments were also presented as continuous process
	The key experimental scenes and results are shown in Fig. \ref{Figure:Robotic_grasping}.
	The forceps were driven from the initial state to touch the tissue during the first touching phase (0$\sim$12s).
	Then, the forceps were fully opened during the opening phase (12$\sim$19s) to prepare for the tissue grasping.
	To ensure firm grasping, the forceps were driven to touch the tissue again during the second touching phase (19$\sim$26s).
	Finally, the forceps performed the tissue grasping and pulling in the last two phases.
	The desired grasping forces of these two experiments were different and controlled by the flexure's axial deformation.
	For experiment one (Exp1), the deformation was set as 2mm, while for experiment two (Exp2), it was set as 1mm.
	Consequently, the periods for forceps to perform grasping and pulling are different. 
	They are 26$\sim$38s and 38$\sim$63s for Exp1, while 26$\sim$46s and 46$\sim$71s for Exp2. 

    % % % % % % % % % % % % Figure Pulling force estimation in various orientation % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Pulling_Orientations.pdf}
		\caption{The robotic pulling force estimation in various directions.
			(a) A grasped tissue was pulled by a cable from the initial state o$_0$ to $+y$ (o$_1$), $+x$ (o$_2$), $-y$ (o$_3$) and $-x$ (o$_4$).
			Snapshots show states when the flexure has maximum deformation in different orientations, with the corresponding target positions (in mm) annotated below them.
			(b) The pulling forces $F_p$ estimated by our proposed sensing module (o$_{e,i}$) and that measured by the reference commercial sensor (o$_{r,i}$).}
		\label{Figure:Pulling_Orientations}	
	\end{figure}

   \begin{table}[t]
		\centering
		\caption{Errors of robotic pulling force}	
		\begin{tabular}{ccccc}
			\toprule
			Error &o$_{1}$ & o$_{2}$ & o$_{3}$ & o$_{4}$ \\ \toprule
			Average/N & 0.0302  & 0.0380 & 0.0279 & 0.0233\\
			Max/N & 0.0766 & 0.1309 & 0.1281 & 0.0637\\
			\hline
		\end{tabular}
		\label{Table:Robotic_Pulling}
	\end{table}
	
	%The forceps grasped the tissue and reached the targeted grasping force during  
	%Fig. \ref{Figure:Robotic_grasping}(a) shows key experimental captures of automatic tissue grasping procedure. 
	Fig. \ref{Figure:Robotic_grasping}(b) shows the estimated sensing module's elastic force $F_s$.
	The forces of the two experiments are almost the same during the first three phases, as they followed the same procedure.
	However, because the flexure axial deformation of Exp1 is larger than that of Exp2, $F_s$ of Exp1 is also greater than that of Exp2 in the last two phases.  
	Fig. \ref{Figure:Robotic_grasping}(c) shows the driving force $F_d$ recorded by the force sensor that is connected to the driving cable, which also shows the same trends as $F_s$.
	Fig. \ref{Figure:Robotic_grasping}(d) shows the estimated pulling force $F_p$.
	Although the pulling distances of Exp1 and Exp2 are equal, the pulling force of Exp1 is greater than that of Exp2.
	This is likely because Exp1 was conducted before Exp2, and the tissue has been loosened in Exp1.
	Fig. \ref{Figure:Robotic_grasping}(e) shows the estimated grasping force $F_g$, where the final $F_g$ of Exp1 doubles that of Exp2 because the displacement of Exp1 is twice that of Exp2.
	%does here need an algorithm to describe the whole procedure?

	\subsubsection{Pulling Force Estimation in Various Orientations}
	In this experiment, the grasped tissue was pulled in various directions by a cable connected to a force sensor.
	Fig. \ref{Figure:Pulling_Orientations}(a) shows that the grasped tissue is pulled from the initial state o$_0$ to $+y$ (o$_1$), $+x$ (o$_2$), $-y$ (o$_3$) and $-x$ (o$_4$).
	Snapshots show the states when the flexure has maximum deformation in different orientations, and the corresponding target positions are annotated below them.
	Fig. \ref{Figure:Pulling_Orientations}(b) shows the forces estimated by the proposed sensing module (o$_{e,i}$) and that measured by the commercial sensor (o$_{r,i}$) connected to the tissue-pulling cable.
	Table \ref{Table:Robotic_Pulling} lists the comparison result of $F_p$ between o$_{e,i}$ and o$_{r,i}$.
	The average and maximum errors of the four orientations are under 0.02N and 0.14N, which indicates the forceps are valid for estimating the pulling force in various orientations. 
  
	According to the experiment results, we can see that the haptics-enabled forceps can be implemented in robotic surgery for multi-modal force sensing.
	%, including contact force, grasping force and pulling force.
	With the proposed forceps, some surgical procedures can be performed automatically, and more information will be outputted for decision making, i.e., it is potentially beneficial to task-autonomous robotic surgery such as thyroidectomy, ENT surgery, and laparoscopic surgery.
	%\textbf{More complex and delicate application will be investigated in future works.}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%															                      % 
	%       		            													  %
	%   				         	discussion   					      		      %
	%															                      % 
	%															                      % 
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Discussion}
	\label{Section:Discussion}
 	
	For different applications and instruments, the requirements for sensing resolution and size are various.
	As mentioned before, the proposed sensing module and force estimation method have a certain generality, i.e., the module can be customized into different configurations.
	In this section, a brief discussion is presented to illustrate the situations of the sensing module with different configurations.
	%In this section, a brief discussion is presented including sensors with different configurations, current limitations, and potential optimizations.
	
	\subsection{Different Flexures}
	The resolution can be changed by choosing flexures with different stiffness properties.
	To demonstrate this, we replaced the spring of the prototype presented in Fig. \ref{Figure:Sensor_Calculation_Design} with a less stiff one.
	The new spring had the same parameters as the old one, but the wire diameter changed from 0.5mm to 0.3mm. 
	Following the same procedure of Section \ref{Experiments:stiffness_matrix}, we obtained the stiffness matrix of the 0.3mm module as  
	\begin{equation*}
		\mathbf{K}_s = \left[ {\begin{array}{*{20}{c}}
				{24.68}&{3.65}&{-1.08}\\
				{-1.26}&{23.99}&{-0.96}\\
				{-0.00}&{-0.01}&{45.47}\\
		\end{array}} \right] \, . \notag
		\label{equation:stiffness_matrix_bigger}
	\end{equation*}
	%A verification of this calibrated matrix was also conducted, and the result is shown in Fig. \ref{Figure:Resolution_comparsion}(a).
	To compare the resolutions of these two modules intuitively, we draw their corresponding forces along $x$, $y$, and $z$ axes with the same displacements (0$\sim$1mm), as shown in Fig. \ref{Figure:Resolution_comparsion}.
    We can see that the force magnitudes of the 0.5mm-wire module are around 3.9, 3.7, and 8.5 times that of the 0.3mm-wire module along $x$, $y$, and $z$ axes.
	This indicates the resolution of the 0.3mm-wire module is higher than that of the 0.5mm-wire module, and the resolutions can be configured by choosing different flexures.

 	% % % % % % % % % % % % Figure resolution comparison % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Discussion_resolution.pdf}
		\caption{The estimated forces of the 0.5mm-wire ($F_1$) and 0.3mm-wire ($F_2$) sensing moudles with the same displacements (0$\sim$1mm) in $x$, $y$, and $z$ directions. 
		}
		\label{Figure:Resolution_comparsion}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	% % % % % % % % % % % % Figure Pose estimation % % % % % % % % % % % % %
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Tracking_6.pdf}
		\caption{(a) The orientation comparison between camera estimation and EM tracking of a 5.4mm-diameter target that was installed 10mm to the camera, where the average deviation is 0.063rad, and the max deviation is 0.1089rad occurs at $\beta$;
			(b) The position estimation result of the 5.4mm-diameter target, where the average deviation is 7.4e-6mm, and the max deviation (Hausdorff distance) is 0.796mm.}
		\label{Figure:Different_marker}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	%we moved their heads in $x$ and $z$ axis with 1mm, respectively.	
	
	%\subsection{Different Sensors Configurations}
	\subsection{Different Targets}
	Resolution changes may also accompany size changes due to different flexure sizes.
	When the sensing module is made in a different size, the target size and distance to the camera may change.
	%This may have an influence on the marker's pose and position estimation and affect the force calculation.
	To study whether this has an influence on the target pose and force estimations, we made a 6mm-diameter module with a 5.4mm target that was installed 10mm to the camera.
	Fig. \ref{Figure:Different_marker} shows the target's pose estimation results of this module.
	Compared to the EM tracking results, the average and maximum deviations of the estimated results are 0.063rad and 0.1089rad in the orientation, and 7.4$\times10^{-6}$mm and 0.796mm in the position, respectively. 
	%a $0.063rad$ in average and $0.109rad$ maximum deviation compared to the EM tracking results.
	%This result 
	These results are generally similar to that of the 4mm diameter prototype presented in Section \ref{subsection:pose_position_evaluation}.     
	%camera estimation results are relatively similar in 6mm-diameter(10mm in length) and 4mm-diameter (5 mm in length)sensors.    
	This indicates the proposed method for pose estimation has certain robustness for the target size and installing distance, which can support the size customization without compromising the precision.
	
	The above discussion showed that one can choose a suitable sensing module by configuring the size and stiffness according to different applications and instruments.
    In addition, since the sensing module has the ability to estimate the elastic force $F_s$, it can also be used as an independent multi-dimensional force sensor as well.

	
	
	%% % % % % % % % % % % % Figure stifnees calibration/verification 6mm % % % % % % % % % % % % %
	%\begin{figure}[t]
	%	\centering
	%	\includegraphics[width=0.45\textwidth]{figure/ToDo.jpg}
	%	\caption{The stiffness matrix calibration and verification.}
	%	\label{Figure:Calbiration_bigger}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	
	
	% and the statistical results are listed in Table .\ref{Table:Calibration_result_bigger}. 
	
	%\begin{table}
	%	\centering
	%	\caption{Force/Torque calibration verification}	
	%	\begin{tabular}{ccccccc}
		%		\toprule
		%		Experiments & $F_x$/N & $F_y$/N & $F_z$/N \\
		%		Average Error & &  &  \\
		%		Max Error & &  & \\
		%		\hline
		%	\end{tabular}
	%	\label{Table:Calibration_result_bigger}
	%\end{table}
	
	%\textbf{The resolution of the proposed force sensor relates to both the camera's rate and resolution and the flexure's stiffness.}
	%1mm movement of the marker corresponds xxmm(xx pixels) in image planes.
	%As the stiffness matrix has changed, the resolution of the force estimation will be different.
	
	%In order to intuitively present this, moving and rotating the sensors used in this Section and Section \ref{Section:experiments} in $x$ and $z$ axis, a comparison was conducted.
	%Fig. \ref{Figure:Resolution_comparsion}(b) shows that \textbf{resolution} FFFFFFF\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%F\\
	%In addition, with increasing stiffness, smaller displacement is required for the sensor to counteract external forces.
	%For example, a smaller and high-resolution sensor can be made for some delicate organs and curves. 
	%In contrast, a stiffer sensor can be adopted in some application that requires counteracting greater external forces.
	
	%\subsection{Limitations and Potential Optimizations}
	%There are still some limitations and potential optimizations.
	%The first limitation is that even in a cylindrical shape, the current sensor's anisotropy still exists, i.e., the force range and resolution in the axial and lateral directions are different.
	%Two aspects cause this difference.
	%The first is that the stiffness characteristic of the spring varies in directions.
	%The second is that the camera is more sensitive to the marker's displacement aligning to $x$ and $y$ directions. 
	%Finally, because the manufacturing process is not yet standardized, each sensor requires calibration to get its practical stiffness matrix. 
	%The potential optimization includes customizing the flexure and making it has approximated stiffness in various directions, and manufacturing the marker in shape with minimized anisotropy such as hemispherical.   
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%   	   		  conclusion				      %
	%                                                 %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Conclusion}
	\label{Section:conclusion}
	This paper presented a micro-sized vision-based multi-axis force sensing module, which can be customized for different applications and instruments by reconfiguring its target and flexure.
    A pair of haptics-enabled forceps have been proposed by integrating this force sensing module with a micro-level actuator to drive the forceps and compensate for the flexure motion.  
    % This sensing module has been integrated into a pair of forceps to enable multi-modal force sensing with
    The methods for estimating multiple forces were presented with experimental verification.
	A series of experiments, including an automatic robotic ex-vivo tissue grasping procedure, further verified the feasibility of the proposed sensing module and forceps. 
	The results show that the proposed sensing module can be integrated with a micro-sized surgical instrument, and the haptics-enabled forceps are potentially beneficial to automatic tissue manipulation with multi-modal force feedback such as thyroidectomy, ENT surgery, and laparoscopic surgery.
	The forceps will be integrated into a flexible robotic surgical system to further study the benefits of multi-modal force sensing for MIS. 
	More complex tissue manipulation will be conducted based on the new system in our future work.  
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%																	%
	%																	%
	%																	%
	%																	%
	%																	%
	%						     References  							%
	%																	%
	%																	%
	%																	%
	%																	%
	%																	%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\bibliographystyle{IEEEtran}
	\bibliography{reference_haptic}
\end{document}

