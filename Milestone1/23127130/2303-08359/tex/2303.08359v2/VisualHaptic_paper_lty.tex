\documentclass[journal]{IEEEtran}

\usepackage[colorlinks=true,
allcolors=green]{hyperref}
\usepackage{url}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage[noadjust]{cite}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{indentfirst}
\usepackage{algpseudocode}
\usepackage{algorithmicx,algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{makecell}
\usepackage{hyperref}
%\hypersetup{draft}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\pdfoptionpdfminorversion=6
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
	
	
	%
	% paper title
	% \title{Vision-based Multi-modal Force Sensing of Forceps: Towards Task-autonomous Robotic Surgery}
 \title{Haptics-Enabled Forceps with Multi-Modal Force Sensing: Towards Task-Autonomous Surgery}
	%
	\author{Tangyou Liu, Tinghua Zhang, Jay Katupitiya, Jiaole Wang$^{\ast}$, \textit{Member, IEEE} and Liao Wu$^{\ast}$, \textit{Member, IEEE}
		\thanks{Research was partly supported by Australian Research
Council under Grant DP210100879 and Heart Foundation under Vanguard Grant 106988 awarded to Liao Wu, partly by the Science and Technology Innovation Committee of Shenzhen under Grant JCYJ20220818102408018 awarded to Jiaole Wang, and partly by the Tyree IHealthE PhD Top-Up Scholarship awarded to Tangyou Liu. $^{\ast}$Corresponding authors: Jiaole Wang (\textit{wangjiaole@hit.edu.cn}) and Liao Wu (\textit{liao.wu@unsw.edu.au}).}
		\thanks{Tangyou Liu, Jay Katupitiya, and Liao Wu are with the School of Mechanical \& Manufacturing Engineering, The University of New South Wales, Sydney, NSW 2052, Australia.}
		\thanks{Tinghua Zhang and Jiaole Wang are with the School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, 518055, China.}
	}
	
	\definecolor{d_c}{RGB}{0,0,250}
	% The paper headers
	%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
	%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
	%% If you want to put a publisher's ID mark on the page you can do it like
	%% this:
	%%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
	%% Remember, if you use this you must call \IEEEpubidadjcol in the second
	%% column for its text to clear the IEEEpubid mark.
	%% use for special paper notices
	%%\IEEEspecialpapernotice{(Invited Paper)}
	
	
	% make the title area
	\maketitle
	
	\begin{abstract}
		%Despite the last centuries have witnessed a great progression toward minimally invasive and robot-assisted surgery, 
		% One of the major shortcomings of 
		Many robotic surgical systems have been developed with micro-sized biopsy forceps for tissue manipulation.
		However, these systems often lack force sensing at the tool side.
		This paper presents a vision-based force sensing method for micro-sized biopsy forceps. 
		A miniature sensing module adaptive to common biopsy forceps is proposed, consisting of a flexure, a camera, and a customised target.
		The deformation of the flexure is obtained by the camera estimating the pose variation of the top-mounted target. 
		Then, the external force applied to the sensing module is calculated using the flexure's displacement and stiffness matrix.
		Integrating the sensing module into the biopsy forceps, in conjunction with a single-axial force sensor at the proximal end, we equip the forceps with haptic sensing capabilities.
		Mathematical equations are derived to estimate the multi-modal force sensing of the haptics-enabled forceps, including pushing/pulling forces (Mode-I) and grasping forces (Mode-II).
%       	Integrating the sensing module, cwe propose an algorithm that relies on the geometric information of the biopsy forceps to enable tactile sensing with the biopsy forceps.
%        The enabled force sensing includes touching, grasping, and pulling when the forceps manipulate tissues.
		%To actuate the haptics-enabled forceps and conduct experiments for verification, we design a micro-level actuator, which can also minimize the unexpected sliding between the forceps' clips by compensating for the motion introduced by the flexure's deformation.
		A series of experiments on phantoms and ex vivo tissues are conducted to verify the feasibility of the proposed design and method.
		Results indicate that the haptics-enabled forceps can achieve multi-modal force estimation effectively and potentially realize autonomous robotic tissue grasping procedures with controlled forces.
        A video demonstrating the experiments can be found at \href{https://youtu.be/JfLmSdaFufw}{https://youtu.be/JfLmSdaFufw}.  
	\end{abstract}
	
	
	% Note that keywords are not normally used for peerreview papers.
	\begin{IEEEkeywords}
		Surgical robotics, autonomous surgery, multi-modal force sensing, tissue manipulation.
	\end{IEEEkeywords}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%                introduction                     %
	%												  %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Introduction}
	\label{Section:introduction}
	
	\IEEEPARstart{A}{n} increasing number of robotic systems have been developed with miniature instruments to facilitate minimally invasive surgery (MIS) in the past decade \cite{dupont2021decade,dupont2022continuum}.
	For example, micro-sized biopsy forceps have been widely adopted in recently developed robotic systems for tissue manipulation in narrow spaces \cite{wu2022camera, feng2022development,cao2022spatial}.
	However, one notable limitation of these systems is their lack of force sensing at the tool side for tissue manipulation, including pushing (Fig. \ref{Figure:Introduction}(a)), grasping (Fig. \ref{Figure:Introduction}(b)), and pulling (Fig. \ref{Figure:Introduction}(c)) forces. 
	
	Force sensing for tissue manipulation is critically needed for two reasons. 
        Firstly, information about these forces can enhance surgeons' operation and decision-making if integrated into the systems appropriately \cite{patel2022haptic}.
	For instance, Talasaz \textit{et al}. \cite{talasaz2016role} verified that a haptics-enabled teleoperation system could perform better during a robot-assisted suturing task.
	Secondly, there is a consensus on the growth of autonomy in surgical robots \cite{yang2017medical} accompanied by more dexterous mechanisms such as snake-like robots \cite{razjigaev2022end,wang2021eccentric}.
	Force sensing at the tool side will play an essential role in the evolution of autonomous robotic surgery \cite{yang2017medical,attanasio2021autonomy}.
	
	% % % % % % % % % % % % The First figure % % % % % % % % % % % % %
	\begin{figure*}[t!]
		\centering
		\includegraphics[width=0.98\textwidth]{figure/First.pdf}
		\caption{Typical tissue manipulations using a pair of forceps in robotic thyroid tumour treatment, an example scenario of robotic surgery, and the critically required force information.
			(a) A pair of forceps touches the targeted tissue while measuring the pushing forces.
			(b) The forceps grasp the targeted tissue while measuring and controlling the grasping force.
			(c) The forceps pull the grasped tissue while measuring and controlling the pulling force.
			(d) The micro-level actuator used for driving the forceps, where a commercial single-axis force sensor measures the driving force applied to the forceps' driving cable.
			The upper and lower linear drivers are responsible for grasping and pushing/pulling, respectively.
			The lower driver also compensates for the motion introduced by the flexure's deformation when the upper driver grasps tissue.
			(e) The haptics-enabled biopsy forceps, where the forceps' base is concentrically installed to the vison-based sensing module's head (2).
			Two LEDs (1) are mounted in the sensing module's head (2) to provide light source in our prototype, which comes through the target's (3) holes and is captured by the camera (5) mounted on the other end of the flexure (4). 
			%The other camera is a backup of (6).
			The upper-right inset shows the installed target and the holes used as markers.
			The cylindrical base (6) provides a mount to connect to instruments, which is a stainless tube (8) in our prototype.
			Concentric to the module, a channel is reserved for the passage of the forceps' driving cable (7).	
			%			A soft sleeve (5) can also be installed to minimize the influence of intense external light disturbances in some particular situations.
			The lower right inset shows a prototype that has a 4mm diameter and 22mm length. 
			Plot (e) also shows the camera frame \{$C$\}, the target frames of the initial \{$M_0$\} and current \{$M_t$\} states, and the forces applied to the forceps when they push or pull the tissue.
			$\mathbf{f}_d$ is the driving force on the cable and is measured by the proximal commercial force sensor.
            $\mathbf{f}^{\prime}_d$ is the driving force transmitted to the forceps' jaws through the central cable.
            $\mathbf{f}_s$ is the supporting force from the sensing module, and $\mathbf{f}_p$ is the pushing/pulling force from the tissue.}
		\label{Figure:Introduction}	
	\end{figure*}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	

%	\subsection{Related Works} 
	
    Researchers have conducted various investigations to enable force sensing of different grippers for tissue manipulation.
    Zarrin \textit{et al}. \cite{zarrin2018development} fabricated a two-dimensional gripper that can measure grasping and axial forces with two Fiber Bragg grating (FBG) sensors embedded into the gripper's jaws. 
    Using the same principle, Lai \textit{et al}. \cite{lai2021three} developed a gripper that can estimate pulling and lateral forces.
    While these FBG-based designs usually present a high resolution of force detection, they always suffer from extreme sensitivity to temperature variation \cite{taghipour2019temperature}. 
    In addition, highly expensive interrogation systems are needed for these sensors to work. 
    By integrating capacitance into a pair of surgical forceps' two jaws, Kim \textit{et al}. \cite{kim2018sensorized} enabled the forceps with multi-axis force sensing.
    However, the capacitance-based method is still susceptible to temperature changes \cite{brinkmann2021temperature}.
    Moreover, it is vulnerable to electromagnetic variation, which is inevitable when polarized instruments are adopted for tissue manipulation \cite{gokcal2019robotic}.
    These limitations may invalidate the forceps' force sensing capability when used in practical surgical tasks \cite{dimaio2011vinci}.
    In addition, all the above methods require re-machining the forceps' two jaws to install the proposed sensors, making them unsuitable for micro-sized biopsy forceps.
	
	Recently, vision-based methods have also been explored for developing low-cost force sensors.
	Ouyang \textit{et al}. \cite{ouyang2020low} developed a multi-axis force sensor with a camera tracking a fiducial marker supported by four compression springs. 
	The displacement of the marker was captured by the camera and converted to force information by multiplying a linear transformation matrix.
	Fernandez et al. \cite{fernandez2021visiflex} presented a vision-based force sensor for a humanoid robot's fingers with almost the same principle.
	In addition to providing multi-dimensional force feedback, vision-based force sensing is relatively robust to magnetic, electrical, and temperature variations.
	However, these vision-based sensors developed so far are large in size and can only estimate contact forces that directly connect to the fiducial markers' displacement.
	
	Although vision has already been introduced into surgical systems, little work has investigated its force-sensing capability for surgical instruments.
	An exception is a visual-haptic module developed by Fagogenis \textit{et al}. \cite{fagogenis2019autonomous} for autonomous robotic intracardiac catheter navigation.
	The proposed module uses a camera to sense the contact between a silicone head mounted to the instrument's tip and the heart, according to the area and duration that the silicone head is covered by the tissues. 
    However, it cannot be used to measure either the pushing/pulling force or the grasping force of forceps as it relies on the contact between the silicone head and the surrounding tissues.
 
 % with two working modes: continuous and intermittent contacts.
	% The former mode estimates the forces according to the amount of tissue captured by the camera, while the latter bases the estimation on the contact fraction with the heart in a beat cycle.
	% That means this module relies on the movement rhythm of contacted tissue, which limits its application only to regular moving organs.
	
	In summary, despite many achievements for gripper force sensing based on various principles in the past few years, force estimation of micro-sized biopsy forceps for MIS, especially their grasping and pulling force measurement, is still challenging. 
	This paper proposes a method to enable multi-modal force sensing of the widely adopted micro-sized biopsy forceps by combining a three-dimensional vision-based force sensing module at the tool side and a single-axis commercial strain gauge sensor at the proximal side, as shown in Fig. \ref{Figure:Introduction}(d).
    The developed sensing module that can be easily integrated at the tool side is responsible for perceiving the interaction between the forceps and the manipulated tissues (Fig. \ref{Figure:Introduction}(e)).
    The commercial strain gauge sensor mounted at the proximal side measures the force applied to the forceps' driving cable.
	By integrating these pieces of sensed information, mathematical equations are derived to estimate the forceps' touching, grasping, and pulling forces.
    
	
	The main contributions of this article are two-fold:
 \begin{enumerate}
	\item A vision-based force-sensing module that can be easily integrated into micro-sized biopsy forceps is developed. 
	An algorithm is designed to calculate the force applied to the sensing module with a registration method for tracking and estimating the pose of the sensing module's target. 
	\item A design of haptics-enabled forceps is further proposed by assembling the widely used biopsy forceps with the developed sensing module embedded at the tool end and a single-axis strain gauge sensor mounted at the proximal end. Mathematical formulae are derived to estimate the multi-modal force sensing of the haptics-enabled forceps, including pushing/pulling force (Mode-I) and grasping force (Mode-II).
\end{enumerate}
%	and forceps' geometry is further proposed to estimate the touching, grasping, and 3D pulling forceps.	
 These contributions are validated by various carefully designed experiments on phantoms and ex vivo tissues.
    Results indicate that the haptics-enabled forceps can achieve multi-modal force estimation effectively and potentially realize autonomous robotic tissue grasping procedures with controlled forces.


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%												  %
	%												  %
	%       Haptics-enabled forceps development       %
	%												  %
	%												  %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Haptics-Enabled Biopsy Forceps}
	\label{Section:sensor_development}

	\subsection{Design of A Vision-Based Force Sensing Module}
	\label{Subsection:sensor_design}
	\subsubsection {Overall Structure}
	The design of the  sensing module (Fig. \ref{Figure:Introduction}(e)) has followed two requirements: 1) small in size, 2) adaptive to biopsy forceps.
	The module is designed in a cylindrical shape to minimize anisotropy, and the main components are a flexure, a camera, and a target.
	A central channel is reserved for the passage of instruments.
	%Here, a backup camera is installed to avoid accidental failure.
	The target, supported by the flexure, is manufactured with multiple holes that allow light to pass from a light source, and these holes can then be captured by a camera as markers for the estimation of the target's pose, as depicted in Fig. \ref{Figure:Introduction}(e).
	The module's size and stiffness can be customized based on the instruments' and applications' needs.
	Here, we assemble a prototype with a diameter of 4mm and a length of 12mm and integrate it in a pair of biopsy forceps, as shown in the lower-right of Fig. \ref{Figure:Introduction}(e). 
	\subsubsection{Flexure}
	Material properties and geometry play a dominant role in the multidimensional stiffness of the flexure.
	Different approaches have been adopted for flexure design, including the pseudo-rigid-body replacement method \cite{pucheta2010design}, topology optimization \cite{deng2014topology}, etc.
	For force sensing in different surgical tissue manipulations, the flexure should also be changeable according to the task requirements.
	One commercial and readily available flexure is the compression spring \cite{ouyang2020low,fernandez2021visiflex}.
	Although compression springs typically show dominant stiffness in the axial direction, they also exhibit a certain degree of stiffness in lateral directions and can be linearly approximated \cite{keller2011equivalent}.
	Moreover, they can be easily reconfigured by choosing different wire diameters, outer diameters, and coil numbers. 
	For the prototype, we chose a stainless steel compression spring with a wire diameter of 0.5mm, outer diameter of 4mm, rest length of 5mm, and four coils. 
	The selected spring has a force response range of 0$\sim$5N and 0$\sim$2N at axial and lateral displacements of 0$\sim$2mm and 0$\sim$1mm, respectively, which can meet the requirement of most tissue manipulations \cite{golahmadi2021tool}.
	%And the marker's size can be made to suit the size.
	\subsubsection{Camera and Target}
	The camera for the sensing module should have a compact size, sufficient resolution, and surgery compatibility.
	For the prototype, we chose OVM6946 (Ominivision Inc., USA), which has been adopted in many surgical applications \cite{banach2021visually,cao2022spatial}. 
	It is 1mm in width and 2.27mm in length and has a 400$\times$400 resolution and 30Hz rate.
	%Generally, four nonlinear points on an object are required for a camera to estimate the object's pose in 3D space \cite{lepetit2009epnp}.
	%However, points may get lost due to occlusion and halation. 
	To ensure continuous tracking and estimation, we fabricated the target with twelve cycle-distributed 0.15mm-diameter holes used as markers for the camera to track and estimate the target pose, and a central hole is reserved for tools to pass through.
	These holes can also be customized based on the instruments and applications.
	Considering the integration into the biopsy forceps for MIS, the prototypeâ€™s target in Fig. \ref{Figure:Introduction}(e) has a 3.4mm outer diameter and a 0.6mm-diameter central hole for the driving cable.
	


%	% % % % % % % % % % % % Figure 1 % % % % % % % % % % % % %
%	\begin{figure*}[t]
%		\centering
%		\includegraphics[width=0.80\textwidth]{figure/pose_estimation.pdf}
%		\caption{(a) The directly captured image at the initial state. 
%			(b) The blob detected result at the initial state based on the filtered image. 
%			These detected marker centers are returned for pose estimation.
%			(c) The projection relationship between the $i$-th marker ($^{M}{\mathbf{p}}_{i}$) in the target frame \{\textit{M}\} and its corresponding pixel position ($^{I}{\mathbf{p}}_{i}$) in image frame \{\textit{I}\}. 
%            (c) also shows the original index of markers.
%			(d) The blob detection result at a random pose, where the index of detected markers is different, and the points labelled with 3 and 6 in the initial state are lost. 
%			(e) The registration result of (d), where markers were registered to their original indexes.}
%		\label{Figure:pose_estimation}	
%	\end{figure*}
%	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 
	\subsection{Force Estimation of Sensing Module}
	\label{Subsection:force_estimation}
	Ideally, for a linear-helix compression spring, the external force $\mathbf{f}_s$ applied to the sensing module's head has an approximated relationship with the spring's displacement \cite{keller2011equivalent} as
	\begin{equation}
		{\mathbf{f}_s} = {{\mathbf{K}}_s}\, {\mathbf{d}_s} = \left[ {\begin{array}{*{20}{c}}
				{{k_x}}&0&0\\
				0&{{k_y}}&0\\
				0&0&{{k_z}}
		\end{array}} \right] 
  \mathbf{d}_s
 % \left[ {\begin{array}{*{20}{c}}
%				{{d_x}}\\
%				{{d_y}}\\
%				{{d_z}}
%		\end{array}} \right] 
\, ,
		\label{equation:Forcecalculation}
	\end{equation}
	%\begin{align}
	%	{F_e} &= {k_s} \times {x_s} \notag \\ 
	%	&= \left[ {\begin{array}{*{20}{c}}
			%			{{k_{{w_x}}}}&{}&{}&{}&{}&{}\\
			%			{}&{{k_{{w_y}}}}&{}&{}&0&{}\\
			%			{}&{}&{{k_{{w_z}}}}&{}&{}&{}\\
			%			{}&{}&{}&{{k_{{v_x}}}}&{}&{}\\
			%			{}&0&{}&{}&{{k_{{v_y}}}}&{}\\
			%			{}&{}&{}&{}&{}&{{k_{{v_z}}}}
			%	\end{array}} \right] \times \left[ {\begin{array}{*{20}{c}}
			%			{{d_x}}\\
			%			{{d_y}}\\
			%			{{d_z}}\\
			%			{{\varphi}}\\
			%			{{\theta}}\\
			%			{{\psi}}
			%	\end{array}} \right] \, ,
	%	\label{equation:Forcecalculation}
	%\end{align}
	where $\mathbf{f}_s \in \mathbb{R}^{3}$ denotes the external forces, ${\mathbf{K}}_s$ denotes the stiffness matrix of the spring, 
	and $\mathbf{d}_s \in \mathbb{R}^{3}$ is the displacement of the spring's end relative to its relaxed original position.
	%\textit{And we assume the spring has low hysteresis, stress relaxation, and creep, and in cases where the distal side's velocity and acceleration are non-negligible, low viscosity and mass.}
    In this paper, $\mathbf{d}_s$ is equal to the target's current relative displacement to its initial state.
%	In this section, the marker's pose estimation and external force calculation are introduced. 
	%spring's stiffness matrix calibration
	%Therefore, to calculate the external force applied on the proposed sensor, marker's pose and spring's stiffness matrix should be measured first.
	%In this section, they are introduced in turn. 


	\subsubsection{Image-Based Target Pose Estimation}
	In order to estimate the target pose, each marker needs to be tracked by the camera first.
	However, as the raw image captured by the camera is full of noise, as seen in Fig. \ref{Figure:Sensor_design_estimation}(b), markers cannot be detected robustly.
	Therefore, a bilateral filter and threshold are adopted to minimize the noise and preserve the boundary in images \cite{singh2019advanced}.
	Then, markers are circled by a parameterized blob detection \cite{kaspers2011blob}, as shown in Fig. \ref{Figure:Sensor_design_estimation}(c).
	Finally, each marker's central pixel position $^{I}{\mathbf{p}}$ is returned.
	%\begin{equation}
		%({u^*},{v^*},{\delta ^*}) = \mathop {\arg \max }\limits_{(u,v,\delta )} |{\delta ^2}{\nabla ^2}{n_\delta }*I(u,v)| \, ,
		%\label{Equation:blob_detection}
	%\end{equation}
	%where $({u^*},{v^*})$ is the pixel position of the blob's centre, and ${\delta ^*}$ is the size of the blob.
	


	The positions of markers $^{M}{\mathbf{P}} \in \mathbb{R}^{2\times 12}$ in the target frame \{\textit{M}\} are known.
	The projection between the $i$-th marker centre $^{M}{\mathbf{p}}_{i} = [x_i, \, y_i]^{\textnormal{T}}$ and its corresponding pixel $^{I}{\mathbf{p}}_{i} = [u_i, \, v_i]^{\textnormal{T}}$ in the image frame \{\textit{I}\}, as shown in Fig. \ref{Figure:Sensor_design_estimation}(a), can be formulated as 
 	\begin{align}
		^{\mathop{I}}{\tilde{\mathbf{p}}}_{i} 
		& = \frac{1}{z} \, \left[ {\begin{array}{*{20}{c}}
				{\frac{1}{{{\rho _w}}}}&0&{{u_0}}\\
				0&{\frac{1}{{{\rho _h}}}}&{{v_0}}\\
				0&0&1
		\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
				f&0&0&0\\
				0&f&0&0\\
				0&0&1&0
		\end{array}} \right]{}_{M}^{C}{\mathbf{T}}{\,}^{M}{\tilde{\mathbf{p}}_{i}} \notag \\
		& =  \frac{1}{z} \, \underbrace{\left[ {\begin{array}{*{20}{c}}
					{\frac{f}{{{\rho _w}}}}&0&{{u_0}}\\
					0&{\frac{f}{{{\rho _h}}}}&{{v_0}}\\
					0&0&1
			\end{array}} \right]}_{camera \, \, intrinsic \, \, {\mathbf{A}}}
		\left[ {\begin{array}{*{20}{c}}
				r_{11}&r_{12}&t_x\\
				r_{21}&r_{22}&t_y\\
				r_{31}&r_{22}&t_z
		\end{array}} \right]
		\left[ {\begin{array}{*{20}{c}}
				x_i\\
				y_i\\
				1
		\end{array}} \right] \notag \\
		& = \frac{t_z}{z}\, \underbrace{\left[ {\begin{array}{*{20}{c}}
					{h_{11}}&{h_{12}}&{h_{13}}\\
					{h_{21}}&{h_{22}}&{h_{23}}\\
					{h_{31}}&{h_{32}}&1
			\end{array}} \right]}_{homography \, \, \mathbf{H}}  
		\left[ {\begin{array}{*{20}{c}}
				x_i\\
				y_i\\
				1
		\end{array}} \right] \, ,
		\label{equation:projection}
	\end{align} \\ 
	where ${}_{M}^{C}{\mathbf{T}} = \left[ \begin{array}{*{20}{c}}
		\mathbf{r}_1 & \mathbf{r}_2 & \mathbf{r}_3 &\mathbf{t} \\
		0& 0& 0&1
	\end{array}\right] = \left[ {\begin{array}{*{20}{c}}
			r_{11}&r_{12}&r_{13}&t_x\\
			r_{21}&r_{22}&r_{23}&t_y\\
			r_{31}&r_{32}&r_{33}&t_z\\
			0&0&0&1
	\end{array}} \right]$ is the transformation from camera frame \{\textit{C}\} to marker frame \{\textit{M}\},
	$^{M}\tilde{\mathbf{p}}_{i} = [x_i, \, y_i, \, 0, \, 1]^{\textnormal{T}}$ is the homogeneous form of $^{M}{\mathbf{p}}_{i}$,
	$^{I}\tilde{\mathbf{p}}_{i} = [u_i, \, v_i, \, 1]^{\textnormal{T}}$ is the homogeneous form of $^{I}{\mathbf{p}}_{i}$,
	$i\in[0,n]$ ($n=11$ for the prototype), 
	$f$ is the camera's focal length, $\rho_w$ and $\rho_h$ are the width and height of each pixel, respectively, 
 $z = {(h_{31}x_i+h_{32}y_i+1)} \, t_z$, 
 and the camera intrinsic matrix $\mathbf{A}$ is obtained via calibration with MATLAB camera toolbox \cite{fetic2012procedure}.
	%$t_z = \frac{z}{h_{31}x_i+h_{32}y_i+1}$.
	
	%In this paper, we regard frame \{\textit{Im}\} is coincide with frame \{\textit{c}\}. 
	
	%%%%%%% can be putted to the modelling section %%%%%%%
	According to the transformation relationship shown in Fig. \ref{Figure:Sensor_design_estimation}(a), the current target pose related to its initial state ${}^{M_0}_{M_t}\mathbf{T}$ can be formulated as
	\begin{equation}
		{}^{M_0}_{M_t}\mathbf{T} = ({}^{C}_{M_0}\mathbf{T})^{-1}{\, \,}^{C}_{M_t}\mathbf{T} \, .
		\label{equation:markerpose}
	\end{equation} 
	The translation of ${}^{M_0}_{M_t}\mathbf{T}$ is equal to the displacement $\mathbf{d}_s$ in (\ref{equation:Forcecalculation}).
	%Furthermore, the orientation information $[\varphi, \theta, \psi]$ of the marker can be derived by
	%\begin{align}
		%\left\{\begin{array}{l}
			%\mathbf{\varphi} = tan^{-1}(\frac{r_{32}}{r_{33}})\\
			%\mathbf{\theta} = tan^{-1}(\frac{{-r}_{31}}{\sqrt{{r}^2_{32}+{r}^2_{33}}})\\
			%\mathbf{\psi} = tan^{-1}(\frac{r_{21}}{r_{11}})
		%\end{array}\right. \, .
		%\label{equation:displacement}
	%\end{align} 
	%where 
	%\begin{equation}
	%	{}^{c_o}_{c_c}\mathbf{T} = {}^{m_o}_{c_c}\mathbf{T} {\,}{{}^{m_o}_{c_o}\mathbf{T}}^{-1} \, .
	%\end{equation}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Then, the question becomes to get ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$.
	The relationship between ${}^{C}_{M}\mathbf{T}$ and homography matrix $\mathbf{H}$ can be formulated as
	%\begin{align}
	%	\left\{\begin{array}{l}
		%		\mathbf{r}_1 = {t_z}\mathbf{I}_{c}^{-1}[h_{11}, \, h_{21}, \, h_{31}]^{\textnormal{T}}\\
		%		\mathbf{r}_2 = {t_z}\mathbf{I}_{c}^{-1}[h_{12}, \, h_{22}, \, h_{32}]^{\textnormal{T}}\\
		%		\mathbf{t} = {t_z}\mathbf{I}_{c}^{-1}[h_{13}, \, h_{23}, \, 1]^{\textnormal{T}}\\
		%		\mathbf{r}_3 = r_1 \times r_2
		%	\end{array}\right. \, .
	%	\label{equation:RandT}
	%\end{align}
	%where $t_z = \frac{z}{h_{31}x_i+h_{32}y_i+1}$.
	%$\mathbf{r}_1$, $\mathbf{r}_2$, and $\mathbf{r}_3$
	\begin{align}
		\left\{\begin{array}{l}
			\mathbf{r}_1 = {t_z}\mathbf{A}^{-1}[h_{11}, h_{21}, h_{31}]^{\rm T}\\
			\mathbf{r}_2 = {t_z}\mathbf{A}^{-1}[h_{12}, h_{22}, h_{32}]^{\rm T}\\
			\mathbf{t} = {t_z}\mathbf{A}^{-1}[h_{13}, h_{23}, h_{33}]^{\rm T}\\
			\mathbf{r}_3 = \mathbf{r}_1 \times \mathbf{r}_2
		\end{array}\right. \, .
		\label{equation:RandT}
	\end{align}
	Because $\mathbf{r}_1$ is a unit vector, $t_z$ can be calculated by $t_z = \frac{1}{||\mathbf{A}^{-1}[h_{11}, h_{21}, h_{31}]^{\rm T}||}$.
	Therefore, to estimate ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$, we only need to solve the homography matrix $\mathbf{H}^{0}$ and $\mathbf{H}^{t}$ of the initial and current states.
	
	%% % % % % % % % % % % % Figure 1 % % % % % % % % % % % % %
	%\begin{figure}[t]
	%	\centering
	%	\includegraphics[width=0.43\textwidth]{figure/blob_detection.pdf}
	%	\caption{(a) the original image captured by the camera. (b) the bilateral filter processed result. (c) the blob detection on the filed image, where nine red-number-marked holes are detected in the original position.}
	%	\label{Figure:blob_detection}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	
	Because each homography matrix $\mathbf{H}$ has eight variables, at least four nonlinearly relative projections between $^{M}{\mathbf{p}}_{i}$ and $^{I}{\mathbf{p}}_{i}$ are needed to solve it, and each pair's relationship can be formulated as
	\begin{small}
		\begin{equation}
			{\left[ {\begin{array}{*{20}{c}}
						{{}^M{p_{i,x}}}&0\\
						{{}^M{p_{i,y}}}&0\\
						1&0\\
						0&{{}^M{p_{i,x}}}\\
						0&{{}^M{p_{i,y}}}\\
						0&1\\
						{ - {}^M{p_{i,x}}{}^{I}{p_{i,u}}}&{ - {}^M{p_{i,x}}{}^{I}{p_{i,v}}}\\
						{ - {}^M{p_{i,y}}{}^{I}{p_{i,u}}}&{ - {}^M{p_{i,y}}{}^{I}{p_{i,v}}}
				\end{array}} \right]^{\rm T}} \, \left[ {\begin{array}{*{20}{c}}
					{{h_{11}}}\\
					{{h_{12}}}\\
					{{h_{13}}}\\
					{{h_{21}}}\\
					{{h_{22}}}\\
					{{h_{23}}}\\
					{{h_{31}}}\\
					{{h_{32}}}
			\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
					{{}^{I}{p_{i,u}}}\\
					{{}^{I}{p_{i,v}}}
			\end{array}} \right] \, ,
			\label{equation:homography}
		\end{equation}
	\end{small} 
where $({}^M{p_{i,x}},{}^M{p_{i,y}})$ and $({}^{I}{p_{i,u}},{}^{I}{p_{i,v}})$ are coordinates of the $i$-th marker in the frame \{\textit{M}\} and its projection in the frame \{\textit{I}\}.
	%Then we can further get $\frac{t_z}{z}$ for each pair of $^{m}{\mathbf{p}}_{h_i}$ and $^{Im}{\mathbf{p}}_{h_i}$ as:
	%\begin{equation}
	%	\frac{t_z}{z} = \frac{1}{h_{31}x_i + h_{32}y_i+1} \, .
	%	\label{equation:t_z/z}
	%\end{equation}
	Based on this relationship and four pairs of projections, a set of equations can be established to solve $\mathbf{H}$.
	Substituting the solved $\mathbf{H}$ into (\ref{equation:RandT}), ${}^{C}_{M}\mathbf{T}$ can be further obtained.


    % % % % % % % % % % % % Figure sensor design and forces/torques calculation % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.43\textwidth]{figure/Sensor_estimation.pdf}
		\caption{(a) The projection relationship between the $i$-th marker ($^{M}{\mathbf{p}}_{i}$) in the target frame \{\textit{M}\} and its corresponding pixel position ($^{I}{\mathbf{p}}_{i}$) in image frame \{\textit{I}\}. 
			(a) also shows the original index of markers.
			(b) The directly captured image at the initial state. 
			(c) The blob detected result at the initial state based on the filtered image. 
			These detected marker centers are returned for pose estimation.
			(d) The blob detection result at a random pose, where the index of detected markers is different, and the points labelled with 3, 6 and 11 in the initial state are lost. 
			(e) The registration result of (e), where markers were registered to their original indexes.}
		\label{Figure:Sensor_design_estimation}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

	\subsubsection{Marker Image Registration}
	Ideally, through substituting four detected $^{I}{\mathbf{p}}_{i}$ and their corresponding $^{M}{\mathbf{p}}_{i}$ into (\ref{equation:homography}) we can calculate the homography matrix $\mathbf{H}$ for each state.
	However, under external forces, the pose variation of the target causes the occlusion or halation of some markers and results in wrong correspondence between the image and the marker.
 %as the target will move when an external force applies to the sensing module, the order of the detection may change, and some markers may be lost-tracked.
	As a result, the corresponding relationship between $^{I}{\mathbf{p}}_{i}$ and $^{M}{\mathbf{p}}_{i}$ changes over time, which invalidates the direct use of (\ref{equation:homography}).
	Therefore, a registration process is required to label the currently detected markers' pixel positions $^{I}{\mathbf{P}}^{t}$ to their original indexes, as shown in Fig. \ref{Figure:Sensor_design_estimation}(a), and we define this registered result as $^{I}{\mathbf{P}}^{t,r}$. %In the following description, we take the case shown in Fig. \ref{Figure:pose_estimation}(d and e) as an example of the currently detected markers, while noting that the derivation applies to an arbitrary situation.
    
	At the initial state $t = 0$, the target is installed perpendicularly to the optical axis.
	For the prototype we built, all the markers can be detected except the 11$^{th}$ marker (top marker) that is occluded by the channel, as shown in Fig. \ref{Figure:Sensor_design_estimation}(b).
	Therefore, we define $^{M}{\mathbf{P}^0}$ by removing the 11$^{th}$ marker from $^{M}{\mathbf{P}}$ as the marker set at the initial state, and the projection $\mathbf{H}^{0}$ between $^{M}{\mathbf{P}^0}$ and $^{I}{\mathbf{P}}^{0}$ can be obtained according to (\ref{equation:homography}). 
	Substituting $^{M}{\mathbf{p}}_{11}$ and $\mathbf{H}^{0}$ back to (\ref{equation:homography}), we can estimate $^{I}{\mathbf{p}}^{0}_{11}$.
	Then, the complete pixel-position array of the initial state $^{I}{\mathbf{P}}^{0,c}$ can be obtained by appending $^{I}{\mathbf{p}}^{0}_{11}$ to $^{I}{\mathbf{P}}^{0}$.
    % $^{I}{\mathbf{P}}^{0,c}$ is used as the reference in the registration at the next state.  
	

	For the current state $t$, the pixel positions of the detected markers $^{I}{\mathbf{P}}^{t} \in \mathbb{R}^{2\times m}$ ($m$ is the number of detected markers, $m\leqslant n$) are returned by the blob detection.
	Nevertheless, these detected markers are directly labelled from bottom to top in the image.
    For example, the blob detection result of a random pose is shown in Fig. \ref{Figure:Sensor_design_estimation}(d). 
	Assuming $^{I}{\mathbf{P}}^{t-1,c}$ is known, we calculate the L1 norm of the distance vector between each $^{I}{\mathbf{p}}^{t}_{i}$ and $^{I}{\mathbf{p}}^{t-1,c}_{j}$, $i \in [1,m]$ and $j \in [1,n]$, and define it as $e$.
	%\begin{equation}
	%	e = |^{Im}{\mathbf{P}}^{cur}_{h_i,u} - ^{Im}{\mathbf{P}}^{pre}_{h_j,u}| + |^{Im}{\mathbf{P}}^{cur}_{h_i,v} - ^{Im}{\mathbf{P}}^{pre}_{h_j,v}|\, ,
	%	\label{equation:registration} 
	%\end{equation}
	%where $i \in [1,m]$ and $j \in [1,12]$. 
	The corresponding marker of $^{I}{\mathbf{p}}^{t}_{i}$ in $^{I}{\mathbf{P}}^{t-1,c}$ is the one that results in the minimal $e$, and its index is the desired $j^*$.
	Without loss of generality, the registration problem for the detected \textit{i}-th hole is defined as
	\begin{equation}
		j^{*} = \arg \min_j ||^{I}{\mathbf{p}}^{t}_{i} - \, ^{I}{\mathbf{p}}^{t-1,c}_{j}||_1\, .
		\label{equation:registration} 
	\end{equation} 
 %where $j^{*}$ is the index of the marker that needs to be updated.
	%Then, let $^{Im}{\mathbf{P}}^{reg}_{h_{index}} = [^{Im}{\mathbf{P}}^{cur}_{h_i,u}, ^{Im}{\mathbf{P}}^{cur}_{h_i,v}]$.
	Repeating this registration for each detected marker $^{I}{\mathbf{p}}^{t}_{i}$ and updating $^{I}{\mathbf{p}}^{t,r}_{j^{*}} \leftarrow {}^{I}{\mathbf{p}}^{t}_{i}$, we can obtain $^{I}{\mathbf{P}}^{t,r}$ with the original indexes.
	%Finally, let $^{Im}{\mathbf{P}}^{cur} = ^{Im}{\mathbf{P}}^{reg}$, we can get the registered result of current detected holes and label them with original index.
    %%%%%%%%%%%%%%%%%%%%
    % and substitute it to (\ref{equation:homography}) to get $\mathbf{H}^t$. 
    % Then, by substituting undetected markers' coordinates to $\mathbf{H}^t$, we can estimate their corresponding pixel positions.
    % Merging the estimated result with $^{I}{\mathbf{P}}^{t,r}$, we can further obtain the complete pixel-position set $^{I}{\mathbf{P}}^{t,c}$ of the current state, which is used as the reference for next registration. 
    % Then, the complete pixel-position set $^{I}{\mathbf{P}}^{t,c}$ of each state can be obtained by estimating pixel coordinates of undetected markers via substituting $\mathbf{H}^t$ (\ref{equation:homography}) and merging them to $^{I}{\mathbf{P}}^{t,r}$.
    We define the corresponding marker position set of $^{I}{\mathbf{P}}^{t,r}$ as $^{M}{\mathbf{P}}^{t}$.
    Substituting $^{I}{\mathbf{P}}^{t,r}$ and $^{M}{\mathbf{P}}^{t}$ to (\ref{equation:homography}), the homography matrix of the current state $\mathbf{H}^{t}$ can be calculated.
	%with the registered $^{Im}{\mathbf{P}}^{reg}$ and its corresponding holes' original position $^{m_o}{\mathbf{P}}^{reg}$ by (\ref{equation:homography}).
 
	To ensure the image of the current state can be used as the reference for the next registration, i.e., the $t$+1 state, the complete pixel-position set $^{I}{\mathbf{P}}^{t,c}$ of the current state also should be generated. 
 % for reordering the holes' index in the next captured frame, the lost-tracked holes' pixel positions need to be estimated and filled into $^{I}{\mathbf{P}}^{c}$.
    By substituting undetected markers' coordinates in $^{M}{\mathbf{P}}$ and $\mathbf{H}^t$ to (\ref{equation:homography}), we can estimate their corresponding pixel positions $^{I}{\mathbf{P}}^{t,e}$.
    Then, $^{I}{\mathbf{P}}^{t,c}$ can be obtained by merging $^{I}{\mathbf{P}}^{t,e}$ with $^{I}{\mathbf{P}}^{t,r}$.
	% By removing $^{M}{\mathbf{P}}^{r}$ from $^{M}{\mathbf{P}}$, we can obtain positions of undetected holes $^{M}{\mathbf{P}}^{u}$.
	% Then, substituting $^{M}{\mathbf{P}}^{u}$ and $\mathbf{H}^{c}$ to (\ref{equation:projection}), we can estimate the pixel positions of these undetected holes.  
	%As we have $\mathbf{H}^{cur}$, the index and positions $^{m}{\mathbf{P}}^{und}$, by removing $^{m}{\mathbf{P}}^{reg}$ from $^{m}{\mathbf{P}}$, of undetected holes, the pixel positions of the undetected holes can be estimated by (\ref{equation:projection}).
	% Let this estimation result be $^{I}{\mathbf{P}}^{e}$ and by merging it with $^{I}{\mathbf{P}}^{r}$, we can further obtain the complete $^{I}{\mathbf{P}}^{c}$.
	For example, Fig. \ref{Figure:Sensor_design_estimation}(e) shows the registration result and the generated complete image of Fig. \ref{Figure:Sensor_design_estimation}(d).
	% This complete $^{I}{\mathbf{P}}^{c} \subseteq \mathbb{R}^{2\times n}$ is used as $^{I}{\mathbf{P}}^{p}$ in the registration of the next state. 
	
	\subsubsection{Felxure Deformation Estimation}
	%According to $^{Im}{\mathbf{P}}^{reg}$ and $^{m}{\mathbf{P}}^{reg}$,  $^{Im}{\mathbf{P}}^{or}$ and $^{m_o}{\mathbf{P}}^{or}$ the homographic matrix at current pose $\mathbf{H}^{cur}$ and orginal pose $\mathbf{H}^{or}$ can be calculated by (\ref{equation:homography}), respectively. 
	The transformation between the target and the camera at the initial state ${}^{C}_{M_0}\mathbf{T}$ can be calculated by substituting $^{I}{\mathbf{P}}^{0}$ and $\mathbf{H}^{0}$ into (\ref{equation:RandT}).
	Similarly, the transformation at the current state ${}^{C}_{M_t}\mathbf{T}$ can be obtained by substituting $^{I}{\mathbf{P}}^{t,r}$ and $\mathbf{H}^{t}$ into (\ref{equation:RandT}).
	Then, the transformation ${}^{M_0}_{M_t}\mathbf{T}$ can be calculated by substituting ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$ into (\ref{equation:markerpose}), which reflects the deformation of the flexure.
	%  then as we have the current homographic matrix and orginal homographic matrix, the mo_cc_T and mo_co_T can be calcualted by (4). Then the current pose of maker can also be gotten by (2)
	
	
	
	\subsubsection{Force Calculation}
	%With the transformation ${}^{m_c}_{m_o}\mathbf{T}$, 
	The displacement of the spring $\mathbf{d}_s$ is the translation vector of ${}^{M_0}_{M_t}\mathbf{T}$.
	Substituting $\mathbf{d}_s$ and the spring's stiffness matrix $\mathbf{K}_s$ into (\ref{equation:Forcecalculation}), we can estimate the external force $\mathbf{f}_s$.
	Ideally, the stiffness matrix $\mathbf{K}_s$ represents the spring's characteristics that connect the camera and the target. 
	%The the external force/torque exerted on the forceps can be calculated by (\ref{equation:Forcecalculation}).
	However, this stiffness matrix $\mathbf{K}_s$ should also consider the influence of wires and shelter, as they are parallel to the spring.
	In order to get the practical stiffness matrix $\mathbf{K}_s$, a calibration is carried out on the prototype and detailed in section \ref{Section:experiments}. 
	%\textit{ATI nano43} which has 1/520 N and 1/40 Nmm resolution, as the reference and collected series of data to calculate the elements of $K_s$.
	
		\begin{algorithm}[t]
		\begin{algorithmic}[1]
			\Require
			$^{M}{\mathbf{P}}$, $\mathbf{A}$, $\mathbf{K}_s$ and \textit{frame}
			\Ensure
			${}^{M_t}_{M_0}\mathbf{T}$, $\mathbf{f}_s$
			%		\State Image capture and filtering
			\State set $t = 0$;
			\State get $^{I}{\mathbf{P}}^{t}$ via blob detection based on \textit{frame};
			\State get $^{M}{\mathbf{P}}^{t}$ by removing $^{M}{\mathbf{p}_{n-1}}$ from $^{M}{\mathbf{P}}$;
			\State calc $\mathbf{H}^{t}$ by substituting $^{I}{\mathbf{P}}^{t}$ and $^{M}{\mathbf{P}}^{t}$ to (\ref{equation:homography});
			\State estimate $^{I}{\mathbf{P}}^{0}_{n-1}$ by substituting $^{M}{\mathbf{p}}_{n-1}$ and $\mathbf{H}^{0}$ to (\ref{equation:homography});
			\State get $^{I}{\mathbf{P}}^{t,c}$ by appending $^{I}{\mathbf{P}}^{t}_{n-1}$ to $^{I}{\mathbf{P}}^{t}$;
			\State calc ${}^{C}_{M_0}\mathbf{T}$ by substituting $\mathbf{H}^{t}$ to (\ref{equation:RandT});
			%		\For{$i = 1; i< lens(^{Im}{\mathbf{P}}^{or})+1; i++$} 		
			%		\State register the current detected holes according to the vector matrix 
			%		\EndFor		
			%		\State Calculate $\mathbf{H}^{or}$ by submitting $^{c_o}{\mathbf{P}}^{re}$ and $^{m_o}{\mathbf{P}}^{re}$ to (\ref{equation:homography})
			%		\State Estimate ${}^{m_o}_{c_o}\mathbf{T}$ by submitting $\mathbf{H}^{or}$ to (\ref{equation:RandT})
			%		\State Get$^{c_o}{\mathbf{P}}^{und}$ by submitting $^{m_o}{\mathbf{P}}^{und}$ to (\ref{equation:projection})
			%		\State Get $^{c_o}{\mathbf{P}}^{com}$ by merging $^{c_o}{\mathbf{P}}^{re}$ and $^{c_o}{\mathbf{P}}^{und}$
			
			\While{\textit{frame}}
			\State $t$++;
			\State get $^{I}{\mathbf{P}}^{t}$ via blob detection based on \textit{frame};
			\For{$i = 0; i< col(^{I}{\mathbf{P}}^{t}); i++$}; 
			\State $e'$ = infinity; $j^{*}$ = infinity;
			\For{$j = 0; j< col(^{I}{\mathbf{P}}^{t-1,c}); j++$};
			\State $e = ||^{I}{\mathbf{p}}^{t}_{i} - ^{I}{\mathbf{p}}^{t-1,c}_{j}||_1$;
			\If{$e<e'$}
			\State $j^{*}$ = j; $e'$ = $e$;
			\Else
			\State $e'$ = $e$;
			\EndIf
			\EndFor 
			\If{$j^{*} \leq col(^{I}{\mathbf{P}}^{t-1,c})$}	
			\State $^{I}{\mathbf{p}}^{t,r}_{j^{*}} \leftarrow ^{I}{\mathbf{p}}^{t}_{i}$;	
			\EndIf	
			\EndFor
			\State get corresponding $^{M}{\mathbf{P}^t}$ of $^{I}{\mathbf{P}}^{t,r}$ from $^{M}{\mathbf{P}}$;
			\State calc $\mathbf{H}^{t}$ by substituting $^{I}{\mathbf{P}}^{t,r}$ and $^{M}{\mathbf{P}^t}$ to (\ref{equation:homography});            
			% \State get $^{M}{\mathbf{P}}^{u}$ by removing $^{M}{\mathbf{P}}^{r}$ from $^{M}{\mathbf{P}}$; 
			\State get $^{I}{\mathbf{P}}^{t,e}$ by substituting $^{M}{\mathbf{P}}$ and $\mathbf{H}^{t}$ to (\ref{equation:projection});
			\State generate $^{I}{\mathbf{P}}^{t,c}$ by merging $^{I}{\mathbf{P}}^{t,r}$ with $^{I}{\mathbf{P}}^{t,e}$;
			\State calc ${}^{C}_{M_t}\mathbf{T}$ by substituting $\mathbf{H}^{t}$ to (\ref{equation:RandT});
			\State calc ${}^{M_0}_{M_t}\mathbf{T}$ by substituting ${}^{C}_{M_0}\mathbf{T}$ and ${}^{C}_{M_t}\mathbf{T}$ to (\ref{equation:markerpose});
			\State calc $\mathbf{f}_s$ by substituting ${}^{M_0}_{M_t}\mathbf{T}$ and $\mathbf{K}_s$ to (\ref{equation:Forcecalculation});
%			\State calc $\alpha$ by substituting ${}^{M_0}_{M_t}\mathbf{T}$ and $t_d$ to (\ref{equation:alpha}) ;
%			\State calc $F_g$ and $F_p$ by substituting $F_s$, $F_d$ and $\alpha$ to (\ref{equation:grasping_force});
			%		\State Let $^{c_o}{\mathbf{P}}^{pre} = {}^{c_o}{\mathbf{P}}^{com}$
			\EndWhile
		\end{algorithmic}	
		\caption{Pose and Force estimation of sensing module}
		\label{Algorithm:Force_Calculation}
	\end{algorithm}
	

	\subsection{Haptics-Enabled Forceps and Force Estimation}
	
	\subsubsection{Micro-level Forceps Actuator}
	\label{Subsection:forceps_design}
	%	Thanks to the cylindrical structure and the sensing module's central channel, it can be easily integrated with the forceps.  
	To drive the forceps and conduct experimental verification, we designed a micro-level actuator, as shown in Fig. \ref{Figure:Introduction}(d), which consists of upper and lower linear drivers that are responsible for grasping and pushing/pulling, respectively.
	The lower driver also compensates for the motion introduced by the flexure's deformation when the upper driver drives the forceps to grasp tissue.
	The compensation is achieved by moving the lower driver's carrier for the same amount of the flexure's axial deformation estimated by the camera in the reverse direction. 
	A commercial single-axis force sensor (ZNLBS-5kg, CHINO, China), with a resolution of 0.015N, is installed collinearly to the forceps and connected to the driving cable to measure the driving force.
	%% % % % % % % % % % % % Figure linear moudule % % % % % % % % % % % % %
	%\begin{figure}[t!]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/linear_module.pdf}
	%	\caption{(a) The setup of dynamic experiment. FFFFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFF. 
		%		(b) The controller block diagram of the robotic system. FFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFF. FFFFFFFFFFFFFFFFFFFFFF.}
	%	\label{Figure:Linear_Module}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	\label{Subsection:forceps_grasping}
	\subsubsection{Pushing/Pulling Force Estimation (Mode-I)}
	Integrating the sensing module into the forceps, we can enable its haptic sensing, and the haptics-enabled forceps are shown in Fig. \ref{Figure:Introduction}(e).
	The base of the forceps is installed concentrically to the sensing module's head, and the driving cable passes through the reserved central channel.
	The cylindrical base connects the forceps to its carrying instrument (stainless tube).
	A prototype is also shown in the inset of Fig. \ref{Figure:Introduction}(e), which has a 4mm diameter and 22mm total length and is used in the following description and experiments.
	The proposed haptics-enabled forceps can be easily integrated into a robotic surgical system with the micro-level actuator, such as the robot presented in Fig. \ref{Figure:Introduction}(a).
	The forces applied to the forceps when they push or pull a tissue are also shown in Fig. \ref{Figure:Introduction}(e).
	$\mathbf{f}_p$ is the pushing/pulling force from the interactive tissue.
	$\mathbf{f}_s$ is the elastic force from the spring, which can be directly estimated by (\ref{equation:Forcecalculation}).
	$\mathbf{f}^\prime_d$ is the driving force of the cable at the tool end.
	Ignoring friction, we assume $|\mathbf{f}^\prime_d|$ equals $|\mathbf{f}_d|$ measured by the single-axis force sensor connected to the driving cable at the proximal side.
	Then, $\mathbf{f}^\prime_d$ can be formulated as $\mathbf{f}^\prime_d = {}^{M_0}_{M_t}{}\mathbf{T}\,{}\mathbf{f}_d$. 
	Finally, we can obtain the pushing/pulling force $\mathbf{f}_p$ as
	\begin{equation}
		\mathbf{f}_p = -{}^{M_0}_{M_t}{}\mathbf{T}\,{}\mathbf{f}_d \,-\mathbf{f}_s \, .
		\label{equation:fp} 
	\end{equation} 
    For tissue touching, the contact force can also be estimated by (\ref{equation:fp}) no matter if the forceps are closed or open.
	
%	At this stage, the contact force between the tissue and the forceps' tip can be directly estimated via (\ref{equation:Forcecalculation}), as it equals the force applied to the spring $\mathbf{f}_s$.
%	The contact force when the forceps touch tissue in the closed form can be directly estimated via (\ref{equation:Forcecalculation}).

	\subsubsection{Grasping Force Estimation (Mode-II)}
	The estimation of the grasping force is more challenging because it relates to the geometry and driving force applied to the forceps' cable, as depicted in Fig. \ref{Figure:forcpes}.
	Although the sensing module can estimate the 3D elastic force $\mathbf{f}_s$ and the orientation of $\mathbf{f}^\prime_d$, we mainly consider the situation where the forceps grasp and pull tissue straightly as grasping is performed in the release condition.
	
	% % % % % % % % % % % % Figure forceps integration % % % % % % % % % % % % %
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Grasping_calculation.pdf}
		\caption{ (a) The sectional view of the haptics-enabled forceps when the two jaws are closed.
			(b) shows a state when the two jaws are open at $\theta$.
			$t_d$ is the movement of the driving cable, while $t_s$ is the transformation of the sensing module's head estimated by the camera. 
			(c) The forces applied to the forceps when they grasp and pull a tissue include driving force $F_d$ from the cable, supporting force $F_s$ from the sensing module, contact force $F_N$, and friction force $F_f$ from the tissue.
			$F_g$ denotes the grasping force that is equal to the sum of vertical components of $F_N$ and $F_f$.  
			The left inset shows the forces applied to the grasped tissue, where $F_p$ is the pulling force from the tissue body, $F^\prime_N = F_N $ and $F^\prime_f = F_f$ are forces from the forceps.
			The right inset shows the forces that generate the momentum of one jaw about joint $j_2$.
			$F_d$ is measured by the proximal force sensor, while the driving distance $t_d$ is measured by the upper driver's encoder of the micro-level actuator.
		}
		\label{Figure:forcpes}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
%	the sectional review of the haptics-enabled forceps is shown in Fig. \ref{Figure:forcpes}(a)
	The forces applied to the forceps when they grasp and pull a tissue are shown in Fig. \ref{Figure:forcpes}(c).
    For convenience, we adopt $F_{-}$ to denote the magnitude of force $\mathbf{f}_{-}$ in the following sections.
	%As the grasping and pulling force $F_g$ is significant for tissue manipulation, it should be estimated for supervising and decision making when the forceps grasps tissue.
	$F_s$ is the elastic force from the spring and is directly estimated by (\ref{equation:Forcecalculation}).
	$F_d$ is the driving force applied to the forceps' hinge and measured by the proximal single-axis force sensor.
	$F_N$ and $F_f$ are contact and friction forces from the grasped tissue, and we define the sum of the vertical components of $F_N$ and $F_f$ as the grasping force $F_g$, as shown in Fig. \ref{Figure:forcpes}(c), and it can be formulated as
	\begin{equation}
		{F}_{g} = F_{N_1} \cos{\frac{\theta}{2}} + F_{f_1} \sin{\frac{\theta}{2}} \, ,
		\label{equation:fg_def} 
	\end{equation}
	where $\theta$ is the angle between the forceps' two jaws.
	According to the equilibrium equation on the grasped tissue, as depicted in the left inset of Fig. \ref{Figure:forcpes}(c), the relationship between $F_N$, $F_f$, and external pulling force $F_p$ can be formulated as
	\begin{align}
		\left\{	
		\begin{array}{l}
			F_p=(F_{f_1} + F_{f_2})\cos{\frac{\theta}{2}}-(F_{N_1} + F_{N_2})\sin{\frac{\theta}{2}}\\
			F_{N_1}\cos{\frac{\theta}{2}} + F_{f_1} \sin{\frac{\theta}{2}} = F_{N_2}\cos{\frac{\theta}{2}} + F_{f_2} \sin{\frac{\theta}{2}}
		\end{array} \right. \,.
		\label{equation:tissue_balance}
	\end{align}
	Since the momentum of one jaw about joint $j_2$ is zero, as plotted in the right inset of Fig. \ref{Figure:forcpes}(c), we can further obtain the relationship between $F_N$ and $F_d$ as 
	\begin{align}
		F_{N_1} = \frac{F_d \, l_2 \sin{\alpha}}{2l_3}\, ,
		\label{equation:F_N}
	\end{align}
	where $\alpha = \frac{\theta}{2} + \alpha_0$, the definitions of $\alpha$, $\alpha_0$, $l_2$ and $l_3$ are shown in Fig. \ref{Figure:forcpes}(b).
	$\theta$ and $\alpha_0$ can be calculated based on the geometry relationship of the forceps' links.
	Fig. \ref{Figure:forcpes}(a) shows the forceps' geometry at the initial state, i.e., the forceps are in the closed form, and the flexure is relaxed.
	According to the relationship of forceps' links at the initial state, we can get $\alpha_0$ and formulate it as
	\begin{equation}
		\alpha_0  = \arccos \frac{{l_1^2 - l_2^2 - l_{1,2}^2}}{{2{l_1}{\,}{l_{1,2}}}} \, ,
	\end{equation}
	where $l_{1,2}$ is the distance between forceps' two joints $j_1$ and $j_2$.
	When the forceps are open at angle $\theta$, the geometry changes to Fig. \ref{Figure:forcpes}(b).
	At this stage, the angle ${\alpha}$ can be reformulated as
	\begin{equation}
		{\alpha} = \arccos \frac{{l_1^2 - l_2^2 - l{{_{1,2}^{\prime2}}}}}{{2{l_1}\, {l_{1,2}^\prime}}} \, ,
		\label{equation:alpha}
	\end{equation}
	where $ l{_{1,2}^\prime} = l{_{1,2}} - {t}_d + {t}_s$,
	$t_d$ is the movement of the driving cable measured by the upper driver's encoder of the micro-level actuator,
	and $t_s$ is the transformation of the sensor's head estimated by the sensing module. 
	Then, we can get $\theta$ as
	\begin{equation}
		\theta = 2(\alpha-\alpha_0) \,.
        \label{euquation:theta}
	\end{equation}
	For a pair of forceps, we assume $F_{N_1}$ equals $F_{N_2}$, and $F_{f_1}$ equals $F_{f_2}$.
	Substituting (\ref{equation:fp}), (\ref{equation:tissue_balance}), (\ref{equation:F_N}), and (\ref{euquation:theta}) into (\ref{equation:fg_def}) we obtain ${F}_{g}$ and formulate it as
%	Then, using the equilibrium equation between $F_p$, $F_s$, and $F_d$, we can get the wanted pulling and grasping forces as
%	
%        \begin{align}
%		\left\{\begin{array}{l}
%            F_{p} = F_d-F_s\\
%		  {F}_{g}= \frac{{F}_d{\,}{l_2}{\,}{\sin\alpha}+F_p{\,}l_3{\,}\sin(\alpha-\alpha_0)}{2{l_3}{\,}{\cos(\alpha-\alpha_0)}}	
%        \end{array}
%		\right. \, .
%		\label{equation:grasping_force}	
%	\end{align}
    \begin{equation}
	  {F}_{g}= \frac{{F}_d{\,}{l_2}{\,}{\sin\alpha}+F_p{\,}l_3{\,}\sin(\alpha-\alpha_0)}{2{l_3}{\,}{\cos(\alpha-\alpha_0)}} \, .
	\label{equation:grasping_force}	
	\end{equation}
	% Following the same derivation process, $F_g$ can also be obtained when the forceps grasp and push a tissue, as shown in Fig. \ref{Figure:forcpes}(d), and be formulated as
 % 	\begin{equation}
	% 	{F}_{g}= \frac{{F}_d{\,}{l_2}{\,}{\sin\alpha}-F_p{\,}l_3{\,}\sin(\alpha-\alpha_0)}{2{l_3}{\,}{\cos(\alpha-\alpha_0)}} \, .
	% 	\label{equation:grasping_force2}	
	% \end{equation}
	% Therefore, the grasping force $F_g$ when the forceps push and pull a grasped tissue can be calculated by 
	%  \begin{equation}
	% 	{F}_{g}= \frac{{F}_d{\,}{l_2}{\,}{\sin\alpha}+|F_p|{\,}l_3{\,}\sin(\alpha-\alpha_0)}{2{l_3}{\,}{\cos(\alpha-\alpha_0)}} \, ,
	% 	\label{equation:grasping_force}	
	% \end{equation}

	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%															                      % 
	%       		            													  %
	%   				         	experiments   					      		      %
	%															                      % 
	%															                      % 
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% % % % % % % % % % % % Figure Pose estimation % % % % % % % % % % % % %
\begin{figure}[t]
	\centering
	\includegraphics[width=0.46\textwidth]{figure/EM_tracking.pdf}
	\caption{(a) The experimental setup for evaluating pose estimation.
		The inset shows the configuration, where the EM tracking sensor was installed concentrically to the target.
		(b) The sensor's head was moved towards $x$, $y$ and $z$ directions during the experiments, and $t_z$ denotes the transformation along $z$ axis. 
		(c) The orientation comparison between nine pairs of points that were estimated by the EM tracking system ($\star$) and camera ($\ast$). 
		(d) The position comparison between the continued EM tracking and camera estimation results.
	}
	\label{Figure:EM_tracking}	
\end{figure}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\begin{table}
	\centering
	\caption{Orientation and position evaluation comparison }	
	\begin{tabular}{ccc}
		\toprule
		Experiment & Average Deviation & Max Deviation\\ \toprule
		Orientation & 0.056rad & 0.147rad \\
		Position & 1.8233$\times10^{-6}$mm & 0.2265mm\\
		\hline
	\end{tabular}
	\label{Table:EM_tracking}
\end{table}
  
	\section{Experiments}
	\label{Section:experiments}
    In this section, we first present the experiments on evaluating the method for target tracking and calibrating the stiffness matrix for external force estimation.
	%In addition to force, the marker's pose is vital for the instrument's kinematic model when integrated with the sensor on its tip.
	%In this section, before we calibrate the stiffness matrix for external forces calculation, the maker's pose and position estimations are analyzed to evaluate their feasibility.
	Then, the evaluation of two modes of force estimation is described.
	Finally, groups of automatic robotic grasping procedures with the proposed system are illustrated as potential applications.
	% All the experiments were based on the prototype presented in Fig. \ref{Figure:Introduction}.
	% The experimental setups and results are illustrated in the following.

	\subsection{Evaluation of Target's Pose Estimation}
	\label{subsection:pose_position_evaluation}
	\subsubsection{Experimental Setup}
	Fig. \ref{Figure:EM_tracking}(a) shows the experimental setup for pose estimation evaluation with an electromagnetic (EM) tracking system (Northern Digital Inc, Canada) with resolutions less than 0.1mm in position and 0.1$^\circ$ in orientation.
	The EM tracking sensor was installed concentrically to the proposed force sensing module. 
	During the experiment, the sensor's head was moved along $x$ (red arrow), $y$ (green arrow), and $z$ (blue arrow) as indicated in Fig. \ref{Figure:EM_tracking}(b).
	
	\subsubsection{Pose Estimation Results}
	The orientation comparison is based on nine pairs of samples distributed in the sensing module's workspace, as plotted in Fig. \ref{Figure:EM_tracking}(c).
	Here, we use ($\alpha_{C,i}$, $\beta_{C,i}$, $\gamma_{C,i}$) and ($\alpha_{E,i}$, $\beta_{E,i}$, $\gamma_{E,i}$) to denote the pitch, roll, and yaw angles of camera estimation and EM tracking results of the $i$-th pair.
	The average and maximum deviations between EM tracking and our estimation result are calculated by $\max \frac{1}{n}({\sum\limits_{i = 1}^n {{\alpha_{C,i}} - {\alpha_{E,i}}}} \, ,{\sum\limits_{i = 1}^n {{\beta_{C,i}} - {\beta_{E,i}}}} \, , {\sum\limits_{i = 1}^n {{\gamma_{C,i}} - {\gamma _{E,i}}} })$ and $\max( \max\limits_{i = 1}^n({\alpha_{C,i}} - {\alpha_{E,i}})\, ,\max\limits_{i = 1}^n({\beta_{C,i}} - {\beta_{E,i}}) \, ,\max\limits_{i = 1}^n({\gamma_{C,i}} - {\gamma_{E,i}}))$, where $n=9$.
	For position evaluation, the EM sensor and target were tracked continuously by the EM tracking system and camera, respectively, and the comparison is shown in Fig. \ref{Figure:EM_tracking}(d). 
	To calculate the average and max deviations between the EM and camera-tracking traces, we further calculated the average ($d_a$) and Hausdorff ($d_h$) distance by
	\begin{align}
		\left\{\begin{array}{l}
			d_a = \max(\frac{1}{n}\sum\limits_{i = 1}^n {{d_{{C_i},E}}} \, ,\frac{1}{m}\sum\limits_{j = 1}^m {{d_{{E_j},C}}})\\
			{d_h} = \max (\mathop {\max }\limits_{i = 1}^n ({d_{C_i,E}}),\mathop {\max }\limits_{j = 1}^m ({d_{E_j,C}}))
		\end{array}
		\right. \, ,
		\label{equation:hausdorff_distance}	
	\end{align}
	where ${d_{{C_i},E}} = \mathop {\min }\limits_{j = 1}^m (||{\mathbf{p}_{C,i} - \mathbf{p}_{E,j}}||_2)$, 
 $\mathbf{p}_{C,i}$ and $\mathbf{p}_{E,j}$ denote positions of the camera estimation of point $i$ and EM tracking of point $j$, 
 ${d_{{E_j},C}} = \mathop {\min }\limits_{i = 1}^n (||{\mathbf{p}_{E,j} - \mathbf{p}_{C,i}}||_2)$,
    $n$=1336 and $m$=2181 are the numbers of points recorded by the camera and EM tracking system, respectively.
	As listed in Table \ref{Table:EM_tracking}, the average and maximum deviations of the orientation are 0.056rad and 0.147rad, and those of the position are 1.8$\times10^{-6}$mm and 0.2265mm.
	This indicates the proposed method can track and estimate the target's pose reliably.
	
	\subsection{Stiffness Matrix Calibration and Verification}
	\label{Experiments:stiffness_matrix}
	%A group of experiments will be carried out with 4mm, 5mm, and 6mm sensors, and compression will also be made based on these experiments.
	%Then a comparison between with and without integrated forceps will also be made.
	%Finally, we will show the grasping force measurement when the sensor is integrated with a pair of forceps.	
	\subsubsection{Experimental Setup}
	We used a series of weights to calibrate the haptics-enabled forceps' stiffness matrix.
	The calibration platform is shown in Fig. \ref{Figure:TFCalibration}(a).
	%It comprises the micro-level actuator and an orientation module.
	Here, the micro-level actuator is used to adjust the position and hold the forceps.
	An orientation module is installed concentrically to the forceps for adjusting the direction of the pulling force provided by a cable.
	This cable is connected to the forceps' jaws, and the force $F_w$ applied to it is adjusted by adding/removing weights.
	%A linear movement platform was adopted as the based of the proposed sensor, which could move and rotated the sensor in different directions. 

 	% % % % % % % % % % % % Figure: calibration setup and verification % % % % % % % % % % % % %
	\begin{figure}[t!]
		\centering
		\includegraphics[width=0.46\textwidth]{figure/TFCalibration.pdf}
		\caption{(a) The experimental setup for stiffness matrix calibration.
			The forceps were pulled by a cable connected with changeable weights.
			The orientation module was used to adjust the pull direction of the forceps.
			(b) shows the setup for calibration data collection.
			(c) shows the setup for verification data collection, where the orientation module was rotated 45$^\circ$ relative to that for calibration.
			$v_1$, $v_2$, $v_3$, and $v_4$ indicate the pulling directions for verification in $x$-$y$ panel.
			(d) and (e) show the verification results in $x$-$y$ panel and $z$-axial direction, respectively.
			$F_w$ and $F_e$ denote the force generated by the weight and the estimated result. 
		}
		\label{Figure:TFCalibration}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
 
	
	\subsubsection{Calibration and Verification}
	Two groups of data were collected for calibration and verification, respectively.
	When collecting the data for calibration in the $x$-$y$ panel, as shown in Fig. \ref{Figure:TFCalibration}(b), the orientation module made the cable align with the $x$ and $y$ axis, and the weight was increased from 0N to 0.49N at intervals of 0.07N.
	Then, for the $z$-axis direction, the driving force was increased from 0N to 5N at intervals of 0.5N.
	The target pose for each weight was recorded. 
	%\textbf{Some theoretical articles are also need to be cited in this section to support our calibration method. \cite{,}} 
	Then, substituting the calibration data to (\ref{equation:Forcecalculation}), we obtained the stiffness matrix $\mathbf{K}_s$ as
	\begin{align}
		\mathbf{K}_s = \left[ 
		{\begin{array}{*{20}{c}}
				{95.92}&{9.32}&{-1.84}\\
				{12.10}&{88.07}&{-1.70}\\
				{0.13}&{0.12}&{385.20}
		\end{array}} 
		\right] \, . \notag
		\label{equation:stiffness matrix}
	\end{align}
	
	To verify the calibrated $\mathbf{K}_s$, we rotated the orientation module for $45^\circ$, as shown in Fig. \ref{Figure:TFCalibration}(c), and collected the data following the same procedure of calibration.
	For the $x$-$y$ panel the interval was set to 0.035N, while for the $z$-axial direction the interval was 0.02N. 
	The comparison of weights and estimated forces are plotted in Fig. \ref{Figure:TFCalibration}(d) and (e), which show the results of the $x$-$y$ panel and the $z$-axial direction, respectively. 
	The average and maximum errors of ($f_x$, $f_y$, $f_z$) in these two comparison are ($0.0009$, $0.0013$, $0.0105$)N and ($0.0177$, $0.0154$, $0.0995$)N.
	This indicates the calibrated $\mathbf{K}_s$ can be used to estimate the force applied to the forceps.




\subsection{Evaluation of Mode-I: $F_p$ Estimation}
In this experiment, the grasped tissue was pulled in various directions by a cable connected to a force sensor.
Fig. \ref{Figure:Pulling_Orientations}(a) shows that the grasped tissue is pulled from the initial state o$_0$ to $+y$ (o$_1$), $+x$ (o$_2$), $-y$ (o$_3$) and $-x$ (o$_4$).
Snapshots show the states when the flexure has maximum deformation in different orientations, and the corresponding target positions are annotated below them.
Fig. \ref{Figure:Pulling_Orientations}(b) shows the forces estimated by the proposed sensing module (o$_{e,i}$) and that measured by the commercial sensor (o$_{r,i}$) connected to the tissue-pulling cable.
Table \ref{Table:Robotic_Pulling} lists the comparison result of $F_p$ between o$_{e,i}$ and o$_{r,i}$.
The average and maximum errors of the four tests are under 0.02N and 0.14N, which indicates the forceps are valid for estimating $F_p$ in various directions. 


% % % % % % % % % % % % Figure Pulling force estimation in various orientation % % % % % % % % % % % % %
    \begin{figure}[t!]
    	\centering
    	\includegraphics[width=0.45\textwidth]{figure/Pulling_Orientations.pdf}
    	\caption{The evaluation of Mode-I ($F_p$ estimation) in various directions.
    		(a) A grasped tissue was pulled by a cable from the initial state o$_0$ to $+y$ (o$_1$), $+x$ (o$_2$), $-y$ (o$_3$) and $-x$ (o$_4$).
    		Snapshots show states when the flexure has maximum deformation in different orientations, with the corresponding target positions (in mm) annotated below them.
    		(b) $F_p$ estimated by our proposed sensing module (o$_{e,i}$) and that measured by the reference commercial sensor (o$_{r,i}$).}
    	\label{Figure:Pulling_Orientations}	
    \end{figure}
    \begin{table}[t]
    	\centering
    	\caption{Errors of $F_p$ estimation in various orientation}	
    	\begin{tabular}{ccccc}
    		\toprule
    		Error &o$_{1}$ & o$_{2}$ & o$_{3}$ & o$_{4}$ \\ \toprule
    		Average/N & 0.0302  & 0.0380 & 0.0279 & 0.0233\\
    		Max/N & 0.0766 & 0.1309 & 0.1281 & 0.0637\\
    		\hline
    	\end{tabular}
    	\label{Table:Robotic_Pulling}
    \end{table}	

\subsection{Evaluation of Mode-II: $F_g$ Estimation}
	\label{subsection:grasping_experiment}

	\subsubsection{Experimental Setup}
	Fig. \ref{Figure:Grasping_setup}(a) shows the experimental setup, where a commercial pressure sensor (FlexiForce A201-1 lbs, Tekscan, USA), with a resolution of 0.02N, and a force sensor (ZNLBS-5kg, CHINO, China) were adopted as references.
	The pressure sensor was grasped by the forceps for direct measurement of the grasping force.
	Because the pressure sensor's sensing area was a 10mm-diameter circle, 3D-printed surfaces were attached to the forceps' jaws to ensure sufficient contact.
	The driving force reached around 7N during the grasping action in each experiment.
    Since, from (\ref{equation:grasping_force}), we can see grasping force $F_g$ is also related to pulling force $F_p$, we connected the pressure sensor to a commercial force sensor by a flexure to measure the pulling force applied to it.



	\subsubsection{Evaluations}
	We carried out three experiments with different $\theta$ configurations.
	Each experiment was presented as a continuous process, and the results are shown in Fig. \ref{Figure:Grasping_setup}(b), (c), and (d), where the subscript $i\in$(1,2,3) denotes the $i$-th experiment that corresponds to  $\theta = 10^\circ$, $30^\circ$, and $50^\circ$.
	The forceps grasped the pressure sensor and reached the target grasping force during 0$\sim$18s.
	After a pause phase (18$\sim$26s), the forceps pulled the grasped sensor backward for 25mm in the pulling phase (26$\sim$65s). 



 
	%Here, three groups of experiments have been carried out to evaluate the estimation results with different $\theta$ configurations.
	%Here, we use $f_{g_{r,i}}$ to denote the $i$-th experiment's force measured by pressure sensor, and $f_{g_{e,i}}$ to denote our sensor's estimation result.
	%The results are shown in Fig. \ref{Figure:Grasping_setup}(b), (c), and (d), and the procedure can be divided into three phases: grasping, pause, and pulling.
	
	Fig. \ref{Figure:Grasping_setup}(b) depicts $F_d$ provided by the driving cable and $F_s$ measured by our proposed sensing module.
	%During the grasping phase, the micro-level actuator drives the forceps to a targeted axial displacement, 2mm at here.
	%The driving force $f_d$ and the sensor's supporting force $f_s$ increase until the procedure transforms to pause.
	$F_s$ and $F_d$ are almost equal in the grasping and pause phases.
	This is because the pulling force applied to the forceps was almost zero in these two phases, as there was no pulling action, and the pressure sensor was floating.
	On the contrary, in the pulling phase, $F_d$ increases while $F_s$ keeps constant.
	This is because the pressure sensor has been pulled, but the flexure deformation keeps constant. 
	The difference between $F_d$ and $F_s$ is the pulling force $F_p$ applied to the pressure sensor.
	As the three experiments followed the same procedure, their data almost overlay each other.
	However, because $\theta$ is configured differently, the time for performing the grasping is slightly different.
	The inset in Fig. \ref{Figure:Grasping_setup}(b) shows that the bigger $\theta$ is, the quicker the grasping finishes.


	Fig. \ref{Figure:Grasping_setup}(c) shows the commercial sensor measured pulling force $F_{p_{r,i}}$ in the experimental procedure, and we compared it with forceps estimated result $F_{p_{e,i}}$.
 % compares the estimated and the measured pulling forces $F_p$ in this experimental procedure, where $F_{p_{e,i}}$ and $F_{p_{r,i}}$ denote the forces of the $i$-th experiment measured by our proposed forceps and the reference commercial sensor, respectively.
	We can see that the pulling force of these three experiments almost overlay each other as the pulling distances are equal.
	%The average and maximum differences between the $f_{p_e}$ and $f_{p_r}$ are listed in Table \ref{Table:Grasping_Pulling}.
	A comparison between the commercial sensor results $F_{p_r}$ and our estimations $F_{p_e}$ is listed in Table \ref{Table:Grasping_Pulling}. 
	The average and maximum errors of the three experiments are under 0.11N and 0.3N, respectively.
	This also reflects the forceps are valid for estimating the pulling force when they grasp tissue. 
	%0.299N and 0.090N for $10^\circ$, 0.278N and 0.014N for $30^\circ$, 0.1937N and -0.1055N for $50^\circ$. 

	
	Fig. \ref{Figure:Grasping_setup}(d) compares the grasping force $F_g$, where $F_{g_{e,i}}$ and $F_{g_{r,i}}$ denote the forces of $i$-th experiment estimated by our proposed forceps and commercial pressure sensor, respectively.
	Because of the additional surfaces that are shown in the inlet of Fig. \ref{Figure:Grasping_setup}(a), $l_3$ in (\ref{equation:grasping_force}) was set as $l^{\prime_3}$ in these experiments.
	%the grasping force measured by the pressure force sensor was equal $\frac{l^{\prime_3}}{l_3}$ of the force applied on the forceps' clips.
	%For convenient comparison, we scaled the latter to the scale of the former.
	According to Fig. \ref{Figure:Grasping_setup}(d), we can see that when the driving force $F_d$ is equal, the grasping force $F_g$ is positively related to $\theta$.
	This phenomenon conforms with the force calculation method (\ref{equation:grasping_force}).
	%A comparison between the commercial sensor results $f_{g_r}$ and our estimations $f_{g_e}$ is also listed in Table \ref{Table:Grasping_Pulling}. 
	%the bigger the $\theta$ is, the bigger the grasping force $f_g$ will be. 
	%This is verified through these three experiments.
	%Table \ref{Table:Grasping_Pulling} lists the average and maximum differences between the $f_{g_{e,i}}$ and $f_{g_{r,i}}$. 
	%The average and maximum differences between the $f_{g_{e,i}}$ and $f_{g_{r,i}}$ are listed in Table \ref{Table:Grasping_Pulling}.
	Table \ref{Table:Grasping_Pulling} lists the average and maximum errors between $F_{g_r}$ and $F_{g_e}$ of the three experiments, and they are under 0.03N and 0.11N, respectively.
	This indicates the forceps are valid for estimating the grasping force.
%	Another interesting phenomenon observed from Fig. \ref{Figure:Grasping_setup}(d) is that although $F_g$ is related to $F_p$, the dependence ratio is relative lower compared to that of $F_s$ especially when the $\theta$ is small, and this can also be confirmed by (\ref{equation:grasping_force}).   
	%0.0278N and -9.7853e-04N for $10^\circ$, 0.0298N and -0.0046N for $30^\circ$, 0.0104N and -0.1022N for $50^\circ$.

		% % % % % % % % % % % % Figure grasping setup % % % % % % % % % % % % %
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.48\textwidth]{figure/Grasping_setup.pdf}
		\caption{The evaluation of Mode-II ($F_g$ estimation).
			(a) The experimental setup, where a commercial pressure sensor and a force sensor were used as references for grasping and pulling forces, respectively.
			The inset shows three 3D-printed contact surfaces configured with $\theta$ = $10^\circ$, $30^\circ$ and $50^\circ$. 
			(b) shows $F_d$ provided by the driving cable and $F_s$ measured by our proposed force sensing module.
			Subscript $i \in$(1,2,3) denotes the $i$-th experiment that correspond to $\theta = 10^\circ$, $30^\circ$ and $50^\circ$.
			The inset shows that grasping time varies for different $\theta$, and the red arrow shows the increased direction of $\theta$. 
			(c) The comparison of pulling force $F_p$, where $F_{p_{e,i}}$ and $F_{p_{r,i}}$ denote the forces of the $i$-th experiment estimated by our proposed sensing module and that measured by the commercial sensor, respectively.
			(d) The comparison of grasping force $F_g$, where $F_{g_{e,i}}$ and $F_{g_{r,i}}$ denote the forces of the $i$-th experiment estimated by our proposed sensing module and that measured by the reference commercial pressure sensor, respectively.
		}
		\label{Figure:Grasping_setup}	
	\end{figure}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	
	\begin{table}
		\centering
		\caption{Errors of $F_g$ and $F_p$ estimations in straightly grasping procedure}	
		\begin{tabular}{ccccc}
			\toprule
			Force & Error & $10^\circ$ & $30^\circ$ & $50^\circ$ \\ \toprule
			$F_p$/N & \makecell{Average \\ Max}  & \makecell{0.0898\\0.2985} & \makecell{0.0138\\0.2782 } & \makecell{0.1144\\0.1803}\\ \hline
			$F_g$/N & \makecell{Average \\ Max} & \makecell{9.7853$\times10^{-4}$\\0.0280} &\makecell{0.0046\\0.0298}&\makecell{0.0049\\0.0229}\\
			\hline
		\end{tabular}
		\label{Table:Grasping_Pulling}
	\end{table}
 

	
	%% % % % % % % % % % % % Figure grasping force result % % % % % % % % % % % % %
	%\begin{figure}[t!]
	%	\centering
	%	\includegraphics[width=0.48\textwidth]{figure/Grasping_force.PNG}
	%	\caption{Grasping forces result. (a), (b), (c) and (d) correspond the forceps grasped pressure sensor with $\theta$ = $10^\circ$, $30^\circ$, $50^\circ$ and $70^\circ$(maximum angle), respectively.}
	%	\label{Figure:Grasping_result}	
	%\end{figure}
	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
	


 
\subsection{Ex vivo Robotic Experiments}
\label{Subsection:Dynamic_Experiment}
\subsubsection{Experimental Setup}
To further verify the feasibility of the proposed system, we conducted a group of robotic experiments on an ex vivo tissue.
Fig. \ref{Figure:Robotic_setup} shows the experimental setup, where a \textit{UR}-5 robotic arm was adopted as the macro-level actuator. 
The ex vivo chicken tissue was placed in a human body phantom to simulate the lesion, and the instrument was implemented through a simulated minimally invasive port on the phantom.

%	\subsubsection{Touching from Various Orientations}
%	%One of the straightforward applications of force feedback in minimally invasive surgery is palpation \cite{konstantinova2014implementation}.
%	Firstly, a series of robotic touching experiments were conducted.
%	A 3mm bean was placed under the targeted tissue to mimic the tumour, and the forceps touched the targeted tissue from various orientations.
%	The experimental scenes and results are shown in Fig. \ref{Figure:TouchforGrasping}, where o$_1$, o$_2$, o$_3$, o$_4$, and o$_5$ indicate different touching orientations.
%	The threshold for touch detection in this experiment was 0.05N in the axial direction.
%	The $x$ and $y$ components of the target's position changed visibly when the forceps touched the targeted tissue from non-vertical orientations, such as in o$_2$, o$_3$, o$_4$, and o$_5$.
%	This was because in this situation a lateral force was applied to the forceps, and the tip slides.
%	However, when the forceps touched the tissue from a vertical orientation, as in o$_1$, the $x$ and $y$ components almost kept constant.
%	%When the forceps touch the targeted tissue in an none-vertical orientation, slides occurred, denoted as the white arrows in Fig. \ref{Figure:TouchforGrasping}. 
%	%Five traces show the change in the marker's position during the touching procedures.  
%	%Furthermore, the slide is small when the forceps touch the targeted tissue from an approximated vertical direction (o$_1$).
%	The final displacements of the target are also attached below the experimental scenes.
%	%to indicate the tip orientations of the forceps.
%	With this feature, the haptics-enabled forceps can help the operator or robotic system to evaluate whether the forceps' tip are in an optimal position for grasping.


	\subsubsection{Automatic Tissue Grasping}
	We carried out automatic tissue grasping procedure with one targeted grasping force $F_g^{\ast}$ and two different targeted pulling forces $F_p^{\ast,1}$ and $F_p^{\ast,2}$.
	%Each experiments were also presented as continuous process
	The key experimental scenes and results are shown in Fig. \ref{Figure:Robotic_grasping}, and the experiments can be roughly divided into five phases.
	Transformations between these phases were automatically performed depending on the grasping force $F_g$ and pushing/pulling force $F_p$ estimated by the haptics-enabled forceps.
	
	With Mode-I enabled, the forceps were driven to touch the tissue during the touching phase (0$\sim$7.5s) until $F_p$ reached the touching detection threshold $F_p^{\ast,t}$ followed by a short pending period (7.5$\sim$9.5s). 
%	Then, the forceps were fully opened during the opening phase (12$\sim$19s) to prepare for the tissue grasping.
%	To ensure firm grasping, the forceps were driven to touch the tissue again during the second touching phase (19$\sim$26s).
%	
	Then, Mode-II was enabled, and the forceps performed the tissue grasping in the grasping phase (9.5$\sim$13.5s) until $F_g$ reached the targeted value $F_g^{\ast}$. 
	Finally, with another short pending period (13.5$\sim$15.5s), the grasped tissue was pulled up and held for a while with targeted pulling force $F_p^{\ast}$.
	Because the targeted pulling force $F_p^{\ast}$ for the first and second group experiments were different, the time consumed for pulling varied. 
	For the first group with $F_p^{\ast,1}$=0.4N, the period was around 6s, while for the second group with $F_p^{\ast,2}$=0.2N, it was around 2s.
	% $F_p^{\ast,1}$ and $F_p^{\ast,2}$ were estimated and controlled by the haptics-enabled forceps.
	
%	The desired grasping forces of these two experiments were different and controlled by the flexure's axial deformation.
%	For experiment one (Exp1), the deformation was set as 2mm, while for experiment two (Exp2), it was set as 1mm.
%	Consequently, the periods for forceps to perform grasping and pulling are different. 
%	They are 26$\sim$38s and 38$\sim$63s for Exp1, while 26$\sim$46s and 46$\sim$71s for Exp2. 

	
% % % % % % % % % % % % Figure: The setup of robotic experiment % % % % % % % % % % % % %
\begin{figure}[t!]
	\centering
	\includegraphics[width=0.45\textwidth]{figure/Robotic_setup.pdf}
	\caption{The setup of robotic experiments. 
		A 3D-printed human body phantom was used for placing ex vivo chicken tissue that simulated the lesion.
		The instrument was implemented through a simulated MIS port on the phantom.}
	\label{Figure:Robotic_setup}	
\end{figure}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


%% % % % % % % % % % % % Figure: touch for grasping optimization % % % % % % % % % % % % %
%\begin{figure}[t!]
%	\centering
%	\includegraphics[width=0.48\textwidth]{figure/Robotic_touch.pdf}
%	\caption{The results of robotic touch experiment.
%		o$_1$, o$_2$, o$_3$, o$_4$, and o$_5$ denote the tip touching orientation of the forceps.
%		White arrows indicate the tip's sliding directions of the forceps.
%		Five colored traces show the changes in the target's position during the touching procedures.  
%		The final positions of the target are shown below the experimental scenes of these five touching procedures, which indicate the final tip orientations of the forceps.
%	}
%	\label{Figure:TouchforGrasping}	
%\end{figure}
%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

% % % % % % % % % % % % Figure: Robotic_grasping % % % % % % % % % % % % %
\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.96\textwidth]{figure/Robotic_procedure.pdf}
	\caption{The results of automatic tissue grasping experiments, where experiments can be classified into two groups with two different target grasping forces $F_p^{\ast}$. 
		Experiments of the first group are marked with footnotes 1, 2, 3, and 4, while those of the second are marked with footnotes 5, 6, 7, and 8.
		(a) Key snapshots of the experimental procedure are attached with target tracking results. 
		(b) shows the measured driving force $F_d$ and the estimated elastic force $F_s$ of these experiments.
		Plot (b) also shows the experimental phases.
		Touching means the forceps touch the targeted tissue in the opened form.
		Pending means the speeds of the system's motors have been set to zero for waiting.
		Grasping means the forceps grasp the touched tissue. 
		Pulling means the forceps pull the grasped tissue.
		(c) shows the estimated $F_g$, where $F_g^{\ast}$=0.4N is the targeted grasping force for grasping action.
		The partial enlargement shows each experiment has reached $F_g^{\ast}$ successfully and is marked with a hexagon.   
		(d) shows the estimated $F_p$, where $F_t^{\ast}$=-0.05N is the threshold for touch detection, while $F_p^{\ast,1}$=0.4N and $F_p^{\ast,2}$=0.2N are two target pulling forces for the first and second group experiments, respectively.
		The left partial enlargement shows the touching detection and is marked with a four-pointed star for each experiment.
		The two right partial enlargements show the reaching of $F_p^{\ast}$ of two group experiments and are marked with the pentagram.
	}
	\label{Figure:Robotic_grasping}	
\end{figure*}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

	%The forceps grasped the tissue and reached the targeted grasping force during  
	%Fig. \ref{Figure:Robotic_grasping}(a) shows key experimental captures of automatic tissue grasping procedure. 
%	Fig. \ref{Figure:Robotic_grasping}(b) shows the estimated sensing module's elastic force $F_s$.
%	The forces of the two experiments are almost the same during the first three phases, as they followed the same procedure.
%	However, because the flexure axial deformation of Exp1 is larger than that of Exp2, $F_s$ of Exp1 is also greater than that of Exp2 in the last two phases.  
%	Fig. \ref{Figure:Robotic_grasping}(c) shows the driving force $F_d$ recorded by the force sensor that is connected to the driving cable, which also shows the same trends as $F_s$.
%	Fig. \ref{Figure:Robotic_grasping}(d) shows the estimated pulling force $F_p$.
%	Although the pulling distances of Exp1 and Exp2 are equal, the pulling force of Exp1 is greater than that of Exp2.
%	This is likely because Exp1 was conducted before Exp2, and the tissue has been loosened in Exp1.
%	Fig. \ref{Figure:Robotic_grasping}(e) shows the estimated grasping force $F_g$, where the final $F_g$ of Exp1 doubles that of Exp2 because the displacement of Exp1 is twice that of Exp2.
	%does here need an algorithm to describe the whole procedure?
	Fig. \ref{Figure:Robotic_grasping}(b) shows the measured driving force $F_d$ and the estimated elastic force $F_s$ of these experiments.
	Fig. \ref{Figure:Robotic_grasping}(c) shows the estimated $F_g$, where $F_g^{\ast}$=0.4N is the targeted grasping force for grasping action.
	% The partial enlargement shows each experiment has reached $F_g^{\ast}$ successfully and is marked with a hexagon.   
	Fig. \ref{Figure:Robotic_grasping}(d) shows the estimated $F_p$, where $F_g^{\ast,t}$=-0.05N is the threshold for touch detection, while $F_p^{\ast,1}$=0.4N and $F_p^{\ast,2}$=0.2N are two target pulling forces for the first and second group experiments, respectively.
	% The left partial enlargement shows the touching detection and is marked with a four-pointed star for each experiment.
	% The two right partial enlargements show the reaching of $F_p^{\ast}$ of two group experiments and are marked with the pentagram.
	According to the experiment results, we can see that the haptics-enabled forceps can be implemented in robotic surgery for multi-modal force sensing.
	%, including contact force, grasping force and pulling force.
	With the proposed forceps, some surgical procedures can be performed automatically, and more information will be output for decision-making, i.e., it is potentially beneficial to task-autonomous robotic surgery. 
%	such as thyroidectomy, ENT surgery, and laparoscopic surgery.
	%\textbf{More complex and delicate application will be investigated in future works.}
	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %		

%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	%															                      % 
%	%       		            													  %
%	%   				         	discussion   					      		      %
%	%															                      % 
%	%															                      % 
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	
%	\section{Discussion}
%	\label{Section:Discussion}
% 	
%	For different applications and instruments, the requirements for sensing resolution and size are various.
%	As mentioned before, the proposed sensing module and force estimation method have a certain generality, i.e., the module can be customized into different configurations.
%	In this section, a brief discussion is presented to illustrate the situations of the sensing module with different configurations.
%	%In this section, a brief discussion is presented including sensors with different configurations, current limitations, and potential optimizations.
%	
%	\subsection{Different Flexures}
%	The resolution can be changed by choosing flexures with different stiffness properties.
%	To demonstrate this, we replaced the spring of the prototype presented in Fig. \ref{Figure:Sensor_Calculation_Design} with a less stiff one.
%	The new spring had the same parameters as the old one, but the wire diameter changed from 0.5mm to 0.3mm. 
%	Following the same procedure of Section \ref{Experiments:stiffness_matrix}, we obtained the stiffness matrix of the 0.3mm module as  
%	\begin{equation*}
%		\mathbf{K}_s = \left[ {\begin{array}{*{20}{c}}
%				{24.68}&{3.65}&{-1.08}\\
%				{-1.26}&{23.99}&{-0.96}\\
%				{-0.00}&{-0.01}&{45.47}\\
%		\end{array}} \right] \, . \notag
%		\label{equation:stiffness_matrix_bigger}
%	\end{equation*}
%	%A verification of this calibrated matrix was also conducted, and the result is shown in Fig. \ref{Figure:Resolution_comparsion}(a).
%	To compare the resolutions of these two modules intuitively, we draw their corresponding forces along $x$, $y$, and $z$ axes with the same displacements (0$\sim$1mm), as shown in Fig. \ref{Figure:Resolution_comparsion}.
%    We can see that the force magnitudes of the 0.5mm-wire module are around 3.9, 3.7, and 8.5 times that of the 0.3mm-wire module along $x$, $y$, and $z$ axes.
%	This indicates the resolution of the 0.3mm-wire module is higher than that of the 0.5mm-wire module, and the resolutions can be configured by choosing different flexures.
%
% 	% % % % % % % % % % % % Figure resolution comparison % % % % % % % % % % % % %
%	\begin{figure}[t!]
%		\centering
%		\includegraphics[width=0.48\textwidth]{figure/Discussion_resolution.pdf}
%		\caption{The estimated forces of the 0.5mm-wire ($F_1$) and 0.3mm-wire ($F_2$) sensing moudles with the same displacements (0$\sim$1mm) in $x$, $y$, and $z$ directions. 
%		}
%		\label{Figure:Resolution_comparsion}	
%	\end{figure}
%	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%	
%	% % % % % % % % % % % % Figure Pose estimation % % % % % % % % % % % % %
%	\begin{figure}[t]
%		\centering
%		\includegraphics[width=0.48\textwidth]{figure/Tracking_6.pdf}
%		\caption{(a) The orientation comparison between camera estimation and EM tracking of a 5.4mm-diameter target that was installed 10mm to the camera, where the average deviation is 0.063rad, and the max deviation is 0.1089rad occurs at $\beta$;
%			(b) The position estimation result of the 5.4mm-diameter target, where the average deviation is 7.4e-6mm, and the max deviation (Hausdorff distance) is 0.796mm.}
%		\label{Figure:Different_marker}	
%	\end{figure}
%	% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%	
%	%we moved their heads in $x$ and $z$ axis with 1mm, respectively.	
%	
%	%\subsection{Different Sensors Configurations}
%	\subsection{Different Targets}
%	Resolution changes may also accompany size changes due to different flexure sizes.
%	When the sensing module is made in a different size, the target size and distance to the camera may change.
%	%This may have an influence on the marker's pose and position estimation and affect the force calculation.
%	To study whether this has an influence on the target pose and force estimations, we made a 6mm-diameter module with a 5.4mm target that was installed 10mm to the camera.
%	Fig. \ref{Figure:Different_marker} shows the target's pose estimation results of this module.
%	Compared to the EM tracking results, the average and maximum deviations of the estimated results are 0.063rad and 0.1089rad in the orientation, and 7.4$\times10^{-6}$mm and 0.796mm in the position, respectively. 
%	%a $0.063rad$ in average and $0.109rad$ maximum deviation compared to the EM tracking results.
%	%This result 
%	These results are generally similar to that of the 4mm diameter prototype presented in Section \ref{subsection:pose_position_evaluation}.     
%	%camera estimation results are relatively similar in 6mm-diameter(10mm in length) and 4mm-diameter (5 mm in length)sensors.    
%	This indicates the proposed method for pose estimation has certain robustness for the target size and installing distance, which can support the size customization without compromising the precision.
%	
%	The above discussion showed that one can choose a suitable sensing module by configuring the size and stiffness according to different applications and instruments.
%    In addition, since the sensing module has the ability to estimate the elastic force $F_s$, it can also be used as an independent multi-dimensional force sensor as well.
%
%	
%	
%	%% % % % % % % % % % % % Figure stifnees calibration/verification 6mm % % % % % % % % % % % % %
%	%\begin{figure}[t]
%	%	\centering
%	%	\includegraphics[width=0.45\textwidth]{figure/ToDo.jpg}
%	%	\caption{The stiffness matrix calibration and verification.}
%	%	\label{Figure:Calbiration_bigger}	
%	%\end{figure}
%	%% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
%	
%	
%	
%	% and the statistical results are listed in Table .\ref{Table:Calibration_result_bigger}. 
%	
%	%\begin{table}
%	%	\centering
%	%	\caption{Force/Torque calibration verification}	
%	%	\begin{tabular}{ccccccc}
%		%		\toprule
%		%		Experiments & $F_x$/N & $F_y$/N & $F_z$/N \\
%		%		Average Error & &  &  \\
%		%		Max Error & &  & \\
%		%		\hline
%		%	\end{tabular}
%	%	\label{Table:Calibration_result_bigger}
%	%\end{table}
%	
%	%\textbf{The resolution of the proposed force sensor relates to both the camera's rate and resolution and the flexure's stiffness.}
%	%1mm movement of the marker corresponds xxmm(xx pixels) in image planes.
%	%As the stiffness matrix has changed, the resolution of the force estimation will be different.
%	
%	%In order to intuitively present this, moving and rotating the sensors used in this Section and Section \ref{Section:experiments} in $x$ and $z$ axis, a comparison was conducted.
%	%Fig. \ref{Figure:Resolution_comparsion}(b) shows that \textbf{resolution} FFFFFFF\\
%	%F\\
%	%F\\
%	%F\\
%	%F\\
%	%F\\
%	%F\\
%	%In addition, with increasing stiffness, smaller displacement is required for the sensor to counteract external forces.
%	%For example, a smaller and high-resolution sensor can be made for some delicate organs and curves. 
%	%In contrast, a stiffer sensor can be adopted in some application that requires counteracting greater external forces.
%	
%	%\subsection{Limitations and Potential Optimizations}
%	%There are still some limitations and potential optimizations.
%	%The first limitation is that even in a cylindrical shape, the current sensor's anisotropy still exists, i.e., the force range and resolution in the axial and lateral directions are different.
%	%Two aspects cause this difference.
%	%The first is that the stiffness characteristic of the spring varies in directions.
%	%The second is that the camera is more sensitive to the marker's displacement aligning to $x$ and $y$ directions. 
%	%Finally, because the manufacturing process is not yet standardized, each sensor requires calibration to get its practical stiffness matrix. 
%	%The potential optimization includes customizing the flexure and making it has approximated stiffness in various directions, and manufacturing the marker in shape with minimized anisotropy such as hemispherical.   
%	
%	
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	%												  %
%	%												  %
%	%   	   		  conclusion				      %
%	%                                                 %
%	%												  %
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Conclusion}
	\label{Section:conclusion}
	This paper presented a vision-based force sensing module that is adaptive to micro-sized biopsy forceps.
	An algorithm was designed to calculate the force applied to the sensing module with a registration method for tracking and estimating the pose of the sensing module's target.
	Integrating the developed sensing module into the biopsy forceps, in conjunction with a single-axis force sensor at the proximal end, a haptics-enabled forceps was further proposed.
	Mathematical equations were derived to estimate the multi-modal force sensing of the haptics-enabled forceps, including pushing/pulling force (Mode-I) and grasping force (Mode-II).
	The methods for estimating multi-modal forces were presented with experimental verification.
	Groups of automatic robotic ex vivo tissue grasping procedures were conducted to further verify the feasibility of the proposed sensing method and forceps. 
	The results show that the proposed method can be used for enabling multi-modal force sensing of the biopsy forceps, and the haptics-enabled forceps are potentially beneficial to automatic tissue manipulation in applications such as thyroidectomy, ENT surgery, and laparoscopic surgery.
	The forceps will be integrated into a flexible robotic surgical system to further study the benefits of multi-modal force sensing for MIS. 
	% More complex tissue manipulation will be conducted based on the new system in our future work.
 	
%	1) A vision-based force-sensing module for micro-sized biopsy forceps was developed. 
%	An algorithm is designed to calculate the force applied to the sensing module with a registration method for tracking and estimating the pose of sensing module's target.\\ 
%	2) Integrating the developed sensing module into the biopsy forceps, in conjunction with a single-axial force sensor at the proximal end, a haptics-enabled forceps is further proposed.\\ 
%	3) Mathematical equations are derived to estimate the multi-modal force sensing of the haptics-enabled forceps, including pushing/pulling force estimation (modal-I) and grasping force estimation (modal-II).\\
%	%	and forceps' geometry is further proposed to estimate the touching, grasping, and 3D pulling forceps.
%	These contributions are validated by various carefully designed experiments, including an automatic tissue grasping procedure on ex vivo tissues. 
	
	
%	This paper presented a micro-sized vision-based multi-axis force sensing module, which can be customized for different applications and instruments by reconfiguring its target and flexure.
%    A pair of haptics-enabled forceps have been proposed by integrating this force sensing module with a micro-level actuator to drive the forceps and compensate for the flexure motion.  
    % This sensing module has been integrated into a pair of forceps to enable multi-modal force sensing with

	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%																	%
	%																	%
	%																	%
	%																	%
	%																	%
	%						     References  							%
	%																	%
	%																	%
	%																	%
	%																	%
	%																	%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\bibliographystyle{IEEEtran}
	\bibliography{reference_haptic}
	
	\begin{IEEEbiography}[
		{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figure/Liu.pdf}}
		]{Tangyou Liu} received the B.S. degree from the Southwest University of Science and Technology (SWUST) in 2017, Mianyang, China, and the M.S. degree as an outstanding graduate from the Harbin Institute of Technology, Shenzhen (HITSZ), Shenzhen, China, in 2021, supervised by Prof. Max Q.-H.Meng. Then, he worked in KUKA, China, in 2021, as a system development engineer and was awarded the champion of the KUKA China R\&D Innovation Challenge. Now, he is pursuing his Ph.D. degree at the University of New South Wales (UNSW), Sydney, Australia, under the supervision of Dr. Liao Wu. His current research interests include medical and surgical robotics and human-robot interaction for minimally invasive surgery.
	\end{IEEEbiography}
	
	\begin{IEEEbiography}[
		{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figure/Zhang.pdf}}
		]{Tinghua Zhang} received the B.S. and M.S. degrees in Mechanical Engineering from Jimei University (JMU) in 2020 and Harbin Institute of Technology (Shenzhen) (HITSZ) in 2023, respectively. He is working at The Chinese University of Hong Kong (CUHK) as a research assistant.
		His research interest is mainly in medical robotics, including robotic OCT, optical tracking and force sensing. He was awarded the best student paper finiallist of IEEE ROBIO 2022.
	\end{IEEEbiography}

	\begin{IEEEbiography}[
	{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figure/Jay.pdf}}
	]{Jay Katupitiya} received his B.S. degree in Production Engineering from the University of Peradeniya, Sri Lanka and Ph.D degree from the Katholieke Universiteit Leuven, Belgium. He is currently an associate professor at the University of New South Wales in Sydney, Australia. His main research interest is the development of sophisticated control methodologies for the precision guidance of field vehicles on rough terrain at high speeds.
\end{IEEEbiography}
	
	\begin{IEEEbiography}[
		{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figure/Wang.pdf}}
		]{Jiaole Wang} received the B.E. degree in mechanical engineering from Beijing Information Science and Technology University, Beijing, China, in 2007, the M.E. degree from the Department of Human and Artificial Intelligent Systems, University of Fukui, Fukui, Japan, in 2010, and the Ph.D. degree from the Department of Electronic Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong, in 2016. He was a Research Fellow with the Pediatric Cardiac Bioengineering Laboratory, Department of Cardiovascular Surgery, Boston Childrenâ€™s Hospital and Harvard Medical School, Boston, MA, USA. He is currently an Associate Professor with the School of Mechanical Engineering and Automation, Harbin Institute of Technology, Shenzhen, China. His main research interests include medical and surgical robotics, image-guided surgery, human-robot interaction, and magnetic tracking and actuation for biomedical applications.
	\end{IEEEbiography}

	\begin{IEEEbiography}[
	{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figure/Wu.pdf}}
	]{ Liao Wu} is a Senior Lecturer with the School of Mechanical and Manufacturing Engineering, University of New South Wales, Sydney, Australia. He received his B.S. and Ph.D. degrees in mechanical engineering from Tsinghua University, Beijing, China, in 2008 and 2013, respectively. From 2014 to 2015, he was a Research Fellow at the National University of Singapore. He then worked as a Vice-Chancellorâ€™s Research Fellow at the Queensland University of Technology, Brisbane, Australia from 2016 to 2018. Between 2016 and 2020, he was affiliated with the Australian Centre for Robotic Vision, an ARC Centre of Excellence. He has worked on applying Lie groups theory to robotics, kinematic modeling and calibration, etc. His current research focuses on medical robotics, including flexible robots and intelligent perception for minimally invasive surgery.
	\end{IEEEbiography}
	
\end{document}

