\section{Related Work}
\label{sec:bg_related_work}

%\subsection{\acrfull{gan}}




\subsection{Text-to-Text-Content-Image Synthesis}

Text-to-Text-Content-Image synthesis refers to the task of generating an image that depicts a specific text, whether it is on character-, word-, sentence-, or page-level, given that text as input condition.
%A field directly related to this task is Document Image Analysis and Recognition, notably one of the suffering domains considering the limitation of annotated data, compared to natural images~\cite{5206848},~\cite{Lin2014MicrosoftCC}.
A field directly related to this task is Document Image Analysis and Recognition, notably one of the resource-constrained domains with respect to the availability of annotated data, at least compared to the current state in natural image-related tasks ~\cite{5206848},~\cite{Lin2014MicrosoftCC}.

Most existing works focus on conditioning on a string prompt and a writer style to generate images of realistic handwritten text using GAN-based approaches.
GANwriting~\cite{kang2020ganwriting} creates realistic handwritten word images conditioned on text and writer style by guiding the generator.
The method manages to produce out-of-vocabulary words.
The authors extend this work in~\cite{kang2021content}, generating realistic handwritten text-lines.
SmartPatch~\cite{mattick2021smartpatch} fixes artifact issues that GANwriting faces, by deploying a patch discriminator loss.
ScrabbleGAN~\cite{Fogel2020ScrabbleGANSV} uses a semi-supervised method to generate long handwritten sentences of different style and content.
A different approach that uses Transformers instead of GANs is presented in~\cite{Bhunia_2021_ICCV}.
The method presents a typical Transformer Encoder-Decoder architecture that takes as input style features of handwritten sentence images extracted by a CNN encoder and a query text in decoding part.
The model is trained using a four-part loss function, containing an adversarial loss, a text recognition loss, a cycle loss that minimizes the error between the reconstructed image and the style features and a writer prediction loss.

Historical Document Analysis is known to be one of the limited resource areas due to the variation in scripts, eras, degradations and dataset size~\cite{Nikolaidou2022ASO}, making the use of generative models to augment datasets inevitable.
The work presented in~\cite{8893115} initially generates modern documents using \LaTeX\ and then attempts to convert them in a historical style with the use of Neural Style Transfer and CycleGAN.
As the generation of a whole page seems to be a quite challenging task, the results they present are more convincing on crops of images.
The work is further extended in~\cite{vogtlin2021generating}, by adding text recognition to the framework and the loss function, which gives better readable text in the image synthesis.
