\section{Evaluation and Results}
\label{sec:experiments}

We evaluate the quality of the generated word-images using our method in three aspects: visual quality, text quality, and style quality. 
To assess visual quality, we compute the commonly used Fréchet Inception Distance (FID) score and provide examples of in-vocabulary and out-of-vocabulary words. 
Additionally, we demonstrate the results of blending two distinct writer styles through interpolation.
To determine the effectiveness and text quality of our approach, we create a pseudo training set from the IAM database and conduct several experiments for handwriting text recognition (HTR). 
For comparison with other methods, we perform the same experiments using two GAN-based approaches, SmartPatch and GANwriting.
Finally, we evaluate style quality in two ways.
First, we train a standard Convolutional Neural Network (CNN) on the real IAM database for style classification and test it on the generated samples.
Second, we apply a writer retrieval method and compare its performances using real or synthetic data.
This enables us to measure the extent to which our method accurately captures the style of the original IAM database.

\subsection{Qualitative Results}

%Figure \ref{fig:iv_oov} presents generated samples using our method of In-Vocabulary (IV) and Out-of-Vocabulary (OOV) words and random styles picked from the IAM training set.
%We can observe a notable variety over the writing style, indicating the good behavior of the proposed method, even for the case of OOV words that were never met in the training phase.
%Furthermore, a comparative qualitative evaluation can be found in Figure \ref{tab:comparison_gen}, where both SmartPatch and GANwriting methods have been used to generate a set of word images. 
A comparative qualitative evaluation can be found in Figure \ref{tab:comparison_gen}, where both SmartPatch and GANwriting methods have been used to generate a set of word images. 
Specifically, the goal was to recreate the original images (further left column).
As we can see, all methods generate ``readable" words without notable artifacts/deformations. 
Nonetheless, SmartPatch has a smoother appearance compared to GANwriting, as it was designed to do, while the proposed Diffusion approach retains the original style to an outstanding degree. 
%Finally, in Figure~\ref{tab:multisample} we present different instances of sampling with our diffusion model, when a fixed word/style pair is used. As we can see, the proposed method generates different images for the same ``category'', providing variations close to the used style. 

Furthermore, to validate the variety of styles and the ability to generalize beyond already seen words, in Figure \ref{fig:iv_oov} we present generated samples using our method of In-Vocabulary (IV) and Out-of-Vocabulary (OOV) words and random styles picked from the IAM training set.
We can observe a notable variety over the writing style, indicating the good behavior of the proposed method, even for the case of OOV words that were never met in the training phase.

%Several examples of all cases can be found in Figure \ref{tab:comparison_gen}.

\input{tables/comparison.tex}

\input{tables/iv_oov_results.tex}

%\input{tables/multirun_word.tex}

Towards measuring the quality of the generation,
a metric commonly used to evaluate generative models is the Fréchet Inception Distance (FID) score~\cite{dowson1982frechet}.
The metric computes the distance between two dataset feature vectors extracted by an InceptionV3 network~\cite{szegedy2016rethinking} pre-trained on ImageNet~\cite{5206848}.
Our approach achieves an \textbf{FID} score of \textbf{22.74}, which is comparable to SmartPatch's score of \textbf{22.55}. 
GANwriting performs with an FID of \textbf{29.94}. While FID is a widely used metric for evaluating generative models, it may not be appropriate for tasks that do not involve natural images similar to those in ImageNet, on which the network was trained.
In fact, this domain shift between natural images and handwritten documents lessens the fidelity of the evaluation protocol, but adapting this metric, by fine-tuning the FID network on document images, is out of the scope of this work. Despite this, FID metric is still an indication of realistic images.


%add figure of the same style-text pair sampled multiple times
%\input{tables/multirun_word.tex}





\subsection{Latent Space Interpolation}
Following the paradigm of GANwriting~\cite{kang2020ganwriting}, we further interpolate between two writer styles $Y_A$ and $Y_B$ by a weight $\lambda_{AB}$ to create mixed styles.
Using a weighted average $Y_{AB} = (1-\lambda_{AB})Y_A + \lambda_{AB}Y_B$, we interpolate between $Y_A$ and $Y_B$ for a fixed text condition.
Figure~\ref{tab:interpol} shows the results on fixed words with interpolation between two writing styles with various $\lambda_{AB}$ values.
One can observe the smooth progression between styles as the mix parameter $\lambda_{AB}$ increases.
This interpolation concept could be a useful tool for generating words of unseen/unknown style, especially if the goal is to create an augmented dataset for training document analysis methods.

\input{tables/interpolation.tex}

\subsection{Handwriting Text Recognition (HTR)}
 We evaluate the generated data on the task of Handwriting Text Recognition and assess the usefulness of the data on a standard downstream task.
 We use the HTR system presented in~\cite{10.1007/978-3-031-06555-2_17}.
 %\todo{Write about the katapliktiko HTR system}.
 Specifically, the used HTR system is a hybrid CNN-LSTM network with a ResNet-like CNN backbone followed by a 3-layers bi-directional LSTM head, trained with Connectionist Temporal Classification (CTC) loss~\cite{graves2008novel}. 
 We followed the modifications proposed in \cite{10.1007/978-3-031-06555-2_17} and used a column-wise max-pooling operation between the CNN backbone and the recurrent head, as well as a CTC shortcut of a shallow 1D CNN head. 
 This shortcut module, as described in the initial work, is discarded during testing and is used only for assisting the training procedure.
 Input word images have a fixed size of $64 \times 256$ by performing a padding operation (or resize them if the exceed this pre-defined size).
 
For comparison with the related work on word-image generation, we further evaluate GANwriting and SmartPatch on the HTR task with the same model.

Using the generated images to train an HTR system and then evaluate the trained system on the original test set of real images aims to a multifaceted insight on the quality of the data; Achieving good results in the test set translates to ``readable" words (at least in their majority), so that the system can understand the existing characters during training with CTC, as well as to a variability in writing styles, so that the training system could generalize well in the test set of unseen writing styles. The ideal generative model should abide to both these properties and thus can be used to train a well-performing HTR system.
%\textcolor{red}{To better understand how }

Following the protocol of~\cite{mattick2021smartpatch}, for this recognition task, we discarded, both from the training and the test set, words containing non-alphanumeric characters, as well as words with more than 10 characters, since the generative models have been trained considering the same setup.
We used the generative models to recreate the train set, both in text and in style. 
The results of this experiment are reported in Table~\ref{HTR_results}, where we present the character error and word error rates (CER/WER) for the initial IAM train set, the recreated sets of the generative models (i.e., GANwriting, SmartPatch and our proposed Diffusion approach), as well as the combination of the original set with each one of the recreated (i.e., with $\times2$ training images, compared to the initial set).
The reported results correspond to the mean value and the standard deviation over 3 different training/evaluation runs for each setup.
The following observations can be made:
\begin{itemize}
    \item The generated synthetic datasets under-perform with respect to the original IAM dataset. However, both GANwriting and SmartPatch approaches lead to a notable decrease in performance, indicating lack of writing style variability. 
    On the other hand, the proposed method achieves considerably low error rates, but not on par with the real data.
    \item Combining the synthetic datasets with the real IAM train set, the performance is improved compared to training only on the original IAM set, with the exception of GANwriting and the CER metric, which is practically on par with the baseline model.
    \item SmartPatch, despite visually improving the results of GANwriting, does little to improve the HTR performance.
    \item The synthetic set, generated by our proposed method, along with the real set, considerably outperforms all other settings and is statistically significant with a p value of 0.035. %This boost in performance raises the question of how this method compares against to other state-of-the-art recognition results, which  
\end{itemize}
%To better distinguish the ability of the generated images to assist training, we trained different HTR models fo



\begin{table}
\caption{HTR results, reporting the Character Error Rate (CER) and Word Error Rate (WER). For both metrics, the lower the better.}\label{HTR_results}
\begin{adjustbox}{scale=1.15,center}
\centering
\renewcommand{\arraystretch}{1.3}
\addtolength{\tabcolsep}{2.5pt}  
%\begin{tabular}{lccc}
\begin{tabularx}{0.82\textwidth}{lcc}

 %&  \multicolumn{2}{c}{IAM} &\multicolumn{2}{c}{NEXT}& \multicolumn{2}{c}{NEXT}\\
\hline
\textbf{Training Data} &  CER (\%) $\downarrow$ & WER (\%) $\downarrow$ \\
\hline

Real IAM & \phantom{3}$4.86\pm0.07$& $14.11\pm0.12$ \\

GANwriting IAM  &  $38.74\pm0.57$&  $68.47\pm0.32$ \\

SmartPatch IAM  &  $36.63\pm0.71$ & $65.25\pm1.02$ \\

WordStylist IAM (Ours) & \phantom{3}$8.80\pm0.12$  & $21.93\pm0.17$\\

Real IAM + GANwriting IAM   & \phantom{3}$4.87\pm0.09$ & $13.88\pm0.10$ \\

Real IAM + SmartPatch IAM  & \phantom{3}$4.83\pm0.08$ & $13.90\pm0.22$ \\

Real IAM + WordStylist IAM (Ours)  & \phantom{3}$\textbf{4.67}\pm\textbf{0.08}$ & $\textbf{13.28}\pm\textbf{0.20}$ \\



\hline
%\end{tabular}
\end{tabularx}
\end{adjustbox}
\end{table}


\subsection{Handwriting Style Evaluation}
\label{ssct:style-evaluation}

Qualitative results show that out proposed method is able to nicely capture the style of each writer present in the IAM database.
In order to quantify this property, we employ an implicit evaluation via writer identification.

The most straightforward way to address this is via a writer classification formulation. Specifically, to evaluate the generated styles, we finetuned a ResNet18 CNN~\cite{he2016deep}, pre-trained on ImageNet, on the IAM database for the task of writer classification.
Then, we use the generated datasets from the three generative methods as test sets and present the obtained accuracy in Table \ref{style_accuracy}.
The network manages to successfully classify most of the generated samples from our proposed method with an accuracy of 70.67\%, while it fails to recognize classes on samples from the other two methods.
This result comes as no surprise since the proposed method learns explicitly the existing styles, while both the GAN-based approaches adapt the style based on a few-shot scheme. 
Furthermore, we use the features extracted by the model to plot t-SNE embeddings on the different datasets in Figure \ref{tsne}.
In more detail, we used the 512-dimensional feature vector extracted by the second-to-last layer, trying to simulate a style-based representation space. 
Again, the resulted projection of the data generated by our Diffusion approach appears to be much closer to the real data.
On the contrary, the GAN-based methods create ``noisy`` visualization with no distinct style neighborhoods.
In fact, even the proposed method seems to have a similar noisy behavior (in the center of the plot) but to a much lesser extent. 
This phenomenon is in line the the HTR results, where the diffusion method provided results much closer to the real IAM, but not on par.




\begin{table}[t]
\caption{Classification accuracy of a ResNet18 trained for writer identification on real data.}\label{style_accuracy}
%\begin{adjustbox}{scale=1.2,center}
\centering
\renewcommand{\arraystretch}{1.3}
\addtolength{\tabcolsep}{1.5pt}  
%\begin{tabular}{lccc}
\begin{tabularx}{0.44\textwidth}{lc}
 %&  \multicolumn{2}{c}{IAM} &\multicolumn{2}{c}{NEXT}& \multicolumn{2}{c}{NEXT}\\
\toprule
\textbf{Test Set} & Accuracy (\%) \textuparrow \\
\midrule
GANwriting &  \phantom{7}4.81\\
SmartPatch & \phantom{7}4.09 \\
WordStylist (Ours) & \textbf{70.67} \\
\bottomrule
%\end{tabular}
\end{tabularx}
%\end{adjustbox}
\end{table}

\begin{figure}[tb]
\centering
\subfloat[Real IAM.]{\label{4figs-a} \includegraphics[width=0.4\textwidth]{images/tsne/tsne_real.png}}% The "%" masks the line break.
\hfill
\subfloat[WordStylist IAM (ours).]{\label{4figs-b} \includegraphics[width=0.4\textwidth]{images/tsne/tsne_diffusion.png}}%
\hfill
\subfloat[SmartPatch IAM.]{\label{4figs-c} \includegraphics[width=0.4\textwidth]{images/tsne/tsne_smartpatch.png}}%
\hfill
\subfloat[GANwriting IAM.]{\label{4figs-d} \includegraphics[width=0.4\textwidth]{images/tsne/tsne_ganwriting.png}}%
\caption{T-SNE projections of the feature vector produced by the ResNet18 trained for writer identification, as described in Section~\ref{ssct:style-evaluation}.}
\label{tsne}
\end{figure}


%\subsection{Writing Retrieval}
As an alternative to the straightforward implementation of writer classification, we also use a classic writer retrieval pipeline consisting of local feature extraction and computing a global feature representation~\cite{Christlein17PR,Christlein18DAS,Christlein15ICDAR}.
While the local descriptors can also be trained in a self-supervised~\cite{Christlein17ICDAR}, we just use SIFT~\cite{Lowe04} descriptors extracted on SIFT keypoints. 
The descriptors are normalized using Hellinger normalization~\cite{Arandjelovic12} (a.\,k.\,a.\ as RootSIFT) and are subsequently jointly whitened and dimensionality-reduced using PCA~\cite{Christlein17PR}. 
% I guess this is too detailed? 
%The global feature representation is computed using VLAD in combination with generalized max-pooling. 
%To further increase the performance, we use multi-VLAD encoding~\cite{todo}, \ie multiple VLAD representations are concatenated and once more jointly whitened and dimensionality-reduced. 
The global feature representation is computed using multi-VLAD~\cite{Christlein15ICDAR}, where the individual VLAD representations use generalized max-pooling~\cite{Christlein18DAS}. 

This pipeline needs paragraphs as input in order to gather a sufficient amount of information.
To produce synthetic text paragraphs, we paste randomly-selected synthetic words on a blank background, following a similar structure as the printed text of IAM: same number of lines, similar number of characters per line.
Thus, no information from the handwritten text is used.
Line spacing is constant, and a small randomness is added to word spacing.

We use a leave-one-image-out cross-validation, i.\,e., each sample is used as query and the results are averaged. 
As metrics, we give the top-1 accuracy and mean average precision (mAP).
For our experiment, we use two paragraphs of 157 writers (IAM + IAM). 
In subsequent experiments, we replace the second paragraph by the synthesizers (GANWriting, SmartPatch, WordStylist). 
In this way, the query sample is either an original sample and the closest match should be the synthetic one or vice-versa.

The results, given in Table~\ref{tab:writer_retrieval}, show little difference between real data (IAM + IAM) and data produced by our method (IAM + WordStylist).
Thus, our method produces persistent writing styles that are nearly indistinguishable for the writer retrieval pipeline.
It is able to imitate handwriting much better than GANwriting and SmartPatch, which both achieve significantly lower scores in this experiment.

% TODO: possible put next to the previous table
\begin{table}[t]
    \centering
    \caption{Writer retrieval results using a 157 writers subset of IAM.}
    \label{tab:writer_retrieval}
    \begin{tabular}{lc@{\hspace{0.005cm}}c}
    \toprule
                    & Top-1 [\%] \textuparrow & mAP [\%] \textuparrow\\
                                \midrule
         IAM + IAM              & 97.45 & 97.61\\
         \midrule
         IAM + GANwriting       & \phantom{9}3.18  & \phantom{9}7.23\\
         IAM + SmartPatch       & \phantom{9}3.18  & \phantom{9}7.72\\
         IAM + WordStylist (Ours) & \textbf{97.13} & \textbf{97.84}\\
         \bottomrule
    \end{tabular}
\end{table}


%\subsubsection{Qualitative Results on IAM and Other Data.}


% The visual abstract
% \begin{figure}[t] % Always use [t] or [!t]
%  \includegraphics[width=\textwidth, scale=0.1]{images/interpolation2.pdf}
%     \centering
%     \caption{Interpolation between classes.}
%     \label{fig:interpolation}
% \end{figure}


%\section{Discussion}


%%We'll see whether this will be included
% \subsection{Ablation Study}

% We perform several ablation studies on the IAM offline word dataset to demonstrate how each part of the framework affects the generation.