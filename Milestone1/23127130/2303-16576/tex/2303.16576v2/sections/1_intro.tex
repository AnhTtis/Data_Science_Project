\section{Introduction}
\label{sec:introduction}

%Image synthesis is a fundamental problem of Computer Vision, mainly because of the urgent need for annotated data to train Deep Learning methods.
Image synthesis is a very challenging problem in Computer Vision, which has gained traction with the rekindling of interest in neural networks a decade prior, and especially the introduction of models and concepts such as Generative Adversarial Networks (GANs)~\cite{NIPS2014_5ca3e9b1}, Variational Autoencoders (VAEs)~\cite{kingma2013auto} or Normalizing Flows (NFs) \cite{kingma2018glow}.
Apart from the utility of the generated image in itself, image synthesis has been employed as a tool to artificially augment training sets.
This is an aspect that is critical when it comes to training Deep Learning models, which are notorious for typically requiring vast amounts of data to attain optimal performance.
%mainly because of the urgent need for annotated data to train Deep Learning methods.
Annotating data is an expensive and time-consuming task that requires a lot of human effort and expertise.
A particular variant of image synthesis is text-to-image synthesis, where 
the task is to generate an image given a text description. 
As stated in~\cite{frolov2021adversarial}, a text description can indeed give more semantic and spatial information about the objects depicted in an image than a single label.
Text-to-image synthesis has been established as a whole independent field as several applications have gained relative prominence. 
\newline
\indent
Conditional Generative Adversarial Networks (cGANs) ~\cite{mirza2014conditional}, the conditional variant of GANs,  have further enabled the augmentation of existing datasets by generating data given a specific class or a specific input.
With the advent of these models, adversarial training has been established as the standard for image generation, where a minimax game is ``played'' between two networks, aptly named Generator and Discriminator.
The Generator is tasked with creating a sample -- in the current context, the synthesized image -- while the Discriminator is tasked with detecting instances that are outliers with respect to the training data.
Unlike GANs, which do not explicitly define a data density, other state-of-the-art approaches have attempted to approach data generation as sampling from a probability density function (pdf).
Variational Autoencoders cast the problem as one of estimating a latent representation for members of a given dataset, given the prior knowledge that latent embeddings are Gaussian-distributed.
They are comprised of two network parts, named the Encoder and the Decoder.
The Encoder produces (probabilistic) latent representations given a datum,
while the Decoder is tasked with the inverse task, that is producing a sample given a latent representation.
Normalizing flows also deal with estimating the pdf of a given set, and also assume the existence of a latent space that is to be estimated, like VAEs.
Latent data are equidimensional to the image data, and training is performed by learning a series of non-linear mappings that gradually convert the data distribution from and to a Gaussian distribution.
In VAEs as in NFs, once the model is trained, image generation can simply be performed by sampling from the latent space and applying the learned transformation back to the image / original space.
%Thus, there is a variety of architectures to choose that fits a specific type of data and problem.
%This performance is achieved mainly by using features from pre-trained models using millions of image-caption pairs.
%The outburst of Denoising Diffusion Probabilistic Models and their incredible performance, as seen in systems such as DALL$\cdot$E-2~\cite{Ramesh2022HierarchicalTI} and Imagen~\cite{Saharia2022PhotorealisticTD}, has tempted to experiment their use in other applications as well.
The outburst of Diffusion Models, and in particular more recent variants such as Denoising Diffusion Probabilistic Models (DDPMs) or Latent Diffusion Models (LDM) have quickly begun to change the picture of the state of the art
with achievements that can often be described to be no less than astonishing.
The results of systems
%incredible performance, as seen in systems 
such as DALL$\cdot$E-2~\cite{Ramesh2022HierarchicalTI} and Imagen~\cite{Saharia2022PhotorealisticTD} have %tempted 
prompted many researchers to experiment with their use in different applications. %other applications as well.
Diffusion models \cite{SohlDickstein2015DeepUL} are based on a probabilistic framework like VAEs or NFs, but propose a different approach to the problem of image synthesis, cast in its standard form as density estimation followed by sampling.
Like NFs, in their standard form the latent space dimensionality is defined to be equal to that of the original space, and learning is performed by estimating a series of non-linear transformations between latent space and original space.
A ``forward/diffusion'' process gradually adds noise to inputs according to a predetermined schedule;
with the ``reverse'' process the aim is to produce an estimate of an image given a latent, noisy sample.

In this work, instead of using text only as a description of the image contents, 
%we also use it as a literal content word to depict in the image, thus we address the task as Text-
we also use it literally as image content, in the sense of generating handwriting.
Thus, we address a task of Text-to-Text-Content-Image Synthesis.
The main contributions of this work are the following:
\begin{enumerate}
    \item We present a method based on a conditional Latent Diffusion Model, that takes as input a word string and a style class and generates a synthetic image containing that word.
    \item We compare qualitative results of our method with other GAN-based generative model approaches.   
    \item We further evaluate our results by presenting qualitative and quantitative results for text recognition using the synthetic data. The synthetic data is used for data augmentation, resulting in boosting the performance of a state-of-the-art Handwriting Text Recognition (HTR) system.
    \item And finally, we compare synthetic data and real handwritten paragraphs using a writer retrieval system. We show that data produced by our method show no significant difference in style to real data, and outperforms the other methods by a tremendous margin.
\end{enumerate}

%% Structure of the paper
The paper is organized as follows.
In Section \ref{sec:bg_related_work}, we present an overview of the related work. %briefly review the theoretical background for the used methods and
Our proposed method is introduced in Section \ref{sec:method}, while Section \ref{sec:experiments} includes the evaluation process and results.
Section \ref{sec:limitations} presents limitations and possible future directions.
Finally, we discuss conclusions in Section \ref{sec:conclusion}. 
