\section{Method}
\label{sec:method}

In this section, we present some general background information for the standard Diffusion and Latent Diffusion Models. 
We then illustrate in detail the proposed method that includes the forward process, model components, sampling and experimental setup for training and sampling from the model.

\subsection{Diffusion Models Background}

\subsubsection{\acrfull{ddpm}.} 
%paragraph checked
Diffusion Models are a type of generative model that employ Markov chains to add noise and disrupt the structure of data. 
The models then learn to reverse this process and reconstruct the data. Inspired by Thermodynamics~\cite{SohlDickstein2015DeepUL}, Diffusion Models have gained popularity in the field of image synthesis due to their ability to generate high quality samples.

The Diffusion Model consists of two phases: the forward (diffusion) process and the reverse (denoising) process.
In the forward process, a sample $x_0$ is initially drawn from a 
%real 
distribution $x_0\sim q(x_0)$ corresponding to the observed data.
This is subjected to Gaussian noise, which produces a latent variable $x_1$;
noise is again added to $x_1$, giving latent variable $x_2$, and so on, until some predefined hyper-parameter $T$.
This process forms a series of latent variables $x_1, x_2, \cdot, x_T$,
%At each timestep $t$ to produce latent variables $x_1,...,x_T$. 
Formally, we can write:
%The real distribution $q(x_0)$ is transformed into a tractable form using a Gaussian perturbation defined as: 
\begin{equation}
    q(x_{1:T}|x_0) = \prod_{t=1}^{T} q(x_t|x_{t-1}),\quad
    q(x_t|x_{t-1}) = N(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_tI),
\end{equation}
where we have $\beta_i \in [0, 1], \forall i \in [1,T]$.
Hyper-parameters $\beta_1,\beta_2,...,\beta_T$ collectively form 
a noise variance schedule, used to control the amount of noise added at each timestep.
In the final timestep, given large enough $T$ and suitable noise schedule, we will have $q(x_T|x_0) = q(x_T) \approx N(0, I)$, i.e. 
the end result becomes practically a pure Gaussian noise sample with no structure.
In the reverse (denoising) phase, a neural network learns to gradually remove the noise from the sampled by a stationary distribution until ending up with actual data.
Hence, image synthesis will be performed according to an ancestral sampling scheme.
This means that first we need to sample from $q(x_T)$, then we sample by the previous time-step conditioned on the sampled value of $x_T$, and so and so forth until we sample the required $x_0$.

The noise is gradually removed in reverse timesteps using the following transition:
\begin{equation}
\resizebox{.92\hsize}{!}{$
p_{\theta}(x_{0:T}) = p(x_T)\prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t),\quad p_{\theta}(x_{t-1}|x_t) = N(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) .$}
%\label{eq:backward_ddpm}
\end{equation}
The network is trained by optimizing the variational lower bound between the forward process posterior and the joint distribution of the reverse process $p_\theta$.
The training loss 
%This can be expressed as a Mean Squared Error (MSE) loss, such as $||\epsilon - \epsilon_\theta(x_t, t)||_2^2$.
\begin{equation}
%\resizebox{.92\hsize}{!}{$
L = \mathbb{E}_{x_0,t,\epsilon}[||\epsilon - \epsilon_\theta(x_t, t)||^2]
\end{equation}
is calculated as the reconstruction error between the actual noise, $\epsilon$, and the estimated noise, $\epsilon_\theta$.
In the case of Latent Diffusion models the loss will be adapted to the latent representation $z_t$.

\subsubsection{Latent Diffusion Models (LDM).} Diffusion Models have demonstrated remarkable performance in image generation and transformation tasks~\cite{ho2020denoising,kingma2021variational,kong2020diffwave,mittal2021symbolic}.
However, their computational cost is high due to the size of the input data and the use of cross-attention in images.
To address this issue, Latent Diffusion Models were introduced in~\cite{rombach2022high} to model the data distribution in a lower-dimensional latent representation space.
This is accomplished by mapping the input images to a latent representation using an encoder, and then decoding the sampled latents back into an image using a decoder, both from a variational autoencoder architecture. 



\subsection{Proposed Approach}

The goal of this work is to generate synthetic word-image samples given a word string and a style class as conditions from a known distribution.
We approach this problem with the use of latent diffusion models to minimize training time and computational cost.
To move to the latent space we use the pre-trained ``stable-diffusion" VAE implementation from the Hugging Face repository\footnote{\url{https://huggingface.co/CompVis/stable-diffusion}}.
Figure \ref{fig:model} presents the overall architecture of the proposed method.



% The visual abstract
\begin{figure}[t] % Always use [t] or [!t]
 \includegraphics[width=0.98\textwidth, scale=0.1]{images/model3.pdf}
    \centering
    \caption{\textbf{The overall architecture.} During training, an input image is fed to the encoder $V_E$ to create a latent representation $z$, then noise is added to the latent. The noisy latent $z_t$ is then fed to the U-Net noise predictor along with a style class index for the writer style and an encoded word as the content. The UNet predicts the noise of the noisy latent $z_{t-1}$ where $t-1$ is the corresponding timestep. During sampling, a random noise latent $z_T$ is given to predict its noise. Then the model uses the two noise predictions to reconstruct the latent of the image $z_0$ that is finally decoded by the decoder $V_D$ that creates the synthetic image.}
    \label{fig:model}
\end{figure}


\subsubsection{Forward Process and Training.}
For the forward process, the VAE encoder $V_E$ initially transforms an input image to a latent representation $z$.
A diffusion model $p_\theta(x|Y,c_\tau)$ is learned on the style $Y$ and text-condition $c_\tau$ pairs.
Timesteps $t$ are sampled from a uniform distribution and the latent representation $z$ gets gradually corrupted by the diffusion process in every timestep.
For the noise prediction, we use a U-Net architecture~\cite{ronneberger2015u} with Residual Blocks~\cite{he2016deep} and intermediate Transformer Blocks~\cite{vaswani2017attention} to add the text condition to the model, as typically used by Ho et al.~\cite{ho2020denoising}.
The network takes as input the noisy image latents, the corresponding timestep, and the desired conditions $Y$ and $\tau$.
Timesteps are encoded using a sinusoidal position embedding, similar to~\cite{vaswani2017attention} to inform the model about each particular timestep that is operating.
The training objective is to minimize the reconstruction error between the network's noise prediction and the noise present in the image.
For the diffusion process, a noise scheduler increases the amount of noise linearly from $\beta_1=10^{-4}$ to $\beta_T=0.02$ for $T = 1000$ timesteps.
%While most works use multiple ResNet blocks within the U-Net parts, our problem uses much less data than usual cases and the network becomes too complex and collapses.
While most works use multiple ResNet blocks within the U-Net components, in the context of the current problem we need to take into account that we must work with scarce data compared to other use-cases;
larger models correspond to larger parameter spaces, which are exponentially harder to explore.
Hence, we use $1$ ResNet block in every module of the U-Net.
To further reduce the parameters and complexity of the network we use an inner model dimension of $320$ and $4$ heads in the Multi-Headed Attention layers within the U-Net.


\subsubsection{Sampling.}
We generate synthetic samples by deploying the reverse denoising process learned from the model.
To this end, the noise of a random noisy sample $z_T$ is predicted by the learned network $p_\theta$ and gradually removed in every timestep of the reversed process starting from $T$ to $t=0$.
One of the main challenges associated with DDPM is the time required for sampling.
Our experiments indicate that reducing the number of time steps from 1,000 to 600 does not compromise the quality of the generated samples. 
The final image is obtained in pixel space by decoding the denoised latent variable using decoder $V_D$.
We demonstrate how the reduction of timesteps affects the quality of the generated sample in Figure \ref{tab:timesteps}.
The figure shows that below 500 timesteps the quality of the images is really affected, thus to make sure the generated samples are not affected dramatically we proceed with a value of 600.

\begin{figure}[ht]
\begin{center}

\centering
\renewcommand{\arraystretch}{1.5}
\addtolength{\tabcolsep}{2.5pt} 
%\begin{adjustbox}{scale=0.53,center}
\begin{tabular}{cccccccccc}

%Real IAM & Diffusion (ours) & SmartPatch & GANwriting\\

$T=100$ & $T=200$ &$T=300$ &$T=400$ &$T=500$ \\
 %1st row Labour
\includegraphics[scale=0.25]{images/time steps/what_254_100.png} & \includegraphics[scale=0.25]{images/time steps/what_254_200.png} & \includegraphics[scale=0.25]{images/time steps/what_254_300.png} & \includegraphics[scale=0.25]{images/time steps/what_254_400.png} &
\includegraphics[scale=0.25]{images/time steps/what_254_500.png}
\\
$T=600$ &$T=700$ &$T=800$ &$T=900$ &$T=1000$ \\
\includegraphics[scale=0.25]{images/time steps/what_254_700.png}
&
\includegraphics[scale=0.25]{images/time steps/what_254_800.png}
&
\includegraphics[scale=0.25]{images/time steps/what_254_900.png}
&
\includegraphics[scale=0.25]{images/time steps/what_254_1000.png}&
\includegraphics[scale=0.25]{images/time steps/what_254_600.png}\\


\end{tabular}
%\end{adjustbox}
\caption{Sampling outputs using various timesteps values in the reverse denoising process.}
\label{tab:timesteps}
\end{center}
\end{figure}

\subsubsection{Style and Text Conditions.}
The input style condition $Y$ is processed with an embedding layer and then added to the timestep embedding.
For the text condition, a content encoder $C_E$ is used to transform an input string $\tau$ into a meaningful context representation $c_\tau = C_E(\tau)$ for the model.
Initially, the string is tokenized using a unique index for each letter and then passed through an embedding layer to transform it to an appropriate embedding dimension according to the vocabulary size which is the number of characters present in the training set.
Then, positional encodings similar to~\cite{vaswani2017attention} are used to inform the model about the character position in the sequence with the use of sine and cosine functions as $PE_{pos, i} = \sin(pos/1000^{i/emb\_dim})$ and $PE_{pos, i+1} = \sin(pos/1000^{i/emb\_dim})$, where $pos$ is the position of each letter in the sequence and $emb\_dim$ is $320$ as mentioned previously. 
Finally, to create the text input condition $c_\tau$ a dot-product attention layer is used, defined as $Att(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$, to create a weighted sum of the character representations.
To support the choice of the positional text encoding we present a few samples as ablation with and without the positional encoding and self-attention layers in Figure \ref{fig:encoder}.

\subsection{Experimental Setup}

We conducted extensive experiments using the IAM offline handwriting database on word-level~\cite{marti2002iam}.
Similar to \cite{mattick2021smartpatch} and \cite{kang2020ganwriting}, we used the Aachen split train set and included words of 2-7 characters to train the diffusion model.
Thus, during training the model sees 339 writer styles and approximately 45K words.
For consistency, all images were resized to a fixed height of $64$ pixels, retaining their aspect ratio. To handle variations in width, images of width smaller than $256$ pixels were center-padded, while larger ones were resized to the maximum width. Since the maximum number of characters is $7$, this resizing did not cause significant distortions in the images. Moreover, these images were intended for training other models, which could eventually lead to resizing or modifications of the original images.
AdamW~\cite{loshchilov2017decoupled} is used as the optimizer during training with a learning rate of $10^{-4}$.
To better understand the nature of the model, no augmentation is used on the images during training.
Each model was trained for 1K epochs with a batch size of 224 on a single A100 SXM GPU.

%The transformer uses ... .
%The embedding size is set to 320.

\input{tables/encoder.tex}

