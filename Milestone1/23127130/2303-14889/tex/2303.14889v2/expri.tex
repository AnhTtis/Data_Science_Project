\section{Experiments}

\subsection{Experimental Setup}
\vspace{-5pt}
\myparagraph{Benchmarks.}
We evaluate \model{} on two RL environments:
\begin{itemize}[leftmargin=*]
\vspace{-3pt}
    \item
    \textbf{CARLA} \citep{DBLP:conf/corl/DosovitskiyRCLK17}: CARLA is a simulator with complex and realistic visual observations for autonomous driving research. We train our model to perform the task of first-person highway driving in ``Town04'', where the agent’s goal is to drive as far as possible in $1{,}000$ time steps without colliding with any of the $30$ other moving vehicles or barriers. In addition to our conference paper, we incorporate more diverse settings into our study, including both day and night modes as shown in \fig{fig:day_night_vis}.
    \item
    \textbf{DeepMind Control Suite} \citep{tassa2018deepmind}: 
    DMC contains a set of continuous control tasks and serves as a standard benchmark for vision-based RL. To evaluate the generalization of our method by disentangling different components under complex visual dynamics, we use two modified benchmarks \citep{hansen2021softda}, namely \texttt{video\_easy}, which contains $10$ simple videos, and \texttt{video\_hard}, which contains $100$ complex videos. 
    %
\end{itemize}



\vspace{-5pt}
\myparagraph{Compared methods.}
We compare \model{} with the following visual RL approaches:
\begin{itemize}[leftmargin=*]
\vspace{-3pt}
    \item \textbf{DreamerV2 \citep{hafner2020mastering}}: A model-based RL method that learns directly from latent variables in world models. The latent representation allows agents to imagine thousands of trajectories in parallel.
    \item \textbf{DreamerV3 \citep{hafner2023mastering}}:
    A further improved version of Dreamer that learns to master diverse domains with fixed hyperparameters.
    \item \textbf{DreamerPro \citep{deng2022dreamerpro}}: A non-contrastive, reconstruction-free model-based RL method that combines Dreamer \citep{hafner2019dream} with prototypes to enhance robustness to distractions.
    \item \textbf{CURL \citep{srinivas2020curl}}: A model-free RL method that uses contrastive learning to extract high-level features from raw pixels, maximizing agreement between augmented data of the same observation.
    \item \textbf{SVEA \citep{hansen2021stabilizing}}: A framework for data augmentation in deep Q-learning algorithms that improves stability and generalization on off-policy RL.
    \item \textbf{SAC \citep{haarnoja2018soft}}: A model-free actor-critic method that optimizes a stochastic policy in an off-policy way.
    \item \textbf{DBC \citep{zhang2021learning}}: A method that learns a bisimulation metric representation without reconstruction loss. This representation is invariant to different task-irrelevant details in the observation. 
    \item \textbf{Denoised-MDP \citep{wang2022denoised}}: A framework that categorizes information out in the wild into four types based on controllability and relation with reward, and formulates useful information as that which is both controllable and reward-relevant.
\end{itemize}



\begin{figure}[t]
\vspace{-5pt}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/day_night_vis.pdf}
    \vspace{-3pt}
    \caption{Examples of day and night modes in CARLA.}
    \label{fig:day_night_vis}
    \vspace{-12pt}
\end{figure}




\begin{figure*}[t] 
    \centering
	  \subfloat[]{
       \includegraphics[width=3.5in]{figures/compare_sota_carla.pdf}}
    \label{fig:compare_sota_carla}
    \hfil
	  \subfloat[]{
        \includegraphics[width=3.5in]{figures/ablation_big.pdf}}
    \label{fig:ablation_carla_nips}
    \vspace{-5pt}
	  \caption{(a) Quantitative comparison with existing approaches for CARLA driving. (b) Ablation studies of individual effectiveness of inverse dynamics optimization (\textcolor{MyDarkGreen}{green}), noncontrollable rollouts (\textcolor{red}{red}), future-state attention (\textcolor{MyPurple}{purple}), and the separate branch for static information modeling (\textcolor{brown}{brown}). We also compare \model{} with its predecessor in our conference paper (\textcolor{orange}{orange}).}
	  \label{fig:compare_carla}
   \vspace{-5pt}
\end{figure*}




\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/sparse_ablation_vis.pdf}
    \vspace{-18pt}
    \caption{Examples of \model{} without (\textbf{top}) and with (\textbf{bottom}) sparse dependency. The agent without sparse dependency tends to follow the vehicle in front of it when there are many vehicles in the way. In the bottom row, the agent overtakes and accelerates flexibly.}
    \label{fig:sparse_ablation_vis}
    \vspace{-5pt}
\end{figure*}




\subsection{CARLA Autonomous Driving Environment}


\vspace{-5pt}
\myparagraph{Implementation.}
In the autonomous driving task, We use a camera with a $60$ degree view on the roof of the ego-vehicle, which obtains images of $64 \times 64$ pixels. Following the setting in the DBC \citep{zhang2021learning}, in order to encourage highway progression and penalize collisions, the reward is formulated as $r_t=v^T_{ego}\hat{u}_h \cdot \Delta t -\xi_1 \cdot \mathbbm{1} - \xi_2 \cdot |steer| $, where $v_{ego}$ is the velocity vector of the ego-vehicle, projected onto the highway’s unit vector $\hat{u}_h$, and multiplied by time discretization $\Delta t = 0.05$ to measure highway progression in meters. We use $\mathbbm{1} \in \mathbb{R^+}$ for collisions and a steering penalty $steer \in [-1,1]$ to facilitate lane-keeping. The hyper-parameters are set to $\xi_1=10^{-4}$ and $\xi_2=1$, respectively. We use $\beta_1 = \beta_2 = 1$ and $\alpha = 1$ in Eq. \eqref{eq:loss}, $\lambda_1 = \lambda_2 = 1$ in \eqn{eq:all_loss}, and $\tau=5$ in Eq. \eqref{eq:attention}.


% \subsubsection{Quantitative Comparisons}
\myparagraph{Quantitative comparisons.}
We present the quantitative results in CARLA in \fig{fig:compare_carla}(a). 
%
\model{} outperforms the compared models, including DreamerV2, DreamerV3, DreamerPro, and Denoised-MDP, significantly. 
%
After $500k$ environment steps, \model{} achieves an average return of around $60$, while DreamerV2 and Denoised-MDP achieve $10$ and $25$ respectively. 
%
In DreamerV2, the latent representations contain both controllable and noncontrollable dynamics, which increases the complexity of modeling the state transitions in imagination. 
% 
Compared with Denoised-MDP, which also decouples information according to controllability, \model{} has the advantage of making forward-looking decisions by rolling out future noncontrollable states.
%The empirical success of our approach demonstrates that (i) isolating controllable and noncontrollable states facilitates representation learning and (ii) forecasting future noncontrollable dynamics improves behavior learning.



% \subsubsection{Ablation Studies}
\myparagraph{Ablation studies.}
\fig{fig:compare_carla}(b) provides the ablation study results that validate the effectiveness of inverse dynamics, the rolling-out strategy of noncontrollable states, the attention mechanism, and the modeling of static information.
%
As shown by the green curve, removing the Inverse Cell reduces the performance of \model{}, which indicates the importance of isolating controllable and noncontrollable components.
%
For the model indicated by the red curve, we do not use the rollouts of future noncontrollable states as the inputs of the action model. The results demonstrate that rolling out noncontrollable states in the action-free branch significantly improves the agent's decision-making results by perceiving potential risks in advance.
%
Moreover, we evaluate \model{} without attention mechanism where the action model directly concatenates the current controllable state with a sequence of future noncontrollable states and takes them as inputs. As shown by the purple curve, the attention mechanism extracts valuable information from future noncontrollable dynamics better than concatenation.
% 
Furthermore, as shown by the brown curve, our approach’s performance decreases by about $15\%$ without a separate network branch for capturing the static information.
% \myparagraph{Effectiveness of sparse dependency modeling.}
%
Moreover, a comparison between the blue curve and the orange curve reveals a decline in our model's performance when we remove the min-max variance constraints and sparse dependency modeling. 
%
Unlike the DMC suite, where the original Iso-Dream is more vulnerable to training collapse, in the CARLA environment, the sparse dependency modeling method plays a crucial role in the improved performance of \model{}.
%
In \fig{fig:sparse_ablation_vis}, we present visual examples produced by our models with and without the sparse dependency.
%
Without sparse dependency (top row), the agent fails to predict that other vehicles will slow down or brake when changing lanes, making it safer to follow the vehicle ahead rather than overtake it during traffic congestion.
%
However, as shown in the bottom row of \fig{fig:sparse_ablation_vis}, the agent can decide whether to overtake or not based on its surroundings. 
%
These results indicate that sparse dependency greatly models the situation that the noncontrollable dynamics are affected by the controllable dynamics, which is conducive to the downstream decision-making task by accurately predicting the noncontrollable dynamics at future moments. 






\begin{figure*}[t]
\begin{center}
\vspace{-5pt}
\centerline{\includegraphics[width=0.99\linewidth]{figures/carla_showcases.pdf}}
\vspace{-10pt}
\caption{Video prediction results in CARLA. For each sequence, we use the first $5$ images as context frames. The visual decoupled components (Rows 3, 5, 7) and masks (Rows 4, 6, 8) of each branch are presented. \model{} successfully isolates noncontrollable dynamics from the complicated environment, \textit{i.e.}, other driving vehicles.}
\label{fig:carla-visual}
\end{center}
\vspace{-25pt}
\end{figure*}


\begin{figure}[t]
\vspace{-5pt}
    \centering
    \includegraphics[width=\linewidth]{figures/different_tau.pdf}
    \vspace{-22pt}
    \caption{The hyperparameter analysis of $\tau$ in CARLA.}
    \label{fig:different_tau}
    \vspace{-18pt}
\end{figure}

\vspace{-2pt}
\myparagraph{Qualitative results.}
We show the video prediction results of \model{} in the CARLA environment in \fig{fig:carla-visual}. 
%
Because of the first-person view in this environment, the agent actions potentially affect all pixel values in the observation, as the camera on the main car (\textit{i.e.}, the agent) moves. Therefore, we can view the dynamics of other vehicles as a combination of controllable and non-controllable states. 
%
Accordingly, our model determines which component is dominant by learning attention mask values between $0$ and $1$ across the action-conditioned and action-free branches. The ``action-free masks'' present hot spots around other vehicles, while the attention values in corresponding areas on the ``action-cond masks'' are still greater than zero.
%
As shown in the third and fifth lines, \model{} mainly learns the dynamics of mountains and trees in the action-conditioned branch and the dynamics of other driving vehicles in the action-free branch, respectively, which helps the agent avoid collisions by rolling out noncontrollable components to preview possible future states of other vehicles.








\myparagraph{Hyperparameter analyses of $\tau$.}
We evaluate the effect of using different numbers of rollout steps of the noncontrollable states as the inputs of the action model. 
%
From the results in \fig{fig:different_tau}, we observe that our model achieves the best performance at $\tau=5$.
%
However, there are no remarkable differences among $\tau \in [5,10,15]$, as long-term predictions for noncontrollable states may increase model errors. 
%
Besides, we implement a model without rolling out noncontrollable states into the future, \textit{i.e.}, $\tau=0$. It performs significantly worse than other baselines with $\tau \in [5,10,15]$, which demonstrates the benefit of rolling out the disentangled action-free branch in policy optimization.





\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_variance_dmc.pdf}
    \vspace{-18pt}
    \caption{The ablation study of the proposed variance constraints in DMC. We report the results averaged over $10$ seeds.}
    \vspace{-10pt}
    \label{fig:ablation_variance_dmc}
\end{figure*}

\begin{table}[t]
  \centering
  \vspace{3pt}
  \caption{Qualitative results in DMC. The agents are trained and evaluated in environments with \texttt{video\_easy} background. * indicates a different setup from that of DBC. Iso-Dream (\textit{conf.}) is the model from our conference paper, which only uses the reconstruction loss (w/o KL divergence) in the action-free branch.}
  \label{tab:dmc_result}
  \vspace{-10pt}
    \setlength\tabcolsep{4.5pt}
    \begin{center}
    \begin{tabular}{l|ccccccc}
    \toprule    
    \multirow{2}{*}{Method}  & Finger & Hopper &  Walker  & Cheetah   \\
    & Spin & Stand & Walk & Run \\
    \midrule
    SVEA  & 562 $\pm$ 22 & 6 $\pm$ 8 & 826 $\pm$ 65 & 178 $\pm$ 64 \\
    CURL  & 280 $\pm$ 50 & 451 $\pm$ 250 & 443 $\pm$ 206 & 269 $\pm$ 24\\
    DBC*  & 1 $\pm$ 2 & 5 $\pm$ 9 & 32 $\pm$ 7 & 15 $\pm$ 5 \\
    DreamerV2   & 755 $\pm$ 92 & 260 $\pm$ 366 & 655 $\pm$ 47 & 475 $\pm$ 159 \\
    DreamerV3 & 124 $\pm$ 52 & 472 $\pm$ 328 & 701 $\pm$ 114 & 546 $\pm$ 117 \\
    DreamerPro   & 721 $\pm$ 147 & 295 $\pm$ 129 & 813 $\pm$ 88 & 297 $\pm$ 63 \\
    Denoised-MDP  & 635 $\pm$ 284 & 104 $\pm$ 117 & 214 $\pm$ 56 & 233 $\pm$ 119 \\
    \midrule
    Iso-Dream (\textit{conf.})  & 800 $\pm$ 59 & 746 $\pm$ 312 & 911 $\pm$ 50 & \textbf{659 $\pm$ 62} \\
    Iso-Dream & 816 $\pm$ 16 & 769 $\pm$ 173 & 852 $\pm$ 97 & 597 $\pm$ 156  \\
    Iso-Dream++   &\textbf{ 938 $\pm$ 51} & \textbf{877 $\pm$ 34} & \textbf{932 $\pm$ 37} & \textbf{639 $\pm$ 19} \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-12pt}
\end{table}


\subsection{DeepMind Control Suite}
\label{expri:DMC}

\vspace{-3pt}
\myparagraph{Implementation.}
We evaluate our model on \texttt{video\_easy} and \texttt{video\_hard} benchmarks from the DMC Generalization Benchmark~\citep{hansen2021softda}, where the background is continually changing throughout an episode. 
%
All experiments use visual observations only, of shape $64 \times 64 \times 3$. The episodes last for $1{,}000$ steps and have randomized initial states. We apply a fixed action repeat of $R = 2$ across tasks. 
% We run every experiment with three different random seeds with standard deviation shown in shaded region.
%   
In this environment, since the background is randomly replaced by a real-world video, the noncontrollable motion of the background will affect the procedure of dynamics learning and behavior learning.
% 
Therefore, to obtain a better decision policy and avoid the disruption from noisy backgrounds, the agent may decouple noncontrollable representation (\textit{i.e.}, dynamic background) and controllable representation in spacetime, and only use controllable representation for control, thereby removing the modeling sparse dependency.
%
Instead of training the action-free branch with only reconstruction loss in our preliminary work \citep{paniso}, we follow the structure described in \sect{sec:wm} since the noncontrollable dynamics in some video backgrounds are complicated for learning, particularly \texttt{video\_hard} benchmark.
%
We evaluate the models using $4$ tasks, \textit{i.e.}, Finger Spin, Cheetah Run, Walker Walk, and Hopper Stand. The maximum number of environmental steps is $500k$. We use $\beta_1 = \beta_2 = 1$ and $\alpha = 1$ in Eq. \eqref{eq:loss} and $\lambda_1 = \lambda_2 = 1$ in Eq. \eqref{eq:all_loss}.






\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_variance_dmc_vis.pdf}
    \vspace{-20pt}
    \caption{Video prediction results from our approaches w/ and w/o the proposed variance constraints.
   }
    \label{fig:ablation_variance_dmc_vis}
    \vspace{-12pt}
\end{figure}



\begin{figure*}[!t]
\begin{center}
% \vspace{-5pt}
\centerline{\includegraphics[width=0.99\linewidth]{figures/dmc_showcases.pdf}}
\vspace{-10pt}
\caption{Video prediction results with noisy backgrounds in DMC. For each sequence, we use the first $5$ images as context frames. \model{} successfully disentangles controllable and noncontrollable components.}
\label{fig:dmc-visual}
\end{center}
\vspace{-20pt}
\end{figure*}



% \subsubsection{Quantitative Comparisons}
\myparagraph{Quantitative comparisons.} 
We present the quantitative results of \model{} for \texttt{video\_easy} benchmark in \tab{tab:dmc_result}. 
%
Our final model outperforms DreamerV2 and other baselines significantly in all tasks. 
% 
Compared with DBC and Denoised-MDP, which both aim to extract task-relevant representation from complex visual distractions, our method is more powerful with large performance gains on all four tasks, indicating that disentangling different dynamics by modular structure and variance constraints provides more cleaner and useful information for the downstream task.
%
Moreover, we have a better performance than DreamerPro, which is also based on Dreamer but learns the world model without reconstructing the observations. 
%
This demonstrates that our model effectively helps the agent to learn controllable visual representations and alleviate complex background interference.


% \subsubsection{Model Analyses}
\myparagraph{Analyses of the min-max variance constraints.}
We investigate the effectiveness of the proposed variance constraints described in \sect{sec:variance} by removing it from the training process of \model{}.  
%
As shown in \fig{fig:ablation_variance_dmc}, the compared models are trained for $10$ seeds, and the proposed method improves the performance of our model in most tasks, especially in \textit{finger spin}, where we have witnessed significant training collapse (see \fig{fig:collapse}).
%
In \fig{fig:ablation_variance_dmc_vis}, we provide a qualitative comparison of the disentanglement results between models trained with and without variance constraints.
% 
Comparing the fifth and sixth row of action-free branch outputs, we observe that the action-free dynamics (such as the light over the lake) are correctly assigned to the action-free branch by variance constraints, preventing the action-conditioned branch from capturing all dynamic information, \textit{i.e.}, training collapse. Because of the pure dynamics captured in the action-conditioned branch, our model with variance constraints gains definite improvements.



\begin{table}[t]
  \centering
  \vspace{3pt}
  \caption{The study of the generalization ability and robustness of the compared models to immediate visual distractions in DMC.}
  \label{tab:resistance}
    \vspace{-10pt}
    \begin{center}
    \begin{tabular}{l|cccc}
    \toprule    
    \multirow{2}{*}{Method}  & Finger & Hopper &  Walker  & Cheetah   \\
    & Spin & Stand & Walk & Run \\
    \midrule
    \multicolumn{5}{l}{Train: \textit{video\_easy}; Test: \textit{video\_hard}} \\
    \midrule
    DreamerPro   & 628 $\pm$  151 & 180 $\pm$ 96 & 533 $\pm$ 212 &   244 $\pm$  27 \\
    Denoised-MDP   & 27   $\pm$  21 & 44  $\pm$   25 & 169  $\pm$   61 & 103 $\pm$ 46 \\
    Iso-Dream++  & \textbf{692 $\pm$ 185}  & \textbf{643 $\pm$  155} & \textbf{642 $\pm$ 129} & \textbf{441 $\pm$ 183} \\
    \midrule
    \multicolumn{5}{l}{Train: \textit{video\_easy}; Test: \textit{video\_easy with Gaussian noises}} \\
    \midrule
    DreamerPro   & 663 $\pm$  129 & 223   $\pm$  76 & 824 $\pm$  72 &  263 $\pm$ 55 \\
    Denoised-MDP   & 652  $\pm$ 306 & 103  $\pm$  110 & 180  $\pm$ 79 & 195 $\pm$  131  \\
    Iso-Dream++   & \textbf{851 $\pm$ 109} & \textbf{806 $\pm$  74} &  \textbf{906 $\pm$ 31} & \textbf{582 $\pm$ 69} \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-18pt}
\end{table}


\myparagraph{Qualitative results.}
We use \model{} to perform video prediction in the DMC environment with \texttt{video\_easy} backgrounds. The frame sequence and actions are randomly collected from test episodes. The first $5$ frames are given to the model and the next $45$ frames are predicted only based on action inputs. 
%
In this environment, the video background can be viewed as a combination of noncontrollable dynamics and static representations.
% 
\fig{fig:dmc-visual} visualizes the entire generated RGB images, the decoupled RGB components, and the corresponding masks of the three network branches. 
%
From these results, we observe that our approach has the ability to predict long-term sequence and disentangle controllable (agent) and noncontrollable dynamics (background motion) from complex visual images. As shown in the third and fourth rows in \fig{fig:dmc-visual}, the controllable representation has been successfully isolated and matches its mask. 
% 
As shown in the fifth and sixth rows, the motion of fires and sea waves are captured as noncontrollable dynamics by the action-free branch.


\myparagraph{Robustness to immediate distractions.}
%
To assess the ability of \model{} to resist immediate visual distractions, we train the RL models on the \textit{video\_easy} benchmark and evaluate them on (i) \textit{video\_hard}; (ii) \textit{video\_easy with Gaussian noises}.
%
In Table \ref{tab:resistance}, we compare the results from \model{} with those from DreamerPro and Denoised-MDP, which both focus on learning robust representations against visual noises.
%
We observe a remarkable advantage of \model{} against unexpected distractions, which consistently outperforms the DreamerPro and Denoised-MDP across all tasks.




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/carla_day_to_night.pdf}
    \vspace{-22pt}
    \caption{Transfer learning results across \texttt{Day} and \texttt{Night} modes in DMC. Leveraging a pretrained \textit{Day-mode} action-free branch can greatly benefit the finetuning results of \model{} in the \textit{Night mode}. Results are averaged over $10$ seeds.}
    \label{fig:day_to_night}
    \vspace{-8pt}
\end{figure}





\begin{figure*}[!b]
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{figures/easy_to_hard_two_tasks.pdf}
    \vspace{-22pt}
    \caption{Transfer learning results in DMC across environments with \texttt{video\_easy} and \texttt{video\_hard} backgrounds. 
    %
    Unlike DreamerV2, which can only transfer the entire world model (\textcolor{red}{red curve}) to the target domain, \model{} enables us to separately transfer the pretrained action-conditioned branch and obtain significantly better finetuning results (\textcolor{blue}{blue curve}). }
    \label{fig:easy_to_hard_two_tasks}
    % \vspace{-5pt}
\end{figure*}


\subsection{Transfer Learning Analyses}
\vspace{-3pt}
\myparagraph{Transfer of noncontrollable dynamics in CARLA.}
% 
Our model learns different dynamics in different branches, which makes it naturally suitable for transfer learning. Unlike common methods that transfer all knowledge from a pretrained source task, we can selectively transfer specific knowledge for a target task. 
% 
Specifically, we only transfer relevant knowledge, such as shared dynamics between source and target tasks, to achieve precise disentanglement and robust decision-making on the target task.
% 
In \fig{fig:day_night_vis}, We can see that the noncontrollable dynamics are similar between day and night modes, \textit{i.e.}, the movement of other driving vehicles.
%
We keep the action-free branch pretrained on the day mode of the CARLA environment and then train it on the night mode. The results are shown in \fig{fig:day_to_night}.
%
Comparing the orange curve and the blue curve, our model that transfers noncontrollable dynamics in the action-free branch has a significant improvement. However, the performance gain of DreamerV2 is small. 
%
Therefore, due to the modular structure in our \model{}, when there are two environments with similar dynamics, we can train on the easy environment first, and then load the specific pretrained branch to help the model learn on difficult tasks.
%
Therefore, the modular structure of our \model{} allows us to selectively transfer controllable or noncontrollable parts to novel domains based on our prior knowledge of the domain gap.





\myparagraph{Transfer of controllable dynamics in DMC.}
For DMC, we use the \texttt{video\_easy} (source) and \texttt{video\_hard} (target) benchmarks to evaluate the transfer ability of our model.
%
We transfer the controllable information in the action-conditioned branch because the controllable dynamics are the same in both environments, \textit{i.e.}, the motion of the agent.
%
From
\fig{fig:easy_to_hard_two_tasks}, we have two key observations. 
%
First, upon loading the pretrained action-conditioned branch, \model{} exhibits a significant advantage over the non-pretrained counterpart.
% 
Second, it is noteworthy that the performance improvement achieved by our model through pretraining surpasses that of DreamerV2 by a considerable margin.

