\section{Introduction}

\begin{figure*}[t]
% \begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{figures/intro.pdf}}
% \vspace{-5pt}
\caption{Graphic model of our approach. The world model learns to decouple mixed visual dynamics into controllable states ($s_t$) and noncontrollable states ($z_t$) by optimizing the inverse dynamics (\textcolor{MyDarkRed}{as indicated by the red dashed arrows}). With state decoupling, the RL agent can make decisions based on the forecasts of future noncontrollable dynamics of the environment (\textcolor{MyDarkBlue}{blue arrows}).
%
In forward modeling, we consider the sparse dependency of next-step noncontrollable states on current controllable states (\textcolor{MyDarkGreen}{green arrows}). 
%
In representation learning, we further cope with the imbalance of dynamic information learned in different state transition branches.
}
\label{fig:intro}
% \end{center}
\vskip -1mm
\end{figure*}


Humans can infer and predict real-world dynamics by simply observing and interacting with the environment. 
%
Inspired by this, many cutting-edge AI agents use self-supervised learning \cite{oh2015action,ha2018world,ebert2018visual} or reinforcement learning \cite{oh2017value,hafner2019dream,sekar2020planning} techniques to acquire knowledge from their surroundings. 
%
Among them, world models \cite{ha2018world} have received widespread attention in the field of robotic visuomotor control, and led the recent progress in model-based reinforcement learning (MBRL) with visual inputs \cite{hafner2019dream,sekar2020planning,hafner2020mastering,kaiser2020model}.
%
One representative approach called Dreamer~\cite{hafner2019dream} learns a differentiable simulator of the environment (\ie, the world model) using observations and actions of an actor-critic agent, then updates the agent by optimizing its behaviors based on future latent states and rewards (\ie, latent imagination) generated by the world model.
%
However, since the observation trajectories are high-dimensional, highly non-stationary, and often driven by multiple sources of physical dynamics, how to learn effective world models in complex visual scenes remains an open problem.


In this paper, we propose to understand the world by decomposing it into \textit{controllable} and \textit{noncontrollable} state transitions, \ie, $s_{t+1}\sim p(\cdot \mid s_t,a_t)$ and $z_{t+1}\sim p(\cdot \mid z_t)$, according to the responses to action signals.
%
This idea is largely inspired by practical scenarios such as autonomous driving, in which we can naturally divide spatiotemporal dynamics in the system into controllable parts that perfectly respond to the actions (\eg, accelerating and steering) and parts beyond the control of the agent (\eg, movement of other vehicles).
%
Decoupling latent state transitions in this way can improve MBRL in three aspects: 
\begin{itemize}[leftmargin=*]
  \item It allows decisions to be made based on predictions of future noncontrollable dynamics that are independent (or indirectly dependent) of the action, thereby improving the performance on long-horizon control tasks. For example, in the CARLA self-driving environment, potential risks can be better avoided by anticipating the movement of other vehicles.
  \item Modular world models improve the robustness of the RL agent in noisy environments, as demonstrated in our modified DeepMind Control Suite with the time-varying background.
  \item The isolation of controllable state transitions further facilitates transfer learning across different but related domains. We can adapt parts of the world model to novel domains based on our prior knowledge of the domain gap. 
\end{itemize}


Specifically, we present \model{}, a novel MBRL framework that learns to decouple and leverage the controllable and noncontrollable state transitions.
%
Accordingly, it improves the original Dreamer \cite{hafner2019dream} from two perspectives: (i) \textit{a new form of world model representation} and (ii) \textit{a new actor-critic algorithm to derive the behavior from the world model.}

\subsection{How to learn a decoupled world model?}

From the perspective of representation learning, we improve the world model to separate mixed visual dynamics into an action-conditioned branch and an action-free branch of latent state transitions (see \fig{fig:intro}). 
%
These components are jointly trained to maximize the variational lower bounds.
%
Besides, the action-conditioned branch is particularly optimized with \textit{inverse dynamics} as an additional objective function, that is, to reason about the actions that have driven the ``controllable'' state transitions between adjacent time steps.  


Nonetheless, as we have observed in our preliminary work at NeurIPS'2022~\cite{paniso}, which we call Iso-Dream, the learning process of inverse dynamics is prone to the problem of ``training collapse'', where the action-conditioned branch captures all dynamic information, while the action-free branch learns almost nothing. 
%
To further isolate different dynamics in an unsupervised manner, we use new forms of min-max variance constraints to regularize the information flow of dynamics in the decoupled world model.
%
More concretely, we provide a batch of hypothetical actions to the world model, and encourage the action-conditioned branch to produce different state transitions based on the same state, while penalizing the diversity of those in the action-free branch.




\subsection{How to improve behavior learning based on decoupled world models?}


Humans can decide how to interact with the environment at each moment based on their anticipation of future changes in their surroundings. 
%
Accordingly, by decoupling the state transitions, our approach can explicitly forecast the evolution of action-independent dynamics in the system, thereby greatly benefiting downstream decision-making tasks.
%
Unlike Dreamer, it performs latent state imagination in both the training phase and testing phase of the agent behaviors to make more forward-looking decisions.
%
As shown by the blue arrows in \fig{fig:intro}, the policy network integrates the current controllable state and multiple steps of predicted noncontrollable states through an attention mechanism. 
%
Intuitively, since future noncontrollable states at different steps may have different weights of impact on the current decision of the agent, the attention mechanism enables the agent to adaptively consider possible future interactions with the environment. It ensures that only appropriate future states are fed back into the policy.




Despite the effectiveness of the new behavior learning scheme, it only considers the indirect influence of action-free dynamics on future action-conditioned dynamics through agent behaviors (\ie, $z_{t:t+\tau} \rightarrow a_t \rightarrow s_{t+1}$). 
%
Another improvement of our approach over Iso-Dream is that it further models the \textit{sparse dependency} of future noncontrollable states on current controllable states (\ie, $s_{t} \rightarrow z_{t+1}$), which is indicated by the green arrows in \fig{fig:intro}. 
%
In practical scenarios, for example, when we program a robot to compete with another one in a dynamic game, the opponent can adjust its policy according to the behavior of our agent.
%
In autonomous driving, when an agent vehicle veers into the lane of other vehicles, typically those vehicles will slow down to avoid a collision.
%
Because of the proposed solution to training collapse, modeling the sparse dependency does not affect the disentanglement learning ability of the world model.
%
In behavior learning, actions are sampled from $\pi(s_t,z_{t:t+\tau})$, where $z_{t+1}\sim p(\cdot|z_t,s_t)$, while due to the sparsity of the cross-branch dependencies, the long-horizon noncontrollable future can be approximated as $z_{t+2:t+\tau}\sim p(\cdot|z_{t+1:t+\tau-1})$.





We evaluate \model{} in the following domains: (i) the CARLA autonomous driving environment in which other vehicles can be naturally viewed as noncontrollable components; (ii) the modified DeepMind Control Suite with noisy video background. 
%
Our approach outperforms existing approaches by large margins and further achieves significant advantages in transfer learning by isolating dynamics.
%
It can selectively transfer controllable or noncontrollable parts of the learned state transition functions from the source domain to the target domain according to the prior information.


The main contributions of this paper are summarized as follows:
\begin{itemize}[leftmargin=*]
% \vspace{-5pt}
  \item We present a new world model and encourage the decomposition of latent state transitions by optimizing \textit{inverse dynamics}. 
  \item We introduce the \textit{min-max variance constraints} to prevent all information from collapsing into a single state transition branch.
  \item We improve the actor-critic algorithm to make \textit{forward-looking decisions} based on the forecasts of future noncontrollable states.
  \item We model the \textit{sparse dependency} of the next-step noncontrollable dynamics on current controllable dynamics to provide a more accurate simulation of some practical dynamic environments.
  \item We empirically demonstrate the advantages of \model{} over existing methods in standard, noisy, and \textit{transfer learning} setups.
\end{itemize}
In summary, we extend our previous studies with (i) the min-max variance constraints, (ii) the sparse dependence between the decoupled latent states, and (iii) the transfer learning experiments. 



