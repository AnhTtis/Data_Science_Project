\section{Problem Overview}

\begin{figure*}[t] 
    \centering
	  \subfloat[]{
       \includegraphics[width=3.2in]{figures/ball_sparse_vis.pdf}}
    \label{fig:ball_sparse_vis} 
    \hfil
	  \subfloat[]{
        \includegraphics[width=2.9in]{figures/car_sparse_vis.pdf}}
    \label{fig:car_sparse_vis}
    \vspace{-5pt}
	\caption{Examples of sparse dependency of the noncontrollable state on the controllable state. (a) A game resembling ice hockey that is played on a desk, in which the ego-agent is controllable, while the opponent robot can be seen as the noncontrollable part, and the hockey puck can be mostly considered to have predictable dynamics independent of the agent's actions. \textit{Sparse dependency} occurs at the moment the agent hits the puck because it may change direction depending on the agent's current state. (b) In autonomous driving, other vehicles (\textit{yellow}) will change their driving directions to avoid collision when the ego-agent (\textit{white}) takes up their driveway.}
	  \label{fig:sparse_vis} 
   \vspace{-5pt}
\end{figure*}

\subsection{Problem Definition}

In visual control tasks, the agent learns the action policy directly from high-dimensional observations. We formulate visual control as a partially observable Markov decision process (POMDP) with a tuple $(\mathcal S,\mathcal A,\mathcal T,\mathcal R,\mathcal O)$, where $\mathcal S$ is the state space, $\mathcal A$ is the action space, $\mathcal O$ is the observation space, $\mathcal R(s_t, a_t)$ is the reward function, and $ \mathcal T(s_{t+1} \mid s_t, a_t) $ is the state-transition distribution. 
%
At each timestep $t\in[1; T]$, the agent takes an action $a_t \in A$ to interact with the environment and receives a reward $r_t = \mathcal R(s_t, a_t)$. The objective is to learn a policy that maximizes the expected cumulative reward $\mathbb E_p[\sum^T_{\tau=1} r_{\tau}]$.
%
In this setting, the agent cannot access the true states in $\mathcal S$.

\subsection{Key Challenges}
\label{sec:challenges}

\myparagraph{Challenge 1: How to learn future-conditioned policies without the expensive Monte-Carlo planning?}
%
Forecasting future environmental changes is useful for decision-making in a non-stationary system.
%
A typical solution, such as the cross-entropy method (CEM), is to perform Monte-Carlo sampling over future actions and value the consequences of multiple action trajectories \cite{finn2017deep,ebert2018visual,hafner2019learning}.
% 
These algorithms are expensive in computational cost, especially when we have large action and state spaces.
%
The question is: \textit{Can we design an RL algorithm that allows for future-conditioned decision-making without playing dice in the action space?}




\myparagraph{Challenge 2: How to avoid ``training collapse'' in unsupervised dynamics disentanglement?} 
%
Despite the great success in unsupervised representation learning \cite{locatello2019challenging,he2020momentum,qian2022unsupervised}, it remains a challenge to disentangle the controllable and noncontrollable dynamic patterns in non-stationary visual scenes. 
% 
One potential solution is to employ modular structures that learn different dynamics in separate branches. However, without proper constraints, the model may suffer from ``training collapse'', where one branch captures all useful information and the other learns almost nothing.
%
This phenomenon may occur when the noncontrollable dynamics components are easy to predict. In this case, we consider adding further constraints to the learning objects of the action-conditioned and action-free state transition branches, encouraging them to isolate the noncontrollable part from the mixed dynamics.




\myparagraph{Challenge 3: How to model situations where the agent behavior has only a sparse/indirect impact on noncontrollable dynamics?}
% 
As we know, in realistic scenarios, the noncontrollable component of the dynamics may not evolve independently but may depend on the motions of the controllable component. 
%
For instance, in \fig{fig:sparse_vis} (a), the hockey puck on the desk (noncontrollable part) changes its direction when the agent (controllable part) hits it. 
%
For autonomous driving, in \fig{fig:sparse_vis} (b), other vehicles (noncontrollable part) will slow down to avoid a collision when the agent (controllable part) takes their lane. 
%
If we assume that our actions do not indirectly affect other vehicles on the road, then for safety reasons, a sub-optimal policy for handling heavy traffic could be to follow the vehicle in front of us instead of changing lanes.
%
Accordingly, we propose a sparse dependency mechanism that enhances our model's decision-making ability. 
%
Empirical results are illustrated in \fig{fig:sparse_ablation_vis}.




% \vspace{-5pt}
\subsection{Basic Assumptions}
\label{sec:motivation}
% \vspace{-5pt}


In our proposed framework as shown in \fig{fig:intro}, when the agent receives a sequence of visual observations $o_{1:T}$, the underlying spatiotemporal dynamics can be defined as $u_{1:T}$. The evolution of different dynamics can be caused by different forces, but here we aim to decouple $u_{1:T}$ into controllable latent states $s_{1:T}$ and time-varying noncontrollable latent states $z_{1:T}$, such that:
% \vspace{-2pt}
\begin{equation}
\label{eq:basic_assum}
  \begin{aligned}
    u_{1:T} &\sim  (s, z)_{1:T}, \\
    s_{t+1} &\sim p(s_{t+1} \mid s_t, a_t), \\
    z_{t+1} &\sim p(z_{t+1} \mid z_t),
  \end{aligned}
\end{equation}
where $a_t$ is the action signal. 
%
By isolating $s_t$ and $z_t$ to each other, we model their state transitions of $p(s_{t+1} \mid s_t, a_t)$ and $p(z_{t+1} \mid z_t)$ respectively. 
% 
We assume that a more clear decoupling of $s_t$ and $z_t$ can benefit both long-term predictions and decision-making.
%
As an extension of our preliminary work \cite{paniso}, we additionally model the sparse dependency of noncontrollable dynamics on controllable dynamics (as described below). Thus, when a sparse event is detected, the transition of noncontrollable state in \eqn{eq:basic_assum} can be rewritten as $z_{t+1} \sim p(z_{t+1} \mid z_t, s_t)$.



% 
It assumes that the agent can greatly benefit from predicting the consequences of external noncontrollable forces.
%
During behavior learning, we roll out the noncontrollable states and then associate them with the current controllable states for more proactive decision-making.
% 
We derive the action policy by
% \vspace{-2pt}
\begin{equation}  
    a_t \sim \pi(a_t \mid s_t, \mathbbm{1} \odot z_{t:t+\tau}),
\end{equation}
where $\mathbbm{1}$ is an indicator according to our prior knowledge about the environment. For example, in autonomous driving, since it is reasonable for the ego-agent to make decisions based on the predictions about the future states of other vehicles, we have $\mathbbm{1}=1$ and calculate the relations between $s_t$ and the imagined noncontrollable states in a time horizon $\tau$. 
%
Otherwise, for some specific tasks where the noncontrollable components are irrelevant to decision-making, we can simply set the indicator function to $\mathbbm{1}=0$ and treat them as noisy distractions.






