\section{Related Work}

\vspace{-3pt}
\myparagraph{Visual RL.}
% \subsection{Visual RL.}
In visual control tasks, RL agents learn the action policy directly from high-dimensional observations.
% 
They can be roughly grouped into two categories, that is, model-free methods \citep{haarnoja2018soft,srinivas2020curl,yarats2019improving,kostrikov2020image,laskin2020reinforcement,hansen2021stabilizing} and model-based methods \citep{finn2017deep,oh2017value,ha2018world,hafner2019learning,hafner2019dream,kaiser2020model,sekar2020planning,zhang2021learning,bharadhwaj2022information,wang2022denoised,deng2022dreamerpro,xu2022RewardFree,ji2022update}.
%
% The model-free methods optimize the RL objective together with some auxiliary loss \citep{srinivas2020curl, yarats2019improving}, or improve performance and sample efficiency or generalization ability of agents through data augmentation \citep{kostrikov2020image, laskin2020reinforcement, hansen2021stabilizing}.
%
Among them, the MBRL approaches explicitly model the state transitions and generally yield higher sample efficiency than the model-free methods.
%
A notable branch of work is the MuZero models, such as Stochastic MuZero \citep{antonoglou2022planning}. These models simulate and explore possible future action trajectories through Monte Carlo tree search (MCTS), which can effectively improve long-term decision-making but introduce a vast computational cost.
%
Notably, our model is different from Stochastic MuZero in two ways. First, we improve dynamics learning by encouraging representation decoupling, which we assume can enable the model to better understand the controllable and noncontrollable parts of the environment and greatly benefit the learned policy.
%
Second, unlike in Stochastic MuZero, our model performs an actor-critic algorithm without MCTS, which is practical in short-term control tasks such as autonomous driving and ensures higher efficiency for both policy optimization and deployment.
%
Another line of work is the so-called \textit{World Models}. 
%
Ha and Schmidhuber \cite{ha2018world} proposed to learn compressed latent states of the environment in a self-supervised manner and optimize potential behaviors based on the latent states generated by the world model.
%
Similarly, PlaNet \citep{hafner2019learning} introduces the \textit{recurrent state-space model} (RSSM) as the world model and performs the cross-entropy method over the imagined recurrent states. 
%
DreamerV1-V3 \citep{hafner2019dream,hafner2020mastering,hafner2023mastering} employ actor-critic methods to optimize the expected values and agent's behaviors over the predicted latent states in RSSM.
%
Specifically, our model based on DreamerV2 outperforms DreamerV2-V3 remarkably in CARLA and DMC. We also note that the state decoupling and future-conditioned behavior learning techniques proposed in \model{} can be seamlessly integrated with DreamerV2-V3, consistently enhancing their overall performance and convergence rate.



\myparagraph{Visual RL with visual distractions.}
% \subsection{Visual RL with Visual Distractions}
However, for complex visual environments with background or even dynamic distractions, it is still challenging to learn effective behavior policies. 
%
To tackle this problem, some approaches \citep{zhang2021learning,bharadhwaj2022information,wang2022denoised,deng2022dreamerpro} learn a more robust representation by discarding pixel-reconstruction to avoid struggling with the presence of visual noises. 
%
DreamerPro \citep{deng2022dreamerpro} uses online clustering to learn prototypes from the recurrent states of the world model, eliminating the need for reconstruction. 
%
Denoised-MDP \citep{wang2022denoised} categorizes system dynamics into four types based on their controllability and relation to rewards, and optimizes the policy model only with information that is both controllable and relevant to rewards.
%
It is worth noting that \model{} differs significantly from the aforementioned methods in two key ways. 
%
First, we explicitly model the state transitions of controllable and noncontrollable dynamics in two distinct branches. 
%
This modular structure empirically facilitates transfer learning between related but distinct domains.
%
Second, the decoupled world model offers a more versatile method of learning behavior. By previewing possible future states of noncontrollable patterns, we can make informed decisions at present. This also allows us to choose whether or not to incorporate noncontrollable states into our decision-making process, based on our prior knowledge of the specific domain.

% \pmt{\myparagraph{DBC} DBC \citep{DBLP:conf/iclr/0001MCGL21} proposes learning an invariant representation which encode only the task-relevant information from observations using the bisimulation metric without pixel-reconstruction.}



\myparagraph{Action-conditioned video prediction.}
% \subsection{Action-Conditioned Video Prediction}
Another branch of deep learning solutions to visual control problems is to learn action-conditioned video prediction models \cite{oh2015action,Finn2016Unsupervised,chiappa2017recurrent,wang2021predrnn,babaeizadeh2021fitvid} and then perform Monte-Carlo importance sampling and optimization algorithms, such as the \textit{cross-entropy methods}, over available behaviors \cite{finn2017deep,ebert2018visual,jung2019goal}.  
%
Hot topics in video prediction mainly include long-term and high-fidelity future frames generation \citep{srivastava2015unsupervised,shi2015convolutional,vondrick2016generating,bhattacharjee2017temporal,wang2017predrnn,villegas2017learning,wichers2018hierarchical,reda2018sdc,oliu2018folded,liu2018dyan,xu2018structure,jin2020exploring,behrmann2021unsupervised}, dynamics uncertainty modeling \cite{babaeizadeh2017stochastic,denton2018stochastic,villegas2019high,kim2019variational,castrejon2019improved,franceschi2020stochastic,wu2021greedy}, object-centric scene decomposition \cite{van2018relational,hsieh2018learning,greff2019multi,zablotskaia2020unsupervised,bei2021learning,greff2017neural,kosiorek2018sequential}, and space-time disentanglement \cite{Villegas2017Decomposing,hsieh2018learning,guen2020disentangling,bodla2021hierarchical}.
%
The corresponding technical improvements mainly involve the use of more effective neural architectures, novel probabilistic modeling methods, and specific forms of video representations. 
%
The disentanglement methods are closely related to the world model in \model{}.
%
They commonly separate visual dynamics into content and motion vectors, or long-term and short-term states.
%
In contrast, \model{} is designed to learn a decoupled world model based on controllability, which contributes more to the downstream behavior learning process.



