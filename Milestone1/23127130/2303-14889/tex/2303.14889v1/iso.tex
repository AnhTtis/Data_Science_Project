\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
\ifpdf
  % pdf code
\else
  % dvi code
\fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
% \usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
 \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




\ifCLASSOPTIONcaptionsoff
 \usepackage[nomarkers]{endfloat}
\let\MYoriglatexcaption\caption
\renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
\let\MYorigsubfloat\subfloat
\renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{xspace}
% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\usepackage{enumitem}
\usepackage{caption}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{eso-pic}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url}
\usepackage{graphicx}
\frenchspacing
\usepackage{mathrsfs}
% \usepackage{color}
\usepackage{multirow}
% \usepackage{subfigure}
\usepackage{comment}
\usepackage{marvosym}
\usepackage{bbm}
\usepackage{enumerate}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
% \usepackage{multirow}
\usepackage{booktabs} % for pretty plots
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{makecell}
\usepackage{multirow}
\let\Cross\relax
\let\Square\relax
\usepackage{bbding}
\usepackage[numbers]{natbib}
\usepackage[breaklinks=true,bookmarks=false,colorlinks]{hyperref}
%\usepackage{graphicx}
%\usepackage{subfig}
\usepackage{cleveref}

\usepackage[normalem]{ulem}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\eqn}[1]{Eq.~\eqref{#1}}
\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\tab}[1]{Table~\ref{#1}}
\newcommand{\fig}[1]{Fig.~\ref{#1}}
\newcommand{\alg}[1]{Alg.~\ref{#1}}

\newcommand{\myparagraph}[1]{\vspace{5pt} \noindent \textbf{#1}}

\newcommand{\del}[1]{\textcolor{red}{\sout{#1}}}
\newcommand{\revise}[1]{\textcolor{red}{#1}}


\definecolor{MyDarkBlue}{rgb}{0,0.5,1}
\definecolor{MyDarkGreen}{rgb}{0.02,0.6,0.02}
\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}
\definecolor{MyDarkOrange}{rgb}{0.40,0.2,0.02}
\definecolor{MyPurple}{RGB}{111,0,255}
\definecolor{MyRed}{rgb}{1.0,0.0,0.0}
\definecolor{MyGold}{rgb}{0.75,0.6,0.12}
\definecolor{MyDarkgray}{rgb}{0.66, 0.66, 0.66}
\newcommand{\model}{Iso-Dream++}
\newcommand{\icml}{Iso-Dream}


% \newcommand{\icml}[1]{{\textcolor{MyDarkBlue}{#1}}}

\newcommand{\myitem}{\vspace{2pt}\item}

\newcommand{\yb}[1]{{\textcolor{MyDarkGreen}{[wyb: #1]}}}
\newcommand{\zxm}[1]{{\textcolor{MyPurple}{#1}}}
\newcommand{\pmt}[1]{{\textcolor{MyDarkBlue}{#1}}}

\newcommand{\todo}{\textcolor{MyRed} }

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\begin{document}

% \title{Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models}
% \title{Iso-Dream++: Model-Based Reinforcement Learning with Isolated Imaginations}
\title{Model-Based Reinforcement Learning with Isolated Imaginations}


%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Minting Pan,
        Xiangming Zhu,
        Yunbo~Wang,
        Xiaokang Yang,~\IEEEmembership{Fellow,~IEEE}
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem The authors are with MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China.
\IEEEcompsocthanksitem Corresponding author: Y. Wang, yunbow@sjtu.edu.cn.
\IEEEcompsocthanksitem Code: \url{https://github.com/panmt/MBRL_with_Isolated_Imaginations}.
}
}

\markboth{IEEE Transactions on Pattern Analysis and Machine Intelligence,~Vol.~XX, No.~X, March~2023}%
{Pan \MakeLowercase{\textit{et al.}}: Iso-Dream++: Model-Based Reinforcement Learning with Isolated Imaginations}
% {Pan \MakeLowercase{\textit{et al.}}: Iso-Dream++: Future-Conditioned MBRL with Isolated Imaginations}

\IEEEtitleabstractindextext{%

\begin{abstract}

World models learn the consequences of actions in vision-based interactive systems. However, in practical scenarios like autonomous driving, noncontrollable dynamics that are independent or sparsely dependent on action signals often exist, making it challenging to learn effective world models. To address this issue, we propose Iso-Dream++, a model-based reinforcement learning approach that has two main contributions. First, we optimize the inverse dynamics to encourage the world model to isolate controllable state transitions from the mixed spatiotemporal variations of the environment. Second, we perform policy optimization based on the decoupled latent imaginations, where we roll out noncontrollable states into the future and adaptively associate them with the current controllable state. This enables long-horizon visuomotor control tasks to benefit from isolating mixed dynamics sources in the wild, such as self-driving cars that can anticipate the movement of other vehicles, thereby avoiding potential risks. On top of our previous work~\cite{paniso}, we further consider the sparse dependencies between controllable and noncontrollable states, address the training collapse problem of state decoupling, and validate our approach in transfer learning setups. Our empirical study demonstrates that Iso-Dream++ outperforms existing reinforcement learning models significantly on CARLA and DeepMind Control.


\end{abstract}

}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



% marked
\section{Introduction}
% \blfootnote{Code available at \url{https://github.com/panmt/Iso-Dream}}


% Contrib 1: isolation
% Contrib 2: RL with rollouts
% Ext 1: sparse correlation
% Ext 2: training collapse
% Ext 3: transfer (experiment)


\begin{figure*}[t]
% \begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{figures/intro.pdf}}
% \vspace{-5pt}
\caption{Graphic model of our approach. The world model learns to decouple mixed visual dynamics into controllable states ($s_t$) and noncontrollable states ($z_t$) by optimizing the inverse dynamics (\textcolor{MyDarkRed}{as indicated by the red dashed arrows}). With state decoupling, the RL agent can make decisions based on the forecasts of future noncontrollable dynamics of the environment (\textcolor{MyDarkBlue}{blue arrows}).
%
In forward modeling, we consider the sparse dependency of next-step noncontrollable states on current controllable states (\textcolor{MyDarkGreen}{green arrows}). 
%
In representation learning, we further cope with the imbalance of dynamic information learned in different state transition branches.
}
\label{fig:intro}
% \end{center}
\vskip -3mm
\end{figure*}


Humans can infer and predict real-world dynamics by simply observing and interacting with the environment. 
%
Inspired by this, many cutting-edge AI agents use self-supervised learning \cite{oh2015action,ha2018world,ebert2018visual} or reinforcement learning \cite{oh2017value,hafner2019dream,sekar2020planning} techniques to acquire knowledge from their surroundings. 
%
Among them, world models \cite{ha2018world} have received widespread attention in the field of robotic visuomotor control, and led the recent progress in model-based reinforcement learning (MBRL) with visual inputs \cite{hafner2019dream,sekar2020planning,hafner2020mastering,kaiser2020model}.
%
One representative approach called Dreamer~\cite{hafner2019dream} learns a differentiable simulator of the environment (\ie, the world model) using observations and actions of an actor-critic agent, then updates the agent by optimizing its behaviors based on future latent states and rewards (\ie, latent imagination) generated by the world model.
%
However, since the observation trajectories are high-dimensional, highly non-stationary, and often driven by multiple sources of physical dynamics, how to learn effective world models in complex visual scenes remains an open problem.


In this paper, we propose to understand the world by decomposing it into \textit{controllable} and \textit{noncontrollable} state transitions, \ie, $s_{t+1}\sim p(\cdot \mid s_t,a_t)$ and $z_{t+1}\sim p(\cdot \mid z_t)$, according to the responses to action signals.
%
This idea is largely inspired by practical scenarios such as autonomous driving, in which we can naturally divide spatiotemporal dynamics in the system into controllable parts that perfectly respond to the actions (\eg, accelerating and steering) and parts beyond the control of the agent (\eg, movement of other vehicles).
%
Decoupling latent state transitions in this way can improve MBRL in three aspects: 
\begin{itemize}[leftmargin=*]
  \item It allows decisions to be made based on predictions of future noncontrollable dynamics that are independent (or indirectly dependent) of the action, thereby improving the performance on long-horizon control tasks. For example, in the CARLA self-driving environment, potential risks can be better avoided by anticipating the movement of other vehicles.
  \myitem Modular world models improve the robustness of the RL agent in noisy environments, as demonstrated in our modified DeepMind Control Suite with the time-varying background.
  \myitem The isolation of controllable state transitions further facilitates transfer learning across different but related domains. We can adapt parts of the world model to novel domains based on our prior knowledge of the domain gap. 
\end{itemize}


Specifically, we present \model{}, a novel MBRL framework that learns to decouple and leverage the controllable and noncontrollable state transitions.
%
Accordingly, it improves the original Dreamer \cite{hafner2019dream} from two perspectives: (i) \textit{a new form of world model representation} and (ii) \textit{a new actor-critic algorithm to derive the behavior from the world model.}

\subsection{How to learn a decoupled world model?}

From the perspective of representation learning, we improve the world model to separate mixed visual dynamics into an action-conditioned branch and an action-free branch of latent state transitions (see \fig{fig:intro}). 
%
These components are jointly trained to maximize the variational lower bounds.
%
Besides, the action-conditioned branch is particularly optimized with \textit{inverse dynamics} as an additional objective function, that is, to reason about the actions that have driven the ``controllable'' state transitions between adjacent time steps.  


Nonetheless, as we have observed in our preliminary work at NeurIPS'2022~\cite{paniso}, which we call Iso-Dream, the learning process of inverse dynamics is prone to the problem of ``training collapse'', where the action-conditioned branch captures all dynamic information, while the action-free branch learns almost nothing. 
%
To further isolate different dynamics in an unsupervised manner, we use new forms of min-max variance constraints to regularize the information flow of dynamics in the decoupled world model.
%
More concretely, we provide a batch of hypothetical actions to the world model, and encourage the action-conditioned branch to produce different state transitions based on the same state, while penalizing the diversity of those in the action-free branch.




\subsection{How to improve behavior learning based on decoupled world models?}


Humans can decide how to interact with the environment at each moment based on their anticipation of future changes in their surroundings. 
%
Accordingly, by decoupling the state transitions, our approach can explicitly forecast the evolution of action-independent dynamics in the system, thereby greatly benefiting downstream decision-making tasks.
%
Unlike Dreamer, it performs latent state imagination in both the training phase and testing phase of the agent behaviors to make more forward-looking decisions.
%
As shown by the blue arrows in \fig{fig:intro}, the policy network integrates the current controllable state and multiple steps of predicted noncontrollable states through an attention mechanism. 
%
Intuitively, since future noncontrollable states at different steps may have different weights of impact on the current decision of the agent, the attention mechanism enables the agent to adaptively consider possible future interactions with the environment. It ensures that only appropriate future states are fed back into the policy.




Despite the effectiveness of the new behavior learning scheme, it only considers the indirect influence of action-free dynamics on future action-conditioned dynamics through agent behaviors (\ie, $z_{t:t+\tau} \rightarrow a_t \rightarrow s_{t+1}$). 
% \pmt{
% The effect of noncontrollable dynamics on controllable dynamics can be reflected in agent behaviors. As for the reverse, we use sparse interaction to model the influence of controllable dynamics on noncontrollable dynamics.
% }
%
Another improvement of our approach over Iso-Dream is that it further models the \textit{sparse dependency} of future noncontrollable states on current controllable states (\ie, $s_{t} \rightarrow z_{t+1}$), which is indicated by the green arrows in \fig{fig:intro}. 
%
In practical scenarios, for example, when we program a robot to compete with another one in a dynamic game, the opponent can adjust its policy according to the behavior of our agent.
%
In autonomous driving, when an agent vehicle veers into the lane of other vehicles, typically those vehicles will slow down to avoid a collision.
%
Because of the proposed solution to training collapse, modeling the sparse dependency does not affect the disentanglement learning ability of the world model.
%
In behavior learning, actions are sampled from $\pi(s_t,z_{t:t+\tau})$, where $z_{t+1}\sim p(\cdot|z_t,s_t)$, while due to the sparsity of the cross-branch dependencies, the long-horizon noncontrollable future can be approximated as $z_{t+2:t+\tau}\sim p(\cdot|z_{t+1:t+\tau-1})$.





We evaluate \model{} in the following domains: (i) the CARLA autonomous driving environment in which other vehicles can be naturally viewed as noncontrollable components; (ii) the modified DeepMind Control Suite with noisy video background; (iii) BAIR and RoboNet, the real-world robotic visual forecasting datasets that are helpful to validate the effectiveness of the world model for disentanglement learning. 
%
Our approach outperforms existing approaches by large margins and further achieves significant advantages in transfer learning by isolating dynamics.
%
It can selectively transfer controllable or noncontrollable parts of the learned state transition functions from the source domain to the target domain according to the prior information.


The main contributions of this paper are summarized as follows:
\begin{itemize}[leftmargin=*]
% \vspace{-5pt}
  \item We present a new world model and encourage the decomposition of latent state transitions by optimizing \textit{inverse dynamics}. 
  \myitem We introduce the \textit{min-max variance constraints} to prevent all information from collapsing into a single state transition branch.
  % To further achieve a more accurate state transition of noncontrollable component, we employ a mechanism of sparse interaction.
  \myitem We improve the actor-critic algorithm to make \textit{forward-looking decisions} based on the forecasts of future noncontrollable states.
  \myitem We model the \textit{sparse dependency} of the next-step noncontrollable dynamics on current controllable dynamics to provide a more accurate simulation of some practical dynamic environments.
  \myitem We empirically demonstrate the advantages of \model{} over existing methods in standard, noisy, and \textit{transfer learning} setups.
\end{itemize}
In summary, we extend our previous studies with (i) the min-max variance constraints, (ii) the sparse dependence between the decoupled latent states, and (iii) the transfer learning experiments. 



\section{Problem Overview}

\begin{figure*}[t] 
    \centering
	  \subfloat[]{
       \includegraphics[width=3.2in]{figures/ball_sparse_vis.pdf}}
    \label{fig:ball_sparse_vis} 
    \hfil
	  \subfloat[]{
        \includegraphics[width=2.9in]{figures/car_sparse_vis.pdf}}
    \label{fig:car_sparse_vis}
	\caption{Examples of sparse dependency of the noncontrollable state on the controllable state. (a) A game resembling ice hockey that is played on a desk, in which the ego-agent is controllable, while the opponent robot can be seen as the noncontrollable part, and the hockey puck can be mostly considered to have predictable dynamics independent of the agent's actions. \textit{Sparse dependency} occurs at the moment the agent hits the puck because it may change direction depending on the agent's current state. (b) In autonomous driving, other vehicles (\textit{yellow}) will change their driving directions to avoid collision when the ego-agent (\textit{white}) takes up their driveway.}
	  \label{fig:sparse_vis} 
   % \vspace{-10pt}
\end{figure*}

\subsection{Problem Definition}

In visual control tasks, the agent learns the action policy directly from high-dimensional observations. We formulate visual control as a partially observable Markov decision process (POMDP) with a tuple $(\mathcal S,\mathcal A,\mathcal T,\mathcal R,\mathcal O)$, where $\mathcal S$ is the state space, $\mathcal A$ is the action space, $\mathcal O$ is the observation space, $\mathcal R(s_t, a_t)$ is the reward function, and $ \mathcal T(s_{t+1} \mid s_t, a_t) $ is the state-transition distribution. 
%
At each timestep $t\in[1; T]$, the agent takes an action $a_t \in A$ to interact with the environment and receives a reward $r_t = \mathcal R(s_t, a_t)$. The objective is to learn a policy that maximizes the expected cumulative reward $\mathbb E_p[\sum^T_{\tau=1} r_{\tau}]$.
%
In this setting, the agent cannot access the true states in $\mathcal S$.

\subsection{Key Challenges}
\label{sec:challenges}

\myparagraph{Challenge 1: How to learn future-conditioned policies without the expensive Monte-Carlo planning?}
%
Predicting how the environment will change is useful for long-horizon decision-making in a non-stationary system.
%
A typical solution, such as the cross-entropy method (CEM), is to perform Monte-Carlo sampling over future actions and value the consequences of multiple action trajectories \cite{finn2017deep,ebert2018visual,hafner2019learning}.
% 
However, these planning algorithms are expensive in computational cost, especially when we have large action and state spaces.
%
The question is: \textit{Can we design an RL algorithm that allows for future-conditioned decision-making without playing dice in the action space?}




\myparagraph{Challenge 2: How to avoid ``training collapse'' in unsupervised dynamics disentanglement?} 
%
Despite the great success in unsupervised representation learning \cite{locatello2019challenging,he2020momentum,qian2022unsupervised}, it remains a challenge to disentangle the controllable and noncontrollable dynamic patterns in non-stationary visual scenes. 
% 
One potential solution is to employ modular structures that learn different dynamics in separate branches. However, without proper constraints, the model may suffer from ``training collapse'', where one branch captures all useful information and the other learns almost nothing.
%
This phenomenon may occur when the noncontrollable dynamics components are easy to predict. In this case, we consider adding further constraints to the learning objects of the action-conditioned and action-free state transition branches, encouraging them to isolate the noncontrollable part from the mixed dynamics.

% To avoid this problem, we use the min-max variance constraints to constrain the informative representations.
%In the action-conditioned branch, state transitions are dependent on the action signals. Given different action signals, we maximize the variance of the consequences of state transitions in this branch, which is sensitive to action signals. On the contrary, in the action-free branch, we minimize the variance of the consequences of state transition, encouraging this branch to model the dynamics beyond the control of the agent.



\myparagraph{Challenge 3: How to model situations where the agent behavior has only a sparse/indirect impact on noncontrollable dynamics?}
% 
As we know, in realistic scenarios, the noncontrollable component of the dynamics may not evolve independently but may depend on the motions of the controllable component. 
%
For instance, in \fig{fig:sparse_vis} (a), the hockey puck on the desk (noncontrollable part) changes its direction when the agent (controllable part) hits it. 
%
For autonomous driving, in \fig{fig:sparse_vis} (b), other vehicles (noncontrollable part) will slow down to avoid a collision when the agent (controllable part) takes their lane. 
%
If we assume that our actions do not indirectly affect other vehicles on the road, then for safety reasons, a sub-optimal policy for handling heavy traffic could be to follow the vehicle in front of us instead of changing lanes.
%
To handle this phenomenon, we propose a sparse dependency mechanism that enhances our model's decision-making ability. 
%
The corresponding results are illustrated in \fig{fig:sparse_ablation_vis}.




% \vspace{-5pt}
\subsection{Basic Assumptions}
\label{sec:motivation}
% \vspace{-5pt}

% In our approach, 

In our proposed framework as shown in \fig{fig:intro}, when the agent receives a sequence of visual observations $o_{1:T}$, the underlying spatiotemporal dynamics can be defined as $u_{1:T}$. The evolution of different dynamics can be caused by different forces, but here we aim to decouple $u_{1:T}$ into controllable latent states $s_{1:T}$ and time-varying noncontrollable latent states $z_{1:T}$, such that:
% \vspace{-2pt}
\begin{equation}
\label{eq:basic_assum}
  \begin{aligned}
    u_{1:T} &\sim  (s, z)_{1:T}, \\
    s_{t+1} &\sim p(s_{t+1} \mid s_t, a_t), \\
    z_{t+1} &\sim p(z_{t+1} \mid z_t),
  \end{aligned}
\end{equation}
where $a_t$ is the action signal. 
%
By isolating $s_t$ and $z_t$ to each other, we model their state transitions of $p(s_{t+1} \mid s_t, a_t)$ and $p(z_{t+1} \mid z_t)$ respectively. 
% 
We assume that a more clear decoupling of $s_t$ and $z_t$ can benefit both long-term predictions and decision-making.
%
As an extension of our preliminary work \cite{paniso}, we additionally model the sparse dependency of noncontrollable dynamics on controllable dynamics (as described below). Thus, when a sparse event is detected, the transition of noncontrollable state in \eqn{eq:basic_assum} can be rewritten as $z_{t+1} \sim p(z_{t+1} \mid z_t, s_t)$.




% In our work, we isolate the controllable and noncontrollable states from complex visual dynamics, and then forecast future noncontrollable dynamics by rolling out the noncontrollable states without action, which avoids interacting with the environment.
% 
It assumes that, in long-horizon tasks, the agent can greatly benefit from predicting the consequences of external noncontrollable forces.
%
During behavior learning, we roll out the noncontrollable states and then associate them with the current controllable states for more proactive decision-making.
% 
Therefore, we derive the action policy by
% \vspace{-2pt}
\begin{equation}  
    a_t \sim \pi(a_t \mid s_t, \mathbbm{1} \odot z_{t:t+\tau}),
\end{equation}
where $\mathbbm{1}$ is an indicator according to our prior knowledge about the environment. For example, in autonomous driving, since it is reasonable for the ego-agent to make decisions based on the predictions about the future states of other vehicles, we have $\mathbbm{1}=1$ and calculate the relations between $s_t$ and the imagined noncontrollable states in a time horizon $\tau$. 
%
Otherwise, for some specific tasks where the noncontrollable components are irrelevant to decision-making, we can simply set the indicator function to $\mathbbm{1}=0$ and treat them as noisy distractions.


\section{Method}
% \vspace{-5pt}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/rl_model_and_policy.pdf}
    % \vspace{-5pt}
    \caption{The overall architecture of the world model in \model{}. \textbf{Left:} The world model has three branches to explicitly disentangle controllable and noncontrollable state transitions, as well as the static components from visual data. 
    \textbf{Right:} Illustration of calculating variance in different branches. Given the different action signals, our objective is to minimize the diversity of state transitions in the action-free branch and maximize the diversity of those in the action-conditioned branch. 
   }
    \label{fig:rl_model_and_policy}
    \vspace{-10pt}
\end{figure*}


In this section, we present the technical details of \model{} for decoupling and leveraging controllable and noncontrollable dynamics for visual MBRL. 
% 
The overall pipeline is based on Dreamer \cite{hafner2019dream}, where we learn the world model from a dataset of past experience, learn behaviors from imagined sequences of compact model states, and execute the behavior policy in the environment to grow the experience dataset.


In \sect{sec:inverse_dynamics}, we first introduce the three-branch world model and its training objectives of \textit{inverse dynamics}. 
%
In \sect{sec:variance}, we propose the \textit{min-max variance constraints} to regularize the dynamics representations in each state transition branch to enhance disentanglement learning and avoid training collapse. 
%
In \sect{sec:spare-dependency}, we present a network structure to model the phenomenon that future noncontrollable dynamics can be sparsely influenced by current controllable dynamics.
%
In \sect{sec:behavior-learning}, we present an actor-critic method that is trained on the imaginations of the decoupled world model latent states, so that the agent may consider possible future states of noncontrollable dynamics in behavior learning.
%
Finally, in \sect{sec:Policy-Deployment}, we discuss how our model is deployed to interact with the environment.



% \begin{figure*}[t]
% \begin{center}
% \centerline{
% \subfigure{\includegraphics[width=\columnwidth]{figures/rl_model.pdf}
% \label{fig:world_model}}
% % \hspace{8mm}
% \subfigure{\includegraphics[width=\columnwidth]{figures/imagine.pdf}
% \label{fig:learn_behavior}
% }}
% \vspace{5pt}
% \caption{The overall architecture of the world model and the behavior learning algorithm in \model{}. \textbf{Right:} world model with three branches to explicitly disentangle controllable, noncontrollable, and static components from visual data, where the action-conditioned branch learns controllable state transitions by modeling inverse dynamics. \textbf{Left:} the agent optimizes the behaviors in imaginations of the world model through a future state attention mechanism.}
% \label{fig:model_MBRL}
% \end{center}
% \vspace{-20pt}
% \end{figure*}



\subsection{World Models with Dynamics Isolation}
\label{sec:wm}




Inspired by prior research \cite{slotattention,rim} that demonstrates the efficacy of modular structures for disentanglement learning, we use an architecture with multiple branches to model different dynamics independently, according to their respective physical laws. 
%
Each individual branch tends to present robust features, even when the dynamic patterns in other branches undergo changes.
%
Specifically, our three-branch model, illustrated in the left panel of \fig{fig:rl_model_and_policy}, disentangles visual observations into controllable dynamics state $s_t$, noncontrollable dynamics state $z_t$, and a time-invariant component of the environment.
%
The action-conditioned branch models the controllable state transition $p(s_{t+1} \mid s_t, a_t)$. 
%
It follows the RSSM architecture from PlaNet \cite{hafner2019learning} to use a recurrent neural network $\texttt{GRU}_s(\cdot)$, the deterministic hidden state $h_t$, and the stochastic state $s_t$ to form the transition model, where the GRU keeps the historical information of the controllable dynamics. 
%
The action-free branch models $p(z_{t+1} \mid z_t)$ with similar network structures. The transition models with separate parameters can be written as follows:
\vspace{2pt}
\begin{equation}
  \begin{split}
  p(\tilde{s}_t \mid s_{<t}, a_{<t}) = p(\tilde{s}_t \mid h_t), \\ 
  p(\tilde{z}_t \mid z_{<t}) = p(\tilde{z}_t \mid h_t^\prime), \\ 
  \end{split}
\end{equation}
where $ h_t = \texttt{GRU}_s(h_{t-1}, s_{t-1}, a_{t-1}), \ h_t^\prime = \texttt{GRU}_z(h_{t-1}^\prime, z_{t-1})$.
We here use $\tilde{s}_t$ and $\tilde{z}_t$ to denote the prior representations.
%
We optimize the transition models with posterior representations that are derived from $s_t \sim q(s_t \mid h_{t}, o_t, a_{t-1})$ and $z_t \sim q(z_t \mid h_{t}^\prime, o_t, a_{t-1})$. 
%
We learn the posteriors from the observation at current time step $o_t \in \mathbb{R}^{3 \times H \times W}$ by a shared encoder $\texttt{Enc}_{\theta}$ and subsequent branch-specific encoders $\texttt{Enc}_{\phi_1}$ and $\texttt{Enc}_{\phi_2}$.
%
Notably, we feed actions into both $\texttt{Enc}{\phi_1}$ and $\texttt{Enc}{\phi_2}$, which differs from our previous work \cite{paniso}. In the static branch, where there is no state transition, we only use an encoder $\texttt{Enc}{\phi_3}$ and a decoder $\texttt{Dec}{\varphi_3}$ to model simple time-invariant information in the environment.


\subsubsection{Inverse Dynamics}
\label{sec:inverse_dynamics}
To improve the disentanglement of the representation learning that corresponds to control signals, we introduce the training objective of \textit{inverse dynamics}. 
%
This objective aims to encourage the action-conditioned branch to learn a more deterministic state transition based on specific actions, while the action-free branch will learn the remaining noncontrollable dynamics independent of the control signals.
% 
Accordingly, we design an \textit{Inverse Cell} of a $2$-layer MLP to infer the actions that lead to certain transitions of the controllable states:
\vspace{1pt}
\begin{equation}
    \label{eq:inverse}
    \text{Inverse dynamics:} \quad \tilde{a}_{t-1}=\texttt{MLP}(s_{t-1}, s_t),
\end{equation}
where the inputs are the posterior representations in the action-conditioned branch.
%
By learning to regress the true behavior $a_{t-1}$, the Inverse Cell facilitates the action-conditioned branch to isolate the representation of the controllable dynamics.
%
We respectively use the prior state $\tilde{s}_t$ and the posterior state ${z}_t$ to generate the controllable visual component $\hat{o}_t^s \in \mathbb{R}^{3 \times H \times W}$ with mask $M^s_t \in \mathbb{R}^{1 \times H \times W}$ and the noncontrollable component $\hat{o}_t^z \in \mathbb{R}^{3 \times H \times W}$ with $M^z_t \in \mathbb{R}^{1 \times H \times W}$. 
%
By further integrating the static information extracted from the first $K$ frames, we have
\begin{equation}  
% \vspace{-5pt}
    \label{eq:combine_rl}
     \hat{o}_{t} = M_{t}^s \odot \hat{o}_t^s + M_{t}^z \odot \hat{o}_t^z +  (1-M_{t}^s - M_{t}^z) \odot \hat{o}^b,
\end{equation}
where $\hat{o}^b = \texttt{Dec}_{\varphi_3}(\texttt{Enc}_{\theta, \phi_3}(o_{1:K})))$.


For reward modeling, we have two options concerning the action-free branch. First, we may regard noncontrollable dynamics as irrelevant noises that do not contribute to the task and therefore do not involve $z_t$ in imagination.
%
In other words, the policy and predicted rewards would solely rely on controllable states, \textit{e.g.}, $p(r_t \mid s_t)$.  
%
Alternatively in other cases, we need to consider the influence of future noncontrollable states on the agent's decision-making process and incorporate the action-free components during behavior learning.
%
To achieve this, we train the reward predictor to model $p(r_t \mid s_t, z_t)$ in the form of MLPs.


For a training sequence of $(o_t, a_t, r_t)_{t=1}^T$ sampled from the replay buffer, the world model can be optimized using the following loss functions, where $\alpha$, $\beta_1$, and $\beta_2$ are hyper-parameters:
% \vspace{-4pt}
\begin{equation}
\label{eq:loss}
\begin{split}
\mathcal{L}_\text{base} = &\mathbb{E} \ \{
\sum_{t=1}^{T} \underbrace{-\ln p(o_{t} \mid h_{t}, s_{t}, h^\prime_t, z_{t})}_{\text {image log loss }}  +\underbrace{\alpha \ell_2(a_t, \tilde{a}_{t})}_{\text {action loss }}\\
&\underbrace{-\ln p(r_{t} \mid h_{t}, s_{t}, h^\prime_t, z_{t})}_{\text {reward log loss }} 
 \underbrace{- \ln p(\gamma_{t} \mid h_{t}, s_{t}, h^\prime_t, z_{t})}_{\text {discount log loss }}  \\ 
 &+\underbrace{\beta_1 \mathrm{KL}[q(s_{t} \mid h_{t}, o_{t}) \mid p(s_{t} \mid h_{t})]}_{\mathrm{KL} \text { divergence in the action-conditioned branch }} \\ &+\underbrace{\beta_2 \mathrm{KL}[q(z_{t} \mid h^\prime_{t}, o_{t}) \mid p(z_{t} \mid h^\prime_{t})]}_{\mathrm{KL} \text { divergence in the action-free branch }}\}.
\end{split}
\end{equation}

\vspace{-4pt}

% The world model training approach can be partly customized for different environments. 
% %
% In situations where noncontrollable states are indeed involved in behavior learning, minimizing the ELBO objective can maintain the semantics of $\tilde z_t$. 
% %
% Otherwise, if the action-free features are only used to prevent noisy distractions from affecting the training process of \model{}, rather than being used for behavior learning, we can simply train the action-free branch with the reconstruction loss alone.




\subsubsection{Training Collapse and Min-Max Variance Constraints}
\label{sec:variance}




\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/collapse_nips.pdf}
    % \vspace{-5pt}
    \caption{A showcase of training collapse that the action-conditioned branch in the original version of Iso-Dream dominates the learning process of both controllable and noncontrollable dynamics. The corresponding results of \model{} are shown in \fig{fig:dmc-visual}.}
    \label{fig:collapse}
    \vspace{-5pt}
\end{figure}



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{iso_journal/figures/action_cross_loss.pdf}
%     % \vspace{-5pt}
%     \caption{Illustration of calculating variance in different branches. Given the different action signals, our objective is to minimize the diversity of state transitions in the action-free branch and maximize the diversity of those in the action-conditioned branch.
%    }
%     \label{fig:action_cross_loss}
%     \vspace{-10pt}
% \end{figure}




Despite modeling inverse dynamics, for the original Iso-Dream, we find that it is still challenging for the world model to isolate controllable and noncontrollable dynamics. 
%
We observed in the preliminary experiments that the disentanglement results were unstable over multiple runs of world model training, and that the action-conditioned branch occasionally learned mismatched representations of noncontrollable state transitions.
%
An example is shown in \fig{fig:collapse}.
%
It implies that most useful information may collapse into the action-conditioned branch while the action-free branch learns almost nothing, which we call ``\textit{training collapse}''.
%
This phenomenon occurs because the training objective of inverse dynamics cannot guarantee to completely exclude action-independent state transitions, especially when the action-conditioned network branch has a strong dynamics modeling capacity.




To keep the state transition branches from training collapse, we propose the \textit{min-max variance constraints}, whose key idea is to (i) maximize the diversity of outcomes in the action-conditioned branch given distinct action inputs and (ii) minimize the diversity of outcomes in the action-free branch under similar conditions.
%
To this end, unlike in the original Iso-Dream, we make the action-free branch also aware of the action signal during the world model learning process. But for behavior learning and policy deployment, we simply set the input action to $0$-values. 


There is an information-theoretic interpretation behind calculating variance. In order to investigate the connection between dynamics and action signals, the world model is enforced to identify the dynamics that provide information about our beliefs regarding the action signals. The expected information gain can be expressed as the conditional entropy of state and action:
\begin{equation}  
% \vspace{-5pt}
     I(s_{t}; a_{t-1}{\mid}s_{t-1}) = H(s_{t}{\mid}s_{t-1}) - H(s_{t}{\mid}s_{t-1}, a_{t-1}).
\end{equation}


As shown in \fig{fig:rl_model_and_policy} (right), for the action-conditioned branch, we maximize the mutual information between the state and action signal to focus on the state transition of a specific action.
%
Given a batch of hypothetical actions $\{a_{t-1}^i{\mid}i \in [1, n]\}$, for the same controllable state $s_{t-1}$, we have different state transitions based on these actions:
\begin{equation}  
% \vspace{-5pt}
     \tilde{s}_t^i \sim p(\tilde{s}_t^i {\mid} s_{t-1}, a_{t-1}^i), \ i\in[1, n].
\end{equation}
The empirical variance is used to approximate the information gain, and the objective can be written as
\begin{equation} 
\begin{split}
% \vspace{-5pt}
     L_{s} &= \max \sum\limits_t^{T} \mathrm{Var}(\tilde{s}_t^i) = \max \sum\limits_t^{T} \frac{1}{n-1} \sum\limits_i(\tilde{s}_t^i - \bar{s}_t)^2, \\
     \bar{s}_t &= \frac{1}{n} \sum\limits_i \tilde{s}_t^i, \quad  i\in[1, n].
\end{split}
\end{equation}
On the contrary, in the action-free branch, we minimize the variance of output states resulting from different actions, penalizing the diversity of state transitions:
\begin{equation} 
\begin{split}
% \vspace{-5pt}
     L_{z} &=  \min \sum\limits_t^{T} \mathrm{Var}(\tilde{z}_t^i) =  \min \sum\limits_t^{T} \frac{1}{n-1} \sum\limits_i(\tilde{z}_t^i - \bar{z}_t)^2, \\
     \bar{z}_t &= \frac{1}{n} \sum\limits_i \tilde{z}_t^i, \quad  i\in[1, n].
\end{split}
\end{equation}
The overall training objective of the world model is
\begin{equation} 
\begin{split}
\label{eq:all_loss}
% \vspace{-5pt}
     L_\text{all} = L_\text{base} + L_\text{var},
\end{split}
\end{equation}
where $L_\text{var} = \lambda_1 L_s + \lambda_2 L_z$. $\lambda_1$ and $\lambda_2$ are hyper-parameters.
% the cosine similarity.

For convenience, we only use two opposite actions $\{a_t, -a_t\}$ in the action-conditioned branch, and use the action set $\{a_t, 0, -a_t\}$ in the action-free branch to figure out $L_s$ and $L_z$. As for subsequent learning, we use $a_t$ and $0$ in the action-conditioned and action-free branches, respectively. 
%


\subsubsection{Sparse Dependency between Decoupled States}
\label{sec:spare-dependency}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/sparse_model.pdf}
    % \vspace{-5pt}
    \caption{
    The dependency gate involves a binary gate that can either be open or closed, represented by $w_t=1$ and $w_t=0$ respectively. When the gate is open ($w_t=1$), the transition of the next noncontrollable state $\tilde{z}_{t+1}$ takes into account the dependency between $\tilde{s}_t$ and $\tilde{z}_t$.}
    \label{fig:sparse_model}
    \vspace{-10pt}
\end{figure}


In certain situations, the controllable and noncontrollable dynamics are not entirely independent, as shown in \fig{fig:sparse_vis}. This is particularly true in autonomous driving, where the actions of the ego-agent can influence the behavior of other vehicles, causing them to steer or slow down. To accurately predict future noncontrollable states based on current controllable states, it is essential to account for these sparse dependencies.



To achieve effective modeling of sparse dependency, it is essential to identify the moment when controllable states exert a significant influence on noncontrollable states. 
%
In order to facilitate this, we present a compact module called the \textit{dependency gate}, which connects the previously isolated action-free and action-conditioned branches, as shown in \fig{fig:rl_model_and_policy} (left).
%
We unfold the detailed structure dependency gate in time (see \fig{fig:sparse_model}), where the controllable state $\tilde{s}_t$ and noncontrollable state $\tilde{z}_t$ are concatenated and passed through a fully connected layer represented by $f(\tilde{s}_t,\tilde{z}_t)$. A sigmoid function is then applied as an activation signal to control the gate, which is formulated as
\begin{equation}
\label{sparse_gate}
\delta_t(w_t=1{\mid}\tilde{s}_t, \tilde{z}_t)=\left\{
\begin{aligned}
1,& \quad \mathrm{sigmoid}(f(\tilde{s}_t, \tilde{z}_t)) \geq 0.5, \\
0,& \quad \mathrm{otherwise}.
\end{aligned}
\right.
\end{equation}
When the gate detects a dependency between controllable and noncontrollable states ($w_t = 1$), the next noncontrollable state $\tilde{z}_{t+1}$ is determined by both $\tilde{s}_t$ and $\tilde{z}_t$ using the action-free transition defined as:
\begin{equation}
\label{eq:sparse}
\tilde{z}_{t+1} \sim p(\tilde{z}_{t+1} \mid \tilde{z}_t, w_t \odot \tilde{s}_t)
\end{equation}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/policy.pdf}
    % \vspace{-5pt}
    \caption{Illustration of behavior learning. The agent learns \textit{future-dependent} policies in the imaginations of the world model through a \textit{future state attention} mechanism.
   }
    \label{fig:policy}
    \vspace{-10pt}
\end{figure}


%%%%%%
\begin{algorithm*}[t] 
  \caption{\model{} (Highlight: Our modifications to \textcolor{MyDarkBlue}{behavior learning} \& \textcolor{MyDarkGreen}{policy deployment} of the original Dreamer})  
  \label{alg:algorithm}  
  \small
  \textbf{Hyper-parameters: }{$L$: Imagination horizon; $\tau$: Window size for future state attention}
  \begin{algorithmic}[1]
    \State Initialize the replay buffer $\mathcal{B}$ with random episodes. 
    \While{not converged}
    \For{update step $c = 1 \dots C$}
        \State \texttt{// Representation learning} 
        \State Draw data sequences $\left\{\left(o_{t}, a_{t}, r_{t}\right)\right\}_{t=1}^{T} \sim \mathcal{B}$. 
        \State Compute the controllable state $s_t \sim q(s_t {\mid} h_t, o_t, a_{t-1})$ and the noncontrollable state $z_t \sim q(z_t {\mid} h^{\prime}_t, o_t, a_{t-1})$. 
        \State Compute world model loss using Eq. \eqref{eq:all_loss} and update model parameters. 
        % 
        \State \texttt{// Behavior learning}  
        \For{time step $i = t \dots t+L$}
        \State \textcolor{MyDarkBlue}{Compute the next noncontrollable state $\tilde{z}_{i+1}$ using Eq. \eqref{eq:sparse}.}
        \State \textcolor{MyDarkBlue}{Roll-out the noncontrollable states $\{\tilde z_j\}_{j=i+2}^{i+\tau}$ from $\tilde{z}_{i+1}$ through the action-free branch alone.} 
        \State \textcolor{MyDarkBlue}{Compute latent state $e_i \sim \texttt{Attention}(\tilde s_i, \tilde z_{i: i+\tau})$ using Eq. \eqref{eq:attention}.}
        \State \textcolor{MyDarkBlue}{Imagine an action ${a}_i \sim \pi (a_i {\mid} e_i)$.} 
        \State \textcolor{MyDarkBlue}{Predict the next controllable state $\tilde s_{i+1} \sim p(\tilde s_{i}, {a}_{i})$ using the action-conditioned branch alone.}
        \EndFor
        \State Update the policy and value models in Eq. \eqref{eq:ac-model} using estimated rewards and values.
    \EndFor
    % 
    % 
    \State \texttt{// Environment interaction} 
    \State $o_{1} \leftarrow$ \texttt{env.reset}() %\tcc*[r]{Environment interaction}
    \For{time step $t = 1\dots T$}
    % \textcolor{MyDarkGreen}{
    % $h_t = \texttt{GRU}_s(h_{t-1}, s_{t-1}, a_{t-1}),  h_t^\prime = \texttt{GRU}_z(h_{t-1}^\prime, z_{t-1})$.} \\
    \State Calculate the posterior representation $s_{t} \sim q\left(s_{t} \mid h_t, o_{t}, a_{t-1}\right), z_{t} \sim q\left(z_{t} \mid h_t^\prime, o_{t}, a_{t-1}\right)$. 
    % 
    \State \textcolor{MyDarkGreen}{Compute the next noncontrollable state $\tilde{z}_{t+1} \sim p(\tilde{z}_{t+1}{\mid}z_{t}, w_t \odot s_{t})$ using Eq. \eqref{eq:sparse}.}
    \State \textcolor{MyDarkGreen}{Roll-out the noncontrollable states $\tilde{z}_{t+2:t+\tau}$ from $\tilde{z}_{t+1}$ through the action-free branch alone.}
    \State \textcolor{MyDarkGreen}{Generate $a_t \sim  \pi (a_t \mid s_t, z_t, \tilde{z}_{t+1:t+\tau})$ using future state attention in Eq. \eqref{eq:attention}.} 
    \State $r_t, o_{t+1} \leftarrow$ \texttt{env.step}($a_t$)
    \EndFor
    \State Add experience to the replay buffer $\mathcal{B} \leftarrow \mathcal{B} \cup\{\left(o_{t}, a_{t}, r_{t}\right)_{t=1}^{T}\}$.
    \EndWhile
  \end{algorithmic}  
\end{algorithm*}
%%%%%%%%%%%%%% Alg end



% \texttt{// Environment interaction} \tcc*[r]{xxx}
% \vspace{-3pt}
\subsection{Behavior Learning in Isolated Imaginations}
\label{sec:behavior-learning}
% \vspace{-3pt}


Thanks to the decoupled world model, we can optimize agent behavior to adaptively consider the relationship between available actions and potential future states of the noncontrollable dynamics. 
%
A practical example is autonomous driving, where the movement of other vehicles can be naturally viewed as noncontrollable but predictable components. 
%
As shown in \fig{fig:policy}, we here propose an improved actor-critic learning algorithm that \textit{(i) allows the action-free branch to foresee the future ahead of the action-conditioned branch, and (ii) exploits the predicted future information of noncontrollable dynamics to make more forward-looking decisions}.


Suppose we are making decisions at time step $t$ in the imagination period.
%
A straightforward solution from the original Dreamer method is to learn an action model and a value model based on the isolated controllable state $\tilde{s}_t \in \mathbb{R}^{1 \times d}$. 
%
With the aid of an attention mechanism, we can establish a connection between it and future noncontrollable states. It is important to note that we only employ sparse dependency in the initial imagination step to obtain $\tilde{z}_{t+1}$, as the subsequent controllable states are not yet available at time step $t$. Once we have predicted a sequence of future noncontrollable states $\tilde{z}_{t:t+\tau} \in \mathbb{R}^{\tau \times d}$, where $\tau$ is the sliding window length from the present time, we explicitly compute the relations between them using the following equation:
\begin{equation} 
\begin{split}
    \label{eq:attention}
     e_t = \mathrm{softmax}(\tilde s_t \ \tilde{z}^T_{t:t+\tau}) \ \tilde{z}_{t:t+\tau} + \tilde s_t.
\end{split}
\end{equation}
This equation allows us to dynamically adjust the horizon of future noncontrollable states using the attention mechanism.
%
In this way, $\tilde{s}_t$ evolves to a more ``\textit{visionary}'' representation $e_t \in \mathbb{R}^{1 \times d}$.  We update the action model and the value model in Dreamer \cite{hafner2019dream} as follows:
% \vspace{-5pt}
\begin{equation}
\label{eq:ac-model}
% \vspace{-5pt}
% \renewcommand{\arraystretch}{1.2}
\begin{split}
    \text{Action model:}& \quad {a}_{t} \sim \pi(a_{t} \mid e_t ), \\
    \text{Value model:}& \quad v_{\xi}(e_{t}) \approx \mathbb{E}_{\pi\left(\cdot \mid e_{t}\right)} \sum_{k=t}^{t+L} \gamma^{k-t} r_{k},
\end{split}
\end{equation}
where $L$ is the imagination time horizon. 
%
As shown in \alg{alg:algorithm}, during imagination, we first use the action-free transition model to obtain sequences of noncontrollable states of length $L+\tau$, denoted by $\{\tilde{z}_i \}_{i=t}^{i+L+\tau}$.
%
At each time step in the imagination period, the agent draws an action ${a}_j$ from the visionary state $e_j$, which is derived from Eq. \eqref{eq:attention}. The action-conditioned branch uses the action ${a}_j$ in latent imagination and predicts the next controllable state $s_{j+1}$.
%
We follow DreamerV2 \cite{hafner2020mastering} to train our action model with the objective of maximizing the $\lambda$-return \cite{sutton2018reinforcement}, while our value model was trained to perform regression on the $\lambda$-return. For further information on the loss functions, please refer to Eq. (5-6) as detailed in the paper of DreamerV2 \cite{hafner2020mastering}.


\subsection{Policy Deployment by Rolling out Noncontrollable Dynamics}
\label{sec:Policy-Deployment}

During policy deployment, as shown in Lines 22-24 in Alg. \ref{alg:algorithm}, the action-free branch predicts the next-step noncontrollable states $\tilde{z}_{t+1}$ using Eq. \eqref{eq:sparse} and then consecutively rolls out the future noncontrollable states $\tilde{z}_{t+2:t+\tau}$ starting from $\tilde{z}_{t+1}$. 
%
Similar to Eq. \eqref{eq:attention} used in the process of behavior learning, the learned future state attention network is used to adaptively integrate $s_t$, $z_t$ and $\tilde{z}_{t+1:t+\tau}$. 
%
Based on the integrated feature $e_t$, the \model{} agent then draws $a_t$ from the action model to interact with the environment.
%
As discussed in \sect{sec:challenges}, if the noncontrollable dynamics are irrelevant to the control task, the policy at each time step $t$ is generated using only the state of controllable dynamics when interacting with the environment.


\section{Experiments}

\subsection{Experimental Setup}

\myparagraph{Benchmarks.}
We evaluate the performance of \model{} on two reinforcement learning environments, \textit{i.e.}, CARLA \cite{DBLP:conf/corl/DosovitskiyRCLK17} and DeepMind Control Suite \cite{tassa2018deepmind}, and two real-world datasets for action-conditioned video prediction, \textit{i.e.}, BAIR robot pushing \cite{ebert2017self} and RoboNet \cite{dasari2019robonet}. 
%
% The video prediction experiments can provide more intuitive visualizations of disentanglement learning. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/day_night_vis.pdf}
    % \vspace{-5pt}
    \caption{Examples of day and night modes on CARLA autonomous driving environment.}
    \label{fig:day_night_vis}
    \vspace{-13pt}
\end{figure}


\begin{itemize}[leftmargin=*]
% \vspace{-5pt}
    \item
    \textbf{CARLA} \cite{DBLP:conf/corl/DosovitskiyRCLK17}: An open-source simulator with more complex and realistic visual observations for autonomous driving research. We evaluate our model on a first-person highway driving task in ``Town04'', where the agents goal is to drive as far as possible in $1{,}000$ time steps without colliding with any of the $30$ other moving vehicles or barriers. In this journal paper, we use more complex and diverse settings, including day and night modes, as shown in \fig{fig:day_night_vis}.
    \myitem
    \textbf{DeepMind Control Suite} \cite{tassa2018deepmind}: 
     A set of challenging and diverse continuous control tasks that serve as a standard benchmark for vision-based RL. To evaluate the generalization of our method by disentangling different components under complex visual dynamics, we use two modified benchmarks \cite{hansen2021softda}, namely \texttt{video\_easy}, which contains $10$ simple videos, and \texttt{video\_hard}, which contains $100$ complex videos. 
    \myitem
    \textbf{BAIR Robot Pushing} \cite{ebert2017self}: An action-conditioned video prediction dataset that consists of hours of self-supervised learning with the robotic arm Sawyer.
    %
    Each video shows a random moving robotic arm pushing a variety of objects on similar tables with a static background. 
    %
    The video also records the actions of the robotic arm, which correspond to the commanded gripper pose.
    \myitem
    \textbf{RoboNet} \cite{dasari2019robonet}: A large-scale dataset that contains action-conditioned videos of seven robotic arms interacting with a variety of objects from four different research laboratories, \textit{i.e.}, Berkeley, Google, Penn, and Stanford.
    %
    % We have a training set of 99,360 sequences and a test set of 7,320 sequences.
\end{itemize}


\myparagraph{Compared methods.}
For visual MBRL, we compare our method with the following baselines and existing approaches:
\begin{itemize}[leftmargin=*]
% \vspace{-5pt}
    \item \textbf{DreamerV2 \cite{hafner2020mastering}}: A model-based RL method that learns directly from latent variables in world models. The latent representation allows agents to imagine thousands of trajectories in parallel.
    \myitem \textbf{DreamerPro \cite{deng2022dreamerpro}}: A non-contrastive reconstruction-free model-based RL method that combines Dreamer \cite{hafner2019dream} with prototypes to enhance robustness to distractions.
    \myitem \textbf{CURL \cite{srinivas2020curl}}: A model-free RL method that uses contrastive learning to extract high-level features from raw pixels, maximizing agreement between augmented data of the same observation.
    \myitem \textbf{SVEA \cite{hansen2021stabilizing}}: A framework for data augmentation in deep Q-learning algorithms that improves stability and generalization on off-policy RL.
    \myitem \textbf{SAC \cite{haarnoja2018soft}}: A model-free actor-critic method that optimizes a stochastic policy in an off-policy way.
    \myitem \textbf{DBC \cite{zhang2021learning}}: A method that learns a bisimulation metric representation without reconstruction loss. This representation is invariant to different task-irrelevant details in the observation. 
    \myitem \textbf{Denoised-MDP \cite{wang2022denoised}}: A framework that categorizes information out in the wild into four types based on controllability and relation with reward, and formulates useful information as that which is both controllable and reward-relevant.
\end{itemize}


For video prediction, we compare the proposed world model with the following approaches:
\begin{itemize}[leftmargin=*]
    % \item PredRNN-V2 \cite{wang2021predrnn}: 
    % This model improves the training process of the original PredRNN \cite{wang2017predrnn} with memory decoupling and reverse scheduled sampling. We use it as backbone in our model.
    % \vspace{-5pt}
    \item \textbf{SVG} \cite{denton2018stochastic}: 
    This model introduces random variables into latent space to ensure the inherent randomness of future trajectories.
    \item \textbf{SA-ConvLSTM} \cite{lin2020self}:
    Based on the self-attention mechanism, this model captures long-term spatial dependency using self-attention memory.
    \item \textbf{PhyDNet} \cite{guen2020disentangling}:
    This model uses a two-branch architecture to disentangle PDE dynamics from unknown complementary information.
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/compare_sota_carla.pdf}
    % \vspace{-5pt}
    \caption{Performance comparison with the state-of-the-art on the CARLA driving task. Our approach outperforms the baselines by a large margin. Results are averages over 3 seeds.}
    \label{fig:compare_sota_carla}
    % \vspace{-10pt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_carla_nips.pdf}
    % \vspace{-5pt}
    \caption{Ablation studies of optimizing the inverse dynamics (\textcolor{orange}{orange}), rolling out noncontrollable states (\textcolor{MyDarkGreen}{green}), using the attention mechanism (\textcolor{red}{red}), and modeling the static information with a separate network branch (\textcolor{MyPurple}{purple}).}
    \label{fig:ablation_carla_nips}
    % \vspace{-10pt}
\end{figure}


% To facilitate the discussion of the ablation study, we refer to different versions of \model{} as:
% \begin{itemize}[leftmargin=*]
% % \vspace{-5pt}
%     \item
%     \textbf{Iso-Dream}: This model uses a modular structure to decouple controllable and noncontrollable states by inverse dynamics. 
%     % It was proposed in our conference paper \cite{paniso}.
%     % \item
%     % \textbf{Iso-Dream (\textit{conf.})}: This model only uses reconstruction loss in the action-free branch for the DMC environment, which is proposed in our conference paper \cite{paniso}.
%     \item
%     \textbf{\model{}}: This is the final proposed model that improves
% the performance of the original Iso-Dream with the min-max variance constraints and sparse dependency between decoupled states.
% \end{itemize}



\subsection{CARLA Autonomous Driving Environment}

\begin{figure*}[t] 
    \centering
	  \subfloat[]{
       \includegraphics[width=3.5in]{figures/ablation_sparse_day.pdf}}
    \label{fig:ablation_sparse_day} 
    \hfil
	  \subfloat[]{
        \includegraphics[width=3.5in]{figures/ablation_sparse_night.pdf}}
    \label{fig:ablation_sparse_night}
	  \caption{Ablation studies of the variance constraints (\textcolor{orange}{orange}) for disentanglement, the sparse dependency (\textcolor{MyDarkGreen}{green}) between two dynamics on the day mode (a) and the night mode (b) of CARLA environment. Using the variance constraints and sparse dependency, the overall performance of our model is improved.}
	  \label{fig:ablation_sparse} 
   % \vspace{-10pt}
\end{figure*}

%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sparse_ablation_vis.pdf}
    % \vspace{-5pt}
    \caption{Examples of \model{} with (\textbf{bottom}) and without (\textbf{top}) sparse dependency. In the top row, the agent without sparse dependency tends to follow the vehicle in front of it when there are many vehicles in the way. In the bottom row, the agent overtakes and accelerates flexibly.}
    \label{fig:sparse_ablation_vis}
    % \vspace{-10pt}
\end{figure*}


\myparagraph{Implementation details.}
In the autonomous driving task, We use a camera with a $60$ degree view on the roof of the ego-vehicle, which obtains images of $64 \times 64$ pixels. Following the setting in the DBC \cite{zhang2021learning}, in order to encourage highway progression and penalize collisions, the reward is formulated as $r_t=v^T_{ego}\hat{u}_h \cdot \Delta t -\xi_1 \cdot \mathbbm{1} - \xi_2 \cdot |steer| $, where $v_{ego}$ is the velocity vector of the ego-vehicle, projected onto the highways unit vector $\hat{u}_h$, and multiplied by time discretization $\Delta t = 0.05$ to measure highway progression in meters. Impulse $\mathbbm{1} \in \mathbb{R^+}$ is caused by collisions, and a steering penalty $steer \in [-1,1]$ facilitates lane-keeping. The hyper-parameters $\xi_1$ and $\xi_2$ are set to $10^{-4}$ and $1$, respectively. We use $\beta_1 = \beta_2 = 1$ and $\alpha = 1$ in Eq. \eqref{eq:loss}, $\lambda_1 = \lambda_2 = 1$ in \eqn{eq:all_loss}, and $\tau=5$ in Eq. \eqref{eq:attention}.


\myparagraph{Quantitative comparison with baselines.}
%
\fig{fig:compare_sota_carla} shows the comparison results . Our \model{} outperforms baselines without reconstruction (DreamerPro, Denoised-MDP) or disentanglement (DreamerV2) significantly. 
%
After $500k$ environment steps, \model{} achieves an average performance of $60$, while DreamerV2 and Denoised-MDP achieve only $10$ and $25$ respectively. 
%
In DreamerV2, the latent representations contain both controllable and noncontrollable dynamics, which is complicated for state transitions in imagination. 
% 
Compared with Denoised-MDP, which also decouples information according to the controllability, our method has the advantage to make forward-looking decisions by rolling-out the future noncontrollable states.
%
The empirical success of our model demonstrates that isolating controllable and noncontrollable states facilitates the representation learning, and forecasting the future noncontrollable dynamics improve the behavior learning.

\begin{figure*}[t]
\begin{center}
\vspace{-5pt}
\centerline{\includegraphics[width=\linewidth]{figures/carla_showcases.pdf}}
% \vspace{-5pt}
\caption{Video prediction results on the CARLA environment. For each sequence, we use the first $5$ images as context frames. The visual decoupled components (Lines 3, 5, 7) and masks (Lines 4, 6, 8) of each branch are presented. \model{} successfully isolates noncontrollable dynamics from the complicated environment, \textit{i.e.}, other driving vehicles.}
\label{fig:carla-visual}
\end{center}
\vskip -0.3in
\end{figure*}



\myparagraph{Ablation study on all the proposed techniques.}
%
\fig{fig:ablation_carla_nips} shows ablation studies to confirm the validity of inverse dynamics, the rolling-out strategy of noncontrollable states, the attention mechanism, and the modeling of static information.
%
Removing the Inverse Cell reduces the performance of our model which indicates the importance of isolating controllable and noncontrollable components using inverse dynamics.
%
We give up the rolling-out strategy (green curve) and observe that rolling-out noncontrollable states in the action-free branch significantly improves the agent's decision-making results by perceiving potential risks in advance.
%
Moreover, we evaluate \model{} without attention mechanism where the policy network concatenates the current controllable state with multiple steps of the predicted noncontrollable state as input. The attention mechanism (blue curve) extracts valuable information from future noncontrollable dynamics better than concatenation (red curve).
% 
The brown curve shows that our approachs performance decreases by about $15\%$ without a separate network branch for capturing the static information.
%
Additionally, in \fig{fig:ablation_sparse}, we verify the techniques of variance constraints and sparse dependency on the day and night modes of the CARLA environment.
%
From the results of the two modes, we observe that these techniques further improve the performance of our model.  



\myparagraph{Analyses on ``sparse dependency''.}
%
As shown in \fig{fig:ablation_sparse}, the sparse dependency can benefit policy learning. To clarify this point further, we show visual examples with and without sparse dependency in \fig{fig:sparse_ablation_vis}. 
%
Without sparse dependency (top row), the agent fails to predict that other vehicles will slow down or brake when changing lanes, making it safer to follow the vehicle ahead rather than overtake it during traffic congestion.
%
However, as shown in the bottom row of \fig{fig:sparse_ablation_vis}, the agent can decide whether to overtake or not based on its surroundings. 
%
These results indicate that sparse dependency greatly models the situation that the noncontrollable dynamics are affected by the controllable dynamics, which is conducive to the downstream decision-making task by accurately predicting the noncontrollable dynamics at future moments. 


\myparagraph{Qualitative results.}
%
Reconstruction results of predictions in the CARLA environment are shown in \fig{fig:carla-visual}. 
%
Because of the first-person view in this environment, the agent actions potentially affect all pixel values in the observation, as the camera on the main car (\textit{i.e.}, the agent) moves. Therefore, we can view the dynamics of other vehicles as a combination of controllable and non-controllable states. 
%
Accordingly, our model can determine which component is dominant by learning attention masks (values between $0$ and $1$) across the action-conditioned and action-free branches. The ``action-free masks'' present hot spots around other vehicles, while the attention values in corresponding areas on the ``action-cond masks'' are still greater than zero.
%
As shown in the third and fifth lines, \model{} mainly learns the dynamics of mountains and trees in the action-conditioned branch and the dynamics of other driving vehicles in the action-free branch, respectively, which helps the agent avoid collisions by rolling-out noncontrollable components to preview possible future states of other vehicles.


\begin{figure*}[t]
\begin{center}
\vspace{-5pt}
\centerline{\includegraphics[width=\linewidth]{figures/dmc_showcases.pdf}}
% \vspace{-5pt}
\caption{Video prediction results with noisy backgrounds on the DMC. For each sequence, we use the first $5$ images as context frames. \model{} successfully disentangles controllable and noncontrollable components.}
\label{fig:dmc-visual}
\end{center}
\vskip -0.3in
\end{figure*}


\begin{table*}[t]
  \centering
  \caption{Performance of visual control tasks in the DMC Suite. The agents are trained and evaluated in environments with \texttt{video\_easy} dynamic background. We report the mean and standard deviation of final performance over $3$ seeds and $5$ trajectories. *We use a different setup from that in the paper of DBC. Iso-Dream (\textit{conf.}): This model only uses reconstruction loss (without $KL$ divergence) in the action-free branch for the DMC environment, which is proposed in our conference paper \cite{paniso}.}
  \label{tab:dmc_result}
  \setlength\tabcolsep{4.8pt}
    \vskip -0.2in
    \begin{center}
    % \begin{small}
    % \begin{sc}
    \begin{tabular}{l|ccccccccc}
    \toprule
    Task  & SVEA  & CURL & DBC* & DreamerV2  & DreamerPro  & Denoised-MDP & Iso-Dream (\textit{conf.}) & Iso-Dream & Iso-Dream w/ $L_{var}$ \\
    \midrule
    Finger Spin & 562 $\pm$ 22 & 280 $\pm$ 50 &  1 $\pm$ 2  & 755 $\pm$ 92  & 721 $\pm$ 147 & 635 $\pm$ 284 & 800 $\pm$ 59 & 816 $\pm$ 16 &\textbf{ 938 $\pm$ 51} \\
    Hopper Stand & 6 $\pm$ 8 & 451 $\pm$ 250  &  5 $\pm$ 9 & 260 $\pm$ 366   & 295 $\pm$ 129 & 104 $\pm$ 117 & 746 $\pm$ 312 & 769 $\pm$ 173 & \textbf{877 $\pm$ 34} \\
    Walker Walk & 826 $\pm$ 65 & 443 $\pm$ 206   &  32 $\pm$ 7 & 655 $\pm$ 47  & 813 $\pm$ 88 & 214 $\pm$ 56 & 911 $\pm$ 50 & 852 $\pm$ 97 & \bfseries 932 $\pm$ 37 \\
    Cheetah Run & 178 $\pm$ 64 & 269 $\pm$ 24   & 15 $\pm$ 5 & 475 $\pm$ 159  & 297 $\pm$ 63 & 233 $\pm$ 119 & \textbf{659 $\pm$ 62} & 597 $\pm$ 156 & \textbf{639 $\pm$ 19} \\
    \bottomrule
    \end{tabular}
    % \end{sc}
% \end{small}
\end{center}
\end{table*}



\subsection{DeepMind Control Suite}
\label{expri:DMC}

% \begin{table}[t]
%   \centering
%   \caption{Performance of visual control tasks in the DMC Suite. The agents are trained and evaluated in environments with \texttt{video\_easy} dynamic background. We report the mean and std of final performance over $3$ seeds and $5$ trajectories. *We use a different setup from that in the paper of DBC. VC means the variance constraint.}
%   \label{tab:dmc_result}
%     % \vskip 0.15in
%     \setlength\tabcolsep{5pt}
%     \begin{center}
%     \begin{tabular}{l|ccccccc}
%     \toprule    
%     \multirow{2}{*}{Method}  &  Walker  & Cheetah & Finger & Hopper   \\
%     & Walk & Run & Spin & Stand \\
%     \midrule
%     SVEA & 826 $\pm$ 65 & 178 $\pm$ 64 & 562 $\pm$ 22 & 6 $\pm$ 8 \\
%     CURL & 443 $\pm$ 206 & 269 $\pm$ 24 & 280 $\pm$ 50 & 451 $\pm$ 250 \\
%     DBC* & 32 $\pm$ 7 & 15 $\pm$ 5 & 1 $\pm$ 2 & 5 $\pm$ 9 \\
%     DreamerV2  & 655 $\pm$ 47 & 475 $\pm$ 159 & 755 $\pm$ 92 & 260 $\pm$ 366 \\
%     DreamerPro  & 813 $\pm$ 88 & 297 $\pm$ 63 & 721 $\pm$ 147 & 295 $\pm$ 129 \\
%     Denoised-MDP & 214 $\pm$ 56 & 233 $\pm$ 119 & 635 $\pm$ 284 & 104 $\pm$ 117 \\
%     \midrule
%     \model{} (\textit{conf.}) & 911 $\pm$ 50 & 659 $\pm$ 62 & 800 $\pm$ 59 & 746 $\pm$ 312 \\
%     \model{} & 852 $\pm$ 97 & 597 $\pm$ 156 & 767 $\pm$ 61 & 842 $\pm$ 65 \\
%     \model{} + VC  & \bfseries{932 $\pm$ 37} & \textbf{639 $\pm$ 19} &\textbf{ 938 $\pm$ 51} & \textbf{877 $\pm$ 34} \\
%     \bottomrule
%     \end{tabular}
% \end{center}
% \end{table}


\myparagraph{Implementation details.}
We evaluate our model on \texttt{video\_easy} \texttt{video\_hard} benchmarks from DMC Generalization Benchmark, where the background is continually changing throughout an episode. 
%
All experiments use visual observations only, of shape $64 \times 64 \times 3$. The episodes last for $1{,}000$ steps and have randomized initial states. We apply a fixed action repeat of $R = 2$ across tasks. 
% We run every experiment with three different random seeds with standard deviation shown in shaded region.
%   
In this environment, since the background is randomly replaced by a real-world video, the noncontrollable motion of the background will affect the procedure of dynamics learning and behavior learning of agents.
% 
Therefore, to obtain a better decision policy and avoid the disruption from noisy backgrounds, the agent may decouple noncontrollable representation (\textit{i.e.}, dynamic background) and controllable representation in spacetime, and only use controllable representation for control.
%
Instead of training the action-free branch with only reconstruction loss in our preliminary work \cite{paniso}, we follow the structure described in \sect{sec:wm} since the noncontrollable dynamics in some video backgrounds are complicated for learning, particularly \texttt{video\_hard} benchmark.
%
We evaluate our model with baselines in $4$ tasks from four different domains, \textit{i.e.}, Finger Spin, Cheetah Run, Walker Walk, and Hopper Stand. The number of environmental steps is limited to $500$k. We use $\beta_1 = \beta_2 = 1$ and $\alpha = 1$ in Eq. \eqref{eq:loss} and $\lambda_1 = \lambda_2 = 1$ in Eq. \eqref{eq:all_loss}.


\myparagraph{Quantitative comparison with baselines} 
% 
We compare our method with baselines in \texttt{video\_easy} benchmark, and the results are shown in \tab{tab:dmc_result}. 
%
Our final model outperforms DreamerV2 and other baselines significantly in all tasks. 
% 
Compared with DBC and Denoised-MDP, which both aim to extract task-relevant representation from complex visual distractions, our method is more powerful with large performance gains on all four tasks, indicating that disentangling different dynamics by modular structure and variance constraints provides more cleaner and useful information for downstream task.
%
Moreover, we have a better performance than DreamerPro, which is also based on Dreamer but learns the world model without reconstructing the observations. 
%
This demonstrates that our model effectively helps the agent to learn controllable visual representations and alleviate complex background interference.
%

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_variance_dmc.pdf}
    % \vspace{-5pt}
    \caption{Ablation study on variance constraints run for $3$ seeds on each of the $4$ selected DMControl environments. In all tasks, the technique of variance constraints improves the performance of our model.}
    \label{fig:ablation_variance_dmc}
    % \vspace{-10pt}
\end{figure*}
% 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_variance_dmc_vis.pdf}
    % \vspace{-5pt}
    \caption{Video prediction results of our model with and without variance constraints. variance constraints further enhances the decoupling capability.
   }
    \label{fig:ablation_variance_dmc_vis}
    \vspace{-15pt}
\end{figure}


\myparagraph{Qualitative results.}
% 
We leverage our model to complete video prediction tasks in \texttt{video\_easy} environments. The sequence of frames and actions is randomly collected during test episodes. The first $5$ frames are given to the model and the next $45$ frames are predicted only based on action inputs. 
%
In this environment, the video background can be viewed as a combination of noncontrollable dynamics and static representations.
% 
\fig{fig:dmc-visual} shows the qualitative results, where we visualize the masks and decoupled components from three branches. 
%
From this prediction result, we can find that our approach has the ability to predict long-term sequence and disentangle controllable (agent) and noncontrollable dynamics (background motion) from complex visual images. As shown in the third and fourth rows of action-conditioned branch output in \fig{fig:dmc-visual}, the controllable representation has been successfully isolated and matches its mask. 
% 
Moreover, the motion of fires and sea waves are captured as noncontrollable dynamics by the action-free branch, as shown in the fifth and sixth rows.



\myparagraph{Analyses of the min-max variance constraints.}
% 
% 
We investigate the effectiveness of the proposed variance constraints described in \sect{sec:variance}. As shown in \fig{fig:ablation_variance_dmc}, this technique improves the performance of our model across all tasks. We visualize the disentanglement results with and without variance constraints in \fig{fig:ablation_variance_dmc_vis}. 
% 
Comparing the fifth and sixth row of action-free branch outputs, we observe that the action-free dynamics (such as the light over the lake) are correctly assigned to the action-free branch by variance constraints, preventing the action-conditioned branch from capturing all dynamic information, \textit{i.e.}, training collapse. Because of the pure dynamics captured in the action-conditioned branch, our model with variance constraints gains definite improvements.


\subsection{Transfer learning}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/day_to_night.pdf}
    % \vspace{-5pt}
    \caption{Performance comparison on the night mode between \model{} with and without loading action-free branch that is pretrained on the day mode. Results are averaged over $3$ seeds.}
    \label{fig:day_to_night}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/easy_to_hard_two_tasks.pdf}
    \vspace{-5pt}
    \caption{Comparison results of using pretrained decoupled module on \texttt{video\_hard} benchmark. The performance gain of \model{} that load action-conditioned branch pretrained on \texttt{video\_easy} benchmark (\textcolor{orange}{orange}) is much greater than DreamerV2 that load all modules pretrained on \texttt{video\_easy} benchmark (\textcolor{red}{red}). }
    \label{fig:easy_to_hard_two_tasks}
    % \vspace{-10pt}
\end{figure*}


\myparagraph{Transfer of noncontrollable dynamics in CARLA.}
% 
Our model learns different dynamics in different branches, which makes it naturally suitable for transfer learning. Unlike common methods that transfer all knowledge from a pretrained source task, we can selectively transfer specific knowledge for a target task. 
% 
Specifically, we only transfer relevant knowledge, such as shared dynamics between source and target tasks, to achieve precise disentanglement and robust decision-making on the target task.
% 
In \fig{fig:day_night_vis}, We can see that the noncontrollable dynamics are similar between day and night modes, \textit{i.e.}, the movement of other driving vehicles.
%
We keep the action-free branch pretrained on the day mode of the CARLA environment, and then train it on the night mode. The results are shown in \fig{fig:day_to_night}.
%
Comparing the orange curve and the blue curve, our model that transfers noncontrollable dynamics in the action-free branch has a significant improvement. However, the performance gain of DreamerV2 is small. 
%
Therefore, due to the modular structure in our \model{}, when there are two environments with similar dynamics, we can train on the easy environment first, and then load the specific pretrained branch to help the model learn on difficult tasks.

Therefore, the modular structure of our \model{} allows us selectively transfer controllable or noncontrollable parts to novel domains based on our prior knowledge of the domain gap.


\myparagraph{Transfer of controllable dynamics in DMC.}
%
Likewise, we use \texttt{video\_easy} (source) and \texttt{video\_hard} (target) benchmarks of the DMC environment for transfer learning.
%
We transfer the controllable information in the action-conditioned branch because the controllable dynamics are the same in both environments, \textit{i.e.}, the motion of the agent.
%
\fig{fig:easy_to_hard_two_tasks} presents the comparison results in \model{} and DreamerV2. We can see that, first, loading the pretrained action-conditioned branch, \model{} has a strong advantage over that without pretraining.
% 
Second, the performance gain of our model from pretraining is much greater than that of DreamerV2.

\subsection{Action-Conditioned Video Prediction}

\myparagraph{Implementation details.}
In order to evaluate the effectiveness of our world model in a more complex environment, we test the video prediction ability of the proposed structure on the BAIR and RoboNet datasets. Moreover, we add predictable visual dynamics unrelated to the control signals to the raw observations, \textit{i.e.}, bouncing balls of the same size and speed.   
In the training phase,  we train the model to predict $10$ frames into the future from $2$ observations. For testing, we use the first $2$ frames as input to predict the next $28$ frames in the BAIR dataset, and the next $18$ frames in the RoboNet dataset. 
%
All inputs for training and testing are resized to $64\times64$.
Considering the simplicity and predictability of bouncing balls, in the action-free branch, we use a similar structure as in the DMC experiment. Moreover, we replace the GRU cell with two layers of ST-LSTM unit \cite{wang2017predrnn} in both branches.
%
The optimization objective consists of image reconstruction loss and action reconstruction loss of the Inverse Cell.
SSIM and PSNR are adopted as evaluation metrics.






\begin{table}[t]
\caption{Video prediction results on BAIR and RoboNet datasets with bouncing balls. We use the first $2$ frames as input to predict the next $28$ frames on BAIR and the next $18$ frames on RoboNet.} 
\label{tab:Comparing_bair_robonet}
% \vskip 0.15in
\begin{center}
\begin{tabular}{l|cccc}
\toprule
\multirow{2}{*}{Model} 
& \multicolumn{2}{c|}{BAIR} & \multicolumn{2}{c}{RoboNet} \\  
& PSNR $\uparrow$  & \multicolumn{1}{c|}{SSIM $\uparrow$}  &  PSNR $\uparrow$  & SSIM $\uparrow$  \\ 
\midrule
% \hline
SVG ~\cite{denton2018stochastic}  & 18.12 & \multicolumn{1}{c|}{0.712} & 19.86  & 0.708 \\
SA-ConvLSTM ~\cite{lin2020self}  & 18.28  &   \multicolumn{1}{c|}{0.677}  & 19.30 & 0.638 \\
PhyDNet ~\cite{guen2020disentangling}  & 18.91   & \multicolumn{1}{c|}{0.743}  & 20.89  & 0.727\\
\model{} & \bfseries 19.51 & \multicolumn{1}{c|}{\bfseries 0.768}  & \bfseries 21.71 & \bfseries 0.769   \\ 
\bottomrule
\end{tabular}
\end{center}
\vskip -0.2in
\end{table}


\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{figures/histogram_18.pdf}}
\vspace{-3pt}
\caption{The results of models trained on BAIR (blue) and BAIR + bouncing balls (red), and tested on BAIR. We use the first $2$ frames as input to predict the next $18$ frames. The horizontal axis represents the different models, and the vertical axes represent the test results of PSNR and SSIM.}
\label{fig: the results of training with bouncing balls and testing without bouncing balls}
\end{center}
\vspace{-10pt}
\end{figure*}


\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figures/bair_minting.pdf}}
\vspace{-3pt}
\caption{Showcases of video prediction results on the BAIR robot pushing dataset. We display every $3$ frames in the prediction horizon. The generated masks show that each branch of \model{} captures coarse localization of controllable representations and noncontrollable representations.}
\label{fig:BAIR-visual}
\end{center}
\vskip -0.2in
\end{figure*}



\myparagraph{Quantitative results.}
%
Table~\ref{tab:Comparing_bair_robonet} gives the quantitative results on BAIR and RoboNet with bouncing balls in the training and testing phase. Compared with other models, \model{} shows the competitive performance in two datasets.
%
For PSNR, \model{} improves SVG by $7.7\%$ in BAIR and $9.3\%$ in RoboNet. Compared with PhyDNet, which also disentangles features in two branches, \model{} achieves better performance in both PSNR and SSIM.
It shows that our \model{} has a stronger ability of disentanglement learning to achieve long-term prediction. 
%Please refer to the supplementary materials for more empirical analyses of the proposed world model on the action-conditioned video prediction datasets.
%
Moreover, \fig{fig: the results of training with bouncing balls and testing without bouncing balls} shows an interesting result of the different training sets (\textit{i.e.}, BAIR, BAIR+bouncing balls) and the same testing set (\textit{i.e.}, BAIR). 
% After training in BAIR with bouncing balls, \model{} can improve the performance of training in BAIR.
%
\model{} is the only approach that achieves improvements when training on noisy data with bouncing balls, as shown in \fig{fig: the results of training with bouncing balls and testing without bouncing balls}(red bars). In this training setup, it performs best on the standard test set without balls. \model{} is built on a more efficient architecture than the baseline models. It
provides a general framework that can be easily extended to
other backbones. 



\myparagraph{Ablation study.}
%
% \myparagraph{Ablation study.}
In Table~\ref{tab:Ablation_study_for_bair}, the first row shows the results of removing the action-free branch in the world model of \model{}. The performance has decreased from $21.43$ to $20.47$ and from $19.51$ to $18.51$ in PSNR for predicting the next 18 frames and next 28 frames respectively, indicating that modular network structures are effective for predictive learning by decoupling the controllable and noncontrollable representations. Comparing the second row and third row in Table~\ref{tab:Ablation_study_for_bair}, we observe that modeling inverse dynamics can improve performance by learning more deterministic state transitions given particular actions in the action-conditioned branch.



\begin{table}[t]
\caption{Ablation study for each component of \model{} for video prediction on BAIR with bouncing balls. Lines 2-3 show the results of removing the action-free branch and Inverse cell, respectively. We use the first $2$ frames as input to predict the next $18$ frames and the next $28$ frames.} 
\label{tab:Ablation_study_for_bair}
% \setlength\tabcolsep{5pt} 
\begin{center}
\begin{tabular}{l|cccc}
\toprule
\multirow{2}{*}{Model} 
& \multicolumn{2}{c|}{18 frames} & \multicolumn{2}{c}{28 frames} \\  
& PSNR $\uparrow$  & \multicolumn{1}{c|}{SSIM $\uparrow$}  &  PSNR $\uparrow$  & SSIM $\uparrow$  \\ 
\midrule
\model{} & \bfseries 21.43 & \multicolumn{1}{c|}{\bfseries 0.832}  & \bfseries 19.51 & \bfseries 0.768   \\ 
w/o Action-free branch & 20.47  & \multicolumn{1}{c|}{0.795}  & 18.51 & 0.690  \\ 
w/o Inverse cell & 21.42   & \multicolumn{1}{c|}{0.829}  &  19.34 & 0.759  \\ 
\bottomrule
\end{tabular}
\end{center}
\end{table}


\myparagraph{Qualitative results.}
%
We visualize a sequence of predicted frames on BAIR with bouncing balls in \fig{fig:BAIR-visual}. Specifically, the output of two branches and corresponding masks are provided. We can see from these demonstrations that the world model of \model{} is more accurate in modeling future dynamics for long-term prediction. It shows the fact that the action-free branch learns noncontrollable dynamics, while the action-conditioned branch learns controllable dynamics related to input action. 


\section{Related Work}


\subsection{Visual MBRL}

In visual control tasks, the agents have to learn the action policy directly from high-dimensional observations.
% 
They can be roughly grouped into two categories, that is, model-free methods \cite{haarnoja2018soft,srinivas2020curl,yarats2019improving,kostrikov2020image,laskin2020reinforcement,hansen2021stabilizing} and model-based methods \cite{finn2017deep,oh2017value,ha2018world,hafner2019learning,hafner2019dream,kaiser2020model,sekar2020planning,zhang2021learning,bharadhwaj2022information,wang2022denoised,deng2022dreamerpro,xu2022RewardFree,ji2022update}.
%
% The model-free methods optimize the RL objective together with some auxiliary loss \cite{srinivas2020curl, yarats2019improving}, or improve performance and sample efficiency or generalization ability of agents through data augmentation \cite{kostrikov2020image, laskin2020reinforcement, hansen2021stabilizing}.
%
Among them, the MBRL approaches explicitly model the state transitions and generally yield higher sample efficiency than the model-free methods.
%
Ha and Schmidhuber \cite{ha2018world} proposed the World Models that first learn compressed latent states of the environment in a self-supervised manner, and then train the agent on the latent states generated by the world model.
%
Following the two-stage training procedure, PlaNet \cite{hafner2019learning} uses an action-conditioned, \textit{recurrent state-space model} (RSSM) as the world model, and optimizes the action policy on the recurrent states with the cross-entropy methods. 
%
In Dreamer \cite{hafner2019dream} and DreamerV2 \cite{hafner2020mastering}, agents learn behaviors by optimizing the expected values over the predicted latent states in RSSM. 



However, for complex visual environments with background or even dynamic distractions, the agent fails to learn effective behavior policies. 
%
To tackle this problem, some approaches \cite{zhang2021learning,bharadhwaj2022information,wang2022denoised,deng2022dreamerpro} learn a more robust representation by discarding pixel-reconstruction to avoid struggling with the presence of visual noises. 
%
DreamerPro \cite{deng2022dreamerpro} uses online clustering to learn prototypes from the recurrent states of the world model, eliminating the need for reconstruction. 
%
Denoised-MDP \cite{wang2022denoised} categorizes system dynamics into four types based on their controllability and relation to rewards, and optimizes the policy model only with information that is both controllable and relevant to rewards.
%
It is worth noting that \model{} differs significantly from the aforementioned methods in two key ways. 
%
First, we explicitly model the state transitions of controllable and noncontrollable dynamics in two distinct branches. 
%
This modular structure empirically facilitates transfer learning between related but distinct domains.
%
Second, the decoupled world model offers a more versatile method of learning behavior. By previewing possible future states of noncontrollable patterns, we can make informed decisions at present. This also allows us to choose whether or not to incorporate noncontrollable states into our decision-making process, based on our prior knowledge of the specific domain.

% \pmt{\myparagraph{DBC} DBC \cite{DBLP:conf/iclr/0001MCGL21} proposes learning an invariant representation which encode only the task-relevant information from observations using the bisimulation metric without pixel-reconstruction.}


\subsection{Action-Conditioned Video Prediction}

A straightforward deep learning solution to visual control problems is to learn action-conditioned video prediction models \cite{oh2015action,Finn2016Unsupervised,chiappa2017recurrent,wang2021predrnn,babaeizadeh2021fitvid} and then perform Monte-Carlo importance sampling and optimization algorithms, such as the \textit{cross-entropy methods}, over available behaviors \cite{finn2017deep,ebert2018visual,jung2019goal}.  
%
Hot topics in video prediction mainly include long-term and high-fidelity future frames generation \cite{srivastava2015unsupervised,shi2015convolutional,vondrick2016generating,bhattacharjee2017temporal,wang2017predrnn,villegas2017learning,wichers2018hierarchical,reda2018sdc,oliu2018folded,liu2018dyan,xu2018structure,jin2020exploring,behrmann2021unsupervised}, dynamics uncertainty modeling \cite{babaeizadeh2017stochastic,denton2018stochastic,villegas2019high,kim2019variational,castrejon2019improved,franceschi2020stochastic,wu2021greedy}, object-centric scene decomposition \cite{van2018relational,hsieh2018learning,greff2019multi,zablotskaia2020unsupervised,bei2021learning,greff2017neural,kosiorek2018sequential}, and space-time disentanglement \cite{Villegas2017Decomposing,hsieh2018learning,guen2020disentangling,bodla2021hierarchical}.
%
The corresponding technical improvements mainly involve the use of more effective neural architectures, novel probabilistic modeling methods, and specific forms of video representations. 
%
The disentanglement methods are closely related to the world model in \model{}.
%
They commonly separate visual dynamics into content and motion vectors, or long-term and short-term states.
%
In contrast, \model{} is designed to learn a decoupled world model based on controllability, which contributes more to the downstream behavior learning process.


% \vspace{-5pt}
\section{Conclusion}
% \vspace{-5pt}


In this paper, we proposed an MBRL framework named \model{}, which mainly tackles the difficulty of vision-based prediction and control in the presence of complex visual dynamics.
%
Our approach has four novel contributions to world model representation learning and corresponding MBRL algorithms.
%
First, it learns to decouple controllable and noncontrollable latent state transitions via modular network structures and inverse dynamics. 
%
Second, it introduces the min-max variance constraints to prevent ``training collapse'', where a single state transition branch captures all information.
%
Third, it makes long-horizon decisions by rolling out the noncontrollable dynamics into the future and learning their influences on current behavior. 
%
Fourth, it models the sparse dependency of future noncontrollable dynamics on current controllable dynamics to deal with some practical dynamic environments.
%
\model{} achieves competitive results on the CARLA autonomous driving task, where other vehicles can be naturally viewed as noncontrollable components, indicating that with the help of decoupled latent states, the agent can make more forward-looking decisions by previewing possible future states in the action-free network branch.
%
Besides, Our approach was shown to effectively improve the visual control task in a modified DeepMind Control Suite, as well as the visual prediction task on the BAIR robot pushing dataset and the RoboNet dataset, achieving significant advantages over existing methods in standard, noisy, and transfer learning setups.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.




% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


This work was supported by the National Natural Science Foundation of China (Grant No. U19B2035, 62250062, 62106144), the Shanghai Municipal Science and Technology Major Project (Grant No. 2021SHZDZX0102), the Fundamental Research Funds for the Central Universities, and the Shanghai Sailing Program (Grant No. 21Z510202133). 


% National Natural Science Foundation of China (No.~61772299, 71690231, 61672313). 
% This work was supported by the National Key R\&D Program of China (2020AAA0109201), NSFC grants 62022050, 61772299, 62021002, 71690231, Beijing Nova Program (Z201100006820041), and CAAI-Huawei MindSpore Open Fund. The work was in part done when Y. Wang was a student at Tsinghua University. Y. Wang and H. Wu contributed equally to this work.




% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\begin{thebibliography}{1}
%\bibliographystyle{IEEEtran}
%\bibliography{IEEEabrv,iso.bib}
\input{iso.bbl}
%\end{thebibliography}


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:




% \vspace{-30pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/gsy.jpeg}}]{Shanyan Guan} received the B.E. degree from Xidian University in 2017. He is currently pursuing his PhD degree in Shanghai Jiao Tong University. His research interests lie on the intersection of deep learning and computer graphics, especially human mesh reconstruction and character animation.
% \end{IEEEbiography}

% \vspace{-30pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/xjw.jpeg}}]{Jingwei Xu} received the B.E. degree in electronic engineering from Shanghai Jiao Tong University (SJTU), Shanghai, China, in 2016 and the Ph.D. degree with the Department of Electrical Engineering, SJTU in 2021. His research interests include human mesh reconstruction, pose estimation, video generation and video prediction.
% \end{IEEEbiography}

% \vspace{-30pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/hmx.jpeg}}]{Michelle Z. He} is currently studying computer science at Purdue University and expected to receive her B.S. degree in 2023. Her research interests include machine learning and computer vision.
% \end{IEEEbiography}

% \vspace{-30pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/wyb.jpeg}}]{Yunbo Wang} received the B.E. degree from Xi'an Jiaotong University in 2012, and the M.E. and Ph.D. degrees from Tsinghua University in 2015 and 2020. He received the CCF Outstanding Doctoral Dissertation Award in 2020, advised by Philip S. Yu and Mingsheng Long. He is now an assistant professor at the AI Institute and the Department of Computer Science at Shanghai Jiao Tong University. He does research in deep learning, especially predictive learning, spatiotemporal modeling, and model-based decision making. 
% \end{IEEEbiography}

% \vspace{-30pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/nbb.jpeg}}]{Bingbing Ni} received the B.E. degree from Shanghai Jiao Tong University in 2005, and the Ph.D. degree from the National University of Singapore in 2011. He is currently a Professor with the Department of Electrical Engineering, Shanghai Jiao Tong University. His current research interests include computer vision, pattern recognition, and deep learning.
% \end{IEEEbiography}

% \vspace{-30pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/yxk.jpeg}}]{Xiaokang Yang} received the B.S. degree from Xiamen University in 1994, the M.S. degree from the Chinese Academy of Sciences in 1997, and the Ph.D. degree from Shanghai Jiao Tong University in 2000. He is
% currently a Distinguished Professor, Shanghai Jiao Tong University, Shanghai, China. His current research interests include visual signal processing and communication, media analysis and retrieval, and pattern recognition. He serves as an Associate Editor of IEEE Transactions on Multimedia.
% \end{IEEEbiography}




% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




% that's all folks
\end{document}


