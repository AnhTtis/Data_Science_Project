\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{booktabs}

\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{fontawesome}
\usepackage{multicol}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{svg}
\usepackage{comment}
\usepackage{mathrsfs}
%\usepackage[dvipsnames]{xcolor}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage{subfiles} 
\usepackage[T1]{fontenc}
\usepackage{hyperref}
%\usepackage[dvipsnames]{xcolor}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{***} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\def\semichecked{\Checkmark\!\!\!\raisebox{0.4 em}{$\bold{\smallsetminus}$}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE

\title{\texttt{NAISR}: A 3D Neural Additive Model \\ for Interpretable Shape Representation}

\author{
Yining Jiao\\
UNC-Chapel Hill\\
%Institution1 address\\
{\tt\small jyn@cs.unc.edu}
\and
Carlton Zdanski\\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small zdanski@med.unc.edu}
\and
Julia Kimbell\\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small julia\_kimbell@med.unc.edu}
\and
Andrew Prince\\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small andrew.prince@unchealth.unc.edu}
\and
Cameron Worden\\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small cameron.worden@unchealth.unc.edu}
\and
Samuel Kirse\\
Wake Forest School of Medicine \\
%First line of institution2 address\\
{\tt\small kirse.sam@gmail.com}
\and
Christopher Rutter \\
The Ohio State University College of Medicine\\
%First line of institution2 address\\
{\tt\small Christopher.Rutter@osumc.edu}
\and
Benjamin Shields  \\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small bhs@email.sc.edu}
\and
William Dunn  \\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small willad@live.unc.edu}
\and
Jisan Mahmud \\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small jisan@cs.unc.edu}
\and
Marc Niethammer \\
UNC-Chapel Hill\\
%First line of institution2 address\\
{\tt\small mn@cs.unc.edu}
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Deep implicit functions (DIFs) have emerged as a powerful paradigm for many computer vision tasks such as 3D shape reconstruction, generation, registration, completion, editing, and understanding. However, given a set of 3D shapes with associated covariates there is at present no shape representation method which allows to precisely represent the shapes while capturing the individual dependencies on each covariate. Such a method would be of high utility to researchers to discover knowledge hidden in a population of shapes. We propose a 3D Neural Additive Model for Interpretable Shape Representation (\texttt{NAISR}) which describes individual shapes by deforming a shape atlas in accordance to the effect of disentangled covariates. Our approach captures shape population trends and allows for patient-specific predictions through shape transfer. \texttt{NAISR} is the first approach to combine the benefits of deep implicit shape representations with an atlas deforming according to specified covariates. Although our driving problem is the construction of an airway atlas, \texttt{NAISR} is a general approach for modeling, representing, and investigating shape populations. We evaluate \texttt{NAISR} with respect to shape reconstruction, shape disentanglement, shape evolution, and shape transfer for the pediatric upper airway. Our experiments demonstrate that \texttt{NAISR} achieves competitive shape reconstruction performance while retaining interpretability. Our code is available at \href{https://github.com/uncbiag/NAISR}{https://github.com/uncbiag/NAISR}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\label{sec:intro}
% DIF can represent different kinds of signals and can be cooperate with mathematical analysis tools like differential geometry to help us deeply explore the shape 
Deep implicit functions (DIFs) have emerged as efficient representations of 3D shapes~\cite{park2019deepsdf, novello2022exploring, mescheder2019occupancy, yang2021geometry}, deformation fields~\cite{wolterink2022registration}, images and videos~\cite{sitzmann2020siren}, graphs, and manifolds~\cite{grattarola2022generalised}. For example, DeepSDF~\cite{park2019deepsdf} represents shapes as the level set of a signed distance field (SDF) with a neural network. In this way, 3D shapes are compactly represented as continuous and differentiable functions with infinite resolution. In addition to representations of geometry such as voxel grids~\cite{wu2016learning, wu20153d, wu2018learning}, point clouds~\cite{achlioptas2018learning, yang2018foldingnet, zamorski2020adversarial} and meshes~\cite{groueix2018papier, wen2019pixel2mesh++, zhu2019detailed}, DIFs have emerged as a powerful paradigm for many computer vision tasks. DIFs are used for 3D shape reconstruction~\cite{park2019deepsdf, mescheder2019occupancy, sitzmann2020siren}, generation~\cite{gao2022get3d}, registration~\cite{deng2021DIF, zheng2021DIT, sun2022topology, wolterink2022registration}, completion~\cite{park2019deepsdf}, editing~\cite{yang2022neumesh} and understanding~\cite{palafox2022spams}. %In addition, traditional shapes analysis theories such as differential geometry and level set theory are revived to help better investigate shapes with DIFs~\cite{novello2022exploring, mehta2022level, sitzmann2020siren}. For example, many shape operators like curvature become compatible with DIFs since gradients and second-order gradients with respect to coordinates can easily be calculated for a coordinate-based neural network.

% A interpretable shape analysis is needed
Limited attention has been paid to shape analysis with DIFs. Specifically, given a set of 3D shapes with a set of covariates attributed to each shape, a shape representation method is still desired which can precisely represent shapes and capture dependencies among a set of shapes. Such a representation could greatly help researchers understand shape population. There is currently no shape representation method that can quantitatively capture how covariates geometrically and temporally affect a population of 3D shapes; neither on average nor for an individual. However, capturing such effects is desirable as it is often difficult and sometimes impossible to control covariates (such as age, sex, and weight) when collecting data.  Further,  understanding the effect of such covariates is frequently a goal of medical studies. Therefore, it is critical to be able to disentangle covariate shape effects on the individual and the population-level to better understand and describe shape populations. Our approach is grounded in the estimation of a shape atlas (i.e., a template shape) whose deformation allows to capture covariate effects and to model shape differences. Taking an airway atlas as an example, a desired atlas representation should be able to answer the following questions:

\begin{itemize}
    \item Given an atlas shape, how can one accurately represent shapes and their dependencies?
    \item Given the shape of an airway, how can one disentangle covariate effects from each other? 
    \item Given a covariate, e.g., age, how does an airway change based on this covariate?
    \item Given a random shape, how will the airway develop after a period of time?
\end{itemize}

% what properties we provide with our proposed method
To answer these questions, we propose a Neural Additive Interpretable Shape Representation (\texttt{NAISR}), an interpretable way of modeling shapes associated with covariates via a shape atlas. Tab.~\ref{tab.lit} compares  \texttt{NAISR} to existing shape representations with respect to the following properties: 
 \begin{itemize}
     \item \textbf{Representation} relates to how a shape is described. Generally, a shape can be represented explicitly (e.g., as point clouds, voxels, and meshes) and implicitly with a function, e.g., parameterized by a neural network. Implicit representations are desirable as they naturally adapt to the size of a shape and also allow shape completion (i.e., reconstructing a complete shape from a partial shape, which is common in medical scenarios) with no additional effort.%since inherently the coordinate-based neural network can process incomplete shapes (which is quite common in medical scenarios) with no additional effort.
     \item \textbf{Deformable} captures if a shape representation results in point correspondences between shapes, e.g., via a displacement field. Specifically, we care about point correspondences between the target shapes and the atlas shape. A deformable shape representation helps to relate different shapes.
     \item \textbf{Disentangleable} indicates whether a shape representation can disentangle individual covariate effects for a shape. These covariate-specific effects can then be composed to obtain the overall displacement of an atlas/template shape. %Although A-SDF~\cite{mu2021asdf} and 3DAttriFlow~\cite{wen20223d} can disentangle  articulations from geometry, they do not disentangle different covariates and their disentanglements are not composable.
     \item \textbf{Evolvable} denotes whether a shape representation can evolve shapes based on changes of a covariate, capturing the influence of \emph{individual} covariates on shape.  
     \item \textbf{Transferable} indicates whether shape changes can be transferred to a given shape. E.g., one might want to edit an airway based on a simulated surgery and predict how such a surgical change manifests later in life. %given the shape of a particular anatomical structure edited by simulated surgery, there is usually a need to predict how the shape will evolve after a period of time to preview the treatment effect. 
     %In this case, the intrinsics of the anatomical structures stay the same while the covariates such as age and weight change. 
     \item \textbf{Interpretable} indicates a shape representation that is simultaneously \emph{deformable}, \emph{disentangleable}, \emph{evolvable}, and \emph{transferable}. Such an interpretable model is applicable to tasks ranging from the estimation of disease progression to assessing the effects of normal aging or weight gain on shape. 
 \end{itemize}

\texttt{NAISR} is the first shape representation method to investigate an atlas-based representation of 3D shapes in a deformable, disentangleable, transferable and evolvable way. %Such an approach can help researchers to discover hidden knowledge behind the data. 
We test \texttt{NAISR} on a 3D airway shape dataset. However, \texttt{NAISR} is a general method to represent and investigate shapes in an interpretable way.



\begin{table*}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c}
\hline
Method & Representation & Deformable  & Disentangleable &  Evolvable & Transferable & Interpretable\\
\hline\hline
ConditionalTemplate~\cite{dalca2019learning} & Voxels & \Checkmark & \XSolidBrush & \Checkmark & \XSolidBrush & \XSolidBrush \\
\hline
3DAttriFlow~\cite{wen20223d}  & Point Clouds & \Checkmark & \XSolidBrush & \Checkmark & \XSolidBrush & \XSolidBrush \\
\hline
DeepSDF~\cite{park2019deepsdf} & Implicit & \XSolidBrush & \XSolidBrush & \XSolidBrush & \XSolidBrush & \XSolidBrush\\ 
\hline
A-SDF~\cite{mu2021asdf}  & Implicit  & \XSolidBrush & \XSolidBrush  & \Checkmark &\Checkmark & \XSolidBrush \\
\hline
DIT~\cite{zheng2021DIT}, DIF~\cite{deng2021DIF}, NDF~\cite{sun2022topology} & Implicit & \Checkmark & \XSolidBrush & \XSolidBrush & \XSolidBrush & \XSolidBrush\\
\hline
NASAM~\cite{wei2022nasam} & Implicit & \Checkmark & \XSolidBrush & \Checkmark & \XSolidBrush & \XSolidBrush\\
\hline
Ours (\texttt{NAISR}) & Implicit  & \Checkmark & \Checkmark & \Checkmark & \Checkmark  & \Checkmark \\
\hline
\end{tabular}
}
\end{center}
\caption{Comparison of shape representations in the literature based on the desirable properties discussed in Sec.~\ref{sec:intro}. A \Checkmark indicates that a model has the desired property; a \XSolidBrush indicates that it does not. Only our \texttt{NAISR} model has all the desired properties.}
\label{tab.lit}
\end{table*}





\section{Related Work}
%
\paragraph{Deep Implicit Functions.} 
 Compared with geometry representations such as voxel grids~\cite{wu2016learning, wu20153d, wu2018learning}, point clouds~\cite{achlioptas2018learning, yang2018foldingnet, zamorski2020adversarial} and meshes~\cite{groueix2018papier, wen2019pixel2mesh++, zhu2019detailed}, DIFs are able to capture highly detailed and complex 3D shapes using a relatively small amount of data~\cite{park2019deepsdf, mu2021asdf, zheng2021DIT, sun2022topology, deng2021DIF}. They are based on classical ideas of level set representations~\cite{sethian1999level,osher2005level}; however, whereas  these classical level set methods represent the level set function on a grid, DIFs parameterize it as an actual continuous function, e.g., by a neural network. Hence, DIFs are not reliant on meshes, grids, or a discrete set of points. This allows them to efficiently represent natural-looking surfaces~\cite{gropp2020implicit, sitzmann2020siren, niemeyer2019occupancy}. Further, DIFs can be trained on a diverse range of data (e.g., shapes and images), making them more versatile than other shape representation methods. This makes them useful in applications ranging from computer graphics, to virtual reality, and robotics~\cite{gao2022get3d, yang2022neumesh, phongthawee2022nex360, shen2021deep}. \emph{We therefore formulate \texttt{NAISR} based on DIFs.}

%Another main advantage of DIF approaches is their flexibility when defining the optimization objective. This is because the implicit shape representation allows for computing shape related variables like higher order derivatives (related to curvatures) with auto-differentiation in a continuous field, which favors smooth and natural zero level set surfaces by focusing on regions with high curvatures~\cite{novello2022exploring, gropp2020implicit, sitzmann2020siren}. 


    

\paragraph{Point Correspondence.} 
Establishing point correspondences is important to help experts to detect, understand, diagnose, and track diseases. Recently, ImplicitAtlas~\cite{yang2022implicitatlas}, DIF-Net~\cite{deng2021DIF}, DIT~\cite{zheng2021DIT}, and NDF~\cite{sun2022topology} were proposed to capture  point correspondence within implicit shape representations. %DIT~\cite{zheng2021DIT} applies an LSTM to obtain smooth deformations, while NDF~\cite{sun2022topology} uses a neural ODE to obtain a smooth, invertible deformation which preserves shape topology. 
Dalca et al.~\cite{dalca2019learning} use templates conditioned on covariates for image registration. However, they did not explore covariate-specific deformations, shape representations or shape transfer. Currently no continuous shape representation which models the effects of covariates exists. \emph{\texttt{NAISR} will provide a model with such capabilities.}

\paragraph{Disentangled Representation Learning}
Disentangled representation learning (DRL) has been explored in a variety of domains, including computer vision~\cite{shoshan2021gan, ding2020guided, zhang2018unsupervised, zhang2018learning, xu2021learning, yang2020dsm}, natural language processing~\cite{john2018disentangled}, and medical image analysis~\cite{chartsias2019disentangled, bercea2022federated}.
DRL has also emerged in the context of implicit representations as a promising approach for 3D computer vision. By disentangling the underlying factors of variation, such as object shape, orientation, and texture, DRL can facilitate more effective 3D object recognition, reconstruction, and manipulation~\cite{stammer2022interactive, zhang2018unsupervised, zhang2018learning, xu2021learning, yang2020dsm, yang2022neumesh, gao2022get3d, tewari2022disentangled3d}. 

Besides DRL in computer vision, medical data is typically associated with various covariates which should be taken into account during analyses. Taking~\cite{chu2022disentangled} as an example, when observing a tumor's progression, it is difficult to know whether the variation of a tumor's progression is due to time-varying covariates or due to treatment effects. Therefore, being able to disentangle different effects is highly useful for a representation to promote understanding and to be able to quantify the effect of covariates on observations. \emph{\texttt{NAISR} will provide a disentangled representation and will allow us to capture the shape effects of covariates.}




\paragraph{Articulated Shapes}
There is significant research focusing on articulated shapes, mostly on humans~\cite{palafox2021npms, chen2021snarf, tretschk2020patchnets, deng2020nasa}. There is also a line of work on articulated general objects, e.g., A-SDF~\cite{mu2021asdf} and NASAM~\cite{wei2022nasam}. A-SDF~\cite{mu2021asdf} uses articulation as an additional input to control generated shapes, while NASAM~\cite{wei2022nasam} learns the latent space of articulation without articulation as supervision. %Recent work has explored the inverse problem of how to obtain knowledge from visual information with neural field~\cite{hong20223d, hong2021ptr}, i.e. visual reasoning. These works result in implicit shape representations capable of concept grounding.

The aforementioned works on articulated objects assume that each articulation affects a separate object part. This is easy to observe, e.g., the angles of the two legs of a pair of eyeglasses. Hence, although A-SDF~\cite{mu2021asdf} and 3DAttriFlow~\cite{wen20223d} can disentangle  articulations from geometry, they do not disentangle different covariates and their disentanglements are not composable. However, in medical scenarios, covariates often affect shapes in a more entangled and complex way, for example, a shape might simultaneously be influenced by sex, age, and weight. \emph{\texttt{NAISR} will allow us to account for such complex covariate interactions.}%have 3D representation is desired to explore the hidden relations between concepts and shapes behind data. 

\paragraph{Explainable Artificial Intelligence}
The goal of eXplainable Artificial Intelligence (XAI) is to provide human-understandable explanations for decisions and actions of an AI model. Various flavors of XAI exist, including counterfactual inference~\cite{berrevoets2021learning, moraffah2020causal, thiagarajan2020calibrating, chen2022covariate}, attention maps~\cite{zhou2016cvpr, jung2021towards,woo2018cbam}, feature importance~\cite{arik2021tabnet, ribeiro2016should, agarwal2020neural}, and instance retrieval~\cite{Crabbe2021Simplex}.  \texttt{NAISR} is inspired by neural additive models (NAMs)~\cite{agarwal2020neural} which in turn are inspired by generalized additive models (GAMs)~\cite{hastie2017generalized}. NAMs are based on a linear combination of neural networks each attending to a single input feature. \texttt{NAISR} extends this concept to interpretable 3D shape representations. This is significantly more involved as, unlike for NAMs and GAMs, we are no longer dealing with scalar values, but with 3D shapes. \emph{\texttt{NAISR} will provide interpretable results by capturing spatial deformations with respect to an estimated atlas shape such that individual covariate effects can be distinguished.}


\section{Method}
This section discusses our \texttt{NAISR} model and how we obtain the desired model properties of Section~\ref{sec:intro}.
 
\begin{figure*}[ht]
\vskip -0.1in
\begin{center}
\centerline{\includegraphics[width=2.\columnwidth]{figs/NAISRoverviewwide.pdf}}
\caption{Neural Additive Implicit Shape Representation. During training we learn the template $\mathcal{T}$ and the neural networks $\{g_i\}$ predicting the covariate-wise displacement fields $\{\mathbf{d}_i\}$. The displacement fields are added to obtain the overall displacement field $\mathbf{d}$ defined in the target space;  $\mathbf{d}$ provides the displacement between the deformed template shape $\mathcal{T}$ and the target shape that is to be captured. Specifically the template shape is queried not at its original coordinates $\mathbf{p}$, but instead at $\Tilde{\mathbf{p}}=\mathbf{p}+\mathbf{d}$ effectively spatially deforming the template. At test time we evaluate the trained multi-layer perceptrons (MLPs) to use them for shape reconstruction, evolution, disentanglement, and shape transfer.}
\label{fig.method}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Problem Description}
Consider a set of shapes $\mathcal{S}=\{\mathcal{S}^k\}$ where each shape $\mathcal{S}^k$ has an associated vector of covariates $\mathbf{c}=[c_1, ..., c_i, ..., c_N]$ (e.g., age, weight, sex). Our goal is to obtain a representation which describes the entire set $\mathcal{S}$ while accounting for the covariates. Our approach estimates a template shape, $\mathcal{T}$ (the shape atlas), which approximates the entire set $\mathcal{S}$. Specifically, $\mathcal{T}$ is deformed based on a set of displacement fields $\{\mathcal{D}^k\}$ such that the individual shapes $\{\mathcal{S}^k\}$ are approximated well by the transformed template.
%Therefore, a particular shape ${\mathcal{S}}$ can be represented with the displacement $\mathcal{D}$ and the template shape $\mathcal{T}(\cdot)$ as $\mathcal{T}(\mathcal{S} + \mathcal{D})$. 

A displacement field $\mathcal{D}^k$ describes how a shape is related to the template shape $\mathcal{T}$. The factors that cause this displacement may be directly observed or not. For example, observed factors may be covariates such as subject age, weight, or sex; i.e., $\mathbf{c}_k$ for subject $k$. . Factors that are not directly observed may be due to individual shape variation, unknown or missing covariates, or variations due to differences in data acquisition or data preprocessing errors. We capture these not directly observed factors by a latent code $\mathbf{z}$. The covariates $\mathbf{c}$ and the latent code $\mathbf{z}$ then contribute jointly to the displacement $\mathcal{D}$ with respect to the template shape $\mathcal{T}$.

Inspired by neural additive~\cite{agarwal2020neural} and generalized additive~\cite{hastie2017generalized} models, we assume the overall displacement field is the sum of displacement fields that are controlled by individual covariates: $\mathcal{D}=\Sigma_i \mathcal{D}_i$. Here, $\mathcal{D}_i$ is the displacement field controlled by the i-th covariate, $c_i$. This results by construction in an overall displacement $\mathcal{D}$ that is disentangled into several sub-displacement fields $\{\mathcal{D}_i\}$. Conversely, the sub-displacement field $\{\mathcal{D}_i\}$ controlled by the covariates $\mathbf{c}$ jointly determine the overall displacement $\mathcal{D}$.

\subsection{Model Formulation}
\label{subsec.model_f}
Fig.~\ref{fig.method} gives an overview of \texttt{NAISR}. To obtain a continuous atlas representation, we use DIFs to represent both the template $\mathcal{T}$ and the displacement field $\mathcal{D}$. The template shape $\mathcal{T}$ is represented by a signed distance function, where the zero level set $\{\mathbf{p} \in \mathbb{R}^3 | \mathcal{T}(\mathbf{p})=0\}$ captures the desired template shape. %any point $\mathbf{p} \in \mathbb{R}^3$ in the template space is mapped to the signed distance to the surface $\mathcal{S}$ as $s=\mathcal{T}(\mathbf{\mathbf{p}}), s \in \mathbb{R}$. The zero level set $\{\mathbf{p} \in \mathbb{R}^3 | \mathcal{T}(\mathbf{p})=0\}$ of the signed distance function $\mathcal{T}(\cdot)$ is the point set on the template surface. 
The displacement $\mathcal{D}_i$ of a particular point $\mathbf{p}$ can also be represented implicitly as $\mathbf{d}_i = f_i(\mathbf{p}, c_i, \mathbf{z}) \in \mathbb{R}^3$. We use \texttt{SIREN}~\cite{sitzmann2020siren} as the backbone for $\mathcal{T}(\cdot)$ and $\{f_i(\cdot)\}$. Considering that the not directly observed factors might influence the geometry of all covariate-specific networks, we make the latent code, $\mathbf{z}$, visible to all subnetworks $\{f_i(\cdot)\}$. We normalize the covariates so that they are centered at zero. To assure that a zero covariate value results in a zero displacement we parameterize the displacement fields as $\mathbf{d}_i = g_i(\mathbf{p}, c_i, 0, \mathbf{z})$ where
\begin{equation}
%\mathbf{d}_i = g_i(\mathbf{p}, c_i, 0, \mathbf{z}) = g_i(\mathbf{p}, c_i, \mathbf{z}) = f_i(\mathbf{p}, c_i, \mathbf{z}) - f_i(\mathbf{p}, 0, \mathbf{z})
g_i(\mathbf{p}, x, y, \mathbf{z}) = f_i(\mathbf{p}, x, \mathbf{z}) - f_i(\mathbf{p}, y, \mathbf{z})\,.
\label{eq.disp_1}
\end{equation}

The sub-displacement fields are added to obtain the overall displacement field
\begin{equation}
\mathbf{d} = g(\mathbf{p}, \mathbf{c}, \mathbf{z}) = \sum_{i=1}^N g_i(\mathbf{p}, c_i, 0,\mathbf{z})\,.
\label{eq.disp_2}
\end{equation}

Based on this displacement field we deform the template shape  $\mathcal{T}$ to obtain an implicit representation of a target shape
\begin{equation}
s = \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z}) = \mathcal{T}(\Tilde{\mathbf{p}}) = \mathcal{T}(\mathbf{p} + \sum_{i=1}^Ng_i(\mathbf{p}, c_i, \mathbf{z}))\,,
\end{equation}
where $\mathbf{p}$ is a point in the source shape space, e.g., a point on the surface of shape $\mathcal{S}_i$ and $\Tilde{\mathbf{p}}$ represents a point in the template shape space, e.g., a point on the surface of the template shape $\mathcal{T}$. To investigate how an individual covariate $c_i$ affects a shape we can simply extract the zero level set of
\begin{equation}
s_i = \Phi_i(\mathbf{p}, c_i, \mathbf{z})=\mathcal{T}(\mathbf{p}+\mathbf{d}_i) = \mathcal{T}(\mathbf{p}+g_i(\mathbf{p}, c_i, \mathbf{z}))\,.
 \end{equation}
 



\subsection{Training}

\paragraph{Training Strategy}
%To encourage disentanglement of covariates $\mathbf{c}$ from latent vectors $\mathbf{z}$, 
We use the zero-padding strategy of~\cite{li2020gait, martinbrualla2020nerfw} to minimize the influence of the latent code $\mathbf{z}$ on shape differences that should be explained by the covariates $\mathbf{c}$. By also setting $\mathbf{z}$ to $\mathbf{0}$ while optimizing the objective function, the model is pushed to rely on $\mathbf{c}$ as much as possible. 

\paragraph{Losses}
All our losses are ultimately summed over all shapes of the training population with the appropriate covariates $\mathbf{c}_k$ and shape code $\mathbf{z}_k$ for each shape $\mathcal{S}^k$. For ease of notation, we describe them for individual shapes in the following. For each shape, we sample on-surface and off-surface points. On-surface points have zero signed distance 
 values and normal vectors extracted from the gold standard\footnote{In medical imaging, there is typically no groundtruth. We use \textit{gold standard} to indicate shapes based off of manual or automatic segmentations, which are our targets for shape reconstruction.} mesh. Off-surface points have non-zero signed distance values but no normal vectors. Our losses for reconstruction follow~\cite {sitzmann2020siren, novello2022exploring}. For points on the surface, the losses are
\begin{equation}
\begin{aligned}
    &\mathcal{L}_{\mathrm{on}} (\Phi, \mathbf{c}, \mathbf{z}) = \int_{\mathcal{S}} \lambda_{1} \underbrace{\left\|\left|\nabla_{\mathbf{p}} \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})\right|-1\right\|}_{\mathcal{L}_{\text{Eikonal}}} \\ 
    & + \lambda_{2} \underbrace{\|\Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})\|}_{\mathcal{L}_{\text{Dirichlet}}} + \lambda_{3} \underbrace{\left(1-\left\langle\nabla_{\mathbf{p}} \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z}), \mathbf{n}(\mathbf{p})\right\rangle\right)}_{\mathcal{L}_{\text {Neumann}}} d \mathbf{p} \,,
\end{aligned}
\end{equation}
where $\mathbf{n}(\mathbf{p})$ is the normal vector at $\mathbf{p}$ and $\langle \cdot \rangle$ denotes the cosine similarity. For points off the surface, we use
\begin{equation} 
\begin{aligned}
&\mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{z}) = \int_{\Omega \backslash \mathcal{S}} \lambda_4 \underbrace{|\Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})-s_{tgt}(\mathbf{p})|}_{\mathcal{L}_{\text{Dirichlet}}} \\
&+ \lambda_{5} \underbrace{\left\|\left|\nabla_{\mathbf{p}} \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})\right|-1\right\|}_{\mathcal{L}_{\text{Eikonal}}} d \mathbf{p}\,,
\end{aligned}
\end{equation}
where $s_{tgt}(\mathbf{p})$ is the signed distance value at $\mathbf{p}$ corresponding to a given target shape. Similar to~\cite{park2019deepsdf, mu2021asdf} we penalize the squared $L^2$ norm of the latent code $z$ as
\begin{equation}
    \mathcal{L}_{\mathrm{lat}}(\mathbf{z}) = \lambda_{6}\frac{1}{\sigma^{2}}\left\|\boldsymbol{z}\right\|_{2}^{2}\,.
\end{equation}

Further, we use an approximate inverse consistency loss~\cite{greer2021icon, tian2022gradicon} to regularize the displacement fields. The basic idea is that the approximate inverse transformation $g_i^{-1}(\cdot)$ of $g_i(\cdot)$ should be able to map a point in the template space back to the point in the source space where
\begin{equation}
\begin{aligned}
g_i^{-1}(\Tilde{\mathbf{p}}) &= g_i(\mathbf{p+\mathbf{d}_i}, 0, c_i, \mathbf{z}) \\
&= f_i(\mathbf{p+\mathbf{d}_i}, 0, \mathbf{z}) - f_i(\mathbf{p+\mathbf{d}_i}, c_i, \mathbf{z})\,. 
\end{aligned}
\end{equation}

The inverse consistency loss can then be written as
\begin{equation}
\mathcal{L}_{\mathrm{inv}}(\{g_i\}, \mathbf{z}) = \lambda_7\int_{\mathcal{S}} \Sigma_{i=1}^{N} \|(-\mathbf{d}_i) - g_i^{-1}(\mathbf{p+\mathbf{d}_i})\| d \mathbf{p} \,.
\label{eq.inv_loss}
\end{equation}

As a result, our overall loss (for a given shape) is
\begin{equation}
\begin{aligned}
&\mathcal{L}(\Phi, \{g_i\}, \mathbf{c}, \mathbf{z}) = \underbrace{\mathcal{L}_{\mathrm{lat}}(\mathbf{z})}_{\mathcal{L}_{latent\_space}} \\
&+ \underbrace{\mathcal{L}_{\mathrm{on}}(\Phi, \mathbf{c}, \mathbf{z}) 
 + \mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{z}) 
 + \mathcal{L}_{\mathrm{inv}}(\{g_i\}, \mathbf{z})}_{\mathcal{L}_{reconstrution}} \\
 &+ \underbrace{\mathcal{L}_{\mathrm{on}}(\Phi, \mathbf{c}, \mathbf{0}) 
 + \mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{0}) 
 + \mathcal{L}_{\mathrm{inv}}(\{g_i\}, \mathbf{0})}_{\mathcal{L}_{zero\_padding}} \,,
\end{aligned}
\end{equation}
where the parameters of $\Phi$, $\{g_i\}$, and $\mathbf{z}$ are trainable.


\subsection{Testing}
\label{sec.testing}
As shown in Fig.~\ref{fig.method}, our proposed \texttt{NAISR} is designed to accomplish the following tasks: shape reconstruction, shape disentanglement, shape evolution, and shape transfer. Further, \texttt{NAISR} also allows for shape interpolation, shape completion, and shape correspondence by design as~\cite{zheng2021DIT, sun2022topology}.

\paragraph{Shape Reconstruction and Generation}
As illustrated in the inference section in Fig.~\ref{fig.method}, a shape $s_{tgt}$ is given and the goal is to recover its corresponding latent code $\mathbf{z}$ and the covariates $\mathbf{c}$. %This can be solved by backpropagation. 
To estimate these quantities, the network parameters stay fixed and we optimize over the covariates $\mathbf{c}$ and the latent code $\mathbf{z}$ which are both randomly initialized~\cite{park2019deepsdf, mu2021asdf}. Specifically, we solve the optimization problem
\begin{equation}
\hat{\mathbf{c}}, \hat{\mathbf{z}}=\underset{\mathbf{c}, \mathbf{z}}{\arg \min }~ \mathcal{L}(\Phi, \mathbf{c}, \mathbf{z})\,.
\label{eq.infer_cz}
\end{equation}
In clinical scenarios, the covariates $\mathbf{c}$ might be known (e.g., recorded age or weight at imaging time). In this case, we only infer the latent code $\mathbf{z}$ by the optimization
\begin{equation}
    \hat{\mathbf{z}}= \underset{\mathbf{z}}{\arg \min }~\mathcal{L}(\Phi,\mathbf{c}, \mathbf{z})\,.
\label{eq.infer_z}
\end{equation}
A new shape of a patient with only the covariates altered can be generated by extracting the zero level set of $\Phi(\mathbf{p}, \mathbf{c}_{\mathrm{new}}, \hat{\mathbf{z}})$. 

\paragraph{Shape Evolution}
\label{para.evo}
Shape evolution along covariates $\{c_i\}$ is desirable in shape analysis to obtain knowledge of disease progression or population trends in the shape population $\mathcal{S}$. For a time-varying covariate $(c_i^0, ...,c_i^t, ...,c_i^T)$, we obtain the corresponding shape evolution by $(\Phi_i(\mathbf{p}, c_i^0, \hat{\mathbf{z}}), ...,\Phi_i(\mathbf{p}, c_i^t, \hat{\mathbf{z}}),..., \Phi_i(\mathbf{p}, c_i^T, \hat{\mathbf{z}}))$. If some covariates are correlated (e.g., age and weight), we can first obtain a reasonable evolution of the covariates $(\mathbf{c}^0, ..., \mathbf{c}^t, ..., \mathbf{c}^T)$ and the corresponding shape evolution as  $(\Phi(\mathbf{p}, \mathbf{c}^0, \hat{\mathbf{z}}), ...,\Phi(\mathbf{p}, \mathbf{c}^t,\hat{\mathbf{z}}),..., \Phi(\mathbf{p}, \mathbf{c}^T, \hat{\mathbf{z}}))$.

\paragraph{Shape Disentanglement}
As shown in the disentanglement section in Fig.~\ref{fig.method}, the displacement for a particular covariate $c_i$ displaces point $\mathbf{p}$ in the source space to $\mathbf{p}+\mathbf{d}_i$ in the template space for a given or inferred $\hat{\mathbf{z}}$ and $c_i$. We obtain the corresponding implicit signed distance field as
\begin{equation}
s_i=\Phi_i(\mathbf{p}, c_i, \hat{\mathbf{z}})=\mathcal{T}(\mathbf{p} + \mathbf{d}_i) = \mathcal{T}(\mathbf{p} + g_i(\mathbf{p}, c_i, \hat{\mathbf{z}}))\,.
\end{equation}
As a result, the zero level sets of $\{\Phi_i(\cdot)\}$ represent shapes warped by the sub-displacement fields controlled by $c_i$.

\paragraph{Shape Transfer}
We use the following clinical scenario to introduce the shape transfer task. Suppose a doctor has conducted simulated surgery on an airway shape with the goal of previewing treatment effects on the shape after a period of time. This question can be answered by our shape transfer approach. Specifically, as shown in the transfer section in Fig.~\ref{fig.method}, after obtaining the inferred latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}$ from reconstruction, one can transfer the shape from the current covariates $\hat{\mathbf{c}}$ to new covariates $\hat{\mathbf{c}}+{\Delta \mathbf{c}}$ with $\Phi(\mathbf{p},\hat{\mathbf{c}}+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$. As a result, the transferred shape is a prediction of the outcome of the simulated surgery; it is the zero level set of $\Phi(\mathbf{p},\hat{\mathbf{c}}+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$. In more general scenarios, the covariates are unavailable but it is possible to infer them from the measured shapes themselves (see Eqs.~\eqref{eq.infer_cz}-\eqref{eq.infer_z}). %~\footnote{For example, cancer patients have some specialized blood tests while the normal control group does not. However, one might want to jointly analyze both groups of shapes with covariates including blood tests. }. 
Therefore, in shape transfer we are not only evolving a shape, but may also first estimate the initial state to be evolved. 

%We borrow the idea of deformation transfer to define our shape transfer task. As shown in the upper section of Figure~\ref{fig.ex_transfer}, the goal of shape transfer is to infer how a new shape (purple star) should deform in accordance with the deformation for a given pair of source and target shapes (cyan stars). In real scenarios, it is even difficult to get the pairs of cyan stars yet both forward and backward directions of displacements are needed, as the bottom section of Figure~\ref{fig.ex_transfer}. For example, there are many shapes from different patients collected at different time points. To infer what a shape will be like two years later for a certain patient with only one observation is a common but challenging question to answer. With \texttt{NAISR} we can easily answer such a question in a quantitative and continuous way.
%As shown in the transfer section in Figure~\ref{fig.method}, our \texttt{NAISR} is capable of transfering shapes with the inferred latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}$ for the current observation. Specifically, by evolving the current covariates $\hat{\mathbf{c}}$ to new covariates $\hat{\mathbf{c}}+{\mathbf{c}}$, the shape can be transfered to former or later states with $\Phi(\mathbf{p},\hat{\mathbf{c}}+{\mathbf{c}}, \hat{\mathbf{z}})$. 

% \begin{figure}[ht]
% \begin{center}
% \centerline{\includesvg[width=0.7\columnwidth]{figs/example.svg}}
% \caption{An example of parallel transfer for shapes}
% \label{fig.ex_transfer}
% \vskip -0.4in
% \end{center}
% \end{figure}

\section{Experiments}
We evaluate \texttt{NAISR} in terms of shape reconstruction, shape disentanglement, shape evolution, and shape transfer. We can quantitatively evaluate $\texttt{NAISR}$ for shape reconstruction and shape transfer because our dataset contains longitudinal observations. For shape evolution and shape disentanglement, we provide visualizations of covariate extrapolations to demonstrate that our method is capable of learning a reasonable representation of the deformations governed by the covariates. More details of the dataset and the experiments are available in the supplementary material.
 
\paragraph{Dataset.} We use a pediatric airway dataset containing 357 airway shapes to evaluate our method. These airway shapes are obtained from automatic airway segmentations of computed tomography (CT) images of the upper airway of children with a radiographically normal airway. These 357 airway shape are from 263 patients, 34 of whom have longitudinal observations and 229 of whom have only been observed once. We use a 80\%-20\% train-test split by patient (instead of shapes); i.e., a given patient cannot simultaneously be in the train and the test set and therefore no information can leak between these two sets. Each shape has 3 covariates (age, weight, sex) and 11 annotated anatomical landmarks. Errors in the shapes $\{\mathcal{S}^k\}$ may arise from image segmentation error, differences in head positioning, missing parts of the airway shapes due to incomplete image coverage, and dynamic airway deformations due to breathing. %As shown in Figure~\ref{fig.dataset}, the error may arise from image segmentation error, head angle to the airway, missing airway shapes due to incomplete image range, and dynamic area due to breathing. 



\paragraph{Data Processing}
The shape meshes are extracted using Marching Cubes~\cite{lorensen1987marching, van2014scikit} to obtain coordinates and normal vectors of on-surface points. The airway shapes are rigidly aligned using the anatomical landmarks. The true vocal cords landmark is set to the origin. We follow the implementation in~\cite{park2019deepsdf} to sample 500,000 off-surface points. During training, it is important to preserve the scale information. We therefore scale all meshes with the same constant. %The resulting meshes from reconstruction are extracted with marching cubes~\cite{lorensen1987marching,van2014scikit}.

\paragraph{Comparison Methods} For shape reconstruction of unseen shapes, we compare our method on the test set with DeepSDF~\cite{park2019deepsdf} A-SDF~\cite{mu2021asdf} DIT~\cite{zheng2021DIT} and NDF~\cite{sun2022topology}. For shape transfer, we compare our method with A-SDF~\cite{mu2021asdf} because other comparison methods cannot model covariates as summarized in Tab.~\ref{tab.lit}. The original implementations of the comparison methods did not produce satisfying reconstructions on our dataset. We therefore improved them by using our reconstruction losses and by using the \texttt{SIREN} backbone~\cite{sitzmann2020siren} in DeepSDF~\cite{park2019deepsdf}, A-SDF~\cite{mu2021asdf}, and the template networks in DIT~\cite{zheng2021DIT} and NDF~\cite{sun2022topology}.%DeepSDF~\cite{park2019deepsdf} and A-SDF~\cite{mu2021asdf} directly use MLP layers to map 3d coordinates to the distance from the surface while DIT~\cite{zheng2021DIT} and NDF~\cite{sun2022topology} calculate the displacement to warp the source shape to the template.
 
\paragraph{Metrics} For evaluation, all target shapes and reconstructed meshes are normalized to a unit sphere as in~\cite{park2019deepsdf} to assure that  large shapes and small shapes contribute equally to error measurements.  We use the Hausdorff distance, Chamfer distance, and earth mover's distance to evaluate the performance of our shape reconstructions. We also report the number of model parameters. For shape transfer, considering that a perfectly consistent image acquisition process is impossible for different observations (e.g., head positioning might slightly vary across timepoints), we visualize the transferred shapes and evaluate based on the difference between the \emph{volumes} of the reconstructed shapes and the target shapes.

\subsection{Shape Reconstruction}
\begin{table*}[ht]
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcrrrrrrr}
\toprule
      Methods & Deformable &\# params & CD mean &  CD median &  EMD mean &  EMD median &  HD mean &  HD median \\
\midrule
      DeepSDF~\cite{park2019deepsdf} & \XSolidBrush & 2.24M  & 0.143 &      \textcolor{purple}{\textbf{0.023}} &     1.249 &       \textcolor{purple}{\textbf{0.991}} &    \textcolor{purple}{\textbf{9.860}} &      \textcolor{purple}{\textbf{6.750}} \\
      DIT~\cite{zheng2021DIT} & \Checkmark &  3.70M  &  0.061 &      0.032 &     \textcolor{purple}{\textbf{1.135}}&       \textbf{1.032} &   10.528 &      8.192 \\
      NDF~\cite{sun2022topology} & \Checkmark & 0.34M & 0.091 &      0.056 &     1.461 &       1.371 &   12.110 &      9.986 \\
      A-SDF~\cite{mu2021asdf} (with Cov.) & \XSolidBrush &  1.98M  &  0.658 &      0.083 &     3.028 &       1.744 &   26.678 &     18.623  \\
      A-SDF~\cite{mu2021asdf} & \XSolidBrush & 1.98M &     0.412 &      0.097 &     2.900 &       2.023 &   24.266 &     16.755 \\
      NAISR (with Cov.) & \Checkmark & 2.33M  & \textbf{0.059} &      0.031 &     1.200 &       1.109 &   10.334 &      8.287\\
           NAISR & \Checkmark & 2.33M   &     \textcolor{purple}{\textbf{0.056}} &      \textbf{0.030} &     \textbf{1.165} &       1.061 &   \textbf{9.968} &      \textbf{8.104} \\
\bottomrule
\end{tabular}}
\end{center}
\caption{Quantitative evaluation of shape reconstruction. \(\mathrm{CD}=\) Chamfer distance. \(\mathrm{EMD}=\) Earth mover's distance. \(\mathrm{HD}=\) Hausdorff distance. All metrics are multiplied by $10^2$. Bold red values indicate the best scores across all methods. Bold black values indicate the 2nd best scores of all methods. \textbf{NAISR (with Cov.)} means the covariates are used as input during inference (see Eq.~\eqref{eq.infer_z}). \textbf{NAISR} means the covariates were inferred during testing (see Eq.~\eqref{eq.infer_cz}). \textbf{NAISR} performs well for reconstruction allowing for interpretability.}
\label{tab.recons}
\end{table*}

\begin{figure*}[ht]
\includegraphics[width=2.1\columnwidth]{figs/reconstructions.pdf}
\caption{Visualizations of shape reconstructions with different methods. The red and blue circles show the structure in the black circle from two different views. \textbf{NAISR} can produce detailed and accurate reconstructions as well as impute missing airway parts.}
\label{fig.exp_recons}
%\vskip -0.2in
\label{fig.recons}
\end{figure*}

The goal of our shape reconstruction experiment is to demonstrate that \texttt{NAISR} can provide competitive reconstruction performance while providing interpretability. Tab.~\ref{tab.recons} shows the quantitative evaluations. Fig.~\ref{fig.exp_recons} visualizes reconstructed shapes. We observe that implicit shape representations can complete missing shape parts which can benefit further shape analysis. DeepSDF~\cite{park2019deepsdf} performs better than the deformation-based methods~\cite{simeonovdu2021ndf, zheng2021DIT}, likely because it directly optimizes individual shape reconstructions instead of relying on the deformations of a template shape. A-SDF~\cite{mu2021asdf} works well for representing shapes in the training set but cannot reconstruct unseen shapes successfully. The reason might be small amount of available longitudinal data, with A-SDF memorizing each shape and its covariates instead of learning to infer how the covariates affect shapes in general. 

\subsection{Shape Transfer}

\begin{table*}
%\hfill\includegraphics[width=2.\columnwidth]{figs/1181_comp.png}
%{c{0.05\textwidth}c{0.06\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}c{0.05\textwidth}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{p{0.05\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}}
\toprule
\# time &      0&      1  &      2  &      3  &      4  &      5  &      6  &      7  &      8  &      9  &      10 \\
\midrule
$\{\mathcal{S}^t\}$&
\includegraphics[width=0.2\columnwidth]{figs/1181/0.png} &  
\includegraphics[width=0.2\columnwidth]{figs/1181/1.png} & 
\includegraphics[width=0.2\columnwidth]{figs/1181/2.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/3.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/4.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/5.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/6.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/7.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/8.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/9.png}& 
\includegraphics[width=0.2\columnwidth]{figs/1181/10.png}\\
\bottomrule
\end{tabular}}

\begin{tabular}{p{0.05\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}p{0.06\textwidth}}
\toprule
\# time &      0&      1  &      2  &      3  &      4  &      5  &      6  &      7  &      8  &      9  &      10 \\
\midrule
weight     &   55.20 &   60.90 &   64.30 &   65.25 &   59.25 &   59.20 &   65.30 &   68.00 &   77.10 &   75.60 &   75.60 \\
age  &  154 &  155 &  157 &  159 &  163 &  164 &  167 &  170 &  194 &  227  &  233 \\
sex         &    male &    male &    male &  male &  male  &  male &  male &  male &  male &   male &  male \\
p-vol &   88.40 &   89.47 &   90.42 &   90.95 &   90.71 &   90.91 &   92.46 &   93.48 &   99.66 &  107.99 &  110.16 \\
m-vol &   86.33 &   82.66 &   63.23 &   90.65 &   98.11 &   84.35 &   94.14 &  127.45 &   98.81 &  100.17 &  113.84 \\
\bottomrule

\end{tabular}
\caption{Shape transfer for a patient. Blue: gold standard shapes; red: transferred shapes with \texttt{NAISR}. The table below lists the covariates (age/month, weight/kg, sex) for the shapes above. P-vol(predicted volume) is the volume ($cm^3$) of the transferred shape by NAISR w/o covariates following Eq.~\eqref{eq.infer_cz}. M-vol (measured volume) is the volume ($cm^3$) of the shapes based on the actual imaging. \texttt{NAISR} can capture the trend of growing volume with age and weight while producing clear, complete, and topology-consistent shapes. }
\label{tab.transp_for_case}
\end{table*}

%\end{figure*}


\begin{figure}[ht]
\vskip -0.2in
\centerline{\includegraphics[width=0.8\columnwidth]{figs/summary_test_metrics_transport_volume_0307.png}}
\bigskip
\begin{center}
\begin{tabular}{lrr}
\toprule
Methods &  VD mean &  VD median\\
\midrule
A-SDF (with Cov.) &  46.24 &        38.52 \\
A-SDF & 51.72 &        51.18 \\
NAISR (with Cov.) &  \textbf{11.16} &         8.87  \\
NAISR  &  11.94 &         \textbf{7.61}\\
\bottomrule
\end{tabular}
\vskip-0.6in
\end{center}
\caption{Statistics of Volume Difference (VD, $cm^3$) between transferred shapes and gold standard shapes. Bold scores indicate the best performance across all methods. The blue results in the boxplot represent the reconstruction step following Eq.~\eqref{eq.infer_z}. The orange results represent the reconstruction step following Eq.~\eqref{eq.infer_cz}. \textbf{NAISR} results in significantly improved volume estimates.}
\label{tab.transfer}
\end{figure}



The goal of our shape transfer experiment is to evaluate \texttt{NAISR}'s ability to transfer a shape $\mathcal{S}^0$ from the current covariates $\mathbf{c}^0$ (which might be unknown) to different covariates $\mathbf{c}^t$.  We use the longitudinal data in the dataset for evaluation. Specifically, we treat the earliest shape observation of a patient as the current shape $\mathcal{S}^0$ and attempt to transfer $\mathcal{S}^0$ from $\mathbf{c}^0$ to $\mathbf{c}^t$ to capture the later shape $\mathcal{S}^t$. 

First, we obtain the inferred latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}^0$ of $\mathcal{S}^0$ by reconstruction. Then, we transfer $\mathcal{S}^0$ to $\mathcal{S}^t$ with  $\Phi(\mathbf{p},\hat{\mathbf{c}}^0+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$ by setting $\Delta\mathbf{c}$ to $\mathbf{c}^t - \mathbf{c}^0$, where $\mathbf{c}^t$ and $\mathbf{c}^0$ are the covariates of the longitudinal data. Ideally, the inferred $\hat{\mathbf{c}}^0$ is exactly $\mathbf{c}^0$. The zero level set of $\Phi(\mathbf{p},\hat{\mathbf{c}}^0+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$ is then the transferred shape and it is evaluated by comparing it with the gold standard shape $\mathcal{S}^t$. 


%\hat{\mathbf{c}}+\Delta\mathbf{c}$. By , we can transfer $\mathcal{S}^0$ with  $\Phi(\mathbf{p},\hat{\mathbf{c}}+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$. 



%Given an observed shape $\mathcal{S}_0$, we first reconstruct the shape to infer its latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}_0$ from the reconstruction. Then we transfer the shape to a later time point by evolving the covariates to $\hat{\mathbf{c}}(0)+{\mathbf{c}}$, where ${\Delta\mathbf{c}}=\mathbf{c}(t) - \mathbf{c}(0)$ is the difference between the covariates at different observations. 

Tab.~\ref{tab.transp_for_case} shows a shape transfer example for a cancer patient who was scanned 11 times. We observe that our method can produce transferred shapes that correspond well with the measured shapes. Further, we are able to infer \emph{complete} shapes. % along the covariate reasonably in terms of producing complete shapes and developing volumes. 
Fig.~\ref{tab.transfer} shows quantitative results for the volume differences between our transferred shapes and the gold standard shapes. Our method performs best. Our results demonstrate that \texttt{NAISR} is capable of transferring shapes to other timepoints $\mathcal{S}^t$ from a given initial state $\mathcal{S}^0$. %Such application is very desirable in medical scenarios when inferring treatment effects. %Due to the error produced in the data acquisition steps, it is impossible to get the exact volume for every shape. Thus, we use the boxplot to evaluate the variance of the volume estimation error. Shorter error bars are considered as less error variance. Figure~\ref{tab.transfer} evaluates the variance of the volume estimation error.





\subsection{Shape Disentanglement and Evolution}
\begin{figure}[htbp!]
\vskip-0.2in
\centering
\includegraphics[width=1.\columnwidth]{figs/manifold.pdf}
\caption{Shape extrapolation in covariate space. Example shapes in the age-weight space are visualized with their volumes ($cm^3$) below. Cyan points represent male and purple points female children in the dataset. The points represent the covariates of all children in the dataset. The colored shades represent the age and weight distributions stratified by sex. The latent code $\mathbf{z}$ is kept constant to create an individualized covariate shape space. The shapes in the green and yellow boxes are plotted with $\{\Phi_i\}$ (see Sec.~\ref{sec.testing}), representing the disentangled shape evolutions along weight and age respectively. (Best viewed zoomed.) }
\label{fig.manifold}
\vskip-0.1in
\end{figure}


Fig.~\ref{fig.manifold} shows that \texttt{NAISR} is able to extrapolate reasonable shapes changes when varying age and weight. These results illustrate the capabilities of \texttt{NAISR} for shape disentanglement and to capture shape evolutions. Note that when evolving shapes along one covariate $c_i$,  the deformations from other covariates are naturally set to $0$ by our model construction (see Sec.~\ref{subsec.model_f}). As a result, the shapes in the yellow and green boxes in Figure~\ref{fig.manifold} represent the disentangled shape evolutions along age and weight respectively. The shapes in the other grid positions can be extrapolated using $\Phi(\cdot)$. By inspecting the volume changes along age and weight in Fig.~\ref{fig.manifold}, we observe that age is more important for airway volume than weight. These observations from our generated shapes are consistent with clinical expectations for pediatric airways~\cite{luscan2020developmental}, suggesting that \texttt{NAISR} is able to extract hidden knowledge from data and is able to generate interpretable results direcly as 3D shapes.

\section{Limitations and Future Work}
Invertible transformations are typically desirable for shape correspondence. However, our approach cannot guarantee invertibility. Invertibility could be guaranteed by representing deformations via velocity fields instead of directly parameterizing deformation fields. However, velocity field parameterizations are costly as they require numerical integration. Hence, training such velocity field models for implicit shapes would consume more time and GPU resources. In future work, we will develop more efficient invertible representations, which will then assure that shape topology is preserved by construction. %and will assure that  we can get point-wise evolution trajectories by evolving any point $\mathbf{p}$ on the template $\mathcal{T}$ to the source shape $\mathcal{S}$.
We also do not know the true effects of covariates on the shapes. We can so far only indirectly assess our model by quantifying reconstruction, evolution, and transfer performance. In the future, we will include patients with airway abnormalities (e.g., with subglottic stenosis, an airway disease affecting airway geometry) in our analyses. This would allow us to further assess our \texttt{NAISR} model by exploring if our estimated model of normal airway shape can be used to detect airway abnormalities. 

\section{Conclusion}
In this work, we proposed \texttt{NAISR}, a 3D neural additive model for interpretable shape representation. Compared to other shape representation methods, \texttt{NAISR} 1) captures the effect of individual covariates on shapes; 2) can transfer shapes to new covariates, e.g.,  to infer anatomy development; and 3) can provide shapes based on extrapolated covariates. %To the best of our knowledge, 
\texttt{NAISR} is the first approach combining deep implicit shape representations based on template deformation with the ability to account for covariates. Although our driving problem is to provide an atlas of the upper airway in children, our approach can be used for general interpretable shape representation and shape analysis.


%-------------------------------------------------------------------------

%%%%%%%%% TITLE

\clearpage
%\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\clearpage
\newpage
\appendix
\input{supplementary}


\end{document}
