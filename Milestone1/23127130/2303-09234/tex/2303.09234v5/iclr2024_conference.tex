
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{booktabs}

\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{fontawesome}


\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{svg}
\usepackage{comment}
\usepackage{mathrsfs}
\usepackage{xcolor}
%\usepackage{adjustbox}
\usepackage[dvipsnames,table,xcdraw]{colortbl}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{xr} 
\usepackage{caption}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{subfiles} 
\usepackage[T1]{fontenc}
\usepackage{tabu}
\usepackage{circuitikz}
\usepackage{tikz}
%\usepackage[affil-it]{authblk}

\usepackage[colorlinks=true]{hyperref}
\usepackage{url}
\hypersetup{citecolor=gray}

% \renewcommand\Authfont{\fontsize{9}{10.8}\selectfont}
% \renewcommand\Affilfont{\fontsize{10}{10.8}\itshape}
\captionsetup[figure]{font=small,labelfont=small}
\captionsetup[table]{font=small,labelfont=small}

\title{\texttt{NAISR}: A 3D Neural Additive Model \\ for Interpretable Shape Representation}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{Yining Jiao\textsuperscript{1}, Carlton Zdanski\textsuperscript{1}, Julia Kimbell\textsuperscript{1}, Andrew Prince\textsuperscript{1}, Cameron Worden\textsuperscript{1}, \\ \textbf{Samuel Kirse\textsuperscript{2}, Christopher Rutter\textsuperscript{3}, Benjamin Shields\textsuperscript{1}, William Dunn\textsuperscript{1}, Jisan Mahmud\textsuperscript{1}}, \\ \textbf{Marc Niethammer\textsuperscript{1}}\thanks{Corresponding author.}~; \textbf{for the Alzheimer's Disease Neuroimaging Initiative}\thanks{\small Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (\url{adni.loni.usc.edu}). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: \url{http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf}} 
\\\textsuperscript{1}University of North Carolina at Chapel Hill,
\textsuperscript{2}Wake Forest School of Medicine, \\\textsuperscript{3}The Ohio State University College of Medicine\\
%\texttt{\{jyn}, \texttt{mn\}@cs.unc.edu} 
%\texttt{\{zdanski}, \texttt{julia\_kimbell\}@med.unc.edu}, \\
% \texttt{\{cameron.worden},\texttt{andrew.prince\}@unchealth.unc.edu},\\
% \texttt{willad@live.unc.edu}, \texttt{bhs@email.sc.edu}, \texttt{Christopher.Rutter@osumc.edu}
}
% \author{Matthew Ho\thanks{Equal contribution} , Aditya Sharma\footnotemark[1] , Justin Chang\footnotemark[1] , \\
% \textbf{Michael Saxon, Sharon Levy, Yujie Lu, William Yang Wang}\\
% Department of Computer Science, University of California, Santa Barbara, USA\\
% \texttt{\{msho}, \texttt{aditya\_sharma}, \texttt{justin\_chang\}@ucsb.edu},\\
% \texttt{\{saxon}, \texttt{sharonlevy}, \texttt{yujielu\}@ucsb.edu}, \texttt{william @cs.ucsb.edu}\\
% }



%Wake Forest School of Medicine\textsuperscript{2}, The Ohio State University College of Medicine\textsuperscript{3}

% \author{Chenmien Tan\textsuperscript{1}\thanks{Work done while interning at HKUST.}, Ge Zhang\textsuperscript{2}\textsuperscript{3}\samethanks[1], Jie Fu\textsuperscript{4}\thanks{Corresponding author.}\\
% University of Edinburgh\textsuperscript{1}, University of Waterloo\textsuperscript{2}, 01.AI\textsuperscript{3}, HKUST\textsuperscript{4}\\
% \texttt{chenmien.tan@ed.ac.uk, gezhang@umich.edu, jiefu@ust.hk} \\
% % \And
% % Ge Zhang\samethanks[1]\\
% % University of Waterloo, 01.AI\\
% % \texttt{gezhang@umich.edu}\\
% % \And
% % Jie Fu \\
% % Hong Kong University of Science and Technology\\
% % \texttt{jiefu@ust.hk} \\
% }


% \author{
% Yining Jiao\\
% UNC-Chapel Hill\\
% %Institution1 address\\
% {\tt\small jyn@cs.unc.edu}
% \and
% Carlton Zdanski\\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small zdanski@med.unc.edu}
% \and
% Julia Kimbell\\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small julia\_kimbell@med.unc.edu}
% \and
% Andrew Prince\\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small andrew.prince@unchealth.unc.edu}
% \and
% Cameron Worden\\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small cameron.worden@unchealth.unc.edu}
% \and
% Samuel Kirse\\
% Wake Forest School of Medicine \\
% %First line of institution2 address\\
% {\tt\small kirse.sam@gmail.com}
% \and
% Christopher Rutter \\
% The Ohio State University College of Medicine\\
% %First line of institution2 address\\
% {\tt\small Christopher.Rutter@osumc.edu}
% \and
% Benjamin Shields  \\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small bhs@email.sc.edu}
% \and
% William Dunn  \\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small willad@live.unc.edu}
% \and
% Jisan Mahmud \\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small jisan@cs.unc.edu}
% \and
% Marc Niethammer \\
% UNC-Chapel Hill\\
% %First line of institution2 address\\
% {\tt\small mn@cs.unc.edu}
% }


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Deep implicit functions (DIFs) have emerged as a powerful paradigm for many computer vision tasks such as 3D shape reconstruction, generation, registration, completion, editing, and understanding. However, given a set of 3D shapes with associated covariates there is at present no shape representation method which allows to precisely represent the shapes while capturing the individual dependencies on each covariate. Such a method would be of high utility to researchers to discover knowledge hidden in a population of shapes. For scientific shape discovery, we propose a 3D Neural Additive Model for Interpretable Shape Representation (\texttt{NAISR}) which describes individual shapes by deforming a shape atlas in accordance to the effect of disentangled covariates. Our approach captures shape population trends and allows for patient-specific predictions through shape transfer. \texttt{NAISR} is the first approach to combine the benefits of deep implicit shape representations with an atlas deforming according to specified covariates. We evaluate \texttt{NAISR} with respect to shape reconstruction, shape disentanglement, shape evolution, and shape transfer on three datasets: 1) \textit{Starman}, a simulated 2D shape dataset; 2) the ADNI hippocampus 3D shape dataset; and 3) a pediatric airway 3D shape dataset. Our experiments demonstrate that \texttt{NAISR} achieves excellent shape reconstruction performance while retaining interpretability. \href{https://github.com/uncbiag/NAISR}{Our code} is publicly available.
\end{abstract}

\section{Introduction}

\label{sec:intro}
% DIF can represent different kinds of signals and can be cooperate with mathematical analysis tools like differential geometry to help us deeply explore the shape 
Deep implicit functions (DIFs) have emerged as efficient representations of 3D shapes~\citep{park2019deepsdf, novello2022exploring, mescheder2019occupancy, yang2021geometry}, deformation fields~\citep{wolterink2022registration}, images and videos~\citep{sitzmann2020siren}, graphs, and manifolds~\citep{grattarola2022generalised}. For example, DeepSDF~\citep{park2019deepsdf} represents shapes as the level set of a signed distance field (SDF) with a neural network. In this way, 3D shapes are compactly represented as continuous and differentiable functions with infinite resolution. In addition to representations of geometry such as voxel grids~\citep{wu2016learning, wu20153d, wu2018learning}, point clouds~\citep{achlioptas2018learning, yang2018foldingnet, zamorski2020adversarial} and meshes~\citep{groueix2018papier, wen2019pixel2mesh++, zhu2019detailed}, DIFs have emerged as a powerful paradigm for many computer vision tasks. DIFs are used for 3D shape reconstruction~\citep{park2019deepsdf, mescheder2019occupancy, sitzmann2020siren}, generation~\citep{gao2022get3d}, registration~\citep{deng2021DIF, zheng2021DIT, sun2022topology, wolterink2022registration}, completion~\citep{park2019deepsdf}, editing~\citep{yang2022neumesh} and understanding~\citep{palafox2022spams}. %In addition, traditional shapes analysis theories such as differential geometry and level set theory are revived to help better investigate shapes with DIFs~\citep{novello2022exploring, mehta2022level, sitzmann2020siren}. For example, many shape operators like curvature become compatible with DIFs since gradients and second-order gradients with respect to coordinates can easily be calculated for a coordinate-based neural network.

% A interpretable shape analysis is needed
Limited attention has been paid to shape analysis with DIFs. Specifically, given a set of 3D shapes with a set of covariates attributed to each shape, a shape representation method is still desired which can precisely represent shapes and capture dependencies among a set of shapes. There is currently no shape representation method that can quantitatively capture how covariates geometrically and temporally affect a population of 3D shapes; neither on average nor for an individual. However, capturing such effects is desirable as it is often difficult and sometimes impossible to control covariates (such as age, sex, and weight) when collecting data. Further,  understanding the effect of such covariates is frequently a goal of medical studies. Therefore, it is critical to be able to disentangle covariate shape effects on the individual and the population-level to better understand and describe shape populations. Our approach is grounded in the estimation of a shape atlas (i.e., a template shape) whose deformation allows to capture covariate effects and to model shape differences. Taking the airway as an example, a desired atlas representation should be able to answer the following questions:

\begin{itemize}
    \item Given an atlas shape, how can one accurately represent shapes and their dependencies?
    \item Given the shape of an airway, how can one disentangle covariate effects from each other? 
    \item Given a covariate, e.g., age, how does an airway atlas change based on this covariate?
    \item Given a random shape, how will the airway develop after a period of time?
\end{itemize}

% what properties we provide with our proposed method
To answer these questions, we propose a Neural Additive Interpretable Shape Representation (\texttt{NAISR}), an interpretable way of modeling shapes associated with covariates via a shape atlas. Table~\ref{tab.lit} compares  \texttt{NAISR} to existing shape representations with respect to the following properties: 
 \begin{itemize}
     \item \textbf{Implicit} relates to how a shape is described. % Generally, a shape can be represented explicitly (e.g., as point clouds, voxels, and meshes) and implicitly with a function, e.g., parameterized by a neural network. 
     Implicit representations are desirable as they naturally adapt to different resolutions of a shape and also allow shape completion (i.e., reconstructing a complete shape from a partial shape, which is common in medical scenarios) with no additional effort. %since inherently the coordinate-based neural network can process incomplete shapes (which is quite common in medical scenarios) with no additional effort.
     \item \textbf{Deformable} captures if a shape representation results in point correspondences between shapes, e.g., via a displacement field. Specifically, we care about point correspondences between the target shapes and the atlas shape. A deformable shape representation helps to relate different shapes.
     \item \textbf{Disentangleable} indicates whether a shape representation can disentangle individual covariate effects for a shape. These covariate-specific effects can then be composed to obtain the overall displacement of an atlas/template shape. %Although A-SDF~\citep{mu2021asdf} and 3DAttriFlow~\citep{wen20223d} can disentangle  articulations from geometry, they do not disentangle different covariates and their disentanglements are not composable.
     \item \textbf{Evolvable} denotes whether a shape representation can evolve the shape based on changes of a covariate, capturing the influence of \emph{individual} covariates on the shape. An evolvable atlas statistically captures how each covariate influences the shape population, e.g., in scientific discovery scenarios.
     \item \textbf{Transferable} indicates whether shape changes can be transferred to a given shape. E.g., one might want to edit an airway based on a simulated surgery and predict how such a surgical change manifests later in life. %given the shape of a particular anatomical structure edited by simulated surgery, there is usually a need to predict how the shape will evolve after a period of time to preview the treatment effect. 
     %In this case, the intrinsics of the anatomical structures stay the same while the covariates such as age and weight change. 
     \item \textbf{Interpretable} indicates a shape representation that is simultaneously \emph{deformable}, \emph{disentangleable}, \emph{evolvable}, and \emph{transferable}. Such an interpretable model is applicable to tasks ranging from the estimation of disease progression to assessing the effects of normal aging or weight gain on shape. 
 \end{itemize}

\texttt{NAISR} is the first implicit shape representation method to investigate an atlas-based representation of 3D shapes in a deformable, disentangleable, transferable and evolvable way.
To demonstrate the generalizability of \texttt{NAISR}, we test \texttt{NAISR} on a simulated dataset and two realistic medical datasets~\footnote{Medical shape datasets are our primary choice because quantitative shape analysis is a common need for scientific discovery for such datasets.}: 1) \textit{Starman}, a simulated 2D shape dataset~\citep{bone2020learning}; 2) the ADNI hippocampus 3D shape dataset~\citep{jack2008alzheimer}; and 3) a pediatric airway 3D shape dataset. We quantitatively demonstrate superior performance over the baselines. %However, \texttt{NAISR} is a general method to represent and investigate shapes in an interpretable way.

\begin{comment}
Specifically, we propose a NAISR (Neural Additive Interpretable Surface Representation) to analyze an atlas of shapes described by a set of covariates. We summarize our contribution as follows,


\begin{itemize}
    \item Given a covariates, NAISR is able to derive the shape evolution controled by the covariate.
    \item Given a point/region, NAISR is able to analyze how the covariates influence the point jointly.
    \item Given a shape, NAISR is able to show which region is the most sensitive to a particular covariate.

\end{itemize}
\end{comment}

\begin{table*}[t]
\vspace{-0.5in}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c}
\hline
Method & Implicit & Deformable  & Disentangleable &  Evolvable & Transferable & Interpretable\\
\hline\hline
ConditionalTemplate~\citep{dalca2019learning} &  \XSolidBrush & \Checkmark & \XSolidBrush  & \Checkmark & \XSolidBrush  & \XSolidBrush  \\
\hline
3DAttriFlow~\citep{wen20223d}  &  \XSolidBrush  & \Checkmark& \XSolidBrush  & \Checkmark & \XSolidBrush & \XSolidBrush  \\
\hline
DeepSDF~\citep{park2019deepsdf} & \Checkmark & \XSolidBrush  & \XSolidBrush  & \XSolidBrush  & \XSolidBrush  & \XSolidBrush  \\ 
\hline
A-SDF~\citep{mu2021asdf}  & \Checkmark   & \XSolidBrush  & \XSolidBrush   & \Checkmark & \Checkmark & \XSolidBrush  \\
\hline
DIT~\citep{zheng2021DIT}, DIF~\citep{deng2021DIF}, NDF~\citep{sun2022topology} & \Checkmark & \Checkmark & \XSolidBrush  & \XSolidBrush  & \XSolidBrush  & \XSolidBrush \\
\hline
NASAM~\citep{wei2022nasam} & \Checkmark & \Checkmark & \XSolidBrush  & \Checkmark & \XSolidBrush  & \XSolidBrush \\
\hline
Ours (\texttt{NAISR}) & \Checkmark   & \Checkmark & \Checkmark & \Checkmark & \Checkmark  & \Checkmark \\
\hline
\end{tabular}
}
\end{center}
\caption{Comparison of shape representations based on the desirable properties discussed in \Secref{sec:intro}. A \Checkmark indicates that a model has a property; a \XSolidBrush  indicates that it does not. Only \texttt{NAISR} has all the desired properties.}
\label{tab.lit}
\vspace{-0.24in}
\end{table*}

\section{Related Work}
We introduce the three most related research directions here. A more comprehensive discussion of related work is available in~\Secref{sec.supp_related_work} of the supplementary material. 

\paragraph{Deep Implicit Functions.} 
 Compared with geometry representations such as voxel grids~\citep{wu2016learning, wu20153d, wu2018learning}, point clouds~\citep{achlioptas2018learning, yang2018foldingnet, zamorski2020adversarial} and meshes~\citep{groueix2018papier, wen2019pixel2mesh++, zhu2019detailed}, DIFs are able to capture highly detailed and complex 3D shapes using a relatively small amount of data~\citep{park2019deepsdf, mu2021asdf, zheng2021DIT, sun2022topology, deng2021DIF}. They are based on classical ideas of level set representations~\citep{sethian1999level,osher2005level}; however, whereas these classical level set methods represent the level set function on a grid, DIFs parameterize it as a \emph{continuous function}, e.g., by a neural network. Hence, DIFs are not reliant on meshes, grids, or a discrete set of points. This allows them to efficiently represent natural-looking surfaces~\citep{gropp2020implicit, sitzmann2020siren, niemeyer2019occupancy}. Further, DIFs can be trained on a diverse range of data (e.g., shapes and images), making them more versatile than other shape representation methods. This makes them useful in applications ranging from computer graphics, to virtual reality, and robotics~\citep{gao2022get3d, yang2022neumesh, phongthawee2022nex360, shen2021deep}. \emph{We therefore formulate \texttt{NAISR} based on DIFs.}

%Another main advantage of DIF approaches is their flexibility when defining the optimization objective. This is because the implicit shape representation allows for computing shape related variables like higher order derivatives (related to curvatures) with auto-differentiation in a continuous field, which favors smooth and natural zero level set surfaces by focusing on regions with high curvatures~\citep{novello2022exploring, gropp2020implicit, sitzmann2020siren}. 

\begin{comment}
    
DeepSDF~\citep{park2019deepsdf} uses a coordinate based neural network to learn the signed distance function of a shape. Specifically, given any query point $\mathbf{p}=(x, y, z)$, the parameterized implicit function $SDF(\boldsymbol{x})=s: \boldsymbol{x} \in \mathbb{R}^{3}, s \in \mathbb{R}$  maps the point to the signed distance value. to models representing continuous SDF.
Many following works tries to investigate and improve DeepSDF in terms of regularization~\citep{gropp2020implicit}, model design~\citep{sitzmann2020siren}, and optimization~\citep{duan2020curriculum} . 
\end{comment}
    

% \paragraph{Point Correspondences.} 
% Establishing point correspondences is important to help experts to detect, understand, diagnose, and track diseases. Recently, ImplicitAtlas~\citep{yang2022implicitatlas}, DIF-Net~\citep{deng2021DIF}, DIT~\citep{zheng2021DIT}, and NDF~\citep{sun2022topology} were proposed to capture  point correspondence within implicit shape representations. %DIT~\citep{zheng2021DIT} applies an LSTM to obtain smooth deformations, while NDF~\citep{sun2022topology} uses a neural ODE to obtain a smooth, invertible deformation which preserves shape topology. 
% Dalca et al.~\citep{dalca2019learning} use templates conditioned on covariates for image registration. However, they did not explore covariate-specific deformations, shape representations or shape transfer. Currently no continuous shape representation which models the effects of covariates exists. \emph{\texttt{NAISR} will provide a model with such capabilities.}

\paragraph{Neural Deformable Models}
Neural Deformable Models (NDMs) establish point correspondences with DIFs. In computer graphics, there has been increasing interest in NDMs to animate scenes~\citep{liu2022devrf, bao2023sine, zheng2023editablenerf}, objects~\citep{lei2022cadex, bao2023sine, zhang2023self}, and digital humans~\citep{ park2021hypernerf, zhang2022ndf, niemeyer2019occupancy}. Establishing point correspondences is also important to help experts to detect, understand, diagnose, and track diseases. NDMs have shown to be effective in exploring point correspondences for medical images~\citep{han2023diffeomorphic, tian2023nephi, wolterink2022implicit, zou2023homeomorphic} and shapes~\citep{sun2022topology, yang2022implicitatlas} Among the NDMs for shape representations, ImplicitAtlas~\citep{yang2022implicitatlas}, DIF-Net~\citep{deng2021DIF}, DIT~\citep{zheng2021DIT}, and NDF~\citep{sun2022topology} capture point correspondences between target and template shapes within NDMs. Currently, no continuous deformable shape representation which models the effects of covariates exists. \emph{\texttt{NAISR} provides a model with such capabilities.}

% \paragraph{Neural Deformable Models}
% \textcolor{blue}{Neural Deformable Models (NDMs) attempts to explore point correspondence with DIFs. In computer graphics, there has been increasing interest in NDMs to animate or edit scenes~\citep{liu2022devrf, park2021nerfies, bao2023sine, zheng2023editablenerf}, objects~\citep{duggal2022topologicallysinglebiew, lei2022cadex, bao2023sine, niemeyer2019occupancy, zhang2023self,shuai2023dpf}, and digital humans~\citep{liu2021neuralactor, chen2021snarf, niemeyer2019occupancy, park2021hypernerf,park2021nerfies, peng2021animatable, zhang2022ndf, zhang2023self}. }

% \textcolor{blue}{Establishing point correspondences is also important to help experts to detect, understand, diagnose, and track diseases. NDMs has been proved effective in exploring point correspondence for medical images~\citep{han2023diffeomorphic, tian2023nephi, sun2022mirnf, wolterink2022implicit, zou2023homeomorphic} and shapes~\citep{sun2022topology, yang2022implicitatlas, han2023hybrid}}

% \textcolor{blue}{Among NDMs for shape representations, ImplicitAtlas~\citep{yang2022implicitatlas}, DIF-Net~\citep{deng2021DIF}, DIT~\citep{zheng2021DIT}, and NDF~\citep{sun2022topology} were proposed to capture point correspondence between target and template shapes within NDMs.  Currently, no continuous deformable shape representation which models the effects of covariates exists. \emph{\texttt{NAISR} provides a model with such capabilities.}}

\begin{comment}

Zhang et al. [60] introduced DRL to the field of gait recognition for the first time, where pose and appearance features of subjects were disentangled from RGB imagery. Although DRL is new in studies on gait, it has been well explored in studies on other biometrics (i.e., face recognition). For example, Tran et al. [45] and Peng et al. [40] disentangled pose variation from face images for pose-invariant face recognition, while Zhao et al. [61] generated age-invariant face features through the disentanglement of age variation.

In contrast with [60], our method avoids the difficulty of the disentanglement of RGB information and gives new meaningful disentangled variables (i.e., identity and covariate features) for silhouette-based gait representation. Compared with DRL in face recognition [45, 40, 61], which requires additional covariate labels (e.g., pose or age labels), 
\end{comment}



\paragraph{Explainable Artificial Intelligence.}
The goal of eXplainable Artificial Intelligence (XAI) is to provide human-understandable explanations for decisions and actions of a AI model. Various flavors of XAI exist, including counterfactual inference~\citep{berrevoets2021learning, moraffah2020causal, thiagarajan2020calibrating, chen2022covariate}, attention maps~\citep{zhou2016cvpr, jung2021towards,woo2018cbam}, feature importance~\citep{arik2021tabnet, ribeiro2016should, agarwal2020neural}, and instance retrieval~\citep{Crabbe2021Simplex}.  \texttt{NAISR} is inspired by neural additive models (NAMs)~\citep{agarwal2020neural} which in turn are inspired by generalized additive models (GAMs)~\citep{hastie2017generalized}. NAMs are based on a linear combination of neural networks each attending to a \emph{single} input feature, thereby allowing for interpretability. \texttt{NAISR} extends this concept to interpretable 3D shape representations. This is significantly more involved as, unlike for NAMs and GAMs, we are no longer dealing with scalar values, but with 3D shapes. \emph{\texttt{NAISR} will provide interpretable results by capturing spatial deformations with respect to an estimated atlas shape such that individial covariate effects can be distinguished.}


\section{Method}
\label{sec.method_model_arch}
This section disccuses our \texttt{NAISR} model and how we obtain the desired model properties of Section~\ref{sec:intro}.
 
\begin{figure*}[ht]
\vskip -0.5in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{figs/NAISRoverviewwide.pdf}}
\caption{\small \textbf{Neural Additive Implicit Shape Representation.} During training we learn the template $\mathcal{T}$ and the multi-layer perceptrons (MLPs)  $\{g_i\}$ predicting the covariate-wise displacement fields $\{\mathbf{d}_i\}$. The displacement fields are added to obtain the overall displacement field $\mathbf{d}$ defined in the target space;  $\mathbf{d}$ provides the displacement between the deformed template shape $\mathcal{T}$ and the target shape. Specifically the template shape is queried not at its original coordinates $\mathbf{p}$, but at $\Tilde{\mathbf{p}}=\mathbf{p}+\mathbf{d}$ effectively spatially deforming the template. At test time we evaluate the trained MLPs for shape reconstruction, evolution, disentanglement, and shape transfer.}
\label{fig.method}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Problem Description}
Consider a set of shapes $\mathcal{S}=\{\mathcal{S}^k\}$ where each shape $\mathcal{S}^k$ has an associated vector of covariates $\mathbf{c}=[c_1, ..., c_i, ..., c_N]$ (e.g., age, weight, sex). Suppose $\{\mathcal{S}^k\}$ are well pre-aligned and centered (e.g., based on Iterative Closest Point (ICP) registration~\citep{arun1987least} or landmark registration; see Section~\ref{supp.sec.dataset} for details). Our goal is to obtain a representation which describes the entire set $\mathcal{S}$ while accounting for the covariates. Our approach estimates a template shape, $\mathcal{T}$ (the shape atlas), which approximates $\mathcal{S}$. Specifically, $\mathcal{T}$ is deformed based on a set of displacement fields $\{\mathcal{D}^k\}$ such that the individual shapes $\{\mathcal{S}^k\}$ are approximated well by the transformed template.
%Therefore, a particular shape ${\mathcal{S}}$ can be represented with the displacement $\mathcal{D}$ and the template shape $\mathcal{T}(\cdot)$ as $\mathcal{T}(\mathcal{S} + \mathcal{D})$. 

A displacement field $\mathcal{D}^k$ describes how a shape is related to the template shape $\mathcal{T}$. The factors that cause this displacement may be directly observed or not. For example, observed factors may be covariates such as subject age, weight, or sex; i.e., $\mathbf{c}_k$ for subject $k$. Factors that are not directly observed may be due to individual shape variation, unknown or missing covariates, or variations due to differences in data acquisition or data preprocessing errors. We capture these not directly observed factors by a latent code $\mathbf{z}$. The covariates $\mathbf{c}$ and the latent code $\mathbf{z}$ then contribute jointly to the displacement $\mathcal{D}$ with respect to the template shape $\mathcal{T}$.

Inspired by neural additive~\citep{agarwal2020neural} and generalized additive~\citep{hastie2017generalized} models, we assume the overall displacement field is the sum of displacement fields that are controlled by individual covariates: $\mathcal{D}=\Sigma_i \mathcal{D}_i$. Here, $\mathcal{D}_i$ is the displacement field controlled by the i-th covariate, $c_i$. This results by construction in an overall displacement $\mathcal{D}$ that is disentangled into several sub-displacement fields $\{\mathcal{D}_i\}$. %Conversely, the sub-displacement field $\{\mathcal{D}_i\}$ controlled by the covariates $\mathbf{c}$ jointly determine the overall displacement $\mathcal{D}$.

\subsection{Model Formulation}
\label{subsec.model_f}
\Figref{fig.method} gives an overview of \texttt{NAISR}. To obtain a continuous atlas representation, we use DIFs to represent both the template $\mathcal{T}$ and the displacement field $\mathcal{D}$. The template shape $\mathcal{T}$ is represented by a signed distance function, where the zero level set $\{\mathbf{p} \in \mathbb{R}^3 | \mathcal{T}(\mathbf{p})=0\}$ captures the desired template shape. %any point $\mathbf{p} \in \mathbb{R}^3$ in the template space is mapped to the signed distance to the surface $\mathcal{S}$ as $s=\mathcal{T}(\mathbf{\mathbf{p}}), s \in \mathbb{R}$. The zero level set $\{\mathbf{p} \in \mathbb{R}^3 | \mathcal{T}(\mathbf{p})=0\}$ of the signed distance function $\mathcal{T}(\cdot)$ is the point set on the template surface. 
A displacement $\mathcal{D}_i$ of a particular point $\mathbf{p}$ can also be represented as a function $\mathbf{d}_i = f_i(\mathbf{p}, c_i, \mathbf{z}) \in \mathbb{R}^3$. We use \texttt{SIREN}~\citep{sitzmann2020siren} as the backbone for $\mathcal{T}(\cdot)$ and $\{f_i(\cdot)\}$. Considering that the not directly observed factors might influence the geometry of all covariate-specific networks, we make the latent code, $\mathbf{z}$, visible to all subnetworks $\{f_i(\cdot)\}$. We normalize the covariates so that they are centered at zero. To assure that a zero covariate value results in a zero displacement we parameterize the displacement fields as $\mathbf{d}_i = g_i(\mathbf{p}, c_i, \mathbf{z})$ where
\begin{equation}
g_i(\mathbf{p}, c_i, \mathbf{z}) = f_i(\mathbf{p}, c_i, \mathbf{z}) - f_i(\mathbf{p}, 0, \mathbf{z})\,.
\label{eq.disp_1}
\end{equation}

The sub-displacement fields are added to obtain the overall displacement field
\begin{equation}
\mathbf{d} = g(\mathbf{p}, \mathbf{c}, \mathbf{z}) = \sum_{i=1}^N g_i(\mathbf{p}, c_i,\mathbf{z})\,.
\label{eq.disp_2}
\end{equation}

%Based on this displacement field 
We then deform the template shape $\mathcal{T}$ to obtain an implicit representation of a target shape
\begin{equation}
s = \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z}) = \mathcal{T}(\Tilde{\mathbf{p}}) = \mathcal{T}(\mathbf{p} + \sum_{i=1}^Ng_i(\mathbf{p}, c_i, \mathbf{z}))\,,
\end{equation}
where $\mathbf{p}$ is a point in the source shape space, e.g., a point on the surface of shape $\mathcal{S}_i$ and $\Tilde{\mathbf{p}}$ represents a point in the template shape space, e.g., a point on the surface of the template shape $\mathcal{T}$. To investigate how an individual covariate $c_i$ affects a shape we can simply extract the zero level set of
\begin{equation}
s_i = \Phi_i(\mathbf{p}, c_i, \mathbf{z})=\mathcal{T}(\mathbf{p}+\mathbf{d}_i) = \mathcal{T}(\mathbf{p}+g_i(\mathbf{p}, c_i, \mathbf{z}))\,.
 \end{equation}
 
\begin{comment}
The latent code $\mathbf{z}$ represents the information irrelevant to the covariates. We take the airway shape atlas as an example, the $\mathbf{z}$ here should include the information of airway pose, i.e., the person's head angle during scanning, and dynamic shape change due to breathing, and error from data preprocessing (e.g., segmentaiton). 



\begin{equation}
\mathbf{d}_i = f_i(\mathbf{p}, c_i, \mathbf{z})
\end{equation}

In order to eliminate the shift from the latent code $\mathbf{z}$, we normalize the displacement field by construction 
\begin{equation}
g_i(\mathbf{p}, c_i, \mathbf{z})= f_i(\mathbf{p}, c_i, \mathbf{z}) - f_i(\mathbf{p}, 0, \mathbf{z}) 
\end{equation}
This might sacrifice the performance because when the covaraites are all at average, the displacement field will become zeros regardless of the latent codes.

 and all of the sub-displacement field can be aggregate together by additive operation as ,


\begin{equation}
g(\mathbf{p}, \mathbf{c}, \mathbf{z}) = \sum_{i=1}^C g_i(\mathbf{p}, c_i, \mathbf{z})
\end{equation}
can  These descriptors includes the covariates and latent codes.
There the shape representation can be formulated as,

\begin{equation}
s = \mathcal{T}(\mathbf{p}+\mathbf{d}) \\
  = \mathcal{T}(\mathbf{p} + \sum_{i=11}^Cg_i(\mathbf{p}, c_i, \mathbf{z}))
\end{equation}

\end{comment}


\subsection{Training}

% \paragraph{Training Strategy}
% %To encourage disentanglement of covariates $\mathbf{c}$ from latent vectors $\mathbf{z}$, 
% We use the zero-padding strategy of~\citep{li2020gait, martinbrualla2020nerfw} to minimize the influence of the latent code $\mathbf{z}$ on shape differences that should be explained by the covariates $\mathbf{c}$. By also setting $\mathbf{z}$ to $\mathbf{0}$ while optimizing the objective function, the model is pushed to rely on $\mathbf{c}$ as much as possible. 

\paragraph{Losses.}
All our losses are ultimately summed over all shapes of the training population with the appropriate covariates $\mathbf{c}_k$ and shape code $\mathbf{z}_k$ for each shape $\mathcal{S}^k$. For ease of notation, we describe them for individual shapes. %in the following. 
For each shape, we sample on-surface and off-surface points. On-surface points have zero signed distance values and normal vectors extracted from the gold standard\footnote{In medical imaging, there is typically no groundtruth. We use \textit{gold standard} to indicate shapes based off of manual or automatic segmentations, which are our targets for shape reconstruction.} mesh. Off-surface points have non-zero signed distance values but no normal vectors. Our  reconstruction losses follow~\cite {sitzmann2020siren, novello2022exploring}. For points on the surface, the losses are
\begin{equation}
    \mathcal{L}_{\mathrm{on}} (\Phi, \mathbf{c}, \mathbf{z}) = \int_{\mathcal{S}} \lambda_{1} \underbrace{\left\|\left|\nabla_{\mathbf{p}} \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})\right|-1\right\|}_{\mathcal{L}_{\text{Eikonal}}} \\ 
     + \lambda_{2} \underbrace{\|\Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})\|}_{\mathcal{L}_{\text{Dirichlet}}} + \lambda_{3} \underbrace{\left(1-\left\langle\nabla_{\mathbf{p}} \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z}), \mathbf{n}(\mathbf{p})\right\rangle\right)}_{\mathcal{L}_{\text {Neumann}}} d \mathbf{p} \,,
\end{equation}
where $\mathbf{n}(\mathbf{p})$ is the normal vector at $\mathbf{p}$ and $\langle \cdot \rangle$ denotes cosine similarity. For off-surface points, we use
\begin{equation} 
\begin{aligned}
\mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{z}) = \int_{\Omega \backslash \mathcal{S}} \lambda_4 \underbrace{|\Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})-s_{tgt}(\mathbf{p})|}_{\mathcal{L}_{\text{Dirichlet}}}  + \lambda_{5} \underbrace{\left\|\left|\nabla_{\mathbf{p}} \Phi(\mathbf{p}, \mathbf{c}, \mathbf{z})\right|-1\right\|}_{\mathcal{L}_{\text{Eikonal}}} d \mathbf{p}\,,
\end{aligned}
\end{equation}
where $s_{tgt}(\mathbf{p})$ is the signed distance value at $\mathbf{p}$ corresponding to a given target shape. Similar to~\citep{park2019deepsdf, mu2021asdf} we penalize the squared $L^2$ norm of the latent code $z$ as
\begin{equation}
    \mathcal{L}_{\mathrm{lat}}(\mathbf{z}) = \lambda_{6}\frac{1}{\sigma^{2}}\left\|\boldsymbol{z}\right\|_{2}^{2}\,.
\end{equation}

\begin{comment}
Further, we use an approximate inverse consistency loss~\citep{greer2021icon, tian2022gradicon} to regularize the displacement fields. The basic idea is that the approximate inverse transformation $g_i^{-1}(\cdot)$ of $g_i(\cdot)$ should be able to map a point in the template space back to the point in the source space where
\begin{equation}
\begin{aligned}
g_i^{-1}(\Tilde{\mathbf{p}}) &= g_i(\mathbf{p+\mathbf{d}_i}, 0, c_i, \mathbf{z}) \\
&= f_i(\mathbf{p+\mathbf{d}_i}, 0, \mathbf{z}) - f_i(\mathbf{p+\mathbf{d}_i}, c_i, \mathbf{z})\,. 
\end{aligned}
\end{equation}

The inverse consistency loss can then be written as
\begin{equation}
\mathcal{L}_{\mathrm{inv}}(\{g_i\}, \mathbf{z}) = \lambda_7\int_{\mathcal{S}} \Sigma_{i=1}^{N} \|(-\mathbf{d}_i) - g_i^{-1}(\mathbf{p+\mathbf{d}_i})\| d \mathbf{p} \,.
\label{eq.inv_loss}
\end{equation}


As a result, our overall loss (for a given shape) is
\begin{equation}
\begin{aligned}
&\mathcal{L}(\Phi, \{g_i\}, \mathbf{c}, \mathbf{z}) = \underbrace{\mathcal{L}_{\mathrm{lat}}(\mathbf{z})}_{\mathcal{L}_{latent\_space}} \\
&+ \underbrace{\mathcal{L}_{\mathrm{on}}(\Phi, \mathbf{c}, \mathbf{z}) 
 + \mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{z}) 
 + \mathcal{L}_{\mathrm{inv}}(\{g_i\}, \mathbf{z})}_{\mathcal{L}_{reconstrution}} \\
 &+ \underbrace{\mathcal{L}_{\mathrm{on}}(\Phi, \mathbf{c}, \mathbf{0}) 
 + \mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{0}) 
 + \mathcal{L}_{\mathrm{inv}}(\{g_i\}, \mathbf{0})}_{\mathcal{L}_{zero\_padding}} \,,
\end{aligned}
\end{equation}
where the parameters of $\Phi$, $\{g_i\}$, and $\mathbf{z}$ are trainable.
\end{comment}


As a result, our overall loss (for a given shape) is
\begin{equation}
\begin{aligned}
\mathcal{L}(\Phi, \mathbf{c}, \mathbf{z}) = \underbrace{\mathcal{L}_{\mathrm{lat}}(\mathbf{z})}_{\text{as regularizer}} + \underbrace{\mathcal{L}_{\mathrm{on}}(\Phi, \mathbf{c}, \mathbf{z}) 
 + \mathcal{L}_{\mathrm{off}}(\Phi, \mathbf{c}, \mathbf{z})}_{\text{for reconstrution}} \,,
\end{aligned}
\end{equation}
where the parameters of $\Phi$ and $\mathbf{z}$ are trainable.


\subsection{Testing}
\label{sec.testing}
As shown in \Figref{fig.method}, our proposed \texttt{NAISR} is designed for shape reconstruction, shape disentanglement, shape evolution, and shape transfer. %Further, \texttt{NAISR} also allows for shape interpolation, shape completion, and shape correspondence by design as in~\citep{zheng2021DIT, sun2022topology}.

\paragraph{Shape Reconstruction and Generation.}
As illustrated in the inference section in \Figref{fig.method}, a shape $s_{tgt}$ is given and the goal is to recover its corresponding latent code $\mathbf{z}$ and the covariates $\mathbf{c}$. %This can be solved by backpropagation. 
To estimate these quantities, the network parameters stay fixed and we optimize over the covariates $\mathbf{c}$ and the latent code $\mathbf{z}$ which are both randomly initialized~\citep{park2019deepsdf, mu2021asdf}. Specifically, we solve the optimization problem
\begin{equation}
\hat{\mathbf{c}}, \hat{\mathbf{z}}=\underset{\mathbf{c}, \mathbf{z}}{\arg \min }~ \mathcal{L}(\Phi, \mathbf{c}, \mathbf{z})\,.
\label{eq.infer_cz}
\end{equation}
In clinical scenarios, the covariates $\mathbf{c}$ might be known (e.g., recorded age or weight at imaging time). In this case, we only infer the latent code $\mathbf{z}$ by the optimization
\begin{equation}
    \hat{\mathbf{z}}= \underset{\mathbf{z}}{\arg \min }~\mathcal{L}(\Phi,\mathbf{c}, \mathbf{z})\,.
\label{eq.infer_z}
\end{equation}
A new patient shape with different covariates can be generated by extracting the zero level set of $\Phi(\mathbf{p}, \mathbf{c}_{\mathrm{new}}, \hat{\mathbf{z}})$. 


\paragraph{Shape Evolution.}
\label{para.evo}
Shape evolution along covariates $\{c_i\}$ is desirable in shape analysis to obtain knowledge of disease progression or population trends in the shape population $\mathcal{S}$. For a time-varying covariate $(c_i^0, ...,c_i^t, ...,c_i^T)$, we obtain the corresponding shape evolution by $(\Phi_i(\mathbf{p}, c_i^0, \hat{\mathbf{z}}), ...,\Phi_i(\mathbf{p}, c_i^t, \hat{\mathbf{z}}),..., \Phi_i(\mathbf{p}, c_i^T, \hat{\mathbf{z}}))$. If some covariates are correlated (e.g., age and weight), we can first obtain a reasonable evolution of the covariates $(\mathbf{c}^0, ..., \mathbf{c}^t, ..., \mathbf{c}^T)$ and the corresponding shape evolution as  $(\Phi(\mathbf{p}, \mathbf{c}^0, \hat{\mathbf{z}}), ...,\Phi(\mathbf{p}, \mathbf{c}^t,\hat{\mathbf{z}}),..., \Phi(\mathbf{p}, \mathbf{c}^T, \hat{\mathbf{z}}))$. By setting $\hat{\mathbf{z}}$ to $\mathbf{0}$, one can observe how a certain covariate influences the shape population on average.

\paragraph{Shape Disentanglement.}
As shown in the disentanglement section in \Figref{fig.method}, the displacement for a particular covariate $c_i$ displaces point $\mathbf{p}$ in the source space to $\mathbf{p}+\mathbf{d}_i$ in the template space for a given or inferred $\hat{\mathbf{z}}$ and $c_i$. We obtain the corresponding signed distance field as
\begin{equation}
s_i=\Phi_i(\mathbf{p}, c_i, \hat{\mathbf{z}})=\mathcal{T}(\mathbf{p} + \mathbf{d}_i) = \mathcal{T}(\mathbf{p} + g_i(\mathbf{p}, c_i, \hat{\mathbf{z}}))\,.
\end{equation}
As a result, the zero level sets of $\{\Phi_i(\cdot)\}$ represent shapes warped by the sub-displacement fields controlled by $c_i$.

\paragraph{Shape Transfer.}
We use the following clinical scenario to introduce the shape transfer task. Suppose a doctor has conducted simulated surgery on an airway shape with the goal of previewing treatment effects on the shape after a period of time. This question can be answered by our shape transfer approach. Specifically, as shown in the transfer section in \Figref{fig.method}, after obtaining the inferred latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}$ from reconstruction, one can transfer the shape from the current covariates $\hat{\mathbf{c}}$ to new covariates $\hat{\mathbf{c}}+{\Delta \mathbf{c}}$ with $\Phi(\mathbf{p},\hat{\mathbf{c}}+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$. As a result, the transferred shape is a prediction of the outcome of the simulated surgery; it is the zero level set of $\Phi(\mathbf{p},\hat{\mathbf{c}}+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$. In more general scenarios, the covariates are unavailable but it is possible to infer them from the measured shapes themselves (see Eqs.~\ref{eq.infer_cz}-\ref{eq.infer_z}). %~\footnote{For example, cancer patients have some specialized blood tests while the normal control group does not. However, one might want to jointly analyze both groups of shapes with covariates including blood tests. }. 
Therefore, in shape transfer we are not only evolving a shape, but may also first estimate the initial state to be evolved. 

%We borrow the idea of deformation transfer to define our shape transfer task. As shown in the upper section of Figure~\ref{fig.ex_transfer}, the goal of shape transfer is to infer how a new shape (purple star) should deform in accordance with the deformation for a given pair of source and target shapes (cyan stars). In real scenarios, it is even difficult to get the pairs of cyan stars yet both forward and backward directions of displacements are needed, as the bottom section of Figure~\ref{fig.ex_transfer}. For example, there are many shapes from different patients collected at different time points. To infer what a shape will be like two years later for a certain patient with only one observation is a common but challenging question to answer. With \texttt{NAISR} we can easily answer such a question in a quantitative and continuous way.
%As shown in the transfer section in Figure~\ref{fig.method}, our \texttt{NAISR} is capable of transfering shapes with the inferred latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}$ for the current observation. Specifically, by evolving the current covariates $\hat{\mathbf{c}}$ to new covariates $\hat{\mathbf{c}}+{\mathbf{c}}$, the shape can be transfered to former or later states with $\Phi(\mathbf{p},\hat{\mathbf{c}}+{\mathbf{c}}, \hat{\mathbf{z}})$. 

% \begin{figure}[ht]
% \begin{center}
% \centerline{\includesvg[width=0.7\columnwidth]{figs/example.svg}}
% \caption{An example of parallel transfer for shapes}
% \label{fig.ex_transfer}
% \vskip -0.4in
% \end{center}
% \end{figure}

\section{Experiments}
We evaluate \texttt{NAISR} in terms of shape reconstruction, shape disentanglement, shape evolution, and shape transfer on three datasets: 1) \textit{Starman}, a simulated 2D shape dataset used in~\citep{bone2020learning}; 2) the ADNI hippocampus 3D shape dataset~\citep{petersen2010alzheimer}; and 3) a pediatric airway 3D shape dataset. \textit{Starman} serves as the simplest and ideal scenario where sufficient noise-free data for training and evaluating the model is available.  While the airway and hippocampus datasets allow for testing on real-world problems of scientific shape analysis, which motivates \texttt{NAISR}.

We can quantitatively evaluate $\texttt{NAISR}$ for shape reconstruction and shape transfer because our dataset contains longitudinal observations. For shape evolution and shape disentanglement, we provide visualizations of shape extrapolations in covariate space to demonstrate that our method can learn a reasonable representation of the deformations governed by the covariates.

Implementation details and ablation studies are available in \Secref{subsec:implementation_details} and \Secref{subsec.ablation_study} in the supplementary material.


\subsection{Dataset and Experimental Protocol}
%\subsection{Experimental Protocol}
%\subsubsection{Dataset.} 
\textbf{\textit{Starman} Dataset.} This is a synthetic 2D shape dataset obtained from a predefined model as illustrated in~\Secref{subsec.starman_dataset} without additional noise. As shown in Fig.~\ref{fig.starmandataset}, each starman shape is synthesized by imposing a random deformation representing individual-level variation to the template starman shape. This is followed by a covariate-controlled deformation to the individualized starman shape, representing different poses produced by a starman. 5041 shapes from 1000 starmans are synthesized as the training set; 4966 shapes from another 1000 starmans are synthesized as a testing set. 

\textbf{ADNI Hippocampus Dataset.} These hippocampus shapes were obtained from the  Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The dataset consists of 1632 hippocampus segmentations from magnetic resonance (MR) images. We use an 80\%-20\% train-test split by patient (instead of shapes); i.e., a given patient cannot simultaneously be in the train and the test set, and therefore no information can leak between these two sets. As a result, the training set consists of 1297 shapes while the testing set contains 335 shapes. Each shape is associated with 4 covariates (age, sex, AD, education length). AD is a binary variable indicating whether a person has Alzheimer disease.  

\textbf{Pediatric Airway Dataset.} This dataset contains 357 upper airway shapes to evaluate our method. These shapes are obtained from automatic airway segmentations of computed tomography (CT) images of children with a radiographically normal airway. These 357 airway shape are from 263 patients, 34 of whom have longitudinal observations and 229 of whom have only been observed once. We use a 80\%-20\% train-test split by patient (instead of shapes). Each shape has 3 covariates (age, weight, sex). % and 11 annotated anatomical landmarks. Errors in the shapes $\{\mathcal{S}^k\}$ may arise from image segmentation error, differences in head positioning, missing parts of the airway shapes due to incomplete image coverage, and dynamic airway deformations due to breathing.

More details, including demographic information, visualizations, and preprocessing steps of the datasets are available in \Secref{supp.sec.dataset} in the supplementary material.
\begin{comment}

\textbf{Pediatric Airway Dataset} contains 357 airway shapes to evaluate our method. These airway shapes are obtained from automatic airway segmentations of computed tomography (CT) images of the upper airway of children with a radiographically normal airway. These 357 airway shape are from 263 patients, 34 of whom have longitudinal observations and 229 of whom have only been observed once. We use a 80\%-20\% train-test split by patient (instead of shapes); i.e., a given patient cannot simultaneously be in the train and the test set and therefore no information can leak between these two sets. Each shape has 3 covariates (age, weight, sex) and 11 annotated anatomical landmarks. Errors in the shapes $\{\mathcal{S}^k\}$ may arise from image segmentation error, differences in head positioning, missing parts of the airway shapes due to incomplete image coverage, and dynamic airway deformations due to breathing. %As shown in Figure~\ref{fig.dataset}, the error may arise from image segmentation error, head angle to the airway, missing airway shapes due to incomplete image range, and dynamic area due to breathing. 
\end{comment}


\begin{comment}
\begin{figure}[htbp]
    \centering
    \subfigure[Airway shape]{
        \includegraphics[width=0.75in]{figs/TVC.png}
    }
    \subfigure[Shapes and covariates]{
	\includegraphics[width=2.3in]{figs/dataset.png}
    }
    \caption{This is a Demo of $2\times 2$}
    \vskip -0.3in
    \label{fig.dataset}
\end{figure}
\end{comment}

\begin{comment}
\paragraph{Data Processing}
The shape meshes are extracted using Marching Cubes~\citep{lorensen1987marching, van2014scikit} to obtain coordinates and normal vectors of on-surface points. The airway shapes are rigidly aligned using the anatomical landmarks. The true vocal cords landmark is set to the origin. We follow the implementation in~\citep{park2019deepsdf} to sample 500,000 off-surface points. During training, it is important to preserve the scale information. We therefore scale all meshes with the same constant. %The resulting meshes from reconstruction are extracted with marching cubes~\citep{lorensen1987marching,van2014scikit}.
\end{comment}


\paragraph{Comparison Methods.} For shape reconstruction of unseen shapes, we compare our method on the test set with DeepSDF~\citep{park2019deepsdf}, A-SDF~\citep{mu2021asdf}, DIT~\citep{zheng2021DIT}, and NDF~\citep{sun2022topology}. For shape transfer, we compare our method with A-SDF~\citep{mu2021asdf} because other comparison methods cannot model covariates as summarized in Table ~\ref{tab.lit}. %The original implementations of the comparison methods did not produce satisfying reconstructions on our dataset. We therefore improved them by using our reconstruction losses and by using the \texttt{SIREN} backbone~\citep{sitzmann2020siren} in DeepSDF~\citep{park2019deepsdf}, A-SDF~\citep{mu2021asdf}, and the template networks in DIT~\citep{zheng2021DIT} and NDF~\citep{sun2022topology}.%DeepSDF~\citep{park2019deepsdf} and A-SDF~\citep{mu2021asdf} directly use MLP layers to map 3d coordinates to the distance from the surface while DIT~\citep{zheng2021DIT} and NDF~\citep{sun2022topology} calculate the displacement to warp the source shape to the template.
 
\paragraph{Metrics.} For evaluation, all target shapes and reconstructed meshes are normalized to a unit sphere to assure that large shapes and small shapes contribute equally to error measurements.  We use the Hausdorff distance, Chamfer distance, and earth mover's distance to evaluate the performance of our shape reconstructions. For shape transfer, considering that a perfectly consistent image acquisition process is impossible for different observations (e.g., head positioning might slightly vary across timepoints for the airway data), we visualize the transferred shapes and evaluate based on the difference between the \emph{volumes} of the reconstructed shapes and the target shapes on the hippocampus and airway dataset.

\subsection{Shape Reconstruction}
\label{subsec.shape_reconstruction}
The goal of our shape reconstruction experiment is to demonstrate that \texttt{NAISR} can provide competitive reconstruction performance while providing interpretability. Table ~\ref{tab.recons} shows the quantitative evaluations and demonstrates the excellent reconstruction performance of \texttt{NAISR}. \Figref{fig.exp_recons} visualizes reconstructed shapes. We observe that implicit shape representations can complete missing shape parts which can benefit further shape analysis. A-SDF~\citep{mu2021asdf} works well for representing \textit{Starman} shapes but cannot reconstruct real 3D medical shapes successfully. The reason might be the time span of our longitudinal data for each patient is far shorter than the time span across the entire dataset, mixing individual differences and covariate-controlling differences. A-SDF may fail to disentangle such mixed effects (from individuals and covariates), but instead memorizes training shapes by their covariates $\mathbf{c}$. In contrast, the additive architecture of \texttt{NAISR} prevents the model from memorizing training shapes through covariates $\mathbf{c}$. More discussions are available at \Secref{subsec.supp_shape_reconstruction} in the supplementary material.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[t]
\vspace{-0.4in}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccccccccccccccccc}
\toprule
{\color[HTML]{000000} } & \multicolumn{6}{c}{\cellcolor[HTML]{DDFFDD}{\color[HTML]{000000} \textbf{Starman}}} & \multicolumn{6}{c}{\cellcolor[HTML]{FFFFC7}{\color[HTML]{000000} \textbf{ADNI Hippocampus}}} & \multicolumn{6}{c}{\cellcolor[HTML]{D7D9FA}{\color[HTML]{000000} \textbf{Pediatric Airway}}} \\ \cline{2-19} 
{\color[HTML]{000000} } & \multicolumn{2}{c}{{\color[HTML]{000000} CD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} EMD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} HD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} CD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} EMD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} HD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} CD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} EMD $\downarrow$}} & \multicolumn{2}{c}{{\color[HTML]{000000} HD $\downarrow$}} \\
\multirow{-3}{*}{{\color[HTML]{000000} }} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} & {\color[HTML]{000000} $\mu$} & {\color[HTML]{000000} M} \\ \hline
{\color[HTML]{000000} DeepSDF} & {\color[HTML]{000000} 0.117} & {\color[HTML]{000000} 0.105} & {\color[HTML]{000000} 1.941} & {\color[HTML]{000000} 1.887} & {\color[HTML]{000000} 6.482} & {\color[HTML]{000000} 6.271} & {\color[HTML]{000000} 0.157} & {\color[HTML]{000000} \textbf{0.140}} & {\color[HTML]{000000} 2.098} & {\color[HTML]{000000} \textbf{2.035}} & {\color[HTML]{000000} 9.762} & {\color[HTML]{000000} 9.276} & {\color[HTML]{000000} \textbf{0.077}} & {\color[HTML]{000000} 0.052} & {\color[HTML]{000000} 1.401} & {\color[HTML]{000000} 1.266} & {\color[HTML]{000000} 10.765} & {\color[HTML]{000000} 9.446} \\
{\color[HTML]{000000} A-SDF} & {\color[HTML]{000000} 0.173} & {\color[HTML]{000000} 0.092} & {\color[HTML]{000000} 2.01} & {\color[HTML]{000000} 1.668} & {\color[HTML]{000000} 8.806} & {\color[HTML]{000000} 6.949} & {\color[HTML]{000000} 1.094} & {\color[HTML]{000000} 1.162} & {\color[HTML]{000000} 7.156} & {\color[HTML]{000000} 7.667} & {\color[HTML]{000000} 25.092} & {\color[HTML]{000000} 25.938} & {\color[HTML]{000000} 2.647} & {\color[HTML]{000000} 1.178} & {\color[HTML]{000000} 10.302} & {\color[HTML]{000000} 9.068} & {\color[HTML]{000000} 47.172} & {\color[HTML]{000000} 37.835} \\
{\color[HTML]{000000} A-SDF (c)} & {\color[HTML]{9A0000} \textbf{0.049}} & {\color[HTML]{000000} \textbf{0.043}} & {\color[HTML]{000000} \textbf{1.298}} & {\color[HTML]{000000} \textbf{1.261}} & {\color[HTML]{000000} \textbf{5.388}} & {\color[HTML]{000000} \textbf{4.964}} & {\color[HTML]{000000} 0.311} & {\color[HTML]{000000} 0.294} & {\color[HTML]{000000} 3.136} & {\color[HTML]{000000} 3.099} & {\color[HTML]{000000} 13.852} & {\color[HTML]{000000} 13.003} & {\color[HTML]{000000} 0.852} & {\color[HTML]{000000} 0.226} & {\color[HTML]{000000} 4.090} & {\color[HTML]{000000} 2.890} & {\color[HTML]{000000} 30.848} & {\color[HTML]{000000} 21.965} \\
{\color[HTML]{000000} DIT} & {\color[HTML]{000000} 0.281} & {\color[HTML]{000000} 0.181} & {\color[HTML]{000000} 2.727} & {\color[HTML]{000000} 2.497} & {\color[HTML]{000000} 10.295} & {\color[HTML]{000000} 8.442} & {\color[HTML]{000000} \textbf{0.156}} & {\color[HTML]{000000} 0.142} & {\color[HTML]{000000} \textbf{2.096}} & {\color[HTML]{000000} 2.054} & {\color[HTML]{000000} \textbf{9.465}} & {\color[HTML]{000000} \textbf{9.123}} & {\color[HTML]{000000} 0.094} & {\color[HTML]{000000} 0.049} & {\color[HTML]{000000} 1.414} & {\color[HTML]{000000} 1.262} & {\color[HTML]{000000} 11.524} & {\color[HTML]{000000} 10.228} \\
{\color[HTML]{000000} NDF} & {\color[HTML]{000000} 1.086} & {\color[HTML]{000000} 0.736} & {\color[HTML]{000000} 5.364} & {\color[HTML]{000000} 4.821} & {\color[HTML]{000000} 21.098} & {\color[HTML]{000000} 19.705} & {\color[HTML]{000000} 0.253} & {\color[HTML]{000000} 0.213} & {\color[HTML]{000000} 2.699} & {\color[HTML]{000000} 2.58} & {\color[HTML]{000000} 11.328} & {\color[HTML]{000000} 10.947} & {\color[HTML]{000000} 0.238} & {\color[HTML]{000000} 0.117} & {\color[HTML]{000000} 2.174} & {\color[HTML]{000000} 1.737} & {\color[HTML]{000000} 14.950} & {\color[HTML]{000000} 12.516} \\ \hline
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{000000} Ours} & {\color[HTML]{000000} 0.111} & {\color[HTML]{000000} 0.072} & {\color[HTML]{000000} 1.709} & {\color[HTML]{000000} 1.515} & {\color[HTML]{000000} 7.951} & {\color[HTML]{000000} 7.141} & {\color[HTML]{000000} 0.174} & {\color[HTML]{000000} 0.153} & {\color[HTML]{000000} 2.258} & {\color[HTML]{000000} 2.191} & {\color[HTML]{000000} 10.019} & {\color[HTML]{000000} 9.521} & {\color[HTML]{9A0000} \textbf{0.067}} & {\color[HTML]{9A0000} \textbf{0.039}} & {\color[HTML]{9A0000} \textbf{1.233}} & {\color[HTML]{9A0000} \textbf{1.132}} & {\color[HTML]{9A0000} \textbf{10.333}} & {\color[HTML]{9A0000} \textbf{8.404}} \\
\rowcolor[HTML]{EFEFEF} 
{\color[HTML]{000000} Ours (c)} & {\color[HTML]{9A0000} \textbf{0.049}} & {\color[HTML]{9A0000} \textbf{0.036}} & {\color[HTML]{9A0000} \textbf{1.276}} & {\color[HTML]{9A0000} \textbf{1.156}} & {\color[HTML]{9A0000} \textbf{5.051}} & {\color[HTML]{9A0000} \textbf{4.666}} & {\color[HTML]{9A0000} \textbf{0.126}} & {\color[HTML]{9A0000} \textbf{0.116}} & {\color[HTML]{9A0000} \textbf{1.847}} & {\color[HTML]{9A0000} \textbf{1.81}} & {\color[HTML]{9A0000} \textbf{8.586}} & {\color[HTML]{9A0000} \textbf{8.153}} & {\color[HTML]{000000} 0.084} & {\color[HTML]{000000} \textbf{0.044}} & {\color[HTML]{000000} \textbf{1.345}} & {\color[HTML]{000000} \textbf{1.190}} & {\color[HTML]{000000} \textbf{10.719}} & {\color[HTML]{000000} \textbf{8.577}} \\ \bottomrule
\end{tabular}%
}

\caption{\small Quantitative evaluation of shape reconstruction. \(\mathrm{CD}=\) Chamfer distance. \(\mathrm{EMD}=\) Earth mover's distance. \(\mathrm{HD}=\) Hausdorff distance. All metrics are multiplied by $10^2$. \textbf{\textcolor{purple}{Bold red values}} indicate the best scores across all methods. \textbf{Bold black values } indicate the 2nd best scores of all methods. \textbf{Ours} means the covariates were inferred during testing (see~\Eqref{eq.infer_cz}). \textbf{Ours(c)} means the covariates are used as input during inference (see \Eqref{eq.infer_z}).  $\mu$ and M indicate the mean and median of the measurements on the testing shapes respectively. \texttt{NAISR} performs well for all three reconstruction tasks while allowing for interpretability.}
\label{tab.recons}
\vspace{-0.2in}
\end{table}


\begin{figure}[t]
\centering
\vspace{-0.5in}
\includegraphics[width=0.9\columnwidth]{figs/reconstruction_airway_adni.pdf}
\vspace{-0.2in}
\caption{\small Visualizations of airway and hippocampus reconstruction with different methods. The red and blue circles show the structure in the black circle from two different views. Hippocampus shapes are plotted with two $180^{\circ}$-flipped views. \texttt{NAISR} can produce detailed and accurate reconstructions as well as impute missing airway parts. More visualizations are available in \Secref{subsec.supp_shape_reconstruction} of the supplementary material.}
\label{fig.exp_recons}
%\vskip -0.2in
\label{fig.recons}
\end{figure}



\subsection{Shape Transfer}
% The goal of our shape transfer experiment is to evaluate \texttt{NAISR}'s ability to transfer a shape $\mathcal{S}^0$ from the current covariates $\mathbf{c}^0$ (which might be unknown) to different covariates $\mathbf{c}^t$.  We use the longitudinal data in the dataset for evaluation. Specifically, we treat the earliest shape observation of a patient as the current shape $\mathcal{S}^0$ and attempt to transfer $\mathcal{S}^0$ from $\mathbf{c}^0$ to $\mathbf{c}^t$ to capture the later shape $\mathcal{S}^t$. 

% First, we obtain the inferred latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}^0$ of $\mathcal{S}^0$ by reconstruction. Then, we transfer $\mathcal{S}^0$ to $\mathcal{S}^t$ with  $\Phi(\mathbf{p},\hat{\mathbf{c}}^0+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$ by setting $\Delta\mathbf{c}$ to $\mathbf{c}^t - \mathbf{c}^0$, where $\mathbf{c}^t$ and $\mathbf{c}^0$ are the covariates of the longitudinal data. Ideally, the inferred $\hat{\mathbf{c}}^0$ is exactly $\mathbf{c}^0$. The zero level set of $\Phi(\mathbf{p},\hat{\mathbf{c}}^0+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$ is then the transferred shape and it is evaluated by comparing it with the gold standard shape $\mathcal{S}^t$. 
Table~\ref{tab.transp_for_case} shows an airway shape transfer example for a cancer patient who was scanned 11 times. We observe that our method can produce complete transferred shapes that correspond well with the measured shapes. %Further, we are able to infer \emph{complete} shapes. % along the covariate reasonably in terms of producing complete shapes and developing volumes. 
Table~\ref{tab.transfer} shows quantitative results for the volume differences between our transferred shapes and the gold standard shapes. Our method performs best on the real datasets while A-SDF (the only other model supporting shape transfer) works slightly better on the synthetic \emph{Starman} dataset. Our results demonstrate that \texttt{NAISR} is capable of transferring shapes to other timepoints $\mathcal{S}^t$ from a given initial state $\mathcal{S}^0$. 
%Such application is very desirable in medical scenarios when inferring treatment effects. %Due to the error produced in the data acquisition steps, it is impossible to get the exact volume for every shape. Thus, we use the boxplot to evaluate the variance of the volume estimation error. Shorter error bars are considered as less error variance. Figure~\ref{tab.transfer} evaluates the variance of the volume estimation error.


\begin{table*}[t] 
\centering
\vspace{-0.1in}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{llcccccccccc}
\toprule
 &  & \multicolumn{6}{c}{\cellcolor[HTML]{DDFFDD}{\color[HTML]{000000} \textbf{Starman}}} & \multicolumn{2}{l}{\cellcolor[HTML]{FCFCDA}\textbf{ADNI Hippocampus}} & \multicolumn{2}{l}{\cellcolor[HTML]{D7D9FA}\textbf{Pediatric Airway}} \\ \cline{3-12} 
 & \multirow{3}{*}{} & \multicolumn{2}{c}{CD $\downarrow$} & \multicolumn{2}{c}{EMD $\downarrow$} & \multicolumn{2}{c}{HD $\downarrow$} & \multicolumn{2}{c}{VD $\downarrow$} & \multicolumn{2}{c}{VD $\downarrow$} \\
\multirow{3}{*}{} & w.C. & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{$\mu$}} & \multicolumn{1}{c}{M} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{$\mu$}} & \multicolumn{1}{c}{M} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{$\mu$}} & \multicolumn{1}{c}{M} & \multicolumn{1}{c}{\cellcolor[HTML]{FFFFFF}\textbf{$\mu$}} & \multicolumn{1}{c}{M} & \cellcolor[HTML]{FFFFFF}\textbf{$\mu$} & M \\ \hline
A-SDF & \XSolidBrush & {\color[HTML]{9A0000} \textbf{0}} & {\color[HTML]{9A0000} \textbf{0}} & {\color[HTML]{9A0000} \textbf{0.009}} & {\color[HTML]{9A0000} \textbf{0.008}} & {\color[HTML]{9A0000} \textbf{0.036}} & {\color[HTML]{9A0000} \textbf{0.034}} & 0.518 &        0.488 & 81.07 & 82.92 \\
A-SDF & \Checkmark & {\color[HTML]{9A0000} \textbf{0}} & {\color[HTML]{9A0000} \textbf{0}} & {\color[HTML]{9A0000} \textbf{0.009}} & {\color[HTML]{9A0000} \textbf{0.009}} & {\color[HTML]{9A0000} \textbf{0.036}} & {\color[HTML]{9A0000} \textbf{0.035}} & 0.215 &        0.177 & 41.46 & 40.96 \\ \hline
\rowcolor[HTML]{EFEFEF} 
Ours & \XSolidBrush & 0.003 & 0.002 & \cellcolor[HTML]{EFEFEF}0.025 & \cellcolor[HTML]{EFEFEF}0.023 & \cellcolor[HTML]{EFEFEF}0.094 & \cellcolor[HTML]{EFEFEF}0.077 & {\color[HTML]{9A0000} \textbf{0.086}} & {\color[HTML]{9A0000} \textbf{0.063}} & \textbf{12.82} & {\color[HTML]{9A0000} \textbf{8.84}} \\
\rowcolor[HTML]{EFEFEF} 
Ours & \Checkmark & 0.009 & 0.002 & \cellcolor[HTML]{EFEFEF}0.031 & \cellcolor[HTML]{EFEFEF}0.025 & \cellcolor[HTML]{EFEFEF}0.116 & \cellcolor[HTML]{EFEFEF}0.083 & \textbf{0.089} & \textbf{0.071} & {\color[HTML]{9A0000} \textbf{11.23}} & \textbf{9.65}\\ \bottomrule
\end{tabular}%
}
\caption{\small Quantitative evaluation of shape transfer. Statistics of Volume Difference (VD, $cm^3$) between transferred shapes and gold standard shapes. w.C. abbreviates \emph{with covariates}. A \Checkmark in w.C. indicates the inference follows \Eqref{eq.infer_z}. A \XSolidBrush in w.C. indicates the inference follows \Eqref{eq.infer_cz}. \textbf{\textcolor{purple}{Red bold scores}} indicate the best performance across all methods and \textbf{bold scores} indicate the 2nd best. \texttt{NAISR} results in significantly improved volume estimates for real medical shapes.}
\label{tab.transfer}
\vspace{-0.25in}
\end{table*}

\begin{table*}[htbp] 
\centering
\vspace{-0.1in}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}p{0.05\textwidth}}
\toprule
\#time &      0&      1  &      2  &      3  &      4  &      5  &      6  &      7  &      8  &      9  &      10 \\
\midrule
$\{\mathcal{S}^t\}$&
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/0.png} &  
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/1.png} & 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/2.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/3.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/4.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/5.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/6.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/7.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/8.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/9.png}& 
\includegraphics[width=0.1\columnwidth]{figs/airway_transfer/withcov/10.png}\\
\bottomrule
\end{tabular}}

\resizebox{0.8\columnwidth}{!}{%
\begin{tabular}{lccccccccccc}
\hline
\# time & \multicolumn{1}{l}{0} & \multicolumn{1}{l}{1} & \multicolumn{1}{l}{2} & \multicolumn{1}{l}{3} & \multicolumn{1}{l}{4} & \multicolumn{1}{l}{5} & \multicolumn{1}{l}{6} & \multicolumn{1}{l}{7} & \multicolumn{1}{l}{8} & \multicolumn{1}{l}{9} & \multicolumn{1}{l}{10} \\ \hline
age & 154 & 155 & 157 & 159 & 163 & 164 & 167 & 170 & 194 & 227 & 233 \\
weight & 55.2 & 60.9 & 64.3 & 65.25 & 59.25 & 59.2 & 65.3 & 68 & 77.1 & 75.6 & 75.6 \\
sex & M & M & M & M & M & M & M & M & M & M & M \\
p-vol & 92.5 & 93.59 & 94.64 & 95.45 & 96.33 & 96.69 & 98.4 & 99.72 & 109.47 & 118.41 & 118.76 \\
m-vol & 86.33 & 82.66 & 63.23 & 90.65 & 98.11 & 84.35 & 94.14 & 127.45 & 98.81 & 100.17 & 113.84 \\ \hline
\end{tabular}%
}
\caption{\small Airway shape transfer for a patient. Blue: gold standard shapes; red: transferred shapes with \texttt{NAISR}. The table below lists the covariates (age/month, weight/kg, sex) for the shapes above. P-vol(predicted volume) is the volume ($cm^3$) of the transferred shape by \texttt{NAISR} covariates following \Eqref{eq.infer_cz}. M-vol (measured volume) is the volume ($cm^3$) of the shapes based on the actual imaging. \texttt{NAISR} can capture the trend of growing volume with age and weight while producing clear, complete, and topology-consistent shapes. Note that measured volumes may differ depending on the CT imaging field of view. More visualizations are available in \Secref{subsec:shape_transfer} in the supplementary material.}
\label{tab.transp_for_case}
\vspace{-0.25in}
\end{table*}





%\hat{\mathbf{c}}+\Delta\mathbf{c}$. By , we can transfer $\mathcal{S}^0$ with  $\Phi(\mathbf{p},\hat{\mathbf{c}}+\Delta{\mathbf{c}}, \hat{\mathbf{z}})$. 



%Given an observed shape $\mathcal{S}_0$, we first reconstruct the shape to infer its latent code $\hat{\mathbf{z}}$ and covariates $\hat{\mathbf{c}}_0$ from the reconstruction. Then we transfer the shape to a later time point by evolving the covariates to $\hat{\mathbf{c}}(0)+{\mathbf{c}}$, where ${\Delta\mathbf{c}}=\mathbf{c}(t) - \mathbf{c}(0)$ is the difference between the covariates at different observations. 

\subsection{Shape Disentanglement and Evolution}

\begin{figure*}[ht]
\vspace{-0.4in}
    \centering
    \begin{minipage}[a]{.3\textwidth}
        \centering
        %\vspace{-0.1in}
        \includegraphics[width=\linewidth, height=0.18\textheight]{figs/starman_shapematrix_template_asdf.pdf}\\
        \small  A-SDF, \textit{Starman}
        \label{fig.asdf_starman}
    \end{minipage}%
    \begin{minipage}[f]{0.35\textwidth}
        \centering
        \vspace{-0.15in}
        \includegraphics[width=\linewidth, height=0.22\textheight]{figs/shapematrix_template_adni_asdf.pdf}\\
        \small A-SDF, ADNI Hippocampus
        \label{fig:asdf_adni}
    \end{minipage}
    \hspace{-0.25in}
        \begin{minipage}[f]{0.35\textwidth}
        \centering
         \vspace{-0.15in}
        \includegraphics[width=\linewidth, height=0.22\textheight]{figs/shapematrix_template_airway_asdf.pdf}\\
        \small  A-SDF, Pediatric Airway
        \label{fig:prob1_6_1}
    \end{minipage}
    
    \medskip
    \vspace{0.3in}
    %\hspace{-0.25in}
    \begin{minipage}[a]{.3\textwidth}
        \centering
        \vspace{-0.2in}
        \includegraphics[width=\linewidth, height=0.18\textheight]{figs/starman_shapematrix_template.pdf}\\
        \small Ours, \textit{Starman}
        \label{fig.ours_starman}
    \end{minipage}%
    %\hspace{-0.25in}
    \begin{minipage}{0.35\textwidth}
        \centering
       \vspace{-0.35in}
        \includegraphics[width=\linewidth, height=0.22\textheight]{figs/shapematrix_template_adni.pdf}\\
        \small  Ours, ADNI Hippocampus
        \label{fig.ours_adni}
    \end{minipage}
    \hspace{-0.25in}
        \begin{minipage}{0.35\textwidth}
        \centering
       \vspace{-0.35in}
        \includegraphics[width=\linewidth, height=0.22\textheight]{figs/shapematrix_template_airway.pdf}\\
        \small  Ours, Pediatric Airway
        \label{fig.ours_airway}
    \end{minipage}
\caption{\small Template shape extrapolation in covariate space using A-SDF and \texttt{NAISR} on three datasets. For the \textit{Starman} shape extrapolations, the blue shapes are the groundtruth shapes and the red shapes are the reconstructions. The shapes in the middle white circles are the template shapes. The template shape is generated with zero latent code and is used to create a template covariate space. The shapes in the green and yellow boxes are plotted with $\{\Phi_i\}$, representing the disentangled shape evolutions along each covariate respectively. The purple shadows over the space indicate the covariate range that the dataset covers. Cyan points represent male and purple points female patients in the dataset. The points represent the covariates of all patients in the dataset. The colored shades at the boundary represent the covariate distributions stratified by sex. Example 3D shapes in the covariate space are visualized with their volumes ($cm^3$) below. \texttt{NAISR} is able to extrapolate the shapes in the covariate space given either an individualized latent code $\mathbf{z}$ or template latent code $\mathbf{0}$, whereas A-SDF struggles. The supplementary material provides more visualizations of individualized covariate spaces in \Secref{subsec:disentangled_shape_evolution}. (Best viewed zoomed.)}
\label{fig.manifold}
\vspace{-0.2in}
\end{figure*}

\Figref{fig.manifold} shows that \texttt{NAISR} is able to extrapolate reasonable shape changes when varying covariates. These results illustrate the capabilities of \texttt{NAISR} for shape disentanglement and to capture shape evolutions. A-SDF and \texttt{NAISR} both produce high-quality \textit{Starman} shapes because the longitudinal data is sufficient for the model to discover the covariate space. However, for real data, only \texttt{NAISR} can produce realistic 3D hippocampi and airways reflecting the covariates' influences on template shapes. Note that when evolving shapes along a single covariate $c_i$, the deformations from other covariates are naturally set to $\mathbf{0}$ by our model construction (see \Secref{subsec.model_f}). As a result, the shapes in the yellow and green boxes in Figure~\ref{fig.manifold} represent the disentangled shape evolutions along different covariates respectively. The shapes in the other grid positions can be extrapolated using $\Phi(\cdot)$. By inspecting the volume changes in the covariate space in \Figref{fig.manifold}, we observe that age is more important for airway volume than weight, and Alzheimer disease influences hippocampal volume. These observations from our generated shapes are consistent with clinical expectations~\citep{luscan2020developmental, gosche2002hippocampal}, suggesting that \texttt{NAISR} is able to extract hidden knowledge from data and is able to generate interpretable results directly as 3D shapes.


\section{Limitations and Future Work}
\label{sec.limitations_and_future_work}
Invertible transforms are often desirable for shape correspondences but not guaranteed in \texttt{NAISR}. Invertibility could be guaranteed by representing deformations via velocity fields, but such parameterizations are costly because of the required numerical integration. In future work, we will develop  efficient invertible representations, which will ensure topology preservation. So far we only indirectly assess our model by shape reconstruction and transfer performance. Going forward we will include patients with airway abnormalities. This will allow us to  explore if our estimated model of normal airway shape can be used to detect airway abnormalities. Introducing group sparsity~\citep{yin2012group, chen2017group} to \texttt{NAISR} for high-dimensional covariates is also promising future work.

%\textcolor{blue}{We also do not know the true effects of the covariates on the shapes. We can so far only indirectly assess our model by quantifying reconstruction, evolution, and transfer performance. In future work, we will include patients with airway abnormalities in our analyses. This would allow us to further assess our \texttt{NAISR} model by exploring if our estimated model of normal airway shape can be used to detect airway abnormalities.}

\section{Conclusion}
%In this work, we 
We proposed \texttt{NAISR}, a 3D neural additive model for interpretable shape representation. We tested \texttt{NAISR} on three different datasets and observed particularly good performance on real 3D medical datasets. Compared to other shape representation methods, \texttt{NAISR} 1) captures the effect of individual covariates on shapes; 2) can transfer shapes to new covariates, e.g.,  to infer anatomy development; and 3) can provide shapes based on extrapolated covariates. %To the best of our knowledge, 
\texttt{NAISR} is the first approach combining deep implicit shape representations based on template deformation with the ability to account for covariates. We believe our work is an exciting start for a new line of research: interpretable neural shape models for scientific discovery. %We discuss the future work of \texttt{NAISR} in \Secref{supp.limitations_and_future_work} of the supplementary material. %Although our driving problem is to provide an atlas of the upper airway in children, our approach can be used for general interpretable shape representation and shape analysis.


%-------------------------------------------------------------------------
\newpage
\section*{\centering{Acknowledgement}}

The research reported in this publication was supported by NIH grant 1R01HL154429. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH. Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.;Janssen Alzheimer Immunotherapy Research \& Development, LLC.; Johnson \& Johnson Pharmaceutical Research \& Development LLC.; Lumosity; Lundbeck; Merck \& Co., Inc.;Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.

%-------------------------------------------------------------------------
\newpage
\section*{\centering{Reproducibility Statement}}

We are dedicated to ensuring the reproducibility of \texttt{NAISR} to facilitate more scientific discoveries on shapes. To assist researchers in replicating and building upon our work, we made the following efforts.

\begin{itemize}
    \item \textbf{Model \& Algorithm}: Our paper provides detailed descriptions of the model architectures (see \Secref{sec.method_model_arch}), implementation details (see \Secref{subsec:implementation_details}), and ablation studies (see \Secref{subsec.ablation_study}). We have submitted our source code. The implementation of \texttt{NAISR} will be made publicly available.
    \item \textbf{Datasets \& Experiments}: We provide extensive illustrations and visualizations for the datasets we used. To ensure transparency and ease of replication, the exact data processing steps, from raw data to processed input, are outlined at \Secref{supp.sec.dataset} in the supplementary materials. We expect our thorough supplementary material to ensure the reproducibility of our method and the understandability of our experiment results. We also submit the code for synthesizing the 2D \textit{Starman} dataset so that researchers can easily reproduce the results. 
\end{itemize}

\newpage
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\clearpage
\newpage
\appendix
\input{supplementary}
%\input{supplementary}
% \appendix
% \section{Appendix}
\end{document}

\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
