\documentclass[12pt]{article}
\usepackage{float}
\usepackage[german,english]{babel}
%\usepackage[utf8]{inputenc}
\usepackage{epsf,array}
\usepackage{latexsym,bbm}
\usepackage{epsfig,appendix}
\usepackage[]{algorithm2e}
\usepackage {amssymb,amsthm,amsfonts}
\usepackage {amsmath,comment,nccmath}
\usepackage{color,enumitem}
\setlength{\parindent}{0 cm}
\usepackage{setspace}
\usepackage{url,caption,subcaption}
\usepackage{hyperref,appendix}
\linespread{0.95}
\def\bm#1{\mbox{\boldmath{$#1$}}}
\newcommand{\TT}[1]{{\texttt{#1}}}
\def\o{{\scriptstyle\mathcal{O}}}

\setlength{\textwidth}{15cm} \setlength{\textheight}{22cm}
\topmargin-1.5cm \evensidemargin0.5cm \oddsidemargin0.5cm

%\headheight0cm \headsep0cm \topskip0cm
\parindent3ex \parskip1.5ex plus 0.5ex minus 0.3ex
\usepackage{rotating}
\usepackage[longnamesfirst]{natbib}
\bibpunct{(}{)}{;}{´a}{,}{,}
\renewcommand{\cite}{\citep}
\newcommand{\biblist}{
\bibliographystyle{apalike}
\bibliography{Literature}
}

\setlength{\bibsep}{0cm}
\parindent0pt

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}

\makeatother
\makeatletter
\renewcommand{\@fnsymbol}[1]{\@arabic{#1}}
\makeatother
\title{Efficient nonparametric estimation of Toeplitz covariance matrices}

\author{Karolina Klockmann\footnote{Department of Statistics and Operations Research, Universit\"at Wien,
	Oskar-Morgenstern-Platz 1, 1090 Wien, Austria }%\\{Universit\"at Wien}
\and Tatyana Krivobokova$^{1}$}


\begin{document}
%\baselineskip=25pt
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
	\baselineskip=15pt \noindent 
	\\
	A new nonparametric estimator for Toeplitz covariance matrices is proposed. This estimator is based on a data transformation that translates the problem of Toeplitz covariance matrix estimation to the problem of mean estimation in an approximate Gaussian regression. The resulting Toeplitz covariance matrix estimator is positive definite by construction, fully data-driven and computationally very fast. 
	Moreover, this estimator is shown to be minimax optimal under the spectral norm for a large class of Toeplitz matrices. These results are readily extended to estimation of inverses of Toeplitz covariance matrices. 
	Also, an alternative version of the Whittle likelihood for the spectral density based on the Discrete Cosine Transform (DCT) is proposed. The method is implemented in the R package \textit{vstdct} that accompanies the paper. \\\\
	{\textit{Keywords:}} Discrete cosine transform, spectral density, Whittle likelihood, periodogram, variance stabilizing transform.
\end{abstract}
\baselineskip=20pt

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
\doublespacing
\linespread{1.2} 
Estimation of covariance and precision matrices is a fundamental problem in statistical data analysis with countless applications in the natural and social sciences. Covariance matrices with a Toeplitz structure arise in the study of stationary stochastic processes, which are an important modeling tool in many applications, such as radar target detection, speech recognition,  modeling internet economic activity, electrical brain activity and the motion of crystal structures \citep[p.232]{du2020toeplitz, roberts2000hidden,quah2000internet,franaszczuk1985application,grenander1958toeplitz}. \\
The data for estimation are given as $n$ independent and identically distributed realizations of a $p$-dimensional vector having a zero mean and a covariance matrix $\bm \Sigma$ with a Toeplitz structure. Thereby, $p$ is assumed to grow while $n$ may be equal to $1$ (a single realization of a stationary time series) or may tend to infinity as well. 
%In any setting the matrix of the sample (auto-)covariances is known to be an inconsistent estimator of $\bm \Sigma$ in the spectral norm \citep[see e.g.,][]{wu2009banding}.
For $p\to \infty$ and $p/n \to c \in (0,\infty]$, the sample (auto-)covariance matrix is known to be an inconsistent estimator of $\bm \Sigma$ in the spectral norm \citep[see e.g.,][chap. 2]{wu2009banding,pourahmadi2013high}.
Therefore, tapering, banding and thresholding of the sample covariance matrix have been proposed to regularize this estimator \citep[see][]{wu2012covariance,cai2013optimal}. The optimal rate of convergence for Toeplitz covariance matrix estimators was established in \citet{cai2013optimal}, who in particular showed that tapering and banding estimators attain the minimax optimal convergence rate over certain spaces of Toeplitz covariance matrices. Optimality of the thresholded estimator was shown only for the case $n=1$ \citep[see][]{wu2012covariance}.  However, all of these estimators have several practical drawbacks. First, they are not guaranteed to be positive definite. To enforce positive definiteness, additional manipulations with the estimators must be performed, see e.g., Section 5 in \citet{cai2013optimal}.  Second, the data-driven choice of the tapering, banding or thresholding parameter is not trivial in practice. For $n>1$, \citet{bickel2008regularized} proposed a cross-validation criterion that approximates the risk of the estimator. \citet{fang2016tuning} compared this method with a bootstrap based approximation of the risk in an intensive simulation study and recommended cross-validation over bootstrap. %and provide a theoretical justification. %Nevertheless, for a finite sample, the method is expected to work well only when $n$ is sufficiently large. 
For $n=1$, to the best of our knowledge, there is no fully data-driven approach for selecting the banding/tapering/thresholding parameter. \citet{wu2009banding} suggested first to split the time series into non-overlapping subseries and then apply the cross-validation criterion of \citet{bickel2008regularized}.  However, it turns out that the right choice of the subseries length is crucial for this approach, but there is no data-based method available for this. \\%Finally, inference based on these estimators, such as confidence sets for $\bm\Sigma$, has not been investigated in detail. % {\color{red} is it true? search for the references}\\ % \citet{yang2021estimation} proposed a positive definite estimator of the Toeplitz covariance matrix by minimising  a penalized entropy loss function with eigenvalue constraints.
In this work, an alternative way to estimate a Toeplitz covariance matrix and its inverse is chosen. Our approach exploits the one-to-one correspondence between Toeplitz covariance matrices and their spectral densities. First, the given data are transformed into approximate Gaussian random variables whose mean equals to the logarithm of the spectral density. Then, the log-spectral density is estimated by a periodic smoothing spline with a data-driven smoothing parameter. Finally, the resulting spectral density estimator is transformed into an estimator for $\bm\Sigma$ or its inverse. It is shown that this procedure leads to an estimator that is fully data-driven, automatically positive definite and achieves the minimax optimal convergence rate under the spectral norm over a large class of Toeplitz covariance matrices. In particular, this class includes Toeplitz covariance matrices that correspond to long-memory processes with bounded spectral densities. Moreover, the computation is very efficient, does not require iterative or resampling schemes and allows to apply any inference and adaptive estimation procedures developed in the context of nonparametric Gaussian regression.  \\
Estimation of the spectral density from a stationary time series is a research topic with a long history. Earlier nonparametric methods are based on smoothing of the (log-)periodogram, which itself is not a consistent estimator \citep[][]{bartlett1950periodogram, welch1967use, thomson1982spectrum,wahba1980automatic}. Another line of nonparametric methods for estimating the spectral density is based on the Whittle likelihood, which is an approximation to the exact likelihood of the time series in the frequency domain. For example, \citet{pawitan1994nonparametric} estimated the spectral density from a penalized Whittle likelihood, while \citet{kooperberg1995rate} used polynomial splines to estimate the log-spectral density function maximizing the Whittle likelihood.
Recently, Bayesian methods for spectral density estimation have been proposed  \citep[see][]{choudhuri2004bayesian,edwards2019bayesian,maturana2021bayesian}, but these may become very computationally intensive in large samples due to posterior sampling.  \\
%However, the application of these methods are computationally very intensive for large samples due to posterior sampling.  \\
%Most of these work focus on the computation aspects of spectral density estimation and only few study also the corresponding asymptotic properties. 
The minimax optimal convergence rate for nonparametric estimators of H\"older continuous spectral densities from Gaussian stationary time series was obtained by \citet{bentkus1985rate} under the $L_p, 1\leq p\leq \infty$, norm. 
%The projection estimator of \citet{comte2001adaptive} attains the optimal $L_2$-rate for estimating the spectral density from a Gaussian time series. 
Only few works on spectral density estimation show the optimality of the corresponding estimators. In particular, \citet{kooperberg1995rate}  and \citet{pawitan1994nonparametric} derived convergence rates of their estimators for the log-spectral density under the $L_2$ norm, while neglecting the Whittle likelihood approximation error. \\%Whittle approximation error.
In general, most works on spectral density estimation do not exploit further the close connection to the corresponding Toeplitz covariance matrix estimation. In particular, an upper bound for the $L_\infty$ risk of a spectral density estimator automatically provides an upper bound for the risk of the corresponding Toeplitz covariance matrix estimator under the spectral norm. This fact is used to establish the minimax optimality of our nonparametric estimator for the Toeplitz covariance matrices. The main contribution of this work is to show that our proposed spectral density estimator is not only numerically very efficient, but also achieves the minimax optimal rate in the $L_\infty$ norm, which in turn ensures the minimax optimality of the corresponding Toeplitz covariance matrix estimator.\\ 
The paper is structured as follows. In Section \ref{sec:likelihood}, the model is introduced and approximate diagonalization of Toeplitz covariance matrices with the discrete cosine transform is discussed. Moreover, an alternative version of the Whittle's likelihood is proposed. In Section \ref{sec:method}, new estimators for the Toeplitz covariance matrix and the precision matrix are derived, while in Section \ref{sec:theory} their theoretical properties are presented. Section \ref{sec:simulation} contains simulation results, Section \ref{sec:realdata} presents a real data example, and Section \ref{sec:discussion} closes the paper with a discussion. The proofs are given in the appendix to the paper.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TOEPLITZ MATRIX DIAGONALIZATION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Set up and diagonalization of Toeplitz matrices}
\label{sec:likelihood}
Let $\bm Y_1,\ldots,\bm Y_n\overset{i.i.d.}{\sim}{\mathcal{N}}_p(\bm 0_p,\bm\Sigma)$, where $\bm\Sigma$ is a $(p\times p)$-dimensional positive definite covariance matrix with a Toeplitz structure, that is, $\bm\Sigma=\{\sigma_{|i-j|}\}_{i,j=1}^p\succ 0$. The sample size $n$ may tend to infinity or to be a constant. The case $n=1$ corresponds to a single observation of a stationary time series, and in this case the data are simply denoted by $\bm{Y}{\sim}{\mathcal{N}}_p(\bm 0_p,\bm\Sigma)$. The dimension $p$ is assumed to grow. \\
The spectral density function
$f$, corresponding to a Toeplitz covariance matrix $\bm\Sigma$, is given by
$$ f(x) = \sigma_0+2\sum_{k=1}^\infty\sigma_k\cos(kx), \quad x\in[-\pi,\pi],$$
so that for $f\in L_2(-\pi,\pi)$ the inverse Fourier transform implies
$$\sigma_k=\frac{1}{2\pi}\int_{-\pi}^\pi f(x)\cos(kx)dx=\int_0^1f(\pi x)\cos(k\pi x)dx.$$
Hence, $\bm \Sigma$ is completely characterized by $f$, and the non-negativity of the spectral density function implies the positive definiteness of the covariance matrix. Moreover, the decay of the autocovariance $\sigma_k$ is directly connected to the smoothness of $f$. Finally, the convergence rate of a Toeplitz covariance estimator and that of the corresponding spectral density estimator are directly related via $\|\bm\Sigma\| \leq \|f\|_\infty := \sup_{x\in[-\pi,\pi]}|f(x)|$, where $\|\cdot \|$ denotes the spectral norm \citep[see][chap.~5.2]{grenander1958toeplitz}.    

As in \citet{cai2013optimal}, we introduce a class of positive definite Toeplitz covariance matrices with H\"older continuous spectral densities. For $\beta=\gamma +\alpha>0$, where $\gamma\in\mathbb{N}\cup\{0\}$, $0<\alpha\leq 1$, and $0<\delta,M_0,M_1<\infty$, let
\begin{align*}
\mathcal{P}_\beta(M_0,M_1)= &\left \{f,\bm{\Sigma}(f) \in \mathbb{R}^{p \times p} : \, \|f\|_{\infty}\leqslant M_0, \,  \|f^{(\gamma)}(\cdot+h)-f^{(\gamma)}(\cdot)\|_{\infty}\leqslant M_1|h|^{\alpha}, \, \bm{\Sigma} \succ 0  \right \}.%&\\
%& \cap \left \{ f: \inf_{x} f(x)> \delta \right \}.
\end{align*}
The optimal convergence rate for estimating Toeplitz covariance matrices over ${\mathcal{P}}_\beta(M_0,M_1)$ depends crucially on $\beta$. It is well known that the $k$-th Fourier coefficient of a function whose $\gamma$-th derivative is $\alpha$-Hölder continuous decays at least with order $\mathcal{O}(k^{-\beta})$ \citep[see][]{zygmund2002trigonometric}. Hence, $\beta$ determines the decay rate of the autocovariances $\sigma_k$, which are the Fourier coefficients of the spectral density $f$, as $k\rightarrow\infty$.  In particular, this implies that for $\beta\in(0,1]$, the class $\mathcal{P}_\beta(M_0,M_1)$ includes Toeplitz covariance matrices corresponding to long-memory processes with bounded spectral densities, since the sequence of corresponding autocovariances is not summable.\\
A connection between Toeplitz covariance matrices and their spectral densities is further exploited in the following lemma. 
\begin{lemma} \label{lemma:DCTdiag} %[DCT diagonalization] 
	Let $\bm{\Sigma}\in \mathcal{P}_\beta(M_0,M_1)$ and let $x_j=(j-1)/(p-1),\, j=1,...,p$, then 
	$$\left (\bm{D}^t\bm{\Sigma D}\right )_{i,j} = f(\pi x_j)\delta_{i,j} +\frac{1+(-1)^{|i-j|}}{2}\mathcal{O}\left\{p^{-1}+\log(p) p^{-\beta}\right\},$$
	where $\delta_{i,j}$ is the Kroneker delta, $\mathcal{O}(\cdot)$ terms are uniform over $i,j=1,\dots,p$ and  
	\begin{align*} 
	\bm{D}&= \sqrt{2} (p-1)^{-1/2}  \left [\cos \left \{\pi(i-1)\frac{j-1}{p-1} \right\} \right ]_{i,j=1}^p \text{ divided by $\sqrt{2}$ when $i,j\in\{1,p\}$}
	\end{align*}
	is the Discrete Cosine Transform I (DCT-I) matrix. 
\end{lemma} 
The proof can be found
in Appendix \ref{app:lemma1}.
This result shows that the DCT-I matrix approximately diagonalizes Toeplitz covariance matrices and that the diagonalization error depends to some extent on the smoothness of the corresponding spectral density. \\
In the spectral density literature the discrete Fourier transform (DFT) matrix 
$\bm{F}=p^{-1/2}\{\exp\left(2\pi \texttt{i} ij/p)\right\}_{i,j=1}^p,$
where $\texttt{i}$ is the imaginary unit, is typically employed to approximately diagonalize Toeplitz covariance matrices. Using the fact that $(\bm{F}\bm\Sigma\bm{F}^t)_{i,j}=f(2\pi i/p)+\o(1)$, \citet{whittle1957curve} introduced an approximation for the likelihood of a single Gaussian stationary time series (case $n=1$), the so-called Whittle likelihood
\begin{equation} \label{Whittle} 
\mathcal{L}(\bm Y|f) 	\propto \exp \left [ -\sum_{j=1}^{\left \lfloor p/2 \right \rfloor} \log\{f( 2\pi j/p)\} + \frac{I_j}{f(2\pi j/p )} \right ].
\end{equation} 
The quantity  $I_j=|\bm{F}_j^t\bm{Y}|^2$, where $\bm{F}_j$ denotes the $j$-th column of $\bm{F}$, is known as the periodogram at the $j$-th Fourier frequency.  Note that due to periodogram symmetry, only $\lfloor p/2\rfloor$ data points $I_1,...,I_{\lfloor p/2\rfloor}$ are available for estimating the mean $f(2\pi j/p)$, $j=1,\ldots,\lfloor p/2\rfloor$, where $\lfloor x\rfloor$ denotes the largest integer strictly smaller than $x$.
The Whittle likelihood has become a popular tool for parameter estimation of stationary time series, e.g., for nonparametric and parametric spectral density estimation or for estimation of the Hurst exponent, see e.g., \citet{walker1964asymptotic,taqqu1997robustness}.  \\
Lemma \ref{lemma:DCTdiag} yields the following alternative version of the Whittle likelihood 
\begin{equation}\label{ourWhittle}
\mathcal{L}(\bm Y|f) \propto \exp \left[ -\sum_{j=1}^p \log\{f(\pi x_j)\} + \frac{W_j}{f(\pi x_j)} \right ], %, \quad x_i=(i-1)/(2p-2),\, i=1,..., p.
\end{equation} 
where $W_j=(\bm{D}_j^t\bm{Y})^2$. Note that this likelihood approximation is based on twice as many data points $W_j$ as the standard Whittle likelihood. Thus, it allows for a more efficient use of the data $\bm Y$ to estimate the parameter of interest, such as the spectral density or the Hurst parameter. \\
Equations (\ref{Whittle}) or (\ref{ourWhittle}) invite for the estimation of $f$ by maximizing the (penalized) likelihood over certain linear spaces (e.g., spline spaces), as suggested e.g., in \citet{kooperberg1995rate} or \citet{pawitan1994nonparametric}. However, such an approach requires well-designed numerical methods to solve the corresponding optimization problem, since the spectral density in the second term of (\ref{Whittle}) or (\ref{ourWhittle}) is in the denominator, which does not allow to obtain a closed-form expression for the estimator and often leads to numerical instabilities. Also, the choice of the smoothing parameter becomes challenging. \\
Therefore, we suggest an alternative approach that allows the spectral density to be estimated as a mean in an approximate Gaussian regression. Such estimators have a closed-form expression, do not require an iterative optimization algorithm and a smoothing parameter can be easily obtained with any conventional criterion. First note that if $\bm{Y}\sim \mathcal{N}_p(\bm 0_p,\bm \Sigma)$, with $\bm \Sigma\in\mathcal{P}_\beta(M_0,M_1)$, then $\bm{D}^t\bm{Y}\sim{\cal{N}}_p(\bm 0_p,\bm{D}^t\bm\Sigma\bm{D})$. Hence, for $W_j=(\bm{D}_j^t\bm{Y})^2$, $j=1,\ldots,p$ it follows with Lemma \ref{lemma:DCTdiag} that
\begin{equation} \label{ourGamma}
W_j \sim 
\Gamma \left(1/2, 2f(\pi x_j)+\mathcal{O}\left\{p^{-1}+\log(p) p^{-\beta}\right\}\right ),  
\end{equation} 
where $\Gamma(a,b)$ denotes a gamma distribution with a shape parameter $a$ and a scale parameter $b$. Note that the random variables $W_1,\ldots,W_p$ are only asymptotically independent. Obviously, $\mathbb{E}(W_j)=f(\pi x_j)+\o(1)$, $j=1,\ldots,p$. To estimate $f$ from $W_1,\ldots,W_p$, one could use a generalized nonparametric regression framework with a gamma distributed response, see e.g., the classical monograph by \citet{hastie1990}. However, this approach  requires an iterative procedure for estimation, e.g., a Newton-Raphson algorithm, with a suitable choice for the smoothing parameter at each iteration step.  Deriving the $L_\infty$ rate for the resulting estimator is also not a trivial task. Instead, we suggest to employ a variance stabilizing transform of \citet{cai2010nonparametricfest} that converts the gamma regression into an approximate Gaussian regression. 
In the next section we present the methodology in more detail for a general setting with $n\geq 1$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%METHODOLOGY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:method}
\doublespacing
Let $L_\delta=\{f:\inf_x f(x)\geq \delta\}$ and set ${\cal{F}}_\beta={\cal{P}}_\beta(M_0,M_1)\cap L_\delta$.
We consider estimation of $\bm \Sigma$ and $\bm \Omega=\bm \Sigma^{-1}$ from a sample $\bm Y_1,...,\bm Y_n\overset{i.i.d.}{\sim}\mathcal{N}_p(\bm 0_p,\bm \Sigma)$ where $\bm\Sigma\in{\cal{F}}_\beta$.
For $\bm{Y}_i\sim  \mathcal{N}_p(\bm 0_p,\bm{\Sigma})$, $i=1,\ldots,n$, it was shown in the previous section that with Lemma \ref{lemma:DCTdiag} the data can be transformed into gamma distributed random variables $W_{i,j}=(\bm{D}_j^t\bm{Y}_i)^2$, $i=1,\ldots,n$, $j=1,\ldots,p$, where for each fixed $i$ the random variable $W_{i,j}$ has the same distribution as $W_j$ given in (\ref{ourGamma}). Now the approach of \citet{cai2010nonparametricfest} is adapted to the setting $n\geq 1$. \\
First, the transformed data points $W_{i,j}$ are binned, that is, fewer new variables $Q_k$, $k=1,\ldots, T$, with $T<p$, are built via $Q_k=\sum_{j=(k-1)p/T+1}^{kp/T}\sum_{i=1}^nW_{i,j}$, $k=1,\ldots,T$. Note that the number of observations in a bin is $m=np/T$. In Theorem \ref{theorem1} in Section \ref{sec:theory}, we show that setting $T=\lfloor p^\upsilon\rfloor$ for any $\upsilon\in((4-2\min\{\beta,1\})/3,1)$ leads to the minimax optimal rate for the spectral density estimator. To simplify the notation, $m$ is handled as an integer (otherwise, one can discard several observations in the last bin). %The last bin may contain more than $\lfloor m\rfloor$ points, i.e. $np-\lfloor m\rfloor\cdot(\lfloor T\rfloor-1)$ points. To simplify the notation, we choose $T=p^{3/4}$ in the following and assume that $m$ and $T$ are integers. 
Next, applying the variance stabilizing transform (VST) $G(x)=\log\left(x/m\right)/\sqrt{2}$ to each $Q_k$, yields new random variables $Y_k^*=\log \left (Q_k/m \right )/\sqrt{2} $ that are approximately Gaussian, as shown in \citet{cai2010nonparametricfest}. Since the spectral density is a function that is symmetric around zero and periodic on $[-\pi,\pi]$, one can mirror the resulting observations to use $ Y_{T-1}^*,\ldots,Y_2^*,Y_1^*,\ldots,Y_T^*$ for estimation. Renumerating the observations $Y_k^*$ and scaling the design points into the interval $[0,1)$ for convenience leads to an approximate Gaussian regression problem   %with weakly correlated errors 
\begin{equation*}
Y_k^*\overset{approx.}{\sim} \mathcal{N} \left [ H\left\{f(x_k)\right\},1/m\right ], \quad 
x_k=\frac{k-1}{2T-2}, \, k=1,...,2T-2,
\end{equation*}
where $H(y)= \left \{ \phi(m/2) + \log \left ( 2y/m\right ) \right\}/\sqrt{2}$ and $\phi$ is the digamma function \citep[see][]{cai2010nonparametricfest}. Now, the scaled and shifted log-spectral density $H(f)$ can be estimated with a periodic smoothing spline 
\begin{equation}
\label{eq:Spline}
\widehat{H(f)}(x)= \arg \min_{s \in S_{per}(2q-1)}  \left [ \frac{1}{2T-2} \sum_{k=1}^{2T-2} \{ Y_k^* -s(x_k)\}^2 +h^{2q} \int_0^1 \{ s^{(q)}(x) \}^2 \, dx \right],
\end{equation}
where $h>0$ denotes a smoothing parameter, $q\in\mathbb{N}$ is the penalty order and $S_{per}(2q-1)$ a space of periodic splines of degree $2q-1$. The smoothing parameter $h$ can be chosen either with generalized cross-validation (GCV) as derived in \citet{craven1978smoothing} or with the restricted maximum likelihood, see \citet{Krivobokova:2013}. \\
%By \citet{schwarz2016unified}, the periodic smoothing spline has an exact representation as a kernel estimator, i.e., for $x\in[0,1]$,
%\begin{equation} \label{Hf_kernel} \widehat{H(f)}(x)= \frac{1}{(2T-2)h} \sum_{j=1}^{{2T-2}} K_h (x, {x}_j) {Y}^*_j,\end{equation}
%where $K_h$ is a kernel function depending on the bandwith $h=\lambda^{2q}>0$ and the penalization order $q$.
%For details on periodic smoothing splines and the kernel $K_h$, see Appendix \ref{app:DRB} and \ref{app:SSper}. 
Once an estimator $\widehat{H(f)}$ is obtained, application of the inverse transform function $H^{-1}(y)= m\exp \left \{\sqrt{2}y-\phi\left (m/2\right) \right\}/2$ yields the spectral density estimator $\hat{f}=H^{-1}\left \{\widehat{H(f)}\right \}$. Finally, using the inverse Fourier transform leads to the following covariance matrix estimator
\begin{equation}
\label{eq:Sigma}
\hat{\bm{\Sigma}} = (\hat{\sigma}_{|i-j|})_{i,j=1}^p \text{ with }\hat{\sigma}_j= \int_0^1 \hat{f}(x) \cos \left(k\pi x\right) dx\text{ for } k=1,....,p.
\end{equation}
The precision matrix $\bm{\Omega}$ is  estimated by the inverse Fourier transform of the reciprocal of the spectral density estimator, i.e., 
\begin{equation}
\label{eq:Omega}
\hat{\bm{\Omega}} = (\hat{\omega}_{|i-j|})_{i,j=1}^p \text{ with } \hat{\omega}_k = \int_0^1 \hat{f}(x)^{-1} \cos \left(j\pi x\right )dx \text{ for } k=1,....,p.
\end{equation}
%In practice the discrete Fourier transform can be used to approximate the integrals.\\
The estimation procedure for $\hat{\bm{\Sigma}}$ and $\hat{\bm{\Omega}}$ can be summarised as follows.
\begin{enumerate}
	\item \textbf{Data Transformation}: Define $W_{i,j}=(\bm{D}_j^t\bm{Y}_i)^2$, $i=1,\dots,n$, $j=1,\ldots,p$, where $\bm{D}$ is the $(p\times p)$-dimensional DCT-I matrix as given in Lemma  \ref{lemma:DCTdiag} and $\bm{D}_j$ is its $j$-th column.
	\item \textbf{Binning}: Set $T=\lfloor p^\upsilon\rfloor$ for any $\upsilon\in((4-2\min\{\beta,1\})/3,1)$ and calculate
	$$Q_k=\sum_{j=(k-1)p/T+1}^{kp/T}\sum_{i=1}^n W_{i,j},\;k=1,\ldots,T. $$
	\item \textbf{VST}: Set $Y_k^*{=}\log \left (Q_k/m \right)/\sqrt{2}$, for $k{=}1,...,T$, $m{=}np/T$, and mirror the data to get $2T-2$ approximately Gaussian random variables $ Y_{T-1}^*,\ldots, Y_2^*,Y_1^*,\ldots, Y_T^*$. 
	%Later results suggest $T=p^\upsilon$  where $\upsilon\in((4-2\beta^*)/3,1)$ and $\beta^*=\min\{\beta,1\}.$	
	\item \textbf{Gaussian Regression}: Renumerate observations $Y^*_k$ and estimate $H(f)$ with a periodic smoothing spline in an approximate Gaussian regression model 
	$$Y_k^*=H\{f(x_k)\}+\epsilon_k,\;\;x_k=\frac{k-1}{2T-2},\;k=1,\ldots,2T-2,$$ 
	where $\epsilon_k$ are asymptotically i.i.d. Gaussian variables.   %The finite sample properties of $\epsilon_j$ are discussed in the proof of Theorem \ref{theorem1}.
	\item \textbf{Inverse VST}:  Estimate the spectral density $f$ with $\hat{f}=H^{-1}\left \{\widehat{H(f)} \right \},$ where \newline $H^{-1}(y)= m\exp \left \{\sqrt{2}y-\phi\left (m/2\right) \right\}/2$, for a digamma function $\phi$. % $G^{-1}(y)=\exp(y\sqrt{2})$.\\
	\item \textbf{Estimators}:  Set $\hat{\bm{\Sigma}} = (\hat{\sigma}_{|i-j|})_{i,j=1}^p$ with $\hat{\sigma}_k= \int_0^1 \hat{f}(x) \cos\left (k\pi x\right) dx$ and set  $\hat{\bm{\Omega}} = (\hat{\omega}_{|i-j|})_{i,j=1}^p$ with $\hat{\omega}_k = \int_0^1 \hat{f}(x)^{-1}\cos\left( k\pi x\right)dx.$ %{\color{red} should we write it with a cosine function? how do you calculate?}
\end{enumerate}
Note that $\hat{\bm{\Sigma}}$ and $\hat{\bm{\Omega}}$ are positive definite matrices by construction, since their spectral density functions $\hat{f}$ and $\hat{f}^{-1}$ are non-negative, respectively. Unlike the banding and tapering estimators, the autocovariance estimators $\hat{\sigma}_k$ are controlled by a single smoothing parameter $h$, which can be estimated fully data-driven with several  available automatic methods, which are numerically efficient and well-studied. In addition, one can also use methods for adaptive mean estimation, see e.g., \citet{serra2017adaptive}, which in turn leads to adaptive Toeplitz covariance matrix estimation. All inferential procedures developed in the Gaussian regression context can also be adopted accordingly. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%THEORETICAL PROPERTIES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Properties}
\label{sec:theory}
\doublespacing
\linespread{1.2}
In this section, we study the asymptotic properties of the estimators $\hat{f}$, $\hat{\bm{\Sigma}}$ and $\hat{\bm{\Omega}}$. The results are established under the asymptotic scenario where $p\rightarrow\infty$ and $p/n\to c\in(0,\infty]$, that is, the dimension $p$ grows, while the sample size $n$ either remains fixed or also grows but not faster than $p$. This corresponds to the asymptotic scenario when the sample covariance matrix is inconsistent. 
%Note that for $p/n\rightarrow 0$ the simple empirical estimator for $\Sigma$ is consistent. 
Let $\hat{f}$ be the spectral density estimator defined in Section \ref{sec:method}, i.e.,
$\hat{f}=m\exp\{\sqrt{2}\widehat{H(f)}-\phi(m/2)\}/2,$
where $\widehat{H(f)}$ is given in (\ref{eq:Spline}), $m=np/T$ and $\phi$ is the digamma function. Furthermore, let $\hat{\bm\Sigma}$ be the Toeplitz covariance matrix estimator and $\hat{\bm\Omega}$ the corresponding precision matrix defined in equations (\ref{eq:Sigma}) and (\ref{eq:Omega}), respectively. The following theorem shows that both $\hat{\bm{\Sigma}}$ and $\hat{\bm\Omega}$ attain the minimax optimal rate of convergence over the class $\mathcal{F}_{\beta}$.
%{\color{red}TK: why we do not use $q$ instead of $\gamma$ from the beginning? In the definition of the class of matrices? May be better to change it everywhere}
%We start by establishing a risk bound for the spectral density estimator $\hat{f}$ with respect to the $L_\infty$ norm.  
\begin{theorem} \label{theorem1} 
	Let $\bm Y_1,....,\bm Y_n\overset{i.i.d.}{\sim} \mathcal{N}_p(0, \bm{\Sigma})$ with $\bm{\Sigma}=\bm{\Sigma}(f) \in \mathcal{F}_{\beta}$ and $\beta=\gamma+\alpha>1/2$. If $h>0$ such that  $h\to 0$ and $hT\to \infty$, then with $T=\lfloor p^{\upsilon}\rfloor$ for any $\upsilon\in((4-2\min\{\beta,1\})/3,1)$ and $q=\max\{1,\gamma\}$, the spectral density estimator $\hat{f}$, the corresponding covariance matrix estimator $\hat{\bm\Sigma}$ and the precision matrix estimator $\hat{\bm\Omega}$ satisfy	
	\begin{eqnarray*}
		\sup _{\mathcal{F}_{\beta}}\,	\mathbb{E} \|\hat{\bm{\Sigma}}- \bm{\Sigma}(f)\|^2 \leq \sup _{\mathcal{F}_{\beta}}\, \mathbb{E} \|\hat{f} - f\|_\infty^2 &=&\mathcal{O} \left\{ \frac{\log(np)}{nph}\right\} + \mathcal{O}(h^{2\beta})\nonumber\\
		%		&=&\mathcal{O} \left\{ \frac{\log(np)}{nph}\right\}  + \mathcal{O}(h^{2\beta}) \nonumber \\
		\sup _{\mathcal{F}_{\beta}}\,	\mathbb{E} \|\hat{\bm{\Omega}} - \bm{\Sigma}^{-1}(f)\|^2 &=&\mathcal{O} \left\{ \frac{\log(np)}{nph}\right\}  + \mathcal{O}(h^{2\beta}).
	\end{eqnarray*}
	%where $\mathcal{O}_*=\mathcal{O}(\log^2(p)/(np^\beta)^2)$, if $0<\beta\leq 1$, and $\mathcal{O}_*=\mathcal{O}(1/(np)^2)$, if $\beta>1$.\\
	%If $\beta>1/2$ and $h\asymp\left\{\frac{\log(np)}{np}\right \}^{\frac{1}{2\beta+1}}$, one gets 
	For $h\asymp\left\{\log(np)/(np)\right \}^{\frac{1}{2\beta+1}}$ it follows that 
	\begin{eqnarray*}
		\sup _{\mathcal{F}_{\beta}}\,	\mathbb{E} \|\hat{\bm{\Sigma}}- \bm{\Sigma}(f)\|^2  \leq  \sup _{\mathcal{F}_{\beta}}\,\mathbb{E} \|\hat{f} - f\|_\infty^2 &=& \mathcal{O}\left [ \left \{\frac{\log(np)}{np}\right \} ^{\frac{2\beta}{2\beta+1}}\right ]\nonumber \\
		%	&=&\mathcal{O}\left [ \left \{\frac{\log(np)}{np}\right \} ^{\frac{2\beta}{2\beta+1}}\right ]\nonumber \\
		\sup _{\mathcal{F}_{\beta}}\,	\mathbb{E} \|\hat{\bm{\Omega}} - \bm{\Sigma}^{-1}(f)\|^2  &=&\mathcal{O}\left [ \left \{\frac{\log(np)}{np}\right \} ^{\frac{2\beta}{2\beta+1}}\right ].
	\end{eqnarray*}
\end{theorem}
The proof of Theorem  \ref{theorem1} can be found in the Appendix \ref{app:theorem1} and is the main result of our work. The most important part of this proof is the derivation of the convergence rate for the spectral density estimator $\hat{f}$ under the $L_\infty$ norm. In the original work, \citet{cai2010nonparametricfest} established an $L_2$ rate for a wavelet nonparametric mean estimator in a gamma regression where the data are assumed to be independent. In our work, the spectral density estimator $\hat{f}$ is based on the gamma distributed data $W_{i,1},\ldots,W_{i,p}$, which are only asymptotically independent. Moreover, the mean of these data is not exactly $f(\pi x_1),\ldots,f(\pi x_p)$, but is corrupted by the diagonalization error given in Lemma \ref{lemma:DCTdiag}. This error adds to the error that arises via binning and VST and that describes the deviation from a Gaussian distribution, as derived in \citet{cai2010nonparametricfest}. Finally, we need to obtain an $L_\infty$ rather than an $L_2$ rate for our spectral density estimator. Overall, the proof requires different tools than those used in \citet{cai2010nonparametricfest}. \\
To get the $L_\infty$ rate for $\hat{f}$, we first derive that for the periodic smoothing spline estimator $\widehat{H(f)}$ of the log-spectral density. To do so, we use a closed-form expression of its effective kernel obtained in \citet{schwarz2016unified}, thereby carefully treating various (dependent) errors that describe  deviations from a Gaussian nonparametric regression with independent errors and mean $f(\pi x_i)$. Note also that although the periodic smoothing spline estimator is obtained on $T$ binned points, the rate is given in terms of the vector dimension $p$. Then,  using the Cauchy-Schwarz inequality and a mean value argument, this rate is translated into the $L_\infty$ rate for the spectral density estimator $\hat{f}$. To obtain the rate for the Toeplitz covariance matrix estimator is enough to note that $\mathbb{E}\|\hat{\bm\Sigma}-\bm\Sigma\|^2\leq \mathbb{E}\|\hat{f}-f\|^2_\infty$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%SIMULATION STUDY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulation Study}
\label{sec:simulation}
\doublespacing
In this section, we compare the performance of the proposed Toeplitz covariance estimator, denoted as \textbf{VST-DCT}, with the \textbf{tapering} estimator of \citet{cai2013optimal} and with the \textbf{sample covariance} matrix. We consider Gaussian vectors $\bm Y_1,...,\bm Y_n{\overset{i.i.d.}{\sim}}\mathcal{N}_p(0,\bm \Sigma_p)$  with \textbf{(A)} $p=5000,\, n=1$, \textbf{(B)} $p=1000,\, n=50$ and  \textbf{(C)} $p=5000,\, n=10$,  and  with the autocovariance functions $\sigma:\, \mathbb{Z} \to \mathbb{R}$ 
\begin{enumerate}
	\item[\textbf{(1)}] of a polynomial decay $\sigma(\tau)= 1.44(1+|\tau|)^{-5.1}$,
	\item[\textbf{(2)}] of an $\text{ARMA}(2,2)$ model with coefficients $(0.7,-0.4,-0.2,0.2)$ and innovations variance $1.44$,
	\item[\textbf{(3)}] such that the corresponding spectral density is Lipschitz continuous but not differentiable: $f(x)= 1.44\{|\sin(x+0.5\pi)|^{1.7}+0.45\}$.
\end{enumerate}
In particular,  $\mbox{var}(Y_i)=1.44$ in all three examples. Figure \ref{fig_Simulation_A} shows the spectral densities and the corresponding autocorrelation functions for the three examples. 
\begin{figure}[h]
	\centering
	\begin{subfigure}[H]{0.3\textwidth}
		\caption*{ \textbf{(1) Polynomial $\sigma$}}
		\includegraphics[width=\linewidth]{Figures/sdf_ex1.pdf} %{Figures/sdf_poly5.1_p5000_n1.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}
		\caption*{\textbf{(2) ARMA(2,2)}}
		\includegraphics[width=\linewidth]{Figures/sdf_ex2.pdf} %{Figures/sdf_ARMA_p5000_n1.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}
		\caption*{\textbf{(3) Lipschitz cont. $f$}}
		\includegraphics[width=\linewidth]{Figures/sdf_ex3.pdf} %{Figures/sdf_Lip_p5000_n1.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}
		\includegraphics[width=\linewidth]{Figures/acf_ex1.pdf} %{Figures/acf_poly5.1_p5000_n1.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}		
		\includegraphics[width=\linewidth]{Figures/acf_ex2.pdf} %{Figures/acf_ARMA_p5000_n1.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.3\textwidth}		
		\includegraphics[width=\linewidth]{Figures/acf_ex3.pdf} %{Figures/acf_Lip_p5000_n1.pdf}
	\end{subfigure}
	%\begin{subfigure}[h]{\textwidth}
	%	\centering
	%	\includegraphics[width=0.4\linewidth]{Figures/legend.pdf}
	%\end{subfigure} 
	\caption{ Spectral density functions (first row)  and autocovariance functions (second row) for examples \textbf{1}, \textbf{2}, \textbf{3}. }
	\label{fig_Simulation_A}
\end{figure}

A Monte Carlo simulation with $100$ iterations is performed using R (version $4.1.2$, seed $42$).  For our VST-DCT estimator, we use a cubic periodic spline, i.e., $q=2$ is set in (\ref{eq:Spline}). The binning parameters are set to $T=500$ bins with $m=10$ points for \textbf{(A)} and $T=500$ bins with $m=100$ points for both \textbf{(B)} and \textbf{(C)}. \\
To select the regularisation parameter for our estimator, we implemented the restricted maximum likelihood (ML) method, generalized cross validation (GCV) and the corresponding oracle versions,  i.e., as  if $\bm\Sigma$ were known.% \citep[see][]{craven1978smoothing,Krivobokova:2013}. 
The tapering parameter can be selected using cross-validation \citep[see][]{bickel2008regularized}, if $n>1$, that is under scenarios \textbf{(B)} and \textbf{(C)}. For this, the $n$ observations are divided by $30$ random splits into a training set of size $n_1=2/3n$ and a test set of size $n_2=n/3$. Let $\bm\Sigma_1^{\nu}$ and $\bm\Sigma_2^{\nu}$ be the sample covariance matrices from the $\nu$-th split, where the sample covariance matrix is defined as $(n-1)^{-1}\sum_{i=1}^{n} \bm Y_i\bm Y_i^t$ with averaged diagonals. The tapering parameter $k$ is then estimated
as
$$ \hat{k}_{cv}=\min_{k=2,3,...,p/2} \frac{1}{30}\sum_{\nu=1}^{\TT{30}} \| Tap_k(\bm\Sigma_1^ {\nu})-\bm\Sigma_2^ {\nu}\|,$$
where $Tap_k(\bm\Sigma_2^{\nu})$ is the tapering estimator of \citet{cai2013optimal} with parameter $k$.  %For all optimization problems, the \texttt{R} function \texttt{optimize} is taken. 
If $n=1$, that is, under scenario \textbf{(A)}, \citet{wu2009banding} suggest to split the time series $\bm Y$ into $l$ non-overlapping subseries of length $p/l$ and then proceed as before to select the tuning parameter $k$. 
%In particular, there is a trade-off between the number and the length of the subseries. 
To the best of our knowledge, there is no data-driven method for selecting this parameter $l$. Using the true covariance matrix $\bm\Sigma$, we selected $l=30$ subseries for the example $\bm{1}$ and $l=15$ subseries for the examples $\bm{2}$ and $\bm{3}$. The parameter $k$ can then be chosen by cross-validation as above. We employ this approach under scenario \textbf{(A)} instead of an unavailable fully data-driven criterion and name it semi-oracle. Finally, for all three scenarios \textbf{(A)}, \textbf{(B)} and \textbf{(C)}, the oracle tapering parameter is computed using grid search for each Monte Carlo sample as $\hat{k}_{or}=\arg \min_{k=2,3,...,p/2} \| Tap_k(\tilde{\bm\Sigma})-\bm\Sigma\|, $ where $\tilde{\bm\Sigma}$ is the sample covariance matrix.  To speed up the computation, one can replace the spectral norm by the $\ell_1$ norm, as suggested by \citet{bickel2008regularized}.\\
%Figure \ref{fig_Simulation_A} and \ref{fig_Simulation_B} show the tapering and the VST-DCT estimator for the spectral density function (first row) and the covariance function (second row) for the three examples and the scenarios \textbf{(A)} $p=5000,\, n=1$ and \textbf{(B)} $p=1000,\, n=50$, respectively. For the scenario \textbf{(C)} the plots are indistinguishable from those in Figure \ref{fig_Simulation_B}.{\color{red}TK: for C the tapering estimator looks much worse, so that the plots should be different, please check and eventually add Fig 3; also for the plots choose the fit, which belongs to the median of the risk and plot this; I wrote already this down} \\
In Tables \ref{tab_Simulation_A}, \ref{tab_Simulation_B} and  \ref{tab_Simulation_C}, the errors of the Toeplitz covariance estimators with respect to the spectral norm and the computation time for one Monte Carlo iteration are given for scenarios  \textbf{(A)},  \textbf{(B)} and \textbf{(C)}, respectively. To illustrate the goodness-of-fit of the spectral density, the $L_2$ norm $\|\hat{f}-f\|_2$ is also computed.  \\
The results show that the tapering and VST-DCT estimator perform overall similar in terms of the spectral norm risk. This is not surprising as both estimators are proved to be rate-optimal. Moreover, both the tapering and VST-DCT estimators are clearly superior to the inconsistent sample Toeplitz covariance matrix. A closer look at the numbers shows that the VST-DCT method has better constants, i.e., VST-DCT estimators have somewhat smaller errors in the spectral norm than the tapering estimators across all examples, but especially under scenario \textbf{(C)}. The oracle estimators show similar behaviour, but are slightly less variable compared to the data-driven estimators. In general, both the tapering and VST-DCT estimators perform best for example \textbf{1}, second best for example \textbf{3} and worst for example \textbf{2}, which traces back to functions complexity. 
%Only in example \textbf{(A.1)} does the tapering estimator perform slightly better. For scenario  \textbf{(B)}, the VST-DCT estimator even has a smaller error in the spectral norm than the oracle version of the tapering estimator.  For $n=10$, the cross-validation method fails to select  a suitable tapering parameter, see Table \ref{tab_Simulation_C}.  
In terms of computational time, both methods are similarly fast for scenarios \textbf{(A)} and \textbf{(B)}. For scenario \textbf{(C)}, the tapering method is much slower due to the multiple high-dimensional matrix multiplications in the cross-validation method. It is expected that for larger $p$ the tapering estimator is much more computationally intensive than the corresponding VST-DCT estimator. %Note that the tapering method requires manual parameter selection in the $n=1$ case.
%Note that the resulting Toeplitz covariance estimators are guaranteed to be positive semi-definite only for VST-DCT. For the tapering estimator of the Toeplitz covariance matrix further matrix manipulations are required to obtain positive semi-definiteness.

% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Mon Aug 29 12:05:53 2022
\begin{table}[H]
	\centering
	\scriptsize
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c|}{ \textbf{(3) Lipschitz cont. $f$}} & time  \\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ & in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0069 & 0.0026 & 0.0817 & 0.0107 & 0.0340 & 0.0061 & 4.3658 \\ 
		VST-DCT (ML) & 0.0059 & 0.0022 & 0.0700 & 0.0097 & 0.0375 & 0.0063 & 4.3607 \\ 
		tapering (semi-oracle) & 0.0056 & 0.0022 & 0.0973 & 0.0152 & 0.0355 & 0.0098 & 4.9373 \\
		sample covariance & 162.4089 & 38.1068 & 484.2556 & 48.3138 & 244.3804 & 38.0949 & 0.3744 \\  \hline
		VST-DCT (GCV-oracle) & 0.0042 & 0.0017 & 0.0662 & 0.0094 & 0.0332 & 0.0057 &  \\ 
		VST-DCT (ML-oracle) & 0.0046 & 0.0019 & 0.0663 & 0.0092 & 0.0378 & 0.0062 &  \\ 
		tapering (oracle) & 0.0042 & 0.0017 & 0.0578 & 0.0085 & 0.0155 & 0.0037 &  	\end{tabular}
	\caption{ \textbf{(A)} $\bm{p=5000,\, n=1}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral and $L_2$ norm, respectively, as well as the average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_A}
\end{table}

%\begin{figure}[H]
%\scalebox{.85}{
%	\centering
%	\begin{subfigure}[h]{0.3\textwidth}
%		\caption*{ \textbf{(1) Polynomial $\sigma$}}
%	\includegraphics[width=\linewidth]{Figures/sdf_poly5.1_p1000_n50.pdf}
%	\end{subfigure}
%\begin{subfigure}[h]{0.3\textwidth}
%		\caption*{\textbf{(2) ARMA(2,2)}}
%		\includegraphics[width=\linewidth]{Figures/sdf_ARMA_p1000_n50.pdf}
%	\end{subfigure}
%	\begin{subfigure}[h]{0.3\textwidth}
%		\caption*{\textbf{(3) Lipschitz cont. $f$}}
%		\includegraphics[width=\linewidth]{Figures/sdf_Lip_p1000_n50.pdf}  
%	\end{subfigure}
%	\begin{subfigure}[h]{0.3\textwidth}
%		\includegraphics[width=\linewidth]{Figures/acf_poly5.1_p1000_n50.pdf}
%	\end{subfigure}
%\begin{subfigure}[h]{0.3\textwidth}		
%		\includegraphics[width=\linewidth]{Figures/acf_ARMA_p1000_n50.pdf}
%	\end{subfigure}
%	\begin{subfigure}[h]{0.3\textwidth}		
%		\includegraphics[width=\linewidth]{Figures/acf_Lip_p1000_n50.pdf}  %WRONG
%	\end{subfigure}
%	\caption{\textbf{(B)} $\bm{p=1000,\, n=50}$: Spectral density function (first row)  and autocovariance function (second row) for examples \textbf{1}, \textbf{2}, \textbf{3}. The true functions are in grey, the VST-DCT (ML) estimator is solid black and the tapering estimator is dashed black.}
%	\label{fig_Simulation_B}
%\end{figure}

% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Fri Aug 26 16:29:43 2022
\begin{table}[H]
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c|}{ \textbf{(3) Lipschitz cont. $f$}} &time  \\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ & in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0010 & 0.0003 & 0.0123 & 0.0015 & 0.0053 & 0.0008 & 35.8043 \\ 
		VST-DCT (ML) & 0.0008 & 0.0002 & 0.0123 & 0.0014 & 0.0061 & 0.0009 & 35.6555 \\ 
		tapering (CV) & 0.0026 & 0.0012 & 0.0220 & 0.0031 & 0.0079 & 0.0018 & 22.8628 \\ 
		sample covariance & 0.8833 & 0.5867 & 2.7060 & 0.8482 & 1.4063 & 0.6362 & 0.1200 \\ \hline
		VST-DCT (GCV-oracle) & 0.0006 & 0.0002 & 0.0112 & 0.0014 & 0.0046 & 0.0007 &  \\ 
		VST-DCT (ML-oracle) & 0.0007 & 0.0002 & 0.0121 & 0.0014 & 0.0060 & 0.0009 &  \\ 
		tapering (oracle) & 0.0019 & 0.0011 & 0.0168 & 0.0027 & 0.0065 & 0.0016 & \\ 
	\end{tabular}
	\caption{ \textbf{(B)} $\bm{p=1000,\, n=50}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral and $L_2$ norm, respectively, as well as the average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_B}
\end{table}
% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Mon Aug 29 14:51:26 2022
\begin{table}[H]
	\centering
	\scriptsize
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c|}{ \textbf{(3) Lipschitz cont. $f$}} &time  \\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ & in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0009 & 0.0003 & 0.0114 & 0.0014 & 0.0059 & 0.0008 & 4.2319 \\ 
		VST-DCT (ML) & 0.0008 & 0.0002 & 0.0118 & 0.0014 & 0.0068 & 0.0009 & 4.2376 \\ 
		tapering (CV) & 0.0342 & 0.0260 & 0.1606 & 0.0401 & 0.0759 & 0.0287 & 622.9578 \\ 
		sample covariance & 9.2735 & 4.6053 & 29.5994 & 6.3001 & 12519.7295 & 4.6666 & 1.1721 \\ \hline
		VST-DCT (GCV-oracle) & 0.0006 & 0.0002 & 0.0102 & 0.0013 & 0.0050 & 0.0007 &  \\ 
		VST-DCT (ML-oracle) & 0.0007 & 0.0002 & 0.0115 & 0.0013 & 0.0066 & 0.0008 &  \\ 
		tapering (oracle) & 0.0308 & 0.0259 & 0.1435 & 0.0394 & 0.0697 & 0.0280 & \\ 
		
	\end{tabular}
	\caption{ \textbf{(C)} $\bm{p=5000,\, n=10}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral and $L_2$ norm, respectively, as well as the average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_C}
\end{table}

To test how robust our approach is to deviations from the Gaussian assumption, we simulated the data from gamma and uniform distributions and conducted a simulation study for the same scenarios and examples. The results are very similar to those of the Gaussian distribution, see supplementary materials for the details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%REAL DATATSET
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to Protein Dynamics}
\label{sec:realdata}
%\doublespacing
\linespread{1.2} 
We revisit the data analysis of protein dynamics performed in \citet{krivobokova2012partial} and \citet{singer2016partial}. We consider data generated by the molecular dynamics (MD) simulations for the yeast aquaporin (Aqy1) -- the gated water channel of the yeast {\it Pichi pastoris}. MD simulations are an established tool for studying biological systems at the atomic level on timescales of nano- to microseconds. The data are given as Euclidean coordinates of all $783$ atoms of Aqy1 observed in a $100$ nanosecond time frame, split into $20\,000$ equidistant observations. Additionally, the diameter of the channel $y_t$ at time $t$ is given, measured by the distance between two centers of mass of certain residues of the protein. The aim of the analysis is to identify the collective motions of the atoms responsible for the channel opening. In order to model the response variable $y_t$, which is a distance, based on the motions of the protein atoms, we chose to represent the protein structure by distances between atoms and certain fixed base points instead of Euclidean coordinates. That is, we calculated
$$
X_t=\{d(A_{t,1},B_1),\ldots,d(A_{t,783},B_1),d(A_{t,1},B_2),\ldots,d(A_{t,783},B_y)\}\in\mathbb{R}^{4\cdot 783},
$$
where $A_{t,i}\in\mathbb{R}^3$, $i=1, \ldots,783$ denotes the $i$-th atom of the protein at time $t$, $B_j\in\mathbb{R}^3$, $j=1,2,3,4$, is the $j$-th base point and $d(\cdot,\cdot)$ is the Euclidean distance. Figure \ref{fig_Aqua} shows the diameter $y_t$ and the distance between the first atom and the first center of mass. \\
\begin{figure}[H]
	%\scalebox{.85}{
	\centering
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\linewidth]{Figures/aqy1_X1.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\linewidth]{Figures/aqy1_y.pdf}
	\end{subfigure}
	\caption{Distance between the first atom and the first center of mass of aquaporin (left) and the opening diameter $y_t$ over time $t$ (right).}
	\label{fig_Aqua}
\end{figure}
It can therefore be concluded that a linear model $\bm{Y}=\bm{X}\bm\beta+\bm\epsilon$ holds, where $\bm{Y}=(y_1,\ldots,y_{20\,000})^t$, $\bm{X}=(X_1^t,\ldots,X_{20\,000}^t)^t$, $\bm\beta\in\mathbb{R}^{4\cdot 783}$, $\bm\epsilon\in\mathbb{R}^{20\,000}$. This linear model has two specific features which are intrinsic to the problem: first, the observations are not independent over time and second, $X_t$ is high-dimensional at each $t$ and only few columns of $\bm{X}$ are relevant for $\bm{Y}$. \citet{krivobokova2012partial} have shown that the partial least squares (PLS) algorithm performs exceptionally well on this type of data, leading to a small-dimensional and robust representation of proteins, which is able to identify the atomic dynamics relevant for $\bm{Y}$. \citet{singer2016partial} studied the convergence rates of the PLS algorithm for dependent observations and showed that decorrelating the data before running the PLS algorithm improves its performance. Since $\bm{Y}$ is a linear combination of columns of $\bm{X}$, it can be assumed that $\bm{Y}$ and all columns of $\bm{X}$ have the same correlation structure. Hence, it is sufficient to estimate $\bm\Sigma=\mbox{cov}({\bm{Y}})$ to decorrelate the data for the PLS algorithm, i.e., 
$\bm\Sigma^{-1/2}\bm{Y}=\bm\Sigma^{-1/2}\bm{X}\bm\beta+\bm\Sigma^{-1/2}\bm\epsilon$
results in a standard linear regression with independent errors. \\
Our goal now is to estimate $\bm\Sigma$ and compare the performance of the PLS algorithm on original and decorrelated data.
For this purpose, we divided the data set into a training and a test set (each with $p=10\,000$ observations). First, we tested whether the data are stationary. The augmented Dickey-Fuller test confirmed stationarity for $\bm{Y}$ with a $p$-value$<0.01$. The Hurst exponent of $\bm{Y}$ is $0.85$, indicating moderate long-range dependence supported by a rather slow decay of the sample autocovariances (see grey line in the left plot of Figure \ref{PLS-results}). Therefore, we set $q=1$ for the VST-DCT estimator to match the low smoothness of the corresponding spectral density. Moreover, the smoothing parameter is selected with the restricted maximum likelihood method and $T=550$ bins are used. Figure \ref{PLS-results} (black line in the left plot) confirms that the covariance matrix estimated with our VST-DCT method almost completely decorrelates the channel diameter $\bm{Y}$ on the training data set. Next, we estimated  the regression coefficients $\bm\beta$ with the usual PLS algorithm, ignoring the dependence in the data. Finally, we estimated ${\bm\beta}$ with PLS that takes into account dependence using our covariance estimator $\hat{\bm\Sigma}$. Based on these regression coefficient estimators, the prediction on the test set was calculated. The plot on the right side of Figure \ref{fig_Aqua} shows the Pearson correlation between the true channel diameter on the test set and the prediction on the same test set based on raw (grey) and decorrelated data (black). Obviously, the performance of the PLS algorithm on the decorrelated data is significantly better for the small number of components. In particular, with just one PLS component, the correlation between the true opening diameter on the test set and its prediction that takes into account the dependence in the data is already $0.54$, while it is close to zero for PLS that ignores the dependence in the data. \citet{krivobokova2012partial} showed that the estimator of $\bm\beta$ based on one PLS component is exactly the ensemble-weighted maximally correlated mode (ewMCM), which is defined as the collective mode of atoms that has the highest probability to achieve a specific alteration of the response $\bm{Y}$. Therefore, an accurate estimator of this quantity is crucial for the interpretation of the results and can only be achieved if the dependence in the data is taken into account.  \\
\begin{figure}[H]
	\centering
	\begin{subfigure}[h]{0.48\textwidth} 	
		\includegraphics[width=\linewidth]{Figures/VST_y.decor.train.acf.pdf}
	\end{subfigure}
	\begin{subfigure}[h]{0.48\textwidth}
		\includegraphics[width=\linewidth]{Figures/VST_aqy1_cor.pdf}	
	\end{subfigure}		
	\caption{On the left, the auto-correlation function of $\bm{Y}$ (grey) and of $\hat{\bm\Sigma}^{-1/2}\bm{Y}$ (black), where $\hat{\bm\Sigma}$ is estimated with the VST-DCT method; On the right, correlation between the true values on the test data set and prediction based on partial least squares (in grey) and corrected partial least squares (black).}
	\label{PLS-results}
\end{figure}

Estimating $\bm\Sigma$ with a tapered covariance estimator has two practical problems. First, since we only have a single realization of a time series $\bm{Y}$ ($n=1$), there is no data-driven method for  selecting the tapering parameter. Second, the tapering estimator turned out not to be positive definite for the data at hand. To solve the second problem, we truncated the corresponding spectral density estimator $\hat{f}_{tap}$ to a small positive value, i.e., $\hat{f}_{tap}^+=\max\{\hat{f}_{tap}, 1/\log(p) \}$ \citep[see][]{cai2013optimal,mcmurry2010banded}. To select the tapering parameter with cross-validation, we experimented with different subseries lengths and found that the tapering estimator is very sensitive to this choice. For example, estimating the tapered covariance matrix based on subseries of length $8/15/30$ yields a correlation of $0.42/0.53/0.34$ between the true diameter and the first PLS component, respectively.  \\
Altogether, our proposed estimator is fully data-driven, fast even for large sample sizes, automatically positive definite and can handle certain long-memory processes. In contrast, the tapering estimator is not data-driven and must be manipulated to become positive definite. Our method is implemented in the R package \textit{vstdct}. 
%The aquaporin dataset is included to illustrate the estimators.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%FITTED CORRELATION FUNCTION BY VST-DCT
\begin{comment}
\begin{figure}[H]
\includegraphics[width=0.5\linewidth]{Figures/VST_fitted.acf.pdf}
\caption{Fitted correlation function of VST-DCT for $Y$ on the training set.}
\end{figure}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}
In this paper, we proposed a simple, fast, fully data-driven, automatically positive definite and minimax optimal estimator of Toeplitz covariance matrices from a large class that also includes covariance matrices of certain long-memory processes. \\
Our estimator is derived under the assumption that the data are Gaussian. However, simulations show that the suggested approach yields robust estimators even when the data are not normally distributed. In the context of spectral density estimation and Whittle likelihood, several papers investigated the effects of violating the Gaussian assumption on the   asymptotic distribution of the periodogram and hence on the Whittle likelihood. In particular, it has been shown that  the periodogram distribution $I_j\sim \Gamma\{1,f(2\pi j/p)+\o(1)\}$, $j=1,\ldots,\lfloor p/2\rfloor$ holds asymptotically for linear time series with i.i.d. innovations \citep[see Theorem 10.3.2 of ][]{brockwell1991time}, for mixing processes \citep[see Theorem 5.3 of ][]{rosenblatt2012stationary}, as well as for non-linear processes \citep[see][]{shao2007asymptotic}.  Since DFT and DCT matrices are closely related, we expect that equation (\ref{ourGamma}) also holds asymptotically for these non-Gaussian time series, but consider a rigorous analysis to be beyond the scope of this paper. \\
In fact, our numerical experiments have even shown that if the spectral density is estimated from $W_j=f(\pi x_j)+\epsilon_j$, that is, as if $W_j$ were Gaussian instead of gamma distributed, then the resulting spectral density estimator has almost the same $L_\infty$ risk (and hence the corresponding covariance matrix has almost the same spectral norm). Of course, such an estimator would lead to a wrong inference about $f(\pi x_j)$, since the growing variance of $W_j$ would be ignored. \\
Since our approach translates Toeplitz covariance matrix estimation into a mean estimation in an approximate Gaussian nonparametric regression, all approaches developed in the context of Gaussian nonparametric regression, such as (locally) adaptive estimation, as well as the corresponding (simultaneous) inference, can be directly applied.  %all approaches to (locally) adaptive nonparametric mean estimation, as well as to the corresponding (simultaneous) inference developed in the Gaussian nonparametric regression context can be directly employed.
Bayesian tools for adaptive estimation and inference in Gaussian nonparametric regression as proposed in \citet{serra2017adaptive} can also be employed. %\\The simple construction of our estimator makes it very attractive to employ it as a building block in more complicated models (e.g., for modelling dependent data with non-zero mean), as well as for inference about Toeplitz covariance matrices. 

%\section*{Acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\begin{appendix}
	\label{appendix}
	\appendix
	\allowdisplaybreaks
	\numberwithin{equation}{section}
	\section{Appendix}\label{app:aux}
	Throughout the appendix, we denote by $c, c_1, C, C_1, \dots $ etc. generic constants, that are independent of  $n$ and $p$. To simplify the notation, the constants are sometimes skipped and we write $\lesssim$ for less than or equal to up to constants.
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Proof of Lemma 1} \label{app:lemma1}
	We embed the $p$-dimensional Toeplitz  matrix $\bm \Sigma=\text{toep}(\sigma_0,\dots, \sigma_{p-1})$ in a $(2p-2)$-dimensional circulant matrix  $\tilde{\bm \Sigma}=\text{toep}(\sigma_0,\dots, \sigma_{p-1}, \sigma_{p-2},\dots, \sigma_{1})$. 
	Then, $\tilde{\bm \Sigma}= \bm U \bm \Lambda \bm U^*,$	where $\bm U=(2p-2)^{-1/2}[\exp\left\{\pi \texttt{i} (k-1)(l-1)/(p-1)\right\}]_{k,l=1}^{2p-2}$  with the conjugate transpose $\bm U^*$, and $\bm \Lambda$ is  a diagonal matrix with the $k$-th diagonal value for $k=1,...,p$ given by
	\begin{align*}
	\lambda_k=&\sum_{l=1}^{p} \sigma_{l-1} \exp\{ \pi \texttt{i} x_k(l{-}1)\} {+}\sum_{l=p+1}^{2p-2} \sigma_{2p-1-l} \exp\{ \pi \texttt{i} x_k(l{-}1)\},%=	f(x_k) {+} \mathcal{O}\{ \log(p)p^{-\beta}\},
	\end{align*} where $x_k=(k-1)/(p-1)$.   
	%For the second equality  we used the approximation properties of the discrete Fourier series for H\"older continuous  functions  \citep[see][]{zygmund2002trigonometric}. Since the results are uniform, the hidden constants do not depend on $k$. 
	By symmetry, $\lambda_k=\lambda_{2p-1-k}$ for $k=p+1,...,2p-2$.
	Furthermore,	
	$\bm \Sigma= \bm V^* \bm \Lambda\bm V,$
	where $\bm V \in \mathbb{C}^{(2p-2)\times p}$ contains the first $p$ columns of $\bm U$. %Let $\bm D$ be the $p$-dimensional DCT-I matrix.
	Then, for $i,j=1,\dots,p$	
	\begin{align*}
	(\bm D \bm \Sigma \bm D )_{i,j}=(\bm D \bm V^* \bm \Lambda \bm V  \bm D )_{i,j}=   \sum_{r=1}^{2p-2} \lambda_r \sum_{q=1}^{p} \bm D_{iq} \bm   V^*_{qr} \sum_{s=1}^{p} \bm V_{rs} \bm D_{sj}.
	\end{align*}
	Note that $ (\sum_{s=1}^{p} \bm V_{rs} \bm D_{sj} )^* =  \sum_{q=1}^{p} \bm D_{jq} \bm V^*_{qr}$.
	Define $a_{s}= 1/\sqrt{2}$  for $s=1,p$ and $a_s=1$ for $s=2,...,p-2$.  Then,
	\begin{align}
	\sum_{s=1}^{p} \bm V_{rs} \bm D_{sj} &= (p-1)^{-1} \sum_{s=1}^{p} a_{s} a_{j} \exp\{ \pi \texttt{i} (r-1)x_s\} \cos\{ \pi (j-1)x_s\} \nonumber \\
	&= a_j\frac{\sqrt{2}-2}{p-1} \mathbbm{1}_{\{|j-r| \text{ even}\}} +\frac{a_j}{p-1} \sum_{s=1}^{p}\exp\{ \pi \texttt{i} (r-1)x_s\} \cos\{ \pi (j-1)x_s\}\nonumber\\ 
	&=a_jb(j,r) +a_j\texttt{i}c(j,r), \nonumber
	\end{align}
	where 
	\begin{align*}
	&b(j,r)= \frac{\sqrt{2}-2}{p-1} \mathbbm{1}_{\{|j-r| \text{ even}\}} +\frac{1}{p-1}\sum_{s=1}^{p} \cos\{ \pi (r-1)x_s\} \cos\{ \pi (j-1)x_s\},\\
	&c(j,r)=\frac{1}{p-1}\sum_{s=1}^{p} \sin\{ \pi (r-1)x_s\} \cos\{ \pi (j-1)x_s\}.
	\end{align*}
	In particular, $b(j,r)=b(j,2p-r)$ and $c(j,r)=-c(j,2p-r)$ for  $r=p+1,\dots,2p-2$.
	Together, we have
	\begin{align}
	(\bm D \bm \Sigma \bm D )_{i,j} = %a^2_i\sum_{r=0}^{2p-3} (b(i,r)+\texttt{i}c(i,r))(b(i,r)-\texttt{i}c(i,r))
	&\begin{cases}
	a^2_j\sum_{r=1}^{2p-2} \lambda_r \left \{b(j,r)^2+c(j,r)^2 \right \}, &i{=}j\\
	a_ia_j\sum_{r=1}^{2p-2}  \lambda_r(b(i,r)+\texttt{i}c(i,r))(b(j,r)-\texttt{i}c(j,r)), &i{\neq}j.
	\end{cases} \label{absum}	
	\end{align}
	Some calculations show that for $r=1,\dots p$
	\begin{align}  \linespread{0.3}
	&\frac{1}{p{-}1}\sum_{s=1}^{p} \cos\{ \pi (r{-}1)x_s\}  \cos\{ \pi (j{-}1)x_s\}
	{=}\begin{cases}
	\frac{1}{p-1}, & r{\neq}j, r{-}j\text{ even}\\
	0, & r{-}j\text{ odd}\\
	\frac{p+1}{2p-2},&r{=}j{\neq}1,p\\
	\frac{p}{p-1},&r{=}j{=}1,p
	\end{cases} \label{coscos}\\
	&\frac{1}{p{-}1}\sum_{s=1}^{p} \sin\{ \pi (r{-}1)x_s\}  \cos\{ \pi (j{-}1)x_s\}\nonumber \\
	&=\frac{\{1-(-1)^{r-j}\}}{4(p-1)} \left [\text{cot} \{\pi(r-j)/(2p-2)\}  +\text{cot} \{\pi(j+r-2)/(2p-2)\} \right].\label{sincos} 
	\end{align}
	Using the Taylor expansion of $\cot(x)$ for $0<|x|<\pi$ one obtains for $r=1,\dots p$
	\begin{equation}
	c(j,r)=\frac{(r-1)\{1-(-1)^{r-j}\}}{\pi\{(r-1)-(j-1)\}\{(r-1)+(j-1)\}} +\mathcal{O}\left \{ \frac{2\pi(2r-2)}{3(2p-2)^2}\right \}, \label{sincos2} 
	\end{equation} where the $\mathcal{O}$ term does not depend on $j$ and the hidden constant does not depend on $r,p$.
	%We used that $g(x)=\sin\{ \pi (r-1)x\}  \cos\{ \pi (j-1)x\}$ is an odd function on $[0,1]$ when $r-j$ is even. If $r-j$ is odd, then $g$ is an even and analytic function on $[0,1]$. Thus, $c(r,j)=\int_0^1 g(x)dx +\mathcal{O}(p^{-1})$. What about the constant? Does it depend on $r,j$? The error term is summed below.
	
	\textbf{Case $i=j$}\\
	If $i=j$, equations (\ref{absum}) - (\ref{sincos}) imply
	\begin{align*}
	(\bm D \bm \Sigma \bm D )_{j,j}{=}&\medmath{ a^2_j \sum_{\substack{r=1 \\ r \neq j,\\	r\neq 2p-j}}^{2p-2}  \lambda_r \left [\frac{(\sqrt{2}-1)^2}{(p-1)^{2}}\mathbbm{1}_{\{|r-j| \text{ even}\}} +	c(j,r)^2\right ] }+\lambda_j \cdot\begin{cases}
	\frac{2(\sqrt{2}-3/2 +p/2)^2}{(p-1)^2}, &j{\neq} 1,p\\
	\frac{(\sqrt{2}-2 +p)^2}{2(p-1)^2}, &j{=} 1,p
	\end{cases}\\
	&=\lambda_j\{0.5-\mathcal{O}(p^{-1})\} +\mathcal{O}(p^{-1}) + a^2_j\cdot \begin{cases}
	2\sum_{r=1}^{p-1} \lambda_r  c(j,r)^2, &j{\neq} 1,p\\
	\sum_{r=1}^{p-1} \lambda_r  c(j,r)^2, &j{=} 1,p,
	\end{cases}
	\end{align*} where the $\mathcal{O}$ terms do not depend on $j$.
	%By assumption $f$ is $\tilde{\alpha}$-Hölder continuous with $\tilde{\alpha}=\min\{\beta,1\}$. Thus, it holds $\lambda_{r}= \lambda_j +L_r |r-j|^{\tilde{\alpha}} p^{-\tilde{\alpha}}+\mathcal{O}\{\log(p)p^{-\beta}\}$ where   $L>0$ is the Hölder constant and $-L\leq L_r\leq L$ is a constant depending on $r$. 
	Since the complex exponential function is Lipschitz continuous with constant $L=1$, it holds $\lambda_{r}= \lambda_j +L_{r,j} |r-j|p^{-1}$ where $-1\leq L_{r,j}\leq 1$ is a constant depending on $r,j$.   Then,
	\begin{align*}
	\sum_{r=1}^{p-1}  \lambda_r  c(j,r)^2&=\lambda_j\sum_{r=1}^{p-1}   c(j,r)^2 + p^{-1} \sum_{r=1}^{p-1}  L_{r,j}|r-j| c(r,j)^2.%
	\end{align*}
	Since $\sum_{r=1}^{p-1}  \lambda_r  c(1,r)^2=-\sum_{r=1}^{p-1}  \lambda_r  c(p,r)^2,$ it is sufficient to consider $j=1,...,p-1$. We begin with first sum.  For a shorter notation, we use  $k:=r-1$ and $l:=j-1$ in the following. Then, summing the squares of the first term in (\ref{sincos2}) for $l=0,...,p-2$ yields
	\begin{align*}
	\frac{1}{\pi^2}\sum_{k=0}^{p-2}   \frac{k^2 \{1{-}(-1)^{k-l}\}^2}{ (k{-}l)^2(k{+}l)^2}{=} \frac{4}{\pi^2}\begin{cases}
	\sum_{x=1}^\infty \frac{1}{(2x{-}1)^2} {-} R_1(l,p)=\frac{\pi^2}{8}{-}R_1(l,p), &l=0\\
	\sum_{x=1}^\infty \frac{(2x)^2}{(2x{-}l)^2(2x{+}l)^2} {-} R_2(l,p)=\frac{\pi^2}{16}{-} R_2(l,p), &l \text{ odd}\\
	\sum_{x=1}^\infty \frac{(2x{-}1)^2}{(2x{-}1{-}l)^2(2x-1+l)^2} {-} R_2(l,p)=\frac{\pi^2}{16}{-} R_3(l,p), &\text{else}
	%\sum_{x=1}^\infty \frac{1}{(2x)^2} -R_3(l,p), &l{=}0\\
	\end{cases}	
	\end{align*} see chapter 23.2 of \citet{abramowitz1964handbook} on sums of reciprocal powers.
	If $p$ is even, then the residual terms are given by
	\begin{align*}
	R_1(l,p)&=\sum_{x=(p-2)/2}^\infty \frac{1}{(2x-1)^2} = \frac{ \phi^{(1)}\{(p-3)/2\}}{4}\\
	R_2(l,p)%&= \sum_{x=(p-2)/2+1}^\infty \frac{(2x)^2}{(2x-l)^2(2x+l)^2}\\ 
	&=\frac{2\phi\{(p{-}2{+}l)/2\}{-}2\phi \{(p{-}2{-}l)/2\}{+}
		l [\phi^{(1)}\{(p{-}2{+}l)/2\}{+} \phi^{(1)}\{(p{-}2{-}l)/2\}]}{16l} \\
	R_3(l,p)%&= \sum_{x=(p-2)/2}^{\infty} \frac{(2x-1)^2}{(2x-1-l)^2(2x-1+l)^2}\\
	%16 j^{-1} \left\{2\phi\left ( \frac{j-1}{2}+ p\right)-2\phi\left (\frac{-j-1}{2}+ p\right) +
	%j \left \{ \phi^{(1)}\left ( \frac{j-1}{2}+ p\right)+ \phi^{(1)}\left (\frac{-j-1}{2}+ p\right)\right\} \\
	&=\frac{2\phi\{(p{-}3{+}l)/2\}{-}2\phi \{(p{-}3{-}l)/2\}{+}l [\phi^{(1)}\{(p{-}3{+}l)/2\}{+} \phi^{(1)}\{(p{-}3{-}l)/2\}]}{16l}, 
	\end{align*}
	where $\phi$ and $\phi^{(1)}$ denote the digamma function and its derivative. If $p$ is odd, similar remainder terms can be derived.
	To see that $R_i(l,p)=\mathcal{O}(p^{-1})$ for $i=1,2,3$ and uniformly in $l$ we use that
	asymptotically $\phi(x){\sim} \log(x){-}1/(2x)$ and $\phi^{(1)}(x){\sim} 1/x{+}1/(2x^2).$  %Since $l$ is fixed for sufficiently large $p$ there is a global constant not depending on $l$.
	%For example,
	%\begin{comment}
	%\begin{align*}	
	%R_2(l,p) \sim \left .\left \{2\log\left (\frac{p{-}3{+}l}{p{-}3{-}l} \right )-\frac{2-2l}{p{-}3{+}l} +\frac{2+2l}{p{-}3{-}l} + \frac{2l}{(p{-}3{+}l)^2} +\frac{2l}{(p{-}3{-}l)^2} \right \}\right/(16l)
	%\end{align*} Since $l$ is fixed for sufficiently large $p$ the residual term $R_2(j,p)$ vanishes at the rate $\mathcal{O}(p^{-1})$ with a global constant not depending on $l$.
	\begin{comment}
	$\phi^{(1)}(z)=\sum_{k=0}^\infty 1/(k+z)^2$ for $z>0$ and $\phi(n+1/2)=-\gamma-2\log (2)+\sum_{k=1}^n(2k+1)^{-1}$ for all integers $n$, where $\gamma$ is the Euler-Mascheroni constant \citep[][chapter 6]{abramowitz}. %For example,
	Thus,
	\begin{align*}
	&\phi\{(p{-}2{+}l)/2\}{-}\phi \{(p{-}2{-}l)/2\}= \phi\{(p{-}3{+}l)/2+1/2\}{-}\phi \{(p{-}3{-}l)/2+1/2\}\\
	&=\sum_{k=(p{-}3{-}l)/2+1}^ {(p{-}3{+}l)/2}\frac{1}{2k+1}\leq \frac{l}{p-1-l}.
	\end{align*}
	Since $l$ is fixed for sufficiently large $p$ there is a constant $C>0$ independent of $l$ such that  $\frac{1}{p-1-l} \leq Cp^{-1}$. Furthermore,
	\end{comment}
	
	\begin{comment}
	It remains to show that the mixed term and the squared $\mathcal{O}$ term when summation is taken over $k=r-1=0,...,p-2$ are negligible. This is the cases since for a sufficiently large constant $C_1>0$
	\begin{align*}
	&\left |\frac{C_0 }{3} \sum_{k=0}^{p-2} \frac{k^2\{1-(-1)^{k-l}\}^2}{(k{-}l)(k{+}l)(2p-2)^2} \right| \leq \left |\frac{4C_1 }{3} \sum_{k=0}^{p-2} \frac{1}{(2p-2)^2} \right| =\mathcal{O}(p^{-1}).
	\end{align*}	
	Furthermore, $	\left |\frac{4C^2_0\pi^2}{9} \sum_{k=0}^{p-2}\frac{(2r{-}2)^2}{(2p-2)^4} \right| =\mathcal{O}(p^{-1}).$	
	\end{comment}
	The mixed term $\frac{4}{3} \sum_{k=0}^{p-2} \frac{k^2\{1-(-1)^{k-l}\}^2}{(k{-}l)(k{+}l)(2p-2)^2}$ and the squared $\mathcal{O}$ term $\frac{16\pi^2}{9} \sum_{k=0}^{p-2}\frac{k^2}{(2p-2)^4}$ are both of the order $p^{-1}$.
	Furthermore,
	\begin{align*}
	& \frac{1}{p} \sum_{r=1 }^{p-1}  L_{r,j}|r-j| c(r,j)^2= \frac{1}{p}  \sum_{k=0}^{p-2}   \frac{k^2\{1{-}(-1)^{k-l}\}^2 L_{k,l}|k-l|
	}{\pi^2(k-l)^2(k+l)^2} =\mathcal{O}\{\log(p)p^{-1}\},
	\end{align*}
	since the harmonic sum diverges at a rate  of $\log(p)$. Finally,   $\lambda_{j}= f(x_j)+\mathcal{O}\{\log(p)p^{-\beta}\}$
	by the uniform approximation properties of the discrete Fourier series for H\"older continuous  functions  \citep[see][]{zygmund2002trigonometric}.
	All together, we have shown that $(\bm D \bm \Sigma \bm D )_{j,j}= f(x_j)+ \mathcal{O}\{\log(p)p^{-\beta}\} + \mathcal{O}(p^{-1})$ where the $\mathcal{O}$ terms are uniform over $j=1,...,p$.
	
	\textbf{Case $i\neq j$ and $|i-j|$ is even}\\
	In this case, $(\bm D \bm \Sigma \bm D )_{i,j} = a_ia_j\sum_{r=1}^{2p-2}  \lambda_r \{b(i,r)b(j,r) + c(i,r)c(j,r)\}$ since $|r-j|,$ and $|r-i|$ are  both either odd or even. Note that $b(i,r)b(j,r)=\mathcal{O}(p^{-2})$ for $r\neq i,j$  and $b(i,r)b(j,r)=\mathcal{O}(p^{-1})$ if $i=r$ or $j=r$, where the $ \mathcal{O}(\cdot)$ terms are uniformly in $i,j$. Since $b(i,r)\geq 0$ and $\|f\|_\infty<M_0$, it follows $a_ia_j\sum_{r=1}^{2p-2}  \lambda_r b(i,r)b(j,r) =\mathcal{O}(p^{-1})$ uniformly in $i,j$. To show that $a_ia_j\sum_{r=1}^{2p-2}\lambda_r c(i,r)c(j,r)=\mathcal{O}(p^{-1})$, we proceed similarly as before. %By the Hölder continuity of $f$, we have 
	%\begin{align*}
	%\sum_{r=1}^{p-1}  \lambda_r  c(i,r)c(j,r)&=[\lambda_j+\mathcal{O}\{\log(p)p^{-\beta}\}]\sum_{r=1}^{p-1}   c(i,r)c(j,r) + p^{-\tilde{\alpha}} \sum_{r=1}^{p-1}  L_r|r-j|^{\tilde{\alpha}} c(i,r)c(j,r).
	%\end{align*}
	Setting $k{=}r{-}1, \,l{=}j{-}1, \,m{=}i{-}1$ and using that $l{\neq}m$ and $|l{-}m|$ is even, one obtains 
	\begin{align*}
	&\frac{1}{\pi^2}\sum_{k=0}^{p-2}   \frac{k^2 \{1-(-1)^{k-l}\}\{(-1)^{k-m}{-}1\}}{ (k{-}l)(k{+}l)(k{-}m)(k{+}m)}\\
	&= \frac{4}{\pi^2}\begin{cases}
	\sum_{x=1}^\infty \frac{(2x)^2}{(2x{-}l)(2x{+}l)(2x{-}m)(2x{+}m)} {-} R_1(l,m,p)={-} R_1(l,m,p), &l,m \text{ odd}\\
	\sum_{x=1}^\infty \frac{(2x{-}1)^2}{(2x{-}1{-}l)(2x{-}1{+}l)(2x{-}1{-}m)(2x{-}1{+}m)} {-} R_2(l,m,p)={-} R_2(l,m,p), &l,m \text{ even,}
	%\sum_{x=1}^\infty \frac{1}{(2x)^2} -R_3(l,p), &l{=}0\\
	\end{cases}	
	\end{align*}
	where for even $p$ the residual terms are given by 
	\begin{align*}
	R_1(l,m,p)%&= \sum_{x=(p-2)/2+1}^\infty\frac{(2x)^2}{(2x{-}l)(2x{+}l)(2x{-}m)(2x{+}m)}\\ 
	&=\frac{l\phi\{(p{-}2{+}l)/2\}{-}l\phi\{(p{-}2{-}l)/2\}{-}m\phi\{(p{-}2{+}m)/2\}{+}m\phi\{(p{-}2{-}m)/2\}}{4l^2-4m^2} \\
	R_2(l,m,p)%&= \frac{(2x{-}1)^2}{(2x{-}1{-}l)(2x{-}1{+}l)(2x{-}1{-}m)(2x{-}1{+}m)} \\
	&=\frac{l\phi\{(p{-}3{+}l)/2\}-l\phi\{(p{-}3{-}l)/2\}{-}m\phi\{(p{-}3{+}m)/2\}{+}m\phi\{(p{-}3{-}m)/2\}}{4l^2-4m^2}.
	\end{align*}
	%where $\phi$ is the digamma function. 
	If $p$ is odd, analogous residual terms can be derived. %In a similar way as before, one can show that the two residual terms vanish at a rate $\mathcal{O}(p^{-1})$ and uniformly in $i,j$.
	Using similar techniques as before, one can show that the two residual terms and the remaining mixed and square terms vanish at a rate of the order $\mathcal{O}(p^{-1})$ and uniformly in $i,j$.
	%By the symmetry properties of the cotangens follows   $\sum_{r=0}^{2p-3} c(i,r)c(j,r)=0$. Thus, $(\bm D \bm \Sigma \bm D )_{i,j} = a_ia_j\sum_{r=0}^{2p-3}  \lambda_r b(i,r)b(j,r) = \mathcal{O}(p^{-1})$ uniformly in $i,j$.
	
	\textbf{Case $i\neq j$ and $|i-j|$ is odd}\\
	$|r-i|$ and $|r-j|$ are either odd and even, or even and odd. Without loss of generality, assume that $|r-i|$ is even. Then, $(\bm D \bm \Sigma \bm D )_{i,j} =a_ia_j\sum_{r=1}^{2p-2}\lambda_r b(i,r)c(j,r)$. Since $b(i,\cdot)$ is an even function, $c(j,\cdot) $ is an odd function and $\lambda_r=\lambda_{2p-r}$, it follows $(\bm D \bm \Sigma \bm D )_{i,j} =0$. \hfill \qed
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Periodic Smoothing Splines}\label{app:SSper}
	In this section we recapitulate the results obtained in \citet{schwarz2016unified} for the equivalent kernels of periodic smoothing spline estimators.  Given data points $\{(x_i,Y_i)\}_{i=1}^N$ with true relationship $Y_i=f(x_i)+\epsilon_i$, where $x_i=i/N$ and the residuals $\epsilon_i$ satisfy standard assumptions, the periodic smoothing spline estimator $\hat{f}$ is the solution to
	$$\min_{s\in S_{per}(2q-1,\underline{x}_N)} \left [ \frac{1}{N} \sum_{i=1}^N \{Y_i-s(x_i)\}^2 + h^{2q} \int_0^1 \{ s^{(q)}(x)\}^2\,dx\right ],$$ 
	where $h>0$ is the smoothing parameter and $S_{per}(2q-1,\underline{x}_N)$ the periodic spline space of degree $2q-1$ based on knots $\underline{x}_N{=}(x_i)_{i=1}^N{=}\left (i/N\right)_{i=0}^N$.  
	\citet{schwarz2016unified} have shown that  $\hat{f}(x)=N^{-1}\sum_{i=1}^NW(x,x_i)Y_i$, where $W(x,x_i)$ is known as effective kernel of a periodic smoothing spline estimator and its explicit expression is given by
	$$W(x,x_i)=\sum_{j=1}^N \frac{\Phi \{ Nx + q, \exp (-2\pi  \mathtt{i} j/N)\} \overline{\Phi \{ Nx_i + q, \exp (-2\pi  \mathtt{i} j/N)\}}}{Q_{2q-2}(j/N)+h^{2q}(2\pi j)^{2q} \mbox{sinc} (\pi j/N)^{2q}},$$
	where  $\Phi(\cdot,\cdot)$ denotes exponential splines \citep{Schoenberg73} and polynomials $Q_{2q-2}(x)$ are formally defined as $Q_{2q-2}=\sum_{l=-\infty}^\infty\mbox{sinc}\{\pi(x+l)\}^{2q}$; for the explicit expressions see Section 4.1.1 in \citet{schwarzdiss}. A scaled version $K(x,t)$ of $W(x,t)$ in terms of the bandwidth parameter $h$ is defined as
	$K(x,t)=hW(hx,ht).$ We denote by $K_h(x,t)=K(x/h,t/h)=hW(x,t)$. Also, we use that $W(x,t)=\sum_{l=-\infty}^\infty \mathcal{W}(x,t+l)$ \citep[see Lemma 14 in][]{schwarzdiss}, where 
	$$\mathcal{W}(x,t)=\int_0^N\frac{\Phi \{ Nx + q, \exp (-2\pi  \mathtt{i} u)\} \overline{\Phi \{ Nx_i + q, \exp (-2\pi  \mathtt{i}u)\}}}{Q_{2q-2}(u)+h^{2q}(2\pi j)^{2q} \mbox{sinc} (\pi u)^{2q}}\, du$$  
	is the asymptotic equivalent kernel of smoothing spline estimators on $\mathbb{R}$. Finally, we denote $\mathcal{K}_h(x,t)=\mathcal{K}(x/h,t/h)=h\mathcal{W}(x,t).$ 
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\subsection{Proof of Theorem 1} \label{app:theorem1}
	The structure of the proof is as follows. First, we derive the $L_\infty$ rate of the periodic smoothing spline estimator $\widehat{H(f)}$. Then, using the Cauchy-Schwarz inequality and a mean value argument, the convergence rate of the spectral density estimator $\hat{f}$ is established. With the relationship $\mathbb{E}\|\hat{\bm\Sigma}-\bm\Sigma\|\leq \mathbb{E} \|\hat{f} - f\|_\infty^2 $ the first claim of the theorem follows. Finally, we prove the second statement on the precision matrices. For the sake of clarity, some technical lemmas used in the proof are listed separately in \ref{app:theorem1_aux}. 	
	\subsubsection{An upper bound on $\mathbb{E}  \|\widehat{H(f)} - H(f)\|_\infty^2 $}
	\begin{prop}\label{prop:riskHf}
		Let $\bm{\Sigma} \in \mathcal{F}_{\beta}$ such that $\beta>1/2$. If $h>0$ such that  $h\to 0$ and $hT\to \infty$,  then with $T=\lfloor p^{\upsilon}\rfloor$ for any $\upsilon\in((4-2\min\{1,\beta\})/3,1)$, the estimator $\widehat{H(f)}$ described in Section \ref{sec:method} with $q=\max\{1,\gamma\}$ satisfies
		$$\ \mathbb{E} \|\widehat{H(f)} - H(f)\|_\infty^2 =\mathcal{O} \left\{ \log(np)/(nph)\right\} + \mathcal{O}(h^{2\beta}).$$
	\end{prop}
	{\parindent0pt\textbf{Proof}}:	Application of the triangle inequality yields a bias-variance decomposition
	\begin{align}\label{eq:BiasVar}
	&\mathbb{E}\|\widehat{H(f)}- H(f)\|_\infty^2  \leq 2\mathbb{E}\|\widehat{H(f)}- \mathbb{E}\{\widehat{H(f)}\}\|_\infty^2 + 2 \|\mathbb{E}\{\widehat{H(f)}\}- H(f)\|_\infty^2.
	\end{align}
	Set $\tilde{T}=2T-2$ and ${x}_k=(k-1)/\tilde{T}$ for $k=1,...,\tilde{T}.$ Using Lemma \ref{lemma:representY}, we can write
	\begin{align*}
	\widehat{H(f)}(x)&= \frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}} W(x,x_k) Y^*_k= \frac{1}{\tilde{T}h}  \sum_{k=1}^{\tilde{T}} K_h(x,x_k) \left [H \{f(x_k)\} +\epsilon_k+ \zeta_k+\xi_k\right ], 
	\end{align*}
	where  $x\in[0,1]$, $|\epsilon_k|\lesssim(np)^{-1}+\log(p)(np)^{-\beta}$, $\zeta_k\sim \mathcal{N}(0,m^{-1/2})$ such that for $k\neq l$ holds $Cov(\zeta_k,\zeta_l)= \mathcal{O} \{ p^{-2} + \log(p)^2p^{-2\beta}\}$, and $\xi_k$ is a mean zero random variable satisfying $\mathbb{E}|\xi_k|^\ell \lesssim \log^{2\ell}(m) [m^{-\ell}+\left\{T^{-1}+T^{-1}\log(p) p^{1-\beta}\right\}^\ell]´$ for each integer $\ell>1$. Mirroring and renumerating $\zeta_k, \eta_k,\epsilon_k$ is similar as for $Y_k^*,$ $k=1,...,\tilde{T}$.  \\
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%VARIANCE
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\textbf{Upper bound on the variance}\\
	Using the above representation, one can write
	\begin{equation} \label{var:firstbound}
	\mathbb{E} \|\widehat{H(f)}- \mathbb{E}\{\widehat{H(f)}\}\|_\infty^2= \mathbb{E}  \left \|\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}} W (x, x_k) (\zeta_k+\xi_k) \right \|^2_\infty.
	\end{equation}
	%Lipschitz argument?
	First we reduce the supremum to a maximum over a finite number of points. If $q>1$,	then $W (\cdot, x_k)$ is Lipschitz continuous with constant $L>0$. In this case, it holds  almost surely that
	\begin{align*}
	&\sup_{x\in[0,1]} \left|\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  W (x, x_k)( \zeta_k+\xi_k )\right|^2\\
	&\leq  \left (\max_{1\leqslant j \leqslant \tilde{T}} \left |\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  W (x_j, x_k)( \zeta_k+\xi_k ) \right| +  \sup_{ \substack {x,x^\prime \in [0,1], \\ |x-x^\prime|<1/\tilde{T} }}\left |\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  \{W (x, x_k)-  W (x^\prime, x_k)\} ( \zeta_k+\xi_k )\right |\right )^2\\
	&\leq 2 \max_{1\leqslant j \leqslant \tilde{T}} \left |\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  W (x_j, x_k)( \zeta_k+\xi_k ) \right|^2 +\frac{2L^2}{\tilde{T}^4}\left(\sum_{k=1}^{\tilde{T}} \left| \zeta_k+\xi_k\right|\right )^2 .
	\end{align*}
	Using $\mathbb{E}[(\sum_{k=1}^{\tilde{T}} \left| \zeta_k+\xi_k\right| )^2]\lesssim \mathbb{E}[(\sum_{k=1}^{\tilde{T}} \left| \zeta_k\right|)^2]+\mathbb{E}[(\sum_{k=1}^{\tilde{T}} \left| \xi_k\right|)^2]\lesssim \tilde{T}^{2}m^{-1}$, one gets
	\begin{align} 
	&(\ref{var:firstbound}) \lesssim \mathbb{E}\left \{\max_{1\leqslant j \leqslant \tilde{T}} \left |\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  W (x_j, x_k)( \zeta_k+\xi_k ) \right|^2 \right\} +o\{(np)^{-1}\}. \label{ineq:sup2max}
	\end{align}
	If $q=1$, then  $\sum_{k=1}^{\tilde{T}} W (\cdot, x_k)$ is a piecewise linear function with knots at $x_j=j/\tilde{T}$. The factor $( \zeta_k+\xi_k )$ can be considered as stochastic weights that do not affect the piecewise linear property. Thus,  the supremum is attained at one of the knots $x_j=j/\tilde{T},\,j=1,...,\tilde{T}$, and (\ref{ineq:sup2max}) is also valid for $q=1$. Again with $(a+b)^2\leq 2a^2+2b^2$ we obtain
	\begin{align*} 
	&(\ref{var:firstbound}) \lesssim  \mathbb{E} \left \{\max_{1\leqslant j \leqslant \tilde{T}} \left |\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  W (x_j, x_k) \zeta_k \right|^2 \right\}+\mathbb{E} \left \{\max_{1\leqslant j \leqslant \tilde{T}} \left |\frac{1}{\tilde{T}} \sum_{k=1}^{\tilde{T}}  W (x_j, x_k)\xi_k  \right|^2 \right\}+o\{(np)^{-1}\}.
	\end{align*}
	
	%where we used that  $K_h (\cdot, x_k)$ is Lipschitz continuous if $q>1$. For $q=1$, the kernel function is  at  piecewise linear on $[y,y^\prime]$ with at most one knot. Thus,it consists of two Lipschitz-continuous pieces.  $W^{[0,1]}(\cdot ,t) \in S_{per}(2q-1; \underline{x}_N)$ and $2q-1>1$, it is Lipschitz continuous wrt. some constant $L$. However, the supremum of theses Lipschitz constants may not exost for $m$ to $\infty$.
	%%24.02.23 commented out
	%The exponential decay property of the kernel $K$ stated in Lemma \ref{lemma:decay} yields 
	%\begin{align}
	%&\sup_{x\in[0,1]} 
	%\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} \left |K_h (x, x_k)\right | \cdot \left | \zeta_k+\xi_k \right |\leq \sup_{x\in[0,1]}  \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} \gamma_h(x,x_k)\cdot \left | \zeta_k+\xi_k\right |, \nonumber
	%\end{align} where $\gamma_h(x,t)=C [\gamma^{|x-t|/h} + \frac{\gamma^{1/h}}{1-\gamma^{1/h}} \{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h}\}]$ for some constants $C>0$ and $\gamma\in (0,1)$. Next, we reduce %he supremum over $ [0,1]$ to the supremum over a finite set. 
	%Calculating the derivative of $s(x)=\sum_{k=1}^{\tilde{T}} \gamma_h(x,x_k)\cdot \left | \zeta_k+\xi_k\right |$ for $x\neq x_k,\, k=1,...,\tilde{T},$ gives almost surely
	%\begin{align*}
	%&\frac{\delta}{\delta x}  s(x) =  \frac {C\log(\gamma)}{h}\sum_{k=1}^{\tilde{T}}  \left [\gamma^{|x-t|/h}|x-x_k| + \frac{\gamma^{1/h}}{1-\gamma^{1/h}}\left \{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h} \right\} \right ] \left |\zeta_k+\xi_k\right|.
	%\end{align*}
	%Since $\frac{\delta}{\delta x}  s(x) >0$ almost surely for $x\neq x_k$, the extrema thus occur at $x_k,\, k=1,...,\tilde{T}.$
	%Define $s_1(x)= \sum_{k=1}^{\tilde{T}} \gamma_h(x,x_k) \mathbb{I}{\{(x- x_k)\geq 0\}}  \left |\zeta_k+\xi_k\right |$ and $s_2(x)= \sum_{k=1}^{\tilde{T}} \gamma_h(x,x_k) \mathbb{I}{\{(x- x_k)< 0\}}  \left |\zeta_k+\xi_k\right |$. 
	%Then, for $x\neq x_k$, $k=1,...,\tilde{T}$, the derivative of $s_1$ is 
	%\begin{align*}
	%&\frac{\delta}{\delta x} s_1(x)= -h^{-1}\log(\gamma) \sum_{k=1}^{\tilde{T}} \gamma^{(x_k-x)/h}\mathbb{I}{\{(x- x_k)\geq 0\}} \left | \xi_k+\zeta_k \right |=-h^{-1}\log(\gamma) s_1(x).
	%\end{align*}
	%Since $h^{-1}\log(\gamma) \neq 0$, the derivative of $s_1(x)$ vanishes if and only if the function vanishes itself. Noting that $s_1(x)>0$, the extrema thus occur at points where the function is not differentiable, that is at $\left(x_k\right)_{k=1}^{\tilde{T}}$. Similarly, one can show that $\sup_{x\in[0,1]} s_2(x) \leq \sup_{1\leq j\leq \tilde{T}} s_2(x_j)$ where $s_2(x)=\sum_{k=1}^{\tilde{T}} \gamma^{(x- x_k)/h} \mathbb{I}{\{(x- x_k)< 0\}} \left |\zeta_k+\xi_k\right |$
	
	%Let $s_2(x)=\sum_{k=1}^{\tilde{T}} \gamma^{(x- x_k)/h} \mathbb{I}{\{(x- x_k)< 0\}} \left |\zeta_k+\xi_k\right |$. Then for $x\neq x_k,\, k=1,...,\tilde{T}$,
	%\begin{align*}
	%&\frac{\delta}{\delta x} s_2(x)=  h^{-1}\log(\gamma)\sum_{k=1}^{\tilde{T}} \gamma^{(x- x_k)/h} \mathbb{I}{\{(x- x_k)> 0\}} \left | \xi_k+\zeta_k\right |=h^{-1}\log(\gamma) s_2(x).
	%\end{align*}
	%Hence, the derivative of $s_2(x)$ only  vanishes iff the function vanishes itself. Since $s_2(x)>0$ for $x\in[0,x_{\tilde{T}})$ and $s_2(x)=0$ for $x\in[x_{\tilde{T}},1]$ it follows that $s_2$ attains its minimum for $x\in[x_{\tilde{T}},1]$ and all other extrema occur at points where the function is not differentiable, i.e., at $\left(x_j\right)_{j=1}^{\tilde{T}}$.  Note that the functions $\gamma^{(x-t)/h}$ and $\gamma^{(t-x)/h}$ are strictly decreasing and increasing in $x$, respectively.
	%Together with the inequality $(a+b)^2\leq 2a^2+2b^2$ one obtains 
	
	We start with bounding $T_1=\mathbb{E}\{\max_{1\leq j \leq \tilde{T}}   |\frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}K_h(x_j,x_k) \zeta_k|^2 \}$ with Lemma 1.6 of \citet{tsybakov2009introduction}. This requires a bound on $ \|\frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}K_h(x_j,x_k) \zeta_k\|_{\psi_2}^2$  where $\|\cdot\|_{\psi_2}$ denotes the sub-Gaussian norm. In case of a Gaussian random variable the norm equals to the variance. %See \citet[chpt. 2.5]{vershynin2018high} for a definition of $\|\cdot\|_{\psi_2}$.
	Thus with  Lemma \ref{lemma:decay} and Lemma \ref{lemma:representY}, we obtain
	\begin{align*}
	&\left 	\|\frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}K_h(x_j,x_k) \zeta_k\right\|_{\psi_2}^2= \mathbb{V}\left \{\frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}K_h(x_j,x_k) \zeta_k\right\}\\
	&=\frac{{1}}{\tilde{T}^2h^2} \sum_{k=1}^{\tilde{T}}K_h(x_j,x_k)^2\mathbb{V} (\zeta_k)+ \frac{{1}}{\tilde{T}^2h^2} \sum_{k=1}^{\tilde{T}}K_h(x_j,x_k)\sum_{l=1}^{\tilde{T}}K_h(x_j,x_l) \mathbb{C}\text{ov} (\zeta_k,\zeta_l)\\
	&\leq C(Thm)^{-1}+C^\prime\{ p^{-2} + \log(p)^2p^{-2\beta}\}.
	\end{align*}
	%Note that  $ \log(p)^2p^{-2\beta}=\mathcal{O}\{h^{2\beta}\}$ since $\beta>1/2$ and for any $\nu\in(2/3,1)$ we can find a constant $C_\nu$ such that  $h\to 0$ (CHECK?).
	%If $h\geq c\{n\log(p)^2\}^{-1}$, then $\log(p)^2p^{-2\beta}=\mathcal{O}\{(Thm)^{-1} \}$ since $\beta>1/2$.
	Lemma 1.6 of \citep{tsybakov2009introduction} then yields 
	\begin{align} \label{var:zeta-term}
	T_1\lesssim \log(2\tilde{T})[C(Thm)^{-1}+C^\prime\{ p^{-2} + \log(p)^2p^{-2\beta}\}] =\mathcal{O}\{(Thm)^{-1}\log(2\tilde{T})+ h^{2\beta}\}.
	\end{align} To see this, note that $p/n\to c\in(0,\infty]$ implies $p^{-1}=\mathcal{O}(n^{-1})$. Thus, if $\beta>1$, then $\log(2\tilde{T})\{ p^{-2} + \log(p)^2p^{-2\beta}\} \lesssim\log(2\tilde{T})p^{-2} \lesssim\log(2\tilde{T})(Tm)^{-1}$.
	Now consider $1/2<\beta\leq 1$. Recall that $T= p^\upsilon$ for some fixed $\upsilon\in((4-2\min\{1,\beta\})/3,1)$. Using the inequality $\log(x)\leq x^a/a$ one can find constants $x_\upsilon,C_\upsilon>0$ depending on  $\upsilon$ but  not on $n,p$ such that
	$\log(2\tilde{T})\log(p)^2p^{-2\beta}T^{2\beta} \leq C_\upsilon p^{-x_\upsilon}$. This implies $\log(2\tilde{T})\log(p)^2p^{-2\beta}=\mathcal{O}(h^{2\beta})$ since $Th\to \infty$ by assumption.
	
	Next, we derive a bound for the second term $T_2=\mathbb{E}\{\max_{1\leq j \leq \tilde{T}}  | \frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}K_h(x_j,x_k) \xi_k|^2 \}$. 
	The exponential decay property of the kernel $K$ stated in Lemma \ref{lemma:decay} yields 
	\begin{align}
	&T_2\lesssim \mathbb{E} \left \{\max_{1\leq j \leq \tilde{T}}  \left | \frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k) |\xi_k|\right |^2 \right \} \nonumber
	\end{align} where $\gamma_h(x,t)=\gamma^{|x-t|/h} + \frac{\gamma^{1/h}}{1-\gamma^{1/h}} \{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h}\}$ and $\gamma\in (0,1)$ is a constant. For some threshold $R>0$ specified later, define $\xi_k^-=|\xi_k|\mathbbm{1}{\{|\xi_k|\leq R\}}$ and $\xi_k^+=|\xi_k|\mathbbm{1}{\{|\xi_k|>R\}}$.
	Then, 
	\begin{align} \label{var:Tequation}
	T_2\lesssim \mathbb{E}\left\{ \max_{1\leq j \leq \tilde{T}} \left |\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)  \xi_k^- \right|^2\right\}+\mathbb{E}\left \{\max_{1\leq j \leq \tilde{T}} \left|\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k) \xi_k^+ \right |^2\right \}.
	\end{align}
	The first term in (\ref{var:Tequation}) can be bounded again with Lemma 1.6 of \citet{tsybakov2009introduction}. % To apply the lemma one has to derive  a bound on $ \|\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k) \xi_k^-\|_{\psi_2}$  where $\|\cdot\|_{\psi_2}$ denotes the sub-Gaussian norm. See \citet[chpt. 2.5]{vershynin2018high} for a definition of $\|\cdot\|_{\psi_2}$.
	We use the fact that for not necessarily independent random variables $X_1,...,X_N$ with $|X_i|<R, \, i=1,...,N,$ it holds $  \| \sum_{i=1}^N a_i X_i  \|^2_{\psi_2} \leq 4R^2\sum_{i=1}^N a_i^2$ where  $a_1,...,a_N\in\mathbb{R}$ and $R>0$ are constants. 
	This is a consequence of Lemma 1 of \citet{azuma1967weighted} which yields 
	$\mathbb{E}[\exp (\lambda\sum_{i=1}^{N} a_i X_i)  ]\leq \exp ( \lambda^2R^2 \sum_{i=1}^N a_i^2 )$ for all $\lambda\in\mathbb{R}$. 
	Thus, for all $t>0$ holds $ \mathbb{P} (|\sum_{i=1}^{N}a_i X_i | >t ) \leq 2\exp (-\lambda t + \lambda^2 R^2 \sum_{i=1}^{N}a_i^2).$
	Setting $\lambda= t/2\cdot (R^2 \sum_{i=1}^{N} a_i^2)^{-1}$, it follows that $\sum_{i=1}^{N}a_i  X_i$ has a subGaussain distribution and the subGaussian norm is bounded by $2R(\sum_{i=1}^N a_i^2)^{1/2}$. See \cite{vershynin2018high} for further details on the subgaussian distribution.
	
	Together, we get $\| \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)  \xi_k^-\|^2_{\psi_2}\leq  \frac{4R^2}{\tilde{T}^2h^2} \sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)^2 \lesssim \frac{R^2 }{\tilde{T}h}$. For the second inequality Lemma \ref{lemma:decay}$(ii)$ is used. 
	Applying Lemma 1.6 of \citet{tsybakov2009introduction} then yields
	\begin{equation} \label{var:xi-term-neg} 
	\mathbb{E}\left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{1}{\tilde{T}h}\sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)  \xi_k^- \right |^2 \right\}\lesssim\frac{ R^2}{\tilde{T}h} \log(2\tilde{T}).
	\end{equation}
	To bound the second term in (\ref{var:Tequation}), we use the moment bounds for $\xi_k$ derived in Lemma \ref{lemma:representY}. Then, for all integers $\ell>1$ 
	\begin{align} \label{var:xi-term-pos}
	&\mathbb{E}\left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{1}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}  \gamma_h(x_j,x_k)  \xi_k^+  \right|^2 \right\} =\mathbb{E} \left \{ \max_{1\leq j \leq \tilde{T}} \left |\frac{1}{\tilde{T}h}\sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)    (\xi_k^+)^{\ell} (\xi_k^+)^{1-\ell} \right |^2 \right\}\nonumber\\
	& \leq R^{2-2\ell} \mathbb{E} \left \{ \max_{1\leq j \leq \tilde{T}}  \frac{1}{\tilde{T}^2h^2}\sum_{k=1}^{\tilde{T}}  \gamma_h(x_j,x_k)^2 \cdot \sum_{k=1}^{\tilde{T}}   (\xi_k^+)^{2\ell} \right\}  \leq R^{2-2\ell}    \mathbb{E} \left [ \frac{C_1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}}   \xi_k^{2\ell} \right] \nonumber\\ 
	& \lesssim h^{-1} R^{2-2\ell}  \log^{4\ell}(m) [m^{-2\ell} + \{T^{-1}+T^{-1}\log(p)p^{1-\beta}\}^{2\ell}].
	\end{align}
	
	%WRONG 27.02.2023
	\begin{comment}
	We collect the $\zeta_k$ in a vector $\bm \zeta=(\zeta_1,...,\zeta_T)$. Then, $\bm\zeta\sim \mathcal{N}_T(\bm 0_T,\bm A)$ for some covariance matrix $\bm A$ with $\bm A_{kk}=m^{-1}$ for $k=1,...,T$.
	Define $\bm \zeta^\prime=\bm\eta+\bm\theta$, where  $\bm\eta\sim\mathcal{N}_T(\bm 0_T, \bm A_{1})$ independent from $\bm\theta\sim\mathcal{N}_T(\bm 0_T,\bm A_{2})$ with  $\bm A_{1}=\left(m^{-1}-m^{-2}\right )\bm{I}_T$ and  $\bm A_{2}=\bm A-\bm A_{1}$. In particular, $\bm \zeta^\prime$ is an independent copy of $\bm \zeta$ and we have  
	\begin{align*}
	&\mathbb{E} \left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k)  \left |\zeta_k \right |\right |^2 \right \}=\mathbb{E} \left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k)  \left |\zeta^\prime_k \right |\right |^2 \right \}\nonumber\\ 
	\leq & 2 \mathbb{E} \left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k)  \left |\eta_k \right |\right |^2 \right \} + 2\mathbb{E} \left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{{1}}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k)  \left |\theta_k \right |\right |^2 \right \}.
	\end{align*}
	We proceed with the two terms on the right hand side as with $T_1$ and $T_2$. Note that $ \|(\tilde{T}h)^{-1}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k) |\eta_k|\|^2_{\psi_2} \leq
	(\tilde{T}h)^{-2}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k)^2 \|\eta_k\|^2_{\psi_2}\lesssim (\tilde{T}hm)^{-1}$
	by independence and Gaussianity of the $\eta_k$ and Lemma \ref{lemma:decay}.
	Lemma 1.6 of \citet{tsybakov2009introduction} then yields that $\mathbb{E} [\max_{1\leq j \leq \tilde{T}}|(\tilde{T}h)^{-1}\sum_{k=1}^{\tilde{T}}\gamma_h(x_j,x_k)  |\eta_k ||^2]$ is asymptotically negligible to $T_1$.
	Note that for any integer $\ell>0$, $\mathbb{E}[|\theta_k|^{\ell}]\lesssim m^{-\ell}$. The same approach as for bounding $T_2$ gives
	\begin{align} \label{var:zeta-term-corr} 
	&\mathbb{E}\left \{\max_{1\leq j \leq \tilde{T}} \left | \frac{1}{\tilde{T}h}\sum_{k=1}^{\tilde{T}}  \gamma_h(x_j,x_k)  \theta_k  \right|^2 \right\} 
	\lesssim  h^{-1}R^{2-2\ell} m^{-2\ell}
	\end{align} and shows that this term is asymptotically negligible to $T_2$.
	% Under condition (\ref{np_asymp}), and since $n=\mathcal{O}(p)$ we get 
	\end{comment}
	Combining the error bounds (\ref{var:xi-term-neg}) and (\ref{var:xi-term-pos}) and choosing $R{=}m^{-1/2}$ gives
	\begin{align*} 
	&T_2\lesssim \frac{\log(T)}{mTh}+\frac{\log^{4\ell}(m)}{h m^{1-\ell}} \cdot
	\begin{cases}
	\frac{\log^{2\ell}(p)p^{2\ell(1-\beta)}}{T^{2\ell}} +m^{-2\ell},& 1/2<\beta\leq 1, \\
	T^{-2\ell}+m^{-2\ell},& 1<\beta.
	\end{cases}
	\end{align*} 
	By assumption $T=p^\upsilon$ and $m=np^{(1-\upsilon)}$ for some fixed $\upsilon\in((4-2\min\{1,\beta\})/3,1)$.  If $\ell$ is an integer such that $\ell \geq 1/(1-\upsilon)$, then
	\begin{align*}
	m^{\ell-1} \log^{4\ell}(m)m^{-2\ell}= m^{-1} \log^{4\ell}(m) m^{-\ell} \lesssim n^{-\ell}p^{-(1-\upsilon)\ell} =\mathcal{O}\{(np)^{-1}\},
	\end{align*} where we used $\log(x)\leq x^a/a$ with $a=1/(4\ell)$.
	Consider $1/2<\beta\leq 1$ and let $0<\chi<1$ be a constant. Applying $\log(x)\leq x^a/a$ twice with $a=\chi/(2\ell)$ yields 
	\begin{align*}
	&m^{\ell-1} \log^{4\ell}(m) \log^{2\ell}(p)p^{2\ell(1-\beta)}T^{-2\ell}\lesssim n^{\ell-1+\chi}p^{(1-\upsilon)(\ell-1+\chi)-2\ell\upsilon+2\ell(1-\beta)+\chi}.
	\end{align*}
	Since $\upsilon\in((4-2\min\{1,\beta\})/3,1)$ it holds
	\begin{align} \label{condk}
	&(1-\upsilon)(\ell-1+\chi)-2\ell\upsilon+2\ell(1-\beta)+\chi  <-(\ell-1+\chi)-2 \nonumber\\
	\iff \, &\ell>\{\upsilon(1-\chi)+3\chi\}/(3\upsilon+2\min\{1,\beta\}-4).\end{align}	
	For any fixed $\upsilon{\in}((4-2\min\{1,\beta\})/3,1)$ one can find an integer $\ell$ which is independent of $n,p$ such that the right side of (\ref {condk}) holds. 
	Since $p/n \to c\in (0,\infty]$ and thus $n/p =\mathcal{O}(1)$ and $p^{-1}=\mathcal{O}(n^{-1})$, it follows for $\ell$ satisfying (\ref {condk}) that
	$$ n^{\ell-1+\chi}p^{(1-\upsilon)(\ell-1+\chi)-2\ell\upsilon+2\ell(1-\beta)+\chi}  \leq (n/p)^{\ell-1+\chi} p^{-2} =\mathcal{O}\{(np)^{-1}\}.$$
	In total, choosing an integer $\ell\geq \max[ 1/(1-\upsilon),\{\upsilon(1-\chi)+3\chi\}/(3\upsilon+2\min\{1,\beta\}-4)]$ gives
	\begin{align} \label{var:finalbound}
	\mathbb{E} \|\widehat{H(f)}- \mathbb{E}\{\widehat{H(f)}\}\|_\infty^2= \mathcal{O}\left\{\frac{\log(np)}{nph} +h^{2\beta}\right \}. 
	\end{align}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%BIAS
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\textbf{Upper bound on the bias}\\
	Using the representation in Lemma \ref{lemma:representY} once more gives for each $x\in[0,1]$
	$$\mathbb{E}\{\widehat{H(f)}(x)\}- H\{f(x)\}= \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} K_h(x,x_k) \left [H\{f(x_k)\} +\epsilon_k\right ] -  H\{f(x)\}.$$
	The bounds on $\epsilon_k$ in Lemma \ref{lemma:representY} imply
	\begin{align*} 
	\left |\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}}K_h(x,x_k)\epsilon_k \right | &\lesssim \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k) |\epsilon_k|\lesssim (np)^{-1}+\log(p)(np)^{-\beta}.
	\end{align*}
	Consider the case that $\beta\geq 1$. In particular, $q=\gamma$ and $f^{(q)}$ is $\alpha$-H\"older continuous. Since $f$ is a periodic function with $f(x)\in[\delta, M_0]$ and $H(y)\propto \phi(m/2) {+} \log \left ( 2y/m\right )$, it follows that $\{H(f)\}^{(q)}$ is also $\alpha$-H\"older continuous.
	Extending $g:=H(f)$ to the entire real line, we get 
	$$ \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} K_h(x,x_k)g(x_k)=  \int_{-\infty}^\infty h^{-1}\mathcal{K}_h(x,t)g(t) \, dt +\mathcal{O}(\tilde{T}^{-\beta}).$$
	Expanding $g(t)$ in a Taylor series around $x$ and using that $h^{-1}\mathcal{K}_h$ is a kernel of order $2q$, see Lemma \ref{lemma:decay}$(iii)$, it follows that for any $x\in[0,1]$ 
	\begin{align*}
	&\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} K_h(x,x_k)g(x_k)=g(x) + \int_{-\infty}^\infty h^{-1}\mathcal{K}_h(x,t) {(x-t)^{q} \frac{g^{(q)}(\xi_{x,t})}{q!}}\, dt\ +\mathcal{O}(\tilde{T}^{-\beta})\\
	&= g(x) +  \int_{-\infty}^\infty h^{-1}\mathcal{K}_h(x,t) {(x-t)^{q} \frac{g^{(q)}(\xi_{x,t}) - g^{(q)}(x)}{q!}}\, dt  + \mathcal{O}(\tilde{T}^{-\beta})\\
	&= g(x) + \sum_{l=-\infty}^\infty \int_{x+(l-1)h}^{x+lh} \mathcal{K}_h(x, t)(x-t) ^{q} \frac{g^{(q)}(\xi_{x,t})-g^{(q)}(x)}{hq!}\, dt+ \mathcal{O}(\tilde{T}^{-\beta}),
	\end{align*}
	where $\xi_{x,t}$ is a point between $x$ and $t$. Using the fact that the kernel $\mathcal{K}_h$ decays exponentially and that $g^{(q)}$ is $\alpha$-H\"older continuous on $[\delta,M_0]$ with some constant $L$, one obtains
	
	\begin{align*}
	\left | \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} K_h(x,x_k)g(x_k)-g(x) \right|&\leq  \frac{CL}{q!} \sum_{l=-\infty}^\infty \int_{x+(l-1)h}^{x+lh}\gamma^\frac{|x-t|}{h} |x-t|^{q} \frac{|\xi_{x,t}-x|^{\alpha}}{hq!}\, dt+ \mathcal{O}(\tilde{T}^{-\beta})\\
	&\leq  \frac{CL}{q!} \sum_{l=-\infty}^\infty \int_{x+(l-1)h}^{x+lh} \gamma^\frac{|x-t|}{h} \frac{|x-t|^{\beta} }{h}\, dt+ \mathcal{O}(\tilde{T}^{-\beta})\\
	&\leq h^{\beta} \frac{CL}{q!} \sum_{l=-\infty}^\infty \gamma^{|l-1|} |l|^{\beta} + \mathcal{O}(\tilde{T}^{-\beta})\\
	&= h^{\beta} \frac{2CL}{q!} \sum_{l=1}^\infty \gamma^{l-1} l^{\beta} + \mathcal{O}(\tilde{T}^{-\beta})= \mathcal{O}(h^{\beta}) + \mathcal{O}(\tilde{T}^{-\beta}).
	\end{align*} %The series converges as $\lim_{l\to \infty} \sup \sqrt[\leftroot{-2}\uproot{2}l]{ \gamma^{l-1}l^{2q+1}} \leq \lim_{l\to \infty} \sup \sqrt[\leftroot{-2}\uproot{2}l]{ \gamma^{l-1}}  (\sqrt[\leftroot{-2}\uproot{2}l]{l})^{2q+1}  < 1$.
	If $1/2<\beta\leq 1$, then $q=1$ and $g$ is $\beta$-H\"older continuous. Since $f(x)\in[\delta, M_0]$ and the logarithm is Lipschitz continuous on a compact interval, it follows $g=H(f)$ is $\beta$-H\"older continuous. Expanding $g$ to the entire line and using Lemma \ref{lemma:decay}$(iii)$ with $m=0$ gives
	\begin{align*}
	\frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} K_h(x,x_k)g(x_k)-g(x)=  \int_{-\infty}^\infty \mathcal{K}_h(x,t)\frac{g(t)-g(x)}{h} \, dt +\mathcal{O}(\tilde{T}^{-\beta}).
	\end{align*}
	In a similar way as before, one obtains
	$$ \left | \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} K_h(x,x_k)g(x_k)-g(x) \right| = \mathcal{O}(h^{\beta}) + \mathcal{O}(\tilde{T}^{-\beta}).$$
	Note that $\tilde{T}^{-\beta}{=}o(h^{\beta})$ as $\beta>1/2$,  $Th\to \infty$ and $h\to 0$ by assumption. %, we have    %$\{\log(p)/(np^\beta)\}^2=\mathcal{O}\{\log(np)/(np)\}$ and
	Since the derived bounds are uniform for $x\in[0,1]$ it holds
	\begin{align} \label{bias:finalbound} 
	\left \|\mathbb{E}\{\widehat{H(f)}(x)\}-H\{f(x)\}\right\|_{\infty}=\mathcal{O}(h^{\beta}) +\mathcal{O}\{(np)^{-1}+\log(p)(np)^{-\beta}\}.
	\end{align}
	Putting the bounds \ref{var:finalbound} and \ref{bias:finalbound} together gives
	\begin{flalign*}
	&& \mathbb{E} [\|\widehat{H(f)} - H(f)\|_\infty^2 ]=\mathcal{O} \left\{ \log(np)/(nph)\right\} + \mathcal{O}(h^{2\beta}). && \qed 
	\end{flalign*}	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsubsection{An upper bound on $\mathbb{E}  \|\hat{f} - f\|_\infty^2 $}	\label{A:boundf}
	\begin{prop}\label{prop:riskf}
		Let $\bm{\Sigma} \in \mathcal{F}_{\beta}$ such that $\beta>1/2$.  If $h>0$ such that  $h\to 0$ and $hT\to \infty$,  then with $T=\lfloor p^{\upsilon}\rfloor$ for any $\upsilon\in((4-2\min\{1,\beta\})/3,1)$, the estimator $\hat{f}$ described in Section \ref{sec:method} with $q=\max\{1,\gamma\}$ satisfies
		$$\ \mathbb{E} \|\widehat{f} - f\|_\infty^2 =\mathcal{O} \left\{ \log(np)/(nph)\right\} + \mathcal{O}(h^{2\beta}).$$
	\end{prop}
	{\parindent0pt\textbf{Proof}}:
	By the mean value theorem, it holds for some function  $g$ between $\widehat{H(f)}$ and $H(f)$ that
	\begin{align*}
	\mathbb{E}  \|\hat{f} - f\|_\infty^2 &= \mathbb{E}\|H^{-1} \{\widehat{H(f)}\} - H^{-1} \{H(f)\}   \|_\infty^2  = \mathbb{E}  \|(H^{-1})^\prime (g) ( \widehat{H(f)}-H(f) ) \|_\infty^2 \\
	&\lesssim \mathbb{E}\|  H^{-1}(g) ( \widehat{H(f)}-H(f) )\|_\infty^2.
	\end{align*} Application of the Cauchy-Schwarz inequality with a constant $C>0$ yields
	\begin{equation} \label{CSsplit}
	\mathbb{E}\|\hat{f}-f \|_\infty^2\leq  C^2 \mathbb{E} \|\widehat{H(f)}-H(f)\|_\infty^2
	+ (\mathbb{E}\|\hat{f}-f\|_\infty^4)^{1/2}\cdot \mathbb{P} (\|H^{-1}(g)\|_\infty > C) ^{1/2}.
	\end{equation}
	To show that the second term on the right hand side of (\ref{CSsplit}) is negligible we use the moment generating function of $\|\widehat{H(f)} \|_\infty$. In the next paragraph, we derive the asymptotic order of $\mathbb{E} [ \exp\{ \lambda \|\widehat{H(f)} \|_\infty \}]$ for $n,p\to \infty$, where $\lambda >0$  may depend on $n,p$ or not.\\	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%MMENT GENERATING FUNCTION OF Hf
	\textbf{Moment generating function of $\|\widehat{H(f)} \|_\infty$} \\	
	By the exponential decay property of the kernel $K$ stated in Lemma \ref{lemma:decay} holds
	
	
	%
	\begin{align}
	\|\widehat{H(f)} \|_\infty &\lesssim \sup_{x\in[0,1]}  \frac{1}{\tilde{T}h} \sum_{k=1}^{\tilde{T}} \gamma_h(x,x_k)\cdot \left | Y^*_k\right |, \nonumber
	\end{align} %where $\gamma_h(x,t)=\gamma^{|x-t|/h} + \frac{\gamma^{1/h}}{1-\gamma^{1/h}} \{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h}\}$ for some constant $\gamma\in (0,1)$. 
	First,  $\|\widehat{H(f)} \|_\infty$ is bounded with the maximum over a finite number of points.
	Calculating the derivative of $s:[0,1]\to \mathbb{R},\, x\mapsto s(x)=\sum_{k=1}^{\tilde{T}} \gamma_h(x,x_k) \left | Y^*_k\right |$ for $x\neq x_k,$ $k =1,...,\tilde{T},$ gives almost surely
	\begin{align*}
	&\frac{\delta}{\delta x}  s(x) =  \frac {\log(\gamma)}{h}\sum_{k=1}^{\tilde{T}}  \left [\gamma^{|x-t|/h}|x-x_k| + \frac{\gamma^{1/h}}{1-\gamma^{1/h}}\left \{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h} \right\} \right ] \left |Y^*_k\right|.
	\end{align*}
	Since $\frac{\delta}{\delta x}  s(x) >0$ almost surely for $x\neq x_k$, the extrema occur at $x_k,\, k=1,...,\tilde{T}.$
	Thus, for $\lambda >0$ the moment generating function of $\| \widehat{H(f)} \|_\infty $ is bounded by
	\begin{align*} 
	&\mathbb{E} [\exp\{\lambda \| \widehat{H(f)} \|_\infty  \} ]  \leq\sum_{j=1}^{\tilde{T}}\mathbb{E}\left [ \exp \left\{\frac{\lambda}{\tilde{T}h}\sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)\left | Y_k^* \right | \right\}\right ].
	\end{align*}
	Let $M_j=(\tilde{T}h)^{-1}\sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)$, which by Lemma \ref{lemma:decay} is bounded uniformly in $j$ by some global constant $M>0$. %INDEPENDENT OF PF
	By the convexity of the exponential function we obtain
	\begin{align*} 
	&\mathbb{E}\left [  \exp \left \{\frac{\lambda}{\tilde{T}h}\sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k)| Y_k^* | \right \}\right ] \leq \frac{1}{M_j\tilde{T}h} \sum_{k=1}^{\tilde{T}} \gamma_h(x_j,x_k) \mathbb{E}\left [\exp \left\{ \lambda M_j|Y_k^*| \right\} \right ].
	\end{align*}	
	Recall that $Y_k^*=\log(Q_k/m)/\sqrt{2}$ and  by assumption $0\leq \delta \leq f \leq M_0$. Using Lemma \ref{lemma:rewriteBins}, $Q_k$ can be written as a sum of $m=np/T$ independent gamma random variables, i.e. $Q_k= \sum_{i=1}^{m} A_i$ where $A_i\sim \Gamma(1/2, 2a_k)$ with $a_k\in(c_1\delta,c_2M_0)$ for some constants $c_1,\,c_2$. Thus, we can find random variables $Q_k^-\sim \Gamma(m/2,c\delta)$ and $Q_k^+\sim \Gamma(m/2,cM_0)$ such that $Q_k^-\leq Q_k\leq Q_k^+$ almost surely and $c>0$ is a constant. 
	Then, 
	\begin{align*}
	\mathbb{E} [\exp \{ \lambda M_j |Y_k^*|\} ]&\leq  \mathbb{E}\left [  \exp \left \{ \lambda M_j/\sqrt{2} \max \left\{ | \log ( Q_k^+/m )|,  | \log (Q^-_k/m)| \right\}   \right\} \right ].
	\end{align*} %where $Q_j^-/m\sim \Gamma(m/2, cQ_j/m)$ and  $Q_j^+/m\sim \Gamma(m/2, CM_0/m)$.\\
	The moment generating function of $|\log(X)|$ when $X$ follows a $\Gamma(a,b)$-distribution is given by  
	\begin{align*}
	\mathbb{E}\left [  \exp \{ t \left |\log \left (X \right) \right | \} \right ] &=b^{-t}\frac{  \Gamma(a-t) - \gamma (a - t, 1/b) + b^{2t} \gamma(a+t, 1/b)}{\Gamma(a)}, \quad |t|<a,
	%&\leq\frac{  b^{-t} \gamma(a-t)+ b^{t} \gamma(a+t)}{\gamma(a)},&
	\end{align*}
	where $\Gamma(a)$ is the gamma function and $\gamma(a,b)$ is the lower incomplete gamma function. %Note that $\gamma[a,b]\leq \gamma(a)$ and $\gamma(x+\alpha) \sim \gamma(x)\cdot x^{\alpha}$ for $x \to \infty$ and $\alpha \in \mathbb{C}$. %, it follows $ \mathbb{E}\left [  \exp \{ t \left |\log \left (X \right) \right | \} \right ] =\mathcal{O}\left((ab)^{-t} + (ab)^t\right)$ for $a\to\infty$.\\
	In particular,
	\begin{equation} \label{MGFbound}
	\mathbb{E}[  \exp \{ t  |\log (X )| \}]\leq\frac{  b^{-t} \Gamma(a-t)+ b^{t} \Gamma(a+t)}{\Gamma(a)}.
	\end{equation}
	In our setting, $X=Q_k^-/m\sim \Gamma(m/2, c\delta/m)$ resp.  $X=Q_k^+/m\sim \Gamma(m/2, cM_0/m)$. To derive the asymptotic order of  $\mathbb{E} [ \exp\{ \lambda \|\widehat{H(f)} \|_\infty \}]$ for $n,p\to \infty$ we first establish the asymptotic order of the ratio $\Gamma(a + t)/\Gamma(a)$ for $a\to \infty$. 
	We distinguish the two cases where $t$ is independent of $a$ and where $t$ linearly depends on $a$. Based on Stirling`s approximation one derives for $a>1,\, t>0$ 
	%For the ratio $\Gamma(a + t)/\Gamma(a)$ of two gamma functions 
	\begin{align}
	\frac{\Gamma(a+ t)}{\Gamma(a)}&=\frac{ \sqrt{2\pi/ (a+ t)}(a+ t)^{a+ t}\exp{(-a-t)} \left [ 1+ \mathcal{O}\{1/(a+ t)\} \right] }{ \sqrt{2\pi/ a} a^{a}\exp{(-a)} \left \{ 1+ \mathcal{O}(1/a) \right\} } \nonumber \\
	&=\sqrt{a/ (a+ t)} \left \{(a+ t)/a\right\}^{a} (a+ t)^{t} \exp{(- t)} \left [ \frac{ 1+ \mathcal{O}\{1/(a+ t)\}}{  1+ \mathcal{O}(1/a)}  \right]. \label{gammaratio}
	\end{align}
	Thus, for $0<t<a$ and $t$ independent of $a$, equation (\ref{gammaratio}) implies for $a\to \infty$ that  $\Gamma(a+ t)/\Gamma(a) =\mathcal{O}(a^{t})$. Similarly, it can be seen that  $\Gamma(a-t)/\Gamma(a) =\mathcal{O}(a^{-t})$. 
	If $0<t<a$ and $t$ linearly depends on $a$, i.e. $t=ca$ for some constant $c\in (0,1)$, then we get $\Gamma(a\pm t)/\Gamma(a) =\mathcal{O}(a^{\pm t}  \exp\{a\} )$ for $a\to \infty$.	
	Hence, for a fixed $\lambda$ not depending on $n,p$  and such that $0<\lambda<m/(\sqrt{2}M_j)$ we get for sufficiently large $n,p$
	\begin{align*}
	\mathbb{E}\left [\exp \left\{\lambda M_j|Y_k^*|\right \}\right] &=\mathbb{E}\left [\exp \left\{\lambda M_j/\sqrt{2} \left| \log \left (Q_k/m\right ) \right| \right\} \right ] \\
	&\leq c_3\sum_{b\in\{c\delta/m,cM_0/m\}}(bm/2)^{-\frac{\lambda M_j}{\sqrt{2}}}  +(bm/2)^{\frac{\lambda M_j}{\sqrt{2}}} =\mathcal{O}(1).
	\end{align*} 
	If $\lambda=cm$ such that $0<\lambda<m/(\sqrt{2}M_j)$, then for sufficiently large  $n,p$
	\begin{align*}
	\mathbb{E}\left [\exp \left\{\lambda M_j|Y_k^*|\right \}\right]\leq c_4\exp\{m/2\}{\sum_{b\in\{c\delta/m,cM_0/m\}}}(bm/2)^{-\frac{\lambda M_j}{\sqrt{2}}} +(bm/2)^{\frac{\lambda M_j}{\sqrt{2}}} =\mathcal{O}(L^m),
	\end{align*} for some constant  $L>1$.
	Set $K=\min_{j=1,...,\tilde{T}}1/(\sqrt{2} M_j)$ which is a constant independent of $n,p$. Altogether, we showed that for $0<\lambda <Km$ and $n,p\to \infty$
	\begin{align}
	\mathbb{E} [ \exp\{ \lambda \|\widehat{H(f)} \|_\infty \}]=\mathcal{O}(T), & & & \text{if } \lambda \text{ does not depend on $n,p$,} \label{MGF-o1}\\
	\mathbb{E} [ \exp\{ \lambda \|\widehat{H(f)} \|_\infty \}]=\mathcal{O}(TL^{m}), & & &\text{if }  \lambda=cm \text{ for } c\in(0,K)  \label{MGF-o2}.
	\end{align}
	
	\textbf{Bounding the right hand side of (\ref{CSsplit})}\\
	Noting that $\|f\|_\infty\leq M_0$ and $m/2 \exp \left \{\phi\left (-m/2\right) \right\}\in[1,4]$ for $m\geq1$, (\ref{MGF-o1}) implies for some constants $c_0,c_1>0$ and $n,p \to \infty$
	\begin{align*}
	\mathbb{E} \|\widehat{f} - f\|_\infty^4 &\leq c_0\mathbb{E} \|\widehat{f}\|_\infty^4  +c_1 =  c_0\mathbb{E} \|  (m/2)\exp \{\sqrt{2} \widehat{H(f)}-\phi(m/2)  \}  \|_\infty^4 +c_1 \\
	&\leq c_0\left |(m/2)^4 \exp \{ -4\phi(m/2)\}\right | \cdot \mathbb{E}[ \exp \{4 \sqrt{2}  \|\widehat{H(f)} \|_\infty \}]+c_1  =\mathcal{O}(T).
	\end{align*} 
	Since $g$ lies between $\widehat{H(f)}$ and $H(f)$, and $H^{-1}\propto \exp$,  it follows  $H^{-1}(g)\leq H^{-1}\{\widehat{H(f)}\}+f$ almost surely pointwise. Thus, for $C>\|f\|_\infty=M_0$ it holds
	\begin{align*} 
	\mathbb{P}  ( \|H^{-1}(g) \|_\infty > {C}  ) \leq P ( \|H^{-1}\{\widehat{H(f)}\}\|_\infty >C-M_0)\leq P(\|\widehat{H(f)}\|_\infty>c_1)
	\end{align*} where $c_1:=H(C-M_0)$.
	Applying  Markov inequality for $t=cm$ with $c\in(0,K)$ and $C=2L^{4/c}+M_0$ where $c,K,L$ are the constants in (\ref{MGF-o2}) gives 
	\begin{align*} %\label{probbound}
	\mathbb{P}  ( \|H^{-1}(g) \|_\infty > C  ) &\leq  \exp(-c_1t)  \mathbb{E}  [ \exp\{ t \|\widehat{H(f)}\|_\infty \}  ]=\mathcal{O}(TL^{-m}).
	\end{align*}
	Together with Proposition \ref{prop:riskHf} follows
	\begin{equation*}
	\mathbb{E}  \|\hat{f} - f\|_\infty^2 =\mathcal{O} \left\{ \log(np)/(nph)\right\} + \mathcal{O}(h^{2\beta}).
	\end{equation*}
	\subsubsection{An upper bound on $\mathbb{E}\|\hat{\bm\Sigma}-\bm\Sigma\|^2$}
	Using the fact that the spectral norm of a Toeplitz matrix is upper bounded by the sup norm of its spectral density we get
	\begin{equation} \label{theo:sigmaf} 
	\sup _{\mathcal{F}_{\beta}}\,	\mathbb{E} \|\hat{\bm{\Sigma}}- \bm{\Sigma}(f)\|^2 \leq \sup _{\mathcal{F}_{\beta}}\, \mathbb{E} \|\hat{f} - f\|_\infty^2= 	\mathcal{O} \left\{ \log(np)/(nph)\right\} +\mathcal{O}(h^{2\beta}).
	\end{equation}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsubsection{An upper bound on $\mathbb{E}\|\hat{\bm\Omega}-\bm\Sigma^{-1}\|^2$}
	According to the mean value theorem, for a function  $g$ between $\widehat{H(f)}$ and $H(f)$, it holds that
	\begin{align*}
	\mathbb{E} \|\hat{\bm{\Omega}} - \bm{\Omega}\|^2 &\leq \mathbb{E}  \|1/\hat{f}- 1/f\|_\infty^2= \mathbb{E} \|1/H^{-1} \{\widehat{H(f)}\} - 1/H^{-1} \{H(f)\}\|_\infty^2\\
	&= \mathbb{E} \|(1/H^{-1})^\prime (g) \{ \widehat{H(f)}-H(f) \}\|_\infty^2 \lesssim \mathbb{E}  \| 1/H^{-1}(g) \{ \widehat{H(f)}-H(f)\} \|_\infty^2
	\end{align*} 
	%since $\left(\frac{1}{H^{-1}}\right )^\prime(g)=-\frac{(H^{-1})^\prime(g)}{\{H^{-1}(g)\}^2}=-\frac{\sqrt{2} H^{-1}(g)}{(H^{-1}(g))^2}=-\frac{\sqrt{2}}{H^{-1}(g)}$.
	Application of the Cauchy-Schwarz inequality with a constant $C>0$ yields
	\begin{equation*} %\label{CSsplit3}
	\mathbb{E} \|1/\hat{f}- 1/f\|_\infty^2\leq  C^2 \mathbb{E} \|\widehat{H(f)}-H(f)\|_\infty^2 
	+ (\mathbb{E} \|1/\hat{f}- 1/f\|_\infty^4)^{1/2} \mathbb{P}  ( \|1/\{H^{-1}(g)\}\|_\infty > C ) ^{1/2}.
	\end{equation*} 
	Note that $\left\|1/H^{-1}(g)\right \|_\infty  \leq  2/m \exp \{ \sqrt{2} \|g \|_\infty\}\exp\{\phi (m/2)\}\leq c_1\left\|H^{-1}(g)\right \|_\infty$ for some constant $c_1>0$ not depending on $n,p$.
	Chosing the same constant $C$ as in section \ref{A:boundf} it follows
	\begin{align*}
	\mathbb{P} \left ( \left\|1/\{H^{-1}(g)\}\right \|_\infty > {C} \right )  = \mathcal{O}(T L^{-m}).
	\end{align*}
	Noting that $\|1/f\|_\infty\leq 1/\delta$ and $2/m \exp \left \{ \phi(m/2)\right \} \in[0.25,1]$ for $m\geq1$, (\ref{MGF-o1}) implies for some constants $c_2,c_3>0$ and $n,p \to \infty$
	\begin{align*}
	\mathbb{E} \|1/\hat{f}- 1/f\|_\infty^4%&\leq C\mathbb{E}[\|\hat{f}^{-1}\|_\infty^4] +(C/\delta)^4 = \mathbb{E} [\|  (2/m)^4\exp  \{4 (-\sqrt{2} \widehat{H(f)}+\phi(m/2)  ) \} \|_\infty ]  +(C/\delta)^4\\
	&\leq c_2|(2/m)^4 \exp \{ 4\phi(m/2)\} | \cdot \mathbb{E}   \| \exp  \{-4 \sqrt{2} \widehat{H(f)}  \}  \|_\infty  +c_3 \\
	&\leq  c_2|(2/m)^4 \exp \{ 4\phi(m/2)\} | \cdot \mathbb{E}   \exp  \{4 \sqrt{2} \| \widehat{H(f)}\|_\infty  \}    +c_3 =\mathcal{O}(T). 
	\end{align*} 
	Since the derived bounds hold for each $\bm\Sigma(f)\in\mathcal{F}_\beta$, we get all together 
	\begin{equation} \label{theo:omega}
	\sup _{ \mathcal{F}_{\beta}}\,	\mathbb{E}  [\|\hat{\bm{\Omega}} - \bm{\Sigma}^{-1}(f)\|^2  ]=\mathcal{O} \left\{ \log(np)/(nph))\right\} + \mathcal{O}(h^{2\beta}).
	\end{equation}
	
	\subsubsection{Optimal bandwith parameter $h$}
	Minimizing the right side of (\ref{theo:sigmaf}) and (\ref{theo:omega}) for the bandwidth parameter $h$ yields $h\asymp\left\{\log(np)/(np)\right \}^{\frac{1}{2\beta+1}}$. In particular, $Th\geq  p^{\frac{2}{2\beta+1}}  \{\log(np)/(np) \}^{\frac{1}{2\beta+1}}\to \infty $ and $h\to 0$ since $\upsilon>{2}/({2\beta+1})$ for $\beta>1/2$ and $\upsilon \in ((4-2\min\{1,\beta\})/3,1)$. 
	Thus, substituting $h\asymp\left\{\log(np)/(np)\right \}^{\frac{1}{2\beta+1}}$ into (\ref{theo:sigmaf}) and (\ref{theo:omega})  gives the second result.
	\hfill \qed
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\subsection{Auxiliary Lemmata for Theorem 1} \label{app:theorem1_aux}
	This section states some technical lemmata needed for the proof of Theorem 1. The proofs can be found in the supplementary material. The first lemma lists some properties of the kernel $K_h$ and its extension $\mathcal{K}_h$ on the real line. The proof is based on \citep{schwarz2016unified}. 	
	\begin{lemma} \label{lemma:decay} Let $h>0$ be the bandwith parameter depending on $N$.
		\begin{enumerate}[label=(\roman*)]
			\item There are constants $0<C<\infty$ and $0<\gamma <1$ such that for all for $x,t \in [0,1]$
			\begin{align*}
			&|\mathcal{K}_h(x,t)|< C\gamma^{|x-t|/h},\\ 
			&|K_h(x,t)|< C  [\gamma^{|x-t|/h} + \gamma^{1/h}\{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h} \} /(1-\gamma^{1/h})].
			\end{align*}
			\item  Let $t_i=(i-1)/(N-1),\,i=1,...,N$. If $Nh\to\infty$ for $N\to\infty$, then \\
			$\frac{1}{Nh}\sum_{i=1}^N \gamma_h(x,t_i)=\mathcal{O}(1)$ and $\frac{1}{Nh}\sum_{i=1}^N \gamma^2_h(x,t_i) = \mathcal{O}(1)$  uniformly for $x{\in}[0,1]$, where $\gamma_h(x):= \gamma^{|x-t|/h} + \gamma^{1/h}\{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h} \} /(1-\gamma^{1/h})$.
			\item It holds \begin{align*}
			&h^{-1}\int_{-\infty}^\infty (x-t)^m \mathcal{K}_h(x,t)\, dt = \delta_{m,0},& &\text{ for } m=0,...,2q-1,\\
			&h^{-1}\int_{-\infty}^\infty (x-t)^m \mathcal{K}_h(x,t)\, dt\neq0, &&\text{ for } m=2q.			
			\end{align*}
		\end{enumerate}
	\end{lemma}
	
	Lemma  \ref{lemma:rewriteBins} states that the sum of the correlated gamma random variables in each bin can be rewritten as a sum of independent gamma random variables. 
	\begin{lemma} \label{lemma:rewriteBins}
		If  $\bm{\Sigma}\in\mathcal{F}_{\beta}$, then $Q_k=\sum_{j=(k-1)p/T+1}^{kp/T} \sum_{i=1}^n\tilde{W}_{i,j}, \, k=1,...,T,$
		where $$  \tilde{W}_{i,j}\overset{indep.}{\sim}
		\Gamma \left(1/2,  2f(x_j)+\mathcal{O}\{T^{-1}+T^{-1}\log(p)p^{1-\beta}\}\right)
		$$
		for $i=1,...,n$ and $j=(k-1)m+1,...,km$, and $x_j=(j-1)/(2p-2)$.
	\end{lemma}  
	
	Finally, Lemma \ref{lemma:representY} gives explicit bounds for the stochastic and deterministic errors of the variance stabilizing transform. Thus, it quantifies the difference to an exact Gaussian regression setting. This result is a generalization of Theorem 1 of \citet{cai2010nonparametricfest} adapted to our setting with $n\geq1$ observations and correlated observations.
	\begin{lemma} \label{lemma:representY}
		If $\bm{\Sigma}\in\mathcal{F}_{\beta}$, then $Y^*_{k}=\log(Q_k/m)/\sqrt{2}$  can be written as
		\begin{align*}
		& Y_{k}^*= H\left \{f \left (x_k \right)  \right \} +\epsilon_k+m^{-1/2}Z_k+ \xi_k, &x_k=(k-1)/(2T-2), \, k=1,...,T.
		\end{align*}
		where $|\epsilon_k|\lesssim (np)^{-1}+\log(p)(np)^{-\beta}, \, Z_k\sim\mathcal{N}(0,1),$ and $\mathbb{E}[\xi_k]=0$. Furthermore,
		\begin{equation*}   \begin{aligned}
		&\mathbb{E}|\xi_k|^\ell \lesssim \log^{2\ell}(m) [m^{-\ell}+\left\{T^{-1}+T^{-1}\log(p) p^{1-\beta}\right\}^\ell], & \ell\in\mathbb{N}_{>1},\\
		& Cov(Z_k,Z_l)= \mathcal{O} \{ mp^{-2} + m\log(p)^2p^{-2\beta}\}, &k\neq l =1,...,T.	
		\end{aligned}
		\end{equation*}
	\end{lemma}


\section{Supplementary Material}

\subsection{Proof of Lemma \ref{lemma:decay}}
$(i)$ See Lemma 16  of \citet{schwarzdiss} for the proof of the first statement. Furthermore, for $x,t\in [0,1]$ holds
\begin{align*}
\left |{K}_h(x,t)\right |&\leq\sum_{l=-\infty} ^\infty \left |\mathcal{K}_h (x, t +l) \right |\\
&\leq C \sum_{l=-\infty} ^\infty \gamma^{|x-t-l|/h}=C\left \{ \gamma^{|x-t|/h} + \sum_{l>0} ^\infty \gamma^{(x-t+l)/h} +\sum_{l>0} ^\infty \gamma^{(-x+t+l)/h} \right \}\\
& = C [\gamma^{|x-t|/h} + \gamma^{1/h}\{\gamma^{(x-t)/h}+ \gamma^{(t-x)/h} \} /(1-\gamma^{1/h})].
\end{align*} 
$(ii)$   Since $\gamma_h(x,\cdot)$ is differentiable everywhere except at $x$, it holds 
\begin{align*}
&\frac{1}{Nh}\sum_{i=1}^N \gamma_h(x,t_i)=   h^{-1}\int_0^1 \gamma_h(x,t) \, dt+\mathcal{O}\{(Nh)^{-1}\}\\
&=  h^{-1}\int_0^1 \gamma^{|x-t|/h} \, dt + h^{-1}\frac{\gamma^{1/h}}{1-\gamma^{1/h}} \int_0^1\gamma^{(x-t)/h}+ \gamma^{(t-x)/h}  dt + \mathcal{O}\{(Nh)^{-1}\}.
\end{align*}
Note that 
\begin{align*}
&h^{-1}\int_0^1 \gamma^{|x-t|/h} \, dt =\{\gamma^{(1-x)/h} +\gamma^{x/h} -2\}/\log(\gamma) \\
%=\log(\gamma)^{-1} \left [-h + h \gamma^{t/h}+h \gamma^{(1-t)/h}-h\right ] =\mathcal{O}(h).
\text{and } &h^{-1}\int_0^1\gamma^{(x-t)/h}+ \gamma^{(t-x)/h}  \, dx =  \gamma^{-t/h} \{\gamma^{1/h} -1\}/\log( \gamma) -\gamma^{t/h} \{\gamma^{-1/h} -1\}/\log( \gamma).
\end{align*} 
In particular, for some constants $C_1,C_2>0$ depending on $\gamma\in(0,1)$ but not on $h$ and $x$, it holds $h^{-1}\int_0^1 \gamma^{|x-t|/h} \, dt\leq C_1$ and  $h^{-1}\frac{\gamma^{1/h}}{1-\gamma^{1/h}}\int_0^1\gamma^{(x-t)/h}+ \gamma^{(t-x)/h}  \, dx \leq C_2$. 
The proof to show $h^{-1}\int_0^1\gamma_h(x,t)^2 \, dt\leq C_3$ for some constant $C_3>0$ is similar. By assumption $(Nh)^{-1}=o(1)$ for $N\to \infty$ which concludes the proof. \\
$(iii)$ See Lemma 15 of \citet{schwarzdiss} with $p=2q-1$. \hfill \qed


\subsection{Proof of Lemma \ref{lemma:rewriteBins}}
It is sufficient to show the statement for $n=1$ by independence of the $\bm{Y}_i$.  Then, the number of points per bin is $m=p/T$. For simplicity, the index $i$ is skipped in the following.  First, we write $Q_k$ as a matrix-vector product and refactor it so that it corresponds to a sum of independent scaled $\chi^2$ random variables. In the second step, we calculate the scaling factors.  Let  $\bm{E}^{(km)}$ be a diagonal matrix with ones on the  $(k-1)m+1,...,km$-th entries and otherwise zero diagonal elements. Then, 
$$ Q_k=\sum_{j=(k-1)m+1}^{km} W_{j}= \bm Y^t \bm{DE}^{(km)}\bm{D} \bm Y.$$ 
Define $\bm X=\bm{\Sigma}^{-1/2}\bm Y\sim \mathcal{N}_p(0_p, \bm{I})$ and let $\bm{U}^t\bm{\Gamma U}$ be the eigendecomposition of $\bm{\Sigma}^{1/2}  \bm{DE}^{(jm)} \bm{D\Sigma}^{1/2}$ with eigenvalues $(\lambda_j)_{j=1}^p$. Note that $\bm{\Sigma}^{1/2}$ is  invertible since $\bm{\Sigma}$ invertible. Then,
$$ Q_k= \bm X^t \bm{\Sigma}^{1/2}  \bm{DE}^{(km)} \bm{D \Sigma}^{1/2}\bm X=\bm X^t\bm{U}^t\bm{\Gamma U} \bm X=\tilde{\bm X}^t\bm{\Gamma} \tilde{\bm X} = \sum_{j=1}^p \lambda_j \tilde{\bm X}^2_j,$$
where $\tilde{\bm X}:=\bm{UX}$. Since $\bm U$ is an orthogonal matrix, it follows that $\tilde{\bm X}\sim\mathcal{N}_p(0_p,\bm{I})$. In particular,  $Q_k$ is a sum of independent scaled $\chi^2$ random  variables $\tilde{W}_j:=\lambda_j\tilde{X}^2_j\overset{indep.}{\sim}\Gamma( 1/2, 2\lambda_j)$ where the scaling factors are the eigenvalues $\lambda_j$. It remains to calculate the $\lambda_j$.
Note that the matrix $\bm{\Sigma}^{1/2} \bm{D} \bm{E}^{(km)} \bm{D \Sigma}^{1/2}$ is similar to the matrix 
$$\bm{A}:=\bm{D \Sigma}^{-1/2} \left (\bm{\Sigma}^{1/2} \bm{DE}^{(km)}\bm{D\Sigma}^{1/2} \right) \bm{\Sigma}^{1/2}\bm{D}=\bm{E}^{(km)}\bm{D \Sigma D}.$$
In particular, $\bm{\Sigma}^{1/2} \bm{D} \bm{E}^{(km)}\bm{D \Sigma}^{1/2}$ has the same eigenvalues as $\bm{A}$.
The rows of $\bm{A}$ are zero except for the $(k-1)m+1,...,km$ -th rows, which equal the $(k-1)m+1,...,km$ -th rows of $\bm{D \Sigma D}$.
By Lemma 1,
$$\left ( \bm{D\Sigma D}\right )_{i,j} = f(x_i)\delta_{i,j} + \frac{1+(-1)^{|i-j|}}{2} \mathcal{O}\{p^{-1}+\log(p) p^{-\beta}\}.$$ 
%Rearranging $\bm{E}^{(jm)}\bm{D\Sigma D}$ yields the following block structure 
%\begin{equation*}
%\begin{pmatrix}
%&\bm{B}:=(\bm{E}^{(jm)}\bm{D\Sigma D}\bm{E}^{(jm)})_{r,s=(j-1)m+1}^{jm} &\bm{\mathcal{O}(\log(p)p^{-\beta^*})}_{m\times (p-m)} \\
%&\bm{0}_{(p-m)\times m} &\bm{0}_{(p-m)\times (p-m)}
%\end{pmatrix}.
%\end{equation*}
Define $ \bm{B}:=(\bm{E}^{(km)}\bm{D\Sigma D}\bm{E}^{(km)})_{r,s=(k-1)m+1}^{km}$. Then, $\lambda(\bm A) = \lambda(\bm{B}) \cup \{0\}$ and $\bm A$ has an eigenvalue zero with multiplicity at least $p-m$. Since $\bm{B}$ is symmetric all eigenvalues are real-valued.	Application of the Gershgorin circle theorem  to $\bm{B}$ yields for the remaining $m$ eigenvalues $\lambda_{(k-1)m+1},...,\lambda_{km}$ are contained in the following set
\begin{align*}
& \bigcup_{j=(k-1)m+1}^{km}  \left \{ z \in \mathbb{R}: \, | f(x_j) +\mathcal{O}\{p^{-1}+\log(p) p^{-\beta}\} -z|\right. 
&\left.< \mathcal{O}\{mp^{-1}+m\log(p) p^{-\beta}\} \right \}.
\end{align*} 
Since $f$ is $\min\{\beta,1\}$-H\"older continuous, we have $\lambda_j= f(x_j)+\mathcal{O}\{mp^{-1}+m\log(p) p^{-\beta}\}$, for $j=(k-1)m+1,...,km$.  Noting that $T^{-1}=mp^{-1}$ the statement follows.\hfill \qed %For $n>1$, the proof is similar since $\tilde{W}_{i,k}$ for $i=1,...,n$ are independent by construction.\hfill \qed

\subsection{Proof of Lemma \ref{lemma:representY} }
In the regression setting of \citet{cai2010nonparametricfest} and \citet{brown2010nonparametric} the gamma random variables in each bin are independent. Using Lemma \ref{lemma:rewriteBins}, we can rewrite our setting into one with independent observations per bin. Thus, the first step of the proof  is to apply  the results of \citet{cai2010nonparametricfest} and \citet{brown2010nonparametric}  on the approximation of $Y^*_k$ by a Gaussian random variable $m^{-1/2}Z_k$ and a non-Gaussian remainder $\xi_k$.  Since in our setting the observations of different bins are correlated,  the correlation $Cov(Z_k,Z_l)$ is derived in the second part of the proof.

By Lemma \ref{lemma:rewriteBins}, $Q_k$ can be written as sum of $m=np/T$ independent gamma random variables with  mean function $\tilde{f}(x):=f(x)+S(x)$, where $S(x)=\mathcal{O}\{T^{-1}+T^{-1}\log(p)p^{1-\beta}\}$. % Note that the constant in  $\mathcal{O}_*$ continuously depends on the argument $x$ of $f$ since all operations in Lemma \ref{lemma:DCTdiag} and \ref{lemma:rewriteBins} are linear.
It follows from Lemma 1 and \ref{lemma:rewriteBins} that the $S$ term  depends continuously on $x\in[0,1]$ and that a global constant hidden in the $\mathcal{O}$ term exists which is independent of the $x$.
Furthermore, $f\in[\delta, M_0]\subset \mathbb{R}_{>0}$ by assumption. As $\tilde{f}$ is the mean of gamma random variables, it holds that $\tilde{f}>0$. Thus,  $\tilde{f}$ is continuous and takes values in a compact set $[\epsilon,\nu] \subset \mathbb{R}_{>0}$ of the support of the gamma distribution. By Theorem 1 of \citet{cai2010nonparametricfest} for the gamma distribution %with regression function $\tilde{f}(x)=f(0.5x)$ and using the grid points $\left((i-1)/(p-1)\right)_{i=1}^p$ instead of  $\left(i/p\right)_{i=1}^p$ 
it follows $$Y_{k}^*=H\left \{\tilde{f}(x_k^* )\right\} +m^{-1/2}Z_{k}+\xi_{k},  \quad k=1,...,T,$$ 
where $(k-1)/(2T-2)\leq x_k^* \leq k/(2T-2)$, $Z_{k}\sim \mathcal{N}(0,1)$, and  $\xi_{k}$ is a mean zero random variable.   $Z_k$ is defined via quantile coupling. It is the Gaussian approximation to the standardized version of $Q_k$ where the $W_{ij}$ in the $k$-th bin are assumed to be i.i.d. In particular, $Z_k$ approximates $\bar{Q}_{k}=\frac{\sum_{j=(k-1)p/T+1}^{kp/T} \sum_{i=1}^n \bar{W}_{i,j}-m\tilde{f}(x_k^*)}{\sqrt{2m}\tilde{f}(x_k^*)}$ where $\bar{W}_{i,j}\overset{iid.}{\sim}\Gamma(1/2,2\tilde{f}(x_k^*))$ and such that $Cov(\bar{W}_{i,j},\bar{W}_{i,h})=Cov({W}_{i,j},{W}_{i,h})$ for $j=(k-1)p/T+1,...,kp/T$ and $h\in\{1,...,p\}\setminus\{(k-1)p/T+1,...,kp/T\}$.  

Let $\theta$ be the maximum difference of the observations' means in each bin. Then,
$$\theta=\max_{k=1,...,T} \max_{j,l =(k-1)p/T+1,...,kp/T} \left | \mathbb{E}[ \tilde{W}_{1,j}]-\mathbb{E}[ \tilde{W}_{1,l}] \right| \leq c_1\left\{T^{-1}+T^{-1}\log(p)p^{1-\beta}\right\}.$$ 
Application of Lemma 4 and Theorem 1 of \citet{brown2010nonparametric} yield that $\xi_{k}$ satisfies   $\mathbb{E}[|\xi_{k}|^\ell] \lesssim \log^{2\ell}(m) [m^{-\ell}+\left\{T^{-1}+T^{-1}\log(p) p^{1-\beta}\right\}^\ell]$ for any constant integer $\ell>0$ and where the hidden constant depends on $\ell$.\\
Next, 
$$ Y_{k}^*= H\left \{f \left (\frac{k-1}{2T-2} \right)  \right \} +\epsilon_k+m^{-1/2}Z_k+ \xi_k, \quad k=1,...,T,$$
where $\epsilon_k=H [f \left \{(k-1)/(2T-2) \right\}  ]-H \{\tilde{f} \left (x_k^* \right) \}$ and $|\epsilon_k|\lesssim (np)^{-1}+\log(p)(np)^{-\beta} $ by the Lipschitz continuity of the logarithm and the $\alpha$-H\"older continuity of $f$. 
To compute $Cov(Z_k,Z_l)$ for $k\neq l$ we use  Hoeffding's covariance identity.  In particular, we show that $Cov(Z_k,Z_l)\lesssim Cov(\bar{Q}_k,\bar{Q}_l)$. The claim then follows from noting that
\begin{align*}
0\leq Cov(\bar{Q}_k,\bar{Q}_l)
&=\frac{m\mathcal{O}\{Cov(W_{i,j},W_{i,h})\}}{2\tilde{f}(x_k^*)\tilde{f}(x_l^*)}=\mathcal{O}\{mp^{-2}+m\log(p)p^{-2\beta}\}
\end{align*} where $j=(k-1)p/T+1,...,kp/T$ and $h=(l-1)p/T+1,...,lp/T$.
Let $\Phi$ and $F_{\bar{Q}} $ denote the  cumulative distribution functions of $Z_k,\, Z_l,\, \bar{Q}_k$ and $ \bar{Q}_l$. Since the $Z_k$ are defined via quantile coupling, it holds $Z_k=\Phi^{-1}\{F_{\bar{Q}}(\bar{Q}_k)\}$  \citep[see][]{komlos1975approximation, mason2012quantile}.
Furthermore, define the uniform random variables $U_k=\Phi(Z_k)=F_{\bar{Q}}(\bar{Q}_k)$ and $U_l=\Phi(Z_l)=F_{\bar{Q}}(\bar{Q}_l)$ with cumulative distribution function $F_U$. The corresponding joint cumulative distribution functions are denoted by $F_{Z,Z},\, F_{\bar{Q},\bar{Q}},\, F_{U,U}$. By Hoeffding's covariance identity 
\begin{align*}
Cov(\bar{Q}_k,\bar{Q}_l)&= \int _{\mathbb {R} }\int _{\mathbb {R} }F_{\bar{Q},\bar{Q}}(q,r)-F_{\bar{Q}}(q)F_{\bar{Q}}(r)\,dq\,dr\\
&=\int _{\mathbb {R} }\int _{\mathbb {R} }\left\{F_{Z,Z}(x,y)-\Phi(x)\Phi(y)\right\} \frac{\phi(x)}{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(x)\}}\frac{\phi(y)}{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(y)\}}\,dx\,dy.
\end{align*} 
Let $\rho=Cov(Z_k,Z_l)$. Then, the identity
$$F_{ZZ}(x,y)=\frac{1}{2\pi} \int_0^\rho \frac{1}{\sqrt{(1-r^2)}} \exp \left \{ -\frac{x^2-2rxy+y^2}{2(1-r^2)}\right \}\,dr +\Phi(x)\Phi(y)$$
implies $F_{Z,Z}(x,y)-\Phi(x)\Phi(y)\geq 0\, \text{ for all } x,y\in \mathbb{R} \iff \rho\geq 0, $ \citep[see][9.3.1.3]{patel1996handbook}.
Since $Cov(\bar{Q}_k,\bar{Q}_l)\geq 0$ and the ratio of two densities is non-negative,  it follows $Cov(Z_k,Z_l)\geq 0$.
Furthermore,
\begin{align*}
Cov(Z_l,Z_k)&= \int _{\mathbb {R} }\int _{\mathbb {R} }F_{Z,Z}(x,y)-\Phi(x)\Phi(y)\,dx\,dy\\
&= \int_0^1 \int_0^1 \left\{F_{UU}(u,v)-F_U(u)F_U(v) \right\} \frac{1}{\phi\{\Phi^{-1}(u)\}}\frac{1}{\phi\{\Phi^{-1}(v)\}}\,du\,dv\\
&\leq  \int_{1/2}^1 \int_{1/2}^1 \left\{F_{UU}(u,v)-F_U(u)F_U(v) \right\} \frac{1}{\phi\{\Phi^{-1}(u)\}}\frac{1}{\phi\{\Phi^{-1}(v)\}}\,du\,dv \\
&\, + \int_0^1 \int_0^{1/2} \left\{F_{UU}(u,v)-F_U(u)F_U(v) \right\} \frac{2}{\phi\{\Phi^{-1}(u)\}}\frac{1}{\phi\{\Phi^{-1}(v)\}}\,du\,dv.
\end{align*} 
The last integral corresponds to $E[Z_kZ_l^-]=Cov(Z_k,Z_l)/2$ where $Z_l^-{=}Z_l\mathbbm{1}\{Z_l\leq0\}$. It remains to show that it exists a $c>0$ such that $\frac{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}}{\phi\{\Phi^{-1}(u)\}}\leq   c$ for all $u\in (1/2,1)$. Then, it follows
\begin{align*}
Cov(Z_l,Z_k)%&\leq 2\int_{1/2}^1 \int_{1/2}^1 \left(F_{UU}(u,v)-F_U(u_1)F_U(v) \right) \frac{1}{\phi(\Phi^{-1}(u))}\frac{1}{\phi(\Phi^{-1}(v)}\,du\,dv\\
&\leq 2c^2 \int_0^1 \int_0^1 \left\{F_{UU}(u,v)-F_U(u)F_U(v) \right\} \frac{1}{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}}\frac{1}{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(v)\}}\,du\,dv\\
&=2c^2Cov(\bar{Q}_k,\bar{Q}_l).
\end{align*}
Note that for  $u\in (1/2,1)$
\begin{align*}
\frac{\delta}{\delta u}\frac{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}}{\phi\{\Phi^{-1}(u)\}}%&= \frac{f^\prime_Q(F_m^{-1}(u))\phi(\Phi^{-1}(u))}{f_Q(F_m^{-1}(u)))\phi(\Phi^{-1}(u))^2} +\frac{f_Q(F_m^{-1}(u))\phi^\prime(\Phi^{-1}(u))}{\phi(\Phi^{-1}(u))^3}\\
&= \frac{f^\prime_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}}{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}\phi\{\Phi^{-1}(u)\}} -\frac{\Phi^{-1}(u)f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}}{\phi\{\Phi^{-1}(u)\}^2}.
\end{align*} Since $\Phi^{-1}(u){>}0$ for $u\in(1/2,1)$, the second term is  positive. To see that the first term is  negative for $u\in (1/2,1)$ we rewrite  $f_{\bar{Q}}$ and $F^{-1}_{\bar{Q}}$ with respect to the non-normalized gamma random variable $X=\sum_{j=(k-1)p/T+1}^{kp/T} \sum_{i=1}^n \bar{W}_{i,j}\sim\Gamma(m/2, 2\tilde{f}(x_k^*))$. Hence,
\begin{align*}
&f_{\bar{Q}}(x)=\sigma f_{X}(x\sigma +\mu),\quad  & F_{\bar{Q}}^{-1}(u)=\frac{ F^{-1}_{X}(u)-\mu}{\sigma}
\end{align*} where  $\sigma$ and $\mu$ are the  standard deviation and the mean of $X$. %, and $\gamma(x,y)$ is the lower incomplete Gamma function.
Note that the mode of $A\sim\Gamma(a,b)$ is at $b (a-1)$. Since $x\sigma +\mu=2\tilde{f}(x_k)(\frac{m}{2}-1)$ implies $x=-\sqrt{2/m}$, it follows
that $f_{\bar{Q}}(x)$ is monotone decreasing for $x\geq-\sqrt{2/m}$. Furthermore, $F_{\bar{Q}}(-\sqrt{m/2})\leq 0.5$ for all $m\in\mathbb{N}$ as $f_{\bar{Q}}(x)$ is right-skewed.
%Furthermore, $F_{\bar{Q}}(-\sqrt{m/2})\to 0.5$ from below for $m \to \infty$ , where $F_{\bar{Q}}(x)= \gamma \left(\frac{m}{2},\frac{x\sigma +m\mu}{2\tilde{f}(x_k^*)}\right)/\Gamma \left (\frac{m}{2}\right)$ is the cumulative distribution function of $\bar{Q}$.
In particular, $-\sqrt{m/2}\leq F^{-1}_{\bar{Q}}(1/2)$ for all $m\in\mathbb{N}$.  Finally, since $f_{\bar{Q}}(-\sqrt{2/m})\to \phi(0)$  for $m\to \infty$ there is a constant $c>0$ not depending on $m$ such that
\begin{equation*} 
\frac{f_{\bar{Q}}\{F_{\bar{Q}}^{-1}(u)\}}{\phi\{\Phi^{-1}(u)\}}\leq\frac{f_{\bar{Q}}(-\sqrt{2/m})}{\phi(0)}  \leq c.
\end{equation*}\hfill \qed


\subsection{Simulation Study with non-Gaussian Data}
\label{app:nogauss_simulation}
The simulation study in Section \ref{sec:simulation} is performed in the same way, but with the uniform and the gamma distribution instead of the Gaussian distribution.

\subsubsection{Uniform distribution}
The observations follow a uniform distribution with covariance matrices $\bm \Sigma_1,\,\bm \Sigma_2,\,\bm \Sigma_3$ of examples $\bm{1,\, 2,\, 3}$, i.e., $\bm Y_i=\bm\Sigma^{1/2}_jX_i,\,j=1,3,$ with $X_1,...,X_n \overset{i.i.d.}{\sim} \text{Unif}[-\sqrt{3},\sqrt{3}]$. For example $\bm{2}$, the parameter \textit{innov} of the R function \textit{arima.sim} is used to pass the innovations $X_1,...,X_n \overset{i.i.d.}{\sim} \text{Unif}[-\sqrt{3},\sqrt{3}]$. The resulting process is multiplied by $\sqrt{1.2}$. 	Table \ref{tab_Simulation_A_uniform}, \ref{tab_Simulation_B_uniform} and \ref{tab_Simulation_C_uniform}  show respectively the results for  \textbf{(A)} $p=5000,\, n=1$, \textbf{(B)} $p=1000,\, n=50$ and  \textbf{(C)} $p=5000,\, n=10$.


% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Mon Aug 29 12:34:50 2022
\begin{table}[H] 
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c}{ \textbf{(3) Lipschitz cont. $f$} }&\textbf{time}\\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0074 & 0.0023 & 0.1886 & 0.0330 & 0.0401 & 0.0062 & 3.9605 \\ 
		VST-DCT (ML) & 0.0057 & 0.0019 & 0.1844 & 0.0326 & 0.0399 & 0.0061 & 3.9633 \\ 
		tapering (semi-oracle) & 0.0049 & 0.0016 & 0.1726 & 0.0370 & 0.0284 & 0.0077 & 4.7148 \\ 
		sample covariance & 171.3200 & 35.9913 & 566.5580 & 59.6976 & 269.6001 & 37.7044 & 0.3431 \\ 	\hline
		VST-DCT (GCV-oracle) & 0.0039 & 0.0014 & 0.1711 & 0.0313 & 0.0328 & 0.0054 &  \\ 
		VST-DCT (ML-oracle) & 0.0042 & 0.0015 & 0.1818 & 0.0322 & 0.0390 & 0.0060 &  \\ 
		tapering (oracle) & 0.0037 & 0.0013 & 0.1583 & 0.0311 & 0.0146 & 0.0034 & \\ 
		
	\end{tabular}
	\caption{ \textbf{(A)} $\bm{p=5000,\, n=1}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the $L_2$ norm, respectively. Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_A_uniform}
\end{table}

% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Fri Aug 26 16:27:40 2022
\begin{table}[H]
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c}{ \textbf{(3) Lipschitz cont. $f$} }&\textbf{time}\\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0009 & 0.0002 & 0.0888 & 0.0220 & 0.0068 & 0.0009 & 23.3553 \\ 
		VST-DCT (ML) & 0.0008 & 0.0002 & 0.0959 & 0.0226 & 0.0077 & 0.0010 & 23.2826 \\ 
		tapering (CV) & 0.0030 & 0.0012 & 0.1400 & 0.0364 & 0.0081 & 0.0017 & 24.4997 \\ 
		sample covariance & 0.8885 & 0.5917 & 4.0965 & 0.9704 & 1.3657 & 0.6137 & 0.1525 \\ \hline
		VST-DCT (GCV-oracle) & 0.0006 & 0.0002 & 0.0871 & 0.0219 & 0.0057 & 0.0008 &  \\ 
		VST-DCT (ML-oracle) & 0.0007 & 0.0002 & 0.0957 & 0.0226 & 0.0076 & 0.0009 & \\ 
		tapering (oracle) & 0.0018 & 0.0011 & 0.1306 & 0.0360 & 0.0062 & 0.0015 &  \\ 
		
	\end{tabular}
	\caption{ \textbf{(B)} $\bm{p=1000,\, n=50}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the $L_2$ norm, respectively. Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_B_uniform}
\end{table}
% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Mon Aug 29 14:25:58 2022
\begin{table}[H] 
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c}{ \textbf{(3) Lipschitz cont. $f$} }&\textbf{time}\\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0009 & 0.0002 & 0.0931 & 0.0220 & 0.0057 & 0.0008 & 4.2534 \\ 
		VST-DCT (ML) & 0.0007 & 0.0002 & 0.0998 & 0.0227 & 0.0064 & 0.0009 & 4.2722 \\ 
		tapering (CV) & 0.0332 & 0.0259 & 0.4620 & 0.1300 & 0.0743 & 0.0285 & 649.1202 \\ 
		sample covariance & 9.1367 & 4.5427 & 35.9029 & 7.2986 & 12958.1671 & 4.7971 & 1.1976 \\ \hline 
		VST-DCT (GCV-oracle) & 0.0006 & 0.0002 & 0.0901 & 0.0219 & 0.0048 & 0.0007 &  \\ 
		VST-DCT (ML-oracle) & 0.0007 & 0.0002 & 0.0993 & 0.0227 & 0.0063 & 0.0008 & \\ 
		tapering (oracle) & 0.0301 & 0.0258 & 0.4254 & 0.1292 & 0.0681 & 0.0278 &  \\ 
		
	\end{tabular}
	\caption{ \textbf{(C)} $\bm{p=5000,\, n=10}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the $L_2$ norm, respectively. Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_C_uniform}
\end{table}



\subsubsection{Gamma distribution}
The observations follow  a centered gamma distribution with covariance matrices $\bm \Sigma_1,\,\bm \Sigma_2,\,\bm \Sigma_3$ of examples $\bm{1,\, 2,\, 3}$, i.e., $\bm Y_i=\bm\Sigma^{1/2}_jX_i,\,j=1,3,$ with $X_1+\sqrt{2},...,X_n+\sqrt{2} \overset{i.i.d.}{\sim} \Gamma(2,1/\sqrt{2})$. For example $\bm{2}$, the parameter \textit{innov} of the R function \textit{arima.sim} is used to pass the innovations $X_1+\sqrt{2},...,X_n+\sqrt{2} \overset{i.i.d.}{\sim} \Gamma(2,1/\sqrt{2})$.  The resulting process is multiplied by $\sqrt{1.2}$. Table \ref{tab_Simulation_A_gamma}, \ref{tab_Simulation_B_gamma} and \ref{tab_Simulation_C_gamma} show respectively the results for  \textbf{(A)} $p=5000,\, n=1$, \textbf{(B)} $p=1000,\, n=50$ and  \textbf{(C)} $p=5000,\, n=10$.

% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Fri Aug 26 16:24:52 2022
\begin{table}[H] 
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c}{ \textbf{(3) Lipschitz cont. $f$} }&\textbf{time}\\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0118 & 0.0043 & 0.2035 & 0.0323 & 0.0518 & 0.0088 & 3.8888 \\ 
		VST-DCT (ML) & 0.0097 & 0.0039 & 0.1625 & 0.0306 & 0.0479 & 0.0082 & 3.8831 \\ 
		tapering (semi-oracle) & 0.0095 & 0.0037 & 0.1744 & 0.0374 & 0.0416 & 0.0115 & 4.6909 \\ 
		sample covariance & 167.5072 & 33.8064 & 545.1911 & 62.8606 & 248.2843 & 35.9759 & 0.3418 \\ \hline
		VST-DCT (GCV-oracle) & 0.0079 & 0.0035 & 0.1511 & 0.0293 & 0.0417 & 0.0076 &  \\ 
		VST-DCT (ML-oracle) & 0.0082 & 0.0036 & 0.1610 & 0.0302 & 0.0479 & 0.0081 &  \\ 
		tapering (oracle) & 0.0076 & 0.0033 & 0.1505 & 0.0312 & 0.0199 & 0.0054 & \\ 
		
	\end{tabular}
	\caption{ \textbf{(A)} $\bm{p=5000,\, n=1}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the $L_2$ norm, respectively. Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_A_gamma}
\end{table}

% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Fri Aug 26 16:26:03 2022
\begin{table}[H]
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c}{ \textbf{(3) Lipschitz cont. $f$} }&\textbf{time}\\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0013 & 0.0004 & 0.0862 & 0.0218 & 0.0059 & 0.0010 & 23.3559 \\ 
		VST-DCT (ML) & 0.0011 & 0.0004 & 0.0928 & 0.0225 & 0.0074 & 0.0011 & 23.2890 \\ 
		tapering (CV) & 0.0029 & 0.0014 & 0.1338 & 0.0359 & 0.0085 & 0.0020 & 24.1672 \\ 
		sample covariance & 0.8944 & 0.6033 & 4.0165 & 1.0358 & 1.4070 & 0.6303 & 0.1460 \\\hline
		VST-DCT (GCV-oracle) & 0.0009 & 0.0003 & 0.0818 & 0.0216 & 0.0055 & 0.0009 &  \\ 
		VST-DCT (ML-oracle) & 0.0009 & 0.0004 & 0.0921 & 0.0224 & 0.0073 & 0.0011 &  \\ 
		tapering (oracle) & 0.0022 & 0.0013 & 0.1224 & 0.0355 & 0.0072 & 0.0018 &  \\ 
		
	\end{tabular}
	\caption{ \textbf{(B)} $\bm{p=1000,\, n=50}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the $L_2$ norm, respectively. Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_B_gamma}
\end{table}
% latex table generated in R 4.2.1 by xtable 1.8-4 package
% Mon Aug 29 12:20:31 2022
\begin{table}[H] 
	\centering
	\scriptsize
	\linespread{1.2}
	\begin{tabular}{l|cc|cc|cc|c}
		&\multicolumn{2}{c|}{\textbf{(1) polynomial $\sigma$ }} &\multicolumn{2}{c|}{\textbf{(2)} \textbf{ARMA}$\bm{(2,2)}$} &\multicolumn{2}{c}{ \textbf{(3) Lipschitz cont. $f$} }&\textbf{time}\\
		& $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  $\|\widehat{\Sigma}-\Sigma\|^2$ & $\|\hat{f}-f\|^2_2$ &  in sec \\			
		\noalign{\hrule height 1pt} 
		VST-DCT (GCV) & 0.0014 & 0.0004 & 0.0910 & 0.0220 & 0.0063 & 0.0010 & 4.4351 \\ 
		VST-DCT (ML) & 0.0011 & 0.0004 & 0.0963 & 0.0226 & 0.0070 & 0.0011 & 4.4039 \\ 
		tapering (CV) & 0.0337 & 0.0259 & 0.4376 & 0.1287 & 0.0777 & 0.0284 & 681.1327 \\ 
		sample covariance & 8.9878 & 4.3716 & 36.2328 & 7.2028 & 13099.5464 & 4.7331 & 1.2680 \\ \hline
		VST-DCT (GCV-oracle) & 0.0009 & 0.0003 & 0.0865 & 0.0218 & 0.0056 & 0.0009 &  \\ 
		VST-DCT (ML-oracle) & 0.0010 & 0.0003 & 0.0958 & 0.0226 & 0.0070 & 0.0010 &  \\ 
		tapering (oracle) & 0.0306 & 0.0258 & 0.4085 & 0.1281 & 0.0675 & 0.0276 & \\ 
		
	\end{tabular}
	\caption{ \textbf{(C)} $\bm{p=5000,\, n=10}$: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the $L_2$ norm, respectively. Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).}
	\label{tab_Simulation_C_gamma}
\end{table}
\end{appendix}

%\biblist
\bibliographystyle{apalike}
\bibliography{Literature}

\end{document}