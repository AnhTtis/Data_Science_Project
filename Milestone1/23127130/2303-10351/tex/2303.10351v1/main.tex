
% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\usepackage{xcolor}
\newcommand{\qingming}[1]{\textcolor{green}{ #1 (Qingming)}}
\newcommand{\chiehchi}[1]{\textcolor{red}{ #1 (Chieh-Chi)}}
\newcommand{\viktor}[1]{\textcolor{blue}{ #1 (Viktor)}}
\renewcommand{\qingming}[1]{}
\renewcommand{\chiehchi}[1]{}
\renewcommand{\viktor}[1]{}
\usepackage[normalem]{ulem}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
% Title.
% ------
% \title{Toward Efficient Model Deployment for Acoustic Event Classification by Once-for-all Network Architecture Search}

\title{Weight-sharing Supernet for Searching Specialized Acoustic Event Classification Networks Across Device Constraints}
%
% Single address.
% ---------------
\name{Guan-Ting Lin$^{\dagger12}$\thanks{$^{\dagger}$This work was done during Guan-Tingâ€™s internship at Amazon.}, Qingming Tang$^{2}$, Chieh-Chi Kao$^{2}$, Viktor Rozgic$^{2}$, Chao Wang$^{2}$}
\address{National Taiwan University$^{1}$      Amazon Alexa$^{2}$ }

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%
\maketitle
%
\begin{abstract}
 Acoustic Event Classification (AEC) has been widely used in devices such as smart speakers and mobile phones for home safety or accessibility support \cite{lopatka2016detection}. As AEC models run on more and more devices with diverse computation resource constraints, it became increasingly expensive to develop models that are tuned to achieve optimal accuracy/computation trade-off for each given computation resource constraint. In this paper, we introduce a Once-For-All (OFA) Neural Architecture Search (NAS) framework for AEC. Specifically, we first train a weight-sharing supernet that supports different model architectures, followed by automatically searching for a model given specific computational resource constraints. Our experimental results showed that by just training once, the resulting model from NAS significantly outperforms both models trained individually from scratch and knowledge distillation (25.4\% and 7.3\% relative improvement). 
 We also found that the benefit of weight-sharing supernet training of ultra-small models comes not only from searching but from optimization.
 
 %We also found that it was crucial to design suitable search spaces for the AEC supernet. Furthermore, we show that the benefit of weight-sharing supernet training of ultra-small models mainly comes from optimization. \qingming{On the one hand, we mentioned that 'search space design' is crucial; On the other hand, we say the benefit mainly comes from optimization. I made a small change, ``small" to  ``ultra small".}
 
  %Acoustic Event Classification (AEC) is important and ubiquitous in our daily life, leading to diverse audio-based industrial applications and products, such as smart home assistants and accessibility features on mobile phones. As the number of supporting devices increases, it is crucial to quickly support such devices with suitable AEC models. However, the current solution is to naively design and train individual AEC models for each specific device constraint, causing a huge cost of labor and time. To address this inefficient deployment of the AEC model, we introduce Once-for-all (OFA) Neural Architecture Search (NAS) framework on AEC. Specifically, we train a weight-sharing supernet that supports different model architectures, then search models consume the desired amount of computational resources given specific device constraints. From the experimental results, by just training once, we show that the searched models not only significantly outperform individually-trained models but also yield slightly better performance than individual models trained with knowledge distillation, 25.4\% and 7.3\% relative performance improvement compared to baseline and knowledge distillation models at 1M model size. We explore the AEC supernet with different search space designs, finding that it is crucial to design suitable search spaces for the AEC supernet. Furthermore, we show the benefit of weight-sharing supernet training of small models mainly comes from optimization.  
 % \chiehchi{This last sentence is somehow redundant as you already mentioned the relative improvement over KD above.}
\end{abstract}
%
\begin{keywords}
Acoustic Event Classification, AudioSet, Neural Architecture Search, Weight-sharing Supernet, Knowledge Distillation
\end{keywords}
%
\vspace{-0.3cm}
\section{Introduction}
Acoustic Event Classification (AEC) is the task of detecting the occurrence of certain events based on acoustic signals.
It has been widely used in devices such as smart speakers and mobile phones for home safety or accessibility support \cite{lopatka2016detection}.
% Large model size
Previous works have shown great progress on AEC using Convolutional Neural Network (CNN) \cite{pann, hershey2017cnn, xu2017convolutional, takahashi2016deep} and Audio Spectrogram Transformer (AST) \cite{ast, ssast, paast, maeast, audioMAE}. 
However, such models are usually computationally expensive and not suitable for edge devices (e.g., 86M \qingming{Number of parameters may vary due to implementation? I think we specifically mean the model from this paper \cite{ast}?}model parameters for AST~\cite{ast}).
To address this issue, several \textit{Model Compression} methods have been proposed using \textit{Knowledge Distillation} \cite{choi2022temporal, gao2022multi, jung2020knowledge, shi2019compression, wu2018reducing, shi2019teacher, gong2022cmkd, Shi2019}, which distills the learned knowledge from teacher model to a small student model.

% Acoustic Event Classification (AEC) aims to recognize events from sounds (e.g., smoke alarm, snoring, etc.) It is widely used for smart home assistants to capture noteworthy events from audio. 
% % Large model size
% Previous works have demonstrated remarkable progress on AEC using Convolutional Neural Network (CNN) \cite{pann, hershey2017cnn, xu2017convolutional, takahashi2016deep} or Audio Spectrogram Transformer (AST) \cite{ast, ssast, paast, maeast, audioMAE}. Recent works mainly focus on achieving state-of-the-art (SOTA) AEC performance with large model sizes, which is computationally expensive and thus unsuitable for edge devices (e.g., 86M model parameters for AST). Several \textit{Model Compression} methods have been proposed to address the problem by \textit{Knowledge Distillation} \cite{choi2022temporal, gao2022multi, jung2020knowledge, shi2019compression, wu2018reducing, shi2019teacher, gong2022cmkd, Shi2019}, which distills the learned knowledge from teacher model to a small student model.

% Efficient deployment
As AEC models run on more and more devices with diverse computation resource constraints, one emerging area of research is how to efficiently train models with optimal accuracy/computation trade-off for each given computation resource constraint. For a given constraint, Neural Architecture Search (NAS) can be used to find the optimal model architecture. However, this still requires a dedicated training process for each given constraint, which is inefficient for developing models for devices with diverse computation resource constraints. 
One-shot weight-sharing NAS approaches \cite{ofa, compofa, alphanet, autoformer} were proposed to solve this problem by training a weight-sharing supernet that contains various sub-networks and then searching for the specialized sub-network given diverse resource constraints without re-training, which reduces the training effort for $O(N)$ devices from $O(N)$ to $O(1)$.
Although this approach had shown great performance on image recognition tasks~\cite{ofa, compofa, alphanet, autoformer}, there is no previous work using weight-sharing NAS in the audio domain. The most relevant work is LightHuBERT \cite{lighthubert}, which adopts the once-for-all \cite{ofa} framework on the large transformer-based model. However, it mainly focuses on self-supervised and task-agnostic speech representation learning. 


% Another challenge in AEC is \textit{Efficient Deployment} as the increasing number of devices to be supported. Each edge device has different on-device computational constraints, which makes the model training and deployment extremely labor-intensive, as it is unknown what model architecture is suitable under the given resource constraints. Therefore, \textit{how to efficiently design and deploy the model to certain device constraints} becomes a critical problem in the industries. 

% Researchers studied approaches to find optimal model architectures in the Neural Architecture Search (NAS) field. Notably, the one-shot weight-sharing NAS approaches \cite{ofa, compofa, alphanet, autoformer}  cope with the challenge by training a weight-sharing supernet that contains diverse sub-networks and then searching for the specialized sub-network given resource constraints without re-training. That is, the model design cost for $O(N)$ devices reduces from $O(N)$ to $O(1)$. On image recognition tasks, the searched sub-networks show impressive performance versus the computation trade-off \cite{ofa, compofa, alphanet, autoformer}. In contrast to audio-related tasks, no previous studies have tried weight-sharing NAS in the audio domain. The most relevant work is LightHuBERT \cite{lighthubert}, which adopts the once-for-all \cite{ofa} framework on the large transformer-based model, but they mainly focus on self-supervised and task-agnostic speech representation learning. 

% Our goal
% \chiehchi{Same question on ``deployment''.}
This work is the first attempt to efficiently train models with optimal accuracy/computation trade-off for each given computation resource constraint on AEC. 
Specifically, we first leverage knowledge distillation to train a weight-sharing CNN-based AEC supernet (``Once-for-all" supernet) that contains sub-networks with shared weights. After supernet training, we directly search for specialized sub-network given a specific constraint without re-training. 
We choose CNN-based architectures for our experiments since it requires less computational resource than AST-based architectures, making them more suitable for edge devices. Experiments are conducted on AudioSet \cite{audioset}, which is a popular multi-labeled audio classification benchmark. Our contributions can be summarized below: 
% This work is the first attempt to address the efficient deployment challenge by weight-sharing NAS on AEC. Specifically, we first leverage knowledge distillation to train a weight-sharing CNN-based AEC supernet (we called it a ``Once-for-all (OFA)" supernet) that contains several sizes of sub-networks with shared weights. After supernet training, we directly search for specialized sub-network given devices constraint without re-training. We focus on CNN-based AEC models since they have shown good performance and are much more computationally friendly than the AST-based model, making them suitable for edge devices. Experiments are conducted on AudioSet \cite{audioset}, the popular multi-labeled audio classification benchmark. Our contributions can be summarized below: 
\vspace{-0.1cm}
\begin{itemize}
    \item To the best of our knowledge, this is the first work to efficiently train models for edge devices with diverse computation resource constraints using weight-sharing NAS for AEC. Although this work focuses on AEC, the underlying design could potentially be applied to other audio applications like music genre classification and keyword spotting.
    \item Experimental results show that the proposed method can find specialized AEC sub-networks that significantly outperform models trained from scratch, also outperforming models trained with knowledge distillation (25.4\% and 7.3\% relative improvement).  
    \item We found that adopting elastic depth (Section \ref{sec:2-2})\qingming{Should we refer readers to section 2.2?}in the first few blocks is harmful to the AEC supernet. Using only elastic depth in the last few blocks results in a good trade-off between model performance and computation cost.  
    % \item With weigh-sharing supernet training, the specialized sub-network yields 7.3\% relative improvement than the knowledge distillation model using the \textit{same network architecture}.
\end{itemize}

% \begin{itemize}
%     \item To the best of our knowledge, this is the first work to achieve efficient deployment for edge devices using weight-sharing NAS for acoustic event classification. Though we focus on acoustic event classification, the underlying design can potentially be applied to other tasks like music classification and speech command.
%     \item Experimental results show that our method can find specialized AEC sub-networks that significantly outperform individual models trained from scratch and are slightly better than individual models trained with knowledge distillation. 
%     \item We find out that adopting elastic depth in the first few blocks is detrimental to the AEC supernet, while only using elastic depth in the last few blocks results in a good performance and computation trade-off.  
%     \item With weigh-sharing supernet training, the specialized sub-network yields 7.3\% relative improvement than the knowledge distillation model using the \textit{same network architecture}.
% \end{itemize}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{OFA-AEC.drawio.png}
  \vspace{-0.3cm}
  \caption{The overview of OFA AEC framework. \textit{Left}: During OFA training, we adopt a teacher model to generate soft labels. The students are $K$ sub-networks sampled from the supernet. We adopt knowledge distillation for training supernet, where only students' input spectrograms are distorted (we use SpecAugment). In the sampled subnets, the red-colored layers are with elastic-sampled channel width, and the cross sign means elastic discard layer. \textit{Right}: After supernet training, we search for the best-performing sub-network given specific device constraints. The searched models typically achieve good performance and do not require re-training.}
  \label{fig:overview}
\end{figure*}

\vspace{-0.3cm}
\section{Method}
%In this section, we first define the problem formulation of the weight-sharing NAS on AEC, then explain the detailed training and searching procedures, followed by baseline methods. 
This section introduces our Once-For-All for Acoustic Event Classification (OFA-AEC) framework. We would explain how we design neural architecture space, how we conduct training, and also the architecture search algorithm. 

\vspace{-0.3cm}
\subsection{Problem formulation}
In this work, we focus on acoustic event classification (AEC), a multi-label classification problem. 
We denote $x_n$ as an audio clip in our dataset where $n$ is the sample index, and $f(x_n) \in [0, 1]^C$ is the model output presenting the presence probability of the $C$ labels. $y_n \in \{0, 1\}^C$ represents the label of $x_n$. We would use knowledge distillation in our training, and we denote our trained teacher model (with superior performance) as $f_t$. 

%Assume $x_n$ is the $n$-th audio clip in the dataset. $f(x_n) \in [0, 1]^C$ is the prediction probability of $C$ audio event classes from model $f$. $y_n \in \{0, 1\}^C$ represents the multi-class binary label of $x_n$. 
%We assume that there is a trained teacher model $f_t$ with a larger model size and greater performance. 
As self-explained by its name, Once-for-All (OFA) training would train a supernet where sub-network architectures of this supernet would be used for diverse downstream deployments. We denote the weights of the OFA supernet as $w_o$, and $S(w_o, a)$ as the model selection scheme (via architectural configuration $a$) that selects the part of the supernet model with the configuration $a$. We assume there are in total $M$ supported configurations, and the selection scheme $S(w_o, a_i)$ would create the $i$-th supported sub-network.
%while inherits the 
%that inherits partial of supernet's weights via $a_i$, 
%which is the $i$-th supported architecture among total $M$ configurations. 
The supernet training can be formulated as:
\vspace{-0.3cm}
\begin{equation}
   \mathop{min}_{w_o} \sum_{n=1}^{N} \sum_{i=1}^{M} \mathcal{L}(x_n, y_n; f_t, S(w_o, a_i)), 
   \label{eqn:ofa_1}
\end{equation}
where $\mathcal{L}$ is the knowledge distillation loss function.

\begin{figure}[t]
  \centering
\includegraphics[width=0.7\linewidth]{model.png}
  %\vspace{-0.3cm}
  \caption{The architecture of the once-for-all supernet, which takes $64 \times 1000$ Mel spectrogram feature as input with batch size $B$. The value in blocks indicates the number of channels for CNN layers or feature size for feed-forward layers. 
  }
  \label{fig:model}
\end{figure}

\subsection{Architecture space}
\label{sec:2-2}
% \chiehchi{One high-level question: in the projector, I assume it has a large number of parameters? Is this component essential for this architecture?}
%The first layer of the block is used to project its input into a specific channel size, and the second layer uses that channel size for input and output dimensions.
The architecture of the weight-sharing supernet is shown in Figure \ref{fig:model}. We divide a CNN-based AEC model into a sequence of blocks. The ``Static'' blocks refer to the standard fully-connected or convolutional 2D layers, whereas the ``Dynamic" block means it can be replaced by its sub-network. Each convolutional block contains 2 CNN layers, where an average-pooling layer with $2 \times 2$ pooling size is inserted between two consecutive blocks. On top of the final CNN layer, a global pooling layer, a fully-connected layer and a multi-label classifier with sigmoid activation functions are sequentially applied. 
%Global pooling is applied to the output of the last CNN layer, which is further followed by a fully-connected layer and a final multi-label classifier with the Sigmoid activation functions. 
All CNN layers use $3 \times 3$ kernel size.
%he channel size is denoted as the ratio of the full-size width. 

The once-for-all supernet supports two searching dimensions for each block: 1) \textbf{Elastic Width (EW)}: \textit{arbitrary numbers of channels}. Given a width ratio $r$ and the full width size $D$, the layer uses the first $\floor{r \times D}$ channels. We experiment with four ratios, say $\{0.4, 0.6, 0.8, 1.0\}$. 2) \textbf{Elastic Depth (ED)}: \textit{arbitrary number of layers}. As one block has two layers, the dynamic depth for each block is either $1$ or $2$. 

We include both EW and ED when searching for sub-networks during supernet training; However, we observed that only applying ED in the last convolutional block is preferred according to the ablation study on search space design in Section \ref{ablation:space}. 

\subsection{Weight-sharing supernet training}
% \chiehchi{Is this sampling purely random or you had enforced some stratified sampling in it?}
The basic idea for training a weight-sharing supernet is to uniformly sample a few sub-networks (e.g., $f_{s}^{i}$, $1\leq i \leq K$) from the $M$ supported sub-networks using the model selection scheme $S(w_o, a)$, then encourage the prediction of each sub-network to have high fidelity with respect to the teacher model $f_{t}$ via binary cross entropy loss.
%$f_{s}$ is the uniformly sampled student sub-network from $M$ types of supported architectures using the model selection scheme $S(w_o, a)$. 
%We use soft labels from the teacher model $f_t(x_n)$ with binary cross entropy loss. 
The training objective (Equation \eqref{eqn:ofa_1}) can be approximated as
\begin{equation}
    \sum_{n=1}^{N} \sum_{i=1}^{K} BCE(f_{s}^{i}(x_n), f_t(x_n)),
\end{equation}
where $BCE(f_s(x_n), f_t(x_n)) =  f_t(x_n) \cdot \mathop{ln}f_s(x_n) + (1-f_t(x_n)) \cdot \mathop{ln}(1-f_s(x_n))$ is the knowledge distillation loss $\mathcal{L}$, and $K$ is the number of sampled neural networks and $N$ is the number of samples. 

\subsection{Search algorithm}
During inference, we adopt the random search to find the optimal sub-network under the specific resource constraint. Specifically, we search the best sub-networks given the number of parameters ranging from $[N_{params}-\epsilon, N_{params}+\epsilon]$, where $N_{params}$ is the specific constraint, and $\epsilon$ is the tolerance. We randomly sample $p$ sub-networks from the OFA supernet as the candidate sub-networks, run inference for those candidates on the validation set, and select the best one for each constraint.

\begin{table}[t]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcc|cc}
\toprule
\textbf{Model} & \textbf{\#Params(M)} & \textbf{\%} & \textbf{mAP} & \textbf{\%}   \\ 
\midrule
AST \cite{ast}            & 86.0  &   -  & 34.7 & - \\
SSAST \cite{ssast}          & 86.0  &  -   & 31.0 & -\\
\midrule
CNN10 (B = 32) \cite{pann}  & 5.2  & -   & 27.8 & -\\
CNN10 (B = 64)*  & 5.2  & 100.0   & \textbf{29.6} & 100.0 \\ 
\midrule
\midrule
Baseline & 1.0   & 19.2 & 21.2 & 71.6\\
KD      & 1.0   & 19.2 & 24.8 & 83.7\\
OFA        & 0.9   & 17.3 & \textbf{26.6} & \textbf{89.9}\\ 
\midrule
Baseline & 2.1   & 40.3 & 22.6 & 76.4\\
KD      & 2.1   & 40.3 & 26.4 & 89.2\\
OFA          & 1.8  &  34.6  & \textbf{27.2} & \textbf{91.9}\\ 
% OFA-EW              & 1.9  &  36.5 & \textbf{27.7} & \textbf{93.6}\\ 
\midrule
Baseline & 3.5   & 67.3 & 24.8 & 83.8\\
KD     & 3.5   & 67.3 & \textbf{28.3} & \textbf{95.6}\\
OFA              & 3.6  & 69.2  & 27.9 & 94.3\\ 
% OFA-EW              & 3.7  & 71.2 & \textbf{28.4} & \textbf{96.0}\\ 
\bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\caption{Mean Average Precision (mAP) performance on AudioSet evaluation set. The training data is the balanced train set. ``B" denotes the batch size. ``*" is our \textit{teacher} model. ``\%" means the relative ratio compared to the teacher model.}
\label{tab:performance}
\end{table}

\vspace{-0.3cm}
\section{Experiments}
\subsection{Dataset}
We conduct our experiments on AudioSet \cite{audioset}, a well-known multi-label AEC benchmark corpus. There could be multiple events in a clip. The official balanced and evaluation sets are used as our train and test sets, with 22160 and 20371 10-second samples, respectively. The validation set is a subset randomly sampled from the unbalanced set with 28339 10-second samples without overlapping with the training set. The audio is pre-processed at 32K sampling rate.
% \qingming{So you are using the 10-second-multi-channel in s3?}
We use log-mel spectrogram, $1000$ frames $\times$ 64 mel bins, as our input feature. There could be multiple events present in each 10-second clip, and thus this is a multi-label classification problem. We use well-adopted Mean Average Precision (mAP) as our evaluation metric.

\vspace{-0.2cm}
\subsection{The teacher model and Baslines}
The teacher model $f_t$ uses the CNN10 architecture designed in \cite{pann}. 
In order to achieve good performance, we train the teacher model using data augmentation techniques (e.g., mixup \cite{mixup} and specaugment \cite{specaugment}) up to 400K iterations using batch size of $64$ on AudioSet balanced train set. As shown in Table \ref{tab:performance}, our model achieves 29.6 mAP on the AudioSet evaluation set, which outperforms the CNN10 model (when also trained on balanced set) as shown in \cite{pann}. We compare OFA training with two baseline approaches: \\
%One standard model development approach is to train individual models 
%under different
%resource constraints is to train individual models that can
%fit into the budget. Here we apply two methods: \\
\textbf{Baseline}: Training each model from scratch using BCE loss and specaugment, which is a standard way to train an AEC model.\\
\textbf{Knowledge Distillation (KD)}: Training each model with knowledge distillation and specaugment, which is a stronger baseline. 

For the architecture of individual models, based on the supernet search space, we set each model with a fixed width ratio and depth across all the dynamic blocks. For example, ``width ratio $= 0.4$, depth $= 2$" for all three dynamic CNN blocks. Thus, the individual models are all with a double-increasing number of channels. Note that the architectures of these individually-trained models are also supported architecture configurations in our search space. 
%Each fixed design has a certain number of parameters.

\vspace{-0.4cm}
\subsection{Implementation details}

We use the batch size of 64, and Adam optimizer with a 0.001 learning rate for all the experiments. Our specaugment \cite{specaugment} consists of two time and frequency masks on the spectrogram ($T=64, F=8$). For supernet training, we first only train the largest sub-network for 100K iterations, then train the sampled sub-networks for 200K iterations. The number of sampled sub-networks $K$ is 4. For the baseline methods, we train each model with 100K iterations. As the number of sub-network parameters ranges from 0.6M to 5.2M, we choose four computational constraints $\{0.8M, 1.8M, 2.8M, 3.8M\}$ with $\epsilon=0.2M$ tolerance for the random search. The population size $p$ for random search is 25.

\vspace{-0.4cm}
\subsection{Results}
The main results are shown in Table \ref{tab:performance}. First, for the models of size at ~1M \#Params, our OFA method yields significantly better performance than the baselines. Our searched model retains 89.9\% performance of the teacher model with only 17.3\% \#Params. On the other hand, training from scratch and knowledge distillation models achieve 21.2 and 24.8 mAP, respectively. 
The OFA method also achieves superior performance at around 2M \#Params, surpassing the baseline by 4.6 mAP and the KD method by 0.8 mAP. The results demonstrate that the weight-sharing NAS has the capability to produce small networks with outstanding performance. 
For the larger model size at around 3.5M \#Params, the OFA method achieves 27.9 mAP, which is much better than the baseline method (24.8 mAP) but slightly worse than the knowledge distillation model (28.3 mAP). 
Overall, by just training one weight-sharing supernet, we can search for sub-networks that have superior performance than baselines. Even when compared with KD, our methods also achieve at least comparable and usually superior performance. Please note, it is possible to achieve better performance if we use an advanced search algorithm instead of random search.

%Even compared with KD, 
%the sampled sub-networks also achieve much better performance 
%have at least comparable and usually superior performance compared with KD.
\begin{figure}[t]
  \centering
\includegraphics[width=0.8\linewidth]{ablation_ED.png}
  \vspace{-0.4cm}
  \caption{Comparison of different search spaces for the OFA supernet. We selectively apply ED on certain blocks, indicated with ``X". }
  \label{fig:ablation_ED}
\end{figure}
\section{Discussion}
% \qingming{It seems that we are confusing between layers and blocks? I suspect, saying ED is applied to later/front/early blocks/modules, would be less confusing?}
\subsection{Ablation study of search space}
\label{ablation:space}
% \qingming{Should we just quickly remind the audience that there are 2-layers in each module/block? And try to explain one example like [2, X, X]. }
%As shown in Figure ~\ref{fig:model}, the OFA-AEC supernet has three convolutional blocks each consists of two layers. 
We try different search spaces for the OFA-AEC supernet, showing the performance of the searched sub-networks in Figure \ref{fig:ablation_ED}. We always use EW, and selectively use ED on certain blocks (represented as ``X" in Figure \ref{fig:ablation_ED}). We first experiment with using ED for \textit{all} the dynamic blocks (Figure \ref{fig:ablation_ED} ``OFA EW+ED=[X,X,X]" curve) and it shows significant degradation in performance. To explore suitable ways for using ED, we try not applying elastic depth in some blocks (denoted as ``2" since all the two layers in that block would be retained). We find out that applying elastic depth to the front blocks is detrimental while applying ED to the later blocks is preferred. Especially, applying ED to the last convolutional block (``OFA EW+ED=[2,2,X]" curve) achieves superior performance, especially for ultra-small models (e.g., 26.6 mAP with 0.9M \#Params). 

%Since elastic depth may harm the supernet training in some cases, 
We also investigate only applying elastic width, denoted as \textit{EW-only}. In addition to the four width ratios, we add the minimal width ratio of 0.2 to enable the EW-only supernet containing small models around 1M \#Params. EW-only supernet achieves comparable performance as the EW+ED=[2,2,X] when the model size exceeds 2M. However, it performs poorly at the scale of 1M \#Params (22.8 mAP). These results indicate the necessity of using both EW and ED in supernet training and search, but we need to be careful when applying ED.
%, as it seems preferable to only discard layers in late instead of earlier blocks. As shown in the Figure, Using EW + ED applied to later blocks (e.g., [2, 2, X]) is the best design.

The sensitivity of elastic depth has not been discussed and observed in previous OFA methods on image recognition. Compared to their OFA model, which is a deep CNN model with several residual connection blocks, our AEC model is much shallower and based on standard convolutional layers. Hopefully, our ablation study can provide more insights to apply OFA training on architectures with different levels of complexity and on other audio tasks.
%provides insights and possible solutions to apply OFA for tasks that use the relatively simple neural network. 

\begin{figure}[t]
  \centering
\includegraphics[width=0.8\linewidth]{variance.png}
  \vspace{-0.4cm}
  \caption{The boxplot of candidate sub-network performance with four \#Params constraints. The mean value is shown in the orange line. We selectively apply ED on certain blocks, indicated with ``X".}
  \label{fig:variance}
\end{figure}


\vspace{-0.3cm}
\begin{table}[t]
\centering
\begin{tabular}{cccc|c}
\toprule
\#Params(M) & OFA   & +10K & +100K & Init+100K\\ \midrule
0.9         & 26.6 &  26.7  & 26.7 & 24.8 \\
1.8         & 27.2 &  27.6  & 27.8 & 26.6 \\
2.7         & 27.7 &  28.2  & 28.4 & 28.1 \\
3.6         & 27.9 &  28.3  & 28.5 & 28.3 \\ \bottomrule
\end{tabular}
\vspace{-0.2cm}
\caption{The results of continual fine-tuning for 10K or 100K iterations on the OFA searched sub-networks. The OFA method here is EW+ED=[2,2,X]. ``Init" means we used the identical sub-network architecture found by OFA, and trained with knowledge distillation.}
\label{tab:CF}
\end{table}

\subsection{Performance variance of sub-networks}
We measure the performance variance of sub-networks with similar model sizes. We include different search spaces of supernets for analysis. The result is shown in Figure \ref{fig:variance}. We observe that if using elastic depth in all blocks (green boxes), the average performance is the worst, and the standard deviation is the largest across four \#Params constraints. The large variance implies the instability of supernet training and searching. In contrast, only using elastic depth in the last block (red boxes) achieves the best average performance with low variance, showing that the supernet converges well and is robust for searching. Lastly, though EW-only supernet achieves good performance (see Figure ~\ref{fig:ablation_ED}), the performance variance among different sub-network is large, especially at 1.8M \#Params constraints. 

\subsection{Continual fine-tuning}
To evaluate whether the searched sub-networks are well-trained, we continually fine-tune (with knowledge distillation) the searched sub-networks for 10K and 100K iterations. To better understand OFA from the perspective of optimization, we also train the same architectures of the searched sub-networks from scratch using knowledge distillation and show the results in column ``Init+100K".

According to the results shown in Table \ref{tab:CF}, continually fine-tuning the OFA-identified sub-networks generally helps improve the performance except for the models at 0.9M \#Params (we only observed 0.1 mAP improvement after fine-tuning). In the meantime, the OFA-trained model significantly outperforms the KD-trained counterpart at 0.9M \#Params. These observations suggest that the ultra-small sub-networks are already well-trained. When looking at models at 3.6M \#Params, the OFA-trained sub-networks only achieve on-par performance compared with KD-trained individual models, which aligns with Table ~\ref{tab:performance}. However, continual training does improve the sub-networks at 3.6M \#Params by 0.6 mAP, and finally outperform the KD counterpart.

This analysis shows that the weight-sharing supernet training strategy provides a significant advantage in terms of optimization over training a single model either from scratch or via KD. For ultra-small models, OFA-trained sub-networks can already significantly outperform KD-trained counterparts; For larger model size, OFA-trained sub-networks weights can serve as good initialization and achieves superior (compared with KD) performance after continual fine-tuning. Thus, the main benefit of OFA comes not only from searching for better architecture but also from optimization.

%On the other hand, continual training improves the middle and large sub-networks by around 0.6 mAP. According to Table ~\ref{tab:performance}, OFA-trained sub-networks only achieve on-par performance compared with KD-trained individual models.
%These findings aligned with Table ~\ref{tab:performance}, where OFA-trained sub-networks clearly outperform KD-trained models when \#Params are around 1M; However, for large models, OFA-trained sub-networks only achieve on-par performance compared with KD-trained individual models. 
%For the small sub-network (0.9M \#Params), we only observed 0.1 mAP improvement after fine-tuning; In the meantime, the OFA-trained model significantly outperforms the KD-trained counterpart. This suggests that the small sub-network is already well-trained. 
%Comparing with the column ``Init+100K", we observe that 
%We observe that the weight-sharing supernet training helps weights optimize to outstanding performance for small models (when \#Params is 0.9M and 1.8M). 
%By just inheriting weights from the supernet, the sub-network significantly outperforms the results of knowledge distillation while being competitive in large model sizes. 
%Table \ref{tab:CF} shows knowledge distillation with the searched architecture in column ``Init+100K". 
%That is, the weights of the supernet are discarded and re-initialized.
%presumably because large sub-networks have not been 
%We hypothesize that because the weights of smaller sub-networks are shared for large sub-networks, after OFA training, the smaller sub-networks are more likely to be well-trained.
% \qingming{Are we trying to say, the weights of smaller sub-networks are kind of multi-task trained? In its own weights, and in larger sub-networks?}

\vspace{-0.3cm}
\section{Conclusion}
Efficiently training models satisfying diverse on-device computation resource constraints becomes an emerging topic in the industry as edge computing is gaining popularity. In this work, we apply the once-for-all framework to address this issue for acoustic event classification. To the best of our knowledge, we propose the first weight-sharing NAS method for CNN-based AEC models. 
In addition to the saving in model training efforts brought by the once-for-all training, the performance of searched sub-networks across different computation resource constraints is superior to models trained from scratch and models trained with knowledge distillation. 
Moreover, we analyze the search space design for the OFA AEC model and find that the benefit of weight-sharing supernet training of ultra-small models does not only come from searching, but to a large extent, comes from optimization.
In the future, we plan to explore other AEC model architectures for supernet and extend our framework to other audio applications. 


% Efficient model deployment under device constraints is an emerging challenge in the industry. In this work, we apply the once-for-all framework to address this issue for acoustic event classification. To the best of our knowledge, we propose the first weight-sharing NAS method for CNN-based AEC models, demonstrating the great performance of the searched sub-networks across different computational constraints. Moreover, we analyze the search space design for the OFA AEC model and find out how to apply elastic depth is crucial for achieving good performance. In the future, we plan to explore more kinds of AEC model architecture for supernet and extend our framework to other audio-related tasks. 

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEE}
\bibliography{refs}

\end{document}
