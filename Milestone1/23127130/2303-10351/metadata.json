{
    "arxiv_id": "2303.10351",
    "paper_title": "Weight-sharing Supernet for Searching Specialized Acoustic Event Classification Networks Across Device Constraints",
    "authors": [
        "Guan-Ting Lin",
        "Qingming Tang",
        "Chieh-Chi Kao",
        "Viktor Rozgic",
        "Chao Wang"
    ],
    "submission_date": "2023-03-18",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.SD",
        "eess.AS"
    ],
    "abstract": "Acoustic Event Classification (AEC) has been widely used in devices such as smart speakers and mobile phones for home safety or accessibility support. As AEC models run on more and more devices with diverse computation resource constraints, it became increasingly expensive to develop models that are tuned to achieve optimal accuracy/computation trade-off for each given computation resource constraint. In this paper, we introduce a Once-For-All (OFA) Neural Architecture Search (NAS) framework for AEC. Specifically, we first train a weight-sharing supernet that supports different model architectures, followed by automatically searching for a model given specific computational resource constraints. Our experimental results showed that by just training once, the resulting model from NAS significantly outperforms both models trained individually from scratch and knowledge distillation (25.4% and 7.3% relative improvement). We also found that the benefit of weight-sharing supernet training of ultra-small models comes not only from searching but from optimization.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10351v1"
    ],
    "publication_venue": "Accepted by ICASSP 2023"
}