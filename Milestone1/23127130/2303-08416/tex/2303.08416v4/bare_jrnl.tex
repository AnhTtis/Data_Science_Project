


\documentclass[journal]{IEEEtran}

\usepackage{times}
\renewcommand*\ttdefault{txtt}
\usepackage{soul}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{amsfonts,amssymb}
\usepackage{multirow}
\usepackage{color}
\usepackage{cite}
\usepackage{pifont}
\usepackage{array}
\usepackage[misc]{ifsym} 

\usepackage{makecell}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Lung Nodule Segmentation and Low-Confidence Region Prediction with an Uncertainty-Aware Attention Mechanism}
% \author{
% }
\author{
  Han~Yang, Qiuli~Wang, Yue~Zhang, Zhulin An, Liu~Chen, Xiaohong~Zhang, and S.~Kevin Zhou
  \thanks{Corresponding author: Qiuli~Wang and S.~Kevin Zhou. }
  \thanks{Q.~Wang, Y.~Zhang are with 
  School of Biomedical Engineering, Division of Life Sciences and Medicine,
  University of Science and Technology of China, Hefei, Anhui, 230026, China
  and
  Center for Medical Imaging, Robotics, Analytic Computing \& Learning (MIRACLE),
  Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu, 215123, China.
  E-mail: $\{$wangqiuli, yue\_zhang$\}$@ustc.edu.cn
  }
  \thanks{H.~Yang is with the Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China and also with University of Chinese Academy of Sciences, Beijing 100049, China . E-mail: yanghan223@mails.ucas.ac.cn.}
    \thanks{Z.~An is with the Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China. E-mail: anzhulin@ict.ac.cn.}
  \thanks{C.~Liu is with the Radiology Department, The First Affiliated Hospital of Army Medical University, Chongqing 400032, China. E-mail: liuchen@aifmri.com.}
  \thanks{X.~Zhang is with School of Big Data \& Software Engineering, Chongqing University, Chongqing 401331, China. E-mail: xhongz@cqu.edu.cn.}
  \thanks{S.K. Zhou is with School of Biomedical Engineering, Division of Life Sciences and Medicine,
  University of Science and Technology of China, Hefei, Anhui, 230026, China
  and
  Center for Medical Imaging, Robotics, Analytic Computing \& Learning (MIRACLE),
  Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu, 215123, China
  and also with the Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China. E-mail: skevinzhou@ustc.edu.cn.}
  \thanks{H.~Yang and Q.~Wang contributed equally to this paper. }

}

\maketitle
\begin{abstract}
Radiologists have different training and clinical experiences, which may result in various segmentation annotations for lung nodules, causing segmentation uncertainty. Conventional methods usually select a single annotation as the learning target or try to learn a latent space of various annotations, but these approaches waste the valuable information of consensus or disagreements ingrained in the multiple annotations. 
In this paper, we propose an Uncertainty-Aware Attention Mechanism (UAAM) that utilizes consensus and disagreements among multiple annotations to facilitate better segmentation.
To achieve this, we introduce the Multi-Confidence Mask (MCM), which is a combination of a Low-Confidence (LC) Mask and a High-Confidence (HC) Mask. The LC mask indicates regions with a low segmentation confidence, which may cause different segmentation options among radiologists. Following UAAM, we further design an Uncertainty-Guide Segmentation Network (UGS-Net), which contains three modules: a \emph{Feature Extracting Module} that captures a general feature of a lung nodule, an \emph{Uncertainty-Aware Module} that produces three features for the annotations' union, intersection, and annotation set, and an \emph{Intersection-Union Constraining Module} that uses distances between the three features to balance the predictions of final segmentation, LC mask, and HC mask.
To fully demonstrate the performance of our method, we propose a Complex Nodule Validation on LIDC-IDRI, which tests UGS-Net's segmentation performance on lung nodules that are difficult to segment using U-Net. Experimental results demonstrate that our method can significantly improve the segmentation performance on nodules with poor segmentation by U-Net.

\end{abstract}
\begin{IEEEkeywords}
Lung Nodules Segmentation, Uncertainty, Multiple Annotations, Computed Tomography.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{L}{ung} nodule segmentation is crucial in Computer-Aided Diagnosis (CAD) systems for lung cancer \cite{wang2017central}, providing critical information such as nodule sizes, shapes, and other important medical features.
However, for the general train-and-test paradigm of deep learning methods, each nodule image data has only one annotation mask delineated by one radiologist \cite{pezzano2021cole, shariaty2022texture, agnes2022efficient, oktay2018attention,ronneberger2015u,zhang2021deeprecs}. Thus, each time, the network can only provide a single prediction of nodule regions.


\begin{figure}[htbp]
  \centerline{\includegraphics[width=70mm]{overview.pdf}}
  \vspace{-0.0cm}
  \caption{(A). Uncertainty caused by multiple annotations. HC is High-Confidence mask, which is the intersection of annotation set. LC is Low-Confidence mask, which is the different between annotation set's union and intersection. Multi-Confidence Mask is the combination of LC and HC.
  (B). Hounsfield Unit Kernel Estimations in HC and LC of LIDC-IDRI.}
  \vspace{-0.0cm}
  \label{MCM}
\end{figure}

However, in clinical practice, different radiologists may provide various segmentation annotations for a lung nodule due to their unique training and clinical experience \cite{hu2019supervised, kohl2019hierarchical, xiaojiang2021}. As a result, the traditional methods based on single annotation cannot reflect the diversity of clinical experiences and limit the application of deep learning methods.

A straightforward solution to the problem of varying annotations among radiologists is to incorporate multiple annotations for each lung nodule image.
That leads to another issue: multiple annotations inevitably bring in uncertainty and conflicts, as different radiologists may annotate the same regions differently.
To overcome this problem, Kohl \textit{et al.} proposed a probabilistic U-Net in 2018, which utilized a conditional variational auto-encoder to encode multiple segmentation variants into a low-dimensional latent space \cite{kohl2018probabilistic, kohl2019hierarchical}. 
By sampling from this space, the network could affect the corresponding segmentation map.
Based on this study, Hu \textit{et al.} proposed to combine the ground-truth uncertainty with a probabilistic U-Net, which could improve predictive uncertainty estimates, sample accuracy, and sample diversity \cite{hu2019supervised}.
These methods relied on the latent space and random samples in this space. Consequently, these methods could only produce the probabilistic results of uncertainty distribution and cannot provide a stable uncertainty prediction.

In this paper, we present an argument that \textbf{the uncertainty between multiple annotations follows a particular pattern}. To demonstrate this phenomenon, we introduce the Multi-Confidence Mask (MCM), which combines a High-Confidence (HC) mask and a Low-Confidence (LC) mask, as illustrated in Figure~\ref*{MCM}. A. The HC mask is the intersection of all annotations, while the LC mask is the difference between the intersection and the union. We then calculate the Hounsfield Unit (HU) Kernel Estimations of HC and LC on the LIDC-IDRI dataset \cite{armato2011lung}. As shown in Figure~\ref*{MCM}. B, the HU distribution of the LC mask is significantly different from that of the HC mask. Specifically, the LC regions have lower HU values than the HC regions.
\textcolor{black}{From the pixel distribution perspective, the lower the HU value, the lower the density of the region. From the image features perspective, the LC regions are mainly some boundary-related features, such as nodule edges, spiculation, and ground-glass features, while the HC region is on the contrary, mainly distributed in the nodule core. \textbf{Therefore, we hypothesize that the areas that cause disagreement among radiologists are mainly associated with low-density tissue and boundary-related features.}}


\begin{figure}[htbp]
  \centerline{\includegraphics[width=80mm]{overview_UAAM.pdf}}
  \vspace{-0.0cm}
  \caption{Overview of \emph{Uncertainty-Aware Attention Mechanism}. The union mask guides the learning of low-certainty features, while the intersection mask guides the learning of high-certainty features. The difference between the union and intersection masks represents regions with high segmentation uncertainty. Moreover, the annotation set guides the learning of a plausible segmentation, which is a balance between all annotations.}
  \vspace{-0.0cm}
  \label{UAM}
\end{figure}

Different from traditional methods, we propose to utilize the MCM as the learning guidance of low and high-confidence features, contributing to a better segmentation performance. We call this training \emph{Uncertainty-Aware Attention Mechanism} (UAAM) hereafter, which is illustrated in Figure~\ref*{UAM}. Following this mechanism, we further design an Uncertainty-Guide Segmentation Network (UGS-Net) for lung nodule segmentation, which contains a U-Net-based \emph{Feature Extracting Module}, an \emph{Uncertainty-Aware Module}, and an \emph{Intersection-Union Constraining Module}.

Firstly, the \emph{Feature Extracting Module} extracts general feature maps from the input CT image.
Secondly, the \emph{Uncertainty-Aware Module} transfers the general feature maps into three independent feature maps $R_{LC}$, $R_{HC}$ and $R_{Uni}$ with the guidance of annotations' union, intersection, and the annotation set. 
\textcolor{black}{$R_{LC}$ and $R_{HC}$ are used to predict the union $\cup(GT)'$ and intersection $\cap(GT)'$, and the results are combined as MCM. $R_{Uni}$ is used to predict $GT'_{Uni}$. }
Thirdly, the \emph{Intersection-Union Constraining Module} captures preferred features with Feature-Aware Attention Blocks from $R_{LC}$, $R_{HC}$ and $R_{Uni}$, then constrains the final prediction $X_{S}$ with feature distances, achieving a balance between extreme annotations while ensuring the predicted LC regions in MCM are constrained in a moderate and reasonable way. To better utilize multiple annotations, we also introduce a Multiple Annotation Fusion Loss to optimize \textcolor{black}{the $GT'_{Uni}$ and $X_{S}$}, which calculates the average BCE loss between prediction and all annotations.

The proposed method has two distinct advantages:
(1) Instead of learning from a latent space, the proposed method has specific learning targets, enabling it to provide a stable prediction of uncertain nodule regions.
(2) The method optimizes the prediction with all annotations to ensure that the final prediction balances different conditions, making the most of the available information.

We have reported a preliminary version of this work in a previous publication \cite{yang2022uncertainty}. 
The new contributions of this paper can be summarized as follows:  
\begin{itemize}
  \item We introduce an \emph{Uncertainty-Aware Attention Mechanism} (UAAM), which utilizes the MCM as the learning guidance of low and high-confidence features, contributing to a better segmentation performance. The UAAM can provide a stable prediction for uncertain regions as well as a single lung nodule segmentation.
  \item Based on the mechanism, we propose a new Uncertainty-Guide Segmentation Network (UGS-Net), which contains a Feature Extracting Module, an \emph{Uncertainty-Aware Module} (UAM), and an \emph{Intersection-Union Constraining Module}. To make the most of multiple annotations, we also introduce a Multiple Annotation Fusion Loss, comparing the segmentation with all possible annotations.
  \item We propose a Complex-Nodule Validation, which tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net. 
  \textcolor{black}{Experiments demonstrate that for nodules whose DSC score is lower than 60$\%$ on U-Net, the DSC score of our network can be increased by 11.03$\%$, and the IoU score of our network can be improved by 11.88$\%$.}
\end{itemize}

\section{Related Work}
\label{relatedwork}
\subsection{Lung Nodule Segmentation}
Lung nodule segmentation is a key task in lung nodule Computer-Aided Diagnosis (CAD) systems.
Lung nodule segmentation is a critical task in lung nodule computer-aided detection (CAD) systems. Its primary goal is to accurately delineate the boundary of a target nodule to provide details such as its diameter, size, and semantic features \cite{gonccalves2016hessian, wu2010stratified, pezzano2021cole}.
The main challenge of this task is that lung nodules have various shapes, sizes, and delicate features. In the early years, researchers provided numerous methods for lung nodule segmentation, such as morphology-based methods and region-growing-based methods \cite{diciotti2011automated,dehmeshki2008segmentation}. Recently, deep learning has become the most popular method in this area.

In 2017, Wang \emph{et al.} proposed a multi-view convolutional network for lung nodule segmentation.
The proposed network captured a diverse set of nodule-sensitive features from axial, coronal, and sagittal views in CT images simultaneously. 
These features were analyzed with a multi-branch CNN network, achieving an average DSC similarity coefficient (DSC) of 77.67\% \cite{wang2017multi}. 
Also, in 2017, Wang \emph{et al.} proposed a central focused convolutional neural network with central-pooling layers to analyze nodules in 2D and 3D thoroughly \cite{wang2017central}. In 2020, Cao \emph{et al.} designed a dual-branch residual network with intensity-pooling layers, which enhanced the learning of intensity information and improved the DSC to 82.74\% \cite{cao2020dual}. In 2021, Pezzano \emph{et al.} introduced a CNN network that could learn the context of the nodules by producing two masks representing all the background and secondary-important elements in the CT, so that the network can better discriminate nodule features \cite{pezzano2021cole}.
Later in 2022, Shariaty \emph{et al.} further proposed texture feature extraction and feature selection algorithms to improve the segmentation, achieving a DSC of 84.75\% \cite{shariaty2022texture}.

As seen from the studies above, it is clear that existing methods have primarily focused on achieving more precise segmentation, with little consideration given to the fact that different radiologists may have varying opinions on how to segment the same lung nodule. In this study, we argue that disagreements among annotations are also valuable for the diagnosis. We propose to learn the uncertainty among annotation sets, and produce a segmentation that can balance all annotations.

\subsection{Uncertainty in Lung Nodule Segmentation}
Many medical image vision problems suffer from ambiguities. In clinical situations, it may not be clear from a CT scan alone which particular region is cancer tissue \cite{kohl2018probabilistic}. As a result, even experienced doctors and radiologists may provide different segmentations for the same tissues or tumor. 

In 2018, Kohl \emph{et al.} proposed to model this task as the learning of distribution over diverse but plausible segmentations for lung nodule. Based on U-Net \cite{ronneberger2015u}, they introduced a probabilistic U-Net, which was a combination of a U-Net and a conditional VAE that could produce an unlimited number of plausible segmentations. Later in 2019, Kohl \emph{et al.} further proposed a hierarchical probabilistic U-Net, which used a hierarchical latent space decomposition to formulate the sampling and reconstruction of segmentations with high fidelity \cite{kohl2019hierarchical}. Also in 2019, Hu \emph{et al.} analyzed two types of uncertainty: aleatoric and epistemic \cite{hu2019supervised}. They exploited multiple annotations' variability as a source of `ground truth' aleatoric uncertainty, combined this uncertainty with probabilistic U-Net, and tried to quantitatively analyze the segmentation uncertainty. In 2021, Long \emph{et al.} extended the concept in \cite{hu2019supervised} to the V-Net and 3D lung nodule CT images. As an ideal dataset, which contained multiple annotations for over 1000 lung nodules, all these studies \cite{kohl2018probabilistic,kohl2019hierarchical,hu2019supervised,xiaojiang2021} analyzed LIDC-IDRI.

Unlike previous studies, our method focuses on the uncertainty present in lung nodule segmentation with multiple annotations. Different from the networks based on VAE, our work pays more attention to the reason that causes the various annotations, and introduces an alternative method that specifically targets uncertainty regions, enabling us to make stable predictions for both uncertain nodule regions and overall lung nodule segmentation.

\section{Methods}
\label{method}
\subsection{Uncertainty-Guided Segmentation Network}
\begin{figure*}[htbp]
  \centerline{\includegraphics[width=160mm]{arch.pdf}}
  \vspace{-0.0cm}
  \caption{
  Overview of Uncertainty-Guided Segmentation Network. This network contains three modules:
  (1) \emph{Feature Extracting Module}, (2) \emph{Uncertainty-Aware Module}, and (3) \emph{Intersection-Union Constraining Module}.
  }
  \vspace{-0.0cm}
  \label{arch}
\end{figure*}
  
In Figure~\ref{arch}, we present the architecture of the Uncertainty-Guided Segmentation Network (UGS-Net). 
The network takes the lung nodule CT image $X$ as inputs, and produces two outputs: a predicted Multi-Confidence Mask (MCM) and a final segmentation $X_{S}$. The learning targets of the network are the annotation set $GT$, as well as their union $\cup(GT)$ and intersection $\cap(GT)$. The input images and their masks are both $50\times50$, which are croped from the LIDC-IDRI dataset based on the official annotations.

The UGS-Net contains three modules:
(1) \emph{Feature Extracting Module}, (2) \emph{Uncertainty-Aware Module}, and (3) \emph{Intersection-Union Constraining Module}.
\textcolor{black}{The \emph{Feature Extracting Module} can use any segmentation network based on U-Net structure to initially obtain a feature map $R$ with 64 channels.}
This paper uses Attention U-Net \cite{oktay2018attention} here, it has five down-sampling and up-sampling layers. Each up-sampling layer is composed of two convolutional layers and an attention block.
The \emph{Uncertainty-Aware Module} (UAM) analyzes $R$ and generates $R_{LC}$, $R_{HC}$, and $R_{Uni}$. These feature maps are then fed into three convolutional blocks, which produce the initial $\cup(GT)'$, $\cap(GT)'$, and a plausible segmentation $GT_{Uni}'$. The union $\cup(GT)'$ and intersection $\cap(GT)'$ are computed to obtain the Multi-Confidence Mask (MCM).
\emph{Intersection-Union Constraining Module} (SCM) learns the different characteristics of $R_{LC}$, $R_{HC}$, and $R_{Uni}$, helps to refine the MCM, and then provides a more reasonable final segmentation $X_{S}$ by fusing all information.


\subsection{Uncertainty-Aware Module}
\label{UAM_method}
The key to further improve the performance of network segmentation is making the best use of valuable uncertain information in the annotation set. The \emph{Uncertainty-Aware Module} is introduced to make full and reasonable use of all annotation information by learning $\cup(GT)$, $\cap(GT)$, and $GT$. This module has two tasks: (1) capturing different features from Low-Confidence (LC) regions, High-Confidence (HC) regions, and all annotations; (2) producing initial predictions of the Multi-Confidence Mask (MCM) and a general segmentation.


As shown in Figure~\ref{arch}, UAM adopts a three-branch CNN network as its backbone.
It takes $R$ as input and extracts $R_{LC}$, $R_{HC}$, and $R_{Uni}$ using three different convolution blocks. 
The MCM BCE Loss block receives $R_{LC}$ and $R_{HC}$, which are used to generate $\cup(GT)'$ and $\cap(GT)'$ through two parallel convolutional layers. 
The BCE loss is calculated between $\{\cup(GT)', \cup(GT)\}$ and $\{\cap(GT)', \cap(GT)\}$. The $\cup(GT)'$ and $\cap(GT)'$ are combined as $MCM'$ through the normalization operation $Normal(\cup(GT)'+\cap(GT))$, which reflects the degree of uncertainty in different regions. 
Unlike our previous work \cite{yang2022uncertainty}, the branch for $R_{uni}$ is optimized by the Multiple Annotation Loss block, which will be discussed later. 
Additionally, feature maps $R_{LC}$, $R_{HC}$, and $R_{Uni}$ with different latent visual features will be fed into the next module for further analysis.

\subsection{Intersection-Union Constraining Module}
\label{SCM}
As mentioned above, $\cup(GT)$ and $\cap(GT)$ are the learning targets of UAM. Specifically, $\cup(GT)$ indicates all regions that might be nodule tissues, and $\cap(GT)$ indicates nodule regions with highest confidence. To achieve a balance between extreme situations, we further develop a new module called the \emph{Intersection-Union Constraining Module} (IUCM). This module is designed to capture the features of all three learning targets, and produce a more reasonable segmentation prediction that can strike a balance between extreme situations.


\begin{figure}[htbp]
  \centerline{\includegraphics[width=80mm]{IUCM.pdf}}
  \vspace{-0.0cm}
  \caption{
    Intersection-Union Constraining Module. It contains three Feature-Aware Attention Blocks, and each block has specific feature learning preferences.
    }
  \vspace{-0.0cm}
  \label{UAM_F}
\end{figure}

As shown in Figure~\ref*{UAM_F}, IUCM takes $R_{LC}$, $R_{HC}$, and $R_{Uni}$ as its inputs, and generals corresponding $R'_{LC}$, $R'_{HC}$, and $R'_{Uni}$ with Feature-Aware Attention Block (FAAB). 
FAAB is built based on a self-attention block \cite{vaswani2017attention} and a feature-aware filter.
These attention blocks process $R_{LC}$, $R_{HC}$, and $R_{Uni}$ with different feature-aware filters, which enable the network to formulate different learning preferences for different learning objectives, to obtain more image features that are helpful for segmentation.
More specifically, assuming the input is $R_z$, the process of FAAB can be summarized as:

\begin{equation}
  \begin{aligned}
    R'_z=R_z\oplus \Gamma(A(R_z))
  \end{aligned}
\end{equation}
where $z\in\{Uni,LC,HC\}$, $A$ indicates the self-attention architecture. $\Gamma$ is a feature-aware filter, in this study, the $\Gamma$ for $R_{uni}$ and $R_{LC}$ is Gabor \cite{luan2018gabor}, the $\Gamma$ for $R_{HC}$ is Otsu \cite{2007A}. 
\textcolor{black}{To keep more information from the inputs, the $\Gamma(A(R_z)$ is concatenated with $R_z$.}
The process of self-attention architecture please refer to the study \cite{vaswani2017attention}. 

After getting $R'_{LC}$, $R'_{HC}$, and $R'_{Uni}$, the IUCM gets $S_z=d\{R'_z,R\}$, $d$ is feature $L2$ distance. 
The output of IUCM is $R_{Aug}=Concat(S_{Uni}\times R'_{Uni},S_{LC}\times R'_{LC},S_{HC}\times R'_{HC})$. $R_{Aug}$ will be concatenated with $R$ from the Feature Extracting Module, fed into a convolutional layer, and produce the final segmentation prediction $X_{S}$. The concatenation of $R$ and $R_{Aug}$ can also keep more information from $X$, preventing losing too many original features.

\subsection{Loss Function}
\label{Loss}
In Figure~\ref*{arch}, the loss function of UGS-Net contains two pars: MCM BCE Loss block and Multiple Annotation Loss block.

MCM BCE Loss block calculates BCE loss between $\{\cup(GT)', \cup(GT)\}$ and $\{\cap(GT)', \cap(GT)\}$, which can be represented as:
\begin{equation}
  \begin{aligned}
    L_{MCM}=BCE(\cup(GT)', \cup(GT))+\\BCE(\cap(GT)', \cap(GT))
  \end{aligned}
\end{equation}



We use the \emph{Multiple Annotation Fusion Loss} to optimize $GT_{uni}'$ and $X_{S}$ in the Multiple Annotation Loss block, denoted as $\Phi$. In our previous work, only one annotation from the set was selected for optimizing $GT_{Uni}'$ and $X_{S}$, which resulted in a loss of valuable information from other annotations. In this study, we introduce the Multiple Annotation Fusion Loss, which compares the prediction with all possible annotations.
\textcolor{black}{As illustrated in Figure~\ref*{maloss}, we calculate the BCE losses between the optimized object and each annotation in the set, and incorporate this average losses as a component of the loss fusion. This approach enables us to better utilize the information from all annotations in the set.}
We have:
\begin{equation}
  \begin{aligned}
    \Phi_a=E\{\Sigma_{j}BCE (GT_{Uni}',GT_j) \} \\
    \Phi_b=E\{\Sigma_{j}BCE (X_{S},GT_j) \}
  \end{aligned}
\end{equation}
where $GT_j\in GT$. This fusion encodes all information in annotation sets to better optimize the final prediction.
The loss fusion of the network can be defined as:
\begin{equation}
  \begin{aligned}
    L=\alpha_{1}L_{MCM}+\alpha_2\Phi_a+\alpha_3\Phi_b
  \end{aligned}
\end{equation}
\textcolor{black}{where $\alpha_1$, $\alpha_2$, and $\alpha_3$ are User-defined parameters. 
Empirically, $\alpha_1$ is set to 0.5, $\alpha_2$ to 0.5, and $\alpha_3$ to 1 in this paper.}


\begin{figure}[htbp]
  \centerline{\includegraphics[width=40mm]{multipleloss.pdf}}
  \vspace{-0.0cm}
  \caption{
    Multiple Annotation Fusion Loss.
  }
  \vspace{-0.0cm}
  \label{maloss}
\end{figure}


\section{Experiments}
\label{experiments}
\subsection{Dataset and Experimental Settings}

We evaluate our proposed Uncertainty-Guided Segmentation Network (UGS-Net) using the LIDC-IDRI dataset \cite{armato2011lung}, which consists of 1018 study instances and over 2600 nodules. Each instance includes medical image files annotated in two stages by experienced chest radiologists. In the first stage, each radiologist independently provided a diagnosis of benign and malignant nodules, a diagnosis of semantic features, and delineation of the nodules' lesion region. For this study, we selected 1860 lung nodules with a diameter of 3-30mm and multiple annotations ($\geq2$). For each selected nodule, we obtained its CT image, multiple annotation set $GT$, their union $\cup(GT)$, and intersection $\cap(GT)$. 

We conduct our experiments on a server running Ubuntu 18.04 with a Tesla A100 GPU, using CUDA 11.2. We employ Attention U-Net \cite{oktay2018attention} as the backbone network, implemented with PyTorch-v1.0.1 and Python3.7. We use five-fold validation to evaluate the network's effectiveness, ensuring robustness to data splits. We use Stochastic Gradient Descent with Warm Restarts (SGDR) as the optimizer, with an initial learning rate (LR) of 0.00001, a batch size of 32, a momentum of 0.9, and a weight decay rate of 0.0001. Each network is trained for 200 epochs, with the learning rate updated every 50 epochs.


Three metrics are used to evaluate the predictive ability of the network to the lesion regions in this study: the average Dice Similarity Coefficient (DSC), Intersection over Union (IoU), and Normalized Surface Dice (NSD). The NSD metric is particularly useful for assessing how closely the $mask$ and $prediction$ $result$ align with each other \cite{ma2021abdomenct}. For more detailed information about these metrics, please refer to \cite{yang2022uncertainty}.

\subsection{Performance of Lung Nodule Segmentation}
\label{performanceana}
\begin{table*}[h]\tiny
  \setlength{\tabcolsep}{5pt}
  \renewcommand\arraystretch{1.1}
  \vspace{-0.0cm}
  \caption{Performance comparison between our UGS-Net and eight networks based on the U-Net structure on the LIDC-IDRI dataset.
  UGS-Net-V1 represents a preliminary version of this work in a \cite{yang2022uncertainty}.}
  \centering
  \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}		
  \hline
  \hline
  \multirow{2}*{\textbf{\emph{Method}}} & \multicolumn{3}{c}{\textbf{\emph{Fold1}}} & \multicolumn{3}{c}{\textbf{\emph{Fold2}}} & \multicolumn{3}{c}{\textbf{\emph{Fold3}}} & \multicolumn{3}{c}{\textbf{\emph{Fold4}}} & \multicolumn{3}{c}{\textbf{\emph{Fold5}}} & \multicolumn{3}{c}{\textbf{\emph{Average}}}\\
  \cline{2-19} 
  ~ & \multicolumn{1}{c}{\textbf{\emph{DSC}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{DSC}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{DSC}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{DSC}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{DSC}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{DSC}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} \\
  \hline
  \makecell[l]{FCN32s \cite{DBLP:journals/corr/LongSD14}} & 83.92 & 73.51 & 92.63 & 84.80 & 74.75 & 93.64 & 85.37 & 75.35 & 94.81 & 84.54 & 74.44 & 94.21 & 85.74 & 75.78 & 94.80 & 84.87 $\pm$ 0.64 & 74.77 $\pm$ 0.78 & 94.02 $\pm$ 0.82 \\
  \makecell[l]{U-Net \cite{DBLP:journals/corr/RonnebergerFB15}}  & 84.98 & 75.18 & 93.12 & 86.34 & 76.89 & 93.67 & 87.26 & 78.21 & 95.50 &86.20 & 76.67 & 94.83 & 86.52 & 77.02& 95.17 & 86.26 $\pm$ 0.74 & 76.79 $\pm$ 0.97 & 94.46 $\pm$ 0.91 \\
  \makecell[l]{R2U-Net \cite{DBLP:journals/corr/abs-1802-06955}} & 83.33 & 72.92 & 92.10 &85.66 & 76.02 & 94.17 &86.44 & 77.01 & 95.23 & 84.83 & 74.86 & 93.38 & 85.61 & 75.66 & 95.26 & 85.17 $\pm$ 1.05 & 75.29 $\pm$ 1.37 & 94.03 $\pm$ 1.19 \\
  \makecell[l]{R2AU-Net \cite{R2AU-Net}} & 82.99 & 72.36 & 91.50 & 83.84 & 73.43 & 92.92 &86.13 & 76.50 & 94.58 & 85.10 & 75.24 &94.24 &85.41 & 75.53 & 95.03 & 84.69 $\pm$ 1.13 & 74.61 $\pm$ 1.50 & 93.65 $\pm$ 1.27 \\
  \makecell[l]{Attention U-Net \cite{DBLP:journals/corr/abs-1804-03999}} &85.99 & 76.44 & 93.98 & 86.11 & 76.58 & 93.68 & 87.76 & 78.89 & 96.07 & 86.50 & 77.13 & 95.42 & 86.99 & 77.61 & 95.56 & 86.67 $\pm$ 0.65 & 77.33 $\pm$ 0.88 & 94.94 $\pm$ 0.94 \\
  \makecell[l]{Nested U-Net \cite{DBLP:journals/corr/abs-1807-10165}} & 84.72 & 74.70 & 93.12 & 85.40 & 75.61 & 94.39 & 86.06 & 76.35 & 95.57 & 85.50 & 75.54 & 95.16 & 86.31 & 76.62 & 95.90 & 85.60 $\pm$ 0.55 & 75.76 $\pm$ 0.68 & 94.83 $\pm$ 0.99 \\
  \makecell[l]{Channel U-Net \cite{10.3389/fgene.2019.01110}} & 85.28 & 75.44 & 94.15 & 86.02 & 76.58 & 93.22 & 86.93 & 77.62 & 95.94 & 86.25 & 76.82 & 95.36 & 86.56 & 76.99 & 95.46 & 86.21 $\pm$ 0.56 & 76.69 $\pm$ 0.71 & 94.82 $\pm$ 1.00 \\
  \makecell[l]{UGS-Net-V1 \cite{yang2022uncertainty}} & 86.47 & 77.07 & 93.58 & 87.21 & 78.17 & 95.26 & 88.39 & 79.86 & 96.20 & 86.56 & 77.17 & 95.56 & 87.19 & 77.94 & 95.78 & 87.16 $\pm$ 0.69 & 78.04 $\pm$ 1.00 & 95.28 $\pm$ 0.90 \\ 
  \hline
  \makecell[l]{UGS-Net} & 87.49 & 78.43 & 94.67 & 87.72 & 78.86 & 95.43 & 88.66 & 80.31 & 96.50 & 86.93 & 77.86 & 95.64 & 87.44 & 78.44 & 95.84 & \textbf{87.65 $\pm$ 0.57}\textcolor{green}{$\uparrow$}& \textbf{78.78 $\pm$ 0.83}\textcolor{green}{$\uparrow$} & \textbf{95.62 $\pm$ 0.59}\textcolor{green}{$\uparrow$} \\ 
  \hline
  \hline
  \end{tabular} 
  \label{tabel1}
  \vspace{-0.1cm}
\end{table*}

We compare the proposed UGS-Net with eight commonly used segmentation networks on traditional lung nodule segmentation. These networks include FCN32s \cite{DBLP:journals/corr/LongSD14}, U-Net \cite{DBLP:journals/corr/RonnebergerFB15}, R2U-Net \cite{DBLP:journals/corr/abs-1802-06955}, R2AU-Net \cite{R2AU-Net}, Attention U-Net \cite{DBLP:journals/corr/abs-1804-03999}, Nested U-Net \cite{DBLP:journals/corr/abs-1807-10165}, Channel U-Net \cite{10.3389/fgene.2019.01110}, and UGS-Net-V1 \cite{yang2022uncertainty}.
We have trained UGS-Net-V1 and UGS-Net with multiple annotations, while the other networks have been trained using the traditional strategy of having only one correlated annotation $Label_1$ for each nodule, which is the first in the annotation set.
To evaluate the performance of all methods, we use the same strategy, which involves comparing the final prediction with $Label_1$, and using three metrics: the average dice similarity coefficient (DSC), intersection over union (IoU), and normalized surface dice (NSD). For more information on these metrics, please refer to \cite{yang2022uncertainty}.


Table \ref{tabel1} demonstrates that UGS-Net achieves the highest scores on DSC, IoU, and NSD, with scores of 87.65$\%$ ($\pm$0.56$\%$), 78.78$\%$ ($\pm$0.83$\%$), and 95.62$\%$ ($\pm$0.59$\%$), respectively.
Compared to U-Net, UGS-Net shows an improvement of 1.39$\%$, 1.99$\%$, and 1.16$\%$ on the three metrics, respectively.
Compared to Attention U-Net, UGS-Net shows an improvement of 0.98$\%$, 1.45$\%$, and 0.68$\%$ on the three metrics, respectively.
These results indicate that UGS-Net outperforms other methods in terms of segmentation performance, and the significant improvement in the NSD score reflects the convincing performance of UGS-Net in boundary feature segmentation.
Furthermore, UGS-Net exhibits significant improvements in all indicators compared to UGS-Net-V1, with the DSC score increasing by 0.49$\%$, the IoU score increasing by 0.74$\%$, and the NSD score increasing by 0.34$\%$.
Moreover, the variances of the three metrics obtained in the five-fold cross-validation of UGS-Net are all smaller than those of UGS-Net-V1, indicating that the network is more stable after the addition of multi-annotation fusion loss and constraining operation.
The performance of nnU-Net \cite{nnUnet} has been reported in \cite{yang2022uncertainty}. While it showed slightly better performance than Attention U-Net, it required significant computational resources (about 150 GB of memory). This is why we chose Attention U-Net as the backbone for our Feature Extracting Module.



\begin{figure}[t]
  \centerline{\includegraphics[width=85mm]{segComparison.pdf}}
  \vspace{-0.0cm}
  \caption{Segmentation result of U-Net, Attention U-Net, R2U-Net, Channel U-Net, Nested U-Net, UGS-Net-V1, and UGS-Net.
  The red boxes corresponding to the \textbf{\emph{Input}} column indicate the features that should be noted or the error-prone locations of the nodules during segmentation.
  The red boxes in the \textbf{\emph{UGS-Net}} column indicate the segmentation detail of UGS-Net at these locations.
  The green boxes indicate the inadequacies in the suboptimal segmentation result.}
  \vspace{-0.0cm}
  \label{segComparison}
\end{figure}


Figure~\ref{segComparison} shows partial segmentation results of the aforementioned methods. The red boxes in the \textbf{\emph{Input}} column indicate areas of interest or error-prone segmentation locations for the nodules. Nodules (a)-(c) contain many low-density regions, nodules (d)-(f) have irregular spiculation signs at their boundaries, and nodules (g)-(h) have cavities. The segmentation of these areas by UGS-Net is significantly more consistent with the actual shape of the lesions than other methods.


Figure~\ref*{segComparison} and Table~\ref*{tabel1} demonstrate that: (1) Learning from the annotation set, as well as its union and intersection, provides richer visual information for the segmentation task. (2) Learning from LC areas improves the network's ability to recognize low-density regions. It is interesting that the network is trained with the annotation sets, but it ultimately improves the DSC for the prediction of $Label_1$. We will try to explain this phenomenon in Section~\ref*{CNV}.


\begin{figure*}[h]
  \centerline{\includegraphics[width=180mm]{Experi_MCM.pdf}}
  \vspace{-0.0cm}
  \caption{The predicted intersection $\cap(GT)'$, predicted union $\cup(GT)'$, final segmentation $X_S$, and MCM are generated by the UGS-Net.
  Colors in MCM are used for better visualization, red for $\cap(GT)'$ and blue for $\cup(GT)'$.
  In addition, final segmentation is represented in the MCM and marked with green to facilitate comparison.
  Red boxes indicate areas or features of nodules that are not easily distinguishable.}
  \vspace{-0.0cm}
  \label{Experi_MCM}
\end{figure*}


\subsection{Uncertain Region Prediction}
Besides conventional lung nodule segmentation, the UGS-Net can also predict which region is more likely to be nodule tissues, and which region is lower likely.
Figure~\ref{Experi_MCM} shows the prediction results $\cap(GT)'$ and $\cup(GT)'$, final segmentation result $X_{S}$, and the generated $MCM$.
In $MCM$ and $MCM$+$UGS$-$Net$, red represents the high-confidence region, blue represents the low-confidence region, and green is the final segmentation $X_{S}$.
In the ideal situation, the $X_{S}$ should achieve a balance between the high-confidence regions and low-confidence regions.


As can be intuitively seen from Figure~\ref*{Experi_MCM}, our final segmentation result is between the two extreme possibilities of high and low confidence regions, which is the most reasonable nodule region generated after learning all information.
Our network can have this performance because: (1) the prediction is aware of all possible annotations, (2) the prediction is constrained between $\cap(GT)'$ and $\cup(GT)'$. 

\begin{figure}[h]
  \centerline{\includegraphics[width=100mm]{MCM_HU.png}}
  \vspace{-0.0cm}
  \caption{Comparison of HU value kernel density estimation of real HC, LC, and predicted HC and LC.}
  \vspace{-0.0cm}
  \label{MCM_HU}
\end{figure}

Specifically, the use of MCM in lung nodule feature segmentation provides several advantages that can better display the semantic features of nodules. 
(1) MCM can better highlight the significant cavity features of nodules (Figure \ref{Experi_MCM}.\emph{(g)(h)}). The nodule cavity features are more obvious on the prediction mask with UGS-Net compared with other methods. This is due to UGS-Net's ability to capture the density difference of nodule tissue under the guidance of the intersection mask $\cap(GT)$ and union mask $\cup(GT)$, which helps to retain more features of the nodule cavity.
(2) MCM can better segment spiculation signs, which is an important feature for diagnosing benign and malignant lung nodules (Figure \ref{Experi_MCM}.\emph{(a)-(f)}). Spiculation is a stellate distortion caused by the intrusion of nodules into surrounding tissue and is typically low-density and distributed around the nodule edges. This feature is difficult to segment with traditional deep learning methods. UGS-Net can focus more on nodule boundary characteristics under the guidance of union mask $\cup(GT)$, resulting in better segmentation performance for spiculation distributed at the boundary of a nodule.
(3) MCM can better segment the low-density tissue of nodules (Figure \ref{Experi_MCM}.\emph{(i)-(l)}), which is commonly found in ground-glass nodules and is the main area that causes differences in expert labeling. UGS-Net can identify low-density tissue to the maximum extent through the study of the annotation set $GT$ and union mask $\cup(GT)$, which is very helpful for the diagnosis of ground-glass nodules.

Due to the small size of the LC mask, traditional quantitative evaluation metrics such as DSC and IoU are insufficient for measuring the prediction quality of MCM. To address this issue, we compared the HU distribution of the predicted HC and LC masks with the actual HC and LC masks. This approach allows us to assess the prediction accuracy of MCM in a more meaningful way.
It is assumed that UGS-Net can reasonably predict the degree of uncertainty in different regions. Therefore, the HU value distribution of predicted HC and LC masks should be similar to the actual distribution. Figure \ref{MCM_HU} shows that the predicted curve is in good agreement with the actual curve, indicating that the level of regional uncertainty we predicted is statistically reliable.


\subsection{Ablation Study}
In this section, we show the effectiveness of each module.
To better use the multiple annotations' information and enhance the relationship among $R_{LC}$, $R_{HC}$ and $R_{final}$, we update the UGS-Net-V1 with Multiple Annotation Fusion Loss and Intersection-Union Constraining Module.
To further demonstrate the contribution of these two modules, we constructed UGS-$\Phi_a$, UGS-$\Phi_b$, UGS-$\Phi_a+\Phi_b$, and UGS-IUCM for ablation experiments on the basis of UGS-Net-V1.
In UGS-$\Phi_a$, the Multiple Annotation Fusion Loss is only applied to the $GT_{uni}'$;
In UGS-$\Phi_b$, the Multiple Annotation Fusion Loss is only applied to the final output of the network $X_S$;
In UGS-$\Phi_a+\Phi_b$, the Multiple Annotation Fusion Loss is applied to both the $GT_{uni}'$ and $X_S$;
In UGS-IUCM, we use the Intersection-Union Constraining Module in the USG-Net-V1, but no Multiple Annotation Fusion Loss.

\begin{table}[h]
  \renewcommand\arraystretch{1.2}
  \vspace{-0.0cm}
  \caption{Ablation Study for Modules.}
  \centering
  \begin{tabular}{ccccccc}		
  \hline
  \hline
  \makebox[0.0003\textwidth][c]{\textbf{\emph{V1}}} & \makebox[0.0003\textwidth][c]{\textbf{\emph{$\Phi_{a}$}}} & \makebox[0.0003\textwidth][c]{\textbf{\emph{$\Phi_b$}}} & \makebox[0.01\textwidth][c]{\textbf{\emph{IUCM}}} & \makebox[0.03\textwidth][c]{\textbf{\emph{$DSC$}}} & \makebox[0.03\textwidth][c]{\textbf{\emph{$IoU$}}} & \makebox[0.03\textwidth][c]{\textbf{\emph{$NSD$}}} \\
  \hline
   &  &  &  & 86.67 $\pm$ 0.65 & 77.33 $\pm$ 0.88 & 94.94 $\pm$ 0.94 \\
  \checkmark &  &  &  & 87.16 $\pm$ 0.69 & 78.04 $\pm$ 1.00 & 95.28 $\pm$ 0.90 \\
  \checkmark & \checkmark &  &  & 87.42 $\pm$ 0.59 & 78.42 $\pm$ 0.87 & 95.56 $\pm$ 0.73 \\
  \checkmark &  & \checkmark &  & 87.33 $\pm$ 0.48 & 78.34 $\pm$ 0.70 & 95.44 $\pm$ 0.53 \\
  \checkmark & \checkmark & \checkmark &  & 87.62 $\pm$ 0.54& 78.75 $\pm$ 0.82 & \textbf{95.84 $\pm$ 0.61} \\
  \checkmark &  &  & \checkmark & 87.09 $\pm$ 0.59 & 77.97 $\pm$ 0.93 & 95.34 $\pm$ 0.69 \\
  \checkmark & \checkmark & \checkmark & \checkmark & \textbf{87.65 $\pm$ 0.57}& \textbf{78.78 $\pm$ 0.83} & 95.62 $\pm$ 0.59 \\
  \hline
  \hline
  \end{tabular} 
  \label{ablation}
  \vspace{-0.0cm}
\end{table}
The performance of our UGS-Net and its four variants are listed in Table~\ref{ablation}.
In Table~\ref{ablation}, the network without any additional modules is the backbone network of this paper, Attention U-Net.
V1 refers to UGS-Net-V1, which serves as the base network for the other variants.
The results show that:
(1) The performance improvements of UGS-$\Phi_a$ and UGS-$\Phi_b$ networks over UGS-Net indicate that the fusion of all annotated information can enable the network to more accurately capture the nodule region and achieve better segmentation performance.
(2) The DSC, IoU, and NSD of UGS-$\Phi_a+\Phi_b$ network are higher than those of the UGS-$\Phi_a$ and UGS-$\Phi_b$ networks, indicating that using Multiple Annotation Fusion Loss for both UAM and the final output simultaneously is better than using either one alone.
(3) Our UGS-Net outperforms UGS-$\Phi_a+\Phi_b$ and UGS-IUCM, demonstrating the effectiveness of the Intersection-Union Constraining Module. Although the quantitative performance of the network slightly declined when only the Intersection-Union Constraining Module was added (UGS-IUCM), a significant qualitative improvement was observed, which will be discussed later.
Moreover, the superior performance of UGS-Net suggests that Multiple Annotation Fusion Loss and Intersection-Union Constraining Module can mutually enhance each other, constraining uncertainty, and facilitating better segmentation performance.

\begin{figure*}[h]
  \centerline{\includegraphics[width=180mm]{AblationFig.pdf}}
  \vspace{-0.0cm}
  \caption{Visualization of different convolutional layers.
  The \textbf{\emph{Result}} in each case represents the final prediction of the network.
  \textbf{\emph{M3}}, \textbf{\emph{M2}}, and \textbf{\emph{M1}} respectively represent the visual feature maps of the third from the bottom, the second, and the first convolution layer under different network configurations.
  Red arrows indicate areas of nodules that need attention.}
  \vspace{-0.0cm}
  \label{F2}
\end{figure*}

To further verify the validity of the Multiple Annotation Fusion Loss and Intersection-Union Constraining Module, we demonstrate feature map visualization using Grad CAM in Figure~\ref{F2}.
The segmentation process of five nodules is shown in the Figure~\ref{F2}.
The \textbf{\emph{Result}} in each case represents the final prediction of the network.
\textbf{\emph{M3}}, \textbf{\emph{M2}}, and \textbf{\emph{M1}} respectively represent the visual feature maps of the third from the bottom, the second, and the first convolution layer under different network configurations.
Based on Figure~\ref{F2}, we observe that:
(1) When Multiple Annotation Fusion Loss is applied to the $GT_{uni}'$ or $X_S$, the recognition ability of the network for the low-density organization was significantly improved (\emph{UGS-$\Phi_a$ network and UGS-$\Phi_b$ network, Nodule A-D});
(2) The simultaneous use of Multiple Annotation Fusion Loss in $GT_{uni}'$ and $X_S$ can make the network outline the nodule boundary more clearly on the basis of improving the sensitivity to low-density tissues (\emph{UGS-$\Phi_a+\Phi_b$ network, Nodule A-D}).
(3) Intersection-Union Constraining Module enables the network to learn more boundary features, such as the spiculation (\emph{UGS-IUCM network, Nodule A-C}).   
(4) When Multiple Annotation Fusion Loss and Intersection-Union Constraining Module are used at the same time, the network's attention shifts to the nodule boundary, delineating a more reasonable and complete nodule region (\emph{UGS-Net, Nodule A-E}).


\subsection{Complex-Nodule Validation}
\label{CNV}
\begin{table*}[h]
  \renewcommand\arraystretch{1.2}
  \vspace{-0.0cm}
  \caption{Analysis of segmentation performances in Complex Nodule Validation.}
  \centering
  \begin{tabular}{c|cc|cc|cc|cc|cc|cc}
  \hline  
  \hline
  & \multicolumn{2}{c}{\textbf{\emph{Fold1}}} & \multicolumn{2}{c}{\textbf{\emph{Fold2}}} & \multicolumn{2}{c}{\textbf{\emph{Fold3}}} & \multicolumn{2}{c}{\textbf{\emph{Fold4}}} & \multicolumn{2}{c}{\textbf{\emph{Fold5}}} & \multicolumn{2}{c}{\textbf{\emph{Average}}}\\
  \hline
  & \textbf{\emph{DSC}} & \textbf{\emph{IoU}} & \textbf{\emph{DSC}} & \textbf{\emph{IoU}} & \textbf{\emph{DSC}} & \textbf{\emph{IoU}} & \textbf{\emph{DSC}} & \textbf{\emph{IoU}} & \textbf{\emph{DSC}} & \textbf{\emph{IoU}} & \textbf{\emph{DSC}} & \textbf{\emph{IoU}} \\
  \hline
  &\multicolumn{12}{c}{\textbf{\emph{U-Net DSC $\leq$ 60$\%$}}} \\
  \hline
  \makecell[l]{\textbf{\emph{U-Net}}} & 46.24 & 30.55 & 51.51 & 35.00 & 52.18 & 35.35 & 46.11 & 30.84 & 46.6 & 30.96 & 48.53 & 32.36 \\
  \makecell[l]{\textbf{\emph{UGS-Net}}} & 70.42 & 55.5 & 56.95 & 41.57 & 63.17 & 47.68 & 46.64 & 32.52 & 60.59 & 44.94 & 59.55\textcolor{green}{$\uparrow11.03$} & 44.44\textcolor{green}{$\uparrow11.88$} \\
  \hline
  &\multicolumn{12}{c}{\textbf{\emph{U-Net DSC $\leq$ 70$\%$}}} \\
  \hline
  \makecell[l]{\textbf{\emph{U-Net}}} & 56.88 & 40.78 & 61.37 & 44.79 & 61.54 & 44.83 & 61.08 & 44.78 & 59.53 & 43.24 & 60.08 & 43.68 \\
  \makecell[l]{\textbf{\emph{UGS-Net}}} & 74.09 & 60.02 & 69.32 & 54.74 & 72.02 & 57.62 & 65.90 & 50.99 & 70.60 & 56.05 & 70.39\textcolor{green}{$\uparrow10.31$} & 55.88\textcolor{green}{$\uparrow11.20$} \\
  \hline
  &\multicolumn{12}{c}{\textbf{\emph{U-Net DSC $\leq$ 80$\%$}}} \\
  \hline
  \makecell[l]{\textbf{\emph{U-Net}}}& 70.04 & 55.00 & 72.58 & 57.52 & 72.60 & 57.50 & 70.71 & 55.46 & 71.42 & 56.30 & 71.47 & 56.36 \\
  \makecell[l]{\textbf{\emph{UGS-Net}}} & 79.11 & 66.21 & 77.16 & 63.63 & 77.27 & 63.70 & 74.49 & 60.64 & 77.53 & 64.00 & 77.08\textcolor{green}{$\uparrow5.61$} & 63.64\textcolor{green}{$\uparrow7.28$} \\
  \hline
  \hline
  \end{tabular} 
  \label{complexNoduleTable}
  \vspace{-0.0cm}
\end{table*}

\begin{figure*}[htbp]
  \centerline{\includegraphics[width=160mm]{complexNodule.pdf}}
  \vspace{-0.0cm}
  \caption{Complex-Nodule Validation. This validation tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net by three levels.}
  \vspace{-0.0cm}
  \label{complexNodule}
\end{figure*}

The UGS-Net learns features from regions that may cause segmentation uncertainty, as a result, it can better segment nodules with large low-dense region or complex structures. To better demonstrate its improvement on nodule that are difficult to be segmented by U-Net, we design a Complex-Nodule Validation.
Based on five-fold validation U-Net, we further pick out three groups of nodules which have DSC scores lower than 60$\%$, 70$\%$, and 80$\%$, then train a UGS-Net with the same data settings, test these nodules again, and compare the DSC and IoU.

The validation results are listed in Table~\ref{complexNoduleTable}.
We observe that the improvement of UGS-Net on these complex nodules is very significant.
For nodules with DSC score less than 60$\%$ on U-Net, the average improvement on UGS-Net is 11.03$\%$. 
For nodules with DSC score less than 70$\%$ on U-Net, UGS-Net increased by 10.31$\%$ on average.
For nodules with less than 80$\%$ DSC score on U-Net, UGS-Net improved by 5.61$\%$ on average.
According to Table~\ref*{tabel1}, compared to U-Net, our network improve the DSC by 1.39. It seems to conflict with the fact that our network has the highest performance improvement on these nodule with DSC score less than 60$\%$ on U-Net. It is because nodules with complex structures are only a small part of all data. 

Figure \ref{complexNodule} shows the segmentation results of some complex nodules. 
The red subscripts are the segmentation DSC of UGS-Net, and the black subscripts are the DSC of U-Net. 
It can be perceived that nodules with U-Net segmentation DSC score less than 60$\%$ are some low-density or ground glass tissues. The significant improvement of UGS-Net in these nodules can indicate that UGS-Net can better learn the characteristics of nodules' low-density tissue and more accurately segment low-density nodular lesions regions.
When the U-Net segmentation DSC score is less than 70$\%$, it can be observed that in addition to some low-density nodules, some nodules also have irregular cavities or spiculation, abrupt bright spots in the tissues, or over-bright lung walls. The convincing segmentation performance of UGS-Net on these nodules reflects the learning ability of UGS-Net on boundary features, density differences and good anti-noise ability.
When the U-Net segmentation DSC score is less than 80$\%$, we observe that many new nodules that fit too closely with the lung wall are added. In this case, UGS-Net can accurately determine the nodule region, making the segmentation accuracy of nodules reach more than 90$\%$. In addition, for most nodules, the segmentation results of UGS-Net reflect more semantic features and have stronger interpretability.

In Section~\ref*{performanceana}, we mention that the experimental results show an interesting phenomenon: the network is trained with the annotation sets, but it finally improves the DSC for the prediction of $Label_1$. We believe Table~\ref*{complexNoduleTable} can explain this phenomenon. Although the network is not optimized with $Label_1$, the network still improves the prediction of $Label_1$ by improving the segmentation for those nodules with complex structures and low-dense tissues.

\section{Discussions}
\label{discussions}
The task of lung nodule segmentation has focused on achieving high accuracy for a very long time, with the DSC or IoU being the main target. However, given the small size and complex structures of lung nodules, it may be more helpful for radiologists if segmentation results highlight regions with varying degrees of confidence. High-confidence regions provide the major part of nodules or tumor tissues, while low-confidence regions contain important low-density features, such as ground-glass and spiculation signs, that radiologists should also pay attention to.

The proposed method aims to provide information that is more useful for clinical diagnosis, rather than simply improving the DSC. It does not seek to replace the clinical role of doctors, but to complement it by allowing them to leverage the strengths of artificial intelligence methods. This, we believe, is a better approach to integrating AI into clinical practice.

One limitation of our study is that we only test our proposed method on the LIDC-IDRI dataset, which is currently the only publicly available fully annotated dataset for lung nodules. Despite being over ten years old, this dataset still presents many opportunities for research. However, the need for multiple annotations could limit the practicality of our method in real-world clinical settings, where obtaining multiple annotations may not always be feasible. To address this limitation, we plan to explore techniques that can automatically identify high- and low-confidence regions based on a single annotation, thus increasing the feasibility and applicability of our approach.

\section{Conclusions}
\label{conclusion}
This paper introduces the Uncertainty-Aware Attention Mechanism (UAAM), which leverages the consensus or disagreements among multiple annotations to improve segmentation and identify regions with low segmentation confidence. UAAM features the Multi-Confidence Mask (MCM), a combination of a Low-Confidence (LC) Mask and a High-Confidence (HC) Mask. 
  Based on UAAM, we further design an Uncertainty-Guide Segmentation Network (UGS-Net), which contains a \emph{Feature Extracting Module}, an \emph{Uncertainty-Aware Module}, and an \emph{Intersection-Union Constraining Module}. These modules together learn valuable information from the consensus or disagreements among multiple annotations, providing regions with high and low segmentation confidences, and a segmentation result that can balance all possibilities.
  Besides the traditional validation method, we propose a Complex Nodule Validation on LIDC-IDRI, which tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net. Experimental results demonstrate that our method can significantly improve the segmentation performance on nodules with poor segmentation by U-Net.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
  \bibliographystyle{IEEEtran}
  % argument is your BibTeX string definitions and bibliography database(s)
  \bibliography{ijcai20}


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:


% if you will not have a photo at all:


% insert where needed to balance the two columns on the last page with
% biographies
%\newpage



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}