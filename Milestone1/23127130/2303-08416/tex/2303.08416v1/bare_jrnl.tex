%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.



\usepackage{times}
\renewcommand*\ttdefault{txtt}
\usepackage{soul}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{amsfonts,amssymb}
\usepackage{multirow}
\usepackage{color}
\usepackage{cite}
\usepackage{pifont}
\usepackage{array}
\usepackage[misc]{ifsym} 


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Lung Nodule Segmentation and Low-Confidence Region Prediction with Uncertainty-Aware Attention Mechanism}
% \author{
% }
\author{
  Han~Yang, Qiuli~Wang
\thanks{Corresponding author: Qiuli~Wang. H.~Yang and Q.~Wang contribute equally to this paper. Part of this work was completed by H.~Yang and Q.~Wang in Chongqing University.}
\thanks{Q.~Wang is with the Center for Medical Imaging, Robotics, Analytic Computing and Learning (MIRACLE), the School of Biomedical Engineering and Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China.E-mail: wangqiuli@ustc.edu.cn}
\thanks{H.~Yang is with the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China. H.~Yang is also with University of Chinese Academy of Sciences, Beijing, China. E-mail: yanghan223@mails.ucas.ac.cn.}
}

\maketitle

\begin{abstract}
  Radiologists have different training and clinical experiences, so they may provide various segmentation annotations for a lung nodule, which causes segmentation uncertainty among multiple annotations. Conventional methods usually chose a single annotation as the learning target or tried to learn a latent space of various annotations. Still, they wasted the valuable information of consensus or disagreements ingrained in the multiple annotations.
  This paper proposes an Uncertainty-Aware Attention Mechanism (UAAM), which utilizes consensus or disagreements among annotations to produce a better segmentation. In UAAM, we propose a Multi-Confidence Mask (MCM), which is a combination of a Low-Confidence (LC) Mask and a High-Confidence (HC) Mask. LC mask indicates regions with low segmentation confidence, which may cause different segmentation options among radiologists.
  Following UAAM, we further design an Uncertainty-Guide Segmentation Network (UGS-Net), which contains three modules:\emph{Feature Extracting Module} captures a general feature of a lung nodule.
  \emph{Uncertainty-Aware Module} produce three features for the annotations' union, intersection, and annotation set. Finally, \emph{Intersection-Union Constraining Module} use distances between three features to balance the predictions of final segmentation, LC mask, and HC mask.
  To fully demonstrate the performance of our method, we propose a Complex Nodule Challenge on LIDC-IDRI, which tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net. Experimental results demonstrate that our method can significantly improve the segmentation performance on nodules with poor segmentation by U-Net.
\end{abstract}
\begin{IEEEkeywords}
Lung Nodules Segmentation, Uncertainty, Multiple Annotations, Computed Tomography.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{L}{ung} nodule segmentation plays an important role in CAD systems for lung cancer \cite{wang2017central}, which can provide critical information for other tasks, such as nodule sizes, shapes, and crucial medical features. 
For the general train-and-test paradigm of deep learning methods, each nodule image data has only one annotation mask delineated by one radiologist \cite{pezzano2021cole, shariaty2022texture, agnes2022efficient, oktay2018attention,ronneberger2015u}. And each time, the network can only provide a single prediction of nodule regions.


\begin{figure}[htbp]
  \centerline{\includegraphics[width=70mm]{overview.pdf}}
  \vspace{-0.0cm}
  \caption{A. Uncertainty caused by multiple annotations. HC is High-Confidence mask, which is the intersection of annotation set. LC is Low-Confidence mask, which is the different between annotation set's union and intersection. Multi-Confidence Mask is the combination of LC and HC.
  B. Hounsfield Unit Kernel Estimations in HC and LC of LIDC-IDRI.}
  \vspace{-0.0cm}
  \label{MCM}
\end{figure}

However, in clinical practice, different radiologists may provide various segmentation annotations for a lung nodule due to their unique training and clinical experience \cite{hu2019supervised, kohl2019hierarchical, xiaojiang2021}. As a result, the traditional methods based on single annotation can not reflect various clinical experiences and limit the application of deep learning methods.

A straightforward solution for this problem is introducing multiple annotations annotated by different radiologists. 
That leads to another problem: multiple annotations inevitably bring in uncertainty and conflicts. For example, some regions annotated as lung nodule tissues by a radiologist may be annotated as non-nodule regions by others.
To overcome this problem, Kohl \textit{et al.} proposed a probabilistic U-Net for segmenting ambiguous images in 2018. This network had a conditional variational auto-encoder to encode the multiple segmentation variants into a low-dimensional latent space \cite{kohl2018probabilistic, kohl2019hierarchical}. A random sample from this latent space could affect the corresponding segmentation map. Based on this study, Hu \textit{et al.} proposed to combine the ground-truth uncertainty with a probabilistic U-Net, which could improve predictive uncertainty estimates, sample accuracy, and sample diversity \cite{hu2019supervised}.
These methods relied on the latent space and random samples in this space. Consequently, these methods could only produce the probabilistic results of uncertainty distribution and cannot provide a stable uncertainty prediction.


In this paper, we argue that the \textbf{uncertainty between multiple annotations follows a particular pattern}. 
To demonstrate this phenomenon, we introduce the Multi-Confidence Mask (MCM), which is the combination of High Confidence (HC) mask and Low Confidence (LC) mask. As shown in Figure~\ref*{MCM}. A, the HC is the intersection of all annotations, and LC is the difference between intersection and union.
Then we calculate the HU (Hounsfield Unit) Kernel Estimations of HC and LC of LIDC-IDRI \cite{armato2011lung}. As shown in Figure~\ref*{MCM}. B, HU distribution of LC is apparently different from the HU distribution in HC. 
From the perspective of pixel distribution, the LC regions have lower HU values than the HC regions. From the perspective of medical features, the LC regions are the nodule edges, spiculation, or ground-glass features, and the HC regions are nodule cores or high-dense tissues.
\textcolor{black}{Since low HU indicates low density, we believe that the regions causing radiologists disagreement are mainly low-density tissue and boundary-related features.}

\begin{figure}[htbp]
  \centerline{\includegraphics[width=80mm]{overview_UAAM.pdf}}
  \vspace{-0.0cm}
  \caption{Overview of \emph{Uncertainty-Aware Attention Mechanism} (UAM).} The union mask guides the learning of low-certainty features. The intersection mask guides the learning of high-certainty features. The difference between union and intersection masks is the regions with high segmentation uncertainty. The annotation set guides the learning of a plausible segmentation, which is a balance between all annotations.
  \vspace{-0.0cm}
  \label{UAM}
\end{figure}

Different from traditional methods, we propose to utilize the MCM as the learning guidance of low and high-confidence features, contributing to a better segmentation performance. We call this training \emph{Uncertainty-Aware Attention Mechanism} (UAAM) hereafter, which is shown in Figure~\ref*{UAM}. Following this mechanism, we further design a Uncertainty-Guide Segmentation Network (UGS-Net) for lung nodule segmentation, which contains a U-Net-based \emph{Feature Extracting Module}, a \emph{Uncertainty-Aware Module}, and a \emph{Intersection-Union Constraining Module}.

Firstly, a \emph{Feature Extracting Module} provides general feature maps according to the input CT image.
Secondly, an \emph{Uncertainty-Aware Module} transfers the general feature maps into three independent feature maps $R_{LC}$, $R_{HC}$ and $R_{Uni}$ with the guidance of annotations' union, intersection, and the annotation set. $R_{LC}$, $R_{HC}$ will be transferred into the predicted union, intersection, and further combined as MCM. 
Thirdly, an \emph{Intersection-Union Constraining Module} captures preferred features with Feature-Aware Attention Block from $R_{LC}$, $R_{HC}$ and $R_{Uni}$, then uses feature distances to constrain the $X_{final}$, so that the final prediction can achieve a balance between extreme annotations, while the predicted LC regions in MCM can also be constrained in a moderate and reasonable way. To better utilize multiple annotations, we also introduce a Multiple Annotation Fusion Loss to optimize the $R_{Uni}$ and $X_{final}$, which calculates the average BCE loss between prediction and all annotations.

The proposed method has two advantages:
(1) Rather than learning from a latent space, the proposed method has specific learning targets, and can provide a stable prediction of uncertain nodule regions.
(2) The method makes the most of all annotations, ensuring the final prediction balances different conditions.

We have reported a preliminary version of this work in a previous publication \cite{yang2022uncertainty}. 
The new contributions of this paper can be summarized as follows:  
\begin{itemize}
  \item We introduce a \emph{Uncertainty-Aware Attention Mechanism} (UAAM), which utilizes the MCM as the learning guidance of low and high-confidence features, contributing to a better segmentation performance. The UAAM can provide a stable prediction for uncertain regions as well as a single lung nodule segmentation.
  \item Based on the mechanism, we propose a new Uncertainty-Guide Segmentation Network (UGS-Net), which contains a Feature Extracting Module, an \emph{Uncertainty-Aware Module} (UAM), and an \emph{Intersection-Union Constraining Module}. To make the most of multiple annotations, we also introduce a Multiple Annotation Fusion Loss, comparing the segmentation with all possible annotations.
  \item We propose a Complex-Nodule Validation, which tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net. 
  Experiments demonstrate that our model has the most performance improvement on those nodules that have Dice scores lower than 0.6 with U-Net

\end{itemize}

\section{Related Work}
\label{relatedwork}
\subsection{Lung Nodule Segmentation}
Lung nodule segmentation is a key task in lung nodule CAD systems, which aims to segment the target nodule's boundary so that it can provide the details about nodule diameter, size, and some semantic features \cite{gonccalves2016hessian,wu2010stratified,pezzano2021cole}. 
The main challenge of this task is that lung nodules have various shapes, sizes, and delicate features. In the early years, researchers provided a lot of methods for lung nodule segmentation, such as morphology-based methods and region-growing-based methods \cite{diciotti2011automated,dehmeshki2008segmentation}. Recently, deep learning has become the most popular method in this area.

In 2017, S. Wang \emph{et al.} proposed a multi-view convolutional network for lung nodule segmentation.
The proposed network captured a diverse set of nodule-sensitive features from axial, coronal, and sagittal views in CT images simultaneously. It analyzed them with a multi-branch CNN network, achieving an average dice similarity coefficient (DSC) of 77.67\% \cite{wang2017multi}. Also, in 2017, S. Wang proposed a central focused convolutional neural network with central-pooling layers to analyze nodules in 2D and 3D thoroughly \cite{wang2017central}. In 2020, H. Cao designed a dual-branch residual network with intensity-pooling layers, which enhanced the learning of intensity information and improved the DSC to 82.74\% \cite{cao2020dual}. In 2021, G. Pezzano introduced a CNN network that could learn the context of the nodules by producing two masks representing all the background and secondary-important elements in the CT, so that the model can better discriminate nodule features \cite{pezzano2021cole}
Later in 2022, F. Shariaty further proposed texture feature extraction and feature selection algorithms to improve the segmentation, which achieved a DSC of 84.75\% \cite{shariaty2022texture}.

As seen from the studies above, existing methods pursued a more precise segmentation, ignoring that different radiologists may have diverse segmentation options for the same lung nodule.
In this study, we argue that disagreements among annotations are also valuable for the diagnosis. We propose to learn the uncertainty among annotation sets, and produce a segmentation that can balance all annotations.

\subsection{Uncertainty in Lung Nodule Segmentation}
Many medical image vision problems suffer from ambiguities. In clinical situation, it might not be clear from a CT scan alone which particular region is cancer tissue \cite{kohl2018probabilistic}. As a result, even if experienced doctors and radiologists may provide different segmentation for the same tissues or tumor. 

In 2018, Simon Kohl \emph{et al.} proposed to model this task as the learning of distribution over diverse but plausible segmentations for lung nodule. Based on U-Net \cite{ronneberger2015u}, they introduced a probabilistic U-Net, which was a combination of a U-Net and a conditional VAE that could produce an unlimited plausible segmentations. Later in 2019, Simon Kohl \emph{et al.} further proposed a hierarchical probabilistic U-Net, which used a hierarchical latent space decomposition to formulate the sampling and reconstruction of segmentations with high fidelity \cite{kohl2019hierarchical}. Also in 2019, Shi Hu \emph{et al.} analyzed two main types of uncertainty: aleatoric and epistemic \cite{hu2019supervised}. They exploited multiple annotations' variability as a source of `ground truth' aleatoric uncertainty, combined this uncertainty with probabilistic U-Net, and tried to quantitatively analyze the segmentation uncertainty. 
In 2021, Xiaojiang Long \emph{et al.} extended the concept in \cite{hu2019supervised} to the V-Net and 3D lung nodule CT images.
As a ideal dataset, which contained multiple annotations for over 1000 lung nodules, all these studies \cite{kohl2018probabilistic,kohl2019hierarchical,hu2019supervised,xiaojiang2021} analyzed LIDC-IDRI \cite{armato2011lung}.

Unlike the studies above, our method focuses on the uncertainty in lung nodule segmentation with multiple annotations. Different from the models based on VAE, our work pays more attention to the reason that causes the various annotations, and introduces an alternative method to focus on the uncertainty region, providing stable predictions for both uncertain nodule regions and lung nodule segmentation.

\section{Methods}
\label{method}
\subsection{Uncertainty-Guided Segmentation Network}
\begin{figure*}[htbp]
  \centerline{\includegraphics[width=170mm]{arch.pdf}}
  \vspace{-0.0cm}
  \caption{
  Overview of Uncertainty-Guided Segmentation Network. This network contains three modules:
  (1) \emph{Feature Extracting Module}, (2) \emph{Uncertainty-Aware Module}, and (3) \emph{Intersection-Union Constraining Module}.
  }
  \vspace{-0.0cm}
  \label{arch}
\end{figure*}
  
In Figure~\ref{arch}, we show the architecture of the Uncertainty-Guided Segmentation Network (UGS-Net). 
Generally speaking, the input is the lung nodule CT image $X$, the outputs contain a predicted MCM (Multi-Confidence Mask) and a final segmentation $X_{S}$. The network's learning targets are the annotation set $GT$, their union $\cup(GT)$ and intersection $\cap(GT)$. The input images and their masks have the same size $64\times64$, which are cropped from the LIDC-IDRI \cite{armato2011lung} according to the official annotations.

The UGS-Net contains three modules:
(1) \emph{Feature Extracting Module}, (2) \emph{Uncertainty-Aware Module}, and (3) \emph{Intersection-Union Constraining Module}.
\textcolor{black}{\emph{Feature Extracting Module}
can use any segmentation network based on U-Net structure to initially obtain a feature map $R$ with 64 channels.
This paper uses Attention U-Net \cite{oktay2018attention} here, it has five down-sampling and up-sampling layers.
Each up-sampling layer is composed of two convolutional layers and an attention block.}
% is a network based on Attention U-Net \cite{oktay2018attention}. It has five down-sampling and up-sampling layers. Each up-sampling layer is composed of two convolutional layers and an attention block. It takes $X$ and generate feature maps $R$ with 64 channels.
\emph{Uncertainty-Aware Module} (UAM) analyzes the $R$ and generates $R_{LC}$, $R_{HC}$, and $R_{uni}$, they will be fed into three convolutional blocks and produce initial $\cup(GT)'$, $\cap(GT)'$, and a plausible segmentation $GT_{Uni}'$. 
\textcolor{black}{$\cup(GT)'$ and $\cap(GT)'$ will compute to get MCM.}
\emph{Intersection-Union Constraining Module} (SCM) learns the different characteristics of $R_{LC}$, $R_{HC}$, and $R_{uni}$, helps to refine the MCM, and then provides a \textcolor{black}{more reasonable} final segmentation $X_{S}$ by fusing all information.

\subsection{Uncertainty-Aware Module}
\label{UAM_method}
\textcolor{black}{How to make the best use of the valuable uncertain information in the annotation set is the key to further improve the performance of network segmentation.
\emph{Uncertainty-Aware Module} is introduced to make full and reasonable use of all annotation information by learning $\cup(GT)$, $\cap(GT)$ and $GT$.}
It has two tasks: (1)capture the different features from Low-Confidence (LC) regions, High-Confidence (HC) regions, and all annotations; (2) produce initial predictions of MCM and a general segmentation.


\textcolor{black}{As shown in Figure~\ref{arch}, the backbone of UAM is a three-branch CNN network.
It takes $R$ as input and transfers $R$ into $R_{LC}$, $R_{HC}$, and $R_{uni}$ with three different convolution blocks.
$R_{LC}$ and $R_{HC}$ are fed into the MCM BCE Loss block, and $R_{uni}$ is fed into the Multiple Annotation Loss block.}
MCM BCE Loss block first transfers $R_{LC}$ and $R_{HC}$ into $\cup(GT)'$ and $\cap(GT)'$ with two parallel convolutional layers, and calculate BCE loss between $\{\cup(GT)', \cup(GT)\}$ and $\{\cap(GT)', \cap(GT)\}$.
\textcolor{black}{The $\cup(GT)'$ and $\cap(GT)'$ are combined as $MCM'$ through operation $normalization(\cup(GT)'+\cap(GT))$ to show the degree of uncertainty in different regions.}
Different from our work in \cite{yang2022uncertainty}, the branch for $R_{uni}$ is optimized by \textcolor{black}{Multiple Annotation Loss block}, which will be discussed later.
\textcolor{black}{Moreover, under the guidance of different learning targets, maps $R_{LC}$, $R_{HC}$, and $R_{uni}$ with different latent visual features will be obtained after the convolutional layer. These three feature maps will be fed into the next module for further analysis.}

\subsection{Intersection-Union Constraining Module}
\label{SCM}
As mentioned above, $\cup(GT)$ and $\cap(GT)$ are the learning targets of UAM. $\cup(GT)$ indicates all regions that might be nodule tissues, and $\cap(GT)$ indicates nodule regions with highest confidence. However, we would like our final segmentation prediction can achieve a balance between extreme situation, so we modify the last version of UGS-Net, and design a new \emph{Intersection-Union Constraining Module} (IUCM), which aims to further capture the features of three learning targets, and produce a more reasonable segmentation prediction which can balance extreme situations.
\begin{figure}[htbp]
  \centerline{\includegraphics[width=80mm]{IUCM.pdf}}
  \vspace{-0.0cm}
  \caption{
    Intersection-Union Constraining Module. It contains three Feature-Aware Attention Blocks, and each block has specific feature learning preferences.
    }
  \vspace{-0.0cm}
  \label{UAM_F}
\end{figure}

As shown in Figure~\ref*{UAM_F}, IUCM takes $R_{LC}$, $R_{HC}$, and $R_{uni}$ as its inputs, and generals corresponding $R'_{LC}$, $R'_{HC}$, and $R'_{uni}$ with Feature-Aware Attention Block (FAAB). 
FAAB is built based on a self-attention block \cite{vaswani2017attention} and a feature-aware filter.
These attention blocks process $R_{LC}$, $R_{HC}$, and $R_{uni}$ with different feature-aware filters, which enable the network to formulate different learning preferences for different learning objectives, to obtain more image features that are helpful for segmentation.
More specifically, assuming the input is $R^z$, the process of FAAB can be summarized as

\begin{equation}
  \begin{aligned}
    R'_z=R_z\oplus \Gamma(A(R_z))
  \end{aligned}
\end{equation}
where $z\in\{uni,LC,HC\}$, $A$ indicates the self-attention architecture. $\Gamma$ is a feature-aware filter, in this study, the $\Gamma$ for $R_{uni}$ and $R_{LC}$ is Gabor \cite{luan2018gabor}, the $\Gamma$ for $R_{HC}$ is Otsu \cite{2007A}. 
To keep more information from the inputs, the $R'_z$ is concatenated with $R_z$.
The process of self-attention architecture please refer to the study \cite{vaswani2017attention}. 

After getting $R'_{LC}$, $R'_{HC}$, and $R'_{uni}$, the IUCM gets $s_z=d\{R'_z,R\}$, $d$ is feature $L2$ distance. 
\textcolor{black}{The output of IUCM is $R_{Aug}=\bigodot s_z\times R'_z$, $z\in\{uni,LC,HC\}$.} $R_{Aug}$ will be concatenated with $R$ from the Feature Extracting Module, fed into a convolutional layer, and produce the final segmentation prediction $X_{S}$. The concatenation of $R$ and $R_{Aug}$ can also keep more information from $X$, preventing losing too many original features.

\subsection{Loss Function}
\label{Loss}
In Figure~\ref*{arch}, the loss function of UGS-Net contains two pars: MCM BCE Loss block and Multiple Annotation Loss block.

MCM BCE Loss block calculates BCE loss between $\{\cup(GT)', \cup(GT)\}$ and $\{\cap(GT)', \cap(GT)\}$, which can be represented as:
\begin{equation}
  \begin{aligned}
    L_{MCM}=BCE(\cup(GT)', \cup(GT))+\\BCE(\cap(GT)', \cap(GT))
  \end{aligned}
\end{equation}

\textcolor{black}{\emph{Multiple Annotation Fusion Loss} is used optimize $GT_{uni}'$ and $X_{S}$ in Multiple Annotation Loss block, which we denote by $\Phi$.} In our previous work, $GT_{uni}'$ and $X_{S}$ are optimized with a rand annotation selected from the annotation set.
This choice wastes rich information among different annotations. In this work, we introduce a Multiple Annotation Fusion Loss to compare the prediction with all possible annotations.
As shown in Figure~\ref*{maloss}, we calculate the average dice scores between optimied object and each annotation in annotation set, and use this average dice as a part of loss fusion. 
We have:
\begin{equation}
  \begin{aligned}
    \Phi_a=E\{\Sigma^{j}Dice (GT_{uni}',GT_j) \} \\
    \Phi_b=E\{\Sigma^{j}Dice (X_{S},GT_j) \}
  \end{aligned}
\end{equation}
where $GT_j\in GT$. This fusion encodes all information in annotation sets to better optimize the final prediction.
The loss fusion of the model can be defined as:
\begin{equation}
  \begin{aligned}
    L=\alpha_{1}L_{MCM}+\alpha_2\Phi_a+\alpha_3\Phi_b
  \end{aligned}
\end{equation}
\textcolor{black}{where $\alpha_1$, $\alpha_2$, and $\alpha_3$ are User-defined parameters. 
Empirically, $\alpha_1$ is set to 0.5, $\alpha_2$ to 0.5, and $\alpha_3$ to 1 in this paper.}


\begin{figure}[htbp]
  \centerline{\includegraphics[width=40mm]{multipleloss.pdf}}
  \vspace{-0.0cm}
  \caption{
    Multiple Annotation Fusion Loss.
  }
  \vspace{-0.0cm}
  \label{maloss}
\end{figure}


\section{Experiments}
\label{experiments}
\subsection{Dataset and Experimental Settings}
We validate the proposed Uncertainty-Guided Segmentation Network (UGS-Net) on the LIDC-IDRI \cite{armato2011lung} dataset. 
The dataset includes a total of 1018 study instances and over 2600 nodules. 
For each instance, the medical image files are annotated in two stages by several experienced chest radiologists.
In the first stage, each radiologist independently gives the diagnosis of benign and malignant nodules, the diagnosis of semantic features, and the delineation of the nodules' lesion region. 
This study retained 1860 lung nodules with a diameter of 3-30mm and multiple annotations ($\geq2$).
For each selected nodule, we get its CT image and multiple annotation set $GT$, their union $\cup(GT)$ and intersection $\cap(GT)$. 

We conduct experiments on a ubuntu 18.04 server with Tesla A100 using CUDA 11.2.
Attention U-Net \cite{oktay2018attention} is employed as the backbone model, and is implemented using PyTorch-v1.0.1 Python3.7.
Experiments are repeated five times independently with 5-fold cross-validation.
Stochastic Gradient Descent with Warm Restarts (SGDR) is used as the optimizer. The initial learning rate (LR) is 0.00001, the batch size is 32, the momentum is 0.9, and the weight decay rate is 0.0001.
Each model is trained with 200 epochs. The learning rate is updated every 50 epochs. 

Two metrics are used to evaluate the predictive ability of the network to the lesion regions, including the evaluation based on single annotation and the evaluation based on multiple annotations.
For the former group, following the traditional strategy, the average dice similarity coefficient ($Dice_s$), intersection over union ($IoU_s$) and Normalized surface Dice ($NSD_s$) of all nodules for a single annotation are used to verify the superior segmentation performance of the network beyond the previous work.
Normalized surface Dice (NSD) is used to evaluate how close $G$ and $S$ are to each other \cite{ma2021abdomenct}. 
For the latter group, the average dice similarity coefficient ($Dice_{ave}$), intersection over union ($IoU_{ave}$) and Normalized surface Dice ($NSD_{ave}$) of multiple annotations for all nodules are used to verify the integration ability of the model for multiple annotation information.
Detailed introduction of these metrics can be referred to \cite{yang2022uncertainty}.

\subsection{Performance of Lung Nodule Segmentation}

\begin{table*}[h]\tiny
  \setlength{\tabcolsep}{5pt}
  \renewcommand\arraystretch{1.1}
  \vspace{-0.0cm}
  \caption{Performance comparison between our UGS-Net and eight models based on the U-Net structure on the LIDC-IDRI dataset.
  UGS-Net-V1 represents a preliminary version of this work in a \cite{yang2022uncertainty}.}
  \centering
  \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}		
  \hline
  \hline
  \multirow{2}*{\textbf{\emph{Method}}} & \multicolumn{3}{c}{\textbf{\emph{Fold1}}} & \multicolumn{3}{c}{\textbf{\emph{Fold2}}} & \multicolumn{3}{c}{\textbf{\emph{Fold3}}} & \multicolumn{3}{c}{\textbf{\emph{Fold4}}} & \multicolumn{3}{c}{\textbf{\emph{Fold5}}} & \multicolumn{3}{c}{\textbf{\emph{Average}}}\\
  \cline{2-19} 
  ~ & \multicolumn{1}{c}{\textbf{\emph{Dice}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{Dice}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{Dice}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{Dice}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{Dice}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} & \multicolumn{1}{c}{\textbf{\emph{Dice}}} & {\textbf{\emph{IoU}}} & {\textbf{\emph{NSD}}} \\
  \hline
  FCN32s \cite{DBLP:journals/corr/LongSD14} & 83.92 & 73.51 & 92.63 & 84.80 & 74.75 & 93.64 & 85.37 & 75.35 & 94.81 & 84.54 & 74.44 & 94.21 & 85.74 & 75.78 & 94.80 & 84.87 $\pm$ 0.64 & 74.77 $\pm$ 0.78 & 94.02 $\pm$ 0.82 \\
  U-Net \cite{DBLP:journals/corr/RonnebergerFB15} & 84.98 & 75.18 & 93.12 & 86.34 & 76.89 & 93.67 & 87.26 & 78.21 & 95.50 &86.20 & 76.67 & 94.83 & 86.52 & 77.02& 95.17 & 86.26 $\pm$ 0.74 & 76.79 $\pm$ 0.97 & 94.46 $\pm$ 0.91 \\
  R2U-Net \cite{DBLP:journals/corr/abs-1802-06955} & 83.33 & 72.92 & 92.10 &85.66 & 76.02 & 94.17 &86.44 & 77.01 & 95.23 & 84.83 & 74.86 & 93.38 & 85.61 & 75.66 & 95.26 & 85.17 $\pm$ 1.05 & 75.29 $\pm$ 1.37 & 94.03 $\pm$ 1.19 \\
  R2AU-Net \cite{R2AU-Net} & 82.99 & 72.36 & 91.50 & 83.84 & 73.43 & 92.92 &86.13 & 76.50 & 94.58 & 85.10 & 75.24 &94.24 &85.41 & 75.53 & 95.03 & 84.69 $\pm$ 1.13 & 74.61 $\pm$ 1.50 & 93.65 $\pm$ 1.27 \\
  Attention U-Net \cite{DBLP:journals/corr/abs-1804-03999} &85.99 & 76.44 & 93.98 & 86.11 & 76.58 & 93.68 & 87.76 & 78.89 & 96.07 & 86.50 & 77.13 & 95.42 & 86.99 & 77.61 & 95.56 & 86.67 $\pm$ 0.65 & 77.33 $\pm$ 0.88 & 94.94 $\pm$ 0.94 \\
  Nested U-Net \cite{DBLP:journals/corr/abs-1807-10165} & 84.72 & 74.70 & 93.12 & 85.40 & 75.61 & 94.39 & 86.06 & 76.35 & 95.57 & 85.50 & 75.54 & 95.16 & 86.31 & 76.62 & 95.90 & 85.60 $\pm$ 0.55 & 75.76 $\pm$ 0.68 & 94.83 $\pm$ 0.99 \\
  Channel-Unet \cite{10.3389/fgene.2019.01110} & 85.28 & 75.44 & 94.15 & 86.02 & 76.58 & 93.22 & 86.93 & 77.62 & 95.94 & 86.25 & 76.82 & 95.36 & 86.56 & 76.99 & 95.46 & 86.21 $\pm$ 0.56 & 76.69 $\pm$ 0.71 & 94.82 $\pm$ 1.00 \\
  UGS-Net-V1 \cite{yang2022uncertainty} & 86.47 & 77.07 & 93.58 & 87.21 & 78.17 & 95.26 & 88.39 & 79.86 & 96.20 & 86.56 & 77.17 & 95.56 & 87.19 & 77.94 & 95.78 & 87.16 $\pm$ 0.69 & 78.04 $\pm$ 1.00 & 95.28 $\pm$ 0.90 \\ 
  \hline
  UGS-Net & 87.49 & 78.43 & 94.67 & 87.72 & 78.86 & 95.43 & 88.66 & 80.31 & 96.50 & 86.93 & 77.86 & 95.64 & 87.44 & 78.44 & 95.84 & \textbf{87.65 $\pm$ 0.57}\textcolor{green}{$\uparrow$}& \textbf{78.78 $\pm$ 0.83}\textcolor{green}{$\uparrow$} & \textbf{95.62 $\pm$ 0.59}\textcolor{green}{$\uparrow$} \\ 
  \hline
  \hline
  \end{tabular} 
  \label{tabel1}
  \vspace{-0.1cm}
\end{table*}

We compare the proposed UGS-Net with eight commonly used segmentation models on traditional lung nodule segmentation, including FCN32s \cite{DBLP:journals/corr/LongSD14}, U-Net \cite{DBLP:journals/corr/RonnebergerFB15}, R2U-Net \cite{DBLP:journals/corr/abs-1802-06955}, R2AU-Net \cite{R2AU-Net}, Attention U-Net \cite{DBLP:journals/corr/abs-1804-03999}, Nested U-Net \cite{DBLP:journals/corr/abs-1807-10165}, Channel U-Net \cite{10.3389/fgene.2019.01110}, and UGS-Net-V1 \cite{yang2022uncertainty}.
The UGS-Net-V1 and UGS-Net are trained with multiple annotations. Other models are trained with traditional strategy: each nodule has one correlated annotation, which is the first in the annotation set.

As can be seen from Table \ref{tabel1}, UGS-Net achieves the highest score on Dice, IoU, and NSD, reaching 87.65$\%$ ($\pm$0.56$\%$), 78.78$\%$ ($\pm$0.83$\%$), and 95.62$\%$ ($\pm$0.59$\%$) respectively.
Compared with U-Net, the three metrics are increased by 1.39$\%$, 1.99$\%$, and 1.16$\%$, respectively.
Compared with Attention U-Net, the three metrics are increased by 0.98$\%$, 1.45$\%$, and 0.68$\%$, respectively.
From the quantitative point of view, our UGS-Net has better segmentation performance than other methods, and the substantial improvement of the NSD score reflects the convincing performance of UGS-Net in boundary feature segmentation. Compared with the UGS-Net-V1, UGS-Net also has a considerable improvement in various indicators, with the Dice score increasing by 0.49$\%$, the IoU score increasing by 0.74$\%$, and the NSD score increasing by 0.34$\%$.
Moreover, the variance of the three metrics obtained in the five-fold cross-validation of UGS-Net is all smaller than that of UGS-Net-V1, indicating that the model is more stable after the addition of multi-annotation fusion loss and constraining operation.
The performance of nnU-Net \cite{nnUnet} has been reported in the \cite{yang2022uncertainty}. Its performance was slightly better than Attention U-Net, but it took too many calculation resources (about 150 GB of memory). That is why we choose Attention U-Net as our Feature Extracting Module's backbone.

\begin{figure}[t]
  \centerline{\includegraphics[width=85mm]{segComparison.pdf}}
  \vspace{-0.0cm}
  \caption{Segmentation result of U-Net, Attention U-Net, R2U-Net, Channel U-Net, U-Net++, UGS-Net-V1, and UGS-Net.
  The red boxes corresponding to the \textbf{\emph{Input}} column indicate the features that should be noted or the error-prone locations of the nodules during segmentation.
  The red boxes in the \textbf{\emph{UGS-Net}} column indicate the segmentation detail of UGS-Net at these locations.
  The green boxes indicate the inadequacies in the suboptimal segmentation result.}
  \vspace{-0.0cm}
  \label{segComparison}
\end{figure}

Figure~\ref{segComparison} shows the partial segmentation results of the above methods.
The red boxes in the \textbf{\emph{Input}} column indicate the features that should be noted or the error-prone segmentation locations of the nodules.
There are a large number of low-density regions in nodules (a)-(c), some irregular spiculation signs at the boundary s of nodules (d)-(f), and nodules (g)-(h) have cavities.
The segmentation of these areas by UGS-Net was significantly more consistent with the actual shape of lesions than other methods.

Figure~\ref*{segComparison} and Table~\ref*{tabel1} demonstrate that: (1) The learning of the annotation set and its union and intersection contributes to richer visual information for the segmentation task. (2) The learning of LC areas improves the model's ability to recognize low-dense regions.

\begin{figure*}[h]
  \centerline{\includegraphics[width=180mm]{Experi_MCM.pdf}}
  \vspace{-0.0cm}
  \caption{The predicted intersection $\cap(GT)'$, predicted union $\cup(GT)'$, final segmentation, and MCM are generated by the UGS-Net.
  Colors in MCM are used for marking, red for $\cap(GT)'$ and blue for $\cup(GT)'$.
  In addition, final segmentation is represented in the MCM and marked with green to facilitate comparison.
  Red circles indicate areas or features of nodules that are not easily distinguishable.}
  \vspace{-0.0cm}
  \label{Experi_MCM}
\end{figure*}
\subsection{Uncertain Region Prediction}
Besides conventional lung nodule segmentation, the UGS-Net can also predict which region is more likely to be nodule tissues, and which region is lower likely.
Figure~\ref{Experi_MCM} shows the prediction results $\cap(GT)'$ and $\cup(GT)'$, final segmentation result $X_{S}$, and the generated $MCM$.
In $MCM$ and $MCM+UGS-Net$, red represents the high-confidence region, blue represents the low-confidence region, and green is the final segmentation $X_{S}$.
In the ideal situation, the $X_{S}$ should achieve a balance between the high-confidence regions and low-confidence regions.

As can be intuitively seen from Figure~\ref*{Experi_MCM}, our final segmentation result is between the two extreme possibilities of high and low confidence regions, which is the most reasonable nodule region generated after learning all information.
Our model can have this performance because: (1) the prediction is aware of all possible annotations, (2) the prediction is constrained between $\cap(GT)'$ and $\cup(GT)'$. 

\begin{figure}[h]
  \centerline{\includegraphics[width=100mm]{MCM_HU.png}}
  \vspace{-0.0cm}
  \caption{Comparison of HU value kernel density estimation of real HC, LC, and predicted HC and LC.}
  \vspace{-0.0cm}
  \label{MCM_HU}
\end{figure}
Specifically, MCM has the following advantages in feature segmentation of lung nodules, which can better display the semantic features of nodules:
(1) MCM can better show the significant cavity features of nodules (Figure\ref{Experi_MCM}.(f)(i)).
It can be clearly seen from the figure that, compared with other methods, the nodule cavity features are more obvious on the prediction mask with UGS-Net.
This is because UGS-Net can better capture the density difference of nodule tissue under the guidance of intersection mask $\cap(GT)$ and union mask $\cup(GT)$ to retain more features of the nodule cavity.
(2) MCM can better segment spiculation signs, which is an important feature to help diagnose benign and malignant lung nodules (Figure\ref{Experi_MCM}.(a)-(f)).
The spiculation is a stellate distortion caused by the intrusion of nodules into surrounding tissue, which is low-dense and distributed around the nodule edges. The spiculation signs are usually low-dense, so they are difficult to segment by deep learning methods.
UGS-Net can pay more attention to nodule boundary characteristics under the guidance of union mask $\cup(GT)$, so it naturally has better segmentation performance for spiculation distributed at the boundary of a nodule.
(3) MCM can better segment low-density tissue of nodules (Figure\ref{Experi_MCM}.(j)-(m)).
Low-density tissues, which are commonly found in the Ground-glass nodules, are the main areas that cause differences in expert labeling.
UGS-Net can identify low-density tissue to the maximum extent through the study of the annotation set $GT$ and union mask $\cup(GT)$, which is very helpful for the ground-glass nodule diagnosis.

Due to the small area of the LC mask, quantitative evaluation indexes, such as Dice and IoU, cannot directly measure the prediction quality of MCM.
Therefore, we calculate the HU distribution of the predicted HC and LC mask and compare it with the HU distribution of the actual HC and LC mask.
Suppose UGS-Net can reasonably predict the degree of uncertainty in different regions. In that case, the HU value distribution of predicted HC and LC masks should be similar to the actual distribution.
As shown in Figure \ref{MCM_HU}, the predicted curve is basically consistent with the actual curve, indicating that the level of regional uncertainty we predicted is statistically convincing.

\subsection{Ablation Study}
In this section, we show the effectiveness of each module.
To better use the multiple annotations' information and enhance the relationship among $\cap(GT)'$, $\cup(GT)'$ and $R_{final}$, we update the UGS-Net-V1 with Multiple Annotation Fusion Loss and Intersection-Union Constraining Module.
To further demonstrate the contribution of these two modules, we constructed UGS-$\Phi_a$, UGS-$\Phi_b$, UGS-$\Phi_a+\Phi_b$, and UGS-IUCM for ablation experiments on the basis of UGS-Net-V1.
In UGS-$\Phi_a$, the Multiple Annotation Fusion Loss is only applied to the $R_{uni}$;
In UGS-$\Phi_b$, the Multiple Annotation Fusion Loss is only applied to the final output of the model $X_S$;
In UGS-$\Phi_a+\Phi_b$, the Multiple Annotation Fusion Loss is applied to both the $R_{uni}$ and $X_S$;
In UGS-IUCM, we use the Intersection-Union Constraining Module in the USG-Net-V1, but no Multiple Annotation Fusion Loss.

\begin{table}[h]
  \renewcommand\arraystretch{1.2}
  \vspace{-0.0cm}
  \caption{Ablation Study for Modules.}
  \centering
  \begin{tabular}{ccccccc}		
  \hline
  \hline
  \makebox[0.0003\textwidth][c]{\textbf{\emph{V1}}} & \makebox[0.0003\textwidth][c]{\textbf{\emph{$\Phi_{a}$}}} & \makebox[0.0003\textwidth][c]{\textbf{\emph{$\Phi_b$}}} & \makebox[0.01\textwidth][c]{\textbf{\emph{IUCM}}} & \makebox[0.03\textwidth][c]{\textbf{\emph{$Dice$}}} & \makebox[0.03\textwidth][c]{\textbf{\emph{$IoU$}}} & \makebox[0.03\textwidth][c]{\textbf{\emph{$NSD$}}} \\
  \hline
   &  &  &  & 86.67 $\pm$ 0.65 & 77.33 $\pm$ 0.88 & 94.94 $\pm$ 0.94 \\
  \checkmark &  &  &  & 87.16 $\pm$ 0.69 & 78.04 $\pm$ 1.00 & 95.28 $\pm$ 0.90 \\
  \checkmark & \checkmark &  &  & 87.42 $\pm$ 0.59 & 78.42 $\pm$ 0.87 & 95.56 $\pm$ 0.73 \\
  \checkmark &  & \checkmark &  & 87.33 $\pm$ 0.48 & 78.34 $\pm$ 0.70 & 95.44 $\pm$ 0.53 \\
  \checkmark & \checkmark & \checkmark &  & 87.62 $\pm$ 0.54& 78.75 $\pm$ 0.82 & \textbf{95.84 $\pm$ 0.61} \\
  \checkmark &  &  & \checkmark & 87.09 $\pm$ 0.59 & 77.97 $\pm$ 0.93 & 95.34 $\pm$ 0.69 \\
  \checkmark & \checkmark & \checkmark & \checkmark & \textbf{87.65 $\pm$ 0.57}& \textbf{78.78 $\pm$ 0.83} & 95.62 $\pm$ 0.59 \\
  \hline
  \hline
  \end{tabular} 
  \label{ablation}
  \vspace{-0.0cm}
\end{table}

The performance of our UGS-Net and four variants are listed in Table~\ref{ablation}. 
In Table~\ref{ablation}, the network without any module is the backbone network of this paper, Attention U-Net.
V1 stands for UGS-Net-V1. If other modules are checked, this module is added on the basis of the UGS-Net-V1.
The results show that: 
(1) The performance gains of the UGS-$\Phi_a$ model and UGS-$\Phi_b$ model relative to the UGS-Net model indicate that the fusion of all annotated information can make the network capture the nodule region more accurately and obtain better segmentation performance;  
(2) Dice, IoU, and NSD of UGS-$\Phi_a+\Phi_b$ model are higher than those of the UGS-$\Phi_a$ model and UGS-$\Phi_b$ model, indicating that simultaneous use of Multiple Annotation Fusion Loss for UAM and the final output was better than single use.  
(3) Our UGS-Net is superior to UGS-$\Phi_a+\Phi_b$ and UGS-IUCM, which proved the validity of the Intersection-Union Constraining Module. Although there is a slight quantitative decline in model performance when only the Intersection-Union Constraining Module is added (UGS-IUCM), there is a significant qualitative improvement, which will be discussed later. Meanwhile, the performance advantage of UGS-Net also indicates that Multiple Annotation Fusion Loss and Intersection-Union Constraining Module promote each other, which constrains uncertainty while learning it, helping the network to achieve better segmentation performance.

\begin{figure*}[h]
  \centerline{\includegraphics[width=180mm]{AblationFig.pdf}}
  \vspace{-0.0cm}
  \caption{Visualization of different convolutional layers.
  The \textbf{\emph{Result}} in each case represents the final prediction of the network.
  \textbf{\emph{M3}}, \textbf{\emph{M2}}, and \textbf{\emph{M1}} respectively represent the visual feature maps of the third from the bottom, the second, and the first convolution layer under different network configurations.
  Red arrows indicate areas of nodules that need attention.}
  \vspace{-0.0cm}
  \label{F2}
\end{figure*}

To further verify the validity of the Multiple Annotation Fusion Loss and Intersection-Union Constraining Module, we demonstrate feature map visualization using Grad CAM in Figure~\ref{F2}.
The segmentation process of five nodules is shown in the Figure~\ref{F2}.
The \textbf{\emph{Result}} in each case represents the final prediction of the network.
\textbf{\emph{M3}}, \textbf{\emph{M2}}, and \textbf{\emph{M1}} respectively represent the visual feature maps of the third from the bottom, the second, and the first convolution layer under different network configurations.
Based on Figure~\ref{F2}, we can see that:
(1) When Multiple Annotation Fusion Loss is applied to the $GT_{uni}'$ or $X_S$, the recognition ability of the network for the low-density organization was significantly improved (UGS-$\Phi_a$ model and UGS-$\Phi_b$ model, Nodule1-4);
(2) The simultaneous use of Multiple Annotation Fusion Loss in $GT_{uni}'$ and $X_S$ can make the network outline the nodule boundary more clearly on the basis of improving the sensitivity to low-density tissues (UGS-$\Phi_a+\Phi_b$ model, Nodule1-4).
(3) Intersection-Union Constraining Module enables the network to learn more boundary features, such as the spiculation (UGS-IUCM model, Nodule1-3).   
(4) When Multiple Annotation Fusion Loss and Intersection-Union Constraining Module are used at the same time, the network's attention shifts to the nodule boundary, delineating a more reasonable and complete nodule region (UGS-Net, Nodule1-5).


\subsection{Complex-Nodule Validation}

\begin{table*}[h]
  \renewcommand\arraystretch{1.2}
  \vspace{-0.0cm}
  \caption{Analysis of complex lung nodule.}
  \centering
  \begin{tabular}{c|cc|cc|cc|cc|cc|cc}
  \hline  
  \hline
  & \multicolumn{2}{c}{\textbf{\emph{Fold1}}} & \multicolumn{2}{c}{\textbf{\emph{Fold2}}} & \multicolumn{2}{c}{\textbf{\emph{Fold3}}} & \multicolumn{2}{c}{\textbf{\emph{Fold4}}} & \multicolumn{2}{c}{\textbf{\emph{Fold5}}} & \multicolumn{2}{c}{\textbf{\emph{Average}}}\\
  \hline
  & \textbf{\emph{Dice}} & \textbf{\emph{IoU}} & \textbf{\emph{Dice}} & \textbf{\emph{IoU}} & \textbf{\emph{Dice}} & \textbf{\emph{IoU}} & \textbf{\emph{Dice}} & \textbf{\emph{IoU}} & \textbf{\emph{Dice}} & \textbf{\emph{IoU}} & \textbf{\emph{Dice}} & \textbf{\emph{IoU}} \\
  \hline
  &\multicolumn{12}{c}{\textbf{\emph{U-Net Dice $\leq$ 60$\%$}}} \\
  \hline
  \textbf{\emph{U-Net}} & 46.24 & 30.55 & 51.51 & 35.00 & 52.18 & 35.35 & 46.11 & 30.84 & 46.6 & 30.96 & 48.53 & 32.36 \\
  \textbf{\emph{UGS-Net}} & 70.42 & 55.5 & 56.95 & 41.57 & 63.17 & 47.68 & 46.64 & 32.52 & 60.59 & 44.94 & 59.55\textcolor{green}{$\uparrow11.03$} & 44.44\textcolor{green}{$\uparrow11.88$} \\
  \hline
  &\multicolumn{12}{c}{\textbf{\emph{U-Net Dice $\leq$ 70$\%$}}} \\
  \hline
  \textbf{\emph{U-Net}} & 56.88 & 40.78 & 61.37 & 44.79 & 61.54 & 44.83 & 61.08 & 44.78 & 59.53 & 43.24 & 60.08 & 43.68 \\
  \textbf{\emph{UGS-Net}} & 74.09 & 60.02 & 69.32 & 54.74 & 72.02 & 57.62 & 65.90 & 50.99 & 70.60 & 56.05 & 70.39\textcolor{green}{$\uparrow10.31$} & 55.88\textcolor{green}{$\uparrow11.20$} \\
  \hline
  &\multicolumn{12}{c}{\textbf{\emph{U-Net Dice $\leq$ 80$\%$}}} \\
  \hline
  \textbf{\emph{U-Net}} & 70.04 & 55.00 & 72.58 & 57.52 & 72.60 & 57.50 & 70.71 & 55.46 & 71.42 & 56.30 & 71.47 & 56.36 \\
  \textbf{\emph{UGS-Net}} & 79.11 & 66.21 & 77.16 & 63.63 & 77.27 & 63.70 & 74.49 & 60.64 & 77.53 & 64.00 & 77.08\textcolor{green}{$\uparrow5.61$} & 63.64\textcolor{green}{$\uparrow7.28$} \\
  \hline
  \hline
  \end{tabular} 
  \label{complexNoduleTable}
  \vspace{-0.0cm}
\end{table*}

\begin{figure*}[htbp]
  \centerline{\includegraphics[width=160mm]{complexNodule.pdf}}
  \vspace{-0.0cm}
  \caption{Complex-Nodule Validation. This validation tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net by three levels.}
  \vspace{-0.0cm}
  \label{complexNodule}
\end{figure*}

The UGS-Net learns features from regions that may cause segmentation uncertainty, as a result, it can better segment nodules with large low-dense region or complex structures. To better demonstrate its improvement on nodule that are difficult to be segmented by U-Net, we design a Complex-Nodule Validation.
Based on five-fold validation U-Net, we further pick out three groups of nodules which have Dice scores lower than 60$\%$, 70$\%$, and 80$\%$, then train a UGS-Net with the same data settings, test these nodules again, and compare the Dice and IoU.

The data of these nodules on U-Net and UGS-Net are shown in Table~\ref{complexNoduleTable}.
On the whole, we can see that the improvement of UGS-Net on these complex nodules is very significant.
For nodules with Dice score less than 60$\%$ on U-Net, the average improvement on UGS-Net is 11.03$\%$. 
For nodules with Dice score less than 70$\%$ on U-Net, UGS-Net increased by 10.31$\%$ on average.
For nodules with less than 80$\%$ Dice score on U-Net, UGS-Net improved by 5.61$\%$ on average.
According to Table~\ref*{tabel1}, compared to U-Net, our model improve the Dice by 1.39. It seems to conflict with the fact that our model has the highest performance improvement on these nodule with Dice score less than 60$\%$ on U-Net. It is because nodules with complex structures are only a small part of all data. 

Figure \ref{complexNodule} shows the segmentation results of some complex nodules. 
The red subscript corresponds to the segmentation results of UGS-Net, and the black subscript corresponds to the segmentation results of U-Net. The subscript unit is the percentage.
It can be seen that nodules with U-Net segmentation Dice score less than 60$\%$ are some low-density or ground glass tissues. The significant improvement of UGS-Net in these nodules can indicate that UGS-Net can better learn the characteristics of nodules' low-density tissue and more accurately segment low-density nodular lesions regions.
When the U-Net segmentation Dice score is less than 70$\%$, it can be observed that in addition to some low-density nodules, some nodules also have irregular cavities or spiculation, abrupt bright spots in the tissues, or over-bright lung walls.
The convincing segmentation performance of UGS-Net on these nodules reflects the learning ability of UGS-Net on boundary features, density differences and good anti-noise ability.

\section{Discussions}
\label{discussions}
The lung nodule segmentation task has pursued accuracy for a very long time. Achieving high DSC or IoU seems to be the only target. However, lung nodules are very small, and some have complex structures. Compared to providing accurate segmentation for all possible tissues, pointing out regions with different segmentation confidences is more helpful for radiologists. The high-confidence regions provide the major part of nodules or tumor tissues, and the low-confidence regions provide the low-dense features, such as ground-glass features and spiculation signs, which are also features that radiologists should pay more attention to.

The proposed method does not take improving the DSC as the only target. Rather than replacing the clinical role of doctors, our method tries to provide information that is more useful for clinical diagnosis, letting the radiologists play their role. We believe it might be a better way to let artificial intelligence methods participate in clinical practice.

The limitation of our study is that we only evaluate our method on LIDC-IDRI. However, in the field of lung nodule public dataset, LIDC-IDRI is the only one that has been fully annotated. LIDC-IDRI has annotations for malignant level, eight medical features, location, and multiple segmentation annotations. Although this dataset has been released over ten years, there are still some aspects that can be studied on this dataset.

\section{Conclusions}
\label{conclusion}
  This paper proposes an Uncertainty-Aware Attention Mechanism (UAAM), which utilizes consensus or disagreements among multiple annotations to produce better segmentation and regions with low segmentation confidence. 
  In UAAM, we propose a Multi-Confidence Mask (MCM), which is a combination of a Low-Confidence (LC) Mask and a High-Confidence (HC) Mask. 
  Based on UAAM, we further design an Uncertainty-Guide Segmentation Network (UGS-Net), which contains a \emph{Feature Extracting Module}, a \emph{Uncertainty-Aware Module}, and a \emph{Intersection-Union Constraining Module}. These modules together learn valuable information from the consensus or disagreements among multiple annotations, providing regions with high and low segmentation confidences, and a segmentation result that can balance all possibilities.
  Besides the traditional validation method, we propose a Complex Nodule Challenge on LIDC-IDRI, which tests UGS-Net's segmentation performance on the lung nodules that are difficult to segment by U-Net. Experimental results demonstrate that our method can significantly improve the segmentation performance on nodules with poor segmentation by U-Net.
  


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
  \bibliographystyle{IEEEtran}
  % argument is your BibTeX string definitions and bibliography database(s)
  \bibliography{ijcai20}


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:


% if you will not have a photo at all:


% insert where needed to balance the two columns on the last page with
% biographies
%\newpage



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


