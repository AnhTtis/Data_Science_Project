% Template for ICIP-2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass[onecolumn,a4paper,IEEEtrans]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath,graphicx}
\usepackage{mathtools, nccmath} %https://tex.stackexchange.com/questions/433101/rounding-to-nearest-integer-symbol-in-latex
\usepackage{blindtext}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{lipsum}
\newsavebox{\codebox}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\definecolor{lightgreen}{RGB}{200,210,200}
\newcommand{\mycc}{\cellcolor{lightgreen}}



% definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}



\title{Severity classification of ground-glass opacity via \\2-D convolutional neural networks and lung CTs:\\a 3-day exploration}
\date{}
\author{Lisa Y.~W. Tang}

% Single address.
% ---------------

%\address{Author Affiliation(s)}
%
% For example:
% ------------

%\address{lisat.wyw@gmail.com}
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}


\begin{document}
\maketitle
%\ninept

\begin{abstract}
Ground-glass opacity is a hallmark of numerous lung diseases, including patients with COVID19 and pneumonia. This brief note presents experimental results of a proof-of-concept framework that got implemented and tested over three days as driven by the third challenge entitled "COVID-19 Competition", hosted at the AI-Enabled Medical Image Analysis Workshop of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023). Using a newly built virtual environment (created on March 17, 2023), we investigated various pre-trained two-dimensional convolutional neural networks (CNN) such as Dense Neural Network, Residual Neural Networks, and Vision Transformers, as well as the extent of fine-tuning. Based on empirical experiments, we opted to fine-tune them using ADAM's optimization algorithm with a standard learning rate of 0.001 for all CNN architectures and apply early-stopping whenever the validation loss reached a plateau. For each trained CNN, the model state with the best validation accuracy achieved during training was stored and later reloaded for new classifications of unseen samples drawn from the validation set provided by the challenge organizers. According to the organizers, few of these 2D CNNs yielded performance comparable to the baseline developed by the organizers. As part of the challenge requirement, the source code produced during the course of this exercise is posted at \url{https://github.com/lisatwyw/cov19}. We also hope that other researchers may find this light prototype consisting of few Python files based on PyTorch 1.13.1 and TorchVision 0.14.1 approachable.
 

\end{abstract}
\vspace{1em}
\emph{Keywords:
%Keywords: \begin{IEEEkeywords}
Computed Tomography; pulmonary parenchymal involvement; ground-glass opacity severity; DenseNet; Residual Network; Inception; Wide Residual Network; VisionTransform; SqueezeNet; VGG; AlexNet}
%\end{keywords}
%\end{IEEEkeywords}


\DeclarePairedDelimiter{\nint}\lfloor\rceil
\DeclarePairedDelimiter{\abs}\lvert\rvert %https://tex.stackexchange.com/questions/433101/rounding-to-nearest-integer-symbol-in-latex


\section{Introduction}
\label{sec:intro}

Ground-glass opacity (GGO) is a hallmark of numerous lung
diseases and typically signifies lung consolidation.


This manuscript documents the initial experimental work done on the open-source dataset named ``COV19CT-DB'' (COVID-19 Computed Tomography Database) as part of a challenge submission to the third competition entitled  ``ICASSP: COVID19 severity detection challenge''.

%Thanks to the plethora of open-source frameworks and software packages, deep and wide convolutional neural networks (CNNs) may be trained with and without fine-tuning, thus rendering this overly ambitious effort feasible. 



Numerous studies \cite{tang2020,shaik2022transfer} from the past five years have shown the success of transfer learning of CNNs trained using three-channel two-dimensional inputs. Accordingly, we approach severity classification of GNN similarly. To this end, we explored the use of various CNNs architectures: AlexNet \cite{alexnet}, VGG \cite{vgg}, Residual Networks \cite{resnet},  Vision Transformers \cite{vtb32} and DenseNet \cite{densenet}.

\section{Materials}

The training and validation samples is derived as a subsest of a larger dataset named ``COV19-CT-DB'' that dataset contains over one thousand patient samples of three-Dimensional CT chest scans as archived in ``lossy'' image format (i.e. JPG). As further elaborated in previous articles \cite{kollias2022ai,arsenos2022large,kollias2021mia,kollias2020deep,kollias2020transparent,kollias2018deep}, these CT volumes were collected from different hospitals in the United States. 

Due to time constraints (i.e. three days), this document focuses on the severity classification problem where each CT volume will be labeled as either ``mild'', ``moderate'', ``severe'', or ``critical'' \cite{kollias2022ai}. These categories were predetermined by a pre-existing protocol where experts visually inspected each scan for lung disease morphology known as \emph{groundglass opacity} (GGO) and manually labeled each as mild if none or fewer than 26\% of the lungs were judged to capture ``pulmonary parenchymal involvement'' (see Figures 1-2 for examples);  ``moderate'' if the involvement is 26-50\%; ``severe'' if involvement is 50-70\%; or ``critical'' if involvement is greater than 75\% \cite{morozov2020mosmeddata}. 




\section{Methods}
\label{sec:methods}
We propose the deployment of fine-tuned convolutional neural networks (CNNs) for the classification task.  
For ease of optimization, our approach explores a previous framework that leveraged two-dimensional  Residual Networks \cite{tang2020} for the identification of a lung disease. The source code developed during the course of this experimental prototyping period is posted at \href{https://github.com/lisatwyw/cov19}{https://github.com/lisatwyw/cov19}.

\subsection{Preprocessing}

As the information on field of volume and voxel spacing is no longer accessible in the archival format provided in the challenge (i.e. JPG), the depth of the volume was approximated by the file counts $n$ of each scan / folder. 

The axial section centered at $z$ was computed deterministically ($z=\nint{n*.25}$) based on parameters which were set based on visual inspection on a subset of the images randomly drawn from the training set. Then, three contiguous slices centered at $z$ were concatenated to form three-channel inputs of size $3 \times b \times v$ where $v$ depends on the CNN model \cite{tang2020}.

Lung masks were next computed to retain only regions of the lungs (i.e. regions outside of the rib cage masked out). The implementation code of lung mask generation was adopted from online resource \cite{kaggle} such that the threshold parameter  is adjusted so it will operate on the compressed data as stored in the JPG images from COV19-CT-DB, i.e.:

  
\begin{lrbox}{\codebox}
\begin{lstlisting}[language=python,basicstyle=\small]
from skimage.segmentation import clear_border
init_roi_mask = jpeg_im_slice<100 # Updated 
roi_mask_v0 = clear_border(init_roi_mask)
...
\end{lstlisting}
\end{lrbox}


\noindent
\fbox{%
  \begin{minipage}{\dimexpr\linewidth-2\fboxsep-2\fboxrule}
  \usebox{\codebox}    
  \end{minipage}%
}

%[language=pascale,numbers=left, numberstyle=\small, stepnumber=2, numbersep=5pt]



\subsection{Model-training and selection}

Each model architecture was fine-tuned over a maximum of 500 epochs. We used the categorical cross-entropy objective and optimization algorithm with a standard learning rate of 0.001 for all CNN architectures and applied early-stopping whenever the validation loss reached a plateau. Two optimization algorithms explored were Adams and Stochastic Gradient Descent (SGD). For SGD, the standard setting of using momentum value of 0.9 and weight decay of 0.0001 was used.

The following settings were tuned from a selection of choices: batch size BS=$\{16, 128\}$, LR=$\{0.001, 0.01\}$. Further, to enhance robustness against deformations and scale transformations, we explored the application of random  horizontal flips and random rigid transformations, applied at 50\% of the times (probability of 0.5). 

For each trained CNN, the state with the best validation accuracy achieved during training was selected for the evaluation of each test sample.


\subsection{Extent of fine-tuning}
We initialized each CNN with pretrained weights and subsequently explored two level of network fine-tuning: allowing the network weights of all layers or only the last layer to be changed/optimized.  

\section{Results}

\textbf{This section is still being updated}. 
\\
\\
Table 1 and 2 report preliminary comparisons of the accuracies (F1-macro score) achieved by individual models as evaluated on the unseen validation set. 

\begin{table}[h!]
\begin{center}\small
\begin{tabular}{l | l l | l  l  | l  | l  | l } 
 \hline
 &\multicolumn{2}{c|}{Key settings} & \multicolumn{2}{c|}{Overall F1} &\multicolumn{2}{c|}{Class-wise F1} & \\  \hline
Model & FT  & BS & Val & Unseen* & Val & Unseen* & Pred. distr. \\ \hline
AlexNet \cite{alexnet} & \\ \hline
DenseNet \cite{densenet} & last & 512 & 56.5& 62.2 & &  & 110, 33, 86, 2\\ \hline
DenseNet201 &  last & 16 & 63.2 & 67.9 & & & 105, 51, 73, 2 \\ \hline
DenseNet201 & last & 512 & 57.2 &  64.3 & 100, 78.7, 72.6, 76.4 &  100, 87, 71.9, 77.6 & 91, 78, 51, 11  \\ \hline
InceptionNet \cite{inception}& all & 16 && & & & \\ \hline
InceptionNet \cite{inception}& last & 16 &  & & 100, 67.1, 49.7, 55.2 & 100, 67.6, 49.0, 56.4 &   \\ \hline
SqueezeNet \cite{squeezenet} &  all & 16 & & & & & 231, 0, 0, 0\\ \hline
ResNet152 \cite{resnet} &  last & 512 & 79.6 & 75.2 &   &  & 84, 102, 43, 2 \\ \hline
ResNet152 \cite{resnet} &  last & 64 & 54.0 & 53.6 &   &  & 110, 81, 40, 0 \\ \hline
ResNet152 \cite{resnet} &  last & 32 & 54.6 & 50.6 &   &  & 136, 64, 31, 0 \\ \hline
ResNet152 \cite{resnet} &  all & 16 & 100.0& 100.0& & & \\ \hline
VGG \cite{vgg} & last & 16 & 92.6 & 91.6 &  &  & 198, 31, 2, 0 \\ \hline
VGG \cite{vgg} & last &  512 & 94.0 &94.9 & & & 176, 54, 1, 0\\ \hline
VTB32 \cite{vtb32} & last & 16 & 71.9 & 68.7 & 
100, 94.4, 78.2, 77.3 &
 &
 172, 0, 59, 0 \\ \hline
\end{tabular}
\caption{Comparative analysis of different fine-tuned CNNs under two key settings. Shown are the overall and class-wise F1-scores expressed in percentages. *Performance of CNNs when evaluated on unseen validation set. ``FT'' denotes the extent of fine-tuning: network weights from all layers or only the last layer were optimized. ``Pred. distr.'' denotes the predicted class distribution on the test set. \textbf{Please note that this table is still being updated}.  }
\end{center}
\end{table}

\begin{table*}[h!]
\begin{center}\small
\begin{tabular}{l | l l | l  l  | l  | l  | l } 
 \hline
 &\multicolumn{2}{c|}{Key settings} & \multicolumn{2}{c|}{Overall F1} &\multicolumn{2}{c|}{Class-wise F1} & \\  \hline
 Model &  $BS=32$ &  & Val & Unseen* & Val & Unseen* & Pred. distr. \\ \hline
AlexNet & & & 13.8 & 15.4 & & & 0, 0, 231, 0\\ \hline 
DenseNet & & & 59.7 & 61.7 & & & 104, 38, 89, 0 \\ \hline
\mycc Inception & & & 50.4 & 56.9 & 100, 74.2, 58.6, 60.9 & 100, 73.4, 62.6, 69.3 & 45, 35, 145, 6\\ \hline
\mycc DenseNet201 & & & 57.3 & 61.4 & 100, 81.9, 72.0, 72.0 & 100, 88.6, 62.1, 67.5 & 98, 59, 71, 3 \\ \hline
ResNet152 & & & 54.6 & 50.6 & 100, 84.0, 79.2, 76.8 & 100, 73.5, 78.2, 75.4 &  136, 64, 31, 0 \\ \hline
SqueezeNet  & & & 100 & 100 &  &   & 231, 0, 0, 0  \\ \hline

VGG & & & 94.0 & 91.8 & 100, 94.3, 91.2, 98.0 & 
100, 96.5, 89.9, 97.0 & 194, 35, 2, 0 \\ \hline 
\mycc VTB32 & & & 73.0 & 70.2 & 100, 94.4, 78.2, 78.4 & 100, 96.5, 80.8, 84.0 &  202, 13, 13, 3\\ \hline 
Wide ResNet101  & & &  82.8 & 85.9 &  100, 93.3, 87.3, 91.2 & 100, 94.3, 89.3, 94.0 & 153, 62, 16, 0 \\ \hline
\end{tabular}
\caption{Comparative analysis of different fine-tuned CNNs under another set of hyper-parmeter settings. Batch size $BS=32$. \textbf{Please note that this table is still being updated}. }
\end{center}
\end{table*}

\iffalse 

\begin{center}
\small
\begin{tabular}{l l l l l l l} 
 \hline
 BS&8& 16 & 64 & 128 & 256 & 512 \\ 
 \hline\hline
AlexNet & 56.1 & 56.9 & 44.9  & 61.0 &56.2& 45.5 \\ \hline 
DenseNet121 & & 68.5 &  68.0 & 65.4 & 64.6 \\ \hline
DenseNet201 & & 79.1  & 76.8 & 62.4& 71.4 \\ \hline
InceptionV3 & 39.2&35.0 &&& 47.9& 51.2 \\ \hline  
ResNet152  & &81.6 & 81.6 & &  95.4\\ \hline
SqueezeNet & 56.1 &58.0 & 39.7** & & 50.8 &44.2 \\ \hline
WideRes101 & 85.9 & 91.9 & - & 83.2,91.9 & 87.9 & 72.8  \\ \hline
VGG11 & & 100.0 & - & 100.0 & 100.0 &  N/A* \\ \hline
VTB32 & & 69.6 & 72.6 & 69.5 &  &  \\ \hline
\end{tabular}
\end{center}

 \fi 
 
\begin{figure*}[t]\centering
\includegraphics[width=1\textwidth]{val_covid_ct_scan_146_category1.png}
\caption{Example training input used to fine-tune CNNs.}
\label{fig:in}
\end{figure*}



               
\begin{figure*}[t]\centering
\includegraphics[width=1\textwidth]{test_test_ct_scan_74_category-1.png}
\caption{Example test input.}
\label{fig:in2}
\end{figure*}

\begin{figure*}[t]\centering
\includegraphics[width=1\textwidth]{val_covid_ct_scan_61_category3.png}
\caption{Another training input drawn from the severe class without application of the center-cropping.}
\label{fig:in3}
\end{figure*}


\section{Conclusion}

In this brief note, we shared empirical data that explored the feasibility of severity classification without the deployment of three-dimensional neural networks. We hope that other researchers may find this quick prototype consisting of few Python files based on
PyTorch 1.13.1 and TorchVision 0.14.1 approachable.
\\
\\
\noindent\textbf{\large{Acknowledgements}}
\\
\\
The author sincerely thank Professor Dimitrios Kollias and the organizing committee for provisioning the COV19-CT-DB dataset and hosting this exciting challenge \cite{kollias2022ai,arsenos2022large,kollias2021mia,kollias2020deep,kollias2020transparent,kollias2018deep}. The author would also like to express gratitude to Compute Canada/Alliance Canada as well as Tong Tsui Shan and Kim Chuen Tang for their in-kind support.


\begin{thebibliography}{1}


\bibitem{arsenos2022large} Anastasios Arsenos, Dimitrios Kollias, and Stefanos Kollias, “A large imaging database and novel deep neural architecture for covid-19 diagnosis,” in 2022 IEEE 14th Image, Video, and Multidimensional Signal Pro-
cessing Workshop (IVMSP). IEEE, 2022, p. 1–5.

\bibitem{kollias2022ai} Dimitrios Kollias, Anastasios Arsenos, and Stefanos Kollias, “Ai-mia: Covid-19 detection \& severity
analysis through medical imaging,” arXiv preprint
arXiv:2206.04732, 2022.


\bibitem{kollias2021mia} Dimitrios Kollias, Anastasios Arsenos, Levon Soukissian, and Stefanos Kollias, “Mia-cov19d:
Covid-19 detection through 3-d chest ct image anal-
ysis,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021, pp. 537–544.

\bibitem{kollias2020deep} Dimitrios Kollias, N Bouas, Y Vlaxos, V Brillakis, M Seferis, Ilianna Kollia, Levon Sukissian, James
Wingate, and S Kollias, “Deep transparent prediction
through latent representation analysis,” arXiv preprint
arXiv:2009.07044, 2020.

\bibitem{kollias2020transparent} Dimitris Kollias, Y Vlaxos, M Seferis, Ilianna Kollia, Levon Sukissian, James Wingate, and Stefanos D Kollias, “Transparent adaptation in deep medical image diagnosis.,” in TAILOR, 2020, p. 251– 267.

\bibitem{kollias2018deep} Dimitrios Kollias, Athanasios Tagaris, Andreas Stafylopatis, Stefanos Kollias, and Georgios Tagaris, “Deep
neural architectures for prediction in healthcare,” Complex \& Intelligent Systems, vol. 4, no. 2, pp. 119–131,
2018.

\bibitem{saha} Saha M, Amin SB, Sharma A, Kumar TKS, Kalia RK. AI-driven quantification of ground glass opacities in lungs of COVID-19 patients using 3D computed tomography imaging. PLoS One. 2022;17(3):e0263916. Published 2022 Mar 14. doi:10.1371/journal.pone.0263916

\bibitem{squeezenet} Iandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size." arXiv preprint arXiv:1602.07360 (2016).

\bibitem{inception}
Szegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. "Rethinking the inception architecture for computer vision." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818-2826. 2016.

\bibitem{wideresnet} 
Zagoruyko, Sergey, and Nikos Komodakis. "Wide residual networks." arXiv preprint arXiv:1605.07146 (2016).

\bibitem{resnet} He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.

\bibitem{alexnet} Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Communications of the ACM 60, no. 6 (2017): 84-90.

\bibitem{densenet} Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. "Densely connected convolutional networks." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708. 2017.

\bibitem{vtb32} Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).

\bibitem{vgg} Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).


\bibitem{morozov2020mosmeddata} Sergey P Morozov, Anna E Andreychenko, Ivan A
Blokhin, Pavel B Gelezhe, Anna P Gonchar, Alexan-
der E Nikolaev, Nikolay A Pavlov, Valeria Yu Chernina,
and Victor A Gombolevskiy, “MosMedData: data set of
1110 chest CT scans performed during the COVID-19
epidemic,” Digital Diagnostics, vol. 1, no. 1, pp. 49–59,
2020.


\bibitem{tang2020} Lisa YW Tang, Harvey O Coxson, Stephen Lam,
Jonathon Leipsic, Roger C Tam, and Don D Sin, “To-
wards large-scale case-finding: training and validation
of residual networks for detection of chronic obstructive
pulmonary disease using low-dose ct,” The Lancet Dig-
ital Health, vol. 2, no. 5, pp. e259–e267, 2020.

\bibitem{densenet} Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. "Densely connected convolutional networks." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708. 2017.
% chicago
% Shaik, Nagur Shareef, and Teja Krishna Cherukuri. "Transfer learning based novel ensemble classifier for COVID-19 detection from chest CT-scans." Computers in Biology and Medicine 141 (2022): 105127.

\bibitem{shaik2022transfer} Nagur Shareef Shaik and Teja Krishna Cherukuri, “Transfer learning based novel ensemble classifier for COVID-19 detection from chest ct-scans,” Computers in
Biology and Medicine, vol. 141, pp. 105127, 2022.


\bibitem{pytorch} Nathan Inkawhich, ``Finetuning Torchvision Models,'' \url{https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html}, last modified 2017, accessed Mar 17, 2023.

\bibitem{kaggle} Scott Mader ``DST lung segmentation algorithm,'' \url{https://www.kaggle.com/code/kmader/dsb-lung-segmentation-algorithm}, last modified 2017, accessed Mar 18, 2023. 

\end{thebibliography}

\end{document}
