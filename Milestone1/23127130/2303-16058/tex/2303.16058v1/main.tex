\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color, colortbl}
\usepackage{subcaption}
\usepackage{tabu}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{paralist}
\usepackage{threeparttable}
\usepackage{enumitem}
\usepackage{xcolor}


\def\Modelnamelight{UMT}
\def\Modelname{\textbf{UMT}}
\definecolor{Gray}{gray}{0.5}
\definecolor{LGray}{gray}{0.9}
\definecolor{darkblue}{RGB}{94,110,186}
\definecolor{darkGreen}{RGB}{92, 148, 110}
\definecolor{myblue}{RGB}{14, 121, 178}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\pink}[1]{\textcolor{magenta}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\darkGreen}[1]{\textcolor{darkGreen}{#1}}
\newcommand{\myblue}[1]{\textcolor{myblue}{#1}}
\newcommand{\darkblue}[1]{\textcolor{darkblue}{#1}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{***} % *** Enter the ICCV Paper ID here
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Unmasked Teacher: Towards Training-Efficient Video Foundation Models}

\author{
    Kunchang Li$^{1,2,3}$\thanks{Interns at Shanghai AI Laboratory. \textsuperscript{$\dag$}Corresponding authors.}\quad
    Yali Wang$^{1,3}$\textsuperscript{$\dag$}\quad
    Yizhuo Li$^{3,4}$\textsuperscript{*}\quad
    Yi Wang$^{3}$\quad
    Yinan He$^{3}$\\
    Limin Wang$^{3,5}$\quad
    Yu Qiao$^{1,3}$\textsuperscript{$\dag$}\vspace{0.2em}\\
    % \textsuperscript{\dag}
    \small{$^1$Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences}\\
    \small{$^2$University of Chinese Academy of Sciences\quad
    $^3$Shanghai AI Laboratory\quad
    $^4$The University of Hong Kong}\\
    \small{$^5$State Key Laboratory for Novel Software Technology, Nanjing University}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Video Foundation Models (VFMs) have received limited exploration due to high computational costs and data scarcity. 
Previous VFMs rely on Image Foundation Models (IFMs), 
which face challenges in transferring to the video domain. 
Although VideoMAE has trained a robust ViT from limited data, 
its low-level reconstruction poses convergence difficulties and conflicts with high-level cross-modal alignment.
This paper proposes a training-efficient method for temporal-sensitive VFMs that integrates the benefits of existing methods.
To increase data efficiency,
we mask out most of the low-semantics video tokens,
but selectively align the unmasked tokens with IFM,
which serves as the \textbf{U}n\textbf{M}asked \textbf{T}eacher (\textbf{UMT}).
By providing semantic guidance,
our method enables faster convergence and multimodal friendliness.
With a progressive pre-training framework,
our model can handle various tasks including scene-related, temporal-related, and complex video-language understanding. 
Using only public sources for pre-training in \textbf{6 days} on \textbf{32 A100} GPUs,
our scratch-built ViT-L/16 achieves state-of-the-art performances on various video tasks.
The code and models will be released at \url{https://github.com/OpenGVLab/unmasked_teacher}.
\end{abstract}

%%%%%%%%% BODY TEXT
\input{paragraphs/introduction.tex}
\input{paragraphs/related_work.tex}
\input{paragraphs/method.tex}
\input{paragraphs/experiments.tex}
\input{paragraphs/conclusion.tex}


% \clearpage
% \newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\newpage
\input{paragraphs/appendix.tex}

\end{document}

# data, model (text, image, video), 
# CL + MIM

# Data-efficient decoder
# Source-efficient, Time-efficient  computation
# multi-modality