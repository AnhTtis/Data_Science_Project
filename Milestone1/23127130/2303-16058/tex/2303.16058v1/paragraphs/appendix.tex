\appendix

% --- PDF will be split by an editor (e.g. macOS preview), so need to restart from page 1
% \setcounter{page}{1}

% --- repeat the title (AT: haven't found a more elegant way to do this...)
\twocolumn[
\centering
\Large
\textbf{Appendix}
\vspace{0.5em}
] %< twocolumn


\section{More results}


\begin{figure}[tp]
    \centering
    \begin{minipage}{\linewidth}
        \includegraphics[width=1.0\linewidth]{figs/epoch_ssv2.pdf}
        \vspace{-0.7cm}
        \subcaption{Results on SthSth V2}
    \end{minipage}
    \vspace{-0.1cm}
    \begin{minipage}{\linewidth}
        \includegraphics[width=1.0\linewidth]{figs/epoch_k400.pdf}
        \vspace{-0.7cm}
        \subcaption{Results on Kinetics-400}
    \end{minipage}
\caption{
\textbf{Training schedules.} 
A longer training schedule leads to more significant improvement.
}
\label{fig:ablation_epoch}
% \vspace{-0.1cm}
\end{figure}



\subsection{Other ablation studies}
We conduct more ablation studies based on ViT-B/16.
Results are shown in Figure \ref{fig:ablation_epoch}, Table \ref{tab:teacher} and Table \ref{tab:design_mm}.

\textbf{Training schedule.}
Figure \ref{fig:ablation_epoch} presents the results of different training schedules.
On one hand,
a longer training schedule consistently improves the performances on both benchmarks.
On the other hand,
compared to VideoMAE \cite{videomae},
our method shows a faster convergence speed.
For example,
when pre-training for 200 epochs,
our models achieve 3.9\% and 6.8\% top-1 accuracy on SthSth V2 and Kinetics-400, respectively.


\textbf{Different teachers.}
In Table \ref{tab:teacher},
we adopt different models \cite{dino,clip,beitv2} as the unmasked teachers.
As expected, 
the student models clearly outperform the corresponding teacher models, 
which have undergone elaborate fine-tuning. 
It's important to note that both student and teacher models share the same architecture, 
further emphasizing the effectiveness of our approach.



\input{tables/ablation_teacher.tex}

\input{tables/ablation_design_mm.tex}


\textbf{Other designs.}
Table \ref{tab:design_mm} showcases alternative designs for our multi-modality pre-training.
Firstly, 
we attempt to directly perform Stage 2 with a randomly initialized video encoder. 
For a fair comparison, 
we incorporate Kinetics-710 and conduct the same number of data iterations. 
However, 
the results demonstrate that the one-stage pre-training is challenging to converge, 
leading to poor performance. 
Secondly,
we randomly mask the video without an unmasked teacher for supervision,
which slightly reduces the overall performance. 
Additionally, 
we consider aligning the visual and text projection with the CLIP teacher,
since the teacher model also adopts contrastive learning. 
However, 
introducing extra alignment tasks turns out to be redundant and even harmful. 
Finally, 
we conduct extra pre-training without masks after masked pre-training.
Though it improves zero-shot performance (+1.5\% higher average recall accuracy),
the fine-tuned results are not as good as expected.

\subsection{Video-text retrieval}
Table \ref{tab:more_retrieval_zs} and Table \ref{tab:more_retrieval_ft} show more zero-shot and fine-tuned retrieval results on MARVTT \cite{msrvtt}, DiDeMo \cite{didemo}, ActivityNet \cite{anet}, LSMDC \cite{lsmdc} and MSVD \cite{msvd}.

\input{tables/more_retrieval_zs.tex}
\input{tables/more_retrieval_ft.tex}


\section{More implementation details}


\subsection{Model architecture and training details}
In this section,
we introduce the model architectures and training hyperparameters in our experiments.

\input{tables/model_architecture}

\textbf{Stage 1.}
In Stage 1,
we train the video encoder from scratch,
which is a vanilla ViT \cite{vit} without temporal downsampling.
We use the same patch size for both ViT-B and ViT-L,
\textit{i.e.}, $1$$\times$$16$$\times$$16$ ($T$$\times$$H$$\times$$W$).
To align with the unmasked teacher,
we use a simple linear projection,
including Layer Normalization \cite{ln} and one linear layer.
The example architecture is shown in Table \ref{tab:model_architecture}.
For pre-training,
we follow most of the hyperparameters in VideoMAE \cite{videomae},
as presented in Table \ref{tab:stage1_hyperparameters}.
However, 
to prevent overfitting, 
we use drop path \cite{droppath} in our approach.


\textbf{Stage 2.}
In Stage 2,
we equip the pre-trained video encoder with a text encoder and cross-modal decoder.
Following Singularity \cite{lei2022revealing},
for the base model,
we use the first 9 layers and the last 3 layers of BERT$_{base}$ to initialize the text encoder and recorder, respectively.
While for our large model,
we respectively adopt the first 19 layers and the 5 layers of BERT$_{large}$.
For pre-training,
we set all the loss weights to 1.
And more details are shown in Table \ref{tab:stage2_hyperparameters}.


\textbf{Action Recognition.}
We adopt the Stage-1 pre-trained video encoder and add an extra classification layer for fine-tuning.
Detailed hyperparameters for different datasets are shown in Table \ref{tab:ar_hyperparameters}.
In our experiments,
we have tried to fine-tune the Stage-2 pre-trained video encoder,
but the results on Kinetics are similar.

\textbf{Action Detection.}
Following VideoMAE \cite{videomae} and ST-MAE \cite{st_mae},
we add ROIAlign with MaxPooling to generate the regions of interest.
Since we the Kinetics pre-trained models adopt sparse sampling \cite{tsn},
we use a frame span of 300 for action detection,
which is the default frame number of Kinetics videos.
More details are listed in Table \ref{tab:ad_hyperparameters}.


\input{tables/stage1_hyperparameters}
\input{tables/stage2_hyperparameters}
\input{tables/ar_hyperparameters}

\textbf{Video-text retrieval.}
For fine-tuning,
we adopt the same architecture as in Stage 2,
but we only apply VTC and VTM losses.
For all datasets,
we sparsely sample 12 frames for both training and testing.
More details are listed in Table \ref{tab:ret_hyperparameters}.
For a fair comparison,
we follow Singularity \cite{lei2022revealing} to apply flip augmentation for SSV2 retrieval,
which may harm the performance of this temporal-related dataset.

\textbf{Video question-answering.}
Following the previous works \cite{lei2022revealing,Cheng2022VindLUAR,li2021align},
we formulate this task as text generation instead of classification.
We add an extra multi-modal decoder that takes the output of the cross-modal decoder as the keys/values.
And it decodes the answer text with ``[CLS]'' as a start.
We follow \cite{lei2022revealing,Cheng2022VindLUAR} to adopt the same architecture as the cross-modal decoder,
and initialize it using the pre-trained cross-modal decoder.
As for multiple-choice question-answering,
we follow \cite{lei2022revealing,li2021align,Cheng2022VindLUAR} to convert it to a text-to-video retrieval task,
where the question and candidate answers are concatenated.
The detailed hyperparameters are shown in Table \ref{tab:qa_hyperparameters} and Table \ref{tab:mc_hyperparameters}.


\input{tables/ad_hyperparameters}
\input{tables/qa_hyperparameters}
\input{tables/mc_hyperparameters}

\subsection{Dataset descriptions}
We show the statistics of pre-training datasets in Table \ref{tab:statics_pretrain},
and downstream datasets in Table \ref{tab:statics_downstream}.


\input{tables/ret_hyperparameters}
\input{tables/statistics_pretrain}
\input{tables/statistics_downstream}