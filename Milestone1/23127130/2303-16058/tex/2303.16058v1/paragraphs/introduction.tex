\section{Introduction}

\begin{figure}[tp]
    \includegraphics[width=1.0\linewidth]{figs/intro.pdf}
    \vspace{-0.5cm}
    \caption{
    \textbf{Comparison with SOTA methods.}
    ``ZS'' and ``FT'' refer to ``zero-shot'' and ``fine-tuned''.
    ``T2V'' means video-text retrieval.
    For Kinetics action recognition,
    \cite{mtv} and \cite{Wang2022InternVideoGV} are excluded since they utilize model ensemble.
    With only public sources for pre-training,
    our approach achieves SOTA performances on scene-related, \darkblue{temporal-related} and complex video-language benchmarks.
    Compared with CoCa \cite{coca},
    our method is much more environmentally friendly with \textbf{70$\times$} reduction in carbon emissions.
    }
    \label{fig:intro}
    \vspace{-0.4cm}
\end{figure}


\begin{figure*}[thp]
    \centering
    % \vspace{-0.3cm}
    \includegraphics[width=0.88\textwidth]{figs/framework.pdf}
    \vspace{-0.3cm}
    \caption{
    \textbf{Training-efficient framework for video foundation models.} 
    For general video understanding,
    we propose the \textit{progressive pre-training} with the unmasked teacher, which is \textit{simple, scalable and reproducible}. 
    The resulting models can not only handle scene-related and temporal-related actions well,
    but also conduct complex video-language understanding.
    }
    \label{fig:framework}
    \vspace{-0.3cm}
\end{figure*}

Video understanding has emerged as a critical skill for artificial intelligence systems to analyze and comprehend videos effectively. 
The progress in video understanding is currently driven by the Image Foundation Models (IFMs) \cite{vit,mae,beit,clip,align}, 
which are trained from massive datasets and adapted for different downstream tasks \cite{imagenet,coca,ade,fickr}. 
However, 
IFMs tend to focus more on scenes and objects, 
disregarding the essential motion patterns and object interactions required for complex video understanding. 
The \textit{true} Video Foundation Models (VFMs) are underexplored due to the high computational costs and data scarcity.

While building VFMs on well-learned IFMs reduces training costs, 
it poses significant challenges in transferring knowledge from the image domain to the video domain.
Firstly,
due to limited video data and a substantial domain gap,
video post-pretraining may undermine the generality inherited from IFMs \cite{xue2022clip}. 
Moreover, 
the strong spatial initialization offers a shortcut to perceive videos from scenes in single frames (\textit{e.g.}, ``grass'' in ``horse riding''),
which constrains VFMs from learning spatiotemporal relationships to recognize and localize \darkblue{temporal-related actions}, 
such as \darkblue{``opening" and ``closing"} in Figure \ref{fig:framework}.
Lastly, this paradigm is difficult to scale up as it requires well-prepared IFMs.

The recent success of VideoMAE \cite{videomae, st_mae} offers a data-efficient way to learn effective spatiotemporal features from scratch,
which handles complex temporal action recognition and detection tasks impressively. 
Nonetheless, 
its strong data efficiency and spatiotemporal modeling are traded by long pre-training (\textit{e.g.}, 2400 epochs on 160k videos). 
Besides, 
it is not well-suited for video-language tasks since the low-level pixel reconstruction task conflicts with high-level cross-modal alignment \cite{Shu2022MaskedCP}.
Additionally, 
the extra decoder that handles masked and unmasked tokens causes high memory costs due to global self-attention,
making scaling up this paradigm also challenging.

In this paper, 
we present a training-efficient method for temporal-sensitive VFMs by integrating the benefits of previous methods. 
Rather than directly adapting public IFM, \textit{e.g.}, CLIP \cite{clip}, 
we utilize them as \textbf{U}n\textbf{M}asked \textbf{T}eacher (\Modelname) to train vanilla ViT from scratch. 
We mask out most of the video tokens with low semantics and only align the unmasked tokens with a linear projection to the corresponding ones from the teacher. 
This approach not only inherits data efficiency from VideoMAE but also makes the learned video encoder multimodal-friendly (validated in Table \ref{tab:ablation_target}). 
Moreover,
training with only unmasked tokens without a decoder further saves GPU memory compared to VideoMAE,
and the guidance from the teacher's semantically rich representation leads to faster convergence. 
Notably, 
the resulting model can handle both scene-related \cite{k400,mit} and temporal-related actions \cite{sth,ava} exceptionally well, 
while the alignment to CLIP features enables the model to be compatible with cross-modal learning.

To address various video tasks,
we propose a progressive pre-training framework in Figure \ref{fig:framework}. 
In Stage 1, 
we only use video data for masked video modeling, 
resulting in a model that excels at video-only tasks.
In Stage 2, 
we employ public vision-language data for multi-modality learning. 
This allows the model to conduct complex video-language tasks, 
such as video-text retrieval \cite{msrvtt,lsmdc} and video question answering \cite{anet_qa,msrvtt_qa}. 
We use the \Modelnamelight\ in both stages, 
significantly reducing the training sources and speeding up convergence. 
Thanks to readily-available image and language foundation models \cite{clip,beitv2,lu2019vilbert,opt,flant5}, 
our simple framework is easily scalable for video foundation models.

We conduct extensive experiments to verify the effectiveness and efficiency of our approach.
As shown in Figure \ref{fig:intro},
with public sources (data/models) for pre-training,
our method achieves state-of-the-art performances on various video tasks,
including action recognition \cite{k400,k600,k700,mit,sth} (\textbf{90.6\%} top-1 accuracy on K400), 
spatiotemporal localization \cite{ava} (\textbf{39.8} mAP on AVA), 
video-text retrieval \cite{msrvtt,didemo,anet,lsmdc,msvd} (\textbf{58.8} R@1 on MSRVTT) and video question-answering \cite{anet_qa,msrvtt_qa,msrvtt_mc} (\textbf{47.1\%} accuracy on MSRVTT).
It is worth emphasizing that our method is much more environmentally friendly compared to CoCa \cite{coca}, which uses 2,048 CloudTPUv4 chips for 5 days. 
In contrast, 
our pre-training requires \textbf{32 A100(80G)} GPUs within \textbf{6 days}, 
leading to a remarkable \textbf{70$\times$} reduction in carbon emissions.