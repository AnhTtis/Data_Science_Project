@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{dou2022empirical,
  title={An empirical study of training end-to-end vision-and-language transformers},
  author={Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{fu2022empirical,
  title={An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={ArXiv},  volume={abs/2209.01540},
  year={2022}
}

@inproceedings{gberta_2021_ICML,
    author  = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},
    title = {Is Space-Time Attention All You Need for Video Understanding?},
    booktitle   = {Proceedings of the International Conference on Machine Learning (ICML)}, 
    month = {July},
    year = {2021}
}



@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={ArXiv},  
  volume={abs/1504.00325},
  year={2015}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  year={2017},
}

@article{ordonez2011im2text,
  title={Im2text: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
  journal={Advances in neural information processing systems},
  year={2011}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  year={2018}
}

@inproceedings{changpinyo2021conceptual,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2019}
}

@inproceedings{lei2021less,
  title={Less is more: Clipbert for video-and-language learning via sparse sampling},
  author={Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{zhu2020actbert,
  title={Actbert: Learning global-local video-text representations},
  author={Zhu, Linchao and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  year={2020}
}

@article{xu2021videoclip,
  title={Videoclip: Contrastive pre-training for zero-shot video-text understanding},
  author={Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  journal={ArXiv},  
  volume={abs/2109.14084},
  year={2021}
}

@article{li2020hero,
  title={Hero: Hierarchical encoder for video+ language omni-representation pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  journal={ArXiv},  
  volume={abs/2005.00200},
  year={2020}
}

@article{zellers2021merlot,
  title={Merlot: Multimodal neural script knowledge models},
  author={Zellers, Rowan and Lu, Ximing and Hessel, Jack and Yu, Youngjae and Park, Jae Sung and Cao, Jize and Farhadi, Ali and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{luo2022clip4clip,
  title={CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval and captioning},
  author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
  journal={Neurocomputing},
  year={2022},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={ArXiv},
  volume={abs/2010.11929},
  year={2020}
}
@article{luo2020univl,
  title={Univl: A unified video and language pre-training model for multimodal understanding and generation},
  author={Luo, Huaishao and Ji, Lei and Shi, Botian and Huang, Haoyang and Duan, Nan and Li, Tianrui and Li, Jason and Bharti, Taroon and Zhou, Ming},
  journal={ArXiv},
  volume={abs/2002.06353},
  year={2020}
}

@inproceedings{msrvtt,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2016}
}

@inproceedings{didemo,
  title={Localizing moments in video with natural language},
  author={Anne Hendricks, Lisa and Wang, Oliver and Shechtman, Eli and Sivic, Josef and Darrell, Trevor and Russell, Bryan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2017}
}

@inproceedings{anet,
  title={Dense-captioning events in videos},
  author={Krishna, Ranjay and Hata, Kenji and Ren, Frederic and Fei-Fei, Li and Carlos Niebles, Juan},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2017}
}

@inproceedings{msvd,
  title={Collecting Highly Parallel Data for Paraphrase Evaluation},
  author={David L. Chen and William B. Dolan},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2011}
}

@article{wang2022omnivl,
  title={Omnivl: One foundation model for image-language and video-language tasks},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Zhou, Luowei and Zhao, Yucheng and Xie, Yujia and Liu, Ce and Jiang, Yu-Gang and Yuan, Lu},
  journal={ArXiv},
  volume={abs/2209.07526},
  year={2022}
}
@article{fu2021violet,
  title={VIOLET: End-to-end video-language transformers with masked visual-token modeling},
  author={Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal={ArXiv},  
  volume={abs/2111.12681},
  year={2021}
}
@article{li2022lavender,
  title={Lavender: Unifying video-language understanding as masked language modeling},
  author={Li, Linjie and Gan, Zhe and Lin, Kevin and Lin, Chung-Ching and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={ArXiv}, 
  volume={abs/2206.07160},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  year={2021},
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={ArXiv}, 
  volume={abs/1810.04805},
  year={2018}
}

@inproceedings{li2022align,
  title={Align and Prompt: Video-and-Language Pre-training with Entity Prompts},
  author={Li, Dongxu and Li, Junnan and Li, Hongdong and Niebles, Juan Carlos and Hoi, Steven CH},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={ArXiv}, 
  volume={abs/2106.08254},
  year={2021}
}
@inproceedings{buch2022revisiting,
  title={Revisiting the" Video" in Video-Language Understanding},
  author={Buch, Shyamal and Eyzaguirre, Crist{\'o}bal and Gaidon, Adrien and Wu, Jiajun and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{lei2022revealing,
  title={Revealing Single Frame Bias for Video-and-Language Learning},
  author={Lei, Jie and Berg, Tamara L and Bansal, Mohit},
  journal={ArXiv}, 
  volume={abs/2206.03428},
  year={2022}
}
@article{wang2022all,
  title={All in one: Exploring unified video-language pre-training},
  author={Wang, Alex Jinpeng and Ge, Yixiao and Yan, Rui and Ge, Yuying and Lin, Xudong and Cai, Guanyu and Wu, Jianping and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  journal={ArXiv}, 
  volume={abs/2203.07303},
  year={2022}
}
@inproceedings{seo2022end,
  title={End-to-end generative pretraining for multimodal video captioning},
  author={Seo, Paul Hongsuck and Nagrani, Arsha and Arnab, Anurag and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
@article{xue2022clip,
  title={CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment},
  author={Xue, Hongwei and Sun, Yuchong and Liu, Bei and Fu, Jianlong and Song, Ruihua and Li, Houqiang and Luo, Jiebo},
  journal={ArXiv}, 
  volume={abs/2209.06430},
  year={2022}
}

@inproceedings{liu2022video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{feichtenhofer2022masked,
  title={Masked Autoencoders As Spatiotemporal Learners},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Li, Yanghao and He, Kaiming},
  journal={ArXiv}, 
  volume={abs/2205.09113},
  year={2022}
}
@article{rolfe2016discrete,
  title={Discrete variational autoencoders},
  author={Rolfe, Jason Tyler},
  journal={ArXiv}, 
  volume={abs/1609.02200},
  year={2016}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}
@inproceedings{fan2021multiscale,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}
@article{bain2022clip,
  title={A CLIP-Hitchhiker's Guide to Long Video Retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  journal={ArXiv}, 
  volume={abs/2205.08508},
  year={2022}
}
@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  year={2014},
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={ArXiv},  
  volume={abs/1711.05101},
  year={2017}
}
@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={ArXiv}, 
  volume={abs/1608.03983},
  year={2016}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={ArXiv}, 
  volume={abs/1710.03740},
  year={2017}
}


@inproceedings{yang2021just,
  title={Just ask: Learning to answer questions from millions of narrated videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@article{yang2022zero,
  title={Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={ArXiv}, 
  volume={abs/2206.08155},
  year={2022}
}
@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={ArXiv},  
  volume={abs/1604.06174},
  year={2016}
}

@inproceedings{neimark2021video,
  title={Video transformer network},
  author={Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@article{bugliarello2021multimodal,
  title={Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language BERTs},
  author={Bugliarello, Emanuele and Cotterell, Ryan and Okazaki, Naoaki and Elliott, Desmond},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  year={2021},
  publisher={MIT Press}
}

@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  year={2021}
}
@article{liu2019use,
  title={Use what you have: Video retrieval using representations from collaborative experts},
  author={Liu, Yang and Albanie, Samuel and Nagrani, Arsha and Zisserman, Andrew},
  journal={ArXiv},  
  volume={abs/1907.13487},
  year={2019}
}
@inproceedings{yu2018joint,
  title={A joint sequence fusion model for video question answering and retrieval},
  author={Yu, Youngjae and Kim, Jongseok and Kim, Gunhee},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2018}
}
@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  year={2017}
}
@inproceedings{yu2019activitynet,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  year={2019}
}
@inproceedings{iashin2020multi,
  title={Multi-modal dense video captioning},
  author={Iashin, Vladimir and Rahtu, Esa},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  year={2020}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  year={2021},
}
@inproceedings{wang2018reconstruction,
  title={Reconstruction network for video captioning},
  author={Wang, Bairui and Ma, Lin and Zhang, Wei and Liu, Wei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018}
}
@inproceedings{zolfaghari2018eco,
  title={Eco: Efficient convolutional network for online video understanding},
  author={Zolfaghari, Mohammadreza and Singh, Kamaljeet and Brox, Thomas},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  year={2018}
}
@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2019}
}
@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2017}
}
@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}
@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018}
}
@inproceedings{hara2018can,
  title={Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?},
  author={Hara, Kensho and Kataoka, Hirokatsu and Satoh, Yutaka},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  year={2018}
}
@inproceedings{wang2016temporal,
  title={Temporal segment networks: Towards good practices for deep action recognition},
  author={Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  booktitle={European conference on computer vision},
  year={2016},
}
@InProceedings{Lin_2019_ICCV,
author = {Lin, Ji and Gan, Chuang and Han, Song},
title = {TSM: Temporal Shift Module for Efficient Video Understanding},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}
@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={ArXiv}, 
  volume={abs/1705.06950},
  year={2017}
}
@inproceedings{caba2015activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={Proceedings of the ieee conference on computer vision and pattern recognition},
  year={2015}
}
@article{song2020mpnet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={ArXiv}, 
  volume={abs/2003.10555},
  year={2020}
}
@article{feng2020language,
  title={Language-agnostic bert sentence embedding},
  author={Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
  journal={ArXiv},  
  volume={abs/2007.01852},
  year={2020}
}
@inproceedings{chen2020uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={European conference on computer vision},
  year={2020},
}
@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={ArXiv},  
  volume={abs/1908.07490},
  year={2019}
}
@article{gupta2022maskvit,
  title={Maskvit: Masked visual pre-training for video prediction},
  author={Gupta, Agrim and Tian, Stephen and Zhang, Yunzhi and Wu, Jiajun and Mart{\'\i}n-Mart{\'\i}n, Roberto and Fei-Fei, Li},
  journal={ArXiv}, 
  volume={abs/2206.11894},
  year={2022}
}

@inproceedings{girdhar2022omnivore,
  title={Omnivore: A single model for many visual modalities},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and van der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2017}
}

@article{lei2018tvqa,
  title={Tvqa: Localized, compositional video question answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  journal={ArXiv}, 
  volume={abs/1809.01696},
  year={2018}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={ArXiv},  
  volume={abs/1706.02677},
  year={2017}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{gao2021clip2tv,
  title={Clip2tv: An empirical study on transformer-based methods for video-text retrieval},
  author={Gao, Zijian and Liu, Jingyu and Chen, Sheng and Chang, Dedan and Zhang, Hao and Yuan, Jinwei},
  journal={ArXiv},  
  volume={abs/2111.05610},
  year={2021}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{xie2018rethinking,
  title={Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification},
  author={Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  year={2018}
}

@article{pan2022st,
  title={ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition},
  author={Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
  journal={ArXiv},  
  volume={abs/2206.13559},
  year={2022}
}

@inproceedings{feichtenhofer2020x3d,
  title={X3d: Expanding architectures for efficient video recognition},
  author={Feichtenhofer, Christoph},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020}
}

@InProceedings{Tran_2019_ICCV,
author = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Feiszli, Matt},
title = {Video Classification With Channel-Separated Convolutional Networks},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  year={2019}
}

@inproceedings{zhou2020unified,
  title={Unified vision-language pre-training for image captioning and vqa},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{zhang2021vinvl,
  title={Vinvl: Revisiting visual representations in vision-language models},
  author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{yang2021causal,
  title={Causal attention for vision-language tasks},
  author={Yang, Xu and Zhang, Hanwang and Qi, Guojun and Cai, Jianfei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{kim2021vilt,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{yuan2021florence,
  title={Florence: A new foundation model for computer vision},
  author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal={ArXiv},  
  volume={abs/2111.11432},
  year={2021}
}

@article{wang2021ufo,
  title={UFO: A unified transformer for vision-language representation learning},
  author={Wang, Jianfeng and Hu, Xiaowei and Gan, Zhe and Yang, Zhengyuan and Dai, Xiyang and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  journal={ArXiv},  
  volume={abs/2111.10023},
  year={2021}
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{yang2022unified,
  title={Unified contrastive learning in image-text-label space},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Xiao, Bin and Liu, Ce and Yuan, Lu and Gao, Jianfeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{singh2022flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{hu2022scaling,
  title={Scaling up vision-language pre-training for image captioning},
  author={Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Yang, Zhengyuan and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{wang2022vlmixer,
  title={VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix},
  author={Wang, Teng and Jiang, Wenhao and Lu, Zhichao and Zheng, Feng and Cheng, Ran and Yin, Chengguo and Luo, Ping},
  booktitle={International Conference on Machine Learning},
  year={2022},
}

@article{zeng2021multi,
  title={Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts},
  author={Zeng, Yan and Zhang, Xinsong and Li, Hang},
  journal={ArXiv},  
  volume={abs/2111.08276},
  year={2021}
}

@article{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  journal={ArXiv},  
  volume={abs/2201.12086},
  year={2022}
}
@inproceedings{byun2022grit,
  title={GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training},
  author={Byun, Jaeseok and Hwang, Taebaek and Fu, Jianlong and Moon, Taesup},
  booktitle={European Conference on Computer Vision},
  year={2022},
}
@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={ArXiv}, 
  volume={abs/2208.10442},
  year={2022}
}
@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={ArXiv},  
  volume={abs/2205.01917},
  year={2022}
}
@inproceedings{ni2022expanding,
  title={Expanding language-image pretrained models for general video recognition},
  author={Ni, Bolin and Peng, Houwen and Chen, Minghao and Zhang, Songyang and Meng, Gaofeng and Fu, Jianlong and Xiang, Shiming and Ling, Haibin},
  booktitle={European Conference on Computer Vision},
  year={2022},
}
@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2015}
}

@article{lin2022eclipse,
  title={ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound},
  author={Lin, Yan-Bo and Lei, Jie and Bansal, Mohit and Bertasius, Gedas},
  journal={ArXiv}, 
  volume={abs/2204.02874},
  year={2022}
}


@article{Cheng2022VindLUAR,
  title={VindLU: A Recipe for Effective Video-and-Language Pretraining},
  author={Feng Cheng and Xizi Wang and Jie Lei and David J. Crandall and Mohit Bansal and Gedas Bertasius},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.05051}
}

@article{Wang2022InternVideoGV,
  title={InternVideo: General Video Foundation Models via Generative and Discriminative Learning},
  author={Yi Wang and Kunchang Li and Yizhuo Li and Yinan He and Bingkun Huang and Zhiyu Zhao and Hongjie Zhang and Jilan Xu and Yi Liu and Zun Wang and Sen Xing and Guo Chen and Junting Pan and Jiashuo Yu and Yali Wang and Limin Wang and Yu Qiao},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.03191}
}

@inproceedings{c3d,
  title={Learning Spatiotemporal Features with 3D Convolutional Networks},
  author={Du Tran and Lubomir D. Bourdev and Rob Fergus and Lorenzo Torresani and Manohar Paluri},
  booktitle={IEEE International Conference on Computer Vision },
  year={2015},
}



@inproceedings{r(2+1)d,
  title={A Closer Look at Spatiotemporal Convolutions for Action Recognition},
  author={Du Tran and Hong-xiu Wang and Lorenzo Torresani and Jamie Ray and Yann LeCun and Manohar Paluri},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2018},
}

@inproceedings{p3d,
  title={Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks},
  author={Zhaofan Qiu and Ting Yao and Tao Mei},
  booktitle={IEEE International Conference on Computer Vision},
  year={2017},
}

@inproceedings{x3d,
  title={X3d: Expanding architectures for efficient video recognition},
  author={Feichtenhofer, Christoph},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020}
}

@inproceedings{timesformer,
  author  = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},
    title = {Is Space-Time Attention All You Need for Video Understanding?},
    booktitle   = {International Conference on Machine Learning}, 
    year = {2021}
}
@inproceedings{slowfast,
  title={SlowFast Networks for Video Recognition},
  author={Christoph Feichtenhofer and Haoqi Fan and Jitendra Malik and Kaiming He},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2019},
}

@inproceedings{vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@inproceedings{mvit,
  title={Multiscale vision transformers},
  author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@inproceedings{video_swin,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{k600,
  title={A Short Note about Kinetics-600},
  author={Jo{\~a}o Carreira and Eric Noland and Andras Banki-Horvath and Chloe Hillier and Andrew Zisserman},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.01340}
}

@inproceedings{sth,
  title={The “Something Something” Video Database for Learning and Evaluating Visual Common Sense},
  author={Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fr{\"u}nd and Peter Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic},
  booktitle={IEEE International Conference on Computer Vision},
  year={2017},
}

@inproceedings{
vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{resnet,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2016},
}

@article{densenet,
  title={Densely Connected Convolutional Networks},
  author={Gao Huang and Zhuang Liu and Kilian Q. Weinberger},
  journal={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017},
}

@article{senet,
  title={Squeeze-and-Excitation Networks},
  author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and E. Wu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  volume={42},
}

@inproceedings{preLN,
  title={Learning Deep Transformer Models for Machine Translation},
  author={Qiang Wang and Bei Li and Tong Xiao and Jingbo Zhu and Changliang Li and Derek F. Wong and Lidia S. Chao},
  booktitle={ACL},
  year={2019}
}

@inproceedings{mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018}
}

@inproceedings{lgd,
  title={Learning Spatio-Temporal Representation With Local and Global Diffusion},
  author={Zhaofan Qiu and Ting Yao and C. Ngo and Xinmei Tian and Tao Mei},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2019},
}

@inproceedings{csn,
  title={Video Classification With Channel-Separated Convolutional Networks},
  author={Du Tran and Heng Wang and L. Torresani and Matt Feiszli},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2019},
}

@inproceedings{corrnet,
  title={Video Modeling With Correlation Networks},
  author={Heng Wang and Du Tran and L. Torresani and Matt Feiszli},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020},
}

@inproceedings{movienet,
  title={Movinets: Mobile video networks for efficient video recognition},
  author={Kondratyuk, Dan and Yuan, Liangzhe and Li, Yandong and Zhang, Li and Tan, Mingxing and Brown, Matthew and Gong, Boqing},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}


@inproceedings{tsn,
  title={Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  author={Limin Wang and Yuanjun Xiong and Zhe Wang and Yu Qiao and Dahua Lin and Xiaoou Tang and Luc Van Gool},
  booktitle={European conference on computer vision},
  year={2016}
}

@inproceedings{tsm,
  title={TSM: Temporal Shift Module for Efficient Video Understanding},
  author={Ji Lin and Chuang Gan and Song Han},
  booktitle={IEEE International Conference on Computer Vision},
  year={2019},
}

@inproceedings{tea,
  title={Tea: Temporal excitation and aggregation for action recognition},
  author={Li, Yan and Ji, Bin and Shi, Xintian and Zhang, Jianguo and Kang, Bin and Wang, Limin},
  booktitle={IEEE/CVF conference on computer vision and pattern recognition},
  year={2020}
}

@article{tei,
  title={TEINet: Towards an Efficient Architecture for Video Recognition},
  author={Zhaoyang Liu and D. Luo and Yabiao Wang and L. Wang and Ying Tai and Chengjie Wang and Jilin Li and Feiyue Huang and Tong Lu},
  journal={ArXiv},
  year={2020},
  volume={abs/1911.09435}
}

@inproceedings{tdn,
  title={{TDN}: Temporal difference networks for efficient action recognition},
  author={Wang, Limin and Tong, Zhan and Ji, Bin and Wu, Gangshan},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}


@inproceedings{ct_net,
  title={CT-Net: Channel Tensorization Network for Video Classification},
  author={Li, Kunchang and Li, Xianhang and Wang, Yali and Wang, Jun and Qiao, Yu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{msnet,
  title={MotionSqueeze: Neural Motion Feature Learning for Video Understanding},
  author={Heeseung Kwon and Manjin Kim and Suha Kwak and Minsu Cho},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{pvt,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@inproceedings{transformer,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
}

@article{visualtransformer,
  title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},
  author={B. Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and M. Tomizuka and K. Keutzer and P{\'e}ter Vajda},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.03677}
}

@article{mobilenetv1,
  title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and M. Andreetto and Hartwig Adam},
  journal={ArXiv},
  year={2017},
  volume={abs/1704.04861}
}

@article{cpe,
  title={Do We Really Need Explicit Position Encodings for Vision Transformers?},
  author={Xiangxiang Chu and Bo Zhang and Zhi Tian and Xiaolin Wei and Huaxia Xia},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.10882}
}

@article{understand,
  title={Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},
  author={Yiping Lu and Zhuohan Li and Di He and Zhiqing Sun and Bin Dong and Tao Qin and Liwei Wang and Tie-Yan Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.02762}
}

@inproceedings{x_vit,
  title={Space-time Mixing Attention for Video Transformer},
  author={Bulat, Adrian and Perez-Rua, Juan-Manuel and Sudhakaran, Swathikiran and Martinez, Brais and Tzimiropoulos, Georgios},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@inproceedings{motionformer,
  title={Keeping your eye on the ball: Trajectory attention in video transformers},
  author={Patrick, Mandela and Campbell, Dylan and Asano, Yuki and Misra, Ishan and Metze, Florian and Feichtenhofer, Christoph and Vedaldi, Andrea and Henriques, Jo{\~a}o F},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{non_local,
  title={Non-local Neural Networks},
  author={X. Wang and Ross B. Girshick and Abhinav Gupta and Kaiming He},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
}

@article{transfer,
  title={ConvNets vs. Transformers: Whose Visual Representations are More Transferable?},
  author={Hong-Yu Zhou and Chixiang Lu and Sibei Yang and Yizhou Yu},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.05305}
}

@article{gst,
  title={Grouped Spatial-Temporal Aggregation for Efficient Action Recognition},
  author={Chenxu Luo and Alan L. Yuille},
  journal={2019 IEEE International Conference on Computer Vision},
  year={2019},
}

@article{smallbig,
  title={SmallBigNet: Integrating Core and Contextual Views for Video Classification},
  author={X. Li and Yali Wang and Zhipeng Zhou and Yu Qiao},
  journal={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2020},
}

@article{coatnet,
  title={CoAtNet: Marrying Convolution and Attention for All Data Sizes},
  author={Zihang Dai and Hanxiao Liu and Quoc V. Le and Mingxing Tan},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04803}
}

@article{cvt,
  title={CvT: Introducing Convolutions to Vision Transformers},
  author={Haiping Wu and Bin Xiao and N. Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.15808}
}

@article{convtransformer,
  title={ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis},
  author={Zhouyong Liu and Shun Luo and Wubin Li and Jingben Lu and Yufan Wu and Chunguo Li and Luxi Yang},
  journal={ArXiv},
  year={2020},
  volume={abs/2011.10185}
}


@article{stm,
  title={STM: SpatioTemporal and Motion Encoding for Action Recognition},
  author={Boyuan Jiang and Mengmeng Wang and Weihao Gan and Wei Wu and Junjie Yan},
  journal={2019 IEEE International Conference on Computer Vision},
  year={2019},
}

@inproceedings{swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@article{cait,
  title={Going deeper with Image Transformers},
  author={Hugo Touvron and M. Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Herv'e J'egou},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.17239}
}

@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Hugo Touvron and M. Cord and M. Douze and Francisco Massa and Alexandre Sablayrolles and Herv'e J'egou},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@article{shiftct,
  title={Shifted Chunk Transformer for Spatio-Temporal Representational Learning},
  author={Xuefan Zha and Wentao Zhu and Tingxun Lv and Sen Yang and Ji Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.11575}
}

@article{local_relation,
  title={Local Relation Networks for Image Recognition},
  author={Han Hu and Z. Zhang and Zhenda Xie and S. Lin},
  journal={2019 IEEE/CVF International Conference on Computer Vision},
  year={2019},
}

@article{howmp,
  title={How Much Position Information Do Convolutional Neural Networks Encode?},
  author={Md. Amirul Islam and Sen Jia and Neil D. B. Bruce},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.08248}
}

@article{efficientnet,
  title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author={Mingxing Tan and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.11946}
}

@inproceedings{lvvit,
  title={All Tokens Matter: Token Labeling for Training Better Vision Transformers},
  author={Zihang Jiang and Qibin Hou and Li Yuan and Daquan Zhou and Yujun Shi and Xiaojie Jin and Anran Wang and Jiashi Feng},
  year={2021}
}

@article{nfnet,
  title={High-Performance Large-Scale Image Recognition Without Normalization},
  author={Andrew Brock and Soham De and Samuel L. Smith and K. Simonyan},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.06171}
}

@article{efficientv2,
  title={EfficientNetV2: Smaller Models and Faster Training},
  author={Mingxing Tan and Quoc V. Le},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.00298}
}

@article{levit,
  title={LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},
  author={Benjamin Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv'e J'egou and M. Douze},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.01136}
}



@inproceedings{cswin,
  title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
  author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{regnet,
  title={Designing Network Design Spaces},
  author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross B. Girshick and Kaiming He and Piotr Doll{\'a}r},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020},
}

@article{t2t,
  title={Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},
  author={Li Yuan and Y. Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis E. H. Tay and Jiashi Feng and Shuicheng Yan},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.11986}
}

@article{botnet,
  title={Bottleneck Transformers for Visual Recognition},
  author={A. Srinivas and Tsung-Yi Lin and Niki Parmar and Jonathon Shlens and P. Abbeel and Ashish Vaswani},
  journal={ArXiv},
  year={2021},
  volume={abs/2101.11605}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{grad_cam,
  title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
  author={Ramprasaath R. Selvaraju and Abhishek Das and Ramakrishna Vedantam and Michael Cogswell and Devi Parikh and Dhruv Batra},
  journal={International Journal of Computer Vision},
  year={2019},
  volume={128},
}

@article{container,
  title={Container: Context Aggregation Network},
  author={Peng Gao and Jiasen Lu and Hongsheng Li and R. Mottaghi and Aniruddha Kembhavi},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.01401}
}

@inproceedings{video_transformer,
  title={Video transformer network},
  author={Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}


@article{stam,
  title={An Image is Worth 16x16 Words, What is a Video Worth?},
  author={Gilad Sharir and Asaf Noy and Lihi Zelnik-Manor},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.13915}
}

@inproceedings{vidtr,
  title={Vidtr: Video transformer without convolutions},
  author={Zhang, Yanyi and Li, Xinyu and Liu, Chunhui and Shuai, Bing and Zhu, Yi and Brattoli, Biagio and Chen, Hao and Marsic, Ivan and Tighe, Joseph},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@article{more_is_less,
  title={More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation},
  author={Quanfu Fan and Chun-Fu Chen and Hilde Kuehne and Marco Pistoia and David Cox},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.00869}
}

@article{pem,
  title={Temporal Distinct Representation Learning for Action Recognition},
  author={Junwu Weng and Donghao Luo and Yabiao Wang and Ying Tai and Chengjie Wang and Jilin Li and Feiyue Huang and Xudong Jiang and Junsong Yuan},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.07626}
}

@article{eco,
  title={ECO: Efficient Convolutional Network for Online Video Understanding},
  author={Mohammadreza Zolfaghari and Kamaljeet Singh and Thomas Brox},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.09066}
}

@inproceedings{bn,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  year={2015},
}



@article{ln,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@article{adamw,
  title={Fixing Weight Decay Regularization in Adam},
  author={I. Loshchilov and F. Hutter},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.05101}
}

@inproceedings{cosine,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2017},
}

@article{warmup,
  title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author={Priya Goyal and Piotr Doll{\'a}r and Ross B. Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.02677}
}

@inproceedings{uniformer,
title={UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning},
author={Kunchang Li and Yali Wang and Gao Peng and Guanglu Song and Yu Liu and Hongsheng Li and Yu Qiao},
booktitle={International Conference on Learning Representations},
year={2022},
}

@article{uniformer_all,
  title={UniFormer: Unifying Convolution and Self-attention for Visual Recognition},
  author={Kunchang Li and Yali Wang and Junhao Zhang and Peng Gao and Guanglu Song and Yu Liu and Hongsheng Li and Yu Jiao Qiao},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.09450}
}


@article{mvitv2,
  title={Improved Multiscale Vision Transformers for Classification and Detection},
  author={Yanghao Li and Chaoxia Wu and Haoqi Fan and Karttikeya Mangalam and Bo Xiong and Jitendra Malik and Christoph Feichtenhofer},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.01526}
}

@inproceedings{
tokenlearner,
title={TokenLearner: Adaptive Space-Time Tokenization for Videos},
author={Michael S Ryoo and AJ Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},
booktitle={Advances in Neural Information Processing Systems},
year={2021},
}


@article{xclip,
  title={Expanding Language-Image Pretrained Models for General Video Recognition},
  author={Bolin Ni and Houwen Peng and Minghao Chen and Songyang Zhang and Gaofeng Meng and Jianlong Fu and Shiming Xiang and Haibin Ling},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.02816}
}

@article{florence,
  title={Florence: A New Foundation Model for Computer Vision},
  author={Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel C. F. Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.11432}
}

@article{cover,
  title={Co-training Transformer with Videos and Images Improves Action Recognition},
  author={Bowen Zhang and Jiahui Yu and Christopher Fifty and Wei Han and Andrew M. Dai and Ruoming Pang and Fei Sha},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.07175}
}


@inproceedings{mtv,
  title={Multiview transformers for video recognition},
  author={Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@article{coca,
title={CoCa: Contrastive Captioners are Image-Text Foundation Models},
author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
journal={Transactions on Machine Learning Research},
year={2022},
}

@article{reserve,
  title={MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound},
  author={Rowan Zellers and Jiasen Lu and Ximing Lu and Youngjae Yu and Yanpeng Zhao and Mohammadreza Salehi and Aditya Kusupati and Jack Hessel and Ali Farhadi and Yejin Choi},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.02639}
}

@inproceedings{assemblenet,
title={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},
author={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{vatt,
  title={Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text},
  author={Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@inproceedings{rsanet,
  title={Relational Self-Attention: What's Missing in Attention for Video Understanding},
  author={Kim, Manjin and Kwon, Heeseung and Wang, Chunyu and Kwak, Suha and Cho, Minsu},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{evl,
  title={Frozen CLIP Models are Efficient Video Learners},
  author={Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and de Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={ArXiv}, 
  volume={abs/2208.03550},
  year={2022}
}

@article{nsnet,
  title={NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition},
  author={Boyang Xia and Wenhao Wu and Haoran Wang and Rui Su and Dongliang He and Haosen Yang and Xiaoran Fan and Wanli Ouyang},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.10388}
}

@inproceedings{marl,
  title={Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition},
  author={Wenhao Wu and Dongliang He and Xiao Tan and Shifeng Chen and Shilei Wen},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2019},
}

@article{dsn,
  title={Dynamic Sampling Networks for Efficient Action Recognition in Videos},
  author={Yin-Dong Zheng and Zhaoyang Liu and Tong Lu and Limin Wang},
  journal={IEEE Transactions on Image Processing},
  year={2020},
  volume={29},
}

@article{st_adapter,
  title={Parameter-Efficient Image-to-Video Transfer Learning},
  author={Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
  journal={arXiv},
  year={2022},
  volume={abs/2206.13559}
}

@inproceedings{mae,
  title={Masked Autoencoders Are Scalable Vision Learners},
  author={Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll'ar and Ross B. Girshick},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}

@inproceedings{beit,
  title={BEiT: BERT Pre-Training of Image Transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{clip,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@inproceedings{how_much_clip,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@article{clip4clip,
  title={CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval},
  author={Huaishao Luo and Lei Ji and Ming Zhong and Yang Chen and Wen Lei and Nan Duan and Tianrui Li},
  journal={ArXiv},
  year={2022},
  volume={abs/2104.08860}
}

@article{vit_adapter,
  title={Vision Transformer Adapter for Dense Predictions},
  author={Zhe Chen and Yuchen Duan and Wenhai Wang and Junjun He and Tong Lu and Jifeng Dai and Y. Qiao},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.08534}
}

@article{k700,
  title={A Short Note on the Kinetics-700 Human Action Dataset},
  author={Jo{\~a}o Carreira and Eric Noland and Chloe Hillier and Andrew Zisserman},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.06987}
}

@inproceedings{activitynet,
  title={ActivityNet: A large-scale video benchmark for human activity understanding},
  author={Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2015},
}

@inproceedings{hacs,
  title={HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization},
  author={Hang Zhao and Antonio Torralba and Lorenzo Torresani and Zhicheng Yan},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2019},
}

@article{mit,
  title={Moments in Time Dataset: One Million Videos for Event Understanding},
  author={Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and Alex Andonian and Tom Yan and Kandan Ramakrishnan and Lisa M. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and Aude Oliva},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
}

@misc{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@article{gelu,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={arXiv: Learning},
  year={2016}
}

@article{k400,
  title={The Kinetics Human Action Video Dataset},
  author={Will Kay and Jo{\~a}o Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Apostol Natsev and Mustafa Suleyman and Andrew Zisserman},
  journal={ArXiv},
  year={2017},
  volume={abs/1705.06950}
}

@inproceedings{repeated_sampling,
  title={Augment Your Batch: Improving Generalization Through Instance Repetition},
  author={Elad Hoffer and Tal Ben-Nun and Itay Hubara and Niv Giladi and Torsten Hoefler and Daniel Soudry},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020},
}

@inproceedings{randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Ekin Dogus Cubuk and Barret Zoph and Jonathon Shlens and Quoc V. Le},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  year={2020},
}

@inproceedings{droppath,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle={European conference on computer vision},
  year={2016},
}



@article{dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  journal={J. Mach. Learn. Res.},
  year={2014},
  volume={15},
}

@inproceedings{inception,
  title={Going deeper with convolutions},
  author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott E. Reed and Dragomir Anguelov and D. Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2015},
}

@inproceedings{mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{cutmix,
  title={CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features},
  author={Sangdoo Yun and Dongyoon Han and Seong Joon Oh and Sanghyuk Chun and Junsuk Choe and Young Joon Yoo},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2019},
}

@inproceedings{random_erasing,
  title={Random Erasing Data Augmentation},
  author={Zhun Zhong and Liang Zheng and Guoliang Kang and Shaozi Li and Yi Yang},
  booktitle={AAAI conference on artificial intelligence},
  year={2020}
}

@inproceedings{label_smmoth,
  title={Rethinking the Inception Architecture for Computer Vision},
  author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  year={2016},
}

@inproceedings{dino,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Mathilde Caron and Hugo Touvron and Ishan Misra and Herv'e J'egou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021},
}

@article{deit_v2,
  title={DeiT III: Revenge of the ViT},
  author={Hugo Touvron and Matthieu Cord and Herv'e J'egou},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.07118}
}

@article{cam,
  title={Learning Deep Features for Discriminative Localization},
  author={Bolei Zhou and Aditya Khosla and {\`A}gata Lapedriza and Aude Oliva and Antonio Torralba},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2016},
}


@inproceedings{detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  year={2020},
}


@article{d_detr,
  title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},
  author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.04159}
}



@inproceedings{maskformer,
  title={Per-pixel classification is not all you need for semantic segmentation},
  author={Cheng, Bowen and Schwing, Alex and Kirillov, Alexander},
  booktitle={Neural Information Processing Systems},
  year={2021},
}

@inproceedings{segformer,
  title={SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{PreTrainedIP,
  title={Pre-Trained Image Processing Transformer},
  author={Hanting Chen and Yunhe Wang and Tianyu Guo and Chang Xu and Yiping Deng and Zhenhua Liu and Siwei Ma and Chunjing Xu and Chao Xu and Wen Gao},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021},
}

@inproceedings{swinir,
  title={SwinIR: Image Restoration Using Swin Transformer},
  author={Jingyun Liang and Jie Cao and Guolei Sun and K. Zhang and Luc Van Gool and Radu Timofte},
  booktitle={IEEE/CVF International Conference on Computer Vision Workshops},
  year={2021},
}

@article{actionformer,
  title={ActionFormer: Localizing Moments of Actions with Transformers},
  author={Chen-Lin Zhang and Jian Zhai Wu and Yin Li},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.07925}
}

@article{90k_image_enhance,
  title={You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image Enhancement and Exposure Correction},
  author={Ziteng Cui and Kunchang Li and Lin Gu and Sheng Su and Peng Gao and Zhengkai Jiang and Yu Jiao Qiao and Tatsuya Harada},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.14871}
}

@article{beit3,
  title={Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks},
  author={Wenhui Wang and Hangbo Bao and Li Dong and Johan Bjorck and Zhiliang Peng and Qiang Liu and Kriti Aggarwal and Owais Mohammed and Saksham Singhal and Subhojit Som and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.10442}
}

@inproceedings{early_conv,
  title={Early Convolutions Help Transformers See Better},
  author={Tete Xiao and Mannat Singh and Eric Mintun and Trevor Darrell and Piotr Doll{\'a}r and Ross B. Girshick},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@inproceedings{ceit,
  title={Incorporating convolution designs into visual transformers},
  author={Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and Zhou, Aojun and Yu, Fengwei and Wu, Wei},
  booktitle={IEEE/CVF International Conference on Computer Vision},
  year={2021}
}


@inproceedings{maskfeat,
  title={Masked Feature Prediction for Self-Supervised Visual Pre-Training},
  author={Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022},
}


@inproceedings{videomae,
  title={{VideoMAE}: Masked autoencoders are data-efficient learners for self-supervised video pre-training},
  author={Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  booktitle={Neural Information Processing Systems},
  year={2022},
}

@inproceedings{perceiver,
  title={Perceiver: General Perception with Iterative Attention},
  author={Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Jo{\~a}o Carreira},
  booktitle={ICML},
  year={2021}
}


@article{flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}

@article{uniformerv2,
  title={UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer},
  author={Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi Wang and Limin Wang and Y. Qiao},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.09552}
}

@article{bevt,
  title={BEVT: BERT Pretraining of Video Transformers},
  author={Rui Wang and Dongdong Chen and Zuxuan Wu and Yinpeng Chen and Xiyang Dai and Mengchen Liu and Yu-Gang Jiang and Luowei Zhou and Lu Yuan},
  journal={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  year={2022},
}

@article{st_mae,
  title={Masked Autoencoders As Spatiotemporal Learners},
  author={Christoph Feichtenhofer and Haoqi Fan and Yanghao Li and Kaiming He},
  journal={Advances in neural information processing systems},
  year={2022},
}

@article{videococa,
  title={Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners},
  author={Yan, Shen and Zhu, Tao and Wang, Zirui and Cao, Yuan and Zhang, Mi and Ghosh, Soham and Wu, Yonghui and Yu, Jiahui},
  journal={ArXiv},  
  volume={abs/2212.04979},
  year={2022}
}

@article{milan,
  title={MILAN: Masked Image Pretraining on Language Assisted Representation},
  author={Zejiang Hou and Fei Sun and Yen-Kuang Chen and Yuan Xie and S. Y. Kung},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.06049}
}

@article{Shu2022MaskedCP,
  title={Masked Contrastive Pre-Training for Efficient Video-Text Retrieval},
  author={Fangxun Shu and Biaolong Chen and Yue Liao and Ke Gao and Shuwen Xiao and Wenyu Sun and Xiaobo Li and Yousong Zhu and Jinqiao Wang and Si Liu},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.00986}
}

@article{Dong2022CLIPII,
  title={CLIP Itself is a Strong Fine-tuner: Achieving 85.7\% and 88.0\% Top-1 Accuracy with ViT-B and ViT-L on ImageNet},
  author={Xiaoyi Dong and Jianmin Bao and Ting Zhang and Dongdong Chen and Shuyang Gu and Weiming Zhang and Lu Yuan and Dong Chen and Fang Wen and Nenghai Yu},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.06138}
}

@inproceedings{mvp,
  title={MVP: Multimodality-guided Visual Pre-training},
  author={Longhui Wei and Lingxi Xie and Wen-gang Zhou and Houqiang Li and Qi Tian},
  booktitle={European conference on computer vision},
  year={2022},
}

@article{fd_clip,
  title={Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation},
  author={Yixuan Wei and Han Hu and Zhenda Xie and Zheng Zhang and Yue Cao and Jianmin Bao and Dong Chen and Baining Guo},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.14141}
}

@article{maskdistill,
  title={A Unified View of Masked Image Modeling},
  author={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.10615}
}

@article{maskalign,
  title={Stare at What You See: Masked Image Modeling without Reconstruction},
  author={Hongwei Xue and Peng Gao and Hongyang Li and Yu Jiao Qiao and Hao Sun and Houqiang Li and Jiebo Luo},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.08887}
}

@article{thumos,
  title={The thumos challenge on action recognition for videos “in the wild”},
  author={Idrees, Haroon and Zamir, Amir R and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
  journal={Computer Vision and Image Understanding},
  year={2017},
}

@article{finegym,
  title={FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding},
  author={Dian Shao and Yue Zhao and Bo Dai and Dahua Lin},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020},
}

@article{ava,
  title={AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},
  author={Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and Caroline Pantofaru and David A. Ross and George Toderici and Yeqing Li and Susanna Ricco and Rahul Sukthankar and Cordelia Schmid and Jitendra Malik},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{Gao2022ConvMAEMC,
  title={ConvMAE: Masked Convolution Meets Masked Autoencoders},
  author={Peng Gao and Teli Ma and Hongsheng Li and Jifeng Dai and Yu Jiao Qiao},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.03892}
}

@article{huang2022contrastive,
  title={Contrastive masked autoencoders are stronger vision learners},
  author={Huang, Zhicheng and Jin, Xiaojie and Lu, Chengze and Hou, Qibin and Cheng, Ming-Ming and Fu, Dongmei and Shen, Xiaohui and Feng, Jiashi},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.13532}
}

@article{simmim,
  title={SimMIM: a Simple Framework for Masked Image Modeling},
  author={Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021},
}

@article{Chen2022ContextAF,
  title={Context Autoencoder for Self-Supervised Representation Learning},
  author={Xiaokang Chen and Mingyu Ding and Xiaodi Wang and Ying Xin and Shentong Mo and Yunhao Wang and Shumin Han and Ping Luo and Gang Zeng and Jingdong Wang},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.03026}
}

@inproceedings{Baevski2022data2vecAG,
  title={data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},
  author={Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{peco,
  title={PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers},
  author={Xiaoyi Dong and Jianmin Bao and Ting Zhang and Dongdong Chen and Weiming Zhang and Lu Yuan and Dong Chen and Fang Wen and Nenghai Yu},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.12710}
}

@article{ibot,
  title={iBOT: Image BERT Pre-Training with Online Tokenizer},
  author={Jinghao Zhou and Chen Wei and Huiyu Wang and Wei Shen and Cihang Xie and Alan Loddon Yuille and Tao Kong},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.07832}
}

@article{Tao2022SiameseIM,
  title={Siamese Image Modeling for Self-Supervised Vision Representation Learning},
  author={Chenxin Tao and Xizhou Zhu and Gao Huang and Y. Qiao and Xiaogang Wang and Jifeng Dai},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.01204}
}

@article{beitv2,
  title={BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers},
  author={Zhiliang Peng and Li Dong and Hangbo Bao and Qixiang Ye and Furu Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.06366}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  year={2020},
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  year={2020}
}

@inproceedings{moco,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  year={2020}
}

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  year={2014},
}


@article{ade,
  title={Semantic understanding of scenes through the ade20k dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Xiao, Tete and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  journal={International Journal of Computer Vision},
  year={2019},
}

@inproceedings{nocaps,
  title={Nocaps: Novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  year={2019}
}

@inproceedings{fickr,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  year={2015}
}

@inproceedings{vqa,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2017}
}

@article{lsmdc,
  title={Movie Description},
  author={Anna Rohrbach and Atousa Torabi and Marcus Rohrbach and Niket Tandon and Christopher Joseph Pal and H. Larochelle and Aaron C. Courville and Bernt Schiele},
  journal={International Journal of Computer Vision},
  year={2016},
}

@inproceedings{msrvtt_qa,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the IEEE international conference on Multimedia},
  year={2017}
}

@inproceedings{anet_qa,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2019}
}

@article{opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={ArXiv},  
  volume={abs/2205.01068},
  year={2022}
}

@inproceedings{msrvtt_mc,
  title={A joint sequence fusion model for video question answering and retrieval},
  author={Yu, Youngjae and Kim, Jongseok and Kim, Gunhee},
  booktitle={Proceedings of the European Conference on Computer Vision},
  year={2018}
}

@article{flant5,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{Zhai2021ScalingVT,
  title={Scaling Vision Transformers},
  author={Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
}