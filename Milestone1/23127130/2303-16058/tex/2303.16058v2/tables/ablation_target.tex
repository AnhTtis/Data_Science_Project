\begin{table}[tp]
    \centering
    \setlength\tabcolsep{4.0pt}
    \resizebox{0.9\linewidth}{!}{
        \begin{tabular}{ccc|c|cc|c}
        \textbf{[U]} & \textbf{[M]} & \textbf{MAE} & \textbf{Memory (G)} & \textbf{SSV2} & \textbf{K400} & \textbf{MSR}\\
        \Xhline{1.0pt}
        \xmark & \xmark & \cmark & 44.0 & 67.1 & 78.8 & 55.6 \\
        \cmark & \xmark & \cmark & 52.5 & \textbf{70.2} & 83.9 & 64.5 \\
        \cmark & \cmark & \xmark & 43.6 & 70.0 & 84.6 & 65.2 \\
        \rowcolor{gray!20} 
        \cmark & \xmark & \xmark & \textbf{16.0} & \textbf{70.2} & \textbf{84.9} & \textbf{66.8} \\
        \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{\textbf{Target design.} 
    We benchmark ViT-B/16 in 32 A100 with a batch size of 2048.
    ``[U]'', ``[M]'' and ``MAE'' refers to unmasked token alignment, masked token recovering and pixel reconstruction~\cite{videomae} respectively.
    The pixel reconstruction conflict with our unmasked token alignment,
    and hinder the following multimodal learning.
    }
    \label{tab:ablation_target}
    \vspace{-0.3cm}
\end{table}