\section{Related Works}


\noindent\textbf{Video foundation models.}
The present Video Foundation Models (VFMs) are primarily based on well-prepared Image Foundation Models (IFMs) \cite{coca,mtv,vivit,cover,uniformerv2,fu2021violet,li2022lavender,wang2022omnivl,xue2022clip}. 
However, 
the strong spatial pre-training restricts their ability to learn spatiotemporal representations. 
Despite the impressive results demonstrated by Florence \cite{florence}, CoCa \cite{coca}, MTV \cite{mtv}, and UniFormerV2 \cite{uniformerv2} on video-only tasks \cite{k400,k600,k700}, 
these models struggle to handle temporal-related actions \cite{sth,finegym} and localize actions \cite{ava,thumos}. 
As for video-language tasks, 
there have been promising explorations on model architecture \cite{lei2021less,wang2022all,lei2022revealing} and learning paradigms \cite{xu2021videoclip,zellers2021merlot,fu2021violet,li2022lavender,wang2022omnivl}. 
Recently, 
InternVideo \cite{Wang2022InternVideoGV} introduces general VFMs through generative and discriminative learning.
However, 
the dependence on CLIP pre-training and tremendous training costs make it difficult to scale up.
In this paper, 
we propose an easily scalable framework for VFMs that is much more training-efficient.


\noindent\textbf{Masked vision modeling.} 
Inspired by the success of masked language modeling \cite{lu2019vilbert,dong2019unified}, 
masked vision modeling has been proposed for vision transformers \cite{vit}. 
BeiT \cite{bao2021beit} is the first to propose a BERT-like mask-then-predict framework to recover the discrete tokens \cite{ramesh2021zero}, 
while MAE \cite{mae} designs masked autoencoders to reconstruct normalized pixel values, 
which reduces memory consumption by processing only unmasked tokens in the encoder.
Later works can be roughly divided into BeiT-style \cite{peco,ibot,maskfeat,Baevski2022data2vecAG,maskdistill} and MAE-style \cite{simmim,Chen2022ContextAF,Gao2022ConvMAEMC,huang2022contrastive} with various target supervision, 
such as HOG descriptors \cite{maskfeat} and momentum features \cite{Tao2022SiameseIM}. 
For spatiotemporal learning, 
BEVT \cite{bevt} and VideoMAE \cite{videomae,st_mae} can be seen as extensions of BeiT and MAE, respectively. 
Recent works also indicate that CLIP features provide good guidance for mask modeling \cite{mvp,milan,maskdistill,beitv2,maskalign}, 
but all of them actually perform worse than CLIP itself with elaborate fine-tuning \cite{Dong2022CLIPII}. 
In contrast, 
we demonstrate that in the video domain,
our model with CLIP supervision clearly outperforms the teacher.