\section{Experiments}
\label{sec_exp}

\subsection{Implementation}

\textbf{Datasets.}
Unless otherwise stated,
we use Kinetics-710 dataset \cite{uniformerv2} in Stage 1, which is a combination of Kinetics-400, 600 and 700 \cite{k400,k600,k700} and excludes any repeated or leaked videos.
In Stage 2, 
we utilize image-text data for co-training \cite{wang2022all,lei2022revealing,wang2022omnivl},
where images are treated as single-frame videos. 
We use three corpora as in \cite{Cheng2022VindLUAR}:
\textbf{(i) 5M} Corpus comprises WebVid-2M \cite{bain2021frozen} video-text pairs and CC3M \cite{sharma2018conceptual} image-text pairs.
\textbf{(ii) 17M} Corpus includes four other image-text datasets: COCO \cite{lin2014microsoft}, Visual Genome \cite{krishna2017visual}, SBU Captions \cite{ordonez2011im2text}, and CC12M \cite{changpinyo2021conceptual}.
\textbf{(iii) 25M} Corpus uses a larger version of WebVid containing 10M video-text pairs.

\textbf{Settings.}
In this paper, 
we consider two model configurations: 
ViT-B/16 \cite{vit} with BERT$_{base}$ \cite{devlin2018bert} and ViT-L/16 with BERT$_{large}$. 
And CLIP-ViT-B/16 \cite{clip} and CLIP-ViT-L/14 are adopted as teachers for the base and large models, respectively.
Since CLIP-ViT-L/14 uses a smaller patch size,
we adopt a smaller input resolution (\textit{i.e.}, 196) to align the token number.
For Stage-1 pre-training, 
we follow most of the hyperparameter settings in VideoMAE \cite{videomae}. 
However, 
we sparsely sample \cite{tsn} 8 frames and use a masking ratio of 80\%.  
By default,
we train both models on 32 A100 with a batch size of 2048 for 200 epochs.
The training on Kinetics-710 takes about \textbf{60} and \textbf{90} hours for ViT-B/16 and ViT-L/16, respectively.
In Stage 2,
we follow \cite{lei2022revealing} to sample 4 frames and train for 10 epochs. 
Specifically,
we mask 50\% image and 80\% video tokens.
Both models are trained on 32 A100 with a batch size of 4096. 
The pre-training on 25M Corpus takes about \textbf{24} and \textbf{40} hours respectively for the base and large models. 
For more implementation details about training, 
please refer to the supplemental materials.

\input{tables/ablation_target.tex}
\input{tables/ablation_mask_sampling_downsampling.tex}

\subsection{Ablation Study}
We ablate the properties of \Modelnamelight\  in both stages on both scene-related \cite{k400,msrvtt} and temporal-related tasks \cite{sth,lei2022revealing}.
For single-modality learning, 
we pre-train ViT-B/16 for 200 epochs on SthSth V2 \cite{sth} or K400 \cite{k400} dataset. 
For multi-modality learning, 
we use K710 pre-trained models and further pre-train it for 10 epochs on 5M Corpus. 
Except for Table \ref{tab:ablation_target}, 
where we use K400 pre-training.

\textbf{Target.} 
Table \ref{tab:ablation_target} presents a comparison of training targets.
Compared with pixel reconstruction \cite{videomae},
our unmasked token alignment significantly improves the accuracy with only 36\% memory cost.
However,
combining the two targets results in poor results on K400 and MSRVTT,
indicating a conflict between low-level reconstruction and high-level alignment.
Moreover,
recovering the masked tokens has a detrimental effect,
possibly due to the high masking ratio making high-level recovery too challenging.
The results demonstrate our method is effective to learn temporal-sensitive and multimodal-friendly representation.

\textbf{Mask type, sampling method, and temporal downsampling.}
Table \ref{tab:ablation_mask_sampling_downsampling} indicates that different masking strategies yield comparable results in SthSth V2. 
We contend that recognizing the category of ``something" is not necessary for SthSth V2, but it requires deducing the intricate motion between objects, 
thus random masking suffices. 
However, 
it is critical for K400 to identify the scene and objects, 
making semantic masking advantageous for knowledge distillation. 
Moreover, 
sparse sampling without temporal downsampling is more appropriate for our approach.

\textbf{Aligned layers.}
We try to align more layers in Figure \ref{fig:ablation_layer},
and the losses are averaged across multiple layers.
Since the GPU memory and running speed are similar,
we simply align the last 6 layers for the best results.

\textbf{Masking ratio.}
Figure \ref{fig:ablation_ratio} shows that proper high ratios work better.
When using a ratio of 95\%,
the performances dramatically drop since it is too challenging for token alignment.
Conversely,
when removing masks,
the task is too easy to learn the token relationships in space and time.
By default,
we adopt the ratio of 80\% for better trade-offs.

\begin{figure}[tp]
    \centering
    % \vspace{-0.3cm}
    \includegraphics[width=0.95\linewidth
    ]{figs/layer.pdf}
    \vspace{-0.4cm}
    \caption{
    \textbf{Aligned layers.} 
    Since the GPU memory and running speed are similar,
    we align the last 6 layers.
    }
    \label{fig:ablation_layer}
    \vspace{-0.3cm}
\end{figure}
\begin{figure}[tp]
    \centering
    % \vspace{-0.3cm}
    \includegraphics[width=0.95\linewidth
    ]{figs/mask_ratio.pdf}
    \vspace{-0.4cm}
    \caption{
    \textbf{Masking ratio.} 
    We use the masking ratio of 0.8 for a better trade-off on both datasets.
    }
    \label{fig:ablation_ratio}
    \vspace{-0.3cm}
\end{figure}

\textbf{Training schedule.}
Figure \ref{fig:ablation_epoch} presents the results of different training schedules.
On one hand,
a longer training schedule consistently improves the performances on both benchmarks.
On the other hand,
compared to VideoMAE \cite{videomae},
our method shows a faster convergence speed.
For example,
when pre-training for 200 epochs,
our models achieve 3.9\% and 6.8\% top-1 accuracy on SthSth V2 and Kinetics-400, respectively.

\begin{figure}[tp]
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=0.83\linewidth]{figs/epoch_ssv2.pdf}
        \vspace{-0.225cm}
        \subcaption{Results on SthSth V2}
    \end{minipage}
    \vspace{-0cm}
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=0.83\linewidth]{figs/epoch_k400.pdf}
        \vspace{-0.225cm}
        \subcaption{Results on Kinetics-400}
    \end{minipage}
\vspace{-0.3cm}
\caption{
\textbf{Training schedules.} 
A longer training schedule leads to more significant improvement.
}
\label{fig:ablation_epoch}
\vspace{-0.4cm}
\end{figure}

\input{tables/ablation_works.tex}

\textbf{Why does \Modelnamelight\  work?}
In Table \ref{tab:ablation_works}, 
we investigate the crucial designs of our Unmasked Teacher. 
\textbf{(i) Spatiotemporal attention}: 
In the 2nd and 3rd parts, 
we compare the student with spatial attention and spatiotemporal attention during fine-tuning. 
Our results indicate that utilizing joint attention significantly enhances performance. 
Moreover, 
employing spatiotemporal attention during pre-training further improves performance (the 4th part), 
validating our assumption that joint attention encourages interaction among all unmasked tokens.
\textbf{(ii) Masked modeling}:
In the 4th part,
we observe that masked modeling plays a crucial role. 
However, 
when using spatial attention during pre-training, 
masked modeling becomes detrimental. 
We argue that when processing each frame individually with a high mask ratio of 80\%,
the token alignment task becomes excessively challenging.
\textbf{(iii) Teacher attention}:
The 5th part shows that although CLIP-\textit{ST} achieves better performance after fine-tuning, 
directly applying it as the teacher model leads to a performance drop. 
We contend that without post-training in the video domain, 
CLIP-\textit{ST} may disrupt the representation learned in the image domain.

\textbf{Outperforming the CLIP teacher.}
In the image domain,
the prior research \cite{Dong2022CLIPII} has shown that,
CLIP itself with fine-tuning surpasses existing CLIP-targeted MIM methods \cite{mvp,fd_clip,milan,maskdistill}.
However,
Table \ref{tab:ablation_works} indicates that in the video domain,
the student model (the 4th part) clearly outperforms the teacher,
\textit{i.e.},
CLIP-$ST$ with our elaborate fine-tuning.
We attribute the success to masked video modeling with spatiotemporal attention,
which encourages the model to capture long-term dependencies among objects.

\textbf{Different teachers.}
In Table \ref{tab:teacher},
we adopt different models \cite{dino,clip,beitv2} as the unmasked teachers.
As expected, 
the student models clearly outperform the corresponding teacher models, 
which have undergone elaborate fine-tuning. 
It's important to note that both student and teacher models share the same architecture, 
further emphasizing the effectiveness of our approach.

\input{tables/ablation_teacher.tex}

\input{tables/ablation_ratio_mm.tex}

\textbf{Multi-modality masking ratios.}
In Table \ref{tab:ratio_mm}, 
we first alter the masking ratios of the image and video data.
Since we co-train image-text and video-text data with the same batch size, 
the GPU memory primarily depends on the video masking ratio. 
As expected, 
processing images requires a lower masking ratio of 50\%. 
Although higher masking ratios reduce memory consumption, 
the corresponding performances are lower. 
Additionally, 
masking too few (25\%) or too many (75\%) text tokens leads to inferior results.

\input{tables/ablation_objective_mm.tex}

\input{tables/ablation_design_mm.tex}


\textbf{Multi-modality pre-training objectives.}
For cross-modal retrieval,
utilizing either VTC or VTM for visual-text pairs is necessary.
In Table \ref{tab:objective_mm},
all loss weights are set to 1.
The 1st part reveals that VTM performs better than VTC.
Besides,
the 2nd part shows that combining VTC or MLM with VTM leads to a minor improvement,
while integrating all three objectives significantly enhances the performance.
Lastly,
without our unmasked teacher alignment,
the memory usage triples, 
while the performances drop.


\textbf{Other designs.}
Table \ref{tab:design_mm} showcases alternative designs for our multi-modality pre-training.
Firstly, 
we attempt to directly perform Stage 2 with a randomly initialized video encoder. 
For a fair comparison, 
we incorporate Kinetics-710 and conduct the same number of data iterations. 
However, 
the results demonstrate that the one-stage pre-training is challenging to converge, 
leading to poor performance. 
Secondly,
we randomly mask the video without an unmasked teacher for supervision,
which slightly reduces the overall performance. 
Additionally, 
we consider aligning the visual and text projection with the CLIP teacher,
since the teacher model also adopts contrastive learning. 
However, 
introducing extra alignment tasks turns out to be redundant and even harmful. 
Finally, 
we conduct extra pre-training without masks after masked pre-training.
Though it improves zero-shot performance (+1.5\% higher average recall accuracy),
the fine-tuned results are not as good as expected.


\input{tables/k400.tex}
\input{tables/k600_k700.tex}
\input{tables/mit.tex}

\subsection{Single-modality tasks}
We evaluate our method on two conventional video-only tasks:
recognizing and localizing actions on six large-scale benchmarks,
including the \textit{Kinetics} family (\textit{i.e.}, Kinetics-400, 600 and 700 \cite{k400,k600,k700}),
\textit{Moments in Time} V1 \cite{mit} and \textit{Something-Something} V2 \cite{sth} for action recognition,
and \textit{AVA} V2.2 \cite{ava} for spatiotemporal localization.


\input{tables/ssv2.tex}

\textbf{Kinetics.}
Table \ref{tab:k400} reports the SOTA methods with supervised and self-supervised learning on K400.
On one hand,
our \Modelnamelight\  with intermediate fine-tuning outperforms the previous models that rely on web-scale pre-training,
\textit{e.g.},
the \Modelnamelight-L achieves 0.4\% higher top-1 accuracy than MTV-H \cite{mtv} with only 1/10 of the FLOPs and 1/3 of the parameters.
On the other hand,
our \Modelnamelight\  surpasses its counterparts with masked video modeling,
\textit{e.g.},
compared with VideoMAE \cite{videomae} with 1600-epoch pre-training,
the \Modelnamelight-L with 400-epoch pre-training obtains 3.7\% accuracy improvement.
For K600 and K700,
our \Modelnamelight-L also obtains the SOTA performances (\textbf{90.5\%} and \textbf{83.6\%} see Table \ref{tab:k600_k700}).

\textbf{Moments in Time.}
As shown in Table \ref{tab:mit},
our \Modelnamelight-L achieves \textbf{1.0\%/1.7\%} higher top-1/5 accuracy compared to the advanced UniFormerV2-L \cite{uniformerv2}, 
while utilizing fewer FLOPs.
Note that MiT is more challenging due to the large inter-class and intra-class variation, 
thus the results demonstrate the robustness and effectiveness of our method.

\textbf{Something-Something.}
Distinct from previous benchmarks,
this particular dataset requires complex and long-term modeling to accurately recognize temporal-related actions, 
such as "pretending to close something without actually closing it". 
Without any additional data, 
our \Modelnamelight-L model outperforms the UniFormerV2-L \cite{uniformerv2} (74.4\% \textit{vs.} 73.0\% in Table \ref{tab:ssv2}) which was specifically tailored for temporal modeling. 
Additionally, 
our approach achieves comparable performances to VideoMAE \cite{videomae} with significantly fewer epochs. 
Intriguingly, 
VideoMAE performs worse when utilizing Kinetics for masked modeling, 
while our \Modelnamelight\  performs even better. 
This demonstrates the versatility and adaptability of our method, 
which can be applied to diverse video domains with the same pre-training.



\textbf{AVA.}
Table \ref{tab:ava} presents the results of the action detection on AVA.
Remarkably, 
our \Modelnamelight\  achieves 2.0 mAP improvement over the advanced VideoMAE \cite{videomae} with only K400 pre-training.
Furthermore, 
our method achieves the impressive \textbf{39.8} mAP with K710 pre-training, 
showcasing its robust transferability for spatiotemporal understanding.


\input{tables/ava.tex}
\input{tables/retrieval_zs.tex}


\input{tables/retrieval_ft.tex}
\input{tables/retrieval_ss.tex}

\subsection{Multi-modality tasks}
We further validate our model on two mainstream video-language tasks,
including video-text retrieval (MSRVTT \cite{msrvtt},
DiDeMo \cite{didemo}, ActivityNet \cite{anet}, LSMDC \cite{lsmdc}, MSVD \cite{msvd} and Something-Something \cite{lei2022revealing}) 
and video question-answering (ActivityNet-QA \cite{anet_qa}, MSRVTT-QA \cite{msrvtt_qa}, MSRVTT-MC \cite{msrvtt_mc} and MSVD-QA \cite{msrvtt_qa}).


\textbf{Zero-shot text-to-video retrieval.}
Table \ref{tab:retrieval_zs} indicates that the \Modelnamelight-B outperforms the top-performing models \cite{wang2022omnivl,lei2022revealing,Cheng2022VindLUAR} by \textbf{0.9\%}, \textbf{5.0\%}, and \textbf{4.6\%} R@1 on MSRVTT, DiDeMo, and ActivityNet, respectively.
In addition, our \Modelnamelight-L has set new records across all datasets. 
We see scores of \textbf{42.6\%}, \textbf{48.6\%}, \textbf{42.8\%}, \textbf{25.2\%}, and \textbf{72.2\%} on MSRVTT, DiDeMo, ActivityNet, LSMDC, and MSVD, in respective order. 
These notable results emphasize the exceptional robustness and effectiveness of our method.

\textbf{Text-to-video retrieval.}
Table \ref{tab:retrieval_ft} lists the fine-tuned results, 
where our \Modelnamelight-L significantly outperforms previous methods pre-trained with large-scale pairs \cite{clip4clip,xue2022clip,Wang2022InternVideoGV}. Specifically, 
our \Modelnamelight-L achieves \textbf{58.8\%} (+3.6\%), \textbf{70.4\%} (+9.2\%), \textbf{66.8\%} (+4.6\%), \textbf{43.0\%} (+9.0\%), and \textbf{80.3\%} (+21.9\%) on MSRVTT, DiDeMo, ActivityNet, LSMDC, and MSVD, respectively. 
Furthermore,
Table \ref{tab:retrieval_ssv2} showcases its impressive performances on the temporally-heavy SSV2-label and SSV2-template datasets, \textit{i.e.}, \textbf{73.3\%} and \textbf{90.8\%}, respectively.
These results demonstrate its profound capability for temporal modeling.

\textbf{Video question-answering.}
As shown in Table \ref{tab:vqa},
our \Modelnamelight\  outperforms the methods specifically designed for QA such as JustAsk \cite{yang2021just}, 
and achieves comparable performance with state-of-the-art models that pre-trained with large-scale pairs \cite{yang2022zero,Wang2022InternVideoGV,videococa},
which demonstrates its powerful capability of complex multimodal reasoning.


\input{tables/vqa.tex}