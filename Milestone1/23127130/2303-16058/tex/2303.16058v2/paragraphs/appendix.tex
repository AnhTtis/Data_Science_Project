\appendix

% --- PDF will be split by an editor (e.g. macOS preview), so need to restart from page 1
\setcounter{page}{1}

% --- repeat the title (AT: haven't found a more elegant way to do this...)
\twocolumn[
\centering
\Large
\textbf{Unmasked Teacher: Towards Training-Efficient Video Foundation Models} \\
\vspace{0.5em}Supplementary Material \\
\vspace{1.0em}
] %< twocolumn

\input{tables/model_architecture}

\input{tables/stage1_hyperparameters}


\section{More implementation details}


\subsection{Model architecture and training details}
In this section,
we introduce the model architectures and training hyperparameters in our experiments.


\textbf{Stage 1.}
In Stage 1,
we train the video encoder from scratch,
which is a vanilla ViT \cite{vit} without temporal downsampling.
We use the same patch size for both ViT-B and ViT-L,
\textit{i.e.}, $1$$\times$$16$$\times$$16$ ($T$$\times$$H$$\times$$W$).
To align with the unmasked teacher,
we use a simple linear projection,
including Layer Normalization \cite{ln} and one linear layer.
The example architecture is shown in Table \ref{tab:model_architecture}.
For pre-training,
we follow most of the hyperparameters in VideoMAE \cite{videomae},
as presented in Table \ref{tab:stage1_hyperparameters}.
However, 
to prevent overfitting, 
we use drop path \cite{droppath} in our approach.


\textbf{Stage 2.}
In Stage 2,
we equip the pre-trained video encoder with a text encoder and cross-modal decoder.
Following Singularity \cite{lei2022revealing},
for the base model,
we use the first 9 layers and the last 3 layers of BERT$_{base}$ to initialize the text encoder and decoder, respectively.
While for our large model,
we respectively adopt the first 19 layers and the 5 layers of BERT$_{large}$.
For pre-training,
we set all the loss weights to 1.
And more details are shown in Table \ref{tab:stage2_hyperparameters}.


\input{tables/stage2_hyperparameters}
\input{tables/ar_hyperparameters}

\textbf{Action Recognition.}
We adopt the Stage-1 pre-trained video encoder and add an extra classification layer for fine-tuning.
Detailed hyperparameters for different datasets are shown in Table \ref{tab:ar_hyperparameters}.
In our experiments,
we have tried to fine-tune the Stage-2 pre-trained video encoder,
but the results on Kinetics are similar.


\textbf{Action Detection.}
Following VideoMAE \cite{videomae} and ST-MAE \cite{st_mae},
we add ROIAlign with MaxPooling to generate the regions of interest.
Since we the Kinetics pre-trained models adopt sparse sampling \cite{tsn},
we use a frame span of 300 for action detection,
which is the default frame number of Kinetics videos.
More details are listed in Table \ref{tab:ad_hyperparameters}.

\textbf{Video-text retrieval.}
For fine-tuning,
we adopt the same architecture as in Stage 2,
but we only apply VTC and VTM losses.
For all datasets,
we sparsely sample 12 frames for both training and testing.
More details are listed in Table \ref{tab:ret_hyperparameters}.
For a fair comparison,
we follow Singularity \cite{lei2022revealing} to apply flip augmentation for SSV2 retrieval,
which may harm the performance of this temporal-related dataset.

\textbf{Video question-answering.}
Following the previous works \cite{lei2022revealing,Cheng2022VindLUAR,li2021align},
we formulate this task as text generation instead of classification.
We add an extra multi-modal decoder that takes the output of the cross-modal decoder as the keys/values.
And it decodes the answer text with ``[CLS]'' as a start.
We follow \cite{lei2022revealing,Cheng2022VindLUAR} to adopt the same architecture as the cross-modal decoder,
and initialize it using the pre-trained cross-modal decoder.
As for multiple-choice question-answering,
we follow \cite{lei2022revealing,li2021align,Cheng2022VindLUAR} to convert it to a text-to-video retrieval task,
where the question and candidate answers are concatenated.
The detailed hyperparameters are shown in Table \ref{tab:qa_hyperparameters} and Table \ref{tab:mc_hyperparameters}.


\input{tables/ad_hyperparameters}
\input{tables/qa_hyperparameters}
\input{tables/mc_hyperparameters}

\input{tables/ret_hyperparameters}
\input{tables/more_retrieval_zs.tex}
\input{tables/more_retrieval_ft.tex}

\section{More results}

\subsection{Video-text retrieval}
Table \ref{tab:more_retrieval_zs} and Table \ref{tab:more_retrieval_ft} show more zero-shot and fine-tuned retrieval results on MARVTT \cite{msrvtt}, DiDeMo \cite{didemo}, ActivityNet \cite{anet}, LSMDC \cite{lsmdc} and MSVD \cite{msvd}.


\subsection{Dataset descriptions}
We show the statistics of pre-training datasets in Table \ref{tab:statics_pretrain},
and downstream datasets in Table \ref{tab:statics_downstream}.


\input{tables/statistics_pretrain}
\input{tables/statistics_downstream}