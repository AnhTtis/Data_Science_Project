\section{Method}
In this section, 
we introduce our \textbf{U}n\textbf{M}asked \textbf{T}eacher (\Modelname) for masked video modeling and the progressive pre-training framework for temporal-sensitive video foundation models, 
as illustrated in Figure \ref{fig:framework}.

\subsection{Unmasked Teacher}
As discussed in the introduction, 
directly adapting the public Image Foundation Model (IFM) to Video Foundation Model (VFM) is challenging \cite{xclip,uniformerv2},
thus we propose using IFM as a teacher to train a VFM from scratch. 
Given the limited data scale, 
we leverage mask modeling \cite{mae} to make good use of the video data. 
However, 
unlike VideoMAE \cite{videomae}, 
we 
selectively align the unmasked tokens with the teacher,
removing an extra decoder for efficient training.


\textbf{Architecture.}
We choose CLIP-ViT \cite{clip} as an unmasked teacher due to its rich semantics that are learned with language guidance, 
which is beneficial for our following multi-modality learning. 
To fully impart the teacher's knowledge, 
we maintain its spatial architecture to process each video frame individually.
For our backbone, 
we apply the vanilla ViT without a class token. 
We employ spatiotemporal attention \cite{timesformer} to encourage all the unmasked tokens to interact with each other.
For better alignment with the spatial teacher, 
we do not use temporal downsampling,
thus the tokens can be aligned frame by frame.

\textbf{Masking.}
Following VideoMAE,
we use a high masking ratio (\textit{e.g.}, 80\%) to cut down video redundancies.
However,
the aggressive random masking may only retain the background tokens,
which contain insignificant information and hinder the teacher's knowledge transfer.
To enhance target effectiveness,
we apply the semantic masking \cite{milan} frame by frame,
where the tokens with important clues are maintained at higher probabilities.
Specifically,
given the class token $\mathbf{z}_{cls}$$\in$$\mathbb{R}^{1\times C}$ and the spatial tokens $\mathbf{Z}$$\in$$\mathbb{R}^{L\times C}$ in the $t$-th frame of CLIP-ViT ($L$$=$$H$$\times$$W$ is the token number and $C$ is the token dimension),
we calculate the attention score in the last self-attention \cite{vit} layer:
\begin{align}
\begin{split}
\mathbf{A} = {}& \sum_{n=1}^{N} \mathbf{A}_{n}(Q_{n}(\mathbf{\mathbf{z}_{cls}}), K_{n}(\mathbf{Z})) / N ,
\end{split}\\
\mathbf{A}_{n}(\mathbf{q}, \mathbf{k}) = {}& {\rm softmax}(\mathbf{q}\mathbf{k}^{T}/\sqrt{C/N}),
\end{align}
where $N$ is the head number,
and $Q_{n}(\cdot)$ and $K_{n}(\cdot)$ are the linear projections in the $n$-th head.
The $\mathbf{A}$$\in$$\mathbb{R}^{1\times L}$ represents the semantic importance of each token,
and we select the unmasked tokens by a multinomial distribution based on $\mathbf{A}$ to retain the informative objects in each frame.
Moreover,
we sparsely sample frames from the raw videos \cite{tsn},
which provides a more complicated action context due to the large frame stride.
The strategy encourages the model to reason long-term spatiotemporal relationships among objects.

\textbf{Target.}
For the teacher model, 
we input all $L$ spatial tokens along with the class token, 
frame by frame. 
In contrast, 
for the student model, 
we only input the unmasked tokens, 
which are equal to $L(1-r)T$ tokens, 
where $r$ is the masking ratio and $T$ is the frame number. 
To distill the rich semantics more effectively, 
we process the output teacher tokens using the pre-trained visual projection,
which is designed to establish meaningful connections between visual and text embeddings.
Additionally, 
we add a simple linear projection for the student model to align the token dimension. 
We select the corresponding unmasked token from the student and teacher, 
and compute the mean squared error (MSE) between the normalized pairs.
Compared to low-level pixel reconstruction,
token alignment requires a high-level understanding,
which is beneficial for multi-modality learning.


\subsection{Progressive Pre-training}
For general video understanding,
it is vital for the foundation model to handle video-language tasks.
However,
directly training such a model from scratch is inefficient.
For example,
CoCa \cite{coca} utilizes 4.8B data to train 5 days on 2,048 CloudTPUv4 chips.
Therefore,
we introduce a training-efficient framework with progressive pre-training.

\textbf{Pre-training pipeline.}
Figure \ref{fig:framework} outlines our pipeline.
In Stage 1, 
we train the ViT from scratch using only high-quality videos and guidance from Unmasked Teacher. 
The masked video modeling fully mines knowledge from the videos, 
resulting in a model that excels at video-only tasks.
In Stage 2, 
we equip the pre-trained ViT with a text encoder and cross-modal decoder,
initialized with the well-prepared language model. 
And we conduct multi-modality training with large-scale vision-text pairs, 
enabling the model to handle complex video-language tasks.
It's worth noting that currently, 
open-source language models are larger and more diverse than vision models,
making it easy to scale up our foundation models. 
For example, 
the largest OPT \cite{opt} has 175B parameters, 
while ViT-G \cite{Zhai2021ScalingVT} only has 1.8B.


\textbf{Pre-training objectives.}
For both stages, 
we utilize Unmasked Teacher to perform Unmasked Token Alignment (\textbf{UTA}). 
In Stage 2, 
we employ three other popular objectives:
\textbf{(i)} Video-Text Contrastive (\textbf{VTC}) learning, which aims to align the pooled unmasked video and text embeddings. 
We use the symmetric contrastive loss \cite{bain2021frozen} to maximize the mutual information.
\textbf{(ii)} Video-Text Matching (\textbf{VTM}) enhances cross-modal fusion by aligning the unmasked video and text tokens. 
We adopt the binary cross-entropy loss with hard negative mining \cite{li2021align,lei2022revealing}.
\textbf{(iii)} Masked Language Modeling (\textbf{MLM}) uses the cross-modal decoder to predict masked words from the other text and unmasked video tokens. 
We follow the BERT \cite{devlin2018bert} strategy but mask 50\% of the text tokens.