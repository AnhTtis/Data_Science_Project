\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\input{math_commands.tex}
\usepackage{hyperref}
\usepackage{url}
\usepackage{pifont}
\usepackage{bbm}
\usepackage{microtype}
\usepackage{booktabs} % for professional tables
\usepackage{soul}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{graphbox}
% \usepackage{caption}
% \usepackage{sidecap}
\usepackage{array}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{cuted}

\newcommand{\ours}{CPP\xspace}
\newcommand{\ourloss}{CPL\xspace}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\authorskip}{\qquad}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning}

\author{Zhuowei Li\textsuperscript{1} \authorskip Long Zhao\textsuperscript{2} \authorskip Zizhao Zhang\textsuperscript{3} \authorskip Han Zhang\textsuperscript{2} \authorskip Di Liu\textsuperscript{1}\\ Ting Liu\textsuperscript{2} \authorskip Dimitris N.\ Metaxas\textsuperscript{1}\\[2mm]
\textsuperscript{1}Rutgers University \authorskip \textsuperscript{2}Google Research \authorskip \textsuperscript{3}Google Cloud AI}

\maketitle
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt~(\ours) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that \ours excels in four challenging class-incremental learning benchmarks, resulting in 4\% to 6\% absolute improvements over state-of-the-art methods. Moreover, \ours does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture. Our code will be made publicly available.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Continual learning~\cite{Thrun95k}, the capability of learning sequentially from a continuous stream of potentially correlated data, is critical for modern intelligent systems, as the world is non-stationary~\cite{EmbracingChange}. Yet, existing deep neural networks are known to be prone to \textit{catastrophic forgetting}~\cite{CF1}, \ie models suffer from dramatic performance degeneration on earlier learned tasks when absorbing new information. 

Among the plethora of strategies proposed to harness continual learning challenges~\cite{3cl}, prototypes (\eg, class mean embeddings~\cite{proto_net}) exhibit a promising functionality as they can retain previous knowledge in a memory-efficient manner~\cite{PROTO_AUG} and mitigate bias towards the latest task~\cite{iCaRL, Yu2020SemanticDC} when coupled with a nearest class mean (NCM)~\cite{NCM} classifier. However, prototypes themselves are also subject to abrupt efficacy drops due to the following two issues:
\begin{itemize}
\item \textbf{Semantic drift}. Learning a sequence of tasks with a unified model can be viewed as generating a sequence of snapshots of the model, and only the latest version is retained. Thus a data sample at inference and its corresponding prototype are, in fact, encoded by different embedding functions (except for data samples from the latest task). This inconsistency causes severe semantic drift in the latent space as shown in Fig.~\ref{fig:proto_forget} (top).
\item \textbf{Prototype interference}. When new data samples that bear similar semantics with previous data appear, their embeddings tend to locate near preexisting prototypes, causing interference as illustrated in Fig.~\ref{fig:proto_forget} (bottom).
\end{itemize}
During the continual learning process, both phenomena occur simultaneously and cause \textit{forgetting}.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{./figures/proto_forget.pdf}
\end{center}
\caption{An illustration of \textit{semantic drift} and \textit{prototype interference} in the latent space. Different colors represent separate classes.}
\label{fig:proto_forget}
\end{figure}

Existing prototype-boosting methods manage to mitigate semantic drifts by saving and replaying previously seen exemplars~\cite{iCaRL, De_Lange_2021_ICCV} or post-compensating for the drifts through approximations~\cite{Yu2020SemanticDC,Gu_Deng_Wei_2021}. Yet, these methods either require large memory footprints or are prone to errors as the task sequence accumulates. Moreover, the interference issue is less attended to, and is often left implicitly to additional regularization terms~\cite{MAS, LwF, EMC}. In contrast, we seek to address both semantic drift and prototype interference issues in an explicit and independent manner. Specifically, we propose to explicitly align the embedding functions being used for generating prototypes and inference to avoid semantic drifts, and directly restrain the distances among prototypes in the embedding space to reduce interference. 

Drawing inspiration from prompt-tuning~\cite{prompt_tuning,VPT}, a new transfer learning paradigm which enables a tiny portion of extra learnable tokens to adapt a frozen Transformer~\cite{transformer} to down-stream tasks, we propose to leverage task-specific prompts to avoid semantic-drift. In particular, we allow a data sample at inference to retrieve its corresponding task-specific prompts and assemble the exact embedding function that is used for generating its corresponding prototype. Within our design, a frozen embedding function (\ie, a pre-trained Transformer) is deemed as consolidated global knowledge and shared across tasks to maintain the stability of a system. Task-specific prompts, on the other hand, learn task-level specializations and keep the system plastic.

To reduce prototype interference in the embedding space, we optimize task-specific prompts over a novel contrastive prototypical loss. The proposed learning objective encourages intra-class clustering and increases inter-class distances upon on a mixture of data embeddings and up-to-now prototypes. Since we only retain previous knowledge as prototypes and set them as anchors, task-specific prompts can effectively steer current prototypes to avoid interference without saving and replaying explicit exemplars. Moreover, we propose the multi-centroid prototype strategy, which leverages a group of fictitious embeddings instead of one single mean embedding to characterize the distribution of a class. This strategy is seamlessly merged into our framework and enhances the overall performance.

We term our method \textbf{C}ontrastive \textbf{P}rototypical \textbf{P}rompt (\ours), a simple and novel continual learning framework that explores embedding space holistically. Emperically, \ours excels in four challenging class-incremental benchmarks including split CIFAR-100, 5-datasets, split ImageNet-subset, and split ImageNet-R, bringing around 4\% to 6\% absolute improvements over state-of-the-art methods. Moreover, \ours is rehearsal-free and consumes at least $5\times$ times fewer additional memories than existing alternatives. The main contributions of this work can be summarized as follows:
\begin{itemize}
	\item We propose Contrastive Prototypical Prompt~(\ours), a simple and novel framework for rehearsal-free continual learning. It leverages contrastively learned task-specific prompts to effectively address both semantic drift and prototype interference obstacles.
	\item We present multi-centroid prototype strategy to better characterize the class distribution and improve representativeness of prototypes. It is seamlessly merged into \ours and further boosts the overall performance.
	\item We demonstrate empirically that \ours surpasses state-of-the-art methods by a significant margin under a light memory budget. Each proposed component is thoroughly studied and demonstrates clear and additive benefits.
\end{itemize}

\section{Related Work}
\noindent\textbf{Continual learning}. Existing algorithms can be classified into three categories~\cite{PARISI201954, 3cl, EmbracingChange}. Regularization-based methods aim to strike a balance under \textit{stabilityâ€“plasticity dilemma}. Among them, parameter-regularization methods limit the extend~\cite{EMC, SI} or the direction~\cite{OGD, GPM} of changes in the parameter space, while functional-regularization methods constrain the change of functionality for `anchor points'~\cite{FR, titsias2019functional, NEURIPS2020_2f3bbb97} in the function space. Despite their universality, these methods struggle in performance when handling long sequences of tasks~\cite{EmbracingChange}. Modular-based methods enable context-specific components to reduce knowledge interference by reorganizing existing resources~\cite{PackNet, pmlr-v80-serra18a} or allocating extra learning capacities~\cite{PNN, yoon2018lifelong, Li2019LearnTG}. However, most modular-based methods require test-time task identities and are hard to scale. In practice, rehearsal-based methods~\cite{dark_memory, Co2L} exhibit the most versatility and robustness by saving and rehearsing previous data samples. Nevertheless, this strategy is sensitive to buffer sizes~\cite{GDumb, EmbracingChange} and becomes infeasible under restricted memory budget or privacy-sensitive scenarios. \ours proposed here is a hybrid method that combines certain strengths from different sub-categories without necessarily inheriting their limitations. 

\noindent\textbf{Prototypes for continual learning}.
It has been shown that embedding is less prone to information loss~\cite{Yu2020SemanticDC, Davari_2022_CVPR} and the parameterized linear classifier is one of the critical reasons for abrupt forgetting due to biases towards the latest task~\cite{Zhang_2021_CVPR}. As such, most prototype-related approaches~\cite{iCaRL,Yu2020SemanticDC, PROTO_AUG} use prototypes in combination with a NCM classifier. PASS~\cite{PROTO_AUG}, on the other hand, leverages prototypes as anchors in the latent space to reduce semantic overlap. To mitigate the semantic drift issue, existing methods save explicit exemplars to update prototypes~\cite{iCaRL, De_Lange_2021_ICCV} or post-compensate drifts through approximating the drifts from current data~\cite{Yu2020SemanticDC}. In contrast, \ours prevents semantic drifts from the beginning and also explicitly deals with prototype interference. Furthermore, \ours leverages multi-centroid prototypes instead of the mean embedding to better characterize context distributions. 

\noindent\textbf{Prompt tuning}. Initializing deep neural networks from pre-trained weights is a widely adopted practice. However, typical fine-tuning techniques do not always benefit downstream tasks~\cite{kumar2022finetuning}. Prompt-tuning~\cite{prefix_tuning, prompt_tuning} has emerged in natural language processing as an alternative to reuse pre-learned knowledge. VPT~\cite{VPT} adapts prompt-tuning techniques to the vision domain. Recently, it has also been introduced to continual learning. Both L2P~\cite{l2p} and DualPromt~\cite{DUAL_PROMPT} leverage a prompt pool or global prompts that can be shared across tasks to learn incremental knowledge. S-prompts~\cite{S-prompt} uses domain-specific prompts to tackle domain-incremental learning. Herein, \ours applies task-specific prompts to counteract semantic drifts and prototype interference. Additionally, \ours leverages prototypes as classifiers instead of training a separate parametric classifier.

\section{Methodology}
We begin by describing the problem setup and, along the way, introduce the notations in Sec.~\ref{sec:setup}. Then we present a minimum feasible prototype-based framework which serves as our baseline model in Sec.~\ref{sec:baseline}. In Sec.~\ref{sec:cpp}, we introduce the proposed \ours, which builds upon the baseline model. At last, we elucidate the multi-centroid prototype strategy in Sec.~\ref{sec:multi-centroid}. An overview of our proposed framework is displayed in Fig.~\ref{fig:overview}.
\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{./figures/overview.pdf}
	\end{center}
	\caption{An overview of \ours. Different colors denote different classes. \textbf{Left:} Along the learning process, knowledge from earlier tasks is retained as prototypes and set as anchors in the embedding space. Task-specific prompts are learned to avoid prototype interference. \textbf{Right:} During inference, a group of candidate prompts are first retrieved and then followed by a fine-grained matching process.}
	\label{fig:overview}
\end{figure*}

\subsection{Problem Setup and Notion}\label{sec:setup}
Supervised continual learning can be defined as learning a model over a sequence of $T$ tasks $\mathcal{T}_{1:T}=\{\mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_T\}$. Each task $\mathcal{T}_t$ is associated with a dataset $\mathcal{D}^{t}=\{(\vx_{i}^{t}, y_{i}^{t})_{i=1}^{n_{t}}\}$ containing $n_{t}$ data pairs, where $\vx$ is the input and $y$ is its corresponding label. Each data pair $(\vx_{i}^{t}, y_{i}^{t}) \in  (\mathcal{X}^{t}\times\mathcal{Y}^{t})$ belongs to an unknown distribution $(\mathcal{X}^{t}\times\mathcal{Y}^{t})$ and $\mathcal{Y}^{t} \cap \mathcal{Y}^{t'}=\emptyset$ while ${t}\neq{t'}$. In general, a neural network at session $t$ can be decoupled into an embedding function $f_{\theta^t}(\cdot):\R^{W\times H\times C}\rightarrow \R^D$ and a classifier $g_{\phi^t}(\cdot):\R^D\rightarrow \R^K$ that parameterized by $\theta^t$ and $\phi^t$, respectively. The overall learning objective is to learn a $f_{\theta^t}(\cdot)$ and $g_{\phi^t}(\cdot)$ pair that excels in all learned tasks $\mathcal{T}_{1:t}$. Note that in this paper, we focus on the challenging rehearsal-free class-incremental setting, where the task identity $t$ is not available during inference and we do not augment the dataset $\mathcal{D}^t$ of task $\mathcal{T}_t$ by saving and replaying preceding exemplars during the training stage. 

\subsection{A Training-free Baseline}\label{sec:baseline}
Let $\mathcal{D}_k^t$ denote the dataset belonging to class $k$ at task $t$, and the prototype of class $k$ is produced as the class mean embedding following \cite{iCaRL}:
\begin{equation}
	\label{eq:class_mean}
	\vmu_k = \frac{1}{|\mathcal{D}_k^t|}\sum_{\vx \in \mathcal{D}_k^t} f_{\theta}(\vx).
\end{equation}
Here, $\theta$ is initialized by a pre-trained ViT~\cite{vit} and is kept frozen throughout the entire learning process. We then do classification using the nearest-class-mean (NCM)~\cite{NCM} classifier:
\begin{equation}
	\label{eq:argmax}
	y^*=\underset{y\in\{1, \dots, K\}}{\argmin}\{d(\vu_y, f_\theta(\vx))\},
\end{equation} 
where $d:\R^{D} \times \R^{D} \rightarrow \R$ is a distance measurement. In our design, we always define $d$ as the cosine distance (similarity). This simple and training-free baseline model produces decent results under an appropriate embedding function (see Table~\ref{table:main_ablation}), confirming the crucial role played by the embedding function and effectiveness of prototypes in continual learning. 

\subsection{Contrastive Prototypical Prompt}\label{sec:cpp}

\subsubsection{Steering prototypes with task-specific prompts} 
Ideally, a perfect static embedding function can project data samples to the places that are nearest to their corresponding prototypes in the embedding space, thus preventing forgetting given the model described in Sec.~\ref{sec:baseline}. In practice, expecting such an omnipotent embedding function is not realistic. Therefore, we need to allow the embedding function to dynamically adapt to new inputs while minimizing forgetting. To this end, we leverage a tiny amount of extra learnable tokens for each task (\ie, task-specific prompts) to learn information upon a frozen embedding function. 

Specifically, we prepend a prompt $\vp_i \in \R^{L_p\times D}$ to the existing tokens in the $i$-th layer of a Transformer, where $L_p$ is the length of the prompt and $D$ denotes the embedding dimension. The computation of the $i$-th layer is then defined as:
\begin{equation}
	[\vc_i, \ve_i] = T_i([\vc_{i-1}, \vp_{i-1}, \ve_{i-1}]),
\end{equation}  
where $T_i$ represents a multi-head self-attention block followed by a feed-forward network. Here, $\vc\in\R^{1\times D}$ denotes the class token, and $\ve\in\R^{L_e\times D}$ is the existing data tokens. The operator $[\cdot]$ performs concatenation along the sequence length dimension. We adopt deep prompt~\cite{VPT} by adding prompts to all $S$ layers. Thus the task-specific prompt for task $t$ is given as $P^t=\{\vp^t_1, \vp^t_2, \dots, \vp^t_S\}$, and the embedding function for task $t$ can be rewritten as:
\begin{equation}
	f_{\theta^t}(\cdot) \rightarrow f_{\{\theta, P^t\}}(\cdot).
\end{equation}
We maintain a collection of task-specific prompts in the memory space and each task-specific prompt is associated with a key and value prototypes, which will be elaborated on later. Thanks to the design of task-specific prompts, knowledge interference among tasks can be significantly reduced since different tasks do not share the same parameter space. Moreover, semantic drifts due to the dynamic nature of the embedding function can also be avoided by assembling the frozen embedding function with the correct task-specific prompt at inference (detailed in Sec.~\ref{sec:inference}). 
\subsubsection{Contrastive prototypical loss (\ourloss)}\label{sec:cpl}
In this section, we present a novel loss function to optimize task-specific prompts, so that the prototype interference issue can also be addressed. Recall that our model maintains information from different classes as prototypes and uses a non-parametric classifier (\ie, NCM). Therefore, when added to the frozen embedding function, we expect that a well-optimized task-specific prompt can project data samples to a space where intra-class data samples are compact and inter-class samples are diverged given a measurement of distance (or similarity). 

Based on the above rationale, we propose \ourloss which explicitly encourages the compactness among intra-class entities while pushing away inter-class entities over a union of data embeddings and prototypes. During task $t$, we let $I=\{(\vx_1, y_1),\dots,(\vx_N, y_N)\}$ denote a batch of $N$ image pairs  and $Z=\{\vz_1, \dots,\vz_N\}$ be their corresponding embeddings. Here, omitting subscripts, $\vz$ is acquired as $\vz=m_{\sigma^t}(f_{\{\theta; P^t\}}(\vx))$, where $m_{\sigma^t}(\cdot)$ is a multi-layer perceptron (MLP) neck parameterized by $\sigma^t$. It is worth noting that $m_{\sigma^t}(\cdot)$ is re-initialized at each new task and disposed during inference. The learning objective is defined as:
\begin{gather} 
	\mathcal{L} = \frac{1}{N}\sum_{i\in \{1,\dots, N\}}{\mathcal{L}_i},\\ 
	\mathcal{L}_i = \frac{-1}{|P(i)|}\sum_{\vz_p\in P(i)}\log{\frac{\exp(\vz_i\cdot \vz_p/\tau)}{\underset{\vz_n\in N(i)\cup \hat{U}}\sum\exp(\vz_i \cdot \vz_n/\tau)}},
	\label{eq:proto_con}
\end{gather}
where $P(i)=\{\vz_p \in Z: y_p = y_i\}$ denotes a set of positive samples \wrt embedding $\vz_i$, and $N(i)=\{\vz_n \in Z: y_n \neq y_i\}$ represents a set of negative samples that do not share the same label as $\vz_i$. Additionally, we set $\hat{U}=\{\hat{\vu}_1,\dots,\hat{\vu}_k\}$ which contains value prototypes of preceding $k$ classes as negative anchors. Fig.~\ref{fig:overview} (left) illustrates the idea of our learning objective. To better restrain the discrimination boundaries, we apply prototype augmentation following~\cite{PROTO_AUG}. At each iteration, prototypes in $U$ are randomly sampled and perturbed by a scaled Gaussian noise $\hat{\vmu} = \vmu + m * \ve$, where $\ve\sim\mathcal{N}(0, 1)$ and $m$ is a scale factor computed as the average variance of the corresponding class embeddings.

\ourloss differs from the canonical contrastive loss~\cite{SimCLR, SCL} in the following two main aspects. First, we set prototypes of preceding classes as negative anchors in the latent space. By doing so, negative prototypes can retain spaces for previously seen data, thus preventing prototype interference. Second, we only focus on the alignment of positive embeddings and do not constrain the intra-class uniformity, which is considered one of the critical properties that attribute to the success of contrastive representation learning~\cite{alignment_uniformity}. Specifically, we do not pair samples from the same category as negative pairs in the denominator. Since the NCM classifier selects the closest prototype for classification, increasing intra-class uniformity can adversely enlarge the distance between a sample and its corresponding prototype. Another way to understand this deisgn is from an energy perspective and we refer to supplementary for more details on it.

\subsubsection{Inference by reemerging model snapshots}\label{sec:inference} 
Herein, we present how to do inference with \ours. Given a target data sample, we manage to retrieve its corresponding task-specific prompt to assemble the intact embedding function (\ie, the model snapshot). Note that this process is analog to predicting task identities and can be omitted under task-incremental learning.

Considering the prototype of class $k$ in task $t$, we decouple it into a \textit{key} prototype $\vu_k=\frac{1}{|{D}_k^t|}\sum_{\vx\in\mathcal{D}_k^t}f_{\theta}(\vx)$ and a \textit{value} prototype $\hat{\vu}=\frac{1}{|{D}_k^t|}\sum_{\vx\in\mathcal{D}_k^t}f_{\{\theta, P^t\}}(\vx)$ corresponding to the class mean embedding before and after learning task $t$, respectively. A collection of key prototypes $U = \{\vu_1,\dots,\vu_K\}$ and value prototypes $\hat{U} = \{\hat{\vu}_1,\dots, \hat{\vu}_K\}$ of $K$ learned classes are saved in memory. During inference, a coarse query vector $\vq:\R^{1\times D}$ of target sample $\vx$ is first generated, and then we use a query function $q(\vq, U, r)$ to find $r$ nearest key prototypes and retrieve their corresponding prompt groups $\{P^1,\dots,P^J:J\leq r\}$. Note that we have $J\leq r$ since prototypes of different classes may share the same task-specific prompt. Here, $\vq$ is simply the class token from the last layer: $\vq = f_{\theta}(\vx)$ (exact indexing operation is omitted to avoid notion clutter) and the query function measures the pairwise cosine similarity between $\vq$ and $U$. By treating retrieved prompts as candidates, we generate a set of fine-grained query vectors $\hat{Q}=\{\hat{\vq}^j: j\in[1, J]\}$, where $\hat{\vq}^j=f_{\{\theta, P^j\}}(\vx)$. Then the final class prediction is made following:
\begin{equation}
	\label{eq:final_predict}
	y^*=\underset{y}{\argmin}\{d(\hat{\vu}_y,\hat{\vq}^j): y\in[1,K], j\in[1,J]\}.
\end{equation}
Fig.~\ref{fig:overview} (right) depicts the information flow of the inference process. Detailed algorithms are provided in supplementary. 

There are two key steps that ensure the success of the above inference process. First, we need to retrieve a group of candidate prompts that contain the target prompt. This is secured by an appropriate embedding function through which a data sample tends to locate in the vicinity of its corresponding distribution mass center (empirically validated in supplementary). Second, we require the retrieved prompts, \ie, both target prompt and mismatched prompts, attribute to the final prediction. Thanks to our proposed contrastive learning objective, the target prompt will reduce the distance between the sample and its corresponding value prototype, while mismatched prompts will act in the opposite way. Hence, the behavior of both target and mismatched prompts is beneficial for the NCM classifier.
\begin{figure}[t]
\begin{center}
	\includegraphics[width=0.45\textwidth]{./figures/multi_centroid.pdf}
\end{center}
\caption{Different colors denote different classes. Small and large circles represent data samples and prototypes, respectively.}
\label{fig:multi-centroids}
\end{figure}

\subsection {Multi-centroid Prototypes}\label{sec:multi-centroid}
Existing literature in continual learning simply adopts class mean embeddings as prototypes~\cite{Yu2020SemanticDC,PROTO_AUG,FACT}. In this case, it implicitly assumes that a distribution in the latent space is convex and isotropic, and the distance function belongs to Bregman divergence~\cite{proto_net}. This premise can fail in practice as no strict constraints are imposed on embedding distributions, and the cosine distance is not Bregman divergence (see Fig.~\ref{fig:multi-centroids} for an example). To this end, we propose to use multi-centroid prototype in which we generate a group of fictitious embeddings to characterize the target class distribution. Specifically, given a set of embeddings from class $k$, we calculate the similarity matrix $\mS_k:\R^{|\mathcal{D}_k|\times |\mathcal{D}_k|}$ by measuring the pair-wise cosine similarities between embeddings. $\mS_k$ is used as the affinity matrix for spectral clustering~\cite{spectral_clustering} to generate $C$ centroids $\{\vu_{k,c}\}_{c=1}^C$. During training and inference, we can directly substitute all prototypes with their corresponding multi-centroid prototypes in \ours. 

It is worth pointing out that the multi-centroid strategy is also complementary to our inference process. It can be viewed as test-time hard-case mining. For example, given a data sample which is far from its mass center or it is in the mixture of several class distributions, the multi-centroid strategy will allow more candidate prompts to be retrieved to maintain accuracy. In contrast, when a data sample is close to its corresponding mass center and it is in a region where class distributions are separated, this strategy will retrieve fewer candidate prompts to improve efficiency. The upper and lower bounds of the retrieved prompt number are given as $r$ and $\max(1, r/C)$, respectively.

\begin{table*}[t]
\begin{center}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{l @{\hskip +0.1cm}c cc c cc c cc}
	\toprule 
	\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} &\multirow{2}{*}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{5-datasets}} &\multirow{2}{*}{\textbf{Buffer}} & \multicolumn{2}{c}{\textbf{Split ImageNet-R}} \\
	%  &
	& &  Avg. Acc ($\uparrow$) & Forget ($\downarrow$) & & Avg. Acc ($\uparrow$) & Forget ($\downarrow$) & & Avg. Acc ($\uparrow$) & Forget ($\downarrow$) \\
	\midrule
    ER~\cite{ER} &\multirow{5}{*}{5000 }& 82.53\scriptsize{$\pm$0.17} & 16.46\scriptsize{$\pm$0.25}&\multirow{5}{*}{500}& {84.26\scriptsize{$\pm$0.84}} & 12.85\scriptsize{$\pm$0.62} &\multirow{5}{*}{5000 }& 65.18\scriptsize{$\pm$0.40} & 23.31\scriptsize{$\pm$0.89} \\
	BiC~\cite{BiC} & & 81.42\scriptsize{$\pm$0.85} & 17.31\scriptsize{$\pm$1.02} && 85.53\scriptsize{$\pm$2.06} & 10.27\scriptsize{$\pm$1.32}&& 64.63\scriptsize{$\pm$1.27} & 22.25\scriptsize{$\pm$1.73} \\
	GDumb~\cite{GDumb}  & & 
	81.67\scriptsize{$\pm$0.02} & - &&- &- && 65.90\scriptsize{$\pm$0.28} & -  \\
	DER++~\cite{dark_memory}  & & 83.94\scriptsize{$\pm$0.34} & 14.55\scriptsize{$\pm$0.73} && 84.88\scriptsize{$\pm$0.57} & 10.46\scriptsize{$\pm$1.02} && 66.73\scriptsize{$\pm$0.87} & 20.67\scriptsize{$\pm$1.24} \\
	Co$^2$L~\cite{Co2L}  & & 82.49\scriptsize{$\pm$0.89} & 17.48\scriptsize{$\pm$1.80} && 86.05\scriptsize{$\pm$1.03} & 12.28\scriptsize{$\pm$1.44} && 65.90\scriptsize{$\pm$0.14} & 23.36\scriptsize{$\pm$0.71} \\
	\midrule
	EWC~\cite{EMC} & & 47.01\scriptsize{$\pm$0.29} & 33.27\scriptsize{$\pm$1.17} && 50.93\scriptsize{$\pm$0.09} & 34.94\scriptsize{$\pm$0.07} && 35.00\scriptsize{$\pm$0.43} & 56.16\scriptsize{$\pm$0.88} \\
	LwF ~\cite{LwF} & & 60.69\scriptsize{$\pm$0.63} & 27.77\scriptsize{$\pm$2.17} && 47.91\scriptsize{$\pm$0.33} & 38.01\scriptsize{$\pm$0.28} && 38.54\scriptsize{$\pm$1.23} & 52.37\scriptsize{$\pm$0.64} \\
	L2P~\cite{l2p} & & 83.86\scriptsize{$\pm$0.28} & {7.35\scriptsize{$\pm$0.38}} && 81.14\scriptsize{$\pm$0.93} & {4.64\scriptsize{$\pm$0.52}} &&{61.57\scriptsize{$\pm$0.66}} & {9.73\scriptsize{$\pm$0.47}} \\
	ESN~\cite{ESN} & & 86.34\scriptsize{$\pm$0.52} & 4.76\scriptsize{$\pm$0.14} && 85.71\scriptsize{$\pm$1.47} & 2.58\scriptsize{$\pm$0.61} & & - & - \\
	DualPrompt~\cite{DUAL_PROMPT} & & 86.51\scriptsize{$\pm$0.33} & 5.16\scriptsize{$\pm$0.09} && 88.08\scriptsize{$\pm$0.36} & 2.21\scriptsize{$\pm$0.69} & &  68.13\scriptsize{$\pm$0.49} & 4.68\scriptsize{$\pm$0.20}  \\
	\bf \ours (ours) & & \bf 91.12\scriptsize{$\pm$0.12} & \bf 3.33\scriptsize{$\pm$0.18} && \bf 
	92.92\scriptsize{$\pm$0.17} & \bf 
	0.19\scriptsize{$\pm$0.07} && \bf 74.88\scriptsize{$\pm$0.07} & \bf 3.65\scriptsize{$\pm$0.03} \\
	\midrule
    Upper-bound & -& 93.15\scriptsize{$\pm$0.09} & - & - & 97.81\scriptsize{$\pm$0.02} & - & - & 83.87\scriptsize{$\pm$0.30} & - \\
	\bottomrule
	\end{tabular}
	}
\end{center}
\caption{Comparison to state-of-the-art methods on split CIFAR-100, 5-datasets, and split ImageNet-R. Results of ESN are reported from the original paper~\cite{ESN}. Other results except for the upper-bounds are reported from DualPrompt~\cite{DUAL_PROMPT}.}
\label{table:main_result}
\end{table*} 

\section {Experiments}
\subsection {Datasets}
\noindent\textbf{Split CIFAR-100} is a widely used continual learning benchmark. Following the standard setup, we evenly split CIFAR-100 into 10 disjoint tasks. More results under different splits (\eg, 5 and 20) are reported in supplementary. 

\noindent\textbf{5-datasets} is a collection of CIFAR-10~\cite{cifar10}, MNIST~\cite{mnist}, Fashion-MNIST~\cite{fashionmnist}, SVHN~\cite{svhn}, and notMNIST~\cite{notmnist}, each of which is treated as a new task. It serves as a fair analogue of real-word scenarios where inter-task divergences are large. 

\noindent\textbf{Split ImageNet-subset} is another widely adopted benchmark which divides a subset (100 classes) of ImageNet~\cite{deng2009imagenet} into 10 tasks with 10 classes per task.

\noindent\textbf{Split ImageNet-R} is first adapted for continual learning by DualPrompt~\cite{DUAL_PROMPT} to mimic real-word scenarios, where there exit different image styles and its intra-class diversity is significant. It divides the original ImageNet-R~\cite{imagenetR} dataset into 24,000 training images and 6,000 test images and splits the total 200 classes into 10 disjoint tasks.

\subsection {Configuration and Evaluation Metric}\label{sec:config}
\noindent\textbf{Configuration}. We use the following dataset-agnostic configuration for all experiments unless stated otherwise. All experiments are conducted on four NVIDIA A100 GPUs. We train \ours for 50 epochs with a batch size of 256 using the AdamW optimizer~\cite{adamw}. The initial learning rate is set to $1 \times 10^{-3}$ and anneals to $1 \times 10^{-6}$ according to the cosine scheduler. The prompt length $L_p$ is set to 1 and deep prompt is used by default. The multi-centroid number $C$ and nearest neighbors $r$ are set to 5 and 3, respectively. A 3-layer MLP with 2048 hidden units and 768 output dimensions is randomly initialized at each new task. Other detailed configurations are provided in supplementary. 

\noindent\textbf{Evaluation metric}. We adopt the widely used average accuracy and forgetting from the end session~\cite{EMC, A_GEM, DUAL_PROMPT} as our evaluation metrics. The average value and standard deviation are reported according to five runs with different random seeds. Detailed illustration of each metric and more results under different protocols are included in supplementary.

\subsection {Comparison with State of the Arts} 
In this section, we first compare \ours to state-of-the-art methods that are compatible with the Transformer architecture on split-CIFAR100, 5-datasets, and split ImageNet-R datasets following DualPrompt~\cite{DUAL_PROMPT}. Then we reproduce state-of-the-art prototype-based methods as well as Transformer-based methods and compare \ours to them on split ImageNet-subset and split-CIFAR-100. Note that \textit{all methods reported in this section are implemented using the same pre-trained ViT-B/16.}

\begin{table*}[t]
\begin{center}
\resizebox{0.75\linewidth}{!}{%
\begin{tabular}{l c c cc cc cc}
	\toprule 
	\multirow{2}{*}{\textbf{Method}} &
	\multirow{2}{*}{\textbf{Buffer size}}&
	\multicolumn{3}{c}{\textbf{Split ImageNet-subset}} &
	\multicolumn{3}{c}{\textbf{Split CIFAR-100}} \\
	& & Backbone & Pretrain & Avg. Acc ($\uparrow$) & Backbone &  Pretrain & Avg. Acc ($\uparrow$) \\
	\midrule
	Upper-bound & - & ViT/B-16 & ImageNet & 94.22\scriptsize{$\pm$0.18} &
	ViT/B-16 & MAE & 93.15\scriptsize{$\pm$0.09} \\
	\midrule
	iCaRL~\cite{iCaRL} & 2000 & ResNet-18 & \xmark  &23.77\scriptsize{$\pm$0.35} & ResNet-18 & \xmark  & 51.12\scriptsize{$\pm$0.36}  \\
		
	PASS~\cite{PROTO_AUG} & 0 & ResNet-18 & \xmark & 27.16\scriptsize{$\pm$0.24} & ResNet-18 & \xmark & 36.32\scriptsize{$\pm$0.33} \\
				
	iCaRL~\cite{iCaRL} & 2000 & ViT-B/16 & MAE & 87.96\scriptsize{$\pm$0.26}  & ViT-B/16 & ImageNet & 75.10\scriptsize{$\pm$0.26}  \\
				
	PASS~\cite{PROTO_AUG} & 0 & ViT-B/16 & MAE & 72.72\scriptsize{$\pm$0.31} & ViT-B/16 & ImageNet & 64.10\scriptsize{$\pm$0.20} \\
				
	DualPrompt~\cite{DUAL_PROMPT} & 0 & ViT-B/16 & MAE & 92.50\scriptsize{$\pm$0.24} & ViT-B/16 & ImageNet & 86.51\scriptsize{$\pm$0.33} \\
				
	\bf \ours (ours) & 0 & ViT-B/16 & MAE &\bf 93.82\scriptsize{$\pm$0.06} & ViT-B/16 & ImageNet & \bf 91.12\scriptsize{$\pm$0.12} \\
	\bottomrule
    \end{tabular}
	}
\end{center}
\caption{Comparison to prototype-based methods and DualPrompt on split ImageNet-subset and split CIFAR-100. Results of DualPrompt on split CIFAR-100 are reported from the original paper. All other results are reproduced using the same pre-trained ViT-B/16.}
\label{table:proto}
\end{table*}

\noindent\textbf{Main results on split CIFAR-100, 5-datasets, and split ImageNet-R}. We compare \ours to regularization-based methods (\emph{EWC}~\cite{EMC} and \emph{LwF}~\cite{LwF}), advanced rehearsal-based methods ( \emph{ER}~\cite{ER}, \emph{GDumb}~\cite{GDumb}, \emph{BiC}~\cite{BiC}, \emph{DER++}~\cite{dark_memory}, and \emph{Co$^2$L}~\cite{Co2L}), and Transformer-based methods (\emph{L2P}~\cite{l2p}, \emph{ESN}~\cite{ESN}, and \emph{DualPrompt}~\cite{DUAL_PROMPT}). As shown in Table~\ref{table:main_result}, despite the rehearsal-free property of regularization-based methods, their results are less competitive. Rehearsal-based methods, on the other hand, produce decent results under a large memory budget. Yet, they are still outperformed by prompt-based methods, which do not require rehearsal. Among the family of new emerging Transformer-based methods, CPP surpasses alternatives by a significant margin, showcasing the superiority of our framework which leverages task-specific prompts that are optimized by the contrastive prototypical loss to steer prototypes.

\noindent\textbf{Comparison to prototype-based methods}.\label{sec:proto-related} We further compare \ours against state-of-the-art prototype-based methods including iCaRL~\cite{iCaRL} and PASS~\cite{PROTO_AUG} on split ImageNet-subset and split CIFAR-100. For calibration purpose, we also reproduce DualPrompt~\cite{DUAL_PROMPT} on split ImageNet-subset. \textit{To avoid information leakage, we use self-supervised MAE for model pre-training on split ImageNet-subset}. Details on reproduction are provided in supplementary. As shown in Table~\ref{table:proto}, a pre-trained ViT backbone can significantly boost the performance of existing prototype-based methods, which aligns with the observation in \cite{ramasesh2022effect}. However, \ours still demonstrates the state-of-the-art performance under the same backbone, suggesting a clear advantage of \ours over other prototype-based methods.

 \begin{table*}[t]
	\begin{center}
		\resizebox{0.8\linewidth}{!}{%
			\begin{tabular}{l c c cc cc cc}
				\toprule 
				\multirow{2}{*}{\textbf{Pretrain}} & \multirow{2}{*}{\textbf{\ours}} & \multirow{2}{*}{\textbf{Multi-centroids}} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} & \multicolumn{2}{c}{\textbf{5-datasets}} & \multicolumn{2}{c}{\textbf{Split ImageNet-R}} \\
				%  &
				& &  & Avg. Acc ($\uparrow$) & Forgetting ($\downarrow$) & Avg. Acc ($\uparrow$) & Forgetting ($\downarrow$) & Avg. Acc ($\uparrow$) & Forgetting ($\downarrow$) \\
				\midrule
				\multirow{4}{*}{Deit~\cite{deit}} & & & 71.9 & 9.97 & 70.54 & 0.28 & 51.29 & 5.84 \\
				& \checkmark & & 77.64\scriptsize{$\pm$0.18} & 8.06\scriptsize{$\pm$0.14} & 
				81.67\scriptsize{$\pm$0.13} & 1.71\scriptsize{$\pm$0.21} & 57.85\scriptsize{$\pm$0.07} & 6.74\scriptsize{$\pm$0.10} \\
				& & \checkmark & 74.23\scriptsize{$\pm$0.08} & 9.11\scriptsize{$\pm$0.14} & 72.36\scriptsize{$\pm$0.02} & \bf 0.11\scriptsize{$\pm$0.01} & 49.50\scriptsize{$\pm$0.16} & 6.16\scriptsize{$\pm$0.19} \\
				& \checkmark & \checkmark & \bf 82.24\scriptsize{$\pm$0.20} & \bf 6.05\scriptsize{$\pm$0.19} & \bf 90.94\scriptsize{$\pm$0.23} & 0.22\scriptsize{$\pm$0.04} & \bf 67.45\scriptsize{$\pm$0.25} & \bf 4.94\scriptsize{$\pm$0.12} \\
				\midrule
				\multirow{4}{*}{Dino~\cite{DINO}} & & & 76.69 & 8.91 & 72.18 & 0.68 & 45.59 & 7.77 \\
				& \checkmark & & 80.11\scriptsize{$\pm$0.22} & 6.88\scriptsize{$\pm$0.20} & 81.56\scriptsize{$\pm$0.02} & 0.74\scriptsize{$\pm$0.06} & 51.88\scriptsize{$\pm$0.10} & 8.67\scriptsize{$\pm$0.15} \\
				& & \checkmark & 79.71\scriptsize{$\pm$0.08} & 7.68\scriptsize{$\pm$0.07} & 74.31\scriptsize{$\pm$0.04} & \bf 0.18\scriptsize{$\pm$0.01} & 48.63\scriptsize{$\pm$0.18} & 5.72\scriptsize{$\pm$0.15} \\
				& \checkmark & \checkmark & \bf 83.59\scriptsize{$\pm$0.12} & \bf 5.43\scriptsize{$\pm$0.18} & \bf 89.10\scriptsize{$\pm$0.10} & 0.21\scriptsize{$\pm$0.08} & \bf 61.22\scriptsize{$\pm$0.59} & \bf 5.04\scriptsize{$\pm$0.08} \\
				\midrule
				\multirow{4}{*}{MAE~\cite{MAE}} & & & 74.65 & 8.60 & 72.77 & 0.30 & 55.25 & 6.21 \\
				& \checkmark & & 78.90\scriptsize{$\pm$0.32} & 8.42\scriptsize{$\pm$0.24} & 82.48\scriptsize{$\pm$0.06} & 
				0.41\scriptsize{$\pm$0.05} & 62.78\scriptsize{$\pm$0.09} & 5.88\scriptsize{$\pm$0.19} \\
				& & \checkmark & 76.68\scriptsize{$\pm$0.15} & 8.16\scriptsize{$\pm$0.09} & 73.98\scriptsize{$\pm$0.06} & 0.11\scriptsize{$\pm$0.04} & 54.68\scriptsize{$\pm$0.09} & 5.22\scriptsize{$\pm$0.20} \\
				& \checkmark & \checkmark & \bf 83.36\scriptsize{$\pm$0.18} & \bf 6.50\scriptsize{$\pm$0.38} & \bf 91.76\scriptsize{$\pm$0.02} & \bf 0.11\scriptsize{$\pm$0.01} & \bf 71.04\scriptsize{$\pm$0.30} & \bf 4.23\scriptsize{$\pm$0.07} \\
				\midrule
				\multirow{4}{*}{ViT~\cite{vit}} & & & 82.82 & 5.94 & 69.71 & 0.30 & 60.20 & 5.26 \\
				& \checkmark & & 87.18\scriptsize{$\pm$0.22} & 4.70\scriptsize{$\pm$0.33} & 81.54\scriptsize{$\pm$0.05} & 0.42\scriptsize{$\pm$0.05} & 65.88\scriptsize{$\pm$0.14} & 6.76\scriptsize{$\pm$0.02} \\
				& & \checkmark & 84.09\scriptsize{$\pm$0.05}  & 5.47\scriptsize{$\pm$0.12} & 72.1\scriptsize{$\pm$0.17} & \bf 0.15\scriptsize{$\pm$0.01} & 61.11\scriptsize{$\pm$0.23} & 4.84\scriptsize{$\pm$0.04} \\
				& \checkmark & \checkmark & \bf 91.12\scriptsize{$\pm$0.12} & \bf 3.33\scriptsize{$\pm$0.18} & \bf 92.92\scriptsize{$\pm$0.17} & 0.19\scriptsize{$\pm$0.07} & \bf 74.88\scriptsize{$\pm$0.07} & \bf 3.65\scriptsize{$\pm$0.03} \\
				\bottomrule
			\end{tabular}
		}
	\end{center}
	\caption{Ablation studies of the proposed modules under four different pre-training methods. When neither \ours nor multi-centroid prototype are applied, the model is reduced to the training-free baseline model as described in Sec.~\ref{sec:baseline}.}
	\label{table:main_ablation}
\end{table*} 

\subsection{Ablation Study}
\noindent\textbf{Effectiveness of the proposed modules}. Since the embedding function is one of the key ingredients in our recipe, it is critical to analyze \ours under different embedding functions. To this end, we implement \ours with four up-to-date pre-training methods including \emph{ViT}~\cite{vit}, \emph{Deit}~\cite{deit}, \emph{Dino}~\cite{DINO}, and \emph{MAE}~\cite{MAE} that sweep supervised and self/un-supervised learning as well as discriminative and generative models. As illustrated in Table~\ref{table:main_ablation}, both task-specific prompts and multi-centroid prototypes are robust against all pre-training methods, bringing around 10\% to 20\% absolute improvements over the baseline model. In addition, each module remains effective when applied independently, and their benefits are additive when combined. An interesting observation is that different pre-training methods can induce clear differences when combining with a NCM classifier, and there is a clear positive correlation (\eg, $\rho=1.0$ for split CIFAR-100) between the performances of baseline models and their corresponding \ours models.   

\noindent\textbf{Contrastive prototypical loss outperforms alternatives}. To see the benefits of \ourloss, we first compare it with two widely-used loss functions: \emph{CE} (cross-entropy) and \emph{SupCon} (supervised contrastive loss)~\cite{SCL}. As shown in Table~\ref{table:loss_ablation}, \ourloss outperforms both of them by a clear margin. Among comparisons, \emph{SupCon} is most compatible with ours, further confirming the advantage of coupling the contrastive loss design and a NCM classifier. Then, we independently add uniformity (w/ uniform) or remove prototypes (w/o proto), under different temperatures to validate the efficacy of each proposed component. As shown in Fig.~\ref{fig:4in1} (right), encouraging uniformity will decrease the performance, and removing prototypes can aggravate the prototype interference, which are coherent with our analysis in Sec.~\ref{sec:cpl}.

\noindent\textbf{Stability and plasticity dilemma is mediated by temperature coefficient}. As shown in Fig.~\ref{fig:4in1} (right), small temperatures (leading to more punishment on negative pairs) will reduce forgetting but also decrease the accuracy. Higher temperatures, in opposite, improve the overall performance at the cost of higher forgetting. We empirically found that $\tau=0.6$ makes a good trade-off between stability and plasticity and thus use it across all experiments. 

\noindent\textbf{MLP is non-negligible}. We show in Table~\ref{table:loss_ablation} that non-linearity introduced by the MLP is vital to the success of training prompts, regardless of the loss design. Replacing the MLP neck by a single linear layer consistently leads to inferior results.

\begin{table}[t]
	\begin{center}
		\resizebox{0.8\linewidth}{!}{%
			\begin{tabular}{l c c cc cc cc}
				\toprule 
				\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} \\
				%  &
				&  Avg. Acc ($\uparrow$) & Forgetting ($\downarrow$) \\
				\midrule
				CE (w/ linear neck) & 85.48\scriptsize{$\pm$0.49} & 8.42\scriptsize{$\pm$0.64}  \\
				SupCon (w/ linear neck)  & 87.94\scriptsize{$\pm$0.10} & 3.99\scriptsize{$\pm$0.10}  \\
				\ourloss (w/ linear neck) & 87.20\scriptsize{$\pm$0.12}& 4.08\scriptsize{$\pm$0.08} \\
				\midrule
				CE~\cite{CE} & 90.42\scriptsize{$\pm$0.04} &4.45\scriptsize{$\pm$0.17} \\
				SupCon~\cite{SCL} & 90.63\scriptsize{$\pm$0.27} & 3.35\scriptsize{$\pm$0.29}  \\
				\bf \ourloss (ours) &\bf 91.12\scriptsize{$\pm$0.12} & \bf 3.33\scriptsize{$\pm$0.18} \\
				\bottomrule
			\end{tabular}
		}
	\end{center}
	\caption{\ourloss vs.\ alternative loss functions.}
	\label{table:loss_ablation}
\end{table}

\begin{figure*}[t]
\begin{center}
	\includegraphics[height=0.125\textheight]{./figures/mc_r.pdf}
	\hfill
	\includegraphics[height=0.125\textheight]{./figures/retrieve_num.pdf}
	\hfill
	\includegraphics[height=0.125\textheight]{./figures/lp_dp.pdf}
    \hfill
	\includegraphics[height=0.125\textheight]{./figures/temp.pdf}
\end{center}
	\caption{\textbf{Left:} Centroid number vs.\ query neighbors. \textbf{Middle left:} Our average number of retrieved prompts on different datasets. \textbf{Middle right:} Ablation studies on prompt length and deep prompt. \textbf{Right:} Results of CPL and its variants under different temperature coefficients.}
	\label{fig:4in1}
\end{figure*}

\noindent\textbf{Ablation studies for prompt design}. Both prompt length and deep prompt can affect the performance of the task-specific prompt. As shown in Fig.~\ref{fig:4in1}~(middle right), deep prompts consistently outperforms shallow prompts, manifesting the importance of steering features at different levels of abstraction. Moreover, we see that $L_p=1$ with deep prompts is sufficient to achieve good results on split CIFAR-100 and excessive longer prompts can cause over-fitting.

\noindent\textbf{Centroid number vs.\ query radius}. The combination of centroid number $C$ and query radius $r$ can influence the overall performance. To identify an appropriate configuration, we perform a simple grid search on split CIFAR-100 and found that the setting $C=5$ with $r=3$ works fairly well across all benchmarks. As illustrated in Fig.~\ref{fig:4in1}~(left), the proposed multi-centroid strategy can better characterize a class distribution and thus effectively reduce the query radius and improve the performance.

\noindent\textbf{Effectiveness of the task-specific prompts}. We visualize data samples and their corresponding prototypes from CIFAR-100 with and without inserting task-specific prompts in Fig.~\ref{fig:visualization}. Before inserting task-specific prompts, samples from the same class tend to cluster in the latent space, and samples from different classes still interleave with each other. After adding task-specific prompts, samples from the same category are tightly concentrated, while populations of different classes are more spread out. Please refer to supplementary for more visualization results and analysis.

\subsection{Analysis of Efficiency}
In this section, we analyze the efficiency of \ours from both memory usage and computational overhead. A detailed discussion is further provided in supplementary.

\begin{table}[t]
	\begin{center}
		\resizebox{0.8\linewidth}{!}{%
			\begin{tabular}{l c c cc cc cc}
				\toprule 
				\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Split CIFAR-100}} \\
				%  &
				&  Extra Mem. (MB / task) ($\downarrow$) &Avg. Acc  ($\uparrow$) \\
				\midrule
				DER++~\cite{dark_memory}  & 71.780 & 83.94\scriptsize{$\pm$0.34}\\
				L2P~\cite{l2p}  & 0.194 & 83.86\scriptsize{$\pm$0.28}\\
				DualPrompt~\cite{DUAL_PROMPT}  & 0.190 & 86.51\scriptsize{$\pm$0.33}\\
				\bf \ours (ours) & \bf 0.035 & \bf 91.12\scriptsize{$\pm$0.12} \\
				\bottomrule
			\end{tabular}
		}
	\end{center}
	\caption{Comparison on additional memory usage per task.}
	\label{table:memory_usage}
\end{table}

\noindent\textbf{Memory usage}. To illustrate the memory efficiency, we compare \ours with other methods in terms of \textit{extra memory usage per task}, \ie, the increased memory footprint per incremental task. As shown in Table~\ref{table:memory_usage}, \ours surpasses other methods by a large margin with much smaller additional memory footprint. Hence, \ours can be easily scaled up to a long sequence of tasks. 

\begin{figure}[t]
\begin{center}
	\includegraphics[width=0.4\linewidth]{./figures/train_proto_train.jpg}
	\hspace{0.4cm}
	\includegraphics[width=0.4\linewidth]{./figures/train_pt_proto_train_pt.jpg}
\end{center}
\caption{t-SNE plots for data samples without (left) and with (right) task-specific prompts on CIFAR-100.}
\label{fig:visualization}
\end{figure}

\noindent\textbf{Computational efficiency}. Thanks to the prompt-tuning technique and the usage of prototypes, the computational cost during training is benign: only tiny portion of parameters need to be updated through back-propagation and no explicit exemplars need to be forwarded. At inference, due to the design of task-specific prompts, \ours may need to perform forward propagation more than one time, so it is critical to see if this additional cost is affordable. As shown in Fig.~\ref{fig:4in1} (middle left), larger inter-task divergences usually result in fewer retrieved prompts (\eg, only one for 5-datasets), and thus have the same inference cost as a typical linear classifier, which is in harmony with our analysis in Sec.~\ref{sec:multi-centroid}. As the intra-class divergence increases and inter-class divergence reduces, more prompts will be retrieved. We also note that even on the most challenging ImageNet-R dataset, \ours only needs in average $0.73$ extra forward pass which is affordable under most scenarios.

\section{Conclusion}
We propose a simple and novel framework for rehearsal-free continual learning. It leverages task-specific prompts that optimized over a designed contrastive prototypical loss to avoid semantic drift as well as reducing prototype interference. A multi-centroid prototype strategy is further proposed to improve the representativeness of prototypes. Empirically, \ours surpasses state-of-the-art methods by a significant margin and the effectiveness of each proposed component is extensively analyzed. We believe \ours can shed lights on the design principle of scalable continual learning systems giving current advances in network architecture design and representation learning. For future study, it is valuable to further improve the inference efficiency. Another promising direction is to extend \ours to other composite scenarios, \eg, blurry task boundary, few-shot, and open-vocabulary settings. In practical usage, it is also beneficial to design automatic embedding function selection strategy so as to accommodate diverse downstream contexts.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\onecolumn
\appendix

\section{Gradient Analysis of Contrastive Prototypical Loss}\label{appendix:derivation}
Here, we provide an analysis of gradients for contrastive prototypical loss. To ease the notion, we abbreviate similarity between vector $\vz_i$ and $\vz_j$ as $s_{i,j}$ and denote negative set $N(i)\cup \hat{U}$ as $\hat{N}(i)$. Therefore, the loss of a data sample $\vx_i$ is:
\begin{equation}
	\label{eq:loss_for_k}
	\mathcal{L}_i = \frac{-1}{|P(i)|}\sum_{\vz_p \in P(i)}\log\frac{\exp(s_{i,p}/\tau)}{\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)}.
\end{equation}
The gradient with respect to the similarity $s_{i,j}$ between a positive pair $(\vz_i, \vz_j)$ for $z_j \in P(i)$ can be derived as:
\begin{equation}
	\begin{aligned}
		\frac{\partial \mathcal{L}_i^k}{\partial s_{i,j}}
		&=\frac{-1}{|P(i)|}\sum_{\vz_p \in P(i)}\frac{\partial }{\partial s_{i,j}}\left(s_{i,p}/\tau - \log{\underset{\vz_n \in \hat{N}(i)}{\sum}}\exp(s_{i,n}/\tau)\right), \\
		& = \frac{-1}{|P(i)|}\sum_{\vz_p \in P(i)}\left(\frac{1}{\tau} \cdot \mathbbm{1}[p=j]-\frac{\frac{\partial}{\partial s_{i,j}}\left(\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)\right)}{{\underset{\vz_n \in \hat{N}(i)}{\sum}}\exp(s_{i,n}/\tau)}\right), \\
		& = \frac{-1}{|P(i)|}\sum_{\vz_p \in P(i)}\left(\frac{1}{\tau}\cdot\mathbbm{1}[p=j]-0\right), \\
		& = \frac{-1}{\tau|P(i)|},
	\end{aligned}
\end{equation}
where $\mathbbm{1}$ is an indicator function. Similarly, the gradient with respect to the similarity $s_{i,m}$ between a negative pair $(\vz_i, \vz_m)$ for $\vz_m \in N(i)$ is: 
\begin{equation}
	\begin{aligned}
		\frac{\partial \mathcal{L}_i^k}{\partial s_{i,m}} & =\frac{-1}{|P(i)|}\sum_{\vz_p \in P(i)}\frac{\partial }{\partial s_{i,m}}\left(s_{i,p}/\tau - \log{\underset{\vz_n \in \hat{N}(i)}{\sum}}\exp(s_{i,n}/\tau)\right), \\
		& = \frac{-1}{|P(i)|}\sum_{\vz_p \in P(i)}\left(0-\frac{\frac{\partial}{\partial s_{i,m}}\left(\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)\right)}{{\underset{\vz_n \in \hat{N}(i)}{\sum}}\exp(s_{i,n}/\tau)}\right), \\
		& = \frac{1}{|P(i)|}\sum_{\vz_p \in P(i)}\left(\frac{\underset{\vz_n \in \hat{N}(i)}{\sum}\left(\exp(s_{i,n}/\tau)\cdot \frac{1}{\tau} \cdot \mathbbm{1}[n=m]\right)}{{\underset{\vz_n \in \hat{N}(i)}{\sum}}\exp(s_{i,n}/\tau)}\right), \\
		& = \frac{1}{\tau} \frac{\exp(s_{i,m}/\tau)}{{\underset{\vz_n \in \hat{N}(i)}{\sum}}\exp(s_{i,n}/\tau)} .
	\end{aligned}
\end{equation}
We can see that, the gradient of the proposed loss follows the same pattern as the standard supervised contrastive loss. Positive pairs are treated equally and scaled by the temperature and the cardinality of the positive set. The property of implicit hard-case mining, \ie, proportional to the exponential term $\exp(s_{i,m}/\tau)$, is inherited from a typical contrastive loss in the negative term.

\section{\ours as an Energy-based Model}
\label{appendix:energy}
The overall objective of an energy-based model~\cite{LeCun06atutorial} (EBM) is to obtain an energy function $E_{\theta}(\vx): \R^{D}\rightarrow \R$ parameterized by $\theta$ that maps the high dimensional input $\vx$ to a scalar value. Giving an energy function $E_{\theta}(\cdot)$, its probability density $p(\vx)$ can be expressed through Gibbs distribution:
\begin{equation}
	p_{\theta}(y|\vx) = \frac{\exp(-E_{\theta}(\vx, y)/\tau)}{\int_{y'}\exp(-E_{\theta}(\vx, y')/\tau)} = \frac{\exp(-E_{\theta}(\vx, y)/\tau)}{\exp(-E_{\theta}(\vx)/\tau)},
	\label{eq:energy}
\end{equation}  
where $E_{\theta}(x)$ is the \textit{Helmholtz free energy} and $\tau$ is the temperature factor. Then we have:
\begin{equation}
	E_{\theta}(x) = \tau \cdot -\log\int_{y'}\exp(-E_{\theta}(\vx, y')/\tau).
\end{equation}
When making predictions under our framework, the categorical distribution can be represented as:
\begin{equation}
	p(y|\vx) = \frac{\exp(s_{x, y}/\tau)}{\sum_{y'=1}^{K}\exp(s_{x, y'}/\tau)},
	\label{eq:predict}
\end{equation}
where $s_{x, y} = \langle f_{\{\theta, P\}}(\vx), \hat{\vmu}_y\rangle$. When connecting Eq.~\ref{eq:predict} with  Eq.~\ref{eq:energy} and let $E_{\theta}(\vx, y) = -s_{x,y}$, we see that the energy of $\vx$ can be expressed as:
\begin{equation}
	E_{\theta}(\vx) =  \tau \cdot -\log\sum_{y=1}^{K}\exp(s_{x,y}/\tau),
\end{equation}
which is dominated by the largest similarity $s_{x,y^*}$ given an appropriate temperature $\tau$. The above analysis drives to a conclusion that assigning a data sample to its nearest prototype will generate the lowest energy for the system (\ie, a more stable system). Therefore, the question becomes whether the proposed contrastive prototypical loss serves as a qualified energy loss function. 

To see this, we first simplify Eq.~\ref{eq:loss_for_k} to a formula where there is only one positive sample $\vz_{p}$:
\begin{equation}
	\begin{aligned}
		\label{eq:sim_loss_for_k}
		\mathcal{L}_i &= -\log\frac{\exp(s_{i,p}/\tau)}{\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)}, \\
		& = - s_{i,p} + \log{\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)}.
	\end{aligned}
\end{equation}
By substituting $\vz_{p}$ with the target value prototype $\hat{\vmu}$, we have:
\begin{equation}
	\mathcal{L}_i = \underbrace{-\langle\vz_i, \hat{\vmu}\rangle}_\text{push down energy for target prototype} + \underbrace{\log{\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)}}_\text{pull up energies for other prototypes}.
\end{equation}
When the above loss is minimized, the first term will push down the energy for target value prototype $\hat{\vmu}$ and the second term will increase energies for other prototypes. Hence, the above simplified loss is an effective loss function for the energy model. Note that the \textit{ground-truth} prototype $\hat{\vmu}$ is unavailable during training stage and it is approximated by a group of dynamically evolved positive embeddings from train data. It is worth pointing out that, unlike the contrastive divergence approximation used in \cite{li2020energy}, we also include previous classifiers in our loss. Nevertheless, our formulation does not suffer from the over suppression issue of previous classes likewise, as we use static non-parametric prototypes as classifiers. 

Finally, we show that encouraging uniformity is against the principle of the energy model. By encouraging uniformity as a typical supervised or self-supervised contrastive loss~\cite{SimCLR, SCL}, we rewrite Eq.~\ref{eq:sim_loss_for_k} into:
\begin{equation}
	\begin{aligned}
	\label{eq:with_uniform}
		\mathcal{L}_i &= -\log\frac{\exp(s_{i,p})}{\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau) + \underset{\vz_n\in\hat{P}(i)}{\sum}\exp(s_{i,n}/\tau)}, \\
		& = - s_{i,p} + \log\left({\underset{\vz_n \in \hat{N}(i)}{\sum}\exp(s_{i,n}/\tau)}+ \underbrace{\underset{\vz_n\in\hat{P}(i)}{\sum}\exp(s_{i,n}/\tau)}_\text{harmfully pull up energy for target prototype}\right),
	\end{aligned}
\end{equation}
where $\hat{P}(i) = \{z_j: y_j=y_i, i\neq j\}$. As shown in Eq~\ref{eq:with_uniform}, we can see that the second term in $\log$ operation acts adversely with respect to $-s_{i,p}$ which minimizes the energy between a sample and its corresponding prototype. Hence, pairs that encourage uniformity should be, in principle, removed from the negative set. Through the lens of energy-based models, we acknowledge that the training process of \ours is equivalent to minimize the energies of data samples and the inference process can be interpreted as selecting a task-specific prompt that minimizes the energy of a data sample in a given system.

\section{Training and Inference Algorithms}
\label{appendix:algorithm}
The training and inference algorithms of \ours are summarized in Algorithm~\ref{alg:train} and Algorithm~\ref{alg:test}, respectively. Note that when multi-centroid prototypes are used, the prototype generation process is substituted by a spectrum clustering process as described in Sec. 3.4.

\begin{algorithm}[t]
	\SetAlgoLined
	\textbf{Input:} Frozen embedding function $f_{\theta}$, number of tasks $T$,  training epochs $E$, training set ${\{(\vx_i^t, y_i^t)\}_{i=1}^{n_t}}\}_{t=1}^{T}$, prompt length $L_p$ and centriod number $C$. \\
	\For{$t = 1,\cdots,T$}{
		\textbf{Initialize:} MLP $m_{\sigma^t}$, task-specific group $P^t$, $U=\varnothing$, $\hat{U}=\varnothing$\\
		\For{class $k \in \mathcal{Y}^t$}{Generate key prototype $\vmu_k$ with Eq.1\\
			$U \leftarrow U \cup \vmu_k$}
		\For{$e = 1, \cdots, E$}{
			Optimize $\sigma^t$, $P^t$ through Eq.6
		}
		\For{class $k \in \mathcal{Y}^t$}
		{Generate value prototype $\hat{\vmu}_k$ with Eq.1 after prepending $P^t$\\
			$\hat{U} \leftarrow \hat{U} \cup \hat{\vmu}_k$}}
	\textbf{Output:} $\{P^t \}_{t=1}^T$, $U$ and $\hat{U}$
	\caption{Model Training}
	\label{alg:train}
\end{algorithm}

\begin{algorithm}[t]
	\SetAlgoLined
	\textbf{Given:} $f_{\theta}$, $U$, $\hat{U}$, $\{P^t\}_{t=1}^T$, query function $q(,,r)$. \\
	\textbf{Input:} Test image $\vx$\\ 
	\textbf{Initialize:}  $\hat{Q}=\varnothing$ \\ 
	$\vq = f_{\theta}(\vx)[0, :]$ \tcp*{Use class token as query vector.} 
	$ J = q(\vq, U, r)$ \tcp*{Retrieve indexes of J candidate prompts.}
	\For{$j \in J$}{
		$\vq^j = f_{\theta, P^j}(\vx)$ \\
		$\hat{Q} \leftarrow \hat{Q} \cup \vq^j$  \\
	}
	Make prediction following Eq.7\\
	\textbf{Output:} label $y$ 
	\caption{Model Inference}
	\label{alg:test}
\end{algorithm}

\section{Relation with Existing Taxonomy}\label{appendix:relation}
There exists different taxonomies for continual learning methods~\cite{PARISI201954, EmbracingChange}, and we here take notions from \cite{EmbracingChange}. Overall, \ours in this study is a hybrid method. In terms of the task-specific prompt design, \ours is analogue to modular architecture methods~\cite{PNN, yoon2018lifelong, Li2019LearnTG}, as it allocates additional learning capacities when new tasks are encountered while maintaining task-level specializations. However, \ours does not suffer from excessive memory usage or computational overhead, thanks to its efficiency offered by prompt-tuning. Furthermore, \ours leverages prototypes in combination with contrastively optimized task-specific prompts to avoid the requirement of task identities during inference. From the perspective of prototypes, \ours can be categorized into a memory-based method~\cite{dark_memory, Co2L}, specifically, an episodic memory method~\cite{episodic_memory}. Unlike most memory-based methods that save and replay explicit exemplars, \ours maintains highly compressed prototypes and uses them as classifiers. Finally, from the view of regularization, prompt-tuning techniques inherently provide strong regularization in the parameter space by freezing most parameters. By considering a model's functionality as its extracted data embeddings following \cite{FR}, the contrastive prototypical loss can also be viewed as a regularization term in the function space, where the only constraint is to avoid functional overlaps between samples from different classes.

\section {Discussion on Usability in Real-word Application}\label{appendix:usability}
The usability of a continual learning framework in real-world applications is of great importance. Here, we discuss the usability of \ours from the following aspects: memory usage, privacy protection, and computational efficiency.

\noindent\textbf{Memory usage}. It would be over-optimistic to generalize a model with the fixed learning capacity to ``life-long'' learning scenarios without forgetting. As such, how to efficiently extend the model is critical in practice. In this work, we propose to use the combination of prototypes and task-specific prompts to conduct memory-efficient continual learning. In our design, increasing memory usage boils down to the requirement of saving prototypes and task-specific prompts. For each new class, \ours only needs to save $12.2 \times 768$ additional parameters which is approximately equal to saving $1/15$ of a single ImageNet image (with size $224\times224\times3$). Therefore, \ours can be easily scaled up to handle thousands of classes given a modern computational device.

\noindent\textbf{Privacy protection}. Under privacy sensitive scenarios (\eg, medical applications), it can be vulnerable or even infeasible to maintain explicit exemplars. In contrast to general memory-based methods, \ours maintains information as fictitious prototypes which are inherently under more protection. Therefore, \ours is arguably preferable for privacy sensitive scenarios.

\noindent\textbf{Computational efficiency}. During the training process, thanks to the prompt-tuning paradigm, only a few parameters need to be updated through backpropagation. As such, \ours is more computational efficient than general methods that need to update all parameters under the same architecture. Moreover, \ours maintains previous knowledge as prototypes and only sets them as anchors, so it does not need to forward extra exemplars. During the inference process, \ours can introduce extra computational overhead due to the task-specific prompt design. However, we empirically demonstrate in Sec. 4.5 that this cost is relative small even on the most challenging split ImageNet-R benchmark. Besides, this computational overhead can be controlled by adjusting the centroid number $C$ and the neighbor size $r$ in practice. From an engineering perspective, \ours can potentially be further accelerated through batch processing and we deem the further improvement upon the inference efficiency as a valuable future direction. 

\section{Experimental Setups}
\subsection{Evaluation Metrics}\label{appendix:metrics}
Let $A_{i, j}$ denotes the classification accuracy for the $j$-th task after training on the $i$-th task. The Average Accuracy ($A_i$) and Forgetting ($F_i$) aftering learning task $i$ is then defined as follows:
\begin{equation*}
	\begin{aligned}
		&A_{i}=\frac{1}{i} \sum_{j=1}^{i} A_{i, j},\\
		&F_{i}=\frac{1}{i-1} \sum_{j=1}^{i-1} \max _{j^{\prime} \in\{1, \cdots, i-1\}}\left(A_{j^{\prime},j}-A_{i,j}\right).
	\end{aligned}
\end{equation*}
Assuming there are $T$ tasks in total, we report the accuracy from the end session as $Acc=A_T$ following \cite{EMC, DUAL_PROMPT}. We notice that some previous studies~\cite{LwF, PROTO_AUG, dytox} report the macro average over all sessions, \ie, $Acc=\frac{1}{T}\sum_{i=1}^T A_i$. To ease reference, we provide results under both protocols in Appendix~\ref{appendix:multi_protocal}.

\subsection{Data Augmentations}\label{appendix:aug}
In this section, we detail transformations used in our experiments. In general, we adopt standard data transformations used for contrastive learning, which include:
\begin{itemize}
	\item RandomResizedCrop (size$=224$, scale$=[0.8, 1.0]$)
	\item RandomHorizontalFlip ($p=0.5$)
	\item RandomColorJitter ($p=0.8$, brightness$=0.4$, contrast$=0.4$, saturation$=0.2$, hue$=0.1$)
	\item RandomGrayscale ($p=0.2$)
	\item RandomGaussianBlur ($p=0.1$, min radius$=0.1$, max radius=2)
	\item RandomSolarization ($p=0.2$)
	\item Normalization
\end{itemize}
Note that the normalization operation of an embedding function uses the same statistics as the ones used in its corresponding pre-training method.

\section{Reproduction Details}\label{appendix:reproduce} 
\begin{itemize}

\item \noindent\textbf{Upper-bound}. For the upper-bound results, we fine-tune the pre-trained embedding functions on target downstream datasets with all classes visible. The initial learning rate is set to $1 \times 10^{-4}$. All other settings follow the same configuration as training \ours to keep fair. One exception is fine-tuning MAE on ImageNet-subset where we found the transformations used for \ours generate inferior results and thus we follow the data transformations used in the original paper~\cite{MAE}.

\item \noindent\textbf{iCaRL}. For iCaRL, we first run experiments under the ResNet-18 model following the original setting~\cite{iCaRL}. Since the choice of the embedding function is orthogonal to the technical design in iCaRL, we change the embedding function to the same pre-trained Transformer as used in \ours. The learning rate is set to $1 \times 10^{-4}$ according to a grid search and the memory size is set to 2000. 

\item \noindent\textbf{PASS}. For PASS~\cite{PROTO_AUG}, the original paper uses 50 classes in the initial task and 5 class for each incremental task. To be consistent with our setup, we modify the setting to 10 classes per task and 10 tasks in total. Similar to reproducing iCaRL, we run experiments under both ResNet-18 and pre-trained Transformer backbones. The learning rate is set to $1 \times 10^{-4}$ for Transformer. Other hyper-parameters remain the same as the original paper. 
 
\item \noindent\textbf{DualPrompt}. DualPrompt~\cite{DUAL_PROMPT} is originally designed using Transformer structures, so we only change the pre-trained weights to MAE to avoid information leakage. All other parameters are set following the original paper. Specifically, we set $L_e=20$, $L_g=5$, $start_e=3$, $end_e=5$, $start_g=1$, and $end_g=2$. We train the model for 50 epochs with the constant learning rate of $0.005$ and the Adam optimizer is used.
\end{itemize}

\section{Additional Empirical Results}
\subsection{Results under Different Protocols}\label{appendix:detailed_result}

\noindent\textbf{Detailed results on CIFAR-100 under different splits}. Here, we provide detailed task-wise results on split CIFAR-100 under different splits. As shown in Fig.~\ref{fig:multi_splits}, \ours exhibits clear and consistent improvements over other methods and the gaps are enlarged as the length of the task sequence increases.

\begin{figure*}[t]
\begin{center}
	\includegraphics[width=0.32\textwidth]{./figures/5session.pdf}
	\hfill
	\includegraphics[width=0.32\textwidth]{./figures/10session.pdf}
	\hfill
	\includegraphics[width=0.32\textwidth]{./figures/20session.pdf}
\end{center}
\caption{Comparison with state-of-the-art methods on CIFAR-100 under multiple splits. \textbf{Left:} 5-splits, \textbf{Middle:} 10-splits \textbf{Right:} 20-splits.}
\label{fig:multi_splits}
\end{figure*}

\noindent\textbf{Results under different metric}.\label{appendix:multi_protocal} In Table~\ref{table:diff_protocol}, we provide results under two different protocols as described in Appendix~\ref{appendix:metrics}.

\subsection{Comparison to Architectural Methods}
In this section, we compare \ours with different architectural methods. It is non-trivial to migrate ConvNet-based methods to Transformer-based methods, so we follow the practice in DualPrompt~\cite{DUAL_PROMPT} to measure the difference between a method and its own upper-bound. As shown in Table~\ref{table:architecture}, \ours largely bridges the gap between incremental learning and joint learning. Besides, \ours is also more memory efficient than alternatives. 

\begin{table}[t]
	\begin{center}
	\resizebox{0.7\linewidth}{!}{%
			\begin{tabular}{c c c cc cc cc}
				\toprule 
				\multirow{2}{*}{\textbf{Task num}} &
				\multirow{2}{*}{\textbf{Dataset}} &
				\multirow{2}{*}{\textbf{Pre-train}} &
				\multicolumn{2}{c}{\textbf{Accuracy}} &
				\multicolumn{2}{c}{\textbf{Forgetting}}\\
				& &  & Avg. ($\uparrow$) & Last ($\uparrow$) & Avg. ($\downarrow$) & Last ($\downarrow$)\\
				\midrule
				
				5 & split CIFAR-100 & ViT & 
				93.78\scriptsize{$\pm$0.12} & 91.20\scriptsize{$\pm$0.06} & 2.77\scriptsize{$\pm$0.18} & 2.98\scriptsize{$\pm$0.21} \\
				
				10 & split CIFAR-100 & ViT & 94.18\scriptsize{$\pm$0.09} & 91.12\scriptsize{$\pm$0.12} & 2.39\scriptsize{$\pm$0.22} & 3.33\scriptsize{$\pm$0.18} \\
				
				20 & split CIFAR-100 & ViT & 93.49\scriptsize{$\pm$0.07} & 89.81\scriptsize{$\pm$0.17} & 2.84\scriptsize{$\pm$0.15} & 3.70\scriptsize{$\pm$0.13} \\
				
				5 & 5-datasets & ViT & 
				94.77\scriptsize{$\pm$0.19} & 
				92.92\scriptsize{$\pm$0.17} & 
				0.16\scriptsize{$\pm$0.08} & 
				0.19\scriptsize{$\pm$0.07} \\
				
				10 & split ImageNet-Sub & MAE & 95.14\scriptsize{$\pm$0.06} & 93.82\scriptsize{$\pm$0.06} & 0.82\scriptsize{$\pm$0.04} & 1.98\scriptsize{$\pm$0.06} \\
				
				10 & split ImageNet-R & ViT & 78.22\scriptsize{$\pm$0.14} & 74.88\scriptsize{$\pm$0.07} & 3.32\scriptsize{$\pm$0.19} & 3.65\scriptsize{$\pm$0.03} \\
				\bottomrule
		\end{tabular}
	}
	\end{center}
	\caption{Results of \ours under different protocols.}
	\label{table:diff_protocol}
\end{table}

\begin{table}[t!]
		\begin{center}
			\resizebox{0.85\linewidth}{!}{%
			\begin{tabular}{lclccc>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}}
					\toprule 
					\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Backbone}} & \multirow{2}{*}{\textbf{Avg. Acc ($\uparrow$)}} & \multirow{2}{*}{\textbf{Diff ($\downarrow$)}} & \multirow{2}{*}{\textbf{Pretrained}} &
					\multirow{2}{*}{\textbf{Buffer size}} &  \multicolumn{2}{c}{\textbf{Additional Parameters}} \\
					& & & & & & MB & \% \\
					\midrule
					Upper-bound & \multirow{5}{*}{ResNet18}& 80.41$^\dagger$ & - & - & - & - & - \\
					SupSup~\cite{supsup} & & 28.34\scriptsize{$\pm$2.45}$^\ddagger$ & 52.07 & \xmark & 0 & 3.00 & 6.50\% \\
					DualNet~\cite{DualNet} & & 40.14\scriptsize{$\pm$1.64}$^\ddagger$ & 40.27 & \xmark & 1000 & 5.04 & 10.90\% \\
					RPSNet~\cite{RPSNet} & & 68.60$^\dagger$ & 11.81 & \xmark & 2000 & 181.00 & 40.4\% \\
					DynaER~\cite{DynaER} & & 74.64$^\dagger$ & 5.77 & \xmark & 2000 & 19.80 & 43.80\% \\
					\midrule
					Upper-bound & \multirow{2}{*}{ResNet152}& 88.54$^\dagger$ & - & - & - & - & - \\
					DynaER~\cite{DynaER} & & 71.01\scriptsize{$\pm$0.58}$^\ddagger$ & 17.53 & \xmark & 2000 & 159.00 & 68.50\% \\
					\midrule
					Upper-bound & \multirow{2}{*}{Customized ViT}& 76.12$^\dagger$ & - & - & - & - & - \\
					DyTox~\cite{dytox} & & 62.06\scriptsize{$\pm$0.25}$^\dagger$ & 14.06 & \xmark & 2000 & 0.04 & 0.38\% \\
					\midrule
					Upper-bound & \multirow{4}{*}{ViT-B/16}& 93.15\scriptsize{$\pm$0.09} & - & -  & - & - & - \\
					L2P~\cite{l2p} & & 83.86\scriptsize{$\pm$0.28}$^\ddagger$ & 9.29 & \checkmark & 0 & 1.94 & 0.56\% \\
					DualPrompt~\cite{DUAL_PROMPT} & & 86.51\scriptsize{$\pm$0.33}$^\ddagger$ & 6.64 & \checkmark & 0 & 1.90 & 0.55\%  \\
					\bf \ours (ours) & & \bf 91.12\scriptsize{$\pm$0.12} & \bf 2.03 & \checkmark & 0 & \bf 0.35 & \bf 0.10\%  \\
					\bottomrule
				\end{tabular}
			}
		\end{center}
    \caption{Comparison with architecture-based methods on split CIFAR-100. \texttt{Diff} (lower is better) measures performance gap between a method and its corresponding upper-bound under the same backbone. $^\dagger$ denotes the results reported from the original papers. $^\ddagger$ represents results copied from DualPrompt~\cite{DUAL_PROMPT}.}
    \label{table:architecture}
\end{table}

\subsection{Extra Ablation Studies}\label{appendix:extra_ablation}
\noindent\textbf{Ablation study on MLP designs}. As the MLP neck is important for training task-specific prompts in our framework, we here probe relations between the MLP width (the number of hidden units), depth (the number of layers), and prompt quality. As shown in Fig.~\ref{fig:mlp}, either monotonously increasing the number of layers or hidden units does not bring benefits. A three-layer MLP with 2048 hidden units, which is the same as 
the conventional MLP neck used in self-supervised representation learning, produces the best performance. Therefore, we adopt this setting as default across our experiments.

\noindent\textbf{Generation of multi-centriod prototypes}. In \ours, we leverage spectral clustering to produce multi-centroid prototypes. Herein, we provide results using the commonly used k-means clustering algorithm. As shown in Table~\ref{table:clustering}, spectral clustering empirically demonstrates a better performance. 

\begin{figure}[t!]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[height=0.7\textwidth]{./figures/top_r.pdf} % first figure itself
        \caption{Top-r retrieval accuracy on split CIFAR-100 using 5-centroid prototype.}
        \label{fig:top-r}
    \end{minipage}
    \hspace{1cm}
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[height=0.7\textwidth]{./figures/mlp.pdf} % second figure itself
        \caption{Ablation studies on the layer number and hidden units of MLP neck.}
        \label{fig:mlp}
    \end{minipage}
\end{figure}

\begin{table}[t]
	\begin{center}
		\resizebox{0.35\linewidth}{!}{%
			\begin{tabular}{c c c cc cc cc}
				\toprule 
				\multirow{2}{*}{\textbf{Methods}} &
				\multicolumn{2}{c}{\textbf{Split CIFAR-100}} \\
				& Avg. Acc ($\uparrow$) &  Forget ($\downarrow$) \\
				\midrule
				K-means & 90.77\scriptsize{$\pm$0.28} & 3.39\scriptsize{$\pm$0.34}  \\
				Spectral Clustering & \bf 91.12\scriptsize{$\pm$0.12} & \bf3.33 \scriptsize{$\pm$0.18} \\
				\bottomrule
		\end{tabular}
	}
	\end{center}
	\caption{Results under different clustering algorithms used for generating multi-centroid prototypes.}
	\label{table:clustering}
\end{table}

\noindent\textbf{Effectiveness of the query function}. In the design of our nearest neighbor query function, we surmise that data samples tend to locate near to their corresponding mass centers giving an appropriate embedding function, so it is important to verify that, given $r$ nearest neighbors, whether or not the target prompt falls in the candidate group. To this end, we demonstrate the top-$r$ accuracy 
under 5-centroid prototypes in Fig.~\ref{fig:top-r}. We can see that the top-$r$ accuracy increases monotonically according to the value of $r$ and $r=3$ works fairly well. Hence, a query vector in combination with key prototypes and a reasonable vicinity range can be effectively leveraged to retrieve the candidate task-specific prompts.

\section{Detailed Visualizations and Analysis}\label{appendix:visual} Fig.~\ref{fig:train_visual} displays training samples from CIFAR-100 in the latent space. The first row shows original data samples with their corresponding class means and multi-centroid key prototypes. As shown in the figure, both class means and multi-centroid key prototypes effectively characterize the distribution of each class. In the second row, when replacing key prototypes with their corresponding value prototypes, there is a clear mismatch between the class distributions and their value prototypes. This observation manifests clear distribution shifts in the latent space after adding task-specific prompts, justifying the necessity of decoupling prototypes into the key prototypes and value prototypes. The third row exhibits value prototypes and embeddings of data samples after adding task-specific prompts. We can find that both class means and multi-centroid value prototypes fit the learned class distributions well. Nevertheless, multi-centroid value prototypes can better capture outliers, thus being more representative.

In Fig.~\ref{fig:train_visual}, we show the efficacy of both key and value prototypes for representing train data. Here, in Fig.~\ref{fig:test_visual}, we exhibit their performances on test data. As shown in the first row, key prototypes work fairly well in representing the original test data embeddings and thus can be safely leveraged in the coarse retrieval process. The second row further validates the necessity of decoupling key and value prototypes from the perspective of test data. As shown in the second row, there are a few classes whose key prototypes can still effectively characterize their corresponding data distributions after task-specific prompts were added, suggesting minor distribution shifts. However, most classes fail to reuse key prototypes after adding task-specific prompts. In contrast, as shown the third row, value prototypes can always be reliably used as classifiers for predictions.
\begin{figure*}[t]
\begin{center}
	\includegraphics[width=0.40\textwidth]{figures/train_proto_train.jpg}
	\label{fig:o_k}
    \hfill
	\includegraphics[width=0.40\textwidth]{figures/train_proto_train_mc.jpg}
	\label{fig:o_mk}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/train_proto_train_pt.jpg}
	\label{fig:o_v}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/train_proto_train_pt_mc.jpg}
	\label{fig:o_mv}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/train_pt_proto_train_pt.jpg}
	\label{fig:p_v}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/train_pt_proto_train_pt_mc.jpg}
	\label{fig:p_mv}
\end{center}
\caption{Visualizations for train data in CIFAR-100. \textbf{First row}: Original data embeddings with class mean (left) and multi-centriod (right) \textit{key} prototypes. \textbf{Second row}: Original data embeddings with class mean (left) and multi-centriod (right) 
\textit{value} prototypes. \textbf{Third row}: Data embeddings after adding task-specific prompts with class mean (left) and multi-centriod (right) \textit{value} prototypes.}
\label{fig:train_visual}
\end{figure*}

\begin{figure*}[t]
\begin{center}
	\includegraphics[width=0.40\textwidth]{figures/test_proto_train.jpg}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/test_proto_train_mc.jpg}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/test_pt_proto_train.jpg}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/test_pt_proto_train_mc.jpg}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/test_pt_proto_train_pt.jpg}
	\hfill
	\includegraphics[width=0.40\textwidth]{figures/test_pt_proto_train_pt_mc.jpg}
\end{center}
\caption{Visualizations for test data in CIFAR-100. \textbf{First row}: Original data embeddings with class mean (left) and multi-centriod (right) \textit{key} prototypes. \textbf{Second row}: Original data embeddings with class mean (left) and multi-centriod (right) 
\textit{value} prototypes. \textbf{Third row}: Data embeddings after adding task-specific prompts with class mean (left) and multi-centriod (right) \textit{value} prototypes.}
\label{fig:test_visual}
\end{figure*}

\end{document}