\section{Method}
\label{sec:Med}

\begin{figure*}[t]
	\centering
	\includegraphics[width=1\textwidth]{figure/framework.pdf}
    \vspace{-6mm}
	\caption{The proposed UMIL framework for WSVAD consists of a backbone model $\theta$, an anomaly head $f$, and a cluster head $g$. We use the predictions by $f$ to divide the snippets into a confident set $\mathcal{C}$ and an ambiguous set $\mathcal{A}$. In MIL, the model is only supervised by the confident snippets to further increase the confidence of anomaly prediction (the black arrows on the probability bar). In UMIL, $f$ is additionally supervised by $\mathcal{A}$ to separate the two clusters identified by $g$ for removing the context bias in $\mathcal{C}$. The black arrow on the similarity bar denotes that minimizing the BCE losses in $\mathcal{A}$ will decrease the dot-product similarity of the predictions on the pair, as they are from different clusters ($y_1 \neq y_2$). Best viewed in color.}
	\label{fig:framework}
    \vspace{-4mm}
\end{figure*}

In Weakly Supervised Video Anomaly Detection (WSVAD), each training video is annotated with a binary anomaly label $y \in \{0, 1\}$ (\ie, normal or abnormal) and partitioned into $m$ snippets. We denote $\mathbf{x}_i, i\in\{1,\ldots, m\}$ as the feature of the $i$-th snippet in the video extracted by a backbone parameterized by $\theta$. The goal of WSVAD is to train a snippet-level anomaly classifier $f(\mathbf{x}_i)$ predicting the probability of the snippet being positive (abnormal).

\subsection{From MIL to Unbiased MIL}

The mainstream method in WSVAD is Multiple Instance Learning (MIL). In MIL, the backbone $\theta$ is pre-trained (\eg, on Kinetics400~\cite{carreira2017quo}) and frozen in training. It aims to learn $f$ so as to predict the most anomalous snippet in a normal video (\ie, $y=0$) as normal, and that in an abnormal video (\ie, $y=1$) as abnormal.
Specifically, for each video, MIL creates a tuple containing the prediction of $f$ on the most anomalous snippet and the video's anomaly label, \ie, $(\mathrm{max}\{f(\mathbf{x}_i)\}_{i=1}^m, y)$. Then MIL aggregates the tuple for all videos to construct a labeled confident snippet set $\mathcal{C}$, and trains $f$ by minimizing the binary cross-entropy (BCE) loss:
\begin{equation}
    \textrm{BCE}(\mathcal{C}) = -\mathop{\mathbb{E}}_{(\hat{y},y)\sim \mathcal{C}} \left[ y\mathrm{log} (\hat{y}) + (1-y)\mathrm{log}(1-\hat{y}) \right],
    \label{eq:1}
\end{equation}
where $\hat{y} = \mathrm{max}\{f(\mathbf{x}_i)\}_{i=1}^m$. Note that some methods~\cite{sultani2018real} use the mean squared error loss, which achieves the same outcome as Eq.~\eqref{eq:1}.
In this way, for a normal video with $y=0$, by \emph{minimizing} $\mathrm{max}\{f(\mathbf{x}_i)\}_{i=1}^m$, $f$ must assign low abnormal probability for all the snippets. For an abnormal video with $y=1$, by \emph{maximizing} $\mathrm{max}\{f(\mathbf{x}_i)\}_{i=1}^m$, $f$ is trained to output an even larger probability for the most confident abnormal snippet.
However, the MIL training scheme suffers from biased sample selection: as $f$ is trained to further increase $\mathrm{max}\{f(\mathbf{x}_i)\}_{i=1}^m$ in an abnormal video, the rest ambiguous snippets become even less likely to be selected by $\mathrm{max}$. Hence MIL essentially discards the ambiguous snippets and only trains on the confident ones, which leads to a biased detector (\eg, Figure~\ref{fig:2}).

In contrast, our proposed Unbiased MIL (UMIL) leverages both the confident and ambiguous snippets to train the anomaly classifier $f$. Specifically, in Step 1, we divide the snippets into a labeled confident snippet set $\mathcal{C}$ and an unlabeled ambiguous snippet set $\mathcal{A}$. In Step 2, we cluster $\mathcal{A}$ into 2 groups in an unsupervised fashion to distinguish the normal and abnormal snippets. Finally, in Step 3, $f$ is supervised by both $\mathcal{C}$ and $\mathcal{A}$ to simultaneously predict the binary labels in $\mathcal{C}$ and separate the clusters in $\mathcal{A}$.


\subsection{Step 1: Divide Snippets}
\label{sec:step1}
Based on the predictions from $f$, we divide the snippets into the confident set $\mathcal{C}$ and the ambiguous one $\mathcal{A}$:

\noindent\textbf{Constructing $\mathcal{C}$}. During training, we track the history of the last 5 predictions from $f$ for each snippet. Then, at the start of every epoch, we select $N$ snippets $\mathbf{x}_1,\ldots,\mathbf{x}_N$ with the least prediction variance, and the confident set $\mathcal{C}$ is given by $\{f(\mathbf{x}_i),y_i\}_{i=1}^N$. The rationale is that for the apparent normal or abnormal snippets (\eg, enclosed in red in Figure~\ref{fig:2}), their predictions tend to quickly converge to confident normal or abnormal with small predictive variance over time. This approach is empirically validated in Appendix, and we point out similar method shows promising results in~\cite{zhong2019graph}.

\noindent\textbf{Constructing $\mathcal{A}$}. The rest of the $M$ snippets have large prediction fluctuations, showing that $f$ is still uncertain about them.
They are collected as the ambiguous set $\mathcal{A} = \{\mathbf{x}_i\}_{i=1}^M$. Note that $\mathcal{A}$ is a set of features at this point, awaiting the next clustering step.

\subsection{Step 2: Clustering Ambiguous Snippets}

While the prediction from $f$ is ambiguous on $\mathcal{A}$, the feature distribution can still reflect the intrinsic differences between normal and abnormal snippets. Hence we aim to cluster $\mathcal{A}$ into 2 groups to distinguish them.
Specifically, we learn a cluster head $g$ that takes the snippet feature $\mathbf{x}\in \mathcal{A}$ as input and outputs the softmax-normalized probabilities for being in each of the 2 clusters. The head $g$ is trained in a pair-wise manner such that a pair of similar features have similar predictions from $g$ (\ie, from the same cluster), and vice versa for dissimilar. To accomplish this, we denote the pair-wise form of $\mathcal{A}$ based on cluster prediction from $g$ as:
\begin{equation}
    \mathcal{A}_g = \{ g(\mathbf{x}_i)^\intercal g(\mathbf{x}_j), \mathbbm{1}(\mathbf{x}_i \sim \mathbf{x}_j) \mid \mathbf{x}_i,\mathbf{x}_j \in \mathcal{A} \},
    \label{eq:2}
\end{equation}
where the dot-product is used to measure the prediction similarity, and $\mathbbm{1}(\cdot)$ is an indicator function that returns 1 if the cosine similarity between $\mathbf{x}_i,\mathbf{x}_j$ is larger than a threshold $\tau$ (\ie, $\mathbf{x}_i \sim \mathbf{x}_j$), and returns 0 otherwise. This allows us to train $g$ by minimizing $\textrm{BCE}(\mathcal{A}_g)$.

With the optimized $g$, each feature $\mathbf{x}_i$ in $\mathcal{A}$ is assigned a cluster label $y_i = \mathrm{argmax} \, g(\mathbf{x}_i)$ as the cluster with the highest predicted probability.
Next, we supervise $f$ by $\mathcal{A}$ to separate the clusters and form our overall objective.

\subsection{Step 3: Overall Objective}

Note that unlike the sample-wise supervision provided by labels in $\mathcal{C}$, \ie, whether a feature is normal or abnormal, the cluster labels in $\mathcal{A}$ only provide pair-wise supervision, \ie, whether a feature pair is from the same cluster.
Hence we supervise $f$ with $\mathcal{A}$ using a pair-wise loss: $f$ is trained to produce similar anomaly prediction on feature pairs with the same cluster label, and push away predictions for those in different clusters. This corresponds to minimizing $\textrm{BCE}(\mathcal{A}_f)$ with $\mathcal{A}_f$ based on the pair-wise prediction similarity of $f$:
\begin{equation}
    \mathcal{A}_f = \{ f(\mathbf{x}_i)^\intercal f(\mathbf{x}_j), \mathbbm{1}(y_i=y_j) \mid \mathbf{x}_i,\mathbf{x}_j \in \mathcal{A} \},
    \label{eq:3}
\end{equation}
where $f(\mathbf{x}_i)^\intercal f(\mathbf{x}_j)$ denotes the dot-product similarity of the binary probabilities (\ie, normal or abnormal)\footnote{While $f$ only outputs the probability of being abnormal as $p$, the probability of being normal is easily computed as $1-p$.} with slight abuse of notation. The overall objective of UMIL is given by:
\begin{equation}
    \mathop{\mathrm{min}}_{\theta,f,g} \overbrace{\textrm{BCE}(\mathcal{C})}^{\text{$\mathcal{C}$ supervision}} + \overbrace{\alpha \textrm{BCE}(\mathcal{A}_f)}^{\text{$\mathcal{A}$ supervision}} + \overbrace{\beta \textrm{BCE}(\mathcal{A}_g)}^{\text{Clustering in $\mathcal{A}$}},
    \label{eq:4}
\end{equation}
where $\alpha,\beta$ are trade-off parameters with ablations in Section~\ref{sec:Abla}. 
Hence in addition to the supervision from $\mathcal{C}$ as in MIL, $f$ in UMIL is additionally supervised by $\mathcal{A}$ to separate its 2 clusters identified by $g$ to remove the context bias in $\mathcal{C}$ (Figure~\ref{fig:2}). 
This unbiased objective allows us to train not only $f$, but also to fine-tune the backbone $\theta$ to get a tailored representation for VAD.

\noindent\textbf{Training and Testing}. Before training, the backbone $\theta$ is first pre-trained with MIL, and $f,g$ are randomly initialized. Then the models are trained with our proposed UMIL by iterating Algorithm~\ref{alg:1} until convergence. In testing, anomalies are labeled on the frame level. The model is evaluated with a non-overlapping sliding window of frames (\ie, each window of frames is a snippet) to predict anomaly whenever the window intersects with any anomaly frame.

\input{others/algo1_zq}
% \vspace{-4mm}