\section{Appendix}
This section is organized as follows:

\begin{itemize}[leftmargin=+0.1in,itemsep=2pt,topsep=0pt,parsep=0pt]
    \item Section~\ref{sec:LO} provides more details about our training objectives. We detail the implementation of the self-training used in our experiments: FixMatch~\cite{sohn2020fixmatch}.
    \item Section~\ref{sec:DIS} explains about how the feature clustering boosts the UMIL during training. 
    \item Section~\ref{sec:AE} shows more comparisons and standard deviations on UCF-crime~\cite{sultani2018real}, and TAD~\cite{lv2021localizing}. In particular, we first discuss the statics of anomaly events in UCF-crime in Section~\ref{sec:pp}, and then provide more experimental results of the proposed UMIL.
    \item Section~\ref{sec:roc} gives the full version of ROC curves on various benchmarks. This is a supplement to Figure.~6 in the manuscript.
    \item Codes are also provided, which include the training and testing scripts on the two classic datasets. The setup instructions and commands used in our experiments are included in the \texttt{README.md} file.
\end{itemize}


\section{Loss Objectives}
\label{sec:LO}
In this section, we give the details of the self-training objective $\mathcal{L}_{st}$ used in the MIL pre-training and UMIL training, as in Eq.(1) and Eq.(4) in the manuscript, then the overall MIL pre-training objective is derived as the following:
\begin{equation}
    \mathcal{L}_{mil} =  \textrm{BCE}(\mathcal{C}) + \lambda \mathcal{L}_{st} ,
    \label{eq:1}
\end{equation}
where $\lambda$ stands for the balance weight.
The overall objective of UMIL is derived correspondingly.
Note that self-training strategy is an important approach popular in domain adaption~\cite{french2017self,long2018conditional,liu2021cycle,zou2018unsupervised,kumar2020understanding}.
In this work, we introduce self-training to boost feature learning in WSVAD by incorporating data augmentation of \textbf{FixMatch}~\cite{sohn2020fixmatch}.
Specifically, along with the training of MIL, we generate pseudo labels with original video snippet data and seek to minimize the entropy between the predictions of augmented data as well as original data.
Given the pair of feature $\textbf{x}$ and $\textbf{x}'$ from the original data and random augmentation data, respectively, the \textbf{FixMatch}-driven self-training loss derives as:
\begin{equation}
    \mathcal{L}_{st} = \mathbbm{1} (\textrm{argmax}f(\textbf{x}) > \delta) \textrm{BCE} (\textbf{x}',\textrm{argmax}f(\textbf{x})),
    \label{eq:2}
\end{equation}
here, $\mathbbm{1}$ represents the indicator function that returns 1 if the condition is met, $\delta$ is the confident threshold, and $\textrm{BCE}$ corresponds to the binary cross entropy mentioned in the manuscript.
In the experiments, we used grid searching for finding a proper $\delta$. 
The results are listed in Section~\ref{sec:AE}.
\begin{figure}[t]
    \centering
    \footnotesize
    \begin{subfigure}[t]{0.23\textwidth}
         \includegraphics[width=\textwidth]{figure/ano_leng.png}
         \caption{}
         \label{fig:1a}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth} % the hidden unwanted image
         \includegraphics[width=\textwidth]{figure/ano_rate.png}
         \caption{}
         \label{fig:1b}   
    \end{subfigure}
    % \includegraphics[width=0.48\textwidth]{figure/abstract.pdf}
	\caption{(a) The statistics of anomaly event length and (b) the ratio of anomaly event length to coarse snippet length.}
	\label{fig:ano_rate}
\end{figure}

\section{Discussion on clustering}
\label{sec:DIS}
During the training of UMIL, wrong clustering can bring risks. However, the modern pre-trained backbones (\eg, CLIP) capture rich prior knowledge, such that the intrinsic difference between normal and abnormal snippets is sufficiently expressed in the feature space. This ensures that 1) during clustering, the normal/abnormal snippets can be separated into different clusters; and 2) combining $\mathcal{C}$- and $\mathcal{A}$- supervision in Eq. 4 leads to a classifier using true anomaly features instead of context for prediction (\eg, vertical black line in Figure 2). 
In future work, we will explore other prior knowledge or inductive bias to further separate normal/abnormal snippets.

\section{Additional Experiments}
\label{sec:AE}

\subsection{Pre-processing Analysis}
\label{sec:pp}
In this section, we first analyze the rationality in the pre-processing step of previous WSVAD approaches. 
As mentioned in Section 4.2 of the manuscript, existing works follow the average feature pipeline. 
They first divide video sequences into multiple coarse snippets, \eg 1 video 32 snippets, then take the \textit{snippet-level average features} as inputs into anomaly detectors. 
However, real-world anomalies are extremely rare and short in time. 
The subtle anomaly events are easily diluted or even covered by normal patterns after the spatio-temporal pooling operation.

To better analyze the problem, we annotate the large training set of UCF-crime~\cite{sultani2018real}. 
In detail, 5 trained annotators are involved in the process and the final labels are generated by averaging the results.
In Figure~\ref{fig:ano_rate}, we depict the statistics of the anomaly events' length. 
The average length of anomaly events is about 698 frames (extracted from videos with 30FPS), compared with an average coarse snippets' length of 200 frames. 
Note that  coarse snippets' length is obtained by dividing each video into 32 snippets, which is widely used as the default in existing works~\cite{sultani2018real,lv2021localizing,zhu2019motion}.
We also depict the ratio of anomaly event length to coarse snippet length in Figure~\ref{fig:1b}.
As is shown, there are 164 out of 925 anomaly events whose length ratios are less than 1. 
It means that these anomaly events are short than the coarse snippet.
More importantly, the length ratios of 86 anomaly events are less than 0.5.
Considering the anomalies only take place in a small part of whole frames, the anomaly information is inevitably concealed in the spatio-temporal feature pooling process, which is hurtful for video anomaly detection.

\subsection{Feature Fine-tuning Analysis}
When the backbone is loaded with pre-trained weights on kinetics 400 and frozen during UMIL training, the performance will drop from $86.75\%$ (with fine-tuning) to $83.44\%$ (frozen) on UCF, and $92.93\%$ to $90.71\%$ on TAD. This validates that fine-tuning in UMIL enables learning a representation tailored for WSVAD, which is beneficial for anomaly detection.

\subsection{Self-training In UMIL}
In this work, we use the learned anomaly classifier to generate pseudo-labels on samples in the ambiguous set $\mathcal A$.
Consequently, the ambiguous samples, which are largely neglected in existing MIL, can further participate in our UMIL with pseudo-labels.
When the self-training loss is removed, the results of UMIL are $83.66\%$ ($\downarrow3.09\%$) on UCF and $91.74\%$ ($\downarrow 1.19\%$) on TAD. 
This validates the effectiveness of the self-training loss. Further experimental analysis of the Self-training can be found in the following.

\subsection{Confident Threshold in Self-training}
\label{sec:ct}
In Table~\ref{tab:conf}, we list the results of adding self-training to MIL baseline model with varying confident threshold $\delta$. By increasing the confident threshold, less and highly confident samples are involved in the objective of self-training. As is shown in the table, $0.8$ is a suitable threshold that the self-training tool obtains good results on both datasets. When the threshold is up to 1, few samples will be selected leading to the ineffectiveness of self-training.
\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{ccccccc}
\toprule\hline
Threshold(\%)  & 0.3 & 0.5 & 0.7 & \cellcolor{mygray}\textbf{0.8} & 0.9 & 1.0 \\ \hline \hline
$\mathrm{AUC}_O$ (\%) - UCF & 80.9 & 81.2 & 81.9 & \cellcolor{mygray}\textbf{82.0} & 81.5 & 80.7\\ 
$\mathrm{AUC}_O$ (\%) - TAD & 89.0 & 90.1 & 90.5 & \cellcolor{mygray}\textbf{90.8} & 90.1 & 89.1 \\ \hline
\bottomrule
\end{tabular}%
}
\caption{Ablation on the Confident threshold in self-training based on MIL model on UCF-Crime and TAD.}
\label{tab:conf}
\end{table}

\subsection{Similarity Threshold in Clustering}
\label{sec:st}
The cluster component alone is for separating normal/abnormal snippets in the ambiguous set $\mathcal{A}$ as two clusters. 
It doesn't directly benefit the learning of anomaly classifier $f$. 
If the $\mathcal{A}$-supervision is removed, $f$ will be trained only on confident normal/abnormal snippets, and our approach will basically reduce to the existing MIL with similar performance.
We also conducted experiments to analyze the effect of varying similarity thresholds in clustering.
The experimental results are listed in Table~\ref{tab:simi}.
As we can see, the performance is insensitive to the change of the threshold in cosine similarity in Eq.~(2) of the manuscript.
Because the clustering property is acquired along with the feature fine-tuning of the backbone.
Then $0.8$ is chosen as the default similarity threshold in clustering.
\begin{table}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{cccccc}
\toprule\hline
Threshold(\%) & 0.5 & 0.6 & 0.7 & \cellcolor{mygray}\textbf{0.8} & 0.9\\ \hline \hline
$\mathrm{AUC}_O$ (\%) - UCF & 86.4 & 86.6 & 86.8 & \cellcolor{mygray}\textbf{86.8} & 86.6 \\ 
$\mathrm{AUC}_O$ (\%) - TAD & 92.5 & 92.7 & 92.8 & \cellcolor{mygray}\textbf{93.0} & 92.9 \\ \hline
\bottomrule
\end{tabular}%
}
\caption{Ablation on the similarity threshold in clustering on UCF-Crime and TAD.}
\label{tab:simi}
\end{table}

\subsection{Confident Sample Selection Strategy}
\label{sec:css}
In this section, we also conducted experiments to compare \textit{Historical Variance} with \textit{Max Confidence} in the confident sample selection strategy.
Specifically, we select the top k (\%) abnormal and normal snippets with maximum confidence in abnormal videos as the confident set $\mathcal{C}$.
As we can see, the best AUC performances of \textit{Historical Variance} ($86.8\%$ for UCF-crime and $93.0\%$ for TAD) are superior to those of \textit{Max Confidence} ($85.9\%$ for UCF-crime and $92.2\%$ for TAD).
As mentioned in the manuscript (Section 3.2 Step 1), the predictions of the `easy' normal or abnormal snippets tend to quickly converge to confident normal or anomaly with small variance over time.
As a result, collecting historical information of score variance is a better choice for distinguishing confident and ambiguous samples.
\begin{table}[t]
\centering
\scalebox{1.0}{
\begin{tabular}{cccccc}
\toprule\hline
Threshold(\%) & 10 & 30 & 50 & 70 & 90  \\ \hline \hline
$\mathrm{AUC}_O$ (\%) - UCF & \cellcolor{mygray}\textbf{85.9} & 85.7 & 85.3 & 84.6 & 83.8 \\ 
$\mathrm{AUC}_O$ (\%) - TAD & 92.1 & \cellcolor{mygray}\textbf{92.2} & 92.0 & 91.4 & 90.8 \\ \hline
\bottomrule
\end{tabular}%
}
\caption{Ablation on the max confident threshold to divide the confident/ambiguous snippet set on UCF-Crime and TAD.}
\label{tab:max}
\end{table}

\subsection{Comparison on ShanghaiTech}
We also added experiment on ShanghaiTech benchmark \cite{liu2018future}. We failed to implement our method on Ped2, due to the absence of data splits. We achieved comparative performance with existing SOTAs as shown below, and believed that there is potential for further improvements given more time. Additionally, the high accuracy on this dataset indicates that it may contain mainly apparent anomalies, which explains why MIL-based methods already perform well (Section 3.1).
\begin{table}[h]
    \centering
    \small
    \scalebox{1.2}{
    \begin{tabular}{cccccc}
        \hline
        \textbf{Method} & GCN & RTFM & Baseline & Ours\\ 
        \hline
        \textbf{AUC(\%)} & 84.44 & 97.21 & 95.20 & 96.78\\ 
        \hline
    \end{tabular}}
\end{table}

\section{Visualization of ROC Curves}
\label{sec:roc}
\begin{figure}
    \centering
    \footnotesize
    \scalebox{1.05}{
    \begin{subfigure}[t]{0.23\textwidth}
         \includegraphics[width=\textwidth]{figure/rocs_u_full.pdf}
         \phantomcaption
         \label{fig:roca}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth} % the hidden unwanted image
         \includegraphics[width=\textwidth]{figure/rocs_t_full.pdf}
         \phantomcaption
         \label{fig:rocb}   
    \end{subfigure}}
    \caption{Full version of the ROC curves on UCF (left) and TAD (Right).}
    \label{fig:roc}
\end{figure}