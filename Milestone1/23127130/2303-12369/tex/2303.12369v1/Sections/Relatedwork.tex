\section{Related Work}
\label{sec:RW}
The research lineup of video anomaly detection falls into two classes:  unsupervised and weakly-supervised settings.

\noindent\textbf{Unsupervised methods} include the ones that only use unlabelled training data or directly train and test on testing data. Del~\etal~\cite{del2016discriminative} proposed to detect changes on a sequence of video data to detect unique frames. Tudor~\etal~\cite{tudor2017unmasking} introduced unmasking technology~\cite{koppel2007measuring} to iteratively train a binary classifier to distinguish the most discriminant features. Lately, Zaheer~\etal~\cite{zaheer2022generative} exploited the low frequency of anomalies by building a cross-supervision between a generator and a discriminator.
There are also One-Class Classification (OCC) methods assume the availability of normal training data only and approach the problem in an unsupervised manner. 
Typically, researchers fit a model with only normal data, then detect anomalies by distinguishing the events that deviate from the model. 
Early works used hand-crafted appearance and motion features~\cite{adam2008robust,antic2011video,lu2013abnormal,mahadevan2010anomaly,mehran2009abnormal}.
Thanks to the impressive progress of deep learning, recent works used the features from pre-trained deep neural networks and built an anomaly classifier upon them~\cite{ravanbakhsh2018plug,hasan2016learning}. There are also methods for self-supervised feature learning~\cite{sabokrou2015real,xu2015learning}, where a popular approach is by temporal prediction~\cite{xingjian2015convolutional,liu2018future,lv2021learning}.
However, unsupervised methods suffer from false alarms for unseen normal patterns, since it is impossible to collect all kinds of normality in one dataset.

\noindent\textbf{Weakly-supervised methods} exploit both normal and abnormal training data with weak annotations only on the video-level~\cite{sultani2018real}. 
Multiple instance learning (MIL) is the mainstream paradigm that uses video-level labels for training snippet-level anomaly detectors~\cite{sultani2018real,he2018anomaly,zhu2019motion}. 
Generally, they embrace the two-stage anomaly detection pipeline, which performs anomaly detection upon pre-extracted features.
In particular, Zhong~\etal~\cite{zhong2019graph} considered the WSVAD task as supervised learning under noise labels and they designed an alternate training procedure to enhance the discrimination of action classifiers.
Lv~\etal~\cite{lv2021localizing} focused on anomaly localization and proposed a higher-order context model as well as a margin-based MIL loss.
Tian~\etal~\cite{tian2021weakly} investigated the feature magnitude to facilitate anomaly detection and selected the instances of top-k scores to better represent the video for MIL.
Li~\etal~\cite{li2022self} proposed multiple sequence learning, where consecutive snippets with high anomaly scores are selected in MIL learning.
They attempted to improve the sample selection for improving MIL, whose biased nature is not changed yet. In this paper, our unbiased MIL framework is the first effort on removing the context bias~\cite{yue2020interventional,yue2021transporting} in WSVAD. In addition, we integrate feature representation fine-tuning and anomaly detector learning into an end-to-end training fashion.