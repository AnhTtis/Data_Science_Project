\section{Experiments}
\label{sec:Exp}
\subsection{Datasets and Evaluation Metrics}
We conducted extensive experiments and ablations on two standard WSVAD evaluation datasets~\cite{sultani2018real,lv2021localizing}. As per standard in WSVAD, the training videos only have video-level labels, and the test videos have frame-level labels. Other details are given below:

\noindent\textbf{UCF-Crime}~\cite{sultani2018real} is a large-scale dataset that contains 1,900 untrimmed real-world outdoor and indoor surveillance videos. The total length of the videos is 128 hours, which contains 13 classes of anomalous events.
We follow the standard split: the training set contains 1,610 videos, and the test set contains 290 videos.

\noindent\textbf{TAD} dataset~\cite{lv2021localizing} contains real-world videos of traffic scenes with a total length of 25 hours and 1,075 average frames per video.
The videos contain more than 7 categories of anomalies that are common on roads.
The dataset is partitioned as a training set with 400 videos, and a test set with 100 videos.

\noindent\textbf{Evaluation Metrics}. Following previous works~\cite{sultani2018real,zhong2019graph}, we used the Area Under the Curve (AUC) of the frame-level ROC (Receiver Operating Characteristic) as the main evaluation metric for TAD and UCF-Crime. Intuitively, a larger AUC means a larger margin between the normal and abnormal snippet predictions, hence indicating a better anomaly classifier.
Inspired by Lv~\etal~\cite{lv2021localizing}, besides evaluating AUC on the overall test set with normal and abnormal videos, denoted as $\mathrm{AUC}_O$, we also computed the AUC on abnormal ones alone, denoted as $\mathrm{AUC}_A$.
The rationale is to remove normal videos where all snippets are normal (label 0), and keep only the abnormal ones with both kinds of snippets (label 0,1), which truly challenges a classifier's capability of localizing anomalies.
 
\subsection{Implementation Details}
\label{sec:ID}

\noindent\textbf{Video Sequence Partition}. Existing works partition each video into multiple coarse snippets, and use the \emph{average feature} in each one as the input to their classifiers (Figure~\ref{fig:pip} left). However, we find that the subtle anomaly feature is often diluted by averaging features over the coarse snippets (see Appendix).
This has less impact on the traditional MIL compared to our UMIL, as MIL only leverages the confident snippets with apparent anomalies.
Therefore, in UMIL training, we used fine-grained snippets with one-second lengths. In testing, to generate the prediction for a coarse snippet, we used the \emph{average predictions} over the fine snippets inside the coarse one (Figure~\ref{fig:pip} right).
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figure/pipeline.pdf}
    \vspace{-8mm}
	\caption{Average feature versus average prediction testing. $\theta,f$: the feature backbone and anomaly classifier, respectively.}
	\label{fig:pip}
    \vspace{-6mm}
\end{figure}

\noindent\textbf{Baseline}. We built a baseline to validate that the improvements of UMIL are indeed from the unbiased training scheme (Section~\ref{sec:Abla}), rather than the above testing scheme based on average predictions. Specifically, the baseline has exactly the same model design as UMIL, and we trained it with the MIL objective in Eq.~\eqref{eq:1} on fine snippets and tested it by averaging predictions. Hence the only difference between the baseline and UMIL is the training objective.

\noindent\textbf{Model Training}. We implemented the backbone $\theta$ with the X-CLIP-B/32 model~\cite{xclip} fine-tuned on Kinectics-400~\cite{carreira2017quo} to improve its capabilities in action recognition. We used the fully connected layer to implement the anomaly classifier $f$ and the cluster head $g$.
We trained our model with the AdamW optimizer~\cite{loshchilov2019decoupled} using an initial learning rate of $8$e-$6$, weight decay of $0.001$, and batch size of $8$.
We utilized the cosine annealing scheduler and warmed up the learning rate for 5 epochs.
Our UMIL model was pre-trained with MIL for 30 epochs, followed by 10 epochs of UMIL training.
We conducted all experiments on $4$ TITAN RTX GPUs.
We implement the max value scores as well as max margin scores~\cite{lv2021localizing} in $\mathcal{C}$ supervision of Eq~\ref{eq:4}.
We also incorporated entropy minimization as a standard auxiliary objective~\cite{liu2021cycle,long2018conditional}, and added the self-training loss, which leverages the learned unbiased anomaly classifier $f$ to generate accurate pseudo-labels on samples in the ambiguous set $\mathcal{A}$ for additional supervision. Details in Appendix.

\subsection{Main Results}
\label{sec:4.3}
\noindent\textbf{UCF-Crime and TAD}.
In Table~\ref{tab:ucf-crime}, we compared our UMIL with other state-of-the-art (SOTA) methods in both Unsupervised VAD (UVAD) and WSVAD. On UCF-Crime~\cite{sultani2018real}, UMIL achieves the best $\mathrm{AUC}_O$ and $\mathrm{AUC}_A$ among all the methods, with an improvement of +1.37\% and +1.3\%, respectively. UMIL also significantly outperforms all methods in TAD~\cite{lv2021localizing} by +3.3\% on $\mathrm{AUC}_O$ and +4.2\% on $\mathrm{AUC}_A$.

\begin{table}[t]
    \centering
    \scalebox{0.8}{
    % \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}c|c|c|c}
      \toprule\hline
        Category & Method         & $\mathrm{AUC}_O$ (\%) & $\mathrm{AUC}_A$ (\%)\\ 
      \hline\hline
      \multirow{6}{*}{\rotatebox{90}{UVAD}}
      & SVM Baseline   & 50.00  & 50.00     \\
      & Conv-AE~\cite{hasan2016learning}   & 50.60    & -   \\
      & Sohrab et al.~\cite{sohrab2018subspace}  & 58.50  & -  \\
      & Lu et al.~\cite{lu2013abnormal}  & 65.51  & -  \\
      & BODS~\cite{wang2019gods}           & 68.26  & -  \\
      & GODS~\cite{wang2019gods}           & 70.46  & -  \\ \hline
      \multirow{9}{*}{\rotatebox{90}{WSVAD}}
      & Sultani et al.~\cite{sultani2018real} & 75.41 &54.25    \\
      & Zhang et al.~\cite{zhang2019temporal}            & 78.66  & -    \\
      & Motion-Aware~\cite{zhu2019motion} & 79.10   & 62.18    \\
      & GCN-Anomaly~\cite{zhong2019graph} & 82.12  & 59.02    \\
      & Wu et al.~\cite{Wu2020not} & 82.44  & -    \\
      & RTFM~\cite{tian2021weakly}          & 84.30 & -   \\ 
      & WSAL~\cite{lv2021localizing}          & 85.38  & 67.38\\ 
      & \cellcolor{mygray}Baseline & \cellcolor{mygray}80.67  & \cellcolor{mygray}60.57 \\ 
      & \cellcolor{mygray}\textbf{UMIL}  & \cellcolor{mygray}\textbf{86.75}  & \cellcolor{mygray}\textbf{68.68} \\ \hline\bottomrule
    \end{tabular}%
    % }
    }
    \vspace{-2mm}
    \caption{Frame-level AUC performance on UCF-Crime. Best results in bold. $\mathrm{AUC}_O$ and $\mathrm{AUC}_A$ denote that the AUC computed on the overall test set and only abnormal test videos, respectively. ``UVAD'' and ``WSVAD'' under category denote Unsupervised VAD and Weakly-Supervised VAD, respectively.} 
    \label{tab:ucf-crime}
    \vspace{-4mm}
\end{table}

\noindent\textbf{Overall Observations}.
1) Notice that our baseline performs similarly (\eg, $\mathrm{AUC}_O$ on TAD) or even worse (\eg, 60.57\% versus 67.38\% on UCF-Crime $\mathrm{AUC}_O$) compared to existing MIL-based methods. This validates that the improvements from UMIL are not from the test scheme of averaging predictions. 
2) In particular, our improvement in $\mathrm{AUC}_A$ indicates that the superior performance of UMIL on $\mathrm{AUC}_O$ is not merely from easy normal videos, but also from improved capabilities to identify anomalous snippets in abnormal videos.
3) Moreover, on both datasets, WSVAD significantly improves over UVAD on $\mathrm{AUC}_O$, which empirically validates that detecting open-set anomalies in UVAD is ill-posed (Section~\ref{sec:intro}). However, the improvements in $\mathrm{AUC}_A$ are much smaller (\eg, 54.25\% over 50.00\% on UCF-Crime). This shows that the existing WSVAD methods are still biased toward the apparent normal/abnormal, causing many false positives and negatives on ambiguous snippets from the abnormal videos.
4) Our UMIL significantly improves the $\mathrm{AUC}_A$ over MIL (\eg, +4.2\% on TAD), which demonstrates the effectiveness of using ambiguous snippets in UMIL to learn an unbiased invariant classifier.
5) Interestingly, TAD tends to have larger $\mathrm{AUC}_O$ but lower $\mathrm{AUC}_A$, \eg, from UCF-Crime to TAD, UMIL's $\mathrm{AUC}_O$ is 6.2\% higher, but $\mathrm{AUC}_A$ is 2.8\% lower. The improved overall performance suggests that TAD has stronger context bias in the confident set, \ie, more apparent normal/abnormal snippets, and the dropped $\mathrm{AUC}_A$ indicates that it contains more subtle anomalies in the ambiguous snippets that are hard to detect and localize.
This also explains why our UMIL improves $\mathrm{AUC}_A$ more on TAD than UCF-Crime by incorporating ambiguous snippets to remove the context bias from the confident set.

\begin{table}[t]
    \centering
    \scalebox{0.95}{
    % \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}c|c|c|c}
      \toprule\hline
      Category       & Method         & $\mathrm{AUC}_O$ (\%) & $\mathrm{AUC}_A$ (\%)\\ \hline\hline
      \multirow{3}{*}{\rotatebox{90}{UVAD}}
      & SVM Baseline   & 50.00  & 50.00     \\
      & Luo~\etal~\cite{luo2017revisit}  & 57.89  & 55.84  \\
      & Liu~\etal~\cite{liu2018future}           & 69.13 & 55.38   \\ \hline
      \multirow{6}{*}{\rotatebox{90}{WSVAD}}
      & Sultani~\etal~\cite{sultani2018real} & 81.42 &55.97    \\
      & Motion-Aware~\cite{zhu2019motion} & 83.08  & 56.89    \\
      & GIG~\cite{lv2020global}          & 85.64 & 58.65   \\ 
      & WSAL~\cite{lv2021localizing}          & 89.64  & 61.66\\ 
      & \cellcolor{mygray}Baseline & \cellcolor{mygray}89.10 & \cellcolor{mygray}56.47 \\ 
      %& MIL baseline + RTFM\cite{tian2021weakly}        &  91.28 & 57.65 \\ 
      & \cellcolor{mygray}Ours & \cellcolor{mygray}{\textbf{92.93}} & \cellcolor{mygray}{\textbf{65.82}} \\ \hline\bottomrule
    \end{tabular}%
    % }
    }
    \vspace{-3mm}
    \caption{Frame-level AUC performance on TAD benchmark.} 
    \vspace{-3mm}
    \label{tab:tad}
\end{table}

\begin{table}[t]
\centering
\scalebox{0.75}{
% \resizebox{\linewidth}{!}{%
\begin{tabular}{cccc|cc}
\toprule\hline
Baseline & ST & RTFM* & UMIL &  $\mathrm{AUC}_O$ (\%) - UCF  &  $\mathrm{AUC}_O$ (\%) - TAD\\ \hline \hline
\checkmark & & & & 80.67 & 89.10 \\
\checkmark & \checkmark & & & 82.01 & 90.80\\ \hline
\checkmark & \checkmark & \checkmark & & 83.45 & 91.28 \\ 
\checkmark & & & \checkmark & 83.66 & 91.74 \\ 
\cellcolor{mygray}\checkmark & \cellcolor{mygray}\checkmark & \cellcolor{mygray} & \cellcolor{mygray}\checkmark & \cellcolor{mygray}\textbf{86.75} & \cellcolor{mygray}\textbf{92.93}  \\ \hline
\bottomrule
\end{tabular}%
}
\vspace{-3mm}
\caption{Ablation studies of the components in UMIL on UCF-Crime and TAD. *: we re-implemented RTFM with our backbone and average-prediction-based testing scheme for fair comparison.}
\vspace{-3mm}
\label{tab:ablation}
\end{table}

\begin{table}[t]
\centering
\scalebox{1.0}{
% \resizebox{\linewidth}{!}{%
\begin{tabular}{cccccc}
\toprule\hline
Threshold(\%) & 10 & \cellcolor{mygray}\textbf{30} & 50 & 70 & 90  \\ \hline \hline
$\mathrm{AUC}_O$ (\%) - UCF & 86.8 & \cellcolor{mygray}\textbf{86.8} & 85.9 & 84.3 & 83.1 \\ 
$\mathrm{AUC}_O$ (\%) - TAD & 92.7 & \cellcolor{mygray}\textbf{93.0} & 92.8 & 91.5 & 91.1 \\ \hline
\bottomrule
\end{tabular}%
}
\vspace{-3mm}
\caption{Ablation on the threshold to divide the confident/ambiguous snippet set on UCF-Crime and TAD.}
\label{tab:thre}
\vspace{-5mm}
\end{table}
\subsection{Ablations}
\label{sec:Abla}

\noindent\textbf{Components}. Our approach has 2 main components: 1) the self-training objective; 2) the UMIL objective in Eq.~\eqref{eq:4}. We validate the effectiveness of each component in Table~\ref{tab:ablation} with $\mathrm{AUC}_O$. All ablations in the table are on the equal ground---using average prediction instead of average feature for anomaly detection (\ie, Baseline). By comparing the first two lines, we observe that self-training can improve $\mathrm{AUC}_O$ from $80.67\%$ to $82.01\%$ on UCF-crime and $89.10\%$ to $90.80\%$ on TAD. To independently evaluate the effectiveness of UMIL objective, we re-implement the SOTA RTFM~\cite{tian2021weakly} using our backbone and add the self-training objective, namely RTFM*. The result is listed in line 3. Our UMIL in line 4 still significantly outperforms RTFM* (+$3.3\%$ on UCF-crime and +$1.7\%$ on TAD), hence validating the effectiveness of our unbiased learning objectives.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.4\textwidth]{figure/cm.pdf}
    \vspace{-4mm}
	\caption{Ablations on the trade-off parameters.}
	\label{fig:cm}
    \vspace{-4mm}
\end{figure}

\noindent\textbf{Confident Threshold}. We then conducted experiments to analyze the effects of the variance threshold for dividing confident and ambiguous snippets as in Section~\ref{sec:step1}.
Specifically, we selected $k$ (\%) training snippets with the minimum variance on their prediction history with varying $k$ as in Table~\ref{tab:thre}. Overall the threshold is easy to determine, \ie, 10-50\% is a reasonable range with 30\% being the best.

\noindent\textbf{Trade-off Parameters}. Recall that we use $\alpha$ and $\beta$ in Eq.~\eqref{eq:4} as the trade-off for the supervision from the ambiguous set $\mathcal{A}$ and clustering, respectively. We empirically find in Figure~\ref{fig:cm} that $\alpha,\beta=0.1$ are suitable across the two datasets, hence we used this setting in the experiments by default. In general, the choice of $\alpha$ depends on the strength of the context bias in the confident set, \eg, TAD has strong bias as analyzed in Section~\ref{sec:4.3}, which cannot be overcome with a small $\alpha$ (\eg, $\alpha$=0.01 has low performance).

\begin{figure}
    \centering
    \footnotesize
    \scalebox{1.05}{
    \begin{subfigure}[t]{0.23\textwidth}
         \includegraphics[width=\textwidth]{figure/rocs_u.pdf}
         \phantomcaption
         \label{fig:roca}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth} % the hidden unwanted image
         \includegraphics[width=\textwidth]{figure/rocs_t.pdf}
         \phantomcaption
         \label{fig:rocb}   
    \end{subfigure}}
    \vspace{-8mm}
    \caption{ROC curves on UCF and TAD. Note that we only show part of the curves for visual clarity, as the other part of the methods have a large overlap when the true positive rate approaches 100\%.}
    \label{fig:roc}
    \vspace{-6mm}
\end{figure}

\noindent\textbf{Class-wise AUC}. On UCF-Crime dataset, the class of anomaly in each test video is given. This allows us to plot the class-wise $\mathrm{AUC}_A$ to examine models' capabilities to detect subtle abnormal events. In Figure~\ref{fig:hist}, we compared UMIL with baseline and RTFM*, where ``Average'' shows the overall $\mathrm{AUC}_A$ and the rest shows the class-wise one.
We have the following observation:
1) Both of the two MIL-based methods perform well on human-centric anomaly classes with drastic motions, \eg, ``Assault'' and ``Burglary''. These classes correspond to apparent anomalies as the backbone expresses the human action feature well (fine-tuned on the action recognition Kinetics400 dataset\cite{carreira2017quo}).
2) However, we notice that they easily fail to distinguish anomalies with subtle motions, \eg, ``Arson'' and ``Vandalism'', as well as non-human-centric anomalies, \eg, ``Explosion''. These classes correspond to ambiguous anomalies discarded by the biased training in MIL.
3) Our UMIL performs similarly on the above apparent anomaly classes and much better on the other subtle anomalies, which largely contributes to the superior anomaly detection and localization performance.
Overall, observation 1 and 2 empirically verifies the biased prediction situation of MIL in Figure~\ref{fig:abstract} and Figure~\ref{fig:2}. In contrast, our UMIL convincingly improves the performance on ambiguous anomalies with almost no sacrifice on the confident ones, which validates the effectiveness of our approach, \ie, identifying the invariance between the two types of anomalies to remove the bias in MIL.
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figure/subtle.pdf}
    \vspace{-6mm}
	\caption{Class-wise $\mathrm{AUC}_A$ of three methods on UCF-Crime.}
	\label{fig:hist}
    \vspace{-7mm}
\end{figure}

\begin{figure*}[t!]
	\centering
    \vspace{-4mm}
	\includegraphics[width=1\textwidth]{figure/cases_new.pdf}
    \vspace{-10mm}
	\caption{Visualization cases of ground-truth and anomaly score curves of various approaches. The white and black triangles denote the location of the normal and abnormal frame displayed on the left, respectively. The green curves represent the anomaly predictions of various methods. The pink background corresponds to the ground-truth abnormal regions.}
	\label{fig:cases}
    \vspace{-4mm}
\end{figure*}

\noindent\textbf{ROC Curve}. In Figure~\ref{fig:roc}, we draw the ROC Curve on the overall test set for our baseline, the re-implemented RTFM* and UMIL, which shows the true and false positive rate for detecting anomaly on a sweeping threshold over the predictions. VAD is evaluated using the area under this curve to demonstrate the overall separation of normal and abnormal snippet predictions. However, when applying a detector for real-world usage, we need to choose a specific threshold (\eg, with a maximum tolerable false positive rate). We observe from Figure~\ref{fig:roc} that our UMIL outperforms the two MIL baselines in every inch, which further shows the strength of our proposed unbiased training.

\noindent\textbf{Qualitative Analysis}. In Figure~\ref{fig:cases}, we show the continuous predictions of anomaly probabilities from our baseline, RTFM*, and our UMIL on 4 test videos on UCF-crime. We summarize the observations:
1) For the MIL baseline (2nd column), we observe that it assigns a larger probability on the pre-explosion snippets from B1 and B2 (top two videos), \eg, workers performing maintenance and snippets with smoke, yet the actual explosion may have a lower prediction (\eg, comparing the height of the green lines on the white and black triangle locations). Similarly, on B3, the running person (white triangle) triggers a larger anomaly prediction than the actual vandalism (black triangle). This further illustrates the biased prediction problem in MIL.
2) RTFM (3rd column) uses feature magnitudes to assist anomaly detection by assuming anomalous snippets have larger magnitudes, which indeed improves over the baseline sometimes, \eg, R2 is no longer biased to smoke. However, its assumption has no guarantee to hold and hence the failure on subtle anomalies persists, \eg, false alarm in R1 white triangle location and low prediction in R3 black triangle location.
3) In contrast, our UMIL localizes the anomalies accurately in U1-U3, \eg, having consistently high scores in the pink areas, which holds its ground on the name ``unbiased''.
4) In the 4th video, however, RTFM's prediction in the pink area is more consistent than ours. By inspecting the frames on the left, we realize that the two peaks in the pink area of U4 correspond to the burning fire and the running suspect caught on fire. Hence UMIL's prediction is reasonable and sufficient for triggering the alarm on the first peak.

\noindent\textbf{Computational Efficiency}. Lastly, we investigated the speed of the proposed model.
For inference, our method processes a 5-frame clip in $0.003$ seconds on a Nvidia 2080Ti GPU.
Notably, this is almost $80 \times$ faster than the SOTA RTFM~\cite{tian2021weakly}, which spends 0.76 seconds to process a 16-frame clip on Nvidia 2080Ti.
Thanks to our unbiased training scheme, we can fine-tune the backbone to learn a WSVAD-tailored representation, which achieves even better performance than existing SOTA.
This also shows the promising future of UMIL in real-time applications.