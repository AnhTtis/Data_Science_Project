% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[letterpaper, 10 pt, conference]{ieeeconf} % for initial submission

\usepackage{stfloats}
\usepackage{makecell}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{multirow}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    breaklinks=true,
    citecolor=green
    }


% Commands from IEEE templates
\usepackage{balance}
\usepackage{cite}
\usepackage[caption=false,font=footnotesize,listofformat=subsimple,labelformat=simple]{subfig}
% \usepackage{array}
% \usepackage{textcomp}
% \usepackage{url}
% \usepackage{verbatim}

\def\equationautorefname~#1\null{Eq.~(#1)\null}
\def\sectionautorefname{Sec.}
\def\subsectionautorefname{Sec.}
\def\chapterautorefname{Ch.}
\def\figureautorefname{Fig.}
\renewcommand\thesubfigure{(\alph{subfigure})}
\providecommand\subfigureautorefname{Fig.}

\IEEEoverridecommandlockouts 
\overrideIEEEmargins  

\begin{document}

% for final submission
% \title{Learning from Few Demonstrations with \\ Frame-Weighted Motion Generation}

\title{\LARGE \bf Learning from Few Demonstrations with \\ Frame-Weighted Motion Generation}

\author{Jianyong Sun$^{1}$, Jihong Zhu$^{1,2}$, Jens Kober$^{1}$, and Michael Gienger$^{3}$% <-this % stops a space
\thanks{$^{1}$J. Sun, J. Zhu and J. Kober are with Cognitive Robotics, 3mE, Delft University of Technology, 2628CD Delft, The Netherlands \tt\footnotesize sunjy711@gmail.com, \{J.Zhu-3,J.Kober\}@tudelft.nl}
\thanks{$^{2}$J. Zhu is with the School of Physics, Engineering and Technology, University of York, Heslington, York, YO10 5DD, United Kingdom \tt\footnotesize jihong.zhu@york.ac.uk}
\thanks{$^{3}$M. Gienger is with the Honda Research Institute Europe, 63073 Offenbach/Main, Germany \tt\footnotesize michael.gienger@honda-ri.de}
\thanks{The video of experiments is available at \href{https://youtu.be/JpGjk4eKC3o}{https://youtu.be/JpGjk4eKC3o}.}
}

\maketitle
\begin{abstract}
Learning from Demonstration (LfD) aims to encode versatile skills from human demonstrations. The field has been gaining popularity since it facilitates knowledge transfer to robots without requiring expert knowledge in robotics. During task executions, the robot motion is usually influenced by constraints imposed by environments. In light of this, task-parameterized LfD (TP-LfD) encodes relevant contextual information in reference frames, enabling better skill generalization to new situations. However, most TP-LfD algorithms require multiple demonstrations in various environment conditions to ensure sufficient statistics for a meaningful model. It is not a trivial task for robot users to create different situations and perform demonstrations under all of them. Therefore, this paper presents a novel concept for learning motion policies from few demonstrations by finding the reference frame weights which capture frame importance/relevance during task executions. Experimental results in both simulation and real robotic environments validate our approach.
\end{abstract}

% only for final submission, refer to https://www.ieee-ras.org/publications/ra-l/keywords
% \begin{IEEEkeywords}
% Article submission, IEEE, IEEEtran, journal, \LaTeX, paper, template, typesetting.
% \end{IEEEkeywords}


\section{Introduction} \label{sec: introduction}
Compared to industrial robots that perform repetitive tasks in well-defined working situations, service robots are expected to be dexterous and intelligent to work in unstructured environments, acquire skills quickly, and adapt to unseen scenarios easily. However, traditional robot programming requires expertise to program robot motions, which leads to the lower popularity of robots in society and poor adaptability to various task conditions.

Inspired by the process that humans learn skills by imitating others, Learning from Demonstration (LfD) enables robots to acquire versatile skills by learning motion policies from expert demonstrations. It endows users with an intuitive interface to transfer new skills to robots without the need for time-consuming robot programming and inefficient solution exploration \cite{ravichandar_recent_2020}. To improve the generalization ability of robot learners, \cite{calinon_statistical_2012} proposes the Task-Parameterized LfD (TP-LfD) method by parameterizing the description of task situations into reference frames. Nevertheless, to obtain enough training data, most existing TP learning methods require collecting multiple diverse demonstrations under different environment conditions. This is often not feasible in practice due to time and space limitations, especially for complex tasks. With limited demonstrations, these methods cannot extract enough information from training data, hence showing poor generalization performance.

In this paper, we focus on this challenge and present the following contributions:

\begin{itemize}
    \item We propose a novel motion generation method based on the relevance weights of reference frames that can learn from few demonstrations;
    \item Our method can generate synthetic data to augment the original training dataset, achieving policy improvements for traditional TP learning methods;
    \item Both simulation and real robotic experiments validate that our method demonstrates competitive generalization ability, especially under extrapolation conditions.
\end{itemize}

The remainder of the paper is organized as follows. In \autoref{sec: related work}, we first review related work on improving generalization and data efficiency in LfD, as well as on calculating the relevance of reference frames. We then present the problem statement in \autoref{sec: problem statement}. In \autoref{sec: methods}, We provide a detailed description of the method, which is then validated through simulation in \autoref{sec: simulation} and robotic experiments in \autoref{sec: robotic experiments}. Finally, we conclude in \autoref{sec: conclusion}.

\section{Related Work} \label{sec: related work}
\textbf{Generalization in LfD:} The generalization ability is considered as one of the central properties of robot learning algorithms. Traditional movement encoding methods, such as Dynamic Movement Primitive (DMP) \cite{ijspeert_dynamical_2013}, Probabilistic Movement Primitive (ProMP) \cite{paraschos_probabilistic_2013}, and Gaussian Mixture Model (GMM) \cite{calinon_learning_2007}, can only model the trajectories demonstrated in the same situations, hence exhibiting limited extrapolation ability. To improve that, many LfD algorithms integrate these methods with task parameters that can describe task situations \cite{forte_-line_2012, pervez_learning_2018, osa_guiding_2017, calinon_tutorial_2016}. Since task parameters encode relevant contextual information about the task, these algorithms usually allow the automatic adaptation of learned skills to various situations, showing better extrapolation performance. 

Most of these methods formulate the motion retrieval from task parameters as a regression problem. In \cite{forte_-line_2012}, Gaussian Process Regression (GPR) is used to model the mapping from task parameters to DMP parameters which represent robot trajectories compactly. Similarly, \cite{pervez_learning_2018} presents how to utilize the mixture of GMMs to model the joint distribution of task parameters and nonlinear forcing terms in DMP. \cite{osa_guiding_2017} applies Locally Weighted Regression (LWR) to model the conditional distribution of the demonstrated trajectories with respect to the demonstration contexts.

Using local reference frames as task parameters, task-parameterized Gaussian Mixture Model (TP-GMM) achieves better generalization performance \cite{calinon_tutorial_2016}. The approach first projects the global trajectories into different local reference frames, then encodes projected trajectories using GMMs, and finally generalizes to new situations through the fusion of trajectory distributions in each reference frame. While these methods utilizing task parameters perform better in generalization compared with traditional LfD algorithms, most of them still require a number of demonstrations which are not always easy to collect. Instead, our proposed method requires only few demonstrations.

\textbf{Data efficiency in LfD:} Since data efficiency is important for LfD algorithms, several methods have been proposed to decrease the dependence on the amount of data. \cite{antotsiou_adversarial_2021} uses Corrected Augmentation for Trajectories (CAT) to correct distorted expert actions, which can augment original expert demonstrations. The routine-augmented policy learning (RAPL) framework proposed by \cite{zhao_augmenting_2021} discovers routines from a single demonstration and uses them to augment policy learning. \cite{zhu_learning_2022} augments the original demonstrations using synthetic demonstration, noise injection, and a combination of both. As a noise injection variant of Behavior Cloning (BC), Disturbances for Augmenting Robot Trajectories (DART) provides a broader set of state-action pairs by adding noise during demonstration collection phases \cite{laskey_dart_2017}. Instead of augmenting the original training dataset, our proposed algorithm learns motion policies directly from the existing few demonstrations.


\textbf{Frame relevance calculation:} The robot movements are usually modulated by a set of local reference frames with different individual relevance to the task. Some existing frame relevance-solving schemes build on TP-GMM. In \cite{alizadeh_learning_2014}, the frame importance is computed by the ratio of the precision matrix determinant relative to the other frames. \cite{alizadeh_identifying_2016} and \cite{alizadeh_robot_2017} utilize this definition to identify relevant frames to the task. Instead of solving frame weights from data, \cite{huang_generalized_2018} proposes using a confidence weight to directly modify the covariance matrix of each local model. But its performance is limited by the reliance on the human prior information. By combining the two schemes introduced in \cite{alizadeh_learning_2014} and \cite{huang_generalized_2018}, \cite{sena_improving_2019} utilizes the norm of the covariance matrix of demonstrated data points and modifies it using a shape parameter during frame relevance calculation. However, these methods still have a strong dependence on the amount of data available for training. In contrast, our proposed method has the advantage of being able to calculate frame relevance weights even with a limited number of demonstrations.

%% figures in the method section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/pose_difference_legend.png}

    \vspace{-0.35cm}
    \subfloat[Different target positions]{\includegraphics[width=0.23\textwidth]{images/position_difference.png}
    \label{fig: position difference}}
    \hfil
    \subfloat[Different target orientations]{\includegraphics[width=0.23\textwidth]{images/orientation_difference.png}
    \label{fig: orientation difference}}
    
    \caption{A simulated movement task from the start (the purple U-shape box, frame $\{1\}$) to the target (the blue U-shape box, frame $\{2\}$). The shape of boxes imposes geometric constraints on the starting and ending portions of the motion. Two demonstrations are presented and the red dots indicate the corresponding points between them.}
    \label{fig: relation between demos}
\end{figure}

% the width is set as 0.1355 with seven subplots
\begin{figure*}[!t]
    \centering
    \subfloat[]{\includegraphics[width=0.19\textwidth]{images/method_step0.png}
    \label{fig: method step 0}}
    \hfil
    \subfloat[]{\includegraphics[width=0.19\textwidth]{images/method_step1.png}
    \label{fig: method step 1}}
    \hfil
    \subfloat[]{\includegraphics[width=0.19\textwidth]{images/method_step2.png}
    \label{fig: method step 2}}
    \hfil
    \subfloat[]{\includegraphics[width=0.19\textwidth]{images/method_step3.png}
    \label{fig: method step 3}}
    \hfil
    \subfloat[]{\includegraphics[width=0.19\textwidth]{images/method_step4.png}
    \label{fig: method step 4}}
    
    \caption{Illustration of reference trajectory transformation using frame relevance weights. (a) Motion generation in a new situation through transforming a demonstrated trajectory. (b) Projection of frames $\{2\}$ and the demonstration into corresponding frames $\{1\}$. (c) Frame-weighted translation along the position difference vector between the frame $\{2\}$ associated with the demonstration and the new frame $\{2\}$. (d) Frame-weighted rotation around the origin of the new frame $\{2\}$. (e) Projection back to the global coordinate system.}
    \label{fig: method steps}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem Statement} \label{sec: problem statement}
As introduced in \cite{huang_generalized_2018}, the relevance of reference frames may vary over the course of the task trajectory, which can be interpreted as the influence of a specific frame on a portion of the task. Taking the grasping task as an example, as the robot gripper approaches the target, its motion is more influenced by the reference frame attached to the target, while the frame at its start location becomes less important.

A key step in the proposed method is to determine the frame relevance weights along the task trajectory when only a limited number of demonstrations are available. In the following, we start with the task with two reference frames involved. However, our method is not limited to this. In 
\autoref{sec: multi-frame task solution}, we further describe how to apply it to more complex tasks involving more than two reference frames.

We consider two local reference frames and employ the rotation matrix $\pmb A_{j} \in SO(p)$ and translation vector $\pmb b_{j} \in \mathbb{R}^p$ of each frame $\{ j \}$ with respect to the global coordinate system as task parameters, where $j = 1, 2$ and $p$ is 2 or 3 depending on the dimensionality required for the task.

The training dataset $\{\{\pmb \xi_{t, m}\}_{t=0}^{T_m}\}_{m=1}^M$ consists of $M$ demonstrations, each with the time length $T_m$. Their associated reference frames are denoted by $\{\{ \pmb A_{j, m}, \pmb b_{j, m}\}_{j=1}^2 \}_{m=1}^M$. For the $m$-th demonstration, $\pmb \xi_{t, m} \in \mathbb{R}^{p}$ represents a $p$-dimensional trajectory point recorded at the time step $t$. 

Since the task frame relevance varies along a trajectory, we need to first define a progress scalar to describe the motion process. In our case, we use the relative distance index $d_{t, m}$ of each data point $\pmb \xi_{t, m}$
\begin{equation} \label{eq: dist}
d_{t, m} = \frac{\sum_{i=0}^{t-1} \lVert \pmb \xi_{i, m} - \pmb \xi_{i+1, m} \rVert }{\sum_{i=0}^{T_m-1} \lVert \pmb \xi_{i, m} - \pmb \xi_{i+1, m} \rVert },
\end{equation}
where $\lVert \cdot \rVert$ represents the Euclidean distance between two consecutive trajectory points. Along a trajectory, the index $d_{t, m}$ starts from 0 and reaches 1 at the end. To establish the correspondence between trajectories, we define the corresponding points as those with the same relative distance index.

We then define the relevance weights $f_j(d) \in (0, 1)$ of frame $\{j\}$ as a weighted summation of $Q$ numbers of Radial Basis Functions (RBFs)
\begin{equation} \label{eq: initial weight definition}
    f_j(d) = \pmb{\Phi}^T \pmb{\omega}_j,
\end{equation}
where the vector $\pmb{\Phi}$ is defined as $[\phi_1(d), \dots, \phi_Q(d)]^T$ whose entry denotes a RBF regarding the relative distance index $d$ and $\pmb{\omega}_j \in \mathbb{R}^Q$ is the basis function weight vector. By doing so, we transform the problem of finding frame relevance weights as the solution of basis function weights.

Inspired by the calculation of frame relevance weights introduced in \cite{sena_improving_2019}, we formulate the solution of basis function weights $\pmb{\omega}_j$ for frame $\{j\}$ as an optimization problem
\begin{equation} \label{eq: initial optimization}
\begin{aligned}
    \min_{\pmb{\omega}_j} \quad &  \mathcal{J} (f_j)\\
    \textrm{s.t.} \quad & 0 \leq f_j \leq 1 ,
\end{aligned}
\end{equation}
where $\mathcal{J}$ is an objective function with respect to frame relevance weights. So now the primary challenge is to find an objective function $\mathcal{J}$ that can effectively utilize the limited demonstrations available.

\section{Methods} \label{sec: methods}
To facilitate the explanation of the method, we will introduce it in the context of a two-frame task, where the objective is moving from one U-shape box to the other without any collisions with either of them, as depicted in \autoref{fig: relation between demos}. Taking frame $\{2\}$ as an example, this section introduces how to solve for the relevance weights of frame $\{2\}$ through optimization and how to use them for skill generalization. The same approach applies to frame $\{1\}$ as well.

We first introduce how to utilize frame relevance weights for the description of the relation between demonstrations in \autoref{sec: relation between demos} and reference trajectory transformation in \autoref{sec: reference trajectory transformation}. We then present the construction of the objective function $\mathcal{J}$ in \autoref{eq: initial optimization} and the solution of the optimization problem in \autoref{sec: frame relevance weights optimization}. In \autoref{sec: skill generalization}, we explain the formation of the frame-weighted motion generation method. Finally, we demonstrate how to apply our method to more complex tasks with more than two reference frames involved in \autoref{sec: multi-frame task solution}.

\subsection{Relation Between Demonstrations} \label{sec: relation between demos}
To explain how to describe the relation between demonstrations using frame relevance weights, we show how the task trajectories are influenced by different target positions and orientations in \autoref{fig: position difference} and \autoref{fig: orientation difference}, respectively. Displayed as red dots, the corresponding points $\pmb \xi_{t_1, 1}$ and $\pmb \xi_{t_2, 2}$ are found with the same relative distance index, i.e., $d_{t_1, 1} = d_{t_2, 2}$ (see \autoref{eq: dist} for the definition of $d$).

With the observation that the robot trajectory is modulated by reference frames in mind, in ideal conditions, we can represent the relation between two demonstrations in \autoref{fig: position difference} and \autoref{fig: orientation difference} separately as
\begin{equation} \label{eq: relation between demos}
    \pmb \xi_{t_2, 2} - \pmb \xi_{t_1, 1}  =  f_2(d_{t_1, 1})\, \pmb v, \qquad \beta = f_2(d_{t_1, 1}) \, \alpha,
\end{equation}
where $f_2(d_{t_1, 1})$ represents the relevance weight of frame $\{2\}$ when the relative distance index is $d_{t_1, 1}$. $\pmb v$ and $\alpha$ are respectively the position and angle difference between frames \{2\} in two different situations. Initially, the robot motion is fully constrained by the starting box, resulting in a relevance weight close to 0. As the robot moves towards the target, the influence of frame ${2}$ becomes stronger, leading to a gradual increase in the relevance weight. Finally, it approaches 1 due to the full constraint of the target box.

\subsection{Reference Trajectory Transformation} \label{sec: reference trajectory transformation}
Since \autoref{eq: relation between demos} indicates that we can establish the relation between two demonstrations using frame relevance weights, it follows naturally that we can also 
take a demonstration as the reference trajectory and transform it for motion generation in a new situation. As shown in \autoref{fig: method step 0}, we express the demonstration as $\{\pmb \xi_{t}\}_{t=0}^{T}$ and its associated reference frame as $\{ \pmb A_{j}, \pmb b_{j}\}_{j=1}^2$, and define the new situation using reference frames $\{ \hat{\pmb A}_{j}, \hat{\pmb b}_{j}\}_{j=1}^2$.

\textit{Step 1} - as shown in \autoref{fig: method step 1}, we first project frames $\{2\}$ and the demonstration from the global coordinate system into corresponding frames $\{1\}$ as follows
\begin{align}
    \pmb \xi_{t}^{(1)} & = \pmb A_{1}^{-1}(\pmb \xi_t - \pmb b_{1}),& \\
    \pmb b_{2}^{(1)} & =  \pmb A_{1}^{-1}(\pmb b_{2} - \pmb b_{1}),& \pmb A_{2}^{(1)} & = \pmb A_{1}^{-1} \pmb A_{2},\\
    \hat{\pmb b}_{2}^{(1)} & =  \hat{\pmb A}_{1}^{-1}(\hat{\pmb b}_{2} - \hat{\pmb b}_{1}),& \hat{\pmb A}_{2}^{(1)} & = \hat{\pmb A}_{1}^{-1} \hat{\pmb A}_{2}.
\end{align}
Observed from the local coordinate frame $\{1\}$, the situation in which the demonstration is performed and the new situation only differ in the pose of frame $\{2\}$. By doing so, we only need to consider the influence of frame $\{2\}$ on the reference trajectory transformation. In other words, we will not utilize the relevance weights of frame $\{1\}$ and we can later construct an objective function that only depends on frame $\{2\}$ weights.

% the position difference between the frame {2} associated with the demonstration and the new frame {2}
\textit{Step 2} - as shown in \autoref{fig: method step 2}, we perform a frame-weighted translation transformation on the reference trajectory along the vector that represents the position difference between frames $\{2\}$. Multiplying that by the corresponding frame relevance weight, we can transform each data point $\pmb \xi_{t}^{(1)}$ as
\begin{equation} \label{eq: translation}
    \tilde{\pmb \xi}_{t}^{(1)} =  \pmb \xi_{t}^{(1)} + f_2(d_{t}) \, (\hat{\pmb b}_{2}^{(1)} - \pmb b_{2}^{(1)}),
\end{equation}
where $\tilde{\pmb \xi}_{t}^{(1)}$ is the estimated data point based on the frame position difference only, $d_t$ is the relative distance index of the sample $\pmb \xi_{t}^{(1)}$, and $f_2(d_{t})$ represents the relevance weight of frame $\{2\}$ for this sample.

\textit{Step 3} - as shown in \autoref{fig: method step 3}, we then further transform $\tilde{\pmb \xi}_{t}^{(1)}$ based on the orientation difference between frames $\{2\}$. We calculate the orientation of new frame $\{2\}$ relative to the frame $\{2\}$ associated with the demonstration through the matrix operation $\hat{\pmb A}_{2}^{(1)} (\pmb A_{2}^{(1)})^{-1}$ and represent it as a rotation vector $\theta \, \pmb u$, where $\theta$ is the rotation angle and the unit vector $\pmb u$ indicates the rotation axis. Multiplying the angle $\theta$ with the frame relevance weight $f_2(d_{t})$, we obtain the required rotation vector as $f_2(d_{t}) \, \theta \, \pmb u$. After converting it back to the rotation matrix $\pmb T_t$, we get the final estimated data point as
\begin{equation}  \label{eq: rotation}
   \hat{\pmb \xi}_{t}^{(1)} =  \pmb T_t (\tilde{\pmb \xi}_{t}^{(1)} - \hat{\pmb b}_{2}^{(1)})+ \hat{\pmb b}_{2}^{(1)},
\end{equation}
where we rotate the previous estimated point $\tilde{\pmb \xi}_{t}^{(1)}$ around the origin of new frame $\{2\}$, thus aligning trajectory points with its orientation.

\textit{Step 4} - as shown in \autoref{fig: method step 4}, we finally obtain the desired trajectory $\{ \hat{\pmb \xi}_{t}\}_{t=0}^{T}$ in the new situation by projecting the local estimated point $\hat{\pmb \xi}_{t}^{(1)}$ back to the global coordinate system as
\begin{equation}
    \hat{\pmb \xi}_{t} =  \hat{\pmb A}_1 \hat{\pmb \xi}_t^{(1)} + \hat{\pmb b}_{1}.
\end{equation}

\subsection{Frame Relevance Weights Optimization} \label{sec: frame relevance weights optimization}
In this section, we present how to obtain optimized frame relevance weights by solving the optimization problem defined in \autoref{eq: initial optimization}. Since we assume the task trajectories in the same situation should be similar, a suitable objective function $\mathcal{J}$ can be defined between the expert demonstrations and synthetic trajectories through reference trajectory transformation that aim to reproduce the expert data. 

We first define the dissimilarity between a demonstrated trajectory $\pmb X$ and its reproduction $\hat{\pmb X}$ as their normalized distance computed by dynamic time warping (DTW) (see \cite{muller_dynamic_2007} for details)
\begin{equation}
    L(\pmb X, \, \hat{\pmb X}) = \min_\psi l_\psi(\pmb X, \, \hat{\pmb X}),
\end{equation}
where $\psi$ is the warping function which maps the data point in $\pmb X$ to $\hat{\pmb X}$ and $l_\psi$ calculates the average accumulated dissimilarity between warped $\pmb X$ and $\hat{\pmb X}$.

We then reformulate the optimization problem in \autoref{eq: initial optimization} as follows
\begin{equation} \label{eq: final optimization}
\begin{aligned}
    \min_{\pmb{\omega}_2} \quad &  \frac{1}{M^2} \sum_{i=1}^{M} \sum_{k=1}^{M} L( \pmb X_i, \ \hat{\pmb X}_{i, k})\\
    \textrm{s.t.} \quad & 0 \leq f_2 \leq 1 ,
\end{aligned}
\end{equation}
where we find the optimized basis function weights for frame relevance weights through achieving the best match between demonstrated trajectories and their reproductions. With $M$ demonstrations in the training dataset, we can find at most $M^2$ pairs of demonstrations for trajectory reproduction. For each pair $\{\pmb X_i, \pmb X_k\}$, $\hat{\pmb X}_{i, k}$ is the reproduction of $\pmb X_i$ from $\pmb X_k$. Several popular optimization algorithms can be used to solve this problem, such as Sequential Least Squares Programming (SLSQP).

\subsection{Skill Generalization} \label{sec: skill generalization}
Once we have determined the optimized frame relevance weights, we can utilize them for the generalization in a new situation through the reference trajectory transformation described in \autoref{sec: reference trajectory transformation}, which forms the frame-weighted motion generation method. The generated trajectory is deterministic for a given situation.

For some robotic tasks, such as human-robot interaction scenarios, we may require the variance information to evaluate the path uncertainty, which allows the adjustment of robot stiffness for safe and effective operation. In this case, the frame-weighted motion generation method can be used to augment the original training dataset for probabilistic learning models, which we will showcase using TP-GMM.

\subsection{Multi-Frame Task Solution} \label{sec: multi-frame task solution}
To extend the application of the proposed method to more complex tasks that involve multiple reference frames, we can divide the whole task into a sequence of two-frame subtasks based on our prior knowledge. Note, the demonstrations are still collected for the entire task, and we segment them into individual trajectories modulated by only two reference frames. After employing our method for each movement segment, we can connect the generalized trajectories of each subtask to form a complete one for the whole task.

Various priors can help achieve the segmentation of demonstrations. As described in \cite{kober_learning_2015}, getting into contact with environment or an object can signify transitions within a sequence of movements. So we can detect segmentation points based on the change of contact force between the robot and manipulated objects. If the interaction force is unavailable, the recorded motion information can also be used for segmentation. For example, a sudden shift in the distance to objects \cite{franzese_learning_2020}  or the velocity becoming zero usually indicate the segmentation point.



\section{Simulation} \label{sec: simulation}

\begin{figure}[!t]
    \centering
    \subfloat[]{\includegraphics[width=0.23\textwidth]{images/sim_demos.png}
    \label{fig: simulation demos}}
    \hfil
    \subfloat[]{\includegraphics[width=0.23\textwidth]{images/sim_weight.png}
    \label{fig: simulation weight}}
    
    \caption{(a) The training (red) and validation (green) set for the simulated movement task. Two reference frames are attached to the purple (frame $\{1\}$) and blue (frame $\{2\}$) box, respectively. (b) The optimized relevance weights of frame $\{2\}$ (magenta solid line) with RBFs (dashed lines).}
    \label{fig: simulation demos and weight}
\end{figure}

In this section, the proposed approach is evaluated on the simulated movement task shown in \autoref{fig: relation between demos}. We collect in total six expert demonstrations with different target poses, as shown in \autoref{fig: simulation demos}. Two of them are used for training (marked with red), while the remaining four are used for validation (marked with green).

To approximate the frame weight function defined in \autoref{eq: initial weight definition}, we use 10 RBFs whose centers are uniformly distributed between 0 and 1. And their spreads are all set to be 5. The optimized frame weights are depicted in \autoref{fig: simulation weight}. The result is consistent with our intuitive understanding. The starting part of the trajectory is fully constrained by the purple box, so the relevance weight of frame $\{2\}$ is almost 0. As the relative distance increases, the influence of frame $\{2\}$ becomes higher since the trajectory points become closer to the blue box. Finally, when the trajectory reaches the blue box, the weight is close to 1 because of the geometric constraint of the blue box on the trajectory.

\autoref{fig: simulation validation} displays the generalization performance of our proposed frame-weighted motion generation method in new situations. In \autoref{fig: simulation extrapolation nearby}, we can observe that the generalized trajectories in the validation set are very similar to the expert demonstrations, which indicates the validity of the calculated frame weights. To further demonstrate the generalization capability of our method, we create 4 novel situations which differ significantly from that of training demonstrations, as shown in \autoref{fig: simulation extrapolation far}. In all cases, the generated trajectories are successful in meeting the local constraints of boxes while not displaying many undesired movements. 

In a nutshell, by utilizing the frame relevance weights to adapt reference trajectories to new situations, the frame-weighted motion generation method shows good generalization performance, even though the target situations have a significant difference from those demonstrated.

\begin{figure}[!t]
    \centering
    \subfloat[]{\includegraphics[width=0.23\textwidth]{images/sim_fwbil_validation.png}
    \label{fig: simulation extrapolation nearby}}
    \hfil
    \subfloat[]{\includegraphics[width=0.23\textwidth]{images/sim_fwbil_extrapolation.png}
     \label{fig: simulation extrapolation far}}
    
    \caption{The generalization performance in novel situations. (a) Generalized trajectories (in green) compared with expert demonstrations (in red) in the validation set. (b) Generalizations (in green) in 4 new situations that are far from the regions covered by the training demonstrations whose associated boxes are displayed with higher transparency.}
    \label{fig: simulation validation}
\end{figure}


\section{Robotic Experiments} \label{sec: robotic experiments}
This section describes three real robot experiments conducted to demonstrate the ability of our proposed method to learn skills from few demonstrations.

\subsection{Generalization Performance}
The first two experiments focus on investigating the generalization performance of the frame-weighted motion generalization method through comparison with the other two learning methods.

\begin{figure*}[!t]
    \centering
    \subfloat[]{\includegraphics[width=0.32\textwidth]{images/bimanual_setup.png}
    \label{fig: bimanual setup}}
    \hfil
    \subfloat[]{\includegraphics[width=0.16\textwidth]{images/roller_initial.png}
    \label{fig: roller initial}}
    \subfloat[]{\includegraphics[width=0.16\textwidth]{images/roller_final.png}
    \label{fig: roller final}}
    \hfil
    \subfloat[]{\includegraphics[width=0.16\textwidth]{images/flower_initial.png}
    \label{fig: flower initial}}
    \hfil
    \subfloat[]{\includegraphics[width=0.16\textwidth]{images/flower_final.png}
    \label{fig: flower final}}
    
    \caption{The bimanual setup of robot experiments. (a) shows two Franka Emika Panda robots on a table. The coordinate systems of robot bases are depicted with red, green, and blue (RGB) arrows. (b) and (c) display the initial and final state of the roller-on-holder task, while (d) and (e) present the initial and final state of the flower-in-vase task.}
    \label{fig: experiments}
\end{figure*}


\subsubsection{Experimental Setup} \label{sec: two frame setup}
\begin{table*}[t] 
    \centering
    \caption{Training and Validation Errors of Three Motion Generalization Methods for Two Real Robotic Tasks}
    \begin{tabular}{c|c|c|c|c|c|c}
        \hline
        & \multicolumn{3}{c}{Roller-on-holder task} & \multicolumn{3}{|c}{Flower-in-vase task}\\
        \cline{2-7}
        & \makecell[c]{Frame-weighted \\ motion generation} & \makecell[c]{Augmented \\ TP-GMM} & TP-GMM & \makecell[c]{Frame-weighted \\ motion generation} & \makecell[c]{Augmented \\ TP-GMM} & TP-GMM \\
        \hline
        Training error & \textcolor[rgb]{0, 0.8, 0}{0.0151 (52\%)} & \textcolor[rgb]{0.9, 0, 0}{0.0393 (134\%)} & 0.0293 (100\%) & \textcolor[rgb]{0, 0.8, 0}{0.0214 (50\%)} & \textcolor[rgb]{0.9, 0, 0}{0.0591 (138\%)} & 0.0427 (100\%)\\
        \hline
        Validation error & \textcolor[rgb]{0, 0.8, 0}{0.0805 (12\%)} & \textcolor[rgb]{0, 0.8, 0}{0.1687 (25\%)} & 0.6657 (100\%) & \textcolor[rgb]{0, 0.8, 0}{0.0786 (11\%)} & \textcolor[rgb]{0, 0.8, 0}{0.1639 (23\%)} & 0.7062 (100\%)\\
        \hline
    \end{tabular}
    \label{tab: exp comparison}
\end{table*}

For the experimental platform, we use two 7 DoF Franka Emika Panda placed vertically on a table in the same base orientation, see \autoref{fig: bimanual setup}. Their mutual poses can be calculated by locating the same object from two robot base frames and finding the optimal rigid transformation between two point sets \cite{arun_least-squares_1987}. The impedance controllers are deployed on both manipulators. 

We perform two experiments with the real robot setup: \textit{i}) a roller-on-holder task, where the right robot picks the paper roller from the holder on the table and places it on the other holder mounted on the left robot, see \autoref{fig: roller initial} and \autoref{fig: roller final}, \textit{ii}) a flower-in-vase task, where the right robot grabs the flower in the vase that vertically stands on the table and moves it to the other vase grasped by the left robot, see \autoref{fig: flower initial} and \autoref{fig: flower final}. 

For both experiments, the left robot provides the target pose while the right one performs the task. During the demonstration collection, we record the end-effector poses of both robots. The end-effector pose of the left robot is used for calculating the pose of the ending holder or vase, while that of the right robot provides demonstrated trajectories. The pose of the starting holder or vase can be measured using the right robot in advance.

During task executions, we employ Spherical Linear Interpolation (Slerp) to enable the right robot to automatically align its end-effector orientation with the target \cite{shoemake_animating_1985}. Slerp involves interpolating between two quaternions that describe the orientations of the end-effector and target, respectively. By doing so, we focus on learning the path of the right robot.


The situation of both tasks can be fully described by two reference frames attached to starting and ending holder or vase, i.e., $\{ \pmb A_s, \pmb b_s \}$ and $\{ \pmb A_e, \pmb b_e \}$, where $\pmb b_s$ and $\pmb b_e$ represent the bottom center positions of the holder or vase and the $z$ axes of $\pmb A_s$ and $\pmb A_e$ are aligned with their orientations. 

For each task, we use 2 and 4 expert demonstrations as the training and validation dataset, respectively. We conduct a comprehensive evaluation of our proposed frame-weighted motion generation method on skill generalization by comparing it with TP-GMM and augmented TP-GMM \cite{calinon_tutorial_2016}. The latter trains TP-GMM on a training dataset that is augmented using our method. As a data preprocessing step, the training trajectories need to be temporally aligned using DTW \cite{calinon_learning_2007}. For both experiments, we set the number of Gaussian components of TP-GMM as 6 and augment the original training dataset to 9 demonstrations for augmented TP-GMM.

% Since the collected demonstrations are time-indexed trajectories, the reference frames used for TP-GMM training need to be accordingly augmented with a time dimension as follows:

% \begin{equation}
%     \pmb A' = 
%     \begin{bmatrix}
%     1 & \pmb 0 \\
%     \pmb 0 & \pmb A
%     \end{bmatrix}
%     ,\quad
%     \pmb b' = 
%     \begin{bmatrix}
%     0\\
%     \pmb b
%     \end{bmatrix}.
% \end{equation}


To quantitatively compare three motion generalization methods, we use the DTW normalized distance as the similarity measure between two trajectories and define the model error as follows:

\begin{equation} \label{eq: error}
    \mathcal{J}_\mathrm{DTW} = \frac{1}{\mu} \sum_{i=1}^\mu L (\pmb X, \hat{\pmb X} ), 
\end{equation}
where $\pmb X$ is the demonstrated trajectory, $\hat{\pmb X}$ is the trajectory generated by models, and $\mu$ is the number of trials for generalization (2 for training and 4 for validation).



\subsubsection{Results} \label{sec: two frame results}

\begin{figure}[!t]
    \centering
    \rotatebox{90}{\scriptsize new situation 1}
    \includegraphics[width=0.149\textwidth]{images/flower_fwbil_validation_1.png}
    \label{fig: flower fwbil validation 1}
    \hfil
    \includegraphics[width=0.149\textwidth]{images/flower_a-tpil_validation_1.png}
    \label{fig: flower a-tpil validation 1}
    \hfil
    \includegraphics[width=0.149\textwidth]{images/flower_tpil_validation_1.png}
    \label{fig: flower tpil validation 1}

    \vspace{-0.5cm}
    \rotatebox{90}{\scriptsize new situation 2}
    \subfloat[]{\includegraphics[width=0.149\textwidth]{images/flower_fwbil_validation_2.png}
    \label{fig: flower fwbil validation 2}}
    \hfil
    \subfloat[]{\includegraphics[width=0.149\textwidth]{images/flower_a-tpil_validation_2.png}
    \label{fig: flower a-tpil validation 2}}
    \hfil
    \subfloat[]{\includegraphics[width=0.149\textwidth]{images/flower_tpil_validation_2.png}
    \label{fig: flower tpil validation 2}}
    
    \caption{The generalization performance of three methods on the validation set in the flower-in-vase task. Each row displays a validation situation. The starting and ending vases are shown in cyan and red. The generalized trajectories and ground truths are marked with green and red. Column (a) Frame-weighted motion generation, (b) Augmented TP-GMM, (c) TP-GMM.}
    \label{fig: flower validation}
\end{figure}

\autoref{tab: exp comparison} displays the training and validation errors for both tasks. While the former indicates the model performance in performing reproduction under observed conditions, the latter represents their ability to generalize to unseen situations. We use the error of TP-GMM as the reference value and provide a percentage for each error value. The larger error is marked with red, and the smaller one with green. 


It can be seen that among the three methods tested, the frame-weighted motion generation method incurs the lowest error for both training and validation. This indicates that the frame relevance weight incorporating how the reference frames modulate task trajectories is one of the most important features for skill learning and that our proposed method is effective in determining optimized frame weights, even with only few demonstrations available.

Comparing the model error of TP-GMM and augmented TP-GMM, we observe that when the original training dataset is augmented with synthetic trajectories, the validation error decreases a lot, but the training error increases slightly. This means that TP-GMM trained with few demonstrations is over-fitting to the few and sparse training data, thus showing poor performance in terms of the generalization to new situations. In addition, it proves that our proposed method is valid in data augmentation.

Given additional 7 synthetic trajectories, the validation error of TP-GMM is still a little larger than frame-weighted motion generation. One possible reason is that all the augmented demonstrations are generated by transforming original training trajectories, so TP-GMM is likely to over-fit some certain features embodied by the reference trajectories.

Taking the flower-in-vase task as an example, \autoref{fig: flower validation} presents how these three motion generalization methods perform in two novel situations. With only two training demonstrations, TP-GMM does not obtain enough statistics to model the skill, thus generating meaningless trajectories, as shown in \autoref{fig: flower tpil validation 2}. When the original training dataset is augmented, the generalization performance of TP-GMM improves a lot. Although the two generated trajectories in \autoref{fig: flower a-tpil validation 2} still display some unwanted movements, they are successful in task executions since the geometric constraints of the vase have been satisfied. From \autoref{fig: flower fwbil validation 2}, we can intuitively observe how similar the generalized trajectory using the frame-weighted motion generation method is to the ground truth. While fulfilling the task constraints, the generated trajectories are much smoother.

\begin{figure}[!t]
    \centering
    \subfloat[]{\includegraphics[width=0.256\textwidth]{images/tpil_fwbil_more_demos.png}
    \label{fig: tpil and fwbil with more demos}}
    \hfil
    \subfloat[]{\includegraphics[width=0.204\textwidth]{images/aug_tpil_more_demos.png}
    \label{fig: augmented tpil with more augmentations}}
    
    \caption{The average validation error and the number of demonstrations in the flower-in-vase task. (a) More training demonstrations. (b) More augmentations produced by the frame-weighted motion generation method.}
    \label{fig: more demos}
\end{figure}

\autoref{fig: tpil and fwbil with more demos} displays how the average validation error changes with the increasing number of training demonstrations in the flower-in-vase task. Here, the average means that for each number, we select various combinations of demonstrations for training and take the mean of the resulting errors. It can be observed that as the number of training demonstrations increases, the error of the frame-weighted motion generation method remains almost unchanged, while the error of TP-GMM decreases a lot. This can be explained by the fact that, as an inherent feature of reference frames, the relevance weights do not change with the data size. And our proposed method can determine the optimized frame weights even if only limited data is available. The comparison indicates that our proposed method has less dependence on the amount of data, which further validates its ability to learn skills from few demonstrations.

To further demonstrate the validity of our proposed method in terms of data augmentation, \autoref{fig: augmented tpil with more augmentations} shows how the average validation error changes as the number of augmented demonstrations increases. Similarly, the average is calculated by setting multiple possible frame poses for augmentation and averaging the resulting errors. We can observe that the synthetic data generated by the frame-weighted motion generation method helps to significantly reduce the model error.

\subsection{Multi-Frame Task}
In this section, we take a three-frame task as an example to investigate the ability of the proposed method to deal with tasks with multiple reference frames involved.

\subsubsection{Experimental Setup} \label{sec: three frame setup}
In this experiment, we still use the experimental platform shown in \autoref{fig: bimanual setup}. As shown in \autoref{fig: exp three setup}, we perform a pen-to-box task where the right robot moves the orange pen out of one U-shape box on the table (frame $\{1\}$), into the box mounted on the left robot (frame $\{2\}$), and finally into another box on the table (frame $\{3\}$). The left robot provides the pose of the intermediate box, while the right robot can locate the other two boxes on the table. We collect a total of five demonstrations with various box poses. Two training demonstrations are shown in \autoref{fig: exp three training demos}. And the other three are used for validation.

\begin{figure}[!t]
    \centering
    \subfloat[]{\includegraphics[width=0.153\textwidth]{images/three_state_square.png}
    \label{fig: exp three setup}}
    \hfil
    \subfloat[]{\includegraphics[width=0.153\textwidth]{images/exp_three_training_demos.png}
    \label{fig: exp three training demos}}
    \hfil
    \subfloat[]{\includegraphics[width=0.153\textwidth]{images/exp_three_weights.png}
    \label{fig: exp three weights}}
    
    \caption{(a) The setup of the pen-to-box task. (b) Two training demonstrations in the pen-to-box experiment. The black dot indicates the segmentation of demonstrations. Three frames are attached to the purple (frame $\{1\}$), blue (frame $\{2\}$), and green (frame $\{3\}$) U-shape boxes, respectively. (c) The relevance weights of frame $\{2\}$ (blue) in the first subtask and that of frame $\{3\}$ (green) in the second subtask.}
    \label{fig: exp three training demos and weights}
\end{figure}

As we introduced in \autoref{sec: multi-frame task solution}, to apply our method to tasks involving more than two reference frames, we can segment the whole task into a sequence of two-frame subtasks. For the pen-to-box task, we achieve the segmentation at the point where the trajectory reaches the intermediate box (frame $\{2\}$). As shown in \autoref{fig: exp three training demos}, the trajectory point closest to the bottom of the blue box is treated as the segmentation point. By doing so, we can get two simpler pen-to-box tasks, each of which requires only two reference frames. The first subtask is to move from the purple box to the blue one, while the second is to move from the blue box to the green one. 

% Note that the way the demonstration is segmented is flexible as long as the situation of each subtask can be fully described by two reference frames. For example, in this task, we can also consider the point where the trajectory starts to enter the intermediate box as the segmentation point.

\subsubsection{Results} \label{sec: three frame results}

\begin{figure}[!t]
    \centering
    \subfloat{\includegraphics[width=0.153\textwidth]{images/exp_three_validation_0.png}
    \label{fig: exp three validation 0}}
    \hfil
    \subfloat{\includegraphics[width=0.153\textwidth]{images/exp_three_validation_6.png}
    \label{fig: exp three validation 6}}
    \hfil
    \subfloat{\includegraphics[width=0.153\textwidth]{images/exp_three_validation_7.png}
    \label{fig: exp three validation 7}}
    
    \caption{The generalization performance on the validation set in the pen-to-box task. The generalized trajectories and ground truths are shown in black and red, respectively.}
    \label{fig: exp three validation results}
\end{figure}

\autoref{fig: exp three weights} presents the optimized relevance weights of frame $\{2\}$ and $\{3\}$ in their respective subtasks. For the first subtask from the starting box to the intermediate one, the frame $\{2\}$ weight increases from 0 to 1. Similarly, for the second subtask from the intermediate box to the ending one, the frame $\{3\}$ weight also grows from 0 to 1. 

\autoref{fig: exp three validation results} shows the generalizations compared with expert demonstrations in the validation set. Although the trajectories generated in the two subtasks may not be connected smoothly at the segmentation point, the overall trajectory is very similar to the demonstrated one, which indicates good generalization capability.

The successful application of our proposed approach in this three-frame task demonstrates its ability to learn skills with multiple reference frames involved, thus improving its generality in terms of solving various complex tasks.




\section{Conclusion} \label{sec: conclusion}
Although task-parameterized learning in LfD achieves better generalization performance over non-parameterized alternatives, it requires multiple demonstrations collected in various situations which are not always available due to time and space limitations. This paper presents a method to learn skills from few demonstrations by building on the calculation of frame relevance weights. The method also allows the augmentation of the original training dataset, enabling better performance of TP-GMM. We validate the method using a simulation movement task and three real robot experiments.

We observe that the frame-weighted motion generation method achieves better generalization performance compared to TP-GMM when the training demonstrations are few and sparse. It also shows good extrapolation performance when the generalization situations have a significant difference from what is observed during training. In addition, the better performance of the augmented TP-GMM compared to the original one indicates that the frame-weighted motion generation method is valid in data augmentation.

Our proposed method is mainly limited by its ability to deal with the task whose situation needs to be described by more than two reference frames. The segmentation of demonstrations into movement segments introduces extra time and effort. In addition, it relies on our prior knowledge, which may cause some trouble if the robot user is not a robotics expert. Indeed, the automatic segmentation of human demonstrations has been well-studied and addressed \cite{ravichandar_recent_2020}. For example, \cite{niekum_learning_2012} proposes using a Beta-Process Auto-regressive Hidden Markov model to achieve automatic recognition of repeated skills and segmentation of whole demonstrations. However, these methods still require sufficient data to train the segmentation model, which is contrary to our idea of learning from few demonstrations. Therefore, future work would consider the application of our method to more complex tasks in a simpler way. 

\section*{Acknowledgment}
This work was supported by Honda Research Institute Europe GmbH as part of the project: Learning Physical Human-Robot Cooperation Tasks.


\balance
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,ref_manual}

\end{document}
