\section{Conclusion}

In this paper, we present a method to automatically animate the types of drawings created by children and amateur drawers. We also present a first-of-its-kind dataset of 178,166 in-the-wild drawings by children and amateurs, annotated with user-accepted bounding boxes, segmentation masks, and joint locations.

We demonstrate the value of our method in several ways.
First, we explore the accuracy and success from each stage of our system as a function of training dataset size.
Second, we perform a perceptual study to show the appeal of \textit{twisted-perspective} retargeting when animating these characters.
Third, we built and publicly released a usable version of the system which, within its first nine months, has been used to generate over 24 million animations from 6.7 million images uploaded by over 3.2 million users.

\hjs{
Prior to deciding to create a public-facing data collection tool, we unsuccessfully attempted to generate useful synthetic training data using generative adversarial networks\,\cite{pix2pix2017,CycleGAN2017,MultiModalPix2PixNIPS2017_6650}.
We believe our initial collection of less than 1,000 real children's drawings did not contain enough variation to cover the long-tail distribution of the domain. 
In addition, there were many sources of unanticipated nuisance variation that were not in our initial collection, yet present within in-the-wild drawings (e.g., messy backgrounds, lined paper, blurry shots, bad lighting, erased lines). 
It is possible that synthetic data approaches utilizing the entirety of the Amateur Drawing Dataset, which includes these variations, may have more success.

Ultimately, we pivoted to a bootstrapping approach to collect the data necessary to fine-tune our models. 
We manually annotated the images we had and trained initial models, then iteratively released closed beta versions of the demo, collected additional training data, and retrained the models.
By thoughtfully crafting the user experience, keeping prediction and render times short, and providing the user something of value (a downloadable animation of the drawing) in exchange for their efforts, we were able to collect enough real data from our target domain and no longer needed synthetic data.
We would encourage other researchers focused on user-generated content domains, for which there are not yet any suitable datasets, to likewise consider how they might invite their target audience into the dataset creation process, lowering the need to rely upon synthetic data.
}


















We believe this work is but a first step towards a robust and comprehensive drawing-to-animation storytelling system, and there are many ways our work could be improved.
One step that can clearly be improved is segmentation.
Extracting a usable and accurate mask can be quite difficult and, because it is used to create the character mesh, even small errors can result in bad animations.
There are many reasons why segmentation is hard to do accurately. 
The photograph of the drawing can be out of focus or distorted due to lighting glare or hard shadows.
Color and texture cues are not guaranteed to be helpful, as in the case of hollow characters.
A line can represent the edge of a body part region, or a line can represent the entire body part, such as with stick figures.
Often, characters are drawn on lined paper, the paper contains eraser marks, or background objects that touch the character are drawn with the same pencil or marker.
\hjs{If the character is drawn with limbs touching in places other than joints (as hands touch hips in the \textit{arms akimbo} pose, for example), there is no predicted segmentation mask that will result in a quality animation unless it is possible to add a segmentation differentiating between the two body parts.

Given the importance and difficulty of the segmentation task, methods that improve the robustness of the masking step would greatly increase the success of our pipeline.
A useful next step could be a principled method for choosing between the image processing and Mask R-CNN segmentation masks on a \textit{per image} basis, as each method can fail for different reasons. 
Ideally, such a method could leverage the bounding box and joint location predictions from the other stages of the pipeline.
}

In addition to improving the robustness of the current pipeline, future work should focus on extracting additional information about the drawing prior to animation. 
A natural next step would be to infer the sub-type of the human figure (e.g., robot, monster, snowman, princess).
Such analysis could be used to modify the pose estimation skeleton (e.g., removing the legs when a snowman has been identified)
or determine the types of animation to apply (e.g., making monsters stomp, princesses dance, or superheroes fly).
It could also be used to infer what different character regions represent.
For example, triangles on a cat's head are ears, while triangles on a devil's head are horns; these insights could affect how the characters are ultimately animated.

\balance  % to handle bug in acmart class  https://tex.stackexchange.com/questions/504955/package-balance-warning-in-acmart

Many users of the \AD Demo requested, via a feedback form, additional features.
Many wanted support for additional types of motions, or the ability to specify custom motions.
Several requested facial features, such as smiling, blinking, and gaze cues.
Others requested extending the work to handle quadrupeds, multiple characters in a drawing, or to take the context and background of the scene into account when creating the animation.

%Limitations to Animation Step
\hjs{
While our animation method is an appealing way to breathe life into children's drawings, it has two broad limitations.
First, only certain motions can be appealingly retargeted in this manner.
Not all limb motions can be well represented on a 2D plane. 
Spoke-like and arc-like motions, which primarily vary in one or two dimensions, are well handled while carving motions, which vary in all three spatial dimensions, are less recognizable when flattened.
In addition, we always move the character from left-to-right across the page.
If the character is facing right, this should be reversed.
Robustly determining which direction the character is facing is difficult, as the cues may be subtle; for example, the orientation of the nose may be the only cue as to whether the character is facing left or right (see Figure \ref{fig:segmentation_comparison}.h).

Second, our animation method is also limited by the style of the drawing.
We designed the retargeting technique to take advantage of the style of amateur drawings, which lack foreshortening and mix perspective.
If the figure is drawn with foreshortening and proper perspective, the character-motion stylistic mismatch may be undesirable.
In such cases, constructing a proper 3D model of the figure and using a different retargeting technique, such as\,\cite{weng2019photo}, would be preferable.
}

It is our hope that the released dataset will encourage other researchers to focus on methods to analyze and augment amateur drawings.
This domain is a natural form of creativity and expression available to much of the world's population. 
And, given the reception of the \AD Demo, there appears to be widespread appetite for animation and storytelling tools that build upon user-created drawings.

\acknowledgement{
\subsection{Acknowledgement}
We would like to thank FAIR Interfaces, FAIR X, and other members of Meta who helped in the building and release of the demo.
}