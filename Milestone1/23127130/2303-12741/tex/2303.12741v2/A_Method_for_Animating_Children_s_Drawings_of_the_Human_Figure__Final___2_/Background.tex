Our work builds on existing methods from several fields but is, to our knowledge, the first work focused specifically on fully automatic animation of children's drawings of human figures. 
To ground the work, we provide a summary of salient observations from the field of children's art analysis.
In addition, we briefly review related work on 2D image-to-animation and object and pose estimation for abstract images. 


\subsection{Analysis of Children's Drawings}

\hjs{
Children's drawings have long been of interest to the scientific community.
For well over a century, researchers from multiple fields have 
collected\,\cite{IndianaS55:online,kellogg1967rhoda,AWebbasedDatabaseforDrawingsofGods,geist2002they}
and analyzed them, seeking to understand and measure children's thought processes\,\cite{sully2021studies,barnes1892study,clark1897child,buhler2013mental}, 
intellectual development\,\cite{goodenough1926measurement},
and perceptions\,\cite{chambers1983stereotypic,doi:10.1080/01443410500344167}.
}
Particular attention has been given to drawings of human figures, one of the first and most frequently drawn subjects throughout childhood\,\cite{cox2013children}.

As the child develops, the schemas they employ to represent the human form become more complete (see Figure \ref{fig:tadpole-transitional-conventional}).
Even within these schemas, there is significant variation.
In addition to asymmetries and variation in color and proportion, many body parts appear optional to include; a study of drawings by 4-6 year old children showed that, while heads, legs, and eyes are almost universally present, other body parts (including torsos, arms, hands, and feet) were frequently absent\,\cite{cox2013children}.
Inversely, non-human body parts are frequently added in order to represent other subject classes\,\cite{kellogg1969analyzing}. With the addition of large ears, the figure may represent a cat or bear (Figures \ref{fig:maskrcnn_before_after}.m and \ref{fig:maskrcnn_before_after}.g); with the addition of a crown, it can represent a pineapple (Figure \ref{fig:maskrcnn_before_after}.n).
All of these sources of character variation make automatic character animation from drawings a non-trivial task.

\begin{figure}
\includegraphics[width=\linewidth]{images/tadpole-transition-conventional.png}
\caption{
As children learn to draw the human figure, the morphologies of the schemas they employ vary and evolve considerably\,\cite{cox2014drawings}.
Children frequently begin by drawing a \textit{tadpole figure}, a circular head region from which arms and legs extend. 
Some will progress to a \textit{transitional figure}, dropping the arms down so they extend from the legs. 
When a line is drawn between the legs, creating the separate torso region, the \textit{conventional figure} is formed.
Though these are small changes from the perspective of the drawer, they result in significantly different character morphologies when viewed through the lens of character animation.
A successful drawing-to-animation system must be robust to these types of variations.}
\label{fig:tadpole-transitional-conventional}
\end{figure}

Many researchers have focused closely on the unique style of children's drawings.
The psychologist and artist John Willats argues that, in order to understand the style of children's drawings, one must understand that the primary picture primitives employed by children are \textit{regions}, or 2D areas\,\cite{willats2006making}.
A squat volume, such as a head or torso, may be represented by a circular or ellipsoid region, whereas an elongated volume, such as a leg, may be represented by a long, thin region or even a single line.
These regions are not depictions of the object from any particular point of view. 
Rather, they are \textit{3D volumetric object-centered descriptions}\,\cite{marr1982vision},
2D areas with attributes perceptually similar to those of 3D object they are meant to represent.
%The regions begin as circles and lines, but later become modified to better reflect the perceptually impactful aspects of the objects they represent; a region representing a sugar cube or die may be given square corners, and a long region representing an arm may be given a bend to depict the elbow or split at the end to represent fingers (CITE Willats, 2005).

There are two stylistic outcomes of these \textit{object-centered descriptions} that bear mention.
First, the use of foreshortening is very rare in children's drawings \,\cite{piaget1956, willats1992representation}. 
This design choice is understandable; foreshortening a long region, such as a limb, results in a short region which does not adequately reflect the \textit{longness} of the object.
Second, the human figure may appear to have been drawn from many different perspectives, so as to make each part of the character maximally recognizable.
For example, the head and torso may face forward while the legs and feet are pointed to the side.
This technique, often referred to as \textit{twisted perspective}, is frequently seen and well-documented\,\cite{dziurawiec1992twisted}.
Both of these stylistic aspects are used to guide the design decisions of our system when applying human motion capture data onto the character.


\subsection{2D Image to Animation}

Previous researchers have proposed methods to animate drawings or photographs, many of which rely upon additional modes of user input.
Hornung et al. present a method to animate a 2D character in a photograph, given user-annotated joint locations\,\cite{Hornung2007anim2Dpicmotion}.
Pan and Zhang demonstrate a method to animate 2D characters with user-annotated joint locations via a variable-length needle model\,\cite{Pan2011}.
Jain et al. present an integrated approach to generate 3D proxies for animation given joint locations, segmentation masks, and per-part bounding boxes\,\cite{jain:2012}. 
Levi and Gotsman provide a method to create an articulated 3D object from a set of annotated 2D images and an initial 3D skeletal pose\,\cite{ArtiSketch}.
\textit{Live Sketch}\,\cite{su2018livesketch}
tracks control points from a video and applies their motion to user-specified control points upon a character.
Other approaches allow the user to specify character motions through a puppeteer interface, using RGB or RGB-D cameras\,\cite{held20123d,barnes2008video}.
\textit{ToonCap}\,\cite{Fan:2018:TAL} focuses on an inverse problem, capturing poses of a known cartoon character, given a previous image of the character annotated with layers, joints, and handles. 


\textit{Toonsynth}\,\cite{Dvoroznak18-SIG} and \textit{Neural Puppet}\,\cite{poursaeed2020neural} both present methods to synthesize animations of hand-drawn characters given a small set of drawings of the character in specified poses.
Hinz et al. train a network to generate new animation frames of a single character given 8-15 training images with user-specified keypoint annotations\,\cite{hinz2022charactergan}.

\textit{Monster Mash}\,\cite{Dvoroznak20-SA} presents an intuitive framework for sketch-based modeling and animation, and \textit{2.5D Cartoon Models}\,\cite{10.1145/1778765.1778796} presents a novel method of constructing 3D-like characters from a small number of 2D representations. 
Both of these are intuitive and well designed animation tools targeted towards amateur users.


\hjs{
Some animation methods are specifically tailored toward particular forms, such as faces\,\cite{elor2017bringingPortraits}, coloring book characters\,\cite{magnenat2015live}, or characters with human-like proportions. 
One notable work that is focused on the human form is \textit{Photo Wake Up}\,\cite{weng2019photo}. 
The authors show a method for creating a rigged and textured 3D mesh from a single image of a human-like figure.
Similar to us, their end goal is to allow users to seamlessly bring 2D characters to life; their work does an impressive job of accomplishing this.
Our method differs in two significant ways. 
First, while their work is focused on creating a 3D model for a mixed reality use case, 
ours is specifically focused on animating twisted perspective figures while staying within a 2D plane.
Second, children's drawings are much more abstract, incorrectly proportioned, and non human-like than the examples demonstrated in the paper.
We test our method upon the more abstract examples demonstrated in their paper and, with minor segmentation cleaning, they were successfully animated by our method.
}












\hjs{While the approaches listed here are wonderful tools to ease the burden of animation, none were perfectly suited to our use case.
Some require additional user input beyond the drawing itself, making the animation process more complex.
Others require the user to consistently draw the same character in multiple poses, which is beyond the skills of young children.
Others are focused on animating specific forms, precluding their use on children's drawings of the human figure.}


%Siarohin and colleagues propose a method for animating arbitrary classes of subjects,
%but require training videos of class members moving\,\cite{Siarohin_2019_NeurIPS}, making it unsuitable children's drawings.


\subsection{Detection, Segmentation, and Pose Estimation on Non-Photorealistic Images}

\hjs{
Aided by the the existence of large annotated datasets\,\cite{lin2014microsoft,6909866,6682899}, researchers have made considerable progress solving the problems of object detection, segmentation, and pose estimation from photographs. See, for example\,\cite{MaskRCNNhe2017mask,openpose19,guler2018densepose,alphapose,toshev2014deeppose}.
We explain the methods in this area that we leverage in Sections \ref{sec:character_detection} and \ref{sec:joint_detection}.

While traditional methods for detection, segmentation, and pose estimation of non-photorealistic images exist\,\cite{choi2012retrieval,bregler2002turning,davis2006sketching,eitz2012humans}, the lack of easily available datasets has resulted in slower adoption of deep learning models.
Some researchers are addressing this problem by developing methods and releasing datasets focused on the domain of anime characters\,\cite{chen2022bizarre,10.1145/3011549.3011552}, professional sketches\,\cite{brodt2022sketch2pose}, and mouse doodles\,\cite{ha2017neural}.
Other researchers have presented a non-deep learning method for inferring character poses from \textit{gesture drawings}\,\cite{Gesture3D}.
}
Because the Amateur Drawings Dataset is comprised of in-the-wild photographs of drawings created by the general public, we believe it will complement the value of existing datasets and allow for new dimensions of exploration and analysis.
