We evaluate our system in three ways.
First, we briefly describe the public reception of the demo.
Second, we present a set of experiments exploring the effect of training data size on the system's success rate.
Third, we perform a user study to validate the appeal and desirability of twisted perspective motion retargeting.

\subsection{Public Reception}
On December 16, 2021, a version of the proposed system was publicly released as the \AD Demo\,\cite{animateddrawings}.
The launch was accompanied by several high-profile social media posts and a blog post; however, all subsequent online promotion
came from users organically sharing the demo within their networks.

Over the next nine months, over 3.2 million unique users visited the site and spent, on average, over five minutes using the demo.
They uploaded 6.7 million images and, on average, generated four animations per image.
Based upon a subset of highly visible social media posts, the demo is especially popular among parents, elementary school teachers, technology enthusiasts, and artists.


\subsection{Effect of Training Sample Size}
\label{sec:effect_of_training_sample_size}

Our system incorporates repurposed computer vision models trained on photographs of real-world objects.
Because the domain of children's drawings is significantly different in appearance, these models must be fine-tuned prior to use.
However, given the abstract and varied nature of the drawings, it is not obvious how many drawings must be collected and annotated for training.
Therefore, we present a set of experiments exploring the relationship between training dataset size and model prediction success.

We report the performance of the models in two ways. 
Because the models employed have pre-established accuracy metrics, we first report the achieved mean average precision (mAP) [.5:.95] for each model.
However, our goal--animation--is a somewhat distinct downstream use of these predictions, and the mAP may not fully reflect the rate of success.
For example, a predicted bounding box that overlaps ground truth by 90\% would contribute to a very high mAP; 
however, if the prediction excluded a figure's foot or cut off half of its head, the resulting animation would be considered a failure.
Therefore, we also report the percent of predictions that result in successful animations, as determined by visual inspection.

\hjs{We compare the performance of several different fine-tuned versions of our models.
First, we fine-tune using 177,666 images from the Amateur Drawings Dataset; we excluded 500 images to use for validation (as described below).
However, some of the user-accepted annotations are noisy and inaccurate; therefore, we also fine-tune models using `clean' training datasets of multiple sizes.}
To obtain these, we randomly selected and manually reviewed images and annotations from within the Amateur Drawings Dataset.
Images that had clearly incorrect annotations were rejected.
Common reasons for rejection included: segmentation masks that did not contain the entire figure or included background elements, 
limbs that were fused together, joints that did not lie on the figure. 
In this way, we identified 2,500 images with suitable user-accepted annotations to serve as our training and validation data.

We randomly selected 500 images to serve as the validation set across all training runs, while the remaining 2,000 served as the training sample pool.
We created eight different training sets, varying in size from 10 through 2,000. 
For each training set, we randomly selected data samples from the training sample pool of 2,000 until we obtained the appropriately sized set.

We used the model architecture and training parameters specified in Sections \ref{sec:character_detection} (for both detection and segmentation predictions) and \ref{sec:joint_detection}.
Because our goal is to show the effect of training sample size, rather than optimize absolute accuracy, we restrict ourselves to a single model architecture and keep all hyperparameters constant.

To evaluate the percent of predictions suitable for animation, we used the same training sets as described above, but also included the additional set of all 2,500 images. 
For evaluation, we randomly selected an additional 571 images that were uploaded to the \AD Demo. 
While we reviewed these images to ensure that their contents were suitable for animation, we did not review, nor do we make use of, their user-approved annotations. Instead, model predictions were visually inspected to determine whether they would result in a successful animation.

This evaluation was meant to give an assessment of the models' in-the-wild success rates, and not have it be biased towards simpler drawings that our system could already predict perfectly, or those that took little effort to manually correct.
A detection was classified as failure if it did not detect the human figure, detected it multiple times, falsely detected non-human figures in the scene, had a bounding box that cut off a portion of the figure necessary for animation (such as an arm or foot), or had a bounding box extending to include other markings that were not a part of the figure.
A segmentation was classified as failure if it included background elements that were not part of the figure, did not tightly confirm to the bounds of the figure, contained holes in the interior of the figure, was more than one distinct polygon, or connected figure limbs at locations without a joint.
A pose estimation was classified as failure if the nose, shoulders, hips, elbows, knees, wrists, or ankles were not located on or in close proximity to the correct body part.

\subsubsection{Results}
Validation set mAP as a function of fine-tuning training set size is shown in Table \ref{table:mAP}. %and Fig. \ref{fig:sample-size-vs-map}.
\hjs{Using a Linux server with two NVIDIA Quadro GP100 graphics cards, models trained with 177,666 samples converged in 20 hours, whereas the smaller training sets all converged in under 5 hours.}
For comparison, we also show the mAP obtained when using pretrained model weights (essentially, a fine-tuning training set size of zero) and considering the drawn human figures to be instances of the \textit{person} object class.


\begin{table}[ht]
\resizebox{\columnwidth}{!}{
\begin{tabular}{r|ccc}
\hline
Fine-Tuning &Bounding Box & Segmentation & Pose Estimation\\
Training Set Size & mAP & mAP & mAP \\
\hline
(no fine-tuning) 0 & 0.06 & 0.04 & 0.09 \\
10 & 0.27 & 0.30 & 0.34 \\
100 & 0.51 & 0.51 & 0.76 \\
250 & 0.58 & 0.57 & 0.80 \\
500 & 0.69 & 0.63 & 0.82 \\
1000 & 0.77 & 0.68 & 0.84 \\
1500 & 0.80 & 0.70 & 0.85 \\
2000 & 0.81 & 0.71 & 0.85 \\
\hjs{(noisy) 177,666} & \hjs{0.82} & \hjs{0.49} & \hjs{0.90} \\
\hline
\end{tabular}}
\caption{
Per stage final mAP obtained on validation set as a function of fine-tuning training set size.
} 
\label{table:mAP}
\end{table}



The percentage of successful, animation-ready model predictions on the random 571 test images are given in Table \ref{table:animation_success_rate}.
We report the percentage of predictions that were successful in each stage, as well as the percentage of images for which predictions in all three stages were successful.
Because our system uses the image processing-based approach described in Section \ref{sec:character_segmentation}, we also evaluate this technique's performance using the same segmentation success-failure criteria; 42.4\% of segmentation masks obtained this way were successful.
In parentheses in the rightmost column of Table \ref{table:animation_success_rate}, we report the percentage of images for which predictions in all three stages were successful when the image processing-based segmentation algorithm is used instead of a fine-tuned model prediction.


\begin{table*}
%\resizebox{\textwidth}{!}{
\begin{tabular}{r|cccccc}
\hline
Fine-Tuning       & Bounding Box  & Segmentation  & Segmentation  & Pose Estimation  & All Stages & All Stages\\
Training Set Size & Success Rate  & Success Rate  & Success Rate  & Success Rate     & Success Rate & Success Rate\\
                  &               & \textcolor{gray}{(Mask R-CNN)}  & \textcolor{gray}{(Image Process)}  &            & \textcolor{gray}{(Mask R-CNN Seg.)} & \textcolor{gray}{(Image Process Seg.)}\\
\hline
(no fine-tuning) 0& 0.4  & 0.0  & |    & 0.6  & 0.0  & 0.0\\
                10 & 27.1 & 0.4  & |    & 2.1  & 0.0  & 0.9\\
               100 & 60.9 & 6.4  & |    & 54.1 & 4.9  & 19.4 \\
               250 & 62.2 & 8.2  & |    & 69.5 & 6.4  & 24.0 \\
               500 & 74.4 & 14.5 & 42.4 & 77.4 & 12.8 & 30.5 \\
              1000 & 83.0 & 19.4 & |    & 83.0 & 17.7 & 34.7 \\
              1500 & 89.8 & 20.3 & |    & 87.4 & 19.1 & 37.7 \\
              2000 & 91.8 & 22.7 & |    & 89.5 & 21.2 & 38.9 \\
              2500 & 92.5 & 24.7 & |    & 90.2 & 23.3 & 39.4 \\
\hjs{(noisy) 177,666} & \hjs{92.5} & \hjs{16.1} & |    & \hjs{94.6} & \hjs{16.1} & \hjs{40.6} \\
\hline
\end{tabular}%}
\caption{
Percentage of model predictions that can successfully be used for animation, as a function of model fine-tuning training set size.
We report the successes per stage for the bounding box, segmentation mask (both Mask R-CNN and image processing-based), and pose estimation predictions.
In the two right-most columns, we report the percentage of images for which the bounding box, segmentation mask, and pose estimation model predictions were all successful. 
} 
\label{table:animation_success_rate}
\end{table*}



\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{images/plots.png}
  \caption{
  \textit{Left:} Achieved mean average precision of bounding box, segmentation, and pose estimation predictions as a function of fine-tuning dataset size.
  \textit{Middle:} Percentage of bounding box, segmentation mask, and pose estimation predictions that could be used for animation without manual correction, respectively.
  \textit{Right:} Percentage of images for which bounding box, segmentation mask, and pose estimation predictions could all be used for animation without manual correction. We show the percentages when using both the Mask R-CNN segmentation predictions and the image processing-based segmentation technique described in Section \ref{sec:character_segmentation}.
  }
  \label{fig:plots}
\end{figure*}











\subsubsection{Discussion}
As Table \ref{table:mAP} shows, directly using model weights trained on real-world images results in very low mAP scores for bounding box, segmentation masks, and pose estimation predictions upon children's drawings.
However, fine-tuning results in a large gain in accuracy across all steps.
Continuing to increase the number of fine-tuning training samples results in continuing, yet slowing, improvements in mAP; 
increasing training set size from 1,500 to 2,000 increases performance by a single percentage point for bounding box and segmentation predictions and does not measurably improve pose estimation.

\hjs{
Interestingly, using the dataset of 177,666 images with noisy annotations results in a minor improvement in bounding box predictions, a significant improvement in pose estimation predictions, and a significant deterioration in segmentation predictions.
Fixing segmentation masks within the \AD Demo is more tedious than fixing bounding box or joint locations.
Therefore, it is likely that more users skipped the segmentation clean-up step, resulting in more noisy segmentation masks within the Amateur Drawings dataset, which in turn lowered the performance of the fine-tuned models.
This insight suggests that, depending upon the complexity of the prediction clean-up tasks offloaded onto the user during data collection, it may or may not be worthwhile to perform additional processing and refinement upon the collected annotations.
}

\hjs{Table \ref{table:animation_success_rate} shows the percentage of model predictions that could successfully be used for animation.
Similarly, without fine-tuning only a very small percentage of bounding box and pose estimation predictions are usable; none of the segmentation predictions are usable.
When fine-tuning with 2,500 `clean' samples, the percentages of usable bounding boxes, segmentation masks, and pose estimations increase to 92.5\%, 24.7\%, and 90.2\%, respectively.
When using the noisy training set of 177,666 images, the pose estimation success rate increased to 94.6\%, while the bounding box success rate was unchanged and the segmentation success rate dropped substantially.
} 
In the supplemental materials, we present many examples of successful and unsuccessful detection and pose predictions from the models trained with 2,500 samples.

Segmentation mask predictions, by contrast, require many more training samples to obtain comparable rates of success;
by a large margin, this step is the most difficult and failure-prone.
With even 2,500 training samples, fewer than one quarter of predictions from Mask R-CNN are suitable for animation without some sort of manual clean-up.
In part, this can be attributed to the presence of many `hollow' or `outline' figures within the dataset, for which the texture of the figure and the texture of background are identical. 
A hollow character's predicted segmentation mask often contains holes within the sparse, non-detailed parts of the figure, and includes connections between non-attached body parts that are drawn close together.
Model predictions also often fail on stick legs and stick arms, which are often missed, especially when other parts of the figure are 2D regions with area.
We present examples of all of these types of failures in the supplemental material.

In comparison, our image processing-based segmentation approach results in a 42.4\% success rate. 
While this approach does a better job of following the outline of the figure, it frequently fails on images with hard shadows introduced during the photographing of the drawing, drawings on lined paper, and figures that are not watertight or have limbs that do not connect.
With the image processing-based segmentation approach, 39.4\% of figures could be fully automatically animated without any manual intervention.
Clearly, further work on robustly segmenting hand drawn figures, or automatically refining the segmentation masks, would be useful in improving the overall success rate.

\subsection{Twisted Perspective Animation Retargeting}
\label{sec:twisted_perspective_perceptual_study}
We evaluate our use of twisted perspective retargeting through a perceptual user study on Amazon Mechanical Turk with 66 subjects.
Subjects were shown a set of 20 videos: four figures that were successfully detected, segmented, and rigged by our system, each performing five different motions (see top of Table~\ref{perceptual_study_results_table}).
Within each video were two side-by-side animations: one animation had been created with twisted perspective, by projecting the lower body and upper body onto different planes, while the other animation used only a single plane of projection.
The side upon which the twisted perspective condition appeared was randomized. 
Both animations played simultaneously, and viewers were asked to select, in a forced-choice manner, the animation whose character motion was `more appealing.' 
To ensure subjects paid attention, four `filter' questions were embedded in the stimuli, in which workers were explicitly directed to select either the left or the right animation.

We present the results in Table~\ref{perceptual_study_results_table}.
For each character and each motion type, we report the percentage of viewers who preferred the animation with twisted perspective motion retargeting over a single perspective.
In parentheses we report significance as the result of a binomial test comparing the distribution of responses to random chance.

In 16 of the 20 videos, a significant preference for twisted perspective was observed. 
In the remaining four videos, there was no significant preference for either type.
Taken together, this result shows that, for these character and motion combinations, twisted perspective retargeting often results in more preferable animation. 
\hjs{Interestingly, three of the four videos in which users had no significant preference depicted figures performing the `Wave Hello' motion.
As can been in the supplemental video, there is significantly less bending of the legs in the `Wave Hello' motion relative to the other motions tested; as a result, twisted perspective retargeting and single perspective retargeting result in more similar character poses.
This observation suggests that twisted perspective retargeting may not be necessary in all situations; rather it is more useful when both the arms and the legs have substantial motion in different planes.
}

\begin{table}[ht]
  \centering
  \includegraphics[width=\linewidth]{images/perceptual_study_results.png}
  \caption{The results of our perceptual study on the use of twisted perspective when retargeting motion. For each character and motion type, we show the percentage of viewers who preferred twisted perspective retargeting and the p-value indicating difference from random chance.}
  \label{perceptual_study_results_table}
  
\end{table}

