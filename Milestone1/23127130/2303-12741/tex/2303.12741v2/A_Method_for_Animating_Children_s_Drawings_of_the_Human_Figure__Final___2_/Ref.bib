@book{steele1998draw,
  title={Draw me a story: An illustrated exploration of drawing-as-language},
  author={Steele, Bob},
  year={1998},
  publisher={Portage \& Main Press}
}

@book{kellogg1969analyzing,
  title={Analyzing children's art},
  author={Kellogg, Rhoda},
  year={1969},
  publisher={Mayfield Pub Co}
}

@article{lowenfeld1957creative,
  title={Creative and mental growth},
  author={Lowenfeld, Viktor},
  year={1957},
  publisher={Macmillan}
}

@book{jolley2009children,
  title={Children and pictures: Drawing and understanding},
  author={Jolley, Richard P},
  year={2009},
  publisher={John Wiley \& Sons}
}

@book{willats2006making,
  title={Making sense of children's drawings},
  author={Willats, John},
  year={2006},
  publisher={Psychology Press}
}

@article{willats1992representation,
  title={The representation of extendedness in children's drawings of sticks and discs},
  author={Willats, John},
  journal={Child Development},
  volume={63},
  number={3},
  pages={692--710},
  year={1992},
  publisher={Wiley Online Library}
}

@book{malchiodi1998understanding,
  title={Understanding children's drawings},
  author={Malchiodi, Cathy A},
  year={1998},
  publisher={Guilford Press}
}

@book{matthews2003drawing,
  title={Drawing and painting: Children and visual representation},
  author={Matthews, John},
  year={2003},
  publisher={Sage}
}

@book{cox2013children,
  title={Children's drawings of the human figure},
  author={Cox, Maureen V},
  year={2013},
  publisher={Psychology Press}
}

@inproceedings{chen2022transfer,
  title={Transfer Learning for Pose Estimation of Illustrated Characters},
  author={Chen, Shuhong and Zwicker, Matthias},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={793--802},
  year={2022}
}

@article{eitz2012hdhso,
author={Eitz, Mathias and Hays, James and Alexa, Marc},
title={How Do Humans Sketch Objects?},
journal={ACM Trans. Graph. (Proc. SIGGRAPH)},
year={2012},
volume={31},
number={4},
pages = {44:1--44:10}
}


%%%%%%%%%%%%%%%5% Synthetic Data Generation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{GANNIPS2014,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {2672--2680},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}

@INPROCEEDINGS{pix2pix2017,
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image-to-Image Translation with Conditional Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={5967-5976},
  doi={10.1109/CVPR.2017.632}}



@INPROCEEDINGS{CycleGAN2017,
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2242-2251},
  doi={10.1109/ICCV.2017.244}}

@inproceedings{MultiModalPix2PixNIPS2017_6650,
author = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A. and Wang, Oliver and Shechtman, Eli},
title = {Toward Multimodal Image-to-Image Translation},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {465–476},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Human Pose Estimation %%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
@article{openpose19,
  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2019}
}

@inproceedings{fang2017rmpe,
                   title={RMPE: Regional Multi-person Pose Estimation},
                   author={Fang, Hao-Shu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},
                   booktitle={ICCV},
                   year={2017}
                  }
                
                
@inproceedings{xiao2018simplebaselines,
    author={Xiao, Bin and Wu, Haiping and Wei, Yichen},
    title={Simple Baselines for Human Pose Estimation and Tracking},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2018}
}

@INPROCEEDINGS{deepcut16,
  author={L. {Pishchulin} and E. {Insafutdinov} and S. {Tang} and B. {Andres} and M. {Andriluka} and P. {Gehler} and B. {Schiele}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation}, 
  year={2016},
  volume={},
  number={},
  pages={4929-4937},
  doi={10.1109/CVPR.2016.533}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Object Detection %%%%%%%%%%%%%%%%%%%%%%%
@article{Liu2019DeepLF,
  title={Deep Learning for Generic Object Detection: A Survey},
  author={Li Liu and Wanli Ouyang and X. Wang and P. Fieguth and J. Chen and Xinwang Liu and M. Pietik{\"a}inen},
  journal={International Journal of Computer Vision},
  year={2019},
  volume={128},
  pages={261-318}
}

@InProceedings{MaskRCNNhe2017mask,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
title = {Mask R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}


@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@misc{sermanet2014overfeat,
      title={OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}, 
      author={Pierre Sermanet and David Eigen and Xiang Zhang and Michael Mathieu and Rob Fergus and Yann LeCun},
      year={2014},
      eprint={1312.6229},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{RCNNgirshick2014rich,
      title={Rich feature hierarchies for accurate object detection and semantic segmentation}, 
      author={Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
      year={2014},
      eprint={1311.2524},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ren2016fasterRCNN,
      title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
      author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
      year={2016},
      eprint={1506.01497},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{detectornet2013,
author = {Szegedy, Christian and Toshev, Alexander and Erhan, Dumitru},
title = {Deep Neural Networks for Object Detection},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep Neural Networks (DNNs) have recently shown outstanding performance on image classification tasks [14]. In this paper we go one step further and address the problem of object detection using DNNs, that is not only classifying but also precisely localizing objects of various classes. We present a simple and yet powerful formulation of object detection as a regression problem to object bounding box masks. We define a multi-scale inference procedure which is able to produce high-resolution object detections at a low cost by a few network applications. State-of-the-art performance of the approach is shown on Pascal VOC.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2553–2561},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Animation %%%%%%%%%%%%%%%%%%

@article{igarashi2005asrigidaspossible,
author = {Igarashi, Takeo and Moscovich, Tomer and Hughes, John F.},
title = {As-Rigid-as-Possible Shape Manipulation},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1073204.1073323},
doi = {10.1145/1073204.1073323},
abstract = {We present an interactive system that lets a user move and deform a two-dimensional shape without manually establishing a skeleton or freeform deformation (FFD) domain beforehand. The shape is represented by a triangle mesh and the user moves several vertices of the mesh as constrained handles. The system then computes the positions of the remaining free vertices by minimizing the distortion of each triangle. While physically based simulation or iterative refinement can also be used for this purpose, they tend to be slow. We present a two-step closed-form algorithm that achieves real-time interaction. The first step finds an appropriate rotation for each triangle and the second step adjusts its scale. The key idea is to use quadratic error metrics so that each minimization problem becomes a system of linear equations. After solving the simultaneous equations at the beginning of interaction, we can quickly find the positions of free vertices during interactive manipulation. Our approach successfully conveys a sense of rigidity of the shape, which is difficult in space-warp approaches. With a multiple-point input device, even beginners can easily move, rotate, and deform shapes at will.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {1134–1141},
numpages = {8},
keywords = {interaction, mesh editing, shape manipulation, animation, deformation, image editing}
}

@ARTICLE{botsch2008,
  author={M. {Botsch} and O. {Sorkine}},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={On Linear Variational Surface Deformation Methods}, 
  year={2008},
  volume={14},
  number={1},
  pages={213-230},
  doi={10.1109/TVCG.2007.1054}}

@article{jacobson2011bbw,
author = {Jacobson, Alec and Baran, Ilya and Popovi\'{c}, Jovan and Sorkine, Olga},
title = {Bounded Biharmonic Weights for Real-Time Deformation},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2010324.1964973},
doi = {10.1145/2010324.1964973},
abstract = {Object deformation with linear blending dominates practical use as the fastest approach for transforming raster images, vector graphics, geometric models and animated characters. Unfortunately, linear blending schemes for skeletons or cages are not always easy to use because they may require manual weight painting or modeling closed polyhedral envelopes around objects. Our goal is to make the design and control of deformations simpler by allowing the user to work freely with the most convenient combination of handle types. We develop linear blending weights that produce smooth and intuitive deformations for points, bones and cages of arbitrary topology. Our weights, called bounded biharmonic weights, minimize the Laplacian energy subject to bound constraints. Doing so spreads the influences of the controls in a shape-aware and localized manner, even for objects with complex and concave boundaries. The variational weight optimization also makes it possible to customize the weights so that they preserve the shape of specified essential object features. We demonstrate successful use of our blending weights for real-time deformation of 2D and 3D shapes.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {78},
numpages = {8},
keywords = {generalized barycentric coordinates, articulated character animation, shape deformation, linear blend skinning}
}

%%%%Synthetic data%%%%%%5
@article{Chen2016Synthesizing3DPose,
  title={Synthesizing Training Images for Boosting Human 3D Pose Estimation},
  author={W. Chen and Hongxia Wang and Yangyan Li and H. Su and Z. Wang and C. Tu and D. Lischinski and D. Cohen-Or and B. Chen},
  journal={2016 Fourth International Conference on 3D Vision (3DV)},
  year={2016},
  pages={479-488}
}

@INPROCEEDINGS{varol17_surreal,
  title     = {Learning from Synthetic Humans},
  author    = {Varol, G{\"u}l and Romero, Javier and Martin, Xavier and Mahmood, Naureen and Black, Michael J. and Laptev, Ivan and Schmid, Cordelia},
  booktitle = {CVPR},
  year      = {2017}
}

@INPROCEEDINGS{Kuhnke2019,
  author={F. {Kuhnke} and J. {Ostermann}},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Deep Head Pose Estimation Using Synthetic Images and Partial Adversarial Domain Adaption for Continuous Label Spaces}, 
  year={2019},
  volume={},
  number={},
  pages={10163-10172},
  doi={10.1109/ICCV.2019.01026}}
  
  @INPROCEEDINGS{wu2018poseEstimator,
  author={J. {Wu} and B. {Zhou} and R. {Russell} and V. {Kee} and S. {Wagner} and M. {Hebert} and A. {Torralba} and D. M. S. {Johnson}},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Real-Time Object Pose Estimation with Pose Interpreter Networks}, 
  year={2018},
  volume={},
  number={},
  pages={6798-6805},
  doi={10.1109/IROS.2018.8593662}}
  
%%%%%%%%%%
@inproceedings{benchen2009varharmonicmap,
author = {Ben-Chen, Mirela and Weber, Ofir and Gotsman, Craig},
title = {Variational Harmonic Maps for Space Deformation},
year = {2009},
isbn = {9781605587264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1576246.1531340},
doi = {10.1145/1576246.1531340},
abstract = {A space deformation is a mapping from a source region to a target region within Euclidean space, which best satisfies some userspecified constraints. It can be used to deform shapes embedded in the ambient space and represented in various forms -- polygon meshes, point clouds or volumetric data. For a space deformation method to be useful, it should possess some natural properties: e.g. detail preservation, smoothness and intuitive control. A harmonic map from a domain ω ⊂ Rd to Rd is a mapping whose d components are harmonic functions. Harmonic mappings are smooth and regular, and if their components are coupled in some special way, the mapping can be detail-preserving, making it a natural choice for space deformation applications. The challenge is to find a harmonic mapping of the domain, which will satisfy constraints specified by the user, yet also be detail-preserving, and intuitive to control. We generate harmonic mappings as a linear combination of a set of harmonic basis functions, which have a closed-form expression when the source region boundary is piecewise linear. This is done by defining an energy functional of the mapping, and minimizing it within the linear span of these basis functions. The resulting mapping is harmonic, and a natural "As-Rigid-As-Possible" deformation of the source region. Unlike other space deformation methods, our approach does not require an explicit discretization of the domain. It is shown to be much more efficient, yet generate comparable deformations to state-of-the-art methods. We describe an optimization algorithm to minimize the deformation energy, which is robust, provably convergent, and easy to implement.},
booktitle = {ACM SIGGRAPH 2009 Papers},
articleno = {34},
numpages = {11},
keywords = {harmonic maps, space deformation, shape editing},
location = {New Orleans, Louisiana},
series = {SIGGRAPH '09}
}


@inproceedings{botsch2006primo,
author = {Botsch, Mario and Pauly, Mark and Gross, Markus and Kobbelt, Leif},
title = {PriMo: Coupled Prisms for Intuitive Surface Modeling},
year = {2006},
isbn = {3905673363},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {We present a new method for 3D shape modeling that achieves intuitive and robust deformations by emulating physically plausible surface behavior inspired by thin shells and plates. The surface mesh is embedded in a layer of volumetric prisms, which are coupled through non-linear, elastic forces. To deform the mesh, prisms are rigidly transformed to satisfy user constraints while minimizing the elastic energy. The rigidity of the prisms prevents degenerations even under extreme deformations, making the method numerically stable. For the underlying geometric optimization we employ both local and global shape matching techniques. Our modeling framework allows for the specification of various geometrically intuitive parameters that provide control over the physical surface behavior. While computationally more involved than previous methods, our approach significantly improves robustness and simplifies user interaction for large, complex deformations.},
booktitle = {Proceedings of the Fourth Eurographics Symposium on Geometry Processing},
pages = {11–20},
numpages = {10},
location = {Cagliari, Sardinia, Italy},
series = {SGP '06}
}
@article{hecker2008realtimeRetargeting,
author = {Hecker, Chris and Raabe, Bernd and Enslow, Ryan W. and DeWeese, John and Maynard, Jordan and van Prooijen, Kees},
title = {Real-Time Motion Retargeting to Highly Varied User-Created Morphologies},
year = {2008},
issue_date = {August 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1360612.1360626},
doi = {10.1145/1360612.1360626},
abstract = {Character animation in video games---whether manually keyframed or motion captured---has traditionally relied on codifying skeletons early in a game's development, and creating animations rigidly tied to these fixed skeleton morphologies. This paper introduces a novel system for animating characters whose morphologies are unknown at the time the animation is created. Our authoring tool allows animators to describe motion using familiar posing and key-framing methods. The system records the data in a morphology-independent form, preserving both the animation's structural relationships and its stylistic information. At runtime, the generalized data are applied to specific characters to yield pose goals that are supplied to a robust and efficient inverse kinematics solver. This system allows us to animate characters with highly varying skeleton morphologies that did not exist when the animation was authored, and, indeed, may be radically different than anything the original animator envisioned.},
journal = {ACM Trans. Graph.},
month = aug,
pages = {1–11},
numpages = {11},
keywords = {inverse kinematics, motion retargeting, games, procedural animation, user generated content, character animation}
}

@inproceedings{yamne2010animatingnonhumanoid,
author = {Yamane, Katsu and Ariki, Yuka and Hodgins, Jessica},
title = {Animating Non-Humanoid Characters with Human Motion Data},
year = {2010},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {This paper presents a method for generating animations of non-humanoid characters from human motion capture data. Characters considered in this work have proportion and/or topology significantly different from humans, but are expected to convey expressions and emotions through body language that are understandable to human viewers. Keyframing is most commonly used to animate such characters. Our method provides an alternative for animating non-humanoid characters that leverages motion data from a human subject performing in the style of the target character. The method consists of a statistical mapping function learned from a small set of corresponding key poses, and a physics-based optimization process to improve the physical realism. We demonstrate our approach on three characters and a variety of motions with emotional expressions.},
booktitle = {Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
pages = {169–178},
numpages = {10},
location = {Madrid, Spain},
series = {SCA '10}
}

@inproceedings{10.1145/1457515.1409068,
author = {Assa, Jackie and Cohen-Or, Daniel and Yeh, I-Cheng and Lee, Tong-Yee},
title = {Motion Overview of Human Actions},
year = {2008},
isbn = {9781450318310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1457515.1409068},
doi = {10.1145/1457515.1409068},
abstract = {During the last decade, motion capture data has emerged and gained a leading role in animations, games and 3D environments. Many of these applications require the creation of expressive overview video clips capturing the human motion, however sufficient attention has not been given to this problem. In this paper, we present a technique that generates an overview video based on the analysis of motion capture data. Our method is targeted for applications of 3D character based animations, automating, for example, the action summary and gameplay overview in simulations and computer games. We base our method on quantum annealing optimization with an objective function that respects the analysis of the character motion and the camera movement constraints. It automatically generates a smooth camera control path, splitting it to several shots if required. To evaluate our method, we introduce a novel camera placement metric which is evaluated against previous work and conduct a user study comparing our results with the various systems.},
booktitle = {ACM SIGGRAPH Asia 2008 Papers},
articleno = {115},
numpages = {10},
keywords = {animation summary, mocap, salient action, animation, viewpoint selection, camera},
location = {Singapore},
series = {SIGGRAPH Asia '08}
}

@article{elor2017bringingPortraits,
author = {Averbuch-Elor, Hadar and Cohen-Or, Daniel and Kopf, Johannes and Cohen, Michael F.},
title = {Bringing Portraits to Life},
year = {2017},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3130800.3130818},
doi = {10.1145/3130800.3130818},
abstract = {We present a technique to automatically animate a still portrait, making it possible for the subject in the photo to come to life and express various emotions. We use a driving video (of a different subject) and develop means to transfer the expressiveness of the subject in the driving video to the target portrait. In contrast to previous work that requires an input video of the target face to reenact a facial performance, our technique uses only a single target image. We animate the target image through 2D warps that imitate the facial transformations in the driving video. As warps alone do not carry the full expressiveness of the face, we add fine-scale dynamic details which are commonly associated with facial expressions such as creases and wrinkles. Furthermore, we hallucinate regions that are hidden in the input target face, most notably in the inner mouth. Our technique gives rise to reactive profiles, where people in still images can automatically interact with their viewers. We demonstrate our technique operating on numerous still portraits from the internet.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {196},
numpages = {13},
keywords = {face animation, facial reenactment}
}

@article{Hornung2007anim2Dpicmotion,
author = {Hornung, Alexander and Dekkers, Ellen and Kobbelt, Leif},
title = {Character Animation from 2D Pictures and 3D Motion Data},
year = {2007},
issue_date = {January 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/1189762.1189763},
doi = {10.1145/1189762.1189763},
abstract = {This article presents a new method to animate photos of 2D characters using 3D motion capture data. Given a single image of a person or essentially human-like subject, our method transfers the motion of a 3D skeleton onto the subject's 2D shape in image space, generating the impression of a realistic movement. We present robust solutions to reconstruct a projective camera model and a 3D model pose which matches best to the given 2D image. Depending on the reconstructed view, a 2D shape template is selected which enables the proper handling of occlusions. After fitting the template to the character in the input image, it is deformed as-rigid-as-possible by taking the projected 3D motion data into account. Unlike previous work, our method thereby correctly handles projective shape distortion. It works for images from arbitrary views and requires only a small amount of user interaction. We present animations of a diverse set of human (and nonhuman) characters with different types of motions, such as walking, jumping, or dancing.},
journal = {ACM Trans. Graph.},
month = jan,
pages = {1–es},
numpages = {9},
keywords = {3D motion data, as-rigid-as-possible shape manipulation with perspective correction, 2D character animation, camera and model pose determination}
}

@Inbook{Pan2011,
author="Pan, Junjun
and Zhang, Jian J.",
editor="Pan, Zhigeng
and Cheok, Adrian David
and M{\"u}ller, Wolfgang",
title="Sketch-Based Skeleton-Driven 2D Animation and Motion Capture",
bookTitle="Transactions on Edutainment VI",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="164--181",
abstract="We present a novel sketch-based 2D animation technique, which allows the user to produce 2D character animations efficiently. It consists of two parts, sketch-based skeleton-driven 2D animation production and 2D motion capture. The user inputs one image of the character and sketches the skeleton for each subsequent frame. The system deforms the character and creates animations automatically. To perform 2D shape deformation, a variable-length needle model is introduced to divide the deformation into two stages: skeleton driven deformation and nonlinear deformation in joint areas. It preserves the local geometric features and global area. Compared with existing approaches, it reduces the computation complexity and produces plausible results. Because our technique is skeleton-driven, the motion of character can be captured by tracking joints position and retargeted to a new character. This facilitates the reuse of motion characteristics contained in existing moving images, making the cartoon generation easy for artists and novices alike.",
isbn="978-3-642-22639-7",
doi="10.1007/978-3-642-22639-7_17",
url="https://doi.org/10.1007/978-3-642-22639-7_17"
}

@InProceedings{Siarohin_2019_CVPR,
  author={Siarohin, Aliaksandr and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  title={Animating Arbitrary Objects via Deep Motion Transfer},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2019}
}

@InProceedings{Siarohin_2019_NeurIPS,
  author={Siarohin, Aliaksandr and Lathuilière, Stéphane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  title={First Order Motion Model for Image Animation},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  month = {December},
  year = {2019}
}

@inproceedings{kazi2014draco,
  title={Draco: bringing life to illustrations with kinetic textures},
  author={Kazi, Rubaiat Habib and Chevalier, Fanny and Grossman, Tovi and Zhao, Shengdong and Fitzmaurice, George},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={351--360},
  year={2014}
}

@inproceedings{su2018livesketch,
author = {Su, Qingkun and Bai, Xue and Fu, Hongbo and Tai, Chiew-Lan and Wang, Jue},
title = {Live Sketch: Video-Driven Dynamic Deformation of Static Drawings},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174236},
doi = {10.1145/3173574.3174236},
abstract = {Creating sketch animations using traditional tools requires special artistic skills, and is tedious even for trained professionals. To lower the barrier for creating sketch animations, we propose a new system, emphLive Sketch,</i> which allows novice users to interactively bring static drawings to life by applying deformation-based animation effects that are extracted from video examples. Dynamic deformation is first extracted as a sparse set of moving control points from videos and then transferred to a static drawing. Our system addresses a few major technical challenges, such as motion extraction from video, video-to-sketch alignment, and many-to-one motion-driven sketch animation. While each of the sub-problems could be difficult to solve fully automatically, we present reliable solutions by combining new computational algorithms with intuitive user interactions. Our pilot study shows that our system allows both users with or without animation skills to easily add dynamic deformation to static drawings.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {dynamic deformation, video examples, motion transfer, motion extraction, sketch animation},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@article{jain:2012,
author = {Jain, Eakta and Sheikh, Yaser and Mahler, Moshe and Hodgins, Jessica},
title = {Three-dimensional proxies for hand-drawn characters},
journal = {ACM Trans. Graph.},
issue_date = {January 2012},
volume = {31},
number = {1},
month = feb,
year = {2012},
pages = {8:1--8:16},
articleno = {8},
numpages = {16}
}

@article{Dvoroznak18-SIG,
author = {Dvoro\v{z}n\'{a}k, Marek and Li, Wilmot and Kim, Vladimir G. and S\'{y}kora, Daniel},
title = {Toonsynth: Example-Based Synthesis of Hand-Colored Cartoon Animations},
year = {2018},
issue_date = {August 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3197517.3201326},
doi = {10.1145/3197517.3201326},
abstract = {We present a new example-based approach for synthesizing hand-colored cartoon animations. Our method produces results that preserve the specific visual appearance and stylized motion of manually authored animations without requiring artists to draw every frame from scratch. In our framework, the artist first stylizes a limited set of known source skeletal animations from which we extract a style-aware puppet that encodes the appearance and motion characteristics of the artwork. Given a new target skeletal motion, our method automatically transfers the style from the source examples to create a hand-colored target animation. Compared to previous work, our technique is the first to preserve both the detailed visual appearance and stylized motion of the original hand-drawn content. Our approach has numerous practical applications including traditional animation production and content creation for games.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {167},
numpages = {11},
keywords = {skeletal animation, style transfer}
}


@InProceedings{Zheng_2020_CVPR,
author = {Zheng, Qingyuan and Li, Zhuoru and Bargteil, Adam},
title = {Learning to Shadow Hand-Drawn Sketches},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{eitz2012humans,
author = {Eitz, Mathias and Hays, James and Alexa, Marc},
title = {How Do Humans Sketch Objects?},
year = {2012},
issue_date = {July 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2185520.2185540},
doi = {10.1145/2185520.2185540},
abstract = {Humans have used sketching to depict our visual world since prehistoric times. Even today, sketching is possibly the only rendering technique readily available to all humans. This paper is the first large scale exploration of human sketches. We analyze the distribution of non-expert sketches of everyday objects such as 'teapot' or 'car'. We ask humans to sketch objects of a given category and gather 20,000 unique sketches evenly distributed over 250 object categories. With this dataset we perform a perceptual study and find that humans can correctly identify the object category of a sketch 73\% of the time. We compare human performance against computational recognition methods. We develop a bag-of-features sketch representation and use multi-class support vector machines, trained on our sketch dataset, to classify sketches. The resulting recognition method is able to identify unknown sketches with 56\% accuracy (chance is 0.4\%). Based on the computational model, we demonstrate an interactive sketch recognition system. We release the complete crowd-sourced dataset of sketches to the community.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {44},
numpages = {10},
keywords = {learning, sketch, crowd-sourcing, recognition}
}

@article{lee2011shadowdraw,
author = {Lee, Yong Jae and Zitnick, C. Lawrence and Cohen, Michael F.},
title = {ShadowDraw: Real-Time User Guidance for Freehand Drawing},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2010324.1964922},
doi = {10.1145/2010324.1964922},
abstract = {We present ShadowDraw, a system for guiding the freeform drawing of objects. As the user draws, ShadowDraw dynamically updates a shadow image underlying the user's strokes. The shadows are suggestive of object contours that guide the user as they continue drawing. This paradigm is similar to tracing, with two major differences. First, we do not provide a single image from which the user can trace; rather ShadowDraw automatically blends relevant images from a large database to construct the shadows. Second, the system dynamically adapts to the user's drawings in real-time and produces suggestions accordingly. ShadowDraw works by efficiently matching local edge patches between the query, constructed from the current drawing, and a database of images. A hashing technique enforces both local and global similarity and provides sufficient speed for interactive feedback. Shadows are created by aggregating the edge maps from the best database matches, spatially weighted by their match scores. We test our approach with human subjects and show comparisons between the drawings that were produced with and without the system. The results show that our system produces more realistically proportioned line drawings.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {27},
numpages = {10},
keywords = {interactive drawing, shape matching, large scale image retrieval}
}

@article{DBLP:journals/corr/abs-2001-02600,
  author    = {Peng Xu},
  title     = {Deep Learning for Free-Hand Sketch: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2001.02600},
  year      = {2020},
  url       = {http://arxiv.org/abs/2001.02600},
  archivePrefix = {arXiv},
  eprint    = {2001.02600},
  timestamp = {Mon, 13 Jan 2020 12:40:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-02600.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{davis2006sketching,
author = {Davis, James and Agrawala, Maneesh and Chuang, Erika and Popovi\'{c}, Zoran and Salesin, David},
title = {A Sketching Interface for Articulated Figure Animation},
year = {2006},
isbn = {1595933646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185657.1185776},
doi = {10.1145/1185657.1185776},
abstract = {We introduce a new interface for rapidly creating 3D articulated figure animation, from 2D sketches of the character in the desired key frame poses. Since the exact 3D animation corresponding to a set of 2D drawings is ambiguous we first reconstruct the possible 3D configurations and then apply a set of constraints and assumptions to present the user with the most likely 3D pose. The user can refine this candidate pose by choosing among alternate poses proposed by the system. This interface is supported by pose reconstruction and optimization methods specifically designed to work with imprecise hand drawn figures. Our system provides a simple, intuitive and fast interface for creating rough animations that leverages our users' existing ability to draw. The resulting key framed sequence can be exported to commercial animation packages for interpolation and additional refinement.},
booktitle = {ACM SIGGRAPH 2006 Courses},
pages = {15–es},
location = {Boston, Massachusetts},
series = {SIGGRAPH '06}
}

@inproceedings{bregler2002turning,
author = {Bregler, Christoph and Loeb, Lorie and Chuang, Erika and Deshpande, Hrishi},
title = {Turning to the Masters: Motion Capturing Cartoons},
year = {2002},
isbn = {1581135211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/566570.566595},
doi = {10.1145/566570.566595},
abstract = {In this paper, we present a technique we call "cartoon capture and retargeting" which we use to track the motion from traditionally animated cartoons and retarget it onto 3-D models, 2-D drawings, and photographs. By using animation as the source, we can produce new animations that are expressive, exaggerated or non-realistic.Cartoon capture transforms a digitized cartoon into a cartoon motion representation. Using a combination of affine transformation and key-shape interpolation, cartoon capture tracks non-rigid shape changes in cartoon layers. Cartoon retargeting translates this information into different output media. The result is an animation with a new look but with the movement of the original cartoon.},
booktitle = {Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {399–407},
numpages = {9},
keywords = {animation, video, shape blending, deformations, computer vision, object tracking, morphing},
location = {San Antonio, Texas},
series = {SIGGRAPH '02}
}

@article{ha2017neural,
  title={A neural representation of sketch drawings},
  author={Ha, David and Eck, Douglas},
  journal={arXiv preprint arXiv:1704.03477},
  year={2017}
}

@article{choi2012retrieval,
author = {Choi, M. G. and Yang, K. and Igarashi, T. and Mitani, J. and Lee, J.},
title = {Retrieval and Visualization of Human Motion Data via Stick Figures},
journal = {Computer Graphics Forum},
volume = {31},
number = {7},
pages = {2057-2065},
keywords = {I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Animation, H.5.1 Information Interfaces and Presentation: Multimedia Information Systems—Animations, H.3.3 Information Storage And Retrieval: Information Search and Retrieval—Retrieval models},
doi = {https://doi.org/10.1111/j.1467-8659.2012.03198.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2012.03198.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2012.03198.x},
abstract = {Abstract We propose 2D stick figures as a unified medium for visualizing and searching for human motion data. The stick figures can express a wide range or human motion, and they are easy to be drawn by people without any professional training. In our interface, the user can browse overall motion by viewing the stick figure images generated from the database and retrieve them directly by using sketched stick figures as an input query. We started with a preliminary survey to observe how people draw stick figures. Based on the rules observed from the user study, we developed an algorithm converting motion data to a sequence of stick figures. The feature-based comparison method between the stick figures provides an interactive and progressive search for the users. They assist the user's sketching by showing the current retrieval result at each stroke. We demonstrate the utility of the system with a user study, in which the participants retrieved example motion segments from the database with 102 motion files by using our interface.},
year = {2012}
}



@inproceedings{zhang2018context,
  title={Context-based sketch classification},
  author={Zhang, Jianhui and Chen, Yilan and Li, Lei and Fu, Hongbo and Tai, Chiew-Lan},
  booktitle={Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering},
  pages={1--10},
  year={2018}
}

@book{kellogg1967rhoda,
  title={Rhoda Kellogg child art collection},
  author={Kellogg, Rhoda},
  year={1967},
  publisher={Microcard Editions}
}

 @INPROCEEDINGS{7780494,
  author={H. {Zhang} and S. {Liu} and C. {Zhang} and W. {Ren} and R. {Wang} and X. {Cao}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={SketchNet: Sketch Classification with Web Images}, 
  year={2016},
  volume={},
  number={},
  pages={1105-1113},
  doi={10.1109/CVPR.2016.125}}
  
  @inproceedings{10.1145/2911996.2912067,
author = {Ye, Yuxiang and Lu, Yijuan and Jiang, Hao},
title = {Human's Scene Sketch Understanding},
year = {2016},
isbn = {9781450343596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911996.2912067},
doi = {10.1145/2911996.2912067},
abstract = {Human's sketch understanding is important. It has many applications in human computer interaction, multimedia, and computer vision. Recognizing human sketches is also challenging. Previous methods focus on single-object sketch recognition. Understanding human's scene sketch that involves multiple objects and their complex interactions has not been explored. In this paper, we tackle this new problem. We create the first scene sketch dataset "Scene250" and propose a deep learning method to understand human scene sketches. We propose "Scene-Net", a new deep convolutional neural network (CNN) structure, based on which we build a novel scene sketch recognition system. Our system has been tested on the collected scene sketch dataset and compared with other state-of-the-art CNNs and sketch recognition approaches. Our experimental results demonstrate that our method achieves the state of art.},
booktitle = {Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
pages = {355–358},
numpages = {4},
keywords = {sketch understanding, scene sketch, deep learning},
location = {New York, New York, USA},
series = {ICMR '16}
}

@InProceedings{lin2014microsoft,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Microsoft COCO: Common Objects in Context",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="740--755",
abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
isbn="978-3-319-10602-1"
}

@misc{IndianaS55:online,
author = {Bradford B. Venable},
title = {Web Archive of Children's Art},
howpublished = {\url{http://childart.indstate.edu/}},
month = {11},
year = {2022},
note = {(Accessed on 11/26/2022)}
}

@inproceedings{DBLP:conf/siggraph/Ebihara98,
  author    = {Kazuyuki Ebihara},
  editor    = {Scott Crisson and
               Janet McAndless},
  title     = {Shall we dance?},
  booktitle = {{ACM} {SIGGRAPH} 98 Conference Abstracts and Applications, Orlando,
               Florida, USA, July 19-24, 1998},
  pages     = {124},
  publisher = {{ACM}},
  year      = {1998},
  url       = {https://doi.org/10.1145/280953.281328},
  doi       = {10.1145/280953.281328},
  timestamp = {Tue, 06 Nov 2018 16:59:13 +0100},
  biburl    = {https://dblp.org/rec/conf/siggraph/Ebihara98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2019mask,
  title={Mask scoring r-cnn},
  author={Huang, Zhaojin and Huang, Lichao and Gong, Yongchao and Huang, Chang and Wang, Xinggang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6409--6418},
  year={2019}
}

@article{dziurawiec1992twisted,
  title={‘Twisted perspective’ in young children's drawings},
  author={Dziurawiec, S and Deregowski, JB},
  journal={British Journal of Developmental Psychology},
  volume={10},
  number={1},
  pages={35--49},
  year={1992},
  publisher={Wiley Online Library}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal= {arXiv preprint arXiv:1906.07155},
  year={2019}
}

@misc{mmpose2020,
    title={OpenMMLab Pose Estimation Toolbox and Benchmark},
    author={MMPose Contributors},
    howpublished = {\url{https://github.com/open-mmlab/mmpose}},
    year={2020}
}

@book{gonzalez2008digital,
  abstract = {Completely self-contained-and heavily illustrated-this introduction to basic concepts and methodologies for digital image processing is written at a level that truly is suitable for seniors and first-year graduate students in almost any technical discipline. The leading textbook in its field for more than twenty years, it continues its cutting-edge focus on contemporary developments in all mainstream areas of image processing-e.g., image fundamentals, image enhancement in the spatial and frequency domains, restoration, color image processing, wavelets, image compression, morphology, segmentation, image description, and the fundamentals of object recognition. It focuses on material that is fundamental and has a broad scope of application.},
  added-at = {2014-07-10T10:50:48.000+0200},
  author = {Gonzalez, Rafael C. and Woods, Richard E.},
  biburl = {https://www.bibsonomy.org/bibtex/2bd73f6e1350f31aa5da16268e2b1e694/alex_ruff},
  description = {Digital Image Processing (3rd Edition): Rafael C. Gonzalez, Richard E. Woods: 9780131687288: Amazon.com: Books},
  interhash = {74494247f343d0cedb198c4b4f0c31eb},
  intrahash = {bd73f6e1350f31aa5da16268e2b1e694},
  isbn = {9780131687288 013168728X 9780135052679 013505267X},
  keywords = {book image_processing},
  publisher = {Prentice Hall},
  refid = {137312858},
  timestamp = {2014-07-10T10:50:48.000+0200},
  title = {Digital image processing},
  year = 2008
}

@article{chen2020improved,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2003.04297},
  year={2020}
}

@article{picard2014ipads,
  title={iPads at school? A quantitative comparison of elementary schoolchildren's pen-on-paper versus finger-on-screen drawing skills},
  author={Picard, Delphine and Martin, Perrine and Tsao, Raphaele},
  journal={Journal of Educational Computing Research},
  volume={50},
  number={2},
  pages={203--212},
  year={2014},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
  
  @misc{animateddrawings,
    author = "Meta",
    title = "Animated Drawings",
    year = "2022",
    url = "https://sketch.metademolab.com",
    note = "[Online; accessed 16-August-2022]"
  }  
  
  @article{DeepHistory,
author = {Ysique-Neciosup, Jose and Mercado-Chavez, Nilton and Ugarte, Willy},
title = {DeepHistory: A convolutional neural network for automatic animation of museum paintings},
journal = {Computer Animation and Virtual Worlds},
volume = {n/a},
number = {n/a},
pages = {e2110},
keywords = {convolutional neural network, image animation, keypoints, U-Net, video super-resolution},
doi = {https://doi.org/10.1002/cav.2110},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.2110},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cav.2110},
abstract = {Abstract Deep learning models have shown that it is possible to train neural networks to dispense, to a lesser or greater extent, with the need for human intervention for the task of image animation, which helps to reduce not only the production time of these audiovisual pieces, but also presents benefits with respect to the economic investment they require to be made. However, these models suffer from two common problems: the animations they generate are of very low resolution and they require large amounts of training data to generate good results. To deal with these issues, this article introduces the architectural modification of a state-of-the-art image animation model integrated with a video super-resolution model to make the generated videos more visually pleasing to viewers. Although it is possible to train the animation models with higher resolution images, the time it would take to train them would be much longer, which does not necessarily benefit the quality of the animation, so it is more efficient to complement it with another model focused on improving the animation resolution of the generated video as we demonstrate in our results. We present the design and implementation of a convolutional neural network based on an state-of-art model focused on the image animation task, which is trained with a set of facial data from videos extracted from the YouTube platform. To determine which of all the modifications to the selected state-of-the-art model architecture is better, the results are compared with different metrics that evaluate the performance in image animation and video quality enhancement tasks. The results show that modifying the architecture of the model focused on the detection of characteristic points significantly helps to generate more anatomically and visually attractive videos. In addition, perceptual testing with users shows that using a super-resolution video model as a plugin helps generate more visually appealing videos.}
}

@inproceedings{weng2019photo,
  title={Photo wake-up: 3d character animation from a single photo},
  author={Weng, Chung-Yi and Curless, Brian and Kemelmacher-Shlizerman, Ira},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5908--5917},
  year={2019}
}

@article{yaniv2019face,
author = {Yaniv, Jordan and Newman, Yael and Shamir, Ariel},
title = {The Face of Art: Landmark Detection and Geometric Style in Portraits},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3322984},
doi = {10.1145/3306346.3322984},
abstract = {Facial Landmark detection in natural images is a very active research domain. Impressive progress has been made in recent years, with the rise of neural-network based methods and large-scale datasets. However, it is still a challenging and largely unexplored problem in the artistic portraits domain. Compared to natural face images, artistic portraits are much more diverse. They contain a much wider style variation in both geometry and texture and are more complex to analyze. Moreover, datasets that are necessary to train neural networks are unavailable.We propose a method for artistic augmentation of natural face images that enables training deep neural networks for landmark detection in artistic portraits. We utilize conventional facial landmarks datasets, and transform their content from natural images into "artistic face" images. In addition, we use a feature-based landmark correction step, to reduce the dependency between the different facial features, which is necessary due to position and shape variations of facial landmarks in artworks. To evaluate our landmark detection framework, we created an "Artistic-Faces" dataset, containing 160 artworks of various art genres, artists and styles, with a large variation in both geometry and texture. Using our method, we can detect facial features in artistic portraits and analyze their geometric style. This allows the definition of signatures for artistic styles of artworks and artists, that encode both the geometry and the texture style. It also allows us to present a geometric-aware style transfer method for portraits.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {60},
numpages = {15},
keywords = {neural networks, geometry aware style transfer, artistic image augmentation, facial landmark detection}
}

@inproceedings{Fan:2018:TAL,
author = {Fan, Xinyi and Bermano, Amit H. and Kim, Vladimir G. and Popovi\'{c}, Jovan and Rusinkiewicz, Szymon},
title = {Tooncap: A Layered Deformable Model for Capturing Poses from Cartoon Characters},
year = {2018},
isbn = {9781450358927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229147.3229149},
doi = {10.1145/3229147.3229149},
abstract = {Characters in traditional artwork such as children's books or cartoon animations are typically drawn once, in fixed poses, with little opportunity to change the characters' appearance or re-use them in a different animation. To enable these applications one can fit a consistent parametric deformable model--- a puppet ---to different images of a character, thus establishing consistent segmentation, dense semantic correspondence, and deformation parameters across poses. In this work, we argue that a layered deformable puppet is a natural representation for hand-drawn characters, providing an effective way to deal with the articulation, expressive deformation, and occlusion that are common to this style of artwork. Our main contribution is an automatic pipeline for fitting these models to unlabeled images depicting the same character in various poses. We demonstrate that the output of our pipeline can be used directly for editing and re-targeting animations.},
booktitle = {Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering},
articleno = {16},
numpages = {12},
keywords = {segmentation, character animation, registration, correspondence},
location = {Victoria, British Columbia, Canada},
series = {Expressive '18}
}


@inproceedings{poursaeed2020neural,
  title={Neural puppet: Generative layered cartoon characters},
  author={Poursaeed, Omid and Kim, Vladimir and Shechtman, Eli and Saito, Jun and Belongie, Serge},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={3346--3356},
  year={2020}
}

@article{magnenat2015live,
author = {Magnenat, Stephane and Dat Tien Ngo and Zund, Fabio and Ryffel, Mattia and Noris, Gioacchino and Rothlin, Gerhard and Marra, Alessia and Nitti, Maurizio and Fua, Pascal and Gross, Markus and Sumner, Robert W.},
title = {Live Texturing of Augmented Reality Characters from Colored Drawings},
year = {2015},
issue_date = {Nov. 2015},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {21},
number = {11},
issn = {1077-2626},
url = {https://doi.org/10.1109/TVCG.2015.2459871},
doi = {10.1109/TVCG.2015.2459871},
abstract = {Coloring books capture the imagination of children and provide them with one of their earliest opportunities for creative expression. However, given the proliferation and popularity of digital devices, real-world activities like coloring can seem unexciting, and children become less engaged in them. Augmented reality holds unique potential to impact this situation by providing a bridge between real-world activities and digital enhancements. In this paper, we present an augmented reality coloring book App in which children color characters in a printed coloring book and inspect their work using a mobile device. The drawing is detected and tracked, and the video stream is augmented with an animated 3-D version of the character that is textured according to the child's coloring. This is possible thanks to several novel technical contributions. We present a texturing process that applies the captured texture from a 2-D colored drawing to both the visible and occluded regions of a 3-D character in real time. We develop a deformable surface tracking method designed for colored drawings that uses a new outlier rejection algorithm for real-time tracking and surface deformation recovery. We present a content creation pipeline to efficiently create the 2-D and 3-D content. And, finally, we validate our work with two user studies that examine the quality of our texturing algorithm and the overall App experience.},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = {nov},
pages = {1201–1210},
numpages = {10},
keywords = {Augmented reality, interactive books, drawing coloring, inpainting, deformable surface tracking}
}


@article{barnes2008video,
author = {Barnes, Connelly and Jacobs, David E. and Sanders, Jason and Goldman, Dan B and Rusinkiewicz, Szymon and Finkelstein, Adam and Agrawala, Maneesh},
title = {Video Puppetry: A Performative Interface for Cutout Animation},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/1409060.1409077},
doi = {10.1145/1409060.1409077},
abstract = {We present a video-based interface that allows users of all skill levels to quickly create cutout-style animations by performing the character motions. The puppeteer first creates a cast of physical puppets using paper, markers and scissors. He then physically moves these puppets to tell a story. Using an inexpensive overhead camera our system tracks the motions of the puppets and renders them on a new background while removing the puppeteer's hands. Our system runs in real-time (at 30 fps) so that the puppeteer and the audience can immediately see the animation that is created. Our system also supports a variety of constraints and effects including articulated characters, multi-track animation, scene changes, camera controls, 2 1/2-D environments, shadows, and animation cycles. Users have evaluated our system both quantitatively and qualitatively: In tests of low-level dexterity, our system has similar accuracy to a mouse interface. For simple story telling, users prefer our system over either a mouse interface or traditional puppetry. We demonstrate that even first-time users, including an eleven-year-old, can use our system to quickly turn an original story idea into an animation.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {124},
numpages = {9},
keywords = {animation, tangible user interface, real-time, vision}
}


@article{ArtiSketch,
author = {Levi, Zohar and Gotsman, Craig},
title = {ArtiSketch: A System for Articulated Sketch Modeling},
journal = {Computer Graphics Forum},
volume = {32},
number = {2pt2},
pages = {235-244},
doi = {https://doi.org/10.1111/cgf.12043},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12043},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12043},
abstract = {Abstract We present ArtiSketch – a system which allows the conversion of a wealth of existing 2D content into 3D content by users who do not necessarily possess artistic skills. Using ArtiSketch, a novice user may describe a 3D model as a set of articulated 2D sketches of a shape from different viewpoints. ArtiSketch then automatically converts the sketches to an articulated 3D object. Using common interactive tools, the user provides an initial estimate of the 3D skeleton pose for each frame, which ArtiSketch refines to be consistent between frames. This skeleton may then be manipulated independently to generate novel poses of the 3D model.},
year = {2013}
}

@inproceedings{hinz2022charactergan,
  title={CharacterGAN: Few-Shot Keypoint Character Animation and Reposing},
  author={Hinz, Tobias and Fisher, Matthew and Wang, Oliver and Shechtman, Eli and Wermter, Stefan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1988--1997},
  year={2022}
}

@inproceedings{held20123d,
author = {Held, Robert and Gupta, Ankit and Curless, Brian and Agrawala, Maneesh},
title = {3D Puppetry: A Kinect-Based Interface for 3D Animation},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380170},
doi = {10.1145/2380116.2380170},
abstract = {We present a system for producing 3D animations using physical objects (i.e., puppets) as input. Puppeteers can load 3D models of familiar rigid objects, including toys, into our system and use them as puppets for an animation. During a performance, the puppeteer physically manipulates these puppets in front of a Kinect depth sensor. Our system uses a combination of image-feature matching and 3D shape matching to identify and track the physical puppets. It then renders the corresponding 3D models into a virtual set. Our system operates in real time so that the puppeteer can immediately see the resulting animation and make adjustments on the fly. It also provides 6D virtual camera rev{and lighting} controls, which the puppeteer can adjust before, during, or after a performance. Finally our system supports layered animations to help puppeteers produce animations in which several characters move at the same time. We demonstrate the accessibility of our system with a variety of animations created by puppeteers with no prior animation experience.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {423–434},
numpages = {12},
keywords = {tangible user interface, object tracking, animation},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}


@ARTICLE{6682899,  author={Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},   year={2014},  volume={36},  number={7},  pages={1325-1339},  doi={10.1109/TPAMI.2013.248}}

@article{chen2020monocular,
  title={Monocular human pose estimation: A survey of deep learning-based methods},
  author={Chen, Yucheng and Tian, Yingli and He, Mingyi},
  journal={Computer Vision and Image Understanding},
  volume={192},
  pages={102897},
  year={2020},
  publisher={Elsevier}
}

@INPROCEEDINGS{6126221,  author={Chen, Yu and Kim, Tae-Kyun and Cipolla, Roberto},  booktitle={2011 International Conference on Computer Vision},   title={Silhouette-based object phenotype recognition using 3D shape priors},   year={2011},  volume={},  number={},  pages={25-32},  doi={10.1109/ICCV.2011.6126221}}

@article{gall2010optimization,
  title={Optimization and filtering for human motion capture},
  author={Gall, Juergen and Rosenhahn, Bodo and Brox, Thomas and Seidel, Hans-Peter},
  journal={International journal of computer vision},
  volume={87},
  number={1},
  pages={75--92},
  year={2010},
  publisher={Springer}
}

@inproceedings{chen2022bizarre,
    title={Transfer Learning for Pose Estimation of Illustrated Characters},
    author={Chen, Shuhong and Zwicker, Matthias},
    booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
    year={2022}
}

@article{brodt2022sketch2pose,
author = {Brodt, Kirill and Bessmeltsev, Mikhail},
title = {Sketch2Pose: Estimating a 3D Character Pose from a Bitmap Sketch},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3528223.3530106},
doi = {10.1145/3528223.3530106},
abstract = {Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist's intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work.Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {85},
numpages = {15},
keywords = {character sketches, character posing, sketch-based posing, rigged and skinned characters}
}


@inproceedings{sapp2010cascaded,
  title={Cascaded models for articulated pose estimation},
  author={Sapp, Benjamin and Toshev, Alexander and Taskar, Ben},
  booktitle={European conference on computer vision},
  pages={406--420},
  year={2010},
  organization={Springer}
}

@INPROCEEDINGS{6909866,
  author={Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={2D Human Pose Estimation: New Benchmark and State of the Art Analysis}, 
  year={2014},
  volume={},
  number={},
  pages={3686-3693},
  doi={10.1109/CVPR.2014.471}}
  
@incollection{ramanan2011part,
  title={Part-based models for finding people and estimating their pose},
  author={Ramanan, Deva},
  booktitle={Visual Analysis of Humans},
  pages={199--223},
  year={2011},
  publisher={Springer}
}
  
@inproceedings{10.1145/3011549.3011552,
author = {Khungurn, Pramook and Chou, Derek},
title = {Pose Estimation of Anime/Manga Characters: A Case for Synthetic Data},
year = {2016},
isbn = {9781450347846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3011549.3011552},
doi = {10.1145/3011549.3011552},
abstract = {2D articulated pose estimation is the task of locating the body joint positions of a human figure in an image. A pose estimator that works on anime/manga images could be an important component of an automatic system to create 3D animation from existing manga or anime, which could significantly the lower cost of media production. To create an accurate pose estimator, however, a sizable and high-quality dataset is needed, and such a dataset can be expensive to create.To alleviate data scarcity, we propose using a database of 3D character models and poses to generate synthetic training data for 2D pose estimators of anime/manga characters based on convolutional neural networks (CNN). We demonstrate that a high-performing estimator can be obtained by pretraining a network on a large synthetic dataset and then fine-tuning it on a small dataset of drawings. We also show that the approach yields a pose estimator competitive with many previous works when applied to a photograph-based dataset, establishing our synthetic data's usefulness beyond the intended domain.},
booktitle = {Proceedings of the 1st International Workshop on CoMics ANalysis, Processing and Understanding},
articleno = {3},
numpages = {6},
keywords = {pose estimation, synthetic data},
location = {Cancun, Mexico},
series = {MANPU '16}
}

@book{luquet1913dessins,
  title={Les dessins d'un enfant: {\'e}tude psychologique},
  author={Luquet, Georges Henri},
  year={1913},
  publisher={Alcan}
}

@book{sully2021studies,
  title={Studies of childhood},
  author={Sully, James},
  year={2021},
  publisher={Good Press}
}

@article{barnes1892study,
  title={A study on children's drawings},
  author={Barnes, Earl},
  journal={The pedagogical seminary},
  volume={2},
  number={3},
  pages={455--463},
  year={1892},
  publisher={Taylor \& Francis}
}

@article{clark1897child,
  title={The child's attitude towards perspective problems},
  author={Clark, Arthur B},
  journal={Studies in Education},
  volume={1},
  number={18},
  pages={283--294},
  year={1897},
  publisher={Stanford University Press Stanford, CA}
}

@book{buhler2013mental,
  title={The mental development of the child: A summary of modern psychological Theory},
  author={Buhler, Karl},
  year={2013},
  publisher={Routledge}
}

@article{wilson1982case,
  title={The case of the disappearing two-eyed profile: Or how little children influence the drawings of little children},
  author={Wilson, Marjorie and Wilson, Brent},
  journal={Review of research in visual arts education},
  pages={19--32},
  year={1982},
  publisher={JSTOR}
}

@book{cox2014drawings,
  title={Drawings of people by the under-5s},
  author={Cox, Maureen V and Cox, Maureen},
  year={2014},
  publisher={Routledge}
}

@book{piaget1956,
  title={The child's conception of space},
  author={Piaget, J. \& Inhelder, B},
  year={1956},
  publisher={Routledge \& Kegan Paul}
}

@book{marr1982vision,
  title={Vision: A computational investigation into the human representation and processing of visual information},
  author={Marr, David},
  publisher={W.H. Freeman and Company},
  year={1982}
}

@article{doi:10.1080/01443410500344167,
    author = { Isabelle D.   Cherney  and  Clair S.   Seiwert  and  Tara M.   Dickey  and  Judith D.   Flichtbeil },
    title = {Children’s Drawings: A mirror to their minds},
    journal = {Educational Psychology},
    volume = {26},
    number = {1},
    pages = {127-142},
    year  = {2006},
    publisher = {Routledge},
    doi = {10.1080/01443410500344167},
    URL = {https://doi.org/10.1080/01443410500344167},
    eprint = {https://doi.org/10.1080/01443410500344167}
    }

@book{goodenough1926measurement,
  title={Measurement of intelligence by drawings},
  author={Goodenough, F.L.},
  isbn={9780598637451},
  lccn={26018644},
  year={1926},
  publisher={World Book Co.}
}

@article{chambers1983stereotypic,
author = {Chambers, David Wade},
title = {Stereotypic images of the scientist: The draw-a-scientist test},
journal = {Science Education},
volume = {67},
number = {2},
pages = {255-265},
doi = {https://doi.org/10.1002/sce.3730670213},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sce.3730670213},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sce.3730670213},
year = {1983}
}

@book{geist2002they,
  title={They still draw pictures: Children's art in wartime from the Spanish Civil War to Kosovo},
  author={Geist, Anthony L and Carroll, Peter N},
  year={2002},
  publisher={University of Illinois Press}
}

@article {AWebbasedDatabaseforDrawingsofGods,
      author = "Zhargalma Dandarova Robert and Grégory Dessart and Olga Serbaeva and Camelia Puzdriac and Mohammad Khodayarifard and Saeed Akbari Zardkhaneh and Saeid Zandi and Elena Petanova and Kevin L. Ladd and Pierre-Yves Brandt",
      title = "A Web-based Database for Drawings of Gods: When the Digitals Go Multicultural",
      journal = "Archive for the Psychology of Religion",
      year = "2016",
      publisher = "Brill",
      address = "Leiden, The Netherlands",
      volume = "38",
      number = "3",
      doi = "https://doi.org/10.1163/15736121-12341326",
      pages=      "345 - 352",
      url = "https://brill.com/view/journals/arp/38/3/article-p345_5.xml"
}

@INPROCEEDINGS{toshev2014deeppose,
  author={Toshev, Alexander and Szegedy, Christian},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={DeepPose: Human Pose Estimation via Deep Neural Networks}, 
  year={2014},
  volume={},
  number={},
  pages={1653-1660},
  doi={10.1109/CVPR.2014.214}}
  

@InProceedings{guler2018densepose,
  title={DensePose: Dense Human Pose Estimation In The Wild},
  author={R{\i}za Alp G\"uler, Natalia Neverova, Iasonas Kokkinos},
  journal={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
  }
  

@article{alphapose,
  author = {Fang, Hao-Shu and Li, Jiefeng and Tang, Hongyang and Xu, Chao and Zhu, Haoyi and Xiu, Yuliang and Li, Yong-Lu and Lu, Cewu},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title = {AlphaPose: Whole-Body Regional Multi-Person Pose Estimation and Tracking in Real-Time},
  year = {2022}
}

@article{Dvoroznak20-SA,
author = {Dvoro\v{z}\v{n}\'{a}k, Marek and S\'{y}kora, Daniel and Curtis, Cassidy and Curless, Brian and Sorkine-Hornung, Olga and Salesin, David},
title = {Monster Mash: A Single-View Approach to Casual 3D Modeling and Animation},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417805},
doi = {10.1145/3414685.3417805},
abstract = {We present a new framework for sketch-based modeling and animation of 3D organic shapes that can work entirely in an intuitive 2D domain, enabling a playful, casual experience. Unlike previous sketch-based tools, our approach does not require a tedious part-based multi-view workflow with the explicit specification of an animation rig. Instead, we combine 3D inflation with a novel rigidity-preserving, layered deformation model, ARAP-L, to produce a smooth 3D mesh that is immediately ready for animation. Moreover, the resulting model can be animated from a single viewpoint --- and without the need to handle unwanted inter-penetrations, as required by previous approaches. We demonstrate the benefit of our approach on a variety of examples produced by inexperienced users as well as professional animators. For less experienced users, our single-view approach offers a simpler modeling and animating experience than working in a 3D environment, while for professionals, it offers a quick and casual workspace for ideation.},
journal = {ACM Trans. Graph.},
month = {nov},
articleno = {214},
numpages = {12},
keywords = {casual animation, 2D-to-3D inflation, sketch-based modelling}
}


@article{10.1145/1778765.1778796,
author = {Rivers, Alec and Igarashi, Takeo and Durand, Fr\'{e}do},
title = {2.5D Cartoon Models},
year = {2010},
issue_date = {July 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/1778765.1778796},
doi = {10.1145/1778765.1778796},
abstract = {We present a way to bring cartoon objects and characters into the third dimension, by giving them the ability to rotate and be viewed from any angle. We show how 2D vector art drawings of a cartoon from different views can be used to generate a novel structure, the 2.5D cartoon model, which can be used to simulate 3D rotations and generate plausible renderings of the cartoon from any view. 2.5D cartoon models are easier to create than a full 3D model, and retain the 2D nature of hand-drawn vector art, supporting a wide range of stylizations that need not correspond to any real 3D shape.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {59},
numpages = {7},
keywords = {interpolation, cartoons, non-photorealistic rendering, animation, vector art, billboards}
}

@article{Gesture3D,
author = {Bessmeltsev, Mikhail and Vining, Nicholas and Sheffer, Alla},
title = {Gesture3D: Posing 3D Characters via Gesture Drawings},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2980179.2980240},
doi = {10.1145/2980179.2980240},
abstract = {Artists routinely use gesture drawings to communicate ideated character poses for storyboarding and other digital media. During subsequent posing of the 3D character models, they use these drawing as a reference, and perform the posing itself using 3D interfaces which require time and expert 3D knowledge to operate. We propose the first method for automatically posing 3D characters directly using gesture drawings as an input, sidestepping the manual 3D posing step. We observe that artists are skilled at quickly and effectively conveying poses using such drawings, and design them to facilitate a single perceptually consistent pose interpretation by viewers. Our algorithm leverages perceptual cues to parse the drawings and recover the artist-intended poses. It takes as input a vector-format rough gesture drawing and a rigged 3D character model, and plausibly poses the character to conform to the depicted pose. No other input is required. Our contribution is two-fold: we first analyze and formulate the pose cues encoded in gesture drawings; we then employ these cues to compute a plausible image space projection of the conveyed pose and to imbue it with depth. Our framework is designed to robustly overcome errors and inaccuracies frequent in typical gesture drawings. We exhibit a wide variety of character models posed by our method created from gesture drawings of complex poses, including poses with occlusions and foreshortening. We validate our approach via result comparisons to artist-posed models generated from the same reference drawings, via studies that confirm that our results agree with viewer perception, and via comparison to algorithmic alternatives.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {165},
numpages = {13},
keywords = {character posing, gesture drawing, sketch-based modeling}
}

@inproceedings{Singh:FP:2002,
 title = {A Fresh Perspective},
 author = {Karan Singh},
 booktitle = {Proceedings of the Graphics Interface 2002 Conference},
 url = {http://graphicsinterface.org/wp-content/uploads/gi2002-3.pdf},
 year = {2002},
 month = {May},
 location = {Calgary, Alberta},
 pages = {17--24}
}

