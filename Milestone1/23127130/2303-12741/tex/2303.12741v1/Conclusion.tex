\section{Conclusion}

% reiterate paper
In this paper, we present a method to automatically animate the types of drawings created by children and amateur drawers. We also present a first-of-its-kind dataset of 180,000 in-the-wild drawings by children and amateurs, annotated with user-accepted bounding boxes, segmentation masks, and joint locations.

%%%%%
% We design our method to be as simple to use and universally accessible as possible. We therefore assume the images to not be perfect. We also don't use tablets.... (use following two sentences).


%Assuming the input to be a well-lit, high resolution scan would make the task easier, but would restrict the experience to those with specialized skills or equipment.
%%%%%

%reiterate validation
We demonstrate the value of our method in several ways.
First, we explore the accuracy and success from each stage of our system as a function of training dataset size.
Second, we perform a perceptual study to show the appeal of \textit{twisted-perspective} retargeting when animating these characters.
Third, we built and publicly released a usable version of the system which, within its first nine months, has been used to generate over 24 million animations from 6.7 million images uploaded by over 3.2 million users.



\hjs{
Prior to deciding to create a public-facing data collection tool, we unsuccessfully attempted to generate useful synthetic training data using generative adversarial networks\,\cite{pix2pix2017,CycleGAN2017,MultiModalPix2PixNIPS2017_6650}.
We believe our initial collection of less than 1,000 real children's drawings did not contain enough variation to cover the long-tail distribution of the domain. 
In addition, there were many source of unanticipated nuisance variation which were not in our initial collection, yet present within in-the-wild drawings (messy backgrounds, lined paper, blurry shots, bad lighting, erased lines) 
It is possible that synthetic data approaches utilizing the entirety of the Amateur Drawing Dataset, which includes these variations, may have more success.

Ultimately, we pivoted to a bootstrapping approach to collect the data necessary to fine-tune our models. 
We manually annotated the images we had and trained initial models, then iteratively released closed beta versions of the demo, collected additional training data, and retrained the models.
By thoughtfully crafting the user experience, keeping prediction and render times short, and providing the user something of value (a downloadable animation of their child's drawing) in exchange for their efforts, we were able to collect enough real data from our target domain and no longer need synthetic data.
We would encourage other researchers focused on user-generated content domains, for which there are not yet and suitable datasets, to likewise consider how they might invite their target audience into the dataset creation process, lowering the need to rely upon synthetic data.
}


















We believe this work is but a first step towards a robust and comprehensive drawing-to-animation storytelling system, and there are many ways our work could be improved.
One step that can clearly be improved is segmentation.
Extracting a usable and accurate mask can be quite difficult and, because it is used to create the character mesh, even small errors can result in bad animations.
There are many reasons why segmentation is hard to do accurately. 
Color and texture cues are not guaranteed to be helpful, as in the case of hollow characters.
A line can represent edges of regions depicting body parts, or a line can represent the entire body part, such as with stick figures.
Often, characters are drawn on lined paper, the paper contains eraser marks, or background objects that touch the character are drawn with the same pencil or marker.
And often the photographs of the drawings are distorted due to reflection glare or hard shadows.
\hjs{If the character is drawn with limbs touching in places other than joints (as hands touch hips in the \textit{arms akimbo} pose, for example), there is no predicted segmentation mask that will result in a quality animation unless it is possible to add a segmentation between the two body parts.

Given the importance and difficulty of the segmentation task, methods that improve the robustness of the masking step would greatly increase the success of our pipeline.
A useful next step could be a principled method for choosing between the image processing and Mask R-CNN segmentation masks on a \textit{per image} basis, as each method can fail for different reasons. 
Ideally, such a method could leverage the bounding box and joint location predictions from the other stages of the pipeline.
}

In addition to improving the robustness of the current pipeline, future work should focus on extracting additional information about the drawing prior to animation. 
A natural next step would be to infer the sub-type of the human figure (e.g. robot, monster, snowman, princess, etc.)
Such analysis could be used to modify the pose estimation skeleton (e.g. removing the legs when a snowman has been identified)
or determine the types of animation to apply (making monsters stomp, princesses dance, or superheroes fly).
It could also be used to infer what different character regions represent.
For example, triangles on a cat's head are ears, while triangles on a devil's head are horns; these insights could affect how the characters are ultimately animated.

Many users of the \AD Demo requested, via a feedback form, additional features.
Many wanted support for additional types of motions, or the ability to specify custom motions.
Several requested facial features, such as smiling, blinking, and gaze cues.
Other requested extending the work to handle quadrupeds, multiple characters in a drawing, or to take the context and background of the scene into account when creating the animation.

%Limitations to Animation Step
\hjs{
While our animation method is an appealing way to bring life to children's drawings, it has two broad limitations.
First, only certain motions can be appealing retargeted in this manner.
Not all limb motions can be well represented on a 2-D plane. 
Spoke-like and arc-like motions, which primarily vary in one or two dimensions, are well handled while carving motions, which vary in all three spatial dimensions, are less recognizable when flattened.
In addition, we always move the character from left-to-right across the page.
If the character is facing right, this should be reversed.
Robustly determining which direction the character is facing is difficult, as the cues may be subtle; for example, the orientation of the nose may be the only cue as to whether the character is facing left or right (see Figure \ref{fig:segmentation_comparison}.h).

Second, our animation method is also limited by the style of the drawing.
We designed the retargeting technique to take advantage of the style of amateur drawings, which lack foreshortening and mix perspective.
If the figure is drawn with foreshortening and proper perspective, the character-motion stylistic mismatch may be undesirable.
In such cases, constructing a proper 3D model of the figure and using a different retargeting technique, such as\,\cite{weng2019photo}, would be preferable.
}

It is our hope that the released dataset will encourage other researchers to focus on methods to analyze and augment amateur drawings.
This domain is a natural form of creativity and expression available to much of the world's population. 
And, given the reception of the \AD Demo, there appears to be widespread appetite for animation and storytelling tools that build upon user-created drawings.

\acknowledgement{
\subsection{Acknowledgement}
We would like to thank the members of the FAIR Interfaces and FAIR X teams, along with the other members of Meta who helped in the building and deployment of the demo.
%We would like to thank the members of the \censor{FAIR Interfaces and FAIR X} teams, along with the other members of \censor{Meta} who helped in the building and deployment of the demo.
}