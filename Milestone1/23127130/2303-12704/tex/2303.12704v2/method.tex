
% Comment Fig 2: I don’t think it’s that necessary to have an entirely new panel for the inference diagram, as it’s pretty clear that at inference the losses are turned off.
\begin{figure*}[htp]
    \centering
    \includegraphics[width=\textwidth]{arch_details}
    \caption{\textbf{Architecture Overview.} 
    %To leverage approximately-paired data, AptSim2Real trains a style encoder, ${S}$, to extract a style code from the approximately paired inputs. representing the realism of each image. During training, we condition the generator, ${G}$, on the difference in input and target style, providing additional supervision for the task. Content regularization losses and pooling layers in the style encoder are employed to minimize content leakage. During inference, a real image with a similar style as the sim input is selected from the training data as the target style reference. 
    To leverage approximately-paired data, AptSim2Real trains a style encoder, ${S}$, to extract a style code from the approximately paired inputs. During training, we condition the generator, ${G}$, on the difference in input and target style. Given that the simulation and real images are almost identical in content (as they are approximately-paired), the difference in style features reflects the realism gap between the images as any content feature information is cancelled. Content regularization losses and pooling layers in the style encoder are employed to additionally minimize content leakage. During inference, a real image with a similar style as the sim input is selected from the training data as the target style reference. 
    }
    \label{fig:architecture}
\end{figure*}

% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=0.45\textwidth]{arch_details_v}
%     \caption{\textbf{Architecture Overview.} With Approximately Paired Data, the styles and of the sim and real images are aligned. To leverage this property, AptSim2Real trains a style encoder ${S}$ to extract a style code from the approximately paired inputs. During training, we condition the generator ${G}$ on the difference in input and target style, providing additional supervision for the task. Content regularization losses and pooling layers in the style encoder are employed to minimize content leakage. During inference, a real image with a similar style as the sim input is selected from the training as the target style reference. 
%     }
%     \label{fig:architecture}
% \end{figure}


\subsection{Generating Approximately-paired Images}  

To learn a mapping $G: \boldsymbol x \rightarrow \boldsymbol y$ from simulated images, $\boldsymbol x$, to real images, $\boldsymbol y$, 
  we rely on approximately-paired image pairs, $(\boldsymbol x, \boldsymbol y)$. 
Unlike paired image translation~\cite{isola2017_pix2pix}, our pairs do not require pixel-wise alignment.
Instead, we sample a real image $\boldsymbol y_i$ from the real dataset and generate a corresponding simulated image by utilizing the metadata and the label information present in the image.
Since our real images are collected from sensors on an autonomous vehicle, we have a lot of metadata information including pose of the sensor, time of day, location, lighting, and weather.
Using this contextual information, we render a simulated image $\boldsymbol x_i$ in a graphics engine. 
We create a simulated version of each real object by selecting the corresponding asset from an asset library. 
Our selection process takes into account the label and contextual information present in the real image.
Given the pose information of the autonomous vehicle, we select the background of the simulation scene from a procedurally generated 3D map. 
% We match the lighting of the simulated image to the real image using an environment map that accounts for the time of day and weather conditions present in the real image.
The lighting and weather conditions are matched by selecting an environment map that reflects these properties.
%  Comment: There isn’t any information on how lighting is matched, or how weather effects are included, just that they are accounted for in some vague way using an environment map. Given that using a Cruise dataset will make reproduction of these experiments difficult you need to be as specific and precise as humanly possible. 
% TODO: Not sure how the simulatior actually does this.
Finally, we construct the completed 3D scene in a graphics engine by combining the selected assets, background, and lighting. This produces a simulated image that mirrors the camera pose of the real image.
% Comment:  There is no information on what graphics engine you use. Should we put that in, or is that impossible due to IP concerns?
We repeat this process for each real image and generate a dataset of $N$ approximately-paired images $\{\boldsymbol x_i, \boldsymbol y_i\}_{i=1}^N$.

% This scalable data-generation process produces a dataset of images where the images in each pair $(\boldsymbol x_i, \boldsymbol y_i)$ share information such as object size, object type, background, scene composition, environment lighting, camera pose. 
% Relaxing the ``paired" image requirements to become ``approximately-paired" allows us to utilize the best qualities of both paired and unpaired data. Like unpaired data, approximately paired data is cheap and efficient to generate, with minimal manual human labelling and curation required. 
% Whereas, paired data requires extensive human curation due to the requirement of pixel-wise correspondence. 
% Like paired data, approximately paired data implicitly models contextual information that the model can leverage to simplify the learning process, allowing in faster training and better accuracy. 
This scalable image generation process yields a dataset consisting of image pairs that share attributes such as object size, type, background, scene composition, lighting, and camera pose.
Much like unpaired data, Approximately-paired data is cheap and efficient to generate with minimal manual labeling and curation needed.
In contrast, paired data requires substantial manual curation due to the need for pixel-level correspondence.
However, like paired data, approximately-paired data incorporates contextual information that can guide the model and simplify the learning process, resulting in faster training and improved accuracy.

During training, the model is fed with approximately paired images, ${\boldsymbol{x}, \boldsymbol{y}}$. 
This pairing makes data generation more straightforward and improves supervision. 
At inference, a style image $\boldsymbol{y}$ possessing the desired style is selected from the training dataset.
% For our experiments, we pick a style image $\boldsymbol{y}$ with a timestamp that matches that of the simulated input $\boldsymbol{x}$. This approach is found to be effective in bridging the realism gap.

% Approximately Paired Data also implicitly disentangles content, scene composition, and style. By maintaining style information (for example, time of day, weather, or colour balance) and scene composition but varying the content in the image pairs, the model can be more robust %todo expand

% have another comparison image of simulation & approximately paired images here with some highlights

% Example of another APT domain: camera to camera, lidar to camera or something like taht

\subsection{Model details}
SimApt2Real is formulated as a one-sided generative adversarial network~\cite{onesided_gan_2017}
and consists of a generator $G$, a style encoder $S$, and a discriminator $D$. 
The goal of the network is to learn a generator $G$ which can take as input simulated images $\boldsymbol x$ and generate realistic images reflecting the style of the approximately-paired style image. 
% The models are designed to take maximal advantage of the approximately paired data. Figure  \ref{fig:architecture} contains an architecture overview.
The architecture leverages approximately paired data to maximize its performance, as shown in the overview in Figure \ref{fig:architecture}.

% \textbf{style encoder}. The style encoder $S$ takes an image as $x$ as an input and generates a style code $\boldsymbol s = S(\boldsymbol x)$ which encodes the style and realism of an input image. The style encoder is specifically designed and trained to separate content and style code during training. Given a sample target real image $\boldsymbol y$, the style difference $\boldsymbol{\bar s} = S(\boldsymbol y) - S(\boldsymbol x)$ is given to the generator produce the generated realistic image $\boldsymbol{\hat y} = G(\boldsymbol x,\boldsymbol{\bar s}) = G(\boldsymbol x, S(\boldsymbol y) - S(\boldsymbol x))$. 

% By perturbing the style feature or by changing the magnitude of the style difference, we can control the amount and type of change that the network will make in the generated image. This functionality can be used to adjust the diversity and composition of the generated images. 

% Note that there is a potential risk of the generator over fitting to the style input during training as there's a direct path for gradients to flow from the ground truth to the output. As such, the style encoder is specifically designed to separate content from style. See \ref{} for more details.

% Comment L195: Once again I think this is too imprecise. How do you capture stuff like contrast in a style embedding? Is there a special training trick? 
%The style encoder $S$ takes an image $\boldsymbol {x}$ as an input and generates a style code $\boldsymbol s = S(\boldsymbol x)$ which aims to capture global image properties such as contrast, image noise, color balance, sharpness.  
%\textbf{Style encoder}. The style encoder $S$ takes an image $\boldsymbol {x}$ as an input and generates a style code $\boldsymbol s = S(\boldsymbol x)$.  The style encoder aims to assist the generator by extracting style-specific information. In our experimentation, we observe that the style code can capture global image properties such as contrast, image noise, color balance, sharpness. 
%The style encoder uses average pooling layers that helps remove the content information such as location and pose of different objects present in the scene.
% Given a style input $\boldsymbol y$, we use the style difference, $\triangle{\boldsymbol{s}} = S(\boldsymbol y) - S(\boldsymbol x)$, as an additional input to the generator that helps to apply this difference to the input simulated image $\boldsymbol x$.
% In other words, the output image $\boldsymbol {\hat y} = G(\boldsymbol x, \triangle \boldsymbol{s})$ is generated by applying the style difference $\triangle \boldsymbol{s}$ to the input simulated image $\boldsymbol x$. 

\textbf{Style encoder}. The style encoder $S$ takes an image $\boldsymbol {x}$ as an input and generates a style code $\boldsymbol s = S(\boldsymbol x)$. 
Given the approximately-paired style input $\boldsymbol y$, we compute the style difference, $\triangle{\boldsymbol{s}} = S(\boldsymbol y) - S(\boldsymbol x)$. The style difference is used as an additional input to the generator that identifies what realism differences that must be corrected:
$$\boldsymbol {\hat y} = G(\boldsymbol x, \triangle \boldsymbol{s}).$$
Approximate pairing is critical for the style encoder design as we assume that the simulation and real images are almost identical in content, meaning that the difference in style features will reflect solely the realism gap. Any content-level feature differences will be cancelled when the style codes are subtracted.

To prevent the generator from over-fitting to the style input during training, we remove spatial content information from the style inpuet $\boldsymbol y$ by incorporating pooling layers in the style network.
We use multiple content losses between the input simulated image and the generated image to ensure that the generated image accurately reflects the content of the input image. 
These steps help to ensure that the generator is not solely focused on replicating the style of the real images but is also generating content accurate to the input image.

% \textbf{generator}. The generator $G$ consists of multiple ResNet blocks~\cite{Resnet_He2015} and translates an input simulation image $\boldsymbol{x}$ into a realistic output image $\boldsymbol{\hat y} = G(\boldsymbol{x}, \boldsymbol{s})$ reflecting a ``style difference" $\boldsymbol{\bar s}$, which is provided by the style encoder $S$. (see Figure  \ref{fig:architecture}). $\boldsymbol{\bar s}$ is a 256-dimensional learned latent code which represents both the style of the image and how unrealistic the simulation image is. This style difference provides additional context that $G$ leverages to produce more realistic images.
% %Mention weight modulation and demodulation

\textbf{Generator}. The generator $G$ consists of multiple ResNet blocks~\cite{Resnet_He2015} and translates an input simulated image $\boldsymbol{x}$ into a realistic output image $\boldsymbol{\hat y} = G(\boldsymbol{x}, \triangle \boldsymbol{s})$ guided by the style difference, $\triangle \boldsymbol{s}$, which is a $d$-dimensional learned latent code that represents the ``realism gap" between the simulated image and the corresponding approximately-paired style input.
We ``modulate" and ``demodulate" the weights of the convolution layers in the generator, $G$, using this style difference vector, $\triangle \boldsymbol{s}$, similar to the method proposed in~\cite{Karras2019stylegan2}.
% TODO: Review this
For each convolution layer, a linear layer is used to map the vector $\triangle \boldsymbol{s}$ so that the new dimension is same as the number of input feature maps in the convolution layer. 
Let $s_i$ be the $i^\text{th}$ element of the mapped style vector.
The convolution weights corresponding to the $i^\text{th}$ input feature map are ``modulated" by multiplying it with $s_i$.
Each modulated convolution weight is then ``demodulated" to normalize the output feature map.
% $w''_{ijk} = w'_{ijk} / \sqrt{\sum_{i,k} w'_{ijk}+ \epsilon} $,
% \begin{align*}
% w'_{ijk} &= s_i \cdot w_{ijk}.
% \end{align*}
% \begin{align*}
% w''_{ijk} &= w'_{ijk} / \sqrt{\sum_{i,k} w'_{ijk}},
% \end{align*}
% where $\epsilon$ is a small positive constant to avoid numerical issues. 
These modified convolutions are used to replace a typical instance norm in the generator \cite{Karras2019stylegan2} and allow us to control the weights of a convolution given the style difference $\triangle \boldsymbol{s}$. 
We refer readers to equations 1, 2, and 3 in \cite{Karras2019stylegan2} for details.

\textbf{Discriminator}. The discriminator, $D$, is trained to distinguish between real images, $\boldsymbol{y}$, and generated images, $\boldsymbol{\hat y}$, in a manner similar to standard GAN approaches~\cite{lsgan_2017}. 
Following the approach in \cite{simgan2017}, the discriminator is modeled with multiple convolution layers, producing a feature map of a lower resolution than the input image. 
Each location in the feature map corresponds to a patch in the input image, and the size of each patch is determined by the size of the receptive field of the output features. 
By classifying each location of the output feature map as real or fake, the discriminator classifies each patch in the input.

% TODO: Remove this

% Approximately paired means that there is a bijective mapping $R: Y \to X$ where for any $y \in Y$, $R(Y) = x \in X$ is the simulation image generated by the simulator for the road image $y$, where $x$ has the same content composition, lighting, and positioning as $y$. These pairs are approximate as the content is paired but there is no pixel-wise correspondence between $x$ and $y$ as in previous paired approaches [CITATION] (see Figure \ref{fig:architecture})

% APT is designed to leverage maximal information from the "approximately paired" nature of the Cruise simulation dataset to increase supervision on the model similar to paired methods [CITATION]  while maintaining the data scalability of unpaired methods [CITATION]. 

% This increased supervision is primarily achieved through style mixing.

% TODO: Probably remove the "in contrast to ..." and just state what the model does
% In contrast to existing works leveraging style mixing [CITATION] which typically learn to interpret a randomly drawn latent space $N(0,1)^{256}$ of style codes, APT directly extracts the style tensor $S(\boldsymbol y) \in \mathbb{R}^{256}$ from the approximately paired road image, removing the need for the network to learn to interpret styles in the latent space. 

% The generator is then conditioned on the simulation image and the difference in styles between the simulation and road images to produce the processed image $\hat y = G(\boldsymbol x, S(\boldsymbol y) - S(\boldsymbol x))$. 

% TODO: Not sure if we acutally ended up leveraging approximately paired data here

%The style input also allows for an easy way to adjust the diversity and composition of the generated images. 
%However, note that there is a potential risk of the generator overfitting to the style input during training as there's a direct path for gradients to flow from the ground truth to the output. 
%This could lead to the model performing poorly during test time. 


% \subsection{Approximately Paired Style Transfer}


\subsection{Training Losses}

Following standard GAN methods, we alternate between solving the following two optimization problems:
\begin{align*}
&\min_{D} \; \mathbb{E}_{\boldsymbol x,\boldsymbol y}\left[(D(\boldsymbol y) - 1)^2 + (D(G(\boldsymbol x, \triangle \boldsymbol{s}))-0)^2\right], \nonumber \\ 
&\min_{S, G} \mathcal{L}_{total}(S, G).
\end{align*}
Here, we employ the least squares adversarial loss~\cite{lsgan_2017}, where the discriminator is trained to predict $0$ for generated images and $1$ for real images. 
The total loss for optimizing the generator ($G$) and style encoder ($S$), $\mathcal{L}_{total}$, comprises of an adversarial loss aimed at fooling the discriminator. 
Next, we specify the various components that make up the total loss function.

\textbf{Adversarial loss}. The adversarial loss aims to deceive the discriminator into classifying the generated images as real, meaning that the discriminator should predict $1$ for the generated images.
Given $\triangle \boldsymbol{s} = S(\boldsymbol y) - S(\boldsymbol x)$, 
\begin{align*}
\mathcal{L}_{GAN} = \mathbb{E}_{\boldsymbol x,\boldsymbol y}\left[ (D(G(\boldsymbol x, \triangle \boldsymbol{ s})) - 1)^2\right].  
\end{align*}

\vspace{-0.12in}
\textbf{Style Reconstruction loss.} To ensure style similarity between the generated images, $\hat {\boldsymbol y}$, and target images, $\boldsymbol y$, we minimize the $\ell_1$ loss between the style representations of $\hat {\boldsymbol y}$ and $\boldsymbol y$ as follows:
\begin{align*}
\mathcal{L}_{sty} = \mathbb{E}_{\boldsymbol y}\left[\|S(\boldsymbol y) - S(\hat{\boldsymbol y})\|_1\right].
\end{align*}

Note that the style encoder can learn to output a constant vector to minimize this loss.
To circumvent this issue, we utilize the BCE loss (explained below) to distinguish between real image styles and simulated image styles.

\textbf{Style classification (BCE) loss.} The style encoder's key property is to differentiate the styles of approximately paired simulated and real images. 
To enforce this, we use a binary cross-entropy (BCE) loss to separate the style vectors of simulated and real images.
We train a linear classifier that maps the style vector to a scalar, producing $0$ for simulated style vectors and $1$ for real style vectors. 
The linear classifier is represented by a $d \times 1$ matrix $W_s$. 
The style classification loss is defined as follows:
\begin{align*}
    \mathcal{L}_{bce} = \mathbb{E}_{\boldsymbol x,\boldsymbol y}\left[(\log (f(W_s(S(\boldsymbol y))) + \log(1-f(W_s(S(\boldsymbol x))\right],
\end{align*}

\vspace{-0.11in}
where $f(.)$ is a Sigmoid function: $f(x) = 1/(1+e^{-x})$.
For improved stability and convergence during training, $W_s$ is pre-trained for a single epoch and then fine-tuned with a lower learning rate in conjunction with the other losses during the overall training process.
We found that the BCE loss and the style reconstruction loss are sufficient to capture the style differences between simulation and real images and that the style vectors are not ignored by the generator model.


\textbf{Content NCE loss}. We leverage PatchNCE loss~\cite{park2020cut} to enforce content similarity between $\boldsymbol x$ and $\hat {\boldsymbol y}$. 
Our generator consists of an encoder and a decoder component, i.e. $G(\boldsymbol x, \triangle \boldsymbol s) = G_{dec}(G_{enc}(\boldsymbol x, \triangle \boldsymbol s))$.
Using $G_{enc}$, we compute the encoded feature maps for the input image, $\boldsymbol x_{enc}$, and the generated image, $\hat {\boldsymbol y}_{enc}$, as follows: $\boldsymbol{x}_{enc} = G_{enc}(\boldsymbol{x}, S(\boldsymbol y) - S(\boldsymbol x))$ and $\hat{\boldsymbol{y}}_{enc} = G_{enc}(\hat{\boldsymbol{y}}, S(\boldsymbol y) - S(\hat{\boldsymbol y}))$.
Next, we sample the encoded feature vectors $\hat {\boldsymbol v}$ from the feature map $\hat {\boldsymbol y}_{enc}$ and the corresponding 
positive and negative feature vectors, $\boldsymbol v^+$ and $\boldsymbol v^-$, from the simulated feature map $\boldsymbol{x}_{enc}$. 
Note that $\boldsymbol v^+$ and $\hat {\boldsymbol v}$ correspond to the same spatial location while  $\boldsymbol v^-$ and  $\hat {\boldsymbol v}$ correspond to different spatial locations in the feature map.

To be precise, we first randomly sample $n$ indices from the generated feature map, $\hat {\boldsymbol y}$.
Let this set of indices be represented as $K = \{k_1, \dots, k_n\}$.
For each spatial location $k \in K $, we sample $\hat {\boldsymbol v}_{k} = \hat {\boldsymbol y}_{enc}(k)$, positive feature ${\boldsymbol v}^+_k = \hat {\boldsymbol x}_{enc}(k)$, negative feature ${\boldsymbol v}^-_k = \hat {\boldsymbol x}_{enc}(m)$ where $m \in K$ and $m \neq k$. 
In all our experiments, we set $n=256$.
Using the triplet $(\hat {\boldsymbol v_k}, {\boldsymbol v_k}^+, {\boldsymbol v_k}^-)$, the NCE loss is defined as follows:
% \vspace{-0.1in}
\begin{align*}
& \ell_{nce}^{(k)} = - \log \left[ \frac{\exp(\hat {\boldsymbol v_k} \cdot {\boldsymbol v_k}^+ / \tau)} {\exp (\hat {\boldsymbol v_k} \cdot {\boldsymbol v_k}^+ / \tau + \sum_{n=1}^{N} \exp(\hat {\boldsymbol v_k} \cdot {\boldsymbol v_k}^-/\tau))}\right],   \\
& \mathcal{L}_{NCE} = \mathbb{E}_{\boldsymbol x, \boldsymbol y}\left[ \sum_{k \in K} \ell_{nce}^{(k)} \right],
\end{align*}
% \vspace{-0.05in}
where the sum is over all the features sampled from an image (i.e. $(\hat {\boldsymbol v}, {\boldsymbol v}^+, {\boldsymbol v}^-)$ triplets).
% Note that for brevity, we are ignoring the indices of $\hat {\boldsymbol v}$, ${\boldsymbol v}^+$, and ${\boldsymbol v}^-$.


% \textbf{Identity loss}. To further disentangle content from style, we add a regularizing L1 loss term which ensures when gqipgqe an image from the target domain $Y$ and zero style difference, G will produce an identity mapping:
% $$\mathcal{L}_{idt} = \mathbb{E}_{\boldsymbol y}\left[\left\|G(y, 0) - y\right\|_1\right]$$

\textbf{Content identity loss}. Since we modify the input image $\boldsymbol x$ using the style difference, a zero style difference should result in no modification to the input.
To ensure this identity, we add an $\ell_1$ loss between the input image and the generated image with a zero style difference, expressed as follows:
$$\mathcal{L}_{idt} = \mathbb{E}_{\boldsymbol x}\left[\left\|G(\boldsymbol x, \boldsymbol 0) - \boldsymbol x\right\|_1\right].$$

% \textbf{Luminance loss}. We regularize the generated images $\hat y$ by enforcing that the average luminance of 16x16 patches is similar between the simulation images $x$ and generated images $\hat y$. For an image $x = [R_x, G_x, B_x]$, the luminance of $x$ is defined as $L_x = 0.299 R_x + 0.587 G_x + 0.114 B_x$ \cite{bt470}. Luminances within the image are then normalized to $\mu = 0, \sigma = 1$, preventing the regularization from penalizing global contrast or brightness changes.

% We define luminance loss as:
% $$\mathcal{L}_{lum} = \mathbb{E}_{x, y}\Bigg[\left\|\frac{\bar L_x - \mu(\bar L_x)}{\sigma(\bar L_x)} - \frac{\bar L_{\hat{y}} - \mu(\bar L_{\hat{y}})}{\sigma(\bar L_{\hat{y}})}\right\|_1\Bigg]$$

% where $\bar L = AvgPool(L)$ is the 16x16 average pooling with stride 16 of the luminance of the image. 

% We observe that luminance loss reduces high frequency artifacts in the generated output $\hat y$ by providing a "smoothing effect" without causing any blurring or reduction in sharpness in the final image.
\vspace{-0.1in}
\textbf{Content luminance loss}. 
This loss function aims to preserve the content of the image, such as object locations, shapes, and edges.
A naive approach to preserve the content would be to make sure that the luminance component of the simulated and generated images is identical. 
However, this constraint would prevent the model from making changes to style properties that are dependent on luminance, such as noise, overall brightness, and contrast. 
To overcome this limitation, the difference between normalized luminance patches of size 16x16 is minimized, as described below:
\begin{align*}
    \mathcal{L}_{lum} = \mathbb{E}_{\boldsymbol x, \boldsymbol y}\Bigg[\left\|\frac{\bar L_{\boldsymbol x} - \mu(\bar L_{\boldsymbol x})}{\sigma(\bar L_{\boldsymbol x})} - \frac{\bar L_{\hat{\boldsymbol y}} - \mu(\bar L_{\hat{\boldsymbol y}})}{\sigma(\bar L_{\hat{\boldsymbol y}})}\right\|_1\Bigg],
\end{align*}

where the luminance of an RGB image, $\boldsymbol x$, is calculated as a weighted combination of its red, green, and blue channels, with $L_{\boldsymbol x} = 0.299 R_{\boldsymbol x} + 0.587 G_{\boldsymbol x} + 0.114 B_{\boldsymbol x}$. % \cite{bt470}. 
$\bar L_{\boldsymbol x} $  (or $\bar L_{\boldsymbol y} $ ) is output of the $16 \times 16$ average pooling operation with a stride of $16$ on the input luminance $L_{\boldsymbol x}$ (or the generated luminance $L_{\boldsymbol y}$).
The $\mu(\cdot)$ and $\sigma(\cdot)$ denote the mean and the standard deviation functions, respectively.

\textbf{Total loss}. We sum all the above losses for $S$ and $G$ with the following combination weights,
\begin{align}
\mathcal{L}_{total}(S, G) = & \mathcal{L}_{GAN} + \lambda^{cls} \mathcal{L}_{bce}+ \lambda^{sty} \mathcal{L}_{sty} \nonumber \\
& + \lambda^{nce} \mathcal{L}_{nce} + \lambda^{idt} \mathcal{L}_{idt} + \lambda^{lum} \mathcal{L}_{lum}. \nonumber
\end{align}
In all our experiments, we set $\lambda^{cls}=1.0, \lambda^{sty} = 1.0, \lambda^{nce} = 0.5, \lambda^{idt} = 0.5, \lambda^{lum} = 0.1$.

\textbf{Note on approximate-pairing}. It is worth noting that the computation of style difference, $\triangle \boldsymbol s $, assumes that majority of the content in the simulated input is closely matched with the content in the real style input. 
If the input is not approximately-paired, then the computed $\triangle \boldsymbol s $ would not only capture the style difference but also the content difference. 
Moreover, using approximately-paired data allows for generated and approximately-paired real samples to be included in the same mini-batch for the discriminator training, which enables the adversarial loss to better focus on style differences and encourage the generator to produce outputs that more closely match the target style.