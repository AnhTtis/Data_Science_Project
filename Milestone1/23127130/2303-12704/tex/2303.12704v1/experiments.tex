
\begin{figure*}[htp]
    \centering
    \includegraphics[width=\textwidth]{apt_qualitative}
    \caption{
    % Qualitative results using the proposed AptSim2Real method. 
    % Note that Simulation images (left column) have multiple subtle domain gaps such as foliage, road, contrast, sharpness, compared to randomly sampled real images (the right column).
    % These randomly selected real images are used as the style input at the inference time.
    % The proposed approach (middle column) is able to bridge the distribution gap and improve the realism of the simulated images.
    \textbf{Qualitative results}. The left column in the figure depicts the simulation images, which exhibit several subtle domain gaps in terms of foliage, road, contrast, sharpness, etc., compared to randomly selected real images in the right column. 
    These real images serve as the unpaired style input during inference.
    Our proposed approach, depicted in the middle column, is capable of effectively bridging the distribution gap and enhancing the realism of the simulated images. The results demonstrate the ability of the AptSim2Real method to bridge the distribution gap and improve the realism of the simulated images. 
    }
    \label{fig:apt_qualitative}
\end{figure*}

\begin{figure*}[htp]
    \centering
    \includegraphics[width=\textwidth]{sim_vs_real_style}
    \caption{\textbf{Sim style vs real style input}. Examples of generated image using a simulation style image (center) and a real style image (right). By modifying the style code, we are able to control the style of the generated image.}
    \label{fig:sim_vs_real_style}
\end{figure*}

\begin{figure}[htp]
    \includegraphics[width=\textwidth / 2]{cyclegan_cut_apt}
    \caption{
    % Comparisons or CycleGAN~\cite{CycleGAN2017} and CUT~\cite{park2020cut} to the proposed AptSim2Real method.
    % CycleGAN results (left column) and CUT (middle column) have multiple artifacts. 
    % For example, in the first row, we can notice the high intensity artifacts around the traffic present in CycleGAN and CUT methods.
    % For CycleGAN, we notice a "checkboard" pattern artifact (e.g. notice the blue car on the second row).
    % CUT produces blurry foliage as shown in the CUT result of the second row. 
    % Overall, utilizing the approximate pairing in the proposed method results in much smaller artifacts which is also reflected in our quantitative results.
Comparisons between the proposed AptSim2Real method and other existing approaches such as CycleGAN~\cite{CycleGAN2017} and CUT~\cite{park2020cut} are shown in the figure above. 
We trained the baseline methods with multiple sets of hyper-parameters, and here we are presenting the best results we could obtain.
The results produced by CycleGAN (left column) and CUT (middle column) exhibit multiple artifacts.
For instance, the first row shows high intensity artifacts around the traffic light from both CycleGAN and CUT. 
CycleGAN's generated images also exhibit a noticeable ``checkerboard" pattern artifact, most noticeable in the blue car on the second row. 
CUT results show blurry foliage, as demonstrated in the second row.
Moreover, the road and objects are also blurrier in the baseline methods.
In contrast, our proposed AptSim2Real method, shown in the right column, reduces these artifacts significantly, resulting in much more realistic images. This is also reflected in our quantitative results, which demonstrate the effectiveness of the AptSim2Real method over the other existing approaches.    
\vspace{-0.1in}
    }
    \label{fig:cyclegan_cut_apt}
\end{figure}


\subsection{Experimental Setup}

\textbf{Dataset}. 
% The model was trained on a proprietary Cruise dataset containing 13,000 real world scenes collected from autonomous vehicles in San Francisco. We define a scene as a snapshot of all sensor data and metadata at one specific timestamp with 8 cameras per scene and a full 360 degree coverage. In total, there are $13,000 \times 8 \approx 100,000$ real world images with resolution $1920\times960$
% Data from 50+ Cruise rides in San Francisco are labelled with a combination of auto-labelling and human-curated labeling, providing thousands of rich 3d scenes with the positions of of signs, lights, pedestrians, vehicles, and other objects. The simulator ingests these real scenes alongside a pre-generated 3d rendition of the city to render high quality simulation scenes that closely resemble the real world scenes. The approximately-paired simulation scene will have identical scene layout, similar lighting conditions, similar assets, and similar textures.
% This dataset is labeled with a combination of auto-labelling and human-curated labeling, providing thousands of rich 3d attributes such as positions of of signs, lights, pedestrians, vehicles, and other objects. 
% The simulator ingests these real scenes alongside a pre-generated 3d rendition of the city to render high quality simulation scenes that closely resemble the real world scenes. 
% The approximately-paired simulation scene will have identical scene layout, similar lighting conditions, similar assets, and similar textures.
% From each simulation scene, we capture 8 images using virtual cameras that match the sensors on the autonomous vehicle 1-1. From each scene we produce 8 pairs of real images and approximately paired simulation images. Thus, from the entire dataset of 13,000 scenes we have 104,000 image pairs.
The model was trained on a dataset of $\approx 100,000$ real images of outdoor road scenes collected by autonomous vehicles driving in the city of San Francisco. The vehicle contains multiple cameras with a 360-degree view of the scene.
The dataset was labeled with a combination of both automated and human-verified labeling methods, providing rich 3D attributes such as positions of signs, lights, pedestrians, vehicles, and other objects. 
The simulator ingests these real scenes and a procedurally generated 3D map of the city to render high-quality simulation scenes that closely resemble the real-world scenes. 
% The approximately-paired simulation scenes have similar layout, lighting conditions, object types, background, etc.


\textbf{Training setup}. 
Following the standard GAN training process, the generator and discriminator are alternately trained on mini-batches of training samples. 
Unlike other unpaired image translation methods that randomly select samples from both domains, our method prepares batches such that they contain corresponding approximately-paired image pairs.
Our generator is a fully-convolutional network, can be trained on random smaller crops of images (size $256 \times 256$), and can be applied to images of any resolution during inference.
% During training, we preprocess our source and target images by selecting random location $(x, y)$ then cropping the real and simulated images at the same point.

% The training setup for AptSim2Real mimics CycleGAN \cite{CycleGAN2017} and CUT \cite{park2020cut}. In addition, we carefully prepare minibatches to show pairs of approximately paired images to the network during training. We also add style mixing to leverage the approximately paired data.

% During training, We will try to  $G(x, S(y') - S(x)) \approx y$, where $x$ is a random $256\times256$ of the simulation image, $y$ is the \textit{matching} $256\times256$ crop of the real image, and $y'$ is a \textit{non-overlapping} $256\times256$ crop of the real image, which is used as a style reference. 

% During training, we condition the generator on random $256 \times 256$ crops of the simulation and real images. We pick a random simulation crop $\boldsymbol x$ to be the input, the crop of the real image at the same position $\boldsymbol y$ to be the target, and a \textit{non overlapping} crop of the real image $\boldsymbol{y'}$ to be the style input. We then compute $\triangle \boldsymbol{s} = S(\boldsymbol{y'}) - S(\boldsymbol{x})$ try to approximate $G(\boldsymbol{x}, \triangle \boldsymbol{s}) \approx \boldsymbol{y}$.

% The style input $\boldsymbol y'$ shares the same style as $\boldsymbol y$ without having any content overlap. As such, the generator is able to directly ingest the target style without suffering from content leakage \cite{pmlr-style-equalization}.

% In simpler terms, we give the generator a crop of the simulation image and a \textit{non-overlapping} of the real image as a style input. The generator tries to produce an image that is similar in realism and style to the \textit{corresponding crop} of the real image. 
% Random cropping is done to improve training speed of the model, and has been shown in CITE HERE \cite{CycleGAN2017} to improve performance in certain cases. % TODO: REF & details
% By giving the model a non-overlapping crop of the real image as the style input, we give the model additional direction towards how the generated image should look like, without directly feeding it the output ground truth image. As such, we do not need to worry about content leakage during training \cite{pmlr-style-equalization}.


\textbf{Evaluation Setup}. 
Our aim is to evaluate the effectiveness of our method through a test dataset composed of $2000$ pairs of simulated and corresponding approximately-paired images, ($\boldsymbol{x}$, $\boldsymbol{y}$), that were not utilized during training. 
To avoid giving a night-time style image to the daytime simulated input, we limit our selection to those images that closely match the time of day depicted in the simulated input.
The real images, $\boldsymbol{y}$, are only used as a comparison to gauge the generated quality and not as a style input. 
This is because in practical applications, we do not have access to the corresponding real images for each simulated image during inference.
During inference, we randomly sample a real image from the training set ($\boldsymbol{y'}$) to serve as the style input. 
Given a simulated image $\boldsymbol{x}$ from the test dataset and a randomly selected style image $\boldsymbol{y'}$, we generate $\boldsymbol{\hat y} = G(\boldsymbol{x}, S(\boldsymbol{y'}) - S(\boldsymbol{x}))$.
We compare only with the unpaired method, because paired-methods are not applicable due to lack of pixel-wise correspondence.
% We measure the performance of our method using Fréchet Inception Distance (FID)~\cite{fid} metrics, which compares the generated images $\boldsymbol{\hat y}$ to the real images $\boldsymbol{y}$. 
% Our results demonstrate that our method generalizes well during inference, even when we do not have an approximately-paired style input.
% All the baseline methods were trained on the same generator and discriminator architecture.
% AptSim2Real's goal is to augment previously unseen simulation images to be more realistic. To evaluate AptSim2Real's performance, we feed the trained generator and style encoder an unseen simulation image $\boldsymbol{x}$ from an evaluation dataset created from Cruise approximately paired simulation \& real data and a real style reference $\boldsymbol{y'}$ sampled the training set. We then compute $\boldsymbol{\hat y} = G(\boldsymbol{x}, S(\boldsymbol{y'}) - S(\boldsymbol{x}))$, and evaluate the output images based on realism compared to the real image $\boldsymbol{y}$. 

% TODO: Add a citation on 
%There are two main dimensions to "realism". One aspect of realism is based on the realism of the generated images according to downstream models. A major use of Sim to Real is for salable data generation for training models. To provide a generalized quantitative metric for the improvement of realism according to another downstream model, we compare Frechet Inception Distance (FID) \cite{fid} scores. The second aspect of realism is realism according to humans. For this aspect, we compare examples of generated images to assess qualitative realism.

\subsection{Qualitative Results}

% As seen in Figure \ref{fig:apt_qualitative}, AptSim2Real is able to effectively enhance realism in simulation images. It is able to model high resolution noise and texture changes such as enhanced variation, colour, and texture in foliage; varied concrete and asphalt texture; and vehicle reflections. AptSim2Real is also able to match colour balance and palette to the style input, resulting in more believable simulation images.

% Additionally, APT is able to reduce image artifacts common in GAN architectures by leveraging approximately paired data. Figure \ref{fig:cyclegan_cut_apt} shows a comparison of CycleGAN (left), CUT (middle), and APT (right) images. Images generated using AptSim2real are free of artifacts such as bright spots dubbed the "Tear Drop Artifact" \cite{Karras2019stylegan2} and checkerboard patterns.

% By varying the style target, we can control the amount of change applied by the generator. Figure \ref{fig:sim_vs_real_style} shows an example output with a randomly generated simulation style target and real style target.

To evaluate the effectiveness of our method, we visually analyzed multiple randomly selected examples as depicted in Figure~\ref{fig:apt_qualitative}. 
The comparison of the images in the middle column with the real images in the third column demonstrates the ability of our approach to accurately capture high-resolution details and variations in noise, texture, and color. 
The proposed method not only improves the overall realism of the scene, but it particularly enhances the realism of foliage, concrete and asphalt textures, as well as the reflections on vehicles. 
% Moreover, our method successfully maintains a consistent color balance and accurately reproduces the input style, resulting in highly realistic simulation outputs.

Additionally, our method is able to reduce image artifacts common in GAN architectures by leveraging approximately paired data. 
Figure~\ref{fig:cyclegan_cut_apt} shows a comparison of CycleGAN (left), CUT (middle), and APT (right) images. 
We observed that both the CUT and CycleGAN methods suffer from several visual artifacts such as the presence of checkerboard patterns, traffic light distortions, and the ``Tear Drop Artifact"~\cite{Karras2019stylegan2}. 
In contrast, the AptSim2Real method exhibits a significantly reduced occurrence of these artifacts, resulting in a more visually appealing and realistic output.

We also validate that the style input is not ignored by the generator. 
Figure \ref{fig:sim_vs_real_style} displays an example output with a simulation style input and a real style input. 
It is noteworthy that when the input style corresponds to a simulated image, the output appears less realistic in comparison to an input style based on a real image.

\subsection{Quantitative Comparisons to Baselines} \label{quantatative}
To evaluate the performance of our model against previous baseline works, we use Fréchet Inception Distance (FID) scores as a metric. 
% FID scores measure the similarity in distribution of intermediate feature maps between the generated and real images, when they are processed through the InceptionV3 image network.
FID scores measure the distribution similarity of the intermediate features produced when the real and generated images are processed by the InceptionV3 image network.
A lower FID score indicates that the generated images are more realistic, as the feature map distributions of the generated and real images are more alike. 
% Intuitively, if the generated images are realistic, then their feature map distributions should closely resemble that of the real images, leading to a lower FID score.
% TODO: The above two sentences are repetitive

Table~\ref{tb:fid_compare} demonstrates the effectiveness of AptSim2Real compared to two state-of-the-art methods: CycleGAN\cite{CycleGAN2017} and CUT\cite{park2020cut}.
% All models were trained on the same dataset of $100,000$ simulation images then evaluated on a test dataset of $2000$ images. 
As shown in Table~\ref{tb:fid_compare}, the existing unpaired image translation methods are unable to significantly improve the image quality. 
We observe in our qualitative experiments that this is mainly because of artifacts introduced by the models when trained and evaluated on our complex dataset.
We note that the FID score of the baseline methods is comparable to that of the original simulated images. 
However, this similarity can be attributed to the introduction of artifacts by these methods, as demonstrated by our qualitative results in Figure~\ref{fig:cyclegan_cut_apt}.
It is crucial to avoid artifacts when generating synthetic data for downstream model training, because these models can overfit to artifacts and fail to generalize, leading to poor performance in real-world scenarios.
Our model improves the FID score by more than $24\%$ when compared to unpaired methods. 
% Starting from a baseline CUT implementation, we achieve a $-1.9$ improvement by adding approximately paired data. 
% By conditioning the generator on the style input, we achieve an additional $-3.9$ FID improvement, for a total of a $-5.8 (25\%)$ FID score improvement over CUT. 
We also compute the FID score between two different sets of real images to find the lower bound on FID for our datasets.
The FID score between two sets of real images is $6.6$, which means our method is $35\%$ closer to real images than the baseline simulation.
% More ablation details are present in section \ref{section:ablation}. % TODO: more details. Runtime speed?

\begin{table}[ht]
\begin{center}
\caption{\label{tb:fid_compare} FID score of the compared methods}
\begin{tabular}{@{}lc@{}}
    \toprule[1.5pt]
    {Method} & {FID score (lower is better)}\\
    \midrule
    Unmodified Sim images & $23.4$ \\
   CycleGAN & $24.7$\\
   CUT & $23.3$\\
   AptSim2Real & $\mathbf{17.6}$\\
    \bottomrule[1.5pt]
\end{tabular}
\end{center}
\end{table} 


\subsection{Other Improvements}

% Image examples of the ablation
While we propose a specific novel architecture that improves on baselines by leveraging approximately paired data, GANs and image-to-image translation are constantly improving through newer architectures and more stable training strategies. 
Approximately-paired data can be leveraged in a variety of ways to improve performance independent of specific network design choices.

% TOdo: how to connect the above paragraph and the below two sections

% todo: what was the FID score improvement?
% \textbf{Projected GANs}. We conducted additional experiments with Projected GANs~\cite{Projected_gans}, where images are first passed through the first layer of a frozen pretrained image detector before being processed by the discriminator. 
% By using the first encoding layer of an image detection model we achieved an FID score of $16.96$ (an additional improvement of $4\%$)

\textbf{Projected GANs}. 
To further improve our results, we conducted supplementary experiments utilizing Projected GANs~\cite{Projected_gans}.
In these experiments, the adversarial loss was calculated on image features extracted from the activations of the first encoding layer of a pre-trained object detection model, rather than on the raw RGB image. 
This approach resulted in a noticeable improvement in FID score, which decreased to $16.96$, representing a relative improvement of $4\%$.

\textbf{Multi-Scale Discriminators}. 
% We obtained features from the discriminator at a range of resolutions $(64\times 64, 32\times 32, 16\times 16)$ and compute GAN loss at each resolution.  
% Contrary to existing methods that train separate discriminators from each scale, we directly sample intermediate feature maps from a single discriminator to obtain features at each scale. 
% This approach improves training speed without sacrificing performance. 
In our method, features were extracted from the discriminator at multiple resolutions $(64\times 64, 32\times 32, 16\times 16)$ and the GAN loss was computed at each of these resolutions. 
Unlike traditional approaches that employ separate discriminators for each scale, we sample intermediate feature maps from a single discriminator to increase training efficiency.

\subsection{Additional Training Details}
% Not sure where to put this
% The model was trained for 100,000 iterations with a batch size of 24 (24 tesla V100 GPUs over 8 machines). To stabilize training, we add one sided label smoothing \cite{gantutorial} to the adversarial loss. We use the ADAM \cite{adam} optimizer with $\beta_1=0.5$, $\beta_2 = 0.99$, and $lr = 1e-4$. The learning rate remains constant for 20,000 iterations then linearly decays to zero. We first pre-train the style encoder for one epoch  with just the style classification loss enabled: $\ell_{BCE} = 1.0$ . After, the generator and discriminator are trained with $\ell_{BCE} = 0.01$. For evaluation, we compute the exponential moving average of model parameters for the generator and style encoder \cite{gan_ema}. Weights are initialized to $N(0,0.1)$ and biases to $0$.

The model was trained for a maximum of $100,000$ iterations with a batch size of $24$. 
% For evaluation, we choose the best intermediate checkpoint given FID scores calculated during validation.
To ensure stability during the training process, one-sided label smoothing was applied to the adversarial loss, as described in~\cite{gantutorial}. 
The ADAM optimizer~\cite{adam} was utilized with beta values of $\beta_1 = 0.5$ and $\beta_2 = 0.99$, and a learning rate of $1e-4$. 
The learning rate remained constant for $20,000$ iterations and then decreased linearly until it reached zero.
Before training the generator and discriminator, the style encoder was pre-trained for one epoch using only the style classification loss with $\lambda^{bce} = 1.0$. 
Afterwards, the generator and the discriminator were then trained $\lambda^{bce} = 0.01$.
For evaluation, we calculated the exponential moving average of the model parameters for both the generator and style encoder, as suggested in ~\cite{gan_ema}. 
% The network layers were initialized with weights from a Normal distribution with a mean of $0$ and a standard deviation of $0.1$, and the biases were initialized to 0.
