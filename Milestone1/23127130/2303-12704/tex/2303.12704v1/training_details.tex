% TODO: Decide what to do with this info

% \section{Training Details}

% The model was trained for 100,000 iterations with a batch size of 24 (24 tesla V100 GPUs over 8 machines). To stablize training, we add one sided label smoothing \cite{gantutorial} to the adversarial loss. We use the ADAM \cite{adam} optimizer with $\beta_1=0.5$ and $\beta_2 = 0.99$. We first pretrain the style encoder for one epoch with learning rate $1e-4$. After, the generator and discriminator are trained with $lr=1e-4$ and the style encoder with $lr=1e-6$. For evaluation, we compute the exponential moving average of model parameters for the generator and style encoder \cite{gan_ema}. Weights are initialized to $N(0,0.1)$ and biases to $0$.

%\textbf{Generator Architecture}. Our Generator consists of one downsampling block, eight intermediate blocks, and one upsampling block, all of which use pre-activation residual blocks \cite{residual}. We use weight modulation and demodulation \cite{Karras2019stylegan2} on every layer and no other normalization. Weight demodulation is disabled on the final output block \cite{Karras2019stylegan2}. Leaky ReLU \cite{Maas2013RectifierNI} is used after each block. A learned style code is injected into each weight modulation layer. 

%\textbf{Style Encoder Architecture}. The style encoder consists of four 2x downsampling pre-activation residual blocks \cite{residual} and an adaptive average pooling layer that pools to a $1\times1\times512$ style code. The style code is then normalized to mean 0 and standard deviation 1, providing the normalized style representation of an image. During pre-training, a linear classifier head with a binary cross entropy loss is trained on the style code to predict real or fake. The learning rate of the loss is reduced to 0.01 after pretraining. This practice prevents the style code from collapsing to zero or a single value. 

%\textbf{Discriminator Architecture} The discriminator is based on a PatchGAN discriminator \cite{isola2017_pix2pix} and consists of four 2x downsampline pre-activation residual blocks with leaky ReLU \cite{Maas2013RectifierNI} and Instance Normalization \cite{instancenorm} at each block. We observe that instance normalization in the discriminator is critical to stable training on our diverse and complex sim2real dataset.