% \label{section:ablation}

\begin{table}[ht]
\begin{center}
\caption{\label{tb:fid_ablation} Ablation study of AptSim2Real}
\begin{tabular}{@{}lc@{}}
    \toprule[1.5pt]
    {Method} & {FID score (lower is better)}\\
    \midrule
    AptSim2Real & $\mathbf{17.6}$ \\
    \midrule
    Removing L1 identity loss & $19.1$ \\
    Removing Luminance loss & $19.2$\\
    % Removing NCE loss & $27.4$\\
    AptSim2Real with Unpaired Data & $19.5$\\
    \bottomrule[1.5pt]
\end{tabular}
\end{center}
\end{table} 
\vspace{-0.1in}
% \textbf{Impact of Approximately Paired Data}

% As shown in table \ref{tb:fid_compare}, approximately paired data combined with the style information and our loss formulation make a significant improvement over the baseline models. 
% In Table \ref{tb:fid_ablation}, we include ablations targeted at disentangling the impact of Approximately Paired Data compared to our model changes. 
% Training AptSim2Real on the exact same dataset and procedure but with unpaired data results in worse generated image quality ($10\%$ relative increase to the FID score)
% We see similar results when training CUT \cite{park2020cut} with approximately paired data. 
% Training CUT with the exact same parameters except with approximately-paired data instead of unpaired data gave an FID score of $19.1$. 
% % Even with existing models not tailored to take advantage of approximately paired data, models are able to learn to leverage the additional information present in the training data to make training easier.

% % Conclusion sentence? more data? 

% \textbf{Ablation Study}

% Table \ref{tb:fid_ablation} shows an ablation of our loss functions on the same experimental setup as in section \ref{quantatative}. Removing any of the regularizing loss terms (L1 identity loss, luminance loss, and NCE loss \cite{park2020cut}) significantly degrade results, with NCE loss having the biggest impact. 

% We observe that NCE loss \cite{park2020cut} significantly improves learning speed by strongly enforcing content similarity between the input and the generated output. L1 identity loss helps disentangle style information from content information, encouraging the model to explicitly model style information using the style encoder and preventing the model from collapsing into a modality where the style input is ignored.

% We found that Luminance loss is effective at reducing artifacts common with GAN-based image to image networks such as CUT and CycleGAN \cite{CycleGAN2017} \cite{park2020cut} \cite{Karras2019stylegan2}. By enforcing that average relative luminance is consistent from the input to the output image, we reduce bright spot artifacts that would otherwise produce a large difference in average luminance. Luminance loss also reduces high resolution noise artifacts and colour changes often observed with other models.


% Even with existing models not tailored to take advantage of approximately paired data, models are able to learn to leverage the additional information present in the training data to make training easier.

% Conclusion sentence? more data? 

\subsection{Ablation Study}

 % presents an ablation study of our loss functions using the same experimental setup described in section~\ref{quantatative}. 
% The results demonstrate that the removal of any of the content losses (L1 identity loss, luminance loss, and NCE loss) results in a significant decline in the generated image quality.
% Our experiments show that the NCE loss~\cite{park2020cut} significantly improves the learning speed by imposing a stronger constraint on content similarity between the input and the generated image. 
% On the other hand, identity loss helps the model to explicitly capture style information using the style encoder and preventing it from ignoring the style input.
The ablation study in Table~\ref{tb:fid_ablation} show that identity loss helps the model to explicitly capture style information using the style encoder and preventing it from ignoring the style input.
Moreover, we found that the luminance loss effectively reduces the artifacts that are commonly encountered in GAN-based image-to-image networks such as CUT and CycleGAN. 
By enforcing consistency in the average relative luminance between the input and output images, we reduce the bright spot artifacts that would otherwise cause a large difference in average luminance. 
Our experiments also reveal that training AptSim2Real on unpaired data results in a decrease in generated image quality, leading to a relative increase of $10\%$ in the FID score. 
Similarly, when training CUT with approximately paired data, we observed a drop in the FID score, resulting in an FID score of $19.1$. 
These results emphasize the importance of approximately-paired data for achieving high-quality image generation results.