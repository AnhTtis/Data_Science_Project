%5 paragraphs


 % In general, the story and motivation needs more focus. The novelty of this paper lies in the problem setting, but the intro spends very little time justifying the need for this setting. There is one paragraph describing this setting (starting at L60), but there is not anything said about why we want this setting. In fact, it almost feels like this setting and the unpaired setting both share the same benefits (being easy to scale etc.), and the latter is certainly easier from a data perspective. If the reader doesn’t agree with you that approximate pairing is an interesting problem, then there isn’t any reason to be interested in the proposed method from the start. This is true not only in the introduction, but throughout - I think there needs to be more consistent re-emphasis of why the approximate pairing setting is important.


% Meta Comments: I think the new setting is an interesting one and the output images look nice. I think this is certainly a useful method especially at Cruise, where we already have all this infrastructure handy, and the ablation experiments where you show that using unpaired data and that leads to a performance drop is cool. Obviously we are a bit limited by needing to remain on this internal dataset, but I think we can compensate by just writing a more persuasive story. I don’t necessarily think for an IROS submission we need extra experiments (with the possible exception of (2) below - take a look), but I do think the prose needs some work. 

% In addition to the specific points delineated below in “Specific Comments,” there are a few weaknesses (ignoring the whole internal dataset thing because I know that’s already been discussed) that I think need to be resolved before the paper is ready for primetime. Mainly, I do feel that the story and motivation here are a bit weak - the paper needs to pay more attention to convincing the reader that this new setting is important and that the added multi-loss complexity of the proposed method is worth it. 
% Response: Thank you so much for taking time to review. I revised the paper taking your feedback into account. Please see the comments below.

% Here are some specific high-level points that I think need to be afforded special attention:

% In general, the story and motivation needs more focus. The novelty of this paper lies in the problem setting, but the intro spends very little time justifying the need for this setting. There is one paragraph describing this setting (starting at L60), but there is not anything said about why we want this setting. In fact, it almost feels like this setting and the unpaired setting both share the same benefits (being easy to scale etc.), and the latter is certainly easier from a data perspective. If the reader doesn’t agree with you that approximate pairing is an interesting problem, then there isn’t any reason to be interested in the proposed method from the start. This is true not only in the introduction, but throughout - I think there needs to be more consistent re-emphasis of why the approximate pairing setting is important.
% 	Response: Thank you for this feedback. Revised the paper to put more emphasis on why approximate-pairing is important	.

% In general, you make the case that simulation is important, and some mention that we can use simulated data to train models, but there isn’t any study here where you use your generated images to show downstream modeling benefit. Would that be a useful experiment to try, especially given that we have ready access to Cruise models? 
% 	Response: I agree, results on downstream tasks would make the paper stronger. In its absence, I am relying on previous works (e.g. SimGAN) that showed that improving the quality does improve the downstream task performance. I would make the connection stronger in the paper.

% Why should a practitioner use AptSim2Real when CycleGAN and CUT are simpler algorithms and don’t require an “approximate pairing” data pipeline? Sure, the artifacts look cleaner, but is this enough?
% On that note, what is the runtime (training + inference) of AptSim2Real versus CycleGAN and CUT?
% 	Response: Mostly because generated quality is significantly better with AptSim2Real. The runtime difference is not much, but I will get that info. Since Sim2Real is an offline process (i.e. for generating the training data), its runtime isn’t super important and is usually not the main reason for choosing one algo vs the other.

% How can FID be a reasonable metric here, when the simulated baseline has higher FID than CycleGAN and has parity FID with CUT? By eye it seems clear that the CUT/CycleGAN baselines look better than the simulation baseline, so any metric that shows the opposite trend could be argued to be an (empirically) poor fit here.
% Response: 
% FID is a standard metric for evaluating image quality. CycleGAN and FID both have a lot of artifacts (e.g. checkerboard pattern, washed out details on foliage), which causes FID scores to be lower. While developing the algorithm, we found it to be quite correlated with the visual quality.

% Specific Comments

% Note: I made these notes while I read through, so it’s possible some of them were addressed later in the manuscript. But even in those cases it may be wise to put something in the paper earlier reassuring the reader that more details are coming later, because otherwise they are left hanging in uncertainty.

% Fig 1: I think this figure is dangerously misleading - by labeling the third row “AptSim2Real” you make it seem like the image on the right is an output of your model, and not just an approximate paired image in your dataset. I would actually suggest using this figure to showcase a AptSim2Real actual result instead of purely just displaying ground truth, since it is quite prime real estate.
% Response: For all the methods, we have put samples from source and target domain; not the output of the algorithm. Updated the figure text to avoid confusion.
% Fig 1: Should put the dividing lines between images horizontal instead of vertical. It’s a bit confusing for the reader since we’re expected to compare the two images across the line which is nonintuitive.
% Response: Thanks, updated the figure. 


% Intro: In general, comparisons to paired image generation seem irrelevant because you don’t compare to any paired image generation baselines later.
% Response: You are right we don’t compare with paired methods. The main message is that it’s very hard to get the paired data in sim-to-real applications. Revised the text to make the message clear.

% L78: “But, due to limitations of the simulation, a perfect match is never achieved.” - vague. What does perfect mean? You probably also shouldn’t say perfect unless you want to claim AptSim2Real will produce a “perfect match.”
% Response: Thanks, that’s a good point. Revised the text to “However, due to limitations of the simulation, there remains a distribution gap between the simulated and real images. To bridge this gap, …”

% ”

% L97: is it a “method” or a “category”? Need to be consistent.
% Response: Thank, fixed it to “category”

% L149: Don’t say “etc.” Just put the metadata you need to use.
%   Charles: Made a revision. Please Review.

% The paragraph beginning L141 is not precise enough. For example, take this line starting “We match the lighting…” There isn’t any information on how lighting is matched, or how weather effects are included, just that they are accounted for in some vague way using an environment map. Given that using a Cruise dataset will make reproduction of these experiments difficult you need to be as specific and precise as humanly possible. 
% There is no information on what graphics engine you use. Should we put that in, or is that impossible due to IP concerns?
% L181-183: You say you pick a style image randomly, and then in the next sentence say you pick a style image based on a timestamp. Those seem contradictory.
%   Charles: Oops, reworded to make this consistent. 
% Fig 2: I don’t think it’s that necessary to have an entirely new panel for the inference diagram, as it’s pretty clear that at inference the losses are turned off.
% L195: Once again I think this is too imprecise. How do you capture stuff like contrast in a style embedding? Is there a special training trick? 
%   Charles: Reworded to make it clearer that we don't explicitly model what to capture, but we observe in experimentation that this is what we see captured
% L207: Remove “we employ several methods” 
%   Charles: Seems to be gone
% L208: You already said you use pooling layers to remove spatial content in the previous paragraph. Can you combine these two sentences somehow so you don’t say the same thing twice?
%   Charles: Removed the first mention on L206
% L222: This still seems vague. Is it reasonable to assume people within this subfield will know what you mean by “modulate” and “demodulate”? 
% L225: Don’t understand this. The style difference influences the number of weights?
%   Charles: Added details to modulation & demodulation. Please review
% L246: Don’t need to say it’s the sigmoid function - just give the functional form. 
%   Charles: Not sure about this one - actually quite like it when papers say what the formula is (not just the formula) 
% In general, I am a bit confused how you can enforce that dS is meaningful. I see that you trained explicitly on the boundary condition where dS = 0, but why should the generator find that dS = 1 is different from dS = 2? This could be made clearer.
% L287: Remove “such as” and just be precise.
% L303: “maintain the pairing” = bad wording
%    Charles: Made a revision
% There isn’t any information on how the baselines were trained. Were they trained using the same encoder/decoder network architectures? The same amount of compute, training time, etc.? It would be useful to have some way to reassure the reader that the baseline comparisons are scientifically valid.
% Fig 5: To what extent could it be argued that the baseline artifacts are just because the baselines were not trained optimally? I’ve seen those kinds of convnet artifacts plenty before but I feel you can usually spend some effort to smooth them out. 
% Fig 5: I think there needs to be a clearer argument as to why the AptSim2Real results are better. Sure I can see the artifacts when they are pointed out, but I could also argue that the foliage in the CycleGAN trees look better because the AptSim2Real trees are a bit overly high-contrast. 
% Table 1: I have concerns this is not really a fair comparison, given that both your baselines are unpaired and your test dataset is specifically in this “approximate paired” setting. For instance, it is a bit of an orange flag to me that CycleGAN actually does worse than the simulation baseline, and CUT basically performs at parity. And yet to the human eye it feels like other than some artifacts the CUT and CycleGAN results look pretty on par (or at least close!) from the realism angle compared to the AptSim2Real results (Fig 5). This makes me think that either FID is a poor metric in this case or the way the dataset was collected does not make this a great comparison. This needs to be accounted for.
% Table 2: Removing NCE loss actually makes AptSim2Real worse than both the CycleGAN and CUT baselines and worse than the simulation baseline. Once again this seems a bit strange and brings into question why looking at FID is a good idea here. 
% In general, error bars need to be put on all FID scores.
% L450: Seems a bit odd calling it a “category” of algorithms when you presented one algorithm. Probably should change this.
% Conclusion: overuse of non-scientific descriptive adjectives like “high-quality” and “innovative” and “promising.”

% Simulation technology has advanced in leaps and bounds in recent years. Especially for modern safety-critical robotics applications such as autonomous surgical robots \cite{haiderbhai2022dvrksim2real} and autonomous vehicles \cite{dignet}, simulation data is critical for training and validating machine learning models where real world data is otherwise too expensive, difficult, or prohibitive to obtain. Simulation data provides a safe and salable way of generating novel training data that can be used to either validate entire robotic systems virtually, or to provide additional data to train data-hungry object detection and prediction models that allow robots to understand the world. Prior works have shown the effectiveness of synthetic data for training models that transfer to the real world \cite{wood2021fake} \cite{haiderbhai2022dvrksim2real}. But, despite the rapid progression of computer graphics and simulation technology, there still exists a domain gap when training on simulation data versus real world data \cite{domainrandomization}. Dataset quality is critical to the performance of a model. A systematic gap between simulation training data and real world data will lead to worse performance in the real world. This paper will focus on improving the quality and realism of simulation data for computer vision tasks. 

Recent improvements in simulation technology have made it a vital component in the training and validation of machine learning models for robotics applications, especially for tasks where real world data is costly or impossible to collect.
% such as robotic locomotion~\cite{haiderbhai2022dvrksim2real} and autonomous vehicles~\cite{dignet},
For example, OpenAI demonstrated that models trained only in simulation can be used to solve a Rubik's cube with a robotic hand \cite{akkaya2019solving}, researchers at ETH Zurich were able to learn complex quadrupedal locomotion using simulation data \cite{lee2020learning}, and Microsoft recently released a simulation dataset for learning localization models for drones \cite{wang2020tartanair}.
In particular, training visual detection and understanding algorithms on synthetic image data can produce immense gains for a critical part of a robotic system.
%as they are critical components of a robotic system and image data is difficult to label in a salable way. 
%In contrast, perfect 3d labels for training can be easily extracted from the simulator.
%Previous studies have shown that enhancing the quality of synthetic data improves the performance of downstream models on real data. (\cite{wood2021fake, haiderbhai2022dvrksim2real, simgan2017}). 
Previous studies have shown that enhancing the quality of synthetic data using sim-to-real domain adaptation techniques improves the performance of downstream models on real data. (\cite{wood2021fake, haiderbhai2022dvrksim2real, simgan2017}). 
%Simulation also allows for end-to-end validation of robotic systems through virtual mileage accumulation.
%Simulation data provides a cost-effective and scalable solution for generating new training data and validating object detection and prediction models in robotics and other applications.

% But, despite advancements in graphics simulation technology, a persistent gap remains between synthetic and real-world images, affecting the performance of machine learning models. 
% This paper addresses this challenge by focusing on improving the realism and quality of simulation data for computer vision tasks focused on robotics application.

% Current literature for this simulation to real world domain transfer (sim2real), focuses around two primary problem formulations. In Paired Image Translation \cite{pix2pix2017, andonian2021contrastive, Qu_2019_CVPR}, a model is trained to translate images from a source domain to the target domain. The model is given a datset where each image from the source domain has a corresponding paired image in the target domain where the paired images share a pixel-wise correspondence. For example, models trained to translate semantic segmentation labels to real world labels (see Figure \ref{fig:apt}). Paired image translation problems can be solved with relatively simple models and high accuracy. But, it's very difficult and expensive to generate data with a pixelwise correspondence. For many applications, paired data may be unavailable \cite{CycleGAN2017}. In Unpaired image translation \cite{CycleGAN2017, park2020cut, stylegan, Karras2019stylegan2, simgan2017}, the dataset the model is trained on has no correspondence. For example, translating Horses to Zebras (see Figure \ref{fig:apt}). The unpaired problem formulation is more difficult, but has a much higher availability of data.


Current research in sim-to-real domain adaptation focuses on two main approaches: paired image translation and unpaired image translation. Paired methods train a model to translate images between source and target domains where each source image has a corresponding paired target image with pixel-wise correspondences. 
This approach results in high accuracy, but paired data is often difficult and expensive to obtain. 
In contrast, the unpaired image translation methods train a model with no correspondences between source and target images.
This approach is more challenging but benefits from a higher availability of data. 

% In this paper, we propose a novel problem formulation for sim-to-real domain transfer: Approximately-paired Image Translation  (APT). In Approximatley Paired Image Translation, the model is given data where there is an "approximate pairing" between images in the source and target domains. These approximately paired images share implicit contextual images (such as the camera angle, scene composition, position of assets, and lighting). However, there is no direct pixel to pixel difference; assets, building textures, and building shapes may vary. By maintaining a form of pairing but relaxing constraints compared to fully paired data, approximately paired data leverages the strengths of both paired and unpaired data. Like unpaired data, approximately paired data can be generated salable with current simulation technology. At Cruise, vehicles, pedestrians, and other actors are labelled after being collected on-road. A proprietary simulator is then able to quickly and automatically re-construct the scene in simulation using label position information and metadata from the vehicle that recorded the scene. Additionally, the approximately paired data provides implicit contextual information around content/style separation that the model can leverage to simplify the problem. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\columnwidth]{motivation}
    \caption{
    % Approximately Paired Image Translation: In contrast to paired or unpaired methods which require either a one-to-one pixel-wise correspondence or no correspondence at all, approximately paired images are paired based on semantic scene composition. Approximate pairing provides greater supervision than unpaired data while maintaining data scalability - a key limitation for paired methods.  
    \textbf{AptSim2Real} differs from traditional paired or unpaired methods by rendering simulated images with similar camera pose, background, lighting, and scene composition to real images, resulting in an approximate pairing between the two.
The approximately-paired samples enable the use of stronger supervision compared to unpaired methods during the learning process, while still retaining data scalability, a challenge faced by traditional paired methods.
\vspace{-0.2in}
    }
    \label{fig:apt}
\end{figure}

In this paper, we introduce a novel approach to sim-to-real domain transfer called \emph{a}pproximately-\emph{p}aired image \emph{t}ranslation  (AptSim2Real).
Unlike paired or unpaired methods, this approach utilizes ``approximately-paired" data that shares contextual information such as camera pose, map location, scene composition, and lighting while allowing some variations in assets, textures, appearance, and shapes (see Figure~\ref{fig:apt}). 
For each real image, we use metadata and label information to generate a corresponding simulated image in a graphics engine.
%Simulated images mirroring real images' camera pose are generated by combining assets selected based on real image information and context, with a procedurally generated background and matched lighting.
Simulated images mirror the camera pose of real images and are generated using assets and models similar to those in the real image, a procedurally generated background, and matching lighting.
%Like unpaired translation, this approach allows for scalable data generation while providing some pairing between the source and target images that can be drawn upon in the model architecture.
This approach allows for scalable data generation, like unpaired translation, while additionally providing some pairing between the source and target images that the model can take advantage of. 
We demonstrate that leveraging approximate pairings between source and target domain samples can lead to a superior model architecture and better performance compared to unpaired translation methods.

% Our AptSim2Real approach leverages approximately paired data to improve the realism of simulation images. 
% Starting with a simple image-to-image Generative Adversarial Network (GAN) framework similar to CUT \cite{park2020cut} and CycleGAN \cite{CycleGAN2017}, we improve generated image quality by 8\% percent over the baseline by training on approximately paired data instead of unpaired data. We also introduce a novel a novel network architecture that leveraging style mixing targeted for approximately paired data that results in a overall 34\% improvement in FID over the baseline.

% Our AptSim2Real approach leverages approximately paired data to improve the realism of simulation images. 
% Starting with a simple image-to-image Generative Adversarial Network (GAN) framework similar to CUT \cite{park2020cut} and CycleGAN \cite{CycleGAN2017}, we improve generated image quality by 8\% percent over the baseline by training on approximately paired data instead of unpaired data. We also introduce a novel a novel network architecture that leveraging style mixing targeted for approximately paired data that results in a overall 34\% improvement in FID over the baseline.

The goal of sim-to-real training is to mimic the real images as closely as possible. 
%However, due to limitations of the simulation, there remains a distribution gap between the simulated and real images.
Due to limitations of simulation technology, there is a significant distribution gap between the simulated and real images.
% To bridge this gap, We compute the difference between the simulation and real image styles using a learned style encoder.
% The style encoder is designed so that the difference of style features can capture the differences in lighting, contrast, camera noise, scene fidelity, and other details. 
Our method bridges this gap by exploiting approximately-paired data using style mixing \cite{pmlr-style-equalization}.
During the training process, a style encoder network learns to encode the ``realism gap" between a simulated image and its corresponding approximately-paired real image into a style difference feature. 
% This enables the network to capture and reproduce the visual style of real images, even when working with simulated data.
This style difference is used as an additional input to a generator network to guide image translation and improve the realism of the simulated image.
%Unlike previous image translation methods (e.g.~\cite{simgan2017, park2020cut, CycleGAN2017}), our approach leverages an additional style input and makes the translation task easier by only requiring the model to learn how to apply the difference, rather than what the difference is. 
% Compared to previous image translation methods (e.g.~\cite{simgan2017, park2020cut, CycleGAN2017}), our method's inclusion of an additional style input simplifies the translation task by disentangling the two separate problems of identifying and fixing the realism gap.
% Existing translation models must encode the domain difference in their parameters, which makes the translation task more challenging. 
While prior image translation methods such as \cite{simgan2017, park2020cut, CycleGAN2017} encode the style difference within their parameters, we take a different approach by presenting this difference as an additional style input. 
This alternative strategy not only simplifies the translation task but also provides greater flexibility and control over the style transfer process.
% This explicit style input simplifies the translation task by disentangling the two separate problems of identifying and fixing the realism gap.
% Existing translation models must encode the domain difference in their parameters, which makes the translation task more challenging. 
Note that approximate pairing between the simulated input image and the generated image is crucial for computing the style difference and, therefore, is central to our architecture design.
Following are our main contributions:
% At traing time, style of the desired real images could be different 
% - we know that the style of the sim and the real image should be the same. 
% - Training will be easier 
% - Explicitly mention minibatch selection 
% - Finding the target style is hard (operand and )

% \textbf{Contributions}
\begin{itemize}
    \item We propose approximately-paired image translation -- a novel image translation category for improving the realism of simulated images.

   \item We propose a novel training strategy that leverages the approximate-pairing between the source and target domain by utilizing the latest development in GAN methods.

    \item We conduct extensive qualitative and quantitative experiments to demonstrate that approximately paired images translation can be used to produce results significantly more realistic than existing unpaired methods.

%    \item We conduct extensive qualitative and quantitative experiments and demonstrate that the proposed method works significantly better than existing unpaired methods.

\end{itemize}


% 1. Problem
% - Robotics problem -> object detection & 
% - Training with sim data is important (but not too much emphasis on it) 
% - Development in graphics technology and simulation, important tool in data generation 
% Cite data generation and training papers. These works have shown that synthetic data can be used for training downstrema tasks
% - Ashish's CVPR paper apple eye dataset (sim learning from sim & unsupervised) 
% - Peter biel 
% - hand tracking microsoft
% Quality of data is important for downstream task performance. This paper will focus on improving the quality of simulation data
% 2. What exists today (high level):  image to image translations & gans 
% - issues with the above
% - too strong of an assumption 
% - Content & Style separation
% 3. Introduce approximate pairing. Why is this possible & why it's important. Operation and operand 
% 4. High level overview of the method (style difference) 4-5 lines. Not super precise, but a flavour.
% 5. In summary, main contributions (2-3 bullet points) 
% - Proposed a new category of probem 
% - Architecture that leverages approximate pairing 
% - Experiments that show improvement to quality (and numerics) 

% Note: Cite IROS & robotics papers 
% - Most important iros papers in recent years 

