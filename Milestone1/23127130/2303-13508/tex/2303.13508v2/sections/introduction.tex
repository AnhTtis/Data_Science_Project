\vspace{-2mm}
\section{Introduction}
\vspace{-2mm}

Text-to-Image (T2I) generative models~\cite{imagen,dalle2,ldm,muse} have greatly expanded the ways we can create and edit visual content. 
Recent works~\cite{poole2022dreamfusion,magic3d,latent-nerf,jacobiannet} have demonstrated high-quality Text-to-3D generation by optimizing neural radiance fields (NeRFs)~\cite{nerf} using the T2I diffusion models. Such automatic 3D asset creation with input text prompts alone has applications in a wide range of areas, such as graphics, VR, movies, and gaming.

Although text prompts allow for some degree of control over the generated 3D asset, it is often difficult to precisely control its identity, geometry, and appearance solely with text. 
In particular, these methods lack the ability to generate 3D assets of a specific subject (e.g., a specific dog instead of a generic dog). Enabling the generation of subject-specific 3D assets would significantly ease the workflow for artists and 3D acquisition.
There has been remarkable success~\cite{dreambooth_cite,TI,customdiffusion} in personalizing T2I models for subject-specific 2D image generation. These techniques allow the generation of specific subject images in varying contexts, but they do not generate 3D assets or afford any 3D control, such as viewpoint changes.

In this work, we propose `\Method', a method for subject-driven Text-to-3D generation. Given a few (3-6) casual image captures of a subject (without any additional information such as camera pose), we generate subject-specific 3D assets that also adhere to the contextualization provided in the input text prompts. That is, we can generate 3D assets with geometric and appearance identity of a given subject while also respecting the variations (e.g. sleeping or jumping dog) provided by the input text prompt.


For \Method, we draw inspiration from the recent works~\cite{poole2022dreamfusion} which propose optimizing a NeRF model using a loss derived from T2I diffusion models.
We observe that simply personalizing a T2I model for a given subject and then using that model to optimize a NeRF is prone to several failure modes.
A key issue is that the personalized T2I models tend to overfit to the camera viewpoints that are only present in the sparse subject images. As a result, the resulting loss from such personalized T2I models is not sufficient to optimize a coherent 3D NeRF asset from arbitrary continuous viewpoints.

With \Method, we propose an effective optimization scheme where we optimize both a NeRF asset and T2I model in conjunction with each other to jointly make them subject-specific.
We leverage DreamFusion~\cite{poole2022dreamfusion} for NeRF optimization and use DreamBooth~\cite{dreambooth_cite} for T2I model fine-tuning.
Specifically, we propose a 3-stage optimization framework where in the first stage, we partially finetune a DreamBooth model and then use DreamFusion to optimize a NeRF asset.
The partially finetuned DreamBooth model does not overfit to the given subject views, but also do not capture all the subject-specific details. So the resulting NeRF asset is 3D coherent, but is not subject-specific.
In the second stage, we fully finetune a DreamBooth model to capture fine subject details and use that model to create multiview pseudo-subject images. That is, we translate multiview renderings from the trained NeRF into subject images using the fully-trained DreamBooth model.
In the final stage, we further optimize the DreamBooth model using both the given subject images along with the pseudo multi-view images; which is then used to optimize our final NeRF 3D volume.
In addition, we also use a weak reconstruction loss over the pseudo multi-view dataset to further regularize the final NeRF optimization.
The synergistic optimization of the NeRF and T2I models prevents degenerate solutions and avoids overfitting of the DreamBooth model to specific views of the subject, while ensuring that the resulting NeRF model is faithful to the subject's identity.


For experimental analysis, we use the dataset of 30 subjects proposed in DreamBooth~\cite{dreambooth_cite} which uses the same input setting of sparse casual subject captures.
Results indicate our approach can generate realistic 3D assets with high likeness to a given subject while also respecting the contexts present in the input text prompts. Fig.~\ref{fig:teaser} shows sample results of \Method~ on different subjects and contextualizations.
When compared to several baselines, both quantitative and qualitative results demonstrate that \Method~ generations are more 3D coherent and better capture subject details.

