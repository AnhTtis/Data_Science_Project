\section{Experiments}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/visual_comparisons.pdf}
    \caption{\textbf{Visual Results} on 5 different subjects with two baseline techniques of Latent-NeRF and DreamBooth+Fusion along with those of our technique (DreamBooth3D).
    Results clearly indicate better 3D consistent results with our approach compared to either of the baseline techniques. See the supplement for additional visualizations and videos.}
    \label{fig:visual_comparisons}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\linewidth]{images/initial_final_nerf.pdf}
    \caption{\textbf{Initial vs. Final NeRF Estimates}. Sample multi-view results show that the initial NeRF obtained after stage-1 has only a partial likeness to the given subject whereas the final NeRF from stage-3 of our pipeline has better subject-identity.}
    \label{fig:init_final_nerf}
\end{figure*}

\noindent \textbf{Implementation Details}.
We use the Imagen~\cite{imagen} 
T2I model in our experiments. The Imagen model uses the T5-XXL\cite{t5_transformer} language model for text encoding.
On the NeRF side, we use DreamFusion~\cite{poole2022dreamfusion}. Our model takes around 3 hours per prompt to complete all the 3 stages of the optimization on a 4 core TPUv4. 
We use a fixed 150 iterations to train the partial DreamBooth model $\hat{\mathcal{D}}_{\theta}^{partial}$.
For the full DreamBooth $\hat{\mathcal{D}}_\theta$ training, we use 800 iterations,
which we find to be optimal across different subjects.  
We render 20 images uniformly sampled at a fixed radius from the origin for pseudo multi-view data generation. We finetune the partially trained $\hat{\mathcal{D}}_\theta^*$ for additional 150 iterations in Stage 3. Refer to the supplementary material for more hyperparameter details.

\vspace{1mm}
\noindent \textbf{Datasets}.
We train our personalized text to 3D models on the image collections released by the authors of \cite{dreambooth_cite}. This dataset consists of 30 different image collections with 4-6 casual captures of a wide variety of subjects (dogs, toys, backpack, sunglasses, cartoon etc.). 
We additionally capture few images of some rare objects (like ``owl showpiece'' in Fig.~\ref{fig:init_final_nerf}) to analyze performance on rare objects. Further, we optimize each 3D model on 3--6 prompts to demonstrate 3D contextualizations.

\vspace{1mm}
\noindent \textbf{Baselines}.
We consider two main baselines for comparisons. 
Latent-NeRF \cite{latent-nerf} which learns a 3D NeRF model on a latent feature space instead of in RGB pixel space, using an SDS loss in the latent space of Stable Diffusion~\cite{ldm}.
As a baseline, we run Latent-NeRF using the fully dreamboothed T2I model and refer to it as ``Latent-NeRF'' or ``L-NeRF" in our experiments.
We further compare against a single stage DreamFusion+DreamBooth approach where we first train a DreamBooth diffusion model followed by 3D NeRF optimization using DreamFusion.
We refer to our results as ``\Method'' or ``\method'' in the experiments.

\vspace{1mm}
\noindent \textbf{Evaluation Metrics}. 
We evaluate our approach with the CLIP R-Precision metric, which measures how accurately we can retrieve a text prompt from an image \cite{park2021benchmark}. Similar to \cite{poole2022dreamfusion}, we compute the average CLIP R-Precision over 160 evenly spaces azimuth renders at a fixed elevation of 40 degrees. The CLIP models used for evaluation are the CLIP ViT-B/16, ViT-B/32, and ViT-L-14 models.
Since these CLIP metrics can only approximately capture the quality and subject-fidelity of the generated 3D assets, we additionally perform user studies comparing different results.

\subsection{Results}
\vspace{-2mm}

\noindent \textbf{Visual Results}. 
Fig.~\ref{fig:teaser} shows sample visual results of our approach along with different semantic variations and contextualizations. Results demonstrate high-quality geometry estimation with \Method~for even our uncommon owl object.
Contexualization examples demonstrate that \Method~faithfully respects the context present in the input text prompt. Fig.~\ref{fig:visual_comparisons} shows sample results of our approach in comparison to those of Latent-NeRF and DreamBooth+Fusion baselines. Even though Latent-NeRF works reasonably well in some cases (such as rubber duck in Fig.~\ref{fig:visual_comparisons}), more often it fails to converge to a coherent 3D model with reasonable shapes.
In several cases, DreamBooth+Fusion usually produces the 3D assets with Janus problem (same appearance and geometry imprinted across different view angles).
\Method, on other hand, consistently produces 360$^{\circ}$ consistent 3D assets while capturing both the geometric and appearance details of the given subject.

\vspace{1mm}
\noindent \textbf{Quantitative Comparisons}.
Table.~\ref{tab:r_precision_metrics} shows CLIP R-precision metrics for naive DreamBooth+Fusion (as baseline) and our DreamBooth3D generations. Results clearly demonstrate significantly higher scores for the DreamBooth3D results indicating better 3D consistency and text-prompt alignment of our results.

\vspace{1mm}
\noindent \textbf{Initial vs. Final NeRF}.
Fig.~\ref{fig:init_final_nerf} shows sample initial and final NeRF results generated after stages 1 and 3 of our pipeline. As the visual results illustrate, initial-NeRFs only have partial likeness to the given subject, but are consistent in 3D. The final NeRFs from the stage-3 has better likeness to the given subject while retaining the consistent 3D structure. These examples demonstrate the need for the 3-stage optimization in \Method.

\vspace{1mm}
\noindent \textbf{User Study}. We conduct pairwise user studies comparing DreamBooth3D to baselines in order to evaluate our method under three axes: (1) Subject fidelity, where users are asked to answer the question ``Which 3D item looks more like the original subject?’’; (2) 3D consistency and plausibility where users answer ``Which 3D item has a more plausible and consistent geometry?’’ and (3) Prompt fidelity to the input prompts where users answer ``Which video best respects the provided prompt?". Users can choose either our method or the baseline, or a third option ``Cannot determine / both equally’’.
For the first two user studies on 3D consistency and subject fidelity we compare rotating video results, one for each of the 30 subjects in the dataset and ask 11 users to vote for each pair. For the prompt fidelity study, we generate videos for 54 unique prompt and subject pairs and ask 21 users to respond.
We compute final results using majority voting and present them in Figure~\ref{fig:user_study}.  We find that DreamBooth3D is significantly preferred over the baselines in terms of 3D consistency, subject fidelity as well as prompt fidelity.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/user_study_3.pdf}
    \caption{\textbf{User Study}. Users show a significant preference for our DreamBooth3D over DB+DF and L-NeRF for 3D consistency, subject fidelity and prompt fidelity.}
    \label{fig:user_study}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{images/recontext_visuals.pdf}
    \caption{\textbf{3D Recontextualizations with DreamBooth3D}. With simple edits in the text prompt, we can generate non-rigid 3D articulations and deformations that correspond to the semantics of the input text. Visuals show consistent contexualization of different dogs in different contexts of sitting, sleeping and jumping. See the supplement for videos.}
    \label{fig:recontext}
\end{figure}

\vspace{1mm}

\subsection{Sample Applications}
\vspace{-2mm}

\Method~can faithfully represent the context present in the text prompts while also preserving the subject identity. With simple changes in the text-prompt, \Method~enables many interesting 3D applications, several of which would otherwise require tedious manual effort to tackle using traditional 3D modeling techniques.

\noindent \textbf{Recontextualization}.
Fig.~\ref{fig:recontext} shows sample results on different dog subjects, where we
recontextualize the 3D dog models with simple prompts of sitting, sleeping and jumping.
As the visuals demonstrate, the corresponding 3D models consistently respect the given context
in the text prompt across all the subjects. In addition, the 3D articulations and local deformations
in the output 3D models are highly realistic even though several of these poses are unseen in the
input subject images.

\vspace{1mm}
\noindent \textbf{Color/Material Editing}.
Fig.~\ref{fig:applications} shows sample color editing results, where a pink backpack can be converted into a blue or green backpack with simple text prompts like `a [v] blue backpack'.
Similarly, one could also easily edit the material appearance of the 3D asset (for e.g., metal can to wodden can).
Refer to the supplementary material for more color and material editing results.

\vspace{1mm}
\noindent \textbf{Accessorization}.
Fig.~\ref{fig:applications} shows sample accessorization results on a cat subject, where we put on a tie or a suit into the 3D cat model output.
Likewise, one can think of other accessorizations like putting on a hat or sunglasses etc.

\vspace{1mm}
\noindent \textbf{Stylization}.
Fig.~\ref{fig:applications} also shows sample stylization results, where a cream colored shoe is stylized based on color and the addition of frills. 

\vspace{1mm}
\noindent \textbf{Cartoon-to-3D}.
A rather striking result we find during our experiments is that \Method~can even convert non-photorealistic subject images such as 2D flat cartoon images into plausible 3D shapes.
Fig.~\ref{fig:applications} shows a sample result where the resulting 3D model for the red cartoon character is plausible, even though all the images show the cartoon only from the front.
Refer to the supplementary material for more qualitative results on different applications.

\begin{table}[]
    \centering
    \small
    \begin{tabular}{@{\,\,}l@{\,\,\,\,}c@{\,\,\,\,}c@{\,\,\,\,}c@{\,\,}}
    \toprule
     & ViT-B/16$\uparrow$ & ViT-B/32$\uparrow$ & ViT-L-14$\uparrow$ \\
     \midrule
    DreamBooth+Fusion & 0.509 & 0.490 & 0.506 \\
    DreamBooth3D (Ours) & \textbf{0.783} & \textbf{0.710} & \textbf{0.797}  \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Quantitative comparisons} using CLIP R-precision on DreamBooth+Fusion (baseline) and DreamBooth3D generations indicate that renderings from our 3D model outputs more accurately resemble the text prompts.}
    \label{tab:r_precision_metrics}
\end{table}

\vspace{1mm}
\subsection{Limitations}
While our method allows for high-quality 3D asset creation of a given subject and improves over prior work, we observe several limitations.
First, the optimized 3D representations are sometimes oversaturated and oversmoothed, which is partially caused by SDS-based optimization with high guidance weighting~\cite{poole2022dreamfusion}. This is also a result of being restricted to a relatively low image resolution of $64 \times 64$ pixels. Improvements in the efficiency of both diffusion models and neural rendering will potentially allow for scaling to higher resolutions.  
Furthermore, the optimized 3D representations can sometimes suffer from the Janus problem of appearing to be front-facing from multiple inconsistent viewpoints if the input images do not contain any viewpoint variations.
Finally, our model sometimes struggles to reconstruct thin object structures like sunglasses. 
Fig.~\ref{fig:limitations} shows a couple of failure results.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{images/application_visuals.pdf}
    \caption{\textbf{Sample Applications}. DreamBooth3D's subject preservation and faithfulness to the text prompt enables several applications such as color/material editing, accessorization, stylization, \etc. \Method~can even produce plausible 3D models from unrealistic cartoon images. See the supplemental material for videos.}
    \label{fig:applications}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{images/Limitation.pdf}
    \caption{\textbf{Sample Failure Cases}. We observe~\Method~often fails to reconstruct thin object structures like sunglasses, and sometimes fails to reconstruct objects with not enough view variation in the input images.}
    \label{fig:limitations}
\end{figure}



