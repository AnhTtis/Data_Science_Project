\section{Conclusion}
\vspace{-2mm}

In this paper, we have proposed \Method~, a method for subject-driven text-to-3D generation. Given a few (3-6) casual image captures of a subject (without any additional information such as camera pose), we generate subject-specific 3D assets that also adhere to the contextualization provided in the input text prompts (e.g. sleeping, jumping, red, etc.). 
Our extensive experiments on the DreamBooth dataset~\cite{dreambooth_cite} have shown that our method can generate realistic 3D assets with high likeness to a given subject while also respecting the contexts present in the input text prompts. Our method outperforms several baselines in both quantitative and qualitative evaluations. In the future, we plan to continue to improve the photorealism and controllability of subject-driven 3D generation. 