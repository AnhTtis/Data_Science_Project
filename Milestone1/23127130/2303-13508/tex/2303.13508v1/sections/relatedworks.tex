\section{Related Works}


\noindent \textbf{Text-to-Image Generation.}
Earlier works on generative models are dominated by Generative Adversarial Networks (GANs) which train a generator to synthesis images that are indistinguishable from real images ~\cite{goodfellow2020generative, sauer2023stylegan}. Other generative approaches include autoregressive models that generate images pixel by pixel or patch by patch~\cite{yu2022scaling,gafni2022make} and
masked image models that iteratively predict the marginal distribution of masked patches in the image~\cite{muse,chang2022maskgit}. Recently, denoising diffusion models ~\cite{ho2020denoising} have been proposed for image synthesis, which can generate high-quality images by iteratively denoising a noise image toward a clean image \cite{imagen,dalle2,ldm,dhariwal2021diffusion}. Diffusion models can also be conditioned on various inputs such as depth-map~\cite{control_net}, sketch~\cite{voynov2022sketch}, semantic segmentation~\cite{ldm, bar2023multidiffusion}, text\cite{nichol2022glide,imagen,dalle2,ldm} and others \cite{huang2023composer, control_net, li2023gligen}.
For text conditioning, these models take advantage of pre-trained large language models (LLMs)~\cite{clip,t5_transformer} in order to generate images that are aligned with a natural language text prompt given by the user.
Motivated by the success of T2I diffusion models, many works utilize pre-trained T2I models for various tasks such as text-based image manipulation \cite{kawar2022imagic,mokady2022null,brooks2022instructpix2pix}.


\vspace{1mm}
\noindent \textbf{3D Generation.}
First works on learning-based 3D content generation performed 3D reconstruction from one or multiple images~\cite{Choy2016ECCV,Mescheder2019CVPR,Fan2017CVPR,Wen2019ICCV,Gkioxari2019ICCV}.
While leading to good reconstruction results, they require large-scale datasets of accurate 3D data for training which limits their use in real-world scenarios.
Another line of work~\cite{Schwarz2021NEURIPS,piGAN2021,Niemeyer2021CVPR,egthreed,Gu2022ICLR} circumvents the need for accurate 3D data by training 3D-aware generative models from image collections.
While achieving impressive results, these methods are sensitive to the assumed pose distribution and restricted to single object classes. 
Very recently, text-to-3D methods~\cite{dreamfields,poole2022dreamfusion,magic3d,latent-nerf} have been proposed that can generate 3D assets from text prompts by utilizing large pretrained T2I diffusion models.
In many applications, however, the conditioning are rather input images optionally with text instead of pure text.
As a result, multiple works investigate how input images can be incorporated into the optimization pipeline, \eg by applying a reconstruction loss on the input image and predicted monocular depth~\cite{Xu2022ARXIV,Deng2022ARXIV} or a predicted object mask~\cite{real_fusion}.
This, however, limits their use as it does not exploit the full strength of diffusion models, \eg, the object cannot be recontextualized with additional text input.   
Instead, we propose to not directly reconstruct the input image, but rather the concept of the provided object. This allows not only for reconstruction, but also for recontextualization and more, and the input images do not need to be taken with the same background, lighting, camera \etc.

\vspace{1mm}
\noindent \textbf{Subject-driven Generation.}
Recent advances in subject-driven image generation \cite{dreambooth_cite, TI, customdiffusion} enable users to personalize their image generation for specific subjects and concepts. This has provided T2I models with the ability to capture the visual essence of specific subjects and synthesize novel renditions of them in different contexts. DreamBooth \cite{dreambooth_cite} accomplishes this by expanding the language-vision dictionary of the model using rare tokens, model finetuning, and a prior preservation loss for regularization. Textual Inversion \cite{TI} accomplishes this by optimizing for a new "word" in the embedding space of a pre-trained text-to-image model that represents the input concept. It's worth noting that these methods do not generate 3D assets or 3D coherent images. There have also been developments in guiding image generation with grounding inputs \cite{li2023gligen}, editing instructions \cite{brooks2022instructpix2pix}, and task-specific conditions such as edges, depth, and surface normals \cite{control_net}. However, these techniques do not provide personalization to specific subjects, and do not generate 3D assets.

