\section{Methodology}\label{sec:method}
\noindent\textbf{Overview.} We first introduce our flipped cross-entropy (\textit{f-$\mathbb{CE}$}) loss through the tight connection with the mutual information between the model's predictions and the test images.
% \behzad{between teacher-student models' predictions?}
(Sec. \ref{sec:sl_mi}). Using this equivalence, we derive our test-time loss function that implicitly incorporates teacher-student knowledge distillation. In Sec. \ref{sec:kd_da}, we propose to enhance the test-time teacher-student knowledge distillation by utilizing the consistency of the student model's predictions on the proposed adversarial augmentations with their corresponding refined soft-pseudo labels from the teacher model. We refine the soft-pseudo labels by averaging the teacher model's predictions on (1) weakly augmented test images; (2) nearest neighbors in the feature space. Furthermore, we propose an efficient online algorithm for learning adversarial augmentations (Sec. \ref{sec:leanable_aug}). Finally, in Sec. \ref{sec:TeSLA}, we summarize our online test-time adaptation method, \textbf{TeSLA}, based on self-learning with the proposed automatic adversarial augmentation. The overall framework is shown in Fig. \ref{fig:ours_main}. 

\begin{figure*}[t!]
    \centering
    \includegraphics{figures/self_learning_v2.pdf}
    \caption{\textbf{Overview of TeSLA Framework.} (a) The student model $f_s$  is adapted on the test images by minimizing the proposed test-time objective $\mathcal{L}_\text{pl}$. The high-quality soft-pseudo labels required by $\mathcal{L}_\text{pl}$ are obtained from the exponentially weighted averaged teacher model $f_t$ and refined using the proposed Soft-Pseudo Label Refinement (PLR) on the corresponding test images. The soft-pseudo labels are further utilized for teacher-student knowledge distillation via $\mathcal{L}_\text{kd}$ on the adversarially augmented views of the test images. (b) The adversarial augmentations are obtained by applying learned sub-policies sampled i.i.d from $\mathbb{P}$ using the probability distribution $P$ with their corresponding magnitudes selected from $M$. The parameters $M$ and $P$ of the augmentation module are updated by the \textit{unbiased gradient estimator} (Eq. \ref{eq:policy_update}) of the loss $\mathcal{L}_\text{Aug}$ computed on the augmented test images. \vspace{-0.5em}}
    % \caption{\textbf{TeSLA Framework:} (a) The student model $f_s$ is trained by minimizing the cross-entropy of predictions on the unlabeled test images from their corresponding soft pseudo-labels $\mathcal{L}_\text{pl}$ (Eq. \ref{eq:loss_pl}) obtained from the teacher model $f_t$ and subsequently refined by averaging predictions of $n$-nearest neighbors from the memory queue $\mathbf{Q}$ (Eq. \ref{eq:neighbors}). (b) Concurrently, we augment the online batch of test images by applying learned sub-policies sampled i.i.d from $\mathbb{P}$ using probability distribution $P$ with their corresponding magnitudes selected from $M$. The parameters $M$ and $P$ of the adversarial data augmentation module are updated by the \textit{unbiased gradient estimator} (Eq. \ref{eq:policy_update}). The student model $f_s$ is then trained to match its predictions on the augmented images with their corresponding soft pseudo-labels using $\mathcal{L}_\text{kd}$ (Eq. \ref{eq:loss_kd}).}
    \label{fig:ours_main}
\end{figure*}

\vspace{0.5em}
\noindent\textbf{Problem setup.} Given an existing model $f_{\theta _{0}}$, parametrized by $\theta _{0}$ and pre-trained on the inaccessible source data, we aim to improve its performance by updating all its parameters using the unlabeled streaming test data $\mathbf{X}$ during adaptation. Motivated by the concept of \textit{mean teacher} \cite{tarvainen2017mean} as the weight-averaged model over training steps that can yield a more accurate model, we build our online knowledge distillation framework upon teacher-student models. We utilize identical network architecture for teacher and student models. Each model $f=h\circ g$ comprises a backbone encoder $g: \mathbf{X}\to \mathbb{R}^D$ mapping unlabeled test image $\im \in \mathbf{X}$ to the feature representation $\dense=g\left (\im  \right )\in \mathbb{R}^D$ and a classifier (hypothesis) head $h: \mathbb{R}^D\to \mathbb{R}^K$ mapping $\dense$ to the class prediction $\ps\in \mathbb{R}^K$, where $D$ and $K$ denote the feature dimension and number of classes. The parameters of teacher $\theta_t$ and student $\theta_s$ are first initialized from the source model parameters $\theta _{0}$. Then the teacher model's parameters $\theta_t$ are updated from the student model's parameters $\theta_s$ using an exponential moving average (EMA) with momentum  coefficient $\alpha$ as: $\theta_t \leftarrow \alpha \cdot \theta_t + (1-\alpha) \cdot \theta_s$.
%
% \behzad{Despite previous works \cite{liu2021ttt,su2022revisiting} that rely on light-weight information from the source domain, we tackle the realistic scenario for test time adaptation without modification of source domain training in which the test data are given in an online streaming fashion, and the model must improve/update itself on the current batch of unlabeled test images.}
Following \cite{liang2020we}, we freeze the classifier $h$'s parameters and only update the encoder $g$'s parameters during test-time adaptation.
%\Devavrat{ %In section \ref{sec:kd_da}, we improve the test-time student-teacher knowledge distillation by utilizing the consistency of the student's predictions on the adversarial augmented images with their corresponding refined soft-pseudo labels.
% For improving the quality of the soft-pseudo labels,  Similar to \cite{chen2022contrastive}, we refine the quality of soft-pseudo labels by averaging the teacher's predictions on $n$-nearest neighbors in the feature space.
%}
%\Devavrat{In section \ref{sec:leanable_aug}, we present an efficient online algorithm for learning the optimal adversarial augmentations against the current teacher. Finally, in section \ref{sec:SLug} we summarize our online test-time adaptation method \textbf{SLug} based on Self-Learning with Adversarial Augmentation.}
%
%\subsection{Self-Learning and Mutual Information}\label{sec:sl_mi}
\subsection{Rationale Behind the \textit{f}-CE Objective}\label{sec:sl_mi}
We start by analyzing the rationale behind the proposed \textit{f-$\mathbb{CE}$} loss and its benefit for self-learning through the tight connection with the mutual information between the model's predictions and unlabeled test images. Before that, we first define the notations. 

\vspace{0.5em}
\noindent\textbf{Notations.} We define the random variables of unlabeled test images and the predictions from the student model $f_s$ as $\mathbf{X}$ and $\mathbf{Y}$ and those of the teacher model $f_t$’s soft pseudo-label predictions as $\hat{\mathbf{Y}}$. Furthermore, let $p_{\mathbf{Y}}$ denotes the marginal distribution over $\mathbf{Y}$, $p_{\left ( \mathbf{Y},\mathbf{X} \right )}$ be the joint distribution of $\mathbf{Y}$ and $\mathbf{X}$, and $p_{\mathbf{Y}\mid \mathbf{X}}$ be the conditional distribution of $\mathbf{Y}$ given $\mathbf{X}$. Then, the entropy of $\mathbf{Y}$ and the conditional entropy of $\mathbf{Y}$ given $\mathbf{X}$ can be defined as $\mathcal{H}\left ( \mathbf{Y} \right ):= \mathbb{E}_{p_{\mathbf{Y}}}\left [ -\log p_{\mathbf{Y}}\left ( \mathbf{Y} \right )\right ]$ and $\mathcal{H}\left ( \mathbf{Y}\mid \mathbf{X} \right ):= \mathbb{E}_{p_{\left ( \mathbf{Y},\mathbf{X} \right )}}\left [ -\log p_{\mathbf{Y}\mid \mathbf{X} }\left ( \mathbf{Y}\mid \mathbf{X} \right )\right ]$, respectively.
% In addition, the cross-entropy between $\mathbf{Y}$ and $\hat{\mathbf{Y}}$ is defined as $\mathcal{H}\left ( \mathbf{Y}; \hat{\mathbf{Y}} \right ):= \mathbb{E}_{p_{\mathbf{Y}}}\left [ -\log p_{\hat{\mathbf{Y}}}\left ( \mathbf{Y} \right )\right ]$.
Besides, let the flipped cross-entropy (\textit{f-$\mathbb{CE}$}) given
 $\mathbf{X}$ be $\mathcal{H}\left ( \mathbf{Y}; \hat{\mathbf{Y}}\mid \mathbf{X} \right ):= \mathbb{E}_{{p_{\left ( \mathbf{Y},\mathbf{X} \right )}}}\left [ -\log p_{\hat{\mathbf{Y}}\mid \mathbf{X}}\left ( \mathbf{Y}\mid \mathbf{X} \right )\right ]$.

\vspace{0.5em}
\noindent\textbf{Self-learning and mutual information.} Based on the above definitions, the \textit{f-$\mathbb{CE}$} between $\mathbf{Y}$ and $\hat{\mathbf{Y}}$ conditioned over $\mathbf{X}$ has the tight connection with the mutual information $\mathcal{I}\left ( \mathbf{Y};\mathbf{X} \right )$ between unlabeled test images $\mathbf{X}$ and the predictions from the student model $\mathbf{Y}$ as follows:

\begin{align}
%\begin{split}
\label{eq1}
%\begin{split}
\mathcal{H}\left ( \mathbf{Y}; \hat{\mathbf{Y}}\mid \mathbf{X} \right )=\mathcal{H}\left ( \mathbf{Y}\mid \mathbf{X} \right )+\mathcal{D}_{\text{KL}}\left (\mathbf{Y}\parallel\hat{\mathbf{Y}}\mid \mathbf{X}   \right )\\
=-\mathcal{I}\left ( \mathbf{Y};\mathbf{X} \right )+\mathcal{H}\left ( \mathbf{Y} \right )+\mathcal{D}_{\text{KL}}\left (\mathbf{Y}\parallel\hat{\mathbf{Y}}\mid \mathbf{X}   \right ) \nonumber
%\end{split}
\end{align} 



\begin{comment}
Let $y=f_s(x)$ represent the student's output on the unlabelled test image $x$. Similarly, let $\Hat{y}$ represent the corresponding soft-pseudo label obtained from an oracle (e.g. teacher ($f_t$)). The flipped cross-entropy (\textit{f-CE}) between $y$ and $\Hat{y}$ conditioned over $x$ is given by:
\begin{align}
    H(y,\Hat{y}|x) &= H(y|x) + \mathcal{D}_\text{KL}(y\|\Hat{y}|x)\\
    &= -I(y;x) + H(y) + \mathcal{D}_\text{KL}(y\|\Hat{y}|x)
\end{align}
\begin{align*}
\text{where,} \hspace{1em} H(y,\hat{y}|x)&=-\mathbb{E}_x \big[\textstyle\sum_{k=1}^K y_k(x) \log \hat{y}_k(x)\big]\\
H(y|x) &= -\mathbb{E}_x \big[\textstyle\sum_{k=1}^K y_k(x)\log y_k(x)\big]\\
H(y)&=-\textstyle\sum_{k=1}^K \mathbb{E}_x[y_k(x)] \log \mathbb{E}_x[y_k(x)]
\end{align*} 
\end{comment}

This implies that minimizing the \textit{f-$\mathbb{CE}$} ($ \mathcal{H}\left ( \mathbf{Y}; \hat{\mathbf{Y}}\mid \mathbf{X} \right )$ in Eq. \ref{eq1}) and maximizing the entropy of class-marginal prediction $\mathcal{H}\left ( \mathbf{Y} \right )$ is equivalent to maximizing the mutual information between the test images and the student model's predictions $\mathcal{I}\left ( \mathbf{Y};\mathbf{X} \right )$ with a correction KL-divergence term $\mathcal{D}_{\text{KL}}\left (\mathbf{Y}\parallel\hat{\mathbf{Y}}\mid \mathbf{X}   \right )$ involving soft-pseudo labels as:

\begin{equation}\label{eq:sl_mu}
    \underbrace{\mathcal{H}\left ( \mathbf{Y}; \hat{\mathbf{Y}}\mid \mathbf{X} \right )}_{\text{\textit{f-$\mathbb{CE}$}}}-\mathcal{H}\left ( \mathbf{Y} \right ) = -\underbrace{\mathcal{I}\left ( \mathbf{Y};\mathbf{X} \right )}_{\text{Mutual Info.}}  +\mathcal{D}_{\text{KL}}\left (\mathbf{Y}\parallel\hat{\mathbf{Y}}\mid \mathbf{X}   \right )
\end{equation} 
%
Thus, minimizing the left side of Eq. \ref{eq:sl_mu} with respect to the student model's parameters $\theta_s$ allows the student model to cluster test images using mutual information and criterion-corrected knowledge distillation from soft-pseudo labels. Using the above formulation, we define the following test-time objective to train the student model $f_s$ on a batch of $B$ test images $X=\{\im_1, \dots, \im_B\}$ with their corresponding soft-pseudo labels from teacher model $\Hat{Y}=\{\hat{\ps}_1, \dots, \hat{\ps}_B\}$.
\begin{multline}\label{eq:loss_pl}
    \mathcal{L}_\text{pl}(X, \Hat{Y}) = -\frac{1}{B}\sum_{i=1}^{B}\sum_{k=1}^{K}f_s(\im_i)_k\log((\Hat{\ps}_i)_k)\\
    + \sum_{k=1}^{K}\Hat{f_s}(X)_k\log(\Hat{f_s}(X))_k
\end{multline} 
%
where $\Hat{f_s}(X)=\frac{1}{B}\sum_{i=1}^{B}f_s(\im_i)$ denotes the marginal class distribution over the batch of test images $X$.

% \begin{multline}\label{eq:loss_pl}
%     \mathcal{L}_\text{pl}(\mathcal{B}) = -\frac{1}{B}\sum_{i=1}^{B}\sum_{k=1}^{K}f_s(x_i)_k\log(\Hat{y}_{ik}) \\
%     + \sum_{k=1}^{K}f_s(\mathcal{B})_k\log(f_s(\mathcal{B}))_k
% \end{multline}
% where $f_s(\mathcal{B})=\frac{1}{B}\sum_{i=1}^{B}f_s(x_i)$ denotes the marginal distribution over the batch and $K$ is the number of classes.}
% support size of $\Hat{y}_i$ (number of classes defined for the classification or segmentation task).}
% \begin{equation*}
% \begin{Bmatrix}
% \rho_1=[{\footnotesize\text{Rotate}, \text{Equalize}}]\\
% \rho_2=[{\footnotesize\text{Invert}, \text{Posterize}}]\\
% \vdots
% \end{Bmatrix}
% \begin{bmatrix}
% m^{\rho_1} & p_1\\
% m^{\rho_2} & p_2\\
% \vdots & \vdots
% \end{bmatrix}
% \end{equation*}

%\subsection{Enhancing Knowledge Distillation}\label{sec:kd_da}
\subsection{Knowledge Distillation via Augmentation}\label{sec:kd_da}
% \Devavrat{and that helps the model to learn better feature representations near the uncertainty regions of the fixed decision boundaries by utilizing teacher-student knowledge distillation}
\noindent\textbf{Self-learning from adversarial augmentations.} As illustrated in Fig. \ref{fig:kl_adver_aug}, adversarial augmentations are learned by pushing their feature representations toward the decision boundary (\textit{Expansion} phase), followed by updating the student model $f_s$ to match its prediction on these augmented images with their corresponding soft-pseudo label (\textit{Separation} phase), yielding better separation of features into their respective classes. In our method setup, we continually learn automatic augmentations of the test images (Sec. \ref{sec:leanable_aug}) that are \textbf{adversarial} to the current teacher model $f_t$ and enforce consistency between the student model’s $f_s$ predictions on the augmented views and the corresponding soft-pseudo labels. Since we freeze the classifier module, $f_s$ and $f_t$ share the common decision boundary. Let $\Tilde{\im}$ denote the learned adversarial augmented view of $\im$ and $\Hat{\ps}$ denote the corresponding soft-pseudo label. We minimize the following knowledge distillation loss $\mathcal{L}_\text{kd}$ using KL-divergence to distill knowledge from the teacher model to the student model:
\begin{equation}\label{eq:loss_kd}
    \mathcal{L}_\text{kd}(\Tilde{\im}, \Hat{\ps}) = \mathcal{D}_\text{KL}(\Hat{\ps}\| f_s(\Tilde{\im})) 
\end{equation}
%\Devavrat{As illustrated in Fig. \ref{fig:kl_adver_aug}, the augmentations that push the feature representations of test images toward the decision boundary (\textit{Expansion}), followed by updating the student model $f_s$ to match its prediction on these augmented images with their corresponding soft-pseudo label (\textit{Separation}) help better separate the features into their respective classes.}
% The usefulness of \textit{Expansion} and \textit{Separability} in the feature space for self-learning from pseudo labels is analyzed in \cite{wei2020theoretical}. Their theoretical results show that if the image representations are well \textit{separated} and have consistent class labels in the neighborhood (ensured by \textit{expansion} property), self-learning can provably reduce the error rate of the student when trained on the pseudo labels obtained from the teacher.}
%\Devavrat{In this work, we continually learn augmentations of the test images (see Section \ref{sec:leanable_aug}) that are adversarial to the current teacher $f_t$ and enforce consistency between the student's ($f_s$) predictions on the augmented views and the corresponding soft-pseudo labels. Note that since we freeze the classifier module, $f_s$ and $f_t$ share the common decision boundary.}

\vspace{0.5em}
\noindent\textbf{Soft pseudo label refinement (PLR).} We refine the quality of soft pseudo-labels and the feature representations by averaging the teacher model's outputs on multiple weakly augmented image views $\rho_w(\im)$ (via {\footnotesize\textsc{{flipping, cropping}}} augmentations) of the same test image $\im$ as follows:

\begin{equation}\label{eq:weak_aug_ensemble}
        \dense_t, \ps_t \leftarrow \mathbb{E}_{\denseu \in \rho_w(\im)}[g_t(\denseu), h_t(g_t(\denseu))]
\end{equation} 
%
The refined \textit{feature representations} $\dense_t$ and the \textit{softmaxed pseudo labels} $\ps_t$ from the encoder $g_t$ and the classifier $h_t$ are further stored in an online memory queue $\mathbf{Q}$ of fixed size. The final soft pseudo-label is computed by averaging the refined soft pseudo-labels of $n$-nearest neighbors of the current test image in the feature space as follows:

\begin{equation}\label{eq:neighbors}
\begin{gathered}
    \mathbf{Q}[\arg\max (\ps_t)] \text{.append}(\{\dense_t, \ps_t\}) \\
    \Hat{\ps} = \frac{1}{n}\sum \mathcal{N}_{\mathbf{Q},n}(\dense_t)
\end{gathered}
\end{equation}
%
where $\mathbf{Q}$ denotes an online memory of class-balanced queues (with size $|\mathbf{Q}[\arg\max (\ps_t)]|\leq N_\mathbf{Q}$) of the refined feature representations and the softmaxed label predictions on the previously seen test images, and $\mathcal{N}_{\mathbf{Q},n}(\dense_t)$ denotes soft labels of $n$-nearest neighbors of $\dense_t$ from $\mathbf{Q}$.

% \begin{equation}\label{eq:loss_kd}
%     \mathcal{L}_\text{kd}(\Tilde{x}, \Hat{y}) = -\sum_{k=1}^{K}\Hat{y}\log(f_s(\Tilde{x})) 
% \end{equation}}
% \Devavrat{Let $f=h\circ g$ denote the source model composed of an encoder $g: X\to Z$ that maps a given test image $x \in X$ to its feature representation $z\in Z$ and a classifier (hypothesis) module $h: Z\to Y$ that maps $z$ to its label $y\in Y$.}
% Following \cite{liang2020we}, we freeze the classifier $h$ and only update the parameters of the encoder. Let $\theta$ denote the parameters (all/partial) of the encoder ($g$) that are updated during test-time adaptation by minimizing a given test-time objective $\mathcal{L}_\text{Test}$ as follows:
% \begin{equation}
%     [\theta_g^{t+1}, \theta_h^{t+1}] \leftarrow [\theta_g^t, \theta_h^t] - \eta\cdot\nabla_{[\theta_g^t, \theta_h^t]}\mathcal{L}_\text{Test}(h\circ g(x))
% \end{equation} where $\eta$ is the learning rate.}
\subsection{Learning Adversarial Data Augmentation}\label{sec:leanable_aug}
This section presents our efficient online method for learning adversarial data augmentation for the current teacher $f_{t}$. We first introduce our adversarial augmentation search space. Then, we describe our differentiable strategy to optimize and sample adversarial augmentations that push the feature representations of the test images toward the uncertain region in the close vicinity of the decision boundary.

\vspace{0.5em}
\noindent\textbf{Policy search space.} Let $\mathbb{O}$ be a set of all image transformation operations $\mathcal{O}:\mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W \times 3}$ defined in our search space. In particular, we use image transformations {\footnotesize\textsc{{Auto-Contrast, Equalize, Invert, Solarize, Posterize, Contrast, Brightness, Color, ShearX, ShearY, TranslateX, TranslateY, Rotate, and Sharpness}}}. Each transformation $\mathcal{O}$ is specified with its magnitude parameter $m\in[0,1]$.
% with two parameters: (i) the magnitude of an image operation denoted by $m\in[0,1]$ and (ii) the probability of applying the operation, $p$.
Since the output of some image operations may not be conditional on its magnitude (e.g., {\footnotesize\textsc{{Equalize}}}) or may not be differentiable
% w.r.t. the magnitude
(e.g., {\footnotesize\textsc{{Posterize}}}), for such image operations, we use the straight-through gradient estimate w.r.t. each pixel as: $\partial \mathcal{O}\left ( \im_{ij} \right )/\partial m = \mathbf{1}$ for magnitude optimization.

We define a sub-policy $\rho$ as a combination of $N$ image operations (sub-policy dimension) from $\mathbb{O}$ that is applied sequentially to a given image $\im$ as:
\begin{equation}
\rho\left ( \im, m^{\rho}\right )=\mathcal{O}_{1}^{\rho}\cdots \mathcal{O}_{N}^{\rho}\left ( \im,m_{N}^{\rho} \right )
\end{equation}
%
where $m^\rho=[m_1^\rho, \dots, m_N^\rho]$ denotes the magnitude set of the sub-policy $\rho$. We also define $\mathbb{P}=\{\rho_1, \rho_2, \dots\}$ as the set of all possible sub-policies.
% \begin{comment}
% \Devavrat{\subsubsection{Policy Search Space} Let $\mathbb{O}$ be a set of image transformation operations $O:X\to X$ defined on the image space $X$. In particular, we use image transformations: ShearX, ShearY, TranslateX, TranslateY, Rotate, Auto-Contrast, Equalize, Invert, Solarize, Posterize, Contrast, Brightness, Sharpness, and Color. The magnitude of an image operation is denoted by $m\in[0,1]$. Note that the output of some image operations may not depend on its magnitude (e.g., Equalize) or may not be differentiable (e.g., Posterize). For such image operations, we use the straight-through gradient estimate ($\partial O(x_{ij})/\partial m = \mathbf{1}$) for gradient propagation. We define a sub-policy $\rho$ as a combination of $N$ image transformation operations from $\mathbb{O}$ that is applied sequentially to a given image $x$ as:
% \begin{equation}
%     \rho(x, m^\rho) = O_1^\rho\dots O_N^\rho(x, m^\rho_N)
% \end{equation} where $m^\rho=[m_1^\rho, \dots, m_N^\rho]$ denotes the magnitude set of the sub-policy $\rho$. We also define $\mathbb{P}=\{\rho_1, \rho_2, \dots\}$ as the set of all possible sub-policies.}
% \end{comment}

\vspace{0.5em}
\noindent\textbf{Policy evaluation.} We propose learnable adversarial augmentation, aiming to optimize and sample data augmentations as a proxy for simulating data in the uncertain region of the feature space. Given the teacher model $f_t$, a sub-policy $\rho$ with magnitude $m$ is evaluated for a test image $\im$ using the following adversarial objective:

\begin{equation}\label{eq:policy_eval}
    \mathcal{L}_\text{aug} (\boldsymbol{x}, \rho) = \sum_{k=1}^K f_{t}\left (\tilde{\boldsymbol{x}}  \right )\log \left (f_{t}\left (\tilde{\boldsymbol{x}}  \right )  \right )+ \lambda_1 r(\tilde{\boldsymbol{x}} , \boldsymbol{x}) 
\end{equation}
%
where $\tilde{\boldsymbol{x}}=\rho\left ( \boldsymbol{x},m \right )$ denotes the adversarial augmented image and $r\left ( \cdot,\cdot   \right )$ regularizes augmentation severity. The hyperparameter $\lambda_1$ controls the augmentation severity. We optimize the adversarial augmentation loss $\mathcal{L}_\text{aug}$ with respect to the sub-policy $\rho$'s parameters. Minimizing $\mathcal{L}_\text{aug}$ is equivalent to maximizing the entropy of the teacher model's prediction on the augmented image $\Tilde{\im}$, thus pushing its feature representation towards the decision boundary (first term). For the augmentation regularization (second term), we use the mean squared distance function between the teacher encoder's $L$ internal layers' activations of the adversarial augmentation and non-augmented versions of image $\im$ defined as:

\begin{equation}
    r(\tilde{\boldsymbol{x}}, \im) = \frac{1}{L}\sum_{l=1}^L\|\mu_l(\tilde{\boldsymbol{x}}) - \mu_l(\im)\|^2
\end{equation} 
%
where $\mu_{l}\left ( \cdot  \right )$ denotes the mean activation of the $l^{th}$ layer of the teacher encoder $g_t$.

% We sample $\tau$ different sub-policies from $\mathbb{P}$ using a learnable multinomial distribution during test time adaptation.}
\begin{comment}
\Devavrat{\subsubsection{Policy Evaluation} Given the teacher $f_t$, a sup-policy $\rho$ with magnitude $m$ is evaluated for a test image $x$ using the following adversarial objective:

\begin{equation}\label{eq:policy_eval}
    \mathcal{L}_\text{Aug} (x, \rho) = \sum_{k=1}^K f_t(\Tilde{x})\log(f_t(\Tilde{x})) + r(\Tilde{x}, x) 
\end{equation}
%
where $\Tilde{x} = \rho(x, m)$ denotes the augmented image, and $r$ regularizes augmentation severity. We use mean squared distance function between the teacher's $L$ internal layers' activations of the augmented and non-augmented versions of image $x$ as $r$. \begin{equation}
    r(\Tilde{x}, x) = \frac{1}{L}\sum_{l=1}^L\|\mu_l(\Tilde{x}) - \mu_l(x)\|^2
\end{equation} where $\mu_l$ denotes the mean activation of $l$-th layer of the teacher encoder $g_t$. Note that minimizing Eq. \ref{eq:policy_eval} is equivalent to maximizing the entropy of the teacher's prediction on the augmented image $\Tilde{x}$, thus pushing its feature representation towards the decision boundary. We optimize $\mathcal{L}_\text{Aug}$ with respect to the sub-policy's ($\rho$) parameters.}
% 
\end{comment}

% \begin{table*}[t]
%     \centering
%     \caption{Class Avg. Classification Error (\%) of several test time adaptation methods evaluated on CIFAR-10/100-C and ImageNet-C \cite{hendrycks2018benchmarking}, VisDA-C \cite{visda2017} and Kather \cite{kather_2016_53169,kather2019predicting} datasets. For the image corruption datasets, we report the average error computed over 15 test corruptions.}
    
%     \resizebox{0.85\textwidth}{!}{
%     \begin{tabular}{l|c|ccc|c}
    
%     \toprule
    
%       \multirow{2}[2]{*}{Method} & \multirow{2}[2]{*}{\begin{tabular}[c]{@{}c@{}}Test\\ Protocol\end{tabular}} & \multicolumn{3}{c}{Common Image Corruptions} & \multicolumn{1}{|c}{Syn-to-Real Transfer} \\
       
%       \cmidrule(lr){3-5} \cmidrule(lr){6-6}
       
%       \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{CIFAR10-C} & \multicolumn{1}{c}{CIFAR100-C} & 
%       \multicolumn{1}{c|}{ImageNet-C} &
%       \multicolumn{1}{c|}{VisDA-C} \\
      
    
%     \midrule
    
%     Source & - & 29.1 & 60.4 & 82.1 & 
%     51.5 \\

%     \midrule
    
%     BN & N-O  & 15.6 & 43.7 & - & 35.4 \\
%     Tent & N-O & 14.1 & 39.0 & - & 33.5 \\
%     SHOT$^{\ast}$ & N-O  & 13.9 & 39.2 & - & 29.4 \\
%     AdaContrast$^{\ast}$ & N-O  & / & / & / & 23.1 \\
    
%     \midrule
    
%     TTT++ & Y-O & - & - & / & / \\
%     TTAC & Y-O & - & - & - & / \\
    
%     \midrule
    
%     \textbf{SLug} & N-O & - & - & - & 17.8 \\
    
%     \midrule
%     \midrule
    
%     BN & N-M & 15.4 & 43.3 & 71.5 & 35.0 \\
%     Tent & N-M & 12.9 & 36.5 & 65.7 & 29.3 \\
%     SHOT$^{\ast}$ & N-M & 14.2 & 38.7 & - & 24.5 \\
%     AdaContrast$^{\ast}$ & N-M  & / & / & / & 20.2 \\
    
%     \midrule
%     TTT++ & Y-M & 12.7 & 34.4 & / & / \\
%     TTAC & Y-M & 10.0 & 34.6 & - & / \\
    
%     \midrule
    
%     \textbf{SLug} & N-M & 9.7 & 32.9 & 55.6 & 13.5 \\
    
%     \bottomrule
    
%     \end{tabular}}
%     \label{tab:results_classification}
% \end{table*}

\vspace{0.5em}
\noindent\textbf{Policy optimization.} Let $M=[m^{\rho_1},\dots,m^{\rho_{\|\mathbb{P}\|}}]$ denote the set of magnitudes $m^\rho$ of all sub-policies $\rho\in\mathbb{P}$ and $P=[p_1, \dots, p_{\|\mathbb{P}\|}]$ denote the probability of selecting a sub-policy from $\mathbb{P}$. The expected policy evaluation loss (Eq. \ref{eq:policy_eval}) for an image $\im$ over the policy search space is given by:

\begin{equation}
    \mathbb{E}[\mathcal{L}_\text{aug} (\im)] = \sum_{i=1}^{\|\mathbb{P}\|} p_i\cdot\mathcal{L}_\text{aug} (\im, \rho_i)
\end{equation}
%
Evaluating the gradient of $\mathbb{E}[\mathcal{L}_\text{aug}(\im)]$ w.r.t. $M$ and $P$ can become computationally expensive as $\|\mathbb{P}\|\sim \binom{\|\mathbb{O}\|}{N}$. Thus, we use the following re-parameterization trick to estimate its \textit{unbiased gradient}:

\begin{align}
\label{eq:unbiased}
\nabla\mathbb{E}[\mathcal{L}_\text{aug}(x)] = \delta(\im, \mathbb{P}) = \sum_{i=1}^{\|\mathbb{P}\|}\nabla(p_i\cdot\mathcal{L}_\text{aug}(\im, \rho_i))\\
=\sum_{i=1}^{\|\mathbb{P}\|}p_i(\nabla\mathcal{L}_\text{aug}(\im, \rho_i) + \mathcal{L}_\text{aug}(\im, \rho_i)\cdot\nabla \log p_i)\nonumber
\end{align} 
%
Thus, an unbiased estimator of $\delta(\im, \mathbb{P})$ can be written as:
\begin{equation}
    \Hat{\delta}(\im, \rho_i) = \nabla\mathcal{L}_\text{aug}(\im, \rho_i) + \mathcal{L}_\text{aug}(\im, \rho_i)\cdot\nabla \log p_i
\end{equation} 
%
where index $i$ is sampled from the probability distribution $P$. For an online batch of $B$ test images $\{\im_1, \im_2, \dots, \im_B\}$, we apply augmentation sub-policies $\{\rho_{i_1}, \rho_{i_2} \dots, \rho_{i_B}\}$ from $\mathbb{P}$ where $\{i_1, i_2, \dots, i_B\}$ are sampled i.i.d from distribution $P$, and update the parameters $P$ and $M$ with the following stochastic gradient update rule:
\begin{equation}\label{eq:policy_update}
    [P, M] \leftarrow [P, M] - \frac{\gamma}{B}\sum_{j=1}^B\Hat{\delta}(\im_j, \rho_{i_j})
\end{equation} 
%
where $\gamma$ is the learning rate. We set $\gamma=0.1$ for all experiments without the need for hyperparameter tuning. %showing the merit of hyperparameter insensitivity. 

%\behzad{(Any ablation for $\gamma$? It is fixed at 0.1 for all experiments.) }

\subsection{Self-Learning With Adversarial Augmentation}\label{sec:TeSLA}
We minimize the following overall objective $\mathcal{L}_\text{TeSLA}$ for training the student model $f_s$ on a batch of $B$ test images $X=\{\im_1, \dots, \im_B\}$ and their adversarial augmented views $\Tilde{X}=\{\Tilde{\im}_1, \dots, \Tilde{\im}_B\}$ with the corresponding refined soft-pseudo labels from the teacher model $\Hat{Y}=\{\hat{\ps}_1, \dots, \hat{\ps}_B\}$:

\begin{equation}\label{eq:loss_TeSLA}
    \mathcal{L}_\text{TeSLA}(X,\Tilde{X}, \Hat{Y}) = \mathcal{L}_\text{pl}(X,\Hat{Y}) + \frac{\lambda_2}{B}\sum_{i=1}^{B} \mathcal{L}_\text{kd}(\Tilde{\im}_i, \Hat{\ps}_i)
\end{equation} 
%
where $\lambda_2$ is a hyper-parameter for the knowledge distillation. The adversarial augmented views $\Tilde{X}$ are obtained by sampling \textit{i.i.d.} augmentation sub-policies from $\mathbb{P}$ for every test image in $X$ with probability $P$ and applying the corresponding magnitude from $M$. Concurrently, we also optimize $M$ and $P$ using the policy optimization mentioned in Sec. \ref{sec:leanable_aug}.

% \Devavrat{We now present our test time adaptation framework that utilizes the learnable data augmentation module. The central tenet of our test time adaptation method is to maximize test time mutual information $I$, given the model predictions, followed by knowledge distillation of the teacher model ($\Bar{f}$) to the source model ($f$).  We initialize the teacher and student model as the given source model (Eq. \ref{eq:st_init}) where the teacher is updated as the moving average of the source with momentum $m$ subsequently (Eq. \ref{eq:st_update}):
% \begin{align}
%     f \leftarrow f_\text{source}, \hspace{1em} \Bar{f} \leftarrow f_\text{source}\label{eq:st_init}\\
%     \Bar{f} \leftarrow m\cdot \Bar{f} + (1-m)\cdot f\label{eq:st_update}
% \end{align}}
% \subsubsection{Test Time Information Maximization}
% \Devavrat{Given a batch of $B$ test images ($X$) and the current student model $f$, we compute the following mutual information of the predictions ($Y$):
% \begin{multline}
%     I(Y;X) = H(Y) - H(Y|X)\\
%     = H\Big(\frac{1}{|X|}\cdot\sum_{x\in B}f(x)\Big) - \frac{1}{|X|}\cdot\sum_{x \in B}H(f(x))
% \end{multline}where $H(\cdot)$ and $H(\cdot|\cdot)$ denotes the entropy and conditional entropy respectively.}
% \subsubsection{Knowledge Distillation}\Devavrat{\textit{\textbf{Pseudo Label Refinement:}} In order to refine the pseudo labels of the current batch of test images online, we maintain a balanced queue $Q$ of the past features along with their predictions. The pseudo labels of the current batch of images are then computed as the soft-voting of the $k$ nearest neighbors. The nearest neighbors are computed using cosine similarity between the queue $Q$ and the features of the current batch predicted by the teacher encoder ($\Bar{h}$).
% \begin{align}
%     \Bar{z}, \Bar{y} &\leftarrow \Bar{h}(x), \Bar{f}(x) \hspace{2em}
%     Q[\Bar{y}] \leftarrow Q[\Bar{y}] \cup \{\Bar{z}, \Bar{y}\}\\
%     \hat{y} &= \frac{1}{k}\sum\text{NN}([\Bar{z}, \Bar{y}], Q, k)
% \end{align}where $\hat{y}$ is the refined pseudo label of image $x$ and NN$([z, y], Q, k)$ represent the labels of the $k$ nearest neighbors of $z$.}

% \Devavrat{\textit{\textbf{Adversarial Data Augmentation:}} Given an image $x$, we compute the corresponding adversarial augmented views (Section \ref{sec:leanable_aug}) $\Tilde{x}$ by sampling sub-policies from $\mathbb{P}$ with probabilities and magnitudes given by $P$ and $M$. Note that the parameters of the augmentation module are updated online in parallel using Eq. \ref{eq:policy_update}.}

% \subsection{Test-Time Objective}
% \subsection{Learnable Data Augmentation}
% \subsubsection{Search Space}
% \subsubsection{Optimizing Sub-Policies}
% \subsection{Adversarial Test-Time Training}

% \begin{lstlisting}[language=Python, caption=Pseudo Code for Adversarial Augmentation]
% def train_step(net, batch):
% """
% [batch]: batch of online test images
% [net]  : current model
% """
%     # easy augmentations [random crops, flips]
%     easy_augs = easy_augmentations(batch)
%     # avg. prediction of model on easy augmentations
%     pseudo_labels = ensemble(net, easy_augs)
%     # hard augmentations
%     hard_augs = hard_augmentations(net, batch, pseudo_labels)
%     # minimize entropy on ensemble of easy augmentations
%     loss_ent = sum(entropy(pseudo_labels))/len(batch)
%     # minimize cross-entropy on easy pseudo-labels and hard augmentations
%     w = (1. - entropy(pseudo_labels))
%     loss_ce = sum(w * cross_entropy(hard_augs, pseudo_labels)) / w.sum()
%     # update model
%     net -= lr * grad(loss_ent + loss_ce, net.weights())

% def easy_augmentations(batch):
%     return [rand_crop_flip(batch) for _ in range(aug_mult)]

% def hard_augmenatations(net, batch, pseudo_labels):
%     # sample sub-policies with magnitudes from multinomial distribution without replacement
%     sps, ms = multinomial([prob_sub_policy, mag_sub_policy], aug_mult, replace=False)
%     # perform augmentations
%     hard_aug = [sp(batch, m) for (sp, m) in zip(sps, ms)]
%     # compute policy loss
%     loss = -cross_entropy(net(hard_aug), pseudo_labels) + class_marginals(net(hard_aug)) + bn_loss(hard_aug, batch)
%     # update magnitude of selected sub-policies
%     mag_sub_policy -= lr * grad(loss, ms)
%     # update sub-policy selection probabilities
%     prob_sub_policy -= loss * grad(log(prob_sub_policy[sps]), prob_sub_policy)
%     # return hard augmentations
%     return hard_aug
    
% \end{lstlisting}